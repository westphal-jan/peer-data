{"id": "1501.01460", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jan-2015", "title": "Median evidential c-means algorithm and its application to community detection", "abstract": "Median clustering is of great value for partitioning relational data. In this paper, a new prototype-based clustering method, called Median Evidential C-Means (MECM), which is an extension of median c-means and median fuzzy c-means on the theoretical framework of belief functions is proposed. The median variant relaxes the restriction of a metric space embedding for the objects but constrains the prototypes to be in the original data set. Due to these properties, MECM could be applied to graph clustering problems. A community detection scheme for social networks based on MECM is investigated and the obtained credal partitions of graphs, which are more refined than crisp and fuzzy ones, enable us to have a better understanding of the graph structures. An initial prototype-selection scheme based on evidential semi-centrality is presented to avoid local premature convergence and an evidential modularity function is defined to choose the optimal number of communities. Finally, experiments in synthetic and real data sets illustrate the performance of MECM and show its difference to other methods. The method, based on a general principle of deterministic clustering, is described in a paper to study how it works in a functional framework. The paper describes the principles of the new model and proposes a model based on the theoretical framework of belief function theory. The proposal of the new model is based on the concept of a deterministic cluster.\n\n\nIn the previous paper, we demonstrated how MECM can reduce the accuracy of the results, which is why it was proposed to reduce the accuracy of the results. As a practical example, in real-time, we have implemented MECM and MECM to control the accuracy of the results. In this paper, we present a new algorithm, called the Prediction Model. It implements a model based on a single algorithm (which was implemented at the time of publication of this paper). This algorithm shows how it predicts the accuracy of the results, and the algorithm that optimizes it. In this paper, we are looking to show how to increase the accuracy of the results, which is why it was proposed to reduce the accuracy of the results. The algorithm is based on a single algorithm (which was implemented at the time of publication of this paper). This algorithm shows how it predicts the accuracy of the results, and the algorithm that optimizes it. In this paper, we are looking to show how to increase the accuracy of the results, which is why it was proposed to reduce the accuracy", "histories": [["v1", "Wed, 7 Jan 2015 12:16:50 GMT  (514kb)", "http://arxiv.org/abs/1501.01460v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.SI", "authors": ["kuang zhou", "arnaud martin", "quan pan", "zhun-ga liu"], "accepted": false, "id": "1501.01460"}, "pdf": {"name": "1501.01460.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Kuang Zhou", "Arnaud Martin", "Quan Pan", "Zhun-ga Liu"], "emails": ["kzhoumath@163.com", "Arnaud.Martin@univ-rennes1.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 1.\n01 46\n0v 1\n[ cs\n.A I]\n7 J\nan 2\n01 5\nMedian clustering is of great value for partitioning relational data. In this paper, a new prototypebased clustering method, called Median Evidential C-Means (MECM), which is an extension of median c-means and median fuzzy c-means on the theoretical framework of belief functions is proposed. The median variant relaxes the restriction of a metric space embedding for the objects but constrains the prototypes to be in the original data set. Due to these properties, MECM could be applied to graph clustering problems. A community detection scheme for social networks based on MECM is investigated and the obtained credal partitions of graphs, which are more refined than crisp and fuzzy ones, enable us to have a better understanding of the graph structures. An initial prototype-selection scheme based on evidential semi-centrality is presented to avoid local premature convergence and an evidential modularity function is defined to choose the optimal number of communities. Finally, experiments in synthetic and real data sets illustrate the performance of MECM and show its difference to other methods.\nKeywords: Credal partition, Belief function theory, Median clustering, Community detection, Imprecise communities"}, {"heading": "1. Introduction", "text": "Cluster analysis or clustering is the task of partitioning a set of n objectsX = {x1,x2, \u00b7 \u00b7 \u00b7 ,xn} into c small groups \u2126 = {\u03c91, \u03c92, \u00b7 \u00b7 \u00b7 , \u03c9c} in such a way that objects in the same group (called a cluster) are more similar (in some sense or another, like characteristics or behavior) to each other than to those in other groups. The clustering can be used in many fields such as privacy preserving [24] , information retrieval [5], text analysis [47], etc. It can also be used as the first step of classification problems to identify the distribution of the training set [44]. Among the existing approaches to clustering, the objective function-driven or prototype-based clustering such as cmeans and Gaussian mixture modeling is one of the most widely applied paradigms in statistical pattern recognition. These methods are based on a fundamentally very simple, but nevertheless very effective idea, namely to describe the data under consideration by a set of prototypes. They capture the characteristics of the data distribution (like location, size, and shape), and classify the data set based on the similarities (or dissimilarities) of the objects to their prototypes [6].\nGenerally, a c-partition of n objects in X is a set of n\u00d7c values {uij} arrayed as an n\u00d7c matrix U . Each element uij is the membership of xi to cluster j. The classical C-Means (CM) method\n\u2217Corresponding author Email addresses: kzhoumath@163.com (Kuang Zhou), Arnaud.Martin@univ-rennes1.fr (Arnaud Martin) The figures are not displayed in this version due to the limitation of size. The complete version could be found in http://www.sciencedirect.com/science/article/pii/S095070511400402X or contact the authors to get the paper.\nPreprint submitted to Elsevier January 8, 2015\naims to partition n observations into c groups in which each observation belongs to the class with the nearest mean, serving as a prototype of the cluster. It results in uij is either 0 or 1 depending whether object i is grouped into cluster j, and thus each data point is assigned to a single cluster (hard partitions). Fuzzy C-Means (FCM), proposed by Dunn [11] and later improved by Bezdek [3], is an extension of c-means where each data point can be a member of multiple clusters with membership values (fuzzy partitions) [25].\nBelief functions have already been used to express partial information about data both in supervised and unsupervised learning [32, 41]. Recently, Masson and Denoeux [32] proposed the application of evidential c-means (ECM) to get credal partitions [9] for object data. The credal partition is a general extension of the crisp (hard) and fuzzy ones and it allows the object not only to belong to single clusters, but also to belong to any subsets of \u2126 by allocating a mass of belief for each object in X over the power set 2\u2126. The additional flexibility brought by the power set provides more refined partitioning results than those by the other techniques allowing us to gain a deeper insight into the data [32].\nAll of these aforementioned partition approaches are prototype-based. In CM, FCM and ECM, the prototypes of clusters are the geometric centers of included data points in the corresponding groups. However, this may be inappropriate as it is the case in community detection problems for social networks, where the prototype (center) of one group is likely to be one of the persons (i.e. nodes in the graph) playing the leader role in the community. That is to say, one of the points in the group is better to be selected as a prototype, rather than the center of all the points. Thus we should set some constraints for the prototypes, for example, let them be data objects. Actually this is the basic principle of median clustering methods [16]. These restrictions on prototypes can relax the assumption of a metric space embedding for the objects to be clustered [16, 18], and only similarity or dissimilarity between data objects is required. There are some clustering methods for relational data, such as Relational FCM (RFCM) [20] and Relational ECM (RECM) [33], but an underlying metric is assumed for the given dissimilarities between objects. However, in median clustering this restriction is dropped [16]. Cottrell et al. [8] proposed Median C-Means clustering method (MCM) which is a variant of the classic c-means and proved the convergence of the algorithm. Geweniger et al. [16] combined MCM with the fuzzy c-means approach and investigated the behavior of the resulted Median Fuzzy C-means (MFCM) algorithm.\nCommunity detection, which can extract specific structures from complex networks, has attracted considerable attention crossing many areas from physics, biology, and economics to sociology. Recently, significant progress has been achieved in this research field and several popular algorithms for community detection have been presented. One of the most popular type of classical methods partitions networks by optimizing some criteria. Newman and Girvan [35] proposed a network modularity measure (usually denoted by Q) and several algorithms that try to maximize Q have been designed [4, 7, 10, 42]. But recent researches have found that the modularity based algorithms could not detect communities smaller than a certain size. This problem is famously known as the resolution limit [14]. The single optimization criteria i.e. modularity may not be adequate to represent the structures in complex networks, thus Amiri et al. [1] suggested a new community detection process as a multi-objective optimization problem. Another family of approaches considers hierarchical clustering techniques. It merges or splits clusters according to a topological measure of similarity between the nodes and tries to build a hierarchical tree of partitions [27, 37, 43]. Also there are some ways, such as spectral methods [40] and signal process method [23, 26], to map topological relationship of nodes on networks into geometrical structures\nof vectors in n-dimensional Euclidian space, where classical clustering methods like CM, FCM and ECM could be evoked. However, there must be some loss of accuracy after the mapping process. As mentioned before, for community detection, the prototypes should be some nodes in the graph. Besides, usually only dissimilarities between nodes are known to us. Due to the application of the relaxation on the data objects and the constraints on the prototypes, the median clustering could be applied to the community detection problem in social networks.\nIn this paper, we extend the median clustering methods in the framework of belief functions theory and put forward the Median Evidential C-Means (MECM) algorithm. Moreover, a community detection scheme based on MECM is also presented. Here, we emphasize two key points different from those earlier studies. Firstly, the proposed approach could provide credal partitions for data set with only known dissimilarities. The dissimilarity measure could be neither symmetric nor fulfilling any metric requirements. It is only required to be of intuitive meaning. Thus it expands application scope of credal partitions. Secondly, some practical issues about how to apply the method into community detection problems such as how to determine the initial prototypes and the optimum community number in the sense of credal partitions are discussed. This makes the approach appropriate for graph partitions and gives us a better understanding of the analysed networks, especially for the uncertain and imprecise structures.\nThe rest of this paper is organized as follows: Section 2 recalls the necessary background related to this paper. In Section 3, the median c-means algorithm is presented and in section 4, we show how the proposed method could be applied in the community detection problem. In order to show the effectiveness of our approach, in section 5 we test our algorithm on artificial and real-world data sets and make comparisons with different methods. The final section makes the conclusions."}, {"heading": "2. Background", "text": ""}, {"heading": "2.1. Theory of belief functions", "text": "Let \u2126 = {\u03c91, \u03c92, . . . , \u03c9c} be the finite domain of X , called the discernment frame. The mass\nfunction is defined on the power set 2\u2126 = {A : A \u2286 \u2126}.\nDefinition 1. The function m : 2\u2126 \u2192 [0, 1] is said to be the Basic Belief Assignment (bba) on 2\u2126, if it satisfies:\n\u2211\nA\u2286\u2126\nm(A) = 1. (1)\nEvery A \u2208 2\u2126 such that m(A) > 0 is called a focal element. The credibility and plausibility functions are defined in Eq. (2) and Eq. (3).\nBel(A) = \u2211\nB\u2286A,B 6=\u2205\nm(B), \u2200A \u2286 \u2126, (2)\nPl(A) = \u2211\nB\u2229A 6=\u2205\nm(B), \u2200A \u2286 \u2126. (3)\nEach quantity Bel(A) measures the total support given to A, while Pl(A) can be interpreted as the degree to which the evidence fails to support the complement of A. The function pl : \u2126 \u2192 [0, 1] such that pl(\u03c9i) = Pl({\u03c9i}) (\u03c9i \u2208 \u2126) is called the contour function associated with m. A belief function on the credal level can be transformed into a probability function by Smets method. In this algorithm, each mass of belief m(A) is equally distributed among the elements of A [39]. This\nleads to the concept of pignistic probability, BetP , defined by\nBetP (\u03c9i) = \u2211\n\u03c9i\u2208A\u2286\u2126\nm(A)\n|A|(1 \u2212m(\u2205)) , (4)\nwhere |A| is the number of elements of \u2126 in A.\nPignistic probabilities, which play the same role as fuzzy membership, can easily help us make a decision. In fact, belief functions provide us many decision-making techniques not only in the form of probability measures. For instance, a pessimistic decision can be made by maximizing the credibility function, while maximizing the plausibility function could provide an optimistic one [31]. Another criterion [31] considers the plausibility functions and consists in attributing the class Aj for object i if\nAj = argmax X\u2286\u2126\n{mbi(X)Pli(X)}, (5)\nwhere\nmbi(X) = K b i\u03bbX\n(\n1\n|X |r\n)\n. (6)\nIn Eq. (5) mbi(X) is a weight on Pli(X), and r is a parameter in [0, 1] allowing a decision from a simple class (r = 1) until the total ignorance \u2126 (r = 0). The value \u03bbX allows the integration of the lack of knowledge on one of the focal sets X \u2286 \u2126, and it can be set to be 1 simply. Coefficient Kbi is the normalization factor to constrain the mass to be in the closed world:\nKbi = 1\n1\u2212mi(\u2205) . (7)\n2.2. Median c-means and median fuzzy c-means\nMedian c-means is a variant of the traditional c-means method [8, 16]. We assume that n (p-dimensional) data objects xi = {xi1, xi2, \u00b7 \u00b7 \u00b7 , xip} (i = 1, 2, \u00b7 \u00b7 \u00b7 , n) are given. The object set is denoted by X = {x1,x2, \u00b7 \u00b7 \u00b7 ,xn}. The objective function of MCM is similar to that in CM:\nJMCM = c \u2211\nj=1\nn \u2211\ni=1\nuijd 2 ij , (8)\nwhere c is the number of clusters. As MCM is based on crisp partitions, uij is either 0 or 1 depending whether xi is in cluster j. The value dij is the dissimilarity between xi and the prototype vector vj of cluster j (i = 1, 2, \u00b7 \u00b7 \u00b7 , n, j = 1, 2, \u00b7 \u00b7 \u00b7 , c), which is not assumed to be fulfilling any metric properties but should reflect the common sense of dissimilarity. Due to these weak assumptions, data object xi itself may be a general choice and it does not have to live in a metric space [16]. The main difference between MCM and CM is that the prototypes of MCM are restricted to the data objects.\nMedian fuzzy c-means (MFCM) merges MCM and the standard fuzzy c-means (FCM). As in MCM, it requires the knowledge of the dissimilarity between data objects, and the prototypes are restricted to the objects themselves [16]. MFCM also performs a two-step iteration scheme to minimize the cost function\nJMFCM =\nc \u2211\nj=1\nn \u2211\ni=1\nu\u03b2ijd 2 ij , (9)\nsubject to the constrains c\n\u2211\nk=1\nuik = 1, \u2200i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , n}, (10)\nand n \u2211\ni=1\nuik > 0, \u2200k \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , c}, (11)\nwhere each number uik \u2208 [0, 1] is interpreted as a degree of membership of object i to cluster k, and \u03b2 > 1 is a weighting exponent that controls the fuzziness of the partition. Again, MFCM is preformed by alternating update steps as for MCM:\n\u2022 Assignment update:\nuij = d \u22122/(\u03b2\u22121) ij\n\u2211c k=1 d \u22122/(\u03b2\u22121) ik\n. (12)\n\u2022 Prototype update: the new prototype of cluster j is set to be vj = xl\u2217 with\nxl\u2217 = arg min {vj :vj=xl(\u2208X)}\nn \u2211\ni=1\nu\u03b2ijd 2 ij . (13)\n2.3. Evidential c-means\nEvidential c-means [32] is a direct generalization of FCM in the framework of belief functions, and it is based on the credal partition first proposed by Den\u0153ux and Masson [9]. The credal partition takes advantage of imprecise (meta) classes [30] to express partial knowledge of class memberships. The principle is different from another belief clustering method put forward by Schubert [38], in which conflict between evidence is utilised to cluster the belief functions related to multiple events. In ECM, the evidential membership of an object xi = {xi1,xi2, \u00b7 \u00b7 \u00b7 ,xip} is represented by a bba mi = (mi(Aj) : Aj \u2286 \u2126) over the given frame of discernment \u2126 = {\u03c91, \u03c92, \u00b7 \u00b7 \u00b7 , \u03c9c}. The optimal credal partition is obtained by minimizing the following objective function:\nJECM =\nn \u2211\ni=1\n\u2211\nAj\u2286\u2126,Aj 6=\u2205\n|Aj | \u03b1mi(Aj) \u03b2d2ij +\nn \u2211\ni=1\n\u03b42mi(\u2205) \u03b2 (14)\nconstrained on \u2211\nAj\u2286\u2126,Aj 6=\u2205\nmi(Aj) +mi(\u2205) = 1, (15)\nwhere mi(Aj) , mij is the bba of xi given to the nonempty set Aj , while mi(\u2205) , mi\u2205 is the bba of xi assigned to the emptyset, and | \u00b7 | is the cardinality of the set. Parameter \u03b1 is a tuning parameter allowing to control the degree of penalization for subsets with high cardinality, parameter \u03b2 is a weighting exponent and \u03b4 is an adjustable threshold for detecting the outliers. It is noted that for credal partitions, j is not from 1 to c as before, but ranges in [0, f ] with f = 2c. Here dij denotes the Euclidean distance between xi and the barycenter (i.e. prototype, denoted by vj) associated with Aj :\nd2ij = \u2016xi \u2212 vj\u2016 2, (16)\nwhere vj is defined mathematically by\nvj = 1\n|Aj |\nc \u2211\nk=1\nskjvk, with skj =\n \n\n1 if \u03c9k \u2208 Ak 0 else . (17)\nThe notation \u2016 \u00b7 \u2016 denotes the Euclidean norm of a vector, and vk is the geometrical center of all the points in cluster k. The update for ECM is given by the following two alternating steps and the update formulas can been obtained by Lagrange multipliers method.\n\u2022 Assignment update:\nmij = |Aj |\u2212\u03b1/(\u03b2\u22121)d \u22122/(\u03b2\u22121) ij\n\u2211\nAk 6=\u2205\n|Ak|\u2212\u03b1/(\u03b2\u22121)d \u22122/(\u03b2\u22121) ik + \u03b4\n\u22122/(\u03b2\u22121) , \u2200i = 1, 2 \u00b7 \u00b7 \u00b7 , n, \u2200j/Aj \u2286 \u2126, Aj 6= \u2205 (18)\nmi\u2205 = 1\u2212 \u2211\nAj 6=\u2205\nmij , \u2200i = 1, 2, \u00b7 \u00b7 \u00b7 , n. (19)\n\u2022 Prototype update: The prototypes (centers) of the classes are given by the rows of the matrix\nvc\u00d7p, which is the solution of the following linear system:\nHV = B, (20)\nwhere H is a matrix of size (c\u00d7 c) given by\nHlk = \u2211\ni\n\u2211\nAjk{\u03c9k,\u03c9l}\n|Aj | \u03b1\u22122m\u03b2ij , k, l = 1, 2, \u00b7 \u00b7 \u00b7 , c, (21)\nand B is a matrix of size (c\u00d7 p) defined by\nBlq =\nn \u2211\ni=1\nxiq \u2211\nAj\u220b\u03c9l\n|Aj | \u03b1\u22121m\u03b2ij , l = 1, 2, \u00b7 \u00b7 \u00b7 , c, q = 1, 2, \u00b7 \u00b7 \u00b7 , p. (22)"}, {"heading": "2.4. Some concepts for social networks", "text": "In this work we will investigate how the proposed clustering algorithm could be applied to community detection problems in social networks. In this section some concepts related to social networks will be recalled."}, {"heading": "2.4.1. Centrality and dissimilarity", "text": "The problem of assigning centrality values to nodes in graphs has been widely investigated as it is important for identifying influential nodes [48]. Gao et al. [15] put forward Evidential Semi-local Centrality (ESC) and pointed out that it is more reasonable than the existing centrality measures such as Degree Centrality (DC), Betweenness Centrality (BC) and Closeness Centrality (CC). In the application of ESC, the degree and strength of each node are first expressed by bbas, and then the fused importance is calculated using the combination rule in belief function theory. The higher the ESC value is, the more important the node is. The detail computation process of ESC can be found in [15].\nThe similarity or dissimilarity index signifies to what extent the proximity between two vertices of a graph is. The dissimilarity measure considered in this paper is the one put forward by Zhou [49]. This index relates a network to a discrete-time Markov chain and utilises the mean first-passage time to express the distance between two nodes. One can refer to [49] for more details."}, {"heading": "2.4.2. Modularity", "text": "Recently, many criteria were proposed for evaluating the partition of a network. A widely used measure called modularity, or Q was presented by Newman and Girvan [35]. Given a hard partition with c groups (\u03c91, \u03c92, \u00b7 \u00b7 \u00b7 , \u03c9c) U = (uik)n\u00d7c, where uik is one if vertex i belongs to the kth community, 0 otherwise, and let the c crisp subsets of vertices be {V1, V2, \u00b7 \u00b7 \u00b7 , Vc}, then the modularity can be defined as [13]:\nQh = 1\n\u2016W \u2016\nc \u2211\nk=1\n\u2211\ni,j\u2208Vk\n(\nwij \u2212 kikj \u2016W \u2016\n)\n, (23)\nwhere \u2016W \u2016 = \u2211n i,j=1 wij , ki = \u2211n j=1 wij . The node subsets {Vk, k = 1, 2, \u00b7 \u00b7 \u00b7 , c} are determined by the hard partition Un\u00d7c, but the role of U is somewhat obscured by this form of modularity function. To reveal the role played by the partition U explicitly, Havens et al. [21] rewrote the equations in the form of U . Let k = (k1, k2, \u00b7 \u00b7 \u00b7 , kn)T , B = W \u2212 kTk/ \u2016W\u2016, then\nQh = 1\n\u2016W \u2016\nc \u2211\nk=1\nn \u2211\ni,j=1\n(\nwij \u2212 kikj \u2016W \u2016\n)\nuikujk\n= 1\n\u2016W \u2016\nc \u2211\nk=1\nukBu T k\n= trace ( UTBU ) /\u2016W \u2016, (24)\nwhere uk = (u1k, u2k, \u00b7 \u00b7 \u00b7 , unk) T .\nHavens et al. [21] pointed out that an advantage of Eq. (24) is that it is well defined for any\npartition of the nodes not just crisp ones. The fuzzy modularity of U was derived as\nQf = trace ( UTBU ) /\u2016W \u2016, (25)\nwhere U is the membership matrix and uik represents the membership of community k for node i. If uik is restricted in [0, 1], the fuzzy partition degrades to the hard one, and so Qf equals to Qh at this time.\n3. Median Evidential C-Means (MECM) approach\nWe introduce here median evidential c-means in order to take advantages of both median clustering and credal partitions. Like all the prototype-based clustering methods, for MECM, an objective function should first be found to provide an immediate measure of the quality of clustering results. Our goal then can be characterized as the optimization of the objective function to get the best credal partition."}, {"heading": "3.1. The objective function of MECM", "text": "To group n objects in X = {x1,x2, \u00b7 \u00b7 \u00b7 ,xn} into c clusters \u03c91, \u03c92, \u00b7 \u00b7 \u00b7 , \u03c9c, the credal partition M = {m1,m2, \u00b7 \u00b7 \u00b7 ,mn} defined on \u2126 = {\u03c91, \u03c92, \u00b7 \u00b7 \u00b7 , \u03c9c} is used to represent the class membership of the objects, as in [9, 32]. The quantities mij = mi(Aj) (Aj 6= \u2205, Aj \u2286 \u2126) are determined by the dissimilarity between object xi and focal set Aj which has to be defined first.\nLet the prototype set of specific (singleton) cluster be V = {v1,v2, \u00b7 \u00b7 \u00b7 ,vc}, where vi is the prototype vector of cluster \u03c9i (i = 1, 2, \u00b7 \u00b7 \u00b7 , c) and it must be one of the n objects. If |Aj | = 1, i.e.,\nAj is associated with one of the singleton clusters in \u2126 (suppose to be \u03c9j with prototype vector vj), then the dissimilarity between xi and Aj is defined by\nd 2\nij = d 2(xi,vj), (26)\nwhere d(xi,xj) represents the known dissimilarity between object xi and xj . When |Aj | > 1, it represents an imprecise (meta) cluster. If object xi is to be partitioned into a meta cluster, two conditions should be satisfied. One is the dissimilarity values between xi and the included singleton classes\u2019 prototypes are similar. The other is the object should be close to the prototypes of all these specific clusters. The former measures the degree of uncertainty, while the latter is to avoid the pitfall of partitioning two data objects irrelevant to any included specific clusters into the corresponding imprecise classes. Let the prototype vector of the imprecise cluster associated with Aj be vj , then the dissimilarity between xi and Aj can be defined as:\nd 2\nij =\n\u03b3 1|Aj | \u2211\n\u03c9k\u2208Aj\nd2(xi,vk) + \u03c1ij min{d(xi,vk) : \u03c9k \u2208 Aj}\n\u03b3 + 1 , (27)\nwith\n\u03c1ij =\n\u2211\n\u03c9x,\u03c9y\u2208Aj\n\u221a\n(d (xi,vx)\u2212 d(xi,vy)) 2\n\u03b7 \u2211\n\u03c9x,\u03c9y\u2208Aj\nd(vx,vy) . (28)\nIn Eq. (27) \u03b3 weights the contribution of the dissimilarity of the objects from the consisted specific clusters and it can be tuned according to the applications. If \u03b3 = 0, the imprecise clusters only consider our uncertainty. Discounting factor \u03c1ij reflects the degree of uncertainty. If \u03c1ij = 0, it means that all the dissimilarity values between xi and the included specific classes in Aj are equal, and we are absolutely uncertain about which cluster object xi is actually in. Parameter \u03b7 (\u2208 [0, 1]) can be tuned to control of the discounting degree. In credal partitions, we can distinguish between \u201cequal evidence\u201d (uncertainty) and \u201cignorance\u201d. The ignorance reflects the indistinguishability among the clusters. In fact, imprecise classes take both uncertainty and ignorance into consideration, and we can balance the two types of imprecise information by adjusting \u03b3. Therefore, the dissimilarity between xi and Aj(Aj 6= \u2205, Aj \u2286 \u2126), dij , can be calculated by\nd 2\nij =\n  \n \nd2(xi,vj) |Aj | = 1, \u03b3 1\n|Aj |\n\u2211\n\u03c9k\u2208Aj\nd2(xi,vk)+\u03c1ij min{d(xi,vk):\u03c9k\u2208Aj}\n\u03b3+1 |Aj | > 1\n. (29)\nLike ECM, we propose to look for the credal partition M = {m1,m2, \u00b7 \u00b7 \u00b7 ,mn} \u2208 Rn\u00d72 c and the prototype set V = {v1,v2, \u00b7 \u00b7 \u00b7 ,vc} of specific (singleton) clusters by minimizing the objective function:\nJMECM(M ,V ) =\nn \u2211\ni=1\n\u2211\nAj\u2286\u2126,Aj 6=\u2205\n|Aj | \u03b1m\u03b2ijd\n2 ij +\nn \u2211\ni=1\n\u03b42m\u03b2i\u2205, (30)\nconstrained on \u2211\nAj\u2286\u2126,Aj 6=\u2205\nmij +mi\u2205 = 1, (31)\nwhere mij , mi(Aj) is the bba of ni given to the nonempty set Aj , mi\u2205 , mi(\u2205) is the bba of ni assigned to the empty set, and dij is the dissimilarity between xi and focal set Aj . Parameters\n\u03b1, \u03b2, \u03b4 are adjustable with the same meanings as those in ECM. Note that JMECM depends on the credal partition M and the set V of all prototypes."}, {"heading": "3.2. The optimization", "text": "To minimize JMECM, an optimization scheme via an Expectation-Maximization (EM) algorithm as in MCM [8] and MFCM [16] can be designed, and the alternate update steps are as follows:\nStep 1. Credal partition (M) update.\n\u2022 \u2200Aj \u2286 \u2126, Aj 6= \u2205,\nmij = |Aj |\n\u2212\u03b1/(\u03b2\u22121)d \u22122/(\u03b2\u22121) ij\n\u2211\nAk 6=\u2205\n|Ak|\u2212\u03b1/(\u03b2\u22121)d \u22122/(\u03b2\u22121) ik + \u03b4 \u22122/(\u03b2\u22121)\n(32)\n\u2022 if Aj = \u2205,\nmi\u2205 = 1\u2212 \u2211\nAj 6=\u2205\nmij (33)\nStep 2. Prototype (V ) update.\nThe prototype vi of a specific (singleton) cluster \u03c9i (i = 1, 2, \u00b7 \u00b7 \u00b7 , c) can be updated first and then the dissimilarity between the object and the prototype of each imprecise (meta) clusters associated with subset Aj \u2286 \u2126 can be obtained by Eq. (29). For singleton clusters \u03c9k (k = 1, 2, \u00b7 \u00b7 \u00b7 , c), the corresponding new prototypes vk (k = 1, 2, \u00b7 \u00b7 \u00b7 , c) are set to be sample xl orderly, with\nxl = argmin v \u2032\nk\n \n\nL(v \u2032\nk) ,\nn \u2211\ni=1\n\u2211\n\u03c9k\u2208Aj\n|Aj | \u03b1m\u03b2ijd\n2 ij(v \u2032 k),v \u2032 k \u2208 {x1,x2, \u00b7 \u00b7 \u00b7 ,xn}\n \n\n, (34)\nThe dissimilarity between xi and Aj , d 2 ij , is a function of v \u2032 k, which is the prototype of \u03c9k(\u2208 Aj), and it should be one of the n objects in X = {x1,x2, \u00b7 \u00b7 \u00b7 ,xn}.\nThe bbas of the objects\u2019 class membership are updated identically to ECM [32], but it is worth noting that dij has different meanings and less constraints as explained before. For the prototype updating process the fact that the prototypes are assumed to be one of the data objects is taken into account. Therefore, when the credal partition matrix M is fixed, the new prototypes of the clusters can be obtained in a simpler manner than in the case of ECM application. The MECM algorithm is summarized as Algorithm 1.\nThe convergence of MECM algorithm can be proved in the following lemma, similar to the\nproof of median neural gas [8] and MFCM [16].\nLemma 1. The MECM algorithm (Algorithm 1) converges in a finite number of steps.\nProof: Suppose \u03b8(t) = (Mt,Vt) and \u03b8 (t+1) = (Mt+1,Vt+1) are the parameters from two successive\niterations of MECM. We will first prove that\nJMECM (\u03b8 (t)) \u2265 JMECM (\u03b8 (t+1)), (35)\nAlgorithm 1 : Median evidential c-means algorithm\nInput dissimilarity matrix D , [d(xi, xj)]n\u00d7n for the n objects {x1,x2, \u00b7 \u00b7 \u00b7 ,xn} Parameters c: number clusters 1 < c < n\n\u03b1: weighing exponent for cardinality \u03b2 > 1: weighting exponent \u03b4 > 0: dissimilarity between any object to the emptyset \u03b3 > 0: weight of dissimilarity between data and prototype vectors \u03b7 \u2208 [0, 1]: control of the discounting degree\nInitialization Choose randomly c initial cluster prototypes from the objects\nLoop t \u2190 0 Repeat (1). t \u2190 t+ 1 (2). Compute Mt using Eq. (32), Eq. (33) and Vt\u22121 (3). Compute the new prototype set Vt using Eq. (34) Until the prototypes remain unchanged\nwhich shows MECM always monotonically decreases the objective function. Let\nJ (t) MECM =\nn \u2211\ni=1\n\u2211\nAj\u2286\u2126,Aj 6=\u2205\n|Aj | \u03b1(m (t) ij ) \u03b2(d (t) ij ) 2 +\nn \u2211\ni=1\n\u03b42(m (t) i\u2205 ) \u03b2\n, \u2211\ni\n\u2211\nj\nf1(Mt)f2(Vt) + \u2211\ni\nf3(Mt), (36)\nwhere f1(Mt) = |Aj |\u03b1(m (t) ij ) \u03b2, f2(Vt) = (d (t) ij ) 2, and f3(Mt) = \u03b4 2(m (t) i\u2205 ) \u03b2. Mt+1 is then obtained by maximizing the right hand side of the equation above. Thus,\nJ (t) MECM \u2265\n\u2211\ni\n\u2211\nj\nf1(Mt+1)f2(Vt) + \u2211\ni\nf3(Mt+1) (37)\n\u2265 \u2211\ni\n\u2211\nj\nf1(Mt+1)f2(Vt+1) + \u2211\ni\nf3(Mt+1) (38)\n= J (t+1) MECM . (39)\nThis inequality (37) comes from the fact Mt+1 is determined by differentiating of the respective\nLagrangian of the cost function with respect to Mt. To get Eq. (38), we could use the fact that\nevery prototype vk (k = 1, 2, \u00b7 \u00b7 \u00b7 , c) in Vt+1 is orderly chosen explicitly to be\nargmin v \u2032\nk\n \n\nL(v \u2032 k) , n \u2211\ni=1\n\u2211\n\u03c9k\u2208Aj\n|Aj | \u03b1m\u03b2ijd\n2 ij(v \u2032 k),v \u2032 k \u2208 {x1,x2, \u00b7 \u00b7 \u00b7 ,xn}\n \n\n,\nand thus this formula evaluated at Vt+1 must be equal to or less than the same formula evaluated\nat Vt.\nHence MECM causes the objective function to converge monotonically. Moreover, the bba M\nis a function of the prototypes V and for given V the assignment M is unique. Because MECM\nassumes that the prototypes are original object data in X, so there is a finite number of different\nprototype vectors V and so is the number of corresponding credal partitions M . Consequently we\ncan get the conclusion that the MECM algorithm converges in a finite number of steps.\nRemark 1. Although the objective function of MECM takes the same form as that in ECM [32], we should note that in MECM, it is no longer assumed that there is an underlying Euclidean distance. Thus the dissimilarity measure dij has few restrictions such as the triangle inequality or the symmetry. This freedom distinguishes the MECM from ECM and RECM, and it leads to the constraint for the prototypes to be data objects themselves. The distinct difference in the process of minimization between MECM and ECM lies in the prototype-update step. The purpose of updating the prototypes is to make sure that the cost function would decrease. In ECM the Lagrange multiplier optimization is evoked directly while in MECM a search method is applied. As a result, the objective function may decline more quickly in ECM as the optimization process has few constraints. However, when the centers of clusters in the data set are more likely to be the data object, MECM may converge with few steps.\nRemark 2. Although both MECM and MFCM can be applied to the same type of data set, they are very different. This is due to the fact that they are founded on different models of partitioning. MFCM provides fuzzy partition. In contrast, MECM gives credal partitions. We emphasize that MECM is in line with MCM and MFCM: each class is represented by a prototype which is restricted to the data objects and the dissimilarities are not assumed to be fulfilling any metric properties. MECM is an extension of MCM and MFCM in the framework of belief functions."}, {"heading": "3.3. The parameters of the algorithm", "text": "As in ECM, before running MECM, the values of the parameters have to be set. Parameters \u03b1, \u03b2 and \u03b4 have the same meanings as those in ECM, and \u03b3 weighs the contribution of uncertainty to the dissimilarity between nodes and imprecise clusters. The value \u03b2 can be set to be \u03b2 = 2 in all experiments for which it is a usual choice. The parameter \u03b1 aims to penalize the subsets with high cardinality and control the amount of points assigned to imprecise clusters in both ECM and MECM. As the measures for the dissimilarity between nodes and meta classes are different, thus different values of \u03b1 should be taken even for the same data set. But both in ECM and MECM, the higher \u03b1 is, the less mass belief is assigned to the meta clusters and the less imprecise will be the resulting partition. However, the decrease of imprecision may result in high risk of errors. For instance, in the case of hard partitions, the clustering results are completely precise but there is much more intendancy to partition an object to an unrelated group. As suggested in [32], a value can be used as a starting default one but it can be modified according to what is expected from the user. The choice \u03b4 is more difficult and is strongly data dependent [32].\nFor determining the number of clusters, the validity index of a credal partition defined by\nMasson and Denoeux [32] could be utilised:\nN\u2217(c) , 1\nn log2(c) \u00d7\nn \u2211\ni=1\n\n\n\u2211\nA\u22082\u2126\\\u2205\nmi(A) log2 |A|+mi(\u2205) log2(c)\n\n , (40)\nwhere 0 \u2264 N\u2217(c) \u2264 1. This index has to be minimized to get the optimal number of clusters. When MECM is applied to community detection, a different index is defined to determine the number of communities. We will describe it in the next section."}, {"heading": "4. Application and evaluation issues", "text": "In this section, we will discuss how to apply MECM to community detection problems in social\nnetworks and how to evaluate credal partitions."}, {"heading": "4.1. Evidential modular function", "text": "Assume the obtained credal partition of the graph is\nM = [m1,m2, \u00b7 \u00b7 \u00b7 ,mn] T ,\nwhere mi = (mi1,mi2, \u00b7 \u00b7 \u00b7 ,mic)T. Similarly to the fuzzy modularity by Havens et al. [21], here we introduce an evidential modularity [50]:\nQe = 1\n\u2016W \u2016\nc \u2211\nk=1\nn \u2211\ni,j=1\n(\nwij \u2212 kikj \u2016W \u2016\n)\nplikpljk, (41)\nwhere pli = (pli1, pli2, \u00b7 \u00b7 \u00b7 , plic) T is the contour function associated with mi, which describes the upper value of our belief to the proposition that the ith node belongs to the kth community.\nLet PL = (plik)n\u00d7c, then Eq. (41) can be rewritten as:\nQe = trace(PLT B PL)\n\u2016W \u2016 . (42)\nQe is a directly extension of the crisp and fuzzy modularity functions in Eq. (24). When the credal partition degrades into the hard and fuzzy ones, Qe is equal to Qh and Qf respectively."}, {"heading": "4.2. The initial prototypes for communities", "text": "Generally speaking, the person who is the center in the community in a social network has the following characteristics: he has relation with all the members of the group and their relationship is stronger than usual; he may directly contact with other persons who also play an important role in their own community. For instance, in Twitter network, all the members in the community of the fans of Real Madrid football Club (RMC) are following the official account of the team, and RMC must be the center of this community. RMC follows the famous football player in the club, who is sure to be the center of the community of his fans. In fact, RMC has 10382777 followers and 30 followings (the data on March 16, 2014). Most of the followings have more than 500000 followers. Therefore, the centers of the community can be set to the ones not only with high degree and strength, but also with neighbours who also have high degree and strength. Thanks to the theory of belief functions, the evidential semi-local centrality ranks the nodes considering all these measures. Therefore the initial c prototypes of each community can be set to the nodes with largest ESC values.\nNote that there is usually more than one center in one community. Take Twitter network for example again, the fans of RMC who follow the club official account may also pay attention to Cristiano Ronaldo, the most popular player in the team, who could be another center of the community of RMC\u2019s fans to a great extent. These two centers (the accounts of the club and Ronaldo) both have large ESC values but they are near to each other. This situation violates the rule which requires the chosen seeds as far away from each other as possible [2, 26].\nThe dissimilarity between the nodes could be utilised to solve this problem. Suppose the ranking order of the nodes with respect to their ESCs is n1 \u2265 n2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 nn. In the beginning n1\nis set to be the first prototype as it has the largest ESC, and then node n2 is considered. If d(n1, n2) (the dissimilarity between node 1 and 2) is larger than a threshold \u00b5, it is chosen to be the second prototype. Otherwise, we abandon n2 and turn to check n3. The process continues until all the c prototypes are found. If there are not enough prototypes after checking all the nodes, we should decrease \u00b5 moderately and restart the search from n1. In this paper we test the approach with the dissimilarity measure proposed in [49]. Based on our experiments, [0.7, 1] is a better experiential range of the threshold \u00b5. This seed choosing strategy is similar to that in [26]."}, {"heading": "4.3. The community detection algorithm based on MECM", "text": "The whole community detection algorithm in social networks based on MECM is summarized\nin Algorithm 2.\nAlgorithm 2 : Community detection algorithm based on MECM\nInput: A, the adjacency matrix; W , the weight matrix (if any); \u00b5, the threshold controlling the dissimilarity between the prototypes; cmin, the minimal number of communities; cmax, the maximal of communities; the required parameters in original MECM algorithm Initialization: Calculate the dissimilarity matrix of the nodes in the graph. repeat (1). Set the cluster number c in MECM be c = cmin. (2). Choose the initial c prototypes using the strategy proposed in section 4.2. (3). Run MECM with the corresponding parameters and the initial prototypes got in (2). (4). Calculate the evidential modularity using Eq. (42). (5). Let c = c+ 1. until c reaches at cmax. Output: Choose the number of communities at around which the modular function peaks, and output the corresponding credal partition of the graph.\nIn the algorithm, cmin and cmax can be determined based on the original graph. Note that cmin \u2265 2. It is an empirical range of the community number of the network. If c is given, we can get a credal partition based on MECM and then the evidential modularity can be derived. As we can see, the modularity is a function of c and it should peak at around the optimal value of c for the given network."}, {"heading": "4.4. Performance evaluation", "text": "The objective of the clustering problem is to partition a similar data pair to the same group. There are two types of correct decisions by the clustering result: a true positive (TP) decision assigns two similar objects to the same cluster, while a true negative (TN) decision assigns two dissimilar objects to different clusters. Correspondingly, there are two types of errors we can commit: a false positive (FP) decision assigns two dissimilar objects to the same cluster, while a false negative (FN) decision assigns two similar objects to different clusters. Let a (respectively, b) be the number of pairs of objects simultaneously assigned to identical classes (respectively, different classes) by the stand reference partition and the obtained one. Actually a (respectively, b) is the number of TP (respectively, TN) decisions. Similarly, let c and d be the numbers of FP and FN decisions respectively. Two popular measures that are typically used to evaluate the performance of hard clusterings are precision and recall. Precision (P) is the fraction of relevant instances (pairs in identical groups in the clustering benchmark) out of those retrieved instances (pairs in identical groups of the discovered clusters), while recall (R) is the fraction of relevant instances that are\nretrieved. Then precision and recall can be calculated by\nP = a\na+ c and R =\na\na+ d (43)\nrespectively. The Rand index (RI) measures the percentage of correct decisions and it can be defined as\nRI = 2(a+ b)\nn(n\u2212 1) , (44)\nwhere n is the number of data objects. In fact, precision measures the rate of the first type of errors (FP), recall (R) measures another type (FN), while RI measures both.\nFor fuzzy and evidential clusterings, objects may be partitioned into multiple clusters with different degrees. In such cases precision would be consequently low [34]. Usually the fuzzy and evidential clusters are made crisp before calculating the measures, using for instance the maximum membership criterion [34] and pignistic probabilities [32]. Thus in the work presented in this paper, we have hardened the fuzzy and credal clusters by maximizing the corresponding membership and pignistic probabilities and calculate precision, recall and RI for each case.\nThe introduced imprecise clusters can avoid the risk to group a data into a specific class without strong belief. In other words, a data pair can be clustered into the same specific group only when we are quite confident and thus the misclassification rate will be reduced. However, partitioning too many data into imprecise clusters may cause that many objects are not identified for their precise groups. In order to show the effectiveness of the proposed method in these aspects, we use the evidential precision (EP) and evidential recall (ER):\nEP = ner Ne , ER = ner Nr . (45)\nIn Eq. (45), the notation Ne denotes the number of pairs partitioned into the same specific group by evidential clusterings, and ner is the number of relevant instance pairs out of these specifically clustered pairs. The value Nr denotes the number of pairs in the same group of the clustering benchmark, and ER is the fraction of specifically retrieved instances (grouped into an identical specific cluster) out of these relevant pairs. When the partition degrades to a crisp one, EP and ER equal to the classical precision and recall measures respectively. EP and ER reflect the accuracy of the credal partition from different points of view, but we could not evaluate the clusterings from one single term. For example, if all the objects are partitioned into imprecise clusters except two relevant data object grouped into a specific class, EP = 1 in this case. But we could not say this is a good partition since it does not provide us with any information of great value. In this case ER \u2248 0. Thus ER could be used to express the efficiency of the method for providing valuable partitions. Certainly we can combine EP and ER like RI to get the evidential rank index (ERI) describing the accuracy:\nERI = 2(a\u2217 + b\u2217)\nn(n\u2212 1) , (46)\nwhere a\u2217 (respectively, b\u2217) is the number of pairs of objects simultaneously clustered to the same specific class (i.e., singleton class, respectively, different classes) by the stand reference partition and the obtained credal one. Note that for evidential clusterings, precision, recall and RI measures are calculated after the corresponding hard partitions are got, while EP, ER and ERI are based on hard credal partitions [32].\nExample 1. In order to show the significance of the above performance measures, an example\ncontaining only ten objects from two groups is presented here. The three partitions are given in Fig. 1-b \u2013 1-d. The values of the six evidential indices (P,R,RI,EP,ER,ERI) are listed in Tab. 1.\nWe can see that if we simply partition the nodes in the overlapped area, the risk of misclassification is high in terms of precision. The introduced imprecise cluster \u03c912 , {\u03c91, \u03c92} could enable us to make soft decisions, as a result the accuracy of the specific partitions is high. However, if too many objects are clustered into imprecise classes, which is the case of partition 3, it is pointless although EP is high. Generally, EP denotes the accuracy of the specific decisions, while ER represents the efficiency of the approach. We remark that the evidential indices degrade to the corresponding classical indices (e.g, evidential precision degrades to precision) when the partition is crisp."}, {"heading": "5. Experiments", "text": "In this section a number of experiments are performed on classical data sets in the distance space and on graph data for which only the dissimilarities between nodes are known. The obtained credal partitions are compared with hard and fuzzy ones using the evaluation indices proposed in Section 4.4 to show the merits of MECM."}, {"heading": "5.1. Overlapped data set", "text": "Clustering approaches to detect overlap objects which leads to recent attentions are still inefficiently processed. Due to the introduction of imprecise classes, MECM has the advantage to detect overlapped clusters. In the first example, we will use overlapped data sets to illustrate the behavior of the proposed algorithm.\nWe start by generating 2 \u00d7 100 points uniformly distributed in two overlapped circles with a same radius R = 30 but with different centers. The coordinates of the first circle\u2019s center are (0, 0) while the coordinates of the other circle\u2019s center are (30, 30). The data set is displayed in Fig. 2-a.\nIn order to show the influence of parameters in MECM and ECM, different values of \u03b3, \u03b1, \u03b7 and \u03b4 have been tested for this data set. The figure Fig. 3-a displays the three evidential indices varying with \u03b3 (\u03b1 is fixed to be 2) by MECM, while Fig. 3-b depicts the results of MECM with different \u03b1 but a fixed \u03b3 = 0.4 (\u03b7 and \u03b4 are set 0.7 and 50, respectively, in the tests). For fixed \u03b1 and \u03b3, the results with different \u03b7 and \u03b4 are shown in Fig. 3-c. The effect of \u03b1 and \u03b4 on the clusterings of ECM is illustrated in Fig. 3-e. As we can see, for both MECM and ECM, if we want to make more imprecise decisions to improve ER, parameter \u03b1 can be decreased. In MECM, we can also reduce the value of parameter \u03b3 to accomplish the same purpose. Although both \u03b1 and \u03b3 have effect on imprecise clusters in MECM, the mechanisms they work are different. Parameter \u03b1 tries to adjust the penalty degree to control the imprecise rates of the results. However, for \u03b3, the same aim could be got by regulating the uncertainty degree of imprecise classes. It can be seen from the figures, the effect of \u03b3 is more conspicuous than \u03b1. Moreover, although \u03b1 may be set too high to obtain good clusterings, \u201cgood\u201d partitions can also be got by adjusting \u03b3 in this case. For both MECM and ECM, the stable limiting values of evidential measures are around 0.7 and 0.8. Such values suggest the equivalence of the two methods to a certain extent. Parameter \u03b7 is used for discounting the distance between uncertain objects and specific clusters. As pointed out in Fig. 3-c, if \u03b3 and \u03b1 are well set, it has little effect on the final clusterings. The same is true in the case of \u03b4 which is applied to detect outliers. The effect of the different values of parameter \u03b2 is illustrated in Fig. 3-d. We can see that it has little influence on the final results as long as it is larger than 1. As in FCM and ECM, for which it is a usual choice, we use \u03b2 = 2 in all the following experiments.\nThe improvement of precision will bring about the decline of recall, as more data could not be clustered into specific classes. What we should do is to set parameters based on our own requirement to make a tradeoff between precision and recall. For instance, if we want to make a cautious decision in which EP is relatively high, we can reduce \u03b3 and \u03b1. Values of these parameters can be also learned from historical data if such data are available.\nFor the objects in the overlapped area, it is difficult to make a hard decision i.e. to decide about their specific groups. Thanks to the imprecise clusters, we can make a soft decision. As analysed before, the soft decision will improve the precision of total results and reduce the risk of misclassifications caused by simply partitioning the overlapped objects into specific class. However, too many imprecise decisions will decrease the recall value. Therefore, the ideal partition should make a compromise between the two measures. Set \u03b1 = 1.8, \u03b3 = 0.2, \u03b7 = 0.7 and \u03b4 = 50, the \u201cbest\u201d (with relatively high values on both precision and recall) clustering result by MECM is shown in Fig. 2-b. As we can see, most of the data in the overlapped area are partitioned into imprecise cluster \u03c912 , {\u03c91, \u03c92} by the application of MECM. We adjust the coordinates of the center of the second circle to get overlapped data with different proportions (overlap rates), and the validity indices of the clustering results by different methods are illustrated in Fig. 4. For the application of MECM, MCM and MFCM, each algorithm is evoked 20 times with randomly selected initial prototypes for the same data set and the mean values of the evaluating indices are reported. The figure Fig. 4-d shows the average values of the indices by MECM (plus and minus one standard deviation) for 20 repeated experiments as a function of the overlap rates. As we can see the initial prototypes indeed have effects on the final results, especially when the overlap rates are high. Certainly, we can avoid the influence by repeating the algorithm many times. But this is too expensive for MECM. Therefore, we suggest to use the prototypes obtained in MFCM or MCM as the initial. In the following experiments, we will set the initial prototypes to be the ones got by MFCM.\nAs it can be seen, for different overlap rates, the classical measures such as precision, recall, and RI are almost the same for all the methods. This reflects that pignistic probabilities play a similar role as fuzzy membership. But we can see that for MECM, EP is significantly high, and the increasing of overlap rates has least effects on it compared with the other methods. Such effect can be attributed to the introduced imprecise clusters which enable us to make a compromise decision between hard ones. But as many points are clustered into imprecise classes, the evidential recall value is low.\nOverall, this example reflects one of the superiority of MECM that it can detect overlapped clusters. The objects in the overlapped area could be clustered into imprecise classes by this approach. Other possible available information or special techniques could be utilised for these imprecise data when we have to make hard decisions. Moreover, partitions with different degree of imprecision can be got by adjusting the parameters of the algorithm based on our own requirement."}, {"heading": "5.2. Classical data sets from Gaussian mixture model", "text": "In the second experiment, we test on a data set consisting of 3\u00d7 50 + 2\u00d7 5 points generated from different Gaussian distributions. The first 3 \u00d7 50 points are from Gaussian distributions G(\u00b5k,\u03a3k)(k = 1, 2, 3) with\n\u00b51 =(0, 0) T,\u00b52 = (40, 40) T,\u00b53 = (80, 80) T (47)\n\u03a31 = \u03a32 = \u03a33 =\n(\n120 0\n0 120\n)\n,\nand the last 2\u00d7 10 data are noisy points follow G(\u00b5k,\u03a3k)(k = 4, 5) with\n\u00b54 =(\u221250, 90) T,\u00b55 = (\u221210, 130) T (48)\n\u03a34 = \u03a35 =\n(\n80 0\n0 80\n)\n.\nMECM is applied with the following settings: \u03b1 = 1, \u03b4 = 100, \u03b7 = 0.7, while ECM has been tested using \u03b1 = 1.7, \u03b4 = 100 (The appropriate parameters can be determined similarly as in the first example). One can see from Figs. 5-b and 5-c , MCM and MFCM can partition most of the regular data in \u03c91, \u03c92 and \u03c93 into their correct clusters, but they could not detect the noisy points correctly. These noisy data are simply grouped into a specific cluster by both approaches. As can be seen from Fig. 5-d, for the points located in the middle part of \u03c92, ECM could not find their exact group and misclassify them into imprecise cluster \u03c913 . In the figures \u03c9ij , {\u03c9i, \u03c9j} denotes imprecise clusters.\nAs mentioned before, imprecise classes in MECM can measure ignorance and uncertainty at the same time, and the degree of ignorance in meta clusters can be adjusted by \u03b3. We can see that MECM does not detect many points in the overlapped area between two groups if \u03b3 is set to 0.6. In such a case the test objects are partitioned into imprecise clusters mainly because of our ignorance about their specific classes. These objects attributed to meta classes mainly belong to noisy data in \u03c94 and \u03c95. The distance of these points to the prototypes of specific clusters is large (but not too large or they could be regarded to be in the emptyset, see Fig. 5-e). Thus the distance between the prototype vectors is relatively small so that these specific clusters are indistinguishable. Decreasing \u03b3 to be 0.2 would make imprecise class denoting more uncertainty, as it can be seen from Fig. 5-f, where many points located in the margin of each group are clustered into imprecise classes. In such a case, meta classes rather reflect our uncertainty on the data objects\u2019 specific cluster.\nThe table Tab. 2 lists the indices for evaluating the different methods. Bold entries in each column of this table (and also other tables in the following) indicate that the results are significant as the top performing algorithm(s) in terms of the corresponding evaluation index. We can see that the precision, recall and RI values for all approaches are similar except from those obtained for ECM which are significantly lower. As these classical measures are based on the associated pignistic probabilities for evidential clusterings, it seems that credal partitions can provide the same information as crisp and fuzzy ones. But from the same table, we can also see that the evidential measures EP and ERI obtained for MECM are higher (for hard partitions, the values of evidential measures equal to the corresponding classical ones) than the ones obtained for other methods. This fact confirms the accuracy of the specific decisions i.e. decisions clustering the objects into specific classes. The advantage can be attributed to the introduction of imprecise clusters, with which we\ndo not have to partition the uncertain or unknown objects into a specific cluster. Consequently, it could reduce the risk of misclassification. However, although ECM also deals with imprecise clusters, the accuracy is not improved as much as in the case of applying MECM. As illustrated before in the case of ECM application, many objects of a specific cluster are partitioned into an irrelevant imprecise class and, as a result, the evidential precision value and ERI decrease as well.\nWe also test on \u201cIris flower\u201d, \u201ccat cortex\u201d and \u201cprotein\u201d data sets [12, 17, 22]. The first is object data while the other two are relational data sets. Thus we compare our method with FCM and ECM for the Iris data set, and with RECM and NRFCM (Non-Euclidean Relational Fuzzy Clustering Method [19]) for the last two data sets. The results are displayed in Fig. 6.\nPresented results allow us to sum up the characteristics of MECM. Firstly, one can see that the behavior of MECM is similar to ECM for traditional data. Besides, credal partitions provided by MECM allow to recover the information of crisp and fuzzy partitions. Moreover, we are able to balance influence of our uncertainty and ignorance according to the actual needs. The examples utilised before deal with classical data sets. But the superiority of MECM makes it applicable in the case of data sets for which only dissimilarity measures are known e.g. social networks. Thus in the following experiments, we will use some graph data to illustrate the behaviour of the proposed method on the community detection problem in social networks. The dissimilarity index used here is the one brought forward by Zhou [49]. To have a fair comparison, in the following experiments, we also compare with three classical algorithms for community detection i.e. BGLL [4], LPA [36] and ZFCM (a fuzzy c-means based approach proposed by Zhang et al. [46]). The obtained community structures are compared with known performance measures, i.e., NMI (Normalized Mutual Information), VI (Variation of Information) and Modularity."}, {"heading": "5.3. Artificial graphs and generated benchmarks", "text": "To show the performance of the algorithm in detecting communities in networks, we first apply the method to a sample network generated from Gaussian mixture model. This model has been used for testing community detection approaches by Liu and Liu [29].\nThe artificial graph is composed of 3 \u00d7 50 nodes, n1, n2, \u00b7 \u00b7 \u00b7 , n150, which are represented by 150 sample points, x1,x2, \u00b7 \u00b7 \u00b7 ,x150, in two-dimensional Euclidean space. There are 3\u00d7 50 points generated from Gaussian distributions G(\u00b5k,\u03a3k)(k = 1, 2, 3) with\n\u00b51 = (1.0,4.0) T,\u00b52 = (2.5, 5.5) T,\u00b53 = (0.5, 6.0) T (49)\n\u03a31 = \u03a32 = \u03a33 =\n(\n0.25 0\n0 0.25\n)\n.\nThen, the edges of the graph are generated by the following thresholding strategy: if |xi\u2212xj | \u2264 dist, we set an edge between node i and node j; Otherwise the two nodes are not directly connected. The graph is shown in Fig. 7-a (with dist = 0.8) and the dissimilarity matrix of the nodes is displayed in Fig. 7-b. From the figures we can see that there are three significant communities in the graph, and some nodes in the bordering of their groups seem to be in overlapped classes as they contact with members in different communities simultaneously.\nThe table Tab. 3 lists the indices for evaluating the results. It shows that MECM performs well as the evidential precision resulting from its application is high. MECM utilization also results in decreasing the probabilities of clustering failure thanks to the introduction of imprecise clusters. This makes the decision-making process more cautious and reasonable.\na. Gaussian network b. Dissimilarity matrix\nFigure 7: Artificial network from Gaussian mixture model\nThe algorithms are also compared by means of Lancichinetti et al. [28] benchmark (LFR) networks. The results of different methods in two kinds of LFR networks with 500 and 1000 nodes are displayed in Figs. 8\u20139 respectively. The parameter \u00b5 showed in the x-axis in the figures identifies whether the network has clear communities. When \u00b5 is small, the graph has well community structure. In such a case, almost all the methods perform well. But we can see that when \u00b5 is large, the results by MECM have the largest values of precision. It means that the decisions which partition the nodes into a specific cluster are of great confidence. In terms of NMI, the results are similar to those by BGLL and LPA, but better than those of MCM and MFCM. This fact well explains that the hard or fuzzy partitions could be recovered when necessary.\na. Precision b. NMI\nFigure 8: Comparison of MECM and other algorithms in LFR networks. The number of nodes is N = 500. The average degree is |k| = 15, and the pair for the exponents is (\u03b3, \u03b2) = (2, 1).\na. Precision b. NMI\nFigure 9: Comparison of MECM and other algorithms in LFR networks. The number of nodes is N = 1000. The average degree is |k| = 20, and the pair for the exponents is (\u03b3, \u03b2) = (2, 1)."}, {"heading": "5.4. Some real-world networks", "text": "A. Zachary\u2019s karate club. The Zachary\u2019s Karate Club data [45] is an undirected graph which consists of 34 vertices and 78 edges. The original graph and the dissimilarity of the nodes are shown in Fig. 10-a and 10-b respectively.\nLet the parameters of MECM be \u03b1 = 1.5, \u03b4 = 100, \u03b7 = 0.9, \u03b3 = 0.6. The modularity functions by MECM, MCM, MFCM and ZFCM (Fig. 12-a) peak around c = 2 and c = 3. Let c = 2, all the methods can detect the two communities exactly. If we set c = 3, a small community, which can also be found in the dissimilarity matrix (Fig. 10-b), is separated from \u03c91 by all the approaches (see Fig. 11). But ZFCM assigns the maximum membership to \u03c91 for node 9, which is actually in \u03c92. It seems that the loss of accuracy in the mapping process may cause such results.\nMECM does not find imprecise groups when \u03b3 = 0.6 as the network has apparent community structure, and this reflects the fact that the communities are distinguishable for all the nodes. But there may be some overlap between two communities. The nodes in the overlapped cluster can be detected by decreasing \u03b3 (increasing the uncertainty for imprecise communities). As is displayed in Fig. 11-c and d, by declining \u03b3 to 0.1 and 0.05 respectively (the other parameters remain unchanged), nodes 3 and 9 are clustered into both \u03c91 and \u03c92 (\u03c912) one after another.\nFrom the results we can see that MECM takes both the ignorance and the uncertainty into consideration while introducing imprecise communities. The degree of ignorance and uncertainty could be balanced through adjusting \u03b3. The analysis shows that there appears only uncertainty without ignorance in the original club network. In order to show the performance of MECM when there are noisy conditions such that some communities are indistinguishable, two noisy nodes are added to the original graph in the next experiment.\na. Original karate club network b. Dissimilarity matrix\nFigure 10: Original karate club network\na. Clustering result of ZFCM b. Clustering result of MCM, MFCM, and\nMECM (\u03b3 = 0.1) c. Clustering result of MECM (\u03b3 = 0.1) d. Clustering result of MECM (\u03b3 = 0.05)\nFigure 11: Detected communities of karate club network by different methods\na. Original karate club b. Karate club with noisy nodes\nFigure 12: Modularity functions of karate club network by different methods\nB. Karate club network with some added noisy nodes. In this test, two noisy nodes are added to the original karate club network (see Fig.13-a). The first one is node 35, which is directly connected with nodes 18 and 27. The other one is 36, which is connected to nodes 1 and 33. It can be seen from the dissimilarity matrix that node 36 has stronger relationships with both communities than node 35. This is due to the fact that the nodes connected to node 36 play leader roles in their own group, but node 35 contacts with two marginal nodes with\u201csmall\u201dor insignificant roles in their own group only.\nThe results obtained by the application of different methods are shown in Fig. 14. The MECM parameters are set as follows: \u03b1 = 1.5, \u03b4 = 100, \u03b7 = 0.9 and \u03b3 is tuned according to the the extent that the imprecise communities reflect our ignorance. As we can see, MCM, MFCM and ZFCM simply group the two noisy nodes into \u03c91. With \u03b3 = 0.4 MECM regards node 36 as a member of \u03c91 while node 35 is grouped into imprecise community \u03c912. And \u03c912 mainly reflects our ignorance rather than uncertainty on the actual community of node 36. This is why node 36 is not clustered into \u03c912 since \u03c91 and \u03c92 are distinguishable for him but we are just not sure for the final decision. The increase in the extent of uncertainty in imprecise communities results from the decrease of \u03b3 value. We can see that more nodes (including nodes 36,9,1,12,27, see Figs. 14-e and f) are clustered into \u03c912 or \u03c913 due to uncertainty. The imprecise communities consider both ignorance (node 35) and uncertainty (other nodes).\nThese results reflect the difference between ignorance and uncertainty. As node 35 is only related to one outward node of each community, thus we are ignorant about which community it really belongs to. On the contrary, node 36 connects with the key members (playing an important role in the community), and in this case the dissimilarity between the prototypes of \u03c91 and \u03c92 is relatively large so they are distinguishable. Thus there is uncertainty rather than ignorance about which community node 36 is in. In this network, node 36 is a\u201cgood\u201dmember for both communities, whereas node 35 is a \u201cpoor\u201d member. It can be seen from Fig. 15-a that the fuzzy partition by MFCM also gives large similar membership values to \u03c91 and \u03c92 for node 35, just like in the case of such good members as node 36 and 9. The obtained results show the problem of distinguishing between ignorance and the \u201cequal evidence\u201d (uncertainty) for fuzzy partitions. But Fig. 15-b shows that the credal partition by MECM assigns small mass belief to \u03c91 and \u03c92 for node 35, indicating our ignorance on its situation.\na. Karate club network with added nodes b. Dissimilarity matrix\nFigure 13: Karate club network with two noisy nodes\na. MCM b. ZFCM\nc. MFCM d. MECM (\u03b3 = 0.4)\ne. MECM (\u03b3 = 0.1) f. MECM (\u03b3 = 0.02)\nFigure 14: Detected communities in Karate club network with noisy nodes\na. Fuzzy membership by MFCM b. Mass belief by MECM\nFigure 15: Fuzzy membership and mass belief of the nodes in karate club network with noisy nodes\nWe also test our method on four other real-world graphs: American football network, Dolphins network, Lesmis network and Political books network. The measures applied to evaluate the performance of different methods are listed in Tab. 4\u20137. It can been seen from the tables, for all the graphs MECM application results in a community structure with high evidential precision level. The precision results from a cautious decision making process which clusters the noisy nodes into imprecise communities. In terms of classical performance measures like NMI, VI and modularity, MECM slightly outperforms the other algorithms. Note that these classical measures for hard partitions are calculated by the pignistic probabilities associated with the credal partitions provided by MECM. Therefore, we can also see the possibility to recover the hard decisions here when using the proposed evidential detection approach."}, {"heading": "5.5. Discussion", "text": "We will discuss for which application MECM is designed here. As analysed before, for MECM only dissimilarities between objects are required and only the intuitive assumptions need to be satisfied for the dissimilarity measure. Therefore, the algorithm could be appropriate for many clustering tasks for non-metric data objects. This type of data is very common in social sciences, psychology, etc, where any metric assumptions about the similarities/dissimilarities could not be assured. The freedom for the data set leads to the restriction that the prototypes should be the objects themselves. Nevertheless, this constraint seems reasonable for social networks as the center of a community is usually the person (node) frequently contacting with others. Thus the approach\nThese data sets can be found in http://networkdata.ics.uci.edu/index.php\ncan be applied to community detection problems. Thanks to the introduction of imprecise classes, it could reduce the risk of partitioning the objects which we are uncertain or ignorant into an incorrect cluster. For this reason the algorithm can help us make soft decisions when clustering the data set without distinct cluster/community structures or with overlap.\nDue to the computational complexity, the proposed algorithm is not well directly adapted to handle very large data sets. However, here we discuss the possibility to apply the evidential community detection approach to large-scale networks. Firstly, the number of parameters to be optimized is exponential and depends on the number of clusters [32]. For the number of classes larger than 10, calculations are not tractable. But we can consider only a subclass with a limited number of focal sets [32]. For instance, we could constrain the focal sets to be composed of at most two classes (except \u2126). Secondly, for the network with millions of nodes, MCM or MFCM could be evoked as a first step to merge some nodes into small clusters. After that we can apply MECM to the \u201ccoarsened\u201d network. But how to define the edges or connections of the new graph should be studied. Lastly we emphasize that the evidential community detection algorithm could be utilised for gaining a better insight into the network structure and detecting the imprecise classes. For the large-scale network, it is difficult to make specific decisions for all of nodes due to the limitation of time, money or techniques. In this case we can use the proposed approach to make some \u201csoft\u201d decisions first and then use some techniques special for the imprecise parts of the graph."}, {"heading": "6. Conclusion", "text": "We introduced a Median variant of Evidential C-means (MECM) as a new prototype-based clustering algorithm in the present contribution. The proposed approach is an extension of median c-means and median fuzzy c-means. It is based on the framework of belief function theory. The applied median-based clustering requires the definition of the dissimilarity between the objects only. Therefore, it is not restricted to a metric space application. The prototypes of the clusters are constrained to the data objects themselves. MECM provides us with not only credal partitions but also hard and fuzzy partitions as by-products through computing pignistic probabilities. Moreover, it could distinguish ignorance from uncertainty while the fuzzy or crisp partitions could not. By the introduced imprecise clusters, we could find some overlapped and indistinguishable clusters for related nodes. Thanks to the advantages of belief function theory and median clustering, MECM could be applied to community detection problems in social networks. As other median clustering approaches, MECM tends to get stuck in local minima such that several runs have to be performed to obtain good performance. However, we propose an initial prototype-selection scheme using the evidential semi-centrality for the application of MECM in community detection to solve the problems brought by the initial prototypes. Results of presented experiments on artificial and realworld networks show that the credal partitions on graphs provided by MECM application are more refined than crisp and fuzzy ones. Therefore, they could enable us to gain a better understanding\nof analysed community structure. Some examples on the classical metric space are also given to illustrate the interest of MECM and to show its difference with respect to the existing methods.\nAs mentioned in this paper, there may be more than one center in each community network. Nevertheless, we ignore \u201cmulti-center\u201d to avoid the troubles brought by the need for an initial seed using ESC and the definition of a threshold to control the distance between prototypes. We are aware that this is a drawback of the presented approach as not all the centers in each community are taken into consideration. Therefore, we intend to include the feature of multiple centers in our future research work."}, {"heading": "Acknowledgements", "text": "The authors are grateful to the anonymous reviewers for all their remarks which helped us to clarify and improve the quality of this paper. This work was supported by the National Natural Science Foundation of China (Nos.61135001, 61403310)."}], "references": [{"title": "Community detection in complex networks: Multi\u2013objective enhanced firefly algorithm", "author": ["B. Amiri", "L. Hossain", "J.W. Crawford", "R.T. Wigand"], "venue": "Knowledge-Based Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "k-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial and Applied Mathematics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Pattern recognition with fuzzy objective function algorithms", "author": ["J.C. Bezdek"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1981}, {"title": "Fast unfolding of communities in large networks", "author": ["V.D. Blondel", "Guillaume", "J.-L", "R. Lambiotte", "E. Lefebvre"], "venue": "Journal of Statistical Mechanics: Theory and Experiment", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "A quality driven hierarchical data divisive soft clustering for information retrieval. Knowledge-based systems", "author": ["G. Bordogna", "G. Pasi"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Prototype-based classification and clustering. Ph.D. thesis, Otto-von- Guericke-Universit\u00e4t Magdeburg, Universit\u00e4tsbibliothek", "author": ["C. Borgelt"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Finding community structure in very large networks", "author": ["A. Clauset", "M.E. Newman", "C. Moore"], "venue": "Physical review E", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Batch and median neural gas", "author": ["M. Cottrell", "B. Hammer", "A. Hasenfu\u00df", "T. Villmann"], "venue": "Neural Networks", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "EVCLUS: evidential clustering of proximity data. Systems, Man, and Cybernetics, Part B: Cybernetics", "author": ["T. Den\u0153ux", "Masson", "M.-H"], "venue": "IEEE Transactions on", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Community detection in complex networks using extremal optimization", "author": ["J. Duch", "A. Arenas"], "venue": "Physical review E", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "A fuzzy relative of the isodata process and its use in detecting compact well-separated clusters", "author": ["J.C. Dunn"], "venue": "Journal of Cybernetics", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1973}, {"title": "The use of multiple measurements in taxonomic problems", "author": ["R.A. Fisher"], "venue": "Annals of eugenics", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1936}, {"title": "Community detection in graphs", "author": ["S. Fortunato"], "venue": "Physics Reports", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Resolution limit in community detection", "author": ["S. Fortunato", "M. Barthelemy"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "A modified evidential methodology of identifying influential nodes in weighted networks", "author": ["C. Gao", "D. Wei", "Y. Hu", "S. Mahadevan", "Y. Deng"], "venue": "Physica A: Statistical Mechanics and its Applications", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Median fuzzy c-means for clustering dissimilarity data", "author": ["T. Geweniger", "D. Z\u00fclke", "B. Hammer", "T. Villmann"], "venue": "Neurocomputing", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Classification on pairwise proximity data", "author": ["T. Graepel", "R. Herbrich", "P. Bollmann-Sdorra", "K. Obermayer"], "venue": "Advances in neural information processing systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Relational neural gas. In: KI 2007: Advances in Artificial Intelligence", "author": ["B. Hammer", "A. Hasenfuss"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Nerf c-means: Non-euclidean relational fuzzy clustering", "author": ["R.J. Hathaway", "J.C. Bezdek"], "venue": "Pattern recognition", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1994}, {"title": "Relational duals of the c-means clustering algorithms", "author": ["R.J. Hathaway", "J.W. Davenport", "J.C. Bezdek"], "venue": "Pattern recognition", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1989}, {"title": "A soft modularity function for detecting fuzzy communities in social networks", "author": ["T. Havens", "J. Bezdek", "C. Leckie", "K. Ramamohanarao", "M. Palaniswami"], "venue": "Fuzzy Systems, IEEE Transactions on", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Pairwise data clustering by deterministic annealing", "author": ["T. Hofmann", "J.M. Buhmann"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "Community detection by signaling on complex networks", "author": ["Y. Hu", "M. Li", "P. Zhang", "Y. Fan", "Z. Di"], "venue": "Physical Review E", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Privacy preserving data mining: a noise addition framework using a novel clustering technique", "author": ["M.Z. Islam", "L. Brankovic"], "venue": "Knowledge-Based Systems", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Data clustering: 50 years beyond k-means", "author": ["A.K. Jain"], "venue": "Pattern Recognition Letters", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "An efficient community detection method based on rank centrality", "author": ["Y. Jiang", "C. Jia", "J. Yu"], "venue": "Physica A: Statistical Mechanics and its Applications", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Detecting the overlapping and hierarchical community structure in complex networks", "author": ["A. Lancichinetti", "S. Fortunato", "J. Kert\u00e9sz"], "venue": "New Journal of Physics", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Benchmark graphs for testing community detection algorithms", "author": ["A. Lancichinetti", "S. Fortunato", "F. Radicchi"], "venue": "Physical Review E", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "Detecting community structure in complex networks using simulated annealing with k-means algorithms", "author": ["J. Liu", "T. Liu"], "venue": "Physica A: Statistical Mechanics and its Applications", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Evidential classifier for imprecise data based on belief functions", "author": ["Liu", "Z.-g", "Q. Pan", "J. Dezert"], "venue": "Knowledge-Based Systems", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Decision support with belief functions theory for seabed characterization", "author": ["A. Martin", "I. Quidu"], "venue": "Information Fusion,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "ECM: An evidential version of the fuzzy c-means algorithm", "author": ["Masson", "M.-H", "T. Denoeux"], "venue": "Pattern Recognition", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "RECM: Relational evidential c-means algorithm", "author": ["Masson", "M.-H", "T. Den\u0153ux"], "venue": "Pattern Recognition Letters", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "Evaluating fuzzy clustering for relevance-based information access", "author": ["M. Mendes", "L. Sacks"], "venue": "Fuzzy Systems,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2003}, {"title": "Finding and evaluating community structure in networks", "author": ["M.E. Newman", "M. Girvan"], "venue": "Physical review E", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2004}, {"title": "Near linear time algorithm to detect community structures in large-scale networks", "author": ["U.N. Raghavan", "R. Albert", "S. Kumara"], "venue": "Physical Review E", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}, {"title": "Extracting the hierarchical organization of complex systems", "author": ["M. Sales-Pardo", "R. Guimera", "A.A. Moreira", "L.A.N. Amaral"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}, {"title": "Clustering belief functions based on attracting and conflicting metalevel evidence using potts spin mean field theory", "author": ["J. Schubert"], "venue": "Information Fusion", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2004}, {"title": "Decision making in the TBM: the necessity of the pignistic transformation", "author": ["P. Smets"], "venue": "International Journal of Approximate Reasoning", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "A spectral clustering approach to finding communities in graphs", "author": ["S. Smyth", "S. White"], "venue": "Proceedings of the 5th SIAM International Conference on Data Mining", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2005}, {"title": "Combining complementary information sources in the Dempster\u2013Shafer framework for solving classification problems with imperfect labels", "author": ["M. Tabassian", "R. Ghaderi", "R. Ebrahimpour"], "venue": "Knowledge-Based Systems", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2012}, {"title": "Community detection in complex networks using genetic algorithms. arXiv preprint arXiv:0711.0491", "author": ["M. Tasgin", "A. Herdagdelen", "H. Bingol"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "Hierarchical community detection with applications to real-world network analysis", "author": ["B. Yang", "J. Di", "J. Liu", "D. Liu"], "venue": "Data & Knowledge Engineering", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "A kernel fuzzy c-means clustering-based fuzzy support vector machine algorithm for classification problems with outliers or noises", "author": ["X. Yang", "G. Zhang", "J. Lu", "J. Ma"], "venue": "Fuzzy Systems, IEEE Transactions on", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2011}, {"title": "An information flow model for conflict and fission in small groups", "author": ["W.W. Zachary"], "venue": "Journal of anthropological research,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1977}, {"title": "Identification of overlapping community structure in complex networks using fuzzy c-means clustering", "author": ["S. Zhang", "Wang", "R.-S", "Zhang", "X.-S"], "venue": "Physica A: Statistical Mechanics and its Applications", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2007}, {"title": "Text clustering using frequent itemsets", "author": ["W. Zhang", "T. Yoshida", "X. Tang", "Q. Wang"], "venue": "Knowledge-Based Systems", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2010}, {"title": "Identifying influential nodes in complex networks with community structure", "author": ["X. Zhang", "J. Zhu", "Q. Wang", "H. Zhao"], "venue": "Knowledge-Based Systems", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2013}, {"title": "Distance, dissimilarity index, and network community structure", "author": ["H. Zhou"], "venue": "Physical review E", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2003}, {"title": "Evidential communities for complex networks. In: Information Processing and Management of Uncertainty in Knowledge-Based Systems", "author": ["K. Zhou", "A. Martin", "Q. Pan"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}], "referenceMentions": [{"referenceID": 23, "context": "The clustering can be used in many fields such as privacy preserving [24] , information retrieval [5], text analysis [47], etc.", "startOffset": 69, "endOffset": 73}, {"referenceID": 4, "context": "The clustering can be used in many fields such as privacy preserving [24] , information retrieval [5], text analysis [47], etc.", "startOffset": 98, "endOffset": 101}, {"referenceID": 46, "context": "The clustering can be used in many fields such as privacy preserving [24] , information retrieval [5], text analysis [47], etc.", "startOffset": 117, "endOffset": 121}, {"referenceID": 43, "context": "It can also be used as the first step of classification problems to identify the distribution of the training set [44].", "startOffset": 114, "endOffset": 118}, {"referenceID": 5, "context": "They capture the characteristics of the data distribution (like location, size, and shape), and classify the data set based on the similarities (or dissimilarities) of the objects to their prototypes [6].", "startOffset": 200, "endOffset": 203}, {"referenceID": 10, "context": "Fuzzy C-Means (FCM), proposed by Dunn [11] and later improved by Bezdek [3], is an extension of c-means where each data point can be a member of multiple clusters with membership values (fuzzy partitions) [25].", "startOffset": 38, "endOffset": 42}, {"referenceID": 2, "context": "Fuzzy C-Means (FCM), proposed by Dunn [11] and later improved by Bezdek [3], is an extension of c-means where each data point can be a member of multiple clusters with membership values (fuzzy partitions) [25].", "startOffset": 72, "endOffset": 75}, {"referenceID": 24, "context": "Fuzzy C-Means (FCM), proposed by Dunn [11] and later improved by Bezdek [3], is an extension of c-means where each data point can be a member of multiple clusters with membership values (fuzzy partitions) [25].", "startOffset": 205, "endOffset": 209}, {"referenceID": 31, "context": "Belief functions have already been used to express partial information about data both in supervised and unsupervised learning [32, 41].", "startOffset": 127, "endOffset": 135}, {"referenceID": 40, "context": "Belief functions have already been used to express partial information about data both in supervised and unsupervised learning [32, 41].", "startOffset": 127, "endOffset": 135}, {"referenceID": 31, "context": "Recently, Masson and Denoeux [32] proposed the application of evidential c-means (ECM) to get credal partitions [9] for object data.", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "Recently, Masson and Denoeux [32] proposed the application of evidential c-means (ECM) to get credal partitions [9] for object data.", "startOffset": 112, "endOffset": 115}, {"referenceID": 31, "context": "The additional flexibility brought by the power set provides more refined partitioning results than those by the other techniques allowing us to gain a deeper insight into the data [32].", "startOffset": 181, "endOffset": 185}, {"referenceID": 15, "context": "Actually this is the basic principle of median clustering methods [16].", "startOffset": 66, "endOffset": 70}, {"referenceID": 15, "context": "These restrictions on prototypes can relax the assumption of a metric space embedding for the objects to be clustered [16, 18], and only similarity or dissimilarity between data objects is required.", "startOffset": 118, "endOffset": 126}, {"referenceID": 17, "context": "These restrictions on prototypes can relax the assumption of a metric space embedding for the objects to be clustered [16, 18], and only similarity or dissimilarity between data objects is required.", "startOffset": 118, "endOffset": 126}, {"referenceID": 19, "context": "There are some clustering methods for relational data, such as Relational FCM (RFCM) [20] and Relational ECM (RECM) [33], but an underlying metric is assumed for the given dissimilarities between objects.", "startOffset": 85, "endOffset": 89}, {"referenceID": 32, "context": "There are some clustering methods for relational data, such as Relational FCM (RFCM) [20] and Relational ECM (RECM) [33], but an underlying metric is assumed for the given dissimilarities between objects.", "startOffset": 116, "endOffset": 120}, {"referenceID": 15, "context": "However, in median clustering this restriction is dropped [16].", "startOffset": 58, "endOffset": 62}, {"referenceID": 7, "context": "[8] proposed Median C-Means clustering method (MCM) which is a variant of the classic c-means and proved the convergence of the algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16] combined MCM with the fuzzy c-means approach and investigated the behavior of the resulted Median Fuzzy C-means (MFCM) algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "Newman and Girvan [35] proposed a network modularity measure (usually denoted by Q) and several algorithms that try to maximize Q have been designed [4, 7, 10, 42].", "startOffset": 18, "endOffset": 22}, {"referenceID": 3, "context": "Newman and Girvan [35] proposed a network modularity measure (usually denoted by Q) and several algorithms that try to maximize Q have been designed [4, 7, 10, 42].", "startOffset": 149, "endOffset": 163}, {"referenceID": 6, "context": "Newman and Girvan [35] proposed a network modularity measure (usually denoted by Q) and several algorithms that try to maximize Q have been designed [4, 7, 10, 42].", "startOffset": 149, "endOffset": 163}, {"referenceID": 9, "context": "Newman and Girvan [35] proposed a network modularity measure (usually denoted by Q) and several algorithms that try to maximize Q have been designed [4, 7, 10, 42].", "startOffset": 149, "endOffset": 163}, {"referenceID": 41, "context": "Newman and Girvan [35] proposed a network modularity measure (usually denoted by Q) and several algorithms that try to maximize Q have been designed [4, 7, 10, 42].", "startOffset": 149, "endOffset": 163}, {"referenceID": 13, "context": "This problem is famously known as the resolution limit [14].", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "[1] suggested a new community detection process as a multi-objective optimization problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "It merges or splits clusters according to a topological measure of similarity between the nodes and tries to build a hierarchical tree of partitions [27, 37, 43].", "startOffset": 149, "endOffset": 161}, {"referenceID": 36, "context": "It merges or splits clusters according to a topological measure of similarity between the nodes and tries to build a hierarchical tree of partitions [27, 37, 43].", "startOffset": 149, "endOffset": 161}, {"referenceID": 42, "context": "It merges or splits clusters according to a topological measure of similarity between the nodes and tries to build a hierarchical tree of partitions [27, 37, 43].", "startOffset": 149, "endOffset": 161}, {"referenceID": 39, "context": "Also there are some ways, such as spectral methods [40] and signal process method [23, 26], to map topological relationship of nodes on networks into geometrical structures", "startOffset": 51, "endOffset": 55}, {"referenceID": 22, "context": "Also there are some ways, such as spectral methods [40] and signal process method [23, 26], to map topological relationship of nodes on networks into geometrical structures", "startOffset": 82, "endOffset": 90}, {"referenceID": 25, "context": "Also there are some ways, such as spectral methods [40] and signal process method [23, 26], to map topological relationship of nodes on networks into geometrical structures", "startOffset": 82, "endOffset": 90}, {"referenceID": 0, "context": "The function m : 2 \u2192 [0, 1] is said to be the Basic Belief Assignment (bba) on 2, if it satisfies:", "startOffset": 21, "endOffset": 27}, {"referenceID": 0, "context": "The function pl : \u03a9 \u2192 [0, 1] such that pl(\u03c9i) = Pl({\u03c9i}) (\u03c9i \u2208 \u03a9) is called the contour function associated with m.", "startOffset": 22, "endOffset": 28}, {"referenceID": 38, "context": "In this algorithm, each mass of belief m(A) is equally distributed among the elements of A [39].", "startOffset": 91, "endOffset": 95}, {"referenceID": 30, "context": "For instance, a pessimistic decision can be made by maximizing the credibility function, while maximizing the plausibility function could provide an optimistic one [31].", "startOffset": 164, "endOffset": 168}, {"referenceID": 30, "context": "Another criterion [31] considers the plausibility functions and consists in attributing the class Aj for object i if Aj = argmax X\u2286\u03a9 {mi(X)Pli(X)}, (5) where mi(X) = K b i\u03bbX (", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "(5) mi(X) is a weight on Pli(X), and r is a parameter in [0, 1] allowing a decision from a simple class (r = 1) until the total ignorance \u03a9 (r = 0).", "startOffset": 57, "endOffset": 63}, {"referenceID": 7, "context": "Median c-means is a variant of the traditional c-means method [8, 16].", "startOffset": 62, "endOffset": 69}, {"referenceID": 15, "context": "Median c-means is a variant of the traditional c-means method [8, 16].", "startOffset": 62, "endOffset": 69}, {"referenceID": 15, "context": "Due to these weak assumptions, data object xi itself may be a general choice and it does not have to live in a metric space [16].", "startOffset": 124, "endOffset": 128}, {"referenceID": 15, "context": "As in MCM, it requires the knowledge of the dissimilarity between data objects, and the prototypes are restricted to the objects themselves [16].", "startOffset": 140, "endOffset": 144}, {"referenceID": 0, "context": "where each number uik \u2208 [0, 1] is interpreted as a degree of membership of object i to cluster k, and \u03b2 > 1 is a weighting exponent that controls the fuzziness of the partition.", "startOffset": 24, "endOffset": 30}, {"referenceID": 31, "context": "Evidential c-means Evidential c-means [32] is a direct generalization of FCM in the framework of belief functions, and it is based on the credal partition first proposed by Den\u0153ux and Masson [9].", "startOffset": 38, "endOffset": 42}, {"referenceID": 8, "context": "Evidential c-means Evidential c-means [32] is a direct generalization of FCM in the framework of belief functions, and it is based on the credal partition first proposed by Den\u0153ux and Masson [9].", "startOffset": 191, "endOffset": 194}, {"referenceID": 29, "context": "The credal partition takes advantage of imprecise (meta) classes [30] to express partial knowledge of class memberships.", "startOffset": 65, "endOffset": 69}, {"referenceID": 37, "context": "The principle is different from another belief clustering method put forward by Schubert [38], in which conflict between evidence is utilised to cluster the belief functions related to multiple events.", "startOffset": 89, "endOffset": 93}, {"referenceID": 47, "context": "Centrality and dissimilarity The problem of assigning centrality values to nodes in graphs has been widely investigated as it is important for identifying influential nodes [48].", "startOffset": 173, "endOffset": 177}, {"referenceID": 14, "context": "[15] put forward Evidential Semi-local Centrality (ESC) and pointed out that it is more reasonable than the existing centrality measures such as Degree Centrality (DC), Betweenness Centrality (BC) and Closeness Centrality (CC).", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The detail computation process of ESC can be found in [15].", "startOffset": 54, "endOffset": 58}, {"referenceID": 48, "context": "The dissimilarity measure considered in this paper is the one put forward by Zhou [49].", "startOffset": 82, "endOffset": 86}, {"referenceID": 48, "context": "One can refer to [49] for more details.", "startOffset": 17, "endOffset": 21}, {"referenceID": 34, "context": "A widely used measure called modularity, or Q was presented by Newman and Girvan [35].", "startOffset": 81, "endOffset": 85}, {"referenceID": 12, "context": "Given a hard partition with c groups (\u03c91, \u03c92, \u00b7 \u00b7 \u00b7 , \u03c9c) U = (uik)n\u00d7c, where uik is one if vertex i belongs to the kth community, 0 otherwise, and let the c crisp subsets of vertices be {V1, V2, \u00b7 \u00b7 \u00b7 , Vc}, then the modularity can be defined as [13]:", "startOffset": 247, "endOffset": 251}, {"referenceID": 20, "context": "[21] rewrote the equations in the form of U .", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] pointed out that an advantage of Eq.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "If uik is restricted in [0, 1], the fuzzy partition degrades to the hard one, and so Qf equals to Qh at this time.", "startOffset": 24, "endOffset": 30}, {"referenceID": 8, "context": "The objective function of MECM To group n objects in X = {x1,x2, \u00b7 \u00b7 \u00b7 ,xn} into c clusters \u03c91, \u03c92, \u00b7 \u00b7 \u00b7 , \u03c9c, the credal partition M = {m1,m2, \u00b7 \u00b7 \u00b7 ,mn} defined on \u03a9 = {\u03c91, \u03c92, \u00b7 \u00b7 \u00b7 , \u03c9c} is used to represent the class membership of the objects, as in [9, 32].", "startOffset": 256, "endOffset": 263}, {"referenceID": 31, "context": "The objective function of MECM To group n objects in X = {x1,x2, \u00b7 \u00b7 \u00b7 ,xn} into c clusters \u03c91, \u03c92, \u00b7 \u00b7 \u00b7 , \u03c9c, the credal partition M = {m1,m2, \u00b7 \u00b7 \u00b7 ,mn} defined on \u03a9 = {\u03c91, \u03c92, \u00b7 \u00b7 \u00b7 , \u03c9c} is used to represent the class membership of the objects, as in [9, 32].", "startOffset": 256, "endOffset": 263}, {"referenceID": 0, "context": "Parameter \u03b7 (\u2208 [0, 1]) can be tuned to control of the discounting degree.", "startOffset": 15, "endOffset": 21}, {"referenceID": 7, "context": "To minimize JMECM, an optimization scheme via an Expectation-Maximization (EM) algorithm as in MCM [8] and MFCM [16] can be designed, and the alternate update steps are as follows: Step 1.", "startOffset": 99, "endOffset": 102}, {"referenceID": 15, "context": "To minimize JMECM, an optimization scheme via an Expectation-Maximization (EM) algorithm as in MCM [8] and MFCM [16] can be designed, and the alternate update steps are as follows: Step 1.", "startOffset": 112, "endOffset": 116}, {"referenceID": 31, "context": "The bbas of the objects\u2019 class membership are updated identically to ECM [32], but it is worth noting that dij has different meanings and less constraints as explained before.", "startOffset": 73, "endOffset": 77}, {"referenceID": 7, "context": "The convergence of MECM algorithm can be proved in the following lemma, similar to the proof of median neural gas [8] and MFCM [16].", "startOffset": 114, "endOffset": 117}, {"referenceID": 15, "context": "The convergence of MECM algorithm can be proved in the following lemma, similar to the proof of median neural gas [8] and MFCM [16].", "startOffset": 127, "endOffset": 131}, {"referenceID": 0, "context": "Algorithm 1 : Median evidential c-means algorithm Input dissimilarity matrix D , [d(xi, xj)]n\u00d7n for the n objects {x1,x2, \u00b7 \u00b7 \u00b7 ,xn} Parameters c: number clusters 1 < c < n \u03b1: weighing exponent for cardinality \u03b2 > 1: weighting exponent \u03b4 > 0: dissimilarity between any object to the emptyset \u03b3 > 0: weight of dissimilarity between data and prototype vectors \u03b7 \u2208 [0, 1]: control of the discounting degree Initialization Choose randomly c initial cluster prototypes from the objects Loop t \u2190 0 Repeat (1).", "startOffset": 362, "endOffset": 368}, {"referenceID": 31, "context": "Although the objective function of MECM takes the same form as that in ECM [32], we should note that in MECM, it is no longer assumed that there is an underlying Euclidean distance.", "startOffset": 75, "endOffset": 79}, {"referenceID": 31, "context": "As suggested in [32], a value can be used as a starting default one but it can be modified according to what is expected from the user.", "startOffset": 16, "endOffset": 20}, {"referenceID": 31, "context": "The choice \u03b4 is more difficult and is strongly data dependent [32].", "startOffset": 62, "endOffset": 66}, {"referenceID": 31, "context": "For determining the number of clusters, the validity index of a credal partition defined by Masson and Denoeux [32] could be utilised:", "startOffset": 111, "endOffset": 115}, {"referenceID": 20, "context": "[21], here we introduce an evidential modularity [50]:", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[21], here we introduce an evidential modularity [50]:", "startOffset": 49, "endOffset": 53}, {"referenceID": 1, "context": "This situation violates the rule which requires the chosen seeds as far away from each other as possible [2, 26].", "startOffset": 105, "endOffset": 112}, {"referenceID": 25, "context": "This situation violates the rule which requires the chosen seeds as far away from each other as possible [2, 26].", "startOffset": 105, "endOffset": 112}, {"referenceID": 48, "context": "In this paper we test the approach with the dissimilarity measure proposed in [49].", "startOffset": 78, "endOffset": 82}, {"referenceID": 25, "context": "This seed choosing strategy is similar to that in [26].", "startOffset": 50, "endOffset": 54}, {"referenceID": 33, "context": "In such cases precision would be consequently low [34].", "startOffset": 50, "endOffset": 54}, {"referenceID": 33, "context": "Usually the fuzzy and evidential clusters are made crisp before calculating the measures, using for instance the maximum membership criterion [34] and pignistic probabilities [32].", "startOffset": 142, "endOffset": 146}, {"referenceID": 31, "context": "Usually the fuzzy and evidential clusters are made crisp before calculating the measures, using for instance the maximum membership criterion [34] and pignistic probabilities [32].", "startOffset": 175, "endOffset": 179}, {"referenceID": 31, "context": "Note that for evidential clusterings, precision, recall and RI measures are calculated after the corresponding hard partitions are got, while EP, ER and ERI are based on hard credal partitions [32].", "startOffset": 193, "endOffset": 197}, {"referenceID": 11, "context": "We also test on \u201cIris flower\u201d, \u201ccat cortex\u201d and \u201cprotein\u201d data sets [12, 17, 22].", "startOffset": 68, "endOffset": 80}, {"referenceID": 16, "context": "We also test on \u201cIris flower\u201d, \u201ccat cortex\u201d and \u201cprotein\u201d data sets [12, 17, 22].", "startOffset": 68, "endOffset": 80}, {"referenceID": 21, "context": "We also test on \u201cIris flower\u201d, \u201ccat cortex\u201d and \u201cprotein\u201d data sets [12, 17, 22].", "startOffset": 68, "endOffset": 80}, {"referenceID": 18, "context": "Thus we compare our method with FCM and ECM for the Iris data set, and with RECM and NRFCM (Non-Euclidean Relational Fuzzy Clustering Method [19]) for the last two data sets.", "startOffset": 141, "endOffset": 145}, {"referenceID": 48, "context": "The dissimilarity index used here is the one brought forward by Zhou [49].", "startOffset": 69, "endOffset": 73}, {"referenceID": 3, "context": "BGLL [4], LPA [36] and ZFCM (a fuzzy c-means based approach proposed by Zhang et al.", "startOffset": 5, "endOffset": 8}, {"referenceID": 35, "context": "BGLL [4], LPA [36] and ZFCM (a fuzzy c-means based approach proposed by Zhang et al.", "startOffset": 14, "endOffset": 18}, {"referenceID": 45, "context": "[46]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "This model has been used for testing community detection approaches by Liu and Liu [29].", "startOffset": 83, "endOffset": 87}, {"referenceID": 27, "context": "[28] benchmark (LFR) networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "The Zachary\u2019s Karate Club data [45] is an undirected graph which consists of 34 vertices and 78 edges.", "startOffset": 31, "endOffset": 35}, {"referenceID": 31, "context": "Firstly, the number of parameters to be optimized is exponential and depends on the number of clusters [32].", "startOffset": 103, "endOffset": 107}, {"referenceID": 31, "context": "But we can consider only a subclass with a limited number of focal sets [32].", "startOffset": 72, "endOffset": 76}], "year": 2015, "abstractText": "Median clustering is of great value for partitioning relational data. In this paper, a new prototypebased clustering method, called Median Evidential C-Means (MECM), which is an extension of median c-means and median fuzzy c-means on the theoretical framework of belief functions is proposed. The median variant relaxes the restriction of a metric space embedding for the objects but constrains the prototypes to be in the original data set. Due to these properties, MECM could be applied to graph clustering problems. A community detection scheme for social networks based on MECM is investigated and the obtained credal partitions of graphs, which are more refined than crisp and fuzzy ones, enable us to have a better understanding of the graph structures. An initial prototype-selection scheme based on evidential semi-centrality is presented to avoid local premature convergence and an evidential modularity function is defined to choose the optimal number of communities. Finally, experiments in synthetic and real data sets illustrate the performance of MECM and show its difference to other methods.", "creator": "LaTeX with hyperref package"}}}