{"id": "1501.02393", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2015", "title": "Riemannian Metric Learning for Symmetric Positive Definite Matrices", "abstract": "Over the past few years, symmetric positive definite (SPD) matrices have been receiving considerable attention from computer vision community. Though various distance measures have been proposed in the past for comparing SPD matrices, the two most widely-used measures are affine-invariant distance and log-Euclidean distance. This is because these two measures are true geodesic distances induced by Riemannian geometry. In this work, we focus on the log-Euclidean Riemannian geometry and propose a data-driven approach for learning Riemannian metrics/geodesic distances for SPD matrices. We show that the geodesic distance learned using the proposed approach performs better than various existing distance measures when evaluated on face matching and clustering tasks. In our paper, we examine the results of our previous work on the effect of log-Euclidean Riemannian geometric distances using a matrix-wide approach that is a nonlinear model of the data-driven model. In this paper, we examine the influence of log-Euclidean Riemannian geometry on learning Riemannian statistics/geodesic distances for Riemannian statistics/geodesic distances. For instance, the geometric distance learned using the proposed approach is observed in a nonlinear matrix-wide approach with a linear model-wide approach.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Sat, 10 Jan 2015 21:12:09 GMT  (9kb)", "http://arxiv.org/abs/1501.02393v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["raviteja vemulapalli", "david w jacobs"], "accepted": false, "id": "1501.02393"}, "pdf": {"name": "1501.02393.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Raviteja Vemulapalli", "David W. Jacobs"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 1.\n02 39\n3v 1\n[ cs\n.C V\nNotations\n\u2013 I denotes the identity matrix of appropriate size.\n\u2013 \u3008 , \u3009 denotes an inner product.\n\u2013 Sn denotes the set of n\u00d7 n symmetric matrices.\n\u2013 S++n denotes the set of n\u00d7 n symmetric positive definite matrices.\n\u2013 TpM denotes the tangent space to the manifold M at the point p \u2208 M.\n\u2013 \u2016 \u2016F denotes the matrix Frobenius norm.\n\u2013 Chol(P) denotes the lower triangular matrix obtained from the Cholesky decomposition of a matrix P.\n\u2013 exp() and log() denote matrix exponential and logarithm respectively.\n\u2013 \u2202 \u2202x and \u2202 2 \u2202x2 represent partial derivatives."}, {"heading": "1 Introduction", "text": "Many computer vision applications involve features that obey specific constraints. Such features often lie in non-Euclidean spaces, where the underlying distance metric is not the regular \u21132 norm. For instance, popular features like shapes, rotation matrices, linear subspaces, symmetric positive definite (SPD) matrices, etc. are known to lie on Riemannian manifolds. In such cases, one needs to develop inference techniques that make use of the underlying manifold structure.\nOver the past few years, manifolds have been receiving considerable attention from the computer vision community. In this work, we focus our attention on the set of SPD matrices. Examples of SPD matrices in computer vision include diffusion tensors [1], structure tensors [2] and covariance region descriptors [3]. Diffusion tensors arise naturally in medical imaging [1]. In diffusion tensor magnetic resonance imaging (DTMRI), water diffusion in tissues is represented by a diffusion tensor characterizing the anisotropy within the tissue. In optical flow estimation and motion segmentation, structure tensors are often employed to encode important image features, such as texture and motion [2]. Covariance region descriptors are used in texture classification [3], object detection [4], object tracking, action recognition and face recognition [5]. There are several advantages of using covariance matrices as region descriptors. Covariance matrices provide a natural way of fusing multiple features which might be correlated. The diagonal entries of a covariance matrix represent the variance of individual features and the non-diagonal entries represent the cross correlations. The noise corrupting individual samples is largely filtered out with an averaging filter during covariance computation. Covariance matrices are low dimensional compared to joint feature histograms. Covariance matrices do not have any information regarding the ordering and the number of points. This implies a certain level of scale and rotation invariance over the regions in different images.\nVarious distance measures have been proposed in the literature for the comparison of SPD matrices. Among them, the two most widely-used distance measures are the affine-invariant distance [1] and the log-Frobenius distance [6] (also referred to as logEuclidean distance in the literature). The main reason for their popularity is that they are geodesic distances induced by Riemannian metrics.\nThe log-Euclidean framework [6] proposed by Arsigny et. al. defines a class of Riemannian metrics, rather than a single metric, called log-Euclidean Riemannian metrics. According to this framework, any inner product \u3008 , \u3009 defined on TIS++n = {log(P ) | P \u2208 S++n } = Sn extended to S ++ n by left- or right- multiplication is a bi-invariant Riemannian metric. Equipped with this bi-invariant metric, the space of SPD matrices is a flat Riemannian space and the geodesic distance corresponding to this bi-invariant Riemannian metric is equal to the distance induced by \u3008 , \u3009 in TIS++n . Surprisingly, this remarkable result has not been used by the computer vision community. Since TIS++n = Sn is a vector space, this result allows us to learn log-Euclidean Riemannian metrics and corresponding log-Euclidean geodesic distances from the data by using Mahalanobis distance learning techniques like information-theoretic metric learning (ITML) [7] and large margin nearest neighbor distance learning [8] in TIS++n . In this work, we explore this idea of data driven Riemannian metrics/geodesic distances for the set of SPD matrices. For learning Mahalanobis distances in TIS++n we use the ITML technique.\nOrganization: In section 2, we provide a brief overview of various distance measures used in the literature to compare SPD matrices. We briefly explain the ITML technique in section 3 and present our approach for learning log-Euclidean Riemannian metrics/log-Euclidean geodesic distances from the data in section 4. We provide some experimental results in section 5 and conclude the paper in section 6."}, {"heading": "2 Distances to compare SPD matrices", "text": "Various distance measures have been used in the literature to compare SPD matrices. Each distance has been derived from different geometrical, statistical or informationtheoretic considerations. Though many of these distances try to capture the non-linearity of SPD matrices, not all of them are geodesic distances induced by Riemannian metrics. Tables 1 and 2 summarize these distances and their properties. Among them, the logFrobenius distance[6] and the affine-invariant distance[1] are the most popular ones."}, {"heading": "3 Mahalanobis distance learning using ITML", "text": "Information theoretic metric learning [7] is a technique for learning Mahalanobis distance functions from the data based on similarity and dissimilarity constraints. Let {xi} N i=1 be a set of N points in R\nd. Given pairs of similar points S and pairs of dissimilar points D, the aim of ITML is to learn an SPD matrix M such that the Mahalanobis distance parametrized by M is below a given threshold l for similar pairs of points and above a given threshold u for dissimilar pairs of points.\nLet Dld denote the LogDet divergence between SPD matrices defined as\nDld(P,Q) = trace(PQ\u22121)\u2212 log det(PQ\u22121)\u2212 n; P, Q \u2208 S++n . (1)\nITML formulates the Mahalanobis matrix learning as the following optimization problem:\nminimize M\u227b0, \u03b6 Dld(M,M0) + \u03b3Dld(diag(\u03b6), diag(\u03b60))\nsubject to (xi \u2212 xj)\u22a4M(xi \u2212 xj) \u2264 \u03b6c(i,j), \u2200(i, j) \u2208 S\n(xi \u2212 xj) \u22a4M(xi \u2212 xj) \u2265 \u03b6c(i,j), \u2200(i, j) \u2208 D,\n(2)\nwhere c(i, j) denotes the index of the (i, j)\u2212th constraint, \u03b6 is the vector of variables \u03b6c(i,j), \u03b60 is a vector whose components equal l for similarity constraints and u for dissimilarity constraints, M0 is an SPD matrix that captures the prior knowledge about M , and \u03b3 is a parameter controlling the tradeoff between satisfying the constraints and minimizing Dld(M,M0). This optimization problem can be solved efficiently using Bregman iterations. In this work, we use the publicly available ITML code provided by the authors of [7].\nITML parameters: We need to specify the values for the following parameters while using ITML: M0, \u03b3, l, u. We choose the constraint thresholds l and u as the ath and bth percentiles of the observed distribution of distances between pairs of points within the training dataset. Hence, the parameters for the ITML algorithm are M0, \u03b3, a and b."}, {"heading": "4 Log-Euclidean Riemannian metric learning", "text": "The log-Euclidean framework [6] proposed by Arsigny et. al. defines a class of Riemannian metrics called log-Euclidean metrics. The geodesic distances associated with log-Euclidean metrics are called log-Euclidean distances. Let \u2299 be an operation on SPD matrices defined as P1 \u2299 P2 = exp (log(P1) + log(P2)). We have the following result based on the log-Euclidean framework introduced in [6]:\nResult 4.1: Any inner product \u3008 , \u3009 defined on TIS++n = {log(P ) | P \u2208 S ++ n } = Sn extended to the Lie group (S++n ,\u2299) by left- or right- multiplication is a bi-invariant Riemannian metric. The corresponding geodesic distance between P1 \u2208 S++n and P2 \u2208 S ++ n is given by\nd(P1, P2) = \u2016mlogI(P1)\u2212 mlogI(P2)\u2016I = \u2016log(P1)\u2212 log(P2)\u2016I , (3)\nwhere \u2016 \u2016I is the norm induced by \u3008 , \u3009. Note that here mlogI is the inverse-exponential map at the identity matrix which is equal to the usual matrix logarithm in this case.\nThe set of all n\u00d7 n symmetric matrices form a vector space of dimension d = n(n+1)2 . Let vec(P ) denote the column vector form of the upper triangular part of a matrix P . This vec() operation provides a d dimensional vector representation for Sn. Let \u3008 , \u3009 be an inner product defined on the vector space Sn and M \u2208 S ++ d be the corresponding matrix of inner products between the d basis vectors corresponding to vec() representation. Note that \u3008 , \u3009 is uniquely characterized by M . The distance between two matrices P1 \u2208 Sn and P2 \u2208 Sn induced by this inner product is given by\nd(P1, P2) = (vec(P1)\u2212 vec(P2)) \u22a4 M (vec(P1)\u2212 vec(P2)) . (4)\nResult 4.2: Let M \u2208 S++d , where d = n(n+1)\n2 . Then, M defines a unique inner product denoted by \u3008 , \u3009M on TIS ++ n = {log(P ) | P \u2208 S ++ n } = Sn. This inner product \u3008 , \u3009M also defines a log-Euclidean Riemannian metric which can be obtained by simply extending \u3008 , \u3009M to the Lie group (S ++ n ,\u2299) by left- or right- multiplication. The corresponding log-Euclidean geodesic distance between P1 \u2208 S++n and P2 \u2208 S ++ n is given by\ndM (P1, P2) = (vec(log(P1))\u2212 vec(log(P2))) \u22a4 M (vec(log(P1))\u2212 vec(log(P2))) .\n(5) The above result follows directly from result 4.1. Result 4.2 says that any Mahalanobis distance defined in the vector space {vec(log(P )) | P \u2208 S++n } is a geodesic distance on S++n and the corresponding Riemannian metric is uniquely defined by the Mahalanobis matrix M . Hence, we can learn Riemannian metrics/geodesic distances for S++n from the data by learning Mahalanobis distance functions in the vector space {vec(log(P )) | P \u2208 S++n }. Table 3 summarizes our approach for leaning geodesic distances on S++n . In this work, we use ITML technique for Mahalanobis distance learning."}, {"heading": "5 Experiments", "text": "In the section, we evaluate the performance of the proposed Riemannian metric/geodesic distance learning approach on two applications: (i) Face matching using Labeled Faces in the Wild (LFW) dataset and (ii) Semi-supervised clustering using ETH80 dataset."}, {"heading": "5.1 Face matching using LFW face dataset", "text": "In this experiment our aim is to predict whether a given pair of face images correspond to the same person or not.\nDataset: The LFW dataset [9] is a collection of face photographs designed for studying the problem of unconstrained face recognition. This dataset consists of 13233 labeled face images of 1680 subjects collected from the web. This dataset consists of two subsets:\n\u2013 Development subset: The development subset consists of 2200 training image pairs, where 1100 are similar pairs and 1100 are dissimilar pairs, and 1000 test image pairs, where 500 are similar pairs and 500 are dissimilar pairs. An image pair is said to be similar if both the images correspond to the same person and dissimilar if they correspond to different persons. \u2013 Evaluation subset: The evaluation subset consists of 3000 similar image pairs and 3000 dissimilar image pairs. It is further divided into 10 subsets each of which consists of 300 similar pairs and 300 dissimilar pairs.\nAll the image pairs were generated by randomly selecting images from the 13233 images in the dataset. The development subset is meant for model and parameter selection. The evaluation subset should be used only once for final training and testing. To avoid overfitting, the image pairs in the development subset were chosen to be different from the image pairs in the evaluation subset.\nFeature extraction We crop the face region in each image and resize it to a 64 \u00d7 64 image. Following [3], we convert each pixel in an image into a 9-dimensional feature vector given by [\nx, y, R(x, y), G(x, y), B(x, y),\n\u2223 \u2223 \u2223 \u2223 \u2202W (x, y)\n\u2202x\n\u2223 \u2223 \u2223 \u2223 , \u2223 \u2223 \u2223 \u2223 \u2202W (x, y)\n\u2202y\n\u2223 \u2223 \u2223 \u2223 , \u2223 \u2223 \u2223 \u2223 \u22022W (x, y)\n\u2202x2\n\u2223 \u2223 \u2223 \u2223 , \u2223 \u2223 \u2223 \u2223 \u22022W (x, y)\n\u2202y2\n\u2223 \u2223 \u2223 \u2223 ]\u22a4 ,\nwhere x, y are the column and row coordinates respectively, R,G and B are the color coordinates and W is the grayscale image. We use the 9 \u00d7 9 covariance matrix of the feature vectors to represent the image.\nExperimental protocol Following the standard experimental protocol for this dataset, we use the development set for selecting the parameters of ITML and then use the evaluation set only once for final training and testing. Following steps summarize our experimental procedure:\n\u2013 Parameter selection: We train the ITML algorithm using the 2200 training pairs of the development subset and then test it on the 1000 test pairs of the development subset. We select the ITML parameters that give the best test accuracy. \u2013 Final training and testing: The evaluation set consists of 10 splits and we perform 10-fold cross-validation. In each fold, we use 9 splits (2700 similar pairs and 2700 dissimilar pairs) for training ITML and 1 split (300 similar pairs and 300 dissimilar pairs) for testing. For training ITML, we use the parameters that were selected in the previous step. Since our task is face matching, we need to threshold the learned distance function. In each fold, we find the threshold that gives best training accuracy and use the same threshold for test image pairs.\nComparative methods We compare the performance of the proposed log-Euclidean metric learning approach with the following approaches:\n\u2013 Directly use any of the following distances for matching: \u2022 Frobenius, Cholesky-Frobenius, J-divergence, Jensen-Bregman LogDet divergence, Affine-invariant and Log-Frobenius. \u2013 Use ITML directly with the covariance matrices by treating them as elements of\nthe Euclidean space of symmetric matrices. \u2013 Use ITML with the lower triangular matrix obtained by Cholesky decomposition.\nIn all these methods the distance threshold is obtained in each fold independently based on the training data.\nParameters The following parameter values were used for ITML:\n\u2013 M0 = I, \u03b3 = 103.5, a = 5, b = 95.\nThese parameters were selected using the development subset of the dataset.\nResults Tables 4 and 5 summarize the prediction results for various approaches on the LFW data set. We can draw the following conclusions from these results:\n\u2013 The proposed Riemannian metric/geodesic distance learning approach outperforms the other approaches for comparing covariance matrices. \u2013 The log-Euclidean geodesic distance learned from the data performs much better than the standard log-Frobenius distance. \u2013 Distance learning with original covariance matrices or Cholesky decompositions performs poorly compared to distance learning in the logarithm domain."}, {"heading": "5.2 Semi-supervised clustering using ETH80 object dataset", "text": "In this experiment, we are interested in clustering the images in the ETH80 dataset into different object categories.\nDataset The ETH80 object dataset [10] consists of 256\u00d7 256 images of 8 object categories with each category including 10 different object instances. Each object instance has 41 images captured under different views. So, each object category has 410 images resulting in a total of 3280 images.\nFeature extraction We convert each pixel in an image into a 9-dimensional feature vector given by\n[\nx, y, R(x, y), G(x, y), B(x, y),\n\u2223 \u2223 \u2223 \u2223 \u2202W (x, y)\n\u2202x\n\u2223 \u2223 \u2223 \u2223 , \u2223 \u2223 \u2223 \u2223 \u2202W (x, y)\n\u2202y\n\u2223 \u2223 \u2223 \u2223 , \u2223 \u2223 \u2223 \u2223 \u22022W (x, y)\n\u2202x2\n\u2223 \u2223 \u2223 \u2223 , \u2223 \u2223 \u2223 \u2223 \u22022W (x, y)\n\u2202y2\n\u2223 \u2223 \u2223 \u2223 ]\u22a4 ,\nwhere x, y are the column and row coordinates respectively, R,G and B are the color coordinates and W is the grayscale image. We compute the 9\u00d7 9 covariance matrix of the feature vectors over the entire image and use it to represent the image.\nExperimental protocol and parameters For every object category, we randomly select 4 images from each instance for training. Hence, we use 40 samples from each object category for training, resulting in a total of 320 training images. From each pair of training images, we generate either a similarity constraint or a dissimilarity constraints based on their category labels. We use all such constraints in learning the Mahalanobis distance function. Once we learn the Mahalanobis distance function, we use it for clustering the entire dataset of 3280 images.\nWe repeat the above procedure 5 times and report the average clustering accuracy. In each run, we select the value of ITML parameter \u03b3 using two fold cross-validation on the training data. We use the following values for other ITML parameters in all the 5 runs: M0 = I, a = 5, b = 95.\nWe use K-means algorithm for clustering. To handle the local-optimum issue, we run\nK-means with 20 different random initializations and select the clustering result corresponding to the minimum K-means cost value.\nComparative methods We compare the performance of the proposed log-Euclidean metric learning approach with the following approaches:\n\u2013 Unsupervised: Directly perform K-means clustering using any of the following distances: Frobenius, Cholesky-Frobenius and Log-Frobenius. \u2013 Use ITML directly with the covariance matrices by treating them as elements of the Euclidean space of symmetric matrices. \u2013 Use ITML with the lower triangular matrix obtained by Cholesky decomposition.\nComputation of mean doesn\u2019t have a closed form solution in the case of J-divergence or Jensen-Bregman LogDet divergence or Affine-invariant distance. Hence, we need to use some optimization procedure for computing the mean. This makes K-means algorithm highly computational. Hence, we do not use these distances for comparison in this work.\nResults Table 6 summarizes the clustering results for various approaches on the ETH80 dataset. We can draw the following conclusions from these results:\n\u2013 The proposed Riemannian metric/geodesic distance learning approach performs better than other approaches for clustering SPD matrices. \u2013 The log-Euclidean geodesic distance learned from the data performs much better than the standard log-Frobenius distance. \u2013 Distance learning with original covariance matrices or Cholesky decompositions performs poorly compared to distance learning in the logarithm domain."}, {"heading": "6 Conclusion", "text": "In this work, we have explored the idea of data-driven Riemannian metrics or geodesic distances. Based on the log-Euclidean framework [6], we have shown how geodesic distance functions can be learned for S++n by simply learning Mahalanobis distance functions in the logarithm domain. We have conducted experiments using face and object data sets. The face matching and semi-supervised object categorization results clearly show that the learned log-Euclidean geodesic distance performs much better than other distances."}], "references": [{"title": "A Riemannian Framework for Tensor Com- puting", "author": ["X. Pennec", "P. Fillard", "N. Ayache"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Clustering and Dimensionality Reduction on Riemannian Manifolds", "author": ["A. Goh", "R. Vidal"], "venue": "In CVPR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Region Covariance: A Fast Descriptor for De- tection and Classification", "author": ["O. Tuzel", "F. Porikli", "P. Meer"], "venue": "In ECCV,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Pedestrian Detection via Classification on Rie- mannian Manifolds", "author": ["O. Tuzel", "F. Porikli", "P. Meer"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Lovel, \u201cSparse Coding and Dictio- nary Learning for Symmetric Positive Definite Matrices: A Kernel Approach", "author": ["M. Harandi", "C. Sanderson", "R. Hartley"], "venue": "In ECCV,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Log-Euclidean Metrics for Fast and Simple Calculus on Diffusion Tensors", "author": ["V. Arsigny", "P. Fillard", "X. Pennec", "N. Ayache"], "venue": "Magnetic Resonance in Medicine,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Information-Theoretic Met- ric Learning ", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "In ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Distance Metric Learning for Large Margin", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "Nearest Neighbor Classification\u201d,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "L.-Miller, \u201cLabeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments\u201d, Univer- sity of Massachusetts, Amherst", "author": ["G.B. Huang", "M. Ramesh", "T. Berg"], "venue": "Technical Report 07-49,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Analyzing Appearance and Contour Based Methods for Object Categorization", "author": ["B. Leibe", "B. Schiele"], "venue": "In CVPR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Efficient Similarity Search for Covariance Matrices via the Jensen-Bregman LogDet Divergence", "author": ["A. Cherian", "S. Sra", "A. Banerjee", "N. Papanikolopoulos"], "venue": "In ICCV,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "An Affine Invariant Tensor Dissimilarity Measure and its Applications to Tensor-valued Image Segmentation", "author": ["Z. Wang", "B.C. Vemuri"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Non-Euclidean Statistics for Covari- ance Matrices, with Applications to Diffusion Tensor Imaging", "author": ["I.L. Dryden", "A. Koloydenko", "D. Zhou"], "venue": "The Annals of Ap- plied Statistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Examples of SPD matrices in computer vision include diffusion tensors [1], structure tensors [2] and covariance region descriptors [3].", "startOffset": 70, "endOffset": 73}, {"referenceID": 1, "context": "Examples of SPD matrices in computer vision include diffusion tensors [1], structure tensors [2] and covariance region descriptors [3].", "startOffset": 93, "endOffset": 96}, {"referenceID": 2, "context": "Examples of SPD matrices in computer vision include diffusion tensors [1], structure tensors [2] and covariance region descriptors [3].", "startOffset": 131, "endOffset": 134}, {"referenceID": 0, "context": "Diffusion tensors arise naturally in medical imaging [1].", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "In optical flow estimation and motion segmentation, structure tensors are often employed to encode important image features, such as texture and motion [2].", "startOffset": 152, "endOffset": 155}, {"referenceID": 2, "context": "Covariance region descriptors are used in texture classification [3], object detection [4], object tracking, action recognition and face recognition [5].", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "Covariance region descriptors are used in texture classification [3], object detection [4], object tracking, action recognition and face recognition [5].", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "Covariance region descriptors are used in texture classification [3], object detection [4], object tracking, action recognition and face recognition [5].", "startOffset": 149, "endOffset": 152}, {"referenceID": 0, "context": "Among them, the two most widely-used distance measures are the affine-invariant distance [1] and the log-Frobenius distance [6] (also referred to as logEuclidean distance in the literature).", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "Among them, the two most widely-used distance measures are the affine-invariant distance [1] and the log-Frobenius distance [6] (also referred to as logEuclidean distance in the literature).", "startOffset": 124, "endOffset": 127}, {"referenceID": 5, "context": "The log-Euclidean framework [6] proposed by Arsigny et.", "startOffset": 28, "endOffset": 31}, {"referenceID": 6, "context": "Since TIS n = Sn is a vector space, this result allows us to learn log-Euclidean Riemannian metrics and corresponding log-Euclidean geodesic distances from the data by using Mahalanobis distance learning techniques like information-theoretic metric learning (ITML) [7] and large margin nearest neighbor distance learning [8] in TIS n .", "startOffset": 265, "endOffset": 268}, {"referenceID": 7, "context": "Since TIS n = Sn is a vector space, this result allows us to learn log-Euclidean Riemannian metrics and corresponding log-Euclidean geodesic distances from the data by using Mahalanobis distance learning techniques like information-theoretic metric learning (ITML) [7] and large margin nearest neighbor distance learning [8] in TIS n .", "startOffset": 321, "endOffset": 324}, {"referenceID": 5, "context": "Among them, the logFrobenius distance[6] and the affine-invariant distance[1] are the most popular ones.", "startOffset": 37, "endOffset": 40}, {"referenceID": 0, "context": "Among them, the logFrobenius distance[6] and the affine-invariant distance[1] are the most popular ones.", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "3 Mahalanobis distance learning using ITML Information theoretic metric learning [7] is a technique for learning Mahalanobis distance functions from the data based on similarity and dissimilarity constraints.", "startOffset": 81, "endOffset": 84}, {"referenceID": 6, "context": "In this work, we use the publicly available ITML code provided by the authors of [7].", "startOffset": 81, "endOffset": 84}, {"referenceID": 12, "context": "Frobenius \u2016P1 \u2212 P2\u2016F Yes Yes No CholeskyFrobenius [13] \u2016Chol(P1)\u2212 Chol(P2)\u2016F Yes Yes No", "startOffset": 50, "endOffset": 54}, {"referenceID": 11, "context": "J-divergence [12] 1 2 \u221a", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "trace(P1P 2 + P2P \u22121 1 )\u2212 2n Yes No No Jensen-Bregman LogDet Divergence[11] \u221a", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "Affine-invariant [1] \u2016log (", "startOffset": 17, "endOffset": 20}, {"referenceID": 5, "context": "Log-Frobenius [6] \u2016log(P1)\u2212 log(P2)\u2016F Yes Yes Yes", "startOffset": 14, "endOffset": 17}, {"referenceID": 12, "context": "Table 2: SPD matrix distances and their properties Distance Distance from S n Affine invariance Scale invariance Rotation invariance Inversion invariance Frobenius Finite No No Yes No Cholesky-Frobenius [13] Finite No No No No", "startOffset": 203, "endOffset": 207}, {"referenceID": 11, "context": "J-divergence [12] Infinite Yes Yes Yes Yes Jensen-Bregman LogDet Divergence[11] Infinite Yes Yes Yes Yes", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "J-divergence [12] Infinite Yes Yes Yes Yes Jensen-Bregman LogDet Divergence[11] Infinite Yes Yes Yes Yes", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "Affine-invariant [1] Infinite Yes Yes Yes Yes Log-Frobenius [6] Infinite No Yes Yes Yes", "startOffset": 17, "endOffset": 20}, {"referenceID": 5, "context": "Affine-invariant [1] Infinite Yes Yes Yes Yes Log-Frobenius [6] Infinite No Yes Yes Yes", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "4 Log-Euclidean Riemannian metric learning The log-Euclidean framework [6] proposed by Arsigny et.", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "We have the following result based on the log-Euclidean framework introduced in [6]: Result 4.", "startOffset": 80, "endOffset": 83}, {"referenceID": 8, "context": "Dataset: The LFW dataset [9] is a collection of face photographs designed for studying the problem of unconstrained face recognition.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "Following [3], we convert each pixel in an image into a 9-dimensional feature vector given by", "startOffset": 10, "endOffset": 13}, {"referenceID": 9, "context": "Dataset The ETH80 object dataset [10] consists of 256\u00d7 256 images of 8 object categories with each category including 10 different object instances.", "startOffset": 33, "endOffset": 37}, {"referenceID": 5, "context": "Based on the log-Euclidean framework [6], we have shown how geodesic distance functions can be learned for S n by simply learning Mahalanobis distance functions in the logarithm domain.", "startOffset": 37, "endOffset": 40}], "year": 2015, "abstractText": "Over the past few years, symmetric positive definite matrices (SPD) have been receiving considerable attention from computer vision community. Though various distance measures have been proposed in the past for comparing SPD matrices, the two most widely-used measures are affine-invariant distance and log-Euclidean distance. This is because these two measures are true geodesic distances induced by Riemannian geometry. In this work, we focus on the log-Euclidean Riemannian geometry and propose a data-driven approach for learning Riemannian metrics/geodesic distances for SPD matrices. We show that the geodesic distance learned using the proposed approach performs better than various existing distance measures when evaluated on face matching and clustering tasks. Notations \u2013 I denotes the identity matrix of appropriate size. \u2013 \u3008 , \u3009 denotes an inner product. \u2013 Sn denotes the set of n\u00d7 n symmetric matrices. \u2013 S n denotes the set of n\u00d7 n symmetric positive definite matrices. \u2013 TpM denotes the tangent space to the manifold M at the point p \u2208 M. \u2013 \u2016 \u2016F denotes the matrix Frobenius norm. \u2013 Chol(P) denotes the lower triangular matrix obtained from the Cholesky decomposition of a matrix P. \u2013 exp() and log() denote matrix exponential and logarithm respectively. \u2013 \u2202 \u2202x and \u2202 2 \u2202x represent partial derivatives. 2 Raviteja Vemulapalli, David W. Jacobs", "creator": "LaTeX with hyperref package"}}}