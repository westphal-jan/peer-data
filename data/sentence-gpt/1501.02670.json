{"id": "1501.02670", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jan-2015", "title": "Navigating the Semantic Horizon using Relative Neighborhood Graphs", "abstract": "This paper is concerned with nearest neighbor search in distributional semantic models. A normal nearest neighbor search only returns a ranked list of neighbors, with no information about the structure or topology of the local neighborhood. This is a potentially serious shortcoming of the mode of querying a distributional semantic model, since a ranked list of neighbors may conflate several different senses of the neighborhood or a single neighbor. We therefore show that an even stronger form of partial information such as the index of local area search in the neighborhood is in the form of a partial-information function.\n\n\n\n\nWe show that this is in essence the form of an index of local area search in the neighborhood. It can be done in many ways, by combining the indices of local area searches to a rank of the local area search in the neighborhood. For instance, we can simply combine a list of neighbors with a rank of neighbor search in the neighborhood, and, in most cases, a list of neighbors.\nFigure 1.\nIn order to perform such partial data we have to use the following three methods:\na search for a list of neighbors (a list of neighbors), and a search for neighbor that is less than the index of the local neighborhood search in the neighborhood. (b) search for a list of neighbors for neighbors that is more than the index of the local neighborhood search in the neighborhood. (c) search for a list of neighbors for neighbors that is more than the index of the local neighborhood search in the neighborhood. (d) search for a list of neighbors that is more than the index of the local neighborhood search in the neighborhood.\nUsing the following three methods, we can do even more with a single neighbor search in the neighborhood. For example, we can use the same data for every neighbor in the neighborhood.\nFigure 2.\nIn order to perform such partial data we have to use the following three methods:\na search for a list of neighbors for neighbors that is more than the index of the local neighborhood search in the neighborhood. (e) search for a list of neighbors that is more than the index of the local neighborhood search in the neighborhood. (f) search for a list of neighbors that is more than the index of the local neighborhood search in the neighborhood. (g) search for a list of neighbors that is more than the index of the local neighborhood search in the neighborhood. (h) search for a list of neighbors that is more than the index of the local neighborhood search in the neighborhood. (j) search", "histories": [["v1", "Mon, 12 Jan 2015 14:48:54 GMT  (355kb,D)", "http://arxiv.org/abs/1501.02670v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["amaru cuba gyllensten", "magnus sahlgren"], "accepted": true, "id": "1501.02670"}, "pdf": {"name": "1501.02670.pdf", "metadata": {"source": "CRF", "title": "Navigating the Semantic Horizon using Relative Neighborhood Graphs", "authors": ["Amaru Cuba Gyllensten"], "emails": ["amaru@gavagai.se", "mange@gavagai.se"], "sections": [{"heading": "1 Introduction", "text": "Nearest neighbor search is fundamental operation in data mining, in which we are interested in finding the closest points (to some given reference point). Formally, if we have a reference point r and a set of other points P in a metric space M with some distance function d (or similarity function s), the nearest neighbor search task is to find the point p \u2208 P that minimizes d(p, r). In k-nearest neighbor search (k-NN), we want to find the k closest points to some given reference point. Nearest neighbor search is a well-studied task, and in particular the complexity of the task (a linear search has a running time of O(Ni) where N is the cardinality of P and i the complexity of the distance function d) has generated a lot of research (Bentley, 1975; Arya et al., 1998; Indyk and Motwani, 1998).\nThe problem we are concerned with in this paper is not the complexity of nearest neighbor search, but the question how to identify the internal structure of neighborhoods defined by the nearest neighbors. The problem with a normal k nearest neighbor search is that the result (a sorted list of the k nearest neighbors) does not say anything about the internal structure of the neighborhood. Consider spaces a and b in Figure 1. A nearest neighbor search for the reference point r in these two spaces will generate the exact same result, despite the fact that the neighborhoods are very different with regards to their internal structure (the neighbors in space a display a distinct branching structure, whereas the neighbors in space b are distributed evenly across the space). Such structural properties of nearest neighborhoods can be very important.\nWe propose to use relative neighborhood graphs in order to identify the structural properties of nearest neighborhoods. The use of relative neighborhood graphs also provides a partial solution to the problem of finding a relevant k for a given reference point. Again, consider the neighborhoods in Figure 1. The choice of k = 10 in these spaces is completely arbitrary, and could be argued to be erroneous, since there are in fact 12 relevant neighbors in both spaces. Another way to approach the nearest neighbor search task is to use a radius around the reference point, so that only points within that radius are considered to be neighbors. However, setting a global threshold t seems just as arbitrary as setting a global number of neighbors k. Ideally, t or k should be determined based on the structural properties of the nearest neighborhood around the reference point. We refer to this ar X\niv :1\n50 1.\n02 67\n0v 1\n[ cs\n.C L\n] 1\n2 Ja\nn 20\n15\nfactor as the horizon with respect to the reference point.\nAlthough a relative neighborhood graph is a general method for defining and structuring nearest neighborhoods, we are in this paper primarily interested in its application to nearest neighbor searches in distributional semantic models that collect and represent co-occurrence statistics in high-dimensional vector spaces. The main operation in such models is nearest neighbor search, which is used for finding terms that have similar co-occurrence behavior. However, a ranked list of neighbors does not provide any information on whether the neighbors belong to several different senses. This problem has been misinterpreted as a shortcoming of the distributional representation (Erk and Pado\u0301, 2010). However, as we will demonstrate in this paper, this is not a shortcoming of the distributional representation, but of the mode of querying the distributional model. We argue that information about the different usages (i.e. senses) of a term is encoded in the structural properties of the nearest neighborhoods, and that a relative neighborhood graph is a viable tool for uncovering such structural properties."}, {"heading": "2 Distributional Semantics and Nearest Neighbor Search", "text": "Collecting and comparing co-occurrence statistics for terms in language has become a standard approach for computational semantics, and is now commonly referred to as distributional semantics. There are many different types of models that can be used for this purpose, but their common objective is to represent terms as vectors that record (some function of) their distributional properties. The standard approach for generating such vectors is to collect distributional statistics in a co-occurrence matrix that records co-occurrence counts between terms and contexts. The cooccurrence matrix is then subject to various types of transformations, ranging from the application of simple frequency filters or association measures like pointwise mutual information, to matrix factorization or regression models. The resulting representations are referred to as distributional vectors, and are typically dense with a dimensionality that is considerably lower than that of the original co-occurrence matrix.\nThe distributional vectors are used to compute similarity between terms. There are many ways to compute similarity or distance between points in vector space; the cosine of the angle between vectors is often the preferred metric in distributional semantics because of its simplicity and because it normalizes for vector length. Computing the similarity between distributional vectors using the cosine measure gives us a score ranging from \u22121 \u2014\nnegatively collinear \u2014 to 1 \u2014 positively collinear \u2014 taking the value 0 if the vectors are orthogonal.\nWe can thus use a distributional semantic model to quantify the similarity between any given terms. If the set of given terms is the entire set of terms in our model, we are in effect performing a nearest neighbor search. This is a particularly important operation in distributional semantics, since it answers the question \u201dwhich other terms are similar to this one?\u201d, and this is a central question in semantics; lexica and thesauri are built with the main purpose of answering this question, and a nearest neighbor search in a distributional semantic model could therefore be seen as a compilation step in a distributional lexicon.\nThe result of a nearest neighbor search in a distributional semantic model is often presented as a list of (the top k) neighbors, sorted by descending similarity with the target term. Table 1 illustrates typical sorted nearest neighbor lists produced with three different kinds of distributional semantic models: a vanilla-flavored model based on (positive) Pointwise Mutual Information (PMI),1 the skip-gram model (Mikolov et al., 2013), and GloVe (Pennington et al., 2014).\nIn the vanilla-flavored model, the distributional vector of a word is given by its (positive) PMI with regards to all other words that have occurred within a context window of 2 words to the left and 2 words to the right. That is, a vector for a word a corresponds to the information an observation of a gives when predicting surrounding words. Positive PMI means that negative values are discarded, and only positive PMI values are retained. The cosine similarity of two distributional vectors thus gives a measure of how similar the information gained by observing the corresponding words are. As a\n1For observations a and b, pmi(a, b)= log p(a,b) p(a)p(b) . The probabilities are often replaced in distributional semantic models by co-occurrence counts of a and b and their respective frequency counts.\nway to speed up later computations we apply a Gaussian random projection to reduce the dimensionality down to 2000.\nGloVe on the other hand tries to find distributional vectors such that their dot product approximates their log probability of co-occurring is motivated by the fact that the logarithm of ratios equals the difference of logarithms, which makes the vector differences meaningful in that they encode (logarithms of) ratios of probabilities. Reframed as a weighted least squares problem, where rare cooccurrences are weighted down, it can be solved by standard methods. The performance is comparable to the skip-gram model, and it performs particularly well on word-analogy tasks (Pennington et al., 2014).\nThe objective of the skipgram model is to maximize the probability of observing all context-word pairs given that the probability of one observation of a word c in the context of t is given by exp (w>c vt)\u2211 l\u2208V exp(u > l vt) where va and ua denotes the \u201dinput\u201d and \u201doutput\u201d vectors of the word a, and V is the vocabulary. The embeddings are found using stochastic gradient descent and hierarchical softmax combined with negative sampling and subsampling. Exactly how these methods compose is still unclear, and puts into question what the underlying model actually is (Levy and Goldberg, 2014). Regardless, the skip-gram model delivers state of the art performance on a multitude of tasks, with very low-dimensional vectors (Baroni et al., 2014).\nTable 1 lists the 10 nearest neighbors to suit in three different distributional semantic models using the entire Wikipedia as data.2 As can be expected, there are both similarities and dissimilarities between these neighborhoods; \u201dsuits\u201d and \u201dlawsuit\u201d occur among the 10 nearest neighbors to \u201dsuit\u201d in all three models, whereas other terms are specific for one particular model. Yet all three models feature neighbors of \u201dsuit\u201d that represent different senses: the way \u201dsuit\u201d is not related to \u201djacket\u201d in the same way it is related to \u201dlawsuit\u201d.\nIt has been argued that distributional semantic models that represent terms by a single vector cannot adequately handle polysemy, since they conflate several different usage patterns in one and the same vector (Erk and Pado\u0301, 2010; Ve\u0301ronis, 2004). Examples like the one above is often cited as evidence. We argue that this critique is unfounded and misinformed, and that it is the mode of querying the distributional semantic model that can be susceptible to problems with polysemy. As the above example demonstrates, querying distributional semantic models by k-NN conflates differ-\n2We use a Wikipedia dump from 2010 as data in this and following experiments.\nent usages of terms. The reason for this seems quite obvious: simply ranking the nearest neighbors by similarity (or distance) ignores any local structures of the neighborhood. If \u201dsuit\u201d has as neighbors both \u201ddress\u201d and \u201dlawsuit\u201d, which represent two distinct types of usages of \u201dsuit\u201d, there will be a structural distinction in the neighborhood of \u201dsuit\u201d between these different neighbors, since they will be mutually unrelated (i.e. there is a similarity between \u201dsuit\u201d and \u201ddress\u201d and between \u201dsuit\u201d and \u201dlawsuit\u201d, but not between \u201ddress\u201d and \u201dlawsuit\u201d).\nk-NN also gives rise to another problem related to polysemy in distributional semantic models. The problem is that the most frequent senses will populate the top of the nearest neighbor list, while the less frequent senses will not appear until further down the list, and if we set a too restrictive k, we will only see neighbors relating to the most frequent sense. Consider, for example, a term such as \u201dsuit\u201d, which, as we have seen above, may appear in (at least) two different senses: in usages related to law and in usages related to clothes (or garment). The distributional vector can be thought of as a sum vsuit = fsuit|lawvsuit|law + fsuit|clothesvsuit|clothes, where vsuit|law is an idealized notion of the true distributional vector of \u201dsuit\u201d in the law -sense, and fsuit|law is the relative frequency of this sense. 3 From there one can easily argue that a similarity such as s(vsuit, vgarment) is actually a weighted composite of the similarities s(vsuit|law, vgarment) and s(vsuit|clothes, vgarment).\n4 If \u201dsuit\u201d occurs predominantly in the law -sense in our corpus, the kNN neighborhood of \u201dsuit\u201d will be dominated by words pertaining to its law -sense, while the less frequent senses might not be present at all. A misguided k may thus obscure any other, less frequent, senses of a term.\nAnother problem with setting a global k in distributional semantic models is that some terms will have a much denser neighborhood than others. Using the same k for all terms therefore seems illadvised; terms with a dense neighborhood warrant a larger k than those with a sparse neighborhood. As we have already touched upon in the introduction, determining k is a fundamental question in k-NN, for which there seems to be no clear solution. A more informed approach compared to setting a global k would be to consider the distribution of distances/similarities and attempt to find a gap in the distribution at which to cut off the list. However, the distribution of similarities in distributional semantic models typically does not\n3Weighting schemes muddles this notion quite a bit, but we think the general intuition still holds.\n4In the case of cosine similarity this follows nicely from the distributive property of dot products: v = av1 + bv2, s(v, w) = v\u00b7w \u2016v\u2016\u2016w\u2016 = a(v1\u00b7w)+b(v2\u00b7w) \u2016v\u2016\u2016w\u2016\nhave any clear gaps, as exemplified in Figure 2.\nNote that the curves behave approximately the same in all three models; there are a few (one or two) very close neighbors, and then the similarities decrease very slowly. The difference in magnitude of the similarities between the models is not peculiar for the word \u201dsuit\u201d. On the contrary, PMI, GloVe, and the skipgam model produce vector spaces with different inherent densities. Figure 3 shows both the similarities to 1000 randomly selected points, as well as the similarities to the 10 nearest neighbors to 1000 randomly selected points. The skipgram model produces the highest similarity scores both for related and unrelated points, while the PMI model produces the lowest scores for both related and unrelated points. The GloVe model is in between. All models show a more or less clear distinction between the average similarities to randomly chosen points and the average similarities to the nearest neighbors. This distinction suggest that it might be possible to use the expected similarity to a randomly selected point as a cut-off threshold for k-NN. However, such a global estimate will not be suitable for all terms, for the very same reason alluded to above; different terms have different densities of their neighborhoods. Furthermore, it seems as if the PMI model distinguishes more clearly between the related and the unrelated points, with the skipgram model having the most outliers. This suggests a global estimate might be more useful in some types of models (like the standard PMI model) than in others (like the skipgram model)."}, {"heading": "3 Word-sense Induction", "text": "Selecting a relevant k for a given term and grouping the neighbors according to which senses they represent is an example of word-sense induction. Distributional semantic models are well suited for this task, and there have been a number of different approaches suggested in the literature, which can roughly be divided into context clustering and word\nclustering approaches. Context clustering does not operate on nearest neighbor lists, but instead clusters representations of each individual occurrence of a term. (Schu\u0308tze, 1998) is one of the earliest examples of a context clustering approach, in which context vectors (the centroid of the distributional vectors of the terms that occur in the context) for a given term are clustered into a set of sense vectors that represent the induced senses. Other examples of context clustering include (Purandare and Pedersen, 2004), (Velldal, 2005), (Reisinger and Mooney, 2010), (Pedersen, 2010), and (Jurgens and Stevens, 2010).\nIn contrast to context clustering , word clustering clusters the nearest neighbors into sense groups, and are thus the type of approach that is most relevant for our purposes. The earliest example of a word clustering approach is distributional clustering (Pereira et al., 1993), which clusters nouns that occur as heads of direct objects of verbs according to their distributional similarity. The resulting noun clusters for a verb can be interpreted as a representation of the different senses of the verb.\nAnother example of a word clustering approach is clustering by committee (Pantel and Lin, 2002), which is a distributional clustering procedure in several steps. The first step is to use averagelink clustering to recursively cluster the nearest neighbors into a set of clusters called committees. The committees are then used to define clusters by iteratively adding committees whose similarity to the term exceeds a certain threshold, and that is not too similar to any other added committee. For each added committee, its features are also removed from the distributional representation of the lexeme. This last step ensures that the clusters do not become too similar, and that clusters representing less frequent senses can be discovered.\nThe idea of iteratively removing features from the distributional vector when a sense cluster as been formed is also present in (Dorow and\nWiddows, 2003), who use a graph-based clustering method (Markov clustering (van Dongen, 2000)) to cluster the nearest distributional neighbors of a lexeme. Another graph-based approach to word-sense induction is the HyperLex algorithm (Ve\u0301ronis, 2004), which constructs a graph connecting all pairs of terms that co-occur in the context of an ambiguous term. The resulting graph contains highly connected components (hubs), which represent the different senses of the term. (Agirre et al., 2006) compares HyperLex to PageRank (Brin and Page, 1998) and demonstrates that the two methods perform similarly on a word-sense induction task. Other examples of graph-based approaches include (Biemann, 2006), (Klapaftis and Manandhar, 2008), and (Marco and Navigli, 2013).\nThere have also been several attempts to use various types of matrix factorization to perform word sense induction. The idea is that the factorization uncovers a set of global senses in the form of the latent factors, and that the sense distribution for a given term can be described as a distribution over these latent factors. (Brody and Lapata, 2009), (Se\u0301aghdha and Korhonen, 2011), (Yao and Van Durme, 2011), and (Lau et al., 2012) use different versions of Latent Dirichlet Allocation to produce the factorization, while (Dinu and Lapata, 2010) and (Van de Cruys and Apidianaki, 2011) instead experiment with non-negative matrix factorization.\n(Tomuro et al., 2007) argues that clustering approaches like distributional clustering or clustering by committee may produce clusters that are themselves polysemous, which may not be a desirable property of a word sense induction algorithm. As a solution to this problem, Tomuro et al. suggest using feature domain similarity, which refers to the similarity between the features of items rather than the similarity between the items themselves. The domain feature similarity score is incorporated in a modified version of the clustering by committee algorithm, in which the algorithm is run twice, using the output of the first run as input to the second run. The idea is that this iterative approach may enable the algorithm to utilize higher-order features, and that this will inhibit the formation of polysemous clusters, since the domain feature similarity of a polysemous cluster will be lower than the score for a monosemous cluster.\n(Koptjevskaja Tamm and Sahlgren, 2014) also leverage on the idea of using feature similarity as the basis of sense clustering. The approach, called syntagmatically labeled partitioning, relies on a distributional semantic model that encodes sequential as well as substitutable relations. The method essentially sorts the k nearest (substitutable) neighbors according to which sequential connections they share. The resulting partitioning of the near-\nest distributional neighbors does not only constitute a word-sense induction, but it also provides labels for the induced senses in the form of the sequential connections the neighbors share."}, {"heading": "4 Neighborhood Graphs", "text": "Many of the previous approaches to word-sense induction mentioned in the previous section operate at a global level, utilizing global structural properties of the semantic spaces, e.g. by matrix factorization techniques. We believe this is as illadvised as setting a global k or radius for the nearest neighbor search, since it is the local structures that are important when analyzing nearest neighbors. Other approaches to word-sense induction use various forms of clustering techniques. However, previous studies of the intrinsic dimensionality of distributional semantic spaces using fractal dimensions indicate that neighborhoods in semantic space have a filamentary rather than clustered structure (Karlgren et al., 2008).\nWe therefore propose the use of topological models that take the local structure of neighborhoods in semantic space into account. The method proposed here performs no global clustering, does not concern itself with grammatical preprocessing or parsing, and the distributional vectors are taken as is. The approach discovers different word senses from the local structure of neighborhoods, given nothing but similarities between points. As such it is easy to test on widely different vector models, as long as there exists a well behaved similarity function. The proposed approach not only answers the question which other terms are similar to a given term, but also how are they similar.\nRelative neighborhoods are examples of empty region graphs (Cardinal et al., 2009), where points are neighbors if some region between them is empty. For relative neighborhood graphs the region between two points a and c belonging to some set of points V is defined as the intersection of the two spheres with centers in a and c, with radius d(a, c). In other words, a point b lies between points a and c if it is closer to both a and c than a and c are to each other, and if no such point b exists, a and c are neighbors. Illustrations of this can be seen in Figure 4.\nSuch neighborhoods have been argued to better preserve local topology (Bremer et al., ), and be more robust to deformations of the data than kNN neighborhoods (Correa and Lindstrom, 2012) as they in some sense contain information about direction whereas k-NN neighborhoods only contain information about distance. Going back to the \u201dsuit\u201d example, we can see that if \u201dsuit\u201d in the law sense is more similar to the composite \u201dsuit\u201d than to its clothes sense, and vice versa, then the composite vsuit lies between vsuit|law and vsuit|clothes. This in turn means that out of those two points, both are relative neighbors to \u201dsuit\u201d, and neither of them lies between the other and \u201dsuit\u201d.\nFormally, the set of points between two points a, c \u2208 V can be characterized and computed in the following way:\nbetweens(V, a, c) = {b|b \u2208 V, b lies between a and c}\nrng-nbh(V, a) = {c|c \u2208 V,betweens(V, a, c) = \u2205}\nErng(V ) = {(a, b)|a \u2208 V, b \u2208 rng-nbh(V, a)}\nwhere Erng is the undirected edge set of the RNG. The function betweens(V, a, c) can be straightforwardly translated to an algorithm taking O(|V |) time, making the rng-nbh function take O(|V |2) time, which in turn makes the computation of the complete graph take O(|V |3) time.5 Clearly unfeasible, but we have not found any alternatives that performs better in the high dimensional case.6\nIn (Correa and Lindstrom, 2012) it is noted that the intersection of the relative neighborhood graph and the k-NN graph is a more feasible alternative:\nk-rng-nbh(V, a) = rng-nbh(V \u2032, a)\nwhere V \u2032 = k nearest neighbors of a\nGiven a precompiled \u2014 i.e. constant time \u2014 kNN lookup, the above takes O(k2) time, so using a heap-based O(|V | lg k) k-NN algorithm results in an algorithm taking O(k2 + |V | lg k) time.\nThe same idea can be used to build a tree structure \u2014 here called relative neighborhood tree \u2014 rooted in a reference word a , in the following way:\nrnbh-tree(V, a) = {(c, arg min b\u2208Bc d(b, c))|c \u2208 V }\nwhere Bc = {a} \u222a betweens(V, a, c)\nThis can easily be restricted to the k-nearest neighbors of a in much the same way as above.\nComputing this for a point a produces a tree where the direct children of a are its relative neighbors, and the parent of a point c further down the tree is the point between a and c that is closest to\n5Assuming a constant time distance function. 6It should be noted that there are more efficient\nalgorithms for lower dimensional situations.\nc. Figure 5 illustrates what the resulting structure looks like for \u201dheart\u201d on its 100 nearest neighbors in the PMI model. Note that the root \u201dheart\u201d (at the mid-left in the graph) only has two relative neighbors: \u201dcardiac\u201d and \u201dsoul.\u201d One advantage of using this type of structure for the neighborhood is that it enables us to examine various depths of the tree. Depth one includes only the direct neighbors (\u201dcardiac\u201d and \u201dsoul\u201d); depth two includes all neighbors two steps away in the graph: \u201ddisease,\u201d \u201dcoronary,\u201d \u201dpulmonary,\u201d \u201dcardiovascular,\u201d \u201dventricular,\u201d and \u201dfailure,\u201d which are all children to \u201dcardiac;\u201d depth three also includes the neighbors \u201dkidney,\u201d \u201dsevere,\u201d \u201dcomplications,\u201d and \u201ddiseases\u201d as children to \u201ddisease,\u201d \u201datrial\u201d and \u201darrhythmias\u201d as children to \u201dventricular,\u201d and the neighbors \u201drespiratory,\u201d \u201dlung,\u201d \u201dtumors,\u201d \u201daortic\u201d as children to \u201dpulmonary.\u201d This tree structure can be used to identify neighbors that are themselves polysemous (c.f. the critique mentioned in Section 3 of clustering-based approaches to wordsense induction that they may produce polysemous clusters (Tomuro et al., 2007)). One example is the neighbor \u201ddisease\u201d at depth two, which has six children that refer to different aspects of disease.\nThis tree-structure is thus quite useful in the context of word-sense induction, since the branching structure indicates different usages, and the depth factor enables us to calibrate the granularity of the induced word senses. If we only consider direct neighbors (i.e. depth one), and set k = V (i.e. we do an exhaustive nearest neighbor search), we will extract all terms that have a direct connection to the reference term. We refer to this neighborhood as the semantic horizon. At the most coarse level of analysis, this is the neighborhood that represents the main induced senses of a term. Tables 2 and 3 provide examples of 1000- RNG neighborhoods.\nThese examples demonstrate some interesting\nsimilarities and differences between the three models. First of all, there are some direct neighbors that are present in all three models: \u201dsuit\u201d has \u201dsuits\u201d and \u201dlawsuit\u201d as direct neighbors in all three models, \u201dheart\u201d has \u201dhearts,\u201d \u201dservice\u201d has \u201dservices,\u201d and \u201dabove\u201d has \u201dbelow\u201d. Plural forms are of course reasonable neighbors of their singular counterparts in a semantic model, but their usefulness for word-sense induction can perhaps be questioned. Taking \u201dsuits\u201d to indicate the garmentrelated sense of \u201dsuit,\u201d all three models produce both a garment-related and law-related sense. For \u201dorange,\u201d the skipgram model only represents the color sense, while the PMI and GloVe models also feature a fruit sense. For \u201dheart,\u201d all three models have a disease sense (represented by the neighbors \u201dcardiac\u201d in the PMI and GloVe models, and the neighbor \u201dcongestive\u201d in the skipgram model), and an organ sense (represented by the plural form \u201dhearts\u201d). \u201dService\u201d is a comparably vague term that has a number of different senses in the PMI and GloVe models, nut only one in the skipgram model. \u201dBad\u201d produces both a negativity sense and a German spa town-sense in all three models, both only the GloVe and skipgram models have a separate antonym sense (\u201dgood\u201d is not a direct\nneighbor in the PMI model). \u201dAbove\u201d has both the antonym and direct neighbors relating to measurements in all three models.\nGloVe produces the most branched neighborhoods, with a large number of direct neighbors, while the skipgram model produces the least branched neighborhoods with at most a couple of direct neighbors for each term. The PMI model is somewhere in between. One reason why GloVe produces such branching neighborhoods is that GloVe seems to capture not only semantic relations but also a significant amount of sequential relations. Many of the neighbors in the k-RNG for GloVe come from collocations: the k-RNG for \u201dsuit\u201d includes \u201dmobile\u201d and \u201dgundam,\u201d which come from the collocation \u201dmobile suit gundam\u201d that is an anime series, \u201dtrump\u201d that relates to \u201dtrump suits\u201d in card games, \u201dserenaders\u201d that refer to the retro string band \u201dcheap suit serenaders,\u201d and the very distant neighbor \u201dhev,\u201d which comes from \u201dhev suit\u201d that relates to the Half-life series of first person shooter games. For \u201dorange\u201d we find \u201dktype\u201d that comes from the collocation \u201dktype stars,\u201d which is another term for \u201dorange dwarfs\u201d, as well as the collocations \u201dorange peel\u201d, \u201dorange county\u201d, \u201dorange jumpsuit\u201d, \u201dcherry orange\u201d, and so on. The k-RNG for \u201dheart,\u201d \u201dservice,\u201d \u201dbad,\u201d and \u201dabove\u201d also feature a number of collocations for the GloVe model. There are also some examples of neighbors from sequential relations in the PMI model (e.g. \u201dcostly\u201d as neighbor to \u201dsuit\u201d from the collocation \u201dcostly suit,\u201d \u201dluck\u201d and \u201ddonnersbergkreis\u201d as neighbors to \u201dbad\u201d from the collocations \u201dbad luck\u201d and \u201dbad donnersbergkreis\u201d), but this tendency is not at all as pronounced as it is for the GloVe model.\nThe PMI and GloVe models produce the structurally most similar RNGs, with on average a handful of direct neighbors, of which some can be very distant. The skipgram model on the other hand produces very few direct neighbors. This led us to look further into the structural properties of neighborhoods in the skipgram model. An interesting observation \u2014 and possible complication \u2014 is that the neighborhoods in the skipgram model are highly asymmetric: the first neighbor of \u201dinformation\u201d is \u201dinformations\u201d, whereas \u201dinformation\u201d is only the 1829th neighbor of \u201dinformations.\u201d While such asymmetry occurs in all models, it seems much more prevalent in the skipgram model. Figure 6 demonstrates this: each point corresponds to a random word pair (a, b) with x corresponding to where b is in the ordered list of a\u2019s neighbor, and y to where a is in the ordered list of b\u2019s neighbors, or equivalently: x is the number of points within d(a, b) of a and y is the number of points within d(a, b) of b. This implies that the local densities vary much more in the skipgram\nmodel than in the others, which can complicate the choice of k in the k-RNG algorithm."}, {"heading": "5 Conclusions", "text": "In this paper we have discussed the question how to query semantic models, which is a question that has been long neglected in research on computational semantics. Nearest neighbor search (or kNN) is often treated as the only available option, which leads to misunderstandings regarding how semantic models represent and handle vagueness and polysemy. We have argued that the structure \u2014 or topology \u2014 of the local neighborhoods in semantic models carry useful semantic information regarding the different usages \u2014 or senses \u2013 of a term, and that such topological properties thus can be used to analyze polysemy and to perform\nword-sense induction. We have also argued that the topology of the local neighborhoods in semantic models can be used for selecting a relevant set of neighbors \u2014 a factor we have referred to as the semantic horizon.\nWe have introduced relative neighborhood graphs (RNG) as an alternative to standard kNN, and we have illustrated how k-RNG can be used as a tool for analyzing the topology of local neighborhoods in semantic models. We have exemplified relative neighborhoods in three different well-known semantic models; the standard PMI model, as well as the more recent GloVe and skipgram models. The examples provided in this paper demonstrate that k-RNG can be used for wordsense induction, but that such topological methods are more suitable to use for certain types of semantic models. The k-RNG for the PMI and GloVe models produce pleasant results, while the skipgram model, with its big local variations in density, produces less informative results. It\u2019s quite possible that the complete RNG overcomes these problems, but that does not seem a feasible solution.\nThis illustrates how k-RNG can be used as a tool to gain insight into the topological properties of different models. We have also observed that the GloVe model often produces neighbors that correspond to various collocations, which means that this model is not strictly a semantic representation, since it confounds substitutable and sequential relations. A more sophisticated tokenization, taking n-grams into account, might alleviate this. The standard PMI model is nowadays often overlooked in favor of more recent neural networkinspired models, but our results indicate that the PMI model has a number of comparatively attractive properties that are useful for linguistic applications such as word-sense induction."}], "references": [{"title": "David Mart\u0301\u0131nez", "author": ["Eneko Agirre"], "venue": "Oier L\u00f3pez de Lacalle, and Aitor Soroa.", "citeRegEx": "Agirre et al.2006", "shortCiteRegEx": null, "year": 2006}, {"title": "and Angela Y", "author": ["Sunil Arya", "David M. Mount", "Nathan S. Netanyahu", "Ruth Silverman"], "venue": "Wu.", "citeRegEx": "Arya et al.1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Georgiana Dinu", "author": ["Marco Baroni"], "venue": "and Germ\u00e1n Kruszewski.", "citeRegEx": "Baroni et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["Jon Louis Bentley"], "venue": "Commun. ACM,", "citeRegEx": "Bentley.,? \\Q1975\\E", "shortCiteRegEx": "Bentley.", "year": 1975}, {"title": "Chinese whispers: An efficient graph clustering algorithm and its application to natural language processing problems", "author": ["Chris Biemann"], "venue": "In Proceedings of the First Workshop on Graph Based Methods", "citeRegEx": "Biemann.,? \\Q2006\\E", "shortCiteRegEx": "Biemann.", "year": 2006}, {"title": "The anatomy of a large-scale hypertextual web search engine", "author": ["Brin", "Page1998] Sergey Brin", "Larry Page"], "venue": "In Seventh International World-Wide Web Conference (WWW", "citeRegEx": "Brin et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Brin et al\\.", "year": 1998}, {"title": "Bayesian word sense induction", "author": ["Brody", "Lapata2009] Samuel Brody", "Mirella Lapata"], "venue": "In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL", "citeRegEx": "Brody et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Brody et al\\.", "year": 2009}, {"title": "S\u00e9bastien Collette", "author": ["Jean Cardinal"], "venue": "and Stefan Langerman.", "citeRegEx": "Cardinal et al.2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Locally-scaled spectral clustering using empty region graphs", "author": ["Correa", "Lindstrom2012] Carlos D Correa", "Peter Lindstrom"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and", "citeRegEx": "Correa et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Correa et al\\.", "year": 2012}, {"title": "Measuring distributional similarity in context", "author": ["Dinu", "Lapata2010] Georgiana Dinu", "Mirella Lapata"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Dinu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dinu et al\\.", "year": 2010}, {"title": "Discovering corpusspecific word senses", "author": ["Dorow", "Widdows2003] Beate Dorow", "Dominic Widdows"], "venue": "In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics-Volume", "citeRegEx": "Dorow et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Dorow et al\\.", "year": 2003}, {"title": "Exemplar-based models for word meaning in context", "author": ["Erk", "Pad\u00f32010] Katrin Erk", "Sebastian Pad\u00f3"], "venue": "In Proceedings of the ACL 2010 Conference Short Papers, ACLShort", "citeRegEx": "Erk et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erk et al\\.", "year": 2010}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["Indyk", "Motwani1998] Piotr Indyk", "Rajeev Motwani"], "venue": "In Proceedings of the thirtieth annual ACM symposium on Theory of computing,", "citeRegEx": "Indyk et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Indyk et al\\.", "year": 1998}, {"title": "Hermit: Flexible clustering for the semeval-2 wsi task", "author": ["Jurgens", "Stevens2010] David Jurgens", "Keith Stevens"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Jurgens et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jurgens et al\\.", "year": 2010}, {"title": "Anders Holst", "author": ["Jussi Karlgren"], "venue": "and Magnus Sahlgren.", "citeRegEx": "Karlgren et al.2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Word sense induction using graphs of collocations", "author": ["Klapaftis", "Suresh Manandhar"], "venue": "In Proceedings of the 2008 Conference on ECAI", "citeRegEx": "Klapaftis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Klapaftis et al\\.", "year": 2008}, {"title": "Temperature in word space: Sense exploration of temperature expressions using word-space modelling", "author": ["Koptjevskaja Tamm", "Magnus Sahlgren"], "venue": null, "citeRegEx": "Tamm et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tamm et al\\.", "year": 2014}, {"title": "David Newman", "author": ["Jey Han Lau", "Paul Cook", "Diana McCarthy"], "venue": "and Timothy Baldwin.", "citeRegEx": "Lau et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "2014", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "Neural word embedding as implicit matrix factorization. In Advances in Neural Information Processing Systems, pages 2177\u2013", "citeRegEx": "Levy and Goldberg2014", "shortCiteRegEx": null, "year": 2185}, {"title": "Clustering and diversifying web search results with graph-based word sense induction", "author": ["Marco", "Navigli2013] Antonio Di Marco", "Roberto Navigli"], "venue": null, "citeRegEx": "Marco et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Marco et al\\.", "year": 2013}, {"title": "Corrado", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S"], "venue": "and Jeff Dean.", "citeRegEx": "Mikolov et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Discovering word senses from text", "author": ["Pantel", "Lin2002] Patrick Pantel", "Dekang Lin"], "venue": "In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Pantel et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pantel et al\\.", "year": 2002}, {"title": "Duluth-wsi: Senseclusters applied to the sense induction task of semeval-2", "author": ["Ted Pedersen"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Pedersen.,? \\Q2010\\E", "shortCiteRegEx": "Pedersen.", "year": 2010}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Naftali Tishby", "author": ["Fernando Pereira"], "venue": "and Lillian Lee.", "citeRegEx": "Pereira et al.1993", "shortCiteRegEx": null, "year": 1993}, {"title": "Word sense discrimination by clustering contexts in vector and similarity spaces", "author": ["Purandare", "Pedersen2004] Amruta Purandare", "Ted Pedersen"], "venue": "HLT-NAACL", "citeRegEx": "Purandare et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Purandare et al\\.", "year": 2004}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Reisinger", "Mooney2010] Joseph Reisinger", "Raymond J. Mooney"], "venue": "In Proceedings of the 11th Annual Conference of the North American Chapter of the Association", "citeRegEx": "Reisinger et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reisinger et al\\.", "year": 2010}, {"title": "Automatic word sense discrimination", "author": ["Hinrich Sch\u00fctze"], "venue": "Computational Linguistics,", "citeRegEx": "Sch\u00fctze.,? \\Q1998\\E", "shortCiteRegEx": "Sch\u00fctze.", "year": 1998}, {"title": "Probabilistic models of similarity in syntactic context", "author": ["\u00d3 S\u00e9aghdha", "Anna Korhonen"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "S\u00e9aghdha and Korhonen.,? \\Q2011\\E", "shortCiteRegEx": "S\u00e9aghdha and Korhonen.", "year": 2011}, {"title": "Kyoko Kanzaki", "author": ["Noriko Tomuro", "Steven L. Lytinen"], "venue": "and Hitoshi Isahara.", "citeRegEx": "Tomuro et al.2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Latent semantic word sense induction and disambiguation", "author": ["Van de Cruys", "Apidianaki2011] Tim Van de Cruys", "Marianna Apidianaki"], "venue": "In Proceedings of the 49th Annual Meeting of the Association", "citeRegEx": "Cruys et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cruys et al\\.", "year": 2011}, {"title": "Graph Clustering by Flow Simulation", "author": ["van Dongen2000] Stijn van Dongen"], "venue": "Ph.D. thesis,", "citeRegEx": "Dongen.,? \\Q2000\\E", "shortCiteRegEx": "Dongen.", "year": 2000}, {"title": "A fuzzy clustering approach to word sense discrimination", "author": ["Erik Velldal"], "venue": "In Proceedings of the 7th International conference on Terminology and Knowledge Engineering,", "citeRegEx": "Velldal.,? \\Q2005\\E", "shortCiteRegEx": "Velldal.", "year": 2005}, {"title": "Hyperlex: lexical cartography for information retrieval", "author": ["Jean V\u00e9ronis"], "venue": "Computer Speech & Language,", "citeRegEx": "V\u00e9ronis.,? \\Q2004\\E", "shortCiteRegEx": "V\u00e9ronis.", "year": 2004}, {"title": "Nonparametric bayesian word sense induction", "author": ["Yao", "Van Durme2011] Xuchen Yao", "Benjamin Van Durme"], "venue": "In Proceedings of TextGraphs-6: Graph-based Methods for Natural Language Processing,", "citeRegEx": "Yao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2011}], "referenceMentions": [], "year": 2015, "abstractText": "This paper is concerned with nearest neighbor search in distributional semantic models. A normal nearest neighbor search only returns a ranked list of neighbors, with no information about the structure or topology of the local neighborhood. This is a potentially serious shortcoming of the mode of querying a distributional semantic model, since a ranked list of neighbors may conflate several different senses. We argue that the topology of neighborhoods in semantic space provides important information about the different senses of terms, and that such topological structures can be used for word-sense induction. We also argue that the topology of the neighborhoods in semantic space can be used to determine the semantic horizon of a point, which we define as the set of neighbors that have a direct connection to the point. We introduce relative neighborhood graphs as method to uncover the topological properties of neighborhoods in semantic models. We also provide examples of relative neighborhood graphs for three well-known semantic models; the PMI model, the GloVe model, and the skipgram model.", "creator": "LaTeX with hyperref package"}}}