{"id": "1501.04282", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2015", "title": "Regularized maximum correntropy machine", "abstract": "In this paper we investigate the usage of regularized correntropy framework for learning of classifiers from noisy labels. The class label predictors learned by minimizing transitional loss functions are sensitive to the noisy and outlying labels of training samples, because the transitional loss functions are equally applied to all the samples. To solve this problem, we propose to learn the class label predictors by maximizing the correntropy between the predicted labels and the true labels of the training samples, under the regularized Maximum Correntropy Criteria (MCC) framework. Moreover, we regularize the predictor parameter to control the complexity of the predictor. The learning problem is formulated by an objective function considering the parameter regularization and MCC simultaneously. By optimizing the objective function alternately, we develop a novel predictor learning algorithm. The experiments on two chal- lenging pattern classification tasks show that it significantly outperforms the machines with transitional loss functions. However, a specific task could be performed by learning and applying regularized regularized regularized randomization to all of the samples.\n\n\n\n\n\n\nThe study consisted of a group of 20 trained individuals. Each group was trained on a 2 \u00d7 3 matrix with a fixed threshold. The weights used were computed using a series of supervised conditioning techniques (Laggard et al., 2010). The data were analyzed by analyzing the mean of the predicted prediction. The mean of the predictions was not significant, at all, but the mean predicted was not significant. We determined that the expected prediction on the predicted predictions was statistically significant. In order to find an optimal model for the prediction of the predicted prediction, we analyzed the mean of the predicted predictions in a nonlinear logarithmics manner, and then added the mean to the prediction to estimate the average predicted prediction. In addition, we tested a model in which the predicted predictions are normalized using a Gaussian logarithmics approach that considers that the prediction is more natural and simpler than expected, with no uncertainty. Finally, we determined the predicted predictions in a nonlinear logarithmics manner. We applied Laggard et al. as a model for an inference model. For example, the probability of prediction for the predicted prediction of the predicted prediction of the predicted prediction of the predicted prediction of the prediction of the prediction of the prediction of the predicted prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of the prediction of", "histories": [["v1", "Sun, 18 Jan 2015 11:46:30 GMT  (34kb)", "http://arxiv.org/abs/1501.04282v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jim jing-yan wang", "yunji wang", "bing-yi jing", "xin gao"], "accepted": false, "id": "1501.04282"}, "pdf": {"name": "1501.04282.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jing-Yan Wang", "Yunji Wang", "Bing-Yi Jing", "Xin Gao"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 1.\n04 28\n2v 1\n[ cs\n.L G\n] 1\nIn this paper we investigate the usage of regularized correntropy framework for learning of classifiers from noisy labels. The class label predictors learned by minimizing transitional loss functions are sensitive to the noisy and outlying labels of training samples, because the transitional loss functions are equally applied to all the samples. To solve this problem, we propose to learn the class label predictors by maximizing the correntropy between the predicted labels and the true labels of the training samples, under the regularized Maximum Correntropy Criteria (MCC) framework. Moreover, we regularize the predictor parameter to control the complexity of the predictor. The learning problem is formulated by an objective function considering the parameter regularization and MCC simultaneously. By optimizing the objective function alternately, we develop a novel predictor learning algorithm. The experiments on two challenging pattern classification tasks show that it significantly outperforms the machines with transitional loss functions. Keywords: Pattern classification, Label noise, Maximum Correntropy criteria, Regularization\n\u2217Correspondence should be addressed to Xin Gao. Tel: +966-12-8080323.\nPreprint submitted to Elsevier January 20, 2015"}, {"heading": "1. Introduction", "text": "The classification machine design has been a basic problem in the pattern recognition field. It tries to learn an effective predictor to map the feature vector of a sample to its class label [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. We study the supervised multi-class learning problem with L classes. Suppose we have a training set denoted as D = {(xi, yi)}, i = 1, \u00b7 \u00b7 \u00b7 , N , where xi = [xi1, \u00b7 \u00b7 \u00b7 , xiD ] \u22a4 \u2208 RD is the D dimensional feature vector of the i-th training sample, and yi \u2208 {1, \u00b7 \u00b7 \u00b7 , L} is the class label of i-th training sample. Moreover, we also denote the label indicator matrix as Y = [Yli] \u2208 R L\u00d7N , and Yli = 1 if yi = l, and \u22121 otherwise. We try to learn L class label predictors {f l\u03b8(x)}, l = 1, \u00b7 \u00b7 \u00b7 , L for the multi-class learning problem, where f l\u03b8(x) is the predictor for the l-th class and \u03b8 is its parameter. Given a sample xi, the output of the l-th predictor is denoted as f l \u03b8(xi), and we further denote the prediction result matrix as F\u03b8 = [F\u03b8li] \u2208 R L\u00d7N , and F\u03b8li = f l \u03b8(xi). To make the prediction as precise as possible, the target of predictor learning is to learn parameter \u03b8, so that the difference between true class labels of the training samples in Y and the prediction results in F\u03b8 could be minimized, while keeping the complexity of the predictor as low as possible. To measure how well the prediction results fit the true class label indicator, several loss functions L(F\u03b8, Y ) could be considered to compare the prediction results in F\u03b8 against the true class labels of the training samples in Y , such as the 0-1 loss function, the square loss function, the hinge loss function, and the logistic loss function. We summarize various loss functions in Table 1.\nThese loss functions introduced in Table 1 have been used widely in various learning problems. One common feature of these loss function is that a samplewise loss function is applied to each training sample equally and then the losses of all the samples are summed up to obtain the final overall loss. The samplewise loss functions are of exactly the same form with the same parameter (if they have parameters). The basic assumption behind this loss function is that the training samples are of the same importance. However, due to the limitation of the sampling technology and noises occurred during the sampling procedure,\nthere are some noisy and outlying samples in real-world applications. If we use the transitional loss functions listed in Table 1, the noisy and outlying training samples will play more important roles even than the good samples. Thus the predictors learned by minimizing the transitional loss functions are not robust to the noisy and outlying training samples, and could bring a high error rate when applied to the prediction of test samples.\nRecently, regularized correntropy framework has been proposed for robust pattern recognition problems [11, 12, 13, 14]. In [15], He et, al argued that the classical mean square error (MSE) criterion is sensitive to outliers, and intro-\nduced the correntropy to improve the robustness of the presentation. Moreover, the l1 regularization scheme is imposed on the correntropy to learn robust and sparse representations. Inspired by their work, we propose to use the regularized correntropy as a criterion to compare the prediction results and the true class labels. We use correntropy to compare the predicted labels and the true labels, instead of comparing the feature of test sample and its reconstruction from the training samples in He et, al\u2019s work. Moreover, an l2 norm regularization is introduced to control the complexity of the predictor. In this way, the predictor learned by maximizing the correntropy between prediction results and the true labels will be robust to the noisy and outlying training samples. The proposed classification Machine Maximizing the Regularized CorrEntropy, which is called RegMaxCEM, is supposed to be more insensitive to outlining samples than the ones with transitional loss functions. Yang et, al. [16] also proposed to use correntropy to compare predicted class labels and true labels. However, in their framework, the target is to learn the class labels of the unlabeled samples in a transductive semi-supervised manner, while we try to learn the parameters for the class label predictor in a supervised manner.\nThe rest of this paper is structured as follows: In Section 2, we propose the regularized maximum correntropy machine by constructing an objective function based on the maximum correntropy criterion (MCC) and developing an expectation \u2013 maximization (EM) based alternative algorithm for its optimization. In Section 3, the proposed methods are validated by conducting extensive experiments on two challenging pattern classification tasks. Finally, we give the conclusion in Section 4."}, {"heading": "2. Regularized Maximum Correntropy Machine", "text": "In this section we will introduce the classification machine maximizing the correntropy between the predicted class labels and the true class labels, while keeping the solution as simple as possible."}, {"heading": "2.1. Objective Function", "text": "To design the predictors f l\u03b8(x), we first represent the data sample x as x\u0303 in\nthe linear space and the kernel space as:\nx\u0303 =\n   x, (linear),\nK(\u00b7, x), (kernel), (1)\nwhere K(\u00b7, x) = [K(x1, x), \u00b7 \u00b7 \u00b7 ,K(xN , x)] \u22a4 \u2208 RN and K(xi, xj) is a kernel function between xi and xj . Then a linear predictor f l \u03b8(x) will be designed to predict whether the sample belongs to the l-th class as\nf l\u03b8(x) = w \u22a4 l x\u0303+ bl, l = 1, \u00b7 \u00b7 \u00b7 , L, (2)\nwhere \u03b8 = {(wl, bl)} L l=1 is the parameters of the predictors, wl \u2208 R D is the linear coefficient vector and bl \u2208 R is a bias term for the l-th predictor. The target of predictor designing is to find the optimal parameters to have the prediction result f l\u03b8(xi) of the i-th sample to fit its true class label indicator Yli as well as possible, while keeping the solution as simple as possible. To this end, we consider the following two problems simultaneously when designing the objective function:\nPrediction Accuracy Criterion based on Correntropy To consider the pre-\ndiction accuracy, we could learn the predictor parameters by minimizing a loss function listed in Table 1 as\nmin \u03b8 L(F\u03b8, Y ) (3)\nHowever, as we mentioned in Section 1, all these loss functions are applied to all the training samples equally, which is not robust to the noisy samples and outlying samples. To handle this problem, instead of minimizing a loss function to learn the predictor, we use the MCC [11] framework to learn the predictor by maximizing the correntropy between the predicted results and the true labels.\nRemark 1: In previous studies, it has been claimed that the MCC is insensitive to outliers. For example, in [11], it is claimed that \u201cthe maximum correntropy criterion, ... is much more insensitive to outliers.\u201d Based on this fact, we assume that the predictors developed based on MCC should also be insensitive to outliers.\nCorrentropy is a generalized similarity measure between two arbitrary random variablesA and B. However, the joint probability density function of A and B is usually unknown, and only a finite number of samples of them are available as {(ai, bi)} d i=1. It leads to the following sample estimator of correntropy:\nV (A,B) = 1\nd\nd\u2211\ni=1\ng\u03c3(ai \u2212 bi), (4)\nwhere g\u03c3(ai \u2212 bi) = exp ( \u2212 (ai\u2212bi) 2\n2\u03c32\n) is a Gaussian kernel function, and \u03c3\nis a kernel width parameter. For a learning system, MCC is defined as\nmax\u03d1 1\nd\nd\u2211\ni=1\ng\u03c3(ai \u2212 bi) (5)\nwhere \u03d1 is the parameter to be optimized in the criterion so that B is as correlated to A as possible.\nRemark 2: \u03d1 is usually a parameter to define B, but not the kernel function parameter \u03c3. In the learning system, we try to learn \u03d1 so that with the learned \u03d1, B is correlated to A. For example, in this case, A is the true class label matrix while B is the predicted class label matrix, and \u03d1 is the predictor parameter to define B.\nTo adapt the MCC framework to the predictor learning problem, we let A be the prediction result matrix F\u03b8 parameterized by \u03b8, and B be the true class label matrix Y , and we want to find the predictor parameter \u03b8 such that F\u03b8 becomes as correlated to Y as possible under the MCC framework. Then, the following correntropy-based predictor learning model will be obtained:\nmax \u03b8 V (F\u03b8, Y ),\nV (F\u03b8, Y ) = 1\nL\u00d7N\nL\u2211\nl=1\nN\u2211\ni=1\ng\u03c3(F\u03b8li \u2212 Yli) (6)\nPlease notice that in [11], MCC is used to measure the similarity between a test sample and its sparse linear representation of training samples, while in this work it is used to measure the similarity between the predicted class label and its true label. Also note that the dependence on \u03c3 in (6) and later (8), (11) relies on the dependence of the kernel function g\u03c3(\u00b7). In our experiments, the \u03c3 value is calculated as \u03c3 = 12\u00d7L\u00d7N \u2211L l=1 \u2211N i=1 \u2016F\u03b8li \u2212 Yli\u2016 2 2 following [11].\nPredictor Regularization To control the complexity of the l-th predictor in-\ndependently, we introduce the l2-based regularizer ||wl|| 2 to the coefficient vector wl of the l-th predictor. We assume that the predictors of different classes are equally important, and the following regularizer is introduced for multi-class learning problem:\nmin {wl}Ll=1\n1 L\nL\u2211\nl=1\n||wl|| 2 (7)\nRemark 3:The l2 norm is also used by support vector regression as a measure of model complexity. However, in support vector classification, this regularization term is either obtained by a \u201cmaximal margin\u201d regularization or obtained by a \u201cmaximal robustness\u201d regularization for certain type of feature noises [17]. Thus our l2 norm regularization term can also be regarded as a term to seek maximal margin or robustness.\nRemark 4: The l2-regularization is used in comparison to the l1-regularization in our model. Using l1-regularization we can seek the sparsity of the predictor coefficient vector, but it cannot guarantee the minimal model complexity, maximal margin or maximal robustness like the l2-regularization, thus we choose to use the l2-regularization. In the future, we will ex-\nplore the usage of l1-regularization to see if the prediction results can be improved.\nBy substituting \u03b8 = {wl, bl} L l=1, F\u03b8li = f l wl,bl (xi), and combining both the predictor regularization term in (7) and the prediction accuracy criterion term based on correntropy in (6), we obtain the following maximization problem for the maximum correntropy machine:\nmax {(wl,bl)}Ll=1\n1\nL\u00d7N\nL\u2211\nl=1\nN\u2211\ni=1\ng\u03c3(f l wl,bl (xi)\u2212 Yli)\u2212 \u03b1 1\nL\nL\u2211\nl=1\n||wl|| 2 (8)\nwhere \u03b1 is a tradeoff parameter. This optimization problem is based on correntropy using a Gaussian kernel function g\u03c3(x). It treats the prediction of individual training samples of individual classes differently. By this way, we can give more emphasis on samples with correctly predicted class labels, while those noisy or outlying training samples will have small contributions to the correntropy. In fact, when the regularizer term is introduced, (8) is a case of the regularized correntropy framework [15]."}, {"heading": "2.2. Optimization", "text": "Due to the nonlinear attribute of the kernel function g\u03c3(x) in the objective function in (8), direct optimization is difficult. An attribute of the kernel function g\u03c3(x) is that its derivative is also the same kernel function, and if we set its derivative to zero to seek the optimization of the objective, it is not easy to obtain a close form solution. However, according to the property of the convex conjugate function, we have:\nProposition 1 There exists a convex conjugate function \u03d5 of g\u03c3(x) such that\ng\u03c3(x) = maxp(p||x|| 2 \u2212 \u03d5(p)) (9)\nand for a fixed x, the maximum is reached at p = \u2212g\u03c3(x). This Proposition is taken from [18], which is further derived from the theory of convex conjugated functions. It is further discussed and used in many applications such as [11, 15, 19, 20].\nBy substituting (9) to (8), we have the augmented optimization problem in\nan enlarged parameter space\nmax {(wl,bl)}Ll=1,P\n1\nL\u00d7N\nL\u2211\nl=1\nN\u2211\ni=1\n[ Pli||f l wl,bl (xi)\u2212 Yli|| 2 \u2212 \u03d5(Pli) ] \u2212 \u03b1 1\nL\nL\u2211\nl=1\n||wl|| 2\n= 1\nL\u00d7N\nL\u2211\nl=1\nN\u2211\ni=1\n[ Pli||w \u22a4 l x\u0303i + bl \u2212 Yli|| 2 \u2212 \u03d5(Pli) ] \u2212 \u03b1 1\nL\nL\u2211\nl=1\n||wl|| 2,\n(10)\nwhere P = [Pli] \u2208 R N\u00d7L are the auxiliary variable matrix. To optimize (10), we adapt the EM framework to solve P and {(wl, bl)} L l=1 alternately."}, {"heading": "2.2.1. Expectation Step", "text": "In the expectation step of the EM algorithm, we calculated the auxiliary variable matrix P by fixing \u03b8. Obviously, according to Proposition 1, the maximum of (10) can be reached at\nP = \u2212g\u03c3(F\u03b8 \u2212 Y ),\nPli = \u2212g\u03c3(w \u22a4 l x\u0303i + bl \u2212 Yli).\n(11)\nNote that g\u03c3(X) is the element-wise Gaussian function. With fixed predictor parameters, the auxiliary variable \u2212Pli can be regarded as confidence of prediction result of the i-th training sample regarding to the l-th class. The better the l-th prediction result of the i-th sample fits the true label Yli, the larger the \u2212Pli will be.\nRemark 5: It is interesting to see if there is any relation between the auxiliary variables in P and the slack variables in SVM. Actually, both the auxiliary variables in P and the slack variables in SVM can be viewed as measures of classification losses. The slack variables in SVM are the upper boundaries of hinge losses of the training samples, while the auxiliary variables in P are a dissimilarity measure between the predicted labels and the true labels under the framework of the MCC rule, which is also a loss function. Meanwhile, the auxiliary variables in P also play a role of weights of different training samples as\nin (10), so that the learning can be robust to the noisy labels, but the auxiliary variables in SVM do not have such functions.\nRemark 6: In the expectation step, we actually solve an alternative optimization of solving P while fixing {(wl, bl)} L l=1. However, according to Proposition 1, the solution for this optimization problem is in the form of (11), which can be calculated directly and makes it an expectation step of the EM algorithm."}, {"heading": "2.2.2. Maximization Step", "text": "In the maximization step of the EM algorithm, we solve the predictor parameters {(wl, bl)} L l=1 while fixing P . The optimization problem in (10) turns to\nmax {(wl,bl)}Ll=1\n1\nL\u00d7N\nL\u2211\nl=1\nN\u2211\ni=1\n[ Pli||w \u22a4 l x\u0303i + bl \u2212 Yli|| 2 \u2212 \u03d5(Pli) ] \u2212 \u03b1 1\nL\nL\u2211\nl=1\n||wl|| 2.\n(12)\nNoticing Pli < 0 and removing terms irrelevant to wl and bl, the maximization problem in (12) can be reformulated as the following dual minimization problem:\nmin {(wl,bl)}Ll=1 O(w1, b1, \u00b7 \u00b7 \u00b7 , wL, bL),\nO(w1, b1, \u00b7 \u00b7 \u00b7 , wL, bL) = 1\nL\u00d7N\nL\u2211\nl=1\nN\u2211\ni=1\n(\u2212Pli||w \u22a4 l x\u0303i + bl \u2212 Yli|| 2) + \u03b1\nL\u2211\nl=1\n||wl|| 2.\n(13)\nTo simplify the notations, we define a vector ul = [ul1, \u00b7 \u00b7 \u00b7 , ulN ] \u22a4 \u2208 RN so that u2li = \u2212 1 N Pli. With ul, the objective function in (13) can be rewritten as\nO(w1, b1, \u00b7 \u00b7 \u00b7 , wL, bL) = 1\nL\nL\u2211\nl=1\n[ ||uli(w \u22a4 l x\u0303i + bl \u2212 Yli)|| 2 + \u03b1||wl|| 2 ]\n= 1\nL\nL\u2211\nl=1\n[ (w\u22a4l Xl + blu \u22a4 l \u2212 Y l)(w \u22a4 l Xl + blu \u22a4 l \u2212 Y l) \u22a4 + \u03b1w\u22a4l wl ] ,\n(14)\nwhereXl = [ul1x\u03031, \u00b7 \u00b7 \u00b7 , ulN x\u0303N ] \u2208 R D\u00d7N is the matrix containing all the training sample feature vectors weighted by ul, and Y l = [ul1Yl1, \u00b7 \u00b7 \u00b7 , ulNYlN ] \u2208 R N is\nthe l-th row of Y weighted by ul.\nObviously, the optimization problem in (13) is a linear least squares problem. Analytical solution for Problem (13) could be obtained easily. By setting the derivative of O(w1, b1, \u00b7 \u00b7 \u00b7 , wL, bL) with regard to bl to zero, we have\n\u2202O(w1, b1, \u00b7 \u00b7 \u00b7 , wL, bL)\n\u2202bl =\n1\n2L (w\u22a4l Xl + blu \u22a4 l \u2212 Y l)1N = 0\n\u21d2 bl = (Y l \u2212 w\n\u22a4 l Xl)1N\nu\u22a4l 1N = yl \u2212 w\n\u22a4 l xl,\n(15)\nwhere yl = Y l1N u\u22a4 l 1N and xl = Xl1N u\u22a4 l 1N . By substituting (15) to O(w1, b1, \u00b7 \u00b7 \u00b7 , wL, bL), we have\nO(w1, \u00b7 \u00b7 \u00b7 , wL) = 1\nL\nL\u2211\nl=1\n{ [w\u22a4l (Xl \u2212 xlu \u22a4 l )\u2212 (Y l \u2212 ylu \u22a4 l )][w \u22a4 l (X l \u2212 xlu \u22a4 l )\u2212 (Y l \u2212 ylu \u22a4 l )] \u22a4 + \u03b1w\u22a4l wl }\n(16)\nBy setting the derivative of O(w1, \u00b7 \u00b7 \u00b7 , wL) with regard to wl to zero, we have the optimal solution w\u2217l\n\u2202O(w1, \u00b7 \u00b7 \u00b7 , wL)\n\u2202wl =\n1\n2L [2(Xl \u2212 xlu\n\u22a4 l )(X l \u2212 xlu \u22a4 l ) \u22a4wl \u2212 2(Xl \u2212 xlu \u22a4 l )(Y l \u2212 ylu \u22a4 l ) \u22a4 + 2\u03b1wl] = 0\n\u21d2 w\u2217l = [(X l \u2212 xlu \u22a4 l )(X l \u2212 xlu \u22a4 l ) \u22a4 + \u03b1I]\u22121(X l \u2212 xlu \u22a4 l )(Y l \u2212 ylu \u22a4 l ) \u22a4,\n(17)\nwhere I is an D \u00d7D identity matrix. Then we substitute w\u2217l to (15), and we will have the optimal solution of b\u2217l ,\nb\u2217l = yl \u2212 w \u2217 l \u22a4 xl (18)"}, {"heading": "2.3. Algorithm", "text": "Algorithm 1 summarizes the predictor parameter learning procedure of Reg-\nMaxCEM. The E-step and the M-step will be repeated for T times.\nAlgorithm 1 RegMaxCEM Learning Algorithm.\nInput: Training set: D = {(xi, yi)} N i=1; Initialize the auxiliary variable matrix P 0 = \u22121L\u00d7N ; Represent each sample xi as x\u0303i as in (1); for t = 1, \u00b7 \u00b7 \u00b7 , T do\nMaximization-Step: Update the predictor parameters \u03b8t = {(wtl , b t l)} L l=1 as in (17) and (18) by fixing P t\u22121.\nExpectation-Step: Update the auxiliary variable matrix P t as in (11)\nby fixing the predictor parameters \u03b8t.\nend for Output: Predictor parameters \u03b8T = {(wTl , b T l )} L l=1."}, {"heading": "3. Experiments", "text": "In the experiments, we will evaluate the proposed classification method on two challenging pattern classification tasks \u2014 bacteria identification [21] and prediction of DNA-binding sites in proteins [22]."}, {"heading": "3.1. Experiment I: Bacteria Identification", "text": ""}, {"heading": "3.1.1. Dataset and Setup", "text": "High-precision identification of bacteria is quite important for the diagnosis of cancers and bacterial infections. Recently, ensemble aptamers (ENSaptamers), which utilizes a small set of nonspecific DNA sequences, has been proposed to provide an effective platform for the detection of bacteria [21]. ENSaptamers is a sensor array with seven sensors, and each sensor is designed using a DNA element.\nFor the experiment, we collected in total 66 samples of 6 different bacteria, including S.tyohimurium, S.flexneri, E.coli (CAU 0111), S.sonnei, S.typhi and E.coli (ATCC 25922). The number of samples for each bacteria varies from 9 to 13. Given an unknown bacteria sample with its fluorescence response patterns of ENSaptamer, the task is to identify which bacteria it is. To this end the seven fluorescence response patterns of ENSaptamer against the sample will be\nused to construct the 7-dimensional feature vector, and then the sample will be classified into one of the known bacteria using the RegMaxCEM predictor.\nTo conduct the experiment, we randomly split the entire dataset into two non-overlapping subsets \u2014 the training set and the test set. 33 samples were used as training sample in the training set, while the remaining 33 ones as test samples. The predictor parameters of RegMaxCEM were trained using the feature vectors and class labels of the training samples. Then the class labels of the test samples were predicted by the trained predictor, and compared to their true labels to calculate the classification accuracy. The random split process (training/test) was repeated for ten times and the accuracies over these ten splits were reported as classification performance."}, {"heading": "3.1.2. Results", "text": "We compare our proposed method against other loss function based classifiers, including square loss, hinge loss and logistic loss. 0-1 loss is the simplest loss function, but difficult to optimize, thus is not compared in the experiment. The boxplots of accuracies of different methods using both linear and kernel representations are illuminated in Figure 1. As shown in Figure 1, predictor produced by maximizing the correntropy yields improvements over other loss functions. Given the extremely small variation of classification accuracies over the ten splits, though the improvement of the accuracies are not large in absolute terms (around 0.1), it is consistent and significant. To verify whether the improvements are statistically significant, we performed the paired t-tests to the accuracies of the proposed method and other compared methods. The null hypothesis of the T-test is that the accuracies of the proposed method and the compared methods come from distributions with equal means. The P values of the t-tests are reported as measurements of statistically significance. A low P value implies that the difference between the proposed method and the compared methods are statistically significant. The P values are reported in Table 2. As we can see from the table, all the improvements archived by RegMaxCEM, for both linear representation and kernel representation, are statistically\nsignificant at the 0.05 significance level. This is not surprising: There are some noisy and outlying samples in the training set, which have been utilized by the methods with square loss, hinge loss or logistic loss as equally as other samples, thus they bring some bias to the predictor. However, the RegMaxCEM has the potential of filtering these samples, which can result in reliable learning of predictors in practice. It is also interesting to notice that the square loss, hinge loss and logistic loss have archived very similar classification accuracies. Though they used different loss functions, these loss functions are applied to the training samples equally."}, {"heading": "3.2. Experiment II: DNA-Binding Site Prediction", "text": "It is very important to predict the DNA-binding sites in proteins for understanding the molecular mechanisms of protein-DNA interaction. In this experiment, we will evaluate the proposed method for prediction of DNA-binding sites [22]."}, {"heading": "3.2.1. Dataset and Setup", "text": "The PDNA-62 database for DNA-binding site prediction has been used in this experiment. This database contains 8,163 sites in proteins in total. Among these sites, 1,215 of them are DNA-binding sites, while the remaining 6,948 sites are non-binding sites. We select 1,000 DNA-binding sites and 5,000 nonbinding sites from the PDNA-62 database to construct our database for the experiment. Given a candidate site, the goal of DNA-binding site prediction is to predict whether it is a DNA-binding site or not. To this end, the evolutionary information, solvent accessible surface area and the protein backbone structure features were extracted from the site, and then combined to construct the feature vector. The feature vector was further inputted into the classifier to distinguish DNA-binding sites from the non-binding sites [22].\nTo conduct the experiment, we employed the 10-fold cross validation. The database was split into 10 non-overlapping folds randomly, one of which was used as the test set, while the rest 9 of them were used as the training set. The procedure was repeated for 10 times so that each fold was used as the test set once.\nThe prediction performance was measured by the receiver operating characteristic (ROC) and recall-precision curves. The usage of ROC curve is mainly due to the imbalanced classes. The ROC curve is created by plotting false positive rate (FPR) against true positive rate (TPR), while recall-precision curve is obtained by ploting recall against precision. The FPR, TPR, recall and precision are defined as:\nFPR = FP\nFP + TN , TPR =\nTP\nTP + FN ,\nrecall = TP\nTP + FN , precision =\nTP\nTP + FP ,\n(19)\nwhere TP is the number of DNA-binding sites predicted correctly, FP is the number of non-binding sites predicted as DNA-binding sites wrongly, TN is the number of non-binding sites predicted correctly, while FN is the number of DNA-binding sites predicted as non-binding sites wrongly. For a better predictor, its ROC curve should be closer to the top left corner of the figure, while the recall-precision curve should be closer to the top right corner. Besides the two curves, area under the ROC curve (AUC) is also used as a single measurement of the prediction. A better predictor will have a larger AUC value."}, {"heading": "3.2.2. Results", "text": "The ROC and recall-precision curves of the proposed method and compared methods are reported in Figure 2. The predictors using linear and kernel representations are both illuminated. The AUC values of the ROC curves are reported in Table 3 as well. Overall the proposed methods clearly outperform the other methods significantly, although there is some variability in prediction performance over different representation types. From Table 3, we could see that the accuracy of the predictor is slightly increased by using the kernel representation instead of the linear representation. The regularized correntropy based predictors gives much better results than other methods on both representations. An interesting result from the DNA-binding prediction on this dataset is that the predictor with the hinge loss function outperforms other two methods."}, {"heading": "4. Conclusion and Future Work", "text": "In this paper, we present a novel regularized predictor learning model for multi-class pattern recognition problems. The predictor is learned by maximizing the correntropy between the prediction results and the true class labels. By applying the MCC rule, we could treat different training samples differently, so that the noisy and outlying training samples have less impact on the learning of predictors. Compared with the existing predictor models with various loss functions, it is robust to the noisy and outlying training samples. The experiments on bacteria identification and DNA-binding site prediction show that a good predictor may benefit much from a well designed loss function based on MCC. The proposed method outperformed the predictor with other popularly used loss functions. In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55]."}], "references": [{"title": "K-means clustering with bagging and mapreduce", "author": ["H. Li", "G.-Q. Wu", "X.-G. Hu", "J. Zhang", "L. Li", "X. Wu"], "venue": "in: System Sciences (HICSS), 2011 44th Hawaii International Conference on, IEEE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Noise-robust semi-supervised learning via fast sparse coding", "author": ["Z. Lu", "L. Wang"], "venue": "Pattern Recognition 48 (2) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Online group feature selection from feature streams", "author": ["H. Li", "X. Wu", "Z. Li", "W. Ding"], "venue": "in: Twenty-Seventh AAAI Conference on Artificial Intelligence, AAAI", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Spatial temporal pyramid matching using temporal sparse representation for human motion retrieval", "author": ["L. Zhou", "Z. Lu", "H. Leung", "L. Shang"], "venue": "The Visual Computer 30 (6-8) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning descriptive visual representation by semantic regularized matrix factorization", "author": ["Z. Lu", "Y. Peng"], "venue": "in: IJCAI, AAAI Press", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Region-based high-level semantics extraction with cedd", "author": ["Y. Zhou", "L. Li", "T. Zhao", "H. Zhang"], "venue": "in: Network Infrastructure and Digital Content, 2010 2nd IEEE International Conference on, IEEE", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Y", "author": ["Z. Lu"], "venue": "Peng, Unified constraint propagation on multi-view data., in: AAAI", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "l1-graph construction using structured sparsity", "author": ["G. Zhou", "Z. Lu", "Y. Peng"], "venue": "Neurocomputing 120 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Maximum Correntropy Criterion for Robust Face Recognition", "author": ["R. He", "W.-S. Zheng", "B.-G. Hu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 33 (8) ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Heterogeneous constraint propagation with constrained sparse representation", "author": ["Z. Lu", "Y. Peng"], "venue": "in: Proceedings of IEEE International Conference on Data Mining, IEEE Computer Society", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Exhaustive and efficient constraint propagation: A graphbased learning approach and its applications", "author": ["Z. Lu", "Y. Peng"], "venue": "International Journal of Computer Vision 103 (3) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "A Regularized Correntropy Framework for Robust Pattern Recognition", "author": ["R. He", "W.-S. Zheng", "B.-G. Hu", "X.-W. Kong"], "venue": "Vol. 23", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust Semi-supervised Learning Algorithm Based on Maximum Correntropy Criterion", "author": ["N.-H. Yang", "M.-M. Huang", "R. He", "X.-K. Wang"], "venue": "Journal of Software 23 ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Robustness and regularization of support vector machines", "author": ["H. Xu", "C. Caramanis", "S. Mannor"], "venue": "The Journal of Machine Learning Research 10 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust feature extraction via information theoretic learning", "author": ["X.-T. Yuan", "B.-G. Hu"], "venue": "in: Proceedings of the 26th Annual International Conference on Machine Learning, ACM", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Image annotation by semantic sparse recoding of visual content", "author": ["Z. Lu", "Y. Peng"], "venue": "in: Proceedings of the 20th ACM international conference on Multimedia, ACM", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning descriptive visual representation for image classification and annotation", "author": ["Z. Lu", "L. Wang"], "venue": "Pattern Recognition 48 (2) ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "A Graphene-Based Sensor Array for High-Precision and Adaptive 20  Target Identification with Ensemble Aptamers", "author": ["H. Pei", "J. Li", "M. Lv", "J. Wang", "J. Gao", "J. Lu", "Y. Li", "Q. Huang", "J. Hu", "C. Fan"], "venue": "Journal of the American Chemical Society 134 (33) ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Predna: accurate prediction of dna-binding sites in proteins by integrating sequence and geometric structure information", "author": ["T. Li", "Q. Li", "S. Liu", "G. Fan", "Y. Zuo", "Y. Peng"], "venue": "Bioinformatics 29 (6) ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse structure regularized ranking", "author": ["J.J.-Y. Wang", "Y. Sun", "X. Gao"], "venue": "Multimedia Tools and Applications ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Generalized relevance models for automatic image annotation", "author": ["Z. Lu", "H.H. Ip"], "venue": "in: Advances in Multimedia Information Processing-PCM 2009, Springer Berlin Heidelberg", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Non-informative hierarchical bayesian inference for non-negative matrix factorization", "author": ["Q. Sun", "J. Lu", "Y. Wu", "H. Qiao", "X. Huang", "F. Hu"], "venue": "Signal Processing 108 ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised image segmentation using an iterative entropy regularized likelihood learning algorithm", "author": ["Z. Lu"], "venue": "in: Advances in Neural Networks-ISNN 2006, Springer Berlin Heidelberg", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Unsupervised learning of finite mixtures using entropy regularization and its application to image segmentation", "author": ["Z. Lu", "Y. Peng", "J. Xiao"], "venue": "in: IEEE Conference on Computer Vision and Pattern Recognition, IEEE", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Spectral learning of latent semantics for action recognition", "author": ["Z. Lu", "Y. Peng", "H.H.-S. Ip"], "venue": "in: IEEE International Conference onComputer Vision (ICCV), IEEE", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Latent semantic learning with structured sparse representation for human action recognition", "author": ["Z. Lu", "Y. Peng"], "venue": "Pattern Recognition 46 (7) ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Y", "author": ["Z. Lu"], "venue": "Peng, Latent semantic learning by efficient sparse coding with hypergraph regularization., in: AAAI", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "A regularized minimum cross-entropy algorithm on mixture of experts for curve detection", "author": ["Z. Lu"], "venue": "in: International Conference on Neural Networks and Brain, Vol. 2, IEEE", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "Action recognition based on learnt motion semantic vocabulary", "author": ["Q. Zhao", "Z. Lu", "H.H. Ip"], "venue": "in: Advances in Multimedia Information Processing-PCM 2010, Springer Berlin Heidelberg", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Unsupervised image categorization using constrained entropy-regularized likelihood learning with pairwise constraints", "author": ["Z. Lu", "X. Lu", "Z. Ye"], "venue": "in: Advances in Neural Networks\u2013ISNN 2007, Springer Berlin Heidelberg", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Context awareness emergence for distributed binary pyroelectric sensors", "author": ["S. Qingquan", "H. Fei", "Q. Hao"], "venue": "in: Multisensor Fusion and Integration for Intelligent Systems (MFI), 2010 IEEE Conference on, IEEE", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "A fast raptor codes decoding strategy for real-time communication systems", "author": ["Y. Wu", "F. Hu", "Q. Sun", "K. Bao", "M. Guo"], "venue": "Network and Communication Technologies 2 (2) ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "A multi-agent-based intelligent sensor and actuator network design for smart house and home automation", "author": ["Q. Sun", "W. Yu", "N. Kochurov", "Q. Hao", "F. Hu"], "venue": "Journal of Sensor and Actuator Networks 2 (3) ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised multi-level nonnegative matrix factorization model: Binary data case", "author": ["Q. Sun", "P. Wu", "Y. Wu", "M. Guo", "J. Lu"], "venue": "Journal of Information Security 3 ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Neuro-disorder patient monitoring via gait sensor networks", "author": ["F. Hu", "Q. Sun", "Q. Hao"], "venue": "Intelligent Sensor Networks: The Integration of Sensor Networks, Signal Processing and Machine Learning ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Mobile targets scenario recognition via low-cost pyroelectric sensor network system: Towards an accurate context identification", "author": ["Q. Sun", "F. Hu", "Q. Hao"], "venue": "Proc. 2011 IEEE Students Tech. Sym ", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Spatial markov kernels for image categorization and annotation", "author": ["Z. Lu", "H.H. Ip"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics 41 (4) ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Contextual kernel and spectral methods for learning the semantics of images", "author": ["Z. Lu", "H.H. Ip", "Y. Peng"], "venue": "IEEE Transactions on Image Processing 20 (6) ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Gaussian mixture learning via robust competitive agglomeration", "author": ["Z. Lu", "Y. Peng", "H.H. Ip"], "venue": "Pattern Recognition Letters 31 (7) ", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "An iterative entropy regularized likelihood learning algorithm for cluster analysis with the number of clusters automatically detected", "author": ["Z. Lu"], "venue": "in: International Conference on Neural Networks and Brain, Vol. 2, IEEE", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2005}, {"title": "A publishing framework for digitally augmented paper documents: towards cross-media information integration", "author": ["X. Lu", "Z. Lu"], "venue": "in: Advances in Multimedia Information Processing-PCM 2006, Springer Berlin Heidelberg", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2006}, {"title": "A semi-supervised learning algorithm on gaussian mixture with automatic model selection", "author": ["Z. Lu", "Y. Peng"], "venue": "Neural Processing Letters 27 (1) ", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2008}, {"title": "Context-based multi-label image annotation", "author": ["Z. Lu", "H.H. Ip", "Q. He"], "venue": "in: Proceedings of the ACM International Conference on Image and Video Retrieval, ACM", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2009}, {"title": "Image categorization with spatial mismatch kernels", "author": ["Z. Lu", "H.H.-S. Ip"], "venue": "in: IEEE Conference on Computer Vision and Pattern Recognition, IEEE", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2009}, {"title": "Characterizing honeypot-captured cyber attacks: Statistical framework and case study", "author": ["Z. Zhan", "M. Xu", "S. Xu"], "venue": "Information Forensics and Security, IEEE Transactions on 8 (11) ", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2013}, {"title": "Adaptive epidemic dynamics in networks: Thresholds and control", "author": ["S. Xu", "W. Lu", "L. Xu", "Z. Zhan"], "venue": "ACM Trans. Auton. Adapt. Syst. 8 (4) ", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "Cross-layer detection of malicious websites", "author": ["L. Xu", "Z. Zhan", "S. Xu", "K. Ye"], "venue": "in: CODASPY", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "An evasion and Counter-Evasion study in malicious websites detection", "author": ["L. Xu", "Z. Zhan", "S. Xu", "K. Ye"], "venue": "in: 2014 IEEE Conference on Communications and Network Security (CNS) ", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "A stochastic model of multivirus dynamics", "author": ["S. Xu", "W. Lu", "Z. Zhan"], "venue": "Dependable and Secure Computing, IEEE Transactions on 9 (1) ", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2012}, {"title": "Trustworthy information: concepts and mechanisms", "author": ["S. Xu", "H. Qian", "F. Wang", "Z. Zhan", "E. Bertino", "R. Sandhu"], "venue": "in: Web-Age Information Management, Springer", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "It tries to learn an effective predictor to map the feature vector of a sample to its class label [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 98, "endOffset": 129}, {"referenceID": 1, "context": "It tries to learn an effective predictor to map the feature vector of a sample to its class label [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 98, "endOffset": 129}, {"referenceID": 2, "context": "It tries to learn an effective predictor to map the feature vector of a sample to its class label [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 98, "endOffset": 129}, {"referenceID": 3, "context": "It tries to learn an effective predictor to map the feature vector of a sample to its class label [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 98, "endOffset": 129}, {"referenceID": 4, "context": "It tries to learn an effective predictor to map the feature vector of a sample to its class label [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 98, "endOffset": 129}, {"referenceID": 5, "context": "It tries to learn an effective predictor to map the feature vector of a sample to its class label [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 98, "endOffset": 129}, {"referenceID": 6, "context": "It tries to learn an effective predictor to map the feature vector of a sample to its class label [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 98, "endOffset": 129}, {"referenceID": 7, "context": "It tries to learn an effective predictor to map the feature vector of a sample to its class label [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 98, "endOffset": 129}, {"referenceID": 8, "context": "Recently, regularized correntropy framework has been proposed for robust pattern recognition problems [11, 12, 13, 14].", "startOffset": 102, "endOffset": 118}, {"referenceID": 9, "context": "Recently, regularized correntropy framework has been proposed for robust pattern recognition problems [11, 12, 13, 14].", "startOffset": 102, "endOffset": 118}, {"referenceID": 10, "context": "Recently, regularized correntropy framework has been proposed for robust pattern recognition problems [11, 12, 13, 14].", "startOffset": 102, "endOffset": 118}, {"referenceID": 11, "context": "In [15], He et, al argued that the classical mean square error (MSE) criterion is sensitive to outliers, and intro-", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "[16] also proposed to use correntropy to compare predicted class labels and true labels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "To handle this problem, instead of minimizing a loss function to learn the predictor, we use the MCC [11] framework to learn the predictor by maximizing the correntropy between the predicted results and the true labels.", "startOffset": 101, "endOffset": 105}, {"referenceID": 8, "context": "For example, in [11], it is claimed that \u201cthe maximum correntropy criterion, .", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": "Please notice that in [11], MCC is used to measure the similarity between a test sample and its sparse linear representation of training samples, while in this work it is used to measure the similarity between the predicted class label and its true label.", "startOffset": 22, "endOffset": 26}, {"referenceID": 8, "context": "In our experiments, the \u03c3 value is calculated as \u03c3 = 1 2\u00d7L\u00d7N \u2211L l=1 \u2211N i=1 \u2016F\u03b8li \u2212 Yli\u2016 2 2 following [11].", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "However, in support vector classification, this regularization term is either obtained by a \u201cmaximal margin\u201d regularization or obtained by a \u201cmaximal robustness\u201d regularization for certain type of feature noises [17].", "startOffset": 212, "endOffset": 216}, {"referenceID": 11, "context": "In fact, when the regularizer term is introduced, (8) is a case of the regularized correntropy framework [15].", "startOffset": 105, "endOffset": 109}, {"referenceID": 14, "context": "This Proposition is taken from [18], which is further derived from the theory of convex conjugated functions.", "startOffset": 31, "endOffset": 35}, {"referenceID": 8, "context": "It is further discussed and used in many applications such as [11, 15, 19, 20].", "startOffset": 62, "endOffset": 78}, {"referenceID": 11, "context": "It is further discussed and used in many applications such as [11, 15, 19, 20].", "startOffset": 62, "endOffset": 78}, {"referenceID": 15, "context": "It is further discussed and used in many applications such as [11, 15, 19, 20].", "startOffset": 62, "endOffset": 78}, {"referenceID": 16, "context": "It is further discussed and used in many applications such as [11, 15, 19, 20].", "startOffset": 62, "endOffset": 78}, {"referenceID": 17, "context": "In the experiments, we will evaluate the proposed classification method on two challenging pattern classification tasks \u2014 bacteria identification [21] and prediction of DNA-binding sites in proteins [22].", "startOffset": 146, "endOffset": 150}, {"referenceID": 18, "context": "In the experiments, we will evaluate the proposed classification method on two challenging pattern classification tasks \u2014 bacteria identification [21] and prediction of DNA-binding sites in proteins [22].", "startOffset": 199, "endOffset": 203}, {"referenceID": 17, "context": "Recently, ensemble aptamers (ENSaptamers), which utilizes a small set of nonspecific DNA sequences, has been proposed to provide an effective platform for the detection of bacteria [21].", "startOffset": 181, "endOffset": 185}, {"referenceID": 18, "context": "In this experiment, we will evaluate the proposed method for prediction of DNA-binding sites [22].", "startOffset": 93, "endOffset": 97}, {"referenceID": 18, "context": "The feature vector was further inputted into the classifier to distinguish DNA-binding sites from the non-binding sites [22].", "startOffset": 120, "endOffset": 124}, {"referenceID": 19, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 133, "endOffset": 141}, {"referenceID": 20, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 133, "endOffset": 141}, {"referenceID": 21, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 163, "endOffset": 199}, {"referenceID": 22, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 163, "endOffset": 199}, {"referenceID": 23, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 163, "endOffset": 199}, {"referenceID": 24, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 163, "endOffset": 199}, {"referenceID": 25, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 163, "endOffset": 199}, {"referenceID": 26, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 163, "endOffset": 199}, {"referenceID": 27, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 163, "endOffset": 199}, {"referenceID": 28, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 163, "endOffset": 199}, {"referenceID": 29, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 163, "endOffset": 199}, {"referenceID": 30, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 315, "endOffset": 343}, {"referenceID": 31, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 315, "endOffset": 343}, {"referenceID": 32, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 315, "endOffset": 343}, {"referenceID": 33, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 315, "endOffset": 343}, {"referenceID": 34, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 315, "endOffset": 343}, {"referenceID": 35, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 315, "endOffset": 343}, {"referenceID": 36, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 361, "endOffset": 397}, {"referenceID": 37, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 361, "endOffset": 397}, {"referenceID": 38, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 361, "endOffset": 397}, {"referenceID": 39, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 361, "endOffset": 397}, {"referenceID": 40, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 361, "endOffset": 397}, {"referenceID": 41, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 361, "endOffset": 397}, {"referenceID": 41, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 361, "endOffset": 397}, {"referenceID": 42, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 361, "endOffset": 397}, {"referenceID": 43, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 361, "endOffset": 397}, {"referenceID": 44, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 429, "endOffset": 457}, {"referenceID": 45, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 429, "endOffset": 457}, {"referenceID": 46, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 429, "endOffset": 457}, {"referenceID": 47, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 429, "endOffset": 457}, {"referenceID": 48, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 429, "endOffset": 457}, {"referenceID": 49, "context": "In the future, we will investigate if the regularized maximum correntropy framework can be used to regularize ranking score learning [23, 24], data representation [25, 26, 27, 28, 29, 30, 31, 32, 33] Moreover, we also plan to extend the proposed regularized correntropy based classifier for wireless sensor network [34, 35, 36, 37, 38, 39, 40], computer vision [41, 42, 43, 44, 45, 46, 46, 47, 48], and computer network security [49, 50, 51, 52, 53, 54, 55].", "startOffset": 429, "endOffset": 457}], "year": 2015, "abstractText": "In this paper we investigate the usage of regularized correntropy framework for learning of classifiers from noisy labels. The class label predictors learned by minimizing transitional loss functions are sensitive to the noisy and outlying labels of training samples, because the transitional loss functions are equally applied to all the samples. To solve this problem, we propose to learn the class label predictors by maximizing the correntropy between the predicted labels and the true labels of the training samples, under the regularized Maximum Correntropy Criteria (MCC) framework. Moreover, we regularize the predictor parameter to control the complexity of the predictor. The learning problem is formulated by an objective function considering the parameter regularization and MCC simultaneously. By optimizing the objective function alternately, we develop a novel predictor learning algorithm. The experiments on two challenging pattern classification tasks show that it significantly outperforms the machines with transitional loss functions.", "creator": "LaTeX with hyperref package"}}}