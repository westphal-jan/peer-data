{"id": "1501.04684", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jan-2015", "title": "Slice Sampling for Probabilistic Programming", "abstract": "We introduce the first, general purpose, slice sampling inference engine for probabilistic programs. This engine is released as part of StocPy, a new Turing-Complete probabilistic programming language, available as a Python library. We present a transdimensional generalisation of slice sampling which is necessary for the inference engine to work on traces with different numbers of random variables. We show that StocPy compares favourably to other PPLs in terms of flexibility and usability, and that slice sampling can outperform previously introduced inference methods. Our experiments include a logistic regression, HMM, and Bayesian Neural Net.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 20 Jan 2015 00:24:14 GMT  (933kb,D)", "http://arxiv.org/abs/1501.04684v1", "11 pages"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.AI cs.PL", "authors": ["razvan ranca", "zoubin ghahramani"], "accepted": false, "id": "1501.04684"}, "pdf": {"name": "1501.04684.pdf", "metadata": {"source": "CRF", "title": "Slice Sampling for Probabilistic Programming", "authors": ["Razvan Ranca", "Zoubin Ghahramani"], "emails": [], "sections": [{"heading": null, "text": "We introduce the first, general purpose, slice sampling inference engine for probabilistic programs. This engine is released as part of StocPy, a new Turing-Complete probabilistic programming language, available as a Python library. We present a transdimensional generalisation of slice sampling which is necessary for the inference engine to work on traces with different numbers of random variables. We show that StocPy compares favourably to other PPLs in terms of flexibility and usability, and that slice sampling can outperform previously introduced inference methods. Our experiments include a logistic regression, HMM, and Bayesian Neural Net."}, {"heading": "1 Introduction", "text": "There has been a recent surge of interest in probabilistic programming, as demonstrated by the continued development of new languages (eg: Wood et al. [2014], Goodman et al. [2008], Lunn et al. [2009], Milch et al. [2007]) and by the recent DARPA1 program encouraging further research in this direction. The increase in activity is justified by the promise of probabilistic programming, namely that of disentangling model specification from inference. This abstraction would open up probabilistic modelling to a much larger audience, including domain experts, while also making model design cheaper, faster and clearer.\nTwo of the major challenges lying ahead for Probabilistic Programming Languages (PPLs), before their full promise can be achieved, are those of usability\n1Probabilistic Programming for Advancing Machine Learning: http://ppaml.galois.com/wiki/\nPreliminary work. Under review by AISTATS 2014. Do not distribute.\nand inference performance (Gordon [2013]). In this paper we address usability by presenting a new PPL, StocPy, available online as a Python library (Ranca [2014]). We also take a step towards better inference performance by implementing a novel, slice sampling, inference engine in StocPy.\nStocPy is based on the PPL design presented by Wingate et al. [2011], but is written purely in Python, and works on model definitions written in the same language. This enables us to take full advantage of Python\u2019s excellent prototyping abilities and fast development cycles, and thus allows us to specify models in very flexible ways. For instance models containing complex control flow and elements such as stochastic (mutual) recursion are easily representable. Additionally, the pure Python implementation means StocPy itself provides a good base for further experiments into PPLs, such as defining novel naming techniques for stochastic variables, looking at program recompilation to improve inference performance, or testing out new inference engines. We illustrate the benefits StocPy offers by discussing some of the language\u2019s features and contrasting the definitions of several models in StocPy against those in Anglican (Wood et al. [2014]).\nWe believe that ease of prototyping and implementing novel inference engines is crucial for the future success of PPLs. As decades of compiler design have shown , a \u201cmagic bullet\u201d inference technique is unlikely to be found (eg: [Appel, 1998, p. 401]). This is re-inforced by the fact that PPLs can allow for a great deal of flexibility regarding the types of models and queries posed (eg: propositional logic can be represented by exposing delta functions, probabilistic programs can be conditioned on arbitrary boolean expressions). Such flexibility means a general PPL inference engine has a daunting task ahead, which includes subtasks such as handling boolean satisfiability problems. Therefore, it seems vital to develop a toolbox of inference techniques which work well on different types of modelling tasks. This high-level reasoning supports not only the development of StocPy itself, but also of the slice sampling inference engine, which outperforms previous inference techniques on certain classes of models. ar X\niv :1\n50 1.\n04 68\n4v 1\n[ cs\n.A I]\n2 0\nJa n\n20 15"}, {"heading": "2 StocPy", "text": "We implement a novel PPL, StocPy, which is made available online (Ranca [2014]). StocPy has both a Metropolis-Hastings inference engine (implemented in the style presented by Wingate et al. [2011]) and a Slice sampling inference engine which we further discuss in Section 3. Weighted combinations of the two inference strategies can also be used."}, {"heading": "2.1 Why make a Python PPL?", "text": "One of the main promises of PPLs is to allow quick and cheap development of probabilistic models, including by domain experts who may not be very comfortable with machine learning, or even with programming. The popularity of PPLs such as BUGS is partly due to their simplicity. On the other hand, lisp-based PPLs such as Goodman et al. [2008] offer greater flexibility but at the price of usability. StocPy is able to offer the same power as the lisp-based PPLs while also allowing the user to work in Python, a language both very friendly to beginners and extremely popular for prototypying.\nAdditionally, using Python means we can make StocPy easily available as a library, and that we can make use of Python\u2019s flexible programming style when defining models (eg: stochastic mutually recursive functions, use of globals). A user need only provide a function entry point to their model, which can then call any other functions, make use of any globals defined in the file, and use any common2 Python constructs as needed.\nFinally, using Python offers both the user and the language designer a rich library support. For instance, as language designers, we are able to provide support for all 96 stochastic primitives defined in the \u201cscipy.stats\u201d library3 by defining a single wrapper function. With-\n2More exotic constructs, such as the use of list comprehensions in place of for loops, are not currently supported by the automatic variable naming. However they can be used, if the stochastic primitives involved are manually named by the user.\n3scipy.stats primitives: http://docs.scipy.org/doc/scipy/reference/stats.html\nout this library, all 96 primitives would have had to be specified, in a computationally efficient way, in the language itself, which would have required a significant amount of effort. The library support also reduces the barrier of entry for all manner of PPL research. For instance, Python\u2019s excellent network and graph libraries (eg: Hagberg et al. [2008]) could be used to define a novel naming convention for stochastic primitives, which takes the program\u2019s control flow into account. Such a naming convention is required by the framework of Wingate et al. [2011] and could be an improvement over the simpler version presented in that paper. In general, StocPy tries to accomodate for such avenues of research (eg: StocPy\u2019s automatic variable naming can be turned off by setting a single flag)."}, {"heading": "2.2 StocPy Programming Style", "text": "As mentioned above, a user defined model can make use of most common Python features, as long as the model is confined to one file. The interaction with StocPy is meant to be lightweight. Essentially, the user should perform all stochastic operations via StocPy, rather than another library of random numbers. That is to say, rather than calling scipy.stats.norm(0,1), the user could call either the StocPy stochastic primitive directly: StocPy.normal(0,1)4, or use the StocPy wrapper over scipy.stats: StocPy.stocPrim(\"normal\", (0,1)). By defining stochastic primitives through StocPy, the user will define a valid generative model. Conditioning is done within the same function call that defines a variable, by specifying the \u201ccond\u201d parameter (eg: StocPy.normal(0,1,cond=True)). Finally, the variables or expressions we\u2019d like to observe can be specified in two ways, either directly, in the variable definitions, via the \u201cobs\u201d parameter (eg: stocPy.normal(0,1,obs=True)), or via a bespoke observe statement (eg: StocPy.observe(expression, \"expName\")). The later is more flexible, allowing us to observe the result of arbitrary expressions aggregated, via their name, in arbitrary ways. Once the model is defined, inference can be done by calling one of several inference functions, and passing the entry point to the model as a parameter. A minimal (but complete) model definition, which tries to infer the mean of a gaussian given a single observation, is shown in Figure 1. This model is revisited in Figures 10 and 9 where we see its true posterior and abstract model specification respectively.\nIn Figure 1, line 1 imports our library. Line 3 de-\n4these are only availalble for a few distributions. See the StocPy library for details\nfines the function that is the entry point to our model. Line 4 specifies the prior on our mean (also a gaussian) and tells StocPy that we want to observe the values this variable takes. Line 5 conditions a normally distributed random variable with our sampled mean and variance 1, on our only observation (5). Line 7 performs inference on our model, extracting 10,000 samples via slice sampling. Finaly, line 8 uses a StocPy utility function to plot the resulting distribution, which is shown in Figure 4.\nIn Figures 2, 3 we present more examples of models expressed in StocPy and in Anglican. The models are 2 of those presented by Wood et al. [2014] (more models are included in the supplementary material). The Anglican specifications are taken either directly from the paper or from the Anglican website5. We can see that in the case of the more complex models, we are able to provide a more succint representation in StocPy than in Anglican."}, {"heading": "3 Slice Sampling Inference Engine", "text": ""}, {"heading": "3.1 Advantages of Slice Sampling", "text": "Slice sampling (Neal [2003]) is a Markov Chain Monte Carlo algorithm that can extract samples from a distribution P (x) given that we have a function P \u2217(x), which we can evaluate, and which respects P \u2217(x) = ZP (x), for some constant Z > 0. The idea behind Slice sampling is that, by using an auxiliary height variable u, we can sample uniformly from the region under the graph of the density function of P (x).\nIn order to get an intuition of the algorithm we can consider the one dimensional case. Here we can view the algorithm as a method of transitioning from a point (x, u) which lies under the curve P \u2217(x) to another point (x\u2032, u\u2032) lying under the same curve. The addition of the auxiliary variable u means that we need only sample uniformly from the area under the curve P \u2217(x).\nA basic Slice sampling algorithm can be described as:\nAlgorithm 1 Slice Sampling\n1: Pick initial x1 such that P \u2217(x1) > 0 2: Pick number of samples to extract N 3: for t = 1\u2192 N do 4: Sample a height u uniformly from [0, P \u2217(xt)] 5: Define a segment [xl, xr] at height u 6: Pick xt+1 \u2208 [xl, xr] such that P \u2217(xt+1) > u\nThe key to Slice sampling\u2019s efficiency is the fact that\n5Anglican model repository: http://www.robots.ox.ac.uk/\u223cfwood/anglican/examples/\nthe operations in lines 5 and 6 can be performed with exponential stepping out and shrinking methods. One possible implementation of these operations is shown in the supplementary material. In this way, slice sampling corrects one of the main disadvantages of MH, namely that it has a fixed step size. In MH, picking a wrong step size can significantly hamper progress either by unnecesarily slowing down the random walk\u2019s progress or by resulting in a large proportion of rejected sampled. Slice sampling, however, adjusts an inadequate step size of size S with a cost that is only logarithmic in the size of S (MacKay [2003]).\nWhile some methods to mitigate MH\u2019s fixed step size exist, such as performing pre-run tests to determine a good step-size, these adjustment are not possible in the variant of MH commonly used in PPLs. We refer to the MH proposal described in Wingate et al. [2011], which is the same as the \u201cRDB\u201d benchmark used in Wood et al. [2014]. In this MH implementation, whenever we wish to resample a variable, we run the model until we encounter the variable and then sample it from its prior conditioned on the variables we have seen so far in the current run. Therefore no fine-tuning of the proposal distribution is possible.\nTo get an idea of what can be gained with slice sampling over simple, single-site, MH we look at two examples, one favourable to MH and one unfavourable. In this way we can get an idea of the expected \u201cbest\u201d and \u201cworst\u201d case performances of the two. We choose to look at a simple gaussian mean inference problem, and place different priors over the mean. The best case for MH is a prior that is identical to the posterior, which means MH as described above (i.e. RDB) is already sampling from the correct distribution. Conversely, the worst case for MH is an extremely uninformative prior, such that most samples will end up being rejected (in our example, the prior is uniform between 0 and 10,000 while the posterior is very close to a normal with mean 2 and variance 0.032, more details in the supplementary material).\nThe results are presented in Figures 5, 6. Here we report the Kolmogorov Smirnov (KS) distance between the analytically derived posterior and a running average of the inferred posterior. The x axis shows the number of times the model has been interpreted (and had it\u2019s trace likelihood computed). We run the tests multiple times starting from different random seeds and report both the median (solid line) and the 25% and 75% percentiles (dashed lines) over runs. In this experiment we can see that the potential upside of slice sampling overshadows the downside. In the worst case scenario for slice sampling, we have an inference problem where our prior is equal to the posterior. That is we already know the correct answer before seeing any\ndata. In this case the extra overhead incurred by Slice sampling slows it down roughly by a factor of 2 when compared to MH (Figure 5). In the second case, however, we have a very uninformative prior and here slice sampling significantly outperforms MH (Figure 6). In fact, the difference between the 2 algorithms will only get more pronounced as the prior gets more uninformative. That is, examples can be created, where Slice sampling is arbitrarily faster than MH. However, we must nore that these examples are of very simple, one dimensional, unimodal models. The generalization of the insights gained from these examples to more complex models is non-trivial. Even so, we have shown a category of models where slice sampling performs substantially better than MH. Comparisons on more complex models are carried out in Section 4."}, {"heading": "3.2 Inference engine construction", "text": "Our engine follows the basic slice sampling algorithm presented in Algorithm 1. As presented in Wingate et al. [2011], we consider a sample x to be a program execution trace and P \u2217(x) to be the likelihood of all stochastic variables sampled in trace x.\nThe bottleneck in the inference engines is the trace likelihood calculation. Metropolis calculates this value exactly once per sample whereas Slice sampling needs to calculate it at least 3 times (one each for xl, xr and the next x), and potentially many more times. For this reason, StocPy provides the possibility to do inference until a certain number of trace log likelihood calculations have been performed, which allows us to compare Metropolis and slice sampling directly and fairly. Further, all experiments comparing slice with Metropolis ran the algorithms for the same length of\ntime. We however chose to use trace likelihoods rather than seconds on the x-axis, which implicitely shows that the two engines average the same number of tracelikelihood calculation per unit of time.\nThe main non-trivial aspect, and novel contribution, of the inference engine construction is handling transdimensional models. We discuss this below."}, {"heading": "3.2.1 Trans-dimensional models", "text": "In order to understand the additional complications that trans-dimensional models present for inference engines we look at a simple example taken from Wood et al. [2014], the Branching model. This model has 2 variables whose values determine the distribution of a 3rd variable which we condition on. This model is trans-dimensional since on different traces either 1 or both of the 2 variables will be sampled.\nRe-writing the model so that both variables are always sampled, even if one of them is unused, leaves the posterior invariant. Therefore one method to correctly perform inference in a trans-dimensional model is to always sample all variables that might be used in a trace. This approach will however be extremely inefficient in large models and is not a viable general solution. In Figure 7 we use this trick to see what the space of possible trace likelihoods looks like, and what the true posterior is. Here pois1 and pois2 refer to the Poisson variables sampled in lines 2 and 3, respectively, of the StocPy model shown in Figure 2. Integrating out the pois2 variable from the above trace likelihood space results in the correct posterior distribution.\nThe issue with trans-dimensional jumps comes from the fact that a naive inference algorithm will not sam-\nple the second poisson when it is not necessary, but will still think that the trace likelihoods of runs with different numbers of sampled variables are comparable. In doing so, the inference engine will be pretending to be sampling from a 2D trace likelihood even when it really is 1D. The space of likelihoods implied by the naive inference engine, and the posterior it would obtain by integrating out pois2, is shown in Figure 8. We now describe how to correct the slice sampling inference engine to handle trans-dimensional models."}, {"heading": "3.3 Transdimensional Slice Sampling", "text": "To understand the trans-dimensional corrections we can place them in the framework of Reversible Jump Markov Chain Monte Carlo (RJMCMC, Green and Hastie [2009]). Informally, we can think of slice sampling as a form of RJMCMC in which we carefully choose the next sample so that the acceptance probability will always be 1. We include a short explanation of the RJMCMC notation in the Appendix.\nIn order to place slice sampling in this framework, we can think of a program execution trace as a state x. The choice of move we make is equivalent to choosing which random variable we wish to resample. Therefore, if there are |D| random variables present in the current trace, and we pick one to sample uniformly, then jm(x) = 1/|D|. Once the variable is chosen, we can define a deterministic function hm which produces\na new value for variable m by following the slice sampling specification presented in lines 4-6 of Algorithm 1. The randomness that hm must take as input is u \u223c gm, where u is composed of r random numbers and gm is the joint distribution of these r numbers. The probability of moving from state x to state x\u2032 is then \u03c0(x)gm(u)/|D|, and the probability of going back from x\u2032 to x is \u03c0(x\u2032)g\u2032m(u\n\u2032)/|D\u2032|. Intuitively \u03c0(x) = \u03c0(x\u2032) since slice sampling samples uniformly under its target distribution. Transdimensionality means that the number of variables sampled in traces will be different and therefore that D\u2032 will be different from D and that the dimensionality of u and u\u2032 (r and r\u2032 respectively) will also be different.\nThe correction we aplly to account for this is similar to the one applied by Wingate et al. [2011]. Specifically we update P \u2217(x) = P \u2217(x) + log ( |D|\u2217pstale |D\u2032|\u2217pfresh ) , where pstale is the probability of all variables that are in the current trace but not the new and pfresh is the probability of variables sampled in the new trace that are not in the current."}, {"heading": "4 Empirical Evaluation", "text": ""}, {"heading": "4.1 Inferring the mean of a Gaussian", "text": "We begin with the inference of the mean of a gaussian. This time the prior and posterior are of similar\nsizes, but the posterior is shifted by an unusual observation. In this setting we look at 3 models, namely 1-dimensional, 2-dimensional and trans-dimensional. The model specifications are given in Figure 9 and the models\u2019 posteriors are shown in Figure 10.\nWe now look at the performance of Metropolis, slice sampling and some different mixtures of the two over the 3 models. The mixture methods work by flipping a biased coin before extracting each sample in order to decide which inference method to use.\nTo compare the inference engines, we extract samples until a certain number of trace likelihood calculations are performed and then repeat this process 100 times, starting from different random seeds. We then plot the median and the quartiles of the KS differences from the true posterior, over all generated runs. Figure 11 shows the quartiles of the runs on the three models.\nOn the simple, 1d, model all variants of Slice sampling clearly outperform Metropolis. In the quartile graph we consider mixtures of Metropolis and Slice both with 10% Metropolis and with 50% Metropolis and find that the change doesn\u2019t have a significant impact on performance. This is likely because, if slice picks a good sample, Metropolis is likely to simply keep it unchanged (since it will tend to reject the proposal from the prior if it is worse).\nOn the 2d model, slice still clearly outperforms Metropolis, though the gap is not as pronounced as for the 1d model. Further, as in the 1d model, the 3 different slice variants all get quite similar performance. Additionally, on this model, the fact that the slice mixtures get more samples per trace calculation translates into a slightly better performance for them than for the pure slice sampling method.\nThe third, trans-dimensional, model reveals a more pronounced performance difference between slice and Metropolis than in the 2 dimensional case. Further, on this model, we see a significant gap between the 1:9 Metropolis:Slice method and the other slice sampling approaches. It seems that, in this case, the ratio of 1:9 strikes a good balance between slice sampling\u2019s auto-\nmatic adjustment of the kernel\u2019s width and Metropolis\u2019 efficiency in likelihood calculations per sample."}, {"heading": "4.2 Anglican models", "text": "In order to further test the Slice sampling inference engine we look at 3 of the models defined in Wood et al. [2014]. The model specifications for these are provided in Section 2.2 and in the supplementary material.\nTo evaluate the engines, we use each to generate 100 independent sample runs. In Figure 12 we plot the convergence of the empirical posterior to the true posterior as the number of trace likelihood calculations increase. For continuous distributions we use the Kolmogorov-Smirnov statistic (as before), whereas for discrete ones we employ the Kullback-Leibler divergence to measure the similarity of two distributions.\nOn the Branching and HMM models (12) naive Metropolis-Hastings outperforms all Slice combinations. These models are quite small and are conditioned on few datapoints, which means they have relatively similar priors and posteriors. On such models, the overhead of slice sampling cannot be recovered, since the naive Metropolis actually does quite well by simply sampling the prior. Additionally, in the HMM model, we are only inferring in which of 3 states the model is in. With such a small state space the value that an adjusting proposal kernel can add is limited.\nOn the Marsaglia model however (12), slice sampling does obtain a boost in performance. This is due both to the continuous nature of the model (i.e. larger state space) and to the fact that the prior and posterior are significantly different (figure in Appendix). It\u2019s worth noting that the Marsaglia model is the only one in which Metropolis from the prior outperformed Anglican\u2019s Particle MCMC. Slice sampling is therefore better than both PMCMC and Metropolis on this model.\nIt is interesting to note that, on all models tried, slice outperforms Metropolis on a per-sample basis. This is an unfair comparison since slice does more \u201cwork\u201d to generate a sample than Metropolis, but it illustrates the point that the samples chosen by slice are in gen-\neral better, the only question is if the difference is sufficient as to justify the increased overhead."}, {"heading": "4.3 Models for classification", "text": "Lastly we look at some new models, namely logistic regression and a small neural network. Since we are doing classification, here we can actually plot the mean squared error that the model achieves after a certain number of trace likelihood computations. As before, we perform separate runs starting with different random seeds and plot both the median and the 25% and 75% quartiles.\nFirst we perform logistic regression on the well known\nIris dataset, obtained from Bache and Lichman [2013]. The result is shown in Figure 13 and shows that Slice sampling converges significantly faster than MH. Secondly we train a small Bayesian neural network on the same dataset. Our neural network is composed of an input layer of the same dimensionality as the data (4), two hidden layers of 4 and 2 neurons respectively and an output layer of a single neuron (since we are treating the task as a binary classification problem). In Figure 14 we see that slice sampling does significantly better than the MH baseline on this task. We also notice that the harder the inference problem is, the more the margin by which slice sampling outperforms grows. Iris-setosa, on which the performance gap is smallest,\nis linearly separable from the other 2 classes, which are not linearly separable from one another. In the case of Iris-versicolor, we were not able to run the engine long enough for Metropolis to match slice\u2019s performance, as Metropolis is still lagging behind even after running 10 times longer than slice."}, {"heading": "5 Related Work", "text": "Our slice sampling inference engine is based on the slice sampling method presented by Neal [2003], and influenced by the computer friendly slice sampler shown in MacKay [2003].\nSlice sampling techniques have been applied to a wide range of inference problems (Neal [2003]) and are used in some of the most popular PPLs, such as BUGS (Lunn et al. [2009]) and STAN (Stan Development Team [2014]). However, the slice samplers these languages employ are not exposed directly to the user, but instead only used internally by the language. Therefore, the slice samplers present in these languages are not intended to generalise to all models and, specifically, make no mention of trans-dimensionality corrections. Poon and Domingos [2006], also proposes a slice sampling based solution, this time to Markov Logic problems. However this algorithm is focused on deterministic or near-deterministic inference tasks and so bears little resemblance to our approach.\nThe most similar use-case to ours would be in Venture (Mansinghka et al. [2014]). Here however slice sampling is only mentioned in passing, amongst other\ninference techniques the system could support. No details, nor discussions of trans-dimensionality, are present."}, {"heading": "6 Conclusion", "text": "We have introduced and made available StocPy, the first general purpose Python Probabilistic Programming Language. We have shown the benefits StocPy offers both to users and designers of PPLs, namely flexibility, clarity, succintness and ease of prototyping.\nWe have also implemented a novel, slice sampling, inference engine in StocPy. To our knowledge this is the first general purpose slice sampling inference engine and the first slice sampling procedure that solves the problem of trans-dimensionality.\nWe have empirically evaluated this slice sampling engine and shown that the potential benefits far outweight the potential costs, when compared to singlesite Metropolis. While Metropolis works well on very small models where the prior and the posterior are similar, slice provides substantial benefits as the distributions diverge. Additionally, on the models where Metropolis performs best slice only experiences a constant slowdown due to its overhead, whereas when Metropolis performs poorly the performance difference can be arbitrarily large.\nFinally, we have provided comparisons with Anglican which show promising results despite slice sampling not beeing optimised for runtime speed. A full benchmarking of the systems remains to be performed."}, {"heading": "1 Detailed slice-sampling pseudocode", "text": "Here we present the detailed pseudocode for one possible slice sampling implementation. Algorithm 1 is the high-level logic of slice sampling which was also presented in the paper.\nThe other algorithms expand upon this, showing the lower level operations. Specifically, Algorithm 2 shows how we could perform the operation described in line 5 of Algorithm 1, by exponentially increasing the [xl, xr] interval. Similarly, Algorithm 3 describes how the operation from line 6 of Algorithm 1 is performed, namely by sampling a next x placed on the already defined segment [xl, xr] such that x is choosen \u201cuniformly under the curve\u201d.\nAlgorithm 1 Slice Sampling\n1: Pick initial x1 such that P \u2217(x1) > 0 2: Pick number of samples to extract N 3: for t = 1\u2192 N do 4: Sample a height u uniformly from [0, P \u2217(xt)] 5: Define a segment [xl, xr] at height u 6: Pick xt+1 \u2208 [xl, xr] such that P \u2217(xt+1) > u\nAlgorithm 2 Return appropriate [xl, xr] interval, given current sample x and its height u\n1: function StepOut(x, u) 2: Pick small initial step width wi 3: while P \u2217(x\u2212 w) < u or P \u2217(x + w) < u do 4: wi = wi/2\n5: w = wi 6: while P \u2217(x\u2212 w) > u do 7: w = w \u2217 2 8: xl = x\u2212 w 9: w = wi 10: while P \u2217(x + w) > u do 11: w = w \u2217 2 12: xr = x + w 13: return xl, xr\nAlgorithm 3 Given current sample x and its height u, get next sample xn from the interval [xl, xr] such that P \u2217(xn) > u\n1: function SampNext(x, u, xl, xr) 2: Sample xn uniformly from [xl, xr] 3: while P \u2217(xn) \u2264 u do 4: if xn > x then 5: xr = xn 6: else 7: xl = xn 8: Sample xn uniformly from [xl, xr]\n9: return xn"}, {"heading": "2 True Posteriors", "text": "Here we present 2 true posteriors that support some claims made in the paper. In Figure 1, we see that the \u201cHard\u201d Gaussian mean inference problem results in an analytical posterior that is very close to a Gaussian with mean 2 and standard deviation 0.032. The second posterior, shown in Figure 2 presents the difference between the prior and posterior of the Marsaglia model. This figure gives an intuitive understanding of why Metropolis is outperformed by slice sampling on the Marsaglia model. It is easy to imagine how bad Metropolis will do in this model if it only samples from the prior."}, {"heading": "3 Additional Sample Programs in StocPy and Anglican", "text": "For completeness and as to get a better idea of the \u201cfeel\u201d of StocPy we show the remaining 2 Anglican models that were not presented in the paper. These models are a Dirichlet Process Mixture (Figure 3) and a Marsaglia (Figure 4). We also show these models\u2019 implementation in Anglican, for comparison\u2019s sake.\nAs in the paper, we can see that on the more complex model StocPy manages to be more succint, largely thanks to the Python in-built operators. Additionally,\nar X\niv :1\n50 1.\n04 68\n4v 1\n[ cs\n.A I]\n2 0\nJa n\n20 15\nclarity is a subjective perception, but we would argue that the StocPy formulations are friendlier for people without a strong CS or technical background, such as domain experts."}], "references": [{"title": "Modern compiler implementation in ML", "author": ["A.W. Appel"], "venue": "Cambridge university press,", "citeRegEx": "Appel.,? \\Q1998\\E", "shortCiteRegEx": "Appel.", "year": 1998}, {"title": "A language for generative models", "author": ["N. Goodman", "V. Mansinghka", "D. Roy", "K. Bonawitz", "J. Tenenbaum. Church"], "venue": "In Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Goodman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Goodman et al\\.", "year": 2008}, {"title": "An agenda for probabilistic programming: Usable, portable, and ubiquitous", "author": ["A.D. Gordon"], "venue": null, "citeRegEx": "Gordon.,? \\Q2013\\E", "shortCiteRegEx": "Gordon.", "year": 2013}, {"title": "Reversible jump MCMC", "author": ["P.J. Green", "D.I. Hastie"], "venue": "Genetics, 155(3):1391\u20131403,", "citeRegEx": "Green and Hastie.,? \\Q2009\\E", "shortCiteRegEx": "Green and Hastie.", "year": 2009}, {"title": "Exploring network structure, dynamics, and function using NetworkX", "author": ["A.A. Hagberg", "D.A. Schult", "P.J. Swart"], "venue": "In Proceedings of the 7th Python in Science Conference", "citeRegEx": "Hagberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hagberg et al\\.", "year": 2008}, {"title": "The BUGS project: Evolution, critique and future directions", "author": ["D. Lunn", "D. Spiegelhalter", "A. Thomas", "N. Best"], "venue": "Statistics in medicine,", "citeRegEx": "Lunn et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lunn et al\\.", "year": 2009}, {"title": "Information theory, inference and learning algorithms", "author": ["D.J. MacKay"], "venue": "Cambridge university press,", "citeRegEx": "MacKay.,? \\Q2003\\E", "shortCiteRegEx": "MacKay.", "year": 2003}, {"title": "Venture: a higher-order probabilistic programming platform with programmable inference", "author": ["V. Mansinghka", "D. Selsam", "Y. Perov"], "venue": "arXiv preprint arXiv:1404.0099,", "citeRegEx": "Mansinghka et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mansinghka et al\\.", "year": 2014}, {"title": "BLOG: Probabilistic models with unknown objects", "author": ["B. Milch", "B. Marthi", "S. Russell", "D. Sontag", "D.L. Ong", "A. Kolobov"], "venue": "Statistical relational learning,", "citeRegEx": "Milch et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Milch et al\\.", "year": 2007}, {"title": "Sound and efficient inference with probabilistic and deterministic dependencies", "author": ["H. Poon", "P. Domingos"], "venue": "In AAAI,", "citeRegEx": "Poon and Domingos.,? \\Q2006\\E", "shortCiteRegEx": "Poon and Domingos.", "year": 2006}, {"title": "StocPy: An expressive probabilistic programming language, in python", "author": ["R. Ranca"], "venue": "https://github. com/RazvanRanca/StocPy,", "citeRegEx": "Ranca.,? \\Q2014\\E", "shortCiteRegEx": "Ranca.", "year": 2014}, {"title": "Lightweight implementations of probabilistic programming languages via transformational compilation", "author": ["D. Wingate", "A. Stuhlmueller", "N.D. Goodman"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Wingate et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wingate et al\\.", "year": 2011}, {"title": "A new approach to probabilistic programming inference", "author": ["F. Wood", "J.W. van de Meent", "V. Mansinghka"], "venue": "In Proceedings of the 17th International conference on Artificial Intelligence and Statistics,", "citeRegEx": "Wood et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wood et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "There has been a recent surge of interest in probabilistic programming, as demonstrated by the continued development of new languages (eg: Wood et al. [2014], Goodman et al.", "startOffset": 139, "endOffset": 158}, {"referenceID": 1, "context": "[2014], Goodman et al. [2008], Lunn et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 1, "context": "[2014], Goodman et al. [2008], Lunn et al. [2009], Milch et al.", "startOffset": 8, "endOffset": 50}, {"referenceID": 1, "context": "[2014], Goodman et al. [2008], Lunn et al. [2009], Milch et al. [2007]) and by the recent DARPA program encouraging further research in this direction.", "startOffset": 8, "endOffset": 71}, {"referenceID": 2, "context": "and inference performance (Gordon [2013]).", "startOffset": 27, "endOffset": 41}, {"referenceID": 2, "context": "and inference performance (Gordon [2013]). In this paper we address usability by presenting a new PPL, StocPy, available online as a Python library (Ranca [2014]).", "startOffset": 27, "endOffset": 162}, {"referenceID": 11, "context": "StocPy is based on the PPL design presented by Wingate et al. [2011], but is written purely in Python, and works on model definitions written in the same language.", "startOffset": 47, "endOffset": 69}, {"referenceID": 11, "context": "StocPy is based on the PPL design presented by Wingate et al. [2011], but is written purely in Python, and works on model definitions written in the same language. This enables us to take full advantage of Python\u2019s excellent prototyping abilities and fast development cycles, and thus allows us to specify models in very flexible ways. For instance models containing complex control flow and elements such as stochastic (mutual) recursion are easily representable. Additionally, the pure Python implementation means StocPy itself provides a good base for further experiments into PPLs, such as defining novel naming techniques for stochastic variables, looking at program recompilation to improve inference performance, or testing out new inference engines. We illustrate the benefits StocPy offers by discussing some of the language\u2019s features and contrasting the definitions of several models in StocPy against those in Anglican (Wood et al. [2014]).", "startOffset": 47, "endOffset": 951}, {"referenceID": 10, "context": "We implement a novel PPL, StocPy, which is made available online (Ranca [2014]).", "startOffset": 66, "endOffset": 79}, {"referenceID": 10, "context": "We implement a novel PPL, StocPy, which is made available online (Ranca [2014]). StocPy has both a Metropolis-Hastings inference engine (implemented in the style presented by Wingate et al. [2011]) and a Slice sampling inference engine which we further discuss in Section 3.", "startOffset": 66, "endOffset": 197}, {"referenceID": 1, "context": "On the other hand, lisp-based PPLs such as Goodman et al. [2008] offer greater flexibility but at the price of usability.", "startOffset": 43, "endOffset": 65}, {"referenceID": 4, "context": "For instance, Python\u2019s excellent network and graph libraries (eg: Hagberg et al. [2008]) could be used to define a novel naming convention for stochastic primitives, which takes the program\u2019s control flow into account.", "startOffset": 66, "endOffset": 88}, {"referenceID": 4, "context": "For instance, Python\u2019s excellent network and graph libraries (eg: Hagberg et al. [2008]) could be used to define a novel naming convention for stochastic primitives, which takes the program\u2019s control flow into account. Such a naming convention is required by the framework of Wingate et al. [2011] and could be an improvement over the simpler version presented in that paper.", "startOffset": 66, "endOffset": 298}, {"referenceID": 12, "context": "The models are 2 of those presented by Wood et al. [2014] (more models are included in the supplementary material).", "startOffset": 39, "endOffset": 58}, {"referenceID": 6, "context": "Slice sampling, however, adjusts an inadequate step size of size S with a cost that is only logarithmic in the size of S (MacKay [2003]).", "startOffset": 122, "endOffset": 136}, {"referenceID": 11, "context": "We refer to the MH proposal described in Wingate et al. [2011], which is the same as the \u201cRDB\u201d benchmark used in Wood et al.", "startOffset": 41, "endOffset": 63}, {"referenceID": 11, "context": "We refer to the MH proposal described in Wingate et al. [2011], which is the same as the \u201cRDB\u201d benchmark used in Wood et al. [2014]. In this MH implementation, whenever we wish to resample a variable, we run the model until we encounter the variable and then sample it from its prior conditioned on the variables we have seen so far in the current run.", "startOffset": 41, "endOffset": 132}, {"referenceID": 11, "context": "As presented in Wingate et al. [2011], we consider a sample x to be a program execution trace and P \u2217(x) to be the likelihood of all stochastic variables sampled in trace x.", "startOffset": 16, "endOffset": 38}, {"referenceID": 12, "context": "In order to understand the additional complications that trans-dimensional models present for inference engines we look at a simple example taken from Wood et al. [2014], the Branching model.", "startOffset": 151, "endOffset": 170}, {"referenceID": 3, "context": "To understand the trans-dimensional corrections we can place them in the framework of Reversible Jump Markov Chain Monte Carlo (RJMCMC, Green and Hastie [2009]).", "startOffset": 136, "endOffset": 160}, {"referenceID": 11, "context": "The correction we aplly to account for this is similar to the one applied by Wingate et al. [2011]. Specifically we update P \u2217(x) = P \u2217(x) + log ( |D|\u2217pstale |D\u2032|\u2217pfresh ) , where", "startOffset": 77, "endOffset": 99}, {"referenceID": 12, "context": "In order to further test the Slice sampling inference engine we look at 3 of the models defined in Wood et al. [2014]. The model specifications for these are provided in Section 2.", "startOffset": 99, "endOffset": 118}, {"referenceID": 6, "context": "Our slice sampling inference engine is based on the slice sampling method presented by Neal [2003], and influenced by the computer friendly slice sampler shown in MacKay [2003].", "startOffset": 163, "endOffset": 177}, {"referenceID": 5, "context": "Slice sampling techniques have been applied to a wide range of inference problems (Neal [2003]) and are used in some of the most popular PPLs, such as BUGS (Lunn et al. [2009]) and STAN (Stan Development Team [2014]).", "startOffset": 157, "endOffset": 176}, {"referenceID": 5, "context": "Slice sampling techniques have been applied to a wide range of inference problems (Neal [2003]) and are used in some of the most popular PPLs, such as BUGS (Lunn et al. [2009]) and STAN (Stan Development Team [2014]).", "startOffset": 157, "endOffset": 216}, {"referenceID": 5, "context": "Slice sampling techniques have been applied to a wide range of inference problems (Neal [2003]) and are used in some of the most popular PPLs, such as BUGS (Lunn et al. [2009]) and STAN (Stan Development Team [2014]). However, the slice samplers these languages employ are not exposed directly to the user, but instead only used internally by the language. Therefore, the slice samplers present in these languages are not intended to generalise to all models and, specifically, make no mention of trans-dimensionality corrections. Poon and Domingos [2006], also proposes a slice sampling based solution, this time to Markov Logic problems.", "startOffset": 157, "endOffset": 556}, {"referenceID": 7, "context": "The most similar use-case to ours would be in Venture (Mansinghka et al. [2014]).", "startOffset": 55, "endOffset": 80}], "year": 2015, "abstractText": "We introduce the first, general purpose, slice sampling inference engine for probabilistic programs. This engine is released as part of StocPy, a new Turing-Complete probabilistic programming language, available as a Python library. We present a transdimensional generalisation of slice sampling which is necessary for the inference engine to work on traces with different numbers of random variables. We show that StocPy compares favourably to other PPLs in terms of flexibility and usability, and that slice sampling can outperform previously introduced inference methods. Our experiments include a logistic regression, HMM, and Bayesian Neural Net.", "creator": "LaTeX with hyperref package"}}}