{"id": "1501.04786", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jan-2015", "title": "Consid{\\'e}rant la d{\\'e}pendance dans la th{\\'e}orie des fonctions de croyance", "abstract": "In this paper, we propose to learn sources independence in order to choose the appropriate type of combination rules when aggregating their beliefs. Some combination rules are used with the assumption of their sources independence whereas others combine beliefs of dependent sources. Therefore, the choice of the combination rule depends on the independence of sources involved in the combination. In this paper, we propose also a measure of independence, positive and negative dependence to integrate in mass functions before the combinaision with the independence assumption. Finally, we propose to learn the type of mixed values, for example, if the sum of the two inputs are equal to one and one, then the value will be assigned to one of the two inputs and one output will be assigned to one of the two outputs, as discussed above. We then assume that the ratio of negative and positive to negative is correct and then the ratio of negative to negative is correct by the fact that the ratio of negative to positive is not correct by the fact that the ratio of negative to positive is not correct by the fact that the ratio of negative to positive is not correct by the fact that the ratio of negative to positive is not correct by the fact that the ratio of negative to positive is not correct by the fact that the ratio of negative to positive is not correct by the fact that the ratio of negative to positive is not correct by the fact that the ratio of negative to positive is not correct by the fact that the ratio of negative to positive is not correct by the fact that the ratio of negative to positive is not correct by the fact that the ratio of negative to negative is not correct by the fact that the ratio of negative to positive is not correct by the fact that the ratio of negative to negative is not correct by the fact that the ratio of negative to positive is not correct by the fact that the ratio of negative to positive is not correct by the fact that the ratio of negative to positive is not correct by the fact that the ratio of negative to positive is not correct by the fact that the ratio of negative to positive is not correct by the fact that the ratio of negative to positive is not correct by the fact that the ratio of negative to positive is not correct by the fact that the ratio of negative to positive is not correct by the fact that the ratio of negative to positive is not correct by the fact that the ratio of negative to positive is not correct by the fact that the ratio of negative to positive is not correct by the fact that the ratio of negative to positive is not correct by the fact that the ratio of negative to", "histories": [["v1", "Tue, 20 Jan 2015 12:48:41 GMT  (88kb,D)", "http://arxiv.org/abs/1501.04786v1", "in French"]], "COMMENTS": "in French", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mouna chebbah", "mouloud kharoune", "arnaud martin", "boutheina ben yaghlane"], "accepted": false, "id": "1501.04786"}, "pdf": {"name": "1501.04786.pdf", "metadata": {"source": "CRF", "title": "Conside\u0301rant la de\u0301pendance dans la the\u0301orie des fonctions de croyance", "authors": ["Mouna Chebbah", "Mouloud Kharoune", "Arnaud Martin", "Boutheina Ben Yaghlane"], "emails": ["Mouloud.Kharoune@univ-rennes1.fr,", "Arnaud.Martin@univ-rennes1.fr", "Mouna.Chebbah@univ-rennes1.fr", "boutheina.yaghlane@ihec.rnu.tn"], "sections": [{"heading": null, "text": "Consid\u00e9rant la d\u00e9pendance dans la th\u00e9orie des fonctions de croyance\nMouna Chebbah\u2217 \u2217\u2217, Mouloud Kharoune\u2217\nArnaud Martin\u2217, Boutheina Ben Yaghlane\u2217\u2217\u2217\n\u2217UMR 6074 IRISA, Universit\u00e9 de Rennes1 / IUT de Lannion, Rue Edouard Branly BP 3021, 22302 Lannion cedex\nMouloud.Kharoune@univ-rennes1.fr, Arnaud.Martin@univ-rennes1.fr \u2217\u2217LARODEC, ISG Tunis, 41 Rue de la Libert\u00e9, Cit\u00e9 Bouchoucha\n2000 Le Bardo, Tunisie Mouna.Chebbah@univ-rennes1.fr\n\u2217\u2217\u2217LARODEC, IHEC Carthage, Carthage Pr\u00e9sidence 2016, Tunisie\nboutheina.yaghlane@ihec.rnu.tn\nR\u00e9sum\u00e9. La fusion d\u2019informations issues de plusieurs sources cherche \u00e0 am\u00e9liorer la prise de d\u00e9cision. La th\u00e9orie des fonctions de croyance, pour r\u00e9aliser cette fusion, utilise des r\u00e8gles de combinaison faisant bien souvent l\u2019hypoth\u00e8se forte de l\u2019ind\u00e9pendance des sources. Cette hypoth\u00e8se d\u2019ind\u00e9pendance n\u2019est cependant pas formalis\u00e9e ni v\u00e9rifi\u00e9e. Nous proposons dans cet article un apprentissage de l\u2019ind\u00e9pendance cognitive de sources d\u2019information permettant de mesurer la d\u00e9pendance ou l\u2019ind\u00e9pendance. Cette mesure exprim\u00e9e par une fonction de masse est ensuite int\u00e9gr\u00e9e par une approche d\u2019affaiblissement avant de r\u00e9aliser la combinaison d\u2019informations."}, {"heading": "1 Introduction", "text": "La th\u00e9orie des fonctions de croyance issue des travaux de Dempster (1967) et Shafer (1976) permet une bonne mod\u00e9lisation des donn\u00e9es impr\u00e9cises et/ou incertaines et offre un outil puissant pour fusionner des informations issues de plusieurs sources. Pour ce faire, les donn\u00e9es incertaines et impr\u00e9cises des diff\u00e9rentes sources sont mod\u00e9lis\u00e9es par des fonctions de masse et combin\u00e9es afin de mettre en \u00e9vidence les croyances communes et assurer une prise de d\u00e9cision plus fiable.\nLe choix de la r\u00e8gle de combinaison \u00e0 appliquer repose sur des hypoth\u00e8ses d\u2019ind\u00e9pendance de sources. En effet, certaines r\u00e8gles de combinaison comme celles de Dempster (1967); Smets (1990); Yager (1987); Dubois et Prade (1988) combinent des fonctions de croyance dont les sources sont suppos\u00e9es ind\u00e9pendantes par contre les r\u00e8gles prudente et hardie propos\u00e9es par Den\u0153ux (2006) n\u2019exigent pas d\u2019hypoth\u00e8se d\u2019ind\u00e9pendance. L\u2019ind\u00e9pendance cognitive est une hypoth\u00e8se fondamentale pour le choix du type de r\u00e8gles de combinaison \u00e0 appliquer. Les ind\u00e9pendances \u00e9videntielle, cognitive et doxastique ont \u00e9t\u00e9 d\u00e9finies dans la cadre de la th\u00e9orie des\nar X\niv :1\n50 1.\n04 78\n6v 1\n[ cs\n.A I]\n2 0\nJa n\n20 15\nfonctions de croyance.\nD\u2019une part, les travaux de Ben Yaghlane et al. (2002a,b) \u00e9tudient principalement l\u2019ind\u00e9pendance doxastique des variables. D\u2019autre part, les travaux de Smets (1993) et ceux de Shafer (1976) ont d\u00e9fini l\u2019ind\u00e9pendance cognitive des variables par une absence d\u2019implication sur la modification des croyances d\u2019une variable en cas de changement des croyances sur l\u2019autre variable. Autrement dit, deux variables sont cognitivement ind\u00e9pendantes si la connaissance de la croyance de l\u2019une n\u2019affecte pas celle de l\u2019autre. Dans la litt\u00e9rature, l\u2019ind\u00e9pendance cognitive des variables est formalis\u00e9e mais l\u2019ind\u00e9pendance cognitive des sources n\u2019est pas abord\u00e9e.\nCe papier est focalis\u00e9 sur l\u2019ind\u00e9pendance cognitive des sources, les ind\u00e9pendances doxastique, cognitive et \u00e9videntielle des variables ne sont pas abord\u00e9es. Nous proposons une approche statistique pour l\u2019estimation de l\u2019ind\u00e9pendance cognitive de deux sources. Deux sources sont cognitivement ind\u00e9pendantes si elles ne communiquent pas entre elles et si elles n\u2019ont pas le m\u00eame corpus de croyance 1. La m\u00e9thode propos\u00e9e permet d\u2019\u00e9tudier le comportement g\u00e9n\u00e9ral de deux sources et de les comparer pour d\u00e9celer toute d\u00e9pendance pouvant exister entre elles. Dans le cas de sources d\u00e9pendantes, nous proposons d\u2019\u00e9tudier le type de cette d\u00e9pendance. C\u2019est-\u00e0-dire analyser les donn\u00e9es de sorte \u00e0 voir si les sources sont positivement d\u00e9pendantes ou si elles sont n\u00e9gativement d\u00e9pendantes. Cette mesure d\u2019ind\u00e9pendance peut soit guider le choix du type de r\u00e8gles de combinaison, soit \u00eatre int\u00e9gr\u00e9e dans les fonctions de masse afin de justifier l\u2019hypoth\u00e8se d\u2019ind\u00e9pendance des sources.\nDans la suite de cet article, nous commen\u00e7ons par rappeler quelques notions de base de la th\u00e9orie des fonctions de croyance. Ensuite, nous pr\u00e9sentons dans la troisi\u00e8me section notre approche statistique d\u2019estimation de l\u2019ind\u00e9pendance ou de la d\u00e9pendance cognitive. Lors de cette m\u00e9thode, nous proposons d\u2019appliquer l\u2019algorithme de classification non-supervis\u00e9e sur toutes les informations incertaines de chaque source et de chercher un appariement des clusters. L\u2019ind\u00e9pendance des sources est estim\u00e9e \u00e0 partir des poids attribu\u00e9s \u00e0 chaque couple de clusters li\u00e9s. Dans la quatri\u00e8me section, si les sources sont d\u00e9pendantes, une \u00e9tude de la nature de la d\u00e9pendance est faite afin de voir si cette d\u00e9pendance est positive ou n\u00e9gative. Cette mesure d\u2019ind\u00e9pendance, d\u00e9pendance positive et d\u00e9pendance n\u00e9gative peut \u00eatre prise en compte dans les fonctions de masse des sources afin de justifier l\u2019hypoth\u00e8se d\u2019ind\u00e9pendance dans la cinqui\u00e8me section. Finalement, avant de conclure dans la septi\u00e8me section, nous pr\u00e9sentons dans la sixi\u00e8me section les exp\u00e9rimentations sur des donn\u00e9es g\u00e9n\u00e9r\u00e9es al\u00e9atoirement."}, {"heading": "2 Th\u00e9orie des fonctions de croyance", "text": "La th\u00e9orie des fonctions de croyance initialement introduite par Dempster (1967), formalis\u00e9e ensuite par Shafer (1976) est employ\u00e9e dans des applications de fusion d\u2019informations. Nous pr\u00e9sentons ci-dessous quelques principes de base de cette th\u00e9orie.\n1. Le corpus de croyance est l\u2019ensemble de connaissances ou d\u2019informations acquises par une source."}, {"heading": "2.1 Principes de base", "text": "Soit un cadre de discernement \u2126 = {\u03c91, \u03c92, . . . , \u03c9n} l\u2019ensemble de toutes les hypoth\u00e8ses exclusives et exhaustives. Le cadre de discernement est aussi l\u2019univers de discours d\u2019un probl\u00e8me donn\u00e9.\nL\u2019ensemble 2\u2126 = {A|A \u2286 \u2126} = {\u2205, \u03c91, \u03c92, . . . , \u03c9n, \u03c91 \u222a \u03c92, . . . , \u2126}, est l\u2019ensemble de toutes les hypoth\u00e8ses de \u2126 ainsi que leurs disjonctions.\nUne fonction de masse est une fonction de 2\u2126 vers l\u2019intervalle [0, 1] qui affecte \u00e0 chaque sous-ensemble une masse de croyance \u00e9l\u00e9mentaire. Cette fonction de masse fournie par une source d\u2019information 2 est une repr\u00e9sentation des connaissances incertaines et impr\u00e9cises. Formellement, une fonction de masse, not\u00e9e m\u2126, est d\u00e9finie comme suit :\nm\u2126 : 2\u2126 \u2192 [0, 1] (1)\ntel que : \u2211 A\u2286\u2126 m\u2126(A) = 1 (2)\nUn sous-ensemble ayant une masse de croyance \u00e9l\u00e9mentaire non-nulle est un \u00e9l\u00e9ment focal. La masse d\u2019un \u00e9l\u00e9ment focal A repr\u00e9sente le degr\u00e9 de croyance \u00e9l\u00e9mentaire de la source \u00e0 ce que l\u2019hypoth\u00e8se vraie soit dans A."}, {"heading": "2.2 Combinaison", "text": "Dans le cadre de la th\u00e9orie des fonctions de croyance, plusieurs r\u00e8gles de combinaison sont propos\u00e9es pour la fusion d\u2019informations. Les fonctions de masse sont issues de diff\u00e9rentes sources et sont d\u00e9finies sur le m\u00eame ensemble de discernement. La combinaison permet de synth\u00e9tiser ces diff\u00e9rentes informations en vue d\u2019une prise de d\u00e9cision plus fiable. Le choix des r\u00e8gles de combinaison d\u00e9pend de certaines hypoth\u00e8ses initiales, les op\u00e9rateurs de type conjonctif peuvent \u00eatre employ\u00e9s lorsque les sources sont fiables et ind\u00e9pendantes cognitivement que nous d\u00e9finirons pr\u00e9cis\u00e9ment \u00e0 la section 3. La combinaison conjonctive s\u2019\u00e9crit pour deux fonctions de masse m\u21261 et m \u2126 2 et pour tout A \u2286 \u2126 par :\nm\u2126\u2229\u00a9(A) = m \u2126 1 \u2229\u00a9m \u2126 2 (A) = \u2211 B\u2229C=A m\u21261 (B)\u00d7m\u21262 (C). (3)\nNotons que l\u2019\u00e9l\u00e9ment neutre pour cette r\u00e8gle est la masse : m\u2126(A) = 1 si A = \u2126 et 0 sinon. Lorsque l\u2019hypoth\u00e8se de fiabilit\u00e9 est trop forte et que l\u2019on ne peut supposer que seule une des sources est fiable, la combinaison disjonctive peut alors \u00eatre employ\u00e9e toujours sous l\u2019hypoth\u00e8se d\u2019ind\u00e9pendance cognitive :\nm\u2126\u222a\u00a9(A) = m \u2126 1 \u222a\u00a9m \u2126 2 (A) = \u2211 B\u222aC=A m\u21261 (B)\u00d7m\u21262 (C). (4)\nNotons que l\u2019\u00e9l\u00e9ment neutre pour cette r\u00e8gle est la masse : m\u2126(A) = 1 si A = \u2205 et 0 sinon. Bien que les r\u00e8gles disjonctive et conjonctive soient associatives et commutatives, elles\n2. La source peut \u00eatre un expert humain, un classificateur, un capteur, . . .\nne sont pas idempotentes ce qui justifie leur inefficacit\u00e9 \u00e0 la fusion d\u2019informations issues de sources d\u00e9pendantes cognitivement. La plupart des r\u00e8gles de combinaison issues des r\u00e8gles conjonctives et disjonctives, en particulier pour r\u00e9partir le conflit, supposent que les sources sont ind\u00e9pendantes cognitivement. Martin (2010) en rappelle quelques unes.\nDans cet article, nous utilisons la moyenne pour la combinaison. Cette r\u00e8gle est choisie parce qu\u2019elle est idempotente et commutative en plus elle combine tous types de fonctions de masse. Toute autre r\u00e8gle de combinaison v\u00e9rifiant ces crit\u00e8res peut \u00eatre utilis\u00e9e. Pour chaque \u00e9l\u00e9ment focal A des M fonctions de masse, la masse combin\u00e9e de A, m\u2126Moyenne(A), est calcul\u00e9e \u00e0 partir des M masses de croyance \u00e9l\u00e9mentaires m \u2126 i (A) comme suit :\nm\u2126Moyenne(A) = 1\nM M\u2211 i=1 m\u2126i (A) (5)"}, {"heading": "2.3 Conditionnement et d\u00e9conditionnement", "text": "Apr\u00e8s l\u2019acquisition d\u2019une fonction de masse, une information certaine peut appara\u00eetre confirmant que l\u2019hypoth\u00e8se vraie est (ou n\u2019est pas) dans l\u2019un des sous-ensembles de 2\u2126. Dans ce cas, la fonction de masse doit \u00eatre mise \u00e0 jour afin de prendre en consid\u00e9ration cette nouvelle information certaine. Cette mise \u00e0 jour est r\u00e9alis\u00e9e par l\u2019op\u00e9rateur de conditionnement qui consiste \u00e0 transf\u00e9rer la masse attribu\u00e9e \u00e0 chaque \u00e9l\u00e9ment focal \u00e0 son intersection avec l\u2019ensemble certain. Le conditionnement d\u2019une fonction de masse m\u2126 par l\u2019hypoth\u00e8se A \u2282 \u2126 revient \u00e0 transf\u00e9rer les masses de croyance de tous les \u00e9l\u00e9ments focaux de m\u2126 \u00e0 leurs intersections avec A. La fonction de masse conditionn\u00e9e m\u2126[A] est donn\u00e9e par Smets et Kruse (1997) comme suit :\nm\u2126[A](C) =  0 for C 6\u2286 A\u2211 B\u2286Ac m\u2126(C \u222aB) for C \u2286 A (6)\navec Ac le compl\u00e9mentaire de A, Ac = \u2126 \\ {A}. Notons que le conditionnement sur une hypoth\u00e8se du m\u00eame cadre de discernement revient \u00e0 la combinaison conjonctive de la fonction de masse initiale m\u2126 avec la nouvelle fonction de masse certaine m\u2126(A) = 1 sachant que A est une hypoth\u00e8se certaine, donc m\u2126A \u2229\u00a9m\n\u2126 = m\u2126[A]. Le d\u00e9conditionnement est l\u2019op\u00e9ration inverse du conditionnement qui permet de retrouver une fonction de masse la moins informative \u00e0 partir d\u2019une fonction de masse conditionn\u00e9e. \u00c9tant donn\u00e9e m\u2126[A], la fonction de masse conditionnellement \u00e0 A, il est difficile de retrouver la fonction de masse originale mais il est possible de retrouver la fonction de masse qui engage le moins (Hsia (1991) et Smets (1993)) telle que son conditionnement par A donne m\u2126[A]. Le d\u00e9conditionnement de m\u2126[A] permet de retrouver m\u2126 comme suit :\nm\u2126(C \u222aAc) = m\u2126[A](C) \u2200C \u2286 2\u2126, Ac = \u2126\\A (7)"}, {"heading": "2.4 Affaiblissement", "text": "Shafer (1976) a propos\u00e9 la proc\u00e9dure d\u2019affaiblissement suivante :\n\u03b1m\u2126(A) = \u03b1\u00d7m\u2126(A) \u2200A \u2282 \u2126 (8) \u03b1m\u2126(\u2126) = 1\u2212 \u03b1\u00d7 (1\u2212m\u2126(\u2126)) (9)\no\u00f9 \u03b1 est un facteur d\u2019affaiblissement de [0, 1]. Cette proc\u00e9dure est g\u00e9n\u00e9ralement employ\u00e9e pour affaiblir les fonctions de masse par la fiabilit\u00e9 \u03b1 de leurs sources. Cette proc\u00e9dure a pour effet d\u2019augmenter la masse sur l\u2019ignorance \u2126. Smets (1993) a justifi\u00e9 cette proc\u00e9dure en consid\u00e9rant que :\nm\u2126[F ](A) = m\u2126(A) (10) m\u2126[F\u0304 ](A) = m\u2126(X) (11)\no\u00f9m\u2126(X) = 1 siX = \u2126 et 0 sinon, F et F\u0304 repr\u00e9sentent la fiabilit\u00e9 et la non fiabilit\u00e9 etm\u2126[F ] est une fonction de masse conditionnellement \u00e0 la fiabilit\u00e9 F . Soit F = {F, F\u0304} le cadre de discernement correspondant, et mF la fonction de masse repr\u00e9sentant la connaissance sur la fiabilit\u00e9 de la source : {\nmF (F ) = \u03b1 mF (F) = 1\u2212 \u03b1. (12)\nAfin de combiner les deux sources d\u2019informations fournissant les deux fonctions de masse m\u2126[F ] et mF , il faut pouvoir les repr\u00e9senter dans le m\u00eame espace \u2126\u00d7F . Ainsi, nous devons effectuer une extension \u00e0 vide sur la fonction de massemF , op\u00e9ration que l\u2019on notemF\u2191\u2126\u00d7F :\nmF\u2191\u2126\u00d7F (Y ) = { mF (X) si Y = \u2126\u00d7X, X \u2286 F 0 sinon (13)\nDans le cas de la fonction de masse m\u2126[F ], il faut d\u00e9conditionner :\nm\u2126[F ]\u21d1\u2126\u00d7F ((A\u00d7 F ) \u222a (\u2126\u00d7 F )) = m\u2126 [F ] (A) , A \u2286 \u2126 (14)\nIl est ainsi possible d\u2019effectuer la combinaison :\nm\u2126\u00d7F\u2229\u00a9 (B) = m F\u2191\u2126\u00d7F \u2229\u00a9m\u2126[F ]\u21d1\u2126\u00d7F (B), \u2200B \u2282 \u2126\u00d7F (15)\nEnsuite il faut marginaliser la fonction de masse obtenue pour revenir dans l\u2019espace \u2126 :\nm\u2126\u00d7F\u2193\u2126 (A) = \u2211\n{B\u2286\u2126\u00d7F |Proj(B\u2193\u2126)=A}\nm\u2126\u00d7F\u2229\u00a9 (B) (16)\no\u00f9 Proj (Y \u2193 \u2126) est la projection de Y sur \u2126. Nous retrouvons ainsi :\n\u03b1m\u2126(A) = m\u2126\u00d7F\u2193\u2126 (A) (17)\nMercier (2006) a propos\u00e9 une extension de cet affaiblissement en contextualisant le coefficient d\u2019affaiblissement \u03b1 en fonction des sous-ensembles de \u2126. Smets (1993) d\u00e9taille l\u2019extension \u00e0 vide, le d\u00e9conditionnement ainsi que la marginalisation."}, {"heading": "2.5 Transformation pignistique", "text": "La prise de d\u00e9cision dans la th\u00e9orie des fonctions de croyance est fond\u00e9e sur des probabilit\u00e9s pignistiques issues de la transformation pignistique propos\u00e9e par Smets (2005). Cette transformation calcule une probabilit\u00e9 pignistique \u00e0 partir des fonctions de masse en vue de\nprendre une d\u00e9cision. Si un expert fournit une fonction de masse refl\u00e9tant son avis sur la solution d\u2019un probl\u00e8me pr\u00e9cis, la probabilit\u00e9 pignistique refl\u00e8te la probabilit\u00e9 de chaque hypoth\u00e8se. La probabilit\u00e9 pignisitique BetP d\u2019un \u00e9l\u00e9ment A \u2208 \u2126 est calcul\u00e9e comme suit :\nBetP (A) = \u2211\nC\u22082\u2126,C 6=\u2205\n|A \u2229 C| |C| m\u2126(C) 1\u2212m\u2126(\u2205) . (18)\nLa d\u00e9cision peut \u00eatre prise suivant le principe du maximum de la probabilit\u00e9 pignistique."}, {"heading": "2.6 Classification non-supervis\u00e9e", "text": "Nous proposons ici d\u2019utiliser un algorithme de classification non-supervis\u00e9e de type Cmoyenne, utilisant une distance sur les fonctions de masse d\u00e9finie par Jousselme et al. (2001) comme propos\u00e9 par Ben Hariz et al. (2006), Chebbah et al. (2012a) et Chebbah et al. (2012b). Soit un ensemble T contenant n objets oi : 1 \u2264 i \u2264 n \u00e0 classifier dans C clusters. Les valeurs des oi sont des fonctions de masse m\u2126i d\u00e9finies sur un cadre de discernement \u2126. Le but est de classifier les n fonctions de masse valeurs des objets oi. Les fonctions de masse m\u2126i sont fournies par la m\u00eame source, c\u2019est-\u00e0-dire qu\u2019une m\u00eame source a attribu\u00e9 les valeurs des objets oi. Appliquer un algorithme de classification non-supervis\u00e9e sur ces fonctions de masse revient \u00e0 regrouper les fonctions de masse ayant des \u00e9l\u00e9ments focaux non contradictoires. Une mesure de dissimilarit\u00e9 D(oi, Clk) permet de mesurer la dissimilarit\u00e9 entre un objet oi et un cluster Clk comme suit :\nD(oi, Clk) = 1\nnk nk\u2211 j=1 d(m\u2126i ,m \u2126 j ) (19)\nd(m\u21261 ,m \u2126 2 ) =\n\u221a 1\n2 (m\u21261 \u2212m\u21262 )tD(m\u21261 \u2212m\u21262 ), (20)\nD(A,B) = { 1 si A = B = \u2205 |A\u2229B| |A\u222aB| \u2200A,B \u2208 2 \u2126 (21)\nLa dissimilarit\u00e9 d\u2019un objet oi et un cluster Clk est d\u00e9finie par la moyenne des distances entre la fonction de masse m\u2126i valeur de cet objet et toutes les nk fonctions de masse valeurs des oj : 1 \u2264 j \u2264 nk objets contenus dans le cluster Clk. Chaque objet est affect\u00e9 au cluster qui lui est le plus similaire (ayant une valeur de dissimilarit\u00e9 minimale) de mani\u00e8re it\u00e9rative jusqu\u2019\u00e0 ce qu\u2019une r\u00e9partition stable soit obtenue.\n\u00c0 la fin de la classification non-supervis\u00e9e, C clusters contenant chacun un certain nombre d\u2019objets sont obtenus. Dans cet article nous supposons que le nombre de clusters C soit \u00e9gal \u00e0 la cardinalit\u00e9 du cadre de discernement (C = |\u2126|) puisque \u2126 repr\u00e9sente les classes possibles dans un probl\u00e8me de classification. L\u2019utilisation de cette mesure de dissimilarit\u00e9 pour la classification assure le regroupement des objets dont les valeurs (fonctions de masse) ne sont pas contradictoires, c\u2019est-\u00e0-dire les \u00e9l\u00e9ments focaux sont compatibles et intersectent.\nNotons que l\u2019algorithme de classification non-supervis\u00e9e est fond\u00e9e sur une distance sur les fonctions de masse et non pas sur des centres mobiles comme propos\u00e9 par Ben Hariz et al. (2006) parce que la distance est fortement sensible au nombre d\u2019\u00e9l\u00e9ments focaux des centres. Il suffit d\u2019avoir des fonctions de masse avec diff\u00e9rents \u00e9l\u00e9ments focaux dans le m\u00eame cluster pour que la fonction de masse combin\u00e9e (le centre) ait beaucoup d\u2019\u00e9l\u00e9ments focaux.\n3 Ind\u00e9pendance\nL\u2019ind\u00e9pendance a \u00e9t\u00e9 introduite en premier dans le cadre de la th\u00e9orie des probabilit\u00e9s pour mod\u00e9liser l\u2019ind\u00e9pendance statistique des \u00e9v\u00e8nements. Avec les probabilit\u00e9s, deux \u00e9v\u00e8nements A et B sont ind\u00e9pendants si P (A \u2229B) = P (A)\u00d7 P (B) ou encore si P (A|B) = P (A).\nLes fonctions de masse peuvent \u00eatre per\u00e7ues comme des probabilit\u00e9s subjectives fournies par des sources s\u2019exprimant sur un probl\u00e8me \u00e9tant donn\u00e9 un ensemble de connaissances ou d\u2019informations appel\u00e9 corpus de croyance. Dans le cas d\u2019une hypoth\u00e8se d\u2019ind\u00e9pendance cognitive des sources, les corpus de croyance doivent \u00eatre distincts et aucune communication entre les sources n\u2019est tol\u00e9r\u00e9e.\nShafer (1976) d\u00e9finit l\u2019ind\u00e9pendance cognitive des variables comme \u00e9tant le non changement des croyances de l\u2019une des variables si une nouvelle croyance \u00e9l\u00e9mentaire appara\u00eet sur l\u2019autre. Il d\u00e9finit \u00e9galement l\u2019ind\u00e9pendance \u00e9videntielle, deux variables sont \u00e9videntiellement ind\u00e9pendantes par rapport \u00e0 une fonction de masse si cette fonction de masse peut \u00eatre retrouv\u00e9e en combinant les fonctions de masse de ces variables.\nD\u00e9finition 1. \"Two frames of discernment may be called cognitively independent with respect to the evidence if new evidence that bears on only one of them will not change the degrees of support for propositions discerned by the other\" 3 (Shafer (1976), page 149).\nD\u00e9finition 2. \"Two frames of discernment are evidentially independent with respect to a support function if that support function could be obtained by combining evidence that bears on only one of them with evidence that bears on only the other\" 4 (Shafer (1976), page 149).\nDans ce papier, nous nous int\u00e9ressons \u00e0 l\u2019ind\u00e9pendance des sources et non pas celle des variables.\nD\u00e9finition 3. Deux sources sont cognitivement ind\u00e9pendantes si elles ne communiquent pas et si leurs corpus de croyance sont distincts.\nNous d\u00e9finissons la d\u00e9pendance cognitive comme \u00e9tant la ressemblance du comportement g\u00e9n\u00e9ral de deux sources d\u00fb \u00e0 une d\u00e9pendance de connaissances ou \u00e0 une \u00e9ventuelle communication entre elles. Nous proposons une d\u00e9marche statistique afin d\u2019\u00e9tudier l\u2019ind\u00e9pendance cognitive de deux sources. Le but \u00e9tant soit de tenir compte de cette ind\u00e9pendance dans les fonctions de masse comme d\u00e9taill\u00e9 dans la section 5, soit faire les hypoth\u00e8ses adapt\u00e9es pour le choix du type de r\u00e8gles de combinaison \u00e0 appliquer. Nous introduisons d\u2019abord la mesure d\u2019ind\u00e9pendance de deux sources S1 et S2, not\u00e9e Id(S1, S2), comme \u00e9tant l\u2019ind\u00e9pendance de S1 de S2. Cette mesure v\u00e9rifie les axiomes suivants :\n1. Non-n\u00e9gative : L\u2019ind\u00e9pendance d\u2019une source S1 de S2, Id(S1, S2) est une valeur qui est, soit nulle si S1 est compl\u00e8tement d\u00e9pendante de S2, soit strictement positive.\n3. Deux cadres de discernement sont cognitivement ind\u00e9pendants par rapport \u00e0 une croyance si toute nouvelle croyance apparaissant sur l\u2019un n\u2019affecte pas la croyance sur l\u2019autre.\n4. Deux cadres de discernement sont \u00e9videntiellement ind\u00e9pendants par rapport \u00e0 une fonction de masse si cette fonction de masse est obtenue en combinant les croyances sur ces cadres de discernement.\n2. Normalis\u00e9e : Id(S1, S2) \u2208 [0, 1], si Id est nulle alors S1 est compl\u00e8tement d\u00e9pendante de S2. Si Id = 1, alors S1 est compl\u00e8tement ind\u00e9pendante de S2 autrement c\u2019est un degr\u00e9 de ]0, 1[.\n3. Non-sym\u00e9trique : Si S1 est ind\u00e9pendante de S2, cela n\u2019implique pas forcement que S2 soit ind\u00e9pendante de S1. S1 et S2 peuvent \u00eatre simultan\u00e9ment ind\u00e9pendantes avec des degr\u00e9s d\u2019ind\u00e9pendance \u00e9gaux ou diff\u00e9rents.\n4. Identit\u00e9 : Id(S1, S1) = 0, toute source est compl\u00e8tement d\u00e9pendante d\u2019elle m\u00eame.\nSi deux sources S1 et S2 sont d\u00e9pendantes, alors des \u00e9l\u00e9ments focaux similaires sont choisis pour s\u2019exprimer sur des objets similaires. L\u2019approche propos\u00e9e dans ce papier est une approche statistique pour mesurer le degr\u00e9 d\u2019ind\u00e9pendance cognitive. Nous proposons ainsi de classifier toutes les fonctions de masse des deux sources et de comparer les clusters obtenus. La classification non supervis\u00e9e regroupe les objets ayant pour valeurs des fonctions de masse similaires.\nSi les clusters des deux sources sont similaires, c\u2019est que les sources ont tendance \u00e0 choisir des \u00e9l\u00e9ments focaux similaires voire non contradictoires pour les m\u00eames objets, alors il est fort probable qu\u2019elles soient d\u00e9pendantes. Si les clusters sont fortement li\u00e9s c\u2019est-\u00e0-dire qu\u2019ils contiennent des fonctions de masse relatives aux m\u00eames objets, alors les sources sont d\u00e9pendantes cognitivement."}, {"heading": "3.1 Appariement des clusters", "text": "Dans de nombreuses applications, plusieurs sources s\u2019expriment sur la m\u00eame probl\u00e9matique et fournissent diff\u00e9rentes fonctions de masse comme valeurs aux m\u00eames objets. L\u2019algorithme de classification non-supervis\u00e9e est appliqu\u00e9 aux fonctions de masse de chaque source s\u00e9par\u00e9ment et puis ces clusters doivent \u00eatre compar\u00e9s dans le but de voir s\u2019il y a un lien entre eux. Plus les liens entre ces clusters sont forts plus les sources ont tendance \u00e0 \u00eatre d\u00e9pendantes.\nSoient deux sources S1 et S2, fournissant chacune n fonctions de masse pour les m\u00eames objets. Ceci exprime le fait que la fonction de masse m\u2126i fournie par S1 et celle fournie par S2 se r\u00e9f\u00e8rent au m\u00eame objet oi. Apr\u00e8s avoir classifi\u00e9 les fonctions de masse de S1 et S2, la matrice de correspondance des clusters M est obtenue par :\nM1 =  \u03b211,1 \u03b2 1 1,2 . . . \u03b2 1 1,C . . . . . . . . . . . . \u03b21k,1 \u03b2 1 k,2 . . . \u03b2 1 k,C\n. . . . . . . . . . . . \u03b21C,1 \u03b2 1 C,2 . . . \u03b2 1 C,C\n and M2 =  \u03b221,1 \u03b2 2 1,2 . . . \u03b2 2 1,C . . . . . . . . . . . . \u03b22k,1 \u03b2 2 k,2 . . . \u03b2 2 k,C\n. . . . . . . . . . . . \u03b22C,1 \u03b2 2 C,2 . . . \u03b2 2 C,C  (22) avec\n\u03b2iki,kj = |Cliki \u2229 Cl j kj |\n|Cliki | (23)\nNotons que \u03b2iki,kj est la similarit\u00e9 des clusters Cl i ki de Si et Cl j kj de Sj par rapport \u00e0 Si avec {i, j} \u2208 {1, 2} et i 6= j.\nLa similarit\u00e9 de deux clusters est la proportion des objets class\u00e9s simultan\u00e9ment dans Cliki et Cljkj par rapport \u00e0 la cardinalit\u00e9 du cluster de la source r\u00e9f\u00e9rente. Si par exemple, le cluster Cl11 de S1 contient les 5 objets {o1, o5, o9, o12, o15} et le clusterCl25 de S2 contient les 3 objets\n{o1, o2, o9}. \u03b211,5 est la similarit\u00e9 de Cl25 de S2 par rapport \u00e0 Cl11 de S1 repr\u00e9sentant la proportion des objets deCl25 qui sont aussi class\u00e9s dansCl 1 1 de S1 donc \u03b2 1 1,5 = 2 5 et \u03b2 2 1,5 = 2 3 . Notons que l\u2019algorithme de classification non-supervis\u00e9e est appliqu\u00e9 sur les m\u00eames objets ayant diff\u00e9rentes valeurs puisqu\u2019elles sont fournies par deux sources diff\u00e9rentes S1 et S2. Les matrices M1 etM2 sont ainsi diff\u00e9rentes puisque les clusters de S1 et ceux de S2 sont diff\u00e9rents. Notons \u00e9galement que d\u2019autres coefficients de similarit\u00e9 peuvent aussi \u00eatre utilis\u00e9s. Une fois les deux matrices de correspondances M1 et M2 calcul\u00e9es, une correspondance entre les clusters est \u00e9tablie suivant l\u2019algorithme 1. Chaque cluster est li\u00e9 au cluster qui lui est le plus similaire, ayant le \u03b2 maximal, en v\u00e9rifiant que deux clusters de la m\u00eame source ne peuvent pas \u00eatre li\u00e9s au m\u00eame cluster de l\u2019autre source. Par exemple, nous ne pouvons pas avoir les deux clusters Cl11 et Cl 1 3 de S1 li\u00e9s au m\u00eame cluster Cl 2 3 de S2. Pour d\u00e9terminer la correspondance des clusters pour chaque source, les deux clusters ayant la plus grande similarit\u00e9 \u03b2 sont reli\u00e9s et \u00e9cart\u00e9s de l\u2019ensemble des clusters \u00e0 apparier. La recherche de correspondances des clusters est faite pour les deux sources. Deux correspondances diff\u00e9rentes peuvent \u00eatre obtenues pour les deux sources.\nAlgorithm 1 Appariement de clusters Require: M matrice de correspondances.\n1: while M est non vide do 2: Rechercher le maximum de M ainsi que les indices l et c du maximum. 3: Apparier les clusters l et c. 4: Enlever la ligne l et la colonne c de M . 5: end while 6: return Un appariement de clusters."}, {"heading": "3.2 Ind\u00e9pendance des clusters", "text": "Une fois la correspondance des clusters \u00e9tablie, une fonction de masse d\u00e9finissant l\u2019ind\u00e9pendance de chaque couple de cluster est d\u00e9duite. Ceci revient \u00e0 avoir un agent ayant les correspondances des clusters (ki, kj) avec les similarit\u00e9s correspondantes \u03b2iki,kj comme corpus de croyance pour s\u2019exprimer sur l\u2019ind\u00e9pendance de ces clusters. Pour r\u00e9sumer, nous supposons que les deux sources S1 et S2 fournissent n fonctions de masse comme valeurs aux n objets oi. Les fonctions de masse des deux sources sont classifi\u00e9es chacune \u00e0 part en utilisant l\u2019algorithme de classification d\u00e9taill\u00e9 dans la section 3.1. Diff\u00e9rents C clusters sont le r\u00e9sultat de classification des fonctions de masse de S1 et ceux de S2. Apr\u00e8s appariement de clusters, les clusters de S1 sont li\u00e9s aux clusters de S2 qui leur sont similaires et ceux de S2 sont \u00e9galement li\u00e9s aux clusters de S1 les plus similaires. Diff\u00e9rents appariements sont obtenus pour S1 et S2. Rappelons que le but est d\u2019estimer l\u2019ind\u00e9pendance cognitive des sources \u00e0 partir d\u2019un ensemble de fonctions de masse fourni par chacune.\nDans cette section, nous d\u00e9finissons l\u2019ind\u00e9pendance de chaque couple de clusters li\u00e9s (k1, k2) comme une fonction de masse d\u00e9finie sur le cadre de discernement I = {I\u0304 , I}, o\u00f9\nI\u0304 repr\u00e9sente la d\u00e9pendance et I l\u2019ind\u00e9pendance : mIki,kj (I) = \u03b1 i ki,kj (1\u2212 \u03b2iki,kj ) mIki,kj (I\u0304) = \u03b1 i ki,kj\n\u03b2iki,kj mIki,kj (I) = 1\u2212 \u03b1 i ki,kj\n(24)\nLe coefficient \u03b1iki,kj est un facteur d\u2019affaiblissement utilis\u00e9 pour tenir compte du nombre d\u2019objets contenus dans les clusters de la source r\u00e9f\u00e9rente. Si deux clusters contenant tr\u00e8s peu de fonctions de masse sont reli\u00e9s et que deux autres clusters contenant beaucoup plus d\u2019objets sont aussi reli\u00e9s avec un m\u00eame degr\u00e9 de similarit\u00e9, les fonctions de masse des deux couples de clusters ne doivent pas avoir un m\u00eame poids. Bien que ce facteur ne soit pas encore d\u00e9fini, il d\u00e9pend du nombre d\u2019objets dans le cluster de la source r\u00e9f\u00e9rente ainsi que le nombre total d\u2019objets (fonctions de masse).\nUne fonction de masse est d\u00e9finie pour chaque couple de clusters appari\u00e9s pour chacune des sources. Pour avoir une fonction de masse sur l\u2019ind\u00e9pendance globale de chaque source, toutes ces fonctions de masse sont combin\u00e9es avec la moyenne. Pour r\u00e9sumer, les C clusters de S1 sont appari\u00e9s aux C clusters de S2, une fonction de masse est ainsi obtenue pour chaque couple de clusters reli\u00e9s afin de refl\u00e9ter leur degr\u00e9 d\u2019ind\u00e9pendance. C fonctions de masse sont alors obtenues pour chaque source. Notons que toute autre r\u00e8gle 5 idempotente et commutative peut \u00eatre utilis\u00e9e pour combiner ces fonctions de masse. L\u2019idempotence de la r\u00e8gle est exig\u00e9e parce que l\u2019ind\u00e9pendance des clusters appari\u00e9s peut \u00eatre la m\u00eame, l\u2019ind\u00e9pendance de la source doit aussi \u00eatre \u00e9gale \u00e0 l\u2019ind\u00e9pendance des clusters dans ce cas. La r\u00e8gle de combinaison doit \u00e9galement \u00eatre commutative puisqu\u2019elle mixe C fonctions de masse \u00e0 la fois. La combinaison de ces C fonctions de masse est une fonction de masse mIi d\u00e9crivant l\u2019ind\u00e9pendance globale de la source Si par rapport \u00e0 Sj : mIi (I) = 1 C C\u2211 ki=1 mIki,kj (I) mIi (I\u0304) = 1 C C\u2211 ki=1 mIki,kj (I\u0304) mIi (I\u0304 \u222a I) = 1C C\u2211\nki=1\nmIki,kj (I\u0304 \u222a I)\n(25)\nou encore :  mIi (I) = 1 C C\u2211 ki=1 \u03b1iki,kj (1\u2212 \u03b2 i ki,kj ) mIi (I\u0304) = 1 C C\u2211 ki=1 \u03b1iki,kj\u03b2 i ki,kj mIi (I\u0304 \u222a I) = 1C C\u2211\nki=1\n(1\u2212 \u03b1iki,kj )\n(26)\n5. La r\u00e8gle doit permettre de combiner tous types de fonctions de masse (la r\u00e8gle prudente de Den\u0153ux (2008) ne peut pas \u00eatre utilis\u00e9e dans ce cas puisqu\u2019elle est limit\u00e9e \u00e0 la combinaison des fonctions de masse non dogmatiques).\nLes probabilit\u00e9s pignistiques calcul\u00e9es \u00e0 partir de la fonction de masse combin\u00e9e permettent la prise de d\u00e9cision sur l\u2019ind\u00e9pendance des sources. L\u2019ind\u00e9pendance de la source S1 de la source S2, Id(S1, S2) n\u2019est autre que la probabilit\u00e9 pignistique de I , Id(S1, S2) = BetP (I) et I\u0304d(S1, S2) = BetP (I\u0304) ce qui revient \u00e0 \u00e9crire Id comme suit : Id(Si, Sj) = 1 C C\u2211 ki=1 [\u03b1iki,kj \u03b2 i ki,kj + 1 2 (1\u2212 \u03b1iki,kj )] I\u0304d(Si, Sj) = 1 C\nC\u2211 ki=1 [\u03b1iki,kj (1\u2212 \u03b2 i ki,kj ) + 1 2 (1\u2212 \u03b1iki,kj )]\n(27)\nSi Id(S1, S2) < I\u0304d(S1, S2), alors S1 est d\u00e9pendante de S2, dans le cas contraire S1 est ind\u00e9pendante de S2. Id n\u2019est pas forcement sym\u00e9trique, c\u2019est-\u00e0-dire que le cas o\u00f9 Id(S1, S2) 6= Id(S2, S1) peut \u00eatre fr\u00e9quent puisque la correspondance des clusters est diff\u00e9rente pour S1 et S2, ainsi ces fonctions de masse d\u2019ind\u00e9pendance des clusters li\u00e9s sont aussi diff\u00e9rentes. Cette propri\u00e9t\u00e9 permet par exemple de mettre en \u00e9vidence une ind\u00e9pendance de S1 par rapport \u00e0 S2 et une ind\u00e9pendance de S2 par rapport \u00e0 S1."}, {"heading": "4 D\u00e9pendance positive ou n\u00e9gative", "text": "La mesure Id(S1, S2) informe sur l\u2019ind\u00e9pendance ou a contrario la d\u00e9pendance de la source S1 par rapport \u00e0 la source S2 permettant par exemple de choisir la r\u00e8gle de combinaison \u00e0 utiliser ou encore int\u00e9grer cette information dans ses fonctions de masse. Quant au moins l\u2019une des sources S1 ou S2 est d\u00e9pendante de l\u2019autre (Id(S1, S2) < I\u0304d(S1, S2) ou Id(S2, S1) < I\u0304d(S2, S1)), il est alors pr\u00e9f\u00e9rable d\u2019utiliser les r\u00e8gles de Den\u0153ux (2006); Boubaker et al. (2013); Elouedi et Mellouli (1998) sinon les r\u00e8gles de Dubois et Prade (1988); Martin et Osswald (2007); Murphy (2000); Smets et Kennes (1994); Yager (1987) permettent par exemple de redistribuer la masse de l\u2019ensemble vide. Dans le cas de sources d\u00e9pendantes Id n\u2019est pas suffisante pour indiquer le type de la d\u00e9pendance.\nDeux sources d\u00e9pendantes peuvent \u00eatre positivement ou n\u00e9gativement d\u00e9pendantes. Si S1 est d\u00e9pendante de S2, elle peut soit avoir les m\u00eames croyances si elle lui est positivement d\u00e9pendante soit avoir des croyances contradictoires si elle lui est n\u00e9gativement d\u00e9pendante.\nSi par exemple Id(S1, S2) < I\u0304d(S1, S2), alors S1 est d\u00e9pendante de S2 ce qui signifie que les clusters de S1 ressemblent aux clusters de S2. Nous d\u00e9finissons une mesure de conflit entre les clusters de S1 et S2 quantifiant cette d\u00e9pendance que nous qualifions de positive ou n\u00e9gative. Si les clusters li\u00e9s ne sont pas conflictuels alors S1 est positivement d\u00e9pendante de S2 sinon elle est n\u00e9gativement d\u00e9pendante. Nous d\u00e9finissons alors le conflit entre les deux clusters d\u00e9pendants Cliki et Cl j kj\n({i, j} \u2208 {1, 2} et i 6= j) \u00e0 partir de la moyenne des distances entre les fonctions de masse des objets en commun :\n Conf(Cliki , Cl j kj ) = 1 |Cliki\u2229Cl j kj | \u2211 l\u2208E(Cliki ,Cl j kj ) d(m\u2126,il ,m \u2126,j l ) si |Cliki \u2229 Cl j kj | 6= 0\n1 sinon (28)\navec\nE(Cliki , Cl j kj ) = {l \u2208 [1, n], n = |Cliki \u2229 Cl j kj |,m\u2126,il \u2208 Cl i kiet m \u2126,j l \u2208 Cl j kj } (29)\nCette mesure de conflit est la moyenne des conflits entre les objets contenus dans les clusters Cliki et Cl j kj\nce qui explique le fait de ne consid\u00e9rer que les objets communs. Il ne peut s\u2019agir de conflit entre sources que lorsqu\u2019elles s\u2019expriment sur les m\u00eames probl\u00e8mes c\u2019est \u00e0 dire les m\u00eames objets. Le conflit est calcul\u00e9 pour chaque couple de clusters li\u00e9s. Alors si Cl11 de S1 est li\u00e9 \u00e0 Cl25 de S2 et que S1 est d\u00e9pendante de S2 alors le conflit entre Cl 1 1 et Cl 2 5 est la moyenne des distances des fonctions de masse relatives aux objets qui sont \u00e0 la fois dans Cl11 et Cl 2 5. Une fonction de masse d\u00e9finie sur le cadre de discernement P = {I, P, P\u0304} (o\u00f9 P repr\u00e9sente la d\u00e9pendance positive et P\u0304 la d\u00e9pendance n\u00e9gative) d\u00e9crivant cette d\u00e9pendance est obtenue pour chaque couple de clusters :{\nmPki,kj [I\u0304](P ) = 1\u2212 Conf(Cl i ki , Cljkj ) mPki,kj [I\u0304](P\u0304 ) = Conf(Cl i ki , Cljkj )\n(30)\nNotons que le conflit entre les clusters refl\u00e8te la contradiction entre ces clusters. Puisque les clusters de chaque source groupent des fonctions de masse ayant les \u00e9l\u00e9ments focaux non contradictoires, alors le conflit mesur\u00e9 par l\u2019\u00e9quation (28) compare les clusters en mesurant la contradiction entre des \u00e9l\u00e9ments focaux des fonctions de masse des deux clusters. Plus le conflit est important, plus les sources sont d\u00e9pendantes n\u00e9gativement mais par contre moins il est important plus les sources sont d\u00e9pendantes positivement.\nNotons que ces fonctions de masse sont conditionnelles puisque la d\u00e9pendance positive ou n\u00e9gative des clusters n\u2019est mesur\u00e9e qu\u2019avec une forte hypoth\u00e8se de d\u00e9pendance des clusters li\u00e9s. L\u2019hypoth\u00e8se de d\u00e9pendance ou encore de non ind\u00e9pendance des clusters explique le fait que les fonctions de masse de l\u2019\u00e9quation (30) soient conditionn\u00e9es sur I\u0304 ou encore sur {P\u222aP\u0304}. Afin de pouvoir combiner les fonctions de masse (24) et (30) pour tenir compte du degr\u00e9 de d\u00e9pendance des clusters dans la fonction de masse de la d\u00e9pendance positive ou n\u00e9gative, il faut d\u00e9conditionner les fonctions de masse conditionnelles et red\u00e9finir les deux fonctions de masse sur un cadre le discernement commun P . Le d\u00e9conditionnement de la fonction de masse conditionnelle de l\u2019\u00e9quation (30) utilisant l\u2019\u00e9quation (7) permet de retrouver la fonction de masse la moins informative en supprimant l\u2019hypoth\u00e8se forte sur la d\u00e9pendance de tous les clusters li\u00e9s. Les fonctions de masse obtenues sont alors :{\nmPki,kj (P \u222a I) = 1\u2212 Conf(Cl i ki , Cljkj ) mPki,kj (P\u0304 \u222a I) = Conf(Cl i ki , Cljkj )\n(31)\nLe cadre de discernement I = {I\u0304 , I} peut \u00eatre raffin\u00e9 en raffinant l\u2019hypoth\u00e8se I\u0304 = {P \u222a P\u0304}, ceci m\u00e8nera au cadre de discernement raffin\u00e9 P . Les fonctions de masse marginales de la d\u00e9pendance des clusters li\u00e9s de l\u2019\u00e9quation (24) deviennent apr\u00e8s raffinement : mPki,kj (I) = \u03b1 i ki,kj (1\u2212 \u03b2iki,kj ) mPki,kj (P \u222a P\u0304 ) = \u03b1 i ki,kj\n\u03b2iki,kj mPki,kj (I \u222a P \u222a P\u0304 ) = 1\u2212 \u03b1 i ki,kj\n(32)\nNous d\u00e9finissons ainsi la fonction de masse de l\u2019ind\u00e9pendance, d\u00e9pendance positive et d\u00e9pendance n\u00e9gative de chaque couple de clusters li\u00e9s de S1 et S2 apr\u00e8s combinaison conjonctive des fonctions de masse des \u00e9quations (31) et (32) d\u00e9finies sur le cadre de discernement P : mPki,kj (I) = \u03b1 i ki,kj (1\u2212 \u03b2iki,kj ) mPki,kj (P ) = \u03b1 i ki,kj \u03b2iki,kj (1\u2212 Conf(Cl i ki , Cljkj )) mPki,kj (P\u0304 ) = \u03b1 i ki,kj \u03b2iki,kjConf(Cl i ki , Cljkj ) mPki,kj (I \u222a P ) = (1\u2212 \u03b1 i ki,kj ) (1\u2212 Conf(Cliki , Cl j kj ))\nmPki,kj (I \u222a P\u0304 ) = (1\u2212 \u03b1 i ki,kj )Conf(Cliki , Cl j kj )\n(33)\nLa fonction de masse g\u00e9n\u00e9rale sur la d\u00e9pendance de la source S1 par rapport \u00e0 S2 est donn\u00e9e par :\nmP(A) = 1\nC C\u2211 ki=1 mPki,kj (A) (34)\navec {i, j} \u2208 {1, 2} et i 6= j, o\u00f9 k2 est le cluster de la source S2 associ\u00e9 au cluster k1 de la source S1. Cette fonction de masse repr\u00e9sente ainsi l\u2019ensemble des croyances \u00e9l\u00e9mentaires sur l\u2019ind\u00e9pendance, la d\u00e9pendance positive et n\u00e9gative de la source S1 face \u00e0 la source S2. Cette fonction de masse est la combinaison avec la moyenne de toutes les fonctions de masse sur l\u2019ind\u00e9pendance, d\u00e9pendance positive et d\u00e9pendance n\u00e9gative de tous les clusters li\u00e9s.\nNotons aussi qu\u2019une source est positivement d\u00e9pendante d\u2019elle-m\u00eame puisqu\u2019en appliquant l\u2019algorithme de classification nous obtenons exactement la m\u00eame r\u00e9partition de classes ce qui impliquera Id(S, S) = 0 et mP(P ) = 1. Le degr\u00e9 d\u2019ind\u00e9pendance est la probabilit\u00e9 pignistique de l\u2019hypoth\u00e8se I , BetP (I), ceux des d\u00e9pendances positive et n\u00e9gative sont respectivement BetP (P ) et BetP (P\u0304 )."}, {"heading": "5 Int\u00e9gration de l\u2019ind\u00e9pendance dans une fonction de masse", "text": "Nous avons vu que l\u2019ind\u00e9pendance est g\u00e9n\u00e9ralement une information suppl\u00e9mentaire n\u00e9cessaire \u00e0 la fusion d\u2019informations, mais non prise en compte dans le formalisme choisi. La section 3 propose une mod\u00e9lisation et estimation d\u2019une mesure d\u2019ind\u00e9pendance dans le cadre de la th\u00e9orie des fonctions de croyance. Nous allons ici nous appuyer sur le principe de l\u2019affaiblissement pr\u00e9sent\u00e9 dans la section 2.4 afin de tenir compte de l\u2019ind\u00e9pendance dans les fonctions de masse en vue de la combinaison.\nEn effet, lors de la combinaison conjonctive par exemple, l\u2019hypoth\u00e8se d\u2019ind\u00e9pendance cognitive des sources d\u2019informations est n\u00e9cessaire. Si les sources sont d\u00e9pendantes on peut penser qu\u2019elles ne devraient pas \u00eatre combin\u00e9es par ce biais. Cependant, comme le montre la section 3 les sources peuvent avoir des degr\u00e9s de d\u00e9pendance et d\u2019ind\u00e9pendance. L\u2019information fournie sur l\u2019ind\u00e9pendance n\u2019est pas cat\u00e9gorique. Ainsi, combiner deux sources fortement ind\u00e9pendantes devraient revenir \u00e0 la combinaison de deux sources ind\u00e9pendantes. Si une source est d\u00e9pendante d\u2019une autre source, nous pouvons consid\u00e9rer que cette premi\u00e8re source ne doit pas influer la combinaison avec la seconde. Ainsi cette source doit repr\u00e9senter l\u2019\u00e9l\u00e9ment neutre de la combinaison.\nDans ce cas, il suffit d\u2019appliquer la proc\u00e9dure d\u2019affaiblissement de la section 2.4 sur la fonction de masse m\u2126 de la source S1 en consid\u00e9rant l\u2019ind\u00e9pendance donn\u00e9e par la fonction de masse de l\u2019\u00e9quation (34) au lieu de celle de l\u2019\u00e9quation (12) dans le cas de la fiabilit\u00e9.\n\u00c0 pr\u00e9sent, nous distinguons la d\u00e9pendance positive de la d\u00e9pendance n\u00e9gative. Si une source est d\u00e9pendante positivement d\u2019une autre source, il ne faut pas en tenir compte et donc tendre vers un r\u00e9sultat de combinaison qui prendrait cette premi\u00e8re source comme un \u00e9l\u00e9ment neutre. Enfin si une source est d\u00e9pendante n\u00e9gativement d\u2019une autre source, il peut \u00eatre int\u00e9ressant de marquer cette d\u00e9pendance conflictuelle en augmentant la masse sur l\u2019ensemble vide.\nPour r\u00e9aliser ce sch\u00e9ma, nous proposons d\u2019affaiblir les fonctions de masse d\u2019une source S1 en fonction de sa mesure d\u2019ind\u00e9pendance \u00e0 une autre source S2, donn\u00e9e par la fonction de masse mIi de l\u2019\u00e9quation (34).\nNous consid\u00e9rons ici une fonction de masse d\u2019une source m\u2126 en fonction de son ind\u00e9pendance ou d\u00e9pendance \u00e0 une autre source. Ainsi nous d\u00e9finissons : m \u2126[I](X) = m\u2126(X) m\u2126[P\u0304 ](X) = m\u2126(X) m\u2126(X) = 1 si X = \u2205, 0 sinon m\u2126[P ](X) = m\u2126(X) m\u2126(X) = 1 si X = \u2126, 0 sinon (35)\nSuivant la proc\u00e9dure d\u2019affaiblissement, nous effectuons une extension \u00e0 vide sur la fonction de masse mI :\nmI\u2191\u2126\u00d7I (Y ) = { mI (X) si Y = \u2126\u00d7X, X \u2286 I 0 sinon (36)\nLe d\u00e9conditionnement des fonctions de masse m\u2126[I], m\u2126[P ] et m\u2126[P\u0304 ] est donn\u00e9 par :\nm\u2126\u21d1\u2126\u00d7I [I]((A\u00d7 I) \u222a (\u2126\u00d7 I)) = m\u2126[I](A), A \u2286 \u2126 (37)\no\u00f9 I\u0304 = {P \u222a P\u0304} est un raffinement.\nm\u2126\u21d1\u2126\u00d7I [P\u0304 ]((A\u00d7 P\u0304 ) \u222a (\u2126\u00d7 {I \u222a P})) = m\u2126[P\u0304 ](A), A \u2286 \u2126 (38)\nm\u2126\u21d1\u2126\u00d7I [P ]((A\u00d7 P ) \u222a (\u2126\u00d7 {I \u222a P\u0304})) = m\u2126[P ](A), A \u2286 \u2126 (39)\nCe dernier d\u00e9conditionnement m\u00e8ne en fait \u00e0 la masse de l\u2019ignorance et est l\u2019\u00e9l\u00e9ment neutre de la combinaison conjonctive.\nNous r\u00e9alisons ensuite la combinaison conjonctive :\nm\u2126\u00d7IConj(Y ) = m I\u2191\u2126\u00d7I \u2229\u00a9m\u2126\u21d1\u2126\u00d7I [I] \u2229\u00a9m\u2126\u21d1\u2126\u00d7I\n[ P\u0304 ] (Y ), \u2200Y \u2282 \u2126\u00d7 I (40)\nLa marginalisation de la fonction de masse permet ensuite de revenir dans l\u2019espace \u2126 :\nm\u2126\u00d7I\u2193\u2126 (X) = \u2211\n{Y\u2286\u2126\u00d7I |Proj(Y \u2193\u2126)=X}\nm\u2126\u00d7IConj (Y ) (41)\nCette proc\u00e9dure r\u00e9alis\u00e9e pour la source S1 en rapport \u00e0 la source S2 peut \u00eatre r\u00e9alis\u00e9e pour la source S2 au regard de la source S1. Ainsi les deux fonctions de masse obtenues peuvent \u00eatre combin\u00e9es par la r\u00e8gle de combinaison conjonctive qui suppose l\u2019ind\u00e9pendance.\n6 Exp\u00e9rimentations Pour tester la m\u00e9thode pr\u00e9c\u00e9demment d\u00e9crite, nous avons g\u00e9n\u00e9r\u00e9 des fonctions de masse al\u00e9atoirement. Tous les tirages al\u00e9atoires sont fait suivant la loi uniforme. L\u2019algorithme 2 est utilis\u00e9 pour g\u00e9n\u00e9rer n fonctions de masse.\nAlgorithm 2 G\u00e9n\u00e9rer n fonctions de masse Require: |\u2126|, n : nombre des fonctions de masse \u00e0 g\u00e9n\u00e9rer\n1: for i = 1 to n do 2: Tirer al\u00e9atoirement | F |, le nombre d\u2019\u00e9l\u00e9ments focaux dans l\u2019intervalle [1, |2\u2126|]. 3: Tirer al\u00e9atoirement | F | \u00e9l\u00e9ments focaux not\u00e9s F . 4: Couper l\u2019intervalle [0, 1] en | F | \u22121 sous-intervalles al\u00e9atoires continus. 5: for j = 1 to | F | do 6: La masse de chaque \u00e9l\u00e9ment focal est la longueur de l\u2019un des sous-intervalles. 7: end for 8: end for 9: return n fonctions de masse.\nCet algorithme a \u00e9t\u00e9 utilis\u00e9 pour g\u00e9n\u00e9rer 100 fonctions de masse, d\u00e9finies sur un cadre de discernement de cardinalit\u00e9 5, pour deux sources S1 et S2 avec les trois hypoth\u00e8ses sur la d\u00e9pendance des sources (sources ind\u00e9pendantes, sources d\u00e9pendantes positivement et sources d\u00e9pendantes n\u00e9gativement), les r\u00e9sultats des tests sont pr\u00e9sent\u00e9s dans le tableau 1.\n1. Sources ind\u00e9pendantes : Supposons que deux sources S1 et S2 sont compl\u00e8tement ind\u00e9pendantes. Nous avons alors g\u00e9n\u00e9r\u00e9 100 fonctions de masse pour chaque source comme d\u00e9crit dans l\u2019algorithme 2 avec | \u2126 |= 5.\n2. Sources d\u00e9pendantes positivement : Lorsque deux sources S1 et S2 sont d\u00e9pendantes positivement, les classes de d\u00e9cision (en terme de probabilit\u00e9 pignistique), calcul\u00e9es \u00e0 partir des fonctions de masse qu\u2019elles fournissent, sont les m\u00eames. Les deux sources S1 et S2 s\u2019expriment de la m\u00eame mani\u00e8re puisqu\u2019elles sont d\u00e9pendantes. Nous avons g\u00e9n\u00e9r\u00e9 al\u00e9atoirement 100 fonctions de masse pour chaque source. Ces fonctions de masse sont modifi\u00e9es par la suite suivant l\u2019algorithme 3.\nAlgorithm 3 G\u00e9n\u00e9rer des fonctions de masse d\u00e9pendantes positivement Require: n fonctions de masse g\u00e9n\u00e9r\u00e9es al\u00e9atoirement avec l\u2019algorithme 2, Les classes de\nd\u00e9cision. 1: for i = 1 to n do 2: Recherche des \u00e9l\u00e9ments focaux F de chaque fonction de masse mi 3: for j = 1 to | F | do 4: La masse affect\u00e9e au j\u00e8me \u00e9l\u00e9ment focal est transf\u00e9r\u00e9e \u00e0 son union avec la classe de d\u00e9cision. 5: end for 6: end for 7: return n fonctions de masse modifi\u00e9es.\n3. Sources d\u00e9pendantes n\u00e9gativement : Lorsque deux sources S1 et S2 sont d\u00e9pendantes n\u00e9gativement, les classes de d\u00e9cision (en terme de probabilit\u00e9 pignistique), calcul\u00e9es \u00e0 partir des fonctions de masse qu\u2019elles fournissent, sont contradictoires mais d\u2019une fa\u00e7on ordonn\u00e9e. Les deux sources S1 et S2 sont d\u00e9pendantes mais l\u2019une des deux sources a tendance \u00e0 dire l\u2019oppos\u00e9 de l\u2019autre. Nous avons g\u00e9n\u00e9r\u00e9 al\u00e9atoirement 100 fonctions de masse pour chaque source. Ces fonctions de masse sont modifi\u00e9es par la suite suivant l\u2019algorithme 4.\nAlgorithm 4 G\u00e9n\u00e9rer des fonctions de masse d\u00e9pendantes n\u00e9gativement Require: n fonctions de masse g\u00e9n\u00e9r\u00e9es al\u00e9atoirement avec l\u2019algorithme 2 pour une source,\nLes classes de d\u00e9cision de l\u2019autre source, La correspondance de classes contradictoires. 1: for i = 1 to n do 2: Recherche des \u00e9l\u00e9ments focaux F de chaque fonction de masse mi. 3: for j = 1 to | F | do 4: Transf\u00e9rer la masse de l\u2019\u00e9l\u00e9ment focal j \u00e0 son union avec la classe contradictoire (la\nclasse contradictoire \u00e0 la classe de d\u00e9cision de mi) priv\u00e9 de la classe de d\u00e9cision de mi.\n5: end for 6: end for 7: return n fonctions de masse modifi\u00e9es.\nType de d\u00e9pendance Degr\u00e9s d\u2019ind\u00e9pendance, d\u00e9pendance positive et d\u00e9pendance n\u00e9gative\nInd\u00e9pendance Id(S1, S2) = 0.72, I\u0304d(S1, S2) = 0.28 Id(S2, S1) = 0.66, I\u0304d(S2, S1) = 0.34 D\u00e9pendance positive mI,1(I) = 0.26, mI,1(P ) = 0.56,mI,1(P\u0304 ) = 0.18 mI,2(I) = 0.35, mI,2(P ) = 0.5,mI,2(P\u0304 ) = 0.15 D\u00e9pendance n\u00e9gative mI,1(I) = 0.35, mI,1(P ) = 0.25,mI,1(P\u0304 ) = 0.4\nmI,2(I) = 0.38, mI,2(P ) = 0.18,mI,2(P\u0304 ) = 0.44\nTAB. 1 \u2013 R\u00e9sultats des tests sur 100 fonctions de masse g\u00e9n\u00e9r\u00e9es\nLa complexit\u00e9 temporelle 6 de l\u2019algorithme propos\u00e9 d\u00e9pend fortement de la cardinalit\u00e9 du cadre de discernement. Dans le cas illustr\u00e9, | \u2126 |= 5, la complexit\u00e9 est de quelques secondes mais elle peut \u00eatre beaucoup plus importante pour les plus grands cadres de discernement. Notons que la complexit\u00e9 temporelle de l\u2019algorithme de classification est optimis\u00e9e puisque les distances entre les fonctions de masse ne sont calcul\u00e9es qu\u2019une seule fois.\n6. Les exp\u00e9rimentations ainsi que la complexit\u00e9 temporelle ont \u00e9t\u00e9 test\u00e9 sous Matlab R2010a."}, {"heading": "6.1 Fonctionnement de l\u2019affaiblissement par la mesure d\u2019ind\u00e9pendance", "text": "Nous allons, dans un premier temps, illustrer le fonctionnement de l\u2019affaiblissement par la mesure d\u2019ind\u00e9pendance. Nous consid\u00e9rons ici un cadre de discernement \u2126 = {\u03c91, \u03c92, \u03c93}. Supposons que nous ayons deux sources S1 et S2 donnant deux fonctions de masse :\nm\u21261 (\u03c91) = 0.2, m \u2126 1 (\u03c91 \u222a \u03c92) = 0.5, m\u21261 (\u2126) = 0.3, (42)\nm\u21262 (\u03c92) = 0.1, m \u2126 2 (\u03c91 \u222a \u03c92) = 0.6, m\u21262 (\u2126) = 0.3 (43)\nLa combinaison conjonctive donne :\nm\u21261 \u2229\u00a9 2(\u2205) = 0.02, m\u21261 \u2229\u00a9 2(\u03c91) = 0.18, m\u21261 \u2229\u00a9 2(\u03c92) = 0.08,\nm\u21261 \u2229\u00a9 2(\u03c91 \u222a \u03c92) = 0.63, m\u21261 \u2229\u00a9 2(\u2126) = 0.09\nCette combinaison conjonctive est effectu\u00e9e avec l\u2019hypoth\u00e8se d\u2019ind\u00e9pendance cognitive des deux sources. Si une connaissance externe permet de mesurer la d\u00e9pendance positive et n\u00e9gative de la source S1 par rapport \u00e0 la source S2 telle que fournie par l\u2019\u00e9quation (24), nous devons en tenir compte avant la combinaison conjonctive. Supposons une fonction de masse traduisant donc une forte d\u00e9pendance positive de S1 par rapport \u00e0 S2. Nous avons ainsi la fonction de masse suivante : m P(I) = 0.26 mP(P ) = 0.56 mP(P\u0304 ) = 0.18 (44)\nNotons que mP(I \u222a P ) = 0 et mP(I \u222a P\u0304 ) = 0 puisque les facteurs d\u2019affaiblissement \u03b1i ne sont pas d\u00e9finis donc fix\u00e9s \u00e0 1. Le tableau 2 pr\u00e9sente les diff\u00e9rentes \u00e9tapes d\u2019extension \u00e0 vide, de d\u00e9conditionnement et de combinaison dans l\u2019espace \u2126 \u00d7 P . L\u2019extension \u00e0 vide et le d\u00e9conditionnement transf\u00e8rent les masses sur les \u00e9l\u00e9ments focaux correspondant de l\u2019espace \u2126 \u00d7 P . La combinaison des trois fonctions de masse dans cet espace fait appara\u00eetre la masse sur l\u2019ensemble vide qui correspond \u00e0 la part de d\u00e9pendance n\u00e9gative.\nLe tableau 3 pr\u00e9sente ensuite la marginalisation et le r\u00e9sultat de combinaison avec la fonction de masse m\u21262 non modifi\u00e9e (i.e. que l\u2019hypoth\u00e8se d\u2019ind\u00e9pendance totale de S2 par rapport \u00e0 S1 est faite). Nous constatons que la masse transf\u00e9r\u00e9e sur l\u2019ignorance ne devient plus importante que lors de la combinaison conjonctive sans hypoth\u00e8se sur la d\u00e9pendance positive.\nAfin de bien illustrer le transfert de masse sur l\u2019ensemble vide et sur l\u2019ignorance, les figures 1 et 2 repr\u00e9sentent les masses en fonction des variations de \u03b1i (repr\u00e9sentant un facteur d\u2019affaiblissement de la source Si) , \u03b2i (repr\u00e9sentant le taux de d\u00e9pendance de Si face \u00e0 Sj) et \u03b3i (repr\u00e9sentant le taux de d\u00e9pendance n\u00e9gative de Si face \u00e0 Sj) d\u2019une fonction de masse d\u00e9crite par (35) pour une fonction de masse dogmatique quelconque de la source Si. Ainsi sur la figure 1, repr\u00e9sentant les variations de masse sur l\u2019ensemble vide, \u03b1i est fix\u00e9 \u00e0 1, \u03b2i et \u03b3i varient. Sur la figure 2, repr\u00e9sentant les variations de masse sur l\u2019ignorance, \u03b3i est fix\u00e9 \u00e0 1, \u03b1i et \u03b2i varient.\nLa figure 1 montre ainsi que, pour la source Si, plus \u03b2i et \u03b3i sont grands plus on obtient une masse importante sur l\u2019ensemble vide et donc une d\u00e9pendance n\u00e9gative. La quantit\u00e9 \u03b2i\nfocal mP\u2191\u2126\u00d7P m\u2126[I]\u21d1\u2126\u00d7P m\u2126[P\u0304 ]\u21d1\u2126\u00d7P m\u2126\u00d7PConj \u2205 0.18 \u03c91 \u00d7 I 0.052 (\u03c91 \u222a \u03c92)\u00d7 I 0.13\n\u2126\u00d7 I 0.26 0.078 \u2126\u00d7 P 0.56 0.56\n(\u03c91 \u00d7 I) \u222a (\u2126\u00d7 P ) ((\u03c91 \u222a \u03c92)\u00d7 I) \u222a (\u2126\u00d7 P )\n\u2126\u00d7 P\u0304 0.18 \u2126\u00d7 (I \u222a P ) 1\n(\u03c91 \u00d7 I) \u222a (\u2126\u00d7 (P \u222a P\u0304 )) 0.2 ((\u03c91 \u222a \u03c92)\u00d7 I) \u222a (\u2126\u00d7 (P \u222a P\u0304 )) 0.5\n\u2126\u00d7 P 0.3\nTAB. 2 \u2013 D\u00e9tails de l\u2019affaiblissement de la mesure d\u2019ind\u00e9pendance : fonctions de masse dans \u2126\u00d7 P .\nfocal m\u2126\u00d7P\u2193\u21261 m \u2126 2 m \u2126\u00d7P\u2193\u2126 1 \u2229\u00a9m \u2126 2\n\u2205 0.18 0.25432 \u03c91 0.052 0.0468 \u03c92 0.1 0.00768 \u03c91 \u222a \u03c92 0.13 0.6 0.15528 \u2126 0.638 0.3 0.53592\nTAB. 3 \u2013 D\u00e9tails de l\u2019affaiblissement de la mesure d\u2019ind\u00e9pendance : marginalisation et combinaison\nrepr\u00e9sente la part de d\u00e9pendance de la source et la quantit\u00e9 \u03b3i repr\u00e9sente la part de d\u00e9pendance n\u00e9gative.\nLa figure 2 pr\u00e9sente quand \u00e0 elle, la variation de la masse sur \u2126, l\u2019ignorance. Cette masse est donn\u00e9e directement par \u03b1i(1\u2212 \u03b2i) qui contient donc la part d\u2019ind\u00e9pendance (1\u2212 \u03b2i) et la fiabilit\u00e9 \u03b1i de la source Si.\nNous illustrons ainsi le r\u00e9sultat escompt\u00e9 de l\u2019affaiblissement par la mesure d\u2019ind\u00e9pendance, c\u2019est-\u00e0-dire que nous retrouvons sur la masse de l\u2019ensemble vide la quantit\u00e9 de d\u00e9pendance n\u00e9gative (associ\u00e9 \u00e0 la part de d\u00e9pendance de la source) et sur l\u2019ignorance la quantit\u00e9 de fiabilit\u00e9 et d\u2019ind\u00e9pendance. Nous remarquons aussi que lorsque la source est fiable (\u03b1i = 1) et ind\u00e9pendante (\u03b2i = 0), la fonction de masse de la source n\u2019est pas modifi\u00e9e."}, {"heading": "6.2 Influence sur le r\u00e9sultat de combinaison", "text": "Afin d\u2019illustrer l\u2019influence de la prise en compte de la mesure d\u2019ind\u00e9pendance sur les fonctions de masse, nous allons consid\u00e9rer ici les deux sources pr\u00e9c\u00e9dentes Si et Sj qui fournissent\nles fonctions de masse donn\u00e9es par les \u00e9quations (42) et (43). Nous allons consid\u00e9rer trois cas pour chaque source avec un cas o\u00f9 la source Si est plut\u00f4t ind\u00e9pendante de Sj (\u03b1i = 0.95, \u03b2i = 0.95, \u03b3i = 0.05), un cas o\u00f9 elle est plut\u00f4t d\u00e9pendante positivement (\u03b1i = 0.95, \u03b2i = 0.05, \u03b3i = 0.95) et un cas o\u00f9 elle est plut\u00f4t d\u00e9pendante n\u00e9gativement (\u03b1i = 0.95, \u03b2i = 0.05, \u03b3i = 0.05). Pour la source Sj , nous consid\u00e9rons trois cas moins cat\u00e9goriques en fixant la fiabilit\u00e9 \u03b1j = 0.9 : le cas plut\u00f4t ind\u00e9pendant (\u03b2j = 0.9, \u03b3j = 0.1), le cas plut\u00f4t d\u00e9pendant positivement (\u03b2j = 0.1, \u03b3j = 0.9) et le cas plut\u00f4t d\u00e9pendant n\u00e9gativement (\u03b2j = 0.1, \u03b3j = 0.1).\nAinsi, le tableau 4 pr\u00e9sente les r\u00e9sultats de la combinaison des deux sources en fonction des hypoth\u00e8ses d\u2019ind\u00e9pendance et de d\u00e9pendance positive ou n\u00e9gative des deux sources S1 et S2. Nous constatons que lorsque les deux sources sont plut\u00f4t ind\u00e9pendantes, les r\u00e9sultats obtenus sont proches de la combinaison conjonctive sous l\u2019hypoth\u00e8se d\u2019ind\u00e9pendance. Lorsqu\u2019une des deux sources est d\u00e9pendante n\u00e9gativement de l\u2019autre, la masse transf\u00e9r\u00e9e sur l\u2019ensemble vide est importante. Lorsque l\u2019une des deux sources est d\u00e9pendante positivement de l\u2019autre, la masse est transf\u00e9r\u00e9e sur l\u2019ignorance mais de fa\u00e7on moins importante que pour la d\u00e9pendance n\u00e9gative. En effet, l\u2019ensemble vide est un \u00e9l\u00e9ment absorbant pour la combinaison conjonctive. Cette masse sur l\u2019ensemble vide, \u00e0 l\u2019issue de la combinaison conjonctive, peut ainsi jouer un r\u00f4le d\u2019alerte sur la d\u00e9pendance n\u00e9gative. Une autre alternative serait d\u2019envisager une autre r\u00e8gle de combinaison lorsque la masse issue de la d\u00e9pendance n\u00e9gative est tr\u00e8s importante."}, {"heading": "7 Conclusion et perspectives", "text": "Dans cet article, nous avons propos\u00e9 une m\u00e9thode d\u2019apprentissage de l\u2019ind\u00e9pendance de sources afin de justifier l\u2019hypoth\u00e8se sur l\u2019ind\u00e9pendance lors du choix des r\u00e8gles de combinaison \u00e0 utiliser pour la fusion. Nous avons \u00e9galement propos\u00e9 d\u2019estimer la d\u00e9pendance positive et n\u00e9gative afin de pouvoir tenir compte de cette information dans les fonctions de masse avant de les combiner. Cette nouvelle information sur la d\u00e9pendance des sources peut \u00eatre int\u00e9gr\u00e9e dans les fonctions de masse fournies par ces sources avant de les combiner. Une autre solution \u00e0 la d\u00e9pendance des sources est de proposer une r\u00e8gle de combinaison tenant compte de\nl\u2019ind\u00e9pendance, d\u00e9pendance positive et n\u00e9gative des sources. Dans les prochains travaux, nous proposerons une r\u00e8gle de combinaison mixant la combinaison disjonctive et la combinaison prudente en fonction des degr\u00e9s de d\u00e9pendance. Nous d\u00e9finirons \u00e9galement le facteur d\u2019affaiblissement \u03b1iki,kj permettant de tenir compte du nombre de fonctions de masse dans les clusters appari\u00e9s lors de l\u2019apprentissage de leurs ind\u00e9pendances.\nR\u00e9f\u00e9rences Ben Hariz, S., Z. Elouedi, et K. Mellouli (2006). Clustering approach using belief function\ntheory. In AIMSA, pp. 162\u2013171. Ben Yaghlane, B., P. Smets, et K. Mellouli (2002a). Belief function independence : I. the\nmarginal case. Int. J. Approx. Reasoning 29(1), 47\u201370. Ben Yaghlane, B., P. Smets, et K. Mellouli (2002b). Belief function independence : Ii. the\nconditional case. Int. J. Approx. Reasoning 31(1-2), 31\u201375. Boubaker, J., Z. Elouedi, et E. Lefevre (2013). Conflict management with dependent informa-\ntion sources in the belief function framework. In The 14th IEEE International Symposium on Computational Intelligence and Informatics, CINTI 2013, Budapest, Hongrie.\nChebbah, M., A. Martin, et B. Ben Yaghlane (2012a). About sources dependence in the theory of belief functions. In Belief Functions, pp. 239\u2013246.\nChebbah, M., A. Martin, et B. Ben Yaghlane (2012b). Positive and negative dependence for evidential database enrichment. In IPMU (3), pp. 575\u2013584.\nDempster, A. P. (1967). Upper and Lower probabilities induced by a multivalued mapping. Annals of Mathematical Statistics 38, 325\u2013339.\nDen\u0153ux, T. (2006). The cautious rule of combination for belief functions and some extensions. In International Conference on Information Fusion, Florence, Italy.\nSj : \u03b1j = 0.9 cas \u00e9l\u00e9ment \u03b2j = 0.9, \u03b3j = 0.1 \u03b2j = 0.1, \u03b3j = 0.9 \u03b2j = 0.1, \u03b3j = 0.1\nfocal m\u2126\u00d7P\u2193\u21261 m \u2126\u00d7P\u2193\u2126 2 m \u2126\u00d7P\u2193\u2126 1\u22292 m \u2126\u00d7P\u2193\u2126 2 m \u2126\u00d7P\u2193\u2126 1\u22292 m \u2126\u00d7P\u2193\u2126 2 m \u2126\u00d7P\u2193\u2126 1\u22292\nSi \u2205 0.045125 0.081 0.12257 0.081 0.12337 0.009 0.0545389 \u03b1i = 0.95 \u03c91 0.01 0.00909 0.00829 0.00909 \u03b2i = 0.95 \u03c92 0.01 0.0779175 0.09 0.0850388 0.082 0.0774798 \u03b3i = 0.05 \u03c91 \u222a \u03c92 0.025 0.06 0.07138 0.54 0.517457 0.492 0.475303\n\u2126 0.919875 0.849 0.780974 0.289 0.265844 0.417 0.383588 Si \u2205 0.045125 0.081 0.12437 0.081 0.13957 0.009 0.0692989 \u03b1i = 0.95 \u03c91 0.19 0.17271 0.15751 0.17271 \u03b2i = 0.05 \u03c92 0.01 0.00764875 0.09 0.0688388 0.082 0.0627198 \u03b3i = 0.95 \u03c91 \u222a \u03c92 0.475 0.06 0.449167 0.54 0.550307 0.492 0.574394\n\u2126 0.289875 0.849 0.246104 0.289 0.0837739 0.417 0.120878 Si \u2205 0.002375 0.081 0.0849926 0.081 0.0994726 0.009 0.0261956 \u03b1i = 0.95 \u03c91 0.181 0.164529 0.150049 0.164529 \u03b2i = 0.05 \u03c92 0.01 0.00816625 0.09 0.0734962 0.082 0.0669633 \u03b3i = 0.05 \u03c91 \u222a \u03c92 0.4525 0.06 0.43317 0.54 0.57175 0.492 0.590472\n\u2126 0.364125 0.849 0.309142 0.289 0.105232 0.417 0.15184\nTAB. 4 \u2013 R\u00e9sultats de combinaison selon les hypoth\u00e8ses de d\u00e9pendance et d\u2019ind\u00e9pendance des deux sources Si et Sj .\nDen\u0153ux, T. (2008). Conjunctive and disjunctive combination of belief functions induced by nondistinct bodies of evidence. Artificial Intelligence 172, 234\u2013264.\nDubois, D. et H. Prade (1988). Representation and combination of uncertainty with belief functions and possibility measures. Computational Intelligence 4, 244\u2013264.\nElouedi, Z. et K. Mellouli (1998). Pooling dependent expert opinions using the theory of evidence. In Seventh Information Processing and Management of Uncertainty in KnowledgeBased System (IPMU\u201998), Volume I, pp. 32\u201339.\nHsia, Y.-T. (1991). Characterizing belief with minimum commitment. In IJCAI, pp. 1184\u2013 1189.\nJousselme, A.-L., D. Grenier, et E. Boss\u00e9 (2001). A new distance between two bodies of evidence. Information Fusion 2, 91\u2013101.\nMartin, A. (2010). Le conflit dans la th\u00e9orie des fonctions de croyance. In Actes Extraction et gestion des connaissances (EGC\u20192010), Hammamet, Tunisia, pp. 655\u2013666.\nMartin, A. et C. Osswald (2007). Une nouvelle r\u00e8gle de combinaison r\u00e9partissant le conflit - applications en imagerie sonar et classification de cibles radar. Traitement du Signal 24(2), 71\u201382.\nMercier, D. (2006). Fusion d\u2019informations pour la reconnaissance automatique d\u2019adresses postales dans le cadre de la th\u00e9orie des fonctions de croyance. Ph. D. thesis, Universit\u00e9 de Technologie de Compi\u00e8gne.\nMurphy, C. (2000). Combining belief functions when evidence conflicts. Decision Support Systems 29, 1\u20139.\nShafer, G. (1976). A mathematical theory of evidence. Princeton University Press. Smets, P. (1990). The Combination of Evidence in the Transferable Belief Model. IEEE\nTransactions on Pattern Analysis and Machine Intelligence 12(5), 447\u2013458. Smets, P. (1993). Belief Functions : the Disjunctive Rule of Combination and the Generalized\nBayesian Theorem. International Journal of Approximate Reasoning 9, 1\u201335. Smets, P. (2005). Decision making in the TBM : the necessity of the pignistic transformation.\nInternational Journal of Approximate Reasonning 38, 133\u2013147. Smets, P. et R. Kennes (1994). The Transferable Belief Model. Artificial Intelligent 66, 191\u2013\n234. Smets, P. et R. Kruse (1997). The transferable belief model for belief representation, pp. 343\u2013\n368. Boston : Kluwer Academic Publishers. Yager, R. R. (1987). On the Dempster-Shafer Framework and New Combination Rules. Infor-\nmation Sciences 41, 93\u2013137.\nSummary In this paper, we propose to learn sources independence in order to choose the appropriate type of combination rules when aggregating their beliefs. Some combination rules are used with the assumption of their sources independence whereas others combine beliefs of dependent sources. Therefore, the choice of the combination rule depends on the independence of sources involved in the combination. In this paper, we propose also a measure of independence, positive and negative dependence to integrate in mass functions before the combinaision with the independence assumption."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "R\u00e9sum\u00e9. La fusion d\u2019informations issues de plusieurs sources cherche \u00e0 am\u00e9liorer la prise de d\u00e9cision. La th\u00e9orie des fonctions de croyance, pour r\u00e9aliser cette fusion, utilise des r\u00e8gles de combinaison faisant bien souvent l\u2019hypoth\u00e8se forte de l\u2019ind\u00e9pendance des sources. Cette hypoth\u00e8se d\u2019ind\u00e9pendance n\u2019est cependant pas formalis\u00e9e ni v\u00e9rifi\u00e9e. Nous proposons dans cet article un apprentissage de l\u2019ind\u00e9pendance cognitive de sources d\u2019information permettant de mesurer la d\u00e9pendance ou l\u2019ind\u00e9pendance. Cette mesure exprim\u00e9e par une fonction de masse est ensuite int\u00e9gr\u00e9e par une approche d\u2019affaiblissement avant de r\u00e9aliser la combinaison d\u2019informations.", "creator": "LaTeX with hyperref package"}}}