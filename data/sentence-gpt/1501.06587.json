{"id": "1501.06587", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jan-2015", "title": "Measuring academic influence: Not all citations are equal", "abstract": "The importance of a research article is routinely measured by counting how many times it has been cited. However, treating all citations with equal weight ignores the wide variety of functions that citations perform. We want to automatically identify the subset of references in a bibliography that have a central academic influence on the citing paper. We wanted to create an automated review process where citations will be considered. We have made these steps to assist researchers, including:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 26 Jan 2015 21:06:02 GMT  (51kb,D)", "http://arxiv.org/abs/1501.06587v1", null]], "reviews": [], "SUBJECTS": "cs.DL cs.CL cs.LG", "authors": ["xiaodan zhu", "peter turney", "daniel lemire", "r\\'e vellino"], "accepted": false, "id": "1501.06587"}, "pdf": {"name": "1501.06587.pdf", "metadata": {"source": "CRF", "title": "Measuring academic influence: Not all citations are equal", "authors": ["Xiaodan Zhu", "Peter Turney", "Daniel Lemire", "Andr\u00e9 Vellino"], "emails": ["Xiaodan.Zhu@nrc-cnrc.gc.ca"], "sections": [{"heading": null, "text": "Measuring academic influence: Not all citations are equal"}, {"heading": "Xiaodan Zhu and Peter Turney", "text": ""}, {"heading": "National Research Council Canada, Ottawa, ON K1A 0R6, Canada.", "text": "Email: Xiaodan.Zhu@nrc-cnrc.gc.ca"}, {"heading": "Daniel Lemire", "text": ""}, {"heading": "TELUQ, Universite\u0301 du Que\u0301bec, Montreal, QC H2S 3L5, Canada.", "text": "Andre\u0301 Vellino"}, {"heading": "School of Information Studies, University of Ottawa, Ottawa, ON K1N 6N5, Canada.", "text": "The importance of a research article is routinely measured by counting how many times it has been cited. However, treating all citations with equal weight ignores the wide variety of functions that citations perform. We want to automatically identify the subset of references in a bibliography that have a central academic influence on the citing paper. For this purpose, we examine the effectiveness of a variety of features for determining the academic influence of a citation.\nBy asking authors to identify the key references in their own work, we created a dataset in which citations were labeled according to their academic influence. Using automatic feature selection with supervised machine learning, we found a model for predicting academic influence that achieves good performance on this dataset using only four features.\nThe best features, among those we evaluated, were features based on the number of times a reference is mentioned in the body of a citing paper. The performance of these features inspired us to design an influence-primed h-index (the hip-index). Unlike the conventional h-index, it weights citations by how many times a reference is mentioned. According to our experiments, the hip-index is a better indicator of researcher performance than the conventional h-index."}, {"heading": "Introduction", "text": "One of the functions of citation analysis is to determine the impact of an author\u2019s work on a research community. A first approximation for measuring this impact is to count the number of times an author is cited. Various other measures, such as the h-index (Hirsch, 2005), the g-index (Egghe, 2006) and the hm-index (Schreiber, 2008), refine this basic measure using functions based on the distribution of citations (Bornmann, Mutz, & Daniel, 2008).\nYet other measures of author impact are based on methods for scoring articles with weights and thresholds that depend on the journals in which they were published and the number of times the article was cited (Marchant, 2009). However, all these indexes and score-based rankings treat each citation as having equal significance.\nTo appear at JASIST.\nar X\niv :1\n50 1.\n06 58\n7v 1\n[ cs\n.D L\n] 2\n6 Ja\nn 20\n15\nIt has long been recognized that not all citations are created equal and hence they should not be counted equally. Simkin and Roychowdhury (2003) estimated that authors read only 20% of the works they cite. This estimate was based on a detailed analysis of the frequency of replication of distinctive errors in citations, such as incorrect page numbers or volume numbers. When an error in a citation is replicated many times, it seems likely that the citers have copied the citation without actually reading the cited paper.\nAs an illustration, Dubin (2004) reported that a commonly cited paper by Gerard Salton does not actually exist. An incorrect citation was accidentally created by mixing the citations for two separate papers. This incorrect citation has since been cited by more than 300 papers. If the citers had tried to read the paper before citing it, they would have discovered that the paper does not exist.\nLike Moravcsik and Murugesan (1975), we are concerned about the sideeffects of counting insignificant references: \u201cA large fraction of the references are perfunctory. This raises serious doubts about the use of citations as a quality measure, since it is then quite possible for somebody or some group to chalk up high citation counts by simply writing barely publishable papers on fashionable subjects which will then be cited as perfunctory, \u2018also ran\u2019 references.\u201d (Moravcsik & Murugesan, 1975, p. 91). Indeed, based on an analysis of hundreds of references, Moravcsik and Murugesan (1975) found that a third of the references were redundant and 40% were perfunctory. In an independent study, Teufel et al. (2006) found that the majority (62.7%) of the references could not be attributed a specific function whereas the fraction of references that provided an essential component for the citing paper (definition, tool, starting point) was 18.9%.\nThe aim of our work is to determine the most effective features for identifying references that have high academic influence on the citing paper. An influential reference is one that inspired a new idea, method, experiment, or research problem that is a core contribution of the citing paper. We use the terms influence and influential to indicate the degree of academic influence of a single citation. In contrast, Pinski and Narin (1976) used the term citation influence to refer to the academic influence of a journal.\nMany attempts have been made to automatically identify which citations are most influential. Readers can often tell quickly whether a citation is shallow from the text itself, which has prompted several efforts to categorize citations by the linguistic context of their occurrence; that is, by the words near the citation in the body of the citing paper (Teufel et al., 2006; Hanney, Frame, Grant, Buxton, Young, & Lewison, 2005; Mercer, Di Marco, & Kroon, 2004; Pham & Hoffmann, 2003).\nIn contrast to approaches based solely on linguistic context, our method uses machine learning to evaluate a number of citation features. We examine features based on linguistic context as well as other features, such as\n\u2022 the location of the citation in the text,\n\u2022 the semantic similarities between the titles of the cited papers and the content of the citing paper, \u2022 the frequency with which the articles are cited in the literature, \u2022 and the number of times a given reference is cited in the body of the paper. We test the effectiveness of the features by applying machine learning to the problem of identifying the influential references. One of our most important contributions is to identify a set of four features that are particularly useful to determine influence. For example, the two best features are the number of times a reference appears and the similarity between the title of the cited paper and the core sections of the citing paper.\nA secondary purpose of citation measures is to predict the future performance of authors, such as whether they will win a Nobel Prize (Garfield & Malin, 1968; Gingras & Wallace, 2010). The importance of researchers is reflected in the amount of influence they have on the research of their colleagues. Citation frequency is a measure of this influence, but a better measure would take into account how a researcher is cited, rather than giving all citations equal weight.\nAs a test of our method\u2019s ability to determine whether a cited paper substantially influenced the citing paper, we attempt to identify which researchers in computational linguistics are Fellows of the Association for Computational Linguistics (ACL), based solely on their publication records and citations. For each ACL Fellow, we compare their conventional (unweighted) h-index with an h-index computed from citations weighted by our measure of academic influence. We get a better average precision measure with weighted citations."}, {"heading": "Defining academic influence", "text": "What does it mean to say that one reference had more academic influence on a given citing paper than another reference? If our aim is to distinguish references according to their degrees of academic influence, then we must be precise about the meaning of academic influence. As researchers, we know that some papers have influenced the course of our research more than others, but how can we pin down this intuition?\nA paper written by an evolutionary biologist is likely to have been influenced by Darwin\u2019s On the Origin of Species, but we are more interested in the proximate influences on the paper. A good research paper contributes a new idea to the literature. What prior work was the proximate cause, the impetus for that new idea?\nWe believe that this question is best answered by the authors of the citing paper, because they are in the best position to decide which of their references should be labeled influential and which should be labeled non-influential. It could be said that we avoid the problem of precisely defining influence; instead we give a kind of operational definition: A cited paper is influential for a given citing paper if the authors of the citing paper say that it is influential.\nWe acknowledge that authors may be wrong about whether a paper was influential. Two types of error are possible: Authors may say a reference is influential when it is actually non-influential or authors may say a reference is non-influential when it is actually influential. Both types seem plausible to us. In the first case, the authors might feel obliged to say that a paper is influential, because the paper is very popular, very respected, or very well written. In the second case, a paper might have greatly influenced the authors at a subconscious level, but they might mistakenly say it is non-influential, or they might not want to admit that there was any influence, due to professional jealousy. Nevertheless, although the authors might be wrong, we know of no better, more reliable way of determining which references were influential. Therefore we base our experiments on author-labeled data.\nDietz, Bickel, and Scheffer (2007) also rely on author-labeled data. They collected a data set of twenty-two papers labeled by their authors. Each reference was labeled on a Likert scale and they experimented with unsupervised prediction of citation influence. Unlike us, their purpose was to model topical inheritance via citations."}, {"heading": "Motivation", "text": "Suppose that we have a model for predicting the label (influential or noninfluential) of a paper\u2013reference pair, consisting of a given citing paper and a given citation within that citing paper. We label pairs rather than references alone, because a reference that is influential for one citing paper is not necessarily influential for another citing paper. Such a model would have many potential applications. Wherever citation counts play an important role, the model could be applied to filter or weight the citations. Some potential applications follow.\nSummarizing: Given a paper with a long list of references, the model could identify the most influential references and list them. For those who are familiar with the field of the given paper, this list would rapidly convey the topic and nature of the paper. For those who are new to the field, this list would suggest further reading material. Citations have generally proven useful for summarization (Qazvinian & Radev, 2008, 2010; Abu-Jbara & Radev, 2011; Nanba & Okumura, 1999; Taheriyan, 2011; Kaplan, Iida, & Tokunaga, 2009).\nImproved measures of an author\u2019s impact: Indexes such as the h-index (Hirsch, 2005), g-index (Egghe, 2006), and hm-index (Schreiber, 2008) could be made less sensitive to noise by filtering citation counts with a model of influence. Beyond reducing sensitivity to noise, a model of influence could also put more weight on original contributions. It is known that survey papers and methodology papers tend to be more highly cited than research contributions in general (Ioannidis, 2006). Vanclay (2013) went as far as to recommend focusing on reviews: \u201cPerhaps the best single thing an aspiring author can do to attract citations is to participate in a rigorous review rather than writing a conventional research article.\u201d (Vanclay, 2013, p. 270). However, survey and methodology papers seem less likely to us to be labeled as influential by authors. Filtering citation counts\nby a model of influence might decrease the impact of survey and methodology papers, putting more weight on innovative research.\nImproved journal impact factors: As with measures of author impact, measures of journal impact (Bollen, Van de Sompel, Hagberg, & Chute, 2009) could be made less sensitive to noise by filtering citation counts with a model of influence.\nImproved measures of research organization impact: Citations counts are also used to evaluate research organizations. As with journal impact and author impact, performance measures that are based on citation counts may benefit from filtering by a model of influence.\nMeme tracking: Historians of science are interested in tracking the spread of ideas (memes) (Haque & Ginsparg, 2011; Leskovec, Backstrom, & Kleinberg, 2009). Citations are a noisy way to track how ideas spread, because a reference may be cited for many reasons other than being the source of an influential idea (Bornmann & Daniel, 2008b). Filtering by a model of influence may result in a better analysis of the spread of an idea.\nResearch network analysis: Scientists belong to networks of people who collaborate with each other or influence each other\u2019s work. Filtering citations with a model of influence may make it easier to identify these networks automatically.\nImproved hyperlink analysis: In many ways, hypertext links in web pages are analogous to citation links in research papers. A good model of citation influence could suggest a model of hypertext link importance. This could improve measures of the importance of web pages, such as PageRank (Qi, Nie, & Davison, 2007).\nImproved recommender systems: Researchers often need help identifying relevant work that they should read. Filtering out less relevant citations might help paper recommender systems (Vellino, 2010; Liang, Li, & Qian, 2011)."}, {"heading": "Related work", "text": "The idea that the mere counting of citations is dubious is not new (Chubin & Moitra, 1975): The field of citation context analysis (a phrase coined by Small, 1982) has a long history dating back to the early days of citation indexing. There is a wide variety of reasons for a researcher to cite a source and many ways of categorizing them. For instance, Garfield (1965) identified fifteen such reasons, including giving credit for related work, correcting a work, and criticizing previous work.\nFor articles in the field of high energy physics, Moravcsik and Murugesan (1975) distinguished four major classes of polar opposite pairs, conceptual\u2013 operational, organic\u2013perfunctory, evolutionary\u2013juxtapositional, and confirmative\u2013 negational. They found that the fraction of negational references, i.e., citations indicating that the cited source is wrong, is not negligible (14%).\nGiles, Bollacker, and Lawrence (1998) presented one of the first automatic citation indexing systems (CiteSeer). It could parse citations and use them to compute similarities between documents.\nGarzone and Mercer (2000) might have implemented the first automated classification systems for citations. They used over 200 manually selected rules to classify citations in one of 35 categories.\nMachine learning methods for automatic classification can be applied to the text of a citing document. Teufel et al. (2006) distinguish categories of citations that can be identified via linguistic cues in the text. They are able to classify citations into one of four categories (weak, positive, contrast, neutral) with an average F-measure of 68%. For a classification in three categories (weakness, positive, neutral), they get an average F-measure of 71%. Their classifier relies on 892 manually selected cue phrases, such as whether the citation is a selfcitation, the location of the citation in the text, and manually acquired verb clusters.\nAgarwal, Choubey, and Yu (2010) annotated a corpus of 43 open-access fulltext biomedical articles. They built classifiers using Support Vector Machine (SVM) and Multinomial Na\u0308\u0131ve Bayes (MNB) models using the open-source Java library Weka. They report an average F-measure of 76.5%. They used unigrams (individual words) and bigrams (two consecutive words) as features. They ranked their features using mutual information. They found the SVM models were generally superior to the MNB models.\nOur own methodology differs from Teufel et al. (2006) and Agarwal et al. (2010) in at least one significant way: We asked the authors of the citing papers themselves to identify the influential references whereas they used independent annotations. We believe that it is difficult for an independent annotator to classify citations. This concern was raised by Gilbert (1977): \u201cSince the intentions of the author are not normally available to the content analyst, there seems to be no way of conclusively resolving problems of classifications (. . . ) The difficulties are more compounded when the analyst has only a superficial knowledge of the contexts in which the papers he examines were written and read.\u201d (Gilbert, 1977, p. 120). Nevertheless, Teufel et al. (2006) report moderately good inter-annotator agreement.\nTo address concerns about the consistency of the h-index, ranking and scoring have been proposed as alternative measures (Waltman & van Eck, 2012). Like the h-index, these measures are based on citation counts, and they too could benefit from filtering or weighting citations with a model of influence.\nThere are other good reasons, beside assessing researchers, to make distinctions between different types of citations. The need also arises from the desire by publishers to provide scholarly research with semantic annotations. Thus CiTO, the Citation Typing Ontology (Peroni & Shotton, 2012; Shotton, 2010), provides a rich machine-readable RDF metadata ontology for the characterization of bibliographic citations.\nFrom among the almost ninety semantic relations for citations identified in CiTO (for example, agrees with, obtains background from, supports, and uses conclusions from), it is natural to generalize at least two broad categories of citations, ones that acknowledge a fundamental intellectual legacy (such as critiques, extends, and disputes) and ones that are incidental (such as cites for information, obtains support from, and cites as related).\nSeveral authors have proposed weighting citations based on factors such as the prestige of the citing journal (Ding, 2011; Yan & Ding, 2010; Ding & Cronin, 2011; Gonzalez-Pereira, Guerrero-Bote, & Moya-Anego\u0301n, 2010). Others have proposed weighting citations by the mean number of references of the citing journal (Zitt & Small, 2008). Balaban (2012) proposed giving more weights to citations from more prestigious authors. He also argues that the citation of a paper published in a less prestigious venue should be considered more significant: \u201c. . . if a paper is cited despite its handicap of having appeared in a low-IF [Impact Factor] journal, then this means that this paper has a high intrinsic value. Therefore a citation\u2019s value should be inversely correlated with the IF of the journal in which the cited paper was published.\u201d (Balaban, 2012, p. 244).\nHou, Li, and Niu (2011) propose to use frequency to assess the importance of a citation. That is, if a reference was cited 10 times in the citing paper it gets a weight of 10. They show that by weighting citations by the in-paper frequency, review articles lose part of their advantage over original contributions when counting citations. They state that greater credit is reverted to the discoverers. They also show that closely related references are cited more often in the body of a citing paper than less related references (on average, 3.35 times versus 1.88 times). They define closely related references as papers having at least ten references in common with the given paper.\nDi Marco, Kroon, and Mercer (2006) studied hedging as a means to classify citations. Le, Ho, and Nakamori (2006) used finite-state machines for classification of citations."}, {"heading": "Features for supervised learning", "text": "We are concerned with a binary classification problem: Given a research paper, classify its references as either influential or non-influential.\nThe task is to create a model that takes a pair, consisting of a given author\u2019s paper (the citer) and a reference in the given paper (the cited paper), as input, and generate a label (influential or non-influential) as output. Our goal is to create a model that can predict the labels assigned by the authors in our gold-standard dataset. Our approach to this task is to use supervised machine learning.\nFor supervised machine learning, we must generate feature vectors that represent a variety of properties of each paper\u2013reference pair. Given a training set of manually labeled paper\u2013reference feature vectors, a learning algorithm can create a model for predicting the label of a paper\u2013reference pair in the testing set.\nWe use a standard supervised machine learning algorithm in our experiments (a support vector machine). The main contribution of this paper is that we evaluate a wide range of different features for representing the paper\u2013reference pairs. Finding good features is the key to successful prediction.\nIn our experiments, we consider five general classes of features:\n1. Count-based features\n2. Similarity-based features\n3. Context-based features\n4. Position-based features\n5. Miscellaneous features\nNot all of these features are useful. However, many of these features are intuitively attractive, and we can only find out if (and the extent to which) they are useful through experiments. We describe these features in the following subsections."}, {"heading": "Count-based features", "text": "The count-based features are based on the intuition that a reference that is frequently mentioned in the body of a citing paper is more likely to be influential than a reference that is only mentioned once. We created five different countbased features:\n1. Count-based features\n1.1. countsInPaper whole\n1.2. countsInPaper secNum\n1.3. countsInPaper related\n1.4. countsInPaper intro\n1.5. countsInPaper core\nWe count the occurrences of each reference in the entire citing paper (counts InPaper whole), in the introduction (countsInPaper intro), in the related work (countsInPaper related), and in the core sections (countsInPaper core), where core sections include all the other sections, excluding those already mentioned and excluding the acknowledgment, conclusion, and future-work sections.\nWe added a feature (countsInPaper secNum) to indicate the number of different sections in which a reference appears. This feature is based on the intuition that a reference that appears in several different sections is more significant than a reference that appears in only one section (even if it may have a high frequency within that one section)."}, {"heading": "Similarity-based features", "text": "It seems natural to suppose that the influence of a cited paper on a given citing paper (the citer) is proportional to the overlap in the semantic content of the cited paper and the citer. That is, if there is a high degree of semantic similarity between the text of the citer and the text of the cited paper, then it seems likely that the cited paper had a significant influence on the citer. Accordingly, we explored a variety of features that attempt to capture semantic similarity.\nWe assume that the text of the citer is given, but we do not assume that we have access to the text of the cited paper. The cited paper might not be readily available online, due to subscription charges or the age of the paper. A benefit\nof our approach is that all these features can be implemented efficiently: All features can often be computed from a single document. Even when the titles of references are not included by the journal, it may still be easier to locate the missing titles than the full text.\nSince (by choice) we do not have access to the full text of the cited paper, we use the title of the cited paper as a surrogate for the full text. The first five similarity-based features compare the title of the cited paper to various parts of the citing paper:\n2. Similarity-based features\n2.1. sim titleTitle\n2.2. sim titleCore\n2.3. sim titleIntro\n2.4. sim titleConcl\n2.5. sim titleAbstr\nWe calculate the similarities between the title of a reference and the title (sim titleT itle), the abstract (sim titleAbstr), the introduction (sim titleIntro), the conclusion (sim titleConcl), and the core sections (sim titleCore) of the citing paper. These features should be able to capture the semantic similarity between the citer and a reference; in most cases, a title, abstract, and conclusion section are good summaries of the given citing paper. Core sections here refer to the same sections as in the count-based features.\nMore specifically, we calculated cosine similarity scores. A piece of text (e.g., a title or abstract) is first represented as a vector in the word space, where each dimension is a word (a word type; not a word token). The values in the vector are the word frequencies of each word appearing in this piece of text. Porter\u2019s (1980) stemming algorithm was used to stem the text (remove suffixes) and we kept stop words (function words), since removing them did not improve the performance of our models during their development. Readers can refer to Turney and Pantel (2010) for further discussions of vector space models of semantic similarity.\nIn a given citing paper (the citer), when a reference is mentioned in the body of the citer, the text that appears near the mention is called the citation context. Like the title of the cited paper, the citation context provides information about the cited paper; hence we can use the citation context as a surrogate for the full text of the cited paper, in the same way that we used the title as a surrogate. The next four similarity-based features compare the citation context to various parts of the citing paper:\n2. Similarity-based features\n2.6. sim contextTitle\n2.7. sim contextIntro\n2.8. sim contextConcl\n2.9. sim contextAbstr\nFor each reference, we calculate the similarities between the citation contexts and the title (sim contextT itle), abstract (sim contextAbstr), introduction (sim contextIntro), and conclusion (sim contextConcl) of the citing paper. When a reference appears multiple times in the citer, we take the average of the similarities over all its contexts.\nAs with title similarity (features 2.1 to 2.5), we use cosine similarity, after the text was preprocessed with Porter\u2019s (1980) stemmer. During development, we experimented with different window sizes, ranging from two words around a citation to several sentences around it. We found that using the entire sentence in which the citation appears gave the best results. In contrast, Ritchie, Robertson, and Teufel (2008) found that contexts larger than one sentence were better for indexing purposes: further work might be needed to identify the optimal window."}, {"heading": "Context-based features", "text": "The citation context of a reference could indicate the academic influence of the reference in other ways, beyond its value as as a surrogate for the full text of the cited paper (as in the above features 2.6 to 2.9). For example, if a citation X appears in the context \u201cthe work of X inspired us\u201d, then X seems likely to be influential for the given citing paper.\nFor these features, we define the citation context to be a window of ten words around a citation (five words on each side). If a reference appears multiple times in the citing paper, we calculate its average score.\nThe first three context-based features are based on the relation between the citation and the citation context:\n3. Context-based features\n3.1. contextMeta authorMentioned\n3.2. contextMeta appearAlone\n3.3. contextMeta appearFirst\nThe first feature (contextMeta authorMentioned) indicates whether the authors of a reference are explicitly mentioned in the citation context; for example, \u201cthe work of Smith et al. [4]\u201d mentions the authors (Smith et al.) in the citation context of the reference ([4]). The second feature (contextMeta appearAlone) indicates whether a citation is mentioned by itself (e.g., \u201c[4]\u201d) or together with other citations (e.g., \u201c[3,4,5]\u201d). When a citation is mentioned with other citations, the third feature indicates whether it is mentioned first (e.g., \u201c[4]\u201d is first in \u201c[4,5,6]\u201d).\nThese three features may be biased by the different citation format requirements of various journals, but we leave it to the supervised learning system to decide whether the features are useful. A feature may be useful for prediction even when it has some bias. (However, we will see later that these features were not particularly effective in our experiments.)\nThe next twelve context-based features are based on the meaning of the words in the citation context:\nWe manually created four relatively short lists of words that we designed to detect whether the citation context suggests that the cited paper is especially relevant to the citer (contextLex relevant), whether the citation context signals that the cited paper is new (contextLex recent), whether the citation context implies that the cited paper is extreme in some way (contextLex extreme), and whether the citation context makes some kind of comparison with the cited paper (contextLex comparative). The names of these features convey the kinds of words in the short lists. In Table 1, we give the full lists for contextLex relevant and contextLex recent and a few terms for the other two features (as they both contain over 100 words).\nWe also created features based on Osgood, Suci, and Tannenbaum\u2019s (1957) semantic differential categories (contextLexOsg). Osgood et al. (1957) discov-\nered that three main factors accounted for most of the variation in the connotative meaning of adjectives. The three factors were evaluative (good\u2013bad), potency (strong\u2013weak), and activity (active\u2013passive).\nThe General Inquirer lexicon (Stone, Dunphy, & Smith, 1966) represents these three factors using six labels, Positiv and Negativ for the two ends of the evaluative continuum, Strong and Weak for the two ends of the potency continuum, and Active and Passive for the two ends of the activity continuum.1\nThe feature contextLexOsg giEvaluative is the number of words in the citation context that are labeled Positiv in the General Inquirer lexicon, context LexOsg giPotency is the number of words labeled Strong, and contextLexOsg giActivity is the number of words labeled Active. The intuition behind these features is that a citation is more likely to be influential if positive, strong, active words occur in the citation context.\nThe General Inquirer lexicon has labels for 11,788 words. Using the algorithm of Turney and Littman (2003), we automatically extended the labels to cover 114,271 words. The additional words are from the WordNet lexicon. The WordNet features (contextLexOsg wn) are similar to the corresponding General Inquirer features (contextLexOsg gi), except they include these additional words.2\nSince the citation context is a window of ten words around the citation, the values of these features range from zero to ten. If a reference is cited multiple times in the body of the citing paper, we calculate its average value. For increased precision, we only considered the words in the citation context that have an adjective or adverb sense in WordNet.\nWe used an emotion lexicon (Mohammad & Turney, 2010) to check whether the citation context includes words that convey sentiment (contextLexEmo polarity) or emotion (contextLexEmo emo). The lexicon contains human annotation of emotion associations for about 14,200 words. The annotations in the lexicon indicate whether a word is positive or negative (known as sentiment, polarity, or semantic orientation), and whether it is associated with eight basic emotions (joy, sadness, anger, fear, surprise, anticipation, trust, and disgust).\nThe feature contextLexEmo polarity is the number of words in the citation context that are labeled either positive or negative. The feature context LexEmo emo is the number of words that are labeled with any of the eight basic emotions. The idea behind these features is that any kind of sentiment or emotion in the words in the citation context might indicate that the citation is influential, even if the sentiment or emotion is negative.\nAs with the other contextLex features, the values of contextLexEmo range from zero to ten. Multiple occurrences of a citation are averaged."}, {"heading": "Position-based features", "text": "The location of a citation in the body of a citing paper might be predictive of whether the cited paper was influential. Intuitively, the earlier the citation appears in the text, the more important it seems to us. The first two types of position-based features are based on the location of a citation in a sentence:\n4. Position-based features\n4.1. posInSent begin\n4.2. posInSent end\nThese are binary features indicating whether a citation appears at the beginning (posInSent begin) or the end (posInSent end) of the sentence. If a reference appears more than once in the citing paper, we calculated the percentages; for example, if a reference is cited three times in the paper and two of the three appear at the beginning of the sentences, the posInSent begin feature takes the value of 0.667.\nThe next four position-based features are based on the location of a citation in the entire citing paper:\n4. Position-based features\n4.3. posInPaper stdVar\n4.4. posInPaper mean\n4.5. posInPaper last\n4.6. posInPaper first\nWe measured the positions of the sentences that cite a given reference, including the mean (posInPaper mean), standard variance (posInPaper stdV ar), first (posInPaper first), and last position (posInPaper last) of these sentences. These features are normalized against the total length (the total number of sentences) of the citing paper; thus the position ranges from 0 (the beginning of the citing paper) to 1 (the end of the citing paper).\nMore sophisticated location-based features are possible but not considered. For example, references appearing in a methodology section might be more influential than those appearing solely in the related work section."}, {"heading": "Miscellaneous features", "text": "The next three features do not fit into the previous four classes and they have little in common with each other. We arbitrarily put them together as miscellaneous features:\n5. Miscellaneous features\n5.1. aux citeCount\n5.2. aux selfCite\n5.3. aux yearDiff\nThe citation count a paper has received (in the general literature; not the number of occurrences within a specific paper) is widely used as a metric for estimating the academic contribution of a paper, which in turn is an essential building block in calculating other metrics (e.g., h-index) for evaluating the academic contribution of a researcher, organization, or journal. We are interested in understanding its usefulness in deciding academic influence (in a specific paper). That is, when cited in a given paper, is a more highly cited paper more likely\nto have academic influence on the citer? To explore this question, we collected the raw citation counts of each reference in Google Scholar (aux citeCount).\nIn accordance with convention, self-citation refers to the phenomenon where a citer and a reference share at least one common author. We are interested in knowing whether a self-citation would have a positive or negative correlation with academic influence. To study this, we manually annotated self-citation among the references and used it as a binary feature (aux selfCite).\nAre older papers, if cited, more likely to be academically influential? We incorporated the publication year of a reference as a feature (aux yearDiff). We calculated the difference in publication dates between a reference and the citer by subtracting the former from the latter, which resulted in a non-negative integer feature."}, {"heading": "Contextual normalization", "text": "Many of the above features are sensitive to the length of the citing paper. For example, the number of occurrences of each reference in the entire citing paper (countsInPaper whole) tends to range over larger values in a long paper than in a short paper. For predicting whether a reference is influential, it is useful to normalize the raw feature values for a given paper\u2013reference pair by considering the range of values in the given citing paper. This is a form of contextual normalization (Turney, 1993, 1996), where the citing paper is the context of a feature.\nWe normalize all our features so that their values are in the range [0, 1]. This kind of normalization is standard practice in data mining, as it improves the accuracy of most supervised learning algorithms (Witten, Frank, & Hall, 2011). For example, consider the feature countsInPaper whole, where we count how many times a given reference is cited in the whole text. Suppose we find that, in a given citing paper, one reference is cited ten times, but all other references are cited only once. We would then give a score of 1 to the most cited reference, and a score of 1/10 to the other references. That is, the most often cited references in any given citing paper always get a score of 1.\nWe formalize the normalization as follows. Our feature set contains both binary and real-valued features that take non-negative values. Binary features do not require normalization: Their values are 0 and 1.3 Other features are normalized to [0, 1]. Let \u3008pi, rij\u3009 be a paper\u2013reference pair, where pi is the i-th citing paper and rij is the j-th reference in pi. Let fk be the k-th feature in our feature set and let v(pi, rij , fk) be the value of the feature fk in the paper\u2013reference pair \u3008pi, rij\u3009. Suppose that pi contains n distinct references, \u3008pi, ri1\u3009, . . . , \u3008pi, rin\u3009, resulting in n values for fk, v(pi, ri1, fk), . . . , v(pi, rin, fk). Let max(pi, ri\u2217, fk) be the maximum of the n values, v(pi, ri1, fk), . . . , v(pi, rin, fk). We normalize each v(pi, rij , fk) to range from zero to one, using the formula v(pi, rij , fk)/ max(pi, ri\u2217, fk). If max(pi, ri\u2217, fk) is zero, then we normalize v(pi, rij , fk) to zero."}, {"heading": "Experiments with features", "text": "Using a labeled dataset, we first identify the features that are most correlated with academic influence. We then combine some of these features to achieve a good classification score."}, {"heading": "Gold-standard dataset", "text": "We believe that the authors of a paper are in the best position to determine whether a given reference had a strong influence on their research. In a blog posting, we invited authors to help us create a gold-standard dataset of labeled references.4 The authors were directed to fill in an online form.5 The instructions on the form were as follows:\nWe believe that most papers are based on 1, 2, 3 or 4 essential references. By an essential reference, we mean a reference that was highly influential or inspirational for the core ideas in your paper; that is, a reference that inspired or strongly influenced your new algorithm, your experimental design, or your choice of a research problem. Other references merely support the work.\nWe believe that authors are the best experts to assess which references are essential. We are interested in automatically finding these references. To know how well we are doing, we need your help: please give us the title of a few of your papers and list for each paper the references that you feel are most essential, those without which the work would not have been possible.\nForty different researchers filled out our online form (see Table 2). About half of them are from the USA and Canada. Three quarters of them are in computer science.\nThis gold-standard dataset provides us with a benchmark for supervised machine learning.6 The authors gave us the titles of their papers and they indicated which references in each paper were influential for them. From the titles, we obtained PDF copies of their papers and converted them to plain text. We then extracted the references from the text and labeled them as influential or non-influential.\nIn total, the authors contributed 100 of their papers. OpenNLP was used to detect sentence boundaries and conduct tokenization.7 We then used ParsCit to parse the papers (Councill, Giles, & Kan, 2008). ParsCit is an open-source package for parsing references and document structure in scientific papers. We first ran the papers through ParsCit and then used a few hand-coded regular expressions to capture citation occurrences in paper bodies that were not detected by ParsCit.\nThe contents of the papers were then further annotated. First, the section names were standardized to twelve predefined labels: title, author, abstract, introduction, related, main, conclusion, future, acknowledgment, reference, appendix, and date (the year of publication of the given paper). The default label\nfor a section was main (the core or main body of the paper). For example, previous work and related work would both be standardized to related.\nSecond, the bibliographic items were manually corrected and meta-data about them (e.g., the Google citation counts) was included. The citations of these items in the main body of the paper were also manually corrected. For example, if references were cited as \u201c[7-10]\u201d, we modified the citation to \u201c[7, 8, 9,10]\u201d, so as to explicitly include references [8] and [9].\nPorter\u2019s (1980) stemmer (mentioned in the preceding section) was only applied as a preprocessing step when generating feature vectors; the stemmer was not applied during the corpus annotation step described here.\nThe basic units in our study are paper\u2013reference pairs, not papers. The 100 papers yield 3143 paper\u2013reference pairs (that is, 3143 data points; 3143 feature vectors). In the main bodies of the 100 papers, there are 5394 occurrences of the references, so each paper contains an average of around 31 references (in the bibliographies) and 54 citations (in the main text). The dataset contains 322 (10.3% of 3143) influential references (strictly speaking, 322 influential paper\u2013 reference pairs). That is, their authors identified an average of 3.2 influential references per research paper.\nCorrelation between labels and features\nWe seek to determine which features are better able to predict the academic influence of a reference. The Pearson correlation coefficients between the various features and the gold influence labels are a simple indication of how useful a feature might be. We show the coefficients in Figure 1.\nFirst, consider the correlation coefficients for the count-based features:\n1. Count-based features\n1.1. countsInPaper whole\n1.2. countsInPaper secNum\n1.3. countsInPaper related\n1.4. countsInPaper intro\n1.5. countsInPaper core\nFigure 1 shows that the most correlated individual features to academic influence are in-paper count features (countsInPaper whole and countsInPaper secNum). This is a convenient result, because one of the best features, counts InPaper whole (the number of times a reference is cited in a paper), is also one of the easiest to compute from a technical point of view. Moreover, it suggests simple but potentially effective schemes for modifying the standard citation count (i.e., the number of papers that cite a given paper in the general literature): For each paper X that cites a paper Y , increment the citation count for Y\n\u2022 only if Y was cited more than once in the body of X, \u2022 only if Y is cited more often in the body of X than most of the other\nreferences in X, or\n\u2022 only if Y is cited in more than one section of X. Intuitively, these modified citation counts may also be more robust than the standard citation count, considering that an author seems unlikely to cite a paper more than once when the paper is included in the references because it is de rigueur in the field.\nNext, consider the similarity-based features. These are features that measure the semantic similarity between a citer and a reference. In general, we found such features well correlated with academic influence.\nThe first group of similarity-based features compares the similarities between the title of a cited paper and the title, introduction, conclusion, and abstract of the citer:\n2. Similarity-based features\n2.1. sim titleTitle\n2.2. sim titleCore\n2.3. sim titleIntro\n2.4. sim titleConcl\n2.5. sim titleAbstr\nAs shown in Figure 1, the correlation coefficients of the features (e.g., sim title Abstr) rank right after those of the two in-paper count features. As we will show soon, these features (e.g., sim titleCore) can work synergetically with count-based features for predicting academic influence.\nThe second group of similarity-based features use citation context instead of the title of a cited paper:\n2. Similarity-based features\n2.6. sim contextTitle\n2.7. sim contextIntro\n2.8. sim contextConcl\n2.9. sim contextAbstr\nThese features compare the similarities between citation contexts and the title, abstract, and conclusion of the citing paper. We found that the context\u2013abstract (sim contextAbstr) similarity feature is the one in this group that is most correlated with academic influence, followed by context\u2013conclusion (sim contextIntro), context\u2013title(sim contextT itle), and context\u2013introduction (sim contextConcl).\nWe turn to the context-based features. First, we focus on the features that consider the relation between the citation and the citation context:\n3. Context-based features\n3.1. contextMeta authorMentioned\n3.2. contextMeta appearAlone\n3.3. contextMeta appearFirst\nIn this group, contextMeta authorMentioned has the highest correlation coefficient. This feature indicates whether the names of the authors appear in the citation context (e.g., \u201cSmith et al. [4]\u201d).\nThe second group of context-based features are based on the meaning of the words in the citation context:\n3. Context-based features\n3.4. contextLex relevant\n3.5. contextLex recent\n3.6. contextLex extreme\n3.7. contextLex comparative\n3.8. contextLexOsg wnPotency\n3.9. contextLexOsg wnEvaluative\n3.10. contextLexOsg wnActivity\n3.11. contextLexOsg giPotency\n3.12. contextLexOsg giEvaluative\n3.13. contextLexOsg giActivity\n3.14. contextLexEmo emo\n3.15. contextLexEmo polarity\nWe used several different types of lexicons to capture different aspects of semantics in the citation contexts, including sentiment, emotion, and Osgood et al.\u2019s (1957) semantic differential categories.\nAs we mentioned in the preceding section, contextLexEmo polarity is the number of words in the citation context that are labeled either positive or negative and contextLexEmo emo is the number of words that are labeled with any of the eight basic emotions. Our hope was that any kind of sentiment or\nemotion in the words in the citation context might indicate that the citation is influential, even if the sentiment or emotion is negative, but Figure 1 shows that neither feature has a high correlation with the gold labels. This suggests to us that it might be better to split these features into more specific features for each of the possible categories.\nTo test this idea, we split contextLexEmo polarity into two features, one for positive polarity and one for negative polarity, and we split contextLexEmo emo into eight features, one for each of the eight basic emotions. Figure 2 shows the correlation coefficients for each of these more specific features.\nWe see in Figure 2 that positive polarity has a higher correlation than negative polarity. Among the eight basic emotions, surprise has the highest correlation. These results are intuitively reasonable. However, none of the correlations is greater than 0.06. It seems that none of these features are likely to be of much use for predicting influence.\nLet us consider the position-based features. First, we examine the position of a citation in a sentence:\n4. Position-based features\n4.1. posInSent begin\n4.2. posInSent end\nOur results suggest that references located at the beginning of a sentence might be more influential.\nThe second group of position-based features calculates the locations of citations in the body of the paper:\n4. Position-based features\n4.3. posInPaper stdVar\n4.4. posInPaper mean\n4.5. posInPaper last\n4.6. posInPaper first\nThe best paper-position-based feature is the standard variance of a reference\u2019s positions (posInPaper stdV ar). Note that this feature is likely to overlap with the in-paper-counts features to some degree: A larger in-paper-counts number could correspond to a higher position variance, so these features may not have additive benefit when used together.\nFinally, we examine the miscellaneous features:\n5. Miscellaneous features\n5.1. aux citeCount\n5.2. aux selfCite\n5.3. aux yearDiff\nFigure 1 shows that the correlation coefficient between citation counts and the influence labels is positive. This confirms the previous finding that highly cited papers are more likely to be cited in a meaningful manner (Bornmann & Daniel, 2008a). However, we find that the correlation is moderate: It is smaller than that of half of the features we tested. Hence, while highly cited papers may have more academic influence, the citation count is not an ideal indicator of influence. This result is consistent with the fact that papers are often cited for reasons other than academic influence. When a paper is highly cited, the authors of the citing paper may feel obliged to cite it, yet might not take the time to read it.\nFrom Figure 1, we see a small positive correlation between self-citation and the gold influence labels. We will see later that aux selfCite is useful as a corrective factor in the final model.\nAre older papers more likely to be influential? In Figure 1, the aux yearDiff feature has a small positive correlation with the influence. We discretized the feature over the ranges 0, 1, . . . , 10, 11\u201320, 21\u201330, 31+. The corresponding Pearson coefficients are given in Figure 3. The figure shows that, when the cited paper is one or four to seven years older than the citing paper, there is a positive correlation with academic influence. More recent papers (0, 2 and 3 years) and older papers (\u2265 8 years) are poorly or negatively correlated with academic influence. This result is consistent with our interest (mentioned in the section on defining academic influence) in the proximate influences on a citing paper."}, {"heading": "Results of predicting academic influence", "text": "We used the LIBSVM support vector machine (SVM) package (Chang & Lin, 2011) as our supervised learning algorithm.8 We chose a second-degree polynomial as our kernel function.9\nThe F-measure was applied to evaluate the performance of the learned model. The F-measure is defined as the harmonic mean of precision P and recall R: F = 2PR/(P +R). The precision of a model is the conditional probability that a paper\u2013reference pair is influential (according to the gold-standard author-generated label), given that the model guesses that it is influential. The recall of a model is the conditional probability that the model guesses that a paper\u2013reference pair is influential, given that it actually is influential (according to the gold-standard).\nAnother metric is accuracy, defined as the ratio of properly classified paper\u2013 reference pairs (as either influential or not) over the total number of classified pairs. However, the classes in our data are imbalanced (10.3% in the influential class and 89.7% in the non-influential class). This makes accuracy inappropriate as a performance measure for our task, because we could achieve an accuracy of 89.7% by the trivial strategy of always guessing the non-influential class.\nThe F-measure is a better performance measure for imbalanced classes, because it rewards a model that has a balance of precision and recall. Always guessing non-influential yields an F-measure of zero (we take division by zero to be zero). Always guessing influential yields an F-measure of 18.7% (2 \u00b7 0.103 \u00b7 1/(0.103 + 1) = 0.187). Unlike accuracy, the F-measure penalizes trivial models.\nThe SVM algorithm is designed to optimize accuracy, whereas we want to optimize the F-measure. Since an SVM does not directly optimize the F-measure\nand our data are not balanced, we used a simple down-sampling method to handle this. In each fold of cross validation, we randomly down-sampled the negative instances (non-influential references) in the training data to make their number equal to that of the positive ones (influential references).\nTable 3 shows the scores of different models under ten-fold cross-validation. We included two baselines. The first baseline randomly labels a reference with a probability equal to the distribution of the labels in the training data. The second predicts the academic influence of references based on their Google citation counts (aux citeCount). Note that the macro-averaged F-measure is not necessarily between the averaged precision and recall. For example, for model (3) in the table, 0.35 is not between 0.36 and 0.41.\nStarting with model (3) in the table, we added features greedily: In each round, the feature resulting in the maximum improvement of the F-measure was added. That is, model (3) is the best model that uses only one single feature, and the best performing model (with four features) is model (6).\nIn Table 3, all of the models marked with a dagger sign (\u2020) are statistically significantly better than model (3). The models marked with an asterisk (*) are statistically significantly better than the two baselines. We use a one-tailed paired t-test with a 99% significance level.\nThe first feature chosen for the model by greedy feature selection is counts InPaper whole, the feature with the highest correlation in Figure 1. The best model achieves an F-measure of about 42% (see Table 3). The model uses only four features, two of which are count-based (countsInPaper) and one semanticsbased (sim titleCore). Adding more features to model (6) did not result in further improvement. Using all features presented in Figure 1 results in an F-measure of 37%, which is significantly better than model (1) and (2) (p < 0.01), insignificantly better than the best single-feature model (3) (p > 0.05), and worse than model (6) (p < 0.01). This observation supports the hypothesis that feature selection is useful for this task. In general, feature selection removes useless or detrimental features, which often leads to better performance (e.g., higher F-measures) and greater efficiency.\nWe find it interesting that a semantic feature (sim titleCore, the similarity between the title of the cited paper and the core sections of the citing paper) is the second feature chosen. It seems that this feature complements the count-based feature; it covers some papers that are influential but have lower counts. The improvement of model (4) over model (3) is about 4% in terms of F-measure, which is statistically significant at a level of p < 0.01. Using another count-based feature, countsInPaper secNum, additionally improves the performance. Although aux selfCite by itself has a small correlation with influence (see Figure 1), it seems to be useful when combined with the other three features.\nNote that the F-measure we used here is the macro-averaged F-measure (Lewis, 1991). That is, we calculated the F-measure for each paper individually and then computed the arithmetic average over all the F-measures obtained. For each reference in a given paper, we used the SVM model to estimate the probability that the reference is labeled influential. In the training data, the average paper contained three influential references. Therefore the model guesses that the top three references in the given paper, with the highest estimated probabilities, are influential, and the remaining references, with lower probabilities, are non-influential.\nIt is difficult to describe an SVM model intuitively. Moreover, given an SVM model, it is not straightforward to describe the importance of a feature. To further assess the importance of the four features in model (6), we have also applied logistic regression to our data (Long, 1997). Logistic regression assumes that the probability distribution of some binary random variable Y is of the form\nP (Y = 1| ~X) = 1 1 + e\u2212(\u03b20+ \u2211 i \u03b2iXi)\nwhere ~X = (X1, X2, . . .) are feature values and \u03b20, \u03b21, \u03b22, . . . are weight vectors. Given N training instances, we can solve for the weights ~W = (\u03b20, \u03b21, . . .) by maximizing the likelihood function\nl( ~W ) = \u220fN\ni=1 P (Y = 1|X)ti(1\u2212 P (Y = 1|X))1\u2212ti\nwhere ti \u2208 {0, 1} is the binary gold label of the ith training instance. Once we have solved for the weights, we can classify instances by using a threshold \u03c9. That is, given an instance with feature values ~X, we predict Y = 1 if P (Y = 1| ~X) > \u03c9 and we predict Y = 0 otherwise. For our application, we set the threshold so that the relative number of influential citations is the same as in the training set.\nWith logistic regression, the magnitude (absolute value) of the weights, \u03b2i, indicates the importance of the corresponding feature in the model. We used the mnrfit command in Matlab to conduct logistic regression on our data. The weights assigned to the features countsInPaper whole, sim titleCore, countsInPaper secNum, and aux selfCite are 2.7228, 1.2683, 1.1763, and - 0.0923. Their absolute values correspond to the order they are selected by SVM\nin Table 3: The most important feature is countsInPaper whole and the least important is aux selfCite. Note that the weight for aux selfCite is smallest, which corresponds to the observation that self-citations are less likely to be influential. We have also used logistic regression to classify the references, using the same experimental setup as we used for the SVM. The logistic regression performance is slightly below that of the SVM (\u2248 0.37 vs. 0.41).10\nWhen only one feature is used with SVM (as in model (3)), the classification task can be regarded as setting a threshold on the feature, to separate the influential references from the rest. In Figure 4, we vary such a threshold to provide a full view of the F-measures of the two most relevant features shown in Figure 1. Different thresholds here resulted in different percentages of references being predicted as influential, corresponding to the x-axis in the figure. Note that our thresholds are in the range [0, 1], since we have normalized the count values (and all other features) to this range (as we discussed earlier).\nFigure 4 also includes the F-measure curve of random guesses (random in the figure). This curve serves as a minimum baseline for comparison with the other features. Model (1) in Table 3 corresponds to the point on the random curve such that the percentage of references predicted as influential equals the size of the influential class ( 322/3143 = 10.3%).\nIn Figure 4, the peak F-measure of our best feature, countsInPaper whole, is 0.37 (when the value on the x-axis is 13%). Model (3) attained an F-measure of 0.35 in Table 3, slightly below the value of 0.37 in Figure 4. Model (3) was trained and tested with ten-fold cross validation, whereas the F-measure of 0.37 is based on using the whole dataset as training data, with no independent testing data; thus the small gap in the F-measures (0.35 versus 0.37) indicates that SVM is performing well.\nExperiments with in-paper citation counts\nIn the preceding section, we made a number of observations. A significant one is that the in-paper citation counts (how many times a reference is cited in a paper) are the most predictive features for academic influence. The following experiments are designed to further validate our results. Since the in-paper counts convey influence information, which is ignored in the conventional counting of citations, we wondered whether incorporating in-paper citation numbers into global citation counting would result in different rankings of papers and authors.\nIn contrast to the conventional citation counting, we refer to the methods that take into consideration the in-paper counts as influence-primed citation counts. We conducted two types of experiments. First, we explored the correlation between the rankings of papers and authors with and without influenceprimed citation counts. Second, we tackled the task of identifying ACL Fellows."}, {"heading": "Influence-primed citation counts", "text": "A classical citation network is a graph in which the nodes (vertices) correspond to papers and there is a directed link (directed edge) from one paper to another if the first paper cites the second paper. The network is usually acyclic (it has no loops), due to time: An earlier paper rarely cites a later paper (with some exceptions, due to overlap in the gestation periods of publications).\nA slightly more sophisticated citation network could have weights or labels associated with the edges in the graph. For example, a directed edge from citing paper X to cited paper Y might be labeled as Y provides evidence that supports claims in X, or the directed edge might be weighted with a number that indicates how influential Y is to X.\nThere are various ways that in-paper citation counts can be used to modify classical citation networks. We have experimented with using in-paper citation counts for filtering edges in the graph and for weighting edges in the graph.\nA simple filtering method is to drop the edge from citing paper X to cited paper Y when the in-paper citation count for Y in X is below a threshold. Equivalently, when building a citation network, only add an edge when the in-paper citation count is greater than or equal to a threshold.\nWe have also tried filtering with a combination of two thresholds, T1 and T2. For a given paper\u2013reference pair (i.e., a given citing\u2013citer pair in the citation network), when building a network, we add an edge from citing paper X to cited paper Y based on the in-paper citation count of the reference Y (i.e., the number of times Y is mentioned anywhere in the body of X) and the rank of the reference Y relative to the other references in X (i.e., the rank in a list, sorted in descending order of in-paper citation counts). An edge is added only if the in-paper citation count is at least T1 and the rank is less than T2 (lower rank is better, because the list is sorted in descending order).\nAn alternative to filtering is weighting edges. The edge between a reference and a citer can be weighted by the in-paper citation counts. Given a citing\npaper X and a cited paper Y , suppose that Y is mentioned c times in the body of X. There are many functions that we might apply to convert the in-paper citation count c to a weight. Any linear or polynomial function might be useful.\nIn the following experiments, we use the square of the in-paper count to weight an edge: We weight the edge from X to Y with c2. Squaring c gives more weight to higher values of c.\nThe conventional citation count for a paper is calculated from a citation network. It is the number of edges in the graph that are directed into the vertex that represents the given paper. That is, the conventional citation count for a paper is the number of papers that cite the given paper.\nWe weight citations according to the square of the number of times the reference is mentioned in the text. For example, being cited once in a paper that mentions the reference once counts for one whereas being cited once in a paper that mentions the reference twice counts for four. Weights are added up: Being cited four times by four papers that mention the reference once counts the same as being cited once by a paper that mentions the reference two times.\nThe influence-primed citation count is like the conventional citation count, except it weights each edge by c2, instead of 1. We define influence-primed citation count formally as follows:\nDefinition. Given two papers, pi and pj, let c(pi, pj) be the number of times paper pi mentions paper pj in the body of its text, excluding the reference section. If the paper pi cites paper pj, then c(pi, pj) > 0; otherwise c(pi, pj) = 0. Let L (the literature) be the set of all papers in the given citation network. The influence-primed citation count of paper pj, cip(pj), is\u2211\npi\u2208L c(pi, pj)\n2.\nThe function name, cip(\u00b7), stands for citations, influence-primed. The conventional h-index for an author is the largest number h such that at least h of the author\u2019s papers are cited by at least h other papers. Each citation of a paper has a weight of 1.\nThe influence-primed h-index for an author is like the conventional h-index, except it weights each edge by c2, instead of 1. The influence-primed h-index for an author is the largest number h such that at least h of the author\u2019s papers have an influence-primed citation count of at least h.\nFor example, if an author has four papers, each one cited only once, but each time they are cited, they are mentioned twice, then the influence-primed h-index is 4. In contrast, with the conventional h-index, the same author would receive an h-index of 1.\nWe define influence-primed h-index formally as follows:\nDefinition. An author, ai, with a set of papers O(ai) (the \u0153uvre) has an influence-primed h-index, hip(ai), of h if h is the largest value such that\n|{pj \u2208 O(ai)|cip(pj) \u2265 h}| \u2265 h.\nThe function name, hip(\u00b7), stands for h-index, influence-primed. We refer to this as the hip-index."}, {"heading": "ACL Anthology Network", "text": "For this experiment, we use the AAN (ACL Anthology Network) dataset (Radev, Muthukrishnan, & Qazvinian, 2009). AAN is a citation network constructed from the papers published in Association for Computational Linguistics (ACL) venues (conferences, workshops, and journals since 1965), approximately 20,000 papers. The AAN citation network is a closed graph; edges from or to the papers published outside ACL venues are not included. In effect, by using the AAN citation network, we can measure the impact of researchers and papers on the ACL community. This restriction might be desirable when we try to identify the recipients of honours granted by ACL. Table 4 shows the basic statistics for the dataset.11\nTo obtain in-paper counts, we used regular expressions to locate citations. Our regular expressions are good at both precision and recall according to our manual examination, but they still make a few errors. For example, the regular expressions have trouble with citations that span two lines of text and with multiple papers written by the same author in the same year. Another problem is automatically distinguishing the main body text from the reference section of a paper. The regular expressions may wrongly increment the in-paper count by matching citations in the reference section.\nNumerical citations (e.g., \u201c[1]\u201d or \u201c[1,2,3]\u201d) are more difficult to process than textual citations (e.g., \u201cSmith et al. (1998)\u201d). We used a random number generator to select a sample of 100 papers from AAN and then we manually determined their citation types. In this random sample, 7% used numerical citations. Since numerical citations are relatively rare in the AAN dataset, we simply ignored them.\nWe did not normalize the in-paper citation count c for these experiments with the AAN dataset. In this section, the in-paper citation count c is a nonnegative integer value. The reason for this is that the AAN network is a closed graph: All citations to and from papers outside of the AAN dataset are ignored in the AAN citation network. The maximum value that we used for contextual normalization in the preceding section, max(pi, ri\u2217, fk), could be distorted by the ignored citations. For example, a paper that mainly cites the AAN papers could be normalized very differently from one that mainly cites non-AAN papers. The\nmaximum value may be highly sensitive to whether a citing paper is influenced by cited work that is outside of the AAN network.\nThe in-paper citation count c that we use in this section is essentially a raw (unnormalized) variation of countsInPaper whole. We did not use counts InPaper secNum, because it might be more sensitive to noise introduced by the process of automatically detecting section boundaries. (We manually detected sections in the preceding experiments, but this manual process does not scale up from 100 papers to 20,000 papers.) Figure 4 suggests that countsInPaper whole performs better than countsInPaper secNum, and it is easier to compute."}, {"heading": "Conventional versus influence-primed counting", "text": "A natural question is whether there is any difference between conventional citation counts and influence-primed citation counts. In particular, do the two approaches yield different rankings of the papers?\nTable 5 shows the Spearman correlation coefficients between the AAN papers. We grouped the papers according to their ranks in the conventional counting. For each group, we calculated the Spearman correlation coefficient between the conventional counts and the influence-primed counts.\nFor example, papers 1\u2013100 are the top 100 most highly cited papers, according to conventional citation counts, where each edge directed into a given paper increments that paper\u2019s count by one. For these 100 papers, we have a vector of 100 conventional citation counts. We also calculate a vector of 100 influence-primed citation counts. The Spearman correlation between these two vectors is 0.67.12\nFor the top 100 most highly cited papers, conventional citations counts and influence-primed citation counts have a high correlation. As we move down the list, the correlation drops. The two counts agree on the most highly ranked papers, but they disagree on the less cited papers. Weighting makes a difference.\nTable 6 shows the Spearman correlation coefficients for the AAN authors, under these two different counting methods. The h-indexes of the authors were calculated and were used to rank the authors. For each group of authors, we calculate the Spearman correlation between the h-indexes and the hip-indexes. Comparing Tables 5 and 6, we see that the authors\u2019 correlations steadily decline as we go down the rows of Table 6, but the papers\u2019 correlations fluctuate with no clear trend as we go down the rows of Table 5. This indicates that conventional citation counts and influence-primed citation counts are not related by a simple linear transformation. That is, influence-primed counting is different from conventional counting in a non-trivial way."}, {"heading": "Identifying ACL Fellows", "text": "The Association for Computational Linguistics has seventeen fellows.13 We might assume that these seventeen fellows can be identified by selecting the authors having the best h-index scores. Indeed, Hirsch (2007) found that the h-index was superior at predicting the future performance of a researcher than the number of citations, the number of papers, and mean citations per paper (Lehmann, Jackson, & Lautrup, 2006).\nTable 7 shows the precision of the conventional h-index and influence-primed hip-index at identifying ACL Fellows. For a given value of N , we sort all authors in AAN in descending order of their h-indexes and then count the number of ACL Fellows among the top N authors in the sorted list. We also sort all authors in AAN in descending order of the hip-indexes and then count the number of ACL Fellows among the top N . Precision is the number of ACL Fellows in the top N divided by N . Table 7 shows the precision as N ranges from 1 to 17: we stop at 17 because there are 17 ACL Fellows in total. For example, the second row in the body of the table, with N = 2, shows that zero of the top two h-index ranked authors are ACL Fellows (precision 0%), but one of the top\ntwo hip-index ranked authors is an ACL Fellow (precision 50%). For N = 3, h-index finds zero ACL Fellows but hip-index finds two Fellows.\nThe asterisks (*) in the table mark where the two methods have different results. The table shows that the influence-primed model identified the ACL fellows with a better precision for seven values of N . When we limit N to less than or equal to seventeen, hip-index is never worse but often better than h-index. As N grows, the differences between the two indexes become negligible. At N = 17, both indexes identify 5 out 17 ACL Fellows.\nThe last row of Table 7 shows the average precision measure (AveP), which is commonly used to evaluate search engines (Buckley & Voorhees, 2000). The formula for AveP is\nAveP(nc) =\n\u2211nc k=1(P (k)\u00d7 rel(k))\nnr ,\nwhere nc is the point at which we cut off the list of search results for a search engine (in our case, nc = 17, where we cut off the ranked lists of authors), nr is the total number of relevant documents (in our case, the total number of ACL Fellows, nr = 17), P (k) is the precision at each observation point (in our case, k ranges from 1 to 17), and rel(k) is an indicator function that equals 1 if the k-th document is relevant to the given query and 0 otherwise (in our case, rel(k)\nis 1 if the k-th author is an ACL Fellow and 0 otherwise). From the table, we can see that the AveP score of the influence-primed model is 14% whereas that of the conventional model is 10%.\nThis better average precision measure is encouraging evidence that weighting the citations by our measures of influence could improve the identification of the best scientists."}, {"heading": "Future work and limitations", "text": "Further work is needed to validate these results over more extensive and different datasets. We rely on authors to annotate their own papers. However, we did not assess the reliability of authors at identifying the key references. For papers with several authors, we could ask more than one author to provide annotation. Thus we could quantify the inter-annotator agreement. We could also ask the same authors, after a long delay, to annotate their own papers again. Furthermore, we would find it interesting to compare the performance of a machine learning approach with human-level performance. For this purpose, we could recruit independent annotators having specific degrees of expertise.\nWe assumed that the full text of the citing paper was available. Yet some citation indexing databases have a more limited access to the content due to copyright restrictions or technical limitations. It should not be difficult to extend these databases so that they have the necessary information to identify influential references or to count the number of times a paper mentions another. For example, this information could be provided by the copyright owner without giving access to the full text.\nIntentionally, we limited our feature set so that we did not have to recover full text of the cited work (only the title). However, many other features are possible if we access the full text of both the citer and cited paper: Hou et al. (2011) measure similarity by the overlap in the reference section. We could also add other features such as the prestige of the cited venue or the prestige of the cited authors (Zitt & Small, 2008). We also did not take into account the relationships between authors. Maybe authors who are similar or related are more likely to influence each other. In related work, Ajiferuke, Lu, and Wolfram (2010) proposed to measure the number of citers (authors who cite) rather than the number of citations.\nMoreover, identifying the genuinely significant citations might be viewed as an adversarial problem. Indeed, some authors and editors attempt to game (i.e., manipulate or exploit) citations counts. In a survey, Wilhite and Fong (2012) found that 20% of all authors were coerced into citing some references by an editor, after their manuscript had undergone normal peer review. In fact, the majority of authors reported that they were willing to add superfluous citations if it is an implied requirement by an editor. If we could determine that many non-influential references in some journals are citing some specific journals, this could indicate unethical behavior.\nOur approach for identifying celebrated scientists is simple. State-of-theart approaches such as that of Rokach, Kalech, Blank, and Stern (2011) can achieve better precision and recall. We expect that they would benefit from an identification of the influential citations.\nIn some circumstances counting multiple occurrences of a citation might require coreference resolution (Soon, Ng, & Lim, 2001; Athar & Teufel, 2012). By convention, some authors and editors only ever cite a reference once but mention it several times either with a nominal, or pronominal reference.\nMazloumian (Mazloumian, 2012) found that a useful predictor of future performance was the annual number of citations at the time of citation. Maybe the annual number of influential citations could be a superior predictor.\nA finer view of the problem is to rank or rate the references by their degrees of academic influence, which however could bring further complexity that we are avoiding in the present work; e.g., comparing two less influential or uninfluential references could be a harder task even for human annotators, and such annotation may be difficult to interpret.\nNevertheless, a weighted citation measure based on the number of occurrences of a citation could significantly alter other evaluation metrics that depend on simple citation counts, such as Impact Factor (Garfield, 2006), Eigenfactor, and Article Influence (Bergstrom, 2007). Even though such a refinement would not address the statistical charges leveled against citation-based evaluation metrics (Adler, Ewing, & Taylor, 2009), it would at least address to some degree the need to distinguish between citations that acknowledge an intellectual debt and de-rigeur citations.\nWe conjecture that some types of research papers that tend to be highly cited, such as review or methodological articles, are less likely to be perceived as influential. Hou et al. (2011) found that weighting citations by the in-paper frequency reduced the importance of reviews. We should further investigate this issue to verify whether original contributions are significantly more likely to be perceived as influential."}, {"heading": "Conclusions", "text": "One of our main results is that counting the number of times a paper is cited (countsInPaper whole) is one of the best predictors of how influential a reference is. This confirms an earlier result by Hou et al. (2011) who stated, \u201cCitation frequency of individual articles in other papers more fairly measures their scientific contribution than mere presence in reference lists.\u201d Alternatively, we can count the number of sections in which a paper is cited (countsInPaper secNum).\nWe believe that in assessing the influence of a research paper or researcher, weighting the citations by these features (e.g., countsInPaper whole) would provide more robust results. It should also be used when tracking follow-up work or recommending research papers.\nWe have also shown that we could combine the in-paper citation counts (countsInPaper) and the semantic relatedness between a reference and the citing paper, to derive a superior classifier. Though self-citations are only slightly correlated with academic influence, a classifier can derive some benefits when combining it with other features."}, {"heading": "Acknowledgments", "text": "We are grateful to the volunteers who identified key citations in their own work. Daniel Lemire acknowledges support from the Natural Sciences and Engineering Research Council of Canada (NSERC) with grant number 26143. We thank M. Couture, V. Larivie\u0300re and the anonymous reviewers for their helpful comments."}, {"heading": "Notes", "text": "1See http://www.wjh.harvard.edu/~inquirer/ to obtain a copy of the General Inquirer lexicon.\n2See http://wordnet.princeton.edu/ to download the WordNet lexicon. 3This normalization leaves binary values unchanged, so it makes no difference whether it\nis applied to them. 4See http://tinyurl.com/counting-citations. 5See http://tinyurl.com/influential-references. 6The dataset is freely available online at http://lemire.me/citationdata/. 7See http://opennlp.apache.org/ to download OpenNLP. 8LIBSVM is available for download at http://www.csie.ntu.edu.tw/~cjlin/libsvm/. 9The LIBSVM parameters we used are \u201c-s 0 -d 2 -t 1 -r 1\u201d.\n10With logistic regression, the precision and recall are 0.38 and 0.42. 11The AAN corpus is available at http://clair.eecs.umich.edu/aan/index.php. 12Spearman correlation is specifically intended for comparing ranked lists, whereas Pearson correlation is more appropriate when numerical values are more important than ranks. 13ACL Fellows are listed at http://aclweb.org/aclwiki/index.php?title=ACL_Fellows. When we performed our experiments, there were only seventeen fellows, but there are more now."}], "references": [{"title": "Coherent citation-based summarization", "author": ["A. Abu-Jbara", "D. Radev"], "venue": null, "citeRegEx": "Abu.Jbara and Radev,? \\Q2011\\E", "shortCiteRegEx": "Abu.Jbara and Radev", "year": 2011}, {"title": "Automatically classifying the role", "author": ["S. Agarwal", "L. Choubey", "H. Yu"], "venue": null, "citeRegEx": "Agarwal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2010}, {"title": "A comparison of citer and citationbased measure outcomes for multiple disciplines", "author": ["I. Ajiferuke", "K. Lu", "D. Wolfram"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "Ajiferuke et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ajiferuke et al\\.", "year": 2010}, {"title": "Detection of implicit citations for sentiment detection", "author": ["A. Athar", "S. Teufel"], "venue": "In Proceedings of the Workshop on Detecting Structure in Scholarly Discourse,", "citeRegEx": "Athar and Teufel,? \\Q2012\\E", "shortCiteRegEx": "Athar and Teufel", "year": 2012}, {"title": "Positive and negative aspects of citation indices and journal impact factors", "author": ["A.T. Balaban"], "venue": "Scientometrics, 92 (2), 241\u2013247.", "citeRegEx": "Balaban,? 2012", "shortCiteRegEx": "Balaban", "year": 2012}, {"title": "Measuring the value and prestige of scholarly journals", "author": ["C. Bergstrom"], "venue": "College & Research Library News, 68 (5), 314\u2013316.", "citeRegEx": "Bergstrom,? 2007", "shortCiteRegEx": "Bergstrom", "year": 2007}, {"title": "A principal component analysis of 39 scientific impact measures", "author": ["J. Bollen", "H. Van de Sompel", "A. Hagberg", "R. Chute"], "venue": "PloS One,", "citeRegEx": "Bollen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bollen et al\\.", "year": 2009}, {"title": "Functional use of frequently and infrequently cited articles in citing publications", "author": ["L. Bornmann", "Daniel", "H.-D"], "venue": "European Science Editing,", "citeRegEx": "Bornmann et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bornmann et al\\.", "year": 2008}, {"title": "What do citation counts measure? a review of studies on citing behavior", "author": ["L. Bornmann", "Daniel", "H.-D"], "venue": "Journal of Documentation,", "citeRegEx": "Bornmann et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bornmann et al\\.", "year": 2008}, {"title": "Are there better indices for evaluation purposes than the h index? a comparison of nine different variants of the h index using data from biomedicine", "author": ["L. Bornmann", "R. Mutz", "Daniel", "H.-D"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "Bornmann et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bornmann et al\\.", "year": 2008}, {"title": "Evaluating evaluation measure stability", "author": ["C. Buckley", "E.M. Voorhees"], "venue": "In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Buckley and Voorhees,? \\Q2000\\E", "shortCiteRegEx": "Buckley and Voorhees", "year": 2000}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chang", "C.-C", "Lin", "C.-J"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Content analysis of references: adjunct or alternative to citation counting", "author": ["D.E. Chubin", "S.D. Moitra"], "venue": "Social studies of science,", "citeRegEx": "Chubin and Moitra,? \\Q1975\\E", "shortCiteRegEx": "Chubin and Moitra", "year": 1975}, {"title": "Parscit: An open-source crf reference string parsing package", "author": ["I.G. Councill", "C.L. Giles", "Kan", "M.-Y"], "venue": "In Proceedings of the Language Resources and Evaluation Conference. European Language Resources Association", "citeRegEx": "Councill et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Councill et al\\.", "year": 2008}, {"title": "Using hedges to classify citations in scientific articles", "author": ["C. Di Marco", "F.W. Kroon", "R.E. Mercer"], "venue": "Computing Attitude and Affect in Text: Theory and Applications,", "citeRegEx": "Marco et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marco et al\\.", "year": 2006}, {"title": "Unsupervised prediction of citation influences", "author": ["L. Dietz", "S. Bickel", "T. Scheffer"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Dietz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dietz et al\\.", "year": 2007}, {"title": "Applying weighted pagerank to author citation networks", "author": ["Y. Ding"], "venue": "Journal of the American Society for Information Science and Technology, 62 (2), 236\u2013245.", "citeRegEx": "Ding,? 2011", "shortCiteRegEx": "Ding", "year": 2011}, {"title": "Popular and/or prestigious? measures of scholarly esteem", "author": ["Y. Ding", "B. Cronin"], "venue": "Information processing & management,", "citeRegEx": "Ding and Cronin,? \\Q2011\\E", "shortCiteRegEx": "Ding and Cronin", "year": 2011}, {"title": "The most influential paper Gerard Salton never wrote", "author": ["D. Dubin"], "venue": "Library Trends, 52 (4), 748\u2013764.", "citeRegEx": "Dubin,? 2004", "shortCiteRegEx": "Dubin", "year": 2004}, {"title": "Theory and practise of the g-index", "author": ["L. Egghe"], "venue": "Scientometrics, 69 (1), 131\u2013152.", "citeRegEx": "Egghe,? 2006", "shortCiteRegEx": "Egghe", "year": 2006}, {"title": "Can citation indexing be automated", "author": ["E. Garfield"], "venue": "Statistical association methods for mechanized documentation, Symposium proceedings, ed. M. E. Stevens et al. (Washington: National Bureau of Standards, Miscellaneous Publication 269, 1965), pp. 188\u2013192.", "citeRegEx": "Garfield,? 1965", "shortCiteRegEx": "Garfield", "year": 1965}, {"title": "The history and meaning of the journal impact factor", "author": ["E. Garfield"], "venue": "JAMA: the Journal of the American Medical Association, 295 (1), 90\u201393.", "citeRegEx": "Garfield,? 2006", "shortCiteRegEx": "Garfield", "year": 2006}, {"title": "Can nobel prize winners be predicted. In 135th meetings of the American Association for the Advancement", "author": ["E. Garfield", "M.V. Malin"], "venue": null, "citeRegEx": "Garfield and Malin,? \\Q1968\\E", "shortCiteRegEx": "Garfield and Malin", "year": 1968}, {"title": "Towards an automated citation classifier", "author": ["M. Garzone", "R. Mercer"], "venue": "Advances in Artificial Intelligence,", "citeRegEx": "Garzone and Mercer,? \\Q2000\\E", "shortCiteRegEx": "Garzone and Mercer", "year": 2000}, {"title": "Referencing as persuasion", "author": ["G.N. Gilbert"], "venue": "Social Studies of Science, 7 (1), 113\u2013122.", "citeRegEx": "Gilbert,? 1977", "shortCiteRegEx": "Gilbert", "year": 1977}, {"title": "CiteSeer: an automatic citation indexing system", "author": ["C.L. Giles", "K.D. Bollacker", "S. Lawrence"], "venue": "In Proceedings of the third ACM conference on Digital libraries, DL", "citeRegEx": "Giles et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Giles et al\\.", "year": 1998}, {"title": "Why it has become more difficult to predict nobel prize winners: a bibliometric analysis of nominees and winners of the chemistry and physics prizes (1901\u20132007)", "author": ["Y. Gingras", "M.L. Wallace"], "venue": "Scientometrics,", "citeRegEx": "Gingras and Wallace,? \\Q2010\\E", "shortCiteRegEx": "Gingras and Wallace", "year": 2010}, {"title": "A new approach to the metric of journals scientific prestige: The sjr indicator", "author": ["B. Gonzalez-Pereira", "V.P. Guerrero-Bote", "F. Moya-Aneg\u00f3n"], "venue": "Journal of Informetrics,", "citeRegEx": "Gonzalez.Pereira et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gonzalez.Pereira et al\\.", "year": 2010}, {"title": "Using categorisations of citations when assessing the outcomes from health", "author": ["S. Hanney", "I. Frame", "J. Grant", "M. Buxton", "T. Young", "G. Lewison"], "venue": "research. Scientometrics,", "citeRegEx": "Hanney et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hanney et al\\.", "year": 2005}, {"title": "Phrases as subtopical concepts in scholarly text", "author": ["Haque", "A.-u", "P. Ginsparg"], "venue": "In Proceedings of the 11th annual international ACM/IEEE joint", "citeRegEx": "Haque et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Haque et al\\.", "year": 2011}, {"title": "An index to quantify an individual\u2019s scientific research output", "author": ["J.E. Hirsch"], "venue": "Proceedings of the National Academy of Sciences of the United states of America, 102 (46), 16569.", "citeRegEx": "Hirsch,? 2005", "shortCiteRegEx": "Hirsch", "year": 2005}, {"title": "Does the h index have predictive power", "author": ["J.E. Hirsch"], "venue": "Proceedings of the National Academy of Sciences, 104 (49), 19193\u201319198.", "citeRegEx": "Hirsch,? 2007", "shortCiteRegEx": "Hirsch", "year": 2007}, {"title": "Counting citations in texts rather than reference lists to improve the accuracy of assessing scientific", "author": ["Hou", "W.-R", "M. Li", "Niu", "D.-K"], "venue": "contribution. BioEssays,", "citeRegEx": "Hou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hou et al\\.", "year": 2011}, {"title": "Concentration of the most-cited papers in the scientific literature: analysis of journal ecosystems", "author": ["J.P. Ioannidis"], "venue": "PLoS One, 1 (1), e5.", "citeRegEx": "Ioannidis,? 2006", "shortCiteRegEx": "Ioannidis", "year": 2006}, {"title": "Automatic extraction of citation contexts for research paper summarization: a coreference-chain based approach", "author": ["D. Kaplan", "R. Iida", "T. Tokunaga"], "venue": "In Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries,", "citeRegEx": "Kaplan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kaplan et al\\.", "year": 2009}, {"title": "Detecting citation types using finite-state machines", "author": ["Le", "M.-H", "Ho", "T.-B", "Y. Nakamori"], "venue": "Advances in Knowledge Discovery and Data Mining,", "citeRegEx": "Le et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Le et al\\.", "year": 2006}, {"title": "Meme-tracking and the dynamics of the news cycle", "author": ["J. Leskovec", "L. Backstrom", "J. Kleinberg"], "venue": "In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Leskovec et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Leskovec et al\\.", "year": 2009}, {"title": "Evaluating text categorization", "author": ["D. Lewis"], "venue": "Proceedings of the Speech and Natural Language Workshop, pp. 312\u2013318.", "citeRegEx": "Lewis,? 1991", "shortCiteRegEx": "Lewis", "year": 1991}, {"title": "Finding relevant papers based on citation relations", "author": ["Y. Liang", "Q. Li", "T. Qian"], "venue": "WebAge Information Management,", "citeRegEx": "Liang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "Regression Models for Categorical and Limited Dependent Variables", "author": ["S.J. Long"], "venue": "Sage, New York.", "citeRegEx": "Long,? 1997", "shortCiteRegEx": "Long", "year": 1997}, {"title": "Score-based bibliometric rankings of authors", "author": ["T. Marchant"], "venue": "Journal of the American Society for Information Science and Technology, 60 (6), 1132\u20131137.", "citeRegEx": "Marchant,? 2009", "shortCiteRegEx": "Marchant", "year": 2009}, {"title": "Predicting scholars\u2019 scientific impact", "author": ["A. Mazloumian"], "venue": "PloS One, 7 (11), e49246.", "citeRegEx": "Mazloumian,? 2012", "shortCiteRegEx": "Mazloumian", "year": 2012}, {"title": "The frequency of hedging cues in citation contexts in scientific writing", "author": ["R.E. Mercer", "C. Di Marco", "F.W. Kroon"], "venue": "In Advances in Artificial Intelligence,", "citeRegEx": "Mercer et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mercer et al\\.", "year": 2004}, {"title": "Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon", "author": ["S. Mohammad", "P. Turney"], "venue": "In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text", "citeRegEx": "Mohammad and Turney,? \\Q2010\\E", "shortCiteRegEx": "Mohammad and Turney", "year": 2010}, {"title": "Some results on the function and quality of citations", "author": ["M.J. Moravcsik", "P. Murugesan"], "venue": "Social studies of science,", "citeRegEx": "Moravcsik and Murugesan,? \\Q1975\\E", "shortCiteRegEx": "Moravcsik and Murugesan", "year": 1975}, {"title": "Towards multi-paper summarization using reference information", "author": ["H. Nanba", "M. Okumura"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "Nanba and Okumura,? \\Q1999\\E", "shortCiteRegEx": "Nanba and Okumura", "year": 1999}, {"title": "The measurement of meaning", "author": ["C.E. Osgood", "G.J. Suci", "P.H. Tannenbaum"], "venue": null, "citeRegEx": "Osgood et al\\.,? \\Q1957\\E", "shortCiteRegEx": "Osgood et al\\.", "year": 1957}, {"title": "FaBiO and CiTO: Ontologies for describing bibliographic resources and citations", "author": ["S. Peroni", "D. Shotton"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web,", "citeRegEx": "Peroni and Shotton,? \\Q2012\\E", "shortCiteRegEx": "Peroni and Shotton", "year": 2012}, {"title": "A new approach for scientific citation classification using cue phrases", "author": ["S. Pham", "A. Hoffmann"], "venue": "AI 2003: Advances in Artificial Intelligence,", "citeRegEx": "Pham and Hoffmann,? \\Q2003\\E", "shortCiteRegEx": "Pham and Hoffmann", "year": 2003}, {"title": "Citation influence for journal aggregates of scientific publications: Theory, with application to the literature of physics", "author": ["G. Pinski", "F. Narin"], "venue": "Information Processing & Management,", "citeRegEx": "Pinski and Narin,? \\Q1976\\E", "shortCiteRegEx": "Pinski and Narin", "year": 1976}, {"title": "An algorithm for suffix stripping", "author": ["M. Porter"], "venue": "Program, 14, 130\u2013137.", "citeRegEx": "Porter,? 1980", "shortCiteRegEx": "Porter", "year": 1980}, {"title": "Scientific paper summarization using citation summary networks", "author": ["V. Qazvinian", "D.R. Radev"], "venue": "In Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1,", "citeRegEx": "Qazvinian and Radev,? \\Q2008\\E", "shortCiteRegEx": "Qazvinian and Radev", "year": 2008}, {"title": "Identifying non-explicit citing sentences for citation-based summarization", "author": ["V. Qazvinian", "D.R. Radev"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Qazvinian and Radev,? \\Q2010\\E", "shortCiteRegEx": "Qazvinian and Radev", "year": 2010}, {"title": "Measuring similarity to detect qualified links", "author": ["X. Qi", "L. Nie", "B.D. Davison"], "venue": "In Proceedings of the 3rd international workshop on Adversarial information retrieval on the web,", "citeRegEx": "Qi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Qi et al\\.", "year": 2007}, {"title": "The ACL Anthology Network corpus", "author": ["D.R. Radev", "P. Muthukrishnan", "V. Qazvinian"], "venue": "In Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries,", "citeRegEx": "Radev et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Radev et al\\.", "year": 2009}, {"title": "Comparing citation contexts for information retrieval", "author": ["A. Ritchie", "S. Robertson", "S. Teufel"], "venue": "In Proceedings of the 17th ACM conference on Information and knowledge management,", "citeRegEx": "Ritchie et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ritchie et al\\.", "year": 2008}, {"title": "Who is going to win the next association for the advancement of artificial intelligence fellowship award? evaluating researchers by mining bibliographic data", "author": ["L. Rokach", "M. Kalech", "I. Blank", "R. Stern"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "Rokach et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rokach et al\\.", "year": 2011}, {"title": "To share the fame in a fair way, hm modifies h for multiauthored manuscripts", "author": ["M. Schreiber"], "venue": "New Journal of Physics, 10 (4), 040201.", "citeRegEx": "Schreiber,? 2008", "shortCiteRegEx": "Schreiber", "year": 2008}, {"title": "CiTO, the Citation Typing Ontology", "author": ["D. Shotton"], "venue": "Journal of biomedical semantics, 1 (Suppl 1), S6.", "citeRegEx": "Shotton,? 2010", "shortCiteRegEx": "Shotton", "year": 2010}, {"title": "Read before you cite", "author": ["M.V. Simkin", "V.P. Roychowdhury"], "venue": "Complex Systems,", "citeRegEx": "Simkin and Roychowdhury,? \\Q2003\\E", "shortCiteRegEx": "Simkin and Roychowdhury", "year": 2003}, {"title": "Citation context analysis", "author": ["H. Small"], "venue": "Progress in communication sciences, 3, 287\u2013310.", "citeRegEx": "Small,? 1982", "shortCiteRegEx": "Small", "year": 1982}, {"title": "A machine learning approach to coreference resolution of noun phrases", "author": ["W.M. Soon", "H.T. Ng", "D.C.Y. Lim"], "venue": "Computational linguistics,", "citeRegEx": "Soon et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Soon et al\\.", "year": 2001}, {"title": "The general inquirer: a computer approach to content analysis", "author": ["P.J. Stone", "D.C. Dunphy", "M.S. Smith"], "venue": null, "citeRegEx": "Stone et al\\.,? \\Q1966\\E", "shortCiteRegEx": "Stone et al\\.", "year": 1966}, {"title": "Subject classification of research papers based on interrelationships analysis", "author": ["M. Taheriyan"], "venue": "Proceedings of the 2011 workshop on Knowledge discovery, modeling and simulation, KDMS \u201911, pp. 39\u201344, New York, NY, USA. ACM.", "citeRegEx": "Taheriyan,? 2011", "shortCiteRegEx": "Taheriyan", "year": 2011}, {"title": "Automatic classification of citation function", "author": ["S. Teufel", "A. Siddharthan", "D. Tidhar"], "venue": "In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Teufel et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teufel et al\\.", "year": 2006}, {"title": "Robust classification with context-sensitive features", "author": ["P.D. Turney"], "venue": "Proceedings of the Sixth International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems, pp. 268\u2013276.", "citeRegEx": "Turney,? 1993", "shortCiteRegEx": "Turney", "year": 1993}, {"title": "The management of context-sensitive features: A review of strategies", "author": ["P.D. Turney"], "venue": "Proceedings of Workshop on Learning in ContextSensitive Domains at the 13th International Conference on Machine Learning (ICML96), pp. 60\u201366.", "citeRegEx": "Turney,? 1996", "shortCiteRegEx": "Turney", "year": 1996}, {"title": "Measuring praise and criticism: Inference of semantic orientation from association", "author": ["P.D. Turney", "M.L. Littman"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "Turney and Littman,? \\Q2003\\E", "shortCiteRegEx": "Turney and Littman", "year": 2003}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney and Pantel,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel", "year": 2010}, {"title": "Factors affecting citation rates in environmental science", "author": ["J.K. Vanclay"], "venue": "Journal of Informetrics, 7 (2), 265\u2013271.", "citeRegEx": "Vanclay,? 2013", "shortCiteRegEx": "Vanclay", "year": 2013}, {"title": "A comparison between usage-based and citation-based methods for recommending scholarly research articles", "author": ["A. Vellino"], "venue": "Proceedings of the American Society for Information Science and Technology, 47 (1), 1\u20132.", "citeRegEx": "Vellino,? 2010", "shortCiteRegEx": "Vellino", "year": 2010}, {"title": "The inconsistency of the h-index", "author": ["L. Waltman", "N.J. van Eck"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "Waltman and Eck,? \\Q2012\\E", "shortCiteRegEx": "Waltman and Eck", "year": 2012}, {"title": "Data Mining: Practical Machine Learning Tools and Techniques", "author": ["I.H. Witten", "E. Frank", "M.A. Hall"], "venue": null, "citeRegEx": "Witten et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Witten et al\\.", "year": 2011}, {"title": "Weighted citation: An indicator of an article\u2019s prestige", "author": ["E. Yan", "Y. Ding"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "Yan and Ding,? \\Q2010\\E", "shortCiteRegEx": "Yan and Ding", "year": 2010}, {"title": "Modifying the journal impact factor by fractional citation weighting: The audience factor", "author": ["M. Zitt", "H. Small"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "Zitt and Small,? \\Q2008\\E", "shortCiteRegEx": "Zitt and Small", "year": 2008}], "referenceMentions": [{"referenceID": 30, "context": "Various other measures, such as the h-index (Hirsch, 2005), the g-index (Egghe, 2006) and the hm-index (Schreiber, 2008), refine this basic measure using functions based on the distribution of citations (Bornmann, Mutz, & Daniel, 2008).", "startOffset": 44, "endOffset": 58}, {"referenceID": 19, "context": "Various other measures, such as the h-index (Hirsch, 2005), the g-index (Egghe, 2006) and the hm-index (Schreiber, 2008), refine this basic measure using functions based on the distribution of citations (Bornmann, Mutz, & Daniel, 2008).", "startOffset": 72, "endOffset": 85}, {"referenceID": 57, "context": "Various other measures, such as the h-index (Hirsch, 2005), the g-index (Egghe, 2006) and the hm-index (Schreiber, 2008), refine this basic measure using functions based on the distribution of citations (Bornmann, Mutz, & Daniel, 2008).", "startOffset": 103, "endOffset": 120}, {"referenceID": 40, "context": "Yet other measures of author impact are based on methods for scoring articles with weights and thresholds that depend on the journals in which they were published and the number of times the article was cited (Marchant, 2009).", "startOffset": 209, "endOffset": 225}, {"referenceID": 64, "context": "Readers can often tell quickly whether a citation is shallow from the text itself, which has prompted several efforts to categorize citations by the linguistic context of their occurrence; that is, by the words near the citation in the body of the citing paper (Teufel et al., 2006; Hanney, Frame, Grant, Buxton, Young, & Lewison, 2005; Mercer, Di Marco, & Kroon, 2004; Pham & Hoffmann, 2003).", "startOffset": 261, "endOffset": 392}, {"referenceID": 36, "context": "It has long been recognized that not all citations are created equal and hence they should not be counted equally. Simkin and Roychowdhury (2003) estimated that authors read only 20% of the works they cite.", "startOffset": 7, "endOffset": 146}, {"referenceID": 16, "context": "When an error in a citation is replicated many times, it seems likely that the citers have copied the citation without actually reading the cited paper. As an illustration, Dubin (2004) reported that a commonly cited paper by Gerard Salton does not actually exist.", "startOffset": 131, "endOffset": 186}, {"referenceID": 16, "context": "When an error in a citation is replicated many times, it seems likely that the citers have copied the citation without actually reading the cited paper. As an illustration, Dubin (2004) reported that a commonly cited paper by Gerard Salton does not actually exist. An incorrect citation was accidentally created by mixing the citations for two separate papers. This incorrect citation has since been cited by more than 300 papers. If the citers had tried to read the paper before citing it, they would have discovered that the paper does not exist. Like Moravcsik and Murugesan (1975), we are concerned about the sideeffects of counting insignificant references: \u201cA large fraction of the references are perfunctory.", "startOffset": 131, "endOffset": 585}, {"referenceID": 16, "context": "When an error in a citation is replicated many times, it seems likely that the citers have copied the citation without actually reading the cited paper. As an illustration, Dubin (2004) reported that a commonly cited paper by Gerard Salton does not actually exist. An incorrect citation was accidentally created by mixing the citations for two separate papers. This incorrect citation has since been cited by more than 300 papers. If the citers had tried to read the paper before citing it, they would have discovered that the paper does not exist. Like Moravcsik and Murugesan (1975), we are concerned about the sideeffects of counting insignificant references: \u201cA large fraction of the references are perfunctory. This raises serious doubts about the use of citations as a quality measure, since it is then quite possible for somebody or some group to chalk up high citation counts by simply writing barely publishable papers on fashionable subjects which will then be cited as perfunctory, \u2018also ran\u2019 references.\u201d (Moravcsik & Murugesan, 1975, p. 91). Indeed, based on an analysis of hundreds of references, Moravcsik and Murugesan (1975) found that a third of the references were redundant and 40% were perfunctory.", "startOffset": 131, "endOffset": 1142}, {"referenceID": 16, "context": "When an error in a citation is replicated many times, it seems likely that the citers have copied the citation without actually reading the cited paper. As an illustration, Dubin (2004) reported that a commonly cited paper by Gerard Salton does not actually exist. An incorrect citation was accidentally created by mixing the citations for two separate papers. This incorrect citation has since been cited by more than 300 papers. If the citers had tried to read the paper before citing it, they would have discovered that the paper does not exist. Like Moravcsik and Murugesan (1975), we are concerned about the sideeffects of counting insignificant references: \u201cA large fraction of the references are perfunctory. This raises serious doubts about the use of citations as a quality measure, since it is then quite possible for somebody or some group to chalk up high citation counts by simply writing barely publishable papers on fashionable subjects which will then be cited as perfunctory, \u2018also ran\u2019 references.\u201d (Moravcsik & Murugesan, 1975, p. 91). Indeed, based on an analysis of hundreds of references, Moravcsik and Murugesan (1975) found that a third of the references were redundant and 40% were perfunctory. In an independent study, Teufel et al. (2006) found that the majority (62.", "startOffset": 131, "endOffset": 1266}, {"referenceID": 16, "context": "When an error in a citation is replicated many times, it seems likely that the citers have copied the citation without actually reading the cited paper. As an illustration, Dubin (2004) reported that a commonly cited paper by Gerard Salton does not actually exist. An incorrect citation was accidentally created by mixing the citations for two separate papers. This incorrect citation has since been cited by more than 300 papers. If the citers had tried to read the paper before citing it, they would have discovered that the paper does not exist. Like Moravcsik and Murugesan (1975), we are concerned about the sideeffects of counting insignificant references: \u201cA large fraction of the references are perfunctory. This raises serious doubts about the use of citations as a quality measure, since it is then quite possible for somebody or some group to chalk up high citation counts by simply writing barely publishable papers on fashionable subjects which will then be cited as perfunctory, \u2018also ran\u2019 references.\u201d (Moravcsik & Murugesan, 1975, p. 91). Indeed, based on an analysis of hundreds of references, Moravcsik and Murugesan (1975) found that a third of the references were redundant and 40% were perfunctory. In an independent study, Teufel et al. (2006) found that the majority (62.7%) of the references could not be attributed a specific function whereas the fraction of references that provided an essential component for the citing paper (definition, tool, starting point) was 18.9%. The aim of our work is to determine the most effective features for identifying references that have high academic influence on the citing paper. An influential reference is one that inspired a new idea, method, experiment, or research problem that is a core contribution of the citing paper. We use the terms influence and influential to indicate the degree of academic influence of a single citation. In contrast, Pinski and Narin (1976) used the term citation influence to refer to the academic influence of a journal.", "startOffset": 131, "endOffset": 1939}, {"referenceID": 63, "context": "Citations have generally proven useful for summarization (Qazvinian & Radev, 2008, 2010; Abu-Jbara & Radev, 2011; Nanba & Okumura, 1999; Taheriyan, 2011; Kaplan, Iida, & Tokunaga, 2009).", "startOffset": 57, "endOffset": 185}, {"referenceID": 30, "context": "Improved measures of an author\u2019s impact: Indexes such as the h-index (Hirsch, 2005), g-index (Egghe, 2006), and hm-index (Schreiber, 2008) could be made less sensitive to noise by filtering citation counts with a model of influence.", "startOffset": 69, "endOffset": 83}, {"referenceID": 19, "context": "Improved measures of an author\u2019s impact: Indexes such as the h-index (Hirsch, 2005), g-index (Egghe, 2006), and hm-index (Schreiber, 2008) could be made less sensitive to noise by filtering citation counts with a model of influence.", "startOffset": 93, "endOffset": 106}, {"referenceID": 57, "context": "Improved measures of an author\u2019s impact: Indexes such as the h-index (Hirsch, 2005), g-index (Egghe, 2006), and hm-index (Schreiber, 2008) could be made less sensitive to noise by filtering citation counts with a model of influence.", "startOffset": 121, "endOffset": 138}, {"referenceID": 33, "context": "It is known that survey papers and methodology papers tend to be more highly cited than research contributions in general (Ioannidis, 2006).", "startOffset": 122, "endOffset": 139}, {"referenceID": 16, "context": "For those who are new to the field, this list would suggest further reading material. Citations have generally proven useful for summarization (Qazvinian & Radev, 2008, 2010; Abu-Jbara & Radev, 2011; Nanba & Okumura, 1999; Taheriyan, 2011; Kaplan, Iida, & Tokunaga, 2009). Improved measures of an author\u2019s impact: Indexes such as the h-index (Hirsch, 2005), g-index (Egghe, 2006), and hm-index (Schreiber, 2008) could be made less sensitive to noise by filtering citation counts with a model of influence. Beyond reducing sensitivity to noise, a model of influence could also put more weight on original contributions. It is known that survey papers and methodology papers tend to be more highly cited than research contributions in general (Ioannidis, 2006). Vanclay (2013) went as far as to recommend focusing on reviews: \u201cPerhaps the best single thing an aspiring author can do to attract citations is to participate in a rigorous review rather than writing a conventional research article.", "startOffset": 71, "endOffset": 775}, {"referenceID": 70, "context": "Filtering out less relevant citations might help paper recommender systems (Vellino, 2010; Liang, Li, & Qian, 2011).", "startOffset": 75, "endOffset": 115}, {"referenceID": 19, "context": "For instance, Garfield (1965) identified fifteen such reasons, including giving credit for related work, correcting a work, and criticizing previous work.", "startOffset": 14, "endOffset": 30}, {"referenceID": 16, "context": "For instance, Garfield (1965) identified fifteen such reasons, including giving credit for related work, correcting a work, and criticizing previous work. For articles in the field of high energy physics, Moravcsik and Murugesan (1975) distinguished four major classes of polar opposite pairs, conceptual\u2013 operational, organic\u2013perfunctory, evolutionary\u2013juxtapositional, and confirmative\u2013 negational.", "startOffset": 68, "endOffset": 236}, {"referenceID": 16, "context": "For instance, Garfield (1965) identified fifteen such reasons, including giving credit for related work, correcting a work, and criticizing previous work. For articles in the field of high energy physics, Moravcsik and Murugesan (1975) distinguished four major classes of polar opposite pairs, conceptual\u2013 operational, organic\u2013perfunctory, evolutionary\u2013juxtapositional, and confirmative\u2013 negational. They found that the fraction of negational references, i.e., citations indicating that the cited source is wrong, is not negligible (14%). Giles, Bollacker, and Lawrence (1998) presented one of the first automatic citation indexing systems (CiteSeer).", "startOffset": 68, "endOffset": 577}, {"referenceID": 58, "context": "Thus CiTO, the Citation Typing Ontology (Peroni & Shotton, 2012; Shotton, 2010), provides a rich machine-readable RDF metadata ontology for the characterization of bibliographic citations.", "startOffset": 40, "endOffset": 79}, {"referenceID": 1, "context": "(2006) and Agarwal et al. (2010) in at least one significant way: We asked the authors of the citing papers themselves to identify the influential references whereas they used independent annotations.", "startOffset": 11, "endOffset": 33}, {"referenceID": 1, "context": "(2006) and Agarwal et al. (2010) in at least one significant way: We asked the authors of the citing papers themselves to identify the influential references whereas they used independent annotations. We believe that it is difficult for an independent annotator to classify citations. This concern was raised by Gilbert (1977): \u201cSince the intentions of the author are not normally available to the content analyst, there seems to be no way of conclusively resolving problems of classifications (.", "startOffset": 11, "endOffset": 327}, {"referenceID": 1, "context": "(2006) and Agarwal et al. (2010) in at least one significant way: We asked the authors of the citing papers themselves to identify the influential references whereas they used independent annotations. We believe that it is difficult for an independent annotator to classify citations. This concern was raised by Gilbert (1977): \u201cSince the intentions of the author are not normally available to the content analyst, there seems to be no way of conclusively resolving problems of classifications (. . . ) The difficulties are more compounded when the analyst has only a superficial knowledge of the contexts in which the papers he examines were written and read.\u201d (Gilbert, 1977, p. 120). Nevertheless, Teufel et al. (2006) report moderately good inter-annotator agreement.", "startOffset": 11, "endOffset": 722}, {"referenceID": 16, "context": "Several authors have proposed weighting citations based on factors such as the prestige of the citing journal (Ding, 2011; Yan & Ding, 2010; Ding & Cronin, 2011; Gonzalez-Pereira, Guerrero-Bote, & Moya-Aneg\u00f3n, 2010).", "startOffset": 110, "endOffset": 215}, {"referenceID": 4, "context": "Balaban (2012) proposed giving more weights to citations from more prestigious authors.", "startOffset": 0, "endOffset": 15}, {"referenceID": 4, "context": "Balaban (2012) proposed giving more weights to citations from more prestigious authors. He also argues that the citation of a paper published in a less prestigious venue should be considered more significant: \u201c. . . if a paper is cited despite its handicap of having appeared in a low-IF [Impact Factor] journal, then this means that this paper has a high intrinsic value. Therefore a citation\u2019s value should be inversely correlated with the IF of the journal in which the cited paper was published.\u201d (Balaban, 2012, p. 244). Hou, Li, and Niu (2011) propose to use frequency to assess the importance of a citation.", "startOffset": 0, "endOffset": 550}, {"referenceID": 4, "context": "Balaban (2012) proposed giving more weights to citations from more prestigious authors. He also argues that the citation of a paper published in a less prestigious venue should be considered more significant: \u201c. . . if a paper is cited despite its handicap of having appeared in a low-IF [Impact Factor] journal, then this means that this paper has a high intrinsic value. Therefore a citation\u2019s value should be inversely correlated with the IF of the journal in which the cited paper was published.\u201d (Balaban, 2012, p. 244). Hou, Li, and Niu (2011) propose to use frequency to assess the importance of a citation. That is, if a reference was cited 10 times in the citing paper it gets a weight of 10. They show that by weighting citations by the in-paper frequency, review articles lose part of their advantage over original contributions when counting citations. They state that greater credit is reverted to the discoverers. They also show that closely related references are cited more often in the body of a citing paper than less related references (on average, 3.35 times versus 1.88 times). They define closely related references as papers having at least ten references in common with the given paper. Di Marco, Kroon, and Mercer (2006) studied hedging as a means to classify citations.", "startOffset": 0, "endOffset": 1246}, {"referenceID": 4, "context": "Balaban (2012) proposed giving more weights to citations from more prestigious authors. He also argues that the citation of a paper published in a less prestigious venue should be considered more significant: \u201c. . . if a paper is cited despite its handicap of having appeared in a low-IF [Impact Factor] journal, then this means that this paper has a high intrinsic value. Therefore a citation\u2019s value should be inversely correlated with the IF of the journal in which the cited paper was published.\u201d (Balaban, 2012, p. 244). Hou, Li, and Niu (2011) propose to use frequency to assess the importance of a citation. That is, if a reference was cited 10 times in the citing paper it gets a weight of 10. They show that by weighting citations by the in-paper frequency, review articles lose part of their advantage over original contributions when counting citations. They state that greater credit is reverted to the discoverers. They also show that closely related references are cited more often in the body of a citing paper than less related references (on average, 3.35 times versus 1.88 times). They define closely related references as papers having at least ten references in common with the given paper. Di Marco, Kroon, and Mercer (2006) studied hedging as a means to classify citations. Le, Ho, and Nakamori (2006) used finite-state machines for classification of citations.", "startOffset": 0, "endOffset": 1324}, {"referenceID": 50, "context": "Porter\u2019s (1980) stemming algorithm was used to stem the text (remove suffixes) and we kept stop words (function words), since removing them did not improve the performance of our models during their development.", "startOffset": 0, "endOffset": 16}, {"referenceID": 50, "context": "Porter\u2019s (1980) stemming algorithm was used to stem the text (remove suffixes) and we kept stop words (function words), since removing them did not improve the performance of our models during their development. Readers can refer to Turney and Pantel (2010) for further discussions of vector space models of semantic similarity.", "startOffset": 0, "endOffset": 258}, {"referenceID": 50, "context": "5), we use cosine similarity, after the text was preprocessed with Porter\u2019s (1980) stemmer.", "startOffset": 67, "endOffset": 83}, {"referenceID": 50, "context": "5), we use cosine similarity, after the text was preprocessed with Porter\u2019s (1980) stemmer. During development, we experimented with different window sizes, ranging from two words around a citation to several sentences around it. We found that using the entire sentence in which the citation appears gave the best results. In contrast, Ritchie, Robertson, and Teufel (2008) found that contexts larger than one sentence were better for indexing purposes: further work might be needed to identify the optimal window.", "startOffset": 67, "endOffset": 374}, {"referenceID": 46, "context": "Osgood et al. (1957) discov-", "startOffset": 0, "endOffset": 21}, {"referenceID": 64, "context": "Using the algorithm of Turney and Littman (2003), we automatically extended the labels to cover 114,271 words.", "startOffset": 23, "endOffset": 49}, {"referenceID": 49, "context": "Porter\u2019s (1980) stemmer (mentioned in the preceding section) was only applied as a preprocessing step when generating feature vectors; the stemmer was not applied during the corpus annotation step described here.", "startOffset": 0, "endOffset": 16}, {"referenceID": 16, "context": "We used several different types of lexicons to capture different aspects of semantics in the citation contexts, including sentiment, emotion, and Osgood et al.\u2019s (1957) semantic differential categories.", "startOffset": 117, "endOffset": 169}, {"referenceID": 37, "context": "Note that the F-measure we used here is the macro-averaged F-measure (Lewis, 1991).", "startOffset": 69, "endOffset": 82}, {"referenceID": 39, "context": "To further assess the importance of the four features in model (6), we have also applied logistic regression to our data (Long, 1997).", "startOffset": 121, "endOffset": 133}, {"referenceID": 16, "context": "Our regular expressions are good at both precision and recall according to our manual examination, but they still make a few errors. For example, the regular expressions have trouble with citations that span two lines of text and with multiple papers written by the same author in the same year. Another problem is automatically distinguishing the main body text from the reference section of a paper. The regular expressions may wrongly increment the in-paper count by matching citations in the reference section. Numerical citations (e.g., \u201c[1]\u201d or \u201c[1,2,3]\u201d) are more difficult to process than textual citations (e.g., \u201cSmith et al. (1998)\u201d).", "startOffset": 67, "endOffset": 643}, {"referenceID": 29, "context": "Indeed, Hirsch (2007) found that the h-index was superior at predicting the future performance of a researcher than the number of citations, the number of papers, and mean citations per paper (Lehmann, Jackson, & Lautrup, 2006).", "startOffset": 8, "endOffset": 22}, {"referenceID": 32, "context": "However, many other features are possible if we access the full text of both the citer and cited paper: Hou et al. (2011) measure similarity by the overlap in the reference section.", "startOffset": 104, "endOffset": 122}, {"referenceID": 32, "context": "However, many other features are possible if we access the full text of both the citer and cited paper: Hou et al. (2011) measure similarity by the overlap in the reference section. We could also add other features such as the prestige of the cited venue or the prestige of the cited authors (Zitt & Small, 2008). We also did not take into account the relationships between authors. Maybe authors who are similar or related are more likely to influence each other. In related work, Ajiferuke, Lu, and Wolfram (2010) proposed to measure the number of citers (authors who cite) rather than the number of citations.", "startOffset": 104, "endOffset": 516}, {"referenceID": 32, "context": "However, many other features are possible if we access the full text of both the citer and cited paper: Hou et al. (2011) measure similarity by the overlap in the reference section. We could also add other features such as the prestige of the cited venue or the prestige of the cited authors (Zitt & Small, 2008). We also did not take into account the relationships between authors. Maybe authors who are similar or related are more likely to influence each other. In related work, Ajiferuke, Lu, and Wolfram (2010) proposed to measure the number of citers (authors who cite) rather than the number of citations. Moreover, identifying the genuinely significant citations might be viewed as an adversarial problem. Indeed, some authors and editors attempt to game (i.e., manipulate or exploit) citations counts. In a survey, Wilhite and Fong (2012) found that 20% of all authors were coerced into citing some references by an editor, after their manuscript had undergone normal peer review.", "startOffset": 104, "endOffset": 848}, {"referenceID": 41, "context": "Mazloumian (Mazloumian, 2012) found that a useful predictor of future performance was the annual number of citations at the time of citation.", "startOffset": 11, "endOffset": 29}, {"referenceID": 21, "context": "Nevertheless, a weighted citation measure based on the number of occurrences of a citation could significantly alter other evaluation metrics that depend on simple citation counts, such as Impact Factor (Garfield, 2006), Eigenfactor, and Article Influence (Bergstrom, 2007).", "startOffset": 203, "endOffset": 219}, {"referenceID": 5, "context": "Nevertheless, a weighted citation measure based on the number of occurrences of a citation could significantly alter other evaluation metrics that depend on simple citation counts, such as Impact Factor (Garfield, 2006), Eigenfactor, and Article Influence (Bergstrom, 2007).", "startOffset": 256, "endOffset": 273}, {"referenceID": 5, "context": "Nevertheless, a weighted citation measure based on the number of occurrences of a citation could significantly alter other evaluation metrics that depend on simple citation counts, such as Impact Factor (Garfield, 2006), Eigenfactor, and Article Influence (Bergstrom, 2007). Even though such a refinement would not address the statistical charges leveled against citation-based evaluation metrics (Adler, Ewing, & Taylor, 2009), it would at least address to some degree the need to distinguish between citations that acknowledge an intellectual debt and de-rigeur citations. We conjecture that some types of research papers that tend to be highly cited, such as review or methodological articles, are less likely to be perceived as influential. Hou et al. (2011) found that weighting citations by the in-paper frequency reduced the importance of reviews.", "startOffset": 257, "endOffset": 763}, {"referenceID": 31, "context": "This confirms an earlier result by Hou et al. (2011) who stated, \u201cCitation frequency of individual articles in other papers more fairly measures their scientific contribution than mere presence in reference lists.", "startOffset": 35, "endOffset": 53}], "year": 2015, "abstractText": "The importance of a research article is routinely measured by counting how many times it has been cited. However, treating all citations with equal weight ignores the wide variety of functions that citations perform. We want to automatically identify the subset of references in a bibliography that have a central academic influence on the citing paper. For this purpose, we examine the effectiveness of a variety of features for determining the academic influence of a citation. By asking authors to identify the key references in their own work, we created a dataset in which citations were labeled according to their academic influence. Using automatic feature selection with supervised machine learning, we found a model for predicting academic influence that achieves good performance on this dataset using only four features. The best features, among those we evaluated, were features based on the number of times a reference is mentioned in the body of a citing paper. The performance of these features inspired us to design an influence-primed h-index (the hip-index). Unlike the conventional h-index, it weights citations by how many times a reference is mentioned. According to our experiments, the hip-index is a better indicator of researcher performance than the conventional h-index.", "creator": "TeX"}}}