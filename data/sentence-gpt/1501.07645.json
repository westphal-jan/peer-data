{"id": "1501.07645", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2015", "title": "Hyper-parameter optimization of Deep Convolutional Networks for object recognition", "abstract": "Recently sequential model based optimization (SMBO) has emerged as a promising hyper-parameter optimization strategy in machine learning. In this work, we investigate SMBO to identify architecture hyper-parameters of deep convolution networks (DCNs) object recognition. We propose a simple SMBO strategy that starts from a set of random initial DCN architectures to generate new architectures, which on training perform well on a given dataset. Using the proposed SMBO strategy we are able to identify a number of DCN architectures that produce results that are comparable to state-of-the-art results on object recognition benchmarks. Specifically, we report three DCN networks generated by our proposed algorithm that produce &lt;9% test error rate, with the best network exhibiting a test error rate of 7.81% on the CIFAR-10 benchmark. Our results compare favorably to the current state-of-the-art of 7.97 % test error rate for CIFAR-10 that are obtained by hand tuning.\n\n\n\n\nIn this work we identified the first DCN-accelerated DCN model. A DCN-accelerated DCN model was performed using the following algorithm: DCN-accelerated DCN model (A) is achieved by training a DCN-accelerated DCN model using a DCN-accelerated DCN model with the following algorithm: DCN-accelerated DCN model (A) is obtained using the following algorithm: DCN-accelerated DCN model (A) is achieved by training a DCN-accelerated DCN model with the following algorithm: DCN-accelerated DCN model (A) is achieved by training a DCN-accelerated DCN model with the following algorithm: DCN-accelerated DCN model (A) is achieved by training a DCN-accelerated DCN model with the following algorithm: DCN-accelerated DCN model (A) is obtained using the following algorithm: DCN-accelerated DCN model (A) is achieved by training a DCN-accelerated DCN model with the following algorithm: DCN-accelerated DCN model (A) is obtained using the following algorithm: DCN-accelerated DCN model (A) is achieved by training a DCN-accelerated DCN model with the following algorithm: DCN-accelerated DCN model (A) is achieved by training", "histories": [["v1", "Fri, 30 Jan 2015 02:08:51 GMT  (126kb,D)", "https://arxiv.org/abs/1501.07645v1", "4 pages, 1 figure, 3 tables, Submitted to ICIP 2015"], ["v2", "Sun, 17 May 2015 03:32:22 GMT  (127kb,D)", "http://arxiv.org/abs/1501.07645v2", "4 pages, 1 figure, 3 tables, Submitted to ICIP 2015"]], "COMMENTS": "4 pages, 1 figure, 3 tables, Submitted to ICIP 2015", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["sachin s talathi"], "accepted": false, "id": "1501.07645"}, "pdf": {"name": "1501.07645.pdf", "metadata": {"source": "CRF", "title": "HYPER-PARAMETER OPTIMIZATION OF DEEP CONVOLUTIONAL NETWORKS FOR OBJECT RECOGNITION", "authors": ["Sachin S. Talathi", "Diego CA"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014 hyper-parameter optimization, deep convolution networks, sequential model based optimization"}, {"heading": "1. INTRODUCTION", "text": "The primary task for a supervised machine learning algorithm is to use training dataset {xtr, ytr} to find a function f : x \u2192 y, that also generalize well across the test (or the hold out) dataset {xh, yh}. Very often f is obtained through the optimization of a training criterion, C, with respect to a sets of parameters, \u03b8. The learning algorithm used to optimize C usually contains its own set of free parameters \u03bbl, referred to as the learning algorithm hyper-parameters. These hyper-parameters are often estimated using grid search cross validation. In addition to the learning algorithm hyper-parameters, \u03bbl, neural network models such as deep convolution networks (DCNs) also comprise of hyper-parameters \u03bba /\u2208 \u03bbl, that define the architectural configuration of the network. Grid search techniques are prohibitively expensive to tune \u03bba, given the fact that there are a few tens of these architectural hyper-parameters. As a result, many of the stateof-the-art DCNs are manually designed, making the task of tuning these hyper-parameters more of an art than a science [1].\nIn recent years, there has been a concerted effort in the machine learning community to develop better algorithms to solve the hyper-parameter optimization problem [1, 2, 3, 4, 5]. Many of these works have successfully applied direct search methods for non-linear optimization such as the sequential model based optimization (SMBO) to generate better results on various supervised machine learning tasks than were previously reported. Motivated by these works, in this paper we attempt to address the question: Can SMBO be used to determine superior architectural configurations for DCNs? The paper is organized as follows: In the Methods Section 2, we will briefly formulate the problem of hyper-parameter optimization for DCNs. We then present the general strategy of sequential model based optimization (SMBO) [4] and summarize our approach to SMBO for designing DCN architectures. The results of image classification training and evaluation on the benchmark CIFAR-10 dataset are then presented in the Results Section 3, which is followed by the Conclusion."}, {"heading": "2. METHODS", "text": ""}, {"heading": "2.1. Formulation of the problem", "text": "Let M\u03bb(x,w) represent the DCN model that operates on input data x \u2208 RD and generates an estimate y\u0302 for the output data y \u2208 ZC2 . The DCN model, M , is parameterized by two set of parameters, the first beingw, which are obtained through the optimization of a training criterion, C, using a gradient descent type learning algorithm such as the back-propagation algorithm and the second being \u03bb = {\u03bba, \u03bbl}, which represent the set of the so called hyper-parameters. The hyper-parameters \u03bba define the DCN architecture and the hyper-parameters \u03bbl, are associated with the learning algorithm used to optimize C. The objective for DCN hyper-parameter optimization is\nar X\niv :1\n50 1.\n07 64\n5v 2\n[ cs\n.C V\n] 1\n7 M\nay 2\n01 5\nto solve the joint optimization problem as stated below:\n{w, \u03bb} = argmin \u03bb [\u03a8(y\u0302h, yh)] where,\ny\u0302h = M\u03bb(argmin w (C(xtr, ytr, \u03b8)) , xh) (1)\nwhere {(xtr, ytr), (xh, yh)}\u2208 (x, y), are the input and output training and hold-out (or the test) data set respectively, \u03a8 = \u2211 {yh} Iyh 6=y\u0302h ."}, {"heading": "2.2. Sequential model based optimization", "text": "SMBO is a direct search method for non-linear optimization, in which one begins by selecting a meta-model of the function for which an extrema is sought. One then applies an active learning strategy to select a query point that provides the most potential to approach the extrema. More specifically, let us assume that we have a database D1:t = {\u03bb1:t, e1:t} of t DCN models, where \u03bbi|ti=1 represents the set of hyper-parameters and ei|ti=1 is the validation error on the hold-out dataset generated by each of the t DCN models. The basic idea underlying SMBO is to replace the original optimization problem of finding extrema of a given function, such as \u03a8(\u03bb), which is time consuming and computationally expensive, with an equivalent problem of optimization of expected value of an utility function, u(e) [4]. As we will see below, optimizing over the expected value of the utility function is computationally much cheaper and faster than solving the original problem. Under SMBO, one usually begins by assigning a prior distribution p(e) on e. One then uses the database D1:t to obtain an estimate for the likelihood function p(\u03bb1:t|e). The prior and the likelihood function are used to obtain an estimate for the posterior distribution p(e|\u03bb1:t) \u221d p(\u03bb1:t|e)p(e). The objective then is to choose \u03bbt+1, which maximizes u(e) under p(e|\u03bb1:t), i.e., \u03bbt+1 = argmax\n\u03bb [E(u(e))], where E(u(e)) is given as:\nE(u(e)) = \u222b u(e)p(e|\u03bb1:t)de\n= \u222b u(e)\np(\u03bb1:t|e)p(e) p(\u03bb1:t) de (2)\nA common choice for the utility function u(e), is the Expected Improvement function [4], u(e) = max ((e\u2217 \u2212 e), 0), in which case, Eq. 4 becomes\nE(u(e)) = \u222b e\u2217 0 (e\u2217 \u2212 e)p(e|\u03bb1:t)de (3)\nThen under SMBO we have,\n\u03bbt+1 = argmax \u03bb\n[ e\u2217\np(\u03bb1:t) \u222b e\u2217 0 p(\u03bb1:t|e)p(e)de ] (4)\nHyperOpt (D1:t, p,N ): While t \u2264 N do: 1. Estimate e\u2217 s.t. p(e1:t < e\u2217) = 0.5 2. Use e\u2217 and \u03bb1:t to estimate l(\u03bb) and g(\u03bb) 3. Evaluate \u03bbt+1 according to Eq. 5 and Eq. 6 4. Train the new DCN model with \u03bbt+1 to estimate et+1 5. t\u2190 t+ 1\nTable 1. SMBO algorithm for estimating architecture hyper-parameters for DCN.\nIf p(e < e\u2217) = \u03b3 (a constant), i.e., choose e\u2217 to be some quantile of observed e values, and we define two density functions; l(\u03bb) = p(\u03bb1:t|e) when e \u2264 e\u2217 and g(\u03bb) = p(\u03bb1:t|e) when e > e\u2217 as proposed in [1], then,\n\u03bbt+1 = argmax \u03bb\n[ e\u2217\u03b3l(\u03bb)\n\u03b3l(\u03bb) + (1\u2212 \u03b3)g(\u03bb)\n] (5)\nBergstra et.al., [1], proposed an adaptive Parson estimator algorithm to evaluate Eq. 5 so as to maximize the ratio l(\u03bb)/g(\u03bb). In this work, we propose a simple strategy to evaluate Eq. 5 as follows: let l(\u03bb) + g(\u03bb) = U(\u03bb) = k (a constant). In other words, \u03bb is drawn from a uniform distribution. Then, if \u03b3 = 0.5, we have from Eq. 5\n\u03bbt+1 = 2e\u2217\nk argmax \u03bb [l(\u03bb)] (6)\nOur proposed simplification in Eq. 6, does not require an estimate for both l(\u03bb) and g(\u03bb). We can simply pick \u03bb \u2208 U(\u03bb) and evaluate according to empirical distribution of l(\u03bb) generated from D1:t to evaluate the next potential \u03bbt+1. Since the proposed algorithm chooses \u03bb \u2208 U(\u03bb) at every step, a much larger space of potential \u03bb\u2019s are explored, which may in turn slow down the convergence rate to an optimal \u03bb\u2217. In order to counter this,we adopt a hybrid strategy by combining Eq. 5 and Eq. 6. With probability p, at every iteration, we choose the potential \u03bbt+1 according to Eq. 5 and with probability 1\u2212 p, we choose the potential \u03bbt+1 according to Eq. 6. In Table 1, we summarize the steps involved in our proposed algorithm."}, {"heading": "3. RESULTS", "text": "We evaluate our proposed SMBO algorithm on the CIFAR-10 benchmark, which consists of 60,000 32x32 color images. The collection is split into 50,000 training and 10,000 testing images. All the DCNs generated\nby our proposed algorithm were trained using cudaconvnet21. We used a 3 step cooling procedure; starting with learning rate l = 0.01, the momentum m = 0.9, the weight decay parameter wc = 0.0005 for the first 120 epochs followed by another 20 epochs by reducing learning rate by a factor of 10 (keeping other parameters the same) and then training for 10 more epochs by further reducing learning rate by a factor of 10.\nSince the primary focus for us in this work is to determine whether SMBO can be used to identify suitable DCN architectures, we fixed the DCN hyper-parameters associated with the back-propagation learning algorithm as described above. The set of DCN architecture hyperparameters that we consider for optimization are listed in Table 2.\nIt has been reported in the literature [7] that very deep networks are difficult to train primarily suffering from vanishing gradient problem at larger depths. In order to alleviate this problem, for our implementation of SMBO for DCN architectural hyper-parameters, all the DCNs are generated to comprise a local logistic regression (LR) cost function layer at the output of one or more of the convolution block.\nFor the results presented here, we consider t = 32 as the size of our initial database based on our analysis of random search hyper-parameter optimization [5] and we set p = 0.9.\n1https://code.google.com/p/cuda-convnet2/\nIn Figure 1a, we plot the mean (std. error, shown in yellow) test error (evaluated in multi-view test mode, [6]) E(i) = \u3008ei\u221210:i\u3009 and in Figure 1b, we plot the minimum test error M(i) = min(e0:i) as function of the iteration number i, respectively. We see that the average test error gradually decrease towards an optimal solution, the best minimum found also decreases with increasing iterations. Furthermore, our proposed SMBO procedure generated a large number of \u201cgood\u201d DCN architectures that produce test-error of < 11 % even with only 150 training epochs (not-reported). In comparison, the best hand-tuned DCN architecture, produced by [6] exhibits 11% test error in multi-view mode and requires a longer training time on the order of 500 epochs.\nSince the state-of-the-art performance numbers for the CIFAR-10 benchmark dataset are usually reported in multi-view mode (with data-augmentation [6]), we report multi-view test error of 7.81% for the best DCN generated by our proposed hyper-parameter optimization strategy, which compares favorably to the current stateof-the-art result on CIFAR-10 of 7.97% [8]. In Table 3, we summarize the top 3 DCN architectures found by our proposed SMBO procedure that produced multi-view test error< 9% on the CIFAR-10 benchmark. In ??, we summarize the number of parameters in each of these 3 DCN models.\nAt the time of writing of this manuscript for camera ready version, we came across a recent paper [9], that reported multi-view test error of 7.25% on CIFAR10 benchmark, using a hand designed DCN network, that is comprised of only convolution layers and has 1.3 M parameters. Yet another paper [10], reported the\nutility of using parametric-relu neuron as opposed to the relu neurons. While none of the optimized DCN networks that we report in Table 3 generate better performance numbers than the latest state-of-the-art numbers reported in [9], we wanted to determine whether the use of parametric-relu neuron can boost the performance of the optimized DCN networks that we have identified through the hyper-parameter optimization approach. Accordingly, we retrained the smallest of the three DCN networks from Table 3 using a version of parametric-rectified non-linear neurons of type y = ax(x \u2264 0) + \u221a x(x > 0), where a is a learnable parameter, fixed per neuron layer in the DCN network. We were able to obtain multi-view test error score of 6.9 %, which to the best of our knowledge, represents the state-of-the-art score for CIFAR-10 benchmark. In Table 4, we summarize all the known best in class numbers for CIFAR-10 benchmark."}, {"heading": "4. CONCLUSION", "text": "In this paper, we have proposed a simple SMBO algorithm and a recipe for hyper-parameter optimization of DCN architectures. We have demonstrated that SMBO can be used to generate a large number of \u201cgood\u201d DCN architectures, which may then form a backbone for further investigations. Our results suggest that indeed SMBO can be used to identify superior DCNs. In summary, our work in this paper in addition to those from earlier works [1, 3] broaden the scope of the models that can be realistically investigated, without the need for the researchers to be restricted to manual evaluation of a few architectural parameters at any given time."}, {"heading": "5. REFERENCES", "text": "[1] J. Bergstra, R. Bardenet, Y. Bengio, and B. Kegl, \u201cAlgorithms for hyper-parameter optimization,\u201d in NIPS, 2011, vol. 24.\n[2] Distributed asynchronous hyper parameter optimization. https://github.com/hyperopt/hyperopt.\n[3] J. Snoek, H. Larochelle, and R.P. Adam, \u201cPractical bayesian optimization for machine learning,\u201d in NIPS, 2012, vol. 25.\n[4] E. Brochu, V.M. Cora, and N. Freitas, \u201cA tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning,\u201d Arxiv, vol. arXiv:1012.2599, 2010.\n[5] J. Bergstra and Y. Bengio, \u201cRandom search for hyper-parameter optimization,\u201d Journal of Machine Learning Research, vol. 13, pp. 281\u2013305, 2012.\n[6] A. Krizhevsky, \u201cLearning multiple layers of features from tiny images,\u201d Tech. Rep., Dept. Computer Science, University of Toronto, 2009.\n[7] K. Simonyan and A. Zisserman, \u201cVery deep convolutional networks for large scale image recognition,\u201d Arxiv, vol. arXiv:1409.1556, 2014.\n[8] C. Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu, \u201cDeeply supervised nets,\u201d in NIPS, 2014.\n[9] J.T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller, \u201cStriving for simplicity, all convolution net,\u201d in ICLR, 2015.\n[10] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDelving deep into rectifiers, surpassing human-level performance on imagenet classification,\u201d in Arxiv:1502.01852, 2015.\n[11] I.J. Goodfellow, F. Warde, D. Mirza, A. Courville, and Y. Bengio, \u201cMaxout networks,\u201d in ICML, 2013.\n[12] L. Wan, M. Zeiler, S. Zhang, Y. LeCun, and R. Fergus, \u201cRegularization of neural networks using dropconnect,\u201d in ICML, 2013.\n[13] M. F. Stollenga, J. Masci, F. Gomez, , and J Schmidhuber, \u201cDeep networks with internal selective attention through feedback connections.,\u201d in NIPS, 2014.\n[14] M. Lin, Q. Chen, , and S. Yan, \u201cNetwork in network,\u201d in ICLR, 2014.\n[15] C.Y. Lee, S. Xi, P. Gallagher, Z. Zhang, and Z. Tu, \u201cDeeply supervised nets,\u201d in NIPS, 2014."}], "references": [{"title": "Algorithms for hyper-parameter optimization", "author": ["J. Bergstra", "R. Bardenet", "Y. Bengio", "B. Kegl"], "venue": "NIPS, 2011, vol. 24.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Practical bayesian optimization for machine learning", "author": ["J. Snoek", "H. Larochelle", "R.P. Adam"], "venue": "NIPS, 2012, vol. 25.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["E. Brochu", "V.M. Cora", "N. Freitas"], "venue": "Arxiv, vol. arXiv:1012.2599, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Random search for hyper-parameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "Journal of Machine Learning Research, vol. 13, pp. 281\u2013305, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Tech. Rep., Dept. Computer Science, University of Toronto, 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Very deep convolutional networks for large scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "Arxiv, vol. arXiv:1409.1556, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Deeply supervised nets", "author": ["C.Y. Lee", "S. Xie", "P. Gallagher", "Z. Zhang", "Z. Tu"], "venue": "NIPS, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Striving for simplicity, all convolution net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "ICLR, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Delving deep into rectifiers, surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Arxiv:1502.01852, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1852}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "F. Warde", "D. Mirza", "A. Courville", "Y. Bengio"], "venue": "ICML, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus"], "venue": "ICML, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep networks with internal selective attention through feedback connections", "author": ["M.F. Stollenga", "J. Masci", "F. Gomez", "J Schmidhuber"], "venue": "NIPS, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "ICLR, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deeply supervised nets", "author": ["C.Y. Lee", "S. Xi", "P. Gallagher", "Z. Zhang", "Z. Tu"], "venue": "NIPS, 2014. 5", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "As a result, many of the stateof-the-art DCNs are manually designed, making the task of tuning these hyper-parameters more of an art than a science [1].", "startOffset": 148, "endOffset": 151}, {"referenceID": 0, "context": "In recent years, there has been a concerted effort in the machine learning community to develop better algorithms to solve the hyper-parameter optimization problem [1, 2, 3, 4, 5].", "startOffset": 164, "endOffset": 179}, {"referenceID": 1, "context": "In recent years, there has been a concerted effort in the machine learning community to develop better algorithms to solve the hyper-parameter optimization problem [1, 2, 3, 4, 5].", "startOffset": 164, "endOffset": 179}, {"referenceID": 2, "context": "In recent years, there has been a concerted effort in the machine learning community to develop better algorithms to solve the hyper-parameter optimization problem [1, 2, 3, 4, 5].", "startOffset": 164, "endOffset": 179}, {"referenceID": 3, "context": "In recent years, there has been a concerted effort in the machine learning community to develop better algorithms to solve the hyper-parameter optimization problem [1, 2, 3, 4, 5].", "startOffset": 164, "endOffset": 179}, {"referenceID": 2, "context": "We then present the general strategy of sequential model based optimization (SMBO) [4] and summarize our approach to SMBO for designing DCN architectures.", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "The basic idea underlying SMBO is to replace the original optimization problem of finding extrema of a given function, such as \u03a8(\u03bb), which is time consuming and computationally expensive, with an equivalent problem of optimization of expected value of an utility function, u(e) [4].", "startOffset": 278, "endOffset": 281}, {"referenceID": 2, "context": "A common choice for the utility function u(e), is the Expected Improvement function [4], u(e) = max ((e\u2217 \u2212 e), 0), in which case, Eq.", "startOffset": 84, "endOffset": 87}, {"referenceID": 0, "context": ", choose e\u2217 to be some quantile of observed e values, and we define two density functions; l(\u03bb) = p(\u03bb1:t|e) when e \u2264 e\u2217 and g(\u03bb) = p(\u03bb1:t|e) when e > e\u2217 as proposed in [1], then,", "startOffset": 168, "endOffset": 171}, {"referenceID": 0, "context": ", [1], proposed an adaptive Parson estimator algorithm to evaluate Eq.", "startOffset": 2, "endOffset": 5}, {"referenceID": 4, "context": "For normalization layer; we only consider local response normalization across filter maps [6], with a scaling factor of 0.", "startOffset": 90, "endOffset": 93}, {"referenceID": 5, "context": "It has been reported in the literature [7] that very deep networks are difficult to train primarily suffering from vanishing gradient problem at larger depths.", "startOffset": 39, "endOffset": 42}, {"referenceID": 3, "context": "For the results presented here, we consider t = 32 as the size of our initial database based on our analysis of random search hyper-parameter optimization [5] and we set p = 0.", "startOffset": 155, "endOffset": 158}, {"referenceID": 4, "context": "error, shown in yellow) test error (evaluated in multi-view test mode, [6]) E(i) = \u3008ei\u221210:i\u3009 and in Figure 1b, we plot the minimum test error M(i) = min(e0:i) as function of the iteration number i, respectively.", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "In comparison, the best hand-tuned DCN architecture, produced by [6] exhibits 11% test error in multi-view mode and requires a longer training time on the order of 500 epochs.", "startOffset": 65, "endOffset": 68}, {"referenceID": 4, "context": "Since the state-of-the-art performance numbers for the CIFAR-10 benchmark dataset are usually reported in multi-view mode (with data-augmentation [6]), we report multi-view test error of 7.", "startOffset": 146, "endOffset": 149}, {"referenceID": 6, "context": "97% [8].", "startOffset": 4, "endOffset": 7}, {"referenceID": 7, "context": "At the time of writing of this manuscript for camera ready version, we came across a recent paper [9], that reported multi-view test error of 7.", "startOffset": 98, "endOffset": 101}, {"referenceID": 8, "context": "Yet another paper [10], reported the utility of using parametric-relu neuron as opposed to the relu neurons.", "startOffset": 18, "endOffset": 22}, {"referenceID": 7, "context": "While none of the optimized DCN networks that we report in Table 3 generate better performance numbers than the latest state-of-the-art numbers reported in [9], we wanted to determine whether the use of parametric-relu neuron can boost the performance of the optimized DCN networks that we have identified through the hyper-parameter optimization approach.", "startOffset": 156, "endOffset": 159}, {"referenceID": 9, "context": "Maxout [11] maxout 9.", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": "38 >6 M Dropconnect [12] relu 9.", "startOffset": 20, "endOffset": 24}, {"referenceID": 11, "context": "32 dasNet [13] maxout 9.", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "22 >6 M Network in Network [14] relu 8.", "startOffset": 27, "endOffset": 31}, {"referenceID": 13, "context": "81 \u22481 M Deeply Supervised [15] relu 7.", "startOffset": 26, "endOffset": 30}, {"referenceID": 7, "context": "97 \u22481 M All-CNN [9] relu 7.", "startOffset": 16, "endOffset": 19}, {"referenceID": 0, "context": "In summary, our work in this paper in addition to those from earlier works [1, 3] broaden the scope of the models that can be realistically investigated, without the need for the researchers to be restricted to manual evaluation of a few architectural parameters at any given time.", "startOffset": 75, "endOffset": 81}, {"referenceID": 1, "context": "In summary, our work in this paper in addition to those from earlier works [1, 3] broaden the scope of the models that can be realistically investigated, without the need for the researchers to be restricted to manual evaluation of a few architectural parameters at any given time.", "startOffset": 75, "endOffset": 81}], "year": 2015, "abstractText": "Recently sequential model based optimization (SMBO) has emerged as a promising hyper-parameter optimization strategy in machine learning. In this work, we investigate SMBO to identify architecture hyper-parameters of deep convolution networks (DCNs) object recognition. We propose a simple SMBO strategy that starts from a set of random initial DCN architectures to generate new architectures, which on training perform well on a given dataset. Using the proposed SMBO strategy we are able to identify a number of DCN architectures that produce results that are comparable to state-of-the-art results on object recognition benchmarks.", "creator": "LaTeX with hyperref package"}}}