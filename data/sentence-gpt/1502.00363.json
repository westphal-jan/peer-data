{"id": "1502.00363", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Feb-2015", "title": "Iterated Support Vector Machines for Distance Metric Learning", "abstract": "Distance metric learning aims to learn from the given training data a valid distance metric, with which the similarity between data samples can be more effectively evaluated for classification. Metric learning is often formulated as a convex or nonconvex optimization problem, while many existing metric learning algorithms become inefficient for large scale problems. In this paper, we formulate metric learning as a kernel classification problem, and solve it by iterated training of support vector machines (SVM) to generate some useful information about the underlying kernel data. We summarize the approach using the following following steps.\n\n\n\nThe training data sets are not distributed by a trained machine: the training data are only processed by trained machine. If training data is not a complete training dataset, it is not included.\nThe training data is only processed by trained machine. If training data is not a complete training dataset, it is not included. If training data is not a complete training dataset, it is not included.\nTraining data is only processed by trained machine. If training data is not a complete training dataset, it is not included. The training data is only processed by trained machine. If training data is not a complete training dataset, it is not included.\nTraining data is only processed by trained machine. If training data is not a complete training dataset, it is not included. If training data is not a complete training dataset, it is not included. If training data is not a complete training dataset, it is not included.\nTraining data is only processed by trained machine. If training data is not a complete training dataset, it is not included. If training data is not a complete training dataset, it is not included. If training data is not a complete training dataset, it is not included.\nTraining data is only processed by trained machine. If training data is not a complete training dataset, it is not included. If training data is not a complete training dataset, it is not included. If training data is not a complete training dataset, it is not included. If training data is not a complete training dataset, it is not included.\nTraining data is only processed by trained machine. If training data is not a complete training dataset, it is not included. If training data is not a complete training dataset, it is not included. If training data is not a complete training dataset, it is not included. If training data is not a complete training dataset, it is not included. If training data is not a complete training dataset, it is not included.\nTraining data", "histories": [["v1", "Mon, 2 Feb 2015 05:30:44 GMT  (147kb)", "http://arxiv.org/abs/1502.00363v1", "14 pages, 10 figures"]], "COMMENTS": "14 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["wangmeng zuo", "faqiang wang", "david zhang", "liang lin", "yuchi huang", "deyu meng", "lei zhang"], "accepted": false, "id": "1502.00363"}, "pdf": {"name": "1502.00363.pdf", "metadata": {"source": "CRF", "title": "Iterated Support Vector Machines for Distance Metric Learning", "authors": ["Wangmeng Zuo"], "emails": ["cswmzuo@gmail.com;", "tshfqw@163.com)", "dzhang@comp.polyu.edu.hk;", "cslzhang@comp.polyu.edu.hk)", "linliang@ieee.org)", "yuchi@nec.cn)"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 2.\n00 36\n3v 1\n[ cs\n.L G\n] 2\nF eb\n2 01\n5 1\nIndex Terms\u2014metric learning, support vector machine, kernel method, Lagrange duality, alternative optimization\n\u2726"}, {"heading": "1 INTRODUCTION", "text": "D ISTANCE metric learning aims to train a validdistance metric which can enlarge the distances between samples of different classes and reduce the distances between samples of the same class [1]. Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc. One popular metric learning approach is the Mahalanobis distance metric learning, which is to learn a linear transformation matrix L or a matrix M = LTL from the training data. Given two samples xi and xj , the Mahalanobis distance between them is defined as:\nd2 M (xi,xj) = \u2016L(xi \u2212 xj)\u2016 2 2\n= (xi \u2212 xj) T M (xi \u2212 xj) .\n(1)\nTo satisfy the nonnegative property of a distance metric, M should be positive semidefinite (PSD). According\n\u2022 W. Zuo and F. Wang are with the School of Computer Science and Technology, Harbin Institute of Technology, Harbin, 150001, China. (email: cswmzuo@gmail.com; tshfqw@163.com) \u2022 D. Zhang and L. Zhang are with the Department of Computing, the Hong Kong Polytechnic University, Kowloon, Hong Kong. (e-mail: csdzhang@comp.polyu.edu.hk; cslzhang@comp.polyu.edu.hk) \u2022 L. Lin is with the School of Super-computing, Sun Yat-Sen University, Guangzhou, 510275, China. (e-mail: linliang@ieee.org) \u2022 Y. Huang is with the NEC Laboratories China, Beijing, 100084, China. (e-mail: huang yuchi@nec.cn) \u2022 D. Meng is with the Institute of Information and System Sciences, Faculty of Mathematics and Statistics, Xi\u2019an Jiaotong University, Xi\u2019an, 710049, China. (e-mail: dymeng@mail.xjtu.edu.cn)\nManuscript received XXX; revised XXX.\nto which one of M and L is learned, Mahalanobis distance metric learning methods can be grouped into two categories. Methods that learn L, including neighborhood components analysis (NCA) [16], large margin components analysis (LMCA) [17] and neighborhood repulsed metric learning (NRML) [18], are mostly formulated as nonconvex optimization problems, which are solved by gradient descent based optimizers. Taking the PSD constraint into account, methods that learn M, including large margin nearest neighbor (LMNN) [19] and maximally collapsing metric learning (MCML) [20], are mostly formulated as convex semidefinite programming (SDP) problems, which can be optimized by standard SDP solvers [19], projected gradient [3], Boosting-like [21], or Frank-Wolfe [22] algorithms. Davis et al. [23] proposed an information-theoretic metric learning (ITML) model with an iterative Bregman projection algorithm, which does not need projections onto the PSD cone. Besides, the use of online solvers for metric learning has been discussed in [9], [24], [25].\nOn the other hand, kernel methods [26]\u2013[31] have been widely studied in many learning tasks, e.g., semisupervised learning, multiple instance learning, multitask learning, etc. Kernel learning methods, such as support vector machine (SVM), exhibit good generalization performance. There are many open resources on kernel classification methods, and a variety of toolboxes and libraries have been released [32]\u2013[38]. It is thus important to investigate the connections between metric learning and kernel classification and explore how to utilize the kernel classification resources in the research and development of metric learning methods.\nIn this paper, we propose a novel formulation of metric learning by casting it as a kernel classification problem, which allows us to effectively and efficiently\n2\nlearn distance metrics by iterated training of SVM. The off-the-shelf SVM solvers such as LibSVM [33] can be employed to solve the metric learning problem. Specifically, we propose two novel methods to bridge metric learning with the well-developed SVM techniques, and they are easy to implement. First, we propose a Positive-semidefinite Constrained Metric Learning (PCML) model, which can be solved via iterating between PSD projection and dual SVM learning. Second, by re-parameterizing the matrix M, we transform the PSD constraint into a nonnegative coefficient constraint and consequently propose a Nonnegativecoefficient Constrained Metric Learning (NCML) model, which can be solved by iterated learning of two SVMs. Both PCML and NCML have globally optimal solutions, and our extensive experiments on UCI dataset classification, handwritten digit recognition, face verification and person re-identification clearly demonstrate the effectiveness of them.\nThe remainder of this paper is organized as follows. Section 2 reviews the related works. Section 3 presents the PCML model and the optimization algorithm. Section 4 presents the model and algorithm of NCML. Section 5 presents the experimental results, and Section 6 concludes the paper.\nThe main abbreviations used in this paper are summarized in Table 1."}, {"heading": "2 RELATED WORK", "text": "Compared with nonconvex metric learning models [16], [17], [40], convex formulation of metric learning [2], [3], [20]\u2013[22] has drawn increasing attentions due to its desired properties such as global optimality. Most convex metric learning models can be formulated as SDP or quadratic SDP problems. Standard SDP solvers,\nhowever, are inefficient for metric learning, especially when the size of training samples is big or the feature dimension is high. Therefore, customized optimization algorithm needs to be developed for each specific metric learning model. For LMNN, Weinberger et al. developed an efficient solver based on the sub-gradient descent and the active set techniques [41]. In ITML, Davis et al. [23] suggested an iterative Bregman projection algorithm. Iterative projected gradient descent method [3], [42] has been widely employed for metric learning but it requires an eigenvalue decomposition in each iteration. Other algorithms such as block-coordinate descent [43], smooth optimization [44], and Frank-Wolfe [22] have also been studied for metric learning. Unlike the customized algorithms, in this work we formulate metric learning as a kernel classification problem and solve it using the offthe-shelf SVM solvers, which can guarantee the global optimality and the PSD property of the learned M, and is easy to implement and efficient in training.\nAnother line of work aims to develop metric learning algorithms by solving the Lagrange dual problems. Shen et al. derived the Lagrange dual of the exponential loss based metric learning model, and proposed a boostinglike approach, namely BoostMetric, where the matrix M is learned as a linear positive combination of rank-one matrices [21], [45]. MetricBoost [46] and FrobMetric [47], [48] were further proposed to improve the performance of BoostMetric. Liu and Vemuri incorporated two regularization terms in the duality for robust metric learning [49]. Note that BoostMetric [21], [45], MetricBoost [46], and FrobMetric [47] are proposed for metric learning with triplet constraints, whereas in many applications such as verification, only pairwise constraints are available in the training stage.\nSeveral SVM-based metric learning approaches [50]\u2013 [53] have also been proposed. Using SVM, Nguyen and Guo [50] formulated metric learning as a quadratic semidefinite programming problem, and suggested a projected gradient descent algorithm. The formulations of the proposed PCML and NCML in this work are different from the model in [50], and they are solved by the dual problems with the off-the-shelf SVM solvers. Brunner et al. [51] proposed a pairwise SVM method to learn a dissimilarity function rather than a distance metric. Different from [51], the proposed PCML and NCML learn a distance metric and the matrix M is constrained to be a PSD matrix. Do et al. [52] studied SVM from a metric learning perspective and presented an improved variant of SVM classification. Wang et al. [53] developed a kernel classification framework for metric learning and proposed two learning models which can be efficiently implemented by the standard SVM solvers. However, they adopted a two-step greedy strategy to solve the models and neglected the PSD constraint in the first step. In this work, the proposed PCML and NCML models have different formulations from [53], and their solutions are globally optimal.\n3"}, {"heading": "3 POSITIVE-SEMIDEFINITE CONSTRAINED METRIC LEARNING (PCML)", "text": "Denote by { (xi, yi)| i = 1, 2, \u00b7 \u00b7 \u00b7 , N} a training set, where xi \u2208 R\nd is the ith training sample, and yi is the class label of xi. The Mahalanobis distance between xi and xj can be equivalently written as:\nd2 M (xi,xj) = tr ( M T (xi \u2212 xj)(xi \u2212 xj) T )\n= \u2329 M, (xi \u2212 xj) (xi \u2212 xj) T \u232a , (2)\nwhere M is a PSD matrix, \u3008A,B\u3009 = tr ( A T B )\nis defined as the Frobenius inner product of two matrices A and B, and tr(\u2022) stands for the matrix trace operator. For each pair of xi and xj , we define a matrix Xij = (xi \u2212 xj)(xi \u2212 xj) T\n. With Xij , the Mahalanobis distance can be rewritten as d2\nM (xi,xj) = \u3008M,Xij\u3009.\n3.1 PCML and Its Dual Problem Let S = {(xi,xj) : the class labels of xi and xj are the same} be the set of similar pairs, and let D = {(xi,xj) : the class labels of xi and xj are different} be the set of dissimilar pairs. By introducing an indicator variable hij\nhij =\n{\n1, if (xi, xj ) \u2208 D \u22121, if (xi, xj) \u2208 S,\n(3)\nthe PCML model can be formulated as:\nmin M,b,\u03be\n1 2 \u2016M\u2016 2 F + C \u2211 i,j \u03beij\ns.t. hij (\u3008M,Xij\u3009+ b) \u2265 1\u2212 \u03beij , \u03beij \u2265 0, \u2200i, j\nM < 0,\n(4)\nwhere \u03beij denotes the slack variables, b denotes the bias, and \u2016 \u2016F denotes the Frobenius norm.\nThe PCML model defined above is convex and can be solved using the standard SDP solvers. However, the high complexity of general-purpose interior-point SDP solver makes it only suitable for small-scale problems. In order to improve the efficiency, in the following we first analyze the Lagrange duality of the PCML model, and then propose an algorithm to iterate between SVM training and PSD projection to learn the Mahalanobis distance metric.\nBy introducing the Lagrange multipliers \u03bb and a PSD matrix Y, the Lagrange dual of the problem in (4) can be formulated as:\nmax \u03bb,Y\n\u2212 1\n2\n\u2225 \u2225 \u2225 \u2211\ni,j \u03bbijhijXij +Y\n\u2225 \u2225 \u2225 2\nF\n+ \u2211\ni,j \u03bbij\ns.t. \u2211\ni,j \u03bbijhij = 0, 0 \u2264 \u03bbij \u2264 C, \u2200i, j, Y < 0.\n(5)\nPlease refer to Appendix A for the detailed derivation of the dual problem. Based on the Karush-Kuhn-Tucker (KKT) conditions, the matrix M can be obtained by\nM = \u2211\ni,j \u03bbijhijXij +Y. (6)\nThe strong duality allows us to first solve the equivalent dual problem in (5) and then obtain the matrix M by (6). However, due to the PSD constraint Y < 0, the problem in (5) is still difficult to optimize.\nAlgorithm 1 Algorithm of PCML\nInput: S = {(xi,xj) : the class labels of xi and xj are the same}, D = {(xi,xj) : the class labels of xi and xj are different}, and hij . Output: M. Initialize Y(0), t \u2190 0. repeat\n1. Update \u03b7(t+1) with \u03b7 (t+1) ij = 1\u2212 hij \u2329 Xij ,Y (t) \u232a . 2. Update \u03bb(t+1) by solving the subproblem (7) using an SVM solver. 3. Update Y (t+1) 0 = \u2212 \u2211 i,j \u03bb (t+1) ij hijXij . 4. Update Y(t+1) = U(t+1)\u039b (t+1) + U (t+1)T , where Y (t+1) 0 = U (t+1) \u039b (t+1) U (t+1)T and \u039b (t+1) + = max ( \u039b (t+1),0 )\n. 5. t \u2190 t+ 1. until convergence M = \u2211\ni,j \u03bb (t\u22121) ij hijXij +Y (t\u22121). return M"}, {"heading": "3.2 Alternative Optimization Algorithm", "text": "To solve the dual problem efficiently, we propose an optimization approach by updating \u03bb and Y alternatively. Given Y, we introduce a new variable \u03b7 with \u03b7ij = 1\u2212hij \u3008Xij ,Y\u3009 = 1\u2212hij(xi \u2212 xj) T Y (xi \u2212 xj), and the subproblem on \u03bb can be formulated as:\nmax \u03bb\n\u2212 1\n2\n\u2211\ni,j\n\u2211\nk,l \u03bbij\u03bbklhijhkl \u3008Xij ,Xkl\u3009+\n\u2211\ni,j \u03b7ij\u03bbij\ns.t. \u2211\ni,j \u03bbijhij = 0, 0 \u2264 \u03bbij \u2264 C, \u2200i, j.\n(7) The subproblem (7) is a QP problem. We can define a\nkernel function of sample pairs as follows:\nK ((xi,xj) , (xk,xl)) = \u3008Xij ,Xkl\u3009\n= ( (xi \u2212 xj) T (xk \u2212 xl)\n)2 . (8)\nSubstituting (8) into (7), the subproblem on \u03bb becomes a kernel-based classification problem, and can be efficiently solved by using the existing SVM solvers such as LibSVM [33]. Given \u03bb, the subproblem on Y can be formulated as the projection of a matrix onto the convex cone of PSD matrices:\nmin Y\n\u2016Y \u2212Y0\u2016 2 F , s.t. Y < 0, (9)\nwhere Y0 = \u2212 \u2211 i,j \u03bbijhijXij . Through the eigendecomposition of Y0, i.e., Y0 = U\u039bU T and \u039b is the diagonal matrix of eigenvalues, the solution to the subproblem on Y can be explicitly expressed as Y = U\u039b+U\nT , where \u039b+ = max (\u039b,0). Finally, the PCML algorithm is summarized in Algorithm 1."}, {"heading": "3.3 Optimality Condition", "text": "As shown in [54], [55], the general alternating minimization approach will converge. By alternatively updating\n4 \u03bb and Y, the proposed algorithm can reach the global optimum of the problems in (4) and (5).\nThe optimality condition of the proposed algorithm can be checked by the duality gap in each iteration, which is defined as the difference between the primal and dual objective values:\nDualGap (n) PCML =\n1\n2\n\u2225 \u2225 \u2225 M (n) \u2225 \u2225 \u2225 2\nF + C\n\u2211\ni,j \u03be (n) ij \u2212\n\u2211\ni,j \u03bb (n) ij\n+ 1\n2\n\u2225 \u2225 \u2225 \u2211\ni,j \u03bb (n) ij hijXij +Y\n(n) \u2225 \u2225\n\u2225\n2 F ,\n(10) where M(n), \u03be(n), \u03bb(n), and Y(n) are feasible primal and dual variables, and DualGap (n) PCML is the duality gap in the nth iteration. According to (6), we can derive that\nM (n) =\n\u2211\ni,j \u03bb (n) ij hijXij +Y (n) = Y(n) \u2212Y (n) 0 . (11)\nAs shown in Subsection 3.2, Y (n) 0 = U (n) \u039b (n) U (n)T , Y (n) = U(n)\u039b\n(n) + U (n)T , and hence M(n) =\nU (n) \u039b (n) \u2212 U (n)T , where \u039b (n) \u2212 = \u039b (n) + \u2212 \u039b\n(n). Thus, \u2225 \u2225M (n) \u2225 \u2225 2\nF can be computed by\n\u2225 \u2225 \u2225 M (n) \u2225 \u2225 \u2225 2\nF = tr\n(\nM (n)T M (n)\n)\n= tr ( U (n)\n\u039b (n) \u2212 U (n)T U (n) \u039b (n) \u2212 U (n)T\n)\n= tr ( U (n)\n\u039b (n)2 \u2212 U (n)T\n) = tr (\n\u039b (n)2 \u2212\n)\n.\n(12)\nSubstituting (11) and (12) into (10), the duality gap of PCML can be obtained as follows\nDualGap (n) PCML = C\n\u2211\ni,j \u03be (n) ij \u2212\n\u2211\ni,j \u03bb (n) ij + tr\n(\n\u039b (n) \u2212\n2)\n.\n(13) Based on the KKT conditions of the PCML dual prob-\nlem in (5), \u03be (n) ij can be obtained by\n\u03be (n) ij =\n\n\n\n0, \u2200\u03bb (n) ij < C [\n1\u2212 hij\n(\u2329\nM (n),Xij\n\u232a + b(n) )]\n+ , \u2200\u03bb\n(n) ij = C,\n(14) where\nb(n) = 1 hij \u2212 \u2329 M (n),Xij \u232a , \u22000 < \u03bb (n) ij < C. (15)\nPlease refer to Appendix A for the detailed derivation of \u03be (n) ij and b\n(n). The duality gap is always nonnegative and approaches to zero when the primal problem is convex. Thus, it can be used as the termination condition of the algorithm. Fig. 1 plots the curve of duality gap versus the number of iterations on the PenDigits dataset by PCML. One can see that the duality gap converges to zero in less than 20 iterations and our algorithm will reach the global optimum. In Algorithm 1, we adopt the following termination condition:\nDualGap (t) PCML < \u03b5 \u00b7 DualGap (1) PCML, (16)\nwhere \u03b5 is a small constant and we set \u03b5 = 0.01 in the experiment.\n1 3 5 7 9 11 13 15 1718 0\n50\n100\n150\nNumber of Iterations\nD u\nal it\ny G\nap\nFig. 1: Duality gap vs. number of iterations on the PenDigits dataset for PCML."}, {"heading": "3.4 Remarks", "text": "Warm-start: In the updating of \u03bb, we adopt a simple warm-start strategy. We use the solution of the previous iteration as the initialization of the next iteration. Since the previous solution can serve as a good guess, warmstart results in significant improvement in efficiency. Construction of pairwise constraints: Based on the training set, we can introduce N2 pairwise constraints in total. However, in practice we only need to choose a subset of pairwise constraints to reduce the computational cost. For each sample, we find its k nearest neighbors to construct similar pairs and its k farthest neighbors to construct dissimilar pairs. Thus, we only need 2kN pairwise constraints. By this strategy, we can reduce the scale of pairwise constraints from O ( N2 ) to O (kN). Since k is usually small constant (=1\u223c3) in practice, the computational cost of metric learning is much reduced. Similar strategy for constructing pairwise or triplet constraints can be found in [2], [11]. Computational Complexity: We use the LibSVM library for SVM training. The computational complexity of SMO-type algorithms [34] is O(k2N2d). For PSD projection, the complexity of conventional SVD algorithms is O(d3)."}, {"heading": "4 NONNEGATIVE-COEFFICIENT CONSTRAINED METRIC LEARNING (NCML)", "text": "Given a set of rank-1 PSD matrices Mt = mtm T t (t = 1, \u00b7 \u00b7 \u00b7 , T ), a linear combination of Mt is defined as M = \u2211\nt \u03b1tMt, where \u03b1t is the scalar combination coefficient. One can easily prove the following Theorem 1. Theorem 1: If the scalar coefficient \u03b1t \u2265 0, \u2200t, the matrix M = \u2211\nt \u03b1tMt is a PSD matrix, where Mt = mtm T t is a\nrank-1 PSD matrix. Proof: Denote by u \u2208 Rd a random vector. Based on the expression of M, we have:\nu T Mu = uT\n(\n\u2211\nt \u03b1tmtm\nT t\n)\nu\n= \u2211\nt \u03b1tu\nT mtm T t u =\n\u2211\nt \u03b1t ( u T mt )2 .\n5 Since ( u T mt )2 \u2265 0 and \u03b1t \u2265 0, \u2200t, we have u T Mu \u2265 0. Therefore, M is a PSD matrix."}, {"heading": "4.1 NCML and Its Dual Problem", "text": "Motivated by Theorem 1, we propose to transform the PSD constraint in (4) by re-parameterizing the distance metric M, and develop a nonnegative-coefficient constrained metric learning (NCML) method to learn the PSD matrix M. Given the training data S and D, a rank-1 PSD matrix Xij can be constructed for each pair (xi,xj). By assuming that the learned matrix should be the linear combination of Xij with the nonnegative coefficient constraint, the NCML model can be formulated as:\nmin M,b,\u03b1,\u03be\n1 2 \u2016M\u2016 2 F + C \u2211 i,j \u03beij\ns.t. hij (\u3008M,Xij\u3009+ b) \u2265 1\u2212 \u03beij , \u03b1ij \u2265 0, \u03beij \u2265 0, \u2200i, j\nM = \u2211\ni,j \u03b1ijXij .\n(17) By substituting M with \u2211\ni,j \u03b1ijXij , we reformulate the NCML model as follows:\nmin \u03b1,b,\u03be\n1\n2\n\u2211\ni,j\n\u2211\nk,l \u03b1ij\u03b1kl \u3008Xij ,Xkl\u3009+ C\n\u2211\ni,j \u03beij\ns.t. hij ( \u2211\nk,l \u03b1kl \u3008Xij ,Xkl\u3009+ b\n)\n\u2265 1\u2212 \u03beij\n\u03b1ij \u2265 0, \u03beij \u2265 0, \u2200i, j.\n(18)\nBy introducing the Lagrange multipliers \u03b7 and \u03b2, the Lagrange dual of the primal problem in (18) can be formulated as:\nmax \u03b7,\u03b2\n\u2212 1\n2\n\u2211\ni,j\n\u2211\nk,l (\u03b2ijhij + \u03b7ij) (\u03b2klhkl + \u03b7kl) \u3008Xij ,Xkl\u3009\n+ \u2211\ni,j \u03b2ij\ns.t. \u2211\nk,l \u03b7kl \u3008Xij ,Xkl\u3009 \u2265 0, 0 \u2264 \u03b2ij \u2264 C, \u2200i, j\n\u2211\ni,j \u03b2ijhij = 0.\n(19) Please refer to Appendix B for the detailed derivation of the dual problem. Based on the KKT conditions, the coefficient \u03b1ij can be obtained by:\n\u03b1ij = \u03b2ijhij + \u03b7ij . (20)\nThus, we can first solve the above dual problem, and then obtain the matrix M by\nM = \u2211\ni,j (\u03b2ijhij + \u03b7ij)Xij . (21)"}, {"heading": "4.2 Optimization Algorithm", "text": "There are two groups of variables, \u03b7 and \u03b2, in problem (19). We adopt an alternative optimization approach to solve them. First, given \u03b7, the variables \u03b2ij can be solved as follows:\nmax \u03b2\n\u2212 1\n2\n\u2211\ni,j\n\u2211\nk,l \u03b2ij\u03b2klhijhkl \u3008Xij ,Xkl\u3009+\n\u2211\ni,j \u03b4ij\u03b2ij\ns.t. 0 \u2264 \u03b2ij \u2264 C, \u2200i, j, \u2211\ni,j \u03b2ijhij = 0,\n(22)\nAlgorithm 2 Algorithm of NCML\nInput: Training set {(xi,xj) , hij}. Output: The matrix M. Initialize \u03b7(0) with small random values, t \u2190 0. repeat\n1. Update \u03b4(t+1) with \u03b4 (t+1) ij = (\n1\u2212 hij \u2211 kl \u03b7 (t) kl \u3008Xij ,Xkl\u3009\n)\n.\n2. Update \u03b2(t+1) by solving the subproblem (15) using an SVM solver. 3. Update \u03b3(t+1) with \u03b3 (t+1) ij = \u2211\nkl \u03b2 (t+1) kl hkl \u3008Xij ,Xkl\u3009.\n4. Update \u00b5(t+1) by solving the subproblem (18) using an SVM solver. 5. Update \u03b7(t+1) with \u03b7 (t+1) ij \u2190 \u00b5 (t+1) ij \u2212 hij\u03b2 (t+1) ij .\n6. t \u2190 t+ 1. until convergence M = \u2211\nij \u00b5 (t) ij Xij .\nreturn M\nwhere \u03b4 is the variable with \u03b4ij = (1\u2212 hij \u2211\nkl \u03b7kl \u3008Xij ,Xkl\u3009). Clearly, the subproblem on \u03b2 is exactly the dual problem of SVM, and it can be efficiently solved by any standard SVM solvers, e.g., LibSVM [33].\nGiven \u03b2, the subproblem on \u03b7 can be formulated as follows:\nmin \u03b7\n1\n2\n\u2211\ni,j\n\u2211\nk,l \u03b7ij\u03b7kl \u3008Xij ,Xkl\u3009+\n\u2211\ni,j \u03b7ij\u03b3ij\ns.t. \u2211\nk,l \u03b7ij \u3008Xij ,Xkl\u3009 \u2265 0, \u2200i, j,\n(23)\nwhere \u03b3ij = \u2211\nkl \u03b2klhkl \u3008Xij ,Xkl\u3009. To simplify the subproblem on \u03b7, we derive the Lagrange dual of (23) based on the KKT condition:\n\u03b7ij = \u00b5ij \u2212 hij\u03b2ij , \u2200i, j, (24)\nwhere \u00b5 is the Lagrange dual multiplier. The Lagrange dual problem of (23) is formulated as follows:\nmax \u00b5\n\u2212 1\n2\n\u2211\ni,j\n\u2211\nk,l \u00b5ij\u00b5kl \u3008Xij ,Xkl\u3009+\n\u2211\ni,j \u03b3ij\u00b5ij\ns.t. \u00b5ij \u2265 0, \u2200i, j. (25) Please refer to Appendix C for the detailed derivation. Clearly, problem (25) is a simpler QP problem than (23), which can be efficiently solved by the standard SVM solvers.\nBy alternatively updating \u00b5 and \u03b2, we can solve the NCML dual problem (19). After obtaining the optimal solutions of \u00b5 and \u03b2, the optimal solution of \u03b1 in problem (18) can be obtained by\n\u03b1ij = \u00b5ij , \u2200i, j. (26)\nWe then have M = \u2211\nij \u03b1ijXij . The NCML algorithm is summarized in Algorithm 2.\n6 Analogous to PCML, the updating of \u03b2 and \u00b5 in NCML can be speeded up by using the warm-start strategy. As shown in Fig. 2, the proposed NCML algorithm will converge in 10\u223c15 iterations."}, {"heading": "4.3 Optimality Condition", "text": "We check the duality gap of NCML to investigate the optimality condition of it. From the primal and dual objectives in (18) and (19), the NCML duality gap in the nth iteration is\nDualGap (n) NCML =\n1\n2\n\u2211\ni,j,k,l\n\u03b1 (n) ij \u03b1 (n) kl \u3008Xij ,Xkl\u3009+ C\n\u2211\ni,j\n\u03be (n) ij\n+ 1\n2\n\u2211\ni,j,k,l\n(\n\u03b2 (n) ij hij + \u03b7 (n) ij\n)(\n\u03b2 (n) kl hkl + \u03b7 (n) kl\n)\n\u3008Xij ,Xkl\u3009\n\u2212 \u2211\ni,j\n\u03b2 (n) ij ,\n(27)\nwhere \u03b1 (n) ij and \u03be (n) ij are the feasible solutions to the primal problem, \u03b2 (n) ij and \u03b7 (n) ij are the feasible solutions to the dual problem, and DualGap (n) NCML is the duality gap in the nth iteration. As \u03b7 (n) ij and \u00b5 (n) ij are the optimal solutions to the primal subproblem on \u03b7 in (23) and its dual problem in (25), respectively, the duality gap of subproblem on \u03b7 is zero, i.e.,\n1\n2\n\u2211\ni,j\n\u2211\nk,l \u03b7 (n) ij \u03b7 (n) kl \u3008Xij ,Xkl\u3009+\n\u2211\ni,j \u03b7 (n) ij \u03b3 (n) ij\n+ 1\n2\n\u2211\ni,j\n\u2211\nk,l \u00b5 (n) ij \u00b5 (n) kl \u3008Xij ,Xkl\u3009 \u2212\n\u2211\ni,j \u03b3 (n) ij \u00b5 (n) ij = 0.\n(28) As shown in (26), \u03b1\n(n) ij and \u00b5 (n) ij should be equal. We\nsubstitute (28) into (27) as follows:\nDualGap (n) NCML = C\n\u2211\ni,j \u03be (n) ij \u2212\n\u2211\ni,j \u03b2 (n) ij +\n\u2211\ni,j \u00b5 (n) ij \u03b3 (n) ij .\n(29) Based on the KKT conditions of the NCML dual problem in (19), \u03be (n) ij can be obtained by (30) (see page 7), where [z] = max (z, 0) and b(n) can be obtained by\nb(n) = 1 hij \u2212 \u2211 k,l \u03b1 (n) kl \u3008Xij ,Xkl\u3009\n= \u03b4 (n+1) ij\nhij \u2212 \u03b3\n(n) ij for all 0 < \u03b2 (n) ij < C.\n(31)\nPlease refer to Appendix B for the detailed derivation of \u03be (n) ij and b\n(n). Fig. 2 plots the curve of duality gap versus the number of iterations on the PenDigits dataset by NCML. One can see that the duality gap converges to zero in 15 iterations, and NCML reaches the global optimum. In the implementation of Algorithm 2, we adopt the following termination condition:\nDualGap (t) NCML < \u03b5 \u00b7 DualGap (1) NCML, (32)\nwhere \u03b5 is a small constant and we set \u03b5 = 0.01 in the experiment.\n1 3 5 7 9 11 13 15 0\n200\n400\n600\n800\n1000\n1200\nNumber of Iterations\nD u\nal it\ny G\nap\nFig. 2: Duality gap vs. number of iterations on the PenDigits dataset for NCML."}, {"heading": "4.4 Remarks", "text": "Computational complexity: We use the same strategy as that in PCML to construct the pairwise constraints for NCML. In each iteration, NCML calls for the SVM solver twice while PCML calls for it only once. When the SMO-type algorithm [34] is adopted for SVM training, the computational complexity of NCML is O ( k2N2d )\n. One extra advantage of NCML lies in its lower computational cost with respect to d, which involves the computation of \u3008Xij ,Xkl\u3009 and the construction of matrix M. Since \u3008Xij ,Xkl\u3009 = ( (xi \u2212 xj) T (xk \u2212 xl) )2 , the cost of\ncomputing \u3008Xij ,Xkl\u3009 is O (d). The cost of constructing the matrix M is less than O ( kNd2 )\n, and this operation is required only once after the convergence of \u03b2 and \u00b5. Nonlinear extensions: Note that \u3008Xij ,Xkl\u3009 = tr (\nX T ijXkl\n)\ncan be treated as an inner product of two pairs of samples: (xi,xj) and (xk,xl). Analogous to PCML, if we can define a kernel K ((xi,xj), (xk,xl)) on (xi,xj) and (xk,xl), we can substitute \u3008Xij ,Xkl\u3009 with K ((xi,xj), (xk,xl)) to develop new linear or even nonlinear metric learning algorithms, and the Mahalanobis distance between any two samples xm and xn can be formulated as:\n(xm \u2212 xn) T M (xm \u2212 xn) = \u2211\ni,j \u03b1ijK ((xi,xj) , (xm,xn)) .\n(33)\nAnother nonlinear extension strategy is to define a kernel k (xi,xj) on xi and xj . Since \u3008Xij ,Xkl\u3009 = ( x T i xk \u2212 x T i xl \u2212 x T j xk + x T j xl )2 , we can substitute \u3008Xij ,Xkl\u3009 with (k (xi,xk)\u2212 k (xi,xl)\u2212 k (xj ,xk) + k (xj ,xl)) 2 and formulate the Mahalanobis distance between xm and xn as:\n(xm \u2212 xn) T M (xm \u2212 xn)\n= \u2211\ni,j \u03b1ij\n(\nk (xi,xm)\u2212 k (xi,xn)\n\u2212 k (xj ,xm) + k (xj ,xn)\n)2\n. (34)\nThat is to say, NCML allows us to learn nonlinear metrics for histograms and structural data by designing proper kernel functions and incorporating appropriate regularizations on \u03b1. Metric learning for structural data beyond\n7 \u03be (n) ij =  \n\n0 for all \u03b2 (n) ij < C [\n1\u2212 hij\n(\n\u2211\nk,l \u03b1 (n) kl \u3008Xij ,Xkl\u3009+ b\n(n) )]\n+ =\n[\n\u03b4 (n+1) ij \u2212 hij\n(\n\u03b3 (n) ij + b\n(n) )]\n+ for all \u03b2\n(n) ij = C.\n(30)\nvector data has been recently receiving considerable research interests [5], [56], and NCML can provide a new perspective on this topic. SVM solvers: Although our implementation is based on LibSVM, there are a number of well-studied SVM training algorithms, e.g., core vector machines [35], LaRank [36], BMRM [37], and Pegasos [38], which can be utilized for large scale metric learning. Moreover, we can refer to the progresses in kernel methods [26]\u2013[28] for developing semi-supervised, multiple instance, and multitask metric learning approaches."}, {"heading": "5 EXPERIMENTAL RESULTS", "text": "We evaluate the proposed PCML and NCML models for k-NN classification (k = 1) using 9 UCI datasets, 4 handwritten digit datasets, 2 face verification datasets and 2 person re-identification datasets. We compare PCML and NCML with the baseline Euclidean distance metric and 7 state-of-the-art metric learning models, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22]. On each dataset, if the partition of training set and test set is not defined, we evaluate the performance of each method by 10- fold cross-validation, and the classification error rate and training time are obtained by averaging over 10 runs of 10-fold cross-validation. PCML and NCML are implemented using the LibSVM1 toolbox. The source codes of NCA2, ITML3, MCML4, LDML5, LMNN6, PLML7, and DML-eig8 are online available, and we tune their parameters to get the best results."}, {"heading": "5.1 Results on the UCI Datasets", "text": "We first use 9 datasets from the UCI Machine Learning Repository [57] to evaluate the proposed models. The information of the 9 UCI datasets is summarized in Table 2. On the Satellite, SPECTF Heart, and Letter datasets, the training set and test set are defined. On the other datasets, we use 10-fold cross-validation to evaluate the metric learning models.\nThe proposed PCML and NCML methods involve only one hyper-parameter, i.e., the regularization parameter C. We simply adopt the cross-validation strategy to select C by investigating the influence of C on the\n1. http://www.csie.ntu.edu.tw/\u223ccjlin/libsvm/ 2. http://www.cs.berkeley.edu/\u223cfowlkes/software/nca/ 3. http://www.cs.utexas.edu/\u223cpjain/itml/ 4. http://homepage.tudelft.nl/19j49/Matlab Toolbox for\nDimensionality Reduction.html 5. http://lear.inrialpes.fr/people/guillaumin/code.php 6. http://www.cse.wustl.edu/\u223ckilian/code/code.html 7. http://cui.unige.ch/\u223cwangjun/ 8. http://empslocal.ex.ac.uk/people/staff/yy267/software.html\nclassification error rate. Fig. 3 shows the curves of classification error rate versus C for PCML and NCML on the SPECTF Heart dataset. The curves on other datasets are similar. We can observe that when C < 1, the classification error rates of PCML and NCML will be low and stable. When C is higher than 1, the classification error rates jump dramatically. Thus, we set C < 1 in our experiments.\nWe compare the classification error rates of the competing methods in Table 3. On the Cardiotocography and Segmentation datasets, PCML achieves the lowest error rates. On the Segmentation and SPECTF Heart datasets, NCML achieves the lowest error rates. The average ranks of competing methods are listed in the last row of Table 3. On each dataset, we rank the methods based on their error rates, i.e., we assign rank 1 to the method with the lowest error rate and rank 2 to the method with the second lowest error rate, and so on. The average rank is defined as the mean rank of one method over the nine datasets, which can provide a fair comparison of the learning methods [58]. From Table 3, we can see that both PCML and NCML achieve the first and second best average ranks, respectively, demonstrating strong classification capability for general classification tasks.\nWe then compare the training time of competing metric learning methods in Fig. 4. All the experiments\n8\nare run in a PC with 4 Intel Core i5-2410 CPUs (2.30 GHz) and 16GB RAM. Clearly, the proposed PCML and NCML are the fastest in most cases. Although DML-eig is faster than PCML on the Letter dataset, its classification error rate on this dataset is much higher than PCML and NCML. On average, PCML and NCML are 23 and 18 times faster than PLML, the third fastest algorithm, respectively."}, {"heading": "5.2 Handwritten Digit Recognition", "text": "We further evaluate the proposed methods on four handwritten digit datasets: MNIST, Pen-based recognition of handwritten Digits data set (PenDigits), Semeion and USPS. Table 4 summarizes the basic information of these four handwritten digit datasets. On the MNIST, PenDigits, and USPS datasets, we use the defined training sets to train the metrics, and use the defined test sets to compute the classification error rates. On the Semeion dataset, we use 10-fold cross-validation to evaluate the metric learning methods, and the classification error rate and training time are obtained by averaging over 10 runs of 10-fold cross-validation.\nAs the dimensions of images in the MNIST, Semeion and USPS datasets are relatively high, we use principal component analysis (PCA) to reduce the feature dimension to 100, and train the metrics in the PCA subspace. Table 5 lists the classification error rates of the ten competing methods on the four handwritten digit datasets. The last row of Table 5 lists the average ranks of the competing methods. We do not report the\nerror rate and training time of MCML on the MNIST dataset because MCML requires too large memory space (more than 30 GB) on this dataset and cannot run in our PC. From Table 5, we can see that both PCML and NCML achieve the best average rank. Again, the results indicate that the proposed methods have better classification performance.\nAll the experiments were executed in the same PC as used in Subsection 5.1. Fig. 5 compares the training time of NCA, ITML, MCML, LDML, LMNN, DML-eig, PLML, PCML, and NCML. Clearly, the proposed PCML and NCML methods are much faster than the other methods. On average, PCML and NCML are 61 and 27 times faster than PLML, the third fastest algorithm, respectively. One can conclude that PCML and NCML offer promising solutions to effective and efficient metric learning.\nFinally, we compare the running time of PCML and NCML under different feature dimensions d. As analyzed in Subsections 3.4 and 4.4, the time complexities of PCML and NCML are O(N2d + d3) and O(N2d), respectively. Fig. 6 shows the training time on the Semeion\n9\ndataset with different PCA dimensions. We can see that when the dimension is lower than 110, the training time of NCML is longer than PCML. When the dimension is higher than 110, the training time of PCML increases and becomes longer than NCML."}, {"heading": "5.3 Face Verification", "text": "In this subsection, we evaluate the proposed methods for face verification using two challenging face databases: Labeled Faces in the Wild (LFW) [59] and Public Figures (PubFig) [60]."}, {"heading": "5.3.1 The LFW Database", "text": "The face images in the LFW database were collected from the Internet and demonstrate large variations of pose, illumination, expression, etc. The database consists of 13,233 face images from 5,749 persons. Under the image restricted setting, the performance of a face verification method is evaluated by 10-fold cross validation. For each of the 10 runs, the database provides 300 positive pairs and 300 negative pairs for testing, and 5,400 image pairs for training. The verification rate and Receiver Operator Characteristic (ROC) curve of each method are obtained by averaging over the 10 runs.\nIn our experiments, we use the SIFT [61] features and the attribute features provided by [8] and [60] to evaluate the metric learning methods. Since the dimension of SIFT features is high (i.e., 128 \u00d7 3 \u00d7 9), PCA is used to reduce the feature dimension to 150. Under the restricted setting of the LFW database, we only know whether two images are matched or not for the given pairs. In the training stage, we use the training pairs to train a Mahalanobis distance metric. In the test stage, we\ncompare the Mahalanobis distance of the test pair with a threshold t to decide whether the two images are matched or not.\nWe report the ROC curves of PCML, NCML, DML-eig [22], ITML [23], KISSME [9], LDML [8] and Euclidean distance in Fig. 7. We also compare the verification accuracies of PCML and NCML and other metric learning methods by using the SIFT and the attribute features in Table 6. It can be seen that the proposed PCML and NCML methods perform much better than all the other competing methods. Using the combination of SIFT and Attribute features, the verification accuracies of PCML (89.00%) and NCML (89.50%) are higher than the third best method, i.e. DML-eig (85.65%), by 3.35% and 3.85%, respectively. We also compare the training time of the competing methods in Table 6. The training time of PCML and NCML is shorter than the other methods except for KISSME. The reason is that KISSME is a onepass training approach. Although KISSME is faster, its verification accuracy is much lower than PCML and NCML."}, {"heading": "5.3.2 The PubFig Database", "text": "The PubFig database [60] contains 58,797 face images of 200 persons with large variations in pose, lighting, expression, scene, camera, imaging conditions and parameters, etc. In this database, the face verification methods are also evaluated using 10-fold cross validation. Among the given 20,000 image pairs, we randomly select 18,000 pairs for training and use the remaining 2,000 pairs for testing in each run. The ROC curves and verification rates are obtained by averaging over the 10 runs.\n10\nWe use the attribute features provided by [60] to evaluate the competing methods. Fig. 8 shows the ROC curves of PCML, NCML, KISSME [9], ITML [23], DMLeig [22], Attribute Classifiers [60] and the baseline Euclidean distance. It can be seen that the performance of PCML and NCML is similar, and is superior to that of the other methods.\nWe further report the verification rates of PCML, NCML and the other methods in Table 7. One can see that PCML and NCML perform better than the other methods. The accuracies of PCML (79.71%) and NCML (79.75%) are higher than the third best method, i.e., Attribute Classifiers (78.65%), by 1.06% and 1.10%, respectively. The training time of PCML, NCML and other metric learning methods is also listed in Table 7. It can be seen that PCML and NCML are much faster than ITML and DML-eig."}, {"heading": "5.4 Person Re-identification", "text": "In this subsection, we evaluate the performance of the proposed methods for person re-identification, i.e., recognizing a person at different locations and at different times [62]. Two challenging person re-identification\ndatabases, the Viewpoint Invariant Pedestrian Recognition (VIPeR) database [63] and the Context Aware Vision using Image-based Active Recognition for ReIdentification (CAVIAR4REID) database [64] are used to assess the performance of the proposed methods."}, {"heading": "5.4.1 The VIPeR Database", "text": "The VIPeR database contains 1,264 pedestrian images of 632 persons from two camera viewspoints (camera A and camera B). For each person, there are two images taken from different viewpoints with a change of 90 degrees. In our experiments, we randomly select 316 persons and use their images for training, and use the images of the other 316 persons for testing. For the testing images, we use the images taken by camera B as the probe set and the images from camera A as the gallery set. Finally, 10 partitions of training and test sets are constructed, and the average accuracy over the 10 test sets is computed as the final accuracy.\nWe report the Cumulative Matching Characteristic (CMC) curves of the competing methods in Fig. 9. We also compare their accuracies under different ranks in Table 8. From Fig. 9 and Table 8, one can see that both PCML and NCML outperform LMNN, ITML and Euclidean distance significantly under all ranks. When the rank is no more than 25, PCML performs similarly\n11\nto KISSME, while NCML outperforms KISSME. When the rank is between 25 and 200, both PCML and NCML perform better than KISSME. The training time of the metric learning methods is also reported in Table 8. We can see that both PCML and NCML are much more efficient than LMNN and ITML in training."}, {"heading": "5.4.2 The CAVIAR4REID Database", "text": "CAVIAR4REID consists of 1,220 pedestrian images from 72 persons, where the images are extracted from the shopping center scenario of the CAVIAR database [64]. The database covers a large range of image resolution and pose variation. The minimum and maximum image sizes in the CAVIAR4REID database are 17 \u00d7 39 and 72 \u00d7 144, respectively. Following [65] and [10], we use the hierarchical Gaussian (HG) features to evaluate the metric learning methods.\nAccording to the evaluation protocol in [10], we randomly select 36 persons and use their images for training, and use the rest images for testing. For the testing images, we randomly select one image for each person to construct a probe set consisting of 36 images, and use the other test images as the gallery set. Finally, 10 partitions of training and test sets are constructed, and the final results are obtained by averaging over the 10 runs.\nWe report the CMC curves of PCML, NCML, DML-eig [22], KISSME [9], ITML [23], LMNN [19] and Euclidean\ndistance in Fig. 10. One can see that PCML and NCML perform the best and the second best among all the competing methods, respectively. Table 9 lists the reidentification accuracies and training time by different methods. PCML and NCML perform better than the other metric learning methods under all the ranks. We also report the training times of the competing metric learning methods in Table 9. PCML and NCML are much faster than the other metric learning methods except for KISSME."}, {"heading": "6 CONCLUSION", "text": "We proposed two distance metric learning models, namely Positive-semidefinite Constrained Metric Learning (PCML) and Nonnegative-coefficient Constrained Metric Learning (NCML). The proposed models can guarantee the positive semidefinite property of the learned matrix M, and can be solved efficiently by the existing SVM solvers. Experimental results on nine UCI machine learning repository datasets and four handwritten digit datasets showed that, compared with the state-of-the-art metric learning methods, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22], the proposed PCML and NCML methods can not only achieve higher classification accuracy, but also are much faster in training. On average, they are 35 and 21 times faster than PLML,\n12\nthe 3rd fastest metric learning method, respectively. The experimental results on LFW, PubFig, VIPeR and CAVIAR4REID databases indicate that the proposed methods also perform very well in vision tasks such as face verification and person re-identification, leading to higher verification rates and very competitive training efficiency."}, {"heading": "APPENDIX A THE DUAL OF PCML", "text": "The original problem of PCML is formulated as\nmin M,b,\u03be\n1 2 \u2016M\u2016 2 F + C \u2211 i,j \u03beij\ns.t. hij (\u3008M,Xij\u3009+ b) \u2265 1\u2212 \u03beij , \u03beij \u2265 0, \u2200i, j\nM < 0.\n(35)\nIts Lagrangian is:\nL (\u03bb,\u03ba,Y,M, b, \u03be) = 1\n2 \u2016M\u20162F + C\n\u2211\ni,j \u03beij\n\u2212 \u2211\ni,j \u03bbij [hij (\u3008M,Xij\u3009+ b)\u2212 1 + \u03beij ]\n\u2212 \u2211\ni,j \u03baij\u03beij \u2212 \u3008Y,M\u3009 ,\n(36)\nwhere \u03bb, \u03ba and Y are the Lagrange multipliers which satisfy \u03bbij \u2265 0, \u03baij \u2265 0, \u2200i, j, and Y < 0. Converting the original problem to its dual problem needs the following KKT conditions:\n\u2202L (\u03bb,\u03ba,Y,M, b, \u03be)\n\u2202M = 0 \u21d2 M\u2212\n\u2211\ni,j \u03bbijhijXij \u2212Y = 0,\n(37) \u2202L (\u03bb,\u03ba,Y,M, b, \u03be)\n\u2202b = 0 \u21d2\n\u2211\ni,j \u03bbijhij = 0, (38)\n\u2202L (\u03bb,\u03ba,Y,M, b, \u03be)\n\u2202\u03beij = C \u2212 \u03bbij \u2212 \u03baij = 0 \u21d2\n0 \u2264 \u03bbij \u2264 C, \u2200i, j,\n(39)\nhij (\u3008M,Xij\u3009+ b)\u2212 1 + \u03beij \u2265 0, \u03beij \u2265 0, (40)\n\u03bbij \u2265 0, \u03baij \u2265 0, Y < 0, (41)\n\u03bbij [hij (\u3008M,Xij\u3009+ b)\u2212 1 + \u03beij ] = 0, \u03baij\u03beij = 0. (42)\nEquation (37) implies the following relationship between \u03bb, Y and M:\nM = \u2211\ni,j \u03bbijhijXij +Y. (43)\nSubstituting (37)\u223c(39) back into the Lagrangian, we get the following Lagrange dual problem of PCML:\nmax \u03bb,Y\n\u2212 1\n2\n\u2225 \u2225 \u2225 \u2211\ni,j \u03bbijhijXij +Y\n\u2225 \u2225 \u2225 2\nF + \u2211 i,j \u03bbij\ns.t. \u2211\ni,j \u03bbijhij = 0, 0 \u2264 \u03bbij \u2264 C, \u2200i, j, Y < 0.\n(44)\nAs we can see from (43) and (44), M is explicitly determined by the training procedure, but b is not. Nevertheless, b can be easily found by using the KKT complementarity condition in (39) and (42), which show that \u03beij = 0 if \u03bbij < C, and hij (\u3008M,Xij\u3009+ b)\u22121+\u03beij = 0\nif \u03bbij > 0. Thus we can simply take any training point, for which 0 < \u03bbij < C, to compute b by\nb = 1\nhij \u2212 \u3008M,Xij\u3009 , for all 0 < \u03bbij < C. (45)\nNote that it is numerically wiser to take the average over all such training data points to compute b. After b is computed, we can compute \u03beij by\n\u03beij =\n{\n0, for all \u03bbij < C [1\u2212 hij (\u3008M,Xij\u3009+ b)]+, for all \u03bbij = C, (46)\nwhere the term [z]+ = max (z, 0) denotes the standard hinge loss."}, {"heading": "APPENDIX B THE DUAL OF NCML", "text": "The original problem of NCML is as follows:\nmin \u03b1,b,\u03be\n1\n2\n\u2211\ni,j\n\u2211\nk,l \u03b1ij\u03b1kl \u3008Xij ,Xkl\u3009+ C\n\u2211\ni,j \u03beij\ns.t. hij ( \u2211\nk,l \u03b1kl \u3008Xij ,Xkl\u3009+ b\n)\n\u2265 1\u2212 \u03beij\n\u03beij \u2265 0, \u03b1ij \u2265 0, \u2200i, j.\n(47)\nIts Lagrangian can be defined as:\nL (\u03b2,\u03c3,\u03bd,\u03b1, b, \u03be) = 1\n2\n\u2211\ni,j,k,l \u03b1ij\u03b1kl \u3008Xij ,Xkl\u3009+ C\n\u2211\ni,j \u03beij\n\u2212 \u2211\ni,j \u03b2ij\n[\nhij\n(\n\u2211\nkl \u03b1kl \u3008Xij ,Xkl\u3009+ b\n)\n\u2212 1 + \u03beij\n]\n\u2212 \u2211\ni,j \u03bdij\u03beij \u2212\n\u2211\ni,j \u03c3ij\u03b1ij ,\n(48) where \u03b2, \u03c3 and \u03bd are the Lagrange multipliers which satisfy \u03b2ij \u2265 0, \u03c3ij \u2265 0 and \u03bdij \u2265 0, \u2200i, j. Converting the original problem to its dual problem needs the following KKT conditions:\n\u2202L (\u03b2,\u03c3,\u03bd,\u03b1, b, \u03be)\n\u2202\u03b1ij = 0 \u21d2\n\u2211\nk,l \u03b1kl \u3008Xij ,Xkl\u3009 \u2212\n\u2211\nk,l \u03b2klhkl \u3008Xij ,Xkl\u3009 \u2212 \u03c3ij = 0,\n(49) \u2202L (\u03b2,\u03c3,\u03bd,\u03b1, b, \u03be)\n\u2202b = 0 \u21d2\n\u2211\ni,j \u03b2ijhij = 0, (50)\n\u2202L (\u03b2,\u03c3,\u03bd,\u03b1, b, \u03be)\n\u2202\u03beij = 0 \u21d2 C \u2212 \u03b2ij \u2212 \u03bdij = 0 \u21d2\n0 \u2264 \u03b2ij \u2264 C,\n(51)\nhij\n(\n\u2211\nk,l \u03b1kl \u3008Xij ,Xkl\u3009+ b\n)\n\u2212 1 + \u03beij \u2265 0,\n\u03beij \u2265 0, \u03b1ij \u2265 0, \u2200i, j, (52)\n\u03b2ij \u2265 0, \u03c3ij \u2265 0, \u03bdij \u2265 0, \u2200i, j, (53)\n\u03b2ij\n[\nhij\n(\n\u2211\nk,l \u03b1kl \u3008Xij ,Xkl\u3009+ b\n)\n\u2212 1 + \u03beij\n]\n= 0,\n\u03bdij\u03beij = 0, \u03c3ij\u03b1ij = 0, \u2200i, j. (54)\nHere we introduce a coefficient vector \u03b7, which satisfies \u03c3ij = \u2211\nk,l \u03b7kl \u3008Xij ,Xkl\u3009. Note that \u3008Xij ,Xkl\u3009 is a positive definite kernel. So we can guarantee that every \u03b7\n13\ncorresponds to a unique \u03c3, and vice versa. Equation (49) implies the following relationship between \u03b1, \u03b2 and \u03b7:\n\u03b1ij = \u03b2ijhij + \u03b7ij , \u2200i, j. (55)\nSubstituting (49)\u223c(51) back into the Lagrangian, we get the Lagrange dual problem of NCML as follows:\nmax \u03b7,\u03b2\n\u2212 1\n2\n\u2211\ni,j,k,l\n(\u03b2ijhij + \u03b7ij) (\u03b2klhkl + \u03b7kl) \u3008Xij ,Xkl\u3009\n+ \u2211\ni,j\n\u03b2ij\ns.t. \u2211\nk,l \u03b7kl \u3008Xij ,Xkl\u3009 \u2265 0, 0 \u2264 \u03b2ij \u2264 C, \u2200i, j\n\u2211\ni,j \u03b2ijhij = 0.\n(56) Analogous to PCML, we can use the KKT complementarity condition in (50) to compute b and \u03beij in NCML. Equations (51) and (54) show that \u03beij = 0 if \u03b2ij < C, and hij ( \u2211\nkl \u03b1kl \u3008Xij ,Xkl\u3009+ b) \u2212 1 + \u03beij = 0 if \u03b2ij > 0. Thus we can simply take any training data point, for which 0 < \u03b2ij < C, to compute b by\nb = 1 hij \u2212 \u2211 k,l \u03b1kl \u3008Xij ,Xkl\u3009. (57)\nAfter obtain b, we can compute \u03b2ij by\n\u03beij =\n\n\n\n0, \u2200 \u03b2ij < C [\n1\u2212 hij\n(\n\u2211\nk,l \u03b1kl \u3008Xij ,Xkl\u3009+ b\n)]\n+ , \u2200 \u03b2ij = C,\n(58) where the term [z]+ = max (z, 0) denotes the standard hinge loss."}, {"heading": "APPENDIX C THE DUAL OF THE SUBPROBLEM ON \u03b7 IN", "text": "NCML The subproblem on \u03b7 is formulated as follows:\nmin \u03b7\n1\n2\n\u2211\ni,j\n\u2211\nk,l \u03b7ij\u03b7kl \u3008Xij ,Xkl\u3009+\n\u2211\ni,j \u03b7ij\u03b3ij\ns.t. \u2211\nk,l \u03b7kl \u3008Xij ,Xkl\u3009 \u2265 0, \u2200i, j,\n(59)\nwhere \u03b3ij = \u2211 k,l \u03b2klhkl \u3008Xij ,Xkl\u3009. Its Lagrangian is:\nL (\u00b5,\u03b7) = 1\n2\n\u2211\ni,j\n\u2211\nk,l \u03b7ij\u03b7kl \u3008Xij ,Xkl\u3009\n+ \u2211\ni,j \u03b7ij\u03b3ij \u2212\n\u2211\ni,j \u00b5ij\n\u2211\nk,l \u03b7kl \u3008Xij ,Xkl\u3009,\n(60)\nwhere \u00b5 is the Lagrange multiplier which satisfies \u00b5ij \u2265 0, \u2200i, j. Converting the original problem to its dual problem needs the following KKT condition:\n\u2202L (\u00b5,\u03b7)\n\u2202\u03b7ij = 0 \u21d2\n\u2211\nk,l \u03b7kl \u3008Xij ,Xkl\u3009+ \u03b3ij \u2212\n\u2211\nk,l \u00b5kl \u3008Xij ,Xkl\u3009 = 0.\n(61)\nEquation (61) implies the following relationship between \u00b5, \u03b7 and \u03b2:\n\u03b7ij = \u00b5ij \u2212 hij\u03b2ij , \u2200i, j. (62)\nSubstituting (61) and (62) back into the Lagrangian, we get the following Lagrange dual problem of the subproblem on \u03b7:\nmax \u00b5\n\u2212 1\n2\n\u2211\ni,j\n\u2211\nk,l \u00b5ij\u00b5kl \u3008Xij ,Xkl\u3009+\n\u2211\ni,j \u03b3ij\u00b5ij\n\u2212 1\n2\n\u2211\ni,j\n\u2211\nk,l \u03b2ij\u03b2klhijhkl \u3008Xij ,Xkl\u3009\ns.t. \u00b5ij \u2265 0, \u2200i, j. (63) Since \u03b2 is fixed in this subproblem, \u2211\ni,j\n\u2211\nk,l \u03b2ij\u03b2klhijhkl \u3008Xij ,Xkl\u3009 remains constant in (63). Thus we can omit this term and have the following simplified Lagrange dual problem:\nmax \u00b5\n\u2212 1\n2\n\u2211\ni,j\n\u2211\nk,l \u00b5ij\u00b5kl \u3008Xij ,Xkl\u3009+\n\u2211\ni,j \u03b3ij\u00b5ij\ns.t. \u00b5ij \u2265 0, \u2200i, j. (64)"}], "references": [{"title": "A survey on metric learning for feature vectors and structured data", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": "arXiv:1306.6709, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "J. Mach. Learn. Res., vol. 10, pp. 207\u2013244, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S. Russell"], "venue": "Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2002, pp. 505\u2013512.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Metric learning to rank", "author": ["B. McFee", "G. Lanckriet"], "venue": "Proc. 27th Int. Conf. Mach. Learn. (ICML 2010), 2010, pp. 775\u2013782.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust structural metric learning", "author": ["D. Lim", "B. McFee", "G.R. Lanckriet"], "venue": "Proc. 30th Int. Conf. Mach. Learn. (ICML 2013), 2013, pp. 615\u2013623.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning semi-riemannian metrics for semisupervised feature extraction", "author": ["W. Zhang", "Z. Lin", "X. Tang"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 23, no. 4, pp. 600\u2013611, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Distance metric learning for kernel machines", "author": ["Z. Xu", "K.Q. Weinberger", "O. Chapelle"], "venue": "arXiv:1208.3422, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Is that you? metric learning approaches for face identification", "author": ["M. Guillaumin", "J. Verbeek", "C. Schmid"], "venue": "Proc. IEEE 12th Int. Conf. Comput. Vis. (ICCV 2009), 2009, pp. 498\u2013505.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Large scale metric learning from equivalence constraints", "author": ["M. Kostinger", "M. Hirzer", "P. Wohlhart", "P. Roth", "H. Bischof"], "venue": "Proc. 2012 IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR 2012), 2012, pp. 2288\u20132295.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning locally-adaptive decision functions for person verification", "author": ["Z. Li", "S. Chang", "F. Liang", "T.S. Huang", "L. Cao", "J.R. Smith"], "venue": "Proc. 16th IEEE Int. Conf. Comput. Vis. (ICCV 2013), 2013, pp. 3610\u20133617.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Semi-supervised distance metric learning for collaborative image retrieval", "author": ["S. Hoi", "W. Liu", "S. Chang"], "venue": "Proc. 2008 IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR 2008), 2008, pp. 1\u20137.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "A boosting framework for visuality-preserving distance metric learning and its application to medical image retrieval", "author": ["L. Yang", "R. Jin", "L. Mummert", "R. Sukthankar", "A. Goode", "B. Zheng", "S.C.H. Hoi", "M. Satyanarayanan"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 32, no. 1, pp. 30\u201344, Jan. 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Human activity recognition with metric learning", "author": ["D. Tran", "A. Sorokin"], "venue": "Proc. 10th Eur. Conf. Comput. Vis. (ECCV 2008), 2008, pp. 548\u2013561.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Metric learning for text documents", "author": ["G. Lebanon"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 28, no. 4, pp. 497\u2013 508, Apr. 2006.  14", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning a distance metric from a network", "author": ["B. Shaw", "B. Huang", "T. Jebara"], "venue": "Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2011, pp. 1899\u20131907.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": "Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2004, pp. 513\u2013520.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Large margin component analysis", "author": ["L. Torresani", "K. Lee"], "venue": "Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2006, pp. 1385\u20131392.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Neighborhood repulsed metric learning for kinship verification", "author": ["J. Lu", "X. Zhou", "Y. Tan", "Y. Shang", "J. Zhou"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 36, no. 2, pp. 331\u2013345, Feb. 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2005, pp. 1473\u20131480.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Metric learning by collapsing classes", "author": ["A. Globerson", "S. Roweis"], "venue": "Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2005, pp. 451\u2013458.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Positive semidefinite metric learning using boosting-like algorithms", "author": ["C. Shen", "J. Kim", "L. Wang", "A. Hengel"], "venue": "J. Mach. Learn. Res., vol. 13, pp. 1007\u20131036, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Distance metric learning with eigenvalue optimization", "author": ["Y. Ying", "P. Li"], "venue": "J. Mach. Learn. Res., vol. 13, pp. 1\u201326, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Informationtheoretic metric learning", "author": ["J. Davis", "B. Kulis", "P. Jain", "S. Sra", "I. Dhillon"], "venue": "Proc. 24th Int. Conf. Mach. Learn. (ICML 2007), 2007, pp. 209\u2013216.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Metric learning for large scale image classification: generalizing to new classes at near-zero cost", "author": ["T. Mensink", "J. Verbeek", "F. Perronnin", "G. Csurka"], "venue": "Proc. 2012 Eur. Conf. Comput. Vis. (ECCV 2012), 2012, pp. 488\u2013501.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Large scale online learning of image similarity through ranking", "author": ["G. Checkik", "V. Sharma", "U. Shalit", "S. Bengio"], "venue": "J. Mach. Learn. Res., vol. 11, pp. 1109\u20131135, 2010.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "J. Mach. Learn. Res., vol. 7, pp. 2399\u20132434, 2006.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Support vector machines for multiple-instance learning", "author": ["S. Andrews", "I. Tsochantaridis", "T. Hofmann"], "venue": "Proc. Adv. Neural Inf. Process. Syst., 2002, pp. 561\u2013568.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "Regularized multi-task learning", "author": ["T. Evgeniou", "F. France", "M. Pontil"], "venue": "Proc. 10th ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining, 2004, pp. 109\u2013117.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Kernel discriminant analysis for positive definite and indefinite kernels", "author": ["E. Pekalska", "B. Haasdonk"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 31, no. 6, pp. 1017\u20131032, Jun. 2009.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Semi-supervised kernel mean shift clustering", "author": ["S. Anand", "S. Mittal", "O. Tuzel", "P. Meer"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 36, no. 6, pp. 1201\u20131215, Jun. 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient classification for additive kernel SVMs", "author": ["S. Maji", "A.C. Berg", "J. Malik"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 35, no. 1, pp. 66\u201377, Jan. 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "The nature of statistical learning theory", "author": ["V. Vapnik"], "venue": "New York: Springer,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1995}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.C. Chang", "C.J. Lin"], "venue": "ACM Trans. Intell. Syst. Technol., vol. 2, pp. 1\u201327, 2011.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast training of support vector machines using sequential minimal optimization", "author": ["J.C. Platt"], "venue": "Advances in Kernel Methods: Support Vector Learning. Cambridge, MA: MIT Press, 1999, pp. 185\u2013208.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1999}, {"title": "Core vector machines: Fast SVM training on very large data sets", "author": ["I.W. Tsang", "J.T. Kwok", "P.M. Cheung"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 363\u2013392, 2005.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2005}, {"title": "Solving multiclass support vector machines with larank", "author": ["A. Bordes", "L. Bottou", "P. Gallinari", "J. Weston"], "venue": "Proc. 24th Int. Conf. Mach. Learn. (ICML 2007), 2007, pp. 89\u201396.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "A scalable modular convex solver for regularized risk minimization", "author": ["C.H. Teo", "Q. Le", "A. Smola", "S.V.N. Vishwanathan"], "venue": "Proc. 13th ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining, 2007, pp. 727\u2013736.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2007}, {"title": "Pegasos: primal estimated sub-gradient solver for SVM", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical Programming, vol. 127, no. 1, pp. 3\u201330, 2011.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Parametric local metric learning for nearest neighbor classification", "author": ["J. Wang", "A. Woznica", "A. Kalousis"], "venue": "Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2012, pp. 1610\u20131618.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Informationtheoretic semi-supervised metric learning via entropy regularization", "author": ["G. Niu", "B. Dai", "M. Yamada", "M. Sugiyama"], "venue": "Proc. 29th Int. Conf. Mach. Learn. (ICML 2012), 2012, pp. 89\u201396.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast solvers and efficient implementations for distance metric learning", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "Proc. 25th Int. Conf. Mach. Learn. (ICML 2008), 2008, pp. 1160\u20131167.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2008}, {"title": "Regularized distance metric learning: Theory and algorithm", "author": ["R. Jin", "S. Wang", "Y. Zhou"], "venue": "Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2009, pp. 862\u2013870.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2009}, {"title": "An efficient sparse metric learning in high-dimensional space via l1penalized log-determinant regularization", "author": ["G.-J. Qi", "J. Tang", "Z.-J. Zha", "T.-S. Chua", "H.-J. Zhang"], "venue": "Proc. 26th Int. Conf. Mach. Learn., 2009, pp. 841\u2013848.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparse metric learning via smooth optimization", "author": ["Y. Ying", "K. Huang", "C. Campbell"], "venue": "Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2009, pp. 2214\u20132222.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2009}, {"title": "Positive semidefinite metric learning with boosting", "author": ["C. Shen", "J. Kim", "L. Wang", "A. Hengel"], "venue": "Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2009, pp. 1651\u20131659.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2009}, {"title": "Adaboost on low-rank psd matrices for metric learning", "author": ["J. Bi", "D. Wu", "L. Lu", "M. Liu", "Y. Tao", "M. Wolf"], "venue": "Proc. 2011 IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR 2011), 2011, pp. 2617\u20132624.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "A scalable dual approach to semidefinite metric learning", "author": ["C. Shen", "J. Kim", "L. Wang"], "venue": "Proc. 2011 IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR 2011), 2011, pp. 2601\u20132608.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient dual approach to distance metric learning", "author": ["C. Shen", "J. Kim", "F. Liu", "L. Wang", "A. Hengel"], "venue": "IEEE Trans. Neural Netw. Learn. Syst., vol. 25, no. 2, pp. 394\u2013406, 2014.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "A robust and efficient doubly regularized metric learning approach", "author": ["M. Liu", "B.C. Vemuri"], "venue": "Proc. 2012 Eur. Conf. Comput. Vis. (ECCV 2012), 2012, pp. 646\u2013659.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}, {"title": "Metric learning: A support vector approach", "author": ["N. Nguyen", "Y. Guo"], "venue": "Proc. ECML/PKDD, 2008, pp. 125\u2013136.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2008}, {"title": "Pairwise support vector machines and their applications to large scale problems", "author": ["C. Brunner", "A. Fischer", "K. Luig", "T. Thies"], "venue": "J. Mach. Learn. Res., vol. 13, pp. 2279\u20132292, 2012.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2012}, {"title": "A metric learning perspective of SVM: on the relation of SVM and LMNN", "author": ["H. Do", "A. Kalousis", "J. Wang", "A. Woznica"], "venue": "arXiv:1201.4714, 2012.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2012}, {"title": "A kernel classification framework for metric learning", "author": ["F. Wang", "W. Zuo", "L. Zhang", "D. Meng", "D. Zhang"], "venue": "to be appear in IEEE Trans. Neural Netw. Learn. Syst.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 0}, {"title": "Information geometry and alternating minimization procedures", "author": ["I. Csiszar", "G. Tusnady"], "venue": "Statistics and decisions, Supplement Issue, vol. 1, pp. 205\u2013237, 1984.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1984}, {"title": "Convergence theorems for generalized alternating minimization procedures", "author": ["A. Gunawardana", "W. Byrne"], "venue": "J. Mach. Learn. Res., vol. 6, pp. 2049\u20132073, 2005.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2005}, {"title": "Generalized sparse metric learning with relative comparisons", "author": ["K. Huang", "Y. Ying", "C. Campbell"], "venue": "Knowledge and Inf. Syst., vol. 28, no. 1, pp. 25\u201345, 2011.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2011}, {"title": "UCI machine learning repository [http://archive.ics.uci.edu/ml", "author": ["A. Frank", "A. Asuncion"], "venue": "2010.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2010}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["J. Demsar"], "venue": "J. Mach. Learn. Res., vol. 7, pp. 1\u201330, 2006.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2006}, {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "author": ["G.B. Huang", "M. Ramesh", "T. Berg", "E. Learned-Miller"], "venue": "Univ. of Massachusetts, Tech. Rep., 2007.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2007}, {"title": "Attribute and simile classifiers for face verification", "author": ["N. Kumar", "A.C. Berg", "P.N. Belhumeur", "S.K. Nayar"], "venue": "Proc. 2009 IEEE Int. Conf. Comput. Vis. (ICCV 2009), 2009, pp. 365\u2013372.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2009}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "Int. J. Comput. Vis., vol. 60, no. 2, pp. 91\u2013110, 2004.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2004}, {"title": "Viewpoint invariant pedestrian recognition with an ensemble of localized features", "author": ["D. Gray", "H. Tao"], "venue": "Proc. 2008 Eur. Conf. Comput. Vis. (ECCV 2008), 2008, pp. 262\u2013275.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2008}, {"title": "Custom pictorial structures for re-identification", "author": ["D. Cheng", "M. Cristani", "M. Stoppa", "L. Bazzani", "V. Murino"], "venue": "Proc. British Machine Vision Conf., 2011.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2011}, {"title": "Hierarchical gaussianization for image classification", "author": ["X. Zhou", "N. Cui", "Z. Li", "F. Liang", "T. Huang"], "venue": "Proc. 12th IEEE Int. Conf. Comput. Vis. (ICCV 2009), 2009, pp. 1971\u20131977.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "1 INTRODUCTION D ISTANCE metric learning aims to train a valid distance metric which can enlarge the distances between samples of different classes and reduce the distances between samples of the same class [1].", "startOffset": 207, "endOffset": 210}, {"referenceID": 1, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 95, "endOffset": 98}, {"referenceID": 3, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 108, "endOffset": 111}, {"referenceID": 4, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 113, "endOffset": 116}, {"referenceID": 5, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 137, "endOffset": 140}, {"referenceID": 6, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 174, "endOffset": 177}, {"referenceID": 7, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 227, "endOffset": 230}, {"referenceID": 8, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 257, "endOffset": 260}, {"referenceID": 9, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 262, "endOffset": 266}, {"referenceID": 10, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 284, "endOffset": 288}, {"referenceID": 11, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 290, "endOffset": 294}, {"referenceID": 12, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 317, "endOffset": 321}, {"referenceID": 13, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 347, "endOffset": 351}, {"referenceID": 14, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 373, "endOffset": 377}, {"referenceID": 15, "context": "Methods that learn L, including neighborhood components analysis (NCA) [16], large margin components analysis (LMCA) [17] and neighborhood repulsed metric learning (NRML) [18], are mostly formulated as nonconvex optimization problems, which are solved by gradient descent based optimizers.", "startOffset": 71, "endOffset": 75}, {"referenceID": 16, "context": "Methods that learn L, including neighborhood components analysis (NCA) [16], large margin components analysis (LMCA) [17] and neighborhood repulsed metric learning (NRML) [18], are mostly formulated as nonconvex optimization problems, which are solved by gradient descent based optimizers.", "startOffset": 117, "endOffset": 121}, {"referenceID": 17, "context": "Methods that learn L, including neighborhood components analysis (NCA) [16], large margin components analysis (LMCA) [17] and neighborhood repulsed metric learning (NRML) [18], are mostly formulated as nonconvex optimization problems, which are solved by gradient descent based optimizers.", "startOffset": 171, "endOffset": 175}, {"referenceID": 18, "context": "Taking the PSD constraint into account, methods that learn M, including large margin nearest neighbor (LMNN) [19] and maximally collapsing metric learning (MCML) [20], are mostly formulated as convex semidefinite programming (SDP) problems, which can be optimized by standard SDP solvers [19], projected gradient [3], Boosting-like [21], or Frank-Wolfe [22] algorithms.", "startOffset": 109, "endOffset": 113}, {"referenceID": 19, "context": "Taking the PSD constraint into account, methods that learn M, including large margin nearest neighbor (LMNN) [19] and maximally collapsing metric learning (MCML) [20], are mostly formulated as convex semidefinite programming (SDP) problems, which can be optimized by standard SDP solvers [19], projected gradient [3], Boosting-like [21], or Frank-Wolfe [22] algorithms.", "startOffset": 162, "endOffset": 166}, {"referenceID": 18, "context": "Taking the PSD constraint into account, methods that learn M, including large margin nearest neighbor (LMNN) [19] and maximally collapsing metric learning (MCML) [20], are mostly formulated as convex semidefinite programming (SDP) problems, which can be optimized by standard SDP solvers [19], projected gradient [3], Boosting-like [21], or Frank-Wolfe [22] algorithms.", "startOffset": 288, "endOffset": 292}, {"referenceID": 2, "context": "Taking the PSD constraint into account, methods that learn M, including large margin nearest neighbor (LMNN) [19] and maximally collapsing metric learning (MCML) [20], are mostly formulated as convex semidefinite programming (SDP) problems, which can be optimized by standard SDP solvers [19], projected gradient [3], Boosting-like [21], or Frank-Wolfe [22] algorithms.", "startOffset": 313, "endOffset": 316}, {"referenceID": 20, "context": "Taking the PSD constraint into account, methods that learn M, including large margin nearest neighbor (LMNN) [19] and maximally collapsing metric learning (MCML) [20], are mostly formulated as convex semidefinite programming (SDP) problems, which can be optimized by standard SDP solvers [19], projected gradient [3], Boosting-like [21], or Frank-Wolfe [22] algorithms.", "startOffset": 332, "endOffset": 336}, {"referenceID": 21, "context": "Taking the PSD constraint into account, methods that learn M, including large margin nearest neighbor (LMNN) [19] and maximally collapsing metric learning (MCML) [20], are mostly formulated as convex semidefinite programming (SDP) problems, which can be optimized by standard SDP solvers [19], projected gradient [3], Boosting-like [21], or Frank-Wolfe [22] algorithms.", "startOffset": 353, "endOffset": 357}, {"referenceID": 22, "context": "[23] proposed an information-theoretic metric learning (ITML) model with an iterative Bregman projection algorithm, which does not need projections onto the PSD cone.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Besides, the use of online solvers for metric learning has been discussed in [9], [24], [25].", "startOffset": 77, "endOffset": 80}, {"referenceID": 23, "context": "Besides, the use of online solvers for metric learning has been discussed in [9], [24], [25].", "startOffset": 82, "endOffset": 86}, {"referenceID": 24, "context": "Besides, the use of online solvers for metric learning has been discussed in [9], [24], [25].", "startOffset": 88, "endOffset": 92}, {"referenceID": 25, "context": "On the other hand, kernel methods [26]\u2013[31] have been widely studied in many learning tasks, e.", "startOffset": 34, "endOffset": 38}, {"referenceID": 30, "context": "On the other hand, kernel methods [26]\u2013[31] have been widely studied in many learning tasks, e.", "startOffset": 39, "endOffset": 43}, {"referenceID": 31, "context": "There are many open resources on kernel classification methods, and a variety of toolboxes and libraries have been released [32]\u2013[38].", "startOffset": 124, "endOffset": 128}, {"referenceID": 37, "context": "There are many open resources on kernel classification methods, and a variety of toolboxes and libraries have been released [32]\u2013[38].", "startOffset": 129, "endOffset": 133}, {"referenceID": 16, "context": "Abbreviation Full Name PSD Positive semidefinite (matrix) SDP Semidefinite programming k-NN k-nearest neighbor (classification) KKT Karush-Kuhn-Tucker (condition) SVM Support vector machine LMCA [17] Large margin components analysis LMNN [2] Large margin nearest neighbor NCA [16] Neighborhood components analysis MCML [20] Maximally collapsing metric learning ITML [23] Information-theoretic metric learning LDML [8] Logistic discriminant metric learning DML-eig [22] Distance metric learning with eigenvalue optimization PLML [39] Parametric local metric learning KISSME [9] Keep it simple and straightforward metric learning PCML Positive-semidefinite constrained metric learning NCML Nonnegative-coefficient constrained metric learning", "startOffset": 195, "endOffset": 199}, {"referenceID": 1, "context": "Abbreviation Full Name PSD Positive semidefinite (matrix) SDP Semidefinite programming k-NN k-nearest neighbor (classification) KKT Karush-Kuhn-Tucker (condition) SVM Support vector machine LMCA [17] Large margin components analysis LMNN [2] Large margin nearest neighbor NCA [16] Neighborhood components analysis MCML [20] Maximally collapsing metric learning ITML [23] Information-theoretic metric learning LDML [8] Logistic discriminant metric learning DML-eig [22] Distance metric learning with eigenvalue optimization PLML [39] Parametric local metric learning KISSME [9] Keep it simple and straightforward metric learning PCML Positive-semidefinite constrained metric learning NCML Nonnegative-coefficient constrained metric learning", "startOffset": 238, "endOffset": 241}, {"referenceID": 15, "context": "Abbreviation Full Name PSD Positive semidefinite (matrix) SDP Semidefinite programming k-NN k-nearest neighbor (classification) KKT Karush-Kuhn-Tucker (condition) SVM Support vector machine LMCA [17] Large margin components analysis LMNN [2] Large margin nearest neighbor NCA [16] Neighborhood components analysis MCML [20] Maximally collapsing metric learning ITML [23] Information-theoretic metric learning LDML [8] Logistic discriminant metric learning DML-eig [22] Distance metric learning with eigenvalue optimization PLML [39] Parametric local metric learning KISSME [9] Keep it simple and straightforward metric learning PCML Positive-semidefinite constrained metric learning NCML Nonnegative-coefficient constrained metric learning", "startOffset": 276, "endOffset": 280}, {"referenceID": 19, "context": "Abbreviation Full Name PSD Positive semidefinite (matrix) SDP Semidefinite programming k-NN k-nearest neighbor (classification) KKT Karush-Kuhn-Tucker (condition) SVM Support vector machine LMCA [17] Large margin components analysis LMNN [2] Large margin nearest neighbor NCA [16] Neighborhood components analysis MCML [20] Maximally collapsing metric learning ITML [23] Information-theoretic metric learning LDML [8] Logistic discriminant metric learning DML-eig [22] Distance metric learning with eigenvalue optimization PLML [39] Parametric local metric learning KISSME [9] Keep it simple and straightforward metric learning PCML Positive-semidefinite constrained metric learning NCML Nonnegative-coefficient constrained metric learning", "startOffset": 319, "endOffset": 323}, {"referenceID": 22, "context": "Abbreviation Full Name PSD Positive semidefinite (matrix) SDP Semidefinite programming k-NN k-nearest neighbor (classification) KKT Karush-Kuhn-Tucker (condition) SVM Support vector machine LMCA [17] Large margin components analysis LMNN [2] Large margin nearest neighbor NCA [16] Neighborhood components analysis MCML [20] Maximally collapsing metric learning ITML [23] Information-theoretic metric learning LDML [8] Logistic discriminant metric learning DML-eig [22] Distance metric learning with eigenvalue optimization PLML [39] Parametric local metric learning KISSME [9] Keep it simple and straightforward metric learning PCML Positive-semidefinite constrained metric learning NCML Nonnegative-coefficient constrained metric learning", "startOffset": 366, "endOffset": 370}, {"referenceID": 7, "context": "Abbreviation Full Name PSD Positive semidefinite (matrix) SDP Semidefinite programming k-NN k-nearest neighbor (classification) KKT Karush-Kuhn-Tucker (condition) SVM Support vector machine LMCA [17] Large margin components analysis LMNN [2] Large margin nearest neighbor NCA [16] Neighborhood components analysis MCML [20] Maximally collapsing metric learning ITML [23] Information-theoretic metric learning LDML [8] Logistic discriminant metric learning DML-eig [22] Distance metric learning with eigenvalue optimization PLML [39] Parametric local metric learning KISSME [9] Keep it simple and straightforward metric learning PCML Positive-semidefinite constrained metric learning NCML Nonnegative-coefficient constrained metric learning", "startOffset": 414, "endOffset": 417}, {"referenceID": 21, "context": "Abbreviation Full Name PSD Positive semidefinite (matrix) SDP Semidefinite programming k-NN k-nearest neighbor (classification) KKT Karush-Kuhn-Tucker (condition) SVM Support vector machine LMCA [17] Large margin components analysis LMNN [2] Large margin nearest neighbor NCA [16] Neighborhood components analysis MCML [20] Maximally collapsing metric learning ITML [23] Information-theoretic metric learning LDML [8] Logistic discriminant metric learning DML-eig [22] Distance metric learning with eigenvalue optimization PLML [39] Parametric local metric learning KISSME [9] Keep it simple and straightforward metric learning PCML Positive-semidefinite constrained metric learning NCML Nonnegative-coefficient constrained metric learning", "startOffset": 464, "endOffset": 468}, {"referenceID": 38, "context": "Abbreviation Full Name PSD Positive semidefinite (matrix) SDP Semidefinite programming k-NN k-nearest neighbor (classification) KKT Karush-Kuhn-Tucker (condition) SVM Support vector machine LMCA [17] Large margin components analysis LMNN [2] Large margin nearest neighbor NCA [16] Neighborhood components analysis MCML [20] Maximally collapsing metric learning ITML [23] Information-theoretic metric learning LDML [8] Logistic discriminant metric learning DML-eig [22] Distance metric learning with eigenvalue optimization PLML [39] Parametric local metric learning KISSME [9] Keep it simple and straightforward metric learning PCML Positive-semidefinite constrained metric learning NCML Nonnegative-coefficient constrained metric learning", "startOffset": 528, "endOffset": 532}, {"referenceID": 8, "context": "Abbreviation Full Name PSD Positive semidefinite (matrix) SDP Semidefinite programming k-NN k-nearest neighbor (classification) KKT Karush-Kuhn-Tucker (condition) SVM Support vector machine LMCA [17] Large margin components analysis LMNN [2] Large margin nearest neighbor NCA [16] Neighborhood components analysis MCML [20] Maximally collapsing metric learning ITML [23] Information-theoretic metric learning LDML [8] Logistic discriminant metric learning DML-eig [22] Distance metric learning with eigenvalue optimization PLML [39] Parametric local metric learning KISSME [9] Keep it simple and straightforward metric learning PCML Positive-semidefinite constrained metric learning NCML Nonnegative-coefficient constrained metric learning", "startOffset": 573, "endOffset": 576}, {"referenceID": 32, "context": "The off-the-shelf SVM solvers such as LibSVM [33] can be employed to solve the metric learning problem.", "startOffset": 45, "endOffset": 49}, {"referenceID": 15, "context": "2 RELATED WORK Compared with nonconvex metric learning models [16], [17], [40], convex formulation of metric learning [2], [3], [20]\u2013[22] has drawn increasing attentions due to its desired properties such as global optimality.", "startOffset": 62, "endOffset": 66}, {"referenceID": 16, "context": "2 RELATED WORK Compared with nonconvex metric learning models [16], [17], [40], convex formulation of metric learning [2], [3], [20]\u2013[22] has drawn increasing attentions due to its desired properties such as global optimality.", "startOffset": 68, "endOffset": 72}, {"referenceID": 39, "context": "2 RELATED WORK Compared with nonconvex metric learning models [16], [17], [40], convex formulation of metric learning [2], [3], [20]\u2013[22] has drawn increasing attentions due to its desired properties such as global optimality.", "startOffset": 74, "endOffset": 78}, {"referenceID": 1, "context": "2 RELATED WORK Compared with nonconvex metric learning models [16], [17], [40], convex formulation of metric learning [2], [3], [20]\u2013[22] has drawn increasing attentions due to its desired properties such as global optimality.", "startOffset": 118, "endOffset": 121}, {"referenceID": 2, "context": "2 RELATED WORK Compared with nonconvex metric learning models [16], [17], [40], convex formulation of metric learning [2], [3], [20]\u2013[22] has drawn increasing attentions due to its desired properties such as global optimality.", "startOffset": 123, "endOffset": 126}, {"referenceID": 19, "context": "2 RELATED WORK Compared with nonconvex metric learning models [16], [17], [40], convex formulation of metric learning [2], [3], [20]\u2013[22] has drawn increasing attentions due to its desired properties such as global optimality.", "startOffset": 128, "endOffset": 132}, {"referenceID": 21, "context": "2 RELATED WORK Compared with nonconvex metric learning models [16], [17], [40], convex formulation of metric learning [2], [3], [20]\u2013[22] has drawn increasing attentions due to its desired properties such as global optimality.", "startOffset": 133, "endOffset": 137}, {"referenceID": 40, "context": "developed an efficient solver based on the sub-gradient descent and the active set techniques [41].", "startOffset": 94, "endOffset": 98}, {"referenceID": 22, "context": "[23] suggested an iterative Bregman projection algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Iterative projected gradient descent method [3], [42] has been widely employed for metric learning but it requires an eigenvalue decomposition in each iteration.", "startOffset": 44, "endOffset": 47}, {"referenceID": 41, "context": "Iterative projected gradient descent method [3], [42] has been widely employed for metric learning but it requires an eigenvalue decomposition in each iteration.", "startOffset": 49, "endOffset": 53}, {"referenceID": 42, "context": "Other algorithms such as block-coordinate descent [43], smooth optimization [44], and Frank-Wolfe [22] have also been studied for metric learning.", "startOffset": 50, "endOffset": 54}, {"referenceID": 43, "context": "Other algorithms such as block-coordinate descent [43], smooth optimization [44], and Frank-Wolfe [22] have also been studied for metric learning.", "startOffset": 76, "endOffset": 80}, {"referenceID": 21, "context": "Other algorithms such as block-coordinate descent [43], smooth optimization [44], and Frank-Wolfe [22] have also been studied for metric learning.", "startOffset": 98, "endOffset": 102}, {"referenceID": 20, "context": "derived the Lagrange dual of the exponential loss based metric learning model, and proposed a boostinglike approach, namely BoostMetric, where the matrix M is learned as a linear positive combination of rank-one matrices [21], [45].", "startOffset": 221, "endOffset": 225}, {"referenceID": 44, "context": "derived the Lagrange dual of the exponential loss based metric learning model, and proposed a boostinglike approach, namely BoostMetric, where the matrix M is learned as a linear positive combination of rank-one matrices [21], [45].", "startOffset": 227, "endOffset": 231}, {"referenceID": 45, "context": "MetricBoost [46] and FrobMetric [47], [48] were further proposed to improve the performance of BoostMetric.", "startOffset": 12, "endOffset": 16}, {"referenceID": 46, "context": "MetricBoost [46] and FrobMetric [47], [48] were further proposed to improve the performance of BoostMetric.", "startOffset": 32, "endOffset": 36}, {"referenceID": 47, "context": "MetricBoost [46] and FrobMetric [47], [48] were further proposed to improve the performance of BoostMetric.", "startOffset": 38, "endOffset": 42}, {"referenceID": 48, "context": "Liu and Vemuri incorporated two regularization terms in the duality for robust metric learning [49].", "startOffset": 95, "endOffset": 99}, {"referenceID": 20, "context": "Note that BoostMetric [21], [45], MetricBoost [46], and FrobMetric [47] are proposed for metric learning with triplet constraints, whereas in many applications such as verification, only pairwise constraints are available in the training stage.", "startOffset": 22, "endOffset": 26}, {"referenceID": 44, "context": "Note that BoostMetric [21], [45], MetricBoost [46], and FrobMetric [47] are proposed for metric learning with triplet constraints, whereas in many applications such as verification, only pairwise constraints are available in the training stage.", "startOffset": 28, "endOffset": 32}, {"referenceID": 45, "context": "Note that BoostMetric [21], [45], MetricBoost [46], and FrobMetric [47] are proposed for metric learning with triplet constraints, whereas in many applications such as verification, only pairwise constraints are available in the training stage.", "startOffset": 46, "endOffset": 50}, {"referenceID": 46, "context": "Note that BoostMetric [21], [45], MetricBoost [46], and FrobMetric [47] are proposed for metric learning with triplet constraints, whereas in many applications such as verification, only pairwise constraints are available in the training stage.", "startOffset": 67, "endOffset": 71}, {"referenceID": 49, "context": "Several SVM-based metric learning approaches [50]\u2013 [53] have also been proposed.", "startOffset": 45, "endOffset": 49}, {"referenceID": 52, "context": "Several SVM-based metric learning approaches [50]\u2013 [53] have also been proposed.", "startOffset": 51, "endOffset": 55}, {"referenceID": 49, "context": "Using SVM, Nguyen and Guo [50] formulated metric learning as a quadratic semidefinite programming problem, and suggested a projected gradient descent algorithm.", "startOffset": 26, "endOffset": 30}, {"referenceID": 49, "context": "The formulations of the proposed PCML and NCML in this work are different from the model in [50], and they are solved by the dual problems with the off-the-shelf SVM solvers.", "startOffset": 92, "endOffset": 96}, {"referenceID": 50, "context": "[51] proposed a pairwise SVM method to learn a dissimilarity function rather than a distance metric.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "Different from [51], the proposed PCML and NCML learn a distance metric and the matrix M is constrained to be a PSD matrix.", "startOffset": 15, "endOffset": 19}, {"referenceID": 51, "context": "[52] studied SVM from a metric learning perspective and presented an improved variant of SVM classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[53] developed a kernel classification framework for metric learning and proposed two learning models which can be efficiently implemented by the standard SVM solvers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "In this work, the proposed PCML and NCML models have different formulations from [53], and their solutions are globally optimal.", "startOffset": 81, "endOffset": 85}, {"referenceID": 32, "context": "Substituting (8) into (7), the subproblem on \u03bb becomes a kernel-based classification problem, and can be efficiently solved by using the existing SVM solvers such as LibSVM [33].", "startOffset": 173, "endOffset": 177}, {"referenceID": 53, "context": "3 Optimality Condition As shown in [54], [55], the general alternating minimization approach will converge.", "startOffset": 35, "endOffset": 39}, {"referenceID": 54, "context": "3 Optimality Condition As shown in [54], [55], the general alternating minimization approach will converge.", "startOffset": 41, "endOffset": 45}, {"referenceID": 1, "context": "Similar strategy for constructing pairwise or triplet constraints can be found in [2], [11].", "startOffset": 82, "endOffset": 85}, {"referenceID": 10, "context": "Similar strategy for constructing pairwise or triplet constraints can be found in [2], [11].", "startOffset": 87, "endOffset": 91}, {"referenceID": 33, "context": "The computational complexity of SMO-type algorithms [34] is O(kNd).", "startOffset": 52, "endOffset": 56}, {"referenceID": 32, "context": ", LibSVM [33].", "startOffset": 9, "endOffset": 13}, {"referenceID": 33, "context": "When the SMO-type algorithm [34] is adopted for SVM training, the computational complexity of NCML is O ( kNd )", "startOffset": 28, "endOffset": 32}, {"referenceID": 4, "context": "vector data has been recently receiving considerable research interests [5], [56], and NCML can provide a new perspective on this topic.", "startOffset": 72, "endOffset": 75}, {"referenceID": 55, "context": "vector data has been recently receiving considerable research interests [5], [56], and NCML can provide a new perspective on this topic.", "startOffset": 77, "endOffset": 81}, {"referenceID": 34, "context": ", core vector machines [35], LaRank [36], BMRM [37], and Pegasos [38], which can be utilized for large scale metric learning.", "startOffset": 23, "endOffset": 27}, {"referenceID": 35, "context": ", core vector machines [35], LaRank [36], BMRM [37], and Pegasos [38], which can be utilized for large scale metric learning.", "startOffset": 36, "endOffset": 40}, {"referenceID": 36, "context": ", core vector machines [35], LaRank [36], BMRM [37], and Pegasos [38], which can be utilized for large scale metric learning.", "startOffset": 47, "endOffset": 51}, {"referenceID": 37, "context": ", core vector machines [35], LaRank [36], BMRM [37], and Pegasos [38], which can be utilized for large scale metric learning.", "startOffset": 65, "endOffset": 69}, {"referenceID": 25, "context": "Moreover, we can refer to the progresses in kernel methods [26]\u2013[28] for developing semi-supervised, multiple instance, and multitask metric learning approaches.", "startOffset": 59, "endOffset": 63}, {"referenceID": 27, "context": "Moreover, we can refer to the progresses in kernel methods [26]\u2013[28] for developing semi-supervised, multiple instance, and multitask metric learning approaches.", "startOffset": 64, "endOffset": 68}, {"referenceID": 15, "context": "We compare PCML and NCML with the baseline Euclidean distance metric and 7 state-of-the-art metric learning models, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22].", "startOffset": 130, "endOffset": 134}, {"referenceID": 22, "context": "We compare PCML and NCML with the baseline Euclidean distance metric and 7 state-of-the-art metric learning models, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22].", "startOffset": 141, "endOffset": 145}, {"referenceID": 19, "context": "We compare PCML and NCML with the baseline Euclidean distance metric and 7 state-of-the-art metric learning models, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22].", "startOffset": 152, "endOffset": 156}, {"referenceID": 7, "context": "We compare PCML and NCML with the baseline Euclidean distance metric and 7 state-of-the-art metric learning models, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22].", "startOffset": 163, "endOffset": 166}, {"referenceID": 1, "context": "We compare PCML and NCML with the baseline Euclidean distance metric and 7 state-of-the-art metric learning models, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22].", "startOffset": 173, "endOffset": 176}, {"referenceID": 38, "context": "We compare PCML and NCML with the baseline Euclidean distance metric and 7 state-of-the-art metric learning models, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22].", "startOffset": 183, "endOffset": 187}, {"referenceID": 21, "context": "We compare PCML and NCML with the baseline Euclidean distance metric and 7 state-of-the-art metric learning models, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22].", "startOffset": 201, "endOffset": 205}, {"referenceID": 56, "context": "1 Results on the UCI Datasets We first use 9 datasets from the UCI Machine Learning Repository [57] to evaluate the proposed models.", "startOffset": 95, "endOffset": 99}, {"referenceID": 57, "context": "The average rank is defined as the mean rank of one method over the nine datasets, which can provide a fair comparison of the learning methods [58].", "startOffset": 143, "endOffset": 147}, {"referenceID": 58, "context": "3 Face Verification In this subsection, we evaluate the proposed methods for face verification using two challenging face databases: Labeled Faces in the Wild (LFW) [59] and Public Figures (PubFig) [60].", "startOffset": 165, "endOffset": 169}, {"referenceID": 59, "context": "3 Face Verification In this subsection, we evaluate the proposed methods for face verification using two challenging face databases: Labeled Faces in the Wild (LFW) [59] and Public Figures (PubFig) [60].", "startOffset": 198, "endOffset": 202}, {"referenceID": 60, "context": "In our experiments, we use the SIFT [61] features and the attribute features provided by [8] and [60] to evaluate the metric learning methods.", "startOffset": 36, "endOffset": 40}, {"referenceID": 7, "context": "In our experiments, we use the SIFT [61] features and the attribute features provided by [8] and [60] to evaluate the metric learning methods.", "startOffset": 89, "endOffset": 92}, {"referenceID": 59, "context": "In our experiments, we use the SIFT [61] features and the attribute features provided by [8] and [60] to evaluate the metric learning methods.", "startOffset": 97, "endOffset": 101}, {"referenceID": 21, "context": "55 DML-eig [22] 81.", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "79 ITML [9] 82.", "startOffset": 8, "endOffset": 11}, {"referenceID": 7, "context": "40 LDML [8] 79.", "startOffset": 8, "endOffset": 11}, {"referenceID": 8, "context": "08 KISSME [9] 80.", "startOffset": 10, "endOffset": 13}, {"referenceID": 21, "context": "We report the ROC curves of PCML, NCML, DML-eig [22], ITML [23], KISSME [9], LDML [8] and Euclidean distance in Fig.", "startOffset": 48, "endOffset": 52}, {"referenceID": 22, "context": "We report the ROC curves of PCML, NCML, DML-eig [22], ITML [23], KISSME [9], LDML [8] and Euclidean distance in Fig.", "startOffset": 59, "endOffset": 63}, {"referenceID": 8, "context": "We report the ROC curves of PCML, NCML, DML-eig [22], ITML [23], KISSME [9], LDML [8] and Euclidean distance in Fig.", "startOffset": 72, "endOffset": 75}, {"referenceID": 7, "context": "We report the ROC curves of PCML, NCML, DML-eig [22], ITML [23], KISSME [9], LDML [8] and Euclidean distance in Fig.", "startOffset": 82, "endOffset": 85}, {"referenceID": 59, "context": "2 The PubFig Database The PubFig database [60] contains 58,797 face images of 200 persons with large variations in pose, lighting, expression, scene, camera, imaging conditions and parameters, etc.", "startOffset": 42, "endOffset": 46}, {"referenceID": 7, "context": "7: The ROC curves of different metric learning methods on the LFW-funneled dataset under the image restricted setting [8], [9], [22].", "startOffset": 118, "endOffset": 121}, {"referenceID": 8, "context": "7: The ROC curves of different metric learning methods on the LFW-funneled dataset under the image restricted setting [8], [9], [22].", "startOffset": 123, "endOffset": 126}, {"referenceID": 21, "context": "7: The ROC curves of different metric learning methods on the LFW-funneled dataset under the image restricted setting [8], [9], [22].", "startOffset": 128, "endOffset": 132}, {"referenceID": 59, "context": "We use the attribute features provided by [60] to evaluate the competing methods.", "startOffset": 42, "endOffset": 46}, {"referenceID": 8, "context": "8 shows the ROC curves of PCML, NCML, KISSME [9], ITML [23], DMLeig [22], Attribute Classifiers [60] and the baseline Euclidean distance.", "startOffset": 45, "endOffset": 48}, {"referenceID": 22, "context": "8 shows the ROC curves of PCML, NCML, KISSME [9], ITML [23], DMLeig [22], Attribute Classifiers [60] and the baseline Euclidean distance.", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "8 shows the ROC curves of PCML, NCML, KISSME [9], ITML [23], DMLeig [22], Attribute Classifiers [60] and the baseline Euclidean distance.", "startOffset": 68, "endOffset": 72}, {"referenceID": 59, "context": "8 shows the ROC curves of PCML, NCML, KISSME [9], ITML [23], DMLeig [22], Attribute Classifiers [60] and the baseline Euclidean distance.", "startOffset": 96, "endOffset": 100}, {"referenceID": 8, "context": "38 KISSME [9] 77.", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "09 ITML [9] 69.", "startOffset": 8, "endOffset": 11}, {"referenceID": 59, "context": "50 Attribute Classifiers [60] 78.", "startOffset": 25, "endOffset": 29}, {"referenceID": 21, "context": "65 DML-eig [22] 77.", "startOffset": 11, "endOffset": 15}, {"referenceID": 61, "context": "databases, the Viewpoint Invariant Pedestrian Recognition (VIPeR) database [63] and the Context Aware Vision using Image-based Active Recognition for ReIdentification (CAVIAR4REID) database [64] are used to assess the performance of the proposed methods.", "startOffset": 75, "endOffset": 79}, {"referenceID": 62, "context": "databases, the Viewpoint Invariant Pedestrian Recognition (VIPeR) database [63] and the Context Aware Vision using Image-based Active Recognition for ReIdentification (CAVIAR4REID) database [64] are used to assess the performance of the proposed methods.", "startOffset": 190, "endOffset": 194}, {"referenceID": 8, "context": "05 KISSME [9] 19.", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "LMNN [9] 16.", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "ITML [9] 15.", "startOffset": 5, "endOffset": 8}, {"referenceID": 21, "context": "10 DML-eig [22] 8.", "startOffset": 11, "endOffset": 15}, {"referenceID": 62, "context": "2 The CAVIAR4REID Database CAVIAR4REID consists of 1,220 pedestrian images from 72 persons, where the images are extracted from the shopping center scenario of the CAVIAR database [64].", "startOffset": 180, "endOffset": 184}, {"referenceID": 63, "context": "Following [65] and [10], we use the hierarchical Gaussian (HG) features to evaluate the metric learning methods.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "Following [65] and [10], we use the hierarchical Gaussian (HG) features to evaluate the metric learning methods.", "startOffset": 19, "endOffset": 23}, {"referenceID": 9, "context": "According to the evaluation protocol in [10], we randomly select 36 persons and use their images for training, and use the rest images for testing.", "startOffset": 40, "endOffset": 44}, {"referenceID": 21, "context": "We report the CMC curves of PCML, NCML, DML-eig [22], KISSME [9], ITML [23], LMNN [19] and Euclidean TABLE 9: Person re-identification accuracies (%) and training time (s) on the CAVIAR4REID dataset.", "startOffset": 48, "endOffset": 52}, {"referenceID": 8, "context": "We report the CMC curves of PCML, NCML, DML-eig [22], KISSME [9], ITML [23], LMNN [19] and Euclidean TABLE 9: Person re-identification accuracies (%) and training time (s) on the CAVIAR4REID dataset.", "startOffset": 61, "endOffset": 64}, {"referenceID": 22, "context": "We report the CMC curves of PCML, NCML, DML-eig [22], KISSME [9], ITML [23], LMNN [19] and Euclidean TABLE 9: Person re-identification accuracies (%) and training time (s) on the CAVIAR4REID dataset.", "startOffset": 71, "endOffset": 75}, {"referenceID": 18, "context": "We report the CMC curves of PCML, NCML, DML-eig [22], KISSME [9], ITML [23], LMNN [19] and Euclidean TABLE 9: Person re-identification accuracies (%) and training time (s) on the CAVIAR4REID dataset.", "startOffset": 82, "endOffset": 86}, {"referenceID": 21, "context": "23 DML-eig [22] 30.", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "LMNN [9] 28.", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "ITML [9] 31.", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "18 KISSME [9] 29.", "startOffset": 10, "endOffset": 13}, {"referenceID": 15, "context": "Experimental results on nine UCI machine learning repository datasets and four handwritten digit datasets showed that, compared with the state-of-the-art metric learning methods, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22], the proposed PCML and NCML methods can not only achieve higher classification accuracy, but also are much faster in training.", "startOffset": 193, "endOffset": 197}, {"referenceID": 22, "context": "Experimental results on nine UCI machine learning repository datasets and four handwritten digit datasets showed that, compared with the state-of-the-art metric learning methods, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22], the proposed PCML and NCML methods can not only achieve higher classification accuracy, but also are much faster in training.", "startOffset": 204, "endOffset": 208}, {"referenceID": 19, "context": "Experimental results on nine UCI machine learning repository datasets and four handwritten digit datasets showed that, compared with the state-of-the-art metric learning methods, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22], the proposed PCML and NCML methods can not only achieve higher classification accuracy, but also are much faster in training.", "startOffset": 215, "endOffset": 219}, {"referenceID": 7, "context": "Experimental results on nine UCI machine learning repository datasets and four handwritten digit datasets showed that, compared with the state-of-the-art metric learning methods, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22], the proposed PCML and NCML methods can not only achieve higher classification accuracy, but also are much faster in training.", "startOffset": 226, "endOffset": 229}, {"referenceID": 1, "context": "Experimental results on nine UCI machine learning repository datasets and four handwritten digit datasets showed that, compared with the state-of-the-art metric learning methods, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22], the proposed PCML and NCML methods can not only achieve higher classification accuracy, but also are much faster in training.", "startOffset": 236, "endOffset": 239}, {"referenceID": 38, "context": "Experimental results on nine UCI machine learning repository datasets and four handwritten digit datasets showed that, compared with the state-of-the-art metric learning methods, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22], the proposed PCML and NCML methods can not only achieve higher classification accuracy, but also are much faster in training.", "startOffset": 246, "endOffset": 250}, {"referenceID": 21, "context": "Experimental results on nine UCI machine learning repository datasets and four handwritten digit datasets showed that, compared with the state-of-the-art metric learning methods, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22], the proposed PCML and NCML methods can not only achieve higher classification accuracy, but also are much faster in training.", "startOffset": 264, "endOffset": 268}], "year": 2015, "abstractText": "Distance metric learning aims to learn from the given training data a valid distance metric, with which the similarity between data samples can be more effectively evaluated for classification. Metric learning is often formulated as a convex or nonconvex optimization problem, while many existing metric learning algorithms become inefficient for large scale problems. In this paper, we formulate metric learning as a kernel classification problem, and solve it by iterated training of support vector machines (SVM). The new formulation is easy to implement, efficient in training, and tractable for large-scale problems. Two novel metric learning models, namely Positive-semidefinite Constrained Metric Learning (PCML) and Nonnegative-coefficient Constrained Metric Learning (NCML), are developed. Both PCML and NCML can guarantee the global optimality of their solutions. Experimental results on UCI dataset classification, handwritten digit recognition, face verification and person re-identification demonstrate that the proposed metric learning methods achieve higher classification accuracy than state-of-the-art methods and they are significantly more efficient in training.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}