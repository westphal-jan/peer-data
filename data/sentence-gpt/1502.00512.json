{"id": "1502.00512", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Feb-2015", "title": "Scaling Recurrent Neural Network Language Models", "abstract": "This paper investigates the scaling properties of Recurrent Neural Network Language Models (RNNLMs). We discuss how to train very large RNNs on GPUs and address the questions of how RNNLMs scale with respect to model size, training-set size, computational costs and memory. Our analysis shows that despite being more costly to train, RNNLMs obtain much lower perplexities on standard benchmarks than n-gram models. We train the largest known RNNs and present relative word error rates gains of 18% on an ASR task. We also present the new lowest perplexities on the recently released billion word language modelling benchmark, 1 BLEU point gain on machine translation and a 17% relative hit rate gain in word prediction. As this paper outlines, we propose that RNNLMs will outperform N-gram models in the current algorithm using a set of parameter values (N-gram models) to improve the computational performance of their data. Using two parameter values (N-gram models) we will further describe the scaling properties of Recurrent Neural Network Language Models (RNNLMs), to improve the computational performance of RNNLMs. In addition to the RNNLMs, we also use a set of parameters to help solve the issue of model size and a set of parameter values (N-gram models).\n\n\n\n\n\nThe next step is to understand how a high-performance RNNLMs perform in a simple RNNLM model. In order to gain the best performance in the RNNLMs, we will present an extensive set of parameters (N-gram models) to illustrate the scaling properties of RNNLMs and describe their scaling properties.\n\nIn this paper we look at the scaling properties of the RNNLMs, describing their scaling properties. In our research we will discuss how to train very large RNNs on GPUs and address the questions of how RNNLMs scale with respect to model size, training-set size, computational costs and memory. This is also important to help improve the computational performance of RNNLMs, to be sure.\nFigure 4\n\n\n\nFigure 4\n\n\n\n\nN-gram models, from a deep-learning algorithm to a neural network (RNNLMs).\nN-gram models, from a deep-learning algorithm to a neural network (RNNLMs), to an artificial language learning algorithm.\nN-gram models, from a deep-learning algorithm to a neural network", "histories": [["v1", "Mon, 2 Feb 2015 15:27:37 GMT  (19kb,D)", "http://arxiv.org/abs/1502.00512v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["will williams", "niranjani prasad", "david mrva", "tom ash", "tony robinson"], "accepted": false, "id": "1502.00512"}, "pdf": {"name": "1502.00512.pdf", "metadata": {"source": "CRF", "title": "SCALING RECURRENT NEURAL NETWORK LANGUAGE MODELS", "authors": ["Will Williams", "Niranjani Prasad", "David Mrva", "Tom Ash", "Tony Robinson"], "emails": ["willw@cantabResearch.com", "tonyr@cantabResearch.com"], "sections": [{"heading": null, "text": "Index Terms\u2014 recurrent neural network, language modelling, GPU, speech recognition, RNNLM"}, {"heading": "1. INTRODUCTION", "text": "Statistical language models form a crucial component of many applications such as automatic speech recognition (ASR), machine translation (MT) and prediction for mobile phone text input. One such class of models, Recurrent Neural Network Language Models (RNNLMs), provide a rich and powerful way to model sequential language data. Despite an initial flurry of interest in RNNs in the early \u201990s for acoustic modelling [1, 2, 3], the computational cost and memory overheads of training large RNNs proved prohibitive. Since the earliest days of large vocabulary speech recognition, n-gram language models have been the dominant paradigm. However, recent work by Mikolov [4] on RNNLMs has shown that modestly sized RNNLMs can now be trained and have been shown to be competitive with ngrams. Mikolov trained RNNLMs with 800 hidden state units; Google\u2019s language modelling benchmark [5] subsequently established baseline RNNLM results with 1024 hidden state units. Many of the most recent ASR evaluations involve RNNLMs, highlighting their widespread popularity.\nAlthough RNNs have replaced n-grams as state-of-the-art, the question remains whether these architectures will scale. Single CPU-based implementations have suffered from computational limitations and have been unable to scale to the large number of parameters now commonplace in the neural net literature. Concretely, we address the questions of how RNNLMs scale with respect to model size, training-set size, processing power and memory. Our primary performance metric here is\nThis paper summarises the results of SMART award \u201dLarge scale neural network language models\u201d which ran from March 2013 to August 2014.\nperplexity and therefore all discussion on performance will relate specifically to the ability to reduce perplexity. The results presented here show the largest reductions in perplexity reported so far over KN 5-grams."}, {"heading": "2. DATA SETS", "text": ""}, {"heading": "2.1. Training Data", "text": "The main training corpus we use throughout this paper is an internally collated and processed data set. All data predates Nov. 2013 and totals approximately 8bn words, comprising:\n1. [880m] Spoken News: Derived from transcripts of radio and news shows. 2. [1.7bn] Wikipedia: Copy of Wikipedia articles. 3. [6.1bn] Written News: Derived from a web crawl of\nmany popular news sites. We used a rolling buffer of minimum size of 140 characters split on sentences to deduplicate the corpora. We used multiple hash functions to store past occurrences efficiently and excluded a negligibly small proportion of text due to false positives. We then put the corpus through a typical ASR normalisation and tokenisation pipeline - expanding out numbers into digit strings, removing most punctuation, casting to lower case and inserting sentence markers."}, {"heading": "2.2. Test Data", "text": "Unless otherwise stated, all our testing was done on a standard test benchmark: TED test data IWSLT14.SLT.tst20101. We chose this out-of-domain test set to exclude any possibility of text overlap between training and test sets, and also because of the availability of a publicly available KALDI recipe2."}, {"heading": "2.3. Entropy Filtering", "text": "To filter our corpus we used cross-entropy difference scoring [6, 7]. Typically, sentences are used as the natural choice for segments in this style of filtering. However, for the composite corpus, we wanted to maintain between-sentence context for the benefit of the RNNLM training, and so did not want to filter our corpus on a sentence by sentence basis. We instead implemented a rolling-buffer solution: a cross-entropy difference score was calculated across rolling batches of 16 sentences. If a sentence was ever part of a rolling buffer with\n1http://hltshare.fbk.eu/IWSLT2014/IWSLT14.SLT. tst2010.en-fr.en.tgz\n2Kaldi Revision 4084, recipe: http://sourceforge.net/p/ kaldi/code/HEAD/tree/trunk/egs/tedlium/s5/\nar X\niv :1\n50 2.\n00 51\n2v 1\n[ cs\n.C L\n] 2\nF eb\n2 01\n5\ncross-entropy difference below the threshold, it was maintained in the output corpus. This led to output that was more likely to be drawn from consecutive sentences, maintaining betweensentence information. The entropy threshold was then set at empirically determined levels to produce filtered corpora of the desired size, i.e. 12 , 1 4 etc. of the original corpus size, as determined by word count. For typical in-domain data we used IWSLT13.ASR.train.en3."}, {"heading": "3. SCALING PROPERTIES OF N-GRAMS", "text": "n-gram language models have maintained their dominance in statistical language modelling largely due to their simplicity and ease of computation. Although n-grams can be modelled and queried comparatively fast, they exhibit diminishing gains when built on more data.\nn-grams can be computed on larger data sets more easily than RNNLMs but as the extrapolation in Figure 1 indicates, each order of magnitude increase in the training data above 1012 words gives a reduction in perplexity of less than 6%. The largest current text corpora such as Google n-grams4 and CommonCrawl [8] are about size 1012. The asymptote of an exponential curve fitted to Figure 1 is approximately perplexity 73; these values represent a hard limit on the performance of 5-grams on this test set. n-grams also scale very poorly with respect to memory. At 8bn words the KN 5-gram already takes up 362GB in ARPA format and 69GB in KenLM [9] binary trie format - already impractically large for current commercial ASR. In comparison to RNNLMs, n-grams take up massively more space for modest entropy improvements and therefore do not scale well with respect to data size.\nAdditionally, increasing the order of the n-gram model gives vanishingly small gains for anything above a 5-gram trained on\n3http://hltshare.fbk.eu/IWSLT2013/IWSLT13.ASR. train.en\n4http://googleresearch.blogspot.co.uk/2006/08/allour-n-gram-are-belong-to-you.html\ncurrently available corpora. The number of parameters of an ngram increases significantly with its order, making attempts to increase the order impractical."}, {"heading": "4. RNNLMS ON GPUS", "text": ""}, {"heading": "4.1. Background", "text": "Recent attempts to train very large networks (on the order of billions of parameters) have required many CPU cores to work in concert [5, 10]. High-end Graphics Processing Units (GPUs) represent a viable alternative, being both affordable and capable of extremely high computational throughput. GPUs have therefore been one of the key elements in the resurgence of neural networks over the last decade. RNNLMs are highly amenable to parallelisation on GPUs due to their predominant reliance on linear algebra operations (such as matrix multiplies) for which highly optimised GPU-based libraries are publicly available.\nIn contrast to n-gram models, which simply count the occurrences of particular combinations of words, RNNs learn a distributed hidden representation for each context from which a probability distribution over all words in a given vocabulary can be obtained.\nWe use a standard RNN architecture [11] but dispense with bias units to maximise efficiency on the GPU and because, in our experience, they do not provide any practical benefit. Recent work has improved our understanding of how to effectively train RNNs [12]. However, many of these improvements - such as optimisation and regularisation techniques - make large claims on resources. Very large RNNs with a large number of hidden state units, nstate, can only be trained efficiently on current generation GPU hardware if one simplifies both the architecture and training algorithm as far as possible. We believe our setup from 2013 in this paper gives better speedups than previously reported elsewhere [13, 14, 15, 16, 17]."}, {"heading": "4.2. Implementation", "text": "To achieve high throughput and utilisation on GPUs we train a standard RNN with stochastic gradient descent and rmsprop[18]. Where possible we use CUDA\u2019s highly optimised linear algebra library cuBLAS to make SGEMM calls. Where this wasn\u2019t possible we wrote and optimised our own custom CUDA kernels. We use floats everywhere; the precision which doubles offer is unnecessary to learn good representations. We pack our input data using a data offset scheme which indexes the input corpus at a number of different points, denoted noffset. This entails managing noffset by minibatch size different hidden states throughout training. Typically we use noffset 128, similar to [16]. Empirically, we found a minibatch size of \u223c 8-10 represents a good trade-off between memory usage and perplexity. It also allows us to marshal a large and very efficient matrix multiply to perform all the hidden state-to-state and gradient calculations in one cuBLAS SGEMM operation. We use a very large rmsprop smoothing constant (0.9995). For the input-tostate and state-to-output matrices we approximate the majority\nof the rmsprop values by averaging over nstate units such that each word has just one rmsprop value rather than n values. This almost halves the total memory usage. We train using Noise Contrastive Estimation [19]. When reporting perplexities we explicitly normalise the output distribution. When performing lattice rescoring we find empirically that the small amount of normalisation noise does not significantly change the accuracy, whilst considerably faster than standard class based modelling."}, {"heading": "4.3. Analysis", "text": "Figure 3 shows that as we scale nstate we observe a near linear reduction in log perplexity with log training words. Given that n-grams scale very poorly with respect to their order it is clear that, on a fixed-size data set, RNNs scale much better with model size. For network sizes over nstate 1024, our\nimplementations on an Nvidia GeForce GTX Titan give 100x speedups against the baseline single core Mikolov implementation5 on a 3.4GHz Intel i7 CPU. Despite much improved training times on the GPU, our larger RNNs take on the order of days to train rather than hours which n-grams require. However, more compute power will favour RNNs since larger nstate RNNs can leverage the extra computation to scale the model size with nstate - something which yields a much smaller performance gain for n-grams. With respect to scaling the training set size, Figure 2 shows that the nstate 2048 saturates (i.e. can not make good use of more data) on 4bn words when trained on a randomly chosen splits. We can, however, increase the model size (i.e. nstate) to mitigate this problem - an approach which, as already discussed, is impractical for n-grams. With an nstate 8192 RNN from Figure 3 we have already reduced perplexity to 57.5, which is below the 73 we believe to be the asymptotic minimum for a 5-gram on this task. It therefore seems unlikely that even a 5-gram with unlimited data and a further 15% reduction from entropy filtering could ever outperform an RNNLM. In addition, the nstate 8192 RNN corresponds to a 48% reduction over the 5-gram. Despite being so much better in terms of perplexity on the same amount of data, the RNN uses only 886M parameters - approximately half the number of the 5-gram. Moreover, we can match the perplexity of the 5-gram with an nstate 128 RNN which uses under 1% of the parameters. In summary, although training time is much larger for an RNN and we have to increase nstate when scaling the training set size, RNNs make much better use of the additional data than n-grams and use far fewer parameters to do so."}, {"heading": "5. RESULTS", "text": ""}, {"heading": "5.1. Language Modelling", "text": "We present our RNNLM results on the recently released billion word language modelling benchmark[5]. The vocabulary size for this task is very large (770K) - we therefore train the RNNs on a much smaller 64k vocabulary and interpolate them with a full size 5-gram to fill in rare word probabilities. RNN perplexities in Table 1 are interpolated with the KN 5-gram. The nstate 4096 RNN is larger in state size than those in [5] and\n5http://rnnlm.org/\nwe achieve a better perplexity (42.4) with that one single model than the combination of interpolated models in [5] whilst also using only 3% of the total parameters."}, {"heading": "5.2. ASR", "text": "We evaluated the RNNLMs by rescoring lattices on three different systems, using the IWSLT14.SLT.tst2010 data both with and without the supplied segmentation. The \u2018IWSLT\u2019 system uses the Kaldi TEDLIUM [20] recipe for acoustic models and language models built on [5]6. The nstate 2048 models overtrain on the small entropy filtered corpus. The \u2018en-US\u2019 system uses the same framework but with the RNNLMs from Figure 3 and internal acoustic models. The \u2018SM\u2019 system also uses the RNNLMs from Figure 3, but within the commercial service available at speechmatics.com. Lattices were rescored using a highly efficient internal lattice rescoring tool which operates in considerably less than real time. Over both the \u2018en-US\u2019 and \u2018SM\u2019 task we see an average reduction in WER of 18% relative to rescoring with the n-gram alone by using nstate 8192 RNNLMs."}, {"heading": "5.3. Machine Translation", "text": "In our MT experiments we rescored the WFST lattices from the WMT13 EN-RU system developed at Cambridge University Engineering Department [21]. The evaluation measure commonly used in statistical machine translation, BLEU score, was 32.34 with our baseline n-gram model and increased to 33.45 with an nstate 3520 RNNLM (bigger is better for BLEU scores). The improvement of 1 BLEU point over the baseline demonstrates the utility of RNNLMs in not just ASR but also statistical MT systems.\n6The 12.8% IWSLT result is a competitive baseline that complies with IWSLT rules, using freely redistributable sources and can be recreated from the Kaldi TEDLIUM s5 recipe and http://cantabResearch.com/ cantab-TEDLIUM.tar.bz2."}, {"heading": "5.4. Word prediction", "text": "Word prediction involves predicting the next word in a string, with performance determined by percentage of times the target word is in the top 3 words predicted (known as \u2018hit-rate\u2019). For this task we train on 100M words from the BNC corpus [22] using a vocabulary size of 10K and the RNN using a shortlist of 100 candidate words generated from a heavily pruned 2 MB n-gram. For our task of word prediction on mobile phones we have tight memory restrictions and therefore choose nstate 1024 as an appropriate RNN size. We compress the RNNs by inserting layers with 512 units above and below the hidden state and then train end-to-end. Additionally, we tie the input-state and state-output weights as their transpose and quantise all the weights. At 10 MB the RNN achieves a 17% relative hit-rate gain over the Katz-5 n-gram, proving the utility of RNNs in memory constrained settings."}, {"heading": "6. CONCLUSION", "text": "We have shown that large RNNLMs can be trained efficiently on GPUs by exploiting data parallelism and minimising the number of extra parameters required during training. Such RNNs reduce the perplexity on standard benchmarks by over 40% against 5-grams, whilst using a fraction of the parameters. We believe RNNs now offer a lower perplexity than 5-grams for any amount of training data. In addition, we showed that state of the art ASR systems can be trained with Kaldi and high-end GPUs by rescoring with an RNNLM. Despite being both compute and GPU memory bound, RNNLMs are comfortably ahead of n-grams at present. We believe that future developments in compute power and memory capacity will further favour them."}, {"heading": "7. REFERENCES", "text": "[1] Anthony J. Robinson, \u201cAn application of recurrent nets to phone probability estimation,\u201d Neural Networks, IEEE Transactions on, vol. 5, no. 2, pp. 298\u2013305, 1994.\n[2] Ronald J. Williams, \u201cComplexity of exact gradient computation algorithms for recurrent neural networks,\u201d Tech. Rep., 1989.\n[3] Mike Schuster and Kuldip K. Paliwal, \u201cBidirectional recurrent neural networks,\u201d Signal Processing, IEEE Transactions on, vol. 45, no. 11, pp. 2673\u20132681, 1997.\n[4] Toma\u0301s\u030c Mikolov, Statistical language models based on neural networks, Ph.D. thesis, Brno University of Technology, 2012.\n[5] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson, \u201cOne Billion Word Benchmark for measuring progress in statistical language modeling,\u201d .\n[6] Dietrich Klakow, \u201cSelecting articles from the language model training corpus,\u201d in Acoustics, Speech, and Signal Processing, 2000. ICASSP\u201900. Proceedings. 2000 IEEE International Conference on. IEEE, 2000, vol. 3, pp. 1695\u20131698.\n[7] Robert C. Moore and William Lewis, \u201cIntelligent selection of language model training data,\u201d in Proceedings of the ACL 2010 Conference Short Papers. Association for Computational Linguistics, 2010, pp. 220\u2013224.\n[8] Christian Buck, Kenneth Heafield, and Bas van Ooyen, \u201cN-gram counts and language models from the common crawl,\u201d LREC, 2014.\n[9] Kenneth Heafield, \u201cKenLM: Faster and smaller language model queries,\u201d in Proceedings of the Sixth Workshop on Statistical Machine Translation. Association for Computational Linguistics, 2011, pp. 187\u2013197.\n[10] Quoc V Le, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff Dean, and Andrew Y. Ng, \u201cBuilding high-level features using large scale unsupervised learning,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8595\u20138598.\n[11] Toma\u0301s\u030c Mikolov, Martin Karafia\u0301t, Luka\u0301s\u030c Burget, Jan C\u030cernocky\u0300, and Sanjeev Khudanpur, \u201cRecurrent neural network based language model,\u201d in Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH), 2010, pp. 1045\u2013 1048.\n[12] Yoshua Bengio, Nicolas Boulanger-Lewandowski, and Razvan Pascanu, \u201cAdvances in optimizing recurrent networks,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8624\u20138628.\n[13] Tomas Mikolov, Stefan Kombrink, Anoop Deoras, Lukar Burget, and J Cernocky, \u201cRnnlm-recurrent neural network language modeling toolkit,\u201d .\n[14] Ilya Sutskever, James Martens, and Geoffrey E Hinton, \u201cGenerating text with recurrent neural networks,\u201d in Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011, pp. 1017\u20131024.\n[15] Zhiheng Huang, Geoffrey Zweig, Michael Levit, Benoit Dumoulin, Barlas Oguz, and Shawn Chang, \u201cAccelerating recurrent neural network training via two stage classes and parallelization,\u201d in Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 326\u2013331.\n[16] Xie Chen, Yongqiang Wang, Xunying Liu, Mark JF Gales, and Philip C Woodland, \u201cEfficient GPU-based training of recurrent neural network language models using spliced sentence bunch,\u201d Proc. ISCA Interspeech, 2014.\n[17] Boxun Li, Erjin Zhou, Bo Huang, Jiayi Duan, Yu Wang, Ningyi Xu, Jiaxing Zhang, and Huazhong Yang, \u201cLarge scale recurrent neural network on gpu,\u201d in Neural Networks (IJCNN), 2014 International Joint Conference on. IEEE, 2014, pp. 4062\u20134069.\n[18] T. Tieleman and G. Hinton, \u201cLecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude,\u201d COURSERA: Neural Networks for Machine Learning, vol. 4, 2012.\n[19] Andriy Mnih and Koray Kavukcuoglu, \u201cLearning word embeddings efficiently with noise-contrastive estimation,\u201d in Advances in Neural Information Processing Systems, 2013, pp. 2265\u20132273.\n[20] Anthony Rousseau, Paul Dele\u0301glise, and Yannick Este\u0300ve, \u201cTED-LIUM: an Automatic Speech Recognition dedicated corpus,\u201d in LREC, 2012, pp. 125\u2013129.\n[21] J. Pino, A. Waite, T. Xiao, A. de Gispert, F. Flego, and W. Byrne, \u201cThe University of Cambridge Russian-English system at WMT13,\u201d in Proceedings of the Eighth Workshop on Statistical Machine Translation, 2013, pp. 198\u2013 203.\n[22] BNC Consortium et al., \u201cThe British National Corpus, version 3 (BNC XML Edition), distributed by Oxford University Computing Services on behalf of the BNC Consortium,\u201d 2007."}], "references": [{"title": "An application of recurrent nets to phone probability estimation", "author": ["Anthony J. Robinson"], "venue": "Neural Networks, IEEE Transactions on, vol. 5, no. 2, pp. 298\u2013305, 1994.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1994}, {"title": "Complexity of exact gradient computation algorithms for recurrent neural networks", "author": ["Ronald J. Williams"], "venue": "Tech. Rep., 1989.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1989}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K. Paliwal"], "venue": "Signal Processing, IEEE Transactions on, vol. 45, no. 11, pp. 2673\u20132681, 1997.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Statistical language models based on neural networks, Ph.D", "author": ["Tom\u00e1\u0161 Mikolov"], "venue": "thesis, Brno University of Technology,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "One Billion Word Benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": ".", "citeRegEx": "5", "shortCiteRegEx": null, "year": 0}, {"title": "Selecting articles from the language model training corpus", "author": ["Dietrich Klakow"], "venue": "Acoustics, Speech, and Signal Processing, 2000. ICASSP\u201900. Proceedings. 2000 IEEE International Conference on. IEEE, 2000, vol. 3, pp. 1695\u20131698.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Intelligent selection of language model training data", "author": ["Robert C. Moore", "William Lewis"], "venue": "Proceedings of the ACL 2010 Conference Short Papers. Association for Computational Linguistics, 2010, pp. 220\u2013224.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "N-gram counts and language models from the common crawl", "author": ["Christian Buck", "Kenneth Heafield", "Bas van Ooyen"], "venue": "LREC, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "KenLM: Faster and smaller language model queries", "author": ["Kenneth Heafield"], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation. Association for Computational Linguistics, 2011, pp. 187\u2013197.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Quoc V Le", "Rajat Monga", "Matthieu Devin", "Kai Chen", "Greg S. Corrado", "Jeff Dean", "Andrew Y. Ng"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8595\u20138598.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["Tom\u00e1\u0161 Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH), 2010, pp. 1045\u2013 1048.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Advances in optimizing recurrent networks", "author": ["Yoshua Bengio", "Nicolas Boulanger-Lewandowski", "Razvan Pascanu"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8624\u20138628.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Rnnlm-recurrent neural network language modeling toolkit", "author": ["Tomas Mikolov", "Stefan Kombrink", "Anoop Deoras", "Lukar Burget", "J Cernocky"], "venue": ".", "citeRegEx": "13", "shortCiteRegEx": null, "year": 0}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011, pp. 1017\u20131024.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Accelerating recurrent neural network training via two stage classes and parallelization", "author": ["Zhiheng Huang", "Geoffrey Zweig", "Michael Levit", "Benoit Dumoulin", "Barlas Oguz", "Shawn Chang"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 326\u2013331.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient GPU-based training of recurrent neural network language models using spliced sentence bunch", "author": ["Xie Chen", "Yongqiang Wang", "Xunying Liu", "Mark JF Gales", "Philip C Woodland"], "venue": "Proc. ISCA Interspeech, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Large scale recurrent neural network on gpu", "author": ["Boxun Li", "Erjin Zhou", "Bo Huang", "Jiayi Duan", "Yu Wang", "Ningyi Xu", "Jiaxing Zhang", "Huazhong Yang"], "venue": "Neural Networks (IJCNN), 2014 International Joint Conference on. IEEE, 2014, pp. 4062\u20134069.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning, vol. 4, 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Andriy Mnih", "Koray Kavukcuoglu"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 2265\u20132273.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "TED-LIUM: an Automatic Speech Recognition dedicated corpus", "author": ["Anthony Rousseau", "Paul Del\u00e9glise", "Yannick Est\u00e8ve"], "venue": "LREC, 2012, pp. 125\u2013129.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "The University of Cambridge Russian-English system at WMT13", "author": ["J. Pino", "A. Waite", "T. Xiao", "A. de Gispert", "F. Flego", "W. Byrne"], "venue": "Proceedings of the Eighth Workshop on Statistical Machine Translation, 2013, pp. 198\u2013 203.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "The British National Corpus, version 3 (BNC XML Edition), distributed by Oxford University Computing Services on behalf of the BNC Consortium", "author": ["BNC Consortium"], "venue": "2007.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Despite an initial flurry of interest in RNNs in the early \u201990s for acoustic modelling [1, 2, 3], the computational cost and memory overheads of training large RNNs proved prohibitive.", "startOffset": 87, "endOffset": 96}, {"referenceID": 1, "context": "Despite an initial flurry of interest in RNNs in the early \u201990s for acoustic modelling [1, 2, 3], the computational cost and memory overheads of training large RNNs proved prohibitive.", "startOffset": 87, "endOffset": 96}, {"referenceID": 2, "context": "Despite an initial flurry of interest in RNNs in the early \u201990s for acoustic modelling [1, 2, 3], the computational cost and memory overheads of training large RNNs proved prohibitive.", "startOffset": 87, "endOffset": 96}, {"referenceID": 3, "context": "However, recent work by Mikolov [4] on RNNLMs has shown that modestly sized RNNLMs can now be trained and have been shown to be competitive with ngrams.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "Mikolov trained RNNLMs with 800 hidden state units; Google\u2019s language modelling benchmark [5] subsequently established baseline RNNLM results with 1024 hidden state units.", "startOffset": 90, "endOffset": 93}, {"referenceID": 5, "context": "To filter our corpus we used cross-entropy difference scoring [6, 7].", "startOffset": 62, "endOffset": 68}, {"referenceID": 6, "context": "To filter our corpus we used cross-entropy difference scoring [6, 7].", "startOffset": 62, "endOffset": 68}, {"referenceID": 7, "context": "The largest current text corpora such as Google n-grams4 and CommonCrawl [8] are about size 10.", "startOffset": 73, "endOffset": 76}, {"referenceID": 8, "context": "At 8bn words the KN 5-gram already takes up 362GB in ARPA format and 69GB in KenLM [9] binary trie format - already impractically large for current commercial ASR.", "startOffset": 83, "endOffset": 86}, {"referenceID": 4, "context": "Recent attempts to train very large networks (on the order of billions of parameters) have required many CPU cores to work in concert [5, 10].", "startOffset": 134, "endOffset": 141}, {"referenceID": 9, "context": "Recent attempts to train very large networks (on the order of billions of parameters) have required many CPU cores to work in concert [5, 10].", "startOffset": 134, "endOffset": 141}, {"referenceID": 10, "context": "We use a standard RNN architecture [11] but dispense with bias units to maximise efficiency on the GPU and because, in our experience, they do not provide any practical benefit.", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "Recent work has improved our understanding of how to effectively train RNNs [12].", "startOffset": 76, "endOffset": 80}, {"referenceID": 12, "context": "We believe our setup from 2013 in this paper gives better speedups than previously reported elsewhere [13, 14, 15, 16, 17].", "startOffset": 102, "endOffset": 122}, {"referenceID": 13, "context": "We believe our setup from 2013 in this paper gives better speedups than previously reported elsewhere [13, 14, 15, 16, 17].", "startOffset": 102, "endOffset": 122}, {"referenceID": 14, "context": "We believe our setup from 2013 in this paper gives better speedups than previously reported elsewhere [13, 14, 15, 16, 17].", "startOffset": 102, "endOffset": 122}, {"referenceID": 15, "context": "We believe our setup from 2013 in this paper gives better speedups than previously reported elsewhere [13, 14, 15, 16, 17].", "startOffset": 102, "endOffset": 122}, {"referenceID": 16, "context": "We believe our setup from 2013 in this paper gives better speedups than previously reported elsewhere [13, 14, 15, 16, 17].", "startOffset": 102, "endOffset": 122}, {"referenceID": 17, "context": "To achieve high throughput and utilisation on GPUs we train a standard RNN with stochastic gradient descent and rmsprop[18].", "startOffset": 119, "endOffset": 123}, {"referenceID": 15, "context": "Typically we use noffset 128, similar to [16].", "startOffset": 41, "endOffset": 45}, {"referenceID": 18, "context": "We train using Noise Contrastive Estimation [19].", "startOffset": 44, "endOffset": 48}, {"referenceID": 4, "context": "heldout-00000-of00050\u2019) from [5].", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "Language Modelling We present our RNNLM results on the recently released billion word language modelling benchmark[5].", "startOffset": 114, "endOffset": 117}, {"referenceID": 4, "context": "The nstate 4096 RNN is larger in state size than those in [5] and", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "4) with that one single model than the combination of interpolated models in [5] whilst also using only 3% of the total parameters.", "startOffset": 77, "endOffset": 80}, {"referenceID": 19, "context": "The \u2018IWSLT\u2019 system uses the Kaldi TEDLIUM [20] recipe for acoustic models and language models built on [5]6.", "startOffset": 42, "endOffset": 46}, {"referenceID": 4, "context": "The \u2018IWSLT\u2019 system uses the Kaldi TEDLIUM [20] recipe for acoustic models and language models built on [5]6.", "startOffset": 103, "endOffset": 106}, {"referenceID": 20, "context": "In our MT experiments we rescored the WFST lattices from the WMT13 EN-RU system developed at Cambridge University Engineering Department [21].", "startOffset": 137, "endOffset": 141}, {"referenceID": 21, "context": "For this task we train on 100M words from the BNC corpus [22] using a vocabulary size of 10K and the RNN using a shortlist of 100 candidate words generated from a heavily pruned 2 MB n-gram.", "startOffset": 57, "endOffset": 61}], "year": 2015, "abstractText": "This paper investigates the scaling properties of Recurrent Neural Network Language Models (RNNLMs). We discuss how to train very large RNNs on GPUs and address the questions of how RNNLMs scale with respect to model size, training-set size, computational costs and memory. Our analysis shows that despite being more costly to train, RNNLMs obtain much lower perplexities on standard benchmarks than n-gram models. We train the largest known RNNs and present relative word error rates gains of 18% on an ASR task. We also present the new lowest perplexities on the recently released billion word language modelling benchmark, 1 BLEU point gain on machine translation and a 17% relative hit rate gain in word prediction.", "creator": "LaTeX with hyperref package"}}}