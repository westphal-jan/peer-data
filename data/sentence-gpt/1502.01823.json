{"id": "1502.01823", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2015", "title": "Unsupervised Fusion Weight Learning in Multiple Classifier Systems", "abstract": "In this paper we present an unsupervised method to learn the weights with which the scores of multiple classifiers must be combined in classifier fusion settings. We also introduce a novel metric for ranking instances based on an index which depends upon the rank of weighted scores of test points among the weighted scores of training points. We show that the optimized index can be used for computing measures such as average precision (in the first 100% test points) or rank scale (in the second 100% test points).\n\n\n\n\n\nThe first part of the series explains how a new measurement method that is required for ranking multiple classes with the number of tests with the number of tests with the number of test points for training points is now available.\nWe first present a model to learn the classification algorithm, which is a step along which the classifier fusion system should become a new and better model.\nIn summary, a new metric which is an appropriate metric for evaluating and verifying the rank of test points is now available.\nFinally, we introduce a new method to learn the weighted scores of test points based on the number of test points in classifiers that have an index containing the number of test points in the weighted score of testing points and how that rank of weighted scores can be calculated for the test points for each classifier.\nFor the last two articles, we describe how the algorithm will be implemented in the next installment.\nRelated Articles", "histories": [["v1", "Fri, 6 Feb 2015 08:28:57 GMT  (87kb,D)", "http://arxiv.org/abs/1502.01823v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["anurag kumar", "bhiksha raj"], "accepted": false, "id": "1502.01823"}, "pdf": {"name": "1502.01823.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Fusion Weight Learning in Multiple Classifier Systems", "authors": ["Anurag Kumar", "Bhiksha Raj"], "emails": ["alnu@andrew.cmu.edu", "bhiksha@cs.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": ""}, {"heading": "1.1. Classifier Fusion", "text": "In several pattern recognition tasks we are required to combine information from several sources. Fusion of the information derived from these sources thus becomes an important part of pattern recognition. Fusion may be performed early, by directly considering the features derived from the individual information sources jointly. Late fusion, on the other hand, is performed at the decision level, by somehow combining the decisions made from the information from the individual sources. In this work our focus is on decision-level fusion, specifically of the variety where the final decision is based on a weighted sum of scores produced by individual classifiers. Optimization of fusion amounts to optimally learning the weights with which the classifiers are combined. Unlike the usual ap-\nproach of learning a global set of weights that apply to all test instances, we attempt to learn instance-specific weights for individual test points. Thus, we are able to consider the specific characteristics of individual data points, unlike global weighting schemes which completely ignore the individuality of test instances.\nMost classifier-fusion methods assign a fixed weight to each classifier, the simplest and most common being the method of averaging which assigns equal weight to all classifiers. Even this simple averaging is quite effective and is often hard to beat in several situations, especially when different classifiers are almost independent. Several other methods are discussed in the next sub-section. Nearly all of these methods rely on learning a unique set of weights from training or held-out data; these weights are subsequently used on all test instances, ignoring instance-specific behaviors of the classifiers. Moreover, questions relating to the generalization of weights learned from held-out data to the test data being scored also arise.\nIn this work we propose solutions to learn instancespecific weights for classifier fusion, and investigate their behavior. We consider two scenarios.\n\u2022 In the first, conventional, mechanism for combining classifiers, the final score of an instance is the weighted sum of all classifier scores. Thus all classifiers contribute to the final score of an instance.\n\u2022 In the second, only a subset (N -best) of classifiers contribute scores to an instance. This lets us reject unreliable or noisy classifiers on a by-instance basis\nOur method is based on the bipartite ranking loss [2][4]. Two modifications of the bipartite ranking loss, called the relevance loss and the irrelevance loss, and an index defined on them are sufficient to learn optimal fusion weights for an unlabeled instance. Specifically, the idea is to optimize a raw \u201cclarity index\u201d with respect to the weights, to estimate the instance-specific fusion weights. Moreover, the optimal raw clarity indices of unlabeled instances can themselves be used to rank the unlabeled instances as well. The raw clarity\n1\nar X\niv :1\n50 2.\n01 82\n3v 1\n[ cs\n.L G\n] 6\nF eb\n2 01\nindex thus gives us an entirely novel mechanism of combining classifiers to score instances for ranking, which differs from the usual approach of ranking them by the weighted sum of the scores of the classifiers.\nOur method is unsupervised and the optimization of weights is done directly on actual test instances, rather than using a held out set. The method is unsupervised in the sense that in order to learn the optimal weights for an instance, all we need are the scores from the classifiers for that instance, in addition to classifier outputs on training data. Since the optimization is performed directly on test data, it minimizes generalization concerns that result from learning weights on held out validation data [6]. As a corollary, the optimization is performed on actual test instances whose labels are not known and no intermediate held-out/validation data with known labels are required.\nOurs is a meta algorithm; the training of the individual classifiers themselves is treated as a black box. We only consider the scores output by the classifiers. Our learning method does not know what kind of classifiers or features were used to obtain the scores. The only assumption we make is that within any classifier, higher scores imply a higher ranking of the instance by that classifier. From our perspective, an inverted \u201cbottom-up\u201d order of ranking \u2013 where a lower score implies a higher rank \u2013 is as good as a top-down ranking, provided the direction of ranking is known, since the former can be converted to the latter by a simple affine shift of the scores. Specifically, in our case the classifier scores are assumed to have the aspect of probabilities of belonging to the class; thus the higher it is for an instance the higher the rank of that instance among the set.\nIn next subsection we give a brief description of some related work on classifier fusion. In Section 2 we describe the problem and our solution. In Section 3 we give experimental results using our method on object (flower) categorization. In Section 4 we discuss our results and conclude."}, {"heading": "1.2. Related Work", "text": "Several works have studied the problem of classifier fusion [1] [5], [13] [7] [16] [12]. A particularly popular formalism for combining outputs of classifiers is stacking [15]. Stacking in general is implied in any method which involves \u201clearning\u201d to combine the base classifiers. The fundamental idea of stacking is that the problem of combining the base classifiers can be cast as another learning problem. The outputs (say probabilities) of the base classifiers are treated as an input space to the stacking function, while the output space of the function remains the same as that of the base classifiers [9] [14]. The stacking framework learns the parameters of the stacking function to optimize classification accuracy, generally on some labeled training or held-out data. Our approach, on the other hand, does not optimize classification accuracy \u2013 the objective that is optimized is\nan index called clarity, and makes no reference to the true labels of the data that it is optimized over. The combination function is optimized in an unsupervised manner over the actual test data. Moreover, we preform the optimization separately for each test instance.\nTo the best of our knowledge, few recent works have actually looked into instance-specific weight learning [8] [17]. Some of the most promising results are reported in [8]. The basic idea in this work is to propagate fusion weights of labeled instances to the individual unlabeled instances along a graph built on low-level features. The method has been shown to outperform other fusion methods on a variety of datasets. However, although the learned weights are instance specific, the method not only still requires a held-out set for which labels are known, it also requires knowledge of the low-level features of instances. On the other hand, our method does not require held-out data. Moreover, our solution is a meta algorithm that requires no knowledge of the low-level features of the instances. Another issue with [8] is that the weights learned for different test instances are not disjoint from each other. This has the undesirable aspect that newer test instances cannot be independently introduced into the set.\nGiven the distinctness of our approach, we focus on introducing and investigating our proposed instance-specific weight-learning paradigm, rather than demonstrating improvements over several other global fusion strategies. Unlike the other methods mentioned earlier, our solution does not require a separate held-out set. Also, the optimization of weights for each test instance is disjoint from other test instances. Finally, our method is as true meta algorithm that makes no reference to low-level features or how the classifiers were trained.\nWe also analyze important aspects of the fusion such as selecting only a group of good classifiers for an instance and the effects of noisy classifiers on the weight learning scheme and show that our proposed method is quite robust."}, {"heading": "2. The Proposed Algorithm", "text": ""}, {"heading": "2.1. Problem Setting", "text": "We set up our problem within a retrieval scenario where the objective is to rank positive test instances from the target class ahead of negative instances. Our objective becomes that of determining how to combine the scores produced by a collection of classifiers, in order to optimize the ranking.\nLet p be a sample instance and m be the number of classifiers used for predicting scores. Ci denotes the ith classifier. Thus for any sample instance p we have m outputs scores xi = Ci (p) , i = 1 \u00b7 \u00b7 \u00b7m, where Ci (p) is the output of classifier Ci on some feature vector of p. Let ~x = [x1 x2 x3 ..... xm]T be the vector representing the scores from all m classifiers. Thus all sample instances\nare represented by an m-dimensional score vector. Let y \u2208 {0, 1} represent the label of an instance.\nLet X be the set of available training instances, where each instance in X is represented by an m-dimensional score vector. Class labels y are available for every instance in this set. We note that X here represents the set that will be used to optimize the fusion, not the data used to train the m individual classifiers. In practice, it is sufficient to have X be the same as the training set on which all classifiers are trained, and hence no held-out set is required in the learning process. However, mathematically no such restriction is placed. The positive (y = 1) and negative (y = 0) labeled training instances in X are separated into two sets, X+ and X\u2212, such that each instance inX+ has label y = 1 and each instance in X\u2212 has label y = 0. The number of instances in X+ is represented by n1 and in X\u2212 by n0.\nLet Xtest be the set of unlabeled test instances that must be classified and pu be an unlabeled test instance in Xtest with score vector ~xu. The goal is to learn an optimal weight vector ~wu for each unlabeled instance pu and the final weighted sum of scores for each pu given by su = ~wTu ~x u."}, {"heading": "2.2. Relevance, Irrelevance and Clarity", "text": "The fusion weights are learned directly on test instances for which the class labels are not known. To learn the weights, we must define an objective function that does not refer to the labels. Instead, our objective will relate to the rank of the test instance relative to the training instances.\nFor each test instance pu we aim to find the weight vector ~wu that maximizes the score su if the instance is positive, or minimizes (makes it maximally negative) it if its negative. In order to do so, we define an objective function that, when optimized, can be expected to result in weights that have these characteristics. We do so as follows.1\nWe base our objective on the intuition that if pu is to be classified well, then, if the test instance is positive, its\n1Note that we assume here, without loss of generality, that the classification rule assumes the score to be analogous to the probability of belonging to the target class \u2013 higher scores imply a higher probability and vice versa.\nscore must lie as far to the right of the distribution of the scores of positively labeled instances as possible for confident classification. Empirically, the instance must outscore as many of the positively labeled training points in X+ as possible. On the other hand, if the instance is negative, its score must ideally be lower than that of as many negatively labeled training instances from X\u2212 as possible.\nTo formalize this intuition, we define two losses, the relevance loss and the irrelevance loss, and an index based on these losses [4]. The relevance loss RL(~xu, ~wu) for an unlabeled point pu with score vector ~xu and weight vector ~wu in our setting is defined as the fraction of negatively labeled training instances from X\u2212 that score more than pu, when the scores are combined using ~wu:\nRL(~xu, ~wu) = 1\nn0 n0\u2211 i=1 I ( ~wTu ~xi \u2212 ~wTu ~xu ) \u2200 ~xi \u2208 X\u2212\n(1) where I is the indicator function such that\nI(t) = { 1, if t \u2265 0 0, otherwise\nSimilarly, the irrelevance loss IL(~xu, ~wu) is defined as the fraction of positively labeled training instances fromX+ that score less than pu\nIL(~xu, ~wu) = 1\nn1 n1\u2211 i=1 I ( ~wTu ~x u \u2212 ~wTu ~xi ) \u2200 ~xi \u2208 X+\n(2) If the unlabeled instance pu has a true label ypu = 1, it is desired that its relevance loss be low (0 in the ideal case). Also, the higher the irrelevance loss the more confidence we have for pu to be positive. However, if pu is actually a negative point, i.e. ypu = 0 then the irrelevance loss should be very low, whereas the higher the value of the relevance loss, the higher is our confidence on pu being a negative instance. These two factors can be combined into a single index termed as Clarity Index. The clarity index is defined as the absolute value of the difference between the relevance loss and irrelevance loss.\nCL(~xu, ~wu) = |RL(~xu, ~wu)\u2212 IL(~xu, ~wu)| (3)\nFigure 1 illustrates the relevance and irrelevance losses and the clarity index. It is obvious that the higher the value of the clarity index, the easier it is to make a decision for pu. The range of the clarity index is [0, 1] and it is desired for it to be high for any unlabeled instance. We also define the Raw Clarity Index (RCL) which is just the difference between RL and IL. Thus RCL(~xu, ~wu) = RL(~xu, ~wu) \u2212 IL(~xu, ~wu) and the range of RCL is [\u22121, 1]. CL is the absolute value of RCL. For a positive instance we expect the raw clarity index to be negative; the closer it is to \u22121\nthe better it is. Similarly for a negative instance the desired value RCL is to be positive and high. In all cases, the CL value should be high. This raw clarity index, as we describe in a subsequent subsection, can also be used as another way to rank the test instances along with the weighted sum of scores su.\nHowever, direct optimization of CL with respect to ~wu is intractable in general, because the function I in the definitions of RL and IL is a discrete measure and cannot be differentiated. We approximate it instead by a smooth, differentiable sigmoid function:\nI(t) \u2248 Is(t) = 1\n1 + e\u2212\u03b1t (4)\nBy choosing the correct \u03b1 this function can be made arbitrarily close to the indicator function I .\nUsing this approximation, the relevance loss (RL) and irrelevance loss(IL) are redefined as\nRL(~xu, ~wu) = 1\nn0 n0\u2211 i=1\n1\n1 + e\u2212\u03b1~w T u ( ~xi\u2212~xu)\n\u2200 ~xi \u2208 X\u2212 (5)\nIL(~xu, ~wu) = 1\nn1 n1\u2211 i=1\n1\n1 + e\u2212\u03b1~w T u (~x\nu\u2212 ~xi) \u2200 ~xi \u2208 X+ (6)"}, {"heading": "2.3. Learning Weights", "text": "We now present a method to learn the instance-specific fusion weights. Our goal is to finding the weight that maximizes the clarity index. The clarity index CL is the absolute value of the raw clarity index RCL. The absolute value function, like the indicator function, is non-differentiable at 0. We may bypass this by employing a continuous, differentiable, approximation of the absolute value function; however, we employ the following direct strategy instead.\nThe raw clarity index using the sigmoid functions is\nRCL(~xu, ~wu) = 1\nn0 \u2211 \u2200 ~xi\u2208X\u2212\n1\n1 + e\u2212\u03b1~w T u ( ~xi\u2212~xu)\n\u2212 1 n1 \u2211 \u2200 ~xi\u2208X+\n1\n1 + e\u2212\u03b1~w T u (~x u\u2212 ~xi)\n(7)\nSince CL = |RCL| we can maximize CL as:\n~wmax = argmax ~wu\nRCL(~xu, ~wu)\nRCLmax = RCL(~x u, ~wmax)\n~wmin = argmin ~wu\nRCL(~xu, ~wu)\nRCLmin = RCL(~x u, ~wmin)\n~\u0302wu = { ~wmax, if RCLmax > |RCLmin| ~wmin otherwise\n(8)\nIn other words, we estimate both the maximum and minimum values of RCL, and choose the weights corresponding to whichever of the two has the larger absolute value.\nThe above estimate requires both maximization and minimization of RCL(~xu, ~wu). We find these extrema through a gradient descent/ascent procedure. Starting with some initial weight we estimate the maximum of RCL with respect to ~w by gradient ascent. We employ gradient descent from the same initial location to find the weight which minimizes RCL. Additionally, the weights are subject to constraints of ~w > 0, since we assume all classifiers to be no worse than random. In addition, to keep the weights from exploding we also impose constraints of ||~w||2 = ~wT ~w = 1, giving us a feasible set that lies on the surface of the section of a unit hypersphere that lies in the positive orthant. The weights are projected on the feasible region after each gradient descent/ascent step. Note that in general RCL is not convex and the algorithms may get stuck in local optima in either direction.\nThe overall algorithm for learning the weight for an instance pu is given in Algorithm 1. In Algorithm 1 dRCL(~xu, ~wmax)\nd~wmax represents the derivative of RCL defined in Equation 7 w.r.t ~wmax. \u03b7k is the ascent step size for the kth iteration which can be fixed or chosen by any search method for each iteration. Similar definitions apply for the minimization case.\nComputationally, convergence to local optima is pretty fast. In most test cases in our experiments the algorithm quickly converges and no significant load is observed in spite of the method being an instance specific approach."}, {"heading": "2.4. Ranking Instances", "text": "The algorithm of Algorithm 1 results in the estimation of fusion weights for each test instance. These can now be used to compute scores and rank order the set of test instances in a number of ways. Ranking by weighted score The estimated weights can simply be used to compute the score su for every test instance according to the weighted score su = ~wTu ~xu. Ranking by raw clarity index: We also introduce another ranking method based on the raw clarity index. Since we optimize the raw clarity index, the raw clarity itself is a measure for ranking of unlabeled instances. As discussed previously, for a positive instance we expect the raw clarity index to be negative; the closer it is to\u22121 the better, and for a negative instance the desired value RCL is to be close to +1. After optimization whatever value CL(~xu, ~wu) stores is the best that can be achieved in either of the two directions. Hence we can simply rank the unlabeled instances based on the reverse order of their optimal raw clarity index. In our experiments we show that this ranking method can sometimes actually result in better ranking compared to that based on the weighted score su values. Thus we\nAlgorithm 1 Weight Learning Algorithm 1: procedure LEARNING WEIGHT FOR EACH pu(X+, X\u2212, , ~xu) // Input training score vectors and score vector of pu\n//Obtain weight which maximizes RCL 2: Initialize ~wmax \u2190 [w1 w2 w3 ....wm]T , wi \u2265 0 \u2200i 3: repeat 4: ~wmax \u2190 ~wmax + \u03b7k dRCL(~x u, ~wmax) d~wmax 5: Project onto {~w : ~wmax > 0 & ||~wmax||2 = 1} 6: until Convergence of RCL\n//Now Obtain Weight which minimizes RCL 7: Initialize ~wmin \u2190 [w1 w2 w3 ....wm]T , wi \u2265 0 \u2200i 8: repeat 9: ~wmin \u2190 ~wmin \u2212 \u03b7k dRCL(~x u, ~wmin) d~wmin\n10: Project onto {~w : ~wmax > 0 & ||~wmax||2 = 1} 11: until Convergence of RCL\n//Assign ~wu to weight for which absolute value of raw clarity or clarity index is higher\n12: ~wu = argmax~w (|RCL(~xu, ~wmin)|, |RCL(~xu, ~wmax)|) 13: Clarity(pu) = RCL(~xu, ~wu) 14: end procedure\nalso now have a novel metric for ranking instances, which is based on the optimal rank of the test instances, rather than their weighted-combined score.\n2.5. N-best Selection\nWe note that poor or noisy classifiers can have a detrimental effect to the overall classification. Classifiers are usually trained on a limited amount of training data. Test instances of unseen characteristics can hence evoke erratic behavior from a classifier. Since the classifiers have been trained on the training data, the probability of such behavior will be low on the training data itself. As a result, the score assigned to a test instance may not be well explained by the distribution of scores obtained on the training data. The weight-learning algorithm should ideally be able to identify such detrimental classifiers and assign very low weight to them. In effect, the estimated weights effectively assign an importance to each of the fused classifiers; noisy or mismatched classifiers should obtain low weight in the weight optimization process.\nWe can therefore use the proposed method to select the N -best classifiers to judge any test instance, by selecting the classifiers corresponding to the highest N weights.\nIn this N -best scenario, ranking can subsequently be done in one of several ways:\n1. By simple averaging of the N -best classifiers.\n2. By computing the weighted score suN over the N -best classifiers, employing the already estimated weights."}, {"heading": "3. Experimental Results", "text": "We evaluate the performance of our method on multiclass object categorization. We use the Oxford Flower dataset [11] which has been used in several works such as [3][11][10] to name a few. This dataset contains flowers of 17 different categories. It provides 80 images for each flower class resulting in an overall set of 1360. The dataset has three predefined splits. In each predefined split, all flower classes are split into 40 training images, 20 validation images and 20 test images. The dataset also provides 7 different features for the images. [11] describes the details of features based on Colour Vocabulary, Shape Vocabulary and Texture Vocabulary. [10] gives the details of features based on HSV, SIFT on the foreground internal regions, SIFT on the foreground boundary, and Histogram of Gradients. The \u03c72 distance matrix for all 7 features are also provided. The predefined splits are here referred to as SET1, SET2, SET3.\nOur basic classifiers are \u03c72 kernel based SVM classifiers. For each flower class we train 7 different base SVM classifiers corresponding to the 7 different features in one-versusrest fashion. Experiments are done as per the predefined splits. The best parameters for the SVM classifiers are chosen by performance check on the validation set. The outputs of these base classifiers on the specified training set forms the training setX for our fusion method and the outputs corresponding to specified test set form our test setXtest. Each instance is thus represented by a 7-dimensional score vector corresponding to the outputs from 7 different classifiers.\nSince our focus here is more on analysing the unsupervised instance-specific learning paradigm, which presents a new take on fusion strategies, and for which no trulyequivalent comparator exists, we compare our performance with average fusion (AVG.). This is one of the most commonly used fusion schemes, where the final score is just the average score of all classifiers and can be very hard to beat specially when the performances of individual classifiers are high, as is true in the current case. We consider various aspects of the problem, including basic classification, N -best selection and the performance of our method when noise is deliberately added to the classifiers.\nWe report results in terms of average precision (AP) and mean average precision (MAP), which are effective characterizations of the accuracy of ranked lists, since, from our perspective, this is a retrieval task. For any class, the AP for a list is given by\nAP =\n\u2211n i=i P (i)I+(i)\nN+\nwhere N+ is the number of positive instances in the test set,\nI+(i) is an indicator of whether the ith test instance is a positive instance for the class, and P (i) is the fraction of the top-ranked i instances which are positive. The MAP is the average of the AP of all classes in the test.\nIn all experiments we fixed the learning rate \u03b7 in Algorithm 1 as 0.1."}, {"heading": "3.1. Selecting \u03b1", "text": "The sigmoid approximation of the indicator function given in Equation 4 has a key parameter \u03b1. Setting this to a high value results in closer approximation to the true indicator function, but results in several local optima of the objective function (CL), effectively increasing the variance of the estimator. A low value of \u03b1, on the other hand, results in lower variance, but can have significant bias. Consequently, the actual value chosen for \u03b1 can have a considerable effect on the outcome of the classifier. Figure 2 shows the varia-\ntion in AP as a function of \u03b1 for three flower classes. As can be seen, there can be considerable variation in performance with \u03b1. In all cases, the performance obtained with the best \u03b1 is significantly higher than that obtained with average fusion.\nFor subsequent experiments, we set the \u03b1 used for any class by optimizing performance on the specified validation sets in the data."}, {"heading": "3.2. Ranking by total score", "text": "We now report the performance obtained from the combined scores, where all 7 classifiers were combined. Figure 3 shows the MAP performance over all 17 classes on the data set. Figure 3 shows results on all three sets of the data. The figure shows results obtained using three methods: ranking with scores obtained from average fusion, with scores from weighted fusion using the optimized weights, and based on the optimized raw clarity.\nIts interesting to note that ranking based on raw clarity outperforms weighted fusion in every case; in fact the latter is poorer than average fusion in this test (we see in the next section that this is not always so). Raw clarity based scoring also outperforms average fusion in two of the three sets.\nTo reiterate the effect of \u03b1 on the performance, we show the MAP values when \u03b1 has been provided by an oracle in Figure 4. This essentially means \u03b1 is tuned on test data. From Figure 4 and Figure 2 we make a note of the fact that proper \u03b1 tuning can give significant improvements. Apart from Figure 4 all results are on \u03b1 selected using specified validation sets.\n3.3. N-best selection\nNot all classifiers that are fused are equally effective on any instance. As mentioned earlier, the proposed weightestimation strategy can actually be used to select the best classifiers for each instance.\nThe left panel in Figure 5 shows the result of this approach on Set1. Results for the remaining two sets are submitted as part of the supplemental material. The figure shows the performance obtained with two variants, (1) the top N classifiers are uniformly averaged, and (2) the weighted summed scores of the top N classifiers is considered for ranking. The figure shows the performance as a function of N . The horizontal lines in the figure also show the performance obtained when all 7 classifiers are combined. We note that rejecting the worst scoring classifier improves the performance of both N -best approaches. Further, weight-based selection of classifiers can result in significant improvement over combining all seven classifiers. Here, superior performance is obtained in both, averaged N -best scores and weighted average of N -best scores. It may be noted from the supplemental material that even on\nthe remaining sets, including the difficult Set 3, the performance with averaged N -best scores can be superior to all other methods for the appropriate setting of N .\n3.4. N-best selection on Noisy Classifiers\nTo study this phenomenon of N best selection in a clearer way we look into a harder problem. We deliberately introduce noise in the classifiers and observe if our weight learning algorithm can sustain this corruption by noise. So a classifier is artificially degraded by the addition of Gaussian noise to the scores given by the classifier on the test points. This is done only for a percentage of randomly chosen test points. This simulates the effect of an erroneous classifier that may have been added to the mix. Such a classifier can badly affect the performance of any fusion scheme that combines all classifiers. This process can be done for more than one classifier as well. These noisy scores are then used to learn weights and we assess how much performance in terms of MAP has been sustained in this noisy situation. For observable decay of performance, in the present experiments 20% of test points are corrupted for a classifier. The number of classifiers corrupted is denoted by c. We perform experiments by corrupting c = 1, , c = 3 and c = 4 classifiers. The classifiers to corrupt are chosen randomly. Having more noisy classifiers means more degradation in performance in terms of MAP. Our goal is to see if we can sustain the performance by the N -best selection schema which uses the weights learned by the proposed algorithm to select the N best classifiers. In the right panel in Figure 5, one of the classifiers has been artificially degraded. We note that N -best based methods remain robust to the inclusion of such degraded classifiers in the mix. This difference becomes more visible when number of such noisy classifiers is increased. 2\nThe results for larger number of corrupted classifiers\n2Note here that in this noise tolerance study, for \u03b1 selection using validation set, even the validation set was corrupted. This was done to ensure that validation set is considered no different from test set and hence \u03b1 must be selected based on corrupted validation data. This in fact proves robustness of our method.\n(c = 3 and c = 4) are shown as bar plots in Figure 6. The left panel in Figure 6 shows MAP values for different N - best schemes when 3 classifiers are degraded (c = 3). It is again clear that the N - best selection is robust to noisy classifiers as the performance is sustained to a greater extent. For average fusion the performance drops by 4.03% (from 89.76 to 85.73) in terms of MAP. The numbers for N = 3 for N -BEST-AVG and N -BEST W.AVG are 87.56% and 89.82% respectively showing that performance is sustained to a much greater extent using the learned weights compared to average fusion. Similar higher MAPs are found for N = 4 as well. Plots for c = 4 are shown in the right panel in Figure 6. In this case the MAP numbers for AVG., N - BEST-AVG and N -BEST W.AVG with N = 3 are 81.20%, 85.18, 82.70% respectively. Similar superior performance is observed with N = 4. This clearly shows that the weight learning algorithm can indeed be used for N -best classifier selection which can sustain performance even if some of the classifiers are noisy in the mix. Plots for other sets are provided in the supplementary material."}, {"heading": "4. Conclusions and Discussion", "text": "The results indicate that the proposed fusion method is indeed able to achieve improved results over average fusion, showing its promise. Results also showed that the proposed raw clarity based ranking is a valid metric for ranking instances. In fact it outperformed weighted scoring methods. For several flower classes across different sets, 2\u22125% absolute improvement in AP is observed usingRCL for ranking instances. It is interesting that this score is based primarily on rank order and has no direct probability-based interpretation. Notably though, it is demonstrated that a score that is obtained by unsupervised optimization over the test data is able to provide improvements over average fusion. This opens up the possibility of an unsupervised weight learning method which can outperform the state of art fusion strategies. The advantages of instance specific unsupervised weight learning are manifold. No held-out set is needed in the optimization process, reducing generalization concerns;\nalso features such as the ability to perform N -best selection make the process robust to noise in the outputs of the classifiers.\nThe greater benefit from the method is its ability to accurately identify the most promising classifiers to combine, and eliminate noisy classifiers from contention in an instance-specific manner. This shows that the proposed algorithm can be applied to situations where the test set is large and diverse and it is expected that some classifiers can behave erratically for some test points. We are able to choose the best set of classifiers for each instance with remarkable consistency. This can, in turn, result in significant improvement in performance, for instance the AP for the class \u201cPansy\u201d, an absolute improvement of 4%-9% is achieved in the different sets. In the noise-tolerance study we saw that the N -best selection on the noisy classifiers using our proposed method can outperform average fusion by a huge margin in terms of MAP.\nMany avenues remain for investigation. The performance is heavily dependent on optimal choice of \u03b1 \u2013 while the best \u03b1 provided by an oracle will result in large improvements in every case, optimizing \u03b1 over a held-out development set is unable to find the best \u03b1 in all cases. For the classifier selection case, while there is considerable latitude in the choice of N , the optimal value of N must be identified from a development set.\nFrom theoretical perspective we need to investigate matters such as the optimal selection of labeled training instances to compute clarity. Another candidate for investigation is the objective function itself: enhancing it with regularizers, e.g. imposing sparsity on weights. Transductive learning methods that jointly optimize the individual test instances while ensuring that instances with similar scores achieve similar results are likely to result in further improvements. Among work in progress is also a formal proof that the algorithm will always lead to convergence to the best possible clarity given the training set X and score vector ~xu."}], "references": [{"title": "A classifier ensemble based on fusion of support vector machines for classifying hyperspectral data", "author": ["X. Ceamanos", "B. Waske", "J.A. Benediktsson", "J. Chanussot", "M. Fauvel", "J. Sveinsson"], "venue": "Intl. Journal of Image and Data Fusion,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "An efficient boosting algorithm for combining preferences", "author": ["Y. Freund", "R. Iyer", "R.E. Schapire", "Y. Singer"], "venue": "The Journal of machine learning research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "On feature combination for multiclass object classification", "author": ["P. Gehler", "S. Nowozin"], "venue": "In Computer Vision,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Active learning for in-  teractive multimedia retrieval", "author": ["T.S. Huang", "C.K. Dagli", "S. Rajaram", "E.Y. Chang", "M.I. Mandel", "G.E. Poliner", "D.P. Ellis"], "venue": "Proc. of the IEEE,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Trainable classifier-fusion schemes: an application to pedestrian detection", "author": ["O.L. Junior", "D. Delgado", "V. Gon\u00e7alves", "U. Nunes"], "venue": "In Intelligent Transportation Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "A framework for classifier fusion: Is it still needed? In Advances in Pattern Recognition, pages 45\u201356", "author": ["J. Kittler"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Increasing experts for majority vote in ocr: Theoretical considerations and strategies", "author": ["L. Lam", "C.Y. Suen"], "venue": "In Proc. 4th Int. Workshop on Frontiers in Handwriting Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1994}, {"title": "Sample-specific late fusion for visual category recognition", "author": ["D. Liu", "K.-T. Lai", "G. Ye", "M.-S. Chen", "S.-F. Chang"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Using correspondence analysis to combine classifiers", "author": ["C.J. Merz"], "venue": "Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Automated flower classification over a large number of classes", "author": ["M. Nilsback", "A. Zisserman"], "venue": "In Computer Vision, Graphics & Image Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "A visual vocabulary for flower classification", "author": ["M.-E. Nilsback", "A. Zisserman"], "venue": "In CVPR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "An overview of classifier fusion methods", "author": ["D. Ruta", "B. Gabrys"], "venue": "Computing and Information systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Classification in likelihood", "author": ["R. Singh", "B. Raj"], "venue": "spaces. Technometrics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Combining classifiers with meta decision trees", "author": ["L. Todorovski", "S. D\u017eeroski"], "venue": "Machine learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Stacked generalization", "author": ["D.H. Wolpert"], "venue": "Neural networks,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1992}, {"title": "Methods of combining multiple classifiers and their applications to handwriting recognition", "author": ["L. Xu", "A. Krzyzak", "C.Y. Suen"], "venue": "Systems, Man and Cybernetics, IEEE Trans. on,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1992}, {"title": "Robust late fusion with rank minimization", "author": ["G. Ye", "D. Liu", "I.-H. Jhuo", "S.-F. Chang"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "Our method is based on the bipartite ranking loss [2][4].", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "Our method is based on the bipartite ranking loss [2][4].", "startOffset": 53, "endOffset": 56}, {"referenceID": 5, "context": "Since the optimization is performed directly on test data, it minimizes generalization concerns that result from learning weights on held out validation data [6].", "startOffset": 158, "endOffset": 161}, {"referenceID": 0, "context": "Several works have studied the problem of classifier fusion [1] [5], [13] [7] [16] [12].", "startOffset": 60, "endOffset": 63}, {"referenceID": 4, "context": "Several works have studied the problem of classifier fusion [1] [5], [13] [7] [16] [12].", "startOffset": 64, "endOffset": 67}, {"referenceID": 12, "context": "Several works have studied the problem of classifier fusion [1] [5], [13] [7] [16] [12].", "startOffset": 69, "endOffset": 73}, {"referenceID": 6, "context": "Several works have studied the problem of classifier fusion [1] [5], [13] [7] [16] [12].", "startOffset": 74, "endOffset": 77}, {"referenceID": 15, "context": "Several works have studied the problem of classifier fusion [1] [5], [13] [7] [16] [12].", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "Several works have studied the problem of classifier fusion [1] [5], [13] [7] [16] [12].", "startOffset": 83, "endOffset": 87}, {"referenceID": 14, "context": "A particularly popular formalism for combining outputs of classifiers is stacking [15].", "startOffset": 82, "endOffset": 86}, {"referenceID": 8, "context": "The outputs (say probabilities) of the base classifiers are treated as an input space to the stacking function, while the output space of the function remains the same as that of the base classifiers [9] [14].", "startOffset": 200, "endOffset": 203}, {"referenceID": 13, "context": "The outputs (say probabilities) of the base classifiers are treated as an input space to the stacking function, while the output space of the function remains the same as that of the base classifiers [9] [14].", "startOffset": 204, "endOffset": 208}, {"referenceID": 7, "context": "To the best of our knowledge, few recent works have actually looked into instance-specific weight learning [8] [17].", "startOffset": 107, "endOffset": 110}, {"referenceID": 16, "context": "To the best of our knowledge, few recent works have actually looked into instance-specific weight learning [8] [17].", "startOffset": 111, "endOffset": 115}, {"referenceID": 7, "context": "Some of the most promising results are reported in [8].", "startOffset": 51, "endOffset": 54}, {"referenceID": 7, "context": "Another issue with [8] is that the weights learned for different test instances are not disjoint from each other.", "startOffset": 19, "endOffset": 22}, {"referenceID": 3, "context": "To formalize this intuition, we define two losses, the relevance loss and the irrelevance loss, and an index based on these losses [4].", "startOffset": 131, "endOffset": 134}, {"referenceID": 0, "context": "The range of the clarity index is [0, 1] and it is desired for it to be high for any unlabeled instance.", "startOffset": 34, "endOffset": 40}, {"referenceID": 10, "context": "We use the Oxford Flower dataset [11] which has been used in several works such as [3][11][10] to name a few.", "startOffset": 33, "endOffset": 37}, {"referenceID": 2, "context": "We use the Oxford Flower dataset [11] which has been used in several works such as [3][11][10] to name a few.", "startOffset": 83, "endOffset": 86}, {"referenceID": 10, "context": "We use the Oxford Flower dataset [11] which has been used in several works such as [3][11][10] to name a few.", "startOffset": 86, "endOffset": 90}, {"referenceID": 9, "context": "We use the Oxford Flower dataset [11] which has been used in several works such as [3][11][10] to name a few.", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "[11] describes the details of features based on Colour Vocabulary, Shape Vocabulary and Texture Vocabulary.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] gives the details of features based on HSV, SIFT on the foreground internal regions, SIFT on the foreground boundary, and Histogram of Gradients.", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "In this paper we present an unsupervised method to learn the weights with which the scores of multiple classifiers must be combined in classifier fusion settings. We also introduce a novel metric for ranking instances based on an index which depends upon the rank of weighted scores of test points among the weighted scores of training points. We show that the optimized index can be used for computing measures such as average precision. Unlike most classifier fusion methods where a single weight is learned to weigh all examples our method learns instance-specific weights. The problem is formulated as learning the weight which maximizes a clarity index; subsequently the index itself and the learned weights both are used separately to rank all the test points. Our method gives an unsupervised method of optimizing performance on actual test data, unlike the well known stacking-based methods where optimization is done over a labeled training set. Moreover, we show that our method is tolerant to noisy classifiers and can be used for selecting N\u2212 best classifiers.", "creator": "LaTeX with hyperref package"}}}