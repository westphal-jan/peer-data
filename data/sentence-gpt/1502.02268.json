{"id": "1502.02268", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2015", "title": "SDNA: Stochastic Dual Newton Ascent for Empirical Risk Minimization", "abstract": "We propose a new algorithm for minimizing regularized empirical loss: Stochastic Dual Newton Ascent (SDNA). Our method is dual in nature: in each iteration we update a random subset of the dual variables. However, unlike existing methods such as stochastic dual coordinate ascent, SDNA is capable of utilizing all curvature information contained in the examples, which leads to striking improvements in both theory and practice - sometimes by orders of magnitude.\n\n\n\nThe solution for SDNA is that we can use a Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian Bayesian", "histories": [["v1", "Sun, 8 Feb 2015 16:34:41 GMT  (428kb,D)", "http://arxiv.org/abs/1502.02268v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zheng qu", "peter richt\u00e1rik", "martin tak\u00e1c", "olivier fercoq"], "accepted": true, "id": "1502.02268"}, "pdf": {"name": "1502.02268.pdf", "metadata": {"source": "META", "title": "SDNA: Stochastic Dual Newton Ascent for Empirical Risk Minimization", "authors": ["Zheng Qu", "Peter Richt\u00e1rik"], "emails": ["ZHENG.QU@ED.AC.UK", "PETER.RICHTARIK@ED.AC.UK", "TAKAC.MT@GMAIL.COM", "OLIVIER.FERCOQ@TELECOM-PARISTECH.FR"], "sections": [{"heading": "1. Introduction", "text": "Empirical risk minimization (ERM) is a fundamental paradigm in the theory and practice of statistical inference and machine learning (Shalev-Shwartz & Ben-David, 2014). In the \u201cbig data\u201d era it is increasingly common in practice to solve ERM problems with a massive number of examples, which leads to new algorithmic challenges.\nState-of-the-art optimization methods for ERM include i) stochastic (sub)gradient descent (Shalev-Shwartz et al.,\n2011; Taka\u0301c\u030c et al., 2013), ii) methods based on stochastic estimates of the gradient with diminishing variance such as SAG (Schmidt et al., 2013), SVRG (Johnson & Zhang, 2013), S2GD (Konec\u030cny\u0301 & Richta\u0301rik, 2014), proxSVRG (Xiao & Zhang, 2014), MISO (Mairal, 2014), SAGA (Defazio et al., 2014), minibatch S2GD (Konec\u030cny\u0301 et al., 2014a), S2CD (Konec\u030cny\u0301 et al., 2014b), and iii) variants of stochastic dual coordinate ascent (Shalev-Shwartz & Zhang, 2013d; Zhao & Zhang, 2014; Taka\u0301c\u030c et al., 2013; Shalev-Shwartz & Zhang, 2013b;a; Lin et al., 2014; Qu et al., 2014; Shalev-Shwartz & Zhang, 2013c).\nThere have been several attempts at designing methods that combine randomization with the use of curvature (secondorder) information. For example, methods based on running coordinate ascent in the dual such as those mentioned above and also (Richta\u0301rik & Taka\u0301c\u030c, 2014; 2012; Fercoq & Richta\u0301rik, 2013b; Tappenden et al., 2014; Richta\u0301rik & Taka\u0301c\u030c, 2013a;b; Fercoq & Richta\u0301rik, 2013a; Fercoq et al., 2014; Qu et al., 2014; Qu & Richta\u0301rik, 2014a) use curvature information contained in the diagonal of a bound on the Hessian matrix. Block coordinate descent methods, when equipped with suitable data-dependent norms for the blocks, use information contained in the block diagonal of the Hessian (Tappenden et al., 2013).\nA more direct route to incorporating curvature information was taken by Schraudolph et al. (2007) in their stochastic L-BFGS method, by Byrd et al. (2014) and Sohl-Dickstein et al. (2014) in their stochastic quasi-Newton methods and by Fountoulakis & Tappenden (2014) who proposed a stochastic block coordinate descent methods. While typically efficient in practice, none of the methods mentioned above are equipped with complexity bounds (bounds on the number of iterations). An exception in this regard is the work of Bordes et al. (2009), who give a O(1/ ) complex-\nar X\niv :1\n50 2.\n02 26\n8v 1\n[ cs\n.L G\n] 8\nity bound for a Quasi-Newton SGD method."}, {"heading": "1.1. Contributions", "text": "The main contribution of this paper is the design and analysis of a new algorithm\u2014stochastic dual Newton ascent (SDNA)\u2014for solving a regularized ERM problem with smooth loss functions and a strongly convex regularizer (primal problem). Our method is stochastic in nature and has the capacity to utilize all curvature information inherent in the data. While we do our analysis for an arbitrary strongly convex regularizer, for the purposes of the introduction we shall describe the method in the case of the L2 regularizer. In this case, the dual problem is a concave quadratic maximization problem with a strongly concave separable penalty.\nSDNA in each iteration picks a random subset of the dual variables (which corresponds to picking a minibatch of examples in the primal problem), following an arbitrary probability law, and maximizes, exactly, the dual objective restricted to the random subspace spanned by the coordinates. Equivalently, this can be seen as the solution of a proximal subproblem involving a random principal submatrix of the Hessian of the quadratic function. Hence, SDNA utilizes all curvature information available in the random subspace in which it operates. Note that this is very different from the update strategy of parallel / minibatch coordinate descent methods. Indeed, while these methods also update a random subset of variables in each iteration, they instead only utilize curvature information present in the diagonal of the Hessian.\nAs we will explain in detail in the text, SDCA-like methods need more iterations (and hence more passes through data) to convergence as the minibatch size increases. However, SDNA enjoys the opposite behavior: with increasing minibatch size, SDNA needs fewer iterations (and hence fewer passes over data) to convergence. This observation can be deduced from the complexity results we prove for SDNA, and is also confirmed by our numerical experiments. In particular, we show that the expected duality gap decreases at a geometric rate which i) is better than that of SDCA-like methods such as SDCA (ShalevShwartz & Zhang, 2013d) and QUARTZ (Qu et al., 2014), and ii) improves with increasing minibatch size. This improvement does not come for free: as we increase the minibatch size, the subproblems grow in size as they involve larger portions of the Hessian. We find through experiments that for some, especially dense problems, even relatively small minibatch sizes lead to dramatic speedups in actual runtime.\nWe show that in the case of quadratic loss, and when viewed as a primal method, SDNA can be interpreted as a variant of the recently introduced Iterative Hessian\nSketch algorithm (Pilanci & Wainwright, 2014).\nEn route to developing SDNA which we describe in Section 4, we also develop several other new algorithms: two in Section 2 (where we focus on smooth problems), one in Section 3 (where we focus on composite problems). Besides SDNA, we also develop and analyze a novel minibatch variant of SDCA in Section 4, for the sake of finding suitable method to compare SDNA to. SDNA is equivalent to applying the new method developed in Section 3 to the dual of the ERM problem. However, as we are mainly interested in solving the ERM (primal) problem, we additionally prove that the expected duality gap decreases at a geometric rate. Our technique for doing this is a variant of the one use by Shalev-Shwartz & Zhang (2013d), but generalized to an arbitrary sampling."}, {"heading": "1.2. Notation", "text": "Vectors. By e1, . . . , en we denote the standard basis vectors in Rn. For any x \u2208 Rn, we denote by xi the ith element of x, i.e., xi = e>i x. For any two vectors x, y of equal size, we write \u3008x, y\u3009 = x>y = \u2211 i xiyi, and by x \u25e6 y we denote their Hadamard (i.e., elementwise) product. We also write u\u22121 = (u\u221211 , . . . , u \u22121 n ).\nMatrices. I is the identity matrix in Rn\u00d7n and D(w) is the diagonal matrix in Rn\u00d7n with w \u2208 Rn on its diagonal. We will write M 0 (resp. M 0) to indicate that M is positive semidefinite (resp. positive definite).\nSubsets of coordinates. Let S be a nonempty subset of [n] := {1, 2, . . . , n}. For any matrix M \u2208 Rn\u00d7n we write MS for the matrix obtained from M by retaining elements Mij for which both i \u2208 S and j \u2208 S and zeroing out all other elements. Clearly, MS = ISMIS . Moreover, for any vector h \u2208 Rn we write\nhS := ISh = \u2211n i=1 hiei. (1)\nNote that we can thus write\n(hS) >MhS = h >ISMISh = h >MSh, (2)\nand that for x, y \u2208 Rn we have\n\u3008xS , y\u3009 = \u3008ISx, y\u3009 = \u3008x, ISy\u3009 = \u3008x, yS\u3009. (3)\nBy (MS)\u22121 we denote the matrix in Rn\u00d7n for which\n(MS) \u22121MS = MS(MS) \u22121 = IS . (4)"}, {"heading": "2. Minimization of a Smooth Function", "text": "In this section we consider unconstrained minimization of a differentiable convex function:\nmin x\u2208Rn f(x). (5)\nIn particular, we shall assume smoothness (Lipschitz continuity of the gradient) and strong convexity of f :\nAssumption 1 (Smoothness). There is a positive definite matrix M \u2208 Rn\u00d7n such that for all x, h \u2208 Rn,\nf(x+ h) \u2264 f(x) + \u3008\u2207f(x), h\u3009+ 1 2 \u3008Mh, h\u3009 (6)\nAssumption 2 (Strong convexity). There is a positive definite matrix G \u2208 Rn\u00d7n such that for all x, h \u2208 Rn,\nf(x) + \u3008\u2207f(x), h\u3009+ 1 2 \u3008Gh, h\u3009 \u2264 f(x+ h). (7)"}, {"heading": "2.1. Three stochastic algorithms", "text": "We now describe three algorithmic strategies for solving problem (5), the first two of which are new. All these methods have the form\nxk+1 \u2190 xk + hk, (8)\nwhere hki is only allowed to be nonzero for i \u2208 Sk, where {Sk}k\u22650 are i.i.d. random subsets of [n] := {1, 2, . . . , n} (\u201csamplings\u201d). That is, all methods in each iteration update a random subset of the variables. The four methods will only differ in how the update elements hki for i \u2208 Sk are computed. If we wish the methods to work, we necessarily need to require that every coordinate has a positive probability of being sampled. For certain technical reasons that will be apparent later, we will also assume that Sk is nonempty with probability 1.\nAssumption 3 (Samplings). The random sets {Sk}k\u22650 are i.i.d., proper (i.e., Prob(i \u2208 Sk) > 0 for all i \u2208 [n]) and nonvacuous (i.e., Prob(Sk = \u2205) = 0).\nMuch of our discussion will depend on the distribution of Sk rather than on k. As {Sk}k\u22650 are i.i.d., we will write S\u0302 for a sampling which shares their distribution. We will write p = (p1, . . . , pn) where\npi := Prob(i \u2208 S\u0302), i \u2208 [n]. (9)\nBy Assumption 3, we have pi > 0 for all i. We now describe the methods.\nMethod 1. We compute (MSk)\u22121 and set\nhk = \u2212(MSk)\u22121\u2207f(xk). (Method 1)\nNote that the update only involves the inversion of a random principal submatrix of M of size |Sk| \u00d7 |Sk|. Also, we only need to compute elements i \u2208 Sk of the gradient \u2207f(xk). If |Sk| is reasonably small, the update step is cheap.\nMethod 2. We compute the inverse of E[MS\u0302 ] and set\nhk = \u2212ISk(E[MS\u0302 ]) \u22121D(p)\u2207f(xk). (Method 2)\nThis strategy easily implementable when |S\u0302| = 1 with probability 1 (i.e., if we update a single variable only). This is because then E[MS\u0302 ] is a diagonal matrix with the (i, i) element equal to piMii. Hence, the update step simplifies to hki = \u2212 1Mii \u3008ei,\u2207f(x\nk)\u3009 for i \u2208 Sk and hki = 0 for i /\u2208 Sk. For more complicated samplings S\u0302, however, the matrix E[MS\u0302 ] will be as hard to invert as M.\nMethod 3. We compute a vector v \u2208 Rn for which\nE[MS\u0302 ] D(p)D(v) (10)\nand then set\nhk = \u2212ISk(D(v))\u22121\u2207f(xk). (Method 3)\nAssuming v is easily computable (this should be done before the methods starts), the update is clearly very easy to perform. Indeed, the update can be equivalently written as hki = \u2212 1vi \u3008ei,\u2207f(x\nk)\u3009 for i \u2208 Sk and hki = 0 for i /\u2208 Sk. Method 3 is known as NSync (Richta\u0301rik & Taka\u0301c\u030c, 2013b). For a calculus allowing the computation of closed form formulas for v as a function of the sampling S\u0302 we refer the reader to (Qu & Richta\u0301rik, 2014b).\nNote that all three methods coincide if |S\u0302| = 1 with probability 1."}, {"heading": "2.2. Three linear convergence rates", "text": "We shall now show that, putting the issue of the cost of each iteration of the three methods aside, all enjoy a linear rate of convergence. In particular, we shall show that Method 1 has the fastest rate, followed by Method 2 and finally, Method 3.\nTheorem 1. Let Assumptions 1, 2 and 3 be satisfied. Let {xk}k\u22650 be the sequence of random vectors produced by Method m, for m = 1, 2, 3 and let x\u2217 be the optimal solution of (5). Then\nE[f(xk+1)\u2212 f(x\u2217)] \u2264 (1\u2212 \u03c3m)E[f(xk)\u2212 f(x\u2217)],\nwhere\n\u03c31 := \u03bbmin\n( G1/2 E [( MS\u0302 )\u22121] G1/2 ) , (11)\n\u03c32 := \u03bbmin\n( G1/2D(p) ( E [ MS\u0302 ])\u22121 D(p)G1/2 ) , (12)\n\u03c33 := \u03bbmin\n( G1/2D(p)D(v\u22121)G1/2 ) . (13)\nThe above result means that the number of iterations sufficient for Method m to obtain an -solution (in expectation) is O( 1\u03c3m log(1/ )).\nIn the above theorem (which we prove in Section 2.4), \u03bbmin(X) refers to the smallest eigenvalue of matrix X. It turns out that in all three cases, the matrix X involved is\npositive definite. However, for the matrices in (11) and (12) this will only be apparent if we show that E[MS\u0302 ] 0 and E[(MS\u0302) \u22121] 0, which we shall do next.\nLemma 1. If S\u0302 is a proper sampling, then E [ MS\u0302 ] 0.\nProof. Denote supp{x} = {i \u2208 [n] : xi 6= 0}. Since M 0, any principal submatrix of M is also positive definite. Hence for any x \u2208 Rn\\{0}, x>MSx = 0 implies that supp{x} \u2229 S = \u2205 for all S \u2286 [n]. If x \u2208 Rn is such that\nx> E [ MS\u0302 ] x = \u2211 S\u2286[n] Prob(S\u0302 = S)x >MSx = 0,\nthen Prob(supp{x}\u2229 S\u0302 = \u2205) = 1. Since S\u0302 is proper, this only happens when x = 0. Therefore, E[MS\u0302 ] 0.\nLemma 2. If S\u0302 is proper and nonvacuous, then\n0 \u227a D(p) ( E [ MS\u0302 ])\u22121 D(p) E [( MS\u0302 )\u22121] . (14)\nProof. The first inequality follows from Lemma 1 and the fact for proper S\u0302 we have p > 0 and hence D(p) 0. We now turn to the second inequality. Fix h \u2208 Rn. For arbitrary \u2205 6= S \u2286 [n] and y \u2208 Rn we have:\n1 2h > (MS) \u22121 h = 12h > S (MS) \u22121 hS\n= max x\u2208Rn \u3008x, hS\u3009 \u2212 12x >MSx \u2265 \u3008y, hS\u3009 \u2212 12y >MSy.\nSubstituting S = S\u0302 and taking expectations, we obtain\n1 2 E [ h> ( MS\u0302 )\u22121 h ] \u2265 E [ \u3008y, hS\u0302\u3009 \u2212 1 2y >MS\u0302y ] = y>D(p)h\u2212 12y > E [ MS\u0302 ] y.\nTherefore, 12h > E [( MS\u0302 )\u22121] h \u2265 maxy\u2208Rn y>D(p)h \u2212 1 2y > E [ MS\u0302 ] y = 12h >D(p) ( E [ MS\u0302 ])\u22121 D(p)h.\nWe now establish an important relationship between the quantities \u03c31, \u03c32 and \u03c33, which sheds light on the convergence rates of the three methods.\nTheorem 2. 0 < \u03c33 \u2264 \u03c32 \u2264 \u03c31 \u2264 1.\nProof. We have \u03c3m > 0 for all m since \u03c3m is the smallest eigenvalue of a positive definite matrix. That \u03c3m \u2264 1 follows as a direct corollary Theorem 1. Finally, D(p)D(v\u22121) = D(p)D(p\u22121)D(v\u22121)D(p) (10)\nD(p) ( E [ MS\u0302 ])\u22121 D(p) (14) E [( MS\u0302 )\u22121] ."}, {"heading": "2.3. Example", "text": "Consider the function f : R3 \u2192 R given by\nf(x) = 12x TMx, M = ( 1.0000 0.9900 0.9999 0.9900 1.0000 0.9900 0.9999 0.9900 1.0000 ) .\nNote that Assumption 1 holds, and Assumption 2 holds with G = M. Let S\u0302 be the \u201c2-nice sampling\u201d on [n] = {1, 2, 3}. That is, we set Prob(S\u0302 = {i, j}) = 13 . for (i, j) = (1, 2), (2, 3), (3, 1). A straightforward calculation reveals that:\nE [( MS\u0302 )\u22121] \u2248 ( 1683.50 \u221216.58 \u22121666.58\u221216.58 33.50 \u221216.58 \u22121666.58 \u221216.58 1683.50 ) ,\nD(p) ( E [ MS\u0302 ])\u22121 D(p) \u2248 ( 0.9967 \u22120.3268 \u22120.3365 \u22120.3268 0.9902 \u22120.3268 \u22120.3365 \u22120.3268 0.9967 ) .\nIt can be verified that (10) holds with v = (2, 2, 2); see (Richta\u0301rik & Taka\u0301c\u030c, 2012) or (Qu & Richta\u0301rik, 2014b). Therefore, D(p)D(v\u22121) = 13I. Finally, we obtain:\n\u03c31 \u2248 0.3350, \u03c32 \u2248 1.333 \u00b7 10\u22124, \u03c32 \u2248 0.333 \u00b7 10\u22124.\nNote that: theoretical rate, \u03c31, of Method 1 is 10,000 times better than the rate, \u03c33, of parallel coordinate descent (Method 3)."}, {"heading": "2.4. Proof of Theorem 1", "text": "Proof. By minimizing both sides of (7) in h, we get:\nf(x)\u2212 f(x\u2217) \u2264 1 2 \u3008\u2207f(x),G\u22121\u2207f(x)\u3009. (15)\nIn view of (6) and (2), for for all h \u2208 Rn we have:\nf(xk + ISkh) \u2264 f(xk) + \u3008\u2207f(xk), ISkh\u3009+ 1\n2 \u3008MSkh, h\u3009.\n(16) Method 1: If we use (16) with h \u2190 hk := \u2212(MSk)\u22121\u2207f(xk), and apply (4), we get:\nf(xk+1) \u2264 f(xk)\u2212 1 2 \u3008\u2207f(xk), (MSk)\u22121\u2207f(xk)\u3009.\nTaking expectations on both sides with respect to Sk yields:\nEk[f(xk+1)]\n\u2264 f(xk)\u2212 1 2 \u3008\u2207f(xk),E[ ( MS\u0302 )\u22121 ]\u2207f(xk)\u3009 (11) \u2264 f(xk)\u2212 \u03c31\n2 \u3008\u2207f(xk),G\u22121\u2207f(xk)\u3009\n(15) \u2264 f(xk)\u2212 \u03c31 ( f(xk)\u2212 f(x\u2217) ) ,\nwhere Ek denotes the expectation with respect to Sk. It remains to rearrange the inequality and take expectation.\nMethod 2: Let D = D(p). Taking expectations on both sides of (16) with respect to Sk, we see that for all h \u2208 Rn the following holds: Ek[f(xk + ISkh)] \u2264 f(xk) + \u3008D\u2207f(xk), h\u3009 + 12 \u3008E[MSk ]h, h\u3009. Note that the choice h\u0303k := \u2212(E[MS\u0302 ])\u22121D\u2207f(xk) minimizes the right hand side of the inequality in h. Since hk = ISk h\u0303 k,\nEk[f(xk+1)]\n\u2264 f(xk)\u2212 1 2 \u3008\u2207f(xk),D ( E[MS\u0302 ] )\u22121 D\u2207f(xk)\u3009 (12) \u2264 f(xk)\u2212 \u03c32\n2 \u3008\u2207f(xk),G\u22121\u2207f(xk)\u3009\n(15) \u2264 f(xk)\u2212 \u03c32 ( f(xk)\u2212 f(x\u2217) ) .\nMethod 3: The proof is the same as that for Method 2, except in the first inequality we replace E[MSk ] by D(p)D(v) (see (10))."}, {"heading": "3. Minimization of a Composite Function", "text": "In this section we consider the following composite minimization problem:\nmin x\u2208Rn F (x) \u2261 f(x) + n\u2211 i=1 \u03c8i(xi). (17)\nWe assume that f satisfies Assumptions 6 and 7. The difference from the setup in the previous section is in the inclusion of the separable term \u2211 i \u03c8i. Assumption 4. For each i, \u03c8i : R \u2192 R \u222a {+\u221e} is closed and \u03b3i-strongly convex for some \u03b3i \u2265 0. Let \u03b3 = (\u03b31, . . . , \u03b3n) \u2208 Rn+.\nFor ease of presentation, in this section we only consider uniform sampling S\u0302, which means that Prob(i \u2208 S\u0302) = Prob(j \u2208 S\u0302) for all i, j \u2208 [n]. In particular, this implies that Prob(i \u2208 S\u0302) = E[|S\u0302|]n for all i. Let \u03c4 := E[S\u0302]."}, {"heading": "3.1. New algorithm", "text": "We now propose Algorithm 1, which a variant of Method 1 applicable to problem (17). If \u03c8i \u2261 0 for all i, the methods coincide. The following result states that the method converges at a geometric rate, in expectation.\nTheorem 3. Let Assumptions 1, 2, 3 and 4 be satisfied. Then the output sequence {xk}k\u22650 of Algorithm 1 satisfies:\nE[F (xk+1)\u2212 F (x\u2217)] \u2264 (1\u2212 \u03c3prox1 )E[F (xk)\u2212 F (x\u2217)],\nwhere x\u2217 is the solution of (17), \u03c3prox1 := \u03c4 min(1,s1) n and\ns1 := \u03bbmin [(n \u03c4 E[MS\u0302 ] + D(\u03b3) )\u22121 (D(\u03b3) + G) ] .\nAlgorithm 1 Proximal version of Method 1 1: Parameters: uniform sampling S\u0302 2: Initialization: choose initial point x0 \u2208 Rn 3: for k = 0, 1, 2, . . . do 4: Generate a random set of blocks Sk \u223c S\u0302 5: Compute: hk = arg minh\u2208Rn\u3008\u2207f(xk), hSk\u3009 +\n1 2 \u3008h,MSkh\u3009+ \u2211 i\u2208Sk \u03c8i(x k i + hi)\n6: Update: xk+1 := xk + hkSk 7: end for\nNote for positive definite matrices X,Y, we have \u03bbmin(X \u22121Y) = \u03bbmin(Y 1/2X\u22121Y1/2). It is this latter form we have used in the formulation of Theorem 1. In the special case when \u03b3 \u2261 0 (\u03c8i are merely convex), we have \u03c3prox1 = min{ \u03c4n , \u03c42 n2\u03bbmin(G 1/2(E[MS\u0302 ])\n\u22121G1/2)}. Note that while this rate applies to a proximal/composite variant of Method 1, its rate is best compared to the rate \u03c32 of Method 2. Indeed, looking at (12), and realizing that for uniform S\u0302 we have D(p) = \u03c4nI, we get\n\u03c31 \u2265 \u03c32 = \u03c4 2 n2\u03bbmin(G 1/2(E[MS\u0302 ]) \u22121G1/2) \u2265 \u03c3prox1 .\nSo, the rate we can prove for the composite version of Method 1 (\u03c3prox1 ) is weaker than the rate we get for Method 2 (\u03c32), which by Theorem 2 is weaker than the rate of Method 1 (\u03c31). We believe this is a byproduct of our analysis rather than the weakness of Algorithm 1."}, {"heading": "3.2. PCDM", "text": "We will now compare our new Algorithm 1 with the Parallel Coordinate Descent Method (PCDM) of Richta\u0301rik & Taka\u0301c\u030c (2012), which can also be applied to problem (17).\nAlgorithm 2 PCDM (Richta\u0301rik & Taka\u0301c\u030c, 2012) 1: Parameters: uniform sampling S\u0302; v \u2208 Rn++ 2: Initialization: choose initial point x0 \u2208 Rn 3: for k = 0, 1, 2, . . . do 4: Generate a random set of blocks Sk \u223c S\u0302 5: Compute for i \u2208 Sk\nhki = arg min hi\u2208R e>i \u2207f(xk)hi+ vi 2 |hi|2+\u03c8i(xki +hi)\n6: Update: xk+1 := xk + \u2211 i\u2208Sk h k i ei 7: end for\nProposition 1. Let the same assumptions as those in Theorem 3 be satisfied. Moreover, assume v \u2208 Rn++ is a vector satisfying (10). Then the output sequence {xk}k\u22650 of Algorithm 2 satisfies\nE[F (xk+1)\u2212 F (x\u2217)] \u2264 (1\u2212 \u03c3prox3 )E[F (xk)\u2212 F (x\u2217)],\nwhere \u03c3prox3 := \u03c4 min(1,s3) n and\ns3 := \u03bbmin\n[ (D(v + \u03b3)) \u22121 (D(\u03b3) + G) ] .\nProof. Sketch: The proof is a minor modification of the arguments in (Richta\u0301rik & Taka\u0301c\u030c, 2012)."}, {"heading": "3.3. Comparison of the rates of Algorithms 1 and 2", "text": "We now show that the rate of linear (geometric) convergence of our method is better than that of PCDM.\nProposition 2. \u03c3prox1 \u2265 \u03c3 prox 3 .\nProof. Since pi = \u03c4n for all i, we have D(p) = \u03c4 nI and hence from (10) we deduce that:\nn \u03c4 E[MS\u0302 ] + D(\u03b3) (10) D(v) + D(\u03b3) = D(v + \u03b3),\nwhence s1 \u2265 s3, and the claim follows."}, {"heading": "4. Empirical Risk Minimization", "text": "We now turn our attention to the empirical risk minimization problem:\nmin w\u2208Rd\nP (w) := 1n n\u2211 i=1 \u03c6i(a > i w) + \u03bbg(w). (18)\nWe assume that g : Rd \u2192 R is a 1-strongly convex function with respect to the L2 norm and each loss function \u03c6i : R \u2192 R is convex and 1/\u03b3-smooth. Each ai is a ddimensional vector and for ease of presentation we write A = (a1, . . . , an) = \u2211n i=1 aie > i . Let g\n\u2217 and {\u03c6\u2217i }i be the Fenchel conjugate functions of g and {\u03c6i}i, respectively. In the case of g, for instance, we have g\u2217(s) = supw\u2208Rd\u3008w, s\u3009\u2212g(w). The (Fenchel) dual problem of (18) can be written as:\nmax \u03b1\u2208Rn\nD(\u03b1) := 1n n\u2211 i=1 \u2212\u03c6\u2217i (\u2212\u03b1i)\u2212 \u03bbg\u2217 ( 1 \u03bbnA\u03b1 ) . (19)"}, {"heading": "4.1. SDNA: A new algorithm for ERM", "text": "Note that the dual problem has the form (17)\nmin \u03b1\u2208Rn F (\u03b1) \u2261 f(\u03b1) + n\u2211 i=1 \u03c8i(\u03b1i), (20)\nwhere F (\u03b1) = \u2212D(\u03b1), f(\u03b1) = \u03bbg\u2217( 1\u03bbnA\u03b1) and \u03c8(\u03b1i) = 1 n\u03c6 \u2217 i (\u2212\u03b1i). It is easy to see that f satisfies Assumption 1 with M := 1nX, where X := 1 \u03bbnA\n>A. Moreover, \u03c8i is \u03b3 n -strongly convex. We can therefore apply Algorithm 1 to solve the dual (20). This is what Algorithm 3 does.\nIf \u03b1\u2217 is the optimal solution of (19), then the optimal solution of (18) is given by:\nw\u2217 = \u2207g\u2217 (\n1 \u03bbnA\u03b1\n\u2217) . (21)\nAlgorithm 3 Stochastic Dual Newton Ascent (SDNA) 1: Parameters: proper nonvacuous sampling S\u0302 2: Initialization: \u03b10 \u2208 Rn; \u03b1\u03040 = 1\u03bbnA\u03b1 0\n3: for k = 0, 1, 2, . . . do 4: Primal update: wk = \u2207g\u2217(\u03b1\u0304k) 5: Generate a random set of blocks Sk \u223c S\u0302 6: Compute:\n\u2206\u03b1k = arg min h\u2208Rn \u3008(A>wk)Sk , h\u3009+ 12h >XSkh + \u2211 i\u2208Sk \u03c6 \u2217 i (\u2212\u03b1ki \u2212 hi)\n7: Dual update: \u03b1k+1 := \u03b1k + (\u2206\u03b1k)Sk 8: Average update: \u03b1\u0304k+1 = \u03b1\u0304k + 1\u03bbn \u2211 i\u2208Sk \u2206\u03b1 k i ai 9: end for\nWith each proper sampling S\u0302 we associate the number:\n\u03b8(S\u0302) := min i\npi\u03bb\u03b3n\nvi + \u03bb\u03b3n , (22)\nwhere (p1, . . . , pn) is the vector of probabilities defined in (9) and v = (v1, . . . , vn) \u2208 Rn++ is a vector satisfying:\nE[(A>A)S\u0302 ] D(p)D(v). (23)\nClosed-form expressions for v satisfying this inequality, as a function of the sampling S\u0302 chosen, can be found in (Qu & Richta\u0301rik, 2014b). A rather conservative choice which works for any S\u0302, irrespective of its distribution, is vi = min{\u03c4, \u03bb\u2032(A>A)}\u2016ai\u20162, where \u03bb\u2032(Y) := maxh{h>Yh : h>D(Y)h \u2264 1} and \u03c4 is a number for which |S\u0302| \u2264 \u03c4 with probability 1 (see Theorem 5.1 in the aforementioned reference). Better bounds (with smaller v) can be derived for special classes of samplings.\nNow we can state the main result of this section: Theorem 4 (Complexity of SDNA). Let S\u0302 be a uniform sampling and let \u03c4 := E[|S\u0302|]. The output sequence {wk, \u03b1k}k\u22650 of Algorithm 3 satisfies:\nE[P (wk)\u2212D(\u03b1k)] \u2264 (1\u2212 \u03c3 prox 1 ) k\n\u03b8(S\u0302) (D(\u03b1\u2217)\u2212D(\u03b10)),\nwhere \u03c3prox1 := \u03c4 min(1,s1) n and\ns1 = \u03bbmin\n[( 1\n\u03c4\u03b3\u03bb E[(A>A)S\u0302 ] + I\n)\u22121] . (24)\nIn the case of quadratic losses and quadratic regularizer, we can sharpen the complexity bound: Theorem 5. When both \u03c6i and g are quadratic functions, the output sequence {wk, \u03b1k}k\u22650 of Algorithm 3 satisfies:\nE[P (wk)\u2212D(\u03b1k)] \u2264 (1\u2212 \u03c31) k\n\u03b8(S\u0302)\n( D(\u03b1\u2217)\u2212D(\u03b10) ) where\n\u03c31 := \u03bbmin\n[ E [((\n1 \u03bbnA >A + \u03b3I ) S\u0302 )\u22121 ( 1 \u03bbnA >A + \u03b3I )]] ."}, {"heading": "4.2. Complexity analysis", "text": "We first establish that SDNA is able to solve the dual.\nLemma 3. Let S\u0302 be a uniform sampling and \u03c4 := E[|S\u0302|]. The output sequence {\u03b1k}k\u22650 of Algorithm 3 satisfies:\nE[D(\u03b1\u2217)\u2212D(\u03b1k)] \u2264 (1\u2212 \u03c3prox1 ) k (D(\u03b1\u2217)\u2212D(\u03b10)),\nwhere \u03c3prox1 is as in Theorem 4.\nProof. If S\u0302 is uniform, then the output of Algorithm 3 is equivalent to the output of Algorithm 1 applied to (20). Therefore, the result is obtained by applying Theorem 3 with M = 1\u03bbn2A >A, G = 0 and \u03b3i = \u03b3n for all i.\nWe now prove a sharper result in the case of quadratic loss and quadratic regularizer.\nLemma 4. If {\u03c6i}i and g are quadratic, then the output sequence {\u03b1k}k\u22650 of Algorithm 3 satisfies:\nE[D(\u03b1\u2217)\u2212D(\u03b1k)] \u2264 (1\u2212 \u03c31)k(D(\u03b1\u2217)\u2212D(\u03b10)),\nwhere \u03c31 is as in Theorem 5.\nProof. If {\u03c6i}i and g are all quadratic functions, then the dual objective function is quadratic with Hessian matrix given by \u22072D(\u03b1) \u2261 1\u03bbn2A\n>A + \u03b3nI. It suffices to apply Theorem 1(11), with M = G = \u22072D(\u03b1).\nWe now prove a more general version of a classical result in dual coordinate ascent methods which bounds the duality gap from above by the expected dual increase.\nLemma 5. The output sequence {wk, \u03b1k}k\u22650 of Algorithm 3 satisfies:\nEk[D(\u03b1k+1)\u2212D(\u03b1k)] \u2265 \u03b8(S\u0302)(P (wk)\u2212D(\u03b1k)).\nThe proof of the this lemma is provided in the supplementary material. Theorem 4 (resp. Theorem 5) now follows by combining Lemma 3 (resp. Lemma 4) and Lemma 5."}, {"heading": "4.3. New Algorithm: SDCA with Arbitrary Sampling", "text": "When |S\u0302| = 1 with probability 1, SDNA reduces to a proximal variant of stochastic dual coordinate ascent (SDCA) (Shalev-Shwartz & Zhang, 2013d). However, a minibatch version of standard SDCA in the ERM setup we consider here has not been previously studied in the literature. Taka\u0301c\u030c et al. (2013) developed such a method but in the special case of hinge-loss (which is not smooth and hence does not fit our setup). Shalev-Shwartz & Zhang (2013b) studied minibatching but in conjunction with acceleration and the QUARTZ method of Qu et al. (2014), which has been analyzed for an arbitrary sampling S\u0302, uses\na different primal update than SDNA. Hence, in order to compare SDNA with an SDCA-like method which is as close a match to SDNA as possible, we need to develop a new method. Algorithm 4 is an extension of SDCA to allow it handle an arbitrary uniform sampling S\u0302.\nThe complexity of Minibatch SDCA (we henceforth just write SDCA) is given in Theorem 6. Theorem 6. If (23) holds, then the output sequence {wk, \u03b1k}k\u22650 of Algorithm 4 satisfies:\nE[P (wk)\u2212D(\u03b1k)] \u2264 (1\u2212 \u03b8(S\u0302)) k\n\u03b8(S\u0302)\n( D(\u03b1\u2217)\u2212D(\u03b10) ) .\nAlgorithm 4 Minibatch SDCA 1: Parameters: uniform sampling S\u0302, vector v \u2208 Rn++ 2: Initialization: \u03b10 \u2208 Rn; set \u03b1\u03040 = 1\u03bbnA\u03b1 0\n3: for k = 0, 1, 2, . . . do 4: Primal update: wk = \u2207g\u2217(\u03b1\u0304k) 5: Generate a random set of blocks Sk \u223c S\u0302 6: Compute for each i \u2208 Sk\nhki = arg min hi\u2208R hi(a > i w k) + vi 2 |hi|2 +\u03c6\u2217i (\u2212\u03b1ki \u2212hi)\n7: Dual update: \u03b1k+1 := \u03b1k + \u2211 i\u2208Sk h k i ei\n8: Average update: \u03b1\u0304k+1 = \u03b1\u0304k + 1\u03bbn \u2211 i\u2208Sk h k i ai 9: end for"}, {"heading": "4.4. SDNA vs SDCA", "text": "We now compare the rates of SDNA and SDCA. The next result says that the rate of SDNA is always superior to that of SDCA. We also see that the rate is better in the quadratic case covered by Theorem 5. Theorem 7. If S\u0302 is uniform sampling with \u03c4 = E[|S\u0302|], then\n\u03b8(S\u0302) \u2264 \u03c3prox1 \u2264 \u03c31.\nProof. Since S\u0302 is a uniform sampling, we have pi = \u03c4n for all i \u2208 [n]. In view of (22), we have 1 \u2264 n\u03c4 \u03b8(S\u0302). Next,\ns1 (24)+(23) \u2265 \u03bbmin\n( 1\n\u03c4\u03bb\u03b3 D(v)D(p) + I\n)\u22121 (22) = n\n\u03c4 \u03b8(S\u0302).\nTherefore, \u03c3prox1 = \u03c4 n min(1, s1) \u2265 \u03b8(S\u0302). In order to establish \u03c3prox1 \u2264 \u03c31, we use Lemma 2 and the fact that E[IS\u0302 ] = \u03c4 nI to obtain\n\u03c4 n ( 1 \u03c4\u03b3\u03bb E[(A >A)S\u0302 ] + I )\u22121 = \u03c4 2 n2 ( E [( 1 \u03b3\u03bbnA >A + I ) S\u0302 ])\u22121 (Lemma 2) E [(( 1 \u03b3\u03bbnA >A + I ) S\u0302\n)\u22121] E [(( A>A + \u03b3\u03bbnI ) S\u0302 )\u22121 (A>A + \u03b3\u03bbnI) ] ,\nThe rest of the argument is similar."}, {"heading": "5. SDNA as Iterative Hessian Sketch", "text": "We now apply SDNA to the least squares problem:\nmin w\u2208Rd\n1 2n \u2016A>w \u2212 b\u20162 + \u03bb 2 \u2016w\u20162, (25)\nand show that the resulting primal update can be interpreted as an iterative Hessian sketch, alternative to the one proposed by Pilanci & Wainwright (2014). We first need to establish a simple duality result.\nLemma 6. Let \u03b1\u2217 be the optimal solution of\nmin \u03b1\u2208Rn\n1 2n \u2016\u03b1\u20162 \u2212 1 n \u3008b, \u03b1\u3009+ 1 2\u03bbn2 \u2016A\u03b1\u20162, (26)\nthen the optimal solution w\u2217 of (25) is w\u2217 = 1\u03bbnA\u03b1 \u2217.\nProof. Problem (25) is a special case of (18) for g(w) \u2261 1 2\u2016w\u2016 2 and \u03c6i(a) \u2261 12 (a\u2212bi) 2 for all i \u2208 [n]. Problem (26) is the dual of (25) and the result follows from (21).\nThe interpretation of SDNA as a variant of the Iterative Hessian sketch method of Pilanci & Wainwright (2014) follows immediately from the following theorem.\nTheorem 8. The output sequence {wk, \u03b1k}k\u22650 of Algorithm 3 applied on problem (25) satisfies:\nwk+1 = arg min w\u2208Rd { 1 2n \u2016S>k (A>w \u2212 b)\u20162 + \u03bb 2 \u2016w\u20162\n+ \u3008w, 1 n AISk\u03b1 k \u2212 \u03bbwk\u3009}, (27)\nwhere Sk denotes the n-by-|Sk| submatrix of the identity matrix In with columns in the random subset Sk.\nProof. We know that S>k \u2206\u03b1 k is the optimal solution of\nmin h\u2208R\u03c4\n1 2 \u2016h\u20162 + \u3008S>k (A>wk +\u03b1k\u2212 b), h\u3009+ 1 2\u03bbn \u2016ASkh\u20162\nLet \u03c4 = |Sk|. By Lemma 6, the optimal solution of\nmin w\u2208Rd\n1\n2|Sk| \u2016S>kA>w+S>k (A>wk+\u03b1k\u2212b)\u20162+\n\u03bbn\n2|Sk| \u2016w\u20162,\nis given by 1\u03bbnASkS > k\u2207\u03b1k, which equals \u03b1\u0304k+1 \u2212 \u03b1\u0304k and thus equals wk+1 \u2212 wk. Hence,\nwk+1 = arg min w\u2208Rd { 12n\u2016S > k (A >w+\u03b1k\u2212b)\u20162+\u03bb2 \u2016w\u2212w k\u20162},\nwhich is equivalent to (27) since (In)Sk = SkS > k ."}, {"heading": "6. Numerical Experiments", "text": "In our first experiment (Figure 1) we compare SDNA and our new minibatch version of SDCA on one real (mushrooms; d = 112, n = 8, 124) and one synthetic (d = 1, 024, n = 2, 048) dataset. In both cases, we used \u03bb = 1/n as the regularization parameter and g(w) = 1 2\u2016w\u2016\n2. As \u03c4 increases, SDNA requires less passes over data (epochs), while SDCA requires more passes over data. It can be shown that this behavior can be predicted from the complexity results for these two methods. The difference in performance depends on the choice of the dataset and can be quite dramatic.\nIn the second experiment (Figure 2), we investigate how much time it takes for the methods to process a single epoch, using the same datasets as before. As \u03c4 increases, SDNA does more work as the subproblems it needs to solve in each iteration involve a \u03c4 \u00d7 \u03c4 submatrix of the Hessian of the smooth part of the dual objective function. On the other hand, the work SDCA needs to do is much smaller, and does nearly does not increase with the minibatch size \u03c4 . This is because the subproblems are separable. As before, all experiments are done using a single core (however, both methods would benefit from a parallel implementation).\nFinally, in Figure 3 we put the insights gained from the previous two experiments together: we look at the perfor-\nmance of SDNA for various choices of \u03c4 by comparing runtime and duality gap error. We should expect that increasing \u03c4 would lead to faster method in terms of passes over data, but that this would also lead to slower iterations. The question is, is does the gain outweight the loss? The answer is: yes, for small enough minibatch sizes. Indeed, looking at Figure 3, we see that the runtime of SDNA improved up to the point \u03c4 = 16 for both datasets, and then starts to deteriorate. In situations where it is costly to fetch data from memory to a (fast) processor, much larger minibatch sizes would be optimal."}, {"heading": "APPENDIX: Proof of Theorem 3", "text": "It follows directly from Assumption 1 and the update rule xk+1 = xk + (hk)Sk in Algorithm 1 that:\nLHS := f(xk+1) + n\u2211 i=1 \u03c8i(x k+1 i )\u2212 f(x k)\u2212 \u2211 i/\u2208Sk \u03c8i(x k i )\n\u2264 \u3008\u2207f(xk), (hk)Sk\u3009+ 1\n2 \u3008hk,XSkhk\u3009+ \u2211 i\u2208Sk \u03c8i(x k i + h k i ).\nSince hk is defined as the minimizer of the right hand side in the last inequality, we can further bound this term by replacing hk with h = \u03bb(x\u2217 \u2212 xk) for arbitrary \u03bb \u2208 [0, 1]:\nLHS \u2264 \u03bb\u3008(\u2207f(xk))Sk , x\u2217 \u2212 xk\u3009+ \u2211 i\u2208Sk \u03c8i(x k i + \u03bb(x \u2217 i \u2212 xki )) + \u03bb2 2 \u3008x\u2217 \u2212 xk,XSk(x\u2217 \u2212 xk)\u3009. (28)\nNow we use the fact that \u03c8i is \u03b3i-strongly convex to obtain:\nF (xk+1)\u2212 F (xk) = f(xk+1) + n\u2211 i=1 \u03c8i(x k+1 i )\u2212 f(x k)\u2212 n\u2211 i=1 \u03c8i(x k i )\n(28) \u2264 \u03bb\u3008(\u2207f(xk))Sk , x\u2217 \u2212 xk\u3009+ \u03bb \u2211 i\u2208Sk [\u03c8i(x \u2217 i )\u2212 \u03c8i(xki )]\n\u2212\u03bb(1\u2212 \u03bb) 2 \u3008x\u2217 \u2212 xk,D(\u03b3)Sk(x\u2217 \u2212 xk)\u3009+ \u03bb2 2 \u3008x\u2217 \u2212 xk,XSk(x\u2217 \u2212 xk)\u3009.\nBy taking expectations in Sk on both sides of the last inequality, we see that for any \u03bb \u2208 [0, 1], the following holds:\nEk[F (xk+1)\u2212 F (xk)] \u2264 \u03bb\u03c4\nn\n( \u3008(\u2207f(xk)), x\u2217 \u2212 xk\u3009+\nn\u2211 i=1 ( \u03c8i(x \u2217 i )\u2212 \u03c8i(xki )\n))\n\u2212\u03bb(1\u2212 \u03bb) 2 \u3008x\u2217 \u2212 xk,E [ D(\u03b3)S\u0302 ] (x\u2217 \u2212 xk)\u3009+ \u03bb 2 2 \u3008x\u2217 \u2212 xk,E [ XS\u0302 ] (x\u2217 \u2212 xk)\u3009\n\u2264 \u03bb\u03c4 n\n( F (x\u2217)\u2212 F (xk)\u2212 1\n2 \u3008x\u2217 \u2212 xk,G(x\u2217 \u2212 xk)\u3009 ) + \u03bb2\n2 \u3008x\u2217 \u2212 xk,E\n[ XS\u0302 + D(\u03b3)S\u0302 ] (x\u2217 \u2212 xk)\u3009 \u2212 \u03bb\n2 \u3008x\u2217 \u2212 xk,E\n[ D(\u03b3)S\u0302 ] (x\u2217 \u2212 xk)\u3009\n\u2264 \u03bb\u03c4 n\n( F (x\u2217)\u2212 F (xk) ) \u2212 \u03bb\n2 \u3008x\u2217 \u2212 xk, \u03c4 n (D(\u03b3) + G)(x\u2217 \u2212 xk)\u3009\n+ \u03bb2\n2 \u3008x\u2217 \u2212 xk,E\n[ XS\u0302 + \u03c4\nn D(\u03b3)\n] (x\u2217 \u2212 xk)\u3009,\nwhere the second to last inequality follows from Assumption 7 and in the last one we used the fact that E[D(\u03b3)S\u0302 ] = \u03c4 nD(\u03b3). It remains to replace \u03bb by min(1, s)."}, {"heading": "APPENDIX: Proof of Lemma 5", "text": "Recall that M = 1nX, where X = 1 \u03bbnA >A.\nFor simplicity in this proof we write \u03b8 = \u03b8(S\u0302). First, by the 1-strong convexity of the function g we obtain the 1-smoothness of the function g\u2217, from which we deduce:\n\u2212\u03bbg\u2217(\u03b1\u0304k+1) + \u03bbg\u2217(\u03b1\u0304k) + \u03bb\u3008\u2207g\u2217(\u03b1\u0304k), \u03b1\u0304k+1 \u2212 \u03b1\u0304k\u3009 \u2265 \u2212\u03bb 2 \u3008\u03b1\u0304k+1 \u2212 \u03b1\u0304k, \u03b1\u0304k+1 \u2212 \u03b1\u0304k\u3009.\nNow we replace \u2207g\u2217(\u03b1\u0304k) by wk and \u03b1\u0304 by 1\u03bbnA\u03b1 to obtain:\nD(\u03b1k+1)\u2212D(\u03b1k) \u2265 1 n \u2211 i\u2208Sk [ \u2212\u03c6\u2217i (\u2212\u03b1k+1i ) + \u03c6 \u2217 i (\u2212\u03b1ki ) ] \u2212 1 n \u3008A>wk, \u03b1k+1 \u2212 \u03b1k\u3009 \u2212 1 2\u03bbn2 (\u03b1k+1 \u2212 \u03b1k)>A>A(\u03b1k+1 \u2212 \u03b1k)\n= max h\u2208Rn\n{ 1\nn \u2211 i\u2208Sk [ \u2212\u03c6\u2217i (\u2212\u03b1ki \u2212 hi) + \u03c6\u2217i (\u2212\u03b1ki ) ] \u2212 1 n \u3008(A>wk)Sk , h\u3009 \u2212 1 2n h>XSkh\n} ,\nwhere in the last equality we used the dual update rules in Algorithm 3, as well as relations (3) and (2). Therefore, for arbitrary h \u2208 Rn,\nEk[D(\u03b1k+1)\u2212D(\u03b1k)] \u2265 Ek\n[ 1\nn \u2211 i\u2208Sk [ \u2212\u03c6\u2217i (\u2212\u03b1ki \u2212 hi) + \u03c6\u2217i (\u2212\u03b1ki )\n]] \u2212 Ek [ 1\nn \u3008(A>wk)Sk , h\u3009 \u2212\n1\n2n h>XSkh\n]\n= 1\nn n\u2211 i=1 pi [ \u2212\u03c6\u2217i (\u2212\u03b1ki \u2212 hi) + \u03c6\u2217i (\u2212\u03b1ki )\u2212 (a>i wk)hi ] \u2212 1 2n h> E[XS\u0302 ]h.\nLet uk \u2208 Rn such that uki = \u2207\u03c6i(a>i wk) \u2208 R for all i \u2208 [n]. Let s = (s1, . . . , sn) \u2208 [0, 1]n with si = \u03b8p \u22121 i for all i \u2208 [n], where \u03b8 is given in (22). By using hi = \u2212si(\u03b1ki + uki ) for all i \u2208 [n] in (29), we get:\nEk[D(\u03b1k+1)\u2212D(\u03b1k)] \u2265 1\nn n\u2211 i=1 pi[\u2212\u03c6\u2217i ( \u2212(1\u2212 si)\u03b1ki + siuki ) + \u03c6\u2217i (\u2212\u03b1ki ) + si\u3008a>i wk, \u03b1ki + uki \u3009]\n\u2212 1 2n (\u03b1k + uk)>D(s)E[XS\u0302 ]D(s)(\u03b1 k + uk)\nFrom \u03b3-strong convexity of the functions \u03c6\u2217i we deduce that:\n\u2212\u03c6\u2217i ((1\u2212 si)(\u2212\u03b1ki ) + siuki ) + \u03c6\u2217i (\u2212\u03b1ki ) \u2265 si\u03c6\u2217i (\u2212\u03b1ki )\u2212 si\u03c6\u2217i (uki ) + \u03b3si(1\u2212 si)\n2 |uki + \u03b1ki |2.\nConsequently,\nEk[D(\u03b1k+1)\u2212D(\u03b1k)] \u2265 1\nn n\u2211 i=1 pisi [ \u03c6\u2217i (\u2212\u03b1ki )\u2212 \u03c6\u2217i (uki ) + \u3008a>i wk, \u03b1ki + uki \u3009 ] + 1 n n\u2211 i=1 \u03b3pisi(1\u2212 si) 2 |uki + \u03b1ki |2\n\u2212 1 2n (\u03b1k + uk)>D(s)E[XS\u0302 ]D(s)(\u03b1 k + uk)\n= \u03b8\nn n\u2211 i=1 [ \u03c6\u2217i (\u2212\u03b1ki ) + \u03c6i(a>i wk) + \u3008a>i wk, \u03b1ki \u3009 ] + \u03b3\u03b8 2n \u3008\u03b1k + uk, (I\u2212D(s))(\u03b1k + uk)\u3009\n\u2212 1 2n \u3008\u03b1k + uk,D(s)E[XS\u0302 ]D(s)(\u03b1 k + uk)\u3009\nwhere the equality follows from uki = \u2207\u03c6i(a>i wk). Next, by the definition of \u03b8 in (22), we know that:\n\u03b3I \u03b8\u03b3D(p\u22121) + \u03b8 \u03bbn D(v \u25e6 p\u22121)\n= \u03b3D(s) + 1\n\u03b8\u03bbn D(s)D(v \u25e6 p)D(s)\n(23) \u03b3D(s) + 1\n\u03b8 D(s)E[XS\u0302 ]D(s).\nFinally, it follows that\nEk[D(\u03b1k+1)\u2212D(\u03b1k)] \u2265 \u03b8\nn n\u2211 i=1 [ \u03c6\u2217i (\u2212\u03b1ki ) + \u03c6i(a>i wk) + \u3008a>i wk, \u03b1ki \u3009 ] = \u03b8(P (wk)\u2212D(\u03b1k))."}, {"heading": "APPENDIX: More insight into the relationship between \u03c32 and \u03c33", "text": "In the main text we have shown that \u03c32 \u2265 \u03c33, where \u03c32 is the rate of Method 2 and \u03c33 is the rate of Method 3: NSync (Richta\u0301rik & Taka\u0301c\u030c, 2013b). In this section we give a more detailed description of the relationship between these two quantities in the case when S\u0302 is the \u03c4 -nice sampling (Richta\u0301rik & Taka\u0301c\u030c, 2012). That is, S\u0302 picks subsets of [n] of cardinality \u03c4 , uniformly at random. For this sampling,\npi := Prob(i \u2208 S\u0302) = \u03c4\nn .\nProposition 3. Suppose that G = M and S\u0302 be the \u03c4 -nice sampling. Then there exists \u03b2 \u2208 [1, \u03c4 ] such that one can choose vi = \u03b2Mi,i and\n\u03c32 = \u03b2\u03c33\n(1\u2212 \u03c4\u22121n\u22121 ) + n \u03c4 \u03c4\u22121 n\u22121\u03b2\u03c33\n.\nProof. As explained in (Richta\u0301rik & Taka\u0301c\u030c, 2012), (10) is always true if we take vi = \u03b2Mi,i with \u03b2 = \u03c4 but smaller values (leading to a faster algorithm) may be computable if the problem exhibits a property called \u201cpartial separability\u201d.\nLet us denote by D the diagonal matrix whose entries are the diagonal entries of M.\n(M[S\u0302])i,i = { Mi,i = Di,i if i \u2208 S\u0302 (probability \u03c4n ) 0 otherwise\n(M[S\u0302,S\u0302])i,j = { Mi,j if i \u2208 S\u0302 and j \u2208 S\u0302 (probability \u03c4(\u03c4\u22121)n(n\u22121) ) 0 otherwise.\nHence,\nE[MS\u0302 ] = \u03c4\nn D + \u03c4(\u03c4 \u2212 1) n(n\u2212 1) (M\u2212D) = \u03c4 n\n( (1\u2212 \u03c4 \u2212 1\nn\u2212 1 )D + \u03c4 \u2212 1 n\u2212 1\nM ) .\nLet us denote A = M\u22121/2DM\u22121/2 and \u03b1 = \u03c4\u22121n\u22121 .\n\u03c33 (13) =\n\u03c4 n \u03b2\u22121\u03bbmin(M 1/2D\u22121M1/2) = \u03c4 n \u03b2\u22121\n( \u03bbmax(A) )\u22121\n\u03c32 (12) =\n\u03c42 n2 \u03bbmin(M 1/2(E[MS\u0302 ]) \u22121M1/2) = \u03c42 n2 \u03bbmin(M 1/2n \u03c4 ((1\u2212 \u03b1)D + \u03b1M)\u22121M1/2)\n= \u03c4\nn\n( \u03bbmax(M \u22121/2((1\u2212 \u03b1)D + \u03b1M)M\u22121/2) )\u22121 = \u03c4\nn\n( \u03bbmax((1\u2212 \u03b1)A + \u03b1I) )\u22121 But we have \u03bbmax((1\u2212 \u03b1)A + \u03b1I) = (1\u2212 \u03b1)\u03bbmax(A) + \u03b1, so\n\u03c4 n\u03c32 = (1\u2212 \u03b1) \u03c4 n\u03b2\u03c33 + \u03b1 n\u03c32 \u03c4 = 1\n(1\u2212 \u03b1) \u03c4n\u03b2\u03c33 + \u03b1\n\u03c32 = \u03c33\n(1\u2212 \u03b1)\u03b2\u22121 + \u03b1n\u03c4 \u03c33 =\n\u03b2\u03c33\n(1\u2212 \u03c4\u22121n\u22121 ) + \u03c4\u22121 n\u22121 n \u03c4 \u03b2\u03c33\nNote that if \u03c33 is small, then \u03c32 is of the order of \u03b2\u03c331\u2212 \u03c4\u22121n\u22121 > \u03b2\u03c33 ."}], "references": [{"title": "Sgdqn: Careful quasi-newton stochastic gradient descent", "author": ["Bordes", "Antoine", "Bottou", "L\u00e9on", "Gallinari", "Patrick"], "venue": "JMLR, 10:1737\u20131754,", "citeRegEx": "Bordes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2009}, {"title": "A stochastic quasi-newton method for largescale optimization", "author": ["R.H. Byrd", "S.L. Hansen", "Nocedal", "Jorge", "Singer", "Yoram"], "venue": null, "citeRegEx": "Byrd et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Byrd et al\\.", "year": 2014}, {"title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["Defazio", "Aaron", "Bach", "Francis", "Lacoste-Julien", "Simon"], "venue": null, "citeRegEx": "Defazio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Defazio et al\\.", "year": 2014}, {"title": "Accelerated, parallel and proximal coordinate descent", "author": ["Fercoq", "Olivier", "Richt\u00e1rik", "Peter"], "venue": "SIAM Journal on Optimization (after minor revision),", "citeRegEx": "Fercoq et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fercoq et al\\.", "year": 2013}, {"title": "Smooth minimization of nonsmooth functions by parallel coordinate descent", "author": ["Fercoq", "Olivier", "Richt\u00e1rik", "Peter"], "venue": null, "citeRegEx": "Fercoq et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fercoq et al\\.", "year": 2013}, {"title": "Fast distributed coordinate descent for minimizing non-strongly convex losses", "author": ["Fercoq", "Olivier", "Qu", "Zheng", "Richt\u00e1rik", "Peter", "Tak\u00e1\u010d", "Martin"], "venue": "IEEE International Workshop on Machine Learning for Signal Processing,", "citeRegEx": "Fercoq et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fercoq et al\\.", "year": 2014}, {"title": "Robust block coordinate descent", "author": ["Fountoulakis", "Kimon", "Tappenden", "Rachael"], "venue": null, "citeRegEx": "Fountoulakis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fountoulakis et al\\.", "year": 2014}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Johnson", "Rie", "Zhang", "Tong"], "venue": "In NIPS,", "citeRegEx": "Johnson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2013}, {"title": "S2GD: Semistochastic gradient descent methods", "author": ["Kone\u010dn\u00fd", "Jakub", "Richt\u00e1rik", "Peter"], "venue": null, "citeRegEx": "Kone\u010dn\u00fd et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kone\u010dn\u00fd et al\\.", "year": 2014}, {"title": "mS2GD: Mini-batch semi-stochastic gradient descent in the proximal setting", "author": ["Kone\u010dn\u00fd", "Jakub", "Lu", "Jie", "Richt\u00e1rik", "Peter", "Tak\u00e1\u010d", "Martin"], "venue": null, "citeRegEx": "Kone\u010dn\u00fd et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kone\u010dn\u00fd et al\\.", "year": 2014}, {"title": "An accelerated proximal coordinate gradient method and its application to regularized empirical risk minimization", "author": ["Lin", "Qihang", "Lu", "Zhaosong", "Xiao"], "venue": "Technical Report MSR-TR-2014-94, Microsoft Research,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Incremental majorization-minimization optimization with application to large-scale machine learning", "author": ["Mairal", "Julien"], "venue": null, "citeRegEx": "Mairal and Julien.,? \\Q2014\\E", "shortCiteRegEx": "Mairal and Julien.", "year": 2014}, {"title": "Iterative Hessian sketch: Fast and accurate solution approximation for constrained least-squares", "author": ["Pilanci", "Mert", "Wainwright", "Martin J"], "venue": null, "citeRegEx": "Pilanci et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pilanci et al\\.", "year": 2014}, {"title": "Coordinate descent with arbitrary sampling I: Algorithms and complexity", "author": ["Qu", "Zheng", "Richt\u00e1rik", "Peter"], "venue": null, "citeRegEx": "Qu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qu et al\\.", "year": 2014}, {"title": "Coordinate descent methods with arbitrary sampling II: Expected separable overapproximation", "author": ["Qu", "Zheng", "Richt\u00e1rik", "Peter"], "venue": null, "citeRegEx": "Qu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qu et al\\.", "year": 2014}, {"title": "Randomized dual coordinate ascent with arbitrary sampling", "author": ["Qu", "Zheng", "Richt\u00e1rik", "Peter", "Zhang", "Tong"], "venue": null, "citeRegEx": "Qu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qu et al\\.", "year": 2014}, {"title": "Distributed coordinate descent method for learning with big data", "author": ["Richt\u00e1rik", "Peter", "Tak\u00e1\u010d", "Martin"], "venue": null, "citeRegEx": "Richt\u00e1rik et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richt\u00e1rik et al\\.", "year": 2013}, {"title": "On optimal probabilities in stochastic coordinate descent methods", "author": ["Richt\u00e1rik", "Peter", "Tak\u00e1\u010d", "Martin"], "venue": null, "citeRegEx": "Richt\u00e1rik et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richt\u00e1rik et al\\.", "year": 2013}, {"title": "Parallel coordinate descent methods for big data optimization problems", "author": ["Richt\u00e1rik", "Peter", "Tak\u00e1\u010d", "Martin"], "venue": "Mathematical Programming (after minor revision),", "citeRegEx": "Richt\u00e1rik et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Richt\u00e1rik et al\\.", "year": 2012}, {"title": "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function", "author": ["Richt\u00e1rik", "Peter", "Tak\u00e1\u010d", "Martin"], "venue": "Mathematical Programming,", "citeRegEx": "Richt\u00e1rik et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Richt\u00e1rik et al\\.", "year": 2014}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Schmidt", "Mark", "Le Roux", "Nicolas", "Bach", "Francis"], "venue": null, "citeRegEx": "Schmidt et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2013}, {"title": "A stochastic quasi-newton method for online convex optimization", "author": ["Schraudolph", "Nicol N", "Yu", "Jin", "G\u00fcnter", "Simon"], "venue": "In AISTATS, pp", "citeRegEx": "Schraudolph et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Schraudolph et al\\.", "year": 2007}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["Shalev-Shwartz", "Shai", "Ben-David"], "venue": null, "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2014}, {"title": "Accelerated minibatch stochastic dual coordinate ascent", "author": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"], "venue": "In NIPS, pp", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2013}, {"title": "Accelerated minibatch stochastic dual coordinate ascent", "author": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"], "venue": "In NIPS, pp", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2013}, {"title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "author": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"], "venue": null, "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2013}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2013}, {"title": "Pegasos: Primal estimated sub-gradient solver for SVM", "author": ["Shalev-Shwartz", "Shai", "Singer", "Yoram", "Srebro", "Nati", "Cotter", "Andrew"], "venue": "Mathematical Programming,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Fast large-scale optimization by unifying stochastic gradient and quasi-newton methods", "author": ["Sohl-Dickstein", "Jascha", "Poole", "Ben", "Ganguli", "Surya"], "venue": "In ICML,", "citeRegEx": "Sohl.Dickstein et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sohl.Dickstein et al\\.", "year": 2014}, {"title": "Mini-batch primal and dual methods for SVMs", "author": ["Tak\u00e1\u010d", "Martin", "Bijral", "Avleen", "Richt\u00e1rik", "Peter", "Srebro", "Nathan"], "venue": "In ICML,", "citeRegEx": "Tak\u00e1\u010d et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tak\u00e1\u010d et al\\.", "year": 2013}, {"title": "Inexact block coordinate descent method: complexity and preconditioning", "author": ["Tappenden", "Rachael", "Richt\u00e1rik", "Peter", "Gondzio", "Jacek"], "venue": null, "citeRegEx": "Tappenden et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tappenden et al\\.", "year": 2013}, {"title": "Separable approximations and decomposition methods for the augmented lagrangian", "author": ["Tappenden", "Rachael", "Richt\u00e1rik", "Peter", "B\u00fcke", "Burak"], "venue": "Optimization Methods and Software,", "citeRegEx": "Tappenden et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tappenden et al\\.", "year": 2014}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["Xiao", "Lin", "Zhang", "Tong"], "venue": null, "citeRegEx": "Xiao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2014}, {"title": "Stochastic optimization with importance sampling", "author": ["Zhao", "Peilin", "Zhang", "Tong"], "venue": null, "citeRegEx": "Zhao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 27, "context": "State-of-the-art optimization methods for ERM include i) stochastic (sub)gradient descent (Shalev-Shwartz et al., 2011; Tak\u00e1\u010d et al., 2013), ii) methods based on stochastic estimates of the gradient with diminishing variance such as SAG (Schmidt et al.", "startOffset": 90, "endOffset": 139}, {"referenceID": 29, "context": "State-of-the-art optimization methods for ERM include i) stochastic (sub)gradient descent (Shalev-Shwartz et al., 2011; Tak\u00e1\u010d et al., 2013), ii) methods based on stochastic estimates of the gradient with diminishing variance such as SAG (Schmidt et al.", "startOffset": 90, "endOffset": 139}, {"referenceID": 20, "context": ", 2013), ii) methods based on stochastic estimates of the gradient with diminishing variance such as SAG (Schmidt et al., 2013), SVRG (Johnson & Zhang, 2013), S2GD (Kone\u010dn\u00fd & Richt\u00e1rik, 2014), proxSVRG (Xiao & Zhang, 2014), MISO (Mairal, 2014), SAGA (Defazio et al.", "startOffset": 105, "endOffset": 127}, {"referenceID": 2, "context": ", 2013), SVRG (Johnson & Zhang, 2013), S2GD (Kone\u010dn\u00fd & Richt\u00e1rik, 2014), proxSVRG (Xiao & Zhang, 2014), MISO (Mairal, 2014), SAGA (Defazio et al., 2014), minibatch S2GD (Kone\u010dn\u00fd et al.", "startOffset": 130, "endOffset": 152}, {"referenceID": 29, "context": ", 2014b), and iii) variants of stochastic dual coordinate ascent (Shalev-Shwartz & Zhang, 2013d; Zhao & Zhang, 2014; Tak\u00e1\u010d et al., 2013; Shalev-Shwartz & Zhang, 2013b;a; Lin et al., 2014; Qu et al., 2014; Shalev-Shwartz & Zhang, 2013c).", "startOffset": 65, "endOffset": 235}, {"referenceID": 10, "context": ", 2014b), and iii) variants of stochastic dual coordinate ascent (Shalev-Shwartz & Zhang, 2013d; Zhao & Zhang, 2014; Tak\u00e1\u010d et al., 2013; Shalev-Shwartz & Zhang, 2013b;a; Lin et al., 2014; Qu et al., 2014; Shalev-Shwartz & Zhang, 2013c).", "startOffset": 65, "endOffset": 235}, {"referenceID": 13, "context": ", 2014b), and iii) variants of stochastic dual coordinate ascent (Shalev-Shwartz & Zhang, 2013d; Zhao & Zhang, 2014; Tak\u00e1\u010d et al., 2013; Shalev-Shwartz & Zhang, 2013b;a; Lin et al., 2014; Qu et al., 2014; Shalev-Shwartz & Zhang, 2013c).", "startOffset": 65, "endOffset": 235}, {"referenceID": 31, "context": "For example, methods based on running coordinate ascent in the dual such as those mentioned above and also (Richt\u00e1rik & Tak\u00e1\u010d, 2014; 2012; Fercoq & Richt\u00e1rik, 2013b; Tappenden et al., 2014; Richt\u00e1rik & Tak\u00e1\u010d, 2013a;b; Fercoq & Richt\u00e1rik, 2013a; Fercoq et al., 2014; Qu et al., 2014; Qu & Richt\u00e1rik, 2014a) use curvature information contained in the diagonal of a bound on the Hessian matrix.", "startOffset": 107, "endOffset": 305}, {"referenceID": 5, "context": "For example, methods based on running coordinate ascent in the dual such as those mentioned above and also (Richt\u00e1rik & Tak\u00e1\u010d, 2014; 2012; Fercoq & Richt\u00e1rik, 2013b; Tappenden et al., 2014; Richt\u00e1rik & Tak\u00e1\u010d, 2013a;b; Fercoq & Richt\u00e1rik, 2013a; Fercoq et al., 2014; Qu et al., 2014; Qu & Richt\u00e1rik, 2014a) use curvature information contained in the diagonal of a bound on the Hessian matrix.", "startOffset": 107, "endOffset": 305}, {"referenceID": 13, "context": "For example, methods based on running coordinate ascent in the dual such as those mentioned above and also (Richt\u00e1rik & Tak\u00e1\u010d, 2014; 2012; Fercoq & Richt\u00e1rik, 2013b; Tappenden et al., 2014; Richt\u00e1rik & Tak\u00e1\u010d, 2013a;b; Fercoq & Richt\u00e1rik, 2013a; Fercoq et al., 2014; Qu et al., 2014; Qu & Richt\u00e1rik, 2014a) use curvature information contained in the diagonal of a bound on the Hessian matrix.", "startOffset": 107, "endOffset": 305}, {"referenceID": 30, "context": "Block coordinate descent methods, when equipped with suitable data-dependent norms for the blocks, use information contained in the block diagonal of the Hessian (Tappenden et al., 2013).", "startOffset": 162, "endOffset": 186}, {"referenceID": 0, "context": ", 2013), SVRG (Johnson & Zhang, 2013), S2GD (Kone\u010dn\u00fd & Richt\u00e1rik, 2014), proxSVRG (Xiao & Zhang, 2014), MISO (Mairal, 2014), SAGA (Defazio et al., 2014), minibatch S2GD (Kone\u010dn\u00fd et al., 2014a), S2CD (Kone\u010dn\u00fd et al., 2014b), and iii) variants of stochastic dual coordinate ascent (Shalev-Shwartz & Zhang, 2013d; Zhao & Zhang, 2014; Tak\u00e1\u010d et al., 2013; Shalev-Shwartz & Zhang, 2013b;a; Lin et al., 2014; Qu et al., 2014; Shalev-Shwartz & Zhang, 2013c). There have been several attempts at designing methods that combine randomization with the use of curvature (secondorder) information. For example, methods based on running coordinate ascent in the dual such as those mentioned above and also (Richt\u00e1rik & Tak\u00e1\u010d, 2014; 2012; Fercoq & Richt\u00e1rik, 2013b; Tappenden et al., 2014; Richt\u00e1rik & Tak\u00e1\u010d, 2013a;b; Fercoq & Richt\u00e1rik, 2013a; Fercoq et al., 2014; Qu et al., 2014; Qu & Richt\u00e1rik, 2014a) use curvature information contained in the diagonal of a bound on the Hessian matrix. Block coordinate descent methods, when equipped with suitable data-dependent norms for the blocks, use information contained in the block diagonal of the Hessian (Tappenden et al., 2013). A more direct route to incorporating curvature information was taken by Schraudolph et al. (2007) in their stochastic L-BFGS method, by Byrd et al.", "startOffset": 131, "endOffset": 1263}, {"referenceID": 0, "context": "(2007) in their stochastic L-BFGS method, by Byrd et al. (2014) and Sohl-Dickstein et al.", "startOffset": 45, "endOffset": 64}, {"referenceID": 0, "context": "(2007) in their stochastic L-BFGS method, by Byrd et al. (2014) and Sohl-Dickstein et al. (2014) in their stochastic quasi-Newton methods and by Fountoulakis & Tappenden (2014) who proposed a stochastic block coordinate descent methods.", "startOffset": 45, "endOffset": 97}, {"referenceID": 0, "context": "(2007) in their stochastic L-BFGS method, by Byrd et al. (2014) and Sohl-Dickstein et al. (2014) in their stochastic quasi-Newton methods and by Fountoulakis & Tappenden (2014) who proposed a stochastic block coordinate descent methods.", "startOffset": 45, "endOffset": 177}, {"referenceID": 0, "context": "An exception in this regard is the work of Bordes et al. (2009), who give a O(1/ ) complexar X iv :1 50 2.", "startOffset": 43, "endOffset": 64}, {"referenceID": 13, "context": "In particular, we show that the expected duality gap decreases at a geometric rate which i) is better than that of SDCA-like methods such as SDCA (ShalevShwartz & Zhang, 2013d) and QUARTZ (Qu et al., 2014), and ii) improves with increasing minibatch size.", "startOffset": 188, "endOffset": 205}, {"referenceID": 13, "context": "In particular, we show that the expected duality gap decreases at a geometric rate which i) is better than that of SDCA-like methods such as SDCA (ShalevShwartz & Zhang, 2013d) and QUARTZ (Qu et al., 2014), and ii) improves with increasing minibatch size. This improvement does not come for free: as we increase the minibatch size, the subproblems grow in size as they involve larger portions of the Hessian. We find through experiments that for some, especially dense problems, even relatively small minibatch sizes lead to dramatic speedups in actual runtime. We show that in the case of quadratic loss, and when viewed as a primal method, SDNA can be interpreted as a variant of the recently introduced Iterative Hessian Sketch algorithm (Pilanci & Wainwright, 2014). En route to developing SDNA which we describe in Section 4, we also develop several other new algorithms: two in Section 2 (where we focus on smooth problems), one in Section 3 (where we focus on composite problems). Besides SDNA, we also develop and analyze a novel minibatch variant of SDCA in Section 4, for the sake of finding suitable method to compare SDNA to. SDNA is equivalent to applying the new method developed in Section 3 to the dual of the ERM problem. However, as we are mainly interested in solving the ERM (primal) problem, we additionally prove that the expected duality gap decreases at a geometric rate. Our technique for doing this is a variant of the one use by Shalev-Shwartz & Zhang (2013d), but generalized to an arbitrary sampling.", "startOffset": 189, "endOffset": 1487}, {"referenceID": 26, "context": "Tak\u00e1\u010d et al. (2013) developed such a method but in the special case of hinge-loss (which is not smooth and hence does not fit our setup).", "startOffset": 0, "endOffset": 20}, {"referenceID": 26, "context": "Tak\u00e1\u010d et al. (2013) developed such a method but in the special case of hinge-loss (which is not smooth and hence does not fit our setup). Shalev-Shwartz & Zhang (2013b) studied minibatching but in conjunction with acceleration and the QUARTZ method of Qu et al.", "startOffset": 0, "endOffset": 169}, {"referenceID": 13, "context": "Shalev-Shwartz & Zhang (2013b) studied minibatching but in conjunction with acceleration and the QUARTZ method of Qu et al. (2014), which has been analyzed for an arbitrary sampling \u015c, uses a different primal update than SDNA.", "startOffset": 114, "endOffset": 131}], "year": 2015, "abstractText": "We propose a new algorithm for minimizing regularized empirical loss: Stochastic Dual Newton Ascent (SDNA). Our method is dual in nature: in each iteration we update a random subset of the dual variables. However, unlike existing methods such as stochastic dual coordinate ascent, SDNA is capable of utilizing all curvature information contained in the examples, which leads to striking improvements in both theory and practice \u2013 sometimes by orders of magnitude. In the special case when an L2-regularizer is used in the primal, the dual problem is a concave quadratic maximization problem plus a separable term. In this regime, SDNA in each step solves a proximal subproblem involving a random principal submatrix of the Hessian of the quadratic function; whence the name of the method. If, in addition, the loss functions are quadratic, our method can be interpreted as a novel variant of the recently introduced Iterative Hessian Sketch.", "creator": "LaTeX with hyperref package"}}}