{"id": "1502.02643", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2015", "title": "Random Coordinate Descent Methods for Minimizing Decomposable Submodular Functions", "abstract": "Submodular function minimization is a fundamental optimization problem that arises in several applications in machine learning and computer vision. The problem is known to be solvable in polynomial time, but general purpose algorithms have high running times and are unsuitable for large-scale problems. Recent work have used convex optimization techniques to obtain very practical algorithms for minimizing functions that are sums of ``simple\" functions and not sum of ``complex\" functions.\n\n\n\n\nIn sum, convex optimization is one of the fundamental optimizations problems in computer vision. This optimization problem is known to be solvable in polynomial time, but general purpose algorithms have high running times and are unsuitable for large-scale problems. Recent work have used convex optimization techniques to obtain very practical algorithms for minimizing functions that are sums of ``simple\" functions and not sum of ``complex\" functions.\nIn a related category, concurrency problems, general problems, and general problems, general solutions, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general solvable problems, general", "histories": [["v1", "Mon, 9 Feb 2015 20:31:18 GMT  (390kb,D)", "http://arxiv.org/abs/1502.02643v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["alina ene", "huy l nguyen"], "accepted": true, "id": "1502.02643"}, "pdf": {"name": "1502.02643.pdf", "metadata": {"source": "CRF", "title": "Random Coordinate Descent Methods for Minimizing Decomposable Submodular Functions", "authors": ["Alina Ene", "Huy L. Nguy\u1ec5n"], "emails": ["A.Ene@dcs.warwick.ac.uk", "hlnguyen@cs.princeton.edu"], "sections": [{"heading": "1 Introduction", "text": "Over the past few decades, there has been a significant progress on minimizing submodular functions, leading to several polynomial time algorithms for the problem [4, 17, 5, 3, 14]. Despite this intense focus, the running times of these algorithms are high-order polynomials in the size of the data and designing faster algorithms remains a central and challenging direction in submodular optimization.\nAt the same time, technological advances have made it possible to capture and store data at an ever increasing rate and level of detail. A natural consequence of this \u201cbig data\" phenomenon is that machine learning applications need to cope with data that is quite large and it is growing at a fast pace. Thus there is an increasing need for algorithms that are fast and scalable.\nThe general purpose algorithms for submodular minimization are designed to provide worst-case guarantees even in settings where the only structure that one can exploit is submodularity. At the other extreme, graph cut algorithms are very efficient but they cannot handle more general submodular functions. In many applications, the functions strike a middle ground between these two extremes and it is becoming increasingly more important to use their special structure to obtain significantly faster algorithms.\nFollowing [8, 19, 6, 12], we consider the problem of minimizing decomposable submodular functions that can be expressed as a sum of simple functions. We use the term simple to refer to functions F for which there\nar X\niv :1\n50 2.\n02 64\n3v 1\n[ cs\n.L G\nis an efficient algorithm for minimizing F + w, where w is a linear function. We assume that we are given black-box access to these minimization procedures for simple functions.\nDecomposable functions are a fairly rich class of functions and they arise in several applications in machine learning and computer vision. For example, they model higher-order potential functions for MAP inference in Markov random fields, the cost functions in SVM models for which the examples have only a small number of features, and the graph and hypergraph cut functions in image segmentation.\nThe recent work of [6, 8, 19] has developed several algorithms with very good empirical performance that exploit the special structure of decomposable functions. In particular, [6] have shown that the problem of minimizing decomposable submodular functions can be formulated as a distance minimization problem between two polytopes. This formulation, when coupled with powerful convex optimization techniques such as gradient descent or projection methods, it yields algorithms that are very fast in practice and very simple to implement [6].\nOn the theoretical side, the convergence behaviour of these methods is not very well understood. Very recently, Nishihara et al. [12] have made a significant progress in this direction. Their work shows that the classical alternating projections method, when applied to the distance minimization formulation, converges at a linear rate.\nOur contributions. In this work, we use random coordinate descent methods in order to obtain algorithms for minimizing decomposable submodular functions with faster convergence rates and cheaper iteration costs. We analyze a standard and an accelerated random coordinate descent algorithm and we show that they achieve linear convergence rates. Compared to alternating projection methods, our algorithms do not rely on full-dimensional vector operations and they are faster by a factor equal to the number of simple functions. Moreover, our accelerated algorithm converges in a much smaller number of iterations. We experimentally evaluate our algorithms on image segmentation tasks and we show that they perform very well and they converge much faster than the alternating projection method.\nSubmodular minimization. The first polynomial time algorithm for submodular optimization was obtained by Gr\u00f6tschel et al. [4] using the ellipsoid method. There are several combinatorial algorithms for the problem [17, 5, 3, 14]. Among the combinatorial methods, Orlin\u2019s algorithm [14] achieves the best time complexity of O(n5T +n6), where n is the size of the ground set and T is the maximum amount of time it takes to evaluate the function. Several algorithms have been proposed for minimizing decomposable submodular functions [19, 8, 6, 12]. Stobbe and Krause [19] use gradient descent methods with sublinear convergence rates for minimizing sums of concave functions applied to linear functions. Nishihara et al. [12] give an algorithm based on alternating projections that achieves a linear convergence rate."}, {"heading": "1.1 Preliminaries and Background", "text": "Let V be a finite ground set of size n; without loss of generality, V = {1, 2, . . . , n}. We view each point w \u2208 Rn as a modular set function w(A) = \u2211 i\u2208A wi on the ground set V .\nA set function F : 2V \u2192 R is submodular if F (A)+F (B) \u2265 F (A\u2229B)+F (A\u222aB) for any two sets A,B \u2286 V . A set function Fi : 2V \u2192 R is simple if there is a fast subroutine for minimizing Fi + w for any modular function w \u2208 Rn.\nIn this paper, we consider the problem of minimizing a submodular function F : 2V \u2192 R of the form F = \u2211r i=1 Fi, where each function Fi is a simple submodular set function:\nmin A\u2286V F (A) \u2261 min A\u2286V r\u2211 i=1 Fi(A) (DSM)\nWe assume without loss of generality that the function F is normalized, i.e., F (\u2205) = 0. Additionally,\nwe assume we are given black-box access to oracles for minimizing Fi + w for each function Fi in the decomposition and each w \u2208 Rn.\nThe base polytope B(F ) of F is defined as follows.\nB(F ) = {w \u2208 Rn | w(A) \u2264 F (A) for all A \u2286 V, w(V ) = F (V )}\nThe discrete problem (DSM)1 admits an exact convex programming relaxation based on the Lov\u00e1sz extension of a submodular function. The Lov\u00e1sz extension f of F can be written as the support function of the base polytope B(F ):\nf(x) = max w\u2208B(F ) \u3008w, x\u3009 \u2200x \u2208 Rn\nEven though the base polytope B(F ) has exponentially many vertices, the Lov\u00e1sz extension f can be evaluated efficiently using the greedy algorithm of Edmonds (see for example [18]). Given any point x \u2208 Rn, Edmonds\u2019 algorithm evaluates f(x) using O(n logn) \u00d7 T time, where T is the time needed to evaluate the submodular function F .\nLov\u00e1sz showed that a set function F is submodular if and only if its Lov\u00e1sz extension f is convex [9]. Thus we can relax the problem of minimizing F to the following non-smooth convex optimization problem:\nmin x\u2208[0,1]n f(x) \u2261 min x\u2208[0,1]n r\u2211 i=1 fi(x)\nwhere fi is the Lov\u00e1sz extension of Fi.\nThe relaxation above is exact. Given a fractional solution x to the Lov\u00e1sz Relaxation, the best threshold set of x has cost at most f(x).\nAn important drawback of the Lov\u00e1sz relaxation is that its objective function is not smooth. Following previous work [6, 12], we consider a proximal version of the problem (\u2016\u00b7 \u2016 denotes the `2-norm):\nmin x\u2208Rn ( f(x) + 12 \u2016x\u2016 2 ) \u2261 min x\u2208Rn r\u2211 i=1 ( fi(x) + 1 2r \u2016x\u2016 2 )\n(Proximal)\nGiven an optimal solution x to the proximal problem minx\u2208Rn ( f(x) + 12 \u2016x\u2016 2 ), we can construct an optimal solution to the discrete problem (DSM) by thresholding x at zero; more precisely, the set {v \u2208 V : x(v) \u2265 0} is an optimal solution to (DSM) (Proposition 8.6 in [1]).\nLemma 1 ([6]). The dual of the proximal problem\nmin x\u2208Rn r\u2211 i=1 ( fi(x) + 1 2r \u2016x\u2016 2 )\nis the problem\nmax y(1)\u2208B(F1),...,y(r)\u2208B(Fr) \u221212 \u2225\u2225\u2225\u2225\u2225 r\u2211 i=1 y(i) \u2225\u2225\u2225\u2225\u2225 2\nThe primal and dual variables are linked as x = \u2212 \u2211r i=1 y (i).\nLemma 1 was proved in [6]; we include a proof in Section A for completeness. 1DSM stands for decomposable submodular function minimization.\nRCDM Algorithm for (Prox-DSM) \u3008\u3008We can take the initial point y0 to be 0\u3009\u3009 Start with y0 = (y(1)0 , . . . , y (r) 0 ) \u2208 Y In each iteration k (k \u2265 0) Pick an index ik \u2208 {1, 2, . . . , r} uniformly at random \u3008\u3008Update the block ik\u3009\u3009 y\n(ik) k+1 \u2190 arg min\ny\u2208B(Fik )\n(\u2329 \u2207ikg(yk), y \u2212 y (ik) k \u232a +Lik2\n\u2225\u2225\u2225y \u2212 y(ik)k \u2225\u2225\u22252 )\nFigure 1: Random block coordinate descent method for (Prox-DSM). It finds a solution to (Prox-DSM)\ngiven access to an oracle for miny\u2208B(Fi)\n(\n\u3008y, a\u3009+ \u2016y\u20162\n)\n.\nWe write the dual proximal problem in the following equivalent form:\nmin y(1)\u2208B(F1),...,y(r)\u2208B(Fr) \u2225\u2225\u2225\u2225\u2225 r\u2211 i=1 y(i) \u2225\u2225\u2225\u2225\u2225 2\n(Prox-DSM)\nIt follows from the discussion above that, given an optimal solution y = (y(1), . . . , y(r)) to (Prox-DSM), we can recover an optimal solution to (DSM) by thresholding x = \u2212 \u2211r i=1 y (i) at zero."}, {"heading": "2 Random Coordinate Descent Algorithm", "text": "In this section, we give an algorithm for the problem (Prox-DSM) that is based on the random coordinate gradient descent method (RCDM) of [10]. The algorithm is given in Figure 1. The algorithm is very easy to implement and it uses oracles for problems of the form miny\u2208B(Fi) ( \u3008y, a\u3009+ \u2016y\u20162 ) , where i \u2208 [r] and a \u2208 Rn. Since each function Fi is simple, we have such oracles that are very efficient.\nIn the remainder of this section, we analyze the convergence rate of the RCDM algorithm. We emphasize that the objective function of (Prox-DSM) is not strongly convex and thus we cannot use as a black-box Nesterov\u2019s analysis of the RCDM method for minimizing strongly convex functions. Instead, we exploit the special structure of the problem to achieve convergence guarantees that match the rate achievable for strong convex objectives with strong convexity parameter 1/(n2r). Our analysis shows that the RCDM algorithm is faster by a factor of r than the alternating projections algorithm from [12].\nOutline of the analysis: Our analysis has two main components. First, we build on the work of [12] in order to prove a key theorem (Theorem 2). This theorem exploits the special structure of the (Prox-DSM) problem and it allows us to overcome the fact that the objective function of (Prox-DSM) is not strongly convex. Second, we modify Nesterov\u2019s analysis of the RCDM algorithm for minimizing strongly convex functions and we replace the strong convexity guarantee by the guarantee given by Theorem 2.\nWe start by introducing some notation; for the most part, we follow the notation of [10] and [12]. Let Rnr = \u2297r i=1 Rn. We write a vector y \u2208 Rnr as y = (y(1), . . . , y(r)), where each block y(i) is an n-dimensional\nvector. Let Y = \u2297r\ni=1 B(Fi) be the constraint set of (Prox-DSM). Let g : Rnr \u2192 R be the objective function of (Prox-DSM): g(y) = \u2225\u2225\u2211r i=1 y (i) \u2225\u22252. We use \u2207g to denote the gradient of g, i.e., the (nr)-dimensional vector of partial derivatives. For each i \u2208 {1, . . . , r}, we use \u2207ig(y) \u2208 Rn to denote the i-th block of coordinates of \u2207g(y).\nLet S \u2208 Rn\u00d7nr be the following matrix:\nS = 1\u221a r [ InIn \u00b7 \u00b7 \u00b7 In\ufe38 \ufe37\ufe37 \ufe38 r times ]\nNote that g(y) = r \u2016Sy\u20162 and \u2207g(y) = 2rSTSy. Additionally, for each i \u2208 {1, 2, . . . , r}, \u2207ig is Lipschitz continuous with constant Li = 2:\n\u2016\u2207ig(x)\u2212\u2207ig(y)\u2016 \u2264 Li \u2225\u2225\u2225x(i) \u2212 y(i)\u2225\u2225\u2225 , (1)\nfor all vectors x, y \u2208 Rnr that differ only in block i.\nOur first step is to prove the following key theorem that builds on the work of [12].\nTheorem 2. Let y \u2208 Y be a feasible solution to (Prox-DSM). Let y\u2217 be an optimal solution to (Prox-DSM) that minimizes \u2016y \u2212 y\u2217\u2016. We have\n\u2016S(y \u2212 y\u2217)\u2016 \u2265 1 nr \u2016y \u2212 y\u2217\u2016 .\nThe proof of Theorem 2 uses the following key result from [13]. We will need the following definitions from [13].\nLet d(K1,K2) = inf {\u2016k1 \u2212 k2\u2016 : k1 \u2208 K1, k2 \u2208 K2} be the distance between sets K1 and K2. Let P and Q be two closed convex sets in Rd. Let E \u2286 P and H \u2286 Q be the sets of closest points\nE = {p \u2208 P : d(p,Q) = d(P,Q)} H = {q \u2208 Q : d(q,P) = d(P,Q)}\nSince P and Q are convex, for each point in p \u2208 E, there is a unique point q \u2208 H such that d(p, q) = d(P,Q) and vice versa. Let v = \u03a0Q\u2212P0; note that H = E + v. Let Q\u2032 = Q\u2212 v; Q\u2032 is a translated version of Q and it intersects P at E. Let\n\u03ba\u2217 = sup x\u2208(P\u222aQ\u2032)\\E d(x,E) max {d(x,P), d(x,Q\u2032)} .\nBy combining Corollary 5 and Proposition 11 from [13], we obtain the following theorem. Theorem 3 ([12]). If P is the polyhedron \u2297r i=1 B(Fi) and Q is the polyhedron { y \u2208 Rnr : \u2211r i=1 y (i) = 0 } , we have \u03ba\u2217 \u2264 nr.\nNow we are ready to prove Theorem 2. Let\nP = \u2297r\ni=1 B(Fi) = Y Q = { y \u2208 Rnr : \u2211r i=1 y(i) = 0 } = {y \u2208 Rnr : Sy = 0}\nWe define Q\u2032 and \u03ba\u2217 as above.\nLet y and y\u2217 be the two points in the statement of the theorem. Note that y \u2208 P and y\u2217 \u2208 E, since E is the set of all optimal solutions to (Prox-DSM) (see Proposition 10 in Section B for a proof). We may assume that y /\u2208 E, since otherwise the theorem trivially holds. Since y \u2208 P \\ E, we have\n\u03ba\u2217 \u2265 d(y,E) d(y,Q\u2032)\nSince y\u2217 is an optimal solution that is closest to y, we have d(y,E) = \u2016y \u2212 y\u2217\u2016. Using the fact that the rows of S form a basis for the orthogonal complement of Q, we can show that d(y,Q\u2032) = \u2016S(y \u2212 y\u2217)\u2016 (see Proposition 11 in Section B for a proof). Therefore\n\u03ba\u2217 \u2265 \u2016y \u2212 y\u2217\u2016 \u2016S(y \u2212 y\u2217)\u2016 .\nTheorem 2 now follows from Theorem 3.\nIn the remainder of this section, we use Nesterov\u2019s analysis [10] in conjunction with Theorem 2 in order to show that the RCDM algorithm converges at a linear rate. Recall that E is the set of all optimal solutions to (Prox-DSM).\nTheorem 4. After (k + 1) iterations of the RCDM algorithm, we have\nE [ d(yk, E)2 + g(yk+1)\u2212 g(y\u2217) ] \u2264 (\n1\u2212 2 n2r2 + r\n)k+1 ( d(y0, E)2 + g(y0)\u2212 g(y\u2217) ) ,\nwhere y\u2217 = arg miny\u2208E \u2016y \u2212 yk\u2016 is the optimal solution that is closest to yk.\nWe devote the rest of this section to the proof of Theorem 4. We recall the following well-known lemma, which we refer to as the first-order optimality condition.\nLemma 5 (Theorem 2.2.5 in [11]). Let f : Rd \u2192 R be a differentiable convex function and let Q \u2286 Rd be a closed convex set. A point x\u2217 \u2208 Rd is a solution to the problem minx\u2208Q f(x) if and only if\n\u3008\u2207f(x\u2217), x\u2212 x\u2217\u3009 \u2265 0\nfor all x \u2208 Q.\nIt follows from the first-order optimality condition for y(ik)k+1 that, for any z \u2208 B(Fik ),\u2329 \u2207ikg(yk) + Lik ( y (ik) k+1 \u2212 y (ik) k ) , z \u2212 y(ik)k+1 \u232a \u2265 0 (2)\nWe have g(yk+1) = g(yk) + \u222b 1\n0 \u3008yk+1 \u2212 yk,\u2207g(yk + t(yk+1 \u2212 yk))\u3009dt\n= g(yk) + \u3008\u2207g(yk), yk+1 \u2212 yk\u3009+ \u222b 1\n0\n\u2329 yk+1 \u2212 yk,\u2207g(yk + t(yk+1 \u2212 yk))\u2212\u2207g(yk) \u232a dt\n= g(yk) + \u2329 \u2207ikg(yk), y (ik) k+1 \u2212 y (ik) k \u232a + \u222b 1\n0\n\u2329 y\n(ik) k+1 \u2212 y (ik) k ,\u2207ikg(yk + t(yk+1 \u2212 yk))\u2212\u2207ikg(yk)\n\u232a dt\n\u2264 g(yk) + \u2329 \u2207ikg(yk), y (ik) k+1 \u2212 y (ik) k \u232a + \u222b 1\n0 \u2225\u2225\u2225y(ik)k+1 \u2212 y(ik)k \u2225\u2225\u2225 \u2016\u2207ikg(yk + t(yk+1 \u2212 yk))\u2212\u2207ikg(yk)\u2016 dt (1) \u2264 g(yk) + \u2329 \u2207ikg(yk), y (ik) k+1 \u2212 y (ik) k \u232a + \u222b 1\n0 Lik \u2225\u2225\u2225y(ik)k+1 \u2212 y(ik)k \u2225\u2225\u22252 tdt = g(yk) + \u2329 \u2207ikg(yk), y (ik) k+1 \u2212 y (ik) k \u232a + Lik2\n\u2225\u2225\u2225y(ik)k+1 \u2212 y(ik)k \u2225\u2225\u22252 (3) On the third line, we have used the fact that yk and yk+1 agree on all coordinate blocks except the ik-th block. On the fourth line, we have used the Cauchy-Schwartz inequality. On the fifth line, we have used inequality (1).\nLet y\u2217 = arg miny\u2208E \u2016y \u2212 yk\u2016 be the optimal solution that is closest to yk. We have\n\u2016yk+1 \u2212 y\u2217\u20162 = \u2016yk \u2212 y\u2217\u20162 + \u2016yk+1 \u2212 yk\u20162 + 2\u3008yk \u2212 y\u2217, yk+1 \u2212 yk\u3009 = \u2016yk \u2212 y\u2217\u20162 \u2212 \u2016yk+1 \u2212 yk\u20162 + 2 \u3008yk+1 \u2212 y\u2217, yk+1 \u2212 yk\u3009\n= \u2016yk \u2212 y\u2217\u20162 \u2212 \u2225\u2225\u2225y(ik)k+1 \u2212 y(ik)k \u2225\u2225\u22252 + 2\u2329y(ik)k+1 \u2212 (y\u2217)(ik), y(ik)k+1 \u2212 y(ik)k \u232a\n(2) \u2264 \u2016yk \u2212 y\u2217\u20162 \u2212 \u2225\u2225\u2225y(ik)k+1 \u2212 y(ik)k \u2225\u2225\u22252 + 2Lik \u2329 \u2207ikg(yk), (y\u2217)(ik) \u2212 y (ik) k+1 \u232a = \u2016yk \u2212 y\u2217\u20162 +\n2 Lik\n\u2329 \u2207ikg(yk), (y\u2217)(ik) \u2212 y (ik) k \u232a \u2212 2 Lik ( Lik 2\n\u2225\u2225\u2225y(ik)k+1 \u2212 y(ik)k \u2225\u2225\u22252 + \u2329\u2207ikg(yk), y(ik)k+1 \u2212 y(ik)k \u232a) (3) \u2264 \u2016yk \u2212 y\u2217\u20162 +\n2 Lik\n\u2329 \u2207ikg(yk), (y\u2217)(ik) \u2212 y (ik) k \u232a \u2212 2 Lik (g(yk+1)\u2212 g(yk)) (4)\nOn the third line, we have used the fact that yk and yk+1 agree on all coordinate blocks except the ik-th block. On the fourth line, we have used the inequality (2) with z = (y\u2217)(ik). On the last line, we have used inequality (3).\nIf we rearrange the terms of the inequality (4), take expectation over ik, and substitute Lik = 2, we obtain\nEik [ \u2016yk+1 \u2212 y\u2217\u20162 + g(yk+1)\u2212 g(y\u2217) ] \u2264 \u2016yk \u2212 y\u2217\u20162 + g(yk)\u2212 g(y\u2217) +\n1 r \u3008\u2207g(yk), y\u2217 \u2212 yk\u3009 (5)\nWe can upper bound \u3008\u2207g(yk), y\u2217 \u2212 yk\u3009 as follows.\n\u3008\u2207g(yk), y\u2217 \u2212 yk\u3009 = 2r \u2329 STSyk, y \u2217 \u2212 yk \u232a\n= r \u2329 STSyk + STSy\u2217, y\u2217 \u2212 yk \u232a + r \u2329 STSyk \u2212 STSy\u2217, y\u2217 \u2212 yk \u232a = r \u2329 STSyk + STSy\u2217, y\u2217 \u2212 yk \u232a \u2212 r \u2016S(yk \u2212 y\u2217)\u20162\n= r \u3008S(yk + y\u2217), S(y\u2217 \u2212 yk)\u3009 \u2212 r \u2016S(yk \u2212 y\u2217)\u20162\n= (g(y\u2217)\u2212 g(yk))\u2212 r \u2016S(yk \u2212 y\u2217)\u20162 \u2264 (g(y\u2217)\u2212 g(yk))\u2212 1 n2r \u2016yk \u2212 y\u2217\u20162 (By Theorem 2) (6)\nOn the first and fifth lines, we have used the fact that \u2207g(z) = 2rSTSz and g(z) = r \u2016Sz\u20162 for any z \u2208 Rnr. On the last line, we have used Theorem 2.\nSince y\u2217 is an optimal solution to (Prox-DSM), the first-order optimality condition gives us that\n\u3008\u2207g(y\u2217), y\u2217 \u2212 yk\u3009 = 2r\u3008STSy\u2217, y\u2217 \u2212 yk\u3009 \u2264 0 (7)\nUsing the inequality above, we can also upper bound \u3008\u2207g(yk), y\u2217 \u2212 yk\u3009 as follows.\n\u3008\u2207g(yk), y\u2217 \u2212 yk\u3009 = 2r\u3008STSyk, y\u2217 \u2212 yk\u3009 = 2r\u3008STSy\u2217, y\u2217 \u2212 yk\u3009+ 2r\u3008STSyk \u2212 STSy\u2217, y\u2217 \u2212 yk\u3009 = 2r\u3008STSy\u2217, y\u2217 \u2212 yk\u3009 \u2212 2r \u2016S(yk \u2212 y\u2217)\u20162\n(7) \u2264 \u22122r \u2016S(yk \u2212 y\u2217)\u20162 \u2264 \u2212 2 n2r \u2016yk \u2212 y\u2217\u20162 (By Theorem 2) (8)\nAPPROX algorithm applied to (Prox-DSM) Start with z0 = (z(1)0 , . . . , z (r) 0 ) \u2208 Y \u03b80 \u2190 1r , u0 \u2190 0 In each iteration k (k \u2265 0) Generate a random set of blocks Rk where each block is included independently with probability 1r uk+1 \u2190 uk, zk+1 \u2190 zk For each i \u2208 Rk\nt (i) k \u2190 arg mint+z(i)\nk \u2208B(Fik )\n(\u2329 \u2207ig ( \u03b82kuk + zk ) , t \u232a + 2r\u03b8k \u2016t\u20162 )\nz (i) k+1 \u2190 z (i) k + t (i) k u (i) k+1 \u2190 u (i) k \u2212 1\u2212r\u03b8k \u03b82\nk\nt (i) k\n\u03b8k+1 = \u221a \u03b84 k +4\u03b82 k \u2212\u03b82k\n2 Return \u03b82kuk+1 + zk+1\nFigure 2: The APPROX algorithm of [2] applied to (Prox-DSM). It finds a solution to (Prox-DSM) given\naccess to an oracle for miny\u2208B(Fi)\n(\n\u3008y, a\u3009+ \u2016y\u20162\n)\n. By taking 2n2r+1 \u00d7 (6) + ( 1\u2212 2n2r+1 ) \u00d7 (8), we obtain\n\u3008\u2207g(yk), y\u2217 \u2212 yk\u3009 \u2264 \u2212 2\nn2r + 1\n( g(yk)\u2212 g(y\u2217) + \u2016yk \u2212 y\u2217\u20162 ) (9)\nBy (5) and (9),\nE ik\n[ \u2016yk+1 \u2212 y\u2217\u20162 + g(yk+1)\u2212 g(y\u2217) ] \u2264 (\n1\u2212 2 n2r2 + r\n)( g(yk)\u2212 g(y\u2217) + \u2016yk \u2212 y\u2217\u20162 ) Note that d(yk+1, E)2 \u2264 \u2016yk+1 \u2212 y\u2217\u20162 and d(yk, E)2 = \u2016yk \u2212 y\u2217\u20162. Therefore\nE ik\n[ d(yk+1, E)2 + g(yk+1)\u2212 g(y\u2217) ] \u2264 (\n1\u2212 2 n2r2 + r\n)( d(yk, E)2 + g(yk)\u2212 g(y\u2217) ) By taking expectation over \u03be = (i1, . . . , ik), we get\nE \u03be\n[ d(yk+1, E)2 + g(yk+1)\u2212 g(y\u2217) ] \u2264 (\n1\u2212 2 n2r2 + r\n)k+1 ( d(y0, E)2 + g(y0)\u2212 g(y\u2217) ) Therefore the proof of Theorem 4 is complete."}, {"heading": "3 Accelerated Coordinate Descent Algorithm", "text": "In this section, we give an accelerated random coordinate descent (ACDM) algorithm for (Prox-DSM). The algorithm uses the APPROX algorithm of Fercoq and Richt\u00e1rik [2] as a subroutine. The APPROX algorithm (Algorithm 2 in [2]), when applied to the (Prox-DSM) problem, yields the algorithm in Figure 2. The ACDM algorithm runs in a sequence of epochs (see Figure 3). In each epoch, the algorithm starts with the solution of the previous epoch and it runs the APPROX algorithm for \u0398(nr3/2) iterations. The solution constructed by the APPROX algorithm will be the starting point of the next epoch. Note that, for each i, the gradient\nACDM Algorithm for (Prox-DSM) \u3008\u3008We can take the initial point y0 to be 0\u3009\u3009 Start with y0 = (y(1)0 , . . . , y (r) 0 ) \u2208 Y In each epoch ` (` \u2265 0) Run the algorithm in Figure 2 for (4nr3/2 + 1) iterations with y` as its starting point (z0 = y`) Let y`+1 be the vector returned by the algorithm\nFigure 3: Accelerated block coordinate descent method for (Prox-DSM). It finds a solution to (Prox-DSM)\ngiven access to an oracle for miny\u2208B(Fi)\n(\n\u3008y, a\u3009+ \u2016y\u20162\n)\n.\n\u2207ig(y) = 2 \u2211 j y\n(j) can be easily maintained at a cost of O(n) per block update, and thus the iteration cost is dominated by the time to compute projection.\nIn the remainder of this section, we use the analysis of [2] together with Theorem 2 in order to show that the ACDM algorithm converges at a linear rate. We follow the notation used in Section 2.\nTheorem 6. After ` epochs of the ACDM algorithm (equivalently, (4nr3/2 + 1)` iterations), we have\nE[g(y`+1)\u2212 g(y\u2217)] \u2264 1\n2`+1 (g(y0)\u2212 g(y \u2217))\nIn the following lemma, we show that the objective function of (Prox-DSM) satisfies Assumption 1 in [2] and thus the convergence analysis given in [2] can be applied to our setting.\nLemma 7. Let R \u2286 {1, 2, . . . , r} be a random subset of coordinate blocks with the property that each i \u2208 {1, 2, . . . , r} is in R independently at random with probability 1/r. Let x and h be two vectors in Rnr. Let hR be the vector in Rnr such that (hR)(i) = h(i) for each block i \u2208 R and (hR)(i) = 0 otherwise. We have\nE [g (x+ hR)] \u2264 g(x) + 1 r \u3008\u2207g(x), h\u3009+ 2 r \u2016h\u20162 .\nProof: We have E [g (x+ hR)] = E [ r \u2016S(x+ hR)\u20162 ] = E [ r \u2016Sx\u20162 + r \u2016ShR\u20162 + 2r \u3008Sx, ShR\u3009\n] = E [ r \u2016Sx\u20162 + r \u2016ShR\u20162 + 2r \u2329 STSx, hR\n\u232a] = E\ng(x) + \u2225\u2225\u2225\u2225\u2225 r\u2211 i=1 h (i) R \u2225\u2225\u2225\u2225\u2225 2 + \u3008\u2207g(x), hR\u3009  = g(x) + 1\nr2 \u2211 i 6=j \u3008h(i), h(j)\u3009+ 1 r r\u2211 i=1 \u2225\u2225\u2225h(i)\u2225\u2225\u22252 + 1 r \u3008\u2207g(x), h\u3009\n\u2264 g(x) + 1 r2 \u2211 i 6=j 1 2 (\u2225\u2225\u2225h(i)\u2225\u2225\u22252 + \u2225\u2225\u2225h(j)\u2225\u2225\u22252)+ 1 r r\u2211 i=1 \u2225\u2225\u2225h(i)\u2225\u2225\u22252 + 1 r \u3008\u2207g(x), h\u3009\n\u2264 g(x) + 2 r r\u2211 i=1 \u2225\u2225\u2225h(i)\u2225\u2225\u22252 + 1 r \u3008\u2207g(x), h\u3009 = g(x) + 2 r \u2016h\u20162 + 1 r \u3008\u2207g(x), h\u3009\nLemma 7 together with Theorem 3 in [2] give us the following theorem.\nTheorem 8 (Theorem 3 of [2]). Consider iteration k of the APPROX algorithm (see Figure 2). Let yk = \u03b82kuk+1 + zk+1. Let y\u2217 = arg miny\u2208E \u2016y \u2212 yk\u2016 is the optimal solution that is closest to yk. We have\nE[g(yk)\u2212 g(y\u2217)] \u2264 4r2\n(k \u2212 1 + 2r)2\n(( 1\u2212 1\nr\n) (g(z0)\u2212 g(y\u2217)) + 2 \u2016z0 \u2212 y\u2217\u20162 ) Proof: It follows from Lemma 7 that the objective function g of (Prox-DSM) and the random blocks Rk used by the APPROX algorithm satisfy Assumption 1 in [2] with \u03c4 = 1 and \u03bdi = 4 for each i \u2208 {1, 2, . . . , r}. Thus we can apply Theorem 3 in [2].\nConsider an epoch `. Let y`+1 be the solution constructed by the APPROX algorithm after 4nr3/2 + 1 iterations, starting with y`. Let y\u2217 = arg miny\u2208E \u2016y \u2212 y`+1\u2016 be the optimal solution that is closest to y`+1. Let \u03be` denote the random choices made during epoch `. By Theorem 8,\nE \u03be`\n[g(y`+1)\u2212 g(y\u2217)] \u2264 4r2\n(4nr3/2 + 2r)2\n(( 1\u2212 1\nr\n) (g(y`)\u2212 g(y\u2217)) + 2 \u2016y` \u2212 y\u2217\u20162 ) \u2264 1 (2nr1/2 + 1)2 ( g(y`)\u2212 g(y\u2217) + 2 \u2016y` \u2212 y\u2217\u20162\n) We also have\ng(y`) = g(y\u2217) + \u3008\u2207g(y\u2217), y` \u2212 y\u2217\u3009+ \u222b 1\n0 \u3008\u2207g(y\u2217 + t(y` \u2212 y\u2217))\u2212\u2207g(y\u2217), y` \u2212 y\u2217\u3009dt\n\u2265 g(y\u2217) + \u222b 1\n0 \u3008\u2207g(y\u2217 + t(y` \u2212 y\u2217))\u2212\u2207g(y\u2217), y` \u2212 y\u2217\u3009dt\n= g(y\u2217) + \u222b 1\n0 2tr \u2016S(y` \u2212 y\u2217)\u20162 dt\n= g(y\u2217) + r \u2016S(y` \u2212 y\u2217)\u20162 \u2265 g(y\u2217) + 1 n2r \u2016y` \u2212 y\u2217\u20162 (By Theorem 2)\nIn the second line, we have used the first-order optimality condition for y\u2217 (Lemma 5). In the last line, we have used Theorem 2.\nTherefore \u2016y` \u2212 y\u2217\u20162 \u2264 n2r(g(y`)\u2212 g(y\u2217))\nand hence\nE \u03be`\n[g(y`+1)\u2212 g(y\u2217)] \u2264 2n2r + 1 (2nr1/2 + 1)2 ( g(y`)\u2212 g(y\u2217) ) \u2264 12 ( g(y`)\u2212 g(y\u2217)\n) Let \u03be = (\u03be0, . . . , \u03be`) be the random choices made during the epochs 0 to `. We have\nE \u03be [g(y`+1)\u2212 g(y\u2217)] \u2264 1 2`+1\n( g(y0)\u2212 g(y\u2217) ) This completes the proof of Theorem 6 and the convergence analysis for the ACDM algorithm."}, {"heading": "4 Experiments", "text": "Algorithms. We empirically evaluate and compare the following algorithms: the RCDM described in Section 2, the ACDM described in Section 3, and the alternating projections (AP) algorithm of [12]. The AP algorithm solves the following best approximation problem that is equivalent to (Prox-DSM):\nmin a\u2208A,y\u2208Y\n\u2016a\u2212 y\u20162 (Best-Approx)\nwhere A = { (a(1), a(2), . . . , a(r)) \u2208 Rnr : \u2211r i=1 a (i) = 0 } and Y = \u2297r i=1 B(Fi).\nThe AP algorithm starts with a point a0 \u2208 A and it iteratively constructs a sequence {(ak, yk)}k\u22650 by projecting onto A and Y: yk = \u03a0Y(ak), ak+1 = \u03a0A(yk).\n\u03a0K(\u00b7 ) is the projection operator onto K, that is, \u03a0K(x) = arg minz\u2208K \u2016x\u2212 z\u2016. Since A is a subspace, it is straightforward to project onto A. The projection onto Y can be implemented using the oracles for the projections \u03a0B(Fi) onto the base polytopes of the functions Fi.\nFor all three algorithms, the iteration cost is dominated by the cost of projecting onto the base polytopes\nB(Fi). Therefore the total number of such projections is a suitable measure for comparing the algorithms. In each iteration, the RCDM algorithm performs a single projection for a random block i and the ACDM algorithm performs a single projection in expectation. The AP algorithm performs r projections in each iteration, one for each block.\nImage Segmentation Experiments. We evaluate the algorithms on graph cut problems that arise in image segmentation or MAP inference tasks in Markov Random Fields. Our experimental setup is similar to that of [6]. We set up the image segmentation problems on a 8-neighbor grid graph with unary potentials derived from Gaussian Mixture Models of color features [16]. The weight of a graph edge (i, j) between pixels i and j is a function of exp(\u2212\u2016vi \u2212 vj\u20162), where vi is the RGB color vector of pixel i. The optimization problem that we solve for each segmentation task is a cut problem on the grid graph.\nFunction decomposition: We partition the edges of the grid into a small number of matchings and we decompose the function using the cut functions of these matchings. Note that it is straightforward to project onto the base polytopes of such functions using a sequence of projections onto line segments.\nDuality gaps: We evaluate the convergence behaviours of the algorithms using the following measures. Let y be a feasible solution to the dual of the proximal problem (Proximal). The solution x = \u2212 \u2211r i=1 y\n(i) is a feasible solution for the proximal problem. We define the smooth duality gap to be the difference between the objective values of the primal solution x and the dual solution y: \u03bds = ( f(x) + 12 \u2016x\u2016 2 ) \u2212 ( \u2212 r2 \u2016Sy\u2016 2 ) . Additionally, we compute a discrete duality gap for the discrete problem (DSM) and the dual of its Lov\u00e1sz relaxation; the latter is the problem maxz\u2208B(F )(z)\u2212(V ), where (z)\u2212 = min {z, 0} applied elementwise [6]. The best level set Sx of the proximal solution x = \u2212 \u2211r i=1 y\n(i) is a solution to the discrete problem (DSM). The solution z = \u2212x = \u2211r i=1 y\n(i) is a feasible solution for the dual of the Lov\u00e1sz relaxation. We define the discrete duality gap to be the difference between the objective values of these solutions: \u03bdd(x) = F (Sx) \u2212 (\u2212x)\u2212(V ).\nWe evaluated the algorithms on four image segmentation instances2 [7, 16]. Figure 5 shows the smooth and discrete duality gaps on the four instances. Figure 4 shows some segmentation results for one of the instances.\nAcknowledgements. We thank Stefanie Jegelka for providing us with some of the data used in our experiments.\n2The data is available at http://melodi.ee.washington.edu/~jegelka/cc/index.html and http://research.microsoft. com/en-us/um/cambridge/projects/visionimagevideoediting/segmentation/grabcut.htm"}, {"heading": "A Proof of Lemma 1", "text": "By the definition of the Lov\u00e1sz extension, for each i \u2208 [r], we have\nfi(x) = max y(i)\u2208B(Fi)\n\u3008y(i), x\u3009.\nTherefore\nmin x\u2208Rn r\u2211 i=1 ( fi(x) + 1 2r \u2016x\u2016 2 )\n= min x\u2208Rn r\u2211 i=1 ( max y(i)\u2208B(Fi) \u3008y(i), x\u3009+ 12r \u2016x\u2016 2 )\n= min x\u2208Rn max y(1)\u2208B(F1),...,y(r)\u2208B(Fr) r\u2211 i=1 ( \u3008y(i), x\u3009+ 12r \u2016x\u2016 2 )\n= max y(1)\u2208B(F1),...,y(r)\u2208B(Fr) min x\u2208Rn r\u2211 i=1 ( \u3008y(i), x\u3009+ 12r \u2016x\u2016 2 )\n= max y(1)\u2208B(F1),...,y(r)\u2208B(Fr) \u221212 \u2225\u2225\u2225\u2225\u2225 r\u2211 i=1 y(i) \u2225\u2225\u2225\u2225\u2225 2\nOn the third line, we have used the fact that the function \u3008y, x\u3009 + (1/2r) \u2016x\u20162 is convex in x and linear in y, which allows us to exchange the min and the max (see for example Corollary 37.3.2 in Rockafellar [15]). On the fourth line, we have used the fact that the minimum is achieved at x = \u2212 \u2211r i=1 y (i)."}, {"heading": "B Proofs omitted from Section 2", "text": "If x \u2208 Rnr and X is a subspace of Rnr, we let \u03a0X (x) denote the projection of x on X , that is, \u03a0X (x) = arg minz\u2208Rnr \u2016x\u2212 z\u2016. We let X\u22a5 denote the orthogonal complement of the subspace X .\nProposition 9. For any point x \u2208 Rnr, \u03a0Q\u22a5(x) = STSx and thus \u03a0Q(x) = x\u2212 STSx.\nProof: Since Q is the null space of S, Q\u22a5 is the row space of S. Since the rows of S are orthonormal, they form a basis for Q\u22a5. Therefore, if we let v1, . . . , vn denote the rows of S, we have\n\u03a0Q\u22a5(x) = n\u2211 i=1 \u3008x, vi\u3009vi = STSx.\nProposition 10. The set of all optimal solutions to (Prox-DSM) is equal to E.\nProof: We have\nd(P,Q) = min y\u2208P \u2016y \u2212\u03a0Q(y)\u2016\n= min y\u2208P \u2225\u2225STSy\u2225\u2225 \u3008\u3008By Proposition 9\u3009\u3009 = min\ny\u2208P \u2016Sy\u2016\nSince (Prox-DSM) is the problem miny\u2208P r \u2016Sy\u20162, E is the set of all optimal solutions to (Prox-DSM).\nProposition 11. Let y \u2208 Rnr and let p \u2208 E. We have d(y,Q\u2032) = \u2016S(y \u2212 p)\u2016.\nProof: Since Q\u2032 = Q\u2212 v, we have\nd(y,Q\u2032) = d(y + v,Q) = \u2016\u03a0Q\u22a5(y + v)\u2016 = \u2225\u2225STS(y + v)\u2225\u2225 \u3008\u3008By Proposition 9\u3009\u3009\n= \u2225\u2225STS(y \u2212 STSp)\u2225\u2225 \u3008\u3008Since v = \u2212STSp\u3009\u3009\n= \u2225\u2225STS(y \u2212 p)\u2225\u2225 \u3008\u3008Since SST = In\u3009\u3009\n= \u2016S(y \u2212 p)\u2016"}], "references": [{"title": "Learning with submodular functions: A convex optimization perspective", "author": ["Francis Bach"], "venue": "ArXiv preprint arXiv:1111.6453,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Accelerated, parallel and proximal coordinate descent", "author": ["Olivier Fercoq", "Peter Richt\u00e1rik"], "venue": "ArXiv preprint arXiv:1312.5799,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "A push-relabel framework for submodular function minimization and applications to parametric optimization", "author": ["Lisa Fleischer", "Satoru Iwata"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "The ellipsoid method and its consequences in combinatorial optimization", "author": ["Martin Gr\u00f6tschel", "L\u00e1szl\u00f3 Lov\u00e1sz", "Alexander Schrijver"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1981}, {"title": "A faster scaling algorithm for minimizing submodular functions", "author": ["Satoru Iwata"], "venue": "SIAM Journal on Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Reflection methods for user-friendly submodular optimization", "author": ["Stefanie Jegelka", "Francis Bach", "Suvrit Sra"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Submodularity beyond submodular energies: coupling edges in graph cuts", "author": ["Stefanie Jegelka", "Jeff Bilmes"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Minimizing a sum of submodular functions", "author": ["Vladimir Kolmogorov"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Submodular functions and convexity", "author": ["L\u00e1szl\u00f3 Lov\u00e1sz"], "venue": "In Mathematical Programming The State of the Art,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1983}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Yu. Nesterov"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Introductory lectures on convex optimization: A basic course, volume 87", "author": ["Yurii Nesterov"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "On the convergence rate of decomposable submodular function minimization", "author": ["Robert Nishihara", "Stefanie Jegelka", "Michael I Jordan"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "On the convergence rate of decomposable submodular function minimization", "author": ["Robert Nishihara", "Stefanie Jegelka", "Michael I Jordan"], "venue": "ArXiv preprint arXiv:1406.6474,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "A faster strongly polynomial time algorithm for submodular function minimization", "author": ["James B Orlin"], "venue": "Mathematical Programming,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Convex analysis", "author": ["R Tyrrell Rockafellar"], "venue": "Number 28 in Princeton Mathematical Series. Princeton university press,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1970}, {"title": "Grabcut: Interactive foreground extraction using iterated graph cuts", "author": ["Carsten Rother", "Vladimir Kolmogorov", "Andrew Blake"], "venue": "ACM Transactions on Graphics (TOG),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "A combinatorial algorithm minimizing submodular functions in strongly polynomial time", "author": ["Alexander Schrijver"], "venue": "Journal of Combinatorial Theory, Series B,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Combinatorial optimization: polyhedra and efficiency, volume 24", "author": ["Alexander Schrijver"], "venue": "Springer Science & Business Media,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}], "referenceMentions": [{"referenceID": 3, "context": "Over the past few decades, there has been a significant progress on minimizing submodular functions, leading to several polynomial time algorithms for the problem [4, 17, 5, 3, 14].", "startOffset": 163, "endOffset": 180}, {"referenceID": 16, "context": "Over the past few decades, there has been a significant progress on minimizing submodular functions, leading to several polynomial time algorithms for the problem [4, 17, 5, 3, 14].", "startOffset": 163, "endOffset": 180}, {"referenceID": 4, "context": "Over the past few decades, there has been a significant progress on minimizing submodular functions, leading to several polynomial time algorithms for the problem [4, 17, 5, 3, 14].", "startOffset": 163, "endOffset": 180}, {"referenceID": 2, "context": "Over the past few decades, there has been a significant progress on minimizing submodular functions, leading to several polynomial time algorithms for the problem [4, 17, 5, 3, 14].", "startOffset": 163, "endOffset": 180}, {"referenceID": 13, "context": "Over the past few decades, there has been a significant progress on minimizing submodular functions, leading to several polynomial time algorithms for the problem [4, 17, 5, 3, 14].", "startOffset": 163, "endOffset": 180}, {"referenceID": 7, "context": "Following [8, 19, 6, 12], we consider the problem of minimizing decomposable submodular functions that can be expressed as a sum of simple functions.", "startOffset": 10, "endOffset": 24}, {"referenceID": 5, "context": "Following [8, 19, 6, 12], we consider the problem of minimizing decomposable submodular functions that can be expressed as a sum of simple functions.", "startOffset": 10, "endOffset": 24}, {"referenceID": 11, "context": "Following [8, 19, 6, 12], we consider the problem of minimizing decomposable submodular functions that can be expressed as a sum of simple functions.", "startOffset": 10, "endOffset": 24}, {"referenceID": 5, "context": "The recent work of [6, 8, 19] has developed several algorithms with very good empirical performance that exploit the special structure of decomposable functions.", "startOffset": 19, "endOffset": 29}, {"referenceID": 7, "context": "The recent work of [6, 8, 19] has developed several algorithms with very good empirical performance that exploit the special structure of decomposable functions.", "startOffset": 19, "endOffset": 29}, {"referenceID": 5, "context": "In particular, [6] have shown that the problem of minimizing decomposable submodular functions can be formulated as a distance minimization problem between two polytopes.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "This formulation, when coupled with powerful convex optimization techniques such as gradient descent or projection methods, it yields algorithms that are very fast in practice and very simple to implement [6].", "startOffset": 205, "endOffset": 208}, {"referenceID": 11, "context": "[12] have made a significant progress in this direction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] using the ellipsoid method.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "There are several combinatorial algorithms for the problem [17, 5, 3, 14].", "startOffset": 59, "endOffset": 73}, {"referenceID": 4, "context": "There are several combinatorial algorithms for the problem [17, 5, 3, 14].", "startOffset": 59, "endOffset": 73}, {"referenceID": 2, "context": "There are several combinatorial algorithms for the problem [17, 5, 3, 14].", "startOffset": 59, "endOffset": 73}, {"referenceID": 13, "context": "There are several combinatorial algorithms for the problem [17, 5, 3, 14].", "startOffset": 59, "endOffset": 73}, {"referenceID": 13, "context": "Among the combinatorial methods, Orlin\u2019s algorithm [14] achieves the best time complexity of O(n5T +n6), where n is the size of the ground set and T is the maximum amount of time it takes to evaluate the function.", "startOffset": 51, "endOffset": 55}, {"referenceID": 7, "context": "Several algorithms have been proposed for minimizing decomposable submodular functions [19, 8, 6, 12].", "startOffset": 87, "endOffset": 101}, {"referenceID": 5, "context": "Several algorithms have been proposed for minimizing decomposable submodular functions [19, 8, 6, 12].", "startOffset": 87, "endOffset": 101}, {"referenceID": 11, "context": "Several algorithms have been proposed for minimizing decomposable submodular functions [19, 8, 6, 12].", "startOffset": 87, "endOffset": 101}, {"referenceID": 11, "context": "[12] give an algorithm based on alternating projections that achieves a linear convergence rate.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "The Lov\u00e1sz extension f of F can be written as the support function of the base polytope B(F ): f(x) = max w\u2208B(F ) \u3008w, x\u3009 \u2200x \u2208 R Even though the base polytope B(F ) has exponentially many vertices, the Lov\u00e1sz extension f can be evaluated efficiently using the greedy algorithm of Edmonds (see for example [18]).", "startOffset": 304, "endOffset": 308}, {"referenceID": 8, "context": "Lov\u00e1sz showed that a set function F is submodular if and only if its Lov\u00e1sz extension f is convex [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 0, "context": "min x\u2208[0,1]n f(x) \u2261 min x\u2208[0,1]n r \u2211", "startOffset": 6, "endOffset": 11}, {"referenceID": 0, "context": "min x\u2208[0,1]n f(x) \u2261 min x\u2208[0,1]n r \u2211", "startOffset": 26, "endOffset": 31}, {"referenceID": 5, "context": "Following previous work [6, 12], we consider a proximal version of the problem (\u2016\u00b7 \u2016 denotes the `2-norm):", "startOffset": 24, "endOffset": 31}, {"referenceID": 11, "context": "Following previous work [6, 12], we consider a proximal version of the problem (\u2016\u00b7 \u2016 denotes the `2-norm):", "startOffset": 24, "endOffset": 31}, {"referenceID": 0, "context": "6 in [1]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 5, "context": "Lemma 1 ([6]).", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "Lemma 1 was proved in [6]; we include a proof in Section A for completeness.", "startOffset": 22, "endOffset": 25}, {"referenceID": 9, "context": "In this section, we give an algorithm for the problem (Prox-DSM) that is based on the random coordinate gradient descent method (RCDM) of [10].", "startOffset": 138, "endOffset": 142}, {"referenceID": 11, "context": "Our analysis shows that the RCDM algorithm is faster by a factor of r than the alternating projections algorithm from [12].", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "First, we build on the work of [12] in order to prove a key theorem (Theorem 2).", "startOffset": 31, "endOffset": 35}, {"referenceID": 9, "context": "We start by introducing some notation; for the most part, we follow the notation of [10] and [12].", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "We start by introducing some notation; for the most part, we follow the notation of [10] and [12].", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "Our first step is to prove the following key theorem that builds on the work of [12].", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "The proof of Theorem 2 uses the following key result from [13].", "startOffset": 58, "endOffset": 62}, {"referenceID": 12, "context": "We will need the following definitions from [13].", "startOffset": 44, "endOffset": 48}, {"referenceID": 12, "context": "By combining Corollary 5 and Proposition 11 from [13], we obtain the following theorem.", "startOffset": 49, "endOffset": 53}, {"referenceID": 11, "context": "Theorem 3 ([12]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 9, "context": "In the remainder of this section, we use Nesterov\u2019s analysis [10] in conjunction with Theorem 2 in order to show that the RCDM algorithm converges at a linear rate.", "startOffset": 61, "endOffset": 65}, {"referenceID": 10, "context": "5 in [11]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 1, "context": "Figure 2: The APPROX algorithm of [2] applied to (Prox-DSM).", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": "The algorithm uses the APPROX algorithm of Fercoq and Richt\u00e1rik [2] as a subroutine.", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "The APPROX algorithm (Algorithm 2 in [2]), when applied to the (Prox-DSM) problem, yields the algorithm in Figure 2.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "In the remainder of this section, we use the analysis of [2] together with Theorem 2 in order to show that the ACDM algorithm converges at a linear rate.", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "After ` epochs of the ACDM algorithm (equivalently, (4nr3/2 + 1)` iterations), we have E[g(y`+1)\u2212 g(y\u2217)] \u2264 1 2`+1 (g(y0)\u2212 g(y \u2217)) In the following lemma, we show that the objective function of (Prox-DSM) satisfies Assumption 1 in [2] and thus the convergence analysis given in [2] can be applied to our setting.", "startOffset": 230, "endOffset": 233}, {"referenceID": 1, "context": "After ` epochs of the ACDM algorithm (equivalently, (4nr3/2 + 1)` iterations), we have E[g(y`+1)\u2212 g(y\u2217)] \u2264 1 2`+1 (g(y0)\u2212 g(y \u2217)) In the following lemma, we show that the objective function of (Prox-DSM) satisfies Assumption 1 in [2] and thus the convergence analysis given in [2] can be applied to our setting.", "startOffset": 277, "endOffset": 280}, {"referenceID": 1, "context": "Lemma 7 together with Theorem 3 in [2] give us the following theorem.", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "Theorem 8 (Theorem 3 of [2]).", "startOffset": 24, "endOffset": 27}, {"referenceID": 1, "context": "Proof: It follows from Lemma 7 that the objective function g of (Prox-DSM) and the random blocks Rk used by the APPROX algorithm satisfy Assumption 1 in [2] with \u03c4 = 1 and \u03bdi = 4 for each i \u2208 {1, 2, .", "startOffset": 153, "endOffset": 156}, {"referenceID": 1, "context": "Thus we can apply Theorem 3 in [2].", "startOffset": 31, "endOffset": 34}, {"referenceID": 11, "context": "We empirically evaluate and compare the following algorithms: the RCDM described in Section 2, the ACDM described in Section 3, and the alternating projections (AP) algorithm of [12].", "startOffset": 178, "endOffset": 182}, {"referenceID": 5, "context": "Our experimental setup is similar to that of [6].", "startOffset": 45, "endOffset": 48}, {"referenceID": 15, "context": "We set up the image segmentation problems on a 8-neighbor grid graph with unary potentials derived from Gaussian Mixture Models of color features [16].", "startOffset": 146, "endOffset": 150}, {"referenceID": 5, "context": "Additionally, we compute a discrete duality gap for the discrete problem (DSM) and the dual of its Lov\u00e1sz relaxation; the latter is the problem maxz\u2208B(F )(z)\u2212(V ), where (z)\u2212 = min {z, 0} applied elementwise [6].", "startOffset": 208, "endOffset": 211}, {"referenceID": 6, "context": "We evaluated the algorithms on four image segmentation instances2 [7, 16].", "startOffset": 66, "endOffset": 73}, {"referenceID": 15, "context": "We evaluated the algorithms on four image segmentation instances2 [7, 16].", "startOffset": 66, "endOffset": 73}], "year": 2015, "abstractText": "Submodular function minimization is a fundamental optimization problem that arises in several applications in machine learning and computer vision. The problem is known to be solvable in polynomial time, but general purpose algorithms have high running times and are unsuitable for large-scale problems. Recent work have used convex optimization techniques to obtain very practical algorithms for minimizing functions that are sums of \u201csimple\" functions. In this paper, we use random coordinate descent methods to obtain algorithms with faster linear convergence rates and cheaper iteration costs. Compared to alternating projection methods, our algorithms do not rely on full-dimensional vector operations and they converge in significantly fewer iterations.", "creator": "LaTeX with hyperref package"}}}