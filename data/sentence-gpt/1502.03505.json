{"id": "1502.03505", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2015", "title": "Supervised LogEuclidean Metric Learning for Symmetric Positive Definite Matrices", "abstract": "Metric learning has been shown to be highly effective to improve the performance of nearest neighbor classification. In this paper, we address the problem of metric learning for Symmetric Positive Definite (SPD) matrices such as covariance matrices, which arise in many real-world applications. Naively using standard Mahalanobis metric learning methods under the Euclidean geometry for SPD matrices is not appropriate, because the difference of SPD matrices can be a non-SPD matrix and thus the obtained solution can be uninterpretable. To cope with this problem, we propose to use a properly parameterized LogEuclidean distance and optimize the metric with respect to kernel-target alignment, which is a supervised criterion for kernel learning. Then the resulting non-trivial optimization problem is solved by utilizing the Riemannian geometry. Finally, we experimentally demonstrate the usefulness of our LogEuclidean metric learning algorithm on real-world classification tasks for EEG signals and texture patches.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 12 Feb 2015 01:38:36 GMT  (1831kb,D)", "http://arxiv.org/abs/1502.03505v1", "19 pages, 6 figures, 3 tables"]], "COMMENTS": "19 pages, 6 figures, 3 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["florian yger", "masashi sugiyama"], "accepted": false, "id": "1502.03505"}, "pdf": {"name": "1502.03505.pdf", "metadata": {"source": "CRF", "title": "Supervised LogEuclidean Metric Learning for Symmetric Positive Definite Matrices", "authors": ["Florian Yger", "Masashi Sugiyama"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Defining a distance to compare data points is an important problem occurring in many machine learning tasks. For instance, in classification, the nearest neighbor method Cover & Hart (1967) is equipped with a distance to identify the nearest neighbors. The performance of such a classifier highly depends on the quality of the equipped distance, and hence automatically finding a relevant distance from data has been the aim of many metric learning algorithms. So far, various methods have been developed for learning Mahalanobis distances in the Euclidean setting, and such metric learning methods have been successfully applied to diverse real-world problems including music recommendation Lim et al. (2013), face recognition Lu et al. (2012), image classification Mensink et al. (2012), link prediction in networks Shaw et al. (2011) and bioinformatics Wang et al. (2012). For the survey of recent advances, issues and perspectives in metric learning, see Bellet et al. (2013).\nClassical metric learning approaches focused on an Euclidean setting which do not apply to complex or structured objects. Recently, metric learning for complex objects such as histograms Cuturi & Avis (2014); Kedem et al. (2012), binary codes Norouzi et al. (2012) and strings Bellet et al. (2011) have been actively explored, to which the Euclidean distance is not relevant. However, to the best of our knowledge, metric learning for symmetric positive definite (SPD) matrices has not been investigated thoroughly. Learning from SPD matrices, particularly from covariance matrices, arises in a number of important classification tasks such as brain imaging Arsigny et al.\nar X\niv :1\n50 2.\n03 50\n5v 1\n[ cs\n.L G\n] 1\n(2006); Dryden et al. (2009), brain-computer interfaces Barachant et al. (2010, 2013), pedestrian detection Tuzel et al. (2008) and texture classification Tuzel et al. (2006); Tou et al. (2009). In these applications, covariance matrices are either extracted from a physical model of the studied phenomenon (for diffusion tensor imaging) or as an empirical estimator from observations (for signal processing and computer-vision tasks). The purpose of this paper is to investigate metric learning for SPD matrices to boost the classification performance.\nSPD matrices belong to an Euclidean1 space. For example, 2\u00d72 SPD matrixA can be written asA = [ a b b c ] with ac\u2212 b2 > 0, a > 0 and c > 0. Then symmetric matrices can be represented as points in R3 and the constraints can be plotted as a cone, inside which SPD matrices lie strictly (see Fig. 1). A straightforward approach for learning a metric in this space would be to simply use the Euclidean distance \u03b4e:\n\u03b4e(A,B) = \u2016A\u2212B\u2016F , (1)\nwhere \u2016\u00b7\u2016F denotes the Frobenius norm. The Euclidean geometry of symmetric matrices implies that distances are computed along straight lines according to \u03b4e (see Fig. 1 again).\nHowever, the Euclidean geometry for averaging SPD matrices can result in a swelling effect Arsigny et al. (2007), i.e., the determinant of the average can be bigger than the determinant of each matrix. As also remarked in Fletcher & Joshi (2004) and illustrated in Fig. 1, this geometry creates a non-complete space, meaning that interpolation of SPD matrices is possible, but extrapolation can produce indefinite matrices. Hence, the swelling effect and the non-completeness of the space can result in uninterpretable solutions.\n1Note that, in order to be an Euclidean space, the space should be equipped with the Frobenius inner product \u3008A,B\u3009F = Tr(A>B) and the derived norm ||A||F = \u221a \u3008A,A\u3009F .\nTo avoid this problem, we use a more natural metric to compare SPD matrices, namely, the LogEuclidean distance \u03b4l:\n\u03b4l(A,B) = \u2016log (A)\u2212 log (B)\u2016F , (2)\nwhere log(\u00b7) stands for the matrix logarithm. The LogEuclidean metric (and the derived distance and kernel) has been used in the literature Arsigny et al. (2006); Barachant et al. (2013), but its parameterization remained underestimated until recently Yger (2013). In this paper, we propose a supervised approach to learning the LogEuclidean metric. More specifically, we formulate our LogEuclidean metric learning problem as the kernel-target alignment problem Cristianini et al. (2001); Cortes et al. (2012), and solve the non-trivial optimization problem using the Riemannian geometry for SPD matrices. Through experiments on signal processing and computer vision applications, we demonstrate the efficacy of our proposed metric learning method."}, {"heading": "2 Proposed approach", "text": "In this section, we describe our proposed metric learning method."}, {"heading": "2.1 Formulation of metric learning under LogEuclidean distance", "text": "Let Sd be the set of real symmetric matrices of size d \u00d7 d. A matrix A is said to be positive-definite (A 0) if x>Ax > 0 for all non-zero x \u2208 Rd, and the set of d\u00d7 d SPD matrices is denoted by Pd.\nTo learn a metric, we need to parameterize a distance. In this paper, following Bhatia (2009), we focus on the congruent transform, i.e., for any A,G \u2208 Pd,{\nPd 7\u2192 Pd \u0393 G\u2212 1 2 : A\u2192 G\u2212 12AG\u2212 12 .\nThus, the LogEuclidean distance (2) is parameterized using G \u2208 Pd as\n\u03b4Gl (A,B) = \u03b4l ( \u0393 G\u2212 1 2 (A),\u0393 G\u2212 1 2 (B) )\n(3) = \u2016log ( G\u2212 1 2AG\u2212 1 2 ) \u2212 log ( G\u2212 1 2BG\u2212 1 2 ) \u2016F .\nOur goal is to learn the parameter matrix G from a set of n training samples,\n{(Xi, yi)|Xi \u2208 Pd, yi \u2208 {+1,\u22121}}ni=1,\nso that the performance of the nearest neighbor classifier on Pd is maximally enhanced. G was implicitly chosen as the identity matrix in Arsigny et al. (2006), while G was heuristically chosen as the Riemannian mean of training samples in Barachant et al. (2013) and Yger (2013). Although these choices sound reasonable, we argue that they are sub-optimal when \u03b4Gl is used for nearest neighbor classification. Our aim is to find the optimal G in terms of classification performance."}, {"heading": "2.2 Metric learning with kernel-target alignment", "text": "For metric learning, we employ the (centered) kernel target alignment (KTA) criterion Cristianini et al. (2001); Cortes et al. (2012):\nA(K,K?) = \u3008UKU,UK ?U\u3009F\n\u2016UKU\u2016F\u2016UK?U\u2016F ,\nwhere K is a matrix to be aligned, K? = yy T is a target matrix, y = (y1, . . . , yn) T \u2208 Rn, U = Id \u2212 1d1\n> d\nd is the centering matrix, Id is the identity matrix of size d \u00d7 d, and 1d is the d-dimensional vector with all ones. For simplicity, we suppose that y is centered, i.e., UyyTU \u2192 yyT below.\nLet kG (X,X \u2032) be the LogEuclidean metric, from which the LogEuclidean distance (3) is derived:\nkG (X,X \u2032) = Tr ( log ( G\u2212 1 2XG\u2212 1 2 ) log ( G\u2212 1 2X \u2032G\u2212 1 2 )) ,\nwhere X,X \u2032, G \u2208 Pd. Let h(G) be the gram matrix for kG:\nhij(G) = kG(Xi, Xj).\nThen our metric learning problem is formulated as\nmax G\u2208Pd f (G) , (4)\nwhere\nf (G) = A(h(G), yy T) = \u3008Uh(G)U, yy T\u3009F\n\u2016Uh(G)U\u2016F . (5)\nA naive approach to solving the above optimization problem would be the projected gradient method, i.e., iteratively performing gradient ascent over f(G) and projection of the updated solution back onto Pd. However, since Pd is an open set (the strict interior of the cone is a set of SPD matrices, see Fig. 1 again), projection does not always exist. To cope with this problem, we introduce a more sophisticated approach based on the Riemannian geometry below."}, {"heading": "2.3 Riemanian Gradient Optimization", "text": "Considering Pd as a Riemannian manifold provides us useful tools for solving our optimization problem. A basic optimization algorithm on the Riemannian manifold is the geodesic gradient descent2, which optimizes the objective function along the geodesic computed from the Riemannian gradient gradGf(G). For optimization problem (4), the geodesic from Gt to Gt+1 is given by\nGt+1 =G 1 2 t exp ( \u03b7G \u2212 12 t gradGtf(Gt)G \u2212 12 t ) G 1 2 t , (6)\nwhere \u03b7 \u2265 0 is the step size and exp(.) denotes the matrix exponential. Below, we explain how the above formula was derived.\n2 For more sophisticated optimization methods, see Absil et al. (2009) and Boumal et al. (2014).\nAs stated in Bhatia (2009), Pd is an open subset of the ambient Euclidean space of d \u00d7 d symmetric matrices Sd. As such, it is an instance of an embedded sub-manifold of Sd and so it is a differentiable manifold of dimension d (d+ 1) /2. From this structure of differential manifolds, we can derive the notion of tangent space TGPd at each point G \u2208 Pd. In general, tangent spaces are identified with subspaces of the ambient space. In the current setup, as Pd is an open sub-manifold, we identify the tangent space at G as TGPd = Sd, the space of symmetric matrices of size d\u00d7 d.\nThen, in order to turn this differential manifold into a Riemannian manifold, we need to equip its tangent spaces with a metric. One choice of metric (leading to a complete Riemannian manifold) is the affine-invariant metric, which, for SA, SB \u2208 TGPd, is defined as:\n\u3008SA, SB\u3009G = Tr ( G\u22121SAG \u22121SB ) . (7)\nNote that a Riemannian gradient (that is used by first-order Riemannian optimization methods) is defined with respect to a given tangent space. Then, in order to be coherent with the metric of the tangent space (defined in Eq. (7)), gradGf(G), the Riemannian gradient at G can be obtained from the Euclidean gradient\u2207f as\ngradGf(G) = Gsym(\u2207f(G))G.\nAs detailed in Appendix A.1, the Euclidean gradient\u2207f (G) can be obtained as\n\u2207f (G) = \u2211 ij Zij(G)\u2207hij (G) ,\nwhere\nZ(G) = U\n( yy T\n\u2016Uh(G)U\u2016F \u2212 f (G)Uh (G)U \u2016Uh(G)U\u20162F\n) U,\n\u2207hij (G) = X \u2212 12 i (DQi(G) [sym (Aij)])X \u2212 12 i\n+X \u2212 12 j (DQj(G) [sym (Aji)])X \u2212 12 j , Qi(G) = log ( X \u2212 12 i GX \u2212 12 i ) ,\nAij = X 1 2 i X \u2212 12 j Qj(G)X 1 2 j X \u2212 12 i ,\nsym (A) = A+AT\n2 ,\nwhere Df (G) [ G\u0307 ]\nis the directional derivative of f (G) in the direction G\u0307. To our knowledge, there is no closedform formula for computing the directional derivatives DQi(G) [sym (Aij)] and DQj(G) [sym (Aji)]. However, we can numerically evaluate them, as shown in Boumal & Absil (2011) and Al-Mohy & Higham (2009).\nThen any displacement in a tangent space TGPd can be mapped back on the manifold (and vice versa) using the reciprocal logarithmic and exponential mappings (see Fig. 2). Any symmetric matrix SA belonging to TGPd, the tangent space at G, can be mapped on Pd (with the reciprocal operation) as\nA = expG (SA) = G 1 2 exp\n( G\u2212 1 2SAG \u2212 12 ) G 1 2 , (8)\nSA = logG (A) = G 1 2 log\n( G\u2212 1 2AG\u2212 1 2 ) G 1 2 . (9)\nThese formula give the basic ingredients for implementing a geodesic gradient ascent method (as illustrated in Fig. 3) for our objective function3. Indeed, from Eq. (8), the geodesic is given by\nGt+1 = expGt ( \u03b7gradGtf(Gt) ) ,\nwhich leads to Eq. (6)."}, {"heading": "3 Discussions", "text": "In this section, we motivate the use of the LogEuclidean distance and compare it to other distances in Pd."}, {"heading": "3.1 Geometries on Pd", "text": "Depending on the type of geometry chosen, there exist several choices of distances and divergences for comparing SPD matrices. As already discussed in literature Arsigny et al. (2006); Pennec et al. (2006); Dryden et al. (2009); Fletcher & Joshi (2004); Cherian et al. (2011), different tools come along with various implicit invariance properties and computational complexities.\n3More details about optimization on Riemannian matrix manifolds and the geometry of Pd can be found in Absil et al. (2009) and in Bhatia (2009).\nAs explained in Bhatia (2009), when using the geometry implied by the metric in Eq. 7, the distance between two SPD matrices A and B is computed along a curve (called the geodesic) and the associated affine-invariant Riemannian metric (AIRM) distance is defined as\n\u03b4r(A,B) = \u2016log(A\u2212 1 2BA\u2212 1 2 )\u2016F (10)\n=\n( d\u2211\ni=1\nlog2 \u03bbi (A,B)\n) 1 2\nwhere \u03bbi (A,B) are the eigenvalues of the pencil (A,B). As illustrated in Fig. 1, when using the Riemannian geometry, the space Pd becomes a complete manifold. As already stated in Arsigny et al. (2007), this distance is immune to the swelling effect. Thus, it could be a good candidate for distance metric learning for covariance matrices.\nHowever, the AIRM distance comes along with an invariance to a class of congruent transforms Bhatia (2009) and it leads to the following isometry for any M \u2208 Pd:\n\u03b4r(\u0393M (A),\u0393M (B)) = \u03b4r(A,B). (11)\nHence, unlike the LogEuclidean distance, the AIRM distance is immune to the transform \u0393(.) and it can not be learned using our approach.\nApart from their difference in terms of invariance properties, it should be highlighted that the AIRM distance is not negative-definite and then, contrary to the LogEuclidean distance, cannot be used for defining positive-definite kernels on SPD matrices Haasdonk & Bahlmann (2004); Sra (2011); Barachant et al. (2013).\nUsing information geometry and extending divergences to the matrix case Cherian et al. (2011); Sra (2011, 2012), a symmetrized LogDeterminant divergence (also called the symmetrized Stein loss) can also be used. This divergence can be seen as an approximation of the AIRM distance and as such, there exist some bounds between the AIRM distance and this symmetrized divergence Sra (2011). Moreover, this divergence is invariant to the same transformation as the AIRM distance, but can lead under some conditions to a definite-positive kernel.\nG\nIn this work, we look for a distance on SPD matrices tailored for classification. Due to the non-completeness of the space and the swelling effect, the Euclidean distance is not a valid candidate. Hence, the distances derived from Riemannian geometries (and their approximations) are more promising candidates, but in some cases, they are difficult to parametrize. Using the congruent transform \u0393\nG\u2212 1 2 (.), it is possible to parametrize the LogEuclidean distance but not the AIRM distance nor the symmetrized Stein loss due to their invariance properties."}, {"heading": "3.2 Interpretations of LogEuclidean metric learning", "text": "Compared to the Euclidean and AIRM distances, the LogEuclidean distance has several advantages: it is a Riemannian distance (immune to the swelling effect) and is easy to compute. In fact, using the fact that the mapping in Eq. (9) transforms matrices from the curved space Pd to the flat space Sd, the LogEuclidean distance can be interpreted as the Euclidean distance between the matrices mapped into Sd.\nInterestingly, this LogEuclidean metric can be interpreted as the scalar product, \u3008Si, Sj\u3009G = Tr ( G\u2212 1 2SiG \u22121SjG \u2212 12 ) ,\nin the tangent plane ofPd at the pointG, with Si and Sj the mapping aroundG ofXi andXj through the logarithmic map. Hence, according to the taxonomy developed in Bellet et al. (2013), since we optimizeG through a logarithmic mapping on a Riemannian manifold, our approach is an instance of non-linear similarity learning.\nIn this view, the parameter G can be seen as the center of the tangent plane, and it has been empirically observed Yger (2013) that the choice of G has a strong impact on the metric behaviour. So far, G has been heuristically tuned using either the identity matrix or the Riemannian mean Moakher (2005); Jeuris et al. (2012) of data. However, as highlighted later, mapping a manifold onto a tangent space implies a deformation of the data. This deformation can either be harmful or beneficial to classification. By optimizing the choice of G with respect to a discrimination criterion, we try to force the deformation to improve classification performance. As shown in Eq. (7), every tangent space is equipped with a different metric4. Hence, choosing a new reference for a tangent space leads to choosing a new scalar product. In that sense, learning the reference point of a tangent space is equivalent to learning a metric.\nThe LogEuclidean metric is simply a scalar product (i.e., a linear kernel) applied to data mapped through a matrix logarithm. For this reason, it has sometimes been referred to as the LogEuclidean kernel Barachant et al.\n4The scalar product defined at every tangent space TGPd is close to a Mahalanobis scalar product parametrized by G\u22121.\n(2013); Yger (2013); Jayasumana et al. (2013). In this regard, the optimization of the LogEuclidean metric can also be interpreted as a kernel learning approach.\nThe use of the LogEuclidean distance can be interpreted as flattening the Riemannian manifold. As discussed below, such a mapping implies some deformations of the space of SPD matrices (see Fig. 4). Our approach can then be interpreted as finding the mapping such that the implied deformations benefit classification performance."}, {"heading": "3.3 Geometrical motivation", "text": "The deformation implied by mapping points in a tangent space has been first stated as the exponential metric increasing (EMI) property in Bhatia (2009). It gives inequalities between the Riemannian distance \u03b4r and the Euclidean distance \u03b4e in the tangent space at the identity Id.\nTheorem 1. Bhatia (2009) For each pair of points A, B in Pd, we have\n\u03b4r(A,B) \u2265 \u03b4l(A,B) = \u2016 log(A)\u2212 log(B)\u2016F .\nThe equality occurs when A, B and Id belong to the same geodesic. This property, inherited from the nonpositive curvature of Pd, implies a deformation of the shapes when using the logarithmic map.\nUsing the properties of the bijection \u0393 G\u2212 1 2 (.) and with the notations in Eq. (7) and in Eq. (9), a corollary has been proposed in Yger (2013) which extends Theorem 1 to any tangent space:\nTheorem 2. (Yger, 2013) For any A, B and G in Pd, with \u2016 \u00b7 \u2016G, the norm associated to the natural scalar product in TGPd satisfies\n\u03b4r(A,B) \u2265 \u03b4Gl (A,B) = \u2016 logG(A)\u2212 logG(B)\u2016G.\nSimilarly to Theorem 1, the equality occurs when A, B and G belong to the same geodesic. From these two theorems, as illustrated in Fig. 4, it follows that changing the reference point for centering the tangent space modifies how shapes are deformed once they are mapped in this tangent space. This motivates our metric learning approach."}, {"heading": "4 Numerical experiments", "text": "In this section, we report experimental results."}, {"heading": "4.1 Setup", "text": "Concerning the implementation of our approach, we employed a Riemannian trust-region5 Boumal & Absil (2011); Absil et al. (2009) . In practice, we use the following regularized optimization problem:{\nmax G\u2208Pd f(G) s.t. \u03b4r(G0, G) \u2264\nfor someG0. As we have not found any guarantee concerning the convexity or the geodesic-convexity Wiesel (2012) of the centered KTA, we decided to use the Riemannian mean Moakher (2005); Jeuris et al. (2012) as G0. Below, we set = 10 for all datasets.\n5with the implementation provided in the Manopt toolbox provided by Boumal et al. (2014).\nIn our numerical experiments, we report accuracies on balanced datasets using a 1-nearest neighbor (1-NN) classifier equipped with different distances. Our baseline distances for the 1-NN classifier comprise the Euclidean distance \u03b4e, the Riemannian distance \u03b4r and the LogEuclidean distance (parameterized by the identity matrix or the Riemannian mean of the training samples). We compare all these baseline distances to the LogEuclidean distance \u03b4Gl with parameter G learned by our approach. In this section, when the reported results are averaged over several iterations (as in Tab. 1 and Tab. 3), we show when the improvement is significant6 using a bold font."}, {"heading": "4.2 Toy dataset", "text": "First, in order to gain some insights on the behavior of our approach, we designed a simple experiment in which, the covariance matrices are generated as follows:\nX = Qdiag(\u03bb1, . . . , \u03bbr, \u00b51, . . . , \u00b5r)Q>\n+ V diag(| 1|, . . . , | 2r|)V >\nwithQ and V , two random orthonormal square matrices, and \u03bbi \u223c N (5, 0.2) or \u03bbi \u223c N (4, 0.1) respectively for the positive and negative classes and \u00b5i \u223c U([1, 6]) and i \u223c N (0, 1) independent of the class. The dataset is composed of 50 samples in the training set and 500 samples in the test set. It is re-sampled in order to repeat the experiment 10 times and the results are reported in Tab. 1.\nIn this simple experiment, as it was foreseeable, the Euclidean distance achieves good performances. Indeed, omitting the additive noise matrix, the data generation protocol only involves a fixed orthonormal matrix Q for generating the matrices from both classes. Hence, the separation between the classes just becomes linear in terms of the eigenvalues of the covariance matrices. Interestingly, the Riemannian distances (except our approach) performs badly for all cases.\nFrom this experiment, it is interesting to note that the proposed metric learning method helps the LogEuclidean distance to overcome its limitation.. Hence, the LogEuclidean distance with parameter learned by our proposed method is the best not only among the Riemannian distances but among all candidates. However, this toy experiment may be too simple for our applications and we test our methods on real-life datasets below."}, {"heading": "4.3 Brain Computer-Interface", "text": "We tested our approach on Dataset 2a from the BCI Competition IV7 Naeem et al. (2006). The dataset consists of EEG signals (recorded from 22 electrodes) on 9 subjects who were asked to perform left hand, right hand, foot\n6by comparing the two best results with the two-sided Wilcoxon signed rank test at significance level 0.05. 7http://www.bbci.de/competition/iv/\nand tongue motor imagery Pfurtscheller & Neuper (2001) (i.e., EEG signals are recorded while the subject is only imagining the given limb movements without actually moving it).\nFollowing the protocol described in Lotte & Guan (2011), we only used the EEG signals corresponding to the left and right hand. Hence, for every subject, we have 72 trials (i.e. segments of signal) for each class in both training and testing. Moreover, we applied the same band-pass filtering ([8\u221230] Hz) in order to pre-process the raw signals.\nAs shown in literature Barachant et al. (2010, 2013); Yger (2013), for motor imagery signals, the spatial covariance matrix of the trials is a very promising feature. Based on this observation, we extracted the covariance matrix between sensors (i.e., the spatial covariance matrix) from every filtered time segment (i.e., every example to be classified).\nAlthough we could directly use our approach and try to classify samples, the performance would be poor since this dataset is known to show some non-stationarity between sessions. In order to cope with this problem, we employ an adaptive kernel formulation proposed in Barachant et al. (2013). This method cancels the non-stationarity in the data by whitening the covariance matrices in the training session and test session by subtracting the Riemannian mean of the training and test datasets, respectively. In practice, whitening is carried out by first applying the congruent transformation \u0393\nX\u0304\u2212 1 2 (\u00b7), where X\u0304 is the Riemannian mean of the data set. As illustrated in Fig. 5, this whitening process has the effect of spreading the covariance matrices from both training and test dataset around the identity matrix. For a dataset spread around X\u0304 , this has the effect of \u201ctransporting\u201d the data on the manifold along the geodesic. For real-life applications, this whitening process can be criticized since it uses the test data. However, this can be carried out in a semi-supervised fashion by estimating X\u0304\u2212 1 2 during the calibration phase. Therefore, this process would still be practical in many applications. As reported in Tab. 2, compared to other distances for handling covariance matrices, our proposed approach performs the best on 5 subjects and is never the worst. On average over the subjects, we observe a significant gain by using an optimized reference point for the LogEuclidean distance."}, {"heading": "4.4 Texture classification", "text": "As another example of real-world problems, we consider a subset of four textures (shown in Fig 6) of the Brodatz dataset Brodatz (1966). Similarly to the works of Mairal et al. (2009) and Yger & Rakotomamonjy (2011), we extracted 16\u00d7 16 patches from every texture. For every couple of textures, the training set composes patches from the left half of each texture and the test set composes patches from the right half. In every set, patches may overlap, but the training and test sets do not overlap each other.\nEvery method was trained on 50 patches selected from the training set and tested on 300 patches selected from the test set. Classification rates have been computed as the average of 50 runs after re-sampling of the training and test sets.\nAs proposed in Tou et al. (2009), we built covariance matrices using a bank of 8 Gabor filters (4 angles and 2 scales). Hence, each 16\u00d7 16 patch is transformed into a 8\u00d7 8 covariance matrix.\nTab. 3 summarizes the experiments on texture classification. First, it clearly shows the interest of using the Riemannian geometry. Indeed, for this data, the Euclidean distance performs poorly. Moreover, this experiment shows again that our supervised metric learning algorithm for choosing the reference of a LogEuclidean distance is effective."}, {"heading": "5 Conclusion and perspectives", "text": "In this paper, we have introduced a novel approach for selecting a LogEuclidean metric. This approach is founded on theoretical observations about the distortion implied by a LogEuclidean metric. Then, we proposed to cast the problem of choosing a relevant metric as a kernel learning algorithm with a kernel target alignment criterion. We\nfinally solved this problem using tools from the field of optimization on manifold and applied it to synthetic and real-world data.\nAlthough we restricted ourselves to binary classification problems, the extension of our approach to multi-class problems Ramona et al. (2012) is straightforward.\nWhen huge datasets are involved, our optimization algorithm might not scale well. A stochastic setting on manifold Bonnabel (2013) could be a promising extention.\nWe only considered full-rank covariance matrices, but the extension of our approach to low-rank matrices is a challenging and interesting future work, since this corresponds to supervised feature extraction under the LogEuclidean metric. Exploration along this line of research would bridge the gap between our approach and the one proposed in Harandi et al. (2014)."}, {"heading": "A Appendix", "text": "In the appendix, we give more details about the derivation of the our cost function f and we give the results of an extra numerical experiment on a toy dataset.\nA.1 Gradient of f(G) Deriving the cost function f using the classical tricks of the trade of matrix derivation is not obvious (if not impossible). Indeed, this function is based on the matrix logarithm whose derivative is not simple to obtain.\nTo obtain the Euclidean gradient\u2207f of the function f , we need Df(X)[H], its directional derivative8 at a point X and in a direction H , which is defined as:\nDf(X)[H] = lim h\u21920 f(X + hH)\u2212 f(X) h\n8Note that it is sometimes called Fre\u0301chet derivative.\nIn order to obtain the gradient of f , we express its directional derivative. As the the gradient and the directional derivatives are linked with the following equality\nDf (G) [ G\u0307 ] = \u2329 \u2207f (G) , G\u0307 \u232a we need the adjoint of the directional derivative in order to obtain\u2207f (G).\nUsing standard properties of the directional derivatives, we formulate Df (G) [ G\u0307 ] as :\nDf (G) [ G\u0307 ] = \u3008UDh (G) [ G\u0307 ] U, yy T\u3009F\u2016Uh (G)U\u2016F \u2212 \u3008Uh (G)U, yy T\u3009F \u3008 Uh(G)U\u2016Uh(G)U\u2016F UDh (G) [ G\u0307 ] U\u3009F\n\u2016Uh (G)U\u20162F\n(12)\n= \u2329 Dh (G) [ G\u0307 ] , U ( yy T\n\u2016Uh(G)U\u2016F \u2212 f (G)Uh (G)U \u2016Uh(G)U\u20162F ) U\ufe38 \ufe37\ufe37 \ufe38\nZ(G)\n\u232a F\n(13)\nIn Eq. 13, the core quantity to estimate is Dh (G). Hopefully, the function h (G) has already been studied in Boumal & Absil (2011). Using those results with the same convention (sym (A) = A+A T\n2 ), we have the gradient of hij(G):\n\u2207hij (G) =X \u2212 12 i\n( D log ( X \u2212 12 i GX \u2212 12 i ) [ sym ( X 1 2 i X \u2212 12 j log ( X \u2212 12 j GX \u2212 12 j ) X 1 2 j X \u2212 12 i )]) X \u2212 12 i (14)\n+X \u2212 12 j\n( D log ( X \u2212 12 j GX \u2212 12 j ) [ sym ( X \u2212 12 j X 1 2 i log ( X \u2212 12 i GX \u2212 12 i ) X \u2212 12 i X 1 2 j )]) X \u2212 12 j\nNote that the gradient of\u2207hij depends on the directionnal derivative of the matrix logarithm. To our knowledge, there is no closed-form formula of this directionnal derivative but is can be evaluated numerically using the algorithm in (Boumal & Absil, 2011; Al-Mohy & Higham, 2009).\nThen, the Euclidean gradient of f at G is expressed with respect to the adjoint of Dh (G):\n\u2207f (G) = (Dh (G))? [Z (G)] . (15)\nHence, after some algebra (detailed in the next section), we express the adjoint as :\n\u2329 Z,Dh (G) [ G\u0307 ]\u232a F = \u2329 (Dh(G))?[Z(G)]\ufe37 \ufe38\ufe38 \ufe37\u2211 ij Zij\u2207hij (G), G\u0307 \u232a F . (16)\nTo sum up, using Eq.14 and Eq.13, we have : \u2207f (G) = \u2211 ij Zij\u2207hij (G) . (17)\nNote that, before solving our optimization problem on data, the validity of our Euclidean gradient formula has been numerically checked using the sanity checking tools provided in the Manopt toolbox by Boumal et al. (2014).\nA.2 Adjoint of Dh (G)\nIn order to obtain the adjoint, using the decomposition over the canonical basis G\u0307 = \u2211\nk,l G\u0307klekel T, we write :\u2329 Z,Dh (G) [ G\u0307 ]\u232a F (18)\n= \u2211 ij Zij ( Dh (G) [ G\u0307 ]) ij (19)\n= \u2211 ij \u2329 \u2207hij (G) , G\u0307 \u232a F Zij (20)\n= \u2211 kl \u2211 ij \u2329 \u2207hij (G) , Zijekel T \u232a F G\u0307kl (21) = \u2211 kl \u2211 ij Zij\u2207hij (G)  kl G\u0307kl (22)\n= \u2329 (Dh(G))?[Z(G)]\ufe37 \ufe38\ufe38 \ufe37\u2211 ij Zij\u2207hij (G), G\u0307 \u232a F\n(23)\nA.3 Extra numerical experiments We also report the numerical experiments of a simpler toy experiment. In this experiment, the data are generated in the same way :\nX =Qdiag(\u03bb1, . . . , \u03bbr, \u00b51, . . . , \u00b5r)Q>\n+V diag(| 1|, . . . , | 2r|)V >\nwith Q and V , two random orthonormal square matrices, and \u03bbi \u223c N (5, 0.2) or \u03bbi \u223c N (4, 0.1) respectively for the positive and negative classes and \u00b5i \u223c U([1, 3]) and i \u223c N (0, 1) independently of the class.\nThe difference between those two experiments resides in the range in which the variables \u00b5i are sampled. Here, the range of the uniform distribution is smaller and this toy dataset becomes easier to classify.\nThe datasets were composed of 50 examples in the training set and 500 in test and every experiment was repeated 10 times.\nIn this simple experiment, as in the other toy experiment, the Euclidean distance achieves the best performances for the same reasons.\nEven if the Euclidean distance seems to be the most suited to those data, this toy experiment is interesting. Indeed, among the distances respecting the Riemannian geometry of the data (namely the LogEuclidean and Riemannian distances), the LogEuclidean distance parametrized by our approach performs the best. Optimizing the reference point G (with respect to a KTA criterion) seems to diminish the impact of the uninformative variables \u00b5.\nNote that in this experiment, the results of all approaches (including the Riemannian approaches) improves as the dimensionality grows. Contrary to the results reported in 1, in this experiment, the level of information is higher than the level of noise in the data. Hence, the more dimension we add, the easier it becomes to classify."}], "references": [{"title": "Optimization algorithms on matrix manifolds", "author": ["Absil", "P-A", "Mahony", "Robert", "Sepulchre", "Rodolphe"], "venue": null, "citeRegEx": "Absil et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Absil et al\\.", "year": 2009}, {"title": "Computing the fr\u00e9chet derivative of the matrix exponential, with an application to condition number estimation", "author": ["Al-Mohy", "Awad H", "Higham", "Nicholas J"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Al.Mohy et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Al.Mohy et al\\.", "year": 2009}, {"title": "Log-euclidean metrics for fast and simple calculus on diffusion tensors", "author": ["Arsigny", "Vincent", "Fillard", "Pierre", "Pennec", "Xavier", "Ayache", "Nicholas"], "venue": "Magnetic resonance in medicine,", "citeRegEx": "Arsigny et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Arsigny et al\\.", "year": 2006}, {"title": "Geometric means in a novel vector space structure on symmetric positive-definite matrices", "author": ["Arsigny", "Vincent", "Fillard", "Pierre", "Pennec", "Xavier", "Ayache", "Nicholas"], "venue": "SIAM journal on matrix analysis and applications,", "citeRegEx": "Arsigny et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Arsigny et al\\.", "year": 2007}, {"title": "Riemannian geometry applied to bci classification", "author": ["Barachant", "Alexandre", "Bonnet", "St\u00e9phane", "Congedo", "Marco", "Jutten", "Christian"], "venue": "In LVA/ICA,", "citeRegEx": "Barachant et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Barachant et al\\.", "year": 2010}, {"title": "Classification of covariance matrices using a riemannian-based kernel for bci applications", "author": ["Barachant", "Alexandre", "Bonnet", "St\u00e9phane", "Congedo", "Marco", "Jutten", "Christian"], "venue": null, "citeRegEx": "Barachant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Barachant et al\\.", "year": 2013}, {"title": "Learning good edit similarities with generalization guarantees", "author": ["Bellet", "Aur\u00e9lien", "Habrard", "Amaury", "Sebban", "Marc"], "venue": "In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD),", "citeRegEx": "Bellet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bellet et al\\.", "year": 2011}, {"title": "A survey on metric learning for feature vectors and structured data", "author": ["Bellet", "Aur\u00e9lien", "Habrard", "Amaury", "Sebban", "Marc"], "venue": "arXiv preprint arXiv:1306.6709,", "citeRegEx": "Bellet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellet et al\\.", "year": 2013}, {"title": "Positive definite matrices", "author": ["Bhatia", "Rajendra"], "venue": null, "citeRegEx": "Bhatia and Rajendra.,? \\Q2009\\E", "shortCiteRegEx": "Bhatia and Rajendra.", "year": 2009}, {"title": "Stochastic gradient descent on riemannian manifolds", "author": ["Bonnabel", "Silvere"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Bonnabel and Silvere.,? \\Q2013\\E", "shortCiteRegEx": "Bonnabel and Silvere.", "year": 2013}, {"title": "Discrete regression methods on the cone of positive-definite matrices", "author": ["Boumal", "Nicolas", "Absil", "P-A"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Boumal et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boumal et al\\.", "year": 2011}, {"title": "Manopt, a matlab toolbox for optimization on manifolds", "author": ["Boumal", "Nicolas", "Mishra", "Bamdev", "Absil", "P-A", "Sepulchre", "Rodolphe"], "venue": "The Journal of Machine Learning Research (JMLR),", "citeRegEx": "Boumal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Boumal et al\\.", "year": 2014}, {"title": "Textures: a photographic album for artists and designers", "author": ["Brodatz", "Phil"], "venue": "Dover Pubns,", "citeRegEx": "Brodatz and Phil.,? \\Q1966\\E", "shortCiteRegEx": "Brodatz and Phil.", "year": 1966}, {"title": "Efficient similarity search for covariance matrices via the jensen-bregman logdet divergence", "author": ["Cherian", "Anoop", "Sra", "Suvrit", "Banerjee", "Arindam", "Papanikolopoulos", "Nikolaos"], "venue": "In IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Cherian et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cherian et al\\.", "year": 2011}, {"title": "Algorithms for learning kernels based on centered alignment", "author": ["Cortes", "Corinna", "Mohri", "Mehryar", "Rostamizadeh", "Afshin"], "venue": "The Journal of Machine Learning Research (JMLR),", "citeRegEx": "Cortes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2012}, {"title": "Nearest neighbor pattern classification", "author": ["Cover", "Thomas", "Hart", "Peter"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cover et al\\.,? \\Q1967\\E", "shortCiteRegEx": "Cover et al\\.", "year": 1967}, {"title": "On kernel target alignment", "author": ["Cristianini", "Nello", "Shawe-Taylor", "John", "Elisseeff", "Andre", "Kandola", "Jaz"], "venue": "In NIPS,", "citeRegEx": "Cristianini et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Cristianini et al\\.", "year": 2001}, {"title": "Ground metric learning", "author": ["Cuturi", "Marco", "Avis", "David"], "venue": "The Journal of Machine Learning Research (JMLR),", "citeRegEx": "Cuturi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cuturi et al\\.", "year": 2014}, {"title": "Non-euclidean statistics for covariance matrices, with applications to diffusion tensor imaging", "author": ["Dryden", "Ian L", "Koloydenko", "Alexey", "Zhou", "Diwei"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Dryden et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dryden et al\\.", "year": 2009}, {"title": "Principal geodesic analysis on symmetric spaces: Statistics of diffusion tensors", "author": ["Fletcher", "P Thomas", "Joshi", "Sarang"], "venue": "In Computer Vision and Mathematical Methods in Medical and Biomedical Image Analysis,", "citeRegEx": "Fletcher et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fletcher et al\\.", "year": 2004}, {"title": "Learning with distance substitution kernels", "author": ["Haasdonk", "Bernard", "Bahlmann", "Claus"], "venue": "Pattern Recognition, pp", "citeRegEx": "Haasdonk et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Haasdonk et al\\.", "year": 2004}, {"title": "From manifold to manifold: geometry-aware dimensionality reduction for spd matrices", "author": ["Harandi", "Mehrtash", "Salzmann", "Mathieu", "Hartley", "Richard"], "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),", "citeRegEx": "Harandi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Harandi et al\\.", "year": 2014}, {"title": "Kernel methods on the riemannian manifold of symmetric positive definite matrices", "author": ["Jayasumana", "Sadeep", "Hartley", "Richard", "Salzmann", "Mathieu", "Li", "Hongdong", "Harandi", "Mehrtash"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Jayasumana et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jayasumana et al\\.", "year": 2013}, {"title": "A survey and comparison of contemporary algorithms for computing the matrix geometric mean", "author": ["Jeuris", "Ben", "Vandebril", "Raf", "Vandereycken", "Bart"], "venue": "Electronic Transactions on Numerical Analysis,", "citeRegEx": "Jeuris et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jeuris et al\\.", "year": 2012}, {"title": "Non-linear metric learning", "author": ["Kedem", "Dor", "Tyree", "Stephen", "Sha", "Fei", "Lanckriet", "Gert R", "Weinberger", "Kilian Q"], "venue": "In NIPS, pp", "citeRegEx": "Kedem et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kedem et al\\.", "year": 2012}, {"title": "Robust structural metric learning", "author": ["Lim", "Daryl", "Lanckriet", "Gert", "McFee", "Brian"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Lim et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lim et al\\.", "year": 2013}, {"title": "Regularizing common spatial patterns to improve bci designs: unified theory and new algorithms", "author": ["Lotte", "Fabien", "Guan", "Cuntai"], "venue": "IEEE Transactions on Biomedical Engineering,", "citeRegEx": "Lotte et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lotte et al\\.", "year": 2011}, {"title": "Neighborhood repulsed metric learning for kinship verification", "author": ["Lu", "Jiwen", "Hu", "Junlin", "Zhou", "Xiuzhuang", "Shang", "Yuanyuan", "Tan", "Yap-Peng", "Wang", "Gang"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Lu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2012}, {"title": "Supervised dictionary learning", "author": ["Mairal", "Julien", "Ponce", "Jean", "Sapiro", "Guillermo", "Zisserman", "Andrew", "Bach", "Francis"], "venue": "In NIPS,", "citeRegEx": "Mairal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2009}, {"title": "Metric learning for large scale image classification: Generalizing to new classes at near-zero cost", "author": ["Mensink", "Thomas", "Verbeek", "Jakob", "Perronnin", "Florent", "Csurka", "Gabriela"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "Mensink et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mensink et al\\.", "year": 2012}, {"title": "A differential geometric approach to the geometric mean of symmetric positive-definite matrices", "author": ["Moakher", "Maher"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Moakher and Maher.,? \\Q2005\\E", "shortCiteRegEx": "Moakher and Maher.", "year": 2005}, {"title": "Seperability of four-class motor imagery data using independent components analysis", "author": ["M Naeem", "C Brunner", "R Leeb", "B Graimann", "G. Pfurtscheller"], "venue": "Journal of neural engineering,", "citeRegEx": "Naeem et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Naeem et al\\.", "year": 2006}, {"title": "Hamming distance metric learning", "author": ["Norouzi", "Mohammad", "Blei", "David M", "Salakhutdinov", "Ruslan R"], "venue": "In NIPS, pp", "citeRegEx": "Norouzi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2012}, {"title": "A riemannian framework for tensor computing", "author": ["Pennec", "Xavier", "Fillard", "Pierre", "Ayache", "Nicholas"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Pennec et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pennec et al\\.", "year": 2006}, {"title": "Motor imagery and direct brain-computer communication", "author": ["Pfurtscheller", "Gert", "Neuper", "Christa"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Pfurtscheller et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Pfurtscheller et al\\.", "year": 2001}, {"title": "Multiclass feature selection with kernel gram-matrix-based criteria", "author": ["Ramona", "Mathieu", "Richard", "Ga\u00ebl", "David", "Bertrand"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "Ramona et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ramona et al\\.", "year": 2012}, {"title": "Learning a distance metric from a network", "author": ["Shaw", "Blake", "Huang", "Bert", "Jebara", "Tony"], "venue": "In NIPS, pp. 1899\u20131907,", "citeRegEx": "Shaw et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shaw et al\\.", "year": 2011}, {"title": "Positive definite matrices and the s-divergence", "author": ["Sra", "Suvrit"], "venue": "arXiv preprint arXiv:1110.1773,", "citeRegEx": "Sra and Suvrit.,? \\Q2011\\E", "shortCiteRegEx": "Sra and Suvrit.", "year": 2011}, {"title": "A new metric on the manifold of kernel matrices with application to matrix geometric means", "author": ["Sra", "Suvrit"], "venue": "In NIPS, pp", "citeRegEx": "Sra and Suvrit.,? \\Q2012\\E", "shortCiteRegEx": "Sra and Suvrit.", "year": 2012}, {"title": "Gabor filters as feature images for covariance matrix on texture classification problem", "author": ["Tou", "Jing Yi", "Tay", "Yong Haur", "Lau", "Phooi Yee"], "venue": "In Advances in Neuro-Information Processing,", "citeRegEx": "Tou et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tou et al\\.", "year": 2009}, {"title": "Region covariance: A fast descriptor for detection and classification", "author": ["Tuzel", "Oncel", "Porikli", "Fatih", "Meer", "Peter"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "Tuzel et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tuzel et al\\.", "year": 2006}, {"title": "Pedestrian detection via classification on riemannian manifolds", "author": ["Tuzel", "Oncel", "Porikli", "Fatih", "Meer", "Peter"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Tuzel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tuzel et al\\.", "year": 2008}, {"title": "Prodis-contshc: learning protein dissimilarity measures and hierarchical context coherently for protein-protein comparison in protein database retrieval", "author": ["Wang", "Jingyan", "Gao", "Xin", "Quanquan", "Li", "Yongping"], "venue": "BMC bioinformatics,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Geodesic convexity and covariance estimation", "author": ["Wiesel", "Ami"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Wiesel and Ami.,? \\Q2012\\E", "shortCiteRegEx": "Wiesel and Ami.", "year": 2012}, {"title": "A review of kernels on covariance matrices for bci applications", "author": ["Yger", "Florian"], "venue": "In IEEE International Workshop on Machine Learning for Signal Processing (MLSP),", "citeRegEx": "Yger and Florian.,? \\Q2013\\E", "shortCiteRegEx": "Yger and Florian.", "year": 2013}, {"title": "Wavelet kernel learning", "author": ["Yger", "Florian", "Rakotomamonjy", "Alain"], "venue": "Pattern Recognition,", "citeRegEx": "Yger et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yger et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 20, "context": "So far, various methods have been developed for learning Mahalanobis distances in the Euclidean setting, and such metric learning methods have been successfully applied to diverse real-world problems including music recommendation Lim et al. (2013), face recognition Lu et al.", "startOffset": 231, "endOffset": 249}, {"referenceID": 20, "context": "So far, various methods have been developed for learning Mahalanobis distances in the Euclidean setting, and such metric learning methods have been successfully applied to diverse real-world problems including music recommendation Lim et al. (2013), face recognition Lu et al. (2012), image classification Mensink et al.", "startOffset": 231, "endOffset": 284}, {"referenceID": 20, "context": "So far, various methods have been developed for learning Mahalanobis distances in the Euclidean setting, and such metric learning methods have been successfully applied to diverse real-world problems including music recommendation Lim et al. (2013), face recognition Lu et al. (2012), image classification Mensink et al. (2012), link prediction in networks Shaw et al.", "startOffset": 231, "endOffset": 328}, {"referenceID": 20, "context": "So far, various methods have been developed for learning Mahalanobis distances in the Euclidean setting, and such metric learning methods have been successfully applied to diverse real-world problems including music recommendation Lim et al. (2013), face recognition Lu et al. (2012), image classification Mensink et al. (2012), link prediction in networks Shaw et al. (2011) and bioinformatics Wang et al.", "startOffset": 231, "endOffset": 376}, {"referenceID": 20, "context": "So far, various methods have been developed for learning Mahalanobis distances in the Euclidean setting, and such metric learning methods have been successfully applied to diverse real-world problems including music recommendation Lim et al. (2013), face recognition Lu et al. (2012), image classification Mensink et al. (2012), link prediction in networks Shaw et al. (2011) and bioinformatics Wang et al. (2012). For the survey of recent advances, issues and perspectives in metric learning, see Bellet et al.", "startOffset": 231, "endOffset": 414}, {"referenceID": 4, "context": "For the survey of recent advances, issues and perspectives in metric learning, see Bellet et al. (2013). Classical metric learning approaches focused on an Euclidean setting which do not apply to complex or structured objects.", "startOffset": 83, "endOffset": 104}, {"referenceID": 4, "context": "For the survey of recent advances, issues and perspectives in metric learning, see Bellet et al. (2013). Classical metric learning approaches focused on an Euclidean setting which do not apply to complex or structured objects. Recently, metric learning for complex objects such as histograms Cuturi & Avis (2014); Kedem et al.", "startOffset": 83, "endOffset": 313}, {"referenceID": 4, "context": "For the survey of recent advances, issues and perspectives in metric learning, see Bellet et al. (2013). Classical metric learning approaches focused on an Euclidean setting which do not apply to complex or structured objects. Recently, metric learning for complex objects such as histograms Cuturi & Avis (2014); Kedem et al. (2012), binary codes Norouzi et al.", "startOffset": 83, "endOffset": 334}, {"referenceID": 4, "context": "For the survey of recent advances, issues and perspectives in metric learning, see Bellet et al. (2013). Classical metric learning approaches focused on an Euclidean setting which do not apply to complex or structured objects. Recently, metric learning for complex objects such as histograms Cuturi & Avis (2014); Kedem et al. (2012), binary codes Norouzi et al. (2012) and strings Bellet et al.", "startOffset": 83, "endOffset": 370}, {"referenceID": 4, "context": "For the survey of recent advances, issues and perspectives in metric learning, see Bellet et al. (2013). Classical metric learning approaches focused on an Euclidean setting which do not apply to complex or structured objects. Recently, metric learning for complex objects such as histograms Cuturi & Avis (2014); Kedem et al. (2012), binary codes Norouzi et al. (2012) and strings Bellet et al. (2011) have been actively explored, to which the Euclidean distance is not relevant.", "startOffset": 83, "endOffset": 403}, {"referenceID": 16, "context": "(2006); Dryden et al. (2009), brain-computer interfaces Barachant et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 4, "context": "(2009), brain-computer interfaces Barachant et al. (2010, 2013), pedestrian detection Tuzel et al. (2008) and texture classification Tuzel et al.", "startOffset": 34, "endOffset": 106}, {"referenceID": 4, "context": "(2009), brain-computer interfaces Barachant et al. (2010, 2013), pedestrian detection Tuzel et al. (2008) and texture classification Tuzel et al. (2006); Tou et al.", "startOffset": 34, "endOffset": 153}, {"referenceID": 4, "context": "(2009), brain-computer interfaces Barachant et al. (2010, 2013), pedestrian detection Tuzel et al. (2008) and texture classification Tuzel et al. (2006); Tou et al. (2009). In these applications, covariance matrices are either extracted from a physical model of the studied phenomenon (for diffusion tensor imaging) or as an empirical estimator from observations (for signal processing and computer-vision tasks).", "startOffset": 34, "endOffset": 172}, {"referenceID": 2, "context": "However, the Euclidean geometry for averaging SPD matrices can result in a swelling effect Arsigny et al. (2007), i.", "startOffset": 91, "endOffset": 113}, {"referenceID": 2, "context": "However, the Euclidean geometry for averaging SPD matrices can result in a swelling effect Arsigny et al. (2007), i.e., the determinant of the average can be bigger than the determinant of each matrix. As also remarked in Fletcher & Joshi (2004) and illustrated in Fig.", "startOffset": 91, "endOffset": 246}, {"referenceID": 2, "context": "The LogEuclidean metric (and the derived distance and kernel) has been used in the literature Arsigny et al. (2006); Barachant et al.", "startOffset": 94, "endOffset": 116}, {"referenceID": 2, "context": "The LogEuclidean metric (and the derived distance and kernel) has been used in the literature Arsigny et al. (2006); Barachant et al. (2013), but its parameterization remained underestimated until recently Yger (2013).", "startOffset": 94, "endOffset": 141}, {"referenceID": 2, "context": "The LogEuclidean metric (and the derived distance and kernel) has been used in the literature Arsigny et al. (2006); Barachant et al. (2013), but its parameterization remained underestimated until recently Yger (2013). In this paper, we propose a supervised approach to learning the LogEuclidean metric.", "startOffset": 94, "endOffset": 218}, {"referenceID": 2, "context": "The LogEuclidean metric (and the derived distance and kernel) has been used in the literature Arsigny et al. (2006); Barachant et al. (2013), but its parameterization remained underestimated until recently Yger (2013). In this paper, we propose a supervised approach to learning the LogEuclidean metric. More specifically, we formulate our LogEuclidean metric learning problem as the kernel-target alignment problem Cristianini et al. (2001); Cortes et al.", "startOffset": 94, "endOffset": 442}, {"referenceID": 2, "context": "The LogEuclidean metric (and the derived distance and kernel) has been used in the literature Arsigny et al. (2006); Barachant et al. (2013), but its parameterization remained underestimated until recently Yger (2013). In this paper, we propose a supervised approach to learning the LogEuclidean metric. More specifically, we formulate our LogEuclidean metric learning problem as the kernel-target alignment problem Cristianini et al. (2001); Cortes et al. (2012), and solve the non-trivial optimization problem using the Riemannian geometry for SPD matrices.", "startOffset": 94, "endOffset": 464}, {"referenceID": 2, "context": "G was implicitly chosen as the identity matrix in Arsigny et al. (2006), while G was heuristically chosen as the Riemannian mean of training samples in Barachant et al.", "startOffset": 50, "endOffset": 72}, {"referenceID": 2, "context": "G was implicitly chosen as the identity matrix in Arsigny et al. (2006), while G was heuristically chosen as the Riemannian mean of training samples in Barachant et al. (2013) and Yger (2013).", "startOffset": 50, "endOffset": 176}, {"referenceID": 2, "context": "G was implicitly chosen as the identity matrix in Arsigny et al. (2006), while G was heuristically chosen as the Riemannian mean of training samples in Barachant et al. (2013) and Yger (2013). Although these choices sound reasonable, we argue that they are sub-optimal when \u03b4 l is used for nearest neighbor classification.", "startOffset": 50, "endOffset": 192}, {"referenceID": 15, "context": "2 Metric learning with kernel-target alignment For metric learning, we employ the (centered) kernel target alignment (KTA) criterion Cristianini et al. (2001); Cortes et al.", "startOffset": 133, "endOffset": 159}, {"referenceID": 14, "context": "(2001); Cortes et al. (2012):", "startOffset": 8, "endOffset": 29}, {"referenceID": 0, "context": "2 For more sophisticated optimization methods, see Absil et al. (2009) and Boumal et al.", "startOffset": 51, "endOffset": 71}, {"referenceID": 0, "context": "2 For more sophisticated optimization methods, see Absil et al. (2009) and Boumal et al. (2014).", "startOffset": 51, "endOffset": 96}, {"referenceID": 1, "context": "As already discussed in literature Arsigny et al. (2006); Pennec et al.", "startOffset": 35, "endOffset": 57}, {"referenceID": 1, "context": "As already discussed in literature Arsigny et al. (2006); Pennec et al. (2006); Dryden et al.", "startOffset": 35, "endOffset": 79}, {"referenceID": 1, "context": "As already discussed in literature Arsigny et al. (2006); Pennec et al. (2006); Dryden et al. (2009); Fletcher & Joshi (2004); Cherian et al.", "startOffset": 35, "endOffset": 101}, {"referenceID": 1, "context": "As already discussed in literature Arsigny et al. (2006); Pennec et al. (2006); Dryden et al. (2009); Fletcher & Joshi (2004); Cherian et al.", "startOffset": 35, "endOffset": 126}, {"referenceID": 1, "context": "As already discussed in literature Arsigny et al. (2006); Pennec et al. (2006); Dryden et al. (2009); Fletcher & Joshi (2004); Cherian et al. (2011), different tools come along with various implicit invariance properties and computational complexities.", "startOffset": 35, "endOffset": 149}, {"referenceID": 0, "context": "3More details about optimization on Riemannian matrix manifolds and the geometry of Pd can be found in Absil et al. (2009) and in Bhatia (2009).", "startOffset": 103, "endOffset": 123}, {"referenceID": 0, "context": "3More details about optimization on Riemannian matrix manifolds and the geometry of Pd can be found in Absil et al. (2009) and in Bhatia (2009).", "startOffset": 103, "endOffset": 144}, {"referenceID": 2, "context": "As already stated in Arsigny et al. (2007), this distance is immune to the swelling effect.", "startOffset": 21, "endOffset": 43}, {"referenceID": 2, "context": "As already stated in Arsigny et al. (2007), this distance is immune to the swelling effect. Thus, it could be a good candidate for distance metric learning for covariance matrices. However, the AIRM distance comes along with an invariance to a class of congruent transforms Bhatia (2009) and it leads to the following isometry for any M \u2208 Pd: \u03b4r(\u0393M (A),\u0393M (B)) = \u03b4r(A,B).", "startOffset": 21, "endOffset": 288}, {"referenceID": 4, "context": "Apart from their difference in terms of invariance properties, it should be highlighted that the AIRM distance is not negative-definite and then, contrary to the LogEuclidean distance, cannot be used for defining positive-definite kernels on SPD matrices Haasdonk & Bahlmann (2004); Sra (2011); Barachant et al. (2013). Using information geometry and extending divergences to the matrix case Cherian et al.", "startOffset": 295, "endOffset": 319}, {"referenceID": 4, "context": "Apart from their difference in terms of invariance properties, it should be highlighted that the AIRM distance is not negative-definite and then, contrary to the LogEuclidean distance, cannot be used for defining positive-definite kernels on SPD matrices Haasdonk & Bahlmann (2004); Sra (2011); Barachant et al. (2013). Using information geometry and extending divergences to the matrix case Cherian et al. (2011); Sra (2011, 2012), a symmetrized LogDeterminant divergence (also called the symmetrized Stein loss) can also be used.", "startOffset": 295, "endOffset": 414}, {"referenceID": 4, "context": "Apart from their difference in terms of invariance properties, it should be highlighted that the AIRM distance is not negative-definite and then, contrary to the LogEuclidean distance, cannot be used for defining positive-definite kernels on SPD matrices Haasdonk & Bahlmann (2004); Sra (2011); Barachant et al. (2013). Using information geometry and extending divergences to the matrix case Cherian et al. (2011); Sra (2011, 2012), a symmetrized LogDeterminant divergence (also called the symmetrized Stein loss) can also be used. This divergence can be seen as an approximation of the AIRM distance and as such, there exist some bounds between the AIRM distance and this symmetrized divergence Sra (2011). Moreover, this divergence is invariant to the same transformation as the AIRM distance, but can lead under some conditions to a definite-positive kernel.", "startOffset": 295, "endOffset": 707}, {"referenceID": 4, "context": "Hence, according to the taxonomy developed in Bellet et al. (2013), since we optimizeG through a logarithmic mapping on a Riemannian manifold, our approach is an instance of non-linear similarity learning.", "startOffset": 46, "endOffset": 67}, {"referenceID": 4, "context": "Hence, according to the taxonomy developed in Bellet et al. (2013), since we optimizeG through a logarithmic mapping on a Riemannian manifold, our approach is an instance of non-linear similarity learning. In this view, the parameter G can be seen as the center of the tangent plane, and it has been empirically observed Yger (2013) that the choice of G has a strong impact on the metric behaviour.", "startOffset": 46, "endOffset": 333}, {"referenceID": 4, "context": "Hence, according to the taxonomy developed in Bellet et al. (2013), since we optimizeG through a logarithmic mapping on a Riemannian manifold, our approach is an instance of non-linear similarity learning. In this view, the parameter G can be seen as the center of the tangent plane, and it has been empirically observed Yger (2013) that the choice of G has a strong impact on the metric behaviour. So far, G has been heuristically tuned using either the identity matrix or the Riemannian mean Moakher (2005); Jeuris et al.", "startOffset": 46, "endOffset": 509}, {"referenceID": 4, "context": "Hence, according to the taxonomy developed in Bellet et al. (2013), since we optimizeG through a logarithmic mapping on a Riemannian manifold, our approach is an instance of non-linear similarity learning. In this view, the parameter G can be seen as the center of the tangent plane, and it has been empirically observed Yger (2013) that the choice of G has a strong impact on the metric behaviour. So far, G has been heuristically tuned using either the identity matrix or the Riemannian mean Moakher (2005); Jeuris et al. (2012) of data.", "startOffset": 46, "endOffset": 531}, {"referenceID": 22, "context": "(2013); Yger (2013); Jayasumana et al. (2013). In this regard, the optimization of the LogEuclidean metric can also be interpreted as a kernel learning approach.", "startOffset": 21, "endOffset": 46}, {"referenceID": 0, "context": "1 Setup Concerning the implementation of our approach, we employed a Riemannian trust-region5 Boumal & Absil (2011); Absil et al. (2009) .", "startOffset": 117, "endOffset": 137}, {"referenceID": 21, "context": "As we have not found any guarantee concerning the convexity or the geodesic-convexity Wiesel (2012) of the centered KTA, we decided to use the Riemannian mean Moakher (2005); Jeuris et al. (2012) as G0.", "startOffset": 175, "endOffset": 196}, {"referenceID": 10, "context": "5with the implementation provided in the Manopt toolbox provided by Boumal et al. (2014).", "startOffset": 68, "endOffset": 89}, {"referenceID": 31, "context": "3 Brain Computer-Interface We tested our approach on Dataset 2a from the BCI Competition IV7 Naeem et al. (2006). The dataset consists of EEG signals (recorded from 22 electrodes) on 9 subjects who were asked to perform left hand, right hand, foot 6by comparing the two best results with the two-sided Wilcoxon signed rank test at significance level 0.", "startOffset": 93, "endOffset": 113}, {"referenceID": 4, "context": "As shown in literature Barachant et al. (2010, 2013); Yger (2013), for motor imagery signals, the spatial covariance matrix of the trials is a very promising feature.", "startOffset": 23, "endOffset": 66}, {"referenceID": 4, "context": "As shown in literature Barachant et al. (2010, 2013); Yger (2013), for motor imagery signals, the spatial covariance matrix of the trials is a very promising feature. Based on this observation, we extracted the covariance matrix between sensors (i.e., the spatial covariance matrix) from every filtered time segment (i.e., every example to be classified). Although we could directly use our approach and try to classify samples, the performance would be poor since this dataset is known to show some non-stationarity between sessions. In order to cope with this problem, we employ an adaptive kernel formulation proposed in Barachant et al. (2013). This method cancels the non-stationarity in the data by whitening the covariance matrices in the training session and test session by subtracting the Riemannian mean of the training and test datasets, respectively.", "startOffset": 23, "endOffset": 648}, {"referenceID": 28, "context": "Similarly to the works of Mairal et al. (2009) and Yger & Rakotomamonjy (2011), we extracted 16\u00d7 16 patches from every texture.", "startOffset": 26, "endOffset": 47}, {"referenceID": 28, "context": "Similarly to the works of Mairal et al. (2009) and Yger & Rakotomamonjy (2011), we extracted 16\u00d7 16 patches from every texture.", "startOffset": 26, "endOffset": 79}, {"referenceID": 39, "context": "As proposed in Tou et al. (2009), we built covariance matrices using a bank of 8 Gabor filters (4 angles and 2 scales).", "startOffset": 15, "endOffset": 33}, {"referenceID": 34, "context": "Although we restricted ourselves to binary classification problems, the extension of our approach to multi-class problems Ramona et al. (2012) is straightforward.", "startOffset": 122, "endOffset": 143}, {"referenceID": 34, "context": "Although we restricted ourselves to binary classification problems, the extension of our approach to multi-class problems Ramona et al. (2012) is straightforward. When huge datasets are involved, our optimization algorithm might not scale well. A stochastic setting on manifold Bonnabel (2013) could be a promising extention.", "startOffset": 122, "endOffset": 294}, {"referenceID": 21, "context": "Exploration along this line of research would bridge the gap between our approach and the one proposed in Harandi et al. (2014).", "startOffset": 106, "endOffset": 128}], "year": 2015, "abstractText": "Metric learning has been shown to be highly effective to improve the performance of nearest neighbor classification. In this paper, we address the problem of metric learning for symmetric positive definite (SPD) matrices such as covariance matrices, which arise in many real-world applications. Naively using standard Mahalanobis metric learning methods under the Euclidean geometry for SPD matrices is not appropriate, because the difference of SPD matrices can be a non-SPD matrix and thus the obtained solution can be uninterpretable. To cope with this problem, we propose to use a properly parameterized LogEuclidean distance and optimize the metric with respect to kernel-target alignment, which is a supervised criterion for kernel learning. Then the resulting non-trivial optimization problem is solved by utilizing the Riemannian geometry. Finally, we experimentally demonstrate the usefulness of our LogEuclidean metric learning algorithm on real-world classification tasks for EEG signals and texture patches.", "creator": "LaTeX with hyperref package"}}}