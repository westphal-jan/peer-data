{"id": "1502.03529", "review": {"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2015", "title": "Scalable Stochastic Alternating Direction Method of Multipliers", "abstract": "Stochastic alternating direction method of multipliers (ADMM), which visits only one sample or a mini-batch of samples each time, has recently been proved to achieve better performance than batch ADMM. However, most stochastic methods can only achieve a convergence rate $O(1/\\sqrt T)$ on general convex problems,where T is the number of iterations. Hence, these methods are not scalable with respect to convergence rate (computation cost). There exists only one stochastic method, called SA-ADMM, which can achieve convergence rate $O(1/T )$ on general convex problems. However, an extra memory is needed for SA-ADMM to store the historic gradients on all samples, and thus it is not scalable with respect to storage cost. In this paper, we propose a novel method, called scalable stochastic ADMM(SCAS-ADMM), for large-scale optimization and learning problems. Without the need to store the historic gradients, SCAS-ADMM can achieve the same convergence rate $O(1/T )$ as the best stochastic method SA-ADMM and batch ADMM on general convex problems. Experiments on graph-guided fused lasso show that SCAS-ADMM can achieve state-of-the-art performance in real applications, without any complexity.\n\n\n\nThe idea was inspired by the idea of an infinite-time dimension called qubit-coefficient solutions, which provides a time-invariant dimension for all of the values associated with a value, and can be applied to other sub-categories such as the classical distribution of the problem's distribution. The problem can be modeled on other classical techniques, such as the classical process-driven and discrete-time-invariant-equilibrium approach, or as a classical time-invariant-equilibrium approach.\nAn initial solution of the problem can be obtained with a special way of applying a simple model of a given classical distribution of the problem. We consider a second model, the A-T model, to be similar in concept to the A-T model, which provides a general linear solution, given the classical method A-T, that is a finite-time dimension. As shown in the figure, it can be generalized to a type of A-T which can be generalized to a type of B-T where a finite-time dimension is also an infinite-time dimension. However, it is", "histories": [["v1", "Thu, 12 Feb 2015 04:01:46 GMT  (823kb,D)", "https://arxiv.org/abs/1502.03529v1", null], ["v2", "Sun, 1 Mar 2015 13:15:14 GMT  (825kb,D)", "http://arxiv.org/abs/1502.03529v2", null], ["v3", "Mon, 20 Jul 2015 10:01:27 GMT  (829kb,D)", "http://arxiv.org/abs/1502.03529v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shen-yi zhao", "wu-jun li", "zhi-hua zhou"], "accepted": true, "id": "1502.03529"}, "pdf": {"name": "1502.03529.pdf", "metadata": {"source": "CRF", "title": "Scalable Stochastic Alternating Direction Method of Multipliers", "authors": ["Shen-Yi Zhao", "Wu-Jun Li", "Zhi-Hua Zhou"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The alternating direction method of multipliers (ADMM) [1] is proposed to solve the problems which can be formulated as follows:\nmin x,y P (x,y) = f(x) + g(y) (1)\ns.t. Ax + By = c,\nwhere f(\u00b7) and g(\u00b7) are convex functions, A \u2208 Rl\u00d7p and B \u2208 Rl\u00d7q are matrices, c \u2208 Rl is a vector, x \u2208 Rp and y \u2208 Rq are variables to be optimized (learned). By splitting the objective function P (\u00b7) into two parts f(\u00b7) and g(\u00b7), ADMM provides a flexible framework to handle many optimization problems. For example, by taking f(x) to be the square loss or logistic loss on the training set, g(y) to\nar X\niv :1\n50 2.\n03 52\n9v 3\n[ cs\n.L G\n] 2\nbe the L1-norm and the constraint to be x\u2212 y = 0, we can get the well-known lasso formulation [2]. Similarly, we can take more complex constraints than that in lasso to get more complex regularization problems such as the structured sparse regularization problems [3, 4]. Compared with other optimization methods such as gradient decent, ADMM has demonstrated better performance in many complex regularization problems [3, 4]. Furthermore, ADMM can be easily adapted to solve large-scale distributed problems [1]. Hence, ADMM has been widely used in a large variety of areas [1].\nDeterministic (batch) ADMM needs to visit all the samples in each iteration. Existing works have shown that batch ADMM is not efficient enough for big data applications with a large amount of training samples [5, 6]. Stochastic (online) ADMM, which visits only one sample or a mini-batch of samples each time, has recently been proved to achieve better performance than batch ADMM [5, 6, 3, 7, 4]. Hence, stochastic ADMM has become a hot research topic and attracted much attention [7, 4].\nOnline alternating direction method (OADM) [5] is the first online ADMM method. There is only regret analysis in OADM, based on which we can find that if OADM is adapted for stochastic settings with finite samples, the convergence rate of OADM is O(1/ \u221a T ) for general convex problems where f(x) and g(y) are convex but not necessarily to be strongly convex. Here, T is the number of iterations. Besides OADM, several stochastic ADMM methods have been proposed, including stochastic ADMM (STOC-ADMM) [6], regularized dual averaging ADMM (RDA-ADMM) [3], online proximal gradient descent based ADMM (OPG-ADMM) [3], optimal stochastic ADMM (OS-ADMM) [7], and stochastic average ADMM (SA-ADMM) [4]. STOC-ADMM, RDA-ADMM, OPG-ADMM and OS-ADMM achieve a convergence rate of O(1/ \u221a T ) for general convex problems, worse than batch ADMM that has a convergence rate of O(1/T ) [8]. Different from STOC-ADMM, RDA-ADMM, OPG-ADMM and OS-ADMM, SA-ADMM [4] can achieve a convergence rate of O(1/T ) for general convex problems by using historic gradients to approximate the full gradients in each iteration. Thus, SA-ADMM is the only one which is scalable in terms of convergence rate (computation cost). However, SA-ADMM requires an extra memory which is typically very large to store the historic gradients on all samples, making it not scalable in terms of storage cost.\nIn this paper, we propose a novel method, called scalable stochastic ADMM (SCAS-ADMM), for large-scale optimization and learning problems. The main contributions of SCAS-ADMM are outlined as follows:\n\u2022 SCAS-ADMM achieves the same convergence rate of O(1/T ) for general convex problems as the best existing stochastic ADMM method (SAADMM) and batch ADMM. Therefore, SCAS-ADMM is scalable in terms of convergence rate (computation cost).\n\u2022 Different from SA-ADMM, SCAS-ADMM does not need an extra memory to store the historic gradients on all samples. Therefore, SCAS-ADMM is scalable in terms of memory (storage) cost.\n\u2022 Experimental results on graph-guided fused lasso [9] show that SCASADMM can achieve state-of-the-art performance in real applications."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Convex and Smooth Functions", "text": "We use \u2016a\u2016 to denote the Euclidean (L2) norm of a. A function h(\u00b7) is called \u03bd\u2032hLipschitz continuous if: \u2203\u03bd\u2032h > 0, \u2200a,b, \u2016h(b)\u2212 h(a)\u2016 \u2264 \u03bd\u2032h \u2016b\u2212 a\u2016. Assume h(\u00b7) is differentiable, and let \u2207h(a) denote the gradient of h(\u00b7) at a. A function h(\u00b7) is called convex if: \u2200a,b, h(b) \u2265 h(a) + [\u2207h(a)]T (b\u2212 a). Assume h(\u00b7) is convex and differentiable. h(\u00b7) is called \u03bdh-smooth if: \u2203\u03bdh > 0,\u2200a,b, h(b) \u2264 h(a) + [\u2207h(a)]T (b \u2212 a) + \u03bdh2 \u2016b\u2212 a\u2016 2 . This is equivalent to say that \u2207h(\u00b7) is \u03bdh-Lipschitz continuous. Here, \u03bdh is called the Lipschits constant of h(\u00b7). A function h(\u00b7) is called strongly convex if: \u2203\u00b5h > 0, \u2200a,b, h(b) \u2265 h(a) + [\u2207h(a)]T (b\u2212 a) + \u00b5h2 \u2016b\u2212 a\u2016 2 . A function h(\u00b7) is called general convex if h(\u00b7) is convex but not necessarily to be strongly convex."}, {"heading": "2.2 ADMM", "text": "ADMM solves (1) based on the augmented Lagrangian function:\nL(x,y,\u03b2) =f(x) + g(y) + \u03b2T (Ax + By \u2212 c) + \u03c1 2 \u2016Ax + By \u2212 c\u20162 , (2)\nwhere \u03b2 is a vector of Lagrangian multipliers, and \u03c1 > 0 is a penalty parameter. Just like the Gauss-Seidel method, ADMM iteratively updates the variables in an alternating manner as follows [1]:\nxt+1 = arg min x L(x,yt,\u03b2t), (3)\nyt+1 = arg min y L(xt+1,y,\u03b2t), (4)\n\u03b2t+1 = \u03b2t + \u03c1(Axt+1 + Byt+1 \u2212 c), (5)\nwhere xt, yt and \u03b2t denote the values of x, y and \u03b2 at the tth iteration, respectively.\nIn the regularized risk minimization problem which this paper will focus on, the function f(x) usually has the following structure:\nf(x) = 1\nn n\u2211 i=1 fi(x), (6)\nwhere x denotes the model parameter, n is the number of training samples, and each fi(\u00b7) is the empirical loss caused by the ith sample. The function g(y) is usually a regularization term. For example, fi(x) = log(1 + exp\n\u2212biaTi x) in logistic regression (LR), and fi(x) = (bi \u2212 aTi x)2 in least square, where (ai, bi) is the ith training sample with the class label bi. Taking g(y) = \u2016y\u20161 and the\nconstraint y = x, we can get the lasso formulation [2]. Similarly, we can get more complex regularization problems by taking more complex constraints like y = Ax.\nUnless otherwise stated, f(x) of the problem we are trying to solve in this paper is defined in (6). Then (3) becomes:\nxt+1 = arg min x { 1 n n\u2211 i=1 fi(x) + (\u03b2t) T (Ax + Byt \u2212 c) + \u03c1 2 \u2016Ax + Byt \u2212 c\u20162}.\n(7)\nFrom (7), it is easy to see that ADMM needs to visit all the n samples in each iteration. Hence, this version of ADMM is also called batch ADMM or deterministic ADMM. Some works [5, 8] have proved that the above batch ADMM has a convergence rate O(1/T ) for general convex problems where f(x) and g(y) are convex but not necessarily to be strongly convex, where T is the number of iterations.\nDifferent from batch ADMM, stochastic (online) ADMM visits only one sample or a mini-batch of samples in each iteration. Recent works have shown that stochastic ADMM can achieve better performance than batch ADMM to handle large-scale datasets in terms of computation complexity and accuracy [5, 6]. The computation of (4) and (5) for both batch ADMM and stochastic ADMM are the same, which can typically be easily completed. Hence, different stochastic ADMM methods mainly focus on proposing different solutions for (7)."}, {"heading": "3 Scalable Stochastic ADMM", "text": "In this section, we present the details of our SCAS-ADMM, which is scalable in terms of both convergence rate and storage cost. Similar to most existing stochastic ADMM methods which adapt stochastic gradient descent (SGD) or its variants [10, 11, 12, 13, 14] to solve the problem in (7), SCAS-ADMM is also inspired by an existing SGD method called stochastic variance reduced gradient (SVRG) [15]. But different from SVRG, our SCAS-ADMM can be used to model more complex problems with equality constraints.\nIn this paper, we assume that f(\u00b7) and all the {fi(\u00b7)} are vf -smooth. For g(\u00b7), we only assume it to be convex, but not necessarily to be smooth or Lipschitz continuous. This is a reasonable assumption for many machine learning problems, such as the lasso with logistic loss or square loss. The proof of the theorems of this paper can be found from the Appendix in the supplementary materials."}, {"heading": "3.1 General Convex Problems", "text": "In the general convex problems, f(\u00b7) is vf -smooth and general convex but not necessarily to be strongly convex."}, {"heading": "3.1.1 Algorithm", "text": "As in existing stochastic ADMM methods [6, 4], the update rules for y and \u03b2 are still the same as those in (4) and (5). We only need to design a new strategy to update x. The algorithm for our SCAS-ADMM is briefly presented in Algorithm 1. It changes (7) to be:\nxt+1 = \u2211Mt\u22121 m=0 wm Mt , (8)\nwhere Mt is a parameter denoting the number of iterations in the inner loop, and\nw0 = xt,\nwm+1 = \u03c0X (wm \u2212 \u03b7t[\u2207fim(wm)\u2212\u2207fim(w0) + zt + AT\u03b2t + \u03c1AT (Awm + Byt \u2212 c)]), (9)\nwith im being an index randomly sampled from {1, 2, \u00b7 \u00b7 \u00b7 , n}, zt = \u2207f(xt) = 1 n \u2211n i=1\u2207fi(xt) being the full gradient at xt, X being the domain of x, and \u03c0X (\u00b7) denoting the projection operation onto the domain X .\nAlgorithm 1 SCAS-ADMM for general convex problems\nInitialize: (x0,y0,\u03b20), a convex set X for t = 0 to T \u2212 1 do\nCompute zt = \u2207f(xt) = 1n \u2211n i=1\u2207fi(xt); w0 = xt; s = w0; for m = 0 to Mt \u2212 2 do\nRandomly select an im from {1, 2, \u00b7 \u00b7 \u00b7 , n}; wm+1 = \u03c0X (wm\u2212\u03b7t[\u2207fim(wm)\u2212\u2207fim(w0)+zt+AT\u03b2t+\u03c1AT (Awm+Byt\u2212 c)]); s = s + wm+1;\nend for xt+1 =\n1 Mt\ns; yt+1 = arg miny L(xt+1,y,\u03b2t); \u03b2t+1 = \u03b2t + \u03c1(Axt+1 + Byt+1 \u2212 c);\nend for Output: x\u0304T =\n1 T \u2211T t=1 xt, y\u0304T = 1 T \u2211T t=1 yt\nCompared with SVRG [15], the update rule in (9) has an extra vector AT\u03b2t+ \u03c1AT (Awm + Byt \u2212 c) = \u03c1ATAwm + AT (\u03b2t + \u03c1(Byt \u2212 c)). If matrix A = 0, which means By = c, x and y are independent. Then Algorithm 1 will degenerate to SVRG since we only need to solve the minimization problem about f(x) and g(y) separately. We can find that SCAS-ADMM is more general than SVRG since it can solve the minimization problem with more complex equality constraints.\nBesides the memory to store A and B, the memory to store zt, w0, s, and wm+1 is only O(p), where p is the number of parameters, i.e., the length of vector\nx. Furthermore, it only needs some other memory to store {xt|t = 0, 1, \u00b7 \u00b7 \u00b7 , T} and {yt|t = 0, 1, \u00b7 \u00b7 \u00b7 , T}. This memory cost is typically small because T is not too large in practice. For example, T = 15 is enough for SCAS-ADMM to achieve satisfactory accuracy in our experiments which will be presented in Section 4. Furthermore, we can also find that SCAS-ADMM does not need to store the historic gradients for all samples which are used in SA-ADMM. Hence, SCAS-ADMM is scalable in terms of storage cost."}, {"heading": "3.1.2 Convergence Analysis", "text": "We call a set X is bounded by D if it satisfies: sup x,x\u2032\u2208X \u2016x\u2212 x\u2032\u2016 \u2264 D, where D is a constant. Assume we have got (xt,yt,\u03b2t), and we define:\nL(x) =L(x,yt,\u03b2t). (10)\nWe can get the following convergence theorem.\nTheorem 1. Assume the optimal solution of (2) is (x\u2217,y\u2217,\u03b2\u2217), X is bounded by D and contains x\u2217, f(x) and all the functions {fi(x)} are general convex and \u03bdf -smooth, and the function g(y) is convex. We have the following convergence result for Algorithm 1:\nE [f(x\u0304T ) + g(y\u0304T )\u2212 f(x\u2217)\u2212 g(y\u2217) + \u03b3 \u2016Ax\u0304T + By\u0304T \u2212 c\u2016]\n\u2264 1 T T\u22121\u2211 t=0 [ D2 2Mt\u03b7t + \u03b7t(\u03bd 2 LD 2 +G2t ) ] + \u03c1 2T \u2016y0 \u2212 y\u2217\u20162H + 1 \u03c1T (\u2016\u03b20\u20162 + \u03b32),\n(11)\nwhere H = BTB, \u2016x\u20162H = xTHx, \u03b3 > 0 is a constant, \u03bdL is the Lipschitz constant of L(x), and Gt = \u2016\u2207L(xt)\u2016.\nLet t = D2\n2Mt\u03b7t + \u03b7t(\u03bd 2 LD 2 + G2t ). To make f(x\u0304T ) + g(y\u0304T ) converge to f(x\u2217) + g(y\u2217), we need to make sure that \u2211T\u22121 t=0 t is bounded or not too large. By taking \u03b7t = 1\n(\u03bd2LD 2+G2t )(t+1)\n\u03b4 , Mt = (\u03bd 2 LD 2 +G2t )(t+ 1) 2\u03b4, we have:\n\u2022 If \u03b4 > 1, then \u2211\u221e t=0 t is a constant, which means that f(x\u0304T ) + g(y\u0304T )\nconverges to f(x\u2217) + g(y\u2217) with a convergence rate of O( 1 T ).\n\u2022 If \u03b4 = 1, then \u2211T\u22121 t=0 t = O(log T ), which means that f(x\u0304T ) + g(y\u0304T )\nconverges to f(x\u2217) + g(y\u2217) with a convergence rate of O( log T T ).\nHence, by choosing \u03b4 > 1, we can get a convergence rate O( 1T ) for our SCASADMM on general convex problems, which is the same as the best convergence rate achieved by existing stochastic ADMM method (SA-ADMM)."}, {"heading": "3.2 Strongly Convex Problems", "text": "In Algorithm 1, with the increase of t, the iteration number of the inner loop Mt needs to be increased and the step size \u03b7t needs to be decreased. This might cause large computation when T gets large. We can get a better algorithm when f(x) in (1) is strongly convex."}, {"heading": "3.2.1 Algorithm", "text": "When f(x) is strongly convex, our SCAS-ADMM is briefly presented in Algorithm 2. We can find that Algorithm 2 is similar to Algorithm 1, but with constant values for Mt and \u03b7t.\nAlgorithm 2 SCAS-ADMM for strongly convex problems\nInitialize: (x0,y0,\u03b20), r = 2\u03b7 \u2212 \u03b7 1\u2212 \u03bdL\u03b7\n2\n, s = \u03b7 1\u2212 \u03bdL\u03b7\n2 , a convex set X ; for t = 0 to T \u2212 1 do\nCompute zt = \u2207f(xt) = 1n \u2211n i=1\u2207fi(xt); w0 = xt; s = 0; for m = 0 to M \u2212 1 do\nRandomly select an im from {1, 2, \u00b7 \u00b7 \u00b7 , n}; wm+1 = \u03c0X (wm\u2212\u03b7[\u2207fim(wm)\u2212\u2207fim(w0)+zt+AT\u03b2t+\u03c1AT (Awm+Byt\u2212 c)]); w\u0303m+1 =\n1 2\u03b7\n(rwm + swm+1); s = s + w\u0303m+1;\nend for xt+1 =\n1 M s;\nyt+1 = arg miny L(xt+1,y,\u03b2t); \u03b2t+1 = \u03b2t + \u03c1(Axt+1 + Byt+1 \u2212 c);\nend for Output: x\u0304T =\n1 T \u2211T t=1 xt, y\u0304T = 1 T \u2211T t=1 yt"}, {"heading": "3.2.2 Convergence Analysis", "text": "Theorem 2. Assume the optimal solution of (2) is (x\u2217,y\u2217,\u03b2\u2217), all the functions {fi(x)} are general convex and \u03bdf -smooth, f(x) is strongly convex and \u03bdf -smooth, and g(y) is convex. We have the following result:\nE [f(x\u0304T ) + g(y\u0304T )\u2212 f(x\u2217)\u2212 g(y\u2217) + \u03b3 \u2016Ax\u0304T + By\u0304T \u2212 c\u2016]\n\u2264\u00b5f 4T \u2016x0 \u2212 x\u2217\u20162 + \u03c1 2T \u2016y0 \u2212 y\u2217\u20162H + 1 \u03c1T (\u2016\u03b20\u20162 + \u03b32), (12)\nwhere H = BTB, and \u03b3 > 0 is a constant.\nIn this case, we can set M and \u03b7 to be constants. Please note that in the proof of Theorem 2, M and \u03b7 need to satisfy the following conditions: \u03b7 \u2212 \u03bdL\u03b7 2\n2 > 0, (4\u03bd 2 L + \u00b5f\u03bdL 2 )\u03b7 + \u03c1\u03bb1 \u2264 \u00b5L, \u03b1 2M\u03b7 + 2\u03bd2L\u03b7 2\u2212vL\u03b7 \u2264 \u00b5f 4 , where \u03bb1 denotes\nthe maximum eigenvalue of ATA, and \u03b1 = 1 \u2212 \u03c1\u03bb1s2 \u2212 \u00b5fs 4 . Different from\nAlgorithm 1, we do not need the convex set X in Algorithm 2 to be bounded or we do not even need such a set for unconstrained problems."}, {"heading": "3.3 Comparison to Related Methods", "text": "We compare our SCAS-ADMM to other stochastic ADMM methods in terms of three key factors: penalty term linearization, convergence rate on general convex problems and memory cost. The matrix inversion ( 1\u03b7t I+\u03c1A TA)\u22121 can be avoided by linearizing the penalty term \u03c12 \u2016Ax + By \u2212 c\u2016 2\n[4]. Hence, penalty term linearization can be used to decrease computation cost. The comparison results are summarized in Table 1, where SA-IU-ADMM is a variant of SAADMM with penalty term linearization. Please note that A \u2208 Rl\u00d7p, B \u2208 Rl\u00d7q, x \u2208 Rp, y \u2208 Rq, c \u2208 Rl, p is the number of parameters to learn, and n is the number of training samples.\nIt is easy to see that only SCAS-ADMM can achieve the best performance in terms of both convergence rate and memory cost. Other methods either achieve only sub-optimal convergence rate, or need more memory than SCAS-ADMM. In particular, SA-ADMM and SA-IU-ADMM need an extra memory as large as O(np) to store the historic gradients for all samples. Typically, n is very large in big data applications. Furthermore, SCAS-ADMM can also avoid the matrix inversion by linearizing the penalty term. Hence, SCAS-ADMM does be salable in terms of both computation cost and memory cost."}, {"heading": "4 Experiments", "text": "As in [6, 7, 4], we evaluate our method on the generalized lasso model [16] which can be formulated as follows:\nmin x\n1\nn n\u2211 i=1 fi(x) + \u03bb \u2016Ax\u20161 , (13)\nwhere fi(x) is the logistic loss, A is a matrix to specify the desired structured sparsity pattern for x, and \u03bb is the regularization hyper-parameter. We can get\ndifferent models like fused lasso and wavelet smoothing by specifying different A. In this paper, we focus on the graph-guided fused lasso [9] which is also used in [4]. As in [6, 4], we use sparse inverse covariance selection method [17] to get a graph matrix (sparsity pattern) G, based on which we can get A = [G; I]. In general, both G and A are sparse.\nWe can formulate (13) with the ADMM framework:\nmin x,y\nP (x,y) = 1\nn n\u2211 i=1 fi(x) + g(y), (14)\ns.t. Ax\u2212 y = 0,\nwhere g(y) = \u03bb \u2016y\u20161."}, {"heading": "4.1 Baselines and Datasets", "text": "Three representative ADMM methods are adopted as baselines for comparison. They are:\n\u2022 Batch-ADMM [1]: The deterministic (batch) variant of ADMM which uses (7) to directly update x by visiting all training samples in each iteration.\n\u2022 STOC-ADMM [6]: The stochastic ADMM variant without using historic gradient for optimization, which has a convergence rate of O(1/ \u221a T ) for\ngeneral convex problems and O(log T/T ) for strongly convex problems.\n\u2022 SA-ADMM [4]: The stochastic ADMM variant by using historic gradient to approximate the full gradient, which has a convergence rate of O(1/T ) for general convex problems.\nPlease note that other methods, such as OPG-ADMM, RDA-ADMM and OSADMM, are not adopted for comparison because they have similar convergence rate as STOC-ADMM. Furthermore, both theoretical and empirical results have shown that SA-ADMM can outperform other methods like RDA-ADMM and OPG-ADMM [4]. The variant of SA-ADMM, SA-IU-ADMM, is also not adopted for comparison because it has similar performance as SA-ADMM [4].\nAlthough the Mt in Algorithm 1 should be increased as t increases, we simply set Mt = n in our experiments because SCAS-ADMM can also achieve good performance with this fixed value for Mt. Similarly, we set M = n in Algorithm 2.\nAs in [4], four widely used datasets are adopted to evaluate our method and other baselines. They are a9a, covertype, rcv1 and sido. All of them are for binary classification tasks. The detailed information about these datasets can be found in Table 2.\nAs in [4], for each dataset we randomly choose half of the samples for training and use the rest for testing. This random partition is repeated for 10 times and the average values are reported. The hyper-parameter \u03bb in (14) is set by using the same values in [4], which are also listed in Table 2. We adopt the same strategy as that in [4] to set the hyper-parameters \u03c1 in (2) and the\nstepsize. More specifically, we randomly choose a small subset of 500 samples from the training set, and then choose the hyper-parameters which can achieve the smallest objective value after running 5 data passes for stochastic methods or 100 data passes (iterations) for batch methods. As in [4], we use y(x\u0304T ) = Ax\u0304T to replace y\u0304T since the methods cannot necessarily guarantee that Ax\u0304T = y\u0304T .\nAll the experiments are conducted on a workstation with 12 Intel Xeon CPU cores and 64G RAM."}, {"heading": "4.2 Convergence Results", "text": "As in [4], we study the variation of the objective value on training set and the testing loss versus the number of effective passes over the data. For all methods, one effective pass over the data means n samples are visited. More specifically, one effective pass refers to one iteration in batch ADMM. For stochastic ADMM methods which visit one sample in each iteration, one effective pass refers to n iterations. For SCAS-ADMM, we set Mt = n and each iteration of the outer loop needs to visit 2n training samples. Hence, each iteration of the outer loop will contribute two effective passes. Although different methods will visit different numbers of samples in each iteration, we can see that the number of effective passes over the data is a good metric for fair comparison because it measures the computation costs of different methods in a unified way.\nFigure 1 shows the results for general convex problems with fi(x) being the logistic loss. Please note that the number of recorded points on the curve of SCAS-ADMM is half of those for other methods because each iteration of the outer loop of SCAS-ADMM will contribute two effective passes. As stated above, it is still fair to compare different methods with respect to the number of effective passes. In Figure 1, all the points with the same x-axis value from different curves have the same number of effective passes. Hence, for two points with the same x-axis value from any two different curves, the point with smaller y-axis value is better than the other one. We can find that all the stochastic methods outperform the Batch-ADMM in terms of both training speed and testing accuracy. SCAS-ADMM and SA-ADMM outperform STOC-ADMM, which is consistent with the theoretical analysis about convergence rate. Our SCASADMM can achieve comparable performance as SA-ADMM, which empirically verifies our theoretical result that SCAS-ADMM has the same convergence rate of O(1/T ) as SA-ADMM.\nBy adding a small L2 regularization term to the logistic loss, we can get strongly convex problems. Figure 2 shows the results for strongly convex problems. Once again, we can observe similar phenomenon as that in Figure 1. In particular, our SCAS-ADMM can achieve comparable convergence rate as SA-ADMM.\nAs for the memory (storage) cost, it is obvious that SCAS-ADMM needs much less memory than SA-ADMM from the theoretical analysis in Table 1. Hence, we do not empirically compare between them."}, {"heading": "5 Conclusion", "text": "In this paper, we have proposed a new stochastic ADMM method called SCASADMM, which can achieve the same convergence rate as the best existing stochastic ADMM method SA-ADMM on general convex problems. Furthermore, it costs much less memory than SA-ADMM. Hence, SCAS-ADMM is scalable in terms of both convergence rate and storage cost."}, {"heading": "A Notations for Proof", "text": "We let\nvm,t = \u2207fim(wm)\u2212\u2207fim(w0) + zt, (15) bm,t = \u03b2t + \u03c1(Awm + Byt \u2212 c), (16) pm,t = vm,t + A Tbm,t. (17)\nThen the update rule for wm+1 in the inner loop of Algorithm 1 can be rewritten as\nwm+1 = \u03c0X (wm \u2212 \u03b7t(vm,t + ATbm,t)) = \u03c0X (wm \u2212 \u03b7tpm,t). (18)\nAssume we have got (xt,yt,\u03b2t), and we define:\nL(x) =L(x,yt,\u03b2t), (19)\nLi(x) =fi(x) + g(yt) + \u03b2Tt (Ax + Byt \u2212 c) + \u03c1\n2 \u2016Ax + Byt \u2212 c\u20162 . (20)"}, {"heading": "B Lemmas for the Proof of Theorem 1", "text": "Lemma 1. If f(x) is \u03bdf -smooth, then \u2203\u03bdL > 0 that makes L(x) be \u03bdL-smooth.\nProof. According to the definition about \u03bdf -smooth, \u2200a,b, we have \u2016\u2207L(b)\u2212\u2207L(a)\u2016 = \u2225\u2225\u2207f(b)\u2212\u2207f(a) + \u03c1ATA(b\u2212 a)\u2225\u2225\n\u2264 \u2016\u2207f(b)\u2212\u2207f(a)\u2016+ \u2225\u2225\u03c1ATA(b\u2212 a)\u2225\u2225\n\u2264 \u03bdf \u2016b\u2212 a\u2016+ \u2225\u2225\u03c1ATA(b\u2212 a)\u2225\u2225\n\u2264 \u03bdf \u2016b\u2212 a\u2016+ \u03c1 \u221a \u03bbA \u2016b\u2212 a\u2016\n= (\u03bdf + \u03c1 \u221a \u03bbA) \u2016b\u2212 a\u2016 ,\nwhere \u03bbA \u2265 0 is the largest eigenvalue of ATAATA. Hence, for any value of \u03bdL \u2265 (\u03bdf + \u03c1 \u221a \u03bbA), we can see that L(x) is \u03bdLsmooth.\nWe can find that \u03bdL is only determined by f(x), matrix A and the penalty parameter \u03c1, but has nothing to do with g(yt), yt, B and \u03b2t.\nThen, we have the following lemma about the variance of pm,t.\nLemma 2. The variance of pm,t satisfies:\nE(\u2016pm,t\u20162) \u2264 2\u03bd2LD2 + 2G2t , (21)\nwhere D is the bound of the domain of x, \u03bdL is the Lipschitz constant of the function L(x) defined in (19), and Gt = \u2016\u2207L(xt)\u2016.\nProof. According to (17), we have\npm,t =vm,t + A Tbm,t\n=\u2207fim(wm) + ATbm,t \u2212 fim(w0)\u2212ATb0,t + zt + ATb0,t =\u2207Lim(wm)\u2212\u2207Lim(w0) +\u2207L(w0).\nThen we have:\nE(\u2016pm,t\u20162) = 1\nn n\u2211 i=1 \u2016\u2207Li(wm)\u2212\u2207Li(w0) +\u2207L(w0)\u20162\n\u2264 2 n n\u2211 i=1 {\u2016\u2207Li(wm)\u2212\u2207Li(w0)\u20162 + \u2016\u2207L(w0)\u20162}\n\u2264{ 2 n n\u2211 i=1 \u2016\u2207Li(wm)\u2212\u2207Li(w0)\u20162}+ 2 \u2016\u2207L(w0)\u20162 \u22642\u03bd2LD2 + 2G2t .\nPlease note that w0 = xt, and we use the Lipschitz definition to get the result:\n\u2016\u2207Li(wm)\u2212\u2207Li(w0)\u20162 \u2264 \u03bd2L \u2016wm \u2212w0\u2016 2 \u2264 \u03bd2LD2.\nLemma 3. For the estimation of xt+1, we have the following result:\nE [ f(xt+1)\u2212 f(x) + (AT\u03b1t+1)T (xt+1 \u2212 x) ] \u2264 D 2\n2Mt\u03b7t + \u03b7t(\u03bd\n2 LD 2 +G2t ), (22)\nwhere \u03b1t+1 = \u03b2t + \u03c1(Axt+1 + Byt \u2212 c).\nProof. Since X is convex, we have: \u2200x \u2208 X ,\n\u2016wm+1 \u2212 x\u20162 \u2264 \u2016wm \u2212 \u03b7tpm,t \u2212 x\u20162 (23)\n= \u2016wm \u2212 x\u20162 \u2212 2\u03b7tpTm,t(wm \u2212 x) + \u03b72t \u2016pm,t\u2016 2 .\nFurthermore, it is easy to prove that E [vm,t] = \u2207f(wm). Based on the results in Lemma 2, we can get the expectation on (23):\nE [ \u2016wm+1 \u2212 x\u20162 ] (24)\n\u2264E [ \u2016wm \u2212 x\u20162 ] \u2212 2\u03b7tE[(\u2207f(wm) + ATbm,t)T (wm \u2212 x)] + \u03b72tE(\u2016pm,t\u2016 2 )\n\u2264E [ \u2016wm \u2212 x\u20162 ] + \u03b72t (2\u03bd 2 LD 2 + 2G2t )\u2212 2\u03b7tE[f(wm)\u2212 f(x) + (ATbm,t)T (wm \u2212 x)].\nSumming up (24) from m = 0 to Mt \u2212 1, we can get:\n2\u03b7t Mt\u22121\u2211 m=0 E[f(wm)\u2212 f(x) + (ATbm,t)T (wm \u2212 x)]\n\u2264\u2016w0 \u2212 x\u20162 +Mt\u03b72t (2\u03bd2LD2 + 2G2t )\u2212 E [ \u2016wMt \u2212 x\u2016 2 ]\n\u2264\u2016w0 \u2212 x\u20162 +Mt\u03b72t (2\u03bd2LD2 + 2G2t ) \u2264D2 +Mt\u03b72t (2\u03bd2LD2 + 2G2t ).\nWe can prove that f(wm) \u2212 f(x) + (ATbm,t)T (wm \u2212 x) is convex in wm. Furthermore, we have xt+1 =\n1 Mt \u2211Mt\u22121 m=0 wm. By using the Jensen\u2019s inequality,\nwe have:\n2\u03b7tMtE [ f(xt+1)\u2212 f(x) + (AT\u03b1t+1)T (xt+1 \u2212 x) ] \u22642\u03b7t\nMt\u22121\u2211 m=0 E[f(wm)\u2212 f(x) + (ATbm,t)T (wm \u2212 x)]\n\u2264D2 +Mt\u03b72t (2\u03bd2LD2 + 2G2t ),\nwhere \u03b1t+1 = \u03b2t + \u03c1(Axt+1 + Byt \u2212 c). Then, we can get:\nE [ f(xt+1)\u2212 f(x) + (AT\u03b1t+1)T (xt+1 \u2212 x) ] \u2264 D 2\n2Mt\u03b7t + \u03b7t(\u03bd\n2 LD 2 +G2t ). (25)\nAccording to the results in [4], we have the following Lemma 4 and Lemma 5 about the estimation of yt+1 and \u03b1t+1.\nLemma 4. For the estimation of yt+1, we have: E [ g(yt+1)\u2212 g(y) + (BT\u03b1t+1)T (yt+1 \u2212 y) ] (26)\n\u2264 \u03c1 2 E [ \u2016yt \u2212 y\u20162H \u2212 \u2016yt+1 \u2212 y\u2016 2 H \u2212 \u2016yt \u2212 yt+1\u2016 2 H ] ,\nwhere \u03b1t+1 = \u03b2t + \u03c1(Axt+1 + Byt \u2212 c), H = BTB, and \u2016y\u20162H = yTHy.\nLemma 5. For the estimation of \u03b1t+1, we have: E [ \u2212(Axt+1 + Byt+1 \u2212 c)T (\u03b1t+1 \u2212\u03b1) ] (27)\n\u2264 1 2\u03c1\nE [ \u2016\u03b2t \u2212\u03b1\u20162 \u2212 \u2016\u03b2t+1 \u2212\u03b1\u20162 ] + \u03c1 2 E [ \u2016yt \u2212 yt+1\u20162H ] ,\nwhere \u03b1t+1 = \u03b2t + \u03c1(Axt+1 + Byt \u2212 c), H = BTB, and \u2016y\u20162H = yTHy.\nThe proof of Lemma 4 and Lemma 5 can be directly derived from the results in [4], which is omitted here for space saving."}, {"heading": "C Proof of Theorem 1", "text": "Proof. Let u =  xy \u03b1 , ut =  xtyt\n\u03b1t\n, u\u0304T = 1T \u2211Tt=1 ut, and F (u) =  AT\u03b1BT\u03b1 \u2212(Ax + By \u2212 c) . Summing up the equations in (22), (26) and (27), we have:\nE [ P (xt+1,yt+1)\u2212 P (x,y) + F (ut+1)T (ut+1 \u2212 u) ] (28)\n\u2264 D 2\n2Mt\u03b7t + \u03b7t(\u03bd\n2 LD 2 +G2t )\n+ \u03c1 2 E [ \u2016yt \u2212 y\u20162H \u2212 \u2016yt+1 \u2212 y\u2016 2 H \u2212 \u2016yt \u2212 yt+1\u2016 2 H ] + 1 2\u03c1 E [ \u2016\u03b2t \u2212\u03b1\u20162 \u2212 \u2016\u03b2t+1 \u2212\u03b1\u20162 ] + \u03c1 2 E [ \u2016yt \u2212 yt+1\u20162H ] .\nIt is easy to prove that P (xt,yt)\u2212P (x,y)+F (ut)T (ut\u2212u) is convex in (xt,yt). Moreover, we have x\u0304T = 1 T \u2211T t=1 xt and y\u0304T = 1 T \u2211T t=1 yt. By using the Jensen\u2019s inequality, we have:\nP (x\u0304T , y\u0304T )\u2212 P (x,y) + F (u\u0304T )T (u\u0304T \u2212 u) (29)\n\u2264 1 T T\u2211 t=1 [ P (xt,yt)\u2212 P (x,y) + F (ut)T (ut \u2212 u) ] .\nSumming up (28) from t = 0 to T \u2212 1, and using the result in (29), we have:\nE [ P (x\u0304T , y\u0304T )\u2212 P (x,y) + F (u\u0304T )T (u\u0304T \u2212 u) ] \u2264 1 T T\u22121\u2211 t=0 E [ P (xt+1,yt+1)\u2212 P (x,y) + F (ut+1)T (ut+1 \u2212 u)\n] \u2264 1 T T\u22121\u2211 t=0 [ D2 2Mt\u03b7t + \u03b7t(\u03bd 2 LD 2 +G2t )\n] + \u03c1\n2T \u2016y0 \u2212 y\u20162H +\n1\n2\u03c1T \u2016\u03b20 \u2212\u03b1\u20162 . (30)\nThe result in (30) is satisfied for any (x,y,\u03b1). In particular, if we take x = x\u2217, y = y\u2217 and \u03b1 = \u03b3\nAx\u0304T+By\u0304T\u2212c \u2016Ax\u0304T+By\u0304T\u2212c\u2016 , we have:\nE [P (x\u0304T , y\u0304T )\u2212 P (x\u2217,y\u2217) + \u03b3 \u2016Ax\u0304T + By\u0304T \u2212 c\u2016] (31)\n\u2264 1 T T\u22121\u2211 t=0 [ D2 2Mt\u03b7t + \u03b7t(\u03bd 2 LD 2 +G2t ) ] + \u03c1\n2T \u2016y0 \u2212 y\u2217\u20162H +\n1\n\u03c1T (\u2016\u03b20\u20162 + \u03b32)."}, {"heading": "D Lemmas for the Proof of Theorem 2", "text": "Lemma 6. The variance of pm,t satisfies:\n\u2200x, E(\u2016pm,t\u20162) \u2264 dm + \u2016\u2207L(wm)\u20162\n\u2264 2\u03bd2L \u2016wm \u2212 x\u2016 2 + 2\u03bd2L \u2016w0 \u2212 x\u2016 2 + \u2016\u2207L(wm)\u20162 . (32)\nwhere dm , 1n \u2211n i=1 \u2016\u2207Li(wm)\u2212\u2207Li(w0)\u2016 2 .\nProof. \u2200x\nE(\u2016pm,t\u20162) = 1\nn n\u2211 i=1 \u2016\u2207Li(wm)\u2212\u2207Li(w0) +\u2207L(w0)\u20162\n= \u2016\u2207L(wm)\u20162 \u2212 \u2016\u2207L(wm)\u2212\u2207L(w0)\u20162 + 1\nn n\u2211 i=1 \u2016\u2207Li(wm)\u2212\u2207Li(w0)\u20162\n\u2264\u2016\u2207L(wm)\u20162 + 1\nn n\u2211 i=1 \u2016\u2207Li(wm)\u2212\u2207Li(w0)\u20162\n\u2264\u03bd2L \u2016wm \u2212w0\u2016 2 + \u2016\u2207L(wm)\u20162 \u22642\u03bd2L \u2016wm \u2212 x\u2016 2 + 2\u03bd2L \u2016w0 \u2212 x\u2016 2 + \u2016\u2207L(wm)\u20162 .\nLemma 7. If \u03b7 \u2212 \u03bdL\u03b7 2\n2 > 0, we have the following result for the variance of \u2207L(wm):\n\u2016\u2207L(wm)\u20162 \u2264 1\n\u03b7 \u2212 \u03bdL\u03b722 (L(wm)\u2212 E[L(wm+1)]) +\n\u03bdL\u03b7\n2\u2212 \u03bdL\u03b7 dm. (33)\nProof. Since L(w) is convex in w, we can get\nL(wm+1) \u2264 L(wm) +\u2207L(wm)T (wm+1 \u2212wm) + \u03bdL 2 \u2016wm+1 \u2212wm\u20162 .\nTaking expectation on both sides of the above equation, we get\nE[L(wm+1)] \u2264 L(wm)\u2212 \u03b7 \u2016\u2207L(wm)\u20162 + \u03bdL\u03b7\n2\n2 E(\u2016pm,t\u20162).\nAccording to Lemma 6, we have\nE[L(wm+1)] \u2264 L(wm)\u2212 \u03b7 \u2016\u2207L(wm)\u20162 + \u03bdL\u03b7\n2\n2 (dm + \u2016\u2207L(wm)\u20162).\nThen we have\n(\u03b7 \u2212 \u03bdL\u03b7 2\n2 ) \u2016\u2207L(wm)\u20162 \u2264 L(wm)\u2212 E[L(wm+1)] +\n\u03bdL\u03b7 2\n2 dm. (34)\nChoosing a small \u03b7 such that \u03b7 \u2212 \u03bdL\u03b7 2\n2 > 0, we have\n\u2016\u2207L(wm)\u20162 \u2264 1\n\u03b7 \u2212 \u03bdL\u03b722 (L(wm)\u2212 E[L(wm+1)]) +\n\u03bdL\u03b7\n2\u2212 \u03bdL\u03b7 dm.\nLemma 8. We have the following result:\nE(\u2016wm+1 \u2212 x\u20162) + 2\u03b7\u2207L(wm)T (wm \u2212 x) + \u03b7\n1\u2212 \u03bdL\u03b72 (E[L(wm+1)]\u2212 L(x))\n\u2264\u2016wm \u2212 x\u20162 + \u03b7\n1\u2212 \u03bdL\u03b72 (L(wm)\u2212 L(x)) +\n2\u03b72\n2\u2212 \u03bdL\u03b7 dm.\nProof. From (18), we can get \u2200x,\n\u2016wm+1 \u2212 x\u20162 \u2264 \u2016wm \u2212 x\u20162 \u2212 2\u03b7pTm,t(wm \u2212 x) + \u03b72 \u2016pm,t\u2016 2 .\nThen, we have\nE(\u2016wm+1 \u2212 x\u20162) \u2264 \u2016wm \u2212 x\u20162 \u2212 2\u03b7\u2207L(wm)T (wm \u2212 x) + \u03b72E(\u2016pm,t\u20162).\nAccording to Lemma 6 and Lemma 7, we have\nE(\u2016wm+1 \u2212 x\u20162) + 2\u03b7\u2207L(wm)T (wm \u2212 x) \u2264 \u2016wm \u2212 x\u20162 + \u03b72dm + \u03b72 \u2016\u2207L(wm)\u20162\n\u2264 \u2016wm \u2212 x\u20162 + \u03b72dm + \u03b7\n1\u2212 \u03bdL\u03b72 (L(wm)\u2212 E[L(wm+1)]) +\n\u03bdL\u03b7 3\n2\u2212 \u03bdL\u03b7 dm.\nThen, we can get\nE(\u2016wm+1 \u2212 x\u20162) + 2\u03b7\u2207L(wm)T (wm \u2212 x) + \u03b7\n1\u2212 \u03bdL\u03b72 (E[L(wm+1)]\u2212 L(x))\n\u2264\u2016wm \u2212 x\u20162 + \u03b7\n1\u2212 \u03bdL\u03b72 (L(wm)\u2212 L(x)) +\n2\u03b72\n2\u2212 \u03bdL\u03b7 dm.\nLemma 9. Let \u03bb1 denote the maximum eigenvalue of A TA, \u2200w,x,y,\u03b2,\nL(w)\u2212 L(x) \u2265 f(w)\u2212 f(x) + (ATb)T (w \u2212 x)\u2212 \u03c1\u03bb1 2 \u2016w \u2212 x\u20162 ,\nwhere b = \u03b2 + \u03c1(Aw + By \u2212 c).\nProof. According to the definition L(w) = f(w) + g(y) + \u03b2T (Aw + By\u2212 c) + \u03c1 2 \u2016Aw + By \u2212 c\u2016 2 , we have\nL(w)\u2212 L(x)\n=f(w)\u2212 f(x) + \u03b2TA(w \u2212 x) + \u03c1 2 \u2016Aw + By \u2212 c\u20162 \u2212 \u03c1 2 \u2016Ax + By \u2212 c\u20162 =f(w)\u2212 f(x) + \u03b2TA(w \u2212 x) + \u03c1 2 [ \u2016Aw\u20162 \u2212 \u2016Ax\u20162 + 2(By \u2212 c)TA(w \u2212 x) ] =f(w)\u2212 f(x) + (AT (\u03b2 + \u03c1(By \u2212 c))T (w \u2212 x) + \u03c1\n2 (\u2016Aw\u20162 \u2212 \u2016Ax\u20162)\n=f(w)\u2212 f(x) + (AT (\u03b2 + \u03c1(Aw + By \u2212 c))T (w \u2212 x)\u2212 \u03c1 2 \u2016Aw \u2212Ax\u20162 =f(w)\u2212 f(x) + (ATb)T (w \u2212 x)\u2212 \u03c1 2 \u2016Aw \u2212Ax\u20162 \u2265f(w)\u2212 f(x) + (ATb)T (w \u2212 x)\u2212 \u03c1\u03bb1 2 \u2016w \u2212 x\u20162 .\nLemma 10. If f(w) is strongly convex and \u00b5f > \u03c1\u03bb1, we have the following result\nE [ f(xt+1)\u2212 f(x) + (AT\u03b1t+1)T (xt+1 \u2212 x) ] \u2264 \u00b5f\n4 (\u2016xt \u2212 x\u20162 \u2212 E \u2016xt+1 \u2212 x\u20162),\n(35)\nwhere \u03b1t+1 is the same as that in Lemma 3.\nProof. Note that\nr , 2\u03b7 \u2212 \u03b7 1\u2212 \u03bdL\u03b72 , s , \u03b7\n1\u2212 \u03bdL\u03b72 .\nSince f(w) is strongly convex in w, we can prove that L(w) is also strongly convex in w. Then we have\n\u2207L(wm)T (wm \u2212 x) \u2265 L(wm)\u2212 L(x) + \u00b5L 2 \u2016wm \u2212 x\u20162 . (36)\nBy combining the results in Lemma 8 and (36), we have\nE \u2016wm+1 \u2212 x\u20162 + \u00b5Ls\n2 \u2016wm \u2212 x\u20162 + r\u2207L(wm)T (wm \u2212 x) + s(E[L(wm+1)]\u2212 L(x))\n\u2264\u2016wm \u2212 x\u20162 + 2\u03b72\n2\u2212 vL\u03b7 dm.\nFor convenience, we use\nDm , f(wm)\u2212 f(x) + (ATbm,t)T (wm \u2212 x),\nwhere the definition of bm,t is in (16). And we also have\n\u2207L(wm) = \u2207f(wm) + ATbm,t, \u2207f(wm)T (wm \u2212 x) \u2265 f(wm)\u2212 f(x) + \u00b5f 2 \u2016wm \u2212 x\u20162 .\nThen according to Lemma 6 and Lemma 9, we can get\n(1\u2212 \u03c1s\u03bb1 2 \u2212 \u00b5fs 4 )E(\u2016wm+1 \u2212 x\u20162) + \u00b5Ls 2 \u2016wm \u2212 x\u20162\n+ r(Dm + \u00b5f 2 \u2016wm \u2212 x\u20162) + sE(Dm+1 + \u00b5f 4 \u2016wm+1 \u2212 x\u20162)\n\u2264(1 + 4\u03bd 2 L\u03b7 2\n2\u2212 \u03bdL\u03b7 ) \u2016wm \u2212 x\u20162 +\n4\u03bd2L\u03b7 2\n2\u2212 \u03bdL\u03b7 \u2016w0 \u2212 x\u20162 ,\ni.e.,\n(1\u2212 \u03c1s\u03bb1 2 \u2212 \u00b5fs 4 )E(\u2016wm+1 \u2212 x\u20162)\n+ r(Dm + \u00b5f 4 \u2016wm \u2212 x\u20162) + sE(Dm+1 + \u00b5f 4 \u2016wm+1 \u2212 x\u20162)\n\u2264(1 + 4\u03bd 2 L\u03b7 2 2\u2212 \u03bdL\u03b7 \u2212 \u00b5fr 4 \u2212 \u00b5Ls 2 ) \u2016wm \u2212 x\u20162 + 4\u03bd2L\u03b7 2 2\u2212 \u03bdL\u03b7 \u2016w0 \u2212 x\u20162 .\nHere, \u03b7 need to satisfy the following condition:\n1 + 4\u03bd2L\u03b7 2 2\u2212 \u03bdL\u03b7 \u2212 \u00b5fr 4 \u2212 \u00b5Ls 2 \u2264 1\u2212 \u03c1s\u03bb1 2 \u2212 \u00b5fs 4 ,\ni.e.,\n(4\u03bd2L + \u00b5f\u03bdL\n2 )\u03b7 + \u03c1\u03bb1 \u2264 \u00b5L.\nLet \u03b1 = 1\u2212 \u03c1\u03bb1s2 \u2212 \u00b5fs 4 . We have\n\u03b1E \u2016wm+1 \u2212 x\u20162 + rE(Dm + \u00b5f 4 \u2016wm \u2212 x\u20162) + sE(Dm+1 + \u00b5f 4 \u2016wm+1 \u2212 x\u20162)\n\u2264\u03b1E \u2016wm \u2212 x\u20162 + 4\u03bd2L\u03b7 2\n2\u2212 \u03bdL\u03b7 \u2016w0 \u2212 x\u20162 .\nNote that r + s = 2\u03b7, and Dm is convex in wm. We take\nw\u0303m+1 = 1\n2\u03b7 (rwm + swm+1),\nwhich is a convex combination of wm and wm+1. Then we have\n\u03b1E \u2016wm+1 \u2212 x\u20162 + 2\u03b7E(D\u0303m+1 + \u00b5f 4 \u2016w\u0303m+1 \u2212 x\u20162) \u2264 \u03b1E \u2016wm \u2212 x\u20162 +\n4\u03bd2L\u03b7 2\n2\u2212 \u03bdL\u03b7 \u2016w0 \u2212 x\u20162 .\n(37)\nwhere D\u0303m+1 = f(w\u0303m+1) \u2212 f(x) + (AT b\u0303m+1,t)T (w\u0303m+1 \u2212 x), and b\u0303m+1,t = \u03b2 + \u03c1(Aw\u0303m+1 + By \u2212 c).\nSumming up (37) from m = 0 to M\u22121, and taking xt+1 = 1M \u2211M\u22121 m=0 w\u0303m+1,\nwe have\n2M\u03b7E(f(xt+1)\u2212 f(x) + (AT\u03b1t+1)T (xt+1 \u2212 x) + \u00b5f 4 \u2016xt+1 \u2212 x\u20162)\n\u2264(\u03b1+ 4M\u03bd 2 L\u03b7 2\n2\u2212 \u03bdL\u03b7 ) \u2016w0 \u2212 x\u20162 ,\nwhere \u03b1t+1 = \u03b2 + \u03c1(Axt+1 + By \u2212 c). Then, we have\nE [ f(xt+1)\u2212 f(x) + (AT\u03b1t+1)T (xt+1 \u2212 x) ] \u2264( \u03b1\n2M\u03b7 +\n2\u03bd2L\u03b7\n2\u2212 \u03bdL\u03b7 ) \u2016xt \u2212 x\u20162 \u2212 \u00b5f 4 E(\u2016xt+1 \u2212 x\u20162)\n\u2264\u00b5f 4 (\u2016xt \u2212 x\u20162 \u2212 E(\u2016xt+1 \u2212 x\u20162)),\nwhere we assume that \u03b12M\u03b7 + 2\u03bd2L\u03b7 2\u2212vL\u03b7 \u2264 \u00b5f 4 ."}, {"heading": "E Proof of Theorem 2", "text": "Proof. Let u =  xy \u03b1 , ut =  xtyt\n\u03b1t\n, u\u0304T = 1T \u2211Tt=1 ut, and F (u) =  AT\u03b1BT\u03b1 \u2212(Ax + By \u2212 c) . Summing up the equations in (35), (26) and (27), we have:\nE [ P (xt+1,yt+1)\u2212 P (x,y) + F (ut+1)T (ut+1 \u2212 u) ] \u2264\u00b5f\n4 (\u2016xt \u2212 x\u20162 \u2212 E \u2016xt+1 \u2212 x\u20162)\n+ \u03c1 2 E [ \u2016yt \u2212 y\u20162H \u2212 \u2016yt+1 \u2212 y\u2016 2 H \u2212 \u2016yt \u2212 yt+1\u2016 2 H ] + 1 2\u03c1 E [ \u2016\u03b2t \u2212\u03b1\u20162 \u2212 \u2016\u03b2t+1 \u2212\u03b1\u20162 ] + \u03c1 2 E [ \u2016yt \u2212 yt+1\u20162H ] .\nIt is easy to prove that P (xt,yt)\u2212P (x,y)+F (ut)T (ut\u2212u) is convex in (xt,yt). Moreover, we have x\u0304T = 1 T \u2211T t=1 xt and y\u0304T = 1 T \u2211T t=1 yt. By using the Jensen\u2019s inequality, we have:\nP (x\u0304T , y\u0304T )\u2212 P (x,y) + F (u\u0304T )T (u\u0304T \u2212 u)\n\u2264 1 T T\u2211 t=1 [ P (xt,yt)\u2212 P (x,y) + F (ut)T (ut \u2212 u) ] .\nThen, we have: E [ P (x\u0304T , y\u0304T )\u2212 P (x,y) + F (u\u0304T )T (u\u0304T \u2212 u) ] \u2264 1 T T\u22121\u2211 t=0 E [ P (xt+1,yt+1)\u2212 P (x,y) + F (ut+1)T (ut+1 \u2212 u)\n] \u2264\u00b5f\n4T \u2016x0 \u2212 x\u20162 +\n\u03c1\n2T \u2016y0 \u2212 y\u20162H +\n1\n2\u03c1T \u2016\u03b20 \u2212\u03b1\u20162 . (38)\nThe result in (38) is satisfied for any (x,y,\u03b1). In particular, if we take x = x\u2217, y = y\u2217 and \u03b1 = \u03b3\nAx\u0304T+By\u0304T\u2212c \u2016Ax\u0304T+By\u0304T\u2212c\u2016 , we have:\nE [P (x\u0304T , y\u0304T )\u2212 P (x\u2217,y\u2217) + \u03b3 \u2016Ax\u0304T + By\u0304T \u2212 c\u2016]\n\u2264\u00b5f 4 \u2016x0 \u2212 x\u2217\u20162 + \u03c1 2T \u2016y0 \u2212 y\u2217\u20162H + 1 \u03c1T (\u2016\u03b20\u20162 + \u03b32)."}], "references": [{"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["Stephen P. Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "Dual averaging and proximal gradient descent for online alternating direction multiplier method", "author": ["Taiji Suzuki"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Fast stochastic alternating direction method of multipliers", "author": ["Wenliang Zhong", "James Tin-Yau Kwok"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Online alternating direction method", "author": ["Huahua Wang", "Arindam Banerjee"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Stochastic alternating direction method of multipliers", "author": ["Hua Ouyang", "Niao He", "Long Tran", "Alexander G. Gray"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Towards an optimal stochastic alternating direction method of multipliers", "author": ["Samaneh Azadi", "Suvrit Sra"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "On the o(1/n) convergence rate of the douglas-rachford alternating direction method", "author": ["Bingsheng He", "Xiaoming Yuan"], "venue": "SIAM J. Numerical Analysis,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "A multivariate regression approach to association analysis of a quantitative trait", "author": ["Seyoung Kim", "Kyung-Ah Sohn", "Eric P. Xing"], "venue": "network. Bioinformatics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Dual averaging method for regularized stochastic learning and online optimization", "author": ["Lin Xiao"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["John C. Duchi", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["Nicolas Le Roux", "Mark W. Schmidt", "Francis Bach"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Optimization with first-order surrogate functions", "author": ["Julien Mairal"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "The solution path of the generalized lasso", "author": ["Ryan J. Tibshirani", "Jonathan Taylor"], "venue": "Annals of Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "The alternating direction method of multipliers (ADMM) [1] is proposed to solve the problems which can be formulated as follows:", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "be the L1-norm and the constraint to be x\u2212 y = 0, we can get the well-known lasso formulation [2].", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "Similarly, we can take more complex constraints than that in lasso to get more complex regularization problems such as the structured sparse regularization problems [3, 4].", "startOffset": 165, "endOffset": 171}, {"referenceID": 3, "context": "Similarly, we can take more complex constraints than that in lasso to get more complex regularization problems such as the structured sparse regularization problems [3, 4].", "startOffset": 165, "endOffset": 171}, {"referenceID": 2, "context": "Compared with other optimization methods such as gradient decent, ADMM has demonstrated better performance in many complex regularization problems [3, 4].", "startOffset": 147, "endOffset": 153}, {"referenceID": 3, "context": "Compared with other optimization methods such as gradient decent, ADMM has demonstrated better performance in many complex regularization problems [3, 4].", "startOffset": 147, "endOffset": 153}, {"referenceID": 0, "context": "Furthermore, ADMM can be easily adapted to solve large-scale distributed problems [1].", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "Hence, ADMM has been widely used in a large variety of areas [1].", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "Existing works have shown that batch ADMM is not efficient enough for big data applications with a large amount of training samples [5, 6].", "startOffset": 132, "endOffset": 138}, {"referenceID": 5, "context": "Existing works have shown that batch ADMM is not efficient enough for big data applications with a large amount of training samples [5, 6].", "startOffset": 132, "endOffset": 138}, {"referenceID": 4, "context": "Stochastic (online) ADMM, which visits only one sample or a mini-batch of samples each time, has recently been proved to achieve better performance than batch ADMM [5, 6, 3, 7, 4].", "startOffset": 164, "endOffset": 179}, {"referenceID": 5, "context": "Stochastic (online) ADMM, which visits only one sample or a mini-batch of samples each time, has recently been proved to achieve better performance than batch ADMM [5, 6, 3, 7, 4].", "startOffset": 164, "endOffset": 179}, {"referenceID": 2, "context": "Stochastic (online) ADMM, which visits only one sample or a mini-batch of samples each time, has recently been proved to achieve better performance than batch ADMM [5, 6, 3, 7, 4].", "startOffset": 164, "endOffset": 179}, {"referenceID": 6, "context": "Stochastic (online) ADMM, which visits only one sample or a mini-batch of samples each time, has recently been proved to achieve better performance than batch ADMM [5, 6, 3, 7, 4].", "startOffset": 164, "endOffset": 179}, {"referenceID": 3, "context": "Stochastic (online) ADMM, which visits only one sample or a mini-batch of samples each time, has recently been proved to achieve better performance than batch ADMM [5, 6, 3, 7, 4].", "startOffset": 164, "endOffset": 179}, {"referenceID": 6, "context": "Hence, stochastic ADMM has become a hot research topic and attracted much attention [7, 4].", "startOffset": 84, "endOffset": 90}, {"referenceID": 3, "context": "Hence, stochastic ADMM has become a hot research topic and attracted much attention [7, 4].", "startOffset": 84, "endOffset": 90}, {"referenceID": 4, "context": "Online alternating direction method (OADM) [5] is the first online ADMM method.", "startOffset": 43, "endOffset": 46}, {"referenceID": 5, "context": "Besides OADM, several stochastic ADMM methods have been proposed, including stochastic ADMM (STOC-ADMM) [6], regularized dual averaging ADMM (RDA-ADMM) [3], online proximal gradient descent based ADMM (OPG-ADMM) [3], optimal stochastic ADMM (OS-ADMM) [7], and stochastic average ADMM (SA-ADMM) [4].", "startOffset": 104, "endOffset": 107}, {"referenceID": 2, "context": "Besides OADM, several stochastic ADMM methods have been proposed, including stochastic ADMM (STOC-ADMM) [6], regularized dual averaging ADMM (RDA-ADMM) [3], online proximal gradient descent based ADMM (OPG-ADMM) [3], optimal stochastic ADMM (OS-ADMM) [7], and stochastic average ADMM (SA-ADMM) [4].", "startOffset": 152, "endOffset": 155}, {"referenceID": 2, "context": "Besides OADM, several stochastic ADMM methods have been proposed, including stochastic ADMM (STOC-ADMM) [6], regularized dual averaging ADMM (RDA-ADMM) [3], online proximal gradient descent based ADMM (OPG-ADMM) [3], optimal stochastic ADMM (OS-ADMM) [7], and stochastic average ADMM (SA-ADMM) [4].", "startOffset": 212, "endOffset": 215}, {"referenceID": 6, "context": "Besides OADM, several stochastic ADMM methods have been proposed, including stochastic ADMM (STOC-ADMM) [6], regularized dual averaging ADMM (RDA-ADMM) [3], online proximal gradient descent based ADMM (OPG-ADMM) [3], optimal stochastic ADMM (OS-ADMM) [7], and stochastic average ADMM (SA-ADMM) [4].", "startOffset": 251, "endOffset": 254}, {"referenceID": 3, "context": "Besides OADM, several stochastic ADMM methods have been proposed, including stochastic ADMM (STOC-ADMM) [6], regularized dual averaging ADMM (RDA-ADMM) [3], online proximal gradient descent based ADMM (OPG-ADMM) [3], optimal stochastic ADMM (OS-ADMM) [7], and stochastic average ADMM (SA-ADMM) [4].", "startOffset": 294, "endOffset": 297}, {"referenceID": 7, "context": "STOC-ADMM, RDA-ADMM, OPG-ADMM and OS-ADMM achieve a convergence rate of O(1/ \u221a T ) for general convex problems, worse than batch ADMM that has a convergence rate of O(1/T ) [8].", "startOffset": 173, "endOffset": 176}, {"referenceID": 3, "context": "Different from STOC-ADMM, RDA-ADMM, OPG-ADMM and OS-ADMM, SA-ADMM [4] can achieve a convergence rate of O(1/T ) for general convex problems by using historic gradients to approximate the full gradients in each iteration.", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "\u2022 Experimental results on graph-guided fused lasso [9] show that SCASADMM can achieve state-of-the-art performance in real applications.", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "Just like the Gauss-Seidel method, ADMM iteratively updates the variables in an alternating manner as follows [1]:", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "constraint y = x, we can get the lasso formulation [2].", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "Some works [5, 8] have proved that the above batch ADMM has a convergence rate O(1/T ) for general convex problems where f(x) and g(y) are convex but not necessarily to be strongly convex, where T is the number of iterations.", "startOffset": 11, "endOffset": 17}, {"referenceID": 7, "context": "Some works [5, 8] have proved that the above batch ADMM has a convergence rate O(1/T ) for general convex problems where f(x) and g(y) are convex but not necessarily to be strongly convex, where T is the number of iterations.", "startOffset": 11, "endOffset": 17}, {"referenceID": 4, "context": "Recent works have shown that stochastic ADMM can achieve better performance than batch ADMM to handle large-scale datasets in terms of computation complexity and accuracy [5, 6].", "startOffset": 171, "endOffset": 177}, {"referenceID": 5, "context": "Recent works have shown that stochastic ADMM can achieve better performance than batch ADMM to handle large-scale datasets in terms of computation complexity and accuracy [5, 6].", "startOffset": 171, "endOffset": 177}, {"referenceID": 9, "context": "Similar to most existing stochastic ADMM methods which adapt stochastic gradient descent (SGD) or its variants [10, 11, 12, 13, 14] to solve the problem in (7), SCAS-ADMM is also inspired by an existing SGD method called stochastic variance reduced gradient (SVRG) [15].", "startOffset": 111, "endOffset": 131}, {"referenceID": 10, "context": "Similar to most existing stochastic ADMM methods which adapt stochastic gradient descent (SGD) or its variants [10, 11, 12, 13, 14] to solve the problem in (7), SCAS-ADMM is also inspired by an existing SGD method called stochastic variance reduced gradient (SVRG) [15].", "startOffset": 111, "endOffset": 131}, {"referenceID": 11, "context": "Similar to most existing stochastic ADMM methods which adapt stochastic gradient descent (SGD) or its variants [10, 11, 12, 13, 14] to solve the problem in (7), SCAS-ADMM is also inspired by an existing SGD method called stochastic variance reduced gradient (SVRG) [15].", "startOffset": 111, "endOffset": 131}, {"referenceID": 12, "context": "Similar to most existing stochastic ADMM methods which adapt stochastic gradient descent (SGD) or its variants [10, 11, 12, 13, 14] to solve the problem in (7), SCAS-ADMM is also inspired by an existing SGD method called stochastic variance reduced gradient (SVRG) [15].", "startOffset": 111, "endOffset": 131}, {"referenceID": 13, "context": "Similar to most existing stochastic ADMM methods which adapt stochastic gradient descent (SGD) or its variants [10, 11, 12, 13, 14] to solve the problem in (7), SCAS-ADMM is also inspired by an existing SGD method called stochastic variance reduced gradient (SVRG) [15].", "startOffset": 111, "endOffset": 131}, {"referenceID": 14, "context": "Similar to most existing stochastic ADMM methods which adapt stochastic gradient descent (SGD) or its variants [10, 11, 12, 13, 14] to solve the problem in (7), SCAS-ADMM is also inspired by an existing SGD method called stochastic variance reduced gradient (SVRG) [15].", "startOffset": 265, "endOffset": 269}, {"referenceID": 5, "context": "1 Algorithm As in existing stochastic ADMM methods [6, 4], the update rules for y and \u03b2 are still the same as those in (4) and (5).", "startOffset": 51, "endOffset": 57}, {"referenceID": 3, "context": "1 Algorithm As in existing stochastic ADMM methods [6, 4], the update rules for y and \u03b2 are still the same as those in (4) and (5).", "startOffset": 51, "endOffset": 57}, {"referenceID": 14, "context": "Compared with SVRG [15], the update rule in (9) has an extra vector A\u03b2t+ \u03c1A (Awm + Byt \u2212 c) = \u03c1AAwm + A (\u03b2t + \u03c1(Byt \u2212 c)).", "startOffset": 19, "endOffset": 23}, {"referenceID": 3, "context": "The matrix inversion ( 1 \u03b7t I+\u03c1A TA)\u22121 can be avoided by linearizing the penalty term \u03c12 \u2016Ax + By \u2212 c\u2016 2 [4].", "startOffset": 105, "endOffset": 108}, {"referenceID": 4, "context": "Method Penalty term linearization? Convergence rate Memory cost OADM [5] NO O(1/ \u221a T ) O(lp+ lq) STOC-ADMM [6] NO O(1/ \u221a T ) O(lp+ lq) OPG-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) RDA-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) OS-ADMM [7] YES O(1/ \u221a T ) O(lp+ lq) SA-ADMM [4] NO O(1/T ) O(np+ lp+ lq) SA-IU-ADMM [4] YES O(1/T ) O(np+ lp+ lq) SCAS-ADMM YES O(1/T ) O(lp+ lq)", "startOffset": 69, "endOffset": 72}, {"referenceID": 5, "context": "Method Penalty term linearization? Convergence rate Memory cost OADM [5] NO O(1/ \u221a T ) O(lp+ lq) STOC-ADMM [6] NO O(1/ \u221a T ) O(lp+ lq) OPG-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) RDA-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) OS-ADMM [7] YES O(1/ \u221a T ) O(lp+ lq) SA-ADMM [4] NO O(1/T ) O(np+ lp+ lq) SA-IU-ADMM [4] YES O(1/T ) O(np+ lp+ lq) SCAS-ADMM YES O(1/T ) O(lp+ lq)", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "Method Penalty term linearization? Convergence rate Memory cost OADM [5] NO O(1/ \u221a T ) O(lp+ lq) STOC-ADMM [6] NO O(1/ \u221a T ) O(lp+ lq) OPG-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) RDA-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) OS-ADMM [7] YES O(1/ \u221a T ) O(lp+ lq) SA-ADMM [4] NO O(1/T ) O(np+ lp+ lq) SA-IU-ADMM [4] YES O(1/T ) O(np+ lp+ lq) SCAS-ADMM YES O(1/T ) O(lp+ lq)", "startOffset": 144, "endOffset": 147}, {"referenceID": 2, "context": "Method Penalty term linearization? Convergence rate Memory cost OADM [5] NO O(1/ \u221a T ) O(lp+ lq) STOC-ADMM [6] NO O(1/ \u221a T ) O(lp+ lq) OPG-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) RDA-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) OS-ADMM [7] YES O(1/ \u221a T ) O(lp+ lq) SA-ADMM [4] NO O(1/T ) O(np+ lp+ lq) SA-IU-ADMM [4] YES O(1/T ) O(np+ lp+ lq) SCAS-ADMM YES O(1/T ) O(lp+ lq)", "startOffset": 182, "endOffset": 185}, {"referenceID": 6, "context": "Method Penalty term linearization? Convergence rate Memory cost OADM [5] NO O(1/ \u221a T ) O(lp+ lq) STOC-ADMM [6] NO O(1/ \u221a T ) O(lp+ lq) OPG-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) RDA-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) OS-ADMM [7] YES O(1/ \u221a T ) O(lp+ lq) SA-ADMM [4] NO O(1/T ) O(np+ lp+ lq) SA-IU-ADMM [4] YES O(1/T ) O(np+ lp+ lq) SCAS-ADMM YES O(1/T ) O(lp+ lq)", "startOffset": 219, "endOffset": 222}, {"referenceID": 3, "context": "Method Penalty term linearization? Convergence rate Memory cost OADM [5] NO O(1/ \u221a T ) O(lp+ lq) STOC-ADMM [6] NO O(1/ \u221a T ) O(lp+ lq) OPG-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) RDA-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) OS-ADMM [7] YES O(1/ \u221a T ) O(lp+ lq) SA-ADMM [4] NO O(1/T ) O(np+ lp+ lq) SA-IU-ADMM [4] YES O(1/T ) O(np+ lp+ lq) SCAS-ADMM YES O(1/T ) O(lp+ lq)", "startOffset": 256, "endOffset": 259}, {"referenceID": 3, "context": "Method Penalty term linearization? Convergence rate Memory cost OADM [5] NO O(1/ \u221a T ) O(lp+ lq) STOC-ADMM [6] NO O(1/ \u221a T ) O(lp+ lq) OPG-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) RDA-ADMM [3] YES O(1/ \u221a T ) O(lp+ lq) OS-ADMM [7] YES O(1/ \u221a T ) O(lp+ lq) SA-ADMM [4] NO O(1/T ) O(np+ lp+ lq) SA-IU-ADMM [4] YES O(1/T ) O(np+ lp+ lq) SCAS-ADMM YES O(1/T ) O(lp+ lq)", "startOffset": 296, "endOffset": 299}, {"referenceID": 5, "context": "As in [6, 7, 4], we evaluate our method on the generalized lasso model [16] which can be formulated as follows:", "startOffset": 6, "endOffset": 15}, {"referenceID": 6, "context": "As in [6, 7, 4], we evaluate our method on the generalized lasso model [16] which can be formulated as follows:", "startOffset": 6, "endOffset": 15}, {"referenceID": 3, "context": "As in [6, 7, 4], we evaluate our method on the generalized lasso model [16] which can be formulated as follows:", "startOffset": 6, "endOffset": 15}, {"referenceID": 15, "context": "As in [6, 7, 4], we evaluate our method on the generalized lasso model [16] which can be formulated as follows:", "startOffset": 71, "endOffset": 75}, {"referenceID": 8, "context": "In this paper, we focus on the graph-guided fused lasso [9] which is also used in [4].", "startOffset": 56, "endOffset": 59}, {"referenceID": 3, "context": "In this paper, we focus on the graph-guided fused lasso [9] which is also used in [4].", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "As in [6, 4], we use sparse inverse covariance selection method [17] to get a graph matrix (sparsity pattern) G, based on which we can get A = [G; I].", "startOffset": 6, "endOffset": 12}, {"referenceID": 3, "context": "As in [6, 4], we use sparse inverse covariance selection method [17] to get a graph matrix (sparsity pattern) G, based on which we can get A = [G; I].", "startOffset": 6, "endOffset": 12}, {"referenceID": 0, "context": "They are: \u2022 Batch-ADMM [1]: The deterministic (batch) variant of ADMM which uses (7) to directly update x by visiting all training samples in each iteration.", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "\u2022 STOC-ADMM [6]: The stochastic ADMM variant without using historic gradient for optimization, which has a convergence rate of O(1/ \u221a T ) for general convex problems and O(log T/T ) for strongly convex problems.", "startOffset": 12, "endOffset": 15}, {"referenceID": 3, "context": "\u2022 SA-ADMM [4]: The stochastic ADMM variant by using historic gradient to approximate the full gradient, which has a convergence rate of O(1/T ) for general convex problems.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "Furthermore, both theoretical and empirical results have shown that SA-ADMM can outperform other methods like RDA-ADMM and OPG-ADMM [4].", "startOffset": 132, "endOffset": 135}, {"referenceID": 3, "context": "The variant of SA-ADMM, SA-IU-ADMM, is also not adopted for comparison because it has similar performance as SA-ADMM [4].", "startOffset": 117, "endOffset": 120}, {"referenceID": 3, "context": "As in [4], four widely used datasets are adopted to evaluate our method and other baselines.", "startOffset": 6, "endOffset": 9}, {"referenceID": 3, "context": "As in [4], for each dataset we randomly choose half of the samples for training and use the rest for testing.", "startOffset": 6, "endOffset": 9}, {"referenceID": 3, "context": "The hyper-parameter \u03bb in (14) is set by using the same values in [4], which are also listed in Table 2.", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "We adopt the same strategy as that in [4] to set the hyper-parameters \u03c1 in (2) and the", "startOffset": 38, "endOffset": 41}, {"referenceID": 3, "context": "As in [4], we use y(x\u0304T ) = Ax\u0304T to replace \u0233T since the methods cannot necessarily guarantee that Ax\u0304T = \u0233T .", "startOffset": 6, "endOffset": 9}, {"referenceID": 3, "context": "2 Convergence Results As in [4], we study the variation of the objective value on training set and the testing loss versus the number of effective passes over the data.", "startOffset": 28, "endOffset": 31}, {"referenceID": 0, "context": "[1] Stephen P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Robert Tibshirani.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Taiji Suzuki.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Wenliang Zhong and James Tin-Yau Kwok.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Huahua Wang and Arindam Banerjee.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Hua Ouyang, Niao He, Long Tran, and Alexander G.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Samaneh Azadi and Suvrit Sra.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Bingsheng He and Xiaoming Yuan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Seyoung Kim, Kyung-Ah Sohn, and Eric P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Lin Xiao.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] John C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Nicolas Le Roux, Mark W.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Julien Mairal.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Shai Shalev-Shwartz and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Rie Johnson and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Ryan J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "According to the results in [4], we have the following Lemma 4 and Lemma 5 about the estimation of yt+1 and \u03b1t+1.", "startOffset": 28, "endOffset": 31}, {"referenceID": 3, "context": "The proof of Lemma 4 and Lemma 5 can be directly derived from the results in [4], which is omitted here for space saving.", "startOffset": 77, "endOffset": 80}], "year": 2015, "abstractText": "Most stochastic ADMM (alternating direction method of multipliers) methods can only achieve a convergence rate which is slower than O(1/T ) on general convex problems, where T is the number of iterations. Hence, these methods are not scalable in terms of convergence rate (computation cost). There exists only one stochastic method, called SA-ADMM, which can achieve a convergence rate of O(1/T ) on general convex problems. However, an extra memory is needed for SA-ADMM to store the historic gradients on all samples, and thus it is not scalable in terms of storage cost. In this paper, we propose a novel method, called scalable stochastic ADMM (SCAS-ADMM), for large-scale optimization and learning problems. Without the need to store the historic gradients on all samples, SCAS-ADMM can achieve the same convergence rate of O(1/T ) as the best stochastic method SA-ADMM and batch ADMM on general convex problems. Experiments on graph-guided fused lasso show that SCASADMM can achieve state-of-the-art performance in real applications.", "creator": "LaTeX with hyperref package"}}}