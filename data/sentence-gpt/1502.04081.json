{"id": "1502.04081", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2015", "title": "A Linear Dynamical System Model for Text", "abstract": "Low dimensional representations of words allow accurate NLP models to be trained on limited annotated data. While most representations ignore words' local context, a natural way to induce context-dependent representations is to perform inference in a probabilistic latent-variable sequence model. Given the recent success of continuous vector space word representations, we provide such an inference procedure for continuous states, where words' representations are given by the posterior mean of a linear dynamical system. Here, efficient inference can be performed using Kalman filtering. Our learning algorithm is extremely scalable, operating on simple cooccurrence counts for both parameter initialization using the method of moments and subsequent iterations of EM. In our experiments, we employ our inferred word embeddings as features in standard tagging tasks, obtaining significant accuracy improvements. Finally, the Kalman filter updates can be seen as a linear recurrent neural network. We demonstrate that using the parameters of our model to initialize a non-linear recurrent neural network language model reduces its training time by a day and yields lower perplexity.\n\n\n\n\nWe are currently working on a series of novel and novel ways to generate the models with NLP training. The first is to apply the Kalman filter to neural networks such as neural networks. The second is to apply the Kalman filter to non-linear recurrent neural networks. To determine the posterior mean, we first need to consider three steps: first, we first need to apply the Kalman filter to non-linear recurrent neural networks. Second, we must use the Kalman filter to perform a deep neural network translation. We first need to perform an inference of the posterior mean of the data for each variable at all times. The data for each variable are stored in separate files. We then need to create a layer containing this layer, one that stores the data in separate files, which then stores the data into the training pipeline. In this layer, the data of the model is then mapped to the training pipeline.\n\nThe training pipeline can be used as a test ground for the training pipeline. In this layer, training is supervised by a Kalman filter. For each variable, we first need to perform a regression model based on the distribution of training volume. In the training pipeline, we first need to extract all the training volumes, and we pass that to the training pipeline.\nAfter performing the training pipeline, the training pipeline will be processed for each variable, which we define as the parameters for each variable. It is also possible to perform a linear regression model based on the distribution of training volume", "histories": [["v1", "Fri, 13 Feb 2015 18:39:29 GMT  (62kb,D)", "http://arxiv.org/abs/1502.04081v1", null], ["v2", "Sun, 31 May 2015 20:04:53 GMT  (68kb,D)", "http://arxiv.org/abs/1502.04081v2", "Accepted at International Conference of Machine Learning 2015"]], "reviews": [], "SUBJECTS": "stat.ML cs.CL cs.LG", "authors": ["david belanger", "sham m kakade"], "accepted": true, "id": "1502.04081"}, "pdf": {"name": "1502.04081.pdf", "metadata": {"source": "CRF", "title": "A Linear Dynamical System Model for Text", "authors": ["David Belanger", "Sham Kakade"], "emails": ["BELANGER@CS.UMASS.EDU", "SKAKADE@MICROSOFT.COM"], "sections": [{"heading": "1. Introduction", "text": "In many NLP applications, there is limited available labeled training data, but tremendous quantities of unlabeled, in-domain text. An effective semi-supervised learning technique is to learn word embeddings on the unlabeled data, which map every word to a low dimensional dense vector (Bengio et al., 2006; Mikolov et al., 2013; Penning-\nton et al., 2014), and then use these as features for supervised training on the labeled data (Turian et al., 2010; Passos et al., 2014; Bansal et al., 2014). Furthermore in many deep architectures for NLP, the first layer maps words to low-dimensional vectors, and these parameters are initialized with unsupervised embeddings (Collobert et al., 2011; Socher et al., 2013; Vinyals et al., 2014).\nMost of these methods embed word types, i.e., words independent of local context, as opposed to work tokens, i.e., instances of words within their context. Ideally we would have a different representation per token. For example, depending on the context, \u201cbank\u201d is the side of a river or a financial institution. Furthemore, we would like such embeddings to come from a probablistic sequence model that allows us to study the transition dynamics of text generation in low dimensional space.\nWe present a method for obtaining such context-dependent token embeddings, using a generative model with a vectorvalued latent variable per token and performing posterior inference for each sentence. Specifically, we employ a Gaussian linear dynamical system (LDS), with efficient inference from a Kalman filter. To learn the LDS parameters, we use a two-stage procedure, initializing with the method of moments, and then performing EM with the approximate second order statistics (ASOS) technique of Martens (2010). Overall, after taking a single pass over the training corpus, our the runtime of our approximate maximumlikelihood estimation (MLE) procedure is independent of the amount of training data since it operates on aggregate co-occurrence counts. Furthermore, performing inference to obtain token embeddings has the same time complexity as widely-used discrete first-order sequence models.\nWe fit the LDS to a one-hot encoding of each token in the input text sequence. Therefore, the LDS is a mis-specified generative model, since draws from it are not proper indicator vectors. However, we embrace this multivariate Gaussian model instead of a continuous-state dynamical system with a multinomial link function because the Gaussian LDS offers several desirable scalability properties: (1) Kalman\nar X\niv :1\n50 2.\n04 08\n1v 1\n[ st\nat .M\nL ]\n1 3\nFe b\n20 15\nfilter inference is simple and efficient (2) using ASOS, the cost of our learning iterations does not scale with the corpus size, (3) we can initialize EM using a method-of-moments estimator that requires a single SVD of a co-occurrence matrix, (4) our M-step updates are simple least-squares problems, solvable in closed form, (4) if we had used a multinomial link function, we would have performed inference using extended Kalman filtering, which makes a secondorder approximation of the log-likelihood, and thus leads to a Gaussian LDS anyway (Ghahramani & Roweis, 1999), and (5) by using EM, we avoid stochastic-gradient-based optimization, which requires careful tuning for nonconvex problems. A naive application of our method scales to large amounts of training data, but not high-dimensional observations. In response, we contribute a variety of new methods for scaling up our learning techniques to handle large input vocabularies.\nWe employ our inferred token embeddings as features for part of speech (POS) and named entity recognition (NER) taggers. For POS, we obtain a 30% relative error reduction when applying a local classifier to our context-dependent embeddings rather than Word2Vec context-independent embeddings (Mikolov et al., 2013). When using our token embeddings as additional features in lexicalized POS and NER taggers, which already have explicit features and test-time inference for context-dependence, we obtain signficant gains over the baseline, performing as well as using Word2Vec embeddings. We also present experiments demonstrating that the transition dynamics of the LDS capture salient patterns, such as transforming first names into last names.\nFinally, the functional form of the Kalman filter update equations for our LDS are identical to the updates of a recurrent neural network (RNN) language model without non-linearities (Mikolov, 2012). A key difference between the LDS and an RNN, however, is that the LDS provides a natural backwards pass, using Kalman smoothing, where a token\u2019s embedding depends on text to both the right and left. Drawing on the parallelism between filtering and the RNN, we use the LDS parameters to initialize gradientbased optimization of a nonlinear RNN, where the LDS training time is a small fraction of the RNN time. The RNN initialized with the LDS obtains a signficant decrease in perplexity v.s. the baseline RNN, and only requires 70% as many training epochs, saving 1 day on a single CPU core."}, {"heading": "2. Related Work", "text": "We provide a continuous analog of popular discrete-state generative models used in NLP for inducing class membership for tokens, including class-based language models (Brown et al., 1992; Chelba & Jelinek, 2000) and induction of POS tags (Christodoulopoulos et al., 2010). In par-\nticular, Brown clusters (Brown et al., 1992) are commonly used by practioners with lots of unlabeled in-domain data.\nOur learning algorithm is very scalable because it operates on aggregate count matrices, rather than individual tokens. Similar algorithms have been proposed for obtaining type-level embeddings via matrix factorization (Pennington et al., 2014; Levy & Goldberg, 2014). However, these are context-independent and ignore the transition dynamics that link tokens\u2019 embeddings. Furthermore, they require careful tuning of stochastic gradient algorithms. Previous methods for token-level embeddings either use a rigid set of prototypes (Huang et al., 2012; Neelakantan et al., 2014) or embed the token\u2019s context, ignoring the token itself (Dhillon et al., 2011).\nFor learning discrete-state latent variable models, spectral learning methods also use count matrices, and thus are similarly scalable (Anandkumar et al., 2014). However, an LDS offers key advantages: we do not use third-order moments, which are difficult to estimate, and perform approximate MLE, rather than the method of moments, which exhibits poor statistical efficiency.\nRecently, RNNs have been used to provide impressive results in NLP applications including translation (Sutskever et al., 2014), language modeling (Mikolov et al., 2014), and parsing (Vinyals et al., 2014). We do not attempt to replace these with a Kalman filter, as we expect non-linearities are crucial for capturing long-term interactions and rigid, combinatorial constraints in the outputs. However, RNNs training can take days, even on GPUs, and requires careful tuning of stochastic gradient algorithms. Given the scalability of our parameter-free training algorithm, and our favorable preliminary results using the LDS to initialize a nonlinear RNN, we encourage further work on using linear latent variable models and the Gaussian approximations of multinomial data to develop sophisticated initialization methods. Already, practitioners have started using such techniques for initializing simple nonlinear deep neural networks using the recommendations of Saxe et al. (2014). Finally, our work differs from Pasa & Sperduti (2014), who initialize an RNN using spectral techniques, in that we perform maximum-likelihood learning. We found this crucial for good performance in our NLP experiments."}, {"heading": "3. Background: Gaussian Linear Dynamical Systems", "text": "We consider sequences of observations w1, . . . , wn, where each wi is a V -dimensional vector. An Gaussian LDS follows the following generative model (Kalman, 1960; Roweis & Ghahramani, 1999):\nxt = Axt\u22121 + \u03b7 (1) wt = Cxt + , (2)\nwhere h < V is the dimensionality of the hidden states xt and \u223c N(0, D), \u03b7 \u223c N(0, Q). For simplicity, we assume x0 is constant.\nThe latent space for x is completely unobserved and we could choose any coordinate system for it, while maintaining the same data likelihood. Therefore, without loss of generality, we can either fix A = I or Q = I , and we fix Q. Furthermore, note that the magnitude of the maximum eigenvalue of A must be no larger than 1 if the system is stable. We assume that the data we fit to has been centered, in which case the maximum eigenvalue is strictly less than 1, since this implies xt is asymptotically mean zero (independent of x0), so that xt is also asymptotically mean zero.\nFinally, define the covariance at lag k to be\n\u03a8k = E[wt+kw>t ], (3)\nwhich is valid because we assume the data to be mean zero. Our learning algorithms require only a few \u03a8k (up to about k = 10 in practice) as input. These matrices can be gathered using a single pass over the data, and their size does not depend on the amount of data. Furthermore, constructing these matrices can be accelerated by splitting the data into chunks, and aggregating separate matrices afterwards."}, {"heading": "3.1. Inference", "text": "The xt are distributed as a multivariate Gaussian under both the LDS prior and posterior (conditional on observations w), so they can be fully characterized by a mean and variance. Therefore, we will use x\u0302t and St for the mean and covariance under the posterior for xt given w1:(t\u22121), computed using Kalman filtering, and x\u0304t and ST when considering the posterior for xt given all the data w1:T , computed using Kalman smoothing. In Appendix B.1 we provide the full filtering and smoothing updates, which compute different means and variances for every timestep. Note that the updates require inverting a V \u00d7 V matrix in every step.\nWe employ the widely-used \u2019steady-state\u2019 approximation, which yields substantially more efficient filtering and smoothing updates (Rugh, 1996). A key property of filtering and smoothing is that the updates to St and ST do not depend on the actual observations, but only on the model\u2019s parameters. Furthermore, they will converge quickly to time-independent \u2018steady-state\u2019 values. Define \u03a31 = E[x\u0302tx\u0302>t |w1:(t\u22121)] to be the aymptotic limit of the covariance St under the posterior for each xt given its history (at steady state, this is shared for all t). Here, expectation is taken with respect to both time and the posterior for the latent variables. This satisfies\n\u03a31 = A\u03a31A > +Q,\nwhich can be solved for quickly using fixed point iteration. Similarly, we can solve for \u03a30 = E[x\u0302tx\u0302>t |w1:t]. Note that\nsteady state, a property of the posterior, is unrelated to the stationary distribution of the LDS, which is unconditional on observations.\nUnder, the steady state assumption, we can perform filtering and smoothing using substantially more efficient updates. We have:\nx\u0302t = (A\u2212KCA)x\u0302t\u22121 +Kwt (4) x\u0304t = Jx\u0304t+1 + (I \u2212 JA)x\u0302t (5)\nHere, the steady-state Kalman gain matrix is:\nK = \u03a31C >S\u22121ss \u2208 Rh\u00d7V , (6)\nwhere we define\nSss = C\u03a31C > +D, (7)\nthe unconditional prior covariance for w under the model. Note that (A\u2212KCA) is data-independent and can be precomputed, as can the smoothing matrix J = \u03a30A>(\u03a31)\u22121. For long sequences, steady-state filtering provides asymptotically exact inference. However, for short sequences it is an approximation."}, {"heading": "3.2. Learning: Expectation-Maximization", "text": "See Ghahramani & Hinton (1996) for a full exposition on learning the parameters of an LDS using EM. Under the steady-state assumption, the M step requires:\nE[x\u0304tx\u0304>t ], E[x\u0304tx\u0304>t+1], E[x\u0304tw>t ], (8)\nwhere the expectation is taken with respect to time and the posterior for the latent variables. This can be computed using Kalman smoothing and then averaging over time. The M step can then be done in closed form, since it is solving least-squares regressions for xt+1 against xt andwt against xt to obtain A and C. Lastly, D can be recovered using:\nD = \u03a80 \u2212 CE [ x\u0304tw > t ] \u2212 E [ wtx\u0304 > t ] C> + CE [ x\u0304x\u0304>t ] C> (9)"}, {"heading": "3.3. Learning: EM with ASOS (Martens, 2010)", "text": "EM requires recomputing the second order statistics (8) in every iteration. While these can be computed using Kalman smoothing on the entire training set, we are interested in datasets with billions of timesteps. Fortunately, we can avoid smoothing by employing the ASOS (approximate second order statistics) method of Martens (2010), which directly performs inference about the time-averaged second order statistics.\nUnder the steady-state assumption, this is doable because we can recursively define relationships between second order statistics at lag k and at lag k + 1 using the recursive\nrelationships of the underlying dynamical system. Namely, rather than performing posterior inference by recursively applying the linear operations (4) and (5), and then averaging over time, we switch the order of these operations and apply the linear operators to time-averaged second order statistics. For example, the following equality is an immediate consequences of the filtering equation (4) (where expectation is with respect to t and the posterior for x):\nE[x\u0302ttw>t ] = (A\u2212KCA)E[x\u0302t\u22121t\u22121w>t ] +KE[wtw>t ] (10)\nASOS uses a number of such recursions, along with methods for estimating covariances at a time horizon r. These covariances can be approximated by assuming that they are exactly described by the current estimate of the model parameters. Therefore, unlike standard EM, performing EM with ASOS allows us to precompute an empirical estimate of the \u03a8k at various lags (up to about r = 10) and then never touch the data again. Furthermore, (Martens, 2010) demonstrates that the ASOS approximation is consistent. Namely, the error in approximating the time-averaged second order statistics vanishes with infinite data when evaluated at the MLE parameters. Overall, ASOS scales linearly with r and the cost of multiplying by the \u03a8k."}, {"heading": "3.4. Learning: Subspace Identification", "text": "We initialize EM using Subspace Identification (SSID), a family of method-of-moments estimators that use spectral decomposition to recover LDS parameters (Van Overschee & De Moor, 1996). The rationale for such a combination is that the method of moments is statistically consistent, so performing it on reasonably-sized datasets will yield parameters in the neighborhood of the global optimum, and then EM will perform local hill climbing to find a local optimum of the marginal likelihood. For LDS, this combination yields empirical accuracy gains in Smith et al. (1999). A related two-stage estimator, where the local search of EM is replaced with a single Newton step on the local likelihood surface, is known to be minimax optimal, under certain local asymptotic normality conditions (Le Cam, 1974).\nFor our particular application, we use SSID as an approximate method, where it is not statistically consistent, due to the mis-specification of fitting indicator-vector data as a multivariate Gaussian. In our experiments, we discuss the superiority of SSID+EM rather than just SSID. We do not present results using EM initialized randomly rather than with SSID, since we found it very difficult for our high dimensional problems to generate initial parameters that allowed EM to reach high likelihoods.\nWe employ the \u2018n4sid\u2019 algorithm of Van Overschee & De Moor (1994). Define r to be a small integer. Define the (rV ) \u00d7 h matrix \u0393r = [ C ; CA ; CA2 ; . . . ; CAr\u22121 ] , where \u2018;\u2019 denotes vertical concatenation. Also define the\nAlgorithm 1 Learning an LDS for Text Input: Text Corpus, approximation horizon r (e.g., 10) Output: LDS parameters and filtering matrices: (A,C,D,K, J)\nGather the matrices \u03a8k = E[wt+kw>t ] (k < r) W \u2190 \u03a8\u2212 1 2\n0 (diagonal whitening matrix) \u03a8k \u2190W\u03a8kW> (whitening) Params\u2190 Subspace ID(\u03a80, . . . ,\u03a8r) Params\u2190 ASOS EM(Params,\u03a80, . . . ,\u03a8r)\nx-w covariance at lag 0: G = E[xt+1y>t ] and the h\u00d7 (rV ) matrix \u2206r = [ Ar\u22121G Ar\u22122G . . . AG G ] .\nNext, define the Hankel matrix\nHr =  \u03a8r \u03a8r\u22121 \u03a8r\u22122 . . . \u03a81 \u03a8r+1 \u03a8r \u03a8r\u22121 . . . \u03a82 . . .\n\u03a82r\u22121 \u03a82r\u22122 \u03a8r\u22123 . . . \u03a8r  . (11) Then, we have Hr = \u0393r\u2206r.\nLet (U, S, V ) result from a rank-h SVD of an empirical estimate of Hr, from which we set \u0393r = US 1 2 and \u2206r = S 1 2V >. To recover the LDS parameters, we first define \u22061:(r\u22121)r to be the submatrix of \u2206r corresponding to the first (r \u2212 1) blocks. Similarly, define \u22062:rr . From the definition of \u2206r, we have that A\u22062:rr = \u2206 1:(r\u22121) r , so we can estimate A as A = \u22061:(r\u22121)r (\u22062:rr ) +. Next, one can read off an estimate for E as the first block of \u0393r. Alternatively, since the previous step gives us a value for A one can set up a regression problem similar to the previous step to solve for C by invoking the block structure in \u0393r.\nFinally, we need to recover the covariance matrix D. We first find the asymptotic latent covariance \u03a31 using fixedpoint iteration \u03a31 = A\u03a31A> + Q. From this, we set D using a similar update as (12), except we use \u03a31, unconditional on data, rather than statistics of the LDS posterior:\nD = \u03a80 \u2212 C\u03a31C>. (12)"}, {"heading": "4. Linear Dynamical Systems for Text", "text": "We fit an LDS to text using SSID to initialize EM, where the E step is performed using ASOS. A summary of the procedure is provided in Algorithm 1. SSID and ASOS scale to extremely large training sets, since they only require the \u03a8k matrices, for small k. However, they can not directly handle the very high dimensionality of text observations (vocabulary size V \u2248 105). In this section, we first describe particular properties of the data distribution. Then,\nwe describe novel techniques for leveraging these properties to yield scalable learning algorithms.\nDefine w\u0303t as an indicator vector that is 1 in the index of the word at time t and define \u00b5i to be the corpus frequency of word type i. We fit to the mean-zero observations wt = w\u0303t \u2212 \u00b5. Note that the LDS will not generate observations with the structure of a one-hot vector shifted by a constant mean, so we cannot use it directly as a generative language model. On the other hand, we can still fit models to training data with this structure, perform posterior inference given observations, assess the likelihood of a corpus, etc. In our experiments, we demonstrate the usefulness of these in a variety of applications. We have:\n\u03a80 = E[wtw>t ] = E[w\u0303tw\u0303>t ]\u2212 \u00b5\u00b5> = diag(\u00b5)\u2212 \u00b5\u00b5>, (13)\nwhile at higher lags,\n\u03a8k = E[wtw>t+k] = E[w\u0303tw\u0303>t+k]\u2212 \u00b5\u00b5>. (14)\nApproximating these covariances from a length-T corpus:\n\u00b5i = Ex[w\u0303t] = 1\nT #(word i appears), (15)\nwhere #() denotes the count of an event. We also have\nE[w\u0303tw\u0303>t+k]i,j = (16) 1 T #(word i appears with word j k positions to the right).\nFor real-world data, (16) will be extremely sparse, with the number of nonzeros substantially less than both V 2 and the length of the corpus. The fact that (14) is sparse-minuslow-rank and (13) is diagonal-minus-low-rank is critical for scaling up the learning algorithms. First of all, we do not instantiate these as V \u00d7 V dense matrices, but operate directly on their factorized structure. Second, in Sec. 4.2 we show how the structure of (13) allows us to model fullrank V \u00d7 V noise covariance matrices implicitly. Strictly speaking, the number of nonzeros will in (16) will increase as the corpus size increases, due to heavy-tailed word cooccurence statistics. However, this growth is sublinear in T and can be mitigated by ignoring rare words.\nUnfortunately, each \u03a8k is rank-deficient. Not only is E[wt] = 0, but also the sum of everywt is zero (because w\u0303t is a one-hot vector and \u00b5 is a vector of word frequencies). Define 1 to be the length-V vector of ones. Our data lives on 1\u22a5, the d\u22121 dimensional subspace, orthogonal to 1. Doing maximum likelihood in RV instead of 1\u22a5 will lead to a degenerate likelihood function, since the empirical variance in the 1 direction is 0. However, projecting the data to this subspace breaks the special structure described above, so we instead work in RV and perform projections onto 1\u22a5\nimplicitly as-needed. Fortunately, both SSID and EM find C that lies in the column space of the data, so iterations of our learning algorithm will maintain that 1 /\u2208 col(C). In Appendix A.2, we describe how to handle this rank deficiency when computing the Kalman gain.\nNote that we could have used pretrained type-level embeddings to project our corpus and then train an LDS on lowdimensional dense observations. However, this is vulnerable to the subspace of the type-level embeddings, which are not trained to maximize the likelihod of a sequence model, and thus might not capture proper syntactic and semantic information. We will release the code of our implementation. SSID requires simple scripting on top of a sparse linear algebra library. Our EM implementation consists of small modifications to Martens\u2019 public code."}, {"heading": "4.1. Scalable Spectral Decomposition", "text": "SSID requires a rank-h SVD of the very large block Hankel matrix Hr (11). We employ the randomized approximate SVD algorithm of Halko et al. (2011). To factorize a matrix X , this requires repeated multiplication by X and by X>. All the submatrices in Hr are sparse-minus-low-rank, so we handle the sparse and low-rank terms individually within the multiplication subroutines."}, {"heading": "4.2. Modeling Full-Rank Noise Covariance", "text": "The noise covariance matrix D is V \u00d7 V , which is unmanageably large for our application, and thus it is reasonable to employ a spherical D = dI or diagonal D = diag(d1, . . . , dV ) approximation. For our problem, however, we found that these approximations performed poorly. Because of the property 1>wt = 0, off-diagonal elements ofD are critical for modeling the anti-correlations between coordinates. This would have been captured we passed wt through a logistic multinomial link function. However, this prevents simple inference using Kalman filter. To maintain conjugacy, practitioners sometimes employ the quadratic upper bound to a logistic multinomial likelihood introduced in Bo\u0308hning (1992), which hard-codes the coordinate-wise anticorrelations via D = 12 [ I \u2212 1V+111 > ] . However, we found this data-independent estimator performed poorly.\nInstead, we exploit a particular property of the SSID and EM estimators forD in (12) and (9). Namely, both setD to \u03a80 minus a low-rank matrix, and thusD is diagonal-minuslow-rank, due to the structure in (13). For the LDS, we mostly seek to manipulate the precision matrixD\u22121. While instantiating this dense V \u00d7 V matrix is infeasible, multiplication by D\u22121 and evaluation of det(D\u22121) can both be done efficiently using the matrix inversion lemma, a.k.a. the ShermanWoodbury-Morrison formula (Appendix B.2). In Appendix A.3, we also leverage the lemma to efficiently\nevaluate the training likelihood. These uses of the lemma differ from its common usage for LDS, when not using the steady-state assumption and the posterior precision matrix needs to be updated using rank one updates to the covariance. Our technique is particular to fitting indicator-vector data as a multivariate Gaussian."}, {"heading": "4.3. Whitening", "text": "Before applying our learning algorithms, we first whiten the \u03a8 matrices with the diagonal transformation.\nW = \u03a8 \u2212 12 0 = diag(\u00b5 \u2212 12 1 , . . . , \u00b5 \u2212 12 V ). (17)\nFitting to W\u03a8kW>, rather than \u03a8k, maintains the data\u2019s sparse-minus-low-rank and diagonal-minus-lowrank structures. Furthermore, EM is unaffected, i.e., applying EM to linearly-transformed data is equivalent to learning on the original data and then transforming post-hoc.\nOn the other hand, the SSID output is affected by whitening, since the squared reconstruction loss that SVD implicitly minimizes depends on the coordinate system of the data. We found such whitening crucial for obtaining high-quality initial parameters. Whitening for SSID, which is recommended by Van Overschee & De Moor (1996), solves a very similar factorization problem as canonical correlation analysis between words and their contexts, which has been used successfully to learn word embeddings (Dhillon et al., 2011; 2012) and identify the parameters of class-based language models (Stratos et al., 2014)).\nIn Appendix A.1 we also provide an algorithm, which relies on whitening, for manually ensuring the D returned by SSID is PSD, without needing to factorize a V \u00d7V matrix. Such manual correction is unnecessary during EM, since the estimator (9) is guaranteed to be PSD."}, {"heading": "5. Embedding Tokens using the LDS", "text": "The only data-dependent term in the steady-state filtering and smoothing equations (4) and (5) is Kwt. Since wt can take on only V possible values, we precompute these word-type-level vectors. The computational cost of filtering/smoothing a length T sequence is O(Th2), which is identical to the cost of inference on a discrete first-order sequence model. (6) is not directly usable to obtain K, due to the data\u2019s rank-deficiency, and we provide an efficient alternative in Appendix A.2. This also requires the matrix inversion lemma to avoid instantiating S\u22121 in (6).\nIn our experiments we use the latent space to define features for tokens. However, distances in this space are not well-defined, since the likelihood is invariant to any linear transformation of the latent variables. To place xt in reasonable coordinates, we compute the empirical posterior covariance M = E[x\u0304x\u0304>] on the training data (using\nASOS). Then, we whiten xt using M\u2212 1 2 and project the result onto the unit sphere."}, {"heading": "6. Relation to Recurrent Neural Networks", "text": "We now highlight the similarity between the parametrization of an RNN architecture commonly used for language modeling and our Kalman filter. This allows us to use our LDS as a novel method for initializing the parameters of a non-linear RNN, which we explore in Sec. 7.4. Following (Mikolov, 2012) we consider the network structure:\nht = \u03c3(Aht\u22121 +Bwt\u22121) (18) wt \u223c SoftMax(Cht), (19)\nHere, we employ the SoftMax transformation of a vector v as vi \u2192 exp(vi)/ \u2211 k exp(vk). The coordinate-wise nonlinearity \u03c3(\u00b7) is, for example, a sigmoid, and the network is initialized with some fixed vector h0.\nConsider the use of the steady-state Kalman filter (4) as an online predictor, where the mean prediction for wt is given by Cx\u0302t. Then, if we replace \u03c3 and SoftMax with the identity, the Kalman filter and the RNN have the same set of parameters, where weB corresponds toK andA corresponds to (A \u2212 KCA). In terms of the state dynamics, the LDS may provide parameters that are reasonable for a nonlinear RNN, since the sigmoid \u03c3 has a regime for inputs close to zero where it behaves like the identity. A linear approximation of SoftMax() ignores mutual exclusivity. However, we discuss in Section 4.2 that using a full-rank D captures some coordinate-wise anti-correlations. Also, (19) does not affect the state evolution in (18).\nA key difference between the LDS and the RNN is that the LDS provides a backwards pass, using Kalman smoothing, where x\u0304t depends on words to the right. For RNNs, this would requires separate model (Schuster & Paliwal, 1997)."}, {"heading": "7. Experiments", "text": ""}, {"heading": "7.1. LDS Transition Dynamics", "text": "Many popular word embedding methods learn word-tovector mapping, but do not learn the dynamics of text\u2019s evolution in the latent space. Using the specific LDS model we describe in the next section, we employ the transition matrix A to explore properties of these dynamics. Because the state evolution is linear, it can be studied easily using a spectral decomposition. Namely, A converts its left singular vectors into (scaled) right singular vectors. For each vector, we find the words that are most likely to be generated from this state. In Table 1 we present these singular vector pairs. We find that they reflect interpretable transition dynamics. In all but the last block, the vectors reflect strict state transitions. However, in the case of final block,\nabout food, we found that it contains topical information invariant under A. Overall, we did not find such salient structure in the parameters estimated using SSID."}, {"heading": "7.2. POS Tagging", "text": "Unsupervised learning of generative discrete sequence models for text has been shown to capture part-of-speech (POS) information (Christodoulopoulos et al., 2010). In response, we assess the ability of the LDS to also capture POS structure. Token embeddings can be used to predict POS in two ways: (1) by applying a local classifier to each token\u2019s embedding, or (2) by including each token\u2019s embedding as additional features in a lexicalized tagger. For both, we train the tagging model on the Penn Treebank (PTB) train set, which is not included for LDS training. Token embeddings are obtained from Kalman smoothing. We evaluate tagging accuracy on the PTB test set using the 12 \u2018universal\u2019 POS tags (Petrov et al., 2011) and the original tags. We contrast the LDS with type embeddings from Word2Vec, trained on the LDS data (Mikolov et al., 2013).\nWe fit our LDS using a combination of the APNews, New York Times, and RCV1 newswire corpora, about 1B tokens total. We maintain punctuation and casing of the text, but replace all digits with \u201cNUM\u2019 and all but the most 200k frequent types with \u201cOOV.\u201d We employ r = 4 for SSID, r = 7 for EM, and h = 200. We add 1000 psuedocounts for each type, by adding 1000T to each coordinate of \u00b5.\nThe LDS hyperparameters were selected by maximizing the accuracy of a local classifier on the PTB dev set. This also included when to terminate EM. For Word2Vec, we performed a broad search over hyperparameters, again maximizing for local POS tagging. Our local classifier was\na two-layer neural network with 25 hidden units, which outperformed a linear classifier. The best Word2Vec configuration used the CBOW architecture with a window width of 3. The lexicalized tagger\u2019s hyperparameters were also tuned on the PTB dev set. For the local tagging, we ignored punctuation and few common words types such as \u201cand\u201d in training. Instead, we classified them directly using their majority tag in the training data.\nOverall, we found that the LDS and Word2Vec took about 12 hours to train on a single-core CPU. Since the Word2Vec algorithm is simple and the code is heavily optimized, it performs well, but our learning algorithm would have been substantially faster given a larger training set, since the \u03a8k matrices can be gathered in parallel and the cost of SSID and ASOS is sublinear in the corpus size. In Section 7.4, training the LDS is order of magnitude faster than an RNN.\nOur results are shown in Table 2. Left to right, we compare Word2Vec (W2V), SSID, EM initialized with SSID (EM), our baseline lexicalized tagger (LEX), the lexicalized tagger with extra features from LDS token embeddings (LEX + EM), and the lexicalized tagger with typelevel Word2Vec embeddings (LEX + SSID).\nThe first 3 columns perform local classification. First, while SSID is crucial for EM initialization, we found it performed poorly on its own. However, EM outperforms Word2Vec substantially. We expect this is because the LDS embeds tokens v.s. types and EM explicitly maximizes the likelihood of text sequences, and thus it forces token embeddings to capture the transition dynamics of syntax. All accuracy differences are statistically significant at a .05 significance level using the exact binomial test. In Appendix C, we present a figure demonstrating the importance of SSID v.s. random initialization.\nThe final 3 columns use a carefully-engineered tagger. For universal tags, LDS and Word2Vec both contribute a statistically-significant gain in accuracy over the baseline (Lex). However, their difference is not significant. In the case of the original PTB tags, we find that Word2Vec achieves a statistically significant gain over LEX, but the LDS does not. We expect that our context-dependent embeddings perform as well as context-independent embeddings since the taggers have features and test-time inference for capturing non-local dependencies."}, {"heading": "7.3. Named Entity Recognition", "text": "In Table 3 we consider the effect of unsupervised token features for NER on the Conll 2003 dataset using a lexicalized tagger (Lex). We use the same LDS and Word2Vec models as in the previous section, and also compare to the Brown clusters used for NER in Ratinov & Roth (2009). As before, we find that Word2Vec and LDS provide significant accuracy improvements over the baseline. We expect that the reason the LDS does not outperform Word2Vec is that NER relies mainly on performing local pattern matching, rather than capturing long-range discourse structure."}, {"heading": "7.4. RNN initialization", "text": "As highlighted in Section 2, RNNs can provide impressive accuracy in various applications. We consider the simple RNN architecture of Sec. 6, since it permits natural initialization with an LDS and because Mikolov et al. (2014) demonstrate that small variants of it can outperform LSTMs as a language model (LM). Note that the \u2018context units\u2019 of Mikolov et al. (2014) could also be learned using our EM procedure, by restricting the parametrization of A. We leave exploration of hierarchical softmax observations (Mnih & Hinton, 2009), and other alternative architectures, for future work.\nWe evaluate the usefulness of the LDS for initializing the RNN under two criteria: (1) whether it improves the perplexity of the final model, and (2) whether it leads to faster optimization. A standard dataset for comparing language models is the Penn Treebank (PTB) (Sundermeyer et al., 2012; Pachitariu & Sahani, 2013; Mikolov et al., 2014). We first train a baseline, obtaining the same test set perplexity as Mikolov (2012), with 300 hidden dimensions. This initializes parameters randomly, with lengthscales tuned as in Mikolov (2012). Next, we use the LDS to initialize an\nRNN. In order to maintain a fair comparison v.s. the baseline, we train the LDS on the same PTB data, though in practice one should train it on a substantially larger corpus.\nWe use the popular RNN learning rate schedule where it is kept constant until performance on held-out data fails to improve, and then it is decreased geometrically until the held-out performance again fails to improve (Mikolov, 2012). We tuned both the initial value and decay rate. When initializing with the LDS, it is important to use a small learning rate, since otherwise the optimization immediately jumps far from where it started.\nIn Figure 1, we plot perplexity on the dev set v.s. the number of training epochs. The time to train the LDS, about 30 minutes, is inconsequential compared to training the RNN (4 days) on a single CPU core. LDS training on the PTB is faster than our experiments above with 1B tokens because we use a small vocabulary and run far fewer EM iterations, in order to prevent overfitting. The RNN baseline converged after 17 training epochs, while using the LDS for initialization allowed it to converge after 12, which amounts to about a day of savings on a single CPU core. Next, in Table 4 we compare the final perplexities on the dev and test sets. We find that initializing with the LDS also provides a better model. We encourage further exploration of LDS initialization for RNN LMs, partiularly using larger training sets for both steps.\nWe found that initializing the RNN with LDS parameters trained using SSID, rather than SSID+EM, performed no better than the baseline. Specifically, the best performance was obtained using a high initial learning rate, which allows gradient descent to ignore the SSID values. We expect this is because the method of moments requires lots of data, and the PTB is small. In a setting where one trains the LDS on a very large corpus, it is possible that SSID is effective. Overall, we did not explore initializing the RNN using type-level embeddings such as Word2Vec, since it is unclear how to initialize A and how to set K v.s. C."}, {"heading": "8. Conclusion and Future Work", "text": "We have contributed a scalable method for assigning word tokens context-specific low-dimensional representations. These representations, and the transition dynamics of the LDS parameters, capture useful syntactic and semantic structure. Our algorithm requires a single pass over the training data and no painful tuning of learning rates.\nNext, we will apply our LDS to challenging data such as Twitter and look to extend ASOS to new model structures. Furthermore, we will investigate improving initialization for alternative RNN architectures, including those with hierarchical softmaxes, by leveraging not just the LDS parameters, but also the LDS posterior on the training data."}, {"heading": "A. Scaling UP LDS Learning to Text", "text": "As discussed in Section 4.3, we whiten our data using\nW = \u03a8 \u2212 12 0 = diag(\u00b5 \u2212 12 1 , . . . , \u00b5 \u2212 12 V ). (20)\nBesides improving the empirical performance of SSID, working in the whitened coordinate system also simplifies various details used in Section 4 when scaling up LDS learning for text. Under this transformation, we have \u03a80 = diag(\u00b5)\u2212\u00b5\u00b5>. This simplifies various steps because our estimators (12) and (9) are of the form I \u2212 [low rank matrix], rather than diag(\u00b5)\u2212 [low rank matrix]. In the whitened coordinates, the data are orthogonal to \u00b5 12 , rather than 1.\nA.1. Recovering PSD D in SSID\nWhile SSID is consistent, for finite data the procedure is not guaranteed to yield a positive semidefinite (PSD) estimate for D, which is required because it is a covariance matrix. In our particular case, the D we seek will be singular on the span of \u00b5 1 2 , but Subspace ID will still not guarantee that D will be PSD on \u00b5 1 2 \u22a5 .\nThis is critical because if D is not PSD on this subspace, then we can not define a valid Kalman filtering procedure for the model (see Sec. A.2). However, due to the structure of our data distribution, D can easily be fixed post-hoc.\nFrom (12) we have the estimator\nD = I \u2212 \u00b5 12\u00b5 12 > \u2212 C\u03a31C> (21)\nNext, define D\u03b1 = I\u2212\u00b5 1 2\u00b5 1 2 > \u2212 (1\u2212\u03b1)C\u03a31C> and define the PSD estimator D\u2032 = D\u03b10 , where \u03b10 is the minimal value such that D\u03b1 is PSD on \u00b5 1 2 \u22a5 . We next show how to find \u03b10.\nWe have that D\u03b1 is PSD on \u00b5 1 2 \u22a5 iff the maximum eigenvalue of (1\u2212\u03b1)WC\u03a31C>W> is less than 1. This is because \u00b5 1 2 is a unit vector and we can ignore any cross terms between \u00b5 1 2\u00b5 1 2 > and (1\u2212 \u03b1)C\u03a31C> because col(C) = \u00b5 1 2 \u22a5\n, which is true because the data lies in this subspace. Therefore we can find \u03b10 using the following procedure:\n1. Find s0, the maximal eigenvalue of C\u03a31C>, using power iteration.\n2. If s0 < 1, set \u03b10 = 0. Otherwise, set \u03b10 = s0\u22121s0 .\nA.2. Efficiently Computing the Kalman Gain Matrix\nNext, recall our expression (6) for the steady state Kalman gain K = \u03a31C>S\u22121ss , which comes from solving the system\nKSss = \u03a31C >, (22)\nwhere\nSss = C\u03a31C > +D (23)\nFurthermore, note that both of our estimators for D, (12) and (9), maintain the property that \u00b5 1 2 is an eigenvector of eigenvalue 0 for D.\nSince \u00b5 1 2 is also orthogonal to col(C), we have that \u00b5 1 2 /\u2208 Col(Sss). Therefore, we cannot use (6) directly because Sss is not invertible along this direction. However, we can still solve (22) as K = \u03a31C>S+ss. This pseudoinverse can be characterized as:\nS+ss = [inversion of Sss within col(Sss)] [projection onto col(Sss)] (24)\nFurthermore, note that both estimators for D have the form that\nD = \u03a80 \u2212 (PSD, low rank, and \u22a5 \u00b5 1 2 ) (25)\n= I \u2212 \u00b5 12\u00b5 12 > \u2212 (PSD, low rank and \u22a5 \u00b5 12 ) (26)\n:= I \u2212 \u00b5 12\u00b5 12 > \u2212 L (27)\nTherefore, it remains to define the pseudoinverse of\nSss = I \u2212 \u00b5 1 2\u00b5\n1 2 >\n+ C(\u03a31 \u2212M)C>). (28)\nFurthermore, since col(L) = col(C) = \u00b5 1 2 \u22a5 , we can define L = CMC> for some positive definite M , so we consider\nSss = I \u2212 \u00b5 1 2\u00b5\n1 2 >\n+ C(\u03a31 \u2212M)C>). (29)\nObserve that\n(I + C(\u03a31 \u2212M)C>)\u22121 (30)\nis a valid inverse for Sss on \u00b5 1 2 \u22a5 . This follows from the orthogonality of \u00b5 1 2 and col(C), so we can effectively ignore the \u00b5 1 2 term in (29) when inverting it on \u00b5 1 2 \u22a5 .\nTherefore, we employ\n(Sss) + = (I + C(\u03a31 \u2212M)C>)\u22121(I \u2212 \u00b5 1 2\u00b5 1 2 ), (31)\nwhere the right term is an orthogonal projection onto \u00b5 1 2 \u22a5 .\nThe term in the inverse (31) is diagonal-plus-low-rank and can be manipulated efficiently using the matrix inversion lemma formula (53):\n(I + C(\u03a31 \u2212M)C>)\u22121 = I \u2212 C((\u03a31 \u2212M)\u22121 + C \u2032C)\u22121C>. (32)\nTherefore we can obtain K without instantiating an intermediate matrix of size V \u00d7 V .\nRecall the filtering equation (4):\nx\u0302tt = (A\u2212KCA)x\u0302t\u22121t\u22121 +Kwt.\nWe seek to avoid anyO(V ) (or worse) computation at test time when filtering. First of all, we can precompute (A\u2212KCA). For the second term, there are only V possible values for the unwhitened inputwt = w\u0303t\u2212\u00b5, so we would like to precompute KW (w\u0303t \u2212 \u00b5) for every possible value that the indicator w\u0303t can take on. Let w\u0303t = ei, we have:\nKW (w\u0303t \u2212 \u00b5) = \u03a31C>S+ssW (ei \u2212 \u00b5) (33)\n= \u03a31C >(I + C(\u03a31 \u2212M)C>)\u22121(I \u2212 \u00b5 1 2\u00b5\n1 2 >\n)(Wei \u2212 \u00b5 1 2 ) (34)\n= \u03a31C >(I + C(\u03a31 \u2212M)C>)\u22121(I \u2212 \u00b5 1 2\u00b5\n1 2 >\n)Wei (35)\n= \u03a31C >(I + C(\u03a31 \u2212M)C>)\u22121Wei (36) = [ \u03a31C >(I + C(\u03a31 \u2212M)C>)\u22121W ] i , (37)\n(38)\nIn the final line, the subscript i denotes the ith column of a matrix.\nA.3. Likelihood Computation\nSss is also used when computing the log-likelihood of input data (w1, . . . , wT ):\nLL = \u2212TV log(2\u03c0)\u2212 1 2 log det(Sss) + >\u2211 t=1 (wpredt \u2212 wt)>S\u22121ss (w pred t \u2212 wt). (39)\nHere, wpredt = CAx\u0302t, where x\u0302t is the posterior mean for xt given observations w1:(t\u22121). Sss is only invertible along \u00b5 1 2 \u22a5\n, but (wpredt \u2212 wt) varies only on this subspace, so we can effectively ignore the zero-variance direction \u00b5 1 2 . Therefore, we just use (30) as S\u22121ss in (39).\nFor the data-dependent term in our likelihood, we have:\n\u2212 1 2 >\u2211 t=1 (wpredt \u2212 wt)>S\u22121ss (w pred t \u2212 wt) (40)\n= \u22121 2 tr ( S\u22121ss Et[(w pred t \u2212 wt)(w pred t \u2212 wt)>] ) (41)\n= \u22121 2 tr ( S\u22121ss Et[(wt \u2212 CAx\u0302t)(wt \u2212 CAx\u0302t)>] ) (42) = \u22121 2 ( tr ( S\u22121ss Et[wtw>t ] ) \u2212 2tr ( S\u22121ss Et[wtx\u0302>t ]A>C> ) + tr ( S\u22121ss CAEt[x\u0302tx\u0302>t ]A>C> )) (43) = \u22121 2 ( tr ( S\u22121ss I ) \u2212 2tr ( S\u22121ss Et[wtx\u0302>t ]A>C> ) + tr ( S\u22121ss CAEt[x\u0302tx\u0302>t ]A>C> )) (44)\nNote that the Et[x\u0302tx\u0302>t ] term above is different from \u03a31, since the former is from the posterior distribution given the input data and \u03a31 is from the prior.\nThe first term can be computed using (57). The latter two terms are of the form tr ( S\u22121ss ZW >), where Z and W are both V \u00d7 k, so we can invoke (58). For the log det(Sss) term, we consider Sss only on \u00b5 1 2 \u22a5\n, so we compute \u2212 log det(S\u22121ss ), where S\u22121ss comes from (30) and we employ the formula (55)."}, {"heading": "B. Background", "text": "B.1. Non-Steady-State Kalman Filtering and Smoothing\nWe will use x\u0302\u03c4t and S \u03c4 t for the mean and variance under the posterior for xt given w1:\u03c4 . We will use x\u0304t and S T t when considering the posterior for xt given all the data w1:T . The following are the forward \u2018filtering\u2019 steps (Kalman, 1960; Ghahramani & Hinton, 1996):\nx\u0302t\u22121t = Ax\u0302 t\u22121 t\u22121 (45) St\u22121t = AS t\u22121 t\u22121A > +Q (46)\nKt = S t\u22121 t C \u2032(CSt\u22121t\u22121C > +D)\u22121 (47)\nx\u0302tt = x\u0302 t t\u22121 +Kt(wt \u2212 Cx\u0302t\u22121t ) (48) St\u22121t = S t\u22121 t \u2212KtCSt\u22121t (49)\nNext, we have the backwards \u2018smoothing\u2019 steps:\nJt\u22121 = S t\u22121 t\u22121A \u2032(St\u22121t ) \u22121 (50) x\u0304t\u22121 = x\u0302 t\u22121 t\u22121 + Jt\u22121(x\u0304 T t \u2212Ax\u0302t\u22121t\u22121) (51) STt\u22121 = S t\u22121 t\u22121 + Jt\u22121(S > t \u2212 St\u22121t )JTt\u22121 (52)\nNote that the updates for the variances S are data-independent and just depend on the parameters of the model. They will converge quickly to time-independent \u2018steady state\u2019 quantities.\nB.2. Matrix Inversion Lemma\nFollowing Press et al. (1987), we have\n(A+ USV >)\u22121 = A\u22121 \u2212A\u22121U(S\u22121 + V >A\u22121U)\u22121V >A\u22121 (53)\nand the related expression for determinants:\ndet(A+ USV >) = det(S) det(A) det(S\u22121 + V >A\u22121U). (54)\ni.e. log det(A+ USV >) = log det(S) + log det(A) + log det(S\u22121 + V >A\u22121U). (55)\nExpression (53) is useful if we already have a an inverse for A and want to efficiently compute the inverse of a lowrank perturbation of A. It is also useful in order to be able to do linear algebra using (A + USV >)\u22121 without actually instantiating a V \u00d7 V matrix, which can be unmanageable in terms of both time and space for large V . For example, let M be an V \u00d7 m matrix with m << V , then we can compute M(A + USV >)\u22121 using (53) by carefully placing our parentheses such that no V \u00d7 V matrix is required. In our application, A is diagonal, so computing its inverse is trivial. Also, note that (53) can be used recursively, if A is defined as another sum of an easily invertible matrix and a low rank matrix.\nAlong these lines, here are a few additional useful identities that follow from (53) for quantities that can be computed without V 2 time or storage. Here, we assume that both A\u22121 and tr(A\u22121) can be computed inexpensively (e.g., A is diagonal).\nFor any product XY >, where X and Y are V \u00d7 k matrices, note that we can compute tr(XY T ) in O(V k) time as\ntr(XY T ) = \u2211 i \u2211 j XijYij . (56)\nWe can use this to compute the trace of the inverse of a matrix implicitly defined via the matrix inversion lemma:\ntr [ (A+ USV >)\u22121 ] = tr(A\u22121)\u2212 tr A\u22121U(S\u22121 + V >A\u22121U)\u22121\ufe38 \ufe37\ufe37 \ufe38 X V >A\u22121\ufe38 \ufe37\ufe37 \ufe38 Y >  . (57) More generally, Let Z and W be V \u00d7 k matrices, then we compute\ntr [ (A+ USV >)\u22121ZW> ] = tr(A\u22121Z\ufe38 \ufe37\ufe37 \ufe38\nX W>\ufe38\ufe37\ufe37\ufe38 Y > )\u2212 tr A\u22121U(S\u22121 + V >A\u22121U)\u22121\ufe38 \ufe37\ufe37 \ufe38 X V >A\u22121ZW>\ufe38 \ufe37\ufe37 \ufe38 Y >  (58) We use (58) when computing the Likelihood in Section A.3."}, {"heading": "C. SSID Initialization v.s. Random Initialization", "text": "In Figure 2, we contrast the progress of EM, in terms of the log-likelihood of the training data, when initializing with SSID v.s. initializing randomly (Random). Note that the initial values of SSID and Random are nearly identical. This is due to model mispecification, and the fact that we chose the lengthscales of the random parameters post-hoc, by looking at the lengthscales of the SSID parameters. Over the course of 100 EM iterations, the model initialized with SSID climbs quickly and begins leveling out, whereas it takes a long time for the Random model to begin climbing at all. We truncate at 100 EM iterations, since we actually use the SSID-initialized model after the 50th iteration. After that, we find that local POS tagging accuracy diminished."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Low dimensional representations of words allow<lb>accurate NLP models to be trained on limited<lb>annotated data. While most representations ig-<lb>nore words\u2019 local context, a natural way to in-<lb>duce context-dependent representations is to per-<lb>form inference in a probabilistic latent-variable<lb>sequence model. Given the recent success of<lb>continuous vector space word representations,<lb>we provide such an inference procedure for con-<lb>tinuous states, where words\u2019 representations are<lb>given by the posterior mean of a linear dynam-<lb>ical system. Here, efficient inference can be<lb>performed using Kalman filtering. Our learn-<lb>ing algorithm is extremely scalable, operating<lb>on simple cooccurrence counts for both param-<lb>eter initialization using the method of moments<lb>and subsequent iterations of EM. In our exper-<lb>iments, we employ our inferred word embed-<lb>dings as features in standard tagging tasks, ob-<lb>taining significant accuracy improvements. Fi-<lb>nally, the Kalman filter updates can be seen as a<lb>linear recurrent neural network. We demonstrate<lb>that using the parameters of our model to ini-<lb>tialize a non-linear recurrent neural network lan-<lb>guage model reduces its training time by a day<lb>and yields lower perplexity.", "creator": "LaTeX with hyperref package"}}}