{"id": "1502.04617", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2015", "title": "Deep Transform: Error Correction via Probabilistic Re-Synthesis", "abstract": "Errors in data are usually unwelcome and so some means to correct them is useful. However, it is difficult to define, detect or correct errors in an unsupervised way. Here, we train a deep neural network to re-synthesize its inputs at its output layer for a given class of data (we've been able to detect and detect this with a real-time representation of this output in a single layer). The model is used to estimate the errors in a given class of data (in the example above), then apply these input to it using the model over time.\n\n\nIn a test, it was tested on a group of 5,000 students, but the error rate was low compared to the actual errors in the study, which had an average of 2.5% error rate. That said, the network has been very effective, and we now expect to improve it significantly in a future edition of The Computer Science and Artificial Intelligence Magazine (CISA).", "histories": [["v1", "Mon, 16 Feb 2015 16:41:26 GMT  (197kb)", "http://arxiv.org/abs/1502.04617v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrew j r simpson"], "accepted": false, "id": "1502.04617"}, "pdf": {"name": "1502.04617.pdf", "metadata": {"source": "CRF", "title": "Deep Transform: Error Correction via Probabilistic Re-Synthesis", "authors": ["Andrew J.R. Simpson"], "emails": ["Andrew.Simpson@Surrey.ac.uk"], "sections": [{"heading": null, "text": "means to correct them is useful. However, it is difficult to define, detect or correct errors in an unsupervised way. Here, we train a deep neural network to re-synthesize its inputs at its output layer for a given class of data. We then exploit the fact that this abstract transformation, which we call a deep transform (DT), inherently rejects information (errors) existing outside of the abstract feature space. Using the DT to perform probabilistic resynthesis, we demonstrate the recovery of data that has been subject to extreme degradation.\nIndex terms\u2014Deep learning, unsupervised learning, neural\nnetworks, error correction, deep transform.\nI. INTRODUCTION\nThere are many situations in which errors are introduced into data. For example, errors may be introduced during transmission or storage of data. These errors are usually unwelcome because they make the data less useful by distorting or losing information. Therefore, some means to correct the errors is useful. Even more useful would be some unsupervised means to correct errors that would exploit prior knowledge about the scope of what is represented by the data.\nWithout prior knowledge, it is impossible to know what component of some given data might be error. For example, a list of binary digits might contain errors but since the errors are also binary digits it is impossible to know whether a given digit is an error or not. However, if there is a known degree of abstraction, the situation is different. For example, given a list of phone numbers comprising sequences of integers, it would be possible to detect those integers which are greater than 9 or less than 0. Further knowledge, e.g., of country codes, would allow more abstract errors to be detected. Thus, the greater the degree of known abstraction, the more powerful the detection of error will be. In this case, we may think of the abstractions as features.\nDeep neural networks [1], [2], [3], [4], [5] (DNN) learn abstract feature representations from data [6]. A DNN that is trained to replicate its inputs at its output layer provides a oneto-one mapping and is known as an autoencoder [2]. Autoencoders implicitly learn both abstract encoding and decoding (i.e., synthesis). Once trained, we may think of the autoencoder as an abstract transformation device \u2013 a deep\ntransform (DT) \u2013 that embodies abstract knowledge of the data. The DT is therefore capable of transforming new data through the learned abstract feature space, constituting an abstract filter. Critically, if the error is to pass through the DT (i.e., by being re-synthesized at the output) it must be encoded with the same abstract coding scheme. Thus, in principle, the DT may be employed as an abstract filter to reject error. Furthermore, the DT may therefore also be used to synthesize probabilistic corrections for errors in an unsupervised manner.\nIn this paper, we trained a DNN autoencoder on data that were free of errors and then used the resulting model as a transformation device (i.e., a DT). We then used this DT to probabilistically re-synthesize data that had been degraded by having a random proportion replaced with random data. We then trained a classifier DNN on the same un-degraded training data and demonstrate the recovery of separate test data from serious error using the DT to perform probabilistic re-synthesis.\nII. METHOD\nBy way of case study, we consider the problem of image classification for images that have been arbitrarily degraded by errors. We chose the well-known MNIST hand-written digit classification problem [3], [1], [4] and trained a typical feed-forward DNN to classify the digits. We then quantified the sensitivity of the trained classifier to error by testing the trained model on data that had been subjected to various degrees of nonlinear (non-additive) error. Next, to obtain a DT, we trained an autoencoder DNN on the same hand-written digit images. We then used the DT to re-synthesize each degraded image a number of times, each time with further random errors added, and averaged the result. Finally, to capture the degree of recovery from error, we tested the same classifier model on the re-synthesized (i.e., corrected) images.\nFor the input layer to our networks we unpacked the images of 28x28 pixels into vectors of length 784 (28x28 = 784). Pixel intensities were normalized to zero mean. We constructed two feed-forward DNNs; an autoencoder for use as DT and a classifier. Both networks used biased sigmoid activation functions [6]. The classifier network was of size 784x100x10 units, with a 10-unit softmax output layer\n(corresponding to the 10-way classification problem). The autoencoder was of size 784x1500x784 units and featured a sigmoidal output layer. The autoencoder was trained to replicate its own inputs at its output layer.\nBoth models were independently trained on the 60,000 training examples from the MNIST dataset [4] using stochastic gradient descent (SGD). The models were trained for 10 iterations each. Each iteration of SGD consisted of a complete sweep of the entire training data set. Models were trained without dropout.\nDegradation. In order to degrade the images for the testing process, a certain proportion of pixels from each of the 10,000 separate test images was randomly chosen and replaced with a random intensity selected from a distribution of equivalent mean and standard deviation to the images. Images were degraded at levels between 0 and 100% (of the pixels) at intervals of 10. Fig. 1a provides examples of digit images and images degraded to a level of 75% replacement. Note that this is not the same as additive noise.\nProbabilistic Re-Synthesis via DT. Each degraded image was transformed using the autoencoder 100 times. Prior to each of the 100 separate transforms, 50% of the pixels of the degraded image (chosen at random) were replaced with random intensity values. I.e., each time a degraded image was further degraded, but these secondary degradations were different for each transform. The activation of the output layer of the autoencoder was taken as re-synthesized image. The resulting distribution, of 100 re-synthesized instances of each degraded image, was then averaged to provide an errorcorrected image. In order to account for neurons in the output layer that were invariantly active, all error-corrected images (i.e., of the entire test set) were then averaged and the result subtracted from each image.\nThe same classifier was then separately tested on the both the degraded images and the error corrected images (without any additional training). Note that at 100% degradation the entire image is replaced with random noise. Hence, classifier accuracy is bounded by a ceiling of 10% chance level.\nIII. RESULTS\nFig. 1a provides example images from the MNIST dataset in their original form (left), after 75% degradation (middle) and after recovery via DT probabilistic re-synthesis (right). Even with exact knowledge of what is represented in the degraded images, it is difficult or impossible to visually identify the digits. In contrast, the recovered images are clear and true to the original classification. However, note that the recovered images are not identical to the originals in shape and form. Indeed, it looks rather like the original images have been enhanced, suggesting the action of abstract re-synthesis.\nThis impressive recovery is reflected in the classification accuracy difference before and after DT probabilistic resynthesis. Fig. 1b plots classification error as a function of degradation [%] for the classifier without DT probabilistic resynthesis and for the classifier with DT probabilistic resynthesis. In the degradation range between 20 and 90 % there\nis a large improvement for the classifier using DT probabilistic re-synthesis. This means that the classifier is far more robust to error when the degraded data are pre-processed with the DT probabilistic re-synthesis. Indeed, at a degradation rate of 70%, the classifier has an error rate of nearly 54% whereas with the error-corrected data the same classifier has an error rate of around 18%. In summary, we have demonstrated recovery from errors via DT probabilistic re-synthesis.\nIn order to provide a more practical demonstration of recovery from more typical errors (e.g., from network packet loss during broadcast or transmission), we used the same DT probabilistic re-synthesis to correct images that had been subject to large (~12% of the pixels) continuous regions being removed (zeroed) from a central (i.e., important) region in the image. Fig. 2a shows the original images, the images with missing regions and the images recovered using DT probabilistic re-synthesis. The resulting large gaps in the flowing strokes of each hand-written digit are incongruous. In the recovered images, these gaps have been replaced with smooth, flowing lines that are consistent with the hand-written qualities of the original. In the case of the \u20185\u2019, the upper\nvertical line has actually been adjusted towards a more archetypal \u20185\u2019. Thus, our DT probabilistic re-synthesis not only corrects errors at the level of missing pixels but also appears to correct stylistic mistakes of the original writer. In other words, the DT probabilistic re-synthesis appears to be capable of correcting abstract errors introduced by the original writer.\nTo further illustrate this facility for abstract error correction, we took characters (not digits) from the Verdana Font family, rendered at the same 28x28 resolution, and performed the same DT probabilistic re-synthesis. The resulting images, and the original images, are shown in Fig. 2b. While there is no apriori notion of error in the Verdana font, the abstract error (from the perspective of the model trained on hand-written digits) is a lack of hand-written qualities. The re-synthesized images have a hand-written quality that suggests \u2018correction\u2019 of the type-set style of the font.\nIV. DISCUSSION AND CONCLUSION\nIn this paper, we have interpreted the DNN autoencoder as an abstract transformation device. We call this a deep transform because it is an abstract transformation learned by a deep neural network [6]. We have shown that this DT can be used for probabilistic re-synthesis and that this procedure allows data to be recovered from extreme error. This point was illustrated by contrasting the classification performance, for the same trained classifier, when making predictions about the degraded and corrected data. Indeed, this seems more remarkable given that the classifier was not re-trained to either account for, or to exploit, any training data that had been resynthesized.\nWe have also demonstrated, at a more intuitive level, that DT probabilistic re-synthesis can correct abstract errors and even affect stylistic changes. Thus, it would appear that there is great potential for DT probabilistic re-synthesis to perform similarly in various related tasks. For example, we have shown that typeset font characters can be transformed into characters that have hand-written qualities. This demonstrates that abstract qualities of the hand-written digits learned (by the autoencoder) may be applied to re-synthesize letters of the alphabet that were never encountered in the training stage.\nMore generally, the outcome of the DT probabilistic resynthesis demonstrated here resembles the perceptual error correction facilities of the brain. Indeed, our demonstration of replacement of large regions of missing digit appears consistent with the broadly equivalent auditory perceptual phenomenon of \u2018missing phoneme replacement\u2019. Hence, our findings may provide insight into the exceptionally robust perceptual system of the brain and its illusions.\nACKNOWLEDGMENT\nAJRS did this work on the weekends and was supported by his wife and children.\nREFERENCES [1] Hinton G. E., Osindero S., Teh Y. (2006). \u201cA fast learning algorithm\nfor deep belief nets\u201d, Neural Computation 18: 1527\u20131554. [2] Bengio Y (2009) Learning deep architectures for AI, Foundations and\nTrends in Machine Learning 2:1\u2013127. [3] LeCun Y., Bottou L., Bengio Y., Haffner P. (1998) Gradient-based\nlearning applied to document recognition. Proc. IEEE 86: 2278\u20132324. [4] Ciresan, C. D., Meier U., Gambardella L. M., Schmidhuber J. (2010)\n\u201cDeep Big Simple Neural Nets Excel on Handwritten Digit Recognition\u201d, Neural Computation 22: 3207\u20133220. [5] Hinton G. E., Srivastava N., Krizhevsky A., Sutskever I., Salakhutdinov R. (2012) \u201cImproving neural networks by preventing co-adaptation of feature detectors,\u201d The Computing Research Repository (CoRR), abs/1207.0580. [6] Simpson AJR (2015) Abstract Learning via Demodulation in a Deep Neural Network, arxiv.org abs/1502.04042 [7] Simpson AJR (2015) Over-Sampling in a Deep Neural Network, arxiv.org abs/1502.03648\n."}], "references": [{"title": "A fast learning algorithm for deep belief nets", "author": ["E. Hinton G", "S. Osindero", "Y. Teh"], "venue": "Neural Computation", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Learning deep architectures for AI, Foundations and Trends in Machine Learning 2:1\u2013127", "author": ["Y Bengio"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition", "author": ["Ciresan C. D", "U. Meier", "M. Gambardella L", "J. Schmidhuber"], "venue": "Neural Computation", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors,", "author": ["E. Hinton G", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Computing Research Repository (CoRR),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Abstract Learning via Demodulation in a Deep Neural Network, arxiv.org", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Over-Sampling in a Deep Neural Network, arxiv.org abs/1502.03648", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks [1], [2], [3], [4], [5] (DNN) learn abstract feature representations from data [6].", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "Deep neural networks [1], [2], [3], [4], [5] (DNN) learn abstract feature representations from data [6].", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "Deep neural networks [1], [2], [3], [4], [5] (DNN) learn abstract feature representations from data [6].", "startOffset": 31, "endOffset": 34}, {"referenceID": 3, "context": "Deep neural networks [1], [2], [3], [4], [5] (DNN) learn abstract feature representations from data [6].", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": "Deep neural networks [1], [2], [3], [4], [5] (DNN) learn abstract feature representations from data [6].", "startOffset": 41, "endOffset": 44}, {"referenceID": 5, "context": "Deep neural networks [1], [2], [3], [4], [5] (DNN) learn abstract feature representations from data [6].", "startOffset": 100, "endOffset": 103}, {"referenceID": 1, "context": "A DNN that is trained to replicate its inputs at its output layer provides a oneto-one mapping and is known as an autoencoder [2].", "startOffset": 126, "endOffset": 129}, {"referenceID": 2, "context": "We chose the well-known MNIST hand-written digit classification problem [3], [1], [4] and trained a typical feed-forward DNN to classify the digits.", "startOffset": 72, "endOffset": 75}, {"referenceID": 0, "context": "We chose the well-known MNIST hand-written digit classification problem [3], [1], [4] and trained a typical feed-forward DNN to classify the digits.", "startOffset": 77, "endOffset": 80}, {"referenceID": 3, "context": "We chose the well-known MNIST hand-written digit classification problem [3], [1], [4] and trained a typical feed-forward DNN to classify the digits.", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "Both networks used biased sigmoid activation functions [6].", "startOffset": 55, "endOffset": 58}, {"referenceID": 3, "context": "Both models were independently trained on the 60,000 training examples from the MNIST dataset [4] using stochastic gradient descent (SGD).", "startOffset": 94, "endOffset": 97}, {"referenceID": 5, "context": "We call this a deep transform because it is an abstract transformation learned by a deep neural network [6].", "startOffset": 104, "endOffset": 107}], "year": 2015, "abstractText": "Errors in data are usually unwelcome and so some means to correct them is useful. However, it is difficult to define, detect or correct errors in an unsupervised way. Here, we train a deep neural network to re-synthesize its inputs at its output layer for a given class of data. We then exploit the fact that this abstract transformation, which we call a deep transform (DT), inherently rejects information (errors) existing outside of the abstract feature space. Using the DT to perform probabilistic resynthesis, we demonstrate the recovery of data that has been subject to extreme degradation.", "creator": "PDFCreator Version 1.7.1"}}}