{"id": "1502.05477", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Feb-2015", "title": "Trust Region Policy Optimization", "abstract": "We propose a family of trust region policy optimization (TRPO) algorithms for learning control policies. We first develop a policy update scheme with guaranteed monotonic improvement, and then we describe a finite-sample approximation to this scheme that is practical for large-scale problems. In our experiments, we evaluate the method on two different and very challenging sets of tasks: learning simulated robotic swimming, hopping, and walking gaits, and playing Atari games using images of the screen as input. For these tasks, the policies are neural networks with tens of thousands of parameters, mapping from observations to actions. These can be set in the following parameters: Learning virtual swimming, hopping, and walking gaits in a series of tasks: learning simulated training using random model training, and learning simulated video games using machine learning. The first task involves the learning of a simulated human swimming. The second task involves the training of a simulated robot swimming, and learning simulated video games using random model training. The third task involves the training of a simulated human swimming, and learning simulated video games using a machine learning algorithm. The fourth task involves the training of a simulated human swimming, and learning simulated video games using machine learning. The fifth task involves the training of a simulated human swimming, and learning simulated video games using a machine learning algorithm. Finally, we consider the optimization of training on the training of the robots on the training of a simulated human swimming, and training the training of a simulated human swimming. To evaluate the best performance, we first evaluate the learning of a simulated human swimming. The next task involves the training of a simulated human swimming, and training the training of a simulated human swimming, and training the training of a simulated human swimming. Finally, we consider the optimization of training on the training of a simulated human swimming, and training the training of a simulated human swimming. Finally, we assess the optimization of training on the training of a simulated human swimming. We test the prediction that the prediction of the best performance will be based on the simulation of a simulated human swimming, and test the prediction of the best performance will be based on the simulation of a simulated human swimming, and test the prediction of the best performance will be based on the simulation of a simulated human swimming. We conclude with a single, and limited, hypothesis. We hope to see more of these approaches developed by researchers with deep neural networks.", "histories": [["v1", "Thu, 19 Feb 2015 06:44:25 GMT  (547kb,D)", "http://arxiv.org/abs/1502.05477v1", "16 pages"], ["v2", "Mon, 18 May 2015 14:56:50 GMT  (540kb,D)", "http://arxiv.org/abs/1502.05477v2", "16 pages, ICML 2015"], ["v3", "Mon, 8 Jun 2015 10:47:03 GMT  (540kb,D)", "http://arxiv.org/abs/1502.05477v3", "16 pages, ICML 2015"], ["v4", "Mon, 6 Jun 2016 01:00:57 GMT  (541kb,D)", "http://arxiv.org/abs/1502.05477v4", "16 pages, ICML 2015"], ["v5", "Thu, 20 Apr 2017 18:04:12 GMT  (541kb,D)", "http://arxiv.org/abs/1502.05477v5", "16 pages, ICML 2015"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["john schulman", "sergey levine", "pieter abbeel", "michael i jordan", "philipp moritz"], "accepted": true, "id": "1502.05477"}, "pdf": {"name": "1502.05477.pdf", "metadata": {"source": "META", "title": "Trust Region Policy Optimization", "authors": ["John Schulman"], "emails": ["joschu@eecs.berkeley.edu", "slevine@eecs.berkeley.edu", "pcmoritz@eecs.berkeley.edu", "jordan@cs.berkeley.edu", "pabbeel@cs.berkeley.edu"], "sections": [{"heading": "1. Introduction", "text": "Most algorithms for policy optimization can be classified into three broad categories: policy iteration methods, which alternate between estimating the value function under the current policy and improving the policy (Bertsekas, 2005); policy gradient methods, which use an estimator of the gradient of the expected cost obtained from sample trajectories (Peters & Schaal, 2008a) (and which, as we later discuss, have a close connection to policy iteration); and derivative-free optimization methods, such as the cross-entropy method (CEM) and covariance matrix adaptation (CMA), which treat the cost as a black box function to be optimized in terms of the policy parameters (Fu et al., 2005; Szita & Lo\u0308rincz, 2006).\nGeneral derivative-free stochastic optimization methods such as CEM and CMA are preferred on many problems, because they achieve good results while being simple to understand and implement. For example, while Tetris is a classic benchmark problem for approximate dynamic programming (ADP) methods,\nstochastic optimization methods are difficult to beat on this task (Gabillon et al., 2013). For continuous control problems, methods like CMA have been successful at learning control policies for challenging tasks like locomotion when provided with hand-engineered policy classes with low-dimensional parameterizations (Wampler & Popovic\u0301, 2009). The inability of ADP and gradient-based methods to consistently beat gradientfree random search is unsatisfying, since gradientbased optimization algorithms enjoy much better sample complexity guarantees than gradient-free methods (Nemirovski, 2005). Continuous gradient-based optimization has been very successful at learning function approximators for supervised learning tasks with huge numbers of parameters, and extending their success to reinforcement learning would allow for efficient training of complex and powerful policies.\nIn this paper, we propose a family of trust region policy optimization (TRPO) algorithms. TRPO methods are motivated by theoretical results that show that minimizing a certain surrogate loss function guarantees policy improvement with non-trivial step sizes. A practical approximation to this policy improvement procedure can efficiently optimize nonlinear policies with tens of thousands of parameters, which have previously posed a major challenge for model-free policy search (Deisenroth et al., 2013). In our experiments, we show that the same TRPO method can learn complex policies for swimming, hopping, and walking, as well as playing Atari games directly from raw images."}, {"heading": "2. Preliminaries", "text": "Consider an infinite-horizon discounted Markov decision process (MDP), defined by the tuple (S,A, P, c, \u03c10, \u03b3), where S is a finite set of states, A is a finite set of actions, P : S \u00d7 A \u00d7 S \u2192 R is the transition probability distribution, c : S \u2192 R is the cost function, \u03c10 : S \u2192 R is the distribution of the initial state s0, and \u03b3 \u2208 (0, 1) is the discount factor.\nar X\niv :1\n50 2.\n05 47\n7v 1\n[ cs\n.L G\n] 1\n9 Fe\nb 20\nLet \u03c0 denote a stochastic policy \u03c0 : S \u00d7 A \u2192 [0, 1], and let \u03b7(\u03c0) denote its expected discounted cost:\n\u03b7(\u03c0) = Es0,a0,... [ \u221e\u2211 t=0 \u03b3tc(st) ] , where s0 \u223c \u03c10(s0), at \u223c \u03c0(\u00b7|st), st+1 \u223c P (st+1|st, at).\nWe will use the following standard definitions of the state-action value function Q\u03c0, the value function V\u03c0, and the advantage function A\u03c0:\nQ\u03c0(st, at) = Est+1,at+1,... [ \u221e\u2211 l=0 \u03b3lc(st+l) ] ,\nV\u03c0(st) = Eat,st+1,... [ \u221e\u2211 l=0 \u03b3lc(st+l) ] , A\u03c0(s, a) = Q\u03c0(s, a)\u2212 V\u03c0(s), where at \u223c \u03c0(\u00b7|st), st+1 \u223c P (st+1|st, at) for t \u2265 0.\nThe following useful identity expresses the expected cost of another policy \u03c0\u0303 in terms of the advantage over \u03c0, accumulated over timesteps (see Kakade & Langford (2002) for the proof, which we also reprise in Appendix A using the notation in this paper):\n\u03b7(\u03c0\u0303) = \u03b7(\u03c0) + Es0,a0,s1,a1,... [ \u221e\u2211 t=0 \u03b3tA\u03c0(st, at) ] , where\ns0 \u223c\u03c10(s0), at \u223c \u03c0\u0303(\u00b7|st), st+1 \u223cP (st+1|st, at). (1)\nLet \u03c1\u03c0 be the (unnormalized) discounted visitation frequencies\n\u03c1\u03c0(s)=(P (s0 = s)+\u03b3P (s1 = s)+\u03b3 2P (s2 = s)+. . . ),\nwhere s0 \u223c \u03c10 and the actions are chosen according to \u03c0. Rearranging Equation (1) to sum over states instead of timesteps, we obtain\n\u03b7(\u03c0\u0303) = \u03b7(\u03c0) + \u2211 s \u03c1\u03c0\u0303(s) \u2211 a \u03c0\u0303(a|s)A\u03c0(s, a). (2)\nThis equation implies that any policy update \u03c0 \u2192 \u03c0\u0303 that has a non-positive expected advantage at every state s, i.e., \u2211 a \u03c0\u0303(a|s)A\u03c0(s, a) \u2264 0, is guaranteed to reduce \u03b7, or leave it constant in the case that the expected advantage is zero everywhere. This implies the classic result that the update performed by exact policy iteration, which uses the deterministic policy \u03c0\u0303(s) = arg minaA\u03c0(s, a), improves the policy if there is at least one state-action pair with a negative advantage value and nonzero state visitation probability (otherwise it has converged). However, in the approximate setting, it will typically be unavoidable, due to estimation and approximation error, that there will be\nsome states s for which the expected advantage is positive (i.e., bad), that is, \u2211 a \u03c0\u0303(a|s)A\u03c0(s, a) > 0. The complex dependency of \u03c1\u03c0\u0303(s) on \u03c0\u0303 makes Equation (2) difficult to optimize directly. Instead, we introduce the following local approximation to \u03b7:\nL\u03c0(\u03c0\u0303) = \u03b7(\u03c0) + \u2211 s \u03c1\u03c0(s) \u2211 a \u03c0\u0303(a|s)A\u03c0(s, a). (3)\nNote that L\u03c0 uses the visitation frequency \u03c1\u03c0 rather than \u03c1\u03c0\u0303, ignoring changes in state visitation density due to changes in the policy. However, if we have a parameterized policy \u03c0\u03b8, where \u03c0\u03b8(a|s) is a differentiable function of the parameter vector \u03b8, then L\u03c0 matches \u03b7 to first order. That is, for any parameter value \u03b80,\nL\u03c0\u03b80 (\u03c0\u03b80) = \u03b7(\u03c0\u03b80), \u2207\u03b8L\u03c0\u03b80 (\u03c0\u03b8) \u2223\u2223 \u03b8=\u03b80 = \u2207\u03b8\u03b7(\u03c0\u03b8) \u2223\u2223 \u03b8=\u03b80 . (4)\nEquation (4) implies that a sufficiently small step \u03c0\u03b80 \u2192 \u03c0\u0303 that reduces L\u03c0\u03b8old will also reduce \u03b7, but does not give us any guidance on how big of a step to take. To address this issue, Kakade & Langford (2002) proposed a policy updating scheme called conservative policy iteration, for which they could provide explicit lower bounds on the improvement of \u03b7.\nTo define the conservative policy iteration update, let \u03c0old denote the current policy, and assume that we can solve \u03c0\u2032 = arg min\u03c0\u2032 L\u03c0old(\u03c0\n\u2032). The new policy \u03c0new is taken to be the following mixture policy:\n\u03c0new(a|s) = (1\u2212 \u03b1)\u03c0old(a|s) + \u03b1\u03c0\u2032(a|s) (5)\nKakade and Langford proved the following result for this update:\n\u03b7(\u03c0new)\u2264L\u03c0old(\u03c0new)+ 2 \u03b3\n(1\u2212 \u03b3(1\u2212 \u03b1))(1\u2212 \u03b3) \u03b12, (6)\nwhere is the maximum advantage (positive or negative) of \u03c0\u2032 relative to \u03c0:\n= max s |Ea\u223c\u03c0\u2032(a|s) [A\u03c0(s, a)]| (7)\nSince \u03b1, \u03b3 \u2208 [0, 1], Equation (6) implies the following simpler bound, which we refer to in the next section:\n\u03b7(\u03c0new) \u2264 L\u03c0old(\u03c0new) + 2 \u03b3\n(1\u2212 \u03b3)2 \u03b12. (8)\nThis bound is only slightly weaker when \u03b1 1, which is typically the case in the conservative policy iteration method of Kakade & Langford (2002). Note, however, that so far this bound only applies to mixture policies generated by Equation (5). This policy class is unwieldy and restrictive in practice, and it is desirable for a practical policy update scheme to be applicable to all general stochastic policy classes."}, {"heading": "3. Monotonic Improvement Guarantee for General Stochastic Policies", "text": "Equation (6), which applies to conservative policy iteration, implies that a policy update that improves the right-hand side is guaranteed to improve the true expected cost objective \u03b7. Our principal theoretical result is that the policy improvement bound in Equation (6) can be extended to general stochastic policies, rather than just mixture polices, by replacing \u03b1 with a distance measure between \u03c0 and \u03c0\u0303. Since mixture policies are rarely used in practice, this result is crucial for extending the improvement guarantee to practical problems. The particular distance measure we use is the total variation divergence, which is defined by DTV (p \u2016 q) = 12 \u2211 i|pi \u2212 qi| for discrete probability distributions p, q.1 Define DmaxTV (\u03c0, \u03c0\u0303) as\nDmaxTV (\u03c0, \u03c0\u0303) = max s DTV (\u03c0(\u00b7|s) \u2016 \u03c0\u0303(\u00b7|s)). (9)\nTheorem 1. Let \u03b1 = DmaxTV (\u03c0old, \u03c0new), and let = maxs maxa|A\u03c0(s, a)|. Then Equation (8) holds.\nWe provide two proofs in the appendix. The first proof extends Kakade and Langford\u2019s result using the fact that the random variables from two distributions with total variation divergence less than \u03b1 can be coupled, so that they are equal with probability 1\u2212\u03b1. The second proof uses perturbation theory to prove a slightly stronger version of Equation (8), with a more favorable definition of that depends on \u03c0\u0303.\nNext, we note the following relationship between the total variation divergence and the KL divergence (Pollard (2000), Ch. 3): DTV (p \u2016 q)2 \u2264 DKL(p \u2016 q). Let DmaxKL (\u03c0, \u03c0\u0303) = maxsDKL(\u03c0(\u00b7|s) \u2016 \u03c0\u0303(\u00b7|s)). The following bound then follows directly from Equation (8):\n\u03b7(\u03c0\u0303) \u2264 L\u03c0(\u03c0\u0303) + CDmaxKL (\u03c0, \u03c0\u0303), where C = 2 \u03b3\n(1\u2212 \u03b3)2 .\n(10)\nAlgorithm 1 describes an approximate policy iteration scheme based on the policy improvement bound in Equation (10). Note that for now, we assume exact evaluation of the advantage values A\u03c0.\nIt follows from Equation (10) that Algorithm 1 is guaranteed to generate a sequence of monotonically improving policies \u03b7(\u03c00) \u2265 \u03b7(\u03c01) \u2265 \u03b7(\u03c02) \u2265 . . . . To see this, let Mi(\u03c0) = L\u03c0i(\u03c0) + CD max KL (\u03c0i, \u03c0). Then\n\u03b7(\u03c0i+1) \u2264Mi(\u03c0i+1) by Equation (10) \u03b7(\u03c0i) = Mi(\u03c0i), therefore,\n\u03b7(\u03c0i+1)\u2212 \u03b7(\u03c0i) \u2264Mi(\u03c0i+1)\u2212M(\u03c0i). (11) 1Our result is straightforward to extend to continuous\nstates and actions by replacing the sums with integrals.\nAlgorithm 1 Approximate policy iteration algorithm guaranteeing non-increasing expected cost \u03b7\nInitialize \u03c00. for i = 0, 1, 2, . . . until convergence do\nCompute all advantage values A\u03c0i(s, a). Solve the constrained optimization problem\n\u03c0i+1 = arg min \u03c0\n[ L\u03c0i(\u03c0) + ( 2 \u03b3\n(1\u2212 \u03b3)2\n) DmaxKL (\u03c0i, \u03c0) ] where = max\ns max a |A\u03c0(s, a)| and L\u03c0i(\u03c0)=\u03b7(\u03c0i)+ \u2211 s \u03c1\u03c0i(s) \u2211 a \u03c0(a|s)A\u03c0i(s, a)\nend for\nThus, by minimizing Mi at each iteration, we guarantee that the true objective \u03b7 is non-increasing. This algorithm is a type of majorization-minimization algorithm (Hunter & Lange, 2004), which is a class of methods that also includes expectation maximization. In the terminology of MM algorithms, Mi is the surrogate function that majorizes \u03b7 with equality at \u03c0i. This algorithm is also reminiscent of proximal gradient methods and mirror descent.\nTrust region policy optimization, which we propose in the following section, is an approximation to Algorithm 1, which uses a constraint on the KL divergence rather than a penalty to robustly allow large updates."}, {"heading": "4. Optimization of Parameterized Policies", "text": "In the previous section, we considered the policy optimization problem independently of the parameterization of \u03c0 and under the assumption that the policy can be evaluated at all states. We now describe how to derive a practical algorithm from these theoretical foundations, under finite sample counts and arbitrary parameterizations.\nSince we consider parameterized policies \u03c0\u03b8(a|s) with parameter vector \u03b8, we will overload our previous notation to use functions of \u03b8 rather than \u03c0, e.g. \u03b7(\u03b8) := \u03b7(\u03c0\u03b8), L\u03b8(\u03b8\u0303) := L\u03c0\u03b8 (\u03c0\u03b8\u0303), and DKL(\u03b8 \u2016 \u03b8\u0303) := DKL(\u03c0\u03b8 \u2016 \u03c0\u03b8\u0303). We will use \u03b8old to denote the previous policy parameters that we wish to improve upon.\nThe preceding section shows that \u03b7(\u03b8) \u2264 L\u03b8old(\u03b8) + CDmaxKL (\u03b8old, \u03b8), with equality at \u03b8 = \u03b8old. Thus, by performing the following minimization, we are guaranteed to improve the true objective \u03b7:\nminimize \u03b8\n[CDmaxKL (\u03b8old, \u03b8) + L\u03b8old(\u03b8)] .\nIn practice, if we used the penalty coefficient C recommended by the theory above, the step sizes would be very small. One way to take larger steps in a robust way is to use a constraint on the KL divergence between the new policy and the old policy, i.e., a trust region constraint:\nminimize \u03b8 L\u03b8old(\u03b8) (12)\nsubject to DmaxKL (\u03b8old, \u03b8) \u2264 \u03b4.\nThis problem imposes a constraint that the KL divergence is bounded at every point in the state space. While it is motivated by the theory, this problem is impractical to solve due to the large number of constraints. Instead, we can use a heuristic approximation which considers the average KL divergence:\nD \u03c1\nKL(\u03b81, \u03b82) := Es\u223c\u03c1 [DKL(\u03c0\u03b81(\u00b7|s) \u2016 \u03c0\u03b82(\u00b7|s))] .\nWe therefore propose solving the following optimization problem to generate a policy update:\nminimize \u03b8 L\u03b8old(\u03b8) (13)\nsubject to D \u03c1\u03b8old KL (\u03b8old, \u03b8) \u2264 \u03b4.\nSimilar policy updates have been proposed in prior work (Bagnell & Schneider, 2003; Peters & Schaal, 2008b; Peters et al., 2010), and we compare our approach to prior methods in Section 7 and in the experiments in Section 8. Our experiments also show that this type of constrained update has similar empirical performance to the maximum KL divergence constraint in Equation (12)."}, {"heading": "5. Sample-Based Estimation of the Objective and Constraint", "text": "The previous section proposed a constrained optimization problem on the policy parameters (Equation (13)), which optimizes an estimate of the expected cost \u03b7 subject to a constraint on the change in the policy at each update. This section describes how the objective and constraint functions can be approximated using Monte Carlo simulation.\nWe seek to solve the following optimization problem, obtained by expanding L\u03b8old in Equation (13):\nminimize \u03b8 \u2211 s \u03c1\u03b8old(s) \u2211 a \u03c0\u03b8(a|s)A\u03b8old(s, a)\nsubject to D \u03c1\u03b8old KL (\u03b8old, \u03b8) \u2264 \u03b4. (14)\nWe first replace \u2211 s \u03c1\u03b8old(s) [. . . ] in the objective by the expectation 11\u2212\u03b3Es\u223c\u03c1\u03b8old [. . . ]. Next, we replace\nthe advantage values A\u03b8old by the Q-values Q\u03b8old in Equation (14), which only changes the objective by a constant. Last, we replace the sum over the actions by an importance sampling estimator. Using q to denote the sampling distribution, the contribution of a single sn to the loss function is\u2211 a \u03c0\u03b8(a|sn)A\u03b8old(sn, a) = Ea\u223cq [ \u03c0\u03b8(a|sn) q(a|sn) A\u03b8old(sn, a) ] .\nOur optimization problem in Equation (14) is exactly equivalent to the following one, written in terms of expectations:\nminimize \u03b8 Es\u223c\u03c1\u03b8old ,a\u223cq [ \u03c0\u03b8(a|s) q(a|s) Q\u03b8old(s, a) ] (15)\nsubject to Es\u223c\u03c1\u03b8old [DKL(\u03c0\u03b8old(\u00b7|s) \u2016 \u03c0\u03b8(\u00b7|s))] \u2264 \u03b4.\nAll that remains is to replace the expectations by sample averages and replace the Q value by an empirical estimate. The following sections describe two different schemes for performing this estimation.\nThe first sampling scheme, which we call single path, is the one that is typically used for policy gradient estimation (Bartlett & Baxter, 2011), and is based on sampling individual trajectories. The second scheme, which we call vine, involves constructing a rollout set and then performing multiple actions from each state in the rollout set. This method has mostly been explored in the context of policy iteration methods (Lagoudakis & Parr, 2003; Gabillon et al., 2013)."}, {"heading": "5.1. Single Path", "text": "In this estimation procedure, we collect a sequence of states by sampling s0 \u223c \u03c10 and then simulating the policy \u03c0\u03b8old for some number of timesteps to generate a trajectory s0, a0, s1, a1, . . . , sT\u22121, aT\u22121, sT . Hence, q(a|s) = \u03c0\u03b8old(a|s). Q\u03b8old(s, a) is computed at each state-action pair (st, at) by taking the discounted sum of future costs along the trajectory."}, {"heading": "5.2. Vine", "text": "In this estimation procedure, we first sample s0 \u223c \u03c10 and simulate the policy \u03c0\u03b8i to generate a number of trajectories. We then choose a subset of N states along these trajectories, denoted s1, s2, . . . , sN , which we call the \u201crollout set\u201d. For each state sn in the rollout set, we sample K actions according to an,k \u223c q(\u00b7|sn). Any choice of q(\u00b7|sn) with a support that includes the support of \u03c0\u03b8i(\u00b7|sn) will produce a consistent estimator. In practice, we found that q(\u00b7|sn) = \u03c0\u03b8i(\u00b7|sn) works well on continuous problems, such as robotic locomotion,\nwhile the uniform distribution works well on discrete tasks, such as the Atari games, where it can sometimes achieve better exploration.\nFor each action an,k sampled at each state sn, we estimate Q\u0302\u03b8i(sn, an,k) by performing a rollout (i.e., a short trajectory) starting with state sn and action an,k. We can greatly reduce the variance of the Q-value differences between rollouts by using the same random number sequence for the noise in each of the K rollouts, i.e., common random numbers. See (Bertsekas, 2005) for additional discussion on Monte Carlo estimation of Q-values and (Ng & Jordan, 2000) for a discussion of common random numbers in reinforcement learning.\nIn small, finite action spaces, we can generate a rollout for every possible action from a given state. The contribution to L\u03b8old from a single state sn is as follows:\nLn(\u03b8) = K\u2211 k=1 \u03c0\u03b8(ak|sn)Q\u0302(sn, ak), (16)\nwhere the action space is A = {a1, a2, . . . , aK}. In large or continuous state spaces, we can construct an estimator of the surrogate loss using importance sampling. The self-normalized estimator (Owen (2013), Chapter 8) of L\u03b8old obtained at a single state sn is\nLn(\u03b8) =\n\u2211K k=1\n\u03c0\u03b8(an,k|sn) \u03c0\u03b8old (an,k|sn) Q\u0302(sn, an,k)\u2211K k=1\n\u03c0\u03b8(an,k|sn) \u03c0\u03b8old (an,k|sn)\n, (17)\nassuming that we performed K actions an,1, an,2, . . . , an,K from state sn. This self-normalized estimator removes the need to use a baseline for the Q-values (note that the gradient is unchanged by adding a constant to the Q-values). Averaging over sn \u223c \u03c1(\u03c0), we obtain an estimator for L\u03b8old , as well as its gradient.\nThe vine and single path methods are illustrated in Figure 1. We use the term vine, since the trajectories used for sampling can be likened to the stems of vines, which branch at various points (the rollout set) into several short offshoots (the rollout trajectories).\nThe benefit of the vine method over the single path method that is our local estimate of the objective has much lower variance given the same number of Q-value samples in the surrogate loss. That is, the vine method gives much better estimates of the advantage values. The downside of the vine method is that we must perform far more calls to the simulator for each of these advantage estimates. Furthermore, the vine method requires us to generate multiple trajectories from each state in the rollout set, which limits this algorithm to settings where the system can be reset to an arbitrary state. In contrast, the single path algorithm requires no state resets and can be directly implemented on a physical system (Peters & Schaal, 2008b)."}, {"heading": "6. Practical Algorithm", "text": "Here we present two practical policy optimization algorithm based on the ideas above, which use either the single path or vine sampling scheme from the preceding section. The algorithms repeatedly perform the following steps:\n1. Use the single path or vine procedures to collect a set of state-action pairs along with Monte Carlo estimates of their Q-values.\n2. By averaging over samples, construct the estimated objective and constraint in Equation (15).\n3. Approximately solve this constrained optimization problem to update the policy\u2019s parameter vector \u03b8. We use the conjugate gradient algorithm followed by a line search, which is altogether only slightly more expensive than computing the gradient itself. See Appendix C for details.\nWith regard to (3), we construct the Fisher information matrix (FIM) by analytically computing the Hessian of the KL divergence, rather than using the covariance matrix of the gradients. That is, we estimate Aij\nas 1N \u2211N n=1 \u22022 \u2202\u03b8i\u2202\u03b8j DKL(\u03c0\u03b8old(\u00b7|sn) \u2016 \u03c0\u03b8(\u00b7|sn)), rather\nthan 1N \u2211N n=1 \u2202 \u2202\u03b8i\nlog \u03c0\u03b8(an|sn) \u2202\u2202\u03b8j log \u03c0\u03b8(an|sn). The analytic estimator integrates over the action at each state sn, and does not depend on the action an that was sampled. As described in Appendix C, this analytic estimator has computational benefits in the largescale setting, since it removes the need to store a dense\nHessian or all policy gradients from a batch of trajectories. The rate of improvement in the policy is similar to the empirical FIM, as shown in the experiments."}, {"heading": "7. Connections with Prior Work", "text": "As mentioned in Section 4, our derivation results in a policy update that is related to several prior methods, providing a unifying perspective on a number of policy update schemes. The natural policy gradient (Kakade, 2002) can be obtained as a special case of the update in Equation (13) by using a linear approximation to L and a quadratic approximation to the DKL constraint, resulting in the following problem:\nminimize \u03b8\n[ \u2207\u03b8L\u03b8old(\u03b8) \u2223\u2223 \u03b8=\u03b8old \u00b7 (\u03b8 \u2212 \u03b8old) ]\n(18)\nsubject to 1\n2 (\u03b8old \u2212 \u03b8)TA(\u03b8old)(\u03b8old \u2212 \u03b8) \u2264 \u03b4,\nwhere A(\u03b8old)ij =\n\u2202\n\u2202\u03b8i\n\u2202\n\u2202\u03b8j Es\u223c\u03c1\u03c0 [DKL(\u03c0(\u00b7|s, \u03b8old) \u2016 \u03c0(\u00b7|s, \u03b8))]\n\u2223\u2223 \u03b8=\u03b8old .\nThe update is \u03b8new = \u03b8old \u2212 \u03bbA(\u03b8old)\u22121\u2207\u03b8L(\u03b8) \u2223\u2223 \u03b8=\u03b8old , where the Lagrange multiplier \u03bb is typically treated as an algorithm parameter. This differs from our approach, which enforces the constraint at each update. Though this difference might seem subtle, our experiments demonstrate that it significantly improves the algorithm\u2019s performance on larger problems.\nWe can also obtain the standard policy gradient update by using an `2 constraint or penalty:\nminimize \u03b8\n[ \u2207\u03b8L\u03b8old(\u03b8) \u2223\u2223 \u03b8=\u03b8old \u00b7 (\u03b8 \u2212 \u03b8old) ]\n(19)\nsubject to 1\n2 \u2016\u03b8 \u2212 \u03b8old\u20162 \u2264 \u03b4.\nThe policy iteration update can also be obtained by solving the unconstrained problem minimize\u03c0 L\u03c0old(\u03c0), using L as defined in Equation (3).\nSeveral other methods employ an update similar to Equation (13). Relative entropy policy search (REPS) (Peters et al., 2010) constrains the stateaction marginals p(s, a), while TRPO constraints the conditionals p(a|s). Unlike REPS, our approach does not require a costly nonlinear optimization in the inner loop. Levine and Abbeel (2014) also use a KL divergence constraint, but its purpose is to encourage the policy not to stray from regions where the estimated dynamics model is valid, while we do not attempt to estimate the system dynamics explicitly."}, {"heading": "8. Experiments", "text": "We designed our experiments to investigate the following questions:\n1. What are the performance characteristics of the single path and vine sampling procedures?\n2. TRPO is related to prior methods (e.g. natural policy gradient) but makes several changes, most notably by using a fixed KL divergence rather than a fixed penalty coefficient. How does this affect the performance of the algorithm?\n3. Can TRPO be used to solve challenging largescale problems? How does TRPO compare with other methods when applied to large-scale problems, with regard to final performance, computation time, and sample complexity?\nTo answer (1) and (2), we compare the performance of the single path and vine variants of TRPO, several ablated variants, and a number of prior policy optimization algorithms. With regard to (3), we show that both the single path and vine algorithm can obtain high-quality locomotion controllers from scratch, which is considered to be a hard problem. We also show that these algorithms produce competitive results when learning policies for playing Atari games from images using convolutional neural networks with tens of thousands of parameters."}, {"heading": "8.1. Simulated Robotic Locomotion", "text": "We conducted the robotic locomotion experiments using the MuJoCo simulator (Todorov et al., 2012). The three simulated robots are shown in Figure 2. The states of the robots are their generalized positions and velocities, and the controls are joint torques. Underactuation, high dimensionality, and non-smooth dynamics due to contacts make these tasks very challenging. The following models are included in our evaluation:\n1. Swimmer. 10-dimensional state space, linear reward for forward progress and a quadratic penalty on joint effort to produce the cost cost(x, u) =\nJo in\nta ng\nle s\nan d\nki ne\nm at\nic s\nControl\nStandard deviations\nFully connected\nlayer\n30 units\nInput layer\nMean parameters Sampling\nSc re\nen in\npu t\n4\u00d74\n4\u00d74\n4\u00d74\n4\u00d74\n4\u00d74\n4\u00d74\n4\u00d74\n4\u00d74\nControl\nHidden layer\n20 units\nConv. layer Conv. layer Input layer\n16 filters16 filters\nAction probabilities Sampling\nSee Table 2 in the Appendix for details on the experimental setup. We used neural networks to represent the policy, with the architecture shown in Figure 3, and further details provided in Appendix D. To establish a standard baseline, we also included the classic cart-pole balancing problem, based on the formulation from Barto et al. (1983), using a linear policy with six parameters that is easy to optimize with derivativefree black-box optimization methods.\nThe following algorithms were considered in the comparison: single path TRPO ; vine TRPO ; rewardweighted regression (RWR), an EM-like policy search method (Peters & Schaal, 2007); cross-entropy method (CEM), a gradient-free method (Szita & Lo\u0308rincz, 2006); covariance matrix adaption (CMA), another gradient-free method (Hansen & Ostermeier, 1996); natural gradient, the classic natural policy gradient algorithm (Kakade, 2002), which differs from single path by the use of a fixed penalty coefficient (Lagrange multiplier) instead of the KL divergence constraint; empirical FIM, identical to single path, except that the FIM\nis estimated using the covariance matrix of the gradients rather than the analytic estimate; max KL, which was only tractable on the cart-pole problem, and uses the maximum KL divergence in Equation (12), rather than the average divergence, allowing us to evaluate the quality of this approximation. The parameters used in the experiments are provided in Appendix E. For the natural gradient method, we swept through the possible values of the penalty coefficient (i.e. the step size) in factors of three, and took the best coefficient according to the final performance.\nLearning curves showing the cost averaged across five runs of each algorithm are shown in Figure 4. Single path and vine TRPO solved all of the problems, yielding the best solutions. Natural gradient performed well on the two easier problems, but was unable to generate hopping and walking gaits that made forward progress. These results provide empirical evidence that constraining the KL divergence is a more robust way to choose step sizes and make fast, consistent progress, compared to using a fixed penalty. CEM and CMA are derivative-free algorithms, hence their sample complexity scales unfavorably with the number of parameters, and they performed poorly on the larger problems. The max KL method learned somewhat slower than our final method, due to the more restrictive form of the constraint, but overall the result suggests that the average KL divergence constraint has a similar effect as the theorecally justified maximum KL divergence. Videos of the policies learned by TRPO may be viewed on the project website: http://sites.google.com/site/trpopaper/.\nNote that TRPO learned all of the gaits with generalpurpose policies and simple cost functions, using minimal prior knowledge. This is in contrast with most prior methods for learning locomotion, which typically rely on hand-architected policy classes that explicitly encode notions of balance and stepping (Tedrake et al., 2004; Geng et al., 2006; Wampler & Popovic\u0301, 2009)."}, {"heading": "8.2. Playing Games from Images", "text": "To evaluate TRPO on a partially observed task with complex observations, we trained policies for playing Atari games, using raw images as input. The games require learning a variety of behaviors, such as dodging bullets and hitting balls with paddles. Aside from the high dimensionality, challenging elements of these games include delayed rewards (no immediate penalty is incurred when a life is lost in Breakout or Space Invaders); complex sequences of behavior (Q*bert requires a character to hop on 21 different platforms); and non-stationary image statistics (Enduro involves a changing and flickering background).\nWe tested our algorithms on the same seven games reported on in (Mnih et al., 2013) and (Guo et al., 2014). The images were preprocessed following the protocol in Mnih et al (2013), and the policy was represented by the convolutional neural network shown in Figure 3, with two convolutional layers with 16 channels and stride 2, followed by one fully-connected layer with 20 units, yielding 33,500 parameters.\nThe results of the vine and single path algorithms are summarized in Table 1, which also includes an expert human performance and two recent methods: deepQ-learning (Mnih et al., 2013), and a combination of Monte-Carlo Tree Search with supervised training (Guo et al., 2014), called UCC-I. The 500 iterations of our algorithm took about 30 hours (with slight variation between games) on a 16-core computer. While our method only outperformed the prior methods on some of the games, it consistently achieved reasonable scores. Unlike the prior methods, our approach was not designed specifically for this task. The ability to apply the same policy search method to methods as diverse as robotic locomotion and image-based game playing demonstrates the generality of TRPO."}, {"heading": "9. Discussion", "text": "We proposed and analyzed trust region methods for optimizing stochastic control policies. We proved monotonic improvement for an algorithm that repeatedly optimizes a local approximation to the expected cost of the policy with a KL divergence penalty, and\nwe showed that an approximation to this method that incorporates a KL divergence constraint achieves good empirical results on a range of challenging policy learning tasks, outperforming prior methods. Our analysis also provides a perspective that unifies policy gradient and policy iteration methods, and shows them to be special limiting cases of an algorithm that optimizes a certain objective subject to a trust region constraint.\nIn the domain of robotic locomotion, we successfully learned controllers for swimming, walking and hopping in a physics simulator, using general purpose neural networks and minimally informative costs. To our knowledge, no prior work has learned controllers from scratch for all of these tasks, using a generic policy search method and non-engineered, general-purpose policy representations. In the game-playing domain, we learned convolutional neural network policies that used raw images as inputs. This requires optimizing extremely high-dimensional policies, and only two prior methods report successful results on this task.\nSince the method we proposed is scalable and has strong theoretical foundations, we hope that it will serve as a jumping-off point for future work on training large, rich function approximators for a range of challenging problems. At the intersection of the two experimental domains we explored, there is the possibility of learning robotic control policies that use vision and raw sensory data as input, providing a unified scheme for training robotic controllers that perform both perception and control. The use of more sophisticated policies, including recurrent policies with hidden state, could further make it possible to roll state estimation and control into the same policy in the partially-observed setting. By combining our method with model learning, it would also be possible to substantially reduce its sample complexity, making it applicable to real-world settings where samples are expensive."}, {"heading": "Acknowledgements", "text": "We thank Emo Todorov and Yuval Tassa for providing the MuJoCo simulator, and Bruno Scherrer for insightful comments. This research was funded in part by the Office of Naval Research through a Young Investigator Award and under grant number N00014-11-1-0688, DARPA through a Young Faculty Award, by the Army Research Office through the MAST program."}, {"heading": "A. Proof of Policy Improvement Bound", "text": "We will adapt Kakade and Langford\u2019s proof to the more general setting considered in this paper. First, we review the Kakade and Langford proof, using our own notation. Recall the useful identity introduced in Section 3, which expresses the policy improvement as an accumulation of expected advantages over time:\n\u03b7(\u03c0new) = \u03b7(\u03c0old)+Es0,a0,s1,a1,... [ \u221e\u2211 t=0 \u03b3tA\u03c0old(st, at) ] where s0 \u223c \u03c10(s0), at \u223c \u03c0new(\u00b7|st), st+1 \u223c P (st+1|st, at). (20)\nThis equation can be derived as follows. First note that A\u03c0old(s, a) = Es\u2032\u223cP (s\u2032|s,a) [c(s) + \u03b3V\u03c0old(s\u2032)\u2212 V\u03c0old(s)]. Therefore,\nEs0,a0,s1,a1,... [ \u221e\u2211 t=0 \u03b3tA\u03c0old(st, at) ] (21)\n= Es0,a0,s1,a1,... [ \u221e\u2211 t=0 \u03b3t(c(st) + \u03b3V\u03c0old(st+1)\u2212 V\u03c0old(st)) ] (22)\n= Es0,a0,s1,a1,... [ \u2212V\u03c0old(s0) + \u221e\u2211 t=0 \u03b3tc(st) ] (23)\n= \u2212Es0 [V\u03c0old(s0)] + Es0,a0,s1,a1,... [ \u221e\u2211 t=0 \u03b3tc(st) ] (24) = \u2212\u03b7(\u03c0old) + \u03b7(\u03c0new) (25)\nDefine A\u0304(s) as the expected advantage of \u03c0\u0303 at state s, averaged over actions: A\u0304(s) = \u2211 a A\u03c0old(s, a)\u03c0new(a|s) (26)\nThen Equation (20) can be rewritten as follows \u03b7(\u03c0new) = \u03b7(\u03c0old) + \u2211 t \u03b3tEs\u223cP (st|\u03c0new) [ A\u0304(s) ] . (27)\nRecall that in conservative policy iteration, the new policy \u03c0new is taken to be a mixture of the old policy \u03c0old and an increment \u03c0\u2032, i.e., \u03c0new(a|s) = \u03c0old(a|s) + \u03c0\u2032(a|s). In other words, to sample from \u03c0new, we first draw a Bernoulli random variable, which tells us to choose \u03c0old with probability (1\u2212 \u03b1) and choose \u03c0\u2032 with probability \u03b1. Let ct be the random variable that indicates the number of times \u03c0\n\u2032 was chosen for t\u2032 < t. We can condition on the value of ct to break the probability distribution P (st = s) into two pieces. Let us consider the expected advantage at timestep t. All expectations over st are taken assuming that the policy \u03c0new was executed for t = 0, 1, . . . , t\u2212 1.\nEst [ A\u0304(st) ] = P (ct = 0)Est [ A\u0304(s)|ct = 0 ] + P (ct > 0)Est [ A\u0304(s)|ct > 0 ] (28)\n= P (ct = 0)Est [ A\u0304(st)|ct > 0 ] + P (ct > 0)Est [ A\u0304(st)|ct > 0 ] = (1\u2212 P (ct \u2265 1))Est [ A\u0304(st) ] + P (st|ct \u2265 1)Est [ A\u0304(st)|ct < 0\n] = Est|ct<0 [ A\u0304(st) ] + P (ct \u2265 1) ( Est [ A\u0304(st)|ct = 0 ] \u2212 Est [ A\u0304(st)|ct > 0\n]) Next, note that\nEst [ A\u0304(s)|ct = 0 ] \u2264 ,\n\u2212Est [ A\u0304(s)|ct > 0 ] \u2264 ,\nP (ct \u2265 1) = 1\u2212 (1\u2212 \u03b1)t. (29)\nHence, we can bound the second term in Equation (28): P (ct \u2265 1) ( Es\u223cP (st|ct\u22651) [ A\u0304(s) ] \u2212 Es\u223cP (ct=0) [ A\u0304(s) ]) \u2264 2 (1\u2212 (1\u2212 \u03b1)t). (30)\nReturning to Equation (27), we get \u03b7(\u03c0new) = \u03b7(\u03c0old) + \u2211 t \u03b3t [ P (ct = 0)Est [ A\u0304(s)|ct = 0 ] + P (ct > 0)Est [ A\u0304(s)|ct > 0 ]] \u2264 \u03b7(\u03c0old) +\n\u2211 t \u03b3t ( Es\u223cP (st|\u03c0new,ct=0) [ A\u0304(s) ] + 2 (1\u2212 (1\u2212 \u03b1)t) ) = \u03b7(\u03c0old) + L\u03c0old(\u03c0new) +\n\u2211 t \u03b3t \u00b7 2 (1\u2212 (1\u2212 \u03b1)t)\n= \u03b7(\u03c0old) + L\u03c0old(\u03c0new) + 2 \u00b7 ( 1\n1\u2212 \u03b3 \u2212 1 1\u2212 \u03b3(1\u2212 \u03b1) ) = \u03b7(\u03c0old) + L\u03c0old(\u03c0new) + 2 \u03b3\n(1\u2212 \u03b3)(1\u2212 (1\u2212 \u03b1)\u03b3) (31)\nThat concludes Kakade and Langford\u2019s proof.\nNow we will adapt their proof to the case that \u03c0new is not necessarily a mixture involving \u03c0old, but the total variation divergence DmaxTV (\u03c0old, \u03c0new) is bounded. The basic idea is to couple the old and new policies so that they choose the same action with probability 1\u2212 \u03b1. Then the same line of reasoning will be applied.\nSee (Levin et al., 2009) for an exposition on couplings. We will use the following fundamental result:\nSuppose pX and pY are distributions with DTV (pX \u2016 pY ) = \u03b1. Then there exists a joint distribution (X,Y ) whose marginals are pX , pY , for which X = Y with probability 1\u2212 \u03b1.\nSee (Levin et al., 2009), Proposition 4.7. This joint distribution is constructed as follows:\n(i) With probability 1\u2212 \u03b1, we sample X = Y from the distribution min(pX , pY )/(1\u2212 \u03b1). (ii) With probability \u03b1, we sample X from max(pX \u2212 pY , 0)/\u03b1 and sample Y from max(pY \u2212 pX , 0)/\u03b1.\nProof of 1. Let a\u03c0old,t, a\u03c0new,t be random variables which represent the actions chosen by policies \u03c0old and \u03c0new at time t. By the preceding results, we can define a\u03c0old,t, a\u03c0new,t on a common probability space so that P (a\u03c0old,t 6= a\u03c0new,t) = DTV (\u03c0old(\u00b7|st) \u2016 \u03c0new(\u00b7|st)) \u2264 \u03b1.\nNow consider sampling a trajectory from the MDP as follows. At each timestep, draw the actions a\u03c0old,t, a\u03c0new,t from the coupled distribution described above. Perform the action a\u03c0new,t. Define ct as the number of times t\u2032 < t where case (ii) is chosen, i.e., a\u03c0old,t, a\u03c0new,t. Next we redefine := maxs maxa|A\u03c0(s, a)| in a slightly weaker way. (See Appendix B for an stronger definition.) Then Equations (29), (30), (31) imply the result."}, {"heading": "B. Perturbation Theory Proof of Policy Improvement Bound", "text": "We also provide a different proof of 1 using perturbation theory. This method makes it possible to provide slightly stronger bounds.\nTheorem 1a. Let \u03b1 denote the maximum total variation divergence between stochastic policies \u03c0 and \u03c0\u0303, as defined in Equation (9), and let L be defined as in Equation (3). Then\n\u03b7(\u03c0\u0303) \u2264 L(\u03c0\u0303) + \u03b12 2\u03b3 (1\u2212 \u03b3)2\n(32)\nwhere\n= max s\n{\u2211 a (\u03c0\u0303(a|s)Q\u03c0(s, a)\u2212 \u03c0(a|s)Q\u03c0(s, a))\u2211\na|\u03c0\u0303(a|s)\u2212 \u03c0(a|s)|\n} (33)\nNote that the defined in Equation (33) is less than or equal to the defined in 1, i.e., Equation (7). So 1a is slightly stronger.\nProof. Let G = (1 + \u03b3P\u03c0 + (\u03b3P\u03c0) 2 + . . . ) = (1 \u2212 \u03b3P\u03c0)\u22121, and similarly Let G\u0303 = (1 + \u03b3P\u03c0\u0303 + (\u03b3P\u03c0\u0303)2 + . . . ) = (1 \u2212 \u03b3P\u03c0\u0303)\u22121. We will use the convention that \u03c1 (a density on state space) is a vector and c (a cost function on state space) is a dual vector (i.e., linear functional on vectors), thus c\u03c1 is a scalar meaning the expected cost under density \u03c1. Note that \u03b7(\u03c0) = cG\u03c10, and \u03b7(\u03c0\u0303) = cG\u0303\u03c10. Let \u2206 = P\u03c0\u0303 \u2212 P\u03c0. We wish to bound \u03b7(\u03c0\u0303)\u2212 \u03b7(\u03c0) = c(G\u0303\u2212G)\u03c10. We start with some standard perturbation theory manipulations.\nG\u22121 \u2212 G\u0303\u22121 = (1\u2212 \u03b3P\u03c0)\u2212 (1\u2212 \u03b3P\u03c0\u0303) = \u03b3\u2206. (34)\nLeft multiply by G and right multiply by G\u0303.\nG\u0303\u2212G = \u03b3G\u2206G\u0303 G\u0303 = G+ \u03b3G\u2206G\u0303 (35)\nSubstituting the right-hand side into G\u0303 gives\nG\u0303 = G+ \u03b3G\u2206G+ \u03b32G\u2206G\u2206G\u0303 (36)\nSo we have\n\u03b7(\u03c0\u0303)\u2212 \u03b7(\u03c0) = c(G\u0303\u2212G)\u03c1 = \u03b3cG\u2206G\u03c10 + \u03b32cG\u2206G\u2206G\u0303\u03c10 (37)\nLet us first consider the leading term \u03b3cG\u2206G\u03c10. Note that cG = v, i.e., the infinite-horizon cost-to-go function. Also note that G\u03c10 = \u03c1\u03c0. Thus we can write \u03b3cG\u2206G\u03c10 = \u03b3v\u2206\u03c1\u03c0. We will show that this expression equals the expected advantage L(\u03c0\u0303)\u2212 L(\u03c0).\nL(\u03c0\u0303)\u2212 L(\u03c0) = \u2211 s \u03c1\u03c0(s) \u2211 a (\u03c0\u0303(a|s)\u2212 \u03c0(a|s))A\u03c0(s, a)\n= \u2211 s \u03c1\u03c0(s) \u2211 a ( \u03c0\u03b8(a|s)\u2212 \u03c0\u03b8\u0303(a|s)\n) [ c(s) +\n\u2211 s\u2032 p(s\u2032|s, a)\u03b3v(s\u2032)\u2212 v(s) ] = \u2211 s \u03c1\u03c0(s) \u2211 s\u2032 \u2211 a (\u03c0(a|s)\u2212 \u03c0\u0303(a|s)) p(s\u2032|s, a)\u03b3v(s\u2032)\n= \u2211 s \u03c1\u03c0(s) \u2211 s\u2032 (p\u03c0(s \u2032|s)\u2212 p\u03c0\u0303(s\u2032|s))\u03b3v(s\u2032)\n= \u03b3v\u2206\u03c1\u03c0 (38)\nNext let us bound the O(\u22062) term \u03b32cG\u2206G\u2206G\u0303\u03c1. First we consider the product \u03b3cG\u2206 = \u03b3v\u2206. Consider the component s of this dual vector.\n(\u03b3v\u2206)s = \u2211 a (\u03c0\u0303(s, a)\u2212 \u03c0(s, a))Q\u03c0(s, a)\n= \u2211 a |\u03c0\u0303(a|s)\u2212 \u03c0(a|s)| \u2211 a(\u03c0\u0303(s, a)\u2212 \u03c0(s, a))Q\u03c0(s, a)\u2211 a|\u03c0\u0303(a|s)\u2212 \u03c0(a|s)| \u2264 \u03b1 (39)\nWe bound the other portion G\u2206G\u0303\u03c1 using the `1 operator norm\n\u2016A\u20161 = sup \u03c1 { \u2016A\u03c1\u20161 \u2016\u03c1\u20161 } (40)\nwhere we have that \u2016G\u20161 = \u2016G\u0303\u20161 = 1/(1\u2212 \u03b3) and \u2016\u2206\u20161 = 2\u03b1. That gives\n\u2016G\u2206G\u0303\u03c1\u20161 \u2264 \u2016G\u20161\u2016\u2206\u20161\u2016G\u0303\u20161\u2016\u03c1\u20161\n= 1 1\u2212 \u03b3 \u00b7 \u03b1 \u00b7 1 1\u2212 \u03b3 \u00b7 1 (41)\nSo we have that\n\u03b32cG\u2206G\u2206G\u0303\u03c1 \u2264 \u03b3\u2016\u03b3cG\u2206\u2016\u221e\u2016G\u2206G\u0303\u03c1\u20161\n\u2264 \u03b3 \u00b7 \u03b1 \u00b7 2\u03b1 (1\u2212 \u03b3)2\n= \u03b12 2\u03b3\n(1\u2212 \u03b3)2 (42)"}, {"heading": "C. Efficiently Solving the Trust-Region Constrained Optimization Problem", "text": "This section describes how to efficiently approximately solve the following constrained optimization problem, which we must solve at each iteration of TRPO:\nminimizeL(\u03b8) subject to DKL(\u03b8old, \u03b8) \u2264 \u03b4. (43)\nThe method we will describe involves two steps: (1) compute a search direction, using a linear approximation to objective and quadratic approximation to the constraint; and (2) perform a line search in that direction, ensuring that we improve the nonlinear objective while satisfying the nonlinear constraint.\nThe search direction is computed by approximately solving the equation Ax = \u2212g, where A is the Fisher information matrix, i.e., the quadratic approximation to the KL divergence constraint: DKL(\u03b8old, \u03b8) \u2248 12 (\u03b8 \u2212 \u03b8old)\nTA(\u03b8\u2212\u03b8old), where Aij = \u2202\u2202\u03b8i \u2202 \u2202\u03b8j DKL(\u03b8old, \u03b8). In large-scale problems, it is prohibitively costly (with respect to computation and memory) to form the full matrix A (or A\u22121). However, the conjugate gradient algorithm allows us to approximately solve the equation Ax = b without forming this full matrix, when we merely have access to a function that computes matrix-vector products y \u2192 Ay. Appendix C.1 describes the most efficient way to compute matrix-vector products with the Fisher information matrix. For additional exposition on the use of Hessian-vector products for optimizing neural network objectives, see (Martens & Sutskever, 2012) and (Pascanu & Bengio, 2013).\nHaving computed the search direction s \u2248 A\u22121g, we next need to compute the maximal step length \u03b2 such that \u03b8+\u03b2s will satisfy the KL divergence constraint. To do this, let \u03b4 = DKL \u2248 12 (\u03b2s) TA(\u03b2s) = 12\u03b2 2sTAs. From this,\nwe obtain \u03b2 = \u221a\n2\u03b4/sTAs, where \u03b4 is the desired KL divergence. The term sTAs can be computed through a single Hessian vector product, and it is also an intermediate result produced by the conjugate gradient algorithm.\nLast, we use a line search to ensure improvement of the surrogate objective and satisfaction of the KL divergence constraint, both of which are nonlinear in the parameter vector \u03b8 (and thus depart from the linear and quadratic approximations used to compute the step). We perform the line search on the objective L\u03b8old(\u03b8) + X [DmaxKL (\u03b8old, \u03b8) \u2264 \u03b4], where X [. . . ] equals zero when its argument is true and +\u221e when it is false. Starting with the maximal value of the step length \u03b2 computed in the previous paragraph, we shrink \u03b2 exponentially until the objective improves. Without this line search, the algorithm occasionally computes large steps that cause a catastrophic degradation of performance.\nC.1. Computing the Fisher-Vector Product\nHere we will describe how to compute the matrix-vector product between the averaged Fisher information matrix and arbitrary vectors. This matrix-vector product enables us to perform the conjugate gradient algorithm. Suppose that the parameterized policy maps from the input x to \u201cdistribution parameter\u201d vector \u00b5\u03b8(x), which parameterizes the distribution \u03c0(u|x). Now the KL divergence for a given input x can be written as follows:\nDKL(\u03c0\u03b8old(\u00b7|x) \u2016 \u03c0\u03b8(\u00b7|x)) = kl(\u00b5\u03b8(x), \u00b5old) (44)\nwhere kl is the KL divergence between the distributions corresponding to the two mean parameter vectors. Differentiating kl twice with respect to \u03b8, we obtain\n\u2202\u00b5a(x)\n\u2202\u03b8i\n\u2202\u00b5b(x)\n\u2202\u03b8j kl\u2032\u2032ab(\u00b5\u03b8(x), \u00b5old) +\n\u22022\u00b5a(x)\n\u2202\u03b8i\u2202\u03b8j kl\u2032a(\u00b5\u03b8(x), \u00b5old) (45)\nwhere the primes (\u2032) indicate differentiation with respect to the first argument, and there is an implied summation over indices a, b. The second term vanishes, leaving just the first term. Let J := \u2202\u00b5a(x)\u2202\u03b8i (the Jacobian), then the Fisher information matrix can be written in matrix form as JTMJ , where M = kl\u2032\u2032ab(\u00b5\u03b8(x), \u00b5old) is the Fisher information matrix of the distribution in terms of the mean parameter \u00b5 (as opposed to the parameter \u03b8). This has a simple form for most parameterized distributions of interest.\nThe Fisher-vector product can now be written as a function y \u2192 JTMJy. Multiplication by JT and J can be performed by most automatic differentiation and neural network packages (multiplication by JT is the wellknown backprop operation), and the operation for multiplication by M can be derived for the distribution of interest. Note that this Fisher-vector product is straightforward to average over a set of datapoints, i.e., inputs x to \u00b5.\nOne could alternatively use a generic method for calculating Hessian-vector products using reverse mode automatic differentiation ((Wright & Nocedal, 1999), chapter 8), computing the Hessian of DKL with respect to \u03b8. This method would be slightly less efficient as it does not exploit the fact that the second derivatives of \u00b5(x) (i.e., the second term in Equation (45)) can be ignored, but may be substantially easier to implement.\nWe have described a procedure for computing the Fisher-vector product y \u2192 Ay, where the Fisher information matrix is averaged over a set of inputs to the function \u00b5. Computing the Fisher-vector product is typically about as expensive as computing the gradient of an objective that depends on \u00b5(x) (Wright & Nocedal, 1999). Furthermore, we need to compute k of these Fisher-vector products per gradient, where k is the number of iterations of the conjugate gradient algorithm we perform. We found k = 10 to be quite effective, and using higher k did not result in faster policy improvement. Hence, a na\u0308\u0131ve implementation would spend more than 90% of the computational effort on these Fisher-vector products. However, we can greatly reduce this burden by subsampling the data for the computation of Fisher-vector product. Since the Fisher information matrix merely acts as a metric, it can be computed on a subset of the data without severely degrading the quality of the final step. Hence, we can compute it on 10% of the data, and the total cost of Hessian-vector products will be about the same as computing the gradient. With this optimization, the computation of a natural gradient step A\u22121g does not incur a significant extra computational cost beyond computing the gradient g."}, {"heading": "D. Approximating Factored Policies with Neural Networks", "text": "The policy, which is a conditional probability distribution \u03c0\u03b8(a|s), can be parameterized with a neural network. The most straightforward way to do so is to have the neural network map (deterministically) from the state vector s to a vector \u00b5 that specifies a distribution over action space. Then we can compute the likelihood p(a|\u00b5) and sample a \u223c p(a|\u00b5).\nFor our experiments with continuous state and action spaces, we used a Gaussian distribution, where the covariance matrix was diagonal and independent of the state. A neural network with several fully-connected (dense) layers maps from the input features to the mean of a Gaussian distribution. A separate set of parameters specifies the log standard deviation of each element. More concretely, the parameters include a set of weights and biases for the neural network computing the mean, {Wi, bi}Li=1, and a vector r (log standard deviation) with the same dimension as a. Then, the policy is defined by the normal distribution\nN ( mean = NeuralNet ( s; {Wi, bi}Li=1 ) , stdev = exp(r) ) . Here, \u00b5 = [mean, stdev].\nFor the experiments with discrete actions (Atari), we use a factored discrete action space, where each factor is parameterized as a categorical distribution. That is, the action consists of a tuple (a1, a2, . . . , aK) of integers ak \u2208 {1, 2, . . . , Nk}, and each of these components is assumed to have a categorical distribution, which is specified by a vector \u00b5k = [p1, p2, . . . , pNk ]. Hence, \u00b5 is defined to be the concatenation of the factors\u2019 parameters:\n\u00b5 = [\u00b51, \u00b52, . . . , \u00b5K ] and has dimension dim\u00b5 = \u2211K k=1Nk. The components of \u00b5 are computed by taking\napplying a neural network to the input s and then applying the softmax operator to each slice, yielding normalized probabilities for each factor.\nE. Experiment Parameters"}, {"heading": "F. Learning Curves for the Atari Domain", "text": ""}], "references": [{"title": "Neuronlike adaptive elements that can solve difficult learning control problems", "author": ["A. Barto", "R. Sutton", "C. Anderson"], "venue": "IEEE Transactions on Systems, Man and Cybernetics,", "citeRegEx": "Barto et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Barto et al\\.", "year": 1983}, {"title": "Dynamic programming and optimal control, volume", "author": ["D. Bertsekas"], "venue": null, "citeRegEx": "Bertsekas,? \\Q2005\\E", "shortCiteRegEx": "Bertsekas", "year": 2005}, {"title": "A survey on policy search for robotics", "author": ["M. Deisenroth", "G. Neumann", "J. Peters"], "venue": "Foundations and Trends in Robotics,", "citeRegEx": "Deisenroth et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Deisenroth et al\\.", "year": 2013}, {"title": "Simulation optimization: a review, new developments, and applications", "author": ["Fu", "Michael C", "Glover", "Fred W", "April", "Jay"], "venue": "In Proceedings of the 37th conference on Winter simulation,", "citeRegEx": "Fu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Fu et al\\.", "year": 2005}, {"title": "Approximate dynamic programming finally performs well in the game of Tetris", "author": ["Gabillon", "Victor", "Ghavamzadeh", "Mohammad", "Scherrer", "Bruno"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gabillon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2013}, {"title": "Fast biped walking with a reflexive controller and realtime policy searching", "author": ["T. Geng", "B. Porr", "F. W\u00f6rg\u00f6tter"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Geng et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Geng et al\\.", "year": 2006}, {"title": "Deep learning for real-time atari game play using offline Monte-Carlo tree search planning", "author": ["X. Guo", "S. Singh", "H. Lee", "R.L. Lewis", "X. Wang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Adapting arbitrary normal mutation distributions in evolution strategies: The covariance matrix adaptation", "author": ["Hansen", "Nikolaus", "Ostermeier", "Andreas"], "venue": "In Evolutionary Computation,", "citeRegEx": "Hansen et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 1996}, {"title": "A natural policy gradient", "author": ["Kakade", "Sham"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kakade and Sham.,? \\Q2002\\E", "shortCiteRegEx": "Kakade and Sham.", "year": 2002}, {"title": "Approximately optimal approximate reinforcement learning", "author": ["Kakade", "Sham", "Langford", "John"], "venue": "In ICML,", "citeRegEx": "Kakade et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2002}, {"title": "Reinforcement learning as classification: Leveraging modern classifiers", "author": ["Lagoudakis", "Michail G", "Parr", "Ronald"], "venue": "In ICML,", "citeRegEx": "Lagoudakis et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis et al\\.", "year": 2003}, {"title": "Training deep and recurrent networks with hessian-free optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Martens and Sutskever,? \\Q2012\\E", "shortCiteRegEx": "Martens and Sutskever", "year": 2012}, {"title": "Playing Atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Efficient methods in convex programming", "author": ["Nemirovski", "Arkadi"], "venue": null, "citeRegEx": "Nemirovski and Arkadi.,? \\Q2005\\E", "shortCiteRegEx": "Nemirovski and Arkadi.", "year": 2005}, {"title": "PEGASUS: A policy search method for large mdps and pomdps", "author": ["A.Y. Ng", "M. Jordan"], "venue": "In Uncertainty in artificial intelligence (UAI),", "citeRegEx": "Ng and Jordan,? \\Q2000\\E", "shortCiteRegEx": "Ng and Jordan", "year": 2000}, {"title": "Revisiting natural gradient for deep networks", "author": ["Pascanu", "Razvan", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1301.3584,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Reinforcement learning of motor skills with policy gradients", "author": ["J. Peters", "S. Schaal"], "venue": "Neural Networks,", "citeRegEx": "Peters and Schaal,? \\Q2008\\E", "shortCiteRegEx": "Peters and Schaal", "year": 2008}, {"title": "Relative entropy policy search", "author": ["J. Peters", "K. M\u00fclling", "Y. Alt\u00fcn"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Peters et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2010}, {"title": "Reinforcement learning by reward-weighted regression for operational space control", "author": ["Peters", "Jan", "Schaal", "Stefan"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Peters et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2007}, {"title": "Asymptopia: an exposition of statistical asymptotic theory", "author": ["Pollard", "David"], "venue": "Neural computation,", "citeRegEx": "Pollard and David.,? \\Q2000\\E", "shortCiteRegEx": "Pollard and David.", "year": 2000}, {"title": "Stochastic policy gradient reinforcement learning on a simple 3d biped", "author": ["R. Tedrake", "T. Zhang", "H. Seung"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Tedrake et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Tedrake et al\\.", "year": 2004}, {"title": "MuJoCo: A physics engine for model-based control", "author": ["Todorov", "Emanuel", "Erez", "Tom", "Tassa", "Yuval"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "Optimal gait and form for animal locomotion", "author": ["Wampler", "Kevin", "Popovi\u0107", "Zoran"], "venue": "In ACM Transactions on Graphics (TOG),", "citeRegEx": "Wampler et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wampler et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "Most algorithms for policy optimization can be classified into three broad categories: policy iteration methods, which alternate between estimating the value function under the current policy and improving the policy (Bertsekas, 2005); policy gradient methods, which use an estimator of the gradient of the expected cost obtained from sample trajectories (Peters & Schaal, 2008a) (and which, as we later discuss, have a close connection to policy iteration); and derivative-free optimization methods, such as the cross-entropy method (CEM) and covariance matrix adaptation (CMA), which treat the cost as a black box function to be optimized in terms of the policy parameters (Fu et al.", "startOffset": 217, "endOffset": 234}, {"referenceID": 3, "context": "Most algorithms for policy optimization can be classified into three broad categories: policy iteration methods, which alternate between estimating the value function under the current policy and improving the policy (Bertsekas, 2005); policy gradient methods, which use an estimator of the gradient of the expected cost obtained from sample trajectories (Peters & Schaal, 2008a) (and which, as we later discuss, have a close connection to policy iteration); and derivative-free optimization methods, such as the cross-entropy method (CEM) and covariance matrix adaptation (CMA), which treat the cost as a black box function to be optimized in terms of the policy parameters (Fu et al., 2005; Szita & L\u00f6rincz, 2006).", "startOffset": 675, "endOffset": 715}, {"referenceID": 4, "context": "For example, while Tetris is a classic benchmark problem for approximate dynamic programming (ADP) methods, stochastic optimization methods are difficult to beat on this task (Gabillon et al., 2013).", "startOffset": 175, "endOffset": 198}, {"referenceID": 2, "context": "A practical approximation to this policy improvement procedure can efficiently optimize nonlinear policies with tens of thousands of parameters, which have previously posed a major challenge for model-free policy search (Deisenroth et al., 2013).", "startOffset": 220, "endOffset": 245}, {"referenceID": 17, "context": "Similar policy updates have been proposed in prior work (Bagnell & Schneider, 2003; Peters & Schaal, 2008b; Peters et al., 2010), and we compare our approach to prior methods in Section 7 and in the experiments in Section 8.", "startOffset": 56, "endOffset": 128}, {"referenceID": 4, "context": "This method has mostly been explored in the context of policy iteration methods (Lagoudakis & Parr, 2003; Gabillon et al., 2013).", "startOffset": 80, "endOffset": 128}, {"referenceID": 1, "context": "See (Bertsekas, 2005) for additional discussion on Monte Carlo estimation of Q-values and (Ng & Jordan, 2000) for a discussion of common random numbers in reinforcement learning.", "startOffset": 4, "endOffset": 21}, {"referenceID": 17, "context": "Relative entropy policy search (REPS) (Peters et al., 2010) constrains the stateaction marginals p(s, a), while TRPO constraints the conditionals p(a|s).", "startOffset": 38, "endOffset": 59}, {"referenceID": 17, "context": "Relative entropy policy search (REPS) (Peters et al., 2010) constrains the stateaction marginals p(s, a), while TRPO constraints the conditionals p(a|s). Unlike REPS, our approach does not require a costly nonlinear optimization in the inner loop. Levine and Abbeel (2014) also use a KL divergence constraint, but its purpose is to encourage the policy not to stray from regions where the estimated dynamics model is valid, while we do not attempt to estimate the system dynamics explicitly.", "startOffset": 39, "endOffset": 273}, {"referenceID": 21, "context": "We conducted the robotic locomotion experiments using the MuJoCo simulator (Todorov et al., 2012).", "startOffset": 75, "endOffset": 97}, {"referenceID": 0, "context": "To establish a standard baseline, we also included the classic cart-pole balancing problem, based on the formulation from Barto et al. (1983), using a linear policy with six parameters that is easy to optimize with derivativefree black-box optimization methods.", "startOffset": 122, "endOffset": 142}, {"referenceID": 20, "context": "This is in contrast with most prior methods for learning locomotion, which typically rely on hand-architected policy classes that explicitly encode notions of balance and stepping (Tedrake et al., 2004; Geng et al., 2006; Wampler & Popovi\u0107, 2009).", "startOffset": 180, "endOffset": 246}, {"referenceID": 5, "context": "This is in contrast with most prior methods for learning locomotion, which typically rely on hand-architected policy classes that explicitly encode notions of balance and stepping (Tedrake et al., 2004; Geng et al., 2006; Wampler & Popovi\u0107, 2009).", "startOffset": 180, "endOffset": 246}, {"referenceID": 12, "context": "We tested our algorithms on the same seven games reported on in (Mnih et al., 2013) and (Guo et al.", "startOffset": 64, "endOffset": 83}, {"referenceID": 6, "context": ", 2013) and (Guo et al., 2014).", "startOffset": 12, "endOffset": 30}, {"referenceID": 6, "context": ", 2013) and (Guo et al., 2014). The images were preprocessed following the protocol in Mnih et al (2013), and the policy was represented by the convolutional neural network shown in Figure 3, with two convolutional layers with 16 channels and stride 2, followed by one fully-connected layer with 20 units, yielding 33,500 parameters.", "startOffset": 13, "endOffset": 105}, {"referenceID": 12, "context": "The results of the vine and single path algorithms are summarized in Table 1, which also includes an expert human performance and two recent methods: deepQ-learning (Mnih et al., 2013), and a combination of Monte-Carlo Tree Search with supervised training (Guo et al.", "startOffset": 165, "endOffset": 184}, {"referenceID": 6, "context": ", 2013), and a combination of Monte-Carlo Tree Search with supervised training (Guo et al., 2014), called UCC-I.", "startOffset": 79, "endOffset": 97}], "year": 2017, "abstractText": "We propose a family of trust region policy optimization (TRPO) algorithms for learning control policies. We first develop a policy update scheme with guaranteed monotonic improvement, and then we describe a finitesample approximation to this scheme that is practical for large-scale problems. In our experiments, we evaluate the method on two different and very challenging sets of tasks: learning simulated robotic swimming, hopping, and walking gaits, and playing Atari games using images of the screen as input. For these tasks, the policies are neural networks with tens of thousands of parameters, mapping from observations to actions.", "creator": "LaTeX with hyperref package"}}}