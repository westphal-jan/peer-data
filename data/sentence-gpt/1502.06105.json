{"id": "1502.06105", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2015", "title": "Regularization and Kernelization of the Maximin Correlation Approach", "abstract": "Robust classification becomes challenging when classes contain multiple subclasses. Examples include multi-font optical character recognition and automated protein function prediction. In correlation-based nearest-neighbor classification, the maximin correlation approach (MCA) provides the worst-case optimal solution by minimizing the maximum misclassification risk through an iterative procedure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Sat, 21 Feb 2015 14:37:44 GMT  (2286kb)", "http://arxiv.org/abs/1502.06105v1", null], ["v2", "Tue, 29 Mar 2016 04:42:12 GMT  (1909kb)", "http://arxiv.org/abs/1502.06105v2", "Submitted to IEEE Access"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["taehoon lee", "taesup moon", "seung jean kim", "sungroh yoon"], "accepted": false, "id": "1502.06105"}, "pdf": {"name": "1502.06105.pdf", "metadata": {"source": "CRF", "title": "Regularization and Kernelization of the Maximin Correlation Approach", "authors": ["Taehoon Lee", "Taesup Moon", "Seung Jean Kim", "Sungroh Yoon"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 2.\n06 10\n5v 1\n[ cs\n.C V\n] 2\n1 Fe\nb 20\n15 1\nIndex Terms\nnearest neighbor, correlation, maximin, SOCP, QCLP, QP, regularization, kernel trick.\n\u2726"}, {"heading": "1 INTRODUCTION", "text": "N EAREST neighbor (NN) classifiers [1], [2] are non-parametric methods that classify an object based on itsdistance to the nearest trained class. Owing largely to their simplicity and reasonable performance in practical problems, they have been widely used for various tasks such as image retrieval [3], video indexing [4], object tracking [5], location-dependent information service [6], and sequence alignment [7]. The main problems that arise with NN classifiers are that (1) it becomes computationally intensive to find the neighbors as the number of training samples increases and (2) the notion of nearest neighbors can break down in high-dimensional spaces. Approaches have been proposed to reduce the computational complexity [8] and to adaptively determine nearest neighbors (even in high-dimensional spaces) [9]. Template matching is another widely used technique that pre-computes a representative vector for each class and uses it to locate the nearest neighbor of an object [10], [11]. In multiple subclass classification problems, where each class contains more than one subclass, a template is constructed for each subclass, and then the aggregate template of a class is created based on the subclass templates [12]. In this paper, we consider constructing the aggregate template based on the idea of the maximin correlation approach (MCA) [12]. For correlation-based NN classification problems, it is known that MCA can provide an optimal aggregate template in that MCA iteratively maximizes the minimum correlation with the templates it represents, eventually minimizing the maximum classification error risk. MCA was originally proposed for multi-font optical character recognition [13], and has been successfully applied to automated protein function prediction [14] and typography clustering [15]. Despite the theoretical advantages of MCA, it has inherent limitations that have hindered wider applications in practice, such as susceptibility to noise and outliers, inability to handle nonlinearities in datasets, as well as high computational complexity. This paper proposes the regularized maximin correlation approach (R-MCA), a significantly improved solution method that overcomes these limitations of the original MCA.\n\u2022 T. Lee and S. Yoon are with the Department of Electrical and Computer Engineering, Seoul National University, Seoul 151-744, Korea. E-mail: sryoon@snu.ac.kr \u2022 T. Moon was with the Department of Statistics, University of California, Berkeley, CA 94720, USA and is now with Samsung Advanced Institute of Technology, Suwon 443-803, Korea. \u2022 S. Kim was with Citi Capital Advisors, New York, NY 10013, USA.\nManuscript received February, 2015.\n2 A B C\nPSfrag replacements\ntraditional\ntraditional\nmaximin\nmaximin\nmaximin\nproposed\nr-maximin\nr-maximin\ncentroid\nFig. 1: Geometric interpretation and comparison. [\u2018centroid\u2019: the template vector represented by the centroid of a group; \u2018maximin\u2019: the template vector obtained from the original MCA; \u2018r-maximin\u2019: the template vector returned by the proposed R-MCA] (a) MCA finds a vector whose direction minimizes the worst (i.e., maximum) angle between the vector and the class members. No outlier is assumed. (b) Adding outliers to (a) causes an abrupt swing in the traditional maximin that MCA returns. The resulting angle does not represent the class appropriately. In contrast, the r-maximin that R-MCA finds is more robust to outliers. (c) The character \u2018A\u2019 represented in 10 different fonts (the two boxed fonts can be considered outliers). Also shown are the r-maximin, maximin, and centroid aggregate templates of the ten images, respectively.\nAs opposed to the iterative method employed by MCA, we reformulate it as an instance of quadratically constrained linear programming (QCLP) [16]. The worst-case complexity of the iterative method grows quadratically as the number of objects increases. In contrast, the proposed QCLP formulation can be solved with linear complexity by the interior-point methods (IPMs) [17] when coefficient matrices are positive semidefinite. Based on the QCLP formulation, we incorporate regularization and additional constraints that help the R-MCA to find a robust representative vector even when (noisy) outliers exist. Our formulation has some resemblance to the regularization employed by the soft-margin support vector machine (SVM) [18]. We furthermore develop the Lagrangian dual of the regularized QCLP, which enables us to apply the kernel trick [19] to effectively handle nonlinear structures possibly embedded in data. This paper also presents our experimental results that confirm the effectiveness of R-MCA on various public data sets. According to these results, the proposed R-MCA successfully delivers the following improvements:\n\u2022 QCLP-based reformulation of MCA that enables acceleration, regularization and kernelization \u2022 Regularization to fight overfitting and outliers \u2022 Kernelization for discovering nonlinear structures\nNote that R-MCA devises a robust and scalable solution to not only nearest-neighbor classification but also a variety of other tasks based on finding group representatives. For such tasks, R-MCA can provide an alternative to conventional aggregates such as centroids and medoids."}, {"heading": "2 BACKGROUND", "text": ""}, {"heading": "2.1 Maximin Correlation Approach (MCA)", "text": "We brief the reader on the mathematical formulation of the MCA. Additional details can be found in [12]. Consider two non-zero vectors u,x \u2208 Rm. When u and x are column vectors, the centered correlation is defined as \u03c6(u,x) = uTx/||u||2||x||2. MCA involves maximizing the objective function that is to find the worst-case value among the centered correlation between a non-zero vector u and all of the vectors in the set X \u2286 Rm. MCA can construct a template vector u that maximizes the minimum correlation by the following formulation:\nmaximize minx\u2208X \u03c6(u,x) subject to ||u||2 6= 0. (1)\nThe optimization (1) is referred to as the MCA problem (MCAP). The original MCA [12] assumes that all of the xi\u2019s are linearly independent, ||xi||2 = 1 for all xi \u2208 X , and xTi xj \u2265 0 for all xi,xj \u2208 X (note that these assumptions are not required in the proposed R-MCA). An iterative solution to the MCAP was proposed in [12]: the template vector u is initialized to the centroid vector and is updated at each iteration to find the optimal vector u\u22c6. For fixed m (the dimensionality), the worst-case complexity of this iterative algorithm is O(n2), where n is the number of objects in X ."}, {"heading": "2.2 Geometric Interpretation and Motivation", "text": "Fig. 1 shows the geometric interpretation and comparison of MCA and the proposed R-MCA, which will be formally defined in the next section. As shown in Fig. 1(a), solving MCA is equivalent to finding a template vector whose direction minimizes the worst-case angle between the vector and class members. With no outliers, the maximin template that MCA returns represents the group reasonably.\nThe existence of outliers significantly degrades the performance of MCA. For instance, Fig. 1(b) shows the scenario in which outliers are added to the data shown in Fig. 1(a). The maximin template returned by the original MCA swings abruptly toward the outliers because MCA does not recognize outliers. In contrast, the r-maximin template returned by R-MCA takes into account the outliers, yielding a template that represents the group more reasonably. As an example from real applications, Fig. 1(c) shows the images of the character \u2018A\u2019 in ten different fonts and three types of templates, each of which aims at representing the ten images as a whole. In the centroid template, the two \u2018outlier\u2019 (boxed) fonts are averaged out and do not appear, whereas the maximin template preserves them to some extent. For this reason, in multi-font character recognition, the maximin template, which incorporates outlier information, results in higher accuracy than the centroid template [12], [14]. In other applications, however, representing outliers may hurt classification accuracy. In R-MCA, we can adjust the sensitivity to outliers, providing an intermediate representation between the maximin and centroid templates (e.g., compare the three templates in Fig. 1(c))."}, {"heading": "3 PROPOSED R-MCA METHDOLOGY", "text": "This section presents the details of the proposed R-MCA method; the derived flow is shown in Fig. 2. To propose more efficient solutions to the MCAP (1), we first formulate it as an instance of QCLP [16]. The QCLP formulation (2) enables us to find a solution using the general IPMs [17], instead of the iterative method proposed in [12]. The QCLP formulation also allows us to define slack variables that lead to a regularized version (3) that effectively handles outliers. From the regularized version (3), we further derive its Lagrangian dual form (7), which reveals the structure suitable for applying the kernel trick. To handle nonlinearities, we finally kernelize the dual form (7) into the kernelized R-MCA formulation (10). Note that the original MCA (i.e., the version without regularization) can also be kernelized; starting from the QCLP formulation (2), we derive its dual form (9) and the kernelized MCA (11)."}, {"heading": "3.1 QCLP Formulation of MCA", "text": "A simple trick allows us to reformulate (1) as a tractable convex problem. After normalization of input vectors, (1) becomes equivalent to\nmaximize mini=1,...,n(u T xi)\nsubject to ||u||2 \u2264 1. The maximizer of the above maximin problem coincides with the solution of the following optimization problem:\nmaximize t \u2208 R subject to uTxi \u2265 t, i = 1, . . . , n\nu T u \u2264 1.\n(2)\nThe equivalent formulation (2) for the MCAP with a finite set X is simple; it involves minimizing a linear function over m variables, with n linear equality constraints and one quadratic constraint. It is an instance of QCLP, a special type of optimization problem that can be solved globally and efficiently by the IPMs [17]. Recall that n corresponds to the number of objects and m corresponds to the dimensionality. Since the number of iterations that is necessary for IPM to find a solution is practically constant (typically from 10 to 50) [16], we can see that the QCLP (2) can be solved in O(nm2 +m3) flops. For comparison, the number of flops required for the iterative method [12] is either 4mnp\u2212mp2 or 4n2p\u2212 2np2 +mn2, depending on the implementation, where p is the number of iterations. The empirical study in [12] shows that p grows nearly linearly in n."}, {"heading": "3.2 Regularization of MCA", "text": "To construct a representative vector that is more robust to outliers (see Fig. 1(b) for an example), we apply the regularization to MCA. Regularization is a popular technique to prevent overfitting. Bertsimas and Copenhaver recently described a unifying view of the connection between robustification1 and regularization [20]. Specifically, we introduce a non-negative \u2018slack\u2019 variable \u03bei for each object xi, which can help the optimization problem find a solution insensitive to outliers. Using the slack variables, we can describe the regularized version of QCLP (2) as\n1. shielding a statistical problem against noise in the data\n5 maximize t\u2212 \u03bb n n \u2211\ni=1\n\u03bei\nsubject to uTxi \u2265 t\u2212 \u03bei, i = 1, . . . , n \u03bei \u2265 0, i = 1, . . . , n u T u \u2264 1\n(3)\nwhere \u03bb is a user-specified sensitivity parameter for slack variables that serves as a regularization parameter; larger \u03bb leads to a template vector that is more sensitive to outliers. Section 3.3 presents more details of \u03bb and its effect on the solution of the optimization problem. Fig. 3 presents the geometric interpretation. This formulation is similar to the optimization problem within the soft-margin support vector machine (SVM) [18], which is a relaxation of the original SVM. Leveraged by the regularization, the soft-margin SVM is more robust to labeling error, and we expect the proposed R-MCA to have the same advantage over the original MCA. To understand (3) better, we derive its Lagrange dual [21] problem. This will also allow us to analyze the effects of \u03bb more quantitatively. We first define the Lagrangian L: R \u00d7 Rn \u00d7 Rn \u00d7 Rn \u00d7 Rn \u00d7 R 7\u2192 R associated with the problem (3) as\nL(t, \u03be,u,v,w, z) = \u2212t+ \u03bb n\nn \u2211\ni=1\n\u03bei + z(1\u2212 uTu)\n\u2212 n \u2211\ni=1\n(\nvi(u T xi \u2212 t+ \u03bei) + wi\u03bei\n)\n(4)\nwhere v = (v1, . . . , vn) T ,w = (w1, . . . , wn) T \u2208 Rn, and z \u2208 R are the Lagrange multipliers for the three inequality constraints of (3). We then define the Lagrange dual function g as the minimum value of the Lagrangian over t, \u03be, and u:\ng(v,w, z) = inf t,\u03be,u L(t, \u03be,u,v,w, z). (5)\nTo calculate the infimum of the Lagrangian, we partially differentiate the Lagrangian as follows:\n\u2202L \u2202t = \u22121 +\nn \u2211\ni=1\nvi = 0 \u21d2 n \u2211\ni=1\nvi = 1\n\u2202L \u2202\u03bei = \u03bb n \u2212 vi \u2212 wi = 0 \u21d2\n\u03bb n = vi + wi\n\u2202L \u2202u = \u2212\nn \u2211\ni=1\nvixi + 2zu = 0 \u21d2 u = 1\n2z\nn \u2211\ni=1\nvixi.\nFrom the above equalities, we can rewrite the Lagrange dual function (5) as\ng(v,w, z) = \u2212 1 4z v TCv \u2212 z\nwhere Cij = x T i xj . We can consider this as a function of z that is minimized when z\n\u22c6 = \u221a vTCv/2 \u2265 0. Thus, we\ncan obtain a simplified representation of g(v,w, z) as \u2212 \u221a vTCv by substituting z = z\u22c6 into the above dual function and can finally formulate the dual problem of (3) as\nminimize vTCv subject to vi \u2265 0, wi \u2265 0, i = 1, . . . , n\n\u03bb/n = vi + wi, i = 1, . . . , n n \u2211\ni=1\nvi = 1.\n(6)\nAdditionally, we can combine the top two constraints of (6) into an inequality \u2018\u03bb/n \u2265 vi \u2265 0 for all i\u2019, because vi and wi are complements to each other. The problem now can be described as follows:\nminimize vTCv subject to \u03bb/n v 0\n1 T v = 1.\n(7)\n6 We can identify that (7) is a convex quadratic program (QP) since the gram matrix C is positive semidefinite. Hence, when (7) has a feasible solution, by the strong duality principle [21], the template vector of R-MCA, u\u22c6 \u2208 Rm (the primal solution), can be obtained from the solution of (7), v\u22c6 \u2208 Rn (the dual solution), as follows:\nu \u22c6 = c\u22c6\nn \u2211\ni=1\nv\u22c6i xi (8)\nin which c\u22c6 = 1/ \u221a v\u22c6TCv\u22c6."}, {"heading": "3.3 Analysis of \u03bb and a Comparison with MCA", "text": "We elaborate on the characteristics of the template vectors obtained by R-MCA using the dual form (7). To satisfy the constraint 1Tv = 1 therein, we can consider the following four cases:\n1) [\u03bb < 1] If the Lagrangian multipliers vi\u2019s are lower than 1/n, the constraint \u2211\nvi = 1 cannot be satisfied. That is, (7) is not feasible if \u03bb < 1.\n2) [\u03bb = 1] Because \u03bb is the upper bound of vi\u2019s, the only solution to fit the constraint \u2211\nvi = 1 must be vi = 1/n for all i. In this case, u\n\u22c6 points to the same direction as the centroid of xi\u2019s with the scaling factor (u\u22c6 = c\u22c6 \u2211\nv\u22c6i xi = c \u22c6 \u2211\nxi/n). 3) [1 < \u03bb < n] Larger \u03bb makes vi\u2019s less constrained; when \u03bb becomes large, the upper bound constraints for vi\u2019s\nbecome less restrictive for minimizing the objective vTCv. Hence, the effect of each individual example xi, including the outlier, to the primal solution (8) can increase as \u03bb increases. 4) [\u03bb \u2265 n] If \u03bb \u2265 n, the upper bound constraints for vi\u2019s disappear; this follows from the fact that v is forced to be a probability vector by the other constraints, and thus it will always satisfy the upper bound constraints when \u03bb \u2265 n. By comparing (7) with the dual of the original MCA formulation (2) as below, we deduce that the solution of R-MCA for this case coincides with that of MCA.\nSimilarly as in Section 3.2, in order to obtain a dual for (2), we first write Lagrangian of (2) and its partial derivatives as follows:\nL(t,u,v, z) =\u2212 t\u2212 n \u2211\ni=1\nvi(u T xi \u2212 t)\u2212 z(1\u2212 uTu)\n\u2202L \u2202t =\u2212 1 +\nn \u2211\ni=1\nvi = 0\n\u2202L \u2202u =\u2212\nn \u2211\ni=1\nvixi + 2zu = 0.\nThe Lagrange dual function of MCA thus becomes g(v, z) = \u2212 1 4z v TCv \u2212 z just as in R-MCA. By inserting\nz\u2217 = \u221a vTCv/2 into this Lagrange dual function, we can formulate the dual of the original MCAP formulation (2) as\nminimize vTCv subject to v 0\n1 T v = 1.\n(9)\nThe above is the same as (7), except that the constraint \u03bb/n v is missing. In other words, the dual of R-MCA (7) becomes the dual of MCA (9) if \u03bb > n, hence the upper bound constraints in (7) disappears. Note that minimizing the form vTCv occurs frequently in multivariate data analysis, such as the principal component analysis [22]. MCA minimizes vTCv subject to v being in a probability simplex."}, {"heading": "3.4 Kernelization of (Regularized) MCA", "text": "The nonlinearity in input space can often be handled better in high (possibly infinite) dimensional space. The mapping to and the computation in such high-dimensional spaces can be costly, if not impossible, but when the input data are acessed only through inner products, we can use the so-called kernel trick [19] to perform implicit mapping and efficient computation. Inspecting the dual form of (7) immediately suggests that we can apply the kernel trick to R-MCA. Replacing the inner products in (7) with a kernel matrix K yields\nminimize vTKv subject to \u03bb/n v 0\n1 T v = 1,\n(10)\n7\nTABLE 1. Data Used in Our Experiments\nName n m Number of classes (description)\nKSC [24] 5211 176 13 (land cover types)\nMNIST [25] 10000 784 10 (digits \u20180\u2019\u2013\u20189\u2019)\nSONAR [26] 208 60 2 (rock or mine)\nGEO [27] 606 30954 2 (ulcerative colitis patient or not)\n3D-NUT 272 3 2 (core or shell)\n0.9\n0.91\n0.92\n0.93\n0.94\n0.95\n0.96\n0.97\n0.98\n0.99\n1A\n\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\n\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\n\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\n\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\n\u22c5\u22c5\u22c5\u22c5\u22c5\nco rr\nel at\nio n\nScrub Willow swam p Cabb age Cabb age O ak Slash pine Broad leaf/O ak Hardw ood\nPSfrag replacements\ncentroid\nmaximin\n\u03bb = 1.1 \u03bb = 1.4 \u03bb = 1.7 \u03bb = 2.0\n\u03bb = 1.4\n\u03bb = 1.8\n\u03bb = 2.2\n\u03bb = 2.6\n\u03bb = 3.0 (r-maximin)\nFirst Principal Component\nS e\nc o\nn d\nP ri n\nc ip\na l C\no m\np o\nn e\nn t\n1 4\nB\nPSfrag replacements\ncentroid\ncentroid\nmaximin\n\u03bb = 1.1\n\u03bb = 1.1\n\u03bb = 1.4\n\u03bb = 1.4\n\u03bb = 1.7\n\u03bb = 1.7\n\u03bb = 2.0 \u03bb = 2.0\n\u03bb = 1.4 \u03bb = 1.8 \u03bb = 2.2 \u03bb = 2.6\n\u03bb = 3.0 (r-maximin)\nFig. 4: Effects of regularization and its parameter \u03bb. (a) The minimum correlation between aggregate templates and subclass templates of the KSC data. (b) The part of the MNIST data representing the digits \u20181\u2019 (yellow) and \u20184\u2019 (blue).\nwhere Kij = k(xi,xj) for a Mercer kernel k [23]. The kernelization allows the proposed R-MCA to find template vectors from data with nonlinearities, thus extending the applicability of the R-MCA. Section 4.4 presents more details of the kernelization and supporting experimental results. The kernelized version of the original MCA can also be derived in similar fashion by replacing the dot-products in the dual quadratic program (9) with a kernel:\nminimize vTKv subject to v 0\n1 \u00af T v = 1\n(11)\nwhere the constraint \u03bb/n v included in the regularized version (10) no longer appears."}, {"heading": "4 EXPERIMENTAL RESULTS AND DISCUSSION", "text": "We tested the proposed R-MCA methodology using the datasets listed in Table 1. More details about each dataset will be provided in the following subsections. For our experiments, we implemented the proposed QCLP-based MCA and R-MCA solvers using SeDuMi software, a MATLAB toolbox for optimization over symmetric cones [28]. For comparison, we also prepared implementations of the original iterative solution to MCA as described in [12], the support vector machine (SVM), and the logistic regression."}, {"heading": "4.1 Effect of Regularization on Subtype Correlation", "text": "To see the effect of regularization, we ran R-MCA with different values of parameter \u03bb on a multiple-subclass datasets and measured how the variation of the correlation between subclass objects and the aggregate template. We used the Kennedy Space Center (KSC) dataset [24], which contains 5211 vectors with 176 dimensions. Each vector represents the signal intensities of different wavelengths measured above 13 types of land covers (105\u2013927 vectors per class). Based on the characterization of vegetation, these classes can be grouped into three types or \u2018superclasses\u2019 (upland with seven land-cover subclasses, wetland with five, and water type with one).\nFig. 4(a) shows the correlation of seven subclasses of the \u2018upland\u2019 class with the regularized maximin aggregate templates (r-maximin) of five different \u03bb values (1.4, 1.8, 2.2, 2.6, 3.0). As mentioned earlier, we define \u03bb to manipulate the degree of regularization and can increase the best-case correlation value with the class members instead of sacrificing the worst-case correlation. To verify this effect, the curves for the non-regularized maximin and the centroid template are also presented. As expected, the curves for the r-maximin are placed between the centroid and the non-regularized maximin. Note that the non-regularized maximin template is successful in boosting the worst-case correlation compared to the centroid template, but the price for this is lowering the correlation of other subtypes. For instance, the worstcase correlation occurring for subtype \u2018Willow swamp\u2019 increases by using the non-regularized maximin template, but the correlation for most of the other subtypes decreases compared to the centroid case. When this behavior needs to be avoided, using the r-maximin with various \u03bb values provides a flexible solution. We present additional examples of changing \u03bb in Section 4.2."}, {"heading": "4.2 Effect of Regularization Parameter \u03bb", "text": "In Section 4.1, we discussed that the regularization parameter \u03bb works as a control knob that places the result from using the r-maximin somewhere between those from the centroid template and the non-regularized maximin template. To visualize the effects of varying \u03bb, we utilized the MNIST database of handwritten digits [25]. From this database, we sampled 1135 and 982 images representing the digits \u20181\u2019 and \u20184\u2019, respectively. Each sample is a 28\u00d7 28 image that can be represented by a 784-dimensional vector. We carried out the PCA of these samples and took the first two principal components only, transforming each of them into a 2-dimensional point, as shown in Fig. 4(b). In the figure, each of the two inlets magnifies the centroid and the r-maximin along with the corresponding images for visual inspection. Recall from Section 3.3 that R-MCA eventually produces a centroid when \u03bb = 1. As depicted in Fig. 4(b), we tested 10 different \u03bb values of the interval [1.1, 2.0] to draw the trajectories of the r-maximin. The centroid in the class \u20181\u2019 is located in the upper-right region, because most samples in the \u20181\u2019 class are distributed in that region. However, it is necessary to shift the aggregate template toward the outliers in order to minimize worst-case classification risk. We confirmed that reducing \u03bb puts the regularized maximin template near the centroid, and increasing \u03bb yields the r-maximin closest to the outliers."}, {"heading": "4.3 Effect of Regularization on Classification", "text": "To see the regularization effects in the context of classification, we carried out binary classification of the SONAR data [26], which consists of 111 mine-reflected and 97 rock-reflected sonar signals of 60 dimensions each. For NN classification using templates, we implemented the nearest template classifier that assigns an unknown vector to the class of its nearest (r-maximin, maximin, or centroid) template. For comparison, we also tested logistic regression and the linear SVM. According to the experimental results from using neural networks in [26], nonlinearities exist in the distribution characteristics of the SONAR data. We thus preprocessed the data using the kernel PCA [29] with the Gaussian kernel (\u03c3 = 1). We then divided the data into two sets (for training and validation) and tested the five different\nclassifiers with 2-fold cross-validation (CV). The value of \u03bb was determined by performing the CV with 9 different \u03bb values (1.1, 1.2, . . . , 1.9). Fig. 5 shows the radar operating characteristic (ROC) curves from the first and second rounds of CV with \u03bb = 1.9. The average area under the curve (AUC) values are 0.90, 0.87, 0.84, 0.82, and 0.81 for NN with the rmaximin templates, the linear SVM, NN with the maximin templates, logistic regression, and NN with the centroid templates, respectively. With respect to the AUC value, the r-maximin classification produced the best result, whereas the performance of the original maximin classification was lower than that of the SVM. This result suggests that the regularization can indeed improve the classification accuracy for real applications with noise."}, {"heading": "4.4 Effect of Kernelization", "text": "Through kernelization, we expect R-MCA to become applicable to classification problems that contain complex shapes in the input space. Fig. 6 shows the results from a proof-of-concept experiment using a synthetic dataset termed 3D-NUT, which was generated as follows: we sampled a point x = [x1, x2, x3] from a trivariate normal distribution N (\u00b5,\u03a3), where \u00b5 = [0, 0, 0] and \u03a3 = I. For the sake of visualization, x was discarded if x2 < 0 and x3 < 0. Otherwise, we set the membership of x to the \u2018core\u2019 class if ||x|| < 1 and to the \u2018shell\u2019 class if ||x|| > 2. Fig. 6(a) depicts the distribution of 272 points color-coded with binary membership (either \u2018core\u2019 or \u2018shell\u2019 class) in the input space. Applying the original MCA resulted in incorrect classification, as shown in Fig. 6(c). In contrast, the kernelized MCA (radial basis kernel with \u03b3 = 1) correctly separates the data points according to their membership, as shown in Fig. 6(b). This experiment confirms that the kernelization works for MCA, and that we will be able to apply the kernelized version to other problems existing kernel-based methods (e.g., kernel PCA) can be applied to."}, {"heading": "4.5 Comparison of Execution Time with MCA", "text": "We compare the runtime of the proposed QCLP-based solution and the original iterative solution [12] to the maximin correlation approach. To this end, we carried out two types of experiments. One is varying the number of objects n with the dimensionality m fixed, and the other involves varying m with n fixed. We measured the runtime using a Windows 7 PC equipped with an Intel i5-3570K CPU (3.4GHz, 6MB, 5GT/s) and 16GB RAM. Fig. 7(a) shows the varying-n fixed-m case for recognizing the digit \u20180\u2019 in the MNIST data (fixed m = 784). The time demand of the iterative solution remained the highest and also grew up faster than the others. As described in Section 3, there are additional inequality constraints and variables in the regularized forms [(3) and (7)] in comparison with the original MCA [(2) and (9)]. Consequently, the two regularized versions require longer run times than the unregularized ones when n > m, as shown in Fig. 7(a). The varying-m fixed-n case is presented in Fig. 7(b). We used the NCBI GEO microarray dataset [30] (the accession number: GSE11223), which provides the regional variation of gene expression in ulcerative colitis patients [27]. The dataset has m = 30954 features and n = 606 samples (404 samples were generated by adding white Gaussian noise to the original 202 samples). Even though m increases, the runtime of the dual forms [(7) and (9)] does not increase noticeably, because n \u00d7 n quadratic programming is involved in solving the dual forms. In contrast, the time demand of solving the primal forms [(2) and (3)] increases as m grows. Consequently, if m > n, the n \u00d7 n quadratic programming would take less time, and solving the dual forms would be better.\n10\nNote that we can observe abrupt changes in runtime from both Fig. 7(a) and (b) at the point where m = n. This originates from the design of the SeDuMi toolbox. It uses an approximation based on the Farkas\u2019 lemma [21] and finds the solution y \u2208 Rm such that AT y = 0 if the solution x \u2208 Rn does not exist for Ax \u2265 0. In summary, the primal and dual forms should yield the same solution, and we can always solve either the original MCA or the proposed R-MCA problems faster by using the proposed QCLP formulation than using the original iterative method. When n > m, using the primal forms [(2) and (3)] will be advantageous; otherwise using the dual forms [(7) and (9)] will be desirable. As the primal forms and the dual forms have O(m) and O(n) variables, respectively, the same observations can be made from the computational complexity of SeDuMi, which is O(x2y2.5 + y3.5) [28] (x is the number of variables, and y is the number of independent inequalities)."}, {"heading": "5 CONCLUSION", "text": "The maximin correlation approach (MCA) was originally proposed in the context of multiple-subclass classification problems that range from the optical character recognition problem to the automated protein family prediction. The aggregate templates found by MCA work well for such applications since they can minimize the maximum misclassification risk in the correlation-based nearest-neighbor classification setup. Nonetheless, practical limitations such as susceptibility to noise, inability to handle nonlinearities, and high time demand have hindered a wider application of the MCA to real applications. To address these drawbacks, we first described how to formulate the MCA as an instance of the QCLP and presented an efficient and general solution that can replace the original iterative solution. Based on this QCLPbased formulation, we further explained how to regularize and kernelize MCA in order to render it more robust to outliers and applicable to data with nonlinearities. According to our experimental results, the proposed R-MCA successfully overcomes the limitations of the original MCA. Leveraged by the regularization, the proposed method outperformed the original MCA and the other alternatives tested in terms of classification performance. Given that the degree of regularization in R-MCA can be adjusted conveniently via a single parameter, the proposed R-MCA provides a flexible solution. In addition, we confirmed the computational benefit of the QCLP formulation and the effectiveness of kernelization in the (regularized) maximin correlation approach. We anticipate that the kernelization and regularization of MCA will make MCA more appealing to a wider range of applications that we otherwise cannot satisfactorily analyze with the original MCA.\n11"}], "references": [{"title": "An optimal global nearest neighbor metric", "author": ["K. Fukunaga", "T.E. Flick"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 6, no. 3, pp. 314\u2013318, May 1984.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1984}, {"title": "When is \u201cnearest neighbor", "author": ["K. Beyer"], "venue": "meaningful?\u201d in Database Theory\u2013ICDT\u201999. Springer,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Using discriminant eigenfeatures for image retrieval", "author": ["D.L. Swets", "J.J. Weng"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 18, no. 8, pp. 831\u2013836, 1996.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "Efficient and effective querying by image content", "author": ["C. Faloutsos"], "venue": "Journal of Intelligent Information Systems, vol. 3, no. 3-4, pp. 231\u2013262, 1994.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1994}, {"title": "P-n learning: Bootstrapping binary classifiers by structural constraints", "author": ["Z. Kalal"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, June 2010, pp. 49\u201356.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Grid-partition index: A hybrid method for nearest-neighbor queries in wireless location-based services", "author": ["B. Zheng"], "venue": "The VLDB Journal, vol. 15, no. 1, pp. 21\u201339, Jan. 2006.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Basic local alignment search tool", "author": ["S.F. Altschul"], "venue": "Journal of molecular biology, vol. 215, no. 3, pp. 403\u2013410, 1990.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1990}, {"title": "A fast knn algorithm for text categorization", "author": ["Y. Wang", "Z.-O. Wang"], "venue": "Machine Learning and Cybernetics, 2007 International Conference on, vol. 6, Aug 2007, pp. 3436\u20133441.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Diffusion decision making for adaptive k-nearest neighbor classification", "author": ["Y.-K. Noh", "F. Park", "D.D. Lee"], "venue": "Advances in Neural Information Processing Systems 25, 2012, pp. 1925\u20131933.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Deformable templates for face recognition", "author": ["A.L. Yuille"], "venue": "Journal of Cognitive Neuroscience, vol. 3, no. 1, pp. 59\u201370, 1991.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1991}, {"title": "Recognizing action at a distance", "author": ["A. Efros", "A. Berg", "G. Mori", "J. Malik"], "venue": "Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, Oct 2003, pp. 726\u2013733 vol.2.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Multiple subclass pattern recognition: A maximin correlation approach", "author": ["H.I. Avi-Itzhak", "J.A. Van Mieghem", "L. Rub"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 17, no. 4, pp. 418\u2013431, 1995.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1995}, {"title": "Robust multifont ocr system from gray level images", "author": ["F. Lebourgeois"], "venue": "Document Analysis and Recognition, 1997., Proceedings of the Fourth International Conference on, vol. 1, 1997, pp. 1\u20135 vol.1.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Application of maximin correlation analysis to classifying protein environments for function prediction", "author": ["T. Lee", "H. Min", "S.J. Kim", "S. Yoon"], "venue": "Biochemical and biophysical research communications, vol. 400, no. 2, pp. 219\u2013224, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "K-maximin clustering: a maximin correlation approach to partition-based clustering", "author": ["T. Lee", "S.J. Kim", "E.-Y. Chung", "S. Yoon"], "venue": "IEICE Electronics Express, vol. 6, no. 17, pp. 1205\u20131211, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Applications of second-order cone programming", "author": ["M.S. Lobo", "L. Vandenberghe", "S. Boyd", "H. Lebret"], "venue": "Linear Algebra and its Applications, vol. 284, no. 1-3, pp. 193\u2013228, Nov. 1998.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "An interior-point method for semidefinite programming", "author": ["C. Helmberg", "F. Rendl", "R.J. Vanderbei", "H. Wolkowicz"], "venue": "SIAM Journal on Optimization, vol. 6, pp. 342\u2013361, 1996.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1996}, {"title": "Support-Vector Networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning, vol. 20, 1995, pp. 273\u2013297.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1995}, {"title": "Learning with kernels: Support vector machines, regularization, optimization, and beyond", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "MIT press,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Characterization of the equivalence of robustification and regularization in linear, median, and matrix regression", "author": ["D. Bertsimas", "M.S. Copenhaver"], "venue": "arXiv preprint arXiv:1411.6160, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Principal component analysis", "author": ["I. Jolliffe"], "venue": "Wiley Online Library,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "Functions of positive and negative type and their connection with the theory of integral equations", "author": ["J. Mercer"], "venue": "Philos. Trans. Royal Soc. (A), vol. 83, no. 559, pp. 69\u201370, Nov. 1909.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1909}, {"title": "Hierarchical Fusion of Multiple Classifiers for Hyperspectral Data Analysis", "author": ["S. Kumar", "J. Ghosh", "M.M. Crawford"], "venue": "Pattern Analysis & Applications, vol. 5, no. 2, pp. 210\u2013220, Jun. 2002.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2002}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, November 1998.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1998}, {"title": "Analysis of hidden units in a layered network trained to classify sonar targets", "author": ["R.P. Gorman", "T.J. Sejnowski"], "venue": "Neural Networks, vol. 1, no. 1, pp. 75\u201389, 1988.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1988}, {"title": "Regional variation in gene expression in the healthy colon is dysregulated in ulcerative colitis", "author": ["C.L. Noble"], "venue": "Gut, vol. 57, no. 10, pp. 1398\u2013405, 2008.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Using sedumi 1.02, a matlab toolbox for optimization over symmetric cones", "author": ["J.F. Sturm"], "venue": "1998.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "Kernel principal component analysis", "author": ["B. Sch\u00f6lkopf", "A.J. Smola", "K.R. M\u00fcller"], "venue": "Advances in kernel methods: support vector learning, pp. 327\u2013352, 1999.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1999}, {"title": "NCBI GEO: Mining Millions of Expression Profiles - Database and Tools", "author": ["T. Barrett"], "venue": "Nucleic Acids Research, vol. 33, pp. D562\u2013D566, 2005.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "1 INTRODUCTION N EAREST neighbor (NN) classifiers [1], [2] are non-parametric methods that classify an object based on its distance to the nearest trained class.", "startOffset": 50, "endOffset": 53}, {"referenceID": 1, "context": "1 INTRODUCTION N EAREST neighbor (NN) classifiers [1], [2] are non-parametric methods that classify an object based on its distance to the nearest trained class.", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "Owing largely to their simplicity and reasonable performance in practical problems, they have been widely used for various tasks such as image retrieval [3], video indexing [4], object tracking [5], location-dependent information service [6], and sequence alignment [7].", "startOffset": 153, "endOffset": 156}, {"referenceID": 3, "context": "Owing largely to their simplicity and reasonable performance in practical problems, they have been widely used for various tasks such as image retrieval [3], video indexing [4], object tracking [5], location-dependent information service [6], and sequence alignment [7].", "startOffset": 173, "endOffset": 176}, {"referenceID": 4, "context": "Owing largely to their simplicity and reasonable performance in practical problems, they have been widely used for various tasks such as image retrieval [3], video indexing [4], object tracking [5], location-dependent information service [6], and sequence alignment [7].", "startOffset": 194, "endOffset": 197}, {"referenceID": 5, "context": "Owing largely to their simplicity and reasonable performance in practical problems, they have been widely used for various tasks such as image retrieval [3], video indexing [4], object tracking [5], location-dependent information service [6], and sequence alignment [7].", "startOffset": 238, "endOffset": 241}, {"referenceID": 6, "context": "Owing largely to their simplicity and reasonable performance in practical problems, they have been widely used for various tasks such as image retrieval [3], video indexing [4], object tracking [5], location-dependent information service [6], and sequence alignment [7].", "startOffset": 266, "endOffset": 269}, {"referenceID": 7, "context": "Approaches have been proposed to reduce the computational complexity [8] and to adaptively determine nearest neighbors (even in high-dimensional spaces) [9].", "startOffset": 69, "endOffset": 72}, {"referenceID": 8, "context": "Approaches have been proposed to reduce the computational complexity [8] and to adaptively determine nearest neighbors (even in high-dimensional spaces) [9].", "startOffset": 153, "endOffset": 156}, {"referenceID": 9, "context": "Template matching is another widely used technique that pre-computes a representative vector for each class and uses it to locate the nearest neighbor of an object [10], [11].", "startOffset": 164, "endOffset": 168}, {"referenceID": 10, "context": "Template matching is another widely used technique that pre-computes a representative vector for each class and uses it to locate the nearest neighbor of an object [10], [11].", "startOffset": 170, "endOffset": 174}, {"referenceID": 11, "context": "In multiple subclass classification problems, where each class contains more than one subclass, a template is constructed for each subclass, and then the aggregate template of a class is created based on the subclass templates [12].", "startOffset": 227, "endOffset": 231}, {"referenceID": 11, "context": "In this paper, we consider constructing the aggregate template based on the idea of the maximin correlation approach (MCA) [12].", "startOffset": 123, "endOffset": 127}, {"referenceID": 12, "context": "MCA was originally proposed for multi-font optical character recognition [13], and has been successfully applied to automated protein function prediction [14] and typography clustering [15].", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "MCA was originally proposed for multi-font optical character recognition [13], and has been successfully applied to automated protein function prediction [14] and typography clustering [15].", "startOffset": 154, "endOffset": 158}, {"referenceID": 14, "context": "MCA was originally proposed for multi-font optical character recognition [13], and has been successfully applied to automated protein function prediction [14] and typography clustering [15].", "startOffset": 185, "endOffset": 189}, {"referenceID": 15, "context": "As opposed to the iterative method employed by MCA, we reformulate it as an instance of quadratically constrained linear programming (QCLP) [16].", "startOffset": 140, "endOffset": 144}, {"referenceID": 16, "context": "In contrast, the proposed QCLP formulation can be solved with linear complexity by the interior-point methods (IPMs) [17] when coefficient matrices are positive semidefinite.", "startOffset": 117, "endOffset": 121}, {"referenceID": 17, "context": "Our formulation has some resemblance to the regularization employed by the soft-margin support vector machine (SVM) [18].", "startOffset": 116, "endOffset": 120}, {"referenceID": 18, "context": "We furthermore develop the Lagrangian dual of the regularized QCLP, which enables us to apply the kernel trick [19] to effectively handle nonlinear structures possibly embedded in data.", "startOffset": 111, "endOffset": 115}, {"referenceID": 11, "context": "Additional details can be found in [12].", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "The original MCA [12] assumes that all of the xi\u2019s are linearly independent, ||xi||2 = 1 for all xi \u2208 X , and xi xj \u2265 0 for all xi,xj \u2208 X (note that these assumptions are not required in the proposed R-MCA).", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "An iterative solution to the MCAP was proposed in [12]: the template vector u is initialized to the centroid vector and is updated at each iteration to find the optimal vector u.", "startOffset": 50, "endOffset": 54}, {"referenceID": 11, "context": "For this reason, in multi-font character recognition, the maximin template, which incorporates outlier information, results in higher accuracy than the centroid template [12], [14].", "startOffset": 170, "endOffset": 174}, {"referenceID": 13, "context": "For this reason, in multi-font character recognition, the maximin template, which incorporates outlier information, results in higher accuracy than the centroid template [12], [14].", "startOffset": 176, "endOffset": 180}, {"referenceID": 15, "context": "To propose more efficient solutions to the MCAP (1), we first formulate it as an instance of QCLP [16].", "startOffset": 98, "endOffset": 102}, {"referenceID": 16, "context": "The QCLP formulation (2) enables us to find a solution using the general IPMs [17], instead of the iterative method proposed in [12].", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "The QCLP formulation (2) enables us to find a solution using the general IPMs [17], instead of the iterative method proposed in [12].", "startOffset": 128, "endOffset": 132}, {"referenceID": 16, "context": "It is an instance of QCLP, a special type of optimization problem that can be solved globally and efficiently by the IPMs [17].", "startOffset": 122, "endOffset": 126}, {"referenceID": 15, "context": "Since the number of iterations that is necessary for IPM to find a solution is practically constant (typically from 10 to 50) [16], we can see that the QCLP (2) can be solved in O(nm +m) flops.", "startOffset": 126, "endOffset": 130}, {"referenceID": 11, "context": "For comparison, the number of flops required for the iterative method [12] is either 4mnp\u2212mp2 or 4n2p\u2212 2np +mn, depending on the implementation, where p is the number of iterations.", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "The empirical study in [12] shows that p grows nearly linearly in n.", "startOffset": 23, "endOffset": 27}, {"referenceID": 19, "context": "Bertsimas and Copenhaver recently described a unifying view of the connection between robustification and regularization [20].", "startOffset": 121, "endOffset": 125}, {"referenceID": 17, "context": "This formulation is similar to the optimization problem within the soft-margin support vector machine (SVM) [18], which is a relaxation of the original SVM.", "startOffset": 108, "endOffset": 112}, {"referenceID": 20, "context": "Note that minimizing the form vCv occurs frequently in multivariate data analysis, such as the principal component analysis [22].", "startOffset": 124, "endOffset": 128}, {"referenceID": 18, "context": "The mapping to and the computation in such high-dimensional spaces can be costly, if not impossible, but when the input data are acessed only through inner products, we can use the so-called kernel trick [19] to perform implicit mapping and efficient computation.", "startOffset": 204, "endOffset": 208}, {"referenceID": 22, "context": "Name n m Number of classes (description) KSC [24] 5211 176 13 (land cover types) MNIST [25] 10000 784 10 (digits \u20180\u2019\u2013\u20189\u2019) SONAR [26] 208 60 2 (rock or mine) GEO [27] 606 30954 2 (ulcerative colitis patient or not) 3D-NUT 272 3 2 (core or shell)", "startOffset": 45, "endOffset": 49}, {"referenceID": 23, "context": "Name n m Number of classes (description) KSC [24] 5211 176 13 (land cover types) MNIST [25] 10000 784 10 (digits \u20180\u2019\u2013\u20189\u2019) SONAR [26] 208 60 2 (rock or mine) GEO [27] 606 30954 2 (ulcerative colitis patient or not) 3D-NUT 272 3 2 (core or shell)", "startOffset": 87, "endOffset": 91}, {"referenceID": 24, "context": "Name n m Number of classes (description) KSC [24] 5211 176 13 (land cover types) MNIST [25] 10000 784 10 (digits \u20180\u2019\u2013\u20189\u2019) SONAR [26] 208 60 2 (rock or mine) GEO [27] 606 30954 2 (ulcerative colitis patient or not) 3D-NUT 272 3 2 (core or shell)", "startOffset": 128, "endOffset": 132}, {"referenceID": 25, "context": "Name n m Number of classes (description) KSC [24] 5211 176 13 (land cover types) MNIST [25] 10000 784 10 (digits \u20180\u2019\u2013\u20189\u2019) SONAR [26] 208 60 2 (rock or mine) GEO [27] 606 30954 2 (ulcerative colitis patient or not) 3D-NUT 272 3 2 (core or shell)", "startOffset": 161, "endOffset": 165}, {"referenceID": 21, "context": "where Kij = k(xi,xj) for a Mercer kernel k [23].", "startOffset": 43, "endOffset": 47}, {"referenceID": 26, "context": "For our experiments, we implemented the proposed QCLP-based MCA and R-MCA solvers using SeDuMi software, a MATLAB toolbox for optimization over symmetric cones [28].", "startOffset": 160, "endOffset": 164}, {"referenceID": 11, "context": "For comparison, we also prepared implementations of the original iterative solution to MCA as described in [12], the support vector machine (SVM), and the logistic regression.", "startOffset": 107, "endOffset": 111}, {"referenceID": 22, "context": "We used the Kennedy Space Center (KSC) dataset [24], which contains 5211 vectors with 176 dimensions.", "startOffset": 47, "endOffset": 51}, {"referenceID": 23, "context": "To visualize the effects of varying \u03bb, we utilized the MNIST database of handwritten digits [25].", "startOffset": 92, "endOffset": 96}, {"referenceID": 24, "context": "3 Effect of Regularization on Classification To see the regularization effects in the context of classification, we carried out binary classification of the SONAR data [26], which consists of 111 mine-reflected and 97 rock-reflected sonar signals of 60 dimensions each.", "startOffset": 168, "endOffset": 172}, {"referenceID": 24, "context": "According to the experimental results from using neural networks in [26], nonlinearities exist in the distribution characteristics of the SONAR data.", "startOffset": 68, "endOffset": 72}, {"referenceID": 27, "context": "We thus preprocessed the data using the kernel PCA [29] with the Gaussian kernel (\u03c3 = 1).", "startOffset": 51, "endOffset": 55}, {"referenceID": 11, "context": "5 Comparison of Execution Time with MCA We compare the runtime of the proposed QCLP-based solution and the original iterative solution [12] to the maximin correlation approach.", "startOffset": 135, "endOffset": 139}, {"referenceID": 28, "context": "We used the NCBI GEO microarray dataset [30] (the accession number: GSE11223), which provides the regional variation of gene expression in ulcerative colitis patients [27].", "startOffset": 40, "endOffset": 44}, {"referenceID": 25, "context": "We used the NCBI GEO microarray dataset [30] (the accession number: GSE11223), which provides the regional variation of gene expression in ulcerative colitis patients [27].", "startOffset": 167, "endOffset": 171}, {"referenceID": 11, "context": "PSfrag replacements Iterative method [12] R-MCA dual (7) R-MCA primal (3) MCA dual (9) MCA primal (2)", "startOffset": 37, "endOffset": 41}, {"referenceID": 26, "context": "As the primal forms and the dual forms have O(m) and O(n) variables, respectively, the same observations can be made from the computational complexity of SeDuMi, which is O(xy + y) [28] (x is the number of variables, and y is the number of independent inequalities).", "startOffset": 181, "endOffset": 185}], "year": 2017, "abstractText": "Robust classification becomes challenging when classes contain multiple subclasses. Examples include multi-font optical character recognition and automated protein function prediction. In correlation-based nearest-neighbor classification, the maximin correlation approach (MCA) provides the worst-case optimal solution by minimizing the maximum misclassification risk through an iterative procedure. Despite the optimality, the original MCA has drawbacks that have limited its wide applicability in practice. That is, the MCA tends to be sensitive to outliers, cannot effectively handle nonlinearities in datasets, and suffers from having high computational complexity. To address these limitations, we propose an improved solution, named regularized maximin correlation approach (R-MCA). We first reformulate MCA as a quadratically constrained linear programming (QCLP) problem, incorporate regularization by introducing slack variables into the primal problem of the QCLP, and derive the corresponding Lagrangian dual. The dual formulation enables us to apply the kernel trick to R-MCA so that it can better handle nonlinearities. Our experimental results demonstrate that the regularization and kernelization make the proposed R-MCA more robust and accurate for various classification tasks than the original MCA. Furthermore, when the data size or dimensionality grows, R-MCA runs substantially faster by solving either the primal or dual (whichever has a smaller variable dimension) of the QCLP. Index Terms nearest neighbor, correlation, maximin, SOCP, QCLP, QP, regularization, kernel trick. \u2726", "creator": "LaTeX with hyperref package"}}}