{"id": "1503.00030", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2015", "title": "Parsing as Reduction", "abstract": "We reduce phrase-representation parsing to dependency parsing. Our reduction is grounded on a new intermediate representation, \"head-ordered dependency trees\", shown to be isomorphic to constituent trees. By encoding order information in the dependency labels, we show that any off-the-shelf, trainable dependency parser can be used to produce constituents. When this parser is non-projective, we can perform discontinuous parsing in a very natural manner. Despite the simplicity of our approach, experiments show that the resulting parsers are on par with strong baselines, such as the Berkeley parser for English and the best single system in the SPMRL-2014 shared task. Results are particularly striking for discontinuous parsing of German, where we surpass the current state of the art by a wide margin. This parsers can also be used to construct a generic, and non-overloaded, non-structured, parser for language-wide, cross-compatible languages such as Java, Java and Objective-C, or to extend or replace text from the native language.", "histories": [["v1", "Fri, 27 Feb 2015 22:52:37 GMT  (83kb,D)", "http://arxiv.org/abs/1503.00030v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["daniel fern\u00e1ndez-gonz\u00e1lez", "andr\u00e9 f t martins"], "accepted": true, "id": "1503.00030"}, "pdf": {"name": "1503.00030.pdf", "metadata": {"source": "CRF", "title": "Parsing as Reduction", "authors": ["Daniel Fern\u00e1ndez-Gonz\u00e1lez", "Andr\u00e9 F. T. Martins", "D. Afonso Henriques"], "emails": ["danifg@uvigo.es,", "atm@priberam.pt"], "sections": [{"heading": "1 Introduction", "text": "Constituent parsing is a central problem in NLP\u2014one at which statistical models trained on treebanks have excelled (Charniak, 1996; Klein and Manning, 2003; Petrov and Klein, 2007). However, most existing parsers are slow, since they need to deal with a heavy grammar constant. Dependency parsers are generally faster, but less informative, since they do not produce constituents, which are often required by downstream applications (Johansson and Nugues, 2008; Wu et al., 2009; Berg-Kirkpatrick et al., 2011; Elming et al., 2013). How to get the best of both worlds?\nCoarse-to-fine decoding (Charniak and Johnson, 2005) and shift-reduce parsing (Sagae and Lavie, 2005; Zhu et al., 2013) were a step for-\n\u2217 This research was carried out during an internship at Priberam Labs.\nward to accelerate constituent parsing, but runtimes still lag those of dependency parsers. This is only made worse if discontinuous constituents are allowed\u2014such discontinuities are convenient to represent wh-movement, scrambling, extraposition, and other linguistic phenomena common in free word order languages. While non-projective dependency parsers, which are able to model such phenomena, have been widely developed in the last decade (Nivre et al., 2007; McDonald et al., 2006; Martins et al., 2013), discontinuous constituent parsing is still taking its first steps (Maier and S\u00f8gaard, 2008; Kallmeyer and Maier, 2013).\nIn this paper, we show that an off-the-shelf, trainable, dependency parser is enough to build a highly-competitive constituent parser. This (surprising) result is based on a reduction of constituent to dependency parsing, followed by a simple post-processing procedure to recover unaries. Unlike other constituent parsers, ours does not require estimating a grammar, nor binarizing the treebank. Moreover, when the dependency parser is non-projective, our method can perform discontinuous constituent parsing in a very natural way.\nKey to our approach is the notion of headordered dependency trees (shown in Figure 1): by endowing dependency trees with this additional layer of structure, we show that they become isomorphic to constituent trees. We encode this structure as part of the dependency labels, enabling a dependency-to-constituent conversion. Hall and Nivre (2008) attempted a related conversion to parse German, but their complex encoding scheme blows up the number of arc labels, affecting the final parser\u2019s quality. By contrast, our light encoding achieves a 10-fold decrease in the number of labels, translating into more accurate parsing.\nWhile simple, our reduction-based parsers are on par with the Berkeley parser for English (Petrov and Klein, 2007), and with the best single system in the recent SPMRL shared task (Seddah et al.,\nar X\niv :1\n50 3.\n00 03\n0v 1\n[ cs\n.C L\n] 2\n7 Fe\nb 20\n15\n2014), for eight morphologically rich languages. For discontinuous parsing, we surpass the current state of the art by a wide margin on two German datasets (TIGER and NEGRA), while achieving fast parsing speeds. Our parsers will be released along with this paper as accompanying software."}, {"heading": "2 Background", "text": "We start by reviewing constituent and dependency representations, and setting up the notation. Following Kong and Smith (2014), we use c-/d- prefixes for convenience (e.g., we write c-parser for constituent parser and d-tree for dependency tree)."}, {"heading": "2.1 Constituent Trees", "text": "Constituent-based representations are commonly seen as derivations according to a context-free grammar (CFG). Here, we focus on properties of the c-trees, rather than of the grammars used to generate them. We consider a broad scenario that permits c-trees with discontinuities, such as the ones derived with linear context-free rewriting systems (LCFRS; Vijay-Shanker et al. (1987)). We also assume that the c-trees are lexicalized.\nFormally, let w1w2 . . . wL be a sentence, where wi denotes the word in the ith position. A ctree is a rooted tree whose leaves are the words {wi}Li=1, and whose internal nodes (constituents) are represented as a tuple \u3008Z, h, I\u3009, where Z is a non-terminal symbol, h \u2208 {1, . . . , L} indicates the lexical head, and I \u2286 {1, . . . , L} is the node\u2019s yield. Each word\u2019s parent is a pre-terminal unary node of the form \u3008pi, i, {i}\u3009, where pi denotes the word\u2019s part-of-speech (POS) tag. The yields and lexical heads are defined so that for every constituent \u3008Z, h, I\u3009 with children {\u3008Xk,mk,Jk\u3009}Kk=1, (i) we have I = \u22c3K k=1 Jk; and (ii) there is a unique k such that h = mk. This kth node (called the head-child node) is commonly chosen applying an handwritten set of head rules (Collins, 1999; Yamada and Matsumoto, 2003).\nA c-tree is continuous if all nodes \u3008Z, h, I\u3009 have a contiguous yield I, and discontinuous otherwise. Trees derived from a CFG are always continuous; those derived by a LCFRS may have discontinuities, the yield of a node being a union of spans, possibly with gaps in the middle. Figure 1 shows an example of a continuous and a discontinuous c-tree. Discontinuous c-trees have crossing branches, if the leaves are drawn in left-to-right surface order. An internal node which is not a pre-\nterminal is called a proper node. A node is called unary if it has exactly one child. A c-tree without unary proper nodes is called unaryless. If all proper nodes have exactly two children then it is called a binary c-tree. Continuous binary trees may be regarded as having been generated by a CFG in Chomsky normal form.\nPrior work. There has been a long string of work in statistical c-parsing, shifting from simple models (Charniak, 1996) to more sophisticated ones using structural annotation (Johnson, 1998; Klein and Manning, 2003), latent grammars (Matsuzaki et al., 2005; Petrov and Klein, 2007), and lexicalization (Eisner, 1996; Collins, 1999). An orthogonal line of work uses ensemble or reranking strategies to further improve accuracy (Charniak and Johnson, 2005; Huang, 2008; Bjo\u0308rkelund et al., 2014). Discontinuous c-parsing is considered a much harder problem, involving mildly context-sensitive formalisms such as LCFRS or range concatenation grammars, with treebankderived c-parsers exhibiting near-exponential runtime (Kallmeyer and Maier, 2013, Figure 27). To speed up decoding, prior work has considered restrictons, such as bounding the fan-out (Maier et al., 2012) and requiring well-nestedness (Kuhlmann and Nivre, 2006; Go\u0301mez-Rodr\u0131\u0301guez et al., 2010). Other approaches eliminate the discontinuities via tree transformations (Boyd, 2007; Ku\u0308bler et al., 2008), sometimes as a pruning step followed by reranking (van Cranenburgh and Bod, 2013). However, reported runtimes are still superior to 10 seconds per sentence, which is not practical. Recently, Versley (2014a) proposed an easyfirst approach that leads to considerable speedups, but is less accurate. In this paper, we design fast discontinuous c-parsers that outperform all the ones above by a wide margin, with similar runtimes as Versley (2014a)."}, {"heading": "2.2 Dependency Trees", "text": "In this paper, we use d-parsers as a black box to parse constituents. Given a sentence w1 . . . wL, a d-tree is a directed tree spanning all the words in the sentence.1 Each arc in this tree is a tuple \u3008h,m, `\u3009, expressing a typed dependency relation ` between the head word wh and the modifier wm.\nA d-tree is projective if for every arc \u3008h,m, `\u3009 1We assume throughout that dependency trees have a single root among {w1, . . . , wL}. Therefore, there is no need to consider an extra root symbol, as often done in the literature.\nthere is a directed path from h to all words that lie between h and m in the surface string (Kahane et al., 1998). Projective d-trees can be obtained from continuous c-trees by reading off the lexical heads and dropping the internal nodes (Gaifman, 1965). However, this relation is many-to-one: as shown in Figure 2, several c-trees may project onto the same d-tree, differing on their flatness and on left or right-branching decisions. In the next section, we introduce the concept of head-ordered d-trees and express one-to-one mappings between these two representations.\nPrior work. There has been a considerable amount of work developing rich-feature d-parsers. While projective d-parsers can use dynamic pro-\ngramming (Eisner and Satta, 1999; Koo and Collins, 2010), non-projective d-parsers typically rely on approximate decoders, since the underlying problem is NP-hard beyond arc-factored models (McDonald and Satta, 2007). An alternative are transition-based d-parsers (Nivre et al., 2006; Zhang and Nivre, 2011), which achieve observed linear time. Since d-parsing algorithms do not have a grammar constant, typical implementations are significantly faster than c-parsers (Rush and Petrov, 2012; Martins et al., 2013). The key contribution of this paper is to reduce c-parsing to dparsing, allowing to bring these runtimes closer."}, {"heading": "3 Head-Ordered Dependency Trees", "text": "We next endow d-trees with another layer of structure, namely order information. In this framework, not all modifiers of a head are \u201cborn equal.\u201d Instead, their attachment to the head occurs as a sequence of \u201cevents,\u201d which reflect the head\u2019s preference for attaching some modifiers before others. As we will see, this additional structure will undo the ambiguity expressed in Figure 2."}, {"heading": "3.1 Strictly Ordered Dependency Trees", "text": "Let us start with the simpler case where the attachment order is strict. For each head word h with modifiers Mh = {m1, . . . ,mK}, we endow Mh with a strict order relation \u227ah, so we can organize all the modifiers of h as a chain, mi1 \u227ah\nmi2 \u227ah . . . \u227ah miK . We regard this chain as reflecting the order by which words are attached (i.e., if mi \u227ah mj this means that \u201cmi is attached to h before mj\u201d). We represent this graphically by decorating d-arcs with indices (#1,#2, . . .) to denote the order of events, as we do in Figure 1.\nA d-tree endowed with a strict order for each head is called a strictly ordered d-tree. We establish below a correspondence between strictly ordered d-trees and binary c-trees. Before doing so, we need a few more definitions about c-trees. For each word position h \u2208 {1, . . . , L}, we define \u03c8(h) as the node higher in the c-tree whose lexical head is h. We call the path from \u03c8(h) down to the pre-terminal ph the spine of h. We may regard a c-tree as a set of L spines, one per word, which attach to each other to form a tree (Carreras et al., 2008). We then have the following\nProposition 1. Binary c-trees and strictly-ordered d-trees are isomorphic, i.e., there is a one-to-one correspondence between the two sets, where the number of symbols is preserved.\nProof. We use the construction in Figure 3. We will show that, given an arbitrary strictly-ordered d-tree D, we can perform an invertible transformation to turn it into a binary c-tree C; and viceversa. Let D be given. We visit each node h \u2208 {1, . . . , L} and split it into K + 1 nodes, where K = |Mh|, organized as a linked list, as Figure 3 illustrates (this will become the spine of h in the c-tree). For each modifier mk \u2208 Mh with m1 \u227ah . . . \u227ah mK , move the tail of the arc \u3008h,mk, Zk\u3009 to the (K + 1 \u2212 k)th node of the linked list and assign the label Zk to this node, letting h be its lexical head. Since the incoming and outgoing arcs of the linked list component are the same as in the original node h, the tree structure is preserved. After doing this for every h, add the leaves and propagate the yields bottom up. It is straightforward to show that this procedure yields a valid binary c-tree. Since there is no loss\nof information (the orders \u227ah are implied by the order of the nodes in each spine), this construction can be inverted to recover the original d-tree. Conversely, if we start with a binary c-tree, traverse the spine of each h, and attach the modifiers m1 \u227ah . . . \u227ah mK in order, we get a strictly ordered d-tree (also an invertible procedure)."}, {"heading": "3.2 Weakly Ordered Dependency Trees", "text": "Next, we relax the strict order assumption, restricting the modifier sets Mh = {m1, . . . ,mK} to be only weakly ordered. This means that we can partition the K modifiers into J equivalence classes, Mh = \u22c3J j=1 M\u0304 j h, and define a strict order \u227ah on the quotient set: M\u03041h \u227ah . . . \u227ah M\u0304Jh . Intuitively, there is still a sequence of events (1 to J), but now at each event j it may happen that multiple modifiers (the ones in the equivalence set M\u0304 jh) are simultaneously attached to h. A weakly ordered d-tree is a d-tree endowed with a weak order for each head and such that any pairm,m\u2032 in the same equivalence class (written m \u2261h m\u2032) receive the same dependency label `.\nWe now show that Proposition 1 can be generalized to weakly ordered d-trees.\nProposition 2. Unaryless c-trees and weaklyordered d-trees are isomorphic.\nProof. This is a simple extension of Proposition 1. The construction is the same as in Figure 3, but now we can collapse some of the nodes in the linked list, originating more than one modifier attaching to the same position of the spine\u2014this is only possible for sibling arcs with the same index and the same arc label. Note, however, that if we started with a c-tree with unary nodes and tried to invert this procedure to obtain a d-tree, the unary nodes would be lost, since they do not involve attachment of modifiers. In a chain of unary nodes, only the last node would be recovered when inverting this transformation.\nWe emphasize that Propositions 1\u20132 hold without blowing up the number of symbols. That is, the dependency label alphabet is exactly the same as the set of phrasal symbols in the constituent representations. Algorithms 1\u20132 convert back and forth between the two formalisms, performing the construction of Figure 3. Both algorithms run in linear time with respect to the size of the sentence.\nAlgorithm 1 Conversion from c-tree to d-tree Input: c-tree C. Output: head-ordered d-tree D. 1: Nodes := GETPOSTORDERTRAVERSAL(C). 2: Set j(h) := 1 for every h = 1, . . . , L. 3: for v := \u3008Z, h, I\u3009 \u2208 Nodes do 4: for every u := \u3008X,m,J \u3009 which is a child of v do 5: if m 6= h then 6: Add toD an arc \u3008h,m,Z\u3009, and put it in M\u0304 j(h)h . 7: end if 8: end for 9: Set j(h) := j(h) + 1. 10: end for\nAlgorithm 2 Conversion from d-tree to c-tree Input: head-ordered d-tree D. Output: c-tree C. 1: Nodes := GETPOSTORDERTRAVERSAL(D). 2: for h \u2208 Nodes do 3: Create v := \u3008ph, h, {h}\u3009 and set \u03c8(h) := v. 4: Sort Mh(D), yielding M\u03041h \u227ah M\u03042h \u227ah . . . \u227ah M\u0304Jh . 5: for j = 1, . . . , J do 6: Let Z be the label in {\u3008h,m,Z\u3009 | m \u2208 M\u0304 jh}. 7: Obtain c-nodes \u03c8(h) = \u3008X,h, I\u3009 and \u03c8(m) =\n\u3008Ym,m,Jm\u3009 for all m \u2208 M\u0304 jh. 8: Add c-node v := \u3008Z, h, I \u222a \u22c3 m\u2208M\u0304j\nh Jm\u3009 to C.\n9: Set \u03c8(h) and {\u03c8(m) |m \u2208 M\u0304 jh} as children of v. 10: Set \u03c8(h) := v. 11: end for 12: end for"}, {"heading": "3.3 Continuous and Projective Trees", "text": "What about the more restricted class of projective d-trees? Can we find an equivalence relation with continuous c-trees? In this section, we give a precise answer to this question. It turns out that we need an additional property, illustrated in Figure 4.\nWe say that \u227ah has the nesting property iff closer words in the same direction are always attached first, i.e., iff h < mi < mj or h > mi > mj implies that either mi \u2261h mj or mi \u227ah mj . A weakly-ordered d-tree which is projective and whose orders \u227ah have the nesting property for every h is called a nested-weakly ordered projective d-tree. We then have the following result.\nProposition 3. Continuous unaryless c-trees and nested-weakly ordered projective d-trees are isomorphic.\nProof. We need to show that (i) Algorithm 1, when applied to a continuous c-tree C, retrieves a head ordered d-tree D which is projective and has the nesting property, (ii) vice-versa for Algorithm 2. To see (i), note that the projectiveness of D is ensured by the well-known result of Gaifman (1965) about the projection of continuous trees. To show that it satisfies the nesting property, note\nthat nodes higher in the spine of a word h are always attached by modifiers farther apart (otherwise edges in C would cross, which cannot happen for a continuous C). To prove (ii), we use induction. We need to show that every created c-node in Algorithm 2 has a contiguous span as yield. The base case (line 3) is trivial. Therefore, it suffices to show that in line 8, assuming the yields of (the current) \u03c8(h) and each \u03c8(m) are contiguous spans, the union of these yields is also contiguous. Consider the node v when these children have been appended (line 9), and choose m \u2208 M\u0304 jh arbitrarily. We only need to show that for any d between h and m, d belongs to the yield of v. Since D is projective and there is a d-arc between h and m, we have that d must descend from h. Furthermore, since projective trees cannot have crossing edges, we have that h has a unique child a, also between h and m, which is an ancestor of d (or d itself). Since a is between h and m, from the nesting property, we must have \u3008h,m, `\u3009 6\u227ah \u3008h, a, `\u2032\u3009 Therefore, since we are processing the modifiers in order, we have that \u03c8(a) is already a descendent of v after line 9, which implies that the yield of \u03c8(a) (which must include d, since d descends from a) must be contained in the yield of v.\nTogether, Propositions 1\u20133 have as corollary that nested-strictly ordered projective d-trees are in a one-to-one correspondence with binary continuous c-trees. The intuition is simple: if \u227ah has the nesting property, then, at each point in time, all one needs to decide about the next event is whether to attach the closest available modifier on the left or on the right. This corresponds to choosing between left-branching or right-branching in a ctree. While this is potentially interesting for most continuous c-parsers, which work with binarized c-trees when running the CKY algorithm, our c-\nparsers (to be described in \u00a74) do not require any binarization since they work with weakly-ordered d-trees, using Proposition 2."}, {"heading": "4 Reduction-Based Constituent Parsers", "text": "We next show how to use the equivalence results obtained in the previous section to design cparsers when only a trainable d-parser is available.\nGiven a c-treebank provided as input, our procedure is outlined as follows:\n1. Convert the c-treebank to dependencies (Algorithm 1).\n2. Train a labeled d-parser on this treebank.\n3. For each test sentence, run the labeled d-parser and convert the predicted d-tree into a c-tree without unary nodes (Algorithm 2).\n4. Do post-processing to recover unaries.\nThe next subsections describe each of these steps. Along the way, we illustrate with experiments using the English Penn Treebank (Marcus et al., 1993), which we lexicalized by applying the head rules of Collins (1999).2"}, {"heading": "4.1 Dependency Encoding", "text": "The first step is to convert the c-treebank to headordered dependencies, which we do using Algorithm 1. If the original treebank has discontinuous c-trees, we end up with non-projective d-trees or with violations of the nested property, as established in Proposition 3. We handle this gracefully by training a non-projective d-parser in the subsequent stage (see \u00a74.2). Note also that this conversion drops the unary nodes (a consequence of Proposition 2). These nodes will be recovered in the last stage, as described in \u00a74.4.\nSince in this paper we are assuming that only an off-the-shelf d-parser is available, we need to convert head-ordered d-trees to plain d-trees. We do so by encoding the order information in the dependency labels. We tried two different strategies to do that. The first one is a direct encoding, where we just append suffixes #1, #2, etc., as illustrated in Figure 1. A disadvantage is that the number of dependency labels may grow unbounded with the treebank size, since we may encounter complex substructures where the event sequences are long.\n2We train on \u00a702\u201321, use \u00a722 for validation, and test on \u00a723. We predict automatic POS tags with TurboTagger (Martins et al., 2013), with 10-fold jackknifing on the training set.\nThe second strategy is a delta-encoding scheme where, rather than writing the absolute indices in the dependency label, we write the differences between consecutive indices.3 We used this strategy for the continuous treebanks only, whose d-trees are guaranteed to satisfy the nested property.\nFor comparison, we implemented a third strategy replicating the encoding proposed by Hall and Nivre (2008), which we call H&N-encoding. This scheme concatenates all the c-nodes\u2019 labels in the modifier\u2019s spine with the attachment position in the head\u2019s spine (for example, in Figure 3, if the modifier m2 has a spine with nodes X1, X2, X3, the generated d-label would be X1|X2|X3#2; our direct encoding scheme generates Z2#2 instead). Since their strategy encodes the entire spines into complex arc labels, many such labels will be generated, leading to slower runtimes and poorer generalization, as we will see.\nFor the training portion of the English PTB, which contains 27 non-terminal symbols (excluding the POS tags), the direct encoding strategy yields 75 labels, while delta encoding yields 69 labels (averaging 2.6 indices per symbol). By contrast, the HN-encoding procedure yields 731 labels, more than 10 times as many. We later show (in Table 1) that delta-encoding leads to a slightly higher c-parsing accuracy than direct encoding, and that both strategies are considerably more accurate than H&N-encoding."}, {"heading": "4.2 Training the Labeled Dependency Parser", "text": "The next step is to train a labeled d-parser on the converted treebank. If we are doing continuous cparsing, we train a projective d-parser; otherwise we train a non-projective one.\nIn our experiments, we found it advantageous to perform labeled d-parsing in two stages, as done by McDonald et al. (2006): first, train an unlabeled d-parser; then, train a dependency labeler.4 Table 1 compares this approach against a oneshot strategy, experimenting with various off-theshelf d-parsers: MaltParser (Nivre et al., 2007), MSTParser (McDonald et al., 2005), ZPar (Zhang\n3For example, if #1,#3,#4 and #2,#3,#3,#5 are respectively the sequence of indices from the head to the left and to the right, we encode these sequences as #1,#2,#1 and #2,#1,#0,#2 (using 3 distinct indices instead of 5).\n4The reason why a two-stage approach is preferable is that one-shot d-parsers, for efficiency reasons, use label features parsimoniously. However, for our reduction approach, the dependency labels are crucial and strongly interdependent, since they jointly encode the constituent structure.\nand Nivre, 2011), and TurboParser (Martins et al., 2013), all with the default settings. For TurboParser, we used basic, standard and full models.\nOur separate d-labeler receives as input a backbone d-structure and predicts a label for each arc. For each head h, we decode the modifiers\u2019 labels independently from the other heads, using a simple sequence model, which contains features of the form \u03c6(h,m, `) and \u03c6(h,m,m\u2032, `, `\u2032), where m and m\u2032 are two consecutive modifiers (either on the same side or on opposite sides of the head) and ` and `\u2032 are their labels. We used the same arc label features \u03c6(h,m, `) as TurboParser. For \u03c6(h,m,m\u2032, `, `\u2032), we use the POS triplet \u3008ph, pm, pm\u2032\u3009, plus unilexical versions of this triplet, where each of the three POS is replaced by the word form. Both features are conjoined with the label pair ` and `\u2032. Decoding under this model can be done by running the Viterbi algorithm independently for each head. The runtime is almost negligible compared with the time to parse: it took 2.1 seconds to process PTB \u00a722, a fraction of about 5% of the total runtime."}, {"heading": "4.3 Decoding into Unaryless Constituents", "text": "After training the labeled d-parser, we can run it on the test data. Then, we need to convert the predicted d-tree into a c-tree without unaries.\nTo accomplish this step, we first need to recover, for each head h, the weak order of its modifiers Mh. We do this by looking at the predicted dependency labels, extract the event indices j, and use them to build and sort the equivalent classes {M\u0304 jh} J j=1. If two modifiers have the same index j, we force them to have consistent labels (by always choosing the label of the modifier which is the closest to the head). For continuous c-parsing, we also decrease the index j of the modifier closer to the head as much as necessary to make sure that the nesting property holds. In PTB \u00a722, these corrections were necessary only for 0.6% of the tokens. Having done this, we use Algorithm 2 to obtain a predicted c-tree without unary nodes."}, {"heading": "4.4 Recovery of Unary Nodes", "text": "Finally, the last stage is to recover the unary nodes. Given a unaryless c-tree as input, we predict unaries by running independent multi-class classifiers at each node in the tree (a simple unstructured task). Each class is either NULL (in which case no unary node is appended to the current node) or a concatenation of unary node labels (e.g.,\nS->ADJP for a node JJ); we obtained 64 classes by processing the training sections of the PTB, the fraction of unary nodes being about 11% of the total number of non-terminal nodes. To reduce complexity, for each node symbol we only consider classes that have been observed for that symbol in the training data. In PTB \u00a722, we obtained an average of 9.9 candidate labels per node occurrence.\nThese classifiers are trained on the original ctreebank, stripping off unary nodes and trained to recover those nodes. We used the following features (conjoined with the class and with a flag indicating if the node is a pre-terminal):\n\u2022 The production rules above and beneath the node (e.g., S->NP VP and NP->DT NN);\n\u2022 The node\u2019s label, alone and conjoined with the parent\u2019s label or the left/right sibling\u2019s label;\n\u2022 The leftmost and rightmost word/lemma/POS tag/morpho-syntactic tags in the node\u2019s yield;\n\u2022 If the left/right node is a pre-terminal, the word/lemma/morpho-syntactic tags beneath.\nThis is a relatively easy task: when gold unaryless c-trees provided as input, we obtain an EVALB F1-score of 99.43%. This large figure is explained by the fact that there are few unary nodes in the gold data, so this module does not impact the final parser as much as the d-parser. Being a lightweight unstructured task, this step took only 0.7 seconds to run on PTB \u00a722, representing a tiny fraction (less than 2%) of the total runtime.\nTable 1 reports the accuracies obtained with the d-parser followed by the unary predictor. Since two-stage TP-Full with delta-encoding is the best strategy, we use this configuration in the subsequent experiments."}, {"heading": "5 Experiments", "text": "We now compare our reduction-based parsers with other state-of-the-art c-parsers in a variety of treebanks, both continuous and discontinuous."}, {"heading": "5.1 Results on the English PTB", "text": "Table 2 shows the accuracies and speeds on the English PTB \u00a723. We can see that our simple reduction-based c-parser surpasses the three Stanford parsers (Klein and Manning, 2003; Socher et al., 2013, and Stanford Shift-Reduce), and is on par with the Berkeley parser (Petrov and Klein, 2007), while being more than 5 times faster. The\nbest supervised competitor is the recent shiftreduce parser of Zhu et al. (2013), which achieves slightly better accuracy and speed. Our technique has the advantage of being flexible: since the time for d-parsing is the dominating factor (see \u00a74.4), plugging a faster d-parser automatically yields a faster c-parser. Orthogonal techniques, such as semi-supervised training and reranking, can also be applied to our parser to boost its performance."}, {"heading": "5.2 Results on the SPMRL Datasets", "text": "We experimented with datasets for eight morphologically rich languages, from the SPMRL14 shared task (Seddah et al., 2014).5 We used the official training, development and test sets with the provided predicted POS tags, and different lexicalization rules for each language. For French and\n5We left out the Arabic dataset for licensing reasons.\nGerman we used the head rules detailed in DybroJohansen (2004) and Rehbein (2009), respectively. For Basque, Hungarian and Korean, we always take the rightmost modifier as head-child node. For Hebrew and Polish we use the leftmost modifier instead. For Swedish we induce head rules from the provided dependency treebank, as described in Versley (2014b). These choices were based on dev-set experiments.\nTable 3 shows the results. For all languages except French, our system outperforms the Berkeley parser (Petrov and Klein, 2007), with or without prescribed POS tags. Our average F1-scores are superior to the best single parser6 participating in the shared task (Crabbe\u0301 and Seddah, 2014), and to the system of Hall et al. (2014), achieving the best results for 4 out of 8 languages."}, {"heading": "5.3 Results on the Discontinuous Treebanks", "text": "Finally, we experimented on two widely-used discontinuous German treebanks: TIGER (Brants et al., 2002) and NEGRA (Skut et al., 1997). For the former, we used two different splits: TIGERSPMLR, provided in the SPMRL14 shared task; and TIGER-H&N, used by Hall and Nivre (2008). For NEGRA, we used the standard splits. In these experiments, we skipped the unary recovery stage, since very few unary nodes exist in the data.7 For the TIGER-SPMRL dataset, we used the predicted\n6By \u201csingle parser\u201d we mean a system which does not use ensemble or reranking techniques.\n7NEGRA has no unaries; for the TIGER-SPMRL and H&N dev-sets, the fraction of unaries is 1.45% and 1.01%.\nPOS tags provided in the shared task. For TIGERH&N and NEGRA, we predicted POS tags with TurboTagger. The treebanks were lexicalized using the head-rule sets of Rehbein (2009). For comparison to related work, a sentence length cut-off of 30, 40 and 70 was applied during the evaluation.\nTable 5.3 shows that our approach outperforms all the competitors considerably, achieving stateof-the-art accuracies for both datasets. The best competitor, van Cranenburgh and Bod (2013), is more than 3 points behind, both in TIGER-H&N and in NEGRA. Our reduction-based parsers are also much faster: van Cranenburgh and Bod (2013) report 3 hours to parse NEGRA with L < 40. Our system parses all NEGRA sentences (regardless of length) in 27.1 seconds, which corresponds to a rate of 618 toks/s. This approaches the speed of the easy-first system of Versley (2014a), who reports runtimes in the range 670\u2013920 toks/s., but is much less accurate."}, {"heading": "6 Related Work", "text": "Conversions between constituents and dependencies have been considered by De Marneffe et al. (2006) in the forward direction, and by Collins et al. (1999) and Xia and Palmer (2001) in the backward direction, toward the construction of multirepresentational treebanks (Xia et al., 2008). This prior work aimed at linguistically sound conversions, involving grammar-specific transformation rules to handle the kind of ambiguities expressed in Figure 2. Our work differs in that we are not concerned about the linguistic plausibility of our conversions, but only with the formal aspects that underlie the two representations.\nThe work most related to ours is Hall and Nivre (2008), who also convert dependencies to con-\nstituents to prototype a c-parser for German. Their encoding strategy is compared to ours in \u00a74.1: they encode the entire spines into the dependency labels, which become rather complex and numerous. A similar strategy has been used by Versley (2014a) for discontinuous c-parsing. Both are largely outperformed by our system, as shown in \u00a75.3. The crucial difference is that we encode only the top node\u2019s label and its position in the spine\u2014 besides being a much lighter representation, ours has an interpretation as a weak ordering, leading to the isomorphisms expressed in Propositions 1\u20133.\nJoint constituent and dependency parsing have been tackled by Carreras et al. (2008) and Rush et al. (2010), but the resulting parsers, while accurate, are more expensive than a single c-parser. Very recently, Kong et al. (2015) proposed a much cheaper pipeline in which d-parsing is performed first, followed by a c-parser constrained to be consistent with the predicted d-structure. Our work differs in which we do not need to run a c-parser in the second stage\u2014instead, the d-parser already stores constituent information in the arc labels, and the only necessary post-processing is to recover unary nodes. Another advantage of our method is that it can be readily used for discontinuous parsing, while their constrained CKY algorithm can only produce continuous parses."}, {"heading": "7 Conclusion", "text": "We proposed a reduction technique that allows to implement a constituent parser when only a dependency parser is given. The technique is applicable to any dependency parser, regardless its nature or kind. This reduction was accomplished by endowing dependency trees with a weak order relation, and showing that the resulting class\nof head-ordered dependency trees is isomorphic to constituent trees. We have shown empirically that the proposed reduction, while simple, leads to highly-competitive constituent parsers for English and for eight morphologically rich languages; and that it outperforms the current state of the art in discontinuous parsing of German."}, {"heading": "Acknowledgments", "text": "We thank Slav Petrov, Lingpeng Kong and Carlos Go\u0301mez-Rodr\u0131\u0301guez for their comments and suggestions. This research has been partially funded by the Spanish Ministry of Economy and Competitiveness and FEDER (project TIN201018552-C03-01), Ministry of Education (FPU Grant Program) and Xunta de Galicia (projects R2014/029 and R2014/034). A. M. was supported by the EU/FEDER programme, QREN/POR Lisboa (Portugal), under the Intelligo project (contract 2012/24803), and by the FCT grant UID/EEA/50008/2013."}], "references": [{"title": "Jointly learning to extract and compress", "author": ["Taylor Berg-Kirkpatrick", "Dan Gillick", "Dan Klein."], "venue": "Proc. of Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Berg.Kirkpatrick et al\\.,? 2011", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2011}, {"title": "Introducing the ims-wroc\u0142aw-szeged-cis entry at the spmrl 2014 shared task: Reranking and morpho-syntax meet", "author": ["Anders Bj\u00f6rkelund", "\u00d6zlem \u00c7etino\u011flu", "Agnieszka Fale\u0144ska", "Rich\u00e1rd Farkas", "Thomas Mueller", "Wolfgang Seeker", "Zsolt Sz\u00e1nt\u00f3"], "venue": null, "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2014}, {"title": "Development and evaluation of a broad-coverage probabilistic grammar of english-language computer manuals", "author": ["Ezra Black", "John Lafferty", "Salim Roukos."], "venue": "Proc. of Annual Meeting on Association for Computational Linguistics.", "citeRegEx": "Black et al\\.,? 1992", "shortCiteRegEx": "Black et al\\.", "year": 1992}, {"title": "Discontinuity revisited: An improved conversion to context-free representations", "author": ["Adriane Boyd."], "venue": "Proc. of Linguistic Annotation Workshop.", "citeRegEx": "Boyd.,? 2007", "shortCiteRegEx": "Boyd.", "year": 2007}, {"title": "The TIGER treebank", "author": ["Sabine Brants", "Stefanie Dipper", "Silvia Hansen", "Wolfgang Lezius", "George Smith."], "venue": "Proc. of the workshop on treebanks and linguistic theories.", "citeRegEx": "Brants et al\\.,? 2002", "shortCiteRegEx": "Brants et al\\.", "year": 2002}, {"title": "TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-rich Parsing", "author": ["X. Carreras", "M. Collins", "T. Koo."], "venue": "International Conference on Natural Language Learning.", "citeRegEx": "Carreras et al\\.,? 2008", "shortCiteRegEx": "Carreras et al\\.", "year": 2008}, {"title": "Coarseto-fine n-best parsing and maxent discriminative reranking", "author": ["Eugene Charniak", "Mark Johnson."], "venue": "Proc. of ACL.", "citeRegEx": "Charniak and Johnson.,? 2005", "shortCiteRegEx": "Charniak and Johnson.", "year": 2005}, {"title": "Tree-bank grammars", "author": ["E. Charniak."], "venue": "Proc. of the National Conference on Artificial Intelligence.", "citeRegEx": "Charniak.,? 1996", "shortCiteRegEx": "Charniak.", "year": 1996}, {"title": "A maximum-entropyinspired parser", "author": ["Eugene Charniak."], "venue": "Proc. of the North American Chapter of the Association for Computational Linguistics Conference.", "citeRegEx": "Charniak.,? 2000", "shortCiteRegEx": "Charniak.", "year": 2000}, {"title": "A Statistical Parser for Czech", "author": ["Michael Collins", "Lance Ramshaw", "Jan Haji\u010d", "Christoph Tillmann."], "venue": "Proc. of the Annual Meeting of the Association for Computational Linguistics on Computational Linguistics.", "citeRegEx": "Collins et al\\.,? 1999", "shortCiteRegEx": "Collins et al\\.", "year": 1999}, {"title": "Head-driven statistical models for natural language parsing", "author": ["M. Collins."], "venue": "Ph.D. thesis, University of Pennsylvania.", "citeRegEx": "Collins.,? 1999", "shortCiteRegEx": "Collins.", "year": 1999}, {"title": "Multilingual discriminative shift reduce phrase structure parsing for the SPMRL 2014 shared task", "author": ["Benoit Crabb\u00e9", "Djam\u00e9 Seddah."], "venue": "First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of", "citeRegEx": "Crabb\u00e9 and Seddah.,? 2014", "shortCiteRegEx": "Crabb\u00e9 and Seddah.", "year": 2014}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Marie-Catherine De Marneffe", "Bill MacCartney", "Christopher D Manning"], "venue": "In Proc. of LREC", "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Extraction automatique de Grammaires d\u2019Arbres Adjoints \u00e0 partir d\u2019un corpus arbor\u00e9 du fran\u00e7ais", "author": ["Ane Dybro-Johansen."], "venue": "Master\u2019s thesis, Universit\u00e9 Paris 7.", "citeRegEx": "Dybro.Johansen.,? 2004", "shortCiteRegEx": "Dybro.Johansen.", "year": 2004}, {"title": "Efficient parsing for bilexical context-free grammars and head automaton grammars", "author": ["J. Eisner", "G. Satta."], "venue": "Proc. of Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Eisner and Satta.,? 1999", "shortCiteRegEx": "Eisner and Satta.", "year": 1999}, {"title": "Three new probabilistic models for dependency parsing: An exploration", "author": ["J.M. Eisner."], "venue": "Proc. of International Conference on Computational Linguistics.", "citeRegEx": "Eisner.,? 1996", "shortCiteRegEx": "Eisner.", "year": 1996}, {"title": "Down-stream effects of tree-to-dependency conversions", "author": ["Jakob Elming", "Anders Johannsen", "Sigrid Klerke", "Emanuele Lapponi", "Hector Martinez Alonso", "Anders S\u00f8gaard."], "venue": "HLT-NAACL.", "citeRegEx": "Elming et al\\.,? 2013", "shortCiteRegEx": "Elming et al\\.", "year": 2013}, {"title": "Dependency systems and phrasestructure systems", "author": ["H. Gaifman."], "venue": "Information and control.", "citeRegEx": "Gaifman.,? 1965", "shortCiteRegEx": "Gaifman.", "year": 1965}, {"title": "Efficient parsing of well-nested linear context-free rewriting systems", "author": ["Carlos G\u00f3mez-Rodr\u0131\u0301guez", "Marco Kuhlmann", "Giorgio Satta"], "venue": "In Proc. of the Annual Conference of the North American Chapter of the Association for Computational Linguistics", "citeRegEx": "G\u00f3mez.Rodr\u0131\u0301guez et al\\.,? \\Q2010\\E", "shortCiteRegEx": "G\u00f3mez.Rodr\u0131\u0301guez et al\\.", "year": 2010}, {"title": "A dependencydriven parser for german dependency and constituency representations", "author": ["Johan Hall", "Joakim Nivre."], "venue": "Proc. of the Workshop on Parsing German.", "citeRegEx": "Hall and Nivre.,? 2008", "shortCiteRegEx": "Hall and Nivre.", "year": 2008}, {"title": "Less grammar, more features", "author": ["David Hall", "Greg Durrett", "Dan Klein."], "venue": "Proc. of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Hall et al\\.,? 2014", "shortCiteRegEx": "Hall et al\\.", "year": 2014}, {"title": "Forest reranking: Discriminative parsing with non-local features", "author": ["L. Huang."], "venue": "ACL.", "citeRegEx": "Huang.,? 2008", "shortCiteRegEx": "Huang.", "year": 2008}, {"title": "Dependency-based Semantic Role Labeling of PropBank", "author": ["R. Johansson", "P. Nugues."], "venue": "Empirical Methods for Natural Language Processing.", "citeRegEx": "Johansson and Nugues.,? 2008", "shortCiteRegEx": "Johansson and Nugues.", "year": 2008}, {"title": "PCFG models of linguistic tree representations", "author": ["M. Johnson."], "venue": "Computational Linguistics.", "citeRegEx": "Johnson.,? 1998", "shortCiteRegEx": "Johnson.", "year": 1998}, {"title": "Pseudoprojectivity: a polynomially parsable non-projective dependency grammar", "author": ["S. Kahane", "A. Nasr", "O. Rambow."], "venue": "International Conference on Computational Linguistics.", "citeRegEx": "Kahane et al\\.,? 1998", "shortCiteRegEx": "Kahane et al\\.", "year": 1998}, {"title": "Datadriven parsing using probabilistic linear context-free rewriting systems", "author": ["Laura Kallmeyer", "Wolfgang Maier."], "venue": "Computational Linguistics.", "citeRegEx": "Kallmeyer and Maier.,? 2013", "shortCiteRegEx": "Kallmeyer and Maier.", "year": 2013}, {"title": "Accurate unlexicalized parsing", "author": ["D. Klein", "C.D. Manning."], "venue": "Proc. of Annual Meeting on Association for Computational Linguistics.", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "An empirical comparison of parsing methods for stanford dependencies", "author": ["Lingpeng Kong", "Noah A Smith."], "venue": "arXiv preprint arXiv:1404.4314.", "citeRegEx": "Kong and Smith.,? 2014", "shortCiteRegEx": "Kong and Smith.", "year": 2014}, {"title": "Transforming dependencies into phrase structures", "author": ["Lingpeng Kong", "Alexander M. Rush", "Noah A. Smith."], "venue": "Proc. of the Conference of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Kong et al\\.,? 2015", "shortCiteRegEx": "Kong et al\\.", "year": 2015}, {"title": "Efficient third-order dependency parsers", "author": ["T. Koo", "M. Collins."], "venue": "Proc. of ACL.", "citeRegEx": "Koo and Collins.,? 2010", "shortCiteRegEx": "Koo and Collins.", "year": 2010}, {"title": "How to compare treebanks", "author": ["Sandra K\u00fcbler", "Wolfgang Maier", "Ines Rehbein", "Yannick Versley."], "venue": "LREC.", "citeRegEx": "K\u00fcbler et al\\.,? 2008", "shortCiteRegEx": "K\u00fcbler et al\\.", "year": 2008}, {"title": "Mildly non-projective dependency structures", "author": ["Marco Kuhlmann", "Joakim Nivre."], "venue": "Proc. of the COLING/ACL.", "citeRegEx": "Kuhlmann and Nivre.,? 2006", "shortCiteRegEx": "Kuhlmann and Nivre.", "year": 2006}, {"title": "Treebanks and mild context-sensitivity", "author": ["Wolfgang Maier", "Anders S\u00f8gaard."], "venue": "Proc. of Formal Grammar.", "citeRegEx": "Maier and S\u00f8gaard.,? 2008", "shortCiteRegEx": "Maier and S\u00f8gaard.", "year": 2008}, {"title": "Data-driven plcfrs parsing revisited: Restricting the fan-out to two", "author": ["Wolfgang Maier", "Miriam Kaeshammer", "Laura Kallmeyer."], "venue": "Proc. of the Eleventh International Conference on Tree Adjoining Grammars and Related Formalisms.", "citeRegEx": "Maier et al\\.,? 2012", "shortCiteRegEx": "Maier et al\\.", "year": 2012}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Computational Linguistics.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Turning on the turbo: Fast third-order nonprojective turbo parsers", "author": ["A.F.T. Martins", "M.B. Almeida", "N.A. Smith."], "venue": "Proc. of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Martins et al\\.,? 2013", "shortCiteRegEx": "Martins et al\\.", "year": 2013}, {"title": "Probabilistic CFG with latent annotations", "author": ["T. Matsuzaki", "Y. Miyao", "J. Tsujii."], "venue": "Proc. of ACL.", "citeRegEx": "Matsuzaki et al\\.,? 2005", "shortCiteRegEx": "Matsuzaki et al\\.", "year": 2005}, {"title": "On the complexity of non-projective data-driven dependency parsing", "author": ["R. McDonald", "G. Satta."], "venue": "Proc. of International Conference on Parsing Technologies.", "citeRegEx": "McDonald and Satta.,? 2007", "shortCiteRegEx": "McDonald and Satta.", "year": 2007}, {"title": "Non-projective dependency parsing using spanning tree algorithms", "author": ["R.T. McDonald", "F. Pereira", "K. Ribarov", "J. Hajic."], "venue": "Proc. of Empirical Methods for Natural Language Processing.", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Multilingual dependency analysis with a two-stage discriminative parser", "author": ["R. McDonald", "K. Lerman", "F. Pereira."], "venue": "Proc. of International Conference on Natural Language Learning.", "citeRegEx": "McDonald et al\\.,? 2006", "shortCiteRegEx": "McDonald et al\\.", "year": 2006}, {"title": "Labeled pseudo-projective dependency parsing with support vector machines", "author": ["J. Nivre", "J. Hall", "J. Nilsson", "G. Eryi\u01e7it", "S. Marinov."], "venue": "Procs. of International Conference on Natural Language Learning.", "citeRegEx": "Nivre et al\\.,? 2006", "shortCiteRegEx": "Nivre et al\\.", "year": 2006}, {"title": "Maltparser: A language-independent system for data-driven dependency parsing", "author": ["Joakim Nivre", "Johan Hall", "Jens Nilsson", "Atanas Chanev", "Glsen Eryigit", "Sandra K\u00fcbler", "Svetoslav Marinov", "Erwin Marsi."], "venue": "Natural Language Engineering.", "citeRegEx": "Nivre et al\\.,? 2007", "shortCiteRegEx": "Nivre et al\\.", "year": 2007}, {"title": "Improved inference for unlexicalized parsing", "author": ["Slav Petrov", "Dan Klein."], "venue": "Proc. of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Petrov and Klein.,? 2007", "shortCiteRegEx": "Petrov and Klein.", "year": 2007}, {"title": "Treebank-Based Grammar Acquisition for German", "author": ["Ines Rehbein."], "venue": "Ph.D. thesis, School of Computing, Dublin City University.", "citeRegEx": "Rehbein.,? 2009", "shortCiteRegEx": "Rehbein.", "year": 2009}, {"title": "Vine pruning for efficient multi-pass dependency parsing", "author": ["Alexander M Rush", "Slav Petrov."], "venue": "Proc. of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Rush and Petrov.,? 2012", "shortCiteRegEx": "Rush and Petrov.", "year": 2012}, {"title": "On dual decomposition and linear programming relaxations for natural language processing", "author": ["A. Rush", "D. Sontag", "M. Collins", "T. Jaakkola."], "venue": "Proc. of EMNLP.", "citeRegEx": "Rush et al\\.,? 2010", "shortCiteRegEx": "Rush et al\\.", "year": 2010}, {"title": "A classifier-based parser with linear run-time complexity", "author": ["Kenji Sagae", "Alon Lavie."], "venue": "Proceedings of the Ninth International Workshop on Parsing Technology.", "citeRegEx": "Sagae and Lavie.,? 2005", "shortCiteRegEx": "Sagae and Lavie.", "year": 2005}, {"title": "Introducing the spmrl 2014 shared task on parsing morphologically-rich languages", "author": ["Djam\u00e9 Seddah", "Sandra K\u00fcbler", "Reut Tsarfaty."], "venue": "Proc. of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic", "citeRegEx": "Seddah et al\\.,? 2014", "shortCiteRegEx": "Seddah et al\\.", "year": 2014}, {"title": "An annotation scheme for free word order languages", "author": ["Wojciech Skut", "Brigitte Krenn", "Thorsten Brants", "Hans Uszkoreit."], "venue": "Proc. of the Fifth Conference on Applied Natural Language Processing ANLP-97.", "citeRegEx": "Skut et al\\.,? 1997", "shortCiteRegEx": "Skut et al\\.", "year": 1997}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proc. of ACL.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Discontinuous parsing with an efficient and accurate dop model", "author": ["Andreas van Cranenburgh", "Rens Bod."], "venue": "IWPT-2013.", "citeRegEx": "Cranenburgh and Bod.,? 2013", "shortCiteRegEx": "Cranenburgh and Bod.", "year": 2013}, {"title": "Efficient parsing with linear context-free rewriting systems", "author": ["Andreas van Cranenburgh."], "venue": "Proc. of the Conference of the European Chapter of the Association for Computational Linguistics.", "citeRegEx": "Cranenburgh.,? 2012", "shortCiteRegEx": "Cranenburgh.", "year": 2012}, {"title": "Experiments with easy-first nonprojective constituent parsing", "author": ["Yannick Versley."], "venue": "Proc. of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages.", "citeRegEx": "Versley.,? 2014a", "shortCiteRegEx": "Versley.", "year": 2014}, {"title": "Incorporating semisupervised features into discontinuous easy-first constituent parsing", "author": ["Yannick Versley."], "venue": "CoRR, abs/1409.3813.", "citeRegEx": "Versley.,? 2014b", "shortCiteRegEx": "Versley.", "year": 2014}, {"title": "Phrase dependency parsing for opinion mining", "author": ["Yuanbin Wu", "Qi Zhang", "Xuanjing Huang", "Lide Wu."], "venue": "Proc. of EMNLP.", "citeRegEx": "Wu et al\\.,? 2009", "shortCiteRegEx": "Wu et al\\.", "year": 2009}, {"title": "Converting dependency structures to phrase structures", "author": ["F. Xia", "M. Palmer."], "venue": "Proceedings of the First International Conference on Human Language Technology Research.", "citeRegEx": "Xia and Palmer.,? 2001", "shortCiteRegEx": "Xia and Palmer.", "year": 2001}, {"title": "Towards a multirepresentational treebank", "author": ["Fei Xia", "Owen Rambow", "Rajesh Bhatt", "Martha Palmer", "Dipti Misra Sharma."], "venue": "LOT Occasional Series.", "citeRegEx": "Xia et al\\.,? 2008", "shortCiteRegEx": "Xia et al\\.", "year": 2008}, {"title": "Statistical dependency analysis with support vector machines", "author": ["H. Yamada", "Y. Matsumoto."], "venue": "Proc. of International Conference on Parsing Technologies.", "citeRegEx": "Yamada and Matsumoto.,? 2003", "shortCiteRegEx": "Yamada and Matsumoto.", "year": 2003}, {"title": "Transition-based dependency parsing with rich non-local features", "author": ["Y. Zhang", "J. Nivre."], "venue": "Proc. of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Zhang and Nivre.,? 2011", "shortCiteRegEx": "Zhang and Nivre.", "year": 2011}, {"title": "Fast and accurate shiftreduce constituent parsing", "author": ["Muhua Zhu", "Yue Zhang", "Wenliang Chen", "Min Zhang", "Jingbo Zhu."], "venue": "Proc. of ACL.", "citeRegEx": "Zhu et al\\.,? 2013", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "Constituent parsing is a central problem in NLP\u2014one at which statistical models trained on treebanks have excelled (Charniak, 1996; Klein and Manning, 2003; Petrov and Klein, 2007).", "startOffset": 115, "endOffset": 180}, {"referenceID": 26, "context": "Constituent parsing is a central problem in NLP\u2014one at which statistical models trained on treebanks have excelled (Charniak, 1996; Klein and Manning, 2003; Petrov and Klein, 2007).", "startOffset": 115, "endOffset": 180}, {"referenceID": 42, "context": "Constituent parsing is a central problem in NLP\u2014one at which statistical models trained on treebanks have excelled (Charniak, 1996; Klein and Manning, 2003; Petrov and Klein, 2007).", "startOffset": 115, "endOffset": 180}, {"referenceID": 22, "context": "Dependency parsers are generally faster, but less informative, since they do not produce constituents, which are often required by downstream applications (Johansson and Nugues, 2008; Wu et al., 2009; Berg-Kirkpatrick et al., 2011; Elming et al., 2013).", "startOffset": 155, "endOffset": 252}, {"referenceID": 54, "context": "Dependency parsers are generally faster, but less informative, since they do not produce constituents, which are often required by downstream applications (Johansson and Nugues, 2008; Wu et al., 2009; Berg-Kirkpatrick et al., 2011; Elming et al., 2013).", "startOffset": 155, "endOffset": 252}, {"referenceID": 0, "context": "Dependency parsers are generally faster, but less informative, since they do not produce constituents, which are often required by downstream applications (Johansson and Nugues, 2008; Wu et al., 2009; Berg-Kirkpatrick et al., 2011; Elming et al., 2013).", "startOffset": 155, "endOffset": 252}, {"referenceID": 16, "context": "Dependency parsers are generally faster, but less informative, since they do not produce constituents, which are often required by downstream applications (Johansson and Nugues, 2008; Wu et al., 2009; Berg-Kirkpatrick et al., 2011; Elming et al., 2013).", "startOffset": 155, "endOffset": 252}, {"referenceID": 6, "context": "Coarse-to-fine decoding (Charniak and Johnson, 2005) and shift-reduce parsing (Sagae and Lavie, 2005; Zhu et al.", "startOffset": 24, "endOffset": 52}, {"referenceID": 46, "context": "Coarse-to-fine decoding (Charniak and Johnson, 2005) and shift-reduce parsing (Sagae and Lavie, 2005; Zhu et al., 2013) were a step for-", "startOffset": 78, "endOffset": 119}, {"referenceID": 59, "context": "Coarse-to-fine decoding (Charniak and Johnson, 2005) and shift-reduce parsing (Sagae and Lavie, 2005; Zhu et al., 2013) were a step for-", "startOffset": 78, "endOffset": 119}, {"referenceID": 41, "context": "While non-projective dependency parsers, which are able to model such phenomena, have been widely developed in the last decade (Nivre et al., 2007; McDonald et al., 2006; Martins et al., 2013), discontinuous constituent parsing is still taking its first steps (Maier and S\u00f8gaard, 2008; Kallmeyer and Maier, 2013).", "startOffset": 127, "endOffset": 192}, {"referenceID": 39, "context": "While non-projective dependency parsers, which are able to model such phenomena, have been widely developed in the last decade (Nivre et al., 2007; McDonald et al., 2006; Martins et al., 2013), discontinuous constituent parsing is still taking its first steps (Maier and S\u00f8gaard, 2008; Kallmeyer and Maier, 2013).", "startOffset": 127, "endOffset": 192}, {"referenceID": 35, "context": "While non-projective dependency parsers, which are able to model such phenomena, have been widely developed in the last decade (Nivre et al., 2007; McDonald et al., 2006; Martins et al., 2013), discontinuous constituent parsing is still taking its first steps (Maier and S\u00f8gaard, 2008; Kallmeyer and Maier, 2013).", "startOffset": 127, "endOffset": 192}, {"referenceID": 32, "context": ", 2013), discontinuous constituent parsing is still taking its first steps (Maier and S\u00f8gaard, 2008; Kallmeyer and Maier, 2013).", "startOffset": 75, "endOffset": 127}, {"referenceID": 25, "context": ", 2013), discontinuous constituent parsing is still taking its first steps (Maier and S\u00f8gaard, 2008; Kallmeyer and Maier, 2013).", "startOffset": 75, "endOffset": 127}, {"referenceID": 19, "context": "Hall and Nivre (2008) attempted a related conversion to parse German, but their complex encoding scheme blows up the number of arc labels, affecting the final parser\u2019s quality.", "startOffset": 0, "endOffset": 22}, {"referenceID": 42, "context": "While simple, our reduction-based parsers are on par with the Berkeley parser for English (Petrov and Klein, 2007), and with the best single system in the recent SPMRL shared task (Seddah et al.", "startOffset": 90, "endOffset": 114}, {"referenceID": 27, "context": "Following Kong and Smith (2014), we use c-/d- prefixes for convenience (e.", "startOffset": 10, "endOffset": 32}, {"referenceID": 10, "context": "This kth node (called the head-child node) is commonly chosen applying an handwritten set of head rules (Collins, 1999; Yamada and Matsumoto, 2003).", "startOffset": 104, "endOffset": 147}, {"referenceID": 57, "context": "This kth node (called the head-child node) is commonly chosen applying an handwritten set of head rules (Collins, 1999; Yamada and Matsumoto, 2003).", "startOffset": 104, "endOffset": 147}, {"referenceID": 7, "context": "There has been a long string of work in statistical c-parsing, shifting from simple models (Charniak, 1996) to more sophisticated ones using structural annotation (Johnson, 1998; Klein and Manning, 2003), latent grammars (Matsuzaki et al.", "startOffset": 91, "endOffset": 107}, {"referenceID": 23, "context": "There has been a long string of work in statistical c-parsing, shifting from simple models (Charniak, 1996) to more sophisticated ones using structural annotation (Johnson, 1998; Klein and Manning, 2003), latent grammars (Matsuzaki et al.", "startOffset": 163, "endOffset": 203}, {"referenceID": 26, "context": "There has been a long string of work in statistical c-parsing, shifting from simple models (Charniak, 1996) to more sophisticated ones using structural annotation (Johnson, 1998; Klein and Manning, 2003), latent grammars (Matsuzaki et al.", "startOffset": 163, "endOffset": 203}, {"referenceID": 36, "context": "There has been a long string of work in statistical c-parsing, shifting from simple models (Charniak, 1996) to more sophisticated ones using structural annotation (Johnson, 1998; Klein and Manning, 2003), latent grammars (Matsuzaki et al., 2005; Petrov and Klein, 2007), and lexicalization (Eisner, 1996; Collins, 1999).", "startOffset": 221, "endOffset": 269}, {"referenceID": 42, "context": "There has been a long string of work in statistical c-parsing, shifting from simple models (Charniak, 1996) to more sophisticated ones using structural annotation (Johnson, 1998; Klein and Manning, 2003), latent grammars (Matsuzaki et al., 2005; Petrov and Klein, 2007), and lexicalization (Eisner, 1996; Collins, 1999).", "startOffset": 221, "endOffset": 269}, {"referenceID": 15, "context": ", 2005; Petrov and Klein, 2007), and lexicalization (Eisner, 1996; Collins, 1999).", "startOffset": 52, "endOffset": 81}, {"referenceID": 10, "context": ", 2005; Petrov and Klein, 2007), and lexicalization (Eisner, 1996; Collins, 1999).", "startOffset": 52, "endOffset": 81}, {"referenceID": 6, "context": "An orthogonal line of work uses ensemble or reranking strategies to further improve accuracy (Charniak and Johnson, 2005; Huang, 2008; Bj\u00f6rkelund et al., 2014).", "startOffset": 93, "endOffset": 159}, {"referenceID": 21, "context": "An orthogonal line of work uses ensemble or reranking strategies to further improve accuracy (Charniak and Johnson, 2005; Huang, 2008; Bj\u00f6rkelund et al., 2014).", "startOffset": 93, "endOffset": 159}, {"referenceID": 1, "context": "An orthogonal line of work uses ensemble or reranking strategies to further improve accuracy (Charniak and Johnson, 2005; Huang, 2008; Bj\u00f6rkelund et al., 2014).", "startOffset": 93, "endOffset": 159}, {"referenceID": 33, "context": "To speed up decoding, prior work has considered restrictons, such as bounding the fan-out (Maier et al., 2012) and requiring well-nestedness (Kuhlmann and Nivre, 2006; G\u00f3mez-Rodr\u0131\u0301guez et al.", "startOffset": 90, "endOffset": 110}, {"referenceID": 31, "context": ", 2012) and requiring well-nestedness (Kuhlmann and Nivre, 2006; G\u00f3mez-Rodr\u0131\u0301guez et al., 2010).", "startOffset": 38, "endOffset": 95}, {"referenceID": 18, "context": ", 2012) and requiring well-nestedness (Kuhlmann and Nivre, 2006; G\u00f3mez-Rodr\u0131\u0301guez et al., 2010).", "startOffset": 38, "endOffset": 95}, {"referenceID": 3, "context": "Other approaches eliminate the discontinuities via tree transformations (Boyd, 2007; K\u00fcbler et al., 2008), sometimes as a pruning step followed by reranking (van Cranenburgh and Bod, 2013).", "startOffset": 72, "endOffset": 105}, {"referenceID": 30, "context": "Other approaches eliminate the discontinuities via tree transformations (Boyd, 2007; K\u00fcbler et al., 2008), sometimes as a pruning step followed by reranking (van Cranenburgh and Bod, 2013).", "startOffset": 72, "endOffset": 105}, {"referenceID": 1, "context": "An orthogonal line of work uses ensemble or reranking strategies to further improve accuracy (Charniak and Johnson, 2005; Huang, 2008; Bj\u00f6rkelund et al., 2014). Discontinuous c-parsing is considered a much harder problem, involving mildly context-sensitive formalisms such as LCFRS or range concatenation grammars, with treebankderived c-parsers exhibiting near-exponential runtime (Kallmeyer and Maier, 2013, Figure 27). To speed up decoding, prior work has considered restrictons, such as bounding the fan-out (Maier et al., 2012) and requiring well-nestedness (Kuhlmann and Nivre, 2006; G\u00f3mez-Rodr\u0131\u0301guez et al., 2010). Other approaches eliminate the discontinuities via tree transformations (Boyd, 2007; K\u00fcbler et al., 2008), sometimes as a pruning step followed by reranking (van Cranenburgh and Bod, 2013). However, reported runtimes are still superior to 10 seconds per sentence, which is not practical. Recently, Versley (2014a) proposed an easyfirst approach that leads to considerable speedups, but is less accurate.", "startOffset": 135, "endOffset": 936}, {"referenceID": 1, "context": "An orthogonal line of work uses ensemble or reranking strategies to further improve accuracy (Charniak and Johnson, 2005; Huang, 2008; Bj\u00f6rkelund et al., 2014). Discontinuous c-parsing is considered a much harder problem, involving mildly context-sensitive formalisms such as LCFRS or range concatenation grammars, with treebankderived c-parsers exhibiting near-exponential runtime (Kallmeyer and Maier, 2013, Figure 27). To speed up decoding, prior work has considered restrictons, such as bounding the fan-out (Maier et al., 2012) and requiring well-nestedness (Kuhlmann and Nivre, 2006; G\u00f3mez-Rodr\u0131\u0301guez et al., 2010). Other approaches eliminate the discontinuities via tree transformations (Boyd, 2007; K\u00fcbler et al., 2008), sometimes as a pruning step followed by reranking (van Cranenburgh and Bod, 2013). However, reported runtimes are still superior to 10 seconds per sentence, which is not practical. Recently, Versley (2014a) proposed an easyfirst approach that leads to considerable speedups, but is less accurate. In this paper, we design fast discontinuous c-parsers that outperform all the ones above by a wide margin, with similar runtimes as Versley (2014a).", "startOffset": 135, "endOffset": 1174}, {"referenceID": 24, "context": "there is a directed path from h to all words that lie between h and m in the surface string (Kahane et al., 1998).", "startOffset": 92, "endOffset": 113}, {"referenceID": 17, "context": "Projective d-trees can be obtained from continuous c-trees by reading off the lexical heads and dropping the internal nodes (Gaifman, 1965).", "startOffset": 124, "endOffset": 139}, {"referenceID": 14, "context": "While projective d-parsers can use dynamic programming (Eisner and Satta, 1999; Koo and Collins, 2010), non-projective d-parsers typically rely on approximate decoders, since the underlying problem is NP-hard beyond arc-factored models (McDonald and Satta, 2007).", "startOffset": 55, "endOffset": 102}, {"referenceID": 29, "context": "While projective d-parsers can use dynamic programming (Eisner and Satta, 1999; Koo and Collins, 2010), non-projective d-parsers typically rely on approximate decoders, since the underlying problem is NP-hard beyond arc-factored models (McDonald and Satta, 2007).", "startOffset": 55, "endOffset": 102}, {"referenceID": 37, "context": "While projective d-parsers can use dynamic programming (Eisner and Satta, 1999; Koo and Collins, 2010), non-projective d-parsers typically rely on approximate decoders, since the underlying problem is NP-hard beyond arc-factored models (McDonald and Satta, 2007).", "startOffset": 236, "endOffset": 262}, {"referenceID": 40, "context": "An alternative are transition-based d-parsers (Nivre et al., 2006; Zhang and Nivre, 2011), which achieve observed linear time.", "startOffset": 46, "endOffset": 89}, {"referenceID": 58, "context": "An alternative are transition-based d-parsers (Nivre et al., 2006; Zhang and Nivre, 2011), which achieve observed linear time.", "startOffset": 46, "endOffset": 89}, {"referenceID": 44, "context": "Since d-parsing algorithms do not have a grammar constant, typical implementations are significantly faster than c-parsers (Rush and Petrov, 2012; Martins et al., 2013).", "startOffset": 123, "endOffset": 168}, {"referenceID": 35, "context": "Since d-parsing algorithms do not have a grammar constant, typical implementations are significantly faster than c-parsers (Rush and Petrov, 2012; Martins et al., 2013).", "startOffset": 123, "endOffset": 168}, {"referenceID": 5, "context": "We may regard a c-tree as a set of L spines, one per word, which attach to each other to form a tree (Carreras et al., 2008).", "startOffset": 101, "endOffset": 124}, {"referenceID": 17, "context": "To see (i), note that the projectiveness of D is ensured by the well-known result of Gaifman (1965) about the projection of continuous trees.", "startOffset": 85, "endOffset": 100}, {"referenceID": 34, "context": "Along the way, we illustrate with experiments using the English Penn Treebank (Marcus et al., 1993), which we lexicalized by applying the head rules of Collins (1999).", "startOffset": 78, "endOffset": 99}, {"referenceID": 10, "context": ", 1993), which we lexicalized by applying the head rules of Collins (1999).2", "startOffset": 60, "endOffset": 75}, {"referenceID": 35, "context": "We predict automatic POS tags with TurboTagger (Martins et al., 2013), with 10-fold jackknifing on the training set.", "startOffset": 47, "endOffset": 69}, {"referenceID": 19, "context": "For comparison, we implemented a third strategy replicating the encoding proposed by Hall and Nivre (2008), which we call H&N-encoding.", "startOffset": 85, "endOffset": 107}, {"referenceID": 41, "context": "4 Table 1 compares this approach against a oneshot strategy, experimenting with various off-theshelf d-parsers: MaltParser (Nivre et al., 2007), MSTParser (McDonald et al.", "startOffset": 123, "endOffset": 143}, {"referenceID": 38, "context": ", 2007), MSTParser (McDonald et al., 2005), ZPar (Zhang", "startOffset": 19, "endOffset": 42}, {"referenceID": 38, "context": "In our experiments, we found it advantageous to perform labeled d-parsing in two stages, as done by McDonald et al. (2006): first, train an unlabeled d-parser; then, train a dependency labeler.", "startOffset": 100, "endOffset": 123}, {"referenceID": 35, "context": "and Nivre, 2011), and TurboParser (Martins et al., 2013), all with the default settings.", "startOffset": 34, "endOffset": 56}, {"referenceID": 42, "context": ", 2013, and Stanford Shift-Reduce), and is on par with the Berkeley parser (Petrov and Klein, 2007), while being more than 5 times faster.", "startOffset": 75, "endOffset": 99}, {"referenceID": 2, "context": "For constituents, we show F1-scores (without punctuation and root nodes), as provided by EVALB (Black et al., 1992).", "startOffset": 95, "endOffset": 115}, {"referenceID": 5, "context": "1 169 Carreras et al. (2008) 90.", "startOffset": 6, "endOffset": 29}, {"referenceID": 5, "context": "1 169 Carreras et al. (2008) 90.7 91.4 91.1 \u2013 Zhu et al. (2013) 90.", "startOffset": 6, "endOffset": 64}, {"referenceID": 5, "context": "1 169 Carreras et al. (2008) 90.7 91.4 91.1 \u2013 Zhu et al. (2013) 90.3 90.6 90.4 1,290 Stanford Shift-Reduce (2014) 89.", "startOffset": 6, "endOffset": 114}, {"referenceID": 5, "context": "1 169 Carreras et al. (2008) 90.7 91.4 91.1 \u2013 Zhu et al. (2013) 90.3 90.6 90.4 1,290 Stanford Shift-Reduce (2014) 89.1 89.1 89.1 655 Hall et al. (2014) 88.", "startOffset": 6, "endOffset": 152}, {"referenceID": 5, "context": "1 169 Carreras et al. (2008) 90.7 91.4 91.1 \u2013 Zhu et al. (2013) 90.3 90.6 90.4 1,290 Stanford Shift-Reduce (2014) 89.1 89.1 89.1 655 Hall et al. (2014) 88.4 88.8 88.6 12 This work 89.9 90.4 90.2 957 Charniak and Johnson (2005)\u2217 91.", "startOffset": 6, "endOffset": 227}, {"referenceID": 5, "context": "1 169 Carreras et al. (2008) 90.7 91.4 91.1 \u2013 Zhu et al. (2013) 90.3 90.6 90.4 1,290 Stanford Shift-Reduce (2014) 89.1 89.1 89.1 655 Hall et al. (2014) 88.4 88.8 88.6 12 This work 89.9 90.4 90.2 957 Charniak and Johnson (2005)\u2217 91.2 91.8 91.5 84 Socher et al. (2013)\u2217 89.", "startOffset": 6, "endOffset": 267}, {"referenceID": 59, "context": "best supervised competitor is the recent shiftreduce parser of Zhu et al. (2013), which achieves slightly better accuracy and speed.", "startOffset": 63, "endOffset": 81}, {"referenceID": 47, "context": "We experimented with datasets for eight morphologically rich languages, from the SPMRL14 shared task (Seddah et al., 2014).", "startOffset": 101, "endOffset": 122}, {"referenceID": 43, "context": "German we used the head rules detailed in DybroJohansen (2004) and Rehbein (2009), respectively.", "startOffset": 67, "endOffset": 82}, {"referenceID": 43, "context": "German we used the head rules detailed in DybroJohansen (2004) and Rehbein (2009), respectively. For Basque, Hungarian and Korean, we always take the rightmost modifier as head-child node. For Hebrew and Polish we use the leftmost modifier instead. For Swedish we induce head rules from the provided dependency treebank, as described in Versley (2014b). These choices were based on dev-set experiments.", "startOffset": 67, "endOffset": 353}, {"referenceID": 42, "context": "For all languages except French, our system outperforms the Berkeley parser (Petrov and Klein, 2007), with or without prescribed POS tags.", "startOffset": 76, "endOffset": 100}, {"referenceID": 11, "context": "Our average F1-scores are superior to the best single parser6 participating in the shared task (Crabb\u00e9 and Seddah, 2014), and to the system of Hall et al.", "startOffset": 95, "endOffset": 120}, {"referenceID": 11, "context": "Our average F1-scores are superior to the best single parser6 participating in the shared task (Crabb\u00e9 and Seddah, 2014), and to the system of Hall et al. (2014), achieving the best results for 4 out of 8 languages.", "startOffset": 96, "endOffset": 162}, {"referenceID": 4, "context": "Finally, we experimented on two widely-used discontinuous German treebanks: TIGER (Brants et al., 2002) and NEGRA (Skut et al.", "startOffset": 82, "endOffset": 103}, {"referenceID": 48, "context": ", 2002) and NEGRA (Skut et al., 1997).", "startOffset": 18, "endOffset": 37}, {"referenceID": 4, "context": "Finally, we experimented on two widely-used discontinuous German treebanks: TIGER (Brants et al., 2002) and NEGRA (Skut et al., 1997). For the former, we used two different splits: TIGERSPMLR, provided in the SPMRL14 shared task; and TIGER-H&N, used by Hall and Nivre (2008). For NEGRA, we used the standard splits.", "startOffset": 83, "endOffset": 275}, {"referenceID": 19, "context": "17 Hall et al. (2014) 83.", "startOffset": 3, "endOffset": 22}, {"referenceID": 11, "context": "72 Crabb\u00e9 and Seddah (2014) 85.", "startOffset": 3, "endOffset": 28}, {"referenceID": 40, "context": "Berkeley Tagged is a version of Petrov and Klein (2007) using the predicted POS tags provided by the organizers.", "startOffset": 32, "endOffset": 56}, {"referenceID": 10, "context": "Crabb\u00e9 and Seddah (2014) is the best nonreranking system in the shared task, and Bj\u00f6rkelund et al.", "startOffset": 0, "endOffset": 25}, {"referenceID": 1, "context": "Crabb\u00e9 and Seddah (2014) is the best nonreranking system in the shared task, and Bj\u00f6rkelund et al. (2014) the ensemble and reranking-based system which won the official task.", "startOffset": 81, "endOffset": 106}, {"referenceID": 43, "context": "The treebanks were lexicalized using the head-rule sets of Rehbein (2009). For comparison to related work, a sentence length cut-off of 30, 40 and 70 was applied during the evaluation.", "startOffset": 59, "endOffset": 74}, {"referenceID": 50, "context": "The best competitor, van Cranenburgh and Bod (2013), is more than 3 points behind, both in TIGER-H&N and in NEGRA.", "startOffset": 25, "endOffset": 52}, {"referenceID": 50, "context": "The best competitor, van Cranenburgh and Bod (2013), is more than 3 points behind, both in TIGER-H&N and in NEGRA. Our reduction-based parsers are also much faster: van Cranenburgh and Bod (2013) report 3 hours to parse NEGRA with L < 40.", "startOffset": 25, "endOffset": 196}, {"referenceID": 50, "context": "The best competitor, van Cranenburgh and Bod (2013), is more than 3 points behind, both in TIGER-H&N and in NEGRA. Our reduction-based parsers are also much faster: van Cranenburgh and Bod (2013) report 3 hours to parse NEGRA with L < 40. Our system parses all NEGRA sentences (regardless of length) in 27.1 seconds, which corresponds to a rate of 618 toks/s. This approaches the speed of the easy-first system of Versley (2014a), who reports runtimes in the range 670\u2013920 toks/s.", "startOffset": 25, "endOffset": 430}, {"referenceID": 56, "context": "(1999) and Xia and Palmer (2001) in the backward direction, toward the construction of multirepresentational treebanks (Xia et al., 2008).", "startOffset": 119, "endOffset": 137}, {"referenceID": 10, "context": "Conversions between constituents and dependencies have been considered by De Marneffe et al. (2006) in the forward direction, and by Collins et al.", "startOffset": 77, "endOffset": 100}, {"referenceID": 9, "context": "(2006) in the forward direction, and by Collins et al. (1999) and Xia and Palmer (2001) in the backward direction, toward the construction of multirepresentational treebanks (Xia et al.", "startOffset": 40, "endOffset": 62}, {"referenceID": 9, "context": "(2006) in the forward direction, and by Collins et al. (1999) and Xia and Palmer (2001) in the backward direction, toward the construction of multirepresentational treebanks (Xia et al.", "startOffset": 40, "endOffset": 88}, {"referenceID": 19, "context": "The work most related to ours is Hall and Nivre (2008), who also convert dependencies to constituents to prototype a c-parser for German.", "startOffset": 33, "endOffset": 55}, {"referenceID": 19, "context": "The work most related to ours is Hall and Nivre (2008), who also convert dependencies to constituents to prototype a c-parser for German. Their encoding strategy is compared to ours in \u00a74.1: they encode the entire spines into the dependency labels, which become rather complex and numerous. A similar strategy has been used by Versley (2014a) for discontinuous c-parsing.", "startOffset": 33, "endOffset": 343}, {"referenceID": 5, "context": "Joint constituent and dependency parsing have been tackled by Carreras et al. (2008) and Rush et al.", "startOffset": 62, "endOffset": 85}, {"referenceID": 5, "context": "Joint constituent and dependency parsing have been tackled by Carreras et al. (2008) and Rush et al. (2010), but the resulting parsers, while accurate, are more expensive than a single c-parser.", "startOffset": 62, "endOffset": 108}, {"referenceID": 5, "context": "Joint constituent and dependency parsing have been tackled by Carreras et al. (2008) and Rush et al. (2010), but the resulting parsers, while accurate, are more expensive than a single c-parser. Very recently, Kong et al. (2015) proposed a much cheaper pipeline in which d-parsing is performed first, followed by a c-parser constrained to be consistent with the predicted d-structure.", "startOffset": 62, "endOffset": 229}, {"referenceID": 25, "context": "10 Kallmeyer and Maier (2013), gold 75.", "startOffset": 3, "endOffset": 30}, {"referenceID": 25, "context": "10 Kallmeyer and Maier (2013), gold 75.75 \u2013 \u2013 \u2013 \u2013 \u2013 van Cranenburgh and Bod (2013), gold \u2013 \u2013 76.", "startOffset": 3, "endOffset": 83}, {"referenceID": 50, "context": "van Cranenburgh and Bod (2013), pred \u2013 \u2013 74.", "startOffset": 4, "endOffset": 31}], "year": 2015, "abstractText": "We reduce phrase-representation parsing to dependency parsing. Our reduction is grounded on a new intermediate representation, \u201chead-ordered dependency trees,\u201d shown to be isomorphic to constituent trees. By encoding order information in the dependency labels, we show that any off-the-shelf, trainable dependency parser can be used to produce constituents. When this parser is non-projective, we can perform discontinuous parsing in a very natural manner. Despite the simplicity of our approach, experiments show that the resulting parsers are on par with strong baselines, such as the Berkeley parser for English and the best single system in the SPMRL-2014 shared task. Results are particularly striking for discontinuous parsing of German, where we surpass the current state of the art by a wide margin.", "creator": "TeX"}}}