{"id": "1503.01239", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2015", "title": "Joint Active Learning and Feature Selection via CUR Matrix Decomposition", "abstract": "This paper focuses on the problem of simultaneous sample and feature selection for machine learning in a fully unsupervised setting. Though most existing works tackle these two problems separately that derives two well-studied sub-areas namely active learning and feature selection, a unified approach is inspirational since they are often interleaved with each other. Noisy and high-dimensional features will bring adverse effect on sample selection, while `good' samples will be beneficial to feature selection. We present a unified framework to conduct active learning and feature selection simultaneously. From the data reconstruction perspective, both the selected samples and features can best approximate the original dataset respectively, such that the selected samples characterized by the selected features are very representative. Additionally our method is one-shot without iteratively selecting samples for progressive labeling. Thus our model is especially suitable when the initial labeled samples are scarce or totally absent, which existing works hardly address particularly for simultaneous feature selection. To alleviate the NP-hardness of the raw problem, the proposed formulation involves a convex but non-smooth optimization problem. We solve it efficiently by an iterative algorithm, and prove its global convergence. Experiments on publicly available datasets validate that our method is promising compared with the state-of-the-arts. This is a strong indication that the model is working well. As the authors note, the results of our model are a result of a rigorous computational model of the problem. It would be possible to perform the same procedure over the same time, but the work is still preliminary and therefore difficult.\n\n\n\nThe results were collected during the past two months and represent the result of a comprehensive study conducted for the last two months. Data from the two experiments were analyzed, with the results reported by the authors and presented by their authors. In addition to the results of the two experiments, we provide a detailed detailed summary of the experiments as well as their results for the future. This study also included the results of a complete study on machine learning, where the present data are presented in separate window. Furthermore, the results of the studies in each experiment were submitted via a special online questionnaire.\n\nDiscussion\n\nThe results for the study of simultaneous sample and feature selection were similar to that for concurrent feature selection. We have found that each sample and feature selection is equally robust in the absence of a strong bias, even if it is independent of the other results. However, the performance on the single sample and feature selection may be different in all situations. Moreover, it has been demonstrated that the difference between a", "histories": [["v1", "Wed, 4 Mar 2015 06:47:16 GMT  (582kb,D)", "https://arxiv.org/abs/1503.01239v1", null], ["v2", "Fri, 24 Mar 2017 08:58:34 GMT  (582kb,D)", "http://arxiv.org/abs/1503.01239v2", null], ["v3", "Mon, 3 Apr 2017 02:20:27 GMT  (2161kb,D)", "http://arxiv.org/abs/1503.01239v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["changsheng li", "xiangfeng wang", "weishan dong", "junchi yan", "qingshan liu", "hongyuan zha"], "accepted": false, "id": "1503.01239"}, "pdf": {"name": "1503.01239.pdf", "metadata": {"source": "CRF", "title": "Joint Active Learning with Feature Selection via CUR Matrix Decomposition", "authors": ["Changsheng Li", "Xiangfeng Wang", "Weishan Dong", "Junchi Yan", "Qingshan Liu", "Hongyuan Zha"], "emails": ["lcsheng@cn.ibm.com", "dongweis@cn.ibm.com"], "sections": [{"heading": null, "text": "Index Terms\u2014Active learning, feature selection, matrix factorization\nF"}, {"heading": "1 INTRODUCTION", "text": "In many real-life machine learning tasks, unlabeled data are often easily available whereas labeled data are scarce. In order to build powerful predictive models, one usually requires domain experts to manually annotate samples, but this is an expensive and time-consuming procedure. Active learning [1] provides a means to alleviate this problem by carefully selecting samples to be labeled by experts. Typically, the active learning algorithms prefer to query those unlabeled samples which can improve the prediction performance the most if they were labeled and used as training data. In this way, the active learner aims to pick out as few samples as possible to label for minimizing the total annotating cost, while an accurate supervised learning model can be built based on these labeled data.\nIn the past decade, lots of active learning algorithms have been proposed [2], [3], [4], [5], [6], and have been successfully applied to a variety of problems in computer vision [7], [8], [9], [10], [11], [12]. Generally speaking, there are two main group methods for selecting unlabeled samples to label [13]: One is to select the most informative samples, such as uncertainty sampling [14], [15], query by committee [1], and empirical risk minimization [16]. These algorithms are implemented iteratively, where a model is learned with the existing labeled data and new samples are chosen to be labeled based on the learned model. Since\n\u2022 C. Li and W. Dong are with IBM Research-China, Beijing, China. E-mail: {lcsheng,dongweis}@cn.ibm.com \u2022 X. Wang and J. Yan are with East China Normal University, Shanghai, China. E-mail: {xfwang,jcyan}@sei.ecnu.edu.cn \u2022 Q. Liu is with Nanjing University of Information Science and Technology. E-mail: qsliu@nuist.edu.cn \u2022 H. Zha is with Georgia Institute of Technology, Atlanta, USA. E-mail: zha@cc.gatech.edu\ntraining model usually needs a large number of labeled data to avoid the samples bias, the methods above should be used after sufficient labeled samples are collected [17]. The other group aims at querying the most representative samples from a perspective of data reconstruction [17], [18], [19], [20], [21]. Different from the first group, methods in this group are one-shot and non-iterative for selecting samples. Such active learning methods are usually applied when there is no initial labeled data.\nAlthough active learning has been well studied for years, it still has some issues in many real-world scenarios. For example, the sample is often characterized by highdimensional features, and some of features are often noisy or irrelevant. These noisy or irrelevant features bring adverse influence on selecting informative or representative samples. Moreover, after querying samples, some supervised learning models, such as decision tree, are often trained based on these labeled data for various applications. However, high-dimensional features significantly increase the time and space requirements for model training. Meanwhile, when only limited labeled samples are available, it is difficult to guarantee reliable model parameter estimates in high-dimensional feature space. One may state that, if we apply some state-of-the-art feature selection techniques, such as SPEC [22], Q \u2212 \u03b1 [23], to learn a low-dimensional representation before active learning, these problems might be solved. Of course, this should be helpful for active learning to some extent, while common feature selection techniques and active learning algorithms are independent in designing, directly combining them usually cannot guarantee to obtain the optimal results. Therefore, it will benefit from devising principled model and algorithm for incorporating active learning and feature selection in a unified fashar X iv :1\n50 3.\n01 23\n9v 3\n[ cs\n.L G\n] 3\nA pr\n2 01\n7\n2 ion. Recently, Joshi and Xu [24] presented an active learning method with integrated feature selection based on linear kernel SVMs and GainRatio. Raghavan et al. [25] intended to use human feedback on both features and samples for active learning. Kong et al. [26] proposed a dual feature selection and sample selection method in the context of graph classification. Bilgic [27] proposed a dynamic dimensionality reduction algorithm that determined the appropriate number of dimensions for each active learning iteration. Since all of the above three algorithms are implemented iteratively, and need to train models for querying in each iteration, they are suitable to work in the scenarios of the first group active learning methods. Different from them, we focus on studying the problem of the second active learning group, i.e., in the case when no initial labeled samples are available, by jointly learning important features and samples. This is an unsupervised learning problem, which is much harder due to the absence of labels that would guide the search for relevant information.\nIn this paper, we present a unified view of (sampled based) Active Learning and Feature Selection, called ALFS, which is inspired by the approximation method for CUR matrix decomposition.\nThe main contributions of this paper are:\ni) To our knowledge, this is the first work for presenting a unified view for one-shot active learning and feature selection, which is important for real-world applications, since it dispenses with any label effort unlike those progressive interactive labeling active learning methods.\nii) This work is the first one to formulate and build the natural connection between CUR decomposition and simultaneous sample and feature selection.\niii) We devise a novel model and convex optimization algorithm to solve the one-shot sample and feature learning problem. iv) The convergence of the proposed iterative algorithm is theoretically proved, and extensive empirical results demonstrate the advantages of our approach.\nThe rest of this paper is organized as follows: we propose a unified framework to conduct active learning and feature selection simultaneously in section 2. Section 3 reviews related work on the second group active learning algorithms. The experimental results are reported in Section 4. Section 5 presents concluding remarks and future work.\nNotations. In this paper, matrices are written as boldface uppercase letters and vectors are written as boldface lowercase letters. Given a matrix P, we denote its (i, j)-th entry, i-th row, j-th column as Pij , pi, pj , respectively. The only used vector norm is the l2 norm, denoted by \u2016 \u00b7 \u20162. A variety of norms on matrices will be used. The l1, l2,1, l\u221e norms of a matrix are defined by \u2016P\u20161 = \u2211 i,j |Pij |,\n\u2016P\u20162,1 = \u2211m i=1 \u221a\u2211n j=1 P 2 ij = \u2211m i=1 \u2016pi\u20162, and \u2016P\u2016\u221e = maxi,j |Pij |, respectively. The quasi-norm l2,0 norm of a matrix P is defined as the number of the nonzero rows of P, denoted by \u2016P\u20162,0. The Frobenius norm is denoted by \u2016P\u2016F . The Euclidean inner product between two matrices is \u3008P,Q\u3009 = tr(PTQ), where PT is the transpose of the matrix P and tr(\u00b7) is the trace of a matrix. The rank of a matrix is denoted by rank(\u00b7)."}, {"heading": "2 PROPOSED METHOD", "text": "Given an unlabeled dataset X = [x1, . . . ,xn] \u2208 Rd\u00d7n, our goal is to pick out m (m < n) samples for labeling by user, and simultaneously select r (r < d) features as the new feature representation, such that the potential performance is maximized when the model is trained based on the selected m labeled samples under the new representation. This is a more challenging problem than traditional representativeness based active learning problems, because selecting m samples to best approximate X often leads to an NP-hard problem [18], and finding r features as the most representative feature subset is also often NP-hard [28]."}, {"heading": "2.1 Active Learning and Feature Selection via Matrix Decomposition", "text": "Inspired by the CUR matrix decomposition [29], [30], [31], [32], we propose a unified framework to find the most representative samples and features. To make this paper self-contained, we first introduce CUR matrix factorization.\nDefinition 2.1. Given X \u2208 Rd\u00d7n of rank \u03c1 = rank(X), rank parameter k < \u03c1, and accuracy parameter 0 < \u03b5 < 1, the CUR factorization for X aims to find C \u2208 Rd\u00d7m with m columns from X, R \u2208 Rr\u00d7n with r rows of X, and U \u2208 Rm\u00d7r, with m, r, and rank(U) being as small as possible, such that X is reconstructed within relativeerror:\n\u2016X\u2212CUR\u20162F \u2264 (1 + \u03b5)\u2016X\u2212Xk\u20162F , (1)\nwhere Xk = Uk\u03a3kVTk \u2208 Rd\u00d7n is the best rank k matrix obtained via the SVD of X.\nFrom an algorithmic perspective, the matrices C, U, and R can be obtained by minimizing the approximation error \u2016X \u2212CUR\u20162F . Here we make a key observation that the above definition is closely related to the problem of simultaneous sample and feature selection, though to our surprise, existing works hardly point out or explore this connection to solve the active learning problem: on one hand, UR can be regarded as a reconstruction coefficient matrix, and C denotes the selected m samples, thus minimizing \u2016X \u2212 CUR\u20162F means that the total reconstruction error is minimized, which can make the data points listed in C be the most representative. The reconstruction coefficients UR are related to an r-dimensional feature subset of the dataset. Actually, the reconstruction coefficients of each reconstructed data point xi are formed by a linear combination of its r features. On the other hand, CU can be also regarded as a reconstruction coefficient matrix, and R is the new low-dimensional representation of X, so minimizing \u2016X \u2212 CUR\u20162F also indicates that the selected r features can represent the whole dataset most precisely. The construction of the coefficient matrix CU depends on a sample subset of X. Clearly, active learning and feature selection can be conducted simultaneously in such a joint framework via CUR factorization.\nDespite the above connection from CUR decomposition to feature selection and active learning, the original CUR formulation and its existing solvers can not be directly applied to solve the simultaneous feature and sample selection task due to the under-determination of a general CUR\n3 model. In the context of active sample/feature learning, this paper proposes a tailored objective function rooted from CUR decomposition, while being more informative by adding regularization terms to incorporate prior knowledge. Moreover, unlike most existing CUR solvers being randomized or heuristic algorithms [29], [30], we utilize the structured sparsity-inducing norms to relax the objective from a non-convex optimization problem to a convex one, which allows for devising an efficient variant of the alternating direction method of multipliers (ADMM) [33], [34]."}, {"heading": "2.2 A Convex Formulation", "text": "Let p = (p1, . . . , pn)T \u2208 {0, 1}n and q = (q1, . . . , qd)T \u2208 {0, 1}d denote two indicator variables to represent whether a sample and a feature is selected or not, respectively. pi = 1 (or 0) indicates that the i-th sample is selected (or not), and qi = 1 (or 0) means that the i-th feature is selected (or not). Minimizing \u2016X\u2212CUR\u20162F can be re-written as:\nmin p,q,U\u0302\u2208Rn\u00d7d\n\u2016X\u2212Xdiag(p)U\u0302diag(q)X\u20162F\ns.t. 1Tnp = m,p \u2208 {0, 1}n, (2) 1Td q = r,q \u2208 {0, 1}r,\nwhere diag(p) is a diagonal matrix with its diagonal elements being p, and 1n is an n-dimensional vector with all components being 1. Xdiag(p) in (2) aims to make m columns of X unchanged, and reset the rest n \u2212 m columns to zero vectors. diag(q)X tends to keep r rows of X unchanged, and reset the rest (d\u2212r) rows to zero vectors.\nUsing the matrix l2,0 norm, we formulate the problem in (2) as:\nmin W\u2208Rn\u00d7d\n\u2016X\u2212XWX\u20162F\ns.t. \u2016W\u20162,0 = m, \u2016WT \u20162,0 = r, (3)\nwhere W = diag(p)U\u0302diag(q) \u2208 Rn\u00d7d. Based on (3), we propose to optimize the following objective function:\nmin W\u2208Rn\u00d7d\n\u2016X\u2212XWX\u20162F + \u03b1\u2016W\u20162,0 + \u03b2\u2016WT \u20162,0, (4)\nwhere \u03b1 \u2265 0 and \u03b2 \u2265 0 are two regularization parameters. However, (4) is still an NP-hard problem due to the matrix l2,0 norm. Fortunately, there exists theoretical progress that \u2016W\u20162,1 is the minimum convex hull of \u2016W\u20162,0 [17]. The result of minimizing \u2016W\u20162,1 is the same as that of minimizing \u2016W\u20162,0, as long as W is row-sparse enough. Therefore, (4) can be relaxed to the following convex optimization problem:\nmin W\u2208Rn\u00d7d\n\u2016X\u2212XWX\u20162F + \u03b1\u2016W\u20162,1 + \u03b2\u2016WT \u20162,1. (5)"}, {"heading": "2.3 Local Linear Reconstruction", "text": "In the new objective function (5), we can see that each data point is reconstructed by a linear combination of all the selected points (when the i-th row of the reconstruction coefficient matrix WX in (5) is not a zero vector, xi is chosen as one of the most representative samples. Otherwise, xi is not selected). However, it is more reasonable to suppose that\na data point can be mainly recovered from its neighbors [20], [21]. Intuitively, if the distance between the reconstructed point and the selected point is large, the contribution of the selected point should be small to the reconstruction of the target point, and thus the reconstruction coefficient should be penalized. In light of this point, we incorporate a regularization term into (5) as:\nmin W\u2208Rn\u00d7d\n\u2016X\u2212XWX\u20162F + \u03b1\u2016W\u20162,1 + \u03b2\u2016WT \u20162,1\n+ \u03bb\u2016T (WX)\u20161, (6)\nwhere \u03bb \u2265 0 is a regularization parameter, and denotes the element-wise multiplication of two matrices. T is a weight matrix, where Tij encodes the distance between the i-th and j-th samples. From the data reconstruction perspective, if two unit vectors have the same or opposite directions, their distance should be minimal, since either vector can be fully recovered by the other one; on the contrary, if the two vectors are orthogonal, their distance should be maximal, because they have little contribution to each other\u2019s reconstruction. Therefore, we use the absolute value of the cosine function of the angle between two feature vectors to measure their similarity, and define the inverse of the absolute value as their distance:\nTij = 1\n| cos \u03b8ij | , (7)\nwhere \u03b8ij denotes the angle between xi and xj1. After obtaining the optimal W in (6), we can sort all the samples by the l2 norm of the rows of W in descending order, and select the top m samples as the representative ones. Similarly, we rank all the features by the l2 norm of the columns of W in descending order, and choose the top r features to represent the samples.\nWe take the FG-NET dataset2 as an example to illustrate the effectiveness of the l2,1 norm constraint on W and WT in (6). Fig. 1 (a) and (b) are the visualizations of l2,1 norm of W and WT , respectively. Many rows and columns in W become sparse by adding the l2,1 norm constraints on W and WT , which means that W can conduct sample selection and feature selection simultaneously."}, {"heading": "2.4 Optimization Algorithm", "text": "Although the problem (6) is convex, it is not easy to be solved by sub-gradient type methods since different structured non-smooth terms are involved. In this section, we employ the alternating direction method of multipliers (ADMM) [33] to solve (6). Theoretical results will be given then including the global convergence and iteration complexity.\n1. When cos \u03b8ij = 0, we can regularize Tij as Tij = 1| cos \u03b8ij |+\u03c2 , where \u03c2 is a very small positive constant.\n2. The dataset is available at http://sting.cycollege.ac.cy/alanitis/ fgnetaging/index.htm.\n4\nIn order to solve (6), we first introduce three variables W\u0302, W\u0303 and Z, to convert (6) to the following equivalent objective function:\nmin W,W\u0302,W\u0303,Z\n\u2016X\u2212XWX\u20162F + \u03b1\u2016W\u0302\u20162,1 + \u03b2\u2016W\u0303\u20162,1\n+ \u03bb\u2016T Z\u20161 s.t. WX = Z,W = W\u0302,WT = W\u0303. (8)\nThe augmented Lagrange function of (8) is\nL\u03c11,\u03c12,\u03c13(W,W\u0302,W\u0303,Z,\u039b1,\u039b2,\u039b3) := \u2016X\u2212XWX\u20162F + \u03b1\u2016W\u0302\u20162,1 + \u03b2\u2016W\u0303\u20162,1 + \u03bb\u2016T Z\u20161 + \u3008\u039b1,WX\u2212 Z\u3009 + \u3008\u039b2,W \u2212 W\u0302\u3009+ \u3008\u039b3,WT \u2212 W\u0303\u3009+ \u03c11 2 \u2016WX\u2212 Z\u20162F + \u03c12 2 \u2016W \u2212 W\u0302\u20162F + \u03c13 2 \u2016WT \u2212 W\u0303\u20162F , (9)\nwhere \u039b1, \u039b2 and \u039b3 are Lagrange multipliers. \u03c11, \u03c12 and \u03c13 are the constraint violation penalty parameters. From the augmented Lagrangian function, we can find that the subproblems about W\u0302, W\u0303 and Z are totally separable, as a result we can introduce the classical two-block ADMM here, while considering W and (W\u0302,W\u0303,Z) as two-block variables. Recall that in a ADMM-type algorithm, the basic Gauss-Seidel structure in (t+ 1)-th iteration is as follows,\n Wk+1 = arg minL(W,W\u0302k,W\u0303k,Zk,\u039bk1 ,\u039bk2 ,\u039bk3), W\u0302k+1 = arg minL(Wk+1,W\u0302,W\u0303k,Zk,\u039bk1 ,\u039bk2 ,\u039bk3), W\u0303k+1 = arg minL(Wk+1,W\u0302k+1,W\u0303,Zk,\u039bk1 ,\u039bk2 ,\u039bk3), Zk+1 = arg minL(Wk+1,W\u0302k+1,W\u0303k+1,Z,\u039bk1 ,\u039bk2 ,\u039bk3), \u039bk+11 = \u039b k 1 + \u03c11(W k+1X\u2212 Zk+1), \u039bk+12 = \u039b k 2 + \u03c12(W\nk+1 \u2212 W\u0302k+1), \u039bk+13 = \u039b k 3 + \u03c13((W k+1)T \u2212 W\u0303k+1).\nNext, we will introduce how to solve these subproblems in detail.\ni) Compute the subproblem about Wk+1: When the other variables are fixed with the former iteration result (W\u0302k,W\u0303k,Zk,\u039bk1 ,\u039b k 2 ,\u039b k 3), the subproblem about W k+1 is\nas follows:\nWk+1 = arg min W L\u03c11,\u03c12,\u03c13(W,W\u0302k,W\u0303k,Zk,\u039bk1 ,\u039bk2 ,\u039bk3)\n= arg min W \u2016X\u2212XWX\u20162F + \u03c11 2 \u2016WX\u2212 Zk + \u039b\nk 1\n\u03c11 \u20162F\n+ \u03c12 2 \u2016W \u2212 W\u0302k + \u039b\nk 2\n\u03c12 \u20162F + \u03c13 2 \u2016WT \u2212 W\u0303k + \u039b\nk 3\n\u03c13 \u20162F .\nThe necessary optimality condition further follows as\n\u2202L\u03c11,\u03c12,\u03c13(W,W\u0302k,W\u0303k,Zk,\u039bk1 ,\u039bk2 ,\u039bk3) \u2202W = 0. (10)\nThis implies\n(2XTX + \u03c11I)WXX T + (\u03c12 + \u03c13)W = 2X TXXT\n+\u03c11(Z k \u2212 \u039b\nk 1\n\u03c11 )XT + \u03c12(W\u0302\nk \u2212 \u039b k 2\n\u03c12 ) + \u03c13(W\u0303\nk \u2212 \u039b k 3\n\u03c13 )T .\nFor writing conveniently, let M = 2XTX+\u03c11I, and H = 2XTXXT+\u03c11(Z k\u2212\u039b k 1\n\u03c11 )XT+\u03c12(W\u0302\nk\u2212\u039b k 2\n\u03c12 )+\u03c13(W\u0303\nk\u2212\u039b k 3\n\u03c13 )T ,\nthen the equation above becomes\nMWXXT + (\u03c12 + \u03c13)W = H. (11)\nSince M and XXT are positive semi-definite and symmetric, we can do eigenvalue decomposition with all nonnegative eigenvalues, obtaining{\nM = P\u03981P T , XXT = Q\u03982Q T ,\n(12)\nwhere P and Q are both orthogonal. \u03981 and \u03982 are two diagonal matrices.\nPlugging (12) into (11), we obtain\nP\u03981P TWQ\u03982Q T + (\u03c12 + \u03c13)W = H\n\u21d2\u03981PTWQ\u03982 + (\u03c12 + \u03c13)PTWQ = PTHQ. (13)\nLet Y = PTWQ, then (13) becomes\n\u03981Y\u03982 + (\u03c12 + \u03c13)Y = P THQ\n\u21d2 Yij = (PTHQ)ij\n(\u03981)ii(\u03982)jj+\u03c12+\u03c13 , i = 1, . . . , n, j = 1, . . . , d.\nAs we know, (\u03981)ii \u2265 0, (\u03982)jj \u2265 0. In the meantime, \u03c12 and \u03c13 are greater than zero in practice, so the denominator in the equation above is greater than zero. After obtaining Y, we can easily calculate Wk+1 as\nWk+1 = PYQT (14)\nii) Further we calculate the subproblem about W\u0302k+1, i.e.,\nW\u0302k+1 = arg min W\u0302 L(Wk+1,W\u0302,W\u0303k,Zk,\u039bk1 ,\u039bk2 ,\u039bk3)\n= arg min W\u0302 \u03b1\u2016W\u0302\u20162,1 + \u03c12 2 \u2016W\u0302 \u2212Wk+1 \u2212 \u039b\nk 2\n\u03c12 \u20162F . (15)\nIn order to solve the subproblem (15), we first decouple it as:\nW\u0302k+1 = arg min W\u0302i n\u2211 i=1 \u03b1\u2016W\u0302i\u20162\n+ \u03c12 2 n\u2211 i=1 \u2016W\u0302i \u2212 (Wk+1 + \u039b k 2 \u03c12 )i\u201622, (16)\n5 where W\u0302i and (Wk+1 + 1\u03c12 \u039b k 2) i are the i-th row of matrix W\u0302 and Wk+1 + 1\u03c12 \u039b k 2 respectively. The problem (16) can be solved by the following lemma [35]: Lemma 2.1. For any \u03c3, \u03b7 > 0, and v \u2208 Rq , the minimizer of\nmin u\u2208Rq\n\u03c3\u2016u\u20162 + \u03b7\n2 \u2016u\u2212 v\u201622, (17)\nis given by\nu =\n{ (1\u2212 \u03c3\u03b7\u2016v\u20162 )v, \u2016v\u20162 > \u03c3 \u03b7\n0, \u2016v\u20162 \u2264 \u03c3\u03b7 . (18)\nBased on this lemma, we can obtain the optimal W\u0302k+1\nas\n(W\u0302k+1)i =\n{ (1\u2212 \u03b1\u03c12\u2016s\u20162 )s, \u2016s\u20162 > \u03b1 \u03c12\n0, \u2016s\u20162 \u2264 \u03b1\u03c12 , (19)\nwhere s = (Wk+1 + 1\u03c12 \u039b k 2) i.\niii) W\u0303k+1 is the minimizer for\nmin W\u0303 L(Wk+1,W\u0302k+1,W\u0303,Zk,\u039bk1 ,\u039bk2 ,\u039bk3)\n= min W\u0303 \u03b2\u2016W\u0303\u20162,1 + \u03c13 2 \u2016W\u0303 \u2212\n( (Wk+1)T +\n\u039bk3 \u03c13\n) \u20162F (20)\nSimilar to solve (15), the optimal W\u0303k+1 can be easily obtained by\n(W\u0303k+1)i =\n{ (1\u2212 \u03b2\u03c13\u2016s\u20162 )s, \u2016s\u20162 > \u03b2 \u03c13\n0, \u2016s\u20162 \u2264 \u03b2\u03c13 , (21)\nwhere s = (\n(Wk+1)T + 1\u03c13 \u039b k 3\n)i .\niv) In order to compute the subproblem about Zk+1, we need to solve\nmin Z\n(Wk+1,W\u0302k+1,W\u0303k+1,Z,\u039bk1 ,\u039b k 2 ,\u039b k 3)\n= min Z \u03bb\u2016T Z\u20161 + \u03c11 2 \u2016Z\u2212Wk+1X\u2212 \u039b\nk 1\n\u03c11 \u20162F . (22)\nThe problem (22) can be solved by the following matrix shrinkage operation Lemma [36]: Lemma 2.2. For \u00b5 > 0, and K \u2208 Rs\u00d7t, the solution of the\nproblem\nmin L\u2208Rs\u00d7t\n\u00b5\u2016L\u20161 + 1\n2 \u2016L\u2212K\u20162F ,\nis given by L\u00b5(K) \u2208 Rs\u00d7t, which is defined componentwisely by\n(L\u00b5(K))ij := max{|Kij | \u2212 \u00b5, 0} \u00b7 sgn(Kij), (23)\nwhere sgn(t) is the signum function of t \u2208 R, i.e.,\nsgn(t) :=  +1 if t > 0, 0 if t = 0, \u22121 if t < 0.\nBased on Lemma 2.2, we can obtain a closed-form solution of Zk+1 whose (i, j)-th entry is expressed as\nZk+1ij := max{|(W k+1X + \u039bk1 \u03c1 )ij | \u2212 \u03bb \u00b7Tij \u03c11 , 0}\n\u00b7 sgn((Wk+1X + \u039b k 1\n\u03c11 )ij). (24)\nThe key steps of the proposed ALFS algorithm are summarized in Algorithm 1. We can also extend our method to the kernel version by defining a new data representation to incorporate the kernel information as in [37].\nAlgorithm 1 The ALFS Algorithm Input: The data matrix X \u2208 Rd\u00d7n, parameters \u03b1, \u03b2, and \u03bb. Initialize: W0 = W\u03020 = 0, W\u03030 = 0, Z0 = 0, \u039b01 = 0, \u039b 0 2 = 0, \u039b 0 3 = 0, \u03c11 = \u03c12 = \u03c13 = 10\u22126,max\u03c1 = 1010, \u03c4 = 1.1, = 10\u22123, k = 0. while not converged do\n1. fix the other variables and update Wk+1 by (14); 2. fix the other variables and update W\u0302k+1 by (19); 3. fix the other variables and update W\u0303k+1 by (21); 4. fix the other variables and update Zk+1 by (24); 5. update the multipliers\n\u039bk+11 = \u039b k 1 + \u03c11(W k+1X\u2212 Zk+1), \u039bk+12 = \u039b k 2 + \u03c12(W\nk+1 \u2212 W\u0302k+1), \u039bk+13 = \u039b k 3 + \u03c13 ( (Wk+1)T \u2212 W\u0303k+1 ) ; 6. update the parameters \u03c11, \u03c12, and \u03c13 by \u03c11 = min(\u03c4\u03c11,max\u03c1), \u03c12 = min(\u03c4\u03c12,max\u03c1), \u03c13 = min(\u03c4\u03c13,max\u03c1); 7. k \u2190 k + 1; 8. check the convergence conditions \u2016WkX\u2212 Zk\u2016\u221e < and \u2016Wk \u2212 W\u0302k\u2016\u221e < and \u2016(Wk)T \u2212 W\u0303k\u2016\u221e < and | f(W k)\u2212f(Wk\u22121) f(Wk\u22121)\n| < , where f(Wk) is the objective function value of (6) at the point Wk .\nend while Output: The matrix Wk \u2208 Rn\u00d7d."}, {"heading": "2.5 Algorithm Analysis", "text": "From the framework of ALFS, we can find that Algorithm 1 is the direct application of the classical two-block ADMM, although the problem has more than two block variables. All the subproblems in Algorithm 1 have closed-form solutions. Based on the classical convergence results, we can obtain the global convergence of Algorithm 1 to the primaldual optimal solution of problem (8) (see [38], [39]). In the following we present both the global convergence and the iteration complexity results of Algorithm 1. Theorem 2.1. For given constant parameters \u03b1, \u03b2, \u03b3 and\ngiven constant penalty parameters \u03c11, \u03c12, \u03c13. Denote the iteration sequence generated by Algorithm 1 as \u03a3k := { Wk,W\u0302k,W\u0303k,Zk,\u039bk1 ,\u039b k 2 ,\u039b k 3 } , \u03a3\u0303k := 1k+1 \u2211k t=0 \u03a3 t, \u03a3k1 := { Wk,W\u0302k,W\u0303k,Zk } ,\n\u03a3k2 := { \u039bk1 ,\u039b k 2 ,\u039b k 3 } .\nThen we have the following results: 1) (Global Convergence) The sequence { \u03a3k }\nconverges to a primal-dual optimal solution pair (W\u221e,W\u0302\u221e,W\u0303\u221e,Z\u221e,\u039b\u221e1 ,\u039b \u221e 2 ,\u039b \u221e 3 ), where (W\u221e,W\u0302\u221e,W\u0303\u221e,Z\u221e) is the global optimal solution of problem (8) and W\u221e is the global optimal solution of problem (6). 2) (Constraint Satisfactory) Both constraint violations will converge to zero, e.g. \u2225\u2225WkX\u2212 Zk\u2225\u2225 F \u2192 0,\u2225\u2225\u2225Wk \u2212 W\u0302k\u2225\u2225\u2225\nF \u2192 0,\u2225\u2225\u2225(Wk)T \u2212 W\u0303k\u2225\u2225\u2225 F \u2192 0.\n6\n3) (Ergodic Iteration Complexity [40]) Let (W\u2217,W\u0302\u2217,W\u0303\u2217,Z\u2217,\u039b\u22171,\u039b \u2217 2,\u039b \u2217 3) be an optimal solution\npair, we have\nL\u03c11,\u03c12,\u03c13(\u03a3\u0303k1 ,\u03a3\u22172)\u2212 L\u03c11,\u03c12,\u03c13(\u03a3\u22171, \u03a3\u0303k2) \u2264 C1 k + 1 ,\n(25) where C1 denotes a constant related with \u03a30 and \u03a3\u2217. 4) (Non-ergodic Iteration Complexity [41]) The nonergodic iteration complexity can be written as\u2225\u2225\u2225\u03a3k \u2212\u03a3k+1\u2225\u2225\u22252\nH \u2264 C2 k + 1 , (26)\nwhere C2 also denotes a constant related with \u03a30 and \u03a3\u2217 andH is a matrix related with X as follows,\nH =  S 0 0 0 0 0 0 0 \u03c12I . . . . . . . . . . . . 0 0 . . . \u03c13I . . . . . . . . . 0 0 . . . . . . \u03c11I . . . . . . 0 0 . . . . . . . . . 1 \u03c11 I . . . 0 0 . . . . . . . . . . . . 1 \u03c12 I 0\n0 0 0 0 0 0 1\u03c13 I\n ,\nwhere S = \u03c11XTX + (\u03c12 + \u03c13)I.\nFor the detailed proof, please refer to [39], [40]. We do not present them here to save space. The first and second parts of this theorem show the global convergence of the presented algorithm, including sequence convergence and constraint convergence. From the first part, we can find that the sequence converges to the primal-dual optimal solution pair, while the second part shows the two linear constraints converge to zero in the sense of Frobenius norm.\nThe third and fourth parts above show a global convergence speed of ADMM, in the sense of ergodic and non-ergodic respectively. The inequality (25) is the ergodic iteration complexity, which denotes the characterization of -optimal based on primal-dual optimality gap as follows,\nGap(\u03a31,\u03a32) := L\u03c11,\u03c12(\u03a31,\u03a3\u22172)\u2212 L\u03c11,\u03c12(\u03a3\u22171,\u03a32) \u2264 . (27)\nThus, it means that after k iterations, we can obtain an O(1/k)-optimal solution. The inequality (26) calculates the optimality condition between adjacent iterations, although this can not indicate convergence, but it really can accelerate the global convergence.\nThe above theorem not only shows the global convergence of Algorithm 1, but also presents two cases of iteration complexity. The global convergence means that the generated sequence converges to the optimal solution based on any initial point. Further the iteration complexity results mean that how good the iteration result is after k iterations. We can also find that both iteration complexity results are O(1/k), which is in the same order with many first-order algorithms."}, {"heading": "3 RELATED WORK", "text": "As described, the work most related to our proposed approach is the second group active learning methods that intend to select the most representative samples. In this section, we will briefly provide a review of the approaches in this group. Among them, the most popular one is the Transductive Experimental Design (TED) [18]. TED aimed to find a representative sample subset from the unlabeled dataset, such that the dataset can be best approximated by linear combinations of the selected samples. Since this optimization problem is NP-hard, [18] proposed a suboptimal sequential optimization algorithm and a non-greedy optimization algorithm to solve it, respectively.\nFollowing TED, more active learning algorithms have been developed. Cai and He [20] extended TED to choose samples by utilizing a nearest neighbor graph to capture intrinsic local manifold structure, where the graph Laplacian is incorporated into a manifold adaptive kernel space. Zhang et al. [42] adopted the idea from Locally Linear Embedding (LLE) [43] to find the reconstruction coefficients. They represented each sample by a linear combination of its neighbors, which can well preserve the local geometrical structure of the data. Similar to [42], Hu et al. [21] incorporated the local geometrical information into the active learning process. Specifically, they introduced a regularization term to make the nearer neighbors have much effect on the linear reconstruction of data point, and penalized the selected samples distant from the reconstructed sample severely. Nie et al. [17] proposed a novel method to relax the objective of TED to an efficient convex formulation, and utilized the robust sparse representation loss function to reduce the effect of outliers."}, {"heading": "4 EXPERIMENT", "text": "In this section, we empirically evaluate the proposed method, ALFS, on six publicly available datasets, including artificial data, microarray data, image data, and video data."}, {"heading": "4.1 Experimental Setting", "text": "Dataset We evaluate the performance of our ALFS on six datasets, including 1) an artificial dataset, Madelon [44],\n7 used in the NIPS 2003 feature selection challenge; 2) two preprocessed microarray datasets, TOX-171 [45] and Musk [44], for studying toxicity prediction and musk activity prediction respectively; 3) two image datasets, ORL and FGNET, which are widely used for face recognition [46] and facial age estimation [47], [48], respectively. For FG-NET, we conduct age range estimation as in [49], and divide the dataset into five age groups in the experiment; 4) a video dataset, UCF11 [50], which is collected from YouTube. It contains 11 video action categories, which are very challenging to be recognized due to large variations in camera motion, object appearance, pose, object scale, viewpoint, cluttered background, and illumination conditions. Datasets from different areas serve as a good test bed for a comprehensive evaluation. Table 1 summarizes the details of the datasets used in the experiments. Compared methods Since ALFS is related to the second group of active learning algorithms, we compare it with some state-of-the-art approaches in this group to demonstrate the effectiveness of ALFS. The compared methods in the experiments are listed as follows:\n\u2022 Random Sampling (RS): randomly selects samples from the candidate dataset, which is used as the baseline for active learning. \u2022 Transductive Experimental Design (TED) [18]3: an active learning method developing experimental design in a transductive setting. \u2022 RRSS [17]4: an active learning method taking advantage of robust representation and structure sparsity. \u2022 Active Learning via Neighbor Reconstruction (ALNR) [21]: an active learning method using neighborhood reconstruction. \u2022 R-CUR [30]: a randomized algorithmic approach for solving CUR matrix factorization. We name it R-CUR for short. \u2022 ALFS-I: our proposed method for selecting representative sample and feature selection simultaneously, but without considering local linear reconstruction. \u2022 ALFS-II: our proposed method for simultaneous active learning and feature selection with local linear reconstruction.\nTo further show the benefit of simultaneous active sample selection and feature selection, we also compare our ALFS against some feature selection approaches combined with the active learning approaches above, i.e., first using feature selection methods to reduce the dimensionality, and then applying the active learning methods above to select samples based on the new low-dimensional representation. We use three kinds of unsupervised feature selection methods, Laplacian5 [51], SPEC6 [22], and SOGFS7 [46], to com-\n3. A sequential solver can be downloaded from http://www.dbs.ifi. lmu.de/\u223cyu k/ted/.\n4. The code can be downloaded from http://www.escience.cn/ people/fpnie/papers.html.\n5. We downloaded the code from http://www.cad.zju.edu.cn/ home/dengcai/Data/MCFS.html.\n6. The code can be downloaded from http://featureselection.asu. edu/software.php.\n7. The code can be downloaded from http://www.escience.cn/ people/fpnie/index.html.\nbine with the active learning algorithms in the experiments, respectively. Experiments protocol Following [17], for each dataset, we first randomly select 50% of the data points as candidate samples for training, from which we apply the compared active learning methods to select a subset of samples to request human labeling. Using the selected samples and their queried labels as training data, we learn a classification model, and evaluate the representativeness of the selected samples in terms of classification accuracy on the rest 50% data samples. The latter is regarded as the testing data. In order to demonstrate that our method is not sensitive to different classifiers, we use two kinds of classical classification models, support vector machine (SVM) and decision tree, to evaluate the effectiveness of the proposed method, respectively. For simplicity, we use the linear kernel in SVM, and fix the hyperparameter C = 100 through the experiments. The parameters \u03b1, \u03b2, and \u03bb in our algorithm are searched from {10\u22124, 10\u22123, . . . , 100, . . . , 103}. For a fair comparison, the parameters in TED, RRSS, and ALNR are also searched from the same space. In the experiment, we repeat every test case for 10 times, and report the average classification performance."}, {"heading": "4.2 Experimental Result", "text": "Comparison with Active Learning Algorithms In order to demonstrate the effectiveness of our ALFS in selecting representative samples, we compare ALFS with some stateof-the-art active learning algorithms. For ALFS and R-CUR, we vary the number of selected features from 10 to 100 with an incremental step of 10 on all the datasets8, and report the best results. The results are shown in Fig. 2 and Fig. 3. We can observe that our ALFS-I obtains better performance than all the other candidates on all the datasets, which shows that joint active learning with feature selection is beneficial to improving classification accuracy. ALFS-II achieves the best classification performance among all the datasets under different classifiers. On certain datasets, such as the Madelon dataset, our ALFS-I and ALFS-II are significantly better than the other methods. For the Madelon dataset, when the number of the selected samples is set to 1200 and using SVM as the classifier, ALFS-I and ALFS-II obtain 14.8% and 15.4% relative improvement over the second best result, i.e., ALNR, respectively. When combined with decision tree, ALFS-I and ALFS-II attain 11.3% and 12.3% relative improvement over ALNR, respectively. In addition, ALFS-II outperforms ALFS-I, which means that incorporating local linear reconstruction into the process of active learning and feature selection is helpful for improving performance. Moreover, we also observe some other interesting phenomenon. First, we note that R-CUR does not show its competency, compared to our ALFS-I and ALFS-II on almost all the datasets. The reason is that R-CUR [30] is a general CUR model and adopts a randomized algorithmic approach to seek the matrices C and R for satisfying (1). It does not consider it as an optimization problem, making the selected samples and features hardly be the most representative,\n8. When the user inputs the desired number of samples m and the number of features r, the final outputs m\u2032 and r\u2032 of R-CUR [30] may be slightly different m and r, respectively.\n8 # Query 200 400 600 800 1000 1200 A cc ur ac y 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 RS TED ALNR RRSS R-CUR ALFS-I ALFS-II\n(a) Madelon\n# Query 5 10 15 20 25 30 35 40 45 50 55 60 65 70\nA cc\nur ac\ny\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\nRS TED ALNR RRSS R-CUR ALFS-I ALFS-II\n(b) TOX-171\n# Query 100 200 300 400 500 600 700\nA cc\nur ac\ny\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\nRS TED ALNR RRSS R-CUR ALFS-I ALFS-II\n(c) UCF11\n# Query 10 30 50 70 90 110 130 150 170 190\nA cc\nur ac\ny\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\nRS TED ALNR RRSS R-CUR ALFS-I ALFS-II\n(d) Musk\n# Query 10 30 50 70 90 110 130 150 170\nA cc\nur ac\ny\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nRS TED ALNR RRSS R-CUR ALFS-I ALFS-II\n(e) ORL\n# Query 50 100 150 200 250 300 350 400\nA cc\nur ac\ny\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\nRS TED ALNR RRSS R-CUR ALFS-I ALFS-II\n(f) FG-NET\n9\nwhich limits R-CUR to be directly applied to active learning and feature selection. Second, on the Madelon dataset and the TOX-171 dataset, the state-of-the-art active learning methods, ALNR, RRSS, TED, have poor classification performance, and do not improve the accuracy apparently as the number of the selected samples to be labeled increases, when use SVM as the final classifier. This is because that for the Madelon dataset, there are lots of noisy features (Based on the introduction in the webpage9, there are only 5 informative features, 15 linear combinations of these five features, and 480 distractor features having no predictive power.), and for the TOX-171 dataset, there are 5748 features, being relatively large to the number of the samples. Thus, when using these features to train SVM without dimension reduction or removing noise variables, the model is very easy to overfit, which degrades the generality ability of the model. This also indicates that active learning can benefit from feature selection. In contrast, when combined with decision tree, these active learning methods obtain good results on these two datasets. The reason is that decision tree only employs a small subset of features to from the decision hyperplane, which can play the role of feature selection for removing noisy or redundant features to some extent. Third, active learning methods perform better performance than random sampling in general, especially when combined with decision tree. This shows that it is indeed meaningful to learn to select samples for human labeling in the scenario\n9. https://archive.ics.uci.edu/ml/datasets/Madelon\nof supervised learning. Comparison with Feature Selection + Active Learning In order to demonstrate the necessary of simultaneous sample and feature selection, we compare ALFS with some unsupervised feature selection methods combined with the active learning algorithms above. We fix the number of the selected samples to the truncations as shown in Fig. (2), and test the classification accuracies with different feature dimensions. The results are reported in Table 2-7. We can see that when using SVM or decision tree as the classifier, both of our ALFS-I and ALFS-II outperform those approaches treating sample selection and feature selection as two separate steps. Still taking the Madelon dataset as an example, when the number of selected features is set to 10, ALFS-II achieves 8.0% relative improvement over RRSS combined with Laplacian and SVM, 10.7% relative improvement over RRSS with SPEC and SVM, 15.1% relative improvement over RRSS with Laplacian and decision tree, and 20.8% relative improvement over RRSS with SPEC and decision tree, respectively. This further indicates that simultaneous sample and feature selection is promising for obtaining better performance. In addition, ALFS-II achieves better results than ALFS-I under various dimensions, which can come to the same conclusion as mentioned above. In the meantime, we also observe that our method usually has competitive results at the lower dimensions, and even has higher accuracies than using all the features under most of the datasets. It also verifies that it is meaningful to simultaneous active learning and feature selection.\n10\nCoupling of Active Learning and Feature Selection In order to further show the coupling of active learning and feature, i.e., noisy and redundant features can bring adverse effect on sample selection, while \u2018good\u2019 samples will be beneficial to feature selection, we conduct deep studies on the TOX-171 dataset. We first show the benefit to active learning through embedding feature selection. The results are listed in Table 8(a) and Table 8(b). In Table 8(a), when fixing the number of the queries, the performance of using a small subset of all the features is always better than that of using all the features. And for Table 8(b), the accuracies of using a subset of all the features are supervisor to those of using all the features under most of the cases. Even though it is not higher, the performance of using feature subset is still comparable to that of using all the features. Therefore, it is obvious that embedding feature selection is good for learning representative samples. Next, we will demonstrate that active learning is also helpful to learning informative features. In order to make our experiments more practical and challenging, we add 20% noisy data samples into the original dataset to form a new dataset. The noisy variable is sampled from the standard normal distribution, and the noisy label is drawn from the discrete uniform distribution on [1, 2, 3, 4]. Based on the new dataset, we randomly divide it into two parts: one part is used as the candidate set to query representative\n11\nsamples for training, and the other part is used as the testing set. When querying the selected samples, we use SVM and decision tree as the final classifier for classifying the testing data, respectively. Table 9(a) and Table 9(b) report the results. With the fixed feature dimensions, querying all the samples, i.e., 102 samples, for training can not obtain the best classification performance. In contrast, only select a small subset of samples for requesting human labeling can significantly improve the classification accuracy, compared with querying all the samples. This indicates that \u2018good\u2019 samples are indeed beneficial to learning informative features."}, {"heading": "4.3 CPU Time and Sensitivity Analysis", "text": "We test the CPU running time with different convergence tolerance on the Madelon dataset and the FG-NET dataset. The experiments are conducted on a laptop with Intel(R)Core(TM) CPUs of 3.20 GHz and 4 GB RAM, and ALFS-II is implemented using MATLAB R2014b 64bit edition without parallel operation. The result is shown in Fig. 4. The CPU time grows linearly with increasing on both datasets.\nWe also study the sensitivity of our algorithm to the parameters, \u03b1, \u03b2, and \u03bb, on the Madelon dataset. In the experiment, we first fix the number of the selected features to 10, and set the number of the selected samples to 1200.\n12\nThen, we fix one parameter, and vary the other two parameters. We report the accuracy of our algorithm with SVM as the final classifier. The results are shown in Fig. 5. We can see that our method is not sensitive to all the parameters with wide ranges. In Fig. 5(a), when \u03b1 and \u03b2 are set to small values, the performance of the model degrades significantly. This is because the smaller \u03b1 and \u03b2 are, the lower the weight of the second and the third terms in (6) is. In this way, it is hard to guarantee that the columns and the rows of the matrix W are sparse, which makes our algorithm fail to learn representative samples and features. Therefore, we should set larger \u03b1 and \u03b2 in practice."}, {"heading": "5 CONCLUSIONS AND FUTURE WORK", "text": "In this paper, we present a unified framework to simultaneously conduct active sample learning and feature selection (ALFS). Given an unlabeled dataset, our formulation naturally and effectively incorporates feature and sample selection by solving a regularized optimization problem rooted from CUR factorization. We further relax the original NPhard non-convex problem into a convex one by introducing the structured sparsity-inducing norms, which allows for efficient iterative optimization algorithm (ADMM). The superior performance of our method over the state-of-the-art methods is verified by extensive experimental evaluations with six benchmark datasets.\nSeveral interesting directions can be followed up, which are not covered by our current work:\n\u2022 Leveraging labeled samples: ALFS selects samples and features from a perspective of data reconstruction in an unsupervised setting. If label information is available, we can incorporate such prior information into our framework, e.g., taking the objective function of [52] as a regularization term. This would be helpful if a specific prediction task is actually only\nrelevant to a few features and our \u2018blind\u2019 feature selection method may keep unnecessary features although they are indispensable to represent the sample set itself. \u2022 Online learning: ALFS works in a batch mode, i.e., the unlabeled dataset is available. We can further extend our work to online learning mode, such that ALFS can efficiently and effectively handle the case when new samples are coming in. \u2022 Additional regularization terms: In our work, motivated by the local reconstruction philosophy, we add the cross-sample regularization term as presented in Sec.2.3. This term alleviates the underdetermination condition of the factorization problem, and contributes to the robustness of our method. Symmetrically, a cross-feature regularization term can be also applied."}], "references": [{"title": "Selective sampling using the query by committee algorithm", "author": ["Y. Freund", "H.S. Seung", "E. Shamir", "N. Tishby"], "venue": "Machine Learning, vol. 28, no. 2-3, pp. 133\u2013168, 1997.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Active learning with statistical models", "author": ["D.A. Cohn", "Z. Ghahramani", "M.I. Jordan"], "venue": "Journal of Artificial Intelligence Research, vol. 4, pp. 129\u2013145, 1996.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Active learning by querying informative and representative examples", "author": ["S.-J. Huang", "R. Jin", "Z.-H. Zhou"], "venue": "Advances in Neural Information Processing Systems, 2010, pp. 892\u2013900.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-domain active learning for text classification", "author": ["L. Li", "X. Jin", "S.J. Pan", "J.-T. Sun"], "venue": "Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2012, pp. 1086\u20131094.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "A convex optimization framework for active learning", "author": ["E. Elhamifar", "G. Sapiro", "A. Yang", "S.S. Sasrty"], "venue": "IEEE International Conference on Computer Vision (ICCV). IEEE, 2013, pp. 209\u2013216.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Bidirectional active learning: A two-way exploration into unlabeled and labeled data set", "author": ["X.-Y. Zhang", "S. Wang", "X. Yun"], "venue": "IEEE Transactions on Neural Network and Learning Systems (TNNLS), 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Twodimensional active learning for image classification", "author": ["G.-J. Qi", "X.-S. Hua", "Y. Rui", "J. Tang", "H.-J. Zhang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2008, pp. 1\u20138.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Active learning for large multi-class problems", "author": ["P. Jain", "A. Kapoor"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2009, pp. 762\u2013769.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-class active learning for image classification", "author": ["A.J. Joshi", "F. Porikli", "N. Papanikolopoulos"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2009, pp. 2372\u20132379.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Far-sighted active learning on a budget for image and video recognition", "author": ["S. Vijayanarasimhan", "P. Jain", "K. Grauman"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010, pp. 3035\u20133042.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Beyond comparing image pairs: Setwise active learning for relative attributes", "author": ["L. Liang", "K. Grauman"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEE, 2014, pp. 208\u2013215.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Large-scale live active learning: Training object detectors with crawled data and crowds", "author": ["S. Vijayanarasimhan", "K. Grauman"], "venue": "International Journal of Computer Vision, vol. 108, no. 1-2, pp. 97\u2013114, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Querying discriminative and representative samples for batch mode active learning", "author": ["Z. Wang", "J. Ye"], "venue": "Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2013, pp. 158\u2013166.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "A sequential algorithm for training text classifiers", "author": ["D.D. Lewis", "W.A. Gale"], "venue": "Proceedings of the 17th ACM SIGIR International Conference on Research and Development in Information Retrieval. Springer-Verlag New York, Inc., 1994, pp. 3\u201312.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1994}, {"title": "Multiclass active learning by uncertainty sampling with diversity maximization", "author": ["Y. Yang", "Z. Ma", "F. Nie", "X. Chang", "A.G. Hauptmann"], "venue": "International Journal of Computer Vision, vol. 113, no. 2, pp. 113\u2013127, 2014.  13", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Toward optimal active learning through monte carlo estimation of error reduction", "author": ["N. Roy", "A. McCallum"], "venue": "Proceedings of International Conference on Machine Learning, 2001, pp. 441\u2013448.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "Early active learning via robust representation and structured sparsity", "author": ["F. Nie", "H. Wang", "H. Huang", "C. Ding"], "venue": "Proceedings of the 23th International Joint Conference on Artificial Intelligence. AAAI Press, 2013, pp. 1572\u20131578.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Active learning via transductive experimental design", "author": ["K. Yu", "J. Bi", "V. Tresp"], "venue": "Proceedings of the 23th International Conference on Machine Learning. ACM, 2006, pp. 1081\u20131088.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Batch mode active sampling based on marginal probability distribution matching", "author": ["R. Chattopadhyay", "Z. Wang", "W. Fan", "I. Davidson", "S. Panchanathan", "J. Ye"], "venue": "Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2012, pp. 741\u2013749.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Manifold adaptive experimental design for text categorization", "author": ["D. Cai", "X. He"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 24, no. 4, pp. 707\u2013719, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Active learning via neighborhood reconstruction", "author": ["Y. Hu", "D. Zhang", "Z. Jin", "D. Cai", "X. He"], "venue": "Proceedings of the 23th International Joint Conference on Artificial Intelligence. AAAI Press, 2013, pp. 1415\u20131421.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Spectral feature selection for supervised and unsupervised learning", "author": ["Z. Zhao", "H. Liu"], "venue": "Proceedings of the 24th international conference on Machine learning. ACM, 2007, pp. 1151\u20131157.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Feature selection for unsupervised and supervised inference: The emergence of sparsity in a weight-based approach", "author": ["L. Wolf", "A. Shashua"], "venue": "The Journal of Machine Learning Research, vol. 6, pp. 1855\u20131887, 2005.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1855}, {"title": "Using active learning with integrated feature selection", "author": ["J. Hemant", "X. Xiaowei"], "venue": "Technical Report UALR06-02, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Active learning with feedback on features and instances", "author": ["H. Raghavan", "O. Madani", "R. Jones"], "venue": "The Journal of Machine Learning Research, vol. 7, pp. 1655\u20131686, 2006.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Dual active feature and sample selection for graph classification", "author": ["X. Kong", "W. Fan", "P.S. Yu"], "venue": "Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2011, pp. 654\u2013662.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Combining active learning and dynamic dimensionality reduction.", "author": ["M. Bilgic"], "venue": "SIAM International Conference on Data Mining,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "A variance minimization criterion to feature selection using laplacian regularization", "author": ["X. He", "M. Ji", "C. Zhang", "H. Bao"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 10, pp. 2013\u20132025, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Optimal cur matrix decompositions", "author": ["C. Boutsidis", "D.P. Woodruff"], "venue": "arXiv preprint arXiv:1405.7910, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Cur matrix decompositions for improved data analysis", "author": ["M.W. Mahoney", "P. Drineas"], "venue": "Proceedings of the National Academy of Sciences, vol. 106, no. 3, pp. 697\u2013702, 2009.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Relative-error cur matrix decompositions", "author": ["P. Drineas", "M.W. Mahoney", "S. Muthukrishnan"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 30, no. 2, pp. 844\u2013881, 2008.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Improving cur matrix decomposition and the nystr\u00f6m approximation via adaptive sampling", "author": ["S. Wang", "Z. Zhang"], "venue": "The Journal of Machine Learning Research, vol. 14, no. 1, pp. 2729\u20132769, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "A dual algorithm for the solution of nonlinear variational problems via finite element approximation", "author": ["D. Gabay", "B. Mercier"], "venue": "Computers & Mathematics with Applications, vol. 2, no. 1, pp. 17\u201340, 1976.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1976}, {"title": "Linearized alternating direction method with adaptive penalty for low-rank representation", "author": ["Z. Lin", "R. Liu", "Z. Su"], "venue": "Advances in Neural Information Processing Systems, 2011, pp. 612\u2013620.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "A fast algorithm for edgepreserving variational multichannel image restoration", "author": ["J. Yang", "W. Yin", "Y. Zhang", "Y. Wang"], "venue": "SIAM Journal on Imaging Sciences, vol. 2, no. 2, pp. 569\u2013592, 2009.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices", "author": ["Z. Lin", "M. Chen", "L. Wu", "Y. Ma"], "venue": "Technical report, UIUC Technical Report UILU-ENG-09-2215, 2009.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning high-order task relationships in multi-task learning", "author": ["Y. Zhang", "D.-Y. Yeung"], "venue": "Proceedings of the 23-th International Joint Conference on Artificial Intelligence. AAAI Press, 2013, pp. 1917\u2013 1923.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning, vol. 3, no. 1, pp. 1\u2013122, 2011.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "A new inexact alternating directions method for monotone variational inequalities", "author": ["B. He", "L.-Z. Liao", "D. Han", "Y. Hai"], "venue": "Mathematical Programming, vol. 92, no. 1, pp. 103\u2013118, 2002.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2002}, {"title": "On the o(1/n) convergence rate of the douglas-rachford alternating direction method", "author": ["B. He", "X. Yuan"], "venue": "SIAM Journal on Numerical Analysis, vol. 50, no. 2, pp. 700\u2013709, 2012.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "On non-ergodic convergence rate of douglas\u2013rachford alternating direction method of multipliers", "author": ["\u2014\u2014"], "venue": "Numerische Mathematik, pp. 1\u201311, 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Active learning based on locally linear reconstruction", "author": ["L. Zhang", "C. Chen", "J. Bu", "D. Cai", "X. He", "T.S. Huang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 10, pp. 2026\u20132038, 2011.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2026}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, vol. 290, no. 5500, pp. 2323\u2013 2326, 2000.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2000}, {"title": "UCI machine learning repository", "author": ["M. Lichman"], "venue": "2013. [Online]. Available: http://archive.ics.uci.edu/ml", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "Toxicity from radiation therapy associated with abnormal transcriptional responses to dna damage", "author": ["K.E. Rieger", "W.-J. Hong", "V.G. Tusher", "J. Tang", "R. Tibshirani", "G. Chu"], "venue": "Proceedings of the National Academy of Sciences of the United States of America, vol. 101, no. 17, pp. 6635\u20136640, 2004.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2004}, {"title": "Unsupervised feature selection with structured graph optimization", "author": ["F. Nie", "W. Zhu", "X. Li"], "venue": "Thirtieth AAAI Conference on Artificial Intelligence, 2016, pp. 1302\u20131308.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}, {"title": "Demographic estimation from face images: Human vs. machine performance.", "author": ["H. Han", "C. Otto", "X. Liu", "A.K. Jain"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Learning ordinal discriminative features for age estimation", "author": ["C. Li", "Q. Liu", "J. Liu", "H. Lu"], "venue": "Computer Vision and Pattern Recognition, 2012, pp. 2570\u20132577.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Recognizing realistic actions from videos", "author": ["J. Liu", "J. Luo", "M. Shah"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2009, pp. 1996\u20132003.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2009}, {"title": "Laplacian score for feature selection", "author": ["X. He", "D. Cai", "P. Niyogi"], "venue": "Advances in Neural Information Processing Systems, 2005, pp. 507\u2013514.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2005}, {"title": "Trace ratio criterion for feature selection", "author": ["F. Nie", "S. Xiang", "Y. Jia", "C. Zhang", "S. Yan"], "venue": "AAAI\u201908 Proceedings of the 23rd national conference on Artificial intelligence, vol. 2, 2008, pp. 671\u2013676.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Active learning [1] provides a means to alleviate this problem by carefully selecting samples to be labeled by experts.", "startOffset": 16, "endOffset": 19}, {"referenceID": 1, "context": "In the past decade, lots of active learning algorithms have been proposed [2], [3], [4], [5], [6], and have been successfully applied to a variety of problems in computer vision [7], [8], [9], [10], [11], [12].", "startOffset": 74, "endOffset": 77}, {"referenceID": 2, "context": "In the past decade, lots of active learning algorithms have been proposed [2], [3], [4], [5], [6], and have been successfully applied to a variety of problems in computer vision [7], [8], [9], [10], [11], [12].", "startOffset": 79, "endOffset": 82}, {"referenceID": 3, "context": "In the past decade, lots of active learning algorithms have been proposed [2], [3], [4], [5], [6], and have been successfully applied to a variety of problems in computer vision [7], [8], [9], [10], [11], [12].", "startOffset": 84, "endOffset": 87}, {"referenceID": 4, "context": "In the past decade, lots of active learning algorithms have been proposed [2], [3], [4], [5], [6], and have been successfully applied to a variety of problems in computer vision [7], [8], [9], [10], [11], [12].", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "In the past decade, lots of active learning algorithms have been proposed [2], [3], [4], [5], [6], and have been successfully applied to a variety of problems in computer vision [7], [8], [9], [10], [11], [12].", "startOffset": 94, "endOffset": 97}, {"referenceID": 6, "context": "In the past decade, lots of active learning algorithms have been proposed [2], [3], [4], [5], [6], and have been successfully applied to a variety of problems in computer vision [7], [8], [9], [10], [11], [12].", "startOffset": 178, "endOffset": 181}, {"referenceID": 7, "context": "In the past decade, lots of active learning algorithms have been proposed [2], [3], [4], [5], [6], and have been successfully applied to a variety of problems in computer vision [7], [8], [9], [10], [11], [12].", "startOffset": 183, "endOffset": 186}, {"referenceID": 8, "context": "In the past decade, lots of active learning algorithms have been proposed [2], [3], [4], [5], [6], and have been successfully applied to a variety of problems in computer vision [7], [8], [9], [10], [11], [12].", "startOffset": 188, "endOffset": 191}, {"referenceID": 9, "context": "In the past decade, lots of active learning algorithms have been proposed [2], [3], [4], [5], [6], and have been successfully applied to a variety of problems in computer vision [7], [8], [9], [10], [11], [12].", "startOffset": 193, "endOffset": 197}, {"referenceID": 10, "context": "In the past decade, lots of active learning algorithms have been proposed [2], [3], [4], [5], [6], and have been successfully applied to a variety of problems in computer vision [7], [8], [9], [10], [11], [12].", "startOffset": 199, "endOffset": 203}, {"referenceID": 11, "context": "In the past decade, lots of active learning algorithms have been proposed [2], [3], [4], [5], [6], and have been successfully applied to a variety of problems in computer vision [7], [8], [9], [10], [11], [12].", "startOffset": 205, "endOffset": 209}, {"referenceID": 12, "context": "Generally speaking, there are two main group methods for selecting unlabeled samples to label [13]: One is to select the most informative samples, such as uncertainty sampling [14], [15], query by committee [1], and empirical risk minimization [16].", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": "Generally speaking, there are two main group methods for selecting unlabeled samples to label [13]: One is to select the most informative samples, such as uncertainty sampling [14], [15], query by committee [1], and empirical risk minimization [16].", "startOffset": 176, "endOffset": 180}, {"referenceID": 14, "context": "Generally speaking, there are two main group methods for selecting unlabeled samples to label [13]: One is to select the most informative samples, such as uncertainty sampling [14], [15], query by committee [1], and empirical risk minimization [16].", "startOffset": 182, "endOffset": 186}, {"referenceID": 0, "context": "Generally speaking, there are two main group methods for selecting unlabeled samples to label [13]: One is to select the most informative samples, such as uncertainty sampling [14], [15], query by committee [1], and empirical risk minimization [16].", "startOffset": 207, "endOffset": 210}, {"referenceID": 15, "context": "Generally speaking, there are two main group methods for selecting unlabeled samples to label [13]: One is to select the most informative samples, such as uncertainty sampling [14], [15], query by committee [1], and empirical risk minimization [16].", "startOffset": 244, "endOffset": 248}, {"referenceID": 16, "context": "edu training model usually needs a large number of labeled data to avoid the samples bias, the methods above should be used after sufficient labeled samples are collected [17].", "startOffset": 171, "endOffset": 175}, {"referenceID": 16, "context": "The other group aims at querying the most representative samples from a perspective of data reconstruction [17], [18], [19], [20], [21].", "startOffset": 107, "endOffset": 111}, {"referenceID": 17, "context": "The other group aims at querying the most representative samples from a perspective of data reconstruction [17], [18], [19], [20], [21].", "startOffset": 113, "endOffset": 117}, {"referenceID": 18, "context": "The other group aims at querying the most representative samples from a perspective of data reconstruction [17], [18], [19], [20], [21].", "startOffset": 119, "endOffset": 123}, {"referenceID": 19, "context": "The other group aims at querying the most representative samples from a perspective of data reconstruction [17], [18], [19], [20], [21].", "startOffset": 125, "endOffset": 129}, {"referenceID": 20, "context": "The other group aims at querying the most representative samples from a perspective of data reconstruction [17], [18], [19], [20], [21].", "startOffset": 131, "endOffset": 135}, {"referenceID": 21, "context": "One may state that, if we apply some state-of-the-art feature selection techniques, such as SPEC [22], Q \u2212 \u03b1 [23], to learn a low-dimensional representation before active learning, these problems might be solved.", "startOffset": 97, "endOffset": 101}, {"referenceID": 22, "context": "One may state that, if we apply some state-of-the-art feature selection techniques, such as SPEC [22], Q \u2212 \u03b1 [23], to learn a low-dimensional representation before active learning, these problems might be solved.", "startOffset": 109, "endOffset": 113}, {"referenceID": 23, "context": "Recently, Joshi and Xu [24] presented an active learning method with integrated feature selection based on linear kernel SVMs and GainRatio.", "startOffset": 23, "endOffset": 27}, {"referenceID": 24, "context": "[25] intended to use human feedback on both features and samples for active learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] proposed a dual feature selection and sample selection method in the context of graph classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Bilgic [27] proposed a dynamic dimensionality reduction algorithm that determined the appropriate number of dimensions for each active learning iteration.", "startOffset": 7, "endOffset": 11}, {"referenceID": 17, "context": "This is a more challenging problem than traditional representativeness based active learning problems, because selecting m samples to best approximate X often leads to an NP-hard problem [18], and finding r features as the most representative feature subset is also often NP-hard [28].", "startOffset": 187, "endOffset": 191}, {"referenceID": 27, "context": "This is a more challenging problem than traditional representativeness based active learning problems, because selecting m samples to best approximate X often leads to an NP-hard problem [18], and finding r features as the most representative feature subset is also often NP-hard [28].", "startOffset": 280, "endOffset": 284}, {"referenceID": 28, "context": "Inspired by the CUR matrix decomposition [29], [30], [31], [32], we propose a unified framework to find the most representative samples and features.", "startOffset": 41, "endOffset": 45}, {"referenceID": 29, "context": "Inspired by the CUR matrix decomposition [29], [30], [31], [32], we propose a unified framework to find the most representative samples and features.", "startOffset": 47, "endOffset": 51}, {"referenceID": 30, "context": "Inspired by the CUR matrix decomposition [29], [30], [31], [32], we propose a unified framework to find the most representative samples and features.", "startOffset": 53, "endOffset": 57}, {"referenceID": 31, "context": "Inspired by the CUR matrix decomposition [29], [30], [31], [32], we propose a unified framework to find the most representative samples and features.", "startOffset": 59, "endOffset": 63}, {"referenceID": 28, "context": "Moreover, unlike most existing CUR solvers being randomized or heuristic algorithms [29], [30], we utilize the structured sparsity-inducing norms to relax the objective from a non-convex optimization problem to a convex one, which allows for devising an efficient variant of the alternating direction method of multipliers (ADMM) [33], [34].", "startOffset": 84, "endOffset": 88}, {"referenceID": 29, "context": "Moreover, unlike most existing CUR solvers being randomized or heuristic algorithms [29], [30], we utilize the structured sparsity-inducing norms to relax the objective from a non-convex optimization problem to a convex one, which allows for devising an efficient variant of the alternating direction method of multipliers (ADMM) [33], [34].", "startOffset": 90, "endOffset": 94}, {"referenceID": 32, "context": "Moreover, unlike most existing CUR solvers being randomized or heuristic algorithms [29], [30], we utilize the structured sparsity-inducing norms to relax the objective from a non-convex optimization problem to a convex one, which allows for devising an efficient variant of the alternating direction method of multipliers (ADMM) [33], [34].", "startOffset": 330, "endOffset": 334}, {"referenceID": 33, "context": "Moreover, unlike most existing CUR solvers being randomized or heuristic algorithms [29], [30], we utilize the structured sparsity-inducing norms to relax the objective from a non-convex optimization problem to a convex one, which allows for devising an efficient variant of the alternating direction method of multipliers (ADMM) [33], [34].", "startOffset": 336, "endOffset": 340}, {"referenceID": 16, "context": "Fortunately, there exists theoretical progress that \u2016W\u20162,1 is the minimum convex hull of \u2016W\u20162,0 [17].", "startOffset": 96, "endOffset": 100}, {"referenceID": 19, "context": "However, it is more reasonable to suppose that a data point can be mainly recovered from its neighbors [20], [21].", "startOffset": 103, "endOffset": 107}, {"referenceID": 20, "context": "However, it is more reasonable to suppose that a data point can be mainly recovered from its neighbors [20], [21].", "startOffset": 109, "endOffset": 113}, {"referenceID": 32, "context": "In this section, we employ the alternating direction method of multipliers (ADMM) [33] to solve (6).", "startOffset": 82, "endOffset": 86}, {"referenceID": 34, "context": "The problem (16) can be solved by the following lemma [35]:", "startOffset": 54, "endOffset": 58}, {"referenceID": 35, "context": "The problem (22) can be solved by the following matrix shrinkage operation Lemma [36]: Lemma 2.", "startOffset": 81, "endOffset": 85}, {"referenceID": 36, "context": "We can also extend our method to the kernel version by defining a new data representation to incorporate the kernel information as in [37].", "startOffset": 134, "endOffset": 138}, {"referenceID": 37, "context": "Based on the classical convergence results, we can obtain the global convergence of Algorithm 1 to the primaldual optimal solution of problem (8) (see [38], [39]).", "startOffset": 151, "endOffset": 155}, {"referenceID": 38, "context": "Based on the classical convergence results, we can obtain the global convergence of Algorithm 1 to the primaldual optimal solution of problem (8) (see [38], [39]).", "startOffset": 157, "endOffset": 161}, {"referenceID": 39, "context": "3) (Ergodic Iteration Complexity [40]) Let (W,\u0174,W\u0303,Z,\u039b1,\u039b \u2217 2,\u039b \u2217 3) be an optimal solution pair, we have", "startOffset": 33, "endOffset": 37}, {"referenceID": 40, "context": "4) (Non-ergodic Iteration Complexity [41]) The nonergodic iteration complexity can be written as \u2225\u2225\u2225\u03a3k \u2212\u03a3k+1\u2225\u2225\u22252 H \u2264 C2 k + 1 , (26)", "startOffset": 37, "endOffset": 41}, {"referenceID": 38, "context": "For the detailed proof, please refer to [39], [40].", "startOffset": 40, "endOffset": 44}, {"referenceID": 39, "context": "For the detailed proof, please refer to [39], [40].", "startOffset": 46, "endOffset": 50}, {"referenceID": 17, "context": "Among them, the most popular one is the Transductive Experimental Design (TED) [18].", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "Since this optimization problem is NP-hard, [18] proposed a suboptimal sequential optimization algorithm and a non-greedy optimization algorithm to solve it, respectively.", "startOffset": 44, "endOffset": 48}, {"referenceID": 19, "context": "Cai and He [20] extended TED to choose samples by utilizing a nearest neighbor graph to capture intrinsic local manifold structure, where the graph Laplacian is incorporated into a manifold adaptive kernel space.", "startOffset": 11, "endOffset": 15}, {"referenceID": 41, "context": "[42] adopted the idea from Locally Linear Embedding (LLE) [43] to find the reconstruction coefficients.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[42] adopted the idea from Locally Linear Embedding (LLE) [43] to find the reconstruction coefficients.", "startOffset": 58, "endOffset": 62}, {"referenceID": 41, "context": "Similar to [42], Hu et al.", "startOffset": 11, "endOffset": 15}, {"referenceID": 20, "context": "[21] incorporated the local geometrical information into the active learning process.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] proposed a novel method to relax the objective of TED to an efficient convex formulation, and utilized the robust sparse representation loss function to reduce the effect of outliers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "Dataset We evaluate the performance of our ALFS on six datasets, including 1) an artificial dataset, Madelon [44],", "startOffset": 109, "endOffset": 113}, {"referenceID": 44, "context": "used in the NIPS 2003 feature selection challenge; 2) two preprocessed microarray datasets, TOX-171 [45] and Musk [44], for studying toxicity prediction and musk activity prediction respectively; 3) two image datasets, ORL and FGNET, which are widely used for face recognition [46] and facial age estimation [47], [48], respectively.", "startOffset": 100, "endOffset": 104}, {"referenceID": 43, "context": "used in the NIPS 2003 feature selection challenge; 2) two preprocessed microarray datasets, TOX-171 [45] and Musk [44], for studying toxicity prediction and musk activity prediction respectively; 3) two image datasets, ORL and FGNET, which are widely used for face recognition [46] and facial age estimation [47], [48], respectively.", "startOffset": 114, "endOffset": 118}, {"referenceID": 45, "context": "used in the NIPS 2003 feature selection challenge; 2) two preprocessed microarray datasets, TOX-171 [45] and Musk [44], for studying toxicity prediction and musk activity prediction respectively; 3) two image datasets, ORL and FGNET, which are widely used for face recognition [46] and facial age estimation [47], [48], respectively.", "startOffset": 277, "endOffset": 281}, {"referenceID": 46, "context": "used in the NIPS 2003 feature selection challenge; 2) two preprocessed microarray datasets, TOX-171 [45] and Musk [44], for studying toxicity prediction and musk activity prediction respectively; 3) two image datasets, ORL and FGNET, which are widely used for face recognition [46] and facial age estimation [47], [48], respectively.", "startOffset": 308, "endOffset": 312}, {"referenceID": 47, "context": "used in the NIPS 2003 feature selection challenge; 2) two preprocessed microarray datasets, TOX-171 [45] and Musk [44], for studying toxicity prediction and musk activity prediction respectively; 3) two image datasets, ORL and FGNET, which are widely used for face recognition [46] and facial age estimation [47], [48], respectively.", "startOffset": 314, "endOffset": 318}, {"referenceID": 48, "context": "For FG-NET, we conduct age range estimation as in [49], and divide the dataset into five age groups in the experiment; 4) a video dataset, UCF11 [50], which is collected from YouTube.", "startOffset": 145, "endOffset": 149}, {"referenceID": 17, "context": "\u2022 Transductive Experimental Design (TED) [18]3: an active learning method developing experimental design in a transductive setting.", "startOffset": 41, "endOffset": 45}, {"referenceID": 16, "context": "\u2022 RRSS [17]4: an active learning method taking advantage of robust representation and structure sparsity.", "startOffset": 7, "endOffset": 11}, {"referenceID": 20, "context": "\u2022 Active Learning via Neighbor Reconstruction (ALNR) [21]: an active learning method using neighborhood reconstruction.", "startOffset": 53, "endOffset": 57}, {"referenceID": 29, "context": "\u2022 R-CUR [30]: a randomized algorithmic approach for solving CUR matrix factorization.", "startOffset": 8, "endOffset": 12}, {"referenceID": 49, "context": "We use three kinds of unsupervised feature selection methods, Laplacian5 [51], SPEC6 [22], and SOGFS7 [46], to com-", "startOffset": 73, "endOffset": 77}, {"referenceID": 21, "context": "We use three kinds of unsupervised feature selection methods, Laplacian5 [51], SPEC6 [22], and SOGFS7 [46], to com-", "startOffset": 85, "endOffset": 89}, {"referenceID": 45, "context": "We use three kinds of unsupervised feature selection methods, Laplacian5 [51], SPEC6 [22], and SOGFS7 [46], to com-", "startOffset": 102, "endOffset": 106}, {"referenceID": 16, "context": "Experiments protocol Following [17], for each dataset, we first randomly select 50% of the data points as candidate samples for training, from which we apply the compared active learning methods to select a subset of samples to request human labeling.", "startOffset": 31, "endOffset": 35}, {"referenceID": 29, "context": "The reason is that R-CUR [30] is a general CUR model and adopts a randomized algorithmic approach to seek the matrices C and R for satisfying (1).", "startOffset": 25, "endOffset": 29}, {"referenceID": 29, "context": "When the user inputs the desired number of samples m and the number of features r, the final outputs m\u2032 and r\u2032 of R-CUR [30] may be slightly different m and r, respectively.", "startOffset": 120, "endOffset": 124}, {"referenceID": 0, "context": "The noisy variable is sampled from the standard normal distribution, and the noisy label is drawn from the discrete uniform distribution on [1, 2, 3, 4].", "startOffset": 140, "endOffset": 152}, {"referenceID": 1, "context": "The noisy variable is sampled from the standard normal distribution, and the noisy label is drawn from the discrete uniform distribution on [1, 2, 3, 4].", "startOffset": 140, "endOffset": 152}, {"referenceID": 2, "context": "The noisy variable is sampled from the standard normal distribution, and the noisy label is drawn from the discrete uniform distribution on [1, 2, 3, 4].", "startOffset": 140, "endOffset": 152}, {"referenceID": 3, "context": "The noisy variable is sampled from the standard normal distribution, and the noisy label is drawn from the discrete uniform distribution on [1, 2, 3, 4].", "startOffset": 140, "endOffset": 152}, {"referenceID": 50, "context": ", taking the objective function of [52] as a regularization term.", "startOffset": 35, "endOffset": 39}], "year": 2017, "abstractText": "This paper focuses on the problem of simultaneous sample and feature selection for machine learning in a fully unsupervised setting. Though most existing works tackle these two problems separately that derives two well-studied sub-areas namely active learning and feature selection, a unified approach is inspirational since they are often interleaved with each other. Noisy and high-dimensional features will bring adverse effect on sample selection, while \u2018good\u2019 samples will be beneficial to feature selection. We present a framework to jointly conduct active learning and feature selection based on the CUR matrix decomposition. From the data reconstruction perspective, both the selected samples and features can best approximate the original dataset respectively, such that the selected samples characterized by the selected features are very representative. Additionally our method is one-shot without iteratively selecting samples for progressive labeling. Thus our model is especially suitable when the initial labeled samples are scarce or totally absent, which existing works hardly address particularly for simultaneous feature selection. To alleviate the NP-hardness of the raw problem, the proposed formulation involves a convex but non-smooth optimization problem. We solve it efficiently by an iterative algorithm, and prove its global convergence. Experiments on publicly available datasets validate that our method is promising compared with the state-of-the-arts.", "creator": "LaTeX with hyperref package"}}}