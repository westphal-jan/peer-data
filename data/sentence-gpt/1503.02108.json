{"id": "1503.02108", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2015", "title": "Maximum a Posteriori Adaptation of Network Parameters in Deep Models", "abstract": "We present a Bayesian approach to adapting parameters of a well-trained context-dependent deep-neural-network hid-den Markov models (CD-DNN-HMMs) to improve automatic speech recognition performance. Due to an abundance of DNN parameters but with only a limited amount of adaptation data, the posterior probabilities of unseen CD states (senones) are often pushed towards zero during adaptation, and consequently the ability to model these senones can be degraded. We formulate maximum a posteriori (MAP) adaptation of parameters of a specially designed CD-DNN-HMM with an augmented linear hidden networks connected to the output senones and compare it to the feature space maximum a posteriori linear regression previously proposed (21). A network with sparse connectivity within the data structure may provide a better model for the role of a given set of parameters (23).", "histories": [["v1", "Fri, 6 Mar 2015 22:48:29 GMT  (209kb,D)", "https://arxiv.org/abs/1503.02108v1", null], ["v2", "Wed, 12 Aug 2015 04:53:53 GMT  (162kb,D)", "http://arxiv.org/abs/1503.02108v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["zhen huang", "sabato marco siniscalchi", "i-fan chen", "jiadong wu", "chin-hui lee"], "accepted": false, "id": "1503.02108"}, "pdf": {"name": "1503.02108.pdf", "metadata": {"source": "CRF", "title": "Maximum a Posteriori Adaptation of Network Parameters in Deep Models", "authors": ["Zhen Huang", "Sabato Marco Siniscalchi", "I-Fan Chen", "Jinyu Li", "Jiadong Wu", "Chin-Hui Lee"], "emails": ["huangzhenee@gatech.edu,", "marco.siniscalchi@unikore.it,", "ichen8@gatech.edu,", "jinyli@exchange.microsoft.com,", "jwu65@gatech.edu,", "chl@ece.gatech.edu"], "sections": [{"heading": "1. Introduction", "text": "Despite the recent outstanding results demonstrated by contextdependent, deep-neural-network based hidden Markov models (CD-DNN-HMMs) in various automatic speech recognition (ASR) tasks and data sets [1, 2, 3], these acoustic models, similarly to conventional context-dependent, Gaussian-mixturemodel based HMMs (CD-GMM-HMMs) [4], still suffer from a performance degradation under potential mismatched conditions between training and testing conditions. For standard hybrid system using artificial neural networks (ANNs) and HMMs [5] in which CD-DNN-HMM is a special case, there exist many adaptation techniques. The simplest approach modifies all weights of the connectionist architecture using some adaptation materials. Unfortunately, it leads to over-fitting on the adaptation material when the amount of adaptation patterns is limited [6]. Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.\nTransformation based methods are the most popular connectionist adaptation techniques. The key idea is to augment the structure of the ANN component by adding an affine trans-\nformation network to the input [6], hidden [11], or output layer [22]. They are typically trained while keeping the rest of the network parameters fixed. Motivations for these approaches stem from the concept that only relatively few parameters could be learned during adaptation and therefore it is preferable to training the entire network when the adaptation set is limited. For linear hidden network (LHN) layer approaches, the last hidden layer is usually designed to be a bottleneck to ensure an affordable parameter size [23, 24, 25, 26].\nHowever, adapting parameters in a CD-DNN-HMM is much more challenging than earlier connectionist adaptation schemes because of its huge parameter set size with a large number of network branches connected to a large set of tied HMM states, often referred to as senones [27]. Furthermore, DNN parameters are adapted by every sample frame regardless of its senone class. Therefore, the posterior probabilities for the unobserved and scarcely seen senones are often pushed towards zero during adaptation. Such a phenomenon is commonly referred to as catastrophic forgetting [28]. Conservative ad-hoc solutions for ANNs have been proposed to force the senone distribution estimated from the adapted model to be close to that of the unadapted model. For example, a Kullback-Leibler divergence (KLD) based objective criterion to be used during adaptation was devised in [8] in order to alleviate the catastrophic forgetting problem. A variation to the standard method of assigning the target values was instead discussed in [11]. Nonetheless, Bayesian solutions adopted in the CD-GMM-HMMs to address the same issue [29] have not been fully exploited.\nIn this study, we attempt to cast DNN adaptation within a Bayesian framework in the spirit of maximum a posteriori (MAP) adaptation [30]. The key goal is to re-estimate some DNN parameters by representing available information in an augmented linear hidden network (LHN) added after the last non-linear hidden layer. Experimental results on the 20,000- word open vocabulary Wall Street Journal task demonstrates the feasibility of the proposed approach. Under supervised adaptation, the proposed MAP adaptation scheme can provide a relative word error rate (WER) reduction of more than 10% from an already-strong speaker independent CD-DNN-HMM baseline and consistently outperform conventional transformation based adaptation schemes. It also compares favorably against the feature space maximum a posteriori linear regression approach to speaker adaptation proposed in [31]. We also present an initial attempt to generate hierarchical priors for improving adaptation efficiency with small amounts of adaptation data by exploiting the similarities among senones. ar X\niv :1\n50 3.\n02 10\n8v 2\n[ cs\n.L G\n] 1\n2 A\nug 2\n01 5"}, {"heading": "2. Training of Deep Models", "text": "In DNNs, hidden layers are usually constructed by sigmoid units, and the output layer is a softmax layer. The values of the nodes can therefore be expressed as:\nxi =\n{ W1o t + b1, i = 1\nWiyi\u22121 + bi, i > 1 , (1)\nyi =\n{ sigmoid(xi), i < L\nsoftmax(xi), i = L , (2)\nwhere W1, and Wi are the weight matrices, b1, and bi are the bias vectors, ot is the input frame at time t, L is the total number of the hidden layers, and both sigmoid and softmax functions are element-wise operations. The vector xi corresponds to pre-nonlinearity activations, and yi and yL are the vectors of neuron outputs at the ith hidden layer and the output layer, respectively. The softmax outputs were considered as an estimate of the senone posterior probability:\np(Cj |ot) = ytL(j) = exp(xtL(j))\u2211 i exp(xtL(i)) , (3)\nwhere Cj represents the jth senone and yL(j) is the jth element of yL.\nThe DNN is trained by maximizing the log posterior probability over the training frames. This is equivalent to minimizing the cross-entropy objective function. Let X be the whole training set, which contains T frames, i.e. o1:T \u2208 X , then the loss with respect to X is given by\nL1:T = \u2212 T\u2211 t=1 J\u2211 j=1 p\u0303t(j) log p(Cj |ot), (4)\nwhere p(Cj |ot) is defined in Eq. (3); p\u0303t is the target probability of frame t. In real practices of DNN systems, the target probability p\u0303t is often obtained by a forced alignment with an existing system resulting in only the target entry that is equal to 1. Mini-batch stochastic gradient descent (SGD) [32], with a reasonable size of mini-batches to make all matrices fit into the GPU memory, was used to update all neural parameters during training. Pre-training methods was used for the initialisation of the DNN parameters [33]."}, {"heading": "3. Transformation Based Adaptation for Deep Models", "text": "For DNN adaptation, some researchers choose to add an affine transformation network between the last hidden layer and the output layer weights matrix, i.e., an LHN, and adapt only the LHN parameters while keeping fixed all of the other DNN parameters [11]. In order to reduce the amount of parameters to adapt, usually the last hidden layer is designed to be a bottleneck (less neurons) [23, 24, 25, 26]. Superior results were obtained by this kind of LHN formulation than other transformation based adaptation schemes, such as linear input network (LIN) and linear output network (LON).\nThe LIN approach performs adaptation by adding an augmented linear input layer and only adapts this set of LIN parameters. If we follow the common idea that the hidden layers of a DNN is actually learning a more suitable data representation and extracting better \u201cfeature\u201d for the output layer that is serving as a log-linear model, then by transforming the raw input\nusing the LIN layer, we might harm the ability of data representation of the hidden layers. On the other hand for the LON approach, the issue is that usually we can\u2019t reduce the number of neurons of the output layer because we want to directly model the senones (the number of senones can be more than 10000 in practice), and that means we have to add a huge augmented layer with even more parameters to be adapted.\nIf we deem the hidden layers as a feature extractor and the output layer as the discriminative model. The model parameters are the weights of the output layer\u2019s affine transform matrix, WL. The output yL can now be expressed as:\nyL = softmax(WLyL\u22121), (5)\nwhere the activation at the last hidden layer, yL\u22121, can be used as the new feature representation extracted by the hidden layers. When adding an augmented LHN after the last hidden layer, it is equivalent to applying a transformation matrix Wlhn to the model parameters to obtain an adapted model parameter set:\nyL = softmax(WlhnWLyL\u22121), (6)\nAn LHN adaptation structure is shown in Figure 1. This formulation is quite similar to maximum likelihood linear regression (MLLR) [34]. The difference is that in MLLR the model parameters are Gaussian mean and variance while here the model parameters are the log-linear model\u2019s transformation matrix weights."}, {"heading": "4. MAP Adaptation for Deep Models", "text": "Although conventional DNN adaptation approaches try to alleviate over-fitting issues by reducing the number of parameters to be adapted, such number could still be very big in some cases. Inspired by the MAP adaptation that address the problem effectively in GMM-HMM systems, in this section, we explain how to apply the MAP approach to the LHN adaptation. Note that though we choose LHN for demonstration, the proposed MAP approach can be easily applied to other DNN adaptation frameworks like [8, 14, 16, 17] as well."}, {"heading": "4.1. Prior Estimation", "text": "In order to establish a MAP adaptation framework like in [30], a prior distribution over the weights of the affine transformation network need to be imposed. To analyze and estimate the prior\ndensity, we utilized the training data of the baseline DNN. We adopted an empirical Bayes approach [30, 29] and treated each speaker in the training set as a sample speaker and supervised LHN adaptation was performed. After that, we can get a particular LHN for each speaker. We observed that the histograms for weights of the adapted LHN over speakers are quite like Gaussian, so we assume that the distribution of the weights in Wlhn to be joint Gaussian [31]. By expressing the weights in the LHN transformation matrix Wlhn as a vector w with each entry representing a particular weight, we have the prior density in the following form:\np(Wlhn) = 1 (2\u03c0)M/2|\u03a3|1/2 exp(\u22121 2 (w\u2212\u00b5)T\u03a3\u22121(w\u2212\u00b5)) (7) where only the diagonal entries of the covariance matrix \u03a3 are non-zero (from the independence assumption of the weights).\nWithN adapted speaker weight vectors, the maximum likelihood estimation of the mean \u00b5 and variance \u03a3 can be expressed as:\n\u00b5ML = 1\nN N\u2211 i=1 wi (8)\n\u03a3ML = 1\nN N\u2211 i=1 (wi \u2212 \u00b5ML)(wi \u2212 \u00b5ML)T (9)\nwhere wi is the vector consisting of the adapted transformation weights of speaker i."}, {"heading": "4.2. MAP Formulation", "text": "Formal MAP adaptation is conducted following [31]. Eq. (10) formulates the MAP learning idea by adding the term of prior density p(Wlhn) to the plain cross entropy objective function.\nL1:TMAP = \u2212\u03bb log p(Wlhn) + L1:Txent (10) Applying the prior form of Eq. (7), the objective function for MAP LHN adaptation is in the form of Eq. (11).\nL1:TMAP = \u03bb 2 (w \u2212 \u00b5)T\u03a3\u22121(w \u2212 \u00b5) + L1:Txent (11)\nwhere only the diagonal entries of the covariance matrix \u03a3 are non-zero (from the independence assumption of the weights).\nA close look at Eq. (11), when the prior density is a standard Gaussian N (0, I), MAP learning will degenerate to conventional L2-regularized training. The gradient of LMAP1:N with respect to w can now be expressed as:\n\u2202L1:TMAP \u2202w = \u03bb(w \u2212 \u00b5)T diag(\u03a3\u22121) + \u2202L 1:T xent \u2202w , (12)\nwhere diag(\u03a3\u22121) consists of the diagonal entries of \u03a3\u22121."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Experimental Setup", "text": "This study is concerned with the problem of speaker adaptation, and experiments are reported on the 20k-word open vocabulary Wall Street Journal task [35] using the Kaldi toolkit [36]. The baseline CD-DNN-HMM system was trained using the WSJ0 material (SI-84). The standard adaptation set of WSJ0 (si et ad,\n8 speakers, 40 sentences per speaker) was used to perform adaptation of the affine transformation added to the speakerindependent DNN. The standard open vocabulary 20,000-word (20K) read NVP Senneheiser microphone (si et 20, 8 speakers x 40 sentences) data were used for evaluation. A standard trigram language model was adopted during decoding. The ASR performance was given in terms of the word error rate (WER).\nThe DNN has six hidden layers. The first five hidden layers have 2048 units, whereas the last hidden layer has 216 units. The output layer has 2022 softmax units. This DNN architecture follows conventional configurations used in the speech community except for the last hidden layer, which acts as a bottleneck layer. This configuration was chosen, because a too large dimension of the last non-linear hidden layer might have been harmful for LHN adaptation. The bottleneck based low rank methods have been widely used to achieve more compact DNN models with equivalent performance [23, 24, 25, 26]. The number units equal to 216 was chosen to simulate a sort of three-state phone layer thereby obtaining a kind of hierarchical structure between mono-phones in the hidden layer and senones at the output layer. The input feature vector is a 23- dimension mean-normalized log-filter bank feature with up to second-order derivatives and a context window of 11 frames, forming a vector of 759-dimension (69 \u00d7 11) input. The DNN was trained with an initial learning rate of 0.008 using the crossentropy objective function. It was initialised with the stacked restricted Boltzmann machines by using layer by layer generative pre-training."}, {"heading": "5.2. Experimental Results", "text": "The word error rate (WER) attained with different adaptation techniques are reported in Table 1. All available adaptation material was used for performing adaptation, namely 40 sentences per speaker. The term BASELINE refers to the speaker independent CD-DNN-HMM system. LIN, LIN-KLD, and MAP LIN refer to the adaptation technique based on the standard linear input network approach, the KLD regularisation technique1 in combination with LIN, and the maximum a posteriori transformation based adaptation when a prior is defined over the LIN parameters [31], respectively. The terms LON and LON-KLD are used to denote, with a little abuse of terminology, the direct adaptation of the output layer weights matrix with or without KLD, respectively. LHN adaptation results are also reported along with the corresponding MAP version, MAP LHN, which is the adaptation approach proposed in this paper. Since LHN was inserted between the last hidden layer and the output layer weights matrix, its dimension is 216 \u00d7 216. LHN is initialised to an identity matrix with zero bias, which gives a starting point equivalent to the unadapted model. Supervised adaptation is then performed updating only the LHN parameters. MAP was performed as described in Section 4. For the sake of comparison, LHN-KLD, which denotes standard LHN combined with KLD, was also evaluated.\nIndeed LIN and LHN outperforms LON, which attains the worst performance improvement. KLD always improves over affine transformation based adaptation techniques, as expected. Furthermore, the proposed MAP LHN outperforms all other techniques in the given task, and it attains the best recognition results with a relative improvement of 10.4% over the BASELINE. Finally, we would like to remark that MAP LHN compares favourably against MAP LIN, and that confirms that the\n1The best KLD results obtained in our laboratories are reported.\nintroduction of the bottleneck layer was the key for a proper deployment of MAP LHN.\nTable 2 shows experimental results for LHN, and MAP LHN with different amounts of adaptation sentences, namely {5, 10, 20, 40}, in the second and third columns, respectively. These results confirm that MAP LHN adaptation almost always improves over standard LHN, with the best adaptation results at a WER of 7.92% using 40 utterances. But in very limited adaptation data cases, namely, {5, 10}, there is only slight or even no improvement by only using flat prior in the MAP adaptation, so we turned to our preliminary investigation of hierarchical priors for dealing with the data scarcity problem."}, {"heading": "5.3. Hierarchical Priors: Preliminary Experiments", "text": "Hierarchical structures, such as trees, have long been used in the speech community to address the over-fitting issues during model parameters estimation. For instance, efficient adaptation with a limited amount of adaptation data was obtained through the use of a tree data structure to cluster model parameters of a CD-GMM-HMM system in [37]. Similar ideas have been recently explored in DNN learning for enhancing classification performance for classes with few examples in [38], where hierarchical priors where devised for the output layer weights matrix (top-level weights in a DNN) using a tree data structure either fixed or learnable during training.\nTop-level DNN weights in a hybrid acoustic model can be regarded as senone embeddings [39], and hierarchical priors can be defined by organising those embedding in a tree data structure. Let W(D+1)\u00d7L denote the output layer weights matrix (including the bias terms). Each line in W(D+1)\u00d7L corresponds to a senone embedding. Specifically, the sth senone embedding can be denoted as ws, which is the sth row in W(D+1)\u00d7L. The tree structure used to generate hierarchical priors can be either learnt during training or given. Here, we used a fixed two-layer tree shown in Figure 2: there are L leaf nodes, with each leaf corresponding to a senone embedding, and S parent nodes clustering together similar leaf nodes. Each parent node clusters senone embeddings sharing the same cen-\ntral phone-state; therefore, S is equal to 130 in this work. Hierarchical priors can now be established by associating a vector ws to a leaf node, and a vector \u03b8s to each parent node, and imposing a Gaussian probability density distribution over these two vectors as follows: \u03b8s \u223c N (0, 1\u03c321 I(D+1)), and ws \u223c N (\u03b8s, 1\u03c322 I(D+1)).\nThe objective function with hierarchical priors is in the form of Eq. (13).\nLMAP1:N = Lxent1:N + \u03bb2 2\n\u2211 \u2016ws \u2212 \u03b8s\u20162 +\n\u03bb1 2 \u2016\u03b8\u20162. (13)\nIt is can be verified that \u03b8s is a scaled average of all ws associated to the sth leaf node by minimizing Eq. 13 over \u03b8s with fixed DNN weights (see [38]). We focus our attention on experimental results with very small adaptation data amounts, as shown in Table 3. With limited adaptation data, namely 5, 10 utterances, small performance improvements are observed against using flat priors when adaptation is carried out with hierarchical priors. Although the current improvement is still quite small, we believe more sophisticated trees can be adopted for better performance in future studies."}, {"heading": "6. Conclusion", "text": "We have investigated a maximum a posteriori (MAP) adaptation approach for linear hidden networks. The key idea is to treat the parameters of the augmented affine transformation as random Gaussian variables and incorporate prior information obtained from the training data. Speaker adaptation results show that the proposed MAP approaches can lead to a consistent performance improvement over conventional LHN adaptation. Furthermore, MAP LHN outperforms other regularisation schemes.\nA first attempt to use hierarchical-based priors with a fixed two-layer tree structure was also studied, and small improvements were observed in a set of preliminary ASR experiments using a limited amount of adaptation sentences. Better results might still be hindered by the current fixed tree hierarchy structure employed in this preliminary work. Indeed, it was demonstrated that learning the tree hierarchy during training improves the classification performance [38]. Finally, from the objective function perspective, we are still relying on cross-entropy. Other forms of frame-level and sequence-level discriminative objectives [3, 40] can also be applied."}, {"heading": "7. References", "text": "[1] T. N. Sainath, B. Kingsbury, B. Ramabhadran, P. Fousek, P. No-\nvak, and A. Mohamed, \u201cMaking deep belief networks effective for large vocabulary continuous speech recognition,\u201d in Proc. ASRU, 2011, pp. 30\u201335.\n[2] G. E. Dahl, D. Yu, L. Deng, and A. Acero, \u201cContext-dependent pre-trained deep neural networks for large-vocabulary speech recognition,\u201d IEEE Trans. Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 30\u201342, 2012.\n[3] K. Vesely\u0300, A. Ghoshal, L. Burget, and D. Povey, \u201cSequencediscriminative training of deep neural networks,\u201d in Proc. INTERSPEECH, 2013, pp. 2345\u20132349.\n[4] L. Rabiner, \u201cA tutorial on hidden Markov models and selected applications in speech recognition,\u201d Proceedings of the IEEE, vol. 77, no. 2, pp. 257\u2013286, 1989.\n[5] H. Bourlard and N. Morgan, Connectionist speech recognition: A hybrid approach. Kluwer Academic Publishers, 1994.\n[6] J. Neto, L. Almeida, M. Hochberg, C. Martins, L. Nunes, S. Renals, and T. Robinson, \u201cSpeaker-adaptation for hybrid HMM-ANN continuous speech recognition system,\u201d in Proc. Eurospeech, 1995.\n[7] X. Li and J. Bilmes, \u201cRegularized adaptation of discriminative classifiers,\u201d in Proc. ICASSP, vol. 1, 2006, pp. I\u2013I.\n[8] D. Yu, K. Yao, H. Su, G. Li, and F. Seide, \u201cKL-divergence regularized deep neural network adaptation for improved large vocabulary speech recognition,\u201d in Proc. ICASSP, 2013, pp. 7893\u20137897.\n[9] D. Yu, X. Chen, and L. Deng, \u201cFactorized deep neural networks for adaptive speech recognition,\u201d in Proc. Int. Workshop on Statistical Machine Learning for Speech Processing, 2012.\n[10] D. Yu, L. Deng, and S. Seide, \u201cThe deep tensor neural network with applications to large vocabulary speech recognition,\u201d IEEE Trans. Audio, Speech, and Language Processing, vol. 21, no. 2, pp. 388\u2013396, 2013.\n[11] R. Gemello, F. Mana, S. Scanzio, P. Laface, and R. D. Mori, \u201cLinear hidden transformations for adaptation of hybrid ANN/HMM models,\u201d Speech Communication, vol. 49, no. 10, pp. 827\u2013835, 2007.\n[12] F. Seide, G. Li, X. Chen, and D. Yu, \u201cFeature engineering in context-dependent deep neural networks for conversational speech transcription,\u201d in Proc. ASRU, 2011, pp. 24\u201329.\n[13] K. Yao, D. Yu, F. Seide, H. Su, L. Deng, and Y. Gong, \u201cAdaptation of context-dependent deep neural networks for automatic speech recognition,\u201d in Proc. Spoken Language Technology Workshop, 2012, pp. 366\u2013369.\n[14] G. Saon, H. Soltau, D. Nahamoo, and M. Picheny, \u201cSpeaker adaptation of neural network acoustic models using i-vectors,\u201d in Proc. ASRU, 2013, pp. 55\u201359.\n[15] S. M. Siniscalchi, J. Li, and C.-H. Lee, \u201cHermitian polynomial for speaker adaptation of connectionist speech recognition systems,\u201d IEEE Trans. Audio, Speech, and Language Processing, vol. 21, no. 10, pp. 2152\u20132161, 2013.\n[16] O. Abdel-Hamid and H. Jiang, \u201cRapid and effective speaker adaptation of convolutional neural network based models for speech recognition,\u201d in Proc. INTERSPEECH, 2013, pp. 1248\u20131252.\n[17] P. Swietojanski and S. Renals, \u201cLearning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models,\u201d in Proc. IEEE STL, 2014.\n[18] J. Li, J.-T. Huang, and Y. Gong, \u201cFactorized adaptation for deep neural network,\u201d in Proc. ICASSP, 2014.\n[19] O. Abdel-Hamid and H. Jiang, \u201cFast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code,\u201d in Proc. ICASSP, 2013, pp. 7942\u20137946.\n[20] S. Xue, O. Abdel-Hamid, H. Jiang, and L. Dai, \u201cDirect adaptation of hybrid DNN/HMM model for fast speaker adaptation in LVCSR based on speaker code,\u201d in Proc. ICASSP, 2014, pp. 6339\u20136343.\n[21] S. Xue, O. Abdel-Hamid, H. Jiang, L. Dai, and Q. Liu, \u201cFast adaptation of deep neural network based on discriminant codes for speech recognition,\u201d IEEE/ACM Trans. on Audio, Speech and Lang. Proc., vol. 22, no. 12, pp. 1713\u20131725, 2014.\n[22] B. Li and K. C. Sim, \u201cComparison of discriminative input and output transformations for speaker adaptation in the hybrid NN/HMM systems,\u201d in Proc. INTERSPEECH, 2010, pp. 526\u2013 529.\n[23] T. N. Sainath, B. Kingsbury, V. Sindhwani, E. Arisoy, and B. Ramabhadran, \u201cLow-rank matrix factorization for deep neural network training with high-dimensional output targets,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 6655\u20136659.\n[24] J. Xue, J. Li, and Y. Gong, \u201cRestructuring of deep neural network acoustic models with singular value decomposition.\u201d in INTERSPEECH, 2013, pp. 2365\u20132369.\n[25] J. Xue, J. Li, D. Yu, M. Seltzer, and Y. Gong, \u201cSingular value decomposition based low-footprint speaker adaptation and personalization for deep neural network,\u201d in Proc. ICASSP, 2014.\n[26] S. Xue, H. Jiang, and L. Dai, \u201cSpeaker adaptation of hybrid NN/HMM model for speech recognition based on singular value decomposition,\u201d in Proc. ISCSLP, 2014.\n[27] M.-Y. M.-Y. Hwang and X. Huang, \u201cShared-distribution hidden markov models for speech recognition,\u201d IEEE trans. Speech and Audio Processing, vol. 1, no. 4, pp. 414\u2013420, 1993.\n[28] M. Franch, \u201cCatastrophic forgetting in connectionist networks: causes, consequences and solutions,\u201d Trends in Cognitive Sciences, vol. 3, no. 4, 1994.\n[29] C.-H. Lee and Q. Huo, \u201cOn adaptive decision rules and decision parameter adaptation for automatic speech recognition,\u201d Proc. IEEE, vol. 88, no. 8, 2000.\n[30] J. Gauvain and C.-H. Lee, \u201cMaximum a posteriori estimation for multivariate gaussian mixture observations of Markov chains,\u201d IEEE Trans. Speech and audio processing, vol. 2, no. 2, pp. 291\u2013 298, 1994.\n[31] Z. Huang, J. Li, S. M. Siniscalchi, I.-F. Chen, C. Weng, and C.-H. Lee, \u201cFeature space maximum a posteriori linear regression for adaptation of deep neural networks,\u201d in Proc. INTERSPEECH, 2014, pp. 2992\u20132996.\n[32] O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao, \u201cOptimal distributed online prediction,\u201d in Proc. ICML, 2011, pp. 713\u2013720.\n[33] G. E. Hinton and R. R. Salakhutdinov, \u201cReducing the dimensionality of data with neural networks,\u201d Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.\n[34] C. J. Leggetter and P. C. Woodland, \u201cMaximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models,\u201d Computer Speech & Language, vol. 9, no. 2, pp. 171\u2013185, 1995.\n[35] D. B. Paul and J. M. Baker, \u201cThe design for the wall street journalbased CSR corpus,\u201d in Proc. Workshop on Speech and Natural Language, 1992, pp. 899\u2013902.\n[36] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz, J. Silovsky\u0300, G. Stemmer, and K. Vesely\u0300, \u201cThe Kaldi speech recognition toolkit,\u201d in Proc. ASRU, 2011.\n[37] K. Shinoda and C.-H. Lee, \u201cA structural Bayes approach to speaker adaptation,\u201d IEEE trans. Speech and Audio Processing, vol. 9, no. 3, pp. 276\u2013287, 2001.\n[38] N. Srivastava and R. Salakhutdinov, \u201cDiscriminative transfer learning with tree-based priors,\u201d in Proc. NIST, 2013.\n[39] X. Li and X. Wu, \u201cDecision tree based state tying for speech recognition using DNN derived embeddings,\u201d in Proc. ISCSLP, 2014, pp. 123\u2013127.\n[40] Z. Huang, J. Li, C. Weng, and C.-H. Lee, \u201cBeyond cross-entropy: Towards better frame-level objective functions for deep neural network training in automatic speech recognition,\u201d in Proc. INTERSPEECH, 2014."}], "references": [{"title": "Making deep belief networks effective for large vocabulary continuous speech recognition", "author": ["T.N. Sainath", "B. Kingsbury", "B. Ramabhadran", "P. Fousek", "P. Novak", "A. Mohamed"], "venue": "Proc. ASRU, 2011, pp. 30\u201335.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Trans. Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 30\u201342, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequencediscriminative training of deep neural networks", "author": ["K. Vesel\u1ef3", "A. Ghoshal", "L. Burget", "D. Povey"], "venue": "Proc. INTER- SPEECH, 2013, pp. 2345\u20132349.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "A tutorial on hidden Markov models and selected applications in speech recognition", "author": ["L. Rabiner"], "venue": "Proceedings of the IEEE, vol. 77, no. 2, pp. 257\u2013286, 1989.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1989}, {"title": "Connectionist speech recognition: A hybrid approach", "author": ["H. Bourlard", "N. Morgan"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1994}, {"title": "Speaker-adaptation for hybrid HMM-ANN continuous speech recognition system", "author": ["J. Neto", "L. Almeida", "M. Hochberg", "C. Martins", "L. Nunes", "S. Renals", "T. Robinson"], "venue": "Proc. Eurospeech, 1995.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "Regularized adaptation of discriminative classifiers", "author": ["X. Li", "J. Bilmes"], "venue": "Proc. ICASSP, vol. 1, 2006, pp. I\u2013I.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "KL-divergence regularized deep neural network adaptation for improved large vocabulary speech recognition", "author": ["D. Yu", "K. Yao", "H. Su", "G. Li", "F. Seide"], "venue": "Proc. ICASSP, 2013, pp. 7893\u20137897.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Factorized deep neural networks for adaptive speech recognition", "author": ["D. Yu", "X. Chen", "L. Deng"], "venue": "Proc. Int. Workshop on Statistical Machine Learning for Speech Processing, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "The deep tensor neural network with applications to large vocabulary speech recognition", "author": ["D. Yu", "L. Deng", "S. Seide"], "venue": "IEEE Trans. Audio, Speech, and Language Processing, vol. 21, no. 2, pp. 388\u2013396, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Linear hidden transformations for adaptation of hybrid ANN/HMM models", "author": ["R. Gemello", "F. Mana", "S. Scanzio", "P. Laface", "R.D. Mori"], "venue": "Speech Communication, vol. 49, no. 10, pp. 827\u2013835, 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Feature engineering in context-dependent deep neural networks for conversational speech transcription", "author": ["F. Seide", "G. Li", "X. Chen", "D. Yu"], "venue": "Proc. ASRU, 2011, pp. 24\u201329.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Adaptation of context-dependent deep neural networks for automatic speech recognition", "author": ["K. Yao", "D. Yu", "F. Seide", "H. Su", "L. Deng", "Y. Gong"], "venue": "Proc. Spoken Language Technology Workshop, 2012, pp. 366\u2013369.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Speaker adaptation of neural network acoustic models using i-vectors", "author": ["G. Saon", "H. Soltau", "D. Nahamoo", "M. Picheny"], "venue": "Proc. ASRU, 2013, pp. 55\u201359.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Hermitian polynomial for speaker adaptation of connectionist speech recognition systems", "author": ["S.M. Siniscalchi", "J. Li", "C.-H. Lee"], "venue": "IEEE Trans. Audio, Speech, and Language Processing, vol. 21, no. 10, pp. 2152\u20132161, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Rapid and effective speaker adaptation of convolutional neural network based models for speech recognition", "author": ["O. Abdel-Hamid", "H. Jiang"], "venue": "Proc. INTERSPEECH, 2013, pp. 1248\u20131252.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models", "author": ["P. Swietojanski", "S. Renals"], "venue": "Proc. IEEE STL, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Factorized adaptation for deep neural network", "author": ["J. Li", "J.-T. Huang", "Y. Gong"], "venue": "Proc. ICASSP, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code", "author": ["O. Abdel-Hamid", "H. Jiang"], "venue": "Proc. ICASSP, 2013, pp. 7942\u20137946.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Direct adaptation of hybrid DNN/HMM model for fast speaker adaptation in LVCSR based on speaker code", "author": ["S. Xue", "O. Abdel-Hamid", "H. Jiang", "L. Dai"], "venue": "Proc. ICASSP, 2014, pp. 6339\u20136343.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast adaptation of deep neural network based on discriminant codes for speech recognition", "author": ["S. Xue", "O. Abdel-Hamid", "H. Jiang", "L. Dai", "Q. Liu"], "venue": "IEEE/ACM Trans. on Audio, Speech and Lang. Proc., vol. 22, no. 12, pp. 1713\u20131725, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Comparison of discriminative input and output transformations for speaker adaptation in the hybrid NN/HMM systems", "author": ["B. Li", "K.C. Sim"], "venue": "Proc. INTERSPEECH, 2010, pp. 526\u2013 529.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["T.N. Sainath", "B. Kingsbury", "V. Sindhwani", "E. Arisoy", "B. Ramabhadran"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 6655\u20136659.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition.", "author": ["J. Xue", "J. Li", "Y. Gong"], "venue": "in INTER- SPEECH,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Singular value decomposition based low-footprint speaker adaptation and personalization for deep neural network", "author": ["J. Xue", "J. Li", "D. Yu", "M. Seltzer", "Y. Gong"], "venue": "Proc. ICASSP, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker adaptation of hybrid NN/HMM model for speech recognition based on singular value decomposition", "author": ["S. Xue", "H. Jiang", "L. Dai"], "venue": "Proc. ISCSLP, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Shared-distribution hidden markov models for speech recognition", "author": ["M.-Y.M.-Y. Hwang", "X. Huang"], "venue": "IEEE trans. Speech and Audio Processing, vol. 1, no. 4, pp. 414\u2013420, 1993.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1993}, {"title": "Catastrophic forgetting in connectionist networks: causes, consequences and solutions", "author": ["M. Franch"], "venue": "Trends in Cognitive Sciences, vol. 3, no. 4, 1994.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1994}, {"title": "On adaptive decision rules and decision parameter adaptation for automatic speech recognition", "author": ["C.-H. Lee", "Q. Huo"], "venue": "Proc. IEEE, vol. 88, no. 8, 2000.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2000}, {"title": "Maximum a posteriori estimation for multivariate gaussian mixture observations of Markov chains", "author": ["J. Gauvain", "C.-H. Lee"], "venue": "IEEE Trans. Speech and audio processing, vol. 2, no. 2, pp. 291\u2013 298, 1994.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1994}, {"title": "Feature space maximum a posteriori linear regression for adaptation of deep neural networks", "author": ["Z. Huang", "J. Li", "S.M. Siniscalchi", "I.-F. Chen", "C. Weng", "C.-H. Lee"], "venue": "Proc. INTERSPEECH, 2014, pp. 2992\u20132996.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimal distributed online prediction", "author": ["O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao"], "venue": "Proc. ICML, 2011, pp. 713\u2013720.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "Maximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models", "author": ["C.J. Leggetter", "P.C. Woodland"], "venue": "Computer Speech & Language, vol. 9, no. 2, pp. 171\u2013185, 1995.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1995}, {"title": "The design for the wall street journalbased CSR corpus", "author": ["D.B. Paul", "J.M. Baker"], "venue": "Proc. Workshop on Speech and Natural Language, 1992, pp. 899\u2013902.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1992}, {"title": "The Kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsk\u1ef3", "G. Stemmer", "K. Vesel\u1ef3"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "A structural Bayes approach to speaker adaptation", "author": ["K. Shinoda", "C.-H. Lee"], "venue": "IEEE trans. Speech and Audio Processing, vol. 9, no. 3, pp. 276\u2013287, 2001.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2001}, {"title": "Discriminative transfer learning with tree-based priors", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "Proc. NIST, 2013.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Decision tree based state tying for speech recognition using DNN derived embeddings", "author": ["X. Li", "X. Wu"], "venue": "Proc. ISCSLP, 2014, pp. 123\u2013127.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Beyond cross-entropy: Towards better frame-level objective functions for deep neural network training in automatic speech recognition", "author": ["Z. Huang", "J. Li", "C. Weng", "C.-H. Lee"], "venue": "Proc. IN- TERSPEECH, 2014.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Despite the recent outstanding results demonstrated by contextdependent, deep-neural-network based hidden Markov models (CD-DNN-HMMs) in various automatic speech recognition (ASR) tasks and data sets [1, 2, 3], these acoustic models, similarly to conventional context-dependent, Gaussian-mixturemodel based HMMs (CD-GMM-HMMs) [4], still suffer from a performance degradation under potential mismatched conditions between training and testing conditions.", "startOffset": 200, "endOffset": 209}, {"referenceID": 1, "context": "Despite the recent outstanding results demonstrated by contextdependent, deep-neural-network based hidden Markov models (CD-DNN-HMMs) in various automatic speech recognition (ASR) tasks and data sets [1, 2, 3], these acoustic models, similarly to conventional context-dependent, Gaussian-mixturemodel based HMMs (CD-GMM-HMMs) [4], still suffer from a performance degradation under potential mismatched conditions between training and testing conditions.", "startOffset": 200, "endOffset": 209}, {"referenceID": 2, "context": "Despite the recent outstanding results demonstrated by contextdependent, deep-neural-network based hidden Markov models (CD-DNN-HMMs) in various automatic speech recognition (ASR) tasks and data sets [1, 2, 3], these acoustic models, similarly to conventional context-dependent, Gaussian-mixturemodel based HMMs (CD-GMM-HMMs) [4], still suffer from a performance degradation under potential mismatched conditions between training and testing conditions.", "startOffset": 200, "endOffset": 209}, {"referenceID": 3, "context": "Despite the recent outstanding results demonstrated by contextdependent, deep-neural-network based hidden Markov models (CD-DNN-HMMs) in various automatic speech recognition (ASR) tasks and data sets [1, 2, 3], these acoustic models, similarly to conventional context-dependent, Gaussian-mixturemodel based HMMs (CD-GMM-HMMs) [4], still suffer from a performance degradation under potential mismatched conditions between training and testing conditions.", "startOffset": 326, "endOffset": 329}, {"referenceID": 4, "context": "For standard hybrid system using artificial neural networks (ANNs) and HMMs [5] in which CD-DNN-HMM is a special case, there exist many adaptation techniques.", "startOffset": 76, "endOffset": 79}, {"referenceID": 5, "context": "Unfortunately, it leads to over-fitting on the adaptation material when the amount of adaptation patterns is limited [6].", "startOffset": 117, "endOffset": 120}, {"referenceID": 6, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 48, "endOffset": 54}, {"referenceID": 7, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 48, "endOffset": 54}, {"referenceID": 8, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 71, "endOffset": 78}, {"referenceID": 9, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 71, "endOffset": 78}, {"referenceID": 5, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 101, "endOffset": 116}, {"referenceID": 10, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 101, "endOffset": 116}, {"referenceID": 11, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 101, "endOffset": 116}, {"referenceID": 12, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 101, "endOffset": 116}, {"referenceID": 13, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 133, "endOffset": 137}, {"referenceID": 14, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 167, "endOffset": 179}, {"referenceID": 15, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 167, "endOffset": 179}, {"referenceID": 16, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 167, "endOffset": 179}, {"referenceID": 17, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 201, "endOffset": 205}, {"referenceID": 18, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 270, "endOffset": 286}, {"referenceID": 15, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 270, "endOffset": 286}, {"referenceID": 19, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 270, "endOffset": 286}, {"referenceID": 20, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 270, "endOffset": 286}, {"referenceID": 5, "context": "The key idea is to augment the structure of the ANN component by adding an affine transformation network to the input [6], hidden [11], or output layer [22].", "startOffset": 118, "endOffset": 121}, {"referenceID": 10, "context": "The key idea is to augment the structure of the ANN component by adding an affine transformation network to the input [6], hidden [11], or output layer [22].", "startOffset": 130, "endOffset": 134}, {"referenceID": 21, "context": "The key idea is to augment the structure of the ANN component by adding an affine transformation network to the input [6], hidden [11], or output layer [22].", "startOffset": 152, "endOffset": 156}, {"referenceID": 22, "context": "For linear hidden network (LHN) layer approaches, the last hidden layer is usually designed to be a bottleneck to ensure an affordable parameter size [23, 24, 25, 26].", "startOffset": 150, "endOffset": 166}, {"referenceID": 23, "context": "For linear hidden network (LHN) layer approaches, the last hidden layer is usually designed to be a bottleneck to ensure an affordable parameter size [23, 24, 25, 26].", "startOffset": 150, "endOffset": 166}, {"referenceID": 24, "context": "For linear hidden network (LHN) layer approaches, the last hidden layer is usually designed to be a bottleneck to ensure an affordable parameter size [23, 24, 25, 26].", "startOffset": 150, "endOffset": 166}, {"referenceID": 25, "context": "For linear hidden network (LHN) layer approaches, the last hidden layer is usually designed to be a bottleneck to ensure an affordable parameter size [23, 24, 25, 26].", "startOffset": 150, "endOffset": 166}, {"referenceID": 26, "context": "However, adapting parameters in a CD-DNN-HMM is much more challenging than earlier connectionist adaptation schemes because of its huge parameter set size with a large number of network branches connected to a large set of tied HMM states, often referred to as senones [27].", "startOffset": 269, "endOffset": 273}, {"referenceID": 27, "context": "Such a phenomenon is commonly referred to as catastrophic forgetting [28].", "startOffset": 69, "endOffset": 73}, {"referenceID": 7, "context": "For example, a Kullback-Leibler divergence (KLD) based objective criterion to be used during adaptation was devised in [8] in order to alleviate the catastrophic forgetting problem.", "startOffset": 119, "endOffset": 122}, {"referenceID": 10, "context": "A variation to the standard method of assigning the target values was instead discussed in [11].", "startOffset": 91, "endOffset": 95}, {"referenceID": 28, "context": "Nonetheless, Bayesian solutions adopted in the CD-GMM-HMMs to address the same issue [29] have not been fully exploited.", "startOffset": 85, "endOffset": 89}, {"referenceID": 29, "context": "In this study, we attempt to cast DNN adaptation within a Bayesian framework in the spirit of maximum a posteriori (MAP) adaptation [30].", "startOffset": 132, "endOffset": 136}, {"referenceID": 30, "context": "It also compares favorably against the feature space maximum a posteriori linear regression approach to speaker adaptation proposed in [31].", "startOffset": 135, "endOffset": 139}, {"referenceID": 31, "context": "Mini-batch stochastic gradient descent (SGD) [32], with a reasonable size of mini-batches to make all matrices fit into the GPU memory, was used to update all neural parameters during training.", "startOffset": 45, "endOffset": 49}, {"referenceID": 32, "context": "Pre-training methods was used for the initialisation of the DNN parameters [33].", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": ", an LHN, and adapt only the LHN parameters while keeping fixed all of the other DNN parameters [11].", "startOffset": 96, "endOffset": 100}, {"referenceID": 22, "context": "In order to reduce the amount of parameters to adapt, usually the last hidden layer is designed to be a bottleneck (less neurons) [23, 24, 25, 26].", "startOffset": 130, "endOffset": 146}, {"referenceID": 23, "context": "In order to reduce the amount of parameters to adapt, usually the last hidden layer is designed to be a bottleneck (less neurons) [23, 24, 25, 26].", "startOffset": 130, "endOffset": 146}, {"referenceID": 24, "context": "In order to reduce the amount of parameters to adapt, usually the last hidden layer is designed to be a bottleneck (less neurons) [23, 24, 25, 26].", "startOffset": 130, "endOffset": 146}, {"referenceID": 25, "context": "In order to reduce the amount of parameters to adapt, usually the last hidden layer is designed to be a bottleneck (less neurons) [23, 24, 25, 26].", "startOffset": 130, "endOffset": 146}, {"referenceID": 33, "context": "This formulation is quite similar to maximum likelihood linear regression (MLLR) [34].", "startOffset": 81, "endOffset": 85}, {"referenceID": 7, "context": "Note that though we choose LHN for demonstration, the proposed MAP approach can be easily applied to other DNN adaptation frameworks like [8, 14, 16, 17] as well.", "startOffset": 138, "endOffset": 153}, {"referenceID": 13, "context": "Note that though we choose LHN for demonstration, the proposed MAP approach can be easily applied to other DNN adaptation frameworks like [8, 14, 16, 17] as well.", "startOffset": 138, "endOffset": 153}, {"referenceID": 15, "context": "Note that though we choose LHN for demonstration, the proposed MAP approach can be easily applied to other DNN adaptation frameworks like [8, 14, 16, 17] as well.", "startOffset": 138, "endOffset": 153}, {"referenceID": 16, "context": "Note that though we choose LHN for demonstration, the proposed MAP approach can be easily applied to other DNN adaptation frameworks like [8, 14, 16, 17] as well.", "startOffset": 138, "endOffset": 153}, {"referenceID": 29, "context": "In order to establish a MAP adaptation framework like in [30], a prior distribution over the weights of the affine transformation network need to be imposed.", "startOffset": 57, "endOffset": 61}, {"referenceID": 29, "context": "We adopted an empirical Bayes approach [30, 29] and treated each speaker in the training set as a sample speaker and supervised LHN adaptation was performed.", "startOffset": 39, "endOffset": 47}, {"referenceID": 28, "context": "We adopted an empirical Bayes approach [30, 29] and treated each speaker in the training set as a sample speaker and supervised LHN adaptation was performed.", "startOffset": 39, "endOffset": 47}, {"referenceID": 30, "context": "We observed that the histograms for weights of the adapted LHN over speakers are quite like Gaussian, so we assume that the distribution of the weights in Wlhn to be joint Gaussian [31].", "startOffset": 181, "endOffset": 185}, {"referenceID": 30, "context": "Formal MAP adaptation is conducted following [31].", "startOffset": 45, "endOffset": 49}, {"referenceID": 34, "context": "This study is concerned with the problem of speaker adaptation, and experiments are reported on the 20k-word open vocabulary Wall Street Journal task [35] using the Kaldi toolkit [36].", "startOffset": 150, "endOffset": 154}, {"referenceID": 35, "context": "This study is concerned with the problem of speaker adaptation, and experiments are reported on the 20k-word open vocabulary Wall Street Journal task [35] using the Kaldi toolkit [36].", "startOffset": 179, "endOffset": 183}, {"referenceID": 22, "context": "The bottleneck based low rank methods have been widely used to achieve more compact DNN models with equivalent performance [23, 24, 25, 26].", "startOffset": 123, "endOffset": 139}, {"referenceID": 23, "context": "The bottleneck based low rank methods have been widely used to achieve more compact DNN models with equivalent performance [23, 24, 25, 26].", "startOffset": 123, "endOffset": 139}, {"referenceID": 24, "context": "The bottleneck based low rank methods have been widely used to achieve more compact DNN models with equivalent performance [23, 24, 25, 26].", "startOffset": 123, "endOffset": 139}, {"referenceID": 25, "context": "The bottleneck based low rank methods have been widely used to achieve more compact DNN models with equivalent performance [23, 24, 25, 26].", "startOffset": 123, "endOffset": 139}, {"referenceID": 30, "context": "LIN, LIN-KLD, and MAP LIN refer to the adaptation technique based on the standard linear input network approach, the KLD regularisation technique in combination with LIN, and the maximum a posteriori transformation based adaptation when a prior is defined over the LIN parameters [31], respectively.", "startOffset": 280, "endOffset": 284}, {"referenceID": 36, "context": "For instance, efficient adaptation with a limited amount of adaptation data was obtained through the use of a tree data structure to cluster model parameters of a CD-GMM-HMM system in [37].", "startOffset": 184, "endOffset": 188}, {"referenceID": 37, "context": "Similar ideas have been recently explored in DNN learning for enhancing classification performance for classes with few examples in [38], where hierarchical priors where devised for the output layer weights matrix (top-level weights in a DNN) using a tree data structure either fixed or learnable during training.", "startOffset": 132, "endOffset": 136}, {"referenceID": 38, "context": "Top-level DNN weights in a hybrid acoustic model can be regarded as senone embeddings [39], and hierarchical priors can be defined by organising those embedding in a tree data structure.", "startOffset": 86, "endOffset": 90}, {"referenceID": 37, "context": "13 over \u03b8s with fixed DNN weights (see [38]).", "startOffset": 39, "endOffset": 43}, {"referenceID": 37, "context": "Indeed, it was demonstrated that learning the tree hierarchy during training improves the classification performance [38].", "startOffset": 117, "endOffset": 121}, {"referenceID": 2, "context": "Other forms of frame-level and sequence-level discriminative objectives [3, 40] can also be applied.", "startOffset": 72, "endOffset": 79}, {"referenceID": 39, "context": "Other forms of frame-level and sequence-level discriminative objectives [3, 40] can also be applied.", "startOffset": 72, "endOffset": 79}], "year": 2015, "abstractText": "We present a Bayesian approach to adapting parameters of a well-trained context-dependent, deep-neural-network, hidden Markov model (CD-DNN-HMM) to improve automatic speech recognition performance. Given an abundance of DNN parameters but with only a limited amount of data, the effectiveness of the adapted DNN model can often be compromised. We formulate maximum a posteriori (MAP) adaptation of parameters of a specially designed CD-DNN-HMM with an augmented linear hidden networks connected to the output tied states, or senones, and compare it to feature space MAP linear regression previously proposed. Experimental evidences on the 20,000-word open vocabulary Wall Street Journal task demonstrate the feasibility of the proposed framework. In supervised adaptation, the proposed MAP adaptation approach provides more than 10% relative error reduction and consistently outperforms the conventional transformation based methods. Furthermore, we present an initial attempt to generate hierarchical priors to improve adaptation efficiency and effectiveness with limited adaptation data by exploiting similarities among senones.", "creator": "LaTeX with hyperref package"}}}