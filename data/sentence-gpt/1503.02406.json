{"id": "1503.02406", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2015", "title": "Deep Learning and the Information Bottleneck Principle", "abstract": "Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds.\n\n\n\nWe describe the implementation of our method with the following example, using the following code, which can be used to achieve the goal:\n\nWe create an array of individual layers:\nOur data structures\nA generalization constraint is that we have two possible sizes, each one representing each of a range of possible weights:\nThe first will be a given dimension of the dimension (which means that we have two random samples):\nThe second will be a given dimension of the dimension (which means that we have two random samples):\nThe third will be a given dimension of the dimension (which means that we have two random samples):\nThe fourth will be a given dimension of the dimension (which means that we have two random samples):\nThe fifth will be a given dimension of the dimension (which means that we have two random samples):\nThe fifth will be a given dimension of the dimension (which means that we have two random samples):\nWe define a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer, a set of the parameters for each layer,", "histories": [["v1", "Mon, 9 Mar 2015 09:39:41 GMT  (192kb,D)", "http://arxiv.org/abs/1503.02406v1", "5 pages, 2 figures, Invited paper to ITW 2015; 2015 IEEE Information Theory Workshop (ITW) (IEEE ITW 2015)"]], "COMMENTS": "5 pages, 2 figures, Invited paper to ITW 2015; 2015 IEEE Information Theory Workshop (ITW) (IEEE ITW 2015)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["naftali tishby", "noga zaslavsky"], "accepted": false, "id": "1503.02406"}, "pdf": {"name": "1503.02406.pdf", "metadata": {"source": "CRF", "title": "Deep Learning and the Information Bottleneck Principle", "authors": ["Naftali Tishby", "Noga Zaslavsky"], "emails": ["tishby@cs.huji.ac.il"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nDeep Neural Networks (DNNs) and Deep Learning (DL) algorithms in various forms have become the most successful machine learning method for most supervised learning tasks. Their performance currently surpass most competitor algorithms and DL wins top machine learning competitions on real data challenges [1], [2], [3]. The theoretical understanding of DL remains, however, unsatisfactory. Basic questions about the design principles of deep networks, the optimal architecture, the number of required layers, the sample complexity, and the best optimization algorithms, are not well understood.\nOne step in that direction was recently made in a remarkable paper by Metha and Schwab [4] that showed an exact mapping between the variational Renormalization Group (RG) and DNNs based on Restricted Boltzmann Machines (RBMs). An important insight provided by that paper is that features along the layers become more and more statistically decoupled as the layers gets closer to the RG fixed point.\nIn this work we express this important insight using information theoretic concepts and formulate the goal of deep learning as an information theoretic tradeoff between compression and prediction. We first argue that the goal of any supervised learning is to capture and efficiently represent the relevant information in the input variable about the output - label - variable. Namely, to extract an approximate minimal sufficient statistics of the input with respect to the\n1The Edmond and Lilly Safra Center for Brain Sciences, Hebrew University of Jerusalem, Israel.\n2 School of Engineering and Computer Science, Hebrew University of Jerusalem, Israel. tishby@cs.huji.ac.il\n* This study was funded by the Israeli Science Foundation center of excellence, the DARPA MSEE project, the Israeli Ministry of Science and Technology, and the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI).\noutput. The information theoretic interpretation of minimal sufficient statistics [5] suggests a principled way of doing that: find a maximally compressed mapping of the input variable that preserves as much as possible the information on the output variable. This is precisely the goal of the Information Bottleneck (IB) method [6].\nSeveral interesting issues arise when applying this principle to DNNs. First, the layered structure of the network generates a successive Markov chain of intermediate representations, which together form the (approximate) sufficient statistics. This is closely related to successive refinement of information in Rate Distortion Theory [7]. Each layer in the network can now be quantified by the amount of information it retains on the input variable, on the (desired) output variable, as well as on the predicted output of the network. The Markovian structure and data processing inequalities enable us to examine the efficiency of the internal representations of the network\u2019s hidden layers, which is not possible with other distortion/error measures. It also provides us with the information theoretic limits of the compression/prediction problem and theoretically quantify each proposed DNN for the given training data. In addition, this representation of DNNs gives a new theoretical sample complexity bound, using the known finite sample bounds on the IB [8].\nAnother outcome of this representation is a possible explanation of the layered architecture of the network, different from the one suggested in [4]. Neurons, as non-linear (e.g. sigmoidal) functions of a dot-product of their input, can only capture linearly separable properties of their input layer. Linear separability is possible when the input layer units are close to conditional independence, given the output classification. This is generally not true for the data distribution and intermediate hidden layer are required. We suggest here that the break down of the linear-separability is associated with a representational phase transition (bifurcation) in the IB optimal curve, as both result from the second order dependencies in the data. Our analysis suggests new information theoretic optimality conditions, sample complexity bounds, and design principle for DNN models.\nThe rest of the paper is organized as follows. We first review the structure of DNNs as a Markov cascade of intermediate representations between the input and output layers, made out of layered sigmoidal neurons. Next we review the IB principle as a special type of Rate Distortion problem, and discuss how DNNs can be analyzed in this special rate-distortion distortion plane. In section III we describe the information theoretic constraints on DNNs and suggest a new optimal learning principle, using finite sample bounds on the IB problem. Finally, we suggest an intriguing\nar X\niv :1\n50 3.\n02 40\n6v 1\n[ cs\n.L G\n] 9\nM ar\n2 01\n5\nconnection between the IB structural phase transitions and the layered structure of DNNs."}, {"heading": "II. BACKGROUND", "text": ""}, {"heading": "A. Deep Neural Networks", "text": "DNNs are comprised of multiple layers of artificial neurons, or simply units, and are known for their remarkable performance in learning useful hierarchical representations of the data for various machine learning tasks. While there are many different variants of DNNs [9], here we consider the rather general supervised learning settings of feedforward networks in which multiple hidden layers separate the input and output layers of the network (see figure 1). Typically, the input, denoted by X , is a high dimensional variable, being a low level representation of the data such as pixels of an image, whereas the desired output, Y , has a significantly lower dimensionality of the predicted categories. This generally means that most of the entropy of X is not very informative about Y , and that the relevant features in X are highly distributed and difficult to extract. The remarkable success of DNNs in learning to extract such features is mainly attributed to the sequential processing of the data, namely that each hidden layer operates as the input to the next one, which allows the construction of higher level distributed representations.\nThe computational ability of a single unit in the network is limited, and is often modeled as a sigmoidal neuron. This means that the output of each layer is hk = \u03c3 (Wkhk\u22121 + bk), where Wk is the connectivity matrix which determines the weights of the inputs to hk, bk is a bias term, and \u03c3(u) = 11+exp(\u2212u) is the standard sigmoid function. Given a particular architecture, training the network is reduced to learning the weights between each layer. This is usually done by stochastic gradient decent methods, such as back-propagation, that aim at minimizing some prediction error, or distortion, between the desired and predicted outputs Y and Y\u0302 given the input X . Interestingly, other DNN architectures implement stochastic mapping between the layers, such as the RBM based DNNs [2], but it is not clear so far why or when such stochasticity can improve performance. Symmetries of the data are often taken into account through weight sharing, as in convolutional neural networks [10], [3].\nSingle neurons can (usually) classify only linearly separable inputs, as they can implement only hyperplanes in their input space, u = w \u00b7h+b. Hyperplanes can optimally classify data when the inputs are conditionally independent. To see this, let p(x|y) denote the (binary) class (y) conditional probability of the inputs x. Bayes theorem tells us that\np(y|x) = 1 1 + exp ( \u2212 log p(x|y)p(x|y\u2032) \u2212 log p(y) p(y\u2032) ) (1) which can be written as a sigmoid of a dot-product of the inputs when\np(x|y) p(x|y\u2032) = N\u220f j=1 [ p(xj |y) p(xj |y\u2032) ]np(xj) . (2)\nThe sigmoidal neuron can calculate precisely the posterior probability with weights wj = log\np(xj |y) p(xj |y\u2032) , and bias b =\nlog p(y)p(y\u2032) , when the neuron\u2019s inputs are proportional to the probability of the respective feature in the input layer, i.e. hj = np(xj). As such conditional independence can not be assumed for general data distributions, representational changes through the hidden layers are required, up to linear transformation that can decouple the inputs.\nAs suggested in [4], approximate conditional independence is effectively achieved for RBM based DNNs through successive RG transformations that decouple the units without loss of relevant information. The relevant compression, however, is implicit in the RG transformation and does not hold for more general DNN architectures.\nThe other common way of statistically decoupling the units is by dimension expansion, or embedding in very high dimension, as done implicitly by Kernel machines, or by random expansion. There are nevertheless sample and computational costs to such dimensional expansion and these are clearly not DNN architectures.\nIn this paper we propose a purely information theoretic view of DNNs, which can quantify their performance, provide a theoretical limit on their efficiency, and give new finite sample complexity bounds on their generalization abilities. Moreover, our analysis suggests that the optimal DNN architecture is also determined solely by an information theoretic analysis of the joint distribution of the data, p(X,Y )."}, {"heading": "B. The Information Bottleneck Principle", "text": "The information bottleneck (IB) method was introduced as an information theoretic principle for extracting relevant information that an input random variable X \u2208 X contains about an output random variable Y \u2208 Y . Given their joint distribution p (X,Y ), the relevant information is defined as the mutual information I (X;Y ), where we assume statistical dependence between X and Y . In this case, Y implicitly determines the relevant and irrelevant features in X . An\noptimal representation of X would capture the relevant features, and compress X by dismissing the irrelevant parts which do not contribute to the prediction of Y .\nIn pure statistical terms, the relevant part of X with respect to Y , denoted by X\u0302 , is a minimal sufficient statistics of X with respect Y . Namely, it is the simplest mapping of X that captures the mutual information I(X;Y ). We thus assume the Markov chain Y \u2192 X \u2192 X\u0302 and minimize the mutual information I(X; X\u0302) to obtain the simplest statistics (due to the data processing inequality (DPI) [5]), under a constraint on I(X\u0302;Y ). Namely, finding an optimal representation X\u0302 \u2208 X\u0302 is formulated as the minimization of the following Lagrangian\nL [p (x\u0302|x)] = I ( X; X\u0302 ) \u2212 \u03b2I ( X\u0302;Y ) (3)\nsubject to the Markov chain constraint. The positive Lagrange multiplier \u03b2 operates as a tradeoff parameter between the complexity (rate) of the representation, R = I(X; X\u0302), and the amount of preserved relevant information, IY = I(X\u0302;Y ).\nFor general distributions, p(X,Y ), exact minimal sufficient statistics may not exist, and the prediction Markov chain, X \u2192 X\u0302 \u2192 Y is incorrect. If we denote by Y\u0302 the predicted variable, the DPI implies I(X;Y ) \u2265 I(Y ; Y\u0302 ), with equality if and only if X\u0302 is a sufficient statistic.\nAs was shown in [6], the optimal solutions for the IB variational problem satisfy the following self-consistent equations for some value of \u03b2,\np (x\u0302|x) = p (x\u0302) Z (x;\u03b2) exp (\u2212\u03b2D [p (y|x) \u2016p (y|x\u0302)])\np (y|x\u0302) = \u2211 x p (y|x) p (x|x\u0302)\np (x\u0302) = \u2211 x p (x) p (x\u0302|x)\nwhere Z (x;\u03b2) is the normalization factor, also known as the partition function.\nThe IB can be seen as a rate-distortion problem with a nonfixed distortion measure that depends on the optimal map, defined as dIB (x, x\u0302) = D [p (y|x) \u2016p (y|x\u0302)], where D is the Kullback-Leibler divergence. The self consistent equations can be iterated, as in the Arimoto-Blahut algorithm, for calculating the optimal IB tradeoff, or rate-distortion function, though this is not a convex optimization problem.\nWith this interpretation, the expected IB distortion is then DIB = E [ dIB ( X, X\u0302 )] = I(X;Y |X\u0302)\nwhich is the residual information between X and Y , namely the relevant information not captured by X\u0302 . Clearly, the variational principle in Eq.3 is equivalent to\nL\u0303 [p (x\u0302|x)] = I ( X; X\u0302 ) + \u03b2I (X;Y | X\u0302)\nas they only differ by a constant. The optimal tradeoff for this variational problem is defined by a rate-distortion like curve [11], as depicted by the black curve in figure 2. The\nparameter \u03b2 is the negative inverse slope of this curve, as with rate-distortion functions.\nInterestingly, the IB distortion curve, also known as the information curve for the joint distribution p(X,Y ), may have bifurcation points to sub-optimal curves (the short blue curves in figure 2), at critical values of \u03b2. These bifurcations correspond to phase transitions between different topological representations of X\u0302 , such as different cardinality in clustering by deterministic annealing [12], or dimensionality change for continues variables [13]. These bifurcations are pure properties of the joint distribution, independent of any modeling assumptions.\nOptimally, DNNs should learn to extract the most efficient informative features, or approximate minimal sufficient statistics, with the most compact architecture (i.e. minimal number of layers, with minimal number of units within each layer)."}, {"heading": "III. A NEW INFORMATION THEORETIC LEARNING PRINCIPLE FOR DNNS", "text": "A. Information characteristics of the layers\nAs depicted in figure 1, each layer in a DNN processes inputs only from the previous layer, which means that the network layers form a Markov chain. An immediate consequence of the DPI is that information about Y that is lost in one layer cannot be recovered in higher layers. Namely, for any i \u2265 j it holds that\nI (Y ;X) \u2265 I (Y ;hj) \u2265 I (Y ;hi) \u2265 I ( Y ; Y\u0302 ) . (4)\nAchieving equality in Eq.4 is possible if and only if each layer is a sufficient statistic of its input. By requiring not only the most relevant representation at each layer, but also the most concise representation of the input, each layer should attempt to maximize I (Y ;hi) while minimizing I (hi\u22121;hi) as much as possible.\nFrom a learning theoretic perspective, it may not be immediately clear why the quantities I (hi\u22121;hi) and I (Y ;hi) are relevant for efficient learning and generalization. It has been shown in [8] that the mutual information I(X\u0302;Y ), which corresponds to I (Y ;hi) in our context, can bound the prediction error in classification tasks with multiple classes. In sequential multiple hypotheses testing, the mutual information gives a (tight) bound on the harmonic mean of the log probability of error over the decision time.\nHere we consider I(Y ; Y\u0302 ) as the natural quantifier of the quality of the DNN, as it measures precisely how much of the predictive features in X for Y is captured by the model. Reducing I (hi\u22121;hi) also has a clear learning theoretic interpretation as the minimal description length of the layer.\nThe information distortion of the IB principle provides a new measure of optimality which can be applied not only for the output layer, as done when evaluating the performance of DNNs with other distortion or error measures, but also for evaluating the optimality of each hidden layer or unit of the network. Namely, each layer can be compared to the optimal\nIB limit for some \u03b2,\nI (hi\u22121;hi) + \u03b2I (Y ;hi\u22121|hi)\nwhere we define h0 = X and hm+1 = Y\u0302 . This optimality criterion also give a nice interpretation of the construction of higher level representations along the network. Since each point on the information curve is uniquely defined by \u03b2, shifting from low to higher level representations is analogous to successively decreasing \u03b2. Notice that other cost functions, such as the squared error, are not applicable for evaluating the optimality of the hidden layers, nor can they account for multiple levels of description.\nThe theoretical IB limit and the limitations that are imposed by the DPI on the flow of information between the layers, gives a general picture as to to where each layer of a trained network can be on the information plane. The input level clearly has the least IB distortion, and requires the longest description (even after dimensionality reduction, X is the lowest representation level in the network). Each consecutive layer can only increase the IB distortion level, but it also compresses its inputs, hopefully eliminating only irrelevant information. The green line in figure 2 shows a possible path of the layers in the information plane."}, {"heading": "B. Finite Samples and Generalization Bounds", "text": "It is important to note that the IB curve is a property of the joint distribution p (X,Y ), however this distribution is obviously unknown in actual machine learning tasks. In fact, machine learning algorithms, and in particular training algorithms for DNNs, have only access to a finite sample. Nonetheless, it has been shown in [8] that it is possible to\ngeneralize using the IB principle as a learning objective from finite samples, as long as the representational complexity (i.e. the cardinality of X\u0302) is limited. Assume all variables have finite support, and let K = |X\u0302 |. Denote by I\u0302 the empirical estimate of the mutual information based on the finite sample distribution p\u0302(x, y) for a given sample of size n. The generalization bounds proven in [8] guarantee that\nI ( X\u0302;Y ) \u2264 I\u0302 ( X\u0302;Y ) +O ( K |Y|\u221a\nn ) and that\nI ( X; X\u0302 ) \u2264 I\u0302 ( X; X\u0302 ) +O ( K\u221a n ) .\nNotice that these bounds get worse with K, but do not depend on the cardinality of X . This means that the IB optimal curve can be well estimated for learning compressed representations, and is badly estimated for learning complex models. The complexity of the representation is not precisely the cardinality imposed by the support of X\u0302 , but its effective description length, namely K \u2248 2I(X\u0302;X). This gives a continuous worst case upper bound on the true I(X\u0302;Y ) for any given sample size n. This bound is illustrated in figure 2, when interpreting the information curve (in black) as the empirical curve (i.e. the optimal tradeoff with respect to p\u0302 (X,Y ) rather than p (X,Y )). The red curve is the worstcase bound, and its minimum is the optimal point on the information curve in the sense that it gives the best worst case true tradeoff between the complexity and the accuracy of the representation. Denote this point by (R\u2217 (n) , D\u2217IB (n)). Notice that the empirical information curve might be too optimistic especially at its extreme - most complex - end. Thus that point is not truly the most informative, as opposed to corresponding point on the true information curve.\nFrom this analysis it is clear that the empirical input layer of a DNN alone cannot guarantee good generalization even though it contains more information about the target variable Y than the hidden layers, as its representation of the data is too complex. Compression is thus necessary for generalization. In other words, the hidden layers must compress the input in order to reach a point where the worst case generalization error is tolerable.\nThis analysis also suggests a method for evaluating the network. Let N be a given DNN, and denote by DN the IB distortion of the network\u2019s output layer, i.e. I(X;Y |Y\u0302 ), and by RN the representational complexity of the output layer, i.e. I(X; Y\u0302 ). We can now define two measures for the performance of the network in terms of prediction and compression. The first one is the generalization gap,\n\u2206G = DN \u2212D\u2217IB (n)\nwhich bounds the amount of information about Y that the network did not capture although it could have. The second measure is the complexity gap,\n\u2206C = RN \u2212R\u2217 (n)\nwhich bounds the amount of unnecessary complexity in the network. Clearly, there is no reason to believe that current training algorithms for DNNs will reach the optimal point of the IB finite sample bound. However, we do believe that the improved feature detection along the network\u2019s layers corresponds to improvement on the information plane in this direction. In other words, when placing the layers of a trained DNN on the information plane, they should form a path similar to the green curve in figure 2. It is thus desirable to find new training algorithms that are based on the IB optimality conditions and can shift the DNN layers closer to the optimal limit.\nIV. IB PHASE TRANSITIONS AND THE BREAKDOWN OF LINEAR SEPARABILITY\nThe most intriguing aspect of our IB analysis of DNNs, which we can only begin to address here, is its connection to the network\u2019s architecture, namely, the emergence of the layered structure and the optimal connectivity between the layers.\nThere seems to be an interesting correspondence between the IB phase transitions - the bifurcations to simpler representations along the information curve - and the linear separability condition between the hidden layers. Following the bifurcation analysis of the cluster splits in [14], [12] for the IB phase transitions, one can show that the critical \u03b2 is determined by the largest eigenvalue of the second order correlations of p(X,Y |X\u0302(\u03b2)), at that critical \u03b2.\nOn the other hand, the linear separability condition, Eq.2, breaks down when the conditional second order correlations of the data can not be ignored. This happens at the values of \u03b2 for which the second order (first non-linear term) of the log-likelihood ratio, conditioned on the current representation, X\u0302(\u03b2), become important, with the same eigenvalues that determine the phase transitions. Namely, the linear separability required for the DNN layers is intimately related to the structural representation phase transitions along the IB curve. We therefore conjecture that the optimal points for the DNN layers are at values of \u03b2 right after the bifurcation transitions on the IB optimal curve. When these phase transitions are linearly independent they may be combined within a single layer, as can be done with linear networks (e.g. in the Gaussian IB problem [13])."}, {"heading": "V. DISCUSSION", "text": "We suggest a novel information theoretic analysis of deep neural networks based on the information bottleneck principle. Arguably, DNNs learn to extract efficient representations of the relevant features of the input layer X for predicting the output label Y , given a finite sample of the joint distribution p(X,Y ). This representation can be compered with the theoretically optimal relevant compression of the variable X with respect to Y , provided by the information bottleneck (or information distortion) tradeoff. This is done by introducing a new information theoretic view of DNN training as an successive (Markovian) relevant compression of the input variable X , given the empirical training data. The DNN\u2019s\nprediction is activating the trained compression layered hierarchy to generate a predicted label Y\u0302 . Maximizing the mutual information I(Y ; Y\u0302 ), for a sequence of evoking inputs X , emerges as the natural DNN optimization goal.\nThis new representation of DNNs offers several interesting advantages: \u2022 The network and all its hidden layers can be directly\ncompered to the optimal IB limit, by estimating the mutual information between each layer and the input and the output variables, on the information plane. \u2022 New information theoretic optimization criteria for optimal DNN representations. \u2022 New sample complexity bounds on the network generalization ability using the IB finite sample bounds. \u2022 Stochastic DNN architectures can get closer to the optimal theoretical limit. \u2022 There appears to be a connection, which should be further explored, between the network architecture - the number and structure of the layers - and the structural phase transitions in the IB problem, as both are related to spectral properties of the second order correlations of the data, at the critical points."}], "references": [{"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1798\u2013828, Aug. 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013 507, July 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2012, pp. 1106\u20131114.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "An exact mapping between the variational renormalization group and deep learning", "author": ["P. Mehta", "D.J. Schwab"], "venue": "CoRR, vol. abs/1410.3831, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Elements of information theory", "author": ["T. Cover", "J. Thomas"], "venue": "Wiley New York,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1991}, {"title": "The information bottleneck method", "author": ["N. Tishby", "F.C. Pereira", "W. Bialek"], "venue": "Proceedings of the 37-th Annual Allerton Conference on Communication, Control and Computing, 1999, pp. 368\u2013377.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Successive refinement of information", "author": ["W.H.R. Equitz", "T.M. Cover"], "venue": "IEEE Transactions on Information Theory, vol. 37, no. 2, pp. 269\u2013275, 1991.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1991}, {"title": "Learning and generalization with the information bottleneck", "author": ["O. Shamir", "S. Sabato", "N. Tishby"], "venue": "Theor. Comput. Sci., vol. 411, no. 29-30, pp. 2696\u20132711, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning Deep Architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning, vol. 2, no. 1, pp. 1\u2013127, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Y. LeCun", "Y. Bengio"], "venue": "The handbook of brain theory and neural networks, vol. 3361, p. 310, 1995.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "An information theoretic tradeoff between complexity and accuracy", "author": ["R. Gilad-Bachrach", "A. Navot", "N. Tishby"], "venue": "Proceedings of the COLT, 2003.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Deterministic annealing for clustering, compression, classification, regression, and related optimization problems", "author": ["K. Rose"], "venue": "Proceedings of the IEEE, 1998, pp. 2210\u20132239.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "Information bottleneck for gaussian variables", "author": ["G. Chechik", "A. Globerson", "N. Tishby", "Y. Weiss"], "venue": "Journal of Machine Learning Research, vol. 6, pp. 165\u2013188, 2005.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Statistical mechanics and phase transitions in clustering", "author": ["K. Rose", "E. Gurewitz", "G.C. Fox"], "venue": "Phys. Rev. Lett., vol. 65, pp. 945\u2013948, 1990.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1990}], "referenceMentions": [{"referenceID": 0, "context": "Their performance currently surpass most competitor algorithms and DL wins top machine learning competitions on real data challenges [1], [2], [3].", "startOffset": 133, "endOffset": 136}, {"referenceID": 1, "context": "Their performance currently surpass most competitor algorithms and DL wins top machine learning competitions on real data challenges [1], [2], [3].", "startOffset": 138, "endOffset": 141}, {"referenceID": 2, "context": "Their performance currently surpass most competitor algorithms and DL wins top machine learning competitions on real data challenges [1], [2], [3].", "startOffset": 143, "endOffset": 146}, {"referenceID": 3, "context": "One step in that direction was recently made in a remarkable paper by Metha and Schwab [4] that showed an exact", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "The information theoretic interpretation of minimal sufficient statistics [5] suggests a principled way of doing that: find a maximally compressed mapping of the input variable that preserves as much as possible the information on the output variable.", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "This is precisely the goal of the Information Bottleneck (IB) method [6].", "startOffset": 69, "endOffset": 72}, {"referenceID": 6, "context": "This is closely related to successive refinement of information in Rate Distortion Theory [7].", "startOffset": 90, "endOffset": 93}, {"referenceID": 7, "context": "In addition, this representation of DNNs gives a new theoretical sample complexity bound, using the known finite sample bounds on the IB [8].", "startOffset": 137, "endOffset": 140}, {"referenceID": 3, "context": "Another outcome of this representation is a possible explanation of the layered architecture of the network, different from the one suggested in [4].", "startOffset": 145, "endOffset": 148}, {"referenceID": 8, "context": "While there are many different variants of DNNs [9], here we consider the rather general supervised learning settings of feedforward networks in which multiple hidden layers separate the input and output layers of the network (see figure 1).", "startOffset": 48, "endOffset": 51}, {"referenceID": 1, "context": "Interestingly, other DNN architectures implement stochastic mapping between the layers, such as the RBM based DNNs [2], but it is not clear so far why or when such stochasticity can improve performance.", "startOffset": 115, "endOffset": 118}, {"referenceID": 9, "context": "Symmetries of the data are often taken into account through weight sharing, as in convolutional neural networks [10], [3].", "startOffset": 112, "endOffset": 116}, {"referenceID": 2, "context": "Symmetries of the data are often taken into account through weight sharing, as in convolutional neural networks [10], [3].", "startOffset": 118, "endOffset": 121}, {"referenceID": 3, "context": "As suggested in [4], approximate conditional independence is effectively achieved for RBM based DNNs through successive RG transformations that decouple the units without loss of relevant information.", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "We thus assume the Markov chain Y \u2192 X \u2192 X\u0302 and minimize the mutual information I(X; X\u0302) to obtain the simplest statistics (due to the data processing inequality (DPI) [5]), under a constraint on I(X\u0302;Y ).", "startOffset": 167, "endOffset": 170}, {"referenceID": 5, "context": "As was shown in [6], the optimal solutions for the IB variational problem satisfy the following self-consistent equations for some value of \u03b2,", "startOffset": 16, "endOffset": 19}, {"referenceID": 10, "context": "The optimal tradeoff for this variational problem is defined by a rate-distortion like curve [11], as depicted by the black curve in figure 2.", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "These bifurcations correspond to phase transitions between different topological representations of X\u0302 , such as different cardinality in clustering by deterministic annealing [12], or dimensionality change for continues variables [13].", "startOffset": 176, "endOffset": 180}, {"referenceID": 12, "context": "These bifurcations correspond to phase transitions between different topological representations of X\u0302 , such as different cardinality in clustering by deterministic annealing [12], or dimensionality change for continues variables [13].", "startOffset": 231, "endOffset": 235}, {"referenceID": 7, "context": "It has been shown in [8] that the mutual information I(X\u0302;Y ), which corresponds to I (Y ;hi) in our context, can bound the prediction error in classification tasks with multiple", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "Nonetheless, it has been shown in [8] that it is possible to generalize using the IB principle as a learning objective from finite samples, as long as the representational complexity (i.", "startOffset": 34, "endOffset": 37}, {"referenceID": 7, "context": "The generalization bounds proven in [8] guarantee that", "startOffset": 36, "endOffset": 39}, {"referenceID": 13, "context": "Following the bifurcation analysis of the cluster splits in [14], [12] for the IB phase transitions, one can show that the critical \u03b2 is determined by the largest eigenvalue of the second order correlations of p(X,Y |X\u0302(\u03b2)), at that critical \u03b2.", "startOffset": 60, "endOffset": 64}, {"referenceID": 11, "context": "Following the bifurcation analysis of the cluster splits in [14], [12] for the IB phase transitions, one can show that the critical \u03b2 is determined by the largest eigenvalue of the second order correlations of p(X,Y |X\u0302(\u03b2)), at that critical \u03b2.", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "in the Gaussian IB problem [13]).", "startOffset": 27, "endOffset": 31}], "year": 2015, "abstractText": "Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network\u2019s simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.", "creator": "LaTeX with hyperref package"}}}