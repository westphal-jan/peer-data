{"id": "1503.05018", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2015", "title": "Ultra-Fast Shapelets for Time Series Classification", "abstract": "Time series shapelets are discriminative subsequences and their similarity to a time series can be used for time series classification. Since the discovery of time series shapelets is costly in terms of time, the applicability on long or multivariate time series is difficult. In this work we propose Ultra-Fast Shapelets that uses a number of random shapelets. It is shown that Ultra-Fast Shapelets yield the same prediction quality as current state-of-the-art shapelet-based time series classifiers that carefully select the shapelets by being by up to three orders of magnitudes. Since this method allows a ultra-fast shapelet discovery, using shapelets for long multivariate time series classification becomes feasible.\n\n\n\n\nThe study, published today in the Journal of Comparative Statistical Sciences (JACC), shows that the theoretical application of Ultra-Fast Shapelets to classification is to use only a simple random series that can be observed in the same time series.\nThe Ultra-Fast Shapelet\nThe Ultra-Fast Shapelet was originally created by Professor G. K. S\u00f8rensen. It was the first, yet only, data-based data-based shapelet, which was first introduced on July 25, 2006 by a group of scientists from MIT and the National Institute of Research.\nThe Ultra-Fast Shapelet is made up of several parts. It consists of two sets of large layers that are part of a grid consisting of two separate parts. The layer contains a wide set of layers called the main layer. The main layer is a grid consisting of two rows of squares with an edge and the inner layer contains a large number of edges.\nIn the bottom row, the central layer contains a row of two rows of two rows of four squares. The top row contains a wide set of rows of four rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of five rows of", "histories": [["v1", "Tue, 17 Mar 2015 12:41:30 GMT  (489kb,D)", "http://arxiv.org/abs/1503.05018v1", "Preprint submitted to Journal of Data &amp; Knowledge Engineering January 24, 2015"]], "COMMENTS": "Preprint submitted to Journal of Data &amp; Knowledge Engineering January 24, 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["martin wistuba", "josif grabocka", "lars schmidt-thieme"], "accepted": false, "id": "1503.05018"}, "pdf": {"name": "1503.05018.pdf", "metadata": {"source": "CRF", "title": "Ultra-Fast Shapelets for Time Series Classification", "authors": ["Martin Wistuba", "Josif Grabocka", "Lars Schmidt-Thieme"], "emails": ["wistuba@ismll.uni-hildesheim.de", "josif@ismll.uni-hildesheim.de", "schmidt-thieme@ismll.uni-hildesheim.de"], "sections": [{"heading": null, "text": "Time series shapelets are discriminative subsequences and their similarity to a time series can be used for time series classification. Since the discovery of time series shapelets is costly in terms of time, the applicability on long or multivariate time series is difficult. In this work we propose Ultra-Fast Shapelets that uses a number of random shapelets. It is shown that Ultra-Fast Shapelets yield the same prediction quality as current state-of-theart shapelet-based time series classifiers that carefully select the shapelets by being by up to three orders of magnitudes. Since this method allows a ultra-fast shapelet discovery, using shapelets for long multivariate time series classification becomes feasible.\nA method for using shapelets for multivariate time series is proposed and Ultra-Fast Shapelets is proven to be successful in comparison to state-of-the-art multivariate time series classifiers on 15 multivariate time series datasets from various domains. Finally, time series derivatives that have proven to be useful for other time series classifiers are investigated for the shapelet-based classifiers. It is shown that they have a positive impact and that they are easy to integrate with a simple preprocessing step, without the need of adapting the shapelet discovery algorithm.\nKeywords: Data mining, Mining methods and algorithms, Time Series Classification, Scalability, Time Series Shapelets"}, {"heading": "1. Introduction", "text": "Time series classification is a field of significant interest for researchers because time series occur in various domains such as finance, multimedia, medicine and more. A time series is a sequence of data points that have a temporal relation between each other. For time series classification it is common to identify motifs or local patterns that have discrimination quality towards the target variable. One popular method is to identify shapelets [1]. Shapelets are discriminative subsequences and have the property that the distance between a shapelet and its best matching subsequence of a time series is a good\n\u2217Corresponding author Email addresses: wistuba@ismll.uni-hildesheim.de (Martin Wistuba),\njosif@ismll.uni-hildesheim.de (Josif Grabocka), schmidt-thieme@ismll.uni-hildesheim.de (Lars Schmidt-Thieme)\nPreprint submitted to Journal of Data & Knowledge Engineering January 24, 2015\nar X\niv :1\n50 3.\n05 01\n8v 1\n[ cs\n.L G\n] 1\n7 M\nar 2\n01 5\nECGFiveDays\npredictor for time series classification. Many methods try to find shapelets and apply Shapelet Transformation [2]. Shapelet Transformation is the data transformation method that is used to convert the raw time series data using the shapelets to a different data representation that contains features that correspond to a specific shapelet and its value is the minimal distance to the time series (see Section 4.2.1). The idea of shapelets was mainly applied for univariate time series classification but also for time series clustering [3] and early classification of multivariate time series [4].\nOne of the biggest problems of shapelet discovery is that it is very time-consuming. Hence, there are various methods of pruning the candidate space [5], improving the scoring function that defines how good a shapelet is [6, 1] or by parallelization [7]. Nevertheless, discovering shapelets remains a slow procedure and is infeasible for large datasets.\nWe want to tackle the problem of slow shapelet discovery by introducing an unsupervised method that does not need to compute the prediction accuracy of each single candidate. Instead a pool of unsupervised, sampled shapelets is computed and a model is learned in the end. The idea is to exploit the fact that discriminative motifs are appearing frequently. This improves the shapelet discovery process and still a good accuracy can be achieved. This effect is shown in Figure 1.\nThe contributions are four-fold:\n1. An ultra-fast way of extracting time series subsequences is proposed that allows faster feature extraction than any method that tries to identify discriminative subsequences in a supervised way but is still comparable in terms of classification accuracy.\n2. We propose a method which allows time series classification on multivariate time series that concatenates features extracted on different streams. This idea is evaluated on 15 datasets from various domains and compared to state-of-the-art methods for multivariate time series classification.\n3. Time series derivatives are considered as additional features for shapelet-based classifiers. Its easy integration with a simple preprocessing step is demonstrated\nas well as the positive impact on the classification accuracy for shapelet-based classifiers.\n4. A comparison between different shapelet-based methods using the original authors\u2019 implementation is conducted. Additionally, the sensitivity for the hyperparameter that defines the number of shapelets that are extracted is analyzed.\nThe remainder of this paper is organized as follows. In the next section related work is presented and delimited from our contributions. In Section 4, the notation and problem as well as a brief background description is provided. Also, our idea of shapelet discovery is presented and motivated and finally extended to multivariate time series classification. The section ends with a discussion about time series derivatives. Afterwards, in Section 5, it is presented that our idea of shapelet discovery is faster than the state-of-the-art but nevertheless as accurate for univariate time series. Finally, on 15 multivariate time series datasets it is empirically proven that our method provides better predictors for classification. The work is concluded in Section 6."}, {"heading": "2. Related Work", "text": "The concept of shapelets, discriminative subsequences, was first introduced by Ye et al. [1]. The idea is to consider all subsequences of the training data and assess them regarding a scoring function to estimate how predictive they are with respect to the target. The first proposed scoring function was information gain. Also other measures like F-Stat, Kruskall-Wallis and Mood\u2019s median were considered [2, 8]. It is possible to use the extracted subsequences to transform the data and use an arbitrary classifier [2]. Instead of searching shapelets exhaustively, Grabocka et al. [9] try to learn optimal shapelets with respect to the target and report statistically significant improvements in accuracy compared to other shapelet-based classifiers.\nShapelets have been used in many applications such as medicine [4], gesture [10] and gait recognition [11] and even time series clustering [3].\nSince a time series dataset usually contains many shapelet candidates, a brute-force search is very time-consuming and hence many speed-up techniques exist. On the one hand, there are smart implementations using early abandon of distance computation and entropy pruning for the information gain heuristic [1]. On the other hand, ideas to trade time for speed and reuse computations and to prune the search space [6] as well as pruning candidates by searching possibly interesting candidates on the SAX representation [5] or using infrequent shapelets [12] are applied. Gordon et al. [13] learn a decision tree using random subsequences. For each split they consider random subsequences and compute the information gain. If for some time no better subsequence was chosen, the subsequence is used for the split.\nIn comparison to the related work, discriminative subsequences are not assessed using a scoring function. Instead of that, subsequences are chosen at random and subsequences that provide discriminative features are identified during the learning process of a classifier. This leads to a faster feature extraction process without much impact on the classification accuracy. Our method is also not restricted to a specific classifier.\nShapelets are already considered for the multivariate time series classification by Ghalwash et al. [4]. However, they use multivariate instead of univariate shapelets and use them for early classification. We follow a different idea by using univariate shapelets\nthat are specific for a stream and by considering their interaction among each other using the classifier. Hence, interaction between streams are learned and not assumed.\nA very common method for multivariate time series classification is to apply dimensionality reduction like singular value decomposition on the data and then use any classifier on this data. This overcomes the problem of time series with varying lengths [14, 15]. Other methods try to use similiarity-based methods that have proven to be useful for univariate time series classification. For example dynamic time warping was applied on multivariate time series in the context of accelerometer-based gesture recognition [16, 17].\nBaydogan et al. [18] use a symbolic representation for multivariate time series. This is similar to SAX [19] but in contrast, the symbols are not fixed but learned in a supervised way using random forests."}, {"heading": "3. Baselines", "text": "As it is later shown, Ultra-Fast Shapelets (UFS) is comparably accurate as any other shapelet-based classifier but needs less time to extract the shapelets. UFS is compared to three different methods to extract shapelets from univariate data. Additionally, the reader will notice that the number of hyperparameters needed for UFS is, in comparison, very low."}, {"heading": "3.1. Exhaustive Search (ES)", "text": "The exhaustive search (ES) [2, 6, 1] considers every subsequence in the training data and ranks it using a scoring function s. As discussed in Section 4.2, this is equivalent to variable ranking. The scoring function s is usually the information gain but also other quality measures like Kruskal-Wallis, F-statistic and Mood\u2019s median were already considered.\nAs considering all candidates is infeasible for bigger datasets, the candidates are reduced by considering only subsequences of specific lenghts by choosing a minimum and a maximum length, sometimes a stepsize greater than one. Obviously, as the length of the best subsequence length is unknown, this are very sensitive hyperparameters. A further hyperparamater is the number of shapelets that will be chosen. This is also an important hyperparameter as setting it too low might lead to not considering important features and setting it too high adds to much noise for simple classifiers like Nearest Neighbor."}, {"heading": "3.2. Fast Shapelets (FS)", "text": "Since exhaustively searching shapelets is very slow, the need for a faster way for extracting shapelets exists. Hence, an approximative method, so called Fast Shapelets (FS) [5], was introduced. The idea is reduce the dimension of the data by estimating the SAX representation [19] and searching on the reduced space for features that are likely to be useful. Hence, mapping back from the reduced space to the original space only few candidates are left. The final features are estimated applying variable ranking. Fast Shapelets is the fastest published method for shapelet discovery we are aware of that yields comparable classification accuracy.\nLike in the exhaustive search, there exist the hyperparameters to prune the search space by subsequence length. However, it is also possible to simply take all of them.\nThree new hyperparameters for the dimensionality reduction are needed, i.e. window length, alphabet size and word length, which might be less sensitive."}, {"heading": "3.3. Learning Shapelets (LS)", "text": "Recently, a complete new idea was presented. Instead of restricting the pool of possible candidates to those found in the training data and simply searching them, Grabocka et al. [9] propose to consider the shapelets to be parameters that are optimized regarding the loss as well. Hence, shapelets are not found using an approximative measure for being useful for a model but directly optimized for it. This means, it has an advantage because the method does not consider a limited set of candidates but can choose arbitrary shapelets. Disadvantages of this method are that its accuracy highly depends on the initial shapelets and not every classification model can be used. The number of hyperparameters is the same as the exhaustive search but it will be shown that the running time can be slower. In the following sections, this method is called Learning Shapelets (LS)."}, {"heading": "3.4. Symbolic Representation for Multivariate Timeseries", "text": "Symbolic Representation for Multivariate Timeseries (SMTS) [18] is not based on shapelets but has its own way of generating features. The idea is to represent time series as histograms of symbols where a symbol represents a leaf of a decision tree. The raw data is transformed such that for each time point j of time series Ti with label yi the transformed data contains an instance (j, ti,1,j , . . . , ti,s,j , ti,2,j \u2212 ti,1,j , . . . , ti,m,j \u2212 ti,m\u22121,j) with label yi. A random forest is trained on the transformed data and its leafs are considered to be the symbols. The number of occurences of a symbol in the raw data is counted and these symbol histograms are used for the final classification step using random forests."}, {"heading": "4. Ultra-Fast Shapelets for Univariate and Multivariate Time Series", "text": "Shapelets are often defined as discriminative subsequences and extracted using a supervised quality measure. This process corresponds to a feature subset selection [20] that is computational very expensive as there usually exist many possible shapelet candidates.\nIn Section 4.2 it will be shown that most shapelet discovery methods are nothing else but feature subset selection algorithms. An alternative way which is faster and is based on feature sampling will be presented in Section 4.3. Subsequently, Section 4.4 introduces a way of generating features from multivariate time series using subsequences. Finally, Section 4.5 shows how to integrate derivatives of time series to the proposed concept of Ultra-Fast Shapelets."}, {"heading": "4.1. Notation", "text": "A univariate time series T = (t1, . . . , tm) is a sequence of m data points, ti \u2208 R, where m is called the length of the time series. For notational and illustratic convenience, it is assumed that the length of each time series is the same, although the presented methods can handle time series of varying length.\nA multivariate time series T = (t1, . . . , tm) is a sequence ofm vectors ti = (ti,1, . . . , ti,s) \u2208 Rs with s streams where the time series Tj = (t1,j , . . . , tm,j) is called a stream.\nA dataset for univariate or multivariate time series classification is a pair T = ( (T1, . . . , Tn) T , Y )\nwhere Y = (y1, . . . , yn) \u2208 {1, . . . , C}n and the class of time series Ti is yi. The classification task is now to use T to predict the correct classes for further, unseen time series.\nA shapelet for a dataset T is a time series of length l \u2264 m which is discriminative with respect to the target regarding to a scoring function s : span (T )\u00d7Rl \u2192 R, where s is usually the information gain."}, {"heading": "4.2. Shapelet Discovery as Feature Subset Selection", "text": "In this section the relationship between shapelet discovery and feature subset selection is depicted. This relationship holds only for those methods that choose subsequences as shapelets that occur in the training data which is common in many methods [2, 5, 8, 6, 1]. For a better understanding, Shapelet Transformation is described in Section 4.2.1 and shapelet discovery in Section 4.2.2."}, {"heading": "4.2.1. Shapelet Transformation for Univariate Time Series", "text": "The term Shapelet Transformation was introduced by Lines et al. [8]. Shapelet Transformation is a feature extraction process which uses and extracts shapelets which are discriminative subsequences. This idea is used for univariate time series classification latest since Ye et al. [1]. The extracted shapelets are used to express the original time\nseries dataset T = ( (T1, . . . , Tn) T , Y )\nas a shapelet transformed dataset D = (X,Y ), X \u2208 Rn\u00d7p which can be done in four steps (see Figure 2):\n1. Candidate Extraction: From all time series T1, . . . , Tn subsequences C1, . . . , Cq are selected to be candidates. There exist different methods to choose the candidates. Typically, all subsequences of a specific length [2, 6, 1] or that fulfill another informed criterion [5] are chosen.\n2. Subsequence Transformation: Using the q candidates, the raw data will be transformed. Each candidate C = (c1, . . . , cl) is a predictor in the new representation\n0 50 100 150\nTime Series Shapelet\nbest matching position\nAfter the completion of the Shapelet Transformation, there are no restrictions whatsoever for a classifier."}, {"heading": "4.2.2. Shapelet Discovery is Variable Ranking", "text": "The last section has shown how to apply Shapelet Transformation on univariate time series. Now, the relation between shapelet discovery and feature subset selection will be discussed. Therefor, the definition that is used for a shapelet is recited here.\nDefinition 1. Given is a univariate time series dataset T and a scoring function s that ranks a subsequence of a time series T according to T . Then, a shapelet S is defined as a subsequence of a time series T \u2208 T which is among the p highest ranked subsequences regarding to the scoring function s.\nIn the literature this kind of feature subset selection is called variable ranking and is a filter method [20]. Filter methods have the advantage of being computational fast and\nscalable but have the disadvantages of choosing redundant features and ignore dependencies among features and to the classifier. However, even though variable ranking is one of the fastest feature selection methods, it is still slow for shapelet discovery since the O ( nm2 ) features, where n is the number of time series instances and m is the length of a time series, are not given beforehand. First, the minimal distance (Equation 2) needs to be computed before the quality can be assessed."}, {"heading": "4.3. Shapelet Discovery as Feature Sampling", "text": "Here, a new way of shapelet discovery is proposed. Instead of considering all candidates and applying subsequence transformation and variable ranking (Shapelet Transformation, see Figure 2), the predictors are chosen at random. This idea is sketched in Figure 5. Comparing Figures 2 and Figure 5, one can see that the difference is twofold. For the candidate extraction step a random subset of candidates is chosen and steps 2 and 3 of the Shapelet Transformation, i.e. the feature subset selection, are omitted. From now on, this way of selecting subsequences will be called Ultra-Fast Shapelets.\nThe GunPoint dataset will be used to motivate the idea of Ultra-Fast Shapelets. The GunPoint dataset is an activity recognition dataset where the task is to distinguish whether a human being is lifting his arm to point at something or whether he is drawing a weapon. Example instances of this dataset are shown in Figure 4. This figure also shows the shapelets found using the variable ranking method which are considered to be useful for classification [2]. First, the question is whether similar good subsequences can be found if randomly subsequences of arbitrary length are considered and second, if useless subsequences are a problem. Lets stick to the GunPoint dataset and assume that there are only the two clusters of shapelets shown in Figure 4. Obviously, it is enough to find just one representative for each shapelet cluster since they are redundant and do not give further information. Since they are typical for the data, it is assumed that one out of the two shapelets is found in each time series. The GunPoint dataset contains\n50 training instances and hence approximately 50 shapelets. For this dataset 1% of the subsequences are chosen at random. Hence, the probability for finding a representative for both shapelets clusters is quite unlikely. Nevertheless, subsequences within the same interval as one of the shapelets which are a bit longer or shorter are obviously also very good predictors.\nIn the example, all shapelets are of length 40 but one can assume that subsequences of lengths between 35 and 45 in the same time interval have similar predictive quality. Thus, there are not 50 candidates but 1,230. If one agrees that this holds, the probability of selecting a subsequence which is very close to one of the shapelets discovered by variable ranking is very high. At this point, the reader may have already noticed that there are further discriminative parts that might have not been considered by the variable ranking. Since only the best p features are considered in the variable ranking method and since there are many similar and hence equally high ranked features, the effective number of features in the end can be very small. If p is not chosen high enough, discriminative parts like in the dashed, orange plots around time 50 and in the dotted, blue plots around time 110 (Figure 4) are not considered.\nFinally, it is more likely that Ultra-Fast Shapelets finds interacting features than variable ranking. Lets think about a dataset with two classes (Figure 6). The one class has either only noise or at least one \u2228-shaped and one \u2227-shaped subsequence which are interrupted by arbitrary long subsequences of noise. The other class either has only subsequences of type \u2228 divided by subsequences of noise and none of type \u2227 or vice versa. If features are extracted by variable ranking, the subsequences \u2228 and \u2227 will have a bad score since the accuracy of each of them alone is about 50% which is as good as random. Therefor, they will not be selected and no good classifier can be trained. On the other hand, Ultra-Fast Shapelets will choose some of them as discussed before and a perfect classifier can be trained.\nConcluding, time series contain many duplicates of subsequences where some of them are useful and some are not. This motivates the idea of randomly selecting subsequences. A well regularized classifier can then be used to identify useful subsequences and their interactions."}, {"heading": "4.4. Generalized Ultra-Fast Shapelets", "text": "This section describes how Ultra-Fast Shapelets can be generalized such that the implementation can be used for univariate and multivariate time series classification. The proposition is to transform multivariate time series using shapelets in a similar way as described for the univariate case before. In the first step, shapelets are chosen at random from each of the s streams such that sets of shapelets S1, . . .Ss are extracted. Ignoring temporal relations between the streams, the raw data is transformed into a dataset D such that the features are generated on each stream as in the univariate case and then concatinated. Formally, using the sets Si of size ps , the dataset D = (X,Y ), X \u2208 R n\u00d7p\nis computed where xi,j = minDist ( Sj\u2212(dj spe\u22121) ps , Ti,dj spe ) and Sj\u2212(dj spe\u22121) ps \u2208 Sdj spe.\nAlgorithm 1 explains this process in detail. In line 2, the ratio f between the number of features p and the number of all subsequences of arbitrary length is computed. In lines 4-9, subsequences of different length are extracted uniformly at random. After extracting them, the subsequence-transformed dataset is estimated by computing the distance of a subsequence to each time series in lines 12-16. For the distance function in line 16 we have chosen one of the distance functions defined in Equations 1 and 2.\nFor the experiments on the multivariate datasets, a different distance function between a shapelet and a time series is considered because it provides better results for some datasets: the minimal Euclidean distance between a shapelet and the unnormalized time series\nminDist (S, T ) = min i=1,...,m\u2212l+1  \u221a\u221a\u221a\u221a l\u2211\nj=1\n(sj \u2212 ti+j)2  . (2)\nAlgorithm 1 Generalized Ultra-Fast Shapelets Input: Dataset T = ( (T1, . . . , Tn) T , Y ) , Ti \u2208 Rm\u00d7s, number of features p, c1, . . . , cm\nwhere ci is the number of all subsequences of length i Output: Subsequence transformed dataset (X,Y )\n1: . Choose p subsequences at random 2: f \u2190 p ( \u2211m\nl=3 cl) \u22121\n3: subsequences \u2190 \u2205 4: for l = 3 to m do 5: for k = 1 to s do 6: for round (f \u00b7 cl) times do 7: i\u2190 U (1, n) 8: j \u2190 U (1,m\u2212 l) 9: subsequences \u2190 subsequences \u222a{(i, s, j, l)}\n10: 11: . Generate the transformed dataset using T and the subsequences 12: X \u2208 Rn\u00d7p 13: for j = 1 to p do 14: (i\u2032, s, j\u2032, l)\u2190 subsequences.get(j) 15: for i = 1 to n do 16: xi,j \u2190 dist ((ti\u2032,s,j\u2032 , . . . ti\u2032,s,j\u2032+l) , Ti,s) 17: return (X,Y )"}, {"heading": "4.5. Considering Derivatives of Time Series", "text": "In the next section, Ultra-Fast Shapelets is compared to other state-of-the-art methods for multivariate time series classification. To ensure a fair comparison, the same classification model and the same features shall be used. Since SMTS [18] is not only using the raw time series but also the derivative of it, it is now explained how derivatives can be used for Ultra-Fast Shapelets.\nThe derivative of a time series T = (t1, . . . , tm) is defined as\u2207T = (0, t2 \u2212 t1, . . . , tm \u2212 tm\u22121). The leading 0 ensures that the derivative has the same length as its time series. The use of derivatives of time series is straight forward for Ultra-Fast Shapelets. For each stream Ti of a time series T a new stream \u2207Ti is added which is the derivative of Ti. The original dataset is now twice that large but no adaption to Algorithm 1 needs to be done. An example for a time series derivative is given in Figure 7."}, {"heading": "5. Experiments", "text": "In Section 3, the baselines are summarized in order to make this work self-contained. Section 5.1 provides a detailed description of the experimental setup, explaining which hyperparameters are used and how they have been found for every algorithm.\nOne claim is that sampling of shapelets is in general a scalable way to extract features for time series classification which is also accurate. Therefor, experiments are conducted on 52 univariate datasets from various domains such as speech recognition, activity recognition, medicine, image classification and several more. These datasets are downloaded\nfrom [21, 22] and have different properties such as number of training instances, classes and length (see Table 1). In Section 5.2, Ultra-Fast Shapelets (UFS) is compared to various shapelet-based classifiers and its ability to compete is demonstrated. Furthermore, the runtime is compared and it is shown that Ultra-Fast Shapelets is indead faster.\nFinally, in Section 5.4, UFS is compared to state-of-the-art classifiers for multivariate time series on 15 datasets from different domains such as speech, gesture, motion, handwriting and sign language recogniton provided by [23]. The number of streams varies between 2 and 62, the number of classes between 2 and 95. In contrast to the univariate datasets, some of the multivariate datasets do not have a fixed length per instance per dataset. A detailed description of the datasets is given in Table 5."}, {"heading": "5.1. Experimental Setup", "text": "Ultra-Fast Shapelets (UFS) has only a single hyperparameter that is the percentage of considered candidates. Instead of tuning the percentage hyperparameter carefully for all 67 datasets, the highest non-positive power of 10 was chosen such that the number of expected candidates is smaller than 10,000 per stream. For all univariate datasets the normalized distance function defined in Equation 1 was used, for the multivariate datasets this decision was added to the hyperparameter search and then either the distance function defined in Equation 1 or 2 was used.\nOn the transformed data any classifier may be trained. Random Forest and a linear SVM were chosen because the results for SMTS are reported using a Random Forest and Learning Shapelets is using a linear classifier.\nA random forest has three hyperparameters, i.e. the number of trees J , the number of sampled features v, and the depth d of each tree. A gridsearch was applied on J \u2208 {20, 50, 100, 500, 1000}, v \u2208 {blog (p+ 1)c , p} and d \u2208 {5, . . . , 2 log (p)} and evaluated against the out-of-bag error (OOE). The model with smallest OOE was used to classify the test instances. In all experiments the mean over ten repetitions is reported. In case that there was more than one model with the smallest OOE, one was taken at random.\nA linear SVM has usually only one hyperparameter, i.e. the regularization term C. Another hyperparameter was added that decides whether to use a L1 or L2-regularized SVM. A gridsearch was applied on C \u2208 { 21, . . . , 210 } and both regularization methods.\nThe best combination in a 5-fold cross-validation was used to report the error on test. Again, in all experiments the mean over ten replications is reported.\nThe results of SMTS, nearest neighbor with dynamic time warping distance without a warping window (NNDTW) and a multivariate extension of TSBF [24] (MTSBF) were taken from Baydogan et al. [18]. Results for the other baselines were redone since the authors only report the accuracy on one run and otherwise the runtime comparison would have not been fair. For the experiments, the implementation provided by the original authors were taken and the hyperparameters were used as proposed by the authors. Because of this and the fact that some datasets are too large, not every baseline is evaluated on every dataset but this shall ensure that the best hyperparameters are chosen. Again, the average over ten replications is reported for every dataset."}, {"heading": "5.2. Univariate Datasets", "text": "To support the claim that Ultra-Fast Shapelets is comparably to state-of-the-art shapelet-based classifiers with respect to accuracy but faster, experiments are conducted on 52 univariate time series datasets and compared to three different methods of shapelet discovery using various classifiers. Ultra-Fast Shapelets (UFS) using Random Forest (RF) and a linear SVM (SVM) is compared to the exhaustive search of shapelets (ES) using RF and SVM, to Fast Shapelets (FS) and to Learning Shapelets (LS). These four shapeletbased methods are compared based on classification error in Section 5.2.1 and runtime in Section 5.2.2. The experiments show that UFS is as good as state-of-the-art shapeletbased classifiers but scale to large data and therefor can be used for both, univariate and multivariate time series classification, with decent accuracy and runtime. More detailed information about the baselines is given in Section 3.\nThe datasets are provided by the UCR time series database [22] and by Bagnall et al. [21]. Table 1 contains few statistics about the datasets, further information can be found on the corresponding websites [21, 22]."}, {"heading": "5.2.1. Classification Accuracy", "text": "In this section the results of an empirical comparison on 52 datasets of various domains for different methods of shapelet discovery are presented. Detailed results for each dataset are shown in Table 2.\nThe different methods can be divided into two groups depending on which kind of classifier they use. The methods that are using a linear SVM (UFS (SVM), ES (SVM), LS) yield better results than those which are using a non-linear, tree-based classifier. The tree-based classifiers are only in 9 datasets better than the SVM which confirms the results by Hills et al. [2]. Overall, FS is the worst classifier which is not surprising because it is an approximation of ES. Comparing UFS with ES by classifier, the performance using a SVM is almost similar. ES has better prediction quality on 10 datasets, UFS on 15. UFS using a random forest shows better performance in 19 datasets and is worse than ES in 7. These results show a strong empirical evidence that simply choosing subsequences at random will not deteriorate the accuracy in comparison to more informative methods like ES or FS.\nWhile LS achieves better accuracy than UFS, it has a higher runtime performance by an average of three orders of magnitude and is the slowest among all investigated methods, as shown in the next section. In that context, the proposed method (UFS)\nis more scalable than LS for large datasets. In terms of hyperparameters, LS requires three sensitive hyperparameters while our proposed method has only a single insensitive hyperparameter (see Section 5.3). This excludes the classifier-specific hyperparameters for both methods. Finally, one of the reasons that LS is more accurate is that it is not limited in the number of candidates. While ES, FS and UFS can only choose among subsequences that appear in the training set, LS can choose any subsequence."}, {"heading": "5.2.2. Runtime Analysis", "text": "This section shows that Ultra-Fast Shapelets (UFS) is not only as accurate as stateof-the-art methods but also significantly faster which makes it finally feasible to apply shapelets on multivariate time series datasets. Therefor, the four shapelet-based methods are compared empirically and theoretically.\nStarting with the empirical runtime analysis, Table 4 provides the measured runtime in seconds averaged over 10 replications for each method on 52 datasets. The subsequence lengths that are considered are those proposed by the authors. This means that only UFS and Fast Shapelets (FS) are considering all candidates while the exhaustive search (ES) and Learning Shapelets (LS) only consider a subset of them. Knowing this, note that this is obviously an advantage for ES and LS. Nevertheless, FS and UFS are significantly faster. UFS is slow compared to the baselines for the small datasets but in general faster which means that it is faster than the fastest shapelet discovery method so far (FS) in 45 out of 52 datasets, in some even by a factor of 100.\nThe number of all subsequences respectively candidates in a time series dataset with n instances of length m is c = O ( nm2 ) . Since the exhaustive search compares each possible\npair of candidates of equal length, the runtime is O ( n2m4 ) . FS reduces the number of\ncandidates to a subset r < c which is then compared with all O ( nm2 ) candidates of equal\nlength so that FS needs time O ( rnm2 ) . LS [9] reports a runtime of O ( ipnm2 ) where i is the number of iterations used until the algorithm converges and p is the number of shapelets that have to be found. Since the authors propose a very high number of iterations and shapelets for some datasets, a comparably high runtime is observed. Finally, UFS chooses randomly a subset of p candidates, where for the here executed experiments p < 10, 000. Thus, the runtime is O ( pnm2 ) and p < r for the larger datasets but p \u2248 nm2 for the very small datasets which explains the high runtime for those. Since the number of candidates p is not optimized but a rule of thumb was used, one can further improve the speed without a loss of accuracy as shown in Section 5.3. Theoretical and empirical results are summarized in Table 3."}, {"heading": "5.3. Hyperparameter Sensitivity", "text": "Ultra-Fast Shapelets has only a single hyperparameter, i.e. the number of chosen subsequences plus those needed for the chosen classifier. This section is devoted to two questions: i) is Ultra-Fast Shapelets sensitive to the hyperparameter and is it easy to find an optimal one and ii) is Ultra-Fast Shapelets better because it uses more features respectively does it help the baselines if they also use more features. These two question will be answered exemplarily at four random datasets. Figure 8 shows the accuracy and the time needed for Ultra-Fast Shapelets (UFS) and the exhaustive search (ES) using a linear SVM. The measured time contains the time for discovering the shapelets as well as training the model.\nT a b\nle 2 :\nT es\nt er\nro r\nra te\ns a v er\na g ed\no v er\n1 0\nre p\nli ca\nti o n\ns fo\nr d\niff er\nen t\nu n\niv a ri\na te\nti m\ne se\nri es\ncl a ss\nifi ca\nti o n\nm et\nh o d\ns co\nm p\na re\nd to\nU lt\nra -F\na st\nS h\na p\nel et\ns o n\n5 2\nd a ta\nse ts\n.\nD at\nas et\nU F S (S\nV M\n) E\nS (S\nV M\n) L\nS U\nF S\n(R F\n) E\nS (R\nF )\nF S\nD a ta\nse t\nU F S (S\nV M\n) U\nF S\n(R F\n) F\nS\nA d\nia c\n0 .2\n7 6\n0. 75\n7 0.\n5 8 1\n0 .2\n8 1\n0 .7\n0 4\n0 .4\n4 2\n5 0 w\no rd\ns 0 .1\n8 4\n0 .2\n7 8\n0 .4\n9 7\nB ee\nf 0.\n26 3\n0 .1\n0 0\n0. 19\n3 0 .4\n2 3\n0 .3\n6 0\n0 .4\n8 3\nC B\nF 0 .0\n0 2\n0 .0\n1 4\n0 .0\n6 2\nC h\nlo ri\nn eC\non ce\nn tr\nat io\nn 0 .2\n6 2\n0. 42\n9 0.\n2 7 0\n0 .3\n5 7\n0 .3\n3 8\n0 .4\n2 2\nC in\nC E\nC G\nto rs\no 0 .1\n5 1\n0 .1\n9 9\n0 .2\n7 4\nC off\nee 0.\n01 1\n0 .0\n0 0\n0 .0\n0 0\n0 .0\n5 4\n0 .0\n6 1\n0 .0\n7 5\nC ri\nck et\nX 0 .2\n2 5\n0 .2\n9 4\n0 .5\n3 8\nD ia\nto m\nS iz\neR ed\nu ct\nio n\n0. 07\n5 0.\n08 2\n0 .0\n5 8\n0 .0\n8 4\n0 .2\n1 8\n0 .1\n3 1\nC ri\nck et\nY 0 .2\n3 4\n0 .2\n7 9\n0 .5\n0 6\nD P\nL it\ntl e\n0. 29\n3 0.\n32 3\n0 .2\n5 7\n0 .3\n1 5\n0 .3\n6 4\n0 .4\n3 4\nC ri\nck et\nZ 0 .2\n0 7\n0 .2\n4 8\n0 .5\n6 1\nD P\nM id\nd le\n0. 28\n8 0.\n30 5\n0 .2\n7 1\n0 .2\n8 8\n0 .3\n8 4\n0 .4\n1 9\nE C\nG 2 0 0\n0 .1\n1 9\n0 .1\n8 7\n0 .2\n3 0\nD P\nT h u\nm b\n0. 29\n1 0.\n30 4\n0 .2\n6 4\n0 .3\n0 2\n0 .3\n6 6\n0 .4\n2 0\nF a ce\nA ll\n0 .2\n1 2\n0 .2\n6 0\n0 .3\n6 6\nE C\nG F\niv eD\nay s\n0 .0\n0 0\n0. 00\n1 0 .0\n0 0\n0 .0\n0 7\n0 .0\n1 3\n0 .0\n0 4\nF a ce\nsU C\nR 0 .0\n4 3\n0 .0\n9 0\n0 .3\n1 0\nF ac\neF ou\nr 0.\n03 9\n0. 02\n3 0 .0\n1 9\n0 .0\n4 3\n0 .2\n2 6\n0 .0\n9 2\nF IS\nH 0 .0\n3 5\n0 .1\n0 5\n0 .1\n8 0\nG u\nn P\noi n t\n0. 02\n1 0 .0\n0 0\n0. 00\n6 0 .0\n1 1\n0 .0\n4 6\n0 .0\n6 8\nH a p\nti cs\n0 .5\n0 9\n0 .5\n2 9\n0 .6\n1 1\nIt al\ny P\now er\nD em\nan d\n0. 03\n8 0.\n06 5\n0 .0\n3 6\n0 .0\n5 2\n0 .0\n7 2\n0 .0\n7 9\nIn li\nn eS\nka te\ns 0 .6\n1 8\n0 .5\n7 3\n0 .7\n3 8\nL ig\nh ti\nn g7\n0 .2\n2 2\n0. 34\n2 0.\n3 7 0\n0 .2\n6 4\n0 .3\n6 3\n0 .3\n9 9\nL ig\nh ti\nn g 2\n0 .2\n1 3\n0 .2\n3 6\n0 .3\n1 6\nM ed\nic al\nIm ag\nes 0 .2\n6 0\n0. 47\n7 0.\n3 3 6\n0 .3\n0 0\n0 .5\n1 7\n0 .3\n9 2\nM A\nL L\nA T\n0 .0\n4 4\n0 .0\n2 1\n0 .0\n5 8\nM ot\neS tr\nai n\n0. 13\n4 0.\n11 9\n0 .0\n9 9\n0 .1\n3 1\n0 .1\n4 8\n0 .2\n1 6\nO li\nve O\nil 0 .1\n1 0\n0 .1\n2 3\n0 .2\n9 2\nM P\nL it\ntl e\n0. 28\n3 0.\n29 3\n0 .2\n6 3\n0 .2\n6 8\n0 .3\n3 6\n0 .4\n3 5\nO S\nU L\nea f\n0 .1\n5 8\n0 .2\n4 5\n0 .3\n2 3\nM P\nM id\nd le\n0. 25\n2 0 .2\n3 4\n0. 24\n1 0 .2\n3 5\n0 .3\n0 3\n0 .3\n9 5\nS o n y A\nIB O\nR o b\no tS\nu rf\na ce\nII 0 .0\n8 8\n0 .1\n2 1\n0 .2\n0 9\nO to\nli th\ns 0.\n43 4\n0. 35\n9 0 .3\n1 9\n0 .4\n3 6\n0 .3\n6 4\n0 .4\n4 0\nS ta\nrL ig\nh tC\nu rv\nes 0 .0\n3 1\n0 .0\n2 9\n0 .0\n5 4\nP P\nL it\ntl e\n0. 31\n6 0 .2\n7 9\n0. 29\n7 0 .3\n4 0\n0 .3\n2 8\n0 .4\n5 1\nS w\ned is\nh L\nea f\n0 .0\n6 5\n0 .0\n9 4\n0 .2\n2 5\nP P\nM id\nd le\n0 .2\n5 6\n0. 28\n7 0.\n2 5 8\n0 .2\n6 3\n0 .3\n2 9\n0 .4\n2 0\nT w\no P\na tt\ner n\ns 0 .0\n0 3\n0 .0\n0 1\n0 .0\n7 9\nP P\nT h u\nm b\n0 .3\n0 4\n0. 30\n9 0 .3\n0 4\n0 .3\n4 2\n0 .3\n9 1\n0 .4\n6 5\nu W\nav eG\nes tu\nre L\nib ra\nry X\n0 .1\n9 7\n0 .1\n9 1\n0 .2\n9 7\nS on\ny A\nIB O\nR ob\not S\nu rf\nac e\n0. 12\n2 0 .0\n6 0\n0. 09\n0 0 .1\n0 9\n0 .0\n8 0\n0 .3\n0 2\nu W\nav eG\nes tu\nre L\nib ra\nry Y\n0 .2\n9 1\n0 .2\n8 4\n0 .3\n8 6\nS y m\nb ol\ns 0.\n11 1\n0. 16\n7 0.\n0 6 9\n0 .0\n6 7\n0 .1\n7 6\n0 .0\n7 0\nu W\nav eG\nes tu\nre L\nib ra\nry Z\n0 .2\n7 1\n0 .2\n3 8\n0 .3\n6 3\nsy n th\net ic\nco n tr\nol 0 .0\n0 6\n0. 09\n7 0.\n0 2 1\n0 .0\n0 9\n0 .0\n8 1\n0 .0\n8 3\nw a fe\nr 0 .0\n0 3\n0 .0\n0 8\n0 .0\n0 2\nT ra\nce 0 .0\n0 0\n0 .0\n0 0\n0 .0\n0 0\n0 .0\n0 8\n0 .0\n0 0\n0 .0\n0 0\nW o rd\nsS y n\no n y m\ns 0 .2\n6 9\n0 .3\n7 6\n0 .5\n6 9\nT w\noL ea\nd E\nC G\n0. 02\n5 0 .0\n0 0\n0. 00\n2 0 .0\n7 5\n0 .0\n6 4\n0 .7\n4 1\nyo g a\n0 .1\n2 9\n0 .1\n5 0\n0 .2\n9 4\nFirst of all, it seems that, even if ES uses as many features as UFS, the accuracy does not improve. Actually, with increasing number of shapelets, the accuracy improves until it converges at some point which makes it easy to find the best number of shapelets for both methods. ES profits from the supervised features selection and tends to need less features to achieve its best accuracy. The random subsequence selection of UFS tends to result in the need of more features but needs orders of magnitudes less time to find them. Finally, if the results for UFS in Figure 8 are compared to the reported results in\nTable 2 and 4 it is clear that a hyperparameter search will further improve the runtime without harming the accuracy."}, {"heading": "5.4. Multivariate Datasets", "text": "The 15 multivariate datasets used to evaluate Ultra-Fast Shapelets are from various domains such as sign language recognition (AUSLAN), handwriting recognition (CharacterTrajectories, PenDigits), motion (CMU MOCAP S16), gesture (uWaveGestureLibrary) and speech recognition (ArabicDigits, JapaneseVowels). Detailed characteristics of the datasets are given in Table 5, detailed results are presented in Table 7. A summary of the empirical evaluation is given in Table 6.\nT a b\nle 4 :\nM ea\nsu re\nd ti\nm e\nin se\nco n\nd s\nfo r\nle a rn\nin g\nth e\nm o d\nel a n\nd d\nis co\nv er\nin g\nth e\nsh a p\nel et\ns a v er\na g ed\no v er\n1 0\nre p\nli ca\nti o n\ns fo\nr d\niff er\nen t\nsh a p\nel et\n-b a se d u n iv a ri a te ti m e se ri es cl a ss ifi ca ti o n m et h o d s co m p a re d to U lt ra -F a st S h a p el et s o n 5 2 d a ta se ts . D at as et U F S F S L S E S D a ta se t U F S F S\nA d\nia c\n4 5 .8\n33 2.\n6 20\n38 4 7 .0\n5 5 2 2 .6\n5 0 w\no rd\ns 2 3 .2\n2 1 9 8 .1\nB ee\nf 8 .3\n19 6.\n9 25\n06 3 .5\n2 1 4 8 .0\nC B\nF 8 .7\n1 0 .9\nC h\nlo ri\nn eC\non ce\nn tr\nat io\nn 2 7 1 .9\n76 0.\n3 30\n74 3 .7\n1 3 7 5 6 .5\nC in\nC E\nC G\nto rs\no 3 2 0 1 .6\n4 3 9 8 .9\nC off\nee 1 .6\n22 .5\n37 2.\n6 2 0 4 .7\nC ri\nck et\nX 2 5 .7\n3 7 5 6 .0\nD ia\nto m\nS iz\neR ed\nu ct\nio n\n63 .4\n1 5 .6\n14 18\n4 .1\n9 4 .4\nC ri\nck et\nY 2 6 .6\n3 6 0 5 .7\nD P\nL it\ntl e\n1 7 .1\n97 5.\n1 83\n84 .8\n6 6 5 7 0 .1\nC ri\nck et\nZ 2 5 .9\n4 6 7 9 .2\nD P\nM id\nd le\n1 7 .3\n99 6.\n1 23\n87 5 .4\n1 4 3 0 4 1 .5\nE C\nG 2 0 0\n2 .7\n1 6 .3\nD P\nT h u\nm b\n1 7 .9\n91 6.\n3 91\n07 .0\n1 0 9 3 8 8 .6\nF a ce\nA ll\n5 8 .8\n7 5 7 .5\nE C\nG F\niv eD\nay s\n10 .9\n3 .6\n59 .2\n1 2 2 .0\nF a ce\nsU C\nR 1 9 .1\n2 8 0 .3\nF ac\neF ou\nr 5 .6\n10 2.\n9 11\n07 8 .3\n4 3 8 4 .8\nF IS\nH 2 4 4 .4\n9 3 5 .6\nG u\nn P\noi n t\n0 .7\n9. 5\n14 75\n.3 4 7 4 .1\nH a p\nti cs\n8 1 8 .0\n1 2 4 9 1 .0\nIt al\ny P\now er\nD em\nan d\n1. 8\n0 .4\n20 .8\n1 .5\nIn li\nn eS\nka te\n6 3 7 .0\n2 2 6 7 7 .2\nL ig\nh ti\nn g7\n1 0 .1\n32 2.\n8 39\n02 .6\n1 3 4 0 9 .5\nL ig\nh ti\nn g 2\n1 1 .1\n1 1 3 1 .3\nM ed\nic al\nIm ag\nes 7 .4\n37 1.\n5 64\n57 .7\n1 1 0 8 4 .6\nM A\nL L\nA T\n1 2 1 .7\n1 7 3 6 .5\nM ot\neS tr\nai n\n17 .2\n3 .1\n41 0.\n5 7 .8\nO li\nve O\nil 1 6 .8\n1 0 7 .2\nM P\nL it\ntl e\n1 8 .1\n99 6.\n5 55\n96 4 .1\n7 5 6 6 6 .3\nO S\nU L\nea f\n2 6 .5\n1 6 2 9 .7\nM P\nM id\nd le\n1 7 .0\n99 3.\n3 67\n97 .5\n1 1 7 0 5 6 .5\nS o n y A\nIB O\nR o b\no tS\nu rf\na ce\nII 8 .4\n1 .3\nO to\nli th\ns 4 7 .5\n30 3.\n2 94\n53 .2\n4 6 5 4 6 .0\nS ta\nrL ig\nh tC\nu rv\nes 8 2 7 8 .9\n2 1 4 7 3 .5\nP P\nL it\ntl e\n1 7 .3\n91 8.\n8 27\n69 9 .3\n6 7 9 4 4 .0\nS w\ned is\nh L\nea f\n2 3 .5\n4 5 1 .7\nP P\nM id\nd le\n1 7 .0\n95 0.\n2 35\n42 5 .6\n5 2 2 8 5 .4\nT w\no P\na tt\ner n\ns 1 4 7 .0\n9 5 7 .2\nP P\nT h u\nm b\n1 7 .5\n94 3.\n7 67\n79 8 .2\n7 7 4 0 2 .3\nu W\nav eG\nes tu\nre L\nib ra\nry X\n3 4 4 .6\n4 8 2 7 .5\nS on\ny A\nIB O\nR ob\not S\nu rf\nac e\n4. 9\n1 .1\n11 5.\n0 8 .5\nu W\nav eG\nes tu\nre L\nib ra\nry Y\n3 4 8 .9\n4 3 7 9 .6\nS y m\nb ol\ns 5 2 .0\n93 .0\n11 04\n.3 1 2 5 1 1 .4\nu W\nav eG\nes tu\nre L\nib ra\nry Z\n4 9 7 .0\n5 2 1 5 .9\nsy n th\net ic\nco n tr\nol 5 .6\n63 .9\n78 3.\n3 1 2 8 5 .2\nw a fe\nr 5 6 .6\n1 9 0 .5\nT ra\nce 1 1 .2\n18 1.\n0 11\n56 1 .9\n4 7 6 6 5 .3\nW o rd\nsS y n\no n y m\ns 1 2 1 .8\n1 1 4 0 .0\nT w\noL ea\nd E\nC G\n16 .3\n1 .3\n45 .6\n3 .5\nyo g a\n2 7 1 .7\n1 7 1 1 .6\nThere exist two variations of Ultra-Fast Shapelets: Ultra-Fast Shapelets with (\u2207UFS) and without time series derivatives (UFS). Time series derivatives were added since Symbolic Representation for Multivariate Timeseries (SMTS) are using them by default. For SMTS it was shown that this additional information improves the accuracy. Time series derivatives can improve the accuracy for UFS in some cases. In some they do not help, in some they may deteriorate the accuracy since they are adding less helpful predictors. Nevertheless, UFS with derivatives is in 11 out of 15 cases better than without derivatives.\nUFS with and without derivatives outperforms the baselines. \u2207UFS is for 11, UFS for 10 out of 15 datasets better than SMTS. For NNDTW the number of cases is even 14 or 13, respectively. Comparing UFS to MTSBF, UFS is better in 5 or 6 out of 7 datasets, depending whether time series derivatives are used or not.\nConcluding, UFS with derivatives is more accurate than without. UFS is better than SMTS even if no time series derivatives are used and MTSBF has a similar prediction accuracy as SMTS. NNDTW is the worst of the compared classification methods."}, {"heading": "6. Conclusions", "text": "We proposed an ultra-fast way of extracting shapelets motivated by the knowledge of redundant subsequences. Because the shapelet discovery by most authors so far is nothing but a feature subset selection which is costly in time, it was not surprising that our method, Ultra-Fast Shapelets, reduces the runtime by order of magnitudes and is, to the best of our knowledge, the fastest so far published shapelet discovery method.\nFurthermore, the ultra-fast shapelet discovery method enabled us to apply shapeletbased classifiers on long univariate datasets as well as on multivariate time series. We compared UFS on 52 univariate datasets with current state-of-the-art shapelet-based methods and showed empirically that it is competitive in terms of accuracy. Additionally, a comparison to state-of-the-art methods for multivariate time series classification on 15 datasets from various domains has shown that Ultra-Fast Shapelets creates predictive features. A Random Forest classifier was better with shapelet features in 11 out of 15 cases compared to SMTS features."}, {"heading": "7. Acknowledgements", "text": "Partially co-funded by the Seventh Framework Programme of the European Comission, through project REDUCTION (# 288254). www.reduction-project.eu"}], "references": [{"title": "Time Series Shapelets: A New Primitive for Data Mining", "author": ["L. Ye", "E. Keogh"], "venue": "in: Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201909, ACM, New York, NY, USA", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Classification of time series by shapelet transformation", "author": ["J. Hills", "J. Lines", "E. Baranauskas", "J. Mapp", "A. Bagnall"], "venue": "Data Mining and Knowledge Discovery 28 (4) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Clustering Time Series Using Unsupervised-Shapelets", "author": ["J. Zakaria", "A. Mueen", "E. Keogh"], "venue": "in: Proceedings of the 2012 IEEE 12th International Conference on Data Mining, ICDM \u201912, IEEE Computer Society, Washington, DC, USA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Z", "author": ["M.F. Ghalwash"], "venue": "Obradovic, Early classification of multivariate temporal observations by extraction of interpretable shapelets., BMC Bioinformatics 13 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "T", "author": ["E.J. Keogh"], "venue": "Rakthanmanon, Fast Shapelets: A Scalable Algorithm for Discovering Time Series Shapelets., in: SDM, SIAM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Logical-shapelets: An Expressive Primitive for Time Series Classification", "author": ["A. Mueen", "E. Keogh", "N. Young"], "venue": "in: Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201911, ACM, New York, NY, USA", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient Pattern-Based Time Series Classification on GPU", "author": ["K.-W. Chang", "B. Deka", "W.-M.W. Hwu", "D. Roth"], "venue": "2013 IEEE 13th International Conference on Data Mining 0 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "A Shapelet Transform for Time Series Classification", "author": ["J. Lines", "L.M. Davis", "J. Hills", "A. Bagnall"], "venue": "in: Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201912, ACM, New York, NY, USA", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning Time-Series Shapelets", "author": ["J. Grabocka", "N. Schilling", "M. Wistuba", "L. Schmidt-Thieme"], "venue": "in: Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "N", "author": ["B. Hartmann"], "venue": "Link, Gesture recognition with inertial sensors and optimized DTW prototypes., in: SMC, IEEE", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Human Gait Recognition and Classification Using Time Series Shapelets, 2012", "author": ["S. T", "P.B. Sivakumar"], "venue": "International Conference on Advances in Computing and Communications", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Fast Time Series Classification Based on Infrequent Shapelets", "author": ["Q. He", "Z. Dong", "F. Zhuang", "T. Shang", "Z. Shi"], "venue": "in: ICMLA (1)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "L", "author": ["D. Gordon", "D. Hendler"], "venue": "Rokach, Fast randomized model generation for shapelet-based time series classification ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "B", "author": ["C. Li", "L. Khan"], "venue": "Prabhakaran, Real-time classification of variable length multi-attribute motions., Knowl. Inf. Syst. 10 (2) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Classification of multivariate time series using locality preserving projections", "author": ["X. Weng", "J. Shen"], "venue": "Knowledge-Based Systems 21 (7) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Accelerometer-based gesture recognition via dynamic-time warping", "author": ["A. Akl", "S. Valaee"], "venue": "affinity propagation, & compressive sensing, in: ICASSP\u201910", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "uWave: Accelerometer-based Personalized Gesture Recognition and Its Applications", "author": ["J. Liu", "L. Zhong", "J. Wickramasuriya", "V. Vasudevan"], "venue": "Pervasive Mob. Comput. 5 (6) ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning a symbolic representation for multivariate time series classification", "author": ["M. Baydogan", "G. Runger"], "venue": "Data Mining and Knowledge Discovery ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Experiencing SAX: A Novel Symbolic Representation of Time Series", "author": ["J. Lin", "E. Keogh", "L. Wei", "S. Lonardi"], "venue": "Data Min. Knowl. Discov. 15 (2) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "An Introduction to Variable and Feature Selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "J. Mach. Learn. Res. 3 ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Shapelet based time series classification: https://www.uea.ac.uk/computing/machinelearning/shapelets", "author": ["T. Bagnall"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "The UCR time series classification/clustering homepage: http://www.cs.ucr.edu/ eamonn/time series data", "author": ["E. Keogh"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Multivariate Time Series Classification with Learned Discretization: http://www.mustafabaydogan.com/multivariate-time-series-discretization-for-classification.html (2014). URL http://www.mustafabaydogan.com/multivariate-time-series-discretization-for-classification", "author": ["M. Baydogan"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "A Bag-of-Features Framework to Classify Time Series", "author": ["M.G. Baydogan", "G. Runger", "E. Tuv"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 35 (11) ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "One popular method is to identify shapelets [1].", "startOffset": 44, "endOffset": 47}, {"referenceID": 1, "context": "Many methods try to find shapelets and apply Shapelet Transformation [2].", "startOffset": 69, "endOffset": 72}, {"referenceID": 2, "context": "The idea of shapelets was mainly applied for univariate time series classification but also for time series clustering [3] and early classification of multivariate time series [4].", "startOffset": 119, "endOffset": 122}, {"referenceID": 3, "context": "The idea of shapelets was mainly applied for univariate time series classification but also for time series clustering [3] and early classification of multivariate time series [4].", "startOffset": 176, "endOffset": 179}, {"referenceID": 4, "context": "Hence, there are various methods of pruning the candidate space [5], improving the scoring function that defines how good a shapelet is [6, 1] or by parallelization [7].", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": "Hence, there are various methods of pruning the candidate space [5], improving the scoring function that defines how good a shapelet is [6, 1] or by parallelization [7].", "startOffset": 136, "endOffset": 142}, {"referenceID": 0, "context": "Hence, there are various methods of pruning the candidate space [5], improving the scoring function that defines how good a shapelet is [6, 1] or by parallelization [7].", "startOffset": 136, "endOffset": 142}, {"referenceID": 6, "context": "Hence, there are various methods of pruning the candidate space [5], improving the scoring function that defines how good a shapelet is [6, 1] or by parallelization [7].", "startOffset": 165, "endOffset": 168}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Also other measures like F-Stat, Kruskall-Wallis and Mood\u2019s median were considered [2, 8].", "startOffset": 83, "endOffset": 89}, {"referenceID": 7, "context": "Also other measures like F-Stat, Kruskall-Wallis and Mood\u2019s median were considered [2, 8].", "startOffset": 83, "endOffset": 89}, {"referenceID": 1, "context": "It is possible to use the extracted subsequences to transform the data and use an arbitrary classifier [2].", "startOffset": 103, "endOffset": 106}, {"referenceID": 8, "context": "[9] try to learn optimal shapelets with respect to the target and report statistically significant improvements in accuracy compared to other shapelet-based classifiers.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Shapelets have been used in many applications such as medicine [4], gesture [10] and gait recognition [11] and even time series clustering [3].", "startOffset": 63, "endOffset": 66}, {"referenceID": 9, "context": "Shapelets have been used in many applications such as medicine [4], gesture [10] and gait recognition [11] and even time series clustering [3].", "startOffset": 76, "endOffset": 80}, {"referenceID": 10, "context": "Shapelets have been used in many applications such as medicine [4], gesture [10] and gait recognition [11] and even time series clustering [3].", "startOffset": 102, "endOffset": 106}, {"referenceID": 2, "context": "Shapelets have been used in many applications such as medicine [4], gesture [10] and gait recognition [11] and even time series clustering [3].", "startOffset": 139, "endOffset": 142}, {"referenceID": 0, "context": "On the one hand, there are smart implementations using early abandon of distance computation and entropy pruning for the information gain heuristic [1].", "startOffset": 148, "endOffset": 151}, {"referenceID": 5, "context": "On the other hand, ideas to trade time for speed and reuse computations and to prune the search space [6] as well as pruning candidates by searching possibly interesting candidates on the SAX representation [5] or using infrequent shapelets [12] are applied.", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "On the other hand, ideas to trade time for speed and reuse computations and to prune the search space [6] as well as pruning candidates by searching possibly interesting candidates on the SAX representation [5] or using infrequent shapelets [12] are applied.", "startOffset": 207, "endOffset": 210}, {"referenceID": 11, "context": "On the other hand, ideas to trade time for speed and reuse computations and to prune the search space [6] as well as pruning candidates by searching possibly interesting candidates on the SAX representation [5] or using infrequent shapelets [12] are applied.", "startOffset": 241, "endOffset": 245}, {"referenceID": 12, "context": "[13] learn a decision tree using random subsequences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "This overcomes the problem of time series with varying lengths [14, 15].", "startOffset": 63, "endOffset": 71}, {"referenceID": 14, "context": "This overcomes the problem of time series with varying lengths [14, 15].", "startOffset": 63, "endOffset": 71}, {"referenceID": 15, "context": "For example dynamic time warping was applied on multivariate time series in the context of accelerometer-based gesture recognition [16, 17].", "startOffset": 131, "endOffset": 139}, {"referenceID": 16, "context": "For example dynamic time warping was applied on multivariate time series in the context of accelerometer-based gesture recognition [16, 17].", "startOffset": 131, "endOffset": 139}, {"referenceID": 17, "context": "[18] use a symbolic representation for multivariate time series.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "This is similar to SAX [19] but in contrast, the symbols are not fixed but learned in a supervised way using random forests.", "startOffset": 23, "endOffset": 27}, {"referenceID": 1, "context": "Exhaustive Search (ES) The exhaustive search (ES) [2, 6, 1] considers every subsequence in the training data and ranks it using a scoring function s.", "startOffset": 50, "endOffset": 59}, {"referenceID": 5, "context": "Exhaustive Search (ES) The exhaustive search (ES) [2, 6, 1] considers every subsequence in the training data and ranks it using a scoring function s.", "startOffset": 50, "endOffset": 59}, {"referenceID": 0, "context": "Exhaustive Search (ES) The exhaustive search (ES) [2, 6, 1] considers every subsequence in the training data and ranks it using a scoring function s.", "startOffset": 50, "endOffset": 59}, {"referenceID": 4, "context": "Hence, an approximative method, so called Fast Shapelets (FS) [5], was introduced.", "startOffset": 62, "endOffset": 65}, {"referenceID": 18, "context": "The idea is reduce the dimension of the data by estimating the SAX representation [19] and searching on the reduced space for features that are likely to be useful.", "startOffset": 82, "endOffset": 86}, {"referenceID": 8, "context": "[9] propose to consider the shapelets to be parameters that are optimized regarding the loss as well.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "Symbolic Representation for Multivariate Timeseries Symbolic Representation for Multivariate Timeseries (SMTS) [18] is not based on shapelets but has its own way of generating features.", "startOffset": 111, "endOffset": 115}, {"referenceID": 19, "context": "This process corresponds to a feature subset selection [20] that is computational very expensive as there usually exist many possible shapelet candidates.", "startOffset": 55, "endOffset": 59}, {"referenceID": 1, "context": "This relationship holds only for those methods that choose subsequences as shapelets that occur in the training data which is common in many methods [2, 5, 8, 6, 1].", "startOffset": 149, "endOffset": 164}, {"referenceID": 4, "context": "This relationship holds only for those methods that choose subsequences as shapelets that occur in the training data which is common in many methods [2, 5, 8, 6, 1].", "startOffset": 149, "endOffset": 164}, {"referenceID": 7, "context": "This relationship holds only for those methods that choose subsequences as shapelets that occur in the training data which is common in many methods [2, 5, 8, 6, 1].", "startOffset": 149, "endOffset": 164}, {"referenceID": 5, "context": "This relationship holds only for those methods that choose subsequences as shapelets that occur in the training data which is common in many methods [2, 5, 8, 6, 1].", "startOffset": 149, "endOffset": 164}, {"referenceID": 0, "context": "This relationship holds only for those methods that choose subsequences as shapelets that occur in the training data which is common in many methods [2, 5, 8, 6, 1].", "startOffset": 149, "endOffset": 164}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Typically, all subsequences of a specific length [2, 6, 1] or that fulfill another informed criterion [5] are chosen.", "startOffset": 49, "endOffset": 58}, {"referenceID": 5, "context": "Typically, all subsequences of a specific length [2, 6, 1] or that fulfill another informed criterion [5] are chosen.", "startOffset": 49, "endOffset": 58}, {"referenceID": 0, "context": "Typically, all subsequences of a specific length [2, 6, 1] or that fulfill another informed criterion [5] are chosen.", "startOffset": 49, "endOffset": 58}, {"referenceID": 4, "context": "Typically, all subsequences of a specific length [2, 6, 1] or that fulfill another informed criterion [5] are chosen.", "startOffset": 102, "endOffset": 105}, {"referenceID": 19, "context": "In the literature this kind of feature subset selection is called variable ranking and is a filter method [20].", "startOffset": 106, "endOffset": 110}, {"referenceID": 1, "context": "This figure also shows the shapelets found using the variable ranking method which are considered to be useful for classification [2].", "startOffset": 130, "endOffset": 133}, {"referenceID": 17, "context": "Since SMTS [18] is not only using the raw time series but also the derivative of it, it is now explained how derivatives can be used for Ultra-Fast Shapelets.", "startOffset": 11, "endOffset": 15}, {"referenceID": 20, "context": "from [21, 22] and have different properties such as number of training instances, classes and length (see Table 1).", "startOffset": 5, "endOffset": 13}, {"referenceID": 21, "context": "from [21, 22] and have different properties such as number of training instances, classes and length (see Table 1).", "startOffset": 5, "endOffset": 13}, {"referenceID": 22, "context": "4, UFS is compared to state-of-the-art classifiers for multivariate time series on 15 datasets from different domains such as speech, gesture, motion, handwriting and sign language recogniton provided by [23].", "startOffset": 204, "endOffset": 208}, {"referenceID": 23, "context": "The results of SMTS, nearest neighbor with dynamic time warping distance without a warping window (NNDTW) and a multivariate extension of TSBF [24] (MTSBF) were taken from Baydogan et al.", "startOffset": 143, "endOffset": 147}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "The datasets are provided by the UCR time series database [22] and by Bagnall et al.", "startOffset": 58, "endOffset": 62}, {"referenceID": 20, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Table 1 contains few statistics about the datasets, further information can be found on the corresponding websites [21, 22].", "startOffset": 115, "endOffset": 123}, {"referenceID": 21, "context": "Table 1 contains few statistics about the datasets, further information can be found on the corresponding websites [21, 22].", "startOffset": 115, "endOffset": 123}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "LS [9] reports a runtime of O ( ipnm ) where i is the number of iterations used until the algorithm converges and p is the number of shapelets that have to be found.", "startOffset": 3, "endOffset": 6}], "year": 2015, "abstractText": "Time series shapelets are discriminative subsequences and their similarity to a time series can be used for time series classification. Since the discovery of time series shapelets is costly in terms of time, the applicability on long or multivariate time series is difficult. In this work we propose Ultra-Fast Shapelets that uses a number of random shapelets. It is shown that Ultra-Fast Shapelets yield the same prediction quality as current state-of-theart shapelet-based time series classifiers that carefully select the shapelets by being by up to three orders of magnitudes. Since this method allows a ultra-fast shapelet discovery, using shapelets for long multivariate time series classification becomes feasible. A method for using shapelets for multivariate time series is proposed and Ultra-Fast Shapelets is proven to be successful in comparison to state-of-the-art multivariate time series classifiers on 15 multivariate time series datasets from various domains. Finally, time series derivatives that have proven to be useful for other time series classifiers are investigated for the shapelet-based classifiers. It is shown that they have a positive impact and that they are easy to integrate with a simple preprocessing step, without the need of adapting the shapelet discovery algorithm.", "creator": "LaTeX with hyperref package"}}}