{"id": "1503.05479", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2015", "title": "Interpolating Convex and Non-Convex Tensor Decompositions via the Subspace Norm", "abstract": "We consider the problem of recovering a low-rank tensor from a noisy observation. Previous work has shown $O(n^{\\lceil K/2\\rceil/2})$ recovery guarantee for recovering a $K$th order rank one tensor of size $n\\times .\n\nIf we could predict the function of $O(n^{\\lceil K/2\\rceil/2) $o, we could determine what the function was, and even if it was not, why. But as the function of $O(n^{\\lceil K/2\\rceil/2) and $\\lceil K/2\\rceil/2) $o, we could also compute that the function is always $O(n^{\\lceil K/2\\rceil/2)$ for all its elements, so that we can perform all the necessary tests. The problem arises as follows:\n$$ O(n^{\\lceil K/2\\rceil/2)\\rceil/2.\nThus, the function of $O(n^{\\lceil K/2\\rceil/2)$ is always $O(n^{\\lceil K/2\\rceil/2)\\rceil/2.\nNote the following: if $\\lceil K/2\\rceil/2 is not $O(n^{\\lceil K/2\\rceil/2)\\rceil/2\\rceil/2\\rceil/2\\rceil/2)$ and $O(n^{\\lceil K/2\\rceil/2\\rceil/2)\\rceil/2\\rceil/2\\rceil/2\\rceil/2\\rceil/2\\rceil/2\\rceil/2\\rceil/2\\rceil/2\\rceil/2\\rceil/2\\rceil/2\\rceil/2\\rceil/2\\rceil/2\\rceil/2\\rceil/2\\rceil/2\\rceil/2\\rceil/2\\rceil/2\\rceil/2\\rceil/2\\rce", "histories": [["v1", "Wed, 18 Mar 2015 16:45:04 GMT  (70kb,D)", "https://arxiv.org/abs/1503.05479v1", null], ["v2", "Tue, 27 Oct 2015 01:44:23 GMT  (79kb,D)", "http://arxiv.org/abs/1503.05479v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["qinqing zheng", "ryota tomioka"], "accepted": true, "id": "1503.05479"}, "pdf": {"name": "1503.05479.pdf", "metadata": {"source": "CRF", "title": "Interpolating Convex and Non-Convex Tensor Decompositions via the Subspace Norm", "authors": ["Qinqing Zheng", "Ryota Tomioka"], "emails": [], "sections": [{"heading": null, "text": "\u221a n+ \u221a HK\u22121) bound, in which the parameter H controls\nthe blend from the non-convex estimator to mode-wise nuclear norm minimization. Furthermore, we empirically demonstrate that the subspace norm achieves the nearly ideal denoising performance even with H = O(1)."}, {"heading": "1 Introduction", "text": "Tensor is a natural way to express higher order interactions for a variety of data and tensor decomposition has been successfully applied to wide areas ranging from chemometrics, signal processing, to neuroimaging; see [15, 18] for a survey. Moreover, recently it has become an active area in the context of learning latent variable models [3].\nMany problems related to tensors, such as, finding the rank, or a best rank-one approaximation of a tensor is known to be NP hard [11, 8]. Nevertheless we can address statistical problems, such as, how well we can recover a low-rank tensor from its randomly corrupted version (tensor denoising) or from partial observations (tensor completion). Since we can convert a tensor into a matrix by an operation known as unfolding, recent work [25, 19, 20, 13] has shown that we do get nontrivial guarantees by using some norms or singular value decompositions. More specifically, Richard & Montanari [20] has shown that when a rank-one Kth order tensor of size n \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 n is corrupted by standard Gaussian noise, a nontrivial bound can be shown with high probability if the signal to noise ratio \u03b2/\u03c3 % ndK/2e/2 by a method called the recursive unfolding1. Note\n1We say an % bn if there is a constant C > 0 such that an \u2265 C \u00b7 bn.\nar X\niv :1\n50 3.\n05 47\n9v 2\n[ cs\n.L G\n] 2\n7 O\nthat \u03b2/\u03c3 % \u221a n is sufficient for matrices (K = 2) and also for tensors if we use the best rankone approximation (which is known to be NP hard) as an estimator. On the other hand, Jain & Oh [13] analyzed the tensor completion problem and proposed an algorithm that requires O(n3/2 \u00b7 polylog(n)) samples forK = 3; while information theoretically we need at least \u2126(n) samples and the intractable maximum likelihood estimator would requireO(n\u00b7polylog(n)) samples. Therefore, in both settings, there is a wide gap between the ideal estimator and current polynomial time algorithms. A subtle question that we will address in this paper is whether we need to unfold the tensor so that the resulting matrix become as square as possible, which was the reasoning underlying both [19, 20].\nAs a parallel development, non-convex estimators based on alternating minimization or nonlinear optimization [1, 21] have been widely applied and have performed very well when appropriately set up. Therefore it would be of fundamental importance to connect the wisdom of nonconvex estimators with the more theoretically motivated estimators that recently emerged.\nIn this paper, we explore such a connection by defining a new norm based on Kronecker products of factors that can be obtained by simple mode-wise singular value decomposition (SVD) of unfoldings (see notation section below), also known as the higher-order singular value decomposition (HOSVD) [6, 7]. We first study the non-asymptotic behavior of the leading singular vector from the ordinary (rectangular) unfolding X(k) and show a nontrivial bound for signal to noise ratio \u03b2/\u03c3 % nK/4. Thus the result also applies to odd order tensors confirming a conjecture in [20]. Furthermore, this motivates us to use the solution of mode-wise truncated SVDs to construct a new norm. We propose the subspace norm, which predicts an unknown low-rank tensor as a mixture of K low-rank tensors, in which each term takes the form\nfoldk(M (k)(P\u0302 (1) \u2297 \u00b7 \u00b7 \u00b7 \u2297 P\u0302 (k\u22121) \u2297 P\u0302 (k+1) \u2297 \u00b7 \u00b7 \u00b7 \u2297 P\u0302 (K) )>),\nwhere foldk is the inverse of unfolding (\u00b7)(k), \u2297 denotes the Kronecker product, and P\u0302 (k) \u2208 Rn\u00d7H is a orthonormal matrix estimated from the mode-k unfolding of the observed tensor, for k = 1, . . . , K; H is a user-defined parameter, and M (k) \u2208 Rn\u00d7HK\u22121 . Our theory tells us that with sufficiently high signal-to-noise ratio the estimated P\u0302 (k) spans the true factors.\nWe highlight our contributions below: 1. We prove that the required signal-to-noise ratio for recovering a Kth order rank one tensor from the ordinary unfolding is O(nK/4). Our analysis shows a curious two phase behavior: with high probability, when nK/4 - \u03b2/\u03c3 - nK/2, the error shows a fast decay as 1/\u03b24; for \u03b2/\u03c3 % nK/2, the error decays slowly as 1/\u03b22. We confirm this in a numerical simulation. 2. The proposed subspace norm is an interpolation between the intractable estimators that directly control the rank (e.g., HOSVD) and the tractable norm-based estimators. It becomes equivalent to the latent trace norm [23] when H = n at the cost of increased signal-to-noise ratio threshold (see Table 1). 3. The proposed estimator is more efficient than previously proposed norm based estimators, because the size of the SVD required in the algorithm is reduced from n\u00d7 nK\u22121 to n\u00d7HK\u22121. 4. We also empirically demonstrate that the proposed subspace norm performs nearly optimally for constant order H .\nNotation Let X \u2208 Rn1\u00d7n2\u00d7\u00b7\u00b7\u00b7\u00d7nK be a Kth order tensor. We will often use n1 = \u00b7 \u00b7 \u00b7 = nK = n to simplify the notation but all the results in this paper generalizes to general dimensions. The inner product between a pair of tensors is defined as the inner products of them as vectors; i.e., \u3008X ,W\u3009 = \u3008vec(X ), vec(W)\u3009. For u \u2208 Rn1 ,v \u2208 Rn2 ,w \u2208 Rn3 , u \u25e6 v \u25e6 w denotes the n1 \u00d7 n2 \u00d7 n3 rank-one tensor whose i, j, k entry is uivjwk. The rank of X is the minimum number of rank-one tensors required to write X as a linear combination of them. A mode-k fiber of tensor X is an nk dimensional vector that is obtained by fixing all but the kth index of X . The mode-k unfolding X(k) of tensor X is an nk \u00d7 \u220f k\u2032 6=k nk\u2032 matrix constructed by concatenating all the mode-k fibers along columns. We denote the spectral and Frobenius norms for matrices by \u2016 \u00b7 \u2016 and \u2016 \u00b7 \u2016F , respectively."}, {"heading": "2 The power of ordinary unfolding", "text": ""}, {"heading": "2.1 A perturbation bound for the left singular vector", "text": "We first establish a bound on recovering the left singular vector of a rank-one n\u00d7m matrix (with m > n) perturbed by random Gaussian noise.\nConsider the following model known as the information plus noise model [4]:\nX\u0303 = \u03b2uv> + \u03c3E, (1)\nwhere u and v are unit vectors, \u03b2 is the signal strength, \u03c3 is the noise standard deviation, and the noise matrix E is assumed to be random with entries sampled i.i.d. from the standard normal distribution. Our goal is to lower-bound the correlation between u and the top left singular vector u\u0302 of X\u0303 for signal-to-noise ratio \u03b2/\u03c3 % (mn)1/4 with high probability.\nA direct application of the classic Wedin perturbation theorem [28] to the rectangular matrix X\u0303 does not provide us the desired result. This is because it requires the signal to noise ratio \u03b2/\u03c3 \u2265 2\u2016E\u2016. Since the spectral norm of E scales as Op( \u221a n + \u221a m) [27], this would mean that we require \u03b2/\u03c3 % m1/2; i.e., the threshold is dominated by the number of columns m, if m \u2265 n.\nAlternatively, we can view u\u0302 as the leading eigenvector of X\u0303X\u0303 >\n, a square matrix. Our key insight is that we can decompose X\u0303X\u0303 > as follows:\nX\u0303X\u0303 > = (\u03b22uu> +m\u03c32I) + (\u03c32EE> \u2212m\u03c32I) + \u03b2\u03c3(uv>E> +Evu>).\nNote that u is the leading eigenvector of the first term because adding an identity matrix does not change the eigenvectors. Moreover, we notice that there are two noise terms: the first term is a centered Wishart matrix and it is independent of the signal \u03b2; the second term is Gaussian distributed and depends on the signal \u03b2.\nThis implies a two-phase behavior corresponding to either the Wishart or the Gaussian noise term being dominant, depending on the value of \u03b2. Interestingly, we get a different speed of convergence for each of these phases as we show in the next theorem (the proof is given in Appendix D.1).\nTheorem 1. There exists a constant C such that with probability at least 1\u2212 4e\u2212n, if m/n \u2265 C,\n|\u3008u\u0302,u\u3009| \u2265  1\u2212 Cnm (\u03b2/\u03c3)4 , if \u221a m > \u03b2 \u03c3 \u2265 (Cnm) 1 4 ,\n1\u2212 Cn (\u03b2/\u03c3)2 , if \u03b2 \u03c3 \u2265 \u221a m,\notherwise, |\u3008u\u0302,u\u3009| \u2265 1\u2212 Cn (\u03b2/\u03c3)2\nif \u03b2/\u03c3 \u2265 \u221a Cn.\nIn other words, if X\u0303 has sufficiently many more columns than rows, as the signal to noise ratio \u03b2/\u03c3 increases, u\u0302 first converges to u as 1/\u03b24, and then as 1/\u03b22. Figure 1(a) illustrates these results. We randomly generate a rank-one 100 \u00d7 10000 matrix perturbed by Gaussian noise, and measure the distance between u\u0302 and u. The phase transition happens at \u03b2/\u03c3 = (nm)1/4, and there are two regimes of different convergence rates as Theorem 1 predicts."}, {"heading": "2.2 Tensor Unfolding", "text": "Now let\u2019s apply the above result to the tensor version of information plus noise model studied by [20]. We consider a rank one n\u00d7\u00b7 \u00b7 \u00b7\u00d7n tensor (signal) contaminated by Gaussian noise as follows:\nY = X \u2217 + \u03c3E = \u03b2u(1) \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 u(K) + \u03c3E , (2) where factors u(k) \u2208 Rn, k = 1, . . . , K, are unit vectors, which are not necessarily identical, and the entries of E \u2208 Rn\u00d7\u00b7\u00b7\u00b7\u00d7n are i.i.d samples from the normal distributionN (0, 1). Note that this is slightly more general (and easier to analyze) than the symmetric setting studied by [20].\nSeveral estimators for recovering X \u2217 from its noisy version Y have been proposed (see Table 1). Both the overlapped nuclear norm and latent nuclear norm discussed in [23] achives the relative performance guarantee\n|||X\u0302 \u2212 X \u2217|||F/\u03b2 \u2264 Op ( \u03c3 \u221a nK\u22121/\u03b2 ) , (3)\nwhere X\u0302 is the estimator. This bound implies that if we want to obtain relative error smaller than \u03b5, we need the signal to noise ratio \u03b2/\u03c3 to scale as \u03b2/\u03c3 % \u221a nK\u22121/\u03b5.\nMu et al. [19] proposed the square norm, defined as the nuclear norm of the matrix obtained by grouping the first bK/2c indices along the rows and the last dK/2e indices along the columns. This norm improves the right hand side of inequality (3) to Op(\u03c3 \u221a ndK/2e/\u03b2), which translates to\nrequiring \u03b2/\u03c3 % \u221a ndK/2e/\u03b5 for obtaining relative error \u03b5. The intuition here is the more square the unfolding is the better the bound becomes. However, there is no improvement for K = 3. Richard and Montanari [20] studied the (symmetric version of) model (2) and proved that a recursive unfolding algorithm achieves the factor recovery error dist(u\u0302(k),u(k)) = \u03b5 with \u03b2/\u03c3 %\u221a ndK/2e/\u03b5 with high probability, where dist(u,u\u2032) := min(\u2016u \u2212 u\u2032\u2016, \u2016u + u\u2032\u2016). They also showed that the randomly initialized tensor power method [7, 16, 3] can achieve the same error \u03b5 with slightly worse threshold \u03b2/\u03c3 % max( \u221a n/\u03b52, nK/2) \u221a K logK also with high probability.\nThe reasoning underlying both [19] and [20] is that square unfolding is better. However, if we take the (ordinary) mode-k unfolding\nY (k) = \u03b2u (k) ( u(k\u22121) \u2297 \u00b7 \u00b7 \u00b7 \u2297 u(1) \u2297 u(K) \u2297 \u00b7 \u00b7 \u00b7 \u2297 u(k+1) )> + \u03c3E(k), (4)\nwe can see (4) as an instance of information plus noise model (1) where m/n = nK\u22122. Thus the ordinary unfolding satisfies the condition of Theorem 1 for n or K large enough.\nCorollary 1. Consider a K(\u2265 3)th order rank one tensor contaminated by Gaussian noise as in (2). There exists a constant C such that if nK\u22122 \u2265 C, with probability at least 1\u22124Ke\u2212n, we have\ndist2(u\u0302(k),u(k)) \u2264  2CnK (\u03b2/\u03c3)4 , if n K\u22121 2 > \u03b2/\u03c3 \u2265 C 1 4n K 4 , 2Cn\n(\u03b2/\u03c3)2 , if \u03b2/\u03c3 \u2265 n\nK\u22121 2 ,\nfor k = 1, . . . , K,\nwhere u\u0302(k) is the leading left singular vector of the rectangular unfolding Y (k).\nThis proves that as conjectured by [20], the threshold \u03b2/\u03c3 % nK/4 applies not only to the even order case but also to the odd order case. Note that Hopkins et al. [10] have shown a similar result without the sharp rate of convergence. The above corollary easily extends to more general n1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 nK tensor by replacing the conditions by \u221a\u220f ` 6=k n` > \u03b2/\u03c3 \u2265 (C \u220fK k=1 nk) 1/4 and\n\u03b2/\u03c3 \u2265 \u221a\u220f\n6\u0300=k n`. The result also holds when X \u2217 has rank higher than 1; see Appendix E. We demonstrate this result in Figure 1(b). The models behind the experiment are slightly more general ones in which [n1, n2, n3] = [20, 40, 60] or [40, 80, 120] and the signal X \u2217 is rank two with \u03b21 = 20 and \u03b22 = 10. The plot shows the inner products \u3008u(1)1 , u\u0302 (1) 1 \u3009 and \u3008u (1) 2 , u\u0302 (1) 2 \u3009 as a measure of the quality of estimating the two mode-1 factors. The horizontal axis is the normalized noise standard deviation \u03c3( \u220fK k=1 nk)\n1/4. We can clearly see that the inner product decays symmetrically around \u03b21 and \u03b22 as predicted by Corollary 1 for both tensors."}, {"heading": "3 Subspace norm for tensors", "text": "Suppose the true tensor X \u2217 \u2208 Rn\u00d7\u00b7\u00b7\u00b7\u00d7n admits a minimum Tucker decomposition [26] of rank (R, . . . , R):\nX \u2217 = \u2211R i1=1 \u00b7 \u00b7 \u00b7 \u2211R iK=1 \u03b2i1i2...iKu (1) i1 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 u(K)iK . (5)\nIf the core tensor C = (\u03b2i1...iK ) \u2208 RR\u00d7\u00b7\u00b7\u00b7\u00d7R is superdiagonal, the above decomposition reduces to the canonical polyadic (CP) decomposition [9, 15]. The mode-k unfolding of the true tensor X \u2217 can be written as follows:\nX\u2217(k) = U (k)C(k)\n( U (1) \u2297 \u00b7 \u00b7 \u00b7 \u2297U (k\u22121) \u2297U (k+1) \u2297 \u00b7 \u00b7 \u00b7 \u2297U (K) )> , (6)\nwhereC(k) is the mode-k unfolding of the core tensor C;U (k) is a n\u00d7RmatrixU (k) = [u(k)1 , . . . ,u (k) R ] for k = 1, . . . , K. Note that U (k) is not necessarily orthogonal. LetX\u2217(k) = P (k)\u039b(k)Q(k) > be the SVD ofX\u2217(k). We will observe that\nQ(k) \u2208 Span ( P (1) \u2297 \u00b7 \u00b7 \u00b7 \u2297 P (k\u22121) \u2297 P (k+1) \u2297 \u00b7 \u00b7 \u00b7 \u2297 P (K) ) (7)\nbecause of (6) and U (k) \u2208 Span(P (k)). Corollary 1 shows that the left singular vectors P (k) can be recovered under mild conditions; thus the span of the right singular vectors can also be recovered. Inspired by this, we define a norm that models a tensor X as a mixture of tensors Z(1), . . . ,Z(K). We require that the modek unfolding of Z(k), i.e. Z(k)(k), has a low rank factorization Z (k) (k) = M (k)S(k) > , where M (k) \u2208 Rn\u00d7HK\u22121 is a variable, and S(k) \u2208 RnK\u22121\u00d7HK\u22121 is a fixed arbitrary orthonormal basis of some subspace, which we choose later to have the Kronecker structure in (7).\nIn the following, we define the subspace norm, suggest an approach to construct the right factor S(k), and prove the denoising bound in the end."}, {"heading": "3.1 The subspace norm", "text": "Consider a Kth order tensor of size n\u00d7 \u00b7 \u00b7 \u00b7n.\nDefinition 1. Let S(1), . . . ,S(K) be matrices such that S(k) \u2208 RnK\u22121\u00d7HK\u22121 with H \u2264 n. The subspace norm for a Kth order tensor X associated with {S(k)}Kk=1 is defined as\n|||X |||s :=\n{ inf{M (k)}Kk=1 \u2211K k=1 \u2016M\n(k)\u2016\u2217, if X \u2208 Span({S(k)}Kk=1), +\u221e, otherwise,\nwhere \u2016\u00b7\u2016\u2217 is the nuclear norm, and Span({S(k)}Kk=1) := { X \u2208 Rn\u00d7\u00b7\u00b7\u00b7\u00d7n : \u2203M (1), . . . ,M (K),X =\u2211K\nk=1 foldk(M (k)S(k)\n> ) } .\nIn the next lemma (proven in Appendix D.2), we show the dual norm of the subspace norm has a simple appealing form. As we see in Theorem 2, it avoids the O( \u221a nK\u22121) scaling (see the first column of Table 1) by restricting the influence of the noise term in the subspace defined by S(1), . . . ,S(K).\nLemma 1. The dual norm of |||\u00b7|||s is a semi-norm\n|||X |||s\u2217 = max k=1,...,K \u2016X(k)S(k)\u2016,\nwhere \u2016 \u00b7 \u2016 is the spectral norm."}, {"heading": "3.2 Choosing the subspace", "text": "A natural question that arises is how to choose the matrices S(1), . . . ,S(k).\nLemma 2. Let the X\u2217(k) = P (k)\u039b(k)Q(k) be the SVD of X\u2217(k), where P (k) is n \u00d7 R and Q(k) is nK\u22121 \u00d7R. Assume that R \u2264 n and U (k) has full column rank. It holds that for all k,\ni) U (k) \u2208 Span(P (k)), ii) Q(k) \u2208 Span ( P (1) \u2297 \u00b7 \u00b7 \u00b7 \u2297 P (k\u22121) \u2297 P (k+1) \u2297 \u00b7 \u00b7 \u00b7 \u2297 P (K) ) .\nProof. We prove the lemma in Appendix D.4.\nCorollary 1 shows that when the signal to noise ratio is high enough, we can recover P (k) with high probability. Hence we suggest the following three-step approach for tensor denoising:\n(i) For each k, unfold the observation tensor in mode k and compute the top H left singular\nvectors. Concatenate these vectors to obtain a n\u00d7H matrix P\u0302 (k) .\n(ii) Construct S(k) as S(k) = P\u0302 (1) \u2297 \u00b7 \u00b7 \u00b7 \u2297 P\u0302 (k\u22121) \u2297 P\u0302 (k+1) \u2297 \u00b7 \u00b7 \u00b7 \u2297 P\u0302 (K) .\n(iii) Solve the subspace norm regularized minimization problem\nmin X\n1 2 |||Y \u2212 X |||2F + \u03bb|||X |||s, (8)\nwhere the subspace norm is associated with the above defined {S(k)}Kk=1.\nSee Appendix B for details."}, {"heading": "3.3 Analysis", "text": "Let Y \u2208 Rn\u00d7\u00b7\u00b7\u00b7\u00d7n be a tensor corrupted by Gaussian noise with standard deviation \u03c3 as follows:\nY = X \u2217 + \u03c3E . (9)\nWe define a slightly modified estimator X\u0302 as follows:\nX\u0302 = arg min X ,{M (k)}Kk=1\n{1 2 |||Y \u2212 X |||2F + \u03bb|||X |||s : X = K\u2211 k=1 foldk ( M (k)S(k) >) , {M (k)}Kk=1 \u2208M(\u03c1) } (10)\nwhere M(\u03c1) is a restriction of the set of matrices M (k) \u2208 Rn\u00d7HK\u22121 , k = 1, . . . , K defined as follows:\nM(\u03c1) := { {M (k)}Kk=1 : \u2016foldk(M (k))(`)\u2016 \u2264 \u03c1 K ( \u221a n+ \u221a HK\u22121),\u2200k 6= ` } .\nThis restriction makes sure thatM (k), k = 1, . . . , K, are incoherent, i.e., eachM (k) has a spectral norm that is as low as a random matrix when unfolded at a different mode `. Similar assumptions were used in low-rank plus sparse matrix decomposition [2, 12] and for the denoising bound for the latent nuclear norm [23].\nThen we have the following statement (we prove this in Appendix D.3).\nTheorem 2. Let Xp be any tensor that can be expressed as\nXp = K\u2211 k=1 foldk ( M (k)p S (k)> ) ,\nwhich satisfies the above incoherence condition {M (k)p }Kk=1 \u2208M(\u03c1) and let rk be the rank ofM (k)p for k = 1, . . . , K. In addition, we assume that each S(k) is constructed as S(k) = P\u0302 (k\u22121) \u2297 \u00b7 \u00b7 \u00b7 \u2297 P\u0302 (k+1) with (P\u0302 (k) )>P\u0302 (k)\n= IH . Then there are universal constants c0 and c1 such that any solution X\u0302 of the minimization problem (10) with \u03bb = |||Xp\u2212X \u2217|||s\u2217+c0\u03c3 (\u221a n+ \u221a HK\u22121 + \u221a 2 log(K/\u03b4) ) satisfies the following bound\n|||X\u0302 \u2212 X \u2217|||F \u2264 |||Xp \u2212X \u2217|||F + c1\u03bb \u221a\u2211K k=1 rk,\nwith probability at least 1\u2212 \u03b4.\nNote that the right-hand side of the bound consists of two terms. The first term is the approximation error. This term will be zero if X \u2217 lies in Span({S(k)}Kk=1). This is the case, if we choose S(k) = InK\u22121 as in the latent nuclear norm, or if the condition of Corollary 1 is satisfied for the smallest \u03b2R when we use the Kronecker product construction we proposed. Note that the regularization constant \u03bb should also scale with the dual subspace norm of the residual Xp \u2212X \u2217.\nThe second term is the estimation error with respect to Xp. If we take Xp to be the orthogonal projection of X \u2217 to the Span({S(k)}Kk=1), we can ignore the contribution of the residual to \u03bb, because (Xp \u2212 X \u2217)(k)S(k) = 0. Then the estimation error scales mildly with the dimensions n, HK\u22121 and with the sum of the ranks. Note that if we take S(k) = InK\u22121 , we have HK\u22121 = nK\u22121, and we recover the guarantee (3) ."}, {"heading": "4 Experiments", "text": "In this section, we conduct tensor denoising experiments on synthetic and real datasets, to numerically confirm our analysis in previous sections."}, {"heading": "4.1 Synthetic data", "text": "We randomly generated the true rank two tensor X \u2217 of size 20 \u00d7 30 \u00d7 40 with singular values \u03b21 = 20 and \u03b22 = 10. The true factors are generated as random matrices with orthonormal columns. The observation tensor Y is then generated by adding Gaussian noise with standard deviation \u03c3 to X \u2217.\nOur approach is compared to the CP decomposition, the overlapped approach, and the latent approach. The CP decomposition is computed by the tensorlab [22] with 20 random initializations. We assume CP knows the true rank is 2. For the subspace norm, we use Algorithm 2 described in Section 3. We also select the top 2 singular vectors when constructing U\u0302 (k)\n\u2019s. We computed the solutions for 20 values of regularization parameter \u03bb logarithmically spaced between 1 and 100. For the overlapped and the latent norm, we use ADMM described in [25]; we also computed 20 solutions with the same \u03bb\u2019s used for the subspace norm.\nWe measure the performance in the relative error defined as |||X\u0302 \u2212 X \u2217|||F/|||X \u2217|||F . We report the minimum error obtained by choosing the optimal regularization parameter or the optimal initialization. Although the regularization parameter could be selected by leaving out some entries and measuring the error on these entries, we will not go into tensor completion here for the sake of simplicity.\nFigure 2 (a) and (b) show the result of this experiment. The left panel shows the relative error for 3 representative values of \u03bb for the subspace norm. The black dash-dotted line shows the minimum error across all the \u03bb\u2019s. The magenta dashed line shows the error corresponding to the theoretically motivated choice \u03bb = \u03c3(maxk( \u221a nk + \u221a HK\u22121) + \u221a 2 log(K)) for each \u03c3. The two vertical lines are thresholds of \u03c3 from Corollary 1 corresponding to \u03b21 and \u03b22, namely, \u03b21/( \u220f k nk) 1/4 and \u03b22/( \u220f k nk) 1/4. It confirms that there is a rather sharp increase in the error around the theoretically predicted places (see also Figure 1(b)). We can also see that the optimal \u03bb\nshould grow linearly with \u03c3. For large \u03c3 (small SNR), the best relative error is 1 since the optimal choice of the regularization parameter \u03bb leads to predicting with X\u0302 = 0.\nFigure 2 (b) compares the performance of the subspace norm to other approaches. For each method the smallest error corresponding to the optimal choice of the regularization parameter \u03bb is shown. In addition, to place the numbers in context, we plot the line corresponding to\nRelative error =\n\u221a R \u2211\nk nk log(K)\n|||X \u2217|||F \u00b7 \u03c3, (11)\nwhich we call \u201coptimistic\u201d. This can be motivated from considering the (non-tractable) maximum likelihood estimator for CP decomposition (see Appendix A).\nClearly, the error of CP, the subspace norm, and \u201coptimistic\u201d grows at the same rate, much slower than overlap and latent. The error of CP increases beyond 1, as no regularization is imposed (see Appendix C for more experiments). We can see that both CP and the subspace norm are behaving near optimally in this setting, although such behavior is guaranteed for the subspace norm whereas it is hard to give any such guarantee for the CP decomposition based on nonlinear optimization."}, {"heading": "4.2 Amino acids data", "text": "The amino acid dataset [5] is a semi-realistic dataset commonly used as a benchmark for low rank tensor modeling. It consists of five laboratory-made samples, each one contains different amounts of tyrosine, tryptophan and phenylalanine. The spectrum of their excitation wavelength (250-300 nm) and emission (250-450 nm) are measured by fluorescence, which gives a 5\u00d7 201\u00d7 61 tensor. As the true factors are known to be these three acids, this data perfectly suits the CP model. The true rank is fed into CP and the proposed approach asH = 3. We computed the solutions of CP for 20 different random initializations, and the solutions of other approaches with 20 different values of \u03bb. For the subspace and the overlapped approach, \u03bb\u2019s are logarithmically spaced between 103\nand 105. For the latent approach, \u03bb\u2019s are logarithmically spaced between 104 and 106. Again, we include the optimistic scaling (11) to put the numbers in context.\nFigure 2(c) shows the smallest relative error achieved by all methods we compare. Similar to the synthetic data, both CP and the subspace norm behaves near ideally, though the relative error of CP can be larger than 1 due to the lack of regularization. Interestingly the theoretically suggested scaling of the regularization parameter \u03bb is almost optimal."}, {"heading": "5 Conclusion", "text": "We have settled a conjecture posed by [20] and showed that indeed O(nK/4) signal-to-noise ratio is sufficient also for odd order tensors. Moreover, our analysis shows an interesting two-phase behavior of the error. This finding lead us to the development of the proposed subspace norm. The proposed norm is defined with respect to a set of orthonormal matrices P\u0302 (1) , . . . , P\u0302 (K) , which are estimated by mode-wise singular value decompositions. We have analyzed the denoising performance of the proposed norm, and shown that the error can be bounded by the sum of two terms, which can be interpreted as an approximation error term coming from the first (non-convex) step, and an estimation error term coming from the second (convex) step."}, {"heading": "A Maximum likelihood estimator", "text": "Let Y \u2208 Rn1\u00d7\u00b7\u00b7\u00b7\u00d7nK be a noisy observed tensor generated as follows:\nY = X \u2217 + \u03c3E = R\u2211 r=1 \u03b2ru (1) r \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 u(K)r + \u03c3E ,\nwhere E is a noisy tensor whose entries are i.i.d. normal N (0, 1). Let X\u0302MLE be the (intractable) estimator defined as\nX\u0302MLE = arg min X\n( |||Y \u2212 X |||2F : rank(X ) \u2264 R ) .\nWe have the following performance guarantee for X\u0302MLE:\nTheorem 3. Let R \u2264 mink nk/2. Then there is a constant c such that\n|||X\u0302MLE \u2212X \u2217|||F \u2264 c\u03c3 \u221a\u221a\u221a\u221aRK K\u2211 k=1 nk log(2K/K0) + log(2/\u03b4),\nwith probability at least 1\u2212 \u03b4, where K0 = log(3/2).\nNote that the factor RK in the square root is rather conservative. In the best case, this factor reduces to linear in R and this is what we present in Section 4 as \u201coptimistic\u201d ignoring constants and \u03b4; see Eq. (11).\nProof of Theorem 3. Since X\u0302MLE is a minimizer and X \u2217 is also feasible, we have\n|||Y \u2212 X\u0302MLE|||2F \u2264 |||Y \u2212 X \u2217|||2F ,\nwhich implies\n|||X \u2217 \u2212 X\u0302MLE|||2F \u2264 \u03c3\u3008E , X\u0302MLE \u2212X \u2217\u3009\n\u2264 \u03c3|||E|||op|||X\u0302MLE \u2212X \u2217|||nuc,\nwhere\n|||X |||op := sup u(1),...,u(K) { \u2211 i1,i2,...,iK Xi1,i2,...,iKu (1) i1 u (2) i2 \u00b7 \u00b7 \u00b7u(K)iK :\n\u2016u(1)\u2016 = \u2016u(2)\u2016 = \u00b7 \u00b7 \u00b7 = \u2016u(K)\u2016 = 1 }\nis the tensor spectral norm and the nuclear norm\n|||X |||nuc := inf u(1),...,u(K) {\u2211 r \u2016u(1)r \u2016 \u00b7 \u2016u(2)r \u2016 \u00b7 \u00b7 \u00b7 \u2016u(K)r \u2016 :\nX = R\u2211 r=1 u(1)r \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 u(K)r }\nis the dual of the spectral norm. Since both X\u0302MLE and X \u2217 are rank at most R, the difference X\u0302MLE \u2212 X \u2217 is rank at most 2R. Moreover, any rank-R CP decomposition with R \u2264 mink nkcan be reduced to an orthogonal CP decomposition with rank at most RK via the Tucker decomposition [14]. Thus, denoting this orthogonal decomposition by X\u0302MLE\u2212X \u2217 = \u2211RK r=1 u\u0303 (1) r \u25e6\u00b7 \u00b7 \u00b7\u25e6u\u0303(K)r and using \u03b2r := \u2016u\u0303(1)r \u2016 \u00b7 \u00b7 \u00b7 \u2016u\u0303(K)r \u2016, we have\n|||X\u0302MLE \u2212X \u2217|||nuc \u2264 RK\u2211 r=1 \u03b2r \u2264 \u221a RK \u221a\u2211RK r=1 \u03b22r\n= \u221a RK |||X\u0302MLE \u2212X \u2217|||F ,\nwhere the last equality follows because the decomposition is orthogonal. Finally applying the tail bound for the spectral norm \u2016E\u2016op of random Gaussian tensor E [24], we obtain what we wanted."}, {"heading": "B Details of optimization", "text": "For solving problem (8), we follow the alternating direction method of multipliers described in [25]. We scale the objective function in (8) by 1/\u03bb, and consider the dual problem\nmin D,{W (k)}Kk=1\n\u03bb 2 |||D|||2F \u2212 \u3008D,Y\u3009\ns.t. max k \u2016W (k)\u2016 \u2264 1,\nW (k) = D(k)S (k), k = 1, . . . , K,\n(12)\nwhere D \u2208 Rn1\u00d7n2\u00d7\u00b7\u00b7\u00b7\u00d7nK is the dual tensor that corresponds to the residual in the primal problem (8), andW (k)\u2019s are auxiliary variables introduced to make the problem equality constrained.\nAlgorithm 1: Tensor denoising via the subspace norm Input: noisy tensor Y , subspace dimension H , regularization constant \u03bb for k = 1 to K do P\u0302 (k) \u2190\u2212 top H left singular vectors of Y (k)\nend for for k = 1 to K do S(k) \u2190\u2212 P\u0302 (1) \u2297 \u00b7 \u00b7 \u00b7 \u2297 P\u0302 (k\u22121) \u2297 P\u0302 (k+1) \u2297 \u00b7 \u00b7 \u00b7 \u2297 P\u0302 (K) end for Output: X\u0302 = arg minX 12 |||Y \u2212 X ||| 2 F + \u03bb|||X |||s.\nThe augmented Lagrangian function of problem (12) could be written as follows:\nL\u03b7(D, {W (k)}Kk=1, {M (k)}Kk=1)\n= \u03bb\n2 |||D|||2 \u2212 \u3008D,Y\u3009+ K\u2211 k=1 ( \u3008M (k),D(k)S(k) \u2212W (k)\u3009\n+ \u03b7\n2 \u2016D(k)S(k) \u2212W (k)\u20162F + 1\u2016\u00b7\u2016\u22641(W (k))\n) ,\nwhere M (k)\u2019s are the multipliers, \u03b7 is the augmenting parameter, and 1\u2016\u00b7\u2016\u22641 is the indicator function of the unit spectral norm ball.\nWe follow the derivation in [25] and conclude that the updates of D, M (k) and W (k) can be computed in closed forms. We further combine the updates ofW (k) and other steps so that it needs not to be explicitly computed. The sum of the products ofM (k) and S(k) > finally converges to the solution of the primal problem (8), see Algorithm 2. The update for the Lagrangian multipliers M (k) (k = 1, . . . , K) is written as singular value soft-thresholding operator defined as\nproxtr\u03b7 (Z) = P max(\u03a3\u2212 \u03b7, 0)Q>,\nwhere Z = P\u03a3Q> is the SVD of Z. A notable property of the subspace norm is the computational efficiency. The update of M (k) requires singular value decomposition, which usually dominates the costs of computation. For problem (12), the size ofM (k) is only nk \u00d7HK\u22121. Comparing with previous approaches, e.g. the latent approach whose multipliers are nk \u00d7 \u220f k\u2032 6=k nk\u2032 matrices, the size of our variables is much smaller, so the per-iteration cost is reduced."}, {"heading": "C Additional experiments", "text": "We report the experimental results when the input rank of CP and the subspace approach is are over-specified, on the same synthetic dataset as Section 4. We consider the case where the input rank is 8.\nAlgorithm 2: ADMM for subspace norm minimization\nInput: Y , \u03bb, S(1), . . . ,S(K), \u03b7, initializations D0, {M (1)0 , . . . ,M (K) 0 } t = 0 repeat\nDt+1 = 1\u03bb+\u03b7K ( Y +K\u03b7Dt \u2212 \u2211 k foldk ( (2M (k) t \u2212M (k) t\u22121)S (k)>)) for k = 1 to K do M\n(k) t+1 = prox tr \u03b7\n( M\n(k) t + \u03b7D(k),t+1S\n(k) )\nend for t\u2190 t+ 1\nuntil convergence Output: X\u0302 = \u2211K k=1M (k) t S (k)>.\nWe impose the `2 regularizations on the factors of CP. We test 20 values that are logarithmically spaced between 0.01 and 10 are the regularization parameter. For each value, we compute 20 solutions with random initializations and select the one with lowest objective value.\nFor the subspace approach, we computed solutions for 20 values of the regularization parameter that are logarithmically spaced between 1 and 1000.\nAs before, we report the minimum relative error obtained by the same method. The results are shown in Figure 3. We include the case the rank is specified incorrectly for comparison. Clearly, even if the rank is much larger than the truth, the subspace approach and CP are robust with proper regularization."}, {"heading": "D Proofs", "text": "D.1 Proof of Theorem 1 We consider the second moment of X\u0303:\nX\u0303X\u0303 > = \u03b22uu> + \u03c32EE> + \u03b2\u03c3(uv>E> +Evu>)\n= B\ufe37 \ufe38\ufe38 \ufe37 \u03b22uu> +m\u03c32I +\nG\ufe37 \ufe38\ufe38 \ufe37 \u03c32EE> \u2212m\u03c32I + \u03b2\u03c3(uv>E> +Evu>) .\nThe eigenvalue decomposition ofB can be written as\nB = [u U 2]\n[ \u03b22 +m\u03c32\nm\u03c32I\n] [ u>\nU>2\n] .\nWe first show a deterministic lower bound for |\u3008u\u0302,u\u3009| assuming \u03b22 \u2265 2\u2016G\u2016, where u\u0302 is the leading eigenvector of X\u0303X\u0303 > . Then we bound the spectral norm \u2016G\u2016 of the noise term (Lemma 3) and derive the sufficient condition for \u03b2. Let u\u0302 be the leading eigenvector of X\u0303X\u0303 > with eigenvalue \u03bb\u0302, r = Bu\u0302 \u2212 \u03bb\u0302u\u0302 = \u2212Gu\u0302. We have U>2 r = (m\u03c3 2 \u2212 \u03bb\u0302)U>2 u\u0302. Hence, for all \u03b22 > 2\u2016G\u2016, it holds that\n| sin(u\u0302,u)| = \u2016U>2 u\u0302\u20162 = \u2016U>2 r\u20162 \u03bb\u0302\u2212m\u03c32 \u2264 \u2016G\u2016 \u03b22 \u2212 \u2016G\u2016 \u2264 2\u2016G\u2016 \u03b22 ,\nwhere we used \u2016U>2 r\u20162 = \u2016U>2Gu\u0302\u20162 \u2264 \u2016G\u2016, and \u03bb\u0302 \u2265 u>X\u0303X\u0303 > u> \u2265 \u03b22 + m\u03c32 \u2212 \u2016G\u2016. Therefore,\n|\u3008u\u0302,u\u3009| = | cos(u\u0302,u)| \u2265 \u221a 1\u2212 4\u2016G\u2016 2\n\u03b24 \u2265 1\u2212 4\u2016G\u2016 2 \u03b24 ,\nif \u03b22 \u2265 2\u2016G\u2016. It follows from Lemma 3 (shown below) that\n\u2016G\u2016 \u2264\n{ 2C\u0304\u03c32 \u221a mn, if \u03b2/\u03c3 < \u221a m,\n2C\u0304\u03b2\u03c3 \u221a n, otherwise,\nwhere C\u0304 is a universal constant with probability at least 1\u2212 4e\u2212n.\nNow consider the first case (\u03b2/\u03c3 < \u221a m) and assume \u03b22 \u2265 4C\u0304\u03c32 \u221a mn \u2265 2\u2016G\u2016. Note that this\ncase only arises when \u221a m \u2265 4C\u0304 \u221a n. Denoting C = 16C\u03042, we obtain the first case in the theorem. Next, consider the second case (\u03b2/\u03c3 \u2265 \u221a m). If \u221a m \u2265 4C\u0304 \u221a n as above, we have \u03b2/\u03c3 \u2265 4C\u0304 \u221a n, which implies \u03b22 \u2265 2\u2016G\u2016 and we obtain the second case in the theorem. On the other hand, if\u221a m < 4C\u0304 \u221a n, we require \u03b2/\u03c3 \u2265 4C\u0304 \u221a n to obtain the last case in the theorem.\nLemma 3. Let G be constructed as in Theorem 1. If m \u2265 n, there exists an universal constant C\u0304 such that \u2016G\u2016 \u2264 C\u0304\u03c32 (\u221a mn+ \u221a n(\u03b2/\u03c3)2 ) ,\nwith probability at least 1\u2212 4e\u2212n.\nProof. The proof is an \u03b5-net argument. Let \u03bb = 2\u03c32 (\u221a 4mn+ 4n+ \u221a 8n(\u03b2/\u03c3)2 ) .\nThe goal is to control |x>Gx| for all the vectors x on the unit Euclidean sphere Sn\u22121. In order to do this, we first bound the probability of the tail event |x>Gx| > \u03bb, for any fixed x \u2208 Sn\u22121. Then we bound the probability that |x>Gx| > \u03bb for all the vectors in a \u03b5-net N\u03b5. Finally, we establish the connection between supx\u2208N\u03b5 |x>Gx| and \u2016G\u2016.\nTo bound P(|x>Gx| > \u03bb) for a fix x \u2208 Sn\u22121, we expand x>Gx as\nx>Gx = \u03c32(\u2016z\u20162 \u2212m) + 2\u03b2\u03c3(u>x)\u03b3,\nwhere z = E>x and \u03b3 = v>z. Since z \u223c N (0, I), we can see that \u2016z\u20162 is \u03c72 distributed with m degrees of freedom and \u03b3 \u223c N (0, 1).\nFirst we bound the deviation of the \u03c72 term. By the corollary of Lemma 1 in [17], we have\nP( \u2223\u2223\u2016z\u20162 \u2212m\u2223\u2223 > \u03bb1) \u2264 2e\u22124n, (13)\nwhere \u03bb1 = 2( \u221a\n4mn+ 4n). Next we bound the deviation of the Gaussian term. Using the Gaussian tail inequality, we have\nP (|\u03b3| > \u03bb2) \u2264 2e\u22124n, (14)\nwhere \u03bb2 = \u221a\n8n. Combining inequalities (22) and (14), we have\nP(|x>Gx| > \u03bb) \u2264 P ( \u03c32 \u2223\u2223\u2016z\u20162 \u2212m\u2223\u2223+ |2\u03b2\u03c3(u>x)\u03b3| > \u03c32\u03bb1 + 2\u03b2\u03c3\u03bb2)\n\u2264 P (\u2223\u2223\u2016z\u20162 \u2212m\u2223\u2223 > \u03bb1 \u2228 |\u03b3| > \u03bb2)\n\u2264 P (\u2223\u2223\u2016z\u20162 \u2212m\u2223\u2223 > \u03bb1)+ P (|\u03b3| > \u03bb2) \u2264 4e\u22124n,\nwhere the second to last line follows from the union bound. Furthermore, using Lemma 5.2 and 5.4 of [27], for any \u03b5 \u2208 [0, 1), it holds that\n|N\u03b5| \u2264 (1 + 2/\u03b5)n,\nand \u2016G\u2016 \u2264 (1\u2212 2\u03b5)\u22121 sup\nx\u2208N\u03b5 |x>Gx|.\nTaking the union bound over all the vectors in N1/4, we obtain\nP ( sup\nx\u2208N1/4 |x>Gx| > \u03bb\n) \u2264 |N1/4|4e\u22124n < 4e\u2212n.\nFinally, the statement is obtained by noticing that n \u2264 m. We prove a more general version of the theorem that allows the signal part to be rank R in\nAppendix E.\nD.2 Proof of Lemma 1 Proof. By definition,\n|||Y|||s\u2217 = sup {M (k)}Kk=1 \u3008Y , K\u2211 k=1 foldk(M (k)S(k) > )\u3009\ns.t. K\u2211 k=1 \u2016M (k)\u2016\u2217 \u2264 1\n= sup {M (k)}Kk=1 K\u2211 k=1 \u3008Y (k)S(k),M (k)\u3009\ns.t. K\u2211 k=1 \u2016M (k)\u2016\u2217 \u2264 1\n= max k \u2016Y (k)S(k)\u2016,\nwhere we used the Ho\u0308lder inequality in the last line.\nD.3 Proof of Theorem 2 First we decompose the error as\n|||X \u2217 \u2212 X\u0302 |||F \u2264 |||X \u2217 \u2212Xp|||F + |||Xp \u2212 X\u0302 |||F .\nThe first term is an approximation error that depends on the choice of the subspace S(k). The second term corresponds to an estimation error and we analyze the second term below.\nSince X\u0302 is the minimizer of (10) and Xp is feasible,\n1 2 |||Y \u2212 X\u0302 |||2F + \u03bb K\u2211 k=1 \u2016M\u0302 (k) \u2016\u2217 \u2264 1 2 |||Y \u2212 Xp|||2F + \u03bb K\u2211 k=1 \u2016M (k)p \u2016\u2217,\nfrom which we have\n1 2 |||Xp \u2212 X\u0302 |||2F \u2264 |||Y \u2212 Xp|||s\u2217|||Xp \u2212 X\u0302 |||s + \u03bb K\u2211 k=1 ( \u2016M (k)p \u2016\u2217 \u2212 \u2016M\u0302 (k) \u2016\u2217 ) . (15)\nNext we define \u2206k := M\u0302 (k) \u2212M (k)p \u2208 Rnk\u00d7H K\u22121 and define its orthogonal decomposition \u2206k = \u2206 \u2032 k + \u2206 \u2032\u2032 k as\n\u2206\u2032\u2032k := (Ink \u2212 P Up)\u2206k(IHK\u22121 \u2212 P Vp),\nwhere P Up and P Vp are projection matrices to the column and row spaces of M (k) p , respectively, and \u2206\u2032k := \u2206k \u2212\u2206\u2032\u2032k. The above definition allows us to decompose \u2016M\u0302 (k) \u2016\u2217 as follows:\n\u2016M\u0302 (k) \u2016\u2217 = \u2016M (k)p + \u2206\u2032\u2032k + \u2206\u2032k\u2016\u2217 \u2265 \u2016M (k)p \u2016\u2217 + \u2016\u2206\u2032\u2032k\u2016\u2217 \u2212 \u2016\u2206\u2032k\u2016\u2217. (16)\nMoreover,\n|||Xp \u2212 X\u0302 |||s \u2264 K\u2211 k=1 \u2016\u2206k\u2016\u2217 \u2264 K\u2211 k=1 (\u2016\u2206\u2032k\u2016\u2217 + \u2016\u2206\u2032\u2032k\u2016\u2217) (17)\nCombining inequalities (15)\u2013(17), we have\n1 2 |||Xp \u2212 X\u0302 |||2F \u2264 (|||Y \u2212 Xp|||s\u2217 + \u03bb) K\u2211 k=1 \u2016\u2206\u2032k\u2016\u2217 + (|||Y \u2212 Xp|||s\u2217 \u2212 \u03bb) K\u2211 k=1 \u2016\u2206\u2032\u2032k\u2016\u2217. (18)\nSince\n|||Y \u2212 Xp|||s\u2217 \u2264 \u03c3|||E|||s\u2217 + |||X \u2217 \u2212Xp|||s\u2217 ,\nif \u03bb \u2265 \u03c3|||E|||s\u2217 + |||X \u2217 \u2212 Xp|||s\u2217 , the second term in the right-hand side of inequality (18) can be\nignored and we have\n1 2 |||Xp \u2212 X\u0302 |||2F \u2264 2\u03bb K\u2211 k=1 \u2016\u2206\u2032k\u2016\u2217\n\u2264 2\u03bb K\u2211 k=1 \u221a 2rk\u2016\u2206\u2032k\u2016F\n\u2264 2\u03bb K\u2211 k=1 \u221a 2rk\u2016\u2206k\u2016F\n\u2264 2 \u221a 2\u03bb \u221a\u221a\u221a\u221a K\u2211 k=1 rk \u221a\u221a\u221a\u221a K\u2211 k=1 \u2016\u2206k\u20162F , (19)\nwhere in the second line we used a simple observation that rank(\u2206\u2032k) \u2264 2rk. Next, we relate the norm |||Xp \u2212 X\u0302 |||F to the sum \u2211K k=1 \u2016\u2206k\u20162F in the right-hand side of inequality (19). First suppose that \u2211K k=1 \u2016\u2206k\u20162F \u2264 |||Xp \u2212 X\u0302 ||| 2 F . Then from inequality (19), we have\n|||Xp \u2212 X\u0302 |||F \u2264 4 \u221a 2\u03bb \u221a\u221a\u221a\u221a K\u2211 k=1 rk\nby dividing both sides by |||Xp \u2212 X\u0302 |||F . On the other hand, if |||Xp \u2212 X\u0302 |||2F \u2264 \u2211K k=1 \u2016\u2206k\u20162F , we use the following lemma\nLemma 4. Suppose {M (k)p }Kk=1, {M\u0302 (k) }Kk=1 \u2208 M(\u03c1), and S(k) is constructed as a Kronecker product ofK\u22121 ortho-normal matrices P\u0302 (`) asS(k) = P\u0302 (k\u22121) \u2297\u00b7 \u00b7 \u00b7\u2297P\u0302 (k+1) , where (P\u0302 (`) )>P\u0302 (`) =\nIH for ` = 1, . . . , K. Then forXp = \u2211K k=1 foldk ( M (k)p S (k)> ) and X\u0302 = \u2211K k=1 foldk ( M\u0302 (k) S(k) >) , the following inequality holds:\n1\n2 K\u2211 k=1 \u2016\u2206k\u20162F \u2264 1 2 |||Xp \u2212 X\u0302 |||2F + \u03c1max k ( \u221a nk + \u221a HK\u22121) K\u2211 k=1 \u2016\u2206k\u2016\u2217. (20)\nProof. The proof is presented in Section D.5.\nCombining inequalities (18) and (20), we have\n1\n2 K\u2211 k=1 \u2016\u2206k\u20162F \u2264 ( |||Y \u2212 Xp|||s\u2217 + \u03c1max k ( \u221a nk + \u221a HK\u22121) + \u03bb ) K\u2211 k=1 \u2016\u2206\u2032k\u2016\u2217\n+ ( |||Y \u2212 Xp|||s\u2217 + \u03c1max k ( \u221a nk + \u221a HK\u22121)\u2212 \u03bb ) K\u2211 k=1 \u2016\u2206\u2032\u2032k\u2016\u2217.\nThus if we take \u03bb \u2265 \u03c3|||E|||s\u2217 + |||X \u2217 \u2212 Xp|||s\u2217 + \u03c1maxk( \u221a nk + \u221a HK\u22121), the second term in the right-hand side can be ignored and following the derivation leading to inequality (19) and dividing\nboth sides by \u221a\u2211K\nk=1 \u2016\u2206k\u20162F , we have\n|||Xp \u2212 X\u0302 |||F \u2264 \u221a\u221a\u221a\u221a K\u2211 k=1 \u2016\u2206k\u20162F \u2264 4 \u221a 2\u03bb \u221a\u221a\u221a\u221a K\u2211 k=1 rk,\nwhere the first inequality follows from the assumption. The final step of the proof is to bound the norm |||E|||s\u2217 with sufficiently high probability. By Lemma 1,\n|||E|||s\u2217 = max k \u2016E(k)S(k)\u2016.\nTherefore, taking the union bound, we have\nP (\nmax k \u2016E(k)S(k)\u2016 \u2265 t\n) \u2264 K\u2211 k=1 P ( \u2016E(k)S(k)\u2016 \u2265 t ) . (21)\nNow since each E(k)S(k) \u2208 Rnk\u00d7H K\u22121 is a random matrix with i.i.d. standard Gaussian entries, P ( \u2016E(k)S(k)\u2016 \u2265 \u221a nk + \u221a HK\u22121 + t ) \u2264 exp(\u2212t2/(2\u03c32)).\nTherefore, choosing t = maxk( \u221a nk + \u221a HK\u22121) + \u221a 2 log(K/\u03b4) in inequality (21), we have\nmax k \u2016E(k)S(k)\u2016 \u2264 max k ( \u221a nk + \u221a HK\u22121) +\n\u221a 2 log(K/\u03b4),\nwith probability at least 1\u2212 \u03b4. Plugging this into the condition for the regularization parameter \u03bb, we obtain what we wanted.\nD.4 Proof of Lemma 2 Proof. i) Let \u2297k\u2032\u2208[K]\\kU (k \u2032) denote U (1) \u2297 \u00b7 \u00b7 \u00b7 \u2297U (k\u22121) \u2297U (k+1) \u2297 \u00b7 \u00b7 \u00b7 \u2297U (K). We have\nX\u2217(k) = U (k)C(k) ( \u2297k\u2032\u2208[K]\\kU (k \u2032) )>\n= U (k)C(k) ( \u2297k\u2032\u2208[K]\\k(U (k \u2032))> )\n= P (k)\u039b(k)(Q(k))>.\nBecause of the minimality of the Tucker decomposition (5), X\u2217(k), C(k) and U (k) are all of rank R, for all k \u2208 [K]. Therefore, both C(k) and \u2297k\u2032\u2208[K]\\k(U (k \u2032))> have full row rank. Hence, C(k) has a Moore-Penrose pseudo inverse C \u2020 (k) such that C(k)C \u2020 (k) = I , and so does \u2297k\u2032\u2208[K]\\k(U (k \u2032))>. As a result, we have\nU (k) = P (k)\u039b(k)(Q(k))> ( \u2297k\u2032\u2208[K]\\k(U (k \u2032))> )\u2020 C\u2020(k).\nii) Similarly, we have Q(k)\u039b(k)(P (k))> = ( \u2297k\u2032\u2208[K]\\kU (k \u2032) ) C>(k)(U (k))>.\nBy the definition of SVD, \u039b is invertible and (P (k))>P (k) = I . Hence, Q(k) = ( \u2297k\u2032\u2208[K]\\kU (k \u2032) ) C>(k)(U (k))>P (k)(\u039b(k))\u22121.\nThis meansQ(k) \u2208 span ( \u2297k\u2032\u2208[K]\\kU (k \u2032) ) and we then concludeQ(k) \u2208 span ( \u2297k\u2032\u2208[K]\\kP (k \u2032) )\nusing (i).\nD.5 Proof of Lemma 4 Expanding Xp and X\u0302 , we have\n|||Xp \u2212 X\u0302 |||2F = ||| \u2211K k=1 foldk ( \u2206kS (k)>)|||2F =\nK\u2211 k=1 \u2016\u2206k\u20162F + \u2211 k 6=` \u3008foldk(\u2206kS(k) > ), fold`(\u2206`S (`)>)\u3009\n= K\u2211 k=1 \u2016\u2206k\u20162F + \u2211 k 6=` \u3008foldk(\u2206k)\u00d7k\u2032 6=k P\u0302 (k\u2032) , fold`(\u2206`)\u00d7`\u2032 6=` P\u0302 (`\u2032) \u3009\n= K\u2211 k=1 \u2016\u2206k\u20162F + \u2211 k 6=` \u3008foldk(\u2206k)\u00d7` P\u0302 (`) , fold`(\u2206`)\u00d7k P\u0302 (k) \u3009\n= K\u2211 k=1 \u2016\u2206k\u20162F \u2212 \u2211 k 6=` \u3008\u2206k(IH \u2297 \u00b7 \u00b7 \u00b7 \u2297 P\u0302 (`) \u2297 \u00b7 \u00b7 \u00b7 \u2297 IH)>, P\u0302 (k) (fold`(\u2206`))(k)\u3009\n\u2265 K\u2211 k=1 \u2016\u2206k\u20162F \u2212 \u2211 k 6=` \u2016\u2206k\u2016\u2217 \u00b7 \u2016(fold`(\u2206`))(k)\u2016\n\u2265 K\u2211 k=1 \u2016\u2206k\u20162F \u2212 2\u03c1max k ( \u221a nk + \u221a HK\u22121) K\u2211 k=1 \u2016\u2206k\u2016\u2217,\nfrom which the lemma holds. Here we regarded foldk(\u2206kS(k)) as a Tucker decomposition with the core tensor foldk(\u2206k) and factor matrices P\u0302 (k\u2032)\nfor k\u2032 6= k. Most of the factors except for k and ` cancel out when calculating the inner product between two such tensors in the third line, because (P\u0302 (k\u2032) )>P\u0302 (k\u2032)\n= IH . After unfolding the inner product at the kth mode in the fifth line, we notice that a multiplication by an ortho-normal matrix does not affect the nuclear norm or the spectral norm. In the last line we used {\u2206k}Kk=1 \u2208 M(2\u03c1), which follows from the assumption that both {M (k)p }Kk=1, {M\u0302 (k) }Kk=1 \u2208M(\u03c1)."}, {"heading": "E Generalization of Theorem 1 to the higher rank case", "text": "Theorem 4. Suppose that X = \u2211R\nr=1 \u03b2rurv > r , where u1, . . . ,uR \u2208 Rn and v1, . . . ,vR \u2208 Rm\nare unit orthogonal vectors respectively. Let X\u0303 = X + \u03c3E be the noisy observation ofX . There exists an universal constant C such that with probability at least 1\u2212 3e\u2212n, if m/n \u2265 C(\u03b21/\u03b2R)4, then\n| cos(U\u0302 ,U )| \u2265  1\u2212 Cmn (\u03b2R/\u03c3)4 , if \u03b21\u221a m < \u03c3 \u2264 \u03b2R (Cmn) 1 4 , 1\u2212 Cn(\u03b21/\u03b2R) 2\n(\u03b2R/\u03c3)2 , if \u03c3 \u2264 \u03b21\u221a m ,\notherwise, | cos(U\u0302 ,U)| \u2265 1\u2212 Cn(\u03b21/\u03b2R) 2\n(\u03b2R/\u03c3)2 if \u03c3 \u2264 \u03b22R/(Cn) 1 2\u03b21.\nSuppose thatX = \u2211R\nr=1 \u03b2 2 rurv > r and X\u0303 = X + \u03c3E. We consider the second moment of X\u0303:\nX\u0303X\u0303 > = R\u2211 r=1 \u03b22ruru > r + \u03c3 ( R\u2211 r=1 \u03b2r ( urv > r E > +Evru > r )) + \u03c32EE>\n= B\ufe37 \ufe38\ufe38 \ufe37 R\u2211 r=1 \u03b22ruru > r +m\u03c3 2I +\nG\ufe37 \ufe38\ufe38 \ufe37 \u03c32EE> \u2212m\u03c32I + \u03c3 ( R\u2211 i=1 \u03b2r ( urv > r E > +Evru > r )) .\nThe eigenvalue decomposition ofB can be written as\nB = [U U 2]\n[ \u03a3 +m\u03c32I\nm\u03c32I\n] [ U>\nU>2\n] ,\nwhere U \u2208 Rn\u00d7R and \u03a3 = diag(\u03b221 , . . . , \u03b22R). Similarly, the eigenvalue decomposition of X\u0303X\u0303 > can be written as\nX\u0303X\u0303 >\n= [U\u0302 U\u0302 2]\n[ \u03a3\u0302\n\u03a3\u0302\u2032\n][ U\u0302 >\nU\u0302 > 2\n] ,\nwhere \u03a3\u0302 = diag(\u03bb\u03021, . . . , \u03bb\u0302R) and \u03a3\u0302\u2032 = diag(\u03bb\u0302R+1, . . . , \u03bb\u0302n) s.t. \u03bb\u03021 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bb\u0302n are the eigenvalues of X\u0303X\u0303 T .\nWe first show a deterministic lower bound for | sin(U\u0302 ,U)| assuming \u03b22R \u2265 2\u2016G\u2016. Then we bound the spectral norm \u2016G\u2016 of the noise term (Lemma 3) and derive the sufficient condition for \u03b22R.\nThe maximum singular value of m\u03c32I is m\u03c32. The minimum singular value of \u03a3\u0302 is |\u03bb\u0302R|. By Wely\u2019s theorem, \u2016G\u2016 \u2265 |\u03bb\u0302R \u2212 \u03b22R \u2212m\u03c32|, which means\n\u03bb\u0302R \u2265 m\u03c32 + \u03b22R \u2212 \u2016G\u2016.\nLetR = GU\u0302 . Since \u03b22R \u2265 2\u2016G\u2016, we can apply the Wedin theorem and obtain\n| sin(U\u0302 ,U)| = \u2016U>2 U\u0302\u2016 \u2264 \u2016R\u2016 \u03b22R \u2212 \u2016G\u2016 = \u2016GU\u0302\u2016 \u03b22R \u2212 \u2016G\u2016 \u2264 \u2016G\u2016 \u03b22R \u2212 \u2016G\u2016 \u2264 2\u2016G\u2016 \u03b22R ,\nwhere we used the property that the spectral norm is sub-multiplicative and \u2016U\u0302\u2016 = 1 in the second to last step.\nTherefore,\n| cos(U\u0302 ,U)| \u2265 \u221a 1\u2212 4\u2016G\u2016 2\n\u03b24R \u2265 1\u2212 4\u2016G\u2016 2 \u03b24R ,\nif \u03b22R \u2265 2\u2016G\u2016. It follows from Lemma 5 (shown below) that with probability at least 1\u2212 3e\u2212n\n\u2016G\u2016 \u2264\n{ 2C\u0304\u03c32 \u221a mn, if \u03b21/\u03c3 < \u221a m,\n2C\u0304\u03c3 \u221a n\u03b21, otherwise,\nwhere C\u0304 is an universal constant. Let C = 16C\u03042. Now consider the first situation where m/n > C(\u03b21/\u03b2R)4. If \u03c3 > \u03b21\u221am , we have \u2016G\u2016 \u2264 \u03c32 2 (Cmn) 1 2 . Meanwhile, if \u03c3 \u2264 \u03b2R\n(Cmn) 1 4\n, then we have \u03b22R \u2265 \u03c32(Cmn) 1 2 \u2265 2\u2016G\u2016.\nCombining these two conditions we obtain the first case in the theorem. When \u03c3 \u2264 \u03b21\u221a m , we can see that \u2016G\u2016 \u2264 \u03c3 2 (Cn) 1 2\u03b21. Moreover, since m/n > C(\u03b21/\u03b2R)4, it is implied that \u03c3 \u2264 \u03b22R/(Cn) 1 2\u03b21 and thus \u03b22R \u2265 2\u2016G\u2016. This gives us the second case. On the other hand, if m/n \u2264 C(\u03b21/\u03b2R)4, we require \u03c3 \u2264 \u03b22R/(Cn) 1 2\u03b21 to obtain the last case in the theorem.\nLemma 5. LetG be constructed as in the proof of Theorem 4. If m \u2265 n, there exists an universal constant C\u0304 such that\n\u2016G\u2016 \u2264 C\u0304\u03c32 (\u221a mn+ \u221a n\u03b21/\u03c3 ) ,\nwith probability at least 1\u2212 3e\u2212n.\nProof. The proof is an \u03b5-net argument. Let \u03bb = 2\u03c32 (\u221a 4mn+ 4n+ \u221a R + 8n+ 4 \u221a Rn \u00b7 \u03b21/\u03c3 ) .\nThe goal is to control |x>Gx| for all the vectors x on the unit Euclidean sphere Sn\u22121. In order to do this, we first bound the probability of the tail event |x>Gx| > \u03bb, for any fixed x \u2208 Sn\u22121. Then\nwe bound the probability that |x>Gx| > \u03bb for all the vectors in a \u03b5-net N\u03b5. Finally, we establish the connection between supx\u2208N\u03b5 |x>Gx| and \u2016G\u2016.\nTo bound P(|x>Gx| > \u03bb) for a fix x \u2208 Sn\u22121, we expand x>Gx as\nx>Gx = \u03c32(\u2016z\u20162 \u2212m) + 2\u03c3 R\u2211 r=1 \u03b2r\u03b3r(u > r x),\nwhere z = E>x and \u03b3r = v>r z. It is easy to see that \u03b3r \u223c N (0, 1), z \u223c N (0, I) and \u2016z\u20162 is \u03c72 distributed with m degrees of freedom.\nLet \u03b3 = [\u03b31, . . . , \u03b3R] and \u03c9 = [u>1 x, . . . ,u > Rx]. We have\n|x>Gx| \u2264\u03c32 \u2223\u2223\u2016z\u20162 \u2212m\u2223\u2223+ 2\u03c3\u2223\u2223\u2223\u2223 R\u2211\nr=1\n\u03b2r\u03b3r(u > r x) \u2223\u2223\u2223\u2223 \u2264\u03c32\n\u2223\u2223\u2016z\u20162 \u2212m\u2223\u2223+ 2\u03c3 R\u2211 r=1 max r\u2208[R] |\u03b2r| \u00b7 |\u03b3r| \u00b7 |u>r x|\n\u2264\u03c32 \u2223\u2223\u2016z\u20162 \u2212m\u2223\u2223+ 2\u03c3\u03b21 \u00b7 \u2016\u03b3\u2016 \u00b7 \u2016\u03c9\u2016\n\u2264\u03c32 \u2223\u2223\u2016z\u20162 \u2212m\u2223\u2223+ 2\u03c3\u03b21 \u00b7 \u2016\u03b3\u2016,\nwhere we used the Cauchy-Schwarz inequality in the second to last line and the fact \u2016\u03c9\u2016 \u2264 1 in the last line. Note that \u03b31, . . . , \u03b3R are i.i.d standard Gaussian distributed so that \u2016\u03b3\u20162 is \u03c72 distributed with R degrees.\nFirst we bound the deviation of the \u03c72m term. By the corollary of Lemma 1 in [17], we have P( \u2223\u2223\u2016z\u20162 \u2212m\u2223\u2223 > \u03bb1) \u2264 2e\u22124n, (22)\nwhere \u03bb1 = 2( \u221a\n4mn+ 4n). Next we bound the \u03c72R term. Similarly, we have\nP(\u2016\u03b3\u20162 \u2212R > \u03bb2) \u2264 e\u22124n, (23)\nwhere \u03bb2 = 2( \u221a\n4Rn+ 4n). Combining inequalities (22) and (23), we have\nP(|x>Gx| > \u03bb) \u2264 P ( \u03c32 \u2223\u2223\u2016z\u20162 \u2212m\u2223\u2223+ 2\u03c3\u03b21\u2016\u03b3\u2016 > \u03c32\u03bb1 + 2\u03c3\u03b21\u221aR + \u03bb2)\n\u2264 P (\u2223\u2223\u2016z\u20162 \u2212m\u2223\u2223 > \u03bb1 \u2228 \u2016\u03b3\u2016 >\u221aR + \u03bb2)\n\u2264 P (\u2223\u2223\u2016z\u20162 \u2212m\u2223\u2223 > \u03bb1)+ P(\u2016\u03b3\u2016 >\u221aR + \u03bb2)\n\u2264 3e\u22124n,\nwhere the second to last line follows from the union bound.\nFurthermore, using Lemma 5.2 and 5.4 of [27], for any \u03b5 \u2208 [0, 1), it holds that\n|N\u03b5| \u2264 (1 + 2/\u03b5)n,\nand \u2016G\u2016 \u2264 (1\u2212 2\u03b5)\u22121 sup\nx\u2208N\u03b5 |x>Gx|.\nTaking the union bound over all the vectors in N1/4, we obtain\nP(\u2016G\u2016 \u2264 2\u03bb) \u2264 P ( sup\nx\u2208N1/4 |x>Gx| > \u03bb\n) \u2264 |N1/4|3e\u22124n < 3e\u2212n.\nFinally, the statement is obtained by noticing that n \u2264 m and R \u2264 n."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>We consider the problem of recovering a low-rank tensor from its noisy observation. Previ-<lb>ous work has shown a recovery guarantee with signal to noise ratioO(ndK/2e/2) for recovering<lb>a Kth order rank one tensor of size n\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 n by recursive unfolding. In this paper, we first<lb>improve this bound to O(nK/4) by a much simpler approach, but with a more careful analysis.<lb>Then we propose a new norm called the subspace norm, which is based on the Kronecker prod-<lb>ucts of factors obtained by the proposed simple estimator. The imposed Kronecker structure<lb>allows us to show a nearly ideal O(<lb>\u221a<lb>n+<lb>\u221a<lb>HK\u22121) bound, in which the parameter H controls<lb>the blend from the non-convex estimator to mode-wise nuclear norm minimization. Further-<lb>more, we empirically demonstrate that the subspace norm achieves the nearly ideal denoising<lb>performance even with H = O(1).", "creator": "LaTeX with hyperref package"}}}