{"id": "1503.06902", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2015", "title": "A Note on Information-Directed Sampling and Thompson Sampling", "abstract": "This note introduce three Bayesian style Multi-armed bandit algorithms: Information-directed sampling, Thompson Sampling and Generalized Thompson Sampling. The goal is to give an intuitive explanation for these three algorithms and their regret bounds, and provide some derivations that are omitted in the original papers.\n\n\n\nAcknowledgments\nThe research was funded by the NIH grant No. 4540080.\n\nThis work was supported by the NIH grant No. 112836. This work was supported by the NIH Grant No. 83306.", "histories": [["v1", "Tue, 24 Mar 2015 03:26:28 GMT  (19kb)", "http://arxiv.org/abs/1503.06902v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["li zhou"], "accepted": false, "id": "1503.06902"}, "pdf": {"name": "1503.06902.pdf", "metadata": {"source": "CRF", "title": "A Note on Information-Directed Sampling and Thompson Sampling", "authors": ["Li Zhou"], "emails": ["lizhou@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 3.\n06 90\n2v 1\n[ cs\n.L G\n] 2\n4 M\nar 2\nThis note introduce three Bayesian style Multi-armed bandit algorithms: Information-directed sampling, Thompson Sampling and Generalized Thompson Sampling. The goal is to give an intuitive explanation for these three algorithms and their regret bounds, and provide some derivations that are omitted in the original papers."}, {"heading": "1 Introduction", "text": "Amulti-armed bandit problem [1] is one of the sequential decision making problem. At each time the learner selects an action based on its current knowledge and arm-selection policy, and then receives reward of the action selected. Since the rewards of actions that are not selected are unknown, the learner needs to balance between exploit its current knowledge to select a best arm and explore potential best arms. In this note we describe three Bayesian style Multi-armed bandit algorithms: Information-Directed Sampling[2], Thompson Sampling[3] and Generalized Thompson Sampling[4]. Each of these three algorithms maintains a posterior distribution indicating the probability of each arm/policy being optimal. However they have different rules to update this posterior distribution based on observed rewards."}, {"heading": "2 Information-Directed Sampling", "text": ""}, {"heading": "2.1 Problem Formulation", "text": "Information-Directed Sampling (IDS) [2] consider a Bayesian formulation of Multi-armed bandit problem. In this setting there is a set of actions (arms) A, and at time t \u2208 [1, T ] the decision-maker chooses an action at. Action at then draws a reward ra,t from a reward distribution pa\n1. We assume that all rewards are i.i.d distributed and the reward distribution is stationary with respect to time t \u2208 [1, T ].\nTo formulate Multi-armed bandit in a Bayesian way, We denote a\u2217 = argmaxa\u2208A Era\u223cpa [ra], which means a\u2217 is the arm with highest expected reward with respect to distribution pa, where a \u2208 A. We also denote ra\u2217 the reward drawn from pa\u2217 . The decision-maker do not know the real\n1In the original paper they assume that the arms will first draw an outcome from an outcome distribution, then here is a fixed and known function that maps outcomes to rewards. However here for the sake of simplicity, we assume the outcome is equal to the reward.\nreward distribution pa, so it has its own estimate about these distributions at time step t, which we denote as p\u0302a,t. Because of this uncertainly, for each action a at time t, the decision-maker has a believe on whether this action has the highest expected reward. We denote this believe by \u03b1t(a) = P (a\n\u2217 = a|Ft\u22121), where Ft\u22121 is the history of past observations including the actions selected and the corresponding rewards. The decision-maker will update this posterior distribution at each time step based on Ft\u22121.\nInstead of sampling actions directly based on posterior distribution \u03b1t, IDS sample actions based on a distribution \u03c0. \u03c0 is also a distribution over all actions and is constructed based on the posterior distribution \u03b1t. We are interested in the following expected regret\nE[Regret(T )] = E ra\u2217\u223cpa\u2217\nT \u2211\nt=1 ra\u2217 \u2212 Ea\u223c\u03c0 ra,t\u223cpa\nT \u2211\nt=1\nra,t (1)"}, {"heading": "2.2 Algorithm", "text": "In multi-armed bandit problem, we want to balance between exploitation and exploration. IDS handle this trade-off by defining immediate regret \u25b3t(a) and information gain gt(a) of action a at time t."}, {"heading": "2.2.1 Immediate Regret", "text": "The immediate regret \u25b3t(a) is defined as\n\u25b3t(a) = E a\u2217\u223c\u03b1t\nra\u2217,t\u223cp\u0302a\u2217,t\n[ra\u2217,t|Ft\u22121]\u2212 E ra,t\u223cp\u0302a,t [ra,t|Ft\u22121] (2)\nThe idea behind this is that: the regret is defined by formula (1), however the decision-maker does not know the true pa\u2217 and pa for a \u2208 A, so it uses p\u0302a\u2217 and p\u0302a instead to estimate the regret at time step t. Note that\nP (ra\u2217,t = r) = P (ra,t = r|a\u2217 = a) (3)\nSo\nE[ra\u2217,t|Ft\u22121] = E[ra,t|rb,t \u2264 ra,t \u2200b,Ft\u22121] (4)\nWe will show how to calculate each of these terms in section 2.3."}, {"heading": "2.2.2 Information Gain", "text": "Instead of doing pure exploitation using immediate regret, one would want to do some exploration to seek potential best arms. To do this, IDS defined a term: information gain, denoted as gt(a). The idea is that: we already have a posterior distribution over a\u2217, we hope that after we pull one of the arms, the entropy of this distribution decreases, so that we gain a certain amount of information about which arm has the highest expected reward. Let a\u2217t \u223c \u03b1t and a\u2217t+1 \u223c \u03b1t+1, and let H(a\u2217t ) denote the entropy of a\u2217t , then gt(a) is defined as\ngt(a) = E[H(a \u2217 t )\u2212H(a\u2217t+1)|Ft\u22121, at = a] (5)\nThe expectation is with respect to the random reward of arm a. To calculate this, one can sample reward from p\u0302a and then calculate the expectation above. However in the original paper they used the following way.\nFrom the property of mutual information we have:\nH(X) \u2212H(X|Y ) = I(X,Y ) (6)\nand since E[H(a\u2217t+1)|Ft\u22121, at = a] = H(a\u2217t |ra,t), So\ngt(a) = I(a \u2217 t , ra,t) (7)\nAlso from the property of mutual information we have:\nI(X,Y ) = EDKL(P (Y |X)||P (Y )) (8)\nSince we do not have the true distribution of ra,t, we use the posterior distribution p\u0302a,t, and we have:\ngt(a) = E a\u2032\u223c\u03b1t\nDKL(p\u0302a,t(\u00b7|a\u2032)||p\u0302a,t) (9)\nIn the equation above, p\u0302a,t is just the reward posterior distribution of arm a at time t, and p\u0302a,t(\u00b7|a\u2032) is the reward posterior distribution conditioned on that a\u2032 is the arm that has the highest mean reward. With this condition, the reward posterior distribution has to shift to satisfy this constrain. For example in Figure 1, we show 3 arms with mean reward as Gaussian distribution, suppose we want to calculate the reward posterior distribution of arm 2 and 3 conditioned on that arm 1 has the highest mean reward. We examine one point where the mean reward of arm 1 is 0.8. Then the mean reward of arm 2 and arm 3 cannot be greater than 0.8, so the probability mass of these two arms that is greater than 0.8 has to be cut off, and the remaining has to be normalized."}, {"heading": "2.2.3 Optimization", "text": "The goal of IDS at a single time step is to balance immediate regret \u25b3t(a) and information gain gt(a). There are many ways to do this, and in the paper the author choose the following way:\n\u03c0IDSt = argmin\u03c0\u2208D(A)\n{\n\u03a8t(\u03c0) := \u25b3t(\u03c0)2 gt(\u03c0)\n}\n(10)\nNote that \u03c0 is a distribution over all arms, and assuming g has at least 1 non-zero elements, then to find \u03a8t(\u03c0) it is equal to solve the following optimization problem:\nminimize \u03a8(\u03c0) := (\u03c0T\u25b3)2 \u03c0T g\n(11)\nsubject to \u03c0T e = 1 (12)\n\u03c0 \u2265 0 (13)\nThe author stated that \u03c0 can be very sparse, with only two non-zero elements, and then they try all possible combinations of two arms that gives the lowest \u03a8t(\u03c0). Given \u03c0, IDS sample an arm and pull that arm. I omit the detail here since it\u2019s well described in the IDS paper."}, {"heading": "2.3 Bernoulli Bandit Experiment", "text": "In a K-armed Bernoulli bandit problem, there are K arms, and the reward of the i-th arm follows a Bernoulli distribution with mean Xi. In a Bayesian style learning algorithm, it is standard to model the mean reward of each arm using the Beta distribution:\nXi \u223c Beta(\u03b21i , \u03b22i ) (14) ri \u223c Bernoulli(Xi) (15)\nTo calculate \u25b3t(a) and gt(a), we first calculate \u03b1t(a). Let fi = Beta.pdf(x|\u03b21i , \u03b22i ) and Fi = Beta.cdf(x|\u03b21i , \u03b22i ) for all arm i, that is, fi and Fi are the PDF and CDF of the posterior distribution\nof Xi, then to calculate \u03b1t:\n\u03b1t(a) = P\n\n\n\u22c2\nj 6=i\n{Xj \u2264 Xi}\n\n (16)\n=\n\u222b 1\n0 fi(x)P\n\n\n\u22c2\nj 6=i\n{Xj \u2264 Xi}|Xi = x\n\n dx (17)\n=\n\u222b 1\n0 fi(x)\n\n\n\u220f\nj 6=i\nFj(x)\n\n dx (18)\n=\n\u222b 1\n0\n[\nfi(x) Fi(x)\n]\nF\u0304 (x)dx (19)\nwhere F\u0304 (x) = \u220fK\ni=1 Fi(x). To calculate this integral, we need to sample points from fi, Fi and F\u0304i, and then do summation, so it is quite time consuming.\nNext we need to calculate p\u0302a,t(\u00b7|a\u2217 = a), which is the same as calculating Mij := E[Xj |Xk \u2264 Xi \u2200k]\nMij = E[Xj |Xk \u2264 Xi \u2200k] (20)\n=\n\u222b 1\n0 xP (Xj = x|Xk \u2264 Xi \u2200k) (21)\n=\n\u222b 1 0 x P (Xj = x,Xk \u2264 Xi \u2200k) P (Xk \u2264 Xi \u2200k) dx (22)\nSuppose i 6= j, then\n(22) = 1\n\u03b1t(i)\n\u222b 1\n0 xP (Xk \u2264 Xi \u2200k 6= j,Xj = x,Xi \u2265 x)dx (23)\n= 1\n\u03b1t(i)\n\u222b 1\n0 xP (Xj = x)P (Xk \u2264 Xi \u2200k 6= i or j,Xi \u2265 x)dx (24)\n= 1\n\u03b1t(i)\n\u222b 1\n0 xP (Xj = x)\n\u222b 1\nx P (Xk \u2264 y \u2200k 6= i or j)P (Xi = y)dydx (25)\n= 1\n\u03b1t(i)\n\u222b 1\n0 xP (Xj = x)\n\u222b 1\nx\n(\nfi(y)F\u0304 (y)\nFi(y)Fj(y)\n)\ndydx (26)\n= 1\n\u03b1t(i)\n\u222b 1\n0\n(\nfi(y)F\u0304 (y)\nFi(y)Fj(y)\n)\u222b y\n0 xfj(x)dxdy (27)\n= 1\n\u03b1t(i)\n\u222b 1\n0\n(\nfi(y)F\u0304 (y)\nFi(y)Fj(y)\n)\nQj(y)dy (28)\nWhere Qj(y) = \u222b y 0 xfj(x)dx. To calculate Qj(y) we also need to do sampling and then summation.\nSuppose i = j, then\n(22) = 1\n\u03b1t(t)\n\u222b 1\n0 xP (Xi = x,Xk \u2264 x \u2200k 6= i) (29)\n= 1\n\u03b1t(i)\n\u222b 1\n0 xfi(x)\n\u220f\nj 6=i\nFj(x)dx (30)\n= 1\n\u03b1t(i)\n\u222b 1\n0\nxfi(x)\nFi(x) F\u0304 (x)dx (31)\nNow that we have \u03b1t(a) and Mij = E[Xj |Xk \u2264 Xi \u2200k], we can calculate \u25b3t(a) and gt(a).\n\u03c1\u2217 = K \u2211\ni=1\n\u03b1t(i)Mii (32)\n\u25b3t(i) = \u03c1\u2217 \u2212 \u03b21i\n\u03b21i + \u03b2 2 i\n(33)\ngi =\nK \u2211\ni=1\n\u03b1jKL\n(\nMji|| \u03b21i\n\u03b21i + \u03b2 2 i\n)\n(34)\nWhere KL(p1||p2) is defined as KL(p1||p2) = p1 log(p1p2 ) + (1 \u2212 p1) log( 1\u2212p1 1\u2212p2\n) since p\u0302a,t follows Bernoulli distribution.\nAt each time step, we can calculate \u25b3t(a) and gt(a) by the above procedure and then solve the optimization problem to get \u03c0, and sample an arm based on \u03c0."}, {"heading": "2.4 Regret Bound", "text": "Here we prove a general regret bound, for specific regret bound, we can refer to the IDS paper. For a fixed deterministic \u03bb \u2208 R and a policy \u03c0 such at \u03a8t(\u03c0t) \u2264 \u03bb, we have\nE[Regret(T, \u03c0)] \u2264 \u221a\n\u03bbH(\u03b11)T (35)\nProve:\nE\nT \u2211\nt=1\ngt(\u03c0t) = E T \u2211\nt=1\nE[H(\u03b1t)\u2212H(\u03b1t+1)|Ft\u22121] (36)\n= E\nT \u2211\nt=1\n(H(\u03b1t)\u2212H(\u03b1t+1)) (37)\n= H(\u03b11)\u2212 EH(\u03b1T+1) (38) \u2264 H(\u03b11) (39)\nBy definition, \u03a8t(\u03c0) \u2264 \u03bb, so \u25b3t(\u03c0) \u2264 \u221a \u03bbgt(\u03c0), so\nE(Regret(T, \u03c0)) = E\nT \u2211\nt=1\n\u25b3t(\u03c0) (40)\n\u2264 \u221a \u03bbE T \u2211\nt=1\n\u221a\ngt(\u03c0) (41)\n\u2264 \u221a \u03bbT\n\u221a \u221a \u221a \u221a\nE\nT \u2211\nt=1\ngt(\u03c0) Caushy-Schwardsz inequality (42)\n\u2264 \u221a\n\u03bbH(\u03b11)T (43)\nIn the paper, the author proved that \u03a8\u2217t \u2264 |A/2|, so E(Regret(T, \u03c0IDS)) \u2264 \u221a 1 2 |A|H(\u03b11)T"}, {"heading": "2.5 Potential Problems", "text": "IDS showed a strong empirical results, however there are several potential problems. I think the main problem is that the algorithm is very time consuming as I run it, the reason is that it has 3 integral to calculate so we have to evaluate each integrand at a discrete grid of points. Another problem is that the paper didn\u2019t mention why they choose such format of \u03a8 as the trade-off between \u25b3t and gt, since there are many ways to make this trade-off. Also it would be nice to see some generalization to contextual bandit."}, {"heading": "3 Thompson Sampling", "text": ""}, {"heading": "3.1 Problem Formulation", "text": "Thompson sampling (TS) [3, 5] is also a Bayesian style bandit algorithm, it can apply to both contextual bandit and standard Multi-armed bandit problems. Here we talk about the non-contextual version. Again, we assume there is an action set A, and at time step t Thompson sampling select action a and get reward ra,t. We also assume the reward of each arm ra follows some parametric distribution pa = P (r|a, \u03b8a) with mean \u00b5a, where \u03b8a is the parameter. Define past observations D consists of arms pulled and rewards observed. At the beginning, Thompson sampling assumes a prior distribution on parameters \u03b8a, and then after each time step, it will update the posterior distribution P (\u03b8a|D) based on past observations. Similar to IDS, the goal is to minimize the regret:\nE[Regret(T )] = E ra\u2217\u223cpa\u2217\nT \u2211\nt=1\nra\u2217 \u2212 E ra,t\u223cpa\nT \u2211\nt=1\nra,t (44)\nwhere a\u2217 is the arm with the highest expected reward, and a is the arm selected by Thompson sampling."}, {"heading": "3.2 Algorithm", "text": "Similar to IDS, Thompson sampling randomly select an action a according to its probability of being optimal. So action a is chosen with probability\n\u222b\nI\n[\nE(r|a, \u03b8) = max a\u2032\nE(r|a\u2032, \u03b8) ] P (\u03b8|D)d\u03b8 (45)\nWhich is essential the same as the \u03b1t in IDS. However calculating \u03b1t is time consuming, and since in Thompson sampling, we do not need to use \u03b1t explicitly, and we only need samples from \u03b1t, so it suffices to draw a random parameter \u03b8 from posterior distribution. Algorithm 1 describes the procedure of Thompson sampling with Bernoulli bandit problem.\nAlgorithm 1 Thompson sampling with Bernoulli multi-armed bandit Require: \u03b1, \u03b2: prior parameter of a Beta distribution For each arm i = 1, ...,K set Si = 0, Fi = 0 for t = 1, ..., T do\nfor arm i = 1, ...,K do Draw \u03b8i from Beta(\u03b1+ Si, \u03b2 + Fi) end for Play arm a = argmaxi \u03b8i, and observe reward rt if rt = 1 then Sa = Sa + 1 else Fa = Fa + 1 end if\nend for"}, {"heading": "3.3 Regret", "text": "Although Thompson sampling is a very old algorithm, proposed by [6], but the theoretical analysis is done very recently. We follow [5] and hope to give a intuitive explanation of the regret. Let \u00b5\u2217 = maxi \u00b5i and \u25b3i = \u00b5\u2217 \u2212 \u00b5i, where i \u2208 A, and let ki(t) denote the number of times arm i has been played up to step t\u2212 1. Then the expected total regret in time T + 1 can be written as\nE[Regret(T )] = \u2211\ni\n\u25b3iE(ki(T + 1)) (46)\nHence to bound the expected regret, we need to bound E(ki(T + 1)) for all i \u2208 A. To bound ki(T + 1) we need the following settings [5]: Define F B n,p(\u00b7) the cdf and fBn,p(\u00b7) the pdf of the binomial distribution with parameters n, p. Define F beta\u03b1,\u03b2 (\u00b7) the cdf of beta distribution with parameters \u03b1, \u03b2. Let i(t) denote the arm played at time t, ki(t) denotes the number of plays of arm i until time t\u2212 1, Si(t) denote the number of successes among the plays of arm i until t\u2212 1 for the Bernoulli bandit case, \u00b5\u0302(i) denote the empirical mean and \u03b8i(t) denote the sample mean reward of arm i at time t. We assume the first arm is the unique optimal arm, i.e,\u0307 \u00b5\u2217 = \u00b51. For each arm i, we will choose two thresholds xi and yi such that \u00b5i < xi < yi < \u00b51. With different choices of xi and yi, we can get problem dependent and problem independent bound respectively. We also define E\u00b5i (t) as the event that \u00b5\u0302i(t) \u2264 xi and E\u03b8i (t) as the event that \u03b8i(t) \u2264 yi. Finally,\ndefine Ft\u22121 = {i(w), ri(w)(w), w = 1, ..., t \u2212 1} and pi,t = P (\u03b81(t) > yi|Ft\u22121). pi,t indicates what is the probability of the sample reward of arm 1 is greater than yi at time t.\nWe can decompose E(ki(T + 1)) into\nE[ki(T + 1)] =\nT \u2211\nt=1\nP (i(t) = i) (47)\n=\nT \u2211\nt=1\nP (i(t) = i, E\u00b5i (t), E \u03b8 i (t)) (48)\n+ T \u2211\nt=1\nP (i(t) = i, E\u00b5i (t), E \u03b8 i (t)) (49)\n+\nT \u2211\nt=1\nP (i(t) = i, E\u00b5i (t)) (50)\nSo we need to bound (48), (49) and (50) respectively. To bound (48), [5] proved that\nP (i(t) = i, E\u00b5i (t), E \u03b8 i (t)|Ft\u22121) \u2264 (1\u2212 pi,t) pi,t P (i(t) = 1, E\u00b5i (t), E \u03b8 i (t)|Ft\u22121) (51)\nand so\nT \u2211\nt=1\nP (i(t) = i, E\u00b5i (t), E \u03b8 i (t)) =\nT \u2211\nt=1\nEP (i(t) = i, E\u00b5i (t), E \u03b8 i (t)|Ft\u22121) (52)\n\u2264 T \u2211\nt=1\nE\n[\n(1\u2212 pi,t) pi,t P (i(t) = 1, E\u00b5i (t), E \u03b8 i (t)|Ft\u22121)\n]\n(53)\n\u2264 T\u22121 \u2211\nk=0\nE\n[\n1\npi,\u03c4k+1 \u2212 1\n]\n(54)\nwhere \u03c4k denotes the time step at which arm 1 is played for the k th time. (54) only involves pi,\u03c4k+1 because the posterior distribution of the parameters of arm 1 only changes when arm 1 gets pulled. Now we need to bound (54). Let k1(t) = j and S1(t) = s, from the fact that F beta\u03b1,\u03b2 (y) = 1\u2212 FB\u03b1+\u03b2\u22121,y(\u03b1\u2212 1) we have pi,t = P (\u03b81(t) > yi) = FBj+1(s), and since\nS1(t) \u223c Binomial(k1(t), \u00b51) (55) \u03b81(t) \u223c Beta(S1(t), k1(t)\u2212 S1(t)) (56)\nso each possible value S1(t) = s corresponding to a value of pi,\u03c4j+1 = F B j+1,y(s) with probability fBj,\u00b51(s), so\nE\n[\n1\npi,\u03c4k+1 \u2212 1\n]\n=\nj \u2211\ns=0\nfBj,\u00b51(s)\nFBj+1,y(s) (57)\nSo we have reduced the problem of bounding (54) to the problem of bounding a summation of a series of random variables involving binomial distribution. [5] provide details about how to bound (57), which is quite complicated.\nNow we bound (50). Let \u03c4k denote the time at which k th trial of arm i happens, and \u03c40 = 0.\nWe have\nT \u2211\nt=1\nP (i(t) = i, E\u00b5i (t)) \u2264 E\n\n\nT\u22121 \u2211\nk=0\n\u03c4k+1 \u2211\nt=\u03c4k+1\nI(i(t) = i)I(E\u00b5i (t))\n\n (58)\nSince E\u00b5i (t) doesn\u2019t change unless arm i is pulled, and \u2211\u03c4k+1 t=\u03c4k+1 I(i(t) = i) = 1, so (58) is equal to\n(58) = E\n\n\nT\u22121 \u2211\nk=0\nI(E\u00b5i (\u03c4k + 1))\n\u03c4k+1 \u2211\nt=\u03c4k+1\nI(i(t) = i)\n\n (59)\n= E\n[\nT\u22121 \u2211\nk=0\nI(E\u00b5i (\u03c4k + 1))\n]\n(60)\n\u2264 1 + E [ T\u22121 \u2211\nk=1\nI(E\u00b5i (\u03c4k + 1))\n]\n(61)\n\u2264 1 + T\u22121 \u2211\nk=1\nexp(\u2212kd(xi, \u00b5i)) (62)\n\u2264 1 + 1 d(xi, \u00b5i)\n(63)\nWhere the second last inequality is from Chernoff bound and d(x, y) = x ln xy + (1\u2212 x) ln (1\u2212x) (1\u2212y) .\nSimilarly, [5] bound\nT \u2211\nt=1\nP (i(t) = i, E\u03b8i (t), E \u00b5 i (t)) \u2264 Li(T ) + 1 (64)\nwhere Li(T ) = lnT d(xi,yi) Together with this three bounds and a choice of xi and yi for all i \u2208 A, we\ncan get a problem independent bound O( \u221a NT lnN)."}, {"heading": "4 Generalized Thompson Sampling", "text": ""}, {"heading": "4.1 Problem Formulation", "text": "Generalized Thompson Sampling[4] is a contextual bandit problem, it is similar to expert-learning framework, and include Thompson Sampling as a special case. Let X and A be the set of context and arms, and let K = |A|. At time step t \u2208 1, ..., T , the decision-maker observes the context xt \u2208 X and selects an arm at \u2208 A. Then it receives reward rt \u2208 {0, 1}, with expectation \u00b5(xt, at). In [4] the reward is binary, but it is easy to generalize to continuous space. Different from classic Thompson Sampling algorithm, Generalized Thompson Sampling allows the decision-maker to have access to a set of experts E = {E1, E2, ..., En}, each E makes predicts about the average reward \u00b5(xt, at). Let fi be the associated prediction function of expert Ei, the arm-selection policy is\nEi(x) = maxa\u2208A fi(x, a). Each expert could be a generalized linear model or other prediction model. The regret is defined as\nE [Regret(T )] = max 1\u2264i\u2264N\nN \u2211\nt=1\n\u00b5(xt, Ei(xi))\u2212 E [ T \u2211\nt=1\n\u00b5(xt, at)\n]\n(65)\nThat is, we are competing with the best expert."}, {"heading": "4.2 Algorithm", "text": "Generalized Thompson Sampling is described in Algorithm 2. We can see that it updates the weight wi,t+1 by wi,t+1 \u221d wi,t exp(\u2212\u03b7\u2113(fi(xt, at), rt)), where \u2113 is the loss function. The term \u2018Generalized\u2019 in \u2018Generalized Thompson Sampling\u2019 means that we can use different types of loss functions when updating wi. [4] described two loss functions: logarithmic loss and square loss. Logarithmic loss is defined as \u2113(r\u0302, r) = 1(r = 1) ln 1/r\u0302 + 1(r = 0) ln(1/(1 \u2212 r\u0302)), and square loss is defined as \u2113(r\u0302, r) = (r\u0302 \u2212 r)2. In next section, we will show that if the loss function is logarithmic loss, then Generalized Thompson Sampling takes the form of Thompson Sampling.\nAlgorithm 2 Generalized Thompson Sampling Require: \u03b7 > 0, \u03b3 > 0, E1, ..., EN , prior p For each expert i = 1, ..., N set w1 = p, W1 = ||w1||1 for t = 1, ..., T do\nReceive context xt \u2208 X for arm a = 1, ...,K do\nP (a) = (1\u2212 \u03b3)\u2211Ni=1 wi,t1(Ei(xt)=a) Wt + \u03b3K\nend for Select arm at based on P (a), observe reward rt, update weights: \u2200i : wi,t+1 = wi,t exp(\u2212\u03b7\u2113(fi(xt, at), rt));Wt+1 = \u2211\niwi,t+1 end for"}, {"heading": "4.3 Connection with Expert-Learning and Thompson Sampling", "text": "Generalized Thompson sampling has the format of expert exponential weighting, however it also fits Thompson sampling framework, there are two ways to see this, and in both ways we need to assume the loss is log loss, that is if an expert f predicts that the probability of r = 1 is p1 and the probability of r = 0 is 1\u2212 p1, then the log loss of expert f is ln 1p1 when reward is 1, and is ln 1 1\u2212p1 when reward is 0. The first way to see this: we can think of Generalized Thompson Sampling as maintaining a posterior distribution of the weight of each expert, denoted as wt. This posterior distribution may be interpreted as the posterior probability that fi is the reward-maximizing expert. The update rule, for one step, is\nwi,t+1 \u221d wi,t exp(\u2212\u2113(fi(xt, at), rt)) (66)\n\u221d wi,t exp(\u2212 ln( 1\np(rt|xt, at) )) (67)\n\u221d wi,tpi(rt|xt, at) (68)\nLet f\u2217 = fi be the event that fi is the reward-maximizing expert. From Bayesian rule we have, for one step\nP (f\u2217t+1 = fi|xt, at, rt) \u221d P (r = rt|f\u2217 = fi, xt, at)P (f\u2217 = fi) (69) \u221d wi,tpi(rt|xt, at) (70)\nWe can see that the update rule and Bayesian rule take the same format. Finally, the posterior distribution on f\u2217t is\nP (f\u2217t = fi) = wi,t \u2211\ni wi,t (71)\nWe can also see it from a second way. Let yt, xt and rt be the selected arm, context and reward in time t, yt, xt, rt be the selected arms, contexts and rewards in time 1, ..., t respectively, then from Bayesian rule we have\np(yt|yt\u22121, rt\u22121, xt) = p(yt\u22121, yt, r t\u22121, xt)\np(yt\u22121, rt\u22121, xt) (72) = p(yt|rt\u22121, xt) p(yt\u22121|rt\u22121, xt) (73)\nAssume we have a uniform mixture of the distribution defined by the experts (Note that we are assuming uniform mixture over yt and yt\u22121, not yt), then we have\np(yt|rt\u22121, xt) p(yt\u22121|rt\u22121, xt) =\n\u2211\nf f(y t|rt\u22121, xt)\n\u2211 f f(y t\u22121|rt\u22121, xt) (74)\nFrom update rule we have:\np(yt|yt\u22121, rt\u22121, xt) = \u2211 f wf,t\u22121f(yt|xt) Wt\u22121\n(75)\n=\n\u2211\nf wf,t\u22121f(yt|xt) \u2211\nf wf,t\u22121 (76)\n=\n\u2211\nf w0 pf (r1|y1, x1) pf (r2|y2, x2, r1)... pf (rt\u22121|yt\u22121, xt\u22121, rt\u22122) f(yt|xt) \u2211\nf w0 pf (r1|y1, x1) pf (r2|y2, x2, r1)... pf (rt\u22121|yt\u22121, xt\u22121, rt\u22122) (77)\n=\n\u2211\nf f(y t, xt\u22121, rt\u22121|xt)\n\u2211\nf f(y t\u22121, xt\u22121, rt\u22121)\n(78)\n=\n\u2211\nf f(y t|xt, rt\u22121)\n\u2211 f f(y t\u22121|xt\u22121, rt\u22121) (79)\nSo we can see that the update rule and Bayesian rule have the same format. However notice that in this view we are conditioned on xt while in the first view the posterior distribution of wt is conditioned on the xt\u22121."}, {"heading": "4.4 Regret", "text": "The basic idea of the derivation is that we assume a connection between the loss function and the regret: Define immediate regret \u25b3i(x) = \u00b5(x, E\u2217(x)) \u2212 \u00b5(x, Ei(x)), shifted loss of expert i l\u0302i(r|x, a) = \u2113(fi(x, a), r) \u2212 \u2113(f\u2217(x, a), r), and average shifted loss l\u0304 = Ert,at [ \u2211 iwi,tl\u0302i(rt|xt, at) ] , we assume there is a constant k1, such that \u25b3i(xt) \u2264 k1 \u221a l\u0304t. Also we make use of the self-boundedness\nproperty of the loss function: Er\n[ l\u0302i(r|x, a)2 ] \u2264 k2Er [ l\u0302i(r|x, a) ] , which means the second moment\nis bounded by the first moment of the shifted loss. Then we can bound the expected regret by\n\u221a 4k2(e\u2212 2)k1(1\u2212 \u03b3) \u221a\nT \u00b7 ln 1 p1 + \u03b3T (80)\nDifferent loss has different choice of k1 and k2, and [4] proved that with square loss the expected regret bound is O( \u221a\nln 1p1K 1/3T 2/3) and with logarithmic loss the expected regret bound is\nO( \u221a\nln 1p1K 2/3T 2/3)."}], "references": [{"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S\u00e9bastien Bubeck", "Nicolo Cesa-Bianchi"], "venue": "arXiv preprint arXiv:1204.5721,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Learning to optimize via information-directed sampling", "author": ["Dan Russo", "Benjamin Van Roy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "An empirical evaluation of thompson sampling", "author": ["Olivier Chapelle", "Lihong Li"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Generalized thompson sampling for contextual bandits", "author": ["Lihong Li"], "venue": "arXiv preprint arXiv:1310.7163,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Further optimal regret bounds for thompson sampling", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["William R Thompson"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1933}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction Amulti-armed bandit problem [1] is one of the sequential decision making problem.", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": "In this note we describe three Bayesian style Multi-armed bandit algorithms: Information-Directed Sampling[2], Thompson Sampling[3] and Generalized Thompson Sampling[4].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "In this note we describe three Bayesian style Multi-armed bandit algorithms: Information-Directed Sampling[2], Thompson Sampling[3] and Generalized Thompson Sampling[4].", "startOffset": 128, "endOffset": 131}, {"referenceID": 3, "context": "In this note we describe three Bayesian style Multi-armed bandit algorithms: Information-Directed Sampling[2], Thompson Sampling[3] and Generalized Thompson Sampling[4].", "startOffset": 165, "endOffset": 168}, {"referenceID": 1, "context": "1 Problem Formulation Information-Directed Sampling (IDS) [2] consider a Bayesian formulation of Multi-armed bandit problem.", "startOffset": 58, "endOffset": 61}, {"referenceID": 2, "context": "1 Problem Formulation Thompson sampling (TS) [3, 5] is also a Bayesian style bandit algorithm, it can apply to both contextual bandit and standard Multi-armed bandit problems.", "startOffset": 45, "endOffset": 51}, {"referenceID": 4, "context": "1 Problem Formulation Thompson sampling (TS) [3, 5] is also a Bayesian style bandit algorithm, it can apply to both contextual bandit and standard Multi-armed bandit problems.", "startOffset": 45, "endOffset": 51}, {"referenceID": 5, "context": "3 Regret Although Thompson sampling is a very old algorithm, proposed by [6], but the theoretical analysis is done very recently.", "startOffset": 73, "endOffset": 76}, {"referenceID": 4, "context": "We follow [5] and hope to give a intuitive explanation of the regret.", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "To bound ki(T + 1) we need the following settings [5]: Define F B n,p(\u00b7) the cdf and fB n,p(\u00b7) the pdf of the binomial distribution with parameters n, p.", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": "To bound (48), [5] proved that P (i(t) = i, E i (t), E \u03b8 i (t)|Ft\u22121) \u2264 (1\u2212 pi,t) pi,t P (i(t) = 1, E i (t), E \u03b8 i (t)|Ft\u22121) (51) and so T \u2211", "startOffset": 15, "endOffset": 18}, {"referenceID": 4, "context": "[5] provide details about how to bound (57), which is quite complicated.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Similarly, [5] bound T \u2211", "startOffset": 11, "endOffset": 14}, {"referenceID": 3, "context": "1 Problem Formulation Generalized Thompson Sampling[4] is a contextual bandit problem, it is similar to expert-learning framework, and include Thompson Sampling as a special case.", "startOffset": 51, "endOffset": 54}, {"referenceID": 3, "context": "In [4] the reward is binary, but it is easy to generalize to continuous space.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "[4] described two loss functions: logarithmic loss and square loss.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "T \u00b7 ln 1 p1 + \u03b3T (80) Different loss has different choice of k1 and k2, and [4] proved that with square loss the expected regret bound is O( \u221a", "startOffset": 76, "endOffset": 79}], "year": 2015, "abstractText": "This note introduce three Bayesian style Multi-armed bandit algorithms: Information-directed sampling, Thompson Sampling and Generalized Thompson Sampling. The goal is to give an intuitive explanation for these three algorithms and their regret bounds, and provide some derivations that are omitted in the original papers.", "creator": "LaTeX with hyperref package"}}}