{"id": "1503.06960", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2015", "title": "Sample compression schemes for VC classes", "abstract": "We prove that proper PAC learnability implies compression. Namely, if a concept $C \\subseteq \\Sigma^X$ is properly PAC learnable with $d$ samples, then $C$ has a sample compression scheme of size $2^{O(d)}$. In particular, every boolean concept class with constant VC dimension has a sample compression scheme of constant size $2^{O(d)}$ with $d$ samples, so $c$ is a sample compression scheme of size $2^{O(d)}$ with $d$ samples, so $c$ has a sample compression scheme of size $2^{O(d)}$ with $d$ samples, so $c$ has a sample compression scheme of size $2^{O(d)}$ with $d$ samples, so $c$ has a sample compression scheme of size $2^{O(d)}$ with $d$ samples, so $c$ has a sample compression scheme of size $2^{O(d)}$ with $d$ samples, so $c$ has a sample compression scheme of size $2^{O(d)}$ with $d$ samples, so $c$ has a sample compression scheme of size $2^{O(d)}$ with $d$ samples, so $c$ has a sample compression scheme of size $2^{O(d)}$ with $d$ samples, so $c$ has a sample compression scheme of size $2^{O(d)}$ with $d$ samples, so $c$ has a sample compression scheme of size $2^{O(d)}$ with $d$ samples, so $c$ has a sample compression scheme of size $2^{O(d)}$ with $d$ samples, so $c$ has a sample compression scheme of size $2^{O(d)}$ with $d$ samples, so $c$ has a sample compression scheme of size $2^{O(d)}$ with $d$ samples, so $c$ has a sample compression scheme of size $2^{O(d)}$ with $d$ samples, so $c$ has a sample compression scheme of size $2^{O(d)}$ with $d$ samples, so $c$ has a sample compression scheme of size $2^{O(d)}$ with $d$ samples, so $c$ has a sample compression scheme of size $2^{O(d)}$ with $", "histories": [["v1", "Tue, 24 Mar 2015 09:30:33 GMT  (10kb,D)", "http://arxiv.org/abs/1503.06960v1", "11 pages"], ["v2", "Tue, 14 Apr 2015 12:18:14 GMT  (13kb,D)", "http://arxiv.org/abs/1503.06960v2", "14 pages. The previous version of this text contained an error; Theorem 2.1 in it is false. This error only affects the statement for multi-labeled classes, and the construction for binary-labeled classes still holds. In the new version of the text, we added a relevant discussion in Section 4"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shay moran", "amir yehudayoff"], "accepted": false, "id": "1503.06960"}, "pdf": {"name": "1503.06960.pdf", "metadata": {"source": "CRF", "title": "Proper PAC learning is compressing", "authors": ["Shay Moran", "Amir Yehudayoff"], "emails": ["shaymrn@cs.technion.ac.il.", "amir.yehudayoff@gmail.com."], "sections": [{"heading": null, "text": "We prove that proper PAC learnability implies compression. Namely, if a concept C \u2286 \u03a3X is properly PAC learnable with d samples, then C has a sample compression scheme of size 2O(d). In particular, every boolean concept class with constant VC dimension has a sample compression scheme of constant size. This answers a question of Littlestone and Warmuth (1986). The proof uses an approximate minimax phenomenon for boolean matrices of low VC dimension."}, {"heading": "1 Introduction", "text": "Learning and compression are known to be deeply related to each other. Learning procedures perform compression, and compression is an evidence of and is useful in learning. For example, support vector machines, which are commonly applied to solve classification problems, perform compression (see Chapter 6 in [6]), and compression can be used to boost the accuracy of learning procedures (see [16, 11] and Chapter 4 in [6]).\nAbout thirty years ago, Littlestone and Warmuth [16] provided a mathematical framework for studying compression in the context of learning theory. In a nutshell, they showed that compression implies learnability and asked whether learnability implies compression."}, {"heading": "1.1 Definitions", "text": "Concepts and samples. Let \u03a3, X be finite sets (we focus on this case to eliminate measurability and similar issues but the arguments presented here are more general). A concept is a function c : X \u2192 \u03a3. A concept class C \u2286 \u03a3X is a collection of concepts. A subset Y of X is thought of as a collection of sample points. For Y \u2286 X and c \u2208 C, let c|Y be the restriction of the function c to the set Y . We think of c|Y as the labeling \u2217Departments of Computer Science, Technion-IIT, Israel and Max Planck Institute for Informatics, Saarbru\u0308cken, Germany. shaymrn@cs.technion.ac.il. \u2020Department of Mathematics, Technion-IIT, Israel. amir.yehudayoff@gmail.com. Horev fellow \u2013 supported by the Taub foundation. Research is also supported by ISF and BSF.\nar X\niv :1\n50 3.\n06 96\n0v 1\n[ cs\n.L G\n] 2\n4 M\nar 2\nof Y according to c. A C-labelled sample is a pair (Y, y), where Y \u2286 X and y = c|Y for some c \u2208 C. The size of a C-labelled sample (Y, y) is |Y |. For an integer k, denote by LC(k) the set of C-labelled samples of size at most k. Denote by LC(\u221e) the set of all C-labelled samples of finite size.\nPAC learning. Probably approximately correct (PAC) learning was defined in Valiant\u2019s seminal work [25]. We use the following definition. The concept class C is PAC learnable with d samples if there is a map that generates hypotheses H : LC(d) \u2192 \u03a3X so that for every c \u2208 C and for every probability distribution \u00b5 on X,\nPr \u00b5d\n[{ Y \u2208 Xd : \u00b5({x \u2208 X : hY (x) 6= c(x)}) \u2264 1/3 }] \u2265 2/3,\nwhere hY = H(Y, c|Y ). Roughly speaking, an hypothesis generated by H using d independent samples is a \u00b5-approximation of c with reasonable probability. If the image of H is contained in C, we say that C is properly PAC learnable.\nVC dimension. A boolean concept class is C \u2286 {0, 1}X . A set Y \u2286 X is shattered in C if for every Z \u2286 Y there is c \u2208 C so that c(x) = 1 for all x \u2208 Z and c(x) = 0 for all x \u2208 Y \u2212 Z. The Vapnik-Chervonenkis (VC) dimension of C, denoted VC(C), is the maximum size of a shattered set in C [26].\nA fundamental and well-known result of Blumer, Eherenfeucht, Haussler, and Warmuth [4], which is based on an earlier work of Vapnik and Chervonenkis [26], states that every boolean concept class C can be properly PAC learned with1 O(VC(C)) examples (in fact the sample complexity of PAC learning for boolean classes is captured by the VC dimension).\nSample compression schemes. Sample compression schemes were defined by Littlestone and Warmuth [16]. Roughly speaking, a sample compression scheme takes a long list of samples and compresses it to a short sub-list of samples in a way that allows to invert the compression. Formally, a k-sample compression scheme for C with information I, where I is a finite set, consists of two maps \u03ba, \u03c1 for which the following hold:\n(\u03ba) The compression map\n\u03ba : LC(\u221e)\u2192 LC(k)\u00d7 I\ntakes (Y, y) to ((Z, z), i) with Z \u2286 Y and y|Z = z.\n(\u03c1) The reconstruction map\n\u03c1 : LC(k)\u00d7 I \u2192 \u03a3X\n1Big O and \u2126 notation means up to absolute constants.\nis so that for all (Y, y) in LC(\u221e),\n\u03c1(\u03ba(Y, y))|Y = y.\nThe size of the scheme is2 k + log(|I|+ 1), and its kernel size is k. In the language of coding theory, the side information I can be thought of as list decoding; the map \u03c1 has a short list of possible reconstructions of a given (Z, z), and the information i indicates which element in the list is the correct one.\nSee [9, 10, 18] for more discussions of this definition, and some insightful examples."}, {"heading": "1.2 Background", "text": "Littlestone and Warmuth [16] proved that compression implies learnability (see Theorem 1.1 below), and asked whether learnability implies compression for boolean concept classes: \u201cAre there concept classes with finite dimension for which there is no scheme with bounded kernel size and bounded additional information?\u201d\nThis question and variants of it lead to a rich body of work that revealed profound properties of VC dimension and learning. These works also discovered and utilized connections between sample compression schemes, and model theory, topology, combinatorics, and geometry.\nFloyd and Warmuth [9, 10] constructed sample compression schemes of size log |C| for every concept class C. Freund [11] showed how to compress a sample of size m to a sample of size O(d log(m)) with some side information for boolean classes of VC dimension d.\nAs the study of sample compression schemes deepened, many insightful and optimal schemes for special cases have been constructed: Floyd [9], Helmbold et al. [12], Floyd and Warmuth [10], Ben-David and Litman [3], Chernikov and Simon [5], Kuzmin and Warmuth [13], Rubinstein et al. [22], Rubinstein and Rubinstein [23], Livni and Simon [17] and more.\nFinally, in our recent work with Shpilka and Wigderson [18], we constructed sample compression schemes of size O(d \u00b7 2d \u00b7 log log |C|) using some side information for every boolean concept class C of VC dimension d.\nCompression implies learnability. Littlestone and Warmuth proved that the sample complexity of PAC learning is at most (roughly) the size of a compression scheme [16].\nTheorem 1.1 (Compression implies learnability [16]). Let C \u2286 \u03a3X and c \u2208 C. Let \u00b5 be a distribution on X, and x1, . . . , xm be m independent samples from \u00b5. Let Y = (x1, . . . , xm)\n2Logarithms in this text are of base two.\nand y = c|Y . Let \u03ba, \u03c1 be a k-sample compression scheme for C with additional information I. Let h = \u03c1(\u03ba(Y, y)). Then, for every > 0,\nPr \u00b5m\n[ \u00b5({x \u2208 X : h(x) 6= c(x)}) > ] < |I| k\u2211 j=0 ( m j ) (1\u2212 )m\u2212j.\nIn particular, C can be PAC learned with O(k log(k) + log(|I|+ 1)) samples.\nProof sketch. There are \u2211k\nj=0 ( m j ) subsets T of [m] of size at most k. There are |I| choices\nfor i \u2208 I. Each choice of T, i yields a function hT,i = \u03c1((T, yT ), i) that is measurable with respect to xT = (xt : t \u2208 T ). The function h is one of the functions in {hT,i : |T | \u2264 k, i \u2208 I}. For each hT,i, the coordinates in [m] \u2212 T are independent, and so if \u00b5({x \u2208 X : hT,i(x) 6= c(x)}) > then the probability that all these m\u2212 |T | samples agree with c is less than (1\u2212 )m\u2212|T |. The union bound completes the proof."}, {"heading": "1.3 Learning is compressing", "text": "Our main theorem says that proper PAC learnability implies sample compression schemes of constant size.\nTheorem 1.2 (Proper learnability implies compression). If C \u2286 \u03a3X is properly PAC learnable with d samples, then C has a sample compression scheme of size 2O(d).\nThe theorem specifically answers Littlestone and Warmuth\u2019s question [16]; every boolean concept class of finite VC dimension has a sample compression scheme of finite size. The theorem, however, only provides an exponential dependence on d, whereas many of the known compression schemes for special cases (e.g. [10, 3, 13, 23, 17]) have size O(d). Warmuth\u2019s question [27] whether O(d)-sample compression schemes always exist remains open.\nOur construction (see Section 3) of sample compression schemes is overall quite short and simple, but uses a different perspective of the problem than in previous work (mentioned above). It is inspired by Freund\u2019s work [11] where majority is used to boost the accuracy of learning procedures. It also uses several known properties of PAC learnability and VC dimension, together with von Neumann\u2019s minimax theorem (these appear in Section 2)."}, {"heading": "2 Preliminaries", "text": "Sample complexity. There are many generalization of VC dimension to non-boolean concept classes (see [2] and references within). Here we use the following one. Let C \u2286 \u03a3X .\nFor every c \u2208 C, define a boolean concept class Bc \u2286 {0, 1}X as the set of all bh, for h \u2208 C, defined by bh(x) = 1 if and only if h(x) = c(x). Define the distinguishing dimension of C as\nDD(C) = max{VC(Bc) : c \u2208 C}.\nThis definition of dimension is similar to notions used in [19, 7, 2]. If C is boolean then VC(C) = DD(C).\nVapnik and Chervonenkis [26] and Blumer et al. [4] proved that VC dimension is equivalent to the sample complexity of PAC learning. The distinguishing dimension is a lower bound on the sample complexity of PAC learning (see [4, 8, 2]).\nTheorem 2.1 (Lower bound for sample complexity [4, 8, 2]). The number of samples needed to PAC learn C is at least \u2126(DD(C)).\nDual classes. Let C \u2286 {0, 1}X be a boolean concept class. The dual concept class C\u2217 \u2286 {0, 1}C of C is defined as the set of all functions fx : C \u2192 {0, 1} so that fx(c) = 1 if and only if c(x) = 1. If we think of C as a binary matrix whose rows are concepts in C and columns are elements of X, then C\u2217 corresponds to the distinct rows of the transposed matrix. Assouad [1] bounded VC(C\u2217) in terms of VC(C).\nClaim 2.2 (VC dimension of dual [1]). If VC(C) = d then VC(C\u2217) \u2264 2d+1.\nApproximations. The following theorem shows that every distribution can be approximated by a distribution of small support, when the statistical tests belong to a class of small VC dimension. This phenomenon was first proved by Vapnik and Chervonenkis [26], and was later quantitively improved in [14, 24].\nTheorem 2.3 (Approximations for bounded VC dimension [26, 14, 24]). Let C \u2286 {0, 1}X of VC dimension d. Let \u00b5 be a distribution on X. For all > 0, there exists a multi-set Y \u2286 X of size |Y | \u2264 O(d/ 2) such that for all c \u2208 C,\u2223\u2223\u2223\u2223\u00b5({x \u2208 X : c(x) = 1})\u2212 |{x \u2208 Y : c(x) = 1}||Y | \u2223\u2223\u2223\u2223 \u2264 .\nMinimax. Von Neumann\u2019s minimax theorem [20] is a seminal result in game theory (see the textbook [21]). Assume that there are 2 players, a row player and a column player. A pure strategy of the row player is r \u2208 [m] and a pure strategy of the column player is j \u2208 [n]. Let M be a boolean matrix so that M(r, j) = 1 if and only if the row player wins the game when the pure strategies r, j are played.\nThe minimax theorem says that if for every mixed strategy (a distribution on pure strategies) q of the column player, there is a mixed strategy p of the row player that\nguarantees the row player wins with probability at least V , then there is a mixed strategy p of the row player so that for all mixed strategies q of the column player, the row player wins with probability at least V . A similar statement holds for the column player. This implies that there is a pair of mixed strategies that form a Nash equilibrium (see [21]).\nTheorem 2.4 (Minimax [20]). Let M \u2208 Rm\u00d7n be a real matrix. Then,\nmin p\u2208\u2206m max q\u2208\u2206n ptMq = max q\u2208\u2206n min p\u2208\u2206m ptMq,\nwhere \u2206` is the set of distributions on [`].\nThe arguments in the proof of Theorem 1.2 below imply the following variant of the minimax theorem, which may be of interest in the context of game theory. The minimax theorem holds for a general matrix M . In other words, there is no assumption on the set of winning/losing states in the game.\nWe observe that a combinatorial restriction on the winning/losing states in the game implies that there is an approximate efficient equilibrium state. Namely, if the rows of M have VC dimension d, then for every > 0, there is a multi-set of O(2d/ 2) pure strategies R \u2286 [m] for the row player, and a multi-set of O(d/ 2) pure strategies J \u2286 [n] for the column player, so that a uniformly random choice from R, J guarantees the players a gain that is -close to the gain in the equilibrium strategy.\nLipton, Markakis and Mehta [15] call such a pair of mixed strategies an -Nash equilibrium. They showed that in every game there are -Nash equilibriums with logarithmic support, and used this to find an approximate Nash equilibrium in quasi-polynomial time. The ideas presented here show that if the matrix of the game has constant VC dimension then there are -Nash equilibriums with constant support, and that consequently an approximate Nash equilibrium can be found in polynomial time."}, {"heading": "3 A compression scheme", "text": "In the proof of Theorem 1.2, we use the following simple lemma. The lemma can be seen as an approximate, combinatorial version of Carathe\u0301odory\u2019s theorem from convex geometry. Let C \u2286 {0, 1}n \u2282 Rn and denote by K the convex hull of C in Rn. Carathe\u0301odory\u2019s theorem says that every point p \u2208 K is a convex combination of at most n + 1 points from C. Lemma 3.1 says that if C has constant VC dimension then every p \u2208 K can be approximated by a convex combination of small support. Namely, if VC(C) = d then p can be -approximated in `\u221e by a convex combination of at most O(2 d/ 2) points from C.\nLemma 3.1 (Sampling for bounded VC dimension). Let C \u2286 {0, 1}X of VC dimension d. Let p be a distribution on concepts in C, and let > 0. Then, there is a multi-set F \u2286 C of size |F | \u2264 O(2d/ 2) so that for every x \u2208 X,\u2223\u2223\u2223\u2223p({c \u2208 C : c(x) = 1})\u2212 |{f \u2208 F : f(x) = 1}||F | \u2223\u2223\u2223\u2223 \u2264 .\nProof. By Claim 2.2, the VC dimension of the dual class C\u2217 is at most 2d+1. Every x \u2208 X corresponds to a concept in C\u2217. The distribution p is a distribution on the domain of the functions in C\u2217. The lemma follows by Theorem 2.3 applied to C\u2217."}, {"heading": "3.1 The construction", "text": "Proof of Theorem 1.2. Since C is properly PAC learnable with d samples, let\nH : LC(d)\u2192 C\nbe so that for every c \u2208 C and for every probability distribution q on X, there is Z \u2286 supp(q) of size |Z| \u2264 d so that q({x \u2208 X : hZ(x) 6= c(x)}) \u2264 1/3 where hZ = H(Z, c|Z).\nCompression. Let (Y, y) \u2208 LC(\u221e). Let\nH = HY,y = {H(Z, z) : Z \u2286 Y, |Z| \u2264 d, z = y|Z} \u2286 C.\nThe compression is based on the following claim.\nClaim 3.2. There are T sets Z1, Z2, . . . , ZT \u2286 Y , each of size at most d, with T \u2264 K := 2O(d) so that the following holds. For t \u2208 [T ], let\nft = H(Zt, y|Zt). (1)\nThen, for every x \u2208 Y ,\n|{t \u2208 [T ] : ft(x) = y(x)}| > T/2. (2)\nGiven the claim, the compression \u03ba(Y, y) is defined as\nZ = \u22c3 t\u2208[T ] Zt and z = y|Z .\nThe additional information i \u2208 I allows to recover the sets Z1, . . . , ZT from the set Z. There are many possible ways to encode this information, but the size of I can be chosen\nto be at most kk with k := K \u00b7 d+ 1 \u2264 2O(d).\nProof of Claim 3.2. By choice of H, for every distribution q on Y , there is h \u2208 H so that\nq ({x \u2208 Y : h(x) = y(x)}) \u2265 2/3.\nBy Theorem 2.4, there is a distribution p on H such that for every x \u2208 Y ,\np({h \u2208 H : h(x) = y(x)}) \u2265 2/3.\nLet B \u2286 {0, 1}Y be the set of concepts bh, for h \u2208 H, defined by bh(x) = 1 if and only if h(x) = y(x). The distribution p induces a distribution pB on B so that for every x \u2208 Y ,\npB({b \u2208 B : b(x) = 1}) \u2265 2/3.\nSince C is PAC learnable with d samples, Theorem 2.1 implies DD(C) \u2264 O(d). Hence, VC(B) \u2264 O(d). By Lemma 3.1 applied to B and pB with = 1/8, there is a multi-set E \u2286 B of size |E| \u2264 K := 2O(d) so that for every x \u2208 Y ,\n|{e \u2208 E : e(x) = 1}| |E| \u2265 pB({b \u2208 B : b(x) = 1})\u2212 1/8 > 1/2.\nThe multi-set E \u2286 B corresponds to a multi-set F = {f1, f2, . . . , fT} \u2286 H of size T = |E| so that for every x \u2208 Y ,\n|{t \u2208 [T ] : ft(x) = y(x)}| > T/2. (3)\nFor every t \u2208 [T ], let Zt be a subset of Y of size |Zt| \u2264 d so that\nH(Zt, y|Zt) = ft.\nReconstruction. Given ((Z, z), i), the information i is interpreted as a list of T subsets Z1, . . . , ZT of Z, each of size at most d. For t \u2208 [T ], let\nht = H(Zt, z|Zt).\nDefine h = \u03c1((Z, z), i) as follows: For every x \u2208 X, let h(x) be a symbol that appears most in the list\n\u03bbx((Z, z), i) = (h1(x), h2(x), . . . , hT (x)),\nwhere ties are arbitrarily broken.\nCorrectness. Fix (Y, y) \u2208 LC(\u221e). Let ((Z, z), i) = \u03ba(Y, y) and h = \u03c1((Z, z), i). For x \u2208 Y , consider the list\n\u03c6x(Y, y) = (f1(x), f2(x), . . . , fT (x))\ndefined in the compression process of (Y, y). The list \u03c6x(Y, y) is identical to the list \u03bbx((Z, z), i); this follows from Equation (1), from that i allows to correctly recover Z1, . . . , ZT , and from that y|Zt = z|Zt for all t \u2208 [T ]. By (3), for every x \u2208 Y , the symbol y(x) appears in more than half of the list \u03bbx((Z, z), i) so indeed h(x) = y(x)."}, {"heading": "Acknowledgements", "text": "We thank Amir Shpilka and Avi Wigderson for helpful discussions. We also thank Ben Lee Volk for comments on an earlier version of this text."}], "references": [{"title": "Densite et dimension", "author": ["P. Assouad"], "venue": "Ann. Institut Fourter, 3:232\u2013282", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1983}, {"title": "and P", "author": ["S. Ben-David", "N. Cesa-Bianchi", "D. Haussler"], "venue": "M. Long. Characterizations of learnability for classes of {0,...,n}-valued functions. J. Comput. Syst. Sci., 50(1):74\u2013 86", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Combinatorial variability of Vapnik-Chervonenkis classes with applications to sample compression schemes", "author": ["S. Ben-David", "A. Litman"], "venue": "Discrete Applied Mathematics, 86(1):3\u201325", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Learnability and the Vapnik-Chervonenkis dimension", "author": ["A. Blumer", "A. Ehrenfeucht", "D. Haussler", "M.K. Warmuth"], "venue": "J. Assoc. Comput. Mach., 36(4):929\u2013965", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1989}, {"title": "Externally definable sets and dependent pairs", "author": ["A. Chernikov", "P. Simon"], "venue": "Israel Journal of Mathematics, 194(1):409\u2013425", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "An Introduction to Support Vector Machines and other kernel-based learning methods", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": "Cambridge University Press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Universal Donsker classes and metric entropy", "author": ["R.M. Dudley"], "venue": "Ann. Probab., 15(4):1306\u20131326,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1987}, {"title": "A general lower bound on the number of examples needed for learning", "author": ["A. Ehrenfeucht", "D. Haussler", "M.J. Kearns", "L.G. Valiant"], "venue": "Inf. Comput., 82(3):247\u2013261", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1989}, {"title": "Space-bounded learning and the vapnik-chervonenkis dimension", "author": ["S. Floyd"], "venue": "COLT, pages 349\u2013364", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1989}, {"title": "Sample compression", "author": ["S. Floyd", "M.K. Warmuth"], "venue": "learnability, and the vapnikchervonenkis dimension. Machine Learning, 21(3):269\u2013304", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "Boosting a weak learning algorithm by majority", "author": ["Y. Freund"], "venue": "Inf. Comput., 121(2):256\u2013285", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1995}, {"title": "Learning integer lattices", "author": ["D.P. Helmbold", "R.H. Sloan", "M.K. Warmuth"], "venue": "SIAM J. Comput., 21(2):240\u2013266", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1992}, {"title": "Unlabeled compression schemes for maximum classes", "author": ["D. Kuzmin", "M.K. Warmuth"], "venue": "Journal of Machine Learning Research, 8:2047\u20132081", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Improved bounds on the sample complexity of learning", "author": ["Y. Li", "P.M. Long", "A. Srinivasan"], "venue": "SODA, pages 309\u2013318", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Playing large games using simple strategies", "author": ["R.J. Lipton", "E. Markakis", "A. Mehta"], "venue": "ACM Conference on Electronic Commerce, pages 36\u201341, New York, NY, USA", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Relating data compression and learnability", "author": ["N. Littlewood", "M. Warmuth"], "venue": "Unpublished", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1986}, {"title": "Honest compressions and their application to compression schemes", "author": ["R. Livni", "P. Simon"], "venue": "COLT, pages 77\u201392", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Teaching and compressing for low VC-dimension", "author": ["S. Moran", "A. Shpilka", "A. Wigderson", "A. Yehudayoff"], "venue": "ECCC, TR15-025", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "On learning sets and functions", "author": ["B.K. Natarajan"], "venue": "Machine Learning, 4:67\u201397", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1989}, {"title": "Zur theorie der gesellschaftsspiele", "author": ["J. von Neumann"], "venue": "Mathematische Annalen,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1928}, {"title": "Game Theory", "author": ["G. Owen"], "venue": "Academic Press", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "Shifting: One-inclusion mistake bounds and sample compression", "author": ["B.I.P. Rubinstein", "P.L. Bartlett", "J.H. Rubinstein"], "venue": "J. Comput. Syst. Sci., 75(1):37\u201359", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "A geometric approach to sample compression", "author": ["B.I.P. Rubinstein", "J.H. Rubinstein"], "venue": "Journal of Machine Learning Research, 13:1221\u20131261", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Sharper bounds for Gaussian and empirical processes", "author": ["M. Talagrand"], "venue": "Ann. Probab., 22(1):28\u201376", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1994}, {"title": "A theory of the learnable", "author": ["L.G. Valiant"], "venue": "Commun. ACM, 27:1134\u20131142", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1984}, {"title": "On the uniform convergence of relative frequencies of events to their probabilities", "author": ["V.N. Vapnik", "A.Ya. Chervonenkis"], "venue": "Theory Probab. Appl.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1971}, {"title": "Compressing to VC dimension many points", "author": ["M.K. Warmuth"], "venue": "COLT/Kernel, pages 743\u2013744", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 5, "context": "For example, support vector machines, which are commonly applied to solve classification problems, perform compression (see Chapter 6 in [6]), and compression can be used to boost the accuracy of learning procedures (see [16, 11] and Chapter 4 in [6]).", "startOffset": 137, "endOffset": 140}, {"referenceID": 15, "context": "For example, support vector machines, which are commonly applied to solve classification problems, perform compression (see Chapter 6 in [6]), and compression can be used to boost the accuracy of learning procedures (see [16, 11] and Chapter 4 in [6]).", "startOffset": 221, "endOffset": 229}, {"referenceID": 10, "context": "For example, support vector machines, which are commonly applied to solve classification problems, perform compression (see Chapter 6 in [6]), and compression can be used to boost the accuracy of learning procedures (see [16, 11] and Chapter 4 in [6]).", "startOffset": 221, "endOffset": 229}, {"referenceID": 5, "context": "For example, support vector machines, which are commonly applied to solve classification problems, perform compression (see Chapter 6 in [6]), and compression can be used to boost the accuracy of learning procedures (see [16, 11] and Chapter 4 in [6]).", "startOffset": 247, "endOffset": 250}, {"referenceID": 15, "context": "About thirty years ago, Littlestone and Warmuth [16] provided a mathematical framework for studying compression in the context of learning theory.", "startOffset": 48, "endOffset": 52}, {"referenceID": 24, "context": "Probably approximately correct (PAC) learning was defined in Valiant\u2019s seminal work [25].", "startOffset": 84, "endOffset": 88}, {"referenceID": 25, "context": "The Vapnik-Chervonenkis (VC) dimension of C, denoted VC(C), is the maximum size of a shattered set in C [26].", "startOffset": 104, "endOffset": 108}, {"referenceID": 3, "context": "A fundamental and well-known result of Blumer, Eherenfeucht, Haussler, and Warmuth [4], which is based on an earlier work of Vapnik and Chervonenkis [26], states that every boolean concept class C can be properly PAC learned with O(VC(C)) examples (in fact the sample complexity of PAC learning for boolean classes is captured by the VC dimension).", "startOffset": 83, "endOffset": 86}, {"referenceID": 25, "context": "A fundamental and well-known result of Blumer, Eherenfeucht, Haussler, and Warmuth [4], which is based on an earlier work of Vapnik and Chervonenkis [26], states that every boolean concept class C can be properly PAC learned with O(VC(C)) examples (in fact the sample complexity of PAC learning for boolean classes is captured by the VC dimension).", "startOffset": 149, "endOffset": 153}, {"referenceID": 15, "context": "Sample compression schemes were defined by Littlestone and Warmuth [16].", "startOffset": 67, "endOffset": 71}, {"referenceID": 8, "context": "See [9, 10, 18] for more discussions of this definition, and some insightful examples.", "startOffset": 4, "endOffset": 15}, {"referenceID": 9, "context": "See [9, 10, 18] for more discussions of this definition, and some insightful examples.", "startOffset": 4, "endOffset": 15}, {"referenceID": 17, "context": "See [9, 10, 18] for more discussions of this definition, and some insightful examples.", "startOffset": 4, "endOffset": 15}, {"referenceID": 15, "context": "2 Background Littlestone and Warmuth [16] proved that compression implies learnability (see Theorem 1.", "startOffset": 37, "endOffset": 41}, {"referenceID": 8, "context": "Floyd and Warmuth [9, 10] constructed sample compression schemes of size log |C| for every concept class C.", "startOffset": 18, "endOffset": 25}, {"referenceID": 9, "context": "Floyd and Warmuth [9, 10] constructed sample compression schemes of size log |C| for every concept class C.", "startOffset": 18, "endOffset": 25}, {"referenceID": 10, "context": "Freund [11] showed how to compress a sample of size m to a sample of size O(d log(m)) with some side information for boolean classes of VC dimension d.", "startOffset": 7, "endOffset": 11}, {"referenceID": 8, "context": "As the study of sample compression schemes deepened, many insightful and optimal schemes for special cases have been constructed: Floyd [9], Helmbold et al.", "startOffset": 136, "endOffset": 139}, {"referenceID": 11, "context": "[12], Floyd and Warmuth [10], Ben-David and Litman [3], Chernikov and Simon [5], Kuzmin and Warmuth [13], Rubinstein et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[12], Floyd and Warmuth [10], Ben-David and Litman [3], Chernikov and Simon [5], Kuzmin and Warmuth [13], Rubinstein et al.", "startOffset": 24, "endOffset": 28}, {"referenceID": 2, "context": "[12], Floyd and Warmuth [10], Ben-David and Litman [3], Chernikov and Simon [5], Kuzmin and Warmuth [13], Rubinstein et al.", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "[12], Floyd and Warmuth [10], Ben-David and Litman [3], Chernikov and Simon [5], Kuzmin and Warmuth [13], Rubinstein et al.", "startOffset": 76, "endOffset": 79}, {"referenceID": 12, "context": "[12], Floyd and Warmuth [10], Ben-David and Litman [3], Chernikov and Simon [5], Kuzmin and Warmuth [13], Rubinstein et al.", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "[22], Rubinstein and Rubinstein [23], Livni and Simon [17] and more.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[22], Rubinstein and Rubinstein [23], Livni and Simon [17] and more.", "startOffset": 32, "endOffset": 36}, {"referenceID": 16, "context": "[22], Rubinstein and Rubinstein [23], Livni and Simon [17] and more.", "startOffset": 54, "endOffset": 58}, {"referenceID": 17, "context": "Finally, in our recent work with Shpilka and Wigderson [18], we constructed sample compression schemes of size O(d \u00b7 2 \u00b7 log log |C|) using some side information for every boolean concept class C of VC dimension d.", "startOffset": 55, "endOffset": 59}, {"referenceID": 15, "context": "Littlestone and Warmuth proved that the sample complexity of PAC learning is at most (roughly) the size of a compression scheme [16].", "startOffset": 128, "endOffset": 132}, {"referenceID": 15, "context": "1 (Compression implies learnability [16]).", "startOffset": 36, "endOffset": 40}, {"referenceID": 15, "context": "The theorem specifically answers Littlestone and Warmuth\u2019s question [16]; every boolean concept class of finite VC dimension has a sample compression scheme of finite size.", "startOffset": 68, "endOffset": 72}, {"referenceID": 9, "context": "[10, 3, 13, 23, 17]) have size O(d).", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "[10, 3, 13, 23, 17]) have size O(d).", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "[10, 3, 13, 23, 17]) have size O(d).", "startOffset": 0, "endOffset": 19}, {"referenceID": 22, "context": "[10, 3, 13, 23, 17]) have size O(d).", "startOffset": 0, "endOffset": 19}, {"referenceID": 16, "context": "[10, 3, 13, 23, 17]) have size O(d).", "startOffset": 0, "endOffset": 19}, {"referenceID": 26, "context": "Warmuth\u2019s question [27] whether O(d)-sample compression schemes always exist remains open.", "startOffset": 19, "endOffset": 23}, {"referenceID": 10, "context": "It is inspired by Freund\u2019s work [11] where majority is used to boost the accuracy of learning procedures.", "startOffset": 32, "endOffset": 36}, {"referenceID": 1, "context": "There are many generalization of VC dimension to non-boolean concept classes (see [2] and references within).", "startOffset": 82, "endOffset": 85}, {"referenceID": 18, "context": "This definition of dimension is similar to notions used in [19, 7, 2].", "startOffset": 59, "endOffset": 69}, {"referenceID": 6, "context": "This definition of dimension is similar to notions used in [19, 7, 2].", "startOffset": 59, "endOffset": 69}, {"referenceID": 1, "context": "This definition of dimension is similar to notions used in [19, 7, 2].", "startOffset": 59, "endOffset": 69}, {"referenceID": 25, "context": "Vapnik and Chervonenkis [26] and Blumer et al.", "startOffset": 24, "endOffset": 28}, {"referenceID": 3, "context": "[4] proved that VC dimension is equivalent to the sample complexity of PAC learning.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "The distinguishing dimension is a lower bound on the sample complexity of PAC learning (see [4, 8, 2]).", "startOffset": 92, "endOffset": 101}, {"referenceID": 7, "context": "The distinguishing dimension is a lower bound on the sample complexity of PAC learning (see [4, 8, 2]).", "startOffset": 92, "endOffset": 101}, {"referenceID": 1, "context": "The distinguishing dimension is a lower bound on the sample complexity of PAC learning (see [4, 8, 2]).", "startOffset": 92, "endOffset": 101}, {"referenceID": 3, "context": "1 (Lower bound for sample complexity [4, 8, 2]).", "startOffset": 37, "endOffset": 46}, {"referenceID": 7, "context": "1 (Lower bound for sample complexity [4, 8, 2]).", "startOffset": 37, "endOffset": 46}, {"referenceID": 1, "context": "1 (Lower bound for sample complexity [4, 8, 2]).", "startOffset": 37, "endOffset": 46}, {"referenceID": 0, "context": "Assouad [1] bounded VC(C\u2217) in terms of VC(C).", "startOffset": 8, "endOffset": 11}, {"referenceID": 0, "context": "2 (VC dimension of dual [1]).", "startOffset": 24, "endOffset": 27}, {"referenceID": 25, "context": "This phenomenon was first proved by Vapnik and Chervonenkis [26], and was later quantitively improved in [14, 24].", "startOffset": 60, "endOffset": 64}, {"referenceID": 13, "context": "This phenomenon was first proved by Vapnik and Chervonenkis [26], and was later quantitively improved in [14, 24].", "startOffset": 105, "endOffset": 113}, {"referenceID": 23, "context": "This phenomenon was first proved by Vapnik and Chervonenkis [26], and was later quantitively improved in [14, 24].", "startOffset": 105, "endOffset": 113}, {"referenceID": 25, "context": "3 (Approximations for bounded VC dimension [26, 14, 24]).", "startOffset": 43, "endOffset": 55}, {"referenceID": 13, "context": "3 (Approximations for bounded VC dimension [26, 14, 24]).", "startOffset": 43, "endOffset": 55}, {"referenceID": 23, "context": "3 (Approximations for bounded VC dimension [26, 14, 24]).", "startOffset": 43, "endOffset": 55}, {"referenceID": 19, "context": "Von Neumann\u2019s minimax theorem [20] is a seminal result in game theory (see the textbook [21]).", "startOffset": 30, "endOffset": 34}, {"referenceID": 20, "context": "Von Neumann\u2019s minimax theorem [20] is a seminal result in game theory (see the textbook [21]).", "startOffset": 88, "endOffset": 92}, {"referenceID": 20, "context": "This implies that there is a pair of mixed strategies that form a Nash equilibrium (see [21]).", "startOffset": 88, "endOffset": 92}, {"referenceID": 19, "context": "4 (Minimax [20]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 14, "context": "Lipton, Markakis and Mehta [15] call such a pair of mixed strategies an -Nash equilibrium.", "startOffset": 27, "endOffset": 31}], "year": 2017, "abstractText": "We prove that proper PAC learnability implies compression. Namely, if a concept C \u2286 \u03a3X is properly PAC learnable with d samples, then C has a sample compression scheme of size 2O(d). In particular, every boolean concept class with constant VC dimension has a sample compression scheme of constant size. This answers a question of Littlestone and Warmuth (1986). The proof uses an approximate minimax phenomenon for boolean matrices of low VC dimension.", "creator": "LaTeX with hyperref package"}}}