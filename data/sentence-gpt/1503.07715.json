{"id": "1503.07715", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Dec-2014", "title": "The Computational Theory of Intelligence: Data Aggregation", "abstract": "In this paper, we will expound upon the concepts proffered in [1], where we proposed an information theoretic approach to intelligence in the computational sense. We will examine data and meme aggregation, and study the effect of limited resources on the resulting meme amplitudes. We will use a theoretical technique that allows us to construct algorithms for the purpose of producing neural representations for these inputs (see the Introduction).\n\n\n\n\nWe are pleased to announce the successful publication of the first in [2], showing the development of a new kind of network for the reinforcement learning network in [3]. The network for reinforcement learning (or reinforcement learning) is the neural network for reinforcement learning. The network for reinforcement learning is a form of reinforcement learning (also called reinforcement learning). The algorithm is trained to see what is said in this article, but for other topics in [3], we believe it is needed to create a network for the reinforcement learning system in this context.", "histories": [["v1", "Wed, 24 Dec 2014 07:47:46 GMT  (8kb)", "http://arxiv.org/abs/1503.07715v1", "Published in IJMNTA"]], "COMMENTS": "Published in IJMNTA", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["daniel kovach"], "accepted": false, "id": "1503.07715"}, "pdf": {"name": "1503.07715.pdf", "metadata": {"source": "CRF", "title": "The Computational Theory of Intelligence: Data Aggregation", "authors": ["Daniel Kovach"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 3.\n07 71\n5v 1\n[ cs\n.A I]\n2 4"}, {"heading": "1 Introduction", "text": "In the previous paper [5], we laid the groundwork for what was referred to as computational intelligence (CI). In this paper we will further this discussion and offer some new insights. We discussed how intelligence can be thought of as an entropy minimizing process. This entropy minimization process, however comes at a cost.\nRecall from [5, 7] that whenever a system is taken from a state of higher entropy to a state of lower entropy, there is always some amount of energy involved in this transition, and an increase in the entropy of the rest of the environment greater than or equal to that of the entropy loss [7]. The negative change in entropy will require some amount of work, \u2206E.\nThe central concept of this paper concerns how smaller concepts aggregate together into larger and more complex structures. This process is a function of the complexity of each element, and the energy available to the system. The complexity of the components will delegate the degrees of freedom available to associate with parts of other constituents. The amount of energy available must be enough to supply the constituents with energy necessary to fulfill this task. We call this association between participating degrees of freedom a bond, and assert that the degrees of freedom that form the bond are covariant and thus have minimal entropy than that of the other degrees of freedom, respectively.\nThough we will primarily be concerning ourselves with numerical data, it should be stressed that these concepts apply to a panoply of possibilities including everything from subatomic particles, to atoms, to social groups, to politics. Here, the concept of energy is taken in the computational sense. That is, computational energy refers to the amount of computational work required to form bonds."}, {"heading": "2 Creating Structure", "text": "In virtually all facets of nature, more complicated structures are formed by less complicated structures, after the addition of some amount of energy. Suppose the set C contains I constituents, each with a certain quantity of degrees of freedom J as expressed in notation:cij \u2208 C, i \u2208 {0, .., I}, j \u2208 {0, ...J}. For example, a carbon atom has four valence electrons available for use in forming bonds. Note that the cij \u2208 C need not be homogeneous, as (continuing with our example from Chemistry) a carbon atom is free to form bonds with other elements and molecules. We shall call the aggregation of these constituents a meme, a term coined by Richard Dawkins in The Selfish Gene, [6].\nLet us further suppose that with each of these degrees of freedom, j, there is a certain amount of energy required to participate in the formation of a more complicated structure, and all degrees of freedom participate. This relationship may not be linear. Let \u03c1E(cij) 7\u2192 R denote the energy for each respective cij . Then the total energy, E, required to form a more complex structure, or activation energy, from cij \u2208 C is therefore\nE = I \u2211\ni\nJ \u2211\nj\n\u03c1E(cij), cij \u2208 C (1)\nAgan, it is worth noting that although the term \u2019energy\u2019 derives from physics, we can take it in this particular context to have a more general meaning, in terms of the computational effort required to establish the bond between degrees of freedom."}, {"heading": "2.1 Applications to Data Mining", "text": "We can begin to draw immediate relevance to data mining. To make sense of a data set, we must determine the elements that are correlated and thus have minimal entropy with respect to the others. Some of these degrees of freedom will participate, others will not bear any relevance. Further, some elements may not contain any information at all, as is the case with sparse data sets.\nIt is an active area of research to sieve out data with a higher information content for use in classification or clustering. Processes such as PCA or Cholesky transformation are used to reduce the amount of feature vectors for classification and find a transformation to a basis. This has a clearly defined interpretation in Linear Algebra, though we argue that information content may be another potential approach.\nIt is not uncommon to deal with data sets such that each element is characterized by thousands or hundreds of thousands or feature vectors or more, [4]. Computationally intensive techniques like PCA or the Cholesky transformation may prove intractible for data sets of this magnitude.\nUnder our framework here, using entropic self organization [5] the entropy contribution of redundant data would be very small, and that of random data\nvery large. In such a context, meaningful data would be seen as a comparable deviation between these two extremes.\nAll the notions of this section assume, of course, that the data has been properly preprocessed. After all, treating all data in an unfiltered topological sense would fail for such data sets as the iris data set, and in natural language processing."}, {"heading": "3 Meme Amplitude", "text": "Now that we have determined the total energy necessary for a meme to reach the next state, lets talk about the transition from one state to another. In this case. Let \u25b3E = E2 \u2212 E1, or the difference between the activation energy and the resting energy, or the energy required to sustain the elements unadulterated. Using \u25b3E, we can speak of the relative energy, Er or the initial energy less \u25b3E. Let us further suppose that this Er can be described by some function, y which we may also refer to as the meme amplitude.\nLet us take into account some considerations for y\u2032(t). The rate of change in y(t) is almost certainly proportional to y(t) itself, as the rate of change in the energy of the system should be proportional to that which it already has. If we include a constant of proportionality which we will call affinity A then we have the familiar y\u2032 = Ay, but taking into account the fact that \u2206E is a boundary for y, we have the familiar logistic equation:\ny\u2032 = A\n\u25b3E y(\u25b3E \u2212 y) (2)\nonto which we imposse the following boundary conditions\n1. At t = 0, we start at our initial state, where Er = 0\n2. As t \u2192 \u221e, Er \u2192 \u2206E.\nThe latter implies that after some time, the transition to \u2206E should be final, or that Er should come arbitrarily close to \u2206E as time progresses."}, {"heading": "3.1 Solutions", "text": "Equation 2 has been well studied, and its solutions fall into a broad class of functions known as sigmoid functions known for their characteristic \u2019S\u2019 shape [11], and who\u2019s applications range from their use as cumulative distribution functions in probability and statistics to activation functions in artificial neural networks. Due to the first boundary condition, we restrict the solution to t \u2265 0, which cuts off the left hand side of this \u2019S\u2019 shape, or translates the entire curve such that the lower asymptote is suitably close to t = 0."}, {"heading": "3.1.1 Growth Rate", "text": "Some comments should be made on the growth rate A. This constant will determine the rate of change in the sigmoid curve. With increasing A, the curve gets steeper and the time to reach the meme amplitude decreases. We will use the term aggressive to refer to large A. For drastically large A, the sigmoid curve approaches a step function. A will also depend on the energy applied to the system, EA, and will itself have its own boundary conditions, as the dynamics described by 2 will often break down for EA \u226b \u25b3E. For example Carbon forms graphite at (relatively) low energies, but for higher energies, it forms diamonds. This is a function of the amount of valence electrons used in the bonding process (3 and 4 for graphite and diamond, respectively). A similar analogy may be made for data and the computational effort we are willing to put into the entropic self organization algorithm."}, {"heading": "3.1.2 Amplitude", "text": "The meme amplitude is certainly not restricted to a single meme amplitude, at least not in the general case. Once the transition Er \u2192 \u2206E has been, we are free to apply the logic successively to determine the next successive meme amplitude and so forth. We will call the repeated demonstration of meme amplitude growth via 2 the hierarchical aggregation paradigm."}, {"heading": "3.1.3 Extrema", "text": "Although the solutions to 2 are monotonically increasing, some subtle nuances in its curvature provide useful insights. Consider the second derivative, which is easily obtainable from 2\ny\u2032\u2032 =\n(\nA\n\u25b3E\n)2\ny(\u25b3E \u2212 2y)(\u25b3E \u2212 y) (3)\nThe second rate of change happens at the roots of this equation, which are y = 0, 1\n2 \u25b3E,\u25b3E. The first and last are clearly the asymptotes, when the\nenergy levels off. It is the middle root that is of interest. After all, if sustainable growth is to occur, there must be a transition in the sign of the growth rate. This happens at y = 1\n2 \u25b3E."}, {"heading": "3.2 Stability and Sustainability", "text": "The reality of the dynamics described by 2 are not always so clement and predicable in reality. Although often the affinity may be treated as constant to make calculations and estimations easier, in reality it may be a function of the applied energy, and valid in some kind of threshold. If the applied energy level is outside of this threshold, the model will break down.\nFurther, once the affinity has been determined it can be used as an indicator of the stability, that the transition to the next energy level will occur. By\nobserving the disparities between the model 2, and observed data, we can gain an idea as to if this transition will occur, or if the model will return to its previous energy state, or even one below.\nIf we remove the boundedness condition, equation 2 simply becomes\ny\u2032 = Ay (4)\nwhose solutions are exponential for positiveA. However unbounded growth is unsustainable and leads to unpredictable behavior. If we observe the behavior described in 4 as opposed to 2, we may have an indicator of collapse. Additionally, if the rate of growth is not commensurate with the particular value of A, the fecundibility of the energy level transition may be suspect.\nOnce the transition to the next energy level has been made, there is a certain amount of energy necessary to sustain it. If this energy gets outside of the threshold, the model will not be able to maintain this energy level. Thus the new energy level is not guaranteed to indefinitely stable.\nThe calculation of A, its threshold, and energy to sustain it is always contingent upon underlying factors unique to the system. These must be taken into account, which may not be tractable, but perhaps at least estimable."}, {"heading": "3.3 Bubbles", "text": "Our discussion in section 3.2 has many applications and is of great importance especially to society as of the past five years. We can draw examples examples in real estate, financial markets, and population growth. Indeed this mathematics may be used to identify sustainable growth, or bubbles, periods of extraordinary inflationary growth followed by abject collapse.\nFor example, consider the well studied behavior exhibited in the growth of bacterial colonies. There is an initial lag phase as the RNA of the bacteria start to copy, followed by an explosion of exponential growth called the growth phase, follwed by a brief period of stability before the bacteria exhaust their resources resulting in an expedient decay."}, {"heading": "3.3.1 Determining Bubbles", "text": "It may be difficult to determine whether a given data set is a bubble in the making or the initial stages of a stable transition, though we may gain insight by the application of two observations discussed in this paper.\nFirst, if we are able to calculate the affinity constant, and activation energy, and we notice a disparity in the dynamics the system is exhibiting and that which was forecasted using these constants, then we may have reason to believe a bubble may be forming. Bubbles are caused by disproportionately excessive growth or extraneous factors distorting the dynamics of the system. But such factors are not perpetual and eventually the true dynamics of the system will take over. At this point, the system will fall to its natural energy state, commensurate with our calculations and what the system is able to support.\nSecond, consider the rate of change. After all, we may not be privy to the subtle inner workings of the system so as to calculate the necessary parameters. This may be especially true in very large and complicated systems, where subtle factors may contribute greatly to the outcome of the calculation. We can fall back on our discussions in 3.1.3. After all, if the system is exhibiting extraordinary growth, in order to fit model 2, there must be a point at which rate of change of the growth rate reaches zero as given in 3. We may not be able to determine this a priori, but we might be able to hypothesize whether the activation energy inferred from a given point is fungible or not."}, {"heading": "3.3.2 Outliers", "text": "The difficulty of calculating the activation energy of large and complicated systems should be stressed. After all, there may be a panoply of factors that will influence the overall dynamics of the model, and they can be highly nonlinear."}, {"heading": "4 Competing Memes", "text": "In the previous section, we considered the amplitude of only single meme. Now we will consider meme amplitude in the presence of other memes. In order to repeat the logic of section 3, we must introduce the interaction matrix \u03b1ij , which represents the effect meme i has on meme j. The underlying mechanics behind these \u03b1ij \u2019s include resource allocation, and direct competition. Thus, in the presence of N memes, 2 becomes\ndyi\ndt =\nAi\n\u25b3Ei yi(\u25b3Ei \u2212\nN \u2211\nj\n\u03b1ijyj) (5)\nwhere the constant\u25b3Ei can be absorbed into the interaction matrix without a functional effect on the resulting solutions [1]. Therefore 5 becomes\ndyi\ndt = Aiyi(1\u2212\nN \u2211\nj\n\u03b1ijyj) (6)\nwhich is recognized as a Lotka Volterra equation commonly referred to as the \u201ccompetition equation\u201d. This is another very well studied class of differential equations for which results can be found in [1, 2, 3, 10, 12, 13, 14] just to name a few."}, {"heading": "5 Conclusion", "text": "In this paper , we discussed some ramifications of the original principal detailed in [5]. In essence, the subject of this paper could be summarized as how data aggregates together to form more complex memes, how this is effected by the presence of mutiple memes, and the conditions under which this transition may\nor may not be stable. We also looked at some practical examples of these principals in source code.\nThere is still a great deal of room for future improvements. First, we can improve our understanding of the affinity parameter A and how to better calculate it. We can also apply this knowledge to more areas of study so as to be able to effectively calculate predictable amplitude transitions, burgeoning growth or bubbles.\nAdditionally, we also wish to study the most basal layers of meme aggregation and creation. The fundamental constituents, how they emerge to form systems, and how these systems can aggregate hierarchically as meme amplitude growth describes. We will also look at how this process is perpetuated in computational processes, as opposed to information in static elements and datasets."}], "references": [{"title": "Extinction in Nonautonomous T-periodic Competitive Lotka- Volterra System.", "author": ["S. Ahmad"], "venue": "Applied Mathematics and Computation", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "On the Nonautonomous Volterra-Lotka Competition Equations.", "author": ["Ahmad", "Shair"], "venue": "Proceedings of the American Mathematical Society", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1993}, {"title": "Stable Periodic Solution of a Discrete Periodic Lotka\u2013Volterra Competition System.", "author": ["Y. Chen"], "venue": "Journal of Mathematical Analysis and Applications", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Input Variable Selection for Feature Extraction in Classification Problems.", "author": ["Choi", "Sang-Il", "Jiyong Oh", "Chong-Ho Choi", "Chunghoon Kim"], "venue": "Signal Processing", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "The Computational Theory of Intelligence", "author": ["Kovach", "Daniel J", "Jr."], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "The Selfish Gene", "author": ["Dawkins", "Richard"], "venue": "Oxford: Oxford UP,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "An Introduction to Thermal Physics", "author": ["Schroeder", "Daniel V"], "venue": "San Francisco, CA: Addison Wesley,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Hill Climbing Algorithm", "author": ["Kovach", "Daniel J", "Jr."], "venue": "Kovach Technologies. N.p., n.d. Web", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Partial Permanence and Extinction in an Nspecies Nonautonomous Lotka\u2013Volterra Competitive System.", "author": ["J. Li", "J. Yan"], "venue": "Computers & Mathematics with Applications", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Differential Equations", "author": ["Farkas", "Mikl\u00f3s"], "venue": "Amsterdam: North-Holland Pub.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1977}, {"title": "A Necessary and Sufficient Condition for Permanence of a Lotka\u2013Volterra Discrete System with Delays.", "author": ["Y. Saito"], "venue": "Journal of Mathematical Analysis and Applications", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Generalized Stochastic Delay Lotka-Volterra Systems.", "author": ["Yin", "Juliang", "Xuerong Mao", "Fuke Wu"], "venue": "Stochastic Models", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Periodic Solution for a Two-Species Nonautonomous Competition Lotka\u2013Volterra Patch System with Time Delay,.", "author": ["Z. Zhang"], "venue": "Journal of Mathematical Analysis and Applications", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}], "referenceMentions": [{"referenceID": 4, "context": "Abstract In this paper, we will expound upon the concepts proffered in [5], where we proposed an information theoretic approach to intelligence in the computational sense.", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "In the previous paper [5], we laid the groundwork for what was referred to as computational intelligence (CI).", "startOffset": 22, "endOffset": 25}, {"referenceID": 4, "context": "Recall from [5, 7] that whenever a system is taken from a state of higher entropy to a state of lower entropy, there is always some amount of energy involved in this transition, and an increase in the entropy of the rest of the environment greater than or equal to that of the entropy loss [7].", "startOffset": 12, "endOffset": 18}, {"referenceID": 6, "context": "Recall from [5, 7] that whenever a system is taken from a state of higher entropy to a state of lower entropy, there is always some amount of energy involved in this transition, and an increase in the entropy of the rest of the environment greater than or equal to that of the entropy loss [7].", "startOffset": 12, "endOffset": 18}, {"referenceID": 6, "context": "Recall from [5, 7] that whenever a system is taken from a state of higher entropy to a state of lower entropy, there is always some amount of energy involved in this transition, and an increase in the entropy of the rest of the environment greater than or equal to that of the entropy loss [7].", "startOffset": 290, "endOffset": 293}, {"referenceID": 5, "context": "We shall call the aggregation of these constituents a meme, a term coined by Richard Dawkins in The Selfish Gene, [6].", "startOffset": 114, "endOffset": 117}, {"referenceID": 3, "context": "It is not uncommon to deal with data sets such that each element is characterized by thousands or hundreds of thousands or feature vectors or more, [4].", "startOffset": 148, "endOffset": 151}, {"referenceID": 4, "context": "Under our framework here, using entropic self organization [5] the entropy contribution of redundant data would be very small, and that of random data", "startOffset": 59, "endOffset": 62}, {"referenceID": 9, "context": "1 Solutions Equation 2 has been well studied, and its solutions fall into a broad class of functions known as sigmoid functions known for their characteristic \u2019S\u2019 shape [11], and who\u2019s applications range from their use as cumulative distribution functions in probability and statistics to activation functions in artificial neural networks.", "startOffset": 169, "endOffset": 173}, {"referenceID": 0, "context": "where the constant\u25b3Ei can be absorbed into the interaction matrix without a functional effect on the resulting solutions [1].", "startOffset": 121, "endOffset": 124}, {"referenceID": 0, "context": "This is another very well studied class of differential equations for which results can be found in [1, 2, 3, 10, 12, 13, 14] just to name a few.", "startOffset": 100, "endOffset": 125}, {"referenceID": 1, "context": "This is another very well studied class of differential equations for which results can be found in [1, 2, 3, 10, 12, 13, 14] just to name a few.", "startOffset": 100, "endOffset": 125}, {"referenceID": 2, "context": "This is another very well studied class of differential equations for which results can be found in [1, 2, 3, 10, 12, 13, 14] just to name a few.", "startOffset": 100, "endOffset": 125}, {"referenceID": 8, "context": "This is another very well studied class of differential equations for which results can be found in [1, 2, 3, 10, 12, 13, 14] just to name a few.", "startOffset": 100, "endOffset": 125}, {"referenceID": 10, "context": "This is another very well studied class of differential equations for which results can be found in [1, 2, 3, 10, 12, 13, 14] just to name a few.", "startOffset": 100, "endOffset": 125}, {"referenceID": 11, "context": "This is another very well studied class of differential equations for which results can be found in [1, 2, 3, 10, 12, 13, 14] just to name a few.", "startOffset": 100, "endOffset": 125}, {"referenceID": 12, "context": "This is another very well studied class of differential equations for which results can be found in [1, 2, 3, 10, 12, 13, 14] just to name a few.", "startOffset": 100, "endOffset": 125}, {"referenceID": 4, "context": "In this paper , we discussed some ramifications of the original principal detailed in [5].", "startOffset": 86, "endOffset": 89}], "year": 2015, "abstractText": "In this paper, we will expound upon the concepts proffered in [5], where we proposed an information theoretic approach to intelligence in the computational sense. We will examine data and meme aggregation, and study the effect of limited resources on the resulting meme amplitudes.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}