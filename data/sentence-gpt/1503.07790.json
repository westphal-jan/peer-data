{"id": "1503.07790", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Mar-2015", "title": "Transductive Multi-label Zero-shot Learning", "abstract": "Zero-shot learning has received increasing interest as a means to alleviate the often prohibitive expense of annotating training data for large scale recognition problems. These methods have achieved great success via learning intermediate semantic representations in the form of attributes and more recently, semantic word vectors. However, they have thus far been constrained to the single-label case, in contrast to the growing popularity and importance of more realistic multi-label data structures. Here we have taken a look at four key components of the approach.\n\n\n\nA common solution to the problem that could arise in a large-scale real-time image recognition system is the notion of a single image recognition system. This solution is to produce a single, uniform image that can be used for large-scale real-time image recognition and even for large-scale real-time real-time image recognition systems (PALs). These features are known as the \u2018S-S\u2034\u2010S\u2034\u2010S, which represent the general-purpose (PAL) approach for image recognition. For example, the first instance of the S-S\u2010S is the one for a large-scale real-time image recognition system and the second for a large-scale real-time image recognition system. These features are generally not known as S-S or S\u2010S, but are usually the only ones mentioned in the original design and implementation of PALs. For example, the S\u2010S\u2010S is the one for a large-scale real-time image recognition system. The first example of the S-S is the one for a large-scale real-time image recognition system, and the second for a large-scale real-time image recognition system.\n\n\nThe S\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\u2034\u2010S\ufffd", "histories": [["v1", "Thu, 26 Mar 2015 17:12:34 GMT  (323kb,D)", "http://arxiv.org/abs/1503.07790v1", "12 pages, 6 figures, Accepted to BMVC 2014 (oral)"]], "COMMENTS": "12 pages, 6 figures, Accepted to BMVC 2014 (oral)", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["yanwei fu", "yongxin yang", "tim hospedales", "tao xiang", "shaogang gong"], "accepted": false, "id": "1503.07790"}, "pdf": {"name": "1503.07790.pdf", "metadata": {"source": "CRF", "title": "Transductive Multi-label Zero-shot Learning", "authors": ["Yanwei Fu", "Yongxin Yang", "Timothy Hospedales", "Tao Xiang", "Shaogang Gong"], "emails": ["y.fu@qmul.ac.uk", "yongxin.yang@qmul.ac.uk", "t.hospedales@qmul.ac.uk", "t.xiang@qmul.ac.uk", "s.gong@qmul.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "There are around 30,000 human-distinguishable basic object classes [1] and many more subordinate ones. A major barrier to progress in visual recognition is thus collecting training data for many classes. Zero-shot learning (ZSL) strategies have therefore gained increasing interest as a route to side-step this prohibitive cost, as well as enabling potential new categories emerging over time to be represented and recognised. To classify instances from a class with no examples, ZSL exploits knowledge transferred from a set of seen (auxiliary) classes to unseen (test) classes, typically via an intermediate semantic representation such as attributes. This has recently been explored at large scale on ImageNet [7, 28].\nPrior zero-shot learning methods have assumed that class labels on each instance are mutually exclusive, i.e., multi-class single label classification. Nevertheless many real-world data are intrinsically multi-label. For example, an image on Flickr often contains multiple objects with cluttered background, thus requiring more than one label to describe its content.\nc\u00a9 2012. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.\nar X\niv :1\n50 3.\n07 79\n0v 1\n[ cs\nThere is an even more acute need for zero-shot learning in the case of multi-label classification. This is because different labels are often correlated (e.g. cows often appear on grass). In order to better predict these labels given an image, the label correlation must be modelled. However, for n labels, there are 2n possible multi-label combinations and to collect sufficient training samples for each combination to learn the correlations of labels is infeasible. It is thus surprising to note that there is little if any existing work on multi-label zero-shot learning. Is it because there is a trivial extension of existing single label ZSL approaches to this new problem? By assuming each label is independent from one another, it is indeed possible to decompose a multi-label ZSL problem into multiple single label ZSL problems and solve them using existing single label ZSL methods. However this does not exploit label correlation, and we demonstrate in this work that this naive extension leads to very poor label prediction for unseen classes. Any attempt to model this correlation, in particular for the unseen classes with zero-shot, is extremely challenging.\nIn this paper, a novel framework for multi-label zero-shot learning is proposed. Our framework is based on transfer learning \u2013 given a training/auxiliary dataset containing labelled images, and a test/target dataset with a set of unseen labels/classes (i.e. none of the labels appear in the training set), we aim to learn a multi-label classification model from the training set and generalise/transfer it to the test set with unseen labels. This knowledge transfer is achieved using an intermediate semantic representation in the form of the skip-gram word vectors [22, 23] learned from linguistic knowledge bases. This representation is shared between the training and test classes, thus making the transfer possible.\nMore specifically, our framework has two main components: multi-output deep regression (Mul-DR) and zero-shot multi-label prediction (ZS-MLP). Mul-DR is a 9 layer neural network that exploits the widely used convolutional neural network (CNN) layers [27], and includes two multi-output regression layers as the final layers. It learns from auxiliary data the explicit and direct mapping from raw image pixels to a linguistic representation defined by the skip-gram language model [22, 23]. With Mul-DR, each test image is now projected into the semantic word space where the unseen labels and their combinations can be represented as data points without the need to collect any visual data. ZS-MLP aims to address the multi-label ZSL problem in this semantic word space. Specifically, we note that in this space any label combination can be synthesised. We thus exhaustively synthesise the power set of all possible prototypes (i.e., combinations of multi-labels) to be treated as if they were a set of labelled instances in the space. With this synthetic dataset, we are able to extend conventional multi-label algorithms [13, 17, 32, 34], to propose two new multi-label algorithms \u2013 direct multi-label zero-shot prediction (DMP) and transductive multi-label zero-shot prediction (TraMP). However, since Mul-DR is learned using the auxiliary classes/labels, it may not generalise well to the unseen classes/labels. To overcome this problem, we further exploit self-training to adapt the Mul-DR to the test classes to improve its generalisation capability."}, {"heading": "2 Related Work", "text": "Multi-label classification Multi-label classification has been widely studied \u2013 for a review of the field please see [32, 34]. Most previous studies assume plenty of training data. Recently efforts have been made to relax this assumption. Kong et al. [17] studied transductive multi-label learning with a small set of training instances. Hariharan et al. [13] explored the label correlations of auxiliary data via a multi-label max-margin formulation and bet-\nter incorporated such label correlations as prior for multi-class zero-shot learning problem. However, none of them addresses the multi-label zero-shot learning problem tackled in this work. Zero-shot learning Multi-class single label zero-shot learning has now been widely studied using attribute-based intermediate semantic layers [3, 6, 10, 11, 19, 25] or data-driven [8, 9, 21, 30] representations. However attribute-based strategies have limited ability to scale to many classes because the attribute ontology has to be manually defined. To address this limitation, Socher et al. [31] first employed a linguistic model [15] as the intermediate semantic representation. However, this does not model the syntactic and semantic regularities in language [23] which allows vector-oriented reasoning. Such a reasoning is critical for our ZS-MLP to synthesise label combination prototypes in the semantic word space. For example, Vec(\u201cMoscow\u201d) should be much closer to Vec(\u201cRussia\u201d) +Vec(\u201ccapital\u201d) than Vec(\u201cRussia\u201d) or Vec(\u201ccapital\u201d) only. For this purpose, we employ the skip-gram language model to learn the word space, which has shown to be able to capture such syntactic regularities [22, 23]. Frome et al. [7] also used the skip-gram language model. They learned a visual-semantic embedding model \u2013 DeViSE model for single label zero-shot learning by projecting both visual and semantic information of auxiliary data into a common space. However there are a number of fundamental differences between their work and ours: (1) Comparing the DeViSE model with our Mul-DR, the learning of the mapping between images and the semantic word space by Mul-DR is more explicit and direct. We show in our experiments that this leads to better projections and thus better classification performance. (2) Our Mul-DR can generalise better to the unseen test classes thanks to our self-training based transductive learning strategy. (3) Most critically, we address the multi-label ZSL problem whilst they only focused on the single label ZSL problem. Additionally, zero-shot learning can be taken as the generalisation of class-incremental learning (C-IL) [4, 35] or life-long learning [26]. Our Contributions Overall, we make following contributions: (1) As far as we know this is the first work that addresses the multi-label zero-shot learning problem. (2) Our multioutput deep regression framework exploits correlations across dimensions while learning the direct mapping from images to intermediate skip-gram linguistic word space. (3) Within the linguistic space, two algorithms are proposed for multi-label ZSL. (4) We propose a simple self-training strategy to make the deep regression model generalise better to the unseen test classes. (5) Experimental results on benchmark multi-label datasets show the efficacy of our framework for multi-label ZSL over a variety of baselines."}, {"heading": "3 Methodology", "text": ""}, {"heading": "3.1 Problem setup", "text": "Suppose we have two datasets \u2013 source/auxiliary and target/test. The auxiliary dataset S = {XS,YS,LS,WS} has nS training instances and test dataset T = {XT ,YT ,LT ,WT} has nT test instances. We use S = {1, \u00b7 \u00b7 \u00b7 ,nS} and U = {nS +1, \u00b7 \u00b7 \u00b7 ,nT +nS} to denote the index set for instances in auxiliary and test dataset. XS = { x1, \u00b7 \u00b7 \u00b7 ,xnS } and XT = { xnS+1, \u00b7 \u00b7 \u00b7 ,xnS+nT\n} are the raw image data of all auxiliary and test instances respectively. YS = [ y1, \u00b7 \u00b7 \u00b7 ,ynS ] and\nYT = [ ynS+1, \u00b7 \u00b7 \u00b7 ,ynS+nT ] are the intermediate semantic representations of each auxiliary and test instance \u2013 in our case yi is a 100 dimensional continuous word vector for instance i in the skip-gram language model [23] space. Ls = [ l1, \u00b7 \u00b7 \u00b7 , lnS ] and LT = [ lnS+1, \u00b7 \u00b7 \u00b7 , lnS+nT ] are the label vectors for auxiliary and test dataset to be predicted respectively."}, {"heading": "4 FU ET AL: TRANSDUCTIVE MULTI-LABEL ZERO-SHOT LEARNING", "text": "The possible textual labels for each instance in LS and LT are denotedWS = { w1, \u00b7 \u00b7 \u00b7 ,wmS }\nand WT = { wmS+1, \u00b7 \u00b7 \u00b7 ,wmS+mT }\nrespectively, where mS and mT are the total number of classes/labels in each dataset. Given a label-space of mT binary labels, an instance xi can be tagged with any of the 2mT possible label subsets, li \u2208 {0,1}2\nmT , where li j = 1 means instance i has label j, and li j = 0 means otherwise. Denoting the power sets of textual labels WS andWT as P (WS) and P (WT ), for multi-label classification we need to find the optimal class label set column vector li for the i\u2212 th test instance in the power set space P (WT ). At training time XS,YS,LS,WS are all observed. At test time only new class namesWT and images XT are given, their representation YT and multi-label vectors LT are to be predicted."}, {"heading": "3.2 Learning a semantic word space", "text": "The semantic representations YS and YT are the projection of each instance into a linguistic word vector space V . The semantic word vector space is learned by using the state-of-the-art skip-gram language model [22, 23] on all English Wikipedia articles1. The space V represents almost all available English vocabulary and thus is potentially much more effective than human annotators to measure subtle similarities and differences between any two textual labels. Furthermore, V encodes the syntactic and semantic regularities in language [23] which allows vector-oriented reasoning by its \u2018compositionality\u2019 property. This property enables the critical capability of synthesising the exhaustive set of test label combinations P (WT ). Note that cosine distance is used in the space V because of its robustness against noise [22, 23]. We use v :W \u2192 V to represent the skip-gram projection from textual concepts (words) in W to vectors in V . Such a semantic space thus captures the correlations between labels without any need to collect visual examples \u2013 the meaning of multiple labels for one instance can be inferred by the sum of the word vector projections of its individual labels. Formally, we have\nYS = v(WS) \u00b7LS, YT = v(WT ) \u00b7LT (1)\nwhere v(WS) and v(WT ) are the word vector projections of the label class sets in the auxiliary and test datasets respectively. The next section discusses how to learn a predictive model for YT given visual data XT ."}, {"heading": "3.3 Multi-output deep regression", "text": "We design a multi-output deep regression (Mul-DR) model f : X \u2192V to predict the semantic representation YT \u2208 V from images XT \u2208 X where X is the space of raw image pixel intensity values. Our Mul-DR is inspired by the recent success of the deep convolutional neural network (CNN) features [18, 29] as well as the importance of modelling correlations within the semantic representation. The Mul-DR model is a neural network composed of nine layers: Layer 1\u2212 5 are convolutional layers; Layer 6\u2212 8 are fully connected layers; Layer 9 is the linear mapping layer with 100 least square regressors.\nTwo key components contribute to the effectiveness of Mul-DR. The first component (layers 1-7) provides state-of-the-art feature extraction for many computer vision tasks [27]. It directly maps the raw image to the powerful CNN features2, avoiding the pitful of bad\n1Only articles are used without any user talk/discussion. To 13 Feb. 2014, it includes 2.9 billion words and 4.33 million vocabulary (single and bi/tri-gram words).\n2However, it has more than 148.3 millions parameters and thus to prevent overfitting on small auxiliary dataset, ImageNet with 1.2 million labelled instances are used to train this component [29].\nperformance due to \u201cwrong selection\u201d of features for a given dataset. The second component (layers 8-9) provides the multi-output neural network (NN) regressors. Different from [18, 29], where the 8-th layer is an output layer for classification, the 8-th layer in our model is a fully connected layer of 1024 neurons with Rectified Linear Units (ReLUs) activation functions. This soft-thresholding non-linearity has better properties for generalisation than the widely used tanh activation units. Such a fully connected layer helps explore correlations among the different dimensions in the semantic word space. The final (9-th) layer of least square regressors provide an estimation of the 100 dimensional semantic representation in the space V .\nTo apply this neural network, we resize all images XS and XT to 231\u00d7 231 pixels. The parameters of the first components are pre-trained using ImageNet [29] while the parameters of the second component are trained by gradient descendent with auxiliary data XS and YS. At test time, Mul-DR predicts the semantic word vector y\u0302i for each unseen image xi \u2208 XT , i\u2208U . Here the hat operator indicates the variable is estimated."}, {"heading": "3.4 Zero-shot multi-label prediction", "text": "Given the estimated semantic representation Y\u0302T , we need to infer the labels L\u0302T of the test set. A straightforward solution is to decompose the multi-label classification problem into multiple independent binary classification problems which is equivalent [14] to directly solving Eq (1) by:\nL\u0302T = [ [v(WT )]T v(WT ) ]\u2020 [v(WT )]T \u00b7 Y\u0302T (2)\nwhere \u2020 is the Moore-Penrose pseudo-inverse. Eq (2) directly predicts the labels of each instance by a linear transformation of the intermediate representation Y\u0302T . In a way, this can be considered as an extension of the \u2018Direct Attribute Prediction (DAP)\u2019 [19] to the case of multi-label and continuous representation. We thus term this method exDAP. However, this does not exploit the multi-label correlations and thus has very limited expressive power [5, 33]. Hence we propose two more principled multi-label zero-shot algorithms \u2013 Direct Multi-label zero-shot Prediction (DMP) and Transductive Multi-label zero-shot Prediction(TraMP). Direct Multi-label zero-shot Prediction (DMP) Thanks to the compositionality property of V , label-correlation can be explored by synthesising the representation of every possible multi-label annotations in V: that is the power set of label vector matrix P = v(P (WT )) where P = [p1, \u00b7 \u00b7 \u00b7 ,p2mT ]. Thus Eq (2) is replaced by a nearest neighbour (NN) classifier using all the synthesised instances as training data. The label set li of instance i \u2208 U with representation y\u0302i = f (xi) is then assigned as pa \u2208 v(P(WT )), where a is the index computed by\na = argmin j \u2016 y\u0302i\u2212p j \u2016 (3)\nwhere \u2016 \u00b7 \u2016 refers to the cosine distance. Transductive Multi-label zero-shot Prediction (TraMP) DMP can explore label correlations but only insofar as encoded by the compositionality of the prototypes in V . It would be more desirable if the manifold structure of Y\u0302T given test instances XT could be used to improve multi-label zero-shot learning, i.e. via transductive learning. We therefore propose TramMP, which can be viewed as an extension the TRAM model in [17] for zero-shot learning, or a semi-supervised generalisation of Eq (3). The key idea is to use the power set of prototypes P as a known label set and to perform transductive label propagation from P\nto the inferred semantic representations Y\u0302T . We denote the index of the power set prototypes as L = {nS +nT +1, \u00b7 \u00b7 \u00b7 ,nS +nT +2mT } and its corresponding class label set as LP. Specifically, we define a k-nearest neighbour (kNN) graph among the test instances Y\u0302T and prototypes P. For any two instances i and z, where i,z \u2208 {U ,L},\n\u03c9iz =\n{ 1 Zi exp ( \u2212 \u2016y\u0302i\u2212y\u0302z\u2016 2 2\u03c32 ) , i f z \u2208 NNk ( y\u0302i, [ Y\u0302T ,P ]) 0 otherwise\n(4)\nwhere \u03c3 \u2248 median i,z=1,\u00b7\u00b7\u00b7 ,|{U ,L}|\n\u2016 y\u0302i\u2212 y\u0302z \u20162. NNk ( y\u0302i, [ Y\u0302T ,P ]) indicates the index set of k-nearest\nneighbors of y\u0302i from [ Y\u0302T ,P ] . Zi = \u2211z\u2208NNk(y\u0302i,[Y\u0302T ,P]) exp ( \u2212 \u2016y\u0302i\u2212y\u0302z\u2016 2 2\u03c32 ) is the normalisation term to make sure \u2211z \u03c9iz = 1. We define A = I\u2212\u03c9 and partition the matrix A into blocks,\nA = [\nALL ALU AUL AUU\n] and the label set of test instances can be inferred by the following\nclosed form solution [17], L\u0302T =\u2212A\u22121UUAULLP. (5)"}, {"heading": "3.5 Generalisation of multi-output deep regression", "text": "As described above, our framework consists of two key steps: applying the multi-output deep regression (Mul-DR) model to obtain the estimated semantic representation Y\u0302T , and followed by applying either DMP or TraMP to predict LT . There is however an unsolved issue, that is, our Mul-DR is learned from the auxiliary data with a different set of labels from the target/test data. This projection model is thus not guaranteed to accurately project a test image to be near its ground truth label vector in the semantic word space. For example, if our Mul-DR is learned to project images of cat and dog to the word vector representation of \u201ccat\" and \u201cdog\" (v(\u201ccat\u201d) and v(\u201cdog\u201d)), it may not accurately project an image with a person and a chair to its word vector representation of v(\u201cperson\u201d) + v(\u201cchair\u201d) when both labels were not available for learning the Mul-DR model. Any regression model will have such a generalisation problem especially when the test data are distributed differently from the auxiliary data. To make the Mul-DR model generalise better to the target domain, we transductively exploit the predicted semantic representation Y\u0302T to update the power set of label vector matrix P. In this way the target data would be better aligned with the synthesised label combination vectors in the semantic word space, thus helping generalise the Mul-DR to the target domain. This can be viewed as a semi-supervised learning (SSL) method starting from one instance for each label combination if the synthesised prototypes themselves are treated as instances. We therefore take a simple SSL strategy and perform one step of selftraining [9] to refine each prototype of P,\npi = 1 k \u2211\ny\u0302T\u2208NNk(pi,Y\u0302T ) y\u0302T (6)\nwhere P\u0304 = [p\u03041, \u00b7 \u00b7 \u00b7 , p\u03042mT ] is the updated prototype matrix and k is the number of nearest neighbour3 selected. We use the updated label vector matrix P\u0304 to compute DMP (Eq (3)) and TramMP (Eqs (4) and (5)) in our framework.\n3Note that k is not necessarily with the same k value in Eq (4)."}, {"heading": "4 Experiments", "text": "Datasets Two popular multi-label datasets \u2013 Natural Scene [33] and IAPRTC-12 [12] are used to evaluate our framework. Natural Scene consists of 2000 natural scene images where each image can be labelled as any combinations of desert, mountains, sea, sunset and trees and over 22% of the whole dataset is multi-labelled. For multi-label zero-shot learning on Natural Scene, we use a multi-class single label dataset \u2013 Scene dataset [24] (totally 2688 images) as the auxiliary dataset which have been labelled with a non-overlapping set of labels such as street, coast and highway. IAPRTC-12 consists of 20000 images and a total of 275 different labels. The labels are hierarchically organised into 6 main branches: humans, animals, food, landscape-nature, man-made and other. Our experiments consider the subset of landscape-nature branch (around 9500 images) and use the top 8 most frequent labels from this branch with over 30% of multi-label test images. For zero-shot classification on this dataset, we employ both Scene and Natural Scene as the auxiliary dataset."}, {"heading": "4.1 Experimental setup", "text": "Evaluation metrics (a) Hamming Loss: it measures the percentage of mismatches between estimated and ground-truth labels; (b) MicroF1 [16]: it evaluates both micro average of Precision (Micro-Precision) and micro average of Recall (Micro-Recall) with equal importance; (c) Ranking Loss: given the ranked list of predicted labels, it measures the number of label pairs that are incorrectly ordered by comparing their confidence scores with the ground-truth labels; (d) Average precision: given a ranked list of classes, it measures the area under precision-recall curve. These four criteria evaluate very different aspects of multi-label classification performance. Usually very few algorithms can achieve the best performance on all metrics. High values are preferred for MicroF1 and AP and vice-versa for Ranking and Hamming loss. For ease of interpretation we present 1\u2212MicroF1 and 1\u2212AP; so smaller values for all metrics are preferred. Competitors Our full framework includes two main novel components: Mul-DR and DMP/TraMP. To evaluate the effectiveness of these two components, we define several competitors by replacing each component with possible alternatives. (1) SVR+exDAP: Support Vector Regression (SVR)4 [2] is used to learn f :X \u2192V and infer the representation of each test instance. Using exDAP (Eq (2)) is a straightforward generalisation of [19, 20] to multilabel zero-shot learning. (2) SVR+DMP: SVR replaces Mul-DR and we further use DMP (Eq (3)) for classification; thus it serves as a reference to compare DMP with exDAP. (3) DeViSE+DMP: We use DeViSE [7] to learn the visual-semantic embedding into which the power set P is projected. And we use Eq (3) for final labelling in the embedding space, i.e., DMP. Thus it corresponds to the extension of [7] to multi-label zero-shot learning problems. (4) Mul-DR+exDAP: Our Mul-DR is used to learn the visual-semantic embedding, with exDAP for multi-label classification; thus it can be used to compare Multi-DR with SVR. (5) Mul-DR+DMP/TraMP: Our method with either of the two proposed ZSL algorithms used. For fair comparison, all results use self-training strategy in Eq (6) to update the prototypes."}, {"heading": "4.2 Results", "text": "Our Mul-DR model vs. alternatives The results obtained by various competitors on NaturalScene and IAPRTC-12 are shown in Fig. 1. We first compare our Mul-DR with the alter-\n4For fair comparison, we use the CNN features output by the first component (Layer 1-7) of our Mul-DR framework as the low-level feature for linear SVR used with the cost parameter set to 10.\nnative SVR and DeViSE model for learning the projection from raw images to the semantic word space. It is evident that our Mul-DR significantly improve the results on conventional SVR [19, 20] regression model (Mul-DR+DMP>SVR+DMP, Mul-DR+exDAP>SVR+exDAP). This is because that SVR treats each of the 100 semantic word space dimensions independently, whilst our multi-output regression model, as well as the DeViSE model [7] capture the correlations between different dimensions. Comparing to the DeViSE model [7] (MulDR+DMP vs. DeViSE+DMP), our regression model is also clearly better using three of the four evaluation metrics, suggesting that direct and explicit mapping between the image space and the semantic word space is a better strategy. The only case where a better result is obtained by DeViSE+DMP is on the IAPCTC-12 dataset with Hamming Loss. But this result is worth further discussion. In particular, we note that Hamming Loss treats the false alarm and missing prediction errors equally. However, for multi-label classification problem, the distribution of labels is very unbalanced and each image usually has only a small portion of labels compared to the whole label set. This is particularly the case for IAPCTC-12. The good result of DeViSE on IAPCTC-12 with better Hamming loss but worse MicroF1 and Ranking Loss is an indication that it is mostly predicting no label, and biased against making any predictions. This explains the qualitative results of DeViSE shown in Table 1. Our DMP/TraMP vs. exDAP Given the same regression model, we compared our DAP against the alternative exDAP. The results (SVR+DMP>SVR+exDAP, Mul-DR+DMP>MulDR+exDAP) show that our algorithm, which is based on synthesising the label combinations in order to encode the multi-label correlations, is superior to exDAP which treats each label independently and decomposes the multi-label classification problem as multiple single label classification problems. Comparing the two proposed algorithms \u2013 DMP and TraMP, the main difference is that TraMP transductively exploits the manifold structure of the test data for label prediction. Figure 1 shows that this tranductive label prediction algorithm is better overall. Specifically, TraMP has much better Micro-F1, Ranking Loss and AP than DMP. The NN classifier (Eq (3)) used in DMP is directly minimising the Hamming Loss. This explains why TraMP is slightly worse than DMP on IAPCTC-12 on Hamming Loss. Effectiveness of the self-training step In this experiment we compare the results of our DMP and TraMP with and without the self-training step in Eq (6). We use \u2018-\u2019 and \u2018+\u2019 to indicate algorithms without and with self-training respectively. Both DMP and TraMP use\n0\n0.2\n0.4\n0.6\n0.8 Hamming Loss H a m m in g L o ss\nNatural\u2212Scene IAPRTC\u221212 0.4\n0.5\n0.6\n0.7\n0.8\n1 \u2212\nM ic\nro F\n1\n1\u2212MicroF1\nNatural\u2212Scene IAPRTC\u221212 0\n0.2\n0.4\n0.6\n0.8\nRanking Loss\nR a\nn k\nin g\nL o\nss\nNatural\u2212Scene IAPRTC\u221212 0\n0.2\n0.4\n0.6\n0.8\n1\u2212AvePrec\n1 \u2212\nA v\ne P\nre c\nNatural\u2212Scene IAPRTC\u221212\nDMP\u2212 DMP+ TraMPTraMP+\nFigure 2: Effectiveness of self-training on DMP and TraMP.\nGroundtruth sand-beach, landscape-nature, grass sand-beach,mountain,sky mountain, sky sky Mul-DR+DMP sand-beach, landscape-nature, grass sand-beach,sky mountain, sky sky Mul-DR+TraMP sand-beach, landscape-nature, grass, ground, ground, sky,mountain, sky mountain, sky landscape-nature sand-beach DeViSE+DMP sky \u2013 \u2013 sky\nTable 1: Examples of multi-label zero-shot predictions on IAPRTC-12 dataset. Top 8 most frequent labels of landscape-nature branch are considered.\nMul-DR to infer the word vector Y\u0302T . As shown in Fig. 2, the self-training step clearly has a positive influence on the multi-label prediction performance. This result suggests that this simple step is helpful in making the learned Mul-DR model from the auxiliary data generalise better to the target data. Qualitative results Table 1 gives a qualitative comparison of multi-label annotation by our DMP and TraMP with DeViSE on IAPCTC-12. As discussed, DeViSE is too conservative on this dataset and assigns no label to most instances."}, {"heading": "5 Conclusion and future work", "text": "We have for the first time generalised zero-shot learning from the single label to the multilabel setting. It is somewhat surprising that it turns out to be possible to exploit label correlation at test time in the zero shot case \u2013 since there is no dataset of examples to learn cooccurance statistics in the conventional way. We achieve this via introducing novel strategies to exploit the compositionality of the semantic word space, and by transductively exploiting the unlabelled test data.\nBesides the proposed tailor-made multi-label algorithms \u2013 DMP and TraMP, our strategy could potentially help other existing multi-label algorithms to generalise to the multi-label zero-shot learning problem. Finally, we note that many prototypes of the power set P actually have an extremely low chance to occur in the test dataset. They should not be considered in the same way as the other more likely prototypes. Thus another line of ongoing research is to investigate how to prune low-probability prototypes from the power set P."}], "references": [{"title": "Recognition by components - a theory of human image understanding", "author": ["I. Biederman"], "venue": "Psychological Review,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1987}, {"title": "LIBSVM: a library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Cumulative attribute space for age and crowd density estimation", "author": ["Ke Chen", "Shaogang Gong", "Tao Xiang", "Chen Chang Loy"], "venue": "In CVPR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Learning with augmented class by exploiting unlabeled data", "author": ["Qing Da", "Yang Yu", "Zhi-Hua Zhou"], "venue": "In AAAI,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "A kernel method for multi-labelled classification", "author": ["Andre Elisseeff", "Jason Weston"], "venue": "In NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Learning visual attributes", "author": ["V. Ferrari", "A. Zisserman"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Devise: A deep visual-semantic embedding model andrea", "author": ["Andrea Frome", "Greg S. Corrado", "Jon Shlens", "Samy Bengio", "Jeffrey Dean", "Marc Aurelio Ranzato", "Tomas Mikolov"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Attribute learning for understanding unstructured social activity", "author": ["Yanwei Fu", "Timothy M. Hospedales", "Tao Xiang", "Shaogang Gong"], "venue": "In ECCV,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Learning multimodal latent attributes", "author": ["Yanwei Fu", "Timothy M. Hospedales", "Tao Xiang", "Shaogang Gong"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Transductive multi-view embedding for zero-shot recognition and annotation", "author": ["Yanwei Fu", "Timothy M. Hospedales", "Tao Xiang", "Zhengyong Fu", "Shaogang Gong"], "venue": "In ECCV,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Interestingness prediction by robust learning to rank", "author": ["Yanwei Fu", "Timothy M. Hospedales", "Tao Xiang", "Shaogang Gong", "Yuan Yao"], "venue": "In ECCV,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Analysis and Evaluation of Visual Information Systems Performance", "author": ["Michael Grubinger"], "venue": "PhD thesis, School of Computer Science and Mathematics, Faculty of Health,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Efficient max-margin multi-label classification with applications to zero-shot learning", "author": ["Bharath Hariharan", "S.V. Vishwanathan", "Manik Varma"], "venue": "Mach. Learn.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["Trevor Hastie", "Robert Tibshirani", "Jerome Friedman"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In ACL,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Correlated label propagation with application to multi-label learning", "author": ["Feng Kang", "Rong Jin", "Rahul Sukthankar"], "venue": "In CVPR,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Transductive multilabel learning via label set propagation", "author": ["Xiangnan Kong", "M.K. Ng", "Zhi-Hua Zhou"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["Christoph H. Lampert", "Hannes Nickisch", "Stefan Harmeling"], "venue": "In CVPR,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Attribute-based classification for zero-shot visual object categorization", "author": ["Christoph H. Lampert", "Hannes Nickisch", "Stefan Harmeling"], "venue": "IEEE TPAMI,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Re-id: Hunting attributes in the wild", "author": ["Ryan Layne", "Timothy M. Hospedales", "Shaogang Gong"], "venue": "In BMVC,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Efficient estimation of word representation in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Proceedings of Workshop at ICLR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In NIPS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["Aude Oliva", "Antonio Torralba"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "Zero-shot learning with semantic output codes", "author": ["Mark Palatucci", "Geoffrey Hinton", "Dean Pomerleau", "Tom M. Mitchell"], "venue": "In NIPS,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "A pac-bayesian bound for lifelong learning", "author": ["Anastasia Pentina", "Christoph H. Lampert"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Cnn features off-theshelf : an astounding baseline for recognition", "author": ["Ali Sharif Razavian", "Josephine Sullivan", "Stefan Carlsson"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Evaluating knowledge transfer and zero-shot learning in a large-scale setting", "author": ["Marcus Rohrbach", "Michael Stark", "Bernt Schiele"], "venue": "In CVPR,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Pierre Sermanet", "David Eigen", "Xiang Zhang", "Michael Mathieu", "Rob Fergus", "Yann LeCun"], "venue": "In ICLR,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Augmented attribute representations", "author": ["Viktoriia Sharmanska", "Novi Quadrianto", "Christoph H. Lampert"], "venue": "In ECCV,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["Richard Socher", "Milind Ganjoo", "Hamsa Sridhar", "Osbert Bastani", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Multi-label classification with unlabeled data: An inductive approach", "author": ["Le Wu", "Min-Ling Zhang"], "venue": "In ACML, pages 197\u2013212,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Ml-knn: A lazy learning approach to multi-label learning", "author": ["Min-Ling Zhang", "Zhi-Hua Zhou"], "venue": "Pattern Recognition,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}, {"title": "A review on multi-label learning algorithms", "author": ["Min-Ling Zhang", "Zhi-Hua Zhou"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Hybrid decision tree", "author": ["Zhi-Hua Zhou", "Zhao-Qian Chen"], "venue": "Knowledge-Based Systems,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "There are around 30,000 human-distinguishable basic object classes [1] and many more subordinate ones.", "startOffset": 67, "endOffset": 70}, {"referenceID": 6, "context": "This has recently been explored at large scale on ImageNet [7, 28].", "startOffset": 59, "endOffset": 66}, {"referenceID": 27, "context": "This has recently been explored at large scale on ImageNet [7, 28].", "startOffset": 59, "endOffset": 66}, {"referenceID": 21, "context": "This knowledge transfer is achieved using an intermediate semantic representation in the form of the skip-gram word vectors [22, 23] learned from linguistic knowledge bases.", "startOffset": 124, "endOffset": 132}, {"referenceID": 22, "context": "This knowledge transfer is achieved using an intermediate semantic representation in the form of the skip-gram word vectors [22, 23] learned from linguistic knowledge bases.", "startOffset": 124, "endOffset": 132}, {"referenceID": 26, "context": "Mul-DR is a 9 layer neural network that exploits the widely used convolutional neural network (CNN) layers [27], and includes two multi-output regression layers as the final layers.", "startOffset": 107, "endOffset": 111}, {"referenceID": 21, "context": "It learns from auxiliary data the explicit and direct mapping from raw image pixels to a linguistic representation defined by the skip-gram language model [22, 23].", "startOffset": 155, "endOffset": 163}, {"referenceID": 22, "context": "It learns from auxiliary data the explicit and direct mapping from raw image pixels to a linguistic representation defined by the skip-gram language model [22, 23].", "startOffset": 155, "endOffset": 163}, {"referenceID": 12, "context": "With this synthetic dataset, we are able to extend conventional multi-label algorithms [13, 17, 32, 34], to propose two new multi-label algorithms \u2013 direct multi-label zero-shot prediction (DMP) and transductive multi-label zero-shot prediction (TraMP).", "startOffset": 87, "endOffset": 103}, {"referenceID": 16, "context": "With this synthetic dataset, we are able to extend conventional multi-label algorithms [13, 17, 32, 34], to propose two new multi-label algorithms \u2013 direct multi-label zero-shot prediction (DMP) and transductive multi-label zero-shot prediction (TraMP).", "startOffset": 87, "endOffset": 103}, {"referenceID": 31, "context": "With this synthetic dataset, we are able to extend conventional multi-label algorithms [13, 17, 32, 34], to propose two new multi-label algorithms \u2013 direct multi-label zero-shot prediction (DMP) and transductive multi-label zero-shot prediction (TraMP).", "startOffset": 87, "endOffset": 103}, {"referenceID": 33, "context": "With this synthetic dataset, we are able to extend conventional multi-label algorithms [13, 17, 32, 34], to propose two new multi-label algorithms \u2013 direct multi-label zero-shot prediction (DMP) and transductive multi-label zero-shot prediction (TraMP).", "startOffset": 87, "endOffset": 103}, {"referenceID": 31, "context": "Multi-label classification Multi-label classification has been widely studied \u2013 for a review of the field please see [32, 34].", "startOffset": 117, "endOffset": 125}, {"referenceID": 33, "context": "Multi-label classification Multi-label classification has been widely studied \u2013 for a review of the field please see [32, 34].", "startOffset": 117, "endOffset": 125}, {"referenceID": 16, "context": "[17] studied transductive multi-label learning with a small set of training instances.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] explored the label correlations of auxiliary data via a multi-label max-margin formulation and bet-", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Zero-shot learning Multi-class single label zero-shot learning has now been widely studied using attribute-based intermediate semantic layers [3, 6, 10, 11, 19, 25] or data-driven [8, 9, 21, 30] representations.", "startOffset": 142, "endOffset": 164}, {"referenceID": 5, "context": "Zero-shot learning Multi-class single label zero-shot learning has now been widely studied using attribute-based intermediate semantic layers [3, 6, 10, 11, 19, 25] or data-driven [8, 9, 21, 30] representations.", "startOffset": 142, "endOffset": 164}, {"referenceID": 9, "context": "Zero-shot learning Multi-class single label zero-shot learning has now been widely studied using attribute-based intermediate semantic layers [3, 6, 10, 11, 19, 25] or data-driven [8, 9, 21, 30] representations.", "startOffset": 142, "endOffset": 164}, {"referenceID": 10, "context": "Zero-shot learning Multi-class single label zero-shot learning has now been widely studied using attribute-based intermediate semantic layers [3, 6, 10, 11, 19, 25] or data-driven [8, 9, 21, 30] representations.", "startOffset": 142, "endOffset": 164}, {"referenceID": 18, "context": "Zero-shot learning Multi-class single label zero-shot learning has now been widely studied using attribute-based intermediate semantic layers [3, 6, 10, 11, 19, 25] or data-driven [8, 9, 21, 30] representations.", "startOffset": 142, "endOffset": 164}, {"referenceID": 24, "context": "Zero-shot learning Multi-class single label zero-shot learning has now been widely studied using attribute-based intermediate semantic layers [3, 6, 10, 11, 19, 25] or data-driven [8, 9, 21, 30] representations.", "startOffset": 142, "endOffset": 164}, {"referenceID": 7, "context": "Zero-shot learning Multi-class single label zero-shot learning has now been widely studied using attribute-based intermediate semantic layers [3, 6, 10, 11, 19, 25] or data-driven [8, 9, 21, 30] representations.", "startOffset": 180, "endOffset": 194}, {"referenceID": 8, "context": "Zero-shot learning Multi-class single label zero-shot learning has now been widely studied using attribute-based intermediate semantic layers [3, 6, 10, 11, 19, 25] or data-driven [8, 9, 21, 30] representations.", "startOffset": 180, "endOffset": 194}, {"referenceID": 20, "context": "Zero-shot learning Multi-class single label zero-shot learning has now been widely studied using attribute-based intermediate semantic layers [3, 6, 10, 11, 19, 25] or data-driven [8, 9, 21, 30] representations.", "startOffset": 180, "endOffset": 194}, {"referenceID": 29, "context": "Zero-shot learning Multi-class single label zero-shot learning has now been widely studied using attribute-based intermediate semantic layers [3, 6, 10, 11, 19, 25] or data-driven [8, 9, 21, 30] representations.", "startOffset": 180, "endOffset": 194}, {"referenceID": 30, "context": "[31] first employed a linguistic model [15] as the intermediate semantic representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[31] first employed a linguistic model [15] as the intermediate semantic representation.", "startOffset": 39, "endOffset": 43}, {"referenceID": 22, "context": "However, this does not model the syntactic and semantic regularities in language [23] which allows vector-oriented reasoning.", "startOffset": 81, "endOffset": 85}, {"referenceID": 21, "context": "For this purpose, we employ the skip-gram language model to learn the word space, which has shown to be able to capture such syntactic regularities [22, 23].", "startOffset": 148, "endOffset": 156}, {"referenceID": 22, "context": "For this purpose, we employ the skip-gram language model to learn the word space, which has shown to be able to capture such syntactic regularities [22, 23].", "startOffset": 148, "endOffset": 156}, {"referenceID": 6, "context": "[7] also used the skip-gram language model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Additionally, zero-shot learning can be taken as the generalisation of class-incremental learning (C-IL) [4, 35] or life-long learning [26].", "startOffset": 105, "endOffset": 112}, {"referenceID": 34, "context": "Additionally, zero-shot learning can be taken as the generalisation of class-incremental learning (C-IL) [4, 35] or life-long learning [26].", "startOffset": 105, "endOffset": 112}, {"referenceID": 25, "context": "Additionally, zero-shot learning can be taken as the generalisation of class-incremental learning (C-IL) [4, 35] or life-long learning [26].", "startOffset": 135, "endOffset": 139}, {"referenceID": 22, "context": "YS = [ y1, \u00b7 \u00b7 \u00b7 ,ynS ] and YT = [ ynS+1, \u00b7 \u00b7 \u00b7 ,ynS+nT ] are the intermediate semantic representations of each auxiliary and test instance \u2013 in our case yi is a 100 dimensional continuous word vector for instance i in the skip-gram language model [23] space.", "startOffset": 248, "endOffset": 252}, {"referenceID": 21, "context": "The semantic word vector space is learned by using the state-of-the-art skip-gram language model [22, 23] on all English Wikipedia articles1.", "startOffset": 97, "endOffset": 105}, {"referenceID": 22, "context": "The semantic word vector space is learned by using the state-of-the-art skip-gram language model [22, 23] on all English Wikipedia articles1.", "startOffset": 97, "endOffset": 105}, {"referenceID": 22, "context": "Furthermore, V encodes the syntactic and semantic regularities in language [23] which allows vector-oriented reasoning by its \u2018compositionality\u2019 property.", "startOffset": 75, "endOffset": 79}, {"referenceID": 21, "context": "Note that cosine distance is used in the space V because of its robustness against noise [22, 23].", "startOffset": 89, "endOffset": 97}, {"referenceID": 22, "context": "Note that cosine distance is used in the space V because of its robustness against noise [22, 23].", "startOffset": 89, "endOffset": 97}, {"referenceID": 17, "context": "Our Mul-DR is inspired by the recent success of the deep convolutional neural network (CNN) features [18, 29] as well as the importance of modelling correlations within the semantic representation.", "startOffset": 101, "endOffset": 109}, {"referenceID": 28, "context": "Our Mul-DR is inspired by the recent success of the deep convolutional neural network (CNN) features [18, 29] as well as the importance of modelling correlations within the semantic representation.", "startOffset": 101, "endOffset": 109}, {"referenceID": 26, "context": "The first component (layers 1-7) provides state-of-the-art feature extraction for many computer vision tasks [27].", "startOffset": 109, "endOffset": 113}, {"referenceID": 28, "context": "2 million labelled instances are used to train this component [29].", "startOffset": 62, "endOffset": 66}, {"referenceID": 17, "context": "Different from [18, 29], where the 8-th layer is an output layer for classification, the 8-th layer in our model is a fully connected layer of 1024 neurons with Rectified Linear Units (ReLUs) activation functions.", "startOffset": 15, "endOffset": 23}, {"referenceID": 28, "context": "Different from [18, 29], where the 8-th layer is an output layer for classification, the 8-th layer in our model is a fully connected layer of 1024 neurons with Rectified Linear Units (ReLUs) activation functions.", "startOffset": 15, "endOffset": 23}, {"referenceID": 28, "context": "The parameters of the first components are pre-trained using ImageNet [29] while the parameters of the second component are trained by gradient descendent with auxiliary data XS and YS.", "startOffset": 70, "endOffset": 74}, {"referenceID": 13, "context": "A straightforward solution is to decompose the multi-label classification problem into multiple independent binary classification problems which is equivalent [14] to directly solving Eq (1) by: L\u0302T = [ [v(WT )] v(WT ) ]\u2020 [v(WT )] \u00b7 \u0176T (2)", "startOffset": 159, "endOffset": 163}, {"referenceID": 18, "context": "In a way, this can be considered as an extension of the \u2018Direct Attribute Prediction (DAP)\u2019 [19] to the case of multi-label and continuous representation.", "startOffset": 92, "endOffset": 96}, {"referenceID": 4, "context": "However, this does not exploit the multi-label correlations and thus has very limited expressive power [5, 33].", "startOffset": 103, "endOffset": 110}, {"referenceID": 32, "context": "However, this does not exploit the multi-label correlations and thus has very limited expressive power [5, 33].", "startOffset": 103, "endOffset": 110}, {"referenceID": 16, "context": "We therefore propose TramMP, which can be viewed as an extension the TRAM model in [17] for zero-shot learning, or a semi-supervised generalisation of Eq (3).", "startOffset": 83, "endOffset": 87}, {"referenceID": 16, "context": "closed form solution [17], L\u0302T =\u2212A\u22121 UUAULLP.", "startOffset": 21, "endOffset": 25}, {"referenceID": 8, "context": "We therefore take a simple SSL strategy and perform one step of selftraining [9] to refine each prototype of P,", "startOffset": 77, "endOffset": 80}, {"referenceID": 32, "context": "4 Experiments Datasets Two popular multi-label datasets \u2013 Natural Scene [33] and IAPRTC-12 [12] are used to evaluate our framework.", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "4 Experiments Datasets Two popular multi-label datasets \u2013 Natural Scene [33] and IAPRTC-12 [12] are used to evaluate our framework.", "startOffset": 91, "endOffset": 95}, {"referenceID": 23, "context": "For multi-label zero-shot learning on Natural Scene, we use a multi-class single label dataset \u2013 Scene dataset [24] (totally 2688 images) as the auxiliary dataset which have been labelled with a non-overlapping set of labels such as street, coast and highway.", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "Evaluation metrics (a) Hamming Loss: it measures the percentage of mismatches between estimated and ground-truth labels; (b) MicroF1 [16]: it evaluates both micro average of Precision (Micro-Precision) and micro average of Recall (Micro-Recall) with equal importance; (c) Ranking Loss: given the ranked list of predicted labels, it measures the number of label pairs that are incorrectly ordered by comparing their confidence scores with the ground-truth labels; (d) Average precision: given a ranked list of classes, it measures the area under precision-recall curve.", "startOffset": 133, "endOffset": 137}, {"referenceID": 1, "context": "(1) SVR+exDAP: Support Vector Regression (SVR)4 [2] is used to learn f :X \u2192V and infer the representation of each test instance.", "startOffset": 48, "endOffset": 51}, {"referenceID": 18, "context": "Using exDAP (Eq (2)) is a straightforward generalisation of [19, 20] to multilabel zero-shot learning.", "startOffset": 60, "endOffset": 68}, {"referenceID": 19, "context": "Using exDAP (Eq (2)) is a straightforward generalisation of [19, 20] to multilabel zero-shot learning.", "startOffset": 60, "endOffset": 68}, {"referenceID": 6, "context": "(3) DeViSE+DMP: We use DeViSE [7] to learn the visual-semantic embedding into which the power set P is projected.", "startOffset": 30, "endOffset": 33}, {"referenceID": 6, "context": "Thus it corresponds to the extension of [7] to multi-label zero-shot learning problems.", "startOffset": 40, "endOffset": 43}, {"referenceID": 18, "context": "It is evident that our Mul-DR significantly improve the results on conventional SVR [19, 20] regression model (Mul-DR+DMP>SVR+DMP, Mul-DR+exDAP>SVR+exDAP).", "startOffset": 84, "endOffset": 92}, {"referenceID": 19, "context": "It is evident that our Mul-DR significantly improve the results on conventional SVR [19, 20] regression model (Mul-DR+DMP>SVR+DMP, Mul-DR+exDAP>SVR+exDAP).", "startOffset": 84, "endOffset": 92}, {"referenceID": 6, "context": "This is because that SVR treats each of the 100 semantic word space dimensions independently, whilst our multi-output regression model, as well as the DeViSE model [7] capture the correlations between different dimensions.", "startOffset": 164, "endOffset": 167}, {"referenceID": 6, "context": "Comparing to the DeViSE model [7] (MulDR+DMP vs.", "startOffset": 30, "endOffset": 33}], "year": 2015, "abstractText": "Zero-shot learning has received increasing interest as a means to alleviate the often prohibitive expense of annotating training data for large scale recognition problems. These methods have achieved great success via learning intermediate semantic representations in the form of attributes and more recently, semantic word vectors. However, they have thus far been constrained to the single-label case, in contrast to the growing popularity and importance of more realistic multi-label data. In this paper, for the first time, we investigate and formalise a general framework for multi-label zero-shot learning, addressing the unique challenge therein: how to exploit multi-label correlation at test time with no training data for those classes? In particular, we propose (1) a multi-output deep regression model to project an image into a semantic word space, which explicitly exploits the correlations in the intermediate semantic layer of word vectors; (2) a novel zero-shot learning algorithm for multi-label data that exploits the unique compositionality property of semantic word vector representations; and (3) a transductive learning strategy to enable the regression model learned from seen classes to generalise well to unseen classes. Our zero-shot learning experiments on a number of standard multi-label datasets demonstrate that our method outperforms a variety of baselines.", "creator": "LaTeX with hyperref package"}}}