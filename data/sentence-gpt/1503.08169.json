{"id": "1503.08169", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Mar-2015", "title": "RankMap: A Platform-Aware Framework for Distributed Learning from Dense Datasets", "abstract": "This paper introduces RankMap, a platform-aware end-to-end framework for efficient execution of a broad class of iterative learning algorithms for massive and dense datasets. In contrast to the existing dense (iterative) data analysis methods that are oblivious to the platform, for the first time, we introduce novel scalable data transformation and mapping algorithms that enable optimizing for the underlying computing platforms' cost/constraints. The cost is defined by the number of arithmetic and (within-platform) message passing operations incurred by the variable updates in each iteration, while the constraints are set by the available memory resources. These operations allow the application to quickly transition between the memory resources (at compile time, using a number of processes), the size of the cache, the overall memory space (the size of the memory), and the time-recovery rate (the overall cache rate). In addition, the cost of building a scalable network to scale, and the amount of data needed for an exhaustive analysis of the datasets involved (each iteration will also be constrained by the number of data being sent). The cost of constructing a scalable network is estimated to be about 10 times more expensive than the cost of building a scalable network to run the entire computation, at a cost of approximately $30 to $40,000. Our goal is to demonstrate the computational power of the network as the price of the process increases, allowing us to create scalable algorithms that are efficient in minimizing the cost of learning.", "histories": [["v1", "Fri, 27 Mar 2015 18:02:51 GMT  (1394kb,D)", "https://arxiv.org/abs/1503.08169v1", null], ["v2", "Thu, 27 Oct 2016 14:29:44 GMT  (988kb,D)", "http://arxiv.org/abs/1503.08169v2", "13 pages, 10 figures"]], "reviews": [], "SUBJECTS": "cs.DC cs.LG", "authors": ["azalia mirhoseini", "eva l dyer", "ebrahim m songhori", "richard g baraniuk", "farinaz koushanfar"], "accepted": false, "id": "1503.08169"}, "pdf": {"name": "1503.08169.pdf", "metadata": {"source": "CRF", "title": "RankMap: A Framework for Distributed Learning from Dense Datasets", "authors": ["Azalia Mirhoseini", "Eva. L. Dyer", "Ebrahim. M. Songhori", "Richard Baraniuk", "Farinaz Koushanfar"], "emails": ["farinaz5}@rice.edu,", "edyer2@ric.org"], "sections": [{"heading": null, "text": "Index Terms\u2014Dense and Big Data, Large-Scale Distributed Computing, Iterative Machine Learning, Subspace Factorization.\nI. INTRODUCTION Many modern learning algorithms are based on exploring the underlying patterns, correlations, and dependencies present across the signals in the dataset. Some prominent examples of such algorithms and their applications are linear or penalized regression [29], power iterations [30], belief propagation [48], and expectation maximization [38], [39]. In all of these settings, solving the underlying objective function requires iterative updates of parameters of interest until convergence is achieved. Such iterative updates often require matrix multiplications that involve the data dependency or Gram matrix. In scenarios where data is too large to fit on a single computing node and must be distributed, iterative dependency-based updates become challenging as they incur large computation and communication costs.\nTo facilitate parallel computing, a number of distributed abstractions that target iterative learning algorithms have been developed, e.g., Pregel [35], Spark [50], and GraphLab [34]. These abstractions adopt a graph-parallel model which consists of a sparse graph and a kernel function that runs in parallel on each vertex [25]. Performance gains are achieved due to the communication-minimizing partitioning of the graph and effective control of data movement.\nWhile graph-parallelism has been shown to accelerate machine learning and signal processing tasks for sparse graphs,\nthis approach cannot be readily applied when the data exhibit a non-sparse dependency matrix. The storage of such data in a graph format becomes very inefficient as it requires storing a large number of edges (pairwise non-zero correlation values) for each vertex (data sample). In addition, finding efficient graph cuts and partitions is infeasible when dense dependencies exist. Data with dense dependencies appear in a wide range of fields such as computer vision, medical image processing, boundary element methods and their applications, and N-body problems [8], [26]. Thus, finding efficient solutions for running iterative learning algorithms on densely dependent data is of paramount importance.\nIn this paper, we introduce RankMap, a novel distributed framework for efficient execution of a broad class of iterative learning algorithms on datasets with dense but structured dependencies. Our key observation is that, despite the apparent high dimensionality of data, in many settings, dense datasets are low rank or lie on a union of much lower dimensional subspaces. We exploit this property to reduce the overhead associated with processing dense data dependencies\u2014a factor which has rendered the currently available graph-parallel abstractions impractical for processing dense datasets. RankMap provides a set of interfaces and transformations that enable efficient data-aware content analysis, as well as coordinated mapping and optimization to the specifics of the underlying hardware components. RankMap significantly improves the runtime and energy consumption of the learning algorithms by reducing the amount of required computation, distributed system communication, and storage.\nTo accelerate large matrix multiplications required to compute an iterative update, we decompose dense but structured data and rewrite it as a product of two matrices with far fewer non-zeros than the original data. The decomposed data is then used in subsequent iterative learning algorithms in lieu of the original dense data. We introduce a host of automated methods for partitioning the decomposed factors and ordering the computation flow in a distributed setting. The partitioning algorithm is efficient (within a bound from the optimum) and has a constant runtime. We introduce two different representations and accompanying computational models (a matrix-based and a vertex-centric model) to compute an update. Depending on the data domain and the sparsity of the decomposed components, there are different regimes where each of these two models deliver highest efficiency.\nWe provide APIs for both matrix-based and vertex-centric\nar X\niv :1\n50 3.\n08 16\n9v 2\n[ cs\n.D C\n] 2\n7 O\nct 2\n01 6\n2 iterative update models on the transformed data. Our APIs are open-source and available at [3]. Our matrix-based API uses the general Message Passing Interface (MPI). Our vertexcentric API is based upon the GraphLab programming model. We develop an efficient mapping of the iterative computations\non the sparsified decomposed data within the constraints of the GraphLab distributed framework. Both APIs are written in C++. We evaluate RankMap on the Amazon Elastic Cloud (EC2) computing service and IBM iDataPlex computer cluster. Our experiments utilize up to 244 cores on 12 large computing nodes.\nOur explicit contributions are as follows:\n\u2022 We propose RankMap, a large-scale learning framework that proposes sparse transformations for accelerating iterative learning algorithms on dense but structured data. \u2022 We introduce a scalable transformation which maps structured (low-rank) data onto two matrices which contain far fewer number of non-zeros. A systematic way to tune the transformation error to achieve a desired level of accuracy in the learning applications is provided. \u2022 We develop efficient distributed computational models to conduct iterative updates on the decomposed data. Highly effective partitioning methods for the decomposed data along with data-aware performance bounds are provided. \u2022 We perform proof-of-concept evaluation on applications including eigenvalue decomposition, denoising, and classification that demonstrate up to two orders of magnitude improvement in runtime and memory footprint.\nThis paper is organized as follows. In Section II, we provide a global overview of RankMap. In Section III, we review related work. In Section IV, we introduce our novel data transformation algorithm. In Section V, we study the impact of the decomposition error on learning and provide a method for automatically tuning RankMap to produce a user-specified learning error. In Section VI, we introduce schemes for costefficient distributed partitioning, along with the details of our graph and matrix-based computational models. In Section VII, we provide evaluation results on multiple synthetic and real-world datasets. Finally, in Section VIII, we discuss the practicality of our framework, describe domain-specific usecases of each of the proposed computational models, and conclude."}, {"heading": "II. RANKMAP FRAMEWORK", "text": ""}, {"heading": "A. Overview and approach", "text": "In this paper, we introduce RankMap, a distributed dataaware framework that efficiently executes learning algorithms applied to the data. The main idea underlying our approach is to leverage structure in large collections of data to decompose the correlation (Gram) matrix of the data such that the system costs (e.g, runtime, memory, and energy) associated with iterative learning algorithms are significantly reduced.\nLet the data matrix A \u2208 Rm\u00d7n denote a collection of n signals of m-dimensions, and G = ATA denote the Gram matrix. Many learning algorithms iteratively update a solution\nFig. 1: Schematic of decomposing a dense data matrix into the product of a small dense matrix and a large sparse matrix.\nvector, denoted by x, according to an update function of the following form:\nxiter+1 = f(Gxiter), (1)\nwhere f(\u00b7) is a low-complexity function and iter is the current iteration. Examples are included in Section II-B.\nWhen G is massive and dense, each distributed update in (1) becomes very expensive. To cope with large data sizes, RankMap creates an approximation to G, denoted by G\u0302, to reduce the cost of an update. To be more specific, our aim is to decompose the data matrix A into two components, i.e., A\u0302 = DV, where D \u2208 Rm\u00d7l contains a subset of columns from A, V \u2208 Rl\u00d7n is a sparse matrix, and rank(A) \u2264 l n (see Figure 1). After decomposing A, we then efficiently partition the decomposed data and perform distributed updates using G\u0302 = (DV)T (DV). Mapping the original dense data to a decomposed model directly reduces the memory usage, and the costly computational operations and communication incurred by the iterative updates.\nWhen A is low-rank, it is possible to construct a reduced decomposition that is exact (A = DV). However, we demonstrate that for many real-world datasets, we can achieve significant performance improvements in exchange for a small decomposition error (A \u2248 DV). We discuss the connection between the decomposition error and the accuracy of a target learning method as well as strategies for tuning the decomposition to achieve a desired level of accuracy in the iterative learning algorithms.\nRankMap consists of three main components (see Figure 2): (i) A scalable data decomposition that shrinks the size of the data set by leveraging the data\u2019s structure, (ii) A data partitioning scheme along with an execution flow for performing iterative updates on the decomposed data that significantly reduces the distributed computing costs, (iii) A systematic method for tuning the decomposition error (denoted by \u03b4D) to achieve the desired level of approximation error in the learning algorithms (denoted by \u03b4L)."}, {"heading": "B. Target applications", "text": "Our framework can be used for a broad class of optimization problems that are solved via iterative updates based upon the Gram matrix. A large number of objective functions used in machine learning, e.g., penalized regression methods such as the LASSO or BPDN [9], and ridge regression [29], are typically solved using iterative updates. In all these settings, the complexity of executing these methods is dominated by costly iterative computation on the Gram matrix of the dataset.\nTo ground RankMap in real-world problems, we now discuss two particular learning algorithms that are evaluated\n3\nin this paper: (i) sparse approximation and (ii) the power method for eigenvalue decomposition.\n(i) Sparse approximation for image denoising and classification. Sparse representation is used in a wide range of signal processing and machine learning applications, including denoising [9], classification [47], clustering [18], and outlier detection [17]. The sparse approximation objective function can be written in terms of the `1-norm as follows:\nargmin x\n\u2016Ax\u2212 y\u20162 + \u03bb\u2016x\u20161, (2)\nwhere x is a sparse representation of y with respect to A and \u03bb is a regularization coefficient (increasing this parameter promotes sparser solutions).\nIn an image denoising application, y is a noisy image, x is a sparse coefficient vector, and Ax is a denoised approximation of y. In a classification application, the sparse coefficient vector x is used to determine which class a test signal y belongs to. This can be done by first measuring the sum of the coefficients in each class and then finding the class that has largest number of nonzero coefficient.\nIn the sequel, we evaluate the performance of RankMap for accelerating first-order methods for sparse approximation via `1-minimization. This sparse approximation problem can be solved using the following iterative soft thresholding (IST) algorithm [11]:\nxiter+1 = f(xiter \u2212 \u03b3(Gxiter \u2212ATy)), (3)\nwhere f(.) is a low-complexity thresholding operation (e.g., a soft-thresholding operator [11]) to account for the term \u03bb\u2016x\u20161 at each iteration, and \u03b3 is the step size. In our evaluations, we employ a variant of this algorithm called FISTA [6]. FISTA is an example of a projected gradient descent (PGD) methods [32] which provide a generalization of standard gradient descent methods for certain classes of non-smooth objective functions. RankMap can be readily applied to cost functions\nthat can be solved using PGD.\n(ii) Power method for eigenvalue decomposition. The power method is a simple and iterative algorithm that can be used to sequentially find the eigenvectors and eigenvalues of a matrix in descending order. Recall that an eigenvector x of a matrix A satisfies the following relationship Ax = \u03c3x, where \u03c3 is the eigenvalue associated with the eigenvector x. To find an eigenvector of the symmetric matrix G = ATA, the power method utilizes the following iterative update:\nxiter+1 = Gxiter\n\u2016Gxiter\u20162 . (4)\nOnce the power method converges to an estimate of an eigenvector x, the contribution of this eigenvector is removed from A, and the power method is applied again to the residual to find the next eigenvector.\nIn both applications described above, the main cost of each iteration is due to the computation of Gx, especially when G is large, dense, and distributed onto multiple computing nodes. For example, as a case-study in our evaluations, we perform image reconstruction on a dataset where A is a collection of light field image patches of size 18, 496\u00d7100, 000. In this case, to reconstruct a single noisy image patch y, more than 3.6 billion floating point multiplications are required to perform Gxiter = ATAxiter per iteration."}, {"heading": "III. BACKGROUND AND RELATED WORK", "text": "In this section, we provide background on methods for\nmatrix factorization and describe related work."}, {"heading": "A. Methods for matrix factorization", "text": "High-dimensional data can be modeled by the low rank structures that are present in the data. Extracting low dimensional structures not only reduces dimensionality, but also mitigates the effect of noise and improves the performance of learning and inference tasks [17], [19].\n4 1) Singular value decomposition (SVD): In settings where the column span of A admits a low rank model, the SVD provides a powerful tool for forming low rank approximations. Let A = USVT be the SVD. The best rank-k approximation of A is given by Ak = Uk\u03a3kVTk , where Uk \u2208 Rm\u00d7k and Vk \u2208 Rn\u00d7k are the truncated left and right singular vectors (first k columns of U and V) and \u03a3k \u2208 Rk\u00d7k contains the first k singular values of A along its diagonal. The rank of Ak equals the number of non-zero singular values. The truncated SVD also provides the solution to principal components analysis (PCA), which seeks to find a k-dimensional subspace that best approximates A in the least-squares sense [46].\nThe complexity of computing the SVD directly is m2n. Thus, for large datasets, the power method is used to find the eigenvectors of ATA and AAT , which correspond to the right and left singular vectors respectively.\n2) Sparse factorization: The SVD provides a closed-form solution for finding the best rank-k approximation to a matrix. However, in many settings, enforcing sparse structure, either in the left or right singular vectors can provide a more faithful and compact decomposition of the data. Two widely used sparse factorization methods include sparse PCA (SPCA) [51] and dictionary learning (DL) [4]. However, these approaches are often not applied to large datasets since computing an update of both the left and right factor matrices, at each iteration is costly. To solve SPCA on big datasets, a generalized power method can be employed [30]. The basic idea behind using the power method to find sparse principal components is to simply threshold the output of each power iteration to ensure the resulting eigenvectors are sparse. Unfortunately, the convergence of this method is much slower than standard power iterations.\n3) Column subset selection (CSS)-based matrix factorization: An alternative strategy for low rank matrix factorization is to form a decomposition based upon columns (or rows) from the data. CSS-based solutions form an approximate matrix decomposition in which one factorized component is a subset of the columns of the data itself [16]. CSS-based approaches have been used to provide a scalable and efficient strategy for finding approximate solutions to least-squares regression [15], Gramian matrix decomposition [16], image denoising and clustering [17], and also in spectral clustering [21]. After selecting columns from A, the remaining unsampled columns are completed by finding the least-squares projection onto the subspace spanned by the sampled columns."}, {"heading": "B. Generic distributed abstractions", "text": "A number of successful distributed abstractions for processing large datasets on clusters have been proposed. Examples include MapReduce [13], Apache Spark [49], and SystemML [23]. However, these models become less efficient for applications when direct data-parallelism does not exist. Several new distributed abstractions have been proposed that model data dependency in a graph format, most notably Pregel [35] and GraphLab [34]. They use a vertex-centric computation model, in which the user-defined programs are executed on each vertex in parallel. As graph-based abstractions are suited for\nsparse datasets, efficient data partitioning is not possible when the graph-representation of the data is densely connected. Furthermore, such tools mostly rely on the communication between the vertices for computation. When the data is densely connected, the resulting communication congestion makes the computation dramatically slow. Because of this, most of these tools are designed based on the assumption that the input data is sparse [25], [35], [49].\nBy design, MapReduce-based solutions are not guaranteed to be fast, instead they provide easy and reusable programming frameworks that operate on very large datasets on a distributed computing platform. Users only have to deal with writing the functions of the algorithm in the given MapReduce-based programming model. MapReduce, on the other hand, controls the distributed cluster, manages data partitioning and data transfers between the various parts of the system, and provides fault tolerance."}, {"heading": "IV. COLUMN SELECTION-BASED SPARSE DECOMPOSITION", "text": "(CSSD)\nIn this section, we present a scalable method for matrix decomposition (the Decomposition phase in Figure 2) which we call Column Selection-Based Sparse Decomposition (CSSD)."}, {"heading": "A. Overview of CSSD method", "text": "The main idea behind CSSD is to first select a subset of columns from A, and then use this subset of columns as a basis from which we form sparse representations of the remaining columns. We thus factorize the data as A = DV, where D is formed by subsampling and normalizing the selected columns of A. Each column of V is then computed by finding the sparse approximation of the corresponding column of A with respect to D. This sparse approximation problem can be solved by an efficient greedy routine called orthogonal matching pursuit (OMP) [12]. We provide pseudocode for CSSD in Algorithm 1.\n1) Step 1. Sequential column selection: In order to ensure that the total approximation error in our factorization is sufficiently small, we must ensure that the columns selected from A to form D well approximate the range of the original matrix. Thus, we employ a sequential method to adaptively select columns that are not well approximated by the current set of columns [14].\nAdaptive column selection methods select a new batch of columns according to the following probability distribution:\np(i) \u221d \u2016ASA+S ai \u2212 ai\u20162\n\u2016ai\u20162 , (5)\nwhere p(i) equals the probability of selecting the ith column from A (denoted by ai), S contains the indices of columns already selected, and AS denotes the sub-matrix of sampled columns. We can flexibly execute this subsampling approach by either specifying the maximum number of columns to select and/or specifying the maximum amount of error in each unsampled column of A.\n5 Algorithm 1 : Column Selection Sparse Matrix Decomposition\nInput: Matrix A \u2208 Rm\u00d7n, error tolerance \u03b4D, number of columns to select at each iteration ls, and the maximum number of columns to select l. Output: A sparse matrix V \u2208 Rl\u00d7n and a dense matrix D \u2208 Rm\u00d7l such that for each column of a of A, \u2016a \u2212 Dv\u20162 \u2264 \u03b4D. Initialize: Initialize D by adding and normalizing ls columns from A with uniform random sampling.\nStep 1: Sequential column selection while ncols(D) < l do\nI. Update D by selecting and normalizing ls columns from A according to the distribution in (5). II. If the `2-norm of each column of E = A\u2212DD+A is less than \u03b4D, return D and proceed to Step 2 to compute V.\nend while\nStep 2. Sparse approximation I. Compute V by applying Batch OMP to solve (6)\nwith error tolerance \u03b4D.\n2) Step 2. Sparse approximation: After selecting a subset of l columns AS \u2208 Rm\u00d7l, we normalize each column such that all the columns in matrix D have unit norm. Now, to form the sparse matrix V, we find a sparse representation of the remaining columns in A (i.e., A\u2212S) in terms of the normalized dictionary D. The problem is formally written as follows:\nmin v \u2016v\u20160 s.t. \u2016ai \u2212Dv\u20162 \u2016ai\u20162 \u2264 \u03b4D, \u2200i /\u2208 S. (6)\nwhere \u2016v\u20160 counts the number of nonzero coefficients in v and \u03b4D is a user-specified parameter which controls the decomposition error.\nWe employ a matching pursuit-based solver called Batch OMP [43] to solve (6). We can enforce sparsity either by the number of non-zeros in each column of V (i.e., \u2016v\u20160) or by the total amount of approximation error for each column."}, {"heading": "B. Complexity analysis", "text": "The complexity of sequential column selection (Step 1) is O(l2m + lmn). The complexity terms correspond to computing D+ and DD+A respectively. The projection DD+a can be computed for each column of A independently. The complexity of sparse approximation (Step 2), using the Batch OMP method [43], is O(lmn + k2ln), where k < l is the average number of non-zeros per column of V. Similarly, for each column of A, Batch OMP is applied independently. Let nc be the number of parallel processing nodes. By storing D (which is a small m \u00d7 l matrix) and a uniform fraction of columns of A in each node (i.e., nnc columns), the overall complexity of Algorithm 1 in a distributed setting can be written as O( nnc (lm+ k 2l) + l2m).\nNote that CSSD is linear in terms of both the number of data samples n, and the number of processors nc. This is a key feature of our approach that makes our framework applicable to very large datasets in distributed settings."}, {"heading": "C. Computational benefits of CSSD", "text": "CSSD provides computational benefits when the size of the decomposition is small (i.e., l is small relative to m) and/or when matrix V is sparse. In general, predicting the amount of savings in computation is a function of (i) the structure of the data and (ii) the amount of accuracy required from the learning algorithm. We now discuss some key factors that impact the decomposition results.\nImpact of data structure. Predicting the size and sparsity of the decomposition provided by CSSD for an arbitrary dataset is challenging; however, when the data lies on a single subspace (i.e., exhibits low rank structure) or lies on multiple low-dimensional subspaces, CSSD provides a more compact representation of the data. For example, when data is exactly low rank and its rank is r < m, we must select r linearly independent columns from A to form an exact decomposition (zero error), i.e., l = r. When the data is approximately low rank, there exists a large body of work that characterizes the performance of the sequential column selection method (Step 1) used to form D [14], [24]. In particular, the selection strategy in Step 2 of Algorithm 1 provides exponential decrease in the factorization error with each batch of columns that we select from A [14]. More specifically, assume that at each iteration, we select ls > r samples from the columns of A and let l = tls denote the set of columns selected after t iterations. Let Ar denote the best rank r approximation to A and let A\u0303 = ASA+SA denote the approximation of A based upon the l selected columns As. Then according to [14], the difference between the expected value of the approximation error, i.e., \u2016A\u2212A\u0303\u20162F and that of the best rank r approximation \u2016A\u2212Ar\u20162F decreases exponentially with rate t.\nAnother low-dimensional signal model that has recently gained traction models data with multiple low-dimensional subspaces (union of subspaces). For example, images of objects under different illumination conditions [41], motion trajectories of point-correspondences [31], neural data [17], to structured sparse and block-sparse signals [5] are all well-approximated by a union of low-dimensional subspaces. When A lies on a union of subspaces, this effectively bounds the sparsity level of each column of V [18]. This insight is based upon the fact that when we form a representation of a column of A with respect to other columns in the same dataset (as in CSSD), the sparsity level of each column is bounded by the dimension of the subspace the signal lies on. For instance, if A lives on a union of multiple r-dimensional subspaces of Rn and we select at least r linearly independent columns from each subspace, then no more than r non-zeros are required to represent a signal. In other words, the number of non-zeros per column of V is no more the dimension of the subspace the signal lives in.\n6 Impact of increasing the decomposition error. The decomposition error of CSSD is controlled by the parameter \u03b4D in Algorithm 1. In the case where we set \u03b4D = 0, then we are guaranteed an exact decomposition of the data. Exact decomposition occurs when r linearly independent columns are selected from A, where r = rank(A). In this case, the selected columns in AS will fully represent the data and thus \u2016A\u2212ASA+SA\u2016 = 0, i.e., exact decomposition occurs.\nWhile CSSD can produce an exact decomposition when the data is exactly low rank (or lies on a union of subspaces), in practice, datasets are approximately low rank. In this case, we can introduce a small amount of error into the decomposition by setting \u03b4D > 0. By introducing some error into the decomposition, we observe that both the number of selected columns in Step 1 of Algorithm 1 and the sparsity level of V can be reduced further. In Figure 8, we show how increasing the decomposition error produces a more compact decomposition. In Section V, we study and discuss the impact of the decomposition error \u03b4D on the accuracy of a learning algorithms that we apply RankMap to."}, {"heading": "V. TUNING DECOMPOSITION ERROR FOR A TARGET LEARNING ACCURACY", "text": "In the previous section, we discussed the computational benefits associated with introducing some approximation error into a CSSD decomposition. Naturally, as we increase the decomposition error (controlled by \u03b4D), the accuracy of our learning algorithm will be affected. Thus, the key question is how much decomposition error we can afford to achieve a certain degree of accuracy in learning. The answer to this question heavily depends on the specific learning algorithm and the application of interest.\nPrevious theoretical studies have established a connection between the total error in a factorization of a kernel (or Gram) matrix and the accuracy of certain popular learning algorithms, including: kernel ridge regression and kernel SVM [10]. While for some learning algorithms, our framework can exploit previous work to relate \u03b4D and the learning error which we denote by \u03b4L, the aim of this section is to propose a generic approach for tuning the factorization error to achieve a specified learning accuracy. We do this by iteratively remapping of the data to find a compact decomposition that satisfies a learning error (specified by the user)."}, {"heading": "A. Error tuning", "text": "Given an already established relationship between the decomposition error and a specific algorithm, a practitioner who uses our framework can easily specify the error parameter \u03b4D for CSSD to achieve a particular learning accuracy. Alternatively, if a practitioner specifies a target accuracy for a learning algorithm, the decomposition error \u03b4D can be tuned in order to achieve a particular learning error \u03b4L.\nOur strategy for guaranteeing that we have small learning error, is to solve CSSD for a particular \u03b4D, map the resulting decomposition via the methods described in Section VI, and then compute the accuracy of the learning algorithm \u03b4L. We then iteratively add columns to D such that the decomposition\nerror \u03b4D is small enough to ensure that \u03b4L is within the errorspecified tolerance. Depending on the underlying computing resources available, RankMap can be applied for multiple values of \u03b4D in parallel and the largest value of \u03b4D (most compact representation) that achieves a particular value of \u03b4L is selected.\nWhen computing resources are constrained and thus running the algorithm for multiple values of \u03b4D in parallel is not possible, we use a bisection method. In essence, the idea is to: (i) set the factorization error to predefined maximum value \u03b4maxD (0.4 in our experiments) and evaluate \u03b4L, (ii) if \u03b4L is below a target value then we stop, otherwise we decrease \u03b4D by half. By exponentially decreasing \u03b4D, we are also guaranteed to decrease \u03b4L exponentially, provided that there is a polynomial relationship between the two quantities. We observe a polynomial relationship holds both in theory [10] and in practice. In Section VII, we provide empirical results which demonstrate the connection between the decomposition error and learning accuracy for numerous datasets and algorithms of interest (see Figures 7a and 8b)."}, {"heading": "VI. DISTRIBUTED EXECUTION AND DATA PARTITIONING", "text": "In this section, we introduce our approach for applying iterative updates on the decomposed data (the execution phase in Figure 2). We describe an execution flow for dependencymatrix based updates (i.e., Gx = VT (DTD)Vx) and introduce an efficient method for partitioning the decomposed data in a distributed setting. We also provide performance bounds on memory usage, the number of flop operations, and the number of communicated bytes across the computing nodes."}, {"heading": "A. Computation flow", "text": "We propose two computational models for the distributed implementation of an update in (1). Recall that at each iteration, we must compute z = Gx = VT (DTD)Vx. We break this computation into four steps: (i) p = Vx (ii) r = Dp, (iii) p = DT r, and (iv) z = VTp. The output vector z is used to produce an update of xiter+1 = f(z + b), where b is an offset vector, and f(\u00b7) is a low-complexity function such as a soft-thresholding operator (sparse approximation) or normalization (power method). To carry out the computation described above, we propose and implement a matrix-based and vertex-based model to apply the iterative updates on the decomposed factors. We now describe our implementation of both models."}, {"heading": "B. Matrix-based model", "text": "Figure 3 shows the schematic of our proposed matrixbased model. In this model, the data is stored in arrays. The sparse matrix V is stored and operated upon using the Compressed Sparse Column (CSC) format. The matrix D and vector x are stored using regular dense arrays. By doing so, we exploit sparsity in V. We use C++ Eigen Library for array manipulation and MPI for distributed computing.\n7\n1) Distributed partitioning: We partition columns of V uniformly across the computing nodes to achieve a balanced partitioning. Let us assume that there are nc computing nodes. Thus, nnc number of columns are assigned to each node. The vector x is also divided into chunks of size nnc \u00d7 1. Each chunk is then allocated to the node that hosts the corresponding columns of V.\nMatrix-vector multiplications Vx are performed locally on the columns of V and the portion of x that reside on the same computing node. The resulting l \u00d7 1 vectors are then sent to a central node to create p = Vx. Next, DT (Dp) is computed locally in the central node. The resulting l\u00d71 vector is then broadcasted back to all the computing nodes where it is multiplied by the local VT to update the vector x.\n2) Performance bounds: We now provide bounds on the memory usage, computation, and communication required by our proposed matrix-based model. We also provide the performance bounds for baseline, in which we perform iterative analysis on ATA in matrix format. Let nnz(.) denote the number of non-zeros of its input and nc denote the number of computing nodes. Recall that D is a m \u00d7 l matrix and V is a l \u00d7 n matrix. Note that in our target data scenarios m n and l n. In many cases, the rank of decomposition l is often much smaller than the dimensions of data m. When data exhibits union of subspace property, V will be sparse, i.e., nnz(V) < ln < mn.\nMatrix-based Baseline RankMap Memory usage \u221d:\n# non-zeros mn (nnz(V) + lm) + n+m Computation \u221d :\n# additions 2mn+mnc 2(nnz(V) + lm+ lnc) # multiplications 2mn 2(nnz(V) + lm)\nCommunication \u221d: #edges 2lm 2lnc\nSince V is stored in a CSC format, only the non-zero values are stored and operated on. The matrix D is stored in a regular dense matrix format. The communication corresponds to sending and receiving the l\u00d71 vectors from each computing node to the central node.Clearly, for smaller l and sparser V, both memory footprint and the number of arithmetic operations are reduced. The number of edges, which correspond to the number of broadcasted and reduced values, directly corresponds to l and the number of computing nodes nc."}, {"heading": "C. Graph-based model", "text": "Figure 4 shows an schematic of our proposed graph model. The decomposed data is three-layer graph denoted by GA(SX , SP , SR) with vertex sets SX = {Xi}ni=1 in the bottom layer, SP = {Pi}li=1 in the middle layer, and SR = {R1} in the top layer. Each non-zero element in V, e.g., Vij , is represented by an edge which connects Xi to Pj . Each column of D, e.g., Di, is represented by an edge which connects Pi to R1. Value of vertices in SX correspond to the elements of x.\nWe use GraphLab Distributed API [34] to implement this model. While GraphLab is a highly optimized distributed engine for Graph-based computation on iterative data, we perform extensive customizations in order to adapt GraphLab to our factorized setting. We also force GraphLab to use our developed graph partitioning method as opposed to its automated partitioning schemes. Our proposed partitioning is customized to the factorized data and significantly improves the performance.\n1) Distributed partitioning: In the graph-based model, we partition GA(SX , SP , SR) with the aim of balancing the number of components assigned to each node and also minimizing the inter-node communications characterized by the edges. Since the edge distribution of GA is highly non-uniform (l n), a vertex partitioning inevitably results in many undesirable edge-cuts across the computing nodes. Instead, we apply a vertex-cut method in which the goal is to partition graph edges evenly such that the number of vertices that are spanned across multiple partitions is minimized. As a result of edge partitioning, vertices may reside on two or more computing nodes. In this case, we assign one of the copies to be the master vertex and the others to be the replica vertices (these definitions are borrowed from GraphLab [25]). The replicas directly cause (expensive) inter-node communication costs.\nFigure 4 shows the graph-based distributed design. Our detailed edge partitioning method is as follows. (i) Distribute master of vertices Xi \u2208 SX uniformly onto the available computing nodes such that vertex chunks of size nnc are assigned to each node. (ii) Add the edges between vertices Xi \u2208 SX and Pj \u2208 SP to the node in which the corresponding master of Xi resides. (iii) Add master of vertices Pi \u2208 SP and R1 \u2208 SR to a central node. (iv) Add the edges between the vertices Pi \u2208 SP and R1 \u2208 SR to the central node.\nThe proposed edge partitioning algorithm is highly efficient in that it does not induce any replicas for vertices in SX\n8 and SR. However, from Step (ii), replicas of vertices in SP may exist in computing nodes other than the central node. At the beginning of an iteration, master vertices in SP and their replicas perform vertex updates with respect to SX . The replicas send the updated values to their own master vertices in the central node. The master vertices in SP reduce the received values (p = Vx). Then master vertex R1 performs a vertex update (r = Dp \u2212 y). Next master vertices in SP complete vertex updates with respect to SR and broadcast the results to their own replicas (p = DT r). Finally, master vertices in SX update themselves (x = VTp). We integrate and implement the proposed customized partitioning and distributed computation flow with the distributed GraphLab API [25].\n2) Performance bounds: We now provide bounds on the memory usage, computation, and communication required by our proposed graph-based model. \u2022 Memory usage\n# edges \u221d nnz(V) + l. # vertices \u221d n+ \u2211 1\u2264i\u2264l rep(Pi).\n\u2022 Computation (per iteration) # additions \u221d 2(nnz(V) +ml) + \u2211 1\u2264i\u2264l rep(Pi).\n# multiplications \u221d 2(nnz(V) +ml). \u2022 Communication\n# edge-cuts \u221d 2 \u2211\n1\u2264i\u2264l rep(Pi).\nGraph-based Baseline RankMap Memory usage \u221d:\n# edges mn nnz(V) + l # vertices n+mnc n+ \u2211 1\u2264i\u2264l rep(Pi)\nComputation \u221d : # additions 2mn+mnc 2(nnz(V) +ml) + \u2211 1\u2264i\u2264l rep(Pi)\n# multiplications 2mn 2(nnz(V) + lm) Communication \u221d:\n#cuts 2lm 2 \u2211\n1\u2264i\u2264l rep(Pi)\nEach of the computing nodes receive approximately 1nc (n+\u2211 1\u2264i\u2264l rep(Pi)) vertices and 1 nc nnz(V) edges. The central node has l additional edges between the master vertices in SP and R1. The computation cost is induced by vertex update operations. The communication overhead is incurred by the message passing across master and replica vertices in SP .\nBound on total replicas. From the discussions above, it is clear that reducing number of replicas of SP reduces the communication overhead. The following are the bounds on the total number of replicas:\nl \u2264 \u2211\n1\u2264i\u2264l\nrep(Pi) \u2264 lnc.\nThe inequalities hold since each Pi is replicated at least once and at most nc times (one replica per computing node). Both l and nc are much smaller than the size of the graph. Thus, RankMap\u2019s graph-based model readily provides efficient/balanced computation and reduced communication without using complicated and costly graph partitioning algorithms. The minimum communication is achieved when V is block-diagonal."}, {"heading": "VII. EVALUATIONS", "text": "In this section, we evaluate the performance of RankMap on a variety of datasets. Our evaluations explore: (i) the\nscalability of CSSD and its ability to produce sparse representations, (ii) the connection between decomposition error and learning accuracy for multiple learning applications including face recognition, image denoising, and PCA, (iii) RankMap\u2019s performance improvement in terms of runtime, and memory over prior work, and (iv) the performance of our distributed matrix- and graph-based models for different structured data sets."}, {"heading": "A. Evaluation setup", "text": "1) Datasets: We evaluate RankMap on both real and structured synthetic datasets. Our real datasets include Light Field data [2], hyper spectral images [1], a dictionary of video frames [28], and a collection of images of different faces under varying illumination conditions [22].\nWe apply RankMap to two different Light Field datasets. The first dataset, which we refer to as Light Field (i), consists of 10k randomly selected atoms from a 5\u00d75 Light Field array (collected from Chess Images). Each Light Field patch consists of 25 8\u00d78-patches which produces a dataset of size 1.6k\u00d710k (128MB). The second dataset, which we refer to as Light Field (ii), consists of 100k randomly selected atoms from a 17 \u00d7 17 Light Field array (collected from all available Light Fields in the archive). Each Light Field patch consists of 289 8\u00d7 8-patches which produces a dataset of size 18496\u00d7 100k (14.7GB). The hyper spectral dataset (Salinas) is taken from a region of a remote sensing scene in Salinas, CA. Each pixel in the scene contains information from 203 spectral bands and produces a dataset of size 203\u00d7 54129 (87.9MB). The video dictionary dataset (VideoDict) contains patches of an image over multiple frames and produces a dataset of size 1764 \u00d7 100000 (1.41GB). The face image dataset (Faces) consists of 631 images of 10 different peoples faces under varying illumination conditions. Each image is 48\u00d7 84 pixels, which produces a dataset of size 4032\u00d7631. In addition to real-world datasets, we generate synthetic data for n = 10M , m = 1k with varying l and sparsity levels in V.\n2) Computing platform: To evaluate the decomposition methods on Light Field (i) an 8-core CPU (Intel CoreTMi7 processor) with 12GB of RAM is used. For computations on Light Field dataset (ii), we instanciated a cluster of 16 m3.large nodes (machines) on Amazon EC2. Each node has 16 cores (two Intel Xeon processors) at 7.5GB of RAM per node. The synthetic datasets are evaluated on IBM iDataPlex computing cluster which has 2304 cores in 192 Westmere nodes (12 processor cores per node) at 48GB of RAM per node.\n3) Distributed tools: All RankMap\u2019s APIs are available to the public [3]. The RankMap framework\u2019s sparse matrix-based model is implemented using Eigen library to represent data in a compressed column storage (CCS) format [27]. It uses MPI\u2019s standard system to distribute the data and computation and is written in C++. We have also implemented the distributed update on the factorized data on Apache Spark [49].\nThe RankMap framework\u2019s sparse graph-based design is implemented using GraphLab, a high-level graph-parallel abstraction [25]. GraphLab enables vertex-update-based computations. We implemented RankMap\u2019s customized partitioning\n9 using Graphlab\u2019s ingress class. The proposed architectures are mapped efficiently into GraphLab API (Section VI-C). Note that the GraphLab framework is designed to accelerate distributed learning for sparse graphs and thus it is not suited to process dense data until we sparsify the data using CSSD."}, {"heading": "B. Scaling of CSSD", "text": "Figure 5 shows how the runtime of CSSD scales as the number of processors increases for the VideoDict dataset. We increase the number of cores from 4 to 256 (on IBM iDataPlex cluster). The dotted line shows the ideal scale-out behavior. As can be seen, CSSD is highly parallel as it almost linearly scales with the number of processors. Thus, it can be applied to very large datasets."}, {"heading": "C. Sparse approximation", "text": "To evaluate the performance of RankMap for sparse approximation, we use the fast iterative shrinkage-thresholding algorithm (FISTA) [6] to solve the `1-minimization problem in (2). We study the utility of RankMap for two applications: sparse representation-based classification for face recognition and image denoising (see Section II-B for more details on these applications).\n1) Sparse representation-based classification for face recognition: To employ sparse approximation for classification, our aim is to use a collection of labeled images (training set) as our dictionary A and then form a sparse representation of a test image y in terms of A. After finding a sparse coefficient vector x, we can then determine which signals in the testing set (columns of A) are selected to represent the test signal y. Based upon the class of the selected columns, we then make a decision about which class the test signal lies in. One easy way to do this is to simply sum the absolute value of the coefficients in x in a certain class and then find the class with maximum sum.\nIn Figure 6, we provide a demonstration of sparse representation-based classification for face recognition. We show the test image of interest on top and the corresponding sparse coefficient vector obtained by solving (2) with FISTA, where \u03bb = 1. We solve FISTA with the full Gram matrix ATA and approximate Gram provided by CSSD, where the decomposition error \u03b4D = 0.05 (l = 62).\nTo understand the connection between the decomposition error and learning accuracy for face recognition, we solve\nBaseline (A) CSSD\n100 200 300 400 500 600\nTest Image\nSp ar\nse C\noe ffi\nci en\nts\n0\n0.005\n0.01\n0.015\n0.02\n0.025\nImage Number\nFig. 6: Sparse representation-based face recognition. We show the sparse coefficients obtained with the original Gram matrix (blue, solid) and approximate Gram matrix with CSSD for \u03b4D = 0.05 (red, dash). On top, we show the test image, and four training images that produce significant non-zero coefficients. Both the baseline and our approach result in correct classification, as their largest coefficient is associated with a training example from the same class as the test image. The block of coefficients corresponding to images in the correct class is highlighted.\n(2) using FISTA for two different regularization parameters \u03bb = {0, 1}, where \u03bb = 0 corresponds to the least-squares solution and \u03bb = 1 produces sparse solutions. We vary the decomposition error \u03b4D = {0.4, 0.2, 0.1, 0.05} and solve FISTA for 30 different test images (after removing them from training) for each of these decompositions. We calculate the: learning accuracy by measuring the `2-norm between the solution obtained with the full and approximate Gram (Figure 7a), the sum of coefficients in the correct class (Figure 7b), and the relative density of V (the number of non-zeros in V versus the number of non-zeros in A) (Figure 7c). In Figure 7b, we also display the minimum sum of coefficients required to correctly classify the test image. In this case, we obtain the correct class with this approach when \u03b4D < 0.2. These results suggest that even when we allow a significant amount of decomposition error, correct classification is still possible.\n2) Light Field image denoising: We evaluate RankMap\u2019s performance in denoising Light Field data. A Light Field is a multi-dimensional array of images where each image is captured from a slightly different viewpoint. To reconstruct and denoise light fields, `1-minimization (2) is employed to find a sparse representation of a Light Field image with respect to an overcomplete Light Field dictionary consisting of a large number of Light Field image patches collected from many scenes [36]. This dictionary can be used to reconstruct light field patches for the purpose of super-resolution and denoising.\n10\nWe study the performance of RankMap for reconstructing light field patches from noisy observations (image denoising). In all of our denoising experiments, Light Field (ii) is used. We first apply CSSD for decomposing the dictionary corresponding to the Light Field (ii) dataset, for two different values of l = 240 and l = 1000. The decomposition error \u03b4D is set to 0.1. After decomposing the data, we then evaluate FISTA on the decomposed data with the matrix-based model. We compare RankMap\u2019s performance with that of a tailored distributed MPI-based model to evaluate FISTA on the original dataset (A) using regular dense matrix representations. This implementation is denoted as the baseline in our evaluations.\nTable I shows the total runtime of FISTA to achieve different PSNRs. The Peak Signal to Noise Ratio (PSNR) is the ratio between the maximum possible power of a signal and the power of the corrupting noise, is used to measure the performance of denoising algorithms. The PSNR is defined as 10 log10( MAX\u221a MSE\n) (dB), where MAX is the maximum pixel value of the original image patch and MSE is the mean square reconstruction error defined as \u2016y\u2212y\u0302\u2016 2\nm . Typically in image noise reduction applications, PSNR values of 30 dB and higher are desired [45], [7], [4].\nIn all the experiments, a batch of ten noisy patches are used as the input and the norm of the noise is set to 0.3 times the norm of the input vector (PSNR=21.14). We observe that RankMap can achieve the same PSNR orders of magnitude faster than the baseline implementation. For instance, if our desired PSNR is 30.0dB, running FISTA on the decomposed data takes 13.9s (l = 240) and 162ss (l = 1000), while it takes 1050s for the baseline. However, as expected, the baseline (A) reaches higher PSNRs in comparison with those achieved from running FISTA on the decomposed data. Thus RankMap can be used to tradeoff learning accuracy for speed."}, {"heading": "D. Power method", "text": "We also evaluate our framework on power method for three datasets: Salinas, VideoDict, and Light Field (i) (see Section II-B for more discussion of the power method). The matrixbased model is used and the experiments are run on 64 cores on an IBMiDataplex cluster. We run CSSD with various decomposition errors (\u03b4D) that belong to the following set:\n{0.4, 0.2, 0.1, 0.05, 0.001} and run the power method on each of the decomposition results. Figure 8a shows the sparsity of V as we vary the error. As expected, for larger error tolerances, a sparser decomposition is achieved. Figure 8b shows the impact of the decomposition error on the accuracy of the results of the power method. Here, the learning error (\u03b4L) is defined to be the normalized accumulated error of the first 100 eigenvalues. By lowering the decomposition error, we can observe significant improvements in the accuracy of the power method. Finally, Figure 8c shows the corresponding normalized runtimes to find the first 100 eigenvalues. Our results suggest that significant speedups are achieved in comparison with the baseline."}, {"heading": "E. Graph- vs. matrix-based models", "text": "We compare the performance of RankMap\u2019s vertex and matrix-based models for various synthetic decomposed data. The purpose of these evaluations is to determine the advantages of each of the model, with respect to the structure of the data. In all the experiments, the iterative update in (3) is applied on a random input vector y. The experiments are done on an IBM iDataPlex computing cluster. In all the figures, the runtime results for the dense matrix-based implementation (i.e., regular deployment of the decomposed matrices without using CCS format) are provided to demonstrate the efficiency achieved by exploiting sparsity in V through sparse matrixbased and graph-based models. For the former model, we report analysis based on our C++ MPI implementation and for\n11\nthe latter model we report analysis based on our modification of GraphLab engine to implement RankMap.\nThe performance of RankMap for different (block-diagonal) V\u2019s, with fixed number of non-zeros (set to 100M ), is shown in Figure 9a. As l increases, the density-level of V decreases. The graph-based model\u2019s performance is more consistent as l increases. However, the matrix-based model\u2019s performance degrades for larger l\u2019s. This observation can also be explained due to the fact that the communication overhead of the matrixbased model is more affected by larger l\u2019s.\nFigure 9b shows the performance for a fixed l = 500 on block-diagonal matrices V, for varying densities of V. As density increases, the performance decreases in both models. However, the performance degradation in graph-based model is worse due to the overhead of representing a large number of edges. Lastly, Figure 9c shows the scaling performance of the models for various number of processors. When the number of processors is less than 12, the computations are done on a single node. Thus the reverse scaling behavior while increasing the number of processors from 8 to 16 is due to the high overhead of the inter-node communication cost. For comparison purposes, we report the scaling of the baseline approach that operates on the original dense m = 1k by n = 10M dataset, instead of its decomposed form. It can be seen that as the number of processors increases, the performance gap between different methods shrink. However, even with a large number of processors (\u2265 100), the decomposed models perform up to 2 orders of magnitude better than the baseline.\nThese experiments provide insight into the use-case of each model. Depending on the structure of the decomposed data and the specifications of the platform, an appropriate model should be selected. A more systematic domain-specific approach for model selection is the subject of future work."}, {"heading": "F. Memory Analysis", "text": "Table II compares the required memory for storing matrices V and D. The memory usage of the original matrix A is also provided. We also provided the memory savings for the case in which D is formed in the same fashion as CSSD but V is computed via least-squares, as opposed to OMP.\nRankMap results in up to 77.8\u00d7 (memory usage) improvement over ATA and 8.6\u00d7 improvement over the adaptive norm-2 projection based decomposition. The approximation error for both decomposition methods is set to \u03b4D = 0.1."}, {"heading": "G. Comparison with Spark", "text": "We have already shown the results based on our implementation of RankMap on GraphLab in Figures 9a, 9b, and 9c. Now we provide runtime comparisons between RankMap and a Apache Spark-based implementation of the power method on the baseline A versus on the decomposed data DV. Recall that the core iterative update function used in power method is provided in (4). We report the average runtime per iteration with Spark and our implementations on the same hardware.\nFigure 10 shows the average runtime per iteration on a cluster of 8 nodes, 8 core per processor for Salinas, VideoDict, and Light Field (ii) datasets. As expected, our carefully tailored implementation of RankMap based on C++ MPI, performed significantly better than Spark, by more than 2 orders of magnitude in some cases. This gap in performance is in part due to our particular implementation of RankMap, which carefully partitions the data such that the computation per core is balanced and the communication is reduced (see Section 6.3.2 for performance bounds). In addition, Spark provides fault-tolerance which causes extra overhead due to data replications."}, {"heading": "VIII. DISCUSSION", "text": "This paper introduced RankMap, a novel distributed framework for applying a host of iterative learning algorithms on large-scale dense and structured datasets. Our framework leverages low-dimensional structure in datasets in order to\n12\nquickly map a large dataset with dense dependencies into lower dimensional components with sparse representations. We introduce two computational models, a matrix- and graphbased model, that can be used to execute distributed learning algorithms. Our framework provides an efficient partitioning of the computational flow that guarantees load balancing and significantly lowers communication overhead. We apply our matrix- and graph-based models to numerous real-world and synthetic datasets and demonstrate significant improvements in the runtime and memory footprint.\nThere is an unavoidable cost associated with factorizing the data. For extremely large datasets however, this initial cost can pay off a lot. The performance gain achieved on the subsequent computations justifies the one-time decomposition overhead. For example, we decompose Light Field (ii) dataset (Section VII) on a cluster of 4 nodes (each with 12 cores) on IBM iDataPlex. For l = 240, the decomposition is completed in less than 15 minutes. For 10 sample patches (each of length 18k) the overall reconstruction time is reduced from more than 1000s to below 20s. Thus, the offline decomposition overhead can be justified once considering that there are thousands of patches in a single light field. Moreover, the same dictionary can be used to reconstruct other light field datasets.\nThere are a number of existing column sampling-based methods that aim to improve the performance of specific learning objectives, such as least-squares [37], `2-minimization with square root `1 penalty [40], and SVM [20]. RankMap is unique in that uses column sampling to improve the performance of a broad class of ML algorithms that operates on the Gram matrix. Moreover, RankMap relies of the sparsity of the decomposition for further improvement in runtime, energy and memory usage. Finally, to the best of our knowledge RankMap\nis the first end-to-end framework that is equipped with opensource supported APIs [3].\nSparse matrix factorization approaches such as SPCA and KSVD have objectives similar to CSSD, however, their complexity make them difficult to apply to massive datasets. As we sample the dataset instead of learning a factorization of the data, our proposed decomposition is faster and scalable. Whilst our sampling-based approach is effective, the decomposition phase in RankMap (see Figure 2) can be readily replaced by other sparse decomposition approaches. Tradeoffs between the time to compute a factorization (via learning or sampling) and how sparse we can make the decomposition are likely to exist. Although outside of the scope of this current work, it would be interesting to study the utility of learning in terms of its later computational benefit.\nOur graph-based and matrix-based computational models provide advantages in different data regimes. Thus, it is natural to ask which model to select for data processing. Both models follow the same computational flow and operate only on the non-zeros. In practice, we observe that the matrix-based approach is faster: this is especially true when we exploit sparsity in the decomposition with a sparse matrix-based approach. This is likely due to the fact that the graph-based model requires extra overhead to store and operate on the vertices and edges. The main advantage of the graph-based model is in its reduced communication cost (Section VI-C2). When V is completely block diagonal the communication becomes almost independent of the number of computing nodes nc and is only proportional to 2l. In contrast, the communication cost in the matrix-based model is always proportional to 2lnc. This difference may result in a better overall performance of the graph-based approach, especially for larger l and nc values. In general, the performance of each model is highly dependent on the specifications of the available computing nodes including the communication bandwidth and computation power (e.g., maximum floating point operations per second). Our evaluations in Section VII-E provide further insight into the differences of the two models.\nThroughout our experiments, we used FISTA, an accelerated gradient descent method, as an optimizer. Our computation/communication and memory minimizing framework can also be applied to other optimization methods such as Stochastic Gradient Descent (SGD) [42] and Stochastic Coordinate Descent (SCD) [33]. Both SGD and SCD operate on the entire\n13\nm\u00d7n dataset, however, each iterative update is performed on a subset of rows (as in SGD) or along the columns (in SCD). For this reason, the convergence of stochastic method is slower. SGD can be integrated within RankMap by breaking the m rows of matrix A into batches, and performing RankMap\u2019s decomposition on each batch. SCD can also be applied to the factorized dataset DV instead of A. In general, RankMap is not limited to a particular optimizer, it is beneficial whenever there is a need to store/ and or iteratively perform matrix multiplication on large datasets.\nIn this work, we show how sparse matrix factorization and adaptive sampling can be used to speed up iterative optimization algorithms on large datasets. We have mainly explored its use for computational gains, however, recent theoretical results have shown that subsampling data can also be beneficial for learning [44]. RankMap provides a new computational framework from which we can begin to test the ideas of efficiency, both in terms of quality of learning and computing performance, through randomization and subsampling."}], "references": [{"title": "SVD: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M Aharon", "M Elad", "A Bruckstein"], "venue": "IEEE Trans Sig. Process., 54(11):4311\u20134322", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Model-based compressive sensing", "author": ["R G Baraniuk", "V Cevher", "M F Duarte", "C Hegde"], "venue": "IEEE Trans. Inf. Theory, 56(4):1982\u20132001", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A Beck", "M Teboulle"], "venue": "SIIMS, 2(1):183\u2013202", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Enhancing sparsity by reweighted `1 minimization", "author": ["E J Candes", "M B Wakin", "S P Boyd"], "venue": "JFAA, 14(5-6):877\u2013905", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "On a class of preconditioning methods for dense linear systems from boundary elements", "author": ["K Chen"], "venue": "SISC, 20(2):684\u2013698", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Atomic decomposition by basis pursuit", "author": ["S S Chen", "D L Donoho", "M A Saunders"], "venue": "SISC, 20(1):33\u201361", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "On the impact of kernel approximation on learning accuracy", "author": ["C Cortes", "M Mohri", "A Talwalkar"], "venue": "TAISTATS, pages 113\u2013120", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "author": ["I Daubechies", "M Defrise", "C De Mol"], "venue": "Comm. Pure Appl. Math., pages 1413\u20131457", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Adaptive time-frequency decompositions", "author": ["G M Davis", "S G Mallat", "Z Zhang"], "venue": "OE, 33(7):2183\u20132191", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1994}, {"title": "MapReduce: simplified data processing on large clusters", "author": ["J Dean", "S Ghemawat"], "venue": "CACM, 51(1):107\u2013113", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Matrix approximation and projective clustering via volume sampling", "author": ["A Deshpande", "L Rademacher", "S Vempala", "G Wang"], "venue": "SODA, pages 1117\u20131126. SIAM", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Clustering large graphs via the singular value decomposition", "author": ["P Drineas", "A Frieze", "R Kannan", "S Vempala", "V Vinay"], "venue": "Machine learning, 56(1-3):9\u201333", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "On the Nystr\u00f6m method for approximating a gram matrix for improved kernel-based learning", "author": ["P Drineas", "M W Mahoney"], "venue": "JMLR, 6:2153\u2013 2175", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Self-expressive decompositions for matrix approximation and clustering", "author": ["E L Dyer", "T A Goldstein", "R Patel", "K P Kording", "R G Baraniuk"], "venue": "arXiv:1505.00824", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Greedy feature selection for subspace clustering", "author": ["E L Dyer", "A C Sankaranarayanan", "R G Baraniuk"], "venue": "JMLR, 14(1):2487\u20132517", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse subspace clustering: Algorithm", "author": ["E Elhamifar", "R Vidal"], "venue": "theory, and applications. TPAMI, 35(11):2765\u20132781", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient SVM training using low-rank kernel representations", "author": ["S Fine", "K Scheinberg"], "venue": "JMLR, 2(Dec):243\u2013264", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Spectral grouping using the Nystr\u00f6m method", "author": ["C Fowlkes", "S Belongie", "F Chung", "J Malik"], "venue": "TPAMI, 26(2):214\u2013225", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A S Georghiades", "P N Belhumeur", "D J Kriegman"], "venue": "TPAMI, 23(6):643\u2013660", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2001}, {"title": "SystemML: Declarative machine learning on MapReduce", "author": ["A Ghoting", "R Krishnamurthy", "E Pednault", "B Reinwald", "V Sindhwani", "S Tatikonda", "Y Tian", "S Vaithyanathan"], "venue": "ICDE, pages 231\u2013242. IEEE", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Revisiting the nystrom method for improved large-scale machine learning", "author": ["A Gittens", "M W Mahoney"], "venue": "ICML, pages 567\u2013575", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "PowerGraph: Distributed graph-parallel computation on natural graphs", "author": ["J E Gonzalez", "Y Low", "H Gu", "D Bickson", "C Guestrin"], "venue": "OSDI, pages 17\u201330", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "N-Body\u2019 problems in statistical learning", "author": ["A G Gray", "A W Moore"], "venue": "NIPS, pages 521\u2013527. MIT Press", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2001}, {"title": "Video from a single coded exposure photograph using a learned over-complete dictionary", "author": ["Y Hitomi", "J Gu", "M Gupta", "T Mitsunaga", "S Nayar"], "venue": "ICCV, pages 287\u2013294. IEEE", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Ridge regression: Biased estimation for nonorthogonal problems", "author": ["A E Hoerl", "R W Kennard"], "venue": "Technometrics, 12(1):55\u201367", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1970}, {"title": "Generalized power method for sparse principal component analysis", "author": ["M Journ\u00e9e", "Y Nesterov", "P Richt\u00e1rik", "R Sepulchre"], "venue": "JMLR, 11(Feb):517\u2013553", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Motion segmentation by subspace separation and model selection", "author": ["K Kanatani"], "venue": "ICCV, volume 2, pages 586\u2013591", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "Projected gradient methods for nonnegative matrix factorization", "author": ["C Lin"], "venue": "Neural Comput., 19(10):2756\u20132779", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}, {"title": "An asynchronous parallel stochastic coordinate descent algorithm", "author": ["J Liu", "S J Wright", "C R\u00e9", "V Bittorf", "S Sridhar"], "venue": "JMLR, pages 285\u2013322", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "GraphLab: A new parallel framework for machine learning", "author": ["Y Low", "J E Gonzalez", "A Kyrola", "D Bickson", "C E Guestrin", "J Hellerstein"], "venue": "UAI, pages 340\u2013349", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Pregel: a system for large-scale graph processing", "author": ["G Malewicz", "M H Austern", "A JC Bik", "J C Dehnert", "I Horn", "N Leiser", "G Czajkowski"], "venue": "SIGMOD, pages 135\u2013146. ACM", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Compressive light field photography using overcomplete dictionaries and optimized projections", "author": ["K Marwah", "G Wetzstein", "Y Bando", "R Raskar"], "venue": "TOG, 32(4):46", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "LSRN: A parallel iterative solver for strongly over-or underdetermined systems", "author": ["X Meng", "M A Saunders", "M W Mahoney"], "venue": "SISC, 36(2):95\u2013 118", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Introduction to linear regression analysis", "author": ["D C Montgomery", "E A Peck", "G G Vining"], "venue": "John Wiley & Sons", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Expectation maximization and complex duration distributions for continuous time bayesian networks", "author": ["U Nodelman", "C R Shelton", "D Koller"], "venue": "arXiv preprint arXiv:1207.1402", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust sketching for multiple square-root LASSO problems", "author": ["V Pham", "L El Ghaoui"], "venue": "AISTATS", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Analytic PCA construction for theoretical analysis of lighting variability in images of a lambertian object", "author": ["R Ramamoorthi"], "venue": "TPAMI, 24(10):1322\u20131333", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2002}, {"title": "HOGWILD: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B Recht", "C Re", "S Wright", "F Niu"], "venue": "NIPS, pages 693\u2013701", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient implementation of the K-SVD algorithm using batch orthogonal matching pursuit", "author": ["R Rubinstein", "M Zibulevsky", "M Elad"], "venue": "Technion, Tech. Report, 40(8):1\u201315", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "Less is more: Nystr\u00f6m computational regularization", "author": ["A Rudi", "R Camoriano", "L Rosasco"], "venue": "NIPS, pages 1657\u20131665", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "The curvelet transform for image denoising", "author": ["J Starck", "E J Cand\u00e8s", "D L Donoho"], "venue": "IEEE Trans Image Processing, 11(6):670\u2013684", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2002}, {"title": "Principal submatrices IX: Interlacing inequalities for singular values of submatrices", "author": ["R C Thompson"], "venue": "Linear Algebra and its Applications, 5(1):1\u201312", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1972}, {"title": "Sparse representation for computer vision and pattern recognition", "author": ["J Wright", "Y Ma", "J Mairal", "G Sapiro", "T S Huang", "S Yan"], "venue": "Proceedings of the IEEE, 98(6):1031\u20131044", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}, {"title": "et al", "author": ["J S Yedidia", "W T Freeman", "Y Weiss"], "venue": "Generalized belief propagation. NIPS, pages 689\u2013695", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2000}, {"title": "Resilient distributed datasets: A faulttolerant abstraction for in-memory cluster computing", "author": ["M Zaharia", "M Chowdhury", "T Das", "A Dave", "J Ma", "M McCauley", "M J Franklin", "S Shenker", "I Stoica"], "venue": "NSDI, pages 2\u20132. USENIX", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}, {"title": "Spark: Cluster computing with working sets", "author": ["M Zaharia", "M Chowdhury", "M J Franklin", "S Shenker", "I Stoica"], "venue": "HotCloud, pages 10\u201310. USENIX", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse principal component analysis", "author": ["H Zou", "T Hastie", "R Tibshirani"], "venue": "J. Comp. Graph. Stat., 15(2):265\u2013286", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 24, "context": "Some prominent examples of such algorithms and their applications are linear or penalized regression [29], power iterations [30], belief propagation [48], and expectation maximization [38], [39].", "startOffset": 101, "endOffset": 105}, {"referenceID": 25, "context": "Some prominent examples of such algorithms and their applications are linear or penalized regression [29], power iterations [30], belief propagation [48], and expectation maximization [38], [39].", "startOffset": 124, "endOffset": 128}, {"referenceID": 43, "context": "Some prominent examples of such algorithms and their applications are linear or penalized regression [29], power iterations [30], belief propagation [48], and expectation maximization [38], [39].", "startOffset": 149, "endOffset": 153}, {"referenceID": 33, "context": "Some prominent examples of such algorithms and their applications are linear or penalized regression [29], power iterations [30], belief propagation [48], and expectation maximization [38], [39].", "startOffset": 184, "endOffset": 188}, {"referenceID": 34, "context": "Some prominent examples of such algorithms and their applications are linear or penalized regression [29], power iterations [30], belief propagation [48], and expectation maximization [38], [39].", "startOffset": 190, "endOffset": 194}, {"referenceID": 30, "context": ", Pregel [35], Spark [50], and GraphLab [34].", "startOffset": 9, "endOffset": 13}, {"referenceID": 45, "context": ", Pregel [35], Spark [50], and GraphLab [34].", "startOffset": 21, "endOffset": 25}, {"referenceID": 29, "context": ", Pregel [35], Spark [50], and GraphLab [34].", "startOffset": 40, "endOffset": 44}, {"referenceID": 21, "context": "on each vertex [25].", "startOffset": 15, "endOffset": 19}, {"referenceID": 4, "context": "Data with dense dependencies appear in a wide range of fields such as computer vision, medical image processing, boundary element methods and their applications, and N-body problems [8], [26].", "startOffset": 182, "endOffset": 185}, {"referenceID": 22, "context": "Data with dense dependencies appear in a wide range of fields such as computer vision, medical image processing, boundary element methods and their applications, and N-body problems [8], [26].", "startOffset": 187, "endOffset": 191}, {"referenceID": 5, "context": "as the LASSO or BPDN [9], and ridge regression [29], are typically solved using iterative updates.", "startOffset": 21, "endOffset": 24}, {"referenceID": 24, "context": "as the LASSO or BPDN [9], and ridge regression [29], are typically solved using iterative updates.", "startOffset": 47, "endOffset": 51}, {"referenceID": 5, "context": "Sparse representation is used in a wide range of signal processing and machine learning applications, including denoising [9], classification [47], clustering [18], and outlier detection [17].", "startOffset": 122, "endOffset": 125}, {"referenceID": 42, "context": "Sparse representation is used in a wide range of signal processing and machine learning applications, including denoising [9], classification [47], clustering [18], and outlier detection [17].", "startOffset": 142, "endOffset": 146}, {"referenceID": 14, "context": "Sparse representation is used in a wide range of signal processing and machine learning applications, including denoising [9], classification [47], clustering [18], and outlier detection [17].", "startOffset": 159, "endOffset": 163}, {"referenceID": 13, "context": "Sparse representation is used in a wide range of signal processing and machine learning applications, including denoising [9], classification [47], clustering [18], and outlier detection [17].", "startOffset": 187, "endOffset": 191}, {"referenceID": 7, "context": "This sparse approximation problem can be solved using the following iterative soft thresholding (IST) algorithm [11]:", "startOffset": 112, "endOffset": 116}, {"referenceID": 7, "context": ", a soft-thresholding operator [11]) to account for the term \u03bb\u2016x\u20161 at each iteration, and \u03b3 is the step size.", "startOffset": 31, "endOffset": 35}, {"referenceID": 2, "context": "In our evaluations, we employ a variant of this algorithm called FISTA [6].", "startOffset": 71, "endOffset": 74}, {"referenceID": 27, "context": "is an example of a projected gradient descent (PGD) methods [32] which provide a generalization of standard gradient descent methods for certain classes of non-smooth objective functions.", "startOffset": 60, "endOffset": 64}, {"referenceID": 13, "context": "Extracting low dimensional structures not only reduces dimensionality, but also mitigates the effect of noise and improves the performance of learning and inference tasks [17], [19].", "startOffset": 171, "endOffset": 175}, {"referenceID": 15, "context": "Extracting low dimensional structures not only reduces dimensionality, but also mitigates the effect of noise and improves the performance of learning and inference tasks [17], [19].", "startOffset": 177, "endOffset": 181}, {"referenceID": 41, "context": "The truncated SVD also provides the solution to principal components analysis (PCA), which seeks to find a k-dimensional subspace that best approximates A in the least-squares sense [46].", "startOffset": 182, "endOffset": 186}, {"referenceID": 46, "context": "Two widely used sparse factorization methods include sparse PCA (SPCA) [51] and dictionary learning (DL) [4].", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "Two widely used sparse factorization methods include sparse PCA (SPCA) [51] and dictionary learning (DL) [4].", "startOffset": 105, "endOffset": 108}, {"referenceID": 25, "context": "To solve SPCA on big datasets, a generalized power method can be employed [30].", "startOffset": 74, "endOffset": 78}, {"referenceID": 12, "context": "CSS-based solutions form an approximate matrix decomposition in which one factorized component is a subset of the columns of the data itself [16].", "startOffset": 141, "endOffset": 145}, {"referenceID": 11, "context": "[15], Gramian matrix decomposition [16], image denoising and clustering [17], and also in spectral clustering [21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15], Gramian matrix decomposition [16], image denoising and clustering [17], and also in spectral clustering [21].", "startOffset": 35, "endOffset": 39}, {"referenceID": 13, "context": "[15], Gramian matrix decomposition [16], image denoising and clustering [17], and also in spectral clustering [21].", "startOffset": 72, "endOffset": 76}, {"referenceID": 17, "context": "[15], Gramian matrix decomposition [16], image denoising and clustering [17], and also in spectral clustering [21].", "startOffset": 110, "endOffset": 114}, {"referenceID": 9, "context": "Examples include MapReduce [13], Apache Spark [49], and SystemML [23].", "startOffset": 27, "endOffset": 31}, {"referenceID": 44, "context": "Examples include MapReduce [13], Apache Spark [49], and SystemML [23].", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "Examples include MapReduce [13], Apache Spark [49], and SystemML [23].", "startOffset": 65, "endOffset": 69}, {"referenceID": 30, "context": "distributed abstractions have been proposed that model data dependency in a graph format, most notably Pregel [35] and GraphLab [34].", "startOffset": 110, "endOffset": 114}, {"referenceID": 29, "context": "distributed abstractions have been proposed that model data dependency in a graph format, most notably Pregel [35] and GraphLab [34].", "startOffset": 128, "endOffset": 132}, {"referenceID": 21, "context": "Because of this, most of these tools are designed based on the assumption that the input data is sparse [25], [35], [49].", "startOffset": 104, "endOffset": 108}, {"referenceID": 30, "context": "Because of this, most of these tools are designed based on the assumption that the input data is sparse [25], [35], [49].", "startOffset": 110, "endOffset": 114}, {"referenceID": 44, "context": "Because of this, most of these tools are designed based on the assumption that the input data is sparse [25], [35], [49].", "startOffset": 116, "endOffset": 120}, {"referenceID": 8, "context": "This sparse approximation problem can be solved by an efficient greedy routine called orthogonal matching pursuit (OMP) [12].", "startOffset": 120, "endOffset": 124}, {"referenceID": 10, "context": "Thus, we employ a sequential method to adaptively select columns that are not well approximated by the current set of columns [14].", "startOffset": 126, "endOffset": 130}, {"referenceID": 38, "context": "We employ a matching pursuit-based solver called Batch OMP [43] to solve (6).", "startOffset": 59, "endOffset": 63}, {"referenceID": 38, "context": "complexity of sparse approximation (Step 2), using the Batch OMP method [43], is O(lmn + kln), where k < l is the average number of non-zeros per column of V.", "startOffset": 72, "endOffset": 76}, {"referenceID": 10, "context": "When the data is approximately low rank, there exists a large body of work that characterizes the performance of the sequential column selection method (Step 1) used to form D [14], [24].", "startOffset": 176, "endOffset": 180}, {"referenceID": 20, "context": "When the data is approximately low rank, there exists a large body of work that characterizes the performance of the sequential column selection method (Step 1) used to form D [14], [24].", "startOffset": 182, "endOffset": 186}, {"referenceID": 10, "context": "In particular, the selection strategy in Step 2 of Algorithm 1 provides exponential decrease in the factorization error with each batch of columns that we select from A [14].", "startOffset": 169, "endOffset": 173}, {"referenceID": 10, "context": "Then according to [14], the difference between the expected value of the approximation error, i.", "startOffset": 18, "endOffset": 22}, {"referenceID": 36, "context": "For example, images of objects under different illumination conditions [41], motion trajectories of point-correspondences [31], neural data [17],", "startOffset": 71, "endOffset": 75}, {"referenceID": 26, "context": "For example, images of objects under different illumination conditions [41], motion trajectories of point-correspondences [31], neural data [17],", "startOffset": 122, "endOffset": 126}, {"referenceID": 13, "context": "For example, images of objects under different illumination conditions [41], motion trajectories of point-correspondences [31], neural data [17],", "startOffset": 140, "endOffset": 144}, {"referenceID": 1, "context": "to structured sparse and block-sparse signals [5] are all well-approximated by a union of low-dimensional subspaces.", "startOffset": 46, "endOffset": 49}, {"referenceID": 14, "context": "When A lies on a union of subspaces, this effectively bounds the sparsity level of each column of V [18].", "startOffset": 100, "endOffset": 104}, {"referenceID": 6, "context": "Previous theoretical studies have established a connection between the total error in a factorization of a kernel (or Gram) matrix and the accuracy of certain popular learning algorithms, including: kernel ridge regression and kernel SVM [10].", "startOffset": 238, "endOffset": 242}, {"referenceID": 6, "context": "We observe a polynomial relationship holds both in theory [10] and in practice.", "startOffset": 58, "endOffset": 62}, {"referenceID": 29, "context": "We use GraphLab Distributed API [34] to implement this model.", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "be the master vertex and the others to be the replica vertices (these definitions are borrowed from GraphLab [25]).", "startOffset": 109, "endOffset": 113}, {"referenceID": 21, "context": "We integrate and implement the proposed customized partitioning and distributed computation flow with the distributed GraphLab API [25].", "startOffset": 131, "endOffset": 135}, {"referenceID": 23, "context": "Our real datasets include Light Field data [2], hyper spectral images [1], a dictionary of video frames [28], and a collection of images of different faces under varying illumination conditions [22].", "startOffset": 104, "endOffset": 108}, {"referenceID": 18, "context": "Our real datasets include Light Field data [2], hyper spectral images [1], a dictionary of video frames [28], and a collection of images of different faces under varying illumination conditions [22].", "startOffset": 194, "endOffset": 198}, {"referenceID": 44, "context": "We have also implemented the distributed update on the factorized data on Apache Spark [49].", "startOffset": 87, "endOffset": 91}, {"referenceID": 21, "context": "The RankMap framework\u2019s sparse graph-based design is implemented using GraphLab, a high-level graph-parallel abstraction [25].", "startOffset": 121, "endOffset": 125}, {"referenceID": 2, "context": "To evaluate the performance of RankMap for sparse approximation, we use the fast iterative shrinkage-thresholding algorithm (FISTA) [6] to solve the `1-minimization problem in (2).", "startOffset": 132, "endOffset": 135}, {"referenceID": 31, "context": "to an overcomplete Light Field dictionary consisting of a large number of Light Field image patches collected from many scenes [36].", "startOffset": 127, "endOffset": 131}, {"referenceID": 40, "context": "Typically in image noise reduction applications, PSNR values of 30 dB and higher are desired [45], [7], [4].", "startOffset": 93, "endOffset": 97}, {"referenceID": 3, "context": "Typically in image noise reduction applications, PSNR values of 30 dB and higher are desired [45], [7], [4].", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "Typically in image noise reduction applications, PSNR values of 30 dB and higher are desired [45], [7], [4].", "startOffset": 104, "endOffset": 107}, {"referenceID": 32, "context": "There are a number of existing column sampling-based methods that aim to improve the performance of specific learning objectives, such as least-squares [37], `2-minimization with square root `1 penalty [40], and SVM [20].", "startOffset": 152, "endOffset": 156}, {"referenceID": 35, "context": "There are a number of existing column sampling-based methods that aim to improve the performance of specific learning objectives, such as least-squares [37], `2-minimization with square root `1 penalty [40], and SVM [20].", "startOffset": 202, "endOffset": 206}, {"referenceID": 16, "context": "There are a number of existing column sampling-based methods that aim to improve the performance of specific learning objectives, such as least-squares [37], `2-minimization with square root `1 penalty [40], and SVM [20].", "startOffset": 216, "endOffset": 220}, {"referenceID": 37, "context": "Our computation/communication and memory minimizing framework can also be applied to other optimization methods such as Stochastic Gradient Descent (SGD) [42] and Stochastic Coordinate Descent (SCD) [33].", "startOffset": 154, "endOffset": 158}, {"referenceID": 28, "context": "Our computation/communication and memory minimizing framework can also be applied to other optimization methods such as Stochastic Gradient Descent (SGD) [42] and Stochastic Coordinate Descent (SCD) [33].", "startOffset": 199, "endOffset": 203}, {"referenceID": 39, "context": "We have mainly explored its use for computational gains, however, recent theoretical results have shown that subsampling data can also be beneficial for learning [44].", "startOffset": 162, "endOffset": 166}], "year": 2016, "abstractText": "This paper introduces RankMap, a platform-aware end-to-end framework for efficient execution of a broad class of iterative learning algorithms for massive and dense datasets. Our framework exploits data structure to scalably factorize it into an ensemble of lower rank subspaces. The factorization creates sparse low-dimensional representations of the data, a property which is leveraged to devise effective mapping and scheduling of iterative learning algorithms on the distributed computing machines. We provide two APIs, one matrix-based and one graph-based, which facilitate automated adoption of the framework for performing several contemporary learning applications. To demonstrate the utility of RankMap, we solve sparse recovery and power iteration problems on various realworld datasets with up to 1.8 billion non-zeros. Our evaluations are performed on Amazon EC2 and IBM iDataPlex servers using up to 244 cores. The results demonstrate up to two orders of magnitude improvements in memory usage, execution speed, and bandwidth compared with the best reported prior work, while achieving the same level of learning accuracy.", "creator": "LaTeX with hyperref package"}}}