{"id": "1503.08895", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2015", "title": "End-To-End Memory Networks", "abstract": "In this paper we introduce a variant of Memory Networks that needs significantly less supervision to perform question and answering tasks. The original model requires that the sentences supporting the answer be explicitly indicated during training. In contrast, our approach only requires the answer to the question during training. To get the question to be properly indicated, the previous model can't be provided (with some exception, the option of not being explicitly specified at all).\n\n\n\n\nTo further investigate the effects of memory networks on the task performance, we examined the task-level differences in the tasks and tasks performed (the following values for each task were obtained:\nFor example, for the first time, there were no differences in task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences. For example, there were no differences in task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the task-level differences in the tasks performed on the", "histories": [["v1", "Tue, 31 Mar 2015 03:05:37 GMT  (119kb,D)", "http://arxiv.org/abs/1503.08895v1", null], ["v2", "Fri, 3 Apr 2015 02:23:20 GMT  (120kb,D)", "http://arxiv.org/abs/1503.08895v2", null], ["v3", "Sun, 12 Apr 2015 04:19:33 GMT  (120kb,D)", "http://arxiv.org/abs/1503.08895v3", null], ["v4", "Mon, 8 Jun 2015 21:42:20 GMT  (331kb,D)", "http://arxiv.org/abs/1503.08895v4", null], ["v5", "Tue, 24 Nov 2015 19:41:57 GMT  (332kb,D)", "http://arxiv.org/abs/1503.08895v5", "Accepted to NIPS 2015"]], "reviews": [], "SUBJECTS": "cs.NE cs.CL", "authors": ["sainbayar sukhbaatar", "arthur szlam", "jason weston", "rob fergus"], "accepted": true, "id": "1503.08895"}, "pdf": {"name": "1503.08895.pdf", "metadata": {"source": "CRF", "title": "Weakly Supervised Memory Networks", "authors": ["Sainbayar Sukhbaatar", "Arthur Szlam"], "emails": [], "sections": [{"heading": null, "text": "In this paper we introduce a variant of Memory Networks [16] that needs significantly less supervision to perform question and answering tasks. The original model requires that the sentences supporting the answer be explicitly indicated during training. In contrast, our approach only requires the answer to the question during training. We apply the model to the synthetic bAbI tasks, showing that our approach is competitive with the supervised approach, particularly when trained on a sufficiently large amount of data. Furthermore, it decisively beats other weakly supervised approaches based on LSTMs. The approach is quite general and can potentially be applied to many other tasks that require capturing longterm dependencies."}, {"heading": "1 Introduction", "text": "Memory Networks, introduced by Weston et al. [16], explore how explicit long-term storage can be combined with neural networks. They apply their model to a set of synthetic question and answering tasks [15], constructed to encompass many forms of reasoning including many that cannot be performed by current models such as recurrent neural nets (RNNs). At test time, the input to the model is a set of text sentences, along with a question and the model output is a predicted answer. One limitation of the model is that it needs extensive supervision to train, requiring not just the ground truth answer but also explicit indication of the supporting sentences within the text. This level of supervision simplifies the training of the read and write functions since the correspondence is explicit: the model is told which part of the input it should be using in the memory.\nIn this paper, we explore a model, similar to Memory Networks [16], that is able to learn with weak supervision, i.e. just the answer, without the need for support labels. This enables the model to operate in more general settings where carefully curated training data is not available. Furthermore, it demonstrates that a long-term memory can be integrated into neural network models that rely on standard input/output pairs for training, the prevailing paradigm for deep models at present. We apply our model to the bAbI tasks [15], allowing for a direct comparison of our model to the fully supervised Memory Networks. However the memory mechanism we propose is sufficiently general that it could be used for a range of other tasks requiring the modeling of long-term dependencies.\nOur model implements a content-based memory system that uses continuous functions for the read operation, and sequentially writes all inputs up to a fixed buffer size. This allows back-propagatation of the error signal through multiple memory accesses, back to the input during training. The embedding functions that implement the memory by capturing similarity between pairs of elements can thus be trained without explicit supervision.\nar X\niv :1\n50 3.\n08 89\n5v 1\n[ cs\n.N E\n] 3\n1 M"}, {"heading": "2 Approach", "text": "Our model primarily addresses the bAbI question and answering problems, proposed by Weston et al. , but could easily be adapted to other text applications. A given bAbI task consists of a set of statements, followed by a question whose answer is typically a single word (in a few tasks, answers are a set of words). Consider a sample task:\nSam walks into the kitchen. Sam picks up an apple. Sam walks into the bedroom. Sam drops the apple."}, {"heading": "Q: Where is the apple?", "text": ""}, {"heading": "A. Bedroom", "text": "The answer is available to the model at training time, but must be predicted at test time. Note that for each question, only some subset of the statements contain information needed for the answer, and the others are essentially irrelevant distractors (e.g. the first sentence in the above example). In Weston et al. , this supporting subset was explicitly indicated to the model during training and the key difference between that work and this one is that this information is no longer provided. Hence, the model must deduce for itself at training and test time which sentences are relevant and which are not. There are a total of 20 different types of bAbI tasks that probe different forms of reasoning and deduction. For a more complete description of them, please refer to [15].\nFormally, for one of the 20 bAbI tasks, we are given P example problems, each having a set of I sentences {xpi } where I \u2264 320; a question sentence qp and answer ap. The examples are randomly split into disjoint train and test sets. For brevity, we henceforth drop the example index p. Let the jth word of sentence i be xij , represented by a 1-hot vector of length V (where the vocabulary is of size V = 177, reflecting the simplistic nature of the bAbI language). The same representation is used for the question q and answer a."}, {"heading": "2.1 Single Layer", "text": "We start by describing our model for a single layer case, which implements a single memory lookup operation. We then show it can be stacked to give multiple \u201chops\u201d in memory. Since some of our target tasks require understanding of temporal order, we then describe a way to incorporate time into the model by introducing specific temporal features into the lookup operations.\nEach memory layer has an input and and output portion. The input part implements content-based addressing, with each memory location holding a distinct output vector. For the purposes of explanation, we describe our model for the case where a bag-of-words (BoW) representation is used for each sentence (other representations are considered in Section 2.3).\nInput side: Suppose we are given an input sentence\nxi = {xi1, xi2, ..., xin},\nwhere each xij is represented as a vector of size V with all entries set to zero except the index into the dictionary of xij\u2019s word. The memory vector mi of dimension d is computed by first embedding each word using an embedding matrix A (of size d \u00d7 V ) and then summing over all words in xi (assuming we are using a BoW representation):\nmi = \u2211 j Axij .\nThus, the entire set of sentences {xi} are converted into memory vectors {mi}. The question vector q is also embedded via another embedding matrix B (we use same dimensions as A): u = \u2211 j Bqj . In the embedding space, we compute the match between the question u and each memory mi, by taking the inner product followed by a softmax:\npi = Softmax(uTmi) = Softmax(qTBT \u2211 j Axij). (1)\nDefined in this way, p is a probability vector over the I sentences.\nOutput side: Each memory vector on the input has a corresponding output vector ci, given by another embedding matrix C applied to sentence xi:\nci = \u2211 j Cxij .\nThe output vector from the memory o is then a sum over the ci, weighted by the probability vector from the input: o = \u2211 i pici = \u2211 i \u2211 j piCxij (2)\nBecause the functions from input sentences and question to output is smooth, we can easily compute gradients and back-propagate through it. Other recently proposed forms of memory or attention take this approach, notably [6] and [2].\nAnswer prediction: In the single layer case, the sum of the output vector o and the question embedding u is then passed through a final weight matrix W (of size V \u00d7 d) and a softmax to produce the predicted answer:\na\u0302 = Softmax(W (o+ u)) (3)\nThe overall model is shown in Fig. 1. During training, all three embedding matrices A, B and C, as well as W are jointly learned by minimizing a standard cross-entropy loss between a\u0302 and the true answer a. Training is performed using stochastic gradient descent (see Section 3 for more details)."}, {"heading": "2.2 Multiple Layers", "text": "The single memory layer proposed is only able to answer questions that involve a single memory lookup. However, if a retrieved memory depends on another memory (e.g. as in the example from Section 2), then multiple lookups are required to answer the question. Thus, as in the original Memory Networks approach, we now extend our model to handle multiple lookup operations, or hops.\nThe memory layers are stacked in the following way:\n\u2022 The input to layers above the first is the sum of the output ok and the input uk from layer k:\nuk+1 = uk + ok. (4)\n\u2022 Each layer has its own embedding matrices Ak, Ck, used to embed the input sentences {xi}. However, as discussed below, they are constrained to ease training and reduce the number of parameters.\n\u2022 At the top of the network, the input to W also combines the input and the output of the top memory layer: a\u0302 = Softmax(W (oK + uK)).\nWe explore two types of weight tying within the model:\n1. Adjacent: the output embedding for one layer is the input embedding for the one above, i.e. Ak+1 = Ck.\n2. Layer-wise (RNN): the input and output embeddings are the same across different layers, i.e. A1 = A2 = A3 and C1 = C2 = C3.\nIn adjacent tying, we also constrain (a) the answer prediction matrix to be the same as the final output embedding, i.e WT = C3, and (b) the question embedding to match the input embedding of the first layer, i.e. B = A1.\nA three-layer version of our memory model is shown in Fig. 2. Overall, it is similar to the original Memory Network model, except for that the hard max operations within each layer have been replaced with a continuous weighting from the softmax.\nNote that if we use the layer-wise weight tying scheme, our model can be cast as a traditional RNN where we divide the outputs of the RNN into internal and external outputs. Emitting an internal output corresponds to considering a memory, and emitting an external output corresponds to answering a question. For simplicity, for this discussion, ignore temporal features, and pretend each memory consists of a single word. From the RNN point of view, u in Fig. 2 and (4) is a hidden state, and the model generates an internal output p using A; here, unlike a standard RNN, we explicitly condition on the outputs stored in memory. The model then ingests p using C, updates the hidden state, and so on. In contrast to a traditional RNN, the model makes several computational steps before producing an output meant to be seen by the \u201coutside world\u201d. In this view, the terminology of input and output from Fig. 1 is flipped- when viewed as a traditional RNN with this special conditioning of outputs, A becomes the output embedding of the RNN and C becomes the input embedding. Note that we keep the all outputs and inputs soft, excepting perhaps the final external output at test time. It may be possible to train these discretely using reinforcement learning or some form of structured prediction.\nFinally, note that with layer-wise weight tying, we have found it useful to add a linear mapping H to the update of u between hops; that is, uk+1 = Huk + ok. The value of H will be learned from data."}, {"heading": "2.3 Sentence Representation", "text": "In our experiments we explore two different representations for the sentences. The first is the bagof-words (BoW) representation, described above, which embeds each word and sums the resulting vectors: e.g mi = \u2211 j Axij . The issue with this is that it cannot capture the order of the words in the sentence, which is important for some tasks.\nWe therefore propose a second representation that encodes the position of words within the sentence. This takes the form: mi = \u2211 j lj \u00b7 Axij , where \u00b7 is an element-wise multiplication. lj is a column vector with the structure lkj = (1 \u2212 j/J) \u2212 (k/d)(1 \u2212 2j/J) (assuming 1-based indexing), with J being the number of words in the sentence, and d is the dimension of the embedding. In other words, lj is a linear ramp which goes from 1 to 0 for the first word, 0 to 1 for the last word and smoothly blends between the two as a function of word position. This encoding means that the order of the words now affects mi. We call this a position encoding (PE) representation. The same representation is always used for questions, memory inputs and memory outputs."}, {"heading": "2.4 Temporal Encoding", "text": "Several of the bAbI tasks require some notion of temporal context, i.e. Q: Where was Sam before he entered the bedroom?. To enable our model to address them, we modify the embedding of sentence xi so that mi = \u2211 j Axij + TA(i), where TA(i) is the ith row of a special matrix TA that encodes temporal information. The output embedding is augmented in the same way with a matrix Tc (e.g. ci = \u2211 j Cxij + TC(i)). Both TA and TC are learned during training. They are also subject to the same sharing constraints as A and C. Note that sentences are indexed in reverse order, reflecting their relative importance so that x1 is the last sentence of the story.\nLearning time invariance by injecting random noise: The original memory network model in [16] used a relative representation of time involving triples of memories. This representation uses less parameters than the absolute representation of time we use here, but it is not straightforward to integrate into our model. Because TA uses a significant number of parameters, and because for the bAbI problems we care about the relative order of events rather than the absolute sequence, we have found it helpful to add \u201cdummy\u201d memories to regularize TA. That is, at training time we can randomly add 10% percentage of empty memories to the stories. For example, if the story has 100 sentences, then we insert 10 dummy sentences into random memory locations. These memories do not affect the content of the stories at all, but only shift the sequence of the subsequent sentences in the story. We refer to this approach as random noise (RN)."}, {"heading": "3 Training", "text": "The model was trained using a learning rate of \u03b7 = 0.01, with anneals every 25 epochs by \u03b7/2 until 100 epochs were reached. No momentum or weight decay was used. The weights were initialized randomly from a Gaussian distribution with zero mean and \u03c3 = 0.1. When trained on all tasks simultaneously with 1k training samples (10k training samples), 60 epochs (20 epochs) were used with learning rate anneals of \u03b7/2 every 15 epochs (5 epochs). All training uses a batch size of 32 (but cost is not averaged over a batch), and gradients with an `2 norm larger than 40 are divided by a constant to have norm 40.\nIn some of our experiments, we explored commencing training with the softmax in each memory layer removed, making the model entirely linear except for the final weight matrix W . When the validation loss stopped decreasing, the softmax layers were re-inserted and training recommenced. We refer to this as linear start (LS) training. In LS training, the initial learning rate is set to \u03b7 = 0.005.\nThe capacity of memory is restricted to the most recent 50 sentences. Since the number of sentences and the number of words per sentence varied between problems, a null symbol was used to pad them all to a fixed size. The embedding of the null symbol was constrained to be zero."}, {"heading": "4 Related Work", "text": "A number of recent efforts have explored ways to capturing long-term structure within sequences using RNNs or LSTM-based models [3, 5, 9, 10, 7, 1]. The memory in these models is the state of the network, which is latent and inherently unstable over long timescales. The LSTM-based models\naddress this through local memory cells which lock in the network state from the past. In practice, the performance gains over carefully trained RNNs are modest [10]. Our model differs from these in that it uses a global memory, with shared read and write functions. However, with layer-wise weight tying our model can be viewed as a form of RNN which only outputs after a fixed number of time steps and the intermediary steps being internal outputs that use the memory input/output (see Section 2.2 for further discussion).\nSome of the very early work on neural networks by Steinbuch et al. [13] and Taylor [14] considered a memory that performed nearest-neighbor operations on stored input vectors and then fit parametric models to the retrieved sets. This has similarities to a single layer version of our model.\nSubsequent work in the 1990\u2019s explored other types of memory [12, 4, 11]. For example, Das et al. [4, 11] introduced an explicit stack with push and pop operations which has been revisited recently by Joulin et al. [8] in the context of an RNN model. However, as demonstrated by [8], stack-based memories are less well suited to the bAbI tasks where out of order retrieval is needed for some of them.\nClosely related to our model is the Neural Turing Machine of [6], which also uses a continuous memory representation and can be trained without explicit supervision using reinforcement learning. The NTM memory uses both content and address-based access, unlike ours which only explicitly allows the former, although the temporal features allow a sort of address-based access. However, in part because we always write each memory sequentially, our model is somewhat simpler, not requiring operations like sharpening, and is trained with backpropagation rather than reinforcement learning. Furthermore, we apply our memory model to textual reasoning tasks, which qualitatively differ from the more abstract operations of sorting and recall tackled by the NTM.\nOur model is also related to [2]. In that work, an LSTM based encoder and LSTM based decoder were used for machine translation. The decoder uses an attention model that finds which hidden states from the encoding are most useful for outputting the next translated word; the attention model uses a small neural network that inputs a concatenation of the current hidden state of the decoder and each of the encoders hidden states. Our \u201cmemory\u201d is analogous to their attention mechanism, both using weak supervision during training, although [2] is only over a single sentence rather than many, as in our case. Futhermore, our model makes several views on the memory before making an output. There are also differences in the architecture of the small network used to score the memories compared to our scoring approach; we use a simple linear layer, whereas they use a more sophisticated gated architecture."}, {"heading": "5 Experiments", "text": "The model is evaluated on the Babi question & answer dataset, proposed in [15]. Two versions of the data are used, one that has 1000 training problems per task and a second larger one with 10,000 per task. All experiments used a 3 layer model, implementing 3 memory lookups. Unless otherwise stated, the weight sharing scheme used is adjacent. For all tasks that output lists, we take each possible combination of possible outputs and record them as a separate answer vocabulary word. On some tasks, we observed a large variance in the performance of our model (i.e. sometimes failing badly, other times not, depending on the initialization). This is resolved by averaging the probability output from 10 models, which are same except for different random initialization. Doing this improves the mean performance by a few percent, but more importantly reduces the variance significantly."}, {"heading": "5.1 Baselines", "text": "We compare our approach to a range of alternate models:\n\u2022 MemNN: The fully supervised AM+NG+NL Memory Networks approach, proposed in [15].\n\u2022 MemNN-WS: A weakly supervised version of MemNN where the supporting sentence labels are not used in training. Since we are unable to backpropagate through the max operations in each layer, we enforce that the first hop memory should share at least one word with the question, and that the second hop memory should share at least one word with the first hop and at least one word with the answer. All those memories that conform\nare called valid memories, and the goal during training is to rank them higher than invalid memories using the same ranking criteria as during fully supervised training. \u2022 LSTM: A standard LSTM model, trained using question / answer pairs only (i.e. also\nweakly supervised).\nThose baseline models do not employ model averaging."}, {"heading": "5.2 Model Variants", "text": "We explore a variety of design choices: (i) BoW vs Position Encoding (PE) sentence representation; (ii) training on all 20 tasks jointly vs independent training (joint training used an embedding dimension of d = 50, while independent training used d = 20); (iii) two phase training: first without softmaxes, then with (linear start (LS)) vs training with softmaxes from the start.\nThe results across all 20 tasks are given in Table 1 and Table 2 for the 1k and 10k training sets, respectively. They show a number of interesting points:\n\u2022 For both sizes of training set, the best weakly supervised models are reasonably close to the supervised models (e.g. 1k: 6.7% for MemNN vs 11.3% for position encoding + linear start + random noise, jointly trained and 10k: 3.2% for MemNN vs 6.4% for position encoding + linear start + RNN), although the supervised models are still superior.\n\u2022 All variants of our proposed model comfortably beat the weakly supervised baseline methods (LSTM and MemNN-WS).\n\u2022 The position encoding (PE) representation improves over bag-of-words (BoW), as demonstrated by clear improvements on tasks 4,5,15 and 18, which require understand of the word ordering.\n\u2022 The linear start (LS) to training seems to help avoid local minima. See task 16 in Table 1, where PE alone gets 52.2% error, while using the linear reduces it to 1.4%. The same reduction can be seem for Table 2 (48.0% vs 0.0%). However, in the RNN type model, it did not work well.\n\u2022 Jittering the time index with random empty memories (RN) as described in Section 2.4 gives a small but consistent boost in performance, especially for the smaller training set.\n\u2022 Joint training on all tasks hurts for the 10k case, where the model capacity may be a limiting factor. In the 1k case however, it helps perhaps due to the lack of training data.\n\u2022 We also tried training our model in a fully supervised manner, using the position encoding representation. We use the supporting sentence information to lock the output of the softmax to be a 1-hot vector, corresponding to the correct memory. Note that model averaging was not used in this case. In some tasks (e.g. 4,6,8,10,18), the weakly supervised version outperforms the supervised model. This indicates that the former is finding a more efficient solution to the tasks, leveraging the continuous p in each hop versus the binary version in the supervised model."}, {"heading": "6 Conclusions and Future Work", "text": "In this work we presented an architecture for solving the tasks in [15] with no supervision of supporting facts. By using smooth memory lookups, we are able to train with backpropagation, and the system is able to learn which facts to focus on using only the answers to the questions and the stories. The architecture performs significantly better than the baselines (LSTM, memory networks trained with weak supervision).\nHowever, there is still much to do. Our model is still is unable to match the performance of the memory networks trained with full supervision. Furthermore, smooth lookups may not scale well to very long stories or to question answering tasks with large databases. For these settings, we plan to explore multiscale notions of attention. Furthermore, as in [6], it may be possible to replace backpropagation with some form of reinforcement learning."}], "references": [{"title": "Memory-based neural networks for robot learning", "author": ["C.G. Atkeson", "S. Schaal"], "venue": "Neurocomputing, 9:243\u2013 269", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1409.0473", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "\u00c7. G\u00fcl\u00e7ehre", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1412.3555", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory", "author": ["S. Das", "C.L. Giles", "G.-Z. Sun"], "venue": "In Proceedings of The Fourteenth Annual Conference of Cognitive Science Society", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1992}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "CoRR, abs/1308.0850", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "CoRR, abs/1410.5401", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["A. Joulin", "T. Mikolov"], "venue": "CoRR, abs/1503.01007", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning longer memory in recurrent neural networks", "author": ["T. Mikolov", "A. Joulin", "S. Chopra", "M. Mathieu", "M. Ranzato"], "venue": "CoRR, abs/1412.7753", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "A connectionist symbol manipulator that discovers the structure of context-free languages", "author": ["M.C. Mozer", "S. Das"], "venue": "Advances in Neural Information Processing Systems, pages 863\u2013863", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1993}, {"title": "The induction of dynamical recognizers", "author": ["J. Pollack"], "venue": "Machine Learning, 7(2-3):227\u2013252", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1991}, {"title": "Learning matrices and their applications", "author": ["K. Steinbuch", "U. Piske"], "venue": "IEEE Transactions on Electronic Computers, 12:846\u2013862", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1963}, {"title": "Pattern recognition by means of automatic analogue apparatus", "author": ["W.K. Taylor"], "venue": "Proceedings of The Institution of Electrical Engineers, 106:198\u2013209", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1959}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "arXiv preprint: 1502.05698", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "arXiv preprint: 1410.391v8", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "In this paper we introduce a variant of Memory Networks [16] that needs significantly less supervision to perform question and answering tasks.", "startOffset": 56, "endOffset": 60}, {"referenceID": 14, "context": "[16], explore how explicit long-term storage can be combined with neural networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "They apply their model to a set of synthetic question and answering tasks [15], constructed to encompass many forms of reasoning including many that cannot be performed by current models such as recurrent neural nets (RNNs).", "startOffset": 74, "endOffset": 78}, {"referenceID": 14, "context": "In this paper, we explore a model, similar to Memory Networks [16], that is able to learn with weak supervision, i.", "startOffset": 62, "endOffset": 66}, {"referenceID": 13, "context": "We apply our model to the bAbI tasks [15], allowing for a direct comparison of our model to the fully supervised Memory Networks.", "startOffset": 37, "endOffset": 41}, {"referenceID": 13, "context": "For a more complete description of them, please refer to [15].", "startOffset": 57, "endOffset": 61}, {"referenceID": 5, "context": "Other recently proposed forms of memory or attention take this approach, notably [6] and [2].", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "Other recently proposed forms of memory or attention take this approach, notably [6] and [2].", "startOffset": 89, "endOffset": 92}, {"referenceID": 14, "context": "Learning time invariance by injecting random noise: The original memory network model in [16] used a relative representation of time involving triples of memories.", "startOffset": 89, "endOffset": 93}, {"referenceID": 2, "context": "A number of recent efforts have explored ways to capturing long-term structure within sequences using RNNs or LSTM-based models [3, 5, 9, 10, 7, 1].", "startOffset": 128, "endOffset": 147}, {"referenceID": 4, "context": "A number of recent efforts have explored ways to capturing long-term structure within sequences using RNNs or LSTM-based models [3, 5, 9, 10, 7, 1].", "startOffset": 128, "endOffset": 147}, {"referenceID": 8, "context": "A number of recent efforts have explored ways to capturing long-term structure within sequences using RNNs or LSTM-based models [3, 5, 9, 10, 7, 1].", "startOffset": 128, "endOffset": 147}, {"referenceID": 6, "context": "A number of recent efforts have explored ways to capturing long-term structure within sequences using RNNs or LSTM-based models [3, 5, 9, 10, 7, 1].", "startOffset": 128, "endOffset": 147}, {"referenceID": 0, "context": "A number of recent efforts have explored ways to capturing long-term structure within sequences using RNNs or LSTM-based models [3, 5, 9, 10, 7, 1].", "startOffset": 128, "endOffset": 147}, {"referenceID": 8, "context": "In practice, the performance gains over carefully trained RNNs are modest [10].", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "[13] and Taylor [14] considered a memory that performed nearest-neighbor operations on stored input vectors and then fit parametric models to the retrieved sets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] and Taylor [14] considered a memory that performed nearest-neighbor operations on stored input vectors and then fit parametric models to the retrieved sets.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "Subsequent work in the 1990\u2019s explored other types of memory [12, 4, 11].", "startOffset": 61, "endOffset": 72}, {"referenceID": 3, "context": "Subsequent work in the 1990\u2019s explored other types of memory [12, 4, 11].", "startOffset": 61, "endOffset": 72}, {"referenceID": 9, "context": "Subsequent work in the 1990\u2019s explored other types of memory [12, 4, 11].", "startOffset": 61, "endOffset": 72}, {"referenceID": 3, "context": "[4, 11] introduced an explicit stack with push and pop operations which has been revisited recently by Joulin et al.", "startOffset": 0, "endOffset": 7}, {"referenceID": 9, "context": "[4, 11] introduced an explicit stack with push and pop operations which has been revisited recently by Joulin et al.", "startOffset": 0, "endOffset": 7}, {"referenceID": 7, "context": "[8] in the context of an RNN model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "However, as demonstrated by [8], stack-based memories are less well suited to the bAbI tasks where out of order retrieval is needed for some of them.", "startOffset": 28, "endOffset": 31}, {"referenceID": 5, "context": "Closely related to our model is the Neural Turing Machine of [6], which also uses a continuous memory representation and can be trained without explicit supervision using reinforcement learning.", "startOffset": 61, "endOffset": 64}, {"referenceID": 1, "context": "Our model is also related to [2].", "startOffset": 29, "endOffset": 32}, {"referenceID": 1, "context": "Our \u201cmemory\u201d is analogous to their attention mechanism, both using weak supervision during training, although [2] is only over a single sentence rather than many, as in our case.", "startOffset": 110, "endOffset": 113}, {"referenceID": 13, "context": "The model is evaluated on the Babi question & answer dataset, proposed in [15].", "startOffset": 74, "endOffset": 78}, {"referenceID": 13, "context": "\u2022 MemNN: The fully supervised AM+NG+NL Memory Networks approach, proposed in [15].", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "In this work we presented an architecture for solving the tasks in [15] with no supervision of supporting facts.", "startOffset": 67, "endOffset": 71}, {"referenceID": 5, "context": "Furthermore, as in [6], it may be possible to replace backpropagation with some form of reinforcement learning.", "startOffset": 19, "endOffset": 22}], "year": 2015, "abstractText": "In this paper we introduce a variant of Memory Networks [16] that needs significantly less supervision to perform question and answering tasks. The original model requires that the sentences supporting the answer be explicitly indicated during training. In contrast, our approach only requires the answer to the question during training. We apply the model to the synthetic bAbI tasks, showing that our approach is competitive with the supervised approach, particularly when trained on a sufficiently large amount of data. Furthermore, it decisively beats other weakly supervised approaches based on LSTMs. The approach is quite general and can potentially be applied to many other tasks that require capturing longterm dependencies.", "creator": "LaTeX with hyperref package"}}}