{"id": "1504.00548", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Apr-2015", "title": "Learning to Understand Phrases by Embedding the Dictionary", "abstract": "Distributional models that learn rich semantic word representations are a success story of recent NLP research. However, developing models that learn useful representations of phrases and sentences has proved far harder. We propose using the definitions found in everyday dictionaries as a means of bridging this gap between lexical and phrasal semantics. Using an example, we write a simple semantic lexical grammar that learns semantic words that are a common vocabulary for many different languages. The first class of semantic lexical grammar that comes up for use in dictionaries is the DML grammar. This language, in the definition of a word, is composed of two lexical grammar classes: the DML grammar. The DML grammar is the DML grammar and is composed of two lexical grammar classes: the DML grammar. The DML grammar is the DML grammar and is composed of two lexical grammar classes: the DML grammar. The DML grammar is the DML grammar and is composed of two lexical grammar classes: the DML grammar. The DML grammar is the DML grammar and is composed of two lexical grammar classes: the DML grammar. The DML grammar is the DML grammar and is composed of two lexical grammar classes: the DML grammar. The DML grammar is the DML grammar and is composed of two lexical grammar classes: the DML grammar. The DML grammar is the DML grammar and is composed of two lexical grammar classes: the DML grammar. The DML grammar is the DML grammar and is composed of two lexical grammar classes: the DML grammar. The DML grammar is the DML grammar and is composed of two lexical grammar classes: the DML grammar. The DML grammar is the DML grammar and is composed of two lexical grammar classes: the DML grammar. The DML grammar is the DML grammar and is composed of two lexical grammar classes: the DML grammar. The DML grammar is the DML grammar and is composed of two lexical grammar classes: the DML grammar. The DML grammar is the DML grammar and is composed of two lexical grammar classes: the DML grammar. The DML grammar is the DML grammar and is composed of two lexical grammar classes: the DML grammar. The DML grammar is the DML grammar and is composed of two lexical grammar classes: the DML grammar. The DML grammar is the DML grammar and is composed of two lexical", "histories": [["v1", "Thu, 2 Apr 2015 13:30:27 GMT  (29kb)", "http://arxiv.org/abs/1504.00548v1", null], ["v2", "Fri, 17 Apr 2015 19:34:07 GMT  (30kb)", "http://arxiv.org/abs/1504.00548v2", null], ["v3", "Sun, 30 Aug 2015 21:34:26 GMT  (32kb)", "http://arxiv.org/abs/1504.00548v3", null], ["v4", "Tue, 22 Mar 2016 16:30:17 GMT  (32kb)", "http://arxiv.org/abs/1504.00548v4", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["felix hill", "kyunghyun cho", "anna korhonen", "yoshua bengio"], "accepted": true, "id": "1504.00548"}, "pdf": {"name": "1504.00548.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Kyunghyun Cho", "Yoshua Bengio"], "emails": ["felix.hill@cl.cam.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 4.\n00 54\n8v 1\n[ cs\n.C L\n] 2\nA pr\n2 01\n5"}, {"heading": "1 Introduction", "text": "Much recent research in computational semantics has focussed on learning representations of arbitrary-length phrases and sentences. This task is challenging partly because there is no obvious gold standard of phrasal representation that could be used in training, evaluation and comparison of different systems. Consequently, it is difficult to design approaches that could learn from such a gold standard, and also hard to evaluate or compare different models.\nIn this work, we use dictionary definitions to address this issue. The composed meaning of the words in a dictionary definition (a tall, longnecked, spotted ruminant of Africa) should correspond to the meaning of the word they define (giraffe). This bridge between lexical and phrasal semantics is useful because high quality vector representations of single words can be used as a target when learning to combine the words into a coherent phrasal representation.\nThis approach still requires a model capable of learning to map between arbitrary-length phrases and fixed-length continuous-valued word vectors. For this purpose we use a recurrent neural network (RNN) (Schmidhuber, 1989) with long-short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997). Prior to training the RNN, we learn its target lexical representations by training the Word2Vec software (Mikolov et al., 2013) on billions of words of raw text.\nWe demonstrate the usefulness of our approach for two applications. The first is a reverse dictionary or concept finder: a system that returns words based on user descriptions or definitions (Zock and Bilac, 2004). Reverse dictionaries are used by copywriters, novelists, translators and other professional writers to find words for notions or ideas that might be on the tip of their tongue. For instance, a travel-writer might look to enhance her prose by searching for examples of a country that people associate with warm weather or an activity that is mentally or physically demanding. We show that an RNN-based reverse dictionary trained on only a handful of dictionaries identifies novel definitions and concept descriptions comparably or better than commercial systems, which took many years to develop and rely on a much larger memory footprint. Moreover, thanks to recent work on multilingual embedding spaces (Gouws et al., 2014), we show that\nthe RNN approach can be easily extended to produce a potentially useful cross-lingual reverse dictionary.\nThe second application of our model is as a general-knowledge crossword question answerer. When trained on both dictionary definitions and the opening sentences of Wikipedia articles, the RNN produces plausible answers to (non-cryptic) crossword clues, even those that apparently require detailed world knowledge. Our system outperforms two bespoke commercial crossword solving tools, and again has a smaller memory footprint, making it much more portable. Qualitative analysis reveals that the RNN learns to relate concepts that are not directly connected in the training data and can thus generalise well to unseen input. To facilitate further research, all of our code, training and evaluation sets (together with a system demo) are published online with this paper."}, {"heading": "2 Model Architecture", "text": "The architecture underlining our model is a recurrent neural network (RNN). RNNs operate on variable-length sequences of inputs; in our case, natural language definitions, descriptions or sentences. RNNs (with LSTMs) have achieved state-of-the-art performance in language modelling (Mikolov et al., 2010) image caption generation (Kiros et al., 2014), approach state-of-the-art performance in machine translation (Bahdanau et al., 2015). During training, the input to the RNN is a dictionary definition (for the reverse dictionary model), or sentence from an encyclopedia (in the question answering model). The objective of the model is to map these definitions to an embedding of the word that the definition defines. The target word embeddings are learned independently of the RNN weights, using the Word2Vec software (Mikolov et al., 2013).\nThe set of all words in the training data constitutes the vocabulary of the RNN. For each word in this vocabulary we randomly initialise a realvalued vector (input embedding) of model parameters. The RNN \u2018reads\u2019 the first word in the input by applying a non-linear projection of its embedding v1 parameterised by input weight matrix W and b, a vector of biases.\nA1 = \u03c6(Wv1 + b)\nyielding the first internal activation state A1. In our implementation, we use \u03c6(x) = tan(x),\nthough in theory \u03c6 can be any differentiable nonlinear function. Subsequent internal activations (after time-step t) are computed by projecting the embedding of the tth word and using this information to \u2018update\u2019 the internal activation state.\nAt = \u03c6(UAt\u22121 +Wvt + b)\n. As such, the values of the final internal activation state units AN are a weighted function of all input word embeddings, and constitute a \u2018summary\u2019 of the information in the sentence."}, {"heading": "2.1 Long Short Term Memory", "text": "A known limitation when training RNNs to read language using gradient descent is that the error signal (gradient) on the training examples either vanishes or explodes as the number of time steps (sentence length) increases. Consequently, after reading longer sentences the final internal activation AN typically retains useful information about the most recently read (sentence-final) words, but can neglect important information near the start of the input sentence. LSTMs were designed to mitigate this long-term dependency problem.\nAt each time step t, in place of the single internal layer of units A, the LSTM RNN computes six internal layers gw, gi, gf , go, h and m. The first, gw, represents the core information passed to the LSTM unit by the latest input word at t. It is computed as a simple linear projection of the input embedding vt (by input weights Ww) and the output state of the LSTM at the previous time step ht\u22121 (by update weights Uw):\ngw = Wwvt + U wht\u22121 + bw\nThe layers gi, gf and go are computed as weighted sigmoid functions of the input embeddings, again parameterised by layer-specific weight matrices W and U :\ngx = 1\n1 + exp(\u2212(W xvt + Uxht\u22121 + bx))\nwhere x stands for one of i, f or o. These vectors take values on [0, 1] and are often referred to as gating activations. Finally, the internal memory state, mt and new output state ht, of the LSTM at t are computed as,\nmt = vt \u2299 g i +mt\u22121 \u2299 g f , ht = g o \u2299 \u03c6(mt)\nwhere \u2299 indicates elementwise vector multiplication and \u03c6 is, as before, some non-linear function (we use tanh). Thus, gi determines to what extent the new input word is considered at each time step, gf determines to what extent the existing state of the internal memory is retained or forgotten in computing the new internal memory, and go determines how much this memory is considered when computing the output state at t.\nThe sentence-final memory state of the LSTM, mN , a \u2018summary\u2019 of all the information in the sentence, is then projected via an extra non-linear projection (parameterised by a further weight matrix) to a target embedding space. This layer enables the target (defined) word embedding space to take a different dimension to the activation layers of the RNN, and in principle enables a more complex definition-reading function to be learned.\nThe training objective of the model M is to map the input sentence sc defining word c to the pretrained embedding vc of c. The cost of the wordsentence pair (c, sc) from the training data is then simply the cosine distance between M(sc) and vc."}, {"heading": "2.2 Implementation Details", "text": "The RNN word embeddings in our implementation had length 256, and at each time step each of the four LSTM RNN internal layers (gating and activation states) had 512 units. To create the space for target embeddings, we trained a continuous bag-of-words (CBOW) model using the Word2Vec software on approximately 8 billion words of running text.1 The embeddings in the target space had dimension 500.\nThe model was implemented with Theano (Bergstra et al., 2010) and trained with minibatch SGD on GPUs. The batch size was fixed at 16 and the learning rate was controlled by adadelta (Zeiler, 2012). Training each model took approximately 24 hours. We make all model code publicly available."}, {"heading": "3 Reverse Dictionaries", "text": "The most immediate application of our trained models is as a reverse dictionary or concept finder. It is simple to look up a definition in a dictionary\n1The Word2Vec embedding models are well known; further details can be found at https://code.google.com/p/word2vec/ The training data for our model was compiled from various online text sources using the script demo-train-big-modelv1.sh from the same page.\ngiven a word, but professional writers often also require suitable words for a given idea, concept or definition.2 Reverse dictionaries satisfy this need by returning candidate words given a phrase, description or definition. For instance, when queried with the phrase an activity that requires strength and determination, the OneLook.com reverse dictionary returns the concepts exercise and work. Our trained RNN model can perform a similar function, simply by mapping a phrase to a point in the target (Word2Vec) embedding space, and returning the words corresponding to the embeddings that are closest to that point.\nSeveral other academic studies have proposed reverse dictionary models. These generally rely on common techniques from information retrieval, comparing definitions in their internal database to the input query, and returning the word whose definition is \u2018closest\u2019 to the input query (Bilac et al., 2003; Bilac et al., 2004; Zock and Bilac, 2004). Proximity is quantified differently in each case, but is generally a function of hand-engineered features of the two sentences. For instance, Shaw et al. (2013) propose a method in which the candidates for a given input query are all words in the model\u2019s database whose definitions contain one or more words from the query. This candidate list is then ranked according to a query-definition similarity metric based on the hypernym and hyponym relations in WordNet, features commonly used in IR such as tf-idf and a parser.\nThere are, in addition, at least two commercial online reverse dictionary applications, whose architecture is proprietary knowledge. The first is the Dictionary.com reverse dictionary 3, which retrieves candidate words from the Dictionary.com dictionary based on user definitions or descriptions. The sencond is OneLook.com, whose algorithm searches 1061 indexed dictionaries, including all major freely-available online dictionaries and resources such as Wikipedia and WordNet."}, {"heading": "3.1 Training", "text": "To compile a bank of dictionary definitions for training the model, we started with all words in the target embedding space. For each of these words, we extracted dictionary-style definitions from five electronic resources: Wordnet, The American Her-\n2See the testimony from professional writers at http://www.onelook.com/?c=awards\n3Available at http://dictionary.reference.com/reverse/\nitage Dictionary, The Collaborative International Dictionary of English, Wiktionary and Webster\u2019s. We chose these five dictionaries because they are freely-available via the WordNik API,4 but in theory any dictionary could be chosen. Most words in our training data had multiple definitions. For each word w with definitions {d1 . . . dn} we included all pairs (w, d1) . . . (w, dn) as training examples. This resulted in \u2248 900, 000 word-definition pairs of \u2248 10, 000 unique words. We label the model trained on all of these definitions (except those in the test set) RNN All All.\nWe also wished to explore the effect training on only subsets of this data. To test whether there is any advantage on training on multiple dictionaries, we trained an equivalent model, RNN WN All, on definitions from WordNet only.5 To test whether any advantage is gained by training on multiple definitions for each word, we train an additional model on the first definition in WordNet for each word only RNN WN First. As with other dictionaries, the first definition of a word in WordNet generally corresponds to the most typical or common sense of that word."}, {"heading": "3.2 Comparison and Evaluation", "text": "As a baseline for the RNN approach, we implemented two unsupervised methods using the neural (Word2Vec) word embeddings from the target word space. In the first (W2V add), we compose the embeddings for each word in the input query by pointwise addition, and return as candidates the nearest word embeddings to the resulting composed vector. The second baseline, (W2V mult), is identical except that the embeddings are composed by elementwise multiplication. Both methods of composition were suggested in a recent study on building phrase representations from word embeddings (Milajevs et al., ).\nNone of the models or evaluations from previous academic research on reverse dictionaries is publicly available, so direct comparison is not possible. However, we do compare performance with the commercial systems. The Dictionary.com system returned no candidates for over 96% of our input definitions. We therefore conduct detailed comparison with OneLook.com, which is the first reverse dictionary tool returned by a google search\n4See http://developer.wordnik.com 5Definitions in WordNet are sometimes called glosses.\nseems to be the most popular among writers.\nTo our knowledge there are no established means of measuring reverse dictionary performance. In the only previous academic research on English reverse dictionaries that we are aware of, evaluation was conducted on 300 word-definition pairs written by lexicographers, but which are not publicly available (Shaw et al., 2013). We therefore developed new evaluation sets and make them publicly available for evaluating future models.\nThe evaluation items are of three types, designed to test different properties of the models. To create the seen evaluation, we randomly selected 500 words from the WordNet training data (seen by all models), and then randomly selected a definition for each word. Testing models on the resulting 500 word-definition pairs assesses their ability to recall or decode previously encoded information. For the unseen evaluation, we randomly selected 500 words from WordNet and excluded all definitions of these words from the training data of all models.\nFinally, for a fair comparison with OneLook, which has both the seen and unseen pairs in its internal database, we built a new dataset of concept descriptions that do not appear in the training data for any model. To do so, we randomly selected 200 adjectives, nouns or verbs from among the top 3000 most frequent tokens in the British National Corpus (Leech et al., 1994) (but outside the top 100). We then asked ten native English speakers to write a single-sentence \u2018description\u2019 of these words. To ensure the resulting descriptions were good quality, for each description we asked two participants who did not produce that description to list all words that fitted the description. If the target word was not produced by one of the two checkers, the original participant was asked to rewrite the description. These concept descriptions, together with other evaluation sets, can be downloaded from our website for future comparisons."}, {"heading": "3.3 Results", "text": "Table 1 shows the performance of the different models in the three evaluation settings. Of the baseline methods involving bottom-up composition of Word2Vec embeddings, elementwise addition is clearly more effective than multiplication, which almost never returns the correct word as the nearest neighbour of the composition. Overall, however, the bespoke reverse dictionary models (RNN and OneLook) outperform both baselines. This is unsurprising given that both systems are designed for this task whereas the baselines involve no task-specific training.\nTraining the RNN models on different sets of definitions results in interesting variation in performance. When training is restricted to the first WordNet definition for each word (RNN WN First), the model is effective at retrieving words for definitions it has already seen, but lacks the general knowledge to effectively generalise to new, unseen items. Interestingly, when training is extended to all definitions for each word (RNN WN All), the performance on seen (first) definitions from WordNet improves further. This implies that training data can improve the retrieval of words to which that data does not directly pertain, possibly by increasing the general linguistic and conceptual knowledge of the model. However, there is a limit to this effect: when all definitions from all available dictionaries (RNN All All) are included in the training data, the performance on the (seen) WordNet first definitions degrades. This suggests that directly relevant knowledge can be lost if the model sees too much indirectly-relevant information. Nevertheless, the model trained on all available data is clearly the most robust, in that it performs similarly well on both seen and unseen definitions and descriptions.\nThe results also indicate interesting differences\nbetween the RNN approach and the OneLook dictionary search engine. The Seen (WN first) definitions in Table 1 occur in both the training data for the RNN models and the lookup data for the OneLook model. Clearly the OneLook algorithm is better than the RNN models at retrieving already available information (returning 89% of correct words among the top-ten candidates on this set). However, this comes at the cost of a greater memory footprint, since the model requires access to its database of dictionaries at query time.6\nMoreover, performance on the unseen concept descriptions suggests that the RNN model is better than OneLook at generalising to novel, unseen definitions. While the two models place the correct word among their top candidates on this evaluation with approximately equal frequency (accuracy@10/100), the mean rank and rank variance of the correct word among the RNN candidates is much lower. The RNN is therefore more \u2018consistent\u2019 than OneLook in its ability to assign a reasonably high ranking to the correct word. In the next section we explore the differences in the model output more closely."}, {"heading": "3.4 Qualitative Analysis", "text": "Example queries and top-five candidates from the models are presented in Table 6. They illustrate properties of the RNN output that should also be evident when querying the web demo. The first example demonstrates how the model generalises beyond its training data. Four of the top five responses could be classed as appropriate in that they refer to inhabitants of cold countries. However, there is no mention of cold or anything to do with climate in the dictionary definitions of Es-\n6Our trained model files are approximately half the size of the six training dictionaries stored as plain text, and would therefore be hundreds of times smaller than the OneLook database of 1061 dictionaries.\nkimo, Scandinavian, Scandinavia etc. in the training data. The model has learned that coldness is a characteristic of Scandinavia, Siberia and relates to Eskimos via connections with other concepts that are described or defined as cold. In contrast, the candidates produced by the OneLook and W2V baseline models have nothing to do with coldness, suggesting that they are not capable of drawing such indirect (or higher-order) connections between entities in the training data.\nThe second example demonstrates how the RNN model returns candidates whose linguistic function is appropriate to the query. For a query referring explicitly to a means, method or process, the RNN model produces verbs in different forms or an appropriate deverbal noun. In contrast, OneLook returns words of all types (aerodynamics, draught) that are arbitrarily related to the words in the query. A similar effect is apparent in the third example. While the candidates produced by the OneLook model are the correct part of speech (Noun), and related to the query topic, they are not semantically appropriate. The RNN model is the only one that returns a list of plausible habits, the class of noun requested by the input."}, {"heading": "3.5 Cross-Lingual Reverse Dictionaries", "text": "We now show how the RNN architecture can be easily modified to create a bilingual reverse dictionary - a system that returns candidate words in one language given a description or definition in another. A bilingual reverse dictionary could have clear applications for translators or transcribers. Indeed, the problem of attaching appropriate words to concepts may be more common when searching for words in a second language than in a monolingual context.\nTo create the bilingual variant, we simply replace the Word2Vec target embeddings with those from a bilingual embedding space. Bilingual embedding models use bilingual corpora to learn a space of representations of the words in two languages, such that words from either language that have similar meanings are close together (Hermann and Blunsom, 2013; Lauly et al., 2014; Gouws et al., 2014). For our experiment, we used English-French embeddings learned by the state-of-the-art BilBOWA model (Gouws et al., 2014) from the Wikipedia\n(monolingual) and Europarl (bilingual) corpora.7 We trained the RNN model to map from English definitions to English words in the bilingual space. At test time, after reading an English definition, we then simply return the nearest French word neighbours to that definition.\nBecause no benchmarks exist for quantitative evaluation of bilingual reverse dictionaries, we compare this approach qualitatively with two alternative methods for mapping definitions to words across languages. The first is analogous to the W2V Add model of the previous section: in the bilingual embedding space, we first compose the embeddings of the English words in the query definition with elementwise addition, and then return the French word whose embedding is nearest to this vector sum. The second uses the RNN monolingual reverse dictionary model to identify an English word from an English definition, and then translates that word using Google Translate.\nTable 2 shows that the RNN model can be effectively modified to create a cross-lingual reverse dictionary. It is perhaps unsurprising that the W2V Add model candidates are generally the lowest quality given the performance of the method monolingual setting. In comparing the two RNNbased methods, the fully bilingual RNN appears to have two advantages over the RNN + Google approach. First, it does not require online access to a bilingual word-word mapping as defined e.g. by Google Translate. Second, it less prone to errors caused by word sense ambiguity. For example, in response to the query an emotion you feel after being rejected, the bilingual embedding RNN returns emotions or adjectives describing mental states. In contrast, the monolingal+Google model incorrectly maps the plausible English response regret to the verbal infinitive regretter. The model makes the same error when responding to a description of a fly, returning the verb voler (to fly)."}, {"heading": "3.6 Discussion", "text": "We have shown that simply training the RNN-toword-embedding architecture on six dictionaries yields a reverse dictionary that performs comparably to the leading commercial system, and with certain key advantages. First, it consistently returns syntactically and semantically plausible re-\n7The approach should work with any bilingual embeddings. We thank Stephan Gouws for doing the training.\nsponses as part of a more coherent and homogenous set of candidates. Second, it requires many times less memory, which is a significant advantage given that language applications and tools generally benefit from portability (e.g. deployable on mobile devices). We also showed how the architecture can be easily extended to produce bilingual versions of the same model. Of course, in the analyses performed thus far, we only test the RNN approach on tasks that it was trained to accomplish (mapping definitions or descriptions to words). In the next section, we test the general applicability of the approach by exploring whether the representations learned by the model can be effectively transferred to a novel task."}, {"heading": "4 General Knowledge (crossword) Question Answering", "text": "The automatic answering of questions posed in natural language is a central problem of Artificial Intelligence. Although web search and IR techniques provide a means to find sites or documents related to language queries, at present, internet users requiring a specific fact must still sift through pages to locate the desired information.\nSystems that attempt to overcome this, via fully open-domain or general knowledge questionanswering (open QA), generally require large teams of researchers, modular design and powerful infrastructure, exemplified by IBM\u2019s Watson (Ferrucci et al., 2010). For this reason, much academic research focuses on settings in which\nthe scope of the task is reduced. This has been achieved by restricting questions to a specific topic or domain (Molla\u0301 and Vicedo, 2007), allowing systems access to pre-specified passages of text from which the answer can be inferred (Iyyer et al., 2014; Weston et al., 2015), or centering both questions and answers on a particular knowledge base (Berant and Liang, 2014; Bordes et al., 2014).\nIn what follows, we show RNNs trained on dictionary data may ultimately be a useful component of open QA system. Given the absence of a knowledge base or web-scale information in our architecture, we narrow the scope of the challenged facing our models by focusing on general knowledge crossword questions. General knowledge (noncryptic, or quick) crosswords appear in national newspapers in many countries. Crossword question answering is more tractable that general open QA for two reasons. First, models know the length of the correct answer (in letters), reducing the search space. Second, some crossword questions mirror definitions, in that they refer to fundamental properties of concepts (a twelve-sided shape) or request a category member (a city in Egypt).8\nThe architecture of the model we apply to crossword questions is identical to that used to create the reverse dictionary. However, since many general-knowledge crossword questions refer to\n8As our interest is in the language understanding, we do not address the question of fitting answers into a grid, which is the main concern of end-to-end automated crossword solvers (Littman et al., 2002).\nnamed entities, people and places, we experiment by supplementing the dictionary definitions used by the previous model with content from Wikipedia. For every word in model\u2019s target embedding space that is also the title of an article in Wikipedia, we treat the sentences in the first paragraph of the article as if they were (independent) definitions of that word. When a word in Wikipedia also occurs in one (or more) of the dictionaries used previously, we simply add these pseudo-definitions to the training set of definitions for the word, noting the experiments with WordNet in the previous section, which showed that using more definitions for each word generally improves performance."}, {"heading": "4.1 Evaluation", "text": "General Knowledge crossword questions come in different styles and forms. We used the Eddie James crossword website to compile a bank of sentence-like general-knowledge questions.9 Eddie James is one of the UK\u2019s leading crossword compilers, working for several national newspapers. Our long question set consists of the first 150 questions (starting from puzzle #1) from his general-knowledge crosswords, excluding clues of fewer than four words and those whose answer was not a single word (e.g. kingjames).\nTo evaluate models on a different type of clue, we also compiled a set of shorter questions based on the Guardian Quick Crossword. Guardian questions still require general factual or linguistic knowledge, but are generally shorter and somewhat more cryptic than the longer Eddie James clues. We again formed a list of 150 questions, beginning on 1 January 2015 and excluding any questions with multiple-word answers. For clear contrast, we excluded those few questions of length greater than four words. Of these 150 clues, a subset of 30 were single-word clues. All evaluation datasets are available online with the paper.\n9http://www.eddiejames.co.uk/"}, {"heading": "4.2 Benchmarks and Comparisons", "text": "We evaluate RNN models trained with and without Wikipedia integrated into the training data. As before, candidates are extracted from the model by inputting definitions and returning words corresponding to the closest embeddings in the target space, but in this case we only consider candidate words whose length matches the length specified in the clue. We compare with the baseline of elementwise addition of Word2Vec vectors in the embedding space (we discard the ineffective W2V mult baseline), again restricting candidates to words of the pre-specified length.\nWe also compare to two bespoke online crossword-solving engines. The first, OneAcross (http://www.oneacross.com/) is the candidate generation module of the award-winning Proverb crossword system (Littman et al., 2002). Proverb, which was produced by academic researchers, has featured in national media such as New Scientist, and beaten expert humans in crossword solving tournaments. Our other comparison is with Crossword Maestro (http://www.crosswordmaestro.com/), a commercial crossword solving system that handles both cryptic and non-cryptic crossword clues (we focus only on the non-cryptic setting), and has also been featured in national media.10 We are unable to compare against a third well-known automatic crossword solver, Dr Fill (Ginsberg, 2011), because code for Dr Fill\u2019s candidate-generation module is not readily available. As with the RNN and baseline models, when evaluating existing systems we discard candidates whose length does not match the length specified in the clue.\nCertain principles connect the design of the existing commercial systems and differentiate them from our approach. Unlike the RNN model, they each require query-time access to large databases containing common crossword clues, dictionary definitions, the frequency with which words typically appear as crossword solutions and other hand-engineered and task-specific components (Littman et al., 2002; Ginsberg, 2011).\n10See e.g. http://www.theguardian.com/crosswords/crossword-"}, {"heading": "4.3 Results", "text": "The performance of models on the various question types is presented in Table 6. On the long questions, the RNN models place the correct answer in the top ten candidates for over half of the questions, and in the top 100 candidates almost 80% of the time, clearly outperforming the baseline and commercial systems. Their responses are also more consistent (in terms of rank variance) than the W2V baseline. When evaluating the two commercial systems, One Across and Crossword Maestro, we have access to web interfaces that return up to approximately 100 candidates for each query, so can only reliably record membership of the top ten (accuracy@10). On this metric, One Across beats Crossword Maestro on the long questions, but the RNN model outperforms both commercial systems.\nInterestingly, as the questions get shorter, the advantage of the RNN model diminishes. Both the Word2Vec baseline and the commercial systems answer the short questions more accurately than the RNN model, and generally produce more consistent sets of candidate responses. One obvi-\nous reason for this effect is the clear difference in form and style between these shorter clues are the full definitions or encyclopedia sentences in the RNN training data. As the length of the clue decreases, finding the answer often reduces to generating synonyms (culpability - guilt), or category members (tall animal - giraffe). Word2Vec representations are known to encode these sorts of relationships (even after elementwise addition) (Mikolov et al., 2013), and seem particularly powerful in this case as the nearest neighbour search is constrained by a specified word length. The commercial systems also retrieve good candidates for such clues among their databases of entities, relationships and common crossword answers.\nTo produce an \u2018optimal\u2019 neural crossword solver, which stores all knowledge in weights and embeddings and is good at both long and short questions, the RNN model and Word2Vec baseline can be easily combined. We simply let the length of the clue (in words) determine how to generate candidates. This architecture requires no more memory than the RNN model, since that already stores all Word2Vec embeddings. The RNN-W2V\nrow in Table 6 shows the performance of a model in which the RNN is used for questions of length > 3 words and the W2V add model is used otherwise. This composite model outperforms the commercial systems on questions of any length."}, {"heading": "4.4 Qualitative Analysis", "text": "A better understanding of how the different models arrive at their answers can be gained from considering specific examples, as presented in Table 7. The first three examples show that, despite the apparently superficial nature of its training data (definitions and introductory sentences) the RNN model can answer questions that require factual knowledge about people and places. Another notable characteristic of the RNN model is the consistent semantic appropriateness of the candidate set. In the first case, the top five candidates are all mountains, valleys or places in the Alps; in the second, they are all biblical names, and in the third, four of the five are currencies. None of the alternative approaches exhibits this \u2018smoothness\u2019 or consistency in candidate generation. Despite its simplicity, the W2V add method is at times surprisingly effective, as shown by the fact it returns Joshua in its top candidates for the third query.\nThe final example in Table 7 highlights a limitation of the RNN approach in its current form. In this specific case, although there is an embedding in the target space for Schoenberg, there are no corresponding definitions or articles in the training data. The RNN model is not able to infer the connection between Schoenberg and the (comparatively infrequent) notion of atonality. It seems likely that a model trained only on definitions or introductory passages would always struggle to learn secondary properties of concepts, such as the name of Obama\u2019s second daughter. The existing systems, which may store these characteristics or relations explicitly in their databases, seem to be better at these sorts of questions.\nMore generally, it is an open question whether the world knowledge required for open QA could be encoded and retained as weights in a (larger) dynamic network, or whether it will be necessary to combine the RNN with an external memory that is less frequently (or never) updated. This latter approach has begun to achieve impressive results on certain QA and entailment tasks (Bordes et al., 2014; Graves et al., 2014; Weston et al., 2015)."}, {"heading": "5 Conclusion", "text": "Dictionaries exist in many of the world\u2019s languages. We have shown how these lexical resources can be a valuable resource for training the latest neural language models to interpret and represent the meaning of phrases and sentences. While humans use the phrasal definitions in dictionaries to better understand the meaning of words, machines can use the words to better understand the phrases. We presented an recurrent neural network architecture with a long-short-term memory to explicitly exploit this idea.\nOn the reverse dictionary task that mirrors its training setting, the RNN performs comparably to the best known commercial applications despite having access to many fewer definitions. Moreover, it generates smoother sets of candidates, uses less memory at query time and, perhaps most significantly, requires no linguistic pre-processing or task-specific engineering. We also showed how the description-to-word objective can be used to train models useful for other tasks. The architecture trained additionally on an encyclopedia performs well as a crossword question answerer, outperforming commercial systems on questions containing more than four words. While our QA experiments focused on a particular question type, the results suggest that a similar neural-languagemodel approach may ultimately lead to improved output from more general QA and dialog systems and information retrieval engines in general.\nWe make all code, training data, evaluation sets and both of our linguistic tools publicly available online for future research. In particular, we propose the reverse dictionary task as a comparatively general-purpose and objective way of evaluating how well models compose lexical meaning into phrase or sentence representations (whether or not they involve training on definitions directly).\nIn the next stage of this research, we will explore ways to enhance the RNN model, especially in the question-answering context. The model is currently not trained on any question-like language, and would conceivably improve on exposure to such linguistic forms. Compared to stateof-the-art word representation learning models, it actually sees very few words during training, and may also benefit from learning from both dictionaries and unstructured text. Finally, we intend to explore ways to endow the model with richer world knowledge. This may require the integra-\ntion of an external memory module, similar to the promising approaches proposed in several recent papers (Graves et al., 2014; Weston et al., 2015)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Semantic parsing via paraphrasing", "author": ["J. Berant", "P. Liang."], "venue": "Proceedings of the Association for Computational Linguistics (ACL).", "citeRegEx": "Berant and Liang.,? 2014", "shortCiteRegEx": "Berant and Liang.", "year": 2014}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio."], "venue": "Proceedings", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Improving dictionary accessibility by maximizing use of available knowledge", "author": ["Slaven Bilac", "Timothy Baldwin", "Hozumi Tanaka."], "venue": "Traitement automatique des langues, 44(2):199\u2013224.", "citeRegEx": "Bilac et al\\.,? 2003", "shortCiteRegEx": "Bilac et al\\.", "year": 2003}, {"title": "Dictionary search based on the target word description", "author": ["Slaven Bilac", "Wataru Watanabe", "Taiichi Hashimoto", "Takenobu Tokunaga", "Hozumi Tanaka."], "venue": "Proc. of the Tenth Annual Meeting of The Association for NLP (NLP2004), pages 556\u2013559.", "citeRegEx": "Bilac et al\\.,? 2004", "shortCiteRegEx": "Bilac et al\\.", "year": 2004}, {"title": "Question answering with subgraph embeddings", "author": ["Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Bordes et al\\.,? 2014", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Building watson: An overview of the deepqa project", "author": ["David Ferrucci", "Eric Brown", "Jennifer Chu-Carroll", "James Fan", "David Gondek", "Aditya A Kalyanpur", "Adam Lally", "J William Murdock", "Eric Nyberg", "John Prager"], "venue": "AI magazine,", "citeRegEx": "Ferrucci et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ferrucci et al\\.", "year": 2010}, {"title": "Dr", "author": ["Matthew L Ginsberg."], "venue": "fill: crosswords and an implemented solver for singly weighted csps. Journal of Artificial Intelligence Research, pages 851\u2013 886.", "citeRegEx": "Ginsberg.,? 2011", "shortCiteRegEx": "Ginsberg.", "year": 2011}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["Stephan Gouws", "Yoshua Bengio", "Greg Corrado."], "venue": "Proceedings of NIPS Deep Learning Workshop.", "citeRegEx": "Gouws et al\\.,? 2014", "shortCiteRegEx": "Gouws et al\\.", "year": 2014}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka."], "venue": "arXiv preprint arXiv:1410.5401.", "citeRegEx": "Graves et al\\.,? 2014", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Multilingual distributed representations without word alignment", "author": ["Karl Moritz Hermann", "Phil Blunsom."], "venue": "arXiv preprint arXiv:1312.6173.", "citeRegEx": "Hermann and Blunsom.,? 2013", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daum\u00e9 III."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Iyyer et al\\.,? 2014", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S Zemel."], "venue": "arXiv preprint arXiv:1411.2539.", "citeRegEx": "Kiros et al\\.,? 2014", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Stanislas Lauly", "Hugo Larochelle", "Mitesh Khapra", "Balaraman Ravindran", "Vikas C Raykar", "Amrita Saha."], "venue": "Advances in Neural Information Processing Systems, pages 1853\u2013", "citeRegEx": "Lauly et al\\.,? 2014", "shortCiteRegEx": "Lauly et al\\.", "year": 2014}, {"title": "Claws4: the tagging of the british national corpus", "author": ["Geoffrey Leech", "Roger Garside", "Michael Bryant."], "venue": "Proceedings of the 15th conference on Computational linguistics-Volume 1, pages 622\u2013 628. Association for Computational Linguistics.", "citeRegEx": "Leech et al\\.,? 1994", "shortCiteRegEx": "Leech et al\\.", "year": 1994}, {"title": "A probabilistic approach to solving crossword puzzles", "author": ["Michael L Littman", "Greg A Keim", "Noam Shazeer."], "venue": "Artificial Intelligence, 134(1):23\u201355.", "citeRegEx": "Littman et al\\.,? 2002", "shortCiteRegEx": "Littman et al\\.", "year": 2002}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Evaluating neural word representations in tensor-based compositional settings", "author": ["Dmitrijs Milajevs", "Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Matthew Purver"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Milajevs et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Milajevs et al\\.", "year": 2014}, {"title": "Question answering in restricted domains: An overview", "author": ["Diego Moll\u00e1", "Jos\u00e9 Luis Vicedo."], "venue": "Computational Linguistics, 33(1):41\u201361.", "citeRegEx": "Moll\u00e1 and Vicedo.,? 2007", "shortCiteRegEx": "Moll\u00e1 and Vicedo.", "year": 2007}, {"title": "A local learning algorithm for dynamic feedforward and recurrent networks", "author": ["Jurgen Schmidhuber."], "venue": "Connection Science, 1(4):403\u2013412.", "citeRegEx": "Schmidhuber.,? 1989", "shortCiteRegEx": "Schmidhuber.", "year": 1989}, {"title": "Building a scalable databasedriven reverse dictionary", "author": ["Ryan Shaw", "Anindya Datta", "Debra VanderMeer", "Kaushik Dutta."], "venue": "Knowledge and Data Engineering, IEEE Transactions on, 25(3):528\u2013540.", "citeRegEx": "Shaw et al\\.,? 2013", "shortCiteRegEx": "Shaw et al\\.", "year": 2013}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1502.05698.", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Word lookup on the basis of associations: from an idea to a roadmap", "author": ["Michael Zock", "Slaven Bilac."], "venue": "Proceedings of the Workshop on Enhancing and Using Electronic Dictionaries, pages 29\u201335. Association for Computational Linguistics.", "citeRegEx": "Zock and Bilac.,? 2004", "shortCiteRegEx": "Zock and Bilac.", "year": 2004}], "referenceMentions": [{"referenceID": 21, "context": "For this purpose we use a recurrent neural network (RNN) (Schmidhuber, 1989) with long-short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997).", "startOffset": 57, "endOffset": 76}, {"referenceID": 11, "context": "For this purpose we use a recurrent neural network (RNN) (Schmidhuber, 1989) with long-short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997).", "startOffset": 112, "endOffset": 146}, {"referenceID": 18, "context": "Prior to training the RNN, we learn its target lexical representations by training the Word2Vec software (Mikolov et al., 2013) on billions of words of raw text.", "startOffset": 105, "endOffset": 127}, {"referenceID": 25, "context": "tionary or concept finder: a system that returns words based on user descriptions or definitions (Zock and Bilac, 2004).", "startOffset": 97, "endOffset": 119}, {"referenceID": 8, "context": "Moreover, thanks to recent work on multilingual embedding spaces (Gouws et al., 2014), we show that", "startOffset": 65, "endOffset": 85}, {"referenceID": 17, "context": "RNNs (with LSTMs) have achieved state-of-the-art performance in language modelling (Mikolov et al., 2010) image caption generation (Kiros et al.", "startOffset": 83, "endOffset": 105}, {"referenceID": 13, "context": ", 2010) image caption generation (Kiros et al., 2014), approach state-of-the-art performance in machine translation (Bahdanau et al.", "startOffset": 33, "endOffset": 53}, {"referenceID": 0, "context": ", 2014), approach state-of-the-art performance in machine translation (Bahdanau et al., 2015).", "startOffset": 70, "endOffset": 93}, {"referenceID": 18, "context": "The target word embeddings are learned independently of the RNN weights, using the Word2Vec software (Mikolov et al., 2013).", "startOffset": 101, "endOffset": 123}, {"referenceID": 2, "context": "The model was implemented with Theano (Bergstra et al., 2010) and trained with minibatch SGD on GPUs.", "startOffset": 38, "endOffset": 61}, {"referenceID": 24, "context": "The batch size was fixed at 16 and the learning rate was controlled by adadelta (Zeiler, 2012).", "startOffset": 80, "endOffset": 94}, {"referenceID": 3, "context": "These generally rely on common techniques from information retrieval, comparing definitions in their internal database to the input query, and returning the word whose definition is \u2018closest\u2019 to the input query (Bilac et al., 2003; Bilac et al., 2004; Zock and Bilac, 2004).", "startOffset": 211, "endOffset": 273}, {"referenceID": 4, "context": "These generally rely on common techniques from information retrieval, comparing definitions in their internal database to the input query, and returning the word whose definition is \u2018closest\u2019 to the input query (Bilac et al., 2003; Bilac et al., 2004; Zock and Bilac, 2004).", "startOffset": 211, "endOffset": 273}, {"referenceID": 25, "context": "These generally rely on common techniques from information retrieval, comparing definitions in their internal database to the input query, and returning the word whose definition is \u2018closest\u2019 to the input query (Bilac et al., 2003; Bilac et al., 2004; Zock and Bilac, 2004).", "startOffset": 211, "endOffset": 273}, {"referenceID": 3, "context": "These generally rely on common techniques from information retrieval, comparing definitions in their internal database to the input query, and returning the word whose definition is \u2018closest\u2019 to the input query (Bilac et al., 2003; Bilac et al., 2004; Zock and Bilac, 2004). Proximity is quantified differently in each case, but is generally a function of hand-engineered features of the two sentences. For instance, Shaw et al. (2013) propose a method in which the candidates for a given input query are all words in the model\u2019s database whose definitions contain one or more words from the query.", "startOffset": 212, "endOffset": 436}, {"referenceID": 22, "context": "In the only previous academic research on English reverse dictionaries that we are aware of, evaluation was conducted on 300 word-definition pairs written by lexicographers, but which are not publicly available (Shaw et al., 2013).", "startOffset": 211, "endOffset": 230}, {"referenceID": 15, "context": "To do so, we randomly selected 200 adjectives, nouns or verbs from among the top 3000 most frequent tokens in the British National Corpus (Leech et al., 1994) (but outside the top 100).", "startOffset": 138, "endOffset": 158}, {"referenceID": 10, "context": "Bilingual embedding models use bilingual corpora to learn a space of representations of the words in two languages, such that words from either language that have similar meanings are close together (Hermann and Blunsom, 2013; Lauly et al., 2014; Gouws et al., 2014).", "startOffset": 199, "endOffset": 266}, {"referenceID": 14, "context": "Bilingual embedding models use bilingual corpora to learn a space of representations of the words in two languages, such that words from either language that have similar meanings are close together (Hermann and Blunsom, 2013; Lauly et al., 2014; Gouws et al., 2014).", "startOffset": 199, "endOffset": 266}, {"referenceID": 8, "context": "Bilingual embedding models use bilingual corpora to learn a space of representations of the words in two languages, such that words from either language that have similar meanings are close together (Hermann and Blunsom, 2013; Lauly et al., 2014; Gouws et al., 2014).", "startOffset": 199, "endOffset": 266}, {"referenceID": 8, "context": "experiment, we used English-French embeddings learned by the state-of-the-art BilBOWA model (Gouws et al., 2014) from the Wikipedia (monolingual) and Europarl (bilingual) corpora.", "startOffset": 92, "endOffset": 112}, {"referenceID": 6, "context": "teams of researchers, modular design and powerful infrastructure, exemplified by IBM\u2019s Watson (Ferrucci et al., 2010).", "startOffset": 94, "endOffset": 117}, {"referenceID": 20, "context": "This has been achieved by restricting questions to a specific topic or domain (Moll\u00e1 and Vicedo, 2007), allowing systems access to pre-specified passages of text from which the answer can be inferred (Iyyer et al.", "startOffset": 78, "endOffset": 102}, {"referenceID": 12, "context": "This has been achieved by restricting questions to a specific topic or domain (Moll\u00e1 and Vicedo, 2007), allowing systems access to pre-specified passages of text from which the answer can be inferred (Iyyer et al., 2014; Weston et al., 2015), or centering both questions and answers on a particular knowledge base (Berant and Liang, 2014; Bordes et al.", "startOffset": 200, "endOffset": 241}, {"referenceID": 23, "context": "This has been achieved by restricting questions to a specific topic or domain (Moll\u00e1 and Vicedo, 2007), allowing systems access to pre-specified passages of text from which the answer can be inferred (Iyyer et al., 2014; Weston et al., 2015), or centering both questions and answers on a particular knowledge base (Berant and Liang, 2014; Bordes et al.", "startOffset": 200, "endOffset": 241}, {"referenceID": 1, "context": ", 2015), or centering both questions and answers on a particular knowledge base (Berant and Liang, 2014; Bordes et al., 2014).", "startOffset": 80, "endOffset": 125}, {"referenceID": 5, "context": ", 2015), or centering both questions and answers on a particular knowledge base (Berant and Liang, 2014; Bordes et al., 2014).", "startOffset": 80, "endOffset": 125}, {"referenceID": 16, "context": "As our interest is in the language understanding, we do not address the question of fitting answers into a grid, which is the main concern of end-to-end automated crossword solvers (Littman et al., 2002).", "startOffset": 181, "endOffset": 203}, {"referenceID": 16, "context": "com/) is the candidate generation module of the award-winning Proverb crossword system (Littman et al., 2002).", "startOffset": 87, "endOffset": 109}, {"referenceID": 7, "context": "10 We are unable to compare against a third well-known automatic crossword solver, Dr Fill (Ginsberg, 2011), because code for Dr Fill\u2019s candidate-generation module is not readily available.", "startOffset": 91, "endOffset": 107}, {"referenceID": 16, "context": "nents (Littman et al., 2002; Ginsberg, 2011).", "startOffset": 6, "endOffset": 44}, {"referenceID": 7, "context": "nents (Littman et al., 2002; Ginsberg, 2011).", "startOffset": 6, "endOffset": 44}, {"referenceID": 18, "context": "representations are known to encode these sorts of relationships (even after elementwise addition) (Mikolov et al., 2013), and seem particularly powerful in this case as the nearest neighbour search is constrained by a specified word length.", "startOffset": 99, "endOffset": 121}, {"referenceID": 5, "context": "gun to achieve impressive results on certain QA and entailment tasks (Bordes et al., 2014; Graves et al., 2014; Weston et al., 2015).", "startOffset": 69, "endOffset": 132}, {"referenceID": 9, "context": "gun to achieve impressive results on certain QA and entailment tasks (Bordes et al., 2014; Graves et al., 2014; Weston et al., 2015).", "startOffset": 69, "endOffset": 132}, {"referenceID": 23, "context": "gun to achieve impressive results on certain QA and entailment tasks (Bordes et al., 2014; Graves et al., 2014; Weston et al., 2015).", "startOffset": 69, "endOffset": 132}, {"referenceID": 9, "context": "tion of an external memory module, similar to the promising approaches proposed in several recent papers (Graves et al., 2014; Weston et al., 2015).", "startOffset": 105, "endOffset": 147}, {"referenceID": 23, "context": "tion of an external memory module, similar to the promising approaches proposed in several recent papers (Graves et al., 2014; Weston et al., 2015).", "startOffset": 105, "endOffset": 147}], "year": 2015, "abstractText": "Distributional models that learn rich semantic word representations are a success story of recent NLP research. However, developing models that learn useful representations of phrases and sentences has proved far harder. We propose using the definitions found in everyday dictionaries as a means of bridging this gap between lexical and phrasal semantics. We train a recurrent neural network (RNN) to map dictionary definitions (phrases) to (lexical) representations of the words those definitions define. We present two applications of this architecture: a reverse dictionary, for returning the name of a concept given a definition or description, and a general-knowledge (crossword) question answerer. On both tasks, the RNN trained on definitions from a handful of freelyavailable lexical resources performs comparably or better than existing commercial systems that rely on major task-specific engineering and far greater memory footprints. This strong performance highlights the general effectiveness of both neural language models and definitionbased training for training machines to understand phrases and sentences.", "creator": "LaTeX with hyperref package"}}}