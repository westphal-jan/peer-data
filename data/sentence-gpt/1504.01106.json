{"id": "1504.01106", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Apr-2015", "title": "Discriminative Neural Sentence Modeling by Tree-Based Convolution", "abstract": "This paper proposes a new convolutional neural architecture based on tree-structures, called the tree-based convolutional neural network (TBCNN). Two variants take advantage of constituency trees and dependency trees, respectively, to model sentences. Compared with traditional \"flat\" convolutional neural networks (CNNs), TBCNNs explore explicitly the structural information of sentences; compared with recursive neural networks, TBCNNs have much shorter propagation paths, enabling more effective feature learning and extraction. In the experiment of sentiment analysis, our two models consistently outperform CNNs and RNNs in a controlled setting. Our models are also competitive to most state-of-the-art results, including RNNs based on long short term memory and deep CNNs/RNNs. We propose a new convolutional neural network (CTNN) based on tree-structures, called the tree-based convolutional neural network (TBCNN). Two variants take advantage of constituency trees and dependency trees, respectively, to model sentences. Compared with traditional \"flat\" convolutional neural networks (CNNs), TBCNNs explore explicitly the structural information of sentences; compared with recursive neural networks, TBCNNs have much shorter propagation paths, enabling more effective feature learning and extraction. In the experiment of sentiment analysis, our two models consistently outperform CNNs and RNNs in a controlled setting. Our models are also competitive to most state-of-the-art results, including RNNs based on long short term memory and deep CNNs/RNNs. We propose a new convolutional neural network (CTNN) based on tree-structures, called the tree-based convolutional neural network (TBCNN). Two variants take advantage of constituency trees and dependency trees, respectively, to model sentences. Compared with traditional \"flat\" convolutional neural networks (CNNs), TBCNNs explore explicitly the structural information of sentences; compared with recursive neural networks, TBCNNs have much shorter propagation paths, enabling more effective feature learning and extraction. In the experiment of sentiment analysis, our two models consistently outperform CNNs and RNNs in a controlled setting. Our models are also competitive to most state-of-the-art results, including RNNs based on long short term memory and deep CNNs/RNNs. We propose a new convolutional neural network (CTNN) based on tree-structures, called the tree-based convolutional neural", "histories": [["v1", "Sun, 5 Apr 2015 10:18:32 GMT  (289kb,D)", "http://arxiv.org/abs/1504.01106v1", null], ["v2", "Tue, 7 Apr 2015 07:30:08 GMT  (289kb,D)", "http://arxiv.org/abs/1504.01106v2", null], ["v3", "Thu, 23 Apr 2015 17:16:32 GMT  (289kb,D)", "http://arxiv.org/abs/1504.01106v3", null], ["v4", "Mon, 1 Jun 2015 12:23:16 GMT  (409kb,D)", "http://arxiv.org/abs/1504.01106v4", null], ["v5", "Tue, 2 Jun 2015 05:56:06 GMT  (409kb,D)", "http://arxiv.org/abs/1504.01106v5", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["lili mou", "hao peng", "ge li", "yan xu", "lu zhang 0023", "zhi jin"], "accepted": true, "id": "1504.01106"}, "pdf": {"name": "1504.01106.pdf", "metadata": {"source": "CRF", "title": "Tree-based Convolution: A New Architecture for Sentence Modeling", "authors": ["Lili Mou", "Hao Peng", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin"], "emails": ["zhijin}@sei.pku.edu.cn", "penghao.pku@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Recent years have witnessed an increasing trend in neural natural language processing (NLP). (Bengio et al., 2003; Mikolov et al., 2013) proposed unsupervised approaches to learn word embeddings, mapping discrete words to real-valued vectors and enabling neural networks to capture word meanings. (Collobert and Weston, 2008) proposed a unified convolutional model on sentences, applying it to a variety of tasks, e.g., part-of-speech tagging, named entity recognition, semantic role labeling, etc. (Socher et al., 2011) proposed a class of recursive neural networks, outperforming traditional machine leaning algorithms by nearly 10% in a sentiment analysis task.\n\u2217 These authors contribute equally to this paper. \u2020 Corresponding author.\nSentence modeling aims to capture sentence meanings, particularly important to various supervised classification tasks of interest. In convolutional models like (Collobert et al., 2011; Blunsom et al., 2014; Hu et al., 2014), a fixed-size window slides over time (successive words in sequence) to extract local features of a sentence; then they pool these features to a vector\u2014usually taking the maximum value in each dimension\u2014for supervised learning. Such models do not consider inherent sentence structures and fail to model semantically related words that fall apart.\nRecursive neural networks (RNNs), on the other hand, take advantage of the sentence\u2019s parsing trees (Socher et al., 2011; Socher et al., 2013a; Hermann and Blunsom, 2013). Each node in the tree is represented distributedly as a vector; information is propagated recursively along the tree by some elaborate semantic composition (Blacoe and Lapata, 2012); finally, the root information is used for supervised learning. Although RNNs encode prior knowledge on tree structures into networks to some extent, they may have difficulty in learning deep dependencies because of long propagation paths (Erhan et al., 2009). The root node, which the classification is based on, then becomes the bottleneck of such models.\nTo explore tree structural information effectively with short paths, we propose a novel neural architecture, called the Tree-based Convolutional Neural Network (TBCNN). Two variants are built upon constituency trees and dependency trees, denoted as c-TBCNN and d-TBCNN, respectively. Our models integrate convolution with tree structures. The idea is to apply a fixed-depth subtree sliding over the entire parsing tree of a sentence, serving as a feature extractor; then these features are pooled to one or more vectors for further processing. One virtue of our model is that all features along the tree have short propagation paths to the output layer, and hence structural information can ar X\niv :1\n50 4.\n01 10\n6v 1\n[ cs\n.C L\n] 5\nA pr\n2 01\n5\nbe learned effectively and efficiently by tree-based convolution.\nOur models are evaluated by a sentiment analysis benchmark. TBCNNs outperform the traditional CNN model, a variety of RNN models, including vector/matrix-vector/tensor interactions, and also one implementation of RNN with long short term memory (LSTM). Currently, our accuracy is slightly lower than the most state-of-theart results by a deep RNN in (Irsoy and Cardie, 2014), two implementations of LSTM RNN (Tai et al., 2015; Le and Zuidema, 2015). But the results are competitive. Further, TBCNN models are much easier to train compared to the deep RNN (5 epochs versus 200 epochs); also our models use much simpler interaction compared with LSTM. Based on the above facts, we think TBCNN models are promising; the accuracy may be further improved with more elaborate design interaction, e.g., bilinear/tensor interactions,etc."}, {"heading": "2 Background and Related Work", "text": "In this section, we present the background and related work regarding neural architectures of sentence modeling."}, {"heading": "2.1 Convolutional Neural Networks", "text": "Convolutional neural networks (CNNs), first proposed for image processing (LeCun et al., 1995), turn out to work with natural languages as well. Figure 1 (a) presents the architecture of the classical convolution process on a sentence (Collobert and Weston, 2008). A fixed window, called convolution kernel, slides over the sentence, and outputs the features extracted. Let t be the window size, and x1, \u00b7 \u00b7 \u00b7 ,xt \u2208 Rn be n-dimensional column vectors corresponding to the words in the current window. The output of convolution, evaluated at\nthe current position, is\ny = f (W \u00b7 [x1; \u00b7 \u00b7 \u00b7 ;xt] + b)\nwhere y \u2208 Rm (m is the number of convolution kernels), W \u2208 Rm\u00d7tn, b \u2208 Rm are parameters, and f is the activation function. Semicolons represent column vector concatenation.\nOne layer of convolution can extract neighboring information effectively. But the features are \u201clocal,\u201d because words that are not in one convolution window do not interact even though they may be semantically related. (Blunsom et al., 2014) builds deep convolutional models so that local features can mix at high-level layers. Similar deep CNN architectures include (Kim, 2014; Hu et al., 2014). All these models are \u201cflat,\u201d by which we mean no structural information is used explicitly. As evidence in the literature shows, CNNs are particularly suitable for noisy and short texts, such as twitter or microblogs (Kim, 2014)."}, {"heading": "2.2 Recursive Neural Networks", "text": "Recursive neural networks (RNNs) were proposed in (Socher et al., 2011) to model sentences. In the original version, each node in the tree structure has a distributed, real-valued representation. Let node p be the parent of c1 and c2 in a constituency parsing tree, the vector of which are denoted as p, c1, c2 \u2208 Rn (n is the dimension of embeddings). Then the parent\u2019s representation is composited by its children\u2019s according to the following equation.\np = f(W [c1; c2] + b) (1)\nwhere W \u2208 Rn\u00d72n and b \u2208 Rn are model parameters. Such process, depicted in Figure 1 (b), is done recursively along the tree; the root vector is then used for supervised classification, e.g.,\nsentiment analysis. Dependency trees and combinatory categorical grammar can also be used as RNN structures (Socher et al., 2013a; Hermann and Blunsom, 2013). Deep RNNs are built in (Irsoy and Cardie, 2014) to enhance information interaction. Improvement for semantic compositionality in RNNs include matrix-vector interaction (Socher et al., 2012), tensor interaction (Socher et al., 2013b), etc. These methods are more suitable of capturing the nature of sentences regarding logic expressions like exclamation, negation, etc.\nOne major drawback of RNNs is the long propagation path of information near leaf nodes. As gradient may vanish when propagated through a deep path, such long dependency buries illuminating information under a complicated neural architecture, leading to the difficulty of training. As reported in (Mou et al., 2014a), RNNs may not be effective for training very deep tree structures, such as program source code\u2019s parsing trees. Long short term memory (LSTM), first proposed for modeling temporal data (Hochreiter and Schmidhuber, 1997), is integrated to RNNs to alleviate this problem (Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015).\nA degraded variant of RNNs is the recurrent network (Bengio et al., 1994), whose architecture can be viewed as a right-most tree. As meaningful tree structures are not used, they are also particularly suitable for modeling short or noisy texts, similar to CNNs (Shang et al., 2015).\nDifferent from the above approaches, our proposed c-TBCNN and d-TBCNN models utilize tree structures explicitly in a novel way by convolving a set of subtree feature detectors, which can be learned effectively because of short dependency paths. TBCNN was first proposed in our previous work for programming language processing (Mou et al., 2014b; Mou et al., 2014a). In this paper, we propose two variants, called c-TBCNN and d-TBCNN, extending tree-based convolution to natural languages."}, {"heading": "3 Tree-based Convolution", "text": "In this section, we propose the novel tree-based convolutional neural network (TBCNN) for sentence modeling. Figure 1 (c) depicts the tree convolution process. The dashed triangle represents a convolution kernel, extracting structural features along the tree. Then, the extracted features are packed into a fixed-size vector by max-pooling, allowing short propagation path between the output\nlayer and any position in the tree. The tree-based convolution, along with the pooling method, allows effective structural feature learning.\nIn the rest of this section, we describe two variants of TBCNN, built up on constituency trees (Section 3.1) and dependency trees (Section 3.2). Several heuristics are introduced in Section 3.3; the training objective is presented in Section 3.4."}, {"heading": "3.1 c-TBCNN", "text": "Figure 2 illustrates an example of the constituency tree, corresponding to the sentence \u201cI like it.\u201d (See the blue nodes on the left-hand side.) Leaf nodes in a constituency tree are words in the sentence; non-leaf nodes represent a grammatical constituent, e.g., a noun phrase, a verbal phrase, etc. In our experiment, sentences are parsed by Stanford parser1; further, the constituency trees are binarized for simplicity.\nTo process the constituency tree effectively by convolution, both leaf nodes and non-leaf nodes should be represented as real-valued vectors in the same semantic space. In our c-TBCNN model, leaf nodes have pretrained ne-dimensional embeddings by unsupervised algorithms (Mikolov et al., 2013); non-leaf nodes are coded by Equation 1, similar to the RNN in (Socher et al., 2011).2\nWe now consider a constituency tree-based convolution with two-layer kernels. If we have nc feature detectors for convolution, the output\u2014 evaluated at a local position of subtree p \u2190 (c1, c2) with vector representations p, c1, and c2\u2014is\ny = f ( W (c)p \u00b7 p+W (c) l \u00b7 c1 +W (c) r \u00b7 c2 + b(c) ) where y \u2208 Rnc ; b \u2208 Rc is the bias term. W (c) p ,W (l) p ,W (r) p \u2208 Rnc\u00d7ne are weights associated with parent p, left child c1, and right child c2, respectively. The superscript (c) indicates the weights in the c-TBCNN model. For leaf nodes that do not have enough depth for the convolution window, we set c1 and c2 to be 0. Like other convolution processes, such as (Collobert and Weston, 2008), the convolution weights are shared among different positions along the tree structure.\nTree-based convolution kernels can be extended to arbitrary numbers of layers straightforwardly.\n1 http://nlp.stanford.edu/software/lex-parser.shtml 2It should be mentioned that the perception-like interaction is not the only choice. Although it has already exhibited its power in tree convolutional architecture, other interactions can also be exploited further.\nThe complexity is exponential with respect to the depth of kernels, but it is linear to the number of nodes we convolve at a time. Hence, treebased convolution, compared with \u201cflat\u201d CNNs, does not add to computational cost, provided the same amount of information to process at a time. In our experiment, we use 2-layer kernels."}, {"heading": "3.2 d-TBCNN", "text": "Dependency trees are another representation of sentence structures. Each node in dependency trees corresponds to a word; an edge a \u2192 b indicates a is governed by b in Figure 2 (b). Edges are labeled with grammatical relation types by the parser, e.g., nsubj, (de Marneffe et al., 2006).\nIn our d-TBCNN model, we also apply twolayer convolution kernels to extract structural features. However, in d-TBCNN, different nodes may have different numbers of children. We assign a weight matrix Wr to each relation type r. Rare relation types (less than 3,000 occurrences) are mapped to one shared weight matrix. Let p \u2190 (ci, \u00b7 \u00b7 \u00b7 , cn) be the words and relations in the convolution kernel evaluated a certain position. The output of the feature detectors is\ny = f ( W (d)p \u00b7 p+ n\u2211 i=1 W (d) r[ci] \u00b7 ci + b(d) )\nwhere y \u2208 Rnc ; b(d) \u2208 Rnc is the bias term. W (d) p \u2208 Rnc\u00d7ne is the weight parameter for parent; W (d)r[ci] \u2208 R nc\u00d7ne is the weight for child ci, whose grammatical relation is r[ci]. Superscript (d) indicates the weights are for d-TBCNN.\nBoth c-TBCNN and d-TBCNN have their own advantages: d-TBCNN exploits structural features more efficiently because of the compact expressiveness of dependency trees; c-TBCNN, on the other hand, is more effective in integrating global features due to the underneath coding process."}, {"heading": "3.3 Pooling Heuristics", "text": "As different sentences may have different lengths and tree structures, the extracted features by treebased convolution also vary in size and shape. Dynamic pooling is a prevailing technique for dealing with this problem. We present several heuristics for pooling in both c-TBCNN and d-TBCNN.\n\u2022 Global pooling. All features are pooled to one vector, and we take the maximum value in each dimension. This simple heuristic is applicable to any structure, including cTBCNN and d-TBCNN.\n\u2022 3-way pooling for c-TBCNN. To preserve more information over different regions of constituency trees, we propose 3-way pooling. If a tree has maximum depth d, we pool nodes of less than \u03b1 \u00b7 d layers to a TOP vector (\u03b1 is set to 0.6 in our experiment); lower nodes are pooled to LOWER LEFT or LOWER RIGHT according to their relative position with respect to the root node.\n\u2022 k-way pooling for d-TBCNN. Nodes in dependency trees are one-one corresponding to words in a sentence. Thus, a total order on features extracted by convolution can\nbe defined by their corresponding word order. If we would like to pool the features to k vectors, with indexes 1, 2, \u00b7 \u00b7 \u00b7 , k, we can adopt an \u201cequal allocation\u201d strategy. Let i be the position of a word in a sentence (i = 1, 2, \u00b7 \u00b7 \u00b7 , n). Its feature is pooled to j-th vector, if\n(j \u2212 1) n k \u2264 i \u2264 j n k"}, {"heading": "3.4 Training the Networks", "text": "After pooling, the information is packed into one or more fixed-size vectors, which can be fed forward to a fully-connected hidden layer. Then we apply a softmax layer, predicting the probability of each label, i.e., p(y|x) satisfying p(y|x) satisfies \u2211c i=1 p(yi|x) = 1, where x refers to an input sample and c is the number of classes to predict. The cost function of a sample is the standard cross entropy error\nJ = c\u2211\ni=1\nti log yi\nwhere t \u2208 Rc is the ground truth, with the target label represented as a one-hot vector.\nWe do not apply `2 regularization for weights or embeddings temporarily. Instead, dropout is used, which helps, to some extent, prevent overfitting. Gradients are computed by standard backpropagation; stochastic gradient descent of mini-batch version is used for optimization. Hyperparameter tuning is further discussed in Section 4.3."}, {"heading": "4 Experimental Results", "text": "In Section 4.1, we first describe the task and the dataset used for our model evaluation. We compare c-TBCNN and d-TBCNN with other stateof-the-art results in Section 4.2. Hyperparameter tuning is presented in Section 4.3."}, {"heading": "4.1 The Task and the Dataset", "text": "Our models are evaluated by a widely-used benchmark for sentence modeling. The dataset consists of more than 10,000 movie reviews; our task is to predict reviews\u2019 sentiment among 5 target labels (strongly positive, positive, neutral, negative, and strongly negative). We use the standard split for training, validating and testing, containing 8544, 1101, 2210 sentences, respectively. In the training set, each sentence and phrase is tagged with its sentiment label. We regard phrases as individual samples. For validating and testing, we only\nconsider the sentiment of a complete sentence (the root label).\nBoth c-TBCNN and d-TBCNN use the parsing results of the Stanford parser. A few sentences (or phrases) parsed abnormally are discarded (less than 1%)."}, {"heading": "4.2 Accuracy Compared with other Methods", "text": "Table 1 compares our models to state-of-the-art results in the task of sentiment analysis. As we see, both c-TBCNN and d-TBCNN consistently outperform RNNs of the same interaction by a large extent (5-6%); they also consistently outperform \u201cflat\u201d convolution by more than 10%. These results, in a controlled setting, show that our proposed tree-based convolution, of both variants, can capture tree structural information effectively, which is crucial to sentence modeling.\nFurther, our model also outperforms elaborately designed RNNs with matrix-vector interaction, tensor interaction by 2.5\u20134.7%. For RNNs with long short term memory (LSTM), and deep architectures of RNNs/CNNs, the results are in the same ballpark\u2014our accuracy, for instance, is higher than one implementation of LSTM-based RNN, but slightly less than other 2 implementations.\nIt should be also noticed that our model is much easier to train, compared with deep architectures. (Irsoy and Cardie, 2014) reported 200 epochs for training, whereas it takes typically 5 epochs in our TBCNN models. On the other hand, the interaction in TBCNNs is perception-like, which is a linear combination plus a non-linear activation function, much simpler and more primal than LSTM.\nBased on the above facts, we think the proposed TBCNN models are effective in modeling sentences. The models are also open for improvement with more carefully designed interactions and/or stack of deep architectures."}, {"heading": "4.3 Hyperparameter Tuning", "text": "In this section, we present some details of our hyperparameter tuning.3 Activation function. We tried tanh and ReLU (Nair and Hinton, 2010) as activation functions. We found that ReLU helps optimization, achieving high accuracy in both training and validation sets. One possible reason is that ReLU has larger\n3 As this research is still undergoing, the results may be updated.\nderivatives than tanh and hence, is easier to be trained. Dropout. We tried two settings of dropout: (1) dropout the last hidden layer, and (2) dropout all hidden layers, including convolutional layers. We found that dropout helps prevent overfitting to a large extent in terms of long-term performance during training\u2014the decrease of validation accuracy due to overfitting is much smaller than nondropout networks. The result is conservative: setting (2) is better than setting (1), which is better than no dropout. However, the peak performances do not differ significantly. For the sake of robustness, we adopt the dropout technique (Srivastava et al., 2014) with dropout probability 0.5. Optimization methods. We tried AdaGrad (Duchi et al., 2011) for training. The results turn out to be more stable but slightly lower than high variance optimization methods (by about 1%). Thus, we settled down to stochastic gradient descent with mini-batch."}, {"heading": "4.4 Conclusion and Future Work", "text": "In this paper, we propose a novel tree-based convolutional architecture (TBCNN) for sentence modeling. The model can be built upon either constituency trees or dependency trees. The convolution process makes use of tree structures effectively and efficiently. In a controlled setting, our model outperforms existing \u201cflat\u201d convolutional and recursive neural networks to a large extent. Our models are also competitive to most state-of-\nthe-art results. As this paper first introduces the new TBCNN model, we are empowering the model by designing more meaningful interactions, e.g., bilinear or tensor interaction to model exclamations or negations. We are also trying to stack multiple layers of tree convolution to improve information integration."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["Blacoe", "Lapata2012] William Blacoe", "M. Lapata"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational", "citeRegEx": "Blacoe et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Blacoe et al\\.", "year": 2012}, {"title": "A convolutional neural network for modelling sentences", "author": ["Blunsom et al.2014] Phil Blunsom", "E. Grefenstette", "N. Kalchbrenner"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Blunsom et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Blunsom et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks", "author": ["Collobert", "Weston2008] Ronan Collobert", "J. Weston"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["B. MacCartney", "C. Manning"], "venue": "In Proceedings of Language Resource and Evaluation Conference,", "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "The difficulty of training deep architectures and the effect of unsupervised pre-training", "author": ["Erhan et al.2009] Dumitru Erhan", "P. Manzagol", "Y. Bengio", "S. Bengio", "P. Vincent"], "venue": "In Proceedings of International Conference on Artificial Intelligence", "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "The role of syntax in vector space models of compositional semantics", "author": ["Hermann", "Blunsom2013] Karl Moritz Hermann", "P. Blunsom"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Hermann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu et al.2014] Baotian Hu", "D. Lu", "H. Li", "Q. Chen"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Irsoy", "Cardie2014] Ozan Irsoy", "C. Cardie"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Irsoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Irsoy et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882", "author": ["Yoon Kim"], "venue": null, "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc Le", "T. Mikolov"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Compositional distributional semantics with long short term memory", "author": ["Le", "Zuidema2015] Phong Le", "W. Zuidema"], "venue": "arXiv preprint arXiv:1503.02510", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Comparison of learning algorithms for handwritten digit", "author": ["LeCun et al.1995] Yann LeCun", "L. Jackel", "L. Bottou", "A. Brunot", "C. Cortes", "J. Denker", "H. Drucker", "I. Guyon", "U. Muller", "E. Sackinger"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1995\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1995}, {"title": "Efficient estimation of word representations in vector space", "author": ["K. Chen", "G. Corrado", "J. Dean"], "venue": "In ICLR Workshop", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "TBCNN: A tree-based convolutional neural network for programming language processing", "author": ["Mou et al.2014a] Lili Mou", "G. Li", "Z. Jin", "L. Zhang", "T. Wang"], "venue": "arXiv preprint arXiv:1409.5718", "citeRegEx": "Mou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2014}, {"title": "Building program vector representations for deep learning", "author": ["Mou et al.2014b] Lili Mou", "G. Li", "X. Liu", "H. Peng", "Z. Jin", "Y. Xu", "L. Zhang"], "venue": "arXiv preprint arXiv:1409.3358", "citeRegEx": "Mou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2014}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["Nair", "Hinton2010] Vinod Nair", "G. Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Neural responding machine for short-text conversation", "author": ["Shang et al.2015] Lifeng Shang", "D. Lu", "H. Li"], "venue": "arXiv preprint arXiv:1503.02364", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Semisupervised recursive autoencoders for predicting sentiment distributions", "author": ["J. Pennington", "E. Huang", "A. Ng", "C. Manning"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["B. Huval", "C. Manning", "A. Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Com-", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Q. Le", "C. Manning", "A. Ng"], "venue": "In NIPS Deep Learning Workshop", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["A. Perelygin", "J. Wu", "J. Chuang", "C. Manning", "A. Ng", "C. Potts"], "venue": "In Proceedings of Conference on Empirical Methods in Natural", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075", "author": ["Tai et al.2015] Kai Sheng Tai", "R. Socher", "D. Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Long short-term memory over tree structures. arXiv preprint arXiv:1503.04881", "author": ["Zhu et al.2015] Xiaodan Zhu", "P. Sobhani", "Y. Guo"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "(Bengio et al., 2003; Mikolov et al., 2013) proposed unsupervised approaches to learn word embeddings, mapping discrete words to real-valued vectors and enabling neural networks to capture word meanings.", "startOffset": 0, "endOffset": 43}, {"referenceID": 17, "context": "(Bengio et al., 2003; Mikolov et al., 2013) proposed unsupervised approaches to learn word embeddings, mapping discrete words to real-valued vectors and enabling neural networks to capture word meanings.", "startOffset": 0, "endOffset": 43}, {"referenceID": 22, "context": "(Socher et al., 2011) proposed a class of recursive neural networks, outperforming traditional machine leaning algorithms by nearly 10% in a sentiment analysis task.", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "In convolutional models like (Collobert et al., 2011; Blunsom et al., 2014; Hu et al., 2014), a fixed-size window slides over time (successive words in sequence) to extract local features of a sentence; then they pool these features to a vector\u2014usually taking the maximum value in each dimension\u2014for supervised learning.", "startOffset": 29, "endOffset": 92}, {"referenceID": 3, "context": "In convolutional models like (Collobert et al., 2011; Blunsom et al., 2014; Hu et al., 2014), a fixed-size window slides over time (successive words in sequence) to extract local features of a sentence; then they pool these features to a vector\u2014usually taking the maximum value in each dimension\u2014for supervised learning.", "startOffset": 29, "endOffset": 92}, {"referenceID": 11, "context": "In convolutional models like (Collobert et al., 2011; Blunsom et al., 2014; Hu et al., 2014), a fixed-size window slides over time (successive words in sequence) to extract local features of a sentence; then they pool these features to a vector\u2014usually taking the maximum value in each dimension\u2014for supervised learning.", "startOffset": 29, "endOffset": 92}, {"referenceID": 22, "context": "Recursive neural networks (RNNs), on the other hand, take advantage of the sentence\u2019s parsing trees (Socher et al., 2011; Socher et al., 2013a; Hermann and Blunsom, 2013).", "startOffset": 100, "endOffset": 170}, {"referenceID": 8, "context": "Although RNNs encode prior knowledge on tree structures into networks to some extent, they may have difficulty in learning deep dependencies because of long propagation paths (Erhan et al., 2009).", "startOffset": 175, "endOffset": 195}, {"referenceID": 27, "context": "Currently, our accuracy is slightly lower than the most state-of-theart results by a deep RNN in (Irsoy and Cardie, 2014), two implementations of LSTM RNN (Tai et al., 2015; Le and Zuidema, 2015).", "startOffset": 155, "endOffset": 195}, {"referenceID": 16, "context": "Convolutional neural networks (CNNs), first proposed for image processing (LeCun et al., 1995), turn out to work with natural languages as well.", "startOffset": 74, "endOffset": 94}, {"referenceID": 3, "context": "(Blunsom et al., 2014) builds deep convolutional models so that local features can mix at high-level layers.", "startOffset": 0, "endOffset": 22}, {"referenceID": 13, "context": "Similar deep CNN architectures include (Kim, 2014; Hu et al., 2014).", "startOffset": 39, "endOffset": 67}, {"referenceID": 11, "context": "Similar deep CNN architectures include (Kim, 2014; Hu et al., 2014).", "startOffset": 39, "endOffset": 67}, {"referenceID": 13, "context": "As evidence in the literature shows, CNNs are particularly suitable for noisy and short texts, such as twitter or microblogs (Kim, 2014).", "startOffset": 125, "endOffset": 136}, {"referenceID": 22, "context": "Recursive neural networks (RNNs) were proposed in (Socher et al., 2011) to model sentences.", "startOffset": 50, "endOffset": 71}, {"referenceID": 23, "context": "Improvement for semantic compositionality in RNNs include matrix-vector interaction (Socher et al., 2012), tensor interaction (Socher et al.", "startOffset": 84, "endOffset": 105}, {"referenceID": 27, "context": "Long short term memory (LSTM), first proposed for modeling temporal data (Hochreiter and Schmidhuber, 1997), is integrated to RNNs to alleviate this problem (Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015).", "startOffset": 157, "endOffset": 215}, {"referenceID": 28, "context": "Long short term memory (LSTM), first proposed for modeling temporal data (Hochreiter and Schmidhuber, 1997), is integrated to RNNs to alleviate this problem (Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015).", "startOffset": 157, "endOffset": 215}, {"referenceID": 0, "context": "A degraded variant of RNNs is the recurrent network (Bengio et al., 1994), whose architecture can be viewed as a right-most tree.", "startOffset": 52, "endOffset": 73}, {"referenceID": 21, "context": "As meaningful tree structures are not used, they are also particularly suitable for modeling short or noisy texts, similar to CNNs (Shang et al., 2015).", "startOffset": 131, "endOffset": 151}, {"referenceID": 17, "context": "In our c-TBCNN model, leaf nodes have pretrained ne-dimensional embeddings by unsupervised algorithms (Mikolov et al., 2013); non-leaf nodes are coded by Equation 1, similar to the RNN in (Socher et al.", "startOffset": 102, "endOffset": 124}, {"referenceID": 22, "context": ", 2013); non-leaf nodes are coded by Equation 1, similar to the RNN in (Socher et al., 2011).", "startOffset": 71, "endOffset": 92}, {"referenceID": 3, "context": "4 (Blunsom et al., 2014) Deep CNN 48.", "startOffset": 2, "endOffset": 24}, {"referenceID": 3, "context": "5 (Blunsom et al., 2014)", "startOffset": 2, "endOffset": 24}, {"referenceID": 28, "context": "0 (Zhu et al., 2015) LSTM 49.", "startOffset": 2, "endOffset": 20}, {"referenceID": 27, "context": "6 (Tai et al., 2015) Deep RNN 49.", "startOffset": 2, "endOffset": 20}, {"referenceID": 26, "context": "For the sake of robustness, we adopt the dropout technique (Srivastava et al., 2014) with dropout probability 0.", "startOffset": 59, "endOffset": 84}, {"referenceID": 7, "context": "We tried AdaGrad (Duchi et al., 2011) for training.", "startOffset": 17, "endOffset": 37}], "year": 2017, "abstractText": "This paper proposes a new convolutional neural architecture based on treestructures, called the tree-based convolutional neural network (TBCNN). Two variants take advantage of constituency trees and dependency trees, respectively, to model sentences. Compared with traditional \u201cflat\u201d convolutional neural networks (CNNs), TBCNNs explore explicitly the structural information of sentences; compared with recursive neural networks, TBCNNs have much shorter propagation paths, enabling more effective feature learning and extraction. In the experiment of sentiment analysis, our two models consistently outperform CNNs and RNNs in a controlled setting. Our models are also competitive to most stateof-the-art results, including RNNs based on long short term memory and deep CNNs/RNNs.", "creator": "LaTeX with hyperref package"}}}