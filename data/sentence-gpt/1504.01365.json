{"id": "1504.01365", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2015", "title": "PASSCoDe: Parallel ASynchronous Stochastic dual Co-ordinate Descent", "abstract": "Stochastic Dual Coordinate Descent (SDCD) has become one of the most efficient ways to solve the family of $\\ell_2$-regularized empirical risk minimization problems, including linear SVM, logistic regression, and many others. The vanilla implementation of DCD is quite slow; however, by maintaining primal variables while updating dual variables, the time complexity of SDCD can be significantly reduced. Such a strategy forms the core algorithm in the widely-used LIBLINEAR package. In this paper, we parallelize the SDCD algorithms in LIBLINEAR. In recent research, several synchronized parallel SDCD algorithms have been proposed, however, they fail to achieve good speedup in the shared memory multi-core setting. In this paper, we propose a family of asynchronous stochastic dual coordinate descent algorithms (ASDCD). Each thread repeatedly selects a random dual variable and conducts coordinate updates using the primal variables that are stored in the shared memory. We analyze the convergence properties when different locking/atomic mechanisms are applied. For implementation with atomic operations, we show linear convergence under mild conditions. For implementation without any atomic operations or locking, we present the first {\\it backward error analysis} for ASDCD under the multi-core environment, showing that the converged solution is the exact solution for a primal problem with perturbed regularizer. Experimental results show that our methods are much faster than previous parallel coordinate descent solvers. Our results show that the converged solution is the correct solution for a single sequential problem.\n\n\n\nThis paper is presented as a paper with the following results:\nThe results showed that the sequential parallelization of an algorithm on a single non-conventional network produces a faster and more efficient time than a single parallelized solution.\nIn the following experiments, the parallelization of an algorithm on a single non-conventional network produces a slower and more efficient time than a single parallelized solution.\nFor implementation without atomic operations, we propose a family of asynchronous stochastic dual coordinate descent algorithms (ASDCD). Each thread repeatedly selects a random dual variable and conducts coordinate updates using the primal variables that are stored in the shared memory. We analyze the convergence properties when different locking/atomic mechanisms are applied. For implementation without any atomic operations or locking, we present the first {\\it backward error analysis} for ASDCD under the multi-core environment, showing that the converged solution is the exact solution for a single sequential problem.\nThe results showed that the sequential parallelization of an", "histories": [["v1", "Mon, 6 Apr 2015 19:25:47 GMT  (3084kb,D)", "http://arxiv.org/abs/1504.01365v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["cho-jui hsieh", "hsiang-fu yu", "inderjit s dhillon"], "accepted": true, "id": "1504.01365"}, "pdf": {"name": "1504.01365.pdf", "metadata": {"source": "CRF", "title": "PASSCoDe: Parallel ASynchronous Stochastic dual Co-ordinate Descent", "authors": ["Cho-Jui Hsieh", "Hsiang-Fu Yu", "Inderjit S. Dhillon"], "emails": ["cjhsieh@cs.utexas.edu", "rofuyu@cs.utexas.edu", "inderjit@cs.utexas.edu"], "sections": [{"heading": null, "text": "family of `2-regularized empirical risk minimization problems, including linear SVM, logistic regression, and many others. The vanilla implementation of DCD is quite slow; however, by maintaining primal variables while updating dual variables, the time complexity of DCD can be significantly reduced. Such a strategy forms the core algorithm in the widely-used LIBLINEAR package. In this paper, we parallelize the DCD algorithms in LIBLINEAR. In recent research, several synchronized parallel DCD algorithms have been proposed, however, they fail to achieve good speedup in the shared memory multi-core setting. In this paper, we propose a family of asynchronous stochastic dual coordinate descent algorithms (PASSCoDe). Each thread repeatedly selects a random dual variable and conducts coordinate updates using the primal variables that are stored in the shared memory. We analyze the convergence properties when different locking/atomic mechanisms are applied. For implementation with atomic operations, we show linear convergence under mild conditions. For implementation without any atomic operations or locking, we present the first backward error analysis for PASSCoDe under the multi-core environment, showing that the converged solution is the exact solution for a primal problem with perturbed regularizer. Experimental results show that our methods are much faster than previous parallel coordinate descent solvers."}, {"heading": "1 Introduction", "text": "Given a set of instance-label pairs (x\u0307i, y\u0307i), i = 1, \u00b7 \u00b7 \u00b7 , n, x\u0307i \u2208 Rd, y\u0307i \u2208 R, we focus on the following empirical risk minimization problem with `2-regularization:\nmin w\u2208Rd\nP (w) := 1\n2 \u2016w\u20162 + n\u2211 i=1 `i(w Txi), (1)\nwhere xi = y\u0307ix\u0307i, `i(\u00b7) is the loss function and \u2016\u00b7\u2016 is the 2-norm. A large class of machine learning problems can be formulated as the above optimization problem. Examples include Support Vector Machines (SVMs), logistic regression, ridge regression, and many others. Problem (1) is usually called the primal problem, and can usually be solved by Stochastic Gradient Descent (SGD) (Zhang, 2004; Shalev-Shwartz et al., 2007), second order methods (Lin et al., 2007), or primal coordinate descent algorithms (Chang et al., 2008; Huang et al., 2009).\nInstead of solving the primal problem, another class of algorithms solves the following dual problem of (1):\nmin \u03b1\u2208Rn\nD(\u03b1) := 1\n2 \u2016 n\u2211 i=1 \u03b1ixi\u20162 + n\u2211 i=1 `\u2217i (\u2212\u03b1i), (2)\nar X\niv :1\n50 4.\n01 36\n5v 1\n[ cs\n.L G\n] 6\nA pr\n2 01\nwhere `\u2217i (\u00b7) is the conjugate of the loss function `i(\u00b7), defined by `\u2217i (u) = maxz(zu\u2212 `i(z)). If we define\nw(\u03b1) = \u2211 i=1 \u03b1ixi, (3)\nthen it is known that w(\u03b1\u2217) = w\u2217 and P (w\u2217) = \u2212D(\u03b1\u2217) where w\u2217,\u03b1\u2217 are the optimal primal/dual solutions respectively. Examples include hinge-loss SVM, square hinge SVM and `2-regularized logistic regression.\nStochastic Dual Coordinate Descent (DCD) has become the most widely-used algorithm for solving (2), and it is faster than primal solvers (including SGD) in many large-scale problems. The success of DCD is mainly due to the trick of maintaining the primal variables w based on the primal-dual relationship (3). By maintaining w in memory, Hsieh et al. (2008); Keerthi et al. (2008) showed that the time complexity of each coordinate update can be reduced from O(nnz) to O(nnz/n), where nnz is number of nonzeros in the training dataset. Several DCD algorithms for different machine learning problems are currently implemented in LIBLINEAR (Fan et al., 2008) and they are now widely used in both academia and industry. The success of DCD has also catalyzed a large body of theoretical studies (Nesterov, 2012; Shalev-Shwartz & Zhang, 2013).\nIn this paper, we parallelize the DCD algorithm in a shared memory multicore system. There are two threads of work on parallel coordinate descent. The first thread focuses on synchronized algorithms, including synchronized CD (Richta\u0301rik & Taka\u0301c\u030c, 2012; Bradley et al., 2011) and synchronized DCD algorithms (Yang, 2013; Jaggi et al., 2014). However, choosing the block size is a trade-off problem between communication and convergence speed, so synchronous algorithms usually suffer from slower convergence. To overcome this problem, the other thread of work focuses on asynchronous CD algorithms in multi-core shared memory systems (Liu & Wright, 2014; Liu et al., 2014). However, none of the existing work maintains both the primal and dual variables. As a result, the recent asynchronous CD algorithms end up being much slower than the state-of-the-art serial DCD algorithms that maintain both w and \u03b1, as in the LIBLINEAR software. This leads to a challenging question: how to maintaining both primal and dual in an asynchronous and efficient way?\nIn this paper, we propose the first asynchronous dual coordinate descent (PASSCoDe) algorithms with the address to the issue for the primal variable maintenance in the shared memory multi-core setting. We carefully discuss and analyze three versions of PASSCoDe: PASSCoDe-Lock, PASSCoDe-Atomic, and PASSCoDe-Wild. In PASSCoDe-Lock, convergence is always guaranteed but the overhead for locking makes it even slower than serial DCD. In PASSCoDe-Atomic, the primal-dual relationship (3) is enforced by atomic writes to the shared memory; while PASSCoDe-Wild proceeds without any locking and atomic operations, as a result of which the relationship (3) between primal and dual variables can be violated due to memory conflicts. Our contributions can be summarized below: \u2022 We propose and analyze a family of asynchronous parallelization of the most efficient DCD algorithm:\nPASSCoDe-Lock, PASSCoDe-Atomic, PASSCoDe-Wild. \u2022 We show linear convergence of PASSCoDe-Atomic under certain conditions. \u2022 We present a backward error analysis for PASSCoDe-Wild and show that the converged solution is\nthe exact solution of a primal problem with a perturbed regularizer. Therefore the performance is close-to-optimal on most of the datasets. To best of our knowledge, this is the first attempt to analyze a parallel machine learning algorithm with memory conflicts using backward error analysis, which is a standard tool in numerical analysis (Wilkinson, 1961). \u2022 Experimental results show that our algorithms (PASSCoDe-Atomic and PASSCoDe-Wild) are much\nfaster than existing methods. For example, on the webspam dataset, PASSCoDe-Atomic took 2 sec-\nonds and PASSCoDe-Wild took 1.6 seconds to achieve 99% accuracy, while CoCoA took 11.5 seconds using 10 threads and LIBLINEAR took 10 seconds using 1 thread to achieve the same accuracy."}, {"heading": "2 Related Work", "text": "Stochastic Coordinate Descent. Coordinate descent is a classical optimization technique that has been studied for a long time (Bertsekas, 1999; Luo & Tseng, 1992). Recently it has enjoyed renewed interest due to the success of \u201cstochastic\u201d coordinate descent in real applications (Hsieh et al., 2008; Nesterov, 2012). In terms of theoretical analysis, the convergence of (cyclic) coordinate descent has been studied for a long time (Luo & Tseng, 1992; Bertsekas, 1999), and the global linear convergence is presented recently under certain condition (Saha & Tewari, 2013; Wang & Lin, 2014).\nStochastic Dual Coordinate Descent. Many recent papers (Hsieh et al., 2008; Yu et al., 2011; ShalevShwartz & Zhang, 2013) have shown that solving the dual problem using coordinate descent algorithms is faster on large-scale datasets. The success of SDCD strongly relies on exploiting the primal-dual relationship (3) to speed up the gradient computation in the dual space. DCD has become the state-of-the-art solver implemented in LIBLINEAR (Fan et al., 2008). In terms of convergence of dual objective function, some standard theoretical guarantees for coordinate descent can be directly applied. Different from standard analysis, Shalev-Shwartz & Zhang (2013) presented the convergence rate in terms of duality gap.\nParallel Stochastic Coordinate Descent. In order to conduct coordinate updates in parallel, Richta\u0301rik & Taka\u0301c\u030c (2012) studied the algorithm where each processor updates a randomly selected block (or coordinate) simultaneously, and Bradley et al. (2011) proposed a similar algorithm for `1-regularized problems. Scherrer et al. (2012) studied parallel greedy coordinate descent. However, the above synchronized methods usually face a trade-off in choosing the block size. If the block size is small, the load balancing problem leads to slow running time. If the block size is large, the convergence speed becomes much slower or the algorithm even diverges. These problems can be resolved by developing an asynchronous algorithm. Asynchronous coordinate descent has been studied by (Bertsekas & Tsitsiklis, 1989), but they require the Hessian to be diagonal dominant in order to establish the convergence. Recently, Liu et al. (2014); Liu & Wright (2014) proved linear convergence of asynchronous stochastic coordinate descent algorithms under the essential strong convexity condition and a \u201cbounded staleness\u201d condition, where they consider both \u201cconsistent read\u201d and \u201cinconsistent read\u201d models. Avron et al. (2014) showed linear rate of convergence for the asynchronous randomized Gaussian-Seidel updates, which is a special case of coordinate descent on linear systems.\nParallel Stochastic Dual Coordinate Descent. For solving (5), each coordinate updates only requires the global primal variables w and one local dual variable \u03b1i, thus algorithms only need to synchronize w. Based on this observation, Yang (2013) proposed to update several coordinates or blocks simultaneously and update the globalw, and Jaggi et al. (2014) showed that each block can be solved with other approaches under the same framework. However, both these parallel DCD methods are synchronized algorithms.\nTo the best of our knowledge, this is the first to propose and analyze asynchronous parallel stochastic dual coordinate descent methods. By maintaining a primal solutionw while updating dual variables, our algorithm is much faster than the previous asynchronous coordinate descent methods of (Liu & Wright, 2014; Liu et al., 2014) for solving the dual problem (2). Our algorithms are also faster than synchronized dual coordinate descent methods (Yang, 2013; Jaggi et al., 2014) since the latest values ofw can be accessed by all the threads. In terms of theoretical contribution, the inconsistent read model in (Liu & Wright, 2014) cannot be directly applied to our algorithm because each update on \u03b1i is based on the shared w vector. We further show linear convergence for PASSCoDe-Atomic, and study the properties of the converged solution for the\nwild version of our algorithm (without any locking and atomic operations) using a backward error analysis. Our algorithm has been successfully applied to solve the collaborative ranking problem (Anonymous, 2015)."}, {"heading": "3 Algorithms", "text": ""}, {"heading": "3.1 Stochastic Dual Coordinate Descent", "text": "We first describe the Stochastic Dual Coordinate Descent (DCD) algorithm for solving the dual problem (2). At each iteration, DCD randomly picks a dual variable \u03b1i and updates it by minimizing the one variable subproblem (Eq. (4) in Algorithm 1). Without exploiting the structure of the quadratic term, the subproblems require substantial computation (need O(nnz) time), where nnz is the total number of nonzero elements in the training data. However, ifw(\u03b1) that satisfies (3) is maintained in memory, the subproblem D(\u03b1+ \u03b4ei) can be written as\nD(\u03b1+ \u03b4ei) = 1 2 \u2016w + \u03b4xi\u20162 + `\u2217i (\u2212(\u03b1i + \u03b4)),\nand the optimal solution can be computed by\n\u03b4 = arg min \u03b4\n1 2 (\u03b4 + wTxi \u2016xi\u20162 )2 + 1 \u2016xi\u20162 `\u2217i (\u2212(\u03b1i + \u03b4)).\nNote that all \u2016xi\u2016 can be pre-computed and are constants. For each coordinate update we only need to solve a simple one-variable subproblem, and the main computation is in computing wTxi, which requires O(nnz/n) time. For SVM problems, the subproblem has a closed form solution, while for logistic regression problems it has to be solved by an iterative solver (see Yu et al. (2012) for details). The DCD algorithm, which is part of the popular LIBLINEAR package, is described in Algorithm 1.\nAlgorithm 1 Stochastic Dual Coordinate Descent (DCD) Input: Initial \u03b1 and w = \u2211n i=1 \u03b1ixi\n1: while not converged do 2: Randomly pick i 3: Update \u03b1i \u2190 \u03b1i + \u2206\u03b1i, where\n\u2206\u03b1i \u2190 arg min \u03b4\n1 2 \u2016w + \u03b4xi\u20162 + `\u2217i (\u2212(\u03b1i + \u03b4)) (4)\n4: Update w by w \u2190 w + \u2206\u03b1ixi 5: end while"}, {"heading": "3.2 Asynchronous Stochastic Dual Coordinate Descent", "text": "To parallelize DCD in a shared memory multi-core system, we propose a family of Asynchronous Stochastic Dual Coordinate Descent (PASSCoDe) algorithms. PASSCoDe is very simple but effective. Each thread repeatedly run the updates (steps 2 to 4) in Algorithm 1 using w, \u03b1, and training data stored in a shared memory. The threads do not need to coordinate or synchronize their iterations. The details are shown in Algorithm 2.\nAlgorithm 2 Parallel Asynchronous Stochastic dual Co-ordinate Descent (PASSCoDe) Input: Initial \u03b1 and w = \u2211n i=1 \u03b1ixi\nNumber of threads Lock Atomic Wild 2 98.03s / 0.27x 15.28s / 1.75x 14.08s / 1.90x 4 106.11s / 0.25x 8.35s / 3.20x 7.61s / 3.50x\n10 114.43s / 0.23x 3.86s / 6.91x 3.59s / 7.43x\nAlthough PASSCoDe is a simple extension of DCD in a multi-core setting, there are many options in terms of locking/atomic operations for each step, and these choices lead to variations in speed and convergence properties, as we will show in this paper.\nNote that the \u2206\u03b1i obtained by subproblem (5) is exactly the same as (4) in Algorithm 1 if only one thread is involved. However, when there are multiple threads, the w vector may not be the latest one since some other threads have not completed the writes in step 3.\nPASSCoDe-Lock. To ensure w = \u2211\ni \u03b1ixi for the latest \u03b1, we have to lock the following variables between step 1 and 2:\nstep 1.5: lock variables in Ni := {wt | (xi)t 6= 0}.\nThe locks are then released after step 3. With this locking mechanism, PASSCoDe-Lock will be serializable, i.e., generate the same solution sequence with the serial DCD. Unfortunately, threads will waste a lot of time due to the locks, so PASSCoDe-Lock is very slow comparing to the non-locking version (and even slower than the serial version of DCD). See Table 1 for details.\nPASSCoDe-Atomic. The above locking scheme is to ensure that each thread updates \u03b1i based on the latest w values. However, as shown in (Niu et al., 2011; Liu & Wright, 2014), the effect of using slightly stale values is usually limited in practice. Therefore, we propose an PASSCoDe-Atomic algorithm that avoids locking all the variables in Ni simultaneously. Instead, each thread just reads the current w values from memory without any locking. In practice (see Section 5) we observe that the convergence speed is not significantly affected by using values of w. However, to ensure that the limit point of the algorithm is still the global optimizer of (1), the equation w\u2217 = \u2211 i \u03b1 \u2217 ixi has to be maintained. Therefore, we apply the following \u201catomic writes\u201d in step 3:\nstep 3: For each j \u2208 N(i) Update wj \u2190 wj + \u2206\u03b1i(xi)j atomically\nPASSCoDe-Atomic is much faster than PASSCoDe-Lock as shown in Table 1 since the atomic writes for a single variable is much faster than locking all the variables. However, the convergence of PASSCoDe-Atomic is not guaranteed by any previous convergence analysis. To bridge this gap between practice and theory, we prove linear convergence of PASSCoDe-Atomic under certain conditions in Section 4.\nPASSCoDe-Wild. Finally, we consider Algorithm 2 without any locks and atomic operations. The resulting algorithm, PASSCoDe-Wild, is faster than PASSCoDe-Atomic and PASSCoDe-Lock and can achieve almost linear speedup using a single processing unit. However, due to the memory conflicts in step 3, some of the \u201dupdates\u201d to w will be over-written by other threads. As a result, the w\u0302 and \u03b1\u0302 outputted by the algorithm usually do not satisfy Eq (3):\nw\u0302 6= w\u0304 := \u2211 i \u03b1\u0302ixi, (6)\nwhere w\u0302, \u03b1\u0302 are the primal and dual variables outputted by the algorithm, and w\u0304 defined in (6) is computed from \u03b1\u0302. It is easy to see that \u03b1\u0302 is not the optimal solution of (2). Due to the same reason, in the prediction phase it is not clear whether w\u0302 or w\u0304 should be used. To answer this question, in Section 4 we show that w\u0302 is actually the optimal solution of a perturbed primal problem (1) using a backward error analysis, where the loss function is the same and the regularization term is slightly perturbed. As a result, the prediction should be done using w\u0302, and this also yields much better performance in practice, as shown in Table 2 below.\nWe summarize the behavior of the three algorithms in Figure 1. Using locks, the algorithm PASSCoDeLock is serializable but very slow (even slower than the serial DCD). In the other extreme, the wild version without any lock and atomic operation has very good speed up, but the behavior can be totally different from the serial DCD. Luckily, in Section 4 we provide the convergence guarantee for PASSCoDe-Atomic, and apply a backward error analysis to show that PASSCoDe-Wild will converge to the solution with the same loss function with a slightly perturbed regularizer."}, {"heading": "3.3 Implementation Details", "text": "Deadlock Avoidance. Without a proper implementation, the deadlock can arise in PASSCoDe-Lock because a thread needs to acquire all the locks associated with Ni. A simple way to avoid deadlock is by associating an ordering for all the locks such that each thread follows the same ordering to acquire the locks.\nRandom Permutation. In LIBLINEAR, the random sampling (step 2) of Algorithm 1 is replaced by the index from a random permutation, such that each \u03b1i can be selected in n steps in stead of n log n steps in expectation. Random permutation can be easily implemented asynchronously for Algorithm 2 as follows. Initially, given p threads, {1, . . . , n} is randomly partitioned into p blocks. Then, each thread can asynchronously generate the random permutation on its own block of variables. Shrinking Heuristic. For loss such as hinge and squared-hinge, the optimal \u03b1\u2217 is usually sparse. Based on this property, a shrinking strategy was proposed by Hsieh et al. (2008) to further speed up DCD. This heuristic is also implemented in LIBLINEAR. The idea is to maintain an active set by skipping variables which tend to be fixed. This heuristic can also be implemented in Algorithm 2 by maintaining an active set for each thread. Thread Affinity. The memory design of most modern multi-core machines is non-uniform memory access (NUMA), where a core has faster memory access to its local memory socket. To reduce possible latency due to the remote socket access, we should bind each thread to a physical core and allocate data in its local memory. Note that the current OpenMP does not support this functionality for thread affinity. Library such as libnuma can be used to enforce thread affinity."}, {"heading": "4 Convergence Analysis", "text": "In this section we formally analyze the convergence properties of our proposed algorithms in Section 3. Note that all the proofs can be found in the Appendix. We assign a global counter j for the total number of updates, and the index i(j) denotes the component selected at step j. We define {\u03b11,\u03b12, . . . } to be the sequence generated by our algorithms, and\n\u2206\u03b1j = \u03b1 j+1 i(j) \u2212 \u03b1 j i(j).\nThe update \u2206\u03b1j at iteration j is obtained by solving\n\u2206\u03b1j \u2190 arg min \u03b4\n1 2 \u2016w\u0302j + \u03b4xi(j)\u20162 + `\u2217i(j)(\u2212(\u03b1i(j) + \u03b4)),\nwhere w\u0302j is the currentw in the memory. We usewj = \u2211\ni \u03b1 j ixi to denote the \u201caccurate\u201dw at iteration j.\nIn the PASSCoDe-Lock setting,wj = w\u0302j is ensured by using the locks. However, in PASSCoDe-Atomic and PASSCoDe-Wild, w\u0302j 6= wj because some of the updates have not been written into the shared memory. To capture this phenomenon, we define Zj to be the set of all \u201cupdates to w\u201d before iteration j:\nZj := {(t, k) | t < j, k \u2208 N(i(t))},\nwhere N(i(t)) := {u | Xi(t),u 6= 0} is all nonzero features in xi(t). We define U j \u2286 Zj to be the updates that have already been written into w\u0302j . Therefore, we have\nw\u0302j = \u2211\n(t,k)\u2208Uj (\u2206\u03b1t)Xi(t),kek.\n4.1 Linear Convergence of PASSCoDe-Atomic\nIn PASSCoDe-Atomic, we assume all the updates before the (j \u2212 \u03c4)-th iteration has been written into w\u0302j , therefore,\nAssumption 1. The set U j satisfies Zj\u2212\u03c4 \u2286 U j \u2286 Zj .\nNow we define some constants used in our theoretical analysis. Note that X \u2208 Rn\u00d7d is the data matrix, and we use X\u0304 \u2208 Rn\u00d7d to denote the normalized data matrix where each row is x\u0304Ti = xTi /\u2016xi\u20162. We then define\nMi = max S\u2286[d] \u2016 \u2211 t\u2208S X\u0304:,tXi,t\u2016, M = max i Mi,\nwhere [d] := {1, . . . , d} is the set of all the feature indices, and X\u0304:,t is the t-th column of X\u0304 . We also define Lmax to be the Lipschitz constant of D(\u00b7) within the level set {\u03b1 | D(\u03b1) \u2264 D(\u03b10)}, Rmin = mini \u2016xi\u20162, Rmax = maxi \u2016xi\u20162. We assume that Rmax = 1 and there is no zero training sample, so Rmin > 0.\nTo prove the convergence of asynchronous algorithms, we first show that the expected step size does not increase super-linearly by the following Lemma 1.\nLemma 1. If \u03c4 is small enough such that\n(6\u03c4(\u03c4 + 1)2eM)/ \u221a n \u2264 1, (7)\nthen PASSCoDe-Atomic satisfies the following inequality:\nE(\u2016\u03b1j\u22121 \u2212\u03b1j\u20162) \u2264 \u03c1E(\u2016\u03b1j \u2212\u03b1j+1\u20162), (8)\nwhere \u03c1 = (1 + 6(\u03c4+1)eM\u221a n )2.\nThe detailed proof is in Appendix A.2. We use a similar technique as in (Liu & Wright, 2014) to prove this lemma, but the proof is different from (Liu & Wright, 2014) because \u2022 Their \u201cinconsistent read\u201d model assumes w\u0302j = \u2211 i \u03b1\u0307ixi for some \u03b1\u0307. However, in our case w\u0302\nj may not be written in this form due to incomplete updates in step 3 of Algorithm 2. \u2022 In (Liu & Wright, 2014), each coordinate is updated by \u03b3\u2207tf(\u03b1) with a fixed step size \u03b3. We consider\nthe case that each subproblem (4) is solved exactly. To show the linear convergence of our algorithms, we assume the objective function (2) satisfies the following property:\nDefinition 1. The objective function (2) admits the global error bound if there is a constant \u03ba such that\n\u2016\u03b1\u2212 PS(\u03b1)\u2016 \u2264 \u03ba\u2016T (\u03b1)\u2212\u03b1\u2016, (9)\nwhere PS(\u00b7) is the projection to the set of optimal solutions, and T : Rn \u2192 Rn is the operator defined by\nTt(\u03b1) = arg min u\nD(\u03b1+ (u\u2212 \u03b1t)et) \u2200t = 1, . . . , n.\nThe objective function satisfies the global error bound from the beginning if (9) holds for all \u03b1 satisfying\nD(\u03b1) \u2264 D(\u03b10)\nwhere \u03b10 is the initial point.\nThis definition is a generalized version of Definition 6 in (Wang & Lin, 2014). We list several important machine learning problems that admit global error bounds:\n\u2022 Support Vector Machines (SVM) with hinge loss (Boser et al., 1992):\n`i(zi) = C max(1\u2212 zi, 0)\n`\u2217i (\u2212\u03b1i) = { \u2212\u03b1i if 0 \u2264 \u03b1i \u2264 C, \u221e otherwise.\n(10)\n\u2022 Support Vector Machines (SVM) with square hinge loss:\n`i(zi) = C max(1\u2212 zi, 0)2.\n`\u2217i (\u2212\u03b1i) = { \u2212\u03b1i + \u03b12i /4C if \u03b1i \u2265 0, \u221e otherwise.\n(11)\nNote that C > 0 is the penalty parameter that controls the weights between loss and regularization.\nTheorem 1. The Support Vector Machines (SVM) with hinge loss or square hinge loss satisfy the global error bound (9).\nProof. For SVM with hinge loss, each element of the mapping T (\u00b7) can be written as\nTt(\u03b1) = arg min u\nD(\u03b1+ (u\u2212 \u03b1t)et)\n= arg min u\n1 2 \u2016w(\u03b1) + (u\u2212 \u03b1t)xt\u20162 + `\u2217(\u2212u)\n= \u03a0X (w(\u03b1)Txt \u2212 1 \u2016xt\u20162 ) = \u03a0X (\u2207tD(\u03b1) \u2016xt\u20162 ) ,\nwhere \u03a0X is the projection to the set X, and for hinge-loss SVM X := [0, C]. Using Lemma 26 in (Wang & Lin, 2014), we can show that for all t = 1, . . . , n\n\u2223\u2223\u03b1t \u2212\u03a0X(\u2207tD(\u03b1)\u2016xt\u20162 )\u2223\u2223 \u2265min(1, 1\u2016xt\u20162 )\u2223\u2223\u03b1t \u2212\u03a0X(\u2207tD(\u03b1))\u2223\u2223 \u2265min(1, 1\nR2max ) \u2223\u2223\u03b1t \u2212\u03a0X(\u2207tD(\u03b1))\u2223\u2223\n\u2265 \u2223\u2223\u03b1t \u2212\u03a0X(\u2207tD(\u03b1))\u2223\u2223,\nwhere the last inequality is due to the assumption that Rmax = 1. Therefore,\n\u2016\u03b1\u2212 T (\u03b1)\u20162 \u2265 1\u221a n \u2016\u03b1\u2212 T (\u03b1)\u20161\n\u2265 1\u221a n n\u2211 t=1 |\u03b1t \u2212\u03a0X ( \u2207tD(\u03b1) ) | = 1\u221a n \u2016\u2207+D(\u03b1)\u20161\n\u2265 1\u221a n \u2016\u2207+D(\u03b1)\u20162\n\u2265 1 \u03ba0 \u221a n \u2016\u03b1\u2212 PS(\u03b1)\u20162,\nwhere \u2207+D(\u03b1) is the projected gradient defined in Definition 5 of (Wang & Lin, 2014) and \u03ba0 is the \u03ba defined in Theorem 18 of (Wang & Lin, 2014). Thus, with \u03ba = \u03ba0 \u221a n, we obtain that the dual function of the hinge-loss SVM satisfies the global error bound defined in Definition 1. Similarly, we can show that the SVM with squared-hinge loss satisfies the global error bound.\nNext we explicitly state the linear convergence guarantee for PASSCoDe-Atomic.\nTheorem 2. Assume the objective function (2) admits a global error bound from the beginning and the Lipschitz constant Lmax is finite in the level set. If (7) holds and\n1 \u2265 2Lmax R2min (1 + e\u03c4M\u221a n )( \u03c42M2e2 n )\nthen PASSCoDe-Atomic has a global linear convergence rate in expectation, that is,\nE[D(\u03b1j+1)]\u2212D(\u03b1\u2217) \u2264 \u03b7 ( E[D(\u03b1j)]\u2212D(\u03b1\u2217) ) , (12)\nwhere \u03b1\u2217 is the optimal solution and\n\u03b7 = 1\u2212 \u03ba Lmax (1\u2212 2Lmax R2min (1 + e\u03c4M\u221a n )( \u03c42M2e2 n )) (13)\n4.2 Backward Error Analysis for PASSCoDe-Wild\nIn PASSCoDe-Wild, assume the sequence {\u03b1j} converges to \u03b1\u0302 and {wj} converges to w\u0302. Now we show that the dual solution \u03b1\u0302 and the corresponding primal variables w\u0304 = \u2211n i=1 \u03b1\u0302ixi are actually the dual and primal solutions of a perturbed problem:\nTheorem 3. \u03b1\u0302 is the optimal solution of a perturbed dual problem\n\u03b1\u0302 = arg min \u03b1 D(\u03b1)\u2212 n\u2211 i=1 \u03b1i Txi, (14)\nand w\u0304 = \u2211\ni \u03b1\u0302ixi is the solution of the corresponding primal problem:\nw\u0304 = arg min w\n1 2 wTw + n\u2211 i=1 `i((w \u2212 )Txi), (15)\nwhere \u2208 Rd is given by = w\u0304 \u2212 w\u0302.\nProof. By definition, \u03b1\u0302 is the limit point of PASSCoDe-Wild. Therefore, {\u2206\u03b1i} \u2192 0 for all i. Combining with the fact that {w\u0302j} \u2192 w\u0302, we have\n\u2212w\u0302Txi \u2208 \u2202\u03b1i`\u2217i (\u2212\u03b1\u0302i), \u2200i.\nSince w\u0302 = w\u0304 \u2212 , we have\n\u2212(w\u0304 \u2212 )Txi \u2208 \u2202\u03b1i`\u2217i (\u2212\u03b1\u0302i), \u2200i \u2212w\u0304Txi \u2208 \u2202\u03b1i ( `\u2217i (\u2212\u03b1\u0302i)\u2212 \u03b1\u0302i Txi ) , \u2200i\n0 \u2208 \u2202\u03b1i\n( 1\n2 \u2016 n\u2211 i=1 \u03b1\u0302ixi\u20162 + `\u2217i (\u2212\u03b1\u0302i)\u2212 \u03b1\u0302i Txi\n) , \u2200i\nwhich is the optimality condition of (14). Thus, \u03b1\u0302 is the optimal solution of (14). For the second part of the theorem, let\u2019s consider the following equivalent primal problem and its Lagrangian:\nmin w,\u03be\n1 2 wTw + n\u2211 i=1 `i(\u03bei) s.t. \u03bei = (w \u2212 )Txi \u2200i = 1, . . . , n\nL(w, \u03be,\u03b1) := 1\n2 wTw + n\u2211 i=1 {`i(\u03bei) + \u03b1i(\u03bei \u2212wTxi + Txi)}\nThe corresponding convex version of the dual function can be derived as follows.\nD\u0302(\u03b1) = max w,\u03be \u2212L(w, \u03be,\u03b1)\n= ( max w \u22121 2 wTw + n\u2211 i=1 \u03b1iw Txi ) + n\u2211 i=1 ( max \u03bei \u2212`i(\u03bei)\u2212 \u03b1i\u03bei ) \u2212 \u03b1i Txi\n= 1\n2 \u2016 n\u2211 i=1 \u03b1ixi\u20162 + n\u2211 i=1 `\u2217i (\u2212\u03b1i)\u2212 \u03b1i Txi\n= D(\u03b1)\u2212 n\u2211 i=1 \u03b1i Txi\nThe last second equality comes from 1) the substitution of w\u2217 = \u2211T\ni=1 \u03b1ixi obtained by setting \u2207w \u2212 L(w, \u03be,\u03b1) = 0; 2) the definition of the conjugate function `\u2217i (\u2212\u03b1i). Thus, the second part of the theorem follows.\nNote that is the error caused by the memory conflicts. From Theorem 3, w\u0304 is the optimal solution of the \u201cbiased\u201d primal problem (15), however, in (15) the actual model that fits the loss function should be w\u0302 = w\u0304\u2212 . Therefore after the training process we should use w\u0302 to predict, which is thew we maintained during the parallel coordinate descent updates. Replacing w by w \u2212 in (15), we have the following corollary :\nCorollary 1. w\u0302 computed by PASSCoDe-Wild is the solution of the following perturbed primal problem:\nw\u0302 = arg min w\n1 2 (w + )T (w + ) + n\u2211 i=1 `i(w Txi) (16)\nThe above corollary shows that the computed primal solution w\u0302 is actually the exact solution of a perturbed problem (where the perturbation is on the regularizer). This strategy (of showing that the computed solution to a problem is the exact solution of a perturbed problem) is inspired by the backward error analysis technique commonly employed in numerical analysis (Wilkinson, 1961)1."}, {"heading": "5 Experimental Results", "text": "We conduct several experiments and show that the proposed PASSCoDe-Atomic and PASSCoDe-Wild have superior performance compared to other state-of-the-art parallel coordinate descent algorithms. We consider the hinge loss and five datasets: news20, covtype, rcv1, webspam, and kddb. Detailed information is shown in Table 3. To have a fair comparison, we implement all compared methods in C++ using OpenMP as the parallel programming framework. All the experiments are performed on an Intel multi-core dual-socket machine with 256 GB memory. Each socket is associated with 10 computation cores. We explicitly enforce that all the threads use cores from the same socket to avoid inter-socket communication. Our codes will be publicly available. We focus on solving the (hinge loss) SVM (see (5) in the Appendix) in the experiments, but the algorithms can also be applied to other objective functions. Note that some of the figures are in Appendix 6.\nSerial Baselines. \u2022 DCD: we implement Algorithm 1. Instead of sampling with replacement, a random permutation is\nused to enforce random sampling without replacement. \u2022 LIBLINEAR: we use the implementation in http://www.csie.ntu.edu.tw/\u02dccjlin/liblinear.\nThis implementation is equivalent to DCD with the shrinking strategy. Compared Parallel Implementation. \u2022 PASSCoDe: We implement the proposed three variants of Algorithm 2 using DCD as the building\nblock: Wild, Atomic, and Lock. \u2022 CoCoA: We implement a multi-core version of CoCoA (Jaggi et al., 2014) with \u03b2K = 1 and DCD as\nits local dual method. \u2022 AsySCD: We follow the description in (Liu & Wright, 2014; Liu et al., 2014) to implement AsySCD\nwith the step length \u03b3 = 12 and the shuffling period p = 10 as suggested in (Liu et al., 2014).\n1J. H. Wilkinson received the Turing Award in 1970, partly for his work on backward error analysis"}, {"heading": "5.1 Convergence in terms of iterations.", "text": "The primal objective function value is used to determine the convergence. Note that we still use P (w\u0302) for PASSCoDe-Wild, although the true primal objective should be (16). As long as w\u0302T remains small enough, the trend of (16) and P (w\u0302) are similar.\nFigure 4(a), 5(a), 6(a) show the convergence results of PASSCoDe-Wild, PASSCoDe-Atomic, CoCoA, and AsySCD with 10 threads in terms of number of iterations. The horizontal line in grey indicates the primal objective function value obtained by LIBLINEAR using the default stopping condition. The result for LIBLINEAR is also included for reference. We have the follow observations \u2022 Convergence of three PASSCoDe variants are almost identical and very close to the convergence\nbehavior of serial LIBLINEAR on three large sparse datasets (rcv1, webspam, and kddb). \u2022 PASSCoDe-Wild and PASSCoDe-Atomic converge significantly faster than CoCoA. \u2022 On covtype, a more dense dataset, all three algorithms (PASSCoDe-Wild, PASSCoDe-Atomic, and\nCoCoA) have slower convergence."}, {"heading": "5.2 Efficiency.", "text": "Timing. To have a fair comparison, we include both initialization and computation into the timing results. For DCD, PASSCoDe, CoCoA, initialization takes one pass of entire data matrix (which is O(nnz(X))) to compute \u2016xi\u2016 for each instance. In the initialization stage, AsySCD requires O(n \u00d7 nnz(X)) time and O(n2) space to form and store the Hessian matrix Q for (2). Thus, we only have results on news20 for AsySCD as all other datasets are too large for AsySCD to fit Q in even 256 GB memory. Note that we also parallelize the initialization part for each algorithm in our implementation to have a fair comparison.\nFigures 2(b), 3(b), 4(b), 5(b), 6(b) show the primal objective values in terms of time and Figures 2(c), 3(c), 4(c), 5(c), 6(c) shows the accuracy in terms of time. Note that the x-axis for news20, covtype, and rcv1 is in log-scale. A horizontal line in gray in each figure denotes the objective values/accuracy obtained by LIBLINEAR using the default stopping condition. We have the following observations: \u2022 From Figures 4(b) and 4(c), we can see that AsySCD is orders of magnitude slower than other ap-\nproaches including parallel methods and serial reference (AsySCD using 10 cores takes 0.4 seconds to run 10 iterations, while all the other parallel approaches takes less than 0.14 seconds, and LIBLINEAR takes less than 0.3 seconds). In fact, AsySCD is still slower than other methods even when the initialization time is excluded. This is expected because AsySCD is a parallel version of a standard coordinate descent method, which is known to be much slower than DCD for (2). Since AsySCD runs out of memory for all the other larger datasets, we do not show the results in other figures. \u2022 In most figures, both PASSCoDe approaches outperform CoCoA. In Figure 6(c), kddb shows better\naccuracy performance in the early stage which can be explained by the ensemble nature of CoCoA. In the long term, it still converges to the accuracy obtained by LIBLINEAR. \u2022 For all datasets, PASSCoDe-Wild is shown to be slightly faster than PASSCoDe-Atomic. Given the fact\nthat both methods show similar convergence in terms of iterations, this phenomenon can be explained by the effect of atomic operations. We can observe that more dense the dataset, larger the difference between PASSCoDe-Wild and PASSCoDe-Atomic."}, {"heading": "5.3 Speedup", "text": "We are interested in the following evaluation criterion:\nspeedup := time taken by the target method with p threads time taken by the best serial reference method ,\nThis criterion is different from scaling, where the denominator is replaced by \u201ctime taken for the target method with single thread.\u201d Note that a method can have perfect scaling but very poor speedup. Figures 2(d), 3(d), 4(d), 5(d), 6(d) shows the speedup results, where 1) DCD is used as the best serial reference; 2) the shrinking heuristic is turned off for all PASSCoDe and DCD to have fair comparison; 3) the initialization time is excluded from the computation of speedup. \u2022 PASSCoDe-Wild has very good speedup performance compared to other approaches. It achieves about\n6 to 8 speedups using 10 threads on all the datasets. \u2022 From Figure 2(d), we can see that AsySCD does not have any \u201cspeedup\u201d over the serial reference,\nalthough it is shown to have almost linear scaling (Liu et al., 2014; Liu & Wright, 2014)."}, {"heading": "6 Conclusions", "text": "In this paper, we present a family of parallel asynchronous stochastic dual coordinate descent algorithms in the shared memory multi-core setting, where each thread repeatedly selects a random dual variable and conducts coordinate updates using the primal variables that are stored in the shared memory. We analyze the convergence properties when different locking/atomic mechanism is used. For the setting with atomic updates, we show the linear convergence under certain condition. For the setting without any lock or atomic write, which achieves the best speed up, we present a backward error analysis to show that the primal variables obtained by the algorithm is the exact solution for a primal problem with perturbed regularizer. Experimental results show that our algorithms are much faster than previous parallel coordinate descent solvers.\n(a) Convergence\n(b) Objective\n(c) Accuracy\n(d) Speedup\n(a) Convergence\n(b) Objective\n(c) Accuracy\n(d) Speedup\n(a) Convergence\n(b) Objective\n(c) Accuracy\n(d) Speedup\nFigure 4: rcv1 dataset\n(a) Convergence\n(b) Objective\n(c) Accuracy\n(d) Speedup\nFigure 5: webspam dataset\n(a) Convergence\n(b) Objective\n(c) Accuracy\n(d) Speedup\nFigure 6: kddb dataset"}, {"heading": "A.1 Notations and Prepositions", "text": ""}, {"heading": "A.1.1 Notations", "text": "\u2022 For all i = 1, . . . , n, we have the following definitions:\nhi(u) := `\u2217i (\u2212u) \u2016xi\u20162\nproxi(s) := arg minu\n1 2 (u\u2212 s)2 + hi(u)\nTi(w, s) := arg min u\n1 2 \u2016w + (u\u2212 s)xi\u20162 + `\u2217i (\u2212u)\n= arg min u\n1\n2\n[ u\u2212 (s\u2212 w\nTxi \u2016xi\u20162 )\n]2 + hi(u),\nwherew \u2208 Rd and s \u2208 R. We also denote prox(s) as the proximal operator from Rn to Rn such that (prox(x))i = proxi(si). We can see the connection of the above operator and the proximal operator: Ti(w, s) = proxi(s\u2212 w Txi \u2016xi\u20162 ).\n\u2022 Let {\u03b1j} and {w\u0302j} be the sequence generated/maintained by Algorithm 2 using\n\u03b1j+1t =\n{ Tt(w\u0302\nj , \u03b1jt ) if t = i(j), \u03b1jt if t 6= i(j),\nwhere i(j) is the index selected at j-th iteration. For convenience, we define\n\u2206\u03b1j = \u03b1 j+1 i(j) \u2212 \u03b1 j i(j).\n\u2022 Let {\u03b1\u0303j} be the sequence defined by\n\u03b1\u0303j+1t = Tt(w\u0302 j , \u03b1jt ) \u2200t = 1, . . . , n.\nNote that \u03b1\u0303j+1i(j) = \u03b1 j+1 i(t) and \u03b1\u0303 j+1 = prox(\u03b1j \u2212 X\u0304w\u0302j).\n\u2022 Let w\u0304j = \u2211\ni \u03b1 j ixi be the \u201ctrue\u201d primal variables corresponding to \u03b1 j ."}, {"heading": "A.1.2 Prepositions", "text": "Preposition 1. Ei(j)(\u2016\u03b1j+1 \u2212\u03b1j\u20162) = 1\nn \u2016\u03b1\u0303j+1 \u2212\u03b1j\u20162. (17)\nProof. It can be proved by the definition of \u03b1\u0303 and the assumption that i(j) is uniformly random selected from {1, . . . , n}.\nPreposition 2.\n\u2016X\u0304w\u0304j \u2212 X\u0304w\u0302j\u2016 \u2264M j\u22121\u2211 t=j\u2212\u03c4 |\u2206\u03b1t|. (18)"}, {"heading": "Proof.", "text": "\u2016X\u0304w\u0304j \u2212 X\u0304w\u0302j\u2016 = \u2016X\u0304( \u2211\n(t,k)\u2208Zj\\Uj (\u2206\u03b1t)Xi(t),kek)\u2016 = \u2016 \u2211 (t,k)\u2208Zj\\Uj (\u2206\u03b1t)X\u0304:,kXi(t),k\u2016\n\u2264 j\u2212\u03c4\u2211 t=j\u22121 |\u2206\u03b1t|Mi \u2264M j\u22121\u2211 t=j\u2212\u03c4 |\u2206\u03b1t|\nPreposition 3. For any w1,w2 \u2208 Rd and s1, s2 \u2208 R,\n|Ti(w1, s1)\u2212 Ti(w2, s2)| \u2264 |s1 \u2212 s2 + (w1 \u2212w2)Txi \u2016xi\u20162 |. (19)\nProof. It can be proved by the connection of Ti(w, s) and proxi(\u00b7) and the non-expansiveness of the proximal operator.\nPreposition 4. Let M \u2265 1, q = 6(\u03c4+1)eM\u221a n\n, \u03c1 = (1 + q)2, and \u03b8 = \u2211\u03c4\nt=1 \u03c1 t/2. If M \u2265 1 and q(\u03c4 + 1) \u2264 1,\nthen \u03c1(\u03c4+1)/2 \u2264 e, and\n\u03c1\u22121 \u2264 1\u2212 4 + 4M + 4M\u03b8\u221a n . (20)\nProof. By the definition of \u03c1 and the condition q(\u03c4 + 1) \u2264 1, we have\n\u03c1(\u03c4+1)/2 = (( \u03c11/2 )1/q)q(\u03c4+1) = ( (1 + q)1/q )q(\u03c4+1) \u2264 eq(\u03c4+1) \u2264 e.\nBy the definitions of q, we know that\nq = \u03c11/2 \u2212 1 = 6(\u03c4 + 1)eM\u221a n \u21d2 3 2 =\n\u221a n(\u03c11/2 \u2212 1)\n4(\u03c4 + 1)eM .\nWe can derive\n3 2 =\n\u221a n(\u03c11/2 \u2212 1)\n4(\u03c4 + 1)eM\n\u2264 \u221a n(\u03c11/2 \u2212 1)\n4(\u03c4 + 1)\u03c1(\u03c4+1)/2M \u2235 \u03c1(\u03c4+1)/2 \u2264 e\n\u2264 \u221a n(\u03c11/2 \u2212 1)\n4(1 + \u03b8)\u03c11/2M \u2235 1 + \u03b8 = \u03c4\u2211 t=0 \u03c1t/2 \u2264 (\u03c4 + 1)\u03c1\u03c4/2\n= \u221a n(1\u2212 \u03c1\u22121/2) 4(1 + \u03b8)M \u2264 \u221a n(1\u2212 \u03c1\u22121)\n4(1 + \u03b8)M \u2235 \u03c1\u22121/2 \u2264 1\nCombining the condition that M \u2265 1 and 1 + \u03b8 \u2265 1, we have \u221a n(1\u2212 \u03c1\u22121)\u2212 4 4(1 + \u03b8)M \u2265 \u221a n(1\u2212 \u03c1\u22121) 4(1 + \u03b8)M \u2212 1 2 \u2265 1,\nwhich leads to\n4(1 + \u03b8)M \u2264 \u221a n\u2212 \u221a n\u03c1\u22121 \u2212 4\n\u03c1\u22121 \u2264 1\u2212 4 + 4M + 4M\u03b8\u221a n ."}, {"heading": "A.2 Proof of Lemma 1", "text": "Similar to Liu & Wright (2014), we prove Eq. (8) by induction. First, we know that for any two vectors a and b, we have\n\u2016a\u20162 \u2212 \u2016b\u20162 \u2264 2\u2016a\u2016\u2016b\u2212 a\u2016.\nSee Liu & Wright (2014) for a proof for the above inequality. Thus, for all j , we have\n\u2016\u03b1j\u22121 \u2212 \u03b1\u0303j\u20162 \u2212 \u2016\u03b1j \u2212 \u03b1\u0303j+1\u20162 \u2264 2\u2016\u03b1j\u22121 \u2212 \u03b1\u0303j\u2016\u2016\u03b1j \u2212 \u03b1\u0303j+1 \u2212\u03b1j\u22121 + \u03b1\u0303j\u2016. (21)\nThe second factor in the r.h.s of (21) is bounded as follows:\n\u2016\u03b1j \u2212 \u03b1\u0303j+1 \u2212\u03b1j\u22121 + \u03b1\u0303j\u2016 \u2264 \u2016\u03b1j \u2212\u03b1j\u22121\u2016+ \u2016 prox(\u03b1j \u2212 X\u0304w\u0302j)\u2212 prox(\u03b1j\u22121 \u2212 X\u0304w\u0302j\u22121)\u2016 \u2264 \u2016\u03b1j \u2212\u03b1j\u22121\u2016+ \u2016(\u03b1j \u2212 X\u0304w\u0302j)\u2212 (\u03b1j\u22121 \u2212 X\u0304w\u0302j\u22121)\u2016 \u2264 \u2016\u03b1j \u2212\u03b1j\u22121\u2016+ \u2016\u03b1j \u2212\u03b1j\u22121\u2016+ \u2016X\u0304w\u0302j \u2212 X\u0304w\u0302j\u22121\u2016 = 2\u2016\u03b1j \u2212\u03b1j\u22121\u2016+ \u2016X\u0304w\u0302j \u2212 X\u0304w\u0302j\u22121\u2016 = 2\u2016\u03b1j \u2212\u03b1j\u22121\u2016+ \u2016X\u0304w\u0302j \u2212 X\u0304w\u0304j + X\u0304w\u0304j \u2212 X\u0304w\u0304j\u22121 + X\u0304w\u0304j\u22121 \u2212 X\u0304w\u0302j\u22121\u2016 \u2264 2\u2016\u03b1j \u2212\u03b1j\u22121\u2016+ \u2016X\u0304w\u0304j \u2212 X\u0304w\u0304j\u22121\u2016+ \u2016X\u0304w\u0302j \u2212 X\u0304w\u0304j\u2016+ \u2016X\u0304w\u0304j\u22121 \u2212 X\u0304w\u0302j\u22121\u2016\n\u2264 (2 +M)\u2016\u03b1j \u2212\u03b1j\u22121\u2016+ j\u22121\u2211 t=j\u2212\u03c4 \u2016\u2206\u03b1t\u2016M + j\u22122\u2211 t=j\u2212\u03c4\u22121 \u2016\u2206\u03b1t\u2016M = (2 + 2M)\u2016\u03b1j \u2212\u03b1j\u22121\u2016+ 2M j\u22122\u2211\nt=j\u2212\u03c4\u22121 \u2016\u2206\u03b1t\u2016 (22)\nNow we prove (8) by induction. Induction Hypothesis. Due to Preposition 1, we prove the following equivalent statement. For all j,\nE(\u2016\u03b1j\u22121 \u2212 \u03b1\u0303j\u20162) \u2264 \u03c1E(\u2016\u03b1j \u2212 \u03b1\u0303j+1\u20162), (23)\nInduction Basis. When j = 1,\n\u2016\u03b11 \u2212 \u03b1\u03032 +\u03b10 \u2212 \u03b1\u03031\u2016 \u2264 (2 + 2M)\u2016\u03b11 \u2212\u03b10\u2016.\nBy taking the expectation on (21), we have\nE[\u2016\u03b10 \u2212 \u03b1\u03031\u20162]\u2212 E[\u2016\u03b11 \u2212 \u03b1\u03032\u20162] \u2264 2E[\u2016\u03b10 \u2212 \u03b1\u03031\u2016\u2016\u03b11 \u2212 \u03b1\u03032 \u2212\u03b10 + \u03b1\u03031\u2016] \u2264 (4 + 4M)E(\u2016\u03b10 \u2212 \u03b1\u03031\u2016\u2016\u03b10 \u2212\u03b11\u2016).\nFrom (17) we have E[\u2016\u03b10 \u2212\u03b11\u20162] = 1n\u2016\u03b1 0 \u2212 \u03b1\u03031\u20162. Also, by AM-GM inequality, for any \u00b51, \u00b52 > 0 and any c > 0, we have\n\u00b51\u00b52 \u2264 1\n2 (c\u00b521 + c \u22121\u00b522). (24)\nTherefore, we have\nE[\u2016\u03b10 \u2212 \u03b1\u03031\u2016\u2016\u03b10 \u2212\u03b11\u2016] \u2264 1 2 E [ n1/2\u2016\u03b10 \u2212\u03b11\u20162 + n\u22121/2\u2016\u03b1\u03031 \u2212\u03b10\u20162 ] = 1 2 E [ n\u22121/2\u2016\u03b10 \u2212 \u03b1\u03031\u20162 + n\u22121/2\u2016\u03b1\u03031 \u2212\u03b10\u20162 ] by (17)\n= n\u22121/2E[\u2016\u03b10 \u2212 \u03b1\u03031\u20162].\nTherefore,\nE[\u2016\u03b10 \u2212 \u03b1\u03031\u20162]\u2212 E[\u2016\u03b11 \u2212 \u03b1\u03032\u20162] \u2264 4 + 4M\u221a n E[\u2016\u03b10 \u2212 \u03b1\u03031\u20162],\nwhich implies\nE[\u2016\u03b10 \u2212\u03b11\u20162] \u2264 1 1\u2212 4+4M\u221a\nn\nE[\u2016\u03b11 \u2212 \u03b1\u03032\u20162] \u2264 \u03c1E[\u2016\u03b11 \u2212 \u03b1\u03032\u20162], (25)\nwhere the last inequality is based on Preposition 4 and the fact \u03b8M \u2265 1. Induction Step. By the induction hypothesis, we assume\nE[\u2016\u03b1t\u22121 \u2212 \u03b1\u0303t\u20162] \u2264 \u03c1E[\u2016\u03b1t \u2212 \u03b1\u0303t+1\u20162] \u2200t \u2264 j \u2212 1. (26)\nThe goal is to show E[\u2016\u03b1j\u22121 \u2212 \u03b1\u0303j\u20162] \u2264 \u03c1E[\u2016\u03b1j \u2212 \u03b1\u0303j+1\u20162].\nFirst, we show that for all t < j,\nE [ \u2016\u03b1t \u2212\u03b1t+1\u2016\u2016\u03b1j\u22121 \u2212 \u03b1\u0303j\u2016 ] \u2264 \u03c1\n(j\u22121\u2212t)/2 \u221a n\nE [ \u2016\u03b1j\u22121 \u2212 \u03b1\u0303j\u20162 ] (27)\nProof. By (24) with c = n1/2\u03b2, where \u03b2 = \u03c1(t+1\u2212j)/2,\nE [ \u2016\u03b1t \u2212\u03b1t+1\u2016\u2016\u03b1j\u22121 \u2212 \u03b1\u0303j\u2016 ] \u2264 1 2 E [ n1/2\u03b2\u2016\u03b1t \u2212\u03b1t+1\u20162 + n\u22121/2\u03b2\u22121\u2016\u03b1j\u22121 \u2212 \u03b1\u0303j\u20162\n] = 1 2 E [ n1/2\u03b2E[\u2016\u03b1t \u2212\u03b1t+1\u20162] + n\u22121/2\u03b2\u22121\u2016\u03b1j\u22121 \u2212 \u03b1\u0303j\u20162\n] = 1 2 E [ n\u22121/2\u03b2\u2016\u03b1t \u2212 \u03b1\u0303t+1\u20162 + n\u22121/2\u03b2\u22121\u2016\u03b1j\u22121 \u2212 \u03b1\u0303j\u20162 ] by Preposition 1\n\u2264 1 2 E [ n\u22121/2\u03b2\u03c1j\u22121\u2212t\u2016\u03b1j\u22121 \u2212 \u03b1\u0303j\u20162 + n\u22121/2\u03b2\u22121\u2016\u03b1j\u22121 \u2212 \u03b1\u0303j\u20162 ] by Eq. (26)\n\u2264 1 2 E [ n\u22121/2\u03b2\u22121\u2016\u03b1j\u22121 \u2212 \u03b1\u0303j\u20162 + n\u22121/2\u03b2\u22121\u2016\u03b1\u0303j\u22121 \u2212\u03b1j\u20162 ] by the definition of \u03b2 \u2264 \u03c1 (j\u22121\u2212t)/2 \u221a n E [ \u2016\u03b1j\u22121 \u2212 \u03b1\u0303j\u20162 ]\nLet \u03b8 = \u2211\u03c4\nt=1 \u03c1 t/2. We have\nE[\u2016\u03b1j\u22121 \u2212 \u03b1\u0303j\u20162]\u2212 E[\u2016\u03b1j \u2212 \u03b1\u0303j+1\u20162]\n\u2264 E [ 2\u2016\u03b1j\u22121 \u2212 \u03b1\u0303j\u2016 ( (2 + 2M)\u2016\u03b1j \u2212\u03b1j\u22121\u2016+ 2M j\u22121\u2211 t=j\u2212\u03c4\u22121 \u2016\u03b1t \u2212\u03b1t\u22121\u2016 )] by (21), (22) = (4 + 4M)E(\u2016\u03b1j\u22121 \u2212 \u03b1\u0303j\u2016\u2016\u03b1j \u2212\u03b1j\u22121\u2016) + 4M j\u22121\u2211\nt=j\u2212\u03c4\u22121 E [ \u2016\u03b1j\u22121 \u2212 \u03b1\u0303j\u2016\u2016\u03b1t \u2212\u03b1t\u22121\u2016 ]\n\u2264 (4 + 4M)n\u22121/2E[\u2016\u03b1\u0303j \u2212\u03b1j\u22121\u20162] + 4Mn\u22121/2E[\u2016\u03b1j\u22121 \u2212 \u03b1\u0303j\u20162] j\u22122\u2211\nt=j\u22121\u2212\u03c4 \u03c1(j\u22121\u2212t)/2 by (27)\n\u2264 (4 + 4M)n\u22121/2E[\u2016\u03b1\u0303j \u2212\u03b1j\u22121\u20162] + 4Mn\u22121/2\u03b8E[\u2016\u03b1j\u22121 \u2212 \u03b1\u0303j\u20162]\n\u2264 4 + 4M + 4M\u03b8\u221a n E[\u2016\u03b1j\u22121 \u2212 \u03b1\u0303j\u20162],\nwhich implies that\nE[\u2016\u03b1j\u22121 \u2212 \u03b1\u0303j\u20162] \u2264 1 1\u2212 4+4M+4M\u03b8\u221a\nn\nE[\u2016\u03b1j \u2212 \u03b1\u0303j+1\u20162] \u2264 \u03c1E[\u2016\u03b1j \u2212 \u03b1\u0303j+1\u20162],\nwhere the last inequality is based on Preposition 4."}, {"heading": "A.3 Proof of Theorem 2", "text": "First, we define T (w,\u03b1) to be a n-dimensional vector such that\n(T (w,\u03b1))t = Tt(w,\u03b1t) for all t,\nWe can then bound the distanceE[\u2016T (wj ,\u03b1j)\u2212T (w\u0302j ,\u03b1j)\u20162] by (we omit the expectation in the following derivation):\n\u2016T (wj ,\u03b1j)\u2212 T (w\u0302j ,\u03b1j)\u20162 = n\u2211 t=1 ( Tt(w j , \u03b1jt )\u2212 Tt(w\u0302 j , \u03b1jt ) )2 \u2264 \u2211 t ((wj \u2212 w\u0302j)Txt \u2016xt\u20162\n)2 (By Proposition 2) = \u2016X\u0304(wj \u2212 w\u0302j)\u20162 \u2264M2 ( j\u22121\u2211 t=j\u2212\u03c4 \u2016\u03b1t+1 \u2212\u03b1t\u2016 )2 (By Proposition 3) \u2264 \u03c4M2 ( j\u22121\u2211 t=j\u2212\u03c4 \u2016\u03b1t+1 \u2212\u03b1t\u20162 )\n\u2264 \u03c4M2 ( \u03c4\u2211 t=1 \u03c1t\u2016\u03b1j \u2212\u03b1j+1\u20162 ) (By Lemma 1)\n\u2264 \u03c4M 2\nn ( \u03c4\u2211 t=1 \u03c1t)\u2016T (w\u0302j ,\u03b1j)\u2212\u03b1j\u20162\n\u2264 \u03c4 2M2\nn \u03c1\u03c4\u2016T (w\u0302j ,\u03b1j)\u2212\u03b1j\u20162\nSince \u03c1(\u03c4+1)/2 \u2264 e, we have \u03c1\u03c4+1 \u2264 e2, so \u03c1\u03c4 \u2264 e2 since \u03c1 \u2265 1. Therefore,\n\u2016T (wj ,\u03b1j)\u2212 T (w\u0302j ,\u03b1j)\u20162 \u2264 \u03c4 2M2e2\nn \u2016T (w\u0302j ,\u03b1j)\u2212\u03b1j\u20162. (28)\nAs a result,\n\u2016T (wj ,\u03b1j)\u2212\u03b1j\u20162 = \u2016T (wj ,\u03b1j)\u2212 T (w\u0302j ,\u03b1j) + T (w\u0302j ,\u03b1j)\u2212\u03b1j\u20162 \u2264 2 ( \u2016T (wj ,\u03b1j)\u2212 T (w\u0302j ,\u03b1j)\u20162 + \u2016T (w\u0302j ,\u03b1j)\u2212\u03b1j\u20162 ) \u2264 2(1 + e 2\u03c42M2\nn )\u2016T (w\u0302j ,\u03b1j)\u2212\u03b1j\u20162. (29)\nNext, we bound the decrease of objective function value by\nD(\u03b1j)\u2212D(\u03b1j+1) = D(\u03b1j)\u2212D(\u03b1\u0304j+1) +D(\u03b1\u0304j+1)\u2212D(\u03b1j+1)\n\u2265 \u2016xi(j)\u20162\n2 \u2016\u03b1ji(j) \u2212 Ti(j)(w j ,\u03b1j)\u20162 \u2212 Lmax 2 \u2016Ti(j)(wj ,\u03b1j)\u2212 Ti(j)(w\u0302j ,\u03b1j)\u20162\nSo\nE[D(\u03b1j)]\u2212 E[D(\u03b1j+1)] \u2265 R 2 min 2n E[\u2016T (wj ,\u03b1j)\u20162]\u2212 Lmax 2n E[\u2016T (w\u0302j ,\u03b1j)\u2212\u03b1j\u20162]\n\u2265 R 2 min 2n E[\u2016T (wj ,\u03b1j)\u2212\u03b1j\u20162]\u2212 Lmax 2n \u03c42M2e2 n E[\u2016T (w\u0302j ,\u03b1j)\u2212\u03b1j\u20162] \u2265 R 2 min\n2n E[\u2016T (wj ,\u03b1j)\u2212\u03b1j\u20162]\u2212 2Lmax 2n\n\u03c42M2e2\nn (1 + e\u03c4M\u221a n )E[\u2016T (wj ,\u03b1j)\u2212\u03b1j\u20162]\n\u2265 R 2 min\n2n\n( 1\u2212 2Lmax\nR2min (1 +\ne\u03c4M\u221a n )( \u03c42M2e2 n )\n) E[\u2016T (wj ,\u03b1j)\u2212\u03b1j\u20162]\nLet b = (1\u2212 2Lmax R2min (1 + e\u03c4M\u221a n )( \u03c4 2M2e2 n )) and combine the above inequality with eq (9) we have\nE[D(\u03b1j)]\u2212 E[D(\u03b1j+1)] \u2265 b\u03baE[\u2016\u03b1j \u2212 PS(\u03b1j)\u20162]\n\u2265 b\u03ba Lmax E[D(\u03b1j)\u2212D\u2217].\nTherefore, we have\nE[D(\u03b1j+1)]\u2212D\u2217 = E[D(\u03b1j)]\u2212 (E[D(\u03b1j)]\u2212 E[D(\u03b1j+1)])\u2212D\u2217\n\u2264 (1\u2212 b\u03ba Lmax )(E[D(\u03b1j)]\u2212D\u2217)."}], "references": [{"title": "Revisiting asynchronous linear solvers: Provable convergence rate through randomization", "author": ["H. Avron", "A. Druinsky", "A. Gupta"], "venue": "In IEEE International Parallel and Distributed Processing Symposium,", "citeRegEx": "Avron et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Avron et al\\.", "year": 2014}, {"title": "Parallel and Distributed Computation: Numerical Methods", "author": ["Bertsekas", "Dimitri P", "Tsitsiklis", "John N"], "venue": null, "citeRegEx": "Bertsekas et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Bertsekas et al\\.", "year": 1989}, {"title": "A training algorithm for optimal margin classifiers", "author": ["Boser", "Bernhard E", "Guyon", "Isabelle", "Vapnik", "Vladimir"], "venue": "In Proceedings of the Fifth Annual Workshop on Computational Learning Theory,", "citeRegEx": "Boser et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Boser et al\\.", "year": 1992}, {"title": "Parallel coordinate descent for l1-regularized loss minimization", "author": ["Bradley", "Joseph K", "Kyrola", "Aapo", "Bickson", "Danny", "Guestrin", "Carlos"], "venue": "In ICML,", "citeRegEx": "Bradley et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bradley et al\\.", "year": 2011}, {"title": "Coordinate descent method for large-scale L2-loss linear SVM", "author": ["Chang", "Kai-Wei", "Hsieh", "Cho-Jui", "Lin", "Chih-Jen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Chang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2008}, {"title": "A dual coordinate descent method for large-scale linear SVM", "author": ["Hsieh", "Cho-Jui", "Chang", "Kai-Wei", "Lin", "Chih-Jen", "Keerthi", "S. Sathiya", "Sundararajan", "Sellamanickam"], "venue": "In Proceedings of the Twenty Fifth International Conference on Machine Learning (ICML),", "citeRegEx": "Hsieh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2008}, {"title": "Communication-efficient distributed dual coordinate ascent", "author": ["Jaggi", "Martin", "Smith", "Virginia", "Tak\u00e1\u010d", "Terhorst", "Jonathan", "Hofmann", "Thomas", "Jordan", "Michael I"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Jaggi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaggi et al\\.", "year": 2014}, {"title": "A sequential dual method for large scale multi-class linear SVMs", "author": ["Keerthi", "S. Sathiya", "Sundararajan", "Sellamanickam", "Chang", "Kai-Wei", "Hsieh", "Cho-Jui", "Lin", "Chih-Jen"], "venue": "In Proceedings of the Forteenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 408\u2013416,", "citeRegEx": "Keerthi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Keerthi et al\\.", "year": 2008}, {"title": "Asynchronous stochastic coordinate descent: Parallelism and convergence properties", "author": ["J. Liu", "S.J. Wright"], "venue": null, "citeRegEx": "Liu and Wright,? \\Q2014\\E", "shortCiteRegEx": "Liu and Wright", "year": 2014}, {"title": "An asynchronous parallel stochastic coordinate descent algorithm", "author": ["J. Liu", "S.J. Wright", "C. Re", "V. Bittorf"], "venue": "In ICML,", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "On the convergence of coordinate descent method for convex differentiable minimization", "author": ["Luo", "Zhi-Quan", "Tseng", "Paul"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Luo et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Luo et al\\.", "year": 1992}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Nesterov", "Yurii E"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nesterov and E.,? \\Q2012\\E", "shortCiteRegEx": "Nesterov and E.", "year": 2012}, {"title": "HOGWILD!: a lock-free approach to parallelizing stochastic gradient descent", "author": ["Niu", "Feng", "Recht", "Benjamin", "R\u00e9", "Christopher", "Wright", "Stephen J"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Niu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Niu et al\\.", "year": 2011}, {"title": "Parallel coordinate descent methods for big data optimization", "author": ["Richt\u00e1rik", "Peter", "Tak\u00e1\u010d", "Martin"], "venue": "Mathematical Programming,", "citeRegEx": "Richt\u00e1rik et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Richt\u00e1rik et al\\.", "year": 2012}, {"title": "On the nonasymptotic convergence of cyclic coordinate descent methods", "author": ["Saha", "Ankan", "Tewari", "Ambuj"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Saha et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saha et al\\.", "year": 2013}, {"title": "Feature clustering for accelerating parallel coordinate descent", "author": ["C. Scherrer", "A. Tewari", "M. Halappanavar", "D. Haglin"], "venue": "In NIPS,", "citeRegEx": "Scherrer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Scherrer et al\\.", "year": 2012}, {"title": "Pegasos: primal estimated sub-gradient solver for SVM", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": "In ICML,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2007}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2013}, {"title": "Iteration complexity of feasible descent methods for convex optimization", "author": ["Wang", "Po-Wei", "Lin", "Chih-Jen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Trading computation for communication: Distributed stochastic dual coordinate ascent", "author": ["T. Yang"], "venue": "In NIPS,", "citeRegEx": "Yang,? \\Q2013\\E", "shortCiteRegEx": "Yang", "year": 2013}, {"title": "Dual coordinate descent methods for logistic regression and maximum entropy models", "author": ["Yu", "Hsiang-Fu", "Huang", "Fang-Lan", "Lin", "Chih-Jen"], "venue": "Machine Learning,", "citeRegEx": "Yu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2011}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["Zhang", "Tong"], "venue": "In Proceedings of the 21th International Conference on Machine Learning (ICML),", "citeRegEx": "Zhang and Tong.,? \\Q2004\\E", "shortCiteRegEx": "Zhang and Tong.", "year": 2004}], "referenceMentions": [{"referenceID": 16, "context": "Problem (1) is usually called the primal problem, and can usually be solved by Stochastic Gradient Descent (SGD) (Zhang, 2004; Shalev-Shwartz et al., 2007), second order methods (Lin et al.", "startOffset": 113, "endOffset": 155}, {"referenceID": 4, "context": ", 2007), or primal coordinate descent algorithms (Chang et al., 2008; Huang et al., 2009).", "startOffset": 49, "endOffset": 89}, {"referenceID": 3, "context": "The first thread focuses on synchronized algorithms, including synchronized CD (Richt\u00e1rik & Tak\u00e1\u010d, 2012; Bradley et al., 2011) and synchronized DCD algorithms (Yang, 2013; Jaggi et al.", "startOffset": 79, "endOffset": 126}, {"referenceID": 19, "context": ", 2011) and synchronized DCD algorithms (Yang, 2013; Jaggi et al., 2014).", "startOffset": 40, "endOffset": 72}, {"referenceID": 6, "context": ", 2011) and synchronized DCD algorithms (Yang, 2013; Jaggi et al., 2014).", "startOffset": 40, "endOffset": 72}, {"referenceID": 9, "context": "To overcome this problem, the other thread of work focuses on asynchronous CD algorithms in multi-core shared memory systems (Liu & Wright, 2014; Liu et al., 2014).", "startOffset": 125, "endOffset": 163}, {"referenceID": 4, "context": "By maintaining w in memory, Hsieh et al. (2008); Keerthi et al.", "startOffset": 28, "endOffset": 48}, {"referenceID": 4, "context": "By maintaining w in memory, Hsieh et al. (2008); Keerthi et al. (2008) showed that the time complexity of each coordinate update can be reduced from O(nnz) to O(nnz/n), where nnz is number of nonzeros in the training dataset.", "startOffset": 28, "endOffset": 71}, {"referenceID": 5, "context": "Recently it has enjoyed renewed interest due to the success of \u201cstochastic\u201d coordinate descent in real applications (Hsieh et al., 2008; Nesterov, 2012).", "startOffset": 116, "endOffset": 152}, {"referenceID": 5, "context": "Many recent papers (Hsieh et al., 2008; Yu et al., 2011; ShalevShwartz & Zhang, 2013) have shown that solving the dual problem using coordinate descent algorithms is faster on large-scale datasets.", "startOffset": 19, "endOffset": 85}, {"referenceID": 20, "context": "Many recent papers (Hsieh et al., 2008; Yu et al., 2011; ShalevShwartz & Zhang, 2013) have shown that solving the dual problem using coordinate descent algorithms is faster on large-scale datasets.", "startOffset": 19, "endOffset": 85}, {"referenceID": 9, "context": "By maintaining a primal solutionw while updating dual variables, our algorithm is much faster than the previous asynchronous coordinate descent methods of (Liu & Wright, 2014; Liu et al., 2014) for solving the dual problem (2).", "startOffset": 155, "endOffset": 193}, {"referenceID": 19, "context": "Our algorithms are also faster than synchronized dual coordinate descent methods (Yang, 2013; Jaggi et al., 2014) since the latest values ofw can be accessed by all the threads.", "startOffset": 81, "endOffset": 113}, {"referenceID": 6, "context": "Our algorithms are also faster than synchronized dual coordinate descent methods (Yang, 2013; Jaggi et al., 2014) since the latest values ofw can be accessed by all the threads.", "startOffset": 81, "endOffset": 113}, {"referenceID": 3, "context": "Recently it has enjoyed renewed interest due to the success of \u201cstochastic\u201d coordinate descent in real applications (Hsieh et al., 2008; Nesterov, 2012). In terms of theoretical analysis, the convergence of (cyclic) coordinate descent has been studied for a long time (Luo & Tseng, 1992; Bertsekas, 1999), and the global linear convergence is presented recently under certain condition (Saha & Tewari, 2013; Wang & Lin, 2014). Stochastic Dual Coordinate Descent. Many recent papers (Hsieh et al., 2008; Yu et al., 2011; ShalevShwartz & Zhang, 2013) have shown that solving the dual problem using coordinate descent algorithms is faster on large-scale datasets. The success of SDCD strongly relies on exploiting the primal-dual relationship (3) to speed up the gradient computation in the dual space. DCD has become the state-of-the-art solver implemented in LIBLINEAR (Fan et al., 2008). In terms of convergence of dual objective function, some standard theoretical guarantees for coordinate descent can be directly applied. Different from standard analysis, Shalev-Shwartz & Zhang (2013) presented the convergence rate in terms of duality gap.", "startOffset": 117, "endOffset": 1089}, {"referenceID": 3, "context": "Recently it has enjoyed renewed interest due to the success of \u201cstochastic\u201d coordinate descent in real applications (Hsieh et al., 2008; Nesterov, 2012). In terms of theoretical analysis, the convergence of (cyclic) coordinate descent has been studied for a long time (Luo & Tseng, 1992; Bertsekas, 1999), and the global linear convergence is presented recently under certain condition (Saha & Tewari, 2013; Wang & Lin, 2014). Stochastic Dual Coordinate Descent. Many recent papers (Hsieh et al., 2008; Yu et al., 2011; ShalevShwartz & Zhang, 2013) have shown that solving the dual problem using coordinate descent algorithms is faster on large-scale datasets. The success of SDCD strongly relies on exploiting the primal-dual relationship (3) to speed up the gradient computation in the dual space. DCD has become the state-of-the-art solver implemented in LIBLINEAR (Fan et al., 2008). In terms of convergence of dual objective function, some standard theoretical guarantees for coordinate descent can be directly applied. Different from standard analysis, Shalev-Shwartz & Zhang (2013) presented the convergence rate in terms of duality gap. Parallel Stochastic Coordinate Descent. In order to conduct coordinate updates in parallel, Richt\u00e1rik & Tak\u00e1\u010d (2012) studied the algorithm where each processor updates a randomly selected block (or coordinate) simultaneously, and Bradley et al.", "startOffset": 117, "endOffset": 1262}, {"referenceID": 2, "context": "In order to conduct coordinate updates in parallel, Richt\u00e1rik & Tak\u00e1\u010d (2012) studied the algorithm where each processor updates a randomly selected block (or coordinate) simultaneously, and Bradley et al. (2011) proposed a similar algorithm for `1-regularized problems.", "startOffset": 190, "endOffset": 212}, {"referenceID": 2, "context": "In order to conduct coordinate updates in parallel, Richt\u00e1rik & Tak\u00e1\u010d (2012) studied the algorithm where each processor updates a randomly selected block (or coordinate) simultaneously, and Bradley et al. (2011) proposed a similar algorithm for `1-regularized problems. Scherrer et al. (2012) studied parallel greedy coordinate descent.", "startOffset": 190, "endOffset": 293}, {"referenceID": 2, "context": "In order to conduct coordinate updates in parallel, Richt\u00e1rik & Tak\u00e1\u010d (2012) studied the algorithm where each processor updates a randomly selected block (or coordinate) simultaneously, and Bradley et al. (2011) proposed a similar algorithm for `1-regularized problems. Scherrer et al. (2012) studied parallel greedy coordinate descent. However, the above synchronized methods usually face a trade-off in choosing the block size. If the block size is small, the load balancing problem leads to slow running time. If the block size is large, the convergence speed becomes much slower or the algorithm even diverges. These problems can be resolved by developing an asynchronous algorithm. Asynchronous coordinate descent has been studied by (Bertsekas & Tsitsiklis, 1989), but they require the Hessian to be diagonal dominant in order to establish the convergence. Recently, Liu et al. (2014); Liu & Wright (2014) proved linear convergence of asynchronous stochastic coordinate descent algorithms under the essential strong convexity condition and a \u201cbounded staleness\u201d condition, where they consider both \u201cconsistent read\u201d and \u201cinconsistent read\u201d models.", "startOffset": 190, "endOffset": 891}, {"referenceID": 2, "context": "In order to conduct coordinate updates in parallel, Richt\u00e1rik & Tak\u00e1\u010d (2012) studied the algorithm where each processor updates a randomly selected block (or coordinate) simultaneously, and Bradley et al. (2011) proposed a similar algorithm for `1-regularized problems. Scherrer et al. (2012) studied parallel greedy coordinate descent. However, the above synchronized methods usually face a trade-off in choosing the block size. If the block size is small, the load balancing problem leads to slow running time. If the block size is large, the convergence speed becomes much slower or the algorithm even diverges. These problems can be resolved by developing an asynchronous algorithm. Asynchronous coordinate descent has been studied by (Bertsekas & Tsitsiklis, 1989), but they require the Hessian to be diagonal dominant in order to establish the convergence. Recently, Liu et al. (2014); Liu & Wright (2014) proved linear convergence of asynchronous stochastic coordinate descent algorithms under the essential strong convexity condition and a \u201cbounded staleness\u201d condition, where they consider both \u201cconsistent read\u201d and \u201cinconsistent read\u201d models.", "startOffset": 190, "endOffset": 912}, {"referenceID": 0, "context": "Avron et al. (2014) showed linear rate of convergence for the asynchronous randomized Gaussian-Seidel updates, which is a special case of coordinate descent on linear systems.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Avron et al. (2014) showed linear rate of convergence for the asynchronous randomized Gaussian-Seidel updates, which is a special case of coordinate descent on linear systems. Parallel Stochastic Dual Coordinate Descent. For solving (5), each coordinate updates only requires the global primal variables w and one local dual variable \u03b1i, thus algorithms only need to synchronize w. Based on this observation, Yang (2013) proposed to update several coordinates or blocks simultaneously and update the globalw, and Jaggi et al.", "startOffset": 0, "endOffset": 421}, {"referenceID": 0, "context": "Avron et al. (2014) showed linear rate of convergence for the asynchronous randomized Gaussian-Seidel updates, which is a special case of coordinate descent on linear systems. Parallel Stochastic Dual Coordinate Descent. For solving (5), each coordinate updates only requires the global primal variables w and one local dual variable \u03b1i, thus algorithms only need to synchronize w. Based on this observation, Yang (2013) proposed to update several coordinates or blocks simultaneously and update the globalw, and Jaggi et al. (2014) showed that each block can be solved with other approaches under the same framework.", "startOffset": 0, "endOffset": 533}, {"referenceID": 20, "context": "For SVM problems, the subproblem has a closed form solution, while for logistic regression problems it has to be solved by an iterative solver (see Yu et al. (2012) for details).", "startOffset": 148, "endOffset": 165}, {"referenceID": 12, "context": "However, as shown in (Niu et al., 2011; Liu & Wright, 2014), the effect of using slightly stale values is usually limited in practice.", "startOffset": 21, "endOffset": 59}, {"referenceID": 5, "context": "Based on this property, a shrinking strategy was proposed by Hsieh et al. (2008) to further speed up DCD.", "startOffset": 61, "endOffset": 81}, {"referenceID": 2, "context": "We list several important machine learning problems that admit global error bounds: \u2022 Support Vector Machines (SVM) with hinge loss (Boser et al., 1992): `i(zi) = C max(1\u2212 zi, 0) `i (\u2212\u03b1i) = { \u2212\u03b1i if 0 \u2264 \u03b1i \u2264 C, \u221e otherwise.", "startOffset": 132, "endOffset": 152}, {"referenceID": 6, "context": "\u2022 CoCoA: We implement a multi-core version of CoCoA (Jaggi et al., 2014) with \u03b2K = 1 and DCD as its local dual method.", "startOffset": 52, "endOffset": 72}, {"referenceID": 9, "context": "\u2022 AsySCD: We follow the description in (Liu & Wright, 2014; Liu et al., 2014) to implement AsySCD with the step length \u03b3 = 1 2 and the shuffling period p = 10 as suggested in (Liu et al.", "startOffset": 39, "endOffset": 77}, {"referenceID": 9, "context": ", 2014) to implement AsySCD with the step length \u03b3 = 1 2 and the shuffling period p = 10 as suggested in (Liu et al., 2014).", "startOffset": 105, "endOffset": 123}, {"referenceID": 9, "context": "\u2022 From Figure 2(d), we can see that AsySCD does not have any \u201cspeedup\u201d over the serial reference, although it is shown to have almost linear scaling (Liu et al., 2014; Liu & Wright, 2014).", "startOffset": 149, "endOffset": 187}], "year": 2015, "abstractText": "Stochastic Dual Coordinate Descent (DCD) has become one of the most efficient ways to solve the family of `2-regularized empirical risk minimization problems, including linear SVM, logistic regression, and many others. The vanilla implementation of DCD is quite slow; however, by maintaining primal variables while updating dual variables, the time complexity of DCD can be significantly reduced. Such a strategy forms the core algorithm in the widely-used LIBLINEAR package. In this paper, we parallelize the DCD algorithms in LIBLINEAR. In recent research, several synchronized parallel DCD algorithms have been proposed, however, they fail to achieve good speedup in the shared memory multi-core setting. In this paper, we propose a family of asynchronous stochastic dual coordinate descent algorithms (PASSCoDe). Each thread repeatedly selects a random dual variable and conducts coordinate updates using the primal variables that are stored in the shared memory. We analyze the convergence properties when different locking/atomic mechanisms are applied. For implementation with atomic operations, we show linear convergence under mild conditions. For implementation without any atomic operations or locking, we present the first backward error analysis for PASSCoDe under the multi-core environment, showing that the converged solution is the exact solution for a primal problem with perturbed regularizer. Experimental results show that our methods are much faster than previous parallel coordinate descent solvers.", "creator": "LaTeX with hyperref package"}}}