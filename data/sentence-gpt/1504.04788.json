{"id": "1504.04788", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2015", "title": "Compressing Neural Networks with the Hashing Trick", "abstract": "As deep nets are increasingly used in applications suited for mobile devices, a fundamental dilemma becomes apparent: the trend in deep learning is to grow models to absorb ever-increasing data set sizes; however mobile devices are designed with very little memory and cannot store such large models. We present a novel network architecture, HashedNets, that exploits inherent redundancy in neural networks to achieve drastic reductions in model sizes. HashedNets uses a low-cost hash function to randomly group connection weights into hash buckets, and all connections within the same hash bucket share a single parameter value. These parameters are tuned to adjust to the HashedNets weight sharing architecture with standard backprop during training. Our hashing procedure introduces no additional memory overhead, and we demonstrate on several benchmark data sets that HashedNets shrink the storage requirements of neural networks substantially while mostly preserving generalization performance. For each data set, we obtain a single parameter value for each layer of a network, or to add additional memory for each layer. Our implementation is a low-cost hash function that performs very little computation. In order to produce a large hash function in a high-resolution network, we employ a small, parallel-network architecture that avoids the need for parallel networks. We utilize a low-cost hash function to calculate the hash size of the input network and the network's data structure. We provide a unique algorithm for generating the hash size of the input network.\n\n\n\n\n\nThe HashedNets algorithm is used to generate and store deep learning models. We use the network algorithm to generate neural network and the neural network's information on the network.\nIn addition to the neural network layer, the neural network layer is used to collect deep learning models.\nWe also construct a set of network architecture to perform deep learning and image analysis. The network architecture utilizes only a series of memory management, which in turn enables the user to learn more about how the network is structured.\nWe also use the network algorithm to collect deep learning models. The network architecture uses only a series of memory management, which in turn enables the user to learn more about how the network is structured.\nThe network architecture uses only a series of memory management, which in turn enables the user to learn more about how the network is structured.\nThe network architecture uses only a series of memory management, which in turn enables the user to learn more about how the network is structured. The network architecture uses only a series of memory management, which in turn enables the user to learn more about how the network", "histories": [["v1", "Sun, 19 Apr 2015 04:24:15 GMT  (503kb,D)", "http://arxiv.org/abs/1504.04788v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["wenlin chen", "james t wilson", "stephen tyree", "kilian q weinberger", "yixin chen"], "accepted": true, "id": "1504.04788"}, "pdf": {"name": "1504.04788.pdf", "metadata": {"source": "META", "title": "Compressing Neural Networks with the Hashing Trick", "authors": ["Wenlin Chen", "James T. Wilson", "Stephen Tyree", "Kilian Q. Weinberger", "Yixin Chen"], "emails": ["WENLINCHEN@WUSTL.EDU", "J.WILSON@WUSTL.EDU", "STYREE@NVIDIA.COM", "KILIAN@WUSTL.EDU", "CHEN@CSE.WUSTL.EDU"], "sections": [{"heading": "1. Introduction", "text": "In the past decade deep neural networks have set new performance standards in many high-impact applications. These include object classification (Krizhevsky et al., 2012; Sermanet et al., 2013), speech recognition (Hinton et al., 2012), image caption generation (Vinyals et al., 2014; Karpathy & Fei-Fei, 2014) and domain adaptation (Glorot et al., 2011b). As data sets increase in size, so do the number of parameters in these neural networks in order to absorb the enormous amount of supervision (Coates et al., 2013). Increasingly, these networks are trained on industrial-sized clusters (Le, 2013) or high-performance graphics processing units (GPUs) (Coates et al., 2013).\nSimultaneously, there has been a second trend as applications of machine learning have shifted toward mobile and embedded devices. As examples, modern smart phones are increasingly operated through speech recognition (Schuster, 2010), robots and self-driving cars perform object recognition in real time (Montemerlo et al., 2008), and medical devices collect and analyze patient data (Lee & Verma, 2013). In contrast to GPUs or computing clusters, these devices are designed for low power consumption and long battery life. Most importantly, they typically have small working memory. For example, even the top-of-theline iPhone 6 only features a mere 1GB of RAM.1\nThe disjunction between these two trends creates a dilemma when state-of-the-art deep learning algorithms are designed for deployment on mobile devices. While it is possible to train deep nets offline on industrial-sized clusters (server-side), the sheer size of the most effective models would exceed the available memory, making it prohibitive to perform testing on-device. In speech recognition, one common cure is to transmit processed voice recordings to a computation center, where the voice recognition is performed server-side (Chun & Maniatis, 2009). This approach is problematic, as it only works when sufficient bandwidth is available and incurs artificial delays through network traffic (Kosner, 2012). One solution is to train small models for the on-device classification; however, these tend to significantly impact accuracy (Chun & Maniatis, 2009), leading to customer frustration.\nThis dilemma motivates neural network compression. Recent work by Denil et al. (2013) demonstrates that there is a surprisingly large amount of redundancy among the weights of neural networks. The authors show that a small subset of the weights are sufficient to reconstruct the entire network. They exploit this by training low-rank decompositions of the weight matrices. Ba & Caruana (2014) show that deep neural networks can be successfully compressed\n1http://en.wikipedia.org/wiki/IPhone_6\nar X\niv :1\n50 4.\n04 78\n8v 1\n[ cs\n.L G\n] 1\n9 A\npr 2\ninto \u201cshallow\u201d single-layer neural networks by training the small network on the (log-) outputs of the fully trained deep network (Bucilu et al., 2006). Courbariaux et al. (2014) train neural networks with reduced bit precision, and, long predating this work, LeCun et al. (1989) investigated dropping unimportant weights in neural networks. In summary, the accumulated evidence suggests that much of the information stored within network weights may be redundant.\nIn this paper we propose HashedNets, a novel network architecture to reduce and limit the memory overhead of neural networks. Our approach is compellingly simple: we use a hash function to group network connections into hash buckets uniformly at random such that all connections grouped to the ith hash bucket share the same weight value wi. Our parameter hashing is akin to prior work in feature hashing (Weinberger et al., 2009; Shi et al., 2009; Ganchev & Dredze, 2008) and is similarly fast and requires no additional memory overhead. The backpropagation algorithm (LeCun et al., 2012) can naturally tune the hash bucket parameters and take into account the random weight sharing within the neural network architecture.\nWe demonstrate on several real world deep learning benchmark data sets that HashedNets can drastically reduce the model size of neural networks with little impact in prediction accuracy. Under the same memory constraint, HashedNets have more adjustable free parameters than the lowrank decomposition methods suggested by Denil et al. (2013), leading to smaller drops in descriptive power.\nSimilarly, we also show that for a finite set of parameters it is beneficial to \u201cinflate\u201d the network architecture by reusing each parameter value multiple times. Best results are achieved when networks are inflated by a factor 8\u201316\u00d7. The \u201cinflation\u201d of neural networks with HashedNets imposes no restrictions on other network architecture design choices, such as dropout regularization (Srivastava et al., 2014), activation functions (Glorot et al., 2011a; LeCun et al., 2012), or weight sparsity (Coates et al., 2011)."}, {"heading": "2. Feature Hashing", "text": "Learning under memory constraints has previously been explored in the context of large-scale learning for sparse data sets. Feature hashing (or the hashing trick) (Weinberger et al., 2009; Shi et al., 2009) is a technique to map high-dimensional text documents directly into bag-ofword (Salton & Buckley, 1988) vectors, which would otherwise require use of memory consuming dictionaries for storage of indices corresponding with specific input terms.\nFormally, an input vector x \u2208 Rd is mapped into a feature space with a mapping function \u03c6 :Rd \u2192 Rk where k d. The mapping \u03c6 is based on two (approximately uniform) hash functions h :N\u2192 {1, . . . , k} and \u03be :N\u2192 {\u22121,+1}\nand the kth dimension of the hashed input x is defined as \u03c6k(x) = \u2211 i:h(i)=k xi\u03be(i).\nThe hashing trick leads to large memory savings for two reasons: it can operate directly on the input term strings and avoids the use of a dictionary to translate words into vectors; and the parameter vector of a learning model lives within the much smaller dimensional Rk instead of Rd. The dimensionality reduction comes at the cost of collisions, where multiple words are mapped into the same dimension. This problem is less severe for sparse data sets and can be counteracted through multiple hashing (Shi et al., 2009) or larger hash tables (Weinberger et al., 2009).\nIn addition to memory savings, the hashing trick has the appealing property of being sparsity preserving, fast to compute and storage-free. The most important property of the hashing trick is, arguably, its (approximate) preservation of inner product operations. The second hash function, \u03be, guarantees that inner products are unbiased in expectation (Weinberger et al., 2009); that is,\nE[\u03c6(x)>\u03c6(x\u2032)]\u03c6 = x>x\u2032. (1)\nFinally, Weinberger et al. (2009) also show that the hashing trick can be used to learn multiple classifiers within the same hashed space. In particular, the authors use it for multi-task learning and define multiple hash functions \u03c61, . . . , \u03c6T , one for each task, that map inputs for their respective tasks into one joint space. Let w1, . . . ,wT denote the weight vectors of the respective learning tasks, then if t\u2032 6= t a classifier for task t\u2032 does not interfere with a hashed input for task t; i.e. w>t \u03c6t\u2032(x) \u2248 0."}, {"heading": "3. Notation", "text": "Throughout this paper we type vectors in bold (x), scalars in regular (C or b) and matrices in capital bold (X). Specific entries in vectors or matrices are scalars and follow the corresponding convention, i.e. the ith dimension of vector x is xi and the (i, j)th entry of matrix V is Vij .\nFeed Forward Neural Networks. We define the forward propagation of the `th layer in a neural networks as,\na`+1i = f(z `+1 i ), where z `+1 i = n`\u2211 j=0 V `ija ` j , (2)\nwhere V` is the (virtual) weight matrix in the `th layer. The vectors z`, a` \u2208 Rn` denote the activation units before and after transformation through the transition function f(\u00b7). Typical activation functions are rectifier linear unit (ReLU) (Nair & Hinton, 2010), sigmoid or tanh (LeCun et al., 2012)."}, {"heading": "4. HashedNets", "text": "In this section we present HashedNets, a novel variation of neural networks with drastically reduced model sizes (and memory demands). We first introduce our approach as a method of random weight sharing across the network connections and then describe how to facilitate it with the hashing trick to avoid any additional memory overhead."}, {"heading": "4.1. Random weight sharing", "text": "In a standard fully-connected neural network, there are (n`+1)\u00d7n`+1 weighted connections between a pair of layers, each with a corresponding free parameter in the weight matrix V`. We assume a finite memory budget per layer, K` (n` + 1)\u00d7 n`+1, that cannot be exceeded. The obvious solution is to fit the neural network within budget by reducing the number of nodes n`, n`+1 in layers `, `+ 1 or by reducing the bit precision of the weight matrices (Courbariaux et al., 2014). However if K` is sufficiently small, both approaches significantly reduce the ability of the neural network to generalize (see Section 6). Instead, we propose an alternative: we keep the size of V` untouched but reduce its effective memory footprint through weight sharing. We only allow exactly K` different weights to occur within V`, which we store in a weight vector w` \u2208 RK` . The weights within w` are shared across multiple randomly chosen connections within V`. We refer to the resulting matrix V` as virtual, as its size could be increased (i.e. nodes are added to hidden layer) without increasing the actual number of parameters of the neural network.\nFigure 1 shows a neural network with one hidden layer, four input units and two output units. Connections are randomly grouped into three categories per layer and their weights are shown in the virtual weight matrices V1 and V2. Connections belonging to the same color share the same weight value, which are stored in w1 and w2, respectively. Overall, the entire network is compressed by a factor 1/4, i.e. the 24 weights stored in the virtual matrices V1 and V2 are reduced to only six real values in w1 and w2. On data with four input dimensions and two output dimensions, a conventional neural network with six weights would be restricted to a single (trivial) hidden unit."}, {"heading": "4.2. Hashed Neural Nets (HashedNets)", "text": "A na\u0131\u0308ve implementation of random weight sharing can be trivially achieved by maintaining a secondary matrix consisting of each connection\u2019s group assignment. Unfortunately, this explicit representation places an undesirable limit on potential memory savings.\nWe propose to implement the random weight sharing assignments using the hashing trick. In this way, the shared weight of each connection is determined by a hash function\nthat requires no storage cost with the model. Specifically, we assign to V `ij an element of w` indexed by a hash function h`(i, j), as follows:\nV `ij = w ` h`(i,j), (3)\nwhere the (approximately uniform) hash function h`(\u00b7, \u00b7) maps a key (i, j) to a natural number within {1, . . . ,K`}. In the example of Figure 1, h1(2, 1) = 1 and therefore V 12,1=w\n1=3.2. For our experiments we use the opensource implementation xxHash.2"}, {"heading": "4.3. Feature hashing versus weight sharing", "text": "This section focuses on a single layer throughout and to simplify notation we will drop the super-scripts `. We will denote the input activation as a = a` \u2208Rm of dimensionality m=n`. We denote the output as z= z`+1 \u2208Rn with dimensionality n=n`+1.\nTo facilitate weight sharing within a feed forward neural network, we can simply substitute Eq. (3) into Eq. (2):\nzi = m\u2211 j=1 Vijaj = m\u2211 j=1 wh(i,j)aj . (4)\nAlternatively and more in line with previous work (Weinberger et al., 2009), we may interpret HashedNets in terms of feature hashing. To compute zi, we first hash the activations from the previous layer, a, with the hash mapping function \u03c6i(\u00b7) :Rm \u2192 RK . We then compute the inner product between the hashed representation \u03c6i(a) and the parameter vector w,\nzi = w>\u03c6i(a). (5) 2https://code.google.com/p/xxhash/\nBoth w and \u03c6i(a) areK-dimensional, whereK is the number of hash buckets in this layer. The hash mapping function \u03c6i is defined as follows. The kth element of \u03c6i(a), i.e. [\u03c6i(a)]k, is the sum of variables hashed into bucket k:\n[\u03c6i(a)]k = \u2211\nj:h(i,j)=k\naj . (6)\nStarting from Eq. (5), we show that the two interpretations (Eq. (4) and (5)) are equivalent:\nzi = K\u2211 k=1 wk [\u03c6i(a)]k = K\u2211 k=1 wk \u2211 j:h(i,j)=k aj\n= m\u2211 j=1 K\u2211 k=1 wkaj\u03b4[h(i,j)=k]\n= m\u2211 j=1 wh(i,j)aj .\nThe final term is equivalent to Eq. (4).\nSign factor. With this equivalence between random weight sharing and feature hashing on input activations, HashedNets inherit several beneficial properties of the feature hashing. Weinberger et al. (2009) introduce an additional sign factor \u03be(i, j) to remove the bias of hashed inner-products due to collisions. For the same reasons we multiply (3) by the sign factor \u03be(i, j) for parameterizing V (Weinberger et al., 2009):\nVij = wh(i,j)\u03be(i, j), (7)\nwhere \u03be(i, j) : N \u2192 \u00b11 is a second hash function independent of h. Incorporating \u03be(i, j) to feature hashing and weight sharing does not change the equivalence between them as the proof in the previous section still holds with the sign term (details omitted for improved readability).\nSparsity. As pointed out in Shi et al. (2009) and Weinberger et al. (2009), feature hashing is most effective on sparse feature vectors since the number of hash collisions is minimized. We can encourage this effect in the hidden layers with sparsity inducing transition functions, e.g. rectified linear units (ReLU) (Glorot et al., 2011a) or through specialized regularization (Chen et al., 2014; Boureau et al., 2008). In our implementation, we use ReLU transition functions throughout, as they have also been shown to often result in superior generalization performance in addition to their sparsity inducing properties (Glorot et al., 2011a).\nAlternative neural network architectures. While this work focuses on general, fully connected feed forward neural networks, the technique of HashedNets could naturally\nbe extended to other kinds of neural networks, such as recurrent neural networks (Pineda, 1987) or others (Bishop, 1995). It can also be used in conjunction with other approaches for neural network compression. All weights can be stored with low bit precision (Courbariaux et al., 2014; Gupta et al., 2015), edges could be removed (Cires\u0327an et al., 2011) and HashedNets can be trained on the outputs of larger networks (Ba & Caruana, 2014) \u2014 yielding further reductions in memory requirements."}, {"heading": "4.4. Training HashedNets", "text": "Training HashedNets is equivalent to training a standard neural network with equality constraints for weight sharing. Here, we show how to (a) compute the output of a hash layer during the feed-forward phase, (b) propagate gradients from the output layer back to input layer, and (c) compute the gradient over the shared weights w` during the back propagation phase. We use dedicated hash functions between layers ` and `+ 1, and denote them as h` and \u03be`.\nOutput. Adding the hash functions h`(\u00b7, \u00b7) and \u03be`(\u00b7) and the weight vectors w` into the feed forward update (2) results in the following forward propagation rule:\na`+1i = f  n`\u2211 j w`h`(i,j)\u03be `(i, j)a`j  . (8) Error term. Let L denote the loss function for training the neural network, e.g. cross entropy or the quadratic loss (Bishop, 1995). Further, let \u03b4`j denote the gradient of L over activation j in layer `, also known as the error term. Without shared weights, the error term can be expressed as \u03b4`j = (\u2211n`+1 i=1 V ` ij\u03b4 `+1 i ) f \u2032(z`j), where f\n\u2032(\u00b7) represents the first derivative of the transition function f(\u00b7). If we substitute Eq. (7) into the error term we obtain:\n\u03b4`j = n`+1\u2211 i=1 \u03be`(i, j)w`h`(i,j)\u03b4 `+1 i  f \u2032(z`j). (9) Gradient over parameters. To compute the gradient of L with respect to a weight w`k we need the two gradients,\n\u2202L \u2202V `ij = a`j\u03b4 `+1 i and \u2202V `ij \u2202w`k = \u03be`(i, j)\u03b4h`(i,j)=k. (10)\nHere, the first gradient is the standard gradient of a (virtual) weight with respect to an activation unit and the second gradient ties the virtual weight matrix to the actual weights\nthrough the hashed map. Combining these two, we obtain\n\u2202L \u2202w`k = \u2211 i,j \u2202L \u2202V `ij \u2202V `ij \u2202w`k\n(11)\n= n`+1\u2211 i=1 \u2211 j a`j\u03b4 `+1 i \u03be `(i, j)\u03b4h`(i,j)=k. (12)"}, {"heading": "5. Related Work", "text": "Deep neural networks have achieved great progress on a wide variety of real-world applications, including image classification (Krizhevsky et al., 2012; Donahue et al., 2013; Sermanet et al., 2013; Zeiler & Fergus, 2014), object detection (Girshick et al., 2014; Vinyals et al., 2014), image retrieval (Razavian et al., 2014), speech recognition (Hinton et al., 2012; Graves et al., 2013; Mohamed et al., 2011), and text representation (Mikolov et al., 2013).\nThere have been several previous attempts to reduce the complexity of neural networks under a variety of contexts. Arguably the most popular method is the widely used convolutional neural network (Simard et al., 2003). In the convolutional layers, the same filter is applied to every receptive field, both reducing model size and improving generalization performance. The incorporation of pooling layers (Zeiler & Fergus, 2013) can reduce the number of connections between layers in domains exhibiting locality among input features, such as images. Autoencoders (Glorot et al., 2011b) share the notion of tied weights by using the same weights for the encoder and decoder (up to transpose).\nOther methods have been proposed explicitly to reduce the number of free parameters in neural networks, but not necessarily for reducing memory overhead. Nowlan & Hinton (1992) introduce soft weight sharing for regularization in which the distribution of weight values is modeled as a Gaussian mixture. The weights are clustered such that weights in the same group have similar values. Since weight values are unknown before training, weights are clustered during training. This approach is fundamentally different from HashedNets since it requires auxiliary parameters to record the group membership for every weight.\nInstead of sharing weights, LeCun et al. (1989) introduce \u201coptimal brain damage\u201d to directly drop unimportant weights. This approach requires auxiliary parameters for storing the sparse weights and needs retraining time to fine-tune the resulting architecture. Cires\u0327an et al. (2011) demonstrate in their experiments that randomly removing connections leads to superior empirical performance, which shares the same spirit of HashedNets.\nCourbariaux et al. (2014) and Gupta et al. (2015) learn networks with reduced numerical precision for storing model parameters (e.g. 16-bit fixed-point representation\n(Gupta et al., 2015) for a compression factor of 14 over double-precision floating point). Experiments indicate little reduction in accuracy compared with models trained with double-precision floating point representation. These methods can be readily incorporated with HashedNets, potentially yielding further reduction in model storage size.\nA recent study by Denil et al. (2013) demonstrates significant redundancy in neural network parameters by directly learning a low-rank decomposition of the weight matrix within each layer. They demonstrate that networks composed of weights recovered from the learned decompositions are only slightly less accurate than networks with all weights as free parameters, indicating heavy overparametrization in full weight matrices. A follow-up work by Denton et al. (2014) uses a similar technique to speed up test-time evaluation of convolutional neural networks. The focus of this line of work is not on reducing storage and memory overhead, but evaluation speed during test time. HashedNets is complementary to this research, and the two approaches could be used in combination.\nFollowing the line of model compression, Bucilu et al. (2006), Hinton et al. (2014) and Ba & Caruana (2014) recently introduce approaches to learn a \u201cdistilled\u201d model, training a more compact neural network to reproduce the output of a larger network. Specifically, Hinton et al. (2014) and Ba & Caruana (2014) train a large network on the original training labels, then learn a much smaller \u201cdistilled\u201d model on a weighted combination of the original labels and the (softened) softmax output of the larger model. The authors show that the distilled model has better generalization ability than a model trained on just the labels. In our experimental results, we show that our approach is complementary by learning HashedNets with soft targets. Rippel et al. (2014) propose a novel dropout method, nested dropout, to give an order of importance for hidden neurons. Hypothetically, less important hidden neurons could be removed after training, a method orthogonal to HashedNets.\nGanchev & Dredze (2008) are among the first to recognize the need to reduce the size of natural language processing models to accommodate mobile platform with limited memory and computing power. They propose random feature mixing to group features at random based on a hash function, which dramatically reduces both the number of features and the number of parameters. With the help of feature hashing (Weinberger et al., 2009), Vowpal Wabbit, a large-scale learning system, is able to scale to terafeature datasets (Agarwal et al., 2014)."}, {"heading": "6. Experimental Results", "text": "We conduct extensive experiments to evaluate HashedNets on eight benchmark datasets. For full reproducibility, our\ncode is available at http://www.weinbergerweb.com.\nDatasets. Datasets consist of the original MNIST handwritten digit dataset, along with four challenging variants (Larochelle et al., 2007). Each variation amends the original through digit rotation (ROT), background superimposition (BG-RAND and BG-IMG), or a combination thereof (BG-IMG-ROT). In addition, we include two binary image classification datasets: CONVEX and RECT (Larochelle et al., 2007). All data sets have pre-specified training and testing splits. Original MNIST has splits of sizes n=60000 (training) and n = 10000 (testing). Both CONVEX and RECT and as well as each MNIST variation set has n = 12000 (training) and n=50000 (testing).\nBaselines and method. We compare HashedNets with several existing techniques for size-constrained, feedforward neural networks. Random Edge Removal (RER) (Cires\u0327an et al., 2011) reduces the total number of model parameters by randomly removing weights prior to training. Low-Rank Decomposition (LRD) (Denil et al., 2013) decomposes the weight matrix into two low-rank matrices. One of these component matrices is fixed while the other is learned. Elements of the fixed matrix are generated according to a zero-mean Gaussian distribution with standard\ndeviation 1\u221a n` with n` inputs to the layer.\nEach model is compared against a standard neural network with an equivalent number of stored parameters, Neural Network (Equivalent-Size) (NN). For example, for a network with a single hidden layer of 1000 units and a storage compression factor of 110 , we adopt a size-equivalent baseline with a single hidden layer of 100 units. For deeper networks, all hidden layers are shrunk at the same rate until the number of stored parameters equals the target size. In a similar manner, we examine Dark Knowledge (DK) (Hinton et al., 2014; Ba & Caruana, 2014) by training a distilled model to optimize the cross entropy with both the original labels and soft targets generated by the corresponding full neural network (compression factor 1). The distilled model structure is chosen to be same as the \u201cequivalent-sized\u201d network (NN) at the corresponding compression rate.\nFinally, we examine our method under two settings: learning hashed weights with the original training labels (HashNet) and with combined labels and DK soft targets (HashNetDK). In all cases, memory and storage consumption is defined strictly in terms of free parameters. As such, we count the fixed low rank matrix in the Low-Rank Decomposition method as taking no memory or storage (pro-\nviding this baseline a slight advantage).\nExperimental setting. HashedNets and all accompanying baselines were implemented using Torch7 (Collobert et al., 2011) and run on NVIDIA GTX TITAN graphics cards with 2688 cores and 6GB of global memory. We use 32 bit precision throughout but note that the compression rates of all methods may be improved with lower precision (Courbariaux et al., 2014; Gupta et al., 2015). We verify all implementations by numerical gradient checking. Models are trained via stochastic gradient descent (minibatch size of 50) with dropout and momentum. ReLU is adopted as the activation function for all models. Hyperparameters are selected for all algorithms with Bayesian optimization (Snoek et al., 2012) and hand tuning on 20% validation splits of the training sets. We use the open source Bayesian Optimization MATLAB implementation \u201cbayesopt.m\u201d from Gardner et al. (2014).3\nResults with varying compression. Figures 2 and 3 show the performance of all methods on MNIST and the ROT variant with different compression factors on 3-layer (1 hidden layer) and 5-layer (3 hidden layers) neural networks, respectively. Each hidden layer contains 1000 hidden units. The x-axis in each figure denotes the fractional compression factor. For HashedNets and the low rank decomposition and random edge removal compression baselines, this means we fix the number of hidden units (n`) and\n3http://tinyurl.com/bayesopt\nvary the storage budget (K`) for the weights (w`).\nWe make several observations: The accuracy of HashNet and HashNetDK outperforms all other baseline methods, especially in the most interesting case when the compression factor is small (i.e. very small models). Both compression baseline algorithms, low rank decomposition and random edge removal, tend to not outperform a standard neural network with fewer hidden nodes (black line), trained with dropout. For smaller compression factors, random edge removal likely suffers due to a significant number of nodes being entirely disconnected from neighboring layers. The size-matched NN is consistently the best performing baseline, however its test error is significantly higher than that of HashNet especially at small compression rates. The use of Dark Knowledge training improves the performance of HashedNets and the standard neural network. Of all methods, only HashNet and HashNetDK maintain performance for small compression factors.\nFor completeness, we show the performance of all methods on all eight datasets in Table 1 for compression factor 18 and Table 2 for compression factor 1 64 . HashNet and HashNetDK outperform other baselines in most cases, especially when the compression factor is very small (Table 2). With a compression factor of 164 on average only 0.5 bits of information are stored per (virtual) parameter.\nResults with fixed storage. We also experiment with the setting where the model size is fixed and the virtual network architecture is \u201cinflated\u201d. Essentially we are fixing K` (the\nnumber of \u201creal\u201d weights in w`), and vary the number of hidden nodes (n`). An expansion factor of 1 denotes the case where every virtual weight has a corresponding \u201creal\u201d weight, (n` + 1)n`+1 =K`. Figure 4 shows the test error rate under various expansion rates of a network with one hidden layer (left) and three hidden layers (right). In both scenarios we fix the number of real weights to the size of a standard fully-connected neural network with 50 hidden units in each hidden layer whose test error is shown by the black dashed line.\nWith no expansion (at expansion rate 1), different compression methods perform differently. At this point edge removal is identical to a standard neural network and matches its results. If no expansion is performed, the HashNet performance suffers from collisions at no benefit. Similarly the low-rank method still randomly projects each layer to a random feature space with same dimensionality.\nFor expansion rates greater 1, all methods improve over the fixed-sized neural network. There is a general trend that more expansion decreases the test error until a \u201csweetspot\u201d after which additional expansion tends to hurt. The test error of the HashNet neural network decreases substantially through the introduction of more \u201cvirtual\u201d hidden nodes, despite that no additional parameters are added. In the case of the 5-layer neural network (right) this trend is maintained to an expansion factor of 16\u00d7, resulting in 800 \u201cvirtual\u201d nodes. One could hypothetically increase n` arbitrarily for HashNet, however, in the limit, too many hash collisions would result in increasingly similar gradient updates for all weights in w.\nThe benefit from expanding a network cannot continue forever. In the random edge removal the network will become very sparsely connected; the low-rank decomposition approach will eventually lead to a decomposition into rank1 matrices. HashNet also respects this trend, but is much less sensitive when the expansion goes up. Best results are achieved when networks are inflated by a factor 8\u221216\u00d7."}, {"heading": "7. Conclusion", "text": "Prior work shows that weights learned in neural networks can be highly redundant (Denil et al., 2013). HashedNets exploit this property to create neural networks with \u201cvirtual\u201d connections that seemingly exceed the storage limits of the trained model. This can have surprising effects. Figure 4 in Section 6 shows the test error of neural networks can drop nearly 50%, from 3% to 1.61%, through expanding the number of weights \u201cvirtually\u201d by a factor 8\u00d7. Although the collisions (or weight-sharing) might serve as a form of regularization, we can probably safely ignore this effect as both networks (with and without expansion) were also regularized with dropout (Srivastava et al., 2014) and the hyper-parameters were carefully fine-tuned through Bayesian optimization.\nSo why should additional virtual layers help? One answer is that they probably truly increase the expressiveness of the neural network. As an example, imagine we are provided with a neural network with 100 hidden nodes. The internal weight matrix has 10000 weights. If we add another set of m hidden nodes, this increases the expressiveness of the network. If we require all weights of connections to these m additional nodes to be \u201cre-used\u201d from the set of existing weights, it is not a strong restriction given the large number of weights in existence. In addition, the backprop algorithm can adjust the shared weights carefully to have useful values for all their occurrences.\nAs future work we plan to further investigate model compression for neural networks. One particular direction of interest is to optimize HashedNets for GPUs. GPUs are very fast (through parallel processing) but usually feature small on-board memory. We plan to investigate how to use HashedNets to fit larger networks onto the finite memory of GPUs. A specific challenge in this scenario is to avoid non-coalesced memory accesses due to the pseudo-random hash functions\u2014a sensitive issue for GPU architectures."}], "references": [{"title": "A reliable effective terascale linear learning system", "author": ["Agarwal", "Alekh", "Chapelle", "Olivier", "Dud\u0131\u0301k", "Miroslav", "Langford", "John"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "Do deep nets really need to be deep", "author": ["Ba", "Jimmy", "Caruana", "Rich"], "venue": "In NIPS, pp", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Neural Networks for Pattern Recognition", "author": ["Bishop", "Christopher M"], "venue": null, "citeRegEx": "Bishop and M.,? \\Q1995\\E", "shortCiteRegEx": "Bishop and M.", "year": 1995}, {"title": "Sparse feature learning for deep belief networks", "author": ["Boureau", "Y-lan", "Cun", "Yann L"], "venue": "In NIPS, pp", "citeRegEx": "Boureau et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Boureau et al\\.", "year": 2008}, {"title": "Marginalized denoising auto-encoders for nonlinear representations", "author": ["Chen", "Minmin", "Weinberger", "Kilian Q", "Sha", "Fei", "Bengio", "Yoshua"], "venue": "In ICML, pp", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Augmented smartphone applications through clone cloud execution", "author": ["Chun", "Byung-Gon", "Maniatis", "Petros"], "venue": "In HotOS,", "citeRegEx": "Chun et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chun et al\\.", "year": 2009}, {"title": "Highperformance neural networks for visual object classification", "author": ["Cire\u015fan", "Dan C", "Meier", "Ueli", "Masci", "Jonathan", "Gambardella", "Luca M", "Schmidhuber", "J\u00fcrgen"], "venue": "arXiv preprint arXiv:1102.0183,", "citeRegEx": "Cire\u015fan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cire\u015fan et al\\.", "year": 2011}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Coates", "Adam", "Ng", "Andrew Y", "Lee", "Honglak"], "venue": "In AISTATS,", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Deep learning with cots hpc systems", "author": ["Coates", "Adam", "Huval", "Brody", "Wang", "Tao", "Wu", "David", "Catanzaro", "Bryan", "Andrew", "Ng"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Coates et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2013}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Collobert", "Ronan", "Kavukcuoglu", "Koray", "Farabet", "Cl\u00e9ment"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Low precision storage for deep learning", "author": ["M. Courbariaux", "Y. Bengio", "David", "J.-P"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "Predicting parameters in deep learning", "author": ["Denil", "Misha", "Shakibi", "Babak", "Dinh", "Laurent", "de Freitas", "Nando"], "venue": "In NIPS,", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Denton", "Emily", "Zaremba", "Wojciech", "Bruna", "Joan", "LeCun", "Yann", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1404.0736,", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Donahue", "Jeff", "Jia", "Yangqing", "Vinyals", "Oriol", "Hoffman", "Judy", "Zhang", "Ning", "Tzeng", "Eric", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1310.1531,", "citeRegEx": "Donahue et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2013}, {"title": "Small statistical models by random feature mixing", "author": ["Ganchev", "Kuzman", "Dredze", "Mark"], "venue": "In Workshop on Mobile NLP at ACL,", "citeRegEx": "Ganchev et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ganchev et al\\.", "year": 2008}, {"title": "Bayesian optimization with inequality constraints", "author": ["Gardner", "Jacob", "Kusner", "Matt", "Weinberger", "Kilian", "Cunningham", "John"], "venue": "In ICML,", "citeRegEx": "Gardner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gardner et al\\.", "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra"], "venue": "In CVPR,", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Deep sparse rectifier networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In AISTATS,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In ICML, pp", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alex", "Mohamed", "A-R", "Hinton", "Geoffrey"], "venue": "In ICASSP,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Deep learning with limited numerical precision", "author": ["Gupta", "Suyog", "Agrawal", "Ankur", "Gopalakrishnan", "Kailash", "Narayanan", "Pritish"], "venue": "arXiv preprint arXiv:1502.02551,", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["Hinton", "Geoffrey", "Vinyals", "Oriol", "Dean", "Jeff"], "venue": "NIPS workshop,", "citeRegEx": "Hinton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Andrej", "Fei-Fei", "Li"], "venue": "arXiv preprint arXiv:1412.2306,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Client vs. server architecture: Why google voice search is also much faster than siri @ONLINE", "author": ["A.W. Kosner"], "venue": "URL http://tinyurl.com/ c2d2otr", "citeRegEx": "Kosner,? \\Q2012\\E", "shortCiteRegEx": "Kosner", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["Larochelle", "Hugo", "Erhan", "Dumitru", "Courville", "Aaron C", "Bergstra", "James", "Bengio", "Yoshua"], "venue": "In ICML, pp", "citeRegEx": "Larochelle et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2007}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Le", "Quoc V"], "venue": "In ICASSP,", "citeRegEx": "Le and V.,? \\Q2013\\E", "shortCiteRegEx": "Le and V.", "year": 2013}, {"title": "Optimal brain damage", "author": ["LeCun", "Yann", "Denker", "John S", "Solla", "Sara A", "Howard", "Richard E", "Jackel", "Lawrence D"], "venue": "In NIPS,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Efficient backprop", "author": ["LeCun", "Yann A", "Bottou", "L\u00e9on", "Orr", "Genevieve B", "M\u00fcller", "Klaus-Robert"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "LeCun et al\\.,? \\Q2012\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2012}, {"title": "A low-power processor with configurable embedded machine-learning accelerators for high-order and adaptive analysis of medicalsensor signals. Solid-State Circuits", "author": ["Lee", "Kyong Ho", "Verma", "Naveen"], "venue": "IEEE Journal of,", "citeRegEx": "Lee et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Deep belief networks using discriminative features for phone recognition", "author": ["Mohamed", "Abdel-rahman", "Sainath", "Tara N", "Dahl", "George", "Ramabhadran", "Bhuvana", "Hinton", "Geoffrey E", "Picheny", "Michael A"], "venue": "In ICASSP,", "citeRegEx": "Mohamed et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mohamed et al\\.", "year": 2011}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In ICML, pp", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Simplifying neural networks by soft weight-sharing", "author": ["Nowlan", "Steven J", "Hinton", "Geoffrey E"], "venue": "Neural computation,", "citeRegEx": "Nowlan et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Nowlan et al\\.", "year": 1992}, {"title": "Generalization of back-propagation to recurrent neural networks", "author": ["Pineda", "Fernando J"], "venue": "Physical review letters,", "citeRegEx": "Pineda and J.,? \\Q1987\\E", "shortCiteRegEx": "Pineda and J.", "year": 1987}, {"title": "Cnn features off-theshelf: an astounding baseline for recognition", "author": ["Razavian", "Ali Sharif", "Azizpour", "Hossein", "Sullivan", "Josephine", "Carlsson", "Stefan"], "venue": "In CVPR Workshop,", "citeRegEx": "Razavian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Razavian et al\\.", "year": 2014}, {"title": "Learning ordered representations with nested dropout", "author": ["Rippel", "Oren", "Gelbart", "Michael A", "Adams", "Ryan P"], "venue": "arXiv preprint arXiv:1402.0915,", "citeRegEx": "Rippel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rippel et al\\.", "year": 2014}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["Salton", "Gerard", "Buckley", "Christopher"], "venue": "Information processing & management,", "citeRegEx": "Salton et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Salton et al\\.", "year": 1988}, {"title": "Speech recognition for mobile devices at google", "author": ["Schuster", "Mike"], "venue": "In PRICAI 2010: Trends in Artificial Intelligence,", "citeRegEx": "Schuster and Mike.,? \\Q2010\\E", "shortCiteRegEx": "Schuster and Mike.", "year": 2010}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Micha\u00ebl", "Fergus", "Rob", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Hash kernels for structured data", "author": ["Shi", "Qinfeng", "Petterson", "James", "Dror", "Gideon", "Langford", "John", "Smola", "Alex", "S.V.N. Vishwanathan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Shi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2009}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["Simard", "Patrice Y", "Steinkraus", "Dave", "Platt", "John C"], "venue": "In ICDAR,", "citeRegEx": "Simard et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Simard et al\\.", "year": 2003}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Snoek", "Jasper", "Larochelle", "Hugo", "Adams", "Ryan P"], "venue": "In NIPS,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Feature hashing for large scale multitask learning", "author": ["Weinberger", "Kilian", "Dasgupta", "Anirban", "Langford", "John", "Smola", "Alex", "Attenberg", "Josh"], "venue": "In ICML,", "citeRegEx": "Weinberger et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2009}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1301.3557,", "citeRegEx": "Zeiler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2013}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In ECCV,", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 24, "context": "These include object classification (Krizhevsky et al., 2012; Sermanet et al., 2013), speech recognition (Hinton et al.", "startOffset": 36, "endOffset": 84}, {"referenceID": 39, "context": "These include object classification (Krizhevsky et al., 2012; Sermanet et al., 2013), speech recognition (Hinton et al.", "startOffset": 36, "endOffset": 84}, {"referenceID": 44, "context": ", 2012), image caption generation (Vinyals et al., 2014; Karpathy & Fei-Fei, 2014) and domain adaptation (Glorot et al.", "startOffset": 34, "endOffset": 82}, {"referenceID": 8, "context": "As data sets increase in size, so do the number of parameters in these neural networks in order to absorb the enormous amount of supervision (Coates et al., 2013).", "startOffset": 141, "endOffset": 162}, {"referenceID": 8, "context": "Increasingly, these networks are trained on industrial-sized clusters (Le, 2013) or high-performance graphics processing units (GPUs) (Coates et al., 2013).", "startOffset": 134, "endOffset": 155}, {"referenceID": 23, "context": "This approach is problematic, as it only works when sufficient bandwidth is available and incurs artificial delays through network traffic (Kosner, 2012).", "startOffset": 139, "endOffset": 153}, {"referenceID": 11, "context": "Recent work by Denil et al. (2013) demonstrates that there is a surprisingly large amount of redundancy among the weights of neural networks.", "startOffset": 15, "endOffset": 35}, {"referenceID": 11, "context": "Recent work by Denil et al. (2013) demonstrates that there is a surprisingly large amount of redundancy among the weights of neural networks. The authors show that a small subset of the weights are sufficient to reconstruct the entire network. They exploit this by training low-rank decompositions of the weight matrices. Ba & Caruana (2014) show that deep neural networks can be successfully compressed http://en.", "startOffset": 15, "endOffset": 342}, {"referenceID": 10, "context": "Courbariaux et al. (2014) train neural networks with reduced bit precision, and, long predating this work, LeCun et al.", "startOffset": 0, "endOffset": 26}, {"referenceID": 10, "context": "Courbariaux et al. (2014) train neural networks with reduced bit precision, and, long predating this work, LeCun et al. (1989) investigated dropping unimportant weights in neural networks.", "startOffset": 0, "endOffset": 127}, {"referenceID": 45, "context": "Our parameter hashing is akin to prior work in feature hashing (Weinberger et al., 2009; Shi et al., 2009; Ganchev & Dredze, 2008) and is similarly fast and requires no additional memory overhead.", "startOffset": 63, "endOffset": 130}, {"referenceID": 40, "context": "Our parameter hashing is akin to prior work in feature hashing (Weinberger et al., 2009; Shi et al., 2009; Ganchev & Dredze, 2008) and is similarly fast and requires no additional memory overhead.", "startOffset": 63, "endOffset": 130}, {"referenceID": 28, "context": "The backpropagation algorithm (LeCun et al., 2012) can naturally tune the hash bucket parameters and take into account the random weight sharing within the neural network architecture.", "startOffset": 30, "endOffset": 50}, {"referenceID": 11, "context": "Under the same memory constraint, HashedNets have more adjustable free parameters than the lowrank decomposition methods suggested by Denil et al. (2013), leading to smaller drops in descriptive power.", "startOffset": 134, "endOffset": 154}, {"referenceID": 28, "context": ", 2014), activation functions (Glorot et al., 2011a; LeCun et al., 2012), or weight sparsity (Coates et al.", "startOffset": 30, "endOffset": 72}, {"referenceID": 7, "context": ", 2012), or weight sparsity (Coates et al., 2011).", "startOffset": 28, "endOffset": 49}, {"referenceID": 45, "context": "Feature hashing (or the hashing trick) (Weinberger et al., 2009; Shi et al., 2009) is a technique to map high-dimensional text documents directly into bag-ofword (Salton & Buckley, 1988) vectors, which would otherwise require use of memory consuming dictionaries for storage of indices corresponding with specific input terms.", "startOffset": 39, "endOffset": 82}, {"referenceID": 40, "context": "Feature hashing (or the hashing trick) (Weinberger et al., 2009; Shi et al., 2009) is a technique to map high-dimensional text documents directly into bag-ofword (Salton & Buckley, 1988) vectors, which would otherwise require use of memory consuming dictionaries for storage of indices corresponding with specific input terms.", "startOffset": 39, "endOffset": 82}, {"referenceID": 40, "context": "This problem is less severe for sparse data sets and can be counteracted through multiple hashing (Shi et al., 2009) or larger hash tables (Weinberger et al.", "startOffset": 98, "endOffset": 116}, {"referenceID": 45, "context": ", 2009) or larger hash tables (Weinberger et al., 2009).", "startOffset": 30, "endOffset": 55}, {"referenceID": 45, "context": "The second hash function, \u03be, guarantees that inner products are unbiased in expectation (Weinberger et al., 2009); that is,", "startOffset": 88, "endOffset": 113}, {"referenceID": 45, "context": "Finally, Weinberger et al. (2009) also show that the hashing trick can be used to learn multiple classifiers within the same hashed space.", "startOffset": 9, "endOffset": 34}, {"referenceID": 28, "context": "Typical activation functions are rectifier linear unit (ReLU) (Nair & Hinton, 2010), sigmoid or tanh (LeCun et al., 2012).", "startOffset": 101, "endOffset": 121}, {"referenceID": 10, "context": "The obvious solution is to fit the neural network within budget by reducing the number of nodes n, n in layers `, `+ 1 or by reducing the bit precision of the weight matrices (Courbariaux et al., 2014).", "startOffset": 175, "endOffset": 201}, {"referenceID": 45, "context": "Alternatively and more in line with previous work (Weinberger et al., 2009), we may interpret HashedNets in terms of feature hashing.", "startOffset": 50, "endOffset": 75}, {"referenceID": 45, "context": "For the same reasons we multiply (3) by the sign factor \u03be(i, j) for parameterizing V (Weinberger et al., 2009):", "startOffset": 85, "endOffset": 110}, {"referenceID": 45, "context": "Weinberger et al. (2009) introduce an additional sign factor \u03be(i, j) to remove the bias of hashed inner-products due to collisions.", "startOffset": 0, "endOffset": 25}, {"referenceID": 4, "context": ", 2011a) or through specialized regularization (Chen et al., 2014; Boureau et al., 2008).", "startOffset": 47, "endOffset": 88}, {"referenceID": 3, "context": ", 2011a) or through specialized regularization (Chen et al., 2014; Boureau et al., 2008).", "startOffset": 47, "endOffset": 88}, {"referenceID": 36, "context": "As pointed out in Shi et al. (2009) and Weinberger et al.", "startOffset": 18, "endOffset": 36}, {"referenceID": 36, "context": "As pointed out in Shi et al. (2009) and Weinberger et al. (2009), feature hashing is most effective on sparse feature vectors since the number of hash collisions is minimized.", "startOffset": 18, "endOffset": 65}, {"referenceID": 10, "context": "All weights can be stored with low bit precision (Courbariaux et al., 2014; Gupta et al., 2015), edges could be removed (Cire\u015fan et al.", "startOffset": 49, "endOffset": 95}, {"referenceID": 20, "context": "All weights can be stored with low bit precision (Courbariaux et al., 2014; Gupta et al., 2015), edges could be removed (Cire\u015fan et al.", "startOffset": 49, "endOffset": 95}, {"referenceID": 6, "context": ", 2015), edges could be removed (Cire\u015fan et al., 2011) and HashedNets can be trained on the outputs of larger networks (Ba & Caruana, 2014) \u2014 yielding further reductions in memory requirements.", "startOffset": 32, "endOffset": 54}, {"referenceID": 24, "context": "Deep neural networks have achieved great progress on a wide variety of real-world applications, including image classification (Krizhevsky et al., 2012; Donahue et al., 2013; Sermanet et al., 2013; Zeiler & Fergus, 2014), object detection (Girshick et al.", "startOffset": 127, "endOffset": 220}, {"referenceID": 13, "context": "Deep neural networks have achieved great progress on a wide variety of real-world applications, including image classification (Krizhevsky et al., 2012; Donahue et al., 2013; Sermanet et al., 2013; Zeiler & Fergus, 2014), object detection (Girshick et al.", "startOffset": 127, "endOffset": 220}, {"referenceID": 39, "context": "Deep neural networks have achieved great progress on a wide variety of real-world applications, including image classification (Krizhevsky et al., 2012; Donahue et al., 2013; Sermanet et al., 2013; Zeiler & Fergus, 2014), object detection (Girshick et al.", "startOffset": 127, "endOffset": 220}, {"referenceID": 16, "context": ", 2013; Zeiler & Fergus, 2014), object detection (Girshick et al., 2014; Vinyals et al., 2014), image retrieval (Razavian et al.", "startOffset": 49, "endOffset": 94}, {"referenceID": 44, "context": ", 2013; Zeiler & Fergus, 2014), object detection (Girshick et al., 2014; Vinyals et al., 2014), image retrieval (Razavian et al.", "startOffset": 49, "endOffset": 94}, {"referenceID": 35, "context": ", 2014), image retrieval (Razavian et al., 2014), speech recognition (Hinton et al.", "startOffset": 25, "endOffset": 48}, {"referenceID": 19, "context": ", 2014), speech recognition (Hinton et al., 2012; Graves et al., 2013; Mohamed et al., 2011), and text representation (Mikolov et al.", "startOffset": 28, "endOffset": 92}, {"referenceID": 31, "context": ", 2014), speech recognition (Hinton et al., 2012; Graves et al., 2013; Mohamed et al., 2011), and text representation (Mikolov et al.", "startOffset": 28, "endOffset": 92}, {"referenceID": 30, "context": ", 2011), and text representation (Mikolov et al., 2013).", "startOffset": 33, "endOffset": 55}, {"referenceID": 41, "context": "Arguably the most popular method is the widely used convolutional neural network (Simard et al., 2003).", "startOffset": 81, "endOffset": 102}, {"referenceID": 26, "context": "Instead of sharing weights, LeCun et al. (1989) introduce \u201coptimal brain damage\u201d to directly drop unimportant weights.", "startOffset": 28, "endOffset": 48}, {"referenceID": 6, "context": "Cire\u015fan et al. (2011) demonstrate in their experiments that randomly removing connections leads to superior empirical performance, which shares the same spirit of HashedNets.", "startOffset": 0, "endOffset": 22}, {"referenceID": 20, "context": "16-bit fixed-point representation (Gupta et al., 2015) for a compression factor of 14 over double-precision floating point).", "startOffset": 34, "endOffset": 54}, {"referenceID": 11, "context": "A recent study by Denil et al. (2013) demonstrates significant redundancy in neural network parameters by directly learning a low-rank decomposition of the weight matrix within each layer.", "startOffset": 18, "endOffset": 38}, {"referenceID": 11, "context": "A recent study by Denil et al. (2013) demonstrates significant redundancy in neural network parameters by directly learning a low-rank decomposition of the weight matrix within each layer. They demonstrate that networks composed of weights recovered from the learned decompositions are only slightly less accurate than networks with all weights as free parameters, indicating heavy overparametrization in full weight matrices. A follow-up work by Denton et al. (2014) uses a similar technique to speed up test-time evaluation of convolutional neural networks.", "startOffset": 18, "endOffset": 468}, {"referenceID": 21, "context": "(2006), Hinton et al. (2014) and Ba & Caruana (2014) recently introduce approaches to learn a \u201cdistilled\u201d model, training a more compact neural network to reproduce the output of a larger network.", "startOffset": 8, "endOffset": 29}, {"referenceID": 21, "context": "(2006), Hinton et al. (2014) and Ba & Caruana (2014) recently introduce approaches to learn a \u201cdistilled\u201d model, training a more compact neural network to reproduce the output of a larger network.", "startOffset": 8, "endOffset": 53}, {"referenceID": 21, "context": "(2006), Hinton et al. (2014) and Ba & Caruana (2014) recently introduce approaches to learn a \u201cdistilled\u201d model, training a more compact neural network to reproduce the output of a larger network. Specifically, Hinton et al. (2014) and Ba & Caruana (2014) train a large network on the original training labels, then learn a much smaller \u201cdistilled\u201d model on a weighted combination of the original labels and the (softened) softmax output of the larger model.", "startOffset": 8, "endOffset": 232}, {"referenceID": 21, "context": "(2006), Hinton et al. (2014) and Ba & Caruana (2014) recently introduce approaches to learn a \u201cdistilled\u201d model, training a more compact neural network to reproduce the output of a larger network. Specifically, Hinton et al. (2014) and Ba & Caruana (2014) train a large network on the original training labels, then learn a much smaller \u201cdistilled\u201d model on a weighted combination of the original labels and the (softened) softmax output of the larger model.", "startOffset": 8, "endOffset": 256}, {"referenceID": 21, "context": "(2006), Hinton et al. (2014) and Ba & Caruana (2014) recently introduce approaches to learn a \u201cdistilled\u201d model, training a more compact neural network to reproduce the output of a larger network. Specifically, Hinton et al. (2014) and Ba & Caruana (2014) train a large network on the original training labels, then learn a much smaller \u201cdistilled\u201d model on a weighted combination of the original labels and the (softened) softmax output of the larger model. The authors show that the distilled model has better generalization ability than a model trained on just the labels. In our experimental results, we show that our approach is complementary by learning HashedNets with soft targets. Rippel et al. (2014) propose a novel dropout method, nested dropout, to give an order of importance for hidden neurons.", "startOffset": 8, "endOffset": 711}, {"referenceID": 45, "context": "With the help of feature hashing (Weinberger et al., 2009), Vowpal Wabbit, a large-scale learning system, is able to scale to terafeature datasets (Agarwal et al.", "startOffset": 33, "endOffset": 58}, {"referenceID": 0, "context": ", 2009), Vowpal Wabbit, a large-scale learning system, is able to scale to terafeature datasets (Agarwal et al., 2014).", "startOffset": 96, "endOffset": 118}, {"referenceID": 25, "context": "Datasets consist of the original MNIST handwritten digit dataset, along with four challenging variants (Larochelle et al., 2007).", "startOffset": 103, "endOffset": 128}, {"referenceID": 25, "context": "In addition, we include two binary image classification datasets: CONVEX and RECT (Larochelle et al., 2007).", "startOffset": 82, "endOffset": 107}, {"referenceID": 6, "context": "Random Edge Removal (RER) (Cire\u015fan et al., 2011) reduces the total number of model parameters by randomly removing weights prior to training.", "startOffset": 26, "endOffset": 48}, {"referenceID": 11, "context": "Low-Rank Decomposition (LRD) (Denil et al., 2013) decomposes the weight matrix into two low-rank matrices.", "startOffset": 29, "endOffset": 49}, {"referenceID": 21, "context": "In a similar manner, we examine Dark Knowledge (DK) (Hinton et al., 2014; Ba & Caruana, 2014) by training a distilled model to optimize the cross entropy with both the original labels and soft targets generated by the corresponding full neural network (compression factor 1).", "startOffset": 52, "endOffset": 93}, {"referenceID": 9, "context": "HashedNets and all accompanying baselines were implemented using Torch7 (Collobert et al., 2011) and run on NVIDIA GTX TITAN graphics cards with 2688 cores and 6GB of global memory.", "startOffset": 72, "endOffset": 96}, {"referenceID": 10, "context": "We use 32 bit precision throughout but note that the compression rates of all methods may be improved with lower precision (Courbariaux et al., 2014; Gupta et al., 2015).", "startOffset": 123, "endOffset": 169}, {"referenceID": 20, "context": "We use 32 bit precision throughout but note that the compression rates of all methods may be improved with lower precision (Courbariaux et al., 2014; Gupta et al., 2015).", "startOffset": 123, "endOffset": 169}, {"referenceID": 42, "context": "Hyperparameters are selected for all algorithms with Bayesian optimization (Snoek et al., 2012) and hand tuning on 20% validation splits of the training sets.", "startOffset": 75, "endOffset": 95}, {"referenceID": 9, "context": "HashedNets and all accompanying baselines were implemented using Torch7 (Collobert et al., 2011) and run on NVIDIA GTX TITAN graphics cards with 2688 cores and 6GB of global memory. We use 32 bit precision throughout but note that the compression rates of all methods may be improved with lower precision (Courbariaux et al., 2014; Gupta et al., 2015). We verify all implementations by numerical gradient checking. Models are trained via stochastic gradient descent (minibatch size of 50) with dropout and momentum. ReLU is adopted as the activation function for all models. Hyperparameters are selected for all algorithms with Bayesian optimization (Snoek et al., 2012) and hand tuning on 20% validation splits of the training sets. We use the open source Bayesian Optimization MATLAB implementation \u201cbayesopt.m\u201d from Gardner et al. (2014).3", "startOffset": 73, "endOffset": 841}, {"referenceID": 11, "context": "Prior work shows that weights learned in neural networks can be highly redundant (Denil et al., 2013).", "startOffset": 81, "endOffset": 101}], "year": 2015, "abstractText": "As deep nets are increasingly used in applications suited for mobile devices, a fundamental dilemma becomes apparent: the trend in deep learning is to grow models to absorb everincreasing data set sizes; however mobile devices are designed with very little memory and cannot store such large models. We present a novel network architecture, HashedNets, that exploits inherent redundancy in neural networks to achieve drastic reductions in model sizes. HashedNets uses a low-cost hash function to randomly group connection weights into hash buckets, and all connections within the same hash bucket share a single parameter value. These parameters are tuned to adjust to the HashedNets weight sharing architecture with standard backprop during training. Our hashing procedure introduces no additional memory overhead, and we demonstrate on several benchmark data sets that HashedNets shrink the storage requirements of neural networks substantially while mostly preserving generalization performance.", "creator": "LaTeX with hyperref package"}}}