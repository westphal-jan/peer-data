{"id": "1504.06654", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2015", "title": "Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space", "abstract": "There is rising interest in vector-space word embeddings and their use in NLP, especially given recent methods for their fast estimation at very large scale. Nearly all this work, however, assumes a single vector per word type ignoring polysemy and thus jeopardizing their usefulness for downstream tasks. We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type. It differs from recent related work by jointly performing word sense discrimination and embedding learning, by non-parametrically estimating the number of senses per word type, and by its efficiency and scalability. We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours.", "histories": [["v1", "Fri, 24 Apr 2015 22:12:14 GMT  (203kb,D)", "http://arxiv.org/abs/1504.06654v1", "In Conference on Empirical Methods in Natural Language Processing, 2014"]], "COMMENTS": "In Conference on Empirical Methods in Natural Language Processing, 2014", "reviews": [], "SUBJECTS": "cs.CL stat.ML", "authors": ["arvind neelakantan", "jeevan shankar", "alexandre passos", "andrew mccallum"], "accepted": true, "id": "1504.06654"}, "pdf": {"name": "1504.06654.pdf", "metadata": {"source": "CRF", "title": "Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space", "authors": ["Arvind Neelakantan", "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum"], "emails": ["arvind@cs.umass.edu", "jshankar@cs.umass.edu", "apassos@cs.umass.edu", "mccallum@cs.umass.edu"], "sections": [{"heading": null, "text": "There is rising interest in vector-space word embeddings and their use in NLP, especially given recent methods for their fast estimation at very large scale. Nearly all this work, however, assumes a single vector per word type\u2014ignoring polysemy and thus jeopardizing their usefulness for downstream tasks. We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type. It differs from recent related work by jointly performing word sense discrimination and embedding learning, by non-parametrically estimating the number of senses per word type, and by its efficiency and scalability. We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours."}, {"heading": "1 Introduction", "text": "Representing words by dense, real-valued vector embeddings, also commonly called \u201cdistributed representations,\u201d helps address the curse of dimensionality and improve generalization because they can place near each other words having similar semantic and syntactic roles. This has been shown dramatically in state-of-the-art results on language modeling (Bengio et al, 2003; Mnih and Hinton, 2007) as well as improvements in other natural language processing tasks (Collobert and Weston, 2008; Turian et al, 2010). Substantial benefit arises when embeddings can be trained on large volumes of data. Hence the recent considerable interest in the CBOW and Skip-gram models\n*The first two authors contributed equally to this paper.\nof Mikolov et al (2013a); Mikolov et al (2013b)\u2014 relatively simple log-linear models that can be trained to produce high-quality word embeddings on the entirety of English Wikipedia text in less than half a day on one machine.\nThere is rising enthusiasm for applying these models to improve accuracy in natural language processing, much like Brown clusters (Brown et al, 1992) have become common input features for many tasks, such as named entity extraction (Miller et al, 2004; Ratinov and Roth, 2009) and parsing (Koo et al, 2008; Ta\u0308ckstro\u0308m et al, 2012). In comparison to Brown clusters, the vector embeddings have the advantages of substantially better scalability in their training, and intriguing potential for their continuous and multi-dimensional interrelations. In fact, Passos et al (2014) present new state-of-the-art results in CoNLL 2003 named entity extraction by directly inputting continuous vector embeddings obtained by a version of Skipgram that injects supervision with lexicons. Similarly Bansal et al (2014) show results in dependency parsing using Skip-gram embeddings. They have also recently been applied to machine translation (Zou et al, 2013; Mikolov et al, 2013c).\nA notable deficiency in this prior work is that each word type (e.g. the word string plant) has only one vector representation\u2014polysemy and hononymy are ignored. This results in the word plant having an embedding that is approximately the average of its different contextual semantics relating to biology, placement, manufacturing and power generation. In moderately highdimensional spaces a vector can be relatively \u201cclose\u201d to multiple regions at a time, but this does not negate the unfortunate influence of the triangle inequality2 here: words that are not synonyms but are synonymous with different senses of the same word will be pulled together. For example, pollen and refinery will be inappropriately pulled to a dis-\n2For distance d, d(a, c) \u2264 d(a, b) + d(b, c).\nar X\niv :1\n50 4.\n06 65\n4v 1\n[ cs\n.C L\n] 2\n4 A\npr 2\n01 5\ntance not more than the sum of the distances plant\u2013 pollen and plant\u2013refinery. Fitting the constraints of legitimate continuous gradations of semantics are challenge enough without the additional encumbrance of these illegitimate triangle inequalities.\nDiscovering embeddings for multiple senses per word type is the focus of work by Reisinger and Mooney (2010a) and Huang et al (2012). They both pre-cluster the contexts of a word type\u2019s tokens into discriminated senses, use the clusters to re-label the corpus\u2019 tokens according to sense, and then learn embeddings for these re-labeled words. The second paper improves upon the first by employing an earlier pass of non-discriminated embedding learning to obtain vectors used to represent the contexts. Note that by pre-clustering, these methods lose the opportunity to jointly learn the sense-discriminated vectors and the clustering. Other weaknesses include their fixed number of sense per word type, and the computational expense of the two-step process\u2014the Huang et al (2012) method took one week of computation to learn multiple embeddings for a 6,000 subset of the 100,000 vocabulary on a corpus containing close to billion tokens.3\nThis paper presents a new method for learning vector-space embeddings for multiple senses per word type, designed to provide several advantages over previous approaches. (1) Sensediscriminated vectors are learned jointly with the assignment of token contexts to senses; thus we can use the emerging sense representation to more accurately perform the clustering. (2) A nonparametric variant of our method automatically discovers a varying number of senses per word type. (3) Efficient online joint training makes it fast and scalable. We refer to our method as Multiple-sense Skip-gram, or MSSG, and its nonparametric counterpart as NP-MSSG.\nOur method builds on the Skip-gram model (Mikolov et al, 2013a), but maintains multiple vectors per word type. During online training with a particular token, we use the average of its context words\u2019 vectors to select the token\u2019s sense that is closest, and perform a gradient update on that sense. In the non-parametric version of our method, we build on facility location (Meyerson, 2001): a new cluster is created with probability proportional to the distance from the context to the\n3Personal communication with authors Eric H. Huang and Richard Socher.\nnearest sense. We present experimental results demonstrating the benefits of our approach. We show qualitative improvements over single-sense Skip-gram and Huang et al (2012), comparing against word neighbors from our parametric and non-parametric methods. We present quantitative results in three tasks. On both the SCWS and WordSim353 data sets our methods surpass the previous state-ofthe-art. The Google Analogy task is not especially well-suited for word-sense evaluation since its lack of context makes selecting the sense difficult; however our method dramatically outperforms Huang et al (2012) on this task. Finally we also demonstrate scalabilty, learning multiple senses, training on nearly a billion tokens in less than 6 hours\u2014a 27x improvement on Huang et al."}, {"heading": "2 Related Work", "text": "Much prior work has focused on learning vector representations of words; here we will describe only those most relevant to understanding this paper. Our work is based on neural language models, proposed by Bengio et al (2003), which extend the traditional idea of n-gram language models by replacing the conditional probability table with a neural network, representing each word token by a small vector instead of an indicator variable, and estimating the parameters of the neural network and these vectors jointly. Since the Bengio et al (2003) model is quite expensive to train, much research has focused on optimizing it. Collobert and Weston (2008) replaces the max-likelihood character of the model with a max-margin approach, where the network is encouraged to score the correct n-grams higher than randomly chosen incorrect n-grams. Mnih and Hinton (2007) replaces the global normalization of the Bengio model with a tree-structured probability distribution, and also considers multiple positions for each word in the tree.\nMore relevantly, Mikolov et al (2013a) and Mikolov et al (2013b) propose extremely computationally efficient log-linear neural language models by removing the hidden layers of the neural networks and training from larger context windows with very aggressive subsampling. The goal of the models in Mikolov et al (2013a) and Mikolov et al (2013b) is not so much obtaining a low-perplexity language model as learning word representations which will be useful in\ndownstream tasks. Neural networks or log-linear models also do not appear to be necessary to learn high-quality word embeddings, as Dhillon and Ungar (2011) estimate word vector representations using Canonical Correlation Analysis (CCA).\nWord vector representations or embeddings have been used in various NLP tasks such as named entity recognition (Neelakantan and Collins, 2014; Passos et al, 2014; Turian et al, 2010), dependency parsing (Bansal et al, 2014), chunking (Turian et al, 2010; Dhillon and Ungar, 2011), sentiment analysis (Maas et al, 2011), paraphrase detection (Socher et al, 2011) and learning representations of paragraphs and documents (Le and Mikolov, 2014). The word clusters obtained from Brown clustering (Brown et al, 1992) have similarly been used as features in named entity recognition (Miller et al, 2004; Ratinov and Roth, 2009) and dependency parsing (Koo et al, 2008), among other tasks.\nThere is considerably less prior work on learning multiple vector representations for the same word type. Reisinger and Mooney (2010a) introduce a method for constructing multiple sparse, high-dimensional vector representations of words. Huang et al (2012) extends this approach incorporating global document context to learn multiple dense, low-dimensional embeddings by using recursive neural networks. Both the methods perform word sense discrimination as a preprocessing step by clustering contexts for each word type, making training more expensive. While methods such as those described in Dhillon and Ungar (2011) and Reddy et al (2011) use token-specific representations of words as part of the learning algorithm, the final outputs are still one-to-one mappings between word types and word embeddings."}, {"heading": "3 Background: Skip-gram model", "text": "The Skip-gram model learns word embeddings such that they are useful in predicting the surrounding words in a sentence. In the Skip-gram model, v(w) \u2208 Rd is the vector representation of the word w \u2208 W , where W is the words vocabulary and d is the embedding dimensionality.\nGiven a pair of words (wt, c), the probability that the word c is observed in the context of word\nwt is given by,\nP (D = 1|v(wt), v(c)) = 1\n1 + e\u2212v(wt)T v(c) (1)\nThe probability of not observing word c in the context of wt is given by,\nP (D = 0|v(wt), v(c)) = 1\u2212 P (D = 1|v(wt), v(c))\nGiven a training set containing the sequence of word types w1, w2, . . . , wT , the word embeddings are learned by maximizing the following objective function:\nJ(\u03b8) = \u2211\n(wt,ct)\u2208D+\n\u2211 c\u2208ct logP (D = 1|v(wt), v(c))\n+ \u2211\n(wt,c\u2032t)\u2208D\u2212 \u2211 c\u2032\u2208c\u2032t logP (D = 0|v(wt), v(c\u2032))\nwhere wt is the tth word in the training set, ct is the set of observed context words of word wt and c\u2032t is the set of randomly sampled, noisy context words for the word wt. D+ consists of the set of all observed word-context pairs (wt, ct) (t = 1, 2 . . . , T ). D\u2212 consists of pairs (wt, c\u2032t) (t = 1, 2 . . . , T ) where c\u2032t is the set of randomly sampled, noisy context words for the word wt.\nFor each training word wt, the set of context words ct = {wt\u2212Rt , . . . , wt\u22121, wt+1, . . . , wt+Rt} includesRt words to the left and right of the given word as shown in Figure 1. Rt is the window size considered for the word wt uniformly randomly sampled from the set {1, 2, . . . , N}, where N is the maximum context window size.\nThe set of noisy context words c\u2032t for the word wt is constructed by randomly sampling S noisy context words for each word in the context ct. The noisy context words are randomly sampled from the following distribution,\nP (w) = punigram(w)\n3/4\nZ (2)\nwhere punigram(w) is the unigram distribution of the words and Z is the normalization constant."}, {"heading": "4 Multi-Sense Skip-gram (MSSG) model", "text": "To extend the Skip-gram model to learn multiple embeddings per word we follow previous work (Huang et al, 2012; Reisinger and Mooney, 2010a)\nand let each sense of word have its own embedding, and induce the senses by clustering the embeddings of the context words around each token. The vector representation of the context is the average of its context words\u2019 vectors. For every word type, we maintain clusters of its contexts and the sense of a word token is predicted as the cluster that is closest to its context representation. After predicting the sense of a word token, we perform a gradient update on the embedding of that sense. The crucial difference from previous approaches is that word sense discrimination and learning embeddings are performed jointly by predicting the sense of the word using the current parameter estimates.\nIn the MSSG model, each word w \u2208 W is associated with a global vector vg(w) and each sense of the word has an embedding (sense vector) vs(w, k) (k = 1, 2, . . . ,K) and a context cluster with center \u00b5(w, k) (k = 1, 2, . . . ,K). The K sense vectors and the global vectors are of dimension d and K is a hyperparameter.\nConsider the word wt and let ct = {wt\u2212Rt , . . . , wt\u22121, wt+1, . . . , wt+Rt} be the set of observed context words. The vector representation of the context is defined as the average of the global vector representation of the words in the context. Let vcontext(ct) = 12\u2217Rt \u2211 c\u2208ct vg(c) be the vector representation of the context ct. We use the global vectors of the context words instead of its sense vectors to avoid the computational complexity associated with predicting the sense of the context words. We predict st, the sense\nof word wt when observed with context ct as the context cluster membership of the vector vcontext(ct) as shown in Figure 2. More formally,\nst = argmax k=1,2,...,K sim(\u00b5(wt, k), vcontext(ct)) (3)\nThe hard cluster assignment is similar to the kmeans algorithm. The cluster center is the average of the vector representations of all the contexts which belong to that cluster. For sim we use cosine similarity in our experiments.\nHere, the probability that the word c is observed in the context of word wt given the sense of the word wt is,\nP (D = 1|st,vs(wt, 1), . . . , vs(wt,K), vg(c)) = P (D = 1|vs(wt, st), vg(c))\n= 1\n1 + e\u2212vs(wt,st) T vg(c)\nThe probability of not observing word c in the context of wt given the sense of the word wt is,\nP (D = 0|st,vs(wt, 1), . . . , vs(wt,K), vg(c)) = P (D = 0|vs(wt, st), vg(c)) = 1\u2212 P (D = 1|vs(wt, st), vg(c))\nGiven a training set containing the sequence of word types w1, w2, ..., wT , the word embeddings are learned by maximizing the following objective\nAlgorithm 1 Training Algorithm of MSSG model 1: Input: w1, w2, ..., wT , d, K, N . 2: Initialize vs(w, k) and vg(w), \u2200w \u2208 W,k \u2208 {1, . . . ,K} randomly, \u00b5(w, k) \u2200w \u2208 W,k \u2208 {1, . . . ,K} to 0.\n3: for t = 1, 2, . . . , T do 4: Rt \u223c {1, . . . , N} 5: ct = {wt\u2212Rt , . . . , wt\u22121, wt+1, . . . , wt+Rt} 6: vcontext(ct) =\n1 2\u2217Rt \u2211 c\u2208ct vg(c)\n7: st = argmaxk=1,2,...,K { sim(\u00b5(wt, k), vcontext(ct))} 8: Update context cluster center \u00b5(wt, st) since context ct is added to context cluster st of word wt.\n9: c\u2032t = Noisy Samples(ct) 10: Gradient update on vs(wt, st), global vec-\ntors of words in ct and c\u2032t. 11: end for 12: Output: vs(w, k), vg(w) and context cluster\ncenters \u00b5(w, k), \u2200w \u2208W,k \u2208 {1, . . . ,K}\nfunction:\nJ(\u03b8) =\u2211 (wt,ct)\u2208D+ \u2211 c\u2208ct logP (D = 1|vs(wt, st), vg(c))+\n\u2211 (wt,c\u2032t)\u2208D\u2212 \u2211 c\u2032\u2208c\u2032t logP (D = 0|vs(wt, st), vg(c\u2032))\nwhere wt is the tth word in the sequence, ct is the set of observed context words and c\u2032t is the set of noisy context words for the word wt. D+ and D\u2212 are constructed in the same way as in the Skipgram model.\nAfter predicting the sense of word wt, we update the embedding of the predicted sense for the word wt (vs(wt, st)), the global vector of the words in the context and the global vector of the randomly sampled, noisy context words. The context cluster center of cluster st for the word wt (\u00b5(wt, st)) is updated since context ct is added to the cluster st."}, {"heading": "5 Non-Parametric MSSG model (NP-MSSG)", "text": "The MSSG model learns a fixed number of senses per word type. In this section, we describe a non-parametric version of MSSG, the NP-MSSG model, which learns varying number of senses per word type. Our approach is closely related to\nthe online non-parametric clustering procedure described in Meyerson (2001). We create a new cluster (sense) for a word type with probability proportional to the distance of its context to the nearest cluster (sense).\nEach wordw \u2208W is associated with sense vectors, context clusters and a global vector vg(w) as in the MSSG model. The number of senses for a word is unknown and is learned during training. Initially, the words do not have sense vectors and context clusters. We create the first sense vector and context cluster for each word on its first occurrence in the training data. After creating the first context cluster for a word, a new context cluster and a sense vector are created online during training when the word is observed with a context were the similarity between the vector representation of the context with every existing cluster center of the word is less than \u03bb, where \u03bb is a hyperparameter of the model.\nConsider the word wt and let ct = {wt\u2212Rt , . . . , wt\u22121, wt+1, . . . , wt+Rt} be the set of observed context words. The vector representation of the context is defined as the average of the global vector representation of the words in the context. Let vcontext(ct) = 12\u2217Rt \u2211 c\u2208ct vg(c) be the vector representation of the context ct. Let k(wt) be the number of context clusters or the number of senses currently associated with word wt. st, the sense of word wt when k(wt) > 0 is given by\nst =  k(wt) + 1, ifmaxk=1,2,...,k(wt){sim\n(\u00b5(wt, k), vcontext(ct))} < \u03bb kmax, otherwise\n(4) where \u00b5(wt, k) is the cluster center of the kth cluster of word wt and kmax = argmaxk=1,2,...,k(wt) sim(\u00b5(wt, k), vcontext(ct)).\nThe cluster center is the average of the vector representations of all the contexts which belong to that cluster. If st = k(wt) + 1, a new context cluster and a new sense vector are created for the word wt.\nThe NP-MSSG model and the MSSG model described previously differ only in the way word sense discrimination is performed. The objective function and the probabilistic model associated with observing a (word, context) pair given the sense of the word remain the same."}, {"heading": "6 Experiments", "text": "To evaluate our algorithms we train embeddings using the same corpus and vocabulary as used in Huang et al (2012), which is the April 2010 snapshot of the Wikipedia corpus (Shaoul and Westbury, 2010). It contains approximately 2 million articles and 990 million tokens. In all our experiments we remove all the words with less than 20 occurrences and use a maximum context window (N ) of length 5 (5 words before and after the word occurrence). We fix the number of senses (K) to be 3 for the MSSG model unless otherwise specified. Our hyperparameter values were selected by a small amount of manual exploration on a validation set. In NP-MSSG we set \u03bb to -0.5. The Skip-gram model, MSSG and NP-MSSG models sample one noisy context word (S) for each of the observed context words. We train our models using AdaGrad stochastic gradient decent (Duchi et al, 2011) with initial learning rate set to 0.025. Similarly to Huang et al (2012), we don\u2019t use a regularization penalty.\nBelow we describe qualitative results, displaying the embeddings and the nearest neighbors of each word sense, and quantitative experiments in two benchmark word similarity tasks.\nTable 1 shows time to train our models, compared with other models from previous work. All these times are from single-machine implementations running on similar-sized corpora. We see that our model shows significant improvement in the training time over the model in Huang et al (2012), being within well within an order-ofmagnitude of the training time for Skip-gram models."}, {"heading": "6.1 Nearest Neighbors", "text": "Table 2 shows qualitatively the results of discovering multiple senses by presenting the nearest neighbors associated with various embeddings. The nearest neighbors of a word are computed by comparing the cosine similarity between the embedding for each sense of the word and the context embeddings of all other words in the vocabulary. Note that each of the discovered senses are indeed semantically coherent, and that a reasonable number of senses are created by the non-parametric method. Table 3 shows the nearest neighbors of the word plant for Skip-gram, MSSG , NP-MSSG and Haung\u2019s model (Huang et al, 2012)."}, {"heading": "6.2 Word Similarity", "text": "We evaluate our embeddings on two related datasets: the WordSim-353 (Finkelstein et al, 2001) dataset and the Contextual Word Similarities (SCWS) dataset Huang et al (2012).\nWordSim-353 is a standard dataset for evaluating word vector representations. It consists of a list of pairs of word types, the similarity of which is rated in an integral scale from 1 to 10. Pairs include both monosemic and polysemic words. These scores to each word pairs are given without any contextual information, which makes them tricky to interpret.\nTo overcome this issue, Stanford\u2019s Contextual Word Similarities (SCWS) dataset was developed by Huang et al (2012). The dataset consists of 2003 word pairs and their sentential contexts. It consists of 1328 noun-noun pairs, 399 verb-verb pairs, 140 verb-noun, 97 adjective-adjective, 30 noun-adjective, 9 verb-adjective, and 241 sameword pairs. We evaluate and compare our embeddings on both WordSim-353 and SCWS word similarity corpus.\nSince it is not trivial to deal with multiple embeddings per word, we consider the following similarity measures between words w and w\u2032 given their respective contexts c and c\u2032, where P (w, c, k) is the probability that w takes the kth sense given\nthe context c, and d(vs(w, i), vs(w\u2032, j)) is the similarity measure between the given embeddings vs(w, i) and vs(w\u2032, j).\nThe avgSim metric,\navgSim(w,w\u2032)\n= 1\nK2 K\u2211 i=1 K\u2211 j=1 d (vs(w, i), vs(w \u2032, j)) ,\ncomputes the average similarity over all embeddings for each word, ignoring information from the context.\nTo address this, the avgSimC metric,\navgSimC(w,w\u2032) = K\u2211 j=1 K\u2211 i=1 P (w, c, i)P (w\u2032, c\u2032, j)\n\u00d7 d (vs(w, i), vs(w\u2032, j))\nweighs the similarity between each pair of senses by how well does each sense fit the context at hand.\nThe globalSim metric uses each word\u2019s global context vector, ignoring the many senses:\nglobalSim(w,w\u2032) = d (vg(w), vg(w \u2032)) .\nFinally, localSim metric selects a single sense for each word based independently on its context and computes the similarity by\nlocalSim(w,w\u2032) = d (vs(w, k), vs(w \u2032, k\u2032)) ,\nwhere k = argmaxi P (w, c, i) and k\u2032 = argmaxj P (w\n\u2032, c\u2032, j) and P (w, c, i) is the probability that w takes the ith sense given context c. The probability of being in a cluster is calculated as the inverse of the cosine distance to the cluster center (Huang et al, 2012).\nWe report the Spearman correlation between a model\u2019s similarity scores and the human judgements in the datasets.\nTable 5 shows the results on WordSim-353 task. C&W refers to the language model by Collobert and Weston (2008) and HLBL model is the method described in Mnih and Hinton (2007). On WordSim-353 task, we see that our model performs significantly better than the previous neural network model for learning multi-representations per word (Huang et al, 2012). Among the methods that learn low-dimensional and dense representations, our model performs slightly better than Skip-gram. Table 4 shows the results for the SCWS task. In this task, when the words are\ngiven with their context, our model achieves new state-of-the-art results on SCWS as shown in the Table-4. The previous state-of-art model (Huang et al, 2012) on this task achieves 65.7% using the avgSimC measure, while the MSSG model achieves the best score of 69.3% on this task. The results on the other metrics are similar. For a fixed embedding dimension, the model by Huang et al (2012) has more parameters than our model since it uses a hidden layer. The results show that our model performs better than Huang et al (2012) even when both the models use 50 dimensional vectors and the performance of our model improves as we increase the number of dimensions to 300.\nWe evaluate the models in a word analogy task\nintroduced by Mikolov et al (2013a) where both MSSG and NP-MSSG models achieve 64% accuracy compared to 12% accuracy by Huang et al (2012). Skip-gram which is the state-of-art model for this task achieves 67% accuracy.\nFigure 3 shows the distribution of number of senses learned per word type in the NP-MSSG model. We learn the multiple embeddings for the same set of approximately 6000 words that were used in Huang et al (2012) for all our experiments to ensure fair comparision. These approximately 6000 words were choosen by Huang et al. mainly from the top 30,00 frequent words in the vocabulary. This selection was likely made to avoid the noise of learning multiple senses for infrequent words. However, our method is robust to noise, which can be seen by the good performance of our model that learns multiple embeddings for the top 30,000 most frequent words. We found that even by learning multiple embeddings for the top 30,000 most frequent words in the vocubulary, MSSG model still achieves state-of-art result on SCWS task with an avgSimC score of 69.2 as shown in Table 6."}, {"heading": "7 Conclusion", "text": "We present an extension to the Skip-gram model that efficiently learns multiple embeddings per\nword type. The model jointly performs word sense discrimination and embedding learning, and non-parametrically estimates the number of senses per word type. Our method achieves new stateof-the-art results in the word similarity in context task and learns multiple senses, training on close to billion tokens in less than 6 hours. The global vectors, sense vectors and cluster centers of our model and code for learning them are available at https://people.cs.umass.edu/ \u02dcarvind/emnlp2014wordvectors. In future work we plan to use the multiple embeddings per word type in downstream NLP tasks."}, {"heading": "Acknowledgments", "text": "This work was supported in part by the Center for Intelligent Information Retrieval and in part by DARPA under agreement number FA8750-13-20020. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor."}], "references": [{"title": "Tailoring Continuous Word Representations for Dependency Parsing", "author": ["Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Bansal et al\\.,? 2014", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin."], "venue": "Journal of Machine Learning Research (JMLR).", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Class-based N-gram models of natural language Computational Linguistics", "author": ["Peter F. Brown", "Peter V. Desouza", "Robert L. Mercer", "Vincent J. Della Pietra", "Jenifer C. Lai"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "International Conference on Machine learning (ICML).", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Multi-View Learning of Word Embeddings via CCA", "author": ["Paramveer S. Dhillon", "Dean Foster", "Lyle Ungar."], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Dhillon et al\\.,? 2011", "shortCiteRegEx": "Dhillon et al\\.", "year": 2011}, {"title": "Adaptive sub- gradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research (JMLR).", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Placing search in context: the concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "International Conference on World Wide Web (WWW).", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Computing semantic relatedness using wikipediabased explicit semantic analysis", "author": ["Evgeniy Gabrilovich", "Shaul Markovitch."], "venue": "International Joint Conference on Artificial Intelligence (IJCAI).", "citeRegEx": "Gabrilovich and Markovitch.,? 2007", "shortCiteRegEx": "Gabrilovich and Markovitch.", "year": 2007}, {"title": "Improving Word Representations via Global Context and Multiple Word Prototypes", "author": ["Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Association of Computational Linguistics (ACL).", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Simple Semi-supervised Dependency Parsing", "author": ["Terry Koo", "Xavier Carreras", "Michael Collins."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Koo et al\\.,? 2008", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Quoc V. Le", "Tomas Mikolov"], "venue": "International Conference on Machine Learning", "citeRegEx": "Le and Mikolov.,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Learning Word Vectors for Sentiment Analysis Association for Computational Linguistics (ACL", "author": ["Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts"], "venue": null, "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Online Facility Location", "author": ["Adam Meyerson"], "venue": "IEEE Symposium on Foundations of Computer Science", "citeRegEx": "Meyerson.,? \\Q2001\\E", "shortCiteRegEx": "Meyerson.", "year": 2001}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Workshop at International Conference on Learning Representations (ICLR).", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting Similarities among Languages for Machine Translation", "author": ["Tomas Mikolov", "Quoc V. Le", "Ilya Sutskever."], "venue": "arXiv.", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Name tagging with word clusters and discriminative training", "author": ["Scott Miller", "Jethran Guinness", "Alex Zamanian."], "venue": "North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).", "citeRegEx": "Miller et al\\.,? 2004", "shortCiteRegEx": "Miller et al\\.", "year": 2004}, {"title": "Three new graphical models for statistical language modelling", "author": ["Andriy Mnih", "Geoffrey Hinton."], "venue": "International Conference on Machine learning (ICML).", "citeRegEx": "Mnih and Hinton.,? 2007", "shortCiteRegEx": "Mnih and Hinton.", "year": 2007}, {"title": "Learning Dictionaries for Named Entity Recognition using Minimal Supervision", "author": ["Arvind Neelakantan", "Michael Collins."], "venue": "European Chapter of the Association for Computational Linguistics (EACL).", "citeRegEx": "Neelakantan and Collins.,? 2014", "shortCiteRegEx": "Neelakantan and Collins.", "year": 2014}, {"title": "Lexicon Infused Phrase Embeddings for Named Entity Resolution", "author": ["Alexandre Passos", "Vineet Kumar", "Andrew McCallum."], "venue": "Conference on Natural Language Learning (CoNLL).", "citeRegEx": "Passos et al\\.,? 2014", "shortCiteRegEx": "Passos et al\\.", "year": 2014}, {"title": "Design Challenges and Misconceptions in Named Entity Recognition", "author": ["Lev Ratinov", "Dan Roth."], "venue": "Conference on Natural Language Learning (CoNLL).", "citeRegEx": "Ratinov and Roth.,? 2009", "shortCiteRegEx": "Ratinov and Roth.", "year": 2009}, {"title": "Dynamic and Static Prototype Vectors for Semantic Composition", "author": ["Siva Reddy", "Ioannis P. Klapaftis", "Diana McCarthy."], "venue": "International Joint Conference on Artificial Intelligence (IJCNLP).", "citeRegEx": "Reddy et al\\.,? 2011", "shortCiteRegEx": "Reddy et al\\.", "year": 2011}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Joseph Reisinger", "Raymond J. Mooney."], "venue": "North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)", "citeRegEx": "Reisinger and Mooney.,? 2010a", "shortCiteRegEx": "Reisinger and Mooney.", "year": 2010}, {"title": "A mixture model with sharing for lexical semantics", "author": ["Joseph Reisinger", "Raymond Mooney."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Reisinger and Mooney.,? 2010b", "shortCiteRegEx": "Reisinger and Mooney.", "year": 2010}, {"title": "The Westbury lab wikipedia", "author": ["Cyrus Shaoul", "Chris Westbury"], "venue": null, "citeRegEx": "Shaoul and Westbury.,? \\Q2010\\E", "shortCiteRegEx": "Shaoul and Westbury.", "year": 2010}, {"title": "Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection", "author": ["Richard Socher", "Eric H. Huang", "Jeffrey Pennington", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure", "author": ["Oscar T\u00e4ckstr\u00f6m", "Ryan McDonald", "Jakob Uszkoreit."], "venue": "North American Chapter of the Association for Computational Linguistics: Human Language Technologies.", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? 2012", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "Word Representations: A Simple and General Method for Semi-Supervised Learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Bilingual Word Embeddings for Phrase-Based Machine Translation", "author": ["Will Y. Zou", "Richard Socher", "Daniel Cer", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Zou et al\\.,? 2013", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 17, "context": "This has been shown dramatically in state-of-the-art results on language modeling (Bengio et al, 2003; Mnih and Hinton, 2007) as well as improvements in other natural language processing tasks (Collobert and Weston, 2008; Turian et al, 2010).", "startOffset": 82, "endOffset": 125}, {"referenceID": 3, "context": "This has been shown dramatically in state-of-the-art results on language modeling (Bengio et al, 2003; Mnih and Hinton, 2007) as well as improvements in other natural language processing tasks (Collobert and Weston, 2008; Turian et al, 2010).", "startOffset": 193, "endOffset": 241}, {"referenceID": 20, "context": "There is rising enthusiasm for applying these models to improve accuracy in natural language processing, much like Brown clusters (Brown et al, 1992) have become common input features for many tasks, such as named entity extraction (Miller et al, 2004; Ratinov and Roth, 2009) and parsing (Koo et al, 2008; T\u00e4ckstr\u00f6m et al, 2012).", "startOffset": 232, "endOffset": 276}, {"referenceID": 20, "context": "There is rising enthusiasm for applying these models to improve accuracy in natural language processing, much like Brown clusters (Brown et al, 1992) have become common input features for many tasks, such as named entity extraction (Miller et al, 2004; Ratinov and Roth, 2009) and parsing (Koo et al, 2008; T\u00e4ckstr\u00f6m et al, 2012). In comparison to Brown clusters, the vector embeddings have the advantages of substantially better scalability in their training, and intriguing potential for their continuous and multi-dimensional interrelations. In fact, Passos et al (2014) present new state-of-the-art results in CoNLL 2003 named entity extraction by directly inputting continuous vector embeddings obtained by a version of Skipgram that injects supervision with lexicons.", "startOffset": 253, "endOffset": 574}, {"referenceID": 20, "context": "There is rising enthusiasm for applying these models to improve accuracy in natural language processing, much like Brown clusters (Brown et al, 1992) have become common input features for many tasks, such as named entity extraction (Miller et al, 2004; Ratinov and Roth, 2009) and parsing (Koo et al, 2008; T\u00e4ckstr\u00f6m et al, 2012). In comparison to Brown clusters, the vector embeddings have the advantages of substantially better scalability in their training, and intriguing potential for their continuous and multi-dimensional interrelations. In fact, Passos et al (2014) present new state-of-the-art results in CoNLL 2003 named entity extraction by directly inputting continuous vector embeddings obtained by a version of Skipgram that injects supervision with lexicons. Similarly Bansal et al (2014) show results in dependency parsing using Skip-gram embeddings.", "startOffset": 253, "endOffset": 804}, {"referenceID": 22, "context": "Discovering embeddings for multiple senses per word type is the focus of work by Reisinger and Mooney (2010a) and Huang et al (2012).", "startOffset": 81, "endOffset": 110}, {"referenceID": 22, "context": "Discovering embeddings for multiple senses per word type is the focus of work by Reisinger and Mooney (2010a) and Huang et al (2012). They both pre-cluster the contexts of a word type\u2019s tokens into discriminated senses, use the clusters to re-label the corpus\u2019 tokens according to sense, and then learn embeddings for these re-labeled words.", "startOffset": 81, "endOffset": 133}, {"referenceID": 22, "context": "Discovering embeddings for multiple senses per word type is the focus of work by Reisinger and Mooney (2010a) and Huang et al (2012). They both pre-cluster the contexts of a word type\u2019s tokens into discriminated senses, use the clusters to re-label the corpus\u2019 tokens according to sense, and then learn embeddings for these re-labeled words. The second paper improves upon the first by employing an earlier pass of non-discriminated embedding learning to obtain vectors used to represent the contexts. Note that by pre-clustering, these methods lose the opportunity to jointly learn the sense-discriminated vectors and the clustering. Other weaknesses include their fixed number of sense per word type, and the computational expense of the two-step process\u2014the Huang et al (2012) method took one week of computation to learn multiple embeddings for a 6,000 subset of the 100,000 vocabulary on a corpus containing close to billion tokens.", "startOffset": 81, "endOffset": 780}, {"referenceID": 12, "context": "In the non-parametric version of our method, we build on facility location (Meyerson, 2001): a new cluster is created with probability proportional to the distance from the context to the", "startOffset": 75, "endOffset": 91}, {"referenceID": 3, "context": "Collobert and Weston (2008) replaces the max-likelihood character of the model with a max-margin approach, where the network is encouraged to score the correct n-grams higher than randomly chosen incorrect n-grams.", "startOffset": 0, "endOffset": 28}, {"referenceID": 3, "context": "Collobert and Weston (2008) replaces the max-likelihood character of the model with a max-margin approach, where the network is encouraged to score the correct n-grams higher than randomly chosen incorrect n-grams. Mnih and Hinton (2007) replaces the global normalization of the Bengio model with a tree-structured probability distribution, and also considers multiple positions for each word in the tree.", "startOffset": 0, "endOffset": 238}, {"referenceID": 18, "context": "Word vector representations or embeddings have been used in various NLP tasks such as named entity recognition (Neelakantan and Collins, 2014; Passos et al, 2014; Turian et al, 2010), dependency parsing (Bansal et al, 2014), chunking (Turian et al, 2010; Dhillon and Ungar, 2011), sentiment analysis (Maas et al, 2011), paraphrase detection (Socher et al, 2011) and learning representations of paragraphs and documents (Le and Mikolov, 2014).", "startOffset": 111, "endOffset": 182}, {"referenceID": 10, "context": "Word vector representations or embeddings have been used in various NLP tasks such as named entity recognition (Neelakantan and Collins, 2014; Passos et al, 2014; Turian et al, 2010), dependency parsing (Bansal et al, 2014), chunking (Turian et al, 2010; Dhillon and Ungar, 2011), sentiment analysis (Maas et al, 2011), paraphrase detection (Socher et al, 2011) and learning representations of paragraphs and documents (Le and Mikolov, 2014).", "startOffset": 419, "endOffset": 441}, {"referenceID": 20, "context": "The word clusters obtained from Brown clustering (Brown et al, 1992) have similarly been used as features in named entity recognition (Miller et al, 2004; Ratinov and Roth, 2009) and dependency parsing (Koo et al, 2008), among other tasks.", "startOffset": 134, "endOffset": 178}, {"referenceID": 22, "context": "Reisinger and Mooney (2010a) introduce a method for constructing multiple sparse, high-dimensional vector representations of words.", "startOffset": 0, "endOffset": 29}, {"referenceID": 22, "context": "Reisinger and Mooney (2010a) introduce a method for constructing multiple sparse, high-dimensional vector representations of words. Huang et al (2012) extends this approach incorporating global document context to learn multiple dense, low-dimensional embeddings by using recursive neural networks.", "startOffset": 0, "endOffset": 151}, {"referenceID": 22, "context": "Reisinger and Mooney (2010a) introduce a method for constructing multiple sparse, high-dimensional vector representations of words. Huang et al (2012) extends this approach incorporating global document context to learn multiple dense, low-dimensional embeddings by using recursive neural networks. Both the methods perform word sense discrimination as a preprocessing step by clustering contexts for each word type, making training more expensive. While methods such as those described in Dhillon and Ungar (2011) and Reddy et al (2011) use token-specific representations of words as part of the learning algorithm, the final outputs are still one-to-one mappings between word types and word embeddings.", "startOffset": 0, "endOffset": 515}, {"referenceID": 22, "context": "Reisinger and Mooney (2010a) introduce a method for constructing multiple sparse, high-dimensional vector representations of words. Huang et al (2012) extends this approach incorporating global document context to learn multiple dense, low-dimensional embeddings by using recursive neural networks. Both the methods perform word sense discrimination as a preprocessing step by clustering contexts for each word type, making training more expensive. While methods such as those described in Dhillon and Ungar (2011) and Reddy et al (2011) use token-specific representations of words as part of the learning algorithm, the final outputs are still one-to-one mappings between word types and word embeddings.", "startOffset": 0, "endOffset": 538}, {"referenceID": 22, "context": "To extend the Skip-gram model to learn multiple embeddings per word we follow previous work (Huang et al, 2012; Reisinger and Mooney, 2010a)", "startOffset": 92, "endOffset": 140}, {"referenceID": 12, "context": "Our approach is closely related to the online non-parametric clustering procedure described in Meyerson (2001). We create a new cluster (sense) for a word type with probability proportional to the distance of its context to the nearest cluster (sense).", "startOffset": 95, "endOffset": 111}, {"referenceID": 24, "context": "To evaluate our algorithms we train embeddings using the same corpus and vocabulary as used in Huang et al (2012), which is the April 2010 snapshot of the Wikipedia corpus (Shaoul and Westbury, 2010).", "startOffset": 172, "endOffset": 199}, {"referenceID": 24, "context": "To evaluate our algorithms we train embeddings using the same corpus and vocabulary as used in Huang et al (2012), which is the April 2010 snapshot of the Wikipedia corpus (Shaoul and Westbury, 2010). It contains approximately 2 million articles and 990 million tokens. In all our experiments we remove all the words with less than 20 occurrences and use a maximum context window (N ) of length 5 (5 words before and after the word occurrence). We fix the number of senses (K) to be 3 for the MSSG model unless otherwise specified. Our hyperparameter values were selected by a small amount of manual exploration on a validation set. In NP-MSSG we set \u03bb to -0.5. The Skip-gram model, MSSG and NP-MSSG models sample one noisy context word (S) for each of the observed context words. We train our models using AdaGrad stochastic gradient decent (Duchi et al, 2011) with initial learning rate set to 0.025. Similarly to Huang et al (2012), we don\u2019t use a regularization penalty.", "startOffset": 173, "endOffset": 935}, {"referenceID": 3, "context": "C&W refers to the language model by Collobert and Weston (2008) and HLBL model is the method described in Mnih and Hinton (2007).", "startOffset": 36, "endOffset": 64}, {"referenceID": 3, "context": "C&W refers to the language model by Collobert and Weston (2008) and HLBL model is the method described in Mnih and Hinton (2007). On WordSim-353 task, we see that our model performs significantly better than the previous neural network model for learning multi-representations per word (Huang et al, 2012).", "startOffset": 36, "endOffset": 129}, {"referenceID": 22, "context": "Pruned TF-IDF (Reisinger and Mooney, 2010a), ESA (Gabrilovich and Markovitch, 2007) and Tiered TF-IDF (Reisinger and Mooney, 2010b) construct spare, high-dimensional representations.", "startOffset": 14, "endOffset": 43}, {"referenceID": 7, "context": "Pruned TF-IDF (Reisinger and Mooney, 2010a), ESA (Gabrilovich and Markovitch, 2007) and Tiered TF-IDF (Reisinger and Mooney, 2010b) construct spare, high-dimensional representations.", "startOffset": 49, "endOffset": 83}, {"referenceID": 23, "context": "Pruned TF-IDF (Reisinger and Mooney, 2010a), ESA (Gabrilovich and Markovitch, 2007) and Tiered TF-IDF (Reisinger and Mooney, 2010b) construct spare, high-dimensional representations.", "startOffset": 102, "endOffset": 131}], "year": 2015, "abstractText": "There is rising interest in vector-space word embeddings and their use in NLP, especially given recent methods for their fast estimation at very large scale. Nearly all this work, however, assumes a single vector per word type\u2014ignoring polysemy and thus jeopardizing their usefulness for downstream tasks. We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type. It differs from recent related work by jointly performing word sense discrimination and embedding learning, by non-parametrically estimating the number of senses per word type, and by its efficiency and scalability. We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours.", "creator": "TeX"}}}