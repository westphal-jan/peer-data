{"id": "1505.00908", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2015", "title": "Reinforced Decision Trees", "abstract": "In order to speed-up classification models when facing a large number of categories, one usual approach consists in organizing the categories in a particular structure, this structure being then used as a way to speed-up the prediction computation. This is for example the case when using error-correcting codes or even hierarchies of categories. But in the majority of approaches, this structure is chosen \\textit{by hand}, or during a preliminary step, and not integrated in the learning process. We propose a new model called Reinforced Decision Tree which simultaneously learns how to organize categories in a tree structure and how to classify any input based on this structure. This approach keeps the advantages of existing techniques (low inference complexity) but allows one to build efficient classifiers in one learning step. The learning algorithm is inspired by reinforcement learning and policy-gradient techniques which allows us to integrate the two steps (building the tree, and learning the classifier) in one single algorithm. The learning algorithm is based on the Reinforced Decision Tree which allows us to build efficient classifiers in one learning step.\n\n\n\n\nIt will be useful to describe this procedure for classification using the Reinforced Decision Tree, the Reinforced Decision Tree, and the Reinforced Decision Tree. For example, we can implement a model like a tree that does not consist of a specific input. We will have to use the Reinforced Decision Tree (RNG) of the classification in order to get a better classification for each input. The training algorithm is based on the Reinforced Decision Tree, based on the Reinforced Decision Tree, the Reinforced Decision Tree, and the Reinforced Decision Tree. The training algorithm is based on the Reinforced Decision Tree and the Reinforced Decision Tree.\nWe first define a model called Reinforced Decision Tree. To build a learning algorithm, we define a function which uses the Reinforced Decision Tree as a structure in a hierarchical order. The training algorithm is based on the Reinforced Decision Tree and the Reinforced Decision Tree. It has a learning algorithm based on the Reinforced Decision Tree (RNG) and the Reinforced Decision Tree. The training algorithm is based on the Reinforced Decision Tree and the Reinforced Decision Tree. We then create a model based on the Reinforced Decision Tree with the training algorithm, and then the Reinforced Decision Tree.\nTo build a learning algorithm, we define a function which uses the Reinforced Decision Tree. The training algorithm is based on the Reinforced Decision Tree. We use the Reinforced Decision Tree as a structure", "histories": [["v1", "Tue, 5 May 2015 07:58:40 GMT  (714kb,D)", "http://arxiv.org/abs/1505.00908v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aur\\'elia l\\'eon", "ludovic denoyer"], "accepted": false, "id": "1505.00908"}, "pdf": {"name": "1505.00908.pdf", "metadata": {"source": "CRF", "title": "Reinforced Decision Trees", "authors": ["Aur\u00e9lia L\u00e9on", "Ludovic Denoyer"], "emails": ["aurelia.leon@lip6.fr", "ludovic.denoyer@lip6.fr"], "sections": [{"heading": null, "text": "Keywords: reinforcement learning, machine learning, policy gradient, decision trees, classification"}, {"heading": "1. Introduction", "text": "The complexity of classification models is usually highly related, and typically linear w.r.t the number of possible categories denoted C. When facing problems with a very large number of classes, like text classification in large ontologies, object recognition or word prediction in deep learning language models, this becomes a critical point making classification methods inefficient in term of inference complexity. There is thus a need to develop new methods able to predict in large output spaces at a low cost.\nSeveral methods have been recently developed for reducing the classification speed. They are based on the idea of using a structure that organizes the possible outputs, and allows one to reduce the inference complexity. Two main families of approaches have been proposed: (i) error-correcting codes approaches (Dietterich and Bakiri; Schapire; Cisse\u0301 et al.) that associate a short code to each category; the classification becomes a code prediction problem which can be achieved faster by predicting each element of the code. (ii) The second family is hierarchical methods (Bengio et al.; Liu et al., 2013; Weston et al.) where the possible categories are leaves of a tree. In that case, an output is predicted by choosing a path in the tree like in decision trees. These two methods typically involve a prediction complexity of O(logC) allowing a great speed-up.\nBut these approaches suffer from one major drawback: the structure used for prediction (error correcting codes, or tree) is usually built during a preliminary step - before learning the classifier - following hand-made heuristics, typically by using clustering algorithms or Huffman codes (Le and Mikolov, 2014). The problem is that the quality of the obtained\nar X\niv :1\n50 5.\n00 90\n8v 1\n[ cs\n.L G\n] 5\nM ay\n2 01\n5\nstructure greatly influences the quality of the final classifier, and this step is thus a critical step.\nIn this paper, we propose the Reinforced Decision Tree (RDT) model which is able to simultaneously learn how to organize categories in a hierarchy and learn the corresponding classifier in a single step. The obtained system is a high speed and efficient predictive model where the category structure has been fitted for the particular task to solve, while it is built by hand in existing approaches. The main idea of RDT is to consider the classification problem as a sequential decision process where a learned policy guides any input x in a tree structure from the root to one leaf. The learning algorithm is inspired from policygradient methods (Baxter and Bartlett, 1999) which allows us to act on both the way an input x falls into the tree, but also on the categories associated with the leaves of this tree. The difference with the classical Reinforcement Learning context is that the feedback provided to the system is a derivable loss function as proposed in (Denoyer and Gallinari, 2014) which gives more information than a reward signal, and allows fast learning of the parameters.\nThe contributions are:\n\u2022 A novel model able to simultaneously discover a relevant hierarchy of categories and the associated classification model in one step.\n\u2022 A gradient-based learning method based on injecting a derivable loss function in policy gradient algorithms.\n\u2022 A first set of preliminary experiments over toy datasets allowing to better understand the properties of such an approach."}, {"heading": "2. Notations and Model", "text": "We consider the multi-class classification problem where each input x \u2208 Rn has to be associated with one of the C possible categories1. Let us denote y the label of x, y \u2208 RC such that yi = 1 if x belongs to class i and yi = \u22121 elsewhere. We will denote {(x1, y1), ..., (xN , yN )} the set of N training examples."}, {"heading": "2.1 Reinforced Decision Trees Architecture", "text": "The Reinforced Decision Tree (RDT) architecture shares common points with decision trees. Let us denote T\u03b8,\u03b1 such a tree with parameters \u03b8 and \u03b1 that will be defined in the later:\n\u2022 The tree is composed of a set of nodes denoted nodes(T\u03b8,\u03b1) = {n1, ..., nT } where T is the number of nodes of the tree\n\u2022 n1 is the root of the tree.\n\u2022 parent(ni) = nj means that node nj is the parent of ni. Each node but n1 has only one parent, n1 has no parent since it is the root of the tree.\n1. The model naturally handles multi-label classification problems which will not be detailed here for sake of simplicity\nReinforced Decision Trees\n\u2022 leaf(ni) = true if and only if ni is a leaf of the tree. Note that we do not have any constraint concerning the number of leaves or the topology of the tree. Each node of the tree is associated with its own set of parameters:\n\u2022 A node ni is associated with a set of parameters denoted \u03b8i if ni is an internal node i.e leaf(ni) = false.\n\u2022 A node ni is associated with a set of parameters denoted \u03b1i \u2208 RC if ni is a leaf of the tree.\nParameters \u03b8 = {\u03b8i} are the parameters of the policy that will guide an input x from the root node to one of the leaf of the tree. Parameters \u03b1i correspond to the prediction that will be produced when an input reached the leaf ni. Our architecture is thus very close to classical decision trees, the major difference being that the prediction associated with each leaf is a set of parameters that will be learned during the training process, allowing the model to choose how to match the leaves of the tree with the categories. The model will thus be able to simultaneously learn which path an input has to follow based on the \u03b8i parameters, but also how to organize the categories in the tree based on the \u03b1i parameters. An example of RDT is given in Figure 1."}, {"heading": "2.2 Inference Process", "text": "Let us denote H a trajectory, H = (n(1), ..., n(t)) where n(i) \u2208 nodes(T\u03b8,\u03b1) and (i) is the index of the i-th node of the trajectory. H is thus a sequence of nodes where n(1) = n1, \u2200i > 1, n(i\u22121) = parent(n(i)) and leaf(n(t)) = true.\nAlgorithm 1 Stochastic Gradient RDT Learning Procedure\n1: procedure Learning((x1, y1), ..., (xN , yN )) . the training set 2: is the learning rate 3: \u03b1 \u2248 random 4: \u03b8 \u2248 random 5: repeat 6: i \u2248 uniform(1, N) 7: Sample H = (n(1), ..., n(t)) using the \u03c0\u03b8(x\ni) functions 8: \u03b1(t) \u2190 \u03b1(t) \u2212 \u2207\u03b1\u2206(\u03b1(t), yi) 9: for k \u2208 [1..t\u2212 1] do\n10: \u03b8(k) \u2190 \u03b8(k) \u2212 ( \u2207\u03b8 log \u03c0\u03b8(k)(xi) ) \u2206(\u03b1(t), y\ni) 11: end for 12: until Convergence 13: return T\u03b8,\u03b1 14: end procedure\nEach internal node ni is associated with a function (or policy) \u03c0\u03b8i which role is to compute the probability that a given input x will fall in one of the children of ni. \u03c0\u03b8i is defined as:\n\u03c0\u03b8i : R n \u00d7 nodes(T\u03b8,\u03b1)\u2192 R \u03c0\u03b8i(x, n) = P (n|ni, x) (1)\nwith P (n|ni, x) = 0 if n /\u2208 children(ni). In other words, \u03c0\u03b8i(x, n) is the probability that x moves from ni to n. Note that the different between RDT and DT is that the decision taken at each node is stochastic and not deterministic which will allow us to use gradient-based learning algorithms. The probability of a trajectory H = (n(1), ..., n(t)) given an input x can be written as:\nP (H|x) = t\u22121\u220f i=1 \u03c0\u03b8(i)(x, n(i+1)) (2)\nOnce a trajectory has been sampled, the prediction produced by the model depends on the leaf n(t) reached by x. The model directly outputs \u03b1(t) as a prediction, \u03b1(t) being a vector in RC (one score per class) as explained before. Note that the model produces one score for each possible category, but the inference complexity of this step is O(1) since it just corresponds to returning the value \u03b1(t).\nThe complete inference process is described in Algorithm 1:\n1. Sample a trajectory H = (n(1), ..., n(t)) given x, by sequentially using the policies \u03c0\u03b8(1) , ..., \u03c0\u03b8(t\u22121)\n2. Returns the predicted output \u03b1(t)\nReinforced Decision Trees"}, {"heading": "2.3 Learning Process", "text": "The goal of the learning procedure is to simultaneously learn both the policy functions \u03c0\u03b8i and the output parameters \u03b1i in order to minimize a given learning loss denoted \u2206 which corresponds here to a classification loss (e.g square loss or hinge loss)2\nOur learning algorithm is based on an extension of policy gradient techniques inspired from the Reinforcement Learning literature and similar to Denoyer and Gallinari (2014). More precisely, our learning method is close to the methods proposed in Baxter and Bartlett (1999) with the difference that, instead of considering a reward signal which is usual in reinforcement learning, we consider a loss function \u2206. This function computes the quality of the system, providing a richer feedback information than simple rewards since it can be derivated, and thus gives the direction in which parameters have to be updated.\nThe performance of our system is denoted J(\u03b8, \u03b1):\nJ(\u03b8, \u03b1) = EP\u03b8(x,H,y)[\u2206(F\u03b1(x,H), y)] (3)\nwhere F\u03b1(x,H) is the prediction made following trajectory H - i.e the sequence of nodes chosen by the \u03c0-functions. The optimization of J can be made by gradient-descent techniques and we need to compute the gradient of J :\n\u2207\u03b8,\u03b1J(\u03b8, \u03b1) = \u222b \u2207\u03b8,\u03b1 (P\u03b8(H|x)\u2206(F\u03b1(x,H), y))P (x, y)dHdxdy (4)\nThis gradient can be simplified such that: \u2207\u03b8,\u03b1J(\u03b8, \u03b1) = \u222b \u2207\u03b8,\u03b1 (P\u03b8(H|x)) \u2206(F\u03b1(x,H), y)P (x, y)dHdxdy\n+ \u222b P\u03b8(H|x)\u2207\u03b8,\u03b3\u2206(F\u03b1(x,H), y)P (x, y)dHdxdy\n= \u222b P\u03b8(H|x)\u2207\u03b8,\u03b1 (logP\u03b8(H|x)) \u2206(F\u03b1(x,H), y)P (x, y)dHdxdy\n+ \u222b P\u03b8(H|x)\u2207\u03b8,\u03b1\u2206(F\u03b1(x,H), y)P (x, y)dHdxdy\n(5)\nUsing the Monte Carlo approximation of this expectation by taking M trail histories over the N training examples, and given that \u2206(F\u03b1(x i, H), y) = \u2206(\u03b1(t), y), we obtain:\n\u2207\u03b8,\u03b1J(\u03b8, \u03b1) = 1\nN\n1\nM N\u2211 i=1 M\u2211 k=1  t\u22121\u2211 j=1 ( \u2207\u03b8,\u03b1 ( log \u03c0\u03b8(j)(x i) ) \u2206(\u03b1(t), y) ) +\u2207\u03b8,\u03b1\u2206(\u03b1(t), y)  (6) Intuitively, the gradient is composed of two terms:\n\u2022 The first term aims at penalizing trajectories with high loss - and thus encouraging to find trajectories with low loss. The first term only acts on the \u03b8 parameters, modifying the probabilities computed at the internal nodes levels.\n2. Note that our approach is not restricted to classification and can be used for regression for example, see Section 3.\n\u2022 The second term is the gradient computed over the final loss and concerns the \u03b1 values corresponding to the leaf node where the input x arrives at the end of the process. It thus changes the \u03b1 used for prediction in order to capture the category of x. This gradient term is responsible about how to allocate the categories in the leaves of the tree.\nThe learning algorithm in its stochastic gradient variant is described in Algorithm 1."}, {"heading": "2.4 Discussion", "text": "Inference Complexity: The complexity of the inference process is linear with the depth of the tree. Typically, in a multi-class classification problem, the depth of the tree will be proportional to logC, resulting in a very high speed inference process similar to the one obtained for example using Hierarchical SoftMax modules.\nLearning Complexity: The policy gradient algorithm developed in this paper is an iterative gradient-based method. Each learning iteration complexity is O(N logC) but the number of needed iterations is not known. Moreover, the optimization problem is clearly not a convex problem, and the system can be stuck in a local minimum. As explained in Section 3, a way to avoid problematic local minimum is to choose a number of leaves which is higher than the number of categories, giving more freedom degrees to the model.\nUsing RDT for complex problems Different functions topologies can be used for \u03c0. In the following we have used simple linear functions, but more sophisticated ones can be tested like neural networks. Moreover, in our model, there is no constraints upon the \u03b1 parameters, nor about the loss function \u2206 which only has to be derivable. It thus means that our model can also be used for other tasks like multivariate regression, or even for producing continuous outputs at a low price. In that case, RDTs act as discretization processes where the objective of the task and the discretization are made simultaneously."}, {"heading": "3. Preliminary Experiments", "text": "In this Section, we provide a set of experiments that have been made on toy-datasets to better understand the ability of RDT to perform good predictions and to discover a relevant hierarchy3. Our model has been compared to the same model but where the categories associated with the leaves (the \u03b1i values) have been chosen randomly, each leaf being associated to one possible category - i.e each vector \u03b1i is full of \u22121 with one 1-value at a random position - this model is called Random Tree. In that case, the \u03b1-parameters are not updated during the gradient descent. Hyper-parameters (learning rate and number of iterations) have been tuned by cross-validation, and the results have been averaged on five different runs. We also made comparisons with a linear SVM (one versus all) and decision trees4. These preliminary experiments aim at showing the ability of our integrated approach to learn how to associate categories to leaves of the tree.\nWe consider simple 2D datasets composed of categories sampled following a Gaussian distribution. Each category is composed of 100 vectors (50 for train, 50 for test) sampled\n3. Experiments on real-world datasets are currently made 4. using the implementation of sklearn\nReinforced Decision Trees\nfollowing N (\u00b5c, \u03c3c) where \u00b5c has been uniformly sampled between {(\u22121,\u22121) and (1, 1)} and \u03c3c has been also sampled - see Figure 2-right for the dataset with C = 16 categories and 3-right for the dataset with C = 32 categories. Two preliminary conclusions can be drawn from the presented results: first, one can see that, when considering a particular architecture, the RDT is able to determine how to allocate the categories in the leaves of the tree: the performance of RDT w.r.t random trees where categories have been randomly sampled in the leaves is clearly better. (ii) The second conclusion is that, when considering a problem with C categories, a good option is to build a tree with L > C leaves since it gives more freedom degrees to the model which will more easily find how to allocate the categories in the leaves. Note that, for the problem with 32 categories, a 84% accuracy is obtained when using a tree of depth 6: only 6 binary classifiers are used for predicting the category. In comparison to a linear SVM, where the inference complexity is higher than ours (O(C)) our approach performs better. This is mainly due to the ability of our model to learn non-linear decision frontiers. At last, when considering decision trees, one can see that they are sometimes equivalent to RDT. We think that this is mainly due to the small dimension of the input space, and the small number of examples for which decision trees are well adapted."}, {"heading": "4. Related Work", "text": "In multi-class classification problems, the classical approach is to train one-versus-all classifiers. It is one of the most efficient technique (Jr and Freitas; Babbar and Partalas, 2013) even with a very large number of classes, but the inference complexity is linear w.r.t the number of possible categories resulting in low-speed prediction algorithms.\nHierarchical models have been proposed to reduce this complexity. They have been developed for two cases: (i) a first one where a hierarchy of category is already known; in that case, the hierarchy of classifiers is mapped on the hierarchy of classes. (ii) A second\napproach closer to ours consists in automatically building a hierarchy from the training set. This is usually done in a preliminary step by using for example clustering techniques like spectral clustering on the confusion matrix (Bengio et al.), using probabilistic label tree (Liu et al., 2013) or even partitioning optimization (Weston et al.). Facing these approaches, RDT has the advantage to learn the hierarchy and the classifier in an integrated step only guided by a unique loss function. The closest work is perhaps Choromanska and Langford (2014) which discovers the hierarchy using online learning algorithms, the construction of the tree being made during learning. Other families of methods have been proposed like errorcorrecting codes (Dietterich and Bakiri; Schapire; Cisse\u0301 et al.), sparse coding (Zhao and Xing, 2013) or even using representation learning techniques, representations of categories being obtained by unsupervised models (Weinberger and Chapelle; Bengio et al.).\nAt last, the use of sequential learning models, inspired by reinforcement learning, in the context of classification or regression has been explored recently for different applications like features selection (Dulac-arnold et al.) or image classification (Dulac-Arnold et al., 2014; Gregor et al., 2015). Our model belongs to this family of approaches."}, {"heading": "5. Conclusion and Perspectives", "text": "We have presented Reinforced Decision Trees which is a learning model able to simultaneously learn how to allocate categories in a hierarchy and how to classify inputs. RDTs are sequential decision models where the prediction over one input is made using O(logC) classifiers, making this method suitable for problems with large number of categories. Moreover, the method can be easily adapted to any learning problem like regression or ranking, by changing the loss function. RDTs are learned by using a policy gradient-inspired methods. Preliminary results show the effectiveness of this approach. Future work mainly involves\nReinforced Decision Trees\nreal-world experimentation, but also extension of this model to continuous outputs problems."}], "references": [{"title": "On Flat versus Hierarchical Classification in Large-Scale Taxonomies", "author": ["Rohit Babbar", "Ioannis Partalas"], "venue": "Neural Inf. Process. Syst., pages", "citeRegEx": "Babbar and Partalas.,? \\Q2013\\E", "shortCiteRegEx": "Babbar and Partalas.", "year": 2013}, {"title": "Logarithmic Time Online Multiclass prediction", "author": ["Anna Choromanska", "John Langford"], "venue": null, "citeRegEx": "Choromanska and Langford.,? \\Q2014\\E", "shortCiteRegEx": "Choromanska and Langford.", "year": 2014}, {"title": "Deep sequential neural network. arXiv preprint arXiv:1410.0510 - Workshop Deep Learning", "author": ["Ludovic Denoyer", "Patrick Gallinari"], "venue": null, "citeRegEx": "Denoyer and Gallinari.,? \\Q2014\\E", "shortCiteRegEx": "Denoyer and Gallinari.", "year": 2014}, {"title": "Sequentially generated instance-dependent image representations for classification", "author": ["Gabriel Dulac-Arnold", "Ludovic Denoyer", "Nicolas Thome", "Matthieu Cord", "Patrick Gallinari"], "venue": "Internation Conference on Learning Representations - ICLR 2014,", "citeRegEx": "Dulac.Arnold et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dulac.Arnold et al\\.", "year": 2014}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra"], "venue": "CoRR, abs/1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V. Le", "Tomas Mikolov"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Le and Mikolov.,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Probabilistic label trees for efficient large scale image classification", "author": ["Baoyuan Liu", "Fereshteh Sadeghi", "Marshall Tappen", "Ohad Shamir", "Ce Liu"], "venue": "Comput. Vis. Pattern Recognit.,", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Sparse output coding for large-scale visual recognition", "author": ["Bin Zhao", "Eric P. Xing"], "venue": "Comput. Vis. Pattern Recognit., pages 3350\u20133357,", "citeRegEx": "Zhao and Xing.,? \\Q2013\\E", "shortCiteRegEx": "Zhao and Xing.", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "But these approaches suffer from one major drawback: the structure used for prediction (error correcting codes, or tree) is usually built during a preliminary step - before learning the classifier - following hand-made heuristics, typically by using clustering algorithms or Huffman codes (Le and Mikolov, 2014).", "startOffset": 289, "endOffset": 311}, {"referenceID": 2, "context": "The difference with the classical Reinforcement Learning context is that the feedback provided to the system is a derivable loss function as proposed in (Denoyer and Gallinari, 2014) which gives more information than a reward signal, and allows fast learning of the parameters.", "startOffset": 153, "endOffset": 182}, {"referenceID": 2, "context": "g square loss or hinge loss)2 Our learning algorithm is based on an extension of policy gradient techniques inspired from the Reinforcement Learning literature and similar to Denoyer and Gallinari (2014). More precisely, our learning method is close to the methods proposed in Baxter and Bartlett (1999) with the difference that, instead of considering a reward signal which is usual in reinforcement learning, we consider a loss function \u2206.", "startOffset": 175, "endOffset": 204}, {"referenceID": 2, "context": "g square loss or hinge loss)2 Our learning algorithm is based on an extension of policy gradient techniques inspired from the Reinforcement Learning literature and similar to Denoyer and Gallinari (2014). More precisely, our learning method is close to the methods proposed in Baxter and Bartlett (1999) with the difference that, instead of considering a reward signal which is usual in reinforcement learning, we consider a loss function \u2206.", "startOffset": 175, "endOffset": 304}, {"referenceID": 0, "context": "It is one of the most efficient technique (Jr and Freitas; Babbar and Partalas, 2013) even with a very large number of classes, but the inference complexity is linear w.", "startOffset": 42, "endOffset": 85}, {"referenceID": 6, "context": "), using probabilistic label tree (Liu et al., 2013) or even partitioning optimization (Weston et al.", "startOffset": 34, "endOffset": 52}, {"referenceID": 7, "context": "), sparse coding (Zhao and Xing, 2013) or even using representation learning techniques, representations of categories being obtained by unsupervised models (Weinberger and Chapelle; Bengio et al.", "startOffset": 17, "endOffset": 38}, {"referenceID": 3, "context": ") or image classification (Dulac-Arnold et al., 2014; Gregor et al., 2015).", "startOffset": 26, "endOffset": 74}, {"referenceID": 4, "context": ") or image classification (Dulac-Arnold et al., 2014; Gregor et al., 2015).", "startOffset": 26, "endOffset": 74}, {"referenceID": 1, "context": "The closest work is perhaps Choromanska and Langford (2014) which discovers the hierarchy using online learning algorithms, the construction of the tree being made during learning.", "startOffset": 28, "endOffset": 60}], "year": 2015, "abstractText": "In order to speed-up classification models when facing a large number of categories, one usual approach consists in organizing the categories in a particular structure, this structure being then used as a way to speed-up the prediction computation. This is for example the case when using error-correcting codes or even hierarchies of categories. But in the majority of approaches, this structure is chosen by hand, or during a preliminary step, and not integrated in the learning process. We propose a new model called Reinforced Decision Tree which simultaneously learns how to organize categories in a tree structure and how to classify any input based on this structure. This approach keeps the advantages of existing techniques (low inference complexity) but allows one to build efficient classifiers in one learning step. The learning algorithm is inspired by reinforcement learning and policy-gradient techniques which allows us to integrate the two steps (building the tree, and learning the classifier) in one single algorithm.", "creator": "LaTeX with hyperref package"}}}