{"id": "1505.04627", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2015", "title": "Simple regret for infinitely many armed bandits", "abstract": "We consider a stochastic bandit problem with infinitely many arms. In this setting, the learner has no chance of trying all the arms even once and has to dedicate its limited number of samples only to a certain number of arms. All previous algorithms for this setting were designed for minimizing the cumulative regret of the learner. In this paper, we propose an algorithm aiming at minimizing the simple regret. As in the cumulative regret setting of infinitely many armed bandits, the rate of the simple regret will depend on a parameter $\\beta$ characterizing the distribution of the near-optimal arms. We prove that depending on $\\beta$, our algorithm is minimax optimal either up to a multiplicative constant or up to a $\\log(n)$ factor. We also provide extensions to several important cases: when $\\beta$ is unknown, in a natural setting where the near-optimal arms have a small variance, and in the case of unknown time horizon. This is done to minimize the large variance in the near-optimal arms. To test the possibility of minimizing the infinite potential of our algorithm, we have used a simple solution in which we perform two types of tests: the finite and the infinite. The second test is to perform the infinite or the infinite without knowing the expected probability of a large variance of the large variance. To further test our algorithm we compute the minimum likelihood of a large variance of the near-optimal arms by using an alternative solution. In our next paper, we will demonstrate a solution using the same optimization that would normally be applied in the finite and the infinite, which is the result of the same optimization. This algorithm will be used to solve the infinite, and it will provide a finite and a finite-less-inverse linear model for our algorithm. The first two tests have been used in this paper, and we discuss a potential solution with a simple optimization solution in which we choose a large, finite-less-inverse linear model. We hope to use the next version of our algorithm to solve this problem.\n\n\n\n\nThis paper proposes a possible solution in which we perform two types of tests: the finite and the infinite. The finite and the infinite. The finite and the infinite. The finite and the infinite. The finite and the infinite. The finite and the infinite. The infinite and the infinite. The finite and the infinite. The finite and the infinite.\nWe are also interested in the first possible option: the finite and the infinite. The infinite and the infinite. We also", "histories": [["v1", "Mon, 18 May 2015 13:16:42 GMT  (118kb,D)", "http://arxiv.org/abs/1505.04627v1", "in 32th International Conference on Machine Learning (ICML 2015)"]], "COMMENTS": "in 32th International Conference on Machine Learning (ICML 2015)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["alexandra carpentier", "michal valko"], "accepted": true, "id": "1505.04627"}, "pdf": {"name": "1505.04627.pdf", "metadata": {"source": "META", "title": "Simple regret for infinitely many armed bandits", "authors": ["Alexandra Carpentier", "Michal Valko"], "emails": ["A.CARPENTIER@STATSLAB.CAM.AC.UK", "MICHAL.VALKO@INRIA.FR"], "sections": [{"heading": "1. Introduction", "text": "Sequential decision making has been recently fueled by several industrial applications, e.g., advertisement, and recommendation systems. In many of these situations, the learner is faced with a large number of possible actions, among which it has to make a decision. The setting we consider is a direct extension of a classical decision-making setting, in which we only receive feedback for the actions we choose, the bandit setting. In this setting, at each time t, the learner can choose among all the actions (called the arms) and receives a sample (reward) from the chosen action, which is typically a noisy characterization of the action. The learner performs n such rounds and its performance is then evaluated with respect to some criterion, for instance the cumulative regret or the simple regret.\nProceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).\nIn the classical, multi-armed bandit setting, the number of actions is assumed to be finite and small when compared to the number of decisions. In this paper, we consider an extension of this setting to infinitely many actions, the infinitely many armed bandits (Berry et al., 1997; Wang et al., 2008; Bonald & Proutie\u0300re, 2013). Inevitably, the sheer amount of possible actions makes it impossible to try each of them even once. Such a setting is practically relevant for cases where one faces a finite, but extremely large number of actions. This setting was first formalized by Berry et al. (1997) as follows. At each time t, the learner can either sample an arm (a distribution) that has been already observed in the past, or sample a new arm, whose mean \u00b5 is sampled from the mean reservoir distribution L.\nThe additional challenges of the infinitely many armed bandits with respect to the multi-armed bandits come from two sources. First, we need to find a good arm among the sampled ones. Second, we need to sample (at least once) enough arms in order to have (at least once) a reasonably good one. These two difficulties ask for a while which we call the arm selection tradeoff. It is different from the known exploration/exploitation tradeoff and more linked to model selection principles: On one hand, we want to sample only from a small subsample of arms so that we can decide, with enough accuracy, which one is the best one among them. On the other hand, we want to sample as many arms as possible in order to have a higher chance to sample a good arm at least once. This tradeoff makes the problem of infinitely many armed bandits significantly different from the classical bandit problem.\nBerry et al. (1997) provide asymptotic, minimax-optimal (up to a log n factor) bounds for the average cumulative regret, defined as the difference between n times the highest possible value \u00b5\u0304\u2217 of the mean reservoir distribution and the mean of the sum of all samples that the learner collects. A follow-up on this result was the work of Wang et al. (2008), providing algorithms with finite-time regret bounds and the work of Bonald & Proutie\u0300re (2013), giving an algorithm that is optimal with exact constants in a strictly more specific setting. In all of this prior work, the authors show\nar X\niv :1\n50 5.\n04 62\n7v 1\n[ cs\n.L G\n] 1\n8 M\nay 2\n01 5\nthat it is the shape of the arm reservoir distribution what characterizes the minimax-optimal rate of the average cumulative regret. Specifically, Berry et al. (1997) and Wang et al. (2008) assume that the mean reservoir distribution is such that, for a small \u03b5 > 0, locally around the best arm \u00b5\u0304\u2217, we have that\nP\u00b5\u223cL (\u00b5\u0304\u2217 \u2212 \u00b5 \u2265 \u03b5) \u2248 \u03b5\u03b2 , (1)\nthat is, they assume that the mean reservoir distribution is \u03b2-regularly varying in \u00b5\u0304\u2217. When this assumption is satisfied with a known \u03b2, their algorithms achieve an expected cumulative regret of order\nE [Rn]=O ( max ( n \u03b2 \u03b2+1 polylog n, \u221a npolylog n )) . (2)\nThe limiting factor in the general setting is a 1/ \u221a n rate for estimating the mean of any of the arms with n samples. This gives the rate (2) of \u221a n. It can be refined if the distributions of the arms, that are sampled from the mean reservoir distribution, are Bernoulli of mean \u00b5 and \u00b5\u0304\u2217 = 1 or in the same spirit, if the distributions of the arms are defined on [0, 1] and \u00b5\u0304\u2217 = 1 as\nE [Rn] = O ( n \u03b2 \u03b2+1 polylog n ) . (3)\nBonald & Proutie\u0300re (2013) refine the result (3) even more by removing the polylog n factor and proving upper and lower bounds that exactly match, even in terms of constants, for a specific sub-case of a uniform mean reservoir distribution. Notice that the rate (3) is faster than the more general rate (2). This comes from the fact that they assume that the variances of the arms decay with their quality, making finding a good arm easier. For both rates (2 and 3), \u03b2 is the key parameter for solving the arm selection tradeoff: with smaller \u03b2 it is more likely that the mean reservoir distribution outputs a high value, and therefore, we need fewer arms for the optimal arm selection tradeoff.\nPrevious algorithms for this setting were designed for minimizing the cumulative regret of the learner which optimizes the cumulative sum of the rewards. In this paper, we consider the problem of minimizing the simple regret. We want to select an optimal arm given the time horizon n. The simple regret is the difference between the mean of the arm that the learner selects at time n and the highest possible mean \u00b5\u0304\u2217. The problem of minimizing the simple regret in a multi-armed bandit setting (with finitely many arms) has recently attracted significant attention (Even-Dar et al., 2006; Audibert et al., 2010; Kalyanakrishnan et al., 2012; Kaufmann & Kalyanakrishnan, 2013; Karnin et al., 2013; Gabillon et al., 2012; Jamieson et al., 2014) and algorithms have been developed either in the setting of a fixed budget which aims at finding an optimal arm or in the setting of a floating budget which aims at finding an \u03b5-optimal arm.\nAll prior work on simple regret considers a fixed number of arms that will be ultimately all explored and cannot be applied to an infinitely many armed bandits or to a bandit problem with the number of arms larger than the available time budget. An example where efficient strategies for minimizing the simple regret of an infinitely many armed bandit are relevant is the search of a good biomarker in biology, a single feature that performs best on average (Hauskrecht et al., 2006). There can be too many possibilities that we cannot afford to even try each of them in a reasonable time. Our setting is then relevant for this special case of single feature selection. In this paper, we provide the following results for the simple regret of an infinitely many armed bandit, a problem that was not considered before.\n\u2022 We propose an algorithm that for a fixed horizon n achieves the finite-time simple regret rate\nrn = O ( max ( n\u22121/2, n\u2212 1 \u03b2 polylog n )) .\n\u2022 We prove corresponding lower bounds for this infinitely many armed simple regret problem, that are matching up to a multiplicative constant for \u03b2 < 2, and matching up to a polylog n for \u03b2 \u2265 2.\n\u2022 We provide three important extensions:\n\u2013 The first extension concerns the case where the distributions of the arms are defined on [0, 1] and where \u00b5\u0304\u2217 = 1. In this case, replacing the Hoeffding bound in the confidence term of our algorithm by a Bernstein bound, bounds the simple regret as\nrn=O ( max( 1n polylog n, (n log n) \u2212 1\u03b2 polyloglog n ) .\n\u2013 The second extension treats unknown \u03b2. We prove that it is possible to estimate \u03b2 with enough precision, so that its knowledge is not necessary for implementing the algorithm. This can be also applied to the prior work (Berry et al., 1997; Wang et al., 2008) where \u03b2 is also necessary for implementation and optimal bounds.\n\u2013 Finally, in the third extension we make the algorithm anytime using known tools.\n\u2022 We provide simple numerical simulations of our algorithm and compare it to infinitely many armed bandit algorithms optimizing cumulative regret and to multiarmed bandit algorithms optimizing simple regret.\nBesides research on infinitely many arms bandits, there exist many other settings where the number of actions may be infinite. One class of examples is fixed design such as linear bandits (Dani et al., 2008) other settings consider bandits in known or unknown metric space (Kleinberg et al.,\n2008; Munos, 2014; Azar et al., 2014). These settings assume regularity properties that are very different from the properties assumed in the infinitely many arm bandits and give rise to significantly different approaches and results. Furthermore, in classic optimization settings, one assumes that in addition to the rewards, there is side information available through the position of the arms, combined with a smoothness assumption on the reward, which is much more restrictive. On the contrary, we only assume a bound on the proportion of near-optimal arms. It is not always the case that there is side information through a topology on the arms. In such cases, the infinitely many armed setting is applicable while optimization routines are not."}, {"heading": "2. Setting", "text": "Learning setting Let L\u0303 be a distribution of distributions. We call L\u0303 the arm reservoir distribution, i.e., the distribution of the means of arms. Let L be the distribution of the means of the distributions output by L\u0303, i.e., the mean reservoir distribution. Let At denote the changing set of Kt arms at time t.\nAt each time t + 1, the learner can either choose an arm kt+1 among the set of the Kt arms At = {\u03bd1, . . . , \u03bdKt} that it has already observed (in this case, Kt+1 = Kt and At+1 = At), or choose to get a sample of a new arm that is generated according to L\u0303 (in this case, Kt+1 = Kt + 1 and At+1 = At \u222a {\u03bdKt+1} where \u03bdKt+1 \u223c L\u0303). Let \u00b5i be the mean of arm i, i.e., the mean of distribution \u03bdi for i \u2264 Kt. We assume that \u00b5i always exists.\nIn this setting, the learner observes a sample at each time. At the end of the horizon, which happens at a given time n, the learner has to output an arm k\u0302 \u2264 Kn, and its performance is assessed by the simple regret\nrn = \u00b5\u0304 \u2217 \u2212 \u00b5k\u0302,\nwhere \u00b5\u0304\u2217 = arg infm (P\u00b5\u223cL(\u00b5 \u2264 m) = 1) is the right end point of the domain.\nAssumption on the samples The domain of the arm reservoir distribution L\u0303 are distributions of arm samples. We assume that these distributions \u03bd are bounded. Assumption 1 (Bounded distributions in the domain of L\u0303). Let \u03bd be a distribution in the domain of L\u0303. Then \u03bd is a bounded distribution. Specifically, there exists an universal constant C > 0 such that the domain of \u03bd is contained in [\u2212C,C].\nThis implies that the expectations of all distributions generated by L\u0303 exist, are finite, and bounded by C. In particular, this implies that\n\u00b5\u0304\u2217 = arg inf m (P\u00b5\u223cL(\u00b5 \u2264 m) = 1) < +\u221e,\nwhich implies that the regret is well defined, and that the domain of L is bounded by 2C. Note that all the results that we prove hold also for sub-Gaussian distributions \u03bd and bounded L. Furthermore, it would possible to relax the sub-Gaussianity using different estimators recently developed for heavy-tailed distributions (Catoni, 2012).\nAssumption on the arm reservoir distribution We now assume that the mean reservoir distribution L has a certain regularity in its right end point, which is a standard assumption for infinitely many armed bandits. Note that this implies that the distribution of the means of the arms is in the domain of attraction of a Weibull distribution, and that it is related to assuming that the distribution is \u03b2 regularly varying in its end point \u00b5\u0304\u2217.\nAssumption 2 (\u03b2 regularity in \u00b5\u0304\u2217). Let \u03b2 > 0. There exist E\u0303, E\u0303\u2032 > 0, and 0 < B\u0303 < 1 such that for any 0 \u2264 \u03b5 \u2264 B\u0303,\nE\u0303\u2032\u03b5\u03b2 \u2265 P\u00b5\u223cL (\u00b5 > \u00b5\u0304\u2217 \u2212 \u03b5) \u2265 E\u0303\u03b5\u03b2 .\nThis assumption is the same as the classical one (1). Standard bounded distributions satisfy Assumption 2 for a specific \u03b2, e.g., all the \u03b2 distributions, in particular the uniform distribution, etc."}, {"heading": "3. Main results", "text": "In this section, we first present the information theoretic lower bounds for the infinitely many armed bandits with simple regret as the objective. We then present our algorithm and its analysis proving the upper bounds that match the lower bounds \u2014 in some cases, depending on \u03b2, up to a polylog n factor. This makes our algorithm (almost) minimax optimal. Finally, we provide three important extensions as corollaries."}, {"heading": "3.1. Lower bounds", "text": "The following theorem exhibits the information theoretic complexity of our problem and is proved in Appendix C. Note that the rates crucially depend on \u03b2.\nTheorem 1 (Lower bounds). Let us write S\u03b2 for the set of distributions of arms distributions L\u0303 that satisfy Assumptions 1 and 2 for the parameters \u03b2, E\u0303, E\u0303\u2032, C. Assume that n is larger than a constant that depends on \u03b2, E\u0303, E\u0303\u2032, B\u0303, C. Depending on the value of \u03b2, we have the following results, for any algorithm A, where v is a small enough constant.\n\u2022 Case \u03b2 < 2: With probability larger than 1/3,\ninf A sup L\u0303\u2208S\u03b2\nrn \u2265 vn\u22121/2.\n\u2022 Case \u03b2 \u2265 2: With probability larger than 1/3,\ninf A sup L\u0303\u2208S\u03b2\nrn \u2265 vn\u22121/\u03b2 .\nRemark 1. Comparing these results with the rates for the cumulative regret problem (2) from the prior work, one can notice that there are two regimes for the cumulative regret results. One regime is characterized by a rate of \u221a n for \u03b2 \u2264 1, and the other characterized by a n\u03b2/(1+\u03b2) rate for \u03b2 \u2265 1. Both of these regimes are related to the arm selection tradeoff. The first regime corresponds to easy problems where the mean reservoir distribution puts a high mass close to \u00b5\u0304\u2217, which favors sampling a good arm with high mean from the reservoir. In this regime, the \u221a n rate comes from the parametric 1/ \u221a n rate for estimating the mean of any arm with n samples. The second regime corresponds to more difficult problems where the reservoir is unlikely to output a distribution with mean close to \u00b5\u0304\u2217 and where one has to sample many arms from the reservoir. In this case, the \u221a n rate is not reachable anymore because there are too many arms to choose from sub-samples of arms containing good arms. The same dynamics exists also for the simple regret, where there are again two regimes, one characterized by a n\u22121/2 rate for \u03b2 \u2264 2, and the other characterized by a n\u22121/\u03b2 rate for \u03b2 \u2265 2. Provided that these bounds are tight (which is the case, up to a polylog n, Section 3.2), one can see that there is an interesting difference between the cumulative regret problem and the simple regret one. Indeed, the change of regime is here for \u03b2 = 2 and not for \u03b2 = 1, i.e., the parametric rate of n\u22121/2 is valid for larger values of \u03b2 for the simple regret. This comes from the fact that for the simple regret objective, there is no exploitation phase and everything is about exploring. Therefore, an optimal strategy can spend more time exploring the set of arms and reach the parametric rate also in situations where the cumulative regret does not correspond to the parametric rate. This has also practical implications examined empirically in Section 5."}, {"heading": "3.2. SiRI and its upper bounds", "text": "In this section, we present our algorithm, the Simple Regret for Infinitely many arms (SiRI) and its analysis.\nThe SiRI algorithm Let b = min(\u03b2, 2), and let\nT\u0304\u03b2 = dA(n)nb/2e,\nwhere\nA(n) =  A, if \u03b2 < 2 A/ log(n)2, if \u03b2 = 2 A/ log(n), if \u03b2 > 2\nwhere A is a small constant whose precise value will depend on our analysis. Let log2 be the logarithm in base 2.\nAlgorithm 1 SiRI Simple Regret for Infinitely Many Armed Bandits\nParameters: \u03b2,C, \u03b4 Initial pull of arms from the reservoir: Choose T\u0304\u03b2 arms from the reservoir L\u0303 . Pull each of T\u0304\u03b2 arms once. t\u2190 T\u0304\u03b2 Choice between these arms: while t \u2264 n do\nFor any k \u2264 T\u0304\u03b2 :\nBk,t \u2190 \u00b5\u0302k,t + 2\n\u221a C\nTk,t log ( 22t\u0304\u03b2/b/(Tk,t\u03b4) ) + 2C\nTk,t log ( 22t\u0304\u03b2/b/(Tk,t\u03b4) )\n(4)\nPull Tk,t times the arm kt that maximizes Bk,t and receive Tk,t samples from it. t\u2190 t+ Tk,t\nend while Output: Return the most pulled arm k\u0302.\nLet us define t\u0304\u03b2 = blog2(T\u0304\u03b2)c.\nLet Tk,t be the number of pulls of arm k \u2264 Kt, and Xk,u for the u-th sample of \u03bdk. The empirical mean of the samples of arm k is defined as\n\u00b5\u0302k,t = 1\nTk,t Tk,t\u2211 u=1 Xk,u.\nWith this notation, we provide SiRI as Algorithm 1.\nDiscussion SiRI is a UCB-based algorithm, where the leading confidence term is of order\u221a\nlog (n/(\u03b4Tk,t))\nTk,t \u00b7\nSimilar to the MOSS algorithm (Audibert & Bubeck, 2009), we divide the log(\u00b7) term by Tk,t, in order to avoid additional logarithmic factors in the bound. But a simpler algorithm with a confidence term as in a classic UCB algorithm for cumulative regret,\u221a\nlog(n/\u03b4)\nTk,t ,\nwould provide almost optimal regret, up to a log n, i.e., with a slightly worse regret than what we get. It is quite interesting that with such a confidence term, SiRI is optimal for minimizing the simple regret for infinitely many\narmed bandits, since MOSS, as well as the classic UCB algorithm, targets the cumulative regret. The main difference between our strategy and the cumulative strategies (Berry et al., 1997; Wang et al., 2008; Bonald & Proutie\u0300re, 2013) is in the number of arms sampled from the arm reservoir: For the simple regret, we need to sample more arms. Although the algorithms are related, their analyses are quite different: Our proof is event-based whereas the proof for the cumulative regret targets directly the expectations.\nIt is also interesting to compare SiRI with existing algorithms targeting the simple regret for finitely many arms, as the ones by Audibert et al. (2010). SiRI can be related to their UCB-E with a specific confidence term and a specific choice of the number of arms selected. Consequently, the two algorithms are related but the regret bounds obtained for UCB-E are not informative when there are infinitely many arms. Indeed, the theoretical performance of UCBE is decreasing with the sum of the inverse of the gaps squared, which is infinite when there are infinitely many arms. In order to obtain a useful bound in this case, we need to consider a more refined analysis which is the one that leads to Theorem 2.\nRemark 2. Note that SiRI pulls series of samples from the same arm without updating the estimate which may seem wasteful. In fact, it is possible to update the estimates after each pull. On the other hand, SiRI is already minimax optimal, so one can only hope to get improvement in constants. Therefore, we present this version of SiRI, since its analysis is easier to follow.\nMain result We now state the main result which characterizes SiRI\u2019s simple regret according to \u03b2.\nTheorem 2 (Upper bounds). Let \u03b4 > 0. Assume all Assumptions 1 and 2 of the model and that n is larger than a large constant that depends on \u03b2, E\u0303, E\u0303\u2032, B\u0303, C. Depending on the value of \u03b2, we have the following results, where E is a large enough constant.\n\u2022 Case \u03b2 < 2: With probability larger than 1\u2212 \u03b4,\nrn \u2264 En\u22121/2 log(1/\u03b4)(log(log(1/\u03b4)))96 \u223c n\u22121/2.\n\u2022 Case \u03b2 > 2: With probability larger than 1\u2212 \u03b4,\nrn \u2264 E(n log(n))\u22121/\u03b2(log(log(log(n)/\u03b4)))96\u00d7 \u00d7 log(log(n)/\u03b4) \u223c (n log n)\u22121/\u03b2 polyloglog n.\n\u2022 Case \u03b2 = 2: With probability larger than 1\u2212 \u03b4,\nrn \u2264 E log(n)n\u22121/2(log(log(log(n)/\u03b4)))96\u00d7 \u00d7 log(log(n)/\u03b4) \u223c n\u22121/2 log npolyloglog n.\nShort proof sketch. In order to prove the results, the main tools are events \u03be1 and \u03be2 (Appendix B). One event controls the number of arms at a given distance from \u00b5\u0304\u2217 and the other one controls the distance between the empirical means and the true means of the arms.\nProvided that events \u03be1 and \u03be2 hold, which they do with high probability, we know that there are less than approximately Nu = T\u0304\u03b22\u2212u arms at a distance larger than 2\u2212u/\u03b2 from \u00b5\u0304\u2217, and that each arm that is at a distance larger than 2\u2212u/\u03b2 from \u00b5\u0304\u2217 will be pulled less than Pu = 22u/\u03b2 times. After these many pulls, the algorithm recognizes that it is suboptimal.\nSince a simple computation yields\u2211 0\u2264u\u2264log2(T\u0304\u03b2) NuPu \u2264 n C ,\nwe know that all the suboptimal arms at a distance further than 2\u2212 log2(T\u0304\u03b2)/\u03b2 from the optimal arm are discarded since they are all sampled enough to be proved suboptimal. We thus know that an arm at a distance less than 2\u2212 log2(T\u03b2\u0304)/\u03b2 from the optimal arm is selected in high probability, which concludes the proof.\nThe full proof(Appendix B) is quite technical, since it uses a peeling argument to correctly define the high probability event to avoid a suboptimal rate, in particular in terms of log n terms for \u03b2 < 2, and since we need to control accurately the number of arms at a given distance from \u00b5\u0304\u2217 at the same time as their empirical means.\nDiscussion The bound we obtain is minimax optimal for \u03b2 < 2 without additional log n factors. We emphasize it since the previous results on infinitely many armed bandits give results which are optimal up to a polylog n factor for the cumulative regret, except the one by Bonald & Proutie\u0300re (2013) which considers a very specific and fully parametric setting. For \u03b2 \u2265 2, our result is optimal up to a polylog n factor. We conjecture that the lower bound of Theorem 1 for \u03b2 \u2265 2 can be improved to (log(n)/n)1/\u03b2 and that SiRI is actually optimal up to a polyloglog(n) factor for \u03b2 > 2."}, {"heading": "4. Extensions of SiRI", "text": "We now discuss briefly three extensions of the SiRI algorithm that are very relevant either for practical or computational reasons, or for a comparison with the prior results. In particular, we consider the cases 1) when \u03b2 is unknown, 2) in a natural setting where the near-optimal arms have a small variance, and 3) in the case of unknown time horizon. These extensions are all in some sense following from our results and from the existing literature, and we will therefore state them as corollaries.\nAlgorithm 2 Bernstein-SiRI Parameters: C, \u03b2, \u03b4 Newly defined quantities: Set the number of arms as\nT\u0304\u03b2 = dmin(n/ log(n), A(n)n\u03b2/2)e,\nModify the SiRI algorithm\u2019s UCB (4) with\nBk,t \u2190 \u00b5\u0302k,t + 2\u03c3\u0302k,t\n\u221a C\nTk,t log ( 22t\u0304\u03b2/b/(Tk,t\u03b4) ) + 4C\nTk,t log ( 22t\u0304\u03b2/b/(Tk,t\u03b4) ) ,\nwhere \u03c3\u03022k,t is the empirical variance, defined as\n\u03c3\u03022k,t = 1\nTk,t Tk,t\u2211 l=1 (Xk,t \u2212 \u00b5\u0302k,t)2 .\nCall SiRI: Run SiRI on the samples using these new parameters\n4.1. Case of distributions on [0, 1] with \u00b5\u0304\u2217 = 1\nThe first extension concerns the specific setting, particularly highlighted by Bonald & Proutie\u0300re (2013) but also presented by Berry et al. (1997) and Wang et al. (2008), where the domain of the distributions of the arms are included in [0, 1] and where \u00b5\u0304\u2217 = 1. In this case, the information theoretic complexity of the problem is smaller than the one of the general problem stated in Theorem 1. Specifically, the variance of the near-optimal arms is very small, i.e., in the order of \u03b5 for an \u03b5-optimal arm. This implies a better bound, in particular, that the parametric limitation of 1/ \u221a n can be circumvented. In order to prove it, the simplest way is to modify SiRI into Bernstein-SiRI, displayed in Algorithm 2. It is an Empirical Bernstein-modified SiRI algorithm that accommodates the situation of distributions of support included in [0, 1] with \u00b5\u0304\u2217 = 1. Note that in the general case, it would provide similar results as what is provided in Theorem 2.\nA similar idea was already introduced by Wang et al. (2008) in the infinitely many armed setting for cumulative regret. The idea is that the confidence term is more refined using the empirical variance and hence it will be very large for a near-optimal arm, thereby enhancing exploration. Plugging this term in the proof, conditioning on the event of high probability, such that \u03c3\u03022k,t is close to the true variance, and using similar ideas as Wang et al. (2008), we can immediately deduce the following corollary.\nCorollary 1. Let \u03b4 > 0. Assume Assumptions 1 and 2 of the model and that n is larger than a large constant that\ndepends on \u03b2, E\u0303, E\u0303\u2032, B\u0303, C. Furthermore, assume that all the arms have distributions of support included in [0, 1] and that \u00b5\u0304\u2217 = 1. Depending on \u03b2, we have the following results for Bernstein-SiRI.\n\u2022 Case \u03b2 \u2264 1: The order of the simple regret is with high probability\nrn = O ( 1 n polylog n ) .\n\u2022 Case \u03b2 > 1: The order of the simple regret is with high probability\nrn = O (( 1 n )1/\u03b2 polylog n ) .\nMoreover, the rate\nmax\n( 1 n , ( logn n )1/\u03b2) ,\nis minimax-optimal for this problem, i.e., there exists no algorithm that achieves a better simple regret in a minimax sense.\nThe proof follows immediately from the proof of Theorem 2 using the empirical Bernstein bound as by Wang et al. (2008). Moreover, the lower bounds\u2019 rates follow directly from the two facts: 1) 1/n is clearly a lower bound, and therefore optimal for \u03b2 < 1, since it takes at least n samples of a Bernoulli arm that is constant times 1/n suboptimal, in order to discover that it is not optimal, and 2) n\u22121/\u03b2 can be trivially deduced from Theorem 11. Bernstein-SiRI is thus minimax optimal for \u03b2 \u2265 1 up to a polylog n factor.\nDiscussion Corollary 1 improves the results of Theorem 2 when \u03b2 \u2208 (0, 2). For these \u03b2, it is possible to beat the parametric rate of 1/ \u221a n, since in this case, the variance of the arms decays with the quality of the arms. In this situation, for \u03b2 < 2, it is possible to beat the parametric rate 1/ \u221a n and keep the rate of n\u22121/\u03b2 until \u03b2 \u2264 1, where the limiting rate of 1/n imposes its limitations: the regret cannot be smaller than the second order parametric rate of 1/n. Here, the change point of regime is \u03b2 = 1 which differs from the general simple regret case but is the same as the general case of cumulative regret as discussed in Remark 1. Notice that this comes from the fact that the limiting rate is now 1/n and not for same reasons as for the cumulative regret.\n1Indeed, its proof shows that a lower bound of the order of n\u22121/\u03b2 is valid for any distribution and in particular for Bernoulli with mean \u00b5 and \u00b5\u0304\u2217 = 1, which is a special case of distributions of support included in [0, 1] and that \u00b5\u0304\u2217 = 1."}, {"heading": "4.2. Dealing with unknown \u03b2", "text": "In practice, the parameter \u03b2 is almost never available. Yet its knowledge is crucial for the implementation of SiRI, as well as for all the cumulative regret strategies described in (Berry et al., 1997; Wang et al., 2008; Bonald & Proutie\u0300re, 2013). Consequently, a very important question is whether it is possible to estimate it well enough to obtain good results, which we answer in the affirmative.\nAn interesting remark is that Assumption 2 is actually related to assuming that the distribution function L is \u03b2 regularly varying in \u00b5\u0304\u2217. Therefore, \u03b2 is the tail index of the distribution function ofL and can be estimated with tools from extreme value theory (de Haan & Ferreira, 2006). Many estimators exist for estimating this tail index \u03b2, for instance, the popular Hill\u2019s estimate (Hill, 1975), but also Pickand\u2019s\u2019 estimate (Pickands, 1975) and others.\nHowever, our situation is slightly different from the one where the convergence of these estimators is proved, as the means of the arms are not directly observed. As a result, we propose another estimate, related to the estimate of Carpentier & Kim (2014), which accommodates our setting. Assume that we have observed N arms, and that all of these arms have been sampled N times. Let us write m\u0302k for the empirical mean estimates of the mean mk of these N arms and define\nm\u0302\u2217 = max k m\u0302k.\nWe further define\np\u0302 = 1\nN N\u2211 k=1 1{m\u0302\u2217 \u2212 m\u0302k \u2264 N\u2212\u03b5}\nand set\n\u03b2\u0302 = \u2212 log p\u0302 \u03b5 logN \u00b7 (5)\nThis estimate satisfies the following weak concentration inequality and its proof is in Appendix D. Lemma 1. Let \u03b2 be a lower bound on \u03b2. If Assumptions 1 and 2 are satisfied and \u03b5 < min(\u03b2, 1/2, 1/(\u03b2)), then with probability larger than 1\u2212 \u03b4, for N larger than a constant that depends only on B\u0303 of Assumption 2,\n|\u03b2\u0302 \u2212 \u03b2| \u2264 \u03b4\u22121/\u03b2\n\u03b2 + \u221a log( 1\u03b4 ) + max(1, log(E\u0303 \u2032), | log(E\u0303)|)\n\u03b5 logN\n\u2264 c\u2032max(\n\u221a log(1/\u03b4), \u03b4\u22121/\u03b2)\n\u03b5 logN ,\nwhere c\u2032 > 0 is a constant that depends only on \u03b5 and the parameter C of Assumption 1.\nLet us now modify SiRI in the way as in Algorithm 3. The knowledge of \u03b2 is not anymore required, and one just needs a lower bound \u03b2 on \u03b2. We get \u03b2\u0304-SiRI which satisfies the following corollary.\nAlgorithm 3 \u03b2\u0304-SiRI: \u03b2\u0304-modified SiRI for unknown \u03b2 Parameters: C, \u03b4, \u03b2 Initial phase for estimating \u03b2: Let N \u2190 n1/4 and \u03b5\u2190 1/ log log log(n). Sample N arms from the arm reservoir N times Compute \u03b2\u0302 following (5) Set\n\u03b2\u0304 \u2190 \u03b2\u0302+ c\u2032max\n(\u221a log(1/\u03b4), \u03b4\u22121/\u03b2 ) logloglog n\nlog n (6)\nCall SiRI: Run SiRI using \u03b2\u0304 instead of \u03b2 with n \u2212N2 = n \u2212 \u221a n remaining samples.\nCorollary 2. Let the Assumptions 1 and 2 be satisfied. If n is large enough with respect to a constant that depends on \u03b2, E\u0303, E\u0303\u2032, B\u0303, C, then \u03b2\u0304\u2212SiRI satisfies the following:\n\u2022 Case \u03b2 < 2: The order of the simple regret is with high probability\nrn = O (\n1\u221a n\npolyloglog n ) .\n\u2022 Case \u03b2 > 2: The order of the simple regret is with high probability\nrn = O (( logn n )1/\u03b2 polyloglog n ) .\n\u2022 Case \u03b2 = 2: The order of the simple regret is with high probability\nrn = O ( logn\u221a n polyloglog n ) .\nThe proof can be deduced easily from Theorem 2 using the result from Lemma 1, noting that a 1/ log n rate in learning \u03b2 is fast enough to guarantee that all bounds will only be modified by a constant factor when we use \u03b2\u0302 instead of \u03b2 in the exponent.\nDiscussion Corollary 2 implies that even in situations with unknown \u03b2, it is possible to estimate it accurately enough so that the modified \u03b2\u0304-SiRI remains minimaxoptimal up to a polylog n, by only using a lower bound \u03b2 on \u03b2. This is the same that holds for SiRI with known \u03b2. We would like to emphasize that \u03b2\u0304 estimate (6) of \u03b2 can be used to improve cumulative regret algorithms that need \u03b2, such as the ones by Berry et al. (1997) and Wang et al. (2008). Similarly for these algorithms, one should spend a preliminary phase of N2 = \u221a n rounds to estimate \u03b2 and then run the algorithm of choice. This will\nmodify the cumulative regret rates in the general setting by only a polyloglog n factor, which suggests that our \u03b2 estimation can be useful beyond the scope of this paper. For instance, consider the cumulative regret rate of UCB-F by Wang et al. (2008). If UCB-F uses our estimate of \u03b2 instead of the true \u03b2, it would still satisfy\nE [Rn] = O ( max ( n \u03b2 \u03b2+1 polylog n, \u221a n polylog n )) .\nFinally, this modification can be used to prove that this problem is learnable over all mean reservoir distributions with \u03b2 > 0: This can be seen by setting the lower bound on \u03b2 as \u03b2 = 1/ log log logN , which goes to 0 but very slowly with n. In this case, we only loose a log log(n) factor."}, {"heading": "4.3. Anytime algorithm", "text": "Another interesting question is whether it is possible to make SiRI anytime. This question can be quickly answered in the affirmative. First, we can easily just use a doubling trick to double the size of the sample in each period and throw away the preliminary samples that were used in the previous period. Second, Wang et al. (2008) propose a more refined way to deal with an unknown time horizon (UCB-AIR), that also directly applies to SiRI. Using these modifications it is straightforward to transform SiRI into an anytime algorithm. The simple regret in this anytime setting will only be worsened by a polylog n, where n is the unknown horizon. Specifically, in the anytime setting, the regret of SiRI modified either using the doubling trick or by the construction of UCB-AIR has a simple regret that satisfies with high probability\nrn = O ( polylog(n) max(n\u22121/2, n\u22121/\u03b2 polylog n) ) ."}, {"heading": "5. Numerical simulations", "text": "To simulate different regimes of the performance according to \u03b2-regularity, we consider different reservoir distributions of the arms. In particular, we consider beta distributions B(x, y) with as x = 1 and y = \u03b2. For B(1, \u03b2), the Assumption 2 is satisfied precisely with regularity \u03b2. Since to our best knowledge, SiRI is the first algorithm optimizing simple regret in the infinitely many arms setting, there is no natural competitor for it. Nonetheless, in our experiments we compare to the algorithms designed for linked settings.\nFirst such comparator is UCB-F (Wang et al., 2008), an algorithm that optimizes cumulative regret for this setting. UCB-F is designed for fixed horizon of n evaluations and it is an extension of a version of UCB-V by Audibert et al. (2007). Second, we compare SiRI to lil\u2019UCB (Jamieson et al., 2014) designed for the best-arm identification in the fixed confidence setting. The purpose of comparison with lil\u2019UCB is to show that SiRI performs at par with lil\u2019UCB\nequipped with the optimal number of T\u0304\u03b2 arms. In all our experiments, we set constant A of SiRI to 0.3, constant C to 1, and confidence \u03b4 to 0.01.\nAll the experiments have some specific beta distribution as a reservoir and the arm pulls are noised with N (0, 1) truncated to [0, 1]. We perform 3 experiments based on different regimes of \u03b2 coming from our analysis: \u03b2 < 2, \u03b2 = 2, and \u03b2 > 2. In the first experiment (Figure 1, left) we take \u03b2 = 1, i.e., B(1, 1) which is just a uniform distribution. In the second experiment (Figure 1, right) we consider B(1, 2) as the reservoir. Finally, Figure 2 features the experiments for B(1, 3). The first obvious observation confirming the analysis is that higher \u03b2 leads to a more difficult problem. Second, UCB-F performs well for \u03b2 = 1, slightly worse for \u03b2 = 2, and much worse for \u03b2 = 3. This empirically confirms our discussion in Remark 1. Finally, SiRI performs empirically as well as lil\u2019UCB equipped with the optimal number of arms and the same confidence \u03b4. Figure 2 also compares SiRI with \u03b2\u0304-SiRI for the uniform distribution. For this experiment, using \u221a n samples just for the \u03b2 estimation did not decrease the budget too much and at the same time, the estimated \u03b2\u0304 was precise enough not to hurt the final simple regret.\nConclusion We presented SiRI, a minimax optimal algorithm for simple regret in infinitely many arms bandit setting, which is interesting when we face enormous number of potential actions. Both the lower and upper bounds give different regimes depending on a complexity \u03b2, a parameter for which we also give an efficient estimation procedure.\nAcknowledgments This work was supported by the French Ministry of Higher Education and Research and the French National Research Agency (ANR) under project ExTra-Learn n.ANR-14-CE24-0010-01."}, {"heading": "A. Additional notation", "text": "We write P1 for the probability with respect to the arm reservoir distribution, P2 for the probability with respect to the distribution of the samples from the arms, and P1,2 for the probability both with respect to the arm reservoir distribution and the distribution of the samples from the arms.\nLet F be the distribution function of the mean reservoir distribution L. Let F\u22121 be the pseudo-inverse of the mean reservoir distribution. In order to express the regularity assumption, we define\nG (\u00b7) = \u00b5\u0304\u2217 \u2212 F\u22121 (1\u2212 \u00b7) .\nWe assume that G has a certain regularity in its right end point, which is a standard assumption for infinitely many armed bandits. In particular, we rewrite Assumption 2 by only modifying the constants E\u0303, E\u0303\u2032, and B\u0303. Assumption 3 (\u03b2 regularity in \u00b5\u0304\u2217, version 2). Let \u03b2 > 0. There exist E,E\u2032, B \u2208 (0, 1) such that \u2200u \u2208 [0, B],\nE\u2032u1/\u03b2 \u2265 G (u) \u2265 Eu1/\u03b2 .\nThis assumption is equivalent to Assumption 2 which is the same as the classic one (1) by definition of G and F and we reformulate it for the convenience of analysis. Without loss of generality, we assume that \u00b5\u0304\u2217 > 0."}, {"heading": "B. Full proof of Theorem 2", "text": "B.1. Roadmap\nThe proof of Theorem 2 (upper bounds) is composed of two layers. The first layer consists of proving results on the empirical distributions of the arms emitted by the arms reservoir, the crucial object is event \u03be1. The second layer consists of proving results on the random samples of the arms, and in particular that the empirical means of the arms are not too different from the true means of the arms. For this part, the crucial object is event \u03be2. More precisely, these two layers can be decomposed as follows.\n\u2022 We prove of suitable high probability upper bounds and lower bounds on the number of arms among the T\u0304\u03b2 arms pulled by the algorithm that have a given gap (with respect to \u00b5\u0304\u2217), depending on the considered gap. This is done in Lemma 4. Two important results can be consequently deduced: (i) An upper bound on the number of suboptimal arms depending on how suboptimal they are. The more suboptimal they are, the more arms they are, which depends on \u03b2. (ii) A proof that among the T\u0304\u03b2 arms pulled by the algorithm, there is with high probability at least one arm, and not significantly more than one arm, that has a gap smaller than the simple regret from Theorem 2. This is done in Corollary 3.\n\u2022 In Lemma 5, we prove that with high probability, the empirical means of the arms are not too different from their true means. The main difficulty is that the means of the arms are random. In order to avoid suboptimal log(n) dependency in the case \u03b2 < 2, we use a peeling argument where the peeling is done over these random gaps, using the result from the previous layer, i.e., the bound on the number of arms with a given gap.\nAfterwards, we combine the two results to bound the number of suboptimal pulls (section B.5). Since the algorithm pulls the arms depending on the empirical gaps, then (i) the bounds on the number of suboptimal and near-optimal arms, and (ii) the bounds on the deviations of the empirical means with respect to the true means, will allow to obtain the desired bound on the number of suboptimal arms. By construction of the strategy and in particular, by the choice of T\u0304\u03b2 , we prove that with high probability, the number of pulls of the optimal arms is smaller than a fraction of n. This means that there is a near-optimal arm that is pulled more than n/2 times. This is the one selected by the algorithm which concludes the proof.\nB.2. Concentration inequalities\nWe make several uses of Bernstein\u2019s inequality: Lemma 2 (Bernstein\u2019s inequality). Let E(Xt) = 0, |Xt| \u2264 b > 0, and E(X2t ) \u2264 v > 0. Then for any \u03b4 > 0, with probability at least 1\u2212 \u03b4\nn\u2211 t=1 Xt \u2264 \u221a 2nv log \u03b4\u22121 + b3 log \u03b4 \u22121\nFurthermore, Algorithm 2 along with Corollary 1 are based on the empirical Bernstein concentration inequality.\nLemma 3 (Empirical Bernstein\u2019s inequality). Let E(Xt) = 0, |Xt| \u2264 b > 0. Let for any j = 1, . . . , n\nVj = 1\nj j\u2211 t=1\n( Xt \u2212 1\nj j\u2211 i=1 Xi\n)2\nThen for any \u03b4 > 0, with probability at least 1\u2212 \u03b4\nj\u2211 t=1 Xt \u2264 \u221a 2nVj log (3\u03b4\u22121) + 3b log ( 3\u03b4\u22121 ) B.3. Notation\nFor any i \u2264 Kn, set \u2206i = \u00b5\u0304 \u2217 \u2212 \u00b5i,\nwhere we remind the reader that \u00b5i is the mean of distribution of arm i.\nWithout loss of generality, we assume that \u00b5\u0304\u2217 > 0. For any u \u2208 N, we define\nIu = [ \u00b5\u0304\u2217 \u2212G ( 2\u2212u ) , \u00b5\u0304\u2217 \u2212G ( 2\u2212u\u22121 )] .\nWe also define I\u22121 = [0, \u00b5\u0304 \u2217 \u2212G (B)] .\nWe further define I\u2217 = [ \u00b5\u0304\u2217 \u2212G ( 2\u2212t\u0304\u03b2 ) , \u00b5\u0304\u2217 \u2212G (0) ] = [ \u00b5\u0304\u2217 \u2212G ( 2\u2212t\u0304\u03b2 ) , \u00b5\u0304\u2217 ] .\nLet Nu be the number of arms in segment Iu,\nNu = T\u0304\u03b2\u2211 k=1 1{\u00b5k \u2208 Iu},\nand let N\u0304\u2217 be the number of arms in the segment I\u2217,\nN\u0304\u2217 = T\u0304\u03b2\u2211 k=1 1{\u00b5k \u2208 I\u2217}.\nB.4. Favorable high-probability event\nLet \u03be1 be the event defined as\n\u03be1 = { \u03c9 : \u2200u \u2208 N, u \u2264 t\u0304\u03b2 , \u2223\u2223\u2223Nu \u2212 2t\u0304\u03b2\u2212u\u22121\u2223\u2223\u2223 \u2264\u221a(t\u0304\u03b2 \u2212 u+ 1)2t\u0304\u03b2\u2212u log(1/\u03b4) + (t\u0304\u03b2 \u2212 u+ 1) log(1/\u03b4), and N\u0304\u2217 \u2264 1 + 2 \u221a log(1/\u03b4) + 2 log(1/\u03b4) }\n= { \u03c9 : \u2200u \u2208 N, u \u2264 t\u0304\u03b2 , \u2223\u2223\u2223Nu \u2212 2t\u0304\u03b2\u2212u\u22121\u2223\u2223\u2223 \u2264 2t\u0304\u03b2\u2212u\u22121\u03b5u and N\u0304\u2217 \u2264 1 + \u03b5t\u0304\u03b2 } .\nwhere \u03b5u = 2 \u221a (t\u0304\u03b2 \u2212 u+ 1)2\u2212(t\u0304\u03b2\u2212u) log(1/\u03b4) + 2(t\u0304\u03b2 \u2212 u+ 1)2\u2212(t\u0304\u03b2\u2212u) log(1/\u03b4).\nLemma 4. The probability of \u03be1 under both the distribution of the arm reservoir and the samples of the arms is larger than 1\u2212 ( 1 + ee\u22121 ) \u03b4 for \u03b4 small enough,\nP1(\u03be1) = P1,2 (\u03be1) \u2265 1\u2212 ( 1 + e\ne\u2212 1\n) \u03b4.\nProof of Lemma 4. Let u \u2208 N. We have by definition that\nNu = T\u0304\u03b2\u2211 k=1 1{\u00b5k \u2208 Iu},\nis a sum of independent Bernoulli random variables of parameter 2\u2212u \u2212 2\u2212u\u22121 = 2\u2212u\u22121. By a Bernstein concentration inequality (Lemma 2) for sums of Bernoulli random variables, this implies that with probability 1\u2212 \u03b4u > 0,\u2223\u2223\u2223Nu \u2212 2t\u0304\u03b2\u2212u\u22121\u2223\u2223\u2223 \u2264\u221a2t\u0304\u03b2\u2212u log \u03b4\u22121u + log \u03b4\u22121u . Set \u03b4u = exp (\u2212 (t\u0304\u03b2 \u2212 u+ 1)) \u03b4. Notice that for u \u2264 t\u0304\u03b2 , log \u03b4\u22121u \u2264 (t\u0304\u03b2 \u2212u+ 1) log \u03b4\u22121. Then the result holds by a union bound since for \u03b4 small enough\nt\u0304\u03b2\u2211 u=0 \u03b4u = \u03b4 t\u0304\u03b2\u2211 u=0 exp (\u2212 (t\u0304\u03b2 \u2212 u+ 1)) \u2264 e\u03b4 e\u2212 1 ,\nand by similar argument for N\u0304\u2217 which together with another union bound give the claim.\nThe following corollary follows from Lemma 4.\nCorollary 3. Set t\u0304\u2217 = bt\u0304\u03b2 \u2212 96 log2(log2(log(1/\u03b4)))\u2212 log2(log(1/\u03b4))c\u2212 2. Let \u03b4 be smaller than an universal constant. If n is large enough so that t\u0304\u2217 \u2265 log2(1/B), then on \u03be1, there is at least an arm of index in {1, . . . , T\u0304\u03b2} such that it belongs to It\u0304\u2217 . If k\u2217 is its index, then\n\u2206k\u2217 \u2264 14E \u2032(log2(log(1/\u03b4))) 962\u2212t\u0304\u03b2/\u03b2 log(1/\u03b4).\nProof of Corollary 3. First we have for u \u2264 t\u0304\u2217\n\u03b5u = 2 \u221a (t\u0304\u03b2 \u2212 u+ 1)2\u2212(t\u0304\u03b2\u2212u) log(1/\u03b4) + 2(t\u0304\u03b2 \u2212 u+ 1)2\u2212(t\u0304\u03b2\u2212u) log(1/\u03b4)\n\u2264 2\n\u221a (1 + log2(log(1/\u03b4)) + 96 log2(log2(log(1/\u03b4))))\n96 log2(log(1/\u03b4)) +\n2(1 + log2(log(1/\u03b4)) + 96 log2(log2(log(1/\u03b4))))\n96 log2(log(1/\u03b4)) \u2264 4 \u221a\n1/(96 log2(log(1/\u03b4))) + 1/96 + log2(log2(log(1/\u03b4)))/ log2(log(1/\u03b4))\n\u2264 1/2,\nfor \u03b4 being a small enough constant.\nThis implies that for u \u2264 t\u0304\u2217\n2t\u0304\u03b2\u2212u\u22121(1\u2212 \u03b5u) \u2265 22\u22121 \u00d7 1/2 \u2265 1.\nThis implies that on \u03be1,Nt\u0304\u2217 \u2265 1, which means there is at least one arm in It\u0304\u2217 . Let us call k one of these arms. By definition of It\u0304\u2217 , it satisfies\n\u2206k\u2217 \u2264 G(2\u2212t\u0304 \u2217 ) \u2264 E\u20322\u2212t\u0304 \u2217/\u03b2 \u2264 14E \u2032(log2(log(1/\u03b4))) 962\u2212t\u0304\u03b2/\u03b2 log(1/\u03b4).\nbecause of Assumption 3, since t\u2217 \u2265 log2(1/B).\nLet for any k \u2208 N, 1 \u2264 k \u2264 T\u0304\u03b2\nnk = \u230a log2 ( D log ( max(1, 22t\u0304\u03b2/b\u22062k)/b ) max ( 2\u22122t\u0304\u03b2/b,\u22062k ) )\u230b ,\nwhere D is a large constant, and\nn\u0303u = log2\nD log ( max ( 1, 22t\u0304\u03b2/bG ( 2\u2212(u+1) )2) /\u03b4 )\nmax ( 2\u22122t\u0304\u03b2/b, G ( 2\u2212(u+1)\n)2)  .\nLet also\nn\u0303\u22121 = log2\nD log ( max(1, 22t\u0304\u03b2/bG (B) 2 )/\u03b4 )\nmax ( 2\u22122t\u0304\u03b2/b, G (B) 2 )\n .\nLet \u03be2 = { \u03c9 : \u2200k \u2208 N\u2217, k \u2264 T\u0304\u03b2 ,\u2200v \u2264 nk \u2223\u2223\u2223\u2223\u2223 12v 2v\u2211 i=1 Xk,i \u2212 \u00b5k \u2223\u2223\u2223\u2223\u2223 \u2264 2 \u221a C2\u2212v log(22t\u0304\u03b2/b\u2212v/\u03b4) + 2C2\u2212v log(22t\u0304\u03b2/b\u2212v/\u03b4) } .\nLemma 5. Case \u03b2 < 2: Knowing \u03be1, the probability of \u03be2 is larger than 1\u2212H log(1/\u03b4)2\u03b4,\nP2 (\u03be2|\u03be1) \u2265 1\u2212H log (1/\u03b4)2 \u03b4,\nwhere H is a constant that depends only on D,E,E\u2032, \u03b2.\nCase \u03b2 \u2265 2: Knowing \u03be1, the probability of \u03be2 is larger than 1\u2212H log(1/\u03b4)2 log(n)\u03b4,\nP2 (\u03be2|\u03be1) \u2265 1\u2212H log(1/\u03b4)2 log(n)2\u03b4,\nwhere H is a constant that depends only on D,E,E\u2032, \u03b2.\nProof of Lemma 5. Let (k, v) \u2208 N\u2217 \u00d7 N. Since (Xk,i)i are i.i.d. from distribution bounded by C, we have that with probability (according to the samples) larger than 1\u2212 \u03b4k,v ,\u2223\u2223\u2223\u2223\u2223 12v 2v\u2211 i=1 Xk,i \u2212 \u00b5k\n\u2223\u2223\u2223\u2223\u2223 \u2264\u221a2C2\u2212v log (1/\u03b4k,v) + 2C \u00d7 2\u2212v log (1/\u03b4k,v) . Set \u03b4k,v = 2v2\u22122t\u0304\u03b2/b\u03b4. We have\u2211\nk\u2264T\u0304\u03b2 \u2211 v\u2264nk \u03b4k,v = 2 \u22122t\u0304\u03b2/b\u03b4 \u2211 k\u2264T\u0304\u03b2 \u2211 v\u2264nk 2v \u2264 2\u00d7 2\u22122t\u0304\u03b2/b\u03b4 \u2211 k\u2264T\u03b2 2nk \u2264 2\u00d7 2\u22122t\u0304\u03b2/b\u03b4 \u221e\u2211 u=0 Nu2 n\u0303u ,\nsince 2n\u0303u is increasing in u.\nAgain, since 2n\u0303u is increasing in u, is implies that on \u03be1,\u2211 k\u2264T\u0304\u03b2 \u2211 v\u2264nk \u03b4k,v \u2264 2\u00d7 2\u22122t\u0304\u03b2/b\u03b4 \u221e\u2211 u=0 Nu2 n\u0303u\n\u2264 2\u00d7 2\u22122t\u0304\u03b2/b\u03b4 T\u0304\u03b22n\u0303\u22121 + t\u0304\u03b2\u2211 u=blog2(1/B)c+1 Nu2 n\u0303u + N\u0304\u22172 n\u0303t\u0304\u03b2  (7) \u2264 2\u00d7 2\u22122t\u0304\u03b2/b\u03b4  T\u0304\u03b2D log(22t\u0304\u03b2/bE\u2032B\u22122/\u03b2/\u03b4) EB\u22122/\u03b2 + t\u0304\u03b2\u2211 u=blog2(1/B)c+1 NuD log(2 2t\u0304\u03b2/bG(2\u2212(u+1))2/\u03b4) G(2\u2212(u+1))2 + N\u0304\u2217D log ( 1 \u03b4 ) 22t\u0304\u03b2/b\n \u2264 2\u00d7 2\u22122t\u0304\u03b2/b\u03b4 2t\u0304\u03b2DE\u2032 log (n\u03b4 ) E + t\u0304\u03b2\u2211 u=blog2(1/B)c+1 2t\u0304\u03b2\u2212u\u22121(1 + \u03b5u) D log ( E \u03b4 ) ( 2t\u0304\u03b2 b \u2212 2(u\u22121) \u03b2 ) E2\u22122(u\u22121)/\u03b2 + (1 + \u03b5t\u0304\u03b2 )D log ( 1 \u03b4 ) 22t\u0304\u03b2/b\n \u2264 2\u00d7 2\u22122t\u0304\u03b2/b\u03b4 2t\u0304\u03b2DE\u2032 log (n\u03b4 ) E + t\u0304\u03b2\u2211 u=0 6D/(Eb) log ( E \u03b4 )2 2t\u0304\u03b2\u2212u+2u/\u03b2(2t\u0304\u03b2 \u2212 2(u\u2212 1)) + 5D log ( 1 \u03b4 )2 22t\u0304\u03b2/b\n , since \u03b5u \u2264 4(t\u0304\u03b2 \u2212 u+ 1) log ( 1 \u03b4 ) and since b \u2264 \u03b2, which implies that 2t\u0304\u03b2 \u2212 2(u\u2212 1) \u2265 1.\nCase 1: \u03b2 < 2: In this case, b = \u03b2. Since \u2211\u221e u=0 2 \u2212u/vuv \u2032 <\u221e for any v, v\u2032 > 0 that on \u03be1, the last equation implies\n\u2211 k\u2264T\u0304\u03b2 \u2211 v\u2264nk \u03b4k,v \u2264 2\u00d7 2\u22122t\u0304\u03b2/\u03b2\u03b4\n( 2t\u0304\u03b2DE\u2032 log ( n \u03b4 ) E + 3DF \u20321 E\u03b2 log ( E \u03b4 ) 22t\u0304\u03b2/\u03b2 + 5D log ( 1 \u03b4 )2 22t\u0304\u03b2/\u03b2 ) \u2264 F1 log ( 1 \u03b4 )2 \u03b4,\nwhere F \u20321, F1 > 0 are constants. Case 2: \u03b2 > 2: In this case, b = 2. Since \u2211\u221e u=0 2 \u2212u/v <\u221e for any v > 0 that on \u03be1, the last equation implies\n\u2211 k\u2264T\u0304\u03b2 \u2211 v\u2264nk \u03b4k,v \u2264 2\u00d7 2\u2212t\u0304\u03b2\u03b4\n( 2t\u0304\u03b2DE\u2032 log ( n \u03b4 ) E + 3DF \u20322 E\u03b2 log ( E \u03b4 )2 2t\u0304\u03b2 t\u03b2 + 5D log ( 1 \u03b4 )2 2t\u0304\u03b2 )\n\u2264 F2 log ( 1 \u03b4 )2 t\u0304\u03b2\u03b4 \u2264 F2 log ( 1 \u03b4 )2 log(n)\u03b4,\nwhere F \u20322, F2 > 0 are constants.\nCase 3: \u03b2 = 2: In this case, we have on \u03be1\n\u2211 k\u2264T\u0304\u03b2 \u2211 v\u2264nk \u03b4k,v \u2264 2\u00d7 2\u2212t\u0304\u03b2\u03b4 2t\u0304\u03b2DE\u2032 log (n\u03b4 ) E + t\u0304\u03b2\u2211 u=0 3D E\u03b2 log ( E \u03b4 )2 2t\u0304\u03b2 (2t\u0304\u03b2 \u2212 2(u\u2212 1)) + 5D log ( 1 \u03b4 )2 2t\u0304\u03b2  \u2264 F3 log ( 1 \u03b4 )2 t\u03042\u03b2\u03b4 \u2264 F3 log ( 1 \u03b4 )2 log(n)2\u03b4.\nwhere F3 > 0 is a constant.\nLet \u03be = \u03be1 \u2229 \u03be2. By Lemmas 4 and 5, we know that for a given constant F4 that depends only on \u03b2,D,E,E\u2032,\n\u2022 Case \u03b2 < 2: P(\u03be) \u2265 1\u2212 F4 log ( 1 \u03b4 )2 \u03b4.\n\u2022 Case \u03b2 \u2265 2: P(\u03be) \u2265 1\u2212 F4 log ( 1 \u03b4 )2 log(n)3\u03b4.\nB.5. Upper bound on the number of pulls of the non-near-optimal arms\nLet k be an arm such that k \u2264 T\u0304\u03b2 , and t \u2264 n be a time. On the event \u03be, by definition, we have\n|\u00b5\u0302k,t \u2212 \u00b5k| \u2264 2\n\u221a C log ( 22t\u0304\u03b2/b/(Tk,t\u03b4) ) Tk,t + 2C log ( 22t\u0304\u03b2/b/(Tk,t\u03b4) ) Tk,t ,\nwhich implies by definition of the upper confidence bound that on \u03be\n\u00b5k \u2264 Bk,t \u2264 \u00b5k + \u03b5k,t, where \u03b5k,t = 2\n\u221a C log ( 22t\u0304\u03b2/b/(Tk,t\u03b4) ) Tk,t + 2C log ( 22t\u0304\u03b2/b/(Tk,t\u03b4) ) Tk,t . (8)\nLet us now write k\u2217 for the best arm among the ones in {1, . . . , T\u03b2}. Note that k\u2217 may be different from the best possible arm. By Corollary 3, we know that on \u03be, k\u2217 satisfies\n\u2206k\u2217 \u2264 14E \u2032 (log2 (log (1/\u03b4))) 96 2\u2212t\u0304\u03b2/\u03b2 log ( 1 \u03b4 ) = \u03b5\u2217.\nArm k is pulled at time t instead of k\u2217 only if Bk,t \u2265 Bk\u2217,t.\nOn \u03be, this happens if\n\u00b5\u0304\u2217 \u2212 \u03b5\u2217 \u2264 \u00b5k + \u03b5k,t,\nwhich happens if (on \u03be)\n\u2206k \u2212 \u03b5\u2217 \u2264 \u03b5k,t,\nand if we assume that \u2206k \u2265 2\u03b5\u2217, it implies that on \u03be arm k is pulled at time t only if\n\u2206k \u2264 2\u03b5k,t. (9)\nWe define u such that (i) that \u00b5k \u2208 Iu if u \u2265 blog2(B)c+ 1 or (ii) u = \u22121 otherwise. Assume that\nTk,t \u2265 2n\u0303u \u2265 D log\n( max ( 1, 22t\u0304\u03b2/bG ( 2\u2212(u+1) )2) /\u03b4 )\nmax ( 2\u22122t\u0304\u03b2/b, G ( 2\u2212(u+1) )2) \u2265 D log(22t\u0304\u03b2/bG(2\u2212(u+1))2/\u03b4)G(2\u2212(u+1))2 , since we assumed that \u2206k \u2265 2\u03b5\u2217, which implies that tk,t \u2264 t\u2217 \u2264 t\u0304\u03b2 .\nBy Assumption 3, and since \u00b5k \u2208 Iu, we know that G(2\u2212(u+1)) \u2264 \u2206k. Therefore, the last equation implies\nTk,t \u2265 2n\u0303u \u2265 D log\n( max ( 1, 22t\u0304\u03b2/bG ( 2\u2212(u+1) )2) /\u03b4 )\nmax ( 2\u22122t\u0304\u03b2/b, G ( 2\u2212(u+1)\n)2) \u2265 D log ( 22t\u0304\u03b2/b\u22062k/\u03b4 ) \u22062k .\nFor such a Tk,t, we have\nlog ( 22t\u0304\u03b2/b/ (Tk,t\u03b4) ) Tk,t \u2264 \u22062k log ( D22t\u0304\u03b2/b\u22062k/\u03b4 ) D log ( 22t\u0304\u03b2/b\u22062k/\u03b4 ) \u2264 \u22062k logD D \u2264 \u22062k/(16C),\nfor D large enough so that D \u2265 32(C + 1) log(32(C + 1)). Therefore, by definition of \u03b5k, t, the last equation implies that for Tk,t \u2265 2n\u0303u , we have\n\u03b5k,t \u2264 \u2206k/4.\nThe last equation implies together with (9) that if Tk,t \u2265 2n\u0303u , then on \u03be, arm k is not pulled from time t onwards. In particular, this implies that on \u03be\nTk,n \u2264 2n\u0303u ,\nfor any k \u2264 T\u0304\u03b2 such that \u2206k \u2265 2\u03b5\u2217, and such that (i) \u00b5k \u2208 Iu if u \u2265 blog2(B)c+ 1 or (ii) or u = \u22121 otherwise.\nLet A be the set of arms such that \u2206k \u2265 2\u03b5\u2217. From the previous equation, the number of times that they are pulled is bounded on \u03be as \u2211\nk\u2208A Tk,n \u2264 \u2211 u\u2264t\u0304\u03b2 Nu2 n\u0303u \u2264 T\u0304\u03b22n\u0303\u22121 + \u2211 blog2(B)c\u2264u\u2264t\u0304\u03b2 Nu2 n\u0303u .\nBounding this quantity can be done in essentially the same way as in (7). We again obtain three cases,\n\u2022 Case 1: \u03b2 < 2: In this case on \u03be\n\u2211 k\u2208A Tk,n \u2264 2t\u0304\u03b2DE\u2032 log(n/\u03b4) E + 3DF \u20321 E\u03b2 log(E/\u03b4)222t\u0304\u03b2/\u03b2 \u2264 n/H,\nwhere H is arbitrarily large for A small enough in the definition of T\u0304\u03b2 .\n\u2022 Case 2: \u03b2 > 2: In this case on \u03be\n\u2211 k\u2208A Tk,n \u2264 2t\u0304\u03b2DE\u2032 log(n/\u03b4) E + 3DF \u20322 E\u03b2 log(E/\u03b4)22t\u0304\u03b2 t\u03b2 \u2264 n/H,\nwhere H is arbitrarily large for A small enough in the definition of T\u0304\u03b2 .\n\u2022 Case 3: \u03b2 = 2: In this case on \u03be\n\u2211 k\u2208A Tk,n \u2264 2t\u0304\u03b2DE\u2032 log(n/\u03b4) E + t\u0304\u03b2\u2211 u=0 3D E\u03b2 log(E/\u03b4)22t\u0304\u03b2 (2t\u0304\u03b2 \u2212 2(u\u2212 1)) \u2264 n/H,\nwhere H is arbitrarily large for A small enough in the definition of T\u0304\u03b2 .\nConsider now u\u2217 such that u\u2217 = blog2 ( 1/F\u0304 (\u03b5\u2217) ) c. By definition of \u03b5\u2217, we know that on \u03be, we have\nu\u2217 \u2265 t\u2217 \u2212 \u03c5(\u03b4)\nTherefore with high probability, by Lemma 4 and as in Corollary 3, on \u03be, there are less than N(\u03b4) arms of index smaller than T\u0304\u03b2 such that \u2206k \u2264 2\u03b5\u2217, where N(\u03b4) is a constant that depends only on \u03b4. For H large enough, on \u03be, N(\u03b4) \u2264 H . This implies, together with the three cases, that there is at least an arm of index smaller than T\u0304\u03b2 and such that \u2206k \u2264 2\u03b5\u2217 that is pulled more than n/H times. This implies that the most pulled arm is such that, on \u03be, \u2206k \u2264 2\u03b5\u2217. This implies that the regret is on \u03be bounded as\nrn \u2264 E\u2032(log2(log(1/\u03b4))) 96\n4 2\u2212t\u0304\u03b2/\u03b2 log(1/\u03b4) \u2264 E\u2032\u2032nb/(2\u03b2)A(n)1/\u03b2(log(log(1/\u03b4)))96 log(1/\u03b4)\nTherefore, by Lemmas 4 and 5, the previous equation implies in the three cases for some constants E4, E\u2032\u2032\u2032:\n\u2022 Case \u03b2 < 2: With probability larger than 1\u2212 F4 log(1/\u03b4)2\u03b4, we have\nrn \u2264 E\u2032\u2032\u2032n\u22121/2(log(log(1/\u03b4)))96 log(1/\u03b4),\nhence with probability larger than 1\u2212 \u03b4,\nrn \u2264 E4n\u22121/2 log(1/\u03b4)(log(log(1/\u03b4)))96 \u223c n\u22121/2.\n\u2022 Case \u03b2 > 2: With probability larger than 1\u2212 F4 log(1/\u03b4)2 log(n)3\u03b4, we have\nrn \u2264 E\u2032\u2032\u2032(n log(n))\u22121/\u03b2(log(log(1/\u03b4)))96 log(1/\u03b4),\nhence with probability larger than 1\u2212 \u03b4,\nrn \u2264 E4(n log(n))\u22121/\u03b2(log(log(log(n)/\u03b4)))96 log(log(n)/\u03b4) \u223c (n log(n))\u22121/\u03b2 log log(n) log log log(n)96.\n\u2022 Case \u03b2 = 2: With probability larger than 1\u2212 F4 log(1/\u03b4)2 log(n)3\u03b4, we have\nrn \u2264 E\u2032\u2032\u2032 log(n)n\u22121/2(log(log(1/\u03b4)))96 log(1/\u03b4),\nhence with probability larger than 1\u2212 \u03b4,\nrn \u2264 E4 log(n)n\u22121/2(log(log(log(n)/\u03b4)))96 log(log(n)/\u03b4) \u223c log(n)n\u22121/2 log log(n) log log log(n)96."}, {"heading": "C. Full proof of Theorem 1", "text": "C.1. Case \u03b2 < 2\nBy Assumption 2 (equivalent to Assumption 3), we know that\nE\u2032u1/\u03b2 \u2265 G(u) \u2265 Eu1/\u03b2 .\nAssume that when pulling an arm from the reservoir, its distribution is Gaussian of mean following the distribution associated to G and has variance 1. Since the budget is bounded by n, an algorithm pulls at most n arms from the arm reservoir. Let us define\nI1 =\n[ \u00b5\u0304\u2217 \u2212 E\n\u2032c 1/\u03b2 1\u221a n , \u00b5\u0304\u2217 \u2212 E(c \u2032 1) 1/\u03b2 \u221a n\n] and I2 = [ \u00b5\u0304\u2217 \u2212 E(c \u2032 1) 1/\u03b2\n\u221a n\n, \u00b5\u0304\u2217 ] ,\nwhere c1, c\u20321 are constants. If we denote N1 the number of arms in I1 and N2 the number of arms in I2 among the n first arms pulled from the arm reservoir, we can use Bernstein\u2019s inequality and for n \u2265 1 larger than a large enough constant\nP1 ( N2 \u2265 n\nE\u2032/Ec\u20321 n\u03b2/2 (1 + log (1/\u03b4))\n) \u2264 \u03b4 and P1 ( N1 \u2264 n\nc1 \u2212 c\u20321 n\u03b2/2\n(1\u2212 log (1/\u03b4)) ) \u2264 \u03b4.\nConsequently, for c1 large enough when compared to c\u20321, it implies that with probability larger than 1 \u2212 2\u03b4, we have that N1 > 1 and N1 > N2. Consider the event \u03be of probability 1\u2212 2\u03b4 where this is satisfied.\nOn \u03be, a problem that is strictly easier than the initial problem is the one where an oracle points out two arms to the learner, the best arm in I2 and the worst arm in I1, and where the objective is to distinguish between these two arms and output the arm in I2. Indeed, this problem is on \u03be strictly easier than an intermediate problem where an oracle provides the set of arms in I1 \u222a I2 and asks for an arm in I2, since N1 > N2. On \u03be, this intermediate problem is in turn strictly easier than the original problem of outputting an arm in I2 without oracle knowledge. Therefore, for the purpose of constructing the lower bound, we will now turn to the strictly easier problem of deciding between the arm k\u2217 with highest mean in I2, and the arm k with lowest mean in I1 and prove the lower bound for this strictly easier problem.\nSince the number of pulls on both k\u2217 and k is bounded by n, we use the chain rule and the fact that the distributions are Gaussian to get on \u03be\nKL(k, k\u2217) \u2264 n(\u00b5\u2212 \u00b5\u2217)2,\nwhere \u00b5 is the mean of k and \u00b5\u2217 is the mean of k\u2217. Given \u03be, let p be the probability that k is selected as the best arm, and p\u2217 the probability that k\u2217 is selected as the best arm. By Pinsker\u2019s inequality, we know that on \u03be\n|p\u2212 p\u2217| \u2264 \u221a KL(k1, k\u2217) \u2264 \u221a n|\u00b51 \u2212 \u00b5\u0304\u2217| \u2264 \u221a nE\u2032\nc 1/\u03b2 1\u221a n \u2264 E\u2032c1/\u03b21 .\nSince there are only two arms in this simplified game, we know that on \u03be\np\u2217 \u2264 1/2 + E\u2032c1/\u03b21 \u2264 7/12.\nfor c1 small enough. By definition of \u03be and since the problem we considered is easier than the initial problem, we know that for all algorithms, the probability P \u2217 of selecting an arm in I2 is bounded as follows where we add the probability that \u03be does not hold,\nP \u2217 \u2264 7/12 + 2\u03b4 \u2264 2/3,\nfor \u03b4 small enough. This concludes the proof by definition of I2.\nC.2. Case \u03b2 \u2265 2\nBy Assumption 2 (equivalent to Assumption 3), we know that\nE\u2032u1/\u03b2 \u2265 G(u) \u2265 Eu1/\u03b2 .\nAssume that when pulling an arm from the reservoir, its distribution is Gaussian of mean following the distribution associated to G and has a variance 1. The total number of arms pulled in the reservoir is smaller than n since the budget is bounded by n. Let\nI0 =\n[ \u00b5\u0304\u2217 \u2212 E (c0 n )1/\u03b2 , \u00b5\u0304\u2217 ] .\nwhere c0 is a constant defined in function of \u03b4 > 0 such that, if we denote N0 for the number of arms in I0, we have P1 (N0 = 0) \u2265 (\n1\u2212 c0 n\n)n \u2265 exp (\u2212c0/2) \u2265 1\u2212 \u03b4.\nThereupon, there are no arms in I0 with probability larger than 1\u2212 \u03b4, and therefore, with probability larger than 1\u2212 \u03b4 the regret of the algorithm is larger than\nE (c0 n )1/\u03b2 ."}, {"heading": "D. Proof of Lemma 1", "text": "By a union bound, we know that with probability larger than 1\u2212 \u03b4, for all k \u2264 N , we have\n|m\u0302k \u2212mk| \u2264 c \u221a log (N/\u03b4)\nN .\nNote that by Assumption 2, we have that with probability larger than 1\u2212 \u03b4,\n|m\u0302\u2217 \u2212 \u00b5\u0304\u2217| \u2264 c ( 1\n\u03b4N\n)1/\u03b2 .\nLet us write\nvN = c\n\u221a log (N/\u03b4)\nN + c\n( 1\n\u03b4N\n)1/\u03b2 .\nNote first that with probability larger than 1\u2212 \u03b4 on the samples (not on mk)\n1\nN N\u2211 k=1 1{\u00b5\u0304\u2217 \u2212mk \u2264 N\u2212\u03b5 + vN} \u2265 p\u0302 \u2265 1 N N\u2211 k=1 1{\u00b5\u0304\u2217 \u2212mk \u2264 N\u2212\u03b5 \u2212 vN},\nWe now define for l \u2208 {0, 1}\np+ = Pm\u223cL ( \u00b5\u0304\u2217 \u2212m \u2264 N\u2212\u03b5 + vN ) and p\u2212 = Pm\u223cL ( \u00b5\u0304\u2217 \u2212m \u2264 N\u2212\u03b5 \u2212 vN ) .\nNotice that for n larger than a constant that depends on B\u0303 of Assumption 2, we have by Assumption 2 the following bound for \u2217 \u2208 {+,\u2212}, since (vNN\u03b5) \u2264 1/2\u03b4\u22121/\u03b2 as \u03b5 < min(\u03b2, 1/2), and also for N larger than a constant that depends on B\u0303 only \u2223\u2223\u2223\u2223\u2212 log(p\u2217)\u03b5 logN \u2212 \u03b2\n\u2223\u2223\u2223\u2223 \u2264 (vNN\u03b5)\u03b2/\u03b2 + max(1, log(E\u0303\u2032), | log(E\u0303)|)\u03b5 logN \u2264 \u03b4\u22121/\u03b2/\u03b2 + max(1, log(E\u0303\u2032), | log(E\u0303)|)\u03b5 logN , which implies that\np\u2217 \u2265 c\u2032N\u2212\u03b2\u03b5,\nwhere c\u2032 > 1/2 is a small constant that is larger than E\u0303/2 for n larger than a constant that depends only on B\u0303.\nBy Hoeffding\u2019s inequality applied to Bernoulli random variables, we have that with probability larger than 1\u2212 \u03b4\u2223\u2223\u2223\u2223\u2223 1N N\u2211 k=1 1{\u00b5\u0304\u2217 \u2212mk \u2264 N\u2212\u03b5 + vN} \u2212 p+ \u2223\u2223\u2223\u2223\u2223 \u2264 c \u221a log(1/\u03b4) N def=wN ,\nand the same for p\u2212 with 1N \u2211N k=1 1{\u00b5\u0304\u2217 \u2212mk \u2264 N\u2212\u03b5 \u2212 vN}. All of this implies that with probability larger than 1\u2212 6\u03b4\np+ + wN \u2265 p\u0302 \u2265 p\u2212 \u2212 wN ,\nwhich implies that with probability larger than 1\u2212 6\u03b4\n\u2212 log(p + + wN ) \u03b5 logN \u2264 p\u0302 \u2264 \u2212 log(p \u2212 \u2212 wN ) \u03b5 logN ,\ni.e., with probability larger than 1 \u2212 6\u03b4, since wN/p\u2212 \u2264 1/2 \u221a\nlog(1/\u03b4) as n is large enough (larger than a constant) and since \u03b2 \u2264 1/(2\u03b5)\n\u2212 log(p +) \u03b5 logN \u2212 2wN p+ log(n)\u03b5 \u2264 \u2212 log(p\u0302) \u03b5 logN \u2265 \u2212 log(p \u2212) \u03b5 logN + 2wN p\u2212\u03b5 logN ,\nwhich implies the final claim\u2223\u2223\u2223\u2223\u2212 log(p\u0302)\u03b5 logN \u2212 \u03b2 \u2223\u2223\u2223\u2223 \u2264 \u03b4\u22121/\u03b2/\u03b2 + \u221a log(1/\u03b4) + max(1, log(E\u0303\u2032), | log(E\u0303)|) \u03b5 logN ."}], "references": [{"title": "Minimax Policies for Adversarial and Stochastic Bandits", "author": ["Audibert", "Jean-Yves", "Bubeck", "S\u00e9bastien"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Audibert et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2009}, {"title": "Tuning Bandit Algorithms in Stochastic Environments", "author": ["Audibert", "Jean-Yves", "Munos", "R\u00e9mi", "Szepesv\u00e1ri", "Csaba"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Audibert et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2007}, {"title": "Best arm identification in multi-armed bandits", "author": ["Audibert", "Jean-Yves", "Bubeck", "S\u00e9bastien", "Munos", "R\u00e9mi"], "venue": "Conference on Learning Theory,", "citeRegEx": "Audibert et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2010}, {"title": "Online Stochastic Optimization under Correlated Bandit Feedback", "author": ["Azar", "Mohammad Gheshlaghi", "Lazaric", "Alessandro", "Brunskill", "Emma"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Azar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2014}, {"title": "Bandit problems with infinitely many arms", "author": ["Berry", "Donald A", "Chen", "Robert W", "Zame", "Alan", "Heath", "David C", "Shepp", "Larry A"], "venue": "Annals of Statistics,", "citeRegEx": "Berry et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Berry et al\\.", "year": 1997}, {"title": "Two-Target Algorithms for Infinite-Armed Bandits with Bernoulli Rewards", "author": ["Bonald", "Thomas", "Prouti\u00e8re", "Alexandre"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Bonald et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bonald et al\\.", "year": 2013}, {"title": "Adaptive and minimax optimal estimation of the tail coefficient", "author": ["Carpentier", "Alexandra", "Kim", "Arlene K. H"], "venue": "Statistica Sinica,", "citeRegEx": "Carpentier et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Carpentier et al\\.", "year": 2014}, {"title": "Simple regret for infinitely many armed bandits", "author": ["Carpentier", "Alexandra", "Valko", "Michal"], "venue": "ArXiv e-prints,", "citeRegEx": "Carpentier et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Carpentier et al\\.", "year": 2015}, {"title": "Challenging the empirical mean and empirical variance: a deviation study", "author": ["Catoni", "Olivier"], "venue": "In Annales de l\u2019Institut Henri Poincare\u0301, Probabilite\u0301s et Statistiques,", "citeRegEx": "Catoni and Olivier.,? \\Q2012\\E", "shortCiteRegEx": "Catoni and Olivier.", "year": 2012}, {"title": "Stochastic Linear Optimization under Bandit Feedback", "author": ["Dani", "Varsha", "Hayes", "Thomas P", "Kakade", "Sham M"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Extreme Value Theory: An Introduction", "author": ["de Haan", "Laurens", "Ferreira", "Ana"], "venue": "Springer Series in Operations Research and Financial Engineering. Springer,", "citeRegEx": "Haan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Haan et al\\.", "year": 2006}, {"title": "Action elimination and stopping conditions for the multiarmed bandit and reinforcement learning problems", "author": ["Even-Dar", "Eyal", "Mannor", "Shie", "Mansour", "Yishay"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Even.Dar et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2006}, {"title": "Best arm identification: A unified approach to fixed budget and fixed confidence", "author": ["Gabillon", "Victor", "Ghavamzadeh", "Mohammad", "Lazaric", "Alessandro"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Gabillon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2012}, {"title": "Feature Selection and Dimensionality Reduction in Genomics and Proteomics. In Berrar, Dubitzky, and Granzow (eds.), Fundamentals of Data Mining in Genomics and Proteomics", "author": ["Hauskrecht", "Milos", "Pelikan", "Richard", "Valko", "Michal", "Lyons-Weiler", "James"], "venue": null, "citeRegEx": "Hauskrecht et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hauskrecht et al\\.", "year": 2006}, {"title": "A Simple General Approach to Inference About the Tail of a Distribution", "author": ["Hill", "Bruce M"], "venue": "The Annals of Statistics,", "citeRegEx": "Hill and M.,? \\Q1975\\E", "shortCiteRegEx": "Hill and M.", "year": 1975}, {"title": "lil\u2019UCB: An Optimal Exploration Algorithm for Multi-Armed Bandits", "author": ["Jamieson", "Kevin", "Malloy", "Matthew", "Nowak", "Robert", "Bubeck", "S\u00e9bastien"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Jamieson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jamieson et al\\.", "year": 2014}, {"title": "PAC subset selection in stochastic multi-armed bandits", "author": ["Kalyanakrishnan", "Shivaram", "Tewari", "Ambuj", "Auer", "Peter", "Stone"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Kalyanakrishnan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kalyanakrishnan et al\\.", "year": 2012}, {"title": "Almost optimal exploration in multi-armed bandits", "author": ["Karnin", "Zohar", "Koren", "Tomer", "Somekh", "Oren"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Karnin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Karnin et al\\.", "year": 2013}, {"title": "Information complexity in bandit subset selection", "author": ["Kaufmann", "Emilie", "Kalyanakrishnan", "Shivaram"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Kaufmann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2013}, {"title": "Multi-armed bandit problems in metric spaces", "author": ["Kleinberg", "Robert", "Slivkins", "Alexander", "Upfal", "Eli"], "venue": "In 40th ACM Symposium on Theory Of Computing,", "citeRegEx": "Kleinberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2008}, {"title": "From Bandits to Monte-Carlo Tree Search: The Optimistic Principle Applied to Optimization and Planning", "author": ["Munos", "R\u00e9mi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Munos and R\u00e9mi.,? \\Q2014\\E", "shortCiteRegEx": "Munos and R\u00e9mi.", "year": 2014}, {"title": "Statistical Inference Using Extreme Order Statistics", "author": ["Pickands", "James III"], "venue": "The Annals of Statistics,", "citeRegEx": "Pickands and III.,? \\Q1975\\E", "shortCiteRegEx": "Pickands and III.", "year": 1975}, {"title": "Algorithms for Infinitely Many-Armed Bandits", "author": ["Wang", "Yizao", "Audibert", "Jean-Yves", "Munos", "R\u00e9mi"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Wang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 4, "context": "In this paper, we consider an extension of this setting to infinitely many actions, the infinitely many armed bandits (Berry et al., 1997; Wang et al., 2008; Bonald & Prouti\u00e8re, 2013).", "startOffset": 118, "endOffset": 183}, {"referenceID": 22, "context": "In this paper, we consider an extension of this setting to infinitely many actions, the infinitely many armed bandits (Berry et al., 1997; Wang et al., 2008; Bonald & Prouti\u00e8re, 2013).", "startOffset": 118, "endOffset": 183}, {"referenceID": 4, "context": "In this paper, we consider an extension of this setting to infinitely many actions, the infinitely many armed bandits (Berry et al., 1997; Wang et al., 2008; Bonald & Prouti\u00e8re, 2013). Inevitably, the sheer amount of possible actions makes it impossible to try each of them even once. Such a setting is practically relevant for cases where one faces a finite, but extremely large number of actions. This setting was first formalized by Berry et al. (1997) as follows.", "startOffset": 119, "endOffset": 456}, {"referenceID": 4, "context": "In this paper, we consider an extension of this setting to infinitely many actions, the infinitely many armed bandits (Berry et al., 1997; Wang et al., 2008; Bonald & Prouti\u00e8re, 2013). Inevitably, the sheer amount of possible actions makes it impossible to try each of them even once. Such a setting is practically relevant for cases where one faces a finite, but extremely large number of actions. This setting was first formalized by Berry et al. (1997) as follows. At each time t, the learner can either sample an arm (a distribution) that has been already observed in the past, or sample a new arm, whose mean \u03bc is sampled from the mean reservoir distribution L. The additional challenges of the infinitely many armed bandits with respect to the multi-armed bandits come from two sources. First, we need to find a good arm among the sampled ones. Second, we need to sample (at least once) enough arms in order to have (at least once) a reasonably good one. These two difficulties ask for a while which we call the arm selection tradeoff. It is different from the known exploration/exploitation tradeoff and more linked to model selection principles: On one hand, we want to sample only from a small subsample of arms so that we can decide, with enough accuracy, which one is the best one among them. On the other hand, we want to sample as many arms as possible in order to have a higher chance to sample a good arm at least once. This tradeoff makes the problem of infinitely many armed bandits significantly different from the classical bandit problem. Berry et al. (1997) provide asymptotic, minimax-optimal (up to a log n factor) bounds for the average cumulative regret, defined as the difference between n times the highest possible value \u03bc\u0304\u2217 of the mean reservoir distribution and the mean of the sum of all samples that the learner collects.", "startOffset": 119, "endOffset": 1579}, {"referenceID": 4, "context": "In this paper, we consider an extension of this setting to infinitely many actions, the infinitely many armed bandits (Berry et al., 1997; Wang et al., 2008; Bonald & Prouti\u00e8re, 2013). Inevitably, the sheer amount of possible actions makes it impossible to try each of them even once. Such a setting is practically relevant for cases where one faces a finite, but extremely large number of actions. This setting was first formalized by Berry et al. (1997) as follows. At each time t, the learner can either sample an arm (a distribution) that has been already observed in the past, or sample a new arm, whose mean \u03bc is sampled from the mean reservoir distribution L. The additional challenges of the infinitely many armed bandits with respect to the multi-armed bandits come from two sources. First, we need to find a good arm among the sampled ones. Second, we need to sample (at least once) enough arms in order to have (at least once) a reasonably good one. These two difficulties ask for a while which we call the arm selection tradeoff. It is different from the known exploration/exploitation tradeoff and more linked to model selection principles: On one hand, we want to sample only from a small subsample of arms so that we can decide, with enough accuracy, which one is the best one among them. On the other hand, we want to sample as many arms as possible in order to have a higher chance to sample a good arm at least once. This tradeoff makes the problem of infinitely many armed bandits significantly different from the classical bandit problem. Berry et al. (1997) provide asymptotic, minimax-optimal (up to a log n factor) bounds for the average cumulative regret, defined as the difference between n times the highest possible value \u03bc\u0304\u2217 of the mean reservoir distribution and the mean of the sum of all samples that the learner collects. A follow-up on this result was the work of Wang et al. (2008), providing algorithms with finite-time regret bounds and the work of Bonald & Prouti\u00e8re (2013), giving an algorithm that is optimal with exact constants in a strictly more specific setting.", "startOffset": 119, "endOffset": 1916}, {"referenceID": 4, "context": "In this paper, we consider an extension of this setting to infinitely many actions, the infinitely many armed bandits (Berry et al., 1997; Wang et al., 2008; Bonald & Prouti\u00e8re, 2013). Inevitably, the sheer amount of possible actions makes it impossible to try each of them even once. Such a setting is practically relevant for cases where one faces a finite, but extremely large number of actions. This setting was first formalized by Berry et al. (1997) as follows. At each time t, the learner can either sample an arm (a distribution) that has been already observed in the past, or sample a new arm, whose mean \u03bc is sampled from the mean reservoir distribution L. The additional challenges of the infinitely many armed bandits with respect to the multi-armed bandits come from two sources. First, we need to find a good arm among the sampled ones. Second, we need to sample (at least once) enough arms in order to have (at least once) a reasonably good one. These two difficulties ask for a while which we call the arm selection tradeoff. It is different from the known exploration/exploitation tradeoff and more linked to model selection principles: On one hand, we want to sample only from a small subsample of arms so that we can decide, with enough accuracy, which one is the best one among them. On the other hand, we want to sample as many arms as possible in order to have a higher chance to sample a good arm at least once. This tradeoff makes the problem of infinitely many armed bandits significantly different from the classical bandit problem. Berry et al. (1997) provide asymptotic, minimax-optimal (up to a log n factor) bounds for the average cumulative regret, defined as the difference between n times the highest possible value \u03bc\u0304\u2217 of the mean reservoir distribution and the mean of the sum of all samples that the learner collects. A follow-up on this result was the work of Wang et al. (2008), providing algorithms with finite-time regret bounds and the work of Bonald & Prouti\u00e8re (2013), giving an algorithm that is optimal with exact constants in a strictly more specific setting.", "startOffset": 119, "endOffset": 2011}, {"referenceID": 4, "context": "Specifically, Berry et al. (1997) and Wang et al.", "startOffset": 14, "endOffset": 34}, {"referenceID": 4, "context": "Specifically, Berry et al. (1997) and Wang et al. (2008) assume that the mean reservoir distribution is such that, for a small \u03b5 > 0, locally around the best arm \u03bc\u0304\u2217, we have that P\u03bc\u223cL (\u03bc\u0304\u2217 \u2212 \u03bc \u2265 \u03b5) \u2248 \u03b5 , (1)", "startOffset": 14, "endOffset": 57}, {"referenceID": 11, "context": "The problem of minimizing the simple regret in a multi-armed bandit setting (with finitely many arms) has recently attracted significant attention (Even-Dar et al., 2006; Audibert et al., 2010; Kalyanakrishnan et al., 2012; Kaufmann & Kalyanakrishnan, 2013; Karnin et al., 2013; Gabillon et al., 2012; Jamieson et al., 2014) and algorithms have been developed either in the setting of a fixed budget which aims at finding an optimal arm or in the setting of a floating budget which aims at finding an \u03b5-optimal arm.", "startOffset": 147, "endOffset": 324}, {"referenceID": 2, "context": "The problem of minimizing the simple regret in a multi-armed bandit setting (with finitely many arms) has recently attracted significant attention (Even-Dar et al., 2006; Audibert et al., 2010; Kalyanakrishnan et al., 2012; Kaufmann & Kalyanakrishnan, 2013; Karnin et al., 2013; Gabillon et al., 2012; Jamieson et al., 2014) and algorithms have been developed either in the setting of a fixed budget which aims at finding an optimal arm or in the setting of a floating budget which aims at finding an \u03b5-optimal arm.", "startOffset": 147, "endOffset": 324}, {"referenceID": 16, "context": "The problem of minimizing the simple regret in a multi-armed bandit setting (with finitely many arms) has recently attracted significant attention (Even-Dar et al., 2006; Audibert et al., 2010; Kalyanakrishnan et al., 2012; Kaufmann & Kalyanakrishnan, 2013; Karnin et al., 2013; Gabillon et al., 2012; Jamieson et al., 2014) and algorithms have been developed either in the setting of a fixed budget which aims at finding an optimal arm or in the setting of a floating budget which aims at finding an \u03b5-optimal arm.", "startOffset": 147, "endOffset": 324}, {"referenceID": 17, "context": "The problem of minimizing the simple regret in a multi-armed bandit setting (with finitely many arms) has recently attracted significant attention (Even-Dar et al., 2006; Audibert et al., 2010; Kalyanakrishnan et al., 2012; Kaufmann & Kalyanakrishnan, 2013; Karnin et al., 2013; Gabillon et al., 2012; Jamieson et al., 2014) and algorithms have been developed either in the setting of a fixed budget which aims at finding an optimal arm or in the setting of a floating budget which aims at finding an \u03b5-optimal arm.", "startOffset": 147, "endOffset": 324}, {"referenceID": 12, "context": "The problem of minimizing the simple regret in a multi-armed bandit setting (with finitely many arms) has recently attracted significant attention (Even-Dar et al., 2006; Audibert et al., 2010; Kalyanakrishnan et al., 2012; Kaufmann & Kalyanakrishnan, 2013; Karnin et al., 2013; Gabillon et al., 2012; Jamieson et al., 2014) and algorithms have been developed either in the setting of a fixed budget which aims at finding an optimal arm or in the setting of a floating budget which aims at finding an \u03b5-optimal arm.", "startOffset": 147, "endOffset": 324}, {"referenceID": 15, "context": "The problem of minimizing the simple regret in a multi-armed bandit setting (with finitely many arms) has recently attracted significant attention (Even-Dar et al., 2006; Audibert et al., 2010; Kalyanakrishnan et al., 2012; Kaufmann & Kalyanakrishnan, 2013; Karnin et al., 2013; Gabillon et al., 2012; Jamieson et al., 2014) and algorithms have been developed either in the setting of a fixed budget which aims at finding an optimal arm or in the setting of a floating budget which aims at finding an \u03b5-optimal arm.", "startOffset": 147, "endOffset": 324}, {"referenceID": 13, "context": "An example where efficient strategies for minimizing the simple regret of an infinitely many armed bandit are relevant is the search of a good biomarker in biology, a single feature that performs best on average (Hauskrecht et al., 2006).", "startOffset": 212, "endOffset": 237}, {"referenceID": 4, "context": "This can be also applied to the prior work (Berry et al., 1997; Wang et al., 2008) where \u03b2 is also necessary for implementation and optimal bounds.", "startOffset": 43, "endOffset": 82}, {"referenceID": 22, "context": "This can be also applied to the prior work (Berry et al., 1997; Wang et al., 2008) where \u03b2 is also necessary for implementation and optimal bounds.", "startOffset": 43, "endOffset": 82}, {"referenceID": 9, "context": "One class of examples is fixed design such as linear bandits (Dani et al., 2008) other settings consider bandits in known or unknown metric space (Kleinberg et al.", "startOffset": 61, "endOffset": 80}, {"referenceID": 4, "context": "The main difference between our strategy and the cumulative strategies (Berry et al., 1997; Wang et al., 2008; Bonald & Prouti\u00e8re, 2013) is in the number of arms sampled from the arm reservoir: For the simple regret, we need to sample more arms.", "startOffset": 71, "endOffset": 136}, {"referenceID": 22, "context": "The main difference between our strategy and the cumulative strategies (Berry et al., 1997; Wang et al., 2008; Bonald & Prouti\u00e8re, 2013) is in the number of arms sampled from the arm reservoir: For the simple regret, we need to sample more arms.", "startOffset": 71, "endOffset": 136}, {"referenceID": 0, "context": "It is also interesting to compare SiRI with existing algorithms targeting the simple regret for finitely many arms, as the ones by Audibert et al. (2010). SiRI can be related to their UCB-E with a specific confidence term and a specific choice of the number of arms selected.", "startOffset": 131, "endOffset": 154}, {"referenceID": 4, "context": "Case of distributions on [0, 1] with \u03bc\u0304\u2217 = 1 The first extension concerns the specific setting, particularly highlighted by Bonald & Prouti\u00e8re (2013) but also presented by Berry et al. (1997) and Wang et al.", "startOffset": 172, "endOffset": 192}, {"referenceID": 4, "context": "Case of distributions on [0, 1] with \u03bc\u0304\u2217 = 1 The first extension concerns the specific setting, particularly highlighted by Bonald & Prouti\u00e8re (2013) but also presented by Berry et al. (1997) and Wang et al. (2008), where the domain of the distributions of the arms are included in [0, 1] and where \u03bc\u0304\u2217 = 1.", "startOffset": 172, "endOffset": 215}, {"referenceID": 4, "context": "Case of distributions on [0, 1] with \u03bc\u0304\u2217 = 1 The first extension concerns the specific setting, particularly highlighted by Bonald & Prouti\u00e8re (2013) but also presented by Berry et al. (1997) and Wang et al. (2008), where the domain of the distributions of the arms are included in [0, 1] and where \u03bc\u0304\u2217 = 1. In this case, the information theoretic complexity of the problem is smaller than the one of the general problem stated in Theorem 1. Specifically, the variance of the near-optimal arms is very small, i.e., in the order of \u03b5 for an \u03b5-optimal arm. This implies a better bound, in particular, that the parametric limitation of 1/ \u221a n can be circumvented. In order to prove it, the simplest way is to modify SiRI into Bernstein-SiRI, displayed in Algorithm 2. It is an Empirical Bernstein-modified SiRI algorithm that accommodates the situation of distributions of support included in [0, 1] with \u03bc\u0304\u2217 = 1. Note that in the general case, it would provide similar results as what is provided in Theorem 2. A similar idea was already introduced by Wang et al. (2008) in the infinitely many armed setting for cumulative regret.", "startOffset": 172, "endOffset": 1069}, {"referenceID": 4, "context": "Case of distributions on [0, 1] with \u03bc\u0304\u2217 = 1 The first extension concerns the specific setting, particularly highlighted by Bonald & Prouti\u00e8re (2013) but also presented by Berry et al. (1997) and Wang et al. (2008), where the domain of the distributions of the arms are included in [0, 1] and where \u03bc\u0304\u2217 = 1. In this case, the information theoretic complexity of the problem is smaller than the one of the general problem stated in Theorem 1. Specifically, the variance of the near-optimal arms is very small, i.e., in the order of \u03b5 for an \u03b5-optimal arm. This implies a better bound, in particular, that the parametric limitation of 1/ \u221a n can be circumvented. In order to prove it, the simplest way is to modify SiRI into Bernstein-SiRI, displayed in Algorithm 2. It is an Empirical Bernstein-modified SiRI algorithm that accommodates the situation of distributions of support included in [0, 1] with \u03bc\u0304\u2217 = 1. Note that in the general case, it would provide similar results as what is provided in Theorem 2. A similar idea was already introduced by Wang et al. (2008) in the infinitely many armed setting for cumulative regret. The idea is that the confidence term is more refined using the empirical variance and hence it will be very large for a near-optimal arm, thereby enhancing exploration. Plugging this term in the proof, conditioning on the event of high probability, such that \u03c3\u0302 k,t is close to the true variance, and using similar ideas as Wang et al. (2008), we can immediately deduce the following corollary.", "startOffset": 172, "endOffset": 1472}, {"referenceID": 22, "context": "The proof follows immediately from the proof of Theorem 2 using the empirical Bernstein bound as by Wang et al. (2008). Moreover, the lower bounds\u2019 rates follow directly from the two facts: 1) 1/n is clearly a lower bound, and therefore optimal for \u03b2 < 1, since it takes at least n samples of a Bernoulli arm that is constant times 1/n suboptimal, in order to discover that it is not optimal, and 2) n\u22121/\u03b2 can be trivially deduced from Theorem 11.", "startOffset": 100, "endOffset": 119}, {"referenceID": 4, "context": "Yet its knowledge is crucial for the implementation of SiRI, as well as for all the cumulative regret strategies described in (Berry et al., 1997; Wang et al., 2008; Bonald & Prouti\u00e8re, 2013).", "startOffset": 126, "endOffset": 191}, {"referenceID": 22, "context": "Yet its knowledge is crucial for the implementation of SiRI, as well as for all the cumulative regret strategies described in (Berry et al., 1997; Wang et al., 2008; Bonald & Prouti\u00e8re, 2013).", "startOffset": 126, "endOffset": 191}, {"referenceID": 4, "context": "Yet its knowledge is crucial for the implementation of SiRI, as well as for all the cumulative regret strategies described in (Berry et al., 1997; Wang et al., 2008; Bonald & Prouti\u00e8re, 2013). Consequently, a very important question is whether it is possible to estimate it well enough to obtain good results, which we answer in the affirmative. An interesting remark is that Assumption 2 is actually related to assuming that the distribution function L is \u03b2 regularly varying in \u03bc\u0304\u2217. Therefore, \u03b2 is the tail index of the distribution function ofL and can be estimated with tools from extreme value theory (de Haan & Ferreira, 2006). Many estimators exist for estimating this tail index \u03b2, for instance, the popular Hill\u2019s estimate (Hill, 1975), but also Pickand\u2019s\u2019 estimate (Pickands, 1975) and others. However, our situation is slightly different from the one where the convergence of these estimators is proved, as the means of the arms are not directly observed. As a result, we propose another estimate, related to the estimate of Carpentier & Kim (2014), which accommodates our setting.", "startOffset": 127, "endOffset": 1061}, {"referenceID": 4, "context": "We would like to emphasize that \u03b2\u0304 estimate (6) of \u03b2 can be used to improve cumulative regret algorithms that need \u03b2, such as the ones by Berry et al. (1997) and Wang et al.", "startOffset": 138, "endOffset": 158}, {"referenceID": 4, "context": "We would like to emphasize that \u03b2\u0304 estimate (6) of \u03b2 can be used to improve cumulative regret algorithms that need \u03b2, such as the ones by Berry et al. (1997) and Wang et al. (2008). Similarly for these algorithms, one should spend a preliminary phase of N = \u221a n rounds to estimate \u03b2 and then run the algorithm of choice.", "startOffset": 138, "endOffset": 181}, {"referenceID": 22, "context": "For instance, consider the cumulative regret rate of UCB-F by Wang et al. (2008). If UCB-F uses our estimate of \u03b2 instead of the true \u03b2, it would still satisfy", "startOffset": 62, "endOffset": 81}, {"referenceID": 22, "context": "Second, Wang et al. (2008) propose a more refined way to deal with an unknown time horizon (UCB-AIR), that also directly applies to SiRI.", "startOffset": 8, "endOffset": 27}, {"referenceID": 22, "context": "First such comparator is UCB-F (Wang et al., 2008), an algorithm that optimizes cumulative regret for this setting.", "startOffset": 31, "endOffset": 50}, {"referenceID": 15, "context": "Second, we compare SiRI to lil\u2019UCB (Jamieson et al., 2014) designed for the best-arm identification in the fixed confidence setting.", "startOffset": 35, "endOffset": 58}, {"referenceID": 0, "context": "UCB-F is designed for fixed horizon of n evaluations and it is an extension of a version of UCB-V by Audibert et al. (2007). Second, we compare SiRI to lil\u2019UCB (Jamieson et al.", "startOffset": 101, "endOffset": 124}], "year": 2015, "abstractText": "We consider a stochastic bandit problem with infinitely many arms. In this setting, the learner has no chance of trying all the arms even once and has to dedicate its limited number of samples only to a certain number of arms. All previous algorithms for this setting were designed for minimizing the cumulative regret of the learner. In this paper, we propose an algorithm aiming at minimizing the simple regret. As in the cumulative regret setting of infinitely many armed bandits, the rate of the simple regret will depend on a parameter \u03b2 characterizing the distribution of the near-optimal arms. We prove that depending on \u03b2, our algorithm is minimax optimal either up to a multiplicative constant or up to a log(n) factor. We also provide extensions to several important cases: when \u03b2 is unknown, in a natural setting where the near-optimal arms have a small variance, and in the case of unknown time horizon.", "creator": "LaTeX with hyperref package"}}}