{"id": "1505.04891", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2015", "title": "Learning Better Word Embedding by Asymmetric Low-Rank Projection of Knowledge Graph", "abstract": "Word embedding, which refers to low-dimensional dense vector representations of natural words, has demonstrated its power in many natural language processing tasks. However, it may suffer from the inaccurate and incomplete information contained in the free text corpus as training data. To tackle this challenge, there have been quite a few works that leverage knowledge graphs as an additional information source to improve the quality of word embedding. Although these works have achieved certain success, they have neglected some important facts about knowledge graphs: (i) many relationships in knowledge graphs are \\emph{many-to-one}, \\emph{one-to-many} or even \\emph{many-to-many}, rather than simply \\emph{one-to-one}; (ii) most head entities and tail entities in knowledge graphs come from very different semantic spaces. To address these issues, in this paper, we propose a new algorithm named ProjectNet. ProjecNet models the relationships between head and tail entities after transforming them with different low-rank projection matrices. The low-rank projection can allow non \\emph{one-to-one} relationships between entities, while different projection matrices for head and tail entities allow them to originate in different semantic spaces. The experimental results demonstrate that ProjectNet yields more accurate word embedding than previous works, thus leads to clear improvements in various natural language processing tasks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 19 May 2015 07:08:10 GMT  (23kb)", "https://arxiv.org/abs/1505.04891v1", null], ["v2", "Sun, 14 Jun 2015 08:21:24 GMT  (23kb)", "http://arxiv.org/abs/1505.04891v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["fei tian", "bin gao", "enhong chen", "tie-yan liu"], "accepted": false, "id": "1505.04891"}, "pdf": {"name": "1505.04891.pdf", "metadata": {"source": "CRF", "title": "Learning Better Word Embedding by Asymmetric Low-Rank Projection of Knowledge Graph", "authors": ["Fei Tian"], "emails": ["tianfei@mail.ustc.edu.cn", "bingao@microsoft.com", "cheneh@ustc.edu.cn", "tyliu@microsoft.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 5.\n04 89\n1v 2\n[ cs\n.C L\n] 1\n4 Ju\nn 20\n15\nLearning Better Word Embedding by Asymmetric Low-Rank Projection of Knowledge Graph\nFei Tian University of Science and Technology of China\ntianfei@mail.ustc.edu.cn\nBin Gao Microsoft Research\nbingao@microsoft.com\nEnhong Chen University of Science and Technology of China\ncheneh@ustc.edu.cn\nTie-Yan Liu Microsoft Research\ntyliu@microsoft.com"}, {"heading": "Abstract", "text": "Word embedding, which refers to low-dimensional dense vector representations of natural words, has demonstrated its power in many natural language processing tasks. However, it may suffer from the inaccurate and incomplete information contained in the free text corpus as training data. To tackle this challenge, there have been quite a few works that leverage knowledge graphs as an additional information source to improve the quality of word embedding. Although these works have achieved certain success, they have neglected some important facts about knowledge graphs: (i) many relationships in knowledge graphs are many-to-one, oneto-many or even many-to-many, rather than simply one-to-one; (ii) most head entities and tail entities in knowledge graphs come from very different semantic spaces. To address these issues, in this paper, we propose a new algorithm named ProjectNet. ProjecNet models the relationships between head and tail entities after transforming them with different low-rank projection matrices. The low-rank projection can allow non one-to-one relationships between entities, while different projection matrices for head and tail entities allow them to originate in different semantic spaces. The experimental results demonstrate that ProjectNet yields more accurate word embedding than previous works, thus leads to clear improvements in various natural language processing tasks."}, {"heading": "1 Introduction", "text": "In recent years, the research on word embedding (or distributed word representations) has made promising progresses in many natural language processing tasks [1; 7; 11; 15; 16; 18]. Different from traditional one-hot discrete representations of words, word embedding vectors are dense, continuous, and low-dimensional. They are usually trained with neural networks on a large-scale free text corpus, such as Wikipedia, news articles, and web pages, in an unsupervised manner.\nWhile word embedding has demonstrated its power in many circumstances, it is gradually recognized that conven-\ntional word embedding techniques may suffer from the incomplete and inaccurate information contained in the free text data. On one hand, due to the restrictive topics and coverage of a text corpus, some words might not have sufficient contexts and therefore might not have reliable word embeddings. On the other hand, even if a word has sufficient contextual data, the free texts might be inaccurate, thus might not provide a semantically precise view of the word. As a result, the learned word embedding might be unable to carry on the desirable semantic information. To tackle this problem, recently some researchers have proposed to leverage knowledge graphs, such as WordNet [17] and Freebase [3], as additional data sources to improve word embedding [2; 21; 23].\nA knowledge graph contains a set of nodes representing entities and a set of edges corresponding to the relationships between entities. In other words, a knowledge graph can be regarded as a set of triples (h, r, t), where head entity h and tail entity t share the relationship r. In [23; 21], in addition to the original likelihood loss on the free texts, an extra loss function is imposed to capture the relationships in the knowledge graph. Specifically, the additional loss takes the form LK = \u2211 (h,r,t) ||h+ r\u2212 t|| 2 2, where h, t are the embedding vectors of the words (entities) h and t respectively, and r is the embedding of the relationship r.1 Then the embeddings are learned by minimizing the overall loss on both free text and knowledge graph.\nWhile the above approaches have shown certain success, we would like to point out their limitations.\nFirst, the loss function LK in these works cannot capture complex relationships between entities. In particular, it will encounter problem when the relationships are one-to-many, many-to-one, or many-to-many. For example, r =\u201ccause of death\u201d is a many-to-one relationship, since many different head entities hi (e.g., h1 = \u201cAbraham Lincoln\u201d and h2 = \u201cJohn F Kennedy\u201d) correspond to the same tail entity (e.g., t =\u201cassassination by firearm\u201d). In this case, the minimization of LK will enforce the embedding vectors of all head entities (e.g., h1, h2) to approach each other, which is clearly unreasonable.\nActually such kind of complex relationships are very com-\n1Please note that throughout the paper we will use bold characters to represent the embedding vectors for corresponding items.\nmon in knowledge graphs. Take a widely used benchmark dataset FB13 [19], which is a subset of Freebase, as an instance. For every relationship in FB13, we calculate the average number of head entities corresponding to one tail entity and the average number of tail entities corresponding to one head entity. Then we obtain the means and standard deviations of such values under different relationships. The overall statistical information is listed in Table 1, from which we can see that relationships in FB13 are highly non one-to-one, especially for the mapping from tail entity to head entity, as shown by the large mean value of #Head per Tail. In addition, the standard deviation for #Head per Tail is fairly large, indicating that the degrees of non one-to-one mappings from tail entity to head entity vary drastically across different relationships. This clearly shows that the issue is very serious and we should tackle it in order to learn a reasonable word embedding.\nSecond, the loss function LK adopts simple arithmetic operations on the embedding vectors of the head and tail entities, implying that both entities are located in the same space. However, the fact is that head entities are usually more concrete and tail entities are more abstract, making it unreasonable to simply regard them as in a homogeneous space. Still use the above example, for the relationship r =\u201ccause of death\u201d, all the head entities are real human names whereas all the tail entities are abstract reasons of death. What\u2019s more, according to Table 1, head and tail entities are not symmetric from the statistics perspective: the number of tail entities per head entity is much smaller than that of head entities per tail entity, further indicating the heterogeneity nature of head and tail entities and suggesting that we should treat them separately in the mathematical modeling.\nIn the literature, there are some research works that try to resolve one of the aforementioned issues, however, as far as we know, none of the works successfully addressed both issues. For example, in [22], it is proposed to project the embedding vectors of both entities onto a relation-dependent hyperplane before computing the loss function LK. However, the heterogeneity between head and tail entities is not considered. Furthermore, the projection matrix used in [22] has a fixed rank for all types of relationships, which could not express various degrees of non one-to-one mappings. In [5], different transformations are adopted to head and tail entities respectively, however, no consideration is taken to address the issue of non one-to-one mappings.\nTo address the limitations of existing works, in this paper, we propose a new algorithm called ProjectNet, which adopts different and carefully designed projections to the head and tail entities respectively when defining the loss function LK. First, we show that the necessary condition to resolve the issue of non one-to-one mapping is to ensure the projection matrix to be low-rank. In such a way, we can guarantee the trans-\nlation distance between the entities to be small after projection without forcing their embedding vectors to be the same. Actually, it can be proven that the TransH model in [22] is our special case in the sense that it also adopts a projection matrix of low (and fixed) rank. Our model is more general since we can explicitly control the rank of the projection matrix, so as to adapt to knowledge graphs with different degrees of non one-to-one mappings. Second, by using different projection matrices for head and tail entities respectively, we can avoid the homogeneity assumption on the semantic space and therefore build a more flexible and accurate model. For example, for the knowledge graph FB13, we should adopt a low-rank projection matrix for head entities since the number of head entities is very large for each tail entity; however, it is safe to use a relatively full rank projection matrix for tail entities since the number of tail entities is rather small for each head entity.\nWe have tested the performance of our proposed algorithm on several benchmark datasets, and the experimental results show that our proposal can significantly outperform the baseline methods. This indicates the benefit of carefully modeling entities and relationships when incorporating knowledge graphs into the learning process of word embedding.\nThe rest of the paper is organized as following. In Section 2, we summarize related works in leveraging knowledge graph to help word embedding. Then in Section 3, the detailed model is introduced and its difference with related methods is illustrated. After that, the experimental settings and results are reported in Section 4. The paper is finally concluded in Section 5."}, {"heading": "2 Related Work", "text": "Word embeddings, (a.k.a. distributed word representations) are usually trained with neural networks by maximizing the likelihood of a text corpus. Based on several pioneering efforts [1; 7; 20], the research works in this field have grown rapidly in recent years [11; 15; 16; 18; 6]. Among them, word2vec [15; 16] draws quite a lot of attention from the community due to its simplicity and effectiveness. An interesting result given by word2vec is that the word embedding vectors it produces can reflect human knowledge via some simple arithmetic operations, e.g., v(Japan) \u2212 v(Tokyo) \u2248 v(France)\u2212 v(Pairs).\nHowever, as aforementioned, word embedding models like word2vec usually suffer from the incompleteness and inaccuracy of the free-text training corpus. To address this challenge, there are some attempts that leverage additional structured or unstructured human knowledge to enhance word embeddings. Here are some examples. In [14; 8], the authors adopted morphological knowledge to aid the learning of rare words and new words. In [24], the authors used semantic relational knowledge between words as a constraint in learning word embedding vectors. In [23], the authors leveraged knowledge graphs, the most widely used structured knowledge, to help improve word representations. In particular, the authors did not only minimize the loss on the text corpus, but also minimized the loss on the knowledge graph by sharing embedding vectors between words and entities. In\n[21], the authors proposed a very similar method to [23], but with a different objective of improving knowledge graph understanding with the help of text corpus. Actually, both the models in [23] and [21] are inspired by the TransE model [4], which is a state-of-the-art work in the literature of computing distributed representations for knowledge graphs [5; 12; 19]. In TransE, the relational operation between entities h, t with relationship r is assumed to be a simple linear translation, i.e., min ||h + r \u2212 t||22. However, as pointed out in the introduction, such a simple formulation cannot handle the non one-to-one mappings between entities. To tackle the problem, in [22], the authors proposed a simple projection method named TransH. We will review the detailed mathematical forms of these models in Section 3.3 and discuss their relationship with our proposal."}, {"heading": "3 The ProjectNet Algorithm", "text": "In this section, we introduce our proposed ProjectNet model in details. In general, following [23; 21], given a training text corpus D and a set K of triples in the form (head entity, relation, tail entity) extracted from a knowledge graph, our model jointly minimize a linear combination of the loss items on both text and knowledge:\nL = \u03b1LD + (1\u2212 \u03b1)LK, (1)\nwhere \u03b1 \u2208 [0, 1] is used to trade off the two loss terms. LD and LK share the same parameters, i.e., the embedding vectors for words and their corresponding entities are the same. In the following subsections, we will introduce the text model to specify LD and the knowledge model to specify LK."}, {"heading": "3.1 Text Model", "text": "Similar to [23; 21], we leverage the Skip-Gram model [16] as the text model. In Skip-Gram, the probability of observing the target word wO given its context word wI is modeled as P (wO|wI) = exp(w\u2032 O \u00b7wI)\u2211\nw\u2208V exp(w\u2032\u00b7wI)\n, where w \u2208 Rd and w\u2032 \u2208\nRd denote the input and output embedding vectors for word w respectively, V is the dictionary, and d is dimension of the embedding.\nGiven the training corpus D consisting of |D| token words {p1, \u00b7 \u00b7 \u00b7 , pk, \u00b7 \u00b7 \u00b7 , p|D|}, the loss LD is specified by:\nLD =\n|D|\u2211\nk=1\n\u2211\nj\u2208{\u2212M,\u00b7\u00b7\u00b7 ,M},j 6=0\nP (pk|pk+j), (2)\nwhere 2M is the size of the sliding window. As it is expensive to directly minimize LD due to the denominator of P (wO|wI), we adopt the negative sampling strategy [16] to boost the computation efficiency."}, {"heading": "3.2 Knowledge Model", "text": "The knowledge model in ProjectNet is based on an asymmetric low-rank projection that projects the original entity embedding vectors into a new semantic space. The projection is designed to be asymmetric in order to handle the heterogeneity between head and tail entities, and is designed to be low-rank in order to deal with non one-to-one relationships in the knowledge graphs."}, {"heading": "Asymmetric Projection", "text": "As aforementioned, the head and tail entities in knowledge graphs are usually very different, from both semantic and statistical perspectives. Therefore, we argue that it is unreasonable to adopt the same projection to these two kinds of entities (as TransH [22] does). Instead, it would be better to adopt different projection matrices, denoted as Lr \u2208 Rd\u00d7d and Rr \u2208 Rd\u00d7d respectively, to the head and tail entities. Hence, given a triple (h, r, t), the original embedding vectors for h and t will be transformed to h\u2032 and t\u2032 as follows,\nh\u2032 = Lrh, t \u2032 = Rrt. (3)\nBased on the transformed embeddings, we define a scoring function fd to reflect the confidence level that the triple (h, r, t) is true:\nfd(h, r, t) = ||h \u2032 + r\u2212 t\u2032||22 = ||Lrh+ r\u2212Rrt|| 2 2. (4)\nThen we adopt a margin based ranking loss to distinguish the golden relationship triples from randomly corrupted triples: LK = \u2211\n(h,r,t)\n\u2211\n(h\u2032,r\u2032,t\u2032)\u2208N(h,r,t)\n[\u03b3 \u2212 fd(h \u2032, r\u2032, t\u2032) + fd(h, r, t)]+,\n(5) where [x]+ = max(0, x), \u03b3 > 0 is the margin value, N(h, r, t) is the set of all the corrupted triples built for the triple (h, r, t), and Lr and Rr will be specified in (8)."}, {"heading": "Low-Rank Projection", "text": "As mentioned in the introduction, many relationships in the knowledge graphs are non one-to-one. In this case, in order to achieve reasonable results during the minimization of LK defined above, it is necessary to constrain the projection matrices Lr and Rr to be low-rank, which is described in the following proposition.\nProposition 3.1 Once linear projections are imposed to head and tail entities, the necessary condition to overcome the non one-to-one mapping problem is that the projection matrices"}, {"heading": "Lr and Rr should not be full-ranked.", "text": "Proof Consider the following least-square problem w.r.t. the optimization variable h:\nmin ||Lrh\u2212 c|| 2 2, (6)\nwhere Lrh = h\u2032 and we regard c = t\u2032 \u2212 r as a constant vector. It is easy to obtain that the optimal solution h\u2217 satisfies the following linear system:\nLTr Lrh \u2217 = LTr c. (7)\nTo avoid the non one-to-one mapping problem, the above equation must have multiple solutions. Then it is necessary that LTr Lr is a low-rank matrix. In addition, as rank(LTr Lr) = rank(Lr), the linear projection matrix Lr must not be full-rank either. The same conclusion holds for the projection matrix Rr for the tail entity.\nGiven the above proposition, we use the following tricks to ensure that Lr and Rr are low-rank matrices (whose ranks are mL and mR respectively, with mL < d and mR < d):\nLr =\nmL\u2211\ni=1\n\u00b5(i)r p (i) r q(i) r T , Rr =\nmR\u2211\ni=1\n\u03b6(i)r o (i) r s(i) r\nT , (8)\nwhere \u00b5(i)r , \u03b6 (i) r are scalars, and p (i) r , q (i) r , o (i) r , s (i) r are all d-dimensional real vectors, the outer products of which con-\nstitute (mL + mR) rank 1 matrices p (i) r q (i) r\nT\nand o(i)r s (i) r\nT\n. For simplicity, we set the rank of all the left matrices Lr to be the same (mL) and the rank of all the right matrices Rr to be the same (mR). Please note we can also specify different ranks for different relationships r. We leave the corresponding discussions to the future work."}, {"heading": "3.3 Discussions", "text": "In this section, we discuss the connections of our proposed ProjectNet algorithm with a few previous works and show that they are special cases of ProjectNet.\nRNet. RNet refers to the knowledge models proposed in [23] and [21]. In fact, both models in the two works try to minimize the same scoring function: fd(h, r, t) = ||h + r \u2212 t||22. Their only difference lies in how fd is minimized. In [23], a large margin ranking loss is adopted for the minimization of fd(h, r, t), whereas in [21], an approximate softmax loss is used. It is clear that such a scoring function fd(h, r, t) cannot handle either the non one-to-one relationships between entities or the heterogeneity between head and tail entities. To state it more formally, let us consider the relationship triples (hi, r, t), i \u2208 1, \u00b7 \u00b7 \u00b7 , N , where all head entities hi have the same relationship r with tail entity t. In the ideal case, if all fd(hi, r, t) are fully minimized, we will have hi = t \u2212 r, \u2200i \u2208 1, \u00b7 \u00b7 \u00b7 , N , which implies that h1 = h2 = \u00b7 \u00b7 \u00b7 = hN. It means that all the embedding vectors for the head entities {hi}Ni=1 are the same, which is clearly unreasonable. We may encounter similar issues for one-to-many relationships {(h, r, tj)}j and many-to-many relationships {(hi, r, tj)}i,j .\nNote that RNet corresponds to Lr = Rr = Id\u00d7d in (3) and since the identity matrix Id\u00d7d can be written in the form of (8), RNet can be regarded as a special case of ProjectNet.\nTransH. TransH [22] is proposed to overcome the non oneto-one mapping problem. It first projects the entity embedding vectors h and t onto a hyperplane w.r.t the relationship r, and then the projected vectors h\u22a5 and t\u22a5 are used to define the scoring function fd. Specifically,\nh\u22a5 = h\u2212wr T hwr, t\u22a5 = t\u2212wr T twr,\nfd(h, r, t) = ||h\u22a5 + r\u2212 t\u22a5|| 2 2,\n(9)\nwhere wr \u2208 Rd is the normal vector of the hyperplane with unit length (i.e., wr \u00b7 r = 0 and ||wr||2 = 1).\nOur proposed ProjectNet model differs from TransH in two ways: (i) we adopt different projections to head and tail entities; (ii) we adopt general projection matrices rather than a hyperplane based projection. Actually, TransH (9) can be regarded as a special case of ProjectNet (5), as shown below. Starting from (9), we have\nh\u22a5 = h\u2212wr Thwr = h\u2212wrwr Th = (I \u2212wrwr T )h.\n(10) Hence, by substituting Lr = (I \u2212wrwrT ) in (3)(4), we get TransH. We still need to check whether Lr = (I \u2212wrwrT ) can be written in the form of (8), i.e., the weighted sum of\nmL rank-1 matrices, where mL < d. We answer this question in the following two steps: (i) As Lr = (I \u2212 wrwrT ) is an idempotent matrix (i.e. LrLr = Lr) and wr is a unit length vector, it holds that rank(Lr) = trace(Lr) = d \u2212 1 [10]. Therefore, Lr has d \u2212 1 non-zero eigenvalues. Furthermore, by observing that the eigenvalues of wrwrT are 0 and 1, we can conclude that Lr = (I \u2212 wrwrT ) has 1 as one of its eigenvalues, corresponding to d \u2212 1 linearly independent eigenvectors, and 0 as its another eigenvalue, corresponding to one eigenvector. (ii) Further considering that Lr is a real symmetric matrix, we can decompose Lr as Lr = Ur\u03a3rU T r , where Ur = (u (1) r ,u (2) r , \u00b7 \u00b7 \u00b7 ,u (d) r ) \u2208 Rd\u00d7d and \u03a3r = diag(1, 1, \u00b7 \u00b7 \u00b7 , 1, 0) \u2208 Rd\u00d7d. The first d \u2212 1 columns {u(i)r }d\u22121i=1 of Ur are all the unit-length eigenvectors of Lr corresponding to eigenvalue 1 and \u03a3r stores all the eigenvalues of Lr. Thus we can write Lr = \u2211d\u22121 i=1 u (i) r u (i) r T\n. The same procedure holds for the relation betweenRrt and t\u22a5. Then according to the above discussions, we can obtain the following proposition. Proposition 3.2 In the knowledge model of ProjectNet (8)(4), letting mL = mR = d \u2212 1, \u00b5 (i) r = \u03b6 (i) r = 1 and p(i)r = q (i) r = o (i) r = s (i) r = u (i) r , where u (i) r is the ith eigenvector of the matrix I \u2212 wrwrT with unit length, i = 1, 2, \u00b7 \u00b7 \u00b7 , d\u2212 1, we can obtain the TransH model (9).\nSE. SE [5] adopts the following scoring function: fd(h, r, t) = ||Lrh\u2212Rrt||1. (11)\nSE looks very similar to our proposed knowledge model. However, there is a key difference: SE does not add the lowrank constraints to the matrices Lr and Rr. In other words, they fix the rank of these two matrices to be full, while in our model the rank of the matrices is a variable. Therefore our model is more general than SE and can handle the non one-to-one relationship when the rank is low while SE cannot since its rank is always full. In this sense, we could also regard SE as a special case of our proposed ProjectNet model.\nTransR. [13] TransR treats relationships and entities as different objects and thus separates their embeddings into different spaces,\nfd(h, r, t) = ||Mrh+ r\u2212Mrt|| 2 2. (12)\nDifferent with our formulation (8)(5), they did not add the low-rank constraint to matrix Mr (or we say that it sets the matrix Mr to be full-rank). In addition, TransR adopts the same transformation matrices to head and tail entities, by assuming that they are located in the same space. Therefore, the knowledge model in our ProjectNet algorithm is more general than TransR, and can include it as our special case."}, {"heading": "4 Experiments", "text": "In this section, we conduct a set of experiments to verify the effectiveness of the ProjectNet model."}, {"heading": "4.1 Experiments Setup", "text": ""}, {"heading": "Training Data", "text": "For the free text corpus, we used a public snapshot of English Wikipedia named enwik9.2 The corpus contains about 120\n2http://mattmahoney.net/dc/enwik9.zip\nmillion word tokens. We removed digital words and words with frequency less than 5. Then we leveraged a knowledge graph FB13 [19] to impose relationships onto those entities covered by enwik9. Since FB13 contains many entities whose names have multiple words, in enwik9 we merged these words into phrases and regarded both single words and phrases as embedding units in the dictionary. Finally the dictionary size is about 230k."}, {"heading": "Baseline Methods", "text": "We consider the following algorithms as our baselines (we used the codes released by the authors of these works for implementation):\n1. Skip-Gram (SG): the original Skip-Gram model in word2vec, corresponding to \u03b1 = 0 in (1).\n2. RNet: the joint embedding model in [23] and [21], which adopts the objective min ||h + r \u2212 t||22 in the knowledge model.3\n3. Skip-Gram+TransH (SG+TransH): the combination of Skip-Gram (for the text model) and TransH (for the knowledge model). According to the discussions in Section 3.3, this baseline is a special case of ProjectNet."}, {"heading": "Parameter Setting", "text": "In our experiments, we set the embedding size to d = 100. Stochastic Gradient Descent(SGD) is used to train all the models. We set the initial learning rate to be 0.025 and linearly dropped it during the training process. For the knowledge model in ProjectNet, we initialized the projection matrices Lr and Rr to be diagonal matrices with randomly assigned 0, 1 elements (with mL and mR non-zero elements respectively). For mL and mR, we varied their values according to the set {10, 20, \u00b7 \u00b7 \u00b7 , 80, 90, 95, 100}. For all the joint embedding models, we varied the trade-off parameter \u03b1 in (1) according to the set {0.01, 0.05, 0.1, 0.2, 0.5}. The margin value is set to \u03b3 = 1.\nWe used two tasks to evaluation our algorithm and the baseline models, one is the analogical reasoning task and the other is the word similarity task. The corresponding experiments results are shown in the following two subsections."}, {"heading": "4.2 Analogical Reasoning Task", "text": "The analogical reasoning task is a word relationship inference task proposed in [16]. It consists of several quadruple word questions a:b,c:d, in which the relationship between word a and b is the same as that between c and d. For instance, (a : b, c : d) = (Berlin : Germany, Paris : France) and the relationship r is capital-countries. The task aims to infer word d given words a, b, and c using their word embedding vectors. To be more concrete, the inferred word d\u0302 is given by d\u0302 = argmaxw\u2208V cosine(b \u2212 a + c,w). Once d\u0302 = d, the result on this quadruple word question is right; otherwise, it is wrong.\n3As aforementioned, the models in the two papers differ only in the loss function (i.e., ranking loss vs. approximate softmax loss). Hence, we unify these two models using the name RNet and report the better performance of the two loss functions.\nTo construct the test set for the analogical reasoning task, we randomly sampled 1% triples from FB13, and filtered them according to the dictionary of enwik9.4 This test set consists of about 20k questions belonging to 7 non one-toone relationships. The detailed statistics for this test dataset can be found in Table 2.\nThen, we went through the remaining triples in FB13 and removed all those triples containing overlapped entities with the test data. In this way, we obtained a training set with roughly 76k triples, which has no overlap with the test set in either relationship triples or entities. The goal of doing so is to examine whether the free text corpus can act as a bridge between known and unknown entities, so as to verify the necessity of jointly embedding text and knowledge into the distributed representation space.\nFor ProjectNet, as we imposed Lra \u2212 Rrb = \u2212r = Lrc \u2212 Rrd instead of a \u2212 b = c \u2212 d, we took a twostep approach instead of directly using the original word vectors to perform the analogical reasoning task: (i) we chose an optimal relationship r\u2217 that best describes the relationship between a and b, i.e., r\u2217 = argminr ||Lra + r \u2212 Rrb||22; (ii) under r\u2217, we chose the answer word d\u0302 according to d\u0302 = argminw\u2208V ||Lr\u2217c + r \u2217 \u2212 Rr\u2217w|| 2 2. The same evaluation method was also applied to SG+TransH as well. The experimental results are listed in Table 2, from which we have the following observations:\n\u2022 All the knowledge based models (RNet, SG+TransH, and ProjectNet) outperform the original SG model, indicating that the quality of word embedding can be improved by leveraging knowledge graphs.\n\u2022 The two models that take non one-to-one mappings into consideration (i.e., SG+TransH and ProjecNet) are superior to RNet, showing the necessity of modeling the non one-to-one mappings into the loss functions.\n\u2022 Among all the models, ProjectNet achieves the best performance in all the seven subtasks. For the overall accuracy, it achieves over 30% relative gain than SG+TransH. This well demonstrates the advantages of our proposed model."}, {"heading": "Sensitivity to different ranks", "text": "The best performance of ProjectNet was obtained with \u03b1 = 0.2, mL = 50, and mR = 90. To show the influence of the ranks of the projection matrices, in Figure 1, we plotted two curves that reflect the performance of ProjectNet w.r.t. different rank values mL and mR: one curve corresponds to changing mR while fixing mL = 50, and the other corresponds to changing mL while fixing mR = 90. From the figure, we have the following observations. (i) The performance becomes bad when the rank is too low. This is because in this case the model expressiveness becomes poor due to small number of free parameters in the projection matrices. (ii) For the projection matrix for head entities, medium values of mL correspond to the better performances (the dashdot line),\n4We did not use the analogical reasoning dataset given in [16] because this dataset is too special in the sense that almost all the relationships in it are one-to-one mappings.\nwhile for the projection matrix for tail entities, higher values of mR lead to better performances (the solid line). This result is consistent with the statistical information in Table 1: the degree of non one-to-one mappings for head entities is much higher than that for tail entities, suggesting a lower rank of projection matrix for head entities."}, {"heading": "4.3 Word Similarity Task", "text": "Word similarity is a task to investigate whether the similarity computed from word embedding vectors is consistent with human-labeled word similarity. We used three wordsimilarity tasks in our experiments, namely Word Similarity 353 (WS353) [9], SCWS [11] and Rare Word (RW) [14]. There are 353, 2003, and 2034 word pairs in these datasets respectively. From the word embedding vectors, we obtain the similarity scores (e.g., cosine similarity) for each word pair, based on which a ranked list is derived on the word pairs. Then the generated ranked list is compared to the ranked list produced by the ground-truth similarity scores assigned by human labelers. To evaluate the consistency between two ranking lists, we used Spearman\u2019s Rank Correlation (denoted as \u03c1 \u2208 [\u22121, 1]). Higher \u03c1 corresponds to better word embedding vectors.\nTable 3 summarizes the results. For ProjectNet and SG+TransH, the word embedding vectors were directly used to compute the similarity scores, which is different from the analogical reasoning task. This is because there is no explicit relationship available in the evaluation process. The best performances of ProjectNet on the three datasets were obtained with the parameters setting to (mL = 50,mR = 95, \u03b1 = 0.05), (mL = 40,mR = 90, \u03b1 = 0.01), and (mL =\n60,mR = 95, \u03b1 = 0.05) respectively. Table 3 reveals that ProjetNet achieves the best performance on all the datasets, which further indicates that ProjetNet produce higher quality word embedding vectors than the baseline methods."}, {"heading": "5 Conclusions and Future Work", "text": "In this paper we proposed a novel word embedding algorithm called ProjetNet, which leverages knowledge graphs to improve the quality of word embedding. In ProjetNet, we adopt different asymmetric low rank projections to head and tail entities in an entity-relationship triple, thus successfully maintain both non one-to-one mapping and heterogenous head/tail entities properties of knowledge graph. Experimental results demonstrate that ProjetNet significantly outperforms previous embedding models.\nFor the future work, we plan to apply the proposed approach to fulfill knowledge mining tasks, such as triplet classification and link prediction [22]. In addition, we plan to use the word embedding vectors generated by ProjectNet in some real-world applications such as document classification and web search ranking."}], "references": [{"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Knowledgepowered deep learning for word embedding", "author": ["Jiang Bian", "Bin Gao", "Tie-Yan Liu"], "venue": "In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia- Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Antoine Bordes", "Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": "In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Compositional Morphology for Word Representations and Language Modelling", "author": ["Jan A. Botha", "Phil Blunsom"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML), Beijing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Learning effective word embedding using morphological word similarity", "author": ["Qing Cui", "Bin Gao", "Jiang Bian", "Siyu Qiu", "Tie-Yan Liu"], "venue": "CoRR, abs/1407.1687,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Matrix Analysis", "author": ["R.A. Horn", "C.R. Johnson"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1990}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "A latent factor model for highly multi-relational data", "author": ["Rodolphe Jenatton", "Nicolas L Roux", "Antoine Bordes", "Guillaume R Obozinski"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "learning entity and relation embeddings for knowledge graph completion", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"], "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh-Thang Luong", "Richard Socher", "C Manning"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Distributed representations of  words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1995}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Word representations: a simple and general method for semisupervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Knowledge graph and text jointly embedding", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Rc-net: A general framework for incorporating knowledge into word representations", "author": ["Chang Xu", "Yalong Bai", "Jiang Bian", "Bin Gao", "Gang Wang", "Xiaoguang Liu", "Tie-Yan Liu"], "venue": "In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Mo Yu", "Mark Dredze"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "To tackle this problem, recently some researchers have proposed to leverage knowledge graphs, such as WordNet [17] and Freebase [3], as additional data sources to improve word embedding [2; 21; 23].", "startOffset": 110, "endOffset": 114}, {"referenceID": 2, "context": "To tackle this problem, recently some researchers have proposed to leverage knowledge graphs, such as WordNet [17] and Freebase [3], as additional data sources to improve word embedding [2; 21; 23].", "startOffset": 128, "endOffset": 131}, {"referenceID": 18, "context": "Take a widely used benchmark dataset FB13 [19], which is a subset of Freebase, as an instance.", "startOffset": 42, "endOffset": 46}, {"referenceID": 21, "context": "For example, in [22], it is proposed to project the embedding vectors of both entities onto a relation-dependent hyperplane before computing the loss function LK.", "startOffset": 16, "endOffset": 20}, {"referenceID": 21, "context": "Furthermore, the projection matrix used in [22] has a fixed rank for all types of relationships, which could not express various degrees of non one-to-one mappings.", "startOffset": 43, "endOffset": 47}, {"referenceID": 4, "context": "In [5], different transformations are adopted to head and tail entities respectively, however, no consideration is taken to address the issue of non one-to-one mappings.", "startOffset": 3, "endOffset": 6}, {"referenceID": 21, "context": "Actually, it can be proven that the TransH model in [22] is our special case in the sense that it also adopts a projection matrix of low (and fixed) rank.", "startOffset": 52, "endOffset": 56}, {"referenceID": 23, "context": "In [24], the authors used semantic relational knowledge between words as a constraint in learning word embedding vectors.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "In [23], the authors leveraged knowledge graphs, the most widely used structured knowledge, to help improve word representations.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "[21], the authors proposed a very similar method to [23], but with a different objective of improving knowledge graph understanding with the help of text corpus.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[21], the authors proposed a very similar method to [23], but with a different objective of improving knowledge graph understanding with the help of text corpus.", "startOffset": 52, "endOffset": 56}, {"referenceID": 22, "context": "Actually, both the models in [23] and [21] are inspired by the TransE model [4], which is a state-of-the-art work in the literature of computing distributed representations for knowledge graphs [5; 12; 19].", "startOffset": 29, "endOffset": 33}, {"referenceID": 20, "context": "Actually, both the models in [23] and [21] are inspired by the TransE model [4], which is a state-of-the-art work in the literature of computing distributed representations for knowledge graphs [5; 12; 19].", "startOffset": 38, "endOffset": 42}, {"referenceID": 3, "context": "Actually, both the models in [23] and [21] are inspired by the TransE model [4], which is a state-of-the-art work in the literature of computing distributed representations for knowledge graphs [5; 12; 19].", "startOffset": 76, "endOffset": 79}, {"referenceID": 21, "context": "To tackle the problem, in [22], the authors proposed a simple projection method named TransH.", "startOffset": 26, "endOffset": 30}, {"referenceID": 0, "context": "where \u03b1 \u2208 [0, 1] is used to trade off the two loss terms.", "startOffset": 10, "endOffset": 16}, {"referenceID": 15, "context": "Similar to [23; 21], we leverage the Skip-Gram model [16] as the text model.", "startOffset": 53, "endOffset": 57}, {"referenceID": 15, "context": "P (wO|wI), we adopt the negative sampling strategy [16] to boost the computation efficiency.", "startOffset": 51, "endOffset": 55}, {"referenceID": 21, "context": "Therefore, we argue that it is unreasonable to adopt the same projection to these two kinds of entities (as TransH [22] does).", "startOffset": 115, "endOffset": 119}, {"referenceID": 22, "context": "RNet refers to the knowledge models proposed in [23] and [21].", "startOffset": 48, "endOffset": 52}, {"referenceID": 20, "context": "RNet refers to the knowledge models proposed in [23] and [21].", "startOffset": 57, "endOffset": 61}, {"referenceID": 22, "context": "In [23], a large margin ranking loss is adopted for the minimization of fd(h, r, t), whereas in [21], an approximate softmax loss is used.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "In [23], a large margin ranking loss is adopted for the minimization of fd(h, r, t), whereas in [21], an approximate softmax loss is used.", "startOffset": 96, "endOffset": 100}, {"referenceID": 21, "context": "TransH [22] is proposed to overcome the non oneto-one mapping problem.", "startOffset": 7, "endOffset": 11}, {"referenceID": 9, "context": "LrLr = Lr) and wr is a unit length vector, it holds that rank(Lr) = trace(Lr) = d \u2212 1 [10].", "startOffset": 86, "endOffset": 90}, {"referenceID": 4, "context": "SE [5] adopts the following scoring function:", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "[13] TransR treats relationships and entities as different objects and thus separates their embeddings into different spaces, fd(h, r, t) = ||Mrh+ r\u2212Mrt|| 2 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Then we leveraged a knowledge graph FB13 [19] to impose relationships onto those entities covered by enwik9.", "startOffset": 41, "endOffset": 45}, {"referenceID": 22, "context": "RNet: the joint embedding model in [23] and [21], which adopts the objective min ||h + r \u2212 t||2 in the knowledge model.", "startOffset": 35, "endOffset": 39}, {"referenceID": 20, "context": "RNet: the joint embedding model in [23] and [21], which adopts the objective min ||h + r \u2212 t||2 in the knowledge model.", "startOffset": 44, "endOffset": 48}, {"referenceID": 15, "context": "The analogical reasoning task is a word relationship inference task proposed in [16].", "startOffset": 80, "endOffset": 84}, {"referenceID": 15, "context": "We did not use the analogical reasoning dataset given in [16] because this dataset is too special in the sense that almost all the relationships in it are one-to-one mappings.", "startOffset": 57, "endOffset": 61}, {"referenceID": 8, "context": "We used three wordsimilarity tasks in our experiments, namely Word Similarity 353 (WS353) [9], SCWS [11] and Rare Word (RW) [14].", "startOffset": 90, "endOffset": 93}, {"referenceID": 10, "context": "We used three wordsimilarity tasks in our experiments, namely Word Similarity 353 (WS353) [9], SCWS [11] and Rare Word (RW) [14].", "startOffset": 100, "endOffset": 104}, {"referenceID": 13, "context": "We used three wordsimilarity tasks in our experiments, namely Word Similarity 353 (WS353) [9], SCWS [11] and Rare Word (RW) [14].", "startOffset": 124, "endOffset": 128}, {"referenceID": 21, "context": "For the future work, we plan to apply the proposed approach to fulfill knowledge mining tasks, such as triplet classification and link prediction [22].", "startOffset": 146, "endOffset": 150}], "year": 2015, "abstractText": "Word embedding, which refers to low-dimensional dense vector representations of natural words, has demonstrated its power in many natural language processing tasks. However, it may suffer from the inaccurate and incomplete information contained in the free text corpus as training data. To tackle this challenge, there have been quite a few works that leverage knowledge graphs as an additional information source to improve the quality of word embedding. Although these works have achieved certain success, they have neglected some important facts about knowledge graphs: (i) many relationships in knowledge graphs are many-to-one, oneto-many or even many-to-many, rather than simply one-to-one; (ii) most head entities and tail entities in knowledge graphs come from very different semantic spaces. To address these issues, in this paper, we propose a new algorithm named ProjectNet. ProjecNet models the relationships between head and tail entities after transforming them with different low-rank projection matrices. The low-rank projection can allow non one-to-one relationships between entities, while different projection matrices for head and tail entities allow them to originate in different semantic spaces. The experimental results demonstrate that ProjectNet yields more accurate word embedding than previous works, thus leads to clear improvements in various natural language processing tasks.", "creator": "LaTeX with hyperref package"}}}