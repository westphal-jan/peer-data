{"id": "1505.05114", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2015", "title": "Solving Random Quadratic Systems of Equations Is Nearly as Easy as Solving Linear Systems", "abstract": "We consider the fundamental problem of solving quadratic systems of equations in $n$ variables, where $y_i = |\\langle \\boldsymbol{a}_i, \\boldsymbol{x} \\rangle|^2$, $i = 1, \\ldots, m$ and $\\boldsymbol{x} \\in \\mathbb{R}^n$ is unknown. We propose a novel method, which starting with an initial guess computed by means of a spectral method, proceeds by minimizing a nonconvex functional as in the Wirtinger flow approach. There are several key distinguishing features, most notably, a distinct objective functional and novel update rules, which operate in an adaptive fashion and drop terms bearing too much influence on the search direction. These careful selection rules provide a tighter initial guess, better descent directions, and thus enhanced practical performance. On the theoretical side, we prove that for certain unstructured models of quadratic systems, our algorithms return the correct solution in linear time, i.e. in time proportional to reading the data $\\{\\boldsymbol{a}_i\\}$ and $\\{y_i\\}$ as soon as the ratio $m/n$ between the number of equations and unknowns exceeds a fixed numerical constant. We extend the theory to deal with noisy systems in which we only have $y_i \\approx |\\langle \\boldsymbol{a}_i, \\boldsymbol{x} \\rangle|^2$ and prove that our algorithms achieve a statistical accuracy, which is nearly un-improvable. We complement our theoretical study with numerical examples showing that solving random quadratic systems is both computationally and statistically not much harder than solving linear systems of the same size---hence the title of this paper. For instance, we demonstrate empirically that the computational cost of our algorithm is about four times that of solving a least-squares problem of the same size. For example, with a constant of $n^n$ which is equivalent to $n^n$, we demonstrate that solving a simple linear problem of a higher magnitude is difficult. However, in theory, this means that only the numerical coefficients that come up with computationally are given by the computer and hence are not always represented as the same.\n\n\n\n\n\nThe problem of solving the problem of solving the problem of solving the problem of solving the problem of solving the problem of solving the problem of", "histories": [["v1", "Tue, 19 May 2015 18:37:07 GMT  (3124kb,D)", "http://arxiv.org/abs/1505.05114v1", null], ["v2", "Tue, 22 Mar 2016 17:05:16 GMT  (3658kb,D)", "http://arxiv.org/abs/1505.05114v2", "accepted to Communications on Pure and Applied Mathematics (CPAM)"]], "reviews": [], "SUBJECTS": "cs.IT cs.LG math.IT math.NA math.ST stat.ML stat.TH", "authors": ["yuxin chen", "emmanuel j cand\u00e8s"], "accepted": true, "id": "1505.05114"}, "pdf": {"name": "1505.05114.pdf", "metadata": {"source": "CRF", "title": "Solving Random Quadratic Systems of Equations Is Nearly as Easy as Solving Linear Systems", "authors": ["Yuxin Chen", "Emmanuel J. Cand\u00e8s"], "emails": [], "sections": [{"heading": "1 Introduction", "text": ""}, {"heading": "1.1 Problem formulation", "text": "Imagine we are given a set of m quadratic equations taking the form\nyi = |\u3008ai,x\u3009|2 , i = 1, \u00b7 \u00b7 \u00b7 ,m, (1)\nwhere the data y = [yi]1\u2264i\u2264m and design vectors ai \u2208 Rn/Cn are known whereas x \u2208 Rn/Cn is unknown. Having information about |\u3008ai,x\u3009|2\u2014or, equivalently, |\u3008ai,x\u3009|\u2014means that we a priori know nothing about the phases or signs of the linear products \u3008ai,x\u3009. The problem is this: can we hope to identify a solution, if any, compatible with this nonlinear system of equations?\nThis problem is combinatorial in nature as one can alternatively pose it as recovering the missing signs of \u3008ai,x\u3009 from magnitude-only observations. As is well known, many classical combinatorial problems with Boolean variables may be cast as special instances of (1). As an example, consider the NP-complete stone problem in which we have n stones each of weight wi > 0 (1 \u2264 i \u2264 n), which we would like to divide into two groups of equal sum weight. Letting xi \u2208 {\u22121, 1} indicate which of the two groups the ith stone belongs to, one can formulate this problem as solving the quadratic system{\nx2i = 1, i = 1, \u00b7 \u00b7 \u00b7 , n, (w1x1 + \u00b7 \u00b7 \u00b7+ wnxn)2 = 0.\n(2)\n\u2217Department of Statistics, Stanford University, Stanford, CA 94305, U.S.A. \u2020Department of Mathematics, Stanford University, Stanford, CA 94305, U.S.A.\nar X\niv :1\n50 5.\n05 11\n4v 1\n[ cs\n.I T\n] 1\n9 M\nay 2\n01 5\nHowever simple this formulation may seem, even checking whether a solution to (2) exists or not is known to be NP complete [7].\nMoving from combinatorial optimization to the physical sciences, one application of paramount importance is the phase retrieval [20, 21] problem, which permeates through a wide spectrum of techniques including X-ray crystallography, diffraction imaging, and microscopy. In a nutshell, the problem of phase retrieval arises due to the physical limitation of optical sensors, which are often only able to record the intensities of the diffracted waves scattered by an object under study. Notably, upon illuminating an object x, the diffraction pattern is of the form of Ax; however, it is only possible to obtain intensity measurements y = |Ax|2 leading to the quadratic system (1).1 In the Fraunhofer regime where data is collected in the far-field zone, A is given by the spatial Fourier transform. We refer to [37] for in-depth reviews of this subject.\nContinuing this motivating line of thought, in any real-world application recorded intensities are always corrupted by at least a small amount of noise so that observed data are only about |\u3008ai,x\u3009|2; i.e.\nyi \u2248 |\u3008ai,x\u3009|2 , i = 1, \u00b7 \u00b7 \u00b7 ,m. (3)\nAlthough we present results for arbitrary noise distributions\u2014even for non-stochastic noise\u2014we shall pay a particular attention to the Poisson data model, which assumes\nyi ind.\u223c Poisson ( |\u3008ai,x\u3009|2 ) , i = 1, \u00b7 \u00b7 \u00b7 ,m. (4)\nThe reason why this statistical model is of special interest is that it naturally describes the variation in the number of photons detected by an optical sensor in various imaging applications."}, {"heading": "1.2 Nonconvex optimization", "text": "Under a stochastic noise model with independent samples, a first impulse for solving (3) is to seek the maximum likelihood estimate (MLE), namely,\nminimizez \u2212 \u2211m\ni=1 ` (z; yi) , (5)\nwhere ` (z; yi) denotes the log-likelihood of a candidate solution z given the outcome yi. For instance, under the Poisson noise model (4) one can write\n`(z; yi) = yi log(|a\u2217i z|2)\u2212 |a\u2217i z|2 (6)\nmodulo some constant offset. Unfortunately, the log-likelihood is usually not concave, thus making the problem of finding an MLE NP complete in general.\nTo alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45]. The basic idea is to introduce a rank-one matrix X = xx\u2217 to linearize the quadratic constraints, and then relax the rank-one constraint. Suppose we have Poisson data, then this strategy converts the problem into a convenient convex program:\nminimizeX \u2211m i=1(\u00b5 2 i \u2212 yi log\u00b5i) + \u03bbTr(X) subject to \u00b5i = a>i Xai, 1 \u2264 i \u2264 m, X 0.\nNote that the likelihood function is augmented by the trace functional whose role is to promote low-rank solutions. While such convex relaxation schemes enjoy intriguing performance guarantees in many aspects in the sense that they require near-optimal sample complexity and achieve near-optimal error bounds for certain noise models, the computational cost typically far exceeds the order of n3. This limits applicability to large-dimensional data.\nThis paper follows another route: rather than lifting the problem into higher dimensions by introducing matrix variables, this paradigm maintains its iterates within the vector domain and optimize the nonconvex\n1Here and below, for z \u2208 Cn, |z| (resp. |z|2) is the vector of magnitudes (resp. squared magnitudes).\nobjective directly (e.g. [11, 19, 21, 30, 32, 35, 36]). One promising approach along this line is the recently proposed two-stage algorithm called Wirtinger Flow (WF) [11]. Simply put, WF starts by computing a suitable initial guess z(0) using a spectral method, and then successively refines the estimate via an update rule that bears a strong resemblance to a gradient descent scheme, namely,\nz(t+1) = z(t) + \u00b5t m \u2211m i=1 \u2207`(z(t); yi),\nwhere z(t) denotes the tth iterate of the algorithm, and \u00b5t is the step size (or learning rate). Here, \u2207`(z; yi) stands for the Wirtinger derivative w.r.t. z, which in the real-valued case reduces to the ordinary gradient. The main results of [11] demonstrate that WF is surprisingly accurate for both real-valued and complexvalued Gaussian sampling models. Specifically, when ai \u223c N (0, I) or ai \u223c N (0, I) + jN (0, I):\n1. WF achieves exact recovery from m = O (n log n) quadratic equations when there is no noise;2\n2. WF attains -accuracy\u2014in a relative sense\u2014within O(mn2 log 1/ ) time (or flops);\n3. In the presence of Gaussian noise, WF is stable and converges to the MLE as shown in [39].\nWhile these results formalize the advantages of WF, the computational cost of WF is still larger than the best one can hope for. Moreover, the statistical guarantee in terms of the sample complexity is weaker than that achievable by convex relaxations.3"}, {"heading": "1.3 This paper: Truncated Wirtinger Flow", "text": "This paper develops a novel linear-time algorithm, which also enjoys near-perfect statistical guarantees. Following the spirit of WF, we propose a novel procedure called Truncated Wirtinger Flow (TWF) adopting a more subtle gradient flow. Informally, TWF proceeds in two stages:\n1. Initialization: compute an initial guess z(0) by means of a spectral method applied to a subset T0 of the observations {yi};\n2. Loop: for 0 \u2264 t < T , z(t+1) = z(t) +\n\u00b5t m \u2211 i\u2208Tt+1 \u2207`(z(t); yi) (7)\nfor some index set Tt+1 \u2286 {1, \u00b7 \u00b7 \u00b7 ,m} determined by z(t).\nThree remarks are in order.\n\u2022 Firstly, the index set Tt+1 is data-dependent and iteration-varying, which is a distinguishing feature of TWF in comparison to WF. In words, Tt+1 corresponds to those equations whose resulting gradient components (i.e. \u2207`(z(t); yi)) are in some sense not excessively large; see Sections 2 and 3. As we shall see later, the main point is that this truncation gives us a better search direction.\n\u2022 Secondly, we recommend that the step size \u00b5t is either taken as some appropriate constant or determined by a backtracking line search. For instance, under appropriate conditions, we can take \u00b5t = 0.2.\n\u2022 Finally, the most expensive part of the gradient stage consists in computing \u2207`(z; yi), 1 \u2264 i \u2264 m, which can often be performed in an efficient manner. More concretely, under the real-valued Poisson data model (4) one has\n\u2207`(z; yi) = 2 {\nyi |a>i z|2 aia > i z \u2212 aia>i z\n} = 2 ( yi \u2212 |a>i z|2\na>i z\n) ai, 1 \u2264 i \u2264 m.\n2 The standard notation f(n) = O (g(n)) or f(n) . g(n) (resp. f(n) = \u2126 (g(n)) or f(n) & g(n)) means that there exists a constant c > 0 such that |f(n)| \u2264 c|g(n)| (resp. |f(n)| \u2265 c |g(n)|). f(n) g(n) means that there exist constants c1, c2 > 0 such that c1|g(n)| \u2264 |f(n)| \u2264 c2|g(n)|.\n3 M. Soltanolkotabi recently informed us that the sample complexity of WF may be improved if one employs a better initialization procedure.\nThus, calculating {\u2207`(z; yi)} essentially amounts to two matrix-vector products. LettingA := [a1, \u00b7 \u00b7 \u00b7 ,am]> as before, we have\n\u2211 i\u2208Tt+1\n\u2207`(z(t); yi) = A>v, vi = { 2 yi\u2212|a>i z| 2 a>i z , i \u2208 Tt+1,\n0, otherwise.\nHence, Az gives v and A>v the desired truncated gradient.\nA detailed specification of the algorithm is deferred to Section 2."}, {"heading": "1.4 Numerical surprises", "text": "To give the readers a sense of the practical power of TWF, we present here three illustrative numerical examples. Since it is impossible to recover the global sign\u2014i.e. we cannot distinguish x from \u2212x\u2014we will evaluate our solutions to the quadratic equations through the distance measure put forth in [11] representing the Euclidean distance modulo a global sign: for complex signals,\ndist (z,x) := min\u03d5:\u2208[0,2\u03c0) \u2016e\u2212j\u03d5z \u2212 x\u2016. (8)\nwhile it is simply min \u2016z\u00b1x\u2016 in the real case. We shall use dist(x\u0302,x)/\u2016x\u2016 throughout to denote the relative erorr of an estimate x\u0302. In the sequel, TWF proceeds assuming a Poisson log-likelihood (6). Standalone Matlab implementations of TWF are available at http://web.stanford.edu/~yxchen/TWF/ (see [40] for straight WF implementations).\nThe first numerical example concerns the following two problems under noise-free real-valued data:\n(a) find x \u2208 Rn s.t. bi = a>i x, 1 \u2264 i \u2264 m; (b) find x \u2208 Rn s.t. bi = |a>i x|, 1 \u2264 i \u2264 m.\nApparently, (a) is tantamount to solving a linear least-squares problem, while (b) involves solving a set of quadratic equations. Arguably the most popular method for solving large-scale least squares problems is the conjugate gradient (CG) method [34] applied to the normal equations. We are going to compare the computational efficiency between CG (for solving least squares) and TWF with a step size \u00b5t \u2261 0.2 (for solving a quadratic system). Set m = 8n and generate x \u223c N (0, I) and ai \u223c N (0, I), 1 \u2264 i \u2264 m, independently. This gives a matrix A>A with clustered eigenvalues and a low condition number equal to about (1 + \u221a 1/8)2/(1\u2212 \u221a 1/8)2 \u2248 4.38 by the Marchenko-Pastur law. Therefore, this is an ideal setting for CG as it converges extremely rapidly [42, Theorem 38.5]. Fig. 1 shows the relative estimation error of each method as a function of the iteration count, where TWF is seeded through 10 power iterations. For ease of comparison, we illustrate the iteration counts in different scales so that 4 TWF iterations are equivalent to 1 CG iteration.\nRecognizing that each iteration of CG and TWF involves two matrix vector products Az and A>v, for such a design we reach a suprising observation:\nEven when all phase information is missing, TWF is capable of solving a quadratic system of equations only about 4 times slower than solving a least squares problem of the same size!\nTo illustrate the applicability of TWF on real images, we turn to testing our algorithm on a 320\u00d7 1280 digital photograph of Stanford main quad. We consider a type of measurements that falls under the category of coded diffraction patterns (CDP) [10] and set\ny(l) = |FD(l)x|2, 1 \u2264 l \u2264 L. (9)\nHere, F stands for a discrete Fourier transform (DFT) matrix, and D(l) is a diagonal matrix whose diagonal entries are independently and uniformly drawn from {1,\u22121, j,\u2212j} (phase delays). In phase retrieval, each D(l) represents a random mask placed after the object so as to modulate the illumination patterns. When L masks are employed, the total number of quadratic measurements is m = nL. In this example, L = 12 random coded patterns are generated to measure each color band (i.e. red, green, or blue) separately. The\nexperiment is carried out on a MacBook Pro equipped with a 3 GHz Intel Core i7 and 16GB of memory. We run 50 iterations of the truncated power method for initialization, and 50 truncated gradient iterations, which in total costs 43.9 seconds or 2400 FFTs for each color band. The relative errors after spectral initialization and after 50 TWF iterations are 0.4773 and 2.16\u00d710\u22125, respectively, with the recovered images displayed in Fig. 2. In comparison, WF with 50 power iterations and 100 gradient iterations (which takes 54.5 seconds or 3600 FFTs) returns an image of relative error 1.309, still extremely far from the truth.\nWhile the above experiments concern noiseless data, the numerical surprise extends to the noisy realm. Suppose the data are drawn according to the Poisson noise model (4), with ai \u223c N (0, I) independently generated. Fig. 3 displays the empirical relative mean-square error (MSE) of TWF as a function of the signal-to-noise ratio (SNR), where the relative MSE for an estimate x\u0302 and the SNR are defined as4\nMSE := dist2(x\u0302,x) \u2016x\u20162 , and SNR := 3\u2016x\u2016 2. (10)\nBoth SNR and MSE are displayed on a dB scale (i.e. the values of 10 log10(SNR) and 10 log10(rel. MSE) are plotted). To evaluate the accuracy of the TWF solutions, we consider the performance achieved by MLE applied to an ideal problem in which the true phases are revealed. In this ideal scenario, in addition to the data {yi} we are further given exact phase information {\u03d5i = sign(a>i x)}. Such precious information gives away the phase retrieval problem and makes the MLE efficiently solvable since the MLE problem with side information\nminimizez\u2208Rn \u2212 \u2211m i=1 yi log ( |a>i z|2 ) + (a>i z) 2 subject to \u03d5i = sign(a>i z)\ncan be cast as a convex program\nminimizez\u2208Rn \u2212 \u2211m\ni=1 2yi log\n( \u03d5ia > i z ) + (a>i z) 2.\nFig. 3 illustrates the empirical performance for this ideal problem. The plots demonstrate that even when all phases are erased, TWF yields a solution of nearly the best possible quality, since it only incurs an extra 1.5 dB loss compared to ideal MLE computed with all true phases revealed. This phenomenon arises regardless of the SNR!\nFor experiments with noisy complex-valued data and (untruncated) WF, please see [39]. 4To justify the definition of SNR, note that the signals and noise are captured by \u00b5i = (a>i x) 2 and yi \u2212 \u00b5i (1 \u2264 i \u2264 m),\nrespectively. The ratio of the signal power to the noise power is therefore \u2211m i=1 \u00b5 2 i\u2211m i=1 Var[yi] = \u2211m i=1 |a > i x| 4\u2211m i=1 |a > i x| 2 \u2248 3m\u2016x\u20164 m\u2016x\u20162 = 3\u2016x\u2016 2."}, {"heading": "1.5 Main results", "text": "The preceding numerical discoveries unveil promising features of TWF in three aspects: (1) exponentially fast convergence; (2) exact recovery from noiseless data with sample complexity O (n); (3) nearly minimal mean-square loss in the presence of noise. This paper offers a formal substantiation of all these findings. To this end, we assume a tractable model in which the design vectors ai\u2019s are independent Gaussian:\nai \u223c N (0, In) . (11)\nFor concreteness, our results are concerned with TWF based on the Poisson log-likelihood function\n`i(z) := ` (z; yi) = yi log ( |a>i z|2 ) \u2212 |a>i z|2, (12)\nwhere we shall use `i(z) as a shorthand for `(z; yi) from now on. We begin with the performance guarantees of TWF in the absence of noise.\nTheorem 1 (Exact recovery). Consider the noiseless case (1) with an arbitrary signal x \u2208 Rn. Suppose that the step size \u00b5t is either taken to be a positive constant \u00b5t \u2261 \u00b5 or chosen via a backtracking line search.\nThen there exist some universal constants 0 < \u03c1, \u03bd < 1 and \u00b50, c0, c1, c2 > 0 such that with probability exceeding 1\u2212 c1 exp (\u2212c2m), the truncated Wirtinger Flow estimates (Algorithm 1 with parameters specified in Table 1) obey\ndist(z(t),x) \u2264 \u03bd(1\u2212 \u03c1)t\u2016x\u2016, \u2200t \u2208 N, (13)\nprovided that m \u2265 c0n and \u00b5 \u2264 \u00b50.\nAs explained below, we can often take \u00b50 \u2248 0.3.\nRemark 1. As will be made precise in Section 5 (and in particular Proposition 1), one can take\n\u00b50 = 0.994\u2212 \u03b61 \u2212 \u03b62 \u2212\n\u221a 2/(9\u03c0)\u03b1\u22121h\n2 (1.02 + 0.665/\u03b1h)\nfor some small quantities \u03b61, \u03b62 and some truncation threshold \u03b1h that is usually taken to be \u03b1h \u2265 5. Under appropriate conditions, one can treat \u00b50 as\n\u00b50 \u2248 0.3. (14) Theorem 1 justifies at least two appealing features of TWF: (i) minimal sample complexity and (ii) linear-time complexity. Specifically, TWF allows exact recovery from O(n) quadratic equations, which is optimal since one needs at least n measurements to have a well posed problem. Also, because of the geometric convergence rate, TWF achieves -accuracy (i.e. dist(z(t),x) \u2264 \u2016x\u2016) within at most O (log 1/ ) iterations. The total computational cost is therefore O (mn log 1/ ), which is linear in the problem size. These outperform the provable guarantees of WF [11], which requires O(n log n) sample complexity and runs in O(mn2 log 1/ ) time.\nWe emphasize that enhanced performance vis-\u00e0-vis WF is not the result of a sharper analysis, but rather, the result of key algorithmic changes. In both the initialization and iterative refinement stages, TWF proceeds in a more prudent manner by means of proper truncation. In a nutshell, TWF operates only upon a subset of data whose contributions can be well controlled, thus guaranteeing better initial guess and descent directions. With these in place, we take the step size in a far more liberal fashion\u2014which is bounded away from 0\u2014compared to a step size which is inversely propotional to n as explained in [11]. In fact, what enables the movement to be more aggressive is exactly the cautious choice of Tt, which precludes adverse effects from atypical samples.\nTo be broadly applicable, the proposed algorithm must guarantee reasonably faithful estimates in the presence of noise. Suppose that yi = |\u3008ai,x\u3009|2 + \u03b7i, 1 \u2264 i \u2264 m, (15) where \u03b7i represents an error term. We claim that TWF is stable against additive noise, as demonstrated in the theorem below.\nTheorem 2 (Stability). Consider the noisy case (15). Suppose that the step size \u00b5t is either taken to be a positive constant \u00b5t \u2261 \u00b5 or chosen via a backtracking line search. If\nm \u2265 c0n, \u00b5 \u2264 \u00b50, and \u2016\u03b7\u2016\u221e \u2264 c1 \u2016x\u2016 2 , (16)\nthen with probability at least 1 \u2212 c2 exp (\u2212c3m), the truncated Wirtinger Flow estimates (Algorithm 1 with parameters specified in Table 1) satisfy\ndist(z(t),x) . \u2016\u03b7\u2016\u221a m\u2016x\u2016 + (1\u2212 \u03c1) t\u2016x\u2016, \u2200t \u2208 N (17)\nsimultanesouly for all x \u2208 Rn. Here, 0 < \u03c1 < 1 and \u00b50, c0, c1, c2, c3 > 0 are some universal constants. Under the Poisson noise model (4), there exists an an event of probability at least 1 \u2212 c2 exp(\u2212c3m) on which\nP { dist(z(t),x) . 1 + (1\u2212 \u03c1)t\u2016x\u2016, \u2200t \u2208 N \u2223\u2223\u2223 {ai}1\u2264i\u2264m}\u2192 1. (18)\nholds for all x \u2208 Rn satisfying \u2016x\u2016 \u2265 log1.5m.\nRemark 2. In the main text, we will prove Theorem 2 only for the case where x is fixed and independent of the design vectors {ai}. Interested readers are referred to the supplemental materials [13] for the proof of the universal theory (i.e. the case simultaneously accommodating all x \u2208 Rn). Note that when there is no noise (\u03b7 = 0), this stronger result guarantees the universality of the noiseless recovery. Remark 3. [39] proves similar stability estimates using the WF approach under Gaussian noise. (M. Soltanolkotabi has also informed us that his results extend to other noise models.) There, the sample and computational complexities are still on the order of n log n and mn2 respectively whereas the computational complexity in Theorem 2 is linear, i.e. on the order of mn.\nTheorem 2 essentially reveals that the estimation error of TWF rapidly shrinks to O ( \u2016\u03b7\u2016/ \u221a m\n\u2016x\u2016\n) within\nlogarithmic iterations. Put another way, since the SNR for the model (15) is captured by SNR := \u2211m i=1 |\u3008ai,x\u3009|4 \u2016\u03b7\u20162 \u2248 3m\u2016x\u20164 \u2016\u03b7\u20162 , (19)\nwe immediately arrive at an alternative form of performance guarantee:\ndist(z(t),x) . 1\u221a SNR \u2016x\u2016+ (1\u2212 \u03c1)t\u2016x\u2016, \u2200t \u2208 N, (20)\nrevealing the stability of TWF as a function of SNR. We emphasize that this estimate holds for any error term \u03b7\u2014i.e. any noise structure, even deterministic. This being said, specializing this estimate to the Poisson noise model (4) with \u2016x\u2016 & log1.5m gives an estimation error that will eventually approach a numerical constant, independent of n and m.\nEncouragingly, this is already the best statistical guarantee any algorithm can achieve. We formalize this claim by deriving a fundamental lower bound on the minimax estimation error.\nTheorem 3 (Lower bound on the minimax risk). Suppose that ai \u223c N (0, I), m = \u03ban for some fixed \u03ba independent of n, and n is sufficiently large. For any K \u2265 log1.5m, define5\n\u03a5(K) := {x \u2208 Rn | \u2016x\u2016 \u2208 (1\u00b1 0.1)K}. 5Here, 0.1 can be replaced by any positive constant within (0, 1/2).\nWith probability approaching one, the minimax risk under the Poisson model (4) obeys\ninf x\u0302 sup x\u2208\u03a5(K)\nE [ dist (x\u0302,x) \u2223\u2223 {ai}1\u2264i\u2264m] \u2265 \u03b51\u221a \u03ba , (21)\nwhere the infimum is over all estimator x\u0302. Here, \u03b51 > 0 is a numerical constant independent of n and m.\nWhen the number m of measurements is proportional to n and the energy of the planted solution exceeds log3m, Theorem 3 asserts that there exists absolutely no estimator that can achieve an estimation error that vanishes as n increases. This lower limit matches the estimation error of TWF, which corroborates the optimality of TWF under noisy data.\nCareful readers will naturally wonder whether the regime \u2016x\u2016 \u2265 log1.5m\u2014or \u2016x\u2016 \u2265 \u221an log1.5m if we normalize ai so that \u2016ai\u2016 \u2248 1\u2014is of practical importance. Note that in the optical imaging applications, the Poisson noise model employs x and y to describe the numbers of photons diffracted by the specimen and detected by the optical sensor, respectively. Each specimen under study needs to be sufficiently illuminated in order for the receiver to sense the diffracted light; this means that the number of photons hitting each pixel must be large on the average (typically much larger than log3m). The regime \u2016x\u2016 \u2264 log1.5m is thus of little practical interest as it basically corresponds to a black object.\nIt is worth noting that apart from WF, various other nonconvex procedures have been proposed as well for phase retrieval, including the error reduction schemes dating back to Gerchberg-Saxton and Fienup [20,21], iterated projections [19], alternating minimization [32], generalized approximate message passing [35], and greedy methods that exploit additional sparsity constraint [36], to name just a few. While these paradigms enjoy favorable empirical behavior, most of them fall short of theoretical support, except for a version of alternating minimization (called AltMinPhase) [32] that requires fresh samples for each iteration. In comparison, AltMinPhase attains -accuracy only when the sample complexity exceeds the order of n log3 n+ n log2 n log 1/ , which is at least a factor of log3 n from optimal. Furthermore, none of these algorithms come with provable stability guarantees, which are particularly important in most realistic scenarios. Interesting readers are referred to [11] for a comparison of these non-convex schemes, and [10] for a discussion of other alternative approaches (e.g. [1, 4]) and performance lower bounds (e.g. [5, 18]). On the other hand, the family of two-stage nonconvex procedures\u2014spectral initialization followed by iterative refinement\u2014has proved efficient for other problems that involve latent variables, which leads to theoretical guarantees for general EM algorithms [3] and sparse coding schemes [2]. The truncation idea proposed herein might promise improved performance for these problems as well."}, {"heading": "2 Algorithm: Truncated Wirtinger Flow", "text": "This section describes the two stages of truncated Wirtinger flow in details, presented in a reverse order. For each stage, we start with some algorithmic issues encountered by WF, which is then used to motivate and explain the basic principles of TWF. Here and throughout, we let A : Rn\u00d7n 7\u2192 Rm be the linear map\nM \u2208 Rn\u00d7n 7\u2192 A (M) := { a>i Mai } 1\u2264i\u2264m\nand A the design matrix A := [a1, \u00b7 \u00b7 \u00b7 ,am]>."}, {"heading": "2.1 Truncated gradient stage", "text": "For independent samples, the gradient of the Poisson log-likelihood for any z \u2208 Rn with a>i z 6= 0 for all i, obeys\nm\u2211 i=1 \u2207`i(z) = m\u2211 i=1 2 yi \u2212 |a>i z|2\na>i z\ufe38 \ufe37\ufe37 \ufe38 :=\u03bdi\nai, (22)\nwhere \u03bdi represents the weight assigned to each ai. This forms the descent direction of WF updates.\nUnfortunately, WF moving along the preceding direction does not come close to a meaningful solution under real-valued Poisson data. To see this, it is helpful to consider any fixed vector z \u2208 Rn independent of the design vectors. The typical size of min1\u2264i\u2264m |a>i z| is about on the order of 1m\u2016z\u2016, introducing some unreasonably large weights \u03bdi, which can be as large as m\u2016x\u20162/\u2016z\u2016. (For complex-valued data where ai \u223c N (0, I) + jN (0, I), this issue is less severe since min1\u2264i\u2264m |a>i z| 1\u221am\u2016z\u2016.) Consequently, the iterative updates based on (1.2) often overshoot, and this arises starting from the very initial stage.\nFig. 4 illustrates this phenomenon by showing the locus of \u2212\u2207`i (z) when ai varies. Examination of the figure seems to suggest that most of the gradient components \u2207`i (z) are more or less pointing towards the ground-truth solution x and forming reasonable descent directions. This intuition is corroborated by numerical experiments under real-valued data. As illustrated in Fig. 5, the solutions returned by the untruncated WF (designed for a Poisson log-likelihood) are very far from the ground truth.\nHence, to remedy the aforementioned stability issue, it would be natural to separate the small fraction of abnormal gradient components by regularizing the weights \u03bdi, possibly via appropriate truncation. This\ngives rise to the update rule of TWF:\nz(t+1) = z(t) + \u00b5t m \u2207`tr(z(t)), \u2200t \u2208 N, (23)\nwhere \u2207`tr (\u00b7) denotes the truncated gradient given by 6\n\u2207`tr (z) := m\u2211 i=1 2 yi \u2212 |a>i z|2 a>i z ai1Ei1(z)\u2229Ei2(z). (24)\nfor some truncation criteria specified by E i1 (\u00b7) and E i2 (\u00b7). In our algorithm, we take E i1 (z) and E i2 (z) to be two collections of events given by\nE i1(z) := { \u03b1lbz \u2264 \u2223\u2223a>i z\u2223\u2223 \u2016z\u2016 \u2264 \u03b1 ub z } ; (25)\nE i2(z) := { |yi \u2212 |a>i z|2| \u2264\n\u03b1h m \u2225\u2225y \u2212A (zz>)\u2225\u2225 1 \u2223\u2223a>i z\u2223\u2223 \u2016z\u2016 } , (26)\nwhere \u03b1lbz , \u03b1ubz , \u03b1z are predetermined truncation thresholds. To keep notation light, we shall use E i1 and E i2 rather than E i1 (z) and E i2 (z) whenever it is clear from context.\nWe emphasize that the above truncation simply throws away those weights \u03bdi\u2019s that fall outside some confidence range as to remove the contribution of abnormal components. To achieve this, we regularize both the numerator and denominator of \u03bdi by enforcing separate truncation rules. Recognize that for any fixed z, the denominator obeys\nE [\u2223\u2223a>i z\u2223\u2223] = \u221a2/\u03c0\u2016z\u2016,\nwhich leads up to the rule (25). Regarding the numerator, by the law of large numbers one would expect\nE [\u2223\u2223yi \u2212 |a>i z|2\u2223\u2223] \u2248 1m \u2225\u2225y \u2212A (zz>)\u2225\u22251 ,\nand hence it is natural to regularize the numerator by ensuring\u2223\u2223yi \u2212 |a>i z|2\u2223\u2223 . 1m \u2225\u2225y \u2212A (zz>)\u2225\u22251 . As a remark, we include an extra term |a\n> i z| \u2016z\u2016 in (26) to sharpen the theory, but all our results continue\nto hold (up to some modification of constants) if we drop this term in the truncation rule (26). Detailed procedures are summarized in Algorithm 1 7."}, {"heading": "2.2 Truncated spectral initialization", "text": "In order for the truncated gradient stage to converge rapidly, we need to seed it with a suitable initialization. One natural alternative is the spectral method adopted in [11,32], which amounts to computing the leading eigenvector of Y\u0303 := 1m \u2211m i=1 yiaia > i . This arises from the observation that when ai \u223c N (0, I) and \u2016x\u2016 = 1,\nE[Y\u0303 ] = I + 2xx>,\nwhose leading eigenvector is exactly x with an eigenvalue of 3. Unfortunately, this spectral technique converges to a good initial point only when m & n log n, due to the fact that (a>i x)2aia>i is heavy-tailed, a random quantity which does not have a moment generating 6 In the complex-valued case, the truncation is enforced upon the Wirtinger derivative, which reads \u2207`tr (z) :=\u2211m i=1 2 yi\u2212|z\u2217ai|2 z\u2217ai ai1Ei1(z)\u2229E i 2(z) .\n7Careful readers might note that we include some extra factor \u221a n\n\u2016ai\u2016 (which is approximately 1 in the Gaussian model) in\nAlgorithm 1. This occurs since we present Algorithm 1 in a more general fashion that applies beyond the model ai \u223c N (0, I), but all results / proofs continue to hold in the presence of this extra term.\nAlgorithm 1 Truncated Wirtinger Flow. Input: Measurements {yi | 1 \u2264 i \u2264 m} and sampling vectors {ai | 1 \u2264 i \u2264 m}; truncation thresholds \u03b1lbz , \u03b1ubz , \u03b1h, and \u03b1y (see default values in Table 1).\nInitialize z(0) to be \u221a\nmn\u2211m i=1\u2016ai\u2016\n2\u03bb0z\u0303, where \u03bb0 = \u221a 1 m \u2211m i=1 yi and z\u0303 is the leading eigenvector of\nY = 1\nm m\u2211 i=1 yiaia \u2217 i 1{|yi|\u2264\u03b12y\u03bb20}. (27)\nLoop: for t = 0 : T do\nz(t+1) = z(t) + 2\u00b5t m m\u2211 i=1 yi \u2212 \u2223\u2223a\u2217i z(t)\u2223\u22232 z(t)\u2217ai ai1Ei1\u2229Ei2 , (28)\nwhere E i1 := { \u03b1lbz \u2264 \u221a n\n\u2016ai\u2016 |a\u2217i z(t)| \u2016z(t)\u2016 \u2264 \u03b1 ub z\n} , E i2 := { |yi \u2212 |a\u2217i z(t)|2| \u2264 \u03b1hKt \u221a n\n\u2016ai\u2016 |a\u2217i z(t)| \u2016z(t)\u2016\n} , (29)\nand Kt := 1\nm m\u2211 l=1 \u2223\u2223yl \u2212 |a\u2217l z(t)|2\u2223\u2223. Output zT .\nfunction. To be more precise, consider the noiseless case yi = |a>i x|2 and recall that maxi yi \u2248 2 logm. Letting k = arg maxi yi, one can calculate(\nak \u2016ak\u2016\n)> Y\u0303\nak \u2016ak\u2016 \u2265 ( ak \u2016ak\u2016 )>( 1 m aka > k )( a>k x )2( ak \u2016ak\u2016 ) \u2248 2n logm m ,\nwhich is much larger than x>Y\u0303 x = 3 unless m/n is very large. This tells us that in the regime where m n, there exists some unit vector ak/\u2016ak\u2016 that is closer to the leading eigenvector of Y\u0303 than x. This phenomenon prevents the spectral method from returning a meaningful initial guess.\nTo address this issue, we propose to discard those observations yi that are several times larger than the mean during spectral initialization. Specifically, the initial estimate is obtained by computing the leading eigenvector z\u0303 of\nY := 1\nm m\u2211 i=1 yiaia > i 1{|yi|\u2264\u03b12y( 1m \u2211ml=1 yl)} (30)\nfor some truncation threshold \u03b1y, and then rescaling z\u0303 so as to have roughly the same norm as x (which is estimated to be 1m \u2211m l=1 yl); see Algorithm 1 for the detailed procedure.\nThe above drawback of the spectral method is not merely a theoretical concern but rather a substantial practical issue. We have seen this in Fig. 2 (main quad example) showing the enormous advantage of the truncation scheme. This is also further illustrated in Fig. 6, which compares the empirical efficiency of both methods with \u03b1y = 3 set to be the truncation threshold. For both Gaussian designs and CDP models, the empirical loss incurred by the original spectral method increases as n grows, which is in stark constrast to the truncated spectral method that achieves almost identical accuracy over the same range of n."}, {"heading": "2.3 Choice of algorithmic parameters", "text": "One implementation detail to specify is the step size \u00b5t at each iteration t. There are two alternatives that work well in both theory and practice:\n1. Fixed step size. Take \u00b5t \u2261 \u00b5 (\u2200t \u2208 N) for some constant \u00b5 > 0. As long as \u00b5 is not too large, our main results state that this strategy always works\u2014although the convergence rate depends on \u00b5. Under appropriate conditions, our theorems hold for any constant 0 < \u00b5 < 0.28.\nn: signal dimension 1000 2000 3000 4000 5000\nRe lat\nive e\nrro r\n0.6\n0.7\n0.8\n0.9\n1 spectral method truncated spectral method\nn : signal dimension (105) 0.5 1 1.5 2 2.5 3 3.5 4\nRe lat\nive e\nrro r\n0.4\n0.6\n0.8\n1\n1.2\n1.4 spectral method truncated spectral method\n(a) (b)\nAnother set of important algorithmic parameters to determine is the truncation thresholds \u03b1h, \u03b1lbz , \u03b1ubz , \u03b1y, and \u03b1p (for a backtracking line search only). The present paper isolates the set of (\u03b1h, \u03b1lbz , \u03b1ubz , \u03b1y)\nobeying (31) as given in Table 1 when a fixed step size is employed. More concretely, this range subsumes as special cases all parameters obeying the following constraints:\n0 < \u03b1lbz \u2264 0.5, \u03b1ubz \u2265 5, \u03b1h \u2265 5, and \u03b1y \u2265 3. (35)\nWhen a backtracking line search is adopted, an extra parameter \u03b1p is needed, which we take to be \u03b1p \u2265 5. In all theory presented herein, we assume that the parameters fall within the range singled out in Table 1."}, {"heading": "3 Why TWF works?", "text": "Before proceeding, it is best to develop an intuitive understanding of the TWF iterations. We start with a notation representing the (unrecoverable) global phase [11] for real-valued data\n\u03c6 (z) := { 0, if \u2016z \u2212 x\u2016 \u2264 \u2016z + x\u2016 , \u03c0, else.\n(36)\nIt is self-evident that (\u2212z) + \u00b5\nm \u2207tr`\n( \u2212 z ) = \u2212 { z + \u00b5\nm \u2207tr`(z)\n} ,\nand hence (cf. Definition (8))\ndist (\n(\u2212z) + \u00b5 m \u2207tr`(\u2212z),x\n) = dist ( z + \u00b5\nm \u2207tr` (z) ,x ) despite the global phase uncertainty. For simplicity of presentation, we shall drop the phase term by letting z be e\u2212j\u03c6(z)z and setting h = z \u2212 x, whenever it is clear from context.\nThe first object to consider is the descent direction. To this end, we find it convenient to work with a fixed z independent of the design vectors ai, which is of course heuristic but helpful in developing some intuition. Rewrite\n\u2207`i(z) = 2 (a>i x) 2 \u2212 (a>i z)2 a>i z ai (i) = \u2212 2(a > i h)(2a > i z \u2212 a>i h) a>i z ai\n= \u22124(a>i h)ai + 2 (a>i h) 2\na>i z ai\ufe38 \ufe37\ufe37 \ufe38\n:=ri\n, (37)\nwhere (i) follows from the identity a2 \u2212 b2 = (a+ b)(a\u2212 b). The first component of (37), which on average gives \u22124h, makes a good search direction when averaged over all the observations i = 1, . . . ,m. The issue is that the other term ri\u2014which is in general non-integrable\u2014could be devastating. The reason is that a>i z could be arbitrarily small, thus resulting in an unbounded ri. As a consequence, a non-negligible portion of the ri\u2019s may exert a very strong influence on the descent direction in an undesired manner.\nSuch an issue can be prevented if one can detect and separate those gradient components bearing abnormal ri\u2019s. Since we cannot observe the individual components of the decomposition (37), we cannot reject indices with large values of ri directly. Instead, we examine each gradient component as a whole and discard it if its size is not absolutely controlled. Fortunately, such a strategy is sufficient to ensure that most of the contribution from the truncated gradient comes from the first component of (37), namely, \u22124(a>i h)ai. As will be made precise in Proposition 2 and Lemma 7, the truncated gradient obeys\n\u2212 \u2329 1 m \u2207`tr(z),h \u232a \u2265 (4\u2212 )\u2016h\u20162 \u2212O (\u2016h\u20163 \u2016z\u2016 ) (38)\nand \u2225\u2225\u2225 1 m \u2207`tr(z) \u2225\u2225\u2225 . \u2016h\u2016. (39) Here, one has (4\u2212 )\u2016h\u20162 in (38) instead of 4\u2016h\u20162 to account for the bias introduced by truncation, where is small as long as we only throw away a small fraction of data. Looking at (38) and (39) we see that the search direction is sufficiently aligned with the deviation \u2212h = x\u2212 z of the current iterate; i.e. they form a reasonably good angle that is bounded away from 90\u25e6. Consequently, z is expected to be dragged towards x provided that the step size is appropriately chosen.\nThe observations (38) and (39) are reminiscent of a (local) regularity condition given in [11], which has been shown to be a fundamental criterion that dictates rapid convergence of iterative procedures (including WF and other gradient descent schemes). When specialized to TWF, we say that \u2212 1m\u2207`tr (\u00b7) satisfies the regularity condition, denoted by RC (\u00b5, \u03bb, ), if\u2329\nh,\u2212 1 m \u2207`tr(z)\n\u232a \u2265 \u00b5\n2 \u2225\u2225\u2225 1 m \u2207`tr (z) \u2225\u2225\u22252 + \u03bb 2 \u2016h\u20162 (40)\nholds for all z obeying \u2016z \u2212 x\u2016 \u2264 \u2016x\u2016, where 0 < < 1 is some constant. Such an -ball around x is sometimes referred to as a basin of attraction. Formally, under RC (\u00b5, \u03bb, ), a little algebra gives\ndist2 ( z + \u00b5\nm \u2207`tr (z) ,x\n) \u2264 \u2225\u2225\u2225z + \u00b5 m \u2207`tr (z)\u2212 x \u2225\u2225\u22252 = \u2016h\u20162 + \u2225\u2225\u2225 \u00b5 m \u2207`tr (z) \u2225\u2225\u22252 + 2\u00b5\u2329h, 1 m \u2207`tr (z)\n\u232a \u2264 \u2016h\u20162 + \u2225\u2225\u2225 \u00b5 m \u2207`tr (z) \u2225\u2225\u22252 \u2212 \u00b52 \u2225\u2225\u2225\u2225 1m\u2207`tr (z) \u2225\u2225\u2225\u22252 \u2212 \u00b5\u03bb \u2016h\u20162\n= (1\u2212 \u00b5\u03bb) dist2 (z,x) (41)\nfor any z with \u2016z \u2212 x\u2016 \u2264 . In words, the TWF update rule is locally contractive around the planted solution, provided that RC (\u00b5, \u03bb, ) holds for some nonzero \u00b5 and \u03bb. Apparently, Conditions (38) and (39) already imply the validity of RC for some constants \u00b5, \u03bb 1 when \u2016h\u2016/\u2016z\u2016 is reasonably small, which in turn allows us to take a constant step size \u00b5 and enables a constant contraction rate 1\u2212 \u00b5\u03bb.\nFinally, caution must be exercised when connecting RC with strong convexity, since the former does not necessarily guarantee the latter within the basin of attraction. As an illustration, Fig. 7 plots the graph of a non-convex function obeying RC. The distinction stems from the fact that RC is stated only for those pairs z and h = z\u2212x with x being a fixed component, rather than simultaneously accommodating all possible z and h = z \u2212 z\u0303 with z\u0303 being an arbitrary vector. In contrast, RC says that the only stationary point of the truncated objective in a neighborhood of x is x, which often suffices for a gradient-descent type scheme to succeed.\n0.2 0.4 0.6 0.8 1\n0.9\n1 1.1\nt\nf(t)/ p 1 + t2"}, {"heading": "4 Numerical experiments", "text": "In this section, we report additional numerical results to verify the practical applicability of TWF. In all numerical experiments conducted in the current paper, we set\n\u03b1lbz = 0.3, \u03b1 ub z = 5, \u03b1h = 5, and \u03b1y = 3. (42)\nThis is a concrete combination of parameters satisfying our condition (31). Unless otherwise noted, we employ 50 power iterations for initialization, adopt a fixed step size \u00b5t \u2261 0.2 when updating TWF iterates, and set the maximum number of iterations to be T = 1000 for the iterative refinement stage.\nThe first series of experiments concerns exact recovery from noise-free data. Set n = 1000 and generate a real-valued signal x at random. Then for m varying between 2n and 6n, generate m design vectors ai independently drawn from N (0, I). An experiment is claimed to succeed if the returned estimate x\u0302 satisfies dist (x\u0302,x) / \u2016x\u2016 \u2264 10\u22125. Fig. 8 illustrates the empirical success rate of TWF (over 100 Monte Carlo trials for each m) revealing that exact recovery is practially guaranteed from fewer than 1000 iterations when the number of quadratic constraints is about 5 times the ambient dimension.\nTo see how special the real-valued Gaussian designs are to our theoretical finding, we perform experiments on two other types of measurement models. In the first, TWF is applied to complex-valued data by generating ai \u223c N ( 0, 12I ) + jN ( 0, 12I ) . The other is the model of coded diffraction patterns described in (9). Fig. 9 depicts the average success rate for both types of measurements over 100 Monte Carlo trials, indicating that m > 4.5n and m \u2265 6n are often sufficient under complex-valued Gaussian and CDP models, respectively.\nFor the sake of comparison, we also report the empirical performance of WF in all the above settings, where the step size is set to be the default choice of [11], that is, \u00b5t = min{1\u2212 e\u2212t/330, 0.2}. As can be seen, the empirical success rates of TWF outperform WF when T = 1000 under Gaussian models, suggesting that TWF either converges faster or exhibits better phase transition behavior.\nAnother series of experiments has been carried out to demonstrate the stability of TWF when the number m of quadratic equations varies. We consider the case where n = 1000, and vary the SNR (cf. (10)) from 15 dB to 55dB. The design vectors are real-valued independent Gaussian ai \u223c N (0, I), while the measurements yi are generated according to the Poisson noise model (4). Fig. 10 shows the relative mean square error\u2014 in the dB scale\u2014as a function of SNR, when averaged over 100 independent runs. For all choices of m, the numerical experiments demonstrate that the relative MSE scales inversely proportional to SNR, which matches our stability guarantees in Theorem 2 (since we observe that on the dB scale, the slope is about -1 as predicted by the theory (20))."}, {"heading": "5 Exact recovery from noiseless data", "text": "This section proves the theoretical guarantees of TWF in the absence of noise (i.e. Theorem 1). We separate the noiseless case mainly out of pedagogical reasons, as most of the steps carry over to the noisy case with slight modification.\nThe analysis for the truncated spectral method follows similar argument as in [11, Section 7.8], which we defer to Appendix C. In short, for any fixed \u03b4 > 0 and x \u2208 Rn, the initial point z(0) returned by the truncated spectral method obeys dist(z(0),x) \u2264 \u03b4\u2016x\u2016 with high probability, provided that m/n exceeds some large constant. With this in place, it suffices to show that the TWF update rule is locally contractive, as stated in the following proposition.\nProposition 1 (Local error contraction). Consider the noiseless case (1). Under the condition (31), there exist some universal constants 0 < \u03c10 < 1 and c0, c1, c2 > 0 such that with probability exceeding 1\u2212 c1 exp (\u2212c2m),\ndist2 ( z + \u00b5\nm \u2207`tr (z) ,x\n) \u2264 (1\u2212 \u03c10) dist2 (z,x) (43)\nholds simultaneously for all x, z \u2208 Rn obeying\ndist (z,x) \u2016z\u2016 \u2264 min { 1 11 , \u03b1lbz 3\u03b1h , \u03b1lbz 6 , 5.7 ( \u03b1lbz )2 2\u03b1ubz + \u03b1 lb z } , (44)\nprovided that m \u2265 c0n and that \u00b5 is some constant obeying 0 < \u00b5 \u2264 \u00b50 := 0.994\u2212\u03b61\u2212\u03b62\u2212 \u221a 2/(9\u03c0)\u03b1\u22121h 2(1.02+0.665/\u03b1h) .\nProposition 1 reveals the monotonicity of the estimation error: once entering a neighborhood around x of a reasonably small size, the iterative updates will remain within this neighborhood all the time and be attracted towards x at a geometric rate.\nAs shown in Section 3, under the hypothesis RC (\u00b5, \u03bb, ) one can conclude dist2 ( z + \u00b5\nm \u2207`tr(z),x\n) \u2264 (1\u2212 \u00b5\u03bb)dist2(z,x), \u2200(z,x) with dist(z,x) \u2264 . (45)\nThus, everything now boils down to showing RC (\u00b5, \u03bb, ) for some constants \u00b5, \u03bb, > 0. This occupies the rest of this section.\n5.1 Preliminary facts about {E i1} and {E i2} Before proceeding, we gather a few properties of the events E i1 and E i2, which will prove crucial in establishing RC (\u00b5, \u03bb, ). To begin with, recall that the truncation level given in E i2 depends on 1m \u2225\u2225A (xx> \u2212 zz>)\u2225\u2225 1 . Instead of working with this random variable directly, we use deterministic quantities that are more amenable to analysis. Specifically, we claim that 1m \u2225\u2225A (xx> \u2212 zz>)\u2225\u2225 1 offers a uniform and orderwise tight estimate on \u2016h\u2016 \u2016z\u2016, which can be seen from the following two facts.\nLemma 1. Fix \u03b6 \u2208 (0, 1). If m > c0n\u03b6\u22122 log 1\u03b6 , then with probability at least 1\u2212 C exp(\u2212c1\u03b62m),\n0.9 (1\u2212 \u03b6) \u2016M\u2016F \u2264 1\nm \u2016A (M)\u20161 \u2264 (1 + \u03b6)\n\u221a 2 \u2016M\u2016F (46)\nholds for all symmetric rank-2 matrices M \u2208 Rn\u00d7n. Here, c0, c1, C > 0 are some universal constants.\nProof. Since [12, Lemma 3.1] already establishes the upper bound, it suffices to prove the lower tail bound. Consider all symmetric rank-2 matrices M with eigenvalues 1 and \u2212t for some \u22121 \u2264 t \u2264 1. When t \u2208 [0, 1], it has been shown in the proof of [12, Lemma 3.2] that with high probability,\n1 m \u2016A (M)\u20161 \u2265 (1\u2212 \u03b6) f (t) , (47)\nx\nz\n`(z)\n1\nfor all such rank-2 matrices M , where f (t) := 2\u03c0 { 2 \u221a t+ (1\u2212 t) ( \u03c0/2\u2212 2arc tan( \u221a t) )}\n. The lower bound in this case can then be justified by recognizing that f (t) / \u221a 1 + t2 \u2265 0.9 for all t \u2208 [0, 1], as illustrated in Fig. 11. The case where t \u2208 [\u22121, 0] is an immediate consequence from [12, Lemma 3.1].\nLemma 2. Consider any x, z \u2208 Rn obeying \u2016z \u2212 x\u2016 \u2264 \u03b4 \u2016z\u2016 for some \u03b4 < 12 . Then one has \u221a 2\u2212 4\u03b4 \u2016z \u2212 x\u2016 \u2016z\u2016 \u2264 \u2225\u2225xx> \u2212 zz>\u2225\u2225\nF \u2264 (2 + \u03b4) \u2016z \u2212 x\u2016 \u2016z\u2016 . (48)\nProof. Take h = z \u2212 x and write\u2225\u2225xx> \u2212 zz>\u2225\u22252 F = \u2225\u2225\u2212 hz> \u2212 zh> + hh>\u2225\u22252 F\n= \u2225\u2225hz> + zh>\u2225\u22252\nF + \u2016h\u20164 \u2212 2\u3008hz> + zh>,hh>\u3009\n= 2 \u2016z\u20162 \u2016h\u20162 + 2|h>z|2 + \u2016h\u20164 \u2212 2\u2016h\u20162(h>z + z>h).\nWhen \u2016h\u2016 < 12\u2016z\u2016, the Cauchy\u2013Schwartz inequality gives\n2 \u2016z\u20162 \u2016h\u20162 \u2212 4 \u2016z\u2016 \u2016h\u20163 \u2264 \u2225\u2225xx> \u2212 zz>\u2225\u22252\nF \u2264 4 \u2016z\u20162 \u2016h\u20162 + 4 \u2016h\u20163 \u2016z\u2016+ \u2016h\u20164 , (49)\n\u21d2 \u221a (2 \u2016z\u2016 \u2212 4 \u2016h\u2016) \u2016z\u2016 \u00b7 \u2016h\u2016 \u2264 \u2225\u2225xx> \u2212 zz>\u2225\u2225\nF \u2264 (2 \u2016z\u2016+ \u2016h\u2016) \u00b7 \u2016h\u2016 (50)\nas claimed.\nTaken together the above two facts demonstrate that with probability 1\u2212 exp (\u2212\u2126 (m)),\n1.15 \u2016z \u2212 x\u2016 \u2016z\u2016 \u2264 1 m \u2225\u2225A (xx> \u2212 zz>)\u2225\u2225 1 \u2264 3 \u2016z \u2212 x\u2016 \u2016z\u2016 (51)\nholds simultaneously for all z and x satisfying \u2016h\u2016 \u2264 111 \u2016z\u2016. Conditional on (51), the inclusion\nE i3 \u2286 E i2 \u2286 E i4 (52)\nholds with respect to the following events E i3 : = {\u2223\u2223|a>i x|2 \u2212 |a>i z|2\u2223\u2223 \u2264 1.15\u03b1h \u2016h\u2016 \u00b7 \u2223\u2223a>i z\u2223\u2223} , (53)\nE i4 : = {\u2223\u2223|a>i x|2 \u2212 |a>i z|2\u2223\u2223 \u2264 3\u03b1h \u2016h\u2016 \u00b7 \u2223\u2223a>i z\u2223\u2223} . (54)\nThe point of introducing these new events is that E i3\u2019s (resp. E i4\u2019s) are statistically independent for any fixed x and z and are, therefore, easier to work with.\nNote that each E i3 (resp. E i4) is specified by a quadratic inequality. A closer inspection reveals that in order to satisfy these quadratic inequalities, the quantity a>i h must fall within two intervals centered around 0 and 2a>i z, respectively. One can thus facilitate analysis by decoupling each quadratic inequality of interest into two simple linear inequalities, as stated in the following lemma.\nLemma 3. For any \u03b3 > 0, define Di\u03b3 := {\u2223\u2223|a>i x|2 \u2212 |a>i z|2\u2223\u2223 \u2264 \u03b3 \u2016h\u2016 \u2223\u2223a>i z\u2223\u2223} , (55)\nDi,1\u03b3 := { |a>i h| \u2016h\u2016 \u2264 \u03b3 } , (56)\nand Di,2\u03b3 := {\u2223\u2223\u2223\u2223a>i h\u2016h\u2016 \u2212 2a>i z\u2016h\u2016 \u2223\u2223\u2223\u2223 \u2264 \u03b3} . (57) Thus, Di,1\u03b3 and Di,2\u03b3 represent the two intervals on a>i h centered around 0 and 2a>i z. If \u2016h\u2016\u2016z\u2016 \u2264 \u03b1lbz \u03b3 , then the following inclusion holds( Di,1\u03b3\n1+ \u221a 2\n\u2229 E i1 ) \u222a ( Di,2\u03b3\n1+ \u221a 2\n\u2229 E i1 ) \u2286 Di\u03b3 \u2229 E i1 \u2286 ( Di,1\u03b3 \u2229 E i1 ) \u222a ( Di,2\u03b3 \u2229 E i1 ) . (58)"}, {"heading": "5.2 Proof of the regularity condition", "text": "By definition, one step towards proving the regularity condition (40) is to control the norm of the truncated gradient. In fact, a crude argument already reveals that \u2016 1m\u2207`tr(z)\u2016 . \u2016h\u2016. To see this, introduce v = [vi]1\u2264i\u2264m with vi := 2 |a>i x|2\u2212|a>i z|2 a>i z 1Ei1\u2229Ei2 . It comes from the truncation rule E i 1 as well as the inclusion property (52) that\u2223\u2223a>i z\u2223\u2223 & \u2016z\u2016 and \u2223\u2223\u2223yi \u2212 \u2223\u2223a>i z\u2223\u22232\u2223\u2223\u2223 . 1m\u2016A(xx> \u2212 zz>)\u20161 \u2016h\u2016 \u2016z\u2016 , implying |vi| . \u2016h\u2016 and hence \u2016v\u2016 . \u221a m\u2016h\u2016. The Marchenko\u2013Pastur law gives \u2016A\u2016 . \u221am, whence\n1 m \u2016\u2207`tr(z)\u2016 = 1 m \u2016A>v\u2016 \u2264 1 m \u2016A\u2016 \u00b7 \u2016v\u2016 . \u2016h\u2016. (59)\nA more refined estimate will be provided in Lemma 7. The above argument essentially tells us that to establish RC, it suffices to verify a uniform lower bound of the form \u2212 \u2329 h, 1\nm \u2207`tr (z)\n\u232a & \u2016h\u20162 , (60)\nas formally derived in the following proposition.\nProposition 2. Consider the noise-free measurements yi = |a>i x|2 and any fixed constant > 0. Under the condition (31), if m > c1n, then with probability exceeding 1\u2212 C exp (\u2212c0m),\n\u2212 \u2329 h, 1\nm \u2207`tr (z)\n\u232a \u2265 2 { 1.99\u2212 2 (\u03b61 + \u03b62)\u2212 \u221a 8/(9\u03c0)\u03b1\u22121h \u2212 } \u2016h\u20162 (61)\nholds uniformly over all x, z \u2208 Rn obeying\n\u2016h\u2016 \u2016z\u2016 \u2264 min\n{ 1\n11 , \u03b1lbz 3\u03b1h , \u03b1lbz 6 ,\n5.7 ( \u03b1lbz )2\n2\u03b1ubz + \u03b1 lb z\n} . (62)\nHere, c0, c1, C > 0 are some universal constants, and \u03b61 and \u03b62 are defined in (31).\nThe basic starting point is the observation that (a>i z)\u2212 (a>i x)2 = (a>i h)(2a>i z \u2212 a>i h) and hence\n\u2212 1 2m \u2207`tr (z) = 1 m m\u2211 i=1 (a>i z) 2 \u2212 (a>i x)2 a>i z ai1Ei1\u2229Ei2\n= 1\nm m\u2211 i=1 2(a>i h)ai1Ei1\u2229Ei2 \u2212 1 m m\u2211 i=1 (a>i h) 2 a>i z ai1Ei1\u2229Ei2 . (63)\nOne would expect the contribution of the second term of (63) (which is a second-order quantity) to be small as \u2016h\u2016 / \u2016z\u2016 decreases.\nTo facilitate analysis, we rewrite (63) in terms of the more convenient events Di,1\u03b3 and Di,2\u03b3 . Specifically, the inclusion property (52) together with Lemma 3 reveals that\nDi,1\u03b33 \u2229 E i1 \u2286 E i3 \u2229 E i1 \u2286 E i2 \u2229 E i1 \u2286 E i4 \u2229 E i1 \u2286 ( Di,1\u03b34 \u222a Di,2\u03b34 ) \u2229 E i1, (64)\nwhere the parameters \u03b33, \u03b34 are given by\n\u03b33 := 0.476\u03b1h, and \u03b34 := 3\u03b1h. (65)\nThis taken collectively with the identity (63) leads to a lower estimate\n\u2212 \u2329 1\n2m \u2207`tr(z),h \u232a \u2265 2 m m\u2211 i=1 ( a>i h )2 1Ei1\u2229D i,1 \u03b33 \u2212 1 m m\u2211 i=1 \u2223\u2223a>i h\u2223\u22233\u2223\u2223a>i z\u2223\u2223 1Ei1\u2229Di,1\u03b34 \u2212 1m m\u2211 i=1 \u2223\u2223a>i h\u2223\u22233\u2223\u2223a>i z\u2223\u2223 1Ei1\u2229Di,2\u03b34 , (66) leaving us with three quantities in the right-hand side to deal with. We pause here to explain and compare the influences of these three terms.\nTo begin with, as long as the truncation step does not discard too many samples, the first term should be close to 2m \u2211 i |a>i h|2, which approximately gives 2\u2016h\u20162 from the law of large numbers. This term turns out to be dominant in the right-hand side of (66) as long as \u2016h\u2016/\u2016z\u2016 is reasonably small. To see this, please recognize that the second term in the right-hand side is O(\u2016h\u20163/\u2016z\u2016), simply because both a>i h and a>i z are absolutely controlled on Di,1\u03b34 \u2229 E i1. However, Di,2\u03b34 does not share such a desired feature. By the very definition of Di,2\u03b34 , each nonzero summand of the last term of (66) must obey\n\u2223\u2223a>i h\u2223\u2223 \u2248 2 \u2223\u2223a>i z\u2223\u2223 and, therefore, |a\n> i h|3 |a>i z| 1Ei1\u2229Di,2\u03b34 is roughly of the order of \u2016z\u2016 2; this could be much larger than our target level\n\u2016h\u20162. Fortunately, Di,2\u03b34 is a rare event, thus precluding a noticable influence upon the descent direction. All of this is made rigorous in Lemma 4 (first term), Lemma 5 (second term) and Lemma 6 (third term) together with subsequent analysis.\nLemma 4. Fix \u03b3 > 0, and let E i1 and Di,1\u03b3 be defined in (25) and (56), respectively. Set \u03b61 := 1\u2212min { E [ \u03be21{\u221a1.01\u03b1lbz \u2264|\u03be|\u2264 \u221a 0.99\u03b1ubz } ] ,E [ 1{\u221a1.01\u03b1lbz \u2264|\u03be|\u2264 \u221a 0.99\u03b1ubz } ]} (67)\nand \u03b62 := E [ \u03be21{|\u03be|>\u221a0.99\u03b3} ] , (68)\nwhere \u03be \u223c N (0, 1). For any > 0, if m > c1n \u22122 log \u22121, then with probability at least 1\u2212 C exp(\u2212c0 2m),\n1\nm m\u2211 i=1 \u2223\u2223a>i h\u2223\u22232 1Ei1\u2229Di,1\u03b3 \u2265 (1\u2212 \u03b61 \u2212 \u03b62 \u2212 ) \u2016h\u20162 (69) holds for all non-zero vectors h, z \u2208 Rn. Here, c0, c1, C > 0 are some universal constants.\nWe now move on to the second term in the right-hand side of (66). For any fixed \u03b3 > 0, the definition of E i1 gives rise to an upper estimate\n1\nm m\u2211 i=1 \u2223\u2223a>i h\u2223\u22233\u2223\u2223a>i z\u2223\u2223 1Ei1\u2229Di,1\u03b3 \u2264 1\u03b1lbz \u2016z\u2016 \u00b7 1m m\u2211 i=1 \u2223\u2223a>i h\u2223\u22233 1Di,1\u03b3 \u2264 (1 + ) \u221a 8/\u03c0 \u2016h\u20163 \u03b1lbz \u2016z\u2016 , (70)\nwhere \u221a\n8/\u03c0 \u2016h\u20163 is exactly the untruncated moment E[|a>i h|3]. The second inequality is a consequence of the lemma below, which arises by observing that the summands |a>i h|31Di,1\u03b3 are independent sub-Gaussian random variables.\nLemma 5. For any constant \u03b3 > 0, if m/n \u2265 c0 \u00b7 \u22122 log \u22121, then\n1\nm m\u2211 i=1 \u2223\u2223a>i h\u2223\u22233 1Di,1\u03b3 \u2264 (1 + )\u221a8/\u03c0 \u2016h\u20163 , \u2200h \u2208 Rn (71) with probability at least 1\u2212 C exp(\u2212c1 2m) for some universal constants c0, c1, C > 0.\nIt remains to control the last term of (66). As mentioned above, the influence of this term is small since the set of ai\u2019s satisfying Di,2\u03b3 accounts for a small fraction of measurements. Put formally, the number of equations satisfying \u2223\u2223a>i h\u2223\u2223 \u2265 \u03b3 \u2016h\u2016 decays rapidly for large \u03b3 (at least at a quadratic rate), as stated below. Lemma 6. For any 0 < < 1, there exist some universal constants c0, c1, C > 0 such that\n1\nm m\u2211 i=1 1{|a>i h|\u2265 \u03b3\u2016h\u2016} \u2264 1 0.49\u03b3 exp ( \u22120.485\u03b32 ) + \u03b32 , \u2200h \u2208 Rn\\{0} and \u03b3 \u2265 2 (72)\nwith probability at least 1\u2212 C exp ( \u2212c0 2m ) . This holds with the proviso m/n \u2265 c1 \u00b7 \u22122 log \u22121.\nTo connect this lemma with the last term of (66), we recognize that when \u03b3 \u2264 \u03b1 lb z \u2016z\u2016 \u2016h\u2016 , one has\n1Ei1\u2229D i,2 \u03b3 \u2264 1{|a>i h|\u2265\u03b1lbz \u2016z\u2016}. (73)\nThe constraint \u2223\u2223\u2223a>i h\u2016h\u2016 \u2212 2a>i z\u2016h\u2016 \u2223\u2223\u2223 \u2264 \u03b3 of Di,2\u03b3 necessarily requires\u2223\u2223a>i h\u2223\u2223\n\u2016h\u2016 \u2265 2 \u2223\u2223a>i z\u2223\u2223 \u2016h\u2016 \u2212 \u03b3 \u2265 2\u03b1lbz \u2016z\u2016 \u2016h\u2016 \u2212 \u03b3 \u2265 \u03b1lbz \u2016z\u2016 \u2016h\u2016 , (74)\nwhere the last inequality comes from our assumption on \u03b3. With Lemma 6 in place, (73) immediately gives\nm\u2211 i=1 1Ei1\u2229D i,2 \u03b3 \u2264 \u2016h\u2016 0.49\u03b1lbz \u2016z\u2016 exp\n( \u22120.485 ( \u03b1lbz \u2016z\u2016 \u2016h\u2016 )2) +\n\u2016h\u20162\n(\u03b1lbz ) 2 \u2016z\u20162\n\u2264 1 9800 ( \u2016h\u2016 \u03b1lbz \u2016z\u2016 )4 + (\u03b1lbz ) 2 (\u2016h\u2016 \u2016z\u2016 )2 (75)\nas long as \u2016h\u2016\u2016z\u2016 \u2264 \u03b1lbz 6 , where the last inequality uses the majorization 1 20000x4 \u2265 1x exp\n( \u22120.485x2 ) holding for\nany x \u2265 6. In addition, on E i1 \u2229 Di,2\u03b3 , the amplitude of each summand can be bounded in such a way that\u2223\u2223a>i h\u2223\u22233\u2223\u2223a>i z\u2223\u2223 \u2264\n\u2223\u22232a>i z\u2223\u2223+ \u03b3 \u2016h\u2016\u2223\u2223a>i z\u2223\u2223 (2\u03b1ubz \u2016z\u2016+ \u03b3 \u2016h\u2016)2 (76) \u2264 ( 2 + \u03b3\n\u03b1lbz\n\u2016h\u2016 \u2016z\u2016\n)( 2\u03b1ubz + \u03b3\n\u2016h\u2016 \u2016z\u2016\n)2 \u2016z\u20162 , (77)\nwhere both inequalities are immediate consequences from the definitions of Di,2\u03b3 and E i1 (see (57) and (25)). Taking this together with the cardinality bound (75) and picking appropriately, we get\n1\nm m\u2211 i=1 \u2223\u2223a>i h\u2223\u22233\u2223\u2223a>i z\u2223\u2223 1Ei1\u2229Di,2\u03b3 \u2264  ( 2 + \u03b3 \u03b1lbz \u2016h\u2016 \u2016z\u2016 )( 2\u03b1ubz + \u03b3 \u2016h\u2016 \u2016z\u2016 )2 9800 (\u03b1lbz )\n4\ufe38 \ufe37\ufe37 \ufe38 \u03d11\n\u2016h\u20162 \u2016z\u20162 +  \u2016h\u2016 2 . (78)\nFurthermore, under the condition that\n\u03b3 \u2264 \u03b1lbz \u2016z\u2016 \u2016h\u2016 and \u2016h\u2016 \u2016z\u2016 \u2264\n\u221a 98 ( \u03b1lbz )2 \u221a 3 (2\u03b1ubz + \u03b1 lb z ) ,\none can simplify (78) by observing that \u03d11 \u2264 1100 , which results in\n1\nm m\u2211 i=1 \u2223\u2223a>i h\u2223\u22233\u2223\u2223a>i z\u2223\u2223 1Ei1\u2229Di,2\u03b3 \u2264 ( 1 100 + ) \u2016h\u20162 . (79)\nPutting all preceding results in this subsection together reveals that with probability exceeding 1 \u2212 exp (\u2212\u2126 (m)),\n\u2212 \u2329 h, 1\n2m \u2207`tr (z)\n\u232a \u2265 { 1.99\u2212 2 (\u03b61 + \u03b62)\u2212 \u221a 8/\u03c0\n\u2016h\u2016 \u03b1lbz \u2016z\u2016\n\u2212 3 } \u2016h\u20162\n\u2265 { 1.99\u2212 2 (\u03b61 + \u03b62)\u2212 \u221a 8/\u03c0(3\u03b1h) \u22121 \u2212 3 } \u2016h\u20162 (80)\nholds simultaneously over all x and z satisfying\n\u2016h\u2016 \u2016z\u2016 \u2264 min { \u03b1lbz 3\u03b1h , \u03b1lbz 6 , \u221a 98/3 ( \u03b1lbz )2 2\u03b1ubz + \u03b1 lb z , 1 11 } (81)\nas claimed in Proposition 2. To conclude this section, we provide a tighter estimate about the norm of the truncated gradient.\nLemma 7. Fix \u03b4 > 0, and assume that yi = (a>i x)2. Suppose that m \u2265 c0n for some large constant c0 > 0. There exist some universal constants c, C > 0 such that with probability at least 1\u2212 C exp (\u2212cm),\n1\nm \u2225\u2225\u2207`tr (z)\u2225\u2225 \u2264 (1 + \u03b4) \u00b7 4\u221a1.02 + 0.665/\u03b1h \u2016h\u2016 (82) holds simultaneously for all x, z \u2208 Rn satisfying \u2016h\u2016\u2016z\u2016 \u2264 min { \u03b1lbz 3\u03b1h , \u03b1lbz 6 , \u221a 98/3(\u03b1lbz ) 2 2\u03b1ubz +\u03b1 lb z , 111 } .\nLemma 7 complements the preceding arguments by allowing us to identify a concrete plausible range for the step size. Specifically, putting Lemma 7 and Proposition 2 together suggests that\n\u2212 \u2329 h, 1\nm \u2207`tr (z)\n\u232a \u2265\n2 { 1.99\u2212 2 (\u03b61 + \u03b62)\u2212 \u221a 8/(9\u03c0)\u03b1\u22121h \u2212 }\n(1 + \u03b4) 2 \u00b7 16 (1.02 + 0.665/\u03b1h)\n\u2225\u2225\u2225\u2225 1m\u2207`tr (z) \u2225\u2225\u2225\u22252 . (83)\nTaking and \u03b4 to be sufficiently small we arrive at a feasible range (cf. Definition (40)) \u00b5 \u2264 0.994\u2212 \u03b61 \u2212 \u03b62 \u2212 \u221a\n2/(9\u03c0)\u03b1\u22121h 2 (1.02 + 0.665/\u03b1h) := \u00b50. (84)\nThis establishes Proposition 1 and in turn Theorem 1 when \u00b5t is taken to be a fixed constant. To justify the contraction under backtracking line search, it suffices to prove that the resulting step size falls within this range (84), which we defer to Appendix D."}, {"heading": "6 Stability", "text": "This section goes in the direction of establishing stability guarantees of TWF. We concentrate on the iterative gradient stage, and defer the analysis for the initialization stage to Appendix C.\nBefore continuing, we collect two bounds that we shall use several times. The first is the observation that\n1 m \u2016y \u2212A(zz>)\u20161 \u2264 1 m \u2016A(xx> \u2212 zz>)\u20161 + 1 m \u2016\u03b7\u20161 . \u2016h\u2016\u2016z\u2016+ 1 m \u2016\u03b7\u20161 . \u2016h\u2016\u2016z\u2016+ 1\u221a m \u2016\u03b7\u2016, (85)\nwhere the last inequality follows from Cauchy-Schwarz. Setting\nvi := 2 yi \u2212 |a>i z|2\na>i z 1Ei1\u2229Ei2\nas usual, this inequality together with the truncation rules E i1 and E21 give\n|vi| . \u2016h\u2016+ \u2016\u03b7\u2016\u221am\u2016z\u2016 =\u21d2 \u2225\u2225 1 m\u2207`tr(z) \u2225\u2225 = 1m\u2016A>v\u2016 \u2264 \u2225\u2225\u2225 1\u221amA\u2225\u2225\u2225 1\u221am\u2016v\u2016 (i). 1\u221am\u2016v\u2016 . \u2016h\u2016+ \u2016\u03b7\u2016\u221am\u2016z\u2016 , (86)\nwhere (i) arises from [44, Corollary 5.35]. As discussed in Section 3, the estimation error is contractive if \u2212 1m\u2207`tr (z) satisfies the regularity condition. With (86) in place, RC reduces to\n\u2212 1 m \u3008\u2207`tr (z) ,h\u3009 & \u2016h\u20162. (87)\nUnfortunately, (87) does not hold for all z within the neighborhood of x due to the existence of noise. Instead we establish the following:\n\u2022 The condition (87) holds for all h obeying\nc3 \u2016\u03b7\u2016 /\u221am \u2016z\u2016 \u2264 \u2016h\u2016 \u2264 c4\u2016x\u2016 (88)\nfor some constants c3, c4 > 0 (we shall call it Regime 1); this will be proved later. In this regime, the reasoning in Section 3 gives\ndist ( z + \u00b5\nm \u2207`tr(z), x\n) \u2264 (1\u2212 \u03c1)dist(z,x) (89)\nfor some appropriate constants \u00b5, \u03c1 > 0 and, hence, error contraction occurs as in the noiseless setting.\n\u2022 However, once the iterate enters Regime 2 where\n\u2016h\u2016 \u2264 c3 \u2016\u03b7\u2016\u221a m \u2016z\u2016 (90)\nthe estimation error might no longer be contractive. Fortunately, in this regime each move by \u00b5m\u2207`tr (z) is of size at most O( \u2016\u03b7\u2016\u221a\nm\u2016z\u2016 ), compare (86). As a result, at each iteration the estimation error cannot\nincrease by more than a numerical constant times \u2016\u03b7\u2016\u221a m\u2016z\u2016 before possibly jumping out (of this regime). Therefore,\ndist ( z + \u00b5\nm \u2207`tr(z), x\n) \u2264 c5\n\u2016\u03b7\u2016\u221a m\u2016x\u2016 (91)\nfor some constant c5 > 0. Moreover, as long as \u2016\u03b7\u2016\u221e/\u2016x\u20162 is sufficiently small, one can guarantee that c5 \u2016\u03b7\u2016\u221a m\u2016x\u2016 \u2264 c5 \u2016\u03b7\u2016\u221e \u2016x\u2016 \u2264 c4\u2016x\u2016. In other words, if the iterate jumps out of Regime 2, it will still fall within Regime 1.\nTo summarize, suppose the initial guess z(0) obeys dist(z(0),x) \u2264 c4\u2016x\u2016. Then the estimation error will shrink at a geometric rate 1 \u2212 \u03c1 before it enters Regime 2. Afterwards, z(t) will either stay within Regime 2 or jump back and forth between Regimes 1 and 2. Because of the bounds (91) and (89), the estimation errors will never exceed the order of \u2016\u03b7\u2016\u221a\nm\u2016x\u2016 from then on. Putting these together establishes (17), namely, the first part of the theorem.\nBelow we justify the condition (87) for Regime 1, for which we start by gathering additional properties of the truncation rules. By Cauchy-Schwartz, 1m \u2016\u03b7\u20161 \u2264 1\u221am \u2016\u03b7\u2016 \u2264 1 c3 \u2016h\u2016 \u2016z\u2016. When c3 is sufficiently large, applying Lemmas 1 and 2 gives\n1 m \u2211m l=1 \u2223\u2223\u2223yl \u2212 \u2223\u2223a>l z\u2223\u22232\u2223\u2223\u2223 \u2264 1m \u2225\u2225A (xx> \u2212 zz>)\u2225\u22251 + 1m \u2016\u03b7\u20161 \u2264 2.98\u2016h\u2016\u2016z\u2016; 1 m \u2211m l=1\n\u2223\u2223\u2223yl \u2212 \u2223\u2223a>l z\u2223\u22232\u2223\u2223\u2223 \u2265 1m \u2225\u2225A (xx> \u2212 zz>)\u2225\u22251 \u2212 1m \u2016\u03b7\u20161 \u2265 1.151\u2016h\u2016\u2016z\u2016. (92) From now on, we shall denote E\u0303 i2 :=\n{ \u2223\u2223|a>i x|2 \u2212 |a>i z|2\u2223\u2223 \u2264 \u03b1hm \u2225\u2225y \u2212A (zz>)\u2225\u22251 |a>i z|\u2016z\u2016 } to differentiate from E i2. For any small constant > 0, we introduce the index set G := {i : |\u03b7i| \u2264 C \u2016\u03b7\u2016 / \u221a m} that satisfies |G| = (1\u2212 )m. Note that C must be bounded as n scales, since\n\u2016\u03b7\u20162 \u2265 \u2211\ni/\u2208G \u03b72i \u2265 (m\u2212 |G|) \u00b7 C2 \u2016\u03b7\u20162/m \u2265 C2 \u2016\u03b7\u20162 \u21d2 C \u2264 1/\n\u221a . (93)\nWe are now ready to analyze the truncated gradient, which we separate into several components as follows\n\u2207tr` (z) = 2 \u2211 i\u2208G \u2223\u2223a>i x\u2223\u22232 \u2212 \u2223\u2223a>i z\u2223\u22232 a>i z ai1Ei1\u2229Ei2 + 2 \u2211 i/\u2208G \u2223\u2223a>i x\u2223\u22232 \u2212 \u2223\u2223a>i z\u2223\u22232 a>i z\nai1Ei1\u2229E\u0303i2\ufe38 \ufe37\ufe37 \ufe38 :=\u2207cleantr `(z)\n+ 2 \u2211 i\u2208G \u03b7i a>i z\nai1Ei1\u2229Ei2\ufe38 \ufe37\ufe37 \ufe38 :=\u2207noisetr `(z)\n+ 2 \u2211 i/\u2208G\n( yi \u2212 \u2223\u2223a>i z\u2223\u22232 a>i z 1Ei1\u2229Ei2 \u2212 \u2223\u2223a>i x\u2223\u22232 \u2212 \u2223\u2223a>i z\u2223\u22232 a>i z 1Ei1\u2229E\u0303i2 ) ai\ufe38 \ufe37\ufe37 \ufe38\n:=\u2207extratr `(z)\n. (94)\n\u2022 For each index i \u2208 G, the inclusion property (52) (i.e. E i3 \u2286 E i2 \u2286 E i4) holds. To see this, observe that\u2223\u2223yi \u2212 |a>i z|2\u2223\u2223 \u2208 \u2223\u2223|a>i x|2 \u2212 |a>i z|2\u2223\u2223\u00b1 |\u03b7i|. Since |\u03b7i| \u2264 C \u2016\u03b7\u2016/ \u221a m \u2016h\u2016\u2016z\u2016 when c3 is sufficiently large, one can derive the inclusion (52)\nimmediately from (92). As a result, all the proof arguments for Proposition 2 carry over to \u2207cleantr ` (z), suggesting that\n\u2212 \u2329 h, 1\nm \u2207cleantr ` (z)\n\u232a \u2265 2 { 1.99\u2212 2 (\u03b61 + \u03b62)\u2212 \u221a 8/(9\u03c0)\u03b1\u22121h \u2212 } \u2016h\u20162. (95)\n\u2022 Next, letting wi = 2\u03b7ia>i z1Ei1\u2229Ei21{i\u2208G}, we see that for any constant \u03b4 > 0, the noise component obeys\u2225\u2225\u2225\u2225 1m\u2207noisetr `(z) \u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225 1mA>w \u2225\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225\u2225 1\u221amA \u2225\u2225\u2225\u2225 \u2225\u2225\u2225\u2225 1\u221amw \u2225\u2225\u2225\u2225 (ii)\u2264 1 + \u03b4\u221am \u2016w\u2016 \u2264 (1 + \u03b4)2\u2016\u03b7\u2016/ \u221a m \u03b1lbz \u2016z\u2016 , (96)\nwhen m/n is sufficiently large. Here, (ii) arises from [44, Corollary 5.35], and the last inequality is a consequence of the upper estimate\n\u2016w\u20162 \u2264 4 m\u2211 i=1 |\u03b7i|2 (a>i z) 2 1Ei1\u2229Ei2 \u2264 4 m\u2211 i=1 |\u03b7i|2 (\u03b1lbz \u2016z\u2016)2 = 4 \u2016\u03b7\u20162 (\u03b1lbz \u2016z\u2016)2 . (97)\nIn turn, this immediately gives\u2223\u2223\u2223\u2223\u2329h, 1m\u2207noisetr ` (z)\u232a \u2223\u2223\u2223\u2223 \u2264 \u2016h\u2016\u2225\u2225\u2225\u2225 1m\u2207noisetr ` (z) \u2225\u2225\u2225\u2225 \u2264 2 (1 + \u03b4)\u03b1lbz \u2016\u03b7\u2016\u221am\u2016z\u2016\u2016h\u2016. (98) \u2022 We now turn to the last term\u2207extratr ` (z). According to the definition of E i2 and E\u0303 i2 as well as the property\n(92), the weight qi := 2 ( yi\u2212|a>i z|2 a>i z 1Ei1\u2229Ei2 \u2212 |a>i x|2\u2212|a>i z|2 a>i z 1Ei1\u2229E\u0303i2 ) 1{i/\u2208G} is bounded in magnitude by 6\u2016h\u2016. This gives \u2016q\u2016 \u2264 \u221a m\u2212 |G| \u00b7 6\u2016h\u2016 \u2264 6\u221a m\u2016h\u2016,\n\u21d2 \u2223\u2223\u2223\u2329 1 m \u2207extratr ` (z) ,h \u232a\u2223\u2223\u2223 \u2264 \u2016h\u2016 \u00b7 \u2225\u2225 1 m \u2207extratr ` (z) \u2225\u2225 = 1 m \u2016h\u2016 \u00b7 \u2225\u2225A>q\u2225\u2225 \u2264 6 (1 + \u03b4)\u221a \u2016h\u20162. (99) Taking the above bounds together yields\n\u2212 1 m \u3008\u2207`tr (z) ,h\u3009 \u2265 2\n{ 1.99\u2212 2 (\u03b61 + \u03b62)\u2212 \u221a 8\n9\u03c0\n1 \u03b1h \u2212 6(1 + \u03b4)\u221a \u2212\n} \u2016h\u20162 \u2212 2 (1 + \u03b4)\n\u03b1lbz\n\u2016\u03b7\u2016\u221a m \u2016z\u2016\u2016h\u2016.\nSince \u2016h\u2016 \u2265 c3 \u2016\u03b7\u2016\u221am\u2016z\u2016 for some large constant c3 > 0, setting to be small one obtains\n\u2212 1 m \u3008\u2207`tr (z) ,h\u3009 \u2265 2\n{ 1.95\u2212 2 (\u03b61 + \u03b62)\u2212 \u221a 8/(9\u03c0)\u03b1\u22121h } \u2016h\u20162 (100)\nfor all h obeying\nc3\u2016\u03b7\u2016/ \u221a m \u2016z\u2016 \u2264 \u2016h\u2016 \u2264 min { 1 11 , \u03b1lbz 3\u03b1h , \u03b1lbz 6 , \u221a 98/3 ( \u03b1lbz )2 2\u03b1ubz + \u03b1 lb z } \u2016z\u2016,\nwhich finishes the proof of Theorem 2 for general \u03b7. Up until now, we have established the theorem for general \u03b7, and it remains to specialize it to the Poisson model. Standard concentration results, which we omit, give\n1 m \u2016\u03b7\u20162 \u2248 1 m m\u2211 i=1 E [ \u03b72i ] = 1 m m\u2211 i=1 ( a>i x )2 \u2248 \u2016x\u20162. (101) Substitution into (17) completes the proof."}, {"heading": "7 Minimax lower bound", "text": "The goal of this section is to establish the minimax lower bound given in Theorem 3. For notational simplicity, we denote by P (y | w) the likelihood of yi ind.\u223c Poisson(|a>i w|2), 1 \u2264 i \u2264 m conditional on {ai}. For any two probability measures P and Q, we denote by KL (P\u2016Q) the Kullback\u2013Leibler (KL) divergence between them:\nKL (P\u2016Q) := \u02c6 log\n( dP\ndQ\n) dP, (102)\nThe basic idea is to adopt the general reduction scheme discussed in [43, Section 2.2], which amounts to finding a finite collection of hypotheses that are minimally separated. Below we gather one result useful for constructing and analyzing such hypotheses.\nLemma 8. Suppose that ai \u223c N (0, In), n is sufficiently large, and m = \u03ban for some sufficiently large constant \u03ba > 0. Consider any x \u2208 Rn\\{0}. On an event B of probability approaching one, there exists a collectionM of M = exp (n/30) distinct vectors obeying the following properties: (i) x \u2208M; (ii) for all w(l),w(j) \u2208M,\n1/ \u221a 8\u2212 (2n)\u22121/2 \u2264 \u2225\u2225w(l) \u2212w(j)\u2225\u2225 \u2264 3/2 + n\u22121/2; (103)\n(iii) for all w \u2208M, |a>i (w \u2212 x) |2 |a>i x|2 \u2264 \u2016w \u2212 x\u2016 2 \u2016x\u20162 {2 + 17 log 3m}, 1 \u2264 i \u2264 m. (104)\nIn words, Lemma 8 constructs a set M of exponentially many vectors/hypotheses scattered around x and yet well separated. From (ii) we see that each pair of hypotheses in M is separated by a distance roughly on the order of 1, and all hypotheses reside within a spherical ball centered at x of radius 3/2+o(1). When \u2016x\u2016 \u2265 log1.5m, every hypothesis w \u2208 M satisfies \u2016w\u2016 \u2248 \u2016x\u2016 1. In addition, (iii) says that the quantities |a>i (w \u2212 x) |/|a>i x| are all very well controlled (modulo some logarithmic factor). In particular, when \u2016x\u2016 \u2265 log1.5m, one must have\n|a>i (w \u2212 x) |2 |a>i x|2 . \u2016w \u2212 x\u20162 \u2016x\u20162 log 3m . 1 log3m log3m . 1. (105)\nIn the Poisson model, such a quantity turns out to be crucial in controlling the information divergence between two hypotheses, as demonstrated in the following lemma.\nLemma 9. Fix a family of design vectors {ai}. Then for any w and r \u2208 Rn,\nKL ( P (y | w + r) \u2016 P (y | w) ) \u2264 \u2211m\ni=1 |a>i r|2\n( 8 +\n2|a>i r|2 |a>i w|2\n) . (106)\nLemma 9 and (105) taken collectively suggest that on the event B\u2229C (B is in Lemma 8 and C := {\u2016A\u2016 \u2264\u221a 2m}), the conditional KL divergence (we condition on the ai\u2019s) obeys\nKL ( P (y | w) \u2016 P (y | x) ) \u2264 c3 \u2211m i=1 \u2223\u2223a>i (w \u2212 x)\u2223\u22232 \u2264 2c3m \u2016w \u2212 x\u20162 , \u2200w \u2208M; (107) here, the inequality holds for some constant c3 > 0 provided that \u2016x\u2016 \u2265 log1.5m, and the last inequality is a result of C (which occurs with high probability). We now use hypotheses as in Lemma 8 but rescaled in such a way that \u2016w \u2212 x\u2016 \u03b4, and \u2016w \u2212 w\u0303\u2016 \u03b4, \u2200w, w\u0303 \u2208M with w 6= w\u0303. (108) for some 0 < \u03b4 < 1. This is achieved via the substitution w \u2190\u2212 x+\u03b4(w\u2212x); with a slight abuse of notation, M denotes the new set.\nThe hardness of a minimax estimation problem is known to be dictated by information divergence inequalities such as (107). Indeed, suppose that\n1 M \u2212 1 \u2211 w\u2208M\\{x} KL ( P (y | w) \u2016 P (y | x) ) \u2264 1 10 log (M \u2212 1) (109)\nholds, then the Fano-type minimax lower bound [43, Theorem 2.7] asserts that\ninf x\u0302 sup x\u2208M\nE [ \u2016x\u0302\u2212 x\u2016 \u2223\u2223 {ai}] & min w,w\u0303\u2208M,w 6=w\u0303 \u2016w \u2212 w\u0303\u2016. (110)\nSince M = exp(n/30), (109) would follow from\n2c3\u2016w \u2212 x\u20162 \u2264 n/(300m). w \u2208M. (111)\nHence, we just need to select \u03b4 to be a small multiple of \u221a n/m. This in turn gives\ninf x\u0302 sup x\u2208M\nE [ \u2016x\u0302\u2212 x\u2016 \u2223\u2223 {ai}] & min w,w\u0303\u2208M,w 6=w\u0303 \u2016w \u2212 w\u0303\u2016 & \u221a n/m. (112)\nFinally, it remains to connect \u2016x\u0302 \u2212 x\u2016 with dist (x\u0302,x). Since all the w \u2208 M are clustered around x and are at a mutual distance about \u03b4 that is much smaller than \u2016x\u2016, we can see that for any reasonable estimator, dist(x\u0302,x) = \u2016x\u0302\u2212 x\u2016. This finishes the proof."}, {"heading": "8 Discussion", "text": "To keep our treatment concise, this paper does not strive to explore all possible generalizations of the theory. There are nevertheless a few extensions worth pointing out.\n\u2022 More general objective functions. For concreteness, we restrict our analysis to the Poisson loglikelihood function, but the analysis framework we laid out easily carries over to a broad class of (nonconvex) objective functions. For instance, all results continue to hold if we replace the Poisson likelihood by the Gaussian likelihood; that is, the polynomial function \u2212\u2211mi=1(yi \u2212 |a>i z|2)2 studied in [11]. A general guideline is to first check whether the expected regularity condition\nE [ \u2212 \u2329\n1 m\u2207`tr (z) ,h\n\u232a] & \u2016h\u20162\nholds for any fixed z within a neighborhood around x. If so, then often times RC holds uniformly within this neighborhood due to sharp concentration of measure ensured by the truncation procedure.\n\u2022 Sub-Gaussian measurements. The theory extends to the situation where the ai\u2019s are i.i.d. subGaussian random vectors, although the truncation threshold might need to be tweaked based on the sub-Gaussian norm of ai. A more challenging scenario, however, is the case where the ai\u2019s are generated according to the CDP model, since there is much less randomness to exploit in the mathematical analysis. We leave this to future research.\nHaving demonstrated the power of TWF in recovering a rank-one matrix xx\u2217 from quadratic equations, we remark on the potential of TWF towards recovering low-rank matrices from rank-one measurements. Imagine that we wish to estimate a rank-r matrix X 0 and that all we know about X is\nyi = a > i Xai, 1 \u2264 i \u2264 m.\nIt is known that this problem can be efficiently solved by using more computational-intensive semidefinite programs [8,14]. With the hope of developing a linear-time algorithm, one might consider a modified TWF scheme, which would maintain a rank-r matrix variable and operate as follows: perform truncated spectral initialization, and then successively update the current guess via a truncated gradient descent rule applied to a presumed log-likelihood function.\nMoving away from i.i.d. sub-Gaussian measurements, there is a proliferation of problems that involve completion of a low-rank matrix X from partial entries, where the rank is known a priori. It is selfevident that such entry-wise observations can also be cast as rank-one measurements of X. Therefore, the preceding modified TWF may add to recent literature in applying non-convex schemes for low-rank matrix completion [24,27,28,41] or even robust PCA [33]. A concrete application of this flavor is a simple form of the fundamental alignment/matching problem [6, 15, 25]. Imagine a collection of n instances, each representing an image of the same physical object but with different shift ri \u2208 {0, \u00b7 \u00b7 \u00b7 ,M \u2212 1}. The goal is to align all these instances from observations on the relative shift between pairs of them. Denoting by Xi the cyclic shift by an amount ri of IM , one sees that the collection matrix X := [X>i Xj ]1\u2264i,j\u2264k is a rank-M matrix, and the relative shift observations can be treated as rank-one measurements of X. Running TWF over this problem instance might result in a statistically and computationally efficient solution. This would be of great practical interest."}, {"heading": "A Proofs for Section 5", "text": "A.1 Proof of Lemma 3 First, we make the observation that (a>i z)2\u2212 (a>i x)2 = ( 2a>i z \u2212 a>i h ) a>i h is a quadratic function in a>i h. If we assume \u03b3 \u2264 \u03b1 lb z \u2016z\u2016 \u2016h\u2016 , then on the event E i1 one has\n(a>i z) 2 \u2265 \u03b1lbz \u2016z\u2016 \u00b7 |a>i z| \u2265 \u03b3 \u2016h\u2016 \u2223\u2223a>i z\u2223\u2223 . (113) Solving the quadratic inequality that specifies Di\u03b3 gives\na>i h \u2208 [ a>i z \u2212 \u221a( a>i z )2 + \u03b3 \u2016h\u2016 \u2223\u2223a>i z\u2223\u2223, a>i z \u2212\u221a(a>i z)2 \u2212 \u03b3 \u2016h\u2016 \u2223\u2223a>i z\u2223\u2223] , or a>i h \u2208 [ a>i z + \u221a( a>i z\n)2 \u2212 \u03b3 \u2016h\u2016 \u2223\u2223a>i z\u2223\u2223, a>i z +\u221a(a>i z)2 + \u03b3 \u2016h\u2016 \u2223\u2223a>i z\u2223\u2223] , which we will simplify in the sequel.\nSuppose for the moment that a>i z \u2265 0, then the preceding two intervals are respectively equivalent to\na>i h \u2208  \u2212 \u03b3 \u2016h\u2016 \u2223\u2223a>i z\u2223\u2223 a>i z + \u221a( a>i z )2 + \u03b3 \u2016h\u2016 \u2223\u2223a>i z\u2223\u2223 , \u03b3 \u2016h\u2016 \u2223\u2223a>i z\u2223\u2223 a>i z + \u221a( a>i z )2 \u2212 \u03b3 \u2016h\u2016 \u2223\u2223a>i z\u2223\u2223  := I1;\na>i h\u2212 2a>i z \u2208  \u2212 \u03b3 \u2016h\u2016 \u2223\u2223a>i z\u2223\u2223 a>i z + \u221a( a>i z )2 \u2212 \u03b3 \u2016h\u2016 \u2223\u2223a>i z\u2223\u2223 , \u03b3 \u2016h\u2016 \u2223\u2223a>i z\u2223\u2223 a>i z + \u221a( a>i z )2 + \u03b3 \u2016h\u2016 \u2223\u2223a>i z\u2223\u2223  := I2.\nAssuming (113) and making use of the observations \u03b3 \u2016h\u2016 \u2223\u2223a>i z\u2223\u2223\na>i z + \u221a( a>i z )2 \u2212 \u03b3 \u2016h\u2016 \u2223\u2223a>i z\u2223\u2223 \u2264 \u03b3 \u2016h\u2016 \u2223\u2223a>i z\u2223\u2223 a>i z = \u03b3 \u2016h\u2016\nand \u03b3 \u2016h\u2016 \u2223\u2223a>i z\u2223\u2223 a>i z + \u221a( a>i z )2 + \u03b3 \u2016h\u2016 \u2223\u2223a>i z\u2223\u2223 \u2265 \u03b3 \u2016h\u2016 \u2223\u2223a>i z\u2223\u2223( 1 + \u221a 2 ) \u2223\u2223a>i z\u2223\u2223 = \u03b31 +\u221a2 \u2016h\u2016 ,\nwe obtain the inner and outer bounds[ \u00b1 ( 1 + \u221a 2 )\u22121 \u03b3 \u2016h\u2016 ] \u2286 I1, I2 \u2286 [ \u00b1 \u03b3 \u2016h\u2016 ] .\nSetting \u03b31 := \u03b31+\u221a2 gives( Di,1\u03b31 \u2229 Ei,1 ) \u222a ( Di,2\u03b31 \u2229 Ei,1 ) \u2286 D\u03b3 \u2229 Ei,1 \u2286 ( Di,1\u03b3 \u2229 Ei,1 ) \u222a ( Di,2\u03b3 \u2229 Ei,1 ) .\nProceeding with the same argument, we can derive exactly the same inner and outer bounds in the regime where a>i z < 0, concluding the proof.\nA.2 Proof of Lemma 4 By homogeneity, it suffices to establish the claim for the case where both h and z are unit vectors.\nSuppose for the moment that h and z are statistically independent from {ai}. We introduce two auxiliary Lipschitz functions approximating indicator functions:\n\u03c7z (\u03c4) :=  1, if |\u03c4 | \u2208 [\u221a 1.01\u03b1lbz , \u221a 0.99\u03b1ubz ] ; \u2212100 ( \u03b1ubz )\u22122 \u03c42 + 100, if |\u03c4 | \u2208 [\u221a 0.99\u03b1ubz , \u03b1 ub z ] ; 100 ( \u03b1lbz )\u22122 \u03c42 \u2212 100, if |\u03c4 | \u2208 [ \u03b1lbz , \u221a 1.01\u03b1lbz ] ;\n0, else.\n(114)\n\u03c7h (\u03c4) :=  1, if |\u03c4 | \u2208 [ 0, \u221a 0.99\u03b3 ] ; \u2212 100\u03b32 \u03c42 + 100, if |\u03c4 | \u2208 [\u221a 0.99\u03b3, \u03b3 ] ;\n0, else. (115)\nSince h and z are assumed to be unit vectors, these two functions obey\n0 \u2264 \u03c7z ( a>i z ) \u2264 1Ei1 , and 0 \u2264 \u03c7h ( a>i h ) \u2264 1Di,1\u03b3 (116)\nand thus,\n1\nm m\u2211 i=1 ( a>i h )2 1Ei1\u2229D i,1 \u03b3 \u2265 1 m m\u2211 i=1 (a>i h) 2\u03c7z(a > i z)\u03c7h(a > i h). (117)\nWe proceed to lower bound 1m \u2211m i=1 ( a>i h )2 \u03c7z ( a>i z ) \u03c7h ( a>i h ) .\nFirstly, to compute the mean of (a>i h)2\u03c7z(a>i z)\u03c7h(a>i h), we introduce an auxiliary orthonormal matrix\nUz =\n[ z>/ \u2016z\u2016\n...\n] (118)\nwhose first row is along the direction of z, and set\nh\u0303 := Uzh, and a\u0303i := Uzai. (119)\nAlso, denote by a\u0303i,1 (resp. h\u03031) the first entry of a\u0303i (resp. h\u0303), and a\u0303i,\\1 (resp. h\u0303\\1) the remaining entries of a\u0303i (resp. h\u0303), and let \u03be \u223c N (0, 1). We have\nE [ ( a>i h )2 \u03c7z ( a>i z ) \u03c7h ( a>i h ) ] \u2265 E [ (a>i h) 2\u03c7z ( a>i z ) ] \u2212 E [( a>i h )2 ( 1\u2212 \u03c7h ( a>i h ))] \u2265 E [( a\u0303i,1h\u03031 )2 \u03c7z ( a>i z )] + E [( a\u0303>i,\\1h\u0303\\1\n)2]E [\u03c7z (a>i z)]\u2212 \u2016h\u20162 E [\u03be21{|\u03be|>\u221a0.99\u03b3}] \u2265 |h\u03031|2(1\u2212 \u03b61) + \u2016h\u0303\\1\u20162(1\u2212 \u03b61)\u2212 \u03b62\u2016h\u20162 (120) \u2265 (1\u2212 \u03b61 \u2212 \u03b62) \u2016h\u20162 ,\nwhere the identity (120) arises from (67) and (68). Since ( a>i h )2 \u03c7z ( a>i z ) \u03c7h ( a>i h ) is bounded in magnitude by \u03b32 \u2016h\u20162, it is a sub-Gaussian random variable with sub-Gaussian norm O(\u03b32 \u2016h\u20162). Apply the Hoeffdingtype inequality [44, Proposition 5.10] to deduce that for any > 0,\n1\nm m\u2211 i=1 ( a>i h )2 \u03c7z ( a>i z ) \u03c7h ( a>i h ) \u2265 E [( a>i h )2 \u03c7z ( a>i z ) \u03c7h ( a>i h )] \u2212 \u2016h\u20162 (121)\n\u2265 (1\u2212 \u03b61 \u2212 \u03b62 \u2212 ) \u2016h\u20162 (122)\nwith probability at least 1\u2212 exp(\u2212\u2126( 2m)). The next step is to obtain uniform control over all unit vectors, for which we adopt a basic version of an\n-net argument. Specifically, we construct an -net N with cardinality |N | \u2264 (1 + 2/ )2n (cf. [44]) such that for any (h, z) with \u2016h\u2016 = \u2016z\u2016 = 1, there exists a pair h0, z0 \u2208 N satisfying \u2016h\u2212 h0\u2016 \u2264 and \u2016z \u2212 z0\u2016 \u2264 . Now that we have discretized the unit spheres using a finite set, taking the union bound gives\n1\nm m\u2211 i=1 ( a>i h0 )2 \u03c7z ( a>i z0 ) \u03c7h ( a>i h0 ) \u2265 (1\u2212 \u03b61 \u2212 \u03b62 \u2212 ) \u2016h0\u20162 , \u2200h0, z0 \u2208 N (123)\nwith probability at least 1\u2212 (1 + 2/ )2n exp(\u2212\u2126( 2m)). Define f1(\u00b7) and f2(\u00b7) such that f1(\u03c4) := \u03c4\u03c7h( \u221a \u03c4) and f2(\u03c4) := \u03c7z( \u221a \u03c4), which are both bounded functions with Lipschitz constant O(1). This guarantees that for each unit vector pair h and z,\u2223\u2223\u2223(a>i h)2 \u03c7z (a>i z)\u03c7h (a>i h)\u2212 (a>i h0)2 \u03c7z (a>i z0)\u03c7h (a>i h0)\u2223\u2223\u2223 \u2264 |\u03c7h ( a>i z ) | \u00b7 | ( a>i h )2 \u03c7h ( a>i h ) \u2212 ( a>i h0 )2 \u03c7h ( a>i h0 ) |+ |(a>i h0)2\u03c7h ( a>i h0 ) | \u00b7 |\u03c7h ( a>i z ) \u2212 \u03c7h ( a>i z0 ) |\n\u2264 |\u03c7h ( a>i z ) | \u00b7 \u2223\u2223f1(|a>i h|2)\u2212 f1(|a>i h0|2)\u2223\u2223+ \u2223\u2223(a>i h0)2\u03c7h (a>i h0) \u2223\u2223 \u00b7 \u2223\u2223f2 (|a>i z|2)\u2212 f2 (|a>i z0|2) \u2223\u2223\n. \u2223\u2223(a>i h)2 \u2212 (a>i h0)2\u2223\u2223+ (a>i z)2 \u2212 (a>i z0)2\u2223\u2223.\nConsequently, there exists some universal constant c3 > 0 such that\u2223\u2223\u2223 1 m m\u2211 i=1 ( a>i h )2 \u03c7z ( a>i z ) \u03c7h ( a>i h ) \u2212 1 m m\u2211 i=1 ( a>i h0 )2 \u03c7z ( a>i z0 ) \u03c7h ( a>i h0 ) \u2223\u2223\u2223 . 1\nm \u2225\u2225\u2225A(hh> \u2212 h0h>0 )\u2225\u2225\u2225 1 + 1 m \u2225\u2225A(zz> \u2212 z0z>0 )\u2225\u22251 (i) \u2264 c3\n{\u2225\u2225hh> \u2212 h0h>0 \u2225\u2225F + \u2225\u2225zz> \u2212 z0z>0 \u2225\u2225F} (ii) \u2264 2.5c3\n{\u2225\u2225h\u2212 h0\u2225\u2225 \u00b7 \u2225\u2225h\u2225\u2225+ \u2225\u2225z \u2212 z0\u2225\u2225 \u00b7 \u2225\u2225z\u2225\u2225} \u2264 5c3 , where (i) results from Lemma 1, and (ii) arises from Lemma 2 whenever < 1/2.\nWith the assertion (123) in place, we see that with high probability,\n1\nm m\u2211 i=1 ( a>i h )2 \u03c7z ( a>i z ) \u03c7h ( a>i h ) \u2265 (1\u2212 \u03b61 \u2212 \u03b62 \u2212 (5c3 + 1) ) \u2016h\u20162\nfor all unit vectors h and z. Since can be arbitrary, putting this and (117) together completes the proof.\nA.3 Proof of Lemma 5 The proof makes use of standard concentration of measure and covering arguments, and it suffices to restrict our attention to unit vectors h. We find it convenient to work with an auxiliary function\n\u03c72 (\u03c4) =  |\u03c4 | 32 , if |\u03c4 | \u2264 \u03b32, \u2212\u03b3 ( |\u03c4 | \u2212 \u03b32 ) + \u03b33, if \u03b32 < |\u03c4 | \u2264 2\u03b32,\n0, else.\nApparently, \u03c72 (\u03c4) is a Lipschitz function of \u03c4 with Lipschitz norm O (\u03b3). Recalling the definition of Di,1\u03b3 , we see that each summand is bounded above by\n|a>i h|3 1Di,1\u03b3 \u2264 \u03c72 ( |a>i h|2 ) .\nFor each fixed h and > 0, applying the Bernstein inequality [44, Proposition 5.16] gives\n1\nm m\u2211 i=1 \u2223\u2223a>i h\u2223\u22233 1Di,1\u03b3 \u2264 1m m\u2211 i=1 \u03c72 (\u2223\u2223a>i h\u2223\u22232) \u2264 E [\u03c72 (\u2223\u2223a>i h\u2223\u22232)]+ \u2264 E\n[ \u2223\u2223a>i h\u2223\u22233 ]+ = \u221a8/\u03c0 + with probability exceeding 1\u2212 exp ( \u2212\u2126 ( 2m )) .\nFrom [44, Lemma 5.2], there exists an -net N of the unit sphere with cardinality |N | \u2264 ( 1 + 2 )n. For each h, suppose that \u2016h0 \u2212 h\u2016 \u2264 for some h0 \u2208 N . The Lipschitz property of \u03c72 implies\n1\nm m\u2211 i=1 { \u03c72 (\u2223\u2223a>i h\u2223\u22232)\u2212 \u03c72 (\u2223\u2223a>i h0\u2223\u22232)} . 1m m\u2211 i=1 \u2223\u2223\u2223\u2223\u2223a>i h\u2223\u22232 \u2212 \u2223\u2223a>i h0\u2223\u22232\u2223\u2223\u2223 (i) \u2016h\u2212 h0\u2016 \u2016h\u2016 , where (i) arises by combining Lemmas 1 and 2. This demonstrates that with high probability,\n1\nm m\u2211 i=1 \u2223\u2223a>i h\u2223\u22233 1Di,1\u03b3 \u2264 1m m\u2211 i=1 \u03c72 ( |a>i h|2 ) \u2264 \u221a 8/\u03c0 +O ( )\nfor all unit vectors h, as claimed.\nA.4 Proof of Lemma 6 Without loss of generality, the proof focuses on the case where \u2016h\u2016 = 1. Fix an arbitrary small constant \u03b4 > 0. One can eliminate the difficulty of handling the discontinuous indicator functions by working with the following auxiliary function\n\u03c73 (\u03c4, \u03b3) :=  1, if \u221a \u03c4 \u2265 \u03c8lb (\u03b3) ; 100\u03c4 \u03c82lb(\u03b3) \u2212 99, if \u221a\u03c4 \u2208 [\u221a 0.99\u03c8lb (\u03b3) , \u03c8lb (\u03b3) ] ;\n0, else. (124)\nHere, \u03c8lb (\u00b7) is a piecewise constant function defined as\n\u03c8lb (\u03b3) := (1 + \u03b4) b log \u03b3log(1+\u03b4)c ,\nwhich clearly satisfy \u03b31+\u03b4 \u2264 \u03c8lb (\u03b3) \u2264 \u03b3. Such a function is useful for our purpose since for any 0 < \u03b4 \u2264 0.005,\n1{|a>i h|\u2265\u03b3} \u2264 \u03c73 ( \u2223\u2223a>i h\u2223\u22232 , \u03b3) \u2264 1{|a>i h|\u2265\u221a0.99\u03c8lb(\u03b3)} \u2264 1{|a>i h|\u22650.99\u03b3}. (125)\nFor any fixed unit vector h, the above argument leads to an upper tail estimate: for any 0 < t \u2264 1,\nP { \u03c73 ( \u2223\u2223a>i h\u2223\u22232 , \u03b3) \u2265 t} \u2264 P{1{|a>i h|\u22650.99\u03b3} \u2265 t} = P{1{|a>i h|\u22650.99\u03b3} = 1}\n= 2 \u02c6 \u221e 0.99\u03b3 \u03c6 (x) dx \u2264 2 0.99\u03b3 \u03c6 (0.99\u03b3) , (126)\nwhere \u03c6(x) is the density of a standard normal, and (126) follows from the tail bound \u00b4\u221e x \u03c6(x)dx \u2264 1x\u03c6 (x)\nfor all x > 0. This implies that when \u03b3 \u2265 2, both \u03c73 ( |a>i h|2, \u03b3 ) and 1{|a>i h|\u22650.99\u03b3} are sub-exponential with sub-exponential norm O(\u03b3\u22122) (cf. [44, Definition 5.13]). We apply the Bernstein-type inequality for the sum\nof sub-exponential random variables [44, Corollary 5.17], which indicates that for any fixed h and \u03b3 as well as any sufficiently small \u2208 (0, 1),\n1\nm m\u2211 i=1 \u03c73 ( \u2223\u2223a>i h\u2223\u22232 , \u03b3) \u2264 1m m\u2211 i=1 1{|a>i h|\u22650.99\u03b3} \u2264 E [ 1{|a>i h|\u22650.99\u03b3} ] + 1 \u03b32\n\u2264 2 0.99\u03b3\nexp ( \u22120.49\u03b32 ) + 1\n\u03b32 holds with probability exceeding 1\u2212 exp ( \u2212\u2126( 2m) ) .\nWe now proceed to obtain uniform control over all h and 2 \u2264 \u03b3 \u2264 2n. To begin with, we consider all 2 \u2264 \u03b3 \u2264 m and construct an -net N over the unit sphere such that: (i) |N | \u2264 ( 1 + 2 )n; (ii) for any h with \u2016h\u2016 = 1, there exists a unit vector h0 \u2208 N obeying \u2016h\u2212 h0\u2016 \u2264 . Taking the union bound gives the following: with probability at least 1\u2212 logmlog(1+\u03b4) ( 1 + 2 )n exp(\u2212\u2126( 2m)),\n1\nm m\u2211 i=1 \u03c73 ( \u2223\u2223a>i h0\u2223\u22232 , \u03b30) \u2264 (0.495\u03b30)\u22121 exp (\u22120.49\u03b320)+ \u03b3\u221220\nholds simultaneously for all h0 \u2208 N and \u03b30 \u2208 { (1 + \u03b4) k | 1 \u2264 k \u2264 logmlog(1+\u03b4) } .\nNote that \u03c73 (\u03c4, \u03b30) is a Lipschitz function in \u03c4 with the Lipschitz constant bounded above by 100\u03c82lb(\u03b30) .\nWith this in mind, for any (h, \u03b3) with \u2016h\u2016 = 1 and \u03b30 := (1 + \u03b4)k \u2264 \u03b3 < (1 + \u03b4)k+1, one has\u2223\u2223\u2223\u03c73( \u2223\u2223a>i h0\u2223\u22232 , \u03b30)\u2212 \u03c73( \u2223\u2223a>i h\u2223\u22232 , \u03b3)\u2223\u2223\u2223 = \u2223\u2223\u2223\u03c73( \u2223\u2223a>i h0\u2223\u22232 , \u03b30)\u2212 \u03c73( \u2223\u2223a>i h\u2223\u22232 , \u03b30)\u2223\u2223\u2223 \u2264 100\n\u03c82lb (\u03b30) \u2223\u2223\u2223\u2223\u2223a>i h\u2223\u22232 \u2212 \u2223\u2223a>i h0\u2223\u22232\u2223\u2223\u2223 . It then follows from Lemmas 1-2 that\n1\nm \u2223\u2223\u2223\u2223\u2223 m\u2211 i=1 \u03c73 (\u2223\u2223a>i h0\u2223\u22232 , \u03b30)\u2212 m\u2211 i=1 \u03c73 (\u2223\u2223a>i h\u2223\u22232 , \u03b3) \u2223\u2223\u2223\u2223\u2223 \u2264 100\u03c82lb (\u03b30) 1m \u2225\u2225\u2225A(hh> \u2212 h0h>0 )\u2225\u2225\u2225 1\n\u2264 250 (1 + \u03b4) 2\n\u03b32 \u2016h\u2212 h0\u2016\u2016h\u2016 \u2264\n250(1 + \u03b4)2\n\u03b32 .\nPutting the above results together gives that for all 2 \u2264 \u03b3 \u2264 (1 + \u03b4) logm log(1+\u03b4) = m,\n1\nm m\u2211 i=1 \u03c73 (\u2223\u2223a>i h\u2223\u22232 , \u03b3) \u2264 1m m\u2211 i=1 \u03c73 (\u2223\u2223a>i h0\u2223\u22232 , \u03b30)+ 250 (1 + \u03b4)2\u03b32 \u2264 1\n0.495\u03b30 exp\n( \u22120.49\u03b320 ) + 251 (1 + \u03b4) 2\n\u03b32\n\u2264 1 0.49\u03b3\nexp ( \u22120.485\u03b32 ) + 251 (1 + \u03b4) 2\n\u03b32\nwith probability exceeding 1\u2212 logmlog(1+\u03b4) ( 1 + 2 )n exp ( \u2212c 2m ) . This establishes (72) for all 2 \u2264 \u03b3 \u2264 m.\nIt remains to deal with the case where \u03b3 > m. To this end, we rely on the following observation:\n1\nm m\u2211 i=1 1{|a>i h|\u2265m} \u2264 1 m m\u2211 i=1 \u2223\u2223a>i h\u2223\u22232 m2 (i) \u2264 1 + \u03b4 m2 \u2016h\u20162 1 m , \u2200h with \u2016h\u2016 = 1,\nwhere (i) comes from [12, Lemmas 3.1]. This basically tells us that with high probability, none of the indicator variables can be equal to 1. Consequently, 1m \u2211m i=1 1{|a>i h|\u2265m} = 0, which proves the claim.\nA.5 Proof of Lemma 7 Fix \u03b4 > 0. Recalling the notation vi := 2 {\n2a>i h\u2212 |a>i h|2 a>i z } 1Ei1\u2229Ei2 , we see from the expansion (63) that\u2225\u2225\u2225 1\nm \u2207tr`(z) \u2225\u2225\u2225 = \u2225\u2225\u2225 1 m A>v \u2225\u2225\u2225 \u2264 1 m \u2016A\u2016 \u00b7 \u2016v\u2016 \u2264 (1 + \u03b4) \u2016v\u2016\u221a m (127)\nas soon as m \u2265 c1n for some sufficiently large c1 > 0. Here, the norm estimate \u2016A\u2016 \u2264 \u221a m (1 + \u03b4) arises from standard random matrix results [44, Corollary 5.35]. Everything then comes down to controlling \u2016v\u2016. To this end, making use of the inclusion (64) yields\n1 4m \u2016v\u20162 = 1 m m\u2211 i=1 ( 2a>i h\u2212 |a>i h|2 a>i z )2 1Ei1\u2229Ei2 \u2264 1 m m\u2211 i=1 ( 2 \u2223\u2223a>i h\u2223\u2223+ |a>i h|2|a>i z| )2 1Ei1\u2229(D i,1 \u03b34 \u222aDi,2\u03b34 )\n\u2264 1 m m\u2211 i=1 { 4(a>i h) 2 + ( 4|a>i h|3 |a>i z| + |a>i h|4 |a>i z|2 ) 1Ei1\u2229(D i,1 \u03b34 \u222aDi,2\u03b34 ) }\n= 1\nm m\u2211 i=1 { 4 ( a>i h )2 + ( 4 + |a>i h| |a>i z| ) |a>i h|3 |a>i z| ( 1Ei1\u2229D i,1 \u03b34 + 1Ei1\u2229D i,2 \u03b34 )} .\nThe first term is controlled by [12, Lemma 3.1] in such a way that with probability 1\u2212 exp(\u2212\u2126(m)), 1\nm m\u2211 i=1 4 ( a>i h )2 \u2264 4 (1 + \u03b4) \u2016h\u20162 . Turning to the remaining terms, we see from the definition of Di,1\u03b3 and Di,2\u03b3 that\u2223\u2223a>i h\u2223\u2223\u2223\u2223a>i z\u2223\u2223 \u2264 { \u03b3\u2016h\u2016 \u03b1lbz \u2016z\u2016 , on E i1 \u2229 Di,1\u03b3 2 + \u03b3\u2016h\u2016\n\u03b1lbz \u2016z\u2016 , on E i1 \u2229 Di,2\u03b3\n\u2264 {\n1, on E i1 \u2229 Di,1\u03b3 3, on E i1 \u2229 Di,2\u03b3\nas long as \u03b3 \u2264 \u03b1 lb z \u2016z\u2016 \u2016h\u2016 . Consequently, one can bound\n1\nm m\u2211 i=1 ( 4 + |a>i h| |a>i z| ) |a>i h|3 |a>i z| ( 1Ei1\u2229D i,1 \u03b3 + 1Ei1\u2229D i,2 \u03b3 ) \u2264 5\nm m\u2211 i=1 |a>i h|3 |a>i z| 1Ei1\u2229D i,1 \u03b3 + 7 m m\u2211 i=1 |a>i h|3 |a>i z| 1Ei1\u2229D i,2 \u03b3\n\u2264 5 (1 + \u03b4) \u221a\n8/\u03c0\u2016h\u20163 \u03b1lbz \u2016z\u2016 + 7 100 (1 + \u03b4) \u2016h\u20162 ,\nwhere the last inequality follows from (70) and (79). Recall that \u03b34 = 3\u03b1h. Taken together all these bounds lead to the upper bound\n1\n4m \u2016v\u20162 \u2264 (1 + \u03b4)\n{ 4 + 5 \u221a\n8/\u03c0 \u2016h\u2016 \u03b1lbz \u2016z\u2016 + 7 100\n} \u2016h\u20162 \u2264 (1 + \u03b4) { 4 + 5 \u221a 8/\u03c0\n3\u03b1h +\n7\n100\n} \u2016h\u20162\nwhenever \u2016h\u2016\u2016z\u2016 \u2264 min { \u03b1lbz 3\u03b1h , \u03b1lbz 6 , \u221a 98/3(\u03b1lbz ) 2 2\u03b1ubz +\u03b1 lb z , 111 } . Substituting this into (127) completes the proof."}, {"heading": "B Proofs for Section 7", "text": "B.1 Proof of Lemma 8 Firstly, we collect a few results on the magnitudes of a>i x (1 \u2264 i \u2264 m) that will be useful in constructing the hypotheses. Observe that for any given x and any sufficiently large m,\nP {\nmin 1\u2264i\u2264m \u2223\u2223a>i x\u2223\u2223 \u2265 1m logm \u2016x\u2016 } = ( P { |a>i x| \u2265\n1\nm logm \u2016x\u2016\n})m \u2265 (\n1\u2212 2\u221a 2\u03c0\n1\nm logm\n)m \u2265 1\u2212 o(1).\nBesides, since E [ 1{|a>i x|\u2264 \u2016x\u20165 logm} ] \u2264 1\u221a 2\u03c0 2 5 logm \u2264 15 logm , applying Hoeffding\u2019s inequality yields\nP {\u2211m\ni=1 1{|a>i x|\u2264 \u2016x\u20165 logm} >\nm\n4 logm } = P { 1\nm \u2211m i=1 ( 1{|a>i x|\u2264 \u2016x\u20165 logm} \u2212 E [ 1{|a>i x|\u2264 \u2016x\u20165 logm} ]) >\n1\n20 logm\n} \u2264 exp ( \u2212\u2126 ( m\nlog2m\n)) .\nTo summarize, with probability 1\u2212 o(1), one has\nmin1\u2264i\u2264m \u2223\u2223a>i x\u2223\u2223 \u2265 1m logm\u2016x\u2016; (128)\u2211m\ni=1 1{|a>i x|\u2264 \u2016x\u2016logm} \u2264\nm\n4 logm := k. (129)\nIn the sequel, we will first produce a setM1 of exponentially many vectors surrounding x in such a way that every pair is separated by about the same distance, and then verify that a non-trivial fraction of M1 obeys (104). Without loss of generality, we assume that x takes the form x = [b, 0, \u00b7 \u00b7 \u00b7 , 0]> for some b > 0.\nThe construction of M1 follows a standard random packing argument. Let w = [w1, \u00b7 \u00b7 \u00b7 , wn]> be a random vector with\nwi = xi + 1\u221a 2n zi, 1 \u2264 i \u2264 n,\nwhere zi ind.\u223c N (0, 1). The collectionM1 is then obtained by generating M1 = exp ( n 20 ) independent copies w(l) (1 \u2264 l < M1) of w. For any w(l),w(j) \u2208M1, the concentration inequality [44, Corollary 5.35] gives\nP { 0.5 \u221a n\u2212 1 \u2264 \u221an \u2225\u2225w(l) \u2212w(j)\u2225\u2225 \u2264 1.5\u221an+ 1} \u2265 1\u2212 2 exp (\u2212n/8) ; P { 0.5 \u221a n\u2212 1 \u2264 \u221a 2n \u2225\u2225w(l) \u2212 x\u2225\u2225 \u2264 1.5\u221an+ 1} \u2265 1\u2212 2 exp (\u2212n/8) .\nTaking the union bound over all ( M1 2 ) pairs we obtain\n0.5\u2212 n\u22121/2 \u2264 \u2225\u2225w(l) \u2212w(j)\u2225\u2225 \u2264 1.5 + n\u22121/2, \u2200l 6= j\n1/ \u221a 8\u2212 (2n)\u22121/2 \u2264 \u2225\u2225w(l) \u2212 x\u2225\u2225 \u2264\u221a9/8 + (2n)\u22121/2, 1 \u2264 l \u2264M1 (130)\nwith probability exceeding 1\u2212 2M21 exp ( \u2212n8 ) \u2265 1\u2212 2 exp ( \u2212 n40 ) .\nThe next step is to show that many vectors inM1 satisfy (104). For any given w with r := w \u2212 x, by letting ai,\u22a5 := [ai,2, \u00b7 \u00b7 \u00b7 , ai,n]>, r\u2016 := r1, and r\u22a5 := [r2, \u00b7 \u00b7 \u00b7 , rn]>, we derive\n|a>i r|2 |a>i x|2 \u2264 2|ai,1r\u2016|2 + 2|a>i,\u22a5r\u22a5|2 |ai,1|2 \u2016x\u20162 \u2264 2|r\u2016| 2 \u2016x\u20162 + 2|a>i,\u22a5r\u22a5|2 |ai,1|2 \u2016x\u20162 \u2264 2\u2016r\u2016 2 \u2016x\u20162 + 2|a>i,\u22a5r\u22a5|2 |ai,1|2 \u2016x\u20162 . (131)\nIt then boils down to developing an upper bound on |a > i,\u22a5r\u22a5|2 |ai,1|2\n. This ratio is convenient to work with since the numerator and denominator are stochastically independent. To simplify presentation, we reorder {ai} in a way that\n(m logm)\u22121 \u2016x\u2016 \u2264 \u2223\u2223a>1 x\u2223\u2223 \u2264 \u2223\u2223a>2 x\u2223\u2223 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u2223\u2223a>mx\u2223\u2223 ;\nthis will not affect our subsequent analysis concerning a>i,\u22a5r\u22a5 since it is independent of a > i x.\nTo proceed, we let r(l)\u22a5 consist of all but the first entry of w (l) \u2212x, and introduce the indicator variables\n\u03beli :=  1{\u2223\u2223\u2223a>i,\u22a5r(l)\u22a5 \u2223\u2223\u2223\u2264 1m\u221an\u221212n }, 1 \u2264 i \u2264 k, 1{\u2223\u2223\u2223a>i,\u22a5r(l)\u22a5 \u2223\u2223\u2223\u2264\u221a 2(n\u22121) lognn }, i > k, (132)\nwhere k = m4 logm as before. In words, we divide a > i,\u22a5r (l) \u22a5 , 1 \u2264 i \u2264 m into two groups, with the first group enforcing far more stringent control than the second group. These indicator variables are useful since any\nw(l) obeying \u220fm i=1 \u03be l i = 1 will satisfy (104) when n is sufficiently large. To see this, note that for the first group of indices, \u03beli = 1 requires\u2223\u2223\u2223a>i,\u22a5r(l)\u22a5 \u2223\u2223\u2223 \u2264 1m \u221a n\u2212 1 2n \u2264 2 m \u221a n\u2212 1\u221a n\u2212 2 \u2225\u2225r(l)\u2225\u2225 \u2264 3 m\n\u2225\u2225r(l)\u2225\u2225, 1 \u2264 i \u2264 k, (133) where the second inequality follows from (130). This taken collectively with (128) and (131) yields\u2223\u2223a>i r(l)\u2223\u22232\u2223\u2223a>i x\u2223\u22232 \u2264 2\u2016r(l)\u20162 \u2016x\u20162 + 9 m2 \u2225\u2225r(l)\u2225\u22252 1 m2 log2m \u2016x\u20162 \u2264 (2 + 9 log2m) \u2225\u2225r(l)\u2225\u22252 \u2016x\u20162 , 1 \u2264 i \u2264 k.\nRegarding the second group of indices, \u03beli = 1 gives\u2223\u2223\u2223a>i,\u22a5r(l)\u22a5 \u2223\u2223\u2223 \u2264 \u221a 2 (n\u2212 1) log n n \u2264 \u221a 17 log n \u2225\u2225r(l)\u2225\u2225, i = k + 1, \u00b7 \u00b7 \u00b7 ,m, (134)\nwhere the last inequality again follows from (130). Plugging (134) and (129) into (131) gives\u2223\u2223a>i r(l)\u2223\u22232\u2223\u2223a>i x\u2223\u22232 \u2264 2\u2016r(l)\u20162 \u2016x\u20162 + 17 \u2225\u2225r(l)\u2225\u22252 log n \u2016x\u20162 / log2m \u2264 (2 + 17 log 3m) \u2225\u2225r(l)\u2225\u22252 \u2016x\u20162 , i \u2265 k + 1.\nConsequently, (104) is satified for all 1 \u2264 i \u2264 m. It then suffices to guarantee the existence of exponentially many vectors obeying \u220fm i=1 \u03be l i = 1.\nNote that the first group of indicator variables are quite stringent, namely, for each i only a fraction O(1/m) of the equations could satisfy \u03beli = 1. Fortunately, M1 is exponentially large, and hence even M1/m k is exponentially large. Put formally, we claim that the first group satisfies\nM1\u2211 l=1 k\u220f i=1 \u03beli \u2265 1 2\nM1\n(2\u03c0) k/2 (1 + 4 \u221a k/n)k/2\n( 1\u221a\n2\u03c0m\n)k := M\u03031 (135)\nwith probability exceeding 1\u2212 exp (\u2212\u2126 (k))\u2212 exp(\u2212M\u03031/4). With this claim in place (which will be proved later), one has\nM1\u2211 l=1 k\u220f i=1 \u03beli \u2265 1 2 M1 1 (e2m) k = 1 2 exp (( 1 20 \u2212 k (2 + logm) n ) n ) \u2265 1 2 exp ( 1 25 n )\nwhen n and m/n are sufficiently large. In light of this, we will let M2 be a collection comprising all w(l) obeying \u220fk i=1 \u03be l i = 1, which has size M2 \u2265 12 exp ( 1 25n ) based on the preceding argument. For notational simplicity, it will be assumed that the vectors inM2 are exactly w(j) (1 \u2264 j \u2264M2). We now move on to the second group by examining how many vectors w(j) in M2 further satisfy\u220fm\ni=k+1 \u03be j i = 1. Notably, the above construction of M2 relies only on {ai}1\u2264i\u2264k and is independent of the remaining vectors {ai}i>k. In what follows the argument proceeds conditional on M2 and {ai}1\u2264i\u2264k. Applying the union bound gives\nE [\u2211M2\nj=1\n( 1\u2212 \u220fm i=k+1 \u03beji )] = \u2211M2 j=1 P { \u2203i (k < i \u2264 m) : \u2223\u2223\u2223a>i,\u22a5r(l)\u22a5 \u2223\u2223\u2223 > \u221a 2 (n\u2212 1) log n n } \u2264\nM2\u2211 j=1 m\u2211 i=k+1 P {\u2223\u2223\u2223a>i,\u22a5r(l)\u22a5 \u2223\u2223\u2223 > \u221a 2 (n\u2212 1) log n n } \u2264 M2m 1 n2 .\nThis combined with Markov\u2019s inequality gives\u2211M2 j=1 ( 1\u2212 \u220fm i=k+1 \u03beji ) \u2264 m logm n2 \u00b7M2\nwith probability 1 \u2212 o(1). Putting the above inequalities together suggests that with probability 1 \u2212 o(1), there exist at least (\n1\u2212 m logm n2\n) M2 \u2265 1\n2\n( 1\u2212 m logm\nn2\n) exp ( 1\n25 n\n) \u2265 exp ( n 30 ) vectors in M2 satisfying \u220fm l=k+1 \u03be l i = 1. We then choose M to be the set consisting of all these vectors, which forms a valid collection satisfying the properties of Lemma 8. Finally, the only remaining step is to establish the claim (135). To start with, consider an n\u00d7 k matrix\nB := [b1, \u00b7 \u00b7 \u00b7 , bk] of i.i.d. standard normal entries, and let u \u223c N ( 0, 1nIn ) . Conditional on the {bi\u2019s,\nbu =  b1,u... bk,u  :=  b > 1 u ... b>k u  \u223c N (0, 1 n B>B ) .\nFor sufficiently large m, one has k = m4 logm \u2264 14n. Using [44, Corollary 5.35] we get\u2225\u2225\u2225 1 n B>B \u2212 I \u2225\u2225\u2225 \u2264 4\u221ak/n (136) with probability 1\u2212exp (\u2212\u2126(k)). Thus, for any constant 0 < < 12 , conditional on {bi} and (136) we obtain\nP { k\u22c2 i=1 { |b>i u| \u2264 1 m }} \u2265 (2\u03c0)\u2212 k2 det\u2212 12 ( 1 n B>B )\u02c6 bu\u2208\u03a5 exp ( \u2212 1 2 b>u ( 1 n B>B )\u22121 bu ) dbu\n\u2265 (2\u03c0)\u2212 k2 ( 1 + 4 \u221a k/n )\u2212 k2 \u02c6 bu\u2208\u03a5 exp ( \u2212 1 2 ( 1\u2212 4 \u221a k/n )\u22121 k\u2211 i=1 b2i,u ) dbu (137)\n\u2265 (2\u03c0)\u2212 k2 ( 1 + 4 \u221a k/n )\u2212 k2 (\u221a2\u03c0m)\u2212k, (138) where \u03a5 := {b\u0303 | |b\u0303i| \u2264 m\u22121, 1 \u2264 i \u2264 k} and (137) is a direct consequence from (136).\nWhen it comes to our quantity of interest, the above lower bound (138) indicates that on an event (defined via {ai}) of probability approaching 1, we have\nE [\u2211M1\nl=1 \u220fk i=1 \u03beli ] \u2265 M1 (2\u03c0)\u2212 k 2 ( 1 + 4 \u221a k/n )\u2212 k2 (\u221a 2\u03c0m )\u2212k . (139)\nSince conditional on {ai}, \u220fk i=1 \u03be l i are independent across l, applying the Chernoff-type bound [31, Theorem 4.5] gives \u2211M1 l=1 \u220fk i=1 \u03beli \u2265 M1 2 (2\u03c0) \u2212 k2 ( 1 + 4 \u221a k/n )\u2212 k2 (\u221a 2\u03c0m\n)\u2212k with probability exceeding 1\u2212 exp ( \u2212 18 M1(2\u03c0)k/2(1+4\u221ak/n)k/2 ( 1\u221a 2\u03c0m )k ) . This concludes the proof.\nB.2 Proof of Lemma 9 Before proceeding, we introduce the \u03c72-divergence between two probability measures P and Q as\n\u03c72 (P\u2016Q) := \u02c6 ( dP\ndQ\n)2 dQ\u2212 1. (140)\nIt is well known (e.g. [43, Lemma 2.7]) that\nKL (P\u2016Q) \u2264 log(1 + \u03c72 (P\u2016Q)), (141)\nand hence it suffices to develop an upper bound on the \u03c72 divergence.\nUnder independence, for any w0,w1 \u2208 Rn, the decoupling identity of the \u03c72 divergence [43, Page 96] gives\n\u03c72 (P (y | w1) \u2016 P (y | w0)) = \u220fm\ni=1\n( 1 + \u03c72 (P (yi | w1) \u2016 P (yi | w0)) ) \u2212 1\n= exp (\u2211m i=1 ( |a>i w1|2 \u2212 |a>i w0|2 )2 |a>i w0|2 ) \u2212 1. (142)\nThe preceding identity (142) arises from the following computation: by definition of \u03c72(\u00b7\u2016\u00b7),\n\u03c72 (Poisson (\u03bb1) \u2016 Poisson (\u03bb0)) = {\u2211\u221e\nk=0\n( \u03bbk1 exp (\u2212\u03bb1) )2 \u03bbk0 exp (\u2212\u03bb0) k! } \u2212 1\n= exp ( \u03bb0 \u2212 2\u03bb1 +\n\u03bb21 \u03bb0 ){\u2211\u221e k=0 ( \u03bb21/\u03bb0 )k k! exp ( \u2212 \u03bb 2 1 \u03bb0 )} \u2212 1 = exp ( (\u03bb1 \u2212 \u03bb0)2 \u03bb0 ) \u2212 1.\nSet r := w1 \u2212w0. To summarize,\nKL (P (y | w1) \u2016 P (y | w0)) \u2264 m\u2211 i=1\n( |a>i w1|2 \u2212 |a>i w0|2 )2 |a>i w0|2\n(143)\n\u2264 m\u2211 i=1 \u2223\u2223a>i r\u2223\u22232 (2 \u2223\u2223a>i w0\u2223\u2223+ \u2223\u2223a>i r\u2223\u2223)2 |a>i w0|2\n= m\u2211 i=1 |a>i r|2 ( 8|a>i w0|2 + 2|a>i r|2 |a>i w0|2 ) . (144)\nC Initialization via truncated spectral Method This section demonstrates that the truncated spectral method works whenm n, as stated in the proposition below.\nProposition 3. Fix \u03b4 > 0 and x \u2208 Rn. Consider the model where yi = a>i x+\u03b7i and ai ind.\u223c N (0, I). Suppose that \u2016\u03b7\u2016\u221e \u2264 \u03b5 \u2016x\u2016 2 for some sufficiently small constant \u03b5 > 0. With probability exceeding 1\u2212 exp (\u2212\u2126 (m)), the solution z(0) returned by the truncated spectral method obeys\ndist(z(0),x) \u2264 \u03b4\u2016x\u2016, (145)\nprovided that m > c0n for some constant c0 > 0.\nProof. By homogeneity, it suffices to consider the case where \u2016x\u2016 = 1. Recall from [12, Lemma 3.1] that 1 m \u2211m i=1(a > i x) 2 \u2208 [1\u00b1 \u03b5]\u2016x\u20162. Under the hypothesis \u2016\u03b7\u2016\u221e \u2264 \u03b5\u2016x\u20162, one has 1m \u2016\u03b7\u20161 \u2264 \u03b5\u2016x\u20162, which yields\n1\nm m\u2211 l=1 yl = 1 m m\u2211 l=1 ( a>l x )2 + 1 m m\u2211 l=1 \u03b7l \u2208 [1\u00b1 2\u03b5]\u2016x\u20162\nwith probability 1\u2212 exp(\u2212\u2126(m)). This in turn implies that\n1{|(a>i x)2+\u03b7i|\u2264\u03b12y( 1m \u2211l yl)} \u2264 1{|a>i x|2\u2264\u03b12y( 1m \u2211l yl)+|\u03b7i|} \u2264 1{|a>i x|2\u2264(1+2\u03b5)\u03b12y+\u03b5} 1{|(a>i x)2+\u03b7i|\u2264\u03b12y( 1m \u2211l yl)} \u2265 1{|a>i x|2\u2264\u03b12y( 1m \u2211l yl)\u2212|\u03b7i|} \u2265 1{|a>i x|2\u2264(1\u22122\u03b5)\u03b12y\u2212\u03b5}\nand, hence,\n1\nm m\u2211 i=1 aia > i ( a>i x )2 1{|a>i x|\u2264 \u221a (1\u22122\u03b5)\u03b12y\u2212\u03b5}\ufe38 \ufe37\ufe37 \ufe38\n:=Y 2\nY 1 m m\u2211 i=1 aia > i ( a>i x )2 1{|a>i x|\u2264 \u221a (1+2\u03b5)\u03b12y+\u03b5}\ufe38 \ufe37\ufe37 \ufe38\n:=Y 1\n. (146)\nLetting \u03be \u223c N (0, 1), one can compute\nE [Y 1] = \u03b21xx> + \u03b22I, and E [Y 2] = \u03b23xx> + \u03b24I, (147)\nwhere \u03b21 := E [ \u03be41{|\u03be|\u2264 \u221a (1+2\u03b5)\u03b12y+\u03b5} ] \u2212 E [ \u03be21{|\u03be|\u2264 \u221a (1+2\u03b5)\u03b12y+\u03b5} ] , \u03b22 := E [ \u03be21{|\u03be|\u2264 \u221a (1+2\u03b5)\u03b12y+\u03b5} ] , \u03b23 :=\nE [ \u03be41{|\u03be|\u2264 \u221a (1\u22122\u03b5)\u03b12y\u2212\u03b5} ] \u2212 E [ \u03be21{|\u03be|\u2264 \u221a (1\u22122\u03b5)\u03b12y\u2212\u03b5} ] and \u03b24 := E [ \u03be21{|\u03be|\u2264 \u221a (1\u22122\u03b5)\u03b12y\u2212\u03b5} ] . Recognizing that aia > i ( a>i x )2 1{|a>i x)|\u2264c} can be rewritten as bib > i for some sub-Gaussian vector bi := ai ( a>i x ) 1{|a>i x)|\u2264c}, we apply standard results on random matrices with non-isotropic sub-Gaussian rows [44, Equation (5.26)] to deduce \u2016Y 1 \u2212 E [Y 1]\u2016 \u2264 \u03b4, \u2016Y 2 \u2212 E [Y 2]\u2016 \u2264 \u03b4 (148) with probability 1 \u2212 exp (\u2212\u2126 (m)), provided that m/n exceeds some large constant. Besides, when \u03b5 is sufficiently small, one further has \u2016E [Y 1]\u2212 E [Y 2] \u2016 \u2264 \u03b4. These taken together with (146) give\n\u2016Y \u2212 \u03b21xx> \u2212 \u03b22I\u2016 \u2264 3\u03b4. (149)\nFix \u03b4\u0303 > 0. With (149) in place, repeating the same proof arguments as in [11, Section 7.8] (which we omit in the current paper) and taking \u03b4, \u03b5 to be sufficiently small, we obtain\ndist(z(0),x) \u2264 \u03b4\u0303 (150)\nas long as m/n is sufficiently large, as claimed.\nWe now justify that the Poisson model (4) satisfies the condition \u2016\u03b7\u2016 \u2264 \u03b5\u2016x\u20162 whenever \u2016x\u2016 \u2265 log1.5m. Suppose that \u00b5i = (a>i x)2 and hence yi \u223c Poisson(\u00b5i). It follows from the Chernoff bound that\nP (yi \u2212 \u00b5i \u2265 \u03c4) \u2264 E [etyi ]\nexp (t(\u00b5i + \u03c4)) =\nexp (\u00b5i (e t \u2212 1))\nexp (t(\u00b5i + \u03c4)) = exp\n( \u00b5i ( et \u2212 t\u2212 1 ) \u2212 t\u03c4 ) , \u2200t \u2265 0.\nTaking \u03c4 = 2\u03b5\u0303\u00b5i and t = \u03b5\u0303 for any 0 \u2264 \u03b5\u0303 \u2264 1 gives\nP (yi \u2212 \u00b5i \u2265 2\u03b5\u0303\u00b5i) \u2264 exp ( \u00b5i ( et \u2212 t\u2212 1\u2212 2\u03b5\u0303t )) (i) \u2264 exp ( \u00b5i ( t2 \u2212 2\u03b5\u0303t )) = exp ( \u2212\u00b5i\u03b5\u03032 ) ,\nwhere (i) follows since et \u2264 1 + t+ t2 (0 \u2264 t \u2264 1). Letting \u03bai = \u00b5i/\u2016x\u20162 and setting \u03b5\u0303 = \u03b5/2\u03bai, we obtain\nP ( yi \u2212 \u00b5i \u2265 \u03b5\u2016x\u20162 ) = P (yi \u2212 \u00b5i \u2265 2\u03b5\u0303\u00b5i) \u2264 exp ( \u2212\u03bai\u2016x\u20162\u03b5\u03032 ) = exp ( \u2212 \u03b5\n2\u2016x\u20162 4\u03bai\n) .\nIn addition, standard results on Gaussian measures indicate that max1\u2264i\u2264m \u03bai . log n. As a consequence, if \u2016x\u20162 & log3m, then \u2016x\u2016 2\n\u03bai & log2m (1 \u2264 i \u2264 m), which further gives\nP ( \u2200i : \u03b7i \u2265 \u03b5\u2016x\u20162 ) = P ( \u2200i : yi \u2212 \u00b5i \u2265 \u03b5\u2016x\u20162 ) \u2264 m exp ( \u2212 \u2126 ( \u03b52 log2m ) ) from the union bound. Similarly, applying the same argument on \u2212yi we get \u03b7i \u2265 \u2212\u03b5\u2016x\u20162 for all i, which together with (151) establish that \u2016\u03b7\u2016\u221e \u2264 \u03b5\u2016x\u20162 (151) with high probability. In conclusion, the claim (145) applies to the Poisson model."}, {"heading": "D Local error contraction with backtracking line search", "text": "In this section, we verify the effectiveness of a backtracking line search strategy by showing local error contraction. To keep it concise, we only sketch the proof for the noiseless case, but the proof extends to the noisy case without much difficulty. Also we do not strive to obtain an optimized constant. For concreteness, we prove the following proposition.\nProposition 4. The claim in Proposition 1 continues to hold if \u03b1h \u2265 6, \u03b1ubz \u2265 5, \u03b1lbz \u2264 0.1, \u03b1p \u2265 5, and\n\u2016h\u2016/\u2016z\u2016 \u2264 tr (152)\nfor some constant tr > 0 independent of n and m.\nNote that if \u03b1h \u2265 6, \u03b1ubz \u2265 5 and \u03b1lbz \u2264 0.1, then the boundary step size \u00b50 given in Proposition 1 satisfies 0.994\u2212 \u03b61 \u2212 \u03b62 \u2212 \u221a\n2/(9\u03c0)\u03b1\u22121h 2 ( 1.02 + 0.665\u03b1\u22121h ) \u2265 0.384. Thus, it suffices to show that the step size obtained by a backtracking line search lies within (0,0.384). For notational convenience, we will set\np := m\u22121\u2207`tr (z) and E i3 := {\u2223\u2223a>i z\u2223\u2223 \u2265 \u03b1lbz \u2016z\u2016 and \u2223\u2223a>i p\u2223\u2223 \u2264 \u03b1p \u2016p\u2016}\nthroughout the rest of the proof. We also impose the assumption\n\u2016p\u2016 / \u2016z\u2016 \u2264 (153)\nfor some sufficiently small constant > 0, so that \u2223\u2223a>i p\u2223\u2223 / \u2223\u2223a>i z\u2223\u2223 is small for all non-truncated terms. It is self-evident from (80) that in the regime under study, one has\n\u2016p\u2016 \u2265 2 { 1.99\u2212 2 (\u03b61 + \u03b62)\u2212 \u221a 8/\u03c0(3\u03b1h) \u22121 \u2212 o (1) } \u2016h\u2016 \u2265 3.64 \u2016h\u2016 . (154)\nTo start with, consider three scalars h, b, and \u03b4. Setting b\u03b4 := (b+\u03b4)2\u2212b2\nb2 , we get\n(b+ h) 2 log (b+ \u03b4)\n2\nb2 \u2212 (b+ \u03b4)2 + b2 = (b+ h)2 log (1 + b\u03b4)\u2212 b2b\u03b4\n(i) \u2264 (b+ h)2 { b\u03b4 \u2212 0.4875b2\u03b4 } \u2212 b2b\u03b4 = ((b+ h)2 \u2212 b2)b\u03b4 \u2212 0.4875 (b+ h)2 b2\u03b4 = h\u03b4 (2 + h/b) (2 + \u03b4/b)\u2212 0.4875 (1 + h/b)2 |\u03b4 (2 + \u03b4/b)|2\n= 4h\u03b4 + 2h2\u03b4\nb +\n2h\u03b42 b + h2\u03b42 b2 \u2212 0.4875\u03b42\n( 1 + h\nb\n)2( 2 + \u03b4\nb\n)2 , (155)\nwhere (i) follows from the inequality log (1 + x) \u2264 x\u2212 0.4875x2 for sufficiently small x. To further simplify the bound, observe that\n\u03b42 ( 1 + h\nb\n)2( 2 + \u03b4\nb\n)2 \u2265 4\u03b42 ( 1 + h\nb\n)2 + \u03b42 ( 1 + h\nb\n)2 4\u03b4\nb and\n2h\u03b42 b + h2\u03b42 b2 =\n(( 1 + h\nb\n)2 \u2212 1 ) \u03b42.\nPlugging these two identities into (155) yields\n(155) \u2264 4h\u03b4 + 2h 2\u03b4 b \u2212 ( 0.95 ( 1 + h b )2 + 1 ) \u03b42 \u2212 0.4875\u03b42 ( 1 + h b )2 4\u03b4 b\n\u2264 4h\u03b4 \u2212 1.95\u03b42 + 2h 2 |\u03b4| |b| + 1.9|h| |b| \u03b4 2 + 1.95 \u2223\u2223\u03b43\u2223\u2223 |b| ( 1 + h b )2 .\nReplacing respectively b, \u03b4, and h with a>i z, \u03c4a>i p, and \u2212a>i h, one sees that the log-likelihood `i (z) =\nyi log(|a>i z|2)\u2212 |a>i z|2 obeys\n`i (z + \u03c4p)\u2212 `i (z) = yi log \u2223\u2223a>i (z + \u03c4p)\u2223\u22232\u2223\u2223a>i z\u2223\u22232 \u2212 \u2223\u2223a>i (z + \u03c4p)\u2223\u22232 + \u2223\u2223a>i z\u2223\u22232 \u2264 \u22124\u03c4 ( a>i h ) ( a>i p\n)\ufe38 \ufe37\ufe37 \ufe38 :=I1,i \u2212 1.95\u03c42 ( a>i p )2\ufe38 \ufe37\ufe37 \ufe38 :=I2,i\n+ 2\u03c4 ( a>i h )2 \u2223\u2223a>i p\u2223\u2223\u2223\u2223a>i z\u2223\u2223\ufe38 \ufe37\ufe37 \ufe38 :=I3,i + 1.9\u03c42 \u2223\u2223a>i h\u2223\u2223\u2223\u2223a>i z\u2223\u2223 (a>i p)2\ufe38 \ufe37\ufe37 \ufe38 :=I4,i\n+ 1.95\u03c43 \u2223\u2223a>i p\u2223\u22233\u2223\u2223a>i z\u2223\u2223 ( 1\u2212 a > i h a>i z )2 \ufe38 \ufe37\ufe37 \ufe38\n:=I5,i\n.\nThe next step is then to bound each of these terms separately. Most of the following bounds are straightforward consequences from [12, Lemma 3.1] combined with the truncation rule. For the first term, applying the AM-GM inequality we get\n1\nm m\u2211 i=1 I1,i1Ei3 \u2264 4\u03c4 3.64m m\u2211 i=1 { 3.642 2 ( a>i h )2 + 1 2 ( a>i p )2} \u2264 4\u03c4 (1 + \u03b4) 3.64 { 3.642 2 \u2016h\u20162 + 1 2 \u2016p\u20162 } .\nSecondly, it follows from Lemma 4 that\n1\nm m\u2211 i=1 I2,i1Ei3 = \u22121.95\u03c4 2 1 m m\u2211 i=1 ( a>i p )2 1Ei3 \u2264 \u22121.95 ( 1\u2212 \u03b6\u03031 \u2212 \u03b6\u03032 ) \u03c42 \u2016p\u20162 ,\nwhere \u03b6\u03031 := max{E [ \u03be21{|\u03be|\u2264\u221a1.01\u03b1lbz } ] ,E [ 1{|\u03be|\u2264\u221a1.01\u03b1lbz } ] } and \u03b6\u03032 := E [ \u03be21{|\u03be|>\u221a0.99\u03b1h} ] . The third term is controlled by 1\nm m\u2211 i=1 I3,i1Ei3 \u2264 2\u03c4 \u03b1p \u2016p\u2016 \u03b1lbz \u2016z\u2016\n{ 1\nm m\u2211 i=1 ( a>i h\n)2} . \u03c4 \u2016h\u20162 .\nFourthly, it arises from the AM-GM inequality that\n1\nm m\u2211 i=1 I4,i1Ei3 \u2264 1.9\u03c42\u03b1p \u2016p\u2016 \u03b1lbz \u2016z\u2016 1 m m\u2211 i=1 \u2223\u2223a>i h\u2223\u2223 \u2223\u2223a>i p\u2223\u2223 . \u03c42 1m m\u2211 i=1 { 2 \u2223\u2223a>i h\u2223\u22232 + 18 \u2223\u2223a>i p\u2223\u22232 } . \u03c42 \u2016p\u20162 .\nFinally, the last term is bounded by\n1\nm m\u2211 i=1 I5,i1Ei3 \u2264 1 m m\u2211 i=1 1.95\u03c43 \u2223\u2223a>i p\u2223\u22233\u2223\u2223a>i z\u2223\u2223 ( a>i x a>i z )2 \u2264 1.95\u03c4 3\u03b13p \u2016p\u20163 (\u03b1lbz ) 3 \u2016z\u20163 1 m m\u2211 i=1 ( a>i x )2 . \u03c43 \u2016x\u20162 \u2016z\u20162 \u2016p\u20162 .\nUnder the hypothesis (154), we can further derive 1m \u2211m i=1 I1,i1Ei3 \u2264 \u03c4 (1.1 + \u03b4) \u2016p\u2016\n2. Putting all the above bounds together yields that the truncated objective function is majorized by\n1\nm m\u2211 i=1 {`i (z + \u03c4p)\u2212 `i (z)}1Ei3 \u2264 1 m m\u2211 i=1 (I1,i + I2,i + I3,i + I4,i + I5,i)1Ei3\n\u2264 \u03c4 (1.1 + \u03b4) \u2016p\u20162 \u2212 1.95 ( 1\u2212 \u03b6\u03031 \u2212 \u03b6\u03032 ) \u03c42 \u2016p\u20162 + \u03c4 \u0303 \u2016p\u20162\n= { \u03c4 (1.1 + \u03b4)\u2212 1.95 ( 1\u2212 \u03b6\u03031 \u2212 \u03b6\u03032 ) \u03c42 + \u03c4 \u0303 } \u2016p\u20162 (156)\nfor some constant \u0303 > 0 that is linear in . Note that the backtracking line search seeks a point satisfying 1m \u2211m i=1 {`i (z + \u03c4p)\u2212 `i (z)}1Ei3 \u2265 1 2\u03c4 \u2016p\u2016 2. Given the above majorization (156), this search criterion is satisfied only if\n\u03c4/2 \u2264 \u03c4 (1.1 + \u03b4)\u2212 1.95(1\u2212 \u03b6\u03031 \u2212 \u03b6\u03032)\u03c42 + \u03c4 \u0303\nor, equivalently,\n\u03c4 \u2264 0.6 + \u03b4 + \u0303 1.95(1\u2212 \u03b6\u03031 \u2212 \u03b6\u03032) := \u03c4ub.\nTaking \u03b4 and \u0303 to be sufficiently small, we see that \u03c4 \u2264 \u03c4ub \u2264 0.384, provided that \u03b1lbz \u2264 0.1, \u03b1ubz \u2265 5, \u03b1h \u2265 6, and \u03b1p \u2265 5.\nUsing very similar arguments, one can also show that 1m \u2211m i=1 {`i (z + \u03c4p)\u2212 `i (z)}1Ei3 is minorized by a\nsimilar quadratic function, which combined with the stopping criterion 1m \u2211m i=1 {`i (z + \u03c4p)\u2212 `i (z)}1Ei3 \u2265 1 2\u03c4 \u2016p\u2016 2 suggests that \u03c4 is bounded away from 0. We omit this part for conciseness."}, {"heading": "Acknowledgements", "text": "E. C. is partially supported by NSF under grant CCF-0963835 and by the Math + X Award from the Simons Foundation. Y. C. is supported by the same NSF grant. We thank Carlos Sing-Long and Weijie Su for helpful comments about an early version of the manuscript. E. C. is grateful to Xiaodong Li and Mahdi Soltanolkotabi for many discussions about Wirtinger flows."}], "references": [{"title": "Phase retrieval with polarization", "author": ["B. Alexeev", "A.S. Bandeira", "M. Fickus", "D.G. Mixon"], "venue": "SIAM Journal on Imaging Sciences, 7(1):35\u201366", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Simple", "author": ["S. Arora", "R. Ge", "T. Ma", "A. Moitra"], "venue": "efficient, and neural algorithms for sparse coding. arXiv:1503.00778", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Statistical guarantees for the EM algorithm: From population to sample-based analysis", "author": ["S. Balakrishnan", "M.J. Wainwright", "B. Yu"], "venue": "arXiv preprint arXiv:1408.2156", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Painless reconstruction from magnitudes of frame coefficients", "author": ["R. Balan", "B. Bodmann", "P. Casazza", "D. Edidin"], "venue": "Journal of Fourier Analysis and Applications, 15(4):488\u2013501", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Saving phase: Injectivity and stability for phase retrieval", "author": ["A.S. Bandeira", "J. Cahill", "D.G. Mixon", "A.A. Nelson"], "venue": "Applied and Computational Harmonic Analysis, 37(1):106\u2013125", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Multireference alignment using semidefinite programming", "author": ["A.S. Bandeira", "M. Charikar", "A. Singer", "A. Zhu"], "venue": "Conference on Innovations in theoretical computer science, pages 459\u2013470", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Lectures on modern convex optimization", "author": ["A. Ben-Tal", "A. Nemirovski"], "venue": "volume 2. SIAM", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "ROP: Matrix recovery via rank-one projections", "author": ["T. Cai", "A. Zhang"], "venue": "The Annals of Statistics, 43(1):102\u2013138", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Solving quadratic equations via PhaseLift when there are about as many equations as unknowns", "author": ["E.J. Cand\u00e8s", "X. Li"], "venue": "Foundations of Computational Mathematics, 14(5):1017\u20131026", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Phase retrieval from coded diffraction patterns", "author": ["E.J. Cand\u00e8s", "X. Li", "M. Soltanolkotabi"], "venue": "to appear in Applied and Computational Harmonic Analysis", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Phase retrieval via Wirtinger flow: Theory and algorithms", "author": ["E.J. Cand\u00e8s", "X. Li", "M. Soltanolkotabi"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Phaselift: Exact and stable signal recovery from magnitude measurements via convex programming", "author": ["E.J. Cand\u00e8s", "T. Strohmer", "V. Voroninski"], "venue": "Communications on Pure and Applied Mathematics, 66(8):1017\u20131026", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Supplemental materials for: \u201csolving random quadratic systems of equations is nearly as easy as solving linear systems", "author": ["Y. Chen", "E.J. Cand\u00e8s"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Exact and stable covariance estimation from quadratic sampling via convex programming", "author": ["Y. Chen", "Y. Chi", "A.J. Goldsmith"], "venue": "to appear, IEEE Transactions on Information Theory", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Near-optimal joint optimal matching via convex relaxation", "author": ["Y. Chen", "L.J. Guibas", "Q. Huang"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "A convex formulation for mixed regression with two components: Minimax optimal rates", "author": ["Yudong Chen", "Xinyang Yi", "Constantine Caramanis"], "venue": "In Conf. on Learning Theory,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Stable optimizationless recovery from phaseless linear measurements", "author": ["L. Demanet", "P. Hand"], "venue": "Journal of Fourier Analysis and Applications, 20(1):199\u2013221", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Phase retrieval: Stability and recovery guarantees", "author": ["Y.C. Eldar", "S. Mendelson"], "venue": "Applied and Computational Harmonic Analysis, 36(3):473\u2013494", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Phase retrieval by iterated projections", "author": ["V. Elser"], "venue": "JOSA. A, 20(1):40\u201355", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Phase retrieval algorithms: a comparison", "author": ["J.R. Fienup"], "venue": "Applied optics, 21:2758\u20132769", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1982}, {"title": "A practical algorithm for the determination of phase from image and diffraction plane pictures", "author": ["R.W. Gerchberg"], "venue": "Optik, 35:237", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1972}, {"title": "Improved recovery guarantees for phase retrieval from coded diffraction patterns", "author": ["D. Gross", "F. Krahmer", "R. Kueng"], "venue": "arXiv preprint arXiv:1402.6286", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "A partial derandomization of phaselift using spherical designs", "author": ["D. Gross", "F. Krahmer", "R. Kueng"], "venue": "Journal of Fourier Analysis and Applications, 21(2):229\u2013266", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast matrix completion without the condition number", "author": ["Moritz Hardt", "Mary Wootters"], "venue": "Conference on Learning Theory, pages", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Consistent shape maps via semidefinite programming", "author": ["Q. Huang", "L. Guibas"], "venue": "Computer Graphics Forum, 32(5):177\u2013186", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Recovery of sparse 1-D signals from the magnitudes of their Fourier transform", "author": ["K. Jaganathan", "S. Oymak", "B. Hassibi"], "venue": "IEEE ISIT, pages 1473\u20131477", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["P. Jain", "P. Netrapalli", "S. Sanghavi"], "venue": "ACM symposium on Theory of computing, pages 665\u2013674", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Matrix completion from a few entries", "author": ["R.H. Keshavan", "A. Montanari", "S. Oh"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Sparse signal recovery from quadratic measurements via convex programming", "author": ["X. Li", "V. Voroninski"], "venue": "SIAM Journal on Mathematical Analysis, 45(5):3019\u20133033", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Alternating projection", "author": ["S. Marchesin", "Y. Tu", "H. Wu"], "venue": "ptychographic imaging and phase synchronization. arXiv:1402.0550", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Probability and computing", "author": ["M. Mitzenmacher", "E. Upfal"], "venue": "Cambridge University Press", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "Phase retrieval using alternating minimization", "author": ["P. Netrapalli", "P. Jain", "S. Sanghavi"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Non-convex robust PCA", "author": ["P. Netrapalli", "U. Niranjan", "S. Sanghavi", "A. Anandkumar", "P. Jain"], "venue": "Advances in Neural Information Processing Systems, pages 1107\u20131115", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Numerical Optimization (2nd edition)", "author": ["J. Nocedal", "S.J. Wright"], "venue": "Springer", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2006}, {"title": "Compressive phase retrieval via generalized approximate message passing", "author": ["P. Schniter", "S. Rangan"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "GESPAR: Efficient phase retrieval of sparse signals", "author": ["Y. Shechtman", "A. Beck", "Y.C. Eldar"], "venue": "IEEE Transactions on Signal Processing, 62(4):928\u2013938", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Phase retrieval with application to optical imaging", "author": ["Y. Shechtman", "Y.C. Eldar", "O. Cohen", "H.N. Chapman", "J. Miao", "M. Segev"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Sparsity based sub-wavelength imaging with partially incoherent light via quadratic compressed sensing", "author": ["Y. Shechtman", "Y.C. Eldar", "A. Szameit", "M. Segev"], "venue": "Optics express, 19(16)", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Algorithms and Theory for Clustering and Nonconvex Quadratic Programming", "author": ["M. Soltanolkotabi"], "venue": "PhD thesis, Stanford University", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Guaranteed matrix completion via non-convex factorization", "author": ["R. Sun", "Z. Luo"], "venue": "arXiv:1411.8003", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Numerical linear algebra", "author": ["L.N. Trefethen", "D. Bau III"], "venue": "volume 50. SIAM", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1997}, {"title": "Introduction to nonparametric estimation", "author": ["A.B. Tsybakov", "V. Zaiats"], "venue": "volume 11. Springer", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2009}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": "Compressed Sensing, Theory and Applications, pages 210 \u2013 268", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "A", "author": ["I. Waldspurger"], "venue": "d\u2019Aspremont, and S. Mallat. Phase recovery, maxcut and complex semidefinite programming. Mathematical Programming, 149(1-2):47\u201381", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "We propose a novel method, which starting with an initial guess computed by means of a spectral method, proceeds by minimizing a nonconvex functional as in the Wirtinger flow approach [11].", "startOffset": 184, "endOffset": 188}, {"referenceID": 6, "context": "However simple this formulation may seem, even checking whether a solution to (2) exists or not is known to be NP complete [7].", "startOffset": 123, "endOffset": 126}, {"referenceID": 19, "context": "Moving from combinatorial optimization to the physical sciences, one application of paramount importance is the phase retrieval [20, 21] problem, which permeates through a wide spectrum of techniques including X-ray crystallography, diffraction imaging, and microscopy.", "startOffset": 128, "endOffset": 136}, {"referenceID": 20, "context": "Moving from combinatorial optimization to the physical sciences, one application of paramount importance is the phase retrieval [20, 21] problem, which permeates through a wide spectrum of techniques including X-ray crystallography, diffraction imaging, and microscopy.", "startOffset": 128, "endOffset": 136}, {"referenceID": 36, "context": "We refer to [37] for in-depth reviews of this subject.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 8, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 11, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 13, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 15, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 16, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 21, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 22, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 25, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 28, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 37, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 43, "context": "To alleviate this computational intractability, several convex surrogates have been proposed that work particularly well when the design vectors ai are chosen at random [8, 9, 12, 14, 16, 17, 22, 23, 26, 29, 38, 45].", "startOffset": 169, "endOffset": 215}, {"referenceID": 10, "context": "[11, 19, 21, 30, 32, 35, 36]).", "startOffset": 0, "endOffset": 28}, {"referenceID": 18, "context": "[11, 19, 21, 30, 32, 35, 36]).", "startOffset": 0, "endOffset": 28}, {"referenceID": 20, "context": "[11, 19, 21, 30, 32, 35, 36]).", "startOffset": 0, "endOffset": 28}, {"referenceID": 29, "context": "[11, 19, 21, 30, 32, 35, 36]).", "startOffset": 0, "endOffset": 28}, {"referenceID": 31, "context": "[11, 19, 21, 30, 32, 35, 36]).", "startOffset": 0, "endOffset": 28}, {"referenceID": 34, "context": "[11, 19, 21, 30, 32, 35, 36]).", "startOffset": 0, "endOffset": 28}, {"referenceID": 35, "context": "[11, 19, 21, 30, 32, 35, 36]).", "startOffset": 0, "endOffset": 28}, {"referenceID": 10, "context": "One promising approach along this line is the recently proposed two-stage algorithm called Wirtinger Flow (WF) [11].", "startOffset": 111, "endOffset": 115}, {"referenceID": 10, "context": "The main results of [11] demonstrate that WF is surprisingly accurate for both real-valued and complexvalued Gaussian sampling models.", "startOffset": 20, "endOffset": 24}, {"referenceID": 38, "context": "In the presence of Gaussian noise, WF is stable and converges to the MLE as shown in [39].", "startOffset": 85, "endOffset": 89}, {"referenceID": 10, "context": "we cannot distinguish x from \u2212x\u2014we will evaluate our solutions to the quadratic equations through the distance measure put forth in [11] representing the Euclidean distance modulo a global sign: for complex signals, dist (z,x) := min\u03c6:\u2208[0,2\u03c0) \u2016e\u2212j\u03c6z \u2212 x\u2016.", "startOffset": 132, "endOffset": 136}, {"referenceID": 33, "context": "Arguably the most popular method for solving large-scale least squares problems is the conjugate gradient (CG) method [34] applied to the normal equations.", "startOffset": 118, "endOffset": 122}, {"referenceID": 9, "context": "We consider a type of measurements that falls under the category of coded diffraction patterns (CDP) [10] and set y = |FDx|, 1 \u2264 l \u2264 L.", "startOffset": 101, "endOffset": 105}, {"referenceID": 38, "context": "This phenomenon arises regardless of the SNR! For experiments with noisy complex-valued data and (untruncated) WF, please see [39].", "startOffset": 126, "endOffset": 130}, {"referenceID": 10, "context": "These outperform the provable guarantees of WF [11], which requires O(n log n) sample complexity and runs in O(mn2 log 1/ ) time.", "startOffset": 47, "endOffset": 51}, {"referenceID": 10, "context": "With these in place, we take the step size in a far more liberal fashion\u2014which is bounded away from 0\u2014compared to a step size which is inversely propotional to n as explained in [11].", "startOffset": 178, "endOffset": 182}, {"referenceID": 12, "context": "Interested readers are referred to the supplemental materials [13] for the proof of the universal theory (i.", "startOffset": 62, "endOffset": 66}, {"referenceID": 38, "context": "[39] proves similar stability estimates using the WF approach under Gaussian noise.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "It is worth noting that apart from WF, various other nonconvex procedures have been proposed as well for phase retrieval, including the error reduction schemes dating back to Gerchberg-Saxton and Fienup [20,21], iterated projections [19], alternating minimization [32], generalized approximate message passing [35], and greedy methods that exploit additional sparsity constraint [36], to name just a few.", "startOffset": 203, "endOffset": 210}, {"referenceID": 20, "context": "It is worth noting that apart from WF, various other nonconvex procedures have been proposed as well for phase retrieval, including the error reduction schemes dating back to Gerchberg-Saxton and Fienup [20,21], iterated projections [19], alternating minimization [32], generalized approximate message passing [35], and greedy methods that exploit additional sparsity constraint [36], to name just a few.", "startOffset": 203, "endOffset": 210}, {"referenceID": 18, "context": "It is worth noting that apart from WF, various other nonconvex procedures have been proposed as well for phase retrieval, including the error reduction schemes dating back to Gerchberg-Saxton and Fienup [20,21], iterated projections [19], alternating minimization [32], generalized approximate message passing [35], and greedy methods that exploit additional sparsity constraint [36], to name just a few.", "startOffset": 233, "endOffset": 237}, {"referenceID": 31, "context": "It is worth noting that apart from WF, various other nonconvex procedures have been proposed as well for phase retrieval, including the error reduction schemes dating back to Gerchberg-Saxton and Fienup [20,21], iterated projections [19], alternating minimization [32], generalized approximate message passing [35], and greedy methods that exploit additional sparsity constraint [36], to name just a few.", "startOffset": 264, "endOffset": 268}, {"referenceID": 34, "context": "It is worth noting that apart from WF, various other nonconvex procedures have been proposed as well for phase retrieval, including the error reduction schemes dating back to Gerchberg-Saxton and Fienup [20,21], iterated projections [19], alternating minimization [32], generalized approximate message passing [35], and greedy methods that exploit additional sparsity constraint [36], to name just a few.", "startOffset": 310, "endOffset": 314}, {"referenceID": 35, "context": "It is worth noting that apart from WF, various other nonconvex procedures have been proposed as well for phase retrieval, including the error reduction schemes dating back to Gerchberg-Saxton and Fienup [20,21], iterated projections [19], alternating minimization [32], generalized approximate message passing [35], and greedy methods that exploit additional sparsity constraint [36], to name just a few.", "startOffset": 379, "endOffset": 383}, {"referenceID": 31, "context": "While these paradigms enjoy favorable empirical behavior, most of them fall short of theoretical support, except for a version of alternating minimization (called AltMinPhase) [32] that requires fresh samples for each iteration.", "startOffset": 176, "endOffset": 180}, {"referenceID": 10, "context": "Interesting readers are referred to [11] for a comparison of these non-convex schemes, and [10] for a discussion of other alternative approaches (e.", "startOffset": 36, "endOffset": 40}, {"referenceID": 9, "context": "Interesting readers are referred to [11] for a comparison of these non-convex schemes, and [10] for a discussion of other alternative approaches (e.", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "[1, 4]) and performance lower bounds (e.", "startOffset": 0, "endOffset": 6}, {"referenceID": 3, "context": "[1, 4]) and performance lower bounds (e.", "startOffset": 0, "endOffset": 6}, {"referenceID": 4, "context": "[5, 18]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 17, "context": "[5, 18]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 2, "context": "On the other hand, the family of two-stage nonconvex procedures\u2014spectral initialization followed by iterative refinement\u2014has proved efficient for other problems that involve latent variables, which leads to theoretical guarantees for general EM algorithms [3] and sparse coding schemes [2].", "startOffset": 256, "endOffset": 259}, {"referenceID": 1, "context": "On the other hand, the family of two-stage nonconvex procedures\u2014spectral initialization followed by iterative refinement\u2014has proved efficient for other problems that involve latent variables, which leads to theoretical guarantees for general EM algorithms [3] and sparse coding schemes [2].", "startOffset": 286, "endOffset": 289}, {"referenceID": 10, "context": "One natural alternative is the spectral method adopted in [11,32], which amounts to computing the leading eigenvector of \u1ef8 := 1 m \u2211m i=1 yiaia > i .", "startOffset": 58, "endOffset": 65}, {"referenceID": 31, "context": "One natural alternative is the spectral method adopted in [11,32], which amounts to computing the leading eigenvector of \u1ef8 := 1 m \u2211m i=1 yiaia > i .", "startOffset": 58, "endOffset": 65}, {"referenceID": 10, "context": "We start with a notation representing the (unrecoverable) global phase [11] for real-valued data", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "The observations (38) and (39) are reminiscent of a (local) regularity condition given in [11], which has been shown to be a fundamental criterion that dictates rapid convergence of iterative procedures (including WF and other gradient descent schemes).", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "For the sake of comparison, we also report the empirical performance of WF in all the above settings, where the step size is set to be the default choice of [11], that is, \u03bct = min{1\u2212 e\u2212t/330, 0.", "startOffset": 157, "endOffset": 161}, {"referenceID": 0, "context": "When t \u2208 [0, 1], it has been shown in the proof of [12, Lemma 3.", "startOffset": 9, "endOffset": 15}, {"referenceID": 0, "context": "9 for all t \u2208 [0, 1], as illustrated in Fig.", "startOffset": 14, "endOffset": 20}, {"referenceID": 10, "context": "For instance, all results continue to hold if we replace the Poisson likelihood by the Gaussian likelihood; that is, the polynomial function \u2212\u2211mi=1(yi \u2212 |ai z|2)2 studied in [11].", "startOffset": 174, "endOffset": 178}, {"referenceID": 7, "context": "It is known that this problem can be efficiently solved by using more computational-intensive semidefinite programs [8,14].", "startOffset": 116, "endOffset": 122}, {"referenceID": 13, "context": "It is known that this problem can be efficiently solved by using more computational-intensive semidefinite programs [8,14].", "startOffset": 116, "endOffset": 122}, {"referenceID": 23, "context": "Therefore, the preceding modified TWF may add to recent literature in applying non-convex schemes for low-rank matrix completion [24,27,28,41] or even robust PCA [33].", "startOffset": 129, "endOffset": 142}, {"referenceID": 26, "context": "Therefore, the preceding modified TWF may add to recent literature in applying non-convex schemes for low-rank matrix completion [24,27,28,41] or even robust PCA [33].", "startOffset": 129, "endOffset": 142}, {"referenceID": 27, "context": "Therefore, the preceding modified TWF may add to recent literature in applying non-convex schemes for low-rank matrix completion [24,27,28,41] or even robust PCA [33].", "startOffset": 129, "endOffset": 142}, {"referenceID": 39, "context": "Therefore, the preceding modified TWF may add to recent literature in applying non-convex schemes for low-rank matrix completion [24,27,28,41] or even robust PCA [33].", "startOffset": 129, "endOffset": 142}, {"referenceID": 32, "context": "Therefore, the preceding modified TWF may add to recent literature in applying non-convex schemes for low-rank matrix completion [24,27,28,41] or even robust PCA [33].", "startOffset": 162, "endOffset": 166}, {"referenceID": 5, "context": "A concrete application of this flavor is a simple form of the fundamental alignment/matching problem [6, 15, 25].", "startOffset": 101, "endOffset": 112}, {"referenceID": 14, "context": "A concrete application of this flavor is a simple form of the fundamental alignment/matching problem [6, 15, 25].", "startOffset": 101, "endOffset": 112}, {"referenceID": 24, "context": "A concrete application of this flavor is a simple form of the fundamental alignment/matching problem [6, 15, 25].", "startOffset": 101, "endOffset": 112}, {"referenceID": 42, "context": "[44]) such that for any (h, z) with \u2016h\u2016 = \u2016z\u2016 = 1, there exists a pair h0, z0 \u2208 N satisfying \u2016h\u2212 h0\u2016 \u2264 and \u2016z \u2212 z0\u2016 \u2264 .", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "We consider the fundamental problem of solving quadratic systems of equations in n variables, where yi = |\u3008ai,x\u3009|, i = 1, . . . ,m and x \u2208 R is unknown. We propose a novel method, which starting with an initial guess computed by means of a spectral method, proceeds by minimizing a nonconvex functional as in the Wirtinger flow approach [11]. There are several key distinguishing features, most notably, a distinct objective functional and novel update rules, which operate in an adaptive fashion and drop terms bearing too much influence on the search direction. These careful selection rules provide a tighter initial guess, better descent directions, and thus enhanced practical performance. On the theoretical side, we prove that for certain unstructured models of quadratic systems, our algorithms return the correct solution in linear time, i.e. in time proportional to reading the data {ai} and {yi} as soon as the ratio m/n between the number of equations and unknowns exceeds a fixed numerical constant. We extend the theory to deal with noisy systems in which we only have yi \u2248 |\u3008ai,x\u3009| and prove that our algorithms achieve a statistical accuracy, which is nearly un-improvable. We complement our theoretical study with numerical examples showing that solving random quadratic systems is both computationally and statistically not much harder than solving linear systems of the same size\u2014hence the title of this paper. For instance, we demonstrate empirically that the computational cost of our algorithm is about four times that of solving a least-squares problem of the same size.", "creator": "LaTeX with hyperref package"}}}