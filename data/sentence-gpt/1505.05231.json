{"id": "1505.05231", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2015", "title": "Bounds on the Minimax Rate for Estimating a Prior over a VC Class from Independent Learning Tasks", "abstract": "We study the optimal rates of convergence for estimating a prior distribution over a VC class from a sequence of independent data sets respectively labeled by independent target functions sampled from the prior. We specifically derive upper and lower bounds on the optimal rates under a smoothness condition on the correct prior, with the number of samples per data set equal the VC dimension. These results have implications for the improvements achievable via transfer learning using the MSA system (Fig. 3).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 20 May 2015 02:43:24 GMT  (40kb)", "http://arxiv.org/abs/1505.05231v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["liu yang", "steve hanneke", "jaime carbonell"], "accepted": false, "id": "1505.05231"}, "pdf": {"name": "1505.05231.pdf", "metadata": {"source": "CRF", "title": "Bounds on the Minimax Rate for Estimating a Prior over a VC Class from Independent Learning Tasks", "authors": ["Liu Yang", "Steve Hanneke", "Jaime Carbonell"], "emails": ["yangli@us.ibm.com", "steve.hanneke@gmail.com", "jgc@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 5.\n05 23\n1v 1\n[ cs\n.L G"}, {"heading": "1 Introduction", "text": "In the transfer learning setting, we are presented with a sequence of learning problems, each with some respective target concept we are tasked with learning. The key question in transfer learning is how to leverage our access to past learning problems in order to improve performance on learning problems we will be presented with in the future.\nAmong the several proposed models for transfer learning, one particularly appealing model supposes the learning problems are independent and identically distributed, with unknown distribution, and the advantage of transfer learning then comes from the ability to estimate this shared distribution based on the data from past learning problems [2,12]. For instance, when customizing a speech recognition system to a particular speaker\u2019s voice, we might expect the first few people would need to speak many words or phrases in order for the system to accurately identify the nuances. However, after performing this for many different people, if the software has access to those past training sessions when customizing itself to a new user, it should have identified important properties of the speech patterns, such as the common patterns within each of the major dialects or accents, and other such information about the distribution of speech patterns\nwithin the user population. It should then be able to leverage this information to reduce the number of words or phrases the next user needs to speak in order to train the system, for instance by first trying to identify the individual\u2019s dialect, then presenting phrases that differentiate common subpatterns within that dialect, and so forth.\nIn analyzing the benefits of transfer learning in such a setting, one important question to ask is how quickly we can estimate the distribution from which the learning problems are sampled. In recent work, [12] have shown that under mild conditions on the family of possible distributions, if the target concepts reside in a known VC class, then it is possible to estimate this distribtion using only a bounded number of training samples per task: specifically, a number of samples equal the VC dimension. However, that work left open the question of quantifying the rate of convergence. This rate of convergence can have a direct impact on how much benefit we gain from transfer learning when we are faced with only a finite sequence of learning problems. As such, it is certainly desirable to derive tight characterizations of this rate of convergence.\nThe present work continues that of [12], bounding the rate of convergence for estimating this distribution, under a smoothness condition on the distribution. We derive a generic upper bound, which holds regardless of the VC class the target concepts reside in. The proof of this result builds on that earlier work, but requires several interesting innovations to make the rate of convergence explicit, and to dramatically improve the upper bound implicit in the proofs of those earlier results. We further derive a nontrivial lower bound that holds for certain constructed scenarios, which illustrates a lower limit on how good of a general upper bound we might hope for in results expressed only in terms of the number of tasks, the smoothness conditions, and the VC dimension.\nWe additionally include an extension of the results of [12] to the setting of real-valued functions, establishing consistency (at a uniform rate) for an estimator of a prior over any VC subgraph class. In addition to the application to transfer learning, analogous to the original work of [12], we also discuss an application of this result to a preference elicitation problem in algorithmic economics, in which we are tasked with allocating items to a sequence of customers to approximately maximize the customers\u2019 satisfaction, while permitted access to the customer valuation functions only via value queries."}, {"heading": "2 The Setting", "text": "Let (X ,BX ) be a measurable space [8] (where X is called the instance space), and let D be a distribution on X (called the data distribution). Let C be a VC class of measurable classifiers h : X \u2192 {\u22121,+1} (called the concept space), and denote by d the VC dimension of C [10]. We suppose C is equipped with its Borel \u03c3-algebra B induced by the pseudo-metric \u03c1(h, g) = D({x \u2208 X : h(x) 6= g(x)}). Though our results can be formulated for general D (with somewhat more complicated theorem statements), to simplify the statement of results we\nsuppose \u03c1 is actually a metric, which would follow from appropriate topological conditions on C relative to D.\nFor any two probability measures \u00b51, \u00b52 on a measurable space (\u2126,F), define the total variation distance\n\u2016\u00b51 \u2212 \u00b52\u2016 = sup A\u2208F \u00b51(A)\u2212 \u00b52(A).\nFor a set function \u00b5 on a finite measurable space (\u2126,F), we abbreviate \u00b5(\u03c9) = \u00b5({\u03c9}), \u2200\u03c9 \u2208 \u2126. Let \u03a0\u0398 = {\u03c0\u03b8 : \u03b8 \u2208 \u0398} be a family of probability measures on C (called priors), where \u0398 is an arbitrary index set (called the parameter space). We suppose there exists a probability measure \u03c00 on C (the reference measure) such that every \u03c0\u03b8 is absolutely continuous with respect to \u03c00, and therefore has a density function f\u03b8 given by the Radon-Nikodym derivative\nd\u03c0\u03b8 d\u03c00\n[8]. We consider the following type of estimation problem. There is a collection of C-valued random variables {h\u2217t\u03b8 : t \u2208 N, \u03b8 \u2208 \u0398}, where for any fixed \u03b8 \u2208 \u0398 the {h\u2217t\u03b8} \u221e t=1 variables are i.i.d. with distribution \u03c0\u03b8. For each \u03b8 \u2208 \u0398, there is a sequence Zt(\u03b8) = {(Xt1, Yt1(\u03b8)), (Xt2, Yt2(\u03b8)), . . .}, where {Xti}t,i\u2208N are i.i.d. D, and for each t, i \u2208 N, Yti(\u03b8) = h\u2217t\u03b8(Xti). We additionally denote by Z t k(\u03b8) = {(Xt1, Yt1(\u03b8)), . . . , (Xtk, Ytk(\u03b8))} the first k elements of Zt(\u03b8), for any k \u2208 N, and similarly Xtk = {Xt1, . . . , Xtk} and Ytk(\u03b8) = {Yt1(\u03b8), . . . , Ytk(\u03b8)}. Following the terminology used in the transfer learning literature, we refer to the collection of variables associated with each t collectively as the tth task. We will be concerned with sequences of estimators \u03b8\u0302T\u03b8 = \u03b8\u0302T (Z1k(\u03b8), . . . ,Z T k (\u03b8)), for T \u2208 N, which are based on only a bounded number k of samples per task, among the first T tasks. Our main results specifically study the case of d samples per task. For any such estimator, we measure the risk as E [\n\u2016\u03c0\u03b8\u0302T\u03b8\u22c6 \u2212 \u03c0\u03b8\u22c6\u2016\n]\n, and will be particularly\ninterested in upper-bounding the worst-case risk sup\u03b8\u22c6\u2208\u0398 E [ \u2016\u03c0\u03b8\u0302T\u03b8\u22c6 \u2212 \u03c0\u03b8\u22c6\u2016 ] as a function of T , and lower-bounding the minimum possible value of this worst-case risk over all possible \u03b8\u0302T estimators (called the minimax risk).\nIn previous work, [12] showed that, if \u03a0\u0398 is a totally bounded family, then even with only d number of samples per task, the minimax risk (as a function of the number of tasks T ) converges to zero. In fact, that work also proved this is not necessarily the case in general for any number of samples less than d. However, the actual rates of convergence were not explicitly derived in that work, and indeed the upper bounds on the rates of convergence implicit in that analysis may often have fairly complicated dependences on C, \u03a0\u0398, and D, and furthermore often provide only very slow rates of convergence.\nTo derive explicit bounds on the rates of convergence, in the present work we specifically focus on families of smooth densities. The motivation for involving a notion of smoothness in characterizing rates of convergence is clear if we consider the extreme case in which \u03a0\u0398 contains two priors \u03c01 and \u03c02, with \u03c01({h}) = \u03c02({g}) = 1, where \u03c1(h, g) is a very small but nonzero value; in this case, if we have only a small number of samples per task, we would require many tasks (on the order of 1/\u03c1(h, g)) to observe any data points carrying any information that would distinguish between these two priors (namely, points x with h(x) 6= g(x));\nyet \u2016\u03c01\u2212\u03c02\u2016 = 1, so that we have a slow rate of convergence (at least initially). A total boundedness condition on \u03a0\u0398 would limit the number of such pairs present in \u03a0\u0398, so that for instance we cannot have arbitrarily close h and g, but less extreme variants of this can lead to slow asymptotic rates of convergence as well. Specifically, in the present work we consider the following notion of smoothness. For L \u2208 (0,\u221e) and \u03b1 \u2208 (0, 1], a function f : C \u2192 R is (L, \u03b1)-Ho\u0308lder smooth if\n\u2200h, g \u2208 C, |f(h)\u2212 f(g)| \u2264 L\u03c1(h, g)\u03b1."}, {"heading": "3 An Upper Bound", "text": "We now have the following theorem, holding for an arbitrary VC class C and data distribution D; it is the main result of this work.\nTheorem 1. For \u03a0\u0398 any class of priors on C having (L, \u03b1)-Ho\u0308lder smooth densities {f\u03b8 : \u03b8 \u2208 \u0398}, for any T \u2208 N, there exists an estimator \u03b8\u0302T\u03b8 = \u03b8\u0302T (Z1d (\u03b8), . . . ,Z T d (\u03b8)) such that\nsup \u03b8\u22c6\u2208\u0398 E\u2016\u03c0\u03b8\u0302T \u2212 \u03c0\u03b8\u22c6\u2016 = O\u0303\n(\nLT\u2212 \u03b12 2(d+2\u03b1)(\u03b1+2(d+1))\n)\n.\nProof. By the standard PAC analysis [9,3], for any \u03b3 > 0, with probability greater than 1 \u2212 \u03b3, a sample of k = O((d/\u03b3) log(1/\u03b3)) random points will partition C into regions of width less than \u03b3 (under L1(D)). For brevity, we omit the t subscripts and superscripts on quantities such as Ztk(\u03b8) throughout the following analysis, since the claims hold for any arbitrary value of t.\nFor any \u03b8 \u2208 \u0398, let \u03c0\u2032\u03b8 denote a (conditional on X1, . . . , Xk) distribution defined as follows. Let f \u2032\u03b8 denote the (conditional onX1, . . . , Xk) density function of \u03c0\u2032\u03b8 with respect to \u03c00, and for any g \u2208 C, let f \u2032 \u03b8(g) = \u03c0\u03b8({h\u2208C:\u2200i\u2264k,h(Xi)=g(Xi)}) \u03c00({h\u2208C:\u2200i\u2264k,h(Xi)=g(Xi)}) (or 0 if \u03c00({h \u2208 C : \u2200i \u2264 k, h(Xi) = g(Xi)}) = 0). In other words, \u03c0\u2032\u03b8 has the same probability mass as \u03c0\u03b8 for each of the equivalence classes induced by X1, . . . , Xk, but conditioned on the equivalence class, simply has a constantdensity distribution over that equivalence class. Note that every h \u2208 C has f \u2032\u03b8(h) between the smallest and largest values of f\u03b8(g) among g \u2208 C with \u2200i \u2264 k, g(Xi) = h(Xi); therefore, by the smoothness condition, on the event (of probability greater than 1 \u2212 \u03b3) that each of these regions has diameter less than \u03b3, we have \u2200h \u2208 C, |f\u03b8(h)\u2212 f \u2032\u03b8(h)| < L\u03b3 \u03b1. On this event, for any \u03b8, \u03b8\u2032 \u2208 \u0398,\n\u2016\u03c0\u03b8 \u2212 \u03c0\u03b8\u2032\u2016 = (1/2)\n\u222b\n|f\u03b8 \u2212 f\u03b8\u2032 |d\u03c00 < L\u03b3 \u03b1 + (1/2)\n\u222b\n|f \u2032\u03b8 \u2212 f \u2032 \u03b8\u2032 |d\u03c00.\nFurthermore, since the regions that define f \u2032\u03b8 and f \u2032 \u03b8\u2032 are the same (namely, the partition induced by X1, . . . , Xk), we have\n(1/2)\n\u222b\n|f \u2032\u03b8 \u2212 f \u2032 \u03b8\u2032 |d\u03c00 = (1/2)\n\u2211\ny1,...,yk\u2208{\u22121,+1}\n|\u03c0\u03b8({h \u2208 C : \u2200i \u2264 k, h(Xi) = yi})\n\u2212 \u03c0\u03b8\u2032({h \u2208 C : \u2200i \u2264 k, h(Xi) = yi})|\n= \u2016PYk(\u03b8)|Xk \u2212 PYk(\u03b8\u2032)|Xk\u2016.\nThus, we have that with probability at least 1\u2212 \u03b3,\n\u2016\u03c0\u03b8 \u2212 \u03c0\u03b8\u2032\u2016 < L\u03b3 \u03b1 + \u2016PYk(\u03b8)|Xk \u2212 PYk(\u03b8\u2032)|Xk\u2016.\nFollowing analogous to the inductive argument of [12], suppose I \u2286 {1, . . . , k}, fix x\u0304I \u2208 X |I| and y\u0304I \u2208 {\u22121,+1}|I|. Then the y\u0303I \u2208 {\u22121,+1}|I| for which \u2016y\u0304I \u2212 y\u0303I\u20161 is minimal, subject to the constraint that no h \u2208 C has h(x\u0304I) = y\u0303I , has (1/2)\u2016y\u0304I \u2212 y\u0303I\u20161 \u2264 d+ 1; also, for any i \u2208 I with y\u0304i 6= y\u0303i, letting y\u0304\u2032j = y\u0304j for j \u2208 I \\ {i} and y\u0304\u2032i = y\u0303i, we have\nPYI(\u03b8)|XI (y\u0304I |x\u0304I) = PYI\\{i}(\u03b8)|XI\\{i}(y\u0304I\\{i}|x\u0304I\\{i})\u2212 PYI(\u03b8)|XI (y\u0304 \u2032 I |x\u0304I),\nand similarly for \u03b8\u2032, so that\n|PYI(\u03b8)|XI (y\u0304I |x\u0304I)\u2212 PYI(\u03b8\u2032)|XI (y\u0304I |x\u0304I)|\n\u2264 |PYI\\{i}(\u03b8)|XI\\{i}(y\u0304I\\{i}|x\u0304I\\{i})\u2212 PYI\\{i}(\u03b8\u2032)|XI\\{i}(y\u0304I\\{i}|x\u0304I\\{i})|\n+ |PYI (\u03b8)|XI (y\u0304 \u2032 I |x\u0304I)\u2212 PYI(\u03b8\u2032)|XI (y\u0304 \u2032 I |x\u0304I)|.\nNow consider that these two terms inductively define a binary tree. Every time the tree branches left once, it arrives at a difference of probabilities for a set I of one less element than that of its parent. Every time the tree branches right once, it arrives at a difference of probabilities for a y\u0304I one closer to an unrealized y\u0303I than that of its parent. Say we stop branching the tree upon reaching a set I and a y\u0304I such that either y\u0304I is an unrealized labeling, or |I| = d. Thus, we can bound the original (root node) difference of probabilities by the sum of the differences of probabilities for the leaf nodes with |I| = d. Any path in the tree can branch left at most k \u2212 d times (total) before reaching a set I with only d elements, and can branch right at most d+1 times in a row before reaching a y\u0304I such that both probabilities are zero, so that the difference is zero. So the depth of any leaf node with |I| = d is at most (k\u2212d)d. Furthermore, at any level of the tree, from left to right the nodes have strictly decreasing |I| values, so that the maximum width of the tree is at most k \u2212 d. So the total number of leaf nodes with |I| = d is at most (k \u2212 d)2d. Thus, for any y\u0304 \u2208 {\u22121,+1}k and x\u0304 \u2208 X k,\n|PYk(\u03b8)|Xk(y\u0304|x\u0304)\u2212 PYk(\u03b8\u2032)|Xk(y\u0304|x\u0304)|\n\u2264 (k \u2212 d)2d \u00b7 max y\u0304d\u2208{\u22121,+1}d max D\u2208{1,...,k}d |PYd(\u03b8)|Xd(y\u0304 d|x\u0304D)\u2212 PYd(\u03b8\u2032)|Xd(y\u0304 d|x\u0304D)|.\nSince\n\u2016PYk(\u03b8)|Xk \u2212 PYk(\u03b8\u2032)|Xk\u2016 = (1/2) \u2211\ny\u0304k\u2208{\u22121,+1}k\n|PYk(\u03b8)|Xk(y\u0304 k)\u2212 PYk(\u03b8\u2032)|Xk(y\u0304 k)|,\nand by Sauer\u2019s Lemma this is at most\n(ek)d max y\u0304k\u2208{\u22121,+1}k |PYk(\u03b8)|Xk(y\u0304 k)\u2212 PYk(\u03b8\u2032)|Xk(y\u0304 k)|,\nwe have that\n\u2016PYk(\u03b8)|Xk \u2212 PYk(\u03b8\u2032)|Xk\u2016\n\u2264 (ek)dk2d max y\u0304d\u2208{\u22121,+1}d max D\u2208{1,...,k}d |PYd(\u03b8)|XD(y\u0304 d)\u2212 PYd(\u03b8\u2032)|XD(y\u0304 d)|.\nThus, we have that\n\u2016\u03c0\u03b8 \u2212 \u03c0\u03b8\u2032\u2016 = E\u2016\u03c0\u03b8 \u2212 \u03c0\u03b8\u2032\u2016\n< \u03b3+L\u03b3\u03b1+(ek)dk2dE\n[\nmax y\u0304d\u2208{\u22121,+1}d max D\u2208{1,...,k}d\nPYd(\u03b8)|XD(y\u0304 d)\u2212 PYd(\u03b8\u2032)|XD (y\u0304 d)|\n]\n.\nNote that\nE\n[\nmax y\u0304d\u2208{\u22121,+1}d max D\u2208{1,...,k}d\n|PYd(\u03b8)|XD(y\u0304 d)\u2212 PYd(\u03b8\u2032)|XD(y\u0304 d)|\n]\n\u2264 \u2211\ny\u0304d\u2208{\u22121,+1}d\n\u2211\nD\u2208{1,...,k}d\nE [ |PYd(\u03b8)|XD(y\u0304 d)\u2212 PYd(\u03b8\u2032)|XD (y\u0304 d)| ]\n\u2264 (2k)d max y\u0304d\u2208{\u22121,+1}d max D\u2208{1,...,k}d E [ |PYd(\u03b8)|XD(y\u0304 d)\u2212 PYd(\u03b8\u2032)|XD(y\u0304 d)| ] ,\nand by exchangeability, this last line equals\n(2k)d max y\u0304d\u2208{\u22121,+1}d E [ |PYd(\u03b8)|Xd(y\u0304 d)\u2212 PYd(\u03b8\u2032)|Xd(y\u0304 d)| ] .\n[12] showed that E [ |PYd(\u03b8)|Xd(y\u0304 d)\u2212 PYd(\u03b8\u2032)|Xd(y\u0304 d)| ] \u2264 4 \u221a \u2016PZd(\u03b8) \u2212 PZd(\u03b8\u2032)\u2016, so that in total we have \u2016\u03c0\u03b8 \u2212 \u03c0\u03b8\u2032\u2016 < (L+1)\u03b3\u03b1+4(2ek)2d+2 \u221a\n\u2016PZd(\u03b8)\u2212PZd(\u03b8\u2032)\u2016. Plugging in the value of k = c(d/\u03b3) log(1/\u03b3), this is\n(L+1)\u03b3\u03b1 + 4\n(\n2ec d\n\u03b3 log\n(\n1\n\u03b3\n))2d+2\u221a\n\u2016PZd(\u03b8)\u2212PZd(\u03b8\u2032)\u2016.\nThus, it suffices to bound the rate of convergence (in total variation distance) of some estimator of PZd(\u03b8\u22c6). IfN(\u03b5) is the \u03b5-covering number of {PZd(\u03b8) : \u03b8 \u2208 \u0398}, then taking \u03b8\u0302T\u03b8\u22c6 as the minimum distance skeleton estimate of [13,5] achieves expected total variation distance \u03b5 from PZd(\u03b8\u22c6), for some T = O((1/\u03b5 2) logN(\u03b5/4)). We can partition C into O((L/\u03b5)d/\u03b1) cells of diameter O((\u03b5/L)1/\u03b1), and set a constant density value within each cell, on an O(\u03b5)-grid of density values, and every prior with (L, \u03b1)-Ho\u0308lder smooth density will have density within \u03b5 of some density so-constructed; there are then at most (1/\u03b5)O((L/\u03b5) d/\u03b1) such densities, so this bounds the covering numbers of \u03a0\u0398. Furthermore, the covering number of \u03a0\u0398 upper bounds N(\u03b5) [12], so that N(\u03b5) \u2264 (1/\u03b5)O((L/\u03b5) d/\u03b1).\nSolving T =O(\u03b5\u22122(L/\u03b5)d/\u03b1 log(1/\u03b5)) for \u03b5, we have \u03b5=O\n(\nL (\nlog(TL) T\n) \u03b1\nd+2\u03b1\n)\n.\nSo this bounds the rate of convergence for E\u2016PZd(\u03b8\u0302T ) \u2212 PZd(\u03b8\u22c6)\u2016, for \u03b8\u0302T the\nminimum distance skeleton estimate. Plugging this rate into the bound on the priors, combined with Jensen\u2019s inequality, we have\nE\u2016\u03c0\u03b8\u0302T \u2212 \u03c0\u03b8\u22c6\u2016 < (L+ 1)\u03b3 \u03b1 + 4\n(\n2ec d\n\u03b3 log\n(\n1\n\u03b3\n))2d+2\n\u00d7O\n(\nL\n(\nlog(TL)\nT\n) \u03b1\n2d+4\u03b1\n)\n.\nThis holds for any \u03b3 > 0, so minimizing this expression over \u03b3 > 0 yields a bound on the rate. For instance, with \u03b3 = O\u0303 ( T\u2212 \u03b1 2(d+2\u03b1)(\u03b1+2(d+1)) ) , we have\nE\u2016\u03c0\u03b8\u0302T \u2212 \u03c0\u03b8\u22c6\u2016 = O\u0303\n(\nLT\u2212 \u03b12 2(d+2\u03b1)(\u03b1+2(d+1))\n)\n.\n\u2293\u2294"}, {"heading": "4 A Minimax Lower Bound", "text": "One natural quesiton is whether Theorem 1 can generally be improved. While we expect this to be true for some fixed VC classes (e.g., those of finite size), and in any case we expect that some of the constant factors in the exponent may be improvable, it is not at this time clear whether the general form of T\u2212\u0398(\u03b1\n2/(d+\u03b1)2) is sometimes optimal. One way to investigate this question is to construct specific spaces C and distributions D for which a lower bound can be obtained. In particular, we are generally interested in exhibiting lower bounds that are worse than those that apply to the usual problem of density estimation based on direct access to the h\u2217t\u03b8\u22c6 values (see Theorem 3 below).\nHere we present a lower bound that is interesting for this reason. However, although larger than the optimal rate for methods with direct access to the target concepts, it is still far from matching the upper bound above, so that the question of tightness remains open. Specifically, we have the following result.\nTheorem 2. For any integer d \u2265 1, any L > 0, \u03b1 \u2208 (0, 1], there is a value C(d, L, \u03b1) \u2208 (0,\u221e) such that, for any T \u2208 N, there exists an instance space X , a concept space C of VC dimension d, a distribution D over X , and a distribution \u03c00 over C such that, for \u03a0\u0398 a set of distributions over C with (L, \u03b1)-Ho\u0308lder smooth density functions with respect to \u03c00, any estimator \u03b8\u0302T = \u03b8\u0302T (Z1d (\u03b8\u22c6), . . . ,Z T d (\u03b8\u22c6)) has\nsup \u03b8\u22c6\u2208\u0398 E\n[ \u2016\u03c0\u03b8\u0302T \u2212 \u03c0\u03b8\u22c6\u2016 ] \u2265 C(d, L, \u03b1)T\u2212 \u03b1 2(d+\u03b1) .\nProof. (Sketch) We proceed by a reduction from the task of determining the bias of a coin from among two given possibilities. Specifically, fix any \u03b3 \u2208 (0, 1/2), n \u2208 N, and let B1(p), . . . , Bn(p) be i.i.d Bernoulli(p) random variables, for each p \u2208 [0, 1]; then it is known that, for any (possibly nondeterministic) decision rule\np\u0302n : {0, 1}n \u2192 {(1 + \u03b3)/2, (1\u2212 \u03b3)/2},\n1\n2\n\u2211\np\u2208{(1+\u03b3)/2,(1\u2212\u03b3)/2}\nP(p\u0302n(B1(p), . . . , Bn(p)) 6= p)\n\u2265 (1/32) \u00b7 exp { \u2212128\u03b32n/3 } . (1)\nThis easily follows from the results of [1], combined with a result of [7] bounding the KL divergence (see also [11])\nTo use this result, we construct a learning problem as follows. Fix some m \u2208 N with m \u2265 d, let X = {1, . . . ,m}, and let C be the space of all classifiers h : X \u2192 {\u22121,+1} such that |{x \u2208 X : h(x) = +1}| \u2264 d. Clearly the VC dimension of C is d. Define the distribution D as uniform over X . Finally, we specify a family of (L, \u03b1)-Ho\u0308lder smooth priors, parameterized by \u0398 = {\u22121,+1}( m d), as follows. Let \u03b3m = (L/2)(1/m) \u03b1. First, enumerate the ( m d )\ndistinct d-sized subsets of {1, . . . ,m} as X1,X2, . . . ,X(md ) . Define the reference distribution \u03c00 by the property that, for any h \u2208 C, letting q = |{x : h(x) = +1}|, \u03c00({h}) = (12 ) d ( m\u2212q d\u2212q ) / ( m d ) . For any b = (b1, . . . , b(md ) ) \u2208 {\u22121, 1}( m d ), define the prior \u03c0b as the distribution of a random variable hb specified by the following generative model. Let i\u2217 \u223c Uniform({1, . . . , (\nm d\n)\n}), let Cb(i\u2217) \u223c Bernoulli((1 + \u03b3mbi\u2217)/2); finally, hb \u223c Uniform({h \u2208 C : {x : h(x) = +1} \u2286 Xi\u2217 ,Parity(|{x : h(x) = +1}|) = Cb(i\u2217)}), where Parity(n) is 1 if n is odd, or 0 if n is even. We will refer to the variables in this generative model below. For any h \u2208 C, letting H = {x : h(x) = +1} and q = |H |, we can equivalently express\n\u03c0b({h}) = ( 1 2 )\nd ( m d )\u22121\u2211(md) i=1 1[H \u2286 Xi](1 + \u03b3mbi) Parity(q)(1 \u2212 \u03b3mbi)1\u2212Parity(q).\nFrom this explicit representation, it is clear that, letting fb = d\u03c0b d\u03c00\n, we have fb(h) \u2208 [1\u2212 \u03b3m, 1+ \u03b3m] for all h \u2208 C. The fact that fb is Ho\u0308lder smooth follows from this, since every distinct h, g \u2208 C have D({x : h(x) 6= g(x)}) \u2265 1/m = (2\u03b3m/L)\n1/\u03b1. Next we set up the reduction as follows. For any estimator \u03c0\u0302T = \u03c0\u0302T (Z1d(\u03b8\u22c6),\n. . . ,ZTd (\u03b8\u22c6)), and each i \u2208 {1, . . . , ( m d ) }, let hi be the classifier with {x : hi(x) = +1} = Xi; also, if \u03c0\u0302T ({hi}) > ( 1 2 ) d/ ( m d ) , let b\u0302i = 2Parity(d) \u2212 1, and otherwise b\u0302i = 1 \u2212 2Parity(d). We use these b\u0302i values to estimate the original bi values. Specifically, let p\u0302i = (1 + \u03b3mb\u0302i)/2 and pi = (1 + \u03b3mbi)/2, where b = \u03b8\u22c6. Then\n\u2016\u03c0\u0302T \u2212 \u03c0\u03b8\u22c6\u2016 \u2265 (1/2)\n(md ) \u2211\ni=1\n|\u03c0\u0302T ({hi})\u2212 \u03c0\u03b8\u22c6({hi})|\n\u2265 (1/2)\n(md ) \u2211\ni=1\n\u03b3m\n2d ( m d\n) |b\u0302i \u2212 bi|/2 = (1/2)\n(md ) \u2211\ni=1\n1 2d (\nm d\n) |p\u0302i \u2212 pi|.\nThus, we have reduced from the problem of deciding the biases of these ( m d ) independent Bernoulli random variables. To complete the proof, it suffices to lower bound the expectation of the right side for an arbitrary estimator.\nToward this end, we in fact study an even easier problem. Specifically, consider an estimator q\u0302i = q\u0302i(Z1d(\u03b8\u22c6), . . . ,Z T d (\u03b8\u22c6), i \u2217 1, . . . , i \u2217 T ), where i \u2217 t is the i\n\u2217 random variable in the generative model that defines h\u2217t\u03b8\u22c6 ; that is, i \u2217 t \u223c Uniform({1, . . . , (\nm d\n)\n}), Ct \u223c Bernoulli((1 + \u03b3mbi\u2217t )/2), and h \u2217 t\u03b8\u22c6 \u223c Uniform({h \u2208 C : {x : h(x) = +1} \u2286 Xi\u2217t ,Parity(|{x : h(x) = +1}|) = Ct}), where the i \u2217 t are independent across t, as are the Ct and h \u2217 t\u03b8\u22c6\n. Clearly the p\u0302i from above can be viewed as an estimator of this type, which simply ignores the knowledge of i\u2217t . The knowledge of these i\u2217t variables simplifies the analysis, since given {i \u2217 t : t \u2264 T }, the data can be partitioned into (\nm d\n)\ndisjoint sets, {{Ztd(\u03b8\u22c6) : i \u2217 t = i} : i = 1, . . . ,\n(\nm d\n)\n}, and we can use only the set {Ztd(\u03b8\u22c6) : i \u2217 t = i} to estimate pi. Furthermore, we can use only the subset of these for which Xtd = Xi, since otherwise we have zero information about the value of Parity(|{x : h\u2217t\u03b8\u22c6(x) = +1}|). That is, given i\u2217t = i, any Z t d(\u03b8\u22c6) is conditionally independent from every bj for j 6= i, and is even conditionally independent from bi when Xtd is not completely contained in Xi; specifically, in this case, regardless of bi, the conditional distribution of Ytd(\u03b8\u22c6) given i \u2217 t = i and given Xtd is a product distribution, which deterministically assigns label \u22121 to those Ytk(\u03b8\u22c6) with Xtk /\u2208 Xi, and gives uniform random values to the subset of Ytd(\u03b8\u22c6) with their respective Xtk \u2208 Xi. Finally, letting rt = Parity(|{k \u2264 d : Ytk(\u03b8\u22c6) = +1}|), we note that given i\u2217t = i, Xtd = Xi, and the value rt, bi is conditionally independent from Ztd(\u03b8\u22c6). Thus, the set of values CiT (\u03b8\u22c6) = {rt : i\u2217t = i,Xtd = Xi} is a sufficient statistic for bi (hence for pi). Recall that, when i \u2217 t = i and Xtd = Xi, the value of rt is equal to Ct, a Bernoulli(pi) random variable. Thus, we neither lose nor gain anything (in terms of risk) by restricting ourselves to estimators q\u0302i of the type q\u0302i = q\u0302i(Z1d(\u03b8\u22c6), . . . ,Z T d (\u03b8\u22c6), i \u2217 1, . . . , i \u2217 T ) = q\u0302 \u2032 i(CiT (\u03b8\u22c6)), for some q\u0302 \u2032 i [8]: that is, estimators that are a function of the NiT (\u03b8\u22c6) = |CiT (\u03b8\u22c6)| Bernoulli(pi) random variables, which we should note are conditionally i.i.d. given NiT (\u03b8\u22c6).\nThus, by (1), for any n \u2264 T ,\n1\n2\n\u2211\nbi\u2208{\u22121,+1}\nE\n[ |q\u0302i \u2212 pi| \u2223 \u2223 \u2223 NiT (\u03b8\u22c6) = n ] = 1\n2\n\u2211\nbi\u2208{\u22121,+1}\n\u03b3mP ( q\u0302i 6= pi\n\u2223 \u2223 \u2223 NiT (\u03b8\u22c6) = n )\n\u2265 (\u03b3m/32) \u00b7 exp { \u2212128\u03b32mNi/3 } .\nAlso note that, for each i, E[Ni] = d!(1/m)d\n(md ) T \u2264 (d/m)2dT = d2d(2\u03b3m/L)2d/\u03b1T .\nThus, Jensen\u2019s inequality, linearity of expectation, and the law of total expectation imply\n1\n2\n\u2211\nbi\u2208{\u22121,+1}\nE [|q\u0302i \u2212 pi|] \u2265 (\u03b3m/32) \u00b7 exp { \u221243(2/L)2d/\u03b1d2d\u03b32+2d/\u03b1m T } .\nThus, by linearity of the expectation,\n(\n1\n2\n)(md ) \u2211\nb\u2208{\u22121,+1}( m d )\nE\n\n \n(md ) \u2211\ni=1\n1 2d (\nm d\n) |q\u0302i \u2212 pi|\n\n  =\n(md ) \u2211\ni=1\n1 2d (\nm d\n)\n1\n2\n\u2211\nbi\u2208{\u22121,+1}\nE [|q\u0302i \u2212 pi|]\n\u2265 (\u03b3m/(32 \u00b7 2 d)) \u00b7 exp\n{ \u221243(2/L)2d/\u03b1d2d\u03b32+2d/\u03b1m T } .\nIn particular, taking m = \u2308 (L/2)1/\u03b1 ( 43(2/L)2d/\u03b1d2dT ) 1 2(d+\u03b1) \u2309\n, we have \u03b3m =\n\u0398 ( ( 43(2/L)2d/\u03b1d2dT )\u2212 \u03b1 2(d+\u03b1) ) , so that\n(\n1\n2\n)(md ) \u2211\nb\u2208{\u22121,+1}( m d )\nE\n\n \n(md ) \u2211\ni=1\n1 2d (\nm d\n) |q\u0302i \u2212 pi|\n\n \n= \u2126\n(\n2\u2212d ( 43(2/L)2d/\u03b1d2dT )\u2212 \u03b1 2(d+\u03b1)\n)\n.\nIn particular, this implies there exists some b for which\nE\n\n \n(md) \u2211\ni=1\n1 2d (\nm d\n) |q\u0302i \u2212 pi|\n\n  = \u2126\n(\n2\u2212d ( 43(2/L)2d/\u03b1d2dT )\u2212 \u03b1 2(d+\u03b1)\n)\n.\nApplying this lower bound to the estimator p\u0302i above yields the result. \u2293\u2294\nIt is natural to wonder how these rates might potentially improve if we allow \u03b8\u0302T to depend on more than d samples per data set. To establish limits on such improvements, we note that in the extreme case of allowing the estimator to depend on the full Zt(\u03b8\u22c6) data sets, we may recover the known results lower bounding the risk of density estimation from i.i.d. samples from a smooth density, as indicated by the following result.\nTheorem 3. For any integer d \u2265 1, there exists an instance space X , a concept space C of VC dimension d, a distribution D over X , and a distribution \u03c00 over C such that, for \u03a0\u0398 the set of distributions over C with (L, \u03b1)-Ho\u0308lder smooth density functions with respect to \u03c00, any sequence of estimators, \u03b8\u0302T = \u03b8\u0302T (Z1(\u03b8\u22c6), . . . ,ZT (\u03b8\u22c6)) (T = 1, 2, . . .), has\nsup \u03b8\u22c6\u2208\u0398 E\n[ \u2016\u03c0\u03b8\u0302T \u2212 \u03c0\u03b8\u22c6\u2016 ] = \u2126 ( T\u2212 \u03b1 d+2\u03b1 ) .\nThe proof is a simple reduction from the problem of estimating \u03c0\u03b8\u22c6 based on direct access to h\u22171\u03b8\u22c6 , . . . , h \u2217 T\u03b8\u22c6\n, which is essentially equivalent to the standard model of density estimation, and indeed the lower bound in Theorem 3 is a wellknown result for density estimation from T i.i.d. samples from a Ho\u0308lder smooth density in a d-dimensional space [5]."}, {"heading": "5 Real-Valued Functions and an Application in Algorithmic Economics", "text": "In this section, we present results generalizing the analysis of [12] to classes of real-valued functions. We also present an application of this generalization to a preference elicitation problem."}, {"heading": "5.1 Consistent Estimation of Priors over Real-Valued Functions at a Bounded Rate", "text": "In this section, we let B denote a \u03c3-algebra on X \u00d7 R, and again let BX denote the corresponding \u03c3-algebra on X . Also, for measurable functions h, g : X \u2192 R, let \u03c1(h, g) = \u222b\n|h \u2212 g|dPX , where PX is a distribution over X . Let F be a class of functions X \u2192 R with Borel \u03c3-algebra BF induced by \u03c1. Let \u0398 be a set, and for each \u03b8 \u2208 \u0398, let \u03c0\u03b8 denote a probability measure on (F ,BF ). We suppose {\u03c0\u03b8 : \u03b8 \u2208 \u0398} is totally bounded in total variation distance, and that F is a uniformly bounded VC subgraph class with pseudodimension d. We also suppose \u03c1 is a metric when restricted to F .\nAs above, let {Xti}t,i\u2208N be i.i.d. PX random variables. For each \u03b8 \u2208 \u0398, let {h\u2217t\u03b8}t\u2208N be i.i.d. \u03c0\u03b8 random variables, independent from {Xti}t,i\u2208N. For each t \u2208 N and \u03b8 \u2208 \u0398, let Yti(\u03b8) = h\u2217t\u03b8(Xti) for i \u2208 N, and let Z\nt(\u03b8) = {(Xt1, Yt1(\u03b8)), (Xt2, Yt2(\u03b8)), . . .}; for each k \u2208 N, define Ztk(\u03b8) = {(Xt1, Yt1(\u03b8)), . . . , (Xtk, Ytk(\u03b8))}, Xtk = {Xt1, . . . , Xtk}, and Ytk(\u03b8) = {Yt1(\u03b8), . . . , Ytk(\u03b8)}.\nWe have the following result. The proof parallels that of [12] (who studied the special case of binary functions), with a few important twists (in particular, a significantly different approach in the analogue of their Lemma 3). The details are included in Appendix A.\nTheorem 4. There exists an estimator \u03b8\u0302T\u03b8\u22c6 = \u03b8\u0302T (Z 1 d(\u03b8\u22c6), . . . ,Z T d (\u03b8\u22c6)), and functions R : N0 \u00d7 (0, 1] \u2192 [0,\u221e) and \u03b4 : N0 \u00d7 (0, 1] \u2192 [0, 1] such that, for any \u03b1 > 0, lim\nT\u2192\u221e R(T, \u03b1) = lim T\u2192\u221e \u03b4(T, \u03b1) = 0 and for any T \u2208 N0 and \u03b8\u22c6 \u2208 \u0398,\nP\n(\n\u2016\u03c0\u03b8\u0302T\u03b8\u22c6 \u2212 \u03c0\u03b8\u22c6\u2016 > R(T, \u03b1)\n)\n\u2264 \u03b4(T, \u03b1) \u2264 \u03b1."}, {"heading": "5.2 Maximizing Customer Satisfaction in Combinatorial Auctions", "text": "Theorem 4 has a clear application in the context of transfer learning, following analogous arguments to those given in the special case of binary classification by [12]. In addition to that application, we can also use Theorem 4 in the context of the following problem in algorithmic economics, where the objective is to serve a sequence of customers so as to maximize their satisfaction.\nConsider an online travel agency, where customers go to the site with some idea of what type of travel they are interested in; the site then poses a series of questions to each customer, and identifies a travel package that best suits their desires, budget, and dates. There are many options of travel packages, with\noptions on location, site-seeing tours, hotel and room quality, etc. Because of this, serving the needs of an arbitrary customer might be a lengthy process, requiring many detailed questions. Fortunately, the stream of customers is typically not a worst-case sequence, and in particular obeys many statistical regularities: in particular, it is not too far from reality to think of the customers as being independent and identically distributed samples. With this assumption in mind, it becomes desirable to identify some of these statistical regularities so that we can pose the questions that are typically most relevant, and thereby more quickly identify the travel package that best suits the needs of the typical customer. One straightforward way to do this is to directly estimate the distribution of customer value functions, and optimize the questioning system to minimize the expected number of questions needed to find a suitable travel package.\nOne can model this problem in the style of Bayesian combinatorial auctions, in which each customer has a value function for each possible bundle of items. However, it is slightly different, in that we do not assume the distribution of customers is known, but rather are interested in estimating this distribution; the obtained estimate can then be used in combination with methods based on Bayesian decision theory. In contrast to the literature on Bayesian auctions (and subjectivist Bayesian decision theory in general), this technique is able to maintain general guarantees on performance that hold under an objective interpretation of the problem, rather than merely guarantees holding under an arbitrary assumed prior belief. This general idea is sometimes referred to as Empirical Bayesian decision theory in the machine learning and statistics literatures. The ideal result for an Empirical Bayesian algorithm is to be competitive with the corresponding Bayesian methods based on the actual distribution of the data (assuming the data are random, with an unknown distribution); that is, although the Empirical Bayesian methods only operate with a data-based estimate of the distribution, the aim is to perform nearly as well as methods based on the true (unobservable) distribution. In this work, we present results of this type, in the context of an abstraction of the aforementioned online travel agency problem, where the measure of performance is the expected number of questions to find a suitable package.\nThe specific application we are interested in here may be expressed abstractly as a kind of combinatorial auction with preference elicitation. Specifically, we suppose there is a collection of items on a menu, and each possible bundle of items has an associated fixed price. There is a stream of customers, each with a valuation function that provides a value for each possible bundle of items. The objective is to serve each customer a bundle of items that nearly-maximizes his or her surplus value (value minus price). However, we are not permitted direct observation of the customer valuation functions; rather, we may query for the value of any given bundle of items; this is referred to as a value query in the literature on preference elicitation in combinatorial auctions (see Chapter 14 of [4], [14]). The objective is to achieve this near-maximal surplus guarantee, while making only a small number of queries per customer. We suppose the customer valuation function are sampled i.i.d. according to an unknown distri-\nbution over a known (but arbitrary) class of real-valued functions having finite pseudo-dimension. Reasoning that knowledge of this distribution should allow one to make a smaller number of value queries per customer, we are interested in estimating this unknown distribution, so that as we serve more and more customers, the number of queries per customer required to identify a near-optimal bundle should decrease. In this context, we in fact prove that in the limit, the expected number of queries per customer converges to the number required of a method having direct knowledge of the true distribution of valuation functions.\nFormally, suppose there is a menu of n items [n] = {1, . . . , n}, and each bundle B \u2286 [n] has an associated price p(B) \u2265 0. Suppose also there is a sequence of customers, each with a valuation function vt : 2\n[n] \u2192 R. We suppose these vt functions are i.i.d. samples. We can then calculate the satisfaction function for each customer as st(x), where x \u2208 {0, 1}\nn, and st(x) = vt(Bx) \u2212 p(Bx), where Bx \u2286 [n] contains element i \u2208 [n] iff xi = 1.\nNow suppose we are able to ask each customer a number of questions before serving up a bundle Bx\u0302t to that customer. More specifically, we are able to ask for the value st(x) for any x \u2208 {0, 1}n. This is referred to as a value query in the literature on preference elicitation in combinatorial auctions (see Chapter 14 of [4], [14]). We are interested in asking as few questions as possible, while satisfying the guarantee that E[st(x\u0302t)\u2212maxx st(x)] \u2264 \u03b5.\nNow suppose, for every \u03c0 and \u03b5, we have a method A(\u03c0, \u03b5) such that, given that \u03c0 is the actual distribution of the st functions, A(\u03c0, \u03b5) guarantees that the x\u0302t value it selects has E[maxx st(x) \u2212 st(x\u0302t)] \u2264 \u03b5; also let N\u0302t(\u03c0, \u03b5) denote the actual (random) number of queries the method A(\u03c0, \u03b5) would ask for the st function, and let Q(\u03c0, \u03b5) = E[N\u0302t(\u03c0, \u03b5)]. We suppose the method never queries any st(x) value twice for a given t, so that its number of queries for any given t is bounded.\nAlso suppose F is a VC subgraph class of functions mapping X = {0, 1}n into [\u22121, 1] with pseudodimension d, and that {\u03c0\u03b8 : \u03b8 \u2208 \u0398} is a known totally bounded family of distributions over F such that the st functions have distribution \u03c0\u03b8\u22c6 for some unknown \u03b8\u22c6 \u2208 \u0398. For any \u03b8 \u2208 \u0398 and \u03b3 > 0, let B(\u03b8, \u03b3) = {\u03b8\u2032 \u2208 \u0398 : \u2016\u03c0\u03b8 \u2212 \u03c0\u03b8\u2032\u2016 \u2264 \u03b3}.\nSuppose, in addition to A, we have another method A\u2032(\u03b5) that is not \u03c0dependent, but still provides the \u03b5-correctness guarantee, and makes a bounded number of queries (e.g., in the worst case, we could consider querying all 2n points, but in most cases there are more clever \u03c0-independent methods that use far fewer queries, such as O(1/\u03b52)). Consider the following method; the quantities\n\u03b8\u0302T\u03b8\u22c6 , R(T, \u03b1), and \u03b4(T, \u03b1) from Theorem 4 are here considered with respect PX taken as the uniform distribution on {0, 1}n.\nThe following theorem indicates that this method is correct, and furthermore that the long-run average number of queries is not much worse than that of a method that has direct knowledge of \u03c0\u03b8\u22c6 . The proof of this result parallels that of [12] for the transfer learning setting, but is included here for completeness.\nAlgorithm 1 An algorithm for sequentially maximizing expected customer satisfaction.\nfor t = 1, 2, . . . , T do Pick points Xt1, Xt2, . . . , Xtd uniformly at random from {0, 1} n\nif R(t\u2212 1, \u03b5/2) > \u03b5/8 then Run A\u2032(\u03b5) Take x\u0302t as the returned value else\nLet \u03b8\u030ct\u03b8\u22c6 \u2208 B ( \u03b8\u0302(t\u22121)\u03b8\u22c6 , R(t\u2212 1, \u03b5/2) ) be such that\nQ(\u03c0\u03b8\u030ct\u03b8\u22c6 , \u03b5/4) \u2264 min \u03b8\u2208B(\u03b8\u0302(t\u22121)\u03b8\u22c6 ,R(t\u22121,\u03b5/2)) Q(\u03c0\u03b8, \u03b5/4) +\n1 t\nRun A(\u03c0\u03b8\u030ct\u03b8\u22c6 , \u03b5/4) and let x\u0302t be its return value\nend if\nend for\nTheorem 5. For the above method, \u2200t \u2264 T,E[maxx st(x) \u2212 st(x\u0302t)] \u2264 \u03b5. Furthermore, if ST (\u03b5) is the total number of queries made by the method, then\nlim sup T\u2192\u221e\nE[ST (\u03b5)]\nT \u2264 Q(\u03c0\u03b8\u22c6 , \u03b5/4) + d.\nProof. By Theorem 4, for any t \u2264 T , if R(t\u22121, \u03b5/2) \u2264 \u03b5/8, then with probability at least 1 \u2212 \u03b5/2, \u2016\u03c0\u03b8\u22c6 \u2212 \u03c0\u03b8\u0302(t\u22121)\u03b8\u22c6 \u2016 \u2264 R(t \u2212 1, \u03b5/2), so that a triangle inequality implies \u2016\u03c0\u03b8\u22c6 \u2212 \u03c0\u03b8\u030ct\u03b8\u22c6\u2016 \u2264 2R(t\u2212 1, \u03b5/2) \u2264 \u03b5/4. Thus,\nE\n[\nmax x\nst(x)\u2212 st(x\u0302t) ]\n\u2264 \u03b5/2 + E [ E [\nmax x\nst(x) \u2212 st(x\u0302t) \u2223 \u2223 \u2223\u03b8\u030ct\u03b8\u22c6 ] 1 [ \u2016\u03c0\u03b8\u030ct\u03b8\u22c6 \u2212 \u03c0\u03b8\u22c6\u2016 \u2264 \u03b5/2 ]] .\nFor \u03b8 \u2208 \u0398, let x\u0302t\u03b8 denote the point x that would be returned by A(\u03c0\u03b8\u030ct\u03b8\u22c6 , \u03b5/4) when queries are answered by some st\u03b8 \u223c \u03c0\u03b8 instead of st (and supposing st = st\u03b8\u22c6). If \u2016\u03c0\u03b8\u030ct\u03b8\u22c6 \u2212 \u03c0\u03b8\u22c6\u2016 \u2264 \u03b5/4, then\nE\n[\nmax x\nst(x)\u2212 st(x\u0302t) \u2223 \u2223 \u2223\u03b8\u030ct\u03b8\u22c6 ] = E [\nmax x\nst\u03b8\u22c6(x)\u2212 st\u03b8\u22c6(x\u0302t) \u2223 \u2223 \u2223\u03b8\u030ct\u03b8\u22c6 ]\n\u2264 E [\nmax x st\u03b8\u030ct\u03b8\u22c6 (x) \u2212 st\u03b8\u030ct\u03b8\u22c6 (x\u0302t\u03b8\u030ct\u03b8\u22c6 ) \u2223 \u2223 \u2223\u03b8\u030ct\u03b8\u22c6 ] + \u2016\u03c0\u03b8\u030ct\u03b8\u22c6 \u2212 \u03c0\u03b8\u22c6\u2016 \u2264 \u03b5/4 + \u03b5/4 = \u03b5/2.\nPlugging into the above bound, we have E [maxx st(x)\u2212 st(x\u0302t)] \u2264 \u03b5. For the result on ST (\u03b5), first note that R(t\u22121, \u03b5/2) > \u03b5/8 only finitely many times (due to R(t, \u03b1) = o(1)), so that we can ignore those values of t in the asymptotic calculation (as the number of queries is always bounded), and rely on the correctness guarantee of A\u2032. For the remaining values t, let Nt denote the number of queries made by A(\u03c0\u03b8\u030ct\u03b8\u22c6 , \u03b5/4). Then\nlim sup T\u2192\u221e\nE[ST (\u03b5)]\nT \u2264 d+ lim sup\nT\u2192\u221e\nT \u2211\nt=1\nE [Nt]\nT .\nSince\nlim T\u2192\u221e\n1\nT\nT \u2211\nt=1\nE\n[\nNt1[\u2016\u03c0\u03b8\u0302(t\u22121)\u03b8\u22c6 \u2212 \u03c0\u03b8\u22c6\u2016 > R(t\u2212 1, \u03b5/2)]\n]\n\u2264 lim T\u2192\u221e\n1\nT\nT \u2211\nt=1\n2nP (\n\u2016\u03c0\u03b8\u0302(t\u22121)\u03b8\u22c6 \u2212 \u03c0\u03b8\u22c6\u2016 > R(t\u2212 1, \u03b5/2)\n)\n\u2264 2n lim T\u2192\u221e\n1\nT\nT \u2211\nt=1\n\u03b4(t\u2212 1, \u03b5/2) = 0,\nwe have\nlim sup T\u2192\u221e\nT \u2211\nt=1\nE [Nt]\nT = lim sup\nT\u2192\u221e\n1\nT\nT \u2211\nt=1\nE\n[\nNt1[\u2016\u03c0\u03b8\u0302(t\u22121)\u03b8\u22c6 \u2212 \u03c0\u03b8\u22c6\u2016 \u2264 R(t\u2212 1, \u03b5/2)]\n]\n.\nFor t \u2264 T , let Nt(\u03b8\u030ct\u03b8\u22c6) denote the number of queries A(\u03c0\u03b8\u030ct\u03b8\u22c6 , \u03b5/4) would make if queries were answered with st\u03b8\u030ct\u03b8\u22c6 instead of st. On the event \u2016\u03c0\u03b8\u0302(t\u22121)\u03b8\u22c6 \u2212 \u03c0\u03b8\u22c6\u2016 \u2264 R(t\u2212 1, \u03b5/2), we have\nE\n[\nNt\n\u2223 \u2223 \u2223 \u03b8\u030ct\u03b8\u22c6 ] \u2264 E [ Nt(\u03b8\u030ct\u03b8\u22c6) \u2223 \u2223 \u2223 \u03b8\u030ct\u03b8\u22c6 ] + 2R(t\u2212 1, \u03b5/2)\n= Q(\u03c0\u03b8\u030ct\u03b8\u22c6, \u03b5/4) + 2R(t\u22121, \u03b5/2) \u2264 Q(\u03c0\u03b8\u22c6 , \u03b5/4) + 2R(t\u22121, \u03b5/2)+ 1/t.\nTherefore,\nlim sup T\u2192\u221e\n1\nT\nT \u2211\nt=1\nE\n[\nNt1[\u2016\u03c0\u03b8\u0302(t\u22121)\u03b8\u22c6 \u2212 \u03c0\u03b8\u22c6\u2016 \u2264 R(t\u2212 1, \u03b5/2)]\n]\n\u2264 Q(\u03c0\u03b8\u22c6 , \u03b5/4) + lim sup T\u2192\u221e\n1\nT\nT \u2211\nt=1\n2R(t\u2212 1, \u03b5/2) + 1/t = Q(\u03c0\u03b8\u22c6 , \u03b5/4).\n\u2293\u2294\nIn many cases, this result will even continue to hold with an infinite number of goods (n = \u221e), since Theorem 4 has no dependence on the cardinality of the space X ."}, {"heading": "6 Open Problems", "text": "There are several interesting questions that remain open at this time. Can either the lower bound or upper bound be improved in general? If, instead of d samples per task, we instead use m \u2265 d samples, how does the minimax risk vary with m? Related to this, what is the optimal value of m to optimize the rate of convergence as a function of mT , the total number of samples? More generally, if an estimator is permitted to use N total samples, taken from however many tasks it wishes, what is the optimal rate of convergence as a function of N?"}, {"heading": "A Proofs for Section 5", "text": "The proof of Theorem 4 is based on the following sequence of lemmas, which parallel those used by [12] for establishing the analogous result for consistent estimation of priors over binary functions. The last of these lemmas (namely, Lemma 3) requires substantial modifications to the original argument of [12]; the others use arguments more-directly based on those of [12].\nLemma 1. For any \u03b8, \u03b8\u2032 \u2208 \u0398 and t \u2208 N,\n\u2016\u03c0\u03b8 \u2212 \u03c0\u03b8\u2032\u2016 = \u2016PZt(\u03b8) \u2212 PZt(\u03b8\u2032)\u2016.\nProof. Fix \u03b8, \u03b8\u2032 \u2208 \u0398, t \u2208 N. Let X = {Xt1, Xt2, . . .}, Y(\u03b8) = {Yt1(\u03b8), Yt2(\u03b8), . . .}, and for k \u2208 N let Xk = {Xt1, . . . , Xtk}. and Yk(\u03b8) = {Yt1(\u03b8), . . . , Ytk(\u03b8)}. For h \u2208 F , let cX(h) = {(Xt1, h(Xt1)), (Xt2, h(Xt2)), . . .}.\nFor h, g \u2208 F , define \u03c1X(h, g) = lim m\u2192\u221e\n1 m \u2211m i=1 |h(Xti) \u2212 g(Xti)| (if the limit\nexists), and \u03c1Xk(h, g) = 1 k \u2211k i=1 |h(Xti)\u2212g(Xti)|. Note that since F is a uniformly bounded VC subgraph class, so is the collection of functions {|h\u2212 g| : h, g \u2208 F}, so that the uniform strong law of large numbers implies that with probability one, \u2200h, g \u2208 F , \u03c1X(h, g) exists and has \u03c1X(h, g) = \u03c1(h, g) [10].\nConsider any \u03b8, \u03b8\u2032 \u2208 \u0398, and any A \u2208 BF . Then any h /\u2208 A has \u2200g \u2208 A, \u03c1(h, g) > 0 (by the metric assumption). Thus, if \u03c1X(h, g) = \u03c1(h, g) for all h, g \u2208 F , then \u2200h /\u2208 A,\n\u2200g \u2208 A, \u03c1X(h, g) = \u03c1(h, g) > 0 =\u21d2\n\u2200g \u2208 A, cX(h) 6= cX(g) =\u21d2 cX(h) /\u2208 cX(A).\nThis implies c\u22121 X (cX(A)) = A. Under these conditions,\nPZt(\u03b8)|X(cX(A)) = \u03c0\u03b8(c \u22121 X (cX(A))) = \u03c0\u03b8(A),\nand similarly for \u03b8\u2032. Any measurable set C for the range of Zt(\u03b8) can be expressed as C = {cx\u0304(h) : (h, x\u0304) \u2208 C\u2032} for some appropriate C\u2032 \u2208 BF\u2297B\u221eX . Letting C \u2032 x\u0304 = {h : (h, x\u0304) \u2208 C\n\u2032}, we have\nPZt(\u03b8)(C) =\n\u222b\n\u03c0\u03b8(c \u22121 x\u0304 (cx\u0304(C \u2032 x\u0304)))PX(dx\u0304) =\n\u222b\n\u03c0\u03b8(C \u2032 x\u0304)PX(dx\u0304) = P(h\u2217t\u03b8,X)(C \u2032).\nLikewise, this reasoning holds for \u03b8\u2032. Then\n\u2016PZt(\u03b8) \u2212 PZt(\u03b8\u2032)\u2016 = \u2016P(h\u2217t\u03b8,X) \u2212 P(h\u2217t\u03b8\u2032 ,X)\u2016\n= sup C\u2032\u2208BF\u2297B\u221eX\n\u2223 \u2223 \u2223 \u2223 \u222b (\u03c0\u03b8(C \u2032 x\u0304)\u2212 \u03c0\u03b8\u2032(C \u2032 x\u0304))PX(dx\u0304) \u2223 \u2223 \u2223 \u2223\n\u2264\n\u222b\nsup A\u2208BF |\u03c0\u03b8(A)\u2212 \u03c0\u03b8\u2032(A)|PX(dx\u0304) = \u2016\u03c0\u03b8 \u2212 \u03c0\u03b8\u2032\u2016.\nSince h\u2217t\u03b8 and X are independent, \u2200A \u2208 BF , \u03c0\u03b8(A) = Ph\u2217t\u03b8 (A) = Ph\u2217t\u03b8 (A)PX(X \u221e) = P(h\u2217t\u03b8,X)(A\u00d7X \u221e). Analogous reasoning holds for h\u2217t\u03b8\u2032 . Thus, we have\n\u2016\u03c0\u03b8 \u2212 \u03c0\u03b8\u2032\u2016 = \u2016P(h\u2217t\u03b8,X)(\u00b7 \u00d7 X \u221e)\u2212 P(h\u2217 t\u03b8\u2032 ,X)(\u00b7 \u00d7 X \u221e)\u2016\n\u2264 \u2016P(h\u2217t\u03b8,X) \u2212 P(h\u2217t\u03b8\u2032 ,X)\u2016 = \u2016PZ t(\u03b8) \u2212 PZt(\u03b8\u2032)\u2016.\nAltogether, we have \u2016PZt(\u03b8) \u2212 PZt(\u03b8\u2032)\u2016 = \u2016\u03c0\u03b8 \u2212 \u03c0\u03b8\u2032\u2016. \u2293\u2294\nLemma 2. There exists a sequence rk = o(1) such that, \u2200t, k \u2208 N, \u2200\u03b8, \u03b8\u2032 \u2208 \u0398,\n\u2016PZtk(\u03b8) \u2212 PZtk(\u03b8\u2032)\u2016 \u2264 \u2016\u03c0\u03b8 \u2212 \u03c0\u03b8\u2032\u2016 \u2264 \u2016PZtk(\u03b8) \u2212 PZtk(\u03b8\u2032)\u2016+ rk.\nProof. This proof follows identically to a proof of [12], but is included here for completeness. Since PZtk(\u03b8)(A) = PZt(\u03b8)(A \u00d7 (X \u00d7 R) \u221e) for all measurable A \u2286 (X \u00d7 R)k, and similarly for \u03b8\u2032, we have\n\u2016PZt k (\u03b8) \u2212 PZt k (\u03b8\u2032)\u2016 = sup A\u2208Bk PZt k (\u03b8)(A)\u2212 PZt k (\u03b8\u2032)(A)\n= sup A\u2208Bk\nPZt(\u03b8)(A\u00d7 (X \u00d7 R) \u221e)\u2212 PZt(\u03b8\u2032)(A\u00d7 (X \u00d7 R) \u221e)\n\u2264 sup A\u2208B\u221e PZt(\u03b8)(A)\u2212 PZt(\u03b8\u2032)(A) = \u2016PZt(\u03b8) \u2212 PZt(\u03b8\u2032)\u2016,\nwhich implies the left inequality when combined with Lemma 1. Next, we focus on the right inequality. Fix \u03b8, \u03b8\u2032 \u2208 \u0398 and \u03b3 > 0, and let B \u2208 B\u221e be such that\n\u2016\u03c0\u03b8 \u2212 \u03c0\u03b8\u2032\u2016 = \u2016PZt(\u03b8) \u2212 PZt(\u03b8\u2032)\u2016 < PZt(\u03b8)(B)\u2212 PZt(\u03b8\u2032)(B) + \u03b3.\nLet A = {A \u00d7 (X \u00d7 R)\u221e : A \u2208 Bk, k \u2208 N}. Note that A is an algebra that generates B\u221e. Thus, Carathe\u0301odory\u2019s extension theorem (specifically, the version presented by [8]) implies that there exist disjoint sets {Ai}i\u2208N in A such that B \u2286 \u22c3\ni\u2208N Ai and\nPZt(\u03b8)(B) \u2212 PZt(\u03b8\u2032)(B) < \u2211\ni\u2208N\nPZt(\u03b8)(Ai)\u2212 \u2211\ni\u2208N\nPZt(\u03b8\u2032)(Ai) + \u03b3.\nSince these Ai sets are disjoint, each of these sums is bounded by a probability value, which implies that there exists some n \u2208 N such that\n\u2211\ni\u2208N\nPZt(\u03b8)(Ai) < \u03b3 +\nn \u2211\ni=1\nPZt(\u03b8)(Ai),\nwhich implies\n\u2211\ni\u2208N\nPZt(\u03b8)(Ai)\u2212 \u2211\ni\u2208N\nPZt(\u03b8\u2032)(Ai) < \u03b3 + n \u2211\ni=1\nPZt(\u03b8)(Ai)\u2212 n \u2211\ni=1\nPZt(\u03b8\u2032)(Ai)\n= \u03b3 + PZt(\u03b8)\n(\nn \u22c3\ni=1\nAi\n)\n\u2212 PZt(\u03b8\u2032)\n(\nn \u22c3\ni=1\nAi\n)\n.\nAs \u22c3n i=1 Ai \u2208 A, there exists m \u2208 N and measurable Bm \u2208 B m such that \u22c3n i=1 Ai = Bm \u00d7 (X \u00d7 R) \u221e, and therefore\nPZt(\u03b8)\n(\nn \u22c3\ni=1\nAi\n)\n\u2212 PZt(\u03b8\u2032)\n(\nn \u22c3\ni=1\nAi\n)\n= PZtm(\u03b8)(Bm)\u2212 PZtm(\u03b8\u2032)(Bm)\n\u2264 \u2016PZtm(\u03b8) \u2212 PZtm(\u03b8\u2032)\u2016 \u2264 limk\u2192\u221e \u2016PZtk(\u03b8) \u2212 PZtk(\u03b8\u2032)\u2016.\nCombining the above, we have \u2016\u03c0\u03b8 \u2212 \u03c0\u03b8\u2032\u2016 \u2264 limk\u2192\u221e \u2016PZtk(\u03b8)\u2212PZtk(\u03b8\u2032)\u2016+3\u03b3. By letting \u03b3 approach 0, we have\n\u2016\u03c0\u03b8 \u2212 \u03c0\u03b8\u2032\u2016 \u2264 lim k\u2192\u221e \u2016PZtk(\u03b8) \u2212 PZtk(\u03b8\u2032)\u2016.\nSo there exists a sequence rk(\u03b8, \u03b8 \u2032) = o(1) such that\n\u2200k \u2208 N, \u2016\u03c0\u03b8 \u2212 \u03c0\u03b8\u2032\u2016 \u2264 \u2016PZtk(\u03b8) \u2212 PZtk(\u03b8\u2032)\u2016+ rk(\u03b8, \u03b8 \u2032).\nNow let \u03b3 > 0 and let \u0398\u03b3 be a minimal \u03b3-cover of \u0398. Define the quantity rk(\u03b3) = max\u03b8,\u03b8\u2032\u2208\u0398\u03b3 rk(\u03b8, \u03b8\n\u2032). Then for any \u03b8, \u03b8\u2032 \u2208 \u0398, let \u03b8\u03b3 = argmin\u03b8\u2032\u2032\u2208\u0398\u03b3 \u2016\u03c0\u03b8\u2212\u03c0\u03b8\u2032\u2032\u2016 and \u03b8\u2032\u03b3 = argmin\u03b8\u2032\u2032\u2208\u0398\u03b3 \u2016\u03c0\u03b8\u2032 \u2212 \u03c0\u03b8\u2032\u2032\u2016. Then a triangle inequality implies that \u2200k \u2208 N,\n\u2016\u03c0\u03b8 \u2212 \u03c0\u03b8\u2032\u2016 \u2264 \u2016\u03c0\u03b8 \u2212 \u03c0\u03b8\u03b3\u2016+ \u2016\u03c0\u03b8\u03b3 \u2212 \u03c0\u03b8\u2032\u03b3\u2016+ \u2016\u03c0\u03b8\u2032\u03b3 \u2212 \u03c0\u03b8\u2032\u2016\n< 2\u03b3 + rk(\u03b8\u03b3 , \u03b8 \u2032 \u03b3) + \u2016PZtk(\u03b8\u03b3) \u2212 PZtk(\u03b8\u2032\u03b3)\u2016 \u2264 2\u03b3 + rk(\u03b3) + \u2016PZtk(\u03b8\u03b3) \u2212 PZtk(\u03b8\u2032\u03b3)\u2016.\nTriangle inequalities and the left inequality from the lemma statement (already established) imply\n\u2016PZtk(\u03b8\u03b3)\u2212PZtk(\u03b8\u2032\u03b3)\u2016 \u2264 \u2016PZtk(\u03b8\u03b3)\u2212PZtk(\u03b8)\u2016+ \u2016PZtk(\u03b8)\u2212PZtk(\u03b8\u2032)\u2016+ \u2016PZtk(\u03b8\u2032\u03b3)\u2212PZtk(\u03b8\u2032)\u2016\n\u2264 \u2016\u03c0\u03b8\u03b3 \u2212 \u03c0\u03b8\u2016+ \u2016PZtk(\u03b8) \u2212 PZtk(\u03b8\u2032)\u2016+ \u2016\u03c0\u03b8\u2032\u03b3 \u2212 \u03c0\u03b8\u2032\u2016 < 2\u03b3 + \u2016PZtk(\u03b8) \u2212 PZtk(\u03b8\u2032)\u2016.\nSo in total we have\n\u2016\u03c0\u03b8 \u2212 \u03c0\u03b8\u2032\u2016 \u2264 4\u03b3 + rk(\u03b3) + \u2016PZtk(\u03b8) \u2212 PZtk(\u03b8\u2032)\u2016.\nSince this holds for all \u03b3 > 0, defining rk = inf\u03b3>0(4\u03b3+rk(\u03b3)), we have the right inequality of the lemma statement. Furthermore, since each rk(\u03b8, \u03b8\n\u2032) = o(1), and |\u0398\u03b3 | < \u221e, we have rk(\u03b3) = o(1) for each \u03b3 > 0, and thus we also have rk = o(1).\n\u2293\u2294\nLemma 3. \u2200t, k \u2208 N, there exists a monotone function Mk(x) = o(1) such that, \u2200\u03b8, \u03b8\u2032 \u2208 \u0398,\n\u2016PZtk(\u03b8) \u2212 PZtk(\u03b8\u2032)\u2016 \u2264 Mk ( \u2016PZtd(\u03b8) \u2212 PZtd(\u03b8\u2032)\u2016 ) .\nProof. Fix any t \u2208 N, and let X = {Xt1, Xt2, . . .} andY(\u03b8) = {Yt1(\u03b8), Yt2(\u03b8), . . .}, and for k \u2208 N let Xk = {Xt1, . . . , Xtk} and Yk(\u03b8) = {Yt1(\u03b8), . . . , Ytk(\u03b8)}.\nIf k \u2264 d, then PZtk(\u03b8)(\u00b7) = PZtd(\u03b8)(\u00b7 \u00d7 (X \u00d7 {\u22121,+1}) d\u2212k), so that\n\u2016PZtk(\u03b8) \u2212 PZtk(\u03b8\u2032)\u2016 \u2264 \u2016PZtd(\u03b8) \u2212 PZtd(\u03b8\u2032)\u2016,\nand therefore the result trivially holds. Now suppose k > d. Fix any \u03b3 > 0, and let B\u03b8,\u03b8\u2032 \u2286 (X \u00d7R)k be a measurable set such that\nPZtk(\u03b8) (B\u03b8,\u03b8\u2032)\u2212 PZtk(\u03b8\u2032)(B\u03b8,\u03b8\u2032) \u2264 \u2016PZtk(\u03b8) \u2212 PZtk(\u03b8\u2032)\u2016\n\u2264 PZtk(\u03b8)(B\u03b8,\u03b8\u2032)\u2212 PZtk(\u03b8\u2032)(B\u03b8,\u03b8\u2032) + \u03b3.\nBy Carathe\u0301odory\u2019s extension theorem (specifically, the version presented by [8]), there exists a disjoint sequence of sets {Bi(\u03b8, \u03b8\u2032)}\u221ei=1 such that\nPZt k (\u03b8)(B\u03b8,\u03b8\u2032)\u2212 PZt k (\u03b8\u2032)(B\u03b8,\u03b8\u2032) < \u03b3 +\n\u221e \u2211\ni=1\nPZt k (\u03b8)(Bi(\u03b8, \u03b8\n\u2032))\u2212 \u221e \u2211\ni=1\nPZt k (\u03b8\u2032)(Bi(\u03b8, \u03b8\n\u2032)),\nand such that each Bi(\u03b8, \u03b8 \u2032) is representable as follows; for some \u2113i(\u03b8, \u03b8 \u2032) \u2208 N, and sets Cij = (Aij1 \u00d7 (\u2212\u221e, tij1])\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 (Aijk \u00d7 (\u2212\u221e, tijk]), for j \u2264 \u2113i(\u03b8, \u03b8\u2032), where each Aijp \u2208 BX , the set Bi(\u03b8, \u03b8\u2032) is representable as \u22c3\ns\u2208Si\n\u22c2\u2113i(\u03b8,\u03b8 \u2032)\nj=1 Dijs, where\nSi \u2286 {0, . . . , 2\u2113i(\u03b8,\u03b8 \u2032) \u2212 1}, each Dijs \u2208 {Cij , Ccij}, and s 6= s \u2032 \u21d2 \u22c2\u2113i(\u03b8,\u03b8 \u2032) j=1 Dijs \u2229 \u22c2\u2113i(\u03b8,\u03b8 \u2032)\nj=1 Dijs\u2032 = \u2205. Since the Bi(\u03b8, \u03b8 \u2032) are disjoint, the above sums are bounded,\nso that there exists mk(\u03b8, \u03b8 \u2032, \u03b3) \u2208 N such that every m \u2265 mk(\u03b8, \u03b8\u2032, \u03b3) has\nPZtk(\u03b8) (B\u03b8,\u03b8\u2032)\u2212 PZtk(\u03b8\u2032)(B\u03b8,\u03b8\u2032) < 2\u03b3+\nm \u2211\ni=1\nPZtk(\u03b8) (Bi(\u03b8, \u03b8\n\u2032))\u2212 m \u2211\ni=1\nPZtk(\u03b8 \u2032)(Bi(\u03b8, \u03b8\n\u2032)),\nNow define M\u0303k(\u03b3) = max\u03b8,\u03b8\u2032\u2208\u0398\u03b3 mk(\u03b8, \u03b8 \u2032, \u03b3). Then for any \u03b8, \u03b8\u2032 \u2208 \u0398, let \u03b8\u03b3 , \u03b8\u2032\u03b3 \u2208 \u0398\u03b3 be such that \u2016\u03c0\u03b8 \u2212 \u03c0\u03b8\u03b3\u2016 < \u03b3 and \u2016\u03c0\u03b8\u2032 \u2212 \u03c0\u03b8\u2032\u03b3\u2016 < \u03b3, which implies \u2016PZtk(\u03b8) \u2212 PZtk(\u03b8\u03b3) \u2016 < \u03b3 and \u2016PZtk(\u03b8\u2032) \u2212 PZtk(\u03b8\u2032\u03b3)\u2016 < \u03b3 by Lemma 2. Then\n\u2016PZtk(\u03b8) \u2212 PZtk(\u03b8\u2032)\u2016 < \u2016PZtk(\u03b8\u03b3) \u2212 PZtk(\u03b8\u2032\u03b3)\u2016+ 2\u03b3\n\u2264 PZtk(\u03b8\u03b3)(B\u03b8\u03b3 ,\u03b8\u2032\u03b3 )\u2212 PZtk(\u03b8\u2032\u03b3)(B\u03b8\u03b3 ,\u03b8\u2032\u03b3 ) + 3\u03b3\n\u2264\nM\u0303k(\u03b3) \u2211\ni=1\nPZtk(\u03b8\u03b3) (Bi(\u03b8\u03b3 , \u03b8 \u2032 \u03b3))\u2212 PZtk(\u03b8\u2032\u03b3)(Bi(\u03b8\u03b3 , \u03b8 \u2032 \u03b3)) + 5\u03b3.\nAgain, since the Bi(\u03b8\u03b3 , \u03b8 \u2032 \u03b3) are disjoint, this equals\n5\u03b3 + PZtk(\u03b8\u03b3)\n\n\nM\u0303k(\u03b3) \u22c3\ni=1\nBi(\u03b8\u03b3 , \u03b8 \u2032 \u03b3)\n\n \u2212 PZtk(\u03b8\u2032\u03b3)\n\n\nM\u0303k(\u03b3) \u22c3\ni=1\nBi(\u03b8\u03b3 , \u03b8 \u2032 \u03b3)\n\n\n\u2264 7\u03b3 + PZtk(\u03b8)\n\n\nM\u0303k(\u03b3) \u22c3\ni=1\nBi(\u03b8\u03b3 , \u03b8 \u2032 \u03b3)\n\n\u2212 PZtk(\u03b8\u2032)\n\n\nM\u0303k(\u03b3) \u22c3\ni=1\nBi(\u03b8\u03b3 , \u03b8 \u2032 \u03b3)\n\n\n= 7\u03b3 +\nM\u0303k(\u03b3) \u2211\ni=1\nPZtk(\u03b8) (Bi(\u03b8\u03b3 , \u03b8 \u2032 \u03b3))\u2212 PZtk(\u03b8\u2032)(Bi(\u03b8\u03b3 , \u03b8 \u2032 \u03b3))\n\u2264 7\u03b3 + M\u0303k(\u03b3) max i\u2264M\u0303k(\u03b3)\n\u2223 \u2223 \u2223PZtk(\u03b8) (Bi(\u03b8\u03b3 , \u03b8 \u2032 \u03b3))\u2212 PZtk(\u03b8\u2032)(Bi(\u03b8\u03b3 , \u03b8 \u2032 \u03b3)) \u2223 \u2223 \u2223 .\nThus, if we can show that each term \u2223 \u2223\n\u2223PZtk(\u03b8) (Bi(\u03b8\u03b3 , \u03b8 \u2032 \u03b3))\u2212 PZtk(\u03b8\u2032)(Bi(\u03b8\u03b3 , \u03b8 \u2032 \u03b3))\n\u2223 \u2223 \u2223\nis bounded by a o(1) function of \u2016PZtd(\u03b8) \u2212 PZtd(\u03b8\u2032)\u2016, then the result will follow by substituting this relaxation into the above expression and defining Mk by minimizing the resulting expression over \u03b3 > 0.\nToward this end, let Cij be as above from the definition of Bi(\u03b8\u03b3 , \u03b8 \u2032 \u03b3), and note that IBi(\u03b8\u03b3 ,\u03b8\u2032\u03b3) is representable as a function of the ICij indicators, so that\n\u2223 \u2223 \u2223 PZt\nk (\u03b8)(Bi(\u03b8\u03b3 , \u03b8 \u2032 \u03b3))\u2212PZtk(\u03b8\u2032)(Bi(\u03b8\u03b3 , \u03b8 \u2032 \u03b3))\n\u2223 \u2223 \u2223 = \u2016PIBi(\u03b8\u03b3,\u03b8\u2032\u03b3 )(Z t k (\u03b8))\u2212PIBi(\u03b8\u03b3,\u03b8\u2032\u03b3 )(Z t k (\u03b8\u2032))\u2016\n\u2264 \u2016P(ICi1 (Ztk(\u03b8)),...,ICi\u2113i(\u03b8\u03b3,\u03b8\u2032\u03b3 ) (Ztk(\u03b8))) \u2212 P(ICi1 (Ztk(\u03b8\u2032)),...,ICi\u2113i(\u03b8\u03b3,\u03b8\u2032\u03b3 ) (Ztk(\u03b8 \u2032)))\u2016\n\u2264 2\u2113i(\u03b8\u03b3 ,\u03b8 \u2032 \u03b3) max\nJ\u2286{1,...,\u2113i(\u03b8\u03b3 ,\u03b8\u2032\u03b3)} E\n[(\n\u220f\nj\u2208J\nICij (Z t k(\u03b8))\n)\n\u220f\nj /\u2208J\n(\n1\u2212 ICij (Z t k(\u03b8))\n)\n\u2212\n(\n\u220f\nj\u2208J\nICij (Z t k(\u03b8 \u2032))\n)\n\u220f\nj /\u2208J\n(\n1\u2212 ICij (Z t k(\u03b8 \u2032))\n)]\n\u2264 2\u2113i(\u03b8\u03b3 ,\u03b8 \u2032 \u03b3)\n\u2211\nJ\u2286 { 1,...,2 \u2113i(\u03b8\u03b3 ,\u03b8 \u2032 \u03b3 ) }\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 E   \u220f\nj\u2208J\nICij (Z t k(\u03b8)) \u2212\n\u220f\nj\u2208J\nICij (Z t k(\u03b8 \u2032))\n\n\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n\u2264 4\u2113i(\u03b8\u03b3 ,\u03b8 \u2032 \u03b3) max\nJ\u2286 { 1,...,2 \u2113i(\u03b8\u03b3 ,\u03b8 \u2032 \u03b3 ) }\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 E   \u220f\nj\u2208J\nICij (Z t k(\u03b8)) \u2212\n\u220f\nj\u2208J\nICij (Z t k(\u03b8 \u2032))\n\n\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n= 4\u2113i(\u03b8\u03b3 ,\u03b8 \u2032 \u03b3) max\nJ\u2286 { 1,...,2 \u2113i(\u03b8\u03b3 ,\u03b8 \u2032 \u03b3 ) }\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 PZtk(\u03b8)   \u22c2\nj\u2208J\nCij\n\n\u2212 PZtk(\u03b8\u2032)\n\n\n\u22c2\nj\u2208J\nCij\n\n\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 .\nNote that \u22c2\nj\u2208J Cij can be expressed as some (A1\u00d7(\u2212\u221e, t1])\u00d7\u00b7 \u00b7 \u00b7\u00d7(Ak\u00d7(\u2212\u221e, tk]),\nwhere each Ap \u2208 BX and tp \u2208 R, so that, for \u2113\u0302 = max\u03b8,\u03b8\u2032\u2208\u0398\u03b3 maxi\u2264M\u0303k(\u03b3) \u2113i(\u03b8, \u03b8 \u2032) and Ck = {(A1 \u00d7 (\u2212\u221e, t1])\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 (Ak \u00d7 (\u2212\u221e, tk]) : \u2200j \u2264 k,Aj \u2208 BX , tk \u2208 R}, this last expression is at most\n4\u2113\u0302 sup C\u2208Ck\n\u2223 \u2223 \u2223PZtk(\u03b8) (C)\u2212 PZtk(\u03b8\u2032)(C) \u2223 \u2223 \u2223 .\nNext note that for any C = (A1\u00d7 (\u2212\u221e, t1])\u00d7\u00b7 \u00b7 \u00b7\u00d7 (Ak \u00d7 (\u2212\u221e, tk]) \u2208 Ck, letting C1 = A1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7Ak and C2 = (\u2212\u221e, t1]\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 (\u2212\u221e, tk],\nPZtk(\u03b8) (C)\u2212 PZtk(\u03b8\u2032)(C) = E\n[( PYtk(\u03b8)|Xtk(C2)\u2212 PYtk(\u03b8\u2032)|Xtk(C2) ) IC1(Xtk) ]\n\u2264 E [\u2223 \u2223PYtk(\u03b8)|Xtk(C2)\u2212 PYtk(\u03b8\u2032)|Xtk(C2) \u2223 \u2223 ] .\nFor p \u2208 {1, . . . , k}, let C2p = (\u2212\u221e, tp]. Then note that, by definition of d, for any given x = (x1, . . . , xk), the class Hx = {xp 7\u2192 IC2p(h(xp)) : h \u2208 F} is a VC\nclass over {x1, . . . , xk} with VC dimension at most d. Furthremore, we have\n\u2223 \u2223PYtk(\u03b8)|Xtk(C2)\u2212 PYtk(\u03b8\u2032)|Xtk(C2) \u2223 \u2223\n= \u2223 \u2223\n\u2223 P(IC21 (h \u2217 t\u03b8(Xt1)),...,IC2k (h \u2217 t\u03b8(Xtk)))|Xtk\n({(1, . . . , 1)})\n\u2212 P(IC21 (h\u2217t\u03b8\u2032 (Xt1)),...,IC2k (h \u2217 t\u03b8\u2032\n(Xtk)))|Xtk({(1, . . . , 1)}) \u2223 \u2223 \u2223.\nTherefore, the results of [12] (in the proof of their Lemma 3) imply that\n\u2223 \u2223PYtk(\u03b8)|Xtk(C2)\u2212 PYtk(\u03b8\u2032)|Xtk(C2) \u2223 \u2223\n\u2264 2k max y\u2208{0,1}d max D\u2208{1,...,k}d\n\u2223 \u2223 \u2223P{IC2j (h \u2217 t\u03b8(Xtj))}j\u2208D |{Xtj}j\u2208D ({y})\n\u2212 P{IC2j (h\u2217t\u03b8\u2032 (Xtj))}j\u2208D |{Xtj}j\u2208D ({y}) \u2223 \u2223 \u2223 .\nThus, we have\nE [\u2223 \u2223PYtk(\u03b8)|Xtk(C2)\u2212 PYtk(\u03b8\u2032)|Xtk(C2) \u2223 \u2223 ]\n\u2264 2kE\n[\nmax y\u2208{0,1}d max D\u2208{1,...,k}d\n\u2223 \u2223 \u2223P{IC2j (h \u2217 t\u03b8(Xtj))}j\u2208D |{Xtj}j\u2208D ({y})\n\u2212 P{IC2j (h\u2217t\u03b8\u2032 (Xtj))}j\u2208D |{Xtj}j\u2208D ({y}) \u2223 \u2223 \u2223\n]\n\u2264 2k \u2211\ny\u2208{0,1}d\n\u2211\nD\u2208{1,...,k}d\nE\n[\n\u2223 \u2223 \u2223P{IC2j (h \u2217 t\u03b8(Xtj))}j\u2208D |{Xtj}j\u2208D ({y})\n\u2212 P{IC2j (h\u2217t\u03b8\u2032 (Xtj))}j\u2208D |{Xtj}j\u2208D ({y}) \u2223 \u2223 \u2223\n]\n\u2264 2d+kkd max y\u2208{0,1}d max D\u2208{1,...,k}d E\n[\n\u2223 \u2223 \u2223P{IC2j (h \u2217 t\u03b8(Xtj))}j\u2208D |{Xtj}j\u2208D ({y})\n\u2212 P{IC2j (h\u2217t\u03b8\u2032 (Xtj))}j\u2208D |{Xtj}j\u2208D ({y}) \u2223 \u2223 \u2223\n]\n.\nExchangeability implies this is at most\n2d+kkd max y\u2208{0,1}d sup t1,...,td\u2208R E\n[\n\u2223 \u2223 \u2223P{I(\u2212\u221e,tj ](h \u2217 t\u03b8(Xtj))} d j=1|Xtd ({y})\n\u2212 P{I(\u2212\u221e,tj ](h \u2217 t\u03b8\u2032 (Xtj))}dj=1|Xtd ({y})\n\u2223 \u2223 \u2223\n]\n= 2d+kkd max y\u2208{0,1}d sup t1,...,td\u2208R E\n[\n\u2223 \u2223 \u2223P{I(\u2212\u221e,tj ](Ytj(\u03b8))} d j=1|Xtd ({y})\n\u2212 P{I(\u2212\u221e,tj ](Ytj(\u03b8\u2032))} d j=1|Xtd\n({y}) \u2223 \u2223\n\u2223\n]\n.\n[12] argue that for all y \u2208 {0, 1}d and t1, . . . , td \u2208 R,\nE\n[\u2223\n\u2223 \u2223P{I(\u2212\u221e,tj ](Ytj(\u03b8))} d j=1|Xtd ({y})\u2212 P{I(\u2212\u221e,tj ](Ytj(\u03b8\u2032))} d j=1|Xtd\n({y}) \u2223 \u2223\n\u2223\n]\n\u2264 4 \u221a\n\u2016P{I(\u2212\u221e,tj ](Ytj(\u03b8))} d j=1,Xtd \u2212 P{I(\u2212\u221e,tj ](Ytj(\u03b8\u2032))} d j=1,Xtd \u2016.\nNoting that\n\u2016P{I(\u2212\u221e,tj ](Ytj(\u03b8))} d j=1,Xtd \u2212 P{I(\u2212\u221e,tj ](Ytj(\u03b8\u2032))} d j=1,Xtd \u2016 \u2264 \u2016PZtd(\u03b8) \u2212 PZtd(\u03b8\u2032)\u2016\ncompletes the proof. \u2293\u2294\nWe are now ready for the proof of Theorem 4.\nProof (Proof of Theorem 4). The estimator \u03b8\u0302T\u03b8\u22c6 we will use is precisely the minimum-distance skeleton estimate of PZtd(\u03b8\u22c6) [13,5]. [13] proved that if N(\u03b5) is the \u03b5-covering number of {PZtd(\u03b8\u22c6) : \u03b8 \u2208 \u0398}, then taking this \u03b8\u0302T\u03b8\u22c6 estimator, then for some T\u03b5 = O((1/\u03b5 2) logN(\u03b5/4)), any T \u2265 T\u03b5 has\nE\n[\n\u2016PZtd(\u03b8\u0302T\u03b8\u22c6 ) \u2212 PZtd(\u03b8\u22c6)\u2016\n]\n< \u03b5.\nThus, taking GT = inf{\u03b5 > 0 : T \u2265 T\u03b5}, we have\nE\n[\n\u2016PZtd(\u03b8\u0302T\u03b8\u22c6 ) \u2212 PZtd(\u03b8\u22c6)\u2016\n]\n\u2264 GT = o(1).\nLetting R\u2032(T, \u03b1) be any positive sequence withGT \u226a R\u2032(T, \u03b1) \u226a 1 and R\u2032(T, \u03b1) \u2265 GT /\u03b1, and letting \u03b4(T, \u03b1) = GT /R \u2032(T, \u03b1) = o(1), Markov\u2019s inequality implies\nP\n(\n\u2016PZtd(\u03b8\u0302T\u03b8\u22c6 ) \u2212 PZtd(\u03b8\u22c6)\u2016 > R\n\u2032(T, \u03b1) ) \u2264 \u03b4(T, \u03b1) \u2264 \u03b1. (2)\nLetting R(T, \u03b1) = mink (Mk (R \u2032(T, \u03b1)) + rk), since R \u2032(T, \u03b1) = o(1) and rk = o(1), we have R(T, \u03b1) = o(1). Furthermore, composing (2) with Lemmas 1, 2, and 3, we have\nP\n(\n\u2016\u03c0\u03b8\u0302T\u03b8\u22c6 \u2212 \u03c0\u03b8\u22c6\u2016 > R(T, \u03b1)\n)\n\u2264 \u03b4(T, \u03b1) \u2264 \u03b1.\n\u2293\u2294\nRemark: Although the above proof makes use of the minimum-distance skeleton estimator, which is typically not computationally efficient, it is often possible to achieve this same result (for certain families of distributions) using a simpler estimator, such as the maximum likelihood estimator. All we require is that the risk of the estimator converges to 0 at a known rate that is independent of \u03b8\u22c6. For instance, see [6] for conditions on the family of distributions sufficient for this to be true of the maximum likelihood estimator."}], "references": [{"title": "Sampling lower bounds via information theory", "author": ["Z. Bar-Yossef"], "venue": "Proceedings of the 35th Annual ACM Symposium on the Theory of Computing. pp. 335\u2013344", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "A Bayesian/information theoretic model of learning to learn via multiple task sampling", "author": ["J. Baxter"], "venue": "Machine Learning 28, 7\u201339", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Learnability and the Vapnik-Chervonenkis dimension", "author": ["A. Blumer", "A. Ehrenfeucht", "D. Haussler", "M. Warmuth"], "venue": "Journal of the Association for Computing Machinery 36(4), 929\u2013965", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1989}, {"title": "Combinatorial Auctions", "author": ["P. Cramton", "Y. Shoham", "R. Steinberg"], "venue": "The MIT Press", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Combinatorial Methods in Density Estimation", "author": ["L. Devroye", "G. Lugosi"], "venue": "Springer, New York, NY, USA", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Empirical Processes in M-Estimation", "author": ["S. van de Geer"], "venue": "Cambridge University Press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "MDL convergence speed for Bernoulli sequences", "author": ["J. Poland", "M. Hutter"], "venue": "Statistics and Computing 16, 161\u2013175", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Theory of Statistics", "author": ["M.J. Schervish"], "venue": "Springer, New York, NY, USA", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1995}, {"title": "Estimation of Dependencies Based on Empirical Data", "author": ["V. Vapnik"], "venue": "Springer-Verlag, New York", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1982}, {"title": "On the uniform convergence of relative frequencies of events to their probabilities", "author": ["V. Vapnik", "A. Chervonenkis"], "venue": "Theory of Probability and its Applications 16, 264\u2013280", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1971}, {"title": "Sequential tests of statistical hypotheses", "author": ["A. Wald"], "venue": "The Annals of Mathematical Statistics 16(2), 117\u2013186", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1945}, {"title": "A theory of transfer learning with applications to active learning", "author": ["L. Yang", "S. Hanneke", "J. Carbonell"], "venue": "Machine Learning 90(2), 161\u2013189", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Rates of convergence of minimum distance estimators and Kolmogorov\u2019s entropy", "author": ["Y.G. Yatracos"], "venue": "The Annals of Statistics 13, 768\u2013774", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1985}, {"title": "On polynomial-time preference elicitation with value queries", "author": ["M. Zinkevich", "A. Blum", "T. Sandholm"], "venue": "Proceedings of the 4 ACM Conference on Electronic Commerce. pp. 175\u2013185", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 1, "context": "Among the several proposed models for transfer learning, one particularly appealing model supposes the learning problems are independent and identically distributed, with unknown distribution, and the advantage of transfer learning then comes from the ability to estimate this shared distribution based on the data from past learning problems [2,12].", "startOffset": 343, "endOffset": 349}, {"referenceID": 11, "context": "Among the several proposed models for transfer learning, one particularly appealing model supposes the learning problems are independent and identically distributed, with unknown distribution, and the advantage of transfer learning then comes from the ability to estimate this shared distribution based on the data from past learning problems [2,12].", "startOffset": 343, "endOffset": 349}, {"referenceID": 11, "context": "In recent work, [12] have shown that under mild conditions on the family of possible distributions, if the target concepts reside in a known VC class, then it is possible to estimate this distribtion using only a bounded number of training samples per task: specifically, a number of samples equal the VC dimension.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "The present work continues that of [12], bounding the rate of convergence for estimating this distribution, under a smoothness condition on the distribution.", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "We additionally include an extension of the results of [12] to the setting of real-valued functions, establishing consistency (at a uniform rate) for an estimator of a prior over any VC subgraph class.", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "In addition to the application to transfer learning, analogous to the original work of [12], we also discuss an application of this result to a preference elicitation problem in algorithmic economics, in which we are tasked with allocating items to a sequence of customers to approximately maximize the customers\u2019 satisfaction, while permitted access to the customer valuation functions only via value queries.", "startOffset": 87, "endOffset": 91}, {"referenceID": 7, "context": "2 The Setting Let (X ,BX ) be a measurable space [8] (where X is called the instance space), and let D be a distribution on X (called the data distribution).", "startOffset": 49, "endOffset": 52}, {"referenceID": 9, "context": "Let C be a VC class of measurable classifiers h : X \u2192 {\u22121,+1} (called the concept space), and denote by d the VC dimension of C [10].", "startOffset": 128, "endOffset": 132}, {"referenceID": 7, "context": "We suppose there exists a probability measure \u03c00 on C (the reference measure) such that every \u03c0\u03b8 is absolutely continuous with respect to \u03c00, and therefore has a density function f\u03b8 given by the Radon-Nikodym derivative d\u03c0\u03b8 d\u03c00 [8].", "startOffset": 228, "endOffset": 231}, {"referenceID": 11, "context": "In previous work, [12] showed that, if \u03a0\u0398 is a totally bounded family, then even with only d number of samples per task, the minimax risk (as a function of the number of tasks T ) converges to zero.", "startOffset": 18, "endOffset": 22}, {"referenceID": 8, "context": "By the standard PAC analysis [9,3], for any \u03b3 > 0, with probability greater than 1 \u2212 \u03b3, a sample of k = O((d/\u03b3) log(1/\u03b3)) random points will partition C into regions of width less than \u03b3 (under L1(D)).", "startOffset": 29, "endOffset": 34}, {"referenceID": 2, "context": "By the standard PAC analysis [9,3], for any \u03b3 > 0, with probability greater than 1 \u2212 \u03b3, a sample of k = O((d/\u03b3) log(1/\u03b3)) random points will partition C into regions of width less than \u03b3 (under L1(D)).", "startOffset": 29, "endOffset": 34}, {"referenceID": 11, "context": "Following analogous to the inductive argument of [12], suppose I \u2286 {1, .", "startOffset": 49, "endOffset": 53}, {"referenceID": 11, "context": "[12] showed that E [ |PYd(\u03b8)|Xd(\u0233 )\u2212 PYd(\u03b8\u2032)|Xd(\u0233 )| ] \u2264 4 \u221a \u2016PZd(\u03b8) \u2212 PZd(\u03b8\u2032)\u2016, so that in total we have \u2016\u03c0\u03b8 \u2212 \u03c0\u03b8\u2032\u2016 < (L+1)\u03b3+4(2ek) \u221a \u2016PZd(\u03b8)\u2212PZd(\u03b8\u2032)\u2016.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "IfN(\u03b5) is the \u03b5-covering number of {PZd(\u03b8) : \u03b8 \u2208 \u0398}, then taking \u03b8\u0302T\u03b8\u22c6 as the minimum distance skeleton estimate of [13,5] achieves expected total variation distance \u03b5 from PZd(\u03b8\u22c6), for some T = O((1/\u03b5 ) logN(\u03b5/4)).", "startOffset": 116, "endOffset": 122}, {"referenceID": 4, "context": "IfN(\u03b5) is the \u03b5-covering number of {PZd(\u03b8) : \u03b8 \u2208 \u0398}, then taking \u03b8\u0302T\u03b8\u22c6 as the minimum distance skeleton estimate of [13,5] achieves expected total variation distance \u03b5 from PZd(\u03b8\u22c6), for some T = O((1/\u03b5 ) logN(\u03b5/4)).", "startOffset": 116, "endOffset": 122}, {"referenceID": 11, "context": "Furthermore, the covering number of \u03a0\u0398 upper bounds N(\u03b5) [12], so that N(\u03b5) \u2264 (1/\u03b5) d/\u03b1).", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "d Bernoulli(p) random variables, for each p \u2208 [0, 1]; then it is known that, for any (possibly nondeterministic) decision rule", "startOffset": 46, "endOffset": 52}, {"referenceID": 0, "context": "(1) This easily follows from the results of [1], combined with a result of [7] bounding the KL divergence (see also [11]) To use this result, we construct a learning problem as follows.", "startOffset": 44, "endOffset": 47}, {"referenceID": 6, "context": "(1) This easily follows from the results of [1], combined with a result of [7] bounding the KL divergence (see also [11]) To use this result, we construct a learning problem as follows.", "startOffset": 75, "endOffset": 78}, {"referenceID": 10, "context": "(1) This easily follows from the results of [1], combined with a result of [7] bounding the KL divergence (see also [11]) To use this result, we construct a learning problem as follows.", "startOffset": 116, "endOffset": 120}, {"referenceID": 7, "context": ", i \u2217 T ) = q\u0302 \u2032 i(CiT (\u03b8\u22c6)), for some q\u0302 \u2032 i [8]: that is, estimators that are a function of the NiT (\u03b8\u22c6) = |CiT (\u03b8\u22c6)| Bernoulli(pi) random variables, which we should note are conditionally i.", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "samples from a H\u00f6lder smooth density in a d-dimensional space [5].", "startOffset": 62, "endOffset": 65}, {"referenceID": 11, "context": "Prior Estimation 11 5 Real-Valued Functions and an Application in Algorithmic Economics In this section, we present results generalizing the analysis of [12] to classes of real-valued functions.", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "The proof parallels that of [12] (who studied the special case of binary functions), with a few important twists (in particular, a significantly different approach in the analogue of their Lemma 3).", "startOffset": 28, "endOffset": 32}, {"referenceID": 0, "context": ",Z T d (\u03b8\u22c6)), and functions R : N0 \u00d7 (0, 1] \u2192 [0,\u221e) and \u03b4 : N0 \u00d7 (0, 1] \u2192 [0, 1] such that, for any \u03b1 > 0, lim T\u2192\u221e R(T, \u03b1) = lim T\u2192\u221e \u03b4(T, \u03b1) = 0 and for any T \u2208 N0 and \u03b8\u22c6 \u2208 \u0398, P (", "startOffset": 74, "endOffset": 80}, {"referenceID": 11, "context": "2 Maximizing Customer Satisfaction in Combinatorial Auctions Theorem 4 has a clear application in the context of transfer learning, following analogous arguments to those given in the special case of binary classification by [12].", "startOffset": 225, "endOffset": 229}, {"referenceID": 3, "context": "However, we are not permitted direct observation of the customer valuation functions; rather, we may query for the value of any given bundle of items; this is referred to as a value query in the literature on preference elicitation in combinatorial auctions (see Chapter 14 of [4], [14]).", "startOffset": 277, "endOffset": 280}, {"referenceID": 13, "context": "However, we are not permitted direct observation of the customer valuation functions; rather, we may query for the value of any given bundle of items; this is referred to as a value query in the literature on preference elicitation in combinatorial auctions (see Chapter 14 of [4], [14]).", "startOffset": 282, "endOffset": 286}, {"referenceID": 3, "context": "This is referred to as a value query in the literature on preference elicitation in combinatorial auctions (see Chapter 14 of [4], [14]).", "startOffset": 126, "endOffset": 129}, {"referenceID": 13, "context": "This is referred to as a value query in the literature on preference elicitation in combinatorial auctions (see Chapter 14 of [4], [14]).", "startOffset": 131, "endOffset": 135}, {"referenceID": 11, "context": "The proof of this result parallels that of [12] for the transfer learning setting, but is included here for completeness.", "startOffset": 43, "endOffset": 47}, {"referenceID": 11, "context": "16 Liu Yang, Steve Hanneke, and Jaime Carbonell A Proofs for Section 5 The proof of Theorem 4 is based on the following sequence of lemmas, which parallel those used by [12] for establishing the analogous result for consistent estimation of priors over binary functions.", "startOffset": 169, "endOffset": 173}, {"referenceID": 11, "context": "The last of these lemmas (namely, Lemma 3) requires substantial modifications to the original argument of [12]; the others use arguments more-directly based on those of [12].", "startOffset": 106, "endOffset": 110}, {"referenceID": 11, "context": "The last of these lemmas (namely, Lemma 3) requires substantial modifications to the original argument of [12]; the others use arguments more-directly based on those of [12].", "startOffset": 169, "endOffset": 173}, {"referenceID": 9, "context": "Note that since F is a uniformly bounded VC subgraph class, so is the collection of functions {|h\u2212 g| : h, g \u2208 F}, so that the uniform strong law of large numbers implies that with probability one, \u2200h, g \u2208 F , \u03c1X(h, g) exists and has \u03c1X(h, g) = \u03c1(h, g) [10].", "startOffset": 253, "endOffset": 257}, {"referenceID": 11, "context": "This proof follows identically to a proof of [12], but is included here for completeness.", "startOffset": 45, "endOffset": 49}, {"referenceID": 7, "context": "Thus, Carath\u00e9odory\u2019s extension theorem (specifically, the version presented by [8]) implies that there exist disjoint sets {Ai}i\u2208N in A such that B \u2286 \u22c3 i\u2208N Ai and PZt(\u03b8)(B) \u2212 PZt(\u03b8\u2032)(B) < \u2211", "startOffset": 79, "endOffset": 82}, {"referenceID": 7, "context": "By Carath\u00e9odory\u2019s extension theorem (specifically, the version presented by [8]), there exists a disjoint sequence of sets {Bi(\u03b8, \u03b8)}i=1 such that PZt k (\u03b8)(B\u03b8,\u03b8\u2032)\u2212 PZt k (\u03b8\u2032)(B\u03b8,\u03b8\u2032) < \u03b3 + \u221e \u2211", "startOffset": 76, "endOffset": 79}, {"referenceID": 11, "context": "Therefore, the results of [12] (in the proof of their Lemma 3) imply that", "startOffset": 26, "endOffset": 30}, {"referenceID": 11, "context": "[12] argue that for all y \u2208 {0, 1} and t1, .", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "The estimator \u03b8\u0302T\u03b8\u22c6 we will use is precisely the minimum-distance skeleton estimate of PZt d(\u03b8\u22c6) [13,5].", "startOffset": 97, "endOffset": 103}, {"referenceID": 4, "context": "The estimator \u03b8\u0302T\u03b8\u22c6 we will use is precisely the minimum-distance skeleton estimate of PZt d(\u03b8\u22c6) [13,5].", "startOffset": 97, "endOffset": 103}, {"referenceID": 12, "context": "[13] proved that if N(\u03b5) is the \u03b5-covering number of {PZt d(\u03b8\u22c6) : \u03b8 \u2208 \u0398}, then taking this \u03b8\u0302T\u03b8\u22c6 estimator, then for some T\u03b5 = O((1/\u03b5 ) logN(\u03b5/4)), any T \u2265 T\u03b5 has E [", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "Abstract. We study the optimal rates of convergence for estimating a prior distribution over a VC class from a sequence of independent data sets respectively labeled by independent target functions sampled from the prior. We specifically derive upper and lower bounds on the optimal rates under a smoothness condition on the correct prior, with the number of samples per data set equal the VC dimension. These results have implications for the improvements achievable via transfer learning. We additionally extend this setting to real-valued function, where we establish consistency of an estimator for the prior, and discuss an additional application to a preference elicitation problem in algorithmic economics.", "creator": "LaTeX with hyperref package"}}}