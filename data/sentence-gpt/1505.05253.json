{"id": "1505.05253", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2015", "title": "Knowlege Graph Embedding by Flexible Translation", "abstract": "Knowledge graph embedding refers to projecting entities and relations in knowledge graph into continuous vector spaces. State-of-the-art methods, such as TransE, TransH, and TransR build embeddings by treating relation as translation from head entity to tail entity. However, previous models can not deal with reflexive/one-to-many/many-to-one/many-to-many relations properly, or lack of scalability and efficiency. Thus, we propose a novel method, flexible translation, named TransF, to address the above issues. TransF regards relation as translation between head entity vector and tail entity vector with flexible magnitude. To evaluate the proposed model, we conduct link prediction and triple classification on benchmark datasets. Experimental results show that our method remarkably improve the performance compared with several state-of-the-art baselines. Further, these results reveal a strong advantage for a more accurate model than a traditional representation for the human condition. Moreover, the model has been able to accurately estimate the extent to which a state of the ground is being transported and transported. The first step in the classification is to establish a model that captures all of the parameters, e.g., the direction of movement, velocity, speed, movement, and velocity, along with each other. TransF identifies a wide range of parameters such as distance, elevation, altitude, and velocity that can be used to infer whether a given state is moving at all. This is accomplished by selecting a model that captures all of the parameters and allows for a further evaluation. In this model, we describe our model as a model for estimating the magnitude of a state, and that can be tested in real time. A number of model parameters can be compared with the following data. The basic model is to assume that a state that is moving at any point is being transported in a given time. In addition, the model can also specify an infinite number of parameters. The results are presented in a simplified form. The model itself contains three basic functions: the model and the model, the model and the model. The model consists of a number of models. All three functions are named from the model name and the model are named by the model name in terms of the length of the path. The second function is the first function which has a maximum of 1. A total number of parameters is defined. The parameters include a model name, which includes the name of the target state, the path of the target state and the path of the target state. The second function", "histories": [["v1", "Wed, 20 May 2015 05:57:32 GMT  (801kb,D)", "http://arxiv.org/abs/1505.05253v1", null], ["v2", "Thu, 10 Sep 2015 03:48:55 GMT  (0kb,I)", "http://arxiv.org/abs/1505.05253v2", "This paper has been withdraw by the author due to an error in sec3.1"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jun feng", "mantong zhou", "yu hao", "minlie huang", "xiaoyan zhu"], "accepted": false, "id": "1505.05253"}, "pdf": {"name": "1505.05253.pdf", "metadata": {"source": "CRF", "title": "Knowlege Graph Embedding by Flexible Translation", "authors": ["Jun Feng", "Mantong Zhou", "Yu Hao", "Minlie Huang", "Xiaoyan Zhu"], "emails": ["zmt.keke}@gmail.com,", "haoyu@mail.tsinghua.edu.cn,", "zxy-dcs}@tsinghua.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Knowledge graphs such as Wordnet [Miller, 1995] and Freebase [Bollacker et al., 2008] are useful resources for natural language understanding, question answering, web search, etc. A knowledge graph contains highly-structured and wellorganized data, and is usually represented by directed graph in which nodes correspond to entities and edges correspond to relation, or simply by a set of triples (head entity, relation, tail entity) ((h, r, t) for short). Although there have been substantial achievements in building large-scale knowledge graph, the general paradigm to support computing is not clear. Indeed, traditional knowledge graphs are symbolic and logical frameworks which are not flexible enough to be fruitfully exported, especially to statistical learning approaches which require the knowledge to be computable in numerical forms.\nRecently, knowledge graph embedding, which projects entity or/and relation into continuous vector spaces, has been a new proposal to offer the powerful capability of computing on knowledge graph. In this paradigm, the embedding representation of a single entity/relation encodes the global information of the entire knowledge graph, since it is obtained by minimizing a global loss function involving all entities and relations. Furthermore, the embedding representations are ben-\neficial to a variety of applications such as question answering and web search concerning knowledge computation. Taking knowledge graph completion as an example, we can simply judge the correctness of a triple (h, r, t) by checking the compatibility of the embedding vectors of h, r and t.\nA variety of approaches have been explored for knowledge graph embedding, such as neural network based [Bordes et al., 2011; Socher et al., 2013], and translation based [Bordes et al., 2013] approaches. Some notable works, including TransE [Bordes et al., 2013], TransH [Wang et al., 2014] , and TransR [Lin et al., 2015] are simple, efficient, and effective.\nInspired by [Mikolov et al., 2013], TransE learns vector embedding for both entities and relations. In TransE, relation is represented as translation from head entity to tail entity in the embedding space, which applies h + r \u2248 t, if (h, r, t) holds. However, TransE does not work well when dealing with relations of reflexive/one-to-many/many-to-one/manyto-many. Taking a one-to-many relation publish song as the example, we have triples such as (Michael Jackson, publish song, Beat It), (Michael Jackson, publish song, Billie Jean) and (Michael Jackson, publish song, Thriller). As shown in Figure 1(a), considering the ideal embedding where h + r = t, the entities Beat It, Billie Jean and Thriller will get the same embedding vectors. Although TransE does not enforce h+ r = t for golden triples during training, the tendency still exists.\nAs a variant of TransE, TransR models entities and relations in separate spaces and performs translation in the cor-\nar X\niv :1\n50 5.\n05 25\n3v 1\n[ cs\n.C L\n] 2\n0 M\nay 2\n01 5\nresponding relation space. Unfortunately, since TransR applies hr + r \u2248 tr, if (h, r, t) holds, the problem of TransE also exists in the TransR model. Moreover, TransR are with much more parameters and cost much more training time than TransE.\nTransH is proposed to solve this problem by interpreting a relation as a translating operation on a hyperplane, but it introduces more parameters than TransE. Accordingly, the model complexity and training time of TransH is largely increased. As shown in Table 4, the training time of TransH is nearly 24 times of that of TransE and TransF.\nTo address the above issues, we aim to propose an effective and simple model so that it can overcome the flaws in TransE, but also with good efficiency. The central idea of our model is well-motivated: we regard the entities as vectors and relations as translation between head entity vector and tail entity vector with flexible magnitude, instead of enforcing the strict transition based on the vector addition. We call it Flexible Translation(TransF). Here, flexible translation means unlike conventional vector addition , say h + r = t, the left and right terms need to have the same direction and the same magnitude, flexible translation only needs to keep the same direction between the two terms. Therefore, unlike TransE assumes h + r \u2248 t if (h, r, t) holds, our proposed model takes h+ r \u2248 \u03b1t, where the flexibility is reflected in \u03b1. This relaxation not only fixes the flaws of TransE on modeling reflexive/one-to-many/many-to-one/many-to-many relations, but is practically very efficient. Taking the same one-to-many relation publish song as the example, as shown in Figure 1(b), the ideal embedding where h + r \u2248 \u03b1t is that the embedding vectors of three entities Beat It, Billie Jean and Thriller are with the same direction but are of different vector magnitudes. Actually, the idea of TransF is general. With applying to the state-of-art models, remarkable improvements can be obtained. As an example, in this paper, we upgrade the well performed TransR model to TransRF, which obeys hr + r \u2248 \u03b1tr. Following the principle of flexible translation, our proposed models contribute remarkable improvements over the state-of-the-art models.\nTo sum up, the characteristics and contributions of our work include:\n\u2022 Effectiveness: Our TransF model successfully solve the flaws in TransE when dealing with reflexive/oneto-many/many-to-one/many-to-many relations, and achieve significant improvements over the baseline models including TransH in the link prediction and triple classification tasks. Furthermore, our model can better distinguish the golden and corrupted triples compared with TransE.\n\u2022 Efficiency: Our TransF model learns only one embedding space for both entity and relation, without any extra parameters, and as a result, the training process is very fast.\n\u2022 Portability: Our model is general in nature. The principle of flexible translation can be used to not only improve TransE (the resulting model is transF) but also improve TransR (the resulting model is TransRF). Both models obtain substantial improvements over the origi-\nnal versions. The rest of this paper are organized as follows. In Section 2, we introduce some related works of knowledge graph embedding. In Section 3, we explain our TransF model. Section 4 presents two experiments: link prediction, triple classification. Finally, we conclude the paper in Section 5."}, {"heading": "2 Related Work", "text": "There are a variety of models for knowledge graph embedding. Each model projects entities and relations into a continuous vector space and the triples are assigned with scores to represent their correctness. Two lines of works are surveyed here."}, {"heading": "2.1 Translation-based Models", "text": "TransE [Bordes et al., 2013] represents relationships by translation vectors in an embedding space. It assumes that if (h, r, t) holds, the embedding of the tail entity t should be close to that of the head entity h plus the relation vector r, i.e., h + r \u2248 t. Hence, TransE adopts fr(h, t) = \u2016 h+ r\u2212 t \u2016l1/2 as the score which is low when the triple is correct and high otherwise. TransE is very efficient and performs well to one-to-one relations whilst bad in dealing with reflexive /one-to-many/many-to-one/many-to-many relations.\nTransH [Wang et al., 2014] attempts to alleviate the problems of TransE when dealing with reflexive /one-tomany/many-to-one/many-to-many relations by modeling a relation as a relation-specific hyperplane with wr as the normal vector together with a translation operation dr on it. After projecting entity embeddings h and t to the hyperplane, on the same assumption as in TransE, the projections h\u22a5 and t\u22a5 are connected by dr in a new space. Thus, TransH defines a scoring function fr(h, t) = \u2016 h\u22a5 + dr \u2212 t\u22a5 \u201622 to measure the plausibility that the triple is incorrect.\nTransR [Lin et al., 2015] addresses the issue that some entities are similar in the entity space but comparably different in other specific aspects. The model builds embeddings in distinct entity space and multiple relation spaces and performs translation in the relation space. In TransR, each relation r is associated with a mapping matrix Mr. Entities may be projected from the entity space to the relation space as hr and tr. Similar to TransE, the score function is correspondingly defined as fr(h, t) = \u2016 hr + r\u2212 tr \u201622 which produces lower scores for golden triples.\nAs mentioned, TransE, TransH, and TransR are all based on the score function fr(h, t) = \u2016 h+ r\u2212 t \u2016l1/2 of TransE. However, TransE ignores the relation categories and does not work well in dealing with one-to-many/many-to-one/manyto-many relations. Although TransH is capable of fixing the flaw, it introduces additional parameters, which sacrifices the efficiency of the model. For the TransR model, it still have the same problem as TransE."}, {"heading": "2.2 Other Related Models", "text": "Besides TransE, TransH and TransR, there are many other models proposed for knowledge graph embedding. We introduce several typical models.\nUnstructured Model [Bordes et al., 2014] regards as the naive case of TransE, which sets all translation r = 0, i.e., the score function becomes fr(h, t) = \u2016 h\u2212 t \u2016l1/2 . Obviously, it cannot distinguish different relations. Structured Embedding [Bordes et al., 2011] introduces two independent projections for the entities in a relation. The basic idea is that when two entities belong to the same triple, their embedding should be close to each other in some subspace. The score function for the triple is defined as fr(h, t) = \u2016Wr,1h\u2212Wr,2t \u2016l1 . As pointed out by [Socher et al., 2013],this model is weak to capture the correlations between entities and relations because of the introduction of two separate matrices. Semantic Matching Energy [Bordes et al., 2014] captures the interactions of relation vectors and entity vectors through multiple matrix Hadamard products. Accordingly, the score function has the following linear form fr(h, t) = (W1h + W2r + b1) >(W3t + W4r + b2) or the bilinear form fr(h, t) = (W1h\u2297W2r+ b1)>(W3t\u2297W4r+ b2). Parameters are shared by all relations. Neural Tensor Network [Socher et al., 2013] defines an expressive score function as fr(h, t) = u>r g(h\n>Mrt+Mr,1h+Mr,2t+br) where ur is a relationspecific linear layer, g() is the tanh operation. Even when the tensor Wr degenerates to a matrix, it covers most other models. However the model complexity is much higher, making it difficult to handle large scale graphs.\nWe also treat Single Layer Model [Socher et al., 2013], Latent Factor Model [Jenatton et al., 2012] and RESCAL [Nickel et al., 2011; Nickel et al., 2012] as our baseline in experiments."}, {"heading": "3 Method", "text": "To address the issues of TransE, TransH, and TransR as mentioned before, we propose a knowledge graph embedding model based on flexible translation (TransF). TransF can essentially overcome the problems of TransE in modeling reflexive/one-to-many/many-to-one/many-to-many relations. The details of the model are described in Section 3.1. Following the same principle of flexible translation, we also propose TransRF model which is an improved variant of TransR, as presented in Section 3.2.\nLet\u2019s introduce some notations. S denotes a set of golden triples, while S\u2032 denotes a set of corrupted triples. A triple (h, r, t) consists of two entities h, t \u2208 E (the set of entities) and relation r \u2208 R (the set of relations). We use the bold\nletters h, r and t to denote the corresponding embedding representations."}, {"heading": "3.1 Flexible Translation: TransF", "text": "We first analyze the limitations of TransE, TransR and TransH models. Then we explain our TransF model in detail.\nAs mentioned before, TransE and TransR work well to irreflexive and one-to-one relations but they have problems with reflexive/one-to-many/many-to-one/many-to-many relations. The reason is that both TransE and TransR adopt the score function \u2016 h+r\u2212t \u2016l1/2 . When triple (h, r, t) holds, the score function is low, meaning that h+ r \u2248 t. More specifically, if with the ideal embedding using the function h+r = t when (h, r, t) holds, we can obtain: 1) if (h, r, t) and (t, r, h) are both correct, r is a reflexive relation and r = 0,h = t; 2) if a set of triples (h, r, ti),\u2200i \u2208 0, \u00b7 \u00b7 \u00b7 , n hold, r is a oneto-many relation and t0 = \u00b7 \u00b7 \u00b7 = tn; 3) if a set of triples (hi, r, t),\u2200i \u2208 0, \u00b7 \u00b7 \u00b7 , n hold, r is a many-to-one relation and h0 = \u00b7 \u00b7 \u00b7 = hn. Although TransH can solve this problem, it introduces extra parameters for each relation and are not flexible and efficient enough to improve the TransR model.\nTo alleviate the problem of TransE and maintain the high efficiency, we apply h + r \u2248 \u03b1t, \u03b1 > 0, instead of h + r \u2248 t, when (h, r, t) holds. That means we only need to maintain the directions of vectors h + r and t, but ignore their magnitudes. Therefore, 1)when r is a reflexive relation, we get h = 2\u03b11\u03b12\u22121r, t = \u03b12+1 \u03b11\u03b12\u22121r, where h + r \u2248 \u03b11t, t + r \u2248 \u03b12h; 2)if r is a one-to-many relation, , we get t0 = h+r \u03b10\n, \u00b7 \u00b7 \u00b7 , tn = h+r\u03b1n ; 3) if r is a many-to-one relation, we get h0 = \u03b10t\u2212 r, \u00b7 \u00b7 \u00b7 ,hn = \u03b1nt\u2212 r.\nThe score function is then defined as follows:\nfr(h, t) = (h+ r) >t (1)\nHowever, with the score function, the constraints on head entity(h) and tail entity(t) are unbalanced. More specifically, under the constraints, the range of h is a line, and the range of t is a plane. Considering the perfect no-error embedding, we discuss the constraints on tail entity and head entity separately. As shown in Figure 2(a), when the embedding vectors h and r hold, the range of the no-error embedding vector t is a vector with the right arrow on the dotted line. However, as shown in Figure 2(b), when the embedding vectors r and t are known, the range of the perfect embedding vector h is a vector with the starting point on the dotted line a and the ending point on the dotted line b.\nSince head entity and tail entity need to have the same effect during training, the constraints on them should be balanced. To this end, we design a Flexible Translation(TransF) model to address the unbalanced constraint problem. We modify the score function as follows:\nfr(h, t) = (h+ r) >t+ h>(t\u2212 r) (2)\nThe score is expected to be higher for a golden triple while lower for a corrupted one. As shown in Figure 3, both TransF and TransE are presented the score distribution on benchmark datasets. It is clear that our TransF model discriminate the golden triples and corrupted triples much better than TransE.\nThat well illustrates the reason why our TransF model outperforms TransE. During the training process, we enforce the L2-norm of the embeddings h,r and t is smaller than 1."}, {"heading": "3.2 Enhancement of TransR: TransRF", "text": "We can improve the TransE related models to TransF related ones, since TransF surpass TransE in the discriminating capability. Taking the TransR as an example, we enhance it to TransRF, a hybrid model which takes the advantages of both TransR and TransF, while keeping the model complexity comparable to TransR. Unlike TransR which takes hr + r \u2248 tr if (h, r, t) is a golden triple, TransRF adopts hr + r \u2248 \u03b1tr.\nAccordingly, we can define the score function as\nfr(h, t) = (hr + r) >tr + h > r (tr \u2212 r) (3)\nwhere hr = hMr and tr = tMr. For each relation r, Mr is the projection matrix which projects entities from the entity space to the relation space. We follow the constraints on the norms of the embeddings of h, r and t and the mapping matrices, \u2200h, r, t, we have \u2016 h \u20162 \u2264 1, \u2016 r \u20162 \u2264 1, \u2016 t \u20162 \u2264 1, \u2016 hMr \u20162 \u2264 1 and \u2016 tMr \u20162 \u2264 1."}, {"heading": "3.3 Training Objective", "text": "All models are trained with contrastive max-margin objective functions. The objective is to ensure that a triple (h, r, t) \u2208 S in the golden set should have a higher score than a triple (h\u2032, r, t\u2032) \u2208 S\u2032 in the corrupted triple set, as follows:\nL = \u2211\n(h,r,t)\u2208S \u2211 (h\u2032,r,t\u2032)\u2208S\u2032 max(0, \u03b3 \u2212 fr(h, t) + fr(h\u2032, t\u2032))\n(4) where \u03b3 > 0 is a margin hyperparameter. S is the training set of golden triples. S\u2032 is the set of corrupted triples. The corrupted triples is generated from the training triples with either the head or tail entity replaced by a random entity (but not both at the same time).\nWe adopt the mini-batched stochastic gradient descent(SGD) to optimize the objective function. The additional constraints on the norms of embedding parameters are explained with the model descriptions. For TransF, all embeddings are randomly initialized with a similar process of [Glorot and Bengio, 2010]. For TransRF, the initialization process is the same as TransR, and the projection matrices are initialized as identity matrix."}, {"heading": "4 Experiments", "text": "In this section, we conduct extensive experiments to justify the proposed models. First, we evaluate our models on link prediction [Bordes et al., 2013] and triple classification [Socher et al., 2013] respectively. Second, to explain\nthe remarkable improvements, we assess how well the models can discriminate the golden triples and corrupted triples with the score function, comparing with TransE.\nThree benchmark datasets are adopted in the experiments: WN18 [Bordes et al., 2013] which is extracted from Wordnet [Miller, 1995]; and two dense subgraphs of Freebase[Bollacker et al., 2008], FB15K [Bordes et al., 2013] and FB13 [Socher et al., 2013]. Table 1 shows the statistics of these data sets."}, {"heading": "4.1 Link Prediction", "text": "As reported in [Bordes et al., 2011; Bordes et al., 2013], link prediction is to predict the missing h or t given (h, r) or (r, t) respectively. In this task, we conduct the evaluation by ranking the set of candidate entities in knowledge graph, instead of offering a best matching entity. This experiment is conducted on two datasets, WN18 and FB15K.\nEvaluation protocol. Following the protocol in TransE [Bordes et al., 2013], for each test triple (h, r, t), we replace the head entity h by every entity in the knowledge graph, and rank these corrupted triples in descending order by the similarity score which is given by fr. Similarly, we repeat this procedure by replacing the tail entity t. After collecting all these triples, we use two evaluation metrics: the mean rank of the correct entities (denotes as Mean Rank); the proportion of correct entity ranks within 10 (denotes as Hits@10). We expect lower Mean Rank and higher Hits@10 for a better predictor. However, some corrupted triples should be considered as correct ones, since they actually exist in knowledge graph. Ranking such triples ahead of the original correct one should not be counted as an error. To eliminate such cases, we filter out those corrupted triples which appear either in the training, validation or test datasets. We term the former evaluation setting as \u201dRaw\u201d and the latter as \u201dFilter\u201d.\nImplementation. By sharing the same data sets, we directly refer to the baselines and experimental results reported in [Lin et al., 2015]. We select learning rate \u03bb for SGD among {0.0001, 0.001, 0.0035, 0.01}, the margin \u03b3 among {0.25, 0.45, 0.6, 0.9}, the embedding dimension k among {20, 50, 100, 200} and batch size B among {120, 480, 960, 1440}. In training TransF, the best configurations are \u03bb = 0.01, \u03b3 = 0.9, k = 50, B = 960 on WN18; \u03bb = 0.0035, \u03b3 = 0.45, k = 100, B = 960 on FB15K. For experiments with TransRF, the optimal configurations are: \u03bb = 0.0001, \u03b3 = 0.25, k = 50, B = 960 on WN18; \u03bb = 0.001, \u03b3 = 0.25, k = 50, B = 1440 on FB15K. For all the datasets, we scan all the training triples for 1000 rounds for training TransF and 500 rounds for training TransRF.\nExperiment Results. Table 2 lists the results on WN18 and FB15K. It demonstrates that TransF, which take relation categories into consideration, outperforms TransE and other baselines on WN18 and FB15K. Furthermore, our TransF model, which is more efficient with less parameters, obtains comparable results to TransH. Comparing the TransR to the TransRF model, TransRF is consistently outperforming TransR on WN18 and FB15K. The reason why our models fail on Mean Rank on WN18 may be due to the small number of relations in the WN18 dataset. For a fair comparison, the reason why we first compare the results of TransE,\nTransH, and our TransF model then analyze the results between TransR and our proposed TransRF. is that TransR builds multiple relation spaces instead of one embedding space.\nWe dig into detailed prediction results with different types of relations including one-to-one/one-to-many/manyto-one/many-to-many relations, since TransF aims to handle these relation categories which TransE cannot. We classify the relations by following the same rules in [Bordes et al., 2013]. Then, we obtain 24% one-to-one relations, 23% one-to-many relations, 29% many-to-one relations and 24% many-to-many relations within 1,345 relations on the FB15K dataset. As shown in Table 3, TransF model consistently outperforms the TransE model. Meanwhile, TransF outperforms TransH in one-to-one/many-to-one/many-to-many relations for predicting tail and one-to-one/one-to-many/manyto-many relations for predicting tail. TransRF also outperforms TransR in dealing with many-to-one/many-to-many relations for predicting head and one-to-many/many-to-many relations for predicting tail . Specifically, TransF and TransRF models bring promising improvements on many-to-many relations, and additionally, the performance on one-to-one is also significantly improved, compared with TransE and TransH. This may be due to the fact that our TransF model also relaxes the geometric assumption of TransE.\nModel Complexity Analysis. To analyze the efficiency of our model, we compare the theoretical number of parameters of the baselines and record the ruining time of TransF, TransRF and other baseline models for each training epoch. As\ndemonstrated in Table 4, our TransF model keeps the same number of parameters as the TransE model, and TransRF also keeps the same number of parameters as TransR. However, TransH needs more nrk parameters than TransF. Furthermore, the training time of TransH is nearly 24 times of that of TransE and TransF. The training time of TransRF is nearly the same as TransR. Therefore, our models are able to improve the knowledge graph embedding without sacrificing the efficiency."}, {"heading": "4.2 Triple Classification", "text": "Following the experiment in [Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015], we also evaluate our model on triple classification. Triple classification is a binary classification task which predict whether a given triple (h, r, t) is correct or not. This task is applied for answering question such as Does Michael Jackson publish the song Beat it?.We use three data\nsets in this task: WN11 and FB13 released in NTN [Socher et al., 2013]; FB15K used in TransR [Lin et al., 2015].\nEvaluation protocol. Following the protocol in NTN [Socher et al., 2013], we set a relation-specific threshold Tr for prediction and then, for a triple (h, r, t), if the similarity score obtained by fr is above Tr, the triple (h, r, t) is predicted as positive, otherwise negative. The relation-specific threshold Tr is determined by maximizing the classification accuracy on a validation set.\nImplementation. We compare our models with the baseline methods reported in [Lin et al., 2015] for WN11, FB13 and FB15K. We enumerate learning rate \u03bb for SGD among {0.0001, 0.001, 0.002, 0.01}, the margin \u03b3 among {0.1, 0.5, 1, 2, 2.25, 2.5}, the embedding dimension k among {20, 50, 100} and the batch size B among {120, 480, 960, 1440}. For training TransF, the optimal configuration are: \u03bb = 0.01, \u03b3 = 2.25, k = 100, B = 960 on WN11; \u03bb = 0.005, \u03b3 = 2, k = 100, B = 960 on FB13; \u03bb = 0.002, \u03b3 = 0.5, k = 100, B = 960 on FB15K. For experiments with TransRF, the best configurations are: \u03bb = 0.0001, \u03b3 = 2.5, k = 50, B = 960 on WN11; \u03bb = 0.001, \u03b3 = 2.5, k = 50, B = 960 on FB13; \u03bb = 0.001, \u03b3 = 0.1, d = 50, B = 120 on FB15K. The number of training epochs is limited to 1, 000 for TransF and 500 for TransRF.\nExperiment Result. Evaluation results are reported in Tabel 5. It demonstrates that our TransF model outperforms all the baseline models significantly including TransE, TransH, and even TransR on WN11 and FB15K. On FB13, TransF beats all baseline models except the NTN model. As described in [Wang et al., 2014; Lin et al., 2015], FB13 is much denser than WN11 and FB15K where strong correlations exist between entities, and NTN can achieve better results by learning complicated correlations using tensor transformation from dense graph of FB13. Comparing TransRF and TransR, our TransRF model has higher accuracy than TransR on all datasets, especially on FB13 and FB15K."}, {"heading": "4.3 Score Distribution", "text": "To explain the remarkable improvements obtained by our model on these tasks, we assess whether our model can better distinguish the positive and negative triples, compared with TransE. For embedding vectors, we directly use the embedding results which are trained for the link prediction task on WN18 and FB15K. Then, two triple sets of positive and neg-\native are built for comparison, where the positive triples are the ones in the test dataset. The negative triple set is constructed in three ways: replacing the head entity randomly from each of the positive triple; replacing the tail entity randomly; and randomly selecting two entities and a relation to assemble a new triple. The triples generated by the first two methods is called semi-negative triples and those by the third method negative ones. While generating negative triples, we guarantee the them do not exist in the knowledge graph.\nAs shown in Figure 3, the TransF model distinguish positive and negative triples remarkably better than TransE. In the TransF model, the boundary of scores for positive and negative triples is clear on both WN18 and FB15K datasets, as depicted in Figure 3(b)(d). In comparison, in the TransE model, we cannot find a clear boundary of scores on both WN18 and FB15K datasets, as can be seen from Figure 3(a)(c). For the TransF model, the score of positive triple is expected to be higher than that of negative triple. For the TransE model, the score of positive triple is expected to be lower than that of negative triple."}, {"heading": "5 Conclusion", "text": "In this paper, we propose knowledge embedding models with flexible translation, which are effective, efficient and portable. The idea of flexible translation is to ensure that the sum vector of a head entity vector and a relation vector has the same direction with a tail entity vector but with flexible magnitude. The proposed models, TransF and TransRF, not only well address some existing issues in previous models when dealing with reflexive/one-to-many/many-to-one/many-to-many relations, but also maintain low complexity and high efficiency. We conduct extensive experiments on benchmark datasets for the tasks of link prediction and triple classification, and results show that our TransF and TransRF models obtain substantial improvements over baselines."}], "references": [{"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Bollacker et al", "2008] Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD international conference on Management", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "et al", "author": ["Antoine Bordes", "Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": "Learning structured embeddings of knowledge bases. In AAAI,", "citeRegEx": "Bordes et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko. Translating embeddings for modeling multi-relational data"], "venue": "pages 2787\u20132795,", "citeRegEx": "Bordes et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Machine Learning", "author": ["Antoine Bordes", "Xavier Glorot", "Jason Weston", "Yoshua Bengio. A semantic matching energy function for learning with multi-relational data"], "venue": "94(2):233\u2013259,", "citeRegEx": "Bordes et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In International Conference on Artificial Intelligence and Statistics", "author": ["Xavier Glorot", "Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks"], "venue": "pages 249\u2013256,", "citeRegEx": "Glorot and Bengio. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["Hoffmann et al", "2011] Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld"], "venue": "In Proceedings of the 49th Annual Meeting of the Association", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Rodolphe Jenatton", "Nicolas L Roux", "Antoine Bordes", "Guillaume R Obozinski. A latent factor model for highly multi-relational data"], "venue": "pages 3167\u2013 3175,", "citeRegEx": "Jenatton et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Xuan Zhu", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu"], "venue": "Learning entity and relation embeddings for knowledge graph completion.", "citeRegEx": "Lin et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean. Distributed representations of words", "phrases", "their compositionality"], "venue": "pages 3111\u20133119,", "citeRegEx": "Mikolov et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM, 38(11):39\u201341,", "citeRegEx": "Miller. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al", "2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Pro-", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "In Proceedings of the 28th international conference on machine learning (ICML-11)", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel. A three-way model for collective learning on multi-relational data"], "venue": "pages 809\u2013816,", "citeRegEx": "Nickel et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Factorizing yago: scalable machine learning for linked data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "Proceedings of the 21st international conference on World Wide Web, pages 271\u2013280. ACM,", "citeRegEx": "Nickel et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "pages 148\u2013163", "author": ["Sebastian Riedel", "Limin Yao", "Andrew McCallum. Modeling relations", "their mentions without labeled text. In Machine Learning", "Knowledge Discovery in Databases"], "venue": "Springer,", "citeRegEx": "Riedel et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng. Reasoning with neural tensor networks for knowledge base completion"], "venue": "pages 926\u2013934,", "citeRegEx": "Socher et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Multiinstance multi-label learning for relation extraction", "author": ["Surdeanu et al", "2012] Mihai Surdeanu", "Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computa-", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "In Advances in neural information processing systems", "author": ["Ilya Sutskever", "Joshua B Tenenbaum", "Ruslan Salakhutdinov. Modelling relational data using bayesian clustered tensor factorization"], "venue": "pages 1821\u20131828,", "citeRegEx": "Sutskever et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "In Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen. Knowledge graph embedding by translating on hyperplanes"], "venue": "pages 1112\u20131119,", "citeRegEx": "Wang et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Connecting language and knowledge bases with embedding models for relation extraction", "author": ["Jason Weston", "Antoine Bordes", "Oksana Yakhnenko", "Nicolas Usunier"], "venue": "arXiv preprint arXiv:1307.7973,", "citeRegEx": "Weston et al.. 2013", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 9, "context": "Knowledge graphs such as Wordnet [Miller, 1995] and Freebase [Bollacker et al.", "startOffset": 33, "endOffset": 47}, {"referenceID": 1, "context": "A variety of approaches have been explored for knowledge graph embedding, such as neural network based [Bordes et al., 2011; Socher et al., 2013], and translation based [Bordes et al.", "startOffset": 103, "endOffset": 145}, {"referenceID": 14, "context": "A variety of approaches have been explored for knowledge graph embedding, such as neural network based [Bordes et al., 2011; Socher et al., 2013], and translation based [Bordes et al.", "startOffset": 103, "endOffset": 145}, {"referenceID": 2, "context": ", 2013], and translation based [Bordes et al., 2013] approaches.", "startOffset": 31, "endOffset": 52}, {"referenceID": 2, "context": "Some notable works, including TransE [Bordes et al., 2013], TransH [Wang et al.", "startOffset": 37, "endOffset": 58}, {"referenceID": 17, "context": ", 2013], TransH [Wang et al., 2014] , and TransR [Lin et al.", "startOffset": 16, "endOffset": 35}, {"referenceID": 7, "context": ", 2014] , and TransR [Lin et al., 2015] are simple, efficient, and effective.", "startOffset": 21, "endOffset": 39}, {"referenceID": 8, "context": "Inspired by [Mikolov et al., 2013], TransE learns vector embedding for both entities and relations.", "startOffset": 12, "endOffset": 34}, {"referenceID": 2, "context": "TransE [Bordes et al., 2013] represents relationships by translation vectors in an embedding space.", "startOffset": 7, "endOffset": 28}, {"referenceID": 17, "context": "TransH [Wang et al., 2014] attempts to alleviate the problems of TransE when dealing with reflexive /one-tomany/many-to-one/many-to-many relations by modeling a relation as a relation-specific hyperplane with wr as the normal vector together with a translation operation dr on it.", "startOffset": 7, "endOffset": 26}, {"referenceID": 7, "context": "TransR [Lin et al., 2015] addresses the issue that some entities are similar in the entity space but comparably different in other specific aspects.", "startOffset": 7, "endOffset": 25}, {"referenceID": 3, "context": "Unstructured Model [Bordes et al., 2014] regards as the naive case of TransE, which sets all translation r = 0, i.", "startOffset": 19, "endOffset": 40}, {"referenceID": 1, "context": "Structured Embedding [Bordes et al., 2011] introduces two independent projections for the entities in a relation.", "startOffset": 21, "endOffset": 42}, {"referenceID": 14, "context": "As pointed out by [Socher et al., 2013],this model is weak to capture the correlations between entities and relations because of the introduction of two separate matrices.", "startOffset": 18, "endOffset": 39}, {"referenceID": 3, "context": "Semantic Matching Energy [Bordes et al., 2014] captures the interactions of relation vectors and entity vectors through multiple matrix Hadamard products.", "startOffset": 25, "endOffset": 46}, {"referenceID": 14, "context": "Neural Tensor Network [Socher et al., 2013] defines an expressive score function as fr(h, t) = ur g(h Mrt+Mr,1h+Mr,2t+br) where ur is a relationspecific linear layer, g() is the tanh operation.", "startOffset": 22, "endOffset": 43}, {"referenceID": 14, "context": "We also treat Single Layer Model [Socher et al., 2013], Latent Factor Model [Jenatton et al.", "startOffset": 33, "endOffset": 54}, {"referenceID": 6, "context": ", 2013], Latent Factor Model [Jenatton et al., 2012] and RESCAL [Nickel et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 11, "context": ", 2012] and RESCAL [Nickel et al., 2011; Nickel et al., 2012] as our baseline in experiments.", "startOffset": 19, "endOffset": 61}, {"referenceID": 12, "context": ", 2012] and RESCAL [Nickel et al., 2011; Nickel et al., 2012] as our baseline in experiments.", "startOffset": 19, "endOffset": 61}, {"referenceID": 4, "context": "For TransF, all embeddings are randomly initialized with a similar process of [Glorot and Bengio, 2010].", "startOffset": 78, "endOffset": 103}, {"referenceID": 2, "context": "First, we evaluate our models on link prediction [Bordes et al., 2013] and triple classification [Socher et al.", "startOffset": 49, "endOffset": 70}, {"referenceID": 14, "context": ", 2013] and triple classification [Socher et al., 2013] respectively.", "startOffset": 34, "endOffset": 55}, {"referenceID": 2, "context": "Three benchmark datasets are adopted in the experiments: WN18 [Bordes et al., 2013] which is extracted from Wordnet [Miller, 1995]; and two dense subgraphs of Freebase[Bollacker et al.", "startOffset": 62, "endOffset": 83}, {"referenceID": 9, "context": ", 2013] which is extracted from Wordnet [Miller, 1995]; and two dense subgraphs of Freebase[Bollacker et al.", "startOffset": 40, "endOffset": 54}, {"referenceID": 2, "context": ", 2008], FB15K [Bordes et al., 2013] and FB13 [Socher et al.", "startOffset": 15, "endOffset": 36}, {"referenceID": 14, "context": ", 2013] and FB13 [Socher et al., 2013].", "startOffset": 17, "endOffset": 38}, {"referenceID": 1, "context": "As reported in [Bordes et al., 2011; Bordes et al., 2013], link prediction is to predict the missing h or t given (h, r) or (r, t) respectively.", "startOffset": 15, "endOffset": 57}, {"referenceID": 2, "context": "As reported in [Bordes et al., 2011; Bordes et al., 2013], link prediction is to predict the missing h or t given (h, r) or (r, t) respectively.", "startOffset": 15, "endOffset": 57}, {"referenceID": 2, "context": "Following the protocol in TransE [Bordes et al., 2013], for each test triple (h, r, t), we replace the head entity h by every entity in the knowledge graph, and rank these corrupted triples in descending order by the similarity score which is given by fr.", "startOffset": 33, "endOffset": 54}, {"referenceID": 7, "context": "By sharing the same data sets, we directly refer to the baselines and experimental results reported in [Lin et al., 2015].", "startOffset": 103, "endOffset": 121}, {"referenceID": 3, "context": "Raw Filter Raw Filter Raw Filter Raw Filter Unstructured [Bordes et al., 2014] 315 304 35.", "startOffset": 57, "endOffset": 78}, {"referenceID": 11, "context": "3 RESCAl [Nickel et al., 2011] 1,180 1,163 37.", "startOffset": 9, "endOffset": 30}, {"referenceID": 1, "context": "1 SE [Bordes et al., 2011] 1,011 985 68.", "startOffset": 5, "endOffset": 26}, {"referenceID": 3, "context": "8 SME(linear) [Bordes et al., 2014] 545 533 65.", "startOffset": 14, "endOffset": 35}, {"referenceID": 3, "context": "8 SME(bilinear) [Bordes et al., 2014] 526 509 54.", "startOffset": 16, "endOffset": 37}, {"referenceID": 6, "context": "3 LFM [Jenatton et al., 2012] 469 456 71.", "startOffset": 6, "endOffset": 29}, {"referenceID": 2, "context": "1 TransE [Bordes et al., 2013] 263 251 75.", "startOffset": 9, "endOffset": 30}, {"referenceID": 17, "context": "1 TransH [Wang et al., 2014] 318 303 75.", "startOffset": 9, "endOffset": 28}, {"referenceID": 7, "context": "TransR [Lin et al., 2015] 232 219 78.", "startOffset": 7, "endOffset": 25}, {"referenceID": 3, "context": "Relation Category 1-to-1 1-to-N N-to-1 N-to-N 1-to-1 1-to-N N-to-1 N-to-N Unstructured [Bordes et al., 2014] 34.", "startOffset": 87, "endOffset": 108}, {"referenceID": 1, "context": "6 SE [Bordes et al., 2011] 35.", "startOffset": 5, "endOffset": 26}, {"referenceID": 3, "context": "3 SME(linear) [Bordes et al., 2014] 35.", "startOffset": 14, "endOffset": 35}, {"referenceID": 3, "context": "3 SME(bilinear) [Bordes et al., 2014] 30.", "startOffset": 16, "endOffset": 37}, {"referenceID": 2, "context": "8 TransE [Bordes et al., 2013] 43.", "startOffset": 9, "endOffset": 30}, {"referenceID": 17, "context": "0 TransH [Wang et al., 2014] 66.", "startOffset": 9, "endOffset": 28}, {"referenceID": 7, "context": "TransR [Lin et al., 2015] 76.", "startOffset": 7, "endOffset": 25}, {"referenceID": 2, "context": "We classify the relations by following the same rules in [Bordes et al., 2013].", "startOffset": 57, "endOffset": 78}, {"referenceID": 2, "context": "Following the experiment in [Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015], we also evaluate our model on triple classification.", "startOffset": 28, "endOffset": 86}, {"referenceID": 17, "context": "Following the experiment in [Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015], we also evaluate our model on triple classification.", "startOffset": 28, "endOffset": 86}, {"referenceID": 7, "context": "Following the experiment in [Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015], we also evaluate our model on triple classification.", "startOffset": 28, "endOffset": 86}, {"referenceID": 1, "context": "DataSets WN11 FB13 FB15K SE [Bordes et al., 2011] 53.", "startOffset": 28, "endOffset": 49}, {"referenceID": 3, "context": "2 SME(bilinear) [Bordes et al., 2014] 70.", "startOffset": 16, "endOffset": 37}, {"referenceID": 14, "context": "7 SLM [Socher et al., 2013] 69.", "startOffset": 6, "endOffset": 27}, {"referenceID": 6, "context": "3 LFM [Jenatton et al., 2012] 73.", "startOffset": 6, "endOffset": 29}, {"referenceID": 14, "context": "3 NTN [Socher et al., 2013] 70.", "startOffset": 6, "endOffset": 27}, {"referenceID": 2, "context": "5 TransE [Bordes et al., 2013] 75.", "startOffset": 9, "endOffset": 30}, {"referenceID": 17, "context": "6 TransH [Wang et al., 2014] 77.", "startOffset": 9, "endOffset": 28}, {"referenceID": 7, "context": "TransR [Lin et al., 2015] 85.", "startOffset": 7, "endOffset": 25}, {"referenceID": 14, "context": "sets in this task: WN11 and FB13 released in NTN [Socher et al., 2013]; FB15K used in TransR [Lin et al.", "startOffset": 49, "endOffset": 70}, {"referenceID": 7, "context": ", 2013]; FB15K used in TransR [Lin et al., 2015].", "startOffset": 30, "endOffset": 48}, {"referenceID": 14, "context": "Following the protocol in NTN [Socher et al., 2013], we set a relation-specific threshold Tr for prediction and then, for a triple (h, r, t), if the similarity score obtained by fr is above Tr, the triple (h, r, t) is predicted as positive, otherwise negative.", "startOffset": 30, "endOffset": 51}, {"referenceID": 7, "context": "We compare our models with the baseline methods reported in [Lin et al., 2015] for WN11, FB13 and FB15K.", "startOffset": 60, "endOffset": 78}, {"referenceID": 17, "context": "As described in [Wang et al., 2014; Lin et al., 2015], FB13 is much denser than WN11 and FB15K where strong correlations exist between entities, and NTN can achieve better results by learning complicated correlations using tensor transformation from dense graph of FB13.", "startOffset": 16, "endOffset": 53}, {"referenceID": 7, "context": "As described in [Wang et al., 2014; Lin et al., 2015], FB13 is much denser than WN11 and FB15K where strong correlations exist between entities, and NTN can achieve better results by learning complicated correlations using tensor transformation from dense graph of FB13.", "startOffset": 16, "endOffset": 53}], "year": 2017, "abstractText": "Knowledge graph embedding refers to projecting entities and relations in knowledge graph into continuous vector spaces. State-of-the-art methods, such as TransE, TransH, and TransR build embeddings by treating relation as translation from head entity to tail entity. However, previous models can not deal with reflexive/one-to-many/manyto-one/many-to-many relations properly, or lack of scalability and efficiency. Thus, we propose a novel method, flexible translation, named TransF, to address the above issues. TransF regards relation as translation between head entity vector and tail entity vector with flexible magnitude. To evaluate the proposed model, we conduct link prediction and triple classification on benchmark datasets. Experimental results show that our method remarkably improve the performance compared with several state-of-the-art baselines.", "creator": "LaTeX with hyperref package"}}}