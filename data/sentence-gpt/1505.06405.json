{"id": "1505.06405", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2015", "title": "Domain Adaptation Extreme Learning Machines for Drift Compensation in E-nose Systems", "abstract": "This paper addresses an important issue, known as sensor drift that behaves a nonlinear dynamic property in electronic nose (E-nose), from the viewpoint of machine learning. Traditional methods for drift compensation are laborious and costly due to the frequent acquisition and labeling process for gases samples recalibration. Extreme learning machines (ELMs) have been confirmed to be efficient and effective learning techniques for pattern recognition and regression. The results demonstrate that E-nose processing is not only effective but is used by many machine learning applications for predictive data acquisition, but also in the field of data acquisition.", "histories": [["v1", "Sun, 24 May 2015 04:34:27 GMT  (855kb)", "http://arxiv.org/abs/1505.06405v1", "11 pages, 9 figures, to appear in IEEE Transactions on Instrumentation and Measurement"]], "COMMENTS": "11 pages, 9 figures, to appear in IEEE Transactions on Instrumentation and Measurement", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lei zhang", "david zhang"], "accepted": false, "id": "1505.06405"}, "pdf": {"name": "1505.06405.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["leizhang@cqu.edu.cn)", "csdzhang@comp.polyu.edu.hk)."], "sections": [{"heading": null, "text": " Abstract\u2014This paper addresses an important issue, known as sensor drift that behaves a nonlinear dynamic property in electronic nose (E-nose), from the viewpoint of machine learning. Traditional methods for drift compensation are laborious and costly due to the frequent acquisition and labeling process for gases samples recalibration. Extreme learning machines (ELMs) have been confirmed to be efficient and effective learning techniques for pattern recognition and regression. However, ELMs primarily focus on the supervised, semi-supervised and unsupervised learning problems in single domain (i.e. source domain). To our best knowledge, ELM with cross-domain learning capability has never been studied. This paper proposes a unified framework, referred to as Domain Adaptation Extreme Learning Machine (DAELM), which learns a robust classifier by leveraging a limited number of labeled data from target domain for drift compensation as well as gases recognition in E-nose systems, without loss of the computational efficiency and learning ability of traditional ELM. In the unified framework, two algorithms called DAELM-S and DAELM-T are proposed for the purpose of this paper, respectively. In order to percept the differences among ELM, DAELM-S and DAELM-T, two remarks are provided. Experiments on the popular sensor drift data with multiple batches collected by E-nose system clearly demonstrate that the proposed DAELM significantly outperforms existing drift compensation methods without cumbersome measures, and also bring new perspectives for ELM.\nIndex Terms\u2014Drift compensation, electronic nose, extreme learning machine, domain adaptation, transfer learning\nI. INTRODUCTION XTREME learning machine (ELM), proposed for solving a single layer feed-forward network (SLFN) by Huang et al [1, 2], has been proven to be effective and efficient algorithms for pattern classification and regression in different fields. ELM can analytically determine the output weights between the hidden layer and output layer using Moore-Penrose generalized inverse by adopting the square loss of prediction error, which then turns into solving a regularized least square problem efficiently in closed form. The hidden layer output is activated by an infinitely differentiable function with randomly\nThis work was supported by National Natural Science Foundation of China (61401048), Hong Kong Scholar Program (XJ2013044) and China Postdoctoral Science Foundation (2014M550457).\nL. Zhang is with College of Computer Science, Chongqing University, Chongqing 400044, China and also with Department of Computing, The Hong Kong Polytechnic University, Hong Kong (e-mail: leizhang@cqu.edu.cn)\nD. Zhang is with Department of Computing, The Hong Kong Polytechnic University, Hong Kong (e-mail: csdzhang@comp.polyu.edu.hk).\nselected input weights and biases of the hidden layer. Huang [3] rigorously proved that the input weights and hidden layer biases can be randomly assigned if the activation function is infinitely differentiable, and also showed that single SLFN with randomly generated additive or RBF nodes with such activation functions can universally approximate any continuous function on any compact subspace of Euclidean space [4].\nIn recent years, ELM has witnessed a number of improved versions in models, algorithms and real-world applications. ELM shows a comparable or even higher prediction accuracy than that of SVMs which solves a quadratic programming problem. In [3], their differences have been discussed. Some specific examples of improved ELMs have been listed as follows. As the output weights are computed with predefined input weights and biases, a set of non-optimal input weights and hidden biases may exist. Additionally, ELM may require more hidden neurons than conventional learning algorithms in some special applications. Therefore, Zhu et al [5] proposed an evolutionary ELM for more compact networks that speed the response of trained networks. In terms of the imbalanced number of classes, a weighted ELM was proposed for binary/multiclass classification tasks with both balanced and imbalanced data distribution [6]. Due to that the solution of ELM is dense which will require longer time for training in large scale applications, Bai et al [7] proposed a sparse ELM for reducing storage space and testing time. Besides, Li et al [8] also proposed a fast sparse approximation of ELM for sparse classifiers training at a rather low complexity without reducing the generalization performance. For all the versions of ELM mentioned above, supervised learning framework was widely explored in application which limits its ability due to the difficulty in obtaining the labeled data. Therefore, Huang et al [9] proposed a semi-supervised ELM for classification, in which a manifold regularization with graph Laplacian was set, and an unsupervised ELM was also explored for clustering.\nIn the past years, the contributions to ELM theories and applications have been made substantially by researchers from various fields. However, with the rising of big data, the data distribution obtained in different stages with different experimental conditions may change, i.e. from different domains. It is also well know that E-nose data collection and data labeling is tedious and labor ineffective, while the classifiers trained by a small number of labeled data are not robust and therefore lead to weak generalization, especially for large-scale application. Though ELM performs better generalization when a number of labeled data from source\nDomain Adaptation Extreme Learning Machines for Drift Compensation in E-nose Systems\nLei Zhang, Member, IEEE and David Zhang, Fellow, IEEE\nE\ndomain is used in learning, the transferring capability of ELM is reduced with a limited number of labeled training instances from target domains. Domain adaptation methods have been proposed for robust classifiers learning by leveraging a few labeled instances from target domains [10-14] in machine learning community and computer vision [15]. It is worth noting that domain adaptation is different from semi-supervised learning which assumes that the labeled and unlabeled data are from the same domain in classifier training.\nIn this paper, we extend ELMs to handle domain adaptation problems for improving the transferring capability of ELM between multiple domains with very few labeled guide instances in target domain, and overcome the generalization disadvantages of ELM in multi-domains application. Specifically, we address the problem of sensor drift compensation in E-nose by using the proposed cross-domain learning framework. Inspired by ELM and knowledge adaptation, a unified domain adaptation ELM framework is proposed for sensor drift compensation. The merits of this paper include:  To the best of our knowledge, there is no report that couples\ndomain adaptation with ELM framework in machine learning community, while this paper provides several new perspectives for exploring ELM theory.  We integrate a new methodology i.e. domain adaptation ELM in E-nose for sensor drift compensation and gas recognition. The propose DAELM is a unified classifier learning framework with knowledge adaptability and well addressed the problem of drift as well as gas recognition.  One method of DAELM, called source domain adaptation ELM (DAELM-S) which learns a classifier by using a number of labeled data from the source domain, and leveraging a limited number of labeled samples from target domain as regularization, is proposed intuitively.  Another method of DAELM, called target domain adaptation ELM (DAELM-T) is also proposed. DAELM-T learns a classifier using a limited number of labeled instances from target domain, while the remaining numerous unlabeled data are also fully exploited by approximating the prediction of a pre-learned base classifier trained in source domain to that of the learned classifier, into which many existing classifiers can be incorporated as the base classifier.  Both DAELM-S and DAELM-T can be formed into a unified ELM framework, in which two steps including random feature mapping and output weights training are referred and our DAELM holds the merits of ELM. In both methods, the final solution can be analytically determined, and the generalization performance is guaranteed in E-nose application.\nThe rest of this paper is organized as follows. In Section II, related work in sensor drift compensation and a brief review of ELM are presented. In Section III, the proposed DAELM framework including two specific algorithms: DAELM-S and DAELM-T is presented. In Section IV, we present the experiments on the popular sensor drift data collected by an E-nose for 3 years, and the results of drift compensation and gas\nrecognition. Finally, Section V concludes the paper."}, {"heading": "II. RELATED WORK", "text": ""}, {"heading": "A. Sensor Drift Compensation in Electronic Nose", "text": "Electronic nose is an intelligent multi-sensor system or artificial olfaction system, which is developed as instrument for gas recognition [18, 19], tea quality assessment [20, 21], medical diagnosis [22], environmental monitor and gas concentration estimation [23, 24], etc. by coupling with pattern recognition and gas sensor array with cross-sensitivity and broad spectrum characteristics. An excellent overview of the E-nose and techniques for processing the sensor responses can be referred to as [32], [33].\nHowever, sensors are often operated over a long period in real-world application and lead to aging that seriously reduces the lifetime of sensors. This is so called sensor drift caused by unknown dynamic process, such as poisoning, aging or environmental variations [34]. Sensor drift has deteriorated the performance of classifiers [25] used for gas recognition of chemosensory systems or E-noses, and plagued the sensory community for many years. Therefore, researchers have to re-train the classifier using a number of new samples in a period regularly for recalibration. However, the tedious work for classifier retraining and acquisition of new labeled samples regularly seems to be impossible for recalibration, due to the complicated gaseous experiments of E-nose and labor cost.\nThe drift problem can be formulated as follows. Suppose , , \u22ef , are gas sensor data sets collected by an E-nose with K batches ranked according to the time intervals, where = , = 1, \u22ef , , denotes a feature vector of the j-th sample in batch i, and Ni is the number of samples in batch i. The sensor drift problem is that the feature distributions of , \u22ef , do not obey the distribution of . As a result, the classifier trained using the labeled data of has degraded performance when tested on , \u22ef , due to the deteriorated generalization ability caused by drift. Generally, the mismatch of distribution between and becomes larger with increasing batch index i (i>1) and aging. From the angle of domain adaptation, in this paper, is called source domain/auxiliary domain (without drift) with labeled data,\n, \u22ef , are referred to as target domain (drifted) in which only a limited number of labeled data is available.\nDrift compensation has been studied for many years. Generally, drift compensation methods can be divided into three categories: component correction methods, adaptive methods, and machine learning methods. Specifically, multivariate component correction, such as CCPCA [35] which attempts to find the drift direction using PCA and remove the drift component is recognized as a popular method in periodic calibration. However, CCPCA assumes that the data from all classes behaves in the same way in the presence of drift that is not always the case. Additionally, evolutionary algorithm which optimizes a multiplicative correction factor for drift compensation [36] was proposed as an adaptive method. However, the generalization performance of the correction factor is limited for on-line use due to the nonlinear dynamic behavior of sensor drift. Classifier ensemble in machine learning was first proposed in [26] for drift compensation,\nwhich has shown improved gas recognition accuracy using the data with long term drift. An overview of the drift compensation is referred to as [25]. Other recent methods to cope with drift can be referred to as [27]-[29].\nThough researchers have paid more attention to sensor drift and aim to find some measures for drift compensation, sensor drift is still a challenging issue in machine olfaction community and sensory field. To our best knowledge, the existing methods are limited in dealing with sensor drift due to their weak generalization to completely new data in presence of drift. Therefore, we aim to enhance the adaptive performance of classifiers to new drifting/drifted data using cross-domain learning with very low complexity. It would be very meaningful and interesting to train a classifier using very few labeled new samples (target domain) without giving up the recognized \u201cuseless\u201d old data (source domain), and realize effective and efficient knowledge transfer (i.e. drift compensation) from source domain to multiple target domains."}, {"heading": "B. Principle of ELM", "text": "Given N samples [ , , \u22ef , ] and their corresponding ground truth [ , , \u22ef , ] , where = [ , , \u22ef , ] \u2208 \u211d and = [ , , \u22ef , ] \u2208 \u211d , n and m denote the number of input and output neurons, respectively. The output of the hidden layer is denoted as \u210b( ) \u2208 \u211d \u00d7 , where L is the number of hidden nodes and \u210b(\u2219) is the activation function (e.g. RBF function, sigmoid function). The output weights between the hidden layer and the output layer being learned is denoted as \u2208 \u211d \u00d7 .\nRegularized ELM aims to solve the output weights by minimizing the squared loss summation of prediction errors and the norm of the output weights for over-fitting control, formulated as follows\nmin \u2112 = \u2016 \u2016 + \u2219 \u2219 \u2211 \u2016 \u2016 . . \u210b( ) = \u2212 , = 1, \u2026 ,\n(1)\nwhere denotes the prediction error w.r.t. to the i-th training pattern, and C is a penalty constant on the training errors.\nBy substituting the constraint term in (1) into the objective function, an equivalent unconstrained optimization problem can be obtained as follows\nmin \u2208\u211d \u00d7 \u2112 = \u2016 \u2016 + \u2219 \u2219 \u2016 \u2212 \u2016 (2)\nwhere = [\u210b( ); \u210b( ); \u2026 ; \u210b( )] \u2208 \u211d \u00d7 and = [ , , \u2026 , ] .\nThe optimization problem (2) is a well known regularized least square problem. The closed form solution of can be easily solved by setting the gradient of the objective function (2) with respect to to zero.\nThere are two cases when solving , i.e. if the number N of training patterns is larger than L, the gradient equation is over-determined, and the closed form solution can be obtained as\n\u2217 = + (3)\nwhere \u00d7 denotes the identity matrix.\nIf the number N of training patterns is smaller than L, an under-determined least square problem would be handled. In this case, the solution of (2) can be obtained as\n\u2217 = + (4)\nwhere \u00d7 denotes the identity matrix. Therefore, in classifier training of ELM, the output weights can be computed by using (3) or (4) depending on the number of training instances and the number of hidden nodes. We refer interested readers to as [1] for more details of ELM theory and the algorithms."}, {"heading": "III. PROPOSED DOMAIN ADAPTATION ELM FRAMEWORK", "text": "In this section, we present formulation of the proposed domain adaptation ELM framework, in which two methods referred to as Source Domain Adaptation ELM (DAELM-S) and Target Domain Adaptation ELM (DAELM-T) are introduced with their learning algorithms, respectively."}, {"heading": "A. Source Domain Adaptation ELM (DAELM-S)", "text": "Suppose that the source domain and target domain are represented by \u201cS\u201d and \u201cT\u201d. In this paper, we assume that all the samples in the source domain are labeled data.\nThe proposed DAELM-S aims to learn a classifier using all labeled instances from the source domain by leveraging a limited number of labeled data from target domain. The DAELM-S can be formulated as\nmin , , \u2016 \u2016 + \u2211 + \u2211 (5)\ns. t. = \u2212 , = 1, \u2026 , = \u2212 , = 1, \u2026 ,\n(6)\nwhere \u2208 \u211d \u00d7 , \u2208 \u211d \u00d7 , \u2208 \u211d \u00d7 denote the output of hidden layer, the prediction error and the label w.r.t. the i-th training instance from the source domain, \u2208 \u211d \u00d7 , \u2208 \u211d \u00d7 , \u2208 \u211d \u00d7 denote the output of hidden layer, the prediction error and the label vector with respect to the j-th guide samples from the target domain, \u2208 \u211d \u00d7 is the output weights being solved, NS and NT denote the number of training instances and guide samples from the source domain and target domain, respectively, CS and CT are the penalty coefficients on the prediction errors of the labeled training data from source domain and target domain, respectively. In this paper, , = 1 if pattern belongs to the j-th class, and -1 otherwise. For example, = [1, \u22121, \u22ef , \u22121] if is belong to class 1.\nFrom (5), we can find that the very few labeled guide samples from target domain can make the learning of \u201ctransferable\u201d and realize the knowledge transfer between source domain and target domain by introducing the third term as regularization coupling with the second constraint in (6), which makes the feature mapping of the guide samples from target domain approximate the labels recognized with the output weights . The structure of the proposed DAELM-S algorithm to learn M classifiers is illustrated in Fig.1.\nequation is formulated as\n, , , , = \u2016 \u2016 + \u2211 +\n\u2211 \u2212 \u2212 + \u2212 \u2212 + (7) where and denote the multiplier vectors. By setting the partial derivation with respect to , , , , as zero, we have\n\u23a9 \u23aa \u23aa \u23aa \u23a8\n\u23aa \u23aa \u23aa \u23a7 = 0 \u2192 = + = 0 \u2192 =\n= 0 \u2192 =\n= 0 \u2192 \u2212 + = 0\n= 0 \u2192 \u2212 + = 0\n(8)\nwhere HS and HT are the output matrix of hidden layer with respect to the labeled data from source domain and target domain, respectively. To analytically determine , the multiplier vectors and should be solved first.\nFor the case that the number of training samples NS is smaller than L (NS<L), then will have more columns than rows and be of full row rank, which leads to a under-determined least square problem, and infinite number of solutions may be obtained. To handle this problem, we substitute the 1st, 2nd, and\n3rd equations into the 4th and 5th equations considering that HHT is invertible, and then there is\n+ + = + + = (9)\nLet = , + = , = , + = , then Eq.(9) can be written as\n+ = + = \u2192\n+ = + = (10)\nThen and can be solved as\n= ( \u2212 ) ( \u2212 ) = \u2212 ( \u2212 ) ( \u2212 ) (11)\nConsider the 1st equation in (8), we obtain the output weights as\n= + = ( \u2212 ) ( \u2212 ) + [ \u2212 ( \u2212 ) ( \u2212 )] (12)\nwhere I is the identity matrix with size of NS. For the case that the number of training samples NS is larger than L (NS>L), has more rows than columns and is of full column rank, which is an over-determined least square problem. Then, we can obtain from the 1st equation in (8) that = ( ) ( \u2212 ), after which is substituted into the 4th and 5th equations, we can calculate the output weights as follows\n+ = + = \u2192\n+ =\n+ =\n\u2192 + ( ) ( \u2212 ) =\n= ( \u2212 )\n\u2192 + + = +\n\u2192 = ( + + ) ( + ) (13)\nwhere I is the identity matrix with size of L. In fact, the optimization (5) can be reformulated as an equivalent unconstrained optimization problem in matrix form by substituting the constraints into the objective function,\nmin ( ) = \u2016 \u2016 + \u2016 \u2212 \u2016 + \u2016 \u2212 \u2016 (14)\nBy setting the gradient of w.r.t. to be zero,\n\u2207 = \u2212 ( \u2212 ) \u2212 ( \u2212 ) = 0 (15)\nThen, we can easily solve (15) to obtain formulated in (13). For recognition of the numerous unlabeled data in target\ndomain, we calculate the output of DAELM-S network as\n= \u2219 , = 1, \u2026 , (16)\n, ,\n, ,\n, ,\nwhere denote the hidden layer output with respect to the k-th unlabeled vector in target domain, and is the number of unlabeled vectors in target domain. The index corresponding to the maximum value in is the class of the k-th sample.\nFor implementation, the DAELM-S algorithm is summarized as Algorithm 1."}, {"heading": "B. Target Domain Adaptation ELM (DAELM-T)", "text": "In the proposed DAELM-S, the classifier is learned on the source domain with the very few labeled guide samples from the target domain as regularization. However, the unlabeled data is neglected which can also improve the performance of classification [17]. Different from DAELM-S, DAELM-T aims to learn a classifier on a very limited number of labeled samples from target domain, by leveraging numerous unlabeled data in target domain, into which a base classifier trained by source data is incorporated. The proposed DAELM-T is formulated as\nmin ( ) = \u2016 \u2016 + \u2016 \u2212 \u2016 + \u2016 \u2212 \u2016 (17)\nwhere denotes the learned classifier, , , are the same as that in DAELM-S, , denote the regularization parameter and the output matrix of the hidden layer with respect to the unlabeled data in target domain. The first term is to against the over-fitting, the second term is the least square loss function, and the third term is the regularization which means the domain adaptation between source domain and target domain. Note that is a base classifier learned with source data. In this paper, regularized ELM is used to train a base classifier by solving\nmin ( ) = \u2016 \u2016 + \u2016 \u2212 \u2016 (18)\nwhere , , denote the same meaning as that in DAELM-S. The structure of the proposed DAELM-T is described in Fig. 2, from which we can see that the unlabeled data in target domain have also been exploited.\nTo solve the optimization (17), by setting the gradient of\nwith respect to to be zero, we then have\n\u2207 =\n\u2212 ( \u2212 ) \u2212 ( \u2212 ) = 0 (19)\nIf NT>L, then we can have from (19) that\n= ( + + ) ( + ) (20)\nwhere I is the identity matrix with size of L. If NT<L, we would like to obtain of the proposed DAELM-T according to the solving manner in DAELM-S. Let = , the model (17) can be re-written as\nmin , , \u2016 \u2016 + \u2211 + \u2211\ns. t. = \u2212 , = 1, \u2026 ,\n= \u2212 , = 1, \u2026 , (21)\nThe Lagrange multiplier equation of (21) can be written as\n, , , , = \u2016 \u2016 + \u2211 +\n\u2211 \u2212 \u2212 + \u2212 \u2212 + (22)\nBy setting the partial derivation with respect to , , , , to be zero, we have\n\u23a9 \u23aa\u23aa \u23aa \u23a8\n\u23aa \u23aa\u23aa \u23a7 = 0 \u2192 = + = 0 \u2192 =\n= 0 \u2192 =\n= 0 \u2192 \u2212 + = 0\n= 0 \u2192 \u2212 + = 0\n(23)\nTo solve , let = , + = ,\n= , and + = , By calculating in the same way as (9), (10), and (11), we get\n= ( \u2212 ) ( \u2212 ) = \u2212 ( \u2212 ) ( \u2212 )\n(24) Therefore, when NT<L, the output weights can be obtained as\n= + = ( \u2212 ) ( \u2212 ) + [ \u2212 ( \u2212 ) ( \u2212 )] (25)\nwhere = , and I is the identity matrix with size of NT. For recognition of the numerous unlabeled data in target domain, we calculate the final output of DAELM-T as\n= \u2219 , = 1, \u2026 , (26)\nwhere denote the hidden layer output with respect to the k-th unlabeled sample vector in target domain, and is the number of unlabeled vectors in target domain.\nFor implementation in experiment, the DAELM-T algorithm is summarized as Algorithm 2.\nAlgorithm 2. DAELM-T\nInput: Training samples { , } = { , } of the source domain S; Labeled guide samples { , } = { , } of the target domain T; Unlabeled samples { } = { } of the target domain T; The tradeoff parameters CS, CT and CTu. Output: The output weights ; The predicted output of unlabeled data in target domain. Procedure: 1. Initialize the ELM network of L hidden neurons with random input weights W1 and hidden bias B1. 2. Calculate the output matrix HS of hidden layer with source domain as\n= \u210b( \u2219 + ). 3. If NS<L, compute the output weights of the base classifier using (4); Else, compute the output weights of the base classifier using (3). 4. Initialize the ELM network of L hidden neurons with random input weights W2 and hidden bias B2. 5. Calculate the hidden layer output matrix and of labeled and unlabeled data in target domains as = \u210b( \u2219 + )and = \u210b( \u2219 + ). 6. If NT<L, compute the output weights using (25); Else, compute the output weights using (20). 7. Calculate the predicted output using (26). Return The output weights and predicted output .\nRemark 1: From the algorithms of DAELM-S and DAELM-T, we observe that the same two stages as ELM are included: (1) feature mapping with randomly selected weights and biases; (2) output weights computation. For ELM, the algorithm is constructed and implemented in a single domain (source domain), as a result, the generalization performance is degraded in new domains. In the proposed DAELM framework, a limited number of labeled samples and numerous unlabeled data in target domain are exploited without changing the unified ELM framework, and the merits of ELM are inherited. The framework for DAELM might draw some new perspectives of domain adaptation for developing ELM theory. Remark 2: We observe that the DAELM-S has similar structure in model and algorithm with DAELM-T. The essential difference lies in that numerous unlabeled data which may be useful for improving generalization performance are exploited in DAELM-T through a pre-learned base classifier. Specifically,\nDAELM-S learns a classifier using the labeled training data in source domain but draw some new knowledge by leveraging a limited number of labeled samples from target domain, such that the knowledge from target domain can be effectively transferred to source domain for generalization. Whilst DAELM-T attempts to train a classifier using a limited number of labeled data from target domain as \u201cmain knowledge\u201d but introduces a regularizer that minimizes the error between outputs of DAELM-T classifier and the base classifier computed on the unlabeled input data."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we will employ the sensor drift compensation experiment on the E-nose olfactory data by using the proposed DAELM-S and DAELM-T algorithms."}, {"heading": "A. Description of Experimental Data", "text": "For verification of the proposed DAELM-S and DAELM-T algorithms, the long-term sensor drift big data of three years that was released in UCI Machine Learning Repository [31] by Vergara et al. [26, 30] is exploited and studied in this paper.\nThe sensor drift big dataset was gathered during the period from January 2008 to February 2011 with 36 months in a gas delivery platform. Totally, this dataset contains 13,910 measurements (observations) from an electronic nose system with 16 gas sensors exposed to 6 kinds of pure gaseous substances including acetone, acetaldehyde, ethanol, ethylene, ammonia, and toluene at different concentration levels, individually. For each sensor, 8 features were extracted, and a 128-dimensional feature vector (8 features \u00d7 16 sensors) for each observation is formulated as a result. We refer readers to as [26] for specific technical details on how to select the 8 features for each sensor. In total, 10 batches of sensor data that collected in different time intervals are included in the dataset. The details of the dataset are presented in Table I.\nFor visualization of the drift behavior existing in the dataset,\nwe first plot the sensor response before and after drifting. We view the data in batch 1 as non-drift, and select batch 2, batch 7, and batch 10 as drifted data, respectively, and the response is given in Fig. 3. It\u2019s known that sensor drift shows nonlinear behavior in a multi-dimensional sensor array, and it is impossible to intuitively and directly calibrate the sensor response using some linear or nonlinear transformation. Instead, we consider it as a space distribution adaptation using transfer learning and realize the drift compensation in decision level. Therefore, to observe the space distribution variation with drift, we apply principal component analysis (PCA) on the dataset, and project the data into a 2D subspace based on the first two PCs. The projected 2D subspace for all data in each batch is shown in Fig. 4, from which we can observe the significant changes of data space distribution caused by drift over time.\nIt\u2019s worth noting that sensor responses after drift cannot be calibrated directly due to the nonlinear dynamic behavior or chaotic behavior [28] of sensor drift. Therefore, drift compensation in decision level by data distribution adaptation and machine learning is more appealing.\nConsidering that a small number of labeled samples (guide samples) should be first selected from the target domains in the proposed DAELM-S and DAELM-T algorithms, while the labeled target data plays an important role in knowledge adaptation, we therefore adopt a representative labeled sample selection algorithm (SSA) based on the Euclidean distance\n( , ) of a sample pair ( , ) . For detail, the SSA algorithm is summarized as Algorithm 3.\nThe visual SSA algorithm in 2-dimensional coordinate plane for selecting 5 guide samples from each target domain (batch) is shown in Fig. 5 as an example. The patterns marked as \u201c1\u201d denote the first two selected patterns (farthest distance) in Step2. Then, the patterns marked as \u201c2\u201d, \u201c3\u201d, \u201c4\u201d denote the three selected patterns sequentially. The SSA is for the purpose that the labeled samples selected from target domains should be representative and global in the data space, and promise the generalization performance of domain adaptation."}, {"heading": "B. Experimental Setup", "text": "We strictly follow the experimental setup in [26] to evaluate our DAELM framework. In default, the number of hidden neurons L is set as 1000, and The RBF function (i.e. radbas) with kernel width set as 1 is used as activation function (i.e. feature mapping function) in the hidden layer. The features are scaled appropriately to lie in interval (-1,1). In DAELM-S algorithm, the penalty coefficients CS and CT are empirically set as 0.01 and 10 throughout the experiments, respectively. In DAELM-T algorithm, the penalty coefficient CS for base classifier is set as 0.001, CT and CTu are set as 0.001 and 100 throughout the experiments, respectively. For effective verification of the proposed methods, two experimental settings according to [16] are given as follows.\n Setting-1: Take batch 1 (source domain) as fixed training set and tested on batch K, K=2,\u2026,10 (target domains);  Setting-2: The training set (source domain) is\ndynamically changed with batch K-1 and tested on batch K (target domain), K=2,\u2026,10. Following the two settings, we realize our proposed DAELM framework and compare with multi-class SVM with RBF kernel (SVM-rbf), the geodesic flow kernel (SVM-gfk), and the combination kernel (SVM-comgfk). Besides, we also compared with the semi-supervised methods such as manifold regularization with RBF kernel (ML-rbf) and manifold regularization with combination kernel (ML-comgfk). The above machine learning based methods have been reported for drift compensation [16] using the same dataset. The formulation of geodesic flow kernel as a domain adaptation method can be referred to as [37]. Additionally, the regularized ELM with RBF function in hidden layer (ELM-rbf) from [29] is also compared as baseline in experiments. The popular CC-PCA method [35] and classifier ensemble [26] for drift compensation are also reported in Setting 1 and Setting 2. Due to the random selection of input weights between input layer and hidden layer, and bias in hidden layer under ELM framework, in experiments, we run the ELM, DAELM-S and DAELM-T for 10 times, and the average values are reported. Note that ELM is trained using the same labeled source data and target data as the proposed DAELM."}, {"heading": "C. Results and Comparisons", "text": "We conduct the experiments and discussion on Setting 1 and Setting 2, respectively. The recognition results of 9 batches for different methods under experimental setting 1 are reported in Table II. We consider two conditions of DAELM-S with 20 labeled target samples and 30 labeled target samples, respectively. For DAELM-T, 40 and 50 labeled samples from the target domain are used, respectively, considering that DAELM-T trains a classifier only using a limited number of\nlabeled samples from target domain. For visually observing the performance of all methods, we show the recognition accuracy on batches successively as Fig.6. From Table II and Fig.6, we have the following observations: 1. SVM with the combined kernel of geodesic flow kernels\n(SVM-comgfk) performs better results than the popular CC-PCA method and other SVM based methods in most batches, except the results of batch 4 and batch 8. It demonstrates that machine learning methods show more usefulness in drift compensation than traditional calibration. 2. Manifold learning with combined kernel (ML-comgfk) obtains an average accuracy of 67.3% and outperforms all baseline methods. It demonstrates that manifold regularization and combined kernel are more effective in semi-supervised learning with a limited number of samples. 3. The generalization performance and knowledge transfer capability of regularized ELM have been well improved by the proposed DAELM. The results of our DAELM-S and DAELM-T have an average improvement of about 30% in recognition accuracy than traditional ELM. The highest recognition accuracy of 91.86% under sensor drift is obtained using our proposed algorithm.\n4. Both the proposed DAELM-S and DAELM-T significantly outperform all other existing methods including traditional CC-PCAM, SVM and manifold regularization based machine learning methods. In addition, DAELM-T(50) has an obvious improvement than DAELM-T(40) and DAELM-S, which shows that more labeled target data is expected for DAELM-T. While DAELM-S can also perform well comparatively with fewer labeled target data. From the computations, due to that a base classifier is first trained in DAELM-T and more labeled target data need, DAELM-S maybe a better choice in realistic applications.\nFrom the experimental results in experimental Setting 1, the proposed methods outperform all other methods in drift compensation. We then follow the experimental Setting 2, i.e. trained on batch K-1 and tested on batch K, and report the results in Table III. The performance variations of all methods are illustrated in Fig. 7. From Table III and Fig. 7, we have the following observations: 1. Manifold regularization based combined kernel\n(ML-comgfk) achieves an average accuracy of 79.6% and outperforms other SVM based machine learning algorithms and single kernel methods, which demonstrates that manifold learning and combined kernel can improve the classification accuracy, but limited capacity. 2. The classifier ensemble can improve the performance of the dataset with drift noise (an average accuracy of 80.0%). However, many base classifiers should be trained using the source data for ensemble, and it has no domain adaptability when tested on the data from target domains, which has been well referred in the proposed DAELM. 3. The proposed DAELM methods perform much better (91.82%) than all other existing methods for different tasks in recognition tested on drifted data. The robustness of the proposed methods with domain adaptability is proved for drift compensation in E-nose.\nFor studying the variations of recognition accuracy with the number k of labeled samples in target domain, different number k from the set of {5, 10, 15, 20, 25, 30, 35, 40, 45, 50} is explored by Algorithm 3 (SSA) and the proposed DAELM framework. Specifically, we present comparisons with different number of labeled samples selected from target domains. For fair comparison with ELM, the labeled target samples are feed into ELM together with the source training samples. The results for experimental Setting 1 and Setting 2 are shown in Fig. 8 and Fig. 9, respectively, from which, we have: 1. The traditional ELM has little obvious improvement with\nthe increase of the labeled samples from target domains, which clearly demonstrates that ELM has no the capability of knowledge adaptation. 2. Both DAELM-S and DAELM-T have significant enhancement in classification accuracy with increasing labeled data from target domain. Note that in batch 2 and batch 10 shown in Fig. 8, our DAELM is comparative to ELM. The possible reason may be that little drift exist in batch 2 that leads to the small difference in classification task. While the data in batch 10 may be seriously noised by drift, the E-nose system may lose recognition ability only using batch 1 (Setting 1) for training. The proposed DAELM is still much better than ELM when tested on the seriously noised batch 10 in Setting 2 (Fig. 9).\n3. DAELM-S has superior performance to DAELM-T when the number k of labeled target samples used in knowledge adaptation is smaller, because DAELM-T does not consider the source data in classifier learning. Additionally, with the increase of the number k, DAELM-T has a comparative performance with DAELM-S which maybe a better choice when only a small number of labeled samples in target domain are available.\nThroughout the paper, the proposed DAELM framework is to cope with sensor drift in the perspective of machine learning in decision level, but not intuitively calibrate the single sensor response because the drift rules are difficult to be captured by some linear or nonlinear regression method due to its nonlinear/chaotic dynamic behavior. This work is to construct a learning framework with better knowledge adaptability and generalization capability to drift noise existing in dataset."}, {"heading": "V. CONCLUSION AND FUTURE WORK", "text": "In this paper, the sensor drift problem in electronic nose is addressed by a new knowledge adaptation based machine learning approach. We have proposed a new framework, referred to as Domain Adaptation Extreme Learning Machine (DAELM), for fast knowledge transfer. Specifically, two algorithms, called DAELM-S and DAELM-T are proposed for drift compensation. The former learns a robust classifier based on the source domain by leveraging a limited number of labeled samples from target domain. The latter learns a classifier based on a limited number of labeled data in target domain by leveraging a pre-learned base classifier in source domain. From the angle of machine learning, the proposed methods provide new perspectives for exploring ELM theory, and also inherit the advantages of ELM including the feature mapping with\nrandomly generated input weights and bias, the analytically determined solutions, and good generalization. Another important contribution, the key of this paper, is an effective measure using domain adaptation and ELM framework to cope with sensor drift in E-nose. Experiment on a long term sensor drift data set collected by E-nose clearly demonstrates the efficacy of our proposed framework. Additionally, the proposed framework can realize the recognition directly from the output (16) or (26) of algorithm without any cumbersome measure, which is completely different from SVM based methods that multi-class problem should be divided into multiple binary classification using \u201cone against one\u201d or \u201cone against all\u201d strategy and obtain the predicted label by voting mechanism. It is worth noting that the training time and testing time of proposed algorithms costs about several seconds and microseconds, respectively, due to the analytically determined solutions intuitively without iterations in learning process. In the future, we will investigate on-line domain adaptation for drift compensation in E-nose from the viewpoint of incremental learning. It would be of interest to explore the nonlinear dynamic behavior of drift by constructing on-line dynamic classifiers with knowledge adaptation.\nACKNOWLEDGMENT The authors would like to thank Dr. Alexander Vergara in University of California San Diego for his provided sensor drift data in electronic nose. The authors would also like to express their sincere appreciation to the anonymous reviewers for their\ninsightful comments."}], "references": [{"title": "Extreme learning machine: Theory and applications", "author": ["G.B. Huang", "Q.Y. Zhu", "C.K. Siew"], "venue": "Neurocomputing, vol. 70, pp. 489-501, 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Extreme Learning Machine for Regression and Multiclass Classification", "author": ["G.B. Huang", "H. Zhou", "X. Ding", "R. Zhang"], "venue": "IEEE Trans. Systems, Man, Cybernetics: Part B, vol. 42, no. 2, pp.513-529, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Universal approximation using incremental constructive feedforward networks with random hidden nodes", "author": ["G.B. Huang", "L. Chen", "C.K. Siew"], "venue": "IEEE Trans. Neural. Netw, vol. 17, no. 4, pp. 879-892, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Evolutionary extreme learning machine", "author": ["Q.Y. Zhu", "A.K. Qin", "P.N. Suganthan", "G.B. Huang"], "venue": "Pattern Recognition, vol. 38, pp. 1759-1763, 2005.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Weighted extreme learning machine for imbalance learning", "author": ["W. Zong", "G.B. Huang", "Y. Chen"], "venue": "Neurocomputing, vol. 101, pp. 229-242, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse Extreme Learning Machine for Classification", "author": ["Z. Bai", "G.B. Huang", "D. Wang", "H. Wang", "M. Brandon Westover"], "venue": "IEEE Trans. Cybernetics, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast sparse approximation of extreme learning machine", "author": ["X. Li", "W. Mao", "W. Jiang"], "venue": "Neurocomputing, vol. 128, pp. 96-103, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-Supervised and Unsupervised Extreme Learning Machines", "author": ["G. Huang", "S. Song", "J.N.D. Gupta", "C. Wu"], "venue": "IEEE Trans. Cybernetics, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Domain adaptation with structural correspondence learning", "author": ["J. Blitzer", "R. McDonald", "F. Pereira"], "venue": "Proc. Conf. Emp. Methods Natural Lang. Process., Jul. 2006, pp. 120-128.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Cross-domain video concept detection using adaptive SVMs", "author": ["J. Yang", "R. Yan", "A.G. Hauptmann"], "venue": "Proc. Int. Conf. Multimedia, Sep. 2007, pp. 188-197.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Domain adaptation via transfer component analysis", "author": ["S.J. Pan", "I.W. Tsang", "J.T. Kwok", "Q. Yang"], "venue": "IEEE Trans. Neural Netw., vol. 22, no. 2, pp. 199-210, Feb. 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Domain adaptation from multiple sources via auxiliary classifiers", "author": ["L. Duan", "I.W. Tsang", "D. Xu", "T.S. Chua"], "venue": "Proc. Int. Conf. Mach. Learn., Jun. 2009, pp. 289-296.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Domain Adaptation from Multiple Sources: Domain-Dependent Regularization Approach", "author": ["L. Duan", "D. Xu", "I.W. Tsang"], "venue": "IEEE Trans. Neur. Netw. Learn. Syst., vol. 23, no. 3, pp. 504-518, Mar. 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Domain adaptation for object recognition: An unsupervised approach", "author": ["R. Gopalan", "R. Li", "R. Chellappa"], "venue": "Proc. ICCV, 2011, pp. 999-1006.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Drift Compensation for Electronic Nose by Semi-Supervised Domain Adaptation", "author": ["Q. Liu", "X. Li", "M. Ye", "S. Sam Ge", "X. Du"], "venue": "IEEE Sens. J., vol. 14, no. 3, pp. 657-665, Mar. 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "J. Mach. Learn. Res., vol. 7, pp. 2399-2434, Nov. 2006.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "A new kernel discriminant analysis framework for electronic nose recognition", "author": ["L. Zhang", "F.C. Tian"], "venue": "Analytica Chimica Acta, vol. 816, pp. 8-17, Mar. 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Classification of multiple indoor air contaminants by an electronic nose and a hybrid support vector machine", "author": ["L. Zhang", "F. Tian", "H. Nie", "L. Dang", "G. Li", "Q. Ye", "C. Kadri"], "venue": "Sensors Actuat. B, Chem., vol. 174, pp. 114-125, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Recognition of coffee using differential electronic nose", "author": ["K. Brudzewski", "S. Osowski", "A. Dwulit"], "venue": "IEEE Trans. Instrum. Meas., vol. 61, no. 6,  pp. 1803-1810, June 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1803}, {"title": "Towards Versatile Electronic Nose Pattern Classifier for Black Tea Quality Evaluation: An Incremental Fuzzy Approach", "author": ["B. Tudu", "A. Metla", "B. Das", "N. Bhattacharyya", "A. Jana", "D. Ghosh", "R. Bandyopadhyay"], "venue": "IEEE Trans. Instrum. Meas, vol. 58, no. 9, pp. 3069-3078, Sep. 2009.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "An electronic nose system to diagnose illness", "author": ["J.W. Garnder", "H.W. Shin", "E.L. Hines"], "venue": "Sensors Actuat. B, Chem., vol. 70, pp. 19-24, 2000.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2000}, {"title": "Gases concentration estimation using heuristics and bio-inspired optimization models for experimental chemical electronic nose", "author": ["L. Zhang", "F. Tian", "C. Kadri", "G. Pei", "H. Li", "L. Pan"], "venue": "Sensors Actuat. B, Chem., vol. 160, no. 1, pp. 760-770, Dec. 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Performance Study of Multilayer Perceptrons in a Low-Cost Electronic", "author": ["L. Zhang", "F. Tian"], "venue": "IEEE Trans. Instrum. Meas., vol. 63, no. 7, pp. 1670-1679, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Drift Correction Methods for Gas Chemical Sensors in Artificial Olfaction Systems: Techniques and Challenges", "author": ["S. Di Carlo", "M. Falasconi"], "venue": "Advances in Chemical Sensors, pp. 305-326, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Chemical gas sensor drift compensation using classifier ensembles", "author": ["A. Vergara", "S. Vembu", "T. Ayhan", "M.A. Ryan", "M.L. Homer", "R. Huerta"], "venue": "Sensors Actuat. B, Chem., vol. 166-167, pp. 320-329, May 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Long term stability of metal oxide-based gas sensors for e-nose environmental applications: An overview", "author": ["A.C. Romain", "J. Nicolas"], "venue": "Sensors Actuat. B, Chem., vol. 146, pp. 502-506, 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Chaotic time series prediction of E-nose sensor drift in embedded phase space", "author": ["L. Zhang", "F. Tian", "S. Liu", "L. Dang", "X. Peng", "X. Yin"], "venue": "Sensors Actuat. B, Chem., vol. 182, pp. 71-79, June 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "ELM-Based Ensemble Classifier for Gas Sensor Array Drift Dataset, Computational Intelligence, Cyber Security and Computational Models", "author": ["D. Arul Pon Daniel", "K. Thangavel", "R. Manavalan", "R. Subash Chandra Boss"], "venue": "Advances in Intelligent Systems and Computing, vol. 246, pp. 89-96, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "On the calibration of sensor arrays for pattern recognition using the minimal number of experiments", "author": ["I.R. Lujan", "J. Fonollosa", "A. Vergara", "M. Homer", "R. Huerta"], "venue": "Chemometrics Intell. Lab. Syst., vol. 130, pp. 123-134, Jan. 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Electronic Noses: Principles and Applications", "author": ["J.W. Gardner", "P.N. Bartlett"], "venue": "Oxford University Press, Oxford, 1999.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1999}, {"title": "Pattern analysis for machine olfaction: a review", "author": ["R. Gutierrez-Osuna"], "venue": "IEEE Sens. J., vol. 2, no. 3, pp. 189-202, 2002.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2002}, {"title": "Drift counteraction in odour recognition applications: lifelong calibration method", "author": ["M. Holmberg", "F.A.M. Davide", "C. Di Natale", "A.D. Amico", "F. Winquist", "I. Lundstr\u00f6m"], "venue": "Sensors Actuat. B, Chem., vol. 42, pp. 185-194, 1997.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1997}, {"title": "Drift correction for gas sensors using multivariate methods", "author": ["T. Artursson", "T. Eklov", "I. Lundstrom", "P. Martensson", "M. Sjostrom", "M. Holmberg"], "venue": "J. Chemometrics, vol. 14, no. 5-6, pp. 711-723, Dec. 2000.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2000}, {"title": "Increasing pattern recognition accuracy for chemical sensing by evolutionary based drift compensation", "author": ["S. Di Carlo", "M. Falasconi", "E. Sanchez", "A. Scionti", "G. Squillero", "A. Tonda"], "venue": "Pattern Recognition Letters, vol, 32, no. 13, pp. 1594-1603, 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Geodesic flow kernel for unsupervised domain adaptation", "author": ["B. Gong", "Y. Shi", "F. Sha", "K. Grauman"], "venue": "Proc. CVPR, pp. 2066-2073, 2012.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "XTREME learning machine (ELM), proposed for solving a single layer feed-forward network (SLFN) by Huang et al [1, 2], has been proven to be effective and efficient algorithms for pattern classification and regression in different fields.", "startOffset": 110, "endOffset": 116}, {"referenceID": 1, "context": "Huang [3] rigorously proved that the input weights and hidden layer biases can be randomly assigned if the activation function is infinitely differentiable, and also showed that single SLFN with randomly generated additive or RBF nodes with such activation functions can universally approximate any continuous function on any compact subspace of Euclidean space [4].", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": "Huang [3] rigorously proved that the input weights and hidden layer biases can be randomly assigned if the activation function is infinitely differentiable, and also showed that single SLFN with randomly generated additive or RBF nodes with such activation functions can universally approximate any continuous function on any compact subspace of Euclidean space [4].", "startOffset": 362, "endOffset": 365}, {"referenceID": 1, "context": "In [3], their differences have been discussed.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "Therefore, Zhu et al [5] proposed an evolutionary ELM for more compact networks that speed the response of trained networks.", "startOffset": 21, "endOffset": 24}, {"referenceID": 4, "context": "In terms of the imbalanced number of classes, a weighted ELM was proposed for binary/multiclass classification tasks with both balanced and imbalanced data distribution [6].", "startOffset": 169, "endOffset": 172}, {"referenceID": 5, "context": "Due to that the solution of ELM is dense which will require longer time for training in large scale applications, Bai et al [7] proposed a sparse ELM for reducing storage space and testing time.", "startOffset": 124, "endOffset": 127}, {"referenceID": 6, "context": "Besides, Li et al [8] also proposed a fast sparse approximation of ELM for sparse classifiers training at a rather low complexity without reducing the generalization performance.", "startOffset": 18, "endOffset": 21}, {"referenceID": 7, "context": "Therefore, Huang et al [9] proposed a semi-supervised ELM for classification, in which a manifold regularization with graph Laplacian was set, and an unsupervised ELM was also explored for clustering.", "startOffset": 23, "endOffset": 26}, {"referenceID": 8, "context": "Domain adaptation methods have been proposed for robust classifiers learning by leveraging a few labeled instances from target domains [10-14] in machine learning community and computer vision [15].", "startOffset": 135, "endOffset": 142}, {"referenceID": 9, "context": "Domain adaptation methods have been proposed for robust classifiers learning by leveraging a few labeled instances from target domains [10-14] in machine learning community and computer vision [15].", "startOffset": 135, "endOffset": 142}, {"referenceID": 10, "context": "Domain adaptation methods have been proposed for robust classifiers learning by leveraging a few labeled instances from target domains [10-14] in machine learning community and computer vision [15].", "startOffset": 135, "endOffset": 142}, {"referenceID": 11, "context": "Domain adaptation methods have been proposed for robust classifiers learning by leveraging a few labeled instances from target domains [10-14] in machine learning community and computer vision [15].", "startOffset": 135, "endOffset": 142}, {"referenceID": 12, "context": "Domain adaptation methods have been proposed for robust classifiers learning by leveraging a few labeled instances from target domains [10-14] in machine learning community and computer vision [15].", "startOffset": 135, "endOffset": 142}, {"referenceID": 13, "context": "Domain adaptation methods have been proposed for robust classifiers learning by leveraging a few labeled instances from target domains [10-14] in machine learning community and computer vision [15].", "startOffset": 193, "endOffset": 197}, {"referenceID": 16, "context": "Sensor Drift Compensation in Electronic Nose Electronic nose is an intelligent multi-sensor system or artificial olfaction system, which is developed as instrument for gas recognition [18, 19], tea quality assessment [20, 21], medical diagnosis [22], environmental monitor and gas concentration estimation [23, 24], etc.", "startOffset": 184, "endOffset": 192}, {"referenceID": 17, "context": "Sensor Drift Compensation in Electronic Nose Electronic nose is an intelligent multi-sensor system or artificial olfaction system, which is developed as instrument for gas recognition [18, 19], tea quality assessment [20, 21], medical diagnosis [22], environmental monitor and gas concentration estimation [23, 24], etc.", "startOffset": 184, "endOffset": 192}, {"referenceID": 18, "context": "Sensor Drift Compensation in Electronic Nose Electronic nose is an intelligent multi-sensor system or artificial olfaction system, which is developed as instrument for gas recognition [18, 19], tea quality assessment [20, 21], medical diagnosis [22], environmental monitor and gas concentration estimation [23, 24], etc.", "startOffset": 217, "endOffset": 225}, {"referenceID": 19, "context": "Sensor Drift Compensation in Electronic Nose Electronic nose is an intelligent multi-sensor system or artificial olfaction system, which is developed as instrument for gas recognition [18, 19], tea quality assessment [20, 21], medical diagnosis [22], environmental monitor and gas concentration estimation [23, 24], etc.", "startOffset": 217, "endOffset": 225}, {"referenceID": 20, "context": "Sensor Drift Compensation in Electronic Nose Electronic nose is an intelligent multi-sensor system or artificial olfaction system, which is developed as instrument for gas recognition [18, 19], tea quality assessment [20, 21], medical diagnosis [22], environmental monitor and gas concentration estimation [23, 24], etc.", "startOffset": 245, "endOffset": 249}, {"referenceID": 21, "context": "Sensor Drift Compensation in Electronic Nose Electronic nose is an intelligent multi-sensor system or artificial olfaction system, which is developed as instrument for gas recognition [18, 19], tea quality assessment [20, 21], medical diagnosis [22], environmental monitor and gas concentration estimation [23, 24], etc.", "startOffset": 306, "endOffset": 314}, {"referenceID": 22, "context": "Sensor Drift Compensation in Electronic Nose Electronic nose is an intelligent multi-sensor system or artificial olfaction system, which is developed as instrument for gas recognition [18, 19], tea quality assessment [20, 21], medical diagnosis [22], environmental monitor and gas concentration estimation [23, 24], etc.", "startOffset": 306, "endOffset": 314}, {"referenceID": 29, "context": "An excellent overview of the E-nose and techniques for processing the sensor responses can be referred to as [32], [33].", "startOffset": 109, "endOffset": 113}, {"referenceID": 30, "context": "An excellent overview of the E-nose and techniques for processing the sensor responses can be referred to as [32], [33].", "startOffset": 115, "endOffset": 119}, {"referenceID": 31, "context": "This is so called sensor drift caused by unknown dynamic process, such as poisoning, aging or environmental variations [34].", "startOffset": 119, "endOffset": 123}, {"referenceID": 23, "context": "Sensor drift has deteriorated the performance of classifiers [25] used for gas recognition of chemosensory systems or E-noses, and plagued the sensory community for many years.", "startOffset": 61, "endOffset": 65}, {"referenceID": 32, "context": "Specifically, multivariate component correction, such as CCPCA [35] which attempts to find the drift direction using PCA and remove the drift component is recognized as a popular method in periodic calibration.", "startOffset": 63, "endOffset": 67}, {"referenceID": 33, "context": "Additionally, evolutionary algorithm which optimizes a multiplicative correction factor for drift compensation [36] was proposed as an adaptive method.", "startOffset": 111, "endOffset": 115}, {"referenceID": 24, "context": "Classifier ensemble in machine learning was first proposed in [26] for drift compensation,", "startOffset": 62, "endOffset": 66}, {"referenceID": 23, "context": "An overview of the drift compensation is referred to as [25].", "startOffset": 56, "endOffset": 60}, {"referenceID": 25, "context": "Other recent methods to cope with drift can be referred to as [27]-[29].", "startOffset": 62, "endOffset": 66}, {"referenceID": 27, "context": "Other recent methods to cope with drift can be referred to as [27]-[29].", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "We refer interested readers to as [1] for more details of ELM theory and the algorithms.", "startOffset": 34, "endOffset": 37}, {"referenceID": 15, "context": "However, the unlabeled data is neglected which can also improve the performance of classification [17].", "startOffset": 98, "endOffset": 102}, {"referenceID": 24, "context": "[26, 30] is exploited and studied in this paper.", "startOffset": 0, "endOffset": 8}, {"referenceID": 28, "context": "[26, 30] is exploited and studied in this paper.", "startOffset": 0, "endOffset": 8}, {"referenceID": 24, "context": "We refer readers to as [26] for specific technical details on how to select the 8 features for each sensor.", "startOffset": 23, "endOffset": 27}, {"referenceID": 26, "context": "It\u2019s worth noting that sensor responses after drift cannot be calibrated directly due to the nonlinear dynamic behavior or chaotic behavior [28] of sensor drift.", "startOffset": 140, "endOffset": 144}, {"referenceID": 24, "context": "Experimental Setup We strictly follow the experimental setup in [26] to evaluate our DAELM framework.", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "For effective verification of the proposed methods, two experimental settings according to [16] are given as follows.", "startOffset": 91, "endOffset": 95}, {"referenceID": 14, "context": "The above machine learning based methods have been reported for drift compensation [16] using the same dataset.", "startOffset": 83, "endOffset": 87}, {"referenceID": 34, "context": "The formulation of geodesic flow kernel as a domain adaptation method can be referred to as [37].", "startOffset": 92, "endOffset": 96}, {"referenceID": 27, "context": "Additionally, the regularized ELM with RBF function in hidden layer (ELM-rbf) from [29] is also compared as baseline in experiments.", "startOffset": 83, "endOffset": 87}, {"referenceID": 32, "context": "The popular CC-PCA method [35] and classifier ensemble [26] for drift compensation are also reported in Setting 1 and Setting 2.", "startOffset": 26, "endOffset": 30}, {"referenceID": 24, "context": "The popular CC-PCA method [35] and classifier ensemble [26] for drift compensation are also reported in Setting 1 and Setting 2.", "startOffset": 55, "endOffset": 59}], "year": 2015, "abstractText": "\uf020 Abstract\u2014This paper addresses an important issue, known as sensor drift that behaves a nonlinear dynamic property in electronic nose (E-nose), from the viewpoint of machine learning. Traditional methods for drift compensation are laborious and costly due to the frequent acquisition and labeling process for gases samples recalibration. Extreme learning machines (ELMs) have been confirmed to be efficient and effective learning techniques for pattern recognition and regression. However, ELMs primarily focus on the supervised, semi-supervised and unsupervised learning problems in single domain (i.e. source domain). To our best knowledge, ELM with cross-domain learning capability has never been studied. This paper proposes a unified framework, referred to as Domain Adaptation Extreme Learning Machine (DAELM), which learns a robust classifier by leveraging a limited number of labeled data from target domain for drift compensation as well as gases recognition in E-nose systems, without loss of the computational efficiency and learning ability of traditional ELM. In the unified framework, two algorithms called DAELM-S and DAELM-T are proposed for the purpose of this paper, respectively. In order to percept the differences among ELM, DAELM-S and DAELM-T, two remarks are provided. Experiments on the popular sensor drift data with multiple batches collected by E-nose system clearly demonstrate that the proposed DAELM significantly outperforms existing drift compensation methods without cumbersome measures, and also bring new perspectives for ELM.", "creator": null}}}