{"id": "1505.07570", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2015", "title": "A Practical Guide to Randomized Matrix Computations with MATLAB Implementations", "abstract": "Matrix operations such as matrix inversion, eigenvalue decomposition, singular value decomposition are ubiquitous in real-world applications. Unfortunately, many of these matrix operations so time and memory expensive that they are prohibitive when the scale of data is large. In real-world applications, since the data themselves are noisy, machine-precision matrix operations are not necessary at all, and one can sacrifice a reasonable amount of accuracy for computational efficiency.\n\n\n\n\nThe following are the steps for analyzing the complex algorithms described below.\nThe first step is to apply the optimization matrix. A matrix operations are a simple matrix operation. If the matrix operations are complex, it must be simple and easy to understand, so they cannot be applied to applications of the same dimension. In the previous steps, this should be very easy to use:\nLet\u2019s first apply the optimization matrix to a very simple matrix operation:\nLet\u2019s evaluate the matrix operation for two different data sets:\nlet\u2019s take a matrix operation and see the matrix operation:\nLet\u2019s see the matrix operation:\nThe matrix operation is just one simple matrix operation. Let\u2019s look at the matrix operation:\nLet\u2019s look at the matrix operation:\nLet\u2019s see the matrix operation:\nLet\u2019s look at the matrix operation:\nThe matrix operation can be done with one or more operations:\nLet\u2019s look at the matrix operation:\nHere is a matrix operation that can be done with any given operation:\nLet\u2019s look at the matrix operation:\nLet\u2019s look at the matrix operation:\nHere is a matrix operation that can be done with any given operation:\nLet\u2019s look at the matrix operation:\nLet\u2019s look at the matrix operation:\nLet\u2019s look at the matrix operation:\nLet\u2019s look at the matrix operation:\nLet\u2019s look at the matrix operation:\nLet\u2019s look at the matrix operation:\nLet\u2019s look at the matrix operation:\nLet\u2019s look at the matrix operation:\nLet\u2019s look at the matrix operation:\nLet\u2019s look at the matrix operation:\nLet\u2019s look at the matrix operation:\nLet\u2019s look at the matrix operation:\nLet\u2019s look at the matrix operation:\nLet\u2019s look at the matrix operation:\nHere are the", "histories": [["v1", "Thu, 28 May 2015 07:33:21 GMT  (393kb,D)", "https://arxiv.org/abs/1505.07570v1", null], ["v2", "Wed, 3 Jun 2015 14:33:48 GMT  (394kb,D)", "http://arxiv.org/abs/1505.07570v2", null], ["v3", "Sun, 7 Jun 2015 07:52:43 GMT  (399kb,D)", "http://arxiv.org/abs/1505.07570v3", null], ["v4", "Wed, 29 Jul 2015 09:26:48 GMT  (399kb,D)", "http://arxiv.org/abs/1505.07570v4", null], ["v5", "Mon, 17 Aug 2015 06:39:40 GMT  (399kb,D)", "http://arxiv.org/abs/1505.07570v5", null], ["v6", "Tue, 3 Nov 2015 03:26:49 GMT  (832kb,D)", "http://arxiv.org/abs/1505.07570v6", null]], "reviews": [], "SUBJECTS": "cs.MS cs.LG", "authors": ["shusen wang"], "accepted": false, "id": "1505.07570"}, "pdf": {"name": "1505.07570.pdf", "metadata": {"source": "CRF", "title": "A Practical Guide to Randomized Matrix Computations with MATLAB Implementations", "authors": ["Shusen Wang"], "emails": ["wssatzju@gmail.com"], "sections": [{"heading": null, "text": "A Practical Guide to Randomized Matrix Computations with MATLAB\nImplementations1\nShusen Wang wssatzju@gmail.com\nNovember 4, 2015\n1Sample MATLAB code with demos is available at https://github.com/wangshusen/RandMatrixMatlab.\nar X\niv :1\n50 5.\n07 57\n0v 6\n[ cs\n.M S]\n3 N\nov 2\n01 5\n2\nContents\nAbstract 1"}, {"heading": "1 Introduction 3", "text": ""}, {"heading": "2 Elementary Matrix Algebra 5", "text": "2.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2 Matrix Decompositions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.3 Matrix (Pseudo) Inverse and Orthogonal Projector . . . . . . . . . . . . . . 6\n2.4 Time and Memory Costs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7"}, {"heading": "3 Matrix Sketching 9", "text": "3.1 Theoretical Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n3.2 Random Projection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.2.1 Gaussian Projection . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.2.2 Subsampled Randomized Hadamard Transform (SRHT) . . . . . . . 11\n3.2.3 Count Sketch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n3.2.4 GaussianProjection + CountSketch . . . . . . . . . . . . . . . . . . . 14\n3.3 Column Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n3.3.1 Uniform Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n3.3.2 Leverage Score Sampling . . . . . . . . . . . . . . . . . . . . . . . . . 15\n3.3.3 Local Landmark Selection . . . . . . . . . . . . . . . . . . . . . . . . 16"}, {"heading": "4 Regression 19", "text": "4.1 Standard Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n4.2 Inexact Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n4.2.1 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n4.2.2 Theoretical Explanation . . . . . . . . . . . . . . . . . . . . . . . . . 20\n4.3 Machine-Precision Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n4.3.1 Basic Idea: Preconditioning . . . . . . . . . . . . . . . . . . . . . . . 21\n4.3.2 Algorithm Description . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n4.4 Extension: CX-Type Regression . . . . . . . . . . . . . . . . . . . . . . . . . 22\n4.5 Extension: CUR-Type Regression . . . . . . . . . . . . . . . . . . . . . . . . 23\ni"}, {"heading": "5 Rank k Singular Value Decomposition 25", "text": "5.1 Standard Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 5.2 Prototype Randomized k-SVD Algorithm . . . . . . . . . . . . . . . . . . . . 26\n5.2.1 Theoretical Explanation . . . . . . . . . . . . . . . . . . . . . . . . . 26 5.2.2 Algorithm Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 5.2.3 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n5.3 Faster Randomized k-SVD . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 5.3.1 Theoretical Explanation . . . . . . . . . . . . . . . . . . . . . . . . . 28 5.3.2 Algorithm Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 5.3.3 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29"}, {"heading": "6 SPSD Matrix Sketching 31", "text": "6.1 Motivations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n6.1.1 Forming a Kernel Matrix . . . . . . . . . . . . . . . . . . . . . . . . . 31 6.1.2 Matrix Inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 6.1.3 Eigenvalue Decomposition . . . . . . . . . . . . . . . . . . . . . . . . 33\n6.2 Prototype Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 6.3 Faster SPSD Matrix Sketching . . . . . . . . . . . . . . . . . . . . . . . . . . 34 6.4 The Nystro\u0308m Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 6.5 More Efficient Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n6.5.1 Memory Efficient Kernel Approximation (MEKA) . . . . . . . . . . . 37 6.5.2 Structured Kernel Interpolation (SKI) . . . . . . . . . . . . . . . . . 38\n6.6 Extension to Rectangular Matrices: CUR Matrix Decomposition . . . . . . . 38 6.6.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 6.6.2 Prototype CUR Decomposition . . . . . . . . . . . . . . . . . . . . . 39 6.6.3 Faster CUR Decomposition . . . . . . . . . . . . . . . . . . . . . . . 39 6.7 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 6.7.1 Kernel Principal Component Analysis (KPCA) . . . . . . . . . . . . 41 6.7.2 Spectral Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 6.7.3 Gaussian Process Regression (GPR) . . . . . . . . . . . . . . . . . . . 43"}, {"heading": "A Several Facts of Matrix Algebra 47", "text": ""}, {"heading": "B Notes and Further Reading 49", "text": "Bibliography 49\nii\nAbstract\nMatrix operations such as matrix inversion, eigenvalue decomposition, singular value decomposition are ubiquitous in real-world applications. Unfortunately, many of these matrix operations so time and memory expensive that they are prohibitive when the scale of data is large. In real-world applications, since the data themselves are noisy, machine-precision matrix operations are not necessary at all, and one can sacrifice a reasonable amount of accuracy for computational efficiency.\nIn recent years, a bunch of randomized algorithms have been devised to make matrix computations more scalable. Mahoney [16] and Woodruff [34] have written excellent but very technical reviews of the randomized algorithms. Differently, the focus of this paper is on intuition, algorithm derivation, and implementation. This paper should be accessible to people with knowledge in elementary matrix algebra but unfamiliar with randomized matrix computations. The algorithms introduced in this paper are all summarized in a user-friendly way, and they can be implemented in lines of MATLAB code. The readers can easily follow the implementations even if they do not understand the maths and algorithms.\nKeywords: matrix computation, randomized algorithms, matrix sketching, random projection, random selection, least squares regression, randomized SVD, matrix inversion, eigenvalue decomposition, kernel approximation, the Nystro\u0308m method.\n1\n2\nChapter 1\nIntroduction\nMatrix computation plays a key role in modern data science. However, matrix computations such as matrix inversion, eigenvalue decomposition, SVD, etc, are very time and memory expensive, which limits their scalability and applications. To make large-scale matrix computation possible, randomized matrix approximation techniques have been proposed and widely applied. Especially in the past decade, remarkable progresses in randomized numerical linear algebra has been made, and now large-scale matrix computations are no longer impossible tasks.\nThis paper reviews the most recent progresses of randomized matrix computation. The papers written by Mahoney [16] and Woodruff [34] provide comprehensive and rigorous reviews of the randomized matrix computation algorithms. However, their focus are on the theoretical properties and error analysis techniques, and readers unfamiliar with randomized numerical linear algebra can have difficulty when implementing their algorithms.\nDifferently, the focus of this paper is on intuitions and implementations, and the target readers are those who are familiar with basic matrix algebra but has little knowledge in randomized matrix computations. All the algorithms in this paper are described in a userfriend way. This paper also provides MATLAB implementations of the important algorithms. MATLAB code is easy to understand1, easy to debug, and easy to translate to other languages. The users can even directly use the provided MATLAB code without understanding it.\nThis paper covers the following topics:\n\u2022 Chapter 2 briefly reviews some matrix algebra preliminaries. This chapter can be skipped if the reader is familiar with matrix algebra.\n\u2022 Chapter 3 introduces the techniques for generating a sketch of a large-scale matrix.\n\u2022 Chapter 4 studies the least squares regression (LSR) problem where n d.\n\u2022 Chapter 5 studies efficient algorithms for computing the k-SVD of arbitrary matrices. 1If your are unfamiliar with a MATLAB function, you can simply type \u201chelp + functionname\u201d in MAT-\nLAB and read the documentation.\n3\n\u2022 Chapter 6 introduces techniques for sketching symmetric positive semi-definite (SPSD) matrices. The applications includes spectral clustering, kernel methods (e.g. Gaussian process regression and kernel PCA), and second-order optimization (e.g. Newton\u2019s method).\n4\nChapter 2\nElementary Matrix Algebra\nThis chapter defines the matrix notation and goes through the very basics of matrix decompositions. Particularly, the singular value decomposition (SVD), the QR decomposition, and the Moore-Penrose inverse are used throughout this paper."}, {"heading": "2.1 Notation", "text": "Let A = [aij] be a matrix, a = [ai] be a column vector, and a be a scalar. The i-th row and j-th column of A are denoted by ai: and a:j, respectively. When there is no ambiguity, either column or row can be written as al. Let In be the n\u00d7 n identity matrix, that is, the diagonal entries are ones and off-diagonal entries are zeros. The column space (the space spanned by the columns) of A is the set of all possible linear combinations of its column vectors. Let [n] be the set {1, 2, \u00b7 \u00b7 \u00b7 , n}. Let nnz(A) be the number of nonzero entries of A.\nThe squared vector `2 norm is defined by \u2016a\u201622 = \u2211 i a2i .\nThe squared matrix Frobenius norm is defined by \u2016A\u2016F = \u2211 ij a2ij,\nand the matrix spectral norm is defined by\n\u2016A\u20162 = max x 6=0 \u2016Ax\u20162 \u2016x\u20162 ."}, {"heading": "2.2 Matrix Decompositions", "text": "QR decomposition. Let A be an m\u00d7n matrix with m \u2265 n. The QR decomposition of A is\nA = QA\ufe38\ufe37\ufe37\ufe38 m\u00d7n RA\ufe38\ufe37\ufe37\ufe38 n\u00d7n .\n5\nThe matrix QA has orthonormal columns, that is, Q T AQA = In. The matrix RA is upper triangular, that is, for all i < j, the (i, j)-th entry of RA is zero. SVD. Let A be an m \u00d7 n matrix and \u03c1 = rank(A). The condensed singular value decomposition (SVD) of A is\nA\ufe38\ufe37\ufe37\ufe38 m\u00d7n = UA\ufe38\ufe37\ufe37\ufe38 m\u00d7\u03c1 \u03a3A\ufe38\ufe37\ufe37\ufe38 \u03c1\u00d7\u03c1 VTA\ufe38\ufe37\ufe37\ufe38 \u03c1\u00d7n = \u03c1\u2211 i=1 \u03c3A,iuA,iv T A,i.\nHere \u03c3A,1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3A,\u03c1 > 0 are the singular values, uA,1, \u00b7 \u00b7 \u00b7 ,uA,\u03c1 \u2208 Rm are the left singular vectors, and vA,1, \u00b7 \u00b7 \u00b7 ,vA,\u03c1 \u2208 Rn are the right singular vectors. Unless otherwise specified, \u201cSVD\u201d refers to the condensed SVD.\nk-SVD. In applications such as the principal component analysis (PCA), latent semantic indexing (LSI), word2vec, spectral clustering, we are only interested in the top k ( m,n) singular values and singular vectors. The rank k truncated SVD (k-SVD) is denoted by\nAk := k\u2211 i=1 \u03c3A,iuA,iv T A,i = UA,k\ufe38 \ufe37\ufe37 \ufe38\nm\u00d7k \u03a3A,k\ufe38\ufe37\ufe37\ufe38 k\u00d7k VTA,k\ufe38 \ufe37\ufe37 \ufe38 k\u00d7n .\nHere UA,k consists of the first k singular vectors of UA, and \u03a3A,k and VV,k are analogously defined. Among all the m\u00d7n rank k matrices, Ak is the closest approximation to A in that\nAk = argmin X \u2016A\u2212X\u20162F = argmin X \u2016A\u2212X\u201622, s.t. rank(X) \u2264 k.\nEigenvalue decomposition. The eigenvalue decomposition of an n \u00d7 n symmetric matrix A is defined by\nA = UA\u039bAU T A = n\u2211 i=1 \u03bbA,iuA,iu T A,i.\nHere \u03bbA,1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbA,n are the eigenvalues of A, and uA,1, \u00b7 \u00b7 \u00b7 ,uA,n \u2208 Rn are the corresponding eigenvectors. A symmetric matrix A is called symmetric positive semidefinite (SPSD) if and only if all the eigenvalues are nonnegative. If A is SPSD, its SVD and eigenvalue decomposition are identical."}, {"heading": "2.3 Matrix (Pseudo) Inverse and Orthogonal Projector", "text": "For an n\u00d7 n square matrix A, the matrix inverse exists if A is non-singular (rank(A) = n). Let A\u22121 be the inverse of A. Then AA\u22121 = A\u22121A = In.\nOnly square and full rank matrices have inverse. For the general rectangular matrices or rank deficient matrices, matrix pseudo-inverse is used as a generalization of matrix inverse. The book [1] offers a comprehensive study of the pseudo-inverses.\n6\nThe Moore-Penrose inverse is the most widely used pseudo-inverse, which is defined by\nA\u2020 := VA\u03a3 \u22121 A U T A.\nLet A be any m\u00d7 n and rank \u03c1 matrix. Then\nAA\u2020 = UA\u03a3A V T AVA\ufe38 \ufe37\ufe37 \ufe38 =I\u03c1 \u03a3\u22121A U T A = UA\ufe38\ufe37\ufe37\ufe38 m\u00d7\u03c1 UTA\ufe38\ufe37\ufe37\ufe38 \u03c1\u00d7m ,\nwhich is a orthogonal projector. It is because for any matrix B, the matrix AA\u2020B = UAU T AB is the projection of B onto the column space of A."}, {"heading": "2.4 Time and Memory Costs", "text": "The time complexities of the matrix operations are listed in the following.\n\u2022 Multiplying an m\u00d7 n matrix A by an n\u00d7 p matrix B: O(mnp) float point operations (flops) in general, and O(p \u00b7 nnz(A)) if A is sparse. Here nnz(A) is the number of nonzero entries of A.\n\u2022 QR decomposition, SVD, or Moore-Penrose inverse of an m \u00d7 n matrix (m \u2265 n): O(mn2) flops.\n\u2022 k-SVD of an m \u00d7 n matrix: O(nmk) flops (assuming that the spectral gap and the logarithm of error tolerance are constant)\n\u2022 Matrix inversion or full eigenvalue decomposition of an n\u00d7 n matrix: O(n3) flops\n\u2022 k-eigenvalue decomposition of an n\u00d7 n matrix: O(n2k) flops.\nPass-efficient means that the algorithm goes constant passes through the data. For example, the Frobenius norm of a matrix can be computed pass-efficiently, because each entry is visited only once. In comparison, the spectral norm cannot be computed passefficiently, because the algorithm goes at least log 1 passes through the matrix, which is not constant. Here indicates the desired precision. Memory cost. If an algorithm scans a matrix for constant passes, the matrix can be placed in large volume disks, so the memory cost is not a bottleneck. However, if an algorithm goes through a matrix for many passes (not constant passes), the matrix should be placed in memory, otherwise the swaps between memory and disk would be highly expensive. In this paper, memory cost means the number of entries frequently visited by the algorithm.\n7\n8\nChapter 3\nMatrix Sketching\nLet A \u2208 Rm\u00d7n be the given matrix, S \u2208 Rn\u00d7s be a sketching matrix, e.g. random projection or column selection matrix, and C = AS \u2208 Rm\u00d7s be a sketch of A. The size of C is much smaller than A, but C preserves some important properties of A."}, {"heading": "3.1 Theoretical Properties", "text": "The sketching matrix is useful if it has either or both of the following properties. The two properties are important, and the readers should try to understand them.\nProperty 3.1 (Subspace Embedding). For a fixed m \u00d7 n (m n) matrix A and all mdimension vector y, the inequality\n1 \u03b3 \u2264 \u2016y TAS\u201622 \u2016yTA\u201622 \u2264 \u03b3\nholds with high probability. Here S \u2208 Rn\u00d7s (s n) is a certain sketching matrix.\nThe subspace embedding property can be intuitively understood in the following way. For all n dimensional vectors x in the row space of A (a rank m subspace within Rn),1 the length of vector x does not change much after sketching: \u2016x\u201622 \u2248 \u2016xS\u201622. This property can be applied to speedup the `2 regression problems.\nProperty 3.2 (Low-Rank Approximation). Let A be any m\u00d7n matrix and k be any positive integer far smaller than m and n. Let C = AS \u2208 Rm\u00d7s where S \u2208 Rn\u00d7s is a certain sketching matrix and s \u2265 k. The Frobenius norm error bound2\n\u2016A\u2212CC\u2020A\u20162F \u2264 \u03b7\u2016A\u2212Ak\u20162F\nholds with high probability for some \u03b7 \u2265 1. 1Thus there always exists an m dimensional vector y such that x can be expressed as x = yTA. 2Spectral norm bounds should be more interesting. However, spectral norm error is difficult to analyze, and existing spectral norm bounds are \u201cweak\u201d for their factors \u03b7 are far greater than 1.\n9\nThe following error bound is stronger and more interesting:\nmin rank(X)\u2264k\n\u2016A\u2212CX\u20162F \u2264 \u03b7\u2016A\u2212Ak\u20162F ."}, {"heading": "It is stronger because \u2016A\u2212CC\u2020A\u20162F \u2264 minrank(X)\u2264k \u2016A\u2212CX\u20162F .", "text": "Intuitively speaking, the low-rank approximation property means that the columns of Ak are almost in the column space of C = AP. The low-rank approximation property enables us to solve k-SVD more efficiently (for k \u2264 s). Later on we will see that computing the k-SVD of CC\u2020A is less expensive than the k-SVD of A.\nThe two properties can be verified by a few lines of MATLAB code. The readers are encouraged to have a try. With a proper sketching method and a relatively large s, both \u03b3 and \u03b7 should be near one."}, {"heading": "3.2 Random Projection", "text": "The section presents three matrix sketching techniques: Gaussian projection, subsampled randomized Hadamard transform (SRHT), and count sketch. Gaussian projection and SRHT can be combined with count sketch."}, {"heading": "3.2.1 Gaussian Projection", "text": "The n\u00d7 s Gaussian random projection matrix S is a matrix is formed by S = 1\u221a s G, where each entry of G is sampled i.i.d. from N (0, 1). The Gaussian projection is also well knows as the Johnson-Lindenstrauss transform due to the seminal work [15]. Gaussian projection can be implemented in four lines of MATLAB code.\n1 f unc t i on [C] = Gauss ianPro jec t ion (A, s ) 2 n = s i z e (A, 2) ; 3 S = randn (n , s ) / s q r t ( s ) ; 4 C = A \u2217 S ;\nGaussian projection has the following properties:\n\u2022 Time cost: O(mns)\n\u2022 Theoretical guarantees\n1. When s = O(m/ 2), the subspace embedding property with \u03b3 = 1 + holds with high probability.\n2. When s = k\n+ 1, the low-rank approximation property with \u03b7 = 1 + holds in expectation [3].\n\u2022 Advantages\n10\n1. Easy to implement: four lines of MATLAB code\n2. C is a very high quality sketch of A\n\u2022 Disadvantages:\n1. High time complexity to perform matrix multiplication\n2. Sparsity is destroyed: C is dense even if A is sparse"}, {"heading": "3.2.2 Subsampled Randomized Hadamard Transform (SRHT)", "text": "The Subsampled Randomized Hadamard Transform (SRHT) matrix is defined by S = 1\u221a sn DHnP, where\n\u2022 D \u2208 Rn\u00d7n is a diagonal matrix with diagonal entries sampled uniformly from {+1,\u22121};\n\u2022 Hn \u2208 Rn\u00d7n is defined recursively by\nHn = [ Hn/2 Hn/2 Hn/2 \u2212Hn/2 ] and H2 = [ +1 +1 +1 \u22121 ] ;\nFor all y \u2208 Rn, the matrix vector product yTHn can be performed in O(n log n) time by the fast Walsh\u2013Hadamard transform algorithm in a divide-and-conquer fashion;\n\u2022 P \u2208 Rn\u00d7s samples s from the n columns.\nSRHT can be implemented in nine lines of MATLAB code below. Notice that this implementation of SRHT is has O(mN logN) (N \u2265 n is a power of two) time complexity, which is not efficient.\n1 f unc t i on [C] = s rht (A, s ) 2 n = s i z e (A, 2) ; 3 sgn = randi (2 , [ 1 , n ] ) \u2217 2 \u2212 3 ; % one h a l f are +1 and the r e s t are \u22121 4 A = bsxfun ( @times , A, sgn ) ; % f l i p the s i g n s o f each column w. p . 50% 5 n = 2\u02c6( c e i l ( l og2 (n) ) ) ; 6 C = ( fwht (A\u2019 , n ) ) \u2019 ; % f a s t Walsh\u2212Hadarmard trans form 7 idx = s o r t ( randsample (n , s ) ) ; 8 C = C( : , idx ) ; % subsampling 9 C = C \u2217 (n / s q r t ( s ) ) ;\nThe SRHT matrix has the following properties:\n\u2022 Time complexity: the matrix product AS can be performed in O(mn log s) time, which makes SRHT more efficient than Gaussian projection. (Unfortunately, the MATLAB code above does not have such low time complexity.)\n\u2022 Theoretical property: when s = O( \u22122(m+ log n) logm), SRHT satisfies the subspace embedding property with \u03b3 = 1 + holds with probability 0.99 [34, Theorem 7].\n11"}, {"heading": "3.2.3 Count Sketch", "text": "Count sketch stems from the data stream literature [4; 26]. It was applied to speedup matrix computation by [6; 21]. We describe in the following the count sketch for matrix data.\nThere are different ways to implementing count sketch. This paper describe two quite different ways and refer to them as \u201cmap-reduce fashion\u201d and \u201cstreaming fashion\u201d. Of course, the two are equivalent.\n\u2022 The map-reduce fashion has three steps. First, hash each column with a discrete value uniformly sampled from [s]. Second, flip the sign of each column with probability 50%. Third, sum up columns with the same hash value. This procedure is illustrated in\n12\nAlgorithm 3.1 Count Sketch in the Streaming Fashion.\n1: input: A \u2208 Rm\u00d7n. 2: Initialize C to be an m\u00d7 s all-zero matrix; 3: for i = 1 to n do 4: sample l from the set [s] uniformly at random; 5: sample g from the set {+1,\u22121} uniformly at random; 6: update the l-th column of C by c:l \u2190\u2212 c:l + ga:i; 7: end for 8: return C \u2208 Rm\u00d7s.\nFigure 3.1. As its name suggests, this approach naturally fits the map-reduce systems.\n\u2022 The streaming fashion has two steps. First, initialize C to be the m \u00d7 s all-zero matrix. Second, for each column of A, flip its sign with probability 50%, and add it to a uniformly selected column of C. It is described in Algorithm 3.1 an illustrated in Figure 3.2. It can be implemented in 9 lines of MATLAB code as below. The streaming fashion implementation keeps the sketch C in memory and scans the data A in only one pass. If A does not fit in memory, this approach is better than the mapreduce fashion for it scans the columns sequentially. If A is sparse matrix, randomly accessing the entries may not be efficient, and thus it is better to accessing the column sequentially.\n1 f unc t i on [C] = CountSketch (A, s ) % the streaming f a s h i o n 2 [m, n ] = s i z e (A) ; 3 sgn = randi (2 , [ 1 , n ] ) \u2217 2 \u2212 3 ; % one h a l f are +1 and the r e s t are \u22121 4 A = bsxfun ( @times , A, sgn ) ; % f l i p the s i g n s o f each column w. p . 50% 5 l l = randsample ( s , n , t rue ) ; % sample n items from [ s ] with replacement 6 C = ze ro s (m, s ) ; % i n i t i a l i z e C 7 f o r j = 1 : n 8 C( : , l l ( j ) ) = C( : , l l ( j ) ) + A( : , j ) ; 9 end\nThe readers may have noticed that count sketch does not explicitly form the sketching matrix S. In fact, S is such a matrix that each row has only one nonzero entry. In the example of Figure 3.1, the matrix ST can be explicitly expressed as\nST =  0 0 1 0 1 \u22121 1 \u22121 \u22121 1 0 0 0 0 0\u22121 0 0 \u22121 0 0 0 0 0 0 0 \u22121 1 \u22121 \u22121 0 \u22121 0 0 0 0 0 0 0 0 1 0 0 0 0  . Count sketch has the following properties:\n\u2022 Time cost: O(nnz(A))\n\u2022 Memory cost: O(ms). When A does not fit in memory, the algorithm keeps only C in memory and goes one pass through the columns of A.\n13\n\u2022 Theoretical guarantees\n1. When s = O(m2/ 2), the subspace embedding property holds with \u03b3 = 1 + with high probability.\n2. When s = O(k/ +k2), the low-rank approximation property holds with \u03b7 = 1+ relative error with high probability.\n\u2022 Advantage: the count sketch is very efficient, especially when A is sparse.\n\u2022 Disadvantage: compared with Gaussian projection, the count sketch requires larger s to attain the same accuracy. One simple improvement is to combine the count sketch with Gaussian projection or SRHT."}, {"heading": "3.2.4 GaussianProjection + CountSketch", "text": "Let Ssc be n \u00d7 scs count sketch matrix, Sgp be scs \u00d7 s Gaussian projection matrix, and S = ScsSgp \u2208 Rn\u00d7s. Then S satisfies the following properties.\n\u2022 Time complexity: the matrix product AS can be computed in\nO (\nnnz(A)\ufe38 \ufe37\ufe37 \ufe38 count sketch + mscss\ufe38 \ufe37\ufe37 \ufe38 Gaussian projection ) time.\n\u2022 Theoretical properties:\n14\n1. When scs = O(m2/ 2) and s = O(m/ 2), the GaussianProjection+CountSketch matrix S satisfy the subspace embedding property with \u03b3 = 1 + holds with high probability.\n2. When scs = O(k2 + k/ ) and s = O(k/ ), the GaussianProjection+CountSketch matrix S satisfies the low-rank approximation property with \u03b7 = 1 + [2, Lemma 12].\n\u2022 Advantages:\n1. the size of GaussianProjection+CountSketch is as small as Gaussian projection.\n2. the time complexity is much lower than Gaussian projection when n m."}, {"heading": "3.3 Column Selection", "text": "This section presents three column selection techniques: uniform sampling, leverage score sampling, and local landmark selection. Different from random projection, column selection do not have to visit every entry of A, and column selection preserves the sparsity/nonnegativity properties of A."}, {"heading": "3.3.1 Uniform Sampling", "text": "Uniform sampling is the most efficient way to form a sketch. The most important advantage is that uniform sampling forms a sketch without seeing the whole data matrix. When applied to kernel methods, uniform sampling avoids computing every entry of the kernel matrix.\nThe performance of uniform sampling is data-dependent. When the leverage scores (defined in Section 3.3.2) are uniform, or equivalently, the matrix coherence (namely the greatest leverage score) is small, uniform sampling has good performance. The analysis of uniform sampling can be found in [12; 13]."}, {"heading": "3.3.2 Leverage Score Sampling", "text": "Before studying leverage score sampling, let\u2019s first define leverage scores. Let A be an m\u00d7n matrix, with \u03c1 = rank(A) < n, and V \u2208 Rn\u00d7\u03c1 be the right singular vectors. The (column) leverage scores of A are defined by\nli := \u2016vi:\u201622, for i = 1, \u00b7 \u00b7 \u00b7 , n.\nLeverage score sampling is to select each columns of A with probability proportional to its leverage scores. (Sometimes each selected column should be scaled by \u221a\n\u03c1 sli .) It can be\nroughly implemented in 8 lines MATLAB code.\n15\n1 f unc t i on [C, idx ] = LeverageScoreSampling (A, s ) 2 n = s i z e (A, 2) ; 3 [ \u02dc , \u02dc , V] = svd (A, \u2019 econ \u2019 ) ; 4 l e v e r a g e s c o r e s = sum(V. \u02c6 2 , 2) ; 5 prob = l e v e r a g e s c o r e s / sum( l e v e r a g e s c o r e s ) ; 6 idx = randsample (n , s , true , prob ) ; 7 idx = unique ( idx ) ; % e l i m i n a t e d u p l i c a t e s 8 C = A( : , idx ) ;\nThere are a few things to remark:\n\u2022 To sample columns according to the leverage scores of Ak where k m,n, Line 3 can be replaced by\n3 [ \u02dc , \u02dc , V] = svds (A, k ) ;\n\u2022 Theoretical properties\n1. When s = O(m/ + m logm), the leverage score sampling satisfies the subspace embedding property with \u03b3 = 1 + holds with high probability.\n2. When s = O(k/ +k log k), the leverage score sampling (according to the leverage scores of Ak) satisfies the low-rank approximation property with \u03b7 = 1 + .\n\u2022 Computing the leverage scores is as expensive as computing SVD, so leverage score sampling is not a practical way to sketch the matrix A itself.\n\u2022 When the leverage scores are near uniform, there is little difference between uniform sampling and leverage score sampling."}, {"heading": "3.3.3 Local Landmark Selection", "text": "Local landmark selection is a very effective heuristic for finding representative columns. Zhang and Kwok [36] proposed to set k = s and run k-means or k-centroids clustering algorithm to cluster the columns of A to s class, and use the s centroids as the sketch of A. This heuristic works very well in practice, though it has little theoretical guarantee.\nThere are several tricks to make the local landmark selection more efficient.\n\u2022 One can simply solve k-centroids clustering approximately rather than accurately. For example, it is unnecessary to wait for k-centroids clustering to converge; running kcentroids for a few iterations suffices.\n\u2022 When n is large, one can uniformly sample a subset of the data, e.g. max{0.2n, 20s} data points, and perform local landmark selection on this smaller dataset.\n16\n\u2022 In supervised learning problems, each datum ai is associated with a label yi. We can partition the data to g groups according to the labels and run k-centroids clustering independently on the data in each group. In this way, s = gk data points are selected as a sketch of A.\n17\n18\nChapter 4\nRegression\nLet A be an n\u00d7 d (n \u2265 d) matrix whose rows correspond to data and columns correspond to features, and let b \u2208 Rn contain the response/label of each datum. The least squares regression (LSR)\nmin x \u2016Ax\u2212 b\u201622 (4.1)\nis a ubiquitous problem in statistics, computer science, economics, etc. When n d, LSR can be efficiently solved using randomized algorithms."}, {"heading": "4.1 Standard Solutions", "text": "The least squares regression (LSR) problem (4.1) has closed form solution\nx? = A\u2020b.\nThe Moore-Penrose inverse can be computed by SVD which costs O(nd2) time. LSR can also be solved by numerical algorithms such as the conjugate gradient (CG) algorithm, and machine-precision can be attained in a reasonable number of iterations. Let \u03ba(A) := \u03c31(A)\n\u03c3d(A) be the condition number of A. The convergence of CG depends on \u03ba(A):\n\u2016A(x(t) \u2212 x?)\u201622 \u2016A(x(0) \u2212 x?)\u201622 \u2264 2 ( \u03ba(A)\u2212 1 \u03ba(A) + 1 )t ,\nwhere x(t) is the model in the t-th iteration of CG. The per-iteration time cost of CG is O(nnz(A)). To attain \u2016A(x(t) \u2212 x?)\u201622 \u2264 , the number of iteration is roughly(\nlog 1 + log(InitialError) )\u03ba(A)\u2212 1 2 .\nSince the time cost of CG heavily depends on the unknown condition number \u03ba(A), CG can be very slow if A is ill-conditioned.\n19"}, {"heading": "4.2 Inexact Solution", "text": "Any sketching matrix S \u2208 Rn\u00d7s can be used to solve LSR approximately as long as it satisfies the subspace embedding property. We consider the following LSR problem:\nx\u0303 = min x \u2016 (STA)\ufe38 \ufe37\ufe37 \ufe38\ns\u00d7d\nx\u2212 STb\u201622, (4.2)\nwhich can be solved in O(sd2) time. If S is a Gaussian projection matrix, SRHT matrix, count sketch, or leverage score sampling matrix, and s = poly(d/ ) for any error parameter \u2208 (0, 1], then\n\u2016Ax\u0303\u2212 b\u201622 \u2264 (1 + )2 min x \u2016Ax\u2212 b\u201622\nis guaranteed."}, {"heading": "4.2.1 Implementation", "text": "If S is count sketch matrix, the inexact LSR algorithm can be implemented in 5 lines of MATLAB code. Here CountSketch is a MATLAB function described in Section 3.2.3. The total time cost is O(nnz(A) + poly(d/ )) and memory cost is O(poly(d/ )), which are lower than the cost of exact LSR when d n.\n1 f unc t i on [ x t i l d e ] = InexactLSR (A, b , s ) 2 d = s i z e (A, 2) ; 3 sketch = ( CountSketch ( [A, b ] \u2019 , s ) ) \u2019 ; 4 Asketch = sketch ( : , 1 : d ) ; % Asketch = S \u2019 \u2217 A 5 bsketch = sketch ( : , end ) ; % bsketch = S \u2019 \u2217 b 6 x t i l d e = Asketch \\ bsketch ;\nThere are a few things to remark:\n\u2022 The inexact LSR is useful only when n = \u2126(d/ + d2).\n\u2022 The size of sketch s is a polynomial function of \u22121 rather than logarithm of \u22121, thus the algorithm cannot attain high precision."}, {"heading": "4.2.2 Theoretical Explanation", "text": "By the subspace embedding property, it can be easily shown that x\u0303 is a good solution. Let D = [A,b] \u2208 Rn\u00d7(d+1) and z = [x;\u22121] \u2208 Rn+1. Then\nAx\u2212 b = Dz and STAx\u2212 STb = STDz,\n20\nand the subspace embedding property indicates 1 \u03b7 \u2016Dz\u201622 \u2264 \u2016STDz\u201622 \u2264 \u03b7\u2016Dz\u201622 for all z. Thus\n1 \u03b7 \u2016Ax\u0303\u2212 b\u201622 \u2264 \u2016ST (Ax\u0303\u2212 b)\u201622 and \u2016ST (Ax? \u2212 b)\u201622 \u2264 \u03b7\u2016Ax? \u2212 b\u201622\nThe optimality of x\u0303 indicates \u2016ST (Ax\u0303\u2212 b)\u201622 \u2264 \u2016ST (Ax? \u2212 b)\u201622, and thus 1\n\u03b7 \u2016Ax\u0303\u2212 b\u201622 \u2264 \u2016ST (Ax\u0303\u2212 b)\u201622 \u2264 \u2016ST (Ax? \u2212 b)\u201622 \u2264 \u03b7\u2016Ax? \u2212 b\u201622.\n\u21d2 \u2016Ax\u0303\u2212 b\u201622 \u2264 \u03b72\u2016Ax? \u2212 b\u201622.\nTherefore, as long as S satisfies the subspace embedding property, the approximate solution to LSR is nearly as good as the optimal solution (in terms of objective function value)."}, {"heading": "4.3 Machine-Precision Solution", "text": "Randomized algorithms can also be applied to find machine-precision solution to LSR, and the time complexity is lower than the standard solutions. The state-of-the-art algorithm [18] is based on very similar idea described in this section."}, {"heading": "4.3.1 Basic Idea: Preconditioning", "text": "We have discussed previously that the time cost of the conjugate gradient (CG) algorithm is roughly\n\u03ba(A)\u2212 1 2\n( log 1 + log(InitialError) ) nnz(A),\nwhich dependents on the condition number of A. To make CG efficient, one can find a d\u00d7 d preconditioning matrix T such that \u03ba(AT) is small, solve\nz? = argmin z \u2016(AT)z\u2212 b\u201622 (4.3)\nby CG, and let x? = Tz?. In this way, the time cost of CG is roughly\n\u03ba(AT)\u2212 1 2\n( log 1 + log(InitialError) ) nnz(A).\nIf \u03ba(AT) is a small constant, e.g. \u03ba(AT) = 2, then (4.3) can be very efficiently solved by CG.\nNow let\u2019s consider how to find the preconditioning matrix T. Let A = QARA be the QR decomposition. Obviously T = R\u22121A is a perfect preconditioning matrix because \u03ba(AR\u22121A ) = \u03ba(QA) = 1. Unfortunately, the preconditioning matrix T = R \u22121 A is not a practical choice because computing the QR decomposition is as expensive as solving LSR. Woodruff [34] proposed to use sketching to find RA approximately inO(nnz(A)+poly(d)) time. Let S \u2208 Rn\u00d7s be a sketching matrix and form Y = STA. Let Y = QYRY be the QR decomposition of Y. Theory shows that the sketch size s = O(d2) suffices for \u03ba(AR\u22121Y ) \u2264 2 holding with high probability. Thus R\u22121Y \u2208 Rd\u00d7d is a good preconditioning matrix.\n21\nAlgorithm 4.1 Machine-Precision Solution to LSR.\n1: input: A \u2208 Rn\u00d7d, b \u2208 Rn, and step size \u03b8. 2: Draw a sketching matrix S \u2208 Rn\u00d7s where s = O(d2); 3: Form the sketch Y = STA \u2208 Rs\u00d7d; 4: Compute the QR decomposition Y = QYRY; 5: Compute the preconditioning matrix T = R\u22121Y ; 6: Compute the initial solution z(0) = (STAT)\u2020(STb) = QTY(S\nTb); 7: for t = 1, \u00b7 \u00b7 \u00b7 ,O(log \u22121) do 8: r(t) = b\u2212ATz(t\u22121) ; // the residual 9: z(t) = z(t\u22121) + \u03b8TTAT r(t); // gradient descent\n10: end for 11: return x? = Tz(t) \u2208 Rd."}, {"heading": "4.3.2 Algorithm Description", "text": "The algorithm is described in Algorithm 4.1. We first form a sketch Y = STA \u2208 Rs\u00d7d and compute its QR decomposition Y = QYRY. We can use this QR decomposition to find the initial solution z(0) and the preconditioning matrix T = R\u22121Y . If we set s = O(d2), the initial solution is only constant times worse than the optimal in terms of objective function value. Theory also ensures that the condition number \u03ba(AT) \u2264 2. With the good initialization and good condition number, the vanilla gradient descent1 or CG takes only O(log \u22121) steps to attain 1 + solution. Notice that Lines 8 and 9 in the algorithm should be cautiously implemented. Do not compute the matrix product AT because it would take O(nnz(A)d) time!"}, {"heading": "4.4 Extension: CX-Type Regression", "text": "Given any matrix A \u2208 Rm\u00d7n, CX decomposition considers decomposing A into A \u2248 CX?, where C \u2208 Rm\u00d7c is a sketch of A and X? \u2208 Rc\u00d7n is computed by\nX? = argmin X\n\u2225\u2225A\u2212CX\u2225\u22252 F = C\u2020A.\nIt takes O(mnc) time to compute X?. If c m, this problem can be solved more efficiently by sketching. Specifically, we can draw a sketching matrix S \u2208 Rm\u00d7s and compute the approximate solution\nX\u0303 = argmin X \u2016STC\ufe38\ufe37\ufe37\ufe38\ns\u00d7c X\ufe38\ufe37\ufe37\ufe38 c\u00d7n \u2212STA\ufe38\ufe37\ufe37\ufe38 s\u00d7n \u20162F = (STC)\u2020(STA)\nIf S is a count sketch matrix, we set s = O(c/ + c2); if S samples columns according to the row leverage scores of C, we set s = O(c/ + c log c). It holds with high probability that\u2225\u2225A\u2212CX\u0303\u2225\u22252\nF \u2264 (1 + ) min\nX\n\u2225\u2225A\u2212CX\u2225\u22252 F .\n1Since AT is well conditioned, the vanilla gradient descent and CG has little difference.\n22"}, {"heading": "4.5 Extension: CUR-Type Regression", "text": "A more complicated problem has also been considered in the literature [25; 29; 24]:\nX? = argmin X \u2016 C\ufe38\ufe37\ufe37\ufe38 n\u00d7c X\ufe38\ufe37\ufe37\ufe38 c\u00d7r R\ufe38\ufe37\ufe37\ufe38 r\u00d7n \u2212 A\ufe38\ufe37\ufe37\ufe38 m\u00d7n \u20162F (4.4)\nwhere c, r m,n. The solution is:\nX? = C\u2020AR\u2020,\nwhich cost O(mn \u00b7 min{c, r}) time. Wang et al. [31] proposed an algorithm to solve (4.4) approximately by\nX\u0303 = argmin X \u2016STC(CXR\u2212A)SR\u20162F\nwhere SC \u2208 Rm\u00d7sc and SR \u2208 Rn\u00d7sr are leverage score sampling matrices. When sc = c \u221a q/\nand sr = r \u221a q/ (where q = min{m,n}), it holds with high probability that\n\u2016CX\u0303R\u2212A\u20162F \u2264 (1 + ) min X \u2016CXR\u2212A\u20162F .\nThe total time cost is\nO(scsr \u00b7min{c, r}) = O(cr \u22121 \u00b7min{m,n} \u00b7min{c, r})\ntime, which is useful when max{m,n} c, r. The algorithm can be implemented in 4 lines of MATLAB code:\n1 f unc t i on [ Xt i lde ] = InexactCurTypeRegress ion (C, R, A, sc , s r ) 2 [ \u02dc , idxC ] = LeverageScoreSampling (C\u2019 , sc ) ; 3 [ \u02dc , idxR ] = LeverageScoreSampling (R, s r ) ; 4 Xt i lde = pinv (C( idxC , : ) ) \u2217 A( idxC , idxR ) \u2217 pinv (R( : , idxR ) ) ;\nHere the function \u201cLeverageScoreSampling\u201d is described in Section 3.3.2. Empirically, setting s1 = s2 = O(d1 + d2) suffices for high precision. The experiments in [31] indicates that uniform sampling performs equally well as leverage score sampling.\n23\n24\nChapter 5\nRank k Singular Value Decomposition\nThis chapter considers the k-SVD of a large scale matrix A \u2208 Rm\u00d7n, which may not fit in memory."}, {"heading": "5.1 Standard Solutions", "text": "The standard solutions to k-SVD include the power iteration algorithm and the Krylov subspace methods. Their time complexities are considered to be O\u0303(mnk), where the O\u0303 notation hides parameters such as the spectral gap and logarithm of error tolerance. Here we introduce a simplified version of the block Lanczos method [19]1 which costs time O(mnkq), where q = log n is the number of iterations, and the inherent constant depends weakly on the spectral gap. The block Lanczos algorithm is described in Algorithm 5.1 can be implemented in 18 lines of MATLAB code.\n1 f unc t i on [U, S , V] = BlockLanczos (A, k , q ) 2 s = 2 \u2217 k ; % can be tuned 3 [m, n ] = s i z e (A) ; 4 C = A \u2217 randn (n , s ) ; 5 Krylov = ze ro s (m, s \u2217 q ) ; 6 Krylov ( : , 1 : s ) = C; 7 f o r i = 2 : q 8 C = A\u2019 \u2217 C; 9 C = A \u2217 C;\n10 [C, \u02dc ] = qr (C, 0) ; % opt i ona l 11 Krylov ( : , ( i \u22121)\u2217 s +1: i \u2217 s ) = C; 12 end 13 [Q, \u02dc ] = qr ( Krylov , 0) ; 14 [ Ubar , S , V] = svd (Q\u2019 \u2217 A, \u2019 econ \u2019 ) ; 15 Ubar = Ubar ( : , 1 : k ) ; 16 S = S ( 1 : k , 1 : k ) ;\n1We introduce this algorithm because it is easy to understand. However, as q grows, columns of the Krylov matrix gets increasingly linearly dependent, which sometimes leads to instability. Thus there are many numerical treatments to strengthen stability (see the numerically stable algorithms in [23]).\n25\nAlgorithm 5.1 k-SVD by the Block Lanczos Algorithm. 1: Input: an m\u00d7 n matrix A and the target rank k. 2: Set s = k +O(1) be the over-sampling parameter; 3: Set q = O(log n ) be the number of iteration; 4: Draw a n\u00d7 s sketching matrix S; 5: C = AS; 6: Set K = [ C, (AAT )C, (AAT )2C, \u00b7 \u00b7 \u00b7 , (AAT )q\u22121C ] ;\n7: QR decomposition: [ QC\ufe38\ufe37\ufe37\ufe38 m\u00d7sq ,RC] = qr( K\ufe38\ufe37\ufe37\ufe38 m\u00d7sq ); 8: SVD: [ U\u0304\ufe38\ufe37\ufe37\ufe38 sq\u00d7sq , \u03a3\ufe38\ufe37\ufe37\ufe38 sq\u00d7sq , V\ufe38\ufe37\ufe37\ufe38 n\u00d7sq ] = svd(QTCA\ufe38 \ufe37\ufe37 \ufe38 s\u00d7n );\n9: Retain the top k components of U\u0304, \u03a3, and V to form sq \u00d7 k, k \u00d7 k, n\u00d7 k matrices; 10: U = QU\u0304 \u2208 Rm\u00d7k; 11: return U\u03a3VT \u2248 Ak.\n17 V = V( : , 1 : k ) ; 18 U = Q \u2217 Ubar ;\nAlthough the block Lanczos algorithm can attain machine precision, it inevitably goes many passes through A, and it is thus slow when A does not fit in memory.\nFacing large-scale data, we must trade off between precision and computational costs. We are particularly interested in approximate algorithm that satisfies:\n1. The algorithm goes constant passes through A. Then A can be stored in large volume disks, and there are only constant swaps between disk and memory.\n2. The algorithm only keeps a small-scale sketch of A in memory.\n3. The time cost is O(mnk) or lower.\n5.2 Prototype Randomized k-SVD Algorithm\nThis section describes a randomized algorithm that computes the k-SVD of A up to 1 + Frobenius norm relative error. The algorithm is proposed by [14], and it is described in Algorithm 5.2."}, {"heading": "5.2.1 Theoretical Explanation", "text": "If C = AS \u2208 Rm\u00d7s is a good sketch of A, the column space of C should roughly contain the columns of Ak\u2014this is the low-rank approximation property. If S \u2208 Rn\u00d7s is Gaussian projection matrix or count sketch and s = O(k/ ), then the low-rank approximation property\nmin rank(Z)\u2264k\n\u2016CZ\u2212A\u20162F \u2264 (1 + )\u2016A\u2212Ak\u20162F (5.1)\n26\nAlgorithm 5.2 Prototype Randomized k-SVD Algorithm. 1: Input: an m\u00d7 n matrix A and the target rank k. 2: Draw a n\u00d7 s sketching matrix S where s = O(k ); 3: C = AS; 4: QR decomposition: [QC\ufe38\ufe37\ufe37\ufe38\nm\u00d7s\n,RC] = qr( C\ufe38\ufe37\ufe37\ufe38 m\u00d7s );\n5: k-SVD: [ U\u0304\ufe38\ufe37\ufe37\ufe38 s\u00d7k , \u03a3\u0303\ufe38\ufe37\ufe37\ufe38 k\u00d7k , V\u0303\ufe38\ufe37\ufe37\ufe38 n\u00d7k ] = svds(QTCA\ufe38 \ufe37\ufe37 \ufe38 s\u00d7n , k); 6: U\u0303 = QCU\u0304 \u2208 Rm\u00d7k; 7: return U\u0303\u03a3\u0303V\u0303T \u2248 Ak.\nholds in expectation."}, {"heading": "5.2.2 Algorithm Derivation", "text": "Let QC be any orthonormal bases of C. Since the column space of C is the same to the column space of QC, the minimization problem in (5.1) can be equivalently converted to\nX? = argmin rank(X)\u2264k \u2016 QC\ufe38\ufe37\ufe37\ufe38 m\u00d7s X\ufe38\ufe37\ufe37\ufe38 s\u00d7n \u2212 A\ufe38\ufe37\ufe37\ufe38 m\u00d7n \u20162F = (QTCA)k. (5.2)\nHere the second equality is a well known fact. The matrix Ak is well approximated by A\u0303k := QCX ?, so we need only to find the k-SVD of A\u0303k:\nA\u0303k := QC\ufe38\ufe37\ufe37\ufe38 m\u00d7s X?\ufe38\ufe37\ufe37\ufe38 s\u00d7n = QC (Q T CA)k\ufe38 \ufe37\ufe37 \ufe38\n:=U\u0304\u03a3\u0303V\u0303T = QCU\u0304\ufe38 \ufe37\ufe37 \ufe38 :=U\u0303 \u03a3\u0303V\u0303T = U\u0303\ufe38\ufe37\ufe37\ufe38 m\u00d7k \u03a3\u0303\ufe38\ufe37\ufe37\ufe38 k\u00d7k V\u0303T\ufe38\ufe37\ufe37\ufe38 k\u00d7n .\nIt is easy to check that U\u0303 and V\u0303 have orthonormal columns and \u03a3\u0303 is a diagonal matrix. Notice that the accuracy of the randomized k-SVD depends only on the quality of the sketch matrix C."}, {"heading": "5.2.3 Implementation", "text": "The algorithm is described in Algorithm 5.2 and can be implemented in 5 lines of MATLAB code. Here s = O(k ) is the size of the sketch.\n1 f unc t i on [ Uti lde , S t i l d e , Vt i lde ] = ksvdPrototype (A, k , s ) 2 C = CountSketch (A, s ) ; 3 [Q, R] = qr (C, 0) ; 4 [ Ubar , S t i l d e , Vt i lde ] = svds (Q\u2019 \u2217 A, k ) ; 5 Ut i lde = Q \u2217 Ubar ;\nEmpirically, using \u201csvd(Q\u2032 \u2217 A, \u2032econ\u2032)\u201d followed by discarding the k + 1 to s components should be faster than the \u201csvds\u201d function in Line 4.\nThe algorithm has the following properties:\n27\n1. The algorithm goes 2 passes through A;\n2. The algorithm only keeps an m\u00d7O(k ) sketch C in memory;\n3. The time cost is O(nnz(A)k/ ).\n5.3 Faster Randomized k-SVD\nThe prototype algorithm spends most of its time on solving (5.2); if (5.2) can be solved more efficiently, the randomized k-SVD can be even faster. The readers may have noticed that (5.2) is the least squares regression (LSR) problem discussed in Section 4.4. Yes, we can solve (5.2) efficiently by the inexact LSR algorithm presented in the previous section."}, {"heading": "5.3.1 Theoretical Explanation", "text": "Now we draw a m\u00d7 p GaussianProjection+CountSketch matrix P and solve this problem:\nX\u0303 = argmin rank(X)\u2264k \u2016PTQC\ufe38 \ufe37\ufe37 \ufe38 p\u00d7s X\ufe38\ufe37\ufe37\ufe38 s\u00d7n \u2212PTA\ufe38 \ufe37\ufe37 \ufe38 p\u00d7n \u20162F . (5.3)\nTo understand this trick, the readers can retrospect the extension of LSR in Section 4.4. Let\nP = Pcs\ufe38\ufe37\ufe37\ufe38 m\u00d7pcs Psrht\ufe38 \ufe37\ufe37 \ufe38 pcs\u00d7p\nwhere pcs = O(k/ +k2) and p = O(k/ ). The subspace embedding property of RSHT+CountSketch [6, Theorem 46] implies that\n(1 + )\u22121\u2016QCX\u0303\u2212A\u20162F \u2264 \u2016PT (QCX\u0303\u2212A)\u20162F \u2264 \u2016PT (QCX? \u2212A)\u20162F \u2264 (1 + )\u2016QCX? \u2212A\u20162F , \u21d2 \u2016QCX\u0303\u2212A\u20162F \u2264 (1 + )2\u2016QCX? \u2212A\u20162F \u2264 (1 + )3\u2016A\u2212Ak\u20162F .\nHere the second inequality follows from the optimality of X\u0303, and the last inequality follows from the low-rank approximation property of the sketch C = AS. Thus, by solving (5.3) we get k-SVD up to 1 +O( ) Frobenius norm relative error."}, {"heading": "5.3.2 Algorithm Derivation", "text": "The faster randomized k-SVD is described in Algorithm 5.3 and derived in the following. The algorithm solves\nX\u0303 = argmin rank(X)\u2264k \u2016PTC\ufe38 \ufe37\ufe37 \ufe38 p\u00d7s X\ufe38\ufe37\ufe37\ufe38 s\u00d7n \u2212PTA\ufe38 \ufe37\ufe37 \ufe38 p\u00d7n \u20162F (5.4)\nto obtain the rank k matrix X\u0303 \u2208 Rc\u00d7n, and approximates Ak by\nAk \u2248 CX\u0303.\n28\nAlgorithm 5.3 Faster Randomized k-SVD Algorithm. 1: Input: an m\u00d7 n matrix A and the target rank k. 2: Set the parameters as s = O\u0303(k ), pcs = s 2 log6 s + s , and p = s log s ;\n3: Draw a n\u00d7 s count sketch matrix S and perform sketching: C = AS; 4: Draw an m\u00d7 pcs count sketch matrix Pcs and an pcs \u00d7 p matrix Psrht; 5: Perform Sketching: D = PTsrhtP T csC \u2208 Rp\u00d7s and L = PTsrhtPTcsA \u2208 Rp\u00d7n; 6: QR decomposition: [QD\ufe38\ufe37\ufe37\ufe38 p\u00d7s , RD\ufe38\ufe37\ufe37\ufe38 s\u00d7s ] = qr( D\ufe38\ufe37\ufe37\ufe38 p\u00d7s ); 7: k-SVD: [ U\u0304\ufe38\ufe37\ufe37\ufe38 s\u00d7k , \u03a3\u0304\ufe38\ufe37\ufe37\ufe38 k\u00d7k , V\u0304\ufe38\ufe37\ufe37\ufe38 n\u00d7k ] = svds(QTDL\ufe38 \ufe37\ufe37 \ufe38 s\u00d7n , k); 8: SVD: [ U\u0303\ufe38\ufe37\ufe37\ufe38 n\u00d7k , \u03a3\u0303\ufe38\ufe37\ufe37\ufe38 k\u00d7k , V\u0302\ufe38\ufe37\ufe37\ufe38 k\u00d7k ] = svd(CR\u2020DU\u0304\u03a3\u0304\ufe38 \ufe37\ufe37 \ufe38 s\u00d7k ); 9: V\u0303 = V\u0304\ufe38\ufe37\ufe37\ufe38 n\u00d7k V\u0302\ufe38\ufe37\ufe37\ufe38 k\u00d7k ;\n10: return U\u0303\u03a3\u0303V\u0303T \u2248 Ak.\nDefine D = PTC, L = PTA, and let QDRD = D be the QR decomposition. Then (5.4) becomes\nX\u0303 = argmin rank(X)\u2264k \u2016 D\ufe38\ufe37\ufe37\ufe38 p\u00d7s X\ufe38\ufe37\ufe37\ufe38 s\u00d7n \u2212 L\ufe38\ufe37\ufe37\ufe38 p\u00d7n \u20162F = R \u2020 D\ufe38\ufe37\ufe37\ufe38\ns\u00d7s (QTDL)k\ufe38 \ufe37\ufe37 \ufe38 s\u00d7n .\nBased on the defined notation, we decompose Ak \u2248 CX\u0303 by\nAk \u2248 CX\u0303 = CR\u2020D (Q T DL)k\ufe38 \ufe37\ufe37 \ufe38\n:=U\u0304\u03a3\u0304V\u0304T = CR\u2020DU\u0304\u03a3\u0304\ufe38 \ufe37\ufe37 \ufe38 :=U\u0303\u03a3\u0303V\u0302T V\u0304T = U\u0303\u03a3\u0303 V\u0302T V\u0304T\ufe38 \ufe37\ufe37 \ufe38 :=V\u0303T = U\u0303\ufe38\ufe37\ufe37\ufe38 m\u00d7k \u03a3\u0303\ufe38\ufe37\ufe37\ufe38 k\u00d7k V\u0303T\ufe38\ufe37\ufe37\ufe38 k\u00d7n ."}, {"heading": "5.3.3 Implementation", "text": "The faster randomized k-SVD is described in Algorithm 5.3 and implemented in 18 lines of MATLAB code.\n1 f unc t i on [ Uti lde , S t i l d e , Vt i lde ] = ksvdFaster (A, k , s , p1 , p2 ) 2 n = s i z e (A, 2) ; 3 C = CountSketch (A, s ) ; 4 A = [A, C ] ; 5 A = A\u2019 ; 6 sketch = CountSketch (A, p1 ) ; 7 c l e a r A % A (m\u2212by\u2212n) w i l l not be used 8 sketch = Gauss ianPro ject ion ( sketch , p2 ) ; 9 sketch = sketch \u2019 ;\n10 L = sketch ( : , 1 : n ) ; 11 D = sketch ( : , n+1:end ) ; 12 c l e a r sketch % sketch ( p2\u2212by\u2212(n+c ) ) w i l l not be used 13 [QD, RD] = qr (D, 0) ;\n29\n14 [ Ubar , Sbar , Vbar ] = svds (QD\u2019 \u2217 L , k ) ; 15 c l e a r L % L ( p2\u2212by\u2212n) w i l l not be used 16 C = C \u2217 ( pinv (RD) \u2217 ( Ubar \u2217 Sbar ) ) ; 17 [ Ut i lde , S t i l d e , Vhat ] = svd (C, \u2019 econ \u2019 ) ; 18 Vt i lde = Vbar \u2217 Vhat ;\nThere are a few things to remark:\n1. The algorithm goes only two passes through A. 2. The algorithm costs time O ( nnz(A) + (m+ n)poly(k/ ) ) .\n3. The parameters should be set as k < s < p2 < p1 m,n.\n4. Line 8 can be removed or replaced by other sketching methods.\n5. \u201cA\u201d, \u201csketch\u201d, and \u201cL\u201d are the most memory expensive variables in the program, but fortunately, they are swept only one or two passes. If \u201cA\u201d, \u201csketch\u201d, and \u201cL\u201d do not fit in memory, they should be stored in disk and loaded to memory block-by-block to perform computations.\n6. Unless both m and n are large enough, this algorithm may be slower than the prototype algorithm.\n30\nChapter 6\nSPSD Matrix Sketching\nThis chapter considers SPSD matrix K \u2208 Rn\u00d7n, which can be a kernel matrix, a social network graph, a Hessian matrix, or a Fisher information matrix. Our objective is to find a low-rank decomposition K \u2248 LLT . (Notice that LLT is always SPSD, no matter what L is.) If K is symmetric but not SPSD, it can be approximated by K \u2248 CZCT where Z is symmetric but not necessarily SPSD."}, {"heading": "6.1 Motivations", "text": "This section provides three motivation examples to show why we seek to sketch K by K \u2248 LLT or K \u2248 CZCT ."}, {"heading": "6.1.1 Forming a Kernel Matrix", "text": "In the kernel approximation problems, we are given\n\u2022 an n\u00d7 d matrix X, whose rows are data points x1, \u00b7 \u00b7 \u00b7 ,xn \u2208 Rd,\n\u2022 a kernel function, e.g. the Gaussian RBF kernel function defined by \u03ba(xi,xj) = exp ( \u2212 1\n2\u03c3\u22122 \u2016xi \u2212 xj\u201622 ) where \u03c3 > 0 is the kernel width parameter.\nThe RBF kernel matrix can be computed by the following MATLAB code:\n1 f unc t i on [K] = rb f (X1 , X2 , sigma ) 2 K = X1 \u2217 X2 \u2019 ; 3 X1 row sq = sum(X1. \u02c6 2 , 2) / 2 ; 4 X2 row sq = sum(X2. \u02c6 2 , 2) / 2 ; 5 K = bsxfun (@minus , K, X1 row sq ) ; 6 K = bsxfun (@minus , K, X2 row sq \u2019 ) ; 7 K = K / ( sigma \u02c62) ; 8 K = exp (K) ;\n31\nIf X1 and X2 are respectively n1 \u00d7 d and n2 \u00d7 d matrices, then the output of \u201crbf\u201d is an n1 \u00d7 n2 matrix.\nKernel methods requires forming the n \u00d7 n kernel matrix K whose the (i, j)-th entry is \u03ba(xi,xj). The RBF kernel matrix can be computed by the MATLAB function\n1 K = rb f (X, X, sigma )\nin O(n2d) time. In presence of millions of data points, it is prohibitive to form such a kernel matrix. Fortunately, a sketch of K can be obtained very efficiently. Let S \u2208 Rn\u00d7s be a uniform column selection matrix1 described in Section 3.3, then C = KS can be obtained in O(nsd) time by the following MATLAB code.\n1 f unc t i on [C] = rbfSketch (X, sigma , s ) 2 n = s i z e (X, 1) ; 3 idx = s o r t ( randsample (n , s ) ) ; 4 C = rb f (X, X( idx , : ) , sigma ) ;"}, {"heading": "6.1.2 Matrix Inversion", "text": "Let K be an n\u00d7n kernel matrix, y be an n dimensional vector, and \u03b1 be a positive constant. Kernel methods such as the Gaussian process regression (or the equivalent the kernel ridge regression) and the least squares SVM require solving\n(K + \u03b1In)w = y\nto obtain w \u2208 Rn. The exact solution costs O(n3) time and O(n2) memory. If we have a rank l approximation K \u2248 LLT , then w can be approximately obtained in O(nl2) time and O(nl) memory. Here we need to apply the Sherman-Morrison-Woodbury matrix identity\n(A + BCD)\u22121 = A\u22121 \u2212A\u22121B(C\u22121 + DA\u22121B)\u22121DA\u22121.\nWe expand (LLT + \u03b1In) \u22121 by the above identity and obtain\n(LLT + \u03b1In) \u22121 = \u03b1\u22121In \u2212 \u03b1\u22121L(\u03b1Il + LTL\ufe38 \ufe37\ufe37 \ufe38\nl\u00d7l\n)\u22121LT ,\nand thus w = (K + \u03b1In) \u22121y \u2248 \u03b1\u22121y \u2212 \u03b1\u22121L(\u03b1Il + LTL)\u22121LTy.\nThe matrix inversion problem not only appears in the kernel methods, but also in the second order optimization problems. Newton\u2019s method and the so-called natural gradient\n1The local landmark selection is sometimes a better choice. Do not use random projections, because they inevitably visit every entry of K.\n32\nmethod require computing H\u22121g, where g is the gradient and H is the Hessian matrix or the Fisher information matrix. Since low-rank matrices are not invertible, the naive lowrank approximation H \u2248 CZCT does not work. To make matrix inversion possible, one can use the spectral shifting trick of [30]: fix a small constant \u03b1 > 0, form the low-rank approximation H \u2212 \u03b1In \u2248 CZCT , and compute H\u22121g \u2248 (CZCT + \u03b1In)\u22121g. Besides the low-rank approximation approach, one can approximate H by a block diagonal matrix or even its diagonal, because it is easy to invert a diagonal matrix or a block diagonal matrix."}, {"heading": "6.1.3 Eigenvalue Decomposition", "text": "With the low-rank decomposition K \u2248 LLT at hand, we first approximately decompose K by\nK \u2248 LLT = (UL\u03a3LVTL)(UL\u03a3LVTL)T = UL\u03a32LUTL,\nand then discard the k + 1 to l components in UL and \u03a3L. Here L = UL\u03a3LV T L is the SVD of L, which can be obtained in O(nl2) time and O(nl) memory. In this way, the rank k (k \u2264 rank(L)) eigenvalue decomposition is approximately computed."}, {"heading": "6.2 Prototype Algorithm", "text": "From now on, we will consider how to find the low-rank approximation K \u2248 LLT . As usual, the simplest approach is to form a sketch C = KS \u2208 Rn\u00d7s and solve\nX? = min X \u2016K\u2212CXCT\u20162F = C\u2020K(C\u2020)T or Z? = min Z \u2016K\u2212QCZQC\u20162F = QTCKQC, (6.1) where QC is the orthonormal bases of C computed by SVD or QR decomposition. It is obvious that CX?C = QCZ\n?QTC. In this way, a rank c approximation to K is obtained. This approach is first studied by [14]. Wang et al. [30] showed that if C contains s = O(k/ ) columns of K chosen by adaptive sampling, the error bound\nE\u2016K\u2212QCZ?QTC\u20162F \u2264 (1 + )\u2016K\u2212Kk\u20162F\nis guaranteed. Other sketching methods can also be applied, although currently they do not have 1 + error bound. In the following we implement the prototype algorithm (with the count sketch) in 5 lines of MATLAB code. Since the algorithm goes only two passes through K, when K does not fit in memory, we can store K in the disk and keep one block of K in memory at a time. In this way, O(ns) memory is enough.\n1 f unc t i on [QC, Z ] = spsdPrototype (K, s ) 2 n = s i z e (K, 2) ; 3 C = CountSketch (K, s ) ; 4 [QC, \u02dc ] = qr (C, 0) ; 5 Z = QC\u2019 \u2217 K \u2217 QC;\n33\nAlgorithm 6.1 Faster SPSD Matrix Sketching. 1: Input: an n\u00d7 n matrix K and integers s and p (s \u2264 p n). 2: Draw a column selection matrix S \u2208 Rn\u00d7s; 3: Perform sketching: C = AS; 4: QR decomposition: [QC,RC] = qr(C); 5: Draw a column selection matrix P \u2208 Rn\u00d7p; 6: Compute Z\u0303 = (PTQC) \u2020(PTKP)(QTCP) \u2020;\n7: return QCZ\u0303Q T C \u2248 A.\nDespite its simplicity, the algorithm has several drawbacks.\n\u2022 The time cost of this algorithm is O(ns2 + nnz(K)s), which can be quadratic in n.\n\u2022 The algorithm must visit every entry of K, which can be a serious drawback when applied to kernel methods. It is because computing the kernel matrix K costs O(n2d) time, where d is the dimension of the data points.\nTherefore, we are interested in computing a low-rank approximation in linear time (w.r.t. n) and avoiding visiting every entry of K."}, {"heading": "6.3 Faster SPSD Matrix Sketching", "text": "The readers may have noticed that (6.1) is the problem studied in Section 4.5. We can thus draw a column selection matrix P \u2208 Rn\u00d7p and approximately solve (6.1) by\nZ\u0303 = min Z \u2016PT (K\u2212QCZQC)P\u20162F = (PTQC)\u2020\ufe38 \ufe37\ufe37 \ufe38\ns\u00d7p\n(PTKP)\ufe38 \ufe37\ufe37 \ufe38 p\u00d7p (QTCP) \u2020\ufe38 \ufe37\ufe37 \ufe38 p\u00d7s . (6.2)\nThen we can approximate K by QCZ\u0303Q T C. We describe the faster SPSD matrix sketching in Algorithm 6.1. There are a few things to remark.\n\u2022 Since we are trying to avoid computing every entry of K, we should use uniform sampling or local landmark selection to form C = KS.\n\u2022 Let P \u2208 Rn\u00d7p be a leverage score sampling matrix according to the columns of CT . That is, it samples the i-th column with probability proportional to qi, where qi is the squared `2 norm of the i-th row of QC (for i = 1 to n). When p = O( \u221a ns \u22121/2), the\nfollowing error bounds holds with high probability [31]\n\u2016K\u2212QCZ\u0303QTC\u20162F \u2264 (1 + ) min Z \u2016K\u2212QCZQTC\u20162F .\n\u2022 Let S be a uniform sampling matrix and P be a leverage score sampling matrix. The algorithm visits only ns+ p2 = O(n) entries of K. The overall time and memory costs are linear in n.\n34\n\u2022 Assume S is a column selection matrix. Let the sketch C = KS contains the columns of K indexed by S \u2282 [n], and the columns selected by P are indexed by P \u2282 [n]. Empirically, enforcing S \u2282 P significantly improves the approximation quality.\n\u2022 Empirically, letting p be several times larger than s, e.g. p = 4s, is sufficient for a high quality.\nThe algorithm can be implemented in 12 lines of MATLAB code.\n1 f unc t i on [QC, Z ] = spsdFaster (K, s ) 2 p = 4 \u2217 s ; % can be tuned 3 n = s i z e (K, 2) ; 4 S = s o r t ( randsample (n , s ) ) ; % uniform sampling 5 C = K( : , S ) ; 6 [QC, \u02dc ] = qr (C, 0) ; 7 q = sum(QC. \u02c6 2 , 2) ; % the sampling p r o b a b i l i t y 8 q = q / sum( q ) ; 9 P = randsample (n , p , true , q ) ; % l e v e r a g e s co r e sampling\n10 P = unique ( [P ; S ] ) ; % e n f o r c e P to conta in S 11 PQCinv = pinv (QC(P, : ) ) ; 12 Z = PQCinv \u2217 K(P, P) \u2217 PQCinv \u2019 ;\nThe above implementation assumes that K is a given matrix. In the kernel approximation problems, we are only given a n \u00d7 d matrix X, whose rows are data points, and a kernel function, e.g. the RBF kernel with width parameter \u03c3. We should implement the faster SPSD sketching algorithm in the following way.\n1 f unc t i on [QC, Z ] = spsdFaster (X, sigma , s ) 2 p = 4 \u2217 s ; % can be tuned 3 n = s i z e (X, 1) ; 4 S = s o r t ( randsample (n , s ) ) ; % uniform sampling 5 C = rb f (X, X(S , : ) , sigma ) ; 6 [QC, \u02dc ] = qr (C, 0) ; 7 q = sum(QC. \u02c6 2 , 2) ; % the sampling p r o b a b i l i t y 8 q = q / sum( q ) ; 9 P = randsample (n , p , true , q ) ;\n10 P = unique ( [P ; S ] ) ; % e n f o r c e P conta in s S 11 PQCinv = pinv (QC(P, : ) ) ; 12 Ksub = rb f (X(P, : ) , X(P, : ) , sigma ) ; 13 Z = PQCinv \u2217 Ksub \u2217 PQCinv \u2019 ;\nThe above implementation avoids computing the whole kernel matrix, and is thus highly efficient when applied to kernel methods.\n35"}, {"heading": "6.4 The Nystro\u0308m Method", "text": "Let S be an n\u00d7 s column selection matrix and C = KS \u2208 Rn\u00d7s be a sketch of K. Recall the model (6.2) proposed in the previous section. It is easy to verify that QCZ\u0303Q T C = CX\u0303C\nT , where X\u0303 is defined by\nX\u0303 = min X \u2016PT (K\u2212CXC)P\u20162F = (PTC)\u2020\ufe38 \ufe37\ufe37 \ufe38\ns\u00d7p (PTKP)\ufe38 \ufe37\ufe37 \ufe38 p\u00d7p (CTP)\u2020\ufe38 \ufe37\ufe37 \ufe38 p\u00d7s .\nOne can simply set P = S \u2208 Rn\u00d7s and let W = STC = STKS. Then the solution X\u0303 becomes\nX\u0303 = (STC)\u2020(STKS)(CTS)\u2020 = W\u2020WW\u2020 = W\u2020.\nThe low-rank approximation K \u2248 CW\u2020CT\nis called the Nystro\u0308m method [20; 32]. The Nystro\u0308m method is perhaps the most extensively used kernel approximation approach in the literature. See Figure 6.1 for the illustration of the Nystro\u0308m method.\nThere are a few things to remark:\n\u2022 The Nystro\u0308m is highly efficient. When applied to speedup kernel methods, the scalability can be as large as n = 106.\n\u2022 The Nystro\u0308m method is a rough approximation to K and is well known to be of low accuracy. If a moderately high accuracy is required, one had better use the method in the previous section.\n\u2022 The s \u00d7 s matrix W is usually ill-conditioned, and thus the Moore-Penrose inverse can be numerically instable. (It is because the bottom singular values of W blow up during the Moore-Penrose inverse.) A very effective heuristic is to drop the bottom singular values of W: set a parameter k < s, e.g. k = d0.8se, and approximate K by C(Wk) \u2020CT .\n36\n\u2022 There are many choices of the sampling matrix S. See [13] for more discussions.\nThe Nystro\u0308m method can be implemented in 11 lines of MATLAB code. The output of the algorithm is L \u2208 Rn\u00d7k, where LLT is the Nystro\u0308m approximation to K.2\n1 f unc t i on [ L ] = Nystrom (X, sigma , s ) 2 k = c e i l ( 0 . 8 \u2217 s ) ; % can be tuned 3 n = s i z e (X, 1) ; 4 S = s o r t ( randsample (n , s ) ) ; % uniform sampling 5 C = rb f (X, X(S , : ) , sigma ) ; % C = K( : , S ) 6 W = C(S , : ) ; 7 [UW, SW, \u02dc ] = svd (W) ; 8 SW = diag (SW) ; 9 SW = 1 . / s q r t (SW( 1 : k ) ) ;\n10 UW = bsxfun ( @times , UW( : , 1 : k ) , SW\u2019 ) ; 11 L = C \u2217 UW; % K i s approximated by L \u2217 L \u2019\nHere we use the RBF kernel function implemented in Section 6.1. Line 8 sets k = d0.8ce, which can be better tuned to enhance numerical stability. Notice that k should not be set too small, otherwise the accuracy would be affected."}, {"heading": "6.5 More Efficient Extensions", "text": "Several SPSD matrix approximation methods has been proposed recently, and they are more scalable than the Nystro\u0308m method in certain applications. This section briefly describes some of these methods."}, {"heading": "6.5.1 Memory Efficient Kernel Approximation (MEKA)", "text": "MEKA [24] exploits the block structure of kernel matrices and is more memory efficient than the Nystro\u0308m method. MEKA first partitions the data x1, \u00b7 \u00b7 \u00b7 ,xn into b groups (e.g. by inexact kmeans clustering), accordingly, the kernel matrix K has b\u00d7 b blocks:\nK =  K[1,1] \u00b7 \u00b7 \u00b7 K[1,b]... . . . ... K[b,1] \u00b7 \u00b7 \u00b7 K[b,b]  =  K[1:]... K[b:]  . Then MEKA approximately computes the top left singular vectors of K[1:], \u00b7 \u00b7 \u00b7 ,K[b:], denote U[1], \u00b7 \u00b7 \u00b7 ,U[b], respectively. For each (i, j) \u2208 [b]\u00d7 [b], MEKA finds a very small-scale matrix Z[i,j] by solving\nZ[i,j] = argmin Z \u2225\u2225K[i,j] \u2212U[i]Z[i,j]UT[j]\u2225\u22252F . 2Let Wk = UW,k\u039bW,kU T W,k be the k-eigenvalue decomposition of W and set L = CUW,k\u039b \u22121 W,k \u2208 Rn\u00d7k.\n37\nThis can be done efficiently using the approach in Section 4.5. Finally, the low-rank approximation is\nK \u2248  U[1] 0. . . 0 U[b]   Z[1,1] \u00b7 \u00b7 \u00b7 Z[1,b]... . . . ... Z[b,1] \u00b7 \u00b7 \u00b7 Z[b,b]   U[1] 0. . . 0 U[b]  T = UZUT .\nSince Z and U[1], \u00b7 \u00b7 \u00b7 ,U[b] are small-scale matrices, MEKA is thus very memory efficient. There are several things to remark:\n\u2022 MEKA can be used to speedup Gaussian process regression and least squares SVM. However, MEKA can be hardly applied to speedup k-eigenvalue decomposition, because it requires the k-SVD of UZ1/2, which destroys the sparsity and significantly increases memory cost.\n\u2022 Indiscreet implementation, e.g. the implementation provided by [24], can make MEKA numerically unstable, as was reported by [30; 28]. The readers had better to follow the stabler implementation in [28]."}, {"heading": "6.5.2 Structured Kernel Interpolation (SKI)", "text": "SKI [33] is a memory efficient extension of the Nystro\u0308m method. Let S be a column selection matrix, C = KS, and W = STC = STKS. The Nystro\u0308m method approximates K by CW\u2020CT . SKI further approximates each row of C by a convex combination of two rows of W and obtain C \u2248 XW. Notice that each row of X has only two nonzero entries, which makes X extremely sparse. In this way, K is approximated by\nK \u2248 CW\u2020C \u2248 (XW)W\u2020(XW)T = XWXT .\nMuch accuracy is lost in the second approximation, so SKI is much less accurate than the Nystro\u0308m method. For the same reason as MEKA, there is no point in applying SKI to speedup k-eigenvalue decomposition of K."}, {"heading": "6.6 Extension to Rectangular Matrices: CUR Matrix", "text": "Decomposition\nThis section considers the problem of sketching any rectangular matrix A by the CUR matrix decomposition [16]. The CUR matrix decomposition is an extension of the previously discussed SPSD matrix sketching methods."}, {"heading": "6.6.1 Motivation", "text": "Suppose we are given n training data x1, \u00b7 \u00b7 \u00b7 ,xn \u2208 Rd, m test data x\u20321, \u00b7 \u00b7 \u00b7 ,x\u2032m \u2208 Rd, and a kernel function \u03ba(\u00b7, \u00b7). In their generalization (test) stage, kernel methods such as GPR and\n38\nKPCA form an m \u00d7 n matrix K\u2217, where (K\u2217)ij = \u03ba(x\u2032i,xj), and apply K\u2217 to some vectors or matrices. Notice that it takes O(mnd) time to form K\u2217 and O(mnp) time to multiply K\u2217 by an n \u00d7 p matrix. If m is as large as n, the generalization stage of such kernel methods can be very expensive. Fortunately, with the help of the CUR matrix decomposition, the generalization stage of GPR or KPCA merely costs time linear in m+ n."}, {"heading": "6.6.2 Prototype CUR Decomposition", "text": "Suppose we are given an arbitrary m\u00d7n rectangular matrix A, which can be the aforementioned K\u2217. We sample c columns of A to form C = ASC \u2208 Rm\u00d7c, sample r rows of A to form R = ASR \u2208 Rr\u00d7n, and compute the intersection matrix U? \u2208 Rc\u00d7r by solving\nU? = argmin U \u2016 A\ufe38\ufe37\ufe37\ufe38 m\u00d7n \u2212 C\ufe38\ufe37\ufe37\ufe38 m\u00d7c U\ufe38\ufe37\ufe37\ufe38 c\u00d7r R\ufe38\ufe37\ufe37\ufe38 r\u00d7n \u20162F = C\u2020AR\u2020. (6.3)\nThe approximation A \u2248 CU?R is well known as the CUR decomposition [16]. This formulation bears a strong resemblance with the prototype SPSD matrix sketching method in (6.1).\nThe prototype CUR decomposition is not very useful because (1) its time cost is O(mn \u00b7 min{c, r}) and (2) it visits every entry of A."}, {"heading": "6.6.3 Faster CUR Decomposition", "text": "Analogous to the SPSD matrix sketching, we can compute U? approximately and significantly more efficiently. Let PC \u2208 Rm\u00d7pc and PR \u2208 Rn\u00d7pr be some column selection matrices. Then we solve this problem in stead of (6.3):\nU\u0303 = argmin U \u2016PTCAPR\ufe38 \ufe37\ufe37 \ufe38\npc\u00d7pr \u2212PTCC\ufe38 \ufe37\ufe37 \ufe38 pc\u00d7c U\ufe38\ufe37\ufe37\ufe38 c\u00d7r RPR\ufe38 \ufe37\ufe37 \ufe38 r\u00d7pr \u20162F = (PTCC)\u2020(PTCAPR)(RPR)\u2020. (6.4)\nThe faster CUR decomposition is very similar to the faster SPSD matrix sketching method in Section 6.3. The faster CUR decomposition has the following properties:\n\u2022 It visits only mc+nr+ pcpr entries of A, which is linear in m+n. This is particularly useful when applied to kernel methods, because it avoids forming the whole kernel matrix.\n\u2022 The overall time and memory costs are linear in m+ n.\n\u2022 If PC is the leverage score sampling matrix corresponding to the columns of CT and PR is the leverage score sampling matrix corresponding to the columns of R, then U\u0303 is a very high quality approximation to U? [31]:\n\u2016A\u2212CU\u0303R\u20162F \u2264 (1 + ) min U \u2016A\u2212CUR\u20162F\nholds with high probability.\n39\nEmpirically speaking, setting PC and PR be uniform sampling matrices works nearly as well as leverage score sampling matrices, and setting pc = pr = O(c + r) suffices for a high approximation quality. If A is a full-observed matrix, the CUR matrix decomposition can be computed by the following MATLAB code.\n1 f unc t i on [C, U, R] = curFaste r (A, c , r ) 2 pc = 2 \u2217 ( r + c ) ; % can be tuned 3 pr = 2 \u2217 ( r + c ) ; % can be tuned 4 [m, n ] = s i z e (A) ; 5 SC = s o r t ( randsample (n , c ) ) ; 6 SR = s o r t ( randsample (m, r ) ) ; 7 C = A( : , SC) ; 8 R = A(SR, : ) ; 9 PC = s o r t ( randsample (m, pc ) ) ;\n10 PR = s o r t ( randsample (n , pr ) ) ; 11 PC = unique ( [PC; SR ] ) ; % e n f o r c e PC to conta in SR 12 PR = unique ( [PR; SC ] ) ; % e n f o r c e PR to conta in SC 13 U = pinv (C(PC, : ) ) \u2217 A(PC, PR) \u2217 pinv (R( : , PR) ) ;\nLet\u2019s consider the kernel approximation problem in Section 6.6.1. Let Xtrain \u2208 Rn\u00d7d be the training data and Xtest \u2208 Rm\u00d7d be the test data. We use the RBF kernel with kernel width parameter \u03c3. The m \u00d7 n matrix K\u2217 can be approximated by K\u0303\u2217 = CUR, which is the output of the following MATLAB procedure.\n1 f unc t i on [C, U, R] = curFasterKerne l ( Xtest , Xtrain , sigma , c , r ) 2 pc = 2 \u2217 ( r + c ) ; % can be tuned 3 pr = 2 \u2217 ( r + c ) ; % can be tuned 4 m = s i z e ( Xtest , 1) ; 5 n = s i z e ( Xtrain , 1) ; 6 SC = s o r t ( randsample (n , c ) ) ; 7 SR = s o r t ( randsample (m, r ) ) ; 8 C = rb f ( Xtest , Xtrain (SC, : ) , sigma ) ; 9 R = rb f ( Xtest (SR, : ) , Xtrain , sigma ) ;\n10 PC = s o r t ( randsample (m, pc ) ) ; 11 PR = s o r t ( randsample (n , pr ) ) ; 12 PC = unique ( [PC; SR ] ) ; % e n f o r c e PC to conta in SR 13 PR = unique ( [PR; SC ] ) ; % e n f o r c e PR to conta in SC 14 Kblock = rb f ( Xtest (PC, : ) , Xtrain (PR, : ) , sigma ) ; 15 U = pinv (C(PC, : ) ) \u2217 Kblock \u2217 pinv (R( : , PR) ) ;\nThe time cost of this procedure is linear in m + n, and K\u0303\u2217 = CUR can be applied to n dimensional vector in O ( nr +mc) time. In this way, the generalization of GPR and KPCA can be efficient.\n40"}, {"heading": "6.7 Applications", "text": "This section provides the implementations of kernel PCA, spectral clustering, Gaussian process regression, all sped-up by randomized algorithms."}, {"heading": "6.7.1 Kernel Principal Component Analysis (KPCA)", "text": "Suppose we are given\n\u2022 n training data x1, \u00b7 \u00b7 \u00b7 ,xn \u2208 Rd,\n\u2022 m test data x\u20321, \u00b7 \u00b7 \u00b7 ,x\u2032m \u2208 Rd, (x\u2032i is not the transpose xTi ),\n\u2022 a kernel function \u03ba(\u00b7, \u00b7), e.g. the RBF kernel function,\n\u2022 a target rank k ( n, d).\nThe goal of KPCA is to extract k features of each training datum and each test datum, which may be used in clustering or classification. The standard KPCA consists of the following steps:\n1. Training\n(a) Form the n\u00d7 n kernel matrix K of the training data, whose the (i, j)-th entry is \u03ba(xi,xj);\n(b) Compute the k-eigenvalue decomposition Kk = Uk\u039bkU T k ; (c) Form the n\u00d7 k matrix Uk\u039b1/2k , whose the i-th row is the feature of xi;\n2. Generalization (test)\n(a) Form the m\u00d7 n kernel matrix K\u2217 whose the (i, j)-th entry is \u03ba(x\u2032i,xj); (b) Form the m\u00d7 k matrix K\u2217Uk\u039b\u22121/2k , whose the i-th row is the feature of x\u2032i.\nThe most time and memory expensive step in training is the k-eigenvalue decomposition of K, which can be sped-up by the sketching techniques discussed in this section. Empirically, the faster SPSD matrix sketching in Section 6.3 is much more accurate than the Nystro\u0308m method in Section 6.4, and their time and memory costs are all linear in n. Thus the faster SPSD matrix sketching can be better choice. KPCA can be approximately solved by several lines of MATLAB code.\n1 f unc t i on [U, lambda , f e a t u r e t r a i n ] = kpcaTrain ( Xtrain , sigma , k ) 2 s = k \u2217 10 ; % can be tuned 3 [QC, Z ] = spsdFaster ( Xtrain , sigma , s ) ; % QC has orthogona l columns 4 c l e a r Xtrain 5 [UZ, SZ , \u02dc ] = svd (Z) ; 6 U = QC \u2217 UZ( : , 1 : k ) ; % U conta in s the top k e i g e n v e c t o r s\n41\n7 lambda = diag (SZ) ; 8 lambda = lambda ( 1 : k ) ; % lambda i s the vec to r conta in ing the top k e i g e n v a l u e s 9 f e a t u r e t r a i n = bsxfun ( @times , U, ( s q r t ( lambda ) ) \u2019 ) ;\n10 end\n1 f unc t i on [ f e a t u r e t e s t ] = kpcaTest ( Xtrain , Xtest , sigma , U, lambda ) 2 Ktest = rb f ( Xtest , Xtrain , sigma ) ; 3 U = bsxfun ( @times , U, (1 . / s q r t ( lambda ) ) \u2019 ) ; 4 f e a t u r e t e s t = Ktest \u2217 U; 5 end\nIn the function \u201ckpcaTrain\u201d, the input variable \u201cXtrain\u201d has n rows, each of which corresponds to a training datum. The rows of the output \u201cfeaturetrain\u201d and \u201cfeaturetest\u201d are the features extracted by KPCA, and the features can be used to perform classification. For example, suppose each datum xi is associated with a label yi, and let y = [y1, \u00b7 \u00b7 \u00b7 , yn]T \u2208 Rn. We can use k-nearest-neighbor\n1 [ y t e s t ] = k n n c l a s s i f y ( f e a t u r e t e s t , f e a t u r e t r a i n , y )\nto predict the labels of the test data. When the number of test data m is large, the function \u201ckpcaTest\u201d is costly. The users should apply the CUR decomposition in Section 6.6.3 to speedup computation.\n1 f unc t i on [ f e a t u r e t e s t ] = kpcaTestCUR ( Xtrain , Xtest , sigma , U, lambda ) 2 c = max(100 , c e i l ( s i z e ( Xtrain , 1) / 20) ) ; % can be tuned 3 r = max(100 , c e i l ( s i z e ( Xtest , 1) / 20) ) ; % can be tuned 4 [C, Uti lde , R] = curFasterKerne l ( Xtest , Xtrain , sigma , c , r ) ; 5 U = bsxfun ( @times , U, (1 . / s q r t ( lambda ) ) \u2019 ) ; 6 f e a t u r e t e s t = C \u2217 ( Ut i lde \u2217 (R \u2217 U) ) ; 7 end"}, {"heading": "6.7.2 Spectral Clustering", "text": "Spectral clustering is one of the most popular clustering algorithms. Suppose we are given\n\u2022 n data points x1, \u00b7 \u00b7 \u00b7 ,xn \u2208 Rd,\n\u2022 a kernel function \u03ba(\u00b7, \u00b7),\n\u2022 k: the number of classes.\nSpectral clustering performs the following operations:\n1. Form an n\u00d7 n kernel matrix K, where big kij indicates xi and xj are similar; 2. Form the degree matrix D with dii = \u2211 j kij and dij = 0 for all i 6= j;\n42\n3. Compute the normalized graph Laplacian G = D\u22121/2KD\u22121/2 \u2208 Rn\u00d7n;\n4. Compute the top k eigenvectors of G, denote U \u2208 Rn\u00d7k, and normalize the rows of U;\n5. Apply kmeans clustering on the rows of V to obtain the class labels.\nThe first step costs O(n2d) time and the fourth step costs O(n2k) times, which limit the scalability of spectral clustering. Fowlkes et al. [11] proposed to apply the Nystro\u0308m method to make spectral clustering more scalable by avoiding forming the whole kernel matrix and speeding-up the k-eigenvalue decomposition. Empirically, the algorithm in Section 6.3 is more accurate than the Nystro\u0308m method in Section 6.4, and they both runs in linear time. Spectral clustering with the randomized algorithm in Section 6.3 can be implemented in 16 lines of MATLAB code.\n1 f unc t i on [ l a b e l s ] = S p e c t r a l C l u s t e r i n g F a s t e r (X, sigma , k ) 2 s = k \u2217 10 ; % can be tuned 3 n = s i z e (X, 1) ; 4 [QC, Z ] = spsdFaster (X, sigma , s ) ; % K i s approximated by QC \u2217 Z \u2217 QC\u2019 5 [UZ, SZ , \u02dc ] = svd (Z) ; 6 SZ = s q r t ( diag (SZ) ) ; 7 UZ = bsxfun ( @times , UZ, SZ \u2019 ) ; % now Z = UZ \u2217 UZ\u2019 8 L = QC \u2217 UZ; % now K i s approximated by L \u2217 L \u2019 9 d = ones (n , 1) ;\n10 d = L \u2217 (L \u2019 \u2217 d) ; % diagona l o f the degree matrix D 11 d = 1 . / s q r t (d) ; 12 L = bsxfun ( @times , L , d) ; % now G i s approximated by L\u2217L \u2019 13 [U, \u02dc , \u02dc ] = svd (L , \u2019 econ \u2019 ) ; 14 U = U( : , 1 : k ) ; 15 U = normr (U) ; % normal ize the rows o f U 16 l a b e l s = kmeans (U, k , \u2019 R e p l i c a t e s \u2019 , 3) ;\nWhen the scale of data is too large for the faster SPSD matrix sketching algorithm in Section 6.3, one can instead use the more efficient Nystro\u0308m method in Section 6.4: simply replace Lines 4 to 8 by\n1 L = Nystrom (X, sigma , s ) ;"}, {"heading": "6.7.3 Gaussian Process Regression (GPR)", "text": "The Gaussian process regression (GPR) is one of the most popular machine learning methods. GPR is the foundation of Bayesian optimization and has important applications such as automatically tuning the hyper-parameters of deep neural networks. Suppose we are given\n\u2022 n training data x1, \u00b7 \u00b7 \u00b7 ,xn \u2208 Rd,\n\u2022 labels y = [y1, \u00b7 \u00b7 \u00b7 yn]T \u2208 Rn of the training data,\n\u2022 m test data x\u20321, \u00b7 \u00b7 \u00b7 ,x\u2032m \u2208 Rd, (x\u2032i is not the transpose xTi ),\n43\n\u2022 and a kernel function \u03ba(\u00b7, \u00b7), e.g. the RBF kernel with kernel width parameter \u03c3.\nTraining. In the training stage, GPR requires forming the n \u00d7 n kernel matrix K where kij = \u03ba(xi,xj) and computing the model\nw = (K + \u03b1In) \u22121y.\nHere \u03b1 is a tuning parameter that indicates the noise intensity in the labels y. It takes O(n2d) time to form the kernel matrix and O(n3) time to compute the matrix inversion. To make the training efficient, we can first sketch the SPSD matrix K to obtain K \u2248 LLT and then apply the technique in Section 6.1.2 to obtain w. Empirically, when applied to speedup GPR, the algorithms discussed in Section 6.3 and Section 6.4 has similar accuracy, thus we choose to use the Nystro\u0308m method which is more efficient.\nThe training GPR with the Nystro\u0308m approximation can be implemented in the following MATLAB code. The time cost is O(nl2 + nld) and the space cost is O(nl + nd).\n1 f unc t i on [w] = gprTrain ( Xtrain , ytra in , sigma , alpha ) 2 l = 100 ; % can be tuned 3 L = Nystrom ( Xtrain , sigma , l ) ; % K i s approximated by L \u2217 L \u2019 4 l = s i z e (L , 2) ; 5 w = L \u2019 \u2217 yt ra in ; 6 w = ( alpha \u2217 eye ( l ) + L \u2019 \u2217 L) \\ w; 7 w = yt ra in \u2212 L \u2217 w; 8 w = w / alpha ; 9 end\nThe input \u201csigma\u201d is the kernel width parameter and \u201calpha\u201d indicates the noise intensity in the observation.\nGeneralization (test). After obtaining the trained model w \u2208 Rn, GPR can predict the unknown labels of the m test data x\u20321, \u00b7 \u00b7 \u00b7 ,x\u2032m \u2208 Rd. GPR forms an m\u00d7n kernel matrix K\u2217 whose the (i, j)-th entry is \u03ba(x\u2032i,xj) and compute y\u2217 = K\u2217w \u2208 Rm. The i-th entry in y\u2217 is the predictive label of x\u2032i. The generalization can be implemented in four lines of MATLAB code.\n1 f unc t i on [ y t e s t ] = gprTest ( Xtrain , Xtest , sigma , w) 2 Ktest = rb f ( Xtest , Xtrain , sigma ) ; 3 y t e s t = Ktest \u2217 w; 4 end\nIt costs O(mnd) time to compute K\u2217 and O(mn) time to apply K\u2217 to w. If m is small, the generalization stage can be performed straightforwardly. However, if m is as large as n, the time cost will be quadratic in n, and the user should apply the CUR decomposition in Section 6.6.3 to speedup computation.\n44\n1 f unc t i on [ y t e s t ] = gprTestCUR ( Xtrain , Xtest , sigma , w) 2 c = max(100 , c e i l ( s i z e ( Xtrain , 1) / 20) ) ; % can be tuned 3 r = max(100 , c e i l ( s i z e ( Xtest , 1) / 20) ) ; % can be tuned 4 [C, Uti lde , R] = curFasterKerne l ( Xtest , Xtrain , sigma , c , r ) ; 5 y t e s t = C \u2217 ( Ut i lde \u2217 (R \u2217 w) ) ; 6 end\n45\n46"}, {"heading": "Appendix A", "text": "Several Facts of Matrix Algebra\nThis chapter lists some facts that has been applied in this paper.\nFact A.1. The matrices Q1 \u2208 Rm\u00d7n and Qn\u00d7p (m \u2265 n \u2265 p) have orthonormal columns. Then the matrix Q = Q1Q2 has orthonormal columns.\nFact A.2. Let A be any m\u00d7n and rank \u03c1 matrix. Then AA\u2020B = UAUTAB = AX? = UAZ?, where\nX? = argmin X \u2016B\u2212AX\u20162F , and Z? = argmin Z \u2016B\u2212UAZ\u20162F .\nThis is the reason why AA\u2020B and UAU T AB are called the projection of B onto the column space of A.\nFact A.3. [34, Lemma 44] The matrices Q \u2208 Rm\u00d7s (m \u2265 s) has orthonormal columns. The solution to\nargmin rank(X)\u2264k\n\u2016A\u2212QX\u20162F\nis X? = (QTA)k, where (Q TA)k denotes the closest rank k approximation to Q TA.\nFact A.4. Let A\u2020 be the Moore-Penrose inverse of A. Then AA\u2020A = A and A\u2020AA\u2020 = A\u2020.\nFact A.5. Let A be an m\u00d7n (m \u2265 n) matrix and A = QARA be the QR decomposition of A. Then\nA\u2020\ufe38\ufe37\ufe37\ufe38 n\u00d7m = R\u2020A\ufe38\ufe37\ufe37\ufe38 n\u00d7n QTA\ufe38\ufe37\ufe37\ufe38 n\u00d7m .\nFact A.6. Let C be a full-rank matrix with more rows than columns. Let C = QCRC be the QR decomposition and C = UC\u03a3CVC be the condensed SVD. Then the leverage scores of C, QC, UC are the same.\n47\n48"}, {"heading": "Appendix B", "text": "Notes and Further Reading\nThe `p Regression Problems. Chapter 4 has applied the sketching methods to solve the `2 norm regression problem more efficiently. The more general `p regression problems have also been studied in the literature [5; 7; 17; 8]. Especially, the `1 is of great interest because it demonstrate strong robustness to noise. Currently the strongest result is the `p row sampling by Lewis weights [8].\nDistributed SVD. In the distributed model, each machine holds a subset of columns of A, and the system outputs the top singular values and singular vectors. In this model, the communication cost should also be considered, as well as the time and memory costs. The seminal work [10] proposed to build a coreset to capture the properties of A, which facilitates low computation and communication costs. Later on, several algorithms with stronger error bound and lower communication and computation costs have been proposed. Currently, the state of the art is [2].\nRandom Feature for Kernel Methods. Chapter 6 has introduced the sketching methods for kernel methods. A parallel line of work is the random feature methods [22] which also form low-rank approximations to kernel matrices. Section 6.5.3 of [27] offers simple and elegant proof of a random feature method. Since the sketching methods usually works better than the random feature methods (see the examples in [35]), the users are advised to apply the sketching methods introduced in Chapter 6. Besides the two kinds of low-rank approximation approaches, the stochastic optimization approach [9] also demonstrates very high scalability.\n49\n50"}], "references": [{"title": "Generalized Inverses: Theory and Applications", "author": ["Adi Ben-Israel", "Thomas N.E. Greville"], "venue": "Second Edition. Springer,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Communication-optimal distributed principal component analysis in the column-partition model", "author": ["Christos Boutsidis", "David P Woodruff"], "venue": "arXiv preprint arXiv:1504.06729,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Near-optimal columnbased matrix reconstruction", "author": ["Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail"], "venue": "SIAM Journal on Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Finding frequent items in data streams", "author": ["Moses Charikar", "Kevin Chen", "Martin Farach-Colton"], "venue": "Theoretical Computer Science,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Subgradient and sampling algorithms for l1 regression", "author": ["Kenneth L Clarkson"], "venue": "In Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["Kenneth L. Clarkson", "David P. Woodruff"], "venue": "In Annual ACM Symposium on theory of computing (STOC). ACM,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "The fast cauchy transform and faster robust linear regression", "author": ["Kenneth L Clarkson", "Petros Drineas", "Malik Magdon-Ismail", "Michael W Mahoney", "Xiangrui Meng", "David P Woodruff"], "venue": "In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "`p row sampling by lewis weights", "author": ["Michael B Cohen", "Richard Peng"], "venue": "arXiv preprint arXiv:1412.0588,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Scalable kernel methods via doubly stochastic gradients", "author": ["Bo Dai", "Bo Xie", "Niao He", "Yingyu Liang", "Anant Raj", "Maria-Florina F Balcan", "Le Song"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Turning big data into tiny data: Constant-size coresets for k-means, pca and projective clustering", "author": ["Dan Feldman", "Melanie Schmidt", "Christian Sohler"], "venue": "In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Spectral grouping using the Nystr\u00f6m method", "author": ["Charless Fowlkes", "Serge Belongie", "Fan Chung", "Jitendra Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "The spectral norm error of the naive Nystr\u00f6m extension", "author": ["Alex Gittens"], "venue": "arXiv preprint arXiv:1110.5305,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Revisiting the nystr\u00f6m method for improved large-scale machine learning", "author": ["Alex Gittens", "Michael W. Mahoney"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Nathan Halko", "Per-Gunnar Martinsson", "Joel A. Tropp"], "venue": "SIAM Review,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["William B. Johnson", "Joram Lindenstrauss"], "venue": "Contemporary mathematics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1984}, {"title": "Randomized algorithms for matrices and data", "author": ["Michael W. Mahoney"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Low-distortion subspace embeddings in inputsparsity time and applications to robust linear regression", "author": ["Xiangrui Meng", "Michael W Mahoney"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on theory of computing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Lsrn: A parallel iterative solver for strongly over-or underdetermined systems", "author": ["Xiangrui Meng", "Michael A Saunders", "Michael W Mahoney"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Stronger approximate singular value decomposition via the block Lanczos and power methods", "author": ["Cameron Musco", "Christopher Musco"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "\u00dcber die praktische aufl\u00f6sung von integralgleichungen mit anwendungen auf randwertaufgaben", "author": ["Evert J. Nystr\u00f6m"], "venue": "Acta Mathematica,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1930}, {"title": "Fast and scalable polynomial kernels via explicit feature maps", "author": ["Ninh Pham", "Rasmus Pagh"], "venue": "In the 19th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Random features for large-scale kernel machines. In Advances in neural information processing systems (NIPS)", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Numerical methods for large eigenvalue problems. preparation", "author": ["Yousef Saad"], "venue": "Available from: http://www-users. cs. umn. edu/saad/books. html,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Memory efficient kernel approximation", "author": ["Si Si", "Cho-Jui Hsieh", "Inderjit Dhillon"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Four algorithms for the efficient computation of truncated pivoted QR approximations to a sparse matrix", "author": ["G.W. Stewart"], "venue": "Numerische Mathematik,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1999}, {"title": "Tabulation-based 5-independent hashing with applications to linear probing and second moment estimation", "author": ["Mikkel Thorup", "Yin Zhang"], "venue": "SIAM J. Comput.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "An introduction to matrix concentration inequalities", "author": ["Joel A Tropp"], "venue": "arXiv preprint arXiv:1501.01571,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Structured block basis factorization for scalable kernel matrix evaluation", "author": ["Ruoxi Wang", "Yingzhou Li", "Michael W Mahoney", "Eric Darve"], "venue": "arXiv preprint arXiv:1505.00398,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Improving CUR matrix decomposition and the Nystr\u00f6m approximation via adaptive sampling", "author": ["Shusen Wang", "Zhihua Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Spsd matrix approximation via column selection: Theories, algorithms, and extensions", "author": ["Shusen Wang", "Luo Luo", "Zhihua Zhang"], "venue": "CoRR, abs/1406.5675,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Towards more efficient symmetric matrix sketching and CUR matrix decomposition", "author": ["Shusen Wang", "Zhihua Zhang", "Tong Zhang"], "venue": "arXiv preprint arXiv:1503.08395,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["Christopher Williams", "Matthias Seeger"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2001}, {"title": "Kernel interpolation for scalable structured gaussian processes (kiss-gp)", "author": ["Andrew Gordon Wilson", "Hannes Nickisch"], "venue": "arXiv preprint arXiv:1503.01057,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Sketching as a tool for numerical linear algebra", "author": ["David P Woodruff"], "venue": "arXiv preprint arXiv:1411.4357,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Nystr\u00f6m method vs random fourier features: A theoretical and empirical comparison", "author": ["Tianbao Yang", "Yu-Feng Li", "Mehrdad Mahdavi", "Rong Jin", "Zhi-Hua Zhou"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "Clustered Nystr\u00f6m method for large scale manifold learning and dimension reduction", "author": ["Kai Zhang", "James T. Kwok"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}], "referenceMentions": [{"referenceID": 15, "context": "Mahoney [16] and Woodruff [34] have written excellent but very technical reviews of the randomized algorithms.", "startOffset": 8, "endOffset": 12}, {"referenceID": 33, "context": "Mahoney [16] and Woodruff [34] have written excellent but very technical reviews of the randomized algorithms.", "startOffset": 26, "endOffset": 30}, {"referenceID": 15, "context": "The papers written by Mahoney [16] and Woodruff [34] provide comprehensive and rigorous reviews of the randomized matrix computation algorithms.", "startOffset": 30, "endOffset": 34}, {"referenceID": 33, "context": "The papers written by Mahoney [16] and Woodruff [34] provide comprehensive and rigorous reviews of the randomized matrix computation algorithms.", "startOffset": 48, "endOffset": 52}, {"referenceID": 0, "context": "The book [1] offers a comprehensive study of the pseudo-inverses.", "startOffset": 9, "endOffset": 12}, {"referenceID": 14, "context": "The Gaussian projection is also well knows as the Johnson-Lindenstrauss transform due to the seminal work [15].", "startOffset": 106, "endOffset": 110}, {"referenceID": 2, "context": "When s = k + 1, the low-rank approximation property with \u03b7 = 1 + holds in expectation [3].", "startOffset": 86, "endOffset": 89}, {"referenceID": 35, "context": "Zhang and Kwok [36] proposed to set k = s and run k-means or k-centroids clustering algorithm to cluster the columns of A to s class, and use the s centroids as the sketch of A.", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "The state-of-the-art algorithm [18] is based on very similar idea described in this section.", "startOffset": 31, "endOffset": 35}, {"referenceID": 33, "context": "Woodruff [34] proposed to use sketching to find RA approximately inO(nnz(A)+poly(d)) time.", "startOffset": 9, "endOffset": 13}, {"referenceID": 30, "context": "[31] proposed an algorithm to solve (4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "The experiments in [31] indicates that uniform sampling performs equally well as leverage score sampling.", "startOffset": 19, "endOffset": 23}, {"referenceID": 18, "context": "Here we introduce a simplified version of the block Lanczos method [19] which costs time O(mnkq), where q = log n is the number of iterations, and the inherent constant depends weakly on the spectral gap.", "startOffset": 67, "endOffset": 71}, {"referenceID": 22, "context": "Thus there are many numerical treatments to strengthen stability (see the numerically stable algorithms in [23]).", "startOffset": 107, "endOffset": 111}, {"referenceID": 13, "context": "The algorithm is proposed by [14], and it is described in Algorithm 5.", "startOffset": 29, "endOffset": 33}, {"referenceID": 29, "context": "To make matrix inversion possible, one can use the spectral shifting trick of [30]: fix a small constant \u03b1 > 0, form the low-rank approximation H \u2212 \u03b1In \u2248 CZC , and compute H\u22121g \u2248 (CZC + \u03b1In)g.", "startOffset": 78, "endOffset": 82}, {"referenceID": 13, "context": "This approach is first studied by [14].", "startOffset": 34, "endOffset": 38}, {"referenceID": 29, "context": "[30] showed that if C contains s = O(k/ ) columns of K chosen by adaptive sampling, the error bound", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "When p = O( \u221a ns \u22121/2), the following error bounds holds with high probability [31] \u2016K\u2212QCZ\u0303QC\u2016F \u2264 (1 + ) min Z \u2016K\u2212QCZQC\u2016F .", "startOffset": 79, "endOffset": 83}, {"referenceID": 12, "context": "See [13] for more discussions.", "startOffset": 4, "endOffset": 8}, {"referenceID": 23, "context": "1 Memory Efficient Kernel Approximation (MEKA) MEKA [24] exploits the block structure of kernel matrices and is more memory efficient than the Nystr\u00f6m method.", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "K = \uf8ef\uf8f0 K[1,1] \u00b7 \u00b7 \u00b7 K[1,b] .", "startOffset": 8, "endOffset": 13}, {"referenceID": 0, "context": "K = \uf8ef\uf8f0 K[1,1] \u00b7 \u00b7 \u00b7 K[1,b] .", "startOffset": 8, "endOffset": 13}, {"referenceID": 0, "context": "Then MEKA approximately computes the top left singular vectors of K[1:], \u00b7 \u00b7 \u00b7 ,K[b:], denote U[1], \u00b7 \u00b7 \u00b7 ,U[b], respectively.", "startOffset": 95, "endOffset": 98}, {"referenceID": 0, "context": "K \u2248 \uf8ef\uf8f0 U[1] 0 .", "startOffset": 8, "endOffset": 11}, {"referenceID": 0, "context": "0 U[b] \uf8fa\uf8fb \uf8ef\uf8f0 Z[1,1] \u00b7 \u00b7 \u00b7 Z[1,b] .", "startOffset": 14, "endOffset": 19}, {"referenceID": 0, "context": "0 U[b] \uf8fa\uf8fb \uf8ef\uf8f0 Z[1,1] \u00b7 \u00b7 \u00b7 Z[1,b] .", "startOffset": 14, "endOffset": 19}, {"referenceID": 0, "context": "Z[b,1] \u00b7 \u00b7 \u00b7 Z[b,b] \uf8fa\uf8fb \uf8ef\uf8f0 U[1] 0 .", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "Since Z and U[1], \u00b7 \u00b7 \u00b7 ,U[b] are small-scale matrices, MEKA is thus very memory efficient.", "startOffset": 13, "endOffset": 16}, {"referenceID": 23, "context": "the implementation provided by [24], can make MEKA numerically unstable, as was reported by [30; 28].", "startOffset": 31, "endOffset": 35}, {"referenceID": 27, "context": "The readers had better to follow the stabler implementation in [28].", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "2 Structured Kernel Interpolation (SKI) SKI [33] is a memory efficient extension of the Nystr\u00f6m method.", "startOffset": 44, "endOffset": 48}, {"referenceID": 15, "context": "This section considers the problem of sketching any rectangular matrix A by the CUR matrix decomposition [16].", "startOffset": 105, "endOffset": 109}, {"referenceID": 15, "context": "The approximation A \u2248 CUR is well known as the CUR decomposition [16].", "startOffset": 65, "endOffset": 69}, {"referenceID": 30, "context": "\u2022 If PC is the leverage score sampling matrix corresponding to the columns of C and PR is the leverage score sampling matrix corresponding to the columns of R, then \u0168 is a very high quality approximation to U [31]: \u2016A\u2212C\u0168R\u2016F \u2264 (1 + ) min U \u2016A\u2212CUR\u2016F", "startOffset": 209, "endOffset": 213}, {"referenceID": 10, "context": "[11] proposed to apply the Nystr\u00f6m method to make spectral clustering more scalable by avoiding forming the whole kernel matrix and speeding-up the k-eigenvalue decomposition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Currently the strongest result is the `p row sampling by Lewis weights [8].", "startOffset": 71, "endOffset": 74}, {"referenceID": 9, "context": "The seminal work [10] proposed to build a coreset to capture the properties of A, which facilitates low computation and communication costs.", "startOffset": 17, "endOffset": 21}, {"referenceID": 1, "context": "Currently, the state of the art is [2].", "startOffset": 35, "endOffset": 38}, {"referenceID": 21, "context": "A parallel line of work is the random feature methods [22] which also form low-rank approximations to kernel matrices.", "startOffset": 54, "endOffset": 58}, {"referenceID": 26, "context": "3 of [27] offers simple and elegant proof of a random feature method.", "startOffset": 5, "endOffset": 9}, {"referenceID": 34, "context": "Since the sketching methods usually works better than the random feature methods (see the examples in [35]), the users are advised to apply the sketching methods introduced in Chapter 6.", "startOffset": 102, "endOffset": 106}, {"referenceID": 8, "context": "Besides the two kinds of low-rank approximation approaches, the stochastic optimization approach [9] also demonstrates very high scalability.", "startOffset": 97, "endOffset": 100}], "year": 2015, "abstractText": "1", "creator": "LaTeX with hyperref package"}}}