{"id": "1505.07931", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2015", "title": "Supervised Fine Tuning for Word Embedding with Integrated Knowledge", "abstract": "Learning vector representation for words is an important research field which may benefit many natural language processing tasks. Two limitations exist in nearly all available models, which are the bias caused by the context definition and the lack of knowledge utilization. They are difficult to tackle because these algorithms are essentially unsupervised learning approaches which rely only on learning the meaning of words in real-time. These models have different results, but are not easy to implement.\n\n\nA lot of research has been carried out on these models to test the hypothesis that human learning is biased toward reading words when they are clearly used to represent the meaning of words. However, the data support the hypothesis that the bias is more general and thus more appropriate for learning. For example, for example, for example, a user could use a word as an acronym for a word, which is likely to be used for some words by an English speaker. However, a user could use the word as an acronym for a word, which is likely to be used for some words by an English speaker. Therefore, it is not possible to test this hypothesis by using all of the data to test the hypothesis that human learning is biased towards reading words when they are clearly used to represent the meaning of words.\nThe primary concern is that the problem of the bias can be dealt with in complex situations. As a result, the problem of the bias is often misunderstood due to several possible reasons. First, it is easy to use these models in the same way the model is done. Therefore, the model cannot be taught to avoid the bias that could result from the bias.\nSecond, the problem of the bias is often overlooked because it has the potential to cause a bias in the way that is normally addressed in traditional languages. For example, in traditional languages, an initial word must appear to be represented as a \u2032 (or \u2039) of the following letter. This does not make sense as the word \u2039,\u2039, is actually represented by the symbol \u2033. If a noun becomes \u2032, then the pronoun \u2033 (or \u2039) of the preceding letter may actually be represented by the \u2032.\u2039\nThe fact that the model is not in a correct, \u2032,\u2039-based sense of a letter does not necessarily mean that the spelling of \u2032 is correct. It is also true that in languages like Italian or Japanese, the word \u2033 is usually a \u2033.\u2039, which is more or less a \u2033.\nIt is not uncommon", "histories": [["v1", "Fri, 29 May 2015 06:11:00 GMT  (122kb)", "http://arxiv.org/abs/1505.07931v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xuefeng yang", "kezhi mao"], "accepted": false, "id": "1505.07931"}, "pdf": {"name": "1505.07931.pdf", "metadata": {"source": "CRF", "title": "Supervised Fine Tuning for Word Embedding with Integrated Knowledge", "authors": ["Xuefeng Yang"], "emails": ["yang0302@e.ntu.edu.sg", "ekzmao@ntu.edu.sg"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 5.\n07 93\n1v 1\n[ cs\n.C L\n] 2\n9 M\nLearning a numerical vector to represent the semantic meaning of a word is a research topic of wide interests in computational linguistics. Various applications like information retrieval (Manning et al., 2008), sentiment analysis (Maas et al., 2011) and semantic role labeling (Collobert et al., 2011a) benefit from the vector representation of words. There are two classes of methods for vector representation learning of words, including distributional semantic models (DSMs) and neural language models (word\nembedding). In DSMs, large sparse global cooccurrence matrix representing the context of words is first constructed , dimension reduction techniques such as SVD are then applied to find the low dimension representation of words (Clark, 2012; Turney et al., 2010). Neural language models learn the vector representation of words using artificial neural networks. This method is firstly studied in (Bengio et al., 2003), and many variants have been proposed (Turian et al., 2010; Collobert et al., 2011b; Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014). In neural language methods, the key issue is the formulation of the training target function, minimization or maximization of which may produce meaningful vector representation of words. Ideally, the training target function should reflect the objective of word representation learning, that is the semantic similarity between words represented by distance measures between word vectors is consistent with human cognition. The training target in the above works is to maximise the context prediction ability which is not directly related to the word representations learning objective, therefore they are essentially unsupervised approaches working with a pseudo supervised trick.\nIn both DSMs and neural language models, human defined context for each word is needed. The selection of context may have a strong influence on the word vectors obtained, however, there is no generally best context definition because emphasizing one perspective of context will result in the lack of other valuable information. The examples in Table 1 show that the context length have a impact on the\nlearned word vectors. The three models are trained with the same settings except the size of context window.\nMany research works show that the marriage of unsupervised and supervised learning may achieve better performance. For example, in deep learning, unsupervised feature learning are employed to initialize the neural network to enhance the performance. This inspires the authors to introduce supervised fine tuning framework to the word representation learning, with the goal of addressing the bias problem resulted from context definition.\nHowever, introducing supervised learning into word representation learning is not a trivial issue. The most challenging problem is how to automatically generate labeled data. Manually labelling data is impractical because it is difficult to determine the exact numerical value for semantic similarity, and the size of required training data is very large. Although lexical semantic resources are available, the knowledge graph is not directly applicable for supervised learning. Another challenging problem is how to design a suitable learning algorithm so that the fine tuning will not disrupt too much of the contextbased learning results.\nThe supervised fine tuning learning framework proposed in this paper provides solutions to the above mentioned problems. The core idea behind the proposed framework is to use the ranking of word similarities instead of the exact similarity measure values as the training target. First, an algorithm that automatically generates labeled data based on existing word embeddings and knowledge resources is proposed. Second, an inverse error weighted minibatch stochastic gradient descent optimization algorithm is designed, which can effectively absorb the complementary knowledge to fine tune the word embeddings without disrupting the original geometric\nsimilarity of word embeddings. The remainder of the paper is organized as follows. In Section 2, related work on word representation learning is briefly reviewed. The proposed supervised fine tuning framework is detailed in Section 3. Evaluation and model analysis are given in Section 4. Section 5 concludes the paper."}, {"heading": "1 Related Work", "text": "Vector representation learning for words has received considerable attentions in the past two decades. The methods reported in the literature can be classified into two categories, including distributional semantic models and neural language models.\nDistributional semantic models are based on the principle that words with similar semantic meanings have similar contextual information (Harris, 1954; Firth, 1957). The statistical contextual information is usually summarized into a large sparse matrix, each row of which is the global contextual information of a word in the large corpus, where weighting scheme like tf-idf and PMI are often used to remove the bias of frequency. Dimension reduction techniques like singular value decomposition is then applied to the high dimensional sparse matrix to generate a low dimensional dense matrix with the same number of rows as the original sparse matrix. Each row of the obtained low dimensional matrix is the vector representation of a word.\nThe neural language models are based on artificial neural networks, whose input is the context of a word and the parameters are the vector representation of the words. The parameters of the neural network are adjusted based on the training target and approximation algorithm. A variety of training targets have been proposed. For example, (Bengio et al., 2003) employs sequence of words to predict the next word, (Turian et al., 2010) employs context to predict the middle word, and (Mikolov et al., 2013a) predicts all context given the central word. For the approximation algorithm, hierarchical softmax is borrowed from (Morin and Bengio, 2005) and different methods to construct the hierarchical tree are studied in (Mnih and Hinton, 2009b; Mikolov et al., 2013b), negative sampling series approaches are proposed in (Collobert et al., 2011b;\nMnih and Teh, 2012; Mikolov et al., 2013b). Besides the above data-based neural language models, there are methods that incorporates knowledge into the learning of neural language models. The major training target of these knowledge powered models is still based on the contextual information, and the knowledge is used as auxiliary resources. For example, (Xu et al., 2014) propose a framework to add relational and categorical knowledge as regularization of the original training target, and (Qiu et al., 2014) utilize the morphological knowledge as both additional input representation and auxiliary supervision. (Botha and Blunsom, 2014) trained a compositional morphological vector representation, in which a word vector is the addition of its morphological factor vectors. In spite of the introduction of knowledge into the learning algorithm, these methods still suffer from the bias problem caused by the context definition. (Passos et al., 2014) proposed a lexicon infused word representation learning algorithm for named entity recognition which is built upon the existing skip-gram model.\n(Bansal et al., 2014) employ an ensemble on different word representations which outperforms all individual word representation on the dependency parsing application, this suggests that the complementary information exists in different word representations. However, employing ensemble classifier may dramatically increase the computational burden in prediction because it utilize multiple classifiers. The supervised fine tuning framework proposed in this paper may be treated as a compressed ensemble model, which directly compresses the complementary information into an individual word representation. Instead of training ensemble classifier on all the word embeddings, we may use the complementary information by training an individual classifier on the new word embedding."}, {"heading": "2 Supervised Fine Tuning for Word Representations", "text": ""}, {"heading": "2.1 Overview", "text": "To alleviate the bias problem mentioned above, this study propose a supervised fine tuning framework for word representation learning. The overview of the framework is shown in Figure 1. The proposed\nsupervised fine tuning framework is build upon the established word embedding and lexical semantic resources. There are two parts in the fine tuning framework, including ranking data generation and supervised ranking fine tuning.\nIn the ranking data generation phrase, the ranking information for every word is extracted from each individual word embedding, which is then integrated by a score based multiple criteria fusion algorithm. Human summarized knowledge like WordNet (Miller, 1995) may also be included in the data generation phrase. With the labeled training data obtained above, the supervised ranking learning algorithm may fine tune the original word vectors to encode the complementary knowledge from other word embeddings."}, {"heading": "2.2 Automatic Labeling of Training Data", "text": "As mentioned in introduction, the training target is to learn the semantic similarity ranking instead of similarity measure itself. The goal of training data labeling is to generate the ranking of semantically similar words for each training word."}, {"heading": "2.2.1 Why Ranking ?", "text": "The biggest challenge for supervised word representation fine tuning is the labelling of training data. First, it is difficult to define the exact similarity values between two words. Second, it is impractical to manually label a huge number of training data to support supervised word representation\nlearning. In the previous studies, each method starts from scratch, without using the results of other embeddings. In contrast, our work is built upon existing word embeddings. Thus, labelling of training data can be conducted automatically using the existing word embeddings.\nThe similarity measure is affected by many factors such as the dimensionality of the word vectors, the employed learning algorithms and the corpus size. An example from GloVe embedding (Pennington et al., 2014) may illustrate the phenomenon. The cosine similarity value between words \u201cfish\u201d and \u201csalmon\u201d in Glove embedding 300 dimension version is 0.6596, and the value in Glove embedding 50 dimension version is 0.8340. Although the similarity values are quite different, the word \u201csalmon\u201d is the most similar word to word \u201cfish\u201d in both embeddings. This reveals that the ranking of similarity values is more robust than the similarity values itself. Inspired by this finding, the authors propose to employ ranking information as the supervised training targets"}, {"heading": "2.2.2 Multiple Ranking Integration", "text": "The similarity ranking information can be extracted from existing word embeddings and is used as the training target. But the issue is which word embedding we should use. Even the state-of-art embeddings may not always provide reliable ranking information. Table 2 shows some obvious errors in the related words of words \u201chill\u201d, \u201crun\u201d, and \u201cpaper\u201d in the GloVe word embedding. The words in bold are the errors between the real related words and the number beside the word represents the ranking position. Obviously, errors exist even in the highly frequent words like \u201cpaper\u201d and \u201chill\u201d. To address this problem, the authors propose to enhance ranking information label using multiple word embeddings. The employed multiple word embeddings should be trained with different algorithms, context definition and corpus. The complementary effect of different word embeddings will improve robustness and reliability of the ranking information.\nTo integrate multiple ranking information sources, a score-based multiple criteria fusion algorithm is utilized. Generally, in score-based multiple criteria fusion algorithm, a common used score is defined to measure the credit of each candidate data\nin every criteria. A combination algorithm is then utilized to aggregate the multiple score into one consensus score. Finally a ranking procedure is performed to rank the candidate data based on their consensus scores. In this study, the candidate data is the candidate words to be selected as labeled data, and the score to measure the credit of the candidate words is a normalized cosine similarity measure.\ncredit\u03c9r (\u03c9d) = cosine(\u03c9r, \u03c9d)\nmax{cosine(\u03c9r, \u03c9)|\u03c9 \u2208 V } (1)\nThe score of word \u03c9d in the ranking for word \u03c9r may be obtained by Equation (1), where V represents the whole vocabulary. In Equation (2), the cumulated score for each candidate word in all embeddings is divided by the times the word pair appearing in these embeddings.\nscore\u03c9r(\u03c9d) =\n\u2211 E\u2208Embeds\ncredit\u03c9r E (\u03c9d)\u2211\nE\u2208Embeds I((\u03c9r, \u03c9d) \u2208 E)\n(2)\nWhere the I represents the indicator function, whose value is 1 if the input condition is satisfied. The data with high averaged score is believed to be the trustable knowledge, while the low score data should be the bias or errors."}, {"heading": "2.2.3 Algorithm", "text": "The whole procedure of the labeled data generation and integration is given in Algorithm 11."}, {"heading": "2.2.4 Reduction of Unnecessary Data", "text": "It is impractical to learn the full ranking for each word because the whole similarity matrix is extremely large. The authors find that it is not necessary to employ full ranking as training data for each word. Intuitively, a word is unrelated to most of the\n1The authors\u2019 implementation is based on sparse matrix and nested mapping because the size of matrix is very large.\nAlgorithm 1 Labeled Data Generation and Integration\nInput: VocabList V stores words index in a list 2-D Count Matrix D corresponding to the word in V 2-D Matrix C stores the times of word pair selected 2-D Matrix S stores the times of word pair appearing List of normalized embeddings: embeddings Number of words to extract N data is a dictionary data structure stores the output Initialize D = 0, C = 0 1 Calculate the score of data for all em in embeddings do\nfor all wv in V do (wN , sN ) \u2190 top N of {cosem(wv, wi)|i \u2208 V } smax \u2190max(sN ) for all wi and si in wN , sN do\nD[wv, wi]+ = si/smax C[wv, wi]+ = 1\nend for end for\nend for 2 Remove low score data for all wv1 in V do\nfor all wv2 in V do if C[wv1, wv2] <= 2 then\nD[wv1, wv2] = 0 end if\nend for end for 3 Remove bias of frequency D = D/S hint: point wise division for all wv in V do\ndata[wv]\u2190 select nonzeros in D[wv] and sort end for Output: data stores the ranking of related words\nother words in the vocabulary, the word embeddings should also be consistent with this fact that most of the similarity values should indicate two words are unrelated. Additionally, adjusting whether \u201cpen\u201d or \u201cwater\u201d is more similar to \u201cbasketball\u201d does not make any sense since they are all totally unrelated.\nBased on the analysis above, it is reasonable to only care the meaningful word pairs with strong relations between each other and ignore other unrelated words. Therefore, only the top ranked words in the ranking deserve further fine tuning. Without the consideration of the unrelated words, the data size of the supervised training may be greatly reduced to hundreds per words. In this study, the authors extract the top 200 most similar words as the candidate data for each word from each individual word embedding based on the empirical experience."}, {"heading": "2.3 Supervised Ranking Fine Tuning", "text": ""}, {"heading": "2.3.1 Training Target", "text": "As mentioned in the Introduction, the training target in most neural language models is to maximize the context prediction ability of word embeddings. Different from the previous approachers, the proposed fine tuning framework attempts to adjust the word vectors of a particular word embeddings so that the word similarity ranking is in line with the labeled data. Fitting the ranking of similarity measure may directly alleviate the pain caused by the bias of context, therefore it is a supervised solution to the bias problem caused by context definition. In addition, the ranking of semantic similarity is the most important property of the desired word representation in most tasks. In this sense, it is a supervised solution to the fine tuning of word embeddings.\nTo achieve the above goal, a ranking loss function is employed as the cost function. The ranking loss function Jrank is shown in Equation (3).\nJrank = \u2211\n\u03c9v\u2208V\n\u2211\n\u03c9r\u2208D\u03c9v\n| L\u03c9r \u2212R\u03c9r | 2 (3)\nWhere V is the vocabulary and D\u03c9v is the related data set of word \u03c9v, L and R denote the ranking in the labeled data and the word embedding under study, respectively.\nBecause the ranking loss is not differentiable, the authors choose to minimize the semantic similarity loss between the desired ranking position and the real ranking position. Given the desired ranking position, the similarity value corresponding to the desired ranking position is employed as the real training target. Minimizing the difference of similarity values between desired position and real po-\nsition may also reduce the ranking loss. The similarity value loss function Jsimi is given in Equation 4, where S\u03c9v denotes the sorted similarity values for word \u03c9v\nJsimi= \u2211\n\u03c9v\u2208V\n\u2211\n\u03c9r\u2208D\u03c9v\n|S\u03c9v (L\u03c9r)\u2212S\u03c9v(R\u03c9r) | 2 (4)"}, {"heading": "2.3.2 Inverse error Weighted Mini-batch SGD Optimization", "text": "Stochastic gradient descent(SGD) is a widely used optimization algorithm to minimize the loss function. The iterative update rule for parameters is shown in Equation (5).\n\u03c9 = \u03c9 \u2212 \u03b7 \u00d7 \u2202J(di)\n\u2202\u03c9 (5)\nWhere \u03c9 and \u03b7 denote the parameters and learning rate, and \u2202J(di)\n\u2202\u03c9 is the approximation of gradient\nof the loss function J based on data di. Because the computation of similarity distribution is very expensive, mini-batch SGD is employed to speed up the learning process. In mini-batch SGD, the gradient of the loss function is approximated by a batch of data, which is shown in Equation (6).\ngbatch = 1\nN\n\u2211\ni\u2208batch\n\u2202J(di)\n\u2202\u03c9\n\u03c9 = \u03c9 \u2212 \u03b7 \u00d7 gbatch\n(6)\nWhere N is the size of batch and gbatch denotes the batch gradient. However, the direct applying mini-batch SGD encounters a serious problem: the training loss is reduced, but the performance on all tasks deteriorates. This is quite similar to the overfitting problem in which the models learn the noise of the training data and perform badly on unknown testing dateset. However, there are two differences between this problem and overfitting. The overfitting phenomenon occurs at the last phase and the model generally should have more parameters than necessary, this generalization decreasing problem happens at the start of the training and it exists even the size of parameters is very small.\nAfter exploration of the learning process, the authors find that the reason behind this problem is that the external knowledge of ranking information may disrupt the geometric similarity of the original word\nembedding under the standard mini-batch SGD optimization algorithm. The existing word embeddings are sill far from perfect, and some strongly related words in reality may be ignored, which may be caused by many factors such as polysemy, corpus bias and imbalanced word frequency. Such words may have very large ranking errors, then the gradient of these words become very large because the magnitude of the gradient is proportional to the error. Finally, the gradient of these words dominates the overall gradient and disrupt the original learned geometric similarity.\nIn this fine tuning framework, the principle that the data with larger error deserves more learning is not applicable, the authors propose an inverse error weighted mini-batch SGD optimization algorithm to remove the effect of the large errors.\ngbatch = 1\nN\n\u2211\ni\u2208batch\n1\ne(di)\n\u2202J(di)\n\u2202\u03c9 (7)\nEquation 7 shows the inverse error weighted gradient, where e(di) denotes the error of data di.\nBesides the update from the labeled data, the algorithm also deals with the data that are in the meaningful range but not covered by the labeled data. These data are close to the trained word in the word embedding, but actually not related to the word in reality. These data are also processed by the proposed inverse error weighted mini-batch SGD, but the errors are not included in the cost function.\nTo determine the range of meaningful semantic similarity in original word embeddings, a threshold based on random similarity distribution is utilized. The idea behind this is that the similarity values which are difficult to be randomly generated are the real knowledge the word embedding learned. To calculate the threshold for specific word embedding, a random matrix following uniformly distribution and having the same shape with the word embedding is generated, then the mean of all the similarity values in the top 5 ranking for all words is calculated as the random threshold. To speed up the generation process, a sample may be used to approximate the whole distribution. The similarity values above the random threshold have a large probability to be meaningful data.\nThe detail of the updating rules is provided in Al-\ngorithm 2.\nAlgorithm 2 Ranking Learning Algorithm Input: vocabList V stores words index in a list and em is the initial embedding data is a two tier nested mapping stores all training data datawv and rankwv are mappings between words and their ranking for wv in supervised label and trained embedding respectively ul is a list stores all the local update for word wv update is the global update vector for word wv \u03b4 is the random threshold and \u03c3 is the learning rate Initialize error = len(V )\u00d7 d, stop = False while stop != True do\nfor all wv in V do datawv \u2190 data[wv ] rankwv \u2190 get ranking of all words for wv ul\u2190 \u00f8 for all Wd in datawv do\nsign\u2190 I(rankwv [wd]\u2212 datawv [wd]) ul.add(sign \u00d7 cos(wv, wd) \u00d7 em[wd])\nend for newv \u2190 select rankwv if cosine>\u03b4 and not in datawv for all Wd in newv do\nul.add(\u22121 \u00d7 cos(wv, wd) \u00d7 em[wd]) end for update\u2190 mean of vectors in ul update\u2190 normalization(update) em[wv] = em[wv] + \u03c3 \u00d7 update em[wv] = normalization(em[wv ])\nend for end while"}, {"heading": "2.4 Stopping Criterion and Overfitting", "text": "Overfitting is a frequently encountered problem in machine learning. Generally, overfitting is related to the number of parameters. If the size of parameters is larger than the potential patterns in the observations, the model may suffer from the overfitting problem. The overfitting problem is more serious in this study because the automatically generated labeled data may contain some noise and confliction. The widely employed stopping criterion is given in Equation (8).\nJrank(i)\u2212 Jrank(i+ 1)\nJrank(i) <= \u01eb (8)\nWhere Jrank(i) is the overall ranking loss in the ith epoch, which is the same as Jrank in Equation (3), and\u01eb is a very small value like 0.01. If the relative improvement is smaller than the predefined value \u01eb, the training should stop. However, the authors find that the criterion may not generalize well for the involved embeddings and tasks, even \u01eb is changed according to the number of parameters. The authors employ an alternative stopping criterion, which is related to the initial performance of the word embedding. Equation (9) shows the criterion, and the Jsimi(0) is related to the performance of the embeddings on various tasks. With this criterion, the \u01eb selected according to the dimensionality of word embeddings and the initial performance may work better. This may be explained by the fact that the quality of different embeddings are quite different, the poorly performed models may have more space to grow and the models with good initial status are easier to fall into overfitting.\nJrank(i)\u2212 Jrank(i+ 1)\nJrank(0) <= \u01eb (9)"}, {"heading": "3 Experiment", "text": "To evaluate the proposed supervised fine tuning framework, three groups of experiments are designed.\nThe intrinsic and extrinsic evaluations experiments are conducted to test the effect of fine tuning. 6 word embeddings are further trained by the proposed supervised fine tuning framework and evaluated with 4 tasks. The baselines are the original word embeddings which includes many reputable word embeddings such as SENNA, Word2vec and GloVe.\nThe effect of inverse error weighted mini-batch SGD optimization algorithm is studied in the second group experiments. Two comparison groups are involving, one uses the widely employed standard optimization algorithm and another utilizes the proposed inverse error weighted variant. The comparison uses 6 embeddings and 3 tasks with 3 classical datasets, except the updating rules, all the other settings are the same for two comparison groups.\nThe influence of data size on the model is studied in the last group experiments with two best embeddings and 8 datasets, and the hypothesis about the reduced data size is tested."}, {"heading": "3.1 Evaluation Methods", "text": ""}, {"heading": "3.1.1 Semantic Similarity Prediction", "text": "Measuring the consistency between machine predicted similarity values and human annotated similarity values is the most widely employed task to evaluate the quality of word embeddings. The datasets are usually constructed from crowdsourcing cognitive experiments\nMany datasets are available, the authors select 5 of them to cover of different perspectives and data size. Wordsim353 (Finkelstein et al., 2001) and RG65 (Rubenstein and Goodenough, 1965) are the two most used dataset for semantic similarity evaluation. MEN3000 (Bruni et al., 2014) and Mturk771 (Halawi et al., 2012) are two recently generated large datasets. YP130 dataset (Yang and Powers, 2006) is designed to evaluate semantic similarity between verbs. As in other works, spearman correlation is used to measure the consistency between human annotation and machine prediction."}, {"heading": "3.1.2 Analogical Reasoning", "text": "The analogical reasoning task is designed to test the ability of models for relational similarity tasks. The Google analogical reasoning dataset is introduced by (Mikolov et al., 2013a). The question is in the following format, Man : Women = King : ?, the answer should be the exact word \u201cQueen\u201d. The Microsoft analogical reasoning data set (Mikolov et al., 2013c) is also utilized in this study. The major difference between these two datesets is the different types of relations they focus on.\nAs in other works, the nearest neighbour of the vector(Women + King - Man) exclusive of the three words in questions is selected as the candidate answer. If the candidate word is the same as the answer, the question is correctly solved. In this study, the overall accuracy is employed to evaluate the word embedding. Since the size of the learned word embedding is not as large as the one used in original study (Mikolov et al., 2013a), the question with unknown word is not included in the experiment."}, {"heading": "3.1.3 Sentence Completion", "text": "The sentence completion challenge (Zweig and Burges, 2012) is intended to stimulate research in the area of semantic modeling. The sentence completion questions are to select words which are meaningful and coherent in the the context of a complete sentence. In each sentence, an infrequent word is chosen as the focus of the question and four alternates candidates were chosen from a list of words suggested by an N-gram language model as disturbance. Only the original word is considered as the correct answer to the question.\nIn this study, the author select the candidate answer based on the average similarity values between the candidates and all the words in the sentence. The word with largest average value is selected as the answer. This is the widely employed approach when this dataset is utilized to evaluate the word embeddings."}, {"heading": "3.1.4 Sentiment Analysis", "text": "Two sentence level sentiment classification datasets are used in this task to do the extrinsic evaluation. The customer reviews dataset (Hu and Liu, 2004) contains the reviews of 5 digital products from Amazon web site, and movie review dataset (Pang and Lee, 2005) includes 5331 positive and 5331 negative processed sentences from rotten tomatoes movie review web site. In both datasets, the review sentence should be classified as positive or negative attitude.\nIn this study, the author employ the convolution neural network (CNN) as described in (Kim, 2014) to do the sentence level sentiment classification."}, {"heading": "3.2 Setting and Details", "text": "6 word embeddings are studied in our experiments, they are HLBL50 which is proposed in (Mnih and Hinton, 2009a), SENNA50 (Collobert et al., 2011b), RNNLM640 (Mikolov et al., 2011), GloVe300 (Pennington et al., 2014), DocAndDep2000 (Fyshe et al., 2013), and Word2vec (Mikolov et al., 2013b). All embeddings are available in the websites2 except the Word2vec\n2Senna:http://ml.nec-labs.com/senna/ RNNLM:http://www.fit.vutbr.cz/ imikolov/rnnlm/\nembeddings. The authors train the Word2vec model by the public available Word2vec toolkit using the data from the 1 billion word language modeling benchmark dataset. Hierarchical softmax approximation algorithm and skip gram structure are employed, where the window size is set to 7 and the mini count of words is set to 10. DocAndDep models are constructed from two models which are document model and dependency model, the authors utilize them separately in this study, the document models is not further trained because the initial performance has a large gap with other embeddings.\nFor human summarized knowledge resources, WordNet is employed in this experiment. The directly connected words and the words in the same synsets are extracted as external knowledge. In the integration process, the weights of all relations from wordnet are treated equally as 1.\nThe learning rate \u03c3 is set to 0.1 in most experiments. This is for the fast convergency because the computation of cosine similarity matrix in each epoch is very expensive. To avoid overfitting, the stopping criterion is set based on the embeddings\u2019 dimensionality and original performance, the values of \u01eb for different embeddings is given in Table 3\nThe hyper-parameter setting for convolution neural network used in sentiment analysis follows the default setting utilized in (Kim, 2014)"}, {"heading": "3.3 Result and Analysis", "text": ""}, {"heading": "3.3.1 Intrinsic and Extrinsic Evaluation", "text": "The performance of the proposed supervised fine tuning framework is detailed in Table 4. All word embeddings are significantly enhanced after the supervised fine tuning. The performance of HLBL50 embedding is similar to the SENNA50 and Skip50\nHLBL:http://metaoptimize.com/projects/wordreprs/ Glove:http://nlp.stanford.edu/projects/glove/ DocAndDep:http://www.cs.cmu.edu/ afyshe/papers/conll2013/\nembeddings, although the initial performance of HLBL50 is not satisfactory. The performance of the best word embedding Glove300 in this experiment is also significantly improved in all datasets. These remarkable improvements may demonstrate that the supervised fine tuning framework may transfer the complementary knowledge from the weak embeddings and lexical semantic resources into the strong embeddings.\nIn the perspective of evaluation tasks, the result data supports the conclusion that this supervised fine tuning framework is effective for all the involved tasks. Due to the training label is strongly related to the similarity prediction tasks, the performance for all 5 datasets are significantly improved. The improvement for the analogical reasoning task is not as large as the similarity prediction task, but also remarkable. An interesting finding is that the performance of sentence completion task forms two groups, the word embeddings in high dimension obtain more significant improvement than the low dimensional word embeddings. The authors believe that this is caused by both the learning capability difference and overfitting.\nTable 5 shows the result for sentiment analysis task. No matter the word vectors are directly used as the features or as an initialization of feature parameters, the fine tuned GloVe word embedding outperform the original one for both datasets. The may reveal that the fine tuned word embeddings have better separation capability than the original embeddings for sentiment analysis tasks."}, {"heading": "3.3.2 Comparison of Standard SGD and Inverse Error Weighted SGD", "text": "In the second experiment, the standard mini-batch SGD and the inverse error weighted mini-batch SGD are compared. The learning curve of both approachers for all embeddings on three tasks are shown in Figure 2, 3 and 4, respectively.\nIt is shown that nearly all the solid lines represent-\nFigure 2: Comparison of standard SGD and inverse error weighted SGD on M3K similarity prediction dataset..\n0 1 2 3 4 5 epoch\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nsc o re\nGoogle Analogical Reasoning Dataset\nSENNA50_eu Skip50_eu HLBL50_eu GloVe300_eu RNNLM640_eu Dep1000_eu SENNA50_et Skip50_et HLBL50_et GloVe300_et RNNLM640_et Dep1000_et\nFigure 3: Comparison of standard SGD and inverse error weighted SGD on google analogical reasoning dataset.\ning standard mini-batch SGD in these three figures have a trend of decreasing, which means the standard mini-batch SGD does not help, but decreases\nthe performance. In contrast, the performance of inverse error weighted approacher represented by dash lines have a general trend of increasing. This experiment demonstrates the phenomenon described above that the data with large error is too difficult to learn and may disrupt the original geometric similarity. With the obvious large performance gap in the comparison, the conclusion may be obtained that the inverse error weighted mini-batch SGD optimization algorithm is suitable for supervised fine tuning learning of word representation."}, {"heading": "3.3.3 Effect of Training Data Size", "text": "Table 6 show the effect of different size of training data. Obviously, the performance of the analogical reasoning tasks is improved slightly with the increase of training data size. However, the performance for similarity prediction tasks reveals two diverse paths, the small datasets like RG65 and YP130\nprefer small size of training data, and the performance of large datasets like M3K and Mturk771 is not affected by the size of training data.\nGenerally, the increase of training data size is only slightly helpful to analogical reasoning tasks, so it empirically proves the hypothesis that employing only top related data is enough to support this supervised fine tuning."}, {"heading": "4 Conclusion", "text": "A supervised fine tuning framework is proposed to boost the unsupervised word representation learning algorithms. The proposed automatic label generation approach and the inverse error weighted minibatch SGD optimization algorithm make it possible to provide additional supervised fine tuning phase for all unsupervised word representation learning algorithms. Many experiments involving 10 datasets and 6 well trained embeddings empirically demonstrated that the framework is very effective for improving the quality of word representation."}], "references": [{"title": "Tailoring continuous word representations for dependency parsing", "author": ["Bansal et al.2014] Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Bansal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Compositional Morphology for Word", "author": ["Botha", "Blunsom2014] Jan A. Botha", "Phil Blunsom"], "venue": null, "citeRegEx": "Botha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Botha et al\\.", "year": 2014}, {"title": "Multimodal distributional semantics", "author": ["Bruni et al.2014] Elia Bruni", "Nam-Khanh Tran", "Marco Baroni"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Vector space models of lexical meaning. Handbook of Contemporary Semantics, Wiley-Blackwell, \u00e0 para\u0131\u0302tre", "author": ["Stephen Clark"], "venue": null, "citeRegEx": "Clark.,? \\Q2012\\E", "shortCiteRegEx": "Clark.", "year": 2012}, {"title": "Natural language processing (almost) from scratch", "author": ["J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Placing search in context: The concept revisited", "author": ["Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th international conference on", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "A synopsis of linguistic theory 1930-55", "author": ["J.R. Firth"], "venue": null, "citeRegEx": "Firth.,? \\Q1957\\E", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "Documents and dependencies: an exploration of vector space models for semantic composition", "author": ["Fyshe et al.2013] Alona Fyshe", "Partha Talukdar", "Brian Murphy", "Tom Mitchell"], "venue": "In Proceedings of the Seventeenth Conference on Computational Natural Lan-", "citeRegEx": "Fyshe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fyshe et al\\.", "year": 2013}, {"title": "Mining and summarizing customer reviews", "author": ["Hu", "Liu2004] Minqing Hu", "Bing Liu"], "venue": "In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "citeRegEx": "Hu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2004}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Learning word vectors for sentiment analysis", "author": ["Maas et al.2011] Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Introduction to information retrieval, volume 1", "author": ["Prabhakar Raghavan", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Extensions of recurrent neural network language model", "author": ["Stefan Kombrink", "Lukas Burget", "JH Cernocky", "Sanjeev Khudanpur"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics:", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM,", "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2009a] Andriy Mnih", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2009b] Andriy Mnih", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Mnih", "Teh2012] Andriy Mnih", "Yee Whye Teh"], "venue": "Proceedings of the International Conference on Machine Learning", "citeRegEx": "Mnih et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2012}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Bengio2005] Frederic Morin", "Yoshua Bengio"], "venue": "In AISTATS05,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Pang", "Lee2005] Bo Pang", "Lillian Lee"], "venue": "In Proceedings of ACL,", "citeRegEx": "Pang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2005}, {"title": "Lexicon infused phrase embeddings for named entity resolution", "author": ["Vineet Kumar", "Andrew McCallum"], "venue": "In Proceedings of the Eighteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Passos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Passos et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Co-learning of word representations and morpheme representations", "author": ["Qiu et al.2014] Siyu Qiu", "Qing Cui", "Jiang Bian", "Bin Gao", "Tie-Yan Liu"], "venue": "In Proceedings of COLING", "citeRegEx": "Qiu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qiu et al\\.", "year": 2014}, {"title": "Contextual correlates of synonymy", "author": ["Rubenstein", "John B. Goodenough"], "venue": "Commun. ACM,", "citeRegEx": "Rubenstein et al\\.,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein et al\\.", "year": 1965}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Patrick Pantel"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Turney and Pantel,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel", "year": 2010}, {"title": "Verb similarity on the taxonomy", "author": ["M.W. Powers"], "venue": null, "citeRegEx": "Powers.,? \\Q2006\\E", "shortCiteRegEx": "Powers.", "year": 2006}, {"title": "A challenge set for advancing language modeling", "author": ["Burges."], "venue": "Proceedings of the NAACL-HLT 2012", "citeRegEx": "Burges.,? 2012", "shortCiteRegEx": "Burges.", "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "Various applications like information retrieval (Manning et al., 2008), sentiment analysis (Maas et al.", "startOffset": 48, "endOffset": 70}, {"referenceID": 12, "context": ", 2008), sentiment analysis (Maas et al., 2011) and semantic role labeling (Collobert et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 4, "context": "In DSMs, large sparse global cooccurrence matrix representing the context of words is first constructed , dimension reduction techniques such as SVD are then applied to find the low dimension representation of words (Clark, 2012; Turney et al., 2010).", "startOffset": 216, "endOffset": 250}, {"referenceID": 1, "context": "This method is firstly studied in (Bengio et al., 2003), and many variants have been proposed (Turian et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 28, "context": ", 2003), and many variants have been proposed (Turian et al., 2010; Collobert et al., 2011b; Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014).", "startOffset": 46, "endOffset": 163}, {"referenceID": 25, "context": ", 2003), and many variants have been proposed (Turian et al., 2010; Collobert et al., 2011b; Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014).", "startOffset": 46, "endOffset": 163}, {"referenceID": 8, "context": "Distributional semantic models are based on the principle that words with similar semantic meanings have similar contextual information (Harris, 1954; Firth, 1957).", "startOffset": 136, "endOffset": 163}, {"referenceID": 1, "context": "For example, (Bengio et al., 2003) employs sequence of words to predict the next word, (Turian et al.", "startOffset": 13, "endOffset": 34}, {"referenceID": 28, "context": ", 2003) employs sequence of words to predict the next word, (Turian et al., 2010) employs context to predict the middle word, and (Mikolov et al.", "startOffset": 60, "endOffset": 81}, {"referenceID": 26, "context": ", 2014) propose a framework to add relational and categorical knowledge as regularization of the original training target, and (Qiu et al., 2014) utilize the morphological knowledge as both additional input representation and auxiliary supervision.", "startOffset": 127, "endOffset": 145}, {"referenceID": 24, "context": "(Passos et al., 2014) proposed a lexicon infused word representation learning algorithm for named entity recognition which is built upon the existing skip-gram model.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "(Bansal et al., 2014) employ an ensemble on different word representations which outperforms all individual word representation on the dependency parsing application, this suggests that the complementary information exists in different word representations.", "startOffset": 0, "endOffset": 21}, {"referenceID": 18, "context": "Human summarized knowledge like WordNet (Miller, 1995) may also be included in the data generation phrase.", "startOffset": 40, "endOffset": 54}, {"referenceID": 25, "context": "An example from GloVe embedding (Pennington et al., 2014) may illustrate the phenomenon.", "startOffset": 32, "endOffset": 57}, {"referenceID": 7, "context": "Wordsim353 (Finkelstein et al., 2001) and RG65 (Rubenstein and Goodenough, 1965) are the two most used dataset for semantic similarity evaluation.", "startOffset": 11, "endOffset": 37}, {"referenceID": 3, "context": "MEN3000 (Bruni et al., 2014) and Mturk771 (Halawi et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 11, "context": "In this study, the author employ the convolution neural network (CNN) as described in (Kim, 2014) to do the sentence level sentiment classification.", "startOffset": 86, "endOffset": 97}, {"referenceID": 14, "context": ", 2011b), RNNLM640 (Mikolov et al., 2011), GloVe300 (Pennington et al.", "startOffset": 19, "endOffset": 41}, {"referenceID": 25, "context": ", 2011), GloVe300 (Pennington et al., 2014), DocAndDep2000 (Fyshe et al.", "startOffset": 18, "endOffset": 43}, {"referenceID": 9, "context": ", 2014), DocAndDep2000 (Fyshe et al., 2013), and Word2vec (Mikolov et al.", "startOffset": 23, "endOffset": 43}, {"referenceID": 11, "context": "The hyper-parameter setting for convolution neural network used in sentiment analysis follows the default setting utilized in (Kim, 2014)", "startOffset": 126, "endOffset": 137}], "year": 2015, "abstractText": "Learning vector representation for words is an important research field which may benefit many natural language processing tasks. Two limitations exist in nearly all available models, which are the bias caused by the context definition and the lack of knowledge utilization. They are difficult to tackle because these algorithms are essentially unsupervised learning approaches. Inspired by deep learning, the authors propose a supervised framework for learning vector representation of words to provide additional supervised fine tuning after unsupervised learning. The framework is knowledge rich approacher and compatible with any numerical vectors word representation. The authors perform both intrinsic evaluation like attributional and relational similarity prediction and extrinsic evaluations like the sentence completion and sentiment analysis. Experiments results on 6 embeddings and 4 tasks with 10 datasets show that the proposed fine tuning framework may significantly improve the quality of the vector representation of words. Learning a numerical vector to represent the semantic meaning of a word is a research topic of wide interests in computational linguistics. Various applications like information retrieval (Manning et al., 2008), sentiment analysis (Maas et al., 2011) and semantic role labeling (Collobert et al., 2011a) benefit from the vector representation of words. There are two classes of methods for vector representation learning of words, including distributional semantic models (DSMs) and neural language models (word embedding). In DSMs, large sparse global cooccurrence matrix representing the context of words is first constructed , dimension reduction techniques such as SVD are then applied to find the low dimension representation of words (Clark, 2012; Turney et al., 2010). Neural language models learn the vector representation of words using artificial neural networks. This method is firstly studied in (Bengio et al., 2003), and many variants have been proposed (Turian et al., 2010; Collobert et al., 2011b; Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014). In neural language methods, the key issue is the formulation of the training target function, minimization or maximization of which may produce meaningful vector representation of words. Ideally, the training target function should reflect the objective of word representation learning, that is the semantic similarity between words represented by distance measures between word vectors is consistent with human cognition. The training target in the above works is to maximise the context prediction ability which is not directly related to the word representations learning objective, therefore they are essentially unsupervised approaches working with a pseudo supervised trick. In both DSMs and neural language models, human defined context for each word is needed. The selection of context may have a strong influence on the word vectors obtained, however, there is no generally best context definition because emphasizing one perspective of context will result in the lack of other valuable information. The examples in Table 1 show that the context length have a impact on the learned word vectors. The three models are trained with the same settings except the size of context window. Table 1: Top 5 nearest neighbours for \u201csnake\u201d. window size 5 window size 7 window size 9 tarantula cobra cobra venomous tarantula lizard frog nonvenomous tarantula rattlesnake spider frog lizard python porcupine Many research works show that the marriage of unsupervised and supervised learning may achieve better performance. For example, in deep learning, unsupervised feature learning are employed to initialize the neural network to enhance the performance. This inspires the authors to introduce supervised fine tuning framework to the word representation learning, with the goal of addressing the bias problem resulted from context definition. However, introducing supervised learning into word representation learning is not a trivial issue. The most challenging problem is how to automatically generate labeled data. Manually labelling data is impractical because it is difficult to determine the exact numerical value for semantic similarity, and the size of required training data is very large. Although lexical semantic resources are available, the knowledge graph is not directly applicable for supervised learning. Another challenging problem is how to design a suitable learning algorithm so that the fine tuning will not disrupt too much of the contextbased learning results. The supervised fine tuning learning framework proposed in this paper provides solutions to the above mentioned problems. The core idea behind the proposed framework is to use the ranking of word similarities instead of the exact similarity measure values as the training target. First, an algorithm that automatically generates labeled data based on existing word embeddings and knowledge resources is proposed. Second, an inverse error weighted minibatch stochastic gradient descent optimization algorithm is designed, which can effectively absorb the complementary knowledge to fine tune the word embeddings without disrupting the original geometric similarity of word embeddings. The remainder of the paper is organized as follows. In Section 2, related work on word representation learning is briefly reviewed. The proposed supervised fine tuning framework is detailed in Section 3. Evaluation and model analysis are given in Section 4. Section 5 concludes the paper.", "creator": "LaTeX with hyperref package"}}}