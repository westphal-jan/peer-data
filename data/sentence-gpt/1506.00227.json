{"id": "1506.00227", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2015", "title": "Parallel Spectral Clustering Algorithm Based on Hadoop", "abstract": "Spectral clustering and cloud computing is emerging branch of computer science or related discipline. It overcome the shortcomings of some traditional clustering algorithm and guarantee the convergence to the optimal solution, thus have to the widespread attention. This article first introduced the parallel spectral clustering algorithm research background and significance, and then to Hadoop the cloud computing Framework has carried on the detailed introduction, then has carried on the related to spectral clustering is introduced, then introduces the spectral clustering arithmetic Method of parallel and relevant steps, finally made the related experiments, and the experiment are summarized.\n\n\n\nA group of two groups of four (or more or less) is called a cluster. The group consists of clusters with more than 30 (0.9%) or more than 90 (0.8%) cluster members. The group consists of two groups of two (or more or less) and more than 90 (0.7%) cluster members. The group consists of two groups of three (or more) and more than 90 (0.6%) cluster members. The group consists of two groups of two (or more) and more than 90 (0.5%) cluster members. The group consists of two groups of two (or more) and more than 90 (0.4%) cluster members. The group consists of two groups of two (or more) and more than 90 (0.4%) cluster members. The group consists of two groups of two (or more) and more than 90 (0.4%) cluster members. The group consists of two groups of two (or more) and more than 90 (0.4%) cluster members. The group consists of two groups of two (or more) and more than 90 (0.4%) cluster members. The group consists of two groups of two (or more) and more than 90 (0.4%) cluster members. The group consists of two groups of two (or more) and more than 90 (0.4%) cluster members. The group consists of two groups of two (or more) and more than 90 (0.4%) cluster members. The group consists of two groups of two (or more) and more than 90 (0.4%) cluster members. The group consists of two groups of two (or more) and more than 90 (0.4%) cluster members. The group consists of two groups of two (or more) and more than 90 (0.4%) cluster members. The group consists of two groups of two (or more)", "histories": [["v1", "Sun, 31 May 2015 13:39:41 GMT  (993kb)", "http://arxiv.org/abs/1506.00227v1", null]], "reviews": [], "SUBJECTS": "cs.DC cs.DS cs.LG", "authors": ["yajun cui", "yang zhao", "kafei xiao", "chenglong zhang", "lei wang"], "accepted": false, "id": "1506.00227"}, "pdf": {"name": "1506.00227.pdf", "metadata": {"source": "META", "title": "Parallel Spectral Clustering Algorithm Based on Hadoop", "authors": ["Yang Zhao", "Kafei"], "emails": [], "sections": [{"heading": null, "text": "Parallel Spectral Clustering Algorithm Based on Hadoop\nYang Zhao\uff0cKafei Xiao\uff0cYajun Cui\uff0cLei Wang\uff0cChenglong Zhang\nUniversity of Chinese Academy of Sciences\nBeijing\uff0cChina\nAbstract\uff1aSpectral clustering and cloud computing is emerging branch of\ncomputer science or related discipline. It overcome the shortcomings of some traditional clustering algorithm and guarantee the convergence to the optimal solution, thus have to the widespread attention. This article first introduced the parallel spectral clustering algorithm research background and significance, and then to Hadoop the cloud computing Framework has carried on the detailed introduction, then has carried on the related to spectral clustering is introduced, then introduces the spectral clustering arithmetic Method of parallel and relevant steps, finally made the related experiments, and the experiment are summarized.\nKey words: Spectral Clustering\uff1bCloud Computing\uff1bHadoop\uff1bParallelization"}, {"heading": "Contents", "text": "Chapter 1 Introduction ..................................................................................................... 3\n1.1 The Significance of Topic .................................................................................... 3 1.2 Overseas and Domestic Research Status ............................................................. 3 Chapter 2 Introduction of Hadoop .................................................................................... 4\n2.1 HDFS distributed file system ............................................................................... 4 2.2 Parallel computing programming thought graphs ............................................... 5 2.3 HBase distributed database ................................................................................ 5 2.4 Summary ........................................................................................................... 5 Chapter 3 Overview of spectral clustering algorithm ......................................................... 6\n3.1 Summary ........................................................................................................... 6 3.2 Theoretical basis of spectral clustering ............................................................... 6\n3.2.1 Basic concepts of graph .................................................................................... 6 3.2.2 Laplasse matrix of graph and its properties ....................................................... 7 3.2.3 Realization of spectral clustering algorithm ....................................................... 8\nChapter 4 Parallel spectral clustering algorithm ................................................................ 8\n4.1 Overview of parallel spectral clustering .............................................................. 8 4.2 Analysis of single spectrum clustering algorithm ................................................. 9 4.3 The realization of parallel spectral clustering on Hadoop .................................. 10\n4.3.1 Parallel computing similarity matrix ............................................................... 10 4.3.2 Parallel computing k smallest eigenvectors ..................................................... 11 4.3.3 Parallelization K-means clustering .................................................................. 13 4.4 The algorithm complexity analysis .................................................................... 16 4.5 Summary ......................................................................................................... 17 Chapter 5 Spectral clustering algorithm results in parallel ............................................... 17\n5.1 Empirical Data ................................................................................................. 17 5.2 Experiment results ........................................................................................... 18 Bibliography .................................................................................................................. 20\nChapter 1 Introduction\n1.1 The Significance of Topic\nTo understand a new object or a new phenomenon, people always try to find\nits features, and then compared with the known object or phenomenon, according to certain standards and rules whether similar between them. With the rapid development of Internet, brings us a lot of data and information and at the same time give rise to a large number of data accumulation. In order to better represent and understand these data, using a computer the data classification or effectively. Clustering is particularly important.\nClustering is in accordance with the requirements for a certain similarity to\nthe process of grouping samples. It is a kind of widely used data analysis tools. Comparing with the classification of supervised learning methods, clustering methods have obvious advantage: 1. Collect and tag large sample set is a laborious work and in many cases we\ncan't get data category attributes;\n2. For classification samples can change slowly over time, the nature of the\nchange in unsupervised learning cases compared with supervised learning is easier to get, at this time of learning can be able to get a boost; 3. Unsupervised method can extract some of the basic features of these\ncharacteristics to the further steps to provide preprocessing and feature extraction of effective early treatment; 4. Clustering algorithm can reveal to us the test data of some internal structure\nand rules, if we can get some valuable information through these methods, can be more targeted to design better subsequent classifier.\n1.2 Overseas and Domestic Research Status\nIn recent years, machine learning algorithm based on graph theory is\nbecoming a new research hotspot in the field of machine learning. Spectral\nclustering Algorithms can be classified as segmentation method based on graph theory. At present, all kinds of based on graph partition criterion put forward to promote the development of machine learning algorithms. Recently, a clustering method basing on spectral graph theory began to get attention. The class method using similarity matrix feature clustering feature vectors of the decomposed, this kind of algorithm is referred to as spectral clustering. Although the spectral clustering method has achieved good results, but now still in the early stage of development, there are still many algorithm itself many open questions worth further study, and there is no complete theory to explain the spectral clustering algorithm and analyze its innings sex limited. Spectral clustering algorithm although has obtained the good effect, but the algorithm is still in the early stages of the development and the algorithm still has many issues worthy of further research.\nChapter 2 Introduction of Hadoop\nHadoop is an open mature and widely used open source cloud computing\nframework. It implements a distributed file system (the Hadoop Distributed File System), hereinafter referred to as the HDFS. It also implements a complete graphs the calculation of distributed programming framework, the framework can use HDFS above for large-scale data analysis and data processing.\n2.1 HDFS distributed file system\nHDFS is a distributed file storage system. Through streaming data access\npattern to store large files and large files. Its characteristic is a write, read many times and efficient batch. So the HDFS more focus on high throughput of data access. One of the main design goals of HDFS is under fault condition also can ensure the reliability of data storage. HDFS has a relatively complete redundancy backup and fault recovery mechanism. It be achieved reliably store huge amounts of documents in the clusters.\n2.2 Parallel computing programming thought graphs\nGraphs by adopting the idea of \"divide and rule\", to the operation of the\nlarge-scale data sets, distributed to a master node under the management of each node to complete together, and then through the integration of each node in the middle of the results to get the final result. Highly abstract graphs for two functions: map and reduce, the map is in charge of the task is decomposed into multiple tasks, reduce multitasking is responsible for the decomposition results summary. As for the other total complex problems in parallel programming, such as distributed storage, job scheduling, load balancing and fault tolerance, network communication, etc. It\u2019s responsible for handling all by graphs framework.\n2.3 HBase distributed database\nHBase based on HDFS, it can provide high reliability, high performance,\ncolumns, storage, scalable, and real-time database system to read and write. It can build from the bottom up, simply by adding nodes to achieve linear scaling. HBase is not a relational database, it does not support SQL. But in specific problem space, it can do a RDBMS cannot do: on cheap hardware cluster management of large scale sparse tables.\n2.4 Summary\nThis chapter briefly introduces the two core components of the Hadoop\nframework. HDFS distributed file system and parallel programming framework graphs as well as build upon HDFS HBase distributed no database. Spectral clustering parallel to the help of the three components of HDFS is used to store the initial input files, graphs are the core of the whole parallel computing system, whereas HBase when matrix is calculated to store intermediate results and the final result.\nChapter 3 Overview of spectral clustering algorithm\n3.1 Summary\nSpectral clustering algorithm is developed based on clustering algorithm of\ngraph theory recently. Compared with some traditional clustering algorithms, Spectral clustering algorithm has its obvious advantages. It is not only simple, but also effective that can be able to identify the sample space of arbitrary shape. It can converge to the global optimal solution and be well suited to real problems. The reason of success of spectral clustering: Through the similarity matrix or Laplacian matrix Eigen decomposition, we can get a global optimal solution of a clustering criterion in a continuous domain.\n3.2 Theoretical basis of spectral clustering\nThe thoughts of spectral clustering is from the spectra divided. Spectral\nclustering translate clustering problem into multi-channel partitioning problem of undirected graph .Data points are looked as vertices V in undirected graph G (V, E).Weighted edges\u2019 set  ijSE  represents the similarity between two vertices based on a similarity metric calculation, which uses S to represent the similarity matrix of clustering data points. In the graph G we translate clustering problem into graphic partitioning problem on the graph G. That\u2019s to say, the G (E, V) is partitioned into k disjoint subsets kVVV ,...,, 21 to ensure in each subset jV similarity is high and the similarity between the different sets is low.\n3.2.1 Basic concepts of graph\nSetting up sample points is nXXX ,...,, 21 .We define the similarity of between\ntwo sample points iX and jX is ijS .We also use the similarity graph G (V, E) to express the similarity between samples. Each vertex of the graph represents a point iX .If the weight ijS between the two sample points is greater than 0, it can be considered the connection between the two sample points.\nWe define G= (V, E) is a undirected weight connected graph. The vertex set is\n nvvv ,...,, 21 .The connection weight between the vertices iv and jv that exists\nconnection is ijw .Weighted adjacency matrix of graph is\n),...,2,1,)(( njiwW ij  .If 0ijw ,There is no connection between iv and jv .If\nthe G is the undirected connection graph, we can reach jiij ww  .The degree of the vertex is defined as:\n j iji wd , )(iadjacentj\nWe define a matrix D that diagonal matrix is consisted by nddd ,...,, 21 .Given\na subset of vertices VA .The definition of its complement is A .We define an\nindicator vector  nA fffl ,...,, 21 . If Avi  , 1if .We also define Ai which\nrepresents  Avi i | ,For two subsets VBA , ,we define:\n     BjAi ijwBAW , ,\nThen define two metrics to measure the size of subset VA\n   Ai idAvol )(\n)(Avol defines the size of the set A by the degree of all vertices in A.\nIf the set A is connected, any two vertices in the set A can be connected by a\npath within the set. If there is no connection between the A and the A , we define\nA is a connected component. If  ji AA and\nVAAA k  ...21 ,non-empty set kAAA ,...,, 21 form a graph partition.\n3.2.2 Laplasse matrix of graph and its properties\nThe main tool of spectral clustering is the Laplasse matrix of graph. For the\nnon-normalized Laplasse matrix, we have the following proposition:\nWe suppose an undirected weight graph G. The number of the eigenvalues of\nthe non-normalized Laplasse matrix L is 0 represents the number of independent components connected for its graph. If it has K eigenvalues whose value is 0, it\nwill have K connected component kAAA ,...,, 21 and the feature space of feature\nvalues is 0 that has a liner combination of Indicator vector kAAA lll ,...,, 21 of connected component . For the normalized Laplasse matrix, we have the following proposition:\nWe suppose a undirected weight graph G. The number of the eigenvalues of\nthe normalized Laplasse matrix symL and rwL is 0 represents the number of independent components connected for its graph. If it has K eigenvalues whose value is 0,it will have K connected component kAAA ,...,, 21 and eigenvalue of\nrwL is 0 that has a liner combination of Indicator vector kAAA lDlDlD 2/1 2 2/12/1 ,...,, 1 of connected component .\n3.2.3 Realization of spectral clustering algorithm\nWe use the sample point nxxx ,...,, 21 to represent sample collection for\narbitrary clustering. Similarity is\n22 2/)exp( jiij xxS \nIts corresponding similarity matrix is ),...,2,1,)(( njisS ij  .\nChapter 4 Parallel spectral clustering algorithm\n4.1 Overview of parallel spectral clustering\nAfter years of continuous research and unremitting exploration, Spectral\nClustering has been recognized as a clustering algorithm which is more effective than the traditional clustering algorithm, and its mathematical basis is the graph cut and matrix operation. In general, The time complexity of spectral clustering is O (n3), where n is the number of objects to be entered. Because of its high complexity, it greatly limits its application in the actual production and research.\nTo reduce the time complexity of spectral clustering, this chapter tries to\ncombine spectral clustering algorithm and MapReduce programming ideas of\nHadoop together. Through the analysis of the traditional spectral clustering algorithm steps, we can achieve concurrent steps to separate out and put these steps integration into the MapReduce, combined with Hadoop excellent distributed storage and parallel computing performance, realize the spectral clustering algorithm parallelization, take advantage of the cluster, and reduce the time needed for the clustering ultimately.\n4.2 Analysis of single spectrum clustering algorithm\nAccording to the similarity measurement, single spectral clustering\nalgorithm can be divided into several different methods. In here, we choose normalized spectral clustering to illustrate. Algorithm 4.1 normalized spectral clustering algorithm Input: data points set x1, x2,..., xn, clusters number is k Output\uff1aclusters C1, ..., Ck 1. Calculate the similarity matrix nRS  n , ),x( i jxS is data points xi and xj\nsimilarity and then sparse it 2. Constructing diagonal degree matrix D, and diagonal elements are\n  n j 1 jii )x,S(xd 3. Calculate normalized Laplasse matrix L, 2 1 - 2 1 - SDDLL  4. Calculate k the minimum eigenvectors of L, and the composition matrix\nknRZ contains them.\n5. Standardized Z to knRY 6. The data points with K-means algorithm ),...,2,1i(y nRki  into k clusters\nC1,... Ck.\n4.3 The realization of parallel spectral clustering on Hadoop\n4.3.1 Parallel computing similarity matrix\nBecause Hadoop's MapReduce parallel programming framework can\nprovide excellent distributed computing framework, HBase distributed database building on HDFS can be used to initialize and store intermediate results matrix. So, we choose MapReduce, a core component of the Hadoop, to achieve our parallel spectral cluster with the distributed file system HDFS and HBase distributed database. We first put adjacency matrix which is constitute of the\ndata point 1 2, , , nx x x into HBase table, the table can be clustered access to all\nof the machines, and the key row of each record is set as the index of the data points. Then we use a map function to automatically calculate the similarity between the data points.\n, ,1 ,i j i j n    we just need to calculate  ,i jsim x x .\nBecause these objects can constitute undirected graphs--\n   , = ,i j j isim x x sim x x \uff0cthe calculation of the similarity between each pair of data\npoints needs to be calculated once. And according to the symmetry of undirected graphs, the other half of the similarity values are obtained.\n<1,{index 1,index n}>\n<2,{index 2,index n-1}>\n<3,{index 3,index n-2}>\n...\nInput data\nsplit split\nsplit\nsplit\n. . ."}, {"heading": "Map task", "text": ""}, {"heading": "Map task", "text": "Map task\nIndex 1 similarity compute Index n similarity compute\n\u2026\nIndex 2 similarity compute Index n-1 similarity compute\n\u2026\nIndex 3 similarity compute Index n-2 similarity compute\n\u2026\nSimilarity matrix Table\nMap: Similarity calculation\nFig.1. Map function of Parallel computing similarity matrix\nAlgorithm 4.2 Map function design of constructing similarity matrix Input\uff1a<key\uff0cvalue>\uff0ckey as point index\uff0cvalue as null"}, {"heading": "Output\uff1a<key\u2019\uff0c value\u2019>=<key, null>", "text": "1\u3001index = key, anotherindex = n-key+1 2\u3001for i in {index, anotherindex}\ni_content = getContentFromHBase(i); for j = i to n do\nj_content = getContentFromHBase(j); sim = computeSimilarity(i_content, j_content); storeSimilarity(i,j,sim) into HBase table;\nEnd For\nEnd For 3\u3001Output <key, null> 4\u3001End\nIt should be noted that \"similar value of the subscript i\" need to calculate\nthe value of 1n i  pairs of data point  1, , , , , ,i i i i i nx x x x x x . Therefore,\nin order to load balance ,we calculate the similarity of index i and index n-i+1, which performed on the same machine.\n4.3.2 Parallel computing k smallest eigenvectors\nLanczos algorithm is a kind of algorithm that transforms the symmetry\nmatrix into a symmetric three diagonal matrix by orthogonal transformation, and is named as the Hungarian mathematician Lanczos Cornelius in twentieth Century. Lanczos algorithm is as follows, see algorithm 4.3. Algorithm 4.3 The design idea of Lanczos algorithm\n11 v \u3001 The random vector of norm as 1\n \n0 1\n1\n1\n1 1\n0 0\n2 : 1,2, ,m\n,\nRe\nj j j j\nj j j j j j j\nj j\nj j j\nv\nIteration for j\nw Lv v\nw v\nw w v\nw\nv w\nturn\n\n\n\n\n\n\n\n\n \n \n\n    \n\n\n\u3001\n3\u3001\nNote that  ,x y is the dot product of two vectors, and after iteration, we\nget a three diagonal matrix of j and j :\n1 2 2 2 3\n3 3\n1\n1 1\n0\nmm\nm\nm m m\nm m\nT\n    \n \n\n  \n \n\n \n                   \nAfter getting the matrix mmT , because mmT is a three diagonal matrix, it is\neasy to get its eigenvalues and eigenvectors by some methods (such as QR). It can be proved that these eigenvalues (eigenvectors) are the similarity values of the eigenvalues (eigenvectors) of the original Laplasse matrix L.\nAs can be seen from the Lanczos algorithm, jLv , the multiplication of matrix\nand vector, is a relatively time-consuming process. If the matrix L is not enough to load memory, and each time it is multiplied by a vector to move L, the time consumption will be great.\nIn the distributed storage and computing framework provided by\nMapReduce Hadoop and HDFS, an excellent idea is used: \"mobile computing is more effective than mobile data\". We use a class Matrix Distributed to the matrix L to be decomposed to HBase up, the matrix L on the HBase stored in the time when the line to the segmentation store. Then each iteration of the Lanczos\ndoes not to move the HBase distributed storage of the matrix L, instead, moving\nvector (Mobile Computing), and parallel computing the product of vectors jv\nand matrices L by multiplying the rows of matrices L.\nFig.2. The Map/Reduce of Parallel computing k smallest eigenvectors\njLv is the main time consuming operation of the Lanczos algorithm, now\nwith the matrix L distributed stored in HBase, this operation can complete in map / reduce function. If the K feature vector is needed, the vector is transferred to the data store of L for K times to compute.\n4.3.3 Parallelization K-means clustering\nOnce have gotten the k minimal eigenvector, we can get a new expression\nform iy of metadata point ix by the way of standardization. In the parallel\nK-means clustering algorithm, a table containing the HBase that initialize the center of the K cluster is created and can be accessed by the individual machines on the HBase. Obviously, a data point and the distance between the k centers are calculated and the other data points and the distance between the K centers are independent of each other. So different data points and k center distance can be executed in parallel computing framework of MapReduce. Based on the Hadoop parallel K-means clustering algorithm design, the main work is the design and\nimplementation of map and reduce functions, including the type of input and output (key, value) and the Map and Reduce functions specific logic etc.\nThe MapReduce implementation algorithm of brief clustering K-means as\nthe chart below shows:\nFig.3. The clustering K-means algorithm parallelization on Hadoop\n1. Create a file containing the initial cluster center. This file contains the\ncluster center for each iteration. To the following account conveniently, this file is called \u201cthe center file\u201d.\n2. Map function: read the center file and get the center of the last iteration\n(or initialization). Read the data points, and calculate the distance between each data point and several centers, then assign each data point to the center of its closest (most similar).\nmap(<key, value>, <key\u2019, value>\u2019) { from the value to analysis the sample object, recorded as instance; the maximum value of the auxiliary variable minDis is initialized; index is initialized to -1; for i = 0 to k-1 do {\ndis = distance between instance and i If dis < minDis\n{\nminDis = dis ; index = i ;\n}\n} index as key\u2019; the value of each dimensional coordinate as value\u2019; output(key\u2019, value\u2019); } 3. Reduce function: updating the new center of the coordinates of each\ncluster, and the new center value of the coordinates written to \u201cthe center file\u201d.\nreduce(<key, value>, <key\u2019, value\u2019) { initialize an array for storing the cumulative values of the coordinates of\neach dimension, and the initial value of each component is 0;\ninitialize variable num, the total number of samples assigned to the same\ncluster is recorded, and the initial value is 0;\nwhile(value.hasNext()) {\nfrom the value.next() analysis the each coordinates and the number num\nof a sample;\nadd the values of each dimension to the corresponding component of the\narray;\nnum += num ;\n} each component of the array divide by num, and get a new central point\ncoordinate;\nkey as key\u2019; constructs a string containing the information of the new central points of\nthe coordinates, the string as value\u2019;\noutput(key\u2019, value\u2019); } 4. go on until the center of the cluster changes, or the number of iterations\nreached a preset value.\n4.4 The algorithm complexity analysis\nParallel computing similarity matrix.Because the subscript i need to\ncalculate the i+n-1 similar values of data points,the time complexity of similar matrix is )2/)(()1...)1(( 2 mnnOnnO  , where 2m is said that the cluster\nm machine default each machine starts two Map tasks.\nParallel computing the k minimum eigenvectors. Under the condition of\nnon-parallel, time complexity of k different characteristic vectors Laplacian matrix L calculated using Lanczos is )( 2nkkLO op  ,Where opL is L and vector Vj\nmultiplication. Because we have the matrix L cut into lines stored in the HBase, matrix L and vector Vj multiplied by the distribution of the m machines to run.\nUnder ideal conditions, the time complexity of each multiplication is mL / op , the\ntime complexity of the first k eigenvectors parallelization calculated is )/( 2nkmkLO op  .\nParallel K-means clustering. Each data point of the new expression yi is\nk-dimension, and k center distance calculation in each iteration. Thus, the time complexity of the distance of each data point calculated is )( 2kO . So the time\ncomplexity of the distance computation for each iteration is )n( 2kO . Ideally, all\ndata points are calculated from the distance of the average distribution to the\nmachines, so the time complexity is reduced to )()( 2\niterationsofnum m\nnk O  .\nSo, the total time complexity should be the sum of the three parts:\n) 2\n( 2\nm\nnn O  + )/( 2nkmkLO op  + )()(\n2\niterationsofnum m\nnk O  . It is obvious that\nthe complexity of this time is much smaller than O(n3) and the speed of clustering is improved.\n4.5 Summary\nThis chapter briefly introduces the necessity and possibility of parallel\nspectral clustering. Then, the implementation of the algorithm in Hadoop is described, and the corresponding description is carried out with the pseudo code. In this chapter finally to parallel spectral clustering algorithm the algorithm complexity of the corresponding analysis, in the next chapter will put these algorithms are applied to the actual cluster computing in, to verify the parallel spectral clustering algorithm has special advantages and Hadoop cluster brings speedup.\nChapter 5 Spectral clustering algorithm results in parallel\nThe experiment platform hardware configuration: The host 11 (Intel(R) Core(TM) i5-2300 2.80GHz), the memory of 4 GB, operating system (64 - bit Linux Ubuntu). Among them: a Master machine as the name of the node and job control node, only store metadata file blocks and to control the distribution and operation. The other 10 Slave machine as the actual storage nodes and the compute nodes and they are responsible for the storage of file blocks the execution of real data and computing tasks.\n5.1 Empirical Data\nThe source of the experimental data represent the topology of a text file.\nThere are two per line to one or more spaces separated string. T is representative figure, v represents a vertex, behind of no. 0 1 representative on the edge of the label is 1. E is for an edge, 0 1 2 represents the connection 0 1 point on the edge of the label is 2. A total of 10029 points and 21054 side. According to the topology of a text file as shown in the figure below:\nFig.4. The topology of the original text file\n5.2 Experiment results\nThe result of running under different Slave sets. From the results it can be\nseen in table 5-1, parallel spectral clustering algorithm has been realized, and with the increase of number of the machine, the time needed for parallel computing is less and less. Table 5-1 reflect the parallel computing similarity matrix, parallel computing eigenvalues and eigenvectors, parallel K - means algorithm is accelerated. From the table we can see that with the increase of number of the machine, the three parallel steps have corresponding approximate linear growth. This also proves that under the condition of the Hadoop distributed, spectral clustering has dramatically improved the speed of the algorithm.\nBut increasing 8 machine more than 10 sets of (as in the figure of 10\nmachine), algorithm and the steps are not evident on the speed of ascension, in\nsome cases, even slower, this is because for each data set of size n, and the calculation of specific steps, the Hadoop distributed environment have the corresponding critical machine number, within this threshold calculation speed with approximate linear speed is growing. Communication between machine and once is greater than the critical value, the consumption of the growth is even larger than distributed computing, so the speed with the increase of number of the machine would fall.\nTable.1. The acceleration of the parallel spectral clustering algorithm based on Hadoop\nSlave Number\nParallel --similarity matrix Parallel -- k eigenvectors Parallel--K-means algorithm\nTotal Time\n1 1:41:46 2:28:14 0:28:45 4:24:45 2 0:58:45 1:45:47 0:22:36 3:11:08 4 0:30:56 1:25:10 0:18:09 2:28:15 6 0:23:23 1:10:44 0:14:46 1:47:53 8 0:21:15 1:00:19 0:12:59 1:34:33 10 0:22:29 1:01:39 0:11:45 1:35:53\nAs we can see from figure 5-2, the accelerating trend chart. From 1 to 2 sets\nof that kind of transition, basically is to reduce the time or so commonly. Number of the machine after redouble, speedup growth began to slow, until finally, with the increase of network communication and task allocation overhead, more machines will gradually lose the advantages brought by the machine more.\nFig.5. The trend chart of the parallel spectral clustering algorithm based on Hadoop\nBibliography\n[1] Lars George. HBase The Definitive Guide[M].CA\uff1aO\u2019REILLY 2011 [2] Peng Liu. Hadoop - the shortcut to the cloud computing [M]. Beijing, Electronic Industry Press, 2011-09 [3] E. Gropp and A. Skjellum. Using MPI-2\uff1aAdvanced Features of the Message -Passing Interface. MIT Press\uff0c1999 [4] S. Mahadevan. Fast Spectral Learning Using Lanczos Eigenspace Projections. In AAAI\uff0c2008\uff1a1472-1475 [5] GAO Yan GU Shi-Wen TANG Jin CAI Zi-Xing. Research of spectral clustering method in machine learning [J]. Computer Science\uff0c2007\uff0c34(2)\uff1a 201-203. [6] FIEDLER M. Algebaric connectivity of graphs [J]. Czechoslovak Mathematical Journal\uff0c1973\uff0c23(2)\uff1a298-305. [7] Licheng Jiao\uff0cFang Liu\uff0cShuiping Gou. Intelligent Data Mining and Knowledge Discovery [M]. Xi'an: Xidian University Publishing house\uff0c2006. [8] Jain A,Murty M,Flynn P. Data clustering:A Review[J].ACM COMPUTING SURVEYS,1999,(03):264-323.doi:10.1145/331499.331504. [9] SHI J\uff0cMALIK J. Normalized cuts and image segmentation [J]. IEEE Transactions on Pattern Analysis and Machine Intelligence\uff0c2000\uff0c22(8)\uff1a888-905 [10] I.Dhillon\uff0cY.Guan\uff0cand B.Kulis.Weighted Graph Cuts without Eigenwectors\uff1a A Multilevel Approach. IEEE Trans. On Pattern Analysis and Machine Intelligence\uff0c2007\uff0c29(11)\uff1a1944-1957"}], "references": [{"title": "HBase The Definitive Guide[M].CA:O\u2019REILLY", "author": ["Lars George"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Hadoop - the shortcut to the cloud computing [M", "author": ["Peng Liu"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Using MPI-2:Advanced Features of the Message -Passing Interface", "author": ["E. Gropp", "A. Skjellum"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Fast Spectral Learning Using Lanczos Eigenspace Projections", "author": ["S. Mahadevan"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Research of spectral clustering method in machine learning", "author": [], "venue": "[J]. Computer", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Algebaric connectivity of graphs [J", "author": ["M. FIEDLER"], "venue": "Czechoslovak Mathematical Journal,1973,23(2):298-305", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1973}, {"title": "Intelligent Data Mining and Knowledge Discovery [M]. Xi'an: Xidian University Publishing house,2006", "author": ["Licheng Jiao", "Fang Liu", "Shuiping Gou"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Data clustering:A Review[J].ACM COMPUTING SURVEYS,1999,(03):264-323.doi:10.1145/331499.331504", "author": ["A Jain", "M Murty", "P. Flynn"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Normalized cuts and image segmentation [J", "author": ["J SHI", "J. MALIK"], "venue": "IEEE Transactions on Pattern Analysis and Machine", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}], "referenceMentions": [], "year": 2015, "abstractText": "Spectral clustering and cloud computing is emerging branch of computer science or related discipline. It overcome the shortcomings of some traditional clustering algorithm and guarantee the convergence to the optimal solution, thus have to the widespread attention. This article first introduced the parallel spectral clustering algorithm research background and significance, and then to Hadoop the cloud computing Framework has carried on the detailed introduction, then has carried on the related to spectral clustering is introduced, then introduces the spectral clustering arithmetic Method of parallel and relevant steps, finally made the related experiments, and the experiment are summarized.", "creator": "Microsoft\u00ae Word 2013"}}}