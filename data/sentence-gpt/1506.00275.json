{"id": "1506.00275", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2015", "title": "Diversity in Spectral Learning for Natural Language Parsing", "abstract": "We describe an approach to incorporate diversity into spectral learning of latent-variable PCFGs (L-PCFGs). Our approach works by creating multiple spectral models where noise is added to the underlying features in the training set before the estimation of each model. We describe three ways to decode with multiple models. In addition, we describe a simple variant of the spectral algorithm for L-PCFGs that is fast and leads to compact models. Our experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the $F_1$ score of 90.18, and for German we achieve the $F_1$ score of 83.38. For German, our model is equivalent to the $F_1$ score of 81.36. For French, we achieve the $F_1$ score of 82.25. For German, we achieve the $F_1$ score of 80.35. For German, we achieve the $F_1$ score of 79.33. For German, we achieve the $F_1$ score of 79.29. For German, we achieve the $F_1$ score of 78.31. For German, we achieve the $F_1$ score of 78.31. For German, we achieve the $F_1$ score of 77.31. For German, we achieve the $F_1$ score of 77.31. For German, we achieve the $F_1$ score of 77.31. For German, we achieve the $F_1$ score of 77.31. For German, we achieve the $F_1$ score of 77.31. For German, we achieve the $F_1$ score of 77.31. For German, we achieve the $F_1$ score of 77.31. For German, we achieve the $F_1$ score of 77.31. For German, we achieve the $F_1$ score of 77.31. For German, we achieve the $F_1$ score of 77.31. For German, we achieve the $F_1$ score of 77.31. For German, we achieve the $F_1$ score of 77.31. For German, we achieve the $F_1$ score of 77.31. For German, we achieve the $F_1$ score of 77.31. For", "histories": [["v1", "Sun, 31 May 2015 19:21:26 GMT  (57kb,D)", "https://arxiv.org/abs/1506.00275v1", null], ["v2", "Sat, 15 Aug 2015 12:02:57 GMT  (95kb,D)", "http://arxiv.org/abs/1506.00275v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shashi narayan", "shay b cohen"], "accepted": true, "id": "1506.00275"}, "pdf": {"name": "1506.00275.pdf", "metadata": {"source": "CRF", "title": "Diversity in Spectral Learning for Natural Language Parsing", "authors": ["Shashi Narayan", "Shay B. Cohen"], "emails": ["snaraya2@inf.ed.ac.uk", "scohen@inf.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001).\nThe main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions.\nIn this paper, we explore another angle for the use of a set of parse tree predictions, where all pre-\ndictions are made for the same sentence. More specifically, we describe techniques to exploit diversity with spectral learning algorithms for natural language parsing. Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015).\nCohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006).\nWe further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petrov and Klein (2007). Instead of exploiting diversity from a k-best list from a single model, we estimate multiple models, where the underlying features are perturbed with several perturbation schemes. Each such model, during test time, yields a single parse, and all parses are then used together in several ways to select a single best parse.\nThe main contributions of this paper are twofold. First, we present an algorithm for estimating L-PCFGs, akin to the spectral algorithm of Cohen et al. (2012), but simpler to understand and implement. This algorithm has value for readers who are interested in learning more about spectral algorithms \u2013 it demonstrates some of the core ideas in spectral learning in a rather intuitive way. In addition, this algorithm leads to sparse grammar estimates and compact models.\nSecond, we describe how a diverse set of predictors can be used with spectral learning techniques.\nar X\niv :1\n50 6.\n00 27\n5v 2\n[ cs\n.C L\n] 1\n5 A\nug 2\n01 5\nOur approach relies on adding noise to the feature functions that help the spectral algorithm compute the latent states. Our noise schemes are similar to those described by Wang et al. (2013). We add noise to the whole training data, then train a model using our algorithm (or other spectral algorithms; Cohen et al., 2013), and repeat this process multiple times. We then use the set of parses we get from all models in a recombination step.\nThe rest of the paper is organized as follows. In \u00a72 we describe notation and background about L-PCFG parsing. In \u00a73 we describe our new spectral algorithm for estimating L-PCFGs. It is based on similar intuitions as older spectral algorithms for L-PCFGs. In \u00a74 we describe the various noise schemes we use with our spectral algorithm and the spectral algorithm of Cohen et al. (2013). In \u00a75 we describe how to decode with multiple models, each arising from a different noise setting. In \u00a76 we describe our experiments with natural language parsing for English and German."}, {"heading": "2 Background and Notation", "text": "We denote by [n] the set of integers {1, . . . , n}. For a statement \u0393, we denote by [[\u0393]] its indicator function, with values 0 when the assertion is false and 1 when it is true.\nAn L-PCFG is a 5-tuple (N , I,P,m, n) where:\n\u2022 N is the set of nonterminal symbols in the grammar. I \u2282 N is a finite set of interminals. P \u2282 N is a finite set of preterminals. We assume thatN = I \u222a P , and I \u2229 P = \u2205. Hence we have partitioned the set of nonterminals into two subsets.\n\u2022 [m] is the set of possible hidden states.\n\u2022 [n] is the set of possible words.\n\u2022 For all a \u2208 I, b \u2208 N , c \u2208 N , h1, h2, h3 \u2208 [m], we have a binary context-free rule a(h1)\u2192 b(h2) c(h3).\n\u2022 For all a \u2208 P , h \u2208 [m], x \u2208 [n], we have a lexical context-free rule a(h)\u2192 x.\nLatent-variable PCFGs are essentially equivalent to probabilistic regular tree grammars (PRTGs; Knight and Graehl, 2005) where the righthand side trees are of depth 1. With general PRTGs, the righthand side can be of arbitrary depth, where the leaf nodes of these trees correspond to latent states in the L-PCFG formulation\nabove and the internal nodes of these trees correspond to interminal symbols in the L-PCFG formulation.\nTwo important concepts that will be used throughout of the paper are that of an \u201cinside tree\u201d and an \u201coutside tree.\u201d Given a tree, the inside tree for a node contains the entire subtree below that node; the outside tree contains everything in the tree excluding the inside tree. See Figure 1 for an example. Given a grammar, we denote the space of inside trees by T and the space of outside trees by O."}, {"heading": "3 Clustering Algorithm for Estimating L-PCFGs", "text": "We assume two feature functions, \u03c6 : T \u2192 Rd and \u03c8 : O \u2192 Rd\u2032 , mapping inside and outside trees, respectively, to a real vector. Our training data consist of examples (a(i), t(i), o(i), b(i)) for i \u2208 {1 . . .M}, where a(i) \u2208 N ; t(i) is an inside tree; o(i) is an outside tree; and b(i) = 1 if a(i) is the root of tree, 0 otherwise. These are obtained by splitting all trees in the training set into inside and outside trees at each node in each tree. We then define \u2126a \u2208 Rd\u00d7d\u2032 :\n\u2126a =\n\u2211M i=1[[a\n(i) = a]]\u03c6(t(i))(\u03c8(o(i)))>\u2211M i=1[[a (i) = a]] (1)\nThis matrix is an empirical estimate for the cross-covariance matrix between the inside trees and the outside trees of a given nonterminal a. An inside tree and an outside tree are conditionally independent according to the L-PCFG model, when the latent state at their connecting point is known. This means that the latent state can be identified by finding patterns that co-occur together in inside and outside trees \u2013 it is the only random variable that can explain such correlations. As such, if we reduce the dimensions of \u2126a using singular value decomposition (SVD), we essentially get\nrepresentations for the inside trees and the outside trees that correspond to the latent states.\nThis intuition leads to the algorithm that appears in Figure 2. The algorithm we describe takes as input training data, in the form of a treebank, decomposed into inside and outside trees at each node in each tree in the training set.\nThe algorithm first performs SVD for each of the set of inside and outside trees for all nonterminals.1 This step is akin to CCA, which has been used in various contexts in NLP, mostly to derive representations for words (Dhillon et al., 2015; Rastogi et al., 2015). The algorithm then takes the representations induced by the SVD step, and\n1We normalize features by their variance.\nclusters them \u2013 we use k-means to do the clustering. Finally, it maps each SVD representation to a cluster, and as a result, gets a cluster identifier for each node in each tree in the training data. These clusters are now treated as latent states that are \u201cobserved.\u201d We subsequently follow up with frequency count maximum likelihood estimate to estimate the probabilities of each parameter in the L-PCFG.\nConsider for example the estimation of rules of the form a\u2192 x. Following the clustering step we obtain for each nonterminal a and latent state h a set of rules of the form a[h] \u2192 x. Each such instance comes from a single training example of a lexical rule. Next, we compute the probability of the rule a[h] \u2192 x by counting how many times that rule appears in the training instances, and normalize by the total count of a[h] in the training instances. Similarly, we compute probabilities for binary rules of the form a\u2192 b c.\nThe features that we use for \u03c6 and \u03c8 are similar to those used in Cohen et al. (2013). These features look at the local neighborhood surrounding a given node. More specifically, we indicate the following information with the inside features (throughout these definitions assume that a \u2192 b c is at the root of the inside tree t):\n\u2022 The pair of nonterminals (a, b). E.g., for the inside tree in Figure 1 this would be the pair (VP, V). \u2022 The pair (a, c). E.g., (VP, NP). \u2022 The rule a\u2192 b c. E.g., VP\u2192 V NP. \u2022 The rule a \u2192 b c paired with the rule at the\nnode b. E.g., for the inside tree in Figure 1 this would correspond to the tree fragment (VP (V saw) NP). \u2022 The rule a \u2192 b c paired with the rule at the\nnode c. E.g., the tree fragment (VP V (NP D N)). \u2022 The head part-of-speech of t paired with a.\nE.g., the pair (VP, V). \u2022 The number of words dominated by t paired\nwith a. E.g., the pair (VP, 3).\nIn the case of an inside tree consisting of a single rule a\u2192 x the feature vector simply indicates the identity of that rule.\nFor the outside features, we use:\n\u2022 The rule above the foot node. E.g., for the outside tree in Figure 1 this would be the rule\nS\u2192 NP VP\u2217 (the foot nonterminal is marked with \u2217). \u2022 The two-level and three-level rule fragments\nabove the foot node. These features are absent in the outside tree in Figure 1. \u2022 The label of the foot node, together with the\nlabel of its parent. E.g., the pair (VP, S). \u2022 The label of the foot node, together with the\nlabel of its parent and grandparent. \u2022 The part-of-speech of the first head word\nalong the path from the foot of the outside tree to the root of the tree which is different from the head node of the foot node. \u2022 The width of the spans to the left and to the\nright of the foot node, paired with the label of the foot node.\nOther Spectral Algorithms The SVD step on the \u2126a matrix is pivotal to many algorithms, and has been used in the past for other L-PCFG estimation algorithms. Cohen et al. (2012) used it for developing a spectral algorithm that identifies the parameters of the L-PCFG up to a linear transformation. Their algorithm generalizes the work of Hsu et al. (2009) and Bailly et al. (2010).\nCohen and Collins (2014) also developed an algorithm that makes use of an SVD step on the inside-outside. It relies on the idea of \u201cpivot features\u201d \u2013 features that uniquely identify latent states.\nLouis and Cohen (2015) used a clustering algorithm that resembles ours but does not separate inside trees from outside trees or follows up with a singular value decomposition step. Their algorithm was applied to both L-PCFGs and linear context-free rewriting systems. Their application was the analysis of hierarchical structure of conversations in online forums.\nIn our preliminary experiments, we found out that the clustering algorithm by itself performs worse than the spectral algorithm of Cohen et al. (2013). We believe that the reason is two-fold: (a) k-means finds a local maximum during clustering; (b) we do hard clustering instead of soft clustering. However, we detected that the clustering algorithm gives a more diverse set of solutions, when the features are perturbed. As such, in the next sections, we explain how to perturb the models we get from the clustering algorithm (and the spectral algorithm) in order to improve the accuracy of the clustering and spectral algorithms."}, {"heading": "4 Spectral Estimation with Noise", "text": "It has been shown that a diverse set of predictions can be used to help improve decoder accuracy for various problems in NLP (Henderson and Brill, 1999). Usually a k-best list from a single model is used to exploit model diversity. Instead, we estimate multiple models, where the underlying features are filtered with various noising schemes.\nWe try three different types of noise schemes for the algorithm in Figure 2:\nDropout noise: Let \u03c3 \u2208 [0, 1]. We set each element in the feature vectors \u03c6(t) and \u03c8(o) to 0 with probability \u03c3.\nGaussian (additive): Let \u03c3 > 0. For each x(i), we draw a vector \u03b5 \u2208 R2k of Gaussians with mean 0 and variance \u03c32, and then set x(i) \u2190 x(i) + \u03b5.\nGaussian (multiplicative): Let \u03c3 > 0. For each x(i), we draw a vector \u03b5 \u2208 R2k of Gaussians with mean 0 and variance \u03c32, and then set x(i) \u2190 x(i)\u2297 (1 + \u03b5), where\u2297 is coordinatewise multiplication.\nNote the distinction between the dropout noise and the Gaussian noise schemes: the first is performed on the feature vectors before the SVD step, and the second is performed after the SVD step. It is not feasible to add Gaussian noise prior to the SVD step, since the matrix \u2126a will no longer be sparse, and its SVD computation will be computationally demanding.\nOur use of dropout noise here is inspired by \u201cdropout\u201d as is used in neural network training, where various connections between units in the neural network are dropped during training in order to avoid overfitting of these units to the data (Srivastava et al., 2014).\nThe three schemes we described were also used by Wang et al. (2013) to train log-linear models. Wang et al.\u2019s goal was to prevent overfitting by introducing this noise schemes as additional regularizer terms, but without explicitly changing the training data. We do filter the data through these noise schemes, and show in \u00a76 that all of these noise schemes do not improve the performance of our estimation on their own. However, when multiple models are created with these noise schemes, and then combined together, we get an improved performance. As such, our approach is related to\nthe one of Petrov (2010), who builds a committee of latent-variable PCFGs in order to improve a natural language parser.\nWe also use these perturbation schemes to create multiple models for the algorithm of Cohen et al. (2012). The dropout scheme stays the same, but for the Gaussian noising schemes, we follow a slightly different procedure. After noising the projections of the inside and outside feature functions we get from the SVD step, we use these projected noised features as a new set of inside and outside feature functions, and re-run the spectral algorithm of Cohen et al. (2012) on them.\nWe are required to add this extra SVD step because the spectral algorithm of Cohen et al. assumes the existence of linearly transformed parameter estimates, where the parameters of each nonterminal a is linearly transformed by unknown invertible matrices. These matrices cancel out when the inside-outside algorithm is run with the spectral estimate output. In order to ensure that these matrices still exactly cancel out, we have to follow with another SVD step as described above. The latter SVD step is performed on a dense \u2126a \u2208 Rm\u00d7m but this is not an issue considering m (the number of latent states) is much smaller than d or d\u2032."}, {"heading": "5 Decoding with Multiple Models", "text": "Let G1, . . . , Gp be a set of L-PCFG grammars. In \u00a76, we create such models using the noising techniques described above. The question that remains is how to combine these models together to get a single best output parse tree given an input sentence.\nWith L-PCFGs, decoding a single sentence requires marginalizing out the latent states to find the best skeletal tree2 for a given string. Let s be a sentence. We define t(Gi, s) to be the output tree according to minimum Bayes risk decoding. This means we follow Goodman (1996), who uses dynamic programming to compute the tree that maximizes the sum of all marginals of all nonterminals in the output tree. Each marginal, for each span \u3008a, i, j\u3009 (where a is a nonterminal and i and j are endpoints in the sentence), is computed by using the inside-outside algorithm.\nIn addition, let \u00b5(a, i, j|Gk, s) be the marginal, as computed by the inside-outside algorithm, for\n2A skeletal tree is a derivation tree without latent states decorating the nonterminals.\nthe span \u3008a, i, j\u3009 with grammar Gk for string s. We use the notation \u3008a, i, j\u3009 \u2208 t to denote that a span \u3008a, i, j\u3009 is in a tree t.\nWe suggest the following three ways for decoding with multiple models G1, . . . , Gp:\nMaximal tree coverage: Using dynamic programming, we return the tree that is the solution to:\nt\u2217 = arg max t \u2211 \u3008a,i,j\u3009\u2208t p\u2211 k=1 [[\u3008a, i, j\u3009 \u2208 t(Gk, s)]].\nThis implies that we find the tree that maximizes its coverage with respect to all other trees that are decoded using G1, . . . , Gp.\nMaximal marginal coverage: Using dynamic programming, we return the tree that is the solution to:\nt\u2217 = arg max t \u2211 \u3008a,i,j\u3009\u2208t p\u2211 k=1 \u00b5(a, i, j|Gk, s).\nThis is similar to maximal tree coverage, only instead of considering just the single decoded tree for each model among G1, . . . , Gp, we make our decoding \u201csofter,\u201d and rely on the marginals that each model gives.\nMaxEnt reranking: We train a MaxEnt reranker on a training set that includes outputs from multiple models, and then, during testing time, decode with each of the models, and use the trained reranker to select one of the parses. We use the reranker of Charniak and Johnson (2005).3\nAs we see later in \u00a76, it is sometimes possible to extract more information from the training data by using a network, or a hierarchy of the above tree combination methods. For example, we get our best result for parsing by first using MaxEnt with several subsets of the models, and then combining the output of these MaxEnt models using maximal tree coverage.\n3Implementation: https://github.com/BLLIP/ bllip-parser. More specifically, we used the programs extract-spfeatures, cvlm-lbfgs and best-indices. cvlm-lbfgs was used with the default hyperparameters from the Makefile."}, {"heading": "6 Experiments", "text": "In this section, we describe parsing experiments with two languages: English and German."}, {"heading": "6.1 Results for English", "text": "For our English parsing experiments, we use a standard setup. More specifically, we use the Penn WSJ treebank (Marcus et al., 1993) for our experiments, with sections 2\u201321 as the training data, and section 22 used as the development data. Section 23 is used as the final test set. We binarize the trees in training data, but transform them back before evaluating them.\nFor efficiency, we use a base PCFG without latent states to prune marginals which receive a value less than 0.00005 in the dynamic programming chart. The parser takes part-of-speech tagged sentences as input. We tag all datasets using Turbo Tagger (Martins et al., 2010), trained on sections 2\u201321. We use the F1 measure according to the PARSEVAL metric (Black et al., 1991) for the evaluation.\nPreliminary experiments We first experiment with the number of latent states for the clustering algorithm without perturbations. We use k = 100 for the SVD step. Whenever we need to cluster a set of points, we run the k-means algorithm 10 times with random restarts and choose the clustering result with the lowest objective value. On section 22, the clustering algorithm achieves the following results (F1 measure): m = 8: 84.30%, m = 16: 85.98%, m = 24: 86.48%, m = 32: 85.84%, m = 36: 86.05%, m = 40: 85.43%. As we increase the number of states, performance improves, but plateaus at m = 24. For the rest of our experiments, both with the spectral algorithm of Cohen et al. (2012) and the clustering algorithm presented in this paper, we use m = 24.\nCompact models One of the advantage of the clustering algorithm is that it leads to much more compact models. The number of nonzero parameters with m = 24 for the clustering algorithm is approximately 97K, while the spectral algorithms lead to a significantly larger number of nonzero parameters with the same number of latent states: approximately 54 million.\nOracle experiments To what extent do we get a diverse set of solutions from the different models we estimate? This question can be answered by testing the oracle accuracy in the different settings. For each type of noising scheme, we generated 80\nmodels, 20 for each \u03c3 \u2208 {0.05, 0.1, 0.15, 0.2}. Each noisy model by itself lags behind the best model (see Figure 3). However, when choosing the best tree among these models, the additivelynoised models get an oracle accuracy of 95.91% on section 22; the multiplicatively-noised models get an oracle accuracy of 95.81%; and the dropoutnoised models get an oracle accuracy of 96.03%. Finally all models combined get an oracle accuracy of 96.67%. We found out that these oracle scores are comparable to the one Charniak and Johnson (2005) report.\nWe also tested our oracle results, comparing the spectral algorithm of Cohen et al. (2013) to the clustering algorithm. We generated 20 models for each type of noising scheme, 5 for each \u03c3 \u2208 {0.05, 0.1, 0.15, 0.2}) for the spectral algorithm.4 Surprisingly, even though the spectral models were smoothed, their oracle accuracy was lower than the accuracy of the clustering algorithm: 92.81% vs. 95.73%.5 This reinforces two ideas: (i) that noising acts as a regularizer, and has a similar role to backoff smoothing, as we see below; and (ii) the noisy estimation for the clustering algorithm produces a more diverse set of parses than that produced with the spectral algorithm.\nIt is also important to note that the high oracle accuracy is not just the result of k-means not\n4There are two reasons we use a smaller number of models with the spectral algorithm: (a) models are not compact (see text) and (b) as such, parsing takes comparatively longer. However, in the above comparison, we use 20 models for the clustering algorithm as well.\n5Oracle scores for the clustering algorithm: 95.73% (20 models for each noising scheme) and 96.67% (80 models for each noising scheme).\nfinding the global maximum for the clustering objective. If we just run the clustering algorithms with 80 models as before, without perturbing the features, the oracle accuracy is 95.82%, which is lower than the oracle accuracy with the additive and dropout perturbed models. To add to this, we see below that perturbing the training set with the spectral algorithm of Cohen et al. improves the accuracy of the spectral algorithm. Since the spectral algorithm of Cohen et al. does not maximize any objective locally, it shows that the role of the perturbations we use is important.\nResults Results on the development set are given in Table 1 with our three decoding methods. We present the results from three algorithms: the clustering algorithm and the spectral algorithms (smoothed and unsmoothed).6\nIt seems that dropout noise for the spectral algorithm acts as a regularizer, similarly to the backoff smoothing techniques that are used in Cohen et al. (2013). This is evident from the two spectral algorithm blocks in Table 1, where dropout noise does not substantially improve the smoothed spectral model (Cohen et al. report accuracy of 88.53% with smoothed spectral model form = 24 without noise) \u2013 the accuracy is 88.64%\u201388.71%\u201389.47%, but the accuracy substantially improves for the unsmoothed spectral model, where dropout brings an accuracy of 86.47% up to 89.52%.\nAll three blocks in Table 1 demonstrate that decoding with the MaxEnt reranker performs the best. Also it is interesting to note that our results continue to improve when combining the output of previous combination steps further. The best result on section 22 is achieved when we combine, using maximal tree coverage, all MaxEnt outputs of the clustering algorithm (the first block in Table 1). This yields a 90.68% F1 accuracy. This is also the best result we get on the test set (section 23), 90.18%. See Table 2 for results on section 23.\nOur results are comparable to state-of-the-art results for parsing. For example, Sagae and Lavie (2006), Fossum and Knight (2009) and Zhang et al. (2009) report an accuracy of 93.2%-93.3% us-\n6Cohen et al. (2013) propose two variants of spectral estimation for L-PCFGs: smoothed and unsmoothed. The smoothed model uses a simple backedoff smoothing method which leads to significant improvements over the unsmoothed one. Here we compare our clustering algorithm against both of these models. However unless specified otherwise, the spectral algorithm of Cohen et al. (2013) refers to their best model, i.e. the smoothed model.\ning parsing recombination; Shindo et al. (2012) report an accuracy of 92.4 F1 using a Bayesian tree substitution grammar; Petrov (2010) reports an accuracy of 92.0% using product of L-PCFGs; Charniak and Johnson (2005) report accuracy of 91.4 using a discriminative reranking model; Carreras et al. (2008) report 91.1 F1 accuracy for a discriminative, perceptron-trained model; Petrov and Klein (2007) report an accuracy of 90.1 F1. Collins (2003) reports an accuracy of 88.2 F1."}, {"heading": "6.2 Results for German", "text": "For the German experiments, we used the NEGRA corpus (Skut et al., 1997). We use the same setup as in Petrov (2010), and use the first 18,602 sentences as a training set, the next 1,000 sentences as a development set and the last 1,000 sentences as a test set. This corresponds to an 80%-10%-10% split of the treebank.\nOur German experiments follow the same setting as in our English experiments. For the clustering algorithm we generated 80 models, 20 for each \u03c3 \u2208 {0.05, 0.1, 0.15, 0.2}. For the spectral algorithm, we generate 20 models, 5 for each \u03c3.\nFor the reranking experiment, we had to modify the BLLIP parser (Charniak and Johnson, 2005) to use the head features from the German treebank. We based our modifications on the documentation for the NEGRA corpus (our modifications are based mostly on mapping of nonterminals to coarse syntactic categories).\nPreliminary experiments For German, we also experiment with the number of latent states. On the development set, we observe that the F1 measure is: 75.04% for m = 8, 73.44% for m = 16 and 70.84% form = 24. For the rest of our experiments, we fix the number of latent states atm = 8.\nOracle experiments The additively-noised models get an oracle accuracy of 90.58% on the development set; the multiplicatively-noised\nmodels get an oracle accuracy of 90.47%; and the dropout-noised models get an oracle accuracy of 90.69%. Finally all models combined get an oracle accuracy of 92.38%.\nWe compared our oracle results to those given by the spectral algorithm of Cohen et al. (2013). With 20 models for each type of noising scheme, all spectral models combined achieve an oracle accuracy of 83.45%. The clustering algorithm gets the oracle score of 90.12% when using the same number of models.\nResults Results on the development set and on the test set are given in Table 3 and Table 4 respectively.\nLike English, in all three blocks in Table 3, decoding with the MaxEnt reranking performs the best. Our results continue to improve when further combining the output of previous combination steps. The best result of 82.04% on the development set is achieved when we combine, using maximal tree coverage, all MaxEnt outputs of the clustering algorithm (the first block in Table 3). This also leads to the best result of 83.38% on the test set. See Table 4 for results on the test set.\nOur results are comparable to state-of-the-art results for German parsing. For example, Petrov (2010) reports an accuracy of 84.5% using prod-\nuct of L-PCFGs; Petrov and Klein (2007) report an accuracy of 80.1 F1; and Dubey (2005) reports an accuracy of 76.3 F1."}, {"heading": "7 Discussion", "text": "From a theoretical point of view, one of the great advantages of spectral learning techniques for latent-variable models is that they yield consistent parameter estimates. Our clustering algorithm for L-PCFG estimation breaks this, but there is a work-around to obtain an algorithm which would be statistically consistent.\nThe main reason that our algorithm is not a consistent estimator is that it relies on k-means clustering, which maximizes a non-convex objective using hard clustering steps. The k-means algorithm can be viewed as \u201chard EM\u201d for a Gaussian mixture model (GMM), where each latent state is associated with one of the mixture components in the GMM. This means that instead of following up with k-means, we could have identified the parameters and the posteriors for a GMM, where the observations correspond to the vectors that we cluster. There are now algorithms, some of which are spectral, that aim to solve this estimation problem with theoretical guarantees (Vempala and Wang, 2004; Kannan et al., 2005; Moitra and Valiant, 2010).\nWith theoretical guarantees on the correctness of the posteriors from this step, the subsequent use of maximum likelihood estimation step could yield consistent parameter estimates. The consistency guarantees will largely depend on the amount of information that exists in the base feature functions about the latent states according to the L-PCFG model."}, {"heading": "8 Conclusion", "text": "We presented a novel estimation algorithm for latent-variable PCFGs. This algorithm is based on clustering of continuous tree representations, and it also leads to sparse grammar estimates and compact models. We also showed how to get a diverse set of parse tree predictions with this algorithm and also older spectral algorithms. Each prediction in the set is made by training an L-PCFG model after perturbing the underlying features that estimation algorithm uses from the training data. We showed that such a diverse set of predictions can be used to improve the parsing accuracy of English and German."}, {"heading": "Acknowledgements", "text": "The authors would like to thank David McClosky for his help with running the BLLIP parser and the three anonymous reviewers for their helpful comments. This research was supported by an EPSRC grant (EP/L02411X/1)."}], "references": [{"title": "A spectral approach for probabilistic grammatical inference on trees", "author": ["Rapha\u00ebl Bailly", "Amaury Habrard", "Fran\u00e7ois Denis."], "venue": "Proceedings of ALT.", "citeRegEx": "Bailly et al\\.,? 2010", "shortCiteRegEx": "Bailly et al\\.", "year": 2010}, {"title": "A procedure for quantitatively comparing the syntactic coverage of English grammars", "author": ["torini", "Tomek Strzalkowski"], "venue": "In Proceedings of DARPA Workshop on Speech and Natural Language", "citeRegEx": "torini and Strzalkowski.,? \\Q1991\\E", "shortCiteRegEx": "torini and Strzalkowski.", "year": 1991}, {"title": "TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-rich Parsing", "author": ["Xavier Carreras", "Michael Collins", "Terry Koo."], "venue": "Proceedings of CoNLL.", "citeRegEx": "Carreras et al\\.,? 2008", "shortCiteRegEx": "Carreras et al\\.", "year": 2008}, {"title": "Coarseto-fine n-best parsing and maxent discriminative reranking", "author": ["Eugene Charniak", "Mark Johnson."], "venue": "Proceedings of ACL.", "citeRegEx": "Charniak and Johnson.,? 2005", "shortCiteRegEx": "Charniak and Johnson.", "year": 2005}, {"title": "Syntactic parse fusion", "author": ["Do Kook Choe", "David McClosky", "Eugene Charniak."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Choe et al\\.,? 2015", "shortCiteRegEx": "Choe et al\\.", "year": 2015}, {"title": "A provably correct learning algorithm for latent-variable PCFGs", "author": ["Shay B. Cohen", "Michael Collins."], "venue": "Proceedings of ACL.", "citeRegEx": "Cohen and Collins.,? 2014", "shortCiteRegEx": "Cohen and Collins.", "year": 2014}, {"title": "Spectral learning of latent-variable PCFGs", "author": ["Shay B. Cohen", "Karl Stratos", "Michael Collins", "Dean P. Foster", "Lyle Ungar."], "venue": "Proceedings of ACL.", "citeRegEx": "Cohen et al\\.,? 2012", "shortCiteRegEx": "Cohen et al\\.", "year": 2012}, {"title": "Experiments with spectral learning of latent-variable PCFGs", "author": ["Shay B. Cohen", "Karl Stratos", "Michael Collins", "Dean P. Foster", "Lyle Ungar."], "venue": "Proceedings of NAACL.", "citeRegEx": "Cohen et al\\.,? 2013", "shortCiteRegEx": "Cohen et al\\.", "year": 2013}, {"title": "Head-driven statistical models for natural language processing", "author": ["Michael Collins."], "venue": "Computational Linguistics, 29:589\u2013637.", "citeRegEx": "Collins.,? 2003", "shortCiteRegEx": "Collins.", "year": 2003}, {"title": "Eigenwords: Spectral word embeddings", "author": ["Paramveer Dhillon", "Dean Foster", "Lyle Ungar."], "venue": "Journal of Machine Learning Research (to appear).", "citeRegEx": "Dhillon et al\\.,? 2015", "shortCiteRegEx": "Dhillon et al\\.", "year": 2015}, {"title": "What to do when lexicalization fails: Parsing German with suffix analysis and smoothing", "author": ["Amit Dubey."], "venue": "Proceedings of ACL.", "citeRegEx": "Dubey.,? 2005", "shortCiteRegEx": "Dubey.", "year": 2005}, {"title": "Combining constituent parsers", "author": ["Victoria Fossum", "Kevin Knight."], "venue": "Proceedings of HLT-NAACL.", "citeRegEx": "Fossum and Knight.,? 2009", "shortCiteRegEx": "Fossum and Knight.", "year": 2009}, {"title": "Parsing algorithms and metrics", "author": ["Joshua Goodman."], "venue": "Proceedings of ACL.", "citeRegEx": "Goodman.,? 1996", "shortCiteRegEx": "Goodman.", "year": 1996}, {"title": "Exploiting diversity in natural language processing: Combining parsers", "author": ["John C. Henderson", "Eric Brill."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Henderson and Brill.,? 1999", "shortCiteRegEx": "Henderson and Brill.", "year": 1999}, {"title": "A spectral algorithm for learning hidden Markov models", "author": ["Daniel Hsu", "Sham M. Kakade", "Tong Zhang."], "venue": "Proceedings of COLT.", "citeRegEx": "Hsu et al\\.,? 2009", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "The spectral method for general mixture models", "author": ["Ravindran Kannan", "Hadi Salmasian", "Santosh Vempala."], "venue": "Learning Theory, volume 3559 of Lecture Notes in Computer Science, pages 444\u2013457. Springer.", "citeRegEx": "Kannan et al\\.,? 2005", "shortCiteRegEx": "Kannan et al\\.", "year": 2005}, {"title": "An overview of probabilistic tree transducers for natural language processing", "author": ["Kevin Knight", "Jonathan Graehl."], "venue": "Computational linguistics and intelligent text processing, volume 3406 of Lecture Notes in Computer Science, pages 1\u201324.", "citeRegEx": "Knight and Graehl.,? 2005", "shortCiteRegEx": "Knight and Graehl.", "year": 2005}, {"title": "Conversation trees: A grammar model for topic structure in forums", "author": ["Annie Louis", "Shay B. Cohen."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Louis and Cohen.,? 2015", "shortCiteRegEx": "Louis and Cohen.", "year": 2015}, {"title": "Deep multilingual correlation for improved word embeddings", "author": ["Ang Lu", "Weiran Wang", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Proceedings of NAACL.", "citeRegEx": "Lu et al\\.,? 2015", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Spectral learning for nondeterministic dependency parsing", "author": ["Franco M. Luque", "Ariadna Quattoni", "Borja Balle", "Xavier Carreras."], "venue": "Proceedings of EACL.", "citeRegEx": "Luque et al\\.,? 2012", "shortCiteRegEx": "Luque et al\\.", "year": 2012}, {"title": "An empirical study on computing consensus translations from multiple machine translation systems", "author": ["Wolfgang Macherey", "Franz Josef Och."], "venue": "Proceedings of EMNLP-CoNLL.", "citeRegEx": "Macherey and Och.,? 2007", "shortCiteRegEx": "Macherey and Och.", "year": 2007}, {"title": "Building a large annotated corpus of English: The Penn treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary A. Marcinkiewicz."], "venue": "Computational Linguistics, 19:313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "TurboParsers: Dependency parsing by approximate variational inference", "author": ["Andr\u00e9 F.T. Martins", "Noah A. Smith", "Eric P. Xing", "M\u00e1rio A.T. Figueiredo", "Pedro M.Q. Aguiar."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Martins et al\\.,? 2010", "shortCiteRegEx": "Martins et al\\.", "year": 2010}, {"title": "Probabilistic CFG with latent annotations", "author": ["Takuya Matsuzaki", "Yusuke Miyao", "Junichi Tsujii."], "venue": "Proceedings of ACL.", "citeRegEx": "Matsuzaki et al\\.,? 2005", "shortCiteRegEx": "Matsuzaki et al\\.", "year": 2005}, {"title": "Settling the polynomial learnability of mixtures of gaussians", "author": ["Ankur Moitra", "Gregory Valiant."], "venue": "Proceedings of IEEE Symposium on Foundations of Computer Science (FOCS).", "citeRegEx": "Moitra and Valiant.,? 2010", "shortCiteRegEx": "Moitra and Valiant.", "year": 2010}, {"title": "Is your anchor going up or down? Fast and accurate supervised topic models", "author": ["Thang Nguyen", "Jordan Boyd-Graber", "Jeffrey Lund", "Kevin Seppi", "Eric Ringger."], "venue": "Proceedings of NAACL.", "citeRegEx": "Nguyen et al\\.,? 2015", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Improved inference for unlexicalized parsing", "author": ["Slav Petrov", "Dan Klein."], "venue": "Proceedings of HLTNAACL.", "citeRegEx": "Petrov and Klein.,? 2007", "shortCiteRegEx": "Petrov and Klein.", "year": 2007}, {"title": "Learning accurate, compact, and interpretable tree annotation", "author": ["Slav Petrov", "Leon Barrett", "Romain Thibaux", "Dan Klein."], "venue": "Proceedings of COLING-ACL.", "citeRegEx": "Petrov et al\\.,? 2006", "shortCiteRegEx": "Petrov et al\\.", "year": 2006}, {"title": "Products of random latent variable grammars", "author": ["Slav Petrov."], "venue": "Proceedings of HLT-NAACL.", "citeRegEx": "Petrov.,? 2010", "shortCiteRegEx": "Petrov.", "year": 2010}, {"title": "Multiview LSA: Representation learning via generalized CCA", "author": ["Pushpendre Rastogi", "Benjamin Van Durme", "Raman Arora."], "venue": "Proceedings of NAACL.", "citeRegEx": "Rastogi et al\\.,? 2015", "shortCiteRegEx": "Rastogi et al\\.", "year": 2015}, {"title": "Parser combination by reparsing", "author": ["Kenji Sagae", "Alon Lavie."], "venue": "Proceedings of HLT-NAACL.", "citeRegEx": "Sagae and Lavie.,? 2006", "shortCiteRegEx": "Sagae and Lavie.", "year": 2006}, {"title": "Bayesian symbol-refined tree substitution grammars for syntactic parsing", "author": ["Hiroyuki Shindo", "Yusuke Miyao", "Akinori Fujino", "Masaaki Nagata."], "venue": "Proceedings of ACL.", "citeRegEx": "Shindo et al\\.,? 2012", "shortCiteRegEx": "Shindo et al\\.", "year": 2012}, {"title": "An annotation scheme for free word order languages", "author": ["Wojciech Skut", "Brigitte Krenn", "Thorsten Brants", "Hans Uszkoreit."], "venue": "Proceedings of ANLP.", "citeRegEx": "Skut et al\\.,? 1997", "shortCiteRegEx": "Skut et al\\.", "year": 1997}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "A spectral algorithm for learning class-based n-gram models of natural language", "author": ["Karl Stratos", "Do-kyum Kim", "Michael Collins", "Daniel Hsu."], "venue": "Proceedings of UAI.", "citeRegEx": "Stratos et al\\.,? 2014", "shortCiteRegEx": "Stratos et al\\.", "year": 2014}, {"title": "Improving accuracy in word class tagging through the combination of machine learning systems", "author": ["Hans Van Halteren", "Jakub Zavrel", "Walter Daelemans."], "venue": "Computational linguistics, 27(2):199\u2013229.", "citeRegEx": "Halteren et al\\.,? 2001", "shortCiteRegEx": "Halteren et al\\.", "year": 2001}, {"title": "A spectral algorithm for learning mixture models", "author": ["Santosh Vempala", "Grant Wang."], "venue": "Journal of Computer and System Sciences, 68(4):841\u2013860.", "citeRegEx": "Vempala and Wang.,? 2004", "shortCiteRegEx": "Vempala and Wang.", "year": 2004}, {"title": "Feature noising for log-linear structured prediction", "author": ["Sida Wang", "Mengqiu Wang", "Stefan Wager", "Percy Liang", "Christopher D Manning."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Wang et al\\.,? 2013", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "K-best combination of syntactic parsers", "author": ["Hui Zhang", "Min Zhang", "Chew Lim Tan", "Haizhou Li."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Zhang et al\\.,? 2009", "shortCiteRegEx": "Zhang et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 13, "context": "It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999).", "startOffset": 165, "endOffset": 192}, {"referenceID": 20, "context": "Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al.", "startOffset": 42, "endOffset": 66}, {"referenceID": 3, "context": "Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al.", "startOffset": 86, "endOffset": 215}, {"referenceID": 30, "context": "Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al.", "startOffset": 86, "endOffset": 215}, {"referenceID": 11, "context": "Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al.", "startOffset": 86, "endOffset": 215}, {"referenceID": 38, "context": "Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al.", "startOffset": 86, "endOffset": 215}, {"referenceID": 28, "context": "Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al.", "startOffset": 86, "endOffset": 215}, {"referenceID": 4, "context": "Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al.", "startOffset": 86, "endOffset": 215}, {"referenceID": 19, "context": "Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015).", "startOffset": 195, "endOffset": 339}, {"referenceID": 7, "context": "Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015).", "startOffset": 195, "endOffset": 339}, {"referenceID": 34, "context": "Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015).", "startOffset": 195, "endOffset": 339}, {"referenceID": 9, "context": "Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015).", "startOffset": 195, "endOffset": 339}, {"referenceID": 29, "context": "Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015).", "startOffset": 195, "endOffset": 339}, {"referenceID": 25, "context": "Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015).", "startOffset": 195, "endOffset": 339}, {"referenceID": 18, "context": "Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015).", "startOffset": 195, "endOffset": 339}, {"referenceID": 23, "context": "(2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005).", "startOffset": 149, "endOffset": 173}, {"referenceID": 27, "context": "Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006).", "startOffset": 112, "endOffset": 133}, {"referenceID": 26, "context": "We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petrov and Klein (2007). Instead of exploiting diversity from a k-best list from a single model, we estimate multiple models, where the underlying features are perturbed with several perturbation schemes.", "startOffset": 166, "endOffset": 190}, {"referenceID": 6, "context": "First, we present an algorithm for estimating L-PCFGs, akin to the spectral algorithm of Cohen et al. (2012), but simpler to understand and implement.", "startOffset": 89, "endOffset": 109}, {"referenceID": 7, "context": "We add noise to the whole training data, then train a model using our algorithm (or other spectral algorithms; Cohen et al., 2013), and repeat this process multiple times.", "startOffset": 80, "endOffset": 130}, {"referenceID": 35, "context": "Our noise schemes are similar to those described by Wang et al. (2013). We add noise to the whole training data, then train a model using our algorithm (or other spectral algorithms; Cohen et al.", "startOffset": 52, "endOffset": 71}, {"referenceID": 6, "context": "We add noise to the whole training data, then train a model using our algorithm (or other spectral algorithms; Cohen et al., 2013), and repeat this process multiple times. We then use the set of parses we get from all models in a recombination step. The rest of the paper is organized as follows. In \u00a72 we describe notation and background about L-PCFG parsing. In \u00a73 we describe our new spectral algorithm for estimating L-PCFGs. It is based on similar intuitions as older spectral algorithms for L-PCFGs. In \u00a74 we describe the various noise schemes we use with our spectral algorithm and the spectral algorithm of Cohen et al. (2013). In \u00a75 we describe how to decode with multiple models, each arising from a different noise setting.", "startOffset": 111, "endOffset": 635}, {"referenceID": 16, "context": "Latent-variable PCFGs are essentially equivalent to probabilistic regular tree grammars (PRTGs; Knight and Graehl, 2005) where the righthand side trees are of depth 1.", "startOffset": 88, "endOffset": 120}, {"referenceID": 9, "context": "1 This step is akin to CCA, which has been used in various contexts in NLP, mostly to derive representations for words (Dhillon et al., 2015; Rastogi et al., 2015).", "startOffset": 119, "endOffset": 163}, {"referenceID": 29, "context": "1 This step is akin to CCA, which has been used in various contexts in NLP, mostly to derive representations for words (Dhillon et al., 2015; Rastogi et al., 2015).", "startOffset": 119, "endOffset": 163}, {"referenceID": 6, "context": "The features that we use for \u03c6 and \u03c8 are similar to those used in Cohen et al. (2013). These features look at the local neighborhood surrounding a given node.", "startOffset": 66, "endOffset": 86}, {"referenceID": 5, "context": "Cohen et al. (2012) used it for developing a spectral algorithm that identifies the parameters of the L-PCFG up to a linear transformation.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "Cohen et al. (2012) used it for developing a spectral algorithm that identifies the parameters of the L-PCFG up to a linear transformation. Their algorithm generalizes the work of Hsu et al. (2009) and Bailly et al.", "startOffset": 0, "endOffset": 198}, {"referenceID": 0, "context": "(2009) and Bailly et al. (2010).", "startOffset": 11, "endOffset": 32}, {"referenceID": 6, "context": "In our preliminary experiments, we found out that the clustering algorithm by itself performs worse than the spectral algorithm of Cohen et al. (2013). We believe that the reason is two-fold: (a) k-means finds a local maximum during clustering; (b) we do hard clustering instead of soft clustering.", "startOffset": 131, "endOffset": 151}, {"referenceID": 13, "context": "It has been shown that a diverse set of predictions can be used to help improve decoder accuracy for various problems in NLP (Henderson and Brill, 1999).", "startOffset": 125, "endOffset": 152}, {"referenceID": 33, "context": "Our use of dropout noise here is inspired by \u201cdropout\u201d as is used in neural network training, where various connections between units in the neural network are dropped during training in order to avoid overfitting of these units to the data (Srivastava et al., 2014).", "startOffset": 241, "endOffset": 266}, {"referenceID": 37, "context": "The three schemes we described were also used by Wang et al. (2013) to train log-linear models.", "startOffset": 49, "endOffset": 68}, {"referenceID": 28, "context": "the one of Petrov (2010), who builds a committee of latent-variable PCFGs in order to improve a natural language parser.", "startOffset": 11, "endOffset": 25}, {"referenceID": 6, "context": "We also use these perturbation schemes to create multiple models for the algorithm of Cohen et al. (2012). The dropout scheme stays the same, but for the Gaussian noising schemes, we follow a slightly different procedure.", "startOffset": 86, "endOffset": 106}, {"referenceID": 6, "context": "We also use these perturbation schemes to create multiple models for the algorithm of Cohen et al. (2012). The dropout scheme stays the same, but for the Gaussian noising schemes, we follow a slightly different procedure. After noising the projections of the inside and outside feature functions we get from the SVD step, we use these projected noised features as a new set of inside and outside feature functions, and re-run the spectral algorithm of Cohen et al. (2012) on them.", "startOffset": 86, "endOffset": 472}, {"referenceID": 12, "context": "This means we follow Goodman (1996), who uses dynamic programming to compute the tree that maximizes the sum of all marginals of all nonterminals in the output tree.", "startOffset": 21, "endOffset": 36}, {"referenceID": 3, "context": "We use the reranker of Charniak and Johnson (2005).3", "startOffset": 23, "endOffset": 51}, {"referenceID": 7, "context": "53 (Cohen et al., 2013) 86.", "startOffset": 3, "endOffset": 23}, {"referenceID": 7, "context": "47 (Cohen et al., 2013)", "startOffset": 3, "endOffset": 23}, {"referenceID": 6, "context": "The \u201cNo noise\u201d baseline for the spectral algorithm is taken from Cohen et al. (2013). The best figure in each algorithm block is in boldface.", "startOffset": 65, "endOffset": 85}, {"referenceID": 21, "context": "More specifically, we use the Penn WSJ treebank (Marcus et al., 1993) for our experiments, with sections 2\u201321 as the training data, and section 22 used as the development data.", "startOffset": 48, "endOffset": 69}, {"referenceID": 22, "context": "We tag all datasets using Turbo Tagger (Martins et al., 2010), trained on sections 2\u201321.", "startOffset": 39, "endOffset": 61}, {"referenceID": 6, "context": "For the rest of our experiments, both with the spectral algorithm of Cohen et al. (2012) and the clustering algorithm presented in this paper, we use m = 24.", "startOffset": 69, "endOffset": 89}, {"referenceID": 3, "context": "We found out that these oracle scores are comparable to the one Charniak and Johnson (2005) report.", "startOffset": 64, "endOffset": 92}, {"referenceID": 6, "context": "We also tested our oracle results, comparing the spectral algorithm of Cohen et al. (2013) to the clustering algorithm.", "startOffset": 71, "endOffset": 91}, {"referenceID": 6, "context": "It seems that dropout noise for the spectral algorithm acts as a regularizer, similarly to the backoff smoothing techniques that are used in Cohen et al. (2013). This is evident from the two spectral algorithm blocks in Table 1, where dropout noise does not substantially improve the smoothed spectral model (Cohen et al.", "startOffset": 141, "endOffset": 161}, {"referenceID": 29, "context": "For example, Sagae and Lavie (2006), Fossum and Knight (2009) and Zhang et al.", "startOffset": 13, "endOffset": 36}, {"referenceID": 11, "context": "For example, Sagae and Lavie (2006), Fossum and Knight (2009) and Zhang et al.", "startOffset": 37, "endOffset": 62}, {"referenceID": 11, "context": "For example, Sagae and Lavie (2006), Fossum and Knight (2009) and Zhang et al. (2009) report an accuracy of 93.", "startOffset": 37, "endOffset": 86}, {"referenceID": 26, "context": "ing parsing recombination; Shindo et al. (2012) report an accuracy of 92.", "startOffset": 27, "endOffset": 48}, {"referenceID": 24, "context": "4 F1 using a Bayesian tree substitution grammar; Petrov (2010) reports an accuracy of 92.", "startOffset": 49, "endOffset": 63}, {"referenceID": 2, "context": "0% using product of L-PCFGs; Charniak and Johnson (2005) report accuracy of 91.", "startOffset": 29, "endOffset": 57}, {"referenceID": 2, "context": "4 using a discriminative reranking model; Carreras et al. (2008) report 91.", "startOffset": 42, "endOffset": 65}, {"referenceID": 2, "context": "4 using a discriminative reranking model; Carreras et al. (2008) report 91.1 F1 accuracy for a discriminative, perceptron-trained model; Petrov and Klein (2007) report an accuracy of 90.", "startOffset": 42, "endOffset": 161}, {"referenceID": 2, "context": "4 using a discriminative reranking model; Carreras et al. (2008) report 91.1 F1 accuracy for a discriminative, perceptron-trained model; Petrov and Klein (2007) report an accuracy of 90.1 F1. Collins (2003) reports an accuracy of 88.", "startOffset": 42, "endOffset": 207}, {"referenceID": 32, "context": "For the German experiments, we used the NEGRA corpus (Skut et al., 1997).", "startOffset": 53, "endOffset": 72}, {"referenceID": 28, "context": "We use the same setup as in Petrov (2010), and use the first 18,602 sentences as a training set, the next 1,000 sentences as a development set and the last 1,000 sentences as a test set.", "startOffset": 28, "endOffset": 42}, {"referenceID": 3, "context": "For the reranking experiment, we had to modify the BLLIP parser (Charniak and Johnson, 2005) to use the head features from the German treebank.", "startOffset": 64, "endOffset": 92}, {"referenceID": 6, "context": "We compared our oracle results to those given by the spectral algorithm of Cohen et al. (2013). With 20 models for each type of noising scheme, all spectral models combined achieve an oracle accuracy of 83.", "startOffset": 75, "endOffset": 95}, {"referenceID": 28, "context": "For example, Petrov (2010) reports an accuracy of 84.", "startOffset": 13, "endOffset": 27}, {"referenceID": 25, "context": "uct of L-PCFGs; Petrov and Klein (2007) report an accuracy of 80.", "startOffset": 16, "endOffset": 40}, {"referenceID": 10, "context": "1 F1; and Dubey (2005) reports an accuracy of 76.", "startOffset": 10, "endOffset": 23}, {"referenceID": 36, "context": "There are now algorithms, some of which are spectral, that aim to solve this estimation problem with theoretical guarantees (Vempala and Wang, 2004; Kannan et al., 2005; Moitra and Valiant, 2010).", "startOffset": 124, "endOffset": 195}, {"referenceID": 15, "context": "There are now algorithms, some of which are spectral, that aim to solve this estimation problem with theoretical guarantees (Vempala and Wang, 2004; Kannan et al., 2005; Moitra and Valiant, 2010).", "startOffset": 124, "endOffset": 195}, {"referenceID": 24, "context": "There are now algorithms, some of which are spectral, that aim to solve this estimation problem with theoretical guarantees (Vempala and Wang, 2004; Kannan et al., 2005; Moitra and Valiant, 2010).", "startOffset": 124, "endOffset": 195}], "year": 2015, "abstractText": "We describe an approach to create a diverse set of predictions with spectral learning of latent-variable PCFGs (L-PCFGs). Our approach works by creating multiple spectral models where noise is added to the underlying features in the training set before the estimation of each model. We describe three ways to decode with multiple models. In addition, we describe a simple variant of the spectral algorithm for L-PCFGs that is fast and leads to compact models. Our experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38.", "creator": "TeX"}}}