{"id": "1506.00698", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2015", "title": "Statistical Machine Translation Features with Multitask Tensor Networks", "abstract": "We present a three-pronged approach to improving Statistical Machine Translation (SMT), building on recent success in the application of neural networks to SMT. First, we propose new features based on neural networks to model various non-local translation phenomena. Second, we augment the architecture of the neural network with tensor layers that capture important higher-order interaction among the network units (in general, regions of the network) using a model analysis algorithm. Finally, we expand the spatial and temporal layers that we use to construct neural networks to understand spatial data on different spatial and temporal scales.", "histories": [["v1", "Mon, 1 Jun 2015 22:52:36 GMT  (156kb,D)", "http://arxiv.org/abs/1506.00698v1", "11 pages (9 content + 2 references), 2 figures, accepted to ACL 2015 as a long paper"]], "COMMENTS": "11 pages (9 content + 2 references), 2 figures, accepted to ACL 2015 as a long paper", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hendra setiawan", "zhongqiang huang", "jacob devlin", "thomas lamar", "rabih zbib", "richard m schwartz", "john makhoul"], "accepted": true, "id": "1506.00698"}, "pdf": {"name": "1506.00698.pdf", "metadata": {"source": "CRF", "title": "Statistical Machine Translation Features with Multitask Tensor Networks", "authors": ["Hendra Setiawan", "Zhongqiang Huang", "Jacob Devlin", "Thomas Lamar", "Rabih Zbib", "Richard Schwartz", "John Makhoul"], "emails": ["hsetiawa@bbn.com", "zhuang@bbn.com", "tlamar@bbn.com", "rzbib@bbn.com", "schwartz@bbn.com", "makhoul@bbn.com", "jdevlin@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Recent advances in applying Neural Networks to Statistical Machine Translation (SMT) have generally taken one of two approaches. They either develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement the whole translation process as a single neural network (Bahdanau et al., 2014; Sutskever et al., 2014). The latter approach, sometimes referred to as Neural Machine Translation, attempts to overhaul SMT, while the former capitalizes on the strength of the current SMT paradigm and leverages the modeling power of neural networks to improve the scoring of hypotheses generated\n\u2217* Research conducted when the author was at BBN.\nby phrase-based or hierarchical translation rules. This paper adopts the former approach, as n-best scores from state-of-the-art SMT systems often suggest that these systems can still be significantly improved with better features.\nWe build on (Devlin et al., 2014) who proposed a simple yet powerful feedforward neural network model that estimates the translation probability conditioned on the target history and a large window of source word context. We take advantage of neural networks\u2019 ability to handle sparsity, and to infer useful abstract representations automatically. At the same time, we address the challenge of learning the large set of neural network parameters. In particular, \u2022 We develop new Neural Network Features\nto model non-local translation phenomena related to word reordering. Large fullylexicalized contexts are used to model these phenomena effectively, making the use of neural networks essential. All of the features are useful individually, and their combination results in significant improvements (Section 2). \u2022 We use a Tensor Neural Network Architecture\n(Yu et al., 2012) to automatically learn complex pairwise interactions between the network nodes. The introduction of the tensor hidden layer results in more powerful features with lower model perplexity and significantly improved MT performance for all of neural network features (Section 3). \u2022 We apply Multitask Learning (MTL) (Caru-\nana, 1997) to jointly train related neural network features by sharing parameters. This allows parameters learned for one feature to benefit the learning of the other features. This results in better trained models and achieves additional MT improvements (Section 4).\nWe apply the resulting Multitask Tensor Networks to the new features and to existing ones,\nar X\niv :1\n50 6.\n00 69\n8v 1\n[ cs\n.C L\n] 1\nJ un\n2 01\n5\nobtaining strong experimental results over the strongest previous results of (Devlin et al., 2014). We obtain improvements of +2.5 BLEU points for Arabic-English and +1.8 BLEU points for Chinese-English on the DARPA BOLT Web Forum condition. We also obtain improvements of +2.7 BLEU point for Arabic-English and +1.9 BLEU points for Chinese-English on the NIST Open12 test sets over the best previously published results in (Devlin et al., 2014). Both the tensor architecture and multitask learning are general techniques that are likely to benefit other neural network features."}, {"heading": "2 New Non-Local SMT Features", "text": "Existing SMT features typically focus on local information in the source sentence, in the target hypothesis, or both. For example, the n-gram language model (LM) predicts the next target word by using previously generated target words as context (local on target), while the lexical translation model (LTM) predicts the translation of a source word by taking into account surrounding source words as context (local on source).\nIn this work, we focus on non-local translation phenomena that result from non-monotone reordering, where local context becomes non-local on the other side. We propose a new set of powerful MT features that are motivated by this simple idea. To facilitate the discussion, we categorize the features into hypothesis-enumerating features that estimates a probability for each generated target word (e.g., n-gram language model), and sourceenumerating features that estimates a probability for each source word (e.g., lexical translation).\nMore concretely, we introduce a) Joint Model with Offset Source Context (JMO), a hypothesis enumerating feature that predicts the next target word the source context affiliated to the previous target words; and b) Translation Context Model (TCM), a source-enumerating feature that predicts the context of the translation of a source word rather than the translation itself. These two models extend pre-existing features: the Joint (language and translation) Model (JM) of (Devlin et al., 2014) and the LTM respectively respectively. We use a large lexicalized context for there features, making the choice of implementing them as neural networks essential. We also present neuralnetwork implementations of pre-existing sourceenumerating features: lexical translation, orien-\ntation and fertility models. We obtain additional gains from using tensor networks and multitask learning in the modeling and training of all the features."}, {"heading": "2.1 Hypothesis-Enumerating Features", "text": "As mentioned, hypothesis-enumerating features score each word in the hypothesis, typically by conditioning it on a context of n-1 previous target words as in the n-gram language model. One recent such model, the joint model of Devlin et al. (2014) achieves large improvements to the stateof-the-art SMT by using a large context window of 11 source words and 3 target words. The Joint Model with Offset Source Context (JMO) is an extension of the JM that uses the source words affiliated with the n-gram target history as context. The source contexts of JM and JMO overlap highly when the translation is monotone, but are complementary when the translation requires word reordering."}, {"heading": "2.1.1 Joint Model with Offset Source Context", "text": "Formally, JMO estimates the probability of the target hypothesis E conditioned on the source sentence F and a target-to-source affiliation A:\nP (E|F,A) \u2248 |E|\u220f\ni=1\nP (ei|ei\u2212n+1i\u22121 , Cai\u2212k = f ai\u2212k+m ai\u2212k\u2212m )\nwhere ei is the word being predicted; ei\u2212n+1i\u22121 is the string of n\u2212 1 previously generated words; Cai\u2212k to the source context of m source words around fai\u2212k , the source word affiliated with ei\u2212k. We refer to k as the offset parameter. We use the definition of word affiliation introduced in Devlin et al. (2014). When no source context is used, the model is equivalent to an n-gram language model, while an offset parameter of k = 0 reduces the model to the JM of Devlin et al. (2014).\nWhen k > 0, the JMO captures non-local context in the prediction of the next target word. More specifically, ei\u2212k and ei, which are local on the target side, are affiliated to fai\u2212k and fai which may be distant from each other on the source side due to non-monotone translation, even for k = 1. The offset model captures reordering constraints by encouraging the predicted target word ei to fit well with the previous affiliated source word fai\u2212k and its surrounding words. We implement a separate feature for each value of k, and later train\nthem jointly via multitask learning. As our experiments in Section 5.2.1 confirm, the historyaffiliated source context results in stronger SMT improvement than just increasing the number of surrounding words in JM.\nFig. 1 illustrates the difference between JMO and JM. Assuming n = 3 and m = 1, then JM estimates P (e5|e4, e3, Ca5 = {f6, f7, f8}). On the other hand, for k = 1 , JMOk=1 estimates P (e5|e4, e3, Ca4 = {f8, f9, f10}).\nf9f5. . .\ne5 e6e4 e7e3. . . . . .\nC7 = Ca5 . . . \ufe37 \ufe38\ufe38 \ufe37 f6 f7 f8\nFigure 1: Example to illustrate features. f95 is the source segment, e73 is the corresponding translation and lines refer to the alignment. We show hypothesis-enumerating features that look at f7 and source-enumerating features that look at e5. We surround the source words affiliated with e5 and its n-gram history with a bracket, and surround the source words affiliated with the history of e5 with squares."}, {"heading": "2.2 Source-Enumerating Features", "text": "Source-Enumerating Features iterate over words in the source sentence, including unaligned words, and assign it a score depending on what aspect of translation they are modeling. A sourceenumerating feature can be formulated as follows:\nP (E|F,A) \u2248 |F |\u220f\nj=1\nP (Yj |Cj = f j+mj\u2212m )\nwhere Caj is the source context (similar to the hypothesis-enumerating features above) and Yj is the label being predicted by the feature. We first describe pre-existing source-enumerating features: the lexical translation model, the orientation model and the fertility model, and then discuss a new feature: Translation Context Model (TCM), which is an extension of the lexical translation model."}, {"heading": "2.2.1 Pre-existing Features", "text": "Lexical Translation model (LTM) estimates the probability of translating a source word fj to a tar-\nget word l(fj) = ebj given a source context Cj , bj \u2208 B is the source-to-target word affiliation as defined in (Devlin et al., 2014). When fj is translated to more than one word, we arbitrarily keep the left-most one. The target word vocabulary V is extended with a NULL token to accommodate unaligned source words.\nOrientation model (ORI) describes the probability of orientation of the translation of phrases surrounding a source word fj relative to its own translation. We follow (Setiawan et al., 2013) in modeling the orientation of the left and right phrases of fj with maximal orientation span (the longest neighboring phrase consistent with alignment), which we denote by Lj and Rj respectively. Thus, o(fj) = \u3008oLj (fj), oRj (fj)\u3009, where oLj and oRj refer to the orientation of Lj and Rj respectively. For unaligned fj , we set o(fj) = oLj (Rj), the orientation of Rj with respect to Lj .\nFertility model (FM) models the probability that a source word fj generates \u03c6(fj) words in the hypothesis. Our implemented model only distinguishes between aligned and unaligned source words (i.e., \u03c6(fj) \u2208 {0, 1}). The generalization of the model to account for multiple values of \u03c6(fi) is straightforward."}, {"heading": "2.2.2 Translation Context Model", "text": "As with JMO in Section 2.1.1, we aim to capture translation phenomena that appear local on the target hypothesis but non-local on the source side. Here, we do so by extending the LTM feature to predict not only the translated word ebj , but also its surrounding context. Formally, we model P (l(fj)|Cj), where l(fj) = ebj\u2212d, \u00b7 \u00b7 \u00b7 , ebj , \u00b7 \u00b7 \u00b7 ebj+d is the hypothesis word window around ebj . In practice, we decompose TCM further into +d\u220f\nd\u2032=\u2212d P (ebj+d\u2032 |Cj) and imple-\nmented each as a separate neural network-based feature. Note that TCM is equivalent to the LTM when d = 0. Because of word reordering, a given hypothesis word in l(fj) might not be affiliated with fj or even to the words in Cj . TCM can model non-local information in this way."}, {"heading": "2.2.3 Combined Model", "text": "Since the feature label is undefined for unaligned source words, we make the model hierarchical, based on whether the source word is aligned or\nnot, and thus arrive at the following formulation:\nP (l(fj)) \u00b7 P (ori(fj)) \u00b7 P (\u03c6(fj)) =   P (\u03c6p(fj) = 0) \u00b7 P (oLj (Rj)) P (\u03c6p(fj) \u2265 1) \u00b7 +d\u220f d\u2032=\u2212d P (ebj+d\u2032)\n\u00b7P (oLj (fj), oRj (fj)) We dropped the common context (Cj) for readability.\nWe reuse Fig. 1 to illustrate the sourceenumerating features. Assuming d = 1, the scores associated with f7 are P (\u03c6(f7) \u2265 1|C7) for the FM; P (e4|C7) \u00b7P (e5|C7) \u00b7P (e6)|C7) for the TCM; and P (o(f7) = \u3008oL7(f7) = RA, oR7(f7) = RA\u3009) for the ORI(RA refers to Reverse Adjacent). L7 and R7 (i.e. f6 and f98 respectively), the longest neighboring phrase of f7, are translated in reverse order and adjacent to e5."}, {"heading": "3 Tensor Neural Networks", "text": "The second part of this work improves SMT by improving the neural network architecture. Neural Networks derive their strength from their ability to learn a high-level representation of the input automatically from data. This high-level representation is typically constructed layer by layer through a weighted sum linear operation and a non-linear activation function. With sufficient training data, neural networks often achieve state-of-the-art performance on many tasks. This stands in sharp contrast to other algorithms that require tedious manual feature engineering. For the features presented in this paper, the context words are fed to the network network with minimal engineering.\nWe further strengthen the network\u2019s ability to learn rich interactions between its units by introducing tensors in the hidden layers. The multiplicative property of the tensor bares a close resemblance to collocation of context words which are useful in many natural language processing tasks.\nIn conventional feedforward neural networks, the output of hidden layer l is produced by multiplying the output vector from the previous layer with a weight matrix (Wl) and then applying the activation function \u03c3 to the product. Tensor Neural Networks generalize this formulation by using a tensor Ul of order 3 for the weights. The output of node k in layer l is computed as follows:\nhl[k] = \u03c3 ( hl\u22121 \u00b7 Ul[k] \u00b7 hTl\u22121 )\nwhere Ul[k], the k-th slice of Ul, is a square matrix.\nIn our implementation, we follow (Yu et al., 2012; Hutchinson et al., 2013) and use a low-rank approximation of Ul[k] = Ql[k] \u00b7 Rl[k]T , where Ql[k], Rl[k] \u2208 Rn\u00d7r. The output of node k becomes:\nhl[k] = \u03c3 ( hl\u22121 \u00b7Ql[k] \u00b7Rl[k]T \u00b7 hTl\u22121 )\nIn our experiments, we choose r = 1, and also apply the non-linear activation function \u03c3 distributively. We arrive at the following three equations for computing the hidden layer outputs (0 < l < L):\nvl = \u03c3 (hl\u22121 \u00b7Ql) v\u2032l = \u03c3 (hl\u22121 \u00b7Rl) hl = vl \u2297 v\u2032l\nwhere hl\u22121 is double-projected to vl and v\u2032l, and the two projections are merged using the Hadamard element-wise product operator \u2297.\nThis formulation allows us to use the same infrastructure of the conventional neural networks by projecting the previous layer to two different spaces of the same dimensions, then multiplying them element-wise. The only component that is different from conventional feedforward neural networks is the multiplicative function, which is trivially differentiable with respect to the learnable parameters. Figure 3(b) illustrates the tensor architecture for two hidden layers.\nThe tensor network can learn collocation features more easily. For example, it can learn a collocation feature that is activated only if hl\u22121[i] collocates with hl\u22121[j] by setting Ul[k][i][j] to some positive number. This results in SMT improvements as we describe in Section 5."}, {"heading": "4 Multitask Learning", "text": "The third part of this paper addresses the challenge of effectively learning a large number of neural network parameters without overfitting. The challenge is even larger for tensor network since they practically doubles the number of parameters. In this section, we propose to apply Multitask Learning (MTL) to partially address this issue. We implement MTL as parameter sharing among the networks. This effectively reduces the number of parameters, and more importantly, it takes advantage of parameters learned for one feature to better\nlearn the parameters of the other features. Another way of looking at this is that MTL facilitates regularization through learning the other tasks.\nMTL is suitable for SMT features as they model different but closely related aspects of the same translation process. MTL has long been used by the wider machine learning community (Caruana, 1997) and more recently for natural language processing (Collobert and Weston, 2008; Collobert et al., 2011). The application of MTL to machine translation, however, has been much less restricted, which is rather surprising since SMT features arise from the same translation task and are naturally related.\nWe apply MTL for the features described in Section 2. We design all the features to also share the same neural network architecture (in this case, the tensor architecture described in Section 3) and the same input, thus resulting in two large neural networks: one for the hypothesis-enumerating features and another for the source-enumerating ones. This simplifies the implementation of MTL. Using this setup, it is possible to vary the number of shared hidden layers t from 0 (only sharing the embedding layer) to L \u2212 1 (sharing all the layers except the output). Note that in principle MTL is applicable to other set of networks that have different architecture or even different input set. With MTL, the training procedure is the same as that of standard neural networks.\nWe use the back propagation algorithm, and use as the loss function the product of likelihood of each feature1:\n1In this and in the other parts of the paper, we add the normalization regularization term described in (Devlin et al., 2014) to the loss function to avoid computing the normalization constant at model query/decoding time.\nLoss = \u2211\ni\nM\u2211\nj\nlog (P (Yj(Xi)))\nwhere Xi is the training sample and Yj is one of theM models trained. We use the sum of log likelihoods since we assume that the features are independent.\nFig. 3(c) illustrates MTL between M models sharing the input embedding layer and the first hidden layer (t = 1) compared to the separatelytrained conventional feedforward neural network and tensor neural network."}, {"heading": "5 Experiments", "text": "We demonstrate the impact of our work with extensive MT experiments on Arabic-English and Chinese-English translation for the DARPA BOLT Web Forum and the NIST OpenMT12 conditions."}, {"heading": "5.1 Baseline MT System", "text": "We run our experiments using a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010). The baseline we use includes a set of powerful features as follow: \u2022 Forward and backward rule probabilities \u2022 Contextual lexical smoothing (Devlin, 2009) \u2022 5-gram Kneser-Ney LM \u2022 Dependency LM (Shen et al., 2010) \u2022 Length distribution (Shen et al., 2010) \u2022 Trait features (Devlin and Matsoukas, 2012) \u2022 Factored source syntax (Huang et al., 2013) \u2022 Discriminative sparse feature, totaling 50k\nfeatures (Chiang et al., 2009) \u2022 Neural Network Joint Model (NNJM) and\nNeural Network Lexical Translation Model\n(NNLTM) (Devlin et al., 2014) As shown, our baseline system already includes neural network-based features. NNJM, NNLTM and use two hidden layers with 500 units and use embedding of size 200 for each input.\nWe use the MADA-ARZ tokenizer (Habash et al., 2013) for Arabic word tokenization. For Chinese tokenization, we use a simple longest-matchfirst lexicon-based approach. We align the training data using GIZA++ (Och and Ney, 2003). For tuning the weights of MT features including the new features, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010), and decode the test sets after 5 tuning iteration. We report the lower-cased BLEU and TER scores."}, {"heading": "5.2 BOLT Discussion Forum", "text": "The bulk of our experiments is on the BOLT Web Discussion Forum domain, which uses data collected by the LDC. The parallel training data consists of all of the high-quality NIST training corpora, plus an additional 3 million words of translated forum data. The tuning and test sets consist of roughly 5000 segments each, with 2 independent references for Arabic and 3 for Chinese."}, {"heading": "5.2.1 Effects of New Features", "text": "We first look at the effects of the proposed features compared to the baseline system. Table 1 summarizes the primary results of the Arabic-English and Chinese-English experiments for the BOLT condition. We show the experimental results related to hypothesis-enumerating features (HypEn) in rows S2-S5, those related to source-enumerating features (SrcEn) in rows S6-S9, and the combination of the two in row S10. For all the features, we set the source context length to m = 5 (11-word window). For JM and JMO, we set the target context length to n = 4. For the offset parameter k of JMO, we use values 1 to 3. For TCM, we model one word around the translation (d = 1). Larger values of d did not result in further gains. The baseline is comparable to the best results of (Devlin et al., 2014).\nIn rows S3 to S5, we incrementally add a model with different offset source context, from k = 1 to k = 3. For AR-EN, adding JMOs with different offset source context consistently yields positive effects in BLEU score, while in ZH-EN, it yields positive effects in TER score. Utilizing all offset source contexts \u201c+JMOk\u22643\u201d (row S5) yields\naround 0.9 BLEU point improvement in AR-EN and around 0.3 BLEU in ZH-EN compared to the baseline. The JMO consistently provides better improvement compared to a larger JM context (row S2), validating our hypothesis that using offset source context captures important non-local context.\nRows S6 to S9 present the improvements that result from implementing pre-existing sourceenumerating SMT features as neural networks, and highlight the contribution of our translation context model (TCM). This set of experiments is orthogonal to the HypEn experiments (rows S2S5). Each pre-existing model has a modest positive cumulative effect on both BLEU and TER. We see this result as further confirming the current trend of casting existing SMT features as neural network since our baseline already contains such features. The next row present the results of adding the translation context model, with one word surrounding the translation (d = 1). As shown, TCM yields a positive effect of around 0.5 BLEU and TER improvements in AR-EN and around 0.2 BLEU and TER improvements in ZHEN.\nSeparately, the set of source-enumerating features and the set of target-enumerating features produce around 1.1 to 1.2 points BLEU gain in AR-EN and 0.3 to 0.5 points BLEU gain in ZHEN. The combination of the two sets produces a complementary gain in addition to the gains of the individual models as Row (S10) shows. The combined gain improves to 1.5 BLEU points in AREN and 0.7 BLEU points in ZH-EN."}, {"heading": "5.2.2 Effects of Tensor Network and Multitask Learning", "text": "We first analyze the impact of tensor architecture and MTL intrinsically by reporting the models\u2019 average log-likelihood on the validation sets (a subset of the test set) in Table 2. As mentioned, we group the models to HypEn (JM and JMOk\u22643) and SrcEn (LTM, ORI,FERT and TCM) as we perform MTL on these two groups. Likelihood of these two groups in the previous subsection are in column \u201cNN\u201d (for Neural Network), which serves as a baseline. The application of the tensor architecture improves their likelihood as shown in column \u201cTensor\u201d for both languages and models.\nThe likelihoods of the MTL-related experiments are in columns with \u201cMTL\u201d header. We present two set of results. In the first set (column \u201cMTL,t=0,L=2\u201d), we run MTL for features from column \u201cTensor\u201d by sharing the embedding layer only (t = 0). This allows us to isolate the impact of MTL in the presence of Tensors. Column \u201cMTL,t=1,l=3\u201d corresponds to the experiment that produces the best intrinsic result, where each model uses Tensors with three hidden layers (500x500x500, l = 3) and the models share the embedding and the first hidden layers (t = 1). MTL consistently gives further intrinsic gain compared to tensors. More sharing provides an extra gain for SrcEn as shown in the last column. Note that we only experiment with different l and t for SrcEn and not for HypEn because the models in HypEn have different input sets. In our experiments, further sharing and more hidden layers resulted in no further gain. In total, we see a consistent positive effect in intrinsic evaluation from the tensor networks and multitask learning.\nMoving on to MT evaluation, we summarize the\nexperiments showing the impact of Tensors and MTL in Table 3. For MTL, we use L = 3, t = 2 since it gives the best intrinsic score. Employing tensors instead of regular neural networks gives a significant and consistent positive impact for all models and language pairs. For the system with the baseline features, we use the tensor architecture for both the joint model and the lexical translation model of Devlin et al. resulting in an improvement of around 0.7 BLEU points, and showing the wide applicability of the tensor architecture. On top of this improved baseline, we also observe an improvement of the same scale for other models (column \u201cTensor\u201d), except for HypEn features in AR-EN experiment. Moving to MTL experiments, we see improvements, especially from SrcEn features. MTL gives around 0.5 BLEU point improvement for AR-EN and around 0.4 BLEU point for ZH-EN. When we employ both HypEn and SrcEn together, MTL gives around 0.4 BLEU point in AR-EN and 0.2 BLEU point in ZH-EN. In total, our work results in an improvement of 2.5 BLEU point for AR-EN and 1.8 for BLEU point in ZH-EN on top of the best results in (Devlin et al., 2014)."}, {"heading": "5.3 NIST OpenMT12", "text": "Our NIST system is compatible with the OpenMT12 constrained track, which consists of 10M words of high-quality parallel training for Arabic, and 25M words for Chinese. The n-gram LM is trained on 5B words of data from the English GigaWord. For test, we use the \u201cArabic-ToEnglish Original Progress Test\u201d (1378 segments) and \u201cChinese-to-English Original Progress Test + OpenMT12 Current Test\u201d (2190 segments), which consists of a mix of newswire and web data. All test segments have 4 references. Our tuning set contains 5000 segments, and is a mix of the MT02-05 eval set as well as additional held-out parallel data from the training corpora.\nWe report the experiments for the NIST condition in Table 4. In particular, we investigate the impact of deploying our new features (column \u201cFeat\u201d) and demonstrate the effects of the tensor architecture (column \u201cTensor\u201d) and multitask learning (column \u201cMTL\u201d). As shown the results are inline with the BOLT condition where we observe additive improvements from adding our new features, applying tensor network and multitask learning. On Arabic-English, we see a gain of 2.7\nBLEU point and on Chinese-English, we see a 1.9 BLEU point gain. We also report the mixed-cased BLEU scores for comparison with previous best published results, i.e. Devlin et al. (2014) report 52.8 BLEU for Arabic-English and 34.7 BLEU for Chinese-English. Thus, our results are around 1.3- 1.4 BLEU point better. Note that they use additional rescoring features but we do not."}, {"heading": "6 Related Work", "text": "Our work is most closely related to Devlin et al. (2014). They use a simple feedforward neural network to model two important MT features: A joint language and translation model, and a lexical translation model. They show very large improvements on Arabic-English and ChineseEnglish web forum and newswire baselines. We improve on their work in 3 aspects. First, we model more features using neural networks, including two novel ones: a joint model with offset source context and a translation context model. Second, we enhance the neural network architecture by using tensor layers, which allows us to model richer interactions. Lastly, we improve the performance of the individual features by training them using multitask learning. In the remainder of this section, we describe previous work relating to the three aspect of our work, namely MT modeling, neural network architecture and model learning.\nThe features we propose in this paper address the major aspects of SMT modeling that have informed much of the research since the original IBM models (Brown et al., 1993): lexical translation, reordering, word fertility, and language models. Of particular relevance to our work are approaches that incorporate context-sensitivity into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al., 2003; Schwenk, 2010; Schwenk, 2012), and incorporate source-side context into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012).\nApproaches to incorporating source context into a neural network model differ mainly in how they represent the source sentence and in how long is the history they keep. In terms of representation of the source sentence, we follow (Devlin et al., 2014) in using a window around the affiliated source word. To name some other approaches, Auli et al. (2013) uses latent semantic analysis and source sentence embeddings learned from the recurrent neural network; Sundermeyer et al. (2014) take the representation from a bidirectional LSTM recurrent neural network; and Kalchbrenner and Blunsom (2013) employ a convolutional sentence model. For target context, recent work has tried to look beyond the classical n-gram history. (Auli et al., 2013; Sundermeyer et al., 2014) consider an unbounded history, at the expense of making their model only applicable for N-best rescoring. Another recent line of research (Bahdanau et al., 2014; Sutskever et al., 2014) departs more radically from conventional feature-based SMT and implements the MT system as a single neural network. These models use a representation of the whole input sentence.\nWe use a feedforward neural network in this work. Besides feedforward and recurrent net-\nworks, other network architectures that have been applied to SMT include convolutional networks (Kalchbrenner et al., 2014) and recursive networks (Socher et al., 2011). The simplicity of feedforward networks works to our advantage. More specifically, due to the absence of a feedback loop, the feedforward architecture allows us to treat individual decisions independently, which makes parallelization of the training easy and the querying the network at decoding time straightforward. The use of tensors in the hidden layers strengthens the neural network model, allowing us to model more complex feature interactions like collocation, which has been long recognized as important information for many NLP tasks (e.g. word sense disambiguation (Lee and Ng, 2002)). The tensor formulation we use is similar to that of (Yu et al., 2012; Hutchinson et al., 2013). Tensor Neural Networks have a wide application in other field, but have only been recently applied in NLP (Socher et al., 2013; Pei et al., 2014). To our knowledge, our work is the first to use tensor networks in SMT.\nOur approach to multitask learning is related to work that is often labeled joint training or transfer learning. To name a few of these works, Finkel and Manning (2009) successfully train name entity recognizers and syntactic parsers jointly, and Singh et al. (2013) train models for coreference resolution, named entity recognition and relation extraction jointly. Both efforts are motivated by the minimization of cascading errors. Our work is most closely related to Collobert and Weston (2008; Collobert et al. (2011), who apply multitask learning to train neural networks for multiple NLP models: part-of-speech tagging, semantic role labeling, named-entity recognition and language model variations."}, {"heading": "7 Conclusion", "text": "This paper argues that a relatively simple feedforward neural network can still provides significant improvement to Statistical Machine Translation (SMT). We support this argument by presenting a multi-pronged approach that addresses modeling, architectural and learning aspects of pre-existing neural network-based SMT features. More concretely, we paper present a new set of neural network-based SMT features to capture important translation phenomena, extend feedforward neural network with tensor layers, and apply multi-\ntask learning to integrate the SMT features more tightly. Empirically, all our proposals successfully produce an improvement over state-of-the-art machine translation system for Arabic-to-English and Chinese-to-English and for both BOLT web forum and NIST conditions. Building on the success of this paper, we plan to develop other neuralnetwork-based features, and to also relax the limiteation of current rule extraction heuristics by generating translations word-by-word."}, {"heading": "Acknowledgement", "text": "This work was supported by DARPA/I2O Contract No. HR0011-12-C-0014 under the BOLT Program. The views, opinions, and/or findings contained in this article are those of the author and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense."}], "references": [{"title": "Joint language and translation modeling with recurrent neural networks", "author": ["Auli et al.2013] Michael Auli", "Michel Galley", "Chris Quirk", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Auli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "Technical Report 1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Brown et al.1993] Peter F. Brown", "Vincent J. Della Pietra", "Stephen A. Della Pietra", "Robert L. Mercer"], "venue": "Comput. Linguist.,", "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Improving statistical machine translation using word sense disambiguation", "author": ["Carpuat", "Wu2007] Marine Carpuat", "Dekai Wu"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Com-", "citeRegEx": "Carpuat et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Carpuat et al\\.", "year": 2007}, {"title": "11,001 new features for statistical machine translation", "author": ["Chiang et al.2009] David Chiang", "Kevin Knight", "Wei Wang"], "venue": "In HLT-NAACL,", "citeRegEx": "Chiang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chiang et al\\.", "year": 2009}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Trait-based hypothesis selection for machine translation", "author": ["Devlin", "Matsoukas2012] Jacob Devlin", "Spyros Matsoukas"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association", "citeRegEx": "Devlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2012}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin et al.2014] Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul"], "venue": null, "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Lexical features for statistical machine translation. Master\u2019s thesis, University of Maryland", "author": ["Jacob Devlin"], "venue": null, "citeRegEx": "Devlin.,? \\Q2009\\E", "shortCiteRegEx": "Devlin.", "year": 2009}, {"title": "Joint parsing and named entity recognition", "author": ["Finkel", "Manning2009] Jenny Rose Finkel", "Christopher D. Manning"], "venue": "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter", "citeRegEx": "Finkel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2009}, {"title": "Morphological analysis and disambiguation for dialectal arabic", "author": ["Habash et al.2013] Nizar Habash", "Ryan Roth", "Owen Rambow", "Ramy Eskander", "Nadi Tomeh"], "venue": "In HLT-NAACL,", "citeRegEx": "Habash et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Habash et al\\.", "year": 2013}, {"title": "Factored soft source syntactic constraints for hierarchical machine translation", "author": ["Jacob Devlin", "Rabih Zbib"], "venue": "In EMNLP,", "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Tensor deep stacking networks", "author": ["Li Deng", "Dong Yu"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Hutchinson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hutchinson et al\\.", "year": 2013}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Continuous space translation models with neural networks", "author": ["Le et al.2012] Hai-Son Le", "Alexandre Allauzen", "Fran\u00e7ois Yvon"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguis-", "citeRegEx": "Le et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Le et al\\.", "year": 2012}, {"title": "An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation", "author": ["Lee", "Ng2002] Yoong Keok Lee", "Hwee Tou Ng"], "venue": "In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language", "citeRegEx": "Lee et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2002}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Och", "Ney2003] Franz Josef Och", "Hermann Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Och et al\\.", "year": 2003}, {"title": "Max-margin tensor neural network for chinese word segmentation", "author": ["Pei et al.2014] Wenzhe Pei", "Tao Ge", "Baobao Chang"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Pei et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pei et al\\.", "year": 2014}, {"title": "BBN system description for WMT10 system combination task", "author": ["Rosti et al.2010] Antti Rosti", "Bing Zhang", "Spyros Matsoukas", "Rich Schwartz"], "venue": "In WMT/MetricsMATR,", "citeRegEx": "Rosti et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rosti et al\\.", "year": 2010}, {"title": "Continuousspace language models for statistical machine translation", "author": ["Holger Schwenk"], "venue": "Prague Bull. Math. Linguistics,", "citeRegEx": "Schwenk.,? \\Q2010\\E", "shortCiteRegEx": "Schwenk.", "year": 2010}, {"title": "Continuous space translation models for phrase-based statistical machine translation", "author": ["Holger Schwenk"], "venue": "In COLING (Posters),", "citeRegEx": "Schwenk.,? \\Q2012\\E", "shortCiteRegEx": "Schwenk.", "year": 2012}, {"title": "Two-neighbor orientation model with cross-boundary global contexts", "author": ["Bowen Zhou", "Bing Xiang", "Libin Shen"], "venue": "In Proceedings of the 51st Annual Meeting", "citeRegEx": "Setiawan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Setiawan et al\\.", "year": 2013}, {"title": "String-to-dependency statistical machine translation", "author": ["Shen et al.2010] Libin Shen", "Jinxi Xu", "Ralph Weischedel"], "venue": "Computational Linguistics,", "citeRegEx": "Shen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2010}, {"title": "Joint inference of entities, relations, and coreference", "author": ["Singh et al.2013] Sameer Singh", "Sebastian Riedel", "Brian Martin", "Jiaping Zheng", "Andrew McCallum"], "venue": "In Proceedings of the 2013 Workshop on Automated Knowledge Base Construction,", "citeRegEx": "Singh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2013}, {"title": "Language and translation model adaptation using comparable corpora", "author": ["Bonnie Dorr", "Richard Schwartz"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Snover et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snover et al\\.", "year": 2008}, {"title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks", "author": ["Cliff C. Lin", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Proceedings of the 26th International Conference on Machine Learning", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Translation modeling with bidirectional recurrent neural networks", "author": ["Tamer Alkhouli", "Joern Wuebker", "Hermann Ney"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language", "citeRegEx": "Sundermeyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A unigram orientation model for statistical machine translation", "author": ["Christoph Tillman"], "venue": "HLT-NAACL 2004: Short Papers,", "citeRegEx": "Tillman.,? \\Q2004\\E", "shortCiteRegEx": "Tillman.", "year": 2004}, {"title": "Large vocabulary speech recognition", "author": ["Yu et al.2012] Dong Yu", "Li Deng", "Frank Seide"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 30, "context": "ther develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement", "startOffset": 125, "endOffset": 223}, {"referenceID": 9, "context": "ther develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement", "startOffset": 125, "endOffset": 223}, {"referenceID": 0, "context": "ther develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement", "startOffset": 125, "endOffset": 223}, {"referenceID": 17, "context": "ther develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement", "startOffset": 125, "endOffset": 223}, {"referenceID": 23, "context": "ther develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement", "startOffset": 125, "endOffset": 223}, {"referenceID": 1, "context": "the whole translation process as a single neural network (Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 57, "endOffset": 104}, {"referenceID": 31, "context": "the whole translation process as a single neural network (Bahdanau et al., 2014; Sutskever et al., 2014).", "startOffset": 57, "endOffset": 104}, {"referenceID": 9, "context": "We build on (Devlin et al., 2014) who proposed a simple yet powerful feedforward neural network model that estimates the translation probability conditioned on the target history and a large win-", "startOffset": 12, "endOffset": 33}, {"referenceID": 33, "context": "\u2022 We use a Tensor Neural Network Architecture (Yu et al., 2012) to automatically learn complex pairwise interactions between the network nodes.", "startOffset": 46, "endOffset": 63}, {"referenceID": 9, "context": "obtaining strong experimental results over the strongest previous results of (Devlin et al., 2014).", "startOffset": 77, "endOffset": 98}, {"referenceID": 9, "context": "9 BLEU points for Chinese-English on the NIST Open12 test sets over the best previously published results in (Devlin et al., 2014).", "startOffset": 109, "endOffset": 130}, {"referenceID": 8, "context": "One recent such model, the joint model of Devlin et al. (2014) achieves large improvements to the stateof-the-art SMT by using a large context window of 11 source words and 3 target words.", "startOffset": 42, "endOffset": 63}, {"referenceID": 8, "context": "We use the definition of word affiliation introduced in Devlin et al. (2014). When no source context is used, the model is equivalent to an n-gram language model, while an offset parameter of k = 0 reduces the model to the JM of Devlin et al.", "startOffset": 56, "endOffset": 77}, {"referenceID": 8, "context": "We use the definition of word affiliation introduced in Devlin et al. (2014). When no source context is used, the model is equivalent to an n-gram language model, while an offset parameter of k = 0 reduces the model to the JM of Devlin et al. (2014).", "startOffset": 56, "endOffset": 250}, {"referenceID": 9, "context": "Lexical Translation model (LTM) estimates the probability of translating a source word fj to a target word l(fj) = ebj given a source context Cj , bj \u2208 B is the source-to-target word affiliation as defined in (Devlin et al., 2014).", "startOffset": 209, "endOffset": 230}, {"referenceID": 24, "context": "We follow (Setiawan et al., 2013) in modeling the orientation of the left and right phrases of fj with maximal orientation span (the longest neighboring phrase consistent with alignment), which we denote by Lj and Rj respectively.", "startOffset": 10, "endOffset": 33}, {"referenceID": 33, "context": "In our implementation, we follow (Yu et al., 2012; Hutchinson et al., 2013) and use a low-rank approximation of Ul[k] = Ql[k] \u00b7 Rl[k] , where Ql[k], Rl[k] \u2208 Rn\u00d7r.", "startOffset": 33, "endOffset": 75}, {"referenceID": 14, "context": "In our implementation, we follow (Yu et al., 2012; Hutchinson et al., 2013) and use a low-rank approximation of Ul[k] = Ql[k] \u00b7 Rl[k] , where Ql[k], Rl[k] \u2208 Rn\u00d7r.", "startOffset": 33, "endOffset": 75}, {"referenceID": 7, "context": "the wider machine learning community (Caruana, 1997) and more recently for natural language processing (Collobert and Weston, 2008; Collobert et al., 2011).", "startOffset": 103, "endOffset": 155}, {"referenceID": 9, "context": "In this and in the other parts of the paper, we add the normalization regularization term described in (Devlin et al., 2014) to the loss function to avoid computing the normalization constant at model query/decoding time.", "startOffset": 103, "endOffset": 124}, {"referenceID": 25, "context": "We run our experiments using a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010).", "startOffset": 90, "endOffset": 109}, {"referenceID": 10, "context": "The baseline we use includes a set of powerful features as follow: \u2022 Forward and backward rule probabilities \u2022 Contextual lexical smoothing (Devlin, 2009) \u2022 5-gram Kneser-Ney LM \u2022 Dependency LM (Shen et al.", "startOffset": 140, "endOffset": 154}, {"referenceID": 25, "context": "The baseline we use includes a set of powerful features as follow: \u2022 Forward and backward rule probabilities \u2022 Contextual lexical smoothing (Devlin, 2009) \u2022 5-gram Kneser-Ney LM \u2022 Dependency LM (Shen et al., 2010) \u2022 Length distribution (Shen et al.", "startOffset": 194, "endOffset": 213}, {"referenceID": 25, "context": ", 2010) \u2022 Length distribution (Shen et al., 2010) \u2022 Trait features (Devlin and Matsoukas, 2012) \u2022 Factored source syntax (Huang et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 13, "context": ", 2010) \u2022 Trait features (Devlin and Matsoukas, 2012) \u2022 Factored source syntax (Huang et al., 2013) \u2022 Discriminative sparse feature, totaling 50k features (Chiang et al.", "startOffset": 79, "endOffset": 99}, {"referenceID": 5, "context": ", 2013) \u2022 Discriminative sparse feature, totaling 50k features (Chiang et al., 2009) \u2022 Neural Network Joint Model (NNJM) and Neural Network Lexical Translation Model", "startOffset": 63, "endOffset": 84}, {"referenceID": 9, "context": "(NNLTM) (Devlin et al., 2014)", "startOffset": 8, "endOffset": 29}, {"referenceID": 12, "context": "We use the MADA-ARZ tokenizer (Habash et al., 2013) for Arabic word tokenization.", "startOffset": 30, "endOffset": 51}, {"referenceID": 21, "context": "For tuning the weights of MT features including the new features, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010), and decode the test sets after 5 tuning iteration.", "startOffset": 143, "endOffset": 163}, {"referenceID": 9, "context": "The baseline is comparable to the best results of (Devlin et al., 2014).", "startOffset": 50, "endOffset": 71}, {"referenceID": 9, "context": "BLEU point in ZH-EN on top of the best results in (Devlin et al., 2014).", "startOffset": 50, "endOffset": 71}, {"referenceID": 8, "context": "Devlin et al. (2014) report 52.", "startOffset": 0, "endOffset": 21}, {"referenceID": 8, "context": "Our work is most closely related to Devlin et al. (2014). They use a simple feedforward neural network to model two important MT features: A joint language and translation model, and a lexical translation model.", "startOffset": 36, "endOffset": 57}, {"referenceID": 3, "context": "The features we propose in this paper address the major aspects of SMT modeling that have informed much of the research since the original IBM models (Brown et al., 1993): lexical translation, reordering, word fertility, and language models.", "startOffset": 150, "endOffset": 170}, {"referenceID": 32, "context": "into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al.", "startOffset": 92, "endOffset": 107}, {"referenceID": 2, "context": "into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al., 2003; Schwenk, 2010; Schwenk, 2012), and incorporate source-side con-", "startOffset": 152, "endOffset": 203}, {"referenceID": 22, "context": "into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al., 2003; Schwenk, 2010; Schwenk, 2012), and incorporate source-side con-", "startOffset": 152, "endOffset": 203}, {"referenceID": 23, "context": "into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al., 2003; Schwenk, 2010; Schwenk, 2012), and incorporate source-side con-", "startOffset": 152, "endOffset": 203}, {"referenceID": 9, "context": "text into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012).", "startOffset": 15, "endOffset": 87}, {"referenceID": 0, "context": "text into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012).", "startOffset": 15, "endOffset": 87}, {"referenceID": 17, "context": "text into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012).", "startOffset": 15, "endOffset": 87}, {"referenceID": 23, "context": "text into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012).", "startOffset": 15, "endOffset": 87}, {"referenceID": 9, "context": "tion of the source sentence, we follow (Devlin et al., 2014) in using a window around the affiliated source word.", "startOffset": 39, "endOffset": 60}, {"referenceID": 0, "context": "To name some other approaches, Auli et al. (2013) uses latent semantic analysis and source sentence embeddings learned from the recurrent neural network; Sundermeyer et al.", "startOffset": 31, "endOffset": 50}, {"referenceID": 0, "context": "To name some other approaches, Auli et al. (2013) uses latent semantic analysis and source sentence embeddings learned from the recurrent neural network; Sundermeyer et al. (2014) take the representation from a bidirectional LSTM recurrent neural network; and Kalchbrenner and Blunsom (2013) employ a convolutional sentence model.", "startOffset": 31, "endOffset": 180}, {"referenceID": 0, "context": "To name some other approaches, Auli et al. (2013) uses latent semantic analysis and source sentence embeddings learned from the recurrent neural network; Sundermeyer et al. (2014) take the representation from a bidirectional LSTM recurrent neural network; and Kalchbrenner and Blunsom (2013) employ a convolutional sentence model.", "startOffset": 31, "endOffset": 292}, {"referenceID": 0, "context": "(Auli et al., 2013; Sundermeyer et al., 2014) consider an unbounded history, at the expense of making their model only applicable for N-best rescoring.", "startOffset": 0, "endOffset": 45}, {"referenceID": 30, "context": "(Auli et al., 2013; Sundermeyer et al., 2014) consider an unbounded history, at the expense of making their model only applicable for N-best rescoring.", "startOffset": 0, "endOffset": 45}, {"referenceID": 16, "context": "works, other network architectures that have been applied to SMT include convolutional networks (Kalchbrenner et al., 2014) and recursive networks (Socher et al.", "startOffset": 96, "endOffset": 123}, {"referenceID": 28, "context": ", 2014) and recursive networks (Socher et al., 2011).", "startOffset": 31, "endOffset": 52}, {"referenceID": 33, "context": "(Yu et al., 2012; Hutchinson et al., 2013).", "startOffset": 0, "endOffset": 42}, {"referenceID": 14, "context": "(Yu et al., 2012; Hutchinson et al., 2013).", "startOffset": 0, "endOffset": 42}, {"referenceID": 29, "context": "Tensor Neural Networks have a wide application in other field, but have only been recently applied in NLP (Socher et al., 2013; Pei et al., 2014).", "startOffset": 106, "endOffset": 145}, {"referenceID": 20, "context": "Tensor Neural Networks have a wide application in other field, but have only been recently applied in NLP (Socher et al., 2013; Pei et al., 2014).", "startOffset": 106, "endOffset": 145}, {"referenceID": 26, "context": "and Manning (2009) successfully train name entity recognizers and syntactic parsers jointly, and Singh et al. (2013) train models for coreference resolution, named entity recognition and relation extraction jointly.", "startOffset": 97, "endOffset": 117}, {"referenceID": 6, "context": "Our work is most closely related to Collobert and Weston (2008; Collobert et al. (2011), who apply multitask learning to train neural networks for multiple NLP models: part-of-speech tagging, semantic role labeling, named-entity recognition and language model variations.", "startOffset": 64, "endOffset": 88}], "year": 2015, "abstractText": "We present a three-pronged approach to improving Statistical Machine Translation (SMT), building on recent success in the application of neural networks to SMT. First, we propose new features based on neural networks to model various nonlocal translation phenomena. Second, we augment the architecture of the neural network with tensor layers that capture important higher-order interaction among the network units. Third, we apply multitask learning to estimate the neural network parameters jointly. Each of our proposed methods results in significant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and ChineseEnglish translation over a state-of-the-art system that already includes neural network features.", "creator": "LaTeX with hyperref package"}}}