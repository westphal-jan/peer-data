{"id": "1506.02222", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2015", "title": "No penalty no tears: Least squares in high-dimensional linear models", "abstract": "Ordinary least squares (OLS) is the default method for fitting linear models, but is not applicable for problems with dimensionality larger than the sample size. For these problems, we advocate the use of a generalized version of OLS motivated by ridge regression, and propose two novel three-step algorithms involving least squares fitting and hard thresholding. The algorithms are methodologically simple to understand intuitively, computationally easy to implement efficiently, and theoretically appealing for choosing models consistently. However, the first step of the algorithm is to use a set of algorithms with a minimum of the minimum of the expected values for all elements. In this way, the algorithm is a very efficient approach for predicting and modeling the expected values for the entire space (ie. a uniform subset of the value) using a given set of algorithms. Using these new algorithm methods, the algorithm can easily be modeled in many ways (e.g., using a fixed-mean regression), but it is more complicated than just a set of models, and the first step in our implementation is to use a method similar to the one we\u2019ve used in the earlier work. In addition, the algorithm uses a single set of algorithms, and the algorithms are essentially separate approaches to estimating the expected value for all elements. The most popular of these algorithms is the Bias Linear LSE method in the Python Python framework. The algorithm uses a linear function and a normalization function, and a normalization function.\n\n\nThe algorithm is simple to implement. It can generate a linear function by first finding the maximum number of elements in the space (by looking at a set of linear values). However, for this reason, it is difficult to achieve the desired values for all elements, because the algorithm is not able to specify how many elements they represent as a result of a given linear value. This means that if the algorithm is not efficient (e.g., the average given linear value for a given type of element could be very low, and the likelihood of the given value being a very high) it will not be able to compute the desired values for all elements. Therefore, we recommend that the algorithm be considered only as a part of the equation, not as a part of the equation, and not as a part of the equation, for general linear and linear (e.g., using a regular distribution).\nThe algorithm is able to infer an ideal function from a linear function. It can also model a normalization function, and estimate the maximum number of elements in the space, by", "histories": [["v1", "Sun, 7 Jun 2015 05:45:24 GMT  (2975kb,D)", "http://arxiv.org/abs/1506.02222v1", "Correct several notation errors"], ["v2", "Wed, 10 Jun 2015 03:31:06 GMT  (2975kb,D)", "http://arxiv.org/abs/1506.02222v2", "Correct typos;"], ["v3", "Fri, 9 Oct 2015 21:30:39 GMT  (2979kb,D)", "http://arxiv.org/abs/1506.02222v3", "29 pages, 5 figures, 4 tables"], ["v4", "Mon, 23 Nov 2015 09:21:37 GMT  (2979kb,D)", "http://arxiv.org/abs/1506.02222v4", "corrected citation format"], ["v5", "Thu, 16 Jun 2016 07:13:40 GMT  (2984kb,D)", "http://arxiv.org/abs/1506.02222v5", "Added results for non-sparse models; Added results for elliptical distribution; Added simulations for adaptive lasso"]], "COMMENTS": "Correct several notation errors", "reviews": [], "SUBJECTS": "stat.ME cs.LG math.ST stat.ML stat.TH", "authors": ["xiangyu wang", "david b dunson", "chenlei leng"], "accepted": true, "id": "1506.02222"}, "pdf": {"name": "1506.02222.pdf", "metadata": {"source": "CRF", "title": "No penalty no tears: Least squares in high-dimensional linear models", "authors": ["Xiangyu Wang", "David Dusnon", "Chenlei Leng"], "emails": [], "sections": [{"heading": null, "text": "Ordinary least squares (OLS) is the default method for fitting linear models, but is not applicable for problems with dimensionality larger than the sample size. For these problems, we advocate the use of a generalized version of OLS motivated by ridge regression, and propose two novel three-step algorithms involving least squares fitting and hard thresholding. The algorithms are methodologically simple to understand intuitively, computationally easy to implement efficiently, and theoretically appealing for choosing models consistently. Numerical exercises comparing our methods with penalization-based approaches in simulations and data analyses illustrate the great potential of the proposed algorithms."}, {"heading": "1 Introduction", "text": "Long known for its consistency, simplicity and optimality under mild conditions, ordinary least squares (OLS) is the most widely used technique for fitting linear models. Developed originally for fitting fixed dimensional linear models, unfortunately, classical OLS fails in high dimensional linear models where the number of predictors p far exceeds the number of observations n. To deal with this problem, Tibshirani[1] proposed `1-penalized regression, a.k.a, lasso, which triggered the recent overwhelming exploration in both theory and methodology of penalization-based methods. These methods usually assume that only a small number of coefficients are nonzero (known as the sparsity assumption), and minimize the same least squares loss function as OLS by including an additional penalty on the coefficients, with the typical choice being the `1 norm. Such \u201cpenalization\u201d constrains the solution space to certain directions favoring sparsity of the solution, and thus overcomes the non-unique issue with OLS. It yields a sparse solution and achieves model selection consistency and estimation consistency under certain conditions [2, 3, 4, 5].\nar X\niv :1\n50 6.\n02 22\n2v 1\n[ st\nat .M\nE ]\n7 J\nun 2\n01 5\nDespite the success of the methods based on regularization, there are important issues that can not be easily neglected. On the one hand, methods using convex penalties, such as lasso, usually require strong conditions for model selection consistency[2, 6]. On the other hand, methods using non-convex penalties[3, 4] that can achieve model selection consistency under mild conditions often require huge computational expense. These concerns have limited the practical use of regularized methods, motivating alternative strategies such as direct hard thresholding [7].\nIn this article, we aim to solve the problem of fitting high-dimensional sparse linear models by reconsidering OLS and answering the following simple question: Can ordinary least squares consistently fit these models with some suitable algorithms? Our result provides an affirmative answer to this question under fairly general settings. In particular, we give a generalized form of OLS in high dimensional linear regression, and develop two algorithms that can consistently estimate the coefficients and recover the support. These algorithms involve least squares type of fitting and hard thresholding, and are non-iterative in nature. Extensive empirical experiments are provided in Section 4 to compare the proposed estimators to many existing penalization methods. The performance of the new estimators is very competitive under various setups in terms of model selection, parameter estimation and computational time.\nRelated works The work that is most closely related to ours is [8], in which the authors proposed an algorithm based on OLS and the ridge regression. However, both their methodology and theory are still within the `1 regularization framework, and their conditions (especially their C-Ridge and C-OLS conditions) are overly strong and can be easily violated in practice. [7] proposed an iterative hard thresholding algorithm for sparse regression, which shares a similar spirit of hard thresholding as our algorithm. Nevertheless, their motivation is completely different, their algorithm lacks theoretical guarantees for consistent support recovery, and they require an iterative estimation procedure.\nOur contributions We provide a generalized form of OLS for fitting high dimensional data motivated by ridge regression, and develop two algorithms that can consistently fit a sparse linear model and recover its support. We summarize the advantages of our new algorithms in three points. First, our algorithms work for highly correlated features under random designs. The consistency of the algorithms only needs a conditional number constraint, as opposed to the strong irrepresentable condition[2, 9] required by lasso. Second, our algorithms can achieve consistent support recovery for general noise (with finite second-order moment) in the ultra-high dimension setting where log p = o(n). This is remarkable as most methods\n(c.f. [4, 8, 10, 9, 11, 12]) that work for log p = o(n) case rely on a sub-Gaussian tail/bounded error assumption, which might fail to hold for general noise. [6] proved that lasso also works for a second-order condition similar to ours, but requires two additional strong assumptions. Third, the algorithms are simple, efficient and scale well for large p. In particular, the matrix operations are fully parallelizable with very few communications for very large p, while regularization methods are either hard to be computed in parallel in the feature space, or the parallelization requires a large amount of machine communications.\nThe remainder of this article is organized as follows. In Section 2 we generalize the ordinary least squares estimator for high dimensional problems where p > n, and propose two three-step algorithms consisting only of least squares fitting and hard thresholding in a loose sense. Section 3 provides consistency theory for the algorithms. Section 4 evaluates the empirical performance. We conclude and discuss further implications of our algorithms in the last section. All the proofs are provided in the supplementary materials."}, {"heading": "2 High dimensional ordinary least squares", "text": "Consider the usual linear model\nY = X\u03b2 + \u03b5,\nwhere X is the n\u00d7 p design matrix, Y is the n\u00d7 1 response vector and \u03b2 is the coefficient. As is common in the high dimensional literature, we assume that most \u03b2i\u2019s are zero except for a small subset S = supp(\u03b2) with cardinality s; i.e., S = {i|\u03b2i 6= 0} is the support of \u03b2 and s = card(S).\nTo carefully tailor the low-dimensional OLS estimator for a high dimensional scenario, one needs to answer the following two questions. i) What is the correct form of OLS in the high dimensional setting? ii) How to correctly use this estimator? To answer these, we reconsider OLS from a different perspective. In fact, OLS can be viewed as the limit of the ridge estimator when the ridge parameter goes to zero, i.e.,\n(XTX)\u22121XTY = lim r\u21920 (XTX + rIp) \u22121XTY.\nOne nice property of the ridge estimator is that it exists regardless of the relationship between p and n. A keen observation[12] reveals the following relationship immediately.\nLemma 1. For any p, n, r > 0, we have\n(XTX + rIp) \u22121XTY = XT (XXT + rIn) \u22121Y. (1)\nNotice that the right hand side of (1) exists when p > n and r = 0. Consequently, we can naturally extend the classical OLS to the high dimensional scenario by letting r tend to zero in (1). Denote this high dimensional version of the OLS as\n\u03b2\u0302(HD) = lim r\u21920 XT (XXT + rIn) \u22121Y = XT (XXT )\u22121Y.\nThe above equation indicates that \u03b2\u0302(HD) is essentially an orthogonal projection of \u03b2 onto the row space of X. Unfortunately, this (low dimensional) projection does not have good general performance in estimating sparse vectors in high-dimensional cases. Instead of directly estimating \u03b2 as \u03b2\u0302HD, however, this new estimator of \u03b2 may be used for dimension reduction by observing \u03b2\u0302(HD) = XT (XXT )\u22121X\u03b2 + XT (XXT )\u22121\u03b5 = \u03a6\u03b2 + \u03b7 [12]. Since \u03b7 is stochastically small, if \u03a6 is close to a diagonally dominant matrix and \u03b2 is sparse, then the zero and non-zero coefficients can be separated by simply thresholding the small entries of \u03b2\u0302(HD). The exact meaning of this statement will be discussed in next section. Some simple examples demonstrating the diagonal dominance of XT (XXT )\u22121X are illustrated immediately in Figure 1, where the rows of X in the left two plots are drawn from N(0,\u03a3) with \u03c3ij = 0.6 or \u03c3ij = 0.99 |i\u2212j|. The sample size and data dimension are chosen as (n, p) = (50, 1000). The right plot takes the standardized design matrix directly from the real data in Section 4 with (n, p) = (120, 5000). A clear diagonal dominance pattern is visible in each plot.\nThis ability to separate zero and non-zero coefficients allows us to first obtain a smaller model with size d such that s < d < p which includes all the nonzero variables in S. Once d is below n, one can directly apply the usual OLS to obtain an estimator, which will be thresholded further to obtain a more refined model. The final estimator will then be obtained\nby an OLS fit on the refined model. This three-stage non-iterative algorithm is termed Leastsquares adaptive thresholding (LAT) and the concrete procedure is described in Algorithm 1.\nAlgorithm 1 The Least-squares Adaptive Thresholding Algorithm (LAT) Initialization: 1: Input (Y,X), d, \u03b4 2: # where X, Y are standardized data, n is the sample size, p is the number of features, d is the number of variables selected at stage 1 and \u03b4 \u2208 (0, 1) is a tuning parameter determining the selection confidence Stage 1 : Pre-selection 3: Compute \u03b2\u0302(HD) = XT (XXT )\u22121Y . Rank the importance of the variables by |\u03b2\u0302(HD)i |; 4: Denote the model corresponding to the d largest |\u03b2\u0302(HD)i | as M\u0303d. Alternatively use eBIC\nin [13] in conjunction with the obtained variable importance to select the best submodel. Stage 2 : Hard thresholding\n5: \u03b2\u0302(OLS) = (XTM\u0303d XM\u0303d) \u22121XTM\u0303d Y ; 6: \u03c3\u03022 = \u2211n\ni=1(y \u2212 y\u0302)2/(n\u2212 d); 7: C\u0304 = (XTM\u0303d XM\u0303d) \u22121;\n8: Hard threshold \u03b2\u0302(OLS) by mean( \u221a 2\u03c3\u03022C\u0304ii log(4d/\u03b4)) or use BIC to select the best sub-\nmodel. Denote the chosen model as M\u0302. Stage 3 : Refinement\n9: \u03b2\u0302M\u0302 = (X T M\u0302XM\u0302) \u22121XTM\u0302Y ;\n10: \u03b2\u0302i = 0,\u2200i 6\u2208 M\u0302; 11: return \u03b2\u0302\nThe C\u0304 in Stage 2 can be replaced by its ridge version (XTM\u0303d XM\u0303d + rId) \u22121 to stabilize numerical computation. This variant of the algorithm is referred to as the Ridge Adaptive Thresholding (RAT) algorithm."}, {"heading": "3 Theory", "text": "In this section, we prove the consistency of Algorithm 1 in selecting the true model and provide concrete forms for all the values needed for the algorithm to work. Recall the linear model Y = X\u03b2 + \u03b5. We consider the random design where the rows of X are drawn from a multivariate Gaussian distribution N(0,\u03a3). This random design allows for various correlation structures among predictors and is widely used to illustrate methods that rely on the restricted eigenvalue conditions [14, 15]. The noise \u03b5, as mentioned earlier, is only assumed to have the second-order moment, i.e., var(\u03b5) = \u03c32 < \u221e, in contrast to the subGaussian/bounded error assumption seen in most high dimension literature [4, 8, 10, 9, 11]. This relaxation is similar to [6]; however we do not require any further assumptions needed by [6]. In Algorithm 1, we also propose to use extended BIC and BIC for parameter\ntuning. However, the corresponding details will not be pursued here, as their consistency is straightforwardly implied by the results from this section and the existing literature on extended BIC and BIC [13].\nDefine \u03ba = cond(\u03a3) and \u03c4 = mini\u2208S |\u03b2i|. We state our result in three theorems.\nTheorem 1. Assume p > c0n for some c0 > 1 and var(Y ) \u2264 M0. If s log p = O(n\u03bd) for some \u03bd < 1, n > 4c0/(c0\u2212 1)2, and \u03b3 is chosen to be \u03b3 = c1\u03ba \u22121\u03c4 2 n p , where c1 is some absolute constant specified in Lemma 2 in the supplementary materials, then for any \u03b4 \u2208 (0, 1) we have\nP ( max i 6\u2208S |\u03b2\u0302(HD)i | \u2264 \u03b3 \u2264 min i\u2208S |\u03b2\u0302(HD)i | ) = 1\u2212O ( \u03c32\u03ba4 log p \u03c4 2n1\u2212\u03b4 ) .\nTheorem 1 guarantees the model selection consistency of the first stage of Algorithm 1. The proof of Theorem 1 relies on the diagonal dominance of matrix \u03a6 = XT (XXT )\u22121X. In particular, it is shown that the diagonal terms of \u03a6 are O(n p ) while the off-diagonal terms are O( \u221a n p ) [16]. Thus, with an appropriate signal-to-noise ratio and true model size, \u03a6\u03b2 is likely to preserve a correct magnitude order of zero and nonzero coefficients, which can then be separated by a threshold \u03b3. As \u03b3 is not easily computable based on data, we propose to rank the |\u03b2\u0302i|\u2032s and select d largest coefficients. Alternatively, we can construct a series of nested models formed by ranking the largest n coefficients and adopt the extended BIC [13] to select the best submodel. Once the submodel M\u0303d is obtained, we proceed to the second stage by obtaining an estimate via ordinary least squares \u03b2\u0302(OLS) corresponding to M\u0303d. From Theorem 1, if d > s, we have that with probability tending to one,M\u2217 \u2286 M\u0303d, whereM\u2217 is the true model. Then for \u03b2\u0302(OLS) we have the following result.\nTheorem 2. Assume n \u2265 64\u03bad log p, log p = O(n\u03bd) and d \u2212 s \u2264 c\u0303 for some \u03bd < 1 and c\u0303 > 0. If there exists some \u03b4 \u2208 (0, 1) such that \u03c4 \u2265 2\u03c3\nn\u03b4/2 , then by choosing \u03b3\u2032 = \u03c3 n\u03b4/2 we have\nP ( max i 6\u2208S |\u03b2\u0302(OLS)i | \u2264 \u03b3\u2032 \u2264 min i\u2208S |\u03b2\u0302(OLS)i | ) = 1\u2212O ( \u03ba log p log d n1\u2212\u03b4 ) .\nTheorem 2 states that if \u03c4 = mini\u2208S |\u03b2i| \u2265 \u03b3\u2032, where \u03b3\u2032 = \u03c3/n\u03b4/2, then by thresholding \u03b2\u0302(OLS) at \u03b3\u2032, we can identify the exact model with probability tending to 1. In fact, we have a similar result for ridge regression.\nTheorem 3 (Ridge regression). Assume the conditions in Theorem 2. If there exists some\n\u03b4 \u2208 (0, 1) such that \u03c4 \u2265 4\u03c3 n\u03b4/2 , then if the ridge parameter r satisfies that\nr \u2264 O { min (\u221a n\n\u03ba , \u03c3\n1 2n1\u2212\u03b4/4\n82M 1 2\n0 \u03ba 3 2\n)} ,\nwhere M0 is defined in Theorem 1, then by choosing \u03b3 \u2032 = 2\u03c3\nn\u03b4/2 we have\nP ( max i 6\u2208S |\u03b2\u0302(Ridge)i (r)| \u2264 \u03b3\u2032 \u2264 min i\u2208S |\u03b2\u0302(Ridge)i (r)| ) = 1\u2212O ( \u03ba log p log d n1\u2212\u03b4 ) .\nNote that the ridge parameter r can be chosen as a constant, bypassing the need to specify r at least in theory. When the noise follows a Gaussian distribution, we can obtain a more explicit form of the threshold \u03b3\u2032, as the following Corollary shows.\nCorollary 1 (Gaussian noise). Assume \u03b5 \u223c N(0, \u03c32). For any \u03b4 \u2208 (0, 1), define \u03b3\u2032 = 8 \u221a 2\u03c3\u0302 \u221a\n2\u03ba log(4d/\u03b4) n\n, where \u03c3\u0302 is the estimated standard error as \u03c3\u03022 = \u2211n\ni=1(yi \u2212 y\u0302i)2/(n \u2212 d). For sufficiently large n, if d \u2264 n \u2212 4K2 log(2/\u03b4)/c for some absolute constants c, K and \u03c4 \u2265 24\u03c3 \u221a 2\u03ba log(4d/\u03b4)\nn , then with probability at least 1\u2212 2\u03b4, we have\n|\u03b2\u0302(OLS)i | \u2265 \u03b3\u2032 \u2200i \u2208 S and |\u03b2\u0302 (OLS) i | \u2264 \u03b3\u2032 \u2200i 6\u2208 S.\nWrite C\u0304 = (XTM\u0303d XM\u0303d) \u22121 as in Algorithm 1. In practice, we propose to use \u03b3\u2032 = mean( \u221a 2\u03c3\u03022C\u0304ii log(4d/\u03b4)) as the threshold (see Algorithm 1), because the estimation er-\nror takes a form of \u221a \u03c32C\u0304ii log(4d/\u03b4). Alternatively, instead of identifying an explicit form of the threshold value (as is hard for general noise), one may also use BIC on nested models formed by ranking |\u03b2\u0302(OLS)| to search for the true model. Once the final model is obtained, as in Stage 3 of Algorithm 1, we refit it again using ordinary least squares. The final output will have the same output as if we knew the true model a priori with probability tending to 1, i.e., we have the following result.\nTheorem 4. Let M\u0302 and \u03b2\u0302 be the final output from LAT or RAT. Assume all conditions in Theorem 1, 2 and 3. Then with probability at least 1\u2212O ( \u03c32\u03ba4 log p \u03c42n1\u2212\u03b4 + \u03ba log p log d n1\u2212\u03b4 ) we have\nM\u0302 =M\u2217, \u2016\u03b2\u0302 \u2212 \u03b2\u201622 \u2264 2s\u03c32\nn\u03b4 , and \u2016\u03b2\u0302 \u2212 \u03b2\u2016\u221e \u2264\n2\u03c3\nn\u03b4/2 .\nAs implied by Theorem 1 \u2013 4, LAT and RAT achieve consistent support recovery in the ultra-high dimensional (log p = o(n)) setting only with two assumptions: \u03c4 = O( \u221a (log p)/n) and var(\u03b5) <\u221e, in contrast to most existing methods that require \u03b5 \u223c N(0, \u03c32) or \u2016\u03b5\u2016\u221e < \u221e."}, {"heading": "4 Experiments", "text": "In this section, we provide extensive numerical experiments for assessing the performance of LAT and RAT. In particular, we compare the two methods to existing penalized methods including lasso, elastic net (enet [5]), scad [3] and mc+ [4]. As it is well-known that the lasso estimator is biased, we also consider two variations of it by combining lasso with Stage 2 and 3 of our LAT and RAT algorithms, denoted as lasLAT (las1 in Figures) and lasRAT (las2 in Figures) respectively. We code LAT and RAT in Matlab, use glmnet[17] for enet and lasso, and SparseReg[18, 19] for scad and mc+."}, {"heading": "4.1 Synthetic datasets", "text": "The model used in this section for comparison is the linear model Y = X\u03b2 + \u03b5, where \u03b5 \u223c N(0, \u03c32) and X \u223c N(0,\u03a3). To control the signal-to-noise ratio, we define r = \u2016\u03b2\u20162/\u03c3, which is chosen to be 2.3 for all experiments. The sample size and the data dimension are chosen to be (n, p) = (200, 1000) or (n, p) = (500, 10000) for all experiments. For evaluation purposes, we consider four different structures of \u03a3 below.\n(i) Independent predictors. The support is set as S = {1, 2, 3, 4, 5}. We generate Xi from a standard multivariate normal distribution with independent components. The coefficients are specified as\n\u03b2i = (\u22121)ui(|N(0, 1)|+ 1), where ui \u223c Ber(0.5) for i \u2208 S and \u03b2i = 0 for i 6\u2208 S.\n(ii) Compound symmetry . All predictors are equally correlated with correlation \u03c1 = 0.6. The coefficients are set to be \u03b2i = 3 for i = 1, ..., 5 and \u03b2i = 0 otherwise.\n(iii) Group structure . This example is Example 4 in [5], for which we allocate the 15 true variables into three groups. Specifically, the predictors are generated as\nx1+3m = z1 +N(0, 0.01), x2+3m = z2 +N(0, 0.01), x3+3m = z3 +N(0, 0.01),\nwhere m = 0, 1, 2, 3, 4 and zi \u223c N(0, 1) are independent. The coefficients are set as\n\u03b2i = 3, i = 1, 2, \u00b7 \u00b7 \u00b7 , 15; \u03b2i = 0, i = 16, \u00b7 \u00b7 \u00b7 , p.\n(iv) Factor models. This model is also considered in [20] and [21]. Let \u03c6j, j = 1, 2, \u00b7 \u00b7 \u00b7 , k be independent standard normal variables. We set predictors as xi = \u2211k j=1 \u03c6jfij + \u03b7i, where fij and \u03b7i are generated from independent standard normal distributions. The number of\nfactors is chosen as k = 5 in the simulation while the coefficients are specified the same as in Example (ii).\nTo compare the performance of all methods, we simulate 200 synthetic datasets for (n, p) = (200, 1000) and 100 for (n, p) = (500, 10000) for each example, and record i) the root mean squared error (RMSE): \u2016\u03b2\u0302 \u2212 \u03b2\u20162, ii) the false negatives (# FN), iii) the false positives (# FP) and iv) the actual runtime (in milliseconds). We use the extended BIC [13] to choose the parameters for any regularized algorithm. Due to the huge computation expense for scad and mc+, we only find the first d\u221ape predictors on the solution path (because we know s << \u221a p). For RAT and LAT, d is set to 0.3 \u00d7 n. For RAT and larsRidge, we adopt a 10-fold cross-validation procedure to tune the ridge parameter r for a better finite-sample performance, although the theory allows r to be fixed as a constant. For all hard-thresholding steps, we fix \u03b4 = 0.5. The results for (n, p) = (200, 1000) are plotted in Figure 2, 3, 4 and 5 and more comprehensive results (average values for RMSE, # FPs, # FNs, runtime) are summarized in Table 1 and 2.\nAs can be seen from both the plots and the tables, the performance of LAT and RAT are on par with lasLAT for Example (i), (ii) and (iv), and are often among the best of\nall methods. For Example (iii), RAT and enet achieve the best performance while all the other methods fail to work. In addition, the runtime of LAT and RAT are also competitive compared to that of lasso and enet. We thus conclude that LAT and RAT achieve similar or even better performance compared to the usual regularized methods."}, {"heading": "4.2 Real data", "text": "This dataset, taken from [22], was collected to study mammalian eye diseases, with gene expression for the eye tissues of 120 twelve-week-old male F2 rats recorded. One gene coded as TRIM32 responsible for causing Bardet-Biedl syndrome is of particular interest, and is the response of interest.\nFollowing the method in [22], 18976 probes were selected as they exhibited sufficient signal for reliable analysis and at least 2-fold variation in expressions. Because TRIM32 is believed to be only linked to a small number of genes, we confine our attention to the top 5000 genes with the highest sample variance. The eight methods used in the simulation study are compared, where the performance is assessed via 10-fold cross validation. Because extended BIC does not offer a competitive prediction accuracy (It focuses on ensuring a good\nvariable selection performance) for regularized methods, for a fair comparison, we apply the conventional BIC instead of the extended BIC to all regularization methods, and record the means and the standard errors of the cross-validation. As a reference, we also report these values for the null model.\nIt can be seen that enet and lasso achieve the smallest cross-validation errors overall, followed by RAT and LAT. One caveat for the good performance of enet or lasso is the large number of variables it selected. If a more parsimonious model for interpretability is preferred, one might want to trade-off some accuracy by obtaining a model with a fewer number of variables given by LAT or RAT."}, {"heading": "5 Conclusion", "text": "We have proposed two novel algorithms Lat and Rat that only rely on least-squares type of fitting and hard thresholding, based on a high-dimensional generalization of OLS. The two methods are simple, easily implementable, and can consistently fit a high dimensional linear model and recover its support. The performance of the two methods are competitive compared to existing regularization methods. It is of great interest to further extend this framework to other models such as generalized linear models and models for survival analysis."}, {"heading": "Appendix A: Proof of Theorem 1", "text": "Recall the estimator \u03b2\u0302(HD) = XT (XXT )\u22121Y = XT (XXT )\u22121X\u03b2 + XT (XXT )\u22121\u03b5 = \u03be + \u03b7. The following two lemmas will be used to bound \u03be and \u03b7 respectively.\nLemma 2. Let \u03a6 = XT (XXT )\u22121X. Assume p > c0n for some c0 > 1, then for any C > 0 there exists some 0 < c1 < 1 < c2 and c3 > 0 such that for any t > 0 and any i \u2208 Q, j 6= i,\nP ( |\u03a6ii| < c1\u03ba\u22121 n\np ) \u2264 2e\u2212Cn, P (|\u03a6ii| > c2\u03ba\nn p\n) \u2264 2e\u2212Cn (2)\nand\nP ( |\u03a6ij| > c4\u03bat \u221a n\np\n) \u2264 5e\u2212Cn + 2e\u2212t2/2, (3)\nwhere c4 = \u221a c2(c0\u2212c1)\u221a c3(c0\u22121) .\nThis is exactly the Lemma 3 in [16].\nLemma 3. Assume X follows N(0,\u03a3). If var( ) = \u03c32 and log p = o(n), then for any 0 < \u03b4 < 1 we have\nP ( \u2016\u03b7\u2016\u221e \u2264 c1\u03ba \u22121\u03c4\n4\nn p\n) \u2265 1\u2212O ( \u03c32\u03ba4 log p\n\u03c4 2n1\u2212\u03b4\n) ,\nwhere \u03c4 = mini\u2208S |\u03b2i| and \u03ba = cond(\u03a3).\nTo prove Lemma 3 we need the following two propositions.\nProposition 1. (Lounici, 2008 [6]; Nemirovski, 2000 [23]) Let Yi \u2208 Rp be random vectors with zero means and finite variances. Then we have for any k norm with k \u2208 [2,\u221e] and p \u2265 3, we have\nE \u2225\u2225 n\u2211 i=1 Yi \u2225\u22252 k \u2264 C\u0303 min{k, log p} n\u2211 i=1 E\u2016Yi\u20162k, (4)\nwhere C\u0303 is some absolute constant.\nAs each row of X is an iid draw from N(0,\u03a3), we define Z = X\u03a3\u22121/2, then Z \u223c N(0, Ip). For Z, we have the following result.\nProposition 2. Let Z \u223c N(0, Ip), then we have the minimum eigenvalue of ZZT/p satisfies that\nP ( \u03bbmin(ZZ\nT/p) > (1\u2212 n p \u2212 t p\n)2 ) \u2265 1\u2212 2 exp(\u2212t2/2)\nfor any t > 0. Assume p > c0n for c0 > 1 and take t = \u221a n. When n > 4c20/(c0 \u2212 1)2, we have\nP ( \u03bbmin(ZZ T/p) > c ) \u2265 1\u2212 2 exp(\u2212n/2), (5)\nwhere c = (c0\u22121) 2\n4c20 .\nThe proof follows Corollary 5.35 in [24].\nProof of Lemma 3. Let A = pXT (XXT )\u22121 and define Z = X\u03a3\u22121/2. Consider the standard SVD on Z as Z = V DUT , where V and D are n\u00d7 n matrices and U is a p\u00d7 n matrix. Because Z is a matrix of iid Gaussian variables, its distribution is invariant under both left and right orthogonal transformation. In particular, for any T \u2208 O(n), we have\nTV DUT (d) = V DUT ,\ni.e., V is uniformly distributed onO(n) conditional on U and D (they are in fact independent, but we don\u2019t need such a strong condition). Therefore, we have\nA = pXT (XXT )\u22121 = p\u03a3 1 2ZT (Z\u03a3ZT )\u22121 = p\u03a3 1 2UDV T (V DUT\u03a3UDV T )\u22121\n= p\u03a3 1 2U(UT\u03a3U)\u22121D\u22121V T = \u221a p\u03a3 1 2U(UT\u03a3U)\u22121 ( D \u221a p )\u22121 V T .\nBecause V is uniformly distributed conditional on U and D, the distribution of A is also invariant under right orthogonal transformation conditional on U and D, i.e., for any T \u2208 O(n), we have\nA (d) = AT. (6)\nOur first goal is to bound the magnitude of individual entries Aij. Let vi = e T i AA T ei, which is a function of U and D (see below). From (6), we know that eTi A is uniformly distributed\non the sphere Sn\u22121( \u221a vi) if conditional on vi (i.e., conditional on U,D), which implies that\neTi A (d) = \u221a vi ( x1\u221a\u2211n j=1 x 2 j , x2\u221a\u2211n j=1 x 2 j , \u00b7 \u00b7 \u00b7 , xn\u221a\u2211n j=1 x 2 j ) , (7)\nwhere x\u2032js are iid standard Gaussian variables. Thus, Aij can be bounded easily if we can bound vi. Notice that for vi we have\nvi = e T i AA T ei = pe T i \u03a3\n1 2U(UT\u03a3U)\u22121 (D2 p )\u22121 (UT\u03a3U)\u22121UT\u03a3 1 2 ei.\n= peTi H(U T\u03a3U)\u2212 1 2 (D2 p )\u22121 (UT\u03a3U)\u2212 1 2HT ei\n\u2264 peTi HHT ei \u00b7 \u03bb\u22121min(UT\u03a3U) \u00b7 \u03bb\u22121min (D2 p ) Here H = \u03a3\n1 2U(UT\u03a3U)\u22121/2 is defined the same as in [12] and can be bounded as eTi HH T ei \u2264 c2n\u03ba/p with probability 1 \u2212 2 exp(\u2212Cn) (see the proof of Lemma 3 in [16]). Therefore, we have\nP ( vi \u2264 c2\u03ba2\u03bb\u22121min (D2 p ) n ) \u2265 1\u2212 2 exp(\u2212Cn)\nNow applying the tail bound and the concentration inequality to (7) we have for any t > 0 and any C > 0\nP (|xj| > t) \u2264 2 exp(\u2212t2/2) P (\u2211n j=1 x 2 j\nn \u2264 c3\n) \u2264 exp(\u2212Cn). (8)\nPutting the pieces all together, we have for any t > 0 and any C > 0 that\nP ( max ij |Aij| \u2264 \u03bat \u221a c2 c3 \u03bb \u2212 1 2 min (D2 p )) \u2265 1\u2212 2np exp(\u2212t2/2)\u2212 3p exp(\u2212Cn).\nNow according to (5), we can further bound \u03bbmin(D 2/p) and obtain that\nP ( max ij |Aij| \u2264 \u221a c2 cc3 \u03bat ) \u2265 1\u2212 2np exp(\u2212t2/2)\u2212 3p exp(\u2212Cn)\u2212 2 exp(\u2212n/2). (9)\nThe second step is to use (9) and Proposition 1 to bound \u03b7. The procedure follows almost the same as in Lounici\u2019s paper. Define Zj = (A1j j, A2j j, \u00b7 \u00b7 \u00b7 , Apj j). It\u2019s clear that\n\u03b7 = \u2211n\nj=1 Zj/p. Applying Proposition 1 to Z \u2032 js and choosing the l\u221e norm, we have\nE \u2225\u2225 n\u2211 j=1 Zj \u2225\u22252 \u221e \u2264 log p n\u2211 j=1 E\u2016Zj\u20162\u221e \u2264 c2 cc3 \u03c32\u03ba2t2n log p.\nUsing the Markov inequality on \u03b7, we have for any r > 0\nP ( \u2016\u03b7\u2016\u221e \u2265 \u221a nr\np\n) = P ( p\u221a n \u2016\u03b7\u2016\u221e \u2265 r ) \u2264 p 2E\u2016\u03b7\u20162\u221e nr2 = E\u2016 \u2211n j=1 Zj\u20162\u221e nr2\n\u2264 c2\u03c3 2\u03ba2t2 log p\ncc3r2 .\nTo match our previous result, we take r = c1 \u221a n\u03c4\u03ba\u22121/4 and t = n\u03b4/2 for some small \u03b4,\nP ( \u2016\u03b7\u2016\u221e \u2264 c1\u03ba \u22121\u03c4\n4\nn p\n) \u2265 1\u2212 c2\u03c3 2\u03ba4\nc21cc3\u03c4 2\nlog p n1\u2212\u03b4 \u2212 2np exp(\u2212n\u03b4/2)\u2212 3p exp(\u2212Cn)\u2212 2 exp(\u2212n/2)\n\u2265 1\u2212O ( \u03c32\u03ba4 log p\n\u03c4 2n1\u2212\u03b4\n) .\nNow we are ready to prove Theorem 1\nProof of Theorem 1. Recall the definition of \u03be as \u03be = XT (XXT )\u22121X\u03b2. For any i \u2208 S we have\n\u03bei = e T i X T (XXT )\u22121X\u03b2 = \u2211 j\u2208S \u03a6ii\u03b2i + \u2211 j 6=i,j\u2208S \u03a6ij\u03b2j,\nand for i 6\u2208 S,\n\u03bei = e T i X T (XXT )\u22121X\u03b2 = \u2211 j\u2208S \u03a6ij\u03b2j.\nAccording to our assumption we have mini\u2208S |\u03b2i| \u2265 \u03c4 and var(Y ) = var(X\u03b2) = \u03b2T\u03a3\u03b2 \u2264 M0 for some M0. The latter one imples that\nM0 \u2265 \u03b2T\u03a3\u03b2 \u2265 \u03bbmin(\u03a3)\u2016\u03b2\u201622.\nTherefore, we have for any i \u2208 S\n|\u03bei| \u2265 c1\u03ba\u22121\u03c4 n\np \u2212 \u2016\u03b2\u20162 \u221a \u2211 j 6=i,j\u2208S \u03a62ij \u2265 c1\u03ba\u22121\u03c4 n p \u2212 c4\u03ba \u221a sM0t \u03bb 1 2 min(\u03a3) \u221a n p = 3c1\u03ba \u22121\u03c4 4 n p ,\nif t is taken to be t = c1\u03bb\n1 2 min(\u03a3)\u03ba \u22122\u03c4 \u221a n\n4c4 \u221a M0s\n\u2265 c1\u03ba \u2212 52 \u03c4 \u221a n\n4c4 \u221a M0s . Hence, one can compute the probability to be greater than 1\u2212 7 exp(\u2212Cn)\u2212 2 exp ( \u2212 c 2 1\u03ba \u22125\u03c42 32c24M0s n ) . Similarly, with the same t we can show that for i 6\u2208 S\n|\u03bei| \u2264 \u2016\u03b2\u20162 \u221a \u2211\nj 6=i,j\u2208S\n\u03a62ij \u2264 c1\u03ba \u22121\u03c4\n4\nn p ,\nwith probability greater than 1 \u2212 7 exp(\u2212Cn) \u2212 2 exp ( \u2212 c 2 1\u03ba \u22125\u03c42 32c24M0s n ) . Next, using the result from Lemma 3, we can obtain\nP ( min i\u2208S |\u03b2\u0302i| \u2265 c1\u03ba \u22121\u03c4 2 n p ) \u2265 1\u2212O ( \u03c32\u03ba4 log p \u03c4 2n1\u2212\u03b4 ) ,\nand\nP ( max i\u22086S |\u03b2\u0302i| \u2264 c1\u03ba \u22121\u03c4 2 n p ) \u2265 1\u2212O ( \u03c32\u03ba4 log p \u03c4 2n1\u2212\u03b4 ) .\nTaking \u03b3 = c1\u03ba \u22121\u03c4 2 np, we have\nP ( min i\u2208S |\u03b2\u0302i| \u2265 \u03b3 \u2265 max i 6\u2208S |\u03b2\u0302i| ) \u2265 1\u2212O ( \u03c32\u03ba4 log p \u03c4 2n1\u2212\u03b4 ) .\nProof of Theorem 2 and 3\nLemma 4. Let M\u0303d be a submodel that contains the true model M\u2217 and has a size of d. Define A = n(XTM\u0303d XM\u0303d) \u22121XTM\u0303d\nwhere XM\u0303d is the principal submatrix indexed by M\u0303d. Then for any t > 0 and C > 0, there exists some c3 > 0 such that\nP ( max\n|M\u0303d|=d,M\u2217\u2286M\u0303d max ij |Aij| \u2264 t\u221a c3\u03bb0\n) \u2265 1\u2212 2dn(p\u2212 s)d\u2212s exp ( \u2212 t 2\n2\n) \u2212 d(p\u2212 s)d\u2212s exp(\u2212Cn),\nwhere \u03bb0 = min|M\u0303d|=d,M\u2217\u2286M\u0303d \u03bbmin(X T M\u0303d XM\u0303d/n).\nProof of Lemma 4. The proof is similar to the argument in Lemma 3. For a given M\u0303d, XM\u0303d follows N(0,\u03a3M\u0303d). Similarly, defining Z = X\u03a3 \u22121/2 M\u0303d\n, then Z \u223c N(0, I). Assuming the singular value decomposition of Z is Z = V DUT where V is a n \u00d7 d matrix and D,U are d \u00d7 d matrices, and conditional on U,D, V is uniformly distributed on Vn,d. Therefore, we\nhave\nA = n(XTM\u0303dXM\u0303d) \u22121XTM\u0303d = n\u03a3 1/2 M\u0303d (ZTZ)\u22121ZT = n\u03a3 1/2 M\u0303d UD\u22121V T .\nWe observe that\n\u2016eTi A\u201622 = n2\u03a3 1/2 M\u0303d UD\u22122UT\u03a3 1/2 M\u0303d = n2(XTM\u0303dXM\u0303d) \u22121 \u2264 n \u03bbmin(XTM\u0303d XM\u0303d/n) .\nNext, following exactly the same argument in Lemma 3, we know that the distribution of A is invariant under the right orthogonal transformation and conditional on vi = \u2016eTi A\u20162, eTi A is uniformly distributed on Sn\u22121(vi). Using the same inequality in (8), we have\nP ( max ij |Aij| \u2264\nt\u221a c3\u03bbmin(XTM\u0303d XM\u0303d/n)\n) \u2265 1\u2212 2dn exp(\u2212t2/2)\u2212 d exp(\u2212Cn).\nNow the total number of possible M\u0303d is bounded by (p\u2212s)\u00d7 (p\u2212s\u22121)\u00d7\u00b7 \u00b7 \u00b7\u00d7 (p\u2212d+1) \u2264 (p\u2212 s)(d\u2212s). Therefore, we have\nP ( max\n|M\u0303d|=d,M\u2217\u2286M\u0303d max ij |Aij| \u2264 t\u221a c3\u03bb0\n) \u2265 1\u2212 2dn(p\u2212 s)d\u2212s exp ( \u2212 t 2\n2\n) \u2212 d(p\u2212 s)d\u2212s exp(\u2212Cn),\nwhere \u03bb0 = min|M\u0303d|=d,M\u2217\u2286M\u0303d \u03bbmin(X T M\u0303d XM\u0303d/n).\nLemma 5 (Garvesh, Wainwright and Yu. (2010) [15]). There exists some absolute constant c\u2032, c\u2032\u2032 > 0 such that\n\u2016Xv\u20162\u221a n \u2265 1 4 \u2016\u03a3 1 2v\u20162 \u2212 9\u03c1(\u03a3)\n\u221a log p\nn \u2016v\u20161, \u2200v \u2208 Rp,\nwith probability at least 1\u2212 c\u2032\u2032 exp(\u2212c\u2032n), where \u03c1(\u03a3) = maxi=1,2,\u00b7\u00b7\u00b7 ,p \u03a3ii.\nIn our case, for any v with d nonzero coordinates, we have \u2016v\u20161 \u2264 \u221a d\u2016v\u20162, \u03c1(\u03a3) = 1\nand \u2016\u03a31/2v\u20162 \u2265 \u03ba\u2212 1 2\u2016v\u20162. Therefore,\n\u2016Xv\u20162\u221a n \u2265 ( \u03ba\u22121/2 4 \u2212 9 \u221a d log p n ) \u2016v\u20162, \u2016v\u20160 \u2264 d.\nProof of Theorem 2. Lemma 5 essentially states that for any d \u00d7 d principal submatrix of X, we can bound its smallest eigenvalue. Therefore, for any selected submodel M\u0303d from\nthe first stage, we have with probability at least 1\u2212O(exp(\u2212c\u2032n))\nmin |M\u0303d|=d\n\u03bb 1 2 min(X T M\u0303d XM\u0303d/n) \u2265\n\u03ba\u22121/2 4 \u2212 9 \u221a d log p n \u2265 \u03ba \u22121/2 8 ,\nas long as n \u2265 64\u03bad log p, i.e., \u03bb0 \u2265 \u03ba \u22121\n64 , where \u03bb0 is defined in Lemma 4.\nA direct calculation shows that \u03b2\u0302(OLS) = \u03b2+ (XTM\u0303d XM\u0303d) \u22121XTM\u0303d \u03b5. Therefore, we want to\nbound the error\n\u03b7\u0303 = (XTM\u0303dXM\u0303d) \u22121XTM\u0303d\u03b5 = A\u03b5/n.\nFollowing the same argument as Lemma 3, we define Zj = (A1j\u03b5j, \u00b7 \u00b7 \u00b7 , Adj\u03b5j) and \u03b7\u0303 =\u2211n j=1 Zj/n. Using Proposition 1 and Lemma 4 we have with probability at least 1\u2212 2d(p\u2212 s)d\u2212s exp(\u2212t2/2)\u2212 d(p\u2212 s)d\u2212s exp(\u2212Cn)\nE \u2225\u2225 n\u2211 j=1 Zj \u2225\u22252 \u221e \u2264 log d n\u2211 j=1 E\u2016Zj\u20162\u221e \u2264 \u03c32nt2 log d c3\u03bb0 \u2264 64c\u221213 \u03ba\u03c32t2n log d. (10)\nThus, for any r > 0\nP ( \u2016\u03b7\u0303\u2016\u221e \u2265 r\nn\n) = P (\u2225\u2225 n\u2211 j=1 Zj \u2225\u2225 \u221e \u2265 r ) \u2264 E \u2225\u2225\u2211n j=1 Zj \u2225\u22252 \u221e r2 \u2264 64\u03ban\u03c3 2t2 log d c3r2 .\nIf we take t = \u221a 2(c\u0303+ 3) log p for any \u03b4 \u2208 (0, 1), then it is ensured that\n1\u2212 2dn(p\u2212 s)d\u2212s exp ( \u2212 t 2\n2\n) \u2212 d(p\u2212 s)d\u2212s exp(\u2212Cn)\n\u2265 1\u2212 2 exp ( (c\u0303+ 2) log p\u2212 (c\u0303+ 3) log p ) \u2212 exp ( (c\u0303+ 1) log p\u2212 Cn ) = 1\u2212O ( 1\np\n) \u2265 1\u2212O ( 1\nn\n) .\nNow taking r = \u03c3n1\u2212\u03b4/2 for any \u03b4 \u2208 (0, 1) we have\nP ( \u2016\u03b7\u0303\u2016\u221e \u2264 \u03c3\nn\u03b4/2\n) \u2265 1\u2212O ( \u03ba log p log d\nn1\u2212\u03b4\n) . (11)\nConsequently, for any \u03b4 > 0 we have\n\u2016\u03b2\u0302(OLS) \u2212 \u03b2M\u0303d\u2016\u221e \u2264 \u03c3\nn\u03b4/2 , (12)\nwith probability at least 1 \u2212 O ( \u03ba log p log d\nn1\u2212\u03b4\n) . So if \u03c4 \u2265 2\u03c3\nn\u03b4/2 , then by choosing \u03b3\u2032 = \u03c3 n\u03b4/2 we\nhave\nmin i\u2208S |\u03b2\u0302(OLS)i | \u2265 \u03b3\u2032 \u2265 max i 6\u2208S |\u03b2\u0302(OLS)i |.\nProof of Theorem 3. Denoting XM\u0303d by X, the definition of \u03b2\u0302(r) (Ridge) becomes\n\u03b2\u0302(r)(Ridge) = (XTX + rId) \u22121XTX\u03b2 + (XTX + rId) \u22121XT \u03b5\n= \u03b2 \u2212 r(XTX + rId)\u22121\u03b2 + (XTX + rId)\u22121XT \u03b5 = \u03b2 \u2212 \u03be\u0303(r) + \u03b7\u0303(r).\nFor \u03be\u0303(r) we have\nmax |\u03be\u0303(r)| \u2264 r2\u03b2T (XTX + rId)\u22122\u03b2 \u2264 r2\u2016\u03b2\u201622\nn2\u03bb2min(X TX/n+ r/n)\n\u2264 8 4r2\u03ba3M0 n2\nwith probability 1 \u2212 c\u2032\u2032 exp(\u2212c\u2032n) if n \u2265 64\u03bad log p. This result is because of Lemma 5 and M0 \u2265 var(Y ) \u2265 \u2016\u03b2\u201622\u03bbmax(\u03a3).\nFor \u03b7\u0303(r), we follow the same technique in the proof of Theorem 2. Basically, one just needs to show a similar result as Lemma 4 exists. Let A = n(XTX)\u22121XT , which is the key quantity in Lemma 4, and A\u0303 = n(XTX + rId) \u22121XT . If we can show that A\u0303 does not differ too much from A, then the proof is completed. Consider the singular value decomposition directly on X as X = V DUT (not on Z), where V is a n\u00d7 d matrix and D and U are d\u00d7 d matrices. We then have\nA = n(UD2UT )\u22121UDV T = nUD\u22121V T ,\nand\nA\u0303 = n(UD2UT + rId) \u22121UDV T = nUD\u22121 { Id + r\nn ( D\u221a n )\u22122}\u22121 V T .\nWhen r \u2264 n\u03bbmin(XTX/n)/2, we can apply Taylor expansion on the inverse. Thus\nA\u0303 = nUD\u22121 { Id + \u221e\u2211 k=1 ( r n )k( D\u221a n )\u22122k} V T\n= A+ rUD\u22121 ( D\u221a n )\u22122 V T + nUD\u22121 { \u221e\u2211 k=2 ( r n )k( D\u221a n )\u22122k} V T = A+ rU(D/ \u221a n)\u22123V T\nn1/2 + nUD\u22121 { \u221e\u2211 k=2 ( r n )k( D\u221a n )\u22122k} V T .\nClearly, we have\n\u03bbmax\n( rU(D/ \u221a n)\u22123V T\nn1/2\n) \u2264 8\n3r\u03ba3/2\u221a n ,\nand\n\u03bbmax\n[ nUD\u22121 { \u221e\u2211 k=2 ( r n )k( D\u221a n )\u22122k} V T ] \u2264 \u221a n\u03bb\u22121min ( D\u221a n ) \u221e\u2211 k=2 rk nk \u03bb\u2212kmin ( D2 n )\n\u2264 \u221a n(8\u03ba 1 2 ) \u221e\u2211 k=2 ( 82r\u03ba n )k \u2264 \u221a n(8\u03ba 1 2 )(8 2r\u03ba n )2 1\u2212 82r\u03ba n \u2264 2 \u00b7 8 5\u03ba 5 2 r2\nn3/2 .\nThe last inequality is because we assume r \u2264 n\u03bbmin(XTX/n)/2. Together, we have\n\u2016A\u0303\u2016\u221e \u2264 \u2016A\u2016\u221e + 83r\u03ba3/2\u221a\nn +\n2 \u00b7 85\u03ba 52 r2\nn3/2 ,\nwith probability at least 1 \u2212 c\u2032\u2032 exp(\u2212c\u2032n) if n \u2265 64\u03bad log p and r \u2264 n 128\u03ba . In the proof of Theorem 2, the value of t in Lemma 4 is chosen to be O(log p). Thus, as long as r \u2264 O(\u03ba\u22121 \u221a n), (10) and (11) hold for \u03b7\u0303(r) as well, i.e., for any \u03b4 \u2208 (0, 1) we have\nP ( \u2016\u03b7\u0303(r)\u2016\u221e \u2264 \u03c3\nn\u03b4/2\n) \u2265 1\u2212O ( \u03ba log p log d\nn1\u2212\u03b4\n) .\nOn the other hand, if we require r \u2264 8\u22122M\u22121/20 \u03ba\u22123/2\u03c31/2n1\u2212\u03b4/4, then we have\nmax |\u03be\u0303(r)| \u2264 8 4r2\u03ba3M0 n2 \u2264 \u03c3 n\u03b4/2 .\nConsequently, if the tuning parameter satisfies that\nr \u2264 O { min (\u221a n\n\u03ba , \u03c3\n1 2n1\u2212\u03b4/4\n82M 1 2\n0 \u03ba 3 2\n)} ,\nand n \u2265 64\u03bad log p, then we have\nP ( \u2016\u03b2\u0302(Ridge)(r)\u2212 \u03b2M\u0303d\u2016\u221e \u2264 \u03c3\nn\u03b4/2\n) \u2265 1\u2212O ( \u03ba log p log d\nn1\u2212\u03b4\n) . (13)\nTherefore, if \u03c4 \u2265 4\u03c3 n\u03b4/2 , then by choosing \u03b3\u2032(r) = 2\u03c3 n\u03b4/2 we have\nmin i\u2208S |\u03b2\u0302(Ridge)i (r)| \u2265 \u03b3\u2032 \u2265 max i 6\u2208S |\u03b2\u0302(Ridge)i (r)|.\nProof of Corollary 1. As mentioned before, we have \u03b2\u0302(OLS) = \u03b2M\u0303d+(X T M\u0303d XM\u0303d) \u22121XM\u0303d\u03b5. Because \u03b5i \u223c N(0, \u03c32) for i = 1, 2, \u00b7 \u00b7 \u00b7 , n, we have for any i \u2208 M\u0303d,\n\u03b7\u0303i = e T i (X T M\u0303d XM\u0303d) \u22121XTM\u0303d\u03b5 \u223c N(0, \u03c3 2eTi (X T M\u0303d XM\u0303d) \u22121ei) (d) = \u03c3 \u221a eTi (X T M\u0303d XM\u0303d) \u22121eiN(0, 1).\n(14)\nLikewise in the proof of Lemma 4, we know that as long as n \u2265 64\u03bad log p\n\u03bbmin(X T M\u0303d XM\u0303d/n) \u2265\n1\n64\u03ba .\nThus, we have\nmax i\u2208M\u0303d\neTi (X T M\u0303d XM\u0303d) \u22121ei \u2264 64\u03ba/n.\nTherefore, for any t > 0 and i \u2208 M\u0303d, with probability at least 1 \u2212 c\u2032\u2032 exp(\u2212c\u2032n) \u2212 2 exp(\u2212t2/2) we have\n|\u03b7\u0303i| \u2264 \u03c3t \u221a eTi (X\nT M\u0303d XM\u0303d)\n\u22121ei \u2264 8\u03ba 1 2\u03c3t\u221a n .\nThen for any \u03b4 > 0, if n > log(2c\u2032\u2032/\u03b4)/c\u2032, then with probability at least 1\u2212 \u03b4 we have\nmax i\u2208M\u0303d\n|\u03b7\u0303i| \u2264 8\u03c3 \u221a 2\u03ba log(4d/\u03b4)\nn . (15)\nBecause \u03c3 needs to estimated from the data, we need to obtain a bound as well. Notice that\n\u03c3\u03022 is an unbiased estimator for \u03c3, and\n\u03c3\u03022 = \u03c32 T (In \u2212XM\u0303d(X T M\u0303d XM\u0303d) \u22121XM\u0303d) \u223c \u03c32X 2(n\u2212 d) n\u2212 d ,\nwhere X 2(k) denotes a chi-square random variable with degree of freedom k. Using Proposition 5.16 in [24], we can bound \u03c3\u03022 as follows. Let K = \u2016X 2(1)\u2212 1\u2016\u03c81 . There exists some c5 > 0 such that for any t \u2265 0 we have,\nP (\u2223\u2223\u2223\u2223X 2(n\u2212 d)n\u2212 d \u2212 1 \u2223\u2223\u2223\u2223 \u2265 t) \u2264 2 exp{\u2212 c5 min(t2(n\u2212 d)K2 , t(n\u2212 d)K )} .\nHence for any \u03b4 > 0, if n > d+4K2 log(2/\u03b4)/c5, then with probability at least 1\u2212\u03b4 we have,\n|\u03c3\u03022 \u2212 \u03c32| \u2264 \u03c32/2,\nwhich implies that\n1 2 \u03c32 \u2264 \u03c3\u03022 \u2264 3 2 \u03c32.\nThen we know that\nmax i\u2208M\u0303d\n|\u03b7\u0303i| \u2264 8\u03c3 \u221a 2\u03ba log(4d/\u03b4)\nn \u2264 8 \u221a 2\u03c3\u0302\n\u221a 2\u03ba log(4d/\u03b4)\nn \u2264 8 \u221a 3\u03c3\n\u221a 2\u03ba log(4d/\u03b4)\nn .\nNow define \u03b3\u2032 = 8 \u221a 2\u03c3\u0302 \u221a\n2\u03ba log(4d/\u03b4) n . If the signal \u03c4 = mini\u2208S |\u03b2i| satisfies that\n\u03c4 \u2265 24\u03c3 \u221a 2\u03ba log(4d/\u03b4)\nn ,\nthen with probability at least 1\u2212 2\u03b4, for any i 6\u2208 S\n|\u03b2\u0302i| = |\u03b7\u0303i| \u2264 8\u03c3 \u221a 2\u03ba log(4d/\u03b4)\nn \u2264 \u03b3\u2032,\nand for i \u2208 S we have\n|\u03b2\u0302i| \u2265 \u03c4 \u2212 max i\u2208M\u0303d\n|\u03b7\u0303i| \u2265 16\u03c3 \u221a 2\u03ba log(4d/\u03b4)\nn \u2265 \u03b3\u2032.\nProof of Theorem 4\nThe result of Theorem 4 can be immediately implied from Theorem 1, 2, 3, (12) and (13)."}], "references": [{"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Statistical Methodology),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "On model selection consistency of lasso", "author": ["Peng Zhao", "Bin Yu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Variable selection via nonconcave penalized likelihood and its oracle properties", "author": ["Jianqing Fan", "Runze Li"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Nearly unbiased variable selection under minimax concave penalty", "author": ["Cun-Hui Zhang"], "venue": "The Annals of Statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Regularization and variable selection via the elastic net", "author": ["Hui Zou", "Trevor Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Sup-norm convergence rate and sign concentration property of lasso and dantzig estimators", "author": ["Karim Lounici"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "On iterative hard thresholding methods for high-dimensional m-estimation", "author": ["Prateek Jain", "Ambuj Tewari", "Purushottam Kar"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Elementary estimators for highdimensional linear regression", "author": ["Eunho Yang", "Aurelie Lozano", "Pradeep Ravikumar"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Sharp thresholds for high-dimensional and noisy sparsity recovery using-constrained quadratic programming (lasso)", "author": ["Martin J Wainwright"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Orthogonal matching pursuit for sparse signal recovery with noise", "author": ["T Tony Cai", "Lie Wang"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "The sparsity and bias of the lasso selection in highdimensional linear regression", "author": ["Cun-Hui Zhang", "Jian Huang"], "venue": "The Annals of Statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "High-dimensional ordinary least-squares projection for screening", "author": ["Xiangyu Wang", "Chenlei Leng"], "venue": "variables. https://stat.duke.edu/~xw56/holp-paper.pdf,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Extended bayesian information criteria for model selection with large model spaces", "author": ["Jiahua Chen", "Zehua Chen"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Simultaneous analysis of lasso and dantzig selector", "author": ["Peter J Bickel", "Ya\u2019acov Ritov", "Alexandre B Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Restricted eigenvalue properties for correlated gaussian designs", "author": ["Garvesh Raskutti", "Martin J Wainwright", "Bin Yu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "On the consistency theory of high dimensional variable screening", "author": ["Xiangyu Wang", "Chenlei Leng", "David B Dunson"], "venue": "arXiv preprint arXiv:1502.06895,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Regularization paths for generalized linear models via coordinate descent", "author": ["Jerome Friedman", "Trevor Hastie", "Rob Tibshirani"], "venue": "Journal of Statistical Software,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Path following and empirical bayes model selection for sparse regression", "author": ["Hua Zhou", "Artin Armagan", "David B Dunson"], "venue": "arXiv preprint arXiv:1201.3528,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "A path algorithm for constrained estimation", "author": ["Hua Zhou", "Kenneth Lange"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Stability selection", "author": ["Nicolai Meinshausen", "Peter B\u00fchlmann"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "High dimensional variable selection via tilting", "author": ["Haeran Cho", "Piotr Fryzlewicz"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Regulation of gene expression in the mammalian eye and its relevance to eye disease", "author": ["Todd E Scheetz", "Kwang-Youn A Kim", "Ruth E Swiderski", "Alisdair R Philp", "Terry A Braun", "Kevin L Knudtson", "Anne M Dorrance", "Gerald F DiBona", "Jian Huang", "Thomas L Casavant"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "To deal with this problem, Tibshirani[1] proposed `1-penalized regression, a.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "It yields a sparse solution and achieves model selection consistency and estimation consistency under certain conditions [2, 3, 4, 5].", "startOffset": 121, "endOffset": 133}, {"referenceID": 2, "context": "It yields a sparse solution and achieves model selection consistency and estimation consistency under certain conditions [2, 3, 4, 5].", "startOffset": 121, "endOffset": 133}, {"referenceID": 3, "context": "It yields a sparse solution and achieves model selection consistency and estimation consistency under certain conditions [2, 3, 4, 5].", "startOffset": 121, "endOffset": 133}, {"referenceID": 4, "context": "It yields a sparse solution and achieves model selection consistency and estimation consistency under certain conditions [2, 3, 4, 5].", "startOffset": 121, "endOffset": 133}, {"referenceID": 1, "context": "On the one hand, methods using convex penalties, such as lasso, usually require strong conditions for model selection consistency[2, 6].", "startOffset": 129, "endOffset": 135}, {"referenceID": 5, "context": "On the one hand, methods using convex penalties, such as lasso, usually require strong conditions for model selection consistency[2, 6].", "startOffset": 129, "endOffset": 135}, {"referenceID": 2, "context": "On the other hand, methods using non-convex penalties[3, 4] that can achieve model selection consistency under mild conditions often require huge computational expense.", "startOffset": 53, "endOffset": 59}, {"referenceID": 3, "context": "On the other hand, methods using non-convex penalties[3, 4] that can achieve model selection consistency under mild conditions often require huge computational expense.", "startOffset": 53, "endOffset": 59}, {"referenceID": 6, "context": "These concerns have limited the practical use of regularized methods, motivating alternative strategies such as direct hard thresholding [7].", "startOffset": 137, "endOffset": 140}, {"referenceID": 7, "context": "Related works The work that is most closely related to ours is [8], in which the authors proposed an algorithm based on OLS and the ridge regression.", "startOffset": 63, "endOffset": 66}, {"referenceID": 6, "context": "[7] proposed an iterative hard thresholding algorithm for sparse regression, which shares a similar spirit of hard thresholding as our algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "The consistency of the algorithms only needs a conditional number constraint, as opposed to the strong irrepresentable condition[2, 9] required by lasso.", "startOffset": 128, "endOffset": 134}, {"referenceID": 8, "context": "The consistency of the algorithms only needs a conditional number constraint, as opposed to the strong irrepresentable condition[2, 9] required by lasso.", "startOffset": 128, "endOffset": 134}, {"referenceID": 3, "context": "[4, 8, 10, 9, 11, 12]) that work for log p = o(n) case rely on a sub-Gaussian tail/bounded error assumption, which might fail to hold for general noise.", "startOffset": 0, "endOffset": 21}, {"referenceID": 7, "context": "[4, 8, 10, 9, 11, 12]) that work for log p = o(n) case rely on a sub-Gaussian tail/bounded error assumption, which might fail to hold for general noise.", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "[4, 8, 10, 9, 11, 12]) that work for log p = o(n) case rely on a sub-Gaussian tail/bounded error assumption, which might fail to hold for general noise.", "startOffset": 0, "endOffset": 21}, {"referenceID": 8, "context": "[4, 8, 10, 9, 11, 12]) that work for log p = o(n) case rely on a sub-Gaussian tail/bounded error assumption, which might fail to hold for general noise.", "startOffset": 0, "endOffset": 21}, {"referenceID": 10, "context": "[4, 8, 10, 9, 11, 12]) that work for log p = o(n) case rely on a sub-Gaussian tail/bounded error assumption, which might fail to hold for general noise.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": "[4, 8, 10, 9, 11, 12]) that work for log p = o(n) case rely on a sub-Gaussian tail/bounded error assumption, which might fail to hold for general noise.", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "[6] proved that lasso also works for a second-order condition similar to ours, but requires two additional strong assumptions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "A keen observation[12] reveals the following relationship immediately.", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "Instead of directly estimating \u03b2 as \u03b2\u0302, however, this new estimator of \u03b2 may be used for dimension reduction by observing \u03b2\u0302 = X (XX )\u22121X\u03b2 + X (XX )\u22121\u03b5 = \u03a6\u03b2 + \u03b7 [12].", "startOffset": 161, "endOffset": 165}, {"referenceID": 12, "context": "Alternatively use eBIC in [13] in conjunction with the obtained variable importance to select the best submodel.", "startOffset": 26, "endOffset": 30}, {"referenceID": 13, "context": "This random design allows for various correlation structures among predictors and is widely used to illustrate methods that rely on the restricted eigenvalue conditions [14, 15].", "startOffset": 169, "endOffset": 177}, {"referenceID": 14, "context": "This random design allows for various correlation structures among predictors and is widely used to illustrate methods that rely on the restricted eigenvalue conditions [14, 15].", "startOffset": 169, "endOffset": 177}, {"referenceID": 3, "context": ", var(\u03b5) = \u03c3 < \u221e, in contrast to the subGaussian/bounded error assumption seen in most high dimension literature [4, 8, 10, 9, 11].", "startOffset": 113, "endOffset": 130}, {"referenceID": 7, "context": ", var(\u03b5) = \u03c3 < \u221e, in contrast to the subGaussian/bounded error assumption seen in most high dimension literature [4, 8, 10, 9, 11].", "startOffset": 113, "endOffset": 130}, {"referenceID": 9, "context": ", var(\u03b5) = \u03c3 < \u221e, in contrast to the subGaussian/bounded error assumption seen in most high dimension literature [4, 8, 10, 9, 11].", "startOffset": 113, "endOffset": 130}, {"referenceID": 8, "context": ", var(\u03b5) = \u03c3 < \u221e, in contrast to the subGaussian/bounded error assumption seen in most high dimension literature [4, 8, 10, 9, 11].", "startOffset": 113, "endOffset": 130}, {"referenceID": 10, "context": ", var(\u03b5) = \u03c3 < \u221e, in contrast to the subGaussian/bounded error assumption seen in most high dimension literature [4, 8, 10, 9, 11].", "startOffset": 113, "endOffset": 130}, {"referenceID": 5, "context": "This relaxation is similar to [6]; however we do not require any further assumptions needed by [6].", "startOffset": 30, "endOffset": 33}, {"referenceID": 5, "context": "This relaxation is similar to [6]; however we do not require any further assumptions needed by [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 12, "context": "However, the corresponding details will not be pursued here, as their consistency is straightforwardly implied by the results from this section and the existing literature on extended BIC and BIC [13].", "startOffset": 196, "endOffset": 200}, {"referenceID": 15, "context": "In particular, it is shown that the diagonal terms of \u03a6 are O( p ) while the off-diagonal terms are O( \u221a n p ) [16].", "startOffset": 111, "endOffset": 115}, {"referenceID": 12, "context": "Alternatively, we can construct a series of nested models formed by ranking the largest n coefficients and adopt the extended BIC [13] to select the best submodel.", "startOffset": 130, "endOffset": 134}, {"referenceID": 4, "context": "In particular, we compare the two methods to existing penalized methods including lasso, elastic net (enet [5]), scad [3] and mc+ [4].", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "In particular, we compare the two methods to existing penalized methods including lasso, elastic net (enet [5]), scad [3] and mc+ [4].", "startOffset": 118, "endOffset": 121}, {"referenceID": 3, "context": "In particular, we compare the two methods to existing penalized methods including lasso, elastic net (enet [5]), scad [3] and mc+ [4].", "startOffset": 130, "endOffset": 133}, {"referenceID": 16, "context": "We code LAT and RAT in Matlab, use glmnet[17] for enet and lasso, and SparseReg[18, 19] for scad and mc+.", "startOffset": 41, "endOffset": 45}, {"referenceID": 17, "context": "We code LAT and RAT in Matlab, use glmnet[17] for enet and lasso, and SparseReg[18, 19] for scad and mc+.", "startOffset": 79, "endOffset": 87}, {"referenceID": 18, "context": "We code LAT and RAT in Matlab, use glmnet[17] for enet and lasso, and SparseReg[18, 19] for scad and mc+.", "startOffset": 79, "endOffset": 87}, {"referenceID": 4, "context": "This example is Example 4 in [5], for which we allocate the 15 true variables into three groups.", "startOffset": 29, "endOffset": 32}, {"referenceID": 19, "context": "This model is also considered in [20] and [21].", "startOffset": 33, "endOffset": 37}, {"referenceID": 20, "context": "This model is also considered in [20] and [21].", "startOffset": 42, "endOffset": 46}, {"referenceID": 12, "context": "We use the extended BIC [13] to choose the parameters for any regularized algorithm.", "startOffset": 24, "endOffset": 28}, {"referenceID": 21, "context": "This dataset, taken from [22], was collected to study mammalian eye diseases, with gene expression for the eye tissues of 120 twelve-week-old male F2 rats recorded.", "startOffset": 25, "endOffset": 29}, {"referenceID": 21, "context": "Following the method in [22], 18976 probes were selected as they exhibited sufficient signal for reliable analysis and at least 2-fold variation in expressions.", "startOffset": 24, "endOffset": 28}], "year": 2015, "abstractText": "Ordinary least squares (OLS) is the default method for fitting linear models, but is not applicable for problems with dimensionality larger than the sample size. For these problems, we advocate the use of a generalized version of OLS motivated by ridge regression, and propose two novel three-step algorithms involving least squares fitting and hard thresholding. The algorithms are methodologically simple to understand intuitively, computationally easy to implement efficiently, and theoretically appealing for choosing models consistently. Numerical exercises comparing our methods with penalization-based approaches in simulations and data analyses illustrate the great potential of the proposed algorithms.", "creator": "LaTeX with hyperref package"}}}