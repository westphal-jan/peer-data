{"id": "1506.02327", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2015", "title": "A Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN) for Unsupervised Discovery of Linguistic Units and Generation of High Quality Features", "abstract": "This paper summarizes the work done by the authors for the Zero Resource Speech Challenge organized in the technical program of Interspeech 2015. The goal of the challenge is to discover linguistic units directly from unlabeled speech data. The Multi-layered Acoustic Tokenizer (MAT) proposed in this work automatically discovers multiple sets of acoustic tokens from the given corpus. Each acoustic token set is specified by a set of hyperparameters that describe the model configuration. These sets of acoustic tokens carry different characteristics of the given corpus and the language behind thus can be mutually reinforced. The multiple sets of token labels are then used as the targets of a Multi-target DNN (MDNN) trained on low-level acoustic features. Bottleneck features extracted from the MDNN are used as feedback for the MAT and the MDNN itself. We call this iterative system the Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN) which generates high quality features for track 1 of the challenge and acoustic tokens for track 2 of the challenge. This task consists of performing two steps in the task: selecting the token with the most efficient algorithm (that can be implemented using the MAT model and the MAT model) and selecting the token with the most efficient algorithm (that can be implemented using the MAT model and the MAT model) and selecting the token with the most efficient algorithm (that can be implemented using the MAT model and the MAT model) and selecting the token with the most efficient algorithm (that can be implemented using the MAT model and the MAT model) and selecting the token with the most efficient algorithm (that can be implemented using the MAT model and the MAT model) and selecting the token with the most efficient algorithm (that can be implemented using the MAT model and the MAT model) and selecting the token with the most efficient algorithm (that can be implemented using the MAT model and the MAT model) and selecting the token with the most efficient algorithm (that can be implemented using the MAT model and the MAT model) and selecting the token with the most efficient algorithm (that can be implemented using the MAT model and the MAT model) and selecting the token with the most efficient algorithm (that can be implemented using the MAT model and the MAT model) and selecting the token with the most efficient algorithm (that can be implemented using the MAT model and the MAT model) and selecting the token with the most efficient algorithm (that can be implemented using the MAT model and the MAT model) and selecting the token with the most efficient algorithm (that can be implemented using", "histories": [["v1", "Sun, 7 Jun 2015 23:52:54 GMT  (272kb,D)", "http://arxiv.org/abs/1506.02327v1", "submitted to Interspeech 2015"]], "COMMENTS": "submitted to Interspeech 2015", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["cheng-tao chung", "cheng-yu tsai", "hsiang-hung lu", "yuan-ming liou", "yen-chen wu", "yen-ju lu", "hung-yi lee", "lin-shan lee"], "accepted": false, "id": "1506.02327"}, "pdf": {"name": "1506.02327.pdf", "metadata": {"source": "CRF", "title": "A Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN) for Unsupervised Discovery of Linguistic Units and Generation of High Quality Features", "authors": ["Cheng-Tao Chung", "Cheng-Yu Tsai", "Hsiang-Hung Lu", "Yuan-ming Liou", "Yen-Chen Wu", "Yen-Ju Lu", "Hung-yi Lee", "Lin-shan Lee"], "emails": ["f01921031@ntu.edu.tw,", "r02942067@ntu.edu.tw,", "r03942039@ntu.edu.tw,", "qxesqxes@gmail.com,", "r03942044@ntu.edu.tw,", "r03942063@ntu.edu.tw,", "tlkagkb93901106@gmail.com,", "lslee@gate.sinica.edu.tw"], "sections": [{"heading": "1. Introduction", "text": "Human infants acquire knowledge of a language by mere immersion in a language speaking community. The process is not yet completely understood, and is difficult to be reproduced by current automatic speech recognition (ASR) technologies where the dominant paradigm is supervised learning with large human-annotated data sets[1]. The idea behind the Zero Resource Speech Challenge is to inspire the development of speech recognition under the extreme situation where a whole language has to be learned from scratch[2, 3]. The goal of this challenge is to find linguistic units directly from raw audio with no knowledge of the language, the speaker, or any other supplementary information. This challenge includes two tracks which focuses on subword units and word units respectively. In the first track of unsupervised subword modeling, the aim is to construct a framewise feature representation of speech sounds, that is robust to within-speaker and across-speaker variation. Dynamic Time Warping (DTW) is performed on sequences of these features for predefined phone pair intervals to extract the warping distance. The performance of the feature is evaluated using the ABX discriminability [4] on within and across-speaker phone pairs. The second track focuses on discovery of word units and the aim is to extract timing information of such word units in the hypothesized vocabularies derived from the speech corpus. The intervals in which each word unit appears in the corpus is then evaluated on parsing, clustering and matching quality [5]. This paper serves as the documentation for the work by a team organized in National Taiwan University submitted to the challenge within the Interspeech 2015 technical program.\nIn this work, we propose a completely unsupervised framework of Multi-layered Acoustic Tokenizing Deep Neural Network (MATDNN) for the task. A Multi-layered Acoustic Tokenizer (MAT) is used to generate multiple sets of acoustic tokens. Each acoustic token set is specified by a pair of hyperparameters representing model granularities of the tokens. As a naming convention, we call an acoustic token set obtained from a hyperparameter pair a layer. Each layer carries complementary knowledge about the corpus and the language behind[6]. Since it is well known that speech signals have multi-level structures including at least phonemes and words which are helpful in analysing or decoding speech [7], these sets of acoustic tokens can be further mutually reinforced[8]. The multi-layered token labels generated by the MAT are then used as the training targets of a Multi-target Deep Neural Network[9] (MDNN) to learn the framewise bottleneck features[10] (BNFs). The BNFs are then used as feedback to both\nthe MAT and the MDNN in the next iteration. The BNFs from the MDNN are evaluated in Track 1, while the time intervals for acoustic tokens obtained in the MAT are evaluated in Track 2."}, {"heading": "2. Proposed Approach", "text": ""}, {"heading": "2.1. Overview of the proposed framework", "text": "The framework of the approach is shown in Fig1. In the left part, the Multi-layered Acoustic Tokenizer (MAT) produces many sets of acoustic tokens using unsupervised HMMs, each describing different aspects of the given corpus. These tokens are specified by two hyperparameters describing HMM configurations. A set of acoustic tokens is obtained for each configuration by iteratively optimizing the token models and the token labels on the given acoustic corpus. Multiple pairs of hyperparameters were selected producing multi-layered token labels for the given corpus to be used as the training targets of the Multi-target Deep Neural Network (MDNN) on the right part of Fig.1. The MDNN on the right learns its parameters based on the multi-layered token labels for the given corpus as its targets from the MAT on the left, so the knowledge carried by different token sets on different layers are fused. Bottleneck features are then extracted from this MDNN. In the first iteration, some initial acoustic features are used for both the MAT and the MDNN. This gives the first set of bottleneck features. These bottleneck features are then used as feedback to both the MAT (to replace the initial acoustic features) and the MDNN (to be concatenated with the initial acoustic features to produce tandem features) in the second iteration. Such feedback can be continued iteratively. The complete framework is referred to as Multi-layered Acoustic Tokenizing Deep Neural Network (MATDNN) in this paper. The output of the MDNN (bottleneck features) is evaluated in Track 1 of the Challenge, while the time intervals for the acoustic token labels at the output of the MAT are evaluated in Track 2 of the Challenge."}, {"heading": "2.2. Multi-layered Acoustic Tokenizer", "text": "The goal in this step is to obtain multiple sets of acoustic tokens, each defined by some hyperparameters, which capture complementary aspects of the corpus. There is no knowledge regarding the corpus at all, so the process here is completely unsupervised."}, {"heading": "2.2.1. Unsupervised Token Discovery for Each layer of MAT", "text": "Using unsupervised HMMs, it is straight forward to discover acoustic tokens from the corpus for a chosen hyperparameter pair \u03c8 that determines the HMM configuration (number of states per model and number of distinct models) [11, 12, 13, 14, 15]. This can be achieved by first finding an initial label set \u03c90 based on a set of assumed tokens for all features in the corpus X as in (1) [14]. Then in each iteration t the HMM parameters \u03b8\u03c8t can be trained with the label set \u03c9t\u22121 obtained in the previous iteration as in (2), and the new label set \u03c9t can be obtained by token decoding with the obtained parameters \u03b8\u03c8t as in (3).\n\u03c90 = initialization(X), (1)\n\u03b8\u03c8t = argmax \u03b8\u03c8 P (X|\u03b8\u03c8 , \u03c9t\u22121), (2)\n\u03c9t = argmax \u03c9\nP (X|\u03b8\u03c8t , \u03c9). (3)\nar X\niv :1\n50 6.\n02 32\n7v 1\n[ cs\n.C L\n] 7\nJ un\n2 01\n5\nThe training process can be repeated with enough number of iterations until a converged set of token HMMs is obtained. The processes (2),(3) are referred to as token model optimization and token label optimization in the left part of Fig.1."}, {"heading": "2.2.2. Granularity Space of Multi-layered Acoustic Token Sets", "text": "The process explained above can be performed with different HMM configurations, each characterized by two hyperparameters: the number of states m in each acoustic token HMM, and the total number of distinct acoustic tokens n during initialization, \u03c8 = (m,n). The transcription of a signal decoded with these tokens can be considered as a temporal segmentation of the signal, so the HMM length (or number of states in each HMM) m represents the temporal granularity. The set of all distinct acoustic tokens can be considered as a segmentation of the phonetic space, so the total number n of distinct acoustic tokens represents the phonetic granularity. This gives a two-dimensional representation of the acoustic token configurations in terms of temporal and phonetic granularities as in Fig.2. Any point in this two-dimensional space in Fig.2 corresponds to an acoustic token configuration. Acoustic tokens in different layers have different model granularities that extract complementary characteristics of the corpus and the language behind, so they jointly capture knowledge about the corpus. Although the selection of the hyperparameters can be arbitrary in the above two-dimensional space, here we can select M temporal granularities (m=m1,m2,...mM ) andN phonetic granularities (n=n1,n2,...nN ), forming a two-dimensional array ofM\u00d7N hyperparameter pairs in the granularity space."}, {"heading": "2.3. Mutual Reinforcement of Multi-layered Tokens", "text": "Because all the layers obtained in the MAT above are learned in an unsupervised fashion, they are not precise. But we have many layers, each corresponding to a different pair of hyperparameters \u03c8 = (m,n), so they can be mutually reinforced. This is explained here and shown in Fig.3, including token boundary fusion and LDAbased token label re-initialization as in Fig.3(a)."}, {"heading": "2.3.1. Token Boundary Fusion", "text": "Fig.3(b) shows the token boundary when a part of an utterance is segmented into acoustic tokens on different layers with different hyperparameter pairs \u03c8 = (m,n). We define a boundary function bm,n(j) on each layer with \u03c8 = (m,n) for the possible boundary between every pair of two adjacent frames within the utterance, where j is the time index of such possible boundaries. On each layer bm,n(j)=1 if boundary j is a token boundary and 0 otherwise. All these boundary functions bm,n(j) for all different layers are then weighted and averaged to give a joint boundary function B(j). The weights consider the fact that smaller m or shorter HMMs generate more boundaries. The peaks of B(j) are then selected based on the second derivatives and some filtering and thresholding process. This gives the new segmentation of the utterance as shown at the bottom of Fig.3(b)."}, {"heading": "2.3.2. LDA-based Token Label Re-initialization", "text": "As shown in Fig.3(c), each new segment obtained above usually consists of a sequence of acoustic tokens on each layer based on the tokens defined on that layer. We now consider all the tokens on all the\ndifferent layers as different words, so we have a vocabulary of MN\u2211 i=1 ni words, i.e., there are ni words on the i-th layer and there are a total of MN layers. A new segment here is thus considered as a document (bag-of-words) composed of words (tokens) collected from all different layers. Latent Dirichlet Allocation[16] (LDA) is preformed for topic modeling, and then each document (new segment) is labeled with the most probable topic. Because in LDA a topic is characterized by a word distribution, here a token distribution across different layers may also represent a certain acoustic characteristics or a certain acoustic token. By setting the number of topics in LDA as the number of distinct tokens n (n=n1,n2,...nN ) as in subsection 2.2.2) we have a new initial label set \u03c90 as in (1) of subsection 2.2.1, in which each new segment obtained here is a new acoustic token whose ID is the topic ID obtained by LDA. This new initial label set \u03c90 is then used to re-train all the acoustic tokens on all layers of MAT as in (1)(2)(3).\nblock diagram, (b) token boundary fusion, and (c) a new segment considered as a document (bag-of-words) and a token as a word in LDA based token label re-initialization.\neach metric is shown in bold."}, {"heading": "2.4. The Multi-target DNN (MDNN)", "text": "As shown in the right part of Fig.1, token label sequence from a layer (with a pair of hyperparameters \u03c8 = (m,n)) is a valid target for supervised framewise training, although obtained in an unsupervised way. In the initial work here, we do not use the HMM states as the target, but simply take the token label as the training target. As shown in Fig.1, there are multi-layered token labels with different hyperparameter pair \u03c8 = (m,n) for each utterance, so we jointly consider all the multi-layered token labels by learning the parameters for a single DNN with a uniformly weighted cross-entropy objective at the output layer. As a result, the bottleneck feature (BNF) extracted from this DNN automatically fuse all knowledge about the corpus and the language behind learned from the different sets of acoustic tokens."}, {"heading": "2.5. The Iterative Learning Framework for MAT-DNN", "text": "Once the BNFs are extracted from the MDNN in iteration 1, they can be taken as the input of the MAT on the left of Fig.1(c) replacing the initial acoustic features. The MAT then generates updated sets of multi-layered token labels and these updated sets of multilayered token labels can be used as the updated training objective of the MDNN. The input features of the MDNN can also be updated by concatenating the initial acoustic features with the newly extracted BNFs as the tandem features. This process can be repeated for several iterations until satisfactory results are obtained. The tandem feature used as the input of the MDNN can be further augmented by concatenating unsupervised features obtained in other systems such as the Deep Boltzmann Machine[17] (DBM) posteriorgrams, LongShort Term Memory Recurrent Neural Network[18] (LSTM-RNN) autoencoder bottleneck features, and i-vectors[19] trained on MFCC. Although different from the conventional recurrent neural network (RNN) in which the recurrent structure is included in back propagation training, the concatenation of the bottleneck features with other features in the next iteration in MDNN is a kind of recurrent structure."}, {"heading": "3. Experimental Setup", "text": "The general framework of the MAT-DNN presented above allows several flexible configurations. However, in this work we train the MATDNN in the following manner. We set m=3, 5, 7, 9 states per token HMM and n=50, 100, 300, 500 distinct tokens in the MAT, which gives a total of 16 layers.\nIn the first iteration, we use the 39 dimension Mel-frequency Cepstral Coefficients (MFCC) with energy, delta and double delta as the initial acoustic features for the input to both the MAT and the MDNN. We tandem the MFCC with a window of 4 frames before and after (39x9 dimensions), and an i-vector (400 dimensions) trained on the MFCC of each evaluation interval for the input of the MDNN. The topology of the DNN is set to be 751(input)256(hidden)-256(hidden)-39(bottleneck)-(target) with 3 hidden layers. Even without the feedback and tandem features, the MAT-DNN is a powerful self-contained unsupervised feature extractor. We compared the BNF extracted in the first iteration with the Deep Boltzmann Machine posteriorgrams mentioned in section 2.5 that use the same MFCC as input. To make the comparison fair, we keep the dimensionality of these features to be 39. For the Deep Boltzmann Machine, we used the 39-dimension MFCC with a window of 5 frames before and after as the input. The configuration we used for the DBM is 429(visible)-256(hidden)-256(hidden)-39(hidden). We originally\nextracted another set of LSTM-RNN autoencoder bottleneck features as another baseline but the performance was slightly worse than the MFCC thus we omit it in any discussion here.\nIn the second iteration, we tandem the original MFCC, the BNF extracted from the first iteration, the DBM posteriorgrams, and the ivector forming a (39x9+39x9+39x9+400=1453) dimension input to the MDNN. We used the updated transcriptions as the target and extracted the BNF as the features. The MAT is trained using the zrst[20], a python wrapper for the HTK toolkit[21], srilm[22] that we developed for training unsupervised HMMs with varying model granularity. The LDA tool we used in the Mutual Reinforcement is done with MALLET[23]. The MFCC were extracted using the HTK toolkit[21]. The i-vectors were extracted using Kaldi[24]. The DBM posteriorgram is extracted using libdnn[25]. The MDNN was trained using Caffe[26]."}, {"heading": "3.1. Track 1", "text": "The two official corpora are the Buckeye corpus [27] and NCHLT Xitsonga Speech corpus [28] in English and Tsonga respectively. They are used in the evaluation based on the ABX discriminability test [4] including across and within speaker tests. The final results is in error percentage, which means the lower the better. Our results of track 1 is presented in Table 1.\nRows (1) and (11) are the official baseline MFCC features and official topline supervised phone posteriorgrams provided by the challenge organizers respectively. Row (2) is our baseline of the MFCC features, the initial acoustic features used to train all systems in this work. Row (3) is for the DBM posteriorgrams extracted from the MFCC of row (2), serving as a strong unsupervised baseline. The results in rows (4), (5) and (6) are the performance of the bottleneck features extracted in the first iteration of the MAT-DNN without applying mutual reinforcement (MR) (4), applying MR once (5), and twice (6) respectively. Row (9) is similar to row (5), except we use a wider bottleneck layer with 256 dimensions instead of 39. Rows (7) and (8) are the performance of the bottleneck features extracted in the second iteration of the MAT-DNN without applying MR (7) and applying MR once (8). The MAT of the MAT-DNN in (7) and (8) is trained using the BNF of row(5). Row (10) is similar to row (8), except only the MFCC and i-vectors are tandemed as input without other features.\nAll the features from row (2) to (10) except for (9) are confined to 39 dimensions. This allows fast and fair comparison of different algorithms. We observe that as a stand-alone feature extractor without any iterations, the MAT-DNN in row (5) outperforms the DBM baseline in (3). The effect of mutual reinforcement can be seen in the improvement from row (4) to row (5)(6) and row (7) to row(8). We observe that a single iteration of mutual reinforcement of the target of the MAT-DNN is enough to bring huge improvement to the system. The effect of iterations in the MAT-DNN can be seen by comparing rows (2), (5), (8), respectively corresponding to 0, 1, and 2 iterations. Although the performance improvement from row (2) to row (5) is notable, it dropped in the second iteration in (8). To investigate reasons of the performance drop, we widened the bottleneck feature to 256 dimensions in (9) and observed a dramatic improvement in performance. It is possible that we have not explored the full potential of the MAT-DNN as comparison between algorithms was the original goal when we designed the experiments. For a better tuned set of parameters, improvement in following iterations is to be expected on track 1. Nonetheless, the benefit of the second iteration is better observed in track 2."}, {"heading": "3.2. Track 2", "text": "The evaluation tool for track 2 provided by the challenge organizers[5] gives five main metrics plus two more scores: NED and coverage. Fig.4 shows the results for (a) English and (b) Tsonga in NED, as well as the F-measures for the five main metrics: matching, grouping, type, token, and boundary, each in a subgraph. We omit coverage here because it is almost 100% in all cases. So there are six subfigures in Fig.4(a) and (b). In each subfigure, the results for four cases are shown, they correspond to the four MAT targets used for the MDNN bottleneck features listed in rows (4), (5), (6) and (8) of Table 1. For each of these token sets, the three or six groups of bars correspond to different values of m (m=3, 5, 7 or m=3, 5, 7, 9, 11, 13), while in each group the four bars correspond to the values of n (n=50, 100, 300, 500 from left to right), where \u03c8 = (m,n) are the parameters for the token sets. Those bars in blue are better than the JHU baseline, while those in white are worse. Only the results jointly considering both within and across talker conditions are shown.\nFrom Fig.4(a) for English, it can be seen that the proposed token sets perform well in type, token and boundary scores, although much worse in matching and grouping. we see in many cases the benefits brought by MR (e.g. (6) vs (5) in type of Fig.4(a)) and the second iteration (e.g. (8) vs (6) in boundary of Fig.4(a)), especially for small values of m. In many groups for a given m, smaller values of n seemed better, probably because n=50 is close to the total number of phonemes in the language. Also, a general trend is that larger values\nJHU baseline are in bold.\nto train the bottleneck features listed in four rows of Table 1 as shown at the bottom. The four bars in each group for a value of m are for n=50, 100, 300, 500 from left to right (not shown in the figure) and \u03c8 = (m,n) are parameters for the token sets. Blue, yellow and white bars correspond to better, equal to or worse as compared to the JHU baseline at the upper left corner of each subgraph. The coverage is not shown because it is almost 100% in all cases.\nof m were better, probably because HMMs with more states were better in modelling the relatively long units; this may directly lead to the higher type, token and boundary scores.\nSimilar observations can be made for Tsonga in Fig.4(b), and the overall performance seemed to be even better as the proposed token sets perform well even in matching scores. The improvements brought by MR, the bottleneck features and the second iteration is better observed here, which gives the best cases for all the five main scores. This is probably due to the fact that more sets of tokens were available for MR and MAT-DNN on Tsonga than English. We can conclude from this observation that more token sets introduces more robustness and that leads to better token sets for the next iteration. When m goes to 13, we see that without MR in (4) of Fig.4(b)) almost all metrics degrade except for matching scores, but with MR almost all the scores consistently increases (except for NED) when m becomes larger. This suggests that MR can also prevent degradation from happening while detecting relatively long units.\nWe also selected three typical example token sets (A)(B)(C) out of the many proposed here and shown in Fig.4, and compared them with the JHU baseline[29] in Table 2 including Precision (P), Recall (R) and F-scores (F). These three example sets are also marked in Fig.4. In Table 2 those better than JHU baseline are in bold. The\nmuch higher NED and coverage scores suggest that the proposed approach is a highly permissive matching algorithm. The much higher parsing scores (type, token and boundary scores), especially the Recall and F-scores, imply the proposed approach is more successful in discovering word-like units. However, the matching and grouping scores are much worse probably because the discovered tokens cover almost the whole corpus, including short pauses or silence, and therefore many tokens are actually noises. Another possible reason might be that the values of n used are much smaller than the size of the real word vocabulary, making the same token label used for signal segments of varying characteristics and this degenerated the grouping qualities."}, {"heading": "4. Conclusion", "text": "This paper summarizes the preliminary work done for the Zero Resource Speech Challenge in Interspeech 2015. We propose a MATDNN to generate multi-layer token sets and fuse the various knowledge in different token sets in the bottleneck features. We present the complete results on all evaluations we tested up to the submission deadline, with a hope that these results serve as good references for future investigations."}, {"heading": "5. References", "text": "[1] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed,\nN. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.\n[2] C.-y. Lee and J. Glass, \u201cA nonparametric bayesian approach to acoustic model discovery,\u201d in Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012, pp. 40\u201349.\n[3] M.-h. Siu, H. Gish, A. Chan, W. Belfield, and S. Lowe, \u201cUnsupervised training of an hmm-based self-organizing unit recognizer with applications to topic classification and keyword discovery,\u201d Computer Speech & Language, vol. 28, no. 1, pp. 210\u2013223, 2014.\n[4] T. Schatz, V. Peddinti, F. Bach, A. Jansen, H. Hermansky, and E. Dupoux, \u201cEvaluating speech features with the minimalpair abx task: Analysis of the classical mfc/plp pipeline,\u201d in INTERSPEECH 2013: 14th Annual Conference of the International Speech Communication Association, 2013, pp. 1\u20135.\n[5] B. Ludusan, M. Versteegh, A. Jansen, G. Gravier, X.-N. Cao, M. Johnson, and E. Dupoux, \u201cBridging the gap between speech technology and natural language processing: an evaluation toolbox for term discovery systems,\u201d in Language Resources and Evaluation Conference, 2014.\n[6] C.-T. Chung, C.-a. Chan, and L.-s. Lee, \u201cUnsupervised spoken term detection with spoken queries by multi-level acoustic patterns with varying model granularity,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014.\n[7] Y.-c. Pan and L.-s. Lee, \u201cPerformance analysis for latticebased speech indexing approaches using words and subword units,\u201d Audio, Speech, and Language Processing, IEEE Transactions on, vol. 18, no. 6, pp. 1562\u20131574, 2010.\n[8] C.-T. Chung, W.-N. Hsu, C.-Y. Lee, and L.-S. Lee, \u201cEnhancing automatically discovered multi-level acoustic patterns considering context consistency with applications in spoken term detection,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015.\n[9] N. T. Vu, J. Weiner, and T. Schultz, \u201cInvestigating the learning effect of multilingual bottle-neck features for asr,\u201d in Fifteenth Annual Conference of the International Speech Communication Association, 2014.\n[10] K. Vesely, M. Karafia\u0301t, F. Grezl, M. Janda, and E. Egorova, \u201cThe language-independent bottleneck features,\u201d in Spoken Language Technology Workshop (SLT), 2012 IEEE. IEEE, 2012, pp. 336\u2013341.\n[11] A. Jansen and K. Church, \u201cTowards unsupervised training of speaker independent acoustic models.\u201d in INTERSPEECH, 2011, pp. 1693\u20131692.\n[12] H. Gish, M.-h. Siu, A. Chan, and W. Belfield, \u201cUnsupervised training of an hmm-based speech recognizer for topic classification.\u201d in INTERSPEECH, 2009, pp. 1935\u20131938.\n[13] M.-H. Siu, H. Gish, A. Chan, and W. Belfield, \u201cImproved topic classification and keyword discovery using an hmmbased speech recognizer trained without supervision.\u201d in INTERSPEECH, 2010, pp. 2838\u20132841.\n[14] C.-T. Chung, C.-a. Chan, and L.-s. Lee, \u201cUnsupervised discovery of linguistic structure including two-level acoustic patterns using three cascaded stages of iterative optimization,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8081\u2013 8085.\n[15] M. Creutz and K. Lagus, \u201cUnsupervised models for morpheme segmentation and morphology learning,\u201d ACM Transactions on Speech and Language Processing (TSLP), vol. 4, no. 1, p. 3, 2007.\n[16] D. M. Blei, A. Y. Ng, and M. I. Jordan, \u201cLatent dirichlet allocation,\u201d the Journal of machine Learning research, vol. 3, pp. 993\u20131022, 2003.\n[17] R. Salakhutdinov and G. E. Hinton, \u201cDeep boltzmann machines,\u201d in International Conference on Artificial Intelligence and Statistics, 2009, pp. 448\u2013455.\n[18] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[19] A. Kanagasundaram, R. Vogt, D. B. Dean, S. Sridharan, and M. W. Mason, \u201cI-vector based speaker recognition on short utterances,\u201d in Proceedings of the 12th Annual Conference of the International Speech Communication Association. International Speech Communication Association (ISCA), 2011, pp. 2341\u20132344.\n[20] C.-T. Chung, \u201czrst,\u201d https://github.com/C2Tao/zrst, 2014.\n[21] S. Young, G. Evermann, M. Gales, T. Hain, D. Kershaw, X. Liu, G. Moore, J. Odell, D. Ollason, D. Povey et al., The HTK book. Entropic Cambridge Research Laboratory Cambridge, 1997, vol. 2.\n[22] A. Stolcke et al., \u201cSrilm-an extensible language modeling toolkit.\u201d in INTERSPEECH, 2002.\n[23] A. K. McCallum, \u201c{MALLET: A Machine Learning for Language Toolkit},\u201d 2002.\n[24] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz, J. Silovsky, G. Stemmer, and K. Vesely, \u201cThe kaldi speech recognition toolkit,\u201d in IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society, Dec. 2011, iEEE Catalog No.: CFP11SRWUSB.\n[25] P.-W. Chou, \u201clibdnn,\u201d https://github.com/botonchou/libdnn, 2014.\n[26] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell, \u201cCaffe: Convolutional architecture for fast feature embedding,\u201d arXiv preprint arXiv:1408.5093, 2014.\n[27] M. A. Pitt, L. Dilley, K. Johnson, S. Kiesling, W. Raymond, E. Hume, and E. Fosler-Lussier, \u201cBuckeye corpus of conversational speech (2nd release),\u201d Columbus, OH: Department of Psychology, Ohio State University, 2007.\n[28] N. J. De Vries, M. H. Davel, J. Badenhorst, W. D. Basson, F. De Wet, E. Barnard, and A. De Waal, \u201cA smartphonebased asr data collection tool for under-resourced languages,\u201d Speech communication, vol. 56, pp. 119\u2013131, 2014.\n[29] A. Jansen and B. Van Durme, \u201cEfficient spoken term discovery using randomized algorithms,\u201d in Automatic Speech Recognition and Understanding (ASRU), 2011 IEEE Workshop on. IEEE, 2011, pp. 401\u2013406."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "A nonparametric bayesian approach to acoustic model discovery", "author": ["C.-y. Lee", "J. Glass"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012, pp. 40\u201349.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised training of an hmm-based self-organizing unit recognizer with applications to topic classification and keyword discovery", "author": ["M.-h. Siu", "H. Gish", "A. Chan", "W. Belfield", "S. Lowe"], "venue": "Computer Speech & Language, vol. 28, no. 1, pp. 210\u2013223, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Evaluating speech features with the minimalpair abx task: Analysis of the classical mfc/plp pipeline", "author": ["T. Schatz", "V. Peddinti", "F. Bach", "A. Jansen", "H. Hermansky", "E. Dupoux"], "venue": "INTERSPEECH 2013: 14th Annual Conference of the International Speech Communication Association, 2013, pp. 1\u20135.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Bridging the gap between speech technology and natural language processing: an evaluation toolbox for term discovery systems", "author": ["B. Ludusan", "M. Versteegh", "A. Jansen", "G. Gravier", "X.-N. Cao", "M. Johnson", "E. Dupoux"], "venue": "Language Resources and Evaluation Conference, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised spoken term detection with spoken queries by multi-level acoustic patterns with varying model granularity", "author": ["C.-T. Chung", "C.-a. Chan", "L.-s. Lee"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Performance analysis for latticebased speech indexing approaches using words and subword units", "author": ["Y.-c. Pan", "L.-s. Lee"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 18, no. 6, pp. 1562\u20131574, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Enhancing automatically discovered multi-level acoustic patterns considering context consistency with applications in spoken term detection", "author": ["C.-T. Chung", "W.-N. Hsu", "C.-Y. Lee", "L.-S. Lee"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Investigating the learning effect of multilingual bottle-neck features for asr", "author": ["N.T. Vu", "J. Weiner", "T. Schultz"], "venue": "Fifteenth Annual Conference of the International Speech Communication Association, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "The language-independent bottleneck features", "author": ["K. Vesely", "M. Karafi\u00e1t", "F. Grezl", "M. Janda", "E. Egorova"], "venue": "Spoken Language Technology Workshop (SLT), 2012 IEEE. IEEE, 2012, pp. 336\u2013341.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Towards unsupervised training of speaker independent acoustic models.", "author": ["A. Jansen", "K. Church"], "venue": "in INTERSPEECH,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Unsupervised training of an hmm-based speech recognizer for topic classification.", "author": ["H. Gish", "M.-h. Siu", "A. Chan", "W. Belfield"], "venue": "INTERSPEECH,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Improved topic classification and keyword discovery using an hmmbased speech recognizer trained without supervision.", "author": ["M.-H. Siu", "H. Gish", "A. Chan", "W. Belfield"], "venue": "TERSPEECH,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Unsupervised discovery of linguistic structure including two-level acoustic patterns using three cascaded stages of iterative optimization", "author": ["C.-T. Chung", "C.-a. Chan", "L.-s. Lee"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8081\u2013 8085.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised models for morpheme segmentation and morphology learning", "author": ["M. Creutz", "K. Lagus"], "venue": "ACM Transactions on Speech and Language Processing (TSLP), vol. 4, no. 1, p. 3, 2007.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "the Journal of machine Learning research, vol. 3, pp. 993\u20131022, 2003.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "International Conference on Artificial Intelligence and Statistics, 2009, pp. 448\u2013455.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "I-vector based speaker recognition on short utterances", "author": ["A. Kanagasundaram", "R. Vogt", "D.B. Dean", "S. Sridharan", "M.W. Mason"], "venue": "Proceedings of the 12th Annual Conference of the International Speech Communication Association. International Speech Communication Association (ISCA), 2011, pp. 2341\u20132344.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "zrst", "author": ["C.-T. Chung"], "venue": "https://github.com/C2Tao/zrst, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "The HTK book", "author": ["S. Young", "G. Evermann", "M. Gales", "T. Hain", "D. Kershaw", "X. Liu", "G. Moore", "J. Odell", "D. Ollason", "D. Povey"], "venue": "Entropic Cambridge Research Laboratory Cambridge,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1997}, {"title": "Srilm-an extensible language modeling toolkit.", "author": ["A. Stolcke"], "venue": "in INTERSPEECH,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "MALLET: A Machine Learning for Language Toolkit", "author": ["A.K. McCallum"], "venue": "2002.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely"], "venue": "IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society, Dec. 2011, iEEE Catalog No.: CFP11SRW- USB.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "libdnn", "author": ["P.-W. Chou"], "venue": "https://github.com/botonchou/libdnn, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Buckeye corpus of conversational speech (2nd release)", "author": ["M.A. Pitt", "L. Dilley", "K. Johnson", "S. Kiesling", "W. Raymond", "E. Hume", "E. Fosler-Lussier"], "venue": "Columbus, OH: Department of Psychology, Ohio State University, 2007.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "A smartphonebased asr data collection tool for under-resourced languages", "author": ["N.J. De Vries", "M.H. Davel", "J. Badenhorst", "W.D. Basson", "F. De Wet", "E. Barnard", "A. De Waal"], "venue": "Speech communication, vol. 56, pp. 119\u2013131, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient spoken term discovery using randomized algorithms", "author": ["A. Jansen", "B. Van Durme"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2011 IEEE Workshop on. IEEE, 2011, pp. 401\u2013406.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "The process is not yet completely understood, and is difficult to be reproduced by current automatic speech recognition (ASR) technologies where the dominant paradigm is supervised learning with large human-annotated data sets[1].", "startOffset": 226, "endOffset": 229}, {"referenceID": 1, "context": "The idea behind the Zero Resource Speech Challenge is to inspire the development of speech recognition under the extreme situation where a whole language has to be learned from scratch[2, 3].", "startOffset": 184, "endOffset": 190}, {"referenceID": 2, "context": "The idea behind the Zero Resource Speech Challenge is to inspire the development of speech recognition under the extreme situation where a whole language has to be learned from scratch[2, 3].", "startOffset": 184, "endOffset": 190}, {"referenceID": 3, "context": "The performance of the feature is evaluated using the ABX discriminability [4] on within and across-speaker phone pairs.", "startOffset": 75, "endOffset": 78}, {"referenceID": 4, "context": "The intervals in which each word unit appears in the corpus is then evaluated on parsing, clustering and matching quality [5].", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": "Each layer carries complementary knowledge about the corpus and the language behind[6].", "startOffset": 83, "endOffset": 86}, {"referenceID": 6, "context": "Since it is well known that speech signals have multi-level structures including at least phonemes and words which are helpful in analysing or decoding speech [7], these sets of acoustic tokens can be further mutually reinforced[8].", "startOffset": 159, "endOffset": 162}, {"referenceID": 7, "context": "Since it is well known that speech signals have multi-level structures including at least phonemes and words which are helpful in analysing or decoding speech [7], these sets of acoustic tokens can be further mutually reinforced[8].", "startOffset": 228, "endOffset": 231}, {"referenceID": 8, "context": "The multi-layered token labels generated by the MAT are then used as the training targets of a Multi-target Deep Neural Network[9] (MDNN) to learn the framewise bottleneck features[10] (BNFs).", "startOffset": 127, "endOffset": 130}, {"referenceID": 9, "context": "The multi-layered token labels generated by the MAT are then used as the training targets of a Multi-target Deep Neural Network[9] (MDNN) to learn the framewise bottleneck features[10] (BNFs).", "startOffset": 180, "endOffset": 184}, {"referenceID": 10, "context": "Using unsupervised HMMs, it is straight forward to discover acoustic tokens from the corpus for a chosen hyperparameter pair \u03c8 that determines the HMM configuration (number of states per model and number of distinct models) [11, 12, 13, 14, 15].", "startOffset": 224, "endOffset": 244}, {"referenceID": 11, "context": "Using unsupervised HMMs, it is straight forward to discover acoustic tokens from the corpus for a chosen hyperparameter pair \u03c8 that determines the HMM configuration (number of states per model and number of distinct models) [11, 12, 13, 14, 15].", "startOffset": 224, "endOffset": 244}, {"referenceID": 12, "context": "Using unsupervised HMMs, it is straight forward to discover acoustic tokens from the corpus for a chosen hyperparameter pair \u03c8 that determines the HMM configuration (number of states per model and number of distinct models) [11, 12, 13, 14, 15].", "startOffset": 224, "endOffset": 244}, {"referenceID": 13, "context": "Using unsupervised HMMs, it is straight forward to discover acoustic tokens from the corpus for a chosen hyperparameter pair \u03c8 that determines the HMM configuration (number of states per model and number of distinct models) [11, 12, 13, 14, 15].", "startOffset": 224, "endOffset": 244}, {"referenceID": 14, "context": "Using unsupervised HMMs, it is straight forward to discover acoustic tokens from the corpus for a chosen hyperparameter pair \u03c8 that determines the HMM configuration (number of states per model and number of distinct models) [11, 12, 13, 14, 15].", "startOffset": 224, "endOffset": 244}, {"referenceID": 13, "context": "This can be achieved by first finding an initial label set \u03c90 based on a set of assumed tokens for all features in the corpus X as in (1) [14].", "startOffset": 138, "endOffset": 142}, {"referenceID": 15, "context": "Latent Dirichlet Allocation[16] (LDA) is preformed for topic modeling, and then each document (new segment) is labeled with the most probable topic.", "startOffset": 27, "endOffset": 31}, {"referenceID": 16, "context": "The tandem feature used as the input of the MDNN can be further augmented by concatenating unsupervised features obtained in other systems such as the Deep Boltzmann Machine[17] (DBM) posteriorgrams, LongShort Term Memory Recurrent Neural Network[18] (LSTM-RNN) autoencoder bottleneck features, and i-vectors[19] trained on MFCC.", "startOffset": 173, "endOffset": 177}, {"referenceID": 17, "context": "The tandem feature used as the input of the MDNN can be further augmented by concatenating unsupervised features obtained in other systems such as the Deep Boltzmann Machine[17] (DBM) posteriorgrams, LongShort Term Memory Recurrent Neural Network[18] (LSTM-RNN) autoencoder bottleneck features, and i-vectors[19] trained on MFCC.", "startOffset": 246, "endOffset": 250}, {"referenceID": 18, "context": "The tandem feature used as the input of the MDNN can be further augmented by concatenating unsupervised features obtained in other systems such as the Deep Boltzmann Machine[17] (DBM) posteriorgrams, LongShort Term Memory Recurrent Neural Network[18] (LSTM-RNN) autoencoder bottleneck features, and i-vectors[19] trained on MFCC.", "startOffset": 308, "endOffset": 312}, {"referenceID": 19, "context": "The MAT is trained using the zrst[20], a python wrapper for the HTK toolkit[21], srilm[22] that we developed for training unsupervised HMMs with varying model granularity.", "startOffset": 33, "endOffset": 37}, {"referenceID": 20, "context": "The MAT is trained using the zrst[20], a python wrapper for the HTK toolkit[21], srilm[22] that we developed for training unsupervised HMMs with varying model granularity.", "startOffset": 75, "endOffset": 79}, {"referenceID": 21, "context": "The MAT is trained using the zrst[20], a python wrapper for the HTK toolkit[21], srilm[22] that we developed for training unsupervised HMMs with varying model granularity.", "startOffset": 86, "endOffset": 90}, {"referenceID": 22, "context": "The LDA tool we used in the Mutual Reinforcement is done with MALLET[23].", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "The MFCC were extracted using the HTK toolkit[21].", "startOffset": 45, "endOffset": 49}, {"referenceID": 23, "context": "The i-vectors were extracted using Kaldi[24].", "startOffset": 40, "endOffset": 44}, {"referenceID": 24, "context": "The DBM posteriorgram is extracted using libdnn[25].", "startOffset": 47, "endOffset": 51}, {"referenceID": 25, "context": "The MDNN was trained using Caffe[26].", "startOffset": 32, "endOffset": 36}, {"referenceID": 26, "context": "The two official corpora are the Buckeye corpus [27] and NCHLT Xitsonga Speech corpus [28] in English and Tsonga respectively.", "startOffset": 48, "endOffset": 52}, {"referenceID": 27, "context": "The two official corpora are the Buckeye corpus [27] and NCHLT Xitsonga Speech corpus [28] in English and Tsonga respectively.", "startOffset": 86, "endOffset": 90}, {"referenceID": 3, "context": "They are used in the evaluation based on the ABX discriminability test [4] including across and within speaker tests.", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "The evaluation tool for track 2 provided by the challenge organizers[5] gives five main metrics plus two more scores: NED and coverage.", "startOffset": 68, "endOffset": 71}, {"referenceID": 28, "context": "4, and compared them with the JHU baseline[29] in Table 2 including Precision (P), Recall (R) and F-scores (F).", "startOffset": 42, "endOffset": 46}], "year": 2015, "abstractText": "This paper summarizes the work done by the authors for the Zero Resource Speech Challenge organized in the technical program of Interspeech 2015. The goal of the challenge is to discover linguistic units directly from unlabeled speech data. The Multi-layered Acoustic Tokenizer (MAT) proposed in this work automatically discovers multiple sets of acoustic tokens from the given corpus. Each acoustic token set is specified by a set of hyperparameters that describe the model configuration. These sets of acoustic tokens carry different characteristics of the given corpus and the language behind thus can be mutually reinforced. The multiple sets of token labels are then used as the targets of a Multi-target DNN (MDNN) trained on low-level acoustic features. Bottleneck features extracted from the MDNN are used as feedback for the MAT and the MDNN itself. We call this iterative system the Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN) which generates high quality features for track 1 of the challenge and acoustic tokens for track 2 of the challenge.", "creator": "LaTeX with hyperref package"}}}