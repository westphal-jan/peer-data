{"id": "1506.02428", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Robust Regression via Hard Thresholding", "abstract": "We study the problem of Robust Least Squares Regression (RLSR) where several response variables can be adversarially corrupted. More specifically, for a data matrix X \\in R^{p x n} and an underlying model w*, the response vector is generated as y = X'w* + b where b \\in R^n is the corruption vector supported over at most C.n coordinates. Existing exact recovery results for RLSR focus solely on L1-penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of X. For example, for the linearity matrix W, the regression coefficients for F are 0 and 1 . If F is 0, L1 = 0; F = 0; F = 0; F = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0; C = 0;", "histories": [["v1", "Mon, 8 Jun 2015 10:13:53 GMT  (75kb,D)", "http://arxiv.org/abs/1506.02428v1", "24 pages, 3 figures"]], "COMMENTS": "24 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["kush bhatia", "prateek jain 0002", "purushottam kar"], "accepted": true, "id": "1506.02428"}, "pdf": {"name": "1506.02428.pdf", "metadata": {"source": "CRF", "title": "Robust Regression via Hard Thresholding", "authors": ["Kush Bhatia", "Prateek Jain", "Purushottam Kar"], "emails": ["t-kushb@microsoft.com", "prajain@microsoft.com", "t-purkar@microsoft.com"], "sections": [{"heading": null, "text": "In this work, we study a simple hard-thresholding algorithm called Torrent which, under mild conditions on X, can recover w\u2217 exactly even if b corrupts the response variables in an adversarial manner, i.e. both the support and entries of b are selected adversarially after observing X and w\u2217. Our results hold under deterministic assumptions which are satisfied if X is sampled from any sub-Gaussian distribution. Finally unlike existing results that apply only to a fixed w\u2217, generated independently of X, our results are universal and hold for any w\u2217 \u2208 Rp.\nNext, we propose gradient descent-based extensions of Torrent that can scale efficiently to large scale problems, such as high dimensional sparse recovery and prove similar recovery guarantees for these extensions. Empirically we find Torrent, and more so its extensions, offering significantly faster recovery than the state-of-the-art L1 solvers. For instance, even on moderate-sized datasets (with p = 50K) with around 40% corrupted responses, a variant of our proposed method called Torrent-HYB is more than 20\u00d7 faster than the best L1 solver.\n\u201cIf among these errors are some which appear too large to be admissible, then those equations which produced these errors will be rejected, as coming from too faulty experiments, and the unknowns will be determined by means of the other equations, which will then give much smaller errors.\u201d\nA. M. Legendre, On the Method of Least Squares. 1805."}, {"heading": "1 Introduction", "text": "Robust Least Squares Regression (RLSR) addresses the problem of learning a reliable set of regression coefficients in the presence of several arbitrary corruptions in the response vector. Owing to the wideapplicability of regression, RLSR features as a critical component of several important real-world applications in a variety of domains such as signal processing [13], economics [12], computer vision [16, 15], and astronomy [12].\nGiven a data matrix X = [x1, . . . ,xn] with n data points in Rp and the corresponding response vector y \u2208 Rn, the goal of RLSR is to learn a w\u0302 such that,\n(w\u0302, S\u0302) = arg min w\u2208Rp\nS\u2282[n]:|S|\u2265(1\u2212\u03b2)\u00b7n\n\u2211 i\u2208S (yi \u2212 xTi w)2, (1)\nThat is, we wish to simultaneously determine the set of corruption free points S\u0302 and also estimate the best model parameters over the set of clean points. However, the optimization problem given above is non-\nar X\niv :1\n50 6.\n02 42\n8v 1\n[ cs\n.L G\n] 8\nJ un\n2 01\n5\nconvex (jointly in w and S) in general and might not directly admit efficient solutions. Indeed there exist reformulations of this problem that are known to be NP-hard to optimize [13].\nTo address this problem, most existing methods with provable guarantees assume that the observations are obtained from some generative model. A commonly adopted model is the following\ny = XTw\u2217 + b, (2)\nwhere w\u2217 \u2208 Rp is the true model vector that we wish to estimate and b \u2208 Rn is the corruption vector that can have arbitrary values. A common assumption is that the corruption vector is sparsely supported i.e. \u2016b\u20160 \u2264 \u03b1 \u00b7 n for some \u03b1 > 0.\nRecently, [15] and [9] obtained a surprising result which shows that one can recover w\u2217 exactly even when \u03b1 . 1, i.e., when almost all the points are corrupted, by solving an L1-penalty based convex optimization problem: minw,b \u2016w\u20161+\u03bb \u2016b\u20161, s.t., X>w+b = y. However, these results require the corruption vector b to be selected oblivious of X and w\u2217. Moreover, the results impose severe restrictions on the data distribution, requiring that the data be either sampled from an isotropic Gaussian ensemble [15], or row-sampled from an incoherent orthogonal matrix [9]. Finally, these results hold only for a fixed w\u2217 and are not universal in general.\nIn contrast, [4] studied RLSR with less stringent assumptions, allowing arbitrary corruptions in response variables as well as in the data matrix X, and proposed a trimmed inner product based algorithm for the problem. However, their recovery guarantees are significantly weaker. Firstly, they are able to recover w\u2217 only upto an additive error \u03b1 \u221a p (or \u03b1 \u221a s if w\u2217 is s-sparse). Hence, they require \u03b1 \u2264 1/\u221ap just to claim a non-trivial bound. Note that this amounts to being able to tolerate only a vanishing fraction of corruptions. More importantly, even with n \u2192 \u221e and extremely small \u03b1 they are unable to guarantee exact recovery of w\u2217. A similar result was obtained by [8], albeit using a sub-sampling based algorithm with stronger assumptions on b.\nIn this paper, we focus on a simple and natural thresholding based algorithm for RLSR. At a high level, at each step t, our algorithm alternately estimates an active set St of \u201cclean\u201d points and then updates the model to obtain wt+1 by minimizing the least squares error on the active set. This intuitive algorithm seems to embody a long standing heuristic first proposed by Legendre [1] over two centuries ago (see introductory quotation in this paper) that has been adopted in later literature [10, 11] as well. However, to the best of our knowledge, this technique has never been rigorously analyzed before in non-asymptotic settings, despite its appealing simplicity.\nOur Contributions: The main contribution of this paper is an exact recovery guarantee for the thresholding algorithm mentioned above that we refer to as Torrent-FC (see Algorithm 1). We provide our guarantees in the model given in 2 where the corruptions b are selected adversarially but restricted to have at most \u03b1 \u00b7 n non-zero entries where \u03b1 < 1/2 is a global constant dependent only on X1. Under deterministic conditions on X, namely the subset strong convexity (SSC) and smoothness (SSS) properties (see Definition 1), we guarantee that Torrent-FC converges at a geometric rate and recovers w\u2217 exactly. We further show that these properties (SSC and SSS) are satisfied w.h.p. if a) the data X is sampled from a sub-Gaussian distribution and, b) n \u2265 p log p.\nWe would like to stress three key advantages of our result over the results of [15, 9]: a) we allow b to be adversarial, i.e., both support and values of b to be selected adversarially based on X and w\u2217, b) we make assumptions on data that are natural, as well as significantly less restrictive than what existing methods make, and c) our analysis admits universal guarantees, i.e., holds for any w\u2217.\nWe would also like to stress that while hard-thresholding based methods have been studied rigorously for the sparse-recovery problem [3, 6], hard-thresholding has not been studied formally for the robust regression problem. Moreover, the two problems are completely different and hence techniques from sparse-recovery analysis do not extend to robust regression.\n1Note that for an adaptive adversary, as is the case in our work, recovery cannot be guaranteed for \u03b1 \u2265 1/2 since the adversary can introduce corruptions as bi = x > i (w\u0303\u2212w\u2217) for an adversarially chosen model w\u0303. This would make it impossible for any algorithm to distinguish between w\u2217 and w\u0303 thus making recovery impossible.\nDespite its simplicity, Torrent-FC does not scale very well to datasets with large p as it solves least squares problems at each iteration. We address this issue by designing a gradient descent based algorithm (Torrent-GD), and a hybrid algorithm (Torrent-Hyb), both of which enjoy a geometric rate of convergence and can recover w\u2217 under the model assumptions mentioned above. We also propose extensions of Torrent for the RLSR problem in the sparse regression setting where p n but \u2016w\u2217\u20160 = s\u2217 p. Our algorithm Torrent-HD is based on Torrent-FC but uses the Iterative Hard Thresholding (IHT) algorithm, a popular algorithm for sparse regression. As before, we show that Torrent-HD also converges geometrically to w\u2217 if a) the corruption index \u03b1 is less than some constant C, b) X is sampled from a sub-Gaussian distribution and, c) n \u2265 s\u2217 log p.\nFinally, we experimentally evaluate existing L1-based algorithms and our hard thresholding-based algorithms. The results demonstrate that our proposed algorithms (Torrent-(FC/GD/HYB)) can be significantly faster than the best L1 solvers, exhibit better recovery properties, as well as be more robust to dense white noise. For instance, on a problem with 50K dimensions and 40% corruption, Torrent-HYB was found to be 20\u00d7 faster than L1 solvers, as well as achieve lower error rates.\nPaper Organization: We give a formal definition of the RLSR problem in the next section. We then introduce our family of algorithms in Section 3 and prove their convergence guarantees in Section 4. We present extensions to sparse robust regression in Section 5 and empirical results in Section 6."}, {"heading": "2 Problem Formulation", "text": "Given a set of data points X = [x1,x2, . . . ,xn], where xi \u2208 Rp and the corresponding response vector y \u2208 Rn, the goal is to recover a parameter vector w\u2217 which solves the RLSR problem (1). We assume that the response vector y is generated using the following model:\ny = y\u2217 + b + \u03b5, where y\u2217 = X>w\u2217.\nHence, in the above model, (1) reduces to estimating w\u2217. We allow the model w\u2217 representing the regressor, to be chosen in an adaptive manner after the data features have been generated.\nThe above model allows two kinds of perturbations to yi \u2013 dense but bounded noise \u03b5i (e.g. white noise \u03b5i \u223c N (0, \u03c32), \u03c3 \u2265 0), as well as potentially unbounded corruptions bi \u2013 to be introduced by an adversary. The only requirement we enforce is that the gross corruptions be sparse.\n\u03b5 shall represent the dense noise vector, for example \u03b5 \u223c N (0, \u03c32 \u00b7 In\u00d7n), and b, the corruption vector such that \u2016b\u20160 \u2264 \u03b1 \u00b7 n for some corruption index \u03b1 > 0. We shall use the notation S\u2217 = supp(b) \u2286 [n] to denote the set of \u201cclean\u201d points, i.e. points that have not faced unbounded corruptions. We consider adaptive adversaries that are able to view the generated data points xi, as well as the clean responses y \u2217 i and dense noise values \u03b5i before deciding which locations to corrupt and by what amount. We denote the unit sphere in p dimensions using Sp\u22121. For any \u03b3 \u2208 (0, 1], we let S\u03b3 = {S \u2282 [n] : |S| = \u03b3 \u00b7 n} denote the set of all subsets of size \u03b3 \u00b7 n. For any set S, we let XS := [xi]i\u2208S \u2208 Rp\u00d7|S| denote the matrix whose columns are composed of points in that set. Also, for any vector v \u2208 Rn we use the notation vS to denote the |S|-dimensional vector consisting of those components that are in S. We use \u03bbmin(X) and \u03bbmax(X) to denote, respectively, the smallest and largest eigenvalues of a square symmetric matrix X. We now introduce two properties, namely, Subset Strong Convexity and Subset Strong Smoothness, which are key to our analyses.\nDefinition 1 (SSC and SSS Properties). A matrix X \u2208 Rp\u00d7n satisfies the Subset Strong Convexity Property (resp. Subset Strong Smoothness Property) at level \u03b3 with strong convexity constant \u03bb\u03b3 (resp. strong smoothness constant \u039b\u03b3) if the following holds:\n\u03bb\u03b3 \u2264 min S\u2208S\u03b3 \u03bbmin(XSX > S ) \u2264 max S\u2208S\u03b3 \u03bbmax(XSX > S ) \u2264 \u039b\u03b3 .\nRemark 1. We note that the uniformity enforced in the definitions of the SSC and SSS properties is not for the sake of convenience but rather a necessity. Indeed, a uniform bound is required in face of an adversary\nAlgorithm 1 Torrent: Thresholding Operator-based Robust RegrEssioN meThod Input: Training data {xi, yi} , i = 1 . . . n, step length \u03b7, thresholding parameter \u03b2, tolerance\n1: w0 \u2190 0, S0 = [n], t\u2190 0, r0 \u2190 y 2: while \u2225\u2225rtSt\u2225\u22252 > do 3: wt+1 \u2190 UPDATE(wt, St, \u03b7, rt, St\u22121) 4: rt+1i \u2190 ( yi \u2212 \u2329 wt+1,xi\n\u232a) 5: St+1 \u2190 HT(rt+1, (1\u2212 \u03b2)n) 6: t\u2190 t+ 1 7: end while 8: return wt\nAlgorithm 2 Torrent-FC\nInput: Current model w, current active set S\n1: return arg min w \u2211 i\u2208S (yi \u2212 \u3008w,xi\u3009)2\nAlgorithm 3 Torrent-GD\nInput: Current model w, current active set S, step size \u03b7\n1: g\u2190 XS(X>S w \u2212 yS) 2: return w \u2212 \u03b7 \u00b7 g\nAlgorithm 4 Torrent-HYB\nInput: Current model w, current active set S, step size \u03b7, current residuals r, previous active set S\u2032\n1: // Use the GD update if the active set S is changing a lot 2: if |S\\S\u2032| > \u2206 then 3: w\u2032 \u2190 UPDATE-GD(w, S, \u03b7, r, S\u2032) 4: else 5: // If stable, use the FC update 6: w\u2032 \u2190 UPDATE-FC(w, S) 7: end if 8: return w\u2032\nwhich can perform corruptions after data and response variables have been generated, and choose to corrupt precisely that set of points where the SSC and SSS parameters are the worst."}, {"heading": "3 Torrent: Thresholding Operator-based Robust Regression Method", "text": "We now present Torrent, a Thresholding Operator-based Robust RegrEssioN meThod for performing robust regression at scale. Key to our algorithms is the Hard Thresholding Operator which we define below.\nDefinition 2 (Hard Thresholding Operator). For any vector v \u2208 Rn, let \u03c3v \u2208 Sn be the permutation that orders elements of v in ascending order of their magnitudes i.e. \u2223\u2223v\u03c3v(1)\u2223\u2223 \u2264 \u2223\u2223v\u03c3v(2)\u2223\u2223 \u2264 . . . \u2264 \u2223\u2223v\u03c3v(n)\u2223\u2223. Then for any k \u2264 n, we define the hard thresholding operator as\nHT(v; k) = { i \u2208 [n] : \u03c3\u22121v (i) \u2264 k } Using this operator, we present our algorithm Torrent (Algorithm 1) for robust regression. Torrent follows a most natural iterative strategy of, alternately, estimating an active set of points which have the least residual error on the current regressor, and then updating the regressor to provide a better fit on this active set. We offer three variants of our algorithm, based on how aggressively the algorithm tries to fit the regressor to the current active set.\nWe first propose a fully corrective algorithm Torrent-FC (Algorithm 2) that performs a fully corrective least squares regression step in an effort to minimize the regression error on the active set. This algorithm makes significant progress in each step, but at a cost of more expensive updates. To address this, we then propose a milder, gradient descent-based variant Torrent-GD (Algorithm 3) that performs a much cheaper update of taking a single step in the direction of the gradient of the objective function on the active set. This reduces the regression error on the active set but does not minimize it. This turns out to be beneficial in situations where dense noise is present along with sparse corruptions since it prevents the algorithm from overfitting to the current active set.\nBoth the algorithms proposed above have their pros and cons \u2013 the FC algorithm provides significant improvements with each step, but is expensive to execute whereas the GD variant, although efficient in executing each step, offers slower progress. To get the best of both these algorithms, we propose a third, hybrid variant Torrent-HYB (Algorithm 4) that adaptively selects either the FC or the GD update depending on whether the active set is stable across iterations or not.\nIn the next section we show that this hard thresholding-based strategy offers a linear convergence rate for the algorithm in all its three variations. We shall also demonstrate the applicability of this technique to high dimensional sparse recovery settings in a subsequent section."}, {"heading": "4 Convergence Guarantees", "text": "For the sake of ease of exposition, we will first present our convergence analyses for cases where dense noise is not present i.e. y = X>w\u2217 + b and will handle cases with dense noise and sparse corruptions later. We first analyze the fully corrective Torrent-FC algorithm. The convergence proof in this case relies on the optimality of the two steps carried out by the algorithm, the fully corrective step that selects the best regressor on the active set, and the hard thresholding step that discovers a new active set by selecting points with the least residual error on the current regressor.\nTheorem 3. Let X = [x1, . . . ,xn] \u2208 Rp\u00d7n be the given data matrix and y = XTw\u2217 + b be the corrupted output with \u2016b\u20160 \u2264 \u03b1 \u00b7 n. Let Algorithm 2 be executed on this data with the thresholding parameter set to \u03b2 \u2265 \u03b1. Let \u03a30 be an invertible matrix such that X\u0303 = \u03a3\u22121/20 X satisfies the SSC and SSS properties at level \u03b3 with constants \u03bb\u03b3 and \u039b\u03b3 respectively (see Definition 1). If the data satisfies (1+ \u221a\n2)\u039b\u03b2 \u03bb1\u2212\u03b2 < 1, then after t = O ( log (\n1\u221a n\n\u2016b\u20162 )) iterations, Algorithm 2 obtains an -accurate solution wt i.e. \u2016wt \u2212w\u2217\u20162 \u2264 .\nProof (Sketch). Let rt = y \u2212 X>wt be the vector of residuals at time t and Ct = XStX>St . Also let S\u2217 = supp(b) be the set of uncorrupted points. The fully corrective step ensures that\nwt+1 = C\u22121t XStySt = C \u22121 t XSt ( X>Stw \u2217 + bSt ) = w\u2217 + C\u22121t XStbSt ,\nwhereas the hard thresholding step ensures that \u2225\u2225\u2225rt+1St+1\u2225\u2225\u222522 \u2264 \u2225\u2225rt+1S\u2217 \u2225\u222522. Combining the two gives us\u2225\u2225bSt+1\u2225\u222522 \u2264 \u2225\u2225\u2225X>S\u2217\\St+1C\u22121t XStbSt\u2225\u2225\u222522 + 2 \u00b7 b>St+1X>St+1C\u22121t XStbSt\n\u03b61 = \u2225\u2225\u2225\u2225X\u0303>S\u2217\\St+1 (X\u0303StX\u0303TSt)\u22121 X\u0303StbSt\u2225\u2225\u2225\u22252 2 + 2 \u00b7 b>St+1X\u0303 > St+1 ( X\u0303StX\u0303 T St )\u22121 X\u0303StbSt\n\u03b62 \u2264 \u039b2\u03b2 \u03bb21\u2212\u03b2 \u00b7 \u2016bSt\u2016 2 2 + 2 \u00b7 \u039b\u03b2 \u03bb1\u2212\u03b2\n\u00b7 \u2016bSt\u20162 \u2225\u2225bSt+1\u2225\u22252 ,\nwhere \u03b61 follows from setting X\u0303 = \u03a3 \u22121/2 0 X and X > S C \u22121 t XS\u2032 = X\u0303 > S (X\u0303StX\u0303 > St )\u22121X\u0303S\u2032 and \u03b62 follows from the SSC and SSS properties, \u2016bSt\u20160 \u2264 \u2016b\u20160 \u2264 \u03b2 \u00b7 n and |S\u2217\\St+1| \u2264 \u03b2 \u00b7 n. Solving the quadratic equation and performing other manipulations gives us the claimed result.\nTheorem 3 relies on a deterministic (fixed design) assumption, specifically (1+ \u221a\n2)\u039b\u03b2 \u03bb1\u2212\u03b2 < 1 in order to\nguarantee convergence. We can show that a large class of random designs, including Gaussian and subGaussian designs actually satisfy this requirement. That is to say, data generated from these distributions satisfy the SSC and SSS conditions such that (1+ \u221a\n2)\u039b\u03b2 \u03bb1\u2212\u03b2 < 1 with high probability. Theorem 4 explicates this\nfor the class of Gaussian designs.\nTheorem 4. Let X = [x1, . . . ,xn] \u2208 Rp\u00d7n be the given data matrix with each xi \u223c N (0,\u03a3). Let y = X>w\u2217 + b and \u2016b\u20160 \u2264 \u03b1 \u00b7 n. Also, let \u03b1 \u2264 \u03b2 < 165 and n \u2265 \u2126 ( p+ log 1\u03b4 ) . Then, with probability at least 1 \u2212 \u03b4, the data satisfies (1+ \u221a\n2)\u039b\u03b2 \u03bb1\u2212\u03b2 < 910 . More specifically, after T \u2265 10 log ( 1\u221a n \u2016b\u20162 ) iterations of\nAlgorithm 1 with the thresholding parameter set to \u03b2, we have \u2225\u2225wT \u2212w\u2217\u2225\u2225 \u2264 .\nRemark 2. Note that Theorem 4 provides rates that are independent of the condition number \u03bbmax(\u03a3)\u03bbmin(\u03a3) of the distribution. We also note that results similar to Theorem 4 can be proven for the larger class of sub-Gaussian distributions. We refer the reader to Section G for the same.\nRemark 3. We remind the reader that our analyses can readily accommodate dense noise in addition to sparse unbounded corruptions. We direct the reader to Appendix A which presents convergence proofs for our algorithms in these settings.\nRemark 4. We would like to point out that the design requirements made by our analyses are very mild when compared to existing literature. Indeed, the work of [15] assumes the Bouquet Model where distributions are restricted to be isotropic Gaussians whereas the work of [9] assumes a more stringent model of suborthonormal matrices, something that even Gaussian designs do not satisfy. Our analyses, on the other hand, hold for the general class of sub-Gaussian distributions.\nWe now analyze the Torrent-GD algorithm which performs cheaper, gradient-style updates on the active set. We will show that this method nevertheless enjoys a linear rate of convergence.\nTheorem 5. Let the data settings be as stated in Theorem 3 and let Algorithm 3 be executed on this data with the thresholding parameter set to \u03b2 \u2265 \u03b1 and the step length set to \u03b7 = 1\u039b1\u2212\u03b2 . If the data satisfies\nmax { \u03b7 \u221a \u039b\u03b2 , 1\u2212 \u03b7\u03bb1\u2212\u03b2 } \u2264 14 , then after t = O ( log ( \u2016b\u20162\u221a n 1 )) iterations, Algorithm 1 obtains an -accurate solution wt i.e. \u2016wt \u2212w\u2217\u20162 \u2264 .\nSimilar to Torrent-FC, the assumptions made by the Torrent-GD algorithm are also satisfied by the class of sub-Gaussian distributions. The proof of Theorem 5, given in Appendix D, details these arguments. Given the convergence analyses for Torrent-FC and GD, we now move on to provide a convergence analysis for the hybrid Torrent-HYB algorithm which interleaves FC and GD steps. Since the exact interleaving adopted by the algorithm depends on the data, and not known in advance, this poses a problem. We address this problem by giving below a uniform convergence guarantee, one that applies to every interleaving of the FC and GD update steps.\nTheorem 6. Suppose Algorithm 4 is executed on data that allows Algorithms 2 and 3 a convergence rate of \u03b7FC and \u03b7GD respectively. Suppose we have 2 \u00b7 \u03b7FC \u00b7 \u03b7GD < 1. Then for any interleavings of the FC and GD steps that the policy may enforce, after t = O ( log (\n1\u221a n\n\u2016b\u20162 )) iterations, Algorithm 4 ensures an -optimal\nsolution i.e. \u2016wt \u2212w\u2217\u2016 \u2264 .\nWe point out to the reader that the assumption made by Theorem 6 i.e. 2 \u00b7 \u03b7FC \u00b7 \u03b7GD < 1 is readily satisfied by random sub-Gaussian designs, albeit at the cost of reducing the noise tolerance limit. As we shall see, Torrent-HYB offers attractive convergence properties, merging the fast convergence rates of the FC step, as well as the speed and protection against overfitting provided by the GD step."}, {"heading": "5 High-dimensional Robust Regression", "text": "In this section, we extend our approach to the robust high-dimensional sparse recovery setting. As before, we assume that the response vector y is obtained as: y = X>w\u2217 + b, where \u2016b\u20160 \u2264 \u03b1 \u00b7 n. However, this time, we also assume that w\u2217 is s\u2217-sparse i.e. \u2016w\u2217\u20160 \u2264 s\u2217.\nAs before, we shall neglect white/dense noise for the sake of simplicity. We reiterate that it is not possible to use existing results from sparse recovery (such as [3, 6]) directly to solve this problem.\nOur objective would be to recover a sparse model w\u0302 so that \u2016w\u0302 \u2212w\u2217\u20162 \u2264 . The challenge here is to forgo a sample complexity of n & p and instead, perform recovery with n \u223c s\u2217 log p samples alone. For this setting, we modify the FC update step of Torrent-FC method to the following:\nwt+1 \u2190 inf \u2016w\u20160\u2264s \u2211 i\u2208St (yi \u2212 \u3008w,xi\u3009)2 , (3)\nfor some target sparsity level s p. We refer to this modified algorithm as Torrent-HD. Assuming X satisfies the RSC/RSS properties (defined below), (3) can be solved efficiently using results from sparse recovery (for example the IHT algorithm [3, 5] analyzed in [6]).\nDefinition 7 (RSC and RSS Properties). A matrix X \u2208 Rp\u00d7n will be said to satisfy the Restricted Strong Convexity Property (resp. Restricted Strong Smoothness Property) at level s = s1 +s2 with strong convexity constant \u03b1s1+s2 (resp. strong smoothness constant Ls1+s2) if the following holds for all \u2016w1\u20160 \u2264 s1 and \u2016w2\u20160 \u2264 s2:\n\u03b1s \u2016w1 \u2212w2\u201622 \u2264 \u2225\u2225X>(w1 \u2212w2)\u2225\u222522 \u2264 Ls \u2016w1 \u2212w2\u201622\nFor our results, we shall require the subset versions of both these properties.\nDefinition 8 (SRSC and SRSS Properties). A matrix X \u2208 Rp\u00d7n will be said to satisfy the Subset Restricted Strong Convexity (resp. Subset Restricted Strong Smoothness) Property at level (\u03b3, s) with strong convexity constant \u03b1(\u03b3,s) (resp. strong smoothness constant L(\u03b3,s)) if for all subsets S \u2208 S\u03b3 , the matrix XS satisfies the RSC (resp. RSS) property at level s with constant \u03b1s (resp. Ls).\nWe now state the convergence result for the Torrent-HD algorithm.\nTheorem 9. Let X \u2208 Rp\u00d7n be the given data matrix and y = XTw\u2217 + b be the corrupted output with \u2016w\u2217\u20160 \u2264 s\u2217 and \u2016b\u20160 \u2264 \u03b1 \u00b7n. Let \u03a30 be an invertible matrix such that \u03a3 \u22121/2 0 X satisfies the SRSC and SRSS properties at level (\u03b3, 2s + s\u2217) with constants \u03b1(\u03b3,2s+s\u2217) and L(\u03b3,2s+s\u2217) respectively (see Definition 8). Let Algorithm 2 be executed on this data with the Torrent-HD update, thresholding parameter set to \u03b2 \u2265 \u03b1, and s \u2265 32 ( L(1\u2212\u03b2,2s+s\u2217) \u03b1(1\u2212\u03b2,2s+s\u2217) ) .\nIf X also satisfies 4L(\u03b2,s+s\u2217) \u03b1(1\u2212\u03b2,s+s\u2217)\n< 1, then after t = O ( log (\n1\u221a n\n\u2016b\u20162 )) iterations, Algorithm 2 obtains an\n-accurate solution wt i.e. \u2016wt \u2212w\u2217\u20162 \u2264 . In particular, if X is sampled from a Gaussian distribution N (0,\u03a3) and n \u2265 \u2126 ( s\u2217 \u00b7 \u03bbmax(\u03a3)\u03bbmin(\u03a3) log p ) , then\nfor all values of \u03b1 \u2264 \u03b2 < 165 , we can guarantee \u2016w t \u2212w\u2217\u20162 \u2264 after t = O\n( log (\n1\u221a n\n\u2016b\u20162 )) iterations of\nthe algorithm (w.p. \u2265 1\u2212 1/n10).\nRemark 5. The sample complexity required by Theorem 9 is identical to the one required by analyses for high dimensional sparse recovery [6], save constants. Also note that Torrent-HD can tolerate the same corruption index as Torrent-FC."}, {"heading": "6 Experiments", "text": "Several numerical simulations were carried out on linear regression problems in low-dimensional, as well as sparse high-dimensional settings. The experiments show that Torrent not only offers statistically better recovery properties as compared to L1-style approaches, but that it can be more than an order of magnitude faster as well.\nData: For the low dimensional setting, the regressor w\u2217 \u2208 Rp was chosen to be a random unit norm vector. Data was sampled as xi \u223c N (0, Ip) and response variables were generated as y\u2217i = \u3008w\u2217,xi\u3009. The set of corrupted points S\u2217 was selected as a uniformly random (\u03b1n)-sized subset of [n] and the corruptions were set to bi \u223c U (\u22125 \u2016y\u2217\u2016\u221e , 5 \u2016y\u2217\u2016\u221e) for i \u2208 S\u2217. The corrupted responses were then generated as yi = y\u2217i + bi + \u03b5i where \u03b5i \u223c N (0, \u03c32). For the sparse high-dimensional setting, supp(w\u2217) was selected to be a random s\u2217-sized subset of [p]. Phase-transition diagrams (Figure 1) were generated by repeating each experiment 100 times. For all other plots, each experiment was run over 20 random instances of the data and the plots were drawn to depict the mean results.\nAlgorithms: We compared various variants of our algorithm Torrent to the regularized L1 algorithm for robust regression [15, 9]. Note that the L1 problem can be written as minz \u2016z\u20161 s.t.Az = y, where\nA = [ X> 1\u03bbIm\u00d7m ] and z\u2217 = [w\u2217> \u03bbb>]>. We used the Dual Augmented Lagrange Multiplier (DALM) L1 solver implemented by [17] to solve the L1 problem. We ran a fine tuned grid search over the \u03bb parameter for the L1 solver and quoted the best results obtained from the search. In the low-dimensional setting, we compared the recovery properties of Torrent-FC (Algorithm 2) and Torrent-HYB (Algorithm 4) with the DALM-L1 solver, while for the high-dimensional case, we compared Torrent-HD against the DALM-L1 solver. Both the L1 solver, as well as our methods, were implemented in Matlab and were run on a single core 2.4GHz machine with 8 GB RAM.\nChoice of L1-solver: An extensive comparative study of various L1 minimization algorithms was performed by [17] who showed that the DALM and Homotopy solvers outperform other counterparts both in terms of recovery properties, and timings. We extended their study to our observation model and found the DALM solver to be significantly better than the other L1 solvers; see Figure 3 in the appendix. We also observed, similar to [17], that the Approximate Message Passing (AMP) solver diverges on our problem as the input matrix to the L1 solver is a non-Gaussian matrix A = [X T 1 \u03bbI].\nEvaluation Metric: We measure the performance of various algorithms using the standard L2 error: rw\u0302 = \u2016w\u0302 \u2212w\u2217\u20162. For the phase-transition plots (Figure 1), we deemed an algorithm successful on an instance if it obtained a model w\u0302 with error rw\u0302 < 10\n\u22124 \u00b7 \u2016w\u2217\u20162. We also measured the CPU time required by each of the methods, so as to compare their scalability."}, {"heading": "6.1 Low Dimensional Results", "text": "Recovery Property: The phase-transition plots presented in Figure 1 represent our recovery experiments in graphical form. Both the fully-corrective and hybrid variants of Torrent show better recovery properties than the L1-minimization approach, indicated by the number of runs in which the algorithm was able to correctly recover w\u2217 out of a 100 runs. Figure 2 shows the variation in recovery error as a function of \u03b1 in the presence of white noise and exhibits the superiority of Torrent-FC and Torrent-HYB over L1-DALM. Here again, Torrent-FC and Torrent-HYB achieve significantly lesser recovery error than L1-DALM for all \u03b1 <= 0.5. Figure 3 in the appendix show that the variations of \u2016w\u0302 \u2212w\u2217\u20162 with varying p, \u03c3 and n follow a similar trend with Torrent having significantly lower recovery error in comparison to the L1 approach.\nFigure 1(d) brings out an interesting trend in the recovery property of Torrent. As we increase the magnitude of corruption from U (\u2212\u2016y\u2217\u2016\u221e , \u2016y\u2217\u2016\u221e) to U (\u221220 \u2016y\u2217\u2016\u221e , 20 \u2016y\u2217\u2016\u221e), the recovery error for Torrent-HYB and Torrent-FC decreases as expected since it becomes easier to identify the grossly corrupted points. However the L1-solver was unable to exploit this observation and in fact exhibited an increase in recovery error.\nRun Time: In order to ascertain the recovery guarantees for Torrent on ill-conditioned problems, we performed an experiment where data was sampled as xi \u223c N (0,\u03a3) where diag(\u03a3) \u223c U(0, 5). Figure 2 plots\nthe recovery error as a function of time. Torrent-HYB was able to correctly recover w\u2217 about 50\u00d7 faster than L1-DALM which spent a considerable amount of time pre-processing the data matrix X. Even after allowing the L1 algorithm to run for 500 iterations, it was unable to reach the desired residual error of 10\n\u22124. Figure 2 also shows that our Torrent-HYB algorithm is able to converge to the optimal solution much faster than Torrent-FC or Torrent-GD. This is because Torrent-FC solves a least square problem at each step and thus, even though it requires significantly fewer iterations to converge, each iteration in itself is very expensive. While each iteration of Torrent-GD is cheap, it is still limited by the slow O ( (1\u2212 1\u03ba ) t ) convergence rate of the gradient descent algorithm, where \u03ba is the condition number of the covariance matrix. Torrent-HYB, on the other hand, is able to combine the strengths of both the methods to achieve faster convergence."}, {"heading": "6.2 High Dimensional Results", "text": "Recovery Property: Figure 2 shows the variation in recovery error in the high-dimensional setting as the number of corrupted points was varied. For these experiments, n was set to 5s\u2217 log(p) and the fraction of corrupted points \u03b1 was varied from 0.1 to 0.7. While L1-DALM fails to recover w\n\u2217 for \u03b1 > 0.5, Torrent-HD offers perfect recovery even for \u03b1 values upto 0.7.\nRun Time: Figure 2 shows the variation in recovery error as a function of run time in this setting. L1-DALM was found to be an order of magnitude slower than Torrent-HD, making it infeasible for sparse high-dimensional settings. One key reason for this is that the L1-DALM solver is significantly slower in identifying the set of clean points. For instance, whereas Torrent-HD was able to identify the clean set of points in only 5 iterations, it took L1 around 250 iterations to do the same."}, {"heading": "A Convergence Guarantees with Dense Noise and Sparse Corrup-", "text": "tions\nWe will now present recovery guarantees for the Torrent-FC algorithm when both, dense noise, as well as sparse adversarial corruptions are present. Extensions for Torrent-GD and Torrent-HYB will follow similarly.\nTheorem 10. Let X = [x1, . . . ,xn] \u2208 Rp\u00d7n be the given data matrix and y = XTw\u2217+b+\u03b5 be the corrupted output with sparse corruptions \u2016b\u20160 \u2264 \u03b1 \u00b7n as well as dense bounded noise \u03b5. Let Algorithm 2 be executed on this data with the thresholding parameter set to \u03b2 \u2265 \u03b1. Let \u03a30 be an invertible matrix such that X\u0303 = \u03a3\u22121/20 X satisfies the SSC and SSS properties at level \u03b3 with constants \u03bb\u03b3 and \u039b\u03b3 respectively (see Definition 1). If the data satisfies 4 \u221a\n\u039b\u03b2\u221a \u03bb1\u2212\u03b2\n< 1, then after t = O ( log (\n1\u221a n\n\u2016b\u20162 )) iterations, Algorithm 2 obtains an -accurate\nsolution wt i.e. \u2016wt \u2212w\u2217\u20162 \u2264 + C \u2016\u03b5\u20162\u221a n for some constant C > 0.\nProof. We being by observing that the optimality of the model wt+1 on the active set St ensures\u2225\u2225ySt \u2212X>Stwt+1\u2225\u22252 = \u2225\u2225X>St(w\u2217 \u2212wt+1) + \u03b5St + bSt\u2225\u22252 \u2264 \u2225\u2225yt \u2212X>Stw\u2217\u2225\u22252 = \u2016\u03b5St + bSt\u20162 , which, upon the application of the triangle inequality, gives us\u2225\u2225X>St(w\u2217 \u2212wt+1)\u2225\u22252 \u2264 2 \u2016\u03b5St + bSt\u20162 . Since\n\u2225\u2225X>St(w\u2217 \u2212wt+1)\u2225\u22252 \u2265\u221a\u03bb1\u2212\u03b2 \u2225\u2225w\u2217 \u2212wt+1\u2225\u22252, we get\u2225\u2225w\u2217 \u2212wt+1\u2225\u2225 2 \u2264 2\u221a\n\u03bb1\u2212\u03b2 \u2016\u03b5St + bSt\u20162 \u2264 2\u221a \u03bb1\u2212\u03b2 (\u2016\u03b5\u20162 + \u2016bSt\u20162) .\nThe hard thresholding step, on the other hand, guarantees that\u2225\u2225\u2225X>St+1(w\u2217 \u2212wt+1) + \u03b5St+1 + bSt+1\u2225\u2225\u22252 2 = \u2225\u2225\u2225ySt+1 \u2212X>St+1wt+1\u2225\u2225\u22252 2\n\u2264 \u2225\u2225yS\u2217 \u2212X>S\u2217wt+1\u2225\u22252\n= \u2225\u2225X>S\u2217(w\u2217 \u2212wt+1) + \u03b5S\u2217\u2225\u222522 .\nAs before, let CRt+1 = St+1\\S\u2217 and MDt+1 = S\u2217\\St+1. Then we have\u2225\u2225\u2225X>CRt+1(w\u2217 \u2212wt+1) + \u03b5CRt+1 + bCRt+1\u2225\u2225\u2225 2 \u2264 \u2225\u2225\u2225X>MDt+1(w\u2217 \u2212wt+1) + \u03b5MDt+1\u2225\u2225\u2225 2 .\nAn application of the triangle inequality and the fact that \u2225\u2225bCRt+1\u2225\u22252 = \u2225\u2225bSt+1\u2225\u2225 gives us\u2225\u2225bSt+1\u2225\u22252 \u2264 \u2225\u2225\u2225X>MDt+1(w\u2217 \u2212wt+1)\u2225\u2225\u22252 + \u2225\u2225\u2225X>CRt+1(w\u2217 \u2212wt+1)\u2225\u2225\u22252 + \u2225\u2225\u03b5CRt+1\u2225\u22252 + \u2225\u2225\u03b5MDt+1\u2225\u22252\n\u2264 2 \u221a \u039b\u03b2 \u2225\u2225w\u2217 \u2212wt+1\u2225\u2225 2 + \u221a 2 \u2016\u03b5\u20162 ,\n= 4 \u221a\n\u039b\u03b2\u221a \u03bb1\u2212\u03b2\n\u2016bSt\u20162 + ( 4 \u221a\n\u039b\u03b2\u221a \u03bb1\u2212\u03b2 + \u221a 2) \u2016\u03b5\u20162\n\u2264 \u03b7 \u00b7 \u2016bSt\u20162 + (1 + \u221a 2) \u2016\u03b5\u20162 ,\nwhere the second step uses the fact that max {|CRt+1| , |MDt+1|} \u2264 \u03b2 \u00b7n and the Cauchy-Schwartz inequality, and the last step uses the fact that for sufficiently small \u03b2, we have \u03b7 := 4 \u221a\n\u039b\u03b2\u221a \u03bb1\u2212\u03b2 . Using the inequality for\u2225\u2225wt+1 \u2212w\u2217\u2225\u2225 2\nagain gives us \u2225\u2225w\u2217 \u2212wt+1\u2225\u2225 2 \u2264 2\u221a\n\u03bb1\u2212\u03b2 (\u2016\u03b5\u20162 + \u2016bSt\u20162)\n\u2264 4 + 2 \u221a 2\u221a \u03bb1\u2212\u03b2 \u2016\u03b5\u20162 + 2 \u00b7 \u03b7t\u221a \u03bb1\u2212\u03b2 \u2016b\u20162\nFor large enough n we have \u221a \u03bb1\u2212\u03b2 \u2265 O ( \u221a n), which completes the proof.\nNotice that for random Gaussian noise, this result gives the following convergence guarantee.\nCorollary 11. Let the date be generated as before with random Gaussian dense noise i.e. y = XTw\u2217+b+\u03b5 with \u2016b\u20160 \u2264 \u03b1\u00b7n and \u03b5 \u223c N (0, \u03c32\u00b7I). Let Algorithm 2 be executed on this data with the thresholding parameter set to \u03b2 \u2265 \u03b1. Let \u03a30 be an invertible matrix such that X\u0303 = \u03a3\u22121/20 X satisfies the SSC and SSS properties at\nlevel \u03b3 with constants \u03bb\u03b3 and \u039b\u03b3 respectively (see Definition 1). If the data satisfies 4 \u221a\n\u039b\u03b2\u221a \u03bb1\u2212\u03b2 < 1, then after t = O ( log (\n1\u221a n\n\u2016b\u20162 )) iterations, Algorithm 2 obtains an -accurate solution wt i.e. \u2016wt \u2212w\u2217\u20162 \u2264 +2\u03c3C,\nwhere C > 0 is the constant in Theorem 10.\nProof. Using tail bounds on Chi-squared distributions [7], we get, with probability at least 1\u2212 \u03b4,\n\u2016\u03b5\u201622 \u2264 \u03c3 2\n( n+ 2 \u221a n log 1\n\u03b4 + 2 log\n1\n\u03b4\n) .\nThus, for n > 4 log 1\u03b4 , we have \u2016\u03b5\u2016 2 2 \u2264 2\u03c3n which proves the result. Remark 6. We note that the design assumptions made by Theorem 10 (i..e 4 \u221a\n\u039b\u03b2\u221a \u03bb1\u2212\u03b2 < 1) are similar to\nthose made by Theorem 3 and would be satisfied with high probability by data sampled from sub-Gaussian distributions (see Appendix G for details)."}, {"heading": "B Proof of Theorem 3", "text": "Theorem 3. Let X = [x1, . . . ,xn] \u2208 Rp\u00d7n be the given data matrix and y = XTw\u2217 + b be the corrupted output with \u2016b\u20160 \u2264 \u03b1 \u00b7 n. Let Algorithm 2 be executed on this data with the thresholding parameter set to \u03b2 \u2265 \u03b1. Let \u03a30 be an invertible matrix such that X\u0303 = \u03a3\u22121/20 X satisfies the SSC and SSS properties at level \u03b3 with constants \u03bb\u03b3 and \u039b\u03b3 respectively (see Definition 1). If the data satisfies (1+ \u221a\n2)\u039b\u03b2 \u03bb1\u2212\u03b2 < 1, then after t = O ( log (\n1\u221a n\n\u2016b\u20162 )) iterations, Algorithm 2 obtains an -accurate solution wt i.e. \u2016wt \u2212w\u2217\u20162 \u2264 .\nProof. Let rt = y \u2212X>wt be the vector of residuals at time t and Ct = XStX>St . Since \u03bb\u03b1 > 0 (something which we shall establish later), we get\nwt+1 = C\u22121t XStySt = C \u22121 t XSt ( X>Stw \u2217 + bSt ) = w\u2217 + C\u22121t XStbSt .\nThus, for any set S \u2282 [n], we have\nrt+1S = yS \u2212X > S wt+1 = bS \u2212X>S C\u22121t XStbSt\nThis, gives us\u2225\u2225bSt+1\u2225\u222522 = \u2225\u2225\u2225bSt+1 \u2212X>St+1C\u22121t XStbSt\u2225\u2225\u222522 \u2212 \u2225\u2225\u2225X>St+1C\u22121t XStbSt\u2225\u2225\u222522 + 2 \u00b7 b>St+1X>St+1C\u22121t XStbSt \u03b61 \u2264 \u2225\u2225bS\u2217 \u2212X>S\u2217C\u22121t XStbSt\u2225\u222522 \u2212 \u2225\u2225\u2225X>St+1C\u22121t XStbSt\u2225\u2225\u222522 + 2 \u00b7 b>St+1X>St+1C\u22121t XStbSt\n\u03b62 = \u2225\u2225X>S\u2217C\u22121t XStbSt\u2225\u222522 \u2212 \u2225\u2225\u2225X>St+1C\u22121t XStbSt\u2225\u2225\u222522 + 2 \u00b7 b>St+1X>St+1C\u22121t XStbSt\n\u2264 \u2225\u2225\u2225X>S\u2217\\St+1C\u22121t XStbSt\u2225\u2225\u222522 + 2 \u00b7 b>St+1X>St+1C\u22121t XStbSt\n\u03b63 = \u2225\u2225\u2225\u2225X\u0303>S\u2217\\St+1 (X\u0303StX\u0303TSt)\u22121 X\u0303StbSt\u2225\u2225\u2225\u22252 2 + 2 \u00b7 b>St+1X\u0303 > St+1 ( X\u0303StX\u0303 T St )\u22121 X\u0303StbSt\n\u03b64 \u2264 \u039b2\u03b2 \u03bb21\u2212\u03b2 \u00b7 \u2016bSt\u2016 2 2 + 2 \u00b7 \u039b\u03b2 \u03bb1\u2212\u03b2\n\u00b7 \u2016bSt\u20162 \u2225\u2225bSt+1\u2225\u22252 ,\nwhere \u03b61 follows since the hard thresholding step ensures \u2225\u2225\u2225rt+1St+1\u2225\u2225\u222522 \u2264 \u2225\u2225rt+1S\u2217 \u2225\u222522 (see Claim 19 and use the fact that \u03b2 \u2265 \u03b1), \u03b62 notices the fact that bS\u2217 = 0. \u03b63 follows from setting X\u0303 = \u03a3 \u22121/2 0 X and X > S C \u22121 t XS\u2032 = X\u0303>S (X\u0303StX\u0303 > St\n)\u22121X\u0303S\u2032 . \u03b64 follows from the definition of SSC and SSS properties, \u2016bSt\u20160 \u2264 \u2016b\u20160 \u2264 \u03b2 \u00b7 n and |S\u2217\\St+1| \u2264 \u03b2 \u00b7 n. Solving the quadratic equation gives us\u2225\u2225bSt+1\u2225\u22252 \u2264 (1 +\u221a2) \u00b7 \u039b\u03b2\u03bb1\u2212\u03b2 \u00b7 \u2016bSt\u20162 . (4) Let \u03b7 := (1+ \u221a\n2)\u039b\u03b2 \u03bb1\u2212\u03b2 denote the convergence rate in (4). We shall show below that for a large family of random designs, we have \u03b7 < 1 if n \u2265 \u2126 ( p+ log 1\u03b4 ) . We now recall from our earlier discussion that wt+1 = w\u2217 + C\u22121t XStbSt which gives us\u2225\u2225wt+1 \u2212w\u2217\u2225\u2225 2 = \u2225\u2225C\u22121t XStbSt\u2225\u22252 \u2264 \u221a \u039b\u03b2\n\u03bb1\u2212\u03b2 \u00b7 \u2016bSt\u20162 \u2264 \u03b7\nt \u00b7 \u221a \u039b\u03b2\n\u03bb1\u2212\u03b2 \u2016b\u20162 \u2264 ,\nfor t \u2265 log 1 \u03b7\n(\u221a \u039b\u03b2\n\u03bb1\u2212\u03b2 \u00b7 \u2016b\u20162\n) . Noting that \u221a \u039b\u03b2\n\u03bb1\u2212\u03b2 \u2264 O ( 1\u221a n ) establishes the convergence result."}, {"heading": "C Proof of Theorem 4", "text": "Theorem 4. Let X = [x1, . . . ,xn] \u2208 Rp\u00d7n be the given data matrix with each xi \u223c N (0,\u03a3). Let y = X>w\u2217 + b and \u2016b\u20160 \u2264 \u03b1 \u00b7 n. Also, let \u03b1 \u2264 \u03b2 < 165 and n \u2265 \u2126 ( p+ log 1\u03b4 ) . Then, with probability at least 1 \u2212 \u03b4, the data satisfies (1+ \u221a\n2)\u039b\u03b2 \u03bb1\u2212\u03b2 < 910 . More specifically, after T \u2265 10 log ( 1\u221a n \u2016b\u20162 ) iterations of\nAlgorithm 1 with the thresholding parameter set to \u03b2, we have \u2225\u2225wT \u2212w\u2217\u2225\u2225 \u2264 .\nProof. We note that whenever x \u223c N (0,\u03a3) then \u03a3\u22121/2x \u223c N (0, I). Thus, Theorem 15 assures us that with probability at least 1\u2212 \u03b4, the data matrix X\u0303 = \u03a3\u22121/2X satisfies the SSC and SSS properties with the following constants\n\u039b\u03b2 \u2264 \u03b2n ( 1 + 3e \u221a 6 log e\n\u03b2\n) +O (\u221a np+ n log 1\n\u03b4\n)\n\u03bb1\u2212\u03b2 \u2265 n\u2212 \u03b2n ( 1 + 3e \u221a 6 log e\n\u03b2\n) \u2212 \u2126 (\u221a np+ n log 1\n\u03b4\n)\nThus, the convergence given be Algorithm 1, when invoked with \u03a30 = \u03a3, relies on the quantity \u03b7 = (1+ \u221a\n2)\u039b\u03b2 \u03bb1\u2212\u03b2\nbeing less than unity. This translates to the requirement (1 + \u221a\n2)\u039b\u03b2 \u2264 \u03bb1\u2212\u03b2 . Using the above bounds translates that requirement to\n(2 + \u221a 2)\u03b2 ( 1 + 3e \u221a 6 log e\n\u03b2 ) \ufe38 \ufe37\ufe37 \ufe38\n(A)\n+O\n(\u221a p\nn +\n1 n log 1 \u03b4 ) \ufe38 \ufe37\ufe37 \ufe38\n(B)\n< 1.\nFor n = \u2126 ( p+ log 1\u03b4 ) , the second quantity (B) can be made as small a constant as necessary. Tackling the first quantity (A) turns out to be more challenging. However, we can show that for all \u03b2 < 1190 , we get \u03b7 = (1+ \u221a\n2)\u039b\u03b2 \u03bb1\u2212\u03b2 < 910 which establishes the claimed result. Thus, Algorithm 1 can tolerate a corruption index\nof upto \u03b1 \u2264 1190 . However, we note that using a more finely tuned setting of the constant in the proof of Theorem 15 and a more careful proof using tight tail inequalities for chi-squared distributions [7], we can achieve a better corruption level tolerance of \u03b1 < 165 ."}, {"heading": "D Proof of Theorem 5", "text": "Theorem 5. Let X = [x1, . . . ,xn] \u2208 Rp\u00d7n be the given data matrix and y = XTw\u2217 + b be the corrupted output with \u2016b\u20160 \u2264 \u03b1 \u00b7 n. Let X satisfy the SSC and SSS properties at level \u03b3 with constants \u03bb\u03b3 and \u039b\u03b3 respectively (see Definition 1). Let Algorithm 1 be executed on this data with the GD update (Algorithm 3) with the thresholding parameter set to \u03b2 \u2265 \u03b1 and the step length set to \u03b7 = 1\u039b1\u2212\u03b2 . If the data satisfies\nmax { \u03b7 \u221a \u039b\u03b2 , 1\u2212 \u03b7\u03bb1\u2212\u03b2 } \u2264 14 , then after t = O ( log ( \u2016b\u20162\u221a n 1 )) iterations, Algorithm 1 obtains an -accurate solution wt i.e. \u2016wt \u2212w\u2217\u20162 \u2264 .\nProof. Let rt = y \u2212X>wt be the vector of residuals at time t and Ct = XStX>St . We have\nwt+1 = wt + \u03b7 \u00b7XStrtSt = w t + \u03b7 \u00b7XSt(ySt \u2212X>Stw t)\nThe thresholding step ensures that \u2225\u2225\u2225rt+1St+1\u2225\u2225\u222522 \u2264 \u2225\u2225rt+1S\u2217 \u2225\u222522 (see Claim 19 and use \u03b2 \u2265 \u03b1) which implies\u2225\u2225\u2225rt+1CRt+1\u2225\u2225\u222522 \u2264 \u2225\u2225\u2225rt+1MDt+1\u2225\u2225\u222522 ,\nwhere CRt+1 = St+1\\S\u2217 are the corrupted recoveries and MDt+1 = S\u2217\\St+1 are the clean points missed out from detection. Note that |CRt+1| \u2264 \u03b1 \u00b7 n and |MDt+1| \u2264 \u03b2 \u00b7 n. Since bS\u2217 = 0 and MDt+1 \u2286 S\u2217, we get\u2225\u2225\u2225bCRt+1 +X>CRt+1(w\u2217 \u2212wt+1)\u2225\u2225\u2225 2 \u2264 \u2225\u2225\u2225X>MDt+1(w\u2217 \u2212wt+1)\u2225\u2225\u2225 2\nUsing the SSS conditions and the fact that \u2225\u2225bSt+1\u2225\u22252 = \u2225\u2225bSt+1\\S\u2217\u2225\u22252 gives us\u2225\u2225bSt+1\u2225\u22252 = \u2225\u2225bCRt+1\u2225\u22252 \u2264 (\u221a\u039b\u03b1 +\u221a\u039b\u03b2)\u2225\u2225w\u2217 \u2212wt+1\u2225\u22252 \u2264 2\u221a\u039b\u03b2 \u2225\u2225w\u2217 \u2212wt+1\u2225\u22252\nNow, using the expression for wt+1 gives us\u2225\u2225w\u2217 \u2212wt+1\u2225\u2225 2 \u2264 \u2225\u2225(I \u2212 \u03b7Ct)(w\u2217 \u2212wt)\u2225\u22252 + \u03b7 \u2016XStbSt\u20162\nWe will bound the two terms on the right hand separately. We can bound the second term easily as \u03b7 \u2016XStbSt\u20162 \u2264 \u03b7 \u221a \u039b\u03b1 \u2016bSt\u20162 \u2264 \u03b7 \u221a \u039b\u03b2 \u2016bSt\u20162 ,\nsince \u2016bSt\u20160 \u2264 \u03b1 \u00b7 n. For the first term we observe that for \u03b7 \u2264 1 \u039b1\u2212\u03b2 , we have\n\u2016I \u2212 \u03b7Ct\u20162 = sup v\u2208Sp\u22121 \u2223\u22231\u2212 \u03b7 \u00b7 v>Ctv\u2223\u2223 = sup v\u2208Sp\u22121 { 1\u2212 \u03b7 \u00b7 v>Ctv } \u2264 1\u2212 \u03b7\u03bb1\u2212\u03b2 ,\nwhich we can use to bound\u2225\u2225w\u2217 \u2212wt+1\u2225\u2225 2 \u2264 (1\u2212 \u03b7\u03bb1\u2212\u03b2) \u2225\u2225w\u2217 \u2212wt\u2225\u2225 2 + \u03b7 \u221a \u039b\u03b2 \u2016bSt\u20162\nThis gives us, for \u03b7 = 1\u039b1\u2212\u03b2 ,\u2225\u2225bSt+1\u2225\u22252 \u2264 2\u221a\u039b\u03b2 \u2225\u2225w\u2217 \u2212wt+1\u2225\u22252 \u2264 2 (1\u2212 \u03bb1\u2212\u03b2\u039b1\u2212\u03b2 )\n\ufe38 \ufe37\ufe37 \ufe38 (P )\n\u221a \u039b\u03b2 \u2225\u2225w\u2217 \u2212wt\u2225\u2225\n2 + 2 \u039b\u03b2 \u039b1\u2212\u03b2\ufe38 \ufe37\ufe37 \ufe38\n(Q)\n\u2016bSt\u20162 .\nFor Gaussian designs and small enough \u03b2, we can show (Q) \u2264 14 as we did in Theorem 4. To bound (P ), we use the lower bound on \u03bb1\u2212\u03b2 given by Theorem 15 and use the following tighter upper bound for \u039b1\u2212\u03b2 :\n\u039b1\u2212\u03b2 \u2264 ( (1\u2212 \u03b2) + 3e \u221a\n6\u03b2(1\u2212 \u03b2) log e \u03b2\n) n+O (\u221a np+ n log 1\n\u03b4\n)\nThe above bound is obtained similarly to the one in Theorem 15 but uses the identity ( n k ) = ( n n\u2212k ) \u2264(\nen n\u2212k\n)n\u2212k for values of k \u2265 n/2 instead. For small enough \u03b2 and n = \u2126 ( \u03ba2(\u03a3)(p+ log 1\u03b4 ) ) , we can then\nshow (P ) \u2264 14 as well. Let \u03a8t := \u221a n \u2016w\u2217 \u2212wt\u20162 + \u2016bSt\u2016. Using elementary manipulations and the fact that\u221a\n\u039b\u03b2 \u2265 \u2126 ( \u221a n), we can then show that\n\u03a8t+1 \u2264 3/4 \u00b7\u03a8t. Thus, in t = O ( log (( \u2016w\u2217\u20162 + \u2016b\u20162\u221a n ) 1 )) iterations of the algorithm, we arrive at an -optimal solution i.e. \u2016w\u2217 \u2212wt\u20162 \u2264 . A similar argument holds true for sub-Gaussian designs as well."}, {"heading": "E Proof of Theorem 6", "text": "Theorem 6. Suppose Algorithm 4 is executed on data that allows Algorithms 2 and 3 a convergence rate of \u03b7FC and \u03b7GD respectively. Suppose we have 2 \u00b7 \u03b7FC \u00b7 \u03b7GD < 1. Then for any interleavings of the FC and GD steps that the policy may enforce, after t = O ( log (\n1\u221a n\n\u2016b\u20162 )) iterations, Algorithm 4 ensures an -optimal\nsolution i.e. \u2016wt \u2212w\u2217\u2016 \u2264 .\nProof. Our proof shall essentially show that the FC and GD steps do not undo the progress made by the other if executed in succession and if 2 \u00b7 \u03b7FC \u00b7 \u03b7GD < 1, actually ensure non-trivial progress. Let\n\u03a8FCt = \u2016bSt\u20162 \u03a8GDt = \u221a n \u2225\u2225wt \u2212w\u2217\u2225\u2225+ \u2016bSt\u20162\ndenote the potential functions used in the analyses of the FC and GD algorithms before. Then we will show below that if the FC and GD algorithms are executed in steps t and t+ 1 then we have\n\u03a8FCt+2 \u2264 2 \u00b7 \u03b7FC \u00b7 \u03b7GD \u00b7\u03a8FCt\nAlternatively, if the GD and FC algorithms are executed in steps t and t+ 1 respectively, then\n\u03a8GDt+2 \u2264 2 \u00b7 \u03b7FC \u00b7 \u03b7GD \u00b7\u03a8GDt\nThus, if algorithm executes the FC step at the time step t, then it would at least ensure \u03a8FCt \u2264 (2 \u00b7 \u03b7FC \u00b7 \u03b7GD) t/2\u00b7 \u03a8FC0 (similarly if the last step is a GD step). Since both the FC and GD algorithms ensure \u2016wt \u2212w\u2217\u20162 \u2264 for t \u2265 O ( log (\n1\u221a n\n\u2016b\u20162 )) , the claim would follow.\nWe now prove the two claimed results regarding the two types of interleaving below\n1. FC \u2212\u2192 GD The FC step guarantees \u2225\u2225bSt+1\u2225\u22252 \u2264 \u03b7FC \u00b7 \u2016bSt\u2016 as well as \u2225\u2225wt+1 \u2212w\u2217\u2225\u22252 \u2264 \u03b7FC \u00b7 \u2016bSt\u2016\u221an , whereas the GD step guarantees \u03a8GDt+2 \u2264 \u03b7GD \u00b7\u03a8GDt+1. Together these guarantee\n\u221a n \u2225\u2225wt+2 \u2212w\u2217\u2225\u2225 2 + \u2225\u2225bSt+2\u2225\u22252 \u2264 \u03b7GD \u00b7 \u221an\u2225\u2225wt+1 \u2212w\u2217\u2225\u22252 + \u2225\u2225bSt+1\u2225\u22252\n\u2264 2 \u00b7 \u03b7FC \u00b7 \u03b7GD \u00b7 \u2016bSt\u20162 Since \u221a n \u2225\u2225wt+2 \u2212w\u2217\u2225\u2225\n2 \u2265 0, this yields the result.\n2. GD \u2212\u2192 FC The GD step guarantees \u03a8GDt+1 \u2264 \u03b7GD \u00b7\u03a8GDt whereas the FC step guarantees \u2225\u2225bSt+2\u2225\u22252 \u2264 \u03b7FC \u00b7 \u2225\u2225bSt+1\u2225\u2225 as well as \u2225\u2225wt+2 \u2212w\u2217\u2225\u2225 2 \u2264 \u03b7FC \u00b7 \u2016bSt+1\u2016\u221a n . Together these guarantee\n\u221a n \u2225\u2225wt+2 \u2212w\u2217\u2225\u2225 2 + \u2225\u2225bSt+2\u2225\u22252 \u2264 2\u03b7FC \u2225\u2225bSt+1\u2225\u22252\n\u2264 2 \u00b7 \u03b7FC \u00b7 \u03b7GD \u00b7\u03a8GDt ,\nwhere the second step follows from the GD step guarantee since \u221a n \u2225\u2225wt+1 \u2212w\u2217\u2225\u2225\n2 \u2265 0.\nThis finishes the proof."}, {"heading": "F Proof of Theorem 9", "text": "Theorem 9. Let X = [x1, . . . ,xn] \u2208 Rp\u00d7n be the given data matrix and y = XTw\u2217 + b be the corrupted output with \u2016w\u2217\u20160 \u2264 s\u2217 and \u2016b\u20160 \u2264 \u03b1 \u00b7 n. Let Algorithm 2 be executed on this data with the IHT update from [6] and thresholding parameter set to \u03b2 \u2265 \u03b1. Let \u03a30 be an invertible matrix such that \u03a3\u22121/20 X satisfies the SRSC and SRSS properties at level (\u03b3, 2s + s\u2217) with constants \u03b1(\u03b3,2s+s\u2217) and L(\u03b3,2s+s\u2217) respectively\n(see Definition 8) for s \u2265 32 ( L(\u03b3,2s+s\u2217) \u03b1(\u03b3,2s+s\u2217) ) with \u03b3 = 1 \u2212 \u03b2. If X also satisfies 4L(\u03b2,s+s\u2217)\u03b1(1\u2212\u03b2,s+s\u2217) < 1, then after\nt = O ( log (\n1\u221a n\n\u2016b\u20162 )) iterations, Algorithm 2 obtains an -accurate solution wt i.e. \u2016wt \u2212w\u2217\u20162 \u2264 . In\nparticular, if X is sampled from a Gaussian distribution N (0,\u03a3) and n \u2265 \u2126 ( (2s+ s\u2217) log p+ log 1\u03b4 ) , then for all values of \u03b1 \u2264 \u03b2 < 165 , we can guarantee recovery as \u2016w t \u2212w\u2217\u20162 \u2264 .\nProof. We first begin with the guarantee provided by existing sparse recovery techniques. The results of [6], for example, indicate that if the input to the algorithm indeed satisfies the RSC and RSS properties at the level (1 \u2212 \u03b2, 2s + s\u2217) with constants \u03b12s+s\u2217 and L2s+s\u2217 for s \u2265 32 ( L2s+s\u2217\n\u03b12s+s\u2217\n) , then in time \u03c4 =\nO ( L2s+s\u2217\n\u03b12s+s\u2217 \u00b7 log ( \u2016b\u20162 \u03c1 )) , the IHT algorithm [6, Algorithm 1] outputs an updated model wt+1 that satisfies\u2225\u2225wt+1\u2225\u2225\n0 \u2264 s, as well as \u2225\u2225ySt \u2212X>Stwt+1\u2225\u222522 \u2264 \u2225\u2225ySt \u2212X>Stw\u2217\u2225\u222522 + \u03c1.\nWe will set \u03c1 later. Since the SRSC and SRSS properties ensure the above and y = X>w\u2217+ b, this gives us\u2225\u2225X>St(wt+1 \u2212w\u2217)\u2225\u222522 \u2264 2(wt+1 \u2212w\u2217)>X>StbSt + \u03c1 = 2(wt+1 \u2212w\u2217)>X>St\u2229S\u0304\u2217bSt\u2229S\u0304\u2217 + \u03c1, since bS = 0 for any set S \u2229 S\u0304\u2217 = \u03c6. We now analyze the two sides separately below using the SRSC and SRSS properties below. For any S \u2282 [n], denote X\u0303S := \u03a3\u22121/20 X.\u2225\u2225X>St(wt+1 \u2212w\u2217)\u2225\u222522 = \u2225\u2225\u2225X\u0303>St\u03a31/20 (wt+1 \u2212w\u2217)\u2225\u2225\u222522 \u2265 \u03b1(1\u2212\u03b2,s+s\u2217) \u2225\u2225\u2225\u03a31/20 (wt+1 \u2212w\u2217)\u2225\u2225\u222522\u2225\u2225XSt\u2229S\u0304\u2217(wt+1 \u2212w\u2217)\u2225\u2225 = \u2225\u2225\u2225X\u0303St\u2229S\u0304\u2217\u03a31/20 (wt+1 \u2212w\u2217)\u2225\u2225\u2225 \u2264\u221aL(\u03b2,s+s\u2217) \u2225\u2225\u2225\u03a31/20 (wt+1 \u2212w\u2217)\u2225\u2225\u2225\n2 .\nNow, if \u2225\u2225wt+1 \u2212w\u2217\u2225\u2225\n2 \u2265 , then \u2225\u2225\u2225\u03a31/20 (wt+1 \u2212w\u2217)\u2225\u2225\u2225 2 \u2265 \u221a \u03bbmin(\u03a30) \u00b7 . This give us\n\u2225\u2225\u2225\u03a31/20 (wt+1 \u2212w\u2217)\u2225\u2225\u2225 2 \u2264 2 \u221a L(\u03b2,s+s\u2217)\n\u03b1(1\u2212\u03b2,s+s\u2217) \u2225\u2225bSt\u2229S\u0304\u2217\u2225\u22252 + \u03c1\u03b1(1\u2212\u03b2,s+s\u2217) = 2 \u221a L(\u03b2,s+s\u2217)\n\u03b1(1\u2212\u03b2,s+s\u2217) \u2016bSt\u20162 +\n\u03c1 \u00b7 \u221a \u03bbmin(\u03a30) \u00b7 \u03b1(1\u2212\u03b2,s+s\u2217) .\nWe note that although we declared the SRSC and SRSS properties for the action of matrices on sparse vectors (such as w\u2217 \u2212 wt+1), we instead applied them above to the action of matrices on sparse vectors transformed by \u03a3\n1/2 0 (\u03a3 1/2 0 (w \u2217 \u2212 wt+1)). Since \u03a31/20 v need not be sparse even if v is sparse, this appears to pose a problem. However, all we need to resolve this is to notice that the proof technique of Theorem 18 which would be used to establish the SRSC and SRSS properties, holds in general for not just the action of a matrix on the set of sparse vectors, but on vectors in the union of any fixed set of low dimensional subspaces.\nMore specifically, we can modify the RSC and RSS properties (and by extension, the SRSC and SRSS properties), to requiring that the matrix X act as an approximate isometry on the following set of vectors Sp\u22121(s,\u03a30) := { v : v = \u03a3 \u22121/2 0 v \u2032 for some v\u2032 \u2208 Sp\u22121s } . We refer the reader to the work of [2] which describes this technique in great detail. Proceeding with the proof, the assurance of the thresholding step, as used in the proof of Theorem 5, along with a straightforward application of the (modified) SRSS property gives us\u2225\u2225bSt+1\u2225\u22252 \u2264 \u2225\u2225\u2225X>CRt+1(wt+1 \u2212w\u2217)\u2225\u2225\u22252 + \u2225\u2225\u2225X>MDt+1(wt+1 \u2212w\u2217)\u2225\u2225\u22252\n= \u2225\u2225\u2225X\u0303>CRt+1\u03a31/20 (wt+1 \u2212w\u2217)\u2225\u2225\u2225 2 + \u2225\u2225\u2225X\u0303>MDt+1\u03a31/20 (wt+1 \u2212w\u2217)\u2225\u2225\u2225 2\n\u2264 2 \u221a L(\u03b2,s+s\u2217) \u2225\u2225\u2225\u03a31/20 (wt+1 \u2212w\u2217)\u2225\u2225\u2225 2\n\u2264 4L(\u03b2,s+s\u2217)\n\u03b1(1\u2212\u03b2,s+s\u2217) \u2016bSt\u20162 +\n2\u03c1 \u221a L(\u03b2,s+s\u2217)\n\u00b7 \u221a \u03bbmin(\u03a30) \u00b7 \u03b1(1\u2212\u03b2,s+s\u2217)\nThus, whenever \u2225\u2225wt+1 \u2212w\u2217\u2225\u2225\n2 > , in successive steps, \u2016bSt\u20162 undergoes a linear decrease. Denoting\n\u03b7 := 4L(\u03b2,s+s\u2217) \u03b1(1\u2212\u03b2,s+s\u2217) , we get\n\u2225\u2225bSt+1\u2225\u22252 \u2264 \u03b7t \u00b7 \u2016b\u20162 + (1\u2212 \u03b7t1\u2212 \u03b7 )\n2\u03c1 \u221a L(\u03b2,s+s\u2217)\n\u00b7 \u221a \u03bbmin(\u03a30) \u00b7 \u03b1(1\u2212\u03b2,s+s\u2217)\nand using \u2225\u2225\u2225\u03a31/20 (wt \u2212w\u2217)\u2225\u2225\u2225 2 \u2265 \u221a \u03bbmin(\u03a30) \u2016wt \u2212w\u2217\u20162 gives us\n\u2225\u2225wt+1 \u2212w\u2217\u2225\u2225 2 \u2264 2 \u221a L(\u03b2,s+s\u2217)\u221a\n\u03bbmin(\u03a30) \u00b7 \u03b1(1\u2212\u03b2,s+s\u2217) \u2225\u2225bSt+1\u2225\u22252 + \u03c1\u03bbmin(\u03a30) \u00b7 \u03b1(1\u2212\u03b2,s+s\u2217) \u2264 \u03b7t 2 \u221a L(\u03b2,s+s\u2217)\u221a\n\u03bbmin(\u03a30) \u00b7 \u03b1(1\u2212\u03b2,s+s\u2217) \u2016b\u20162 +\n36\u03c1\n\u00b7 \u03bbmin(\u03a30) \u00b7 \u03b1(1\u2212\u03b2,s+s\u2217) ,\nwhere we have assumed that 4L(\u03b2,s+s\u2217) \u03b1(1\u2212\u03b2,s+s\u2217) < 9/10, something that we shall establish below. Note that \u03bbmin(\u03a30) > 0 since \u03a3 is assumed to be invertible. In the random design settings we shall consider, we\nalso have\n\u221a L(\u03b2,s+s\u2217)\u221a\n\u03bbmin(\u03a30)\u00b7\u03b1(1\u2212\u03b2,s+s\u2217) = O\n( 1\u221a n ) . Then setting \u03c1 \u2264 172\n2 \u00b7 \u03bbmin(\u03a30) \u00b7 \u03b1(1\u2212\u03b2,s+s\u2217) proves the convergence result.\nAs before, we can use the above result to establish sparse recovery guarantees in the statistical setting for Gaussian and sub-Gaussian design models. If our data matrix X is generated from a Gaussian distribution\nN (0,\u03a3) for some invertible \u03a3, then the results in Theorem 18 can be used to establish that \u03a3\u22121/2X satisfies the SRSC and SRSS properties at the required levels and that for \u03b1 < 1190 and n \u2265 \u2126 ( (2s+ s\u2217) log p+ log 1\u03b4 ) , we have \u03b7 = 2L(\u03b2,s+s\u2217) \u03b1(1\u2212\u03b2,s+s\u2217) < 9/10.\nThus, the above result can be applied with \u03a30 = \u03a3 to get convergence guarantees in the general Gaussian setting. We note that the above analysis can tolerate the same level of corruption as Theorem 4 and thus, we can improve the noise tolerance level to \u03b1 \u2264 165 here as well. We also note that these results can be readily extended to the sub-Gaussian setting as well."}, {"heading": "G Robust Statistical Estimation", "text": "This section elaborates on how results on the convergence guarantees of our algorithms can be used to give guarantees for robust statistical estimation problems. We begin with a few definition of sampling models that would be used in our results.\nDefinition 12. A random variable x \u2208 R is called sub-Gaussian if the following quantity is finite\nsup p\u22651\np\u22121/2 (E |x|p)1/p .\nMoreover, the smallest upper bound on this quantity is referred to as the sub-Gaussian norm of x and denoted as \u2016x\u2016\u03c82 . Definition 13. A vector-valued random variable x \u2208 Rp is called sub-Gaussian if its unidimensional marginals \u3008x,v\u3009 are sub-Gaussian for all v \u2208 Sp\u22121. Moreover, its sub-Gaussian norm is defined as follows\n\u2016X\u2016\u03c82 := sup v\u2208Sp\u22121 \u2016\u3008x,v\u3009\u2016\u03c82\nWe will begin with the analysis of Gaussian designs and then extend our analysis for the class of general sub-Gaussian designs.\nLemma 14. Let X \u2208 Rp\u00d7n be a matrix whose columns are sampled i.i.d from a standard Gaussian distribution i.e. xi \u223c N (0, I). Then for any > 0, with probability at least 1\u2212 \u03b4, X satisfies\nsmax(XX >) \u2264 n+ (1\u2212 2 )\u22121 \u221a cnp+ c\u2032n log 2\n\u03b4\nsmin(XX >) \u2265 n\u2212 (1\u2212 2 )\u22121 \u221a cnp+ c\u2032n log 2\n\u03b4 ,\nwhere c = 24e2 log 3 and c \u2032 = 24e2.\nProof. We will first use the fact that X is sampled from a standard Gaussian to show that its covariance concentrates around identity. Thus, we first show that with high probability,\u2225\u2225XX> \u2212 nI\u2225\u2225\n2 \u2264 1\nfor some 1 < 1. Doing so will automatically establish the following result\nn\u2212 1 \u2264 smin(XX>) \u2264 smax(XX>) \u2264 n+ 1.\nLet A := XX> \u2212 I. We will use the technique of covering numbers [14] to establish the above. Let Cp\u22121( ) \u2282 Sp\u22121 be an cover for Sp\u22121 i.e. for all u \u2208 Sp\u22121, there exists at least one v \u2208 Cp\u22121 such that \u2016u\u2212 v\u20162 \u2264 . Standard constructions [14, see Lemma 5.2] guarantee such a cover of size at most( 1 + 2\n)p \u2264 ( 3 )p. Now for any u \u2208 Sp\u22121 and v \u2208 Cp\u22121 such that \u2016u\u2212 v\u20162 \u2264 , we have\u2223\u2223u>Au\u2212 v>Av\u2223\u2223 \u2264 \u2223\u2223u>A(u\u2212 v)\u2223\u2223+ \u2223\u2223v>A(u\u2212 v)\u2223\u2223 \u2264 2 \u2016A\u20162 ,\nwhich gives us \u2225\u2225XX> \u2212 nI\u2225\u2225 2 \u2264 (1\u2212 2 )\u22121 \u00b7 sup\nv\u2208Cp\u22121( )\n\u2223\u2223\u2223\u2225\u2225X>v\u2225\u22252 2 \u2212 n \u2223\u2223\u2223 . Now for a fixed v \u2208 Sn\u22121, the random variable \u2225\u2225X>v\u2225\u22252 2\nis distributed as a \u03c72(n) distribution with n degrees of freedom. Using Lemma 20, we get, for any \u00b5 < 1,\nP [\u2223\u2223\u2223\u2225\u2225X>v\u2225\u222522 \u2212 n\u2223\u2223\u2223 \u2265 \u00b5n] \u2264 2 exp(\u2212min{ \u00b52n224ne2 , \u00b5n4\u221a3e }) \u2264 2 exp ( \u2212 \u00b5 2n 24e2 ) .\nSetting \u00b52 = c \u00b7 pn + c \u2032 \u00b7 log\n2 \u03b4\nn , where c = 24e 2 log 3 and c \u2032 = 24e2, and taking a union bound over all Cp\u22121( ), we get\nP [ sup\nv\u2208Cp\u22121( )\n\u2223\u2223\u2223\u2225\u2225X>v\u2225\u22252 2 \u2212 n \u2223\u2223\u2223 \u2265\u221acnp+ c\u2032n log 2 \u03b4 ] \u2264 2 ( 3 )p exp ( \u2212 \u00b5 2n 24e2 ) \u2264 \u03b4.\nThis implies that with probability at least 1\u2212 \u03b4,\u2225\u2225XX> \u2212 nI\u2225\u2225 2 \u2264 (1\u2212 2 )\u22121 \u221a cnp+ c\u2032n log 2\n\u03b4 ,\nwhich gives us the claimed bounds on the singular values of XX>.\nTheorem 15. Let X \u2208 Rp\u00d7n be a matrix whose columns are sampled i.i.d from a standard Gaussian distribution i.e. xi \u223c N (0, I). Then for any \u03b3 > 0, with probability at least 1\u2212 \u03b4, the matrix X satisfies the SSC and SSS properties with constants\n\u039bGauss\u03b3 \u2264 \u03b3n ( 1 + 3e \u221a 6 log e\n\u03b3\n) +O (\u221a np+ n log 1\n\u03b4\n)\n\u03bbGauss\u03b3 \u2265 n\u2212 (1\u2212 \u03b3)n ( 1 + 3e \u221a 6 log e\n1\u2212 \u03b3\n) \u2212 \u2126 (\u221a np+ n log 1\n\u03b4\n) .\nProof. For any fixed S \u2208 S\u03b3 , Lemma 14 guarantees the following bound\nsmax(XSX > S ) \u2264 \u03b3n+ (1\u2212 2 )\u22121\n\u221a c\u03b3np+ c\u2032\u03b3n log 2\n\u03b4 .\nTaking a union bound over S\u03b3 and noting that ( n k ) \u2264 ( en k )k for all 1 \u2264 k \u2264 n, gives us\n\u039b\u03b3 \u2264 \u03b3n+ (1\u2212 2 )\u22121 \u221a c\u03b3np+ c\u2032\u03b32n2 log e\n\u03b3 + c\u2032\u03b3n log\n2\n\u03b4 \u2264 \u03b3n ( 1 + (1\u2212 2 )\u22121 \u221a c\u2032 log e\n\u03b3\n) + (1\u2212 2 )\u22121 \u221a c\u03b3np+ c\u2032\u03b3n log 2\n\u03b4 ,\nwhich finishes the first bound after setting = 1/6. For the second bound, we use the equality\nXSX > S = XX > \u2212XS\u0304X>S\u0304 ,\nwhich provides the following bound for \u03bb\u03b3\n\u03bb\u03b3 \u2265 smin(XX>)\u2212 sup T\u2208S1\u2212\u03b3 XTX > T = smin(XX >)\u2212 \u039b1\u2212\u03b3 .\nUsing Lemma 14 to bound the first quantity and the first part of this theorem to bound the second quantity gives us, with probability at least 1\u2212 \u03b4,\n\u03bb\u03b3 \u2265 n\u2212 \u03b3\u2032n ( 1 + (1\u2212 2 )\u22121 \u221a c\u2032 log e\n\u03b3\u2032\n) \u2212 (1\u2212 2 )\u22121 ( 1 + \u221a \u03b3\u2032 )\u221a cnp+ c\u2032n log 2\n\u03b4 ,\nwhere \u03b3\u2032 = 1\u2212 \u03b3. This proves the second bound after setting = 1/6.\nWe now extend our analysis to the class of isotropic subGaussian distributions. We note that this analysis is without loss of generality since for non-isotropic sub-Gaussian distributions, we can simply use the fact that Theorem 3 can admit whitened data for calculation of the SSC and SSS constants as we did for the case of non-isotropic Gaussian distributions.\nLemma 16. Let X \u2208 Rp\u00d7n be a matrix with columns sampled from some sub-Gaussian distribution with sub-Gaussian norm K and covariance \u03a3. Then, for any \u03b4 > 0, with probability at least 1 \u2212 \u03b4, each of the following statements holds true:\nsmax(XX >) \u2264 \u03bbmax(\u03a3) \u00b7 n+ CK \u00b7 \u221a pn+ t \u221a n smin(XX >) \u2265 \u03bbmin(\u03a3) \u00b7 n\u2212 CK \u00b7 \u221a pn\u2212 t \u221a n,\nwhere t = \u221a\n1 cK log 2\u03b4 , and cK , CK are absolute constants that depend only on the sub-Gaussian norm K of\nthe distribution.\nProof. Since the singular values of a matrix are unchanged upon transposition, we shall prove the above statements for X>. The benefit of this is that we get to work with a matrix with independent rows, so that standard results can be applied. The proof technique used in [14, Theorem 5.39] (see also Remark 5.40 (1) therein) can be used to establish the following result: with probability at least 1\u2212 \u03b4, with t set as mentioned in the theorem statement, we have \u2225\u2225\u2225\u2225 1nXX> \u2212 \u03a3\n\u2225\u2225\u2225\u2225 \u2264 CK\u221a pn + t\u221an This implies that for any v \u2208 Sp\u22121, we have\u2223\u2223\u2223\u2223 1n \u2225\u2225X>v\u2225\u222522 \u2212 v>\u03a3v \u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223 1nv>XX>v \u2212 v>\u03a3v \u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223 1nXX>v \u2212 \u03a3v\n\u2223\u2223\u2223\u2223 \u2264 CK\u221a pn + t\u221an. The results then follow from elementary manipulations and the fact that the singular values and eigenvalues of real symmetric matrices coincide.\nTheorem 17. Let X \u2208 Rp\u00d7n be a matrix with columns sampled from some sub-Gaussian distribution with sub-Gaussian norm K and covariance \u03a3. Let cK , CK and t be fixed to values as required in Lemma 16. Note that cK and CK are absolute constants depend only on the sub-Gaussian norm K of the distribution. Let \u03b3 \u2208 (0, 1] be some fixed constant. Then, with we have the following:\n\u039bsubGauss(K,\u03a3)\u03b3 \u2264 ( \u03bbmax(\u03a3) \u00b7 \u03b3 + \u221a \u03b3\ncK log\ne\n\u03b3\n) \u00b7 n+ CK \u00b7 \u221a \u03b3pn+ t \u221a n.\nFurthermore, fix any \u2208 (0, 1) and let \u03b3 be a value in (0, 1) satisfying the following\n\u03b3 > 1\u2212min { \u00b7 \u03bbmin(\u03a3) \u03bbmax(\u03a3) , exp ( 1 +W\u22121 ( \u2212cK 2 \u00b7 \u03bb2min(\u03a3) e ))} ,\nwhere W\u22121(\u00b7) is the lower branch of the real valued restriction of the Lambert W function. Then we have, with the same confidence,\n\u03bbsubGauss(K,\u03a3)\u03b3 \u2265 (1\u2212 2 ) \u00b7 \u03bbmin(\u03a3) \u00b7 n\u2212 CK ( 1 + \u221a 1\u2212 \u03b3 )\u221a pn\u2212 2t \u221a n\nProof. The first result follows from an application of Lemma 16, a union bound over sets in S\u03b3 , as well as the bound ( n k ) \u2264 ( en k )k for all 1 \u2264 k \u2264 n which puts a bound on the number of sparse sets as log |S\u03b3 | \u2264 \u03b3 \u00b7n log e\u03b3 .\nFor the second result, we observe that XSX > S = XX > \u2212XS\u0304X>S\u0304 , so that smin(XSX > S ) \u2265 smin(XX>) \u2212\nsmax(XS\u0304X > S\u0304 ). This gives us\ninf S\u2208S\u03b3\nsmin(XSX > S ) \u2265 smin(XX>)\u2212 sup\nS\u2208S1\u2212\u03b3 smax(XSX\n> S ).\nUsing Lemma 16 and the first part of this result gives us\ninf S\u2208S\u03b3\nsmin(XSX > S ) \u2265 \u03bbmin(\u03a3) \u00b7 n\u2212 CK \u00b7\n\u221a pn\u2212 t \u221a n\n\u2212 ( \u03bbmax(\u03a3)(1\u2212 \u03b3) + \u221a 1\u2212 \u03b3 cK log e 1\u2212 \u03b3 ) n\u2212 CK \u221a (1\u2212 \u03b3)pn\u2212 t \u221a n\n= ( \u03bbmin(\u03a3)\u2212 \u03bbmax(\u03a3)(1\u2212 \u03b3)\u2212 \u221a 1\u2212 \u03b3 cK log e 1\u2212 \u03b3 ) n\n\u2212 CK ( 1 + \u221a 1\u2212 \u03b3 )\u221a pn\u2212 2t \u221a n\n\u2265 (1\u2212 2 ) \u00b7 \u03bbmin(\u03a3) \u00b7 n\u2212 CK ( 1 + \u221a 1\u2212 \u03b3 )\u221a pn\u2212 2t \u221a n,\nwhere the last step follows from the assumptions on \u03b3 and by noticing that it suffices to show the following two inequalities to establish the last step\n1. \u03bbmax(\u03a3)(1\u2212 \u03b3) \u2264 \u00b7 \u03bbmin(\u03a3)\n2. (1\u2212 \u03b3) log e1\u2212\u03b3 \u2264 cK 2 \u00b7 \u03bb2min(\u03a3)\nThe first part gives us the condition \u03b3 > 1 \u2212 \u00b7\u03bbmin(\u03a3)\u03bbmax(\u03a3) in a straightforward manner. For the second part, denote v = cK\n2 \u00b7 \u03bb2min(\u03a3). Note that for v \u2265 1, all values of \u03b3 \u2208 (0, 1] satisfy the inequality. Otherwise we require the use of the Lambert W function (also known as the product logarithm function). This function ensures that its value W (z) for any z > \u22121/e satisfies z = W (z)eW (z). In our case, making a change of variable (1\u2212\u03b3) = e\u03b7 gives us the inequality (\u03b7\u22121)e\u03b7\u22121 \u2265 \u2212v/e. Note that since v \u2264 1 in this case, \u2212v/e \u2208 (\u22121/e, 0) i.e. a valid value for the Lambert W function. However, (\u22121/e, 0) is also the region in which the Lambert W function is multi-valued. Taking the worse bound for \u03b3 by choosing the lower branch\nW\u22121(\u00b7) gives us the second condition \u03b3 \u2265 1\u2212 exp ( 1 +W\u22121 ( \u2212 cK 2\u00b7\u03bb2min(\u03a3) e )) .\nIt is important to note that for any \u22121/e \u2264 z < 0, we have exp (1 +W\u22121(z)) > 0 which means that the bounds imposed on \u03b3 by Theorem 17 always allow a non-zero fraction of the data points to be corrupted in an adversarial manner. However, the exact value of that fraction depends, in a complicated manner, on the sub-Gaussian norm of the underlying distribution, as well as the condition number and the smallest eigenvalue of the second moment of the underlying distribution.\nWe also note that due to the generic nature of the previous analysis, which can handle the entire class of sub-Gaussian distributions, the bounds are not as explicitly stated in terms of universal constants as they are for the standard Gaussian design setting (Theorem 15).\nWe now establish that for a wide family of random designs, the SRSC and SRSS properties are satisfied with high probability as well. For sake of simplicity, we will present our analysis for the standard Gaussian design. However, the results would readily extend to general Gaussian and sub-Gaussian designs using techniques similar to Theorem 17.\nTheorem 18. Let X \u2208 Rp\u00d7n be a matrix whose columns are sampled i.i.d from a standard Gaussian distribution i.e. xi \u223c N (0, I). Then for any \u03b3 > 0 and s \u2264 p, with probability at least 1 \u2212 \u03b4, the matrix X satisfies the SRSC and SRSS properties with constants\nLGauss(\u03b3,s) \u2264 \u03b3n ( 1 + 3e \u221a 6 log e\n\u03b3\n) + O\u0303 (\u221a ns+ n log 1\n\u03b4\n)\n\u03b1Gauss(\u03b3,s) \u2265 n\u2212 (1\u2212 \u03b3)n ( 1 + 3e \u221a 6 log e\n1\u2212 \u03b3\n) \u2212 \u2126\u0303 (\u221a ns+ n log 1\n\u03b4\n) .\nProof. The proof of this theorem proceeds similarly to that of Theorem 15. Hence, we simply point out the main differences. First, we shall establish, that for any > 0, with probability at least 1\u2212 \u03b4, X satisfies the RSC and RSS properties at level s with the following constants\nLs \u2264 n+ (1\u2212 2 )\u22121 \u221a bns+ b\u2032n log 2\n\u03b4 \u03b1s \u2265 n\u2212 (1\u2212 2 )\u22121 \u221a bns+ b\u2032n log 2\n\u03b4 ,\nwhere b = 24e2 log 3ep s and b \u2032 = 24e2. To do so we notice that the only change needed to be made would be in the application of the covering number argument. Instead of applying the union bound over an -cover Cp\u22121 of Sp\u22121, we would only have to consider an -cover Cp\u22121s of the set Sp\u22121s of all s-sparse unit vectors in p-dimensions. A straightforward calculation shows us that\n\u2223\u2223Cp\u22121s \u2223\u2223 \u2264 (ps )( 1 + 2 )s \u2264 ( 3ep s )s .\nThus, setting \u00b52 = b \u00b7 sn + b \u2032 \u00b7 log\n2 \u03b4\nn , where b = 24e 2 log 3ep s and b \u2032 = 24e2, we get\nP [ sup\nv\u2208Cp\u22121s\n\u2223\u2223\u2223\u2016Xv\u201622 \u2212 n\u2223\u2223\u2223 \u2265 \u221a bns+ b\u2032n log 2\n\u03b4\n] \u2264 \u03b4,\nwhich establishes the required RSC and RSS constants for X. Now, moving on to the SRSS constant, it follows simply by applying a union bound over all sets in S\u03b3 much like in Theorem 15. One can then proceed to bound the SRSC constant in a similar manner.\nWe note that the nature of the SRSC and SRSS bounds indicate that our Torrent-FC algorithm in the high dimensional sparse recovery setting has noise tolerance properties, characterized by the largest corruption index \u03b1 that can be tolerated, identical to its low dimnensional counterpart - something that Theorem 9 states explicitly."}, {"heading": "H Supplementary Results", "text": "Claim 19. Given any vector v \u2208 Rn, let \u03c3 \u2208 Sn be defined as the permutation that orders elements of v in descending order of their magnitudes i.e. \u2223\u2223v\u03c3(1)\u2223\u2223 \u2265 \u2223\u2223v\u03c3(2)\u2223\u2223 \u2265 . . . \u2265 \u2223\u2223v\u03c3(n)\u2223\u2223. For any 0 < p \u2264 q \u2264 1, let S1 \u2208 Sq be an arbitrary set of size q \u00b7 n and S2 = {\u03c3(i) : n\u2212 p \u00b7 n+ 1 \u2264 i \u2264 n}. Then we have \u2016vS2\u2016 2 2 \u2264 p q \u2016vS1\u2016 2 2 \u2264 \u2016vS1\u2016 2 2.\nProof. Let S3 = {\u03c3(i) : n\u2212 q \u00b7 n+ 1 \u2264 i \u2264 n} and S4 = {\u03c3(i) : n\u2212 q \u00b7 n+ 1 \u2264 i \u2264 n\u2212 p \u00b7 n}. Clearly, we have \u2016vS3\u2016 2 2 \u2264 \u2016vS1\u2016 2 2 since S3 contains the smallest q \u00b7 n elements (by magnitude). Now we have \u2016vS3\u2016 2 2 = \u2016vS2\u2016 2 2 + \u2016vS4\u2016 2 2. Moreover, since each element of S4 is larger in magnitude than every element of S2, we have 1\n|S4| \u2016vS4\u2016 2 2 \u2265\n1\n|S2| \u2016vS2\u2016 2 2 .\nThis gives us\n\u2016vS2\u2016 2 2 = \u2016vS3\u2016 2 2 \u2212 \u2016vS4\u2016 2 2 \u2264 \u2016vS3\u2016 2 2 \u2212 |S4| |S2| \u2016vS2\u2016 2 2 ,\nwhich upon simple manipulations, gives us the claimed result.\nLemma 20. Let Z be distributed according to the chi-squared distribution with k degrees of freedom i.e. Z \u223c \u03c72(k). Then for all t \u2265 0,\nP [|Z \u2212 k| \u2265 t] \u2264 2 exp ( \u2212min { t2\n24ke2 ,\nt 4 \u221a 3e }) Proof. This lemma requires a proof structure that traces several basic results in concentration inequalities for sub-exponential variables [14, Lemma 5.5, 5.15, Proposition 5.17]. The purpose of performing this exercise is to explicate the constants involved so that a crisp bound can be provided on the corruption index that our algorithm can tolerate in the standard Gaussian design case.\nWe first begin by establishing the sub-exponential norm of a chi-squared random variable with a single degree of freedom. Let X \u223c \u03c72(1). Then using standard results on the moments of the standard normal distribution gives us, for all p \u2265 2,\n(E|X|p)1/p = ((2p\u2212 1)!!)1/p = ( (2p)!\n2pp!\n)1/p \u2264 \u221a 3\n2 p\nThus, the sub-exponential norm of X is upper bounded by \u221a\n3/2. By applying the triangle inequality, we obtain, as a corollary, an upper bound on the sub-exponential norm of the centered random variable Y = X \u2212 1 as \u2016Y \u2016\u03c81 \u2264 2 \u2016X\u2016\u03c81 \u2264 \u221a 3.\nNow we bound the moment generating function of the random variable Y . Noting that EY = 0, we have, for any |\u03bb| \u2264 1\n2 \u221a 3e ,\nE exp(\u03bbY ) = 1 + \u221e\u2211 q=2 E(\u03bbY )q q! \u2264 1 + \u221e\u2211 q=2 ( \u221a 3|\u03bb|q)q q! \u2264 1 + \u221e\u2211 q=2 ( \u221a 3e|\u03bb|)q \u2264 1 + 6e2\u03bb2 \u2264 exp(6e2\u03bb2).\nNote that the second step uses the sub-exponentially of Y , the third step uses the fact that q! \u2265 (q/e)q, and the fourth step uses the bound on |\u03bb|. Now let X1, X2, . . . Xk be k independent random variables distributed as \u03c72(1). Then we have Z \u223c \u2211k i=1Xi. Using the exponential Markov\u2019s inequality, and the independence of the random variables Xi gives us\nP [Z \u2212 k \u2265 t] = P [ e\u03bb(Z\u2212k) \u2265 e\u03bbt ] \u2264 e\u2212\u03bbtEe\u03bb(Z\u2212k) = e\u2212\u03bbt k\u220f i=1 E exp(\u03bb(Xi \u2212 1)).\nFor any |\u03bb| \u2264 1 2 \u221a 3e , the above bounds on the moment generating function give us\nP [Z \u2212 k \u2265 t] \u2264 e\u2212\u03bbt k\u220f i=1 exp(6e2\u03bb2) = exp(\u2212\u03bbt+ 6ke2\u03bb2).\nChoosing \u03bb = min {\n1 2 \u221a 3e , t12ke2\n} , we get\nP [Z \u2212 k \u2265 t] \u2264 exp ( \u2212min { t2\n24ke2 ,\nt 4 \u221a 3e\n}) .\nRepeating this argument gives us the same bound for P [k \u2212 Z \u2265 t]. This completes the proof.\nI Supplementary Experimental Results"}], "references": [{"title": "Sampling and reconstructing signals from a union of linear subspaces", "author": ["Thomas Blumensath"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Iterative Hard Thresholding for Compressed Sensing", "author": ["Thomas Blumensath", "Mike E. Davies"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Robust Sparse Regression under Adversarial Corruption", "author": ["Yudong Chen", "Constantine Caramanis", "Shie Mannor"], "venue": "In 30th International Conference on Machine Learning (ICML),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Gradient descent with sparsification: an iterative algorithm for sparse recovery with restricted isometry property", "author": ["Rahul Garg", "Rohit Khandekar"], "venue": "In 26th International Conference on Machine Learning (ICML),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "On Iterative Hard Thresholding Methods for High-dimensional M-Estimation", "author": ["Prateek Jain", "Ambuj Tewari", "Purushottam Kar"], "venue": "In 28th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Adaptive estimation of a quadratic functional by model selection", "author": ["Beatrice Laurent", "Pascal Massart"], "venue": "The Annals of Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Fast and Robust Least Squares Estimation in Corrupted Linear Models", "author": ["Brian McWilliams", "Gabriel Krummenacher", "Mario Lucic", "Joachim M. Buhmann"], "venue": "In 28th Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Exact recoverability from dense corrupted observations via L1 minimization", "author": ["Nam H. Nguyen", "Trac D. Tran"], "venue": "IEEE Transaction on Information Theory,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Least Median of Squares Regression", "author": ["Peter J. Rousseeuw"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1984}, {"title": "Computing LTS Regression for Large Data Sets", "author": ["Peter J. Rousseeuw", "Katrien Driessen"], "venue": "Journal of Data Mining and Knowledge Discovery,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Robust Regression and Outlier Detection", "author": ["Peter J. Rousseeuw", "Annick M. Leroy"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1987}, {"title": "Recovery of Sparsely Corrupted Signals", "author": ["Christoph Studer", "Patrick Kuppinger", "Graeme Pope", "Helmut B\u00f6lcskei"], "venue": "IEEE Transaction on Information Theory,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Roman Vershynin"], "venue": "Compressed Sensing, Theory and Applications,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Dense Error Correction via ` Minimization", "author": ["John Wright", "Yi Ma"], "venue": "IEEE Transaction on Information Theory,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Robust Face Recognition via Sparse Representation", "author": ["John Wright", "Alan Y. Yang", "Arvind Ganesh", "S. Shankar Sastry", "Yi Ma"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}], "referenceMentions": [{"referenceID": 11, "context": "Owing to the wideapplicability of regression, RLSR features as a critical component of several important real-world applications in a variety of domains such as signal processing [13], economics [12], computer vision [16, 15], and astronomy [12].", "startOffset": 179, "endOffset": 183}, {"referenceID": 10, "context": "Owing to the wideapplicability of regression, RLSR features as a critical component of several important real-world applications in a variety of domains such as signal processing [13], economics [12], computer vision [16, 15], and astronomy [12].", "startOffset": 195, "endOffset": 199}, {"referenceID": 14, "context": "Owing to the wideapplicability of regression, RLSR features as a critical component of several important real-world applications in a variety of domains such as signal processing [13], economics [12], computer vision [16, 15], and astronomy [12].", "startOffset": 217, "endOffset": 225}, {"referenceID": 13, "context": "Owing to the wideapplicability of regression, RLSR features as a critical component of several important real-world applications in a variety of domains such as signal processing [13], economics [12], computer vision [16, 15], and astronomy [12].", "startOffset": 217, "endOffset": 225}, {"referenceID": 10, "context": "Owing to the wideapplicability of regression, RLSR features as a critical component of several important real-world applications in a variety of domains such as signal processing [13], economics [12], computer vision [16, 15], and astronomy [12].", "startOffset": 241, "endOffset": 245}, {"referenceID": 11, "context": "Indeed there exist reformulations of this problem that are known to be NP-hard to optimize [13].", "startOffset": 91, "endOffset": 95}, {"referenceID": 13, "context": "Recently, [15] and [9] obtained a surprising result which shows that one can recover w\u2217 exactly even when \u03b1 .", "startOffset": 10, "endOffset": 14}, {"referenceID": 7, "context": "Recently, [15] and [9] obtained a surprising result which shows that one can recover w\u2217 exactly even when \u03b1 .", "startOffset": 19, "endOffset": 22}, {"referenceID": 13, "context": "Moreover, the results impose severe restrictions on the data distribution, requiring that the data be either sampled from an isotropic Gaussian ensemble [15], or row-sampled from an incoherent orthogonal matrix [9].", "startOffset": 153, "endOffset": 157}, {"referenceID": 7, "context": "Moreover, the results impose severe restrictions on the data distribution, requiring that the data be either sampled from an isotropic Gaussian ensemble [15], or row-sampled from an incoherent orthogonal matrix [9].", "startOffset": 211, "endOffset": 214}, {"referenceID": 2, "context": "In contrast, [4] studied RLSR with less stringent assumptions, allowing arbitrary corruptions in response variables as well as in the data matrix X, and proposed a trimmed inner product based algorithm for the problem.", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": "A similar result was obtained by [8], albeit using a sub-sampling based algorithm with stronger assumptions on b.", "startOffset": 33, "endOffset": 36}, {"referenceID": 8, "context": "This intuitive algorithm seems to embody a long standing heuristic first proposed by Legendre [1] over two centuries ago (see introductory quotation in this paper) that has been adopted in later literature [10, 11] as well.", "startOffset": 206, "endOffset": 214}, {"referenceID": 9, "context": "This intuitive algorithm seems to embody a long standing heuristic first proposed by Legendre [1] over two centuries ago (see introductory quotation in this paper) that has been adopted in later literature [10, 11] as well.", "startOffset": 206, "endOffset": 214}, {"referenceID": 13, "context": "We would like to stress three key advantages of our result over the results of [15, 9]: a) we allow b to be adversarial, i.", "startOffset": 79, "endOffset": 86}, {"referenceID": 7, "context": "We would like to stress three key advantages of our result over the results of [15, 9]: a) we allow b to be adversarial, i.", "startOffset": 79, "endOffset": 86}, {"referenceID": 1, "context": "We would also like to stress that while hard-thresholding based methods have been studied rigorously for the sparse-recovery problem [3, 6], hard-thresholding has not been studied formally for the robust regression problem.", "startOffset": 133, "endOffset": 139}, {"referenceID": 4, "context": "We would also like to stress that while hard-thresholding based methods have been studied rigorously for the sparse-recovery problem [3, 6], hard-thresholding has not been studied formally for the robust regression problem.", "startOffset": 133, "endOffset": 139}, {"referenceID": 13, "context": "Indeed, the work of [15] assumes the Bouquet Model where distributions are restricted to be isotropic Gaussians whereas the work of [9] assumes a more stringent model of suborthonormal matrices, something that even Gaussian designs do not satisfy.", "startOffset": 20, "endOffset": 24}, {"referenceID": 7, "context": "Indeed, the work of [15] assumes the Bouquet Model where distributions are restricted to be isotropic Gaussians whereas the work of [9] assumes a more stringent model of suborthonormal matrices, something that even Gaussian designs do not satisfy.", "startOffset": 132, "endOffset": 135}, {"referenceID": 1, "context": "We reiterate that it is not possible to use existing results from sparse recovery (such as [3, 6]) directly to solve this problem.", "startOffset": 91, "endOffset": 97}, {"referenceID": 4, "context": "We reiterate that it is not possible to use existing results from sparse recovery (such as [3, 6]) directly to solve this problem.", "startOffset": 91, "endOffset": 97}, {"referenceID": 1, "context": "Assuming X satisfies the RSC/RSS properties (defined below), (3) can be solved efficiently using results from sparse recovery (for example the IHT algorithm [3, 5] analyzed in [6]).", "startOffset": 157, "endOffset": 163}, {"referenceID": 3, "context": "Assuming X satisfies the RSC/RSS properties (defined below), (3) can be solved efficiently using results from sparse recovery (for example the IHT algorithm [3, 5] analyzed in [6]).", "startOffset": 157, "endOffset": 163}, {"referenceID": 4, "context": "Assuming X satisfies the RSC/RSS properties (defined below), (3) can be solved efficiently using results from sparse recovery (for example the IHT algorithm [3, 5] analyzed in [6]).", "startOffset": 176, "endOffset": 179}, {"referenceID": 4, "context": "The sample complexity required by Theorem 9 is identical to the one required by analyses for high dimensional sparse recovery [6], save constants.", "startOffset": 126, "endOffset": 129}, {"referenceID": 13, "context": "Algorithms: We compared various variants of our algorithm Torrent to the regularized L1 algorithm for robust regression [15, 9].", "startOffset": 120, "endOffset": 127}, {"referenceID": 7, "context": "Algorithms: We compared various variants of our algorithm Torrent to the regularized L1 algorithm for robust regression [15, 9].", "startOffset": 120, "endOffset": 127}, {"referenceID": 0, "context": "[2] Thomas Blumensath.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3] Thomas Blumensath and Mike E.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] Yudong Chen, Constantine Caramanis, and Shie Mannor.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] Rahul Garg and Rohit Khandekar.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] Prateek Jain, Ambuj Tewari, and Purushottam Kar.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] Beatrice Laurent and Pascal Massart.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] Brian McWilliams, Gabriel Krummenacher, Mario Lucic, and Joachim M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] Nam H.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] Peter J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] Peter J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] Peter J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] Christoph Studer, Patrick Kuppinger, Graeme Pope, and Helmut B\u00f6lcskei.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] Roman Vershynin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] John Wright and Yi Ma.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] John Wright, Alan Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Using tail bounds on Chi-squared distributions [7], we get, with probability at least 1\u2212 \u03b4,", "startOffset": 47, "endOffset": 50}, {"referenceID": 5, "context": "However, we note that using a more finely tuned setting of the constant in the proof of Theorem 15 and a more careful proof using tight tail inequalities for chi-squared distributions [7], we can achieve a better corruption level tolerance of \u03b1 < 1 65 .", "startOffset": 184, "endOffset": 187}, {"referenceID": 4, "context": "Let Algorithm 2 be executed on this data with the IHT update from [6] and thresholding parameter set to \u03b2 \u2265 \u03b1.", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "The results of [6], for example, indicate that if the input to the algorithm indeed satisfies the RSC and RSS properties at the level (1 \u2212 \u03b2, 2s + s\u2217) with constants \u03b12s+s\u2217 and L2s+s\u2217 for s \u2265 32 ( L2s+s\u2217 \u03b12s+s\u2217 ) , then in time \u03c4 =", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "We refer the reader to the work of [2] which describes this technique in great detail.", "startOffset": 35, "endOffset": 38}, {"referenceID": 12, "context": "We will use the technique of covering numbers [14] to establish the above.", "startOffset": 46, "endOffset": 50}], "year": 2015, "abstractText": "We study the problem of Robust Least Squares Regression (RLSR) where several response variables can be adversarially corrupted. More specifically, for a data matrix X \u2208 Rp\u00d7n and an underlying model w\u2217, the response vector is generated as y = XTw\u2217+b where b \u2208 R is the corruption vector supported over at most C \u00b7n coordinates. Existing exact recovery results for RLSR focus solely on L1-penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of X. In this work, we study a simple hard-thresholding algorithm called Torrent which, under mild conditions on X, can recover w\u2217 exactly even if b corrupts the response variables in an adversarial manner, i.e. both the support and entries of b are selected adversarially after observing X and w\u2217. Our results hold under deterministic assumptions which are satisfied if X is sampled from any sub-Gaussian distribution. Finally unlike existing results that apply only to a fixed w\u2217, generated independently of X, our results are universal and hold for any w\u2217 \u2208 R. Next, we propose gradient descent-based extensions of Torrent that can scale efficiently to large scale problems, such as high dimensional sparse recovery and prove similar recovery guarantees for these extensions. Empirically we find Torrent, and more so its extensions, offering significantly faster recovery than the state-of-the-art L1 solvers. For instance, even on moderate-sized datasets (with p = 50K) with around 40% corrupted responses, a variant of our proposed method called Torrent-HYB is more than 20\u00d7 faster than the best L1 solver. \u201cIf among these errors are some which appear too large to be admissible, then those equations which produced these errors will be rejected, as coming from too faulty experiments, and the unknowns will be determined by means of the other equations, which will then give much smaller errors.\u201d A. M. Legendre, On the Method of Least Squares. 1805.", "creator": "LaTeX with hyperref package"}}}