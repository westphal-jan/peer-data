{"id": "1506.03662", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2015", "title": "Variance Reduced Stochastic Gradient Descent with Neighbors", "abstract": "Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet it is also known to be slow relative to steepest descent. The variance in the stochastic update directions only allows for sublinear or (with iterate averaging) linear convergence rates. Recently, variance reduction techniques such as SVRG and SAGA have been proposed to overcome this weakness. With asymptotically vanishing variance, a constant step size can be maintained, resulting in geometric convergence rates. However, these methods are either based on occasional computations of full gradients at pivot points (SVRG), or on keeping per data point corrections in memory (SAGA). This has the disadvantage that one cannot employ these methods in a streaming setting and that speed-ups relative to SGD may need a certain number of epochs in order to materialize. This paper investigates a new class of algorithms that can exploit neighborhood structure in the training data to share and re-use information about past stochastic gradients across data points. While not meant to be offering advantages in an asymptotic setting, there are significant benefits in the transient optimization phase, in particular in a streaming or single-epoch setting. We investigate this family of algorithms in a thorough analysis and show supporting experimental results. As a side-product we provide a simple and unified proof technique for a broad class of variance reduction algorithms. It is also worth considering the contribution of a common linear algorithm to stochastic gradient regression.\n\n\n\nThe class of stochastic gradient regression (FEM) algorithm for R. G., J. E. M., and G. T.\n[1] [2] [3] [4] [5] [6] [7] [8]\nThe class of FEM is the first to introduce a new class of gradient regression algorithms with a single point correction process that takes a single point correction process that takes a single point correction process that takes a single point correction process that takes a single point correction process that takes a single point correction process that takes a single point correction process that takes a single point correction process that takes a single point correction process that takes a single point correction process that takes a single point correction process that takes a single point correction process that takes a single point correction process that takes a single point correction process that takes a single point correction process that takes a single point correction process that takes a single point correction process that takes a single point correction process that takes a single point correction", "histories": [["v1", "Thu, 11 Jun 2015 13:14:33 GMT  (793kb,D)", "http://arxiv.org/abs/1506.03662v1", null], ["v2", "Thu, 5 Nov 2015 12:30:49 GMT  (332kb,D)", "http://arxiv.org/abs/1506.03662v2", null], ["v3", "Tue, 17 Nov 2015 22:00:11 GMT  (330kb,D)", "http://arxiv.org/abs/1506.03662v3", null], ["v4", "Fri, 26 Feb 2016 19:55:56 GMT  (334kb,D)", "http://arxiv.org/abs/1506.03662v4", "Appears in: Advances in Neural Information Processing Systems 28 (NIPS 2015). 13 pages"]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["thomas hofmann", "aur\u00e9lien lucchi", "simon lacoste-julien", "brian mcwilliams"], "accepted": true, "id": "1506.03662"}, "pdf": {"name": "1506.03662.pdf", "metadata": {"source": "CRF", "title": "Neighborhood Watch: Stochastic Gradient Descent with Neighbors", "authors": ["Thomas Hofmann", "Aurelien Lucchi", "Brian McWilliams"], "emails": ["@inf.ethz.ch"], "sections": [{"heading": "1 Introduction", "text": "We consider a general problem that is pervasive in machine learning, namely optimization of an empirical or regularized convex risk function. Given a convex loss l and a strongly convex regularizer \u2126, one aims at finding a parameter vector w which minimizes the (empirical) expectation:\nw\u2217 \u2208 argmin w f(w), f(w) := Efi(w) = 1 n n\u2211 i=1 fi(w), fi(w) := l(w, (xi, yi)) + \u2126(w) . (1)\nWe assume throughout the data-dependent functions fi to have Lipschitz-continuous gradients. Steepest descent is a straightforward algorithm to find a minimizer w\u2217 of f , but it requires the repeated computation of full gradients f \u2032(w), which becomes prohibitive in the case of massive data sets and which is impossible in the case of streaming data. Stochastic gradient descent (SGD) is a popular alternative, in particular in the context of large-scale learning [2, 9]. The key advantage of SGD is that each update only involves a single example. This is sufficient to compute a stochastic version of the true gradient as f \u2032i(w), which provides an unbiased estimate since Ef \u2032 i(w) = f \u2032(w).\nIt is a surprising recent finding [10, 5, 8, 6] that the additive structure of f allows for significantly faster convergence in expectation. Instead of O(1/t) or O(log t/t) rates for standard SGD variants, it is possible to obtain geometric rates, i.e. to guarantee exponentially fast convergence. While the classic convergence proof of SGD requires vanishing step sizes, typically at a rate of O(1/t) [7],\nar X\niv :1\n50 6.\n03 66\n2v 1\n[ cs\n.L G\n] 1\n1 Ju\nn 20\nthese more recent methods introduce corrections to the stochastic gradients that ensure convergence for finite step sizes.\nBased on the work mentioned above, the contributions of our paper are as follows: First, we provide a novel, simple, and more general analysis technique for studying variance reduced versions of SGD. Second, based on the above analysis, we present new insights into the trade-offs between freshness and biasedness of the corrections computed from previous stochastic gradients. Third, we present a new class of algorithms that resolves this trade-off by computing corrections based on stochastic gradients at neighboring points. Fourth, we will show experimentally that this new class of algorithms offers advantages in the regime of streaming data and single/few-epoch learning."}, {"heading": "2 Algorithms", "text": "Variance Reduced SGD Given an optimization problem as in (1), we investigate a class of stochastic gradient descent algorithms that generates an iterate sequence wt (t \u2265 0) with updates taking the form:\nw+ = w \u2212 \u03b3gi(w), gi(w) = f \u2032i(w)\u2212 \u03b1\u0304i (2)\nHere w is the current and w+ the new parameter vector, \u03b3 is the step size, and i is an index selected uniformly at random. \u03b1\u0304i are variance correction terms such that E\u03b1\u0304i = 0. The aim is to define updates of asymptotically vanishing variance, i.e. gi(w) \u2192 0 as w \u2192 w\u2217, which requires that \u03b1\u0304i \u2192 f \u2032i(w\u2217). This implies the corrections need to be designed in a way to exactly cancel out the stochasticity of f \u2032i(w \u2217) at the optimum.\nSAGA The SAGA algorithm [3] maintains variance corrections \u03b1i after selecting data point i by memorizing stochastic gradients. The update rule is \u03b1+i = f \u2032 i(w) for the selected i, and \u03b1 + j = \u03b1j , for j 6= i. Note that these corrections will be used, the next time the same index i gets sampled. Bias adjusted versions are then defined via \u03b1\u0304i := \u03b1i \u2212 \u03b1\u0304, where \u03b1\u0304 := 1n \u2211n i=1 \u03b1i. Obviously, \u03b1\u0304 can be updated incrementally. One convenient property of SAGA is that it can reuse the stochastic gradient f \u2032i(w) computed at step t to update both, w as well as the correction \u03b1i at no additional costs.\nWe also consider a variant of SAGA, which we call q-SAGA that on average updates q \u2265 1 randomly chosen \u03b1j variables at each iteration. While this is practically less interesting, it is a convenient reference point in our analysis as well as in the experiments to investigate the advantages of \u201dfresher\u201d corrections. Note that in SAGA the corrections will be on average n iterations \u201dold\u201d. In q-SAGA this can be controlled to be n/q.\nSVRG To show the fruitfulness of our framework, we reformulate a variant of Stochastic Variance Reduced Gradient (SVRG) [5]. We use a randomization argument similar to (but much simpler than) the one suggested in [6] and define a real-valued parameter q > 0. In each iteration we sample r \u223c Uniform[0; 1). If r < q/n we perform a complete update of the \u03b1 variables \u03b1+j = f \u2032j(w) (\u2200j), otherwise they are left unchanged. For q = 1 in expectation one parameter is updated per step, which is the same as in SAGA. The main difference is that SAGA always updates exactly one \u03b1i in each iteration, while SVRG occasionally updates all \u03b1 parameters by triggering an additional sweep through the data. One can also see that here \u03b1\u0304 = f \u2032(w). There is an option to not maintain \u03b1 variables explicitly and to save on space by storing only \u03b1\u0304 and the parameter vector w\u0303 at which the last full re-computation happened.\nUniform Memorization Algorithms Motivated by SAGA and SVRG, we define a class of algorithms, which we call memorization algorithms. Memorization algorithms use the \u03b1i variables to keep track of past stochastic gradients, i.e. \u03b1ti = f \u2032 j(w\n\u03c4i), for some iteration \u03c4i < t. Note that for SAGA as well as SVRG i = j. We refer to this special cases as algorithms without sharing. We will go beyond this restriction by considering i 6= j below. It will turn out to be a technical requirement of our analysis that the probability of updating \u03b1i is the same for all i. We call memorization algorithms with this property uniform memorization algorithms.\nN -SAGA We assume that we have structured our data set such that for each data index i, there is a set of neighbors Ni available. Specifically, we propose two variants: (i) The balanced case in\nwhich |Ni| = q (\u2200i). We call this variant qN -SAGA. It is mainly important for the analysis, since it fulfills the requirements of an uniform memorization algorithm. (ii) The case of -neighborhoods where Ni := {j : \u2016xi \u2212 xj\u2016 \u2264 }. which we refer to as N -SAGA. As the analysis will show, the quantity that matters most is the expectation of the squared pairwise distances \u2016f \u2032i(w) \u2212 f \u2032j(w)\u20162. Since computing those explicitly defeats the purpose of sharing stochastic gradients across points, N -SAGA resorts to the heuristic of constructing neighborhoods based on distances in input space. This can be more formally motivated by assuming a Lipschitz condition of the loss with regard to the inputs.\nBased on a neighborhood system, we define a modified version of SAGA updates on selecting i via\n\u03b1+j = { f \u2032i(w) if i \u2208 Nj \u03b1j otherwise\n(3)\nIntuitively, the stochastic gradient computed for the observed or sampled data point (xi, yi) is propagated to those data points (xj , yj) for which i is a neighbor of j.\nComputing Neighborhoods From a practical point of view it is critical to limit the computational overhead of N -SAGA. However, note that we do not need exactness in finding nearest neighbors. Obviously, the closer the neighbors, the better the expected improvements. Yet, one can be opportunistic here and use plain SAGA as a fallback option, e.g. in the N -variant with occasionally trivial neighborhoods Ni = {i}. From a practical point of view: (i) We should create or maintain a candidate set of k data vectors, where k \u2208 o(n), most drastically k \u2208 O(1). The simplest way to do this is via uniform subsampling of k points from the training data. More refined methods are available from the literature on core sets, e.g. [4]. How will decrease with k remains data set dependent though. (ii) We can use locality-sensitive hashing to index these k data points, perhaps even using data-dependent hashing [1], whenever this cost can be amortized. Note also that as SGD is a sequential method, increasing the computational cost in a highly parallelizable way may not affect data throughput."}, {"heading": "3 Analysis", "text": "Primal Recurrence The evolution equation (2) in expectation implies the recurrence E\u2016w+\u2212w\u2217\u20162 = \u2016w \u2212 w\u2217\u20162 \u2212 2\u03b3\u3008f \u2032(w), w \u2212 w\u2217\u3009+ \u03b32E\u2016gi(w)\u20162 . (4)\nFrom here we utilize a number of well-known bounds (see e.g. [3]), which exploit strong convexity of f (wherever \u00b5 appears) as well as Lipschitz continuity of the fi-gradients (wherever L appears):\n\u3008f \u2032i(w), w \u2212 w\u2217\u3009 \u2265 f(w)\u2212 f(w\u2217) + \u00b5 2 \u2016w \u2212 w\u2217\u20162 , (5)\nE\u2016gi(w)\u20162 \u2264 (1 + \u03b2)E\u2016f \u2032i(w)\u2212 f \u2032i(w\u2217)\u20162 \u2212 \u03b2\u2016f \u2032(w)\u20162 + ( 1 + \u03b2\u22121 ) E\u2016\u03b1\u0304i \u2212 f \u2032i(w\u2217)\u20162 (\u03b2 > 0), (6)\n\u2016f \u2032(w)\u20162 \u2265 2\u00b5 (f(w)\u2212 f(w\u2217)) , (7) \u2016f \u2032i(w)\u2212 f \u2032i(w\u2217)\u20162 \u2264 2Lhi(w), hi(w) := fi(w)\u2212 fi(w\u2217) + \u3008w \u2212 w\u2217, f \u2032i(w\u2217)\u3009 , (8) E\u2016f \u2032i(w)\u2212f \u2032i(w\u2217)\u20162 \u2264 2L(f(w)\u2212 f(w\u2217)) , (9) E\u2016\u03b1\u0304i \u2212 f \u2032i(w\u2217))\u20162 = E\u2016\u03b1i \u2212 f \u2032i(w\u2217)\u20162 \u2212 \u2016\u03b1\u0304\u20162 \u2264 E\u2016\u03b1i \u2212 f \u2032i(w\u2217)\u20162 (10)\nBy applying all of these, we can derive a \u03b2-parameterized bound \u2016w\u2212w\u2217\u20162 \u2212E\u2016w+\u2212w\u2217\u20162 \u2265 \u03b3\u00b5\u2016w \u2212 w\u2217\u20162 \u2212 \u03b32 ( 1 + \u03b2\u22121 ) E\u2016\u03b1i \u2212 f \u2032i(w\u2217))\u20162 + 2\u03b3 [1\u2212 \u03b3(L(1 + \u03b2)\u2212 \u00b5\u03b2)] (f(w)\u2212 f(w\u2217)) . (11) Note that in the ideal case of perfect asymptotic variance reduction, where \u03b1i = f \u2032i(w\n\u2217) we can look at the limit of \u03b2 \u2192 0 and would immediately get a condition for a contraction by choosing \u03b3 = 1L , yielding a contraction rate of 1\u2212\u03c1 with \u03c1 = \u03b3\u00b5 = \u00b5L , which is just the condition number. Our main result will show that we are not losing more than a factor of 4 relative to that gold standard.\nExploiting Properties of Uniform Memorization Algorithms How can we further bound E\u2016\u03b1i \u2212 f \u2032i(w\u2217)\u20162 in the case of variance-reducing SGD? A key insights is that for memorization algorithms without sharing, we can apply the same smoothness bound as in Eq. (8)\n\u2016\u03b1i \u2212 f \u2032i(w\u2217)\u20162 \u2264 2Lhi(w\u03c4i) . (12)\nFor the N -SAGA family things get slightly more complicated, but we can apply a similar parametrized bound as before (with \u03c6 > 0) to split the squared norm\n\u2016\u03b1i \u2212 f \u2032i(w\u2217)\u20162 \u2264(1 + \u03c6)\u2016f \u2032i(w\u03c4i)\u2212 f \u2032i(w\u2217)\u20162 + (1 + \u03c6\u22121)\u2016f \u2032j(w\u03c4i)\u2212 f \u2032i(w\u03c4i)\u20162 . (13) The first of these terms is the same as before, only re-scaled by (1+\u03c6). The second term is the error introduced by making use of neighbor information (i.e. sharing). We assume it can be bounded in expectation by\nE\u2016f \u2032j(w)\u2212 f \u2032i(w)\u20162 < \u03b7 (14) where the expectation is over random index pairs {(i, j) : j \u2208 Ni} with regard to the distribution induced by the update rule. We require this bound to hold for each w along the iterate sequence.\nLyapunov Function We want to show that for a suitable choice of the step size \u03b3 each iteration results in a contraction that brings us closer to the optimum, i.e. E\u2016w+\u2212w\u2217\u20162 \u2264 (1\u2212\u03c1)\u2016w\u2212w\u2217\u20162. where 0 < \u03c1 < 1. However, the main challenge arises from the fact that \u03b1i store stochastic gradients from previous iterations, i.e. they constitute quantities that are not evaluated at the current iterate w. This requires a somewhat more complex proof technique. Inspired by the Lyapunov function method in [3] we define upper bounds Hi \u2265 \u2016\u03b1i \u2212 f \u2032i(w\u2217)\u20162 such that Hi \u2192 0 as w \u2192 w\u2217. We (conceptually) initialize Hi = 2Lhi(w0), start with \u03b10i = 0 and then update Hi in synch with \u03b1i,\nH+i := { 2Lhi(w) if \u03b1i is updated Hi otherwise\n(15)\nso that we always maintain valid bounds \u2016\u03b1i \u2212 f \u2032i(w\u2217)\u20162 \u2264 Hi and E\u2016\u03b1i \u2212 f \u2032i(w\u2217)\u20162 \u2264 H\u0304 with H\u0304 := 1n \u2211n i=1Hi. Note that for N -SAGA \u2016\u03b1i \u2212 f \u2032i(w\u2217)\u20162 \u2264 (1 + \u03c6)Hi + (1 + \u03c6\u22121)\u03b7. It should be clear that Hi are quantities showing up in the analysis, but are not computed in the algorithm.\nWe now define a Lyapunov function (which is much simpler than in [3]) L(w,H) = \u2016w \u2212 w\u2217\u20162 + S\u03c3 H\u0304, with S := ( \u03b3n\nLq\n) and 0 < \u03c3 < 1 . (16)\nIn expectation under a random update, the Lyapunov function L changes as EL(w+, H+) = E\u2016w+ \u2212 w\u2217\u20162 + S\u03c3EH\u0304+. The first part is due to the parameter update and we can apply (11) to bound it. The second part is due to the recurrence (15), which mirrors the update of the \u03b1 variables. For the previously defined uniform memorization algorithms we have\nEH\u0304+ = ( n\u2212 q n ) H\u0304 + 2Lq n (f(w)\u2212 f(w\u2217)) (17)\nwhere we have made use of the fact that Ehi(w) = f(w)\u2212 f(w\u2217). Note that for this expectation to work out correctly without further complication requires the mentioned uniformity property. Also note that in expectation the shrinkage does not depend on the location of previous iterates w\u03c4 and the new increment is always proportional to the sub-optimality of the current iterate w.\nGeneral Convergence Analysis Our main contraction result for uniform memorization algorithms (without sharing) can be stated as follows: Theorem 1. Given an instance of an uniform memorization algorithm without sharing as defined above. For any choice of \u03b2 > 0 and 0 < \u03c3 < 1 there is a step size \u03b3 > 0 such that we get a contraction\nL(w,H)\u2212EL(w+, H+) \u2265 \u03c1(\u03c3, \u03b2)L(w,H) with\n\u03c1(\u03c3, \u03b2) := \u00b5\nL min\n{ \u03c3\nR\u03c3 + (1 + \u03b2\u22121) , 1\u2212 \u03c3 1 + \u03b2\n} , where R := n\u00b5\nqL .\nAs it provides a lot of motivation and insights, we carry out the proof in the main text. From Eq. (5), we can see that it may be reasonable to aim for \u03c1 \u2264 \u03b3\u00b5 based on the \u2016w \u2212 w\u2217\u20162 part of L. The question then is, how H\u0304 changes and what constraints that may impose on \u03b3. First of all note that there is a shrinkage of H\u0304 that only depends on q\n4+H := S\u03c3 H\u0304 \u2212 S (n\u2212 q) n \u03c3H\u0304 = S q n \u03c3H\u0304 = \u03b3 L \u03c3H\u0304 . (18)\nBut as we are using Hi to bound \u2016\u03b1i\u2212 f \u2032i(w\u2217)\u20162 as it appears in (11), we need to subtract a suitable term, namely4\u2212H := \u03b32(1+\u03b2\u22121)H\u0304 . Combining the two terms, we pull out S and collect constants:\n4H := 4+H \u22124\u2212H = S\u03c3 ( Lq\nn\n)[ 1\nL \u2212 \u03b3\n( 1 + \u03b2\u22121 ) \u03c3 ] H\u0304 . (19)\nAs we aim for a contraction rate of \u03c1 = \u03b3\u00b5, this leads to a constraint on the step size\n1 L \u2212 \u03b3\n( 1 + \u03b2\u22121 ) \u03c3 \u2265 ( qL n )\u22121 \u00b5\u03b3 = R\u03b3 \u21d0\u21d2 \u03b3 \u2264 1 L \u00b7 \u03c3 R\u03c3 + (1 + \u03b2\u22121) , (20)\nwith R as defined in the claim of the theorem.\nWe next investigate terms that can be bounded by sub-optimality, i.e. f(w)\u2212 f\u2217(w). From Eq. (11) we get a factor 41f := 2\u03b3 [1\u2212 \u03b3(L(1 + \u03b2)\u2212 \u00b5\u03b2)]. However, we also have f(w) \u2212 f(w\u2217) occur in the H\u0304 recurrence in (17). After working through cancelations of the constants one gets a second factor42f := \u22122\u03b3\u03c3. Combining both terms, we require\n41f +42f \u2265 0 \u21d0\u21d2 \u03c3 + \u03b3L(1 + \u03b2) \u2264 1 + \u03b3\u00b5\u03b2 \u21d0\u21d2 \u03b3 \u2264 1\u2212 \u03c3\nL+ \u03b2(L\u2212 \u00b5) . (21)\nHere, we can directly see that \u03c3 < 1 is needed in order to get a positive step size as \u00b5 \u2264 L implies that the denominator is positive. We would like to simplify the bound, which we can get by strengthening as follows:\n1\u2212 \u03c3 L+ \u03b2(L\u2212 \u00b5) \u2265 1\u2212 \u03c3 L(1 + \u03b2) \u2265 \u03b3 (22)\nSo we have derived two bounds, which \u2013 together with the identity \u03c1 = \u00b5\u03b3 \u2013 can be summarized in the claim of the theorem.\nThe theorem provides a two-dimensional family of bounds as \u03b2 > 0 and 0 < \u03c3 < 1 can be chosen arbitrarily. The question is, which choice of constant gives the best (maximal) rate \u03c1\u2217. The optimal choice of \u03c3 is provided by the following corollary. Corollary 1. In Theorem 1, the maximal rate \u03c1\u2217(\u03b2) = \u03c1(\u03b2, \u03c3\u2217) = max\u03c3 \u03c1(\u03c3, \u03b2) is obtained at\n\u03c3\u2217 = 1\n2R\n[ R\u2212 (a+ b) + \u221a R2 + (a+ b)2 \u2212 2R(a\u2212 b) ] (23)\nwhere a = (1 + \u03b2) and b = (1 + \u03b2\u22121).\nProof. We have to equate both expressions in the minimum of Theorem 1, resulting in\na\u03c3 = (R\u03c3 + b)(1\u2212 \u03c3) \u21d0\u21d2 R\u03c32 + (a+ b\u2212R)\u03c3 \u2212 b = 0 (24) Applying the standard formula for quadratic equations provides the claim.\nSolving the resulting bound for \u03b2 yields the best bound. Here, we simply state a specialization to the case of \u03b2 = 1. Corollary 2. The optimal rate is lower bounded by\n\u03c1\u2217 = max \u03b2 \u03c1\u2217(\u03b2) \u2265 \u03c1\u2217(1) = \u00b5 4L 1 + 4 R \u2212 \u221a 1 + ( 4 R )2 (25) Note that the constant R is the ratio of n/q and the condition number \u00b5/L. In the large data case where n/q \u00b5/L, we get the following result. Corollary 3. The optimal contraction rate is guaranteed to be at least\n\u03c1\u2217 \u2265 \u00b5 4L\n( 1 + 4\nR\n) +O ( R\u22122 ) = 1\n4 (\u00b5 L + q n ) +O ( R\u22122 ) (26)\nProof. Set z = 4/R and perform a Taylor approximation of the bound in Corollary 2 around z = 0:\nb(z) := \u00b5\n4L\n[ 1 + z \u2212 \u221a 1 + z2 ] , b\u2032(z) = \u00b5\n4L\n[ 1\u2212 z\u221a\n1 + z2\n] (27)\nApproximating b(z) = \u00b54L + zb \u2032(0) gives the claim.\nThe effect of q on the rate can be elucidated as follows. Using b\u2032(z) as defined in Eq. (27) and noting that z = 4L\u00b5nq we see that b \u2032(q) = 1n [ 1\u2212 z/ \u221a 1 + z2 ] \u2264 1n . Moreover it is easy to see that b\u2032\u2032(q) < 0. Hence the effect of increasing q on the bound in Corollary 2 is at most 1n . If the condition number \u00b5L 1n , improved freshness has a small effect, yet for small \u00b5 (i.e. because of weak regularization) and a regime where \u00b5L \u2248 1n , the rate will increase proportionally to q.\nN -SAGA We now provide results for the N -SAGA algorithm in the (easier to analyze) qN - variant. Taking the above analysis as a starting point, we first need to incorporate the additional penalty term (1 + \u03c6) > 1. So wherever we had (1 + \u03b2\u22121) before, we now will have a factor b = (1 + \u03b2\u22121)(1 + \u03c6), e.g. in Corollary 1.\nMore challenging is the error \u2016f \u2032i(w)\u2212 f \u2032j(w)\u20162 for i \u2208 Nj as it introduces a finite (non-vanishing) bias that can only be controlled indirectly over the granularity of the neighborhoods. What is possible to obtain in this case is a geometric convergence towards a neighborhood of w\u2217. Theorem 2. Let \u03c1 be a rate guaranteed by Theorem 1 for a uniform memorization algorithm without sharing (e.g. q-SAGA). For a fixed w assume that E\u2016f \u2032i(w) \u2212 f \u2032j(w)\u20162 < \u03b7 (\u2200i \u2208 Nj) and define \u03c1\u2032 = (1 \u2212 \u03b6)\u03c1, 0 < \u03b6 < 1. Then for qN -SAGA we have: L(w,H) \u2212 EL(w+, H+) \u2265 \u03c1\u2032L(w,H) as long as \u2016w \u2212 w\u2217\u2016 \u2264 \u03b4 with \u03b4 := 2(1\u2212\u03b6)\u00b5 \u221a \u03b7\u03c1 \u03b6 .\nProof. Based on the assumptions it is straightforward to derive\nL(w,H)\u2212EL(w+, H+) \u2265 \u03c1L(w,H)\u2212 4\u03b32\u03b7 (28) by setting \u03c6 = 1. In order to get a contraction of \u03c1\u2032 = (1 \u2212 \u03b6)\u03c1, it is required that \u03b6\u03c1L(w,H) \u2265 4\u03b32\u03b7. As L(w,H) \u2265 \u2016w \u2212 w\u2217\u20162 and with an adjusted step size \u03b3 = \u03c1\u2032\u00b5 we get as a sufficient condition\n\u03b6\u03c1\u2016w \u2212 w\u2217\u20162 \u2265 4 ( \u03c1\u2032\n\u00b5\n)2 \u03b7 \u21d0\u21d2 \u2016w \u2212 w\u2217\u20162 \u2265 4\u03b7\n\u00b52 (1\u2212 \u03b6)2\u03c1 \u03b6 (29)\nCorollary 4. As a specialization of Theorem 2 we chose \u03b6 = 2 \u2212 \u221a\n3 and get a contraction of \u03c1\u2032 = (1\u2212 2 + \u221a 3)\u03c1 \u2248 0.732 \u03c1 as long as \u2016w \u2212 w\u2217\u2016 \u2265 2\u00b5 \u221a 2\u03b7\u03c1.\nCorollary 5. Under the assumptions of Theorem 2, if for i \u2208 Nj , E\u2016f \u2032i(w) \u2212 f \u2032j(w)\u20162 < \u03b7 holds for all iterates w = wt, then wt converges towards a \u03b4-ball around w\u2217 at a geometric rate of \u03c1\u2032.\nSo the theoretically expected behavior of qN -SAGA is to achieve very fast initial progress towards w\u2217 (for large enough q, faster than SAGA) until the iterate reaches a \u03b4-ball around the optimum. The size of this \u03b4-ball scales with \u221a \u03b7. Within this ball, we either get a random walk behavior or need to switch to a SGD-style schedule for the step size to ensure convergence. Alternatively, we can lower q or ultimately switch to SAGA without sharing (effectively for q = 1), if highly accurate convergence towards the empirical risk minimizer is desired.\nIn practice we have found N -SAGA to behave better than qN -SAGA. This is sensible as the constant neighborhood size in qN -SAGA seems more like a technical requirement than a principled feature."}, {"heading": "4 Experimental Results", "text": "Algorithms We present experimental results on the performance of the different variants of memorization algorithms for variance reduced SGD as discussed in this paper. Since SAGA has shown uniformly superior behavior than SVRG in our experiments and N -SAGA has been almost uniformly superior to qN -SAGA (albeit, sometimes with small differences), we focus on these algorithms alongside with SGD as a straw man and q-SAGA as a point of reference for speed-ups. We have chosen q = 50 in q-SAGA and (based on some crude experimentation) chose such that the average neighborhood size q \u2248 20. The same setting was used across all data sets and experiments.\nData Sets As special cases for the choice of the loss function and regularizer in Eq. (1), we consider two commonly occurring problems in machine learning, namely least-square regression and `2-regularized logistic regression. We apply least-square regression on the the million song year regression from the UCI repository. This dataset contains n = 515, 345 data points, each described by d = 90 input features. We apply logistic regression on the cov and ijcnn1 datasets obtained from the libsvm website 1. The cov dataset contains n = 581, 012 datapoints, each described by d = 54 input features. The ijcnn1 dataset contains n = 49, 990 datapoints, each described by d = 22 input features. We added an `2-regularizer \u2126(w) = \u00b5\u2016w\u201622 with \u00b5 = 10\u22123 to ensure the objective is strongly convex.\nExperimental Protocol We have run the algorithms in question in an i.i.d. sampling setting and averaged the results over 5 runs. Figure 2 shows the evolution of the value of the objective function f as a function of the number of update steps performed. Note that all algorithms compute one stochastic gradient per update step, with the exception of q-SAGA, which is included here not as a practically relevant algorithm, but as an indication of potential improvements that could be achieved by using fresher corrections. A constant step size \u03b3 has been used everywhere, expect for plain SGD. \u03b3 = 10\u22123 was found to be roughly the best in the set {10\u22121, . . . , 10\u22125}. For plain SGD we used a\n1http://www.csie.ntu.edu.tw/\u02dccjlin/libsvmtools/datasets\nschedule of the form \u03b3t = \u03b30/(T0 + t) with constants optimized coarsely via cross-validation. The x-axis is expressed in units of n (suggestively called \u201depochs\u201d).\nSAGA vs. SGD As we can see, if we run SGD with the same constant step size as SAGA, it takes at least 2-4 epochs until SAGA really shows a significant gain. Of course, if the SGD step size is chosen more conservatively, the gains are more significant from the start.\nSAGA vs. q-SAGA q-SAGA outperforms plain SAGA quite consistently when counting stochastic update steps. This establishes optimistic reference curves of what we can expect to achieve with N -SAGA. The actual speed-up is somewhat data set dependent.\nN -SAGA vs. SAGA and q-SAGA N -SAGA can realize quite a fraction of the possible gains and typically traces nicely between the SAGA and q-SAGA curves. On cov we see solid speed-ups in epoch 1 and 2, which then start wearing off. The same is true for ijcnn1. On year the differences are less significant, but note that here SAGA has only a small edge on SGD with constant step size. At least the difference between N -SAGA and SAGA are bigger than those between SAGA and SGD.\nAsymptotics It should be clearly stated that running N -SAGA at a fixed for longer will not result in good asymptotics on the empirical risk. In our experiments, the cross-over point with SAGA was typically after 5 \u2212 10 epochs. Note that the gains are quite significant though for the single epoch learning. We have found very similar results in a streaming setting, i.e. presenting the data once in random order.\nGeneralization Error Although our analysis was carried out purely for the empirical loss, it is important to also take a look at the expected risk on a test set (as a proxy for generalization performance). Here the curves are somewhat more noisy. On cov and ijcnn1 N -SAGA shows some improvements in early epochs, however on year we get a less satisfying result. Note that here already q-SAGA fails."}, {"heading": "5 Conclusion", "text": "We proposed a novel analysis method for variance reducing SGD methods that demonstrates geometric convergence rates and provides a number of new insights, in particular about the role of the freshness of stochastic gradients evaluated at previous iterates. We have also investigated the effect of additional errors in the variance correction terms on the convergence behavior. Motivated by this, we have proposed N -SAGA, a modification of SAGA that can achieve consistent and at times significant speed-ups in the initial phase of the optimization. Most remarkably, this algorithm can be run in a streaming mode, which is \u2013 to our knowledge \u2013 the first of its kind within the family of variance reduced methods."}], "references": [{"title": "Optimal data-dependent hashing for approximate near neighbors", "author": ["A. Andoni", "I. Razenshteyn"], "venue": "arXiv preprint arXiv:1501.01062,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "In COMPSTAT,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["A. Defazio", "F. Bach", "S. Lacoste-Julien"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Scalable training of mixture models via coresets", "author": ["D. Feldman", "M. Faulkner", "A. Krause"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Semi-stochastic gradient descent methods", "author": ["J. Kone\u010dn\u1ef3", "P. Richt\u00e1rik"], "venue": "arXiv preprint arXiv:1312.1666,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The annals of mathematical statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1951}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["M. Schmidt", "N.L. Roux", "F. Bach"], "venue": "arXiv preprint arXiv:1309.2388,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical programming,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "Stochastic gradient descent (SGD) is a popular alternative, in particular in the context of large-scale learning [2, 9].", "startOffset": 113, "endOffset": 119}, {"referenceID": 8, "context": "Stochastic gradient descent (SGD) is a popular alternative, in particular in the context of large-scale learning [2, 9].", "startOffset": 113, "endOffset": 119}, {"referenceID": 9, "context": "It is a surprising recent finding [10, 5, 8, 6] that the additive structure of f allows for significantly faster convergence in expectation.", "startOffset": 34, "endOffset": 47}, {"referenceID": 4, "context": "It is a surprising recent finding [10, 5, 8, 6] that the additive structure of f allows for significantly faster convergence in expectation.", "startOffset": 34, "endOffset": 47}, {"referenceID": 7, "context": "It is a surprising recent finding [10, 5, 8, 6] that the additive structure of f allows for significantly faster convergence in expectation.", "startOffset": 34, "endOffset": 47}, {"referenceID": 5, "context": "It is a surprising recent finding [10, 5, 8, 6] that the additive structure of f allows for significantly faster convergence in expectation.", "startOffset": 34, "endOffset": 47}, {"referenceID": 6, "context": "While the classic convergence proof of SGD requires vanishing step sizes, typically at a rate of O(1/t) [7],", "startOffset": 104, "endOffset": 107}, {"referenceID": 2, "context": "SAGA The SAGA algorithm [3] maintains variance corrections \u03b1i after selecting data point i by memorizing stochastic gradients.", "startOffset": 24, "endOffset": 27}, {"referenceID": 4, "context": "SVRG To show the fruitfulness of our framework, we reformulate a variant of Stochastic Variance Reduced Gradient (SVRG) [5].", "startOffset": 120, "endOffset": 123}, {"referenceID": 5, "context": "We use a randomization argument similar to (but much simpler than) the one suggested in [6] and define a real-valued parameter q > 0.", "startOffset": 88, "endOffset": 91}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "(ii) We can use locality-sensitive hashing to index these k data points, perhaps even using data-dependent hashing [1], whenever this cost can be amortized.", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "[3]), which exploit strong convexity of f (wherever \u03bc appears) as well as Lipschitz continuity of the fi-gradients (wherever L appears): \u3008f \u2032 i(w), w \u2212 w\u2217\u3009 \u2265 f(w)\u2212 f(w\u2217) + \u03bc 2 \u2016w \u2212 w\u2217\u20162 , (5) E\u2016gi(w)\u2016 \u2264 (1 + \u03b2)E\u2016f \u2032 i(w)\u2212 f \u2032 i(w)\u2016 \u2212 \u03b2\u2016f \u2032(w)\u20162 + ( 1 + \u03b2\u22121 ) E\u2016\u1fb1i \u2212 f \u2032 i(w)\u2016 (\u03b2 > 0), (6) \u2016f \u2032(w)\u20162 \u2265 2\u03bc (f(w)\u2212 f(w\u2217)) , (7) \u2016f \u2032 i(w)\u2212 f \u2032 i(w)\u2016 \u2264 2Lhi(w), hi(w) := fi(w)\u2212 fi(w) + \u3008w \u2212 w\u2217, f \u2032 i(w)\u3009 , (8) E\u2016f \u2032 i(w)\u2212f \u2032 i(w)\u2016 \u2264 2L(f(w)\u2212 f(w\u2217)) , (9) E\u2016\u1fb1i \u2212 f \u2032 i(w))\u2016 = E\u2016\u03b1i \u2212 f \u2032 i(w)\u2016 \u2212 \u2016\u1fb1\u2016 \u2264 E\u2016\u03b1i \u2212 f \u2032 i(w)\u2016 (10)", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Inspired by the Lyapunov function method in [3] we define upper bounds Hi \u2265 \u2016\u03b1i \u2212 f \u2032 i(w)\u2016 such that Hi \u2192 0 as w \u2192 w\u2217.", "startOffset": 44, "endOffset": 47}, {"referenceID": 2, "context": "We now define a Lyapunov function (which is much simpler than in [3])", "startOffset": 65, "endOffset": 68}], "year": 2017, "abstractText": "Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet it is also known to be slow relative to steepest descent. The variance in the stochastic update directions only allows for sublinear or (with iterate averaging) linear convergence rates. Recently, variance reduction techniques such as SVRG and SAGA have been proposed to overcome this weakness. With asymptotically vanishing variance, a constant step size can be maintained, resulting in geometric convergence rates. However, these methods are either based on occasional computations of full gradients at pivot points (SVRG), or on keeping per data point corrections in memory (SAGA). This has the disadvantage that one cannot employ these methods in a streaming setting and that speed-ups relative to SGD may need a certain number of epochs in order to materialize. This paper investigates a new class of algorithms that can exploit neighborhood structure in the training data to share and re-use information about past stochastic gradients across data points. While not meant to be offering advantages in an asymptotic setting, there are significant benefits in the transient optimization phase, in particular in a streaming or singleepoch setting. We investigate this family of algorithms in a thorough analysis and show supporting experimental results. As a side-product we provide a simple and unified proof technique for a broad class of variance reduction algorithms.", "creator": "LaTeX with hyperref package"}}}