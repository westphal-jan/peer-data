{"id": "1506.03694", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2015", "title": "Learning language through pictures", "abstract": "We propose Imaginet, a model of learning visually grounded representations of language from coupled textual and visual input. The model consists of two Gated Recurrent Unit networks with shared word embeddings, and uses a multi-task objective by receiving a textual description of a scene and trying to concurrently predict its visual representation and the next word in the sentence. Like humans, it acquires meaning representations for individual words from descriptions of visual scenes, and the task of processing a corpus of words is related to the visual experience as well as a corpus of images. Although these representations can provide an excellent learning framework, it can be confusing to recognize the specific language that an individual uses for the task of processing or processing. We recommend implementing Imaginet, a model of learning visually grounded representations of language from combined textual and visual input.", "histories": [["v1", "Thu, 11 Jun 2015 14:45:49 GMT  (3719kb,D)", "https://arxiv.org/abs/1506.03694v1", "To appear at ACL 2015"], ["v2", "Fri, 19 Jun 2015 18:56:57 GMT  (3721kb,D)", "http://arxiv.org/abs/1506.03694v2", "To appear at ACL 2015"]], "COMMENTS": "To appear at ACL 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["grzegorz chrupala", "\u00e1kos k\u00e1d\u00e1r", "afra alishahi"], "accepted": true, "id": "1506.03694"}, "pdf": {"name": "1506.03694.pdf", "metadata": {"source": "META", "title": "Learning language through pictures", "authors": ["Grzegorz Chrupa\u0142a", "\u00c1kos K\u00e1d\u00e1r", "Afra Alishahi"], "emails": ["g.chrupala@uvt.nl", "a.kadar@uvt.nl", "a.alishahi@uvt.nl"], "sections": [{"heading": "1 Introduction", "text": "Vision is the most important sense for humans and visual sensory input plays an important role in language acquisition by grounding meanings of words and phrases in perception. Similarly, in practical applications processing multimodal data where text is accompanied by images or videos is increasingly important. In this paper we propose a novel model of learning visually-grounded representations of language from paired textual and visual input. The model learns language through comprehension and production, by receiving a textual description of a scene and trying to \u201cimagine\u201d a visual representation of it, while predicting the next word at the same time.\nThe full model, which we dub IMAGINET, consists of two Gated Recurrent Unit (GRU) networks coupled via shared word embeddings. IMAGINET uses a multi-task Caruana (1997) objective: both networks read the sentence word-by-word in parallel; one of them predicts the feature representation of the image depicting the described scene\nafter reading the whole sentence, while the other one predicts the next word at each position in the word sequence. The importance of the visual and textual objectives can be traded off, and either of them can be switched off entirely, enabling us to investigate the impact of visual vs textual information on the learned language representations.\nOur approach to modeling human language learning has connections to recent models of image captioning (see Section 2). Unlike in many of these models, in IMAGINET the image is the target to predict rather then the input, and the model can build a visually-grounded representation of a sentence independently of an image. We can directly compare the performance of IMAGINET against a simple multivariate linear regression model with bag-of-words features and thus quantify the contribution of the added expressive power of a recurrent neural network.\nWe evaluate our model\u2019s knowledge of word meaning and sentence structure through simulating human judgments of word similarity, retrieving images corresponding to single words as well as full sentences, and retrieving paraphrases of image captions. In all these tasks the model outperforms the baseline; the model significantly correlates with human ratings of word similarity, and predicts appropriate visual interpretations of single and multi-word phrases. The acquired knowledge of sentence structure boosts the model\u2019s performance in both image and caption retrieval."}, {"heading": "2 Related work", "text": "Several computational models have been proposed to study early language acquisition. The acquisition of word meaning has been mainly modeled using connectionist networks that learn to associate word forms with semantic or perceptual features (e.g., Li et al., 2004; Coventry et al., 2005; Regier, 2005), and rule-based or probabilistic implementations which use statistical reg-\nar X\niv :1\n50 6.\n03 69\n4v 2\n[ cs\n.C L\n] 1\n9 Ju\nn 20\n15\nularities observed in the input to detect associations between linguistic labels and visual features or concepts (e.g., Siskind, 1996; Yu, 2008; Fazly et al., 2010). These models either use toy languages as input (e.g., Siskind, 1996), or childdirected utterances from the CHILDES database (MacWhinney, 2014) paired with artificially generated semantic information. Some models have investigated the acquisition of terminology for visual concepts from simple videos (Fleischman and Roy, 2005; Skocaj et al., 2011). Lazaridou et al. (2015) adapt the skip-gram word-embedding model (Mikolov et al., 2013) for learning word representations via a multi-task objective similar to ours, learning from a dataset where some words are individually aligned with corresponding images. All these models ignore sentence structure and treat inputs as bags of words.\nA few models have looked at the concurrent acquisition of words and some aspect of sentence structure, such as lexical categories (Alishahi and Chrupa\u0142a, 2012) or syntactic properties (Howell et al., 2005; Kwiatkowski et al., 2012), from utterances paired with an artificially generated representation of their meaning. To our knowledge, no existing model has been proposed for concurrent learning of grounded word meanings and sentence structure from large scale data and realistic visual input.\nRecently, the engineering task of generating captions for images has received a lot of attention (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2014; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2014). From the point of view of modeling, the research most relevant to our interests is that of Chen and Zitnick (2014). They develop a model based on a contextdependent recurrent neural network (Mikolov and Zweig, 2012) which simultaneously processes textual and visual input and updates two parallel hidden states. Unlike theirs, our model receives the visual target only at the end of the sentence and is thus encouraged to store in the final hidden state of the visual pathway all aspects of the sentence needed to predict the image features successfully. Our setup is more suitable for the goal of learning representations of complete sentences."}, {"heading": "3 Models", "text": "IMAGINET consists of two parallel recurrent path-\nways coupled via shared word embeddings. Both pathways are composed of Gated Recurrent Units (GRU) first introduced by Cho et al. (2014) and Chung et al. (2014). GRUs are related to the Long Short-Term Memory units (Hochreiter and Schmidhuber, 1997), but do not employ a separate memory cell. In a GRU, activation at time t is the linear combination of previous activation, and candidate activation:\nht = (1\u2212 zt) ht\u22121 + zt h\u0303t (1)\nwhere is elementwise multiplication. The update gate determines how much the activation is updated:\nzt = \u03c3s(Wzxt +Uzht\u22121) (2)\nThe candidate activation is computed as:\nh\u0303t = \u03c3(Wxt +U(rt ht\u22121)) (3)\nThe reset gate is defined as:\nrt = \u03c3s(Wrxt +Urht\u22121) (4)\nOur gated recurrent units use steep sigmoids for gate activations:\n\u03c3s(z) = 1\n1 + exp(\u22123.75z)\nand rectified linear units clipped between 0 and 5 for the unit activations:\n\u03c3(z) = clip(0.5(z + abs(z)), 0, 5)\nFigure 1 illustrates the structure of the network. The word embeddings is a matrix of learned parameters We with each column corresponding to a vector for a particular word. The input word symbol St of sentence S at each step t indexes into the embeddings matrix and the vector xt forms input to both GRU networks:\nxt = We[:, St] (5)\nThis input is mapped into two parallel hidden states, hVt along the visual pathway, and h T t along the textual pathway:\nhVt = GRU V (hVt\u22121,xt) (6) hTt = GRU T (hTt\u22121,xt) (7)\nThe final hidden state along the visual pathway hV\u03c4 is then mapped to the predicted target image representation i\u0302 by the fully connected layer with parameters V and the clipped rectifier activation:\ni\u0302 = \u03c3(VhV\u03c4 ) (8)\nEach hidden state along the textual pathway hTt is used to predict the next symbol in the sentence S via a softmax layer with parameters L:\np(St+1|S1:t) = softmax(LhTt ) (9) The loss function whose gradient is backpropagated through time to the GRUs and the embeddings is a composite objective with terms penalizing error on the visual and the textual targets simultaneously:\nL(\u03b8) = \u03b1LT (\u03b8) + (1\u2212 \u03b1)LV (\u03b8) (10)\nwhere \u03b8 is the set of all IMAGINET parameters. LT is the cross entropy function:\nLT (\u03b8) = \u22121 \u03c4\n\u03c4\u2211\nt=1\nlog p(St|S1:t) (11)\nwhile LV is the mean squared error:\nLV (\u03b8) = 1\nK\nK\u2211\nk=1\n(\u0302ik \u2212 ik)2 (12)\nBy setting \u03b1 to 0 we can switch the whole textual pathway off and obtain the VISUAL model variant. Analogously, setting \u03b1 to 1 gives the TEXTUAL model. Intermediate values of \u03b1 (in the experiments below we use 0.1) give the full MULTITASK version. Finally, as baseline for some of the tasks we use a simple linear regression model LINREG with a bag-of-words representation of the sentence:\ni\u0302 = Ax+ b (13)\nwhere i\u0302 is the vector of the predicted image features, x is the vector of word counts for the input sentence and (A, b) the parameters of the linear model estimated via L2-penalized sum-ofsquared-errors loss."}, {"heading": "4 Experiments", "text": "Settings The model was implemented in Theano (Bastien et al., 2012; Bergstra et al., 2010) and optimized by Adam (Kingma and Ba, 2014).1 The fixed 4096-dimensional target image representation come from the pre-softmax layer of the 16- layer CNN (Simonyan and Zisserman, 2014). We used 1024 dimensions for the embeddings and for the hidden states of each of the GRU networks. We ran 8 iterations of training, and we report either full learning curves, or the results for each model after iteration 7 (where they performed best for the image retrieval task). For training we use the standard MS-COCO training data. For validation and test, we take a sample of 5000 images each from the validation data."}, {"heading": "4.1 Word representations", "text": "We assess the quality of the learned embeddings for single words via two tasks: (i) we measure similarity between embeddings of word pairs and compare them to elicited human ratings; (ii) we examine how well the model learns visual representations of words by projecting word embeddings into the visual space, and retrieving images of single concepts from ImageNet.\nWord similarity judgment For similarity judgment correlations, we selected two existing benchmarks that have the largest vocabulary overlap with our data: MEN 3K (Bruni et al., 2014) and SimLex-999 (Hill et al., 2014). We measure the similarity between word pairs by computing the cosine similarity between their embeddings from three versions of our model, VISUAL, MULTITASK and TEXTUAL, and the baseline LINREG.\nTable 1 summarizes the results. All IMAGINET models significantly correlate with human similarity judgments, and outperform LINREG. Examples of word pairs for which MULTITASK cap-\n1Code available at github.com/gchrupala/imaginet.\ntures human similarity judgments better than VISUAL include antonyms (dusk, dawn), collocations (sexy, smile), or related but not visually similar words (college, exhibition).\nSingle-word image retrieval In order to visualize the acquired meaning for individual words, we use images from the ILSVRC2012 subset of ImageNet (Russakovsky et al., 2014) as benchmark. Labels of the images in ImageNet are synsets from WordNet, which identify a single concept in the image rather than providing descriptions of its full content. Since the synset labels in ImageNet are much more precise than the descriptions provided in the captions in our training data (e.g., elkhound), we use synset hypernyms from WordNet as substitute labels when the original labels are not in our vocabulary.\nWe extracted the features from the 50,000 images of the ImageNet validation set. The labels in this set result in 393 distinct (original or hypernym) words from our vocabulary. Each word was projected to the visual space by feeding it through the model as a one-word sentence. We ranked the vectors corresponding to all 50,000 images based on their similarity to the predicted vector, and measured the accuracy of retrieving an image with the correct label among the top 5 ranked images (Accuracy@5). Table 2 summarizes the results: VISUAL and MULTITASK learn more accurate word meaning representations than LINREG."}, {"heading": "4.2 Sentence structure", "text": "In the following experiments, we examine the knowledge of sentence structure learned by IMAGINET, and its impact on the model performance on image and paraphrase retrieval.\nImage retrieval We retrieve images based on the similarity of their vectors with those predicted by IMAGINET in two conditions: sentences are fed to the model in their original order, or scrambled. Figure 2 (left) shows the proportion of sentences for which the correct image was in the top 5 highest ranked images for each model, as a function of the number of training iterations: both models out-\nperform the baseline. MULTITASK is initially better in retrieving the correct image, but eventually the gap disappears. Both models perform substantially better when tested on the original captions compared to the scrambled ones, indicating that models learn to exploit aspects of sentence structure. This ability is to be expected for MULTITASK, but the VISUAL model shows a similar effect to some extent. In the case of VISUAL, this sensitivity to structural aspects of sentence meaning is entirely driven by how they are reflected in the image, as this models only receives the visual supervision signal.\nQualitative analysis of the role of sequential structure suggests that the models are sensitive to the fact that periods terminate a sentence, that sentences tend not to start with conjunctions, that topics appear in sentence-initial position, and that words have different importance as modifiers versus heads. Figure 3 shows an example; see supplementary material for more.\nIMAGINET vs captioning systems While it is not our goal to engineer a state-of-the-art image retrieval system, we want to situate IMAGINET\u2019s performance within the landscape of image retrieval results on captioned images. As most of these are on Flickr30K (Young et al., 2014), we ran MULTITASK on it and got an accuracy@5 of 32%, within the range of numbers reported in previous work: 29.8% (Socher et al., 2014), 31.2% (Mao et al., 2014), 34% (Kiros et al., 2014) and 37.7% (Karpathy and Fei-Fei, 2014). Karpathy and Fei-Fei (2014) report 29.6% on MS-COCO, but with additional training data.\nParaphrase retrieval In our dataset each image is paired with five different captions, which can be seen as paraphrases. This affords us the opportunity to test IMAGINET\u2019s sentence representations on a non-visual task. Although all models receive one caption-image pair at a time, the co-occurrence with the same image can lead the model to learn structural similarities between captions that are different on the surface. We feed the whole set of validation captions through the trained model and record the final hidden visual state hV\u03c4 . For each caption we rank all others according to cosine similarity and measure the proportion of the ones associated with the same image among the top four highest ranked. For the scrambled condition, we rank original captions against a scrambled one. Figure 2 (right) summarizes the results: both models outperform the baseline on ordered captions, but not on scrambled ones. As expected, MULTITASK is more affected by manipulating word order, because it is more sensitive to\nstructure. Table 3 shows concrete examples of the effect of scrambling words in what sentences are retrieved."}, {"heading": "5 Discussion", "text": "IMAGINET is a novel model of grounded language acquisition which simultaneously learns word meaning representations and knowledge of sentence structure from captioned images. It acquires meaning representations for individual words from descriptions of visual scenes, mimicking an important aspect of human language learning, and can effectively use sentence structure in semantic interpretation of multi-word phrases. In future we plan to upgrade the current wordprediction pathway to a sentence reconstruction and/or sentence paraphrasing task in order to encourage the formation of representations of full sentences. We also want to explore the acquired structure further, especially for generalizing the grounded meanings to those words for which visual data is not available."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Angeliki Lazaridou and Marco Baroni for their many insightful comments on the research presented in this paper."}, {"heading": "B Effect of scrambling word order", "text": "In Figures 5\u20137 we show some illustrative cases of the effect for image retrieval of scrambling the input captions to the MULTITASK model trained on un-scrambled ones. These examples suggest that the model learns a number of facts about sentence structure. They range from very obvious, e.g. periods terminate sentences, to quite interesting, such as the distinction between modifiers and heads or the role of word order in encoding information structure (i.e. the distinction between topic and comment)."}, {"heading": "C Propagating distributional information through Multi-Task objective", "text": "Table 4 lists example word pairs for which the MULTITASK model matches human judgments closer than the VISUAL model. Some interesting cases are words which are closely related but which have the opposite meaning (dawn, dusk), or words which denote entities from the same broad class, but which are visually very dissimilar (insect, lizard). There are, however, also examples where there is no obvious prior expectation for the MULTITASK model to do better, e.g. (maple, oak)."}], "references": [{"title": "Concurrent acquisition of word meaning and lexical categories", "author": ["Afra Alishahi", "Grzegorz Chrupa\u0142a."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language", "citeRegEx": "Alishahi and Chrupa\u0142a.,? 2012", "shortCiteRegEx": "Alishahi and Chrupa\u0142a.", "year": 2012}, {"title": "Theano: new features and speed improvements", "author": ["eron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop", "citeRegEx": "eron et al\\.,? \\Q2012\\E", "shortCiteRegEx": "eron et al\\.", "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio."], "venue": "Proceedings", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Multimodal distributional semantics", "author": ["Elia Bruni", "Nam-Khanh Tran", "Marco Baroni."], "venue": "Journal of Artificial Intelligence Research (JAIR), 49:1\u201347.", "citeRegEx": "Bruni et al\\.,? 2014", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Multitask learning", "author": ["Rich Caruana."], "venue": "Machine learning, 28(1):41\u201375.", "citeRegEx": "Caruana.,? 1997", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["Xinlei Chen", "C Lawrence Zitnick."], "venue": "arXiv preprint arXiv:1411.5654.", "citeRegEx": "Chen and Zitnick.,? 2014", "shortCiteRegEx": "Chen and Zitnick.", "year": 2014}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8).", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "NIPS 2014 Deep Learning and Representation Learning Workshop.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Spatial prepositions and vague quantifiers: Implementing the functional geometric framework", "author": ["Kenny R. Coventry", "Angelo Cangelosi", "Rohanna Rajapakse", "Alison Bacon", "Stephen Newstead", "Dan Joyce", "Lynn V. Richards."], "venue": "Christian Freksa,", "citeRegEx": "Coventry et al\\.,? 2005", "shortCiteRegEx": "Coventry et al\\.", "year": 2005}, {"title": "Longterm recurrent convolutional networks for visual recognition and description", "author": ["Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell."], "venue": "arXiv preprint", "citeRegEx": "Donahue et al\\.,? 2014", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "From captions to visual concepts and back. arXiv preprint arXiv:1411.4952", "author": ["Hao Fang", "Saurabh Gupta", "Forrest Iandola", "Rupesh Srivastava", "Li Deng", "Piotr Doll\u00e1r", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John Platt"], "venue": null, "citeRegEx": "Fang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fang et al\\.", "year": 2014}, {"title": "A probabilistic computational model of cross-situational word learning", "author": ["Afsaneh Fazly", "Afra Alishahi", "Suzanen Stevenson."], "venue": "Cognitive Science: A Multidisciplinary Journal, 34(6):1017\u20131063.", "citeRegEx": "Fazly et al\\.,? 2010", "shortCiteRegEx": "Fazly et al\\.", "year": 2010}, {"title": "Intentional context in situated natural language learning", "author": ["Michael Fleischman", "Deb Roy."], "venue": "Proceedings of the Ninth Conference on Computational Natural Language Learning, pages 104\u2013111. Association for Computational Linguistics.", "citeRegEx": "Fleischman and Roy.,? 2005", "shortCiteRegEx": "Fleischman and Roy.", "year": 2005}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "arXiv preprint arXiv:1408.3456.", "citeRegEx": "Hill et al\\.,? 2014", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "A model of grounded language acquisition: Sensorimotor features improve lexical and grammatical learning", "author": ["Steve R Howell", "Damian Jankowicz", "Suzanna Becker."], "venue": "Journal of Memory and Language, 53(2):258\u2013276.", "citeRegEx": "Howell et al\\.,? 2005", "shortCiteRegEx": "Howell et al\\.", "year": 2005}, {"title": "Deep visualsemantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei."], "venue": "arXiv preprint arXiv:1412.2306.", "citeRegEx": "Karpathy and Fei.Fei.,? 2014", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR, abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S Zemel."], "venue": "arXiv preprint arXiv:1411.2539.", "citeRegEx": "Kiros et al\\.,? 2014", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "A probabilistic model of syntactic and semantic acquisition from child-directed utterances and their meanings", "author": ["Tom Kwiatkowski", "Sharon Goldwater", "Luke Zettlemoyer", "Mark Steedman."], "venue": "Proceedings of the 13th Conference of the European", "citeRegEx": "Kwiatkowski et al\\.,? 2012", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2012}, {"title": "Combining language and vision with a multimodal skip-gram model", "author": ["Angeliki Lazaridou", "Nghia The Pham", "Marco Baroni."], "venue": "Proceedings of NAACL HLT 2015 (2015 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Lazaridou et al\\.,? 2015", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2015}, {"title": "Early lexical development in a self-organizing neural network", "author": ["Ping Li", "Igor Farkas", "Brian MacWhinney."], "venue": "Neural Networks, 17:1345\u20131362.", "citeRegEx": "Li et al\\.,? 2004", "shortCiteRegEx": "Li et al\\.", "year": 2004}, {"title": "The CHILDES project: Tools for analyzing talk, Volume I: Transcription format and programs", "author": ["Brian MacWhinney."], "venue": "Psychology Press.", "citeRegEx": "MacWhinney.,? 2014", "shortCiteRegEx": "MacWhinney.", "year": 2014}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan L Yuille."], "venue": "NIPS 2014 Deep Learning Workshop.", "citeRegEx": "Mao et al\\.,? 2014", "shortCiteRegEx": "Mao et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositional", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Context dependent recurrent neural network language model", "author": ["Tomas Mikolov", "Geoffrey Zweig."], "venue": "SLT, pages 234\u2013239.", "citeRegEx": "Mikolov and Zweig.,? 2012", "shortCiteRegEx": "Mikolov and Zweig.", "year": 2012}, {"title": "The emergence of words: Attentional learning in form and meaning", "author": ["Terry Regier."], "venue": "Cognitive Science: A Multidisciplinary Journal, 29:819\u2013865.", "citeRegEx": "Regier.,? 2005", "shortCiteRegEx": "Regier.", "year": 2005}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman."], "venue": "CoRR, abs/1409.1556.", "citeRegEx": "Simonyan and Zisserman.,? 2014", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "A computational study of cross-situational techniques for learning word-tomeaning mappings", "author": ["Jeffrey M. Siskind."], "venue": "Cognition, 61(1-2):39\u201391.", "citeRegEx": "Siskind.,? 1996", "shortCiteRegEx": "Siskind.", "year": 1996}, {"title": "A system for interactive learning in dialogue with a tutor", "author": ["Danijel Skocaj", "Matej Kristan", "Alen Vrecko", "Marko Mahnic", "Miroslav Janicek", "Geert-Jan M Kruijff", "Marc Hanheide", "Nick Hawes", "Thomas Keller", "Michael Zillich"], "venue": null, "citeRegEx": "Skocaj et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Skocaj et al\\.", "year": 2011}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng."], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Socher et al\\.,? 2014", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["Subhashini Venugopalan", "Huijuan Xu", "Jeff Donahue", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko."], "venue": "arXiv preprint arXiv:1412.4729.", "citeRegEx": "Venugopalan et al\\.,? 2014", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "arXiv preprint arXiv:1411.4555.", "citeRegEx": "Vinyals et al\\.,? 2014", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."], "venue": "Transactions of the Association for Computational Linguis-", "citeRegEx": "Young et al\\.,? 2014", "shortCiteRegEx": "Young et al\\.", "year": 2014}, {"title": "A statistical associative account of vocabulary growth in early word learning", "author": ["Chen Yu."], "venue": "Language Learning and Development, 4(1):32\u201362.", "citeRegEx": "Yu.,? 2008", "shortCiteRegEx": "Yu.", "year": 2008}], "referenceMentions": [{"referenceID": 4, "context": "IMAGINET uses a multi-task Caruana (1997) objective: both networks read the sentence word-by-word in parallel; one of them predicts the feature representation of the image depicting the described scene after reading the whole sentence, while the other one predicts the next word at each position in the word sequence.", "startOffset": 27, "endOffset": 42}, {"referenceID": 8, "context": "The acquisition of word meaning has been mainly modeled using connectionist networks that learn to associate word forms with semantic or perceptual features (e.g., Li et al., 2004; Coventry et al., 2005; Regier, 2005), and rule-based or probabilistic implementations which use statistical regar X iv :1 50 6.", "startOffset": 157, "endOffset": 217}, {"referenceID": 26, "context": "The acquisition of word meaning has been mainly modeled using connectionist networks that learn to associate word forms with semantic or perceptual features (e.g., Li et al., 2004; Coventry et al., 2005; Regier, 2005), and rule-based or probabilistic implementations which use statistical regar X iv :1 50 6.", "startOffset": 157, "endOffset": 217}, {"referenceID": 35, "context": "ularities observed in the input to detect associations between linguistic labels and visual features or concepts (e.g., Siskind, 1996; Yu, 2008; Fazly et al., 2010).", "startOffset": 113, "endOffset": 164}, {"referenceID": 11, "context": "ularities observed in the input to detect associations between linguistic labels and visual features or concepts (e.g., Siskind, 1996; Yu, 2008; Fazly et al., 2010).", "startOffset": 113, "endOffset": 164}, {"referenceID": 22, "context": ", Siskind, 1996), or childdirected utterances from the CHILDES database (MacWhinney, 2014) paired with artificially generated semantic information.", "startOffset": 72, "endOffset": 90}, {"referenceID": 12, "context": "Some models have investigated the acquisition of terminology for visual concepts from simple videos (Fleischman and Roy, 2005; Skocaj et al., 2011).", "startOffset": 100, "endOffset": 147}, {"referenceID": 30, "context": "Some models have investigated the acquisition of terminology for visual concepts from simple videos (Fleischman and Roy, 2005; Skocaj et al., 2011).", "startOffset": 100, "endOffset": 147}, {"referenceID": 24, "context": "(2015) adapt the skip-gram word-embedding model (Mikolov et al., 2013) for learning word representations via a multi-task objective similar to ours, learning from a dataset where some words are individually aligned with corresponding images.", "startOffset": 48, "endOffset": 70}, {"referenceID": 11, "context": ", Siskind, 1996; Yu, 2008; Fazly et al., 2010). These models either use toy languages as input (e.g., Siskind, 1996), or childdirected utterances from the CHILDES database (MacWhinney, 2014) paired with artificially generated semantic information. Some models have investigated the acquisition of terminology for visual concepts from simple videos (Fleischman and Roy, 2005; Skocaj et al., 2011). Lazaridou et al. (2015) adapt the skip-gram word-embedding model (Mikolov et al.", "startOffset": 27, "endOffset": 421}, {"referenceID": 0, "context": "A few models have looked at the concurrent acquisition of words and some aspect of sentence structure, such as lexical categories (Alishahi and Chrupa\u0142a, 2012) or syntactic properties (Howell et al.", "startOffset": 130, "endOffset": 159}, {"referenceID": 15, "context": "A few models have looked at the concurrent acquisition of words and some aspect of sentence structure, such as lexical categories (Alishahi and Chrupa\u0142a, 2012) or syntactic properties (Howell et al., 2005; Kwiatkowski et al., 2012), from utterances paired with an artificially generated representation of their meaning.", "startOffset": 184, "endOffset": 231}, {"referenceID": 19, "context": "A few models have looked at the concurrent acquisition of words and some aspect of sentence structure, such as lexical categories (Alishahi and Chrupa\u0142a, 2012) or syntactic properties (Howell et al., 2005; Kwiatkowski et al., 2012), from utterances paired with an artificially generated representation of their meaning.", "startOffset": 184, "endOffset": 231}, {"referenceID": 16, "context": "Recently, the engineering task of generating captions for images has received a lot of attention (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2014; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2014).", "startOffset": 97, "endOffset": 276}, {"referenceID": 23, "context": "Recently, the engineering task of generating captions for images has received a lot of attention (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2014; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2014).", "startOffset": 97, "endOffset": 276}, {"referenceID": 18, "context": "Recently, the engineering task of generating captions for images has received a lot of attention (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2014; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2014).", "startOffset": 97, "endOffset": 276}, {"referenceID": 9, "context": "Recently, the engineering task of generating captions for images has received a lot of attention (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2014; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2014).", "startOffset": 97, "endOffset": 276}, {"referenceID": 33, "context": "Recently, the engineering task of generating captions for images has received a lot of attention (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2014; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2014).", "startOffset": 97, "endOffset": 276}, {"referenceID": 32, "context": "Recently, the engineering task of generating captions for images has received a lot of attention (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2014; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2014).", "startOffset": 97, "endOffset": 276}, {"referenceID": 5, "context": "Recently, the engineering task of generating captions for images has received a lot of attention (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2014; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2014).", "startOffset": 97, "endOffset": 276}, {"referenceID": 10, "context": "Recently, the engineering task of generating captions for images has received a lot of attention (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2014; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2014).", "startOffset": 97, "endOffset": 276}, {"referenceID": 25, "context": "They develop a model based on a contextdependent recurrent neural network (Mikolov and Zweig, 2012) which simultaneously processes textual and visual input and updates two parallel hidden states.", "startOffset": 74, "endOffset": 99}, {"referenceID": 5, "context": ", 2014; Chen and Zitnick, 2014; Fang et al., 2014). From the point of view of modeling, the research most relevant to our interests is that of Chen and Zitnick (2014). They develop a model based on a contextdependent recurrent neural network (Mikolov and Zweig, 2012) which simultaneously processes textual and visual input and updates two parallel hidden states.", "startOffset": 8, "endOffset": 167}, {"referenceID": 14, "context": "GRUs are related to the Long Short-Term Memory units (Hochreiter and Schmidhuber, 1997), but do not employ a separate memory cell.", "startOffset": 53, "endOffset": 87}, {"referenceID": 6, "context": "Both pathways are composed of Gated Recurrent Units (GRU) first introduced by Cho et al. (2014) and Chung et al.", "startOffset": 78, "endOffset": 96}, {"referenceID": 6, "context": "Both pathways are composed of Gated Recurrent Units (GRU) first introduced by Cho et al. (2014) and Chung et al. (2014). GRUs are related to the Long Short-Term Memory units (Hochreiter and Schmidhuber, 1997), but do not employ a separate memory cell.", "startOffset": 78, "endOffset": 120}, {"referenceID": 2, "context": "Settings The model was implemented in Theano (Bastien et al., 2012; Bergstra et al., 2010) and optimized by Adam (Kingma and Ba, 2014).", "startOffset": 45, "endOffset": 90}, {"referenceID": 17, "context": ", 2010) and optimized by Adam (Kingma and Ba, 2014).", "startOffset": 30, "endOffset": 51}, {"referenceID": 28, "context": "1 The fixed 4096-dimensional target image representation come from the pre-softmax layer of the 16layer CNN (Simonyan and Zisserman, 2014).", "startOffset": 108, "endOffset": 138}, {"referenceID": 3, "context": "Word similarity judgment For similarity judgment correlations, we selected two existing benchmarks that have the largest vocabulary overlap with our data: MEN 3K (Bruni et al., 2014) and SimLex-999 (Hill et al.", "startOffset": 162, "endOffset": 182}, {"referenceID": 13, "context": ", 2014) and SimLex-999 (Hill et al., 2014).", "startOffset": 23, "endOffset": 42}, {"referenceID": 27, "context": "Single-word image retrieval In order to visualize the acquired meaning for individual words, we use images from the ILSVRC2012 subset of ImageNet (Russakovsky et al., 2014) as benchmark.", "startOffset": 146, "endOffset": 172}, {"referenceID": 34, "context": "As most of these are on Flickr30K (Young et al., 2014), we ran MULTITASK on it and got an accuracy@5 of 32%, within the range of numbers reported in previous work: 29.", "startOffset": 34, "endOffset": 54}, {"referenceID": 31, "context": "8% (Socher et al., 2014), 31.", "startOffset": 3, "endOffset": 24}, {"referenceID": 23, "context": "2% (Mao et al., 2014), 34% (Kiros et al.", "startOffset": 3, "endOffset": 21}, {"referenceID": 18, "context": ", 2014), 34% (Kiros et al., 2014) and 37.", "startOffset": 13, "endOffset": 33}, {"referenceID": 16, "context": "7% (Karpathy and Fei-Fei, 2014).", "startOffset": 3, "endOffset": 31}, {"referenceID": 16, "context": "7% (Karpathy and Fei-Fei, 2014). Karpathy and Fei-Fei (2014) report 29.", "startOffset": 4, "endOffset": 61}], "year": 2015, "abstractText": "We propose IMAGINET, a model of learning visually grounded representations of language from coupled textual and visual input. The model consists of two Gated Recurrent Unit networks with shared word embeddings, and uses a multi-task objective by receiving a textual description of a scene and trying to concurrently predict its visual representation and the next word in the sentence. Mimicking an important aspect of human language learning, it acquires meaning representations for individual words from descriptions of visual scenes. Moreover, it learns to effectively use sequential structure in semantic interpretation of multi-word phrases.", "creator": "LaTeX with hyperref package"}}}