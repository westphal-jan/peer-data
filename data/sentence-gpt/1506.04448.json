{"id": "1506.04448", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2015", "title": "Fast and Guaranteed Tensor Decomposition via Sketching", "abstract": "Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in statistical learning of latent variable models and in data mining. In this paper, we propose fast and randomized tensor CP decomposition algorithms based on sketching. We build on the idea of count sketches, but introduce many novel ideas which are unique to tensors. We develop novel methods for randomized computation of tensor contractions via FFTs, without explicitly forming the tensors. Such tensor contractions are encountered in decomposition methods such as tensor power iterations and alternating least squares. We also design novel colliding hashes for symmetric tensors to further save time in computing the sketches. We then combine these sketching ideas with existing whitening and tensor power iterative techniques to obtain the fastest algorithm on both sparse and dense tensors. The quality of approximation under our method does not depend on properties such as sparsity, uniformity of elements, etc. We apply the method for topic modeling and obtain competitive results. Finally, our algorithm also includes the potential for rapid development of complex algorithms for complex computations.", "histories": [["v1", "Sun, 14 Jun 2015 23:07:38 GMT  (122kb,D)", "http://arxiv.org/abs/1506.04448v1", "29 pages"], ["v2", "Tue, 20 Oct 2015 14:45:41 GMT  (127kb,D)", "http://arxiv.org/abs/1506.04448v2", "29 pages. Appeared in Proceedings of Advances in Neural Information Processing Systems (NIPS), held at Montreal, Canada in 2015"]], "COMMENTS": "29 pages", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["yining wang", "hsiao-yu fish tung", "alexander j smola", "anima anandkumar"], "accepted": true, "id": "1506.04448"}, "pdf": {"name": "1506.04448.pdf", "metadata": {"source": "CRF", "title": "Fast and Guaranteed Tensor Decomposition via Sketching", "authors": ["Yining Wang", "Hsiao-Yu Tung"], "emails": [], "sections": [{"heading": null, "text": "Keywords: Tensor CP decomposition, count sketch, randomized methods, spectral methods, topic modeling"}, {"heading": "1 Introduction", "text": "In many data-rich domains such as computer vision, neuroscience and social networks consisting of multimodal and multi-relational data, tensors have emerged as a powerful paradigm for handling the data deluge. An important operation with tensor data is its decomposition, where the input tensor is decomposed into a succinct form. One of the popular decomposition methods is the CANDECOMP/PARAFAC (CP) decomposition, also known as canonical polyadic decomposition [12, 5], where the input tensor is decomposed into a succinct sum of rank-1 components. The CP decomposition has found numerous applications in data mining [4, 18, 20], computational neuroscience [10, 21], and recently, in statistical learning for latent variable models [1, 29, 27, 6]. For latent variable modeling, these methods yield consistent estimates under mild conditions such as non-degeneracy and require only polynomial sample and computational complexity [1, 29, 27, 6].\nGiven the importance of tensor methods for large-scale machine learning, there has been an increasing interest in scaling up tensor decomposition algorithms to handle gigantic real-world data tensors [26, 23, 8, 16, 14, 2, 28]. However, the previous works fall short in many ways, as described subsequently. In this paper, we design and analyze efficient randomized tensor methods using ideas from sketching [22]. The idea is to maintain a low-dimensional sketch of an input tensor and then perform implicit tensor decomposition using existing methods such as tensor power updates, alternating least squares or online tensor updates. We obtain the fastest decomposition methods for both sparse and dense tensors. Our framework can easily handle\nar X\niv :1\n50 6.\n04 44\n8v 1\n[ st\nat .M\nL ]\nmodern machine learning applications with billions of training instances, and at the same time, comes with attractive theoretical guarantees.\nOur main contributions are as follows:\nEfficient tensor sketch construction: We propose efficient construction of tensor sketches when the input tensor is available in factored forms such as in the case of empirical moment tensors, where the factor components correspond to rank-1 tensors over individual data samples. We construct the tensor sketch via efficient FFT operations on the component vectors. Sketching each rank-1 component takes O(n + b log b) operations where n is the tensor dimension and b is the sketch length. This is much faster than the O(np) complexity for brute force computations of a pth-order tensor. Since empirical moment tensors are available in the factored form with N components, where N is the number of samples, it takes O((n + b log b)N) operations to compute the sketch.\nImplicit tensor contraction computations: Almost all tensor manipulations can be expressed in terms of tensor contractions, which involves multilinear combinations of different tensor fibres [19]. For example, tensor decomposition methods such as tensor power iterations, alternating least squares (ALS), whitening and online tensor methods all involve tensor contractions. We propose a highly efficient method to directly compute the tensor contractions without forming the input tensor explicitly. In particular, given the sketch of a tensor, each tensor contraction can be computed in O(n + b log b) operations, regardless of order of the source and destination tensors. This significantly accelerates the brute-force implementation that requires O(np) complexity for pth-order tensor contraction. In addition, in many applications, the input tensor is not directly available and needs to be computed from samples, such as the case of empirical moment tensors for spectral learning of latent variable models. In such cases, our method results in huge savings by combining implicit tensor contraction computation with efficient tensor sketch construction.\nNovel colliding hashes for symmetric tensors: When the input tensor is symmetric, which is the case for empirical moment tensors that arise in spectral learning applications, we propose a novel colliding hash design by replacing the Boolean ring with the complex ring C to handle multiplicities. As a result, it makes the sketch building process much faster and avoids repetitive FFT operations. Though the computational complexity remains the same, the proposed colliding hash design results in significant speed-up in practice by reducing the actual number of computations.\nTheoretical and empirical guarantees: We show that the quality of the tensor sketch does not depend on sparseness, uniform entry distribution, or any other properties of the input tensor. On the other hand, previous works assume specific settings such as sparse tensors [23, 8, 16], or tensors having entries with similar magnitude [26]. Such assumptions are unrealistic, and in practice, we may have both dense and spiky tensors, for example, unordered word trigrams in natural language processing. We prove that our proposed randomized method for tensor decomposition does not lead to any significant degradation of accuracy.\nExperiments on synthetic and real-world datasets show highly competitive results. We demonstrate a 10x to 100x speed-up over exact methods for decomposing dense, high-dimensional tensors. For topic modeling, we show a significant reduction in computational time over existing spectral LDA implementations with small performance loss. In addition, our proposed algorithm outperforms collapsed Gibbs sampling when running time is constrained. We also show that if a Gibbs sampler is initialized with our output topics, it converges within several iterations and outperforms a randomly initialized Gibbs sampler run for much more iterations. Since our proposed method is efficient and avoids local optima, it can be used to accelerate the slow burn-in phase in Gibbs sampling.\nRelated Works: There have been numerous works on deploying efficient tensor decomposition methods [26, 23, 8, 16, 14, 2, 28]. Most of these works except [26, 2] implement the alternating least squares (ALS) algorithm [12, 5]. However, this is extremely expensive since the above works run the ALS method in\nthe input space, and require O(n3) operations to execute one least squares step on a n-dimensional (dense) tensor. Thus, such implementations are only suited for extremely sparse tensors.\nAn alternative method is to first reduce the dimension of the input tensor through procedures such as whitening to O(k) dimension, where k is the tensor rank, and then carry out ALS in the dimension-reduced space on k\u00d7k\u00d7k tensor [13]. This results in significant reduction of computational complexity when the rank is small (k n). Nonetheless, in practice, such complexity is still prohibitively high as k could be several thousands in many settings. To make matters even worse, when the tensor corresponds to empirical moments computed from samples, such as in spectral learning of latent variable models, it is actually much slower to construct the reduced dimension k \u00d7 k \u00d7 k tensor from training data than to decompose it, since the number of training samples is typically very large. Another alternative is to carry out online tensor decomposition, as opposed to batch operations in the above works. Such methods are extremely fast [14], but can suffer from high variance. The sketching ideas developed in this paper will improve our ability to handle larger sizes of mini-batches and therefore result in reduced variance in online tensor methods.\nAnother alternative method is to consider a randomized sampling of the input tensor in each iteration of tensor decomposition [26, 2]. However, such methods can be expensive due to I/O calls and are sensitive to the sampling distribution. In particular, [26] employs uniform sampling, which is incapable of handling tensors with spiky elements. Though non-uniform sampling is adopted in [2], it requires an additional pass over the training data to compute the sampling distribution. In contrast, our sketch based method takes only one pass of the data."}, {"heading": "2 Preliminaries", "text": "Tensor, tensor product and tensor decomposition A 3rd order tensor 1 T of dimension n has n3 entries. Each entry can be represented as Tijk for i, j, k \u2208 {1, \u00b7 \u00b7 \u00b7 , n}. For an n \u00d7 n \u00d7 n tensor T and a vector u \u2208 Rn, we define two forms of tensor products (contractions) as follows:\nT(u,u,u) = n\u2211 i,j,k=1 Ti,j,kuiujuk; T(I,u,u) =  n\u2211 j,k=1 T1,j,kujuk, \u00b7 \u00b7 \u00b7 , n\u2211 j,k=1 Tn,j,kujuk  . Note that T(u,u,u) \u2208 R and T(I,u,u) \u2208 Rn. For two complex tensors A,B of the same order and dimension, its inner product is defined as \u3008A,B\u3009 := \u2211 lAlBl, where l takes the value of all tuples that\nindex the tensors. The Frobenius norm of a tensor is simply \u2016A\u2016F = \u221a \u3008A,A\u3009.\nFor a 3rd order tensor T \u2208 Rn\u00d7n\u00d7n its rank-k CP decomposition involves values {\u03bbi}ki=1 \u2286 R and vectors {ai}ki=1, {bi}ki=1, {ci}ki=1 \u2286 Rn such that the residual \u2225\u2225\u2225T\u2212\u2211ki=1 \u03bbiai \u2297 bi \u2297 ci\u2225\u2225\u22252\nF is minimized.\nHere R = a \u2297 b \u2297 c is a 3rd order tensor defined as Rijk = aibjck. Additional notations are defined in Table 1 and Appendix F.\nRobust tensor power method It was proposed in [1] and was shown to provably succeed if the input tensor is a noisy perturbation of the sum of k rank-1 tensors whose base vectors are orthogonal. Fix an input tensor T \u2208 Rn\u00d7n\u00d7n, The basic idea is to randomly generate L initial vectors and perform T power update steps: u\u0302 = T(I,u,u)/\u2016T(I,u,u)\u20162. The vector that results in the largest eigenvalue T(u,u,u) is then kept and subsequent eigenvectors can be obtained via deflation. If implemented naively, the algorithm takes\n1Though we mainly focus on 3rd order tensors in this work, extension to higher order tensors is easy.\nAlgorithm 1 Efficient sketching of factored and empirical moment tensors 1: Input: Tensor T = \u2211N i=1 aiui \u2297 vi \u2297wi, hash length b, number of sketches B.\n2: Initialize: hash functions h(m)1 , h (m) 2 , h (m) 3 , \u03be (m) 1 , \u03be (m) 2 , \u03be (m) 3 for m = 1, \u00b7 \u00b7 \u00b7 , B; s (m) T = 0. 3: for m \u2208 1, \u00b7 \u00b7 \u00b7 , B, i \u2208 {1, \u00b7 \u00b7 \u00b7N} do 4: Compute s(m)1,i (t) = \u2211 h (m) 1 (j)=t \u03be (m) 1 (j)[ui]j and s (m) 2,i (t), s (m) 3,i (t) analogously. 5: Update: s(m)T \u2190 s (m) T + ai \u00b7 F\u22121(F(s (m) 1,i \u25e6 F(s (m) 2,i ) \u25e6 F(s (m) 3,i )). 3ai is defined in Line 1. 6: Output: B sketches: s(1)T , \u00b7 \u00b7 \u00b7 , s (B) T .\nO(kn3LT ) time to run 2, requiring O(n3) storage. In addition, in certain cases when a second-order moment matrix is available, the tensor power method can be carried out on a k \u00d7 k \u00d7 k whitened tensor [1], thus improving the time complexity by avoiding dependence on the ambient dimension n. Apart from the tensor power method, other algorithms such as Alternating Least Squares (ALS, [12, 5]) and Stochastic Gradient Descent (SGD, [14]) have also been applied to tensor CP decomposition."}, {"heading": "3 Fast tensor decomposition via sketching", "text": "In this section we first introduce tensor sketching [22] and show how sketches can be computed efficiently for factored or empirical moment tensors. We then show how to run tensor power method directly on the sketch with reduced computational complexity. In addition, when the input tensor is symmetric (i.e., Tijk the same for all permutations of i, j, k) we propose a novel \u201ccolliding hash\u201d design, which speeds up the sketch building process. Due to space limits we only consider the robust tensor power method in the main text. Methods and experiments for sketching based ALS method are presented in Appendix C.\nTo avoid confusions, we emphasize that n is used to denote the dimension of the tensor to be decomposed, which is not necessarily the same as the dimension of the original data tensor. Indeed, once whitening is applied n could be as small as the intrinsic dimension k of the original data tensor."}, {"heading": "3.1 Tensor sketch", "text": "Tensor sketch was proposed in [22] as a generalization of count sketch [7]. For a tensor T of dimension n1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 np, random hash functions h1, \u00b7 \u00b7 \u00b7 , hp : [n] \u2192 [b] with Prhj [hj(i) = t] = 1/b for every i \u2208 [n], j \u2208 [p], t \u2208 [b] and random Bernoulli variables \u03be1, \u00b7 \u00b7 \u00b7 , \u03bep : [n]\u2192 {+1,\u22121} with Pr\u03bej [\u03bej(i) = 1] = Pr\u03bej [\u03bej(i) = \u22121] = 1/2, the tensor sketch sT : [b]\u2192 R is defined as\nsT(t) = \u2211\nH(i1,\u00b7\u00b7\u00b7 ,ip)=t\n\u03be1(i1) \u00b7 \u00b7 \u00b7 \u03bep(ip)Ti1,\u00b7\u00b7\u00b7 ,ip , (1)\nwhere H(i1, \u00b7 \u00b7 \u00b7 , ip) = (h1(i1) + \u00b7 \u00b7 \u00b7 + hp(ip)) mod b. The corresponding recovery rule is T\u0302i1,\u00b7\u00b7\u00b7 ,ip = \u03be1(i1) \u00b7 \u00b7 \u00b7 \u03bep(ip)sT(H(i1, \u00b7 \u00b7 \u00b7 , ip)). For accurate recovery, H needs to be 2-wise independent, which is achieved by independently selecting h1, \u00b7 \u00b7 \u00b7 , hp from a 2-wise independent hash family [25]. The following proposition upper bounds the recovery error in terms of hash length b and tensor Frobenious norm \u2016T\u2016F . Its proof is deferred to Appendix E.1.\nProposition 1. Fix i1, \u00b7 \u00b7 \u00b7 , ip. For every > 0 the following holds:\nPr H,\u03be [\u2223\u2223T\u0302i1,\u00b7\u00b7\u00b7 ,ip \u2212Ti1,\u00b7\u00b7\u00b7 ,ip \u2223\u2223 \u2265 ] \u2264 \u2016T\u20162F /(b 2). (2)"}, {"heading": "3.2 Efficient sketching of empirical moment tensors", "text": "2L is usually set to be a linear function of k and T is logarithmic in n; see Theorem 5.1 in [1]. 3F and F\u22121 stand for the FFT and inverse FFT operators.\nWe present efficient algorithms to sketch an empirical moment tensor. The proposed method scales linearly with tensor dimension, which is much more efficient than explicitly constructing the data tensor that takes cubic time in the worst case. The main idea is to decompose an empirical moment tensor into the sum of many rank-1 components and then apply FFT for each component."}, {"heading": "3.2.1 Sketching a rank-1 tensor", "text": "For a rank-1 tensor T = u\u2297 v\u2297w with u,v,w \u2208 Rn, its b-dimensional tensor sketch sT can be computed efficiently via the following expression: sT = s1,u \u2217 s2,v \u2217 s3,w = F\u22121(F(s1,u) \u25e6 F(s1,u) \u25e6 F(s1,u)), (3) where \u2217 denotes convolution and \u25e6 stands for element-wise vector product. s1,u(t) = \u2211 h1(i)=t\n\u03be1(i)ui is the count sketch of u and s2,v, s3,w are defined similarly. F and F\u22121 denote the Fast Fourier Transform (FFT) and its inverse operator. By applying FFT, we reduce the convolution computation into element-wise product evaluation in the Fourier space. Therefore, sT can be computed using O(n + b log b) operations, where the O(b log b) term arises from FFT evaluations."}, {"heading": "3.2.2 Extension to factored and empirical moment tensors", "text": "For a tensor T with known rank factorization T = \u2211N i=1 aiui \u2297 vi \u2297wi, we can efficiently compute its tensor sketch sT by directly applying techniques in Sec. 3.2.1 because the sketching operator is linear; that is, s\u03bbA+\u00b5B = \u03bbsA + \u00b5sB for arbitrary scalars \u03bb, \u00b5 and tensors A,B. Consequently, computing sT takes O(N(n+ b log b)) operations, which is linear in tensor dimension n. On the other hand, most empirical moment tensors appeared in spectral learning of latent variable models do have known rank factorizations. For example, a 3rd-order empirical moment E\u0302[x\u22973] can be written as E\u0302[x\u22973] = 1N \u2211N i=1 x \u22973 i , where {xi}Ni=1 are the training data points. Pseudocode for efficient sketch computation of factored and empirical moment tensors is listed in Alg 1. We compute B independent sketches and output the median of the results. Such schemes were shown to effectively reduce the approximation error from sketching and also result in exponentially decaying tails for failure probability [7]. Furthermore, when training data are truly abundant it helps to apply sketching on mini-batches of training data, which keeps the computational cost small and yet has reduced variance compared to purely online methods with batch size equals one [14]."}, {"heading": "3.3 Fast robust tensor power method", "text": "We are now ready to present the fast robust tensor power method, the main algorithm of this paper. The computational bottleneck of the original robust tensor power method is the computation of two tensor products: T(I,u,u) and T(u,u,u). A naive implementation requires O(n3) operations. In this section, we show how to speed up computation of these products. We show that given the sketch of an input tensor T, one can approximately compute both T(I,u,u) and T(u,u,u) in O(b log b+ n) steps, where b is the hash length.\nBefore going into details, we explain the key idea behind our fast tensor product computation. For any two tensors A,B, its inner product \u3008A,B\u3009 can be approximated by 5\n\u3008A,B\u3009 \u2248 \u3008sA, sB\u3009. (4) Eq. (4) immediately results in a fast approximation procedure of T(u,u,u) because T(u,u,u) = \u3008T,X\u3009 where X = u\u2297 u\u2297 u is a rank one tensor, whose sketch can be built in O(n+ b log b) time by Sec. 3.2.1. Consequently, the product can be approximately computed usingO(n+b log b) operations if the tensor sketch of T is available. For tensor product of the form T(I,u,u). The ith coordinate in the result can be expressed as \u3008T,Yi\u3009where Yi = ei\u2297u\u2297u; ei = (0, \u00b7 \u00b7 \u00b7 , 0, 1, 0, \u00b7 \u00b7 \u00b7 , 0) is the ith indicator vector. We can then apply Eq. (4) to approximately compute \u3008T,Yi\u3009 efficiently. However, this method is not completely satisfactory\n4<(\u00b7) denotes the real part of a complex number. med(\u00b7) denotes the median. 5All approximations will be theoretically justified in Section 4 and Appendix E.2.\nAlgorithm 2 Fast robust tensor power method\nPLAIN SKETCH PLAIN+WHITENING SKETCH+WHITENING preprocessing: general tensors - O(n3) O(kn3) O(n3) preprocessing: factored tensors\nO(Nn3) O(N(n+ b log b)) O(N(nk + k3)) O(N(nk + b log b))with N components per tensor contraction time O(n3) O(n+ b log b) O(k3) O(k + b log b)\nbecause it requires sketching n rank-1 tensors (Y1 through Yn), which results in O(n) FFT evaluations by Eq. (3). Below we present a proposition that allows us to use only O(1) FFTs to approximate T(I,u,u).\nProposition 2. \u3008sT, s1,ei \u2217 s2,u \u2217 s3,u\u3009 = \u3008F\u22121(F(sT) \u25e6 F(s2,u) \u25e6 F(s3,u)), s1,ei\u3009.\nProposition 2 is proved in Appendix E.1. The main idea is to \u201cshift\u201d all terms not depending on i to the left side of the inner product and eliminate the inverse FFT operation on the right side so that sei contains only one nonzero entry. As a result, we can computeF\u22121(F(sT)\u25e6F(s2,u)\u25e6F(s3,u)) once and read off each entry of T(I,u,u) in constant time. In addition, the technique can be further extended to symmetric tensor sketches, with details deferred to Appendix B due to space limits. When operating on an n-dimensional tensor, The algorithm requires O(kLT (n + Bb log b)) running time (excluding the time for building s\u0303T\u0304) and O(Bb) memory, which significantly improves the O(kn3LT ) time and O(n3) space complexity over the brute force tensor power method. Here L, T are algorithm parameters for robust tensor power method. Previous analysis shows that T = O(log k) and L = poly(k), where poly(\u00b7) is some low order polynomial function. [1]\nFinally, Table 2 summarizes computational complexity of sketched and plain tensor power method."}, {"heading": "3.4 Colliding hash and symmetric tensor sketch", "text": "For symmetric input tensors, it is possible to design a new style of tensor sketch that can be built more efficiently. The idea is to design hash functions that deliberately collide symmetric entries, i.e., (i, j, k), (j, i, k), etc. Consequently, we only need to consider entries Tijk with i \u2264 j \u2264 k when building tensor sketches. An intuitive idea is to use the same hash function and Rademacher random variable for each order, that is, h1(i) = h2(i) = h3(i) =: h(i) and \u03be1(i) = \u03be2(i) = \u03be3(i) =: \u03be(i). In this way, all permutations of (i, j, k) will collide with each other. However, such a design has an issue with repeated entries because \u03be(i) can only take \u00b11 values. Consider (i, i, k) and (j, j, k) as an example: \u03be(i)2\u03be(k) = \u03be(j)2\u03be(k) with probability 1 even if i 6= j. On the other hand, we need E[\u03be(a)\u03be(b)] = 0 for any pair of distinct 3-tuples a and b.\nTo address the above-mentioned issue, we extend the Rademacher random variables to the complex domain and consider all roots of zm = 1, that is, \u2126 = {\u03c9j}m\u22121j=0 where \u03c9j = ei 2\u03c0j m . Suppose \u03c3(i) is a Rademacher random variable with Pr[\u03c3(i) = \u03c9i] = 1/m. By elementary algebra, E[\u03c3(i)p] = 0 whenever m is relative prime to p or m can be divided by p. Therefore, by setting m = 4 we avoid collisions of repeated entries in a 3rd order tensor. More specifically, The symmetric tensor sketch of a symmetric tensor T \u2208 Rn\u00d7n\u00d7n can be defined as\ns\u0303T(t) := \u2211\nH\u0303(i,j,k)=t\nTi,j,k\u03c3(i)\u03c3(j)\u03c3(k), (5)\nwhere H\u0303(i, j, k) = (h(i) + h(j) + h(k)) mod b. To recover an entry, we use\nT\u0302i,j,k = 1/\u03ba \u00b7 \u03c3(i) \u00b7 \u03c3(j) \u00b7 \u03c3(k) \u00b7 s\u0303T(H(i, j, k)), (6) where \u03ba = 1 if i = j = k; \u03ba = 3 if i = j or j = k or i = k; \u03ba = 6 otherwise. For higher order tensors, the coefficients can be computed via the Young tableaux which characterizes symmetries under the permutation group. Compared to asymmetric tensor sketches, the hash function h needs to satisfy stronger independence conditions because we are using the same hash function for each order. In our case, h needs to be 6-wise independent to make H\u0303 2-wise independent. The fact is due to the following proposition, which is proved in Appendix E.1.\nProposition 3. Fix p and q. For h : [n]\u2192 [b] define symmetric mapping H\u0303 : [n]p \u2192 [b] as H\u0303(i1, \u00b7 \u00b7 \u00b7 , ip) = h(i1) + \u00b7 \u00b7 \u00b7+ h(ip). If h is (pq)-wise independent then H is q-wise independent.\nThe symmetric tensor sketch described above can significantly speed up sketch building processes. For a general tensor with M nonzero entries, to build s\u0303T one only needs to consider roughly M/6 entries (those Tijk 6= 0 with i \u2264 j \u2264 k). For a rank-1 tensor u\u22973, only one FFT is needed to build F(s\u0303); in contrast, to compute Eq. (3) one needs at least 3 FFT evaluations.\nFinally, in Appendix B we give details on how to seamlessly combine symmetric hashing and techniques in previous sections to efficiently construct and decompose a tensor."}, {"heading": "4 Error analysis", "text": "In this section we provide theoretical analysis on approximation error of both tensor sketch and the fast sketched robust tensor power method. We mainly focus on symmetric tensor sketches, while extension to asymmetric settings is trivial. Due to space limits, all proofs are placed in the appendix."}, {"heading": "4.1 Tensor sketch concentration bounds", "text": "Theorem 1 bounds the approximation error of symmetric tensor sketches when computing T(u,u,u) and T(I,u,u). Its proof is deferred to Appendix E.2.\nTheorem 1. Fix a symmetric real tensor T \u2208 Rn\u00d7n\u00d7n and a real vector u \u2208 Rn with \u2016u\u20162 = 1. Suppose \u03b51,T (u) \u2208 R and \u03b52,T (u) \u2208 Rn are estimation errors of T(u,u,u) and T(I,u,u) using B independent symmetric tensor sketches; that is, \u03b51,T (u) = T\u0302(u,u,u) \u2212 T(u,u,u) and \u03b52,T (u) = T\u0302(I,u,u) \u2212 T(I,u,u). If B = \u2126(log(1/\u03b4)) then with probability \u2265 1\u2212 \u03b4 the following error bounds hold:\u2223\u2223\u03b51,T (u)\u2223\u2223 = O(\u2016T\u2016F /\u221ab); \u2223\u2223 [\u03b52,T (u)]i \u2223\u2223 = O(\u2016T\u2016F /\u221ab), \u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 , n}. (7) In addition, for any fixed w \u2208 Rn, \u2016w\u20162 = 1 with probability \u2265 1\u2212 \u03b4 we have\n\u3008w, \u03b52,T (u)\u30092 = O(\u2016T\u20162F /b). (8)"}, {"heading": "4.2 Analysis of the fast tensor power method", "text": "We present a theorem analyzing robust tensor power method with tensor sketch approximations. A more detailed theorem statement along with its proof can be found in Appendix E.3.\nTheorem 2. Suppose T\u0304 = T + E \u2208 Rn\u00d7n\u00d7n where T = \u2211k i=1 \u03bbiv \u22973 i with an orthonormal basis {vi}ki=1, \u03bb1 > \u00b7 \u00b7 \u00b7 > \u03bbk > 0 and \u2016E\u2016 = . Let {(\u03bb\u0302i, v\u0302i)}ki=1 be the eigenvalue/eigenvector pairs obtained by Algorithm 2. Suppose = O(1/(\u03bb1n)), T = \u2126(log(n/\u03b4) + log(1/ ) maxi \u03bbi/(\u03bbi \u2212 \u03bbi\u22121)) and L grows linearly with k. Assume the randomness of the tensor sketch is independent among tensor product evaluations. If B = \u2126(log(n/\u03b4)) and b satisfies\nb = \u2126 ( max { \u22122\u2016T\u20162F\n\u2206(\u03bb)2 , \u03b4\u22124n2\u2016T\u20162F r(\u03bb)2\u03bb21\n}) (9)\nwhere \u2206(\u03bb) = mini(\u03bbi \u2212 \u03bbi\u22121) and r(\u03bb) = maxi,j>i(\u03bbi/\u03bbj), then with probability \u2265 1\u2212 \u03b4 there exists a permutation \u03c0 over [k] such that\n\u2016v\u03c0(i) \u2212 v\u0302i\u20162 \u2264 , |\u03bb\u03c0(i) \u2212 \u03bb\u0302i| \u2264 \u03bbi /2, \u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 , k} (10) and \u2016T\u2212 \u2211k i=1 \u03bb\u0302iv\u0302 \u22973 i \u2016 \u2264 c for some constant c.\nTheorem 1 shows that the sketch length b can be set as o(n3) to provably approximately decompose a 3rdorder tensor with dimension n. Theorem 1 together with time complexity comparison in Table 2 shows that the sketching based fast tensor decomposition algorithm has better computational complexity over brute-force implementation. One potential drawback of our analysis is the assumption that sketches are independently built for each tensor product (contraction) evaluation. This is an artifact of our analysis and we conjecture that it can be removed by incorporating recent development of differentially private adaptive query framework [9]."}, {"heading": "5 Experiments", "text": "We demonstrate the effectiveness and efficiency of our proposed sketch based tensor power method on both synthetic tensors and real-world topic modeling problems. Experimental results involving the fast ALS method are presented in Appendix C.3. All methods are implemented in C++ and tested on a single machine with 8 Intel X5550@2.67Ghz CPUs and 32GB memory. For synthetic tensor decomposition we use only a single thread; for fast spectral LDA 8 to 16 threads are used."}, {"heading": "5.1 Synthetic tensors", "text": "In Table 5 we compare our proposed algorithms with exact decomposition methods on synthetic tensors. Let n = 1000 be the dimension of the input tensor. We first generate a random orthonormal basis {vi}ni=1 and then set the input tensor T as T = normalize( \u2211n i=1 \u03bbiv \u22973 i ) + E, where the eigenvalues \u03bbi satisfy \u03bbi = 1/i. The normalization step makes \u2016T\u20162F = 1 before imposing noise. The Gaussian noise matrix E is symmetric with Eijk \u223c N (0, \u03c3/n1.5) for i \u2264 j \u2264 k and noise-to-signal level \u03c3. Due to time constraints, we only compare the recovery error and running time on the top 10 recovered eigenvectors of the full-rank input tensor T. Both L and T are set to 30. Table 3 shows that our proposed algorithms achieve reasonable\napproximation error within a few minutes, which is much faster then exact methods. A complete version (Table 5) is deferred to Appendix A."}, {"heading": "5.2 Topic modeling", "text": "We implement a fast spectral inference algorithm for Latent Dirichlet Allocation (LDA [3]) by combining tensor sketching with existing whitening technique for dimensionality reduction. Implementation details are provided in Appendix D. We compare our proposed fast spectral LDA algorithm with baseline spectral methods and collapsed Gibbs sampling (using GibbsLDA++ [24] implementation) on two real-world datasets: Wikipedia and Enron. Dataset details are presented in A Only the most frequent V words are kept and the vocabulary size V is set to 10000. For the robust tensor power method the parameters are set to L = 50 and T = 30. For ALS we iterate until convergence, or a maximum number of 1000 iterations is reached. \u03b10 is set to 1.0 and B is set to 30.\nObtained topic models \u03a6 \u2208 RV\u00d7K are evaluated on a held-out dataset consisting of 1000 documents randomly picked out from training datasets. For each testing document d, we fit a topic mixing vector \u03c0\u0302d \u2208 RK by solving the following optimization problem: \u03c0\u0302d = argmin\u2016\u03c0\u20161=1,\u03c0\u22650\u2016wd \u2212\u03a6\u03c0\u20162,wherewd is the empirical word distribution of document d. The per-document log-likelihood is then defined as Ld = 1 nd \u2211nd i=1 ln p(wdi), where p(wdi) = \u2211K k=1 \u03c0\u0302k\u03a6wdi,k. Finally, the average Ld over all testing documents is reported. Figure 1 shows the held-out negative log-likelihood for fast spectral LDA under different hash lengths b. We can see that as b increases, the performance approaches the exact tensor power method because sketching approximation becomes more accurate. On the other hand, Table 6 shows that fast spectral LDA runs much faster than exact tensor decomposition methods while achieving comparable performance on both datasets.\nFigure 2 compares the convergence of collapsed Gibbs sampling with different number of iterations and fast spectral LDA with different hash lengths on Wikipedia dataset. For collapsed Gibbs sampling, we set \u03b1 = 50/K and \u03b2 = 0.1 following [11]. As shown in the figure, fast spectral LDA achieves comparable heldout likelihood while running faster than collapsed Gibbs sampling. We further take the dictionary \u03a6 output by fast spectral LDA and use it as initializations for collapsed Gibbs sampling (the word topic assignments z are obtained by 5-iteration Gibbs sampling, with the dictionary \u03a6 fixed). The resulting Gibbs sampler converges much faster: with only 3 iterations it already performs much better than a randomly initialized Gibbs sampler run for 100 iterations, which takes 10x more running time.\nWe also report performance of fast spectral LDA and collapsed Gibbs sampling on a larger dataset in Table 4. The dataset was built by crawling 1,085,768 random Wikipedia pages and a held-out evaluation set was built by randomly picking out 1000 documents from the dataset. Number of topics k is set to 200 or 300, and after getting topic dictionary \u03a6 from fast spectral LDA we use 2-iteration Gibbs sampling to obtain word topic assignments z. Table 4 shows that the hybrid method (i.e., collapsed Gibbs sampling initialized by spectral LDA) achieves the best likelihood performance in a much shorter time, compared to a randomly initialized Gibbs sampler."}, {"heading": "Appendix A Supplementary experimental results", "text": "The Wikipedia dataset is built by crawling all documents in all subcategories within 3 layers below the science category. The Enron dataset is from the Enron email corpus [17]. After usual cleaning steps, the Wikipedia dataset has 114, 274 documents with an average 512 words per document; the Enron dataset has 186, 501 emails with average 91 words per email."}, {"heading": "Appendix B Fast tensor power method via symmetric sketching", "text": "In this section we show how to do fast tensor power method using symmetric tensor sketches. More specifically, we explain how to approximately compute T(u,u,u) and T(I,u,u) when colliding hashes are used.\nFor symmetric tensors A and B, their inner product can be approximated by \u3008A,B\u3009 \u2248 \u3008s\u0303A, s\u0303B\u0303\u3009, (11)\nwhere B\u0303 is an \u201cupper-triangular\u201d tensor defined as\nB\u0303i,j,k = { Bi,j,k, if i \u2264 j \u2264 k; 0, otherwise. (12)\nNote that in Eq. (11) only the matrix B is \u201ctruncated\u201d. We show this gives consistent estimates of \u3008A,B\u3009 in Appendix E.2.\nRecall that T(u,u,u) = \u3008T,X\u3009 where X = u \u2297 u \u2297 u. The symmetric tensor sketch s\u0303X\u0303 can be computed as\ns\u0303X\u0303 = 1 6 s\u0303\u22973u + 1 2 s\u03032,u\u25e6u \u2217 s\u0303u + 1 3 s\u03033,u\u25e6u\u25e6u, (13)\nwhere s\u03032,u\u25e6u(t) = \u2211 2h(i)=t \u03c3(i) 2u2i and s\u03033,u\u25e6u\u25e6u(t) = \u2211 3h(i)=t \u03c3(i) 3u3i . As a result,\nT(u,u,u) \u2248 1 6 \u3008F(s\u0303T),F(s\u0303u) \u25e6 F(s\u0303u) \u25e6 F(s\u0303u)\u3009+ 1 2 \u3008F(s\u0303T),F(s\u03032,u\u25e6u) \u25e6 F(s\u0303u)\u3009+ 1 3 \u3008s\u0303T, s\u03033,u\u25e6u\u25e6u\u3009.\n(14) For T(I,u,u) recall that [T(I,u,u)]i = \u3008T,Yi\u3009 where Yi = ei \u2297 u \u2297 u. We first symmetrize it by defining Zi = ei \u2297u\u2297u+u\u2297 ei \u2297u+u\u2297u\u2297 ei. 6 The sketch of Z\u0303i can be subsequently computed as\ns\u0303Z\u0303i = 1 2 s\u0303u \u2217 s\u0303u \u2217 s\u0303ei + 1 2 s\u03032,u\u25e6u \u2217 s\u0303ei + s\u03032,ei\u25e6u \u2217 s\u0303u + s\u03033,ei\u25e6u\u25e6u. (15)\nConsequently, T(I,u,u) \u2248 \u2329 F\u22121 ( F(s\u0303T) \u25e6 F(s\u0303u) ) , s\u03032,ei\u25e6u \u232a + 1\n6\n\u2329 F\u22121 ( F(s\u0303T) \u25e6 F(s\u0303u) \u25e6 F(s\u0303u) ) , s\u0303ei \u232a + 1\n6\n\u2329 F\u22121 ( F(s\u0303T) \u25e6 F(s\u03032,u\u25e6u) ) , s\u0303ei \u232a + \u3008s\u0303T, s\u03033,ei\u25e6u\u25e6u\u3009. (16)\nNote that all of s\u0303ei , s\u03032,ei\u25e6u and s\u03033,ei\u25e6u\u25e6u have exactly one nonzero entries. So we can pre-compute all terms on the left sides of inner products in Eq. (16) and then read off the values for each entry in T(I,u,u)."}, {"heading": "Appendix C Fast ALS: method and simulation result", "text": "In this section we describe how to use tensor sketching to accelerate the Alternating Least Squares (ALS) method for tensor CP decomposition. We also provide experimental results on synthetic data and compare our fast ALS implementation with the Matlab tensor toolbox [?, ?], which is widely considered to be the state-of-the-art for tensor decomposition.\nC.1 Alternating Least Squares Alternating Least Squares (ALS) is a popular method for tensor CP decompositions [19]. The algorithm maintains \u03bb \u2208 Rk, A,B,C \u2208 Rn\u00d7k and iteratively perform the following update steps:\nA\u0302 = T(1)(C B)(C>C \u25e6B>B)\u2020. (17)\nB\u0302 = T(1)(A\u0302 C)(A\u0302>A\u0302 \u25e6C>C)\u2020;\nC\u0302 = T(1)(B\u0302 A\u0302)(B\u0302>B\u0302 \u25e6 A\u0302>A\u0302)\u2020.\nAfter each update, \u03bb\u0302r is set to \u2016ar\u20162 (or \u2016br\u20162, \u2016cr\u20162) for r = 1, \u00b7 \u00b7 \u00b7 , k and the matrix A (or B,C) is normalized so that each column has unit norm. The final low-rank approximation is obtained by \u2211k i=1 \u03bb\u0302ia\u0302i \u2297 b\u0302i \u2297 c\u0302i.\nThere is no guarantee that ALS converges or gives a good tensor decomposition. Nevertheless, it works reasonably well in most applications [19]. In general ALS requiresO(T (n3k+k3)) computations andO(n3) storage, where T is the number of iterations.\nC.2 Accelerated ALS via sketching Similar to robust tensor power method, the ALS algorithm can be significantly accelerated by using the idea of sketching as shown in this work. However, for ALS we cannot use colliding hashes because though the input tensor T is symmetric, its CP decomposition is not since we maintain three different solution matrices A,B and C. As a result, we roll back to asymmetric tensor sketches defined in Eq. (1). Recall that given A,B,C \u2208 Rn\u00d7k we want to compute\nA\u0302 = T(1)(C B)(C>C \u25e6B>B)\u2020. (18) 6As long as A is symmetric, we have \u3008A,Yi\u3009 = \u3008A,Zi\u3009/3.\nAlgorithm 3 Fast ALS method\nResidual norm No. of wrong vectors Running time (min.) log2(b): 12 13 14 15 16 12 13 14 15 16 12 13 14 15 16\n\u03c3 = .0"}, {"heading": "1 B = 20 .71 .41 .25 .17 .12 10 9 7 6 4 .11 .22 .49 1.1 2.4", "text": "B = 30 .50 .34 .21 .14 .11 9 8 7 5 3 .17 .33 .75 1.6 3.5 B = 40 .46 .28 .17 .10 .07 9 8 6 5 1 .23 .45 1.0 2.2 4.7 Exact\u2020 .07 1 22.8\n\u03c3 = .1 B = 20 .88 .50 .35 .28 .23 10 8 7 6 6 .13 .32 .78 1.5 3.2 B = 30 .78 .44 .30 .24 .21 9 8 7 5 6 .21 .50 1.1 2.2 4.7 B = 40 .56 .38 .28 .19 .16 9 8 6 4 2 .29 .69 1.5 3.5 6.3 Exact\u2020 .17 2 32.3\n\u2020Calling cp als in Matlab tensor toolbox. It is run for exactly T = 30 iterations.\nWhen k is much smaller than the ambient tensor dimension n the computational bottleneck of Eq. (18) is T(1)(C B), which requires O(n3k) operations. Below we show how to use sketching to speed up this computation.\nLet x \u2208 Rn2 be one row in T(1) and consider (C B)>x. It can be shown that [15][ (C B)>x ] i\n= b>i Xci, \u2200i = 1, \u00b7 \u00b7 \u00b7 , k, (19) where X \u2208 Rn\u00d7n is the reshape of vector x. Subsequently, the product T(1)(C B) can be re-written as\nT(1)(C B) = [T(I, b1, c1); \u00b7 \u00b7 \u00b7 ; T(I, bk, ck)]. (20) Using Proposition 2 we can compute each of T(I, bi, ci) in O(n+ b log b) iterations. Note that in general bi 6= ci, but Proposition 2 still holds by replacing one of the two su sketches. As a result, T(1)(C B) can be computed in O(k(n + b log b)) operations once sT is computed. The pseudocode of fast ALS is listed in Algorithm 3. Its time complexity and space complexity are O(T (k(n+Bb log b) + k3)) (excluding the time for building sT) and O(Bb), respectively.\nC.3 Simulation results We compare the performance of fast ALS with a brute-force implementation under various hash length settings on synthetic datasets in Table 7. Settings for generating the synthetic dataset is exactly the same as in Section 5.1. We use the cp als routine in Matlab tensor toolbox as the reference brute-force implementation of ALS. For fair comparison, exactly T = 30 iterations are performed for both plain and accelerated ALS algorithms. Table 7 shows that when sketch length b is not too small, fast ALS achieves comparable accuracy with exact methods while being much faster in terms of running time.\nAlgorithm 4 Fast spectral LDA 1: Input: Unlabeled documents, V , K, \u03b10, B, b. 2: Compute empirical moments M\u03021 and M\u03022 defined in Eq. (21,22). 3: [U,S,V]\u2190 truncatedSVD(M\u03022, k); Wik \u2190 Uik\u221a\u03c3k . 4: Build B tensor sketches of M\u03023(W,W,W). 5: Find CP decomposition {\u03bbi}ki=1,A = B = C = {vi}ki=1 of M\u03023(W,W,W) using either fast tensor\npower method or fast ALS method. 6: Output: estimates of prior parameters \u03b1\u0302i = 4\u03b10(\u03b10+1)(\u03b10+2)2\u03bb2i and topic distributions \u00b5\u0302i = \u03b10+2 2 \u03bbi(W \u2020)>vi."}, {"heading": "Appendix D Spectral LDA and fast spectral LDA", "text": "Latent Dirichlet Allocation (LDA, [3]) is a powerful tool in topic modeling. In this section we first review the LDA model and introduce the tensor decomposition method for learning LDA models, which was proposed in [1]. We then provide full details of our proposed fast spectral LDA algorithm. Pseudocode for fast spectral LDA is listed in Algorithm 4.\nD.1 LDA and spectral LDA LDA models a collection of documents by a topic dictionary \u03a6 \u2208 RV\u00d7K and a Dirichlet prior\u03b1 \u2208 Rk, where V is the vocabulary size and k is the number of topics. Each column in \u03a6 is a probability distribution (i.e., non-negative and sum to one) representing the word distribution of a particular topic. For each document d, a topic mixing vector hd \u2208 Rk is first sampled from a Dirichlet distribution parameterized by \u03b1. Afterwards, words in document d i.i.d. sampled from a categorical distribution parameterized by \u03a6hd.\nA spectral method for LDA based on 3rd-order robust tensor decomposition was proposed in [1] to provably learn LDA model parameters from a polynomial number of training documents. Let x \u2208 RV represent a single word; that is, for word w we have xw = 1 and xw\u2032 = 0 for all w\u2032 6= w. Define first, second and third order moments M1,M2 and M3 as follows:\nM1 = E[x1]; (21) M2 = E[x1 \u2297 x2]\u2212 \u03b10\n\u03b10 + 1 M1 \u2297M1; (22)\nM3 = E[x1 \u2297 x2 \u2297 x3]\u2212 \u03b10\n\u03b10 + 2 (E[x1 \u2297 x2 \u2297M1] + E[x1 \u2297M1 \u2297 x2] + E[M1 \u2297 x1 \u2297 x2])\n+ 2\u03b120\n(\u03b10 + 1)(\u03b10 + 2) M1 \u2297M1 \u2297M1. (23) Here \u03b10 = \u2211 k \u03b1k is assumed to be a known quantity. Using elementary algebra it can be shown that\nM2 = 1\n\u03b10(\u03b10 + 1) k\u2211 i=1 \u03b1i\u00b5i\u00b5 > i ; (24)\nM3 = 2\n\u03b10(\u03b10 + 1)(\u03b10 + 2) k\u2211 i=1 \u03b1i\u00b5i \u2297 \u00b5i \u2297 \u00b5i. (25)\nTo extract topic vectors {\u00b5i}ki=1 from M2 and M3, a simultaneous diagonalization procedure is carried out. More specifically, the algorithm first finds a whitening matrix W \u2208 RV\u00d7K with orthonormal columns such that W>M2W = IK\u00d7K . In practice, this step can be completed by performing a truncated SVD on M2, M2 = UK\u03a3KVK , and set Wik = Uik/ \u221a \u03a3kk. Afterwards, tensor CP decomposition is performed on the whitened third order moment M3(W,W,W) 7 to obtain a set of eigenvectors {vk}Kk=1. The topic vectors {\u00b5k}Kk=1 can be subsequently obtained by multiplying {vk}Kk=1 with the pseudoinverse of W. Note 7For a tensor T \u2208 RV\u00d7V\u00d7V and a matrix W \u2208 RV\u00d7k , the product Q = T(W,W,W) \u2208 Rk\u00d7k\u00d7k is defined as Qi1,i2,i3 =\u2211V j1,j2,j3=1 Tj1,j2,j3Wj1,i1Wj2,i2Wj3,i3 .\nthat Eq. (21,22,23) are defined in exact word moments. In practice we use empirical moments (e.g., word frequency vector and co-occurrence matrix) to approximate these exact moments.\nD.2 Fast spectral LDA To further accelerate the spectral method mentioned in the previous section, it helps to first identify computational bottlenecks of spectral LDA. In general, the computation of M\u03021, M\u03022 and the whitening step are not the computational bottleneck when V is not too large and each document is not too long. The bottleneck comes from the computation of (the sketch of) M\u03023(W,W,W) and its tensor decomposition. By Eq. (23), the computation of M\u03023(W,W,W) reduces to computing M\u0302\u229731 (W,W,W), E\u0302[x1 \u2297 x2 \u2297 M\u03021](W,W,W), 8 and E\u0302[x1 \u2297 x2 \u2297 x3](W,W,W). The first term M\u0302\u229731 (W,W,W) poses no particular challenge as it can be written as (W>M\u03021)\u22973. Its sketch can then be efficiently obtained by applying techniques in Section 3.2. In the remainder of this section we focus on efficient computation of the sketch of the other two terms mentioned above.\nWe first show how to efficiently sketching E\u0302[x1 \u2297 x2 \u2297 x3](W,W,W) given the whitening matrix W and D training documents. Let TE\u0302[x1 \u2297 x2 \u2297 x3](W,W,W) denote the whitened k \u00d7 k \u00d7 k tensor to be sketched and write T = \u2211D d=1 Td, where Td is the contribution of the dth training document to T. By definition, Td can be expressed as Td = Nd(W,W,W), where W is the V \u00d7 k whitening matrix and Nd is the V \u00d7 V \u00d7 V empirical moment tensor computed on the dth document. More specifically, for i, j, k \u2208 {1, \u00b7 \u00b7 \u00b7 , V } we have\nNd,ijk = 1\nmd(md \u2212 1)(md \u2212 2)  ndi(ndj \u2212 1)(ndk \u2212 2), i = j = k; ndi(ndi \u2212 1)ndk, i = j, j 6= k; ndindj(ndj \u2212 1) j = k, i 6= j; ndi(ndi \u2212 1)ndj , i = k, i 6= j; ndindjndk, otherwise.\nHere md is the length (i.e., number of words) of document d and nd \u2208 RV is the corresponding word count vector. Previous straightforward implementation require at least O(k3 + mdk2) operations per document to build the tensor T and O(k4LT ) to decompose it [29, 28], which is prohibitively slow for real-world applications. In section 3 we discussed how to decompose a tensor efficiently once we have its sketch. We now show how to build the sketch of T efficiently from document word counts {nd}Dd=1.\nBy definition, Td can be decomposed as\nTd = p \u22973 \u2212 V\u2211 i=1 ni(wi \u2297wi \u2297 p+wi \u2297 p\u2297wi+p\u2297wi \u2297wi) + V\u2211 i=1 2niw \u22973 i , (26)\nwhere p = Wn and wi \u2208 Rk is the ith row of the whitening matrix W. A direct implementation is to sketch each of the low-rank components in Eq. (26) and compute their sum. Since there are O(md) tensors, building the sketch of Td requires O(md) FFTs, which is unsatisfactory. However, note that {wi}Vi=1 are fixed and shared across documents. So when scanning the documents we maintain the sum of ni and nip and add the incremental after all documents are scanned. In this way, we only need O(1) FFT per document with an additional O(V ) FFTs. Since the total number of documents D is usually much larger than V , this provides significant speed-ups over the naive method that sketches each term in Eq. (26) independently. As a result, the sketch of T can be computed in O(k( \u2211 dmd) + (D+ V )b log b) operations, which is much more\nefficient than the O(k2( \u2211 dmd) +Dk\n3) brute-force computation. We next turn to the term E\u0302[x1 \u2297 x2 \u2297 M\u03021](W,W,W). Fix a document d and let p = Wnd. Define\nq = WM\u03021. By definition, the whitened empirical moment can be decomposed as\nE\u0302[x1 \u2297 x2 \u2297 M\u03021](W,W,W) = V\u2211 i=1 nip\u2297 p\u2297 q, (27)\n8and also E\u0302[x1 \u2297 M\u03021 \u2297 x2](W,W,W), E\u0302[M\u03021 \u2297 x1 \u2297 x2](W,W,W) by symmetry.\nNote that Eq. (27) is very similar to Eq. (26). Consequently, we can apply the same trick (i.e., adding p and nip up before doing sketching or FFT) to compute Eq. (27) efficiently."}, {"heading": "Appendix E Proofs", "text": "E.1 Proofs of some technical propositions\nProof of Proposition 1. Fix i1, \u00b7 \u00b7 \u00b7 , ip \u2208 {1, \u00b7 \u00b7 \u00b7 , n}. By definition, T\u0302i1,\u00b7\u00b7\u00b7 ,ip can be written as T\u0302i1,\u00b7\u00b7\u00b7 ,ip = \u2211\ni\u20321,\u00b7\u00b7\u00b7 ,i\u2032p\u2208[n]\n\u03be1(i1)\u03be1(i \u2032 1) \u00b7 \u00b7 \u00b7 \u03bep(ip)\u03bep(i\u2032p)Ti\u20321,\u00b7\u00b7\u00b7 ,i\u2032p\u03b4(i, i \u2032),\nwhere \u03b4(i, i\u2032) = 1 if H(i1, \u00b7 \u00b7 \u00b7 , ip) = H(i\u20321, \u00b7 \u00b7 \u00b7 , i\u2032p) and \u03b4(i, i \u2032) = 0 otherwise. Therefore, EH,\u03be[T\u0302i1,\u00b7\u00b7\u00b7 ,ip ] = \u2211\ni\u20321,\u00b7\u00b7\u00b7 ,i\u2032p\u2208[n]\nTi\u20321,\u00b7\u00b7\u00b7 ,i\u2032p \u00b7 E\u03be[\u03be1(i1)\u03be1(i \u2032 1) \u00b7 \u00b7 \u00b7 \u03bep(ip)\u03bep(i\u2032p)] \u00b7 Eh[\u03b4(i, i \u2032)]\n= Ti1,\u00b7\u00b7\u00b7 ,ip . (28) Here for the last equation we used the fact that \u03bej , \u03be\u2032j are independent and furthermore E[\u03bej(i)\u03bej(i\u2032)] = 1 if i = j and E[\u03bej(i)\u03bej(i\u2032)] = 0 otherwise. Consequently, we have shown that T\u0302i1,\u00b7\u00b7\u00b7 ,ip is an unbiased estimator of the true value Ti1,\u00b7\u00b7\u00b7 ,ip .\nWe next turn to bound the variance of T\u0302i1,\u00b7\u00b7\u00b7 ,ip . Let i = (i1, \u00b7 \u00b7 \u00b7 , ip), i \u2032 = (i\u20321, \u00b7 \u00b7 \u00b7 , i\u2032p) and i \u2032\u2032 = (i\u2032\u20321 , \u00b7 \u00b7 \u00b7 , i\u2032\u2032p). Define \u03be(i) = \u03be1(i1) \u00b7 \u00b7 \u00b7 \u03bep(ip). We then have\nEH,\u03be[T\u03022i ] = \u2211\ni\u2032,i\u2032\u2032\u2208[n]\nE\u03be[\u03be(i\u2032)\u03be(i\u2032\u2032)] \u00b7 EH [\u03b4(i, i\u2032)\u03b4(i, i\u2032\u2032)] \u00b7Ti\u2032Ti\u2032\u2032\n= \u2211 i\u2032\u2208[n] E[\u03b4(i, i\u2032)] \u00b7T2i\u2032\n= T2i + 1\nb \u2211 i\u2032 6=i T2i\u2032\n\u2264 T2i + \u2016T\u20162F b .\nHere in the second equation we apply E[\u03be(i\u2032)\u03be(i\u2032\u2032)] = \u03b4(i\u2032, i\u2032\u2032) and the third equation holds due to\nE[\u03b4(i, i\u2032)] = Pr H\n[H(i1, \u00b7 \u00b7 \u00b7 , ip) = H(i\u20321, \u00b7 \u00b7 \u00b7 , i\u2032p)] = { 1, i = i\u2032; 1/b, i 6= i\u2032.\nConsequently,\nVH,\u03be[T\u0302i1,\u00b7\u00b7\u00b7 ,ip ] \u2264 \u2016T\u20162F b . (29)\nFinally, combining Eq. (28) and (29) and applying Chebyshev\u2019s inequality we obtain\nPr H,\u03be [\u2223\u2223T\u0302i1,\u00b7\u00b7\u00b7 ,ip \u2212Ti1,\u00b7\u00b7\u00b7 ,ip \u2223\u2223 > ] \u2264 \u2016T\u20162Fb 2 for every > 0.\nProof of Proposition 3. We prove the proposition for the case q = 2 (i.e., H\u0303 is 2-wise independent). This suffices for our purpose in this paper and generalization to q > 2 cases is straightforward. For notational simplicity we omit all modulo operators. Consider two p-tuples l = (l1, \u00b7 \u00b7 \u00b7 , lp) and l\u2032 = (l\u20321, \u00b7 \u00b7 \u00b7 , l\u2032p) such that l 6= l\u2032. Since H\u0303 is permutation invariant, we assume without loss of generality that for some s < p and 1 \u2264 i \u2264 s we have li = l\u2032i. Fix t, t\u2032 \u2208 [b]. We then have\nPr[H\u0303(l) = t \u2227 H\u0303(l\u2032) = t\u2032] = \u2211 a \u2211 h(l1)+\u00b7\u00b7\u00b7+h(ls)=a Pr[h(l1) + \u00b7 \u00b7 \u00b7+ h(ls) = a]\n\u00b7 \u2211\nrs+1+\u00b7\u00b7\u00b7+rp=t\u2212a r\u2032s+1+\u00b7\u00b7\u00b7+r \u2032 p=t \u2032\u2212a\nPr[h(ls+1) = r1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 h(lp) = rp \u2227 h(l\u2032s+1) = r\u20321 \u2227 \u00b7 \u00b7 \u00b7 \u2227 h(l\u2032p) = r\u2032p]. (30)\nSince h is 2p-wise independent, we have Pr[h(l1) + \u00b7 \u00b7 \u00b7+ h(ls) = a] = \u2211\nr1+\u00b7\u00b7\u00b7+rs=a Pr[h(l1) = r1 \u2227 \u00b7 \u00b7 \u00b7h(ls) = rs] = bs\u22121 \u00b7\n1 bs = 1 b ;\n\u2211 rs+1+\u00b7\u00b7\u00b7+rp=t\u2212a r\u2032s+1+\u00b7\u00b7\u00b7+r \u2032 p=t\u2212a Pr[h(ls+1) = r1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 h(lp) = rp \u2227 h(l\u2032s+1) = r\u20321 \u2227 \u00b7 \u00b7 \u00b7 \u2227 h(l\u2032p) = r\u2032p]\n= b2(p\u2212s\u22121) \u00b7 1 b2(p\u2212s) = 1 b2 .\nSumming everything up we get Pr[H\u0303(l) = t \u2227 H\u0303(l\u2032) = t\u2032] = 1/b2, which is to be demonstrated.\nProof of Proposition 2. Since both FFT and inverse FFT preserve inner products, we have \u3008sT, s1,u \u2217 s2,u \u2217 s3,ei\u3009 = \u3008F(sT),F(s1,u) \u25e6 F(s2,u) \u25e6 F(s3,ei)\u3009\n= \u3008F(sT) \u25e6 F(s1,u) \u25e6 F(s2,u),F(s3,ei)\u3009 = \u3008F\u22121(F(sT) \u25e6 F(s1,u) \u25e6 F(s2,u)), s3,ei\u3009.\nE.2 Analysis of tensor sketch approximation error Proofs of Theorem 1 is based on the following two key lemmas, which states that \u3008s\u0303A, s\u0303B\u0303\u3009 is a consistent estimator of the true inner product \u3008A,B\u3009; furthermore, the variance of the estimator decays linearly with the hash length b. The lemmas are interesting in their own right, providing useful tools for proving approximation accuracy in a wide range of applications when colliding hash and symmetric sketches are used.\nLemma 1. Suppose A,B \u2208 \u2297pRn are two symmetric real tensors and let s\u0303A, s\u0303B\u0303 \u2208 Cb be the symmetric tensor sketches of A and B\u0303. That is,\ns\u0303A(t) = \u2211\nH\u0303(i1,\u00b7\u00b7\u00b7 ,ip)=t\n\u03c3i1 \u00b7 \u00b7 \u00b7\u03c3ipAi1,\u00b7\u00b7\u00b7 ,ip ; (31)\ns\u0303B\u0303(t) = \u2211\nH\u0303(i1,\u00b7\u00b7\u00b7 ,ip)=t i1\u2264\u00b7\u00b7\u00b7\u2264ip\n\u03c3i1 \u00b7 \u00b7 \u00b7\u03c3ipBi1,\u00b7\u00b7\u00b7 ,ip . (32)\nAssume H\u0303(i1, \u00b7 \u00b7 \u00b7 , ip) = (h(i1) + \u00b7 \u00b7 \u00b7 + h(ip)) mod b are drawn from a 2-wise independent hash family. Then the following holds:\nEh,\u03c3 [ \u3008s\u0303A, s\u0303B\u0303\u3009 ] = \u3008A,B\u3009, (33)\nVh,\u03c3 [ \u3008s\u0303A, s\u0303B\u0303\u3009 ] \u2264 4 p\u2016A\u20162F \u2016B\u20162F b . (34)\nLemma 2. Following notations and assumptions in Lemma 1. Let {Ai}mi=1 and {Bi}mi=1 be symmetric real n\u00d7 n\u00d7 n tensors and fix real vector w \u2208 Rm. Then we have\nE \u2211 i,j wiwj\u3008s\u0303Ai , s\u0303B\u0303j \u3009  = \u2211 i,j wiwj\u3008Ai,Bj\u3009; (35)\nV \u2211 i,j wiwj\u3008s\u0303Ai , s\u0303B\u0303j \u3009  \u2264 4p\u2016w\u20164(maxi \u2016Ai\u20162F )(maxi \u2016Bi\u20162F ) b . (36)\nProof of Lemma 1. We first define some notations. Let l = (l1, \u00b7 \u00b7 \u00b7 , lp) \u2208 [d]p be a p-tuple denoting a multiindex. Define Al := Al1,\u00b7\u00b7\u00b7 ,lp and \u03c3(l) := \u03c3l1 \u00b7 \u00b7 \u00b7\u03c3lp . For l, l\n\u2032 \u2208 [n]p, define \u03b4(l, l\u2032) = 1 if h(l1) + \u00b7 \u00b7 \u00b7 + h(lp) \u2261 h(l\u20321) + \u00b7 \u00b7 \u00b7 + h(l\u2032p)( mod b) and \u03b4(l, l\n\u2032) = 0 otherwise. For a p-tuple l \u2208 [n]p, let L(l) \u2208 [n]p denote the p-tuple obtained by re-ordering indices in l in ascending order. Let M(l) \u2208 Nb denote the \u201cexpanded version\u201d of l. That is, [M(l)]i denote the number of occurrences of the index i in l. By definition, \u2016M(l)\u20161 = p. Finally, by definition B\u0303l\u2032 = Bl\u2032 if l\u2032 = L(l\u2032) and B\u0303l\u2032 = 0 otherwise.\nEq. (33) is easy to prove. By definition and linearity of expectation we have E[\u3008s\u0303A, s\u0303B\u0303\u3009] = \u2211 l,l\u2032 \u03b4(l, l\u2032)\u03c3(l)Al\u03c3\u0304(l \u2032)B\u0303l\u2032 . (37)\nNote that \u03b4 and \u03c3 are independent and E\u03c3[\u03c3(l)\u03c3(l\u2032)] = {\n1, if L(l) = L(l\u2032); 0, otherwise. (38)\nAlso \u03b4(l, l\u2032) = 1 with probability 1 whenever L(l) = L(l\u2032). Note that B\u0303l\u2032 = 0 whenever l\u2032 6= L(l\u2032). Consequently,\nE[\u3008s\u0303A, s\u0303B\u0303\u3009] = \u2211 l\u2208[n]p AlB\u0303L(l) = \u3008A,B\u3009. (39)\nFor the variance, we have the following expression for E[\u3008s\u0303A, s\u0303B\u0303\u3009 2]:\nE[\u3008s\u0303A, s\u0303B\u0303\u3009 2] = \u2211 l,l\u2032,r,r\u2032 E[\u03b4(l, l\u2032)\u03b4(r, r\u2032)] \u00b7 E[\u03c3(l)\u03c3\u0304(l\u2032)\u03c3\u0304(r)\u03c3(r\u2032)] \u00b7AlArB\u0303l\u2032B\u0303r\u2032 (40)\n=: \u2211\nl,l\u2032,r,r\u2032\nE[t(l, l\u2032, r, r\u2032)]. (41)\nWe remark that E[\u03c3(l)\u03c3\u0304(l\u2032)\u03c3\u0304(r)\u03c3(r\u2032)] = 0 ifM(l)\u2212M(l\u2032) 6=M(r)\u2212M(r\u2032). In the remainder of the proof we will assume thatM(l)\u2212M(l\u2032) =M(r)\u2212M(r\u2032). This can be further categorized into two cases:\nCase 1: l\u2032 = L(l) and r\u2032 = L(r). By definition E[\u03c3(l)\u03c3\u0304(l\u2032)\u03c3(r)\u03c3\u0304(r\u2032)] = 1 and E[\u03b4(l, l\u2032)\u03b4(r, r\u2032)] = 1. Subsequently E[t(l, l\u2032, r, r\u2032)] = AlArB\u0303l\u2032B\u0303r\u2032 and hence\u2211\nl,r,l\u2032=L(l),r\u2032=L(r) E[t(l, l\u2032, r, r\u2032)] = \u2211 l,r AlArBlBr = \u3008A,B\u30092. (42)\nCase 2: l\u2032 6= L(l) or r\u2032 6= L(r). SinceM(l)\u2212M(l\u2032) =M(r)\u2212M(r\u2032) 6= 0 we have E[\u03b4(l, l\u2032)\u03b4(r, r\u2032)] = 1/b because h is a 2-wise independent hash function. In addition, E[|\u03c3(l)\u03c3\u0304(l\u2032)\u03c3(r)\u03c3\u0304(r\u2032)|] \u2264 1.\nTo enumerate all (l, l\u2032, r, r\u2032) tuples that satisfy the colliding condition M(l) \u2212 M(l\u2032) = M(r) \u2212 M(r\u2032) 6= 0, we fix 9 \u2016M(l) \u2212M(l\u2032)\u20161 = 2q and fix q positions each in l and r (for l\u2032 and r\u2032 the positions of these indices are automatically fixed because indices in l\u2032 and r\u2032 must be in ascending order). Without loss of generality assume the fixed q positions for both l and r are the first q indices. The 4-tuple (l, r, l\u2032, r\u2032) with \u2016M(l)\u2212M(l\u2032)\u20161 = 2q can then be enumerated as follows:\u2211\nl,r,l\u2032,r\u2032\nM(l)\u2212M(l\u2032)=M(r)\u2212M(r\u2032) \u2016M(l)\u2212M(l\u2032)\u20161=2q\nt(l, l\u2032, r, r\u2032)\n= \u2211 i\u2208[n]q \u2211 j\u2208[n]q \u2211 l\u2208[n]p\u2212q\nr\u2208[n]p\u2212q\nt(i \u25e6 l,L(j \u25e6 l), i \u25e6 r,L(j \u25e6 r))\n9Note that sum(M(l)) = sum(M(l\u2032)) and hence \u2016M(l) \u2212M(l\u2032)\u20161 must be even. Furthermore, the sum of positive entries in (M(l)\u2212M(l\u2032)) equals the sum of negative entries.\n\u2264 1 b \u2211 i,j\u2208[n]q\nl,r\u2208[n]p\u2212q\nAi\u25e6lAi\u25e6rBj\u25e6lBj\u25e6r\n= 1\nb \u2211 i,j\u2208[n]q \u3008A(ei1 , \u00b7 \u00b7 \u00b7 , eiq , I, \u00b7 \u00b7 \u00b7 , I),B(ej1 , \u00b7 \u00b7 \u00b7 , ejq , I, \u00b7 \u00b7 \u00b7 , I)\u30092\n\u2264 1 b \u2211 i,j\u2208[n]q \u2016A(ei1 , \u00b7 \u00b7 \u00b7 , eiq , I, \u00b7 \u00b7 \u00b7 , I)\u20162F \u2016B(ej1 , \u00b7 \u00b7 \u00b7 , ejq , I, \u00b7 \u00b7 \u00b7 , I)\u20162F\n= \u2016A\u20162F \u2016B\u20162F\nb . (43)\nHere \u25e6 denotes concatenation, that is, i \u25e6 l = (i1, \u00b7 \u00b7 \u00b7 , iq, l1, \u00b7 \u00b7 \u00b7 , lp\u2212q) \u2208 [n]p. The fourth equation is Cauchy-Schwartz inequality. Finally note that there are no more than 4p ways of assigning q positions to l and l\u2032 each. Combining Eq. (42) and (43) we get\nV[\u3008s\u0303A, s\u0303B\u0303\u3009] = E[\u3008s\u0303A, s\u0303B\u0303\u3009 2]\u2212 \u3008A,B\u30092 \u2264 4 p\u2016A\u20162F \u2016B\u20162F b ,\nwhich completes the proof.\nProof of Lemma 2. Eq. (35) immediately follows Eq. (31) by adding everything together. For the variance bound we cannot use the same argument because in general the m2 random variables are neither independent nor uncorrelated. Instead, we compute the variance by definition. First we compute the expected square term as follows:\nE  \u2211\ni,j\nwiwj\u3008s\u0303Ai , s\u0303B\u0303j \u3009\n2 \n= \u2211\ni,j,i\u2032,j\u2032 l,l\u2032,r,r\u2032\nwiwjwi\u2032wj\u2032 \u00b7 E[\u03b4(l, l\u2032)\u03b4(r, r\u2032)] \u00b7 E[\u03c3(l)\u03c3\u0304(l\u2032)\u03c3\u0304(r)\u03c3(r\u2032)] \u00b7 [Ai]l[Ai\u2032 ]r[B\u0303j ]l\u2032 [B\u0303j\u2032 ]r\u2032 . (44)\nDefine X = \u2211 i wiAi and Y = \u2211 i wiBi. The above equation can then be simplified as\nE  \u2211\ni,j\nwiwj\u3008s\u0303Ai , s\u0303B\u0303j \u3009\n2  = \u2211\nl,l\u2032,r,r\u2032\nE[\u03b4(l, l\u2032)\u03b4(r, r\u2032)] \u00b7 E[\u03c3(l)\u03c3\u0304(l\u2032)\u03c3\u0304(r)\u03c3(r\u2032)] \u00b7XlXrY\u0303l\u2032Y\u0303r\u2032 . (45)\nApplying Lemma 1 we have\nV \u2211 i,j wiwj\u3008s\u0303Ai , s\u0303B\u0303j \u3009  \u2264 4p\u2016X\u20162F \u2016Y\u20162F b . (46)\nFinally, note that \u2016X\u20162F = \u2211 i,j wiwj\u3008Ai,Aj\u3009 \u2264 \u2211 i,j wiwj\u2016Ai\u2016F \u2016Aj\u2016F \u2264 \u2016w\u20162 max i \u2016Ai\u20162F . (47)\nWith Lemma 1 and 2, we can easily prove Theorem 1.\nProof of Theorem 1. First we prove the \u03b51(u) bound. Let A = T and B = u\u22973. Note that \u2016A\u2016F = \u2016T\u2016F and \u2016B\u2016F = \u2016u\u20162 = 1. Note that [T(I,u,u)]i = T(ei,u,u). Next we consider \u03b52(u) and let A = T, B = ei \u2297 u \u2297 u. Again we have \u2016A\u2016F = \u2016T\u2016F and \u2016B\u2016F = 1. A union bound over all i = 1, \u00b7 \u00b7 \u00b7 , n yields the result. For the inequality involving w we apply Lemma 2.\nE.3 Analysis of fast robust tensor power method In this section, we prove Theorem 3, a more refined version of Theorem 2 in Section 4.2. We structure the section by first demonstrating the convergence behavior of noisy tensor power method, and then show how error accumulates with deflation. Finally, the overall bound is derived by combining these two parts.\nE.3.1 Recovering the principal eigenvector\nDefine the angle between two vectors v and u to be \u03b8 (v,u) . First, in Lemma 3 we show that if the initialization vector u0 is randomly chosen from the unit sphere, then the angle \u03b8 between the iteratively updated vector ut and the largest eigenvector of tensor T, v1, will decrease to a point that tan \u03b8 (v1,ut) < 1. Afterwards, in Lemma 4, we use a similar approach as in [?] to prove that the error between the final estimation and the ground truth is bounded.\nSuppose T is the exact low-rank ground truth tensor and Each noisy tensor update can then be written as u\u0303t+1 = T(I,ut,ut) + \u03b5\u0303(ut), (48)\nwhere \u03b5\u0303(ut) = E(I,ut,ut) + \u03b52,T (ut) is the noise coming from statistical and tensor sketch approximation error.\nBefore presenting key lemmas, we first define \u03b3-separation, a concept introduced in [1].\nDefinition 1 (\u03b3-separation, [1]). Fix i\u2217 \u2208 [k], u \u2208 Rn and \u03b3 > 0. u is \u03b3-separated with respect to vi\u2217 if the following holds:\n\u03bbi\u2217\u3008u,vi\u2217\u3009 \u2212 max i\u2208[k]\\{i\u2217} \u03bbi\u3008u,vi\u3009 \u2265 \u03b3\u03bbi\u2217\u3008u,vi\u2217\u3009. (49)\nLemma 3 analyzes the first phase of the noisy tensor power algorithm. It shows that if the initialization vector u0 is \u03b3-separated with respect to v1 and the magnitude of noise \u03b5\u0303(ut) is small at each iteration t, then after a short number of iterations we will have inner product between ut and v1 at least a constant.\nLemma 3. Let {v1,v2, \u00b7 \u00b7 \u00b7 ,vk} and {\u03bb1, \u03bb2, \u00b7 \u00b7 \u00b7 , \u03bbk} be eigenvectors and eigenvalues of tensor T \u2208 Rn\u00d7n\u00d7n, where \u03bb1 |\u3008v1,u0\u3009| = max\ni\u2208[k] \u03bbi |\u3008vi,u0\u3009| . Denote V = (v1, \u00b7 \u00b7 \u00b7 ,vk) \u2208 Rn\u00d7k as the matrix for\neigenvectors. Suppose that for every iteration t the noise satisfies\u2223\u2223\u3008vi, \u03b5\u0303(ut)\u3009\u2223\u2223 \u2264 1 \u2200 i \u2208 [n] and \u2225\u2225V>\u03b5\u0303(ut)\u2225\u2225 \u2264 2; (50) suppose also the initialization u0 is \u03b3-separated with respect to v1 for some \u03b3 \u2208 (0.5, 1). If tan \u03b8 (v1,u0) > 1, and\n1 \u2264 min\n( 1\n4 maxi\u2208[k] \u03bbi\n\u03bb1 + 2\n, 1\u2212 (1 + \u03b1)/2\n2\n) \u03bb1 \u3008v1,u0\u30092 and 2 \u2264\n1\u2212 (1 + \u03b1)/2 2 \u221a 2(1 + \u03b1) \u03bb1 |\u3008v1,u0\u3009| (51)\nfor some \u03b1 > 0, then for a small constant \u03c1 > 0, there exists a T > log1+\u03b1 (1 + \u03c1) tan \u03b8 (v1,u0) such that after T iteration, we have tan \u03b8 (v1,uT ) < 11+\u03c1 ,\nProof. Let u\u0303t+1 = T (I,ut,ut) + \u03b5\u0303(ut) and ut+1 = u\u0303t+1/ \u2016u\u0303t+1\u2016 . For \u03b1 \u2208 (0, 1), we try to prove that there exists a T such that for t > T\n1\ntan \u03b8 (v1,ut+1) = |\u3008v1,ut+1\u3009|( 1\u2212 \u3008v1,ut+1\u30092 )1/2 = |\u3008v1, u\u0303t+1\u3009|( n\u2211 i=2 \u3008vi, u\u0303t+1\u30092 )1/2 \u2265 1. (52)\nFirst we examine the numerator. Using the assumption \u2223\u2223\u3008vi, \u03b5\u0303(ut)\u3009\u2223\u2223 \u2264 1 and the fact that \u3008vi, u\u0303t+1\u3009 = \u03bbi \u3008vi,ut\u30092 + \u3008vi, \u03b5\u0303(ut)\u3009, we have |\u3008vi, u\u0303t+1\u3009| \u2265 \u03bbi \u3008vi,ut\u30092 \u2212 1 \u2265 |\u3008vi,ut\u3009| (\u03bbi |\u3008vi,ut\u3009| \u2212 1/ |\u3008vi,ut\u3009|) . (53) For the denominator, by Ho\u0308lder\u2019s inequality we have( n\u2211 i=2 \u3008vi, u\u0303t+1\u30092 )1/2 = ( n\u2211 i=2 ( \u03bbi \u3008vi,ut\u30092 + \u3008vi, \u03b5\u0303(ut)\u3009 )1/2) (54)\n\u2264 ( n\u2211 i=2 \u03bb2i \u3008vi,ut\u3009 4 )1/2 + ( n\u2211 i=2 \u3008vi, \u03b5\u0303(ut)\u30092 )1/2\n(55)\n\u2264 max i 6=1 \u03bbi |\u3008vi,ut\u3009| ( n\u2211 i=2 \u3008vi,ut\u30092 )1/2 + 2 (56)\n\u2264 ( 1\u2212 \u3008v1,ut\u30092 )1/2(\nmax i 6=1\n\u03bbi |\u3008vi,ut\u3009|+ 2/ ( 1\u2212 \u3008v1,ut\u30092 )1/2)\n(57)\nEquation (53) and (54) yield 1\ntan \u03b8 (v1,ut+1) \u2265 |\u3008v1,ut\u3009|( 1\u2212 \u3008v1,ut\u30092 )1/2 \u03bb1 |\u3008v1,ut\u3009| \u2212 1/ |\u3008v1,ut\u3009|\nmax i 6=1\n\u03bbi |\u3008vi,ut\u3009|+ 2/ ( 1\u2212 \u3008v1,ut\u30092 )1/2 (58)\n= 1\ntan \u03b8 (v1,ut)\n\u03bb1 |\u3008v1,ut\u3009| \u2212 1/ |\u3008v1,ut\u3009|\nmax i 6=1\n\u03bbi |\u3008vi,ut\u3009|+ 2/ ( 1\u2212 \u3008v1,ut\u30092 )1/2 (59)\nTo prove that the second term is larger than 1 +\u03b1, we first show that when t = 0, the inequality holds. Since the initialization vector is a \u03b3\u2212separated vector, we have\n\u03bb1 |\u3008v1,u0\u3009| \u2212max i\u2208[k] \u03bbi |\u3008vi,u0\u3009| \u2265 \u03b3\u03bb1 |\u3008v1,u0\u3009| , (60)\nmax i\u2208[k]\n\u03bbi |\u3008vi,u0\u3009| \u2264 (1\u2212 \u03b3)\u03bb1 |\u3008v1,u0\u3009| \u2264 0.5\u03bb1 |\u3008v1,u0\u3009| , (61)\nthe last inequality holds since \u03b3 > 0.5. Note that we assume tan \u03b8 (v1,u0) > 1 and hence \u3008v1,u0\u30092 < 0.5. Therefore,\n2 \u2264 1\u2212 (1 + \u03b1)/2 2 \u221a 2(1 + \u03b1) \u03bb1 |\u3008v1,u0\u3009| \u2264\n( 1\u2212 \u3008v1,u0\u30092 )1/2 (1\u2212 (1 + \u03b1)/2)\n2(1 + \u03b1) \u03bb1 |\u3008v1,u0\u3009| . (62)\nThus, for t = 0, using the condition for 1 and 2 we have \u03bb1 |\u3008vi,u0\u3009| \u2212 1/ |\u3008vi,u0\u3009|\nmax i6=1\n\u03bbi |\u3008vi,u0\u3009|+ 2/ ( 1\u2212 \u3008v1,u0\u30092 )1/2 \u2265 \u03bb1 |\u3008vi,u0\u3009| \u2212 1/ |\u3008vi,u0\u3009| 0.5\u03bb1 |\u3008v1,u0\u3009|+ 2/ ( 1\u2212 \u3008v1,u0\u30092 )1/2 \u2265 1 + \u03b1.\n(63)\nThe result yields 1/ tan \u03b8 (v1,u1) > (1+\u03b1)/ tan \u03b8 (v1,u0) . This also indicates that |\u3008v1,u1\u3009| > |\u3008v1,u0\u3009| , which implies that\n1 \u2264 min\n( 1\n4 maxi\u2208[k] \u03bbi\n\u03bb1 + 2\n, 1\u2212 (1 + \u03b1)/2\n2\n) \u03bb1 \u3008v1,ut\u30092 and 2 \u2264\n1\u2212 (1 + \u03b1)/2 2 \u221a 2(1 + \u03b1) \u03bb1 |\u3008v1,ut\u3009| (64)\nalso holds for t = 1. Next we need to make sure that for t \u2265 0 max i 6=1 \u03bbi |\u3008vi,ut\u3009| \u2264 0.5\u03bb1 |\u3008v1,ut\u3009| . (65)\nIn other words, we need to show that \u03bb1|\u3008v1,ut\u3009|max i6=1 \u03bbi|\u3008vi,ut\u3009| \u2265 2. From Equation (61), for t = 0, \u03bb1|\u3008v1,ut\u3009| max i6=1 \u03bbi|\u3008vi,ut\u3009| \u2265 1 1\u2212\u03b3 \u2265 2. For every i \u2208 [k], |\u3008vi, u\u0303t+1\u3009| \u2264 \u03bbi |\u3008vi,ut\u3009|2 + 1 \u2264 |\u3008vi,ut\u3009| (\u03bbi |\u3008vi,ut\u3009|+ 1/ |\u3008vi,ut\u3009|) . (66) With equation (53), we have\n\u03bb1 |\u3008v1,ut+1\u3009| \u03bbi |\u3008vi,ut+1\u3009| = \u03bb1 |\u3008v1, u\u0303t+1\u3009| \u03bbi |\u3008vi, u\u0303t+1\u3009|\n\u2265 \u03bb1 |\u3008v1,ut\u3009|\n( \u03bb1 |\u3008v1,ut\u3009| \u2212 1|\u3008v1,ut\u3009| ) \u03bbi |\u3008vi,ut\u3009| ( \u03bbi |\u3008vi,ut\u3009| \u2212 1|\u3008vi,ut\u3009| ) (67)\n= ( \u03bb1 |\u3008v1,ut\u3009| \u03bbi |\u3008vi,ut\u3009| )2 1\u2212 1 \u03bb1\u3008v1,ut\u30092\n1 + \u03bbi\u03bb1 1 \u03bb1\u3008v1,ut\u30092 ( \u03bb1|\u3008v1,ut\u3009| \u03bbi|\u3008vi,ut\u3009| )2 (68) \u2265 ( \u03bb1 |\u3008v1,ut\u3009| \u03bbi |\u3008vi,ut\u3009| )2 1\u2212 1 \u03bb1\u3008v1,ut\u30092\n1 + max i\u2208[k] \u03bbi\n\u03bb1 1 \u03bb1\u3008v1,ut\u30092 ( \u03bb1|\u3008v1,ut\u3009| \u03bbi|\u3008vi,ut\u3009| )2 (69) = 1\u2212 1 \u03bb1\u3008v1,ut\u30092\n1( \u03bb1|\u3008v1,ut\u3009| \u03bbi|\u3008vi,ut\u3009| )2 + maxi\u2208[k] \u03bbi\u03bb1 1\u03bb1\u3008v1,ut\u30092 . (70)\nLet \u03ba = maxi\u2208[k] \u03bbi\u03bb1 . For t = 0, with conditions on 1 the following holds:\n\u03bb1 |\u3008v1,u1\u3009| \u03bbi |\u3008vi,u1\u3009| \u2265 1\u2212 1 \u03bb1\u3008v1,u0\u30092\n1( \u03bb1|\u3008v1,u0\u3009| \u03bbi|\u3008vi,u0\u3009| )2 + maxi\u2208[k] \u03bbi\u03bb1 1\u03bb1\u3008v1,u0\u30092 . (71)\n\u2265 1\u2212 14\u03ba+2 1 4 + \u03ba 4\u03ba+2 = 2 (72)\nWith the two conditions stated in Equation (64), following the same step in (63), we have 1tan \u03b8(v1,u2) \u2265 (1 + \u03b1) 1tan \u03b8(v1,u1) . By induction, 1 tan \u03b8(v1,ut+1)\n\u2265 (1 + \u03b1) 1tan \u03b8(v1,t) . for t \u2265 0. Subsequently, 1\ntan \u03b8 (v1, uT ) \u2265 (1 + \u03b1)T 1 tan \u03b8 (v1,u0) . (73)\nFinally, we complete the proof by setting T > log1+\u03b1 (1 + \u03c1) tan \u03b8 (v1,u0).\nNext, we present Lemma 4, which analyzes the second phase of the noisy tensor power method. The second phase starts with tan \u03b8(v1,u0) < 1, that is, the inner product of v1 and u0 is lower bounded by 1/2.\nLemma 4. Let v1 be the principal eigenvector of a tensor T and let u0 be an arbitrary vector in Rd that satisfies tan \u03b8(v1,u0) < 1. Suppose at every iteration t the noise satisfies\n4\u2016\u03b5\u0303(ut)\u2016 \u2264 (\u03bb1 \u2212 \u03bb2) and 4 \u2223\u2223\u3008v1, \u03b5\u0303(ut)\u3009\u2223\u2223 \u2264 (\u03bb1 \u2212 \u03bb2) cos2 \u03b8 (v1,u0) (74)\nfor some < 1. Then with high probability there exists T = O (\n\u03bb1 \u03bb1\u2212\u03bb2 log(1/ )\n) such that after T iteration\nwe have tan \u03b8 (v1,uT ) \u2264 .\nProof. Define \u2206 := \u03bb1\u2212\u03bb24 and X := v \u22a5 1 . We have the following chain of inequalities: tan \u03b8 (v1,T (I,u,u) + \u03b5\u0303(u)) \u2264 \u2225\u2225XT (T (I,u,u) + \u03b5\u0303(u))\u2225\u2225\u2225\u2225vT1 (T (I,u,u) + \u03b5\u0303(u))\u2225\u2225 (75)\n\u2264 \u2225\u2225XTT (I,u,u)\u2225\u2225+ \u2225\u2225VT \u03b5\u0303(u)\u2225\u2225\u2225\u2225vT1 T (I,u,u)\u2225\u2225\u2212 \u2225\u2225vT1 \u03b5\u0303(u)\u2225\u2225 (76)\n\u2264 \u03bb2 \u2225\u2225XTu\u2225\u22252 + \u2016\u03b5\u0303(u)\u2016\n\u03bb1 \u2223\u2223vT1 u\u2223\u22232 \u2212 \u2223\u2223v>1 \u03b5\u0303(u)\u2223\u2223 (77)\n= \u2225\u2225XTu\u2225\u22252\u2223\u2223vT1 u\u2223\u22232 \u03bb2\n\u03bb1 \u2212 |v>1 \u03b5\u0303(u)| |v>1 u|2\n+\n\u2016\u03b5\u0303(u)\u2016 |v>1 u|2 \u03bb1 \u2212 \u2223\u2223v>1 \u03b5\u0303(u)\u2223\u2223 |v>1 u|2\n(78)\n\u2264 tan2 \u03b8(v1,u) \u03bb2\n\u03bb2 + 3\u2206 +\n\u2206 ( 1 + tan2 \u03b8 (v1,u) ) \u03bb2 + 3\u2206\n(79)\n\u2264 max ( , \u03bb2 + \u2206\n\u03bb2 + 2\u2206 tan2 \u03b8 (v1,u)\n) (80)\n\u2264 max ( , \u03bb2 + \u2206\n\u03bb2 + 2\u2206 tan \u03b8 (v1,u)\n) (81)\nThe second step follows by triangle inequality. For u = u0, using the condition tan (v1,u0) < 1 we obtain tan \u03b8 (v1,u1) \u2264 max ( , \u03bb2 + \u2206\n\u03bb2 + 2\u2206 tan2 \u03b8 (v1,u)\n) \u2264 max ( , \u03bb2 + \u2206\n\u03bb2 + 2\u2206 tan \u03b8 (v1,u)\n) (82)\nSince \u03bb2+\u2206 \u03bb2+2\u2206 \u2264 max ( \u03bb2 \u03bb2+\u2206 , ) \u2264 (\u03bb2/\u03bb1)1/4 < 1, we have\ntan \u03b8 (v1,u1) = tan \u03b8 (v1,T (I,u0,u0) + \u03b5\u0303(ut)) \u2264 max ( , (\u03bb2/\u03bb1) 1/4 tan \u03b8 (v1,u0) ) < 1. (83)\nBy induction, tan \u03b8 (v1,ut+1) = tan \u03b8 (v1,T (I,ut,ut) + \u03b5\u0303(ut)) \u2264 max ( , (\u03bb2/\u03bb1) 1/4 tan \u03b8 (v1,ut) ) < 1.\nfor every t. Eq. (81) then yields tan \u03b8 (v1,uT ) \u2264 max ( ,max , (\u03bb2/\u03bb1) L/4 tan \u03b8 (v1,u0) ) . (84)\nConsequently, after T = log(\u03bb2/\u03bb1)\u22121/4(1/ ) iterations we have tan \u03b8 (v1,uT ) \u2264 .\nLemma 5. Suppose v1 is the principal eigenvector of a tensor T and let u0 \u2208 Rn. For some \u03b1, \u03c1 > 0 and < 1, if at every step, the noise satisfies\n\u2016\u03b5\u0303(ut)\u2016 \u2264 \u03bb1 \u2212 \u03bb2\n4 and \u2223\u2223\u3008v1, \u03b5\u0303(ut)\u3009\u2223\u2223 \u2264 min( 1 4\nmaxi\u2208[k] \u03bbi \u03bb1\n+ 2 \u03bb1, 1\u2212 (1 + \u03b1)/2 2 \u221a 2(1 + \u03b1) \u03bb1\n) 1\n\u03c42n , (85)\nthen with high probability there exists an T = O (\nlog1+\u03b1 (1 + \u03c1) \u03c4 \u221a n+ \u03bb1\u03bb1\u2212\u03bb2 log(1/ )\n) such that after T\niterations we have \u2225\u2225(I \u2212 uTuTT )v1\u2225\u2225 \u2264 .\nProof. By Lemma 2.5 in [?], for any fixed orthonormal matrix V and a random vectoru, we have maxi\u2208[K] tan \u03b8(vi,u) \u2264 \u03c4 \u221a n with all butO(\u03c4\u22121 +e\u2212\u2126(d)) probability. Using the fact that cos \u03b8 (v1,u0) \u2265 1/(1+tan \u03b8 (v1,u0)) \u2265\n1 \u03c4 \u221a n , the following bounds on the noise level imply the conditions in Lemma 3:\u2225\u2225VT \u03b5\u0303(ut)\u2225\u2225 \u2264 1\u2212 (1 + \u03b1)/2\n2 \u221a 2(1 + \u03b1)\u03c4 \u221a n and \u2223\u2223\u3008v1, \u03b5\u0303(ut)\u3009\u2223\u2223 \u2264 min ( 1\n4 maxi\u2208[k] \u03bbi\n\u03bb1 + 2\n\u03bb1, 1\u2212 (1 + \u03b1)/2\n2 \u03bb1\n) 1\n\u03c42n , \u2200t.\nNote that \u2223\u2223\u3008v1, \u03b5\u0303(ut)\u3009\u2223\u2223 \u2264 1\u2212(1+\u03b1)/22\u221a2(1+\u03b1) \u03bb1 1\u03c42n implies the first bound in Eq. (86). In Lemma 4, we assume tan \u03b8 (v1,u0) < 1 and prove that for every ut, tan \u03b8 (v1,ut) < 1, which is equivalent to saying that at every step, cos \u03b8 (v1,ut) > 1\u221a2 . By plugging the inequality into the second condition in Lemma 4, we have\n|\u3008v1, \u03b5\u0303(ut)\u3009| \u2264 (\u03bb1\u2212\u03bb2)8 . The lemma then follows by the fact that \u2225\u2225(I \u2212 uTuT T )v1\u2225\u2225 = sin \u03b8 (uT ,v1) \u2264 tan \u03b8 (uT ,v1) \u2264 .\nE.3.2 Deflation\nIn previous sections we have upper bounded the Euclidean distance between the estimated and the true principal eigenvector of an input tensor T. In this section, we show that error introduced from previous tensor power updates can also be bounded. As a result, we obtain error bounds between the entire set of base vectors {vi}ki=1 and their estimation {v\u0302i}ki=1.\nLemma 6. Let {v1,v2, \u00b7 \u00b7 \u00b7 ,vk} and {\u03bb1, \u03bb2, \u00b7 \u00b7 \u00b7 , \u03bbk} be orthonormal eigenvectors and eigenvalues of an input tensor T . Define \u03bbmax := maxi\u2208[k] \u03bbi. Suppose {v\u0302i}ki=1 and {\u03bb\u0302i}ki=1 are estimated eigenvector/eigenvalue pairs. Fix \u2265 0 and any t \u2208 [k]. If\u2223\u2223\u03bb\u0302i \u2212 \u03bbi\u2223\u2223 \u2264 \u03bbi /2, and \u2016u\u0302i \u2212 ui\u2016 \u2264 (86) for all i \u2208 [t], then for any unit vector u the following holds:\u2225\u2225\u2225\u2225\u2225 t\u2211 i=1 [ \u03bbv\u22973i \u2212 \u03bb\u0302iv\u0302 \u22973 i ] (I,u,u) \u2225\u2225\u2225\u2225\u2225 2 \u22644 (2.5\u03bbmax + (\u03bbmax + 1.5) )2 2 + 9(1 + /2)2\u03bb2max 4 (87)\n+ 8(1 + /2)2\u03bb2max 2 (88) \u226450\u03bb2max 2. (89)\nProof. Following similar approaches in [1], Lemma B.5, we define v\u0302\u22a5 = v\u0302i \u2212 (v>i v\u0302i)vi and Di =[ \u03bbv\u22973i \u2212 \u03bb\u0302iv\u0302 \u22973 i ] . Di(I,u,u) can then be written as the sum of scaled vi and v>i products as follows:\nDi (I,u,u) =\u03bbi(u >vi) 2vi \u2212 \u03bb\u0302i(u>v\u0302i)2v\u0302i (90)\n=\u03bbi(u >vi) 2vi \u2212 \u03bb\u0302i(u> ( v\u0302\u22a5i + (v > i v\u0302i)vi ) )2 ( v\u0302\u22a5 + (v>i v\u0302i)vi ) (91)\n= (( \u03bbi \u2212 \u03bb\u0302i(v>i v\u0302i)3 ) (u>vi) 2 \u2212 2\u03bb\u0302i(u>v\u0302\u22a5i )(v>i v\u0302i)2(u>vi)\u2212 \u03bb\u0302i(v>i v\u0302i)(u>v\u0302 \u22a5) ) vi\n\u2212 \u03bb\u0302i \u2225\u2225\u2225v\u0302\u22a5i \u2225\u2225\u2225((u>vi)(v>i v\u0302i) + u>v\u0302\u22a5i )(v\u0302\u22a5i /\u2225\u2225\u2225v\u0302\u22a5i \u2225\u2225\u2225) (92)\nSuppose Ai and Bi are coefficients of vi and ( v\u0302\u22a5i / \u2225\u2225\u2225v\u0302\u22a5i \u2225\u2225\u2225), respectively. The summation of Di can be bounded as \u2225\u2225\u2225\u2225\u2225 t\u2211 i=1 Di (I,u,u) \u2225\u2225\u2225\u2225\u2225 2 = \u2225\u2225\u2225\u2225\u2225 t\u2211 i=1 Aivi \u2212 t\u2211 i=1 Bi ( v\u0302\u22a5i / \u2225\u2225\u2225v\u0302\u22a5i \u2225\u2225\u2225) \u2225\u2225\u2225\u2225\u2225 2\n2\n\u22642 \u2225\u2225\u2225\u2225\u2225 t\u2211 i=1 Aivi \u2225\u2225\u2225\u2225\u2225 2 + 2 \u2225\u2225\u2225\u2225\u2225 t\u2211 i=1 Bi ( v\u0302\u22a5i / \u2225\u2225\u2225v\u0302\u22a5i \u2225\u2225\u2225) \u2225\u2225\u2225\u2225\u2225 2 \u2264 t\u2211 i=1 A2i + 2 ( t\u2211 i=1 |Bi|\n)2 We then try to upper bound |Ai|.\n|Ai| \u2264 \u2223\u2223\u2223(\u03bbi \u2212 \u03bb\u0302i(v>i v\u0302i)3) (u>vi)2 \u2212 2\u03bb\u0302i(u>v\u0302\u22a5i )(v>i v\u0302i)2(u>vi)\u2212 \u03bb\u0302i(v>i v\u0302i)(u>v\u0302\u22a5)\u2223\u2223\u2223 (93) \u2264 ( \u03bbi \u2223\u22231\u2212 (v>i v\u0302i)3\u2223\u2223+ \u2223\u2223\u2223\u03bbi \u2212 \u03bb\u0302i\u2223\u2223\u2223 (v>i v\u0302i)3) (u>vi)2 + 2(\u03bbi + \u2223\u2223\u2223\u03bbi \u2212 \u03bb\u0302i\u2223\u2223\u2223) \u2016v\u0302i \u2212 vi\u2016 \u2223\u2223u>vi\u2223\u2223\n+ ( \u03bbi + \u2223\u2223\u2223\u03bbi \u2212 \u03bb\u0302i\u2223\u2223\u2223) \u2016v\u0302i \u2212 vi\u20162 (94) \u2264 ( 1.5 \u2016vi \u2212 v\u0302i\u20162 + \u2223\u2223\u2223\u03bbi \u2212 \u03bb\u0302i\u2223\u2223\u2223+ 2(\u03bbi + \u2223\u2223\u2223\u03bbi \u2212 \u03bb\u0302i\u2223\u2223\u2223) \u2016vi \u2212 v\u0302i\u2016) \u2223\u2223u>vi\u2223\u2223\n+ ( \u03bbi + \u2223\u2223\u2223\u03bbi \u2212 \u03bb\u0302i\u2223\u2223\u2223) \u2016v\u0302i \u2212 vi\u20162 (95) \u2264 (2.5\u03bbi + (\u03bbi + 1.5) )\n\u2223\u2223u>vi\u2223\u2223+ (1 + /2)\u03bbi 2 (96) Next, we bound |Bi| in a similar manner.\n|Bi| = \u2223\u2223\u2223\u03bb\u0302i \u2225\u2225\u2225v\u0302\u22a5i \u2225\u2225\u2225((u>vi)(v>i v\u0302i) + u>v\u0302\u22a5i )\u2223\u2223\u2223 (97) \u22642 ( \u03bbi + \u2223\u2223\u2223\u03bbi \u2212 \u03bb\u0302i\u2223\u2223\u2223) \u2225\u2225\u2225v\u0302\u22a5i \u2225\u2225\u2225((u>vi)2 + \u2225\u2225\u2225v\u0302\u22a5i \u2225\u2225\u22252) (98)\n\u22642(1 + /2)\u03bbi (u>vi)2 + 2(1 + /2)\u03bbi 3 (99)\nCombining everything together we have\u2225\u2225\u2225\u2225\u2225 t\u2211 i=1 Di (I,u,u) \u2225\u2225\u2225\u2225\u2225 2 \u22642 t\u2211 i=1 A2i + 2 ( t\u2211 i=1 |Bi| )2 (100)\n\u2264 t\u2211 i=1 4 (5\u03bbi + (\u03bbi + 1.5)) 2 2 \u2223\u2223u>vi\u2223\u22232 + 4(1 + /2)2\u03bb2i 4\n+ 2 ( t\u2211 i=1 2(1 + /2)\u03bbi (u >vi) 2 + 2(1 + /2)\u03bbi 3 )2 (101)\n\u22644 (2.5\u03bbmax + (\u03bbmax + 1.5) )2 2 t\u2211 i=1 \u2223\u2223u>vi\u2223\u22232 + 4(1 + /2)2\u03bb2max 4 + 2 ( 2(1 + /2)\u03bbmax\nt\u2211 i=1 (u>vi) 2 + 2(1 + /2)\u03bbmax 3\n)2 (102)\n\u22644 (2.5\u03bbmax + (\u03bbmax + 1.5) )2 2 + 9(1 + /2)2\u03bb2max 4 + 8(1 + /2)2\u03bb2max 2. (103)\nE.3.3 Main Theorem\nIn this section we present and prove the main theorem that bounds the reconstruction error of fast robust tensor power method under appropriate settings of the hash length b and number of independent hashes B. The theorem presented below is a more detailed version of Theorem 2 presented in Section 4.2.\nTheorem 3. Let T\u0304 = T + E \u2208 Rn\u00d7n\u00d7n, where T = \u2211k i=1 \u03bbiv \u22973 i and {vi}ki=1 is an orthonormal basis. Suppose (v\u03021, \u03bb\u03021), (v\u03021, \u03bb\u03021), \u00b7 \u00b7 \u00b7 (v\u0302k, \u03bb\u0302k) is the sequence of estimated eigenvector/eigenvalue pairs obtained using the fast robust tensor power method. Assume \u2016E\u2016 = . There exists constant C1, C2, C3, \u03b1, \u03c1, \u03c4 \u2265 0 such that the following holds: if\n\u2264 C1 1\nn\u03bbmax , and T = C2\n( log1+\u03b1 (1 + \u03c1) \u03c4 \u221a n+\n\u03bb1 \u03bb1 \u2212 \u03bb2 log(1/ )\n) , (104)\nand\u221a ln(L/ log2(k/\u03b7))\nln(k) \u00b7\n( 1\u2212 ln (lnL/ log2(k/\u03b7)) + C3\n4 ln (L/ log2(k/\u03b7)) \u2212\n\u221a ln(8)\nln(L/ log2(k/\u03b7))\n) \u2265 1.02 ( 1 + \u221a ln(4)\nln(k)\n) .\n(105)\nSuppose the tensor sketch randomness is independent among all tensor product evaluations. IfB = \u2126(log(n/\u03c4)) and the hash length b is set to\nb \u2265  \u2016T\u2016 2 F \u03c4 4n2 min (\n1 4 maxi\u2208[k](\u03bbi/\u03bb1)+2 \u03bb1, 1\u2212(1+\u03b1)/2 2 \u221a 2(1+\u03b1) \u03bb1\n)2 , 16 \u22122\u2016T\u20162Fmini\u2208[k] (\u03bbi \u2212 \u03bbi\u22121)2 , \u22122 \u2016T\u20162F  (106)\nwith probability at least 1\u2212 (\u03b7 + \u03c4\u22121 + e\u2212n), there exists a permutation \u03c0 on k such that\u2225\u2225v\u03c0(j) \u2212 v\u0302i\u2225\u2225 \u2264 , \u2223\u2223\u2223\u03bb\u03c0(j) \u2212 \u03bb\u0302j\u2223\u2223\u2223 \u2264 \u03bb\u03c0(j) 2 , and \u2225\u2225\u2225\u2225\u2225\u2225T\u2212 k\u2211 j=1 \u03bb\u0302j v\u0302 \u22973 j \u2225\u2225\u2225\u2225\u2225\u2225 \u2264 c , (107) for some absolute constant c.\nProof. We prove that at the end of each iteration i \u2208 [k], the following conditions hold \u2022 1. For all j \u2264 i, \u2223\u2223v\u03c0(j) \u2212 v\u0302j\u2223\u2223 \u2264 and \u2223\u2223\u2223\u03bb\u03c0(j) \u2212 \u03bb\u0302j\u2223\u2223\u2223 \u2264 \u03bbi 2\n\u2022 2. The tensor error satisfies\u2225\u2225\u2225\u2225\u2225\u2225 T\u0303\u2212\u2211\nj\u2264i\n\u03bb\u0302j v\u0302 \u22973 j \u2212 \u2211 j\u2265i+1 \u03bb\u03c0(j)v \u22973 \u03c0(j)  (I,u,u) \u2225\u2225\u2225\u2225\u2225\u2225 \u2264 56 (108)\nFirst, we check the case when i = 0. For the tensor error, we have\u2225\u2225\u2225\u2225\u2225\u2225 T\u0303\u2212 K\u2211\nj=1\n\u03bb\u03c0(j)v \u22973 \u03c0(j)  (I,u,u) \u2225\u2225\u2225\u2225\u2225\u2225 = \u2016\u03b5(u)\u2016 \u2264 \u2016\u03b52,T (u)\u2016+ \u2016E (I,u,u)\u2016 \u2264 + = 2 . (109)\nThe last inequality follows Theorem 1 with the condition for b. Next, Using Lemma 5, we have that\u2225\u2225v\u03c0(1) \u2212 v\u03021\u2225\u2225 \u2264 . (110) In addition, conditions for hash length b and Theorem 1 yield\u2223\u2223\u2223\u03bb\u03c0(1) \u2212 \u03bb\u03021\u2223\u2223\u2223 \u2264 \u2016\u03b51,T (v1)\u2016+ \u2016T(v\u03021 \u2212 v1, v\u03021 \u2212 u, v\u03021 \u2212 v1)\u2016 \u2264 \u03bbi \u2212 \u03bbi\u22121\n4 + 3 \u2016T\u2016F \u2264 \u03bbi 2 (111)\nThus, we have proved that for i = 1 both conditions hold. Assume the conditions hold up to i = t \u2212 1 by induction. For the tth iteration, the following holds:\u2225\u2225\u2225\u2225\u2225\u2225 T\u0303\u2212\u2211 j\u2264t \u03bb\u0302j v\u0302 \u22973 j \u2212 \u2211 j\u2265t+1 \u03bb\u03c0(j)v \u22973 \u03c0(j)  (I,u,u) \u2225\u2225\u2225\u2225\u2225\u2225\n\u2264 \u2225\u2225\u2225\u2225\u2225\u2225 T\u0303\u2212 K\u2211\nj=1\n\u03bb\u03c0(j)v \u22973 \u03c0(j)  (I,u,u) \u2225\u2225\u2225\u2225\u2225\u2225+ \u2225\u2225\u2225\u2225\u2225\u2225 t\u2211\nj=1\n\u03bb\u0302j v\u0302 \u22973 j \u2212 \u03bb\u03c0(j)v\u22973\u03c0(j) \u2225\u2225\u2225\u2225\u2225\u2225 \u2264 +\u221a50\u03bbmax . For the last inequality we apply Lemma 6. Since the condition is satisfied, Lemma 5 yields\u2225\u2225v\u03c0(t+1) \u2212 v\u0302t+1\u2225\u2225 \u2264 . (112) Finally, conditions for hash length b and Theorem 1 yield\u2223\u2223\u2223\u03bb\u03c0(t+1) \u2212 \u03bb\u0302t+1\u2223\u2223\u2223 \u2264 \u2016\u03b51,T (v1)\u2016+ \u2016T(v\u0302t \u2212 v1, v\u03021 \u2212 u, v\u03021 \u2212 v1)\u2016\n\u2264 \u03bbi \u2212 \u03bbi\u22121 4 + 3 \u2016T\u2016F \u2264 \u03bbi 2 (113)"}, {"heading": "Appendix F Summary of notations for matrix/vector products", "text": "We assume vectors a, b \u2208 Cn are indexed starting from 0; that is, a = (a0, a1, \u00b7 \u00b7 \u00b7 , an\u22121) and b = (b0, b1, \u00b7 \u00b7 \u00b7 , bn\u22121). Matrices A,B and tensors T are still indexed starting from 1.\nElement-wise product For a, b \u2208 Cn, the element-wise product (Hadamard product) a\u25e6b \u2208 Rn is defined as\na \u25e6 b = (a0b0, a1b1, \u00b7 \u00b7 \u00b7 , an\u22121bn\u22121). (114)\nConvolution For a, b \u2208 Cn, their convolution a \u2217 b \u2208 Cn is defined as\na \u2217 b =  \u2211 (i+j) mod n=0 aibj , \u2211 (i+j) mod n=1 aibj , \u00b7 \u00b7 \u00b7 , \u2211 (i+j) mod n=n\u22121 aibj  . (115)\nInner product For a, b \u2208 Cn, their inner product is defined as\n\u3008a, b\u3009 = n\u2211 i=1 aibi, (116)\nwhere bi denotes the complex conjugate of bi. For tensors A,B \u2208 Cn\u00d7n\u00d7n, their inner product is defined similarly as\n\u3008A,B\u3009 = n\u2211\ni,j,k=1\nAi,j,kBi,j,k. (117)\nTensor product For a, b \u2208 Cn, the tensor product a\u2297b can be either an n\u00d7n matrix or a vector of length n2. For the former case, we have\na\u2297 b =  a0b0 a0b1 \u00b7 \u00b7 \u00b7 a0bn\u22121 a1b0 a1b1 \u00b7 \u00b7 \u00b7 a1bn\u22121 ... ... . . . ...\nan\u22121b0 an\u22121b1 \u00b7 \u00b7 \u00b7 an\u22121bn\u22121  . (118) If a\u2297 b is a vector, it is defined as the expansion of the output matrix. That is,\na\u2297 b = (a0b0, a0b1, \u00b7 \u00b7 \u00b7 , a0bn\u22121, a1b0, a1b1, \u00b7 \u00b7 \u00b7 , an\u22121bn\u22121). (119) Suppose T is an n\u00d7 n\u00d7 n tensor and matrices A \u2208 Rn\u00d7m1 , B \u2208 Rn\u00d7m2 and C \u2208 Rn\u00d7m3 . The tensor\nproduct T(A,B,C) is an m1 \u00d7m2 \u00d7m3 tensor defined by\n[T(A,B,C)]i,j,k = n\u2211 i\u2032,j\u2032,k\u2032=1 Ti\u2032,j\u2032,k\u2032Ai\u2032,iBj\u2032,jCk\u2032,k. (120)\nKhatri-Rao product For A,B \u2208 Cn\u00d7m, their Khatri-Rao product A B \u2208 Cn2\u00d7m is defined as A B = (A(1) \u2297B(1),A(2) \u2297B(2), \u00b7 \u00b7 \u00b7 ,A(m) \u2297B(m)), (121)\nwhere A(i) and B(i) denote the ith rows of A and B.\nMode expansion For a tensor T of dimension n\u00d7 n\u00d7 n, its first mode expansion T(1) \u2208 Rn\u00d7n is defined as\nT(1) =  T1,1,1 T1,1,2 \u00b7 \u00b7 \u00b7 T1,1,n T1,2,1 \u00b7 \u00b7 \u00b7 T1,n,n T2,1,1 T2,1,2 \u00b7 \u00b7 \u00b7 T2,1,n T2,2,1 \u00b7 \u00b7 \u00b7 T2,n,n ... ... ... ... ... ...\n... Tn,1,1 Tn,1,2 \u00b7 \u00b7 \u00b7 Tn,1,n Tn,2,1 \u00b7 \u00b7 \u00b7 Tn,n,n  . (122) The mode expansions T(2) and T(3) can be similarly defined."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in statistical learning of<lb>latent variable models and in data mining. In this paper, we propose fast and randomized tensor CP de-<lb>composition algorithms based on sketching. We build on the idea of count sketches, but introduce many<lb>novel ideas which are unique to tensors. We develop novel methods for randomized computation of tensor<lb>contractions via FFTs, without explicitly forming the tensors. Such tensor contractions are encountered in<lb>decomposition methods such as tensor power iterations and alternating least squares. We also design novel<lb>colliding hashes for symmetric tensors to further save time in computing the sketches. We then combine<lb>these sketching ideas with existing whitening and tensor power iterative techniques to obtain the fastest al-<lb>gorithm on both sparse and dense tensors. The quality of approximation under our method does not depend<lb>on properties such as sparsity, uniformity of elements, etc. We apply the method for topic modeling and<lb>obtain competitive results.<lb>", "creator": "LaTeX with hyperref package"}}}