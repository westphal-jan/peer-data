{"id": "1506.04477", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2015", "title": "Dual Memory Architectures for Fast Deep Learning of Stream Data via an Online-Incremental-Transfer Strategy", "abstract": "The online learning of deep neural networks is an interesting problem of machine learning because, for example, major IT companies want to manage the information of the massive data uploaded on the web daily, and this technology can contribute to the next generation of lifelong learning. We aim to train deep models from new data that consists of new classes, distributions, and tasks at minimal computational cost, which we call online deep learning. Unfortunately, deep neural network learning through classical online and incremental methods does not work well in both theory and practice, especially in real time.", "histories": [["v1", "Mon, 15 Jun 2015 04:44:38 GMT  (669kb)", "http://arxiv.org/abs/1506.04477v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sang-woo lee", "min-oh heo", "jiwon kim", "jeonghee kim", "byoung-tak zhang"], "accepted": false, "id": "1506.04477"}, "pdf": {"name": "1506.04477.pdf", "metadata": {"source": "META", "title": "Dual Memory Architectures for Fast Deep Learning of Stream Data via an Online-Incremental-Transfer Strategy", "authors": ["Sang-Woo Lee", "Jiwon Kim"], "emails": ["SLEE@BI.SNU.AC.KR", "MOHEO@BI.SNU.AC.KR", "G1.KIM@NAVERCORP.COM", "JEONGHEE.KIM@NAVERCORP.COM", "BTZHANG@BI.SNU.AC.KR"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 6.\n04 47\n7v 1\n[ cs\n.L G\n] 1\nThe online learning of deep neural networks is an interesting problem of machine learning because, for example, major IT companies want to manage the information of the massive data uploaded on the web daily, and this technology can contribute to the next generation of lifelong learning. We aim to train deep models from new data that consists of new classes, distributions, and tasks at minimal computational cost, which we call online deep learning. Unfortunately, deep neural network learning through classical online and incremental methods does not work well in both theory and practice. In this paper, we introduce dual memory architectures for online incremental deep learning. The proposed architecture consists of deep representation learners and fast learnable shallow kernel networks, both of which synergize to track the information of new data. During the training phase, we use various online, incremental ensemble, and transfer learning techniques in order to achieve lower error of the architecture. On the MNIST, CIFAR-10, and ImageNet image recognition tasks, the proposed dual memory architectures performs much better than the classical online and incremental ensemble algorithm, and their accuracies are similar to that of the batch learner.\nICML workshop on Deep Learning 2015, Lille, France, 2015. Copyright 2015 by the author(s)."}, {"heading": "1. Introduction", "text": "Learning deep neural networks on new data from a potentially non-stationary stream is an interesting problem in the machine learning field for various reasons. From the engineering perspective, major IT companies may want to update their services based on deep neural networks from the information of massive data uploaded to the web in real time. From the artificial intelligence perspective, for example, we argue that online deep learning is the next probable step towards realizing the next generation of lifelong learning algorithms. Lifelong learning is a problem of learning multiple consecutive tasks, and it is very important for creation of intelligent, general-purpose, and flexible machines (Thrun & O\u2019 Sullivan, 1996; Ruvolo & Eaton, 2013). Online deep learning can have good properties from the perspective of lifelong learning because deep neural networks show good performance on recognition problems, and their transfer and multi-task learning problem (Heigold et al., 2013; Donahue et al., 2014; Yosinski et al., 2014).\nHowever, it is difficult to train deep models in an online manner for several reasons. Most of all, the objective function of neural networks is not convex, thus online stochastic learning algorithms cannot guarantee convergence. Learning new data through neural networks often results in a loss of all previously acquired information, which is known as catastrophic forgetting. Because it is a disadvantageous constraint to learn one instance and then discard it in online learning, we can alleviate the constraint by memorizing a moderate amount of data (e.g., 10K). We discover the online parameter of neural networks with an amount of data, which works reasonably for stationary data, but does not work well for non-stationary data. On the other hand, if we have sufficient memory capacity, we can instead make\nan incremental ensemble of neural networks. Incremental ensemble learning refers to making a weak learner using new parts of an online dataset, and combining multiple weak learners to obtain better predictive performance. There are several studies that use the incremental ensemble approach (Polikar et al., 2001; Oza & Russell, 2001). In practice, however, a part of entire data is not sufficient for learning highly expressive representations of deep neural networks; therefore, the incremental ensemble approach alone does not work well, as illustrated in Section 3.\nTo solve this problem, we use both online parametric and incremental structure learning. Because it is neither trivial nor easy to combine two approaches, we apply transfer learning to intermediate online and parameter learning. This strategy, which we call an online-incremental-transfer strategy, is one of the key ideas for our proposed architecture. For online incremental deep learning, we introduce the dual memory architecture that consists of the following two learning policies, and not simply a group of learning algorithms. First, this architecture trains two memories \u2013 one is an ensemble of deep neural networks, and the other are shallow kernel networks on deep neural networks. Two memories are designed for the different strategies. The ensemble of deep neural networks learns new information in order to adapt its representation, whereas the shallow kernel networks aim to manage non-stationary distribution and new classes in new data more rapidly. Second, we use both online and incremental ensemble learning through the transfer learning technique. In particular, for example, we continually train a general model of the entire data seen in an online manner, and then, transfer to specific modules in order to incrementally generate an ensemble of neural networks. In our approach, online and incremental learning work together to achieve a lower error bound for the architecture.\nThe remainder of this paper is organized as follows. Section 2 briefly introduces the concept of the dual memory architecture. In Section 3 and 4, we propose and validate three specific examples of learning algorithms that satisfy the policies of the dual memory architecture. On the MNIST, CIFAR-10, and ImageNet image recognition tasks, the proposed algorithms performs much better than the classical online and incremental ensemble algorithm, and their accuracies are similar to that of the batch learner. In Section 5, we summarize our arguments."}, {"heading": "2. Dual Memory Architectures", "text": "In addition to the policies described in the previous section, we explain in general terms what dual memory architectures means, and discuss the type of algorithms that could be included in this framework. However, this description is not restricted and can be extended beyond the given ex-\nplanation in follow-up studies. Dual memory architecture is the learnable system that consists of deep and fast memory, both of which are trained concurrently by using online, incremental, and transfer learning.\n1. Dual memory architecture consists of an ensemble of neural networks and shallow kernel networks. We call the former as \u201cdeep memory,\u201d and the latter as \u201cfast memory\u201d (Figure 1).\n2. Deep memory learns from new data in an online and incremental manner. In deep memory learning, first, a general model is trained on the entire data it has seen in an online manner (first layer in Figure 1). Second, the knowledge or parameter of the general model is transferred to incrementally generate an ensemble; weak neural network in the ensemble is specific for each data at a specific time (second layer in Figure 1) as clarified in Section 3.\n3. Fast memory is on the deep memory. In other words, the inputs of the shallow kernel network are the hidden nodes of the higher layer of deep neural networks (third layer in Figure 1). The deep memory transfers its knowledge to the fast memory. The fast memory learns from the new data in an online manner without much loss of accuracy compared with the batch learning process. However, batch learning, because of low computational cost in the parameter learning of shallow networks, can be used when higher accuracy is required.\nWhen new instances \u2013 potentially a part of which has new distributions and additional classes \u2013 arrive gradually, two memories ideally work as follows. First, the weights of the fast memory are updated online with scant loss of the accuracy of the entire training data; for example, in the case of linear regression, no loss exists. In this process, because\nof the transferability of the deep memory, the fast memory has remarkable performance, especially for new distributions and additional classes, as though the fast memory had already trained from many new instances with the same class and similar style (Donahue et al., 2014). Second, representations of the deep memory also learn separately and more slowly from a stored moderate amount of data (e.g., 10K), especially because, when we need more data in order to make a new weak neural learner for an ensemble. After a new weak neural learner is made, the fast memory makes new kernels that are functions of hidden values of both old and new weak learners. In this procedure, the fast structure learning of the explicit kernel is particularly used in the paper. As explained above, learning fast and slow is one of the mechanisms how the dual memory architectures work.\nThe other mechanism, online-incremental-transfer strategy, using both online stochastic and incremental learning through transfer learning technique, is explained in detail with examples. In section 3, we discuss two specific algorithms for deep memory. In section 4, we discuss one specific algorithm for fast memory."}, {"heading": "3. Online Incremental Learning Algorithms for Deep Memory", "text": "For practical online learning from a massive amount of data, it is good to store a reasonable number of instances and discard those that appear less important for learning in the near future. We refer to online learning as a parameter fine-tuning for new instances without retraining new model from an entire dataset that the model has seen ever. As a type of practical online learning setting, we consider the \u201cmini-dataset-shift learning problem,\u201d which allows keeping at most Nsubset training examples in a storage for online learning (Algorithm 1).\nAlgorithm 1 Mini-Dataset-Shift Learning Problem Initialize a model \u03b8 randomly. repeat\nGet new data Dnew. Merge Dnew into the storage D (i.e. D \u2190 D \u22c3 Dnew). Throw away some data in the storage to make |D| \u2264 Nsubset. Train a model \u03b8 with D.\nuntil forever\nTo solve this problem, many researchers study incremental ensemble learning. We refer to incremental learning as structure learning for new instances; following the information of new data, a new structure is made, and useless parts of the structure are removed. Incremental ensemble learning, a type of both incremental and online learning, is\nreferred to as combining multiple weak learners, each of which is trained on a part of that online dataset. In this paper, our proposed algorithms are compared to the simple bagging algorithm or \u201cna\u0131\u0308ve incremental ensemble.\u201d In this na\u0131\u0308ve algorithm, for example, we train the first weak learner or neural network on the 1 \u2013 10,000th data. After that, the second neural network learns the 10,001 \u2013 20,000th data. Then, the third neural network learns the 20,001 \u2013 30,000th data, and so on (if Nsubset is 10,000). As mentioned later, however, this algorithm does not work well in our experiments."}, {"heading": "3.1. Mini-Batch-Shift Gradient Descent Ensemble", "text": "First, we begin from an alternative approach \u2013 online learning \u2013 to complement the simple incremental ensemble approach. The first step of our first algorithm involves using mini-batch gradient descent at each epoch with recent Nsubset training examples for accommodating Nnew new data. We refer to this procedure as \u201cmini-batch-shift gradient descent.\u201d In this algorithm, for example, we first train on the 1 \u2013 10,000th data with mini-batch gradient descent with sufficient epochs. After that, the model learns the 501 \u2013 10,500th instances with one epoch. Then, the model learns the 1,001 \u2013 11,000th instances with one epoch, and so on (if Nsubset is 10,000 and Nnew is 500).\nAlgorithm 2 Mini-Batch-Shift Gradient Descent Ensemble Collect first Nsubset new data Dfirst. Learn a neural network C with Dfirst with enough epochs. Put Dfirst in the storage D (i.e. D \u2190 Dfirst). repeat\nCollect Nnew new data Dnew such that Nnew < Nsubset. Throw away the oldest Nnew instances in D. Merge Dnew into D (i.e. D \u2190 D \u22c3 Dnew). Train a general neural network C with D with one epoch. if D is disjoint to the data used in Wprev then\nInitialize a new weak neural network Wnew by parameters of C. Train Wnew with D until converge. Combine Wnew to a model \u03b8 (i.e. \u03b8 \u2190 \u03b8 \u22c3 {Wnew}).\nRefer to Wnew as Wprev (i.e. Wprev \u2190Wnew). end if\nuntil forever\nIn Section 3.3, we show that mini-batch-shift gradient descent works well and outperforms the na\u0131\u0308ve incremental ensemble. Encouraged by this result, we apply mini-batchshift gradient descent to incremental ensemble learning. To combine online and incremental learning properly, we use\nthe transfer learning technique. Similar to the na\u0131\u0308ve incremental ensemble, we train each neural network on each part of the online dataset. Unlike the na\u0131\u0308ve incremental ensemble, we transfer to each neural network from one trained on the entire data seen in an online manner. We refer to the neural network trained in an online manner for the entire data as the general neural network C, whereas each weak neural network trained in a batch manner for each part of the online dataset is a weak neural network W .\nTo transfer from a general neural network C to each weak neural network W , we use the initialize and fine-tune approach suggested in (Yosinski et al., 2014). The method we use is as follows: 1) initialize a target neural network with all parameters without the last softmax layer of a source neural network 2) fine-tune the entire target neural network. Using this method, (Yosinski et al., 2014) achieved 2.1% improvement for transfer learning from one 500-classes to another 500-classes image classification task on the ImageNet dataset. In the mini-batch-shift gradient descent ensemble, a general neural network C trained by mini-batchshift gradient descent is transferred to each weak neural network W (Algorithm 2) and the ensemble of each weak learner W is used for inference. In mini-batch-shift gradient descent, we use one general neural network C for inference, and do not make other neural networks."}, {"heading": "3.2. Neural Prior Ensemble", "text": "Dual memory architecture is not just a specific learning procedure, but a framework for learning data streams. We introduce \u201cneural prior ensemble,\u201d another learning algorithm for deep memory. In neural prior ensemble, a lastly trained weak neural network Wprev takes the role of the general neural network C used in the mini-batch-shift gradient descent, and it is transferred to a new weak neural network Wnew (Algorithm 3). We refers to \u201cneural prior\u201d as the strategy for using the last neural network Wnew for inference, and neglect the previous neural networks in the next experiments section.\nAlgorithm 3 Neural Prior Ensemble repeat\nCollect Nsubset new data Dnew. Initialize a new neural network Wnew by parameters of Wprev . Train Wnew with Dnew. Combine a weak learner Wnew to a model. \u03b8 (i.e. \u03b8 \u2190 \u03b8 \u22c3 {Wnew})\nRefer to Wnew as Wprev . (i.e. Wprev \u2190Wnew) until forever\nFigure 2 illustrates and summarizes ensemble algorithms for deep memory. There is no knowledge transfer in na\u0131\u0308ve incremental learning. In mini-batch-shift gradient de-\nscent ensemble, a general neural network C transfers their knowledge (first layer in Figure 2 (c)) to each weak neural network W (second layer in Figure 2 (c)). In neural prior ensemble, a lastly trained weak neural networkWprev transfers their knowledge to a newly constructed neural network Wnew."}, {"heading": "3.3. Experiments", "text": "We evaluate the performance of the proposed algorithm on the MNIST, CIFAR-10, and ImageNet image object classification dataset. MNIST consists of 60,000 training and 10,000 test images, from 10 digit classes. CIFAR-10 consists of 50,000 training and 10,000 test images, from 10 different object classes. ImageNet contains 1,281,167 labeled training images and 50,000 test images, with each image labeled with one of the 1,000 classes. In experiments on ImageNet, however, we only use 500,000 images, which will be increased in future studies. Thus, our experiments on ImageNet in the paper is somewhat disadvantageous because online incremental learning algorithms do worse if data is scarce in general. We run various size of deep convolutional neural networks for each dataset using the demo code in MatConvNet, which is a MATLAB toolbox of convolutional neural networks (Vedaldi & Lenc, 2014). In our experiments, we do not aim to optimize performance, but rather to study online learnability on a standard architecture.\nIn the running of the mini-batch-shift gradient descent, we set the learning rate proportional to 1/ \u221a t, where t is a variable proportional to the number of entire data that the model has ever seen. In the other training algorithms, including the batch learning and the neural prior, we first set the learning rate 10\u22122 and drop it by a constant factor \u2013 in our experiments, 10 \u2013 at some predifined steps. In entire experiments, we exploit the momentum of the fast training of neural networks; without momentum, we could not reach the reasonable local minima within a moderate amount of epochs in our experiments.\nThe main results on deep memory models are shown in Figure 3. We randomly split the entire training data into the 10 online dataset to make the distribution of the data stream stationary; we call this setting \u201810-split experiments\u2019. In\nthis setting, we maintain 1/10 of each entire dataset as the number of training examples Nmemory in the storage.\nFirst, these results show that mini-dataset-shift learning algorithms with a single general neural network \u2013 i.e. the mini-batch-shift gradient descent and the neural prior \u2013 outperform the na\u0131\u0308ve incremental ensemble. In other words, the online learning of a neural network with an amount (Nmemory) of stored data is better than simply bagging each weak neural network with the same amount of data. Our experiments show that learning a part of the entire data is not sufficient to make highly expressive representations of deep neural networks.\nMeanwhile, the lower accuracies in the early phase of the mini-batch-shift gradient descent are conspicuous in each figure because we remain as a relatively high learning rate that prevents efficient fine-tuning. We improved the performance of the early phase with batch-style learning of the first online dataset without loss of the accuracy of the latter phase in other experiments not shown in the figures. The figure also illustrates that ensemble algorithms for deep memory \u2013 i.e. mini-batch-shift gradient descent ensemble and neural prior ensemble \u2013 perform better than algorithms with a single neural network. Regardless of the improvement, it is a burden to increase the memory and inference time proportional to data size in the ensemble approach.\nWhen the data distribution is stationary, however, we found that maintaining a small number of neural networks does not decrease accuracy significantly. In our experiment, for example, selecting three over ten neural networks at the end of learning in the neural prior ensemble simply decreases the absolute error to less than 1%.\nThe performances of the proposed online learner may seem insufficient compared with the batch learner. However, by alleviating the condition, the entire dataset is divided into\ntwo online datasets, the performance losses of the proposed ensemble decrease. Figure 4 show the results on CIFAR10 split into two online datasets with various proportions of the source and target parts."}, {"heading": "4. Online Incremental Learning Algorithms for Fast Memory", "text": ""}, {"heading": "4.1. Shallow Kernel Networks on the Neural Networks", "text": "We introduce the fast memory; shallow kernel networks on the neural networks. In dual memory architectures, the input features of shallow kernel networks we used as fast memory are the activation of deep neural networks. Complementing the dual memory, the fast memory plays two important roles for treating stream data. First, a fast memory integrates the information distributed in each neural networks of ensemble. On the non-stationary data stream, not only proposed mini-dataset-shift learning algorithm of a single neural network but also ensemble learning algorithm for deep memory does not work well. Training fast memory with entire training data makes much better performance than deep memory alone, in particular, when new data includes new distributions and additional classes. It is quite practical, because of low computational costs on parameter learning of shallow networks. Second, fast memory can be updated from each one new instance, with a small amount of calculation until the features remain unchanged. It does not require without much gain of loss function comparing to the batch counterpart; in case of the linear regression, loseless. Learning deep memory needs expensive computational costs on inference and backpropagation in deep neural networks, even if deep memory is trained through the online learning algorithm we proposed."}, {"heading": "4.2. Multiplicative Hypernetworks", "text": "In this section, we introduce a multiplicative hypernetwork (mHN) as an example of fast memory. This model is inspired by the sparse population coding model (Zhang et al., 2012) and it is revised to be fit to the classification task we want to solve. We choose mHNs for their good online learnability via sparse well-shared kernels among classes. However, there are alternative choices, e.g., a support vector machine (SVM) (Liu et al., 2008), and an efficient lifelong learning algorithm (ELLA) (Zhou et al., 2012), among which SVM is our comparative model. mHNs are shallow kernel networks that use a multiplicative function as a explicit kernel \u03c6 = {\u03c6(1), ..., \u03c6(P )}T where\n\u03c6(p)(v, y) = (v(p,1) \u00d7 ...\u00d7 v(p,Kp)) & \u03b4(y).\n\u00d7 denotes the scalar multiplication and \u03b4 denotes the indicator function. v is the input feature of mHNs, which is also the activation of deep neural networks, and y is the\ntarget class. {v(p,1), ..., v(p,Kp)} is the set of variables used in pth kernel. Kp is the order, or the number of variable used in pth kernel; in this paper Kp = 2. In the training of parameters that correspond to kernels, we obtain weights by least-mean-square or linear regression formulation. We use one-vs.-rest strategy for classification; i.e., the number of linear regressions is the same as that of the class, and the score of each linear regression model is evaluated. This setting guarantees loseless weight update until the features remain unchanged.\nP0 = I, B0 = 0\nPt = Pt\u22121[I \u2212 \u03c6t\u03c6 T t Pt\u22121\n1+\u03c6T t Pt\u22121\u03c6t\n]\nBt = Bt\u22121 + \u03c6 T t yt\nw\u2217t = PtBt\nWhere yt is the Boolean scalar whether the class is true or false (i.e., 0 or 1), and \u03c6t is a kernel vector of tth instance, the form of kernel \u03c6 can have various features, and the search space of the set of kernels is an exponential of an exponential. To tackle this problem, we use evolutionary approach to find a near optimal set of kernels. We randomly make new kernels and discard some kernels less relevant. Algorithm 4 explains the online learning procedure of multiplicative hypernetworks.\nAlgorithm 4 Learning Multiplicative Hypernetworks repeat\nGet a new instance dnew . if dnew includes new raw feature then\nMake new kernels \u03c6new including the values of new feature explicitly. Merge \u03c6new into kernels of model \u03c6. Fine-tune weights of kernels W of \u03c6 with the storage D. Discard some kernels in \u03c6 which seem to be less relevant to target value.\nend if Update W with dnew. Combine dnew to D (i.e. D \u2190 D\n\u22c3{dnew}). Throw away some data in the storage seem to be less important for learning in the near future.\nuntil forever"}, {"heading": "4.3. Experiments", "text": "We evaluate the performance of the proposed fast memory learning algorithm with convolutional neural networks (CNNs) and mHNs on CIFAR-10 dataset. In this setting, we split the entire training data into the 10 online datasets with non-stationary distribution of the class. In particular, the first online dataset consists of 40% of class 1, 40% of class 2, and 20% of class 3 data. The second online dataset consists of 40% of class 1, and 20% of class 2 \u2013 5 data.\nThe third online dataset consists of 20% each of class 1 \u2013 5 data. The fourth online dataset consists of 20% each of class 2 \u2013 6 data, and so on. We maintain 1/10 of entire dataset as the number of training examples Nmemory in the storage. We mainly validate mHNs on the deep neural networks where the neural prior ensemble is used for learning deep memory. We train mHNs in strictly online manner until new weak learner of ensemble is added; otherwise we allow the model to use previous data it has ever seen. It is limitation of our works and will be discussed and improved in follow-up studies.\nThe main experiment results on the fast memory models are shown in Figure 5. We use neural prior ensemble for deep memory when we validate the fast memory algorithms. Although not illustrated in the figure, the mini-batch-shift gradient descent and neural prior converge rapidly with the new online dataset and forget the information of old online datasets, as indicated by the research on catastrophic for-\ngetting. Thus, the performance of the deep memory algorithm on a single neural network does not exceed 50% because each online dataset does not include more than 50% of the classes. The accuracy of the neural prior ensemble exceeds 60%, but it is not sufficient compared with that of the batch learner. The fast memory algorithms \u2013 the mHNs on the CNNs, the SVMs on the CNNs \u2013 work better than a single deep memory algorithm. A difference of the performance between mHNs and SVMs in the latter phase is conspicuous in the figure, whose meaning and generality is discussed in follow-up studies.\nThe bottom subfigure of Figure 5 shows the performance of the mHNs on the CNNs plotted at the exact time that one new instance arrives. Small squares note the points that before and after a new weak neural network is made by the neural prior ensemble algorithm. The figure shows not only that fast memory rapidly learns from each instance of the data stream, but also that the learning of the weak deep neural networks is also required. In our experiments, learning mHNs is approximately 100 times faster than learning weak neural networks on average."}, {"heading": "5. Conclusion", "text": "We introduced dual memory architectures to train deep representative systems without much loss of online learnability. In this paper, we studied some properties of online deep learning. First, deep neural networks have online learnability on large-scale object classification tasks for stationary data stream. Second, for extreme non-stationary data stream, deep neural networks forget what they learned previously; therefore, making a new module incrementally can alleviate this problem. Third, by transferring knowledge from an old module to a new module, the performance of online learning systems is increased. Fourth, by placing shallow kernel networks on deep neural networks, the online learnability of the architecture is enhanced.\nIn this paper, numerous practical and theoretical issues are revealed, which will be soon discovered in our follow-up studies. We hope these issues will be discussed in the workshop."}, {"heading": "Acknowledgments", "text": "This work was supported by the Naver Labs. This work was partly supported by the NRF grant funded by the Korea government (MSIP) (NRF-2010-0017734Videome) and the IITP grant funded by the Korea government (MSIP) (R0126-15-1072-SW.StarLab, 10035348- mLife, 10044009-HRI.MESSI)."}], "references": [{"title": "Discovering structure in multiple learning tasks: The TC algorithm", "author": ["S. Thrun", "J. O\u2019Sullivan"], "venue": "In ICML,", "citeRegEx": "Thrun and O.Sullivan.,? \\Q1996\\E", "shortCiteRegEx": "Thrun and O.Sullivan.", "year": 1996}, {"title": "ELLA: An Efficient Lifelong Learning Algorithm", "author": ["P. Ruvolo", "E. Eaton"], "venue": "In ICML,", "citeRegEx": "Ruvolo and Eaton.,? \\Q2013\\E", "shortCiteRegEx": "Ruvolo and Eaton.", "year": 2013}, {"title": "Multilingual acoustic models using distributed deep neural networks", "author": ["G. Heigold", "V. Vanhoucke", "A. Senior", "P. Nguyen", "M. Ranzato", "M. Devin", "J. Dean"], "venue": "In ICASSP,", "citeRegEx": "Heigold et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Heigold et al\\.", "year": 2013}, {"title": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "In ICML,", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "How transferable are features in deep neural networks", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "In NIPS,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}, {"title": "Learn++: An Incremental Learning Algorithm for Supervised Neural Networks", "author": ["R. Polikar", "L. Udpa", "S.S. Udpa"], "venue": "IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS PART C: APPLICATIONS AND REVIEWS,", "citeRegEx": "Polikar et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Polikar et al\\.", "year": 2001}, {"title": "Online Bagging and Boosting", "author": ["N.C. Oza", "S. Russell"], "venue": "In AISTATS,", "citeRegEx": "Oza and Russell.,? \\Q2001\\E", "shortCiteRegEx": "Oza and Russell.", "year": 2001}, {"title": "MatConvNet \u2013 Convolutional Neural Networks for MATLAB", "author": ["A. Vedaldi", "K. Lenc"], "venue": null, "citeRegEx": "Vedaldi and Lenc.,? \\Q2014\\E", "shortCiteRegEx": "Vedaldi and Lenc.", "year": 2014}, {"title": "Sparse population code models of word learning in concept drift", "author": ["B.-T. Zhang", "J.-W. Ha", "M. Kang"], "venue": "In CogSci,", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}, {"title": "An Incremental Feature Learning Algorithm Based on Least Square Support Vector Machine", "author": ["X. Liu", "G. Zhang", "Y. Zhan", "E. Zhu"], "venue": "Frontiers in Algorithmics,", "citeRegEx": "Liu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2008}, {"title": "Online Incremental Feature Learning with Denoising Autoencoders", "author": ["G. Zhou", "K. Shon", "H. Lee"], "venue": "In AISTATS,", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": "Online deep learning can have good properties from the perspective of lifelong learning because deep neural networks show good performance on recognition problems, and their transfer and multi-task learning problem (Heigold et al., 2013; Donahue et al., 2014; Yosinski et al., 2014).", "startOffset": 215, "endOffset": 282}, {"referenceID": 3, "context": "Online deep learning can have good properties from the perspective of lifelong learning because deep neural networks show good performance on recognition problems, and their transfer and multi-task learning problem (Heigold et al., 2013; Donahue et al., 2014; Yosinski et al., 2014).", "startOffset": 215, "endOffset": 282}, {"referenceID": 4, "context": "Online deep learning can have good properties from the perspective of lifelong learning because deep neural networks show good performance on recognition problems, and their transfer and multi-task learning problem (Heigold et al., 2013; Donahue et al., 2014; Yosinski et al., 2014).", "startOffset": 215, "endOffset": 282}, {"referenceID": 5, "context": "There are several studies that use the incremental ensemble approach (Polikar et al., 2001; Oza & Russell, 2001).", "startOffset": 69, "endOffset": 112}, {"referenceID": 3, "context": "of the transferability of the deep memory, the fast memory has remarkable performance, especially for new distributions and additional classes, as though the fast memory had already trained from many new instances with the same class and similar style (Donahue et al., 2014).", "startOffset": 252, "endOffset": 274}, {"referenceID": 4, "context": "To transfer from a general neural network C to each weak neural network W , we use the initialize and fine-tune approach suggested in (Yosinski et al., 2014).", "startOffset": 134, "endOffset": 157}, {"referenceID": 4, "context": "Using this method, (Yosinski et al., 2014) achieved 2.", "startOffset": 19, "endOffset": 42}, {"referenceID": 8, "context": "This model is inspired by the sparse population coding model (Zhang et al., 2012) and it is revised to be fit to the classification task we want to solve.", "startOffset": 61, "endOffset": 81}, {"referenceID": 9, "context": ", a support vector machine (SVM) (Liu et al., 2008), and an efficient lifelong learning algorithm (ELLA) (Zhou et al.", "startOffset": 33, "endOffset": 51}, {"referenceID": 10, "context": ", 2008), and an efficient lifelong learning algorithm (ELLA) (Zhou et al., 2012), among which SVM is our comparative model.", "startOffset": 61, "endOffset": 80}], "year": 2015, "abstractText": "The online learning of deep neural networks is an interesting problem of machine learning because, for example, major IT companies want to manage the information of the massive data uploaded on the web daily, and this technology can contribute to the next generation of lifelong learning. We aim to train deep models from new data that consists of new classes, distributions, and tasks at minimal computational cost, which we call online deep learning. Unfortunately, deep neural network learning through classical online and incremental methods does not work well in both theory and practice. In this paper, we introduce dual memory architectures for online incremental deep learning. The proposed architecture consists of deep representation learners and fast learnable shallow kernel networks, both of which synergize to track the information of new data. During the training phase, we use various online, incremental ensemble, and transfer learning techniques in order to achieve lower error of the architecture. On the MNIST, CIFAR-10, and ImageNet image recognition tasks, the proposed dual memory architectures performs much better than the classical online and incremental ensemble algorithm, and their accuracies are similar to that of the batch learner. ICML workshop on Deep Learning 2015, Lille, France, 2015. Copyright 2015 by the author(s).", "creator": "LaTeX with hyperref package"}}}