{"id": "1506.04908", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2015", "title": "Learning with Clustering Structure", "abstract": "We study a supervised clustering problem seeking to cluster either features, tasks or sample points using losses extracted from supervised learning problems. We formulate a unified optimization problem handling these three settings and derive algorithms whose core iteration complexity is concentrated in a k-means clustering step, which can be approximated efficiently. We test our methods on both artificial and realistic data sets extracted from movie reviews and 20NewsGroup.\n\n\n\n\nAs an exploratory step in the analysis, we first calculated the likelihood of clustering using a linear regression in the estimation of a single user. For instance, in the case of Amazon, we computed the likelihood of a particular distribution to predict a number of user groups in a given sample of reviews with a single user (e.g., Amazon, Amazon). Then, in the case of Google, we estimated a number of users in the same sample of reviews with a single user and then we evaluated whether that information matched an approximate user group, or if it matched one or two of these two categories. In all cases, we then used a fixed set of the weighted user groups, based on a weighted distribution of the weighted user groups as a random number. The likelihood of a particular category was computed by averaging the likelihood of a particular group in a given sample and then averaging the probability of each category separately. We then used a linear regression in the estimation of a single user group, based on a random number. We used a set of weighted user groups as a random number to compute the likelihood of a particular group in a given sample and then averaging the likelihood of each category separately.\n\n\n\nThe linear regression model of Amazon was derived from an analysis of the variance in the expected number of reviews, or the average average chance of a particular review group, in which both user and company were assessed using weighted distribution as a random number, respectively. For each reviewer, there was a random number, for each reviewer that was present. This was chosen by the reviewers, for each reviewer that was present, and if the reviewers selected a particular review group, they had to wait 20 minutes. Because the reviews were highly correlated with their expected number of reviews, we then chose to use the weighted distribution as a random number to estimate the number of reviews that were actually present and the probability of a particular review group. The likelihood of a particular review group was computed in terms of expected number of reviews (n = 30), which was chosen by the reviewers.\nIn terms of the likelihood of a particular reviewer group in a given review", "histories": [["v1", "Tue, 16 Jun 2015 10:44:30 GMT  (241kb,D)", "http://arxiv.org/abs/1506.04908v1", "Submitted to NIPS"], ["v2", "Fri, 11 Mar 2016 09:22:47 GMT  (244kb,D)", "http://arxiv.org/abs/1506.04908v2", "Submitted to ICML"], ["v3", "Mon, 19 Sep 2016 08:37:40 GMT  (212kb,D)", "http://arxiv.org/abs/1506.04908v3", "Completely rewritten. New convergence proofs in the clustered and sparse clustered case. New projection algorithm on sparse clustered vectors"]], "COMMENTS": "Submitted to NIPS", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["vincent roulet", "fajwel fogel", "alexandre d'aspremont", "francis bach"], "accepted": false, "id": "1506.04908"}, "pdf": {"name": "1506.04908.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": "1. INTRODUCTION", "text": "Using information from a supervised problem like regression or classification, supervised clustering seeks to discover hidden clusters of features, tasks or samples that both help inference, and provide additional structural insights on the data.\nIn the classical multi-task setting, clustering predictors of related tasks often helps prediction. In computer vision for instance, classifiers associated with different species of cats should be quite similar, but well separated from classifiers associated with cars, and incorporating this structural information at the training stage should improve performance. This problem has been well studied in the multi-task learning literature [1, 2, 3].\nSimilarly, when there exists some groups of highly correlated features, reducing dimensionality by assigning the same weights to some groups of features can be beneficial both in terms of prediction and interpretation [4]. This often occurs in text classification, where it is natural to group together words having the same meaning for a given task [5, 6].\nFinally, in some settings, it can be valuable to cluster sample points, with each cluster having its own distinct prediction function [7]. This last problem may have applications in the context of privacy learning, where the group information of an individual may not be revealed because of confidentiality issues.\nHere, we present a unified and flexible framework for supervised clustering over the \u201cdata cube\u201d of either tasks, features or sample points (a representation introduced by [8]). We directly formulate supervised clustering as an optimization problem on the clustered predictors, where clustering is either a hard constraint or a soft regularization penalty, and losses are adapted to the application at hand (classification or regression, clustering tasks, features or sample points). We propose several optimization schemes to solve these problems efficiently. While the original optimization problem is non-convex, we show that the core nonconvexity is concentrated in a subproblem similar to k-means, which we solve using classical approximation techniques [9]. In the particular case of feature clustering for regression, the k-means steps are performed in dimension one, and can therefore be solved exactly by dynamic programming [10, 11]. Our formulation is then an explicit convex relaxation which can be solved efficiently using the conditional gradient algorithm [12, 13]. We describe experiments on both synthetic and real datasets involving large corpora of text where our method compares favorably with standard benchmarks.\nDate: June 17, 2015. Key words and phrases. Clustering, Multitask, Dimensionality Reduction, Multi Outpout. The two first authors contributed equally.\nar X\niv :1\n50 6.\n04 90\n8v 1\n[ cs\n.L G\n] 1\n6 Ju\nn 20\n15"}, {"heading": "2. SUPERVISED CLUSTERING OF FEATURES, TASKS OR SAMPLES", "text": "We write supervised clustering as a clustering problem penalized by a supervised learning loss. We let n be the number of training examples in the dataset, d the number of features (i.e. the ambient dimension) and K the number of tasks. For regression or binary classification K = 1, while for multi-classification K corresponds to the number of classes (using one-versus-all majority vote, training one binary classifier per class vs all others). For simplicity, we consider only square or logistic losses for linear predictors.\nOur main variable is a matrix of predictors with one column per task, writtenW = [w1, ...,wK ] \u2208 Rd\u00d7K . We let Q be the desired number of clusters and m the number of items to cluster. When clustering tasks, m = K and we are grouping the predictors associated with each task, i.e. the columns of W . When clustering features, m = d and we are clustering the predictors associated with each feature, i.e. the rows of W . Finally, when clustering sample points, m = n and we introduce individual predictor vectors W (i) for each sample point i, which we cluster together to obtain Q distinct predictors associated with a partition of the points into Q groups. Hence, the term \u201cpredictors\u201d either refers to columns of W , rows of W , or individual predictor vectorsW (i), depending on which dimension of the data cube (tasks, features or sample points) the clustering is performed. A summary of these settings is given in Table 1. In the following we use the generic notations U and V to designate predictors.\nA clustering can be seen as a partition C1 \u222a C2 \u222a . . . \u222a CQ = [1,m] of predictors, to which corresponds an assignment matrix Z \u2208 {0, 1}m\u00d7Q such that Zij = 1 if item i is in cluster Cj . As in k-means, we define a matrix of centroids C = [c1, . . . , cQ], with each centroid cj equal to the mean of predictors in cluster j. This information is summarized in the matrix V = CZT , which we call matrix of individual centroids (MIC), whose columns vi are such that vi = cj if predictor i is in cluster Cj .\nGiven a supervised learning loss L(\u00b7) and a regularizing penalty \u2126(\u00b7) on predictors, we formulate the supervised clustering problem as an optimization problem over the set of MIC matrix V . Enforcing the hard constraint that predictors are exactly confounded with centroids, we get the following hard supervised clustering problem (HSC)\nminimize L(V ) + \u2126(V ) subject to V = CZT , Z \u2208 {0, 1}m\u00d7Q, Z1 = 1. (HSC)\nIf instead we want to allow predictors wi to deviate from their assigned centroid vi, we can relax the hard clustering constraint using a regularizer \u2126SC(U, V ) described below. This yields the soft supervised clustering problem\nminimize L(U) + \u2126SC(U, V ) + \u2126(V ) subject to V = CZT , Z \u2208 {0, 1}m\u00d7Q, Z1 = 1. (SSC)\nGiven Q clusters Cq, we let sq = |Cq| be the size of the qth cluster, with s = ZT1 the corresponding vector. We write \u03a0 = I\u2212 1m11T the centering matrix. As in [2], the clustering penalty \u2126SC(W,V ) can be decomposed into three separable terms as follows (see Figure 1 for an illustration). \u2022 A measure of how large the barycenter of the centers c\u0304 = 1m \u2211Q q=1 sqcq is\n\u2126mean(V ) = \u03bbm m\n2 ||c\u0304||22 = \u03bbm 2\nTr(V (I\u2212\u03a0)V T ). \u2022 A measure of the variance between clusters\n\u2126between(V ) = \u03bbb 2\nQ\u2211\nq=1\nsq||cq \u2212 c\u0304||22 = \u03bbb 2 Tr(V\u03a0V T ).\n\u2022 A measure of the variance within clusters\n\u2126within(U, V ) = \u03bbw 2\nQ\u2211\nq=1\n\u2211\ni\u2208Jq\n||ui \u2212 cq||22 = \u03bbw 2 ||U \u2212 V ||2F .\nWe then simply add a penalty with parameter \u00b5 on the Frobenius norm of V , i.e. the norm of the centroids weighted by the number of items in each cluster sq\n\u2126(V ) = \u00b5\n2\nQ\u2211\nq=1\nsq\u2016cq\u201622 = \u00b5\n2 Tr(V TV ).\nWe now detail the losses associated with each dimension of the data cube: tasks, features or sample points. Input samples are given by the matrix X = [x1, . . . ,xn]T \u2208 Rn\u00d7d, labels by (y1, . . . , yn), and x(j) refers to the jth coordinate of x.\n2.1. Clustering features. Given a regression or classification task, we want to reduce dimensionality by grouping together features which have a similar influence on the output. We present the linear regression case [4], which can be extended to logistic regression and classification. Imposing that all predictor coefficients within a cluster with (scalar) centroid cq are identical means the prediction function can be written f : x\u2192\u2211Qq=1 cq \u2211 j\u2208Cq x (j), which leads to the following loss\nL(V ) = 1\nn\nn\u2211\ni=1\nl  yi, Q\u2211\nq=1\ncq \u2211\nj\u2208Cq\nx (j) i\n  = 1\nn\nn\u2211\ni=1\nl  yi, d\u2211\nj=1\nQ\u2211\nq=1\nZjqcqx (j) i\n  ,\nin the variable V = \u2211Q\nq=1 Zjqcq = CZ T \u2208 Rd of predictor coefficients, quantized over Q values. In this\ncase, if we relax the hard clustering by imposing a soft clustering penalty, we lose the benefit of dimensionality reduction.\n2.2. Clustering tasks. Given a set of K supervised tasks like regression or binary classification, multitask learning aims at jointly solving these tasks, hoping that each task can benefit from the information given by other tasks [1, 2, 3]. For simplicity, we illustrate the case of multi-classification, which can be extended to the general multitask setting. When performing classification with one-versus-all majority vote, we train one binary classifier for each class vs all others. We define the total empirical loss as\nL(U) = 1\nn\nK\u2211\nk=1\nn\u2211\ni=1\nl(yi,u T k xi).\nin the matrix variable U \u2208 Rd\u00d7K of classifier vectors (one per task). 2.3. Clustering sample points. Imagine for instance that we are interested in measuring the effect of two treatments, e.g. a medicine and a placebo, on uniformly distributed patients. Clearly, the regression function varies a lot between the groups of patients that have a different treatment. Now suppose that we do not know to which patients different treatments were given. Since patients are uniformly distributed, the groups cannot be predicted without supplementary information. We will use the effect of treatments to simultaneously\nretrieve the groups of patients and the associated regression functions. Note that this setting is different from a mixture of experts model in the sense that the latent cluster assignment variable Z can only be estimated once y is known and cannot be deduced from the input features X (cf. figure 2). 162 163 164 165 166 167 168 169 170 171 172 173\n174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\npatients different treatments were given. Since patients are uniformly distributed, the groups cannot be predicted without supplementary information. We will use the effect of treatments to simultaneously retrieve the groups of patients and the associated regression functions. Note that this setting is different from a mixture of experts model in the sense that the latent cluster assignm nt variable Z can only be estimated once y is known and cannot be deduced from the input features X . Z X Y Z X Y\nFigure 2: Learning multiple diverse predictors (left), mixture of experts model (right).\nGiven a regression or multi-classification task, we want to find Q groups of sample points to maximize the within-group prediction performance using a group specific predictor. This amount to producing Q diverse answers per sample point, considering only the best one. We thus learn Q predictors, each predictor having low error rate on some cluster of points. For simplicity, we illustrate the case of regression, which can be extended to multi-classification. We minimize the loss incurred by the best linear predictor fq : x! cTq x for each point, i.e. L(V ) = L(CZT ) = 1 n nX i=1 min q2{1,...,Q} l(yi, c T q xi) = 1 n nX i=1 l yi, QX q=1 Ziqc T q xi ! , in the matrix variable V 2 Rd\u21e5n of predictor vectors (one per sample point), where C 2 Rd\u21e5Q and Z 2 {0, 1}n\u21e5Q are the centroid and assignment matrices defined above. Loss Dim. U, V Predictors Goal Features 1n Pn i=1 l \u21e3PQ q=1 Pd j=1 Zjqcqxij \u2318 1\u21e5 d rows of W regression Tasks 1n PK k=1 Pn i=1 l(w T k xi) d\u21e5K columns of W classification Samples 1n Pn i=1 l \u21e3PQ q=1 Ziqc T q xi \u2318 d\u21e5 n W (i) regression Table 1: Summary of the presented supervised clustering settings. 3 Algorithms\nWe now present optimization strategies to solve these supervised clustering problems. We begin by\nsimple greedy procedures, then propose a non-convex projected gradient descent scheme and finally a more refined convex relaxation solved using conditional gradient and approximations to k-means. 3.1 Simple strategies A straightforward strategy is to first minimize on predictors, as in a classical supervised learning problem, and then cluster predictors together using k-means. The procedure can be repeated in the case of a soft clustering penalty. In the same spirit, when clustering sample points, one can alternate minimization on the predictors of each group and assignment of each point to the group where its loss is smallest. These methods are fast but very dependent on the initialization. Alternating minimization can optionally be used to refine the solution of the more robust algorithms proposed below. 3.2 Projected gradient descent A natural strategy is to do a projected gradient descent on the non-convex problems (HSC) or (SSC). The projection of a matrix V is made by finding\nargmin Z,C\nkV CZT k2F = argmin QX\nq=1\nX i2Cq kvi cqk22,\nFIGURE 2. Learning ultiple diverse predictors (left), mixture of experts model (right).\nGiven a r gression or multi-classification task, w wa t to findQ groups of sample points t maximize the within-group prediction performance using a group specific predictor. This amount to producing Q diverse answers per sample point, considering only the best one. We thus learn Q predictors, each predictor having low error rate on some cluster of points. For simplicity, we illustrate the case of regression, which can be extended to multi-classification. We minimize the loss incurred by the best linear predictor fq : x \u2192 cTq x for each point, i.e.\nL(V ) = L(CZT ) = 1\nn\nn\u2211\ni=1\nmin q\u2208{1,...,Q}\nl(yi, c T q xi) =\n1\nn\nn\u2211\ni=1\nl  yi, Q\u2211\nq=1\nZiqc T q xi\n  ,\nin the matrix variable V \u2208 Rd\u00d7n of predictor vectors (one per sample point), where C \u2208 Rd\u00d7Q and Z \u2208 {0, 1}n\u00d7Q are the centroid and assignment matrices defined above.\nLoss Dim. U, V Predictors Goal Features 1n \u2211n i=1 l (\u2211Q q=1 \u2211d j=1 Zjqcqxij ) 1\u00d7 d rows of W regression Tasks 1n \u2211K k=1 \u2211n i=1 l(w T k xi) d\u00d7K columns of W classification Samples 1n \u2211n i=1 l (\u2211Q q=1 Ziqc T q xi ) d\u00d7 n W (i) regression\nTABLE 1. Summary of the presented supervised clustering settings."}, {"heading": "3. ALGORITHMS", "text": "We now prese t optimization stra egies to solve these supervised clustering problems. We begin by simple greedy procedures, then propose a non-convex projected gradient descent scheme and finally a more refined convex relaxation solved using conditional gradient and approximations to k-means.\n3.1. Simple strategies. A straightforward strategy is to first minimize on predictors, as in a classical supervised learning problem, and then cluster predictors together using k-means. The procedure can be repeated in the case of a soft clustering penalty. In the same spirit, when clustering sample points, one can alternate minimization on the predictors of each group and assignment of each point to the group where its loss is smallest. These methods are fast but very dependent on the initialization. Alternating minimization can optionally be used to refine the solution of the more robust algorithms proposed below.\n4\n3.2. Projected gradient descent. A natural strategy is to do a projected gradient descent on the non-convex problems (HSC) or (SSC). The projection of a matrix V is made by finding\nargmin Z,C\n\u2016V \u2212 CZT \u20162F = argmin Q\u2211\nq=1\n\u2211\ni\u2208Cq\n\u2016vi \u2212 cq\u201622,\nwhere the minimum is taken over centroids ci and partitions (C1, . . . , CQ). This can be solved with the kmeans++ algorithm which performs alternate minimization on the assignments and the centroids. Although it is a non-convex problem, k-means++ gives general approximation bounds on its solution [9].\nWriting k-means++(V,Q) the approximate solution of the projection. Writing \u03c6 the objective function and using a backtracking line search for the stepsize \u03b1t, the full procedure is summarized in Algorithms 1 and 2. Details of gradient computations for each setting are given in the appendix.\nAlgorithm 1 Proj. Gradient Descent (SSC) Input: X, y,Q, , \u03bbb, \u03bbw, \u03bbm, \u00b5\nInitialize W0 = V0 = 0 while |\u03c6(Wt, Vt)\u2212 \u03c6(Wt\u22121, Vt\u22121)| \u2265 do Wt+1 = Wt \u2212 \u03b1t(\u2207L(Wt) +\u2207\u2126S,C(Vt,Wt)) Vt+ 1\n2 = Vt \u2212 \u03b1t\u2207\u2126S,C(Vt,Wt)\n[Zt+1, Ct+1] = k-means++(Vt+ 1 2 , Q) Vt+1 = Ct+1Z T t+1\nend while Z\u2217 and C\u2217 are given through Kmeans++\nOutput: W \u2217, V \u2217, Z\u2217, C\u2217\nAlgorithm 2 Proj. Gradient Descent (HSC) Input: X, y,Q, , \u00b5\nInitialize V0 = 0 while |\u03c6(Vt)\u2212 \u03c6(Vt\u22121)| \u2265 do Vt+ 1\n2 = Vt \u2212 \u03b1t(\u2207L(V ) +\u2207\u2126(V ))\n[Zt+1, Ct+1] = k-means++(Vt+ 1 2 , Q) Vt+1 = Ct+1Z T t+1\nend while Z\u2217 and C\u2217 are given through Kmeans++\nOutput: V \u2217, Z\u2217, C\u2217\n3.3. Convex relaxation using approximate conditional gradient . Another algorithmic approach to the supervised clustering problem is to minimize with respect to the assignment matrix Z using the Frank-Wolfe algorithm (a.k.a. conditional gradient, [12, 13]). Considering the squared loss l(f(x), y) = 12(y \u2212 f(x))2, we use the analytic form of the minimization in (W,V ) or V to rewrite the problem. The clustering is then captured in terms of the equivalence matrix M = Z(ZTZ)\u22121ZT , which satisfies Mij = 1/|Cq| if item i and j are in the same cluster q and Mij = 0 otherwise.\nWe describe here the simple case of supervised clustering of features in a regression task, introduced in \u00a72.1. Detailed computations and explicit procedures for all settings are given in the appendix. When clustering features, we optimize over the coefficients associated with each feature and the k-means step can be performed exactly using dynamic programming [10, 11].\nThe loss L can be written here\nL(V ) = L(CZT ) = 1\n2n\nn\u2211\ni=1\n yi \u2212 d\u2211\nj=1\nQ\u2211\nq=1\nZjqcqx (j) i\n  2\n,\nwhere C = [c1, . . . , cQ] \u2208 R1\u00d7Q, hence the regularized loss becomes\n\u03c6(C,Z) = 1\n2n\nn\u2211\ni=1\n( yi \u2212 CZT xi )2 + \u00b5\n2 \u2016CZT \u201622\n= 1\n2n Tr(yT y) +\n1 2n Tr(CZTXTXZCT )\u2212 1 n Tr(CZTXT y) + \u00b5 2 Tr(CZTZCT ).\nMinimizing in C and using the Sherman-Woodbury-Morrison formula we get\nG(M) := min C L(CZT ) +\n\u00b5 2 \u2016CZT \u201622\n= 1\n2n\n( y yT ( I\u2212XZ(ZTXTXZ + \u00b5nZTZ)\u22121ZTXT y ))\n= 1\n2n Tr\n( y yT ( I + 1\nn\u00b5 XMXT\n)\u22121) .\nDefiningM as the set of equivalence matrices of the formM = Z(ZTZ)\u22121ZT , forZ an assignment matrix, each iteration of the conditional gradient method requires solving an affine minimization subproblem over hull(M). The setM being discrete, we have argminM\u2208hull(M)Tr(M\u2207G(M)) = argminM\u2208MTr(M\u2207G(M)). Writing P = \u2212\u2207G(M), we get\nP = 1\n2n2\u00b5 XT\n( I + 1\nn\u00b5 XMXT\n)\u22121 y yT ( I + 1\nn\u00b5 XMXT\n)\u22121 X,\nwhich is always semidefinite positive. Writing P 1 2 the matrix square root of P we have\nargmin M\u2208M Tr(M\u2207G(M)) = argmin M\u2208M\n\u2212Tr(MP 12P 12 T )\n= argmin M\u2208M\nTr((I\u2212M)P 12P 12 T ))\n= argmin M\u2208M\n\u2016P 12 \u2212MP 12 \u20162F\n= argmin Z min C \u2016P 12 \u2212 ZCT \u20162F ,\nwhere we recognize the k-means problem defined above. In fact, in this particular case, the k-means subproblem is one-dimensional and can be solved exactly using dynamic programming [10, 11]. We use the classical stepsize for conditional gradient \u03b1k = 2k+2 and Frank-Wolfe rounding. The procedure is summarized in Algorithm 3.\nAlgorithm 3 Conditional gradient on the equivalence matrix Input: X, y,Q,\nInitialize M0 \u2208M while |G(Mk)\u2212G(Mk\u22121)| \u2265 do\nCompute the matrix square root P 1 2 of \u2212\u2207G(Mk) Get oracle \u2206k = k-means(P 1 2 , Q)\nMk+1 = Mk + \u03b1k(\u2206k \u2212Mk) end while Use Frank Wolfe rounding to get a solution M\u2217 \u2208M Z\u2217 is given by k-means C\u2217 is given by the analytic solution of the minimization for Z\u2217 fixed\nOutput: C\u2217, Z\u2217,M\u2217\n3.4. Complexity. The core complexity of Algorithms 1 and 2 is concentrated in the inner k-means subproblem, which standard alternating minimization approximates inO(tQS), where t is the number of alternating steps, Q is the number of clusters, and S is the product of the dimensions of V (see Table 1). Using a proper conditioning of the gradient, the number of iterations before convergence is typically below 100, which make Algorithms 1 and 2 both fast and scalable. For Algorithm 3, we also need to compute a matrix square\nroot of the gradient at each iteration, which can slow down computations for large datasets. The choice of the number of clusters can be done given an a priori on the problem (e.g. if we know the hierarchical structure of classes in a classification problem), or cross-validation, idem for the other regularization parameters.\n4. NUMERICAL EXPERIMENTS\n4.1. Synthetic dataset. Supervised clustering of sample points. We generate n data points (xi, yi) for i = 1, . . . , n with xi \u2208 Rd and yi \u2208 R, divided in two clusters corresponding to regression tasks with weights w1 and w2. Regression labels for points xi in cluster q are given by yi = wTq xi +\u03b7, where \u03b7 \u223c N (0, \u03c32). We test the robustness of the algorithms to noise dimensions, i.e. we complete xi with dn dimensions of noise \u03b7d \u223c N (0, \u03c3d). The results are reported in Table 2.\nHere, the intrinsic dimension is 10. \u201cOracle\u201d refers to the least square fit given the true assignments. It can be seen as the best error rate that can be achieved. PGK refers to projected gradient, OM refers to conditional gradient on M , AM refers to alternate minimization. PGK and OM were followed by AM refinement. 200 points were used for training, 200 for testing. The regularization parameter was set to \u03bb = 10\u22122 for all experiments. Noise on labels and added dimensions \u03c3y = \u03c3d = 2\u00d7 10\u22121. Results were averaged over 100 experiments, figures after the sign \u00b1 correspond to one standard deviation.\nIt appears that that PGK and OM give very similar results, significantly improving on the naive alternating minimization scheme. In view of standard deviations, OM seems more robust. Supervised clustering of features. We generate n data points (xi, yi) for i = 1, . . . , n with xi \u2208 Rd and yi \u2208 R. Regression weights have only 10 different values wq for q = 1, . . . , 10, uniformly distributed around 0. Regression labels are given by yi = wT xi +\u03b7, where \u03b7 \u223c N (0, \u03c32). We test the robustness of the algorithms to the number of learning examples, i.e. the size of the training set n. We measure the l2 norm of the difference between the true vector of weights and the estimated ones.\nWe compare in Table 3 the proposed algorithms to OSCAR [4], Ridge, Lasso and Ridge followed by k-means on the weights (using associated centroids as predictors). \u201cOracle\u201d refers to the mean square fit given the true assignments of features. It can be seen as the best error rate that can be achieved. PGK refers to projected gradient and was intitialized with the solution of Ridge followed by k-means, OM refers to conditional gradient on M and was followed by PGK. Noise on labels is set to \u03c3 = 10\u22121. Algorithm parameters were all cross-validated using a logarithmic grid. Results were averaged over 30 experiments and figures after the sign\u00b1 correspond to one standard deviations. Results were multiplied by 100 to shorten the table.\nPGK and OM appear to give significantly better results than other methods and even reach the performance of the Oracle for n > d, while for n \u2264 d results are in the same range. Note that contrary to OSCAR, Lasso and Ridge, reduction of dimensionality is guaranteed (the number of clusters is fixed).\n4.2. Real data.\n4.2.1. 20NewsGroup classification. We perform classification on 2800 documents extracted from the publicly available 20NewsGroup dataset, which contains 20 different newsgroups, each one corresponding to a different topic. Some of the newsgroups are very closely related to each other (e.g. comp.sys.ibm.pc.hardware and comp.sys.mac.hardware), while others are highly unrelated (e.g. misc.forsale and soc.religion.christian). Our goal is to retrieve clusters of related newsgroups, while performing competitive classification. We used a dictionary of 5000 words chosen by tf-idf and took the empirical distribution over words as covariates for each document.\nIn Table 4, we compare our approach to other classical regularizations such as the Frobenius norm and the trace norm, as implemented in [3], using either a ridge or a logistic loss. As the projected gradient descent scheme is not convex we initialize it by the solution given by the logistic loss non regularized. All algorithms were 5-fold cross-validated on 80% of the data then tested on the remaining 20%. The number of clusters was set to 5, as suggested by the names of newsgroups. Figures after the sign \u00b1 correspond to one standard deviation when varying the training and test sets. As reported in Table 4, it appears that the clustering regularization helps prediction on topics."}, {"heading": "Frobenius Logistic Trace Ridge OM Ridge PGK Logistic", "text": "4.2.2. Predicting ratings associated with reviews using groups of words. We perform \u201csentiment\u201d analysis of newspaper movie reviews. We use the publicly available dataset introduced in [14] which contains movie reviews paired with star ratings. We treat it as a regression problem, taking log-responses for y in (0, 1) and the empirical distribution over words as covariates. With a 5000 term vocabulary chosen by tf-idf, the corpus contains 5006 documents and comprises 1.6M words. We evaluate our algorithms for regression with clustered features against standard regression approaches: Lasso, Ridge, and Ridge followed by kmeans on predictors. All algorithms were 5-fold cross-validated on 80% of the dataset and then tested on the remaining 20%. The number of clusters was arbitrarily set to 10, though we did not notice significant changes when varying it. Results are reported in Table 5, figures after the sign\u00b1 correspond to one standard deviation when varying the training and test sets. While all methods including ours give 10% mean absolute errors on the test set (up to 0.5% accuracy), our approach has the additional benefit of providing clusters of words which have a similar influence. Moreover we noticed that the clusters with highest absolute weights are also the ones with smallest number of words, which confirms the intuition that only a few words are very discriminative. We illustrate this in Table 6, listing words of the first and last clusters."}, {"heading": "PGK OM+PGK Ridge+Kmeans Lasso Ridge", "text": ""}, {"heading": "5. DISCUSSION", "text": "We have developed a unified framework for supervised clustering over tasks, features or samples and provided two robust algorithms for solving the associated optimization problems. Results on synthetic and realistic text datasets suggest that our method is competitive against standard regression and classification methods, while having the additional benefit of providing clusters of tasks, features or samples. Similarly as in compressed sensing, optimization is made on a union of subspaces, hence in analogy with RIP conditions for iterative hard thresholding [15], it would be interesting to see under which assumptions our algorithms can recover the optimal solution to the supervised clustering problem."}, {"heading": "6. APPENDIX", "text": "We give here detailed computations of the function G corresponding to the convex relaxation defined in 3.3 in all settings.\nWe always suppose ZTZ inversible i.e.\u2200q, sq 6= 0 (there is no empty cluster). For any integer p, we let 1p \u2208 Rp be the vector whose coordinates are all ones, Ip the identity matrix in Rp\u00d7p, \u0393p = 1p1 T p\np and \u03a0p = Ip \u2212 \u0393p the corresponding centering matrix.\nFor all settings input samples are represented by the matrix X = (x1, ...,xn)T \u2208 Rn\u00d7d and their respective labels by y = (y1, ....yn) \u2208 Y . For regression problems Y = R. Classification problems are casted into the multitask setting, each class corresponding to a task. We denote by yk = (yk1 , ..., y k n) the vector of binary labels corresponding to the class k \u2208 [[1,K]] and Y = (y1, ...,yK).\n6.1. Minimization in C for Soft Clustering Problems . For soft supervised clustering problems, the minimization in the variable C for W,Z fixed can be made without assumptions on the specific loss L. We let (\u03b4,m) be the dimensions of the predictors of interest, such thatW \u2208 R\u03b4\u00d7m,C \u2208 R\u03b4\u00d7Q andZ \u2208 {0, 1}m\u00d7Q. We denote by \u03bbW , \u03bbB, \u03bbM the weights associated respectively to the within, between and mean penalties. Minimization in C is then given by\nH(W,Z) := min C\u2208R\u03b4\u00d7Q \u03bbW 2 \u2016W \u2212 CZT \u20162F + \u03bbB 2 Tr(CZT\u03a0mZC T ) + \u03bbM 2 Tr(CZT\u0393mZC T )\n= min C\u2208R\u03b4\u00d7Q \u03bbW 2 \u2016W\u20162F + 1 2 Tr ( C ( (\u03bbW + \u03bbB)Z TZ + (\u03bbM \u2212 \u03bbB)ZT\u0393mZ ) CT ) \u2212 \u03bbW Tr(CTWZ)\n= \u03bbW 2 Tr(WW T )\u2212 \u03bb 2 W 2 Tr ( WZ ( (\u03bbW + \u03bbB)ZZ T + (\u03bbM \u2212 \u03bbB)ZT\u0393mZ )\u22121 ZTW ) .\nLet s = ZT1m \u2208 RQ be the vector whose coordinates sq are the sizes of the clusters, denoting by s 1 2\nthe vector whose coordinates are \u221a sq, and by s \u2212 1 2 the vector whose coordinates are 1\u221a\nsq , we can derive the\ninversion in the precedent formula,\nJ\u22121 = ( (\u03bbW + \u03bbB)ZZ T + (\u03bbM \u2212 \u03bbB)ZT\u0393mZ )\u22121\n= ( (\u03bbW + \u03bbB)diag(s) + (\u03bbM \u2212 \u03bbB) ssT\nm\n)\u22121\n= 1\n\u03bbW + \u03bbB diag(s\u2212 1 2 )\n IQ +\n\u03bbM \u2212 \u03bbB \u03bbW + \u03bbB s 1 2 s 1 2 T m\n  \u22121\ndiag(s\u2212 1 2 )\n= 1\n\u03bbW + \u03bbB diag(s\u2212 1 2 )\n IQ \u2212\n\u03bbM \u2212 \u03bbB \u03bbW + \u03bbM s 1 2 s 1 2 T m\n diag(s\u2212 12 )\n= 1 \u03bbW + \u03bbB (ZTZ)\u22121 \u2212 \u03bbM \u2212 \u03bbB (\u03bbW + \u03bbM )(\u03bbW + \u03bbB) 1Q1 T Q m\n= 1\n\u03bbW + \u03bbB ((ZTZ)\u22121 \u2212\n1Q1 T Q\nm ) +\n1\n\u03bbW + \u03bbM\n1Q1 T Q\nm ,\nwhere we used that p = s 1 2 s 1 2 T m = s 1 2 s 1 2 T\n\u2016s 1 2 \u201622 is a projector and therefore (I + \u03b1p)\u22121 = I \u2212 \u03b1\u03b1+1p, added to the fact that diag(s) = ZTZ.\nNow introducing the equivalence matrix M = Z(ZTZ)\u22121ZT , and using that Z 1Q1\nT Q\nm Z T = \u0393m, we\nfinally obtain\nH(W,Z) = \u03bbW 2 Tr(WW T )\u2212 \u03bb 2 W 2 Tr\n( W ( 1\n\u03bbW + \u03bbB (M \u2212 \u0393m) +\n1\n\u03bbW + \u03bbM \u0393m\n) W T )\n= \u03bbW 2\nTr ( W (Im \u2212 \u03b1(M \u2212 \u0393m)\u2212 \u03b2\u0393m)W T ) ,\nwhere \u03b1 = \u03bbW\u03bbW+\u03bbB and \u03b2 = \u03bbW \u03bbW+\u03bbM .\nDenoting P = Im \u2212 \u03b1(M \u2212 \u0393m)\u2212 \u03b2\u0393m, one can also use Kronecker\u2019s formula :\nH(W,Z) = \u03bbW 2 Vec(W )T (P \u2297 I\u03b4) Vec(W ).\n6.2. Clustered Multitask Learning . We derive here the computation of G when clustering tasks. We restrict ourselves to the case of multiclass setting cast as a multitask problem such that each task shares the same input data. GivenK classes, we denote byW = (w1, ...,wK) \u2208 Rd\u00d7K the matrix of linear predictors. Using a squared loss l(f(x), y) = 12(y \u2212 f(x))2 the empirical loss is given by :\nL(W ) = 1\n2n\nn\u2211\ni=1\nK\u2211\nk=1\n(yki \u2212wTk xi)2.\nUsing Kronecker\u2019s product formula, we get\nL(W ) = 1\n2n\nK\u2211\nk=1\nwTk X TXwk\u2212\n1\nn\nK\u2211\nk=1\nwTk X T yk +\n1\n2n \u2016y \u201622\n= 1\n2n\nK\u2211\nk=1\neTk W TXTXW ek\u2212\n1 n Tr(WXTY ) + 1 2n \u2016y \u201622\n= 1\n2n Vec(W )T (IK \u2297XTX) Vec(W )\u2212\n1 n Vec(W )T Vec(XTY ) + 1 2n \u2016y \u201622.\nUsing the expression found by minimizing in C the regularization penalty, we get an expression for G :\nG(M) = min W L(W ) +H(W,Z)\n= min W\n1\n2n Vec(W )T (IK \u2297XTX + P \u2297 Id) Vec(W )\u2212\n1 n Vec(W )T Vec(XTY ) + 1 2n \u2016y \u201622\n= \u2212 1 2n\nVec(XTY )T ( IK \u2297XTX + \u03bbWnP \u2297 Id )\u22121 Vec(XTY ) + 1\n2n \u2016y \u201622\nDenote (v1, ...,vd) \u2208 Rd\u00d7d, (\u03bb1, ..., \u03bbd) \u2208 Rd and (u1, ...,uS) \u2208 RS\u00d7S , (\u00b51, ..., \u00b5S) \u2208 RS the eigenvectors and corresponding eigenvalues respectively of matricesXTX and P = (IK\u2212\u03b1(M \u2212\u0393K)+\u03b2\u0393K). The eigenvectors and corresponding eigenvalues of IK \u2297 XTX + \u03bbWnP \u2297 Id are (ui \u2297 vj)i\u2208[[1,n]]\nj\u2208[[1,d]] and\n(\u03bbWn\u00b5i + \u03bbj)i\u2208[[1,n]] j\u2208[[1,d]] . The inversion in the expression of G is then given by\nJ\u22121 = ( IK \u2297XTX + \u03bbWnP \u2297 Id )\u22121 = n\u2211\ni=1\nd\u2211\nj=1\n1\n\u03bbWn\u00b5i + \u03bbj uiu\nT i \u2297 vjvTj .\nThen we note that the set of eigenvectors of P can be decomposed into three sets. Indeed the matrices IK \u2212M ,M \u2212 \u0393K and \u0393K are orthogonal projectors on orthogonal subspaces spanning the entire space. Denote by IW , IB , IM the sets of eigenvectors corresponding respectively to IK \u2212M , M \u2212 \u0393K and \u0393K , their corresponding eigenvalues in P can easily be computed and we obtain\nP = IK \u2212M + (1\u2212 \u03b1)(M \u2212 \u0393K) + (1\u2212 \u03b2)\u0393K = \u2211\ni\u2208IW\nuiu T i + (1\u2212 \u03b1)\n\u2211\ni\u2208IW\nuiu T i + (1\u2212 \u03b2)\n\u2211\ni\u2208IM\nuiu T i .\nThis decomposition can be used for the inversion :\nJ\u22121 = \u2211\ni\u2208IW\nd\u2211\nj=1\n1\n\u03bbWn+ \u03bbj uiu\nT i \u2297 vjvTj\n+ \u2211\ni\u2208IB\nd\u2211\nj=1\n1\n\u03bbWn(1\u2212 \u03b1) + \u03bbj uiu\nT i \u2297 vjvTj\n+ \u2211\ni\u2208IW\nd\u2211\nj=1\n1\n\u03bbWn(1\u2212 \u03b2) + \u03bbj uiu\nT i \u2297 vjvTj\n= (IK \u2212M)\u2297 (XTX + n\u03bbW Id)\u22121 + (M \u2212 \u0393K)\u2297 (XTX + n\u03bbW (1\u2212 \u03b1)Id)\u22121 + \u0393K \u2297 (XTX + n\u03bbW (1\u2212 \u03b2)Id)\u22121.\nFinally G can be simplified using properties of the Kronecker product\nG(M) =\u2212 1 2n\nVec(XTY )T ( (IK \u2212M)\u2297 (XTX + n\u03bbW Id)\u22121 ) Vec(XTY )\n\u2212 1 2n\nVec(XTY )T ( (M \u2212 \u0393K)\u2297 (XTX + n\u03bbW (1\u2212 \u03b1)Id)\u22121 ) Vec(XTY )\n\u2212 1 2n\nVec(XTY )T ( \u0393K \u2297 (XTX + n\u03bbW (1\u2212 \u03b2)Id)\u22121 ) Vec(XTY ) + 1\n2n \u2016Y \u201622\n=\u2212 1 2n Tr(Y TX(XTX + n\u03bbW Id) \u22121XTY (IK \u2212M))\n\u2212 1 2n Tr(Y TX(XTX + n\u03bbW (1\u2212 \u03b1)Id)\u22121XTY (M \u2212 \u0393K)) \u2212 1 2n Tr(Y TX(XTX + n\u03bbW (1\u2212 \u03b2)Id)\u22121XTY \u0393K) + 1 2n \u2016Y \u20162F .\nTherfore G is a linear function of M whose gradient is given by\n\u2207G(M) = 1 2n Y TX\n( (XTX + n\u03bbW Id) \u22121 \u2212 (XTX + n\u03bbW (1\u2212 \u03b1)Id)\u22121 ) XTY.\nAs 1 \u2265 \u03b1 \u2265 0, we get that \u2212\u2207G(M) 0. For a fixed M , W is given using precedent computations and Kronecker\u2019s formula by\nWM = (X TX + n\u03bbW Id) \u22121XTY (IK \u2212M) + (XTX + n\u03bbW (1\u2212 \u03b1)Id)\u22121XTY (M \u2212 \u0393K) + (XTX + n\u03bbW (1\u2212 \u03b2)Id)\u22121XTY \u0393K .\n6.3. Learning with clustered features. For more generality we cast the problem of learning clustered features in the multiclass learning setting. We restrict here to the soft supervised clustering problem as the hard one has already be done. We keep notations introduced in section 6.2 though we now have W =\n(w1, ...,wK) T \u2208 RK\u00d7d. The empirical loss is given by\nL(W ) = 1\n2n\nn\u2211\ni=1\nK\u2211\nk=1\n(yki \u2212wTk xi)2.\n= 1\n2n\nK\u2211\nk=1\nwTk X TXwk\u2212\n1\nn\nK\u2211\nk=1\nwTk X T yk +\n1\n2n \u2016Y \u20162F\n= 1\n2n\nK\u2211\nk=1\nTr(W TWXTX)\u2212 1 n Tr(WXTY ) + 1 2n \u2016Y \u20162F\nAdding the regularization penalty and minimizing in C we obtain\nG(M) = min W\n1\n2n\nK\u2211\nk=1\nTr ( W TW (XTX + \u03bbWnP ) ) \u2212 1 n Tr(WXTY ) + 1 2n \u2016Y \u201622\n= 1\n2n \u2016Y \u201622 \u2212\n1\n2n\n( Y TX(XTX + \u03bbWnP ) \u22121XTY )\n= 1 2n Tr ( Y Y T (In \u2212X(XTX + \u03bbWnP )\u22121XT )\n= 1\n2n Tr\n( Y Y T (In + 1\n\u03bbWn XP\u22121XT )\u22121\n)\nAs detailed before, the inverse of P can be found analytically by observing that it is composed of orthogonal projectors on orthogonal subspaces spanning the entire space. Hence we have P\u22121 = Id \u2212M + 11\u2212\u03b1(M \u2212 \u0393d) + 1 1\u2212\u03b2\u0393d. We now get\nG(M) = 1\n2n Tr\n( Y Y T (Id + 1\nn\u03bbB X(M \u2212 \u0393d)XT +\n1\nn\u03bbM X\u0393dX\nT )\n)\nNote that we still have \u2212\u2207G(M) 0. For a fixed M , W is given analitically by\nWM = Y TX ( XTX + \u03bbWn(Id \u2212 \u03b1(M \u2212 \u0393d)\u2212 \u03b2\u0393d) )\u22121 .\n6.4. Learning multiple predictors. In the setting of learning multiple predictors, we denote by Cq the set of points whose best linear predictor is wq, having therefore C1, ..., CQ a partition of [[1, n]]. We let as above sq be the cardinal of the set Cq, Xq \u2208 Rsq\u00d7d is the matrix whose columns are the points belonging to the cluster q, and yq is the column vector of labels corresponding to cluster q. We use a squared loss l(f(x), y) = 12(y \u2212 f(x))2.\nFor C1, ..., CQ fixed (or equivalently Z fixed), we can computeG using the Sherman-Woodbury-Morrison formula as\nG(M) = min w1,...,wQ\n1\n2n\nQ\u2211\nq=1\n\u2211\ni\u2208Cq\n(yi \u2212wTq xi)2 + \u00b5\n2\nQ\u2211\nq=1\nsq\u2016wq \u201622\n=\nQ\u2211\nq=1\n1\n2n yTq\n( 1\nsq\u00b5n XqX\nT q + In )\u22121 yq .\nWe define E \u2208 Rn\u00d7n the permutation matrix permuting order of points such that\nE y =   yC1\n... yCQ\n  EX =   X1\n... XQ\n  .\nWe denote for q \u2208 [[1, Q]], Rq = \u2211\u2211q\u22121 p=1 sp\u2264i\u2264 \u2211q p=1 sp eie T i ortohgonal projectors on the ordered sets of points belonging to cluster q such that   X1X T 1 0 0 0 . . . 0\n0 0 XQX T Q\n  = diag(EXXTET ) = Q\u2211\nq=1\nRqEXX TETRq.\nThus we get\nG(M) = 1\n2n yT ET\n  Q\u2211\nq=1\n1\nsq\u00b5n RqEXX\nTETRq + In\n  \u22121\nE y\n= 1\n2n yT\n  Q\u2211\nq=1\n1\nsq\u00b5n ETRqEXX TETRqE + In\n  \u22121\ny .\nThen we notice that ETRqE = diag(Zq) i.e. the projector on the set of points belonging to cluster q, and that \u2211Q q=1 1 sq diag(ZQ)XX T diag(Zq) = M \u25e6XXT , where \u25e6 denotes the Hadamard product. Hence we finally get\nG(M) = 1 2n yT ( 1 \u00b5n XXT \u25e6M + In )\u22121 y .\nIts gradient is given by\n2n\u2207G(M) = \u2212 1 \u00b5n\nXXT \u25e6 ( (In + 1\n\u00b5n XXT \u25e6M)\u22121 y yT (In +\n1\n\u00b5n XXT \u25e6M)\u22121\n) ,\nfor which we have \u2212\u2207G(M) 0. For a fixed Z, denoting by Xq = ZTq X the set of points belonging to cluster q, the linear predictors wq for each cluster of points are given by\nwq = (n\u00b5sqId +X T q Xq) \u22121XTq yq .\nAcknowledgements. AA is at CNRS, at the De\u0301partement d\u2019Informatique at E\u0301cole Normale Supe\u0301rieure, 23 avenue d\u2019Italie, 75013 Paris, France. INRIA, Sierra project-team, PSL Research University. The authors would like to acknowledge support from a starting grant from the European Research Council (ERC project SIPA), an AMX fellowship, the MSR-Inria Joint Centre, as well as support from the chaire E\u0301conomie des nouvelles donne\u0301es, the data science joint research initiative with the fonds AXA pour la recherche and a gift from Socie\u0301te\u0301 Ge\u0301ne\u0301rale Cross Asset Quantitative Research."}, {"heading": "INRIA - SIERRA PROJECT TEAM & D.I., UMR 8548,", "text": ""}, {"heading": "E\u0301COLE NORMALE SUPE\u0301RIEURE, PARIS, FRANCE.", "text": "E-mail address: vincent.roulet@inria.fr"}, {"heading": "C.M.A.P., E\u0301COLE POLYTECHNIQUE, UMR CNRS 7641", "text": "E-mail address: fajwel.fogel@cmap.polytechnique.fr\nCNRS & D.I., UMR 8548, E\u0301COLE NORMALE SUPE\u0301RIEURE, PARIS, FRANCE. E-mail address: aspremon@ens.fr"}, {"heading": "INRIA - SIERRA PROJECT TEAM & D.I., UMR 8548,", "text": ""}, {"heading": "E\u0301COLE NORMALE SUPE\u0301RIEURE, PARIS, FRANCE.", "text": "E-mail address: francis.bach@inria.fr"}], "references": [{"title": "Convex multi-task feature learning", "author": ["Andreas Argyriou", "Theodoros Evgeniou", "Massimiliano Pontil"], "venue": "Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Clustered multi-task learning: A convex formulation", "author": ["Laurent Jacob", "Jean philippe Vert", "Francis R. Bach"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Convex learning of multiple tasks and their structure", "author": ["Carlo Ciliberto", "Tomaso Poggio", "Lorenzo Rosasco"], "venue": "arXiv preprint arXiv:1504.03101,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Simultaneous regression shrinkage, variable selection, and supervised clustering of predictors with oscar", "author": ["Howard D. Bondell", "Brian J. Reich"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "A divisive information theoretic feature clustering algorithm for text classification", "author": ["Inderjit S. Dhillon", "Subramanyam Mallela", "Rahul Kumar"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "A fuzzy self-constructing feature clustering algorithm for text classification", "author": ["Jung-Yi Jiang", "Ren-Jia Liou", "Shie-Jue Lee"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Efficiently enforcing diversity in multi-output structured prediction", "author": ["Abner Guzman-Rivera", "Pushmeet Kohli", "Dhruv Batra", "Rob Rutenbar"], "venue": "In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Large-scale learning for image classification", "author": ["Zaid Harchaoui"], "venue": "http://www.di.ens.fr/ willow/events/cvml2013/materials/slides/thursday/harch_cvml13.pdf,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "k-means++: The advantages of careful seeding", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "A note on cluster analysis and dynamic programming", "author": ["Richard Bellman"], "venue": "Mathematical Biosciences,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1973}, {"title": "Ckmeans. 1d. dp: optimal k-means clustering in one dimension by dynamic programming", "author": ["Haizhou Wang", "Mingzhou Song"], "venue": "The R Journal,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "An algorithm for quadratic programming", "author": ["Marguerite Frank", "Philip Wolfe"], "venue": "Naval research logistics quarterly,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1956}, {"title": "Revisiting frank-wolfe: Projection-free sparse convex optimization", "author": ["Martin Jaggi"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Bo Pang", "Lillian Lee"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Iterative hard thresholding for compressed sensing", "author": ["Thomas Blumensath", "Mike E. Davies"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "This problem has been well studied in the multi-task learning literature [1, 2, 3].", "startOffset": 73, "endOffset": 82}, {"referenceID": 1, "context": "This problem has been well studied in the multi-task learning literature [1, 2, 3].", "startOffset": 73, "endOffset": 82}, {"referenceID": 2, "context": "This problem has been well studied in the multi-task learning literature [1, 2, 3].", "startOffset": 73, "endOffset": 82}, {"referenceID": 3, "context": "Similarly, when there exists some groups of highly correlated features, reducing dimensionality by assigning the same weights to some groups of features can be beneficial both in terms of prediction and interpretation [4].", "startOffset": 218, "endOffset": 221}, {"referenceID": 4, "context": "This often occurs in text classification, where it is natural to group together words having the same meaning for a given task [5, 6].", "startOffset": 127, "endOffset": 133}, {"referenceID": 5, "context": "This often occurs in text classification, where it is natural to group together words having the same meaning for a given task [5, 6].", "startOffset": 127, "endOffset": 133}, {"referenceID": 6, "context": "Finally, in some settings, it can be valuable to cluster sample points, with each cluster having its own distinct prediction function [7].", "startOffset": 134, "endOffset": 137}, {"referenceID": 7, "context": "Here, we present a unified and flexible framework for supervised clustering over the \u201cdata cube\u201d of either tasks, features or sample points (a representation introduced by [8]).", "startOffset": 172, "endOffset": 175}, {"referenceID": 8, "context": "While the original optimization problem is non-convex, we show that the core nonconvexity is concentrated in a subproblem similar to k-means, which we solve using classical approximation techniques [9].", "startOffset": 198, "endOffset": 201}, {"referenceID": 9, "context": "In the particular case of feature clustering for regression, the k-means steps are performed in dimension one, and can therefore be solved exactly by dynamic programming [10, 11].", "startOffset": 170, "endOffset": 178}, {"referenceID": 10, "context": "In the particular case of feature clustering for regression, the k-means steps are performed in dimension one, and can therefore be solved exactly by dynamic programming [10, 11].", "startOffset": 170, "endOffset": 178}, {"referenceID": 11, "context": "Our formulation is then an explicit convex relaxation which can be solved efficiently using the conditional gradient algorithm [12, 13].", "startOffset": 127, "endOffset": 135}, {"referenceID": 12, "context": "Our formulation is then an explicit convex relaxation which can be solved efficiently using the conditional gradient algorithm [12, 13].", "startOffset": 127, "endOffset": 135}, {"referenceID": 1, "context": "As in [2], the clustering penalty \u03a9SC(W,V ) can be decomposed into three separable terms as follows (see Figure 1 for an illustration).", "startOffset": 6, "endOffset": 9}, {"referenceID": 3, "context": "We present the linear regression case [4], which can be extended to logistic regression and classification.", "startOffset": 38, "endOffset": 41}, {"referenceID": 0, "context": "Given a set of K supervised tasks like regression or binary classification, multitask learning aims at jointly solving these tasks, hoping that each task can benefit from the information given by other tasks [1, 2, 3].", "startOffset": 208, "endOffset": 217}, {"referenceID": 1, "context": "Given a set of K supervised tasks like regression or binary classification, multitask learning aims at jointly solving these tasks, hoping that each task can benefit from the information given by other tasks [1, 2, 3].", "startOffset": 208, "endOffset": 217}, {"referenceID": 2, "context": "Given a set of K supervised tasks like regression or binary classification, multitask learning aims at jointly solving these tasks, hoping that each task can benefit from the information given by other tasks [1, 2, 3].", "startOffset": 208, "endOffset": 217}, {"referenceID": 8, "context": "Although it is a non-convex problem, k-means++ gives general approximation bounds on its solution [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 11, "context": "conditional gradient, [12, 13]).", "startOffset": 22, "endOffset": 30}, {"referenceID": 12, "context": "conditional gradient, [12, 13]).", "startOffset": 22, "endOffset": 30}, {"referenceID": 9, "context": "When clustering features, we optimize over the coefficients associated with each feature and the k-means step can be performed exactly using dynamic programming [10, 11].", "startOffset": 161, "endOffset": 169}, {"referenceID": 10, "context": "When clustering features, we optimize over the coefficients associated with each feature and the k-means step can be performed exactly using dynamic programming [10, 11].", "startOffset": 161, "endOffset": 169}, {"referenceID": 9, "context": "In fact, in this particular case, the k-means subproblem is one-dimensional and can be solved exactly using dynamic programming [10, 11].", "startOffset": 128, "endOffset": 136}, {"referenceID": 10, "context": "In fact, in this particular case, the k-means subproblem is one-dimensional and can be solved exactly using dynamic programming [10, 11].", "startOffset": 128, "endOffset": 136}, {"referenceID": 3, "context": "We compare in Table 3 the proposed algorithms to OSCAR [4], Ridge, Lasso and Ridge followed by k-means on the weights (using associated centroids as predictors).", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "In Table 4, we compare our approach to other classical regularizations such as the Frobenius norm and the trace norm, as implemented in [3], using either a ridge or a logistic loss.", "startOffset": 136, "endOffset": 139}, {"referenceID": 13, "context": "We use the publicly available dataset introduced in [14] which contains movie reviews paired with star ratings.", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "Similarly as in compressed sensing, optimization is made on a union of subspaces, hence in analogy with RIP conditions for iterative hard thresholding [15], it would be interesting to see under which assumptions our algorithms can recover the optimal solution to the supervised clustering problem.", "startOffset": 151, "endOffset": 155}], "year": 2015, "abstractText": "We study a supervised clustering problem seeking to cluster either features, tasks or sample points using losses extracted from supervised learning problems. We formulate a unified optimization problem handling these three settings and derive algorithms whose core iteration complexity is concentrated in a k-means clustering step, which can be approximated efficiently. We test our methods on both artificial and realistic data sets extracted from movie reviews and 20NewsGroup.", "creator": "LaTeX with hyperref package"}}}