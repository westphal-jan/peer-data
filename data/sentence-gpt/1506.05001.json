{"id": "1506.05001", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2015", "title": "Using Hankel Matrices for Dynamics-based Facial Emotion Recognition and Pain Detection", "abstract": "This paper proposes a new approach to model the temporal dynamics of a sequence of facial expressions. To this purpose, a sequence of Face Image Descriptors (FID) is regarded as the output of a Linear Time Invariant (LTI) system. The temporal dynamics of such sequence of descriptors are represented by means of a Hankel matrix. The paper presents different strategies to compute dynamics-based representation of a sequence of FID, and reports classification accuracy values of the proposed representations within different standard classification frameworks. The representations have been validated in two very challenging application domains: emotion recognition and pain detection. Experiments on two publicly available benchmarks and comparison with state-of-the-art approaches demonstrate that the dynamics-based FID representation attains competitive performance when off-the-shelf classification tools are adopted. We report a linear classification algorithm for both Face image and facial recognition using multiple RTE, and report classification accuracy values of the simulated facial expressions. We report a linear classification algorithm for the facial expressions.\n\n\n\n\n\nThe first step in the initial analysis was to examine the facial recognition properties of facial expressions in different ways. The facial expressions in the image and facial recognition data obtained were analyzed using the face-recognition API. For example, facial expressions in the facial expressions in the facial expressions in the face-recognition API were evaluated using a facial analysis technique. These facial expressions were constructed as such, and then evaluated with an RTE that incorporates all the facial expression analysis properties obtained from facial expression. The facial expressions in the face-recognition API are evaluated separately from facial expressions in the face-recognition API.\nThe first analysis using the facial expression analysis technique was to identify the facial expression patterns for facial expressions in different ways. Face expressions in the facial expressions in the face-recognition API were analyzed using a facial analysis technique. These facial expressions were constructed as such, and then evaluated with an RTE that incorporates all the facial expression analysis properties obtained from facial expression. The facial expressions in the face-recognition API were evaluated separately from facial expressions in the face-recognition API. This approach provides better resolution of facial expressions of facial expressions in the face-recognition API by analyzing facial expressions in the facial expressions in the face-recognition API. This approach provides better resolution of facial expressions in the face-recognition API by analyzing facial expressions in the face-recognition API by analyzing facial expressions in the face-recognition API by analyzing facial expressions in the face-recognition API", "histories": [["v1", "Tue, 16 Jun 2015 15:22:46 GMT  (135kb)", "http://arxiv.org/abs/1506.05001v1", "in IEEE Proceedings of Workshop on Analysis and Modeling of Face and Gestures (CVPRW 2015)"]], "COMMENTS": "in IEEE Proceedings of Workshop on Analysis and Modeling of Face and Gestures (CVPRW 2015)", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.RO", "authors": ["liliana lo presti", "marco la cascia"], "accepted": false, "id": "1506.05001"}, "pdf": {"name": "1506.05001.pdf", "metadata": {"source": "CRF", "title": "Using Hankel Matrices for Dynamics-based Facial Emotion Recognition and Pain Detection \ufffdPre-Print of Proceedings of AMFG CVPRW 2015)", "authors": ["Liliana Lo Presti", "Marco La Cascia"], "emails": ["lilianalopresti@unipait"], "sections": [{"heading": null, "text": "This paper proposes a new approach to model the temporal dynamics of a sequence of facial expressions. To this purpose, a sequence of Face Image Descriptors (FID) is regarded as the output of a Linear Time Invariant (LTI) system. The temporal dynamics of such sequence of descriptors are represented by means of a Hankel matrix.\nThe paper presents different strategies to compute dynamics-based representation of a sequence of FID, and reports classification accuracy values of the proposed representations within different standard classification frameworks. The representations have been validated in two very challenging application domains: emotion recognition and pain detection. Experiments on two publicly available benchmarks and comparison with state-of-the-art approaches demonstrate that the dynamics-based FID representation attains competitive performance when off-theshelf classification tools are adopted."}, {"heading": "1. Introduction", "text": "Facial expression analysis and emotion recognition are of interest in several domains such as human-computer interaction and social behavior understanding. Especially in socially assistive robotics [23] and computational behavioral science [24], [19], recognition of face expressions and emotions may help either to improve interactions with a robot, or to study people\u2019s social engagement in collaborative tasks [17], [35].\nMoreover, face expression analysis could be useful in automatic pain monitoring, which in turn may help to ensure proper treatment to the patient [20]. Recent works, such as [21],[12], [27], [2],[25], [26], [9], [10] have focused on pain/no-pain detection and pain intensity estimation. These problems require the analysis of spontaneous facial expressions in the wild, namely under strong variations of head\npose and face expressions. Pain detection suffers also of the difficulty to annotate the data in an objective way. Whilst patient\u2019s self-report is inexpensive and does not require for special skills, it has the drawback to be subjective, and it lacks of specific timing information [20]. Therefore, not only there are strong inter-patient variations in face expressions, but there are variations also in the pain self-reports.\nTypically, approaches for emotion recognition tend to extract a representation of the face appearance, and adopt some classification framework. Some approaches [29] use a frame-based representation; others [6], [18], [30] use also temporal information. The latter works are motivated by the fact that face emotions are not instantaneous and the temporal evolution of face image descriptors (FIDs) can help to discriminate among emotions. Holistic representation of a sequence of FIDs, such as [29], lacks of temporal information, which instead proved to be useful [3].\nIn this paper, we propose to use the temporal dynamics of a sequence of FIDs to recognize among different emotions. Dynamics-based methods for emotion recognition have been proposed in [6] where a descriptor based on the movement of facial landmarks along the image sequence, and spatio-temporal appearance features are adopted. While [6] attempts to embed information about the dynamics at a feature representation level, works such as [18], [30] attempt to account for the temporal structure of the sequences of FIDs in the emotion model.\nIn contrast to these works, we propose to model a sequence of FIDs as the output of a Linear Time Invariant (LTI) system in order to perform dynamics-based emotion recognition. System identification [13], [8] or compressive sensing-based techniques [28] could be used to compare different emotion instances. However, previous works [14], [16] have shown that it may be possible to avoid the burden of performing system identification by representing the output of a LTI system through the corresponding Hankel matrix. Therefore, in this paper we explore the use\nof Hankel matrices in the domain of face analysis. The use of Hankel matrices, jointly with the dissimilarity score in [14], presents advantages in terms of space and time complexity.\nIn this paper, we propose different strategies to compute a dynamics-based representation that employs Hankel matrices. Our dynamics-based representation permits to easily compare sequences of different lengths. Comparison of sequences of different length is one of the major challenge in emotion classification. We present experiments in two different kinds of applications: emotion recognition and pain detection, and we conduct an extensive validation on two publicly available benchmarks. The first benchmark is the extended Cohn-Kanade dataset [19], which allows us to study the validity of this kind of feature representation for emotion recognition; the second dataset is the very challenging PAINFUL dataset [20], which allows us to study the proposed feature representation for pain localization in the wild. Our experiments show that, with standard and widely used classifiers such as nearest neighbor, linear SVM and HMM, our dynamics-based emotion representation allows us to consistently achieve state-of-the-art performance in emotion recognition in comparison to methods that use more complex machinery and costly training procedure.\nThe plan of the work is as follows. In Section 2, we present works that are related to our feature representation. In Section 3 we describe how we use a Hankel matrix to represent face emotions. In Section 4 we provide details of the adopted classification frameworks and in Section 5, we present extensive validation of the dynamics-based representation. Finally, in Section 6, we present conclusions and future directions."}, {"heading": "2. Related Work", "text": "There is an extensive literature on face recognition [37], [15] and on facial expression analysis [35], [7]. Here we focus on works that try to embed the temporal structure of the sequence of facial expression either in the feature representation step or in the emotion-model.\nIn particular, [11] uses a Constrained Local Model (CLM) to obtain facial landmarks. Then it extracts patches around these markers. A sparse representation of the patches is obtained by applying non-negative matrix factorization. Classification is performed by least-square SVM. In [6], a descriptor based on the movement of facial landmark points over time, jointly with spatio-temporal appearance features is extracted for each face image sequence. The method attempts to measure horizontal and vertical movements of tracked landmarks of different face parts such as eyebrows, eyelids, cheeks, and lip corners. To account for temporal changes in the face appearance, Complete Local Binary Patterns from Three Orthogonal Planes (LBP-\nTOP) [36] are used. Classification is performed by SVM. In [22], restricted Boltzmann machine with local interactions (LRBM) is used to capture spatio-temporal patterns in the data. RBM is used as a generative model for data representation, and data need to be pre-aligned.\nSince a sequence of FIDs is a time series, and may be affected by temporal warping, in [18] time-series kernel methods are used for emotional expression estimation using landmark data only. The work shows that emotion recognition may be done by adopting either the Dynamic Time Warping (DTW) kernel or the Global Alignment (GA) kernel [4, 5]. Our approach does not require any alignment of the data, and enables the comparison of sequences of different temporal duration.\nTo capture temporal information about the sequence of FIDs, Bayesian networks can be adopted. Wang et al. [33] propose to use Interval Temporal Bayesian Network (ITBN) to capture the spatial and temporal relations among primitive facial events. First, primitive facial events are identified, then ITBN is applied to model the interactions of primitives for expression recognition. In [30], a Bayesian approach is used to model dynamic facial expression temporal transitions. A face appearance representation is computed in terms of Local Binary Patterns (LBP), and an expression manifold is derived for multiple subjects. A Bayesian temporal model (similar to HMM with a non parametric observation model) of the manifold is used to represent facial expression dynamics.\nIn this paper, we model a sequence of FIDs by means of a dynamical system of unknown parameters. The parameters of the dynamical system are embedded in our Hankel matrix-based representation. Such representation aims at capturing the correlation among different face parts over time. Hankel matrices have already been adopted for action recognition in [14], which adopts a Hankel matrix-based bag-of-words approach, and in [16], which models an action as a sequence of Hankel matrices and uses a set of HMM trained in a discriminative way to model the switching between LTI systems. In contrast to these papers, we use Hankel matrices to describe the temporal dynamics of sequences of FIDs. We compare different strategies to compute a dynamics-based representation that employs Hankel matrices. We also test the effectiveness of our representation for emotion recognition within several standard classification frameworks."}, {"heading": "3. Dynamics-based Emotion Representation", "text": "In this paper, a sequence of face images is processed to extract a feature representation on a frame-by-frame basis. This process yields to a time series of feature vectors [yo\ufffd . . . \ufffd y\u03c4 ], where yt is the feature representation associated with the t-th face image. Such temporal sequence can be regarded as the output of an LTI system of unknown pa-\nrameters [31]."}, {"heading": "3.1. Representation of Temporal Dynamics", "text": "In a linear time invariant system, two linear equations\nregulate the behavior of the system as follows:\nxk\ufffd1 = A \u00b7 xk + wk;\nyk = C \u00b7 xk. (1)\nThe first equation is known as the state equation and involves the variable xk \u2208 R u, which represents the udimensional internal state of the LTI system. The second equation is known as the measurement equation and provides a link between the state of the system xk and the v-dimensional observable measurement yk. In such equations the matrices A and C are constant over time, and wk \u223c N\ufffd0\ufffd Q) is uncorrelated zero mean Gaussian measurement noise.\nIt is well known [32] that, given a sequence of output measurements [yo\ufffd . . . \ufffd y\u03c4 ] from Eq.1, its associated truncated block-Hankel matrix is\n\ufffdH =\n\ufffd\n   y0\ufffd y1\ufffd y2\ufffd . . . \ufffd ym y1\ufffd y2\ufffd y3\ufffd . . . \ufffd ym\ufffd1 . . . . . . . . . . . . . . .\nyn\ufffd yn\ufffd1\ufffd yn\ufffd2\ufffd . . . \ufffd y\u03c4\n\n   \ufffd (2)\nwhere n is the maximal order of the system, \u03c4 is the temporal length of the sequence, and it holds that \u03c4 = n+m\u2212 1. The Hankel matrix embeds the observability matrix \u0393 of the system, since \ufffdH = \u0393 \u00b7X , where X = [x0\ufffd x1\ufffd \u00b7 \u00b7 \u00b7 \ufffd x\u03c4 ] is a matrix formed by the sequence of internal states of the LTI system.\nAs previously done in [14], [16], we normalize the Han-\nkel matrix \ufffdH as follows:\nH = \ufffdH \ufffd\n|| \ufffdH \u00b7 \ufffdHT ||\ufffd\n. (3)\nand compare two Hankel matrices Hp and Hq by:\nd\ufffdHp\ufffd Hq) = 2\u2212 ||Hp \u00b7H T p +Hq \u00b7H T q ||\ufffd . (4)\nSuch score, introduced in [14], does not define a distance. Instead, it roughly approximates the subspace angle between the spaces spanned by the columns of the Hankel matrices."}, {"heading": "3.2. Dynamics-based Expression Representation", "text": "In this paper we propose to use a Hankel matrix to represent the dynamics of a sequence of face images whose feature representation yields to a time series of vectors Y = [yo\ufffd . . . \ufffd y\u03c4 ]. We compare three different dynamics-based emotion representations, that we describe in the following.\n\u2022 Single Hankel matrix representation: this representation uses the whole time series Y to build the Hankel\nmatrix. We note that, even if the sequences may have different length, the matrix H \u00b7HT used in Eq. 4 is a squared symmetric matrix and Hankel matrices of sequences of different lengths are easily comparable.\n\u2022 Sliding window-based representation: while the former representation assumes segmentation of the frame\nsequence into emotions, this representation could overcome this requirement by representing Y through a sequence of overlapping temporal window (similar to [16]). However, it may also limit the applicability of some classification frameworks (such as linear SVM) because sequence representations may have different lengths.\n\u2022 LTI Codebook-based representation: in this representation, a bag-of-LTI-systems approach is used.\nFirst, the sequence Y is represented by means of a single Hankel matrix H . From a training set, a codebook of LTI systems C = \ufffdCi}, with Ci representing a Hankel matrix, is computed by using K-medoids on the dissimilarity score in Eq. 4. The representation of a time series Y is formed by concatenating the dissimilarity score of the Hankel matrix H and each of the elements Ci in the codebook. L2-normalization is applied on the extracted descriptor. Sequences of FIDs are represented by descriptors of the same length."}, {"heading": "4. Adopted Classification Framework", "text": "We have tested our dynamics-based emotion representations within several classification frameworks in order to test their robustness.\n\u2022 Nearest Neighbor Classifier \ufffdNN): given the dynamics-based representation of a test sequence, the\npredicted class is determined by the class label of the nearest sequence in the training set;\n\u2022 Codebook-based Support Vector Machine \ufffdCSVM): Linear one-vs-all SVM models1 are\ntrained on the LTI codebook-based representation, and the estimated margin is used to classify the test sequence.\n\u2022 Dynamic Time Warping and NN \ufffdDTW+NN): this method is applied to the sliding window-based repre-\nsentation. DTW2 is used to align sequences of Hankel matrices. The Hankel matrices of each temporal window are compared through the score in Eq. 4. After aligning a test sequence with each sample in the\n1We have used the Matlab implementation for linear SVM. 2We have used a slightly modified implementation of the code available\nat http://www.ee.columbia.edu/ln/rosa/matlab/dtw/\ntraining set, NN classifier is used to predict the test sequence class.\n\u2022 Nearest Neighbor and Majority Vote \ufffdNN+V): this method assumes that the sliding window-based Hankel\nmatrix representation is used. Inspired by [34], we use the NN classifier to predict the class label of each temporal window. Majority vote is used to predict the class of the test sequence.\n\u2022 Hidden Markov Model: this method assumes the sliding window-based Hankel matrix representation is\nused. Similarly to [16], a HMM is used to model the transition from a LTI system to the other. In contrast to [16], we use standard HMM3 with independently estimated state spaces. The number of states has been empirically set to 10. States are initialized by K-medoids and are not updated during the training procedure. We train an HMM for each class and use maximum-likelihood to classify a test sequence."}, {"heading": "5. Experimental Results", "text": "This paper focuses on the analysis of face expressions in two challenging application domains: emotion recognition and pain detection. In the following, first we detail the frame-based representation adopted to obtain the measurements [y0\ufffd \u00b7 \u00b7 \u00b7 y\u03c4 ], later we provide a brief description of each application domain and present our results."}, {"heading": "5.1. Feature Extraction", "text": "Our formulation is general and can be adopted with several kinds of facial features. In this paper, to demonstrate the whole framework, we consider shape features provided by an active appearance model [19], [20]. Therefore, face expressions are represented as trajectories of 2D facial landmarks as shown in Fig. 1. To build the Hankel matrices, we use the following frame-based feature representations:\n\u2022 concatenated 2D facial landmark coordinates (L);\n\u2022 pairwise landmark distances (D);\n\u2022 concatenation of pairwise landmark distances and landmark coordinates (L+D).\nFor each of these representations, principal component analysis (PCA) has been applied for noise and dimensionality reduction. We have selected a number of projections covering 99\ufffd of the total variance. The retained PCA coefficients are then used to build the dynamics-based representation as explained in Section 3.\n3We used the HMM toolbox available at http://www.cs.ubc.ca/\ufffdmurphyk/Software/HMM/hmm.html. We modified the code in such a way that the observation model is an exponential distribution and each state is a LTI system represented by an exemplar Hankel matrix.\nThe adopted features are meant to represent the behavior of different parts of the face. The distance-based expression representation captures also the reciprocal relations among face parts (for example the joint movement of eyebrows and lips), and it is independent on the head movements on the image plane. The concatenation of distances and landmark coordinates permits to represent reciprocal relations of face parts given the face shape."}, {"heading": "5.2. Emotion Recognition", "text": "Emotion recognition deals with the problem of inferring the emotion (such as fear, anger, surprise, etc.) given a sequence of face images. The main difficulty in this domain arises from the strong inter-subject variations, especially in some kind of emotions (such as sadness). Other challenges are connected with the difficulty to extract reliable feature representations due to illumination changes, biometric differences, head pose changes. Moreover, the lack of depth information makes emotion recognition more difficult due to ambiguities in the facial shape representation. To demonstrate the idea behind this paper, we restrict the attention to segmented emotion recognition in frontal view as done also in previous works such as [18], [1], [22], [33]."}, {"heading": "5.2.1 Data and Validation Protocols", "text": "We have performed experiments for emotion recognition on the widely adopted Extended Cohn-Kanade dataset (CK+) [19]. This dataset provides facial expressions of 210 adults. Participants were instructed to perform several facial display representing either single or combinations of action units. Based on the coded action units and by means of a validation procedure of the assigned label, the segmented recording of the participants\u2019 emotions were classified into 7 categories: angry, contempt, disgust, fear, happy, sadness, surprise. In total there are 327 sequences of the 7 annotated emotions, performed by 118 different individuals. The number of frames of these sequences ranges in [6\ufffd 71] with an average value of about 18 \u00b1 8.6. The dataset provides landmark tracking results obtained by an active appearance model, which we use in our experiments. We adopted the validation protocol suggested in [19], which is leave-onesubject-out cross-validation."}, {"heading": "5.2.2 Emotion Recognition \u2013 Results", "text": "We have performed an extensive validation of the dynamicsbased emotion representations whose results are reported in\nTable 1. The table reports the per-class classification accuracy values for each emotion class and the average accuracy value.\nThe table is divided in 4 parts. The first part compares dynamics-based representations when the Hankel matrix is\ncomputed over the whole sequence. We compare the single Hankel matrix representation with the nearest neighbor classifier against the LTI system codebook-based representation and linear one-vs-all SVMs.\nThe second part of the table compares the sliding window-based representation within three classification frameworks: dynamic time warping and NN classifier, NN classifier and majority vote, hidden Markov models.\nThe third part of Table 1 reports the results obtained directly on the raw features (without computing any Han-\nkel matrix) in order to highlight the advantage of using the dynamics-based representation. As the sequences have different lengths, we cannot apply the NN classifier directly on the raw features, but we are forced to align the sequences via DTW. We are not presenting results on the raw data via SVM because this experiment would be similar to the baseline method reported in [19] (in the lower part of the table). The fourth part of the table reports accuracy values of works at the state-of-the-art on equal terms of input data, which means that all the works we compare with use only\n2D facial landmarks.\nDue to the randomness in the codebook generation of the CSVM method and in the state initialization procedure in HMM, the experiments for CSVM and HMM have been repeated 10 times, and average accuracy values are reported.\nOverall, the experimental results show that the dynamics-based emotion representation achieves state-ofthe-art performance almost within all the tested classification frameworks. The highest accuracy values are obtained when the whole sequence is used. Among the sliding window-based approaches, only when adopting NN and majority vote the performance are comparable or higher than the one reported in [22]. These experiments suggest that probably emotions can be represented as the output of just one LTI system and there may be no dynamics switching as instead may happen in human actions [16]. The results also show how, in general, pairwise distances are more informative than 2D landmark trajectories. The concatenation of distances and landmarks (L+D) provides only a small improvement of the performance, with a general increase of the dimensionality of the representation. Finally, we note that the adoption of Hankel matrices permits to achieve an increase of more than 63\ufffd (on average) of the accuracy value obtained classifying directly the raw facial features."}, {"heading": "5.3. Pain Detection", "text": "With respect to the former segmented emotion recognition task, pain detection is even more challenging due to the need of locating the pain event within the frame sequence. Pain may be a sporadic episode of varying duration, and painful facial expressions may vary greatly from subject-tosubject or be confused with other emotions.\nIn this paper we treat pain detection as a binary classification problem. We adopt a sliding window approach and classify each temporal window in order to detect the pain event. We empirically set the length of the temporal window to 10 frames."}, {"heading": "5.3.1 Data and Validation Protocols", "text": "To test our dynamics-based FID representation for pain detection, we use the Painful dataset [20]. This dataset contains videos of patients\u2019 faces while they were moving their painful shoulder. The goal is that of recognizing between pain and no-pain events. The videos were annotated on a frame-per-frame basis with the Prkachin and Solomon pain intensity (PSPI) score [20], which ranges in [0, 15] where 0 means no pain, while a value greater than 0 indicates a certain intensity of pain. However, our method works on temporal windows and the validation requires a temporal window-based annotation. To account for this, we consider the integral score IS, obtained by summing the PSPI score\nin a sliding window. We set the label of the temporal window to 0 if IS < \u03c8, and to 1 if IS \u2265 \u03c8. Here \u03c8 indicates a threshold value on the integral score. A low value of IS means that some frames in the temporal window have PSPI higher than 1; however, most of the frames in the window may score PSPI 0. To test the ability of our descriptor to represent pain events, we test our approach with different values of \u03c8 in leave-one-subject-out cross validation.\nFinally, we note that this dataset is challenging also because faces are not always in frontal view, and the landmark points are affected by strong head movements. Therefore, on this dataset it is of particular interest the comparison between landmark-based and pairwise distance-based dynamics representation."}, {"heading": "5.3.2 Pain Detection \u2013 Results", "text": "Table 2 reports the accuracy values of our sliding windowbased Hankel matrix representation. We test such representation on landmarks (L) and on pairwise distance (D) measurements within two frameworks: NN and CSVM.\nWhen adopting NN, the training set has been reduced by selecting only K medoids for each class, with K set to 300. When adopting CSVM, a codebook of 50 Hankel matrices is learned on the training set by K-medoids.\nWe report in columns the values of the confusion matrices for our binary classification experiment given the corresponding threshold value \u03c8. Positive indicates the pain event, while Negative indicates the no-pain event. Therefore TPR (true positive rate) and TNR (true negative rate) are the diagonal values of the confusion matrix. FNR (false negative rate) and FPR (false positive rate) are the extradiagonal values. We also present the average classification accuracy (the average of the diagonal values).\nTable 2 is divided into 4 parts. The first part reports values of the NN classifier when the Hankel matrix is computed directly on the landmark trajectories. The second part of the table reports values of the NN classifier when the Hankel matrix is computed on the pairwise landmark distances across time. By comparing the results, it is possible to observe that the pairwise distance-based representation achieves higher TPR for all the threshold values. The increase of the average accuracy for \u03c8 = 1 is of around 18\ufffd, which is much higher than what observed on the CK+ dataset. We believe that, on these data, head motion might affect the landmark-based representation. On the contrary, pairwise distances are more invariant to head motion.\nThe third and fourth parts of the table reports accuracy values respectively for the landmark-based and pairwise distance-based measurements when adopting the LTI Codebook-based SVM approach. Again, the distance-based representation proves to be more discriminative than the landmark-based one. However, overall the accuracy values\nobtained with the adopted simple implementation of SVM are lower than the ones obtained via NN."}, {"heading": "6. Conclusions and Future Work", "text": "In this paper we have proposed to adopt Hankel matrices to represent the dynamics of FIDs and recognize among different emotions. Whilst Hankel matrices have already been used in action recognition, at the best of our knowledge this paper is the first to adopt such kind of representation for face expression analysis. To study the performance of our dynamics-based emotion representations, we have performed extensive test within different standard classification frameworks on a widely used publicly available benchmark (CK+). Our experiments show that, on equal terms of classification framework, by using our dynamics-based emotion representations it is possible to achieve an increase of about 63\ufffd of the average accuracy values with respect of using directly the observed measurements. Overall, our approach achieves state-of-the-art performance by adopting standard classification frameworks.\nWe have also performed experiment on the Painful dataset to test if our representation can be used to detect pain events. The experiments show that the pairwise landmark distance-based dynamics representation is more invariant to head motion and, when using NN, it permits to obtain an increase of the average accuracy of about 18\ufffd with respect\nto the landmark-based representation.\nThe main limitation of our current approach is the need for reliable landmarks to represent face expressions. Despite the huge progress in this field, still facial landmark tracking is an open problem. On the other hand, our formulation is general and its not limited to 2D feature tracks. We therefore aim at extending our work considering appearance-based feature representation. Moreover, we have used a sliding window approach where windows have all the same temporal duration. We will explore the use of varying duration windows within a structure learning framework to automatically segment the emotion in a long sequence of face images."}, {"heading": "7. Acknowledgement", "text": "This work was partially supported by the Italian MIUR grant PON01 01687, SINTESYS - Security and INTElligence SYStem."}], "references": [{"title": "Person-independent facial expression detection using constrained local models", "author": ["S.W. Chew", "P. Lucey", "S. Lucey", "J. Saragih", "J.F. Cohn", "S. Sridharan"], "venue": "In Proc. of Conf. and Workshop on Automatic Face \ufffd Gesture Recognition (FG),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Improved facial expression recognition via uni-hyperplane classification", "author": ["S.W. Chew", "S. Lucey", "P. Lucey", "S. Sridharan", "J.F. Conn"], "venue": "In Proc. of Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Sparse temporal representations for facial expression recognition", "author": ["S.W. Chew", "R. Rana", "P. Lucey", "S. Lucey", "S. Sridharan"], "venue": "In Advances in Image and Video Technology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Fast global alignment kernels", "author": ["M. Cuturi"], "venue": "In Int. Conf. on Machine Learning (ICML),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "A kernel for time series based on global alignments", "author": ["M. Cuturi", "J. Vert", "O. Birkenes", "T. Matsui"], "venue": "In Proc. of Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Enabling dynamics in face analysis", "author": ["H. Dibeklio\u011flu"], "venue": "PhD thesis, University of Amsterdam,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Automatic facial expression analysis: a survey", "author": ["B. Fasel", "J. Luettin"], "venue": "Pattern recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Matrix rank minimization with applications", "author": ["M. Fazel"], "venue": "PhD thesis,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Automatic detection of pain intensity", "author": ["Z. Hammal", "J.F. Cohn"], "venue": "In Proceedings of the 14th ACM international conference on Multimodal interaction,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Pain monitoring: A dynamic and context-sensitive system", "author": ["Z. Hammal", "M. Kunz"], "venue": "Pattern Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Continuous AU intensity estimation using localized, sparse  facial feature space", "author": ["L.A. Jeni", "J.M. Girard", "J.F. Cohn", "F. De La Torre"], "venue": "In Proc. of Conf. on Automatic Face \ufffd Gesture Recognition (FG),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Continuous pain intensity estimation from facial expressions", "author": ["S. Kaltwang", "O. Rudovic", "M. Pantic"], "venue": "In Advances in Visual Computing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Controllability of dynamical systems. a survey", "author": ["J. Klamka"], "venue": "Bulletin of the Polish Academy of Sciences: Technical Sciences,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Cross-view activity recognition using Hankelets", "author": ["B. Li", "O.I. Camps", "M. Sznaier"], "venue": "In Proc. of Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "An on-line learning method for face association in personal photo collection", "author": ["L. Lo Presti", "M. La Cascia"], "venue": "Image and Vision Computing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Gesture modeling by Hanklet-based hidden Markov model", "author": ["L. Lo Presti", "M. La Cascia", "S. Sclaroff", "O. Camps"], "venue": "In Computer Vision \u2013 ACCV 2014,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Joint alignment and modeling of correlated behavior streams", "author": ["L. Lo Presti", "S. Sclaroff", "A. Rozga"], "venue": "In Int. Conf. on Computer Vision-Workshops,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Emotional expression classification using time-series kernels", "author": ["A. Lorincz", "L.A. Jeni", "Z. Szab\u00f3", "J.F. Cohn", "T. Kanade"], "venue": "In Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "The Extended Cohn-Kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression", "author": ["P. Lucey", "J.F. Cohn", "T. Kanade", "J. Saragih", "Z. Ambadar", "I. Matthews"], "venue": "In Proc. of Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Painful data: The UNBC-McMaster shoulder pain expression archive database", "author": ["P. Lucey", "J.F. Cohn", "K.M. Prkachin", "P.E. Solomon", "I. Matthews"], "venue": "In Proc. of Conf. and Workshop on Automatic Face \ufffd Gesture Recognition (FG),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Affective state level recognition in naturalistic facial and vocal expressions", "author": ["H. Meng", "N. Bianchi-Berthouze"], "venue": "IEEE Transactions on Cybernetics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "A generative restricted Boltzmann machine based method for high-dimensional motion data modeling", "author": ["S. Nie", "Z. Wang", "Q. Ji"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Integrating socially assistive robotics into mental healthcare interventions: Applications and recommendations for expanded use", "author": ["S.M. Rabbitt", "A.E. Kazdin", "B. Scassellati"], "venue": "Clinical psychology review,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Decoding children\u2019s social behavior", "author": ["J.M. Rehg"], "venue": "In Proc. of Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Exploiting unrelated tasks in multi-task learning", "author": ["B. Romera-Paredes", "A. Argyriou", "N. Berthouze", "M. Pontil"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Multilinear multitask learning", "author": ["B. Romera-Paredes", "H. Aung", "N. Bianchi-Berthouze", "M. Pontil"], "venue": "In Proceedings  of the 30th International Conference on Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Automatic pain intensity estimation with heteroscedastic conditional ordinal random fields", "author": ["O. Rudovic", "V. Pavlovic", "M. Pantic"], "venue": "In Advances in Visual Computing,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Compressive acquisition of dynamic scenes", "author": ["A.C. Sankaranarayanan", "P.K. Turaga", "R.G. Baraniuk", "R. Chellappa"], "venue": "In Proc. of European Conf. on Computer Vision (ECCV),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Robust facial expression recognition using local binary patterns", "author": ["C. Shan", "S. Gong", "P.W. McOwan"], "venue": "In Int. Conf. on Image Processing,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "Dynamic facial expression recognition using a Bayesian temporal manifold model", "author": ["C. Shan", "S. Gong", "P.W. McOwan"], "venue": "In BMVC,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2006}, {"title": "Nonlinear regulation: The piecewise linear approach", "author": ["E.D. Sontag"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1981}, {"title": "Subspace-based methods for the identification of linear time-invariant systems", "author": ["M. Viberg"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1995}, {"title": "Capturing complex spatiotemporal relations among facial muscles for facial expression recognition", "author": ["Z. Wang", "S. Wang", "Q. Ji"], "venue": "In Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "The moving pose: An efficient 3D kinematics descriptor for low-latency action recognition and detection", "author": ["M. Zanfir", "M. Leordeanu", "C. Sminchisescu"], "venue": "In Proc. of Int. Conf. on Computer Vision (ICCV),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions", "author": ["Z. Zeng", "M. Pantic", "G.I. Roisman", "T.S. Huang"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Dynamic texture recognition using local binary patterns with an application to facial expressions", "author": ["G. Zhao", "M. Pietikainen"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}, {"title": "Face recognition: A literature survey", "author": ["W. Zhao", "R. Chellappa", "P.J. Phillips", "A. Rosenfeld"], "venue": "Acm Computing Surveys (CSUR),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2003}], "referenceMentions": [{"referenceID": 22, "context": "Especially in socially assistive robotics [23] and computational behavioral science [24], [19], recognition of face expressions and emotions may help either to improve interactions with a robot, or to study people\u2019s social engagement in collaborative tasks [17], [35].", "startOffset": 42, "endOffset": 46}, {"referenceID": 23, "context": "Especially in socially assistive robotics [23] and computational behavioral science [24], [19], recognition of face expressions and emotions may help either to improve interactions with a robot, or to study people\u2019s social engagement in collaborative tasks [17], [35].", "startOffset": 84, "endOffset": 88}, {"referenceID": 18, "context": "Especially in socially assistive robotics [23] and computational behavioral science [24], [19], recognition of face expressions and emotions may help either to improve interactions with a robot, or to study people\u2019s social engagement in collaborative tasks [17], [35].", "startOffset": 90, "endOffset": 94}, {"referenceID": 16, "context": "Especially in socially assistive robotics [23] and computational behavioral science [24], [19], recognition of face expressions and emotions may help either to improve interactions with a robot, or to study people\u2019s social engagement in collaborative tasks [17], [35].", "startOffset": 257, "endOffset": 261}, {"referenceID": 34, "context": "Especially in socially assistive robotics [23] and computational behavioral science [24], [19], recognition of face expressions and emotions may help either to improve interactions with a robot, or to study people\u2019s social engagement in collaborative tasks [17], [35].", "startOffset": 263, "endOffset": 267}, {"referenceID": 19, "context": "Moreover, face expression analysis could be useful in automatic pain monitoring, which in turn may help to ensure proper treatment to the patient [20].", "startOffset": 146, "endOffset": 150}, {"referenceID": 20, "context": "Recent works, such as [21],[12], [27], [2],[25], [26], [9], [10] have focused on pain/no-pain detection and pain intensity estimation.", "startOffset": 22, "endOffset": 26}, {"referenceID": 11, "context": "Recent works, such as [21],[12], [27], [2],[25], [26], [9], [10] have focused on pain/no-pain detection and pain intensity estimation.", "startOffset": 27, "endOffset": 31}, {"referenceID": 26, "context": "Recent works, such as [21],[12], [27], [2],[25], [26], [9], [10] have focused on pain/no-pain detection and pain intensity estimation.", "startOffset": 33, "endOffset": 37}, {"referenceID": 1, "context": "Recent works, such as [21],[12], [27], [2],[25], [26], [9], [10] have focused on pain/no-pain detection and pain intensity estimation.", "startOffset": 39, "endOffset": 42}, {"referenceID": 24, "context": "Recent works, such as [21],[12], [27], [2],[25], [26], [9], [10] have focused on pain/no-pain detection and pain intensity estimation.", "startOffset": 43, "endOffset": 47}, {"referenceID": 25, "context": "Recent works, such as [21],[12], [27], [2],[25], [26], [9], [10] have focused on pain/no-pain detection and pain intensity estimation.", "startOffset": 49, "endOffset": 53}, {"referenceID": 8, "context": "Recent works, such as [21],[12], [27], [2],[25], [26], [9], [10] have focused on pain/no-pain detection and pain intensity estimation.", "startOffset": 55, "endOffset": 58}, {"referenceID": 9, "context": "Recent works, such as [21],[12], [27], [2],[25], [26], [9], [10] have focused on pain/no-pain detection and pain intensity estimation.", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "Whilst patient\u2019s self-report is inexpensive and does not require for special skills, it has the drawback to be subjective, and it lacks of specific timing information [20].", "startOffset": 167, "endOffset": 171}, {"referenceID": 28, "context": "Some approaches [29] use a frame-based representation; others [6], [18], [30] use also temporal information.", "startOffset": 16, "endOffset": 20}, {"referenceID": 5, "context": "Some approaches [29] use a frame-based representation; others [6], [18], [30] use also temporal information.", "startOffset": 62, "endOffset": 65}, {"referenceID": 17, "context": "Some approaches [29] use a frame-based representation; others [6], [18], [30] use also temporal information.", "startOffset": 67, "endOffset": 71}, {"referenceID": 29, "context": "Some approaches [29] use a frame-based representation; others [6], [18], [30] use also temporal information.", "startOffset": 73, "endOffset": 77}, {"referenceID": 28, "context": "Holistic representation of a sequence of FIDs, such as [29], lacks of temporal information, which instead proved to be useful [3].", "startOffset": 55, "endOffset": 59}, {"referenceID": 2, "context": "Holistic representation of a sequence of FIDs, such as [29], lacks of temporal information, which instead proved to be useful [3].", "startOffset": 126, "endOffset": 129}, {"referenceID": 5, "context": "Dynamics-based methods for emotion recognition have been proposed in [6] where a descriptor based on the movement of facial landmarks along the image sequence, and spatio-temporal appearance features are adopted.", "startOffset": 69, "endOffset": 72}, {"referenceID": 5, "context": "While [6] attempts to embed information about the dynamics at a feature representation level, works such as [18], [30] attempt to account for the temporal structure of the sequences of FIDs in the emotion model.", "startOffset": 6, "endOffset": 9}, {"referenceID": 17, "context": "While [6] attempts to embed information about the dynamics at a feature representation level, works such as [18], [30] attempt to account for the temporal structure of the sequences of FIDs in the emotion model.", "startOffset": 108, "endOffset": 112}, {"referenceID": 29, "context": "While [6] attempts to embed information about the dynamics at a feature representation level, works such as [18], [30] attempt to account for the temporal structure of the sequences of FIDs in the emotion model.", "startOffset": 114, "endOffset": 118}, {"referenceID": 12, "context": "System identification [13], [8] or compressive sensing-based techniques [28] could be used to compare different emotion instances.", "startOffset": 22, "endOffset": 26}, {"referenceID": 7, "context": "System identification [13], [8] or compressive sensing-based techniques [28] could be used to compare different emotion instances.", "startOffset": 28, "endOffset": 31}, {"referenceID": 27, "context": "System identification [13], [8] or compressive sensing-based techniques [28] could be used to compare different emotion instances.", "startOffset": 72, "endOffset": 76}, {"referenceID": 13, "context": "However, previous works [14], [16] have shown that it may be possible to avoid the burden of performing system identification by representing the output of a LTI system through the corresponding Hankel matrix.", "startOffset": 24, "endOffset": 28}, {"referenceID": 15, "context": "However, previous works [14], [16] have shown that it may be possible to avoid the burden of performing system identification by representing the output of a LTI system through the corresponding Hankel matrix.", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "The use of Hankel matrices, jointly with the dissimilarity score in [14], presents advantages in terms of space and time complexity.", "startOffset": 68, "endOffset": 72}, {"referenceID": 18, "context": "The first benchmark is the extended Cohn-Kanade dataset [19], which allows us to study the validity of this kind of feature representation for emotion recognition; the second dataset is the very challenging PAINFUL dataset [20], which allows us to study the proposed feature representation for pain localization in the wild.", "startOffset": 56, "endOffset": 60}, {"referenceID": 19, "context": "The first benchmark is the extended Cohn-Kanade dataset [19], which allows us to study the validity of this kind of feature representation for emotion recognition; the second dataset is the very challenging PAINFUL dataset [20], which allows us to study the proposed feature representation for pain localization in the wild.", "startOffset": 223, "endOffset": 227}, {"referenceID": 36, "context": "There is an extensive literature on face recognition [37], [15] and on facial expression analysis [35], [7].", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "There is an extensive literature on face recognition [37], [15] and on facial expression analysis [35], [7].", "startOffset": 59, "endOffset": 63}, {"referenceID": 34, "context": "There is an extensive literature on face recognition [37], [15] and on facial expression analysis [35], [7].", "startOffset": 98, "endOffset": 102}, {"referenceID": 6, "context": "There is an extensive literature on face recognition [37], [15] and on facial expression analysis [35], [7].", "startOffset": 104, "endOffset": 107}, {"referenceID": 10, "context": "In particular, [11] uses a Constrained Local Model (CLM) to obtain facial landmarks.", "startOffset": 15, "endOffset": 19}, {"referenceID": 5, "context": "In [6], a descriptor based on the movement of facial landmark points over time, jointly with spatio-temporal appearance features is extracted for each face image sequence.", "startOffset": 3, "endOffset": 6}, {"referenceID": 35, "context": "To account for temporal changes in the face appearance, Complete Local Binary Patterns from Three Orthogonal Planes (LBPTOP) [36] are used.", "startOffset": 125, "endOffset": 129}, {"referenceID": 21, "context": "In [22], restricted Boltzmann machine with local interactions (LRBM) is used to capture spatio-temporal patterns in the data.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "Since a sequence of FIDs is a time series, and may be affected by temporal warping, in [18] time-series kernel methods are used for emotional expression estimation using landmark data only.", "startOffset": 87, "endOffset": 91}, {"referenceID": 3, "context": "The work shows that emotion recognition may be done by adopting either the Dynamic Time Warping (DTW) kernel or the Global Alignment (GA) kernel [4, 5].", "startOffset": 145, "endOffset": 151}, {"referenceID": 4, "context": "The work shows that emotion recognition may be done by adopting either the Dynamic Time Warping (DTW) kernel or the Global Alignment (GA) kernel [4, 5].", "startOffset": 145, "endOffset": 151}, {"referenceID": 32, "context": "[33] propose to use Interval Temporal Bayesian Network (ITBN) to capture the spatial and temporal relations among primitive facial events.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "In [30], a Bayesian approach is used to model dynamic facial expression temporal transitions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "Hankel matrices have already been adopted for action recognition in [14], which adopts a Hankel matrix-based bag-of-words approach, and in [16], which models an action as a sequence of Hankel matrices and uses a set of HMM trained in a discriminative way to model the switching between LTI systems.", "startOffset": 68, "endOffset": 72}, {"referenceID": 15, "context": "Hankel matrices have already been adopted for action recognition in [14], which adopts a Hankel matrix-based bag-of-words approach, and in [16], which models an action as a sequence of Hankel matrices and uses a set of HMM trained in a discriminative way to model the switching between LTI systems.", "startOffset": 139, "endOffset": 143}, {"referenceID": 30, "context": "rameters [31].", "startOffset": 9, "endOffset": 13}, {"referenceID": 31, "context": "It is well known [32] that, given a sequence of output measurements [yo\ufffd .", "startOffset": 17, "endOffset": 21}, {"referenceID": 13, "context": "As previously done in [14], [16], we normalize the Hankel matrix \ufffd H as follows:", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "As previously done in [14], [16], we normalize the Hankel matrix \ufffd H as follows:", "startOffset": 28, "endOffset": 32}, {"referenceID": 13, "context": "Such score, introduced in [14], does not define a distance.", "startOffset": 26, "endOffset": 30}, {"referenceID": 15, "context": "\u2022 Sliding window-based representation: while the former representation assumes segmentation of the frame sequence into emotions, this representation could overcome this requirement by representing Y through a sequence of overlapping temporal window (similar to [16]).", "startOffset": 261, "endOffset": 265}, {"referenceID": 33, "context": "Inspired by [34], we use the NN classifier to predict the class label of each temporal window.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "Similarly to [16], a HMM is used to model the transition from a LTI system to the other.", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "In contrast to [16], we use standard HMM with independently estimated state spaces.", "startOffset": 15, "endOffset": 19}, {"referenceID": 18, "context": "In this paper, to demonstrate the whole framework, we consider shape features provided by an active appearance model [19], [20].", "startOffset": 117, "endOffset": 121}, {"referenceID": 19, "context": "In this paper, to demonstrate the whole framework, we consider shape features provided by an active appearance model [19], [20].", "startOffset": 123, "endOffset": 127}, {"referenceID": 17, "context": "To demonstrate the idea behind this paper, we restrict the attention to segmented emotion recognition in frontal view as done also in previous works such as [18], [1], [22], [33].", "startOffset": 157, "endOffset": 161}, {"referenceID": 0, "context": "To demonstrate the idea behind this paper, we restrict the attention to segmented emotion recognition in frontal view as done also in previous works such as [18], [1], [22], [33].", "startOffset": 163, "endOffset": 166}, {"referenceID": 21, "context": "To demonstrate the idea behind this paper, we restrict the attention to segmented emotion recognition in frontal view as done also in previous works such as [18], [1], [22], [33].", "startOffset": 168, "endOffset": 172}, {"referenceID": 32, "context": "To demonstrate the idea behind this paper, we restrict the attention to segmented emotion recognition in frontal view as done also in previous works such as [18], [1], [22], [33].", "startOffset": 174, "endOffset": 178}, {"referenceID": 18, "context": "We have performed experiments for emotion recognition on the widely adopted Extended Cohn-Kanade dataset (CK+) [19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 18, "context": "We adopted the validation protocol suggested in [19], which is leave-onesubject-out cross-validation.", "startOffset": 48, "endOffset": 52}, {"referenceID": 18, "context": "CK+ [19] 35 25 68.", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "4 CLM-based [1] 70.", "startOffset": 12, "endOffset": 15}, {"referenceID": 21, "context": "4 LRBM [22] 97.", "startOffset": 7, "endOffset": 11}, {"referenceID": 32, "context": "6 ITBN [33] 91.", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "We are not presenting results on the raw data via SVM because this experiment would be similar to the baseline method reported in [19] (in the lower part of the table).", "startOffset": 130, "endOffset": 134}, {"referenceID": 21, "context": "Among the sliding window-based approaches, only when adopting NN and majority vote the performance are comparable or higher than the one reported in [22].", "startOffset": 149, "endOffset": 153}, {"referenceID": 15, "context": "These experiments suggest that probably emotions can be represented as the output of just one LTI system and there may be no dynamics switching as instead may happen in human actions [16].", "startOffset": 183, "endOffset": 187}, {"referenceID": 19, "context": "To test our dynamics-based FID representation for pain detection, we use the Painful dataset [20].", "startOffset": 93, "endOffset": 97}, {"referenceID": 19, "context": "The videos were annotated on a frame-per-frame basis with the Prkachin and Solomon pain intensity (PSPI) score [20], which ranges in [0, 15] where 0 means no pain, while a value greater than 0 indicates a certain intensity of pain.", "startOffset": 111, "endOffset": 115}, {"referenceID": 14, "context": "The videos were annotated on a frame-per-frame basis with the Prkachin and Solomon pain intensity (PSPI) score [20], which ranges in [0, 15] where 0 means no pain, while a value greater than 0 indicates a certain intensity of pain.", "startOffset": 133, "endOffset": 140}], "year": 0, "abstractText": "This paper proposes a new approach to model the temporal dynamics of a sequence of facial expressions. To this purpose, a sequence of Face Image Descriptors (FID) is regarded as the output of a Linear Time Invariant (LTI) system. The temporal dynamics of such sequence of descriptors are represented by means of a Hankel matrix. The paper presents different strategies to compute dynamics-based representation of a sequence of FID, and reports classification accuracy values of the proposed representations within different standard classification frameworks. The representations have been validated in two very challenging application domains: emotion recognition and pain detection. Experiments on two publicly available benchmarks and comparison with state-of-the-art approaches demonstrate that the dynamics-based FID representation attains competitive performance when off-theshelf classification tools are adopted.", "creator": "cairo 1.8.8 (http://cairographics.org)"}}}