{"id": "1506.05702", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2015", "title": "Comparing the writing style of real and artificial papers", "abstract": "Recent years have witnessed the increase of competition in science. While promoting the quality of research in many cases, an intense competition among scientists can also trigger unethical scientific behaviors. To increase the total number of published papers, some authors even resort to software tools that are able to produce grammatical, but meaningless scientific manuscripts. Because automatically generated papers can be misunderstood as real papers, it becomes of paramount importance to develop means to identify these scientific frauds. In this paper, I devise a methodology to distinguish real manuscripts from those generated with SCIGen, an automatic paper generator. Upon modeling texts as complex networks (CN), it was possible to discriminate real from fake papers with at least 89\\% of accuracy. A systematic analysis of features relevance revealed that the accessibility and betweenness were useful in particular cases, even though the relevance depended upon the dataset. The successful application of the methods described here show, as a proof of principle, that network features can be used to identify scientific gibberish papers. In addition, the CN-based approach can be combined in a straightforward fashion with traditional statistical language processing methods to improve the performance in identifying artificially generated papers. The authors describe the process to identify fraudulent papers in an understandable fashion. The technique can be used to identify fraudulent papers that are published on the Internet. This enables researchers to identify fraudulent papers that are published in legitimate scientific journals without the need for a specialized program or program. The methods have been validated in this work by the team in the past. This methodology ensures that any genuine work is verified at a systematic scale and is used in the present. Although the method was initially used by the authors, it is now used in many of the studies on the Internet. The technique can be used to identify falsified papers in a timely manner, and to identify fraudulent papers on the Internet. The process can be used to identify falsified papers in a timely manner. The methods were originally used in the first place and are used in more recent work by the researchers who successfully identified fraudulent papers, and to identify fraudulent papers in a timely manner. In this report, I demonstrate that this methodology provides a method that can accurately identify falsified papers in a timely manner, and to identify fraudulent papers in a timely manner. In this report, I demonstrate that this method provides a method that can accurately identify falsified papers in a timely manner, and to identify fraudulent papers in a timely manner. In this report, I demonstrate that this method provides a method that can accurately identify falsified papers in a timely manner, and to identify fraudulent papers", "histories": [["v1", "Thu, 18 Jun 2015 14:46:15 GMT  (2802kb,D)", "http://arxiv.org/abs/1506.05702v1", null], ["v2", "Tue, 28 Jul 2015 15:50:56 GMT  (2802kb,D)", "http://arxiv.org/abs/1506.05702v2", "To appear in Scientometrics (2015)"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["diego r amancio"], "accepted": false, "id": "1506.05702"}, "pdf": {"name": "1506.05702.pdf", "metadata": {"source": "CRF", "title": "Comparing the topological properties of real and artificially generated scientific manuscripts", "authors": ["Diego Raphael Amancio"], "emails": [], "sections": [{"heading": null, "text": "Keywords scientific frauds \u00b7 SCIgen \u00b7 complex networks \u00b7 plagiarisms"}, {"heading": "1 Introduction", "text": "The dissemination of knowledge and the advancement of science strongly depend upon the precise interpretation of the content conveyed in scientific\nDiego Raphael Amancio Institute of Mathematics and Computer Science University of Sa\u0303o Paulo, P. O. Box 369, Postal Code 13560-970 Sa\u0303o Carlos, Sa\u0303o Paulo, Brazil\nar X\niv :1\n50 6.\n05 70\n2v 1\n[ cs\n.C L\n] 1\n8 Ju\nn 20\n15\nmanuscripts. Therefore, the ideas conveyed by high-quality scientific papers should be carefully detailed so that they can be tried and possibly improved. Although many qualitative aspects have been proposed to identify outstanding manuscripts and their respective authors, many quantitative aspects still prevail when the quantification of academic merit is at stake. For example, in recent years, the total number of articles, the number of citations motivated by articles or researchers\u2019 h-index has been widely used for the purpose of merit evaluation [1]. Clearly, there is a correlation between quantitative and qualitative factors [2]. Nevertheless, the drawbacks related to the exclusive use of quantitative factors are well known. For example, recent publications cannot be evaluated via citation counts. Likewise, very young researchers also cannot be assessed according to the number of citations that their articles motivate, since there is an expected delay between paper publication date and wide scientific recognition [3\u20137].\nWhile there are several disadvantages associated with quantitative indices, they unmistakably provide a minimal degree of objectivity required for any scientific merit assessment. Aware of the prevalence of quantitative indexes in scientific merit judgments, some scholars tend to shape their research only to increase their citation counts and other quantitative indexes [8]. The pressure imposed by scientific competition, translated by the maxim \u201cpublish or perish\u201d, literally urge a few scholars not to follow good scientific practices. In order to artificially boost productivity and impact indexes, some authors split the results arising from a single discovery in two or more papers. For these reasons, several scientific low-quality papers with a very weak impact on science have been produced. Other recurrent unethical conducts include the excessive use of self-citations [8,9] and plagiarisms [10\u201312]. More recently, even texts automatically generated have been submitted and surprisingly deemed suitable for publication in several scientific conferences [13]. Currently, one of the most popular software for generating fake papers is the SCIGen [14], an algorithm able to produce gibberish papers that resemble real manuscripts. To produce such meaningless texts, SCIGen uses a complex grammar that is able of generating texts containing all the features expected in a standard scientific manuscript. To complement the grammar, even figures and tables are generated. An incremental modification of the original algorithm has implemented the possibility of self-citations, which has allowed a significant increase in authors\u2019 h-index [15]. Because fake papers as those generated by SCIGen can eventually bewilder even a human referee, it becomes of paramount relevance the identification of particular features able to discriminate real from meaningless texts. In this context, I focus on one approach to identify distinct styles in texts that has proven particularly effective to detect SCIGen texts. More specifically, using a representation of texts as complex networks [16], I show that it is possible to discriminate real and fake manuscripts with significant accuracy if one analyzes the structural organization of the manuscripts. Even though the accuracy of the proposed technique does not outperforms other traditional methods based on the analysis of textual content, it is useful to show\nthe structural patterns of text organization is affected when fake information is conveyed.\nThis manuscript is organized as follows. In Section 2, I present related approaches aiming at the identification of fake scientific manuscripts. A very short introduction to the application of complex networks for text analysis is presented is Section 3. The methods employed for the representation, characterization and classification of text networks are presented in Section 4. The results obtained with both univariate and multivariate analysis of network measurements are presented in Section 5. Finally, the conclusion drawn from the results and the perspectives for further studies are commented in Section 6."}, {"heading": "2 Related works", "text": "Several methods have been devised to identify the authenticity of scientific manuscripts. Such methods can be classified according to the type of information that is employed as features of classifiers. Usually the list of references plays a important role in the task. For example, it has been shown that when many cited references cannot be found online, then there is a high probability that the paper under analysis is fake [17].\nMany heuristics rely on textual content to infer the authenticity of documents [18, 19]. The study developed in [20] proposes some useful rules. One of the main rules tests whether keywords in the title and abstract occurs frequently in the body of the paper. If such pattern does not occur, then the document is considered as fake. Another interesting observation highlights that real scientific papers usually mention keywords from the titles of cited papers. Techniques based on the semantic content of texts also employ traditional similarity measurements [21]. An important contribution to the problem of identifying gibberish publications with similiraty measurements was introduced in [22]. In their study, the authors devised a pairwise similarity measurement that compares two pieces of texts by counting differences in word frequencies. This approach was useful to identify several cases of duplicate and fake publications. An extension of this work was proposed in [23], where not only single word occurrences are considered, but also multi-word phrases. Other interesting approaches relying on textual content include the techniques based on the compressibility rate of texts [24,25]. In [25], the authors show that artificially generated papers display values of compressibility rate that are not compatible with the rates observed in real manuscripts.\nDifferently from approaches mentioned in this section, the method I proposed in this paper does not consider the semantic similarity of texts. Actually, the approach proposed here focus on the analysis of connectivity patterns that are able to capture subtleties of styles in texts from distinct sources. Because the proposed approach is complementary to other traditional techniques, it could potentially be useful to improve the reliability of the classification."}, {"heading": "3 Complex networks and text analysis", "text": "Complex networks have been employed to model a myriad or real complex systems [16]. Of particular interest to the aims of this study are the applications in automatic summarization [26, 27], machine translation [28, 29], complexity analysis [30\u201334] and authorship recognition [35,36]. In all these tasks, the networks obtained from the so called word adjacency model (see Section 4.1) allowed a precise characterization of texts with regard to specific textual features. Networked models even allowed the characterization of unknown manuscripts [37] and many other linguistic aspects [38\u201342]. Interestingly, the particular features of each language could also be classified in terms of the topological structure of complex networks [43\u201345]. A more detailed survey on the application of network methods in text analysis can be found in [46]. Differently from traditional approaches, the method proposed here focus on the structure and organization of texts, rather than on the textual semantic content. The proposed method also differs from the other techniques mentioned here because it is modified in order to to minimize the influence of the vocabulary size on the topological analysis of scientific articles (see Section 5)."}, {"heading": "4 Methodology", "text": "The methodology employed to compare real and fake manuscripts is illustrated in Figure 1. Firstly, the text of the scientific article are obtained by automatically removing the LATEX tags from the original texts. Because some undesirable tokens can still remain in the text, the output is checked manually. Following previous studies, the style of each text is quantified via topological characterization of complex networks (graphs) [35\u201337, 47, 48]. For this reason, the texts are modeled as complex networks. Then, several connectivity measurements are extracted from the networks. In the next step, the measurements are employed as features to discriminate real and fake manuscripts. The discrimination is accomplished with pattern recognition methods. The main steps shown in Figure 1 are described below.\n4.1 Modeling texts as complex networks\nA network can be defined as a set of nodes connected by edges. To represent a network, consider that A = {aij} is the matrix representing the network structure. In texts, each distinct word is a node and edges are established between adjacent words. Therefore, if words i and j appear adjacent in the text, the element aij is set (aij = 1). Otherwise, aij = 0. The total number of links, i.e. the node degree, is defined as k(i) = \u2211 j aij . In several stylebased applications, some pre-processing steps are usually applied before the connection of adjacent words [49]. The pre-processing algorithm encompasses\nLATEX FORMAT\nTEXT FORMAT\nFILTERED MANUSCRIPT\nL T F\nPRE-PROCESSED MANUSCRIPT\nP\n(1) (2) (3) (4)\na two-fold mechanism: (a) the identification and removal of stopwords; and (b) the lemmatization. In (a), words conveying low semantic content (e.g. \u201cand\u201d, \u201cof\u201d, \u201ca\u201d, \u201can\u201d) are removed. Since these words are simply used to connect content words, they can be straightforwardly replaced by edges. In (b), each remaining word is mapped to its canonical form [21]. As a consequence, conjugated verbs are mapped to their infinitive forms. Likewise, nouns are mapped to their singular forms. In order to obtain word lemmas [21], it is imperative to know in advance the part-of-speech of words. In this study, each word was labeled with its part-of-speech using a maximum entropy model [50]. To illustrate the modeling of a text as a network, the pre-processing steps (a) and (b) are applied to a short extract from the book \u201cAdventures of Sally\u201d, by P.G. Wodehouse:\nOriginal text: \u201cIf Sally had been constantly in Bruce Carmyle\u2019s thoughts since they had parted on the Paris express, Mr. Carmyle had been very little in Sally\u2019s\u2013so little, indeed, that she had had to search her memory for a moment before she identified him\u201d.\n(a) Removal of stopwords and punctuation marks: Sally constantly Bruce Carmyle thoughts parted Paris express Carmyle little Sally little search memory moment before identified (b) Lemmatization: Sally constant Bruce Carmyle think part Paris express Carmyle little Sally little search memory moment before identify\nThe network obtained from the pre-processed text is shown in Figure 2. Note that, after step (b), each distinct word becomes a distinct node and edges are established between adjacent words.\nLITTLE\nMEMORYBEFORE\n4.2 Topological measurements of complex networks\nThe topological characterization of complex networks can be performed by computing topological measurements. Currently, there exists several topological measurements [16]. In this study, the measurements usually employed for textual analysis were chosen to characterize the topological attributes of text networks. A swift description of each measurement is provided below.\n\u2013 Average node degree: this local measurement, quantifies the average connectivity of the neighbors:\nkn(i) = \u2211 j aijk(i) / \u2211 j aij . (1)\n\u2013 Clustering coefficient: the clustering coefficient (C) is a quasi-local measurement that quantifies the density of links between neighbors. Mathematically, the clustering coefficient is defined as C = 3na/nb, where\nna = \u2211\nk>j>i\naijaikajk, (2)\nnb = \u2211\nk>j>i\naijaik + ajiajk + akiakj . (3)\nIn textual applications, the clustering coefficient of specific words tends to quantify the number of distinct contexts in which the word appears [35].\n\u2013 Accessibility: the accessibility (or diversity) (\u03b1) is a extension of the degree that is based on both topology and dynamics of networks [26]. This centrality index is relevant to identify topological high-degree nodes that effectively access only a few neighbors [51]. To define this measurement,\nconsider the following definition. Let p (h) ij be the likelihood of a random walker to go from node i to node j in h steps. The accessibility is computed as the irregularity of the distribution of p (h) ij :\n\u03b1(h)(i) = exp ( \u2212 \u2211\np (h) ij ln p (h) ij\n) . (4)\nThis measurement has been employed to detect the border of complex networks [51]. In textual applications, the accessibility has been useful to identify core concepts, allowing thus the construction of informative automatic summarizers [26]. \u2013 Average shortest path length: the shortest path length (l) quantifies the typical distance between two nodes of the network. This measurement was employed because it has been useful in textual applications [35,52]. In word adjacency networks, this measurement has proven relevant to identify keywords, even if they are not very frequent [35]. \u2013 Betweenness: the betweenness (B) is a centrality measurement. This means that the highest values of betweenness are assigned to the most relevant concepts in word adjacency networks. This measurement quantifies how easily a node can be accessed, provided that walks are performed via\nshortest paths. Let g (m) ij be the number of shortest paths between nodes i and j passing through node m. If gij is the total number of shortest paths between i and j (passing through any intermediary node), then the betweenness is defined as\nB(i) = \u2211 i \u2211 j g (m) ij gij . (5)\nIn textual networks, the betweenness quantifies the number of distinct contexts of a given word [35]. Unlike the clustering coefficient, this measurement uses the global network connectivity to infer the number of contexts [35].\n\u2013 Assortativity: the assortativity (r) quantifies degree-degree correlations [54]. In other words, it measures the tendency of nodes with similar degree to be connected. Mathematically, it can be defined as\nr = M\u22121\n\u2211 j>i k(i)k(j)\u2212 [ M\u22121 \u2211 j>i aij(k(i) + k(j))/2 ]2 M\u22121 \u2211 j>i aij(k 2(i) + k2(j))/2\u2212 [ M\u22121 \u2211 j>i aij(k(i) + k(j))/2)\n]2 . (6)\nNetworks whose assortativity take positive values are referred to as assortative networks. On the other hand, if there is a negative correlation between\nthe degree of linked nodes, then the network is disassortative. In word adjacency networks, a disassortative behavior arises even when stopwords are removed from the analysis [55].\n4.3 Pattern recognition methods\nIn a supervised classification task, the objective is to automatically distinguish objects (or instances) according to their classes. The characterization of each object is made with object attributes (or features). In this study, one desires to distinguish between two class: (i) the \u201creal\u201d class, which include real scientific papers; (ii) and the \u201cfake\u201d class, which encompasses the papers automatically generated by the SCIGen algorithm. As features, I chose the network measurements described in Section 4.2. The following pattern recognition methods were employed in this study:\n\u2013 Naive Bayes (NBY): the naive bayes classifier uses the Bayesian optimal decision rule to classify an object. The class c\u2032 is chosen if the condition\nP (c\u2032|m) > P (ck|m), (7)\nholds for each ck 6= c\u2032, where P (ck|m) is the likelihood of class ck to appear in the context represented by the set of network measurements m. In most cases, the exact behavior of P (ck|m) is unknown. To overcome this issue, the Bayes\u2019 theorem can be used:\nc\u2032 = arg max ck P (ck|m) = arg max ck P (m|ck) P (m) P (ck)\n= arg max ck P (m|ck)P (ck) = arg max ck\n[ logP (m|ck) + logP (ck) ] . (8)\nAssuming in eq. (8) attribute independence and considering that the topological context is given by a set of network measurementsm = {m1,m2 . . .}, then P (m|ck) can be written as\nP (ck|m) = P ({mi|mi \u2208 m}|ck) = \u220f\nmi\u2208m P (mi|ck). (9)\nTherefore, the accurate class c\u2032 associated to the unknown instance is\nc\u2032 = arg max ck\n[ logP (ck) + \u2211 mi\u2208m logP (mi|ck) ] . (10)\nTo illustrate the decision process, consider Figure 3. The position of each circle in the x-axis represents the value obtained for a given measurement. Distinct colors represent different classes (c1=\u201cblue\u201d and c2=\u201cred\u201d). Considering that the frequency of occurrence in the dataset of each class is the same, the term logP (ck) can be disregarded in eq. 10. The remaining term, the likelihood P (mi|ck), can be estimated via the Parzen window\nMEASUREMENT\nRED P R O B A B IL IT Y\nBLUE\nBLUE\ntests performed on attributes (see Figure 5). The decision process starts at the root (i.e. the node with no parents). When a leaf node is reached, the class associated to that node is selected. The generation of a decision tree requires the definition of a measure that is able to identify the most informative attribute at each step of the algorithm. More specifically, in this paper, I used the Kullback-Leibler divergence [59]. Mathematically, the Kullback-Leibler divergence \u2126(Str,mi) of the attribute mi computed in the training dataset Str is\n\u2126(Str,mi) = H(Str)\u2212H(Str|mi), (12)\nwhere H(Str) is the entropy computed in the the training dataset Str and H(Str|mi) is the entropy of the dataset when the value of mi is specified. Particularly, H(Str|mi) can be computed from Str as\nH(Str|mi) = \u2211\nv\u2208V (mi)\n|\u03b2(tr) \u2208 Str|\u03b2 (k) (tr) = v|\n|Str| \u00b7 H({\u03b2(tr) \u2208 Str|\u03b2 (k) (tr) = v},\n(13) where V (mi) is the set of all values taken by the attribute mi in the training dataset.\n4.4 Quantifying feature relevance\nThe method employed for quantifying feature relevance assigns high values of relevance for a given attribute if its use usually yields high quality classifiers.\nMore specifically, this method counts the frequency of appearance of each feature among the best classifiers, when one analyzes all possible combination of features. Let F be a set comprising nf features. Using F , it is possible to generate nc = 2\nnf distinct combinations of features. To quantify the relevance of each feature, the nc combinations are sorted in decreasing order according to the accuracy rate provided by each combination. Suppose that \u03beij represents the ordered set of combinations, where\n\u03beij = { 1 if the i-th best combination employed the j-th feature, 0 otherwise.\n(14)\nThen \u03beij can be used to verify if a feature j tends to appear among the best classifiers. This can be done by defining the function f(x) as\nf(x) = x\u2211 i=1 \u03beij , {x \u2208 N\u2217|x \u2264 nc}. (15)\nNote that f(x) increases quickly whenever j is frequent among the best combinations of features. Conversely, if a given feature j is more frequent among the worst classifiers, f(x) increases significantly only for high values of the domain. Therefore, the prominence \u03c1(j) of feature j can be computed as the area underneath the curve f(x):\n\u03c1(f) = \u222b nc 1 f(x)dx = nc\u2211 i=1 i\u22121\u2211 k=1 \u03bekj + 1 2 nc\u2211 i=1 \u03beij . (16)"}, {"heading": "5 Results", "text": "In this study, the style of real and fake manuscripts were compared. As fake manuscripts, I considered the texts generated by the SCIGen algorithm, which produces scientific manuscripts using a proper grammar. The style of the SCIGen papers were compared with the style of real manuscripts recovered from the following sources: (a) the Pattern Recognition Letters journal (PRL) [60]; (b) the arXiv repository comprising Computer Science papers (arXiv/cs) [61]; and (c) the Journal of Informetrics (JI) [62]. Four hundred manuscripts were used in the experiments. Note that most of the measurements presented in Section 4.2 are local measurements. Therefore, each node is associated to a specific value. To characterize each manuscript, I used the global distribution of measurements for all the words in the manuscript. Here, the goal is to obtain quantities characterizing relevant factors of the distributions to be used as global measurements. Using the same strategy of previous studies [37, 52], the average \u3008X\u3009 and the deviation \u2206X of each local measurement X was extracted. Therefore, the features employed to characterize the style of the manuscripts were\n\u3008\u03b1(h=2)\u3009, \u2206\u03b1(h=2), \u3008\u03b1(h=3)\u3009, \u2206\u03b1(h=3), \u3008kn\u3009, \u2206kn, \u3008B\u3009, \u2206B, \u3008C\u3009, \u2206C, r, \u3008l\u3009, and \u2206l.\nTo minimize the correlation of the above measurements with the frequency of words, the following normalization was applied. Let X\u0303 be the value of a given measurement obtained in a text and \u3008X(R)\u3009 the average value of the same measurement obtained in 20 randomized versions of the text. Then, the normalized measurement is computed as\nX = X\u0303\n\u3008X(R)\u3009 . (17)\nAfter characterizing the topological structure of the manuscripts, the hypothesis that real and fake manuscripts yields distinct network properties was probed. A twofold method was employed to accomplish the identification of fake papers: a univariate and a multivariate approach.\n5.1 Univariate analysis\nIn this approach, the discriminability of real and fake papers was analyzed by considering just a single measurement or each classifier generated. The accuracy rate obtained in each dataset is shown in Table 1. The discrimination of PRL and SCIGen papers was accomplished with an accuracy of 79% in the best scenario, when the average neighbor degree \u3008kn\u3009 and the average accessibility \u3008\u03b1h=2\u3009 was employed along with the tree (C4.5) algorithm. The accurate discrimination between arXiv/cs and SCIGen papers could be performed in 88% of the cases. This accuracy rate was obtained with the standard deviation of the average neighbor degree \u2206kn. The highest accuracy rate occurred when distinguishing JI and SCIGen papers. In this case, 91% of the papers could be successfully discriminated. Taken together, these results confirm that the networked representation of texts is useful to distinguish real manuscripts from those automatically generated from SCIGen. Especially, it is possible to note that SCIGen texts are more similar to the PRL manuscripts, which can be explained by both content and structural similarities, because both datasets comprise letters about computer science issues. While the arXiv/cs also comprises Computer Science papers, the format allowed by this repository is much more generic than the structural format generated by the SCIGen algorithm. Hence, as expected, a larger discriminability was found when SCIGen and arXiv/cs were compared. When comparing JI and SCIGen, an even larger distinguishability was obtained probably because both structural and semantical contents are distinct.\nThe individual performance of the attributes employed in the univariate analysis can be summarized as follows:\n\u2013 Accessibility: the average accessibility \u3008\u03b1(h=2)\u3009 presented an average discriminative ability. The average accessibility at the third level was particularly useful in the PRL dataset, since the highest accuracy was found when the \u3008\u03b1(h=3)\u3009 was employed with the C4.5 method. The deviation \u2206\u03b1(h=3) proved specially relevant to identify real papers in the arXiv/cs and JI datasets.\nEven though the univariate analysis is able to identify which attributes are more useful to discriminate specific classes, this analysis does not take into consideration the inter-relationship between different attributes. Because the interaction of attributes may improve the quality of the classifiers, in the next section, I approach the classification task as a multivariate problem.\n5.2 Multivariate analysis\nIn the multivariate analysis, all 13 measurements were combined and applied as features of the classifiers. A two-dimensional projection of the data using\nthe principal component analysis technique [53] is shown in Figure 6. Interestingly, it is possible to note that the worst discrimination occurred in the PRL dataset, as revealed by the large overlapping region. Conversely, a much better discrimination was achieved with the arXiv.org/cs dataset. These results are consistent with the patterns found when the univariate analysis was performed. Another interesting pattern arising from the visualization provided in Figure 6 concerns the variability of style of SCIGen papers. It is clear that the style of SCIGen papers displays a lower variability when compared to the style of real texts. This effect can be easily perceived, e.g. by observing that SCIGen papers are scattered in a small region in Figure 6(b).\nThe accuracy rates obtained with the multivariate classification is shown in Table 2. When one compares the results obtained here with the ones achieved with the univariate analyses, it is clear that the multivariate analysis improved the discriminative ability of the classifiers. The accuracy rate in the PRL dataset improved 10% (from 79% to 89%). In the arXiv/cs dataset, the accuracy went from 88% to 95%. The lowest increase in accuracy occurred for the JI dataset, which already had provided an excellent discriminability with the univariate approach. These results suggest that the interaction of attributes is able to improve the identification of fake papers generated by the SCIGen algorith, especially if the separation between real and fake papers is not so clear when a single measurement is employed to generate the classifiers.\nAlthough all attributes have been used as input to the machine learning methods, only some of them are selected to generate a given model. This is clear when one observes the decision tree shown in Figure 7, which summarizes the patterns recognized in the PRL dataset. The relevance of each attribute employed in the multivariate analysis was quantified with the technique described in Section 4.4. The ranking obtained for each measurement in each classifier is shown in Table 4. Note that there is a strong consistency between rankings of measurements across distinct classifiers in the same dataset. This consistency is confirmed by the high values of Spearman\u2019s rank correlation of rankings (see Table 3). The performance of each measurement for the classification is commented below.\nassortativity\naverage accessibility\nh=2\naverage betweenness\naverage accessibility\nh=2\nvariability of accessibility\nh=2\nPRL\nPRLSCIGen\nSCIGen\nSCIGen\n> 1.56<= 1.56\n<= 0.74 > 0.74\n> 1.07<=1.07\n> 0.75<= 0.75\n<= 0.79 >0.79\nThe best performance using accessibility measurements in the PRL dataset was achieved with the average \u3008\u03b1(h=2)\u3009. \u2013 Neighbors degree: an excellent performance was observed for the deviation \u2206kn in both arXiv/cs and JI datasets. Note that \u2206kn reached second place in both repositories. Particularly, in the PRL dataset, the average \u3008kn\u3009 performed better than the deviation \u2206kn. \u2013 Betweenness: the average \u3008B\u3009 performed very well in all three datasets. This suggests that this measurement becomes very discriminative when combined with other attributes. Note that, in the univariate analysis, the betweenness displayed low accuracy rates (see Table 1). \u2013 Clustering coefficient: the combination with other attributes does not seem to improve the discriminability of this measurement. \u2013 Assortativity: the importance of this measurement depends on the dataset. The best performance, a second position, was achieved in the PRL dataset when the Naive Bayes classifier was used. \u2013 Shortest paths: the average \u3008l\u3009 and specially the deviation \u2206l ranked among the worst measurements. Therefore, similarly to the clustering coefficient, the average shortest path length is not informative even when associated with others measurements.\nAll in all, the combination of attributes improved the performance of the classifications. The attributes with the highest discrimination ability were the average betweenness \u3008B\u3009 (PRL dataset) and the standard deviation of the accessibility \u2206\u03b1(h=3). Although some measurements turned out to be not informative in specific datasets, they still can be useful in other scenarios, as the discriminability may depend on the data distribution. For this reason, the\nclustering coefficient and the average shortest path length should be tried in other datasets."}, {"heading": "6 Conclusions", "text": "In the current paper, I have investigated the hypothesis that artificially generated manuscripts can be distinguished from real scientific papers via topological characterization of complex networks. The combination of network features (extracted from the word adjacency model) and machine learning methods allowed the correct identification of SCIGen papers in 89% of the cases (worst scenario). This means that there are hidden patterns in the organization of papers generated by SCIGen that differs from the structural patterns arising from real texts. Even though the techniques presented in this manuscript does not outperform the methods based on textual content, it could be employed in applications where the complementary nature of the proposed attributes plays a prominent role to discriminate pieces of texts with similar content [42,63].\nThe analysis of relevance of attributes revealed that the combination of distinct topological attributes is the most successful approach. Concerning the individual performance of topological features, the accessibility and the betweenness performed particularly well mainly in the multivariate analysis. Conversely, the clustering coefficient and the shortest path length displayed the poorest performance among the topological features employed. The results presented here confirm, as a proof of principle, that the word adjacency model can be useful to identify fake papers. Future works could pursue an improvement of performance with a fine tuning of classifiers parameters [57]. Another possibility is to propose novel topological measurements to combine the techniques presented in this paper with traditional statistical natural language processing methods [21].\nAcknowledgements I am thankful to Sa\u0303o Paulo Research Foundation (FAPESP) (grant number 14/20830-0) for the financial support."}], "references": [{"title": "Diffusion of scientific credits and the ranking of scientists", "author": ["F. Radicchi", "S. Fortunato", "B. Markines", "A. Vespignani"], "venue": "Phys. Rev. E", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Correlation between journal impact factor and citation performance: an experimental study", "author": ["U. Finardia"], "venue": "Journal of Informetrics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Better late than never? On the chance to become highly cited only beyond the standard time", "author": ["W. Glanzel", "B. Schlemmer", "B. Thijs"], "venue": "horizon. Scientometrics", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "It takes time: a remarkable example of delayed recognition", "author": ["B. Van Calster"], "venue": "Journal of the American Society for Information Science and Technology", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Generalized preferential attachment considering aging", "author": ["Y. Wu", "T.Z.J. Fu", "D.M. Chiu"], "venue": "Journal of Informetrics", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Aging in citation networks", "author": ["K.B. Hajra", "Sen P"], "venue": "Physica A", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Scientific impact evaluation and the effect of self-citations: mitigating the bias by discounting the h-index", "author": ["E. Ferrara", "A.E. Romero"], "venue": "Journal of the American Society for Information Science and Technology", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Classification method for detecting coercive self-citation in journals", "author": ["T. Yua", "G. Yua", "Wang M-Y"], "venue": "Journal of Informetrics", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "A bibliometric analysis of plagiarism and self-plagiarism through D\u00e9j\u00e0", "author": ["A. Gar\u0107\u0131a-Romero", "J.M. Estrada-Lorenzo"], "venue": "vu. Scientometrics", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Patterns of text reuse in a scientific corpus", "author": ["D.T. Citron", "P. Ginsparg"], "venue": "PNAS 112(1),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Ike antkare, one of the great stars in the scientific firmament", "author": ["C. Labb\u00e9"], "venue": "International Society for Scientometrics and Informetrics Newsletter", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Detecting h-index manipulation through selfcitation analysis", "author": ["C. Bartneck", "S. Kokkelmans"], "venue": "Scientometrics 87(1),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Networks: An Introduction", "author": ["M. Newman"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "An effective method to identify machine automatically generated paper", "author": ["J. Xiong", "T. Huang"], "venue": "In Pacific-Asia Conference on Knowledge Engineering and Software Engineering,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Automated screening: arXiv screens spot fake papers", "author": ["P. Ginsparg"], "venue": "Nature", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Publishers withdraw more than 120 gibberish papers", "author": ["R. Van Noorden"], "venue": "Nature", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Algorithmic detection of computer generated text", "author": ["A. Lavoie", "M. Krishnamoorthy"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["C.D. Manning", "H. Schutze"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "Duplicate and fake publications in the scientific literature: how many scigen papers in computer science", "author": ["C. Labb\u00e9", "D. Labb\u00e9"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Measuring structural distances between texts. arXiv: abs/1403.4024", "author": ["U. Fahrenberg", "F. Biondi", "K. Corre", "C. J\u00e9gourel", "S. Kongshoj", "A. Legay"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "The similarity metric", "author": ["M. Li", "X. Chen", "X. Li", "B. Ma", "P. Vitanyi"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "Using compression to identify classes of inauthentic texts", "author": ["M.M. Dalkilic", "W.T. Clark", "J.C. Costello", "P. Radivojac"], "venue": "In Proceedings of the 2006 SIAM Conference on Data Mining", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2006}, {"title": "Extractive summarization using complex networks and syntactic dependency", "author": ["D.R. Amancio", "M.G.V. Nunes", "O.N. Oliveira Jr.", "Costa", "L. da F"], "venue": "Physica A,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "A complex network approach to text summarization", "author": ["L. Antiqueira", "O.N. Oliveira Jr.", "Costa", "L. da F", "M.G.V. Nunes"], "venue": "Information Sciences,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Complex networks analysis of manual and machine translations", "author": ["D.R. Amancio", "L. Antiqueira", "T.A.S. Pardo", "Costa", "L. da F", "O.N. Oliveira Jr.", "Nunes", "M.G. V"], "venue": "International Journal of Modern Physics C", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "Using metrics from complex networks to evaluate machine translation", "author": ["D.R. Amancio", "M.G.V. Nunes", "O.N. Oliveira Jr.", "T.A.S. Pardo", "L. Antiqueira", "Costa", "L. da F"], "venue": "Physica A", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Complex networks analysis of language complexity", "author": ["D.R. Amancio", "S.M. Aluisio", "O.N. Oliveira Jr.", "Costa", "L. da F"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "A practical approach to language complexity: a wikipedia case study", "author": ["T. Yasseri", "A. Kornai", "J. Kert\u00e9sz"], "venue": "PLoS ONE 7,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Can syntactic networks indicate morphological complexity of a language", "author": ["H. Liu", "C. Xu"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Language networks: their structure, function and evolution", "author": ["R.V. Sol\u00e9", "B.B. Corominas-Murtra", "S. Valverde", "L. Steels"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "The complexity of chinese syntactic dependency networks", "author": ["H. Liu"], "venue": "Physica A", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2008}, {"title": "Comparing intermittency and network measurements of words and their dependency on authorship", "author": ["D.R. Amancio", "E.G. Altmann", "O.N. Oliveira Jr.", "Costa", "L. da F"], "venue": "New J. Phys", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2011}, {"title": "Probing the topological properties of complex networks modeling short written texts", "author": ["D.R. Amancio"], "venue": "PLoS ONE 10 e0118394", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Probing the statistical properties of unknown texts: application to the Voynich manuscript", "author": ["D.R. Amancio", "E.G. Altmann", "D. Rybski", "O.N. Oliveira Jr.", "Costa", "L. da F"], "venue": "PLOS ONE", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Graph analysis of dream reports is especially informative about psychosis", "author": ["N.B. Mota", "R. Furtado", "Maia", "P.P.C", "M. Copelli", "S. Ribeiro"], "venue": "Scientific reports", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Links that speak: the global language network and its association with global fame", "author": ["S. Ronen", "B. Gon\u00e7alves", "K.Z. Hu", "A. Vespignani", "S. Pinker", "C.A. Hidalgo"], "venue": "PNAS 111(52),", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "Networks in cognitive science", "author": ["A. Baronchelli", "R. Ferrer-i-Cancho", "R. Pastor-Satorras", "N. Chater", "M.H. Christiansen"], "venue": "Trends in cognitive sciences", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2013}, {"title": "Wikipedia information flow analysis reveals the scale-free architecture of the semantic space", "author": ["A.P. Masucci", "A. Kalampokis", "V.M. Eg\u00fa\u0131luz", "E. Hern\u00e1ndez-Gar\u0107\u0131a"], "venue": "PLoS ONE 6(2),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Discriminating word senses with tourist walks in complex networks", "author": ["T.C. Silva", "D.R. Amancio"], "venue": "The European Physical Journal B", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "Language clusters based on linguistic complex networks", "author": ["H. Liu", "W. Li"], "venue": "Chinese Sci. Bull", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2010}, {"title": "Automatic language classification by means of syntactic dependency networks", "author": ["O. Abramov", "A. Mehler"], "venue": "J. Quant. Linguist", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2011}, {"title": "Language clustering with word co-occurrence networks based on parallel texts", "author": ["H.T. Liu", "J. Cong"], "venue": "Chinese Sci. Bull", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2013}, {"title": "Approaching human language with complex networks", "author": ["Cong J", "H. Liu"], "venue": "Physics of Life Reviews", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2014}, {"title": "Patterns in syntactic dependency networks", "author": ["R. Ferrer i Cancho", "R.V. Sol\u00e9", "R. Kohler"], "venue": "Physical Review E", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2004}, {"title": "BioLemmatizer: a lemmatization tool for morphological processing of biomedical text", "author": ["H. Liu", "T. Christiansen", "W.A. Baumgartner", "K. Verspoor"], "venue": "Journal of Biomedical Semantics", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2012}, {"title": "A maximum entropy approach to natural language processing", "author": ["A.L. Berger", "V.J. Della Pietra", "S.A. Della Pietra"], "venue": "Comput. Linguist.,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1996}, {"title": "Accessibility in complex networks", "author": ["B.A.N. Traven\u00e7olo", "Costa", "L. da F"], "venue": "Phys. Lett. A", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2008}, {"title": "Identification of literary movements using complex networks to represent texts", "author": ["D.R. Amancio", "O.N. Oliveira Jr.", "Costa", "L. da F"], "venue": "New J. Phys", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2012}, {"title": "Shape classification and analysis: theory and practice. CRC Press, 2 edition", "author": ["Costa", "L. da F"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2014}, {"title": "Mixing patterns in networks", "author": ["M.E.J. Newman"], "venue": "Phys. Rev. E", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2003}, {"title": "Finding community structure in networks using the eigenvectors of matrices", "author": ["M.E.J. Newman"], "venue": "Physical Review E", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2006}, {"title": "On estimation of a probability density function and mode", "author": ["E. Parzen"], "venue": "The Annals of Mathematical Statistics", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 1962}, {"title": "A systematic comparison of supervised classifiers", "author": ["D.R. Amancio", "C.H. Comin", "D. Casanova", "G. Travieso", "O.M. Bruno", "F.A. Rodrigues", "Costa", "L. da F"], "venue": "PLOS ONE 9,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2014}, {"title": "Introduction to Algorithms, McGraw-Hill Higher Education", "author": ["T.H. Cormen", "C. Stein", "R.L. Rivest", "C.E. Leiserson"], "venue": null, "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2001}, {"title": "Pattern Classification (2nd Edition)", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": null, "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2000}, {"title": "Word sense disambiguation via high order of learning in complex networks", "author": ["T.C. Silva", "D.R. Amancio"], "venue": "EPL", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "For example, in recent years, the total number of articles, the number of citations motivated by articles or researchers\u2019 h-index has been widely used for the purpose of merit evaluation [1].", "startOffset": 187, "endOffset": 190}, {"referenceID": 1, "context": "Clearly, there is a correlation between quantitative and qualitative factors [2].", "startOffset": 77, "endOffset": 80}, {"referenceID": 2, "context": "Likewise, very young researchers also cannot be assessed according to the number of citations that their articles motivate, since there is an expected delay between paper publication date and wide scientific recognition [3\u20137].", "startOffset": 220, "endOffset": 225}, {"referenceID": 3, "context": "Likewise, very young researchers also cannot be assessed according to the number of citations that their articles motivate, since there is an expected delay between paper publication date and wide scientific recognition [3\u20137].", "startOffset": 220, "endOffset": 225}, {"referenceID": 4, "context": "Likewise, very young researchers also cannot be assessed according to the number of citations that their articles motivate, since there is an expected delay between paper publication date and wide scientific recognition [3\u20137].", "startOffset": 220, "endOffset": 225}, {"referenceID": 5, "context": "Likewise, very young researchers also cannot be assessed according to the number of citations that their articles motivate, since there is an expected delay between paper publication date and wide scientific recognition [3\u20137].", "startOffset": 220, "endOffset": 225}, {"referenceID": 6, "context": "Aware of the prevalence of quantitative indexes in scientific merit judgments, some scholars tend to shape their research only to increase their citation counts and other quantitative indexes [8].", "startOffset": 192, "endOffset": 195}, {"referenceID": 6, "context": "Other recurrent unethical conducts include the excessive use of self-citations [8,9] and plagiarisms [10\u201312].", "startOffset": 79, "endOffset": 84}, {"referenceID": 7, "context": "Other recurrent unethical conducts include the excessive use of self-citations [8,9] and plagiarisms [10\u201312].", "startOffset": 79, "endOffset": 84}, {"referenceID": 8, "context": "Other recurrent unethical conducts include the excessive use of self-citations [8,9] and plagiarisms [10\u201312].", "startOffset": 101, "endOffset": 108}, {"referenceID": 9, "context": "Other recurrent unethical conducts include the excessive use of self-citations [8,9] and plagiarisms [10\u201312].", "startOffset": 101, "endOffset": 108}, {"referenceID": 10, "context": "More recently, even texts automatically generated have been submitted and surprisingly deemed suitable for publication in several scientific conferences [13].", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "An incremental modification of the original algorithm has implemented the possibility of self-citations, which has allowed a significant increase in authors\u2019 h-index [15].", "startOffset": 166, "endOffset": 170}, {"referenceID": 12, "context": "More specifically, using a representation of texts as complex networks [16], I show that it is possible to discriminate real and fake manuscripts with significant accuracy if one analyzes the structural organization of the manuscripts.", "startOffset": 71, "endOffset": 75}, {"referenceID": 13, "context": "For example, it has been shown that when many cited references cannot be found online, then there is a high probability that the paper under analysis is fake [17].", "startOffset": 158, "endOffset": 162}, {"referenceID": 14, "context": "Many heuristics rely on textual content to infer the authenticity of documents [18, 19].", "startOffset": 79, "endOffset": 87}, {"referenceID": 15, "context": "Many heuristics rely on textual content to infer the authenticity of documents [18, 19].", "startOffset": 79, "endOffset": 87}, {"referenceID": 16, "context": "The study developed in [20] proposes some useful rules.", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "Techniques based on the semantic content of texts also employ traditional similarity measurements [21].", "startOffset": 98, "endOffset": 102}, {"referenceID": 18, "context": "An important contribution to the problem of identifying gibberish publications with similiraty measurements was introduced in [22].", "startOffset": 126, "endOffset": 130}, {"referenceID": 19, "context": "An extension of this work was proposed in [23], where not only single word occurrences are considered, but also multi-word phrases.", "startOffset": 42, "endOffset": 46}, {"referenceID": 20, "context": "Other interesting approaches relying on textual content include the techniques based on the compressibility rate of texts [24,25].", "startOffset": 122, "endOffset": 129}, {"referenceID": 21, "context": "Other interesting approaches relying on textual content include the techniques based on the compressibility rate of texts [24,25].", "startOffset": 122, "endOffset": 129}, {"referenceID": 21, "context": "In [25], the authors show that artificially generated papers display values of compressibility rate that are not compatible with the rates observed in real manuscripts.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "Complex networks have been employed to model a myriad or real complex systems [16].", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "Of particular interest to the aims of this study are the applications in automatic summarization [26, 27], machine translation [28, 29], complexity analysis [30\u201334] and authorship recognition [35,36].", "startOffset": 97, "endOffset": 105}, {"referenceID": 23, "context": "Of particular interest to the aims of this study are the applications in automatic summarization [26, 27], machine translation [28, 29], complexity analysis [30\u201334] and authorship recognition [35,36].", "startOffset": 97, "endOffset": 105}, {"referenceID": 24, "context": "Of particular interest to the aims of this study are the applications in automatic summarization [26, 27], machine translation [28, 29], complexity analysis [30\u201334] and authorship recognition [35,36].", "startOffset": 127, "endOffset": 135}, {"referenceID": 25, "context": "Of particular interest to the aims of this study are the applications in automatic summarization [26, 27], machine translation [28, 29], complexity analysis [30\u201334] and authorship recognition [35,36].", "startOffset": 127, "endOffset": 135}, {"referenceID": 26, "context": "Of particular interest to the aims of this study are the applications in automatic summarization [26, 27], machine translation [28, 29], complexity analysis [30\u201334] and authorship recognition [35,36].", "startOffset": 157, "endOffset": 164}, {"referenceID": 27, "context": "Of particular interest to the aims of this study are the applications in automatic summarization [26, 27], machine translation [28, 29], complexity analysis [30\u201334] and authorship recognition [35,36].", "startOffset": 157, "endOffset": 164}, {"referenceID": 28, "context": "Of particular interest to the aims of this study are the applications in automatic summarization [26, 27], machine translation [28, 29], complexity analysis [30\u201334] and authorship recognition [35,36].", "startOffset": 157, "endOffset": 164}, {"referenceID": 29, "context": "Of particular interest to the aims of this study are the applications in automatic summarization [26, 27], machine translation [28, 29], complexity analysis [30\u201334] and authorship recognition [35,36].", "startOffset": 157, "endOffset": 164}, {"referenceID": 30, "context": "Of particular interest to the aims of this study are the applications in automatic summarization [26, 27], machine translation [28, 29], complexity analysis [30\u201334] and authorship recognition [35,36].", "startOffset": 157, "endOffset": 164}, {"referenceID": 31, "context": "Of particular interest to the aims of this study are the applications in automatic summarization [26, 27], machine translation [28, 29], complexity analysis [30\u201334] and authorship recognition [35,36].", "startOffset": 192, "endOffset": 199}, {"referenceID": 32, "context": "Of particular interest to the aims of this study are the applications in automatic summarization [26, 27], machine translation [28, 29], complexity analysis [30\u201334] and authorship recognition [35,36].", "startOffset": 192, "endOffset": 199}, {"referenceID": 33, "context": "Networked models even allowed the characterization of unknown manuscripts [37] and many other linguistic aspects [38\u201342].", "startOffset": 74, "endOffset": 78}, {"referenceID": 34, "context": "Networked models even allowed the characterization of unknown manuscripts [37] and many other linguistic aspects [38\u201342].", "startOffset": 113, "endOffset": 120}, {"referenceID": 35, "context": "Networked models even allowed the characterization of unknown manuscripts [37] and many other linguistic aspects [38\u201342].", "startOffset": 113, "endOffset": 120}, {"referenceID": 36, "context": "Networked models even allowed the characterization of unknown manuscripts [37] and many other linguistic aspects [38\u201342].", "startOffset": 113, "endOffset": 120}, {"referenceID": 37, "context": "Networked models even allowed the characterization of unknown manuscripts [37] and many other linguistic aspects [38\u201342].", "startOffset": 113, "endOffset": 120}, {"referenceID": 38, "context": "Networked models even allowed the characterization of unknown manuscripts [37] and many other linguistic aspects [38\u201342].", "startOffset": 113, "endOffset": 120}, {"referenceID": 39, "context": "Interestingly, the particular features of each language could also be classified in terms of the topological structure of complex networks [43\u201345].", "startOffset": 139, "endOffset": 146}, {"referenceID": 40, "context": "Interestingly, the particular features of each language could also be classified in terms of the topological structure of complex networks [43\u201345].", "startOffset": 139, "endOffset": 146}, {"referenceID": 41, "context": "Interestingly, the particular features of each language could also be classified in terms of the topological structure of complex networks [43\u201345].", "startOffset": 139, "endOffset": 146}, {"referenceID": 42, "context": "A more detailed survey on the application of network methods in text analysis can be found in [46].", "startOffset": 94, "endOffset": 98}, {"referenceID": 31, "context": "Following previous studies, the style of each text is quantified via topological characterization of complex networks (graphs) [35\u201337, 47, 48].", "startOffset": 127, "endOffset": 142}, {"referenceID": 32, "context": "Following previous studies, the style of each text is quantified via topological characterization of complex networks (graphs) [35\u201337, 47, 48].", "startOffset": 127, "endOffset": 142}, {"referenceID": 33, "context": "Following previous studies, the style of each text is quantified via topological characterization of complex networks (graphs) [35\u201337, 47, 48].", "startOffset": 127, "endOffset": 142}, {"referenceID": 43, "context": "Following previous studies, the style of each text is quantified via topological characterization of complex networks (graphs) [35\u201337, 47, 48].", "startOffset": 127, "endOffset": 142}, {"referenceID": 44, "context": "In several stylebased applications, some pre-processing steps are usually applied before the connection of adjacent words [49].", "startOffset": 122, "endOffset": 126}, {"referenceID": 17, "context": "In (b), each remaining word is mapped to its canonical form [21].", "startOffset": 60, "endOffset": 64}, {"referenceID": 17, "context": "In order to obtain word lemmas [21], it is imperative to know in advance the part-of-speech of words.", "startOffset": 31, "endOffset": 35}, {"referenceID": 45, "context": "In this study, each word was labeled with its part-of-speech using a maximum entropy model [50].", "startOffset": 91, "endOffset": 95}, {"referenceID": 12, "context": "Currently, there exists several topological measurements [16].", "startOffset": 57, "endOffset": 61}, {"referenceID": 31, "context": "In textual applications, the clustering coefficient of specific words tends to quantify the number of distinct contexts in which the word appears [35].", "startOffset": 146, "endOffset": 150}, {"referenceID": 22, "context": "\u2013 Accessibility: the accessibility (or diversity) (\u03b1) is a extension of the degree that is based on both topology and dynamics of networks [26].", "startOffset": 139, "endOffset": 143}, {"referenceID": 46, "context": "This centrality index is relevant to identify topological high-degree nodes that effectively access only a few neighbors [51].", "startOffset": 121, "endOffset": 125}, {"referenceID": 46, "context": "This measurement has been employed to detect the border of complex networks [51].", "startOffset": 76, "endOffset": 80}, {"referenceID": 22, "context": "In textual applications, the accessibility has been useful to identify core concepts, allowing thus the construction of informative automatic summarizers [26].", "startOffset": 154, "endOffset": 158}, {"referenceID": 31, "context": "This measurement was employed because it has been useful in textual applications [35,52].", "startOffset": 81, "endOffset": 88}, {"referenceID": 47, "context": "This measurement was employed because it has been useful in textual applications [35,52].", "startOffset": 81, "endOffset": 88}, {"referenceID": 31, "context": "In word adjacency networks, this measurement has proven relevant to identify keywords, even if they are not very frequent [35].", "startOffset": 122, "endOffset": 126}, {"referenceID": 31, "context": "In textual networks, the betweenness quantifies the number of distinct contexts of a given word [35].", "startOffset": 96, "endOffset": 100}, {"referenceID": 31, "context": "Unlike the clustering coefficient, this measurement uses the global network connectivity to infer the number of contexts [35].", "startOffset": 121, "endOffset": 125}, {"referenceID": 49, "context": "\u2013 Assortativity: the assortativity (r) quantifies degree-degree correlations [54].", "startOffset": 77, "endOffset": 81}, {"referenceID": 50, "context": "In word adjacency networks, a disassortative behavior arises even when stopwords are removed from the analysis [55].", "startOffset": 111, "endOffset": 115}, {"referenceID": 51, "context": "method [56].", "startOffset": 7, "endOffset": 11}, {"referenceID": 52, "context": "In this paper, the value k = 1 was used since this value usually provides highest accuracy rates [57].", "startOffset": 97, "endOffset": 101}, {"referenceID": 53, "context": "\u2013 Decision trees (C45): this method uses a tree as a data structure to represent the emergent patterns of the dataset [58].", "startOffset": 118, "endOffset": 122}, {"referenceID": 54, "context": "More specifically, in this paper, I used the Kullback-Leibler divergence [59].", "startOffset": 73, "endOffset": 77}, {"referenceID": 33, "context": "Using the same strategy of previous studies [37, 52], the average \u3008X\u3009 and the deviation \u2206X of each local measurement X was extracted.", "startOffset": 44, "endOffset": 52}, {"referenceID": 47, "context": "Using the same strategy of previous studies [37, 52], the average \u3008X\u3009 and the deviation \u2206X of each local measurement X was extracted.", "startOffset": 44, "endOffset": 52}, {"referenceID": 48, "context": "the principal component analysis technique [53] is shown in Figure 6.", "startOffset": 43, "endOffset": 47}, {"referenceID": 38, "context": "Even though the techniques presented in this manuscript does not outperform the methods based on textual content, it could be employed in applications where the complementary nature of the proposed attributes plays a prominent role to discriminate pieces of texts with similar content [42,63].", "startOffset": 285, "endOffset": 292}, {"referenceID": 55, "context": "Even though the techniques presented in this manuscript does not outperform the methods based on textual content, it could be employed in applications where the complementary nature of the proposed attributes plays a prominent role to discriminate pieces of texts with similar content [42,63].", "startOffset": 285, "endOffset": 292}, {"referenceID": 52, "context": "Future works could pursue an improvement of performance with a fine tuning of classifiers parameters [57].", "startOffset": 101, "endOffset": 105}, {"referenceID": 17, "context": "Another possibility is to propose novel topological measurements to combine the techniques presented in this paper with traditional statistical natural language processing methods [21].", "startOffset": 180, "endOffset": 184}], "year": 2017, "abstractText": "Recent years have witnessed the increase of competition in science. While promoting the quality of research in many cases, an intense competition among scientists can also trigger unethical scientific behaviors. To increase the total number of published papers, some authors even resort to software tools that are able to produce grammatical, but meaningless scientific manuscripts. Because automatically generated papers can be misunderstood as real papers, it becomes of paramount importance to develop means to identify these scientific frauds. In this paper, I devise a methodology to distinguish real manuscripts from those generated with SCIGen, an automatic paper generator. Upon modeling texts as complex networks (CN), it was possible to discriminate real from fake papers with at least 89% of accuracy. A systematic analysis of features relevance revealed that the accessibility and betweenness were useful in particular cases, even though the relevance depended upon the dataset. The successful application of the methods described here show, as a proof of principle, that network features can be used to identify scientific gibberish papers. In addition, the CN-based approach can be combined in a straightforward fashion with traditional statistical language processing methods to improve the performance in identifying artificially generated papers.", "creator": "LaTeX with hyperref package"}}}