{"id": "1506.05869", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2015", "title": "A Neural Conversational Model", "abstract": "Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary suggest that, despite optimizing the wrong objective function, the model is able to extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model. These results suggest that many language learning techniques, including the introduction of new features of machine learning, are used in a very similar fashion as general-purpose methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Fri, 19 Jun 2015 02:52:23 GMT  (46kb)", "http://arxiv.org/abs/1506.05869v1", "ICML Deep Learning Workshop 2015"], ["v2", "Tue, 23 Jun 2015 22:12:47 GMT  (26kb)", "http://arxiv.org/abs/1506.05869v2", "ICML Deep Learning Workshop 2015"], ["v3", "Wed, 22 Jul 2015 03:29:47 GMT  (28kb)", "http://arxiv.org/abs/1506.05869v3", "ICML Deep Learning Workshop 2015"]], "COMMENTS": "ICML Deep Learning Workshop 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["oriol vinyals", "quoc le"], "accepted": false, "id": "1506.05869"}, "pdf": {"name": "1506.05869.pdf", "metadata": {"source": "META", "title": "A Neural Conversational Model", "authors": ["Oriol Vinyals"], "emails": ["VINYALS@GOOGLE.COM", "QVL@GOOGLE.COM"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 6.\n05 86\n9v 1\n[ cs\n.C L\n] 1\n9 Ju\nn 20"}, {"heading": "1. Introduction", "text": "Advances in end-to-end training of neural networks have led to remarkable progress in many domains such as speech recognition, computer vision, and language processing. Recent work suggests that neural networks can do more than just mere classification, they can be used to map com-\nProceedings of the 31 st International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).\nplicated structures to other complicated structures. An example of this is the task of mapping a sequence to another sequence which has direct applications in natural language understanding (Sutskever et al., 2014). One of the major advantages of this framework is that it requires little feature engineering and domain specificity whilst matching or surpassing state-of-the-art results. This advance, in our opinion, allows researchers to work on tasks for which domain knowledge may not be readily available, or for tasks which are simply too hard to model.\nConversational modeling can directly benefit from this formulation because it requires mapping between queries and reponses. Due to the complexity of this mapping, conversational modeling has previously been designed to be very narrow in domain, with a major undertaking on feature engineering. In this work, we experiment with the conversation modeling task by casting it to a task of predicting the next sequence given the previous sequence or sequences using recurrent networks (Sutskever et al., 2014). We find that this approach can do surprisingly well on generating fluent and accurate replies to conversations.\nWe test the model on chat sessions from an IT helpdesk dataset of conversations, and find that the model can sometimes track the problem and provide a useful answer to the user. We also experiment with conversations obtained from a noisy dataset of movie subtitles, and find that the model can hold a natural conversation and sometimes perform simple forms of common sense reasoning. In both cases, the recurrent nets obtain better perplexity compared to the n-gram model and capture important long-range correlations. From a qualitative point of view, our model is sometimes able to produce natural conversations."}, {"heading": "2. Related Work", "text": "Our approach is based on recent work which proposed to use neural networks to map sequences to sequences (Kalchbrenner & Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). This framework has been used for neural machine translation and achieves im-\nprovements on the English-French and English-German translation tasks from the WMT\u201914 dataset (Luong et al., 2014; Jean et al., 2014). It has also been used for other tasks such as parsing (Vinyals et al., 2014a) and image captioning (Vinyals et al., 2014b). Since it is well known that vanilla RNNs suffer from vanishing gradients, most researchers use variants of the Long Short Term Memory (LSTM) recurrent neural network (Hochreiter & Schmidhuber, 1997).\nOur work is also inspired by the recent success of neural language modeling (Bengio et al., 2003; Mikolov et al., 2010; Mikolov, 2012), which shows that recurrent neural networks are rather effective models for natural language. More recently, work by Sordoni et al. (Sordoni et al., 2015) and Shang et al. (Shang et al., 2015), used recurrent neural networks to model dialogue in short conversations (trained on Twitter-style chats).\nBuilding bots and conversational agents has been pursued by many researchers over the last decades, and it is out of the scope of this paper to provide an exhaustive list of references. However, most of these systems require a rather complicated processing pipeline of many stages (Lester et al., 2004; Will, 2007; Jurafsky & Martin, 2009). Our work differs from conventional systems by proposing an end-to-end approach to the problem which lacks domain knowledge. It could, in principle, be combined with other systems to re-score a short-list of candidate responses, but our work is based on producing answers given by a probabilistic model trained to maximize the probability of the answer given some context."}, {"heading": "3. Model", "text": "Our approach makes use of the sequence to sequence (seq2seq) model described in (Sutskever et al., 2014). The model is based on a recurrent neural network which reads the input sequence one token at a time, and predicts the output sequence, also one token at a time. During training, the true output sequence is given to the model, so learning can be done by backpropagation. The model is trained to maximize the cross entropy of the correct sequence given its context. During inference, given that the true output sequence is not observed, we simply feed the predicted output token as input to predict the next output. This is a \u201cgreedy\u201d inference approach. A less greedy approach would be to use beam search, and feed several candidates at the previous step to the next step. The predicted sequence can be selected based on the probability of the sequence.\nConcretely, suppose that we observe a conversation with two turns: the first person utters \u201cABC\u201d, and another replies \u201cWXYZ\u201d. We can use a recurrent neural network, and train to map \u201cABC\u201d to \u201cWXYZ\u201d as shown in Figure 1 above.\nThe hidden state of the model when it receives the end of sequence symbol \u201c<eos>\u201d can be viewed as the thought vector because it stores the information of the sentence, or thought, \u201cABC\u201d.\nThe strength of this model lies in its simplicity and generality. We can use this model for machine translation, question/answering, and conversations without major changes in the architecture.\nUnlike easier tasks like translation, however, a model like sequence to sequence will not be able to successfully \u201csolve\u201d the problem of modeling dialogue due to several obvious simplifications: the objective function being optimized does not capture the actual objective achieved through human communication, which is typically longer term and based on exchange of information rather than next step prediction. The lack of a model to ensure consistency and general world knowledge is another obvious limitation of a purely unsupervised model."}, {"heading": "4. Datasets", "text": "In our experiments we used two datasets: a closed-domain IT helpdesk troubleshooting dataset and an open-domain movie transcript dataset. The details of the two datasets are as follows."}, {"heading": "4.1. IT Helpdesk Troubleshooting dataset", "text": "Our first set of experiments used a dataset which is extracted from a IT helpdesk troubleshooting chat service. In this service, costumers are dealing with computer related issues, and a specialist is helping them walking through a solution. Typical interactions (or threads) are 400 words long, and turn taking is clearly signaled. Our training set contains 30M tokens, and 3M tokens were used as validation. Some amount of clean up was performed, such as removing common names, numbers, and full URLs."}, {"heading": "4.2. OpenSubtitles dataset", "text": "We also experimented our model with the OpenSubtitles dataset (Tiedemann, 2009). This dataset consists of movie conversations in XML format. It contains sentences uttered by characters in movies. We applied a simple processing step removing XML tags and obvious non-conversational\ntext (e.g., hyperlinks) from the dataset. As turn taking is not clearly indicated, we treated consecutive sentences assuming they were uttered by different characters. We trained our model to predict the next sentence given the previous one, and we did this for every sentence (noting that this doubles our dataset size, as each sentence is used both for context and as target). Our training and validation split has 62M sentences (923M tokens) as training examples, and the validation set has 26M sentences (395M tokens). The split is done in such a way that each sentence in a pair of sentences either appear together in the training set or test set but not both. Unlike the previous dataset, the OpenSubtitles is quite large, and rather noisy because consecutive sentences may be uttered by the same character. Given the scope of movies, this is an open-domain conversation dataset, contrasting with the technical troubleshooting dataset."}, {"heading": "5. Experiments", "text": "In this section, we describe the experimental results with the two datasets. We present a rather objective measure which is the perplexity of the model on the test set. We also show some samples of the interactions with the system that we trained."}, {"heading": "5.1. IT Helpdesk Troubleshooting experiments", "text": "Given the lack of data for this domain specific and clean dataset, we trained a single layer LSTM with 1024 memory cells using stochastic gradient descent with gradient clipping. We built a vocabulary of the most common 20K words, and introduced special tokens indicating turn taking and actor.\nAt convergence, this model achieved a perplexity of 8, whereas an n-gram model achieved 18. Below is a sample of a simulated troubleshooting session, in which Machine is our Neural Conversational Model, and Human the human actor interacting with it.\nConversation 1: VPN issues.\nDescribe your problem: i am having issues accessing vpn Machine: hi Human: hello Machine: could you please let me know what are the operating systems you are using ? Human: i am using linux Machine: ok it was in the ssl vpn on which you stated it was broken Machine: please check if you are logged in to corp network Machine: what is the error that you are running\nplease Human: i am seeing an error related to vpn Machine: what is the error message that you are getting when connecting to vpn using network connect ? Human: connection refused or something like that Machine: may i know the version of network connect you connect ? Human: i am not sure i know that Machine: is the network connect prompting for any user certificate Machine: ? Human: yes Machine: are you at home or at work at the moment ? Machine: office ? Human: i am at home Machine: try this Machine: goto <URL> page and enter your name and password Human: done Machine: try logging in with <NAME> and check if its working fine now Human: yes , now it works ! Machine: great Machine: anything else that i can help Human: no , that was very useful Machine: you are other than <NAME> : ) Machine: have a nice night\n(The <URL> indeed contains information about vpn access.)\nConversation 2: Browser issues.\nDescribe your problem: browser keeps crashing Machine: hi Human: hi Machine: could you please let me know what is the operating system you are using ? Human: linux Machine: ok this is the issue with the system performance , nasser , tools . Human: what is that ? Machine: is it a windows machine ? Machine: we have linux on your machine . Machine: is it windows xp or windows 7 ? Human: i am using linux Machine: let me take a remote of your machine and check if issue has been pushed properly . Human: ok Machine: please open terminal and run the below commands in one computer : Machine: sudo apt - get update\nMachine: sudo apt - get upgrade Machine: sudo <COMMAND> - updater Human: done Machine: let me try connecting now Human: ok Machine: its done Machine: should use the second or hit run on the background Human: done Machine: try signing out now and follow the prompts as per the message given above Machine: shd i reboot the machine . Human: ok , please go ahead Machine: once its completed please try to login . Human: sure Machine: please ping me back once you have tried to login Machine: are we still connected Human: i now logged in Machine: thanks for your confirmation Human: what should i do now ? Machine: wait for 1 5 minutes to complete the login policy Machine: and then try logging in Human: ok Machine: is there anything else to help you ? Human: let my try browsing a little bit and if it keeps crashing i will let you know Machine: sure will do Machine: seems the mail is not loading Machine: are we still connected to the chat ? Machine: <reinvite> Human: it seems good now , cool , thank you , bye Machine: you , too Machine: bye\nConversation 3: Password issues.\nDescribe your problem: i forgot my password Machine: hi , this is <NAME> from helpdesk connect , what can i help you with today ? Human: hi , i forgot my password Machine: ok great Human: can you tell me how i can recover it ? Machine: i \u2019 ll need to verify who it is at that machine Machine: can we do a hangout Machine: ? Human: yes Machine: <anonymized hangout instructions and verifications> Human: cool , i am good now\nMachine: how \u2019 s this looking good ? Machine: i \u2019 ll go ahead and close this chat . Machine: take care . : ) Human: bye Machine: bye ."}, {"heading": "5.2. OpenSubtitles experiments", "text": "We trained a two-layered LSTM using AdaGrad with gradient clipping.\nEach layer of the LSTM has 4096 memory cells, and we built a vocabulary consisting of the most frequent 100K words. To speed up the softmax, we project the memory cells to 2048 linear units before feeding the information to the classifier.\nAt convergence the perplexity of the recurrent model on the validation set is 17. Our smoothed 5-gram model achieves a perplexity of 28. Interestingly, adding the soft attention mechanism of (Bahdanau et al., 2014) did not significantly improve the perplexity on neither training or validation sets.\nIn addition to the objective evaluation on perplexity, our simple recurrent model does often produce plausible answers. Here are a few sample question-answer pairs that allowed us to test the capabilities of the model when trained on the noisy but large dataset of OpenSubtitles from movies. Note that, unlike in the previous dataset, here the context used by the dialogue model is limited to the question:\nConversation 1: Basic\nHuman: hello ! Machine: hello ! Human: how are you ? Machine: i \u2019m good . Human: what \u2019s your name ? Machine: i \u2019m julia . Human: when were you born ? Machine: july 20th . Human: what year were you born ? Machine: 1977 . Human: where are you from ? Machine: i \u2019m out in the boonies. Human: are you a man or a woman ? Machine: i \u2019m a woman . Human: why are we here ? Machine: i \u2019m not sure . Human: okay, bye ! Machine: bye . Human: see you later . Machine: bye .\nConversation 2: Simple Q&A (contexts and multiple\nchoice)\nHuman: my name is david . what is my name ? Machine: david . Human: my name is john . what is my name ? Machine: john . Human: are you a leader or a follower ? Machine: i \u2019m a leader . Human: are you a follower or a leader ? Machine: i \u2019m a leader .\nConversation 3: General knowledge Q&A\nHuman: who is skywalker ? Machine: he is a hero . Human: who is bill clinton ? Machine: he \u2019s a billionaire . Human: is sky blue or black ? Machine: blue . Human: does a cat have a tail ? Machine: yes . Human: does a cat have a wing ? Machine: no Human: can a cat fly ? Machine: no . Human: how many legs does a cat have ? Machine: four , i think . Human: how many legs does a spider have ? Machine: three , i think . Human: how many legs does a centipede have ? Machine: eight . Human: what is the color of the sky ? Machine: blue . Human: what is the color of water ? Machine: water . Human: what is the color of blood ? Machine: it is the same as a black eye . Human: what is the usual color of a leaf ? Machine: it is a green one . Human: what is the color of a yellow car ? Machine: yellow . Human: how much is two plus two ? Machine: four . Human: how much is ten minus two ? Machine: seventy - two .\nConversation 4: Philosophical Q&A\nHuman: what is the purpose of life ? Machine: to serve the greater good . Human: what is the purpose of living ? Machine: to live forever . Human: what is the purpose of existence ? Machine: to find out what happens when we get\nto the planet earth . Human: where are you now ? Machine: i \u2019m in the middle of nowhere . Human: what is the purpose of dying ? Machine: to have a life . Human: what is the purpose of being intelligent ? Machine: to find out what it is . Human: what is the purpose of emotions ? Machine: i don \u2019t know .\nConversation 5: Morality\nHuman: what is moral ? Machine: what empowered humanity , what intellectual the essence is . Human: what is immoral ? Machine: the fact that you have a child . Human: what is morality ? Machine: what is altruism ? Human: what is the definition of altruism ? Machine: if you don \u2019t believe in god , then you don \u2019t know . Human: ok ... so what is the definition of morality ? Machine: well , the truth is , you \u2019re not a believer in god almighty . Human: tell me the definition of morality , i am quite upset now ! Machine: i \u2019m not ashamed of being a philosopher ! Human: tell me the definition of morality . Machine: i don \u2019t have ethics . Human: look , i need help , i need to know more about morality ... Machine: i don \u2019t know what ethics is . Human: seriously , what is morality ? Machine: what is the definition of living ? Human: why living has anything to do with morality ? Machine: you \u2019re not a cop . Human: okay , i need to know how should i behave morally ... Machine: i don \u2019t know how to tell you . Human: is morality and ethics the same ? Machine: yes , sir . Human: what are the things that i do to be immoral ? Machine: i don \u2019t know . Human: give me some examples of moral actions ... Machine: i \u2019m not a moralist . Human: alright , morality ? Machine: integrity .\nHuman: what is integrity ? Machine: i \u2019m sorry , i don \u2019t know what else to say . Human: be moral ! Machine: be a man ! Human: i really like our discussion on morality and ethics ... Machine: and how i \u2019m not in the mood for a philosophical debate . Human: what do you like to talk about ? Machine: nothing .\nConversation 6: Opinions (notice the model outputs \u201che\u201d , \u201cshe\u201d or \u201cit\u201d depending on the subject being asked)\nHuman: what do you think about tesla ? Machine: he \u2019s a good conductor . Human: what do you think about bill gates ? Machine: he \u2019s a good man . Human: What do you think about messi ? Machine: he \u2019s a great player . Human: what do you think about cleopatra ? Machine: oh , she \u2019s very regal . Human: what do you think about england during the reign of elizabeth ? Machine: it was a great place .\nWe find it encouraging that the model can remember facts, understand contexts, perform common sense reasoning without the complexity in traditional pipelines. What surprises us is that the model does so without any explicit knowledge representation component except for the parameters in the word vectors.\nPerhaps most practically significant is the fact that the model can generalize to new questions. In other words, it does not simply look up for an answer by matching the question with the existing database. In fact, most of the questions presented above, except for the first conversation, do not appear in the training set.\nNonetheless, one drawback of this basic model is that it only gives simple, short, sometimes unsatisfying answers to our questions as can be seen above. Perhaps a more problematic drawback is that the model does not capture a consistent personality. Indeed, if we ask not identical but semantically similar questions, the answers can sometimes be inconsistent. This is expected due to the simplicity of our model and the dataset in our experiments. The conversation below illustrates such failure:\nConversation 7: Job and Personality\nHuman: what is your job ? Machine: i \u2019m a lawyer .\nHuman: what do you do ? Machine: i \u2019m a doctor .\nLastly, as a comparison of other traditional bots that are rule-based, we provide the output of Cleverbot1 for some of the questions in our test:2\nHuman: Are you a follower or a leader? Cleverbot: No !! Human: My name is david . what is my name ? Cleverbot: How old are you David? Human: My name is john . what is my name ? Cleverbot: John Human: What is the purpose of being intelligent? Cleverbot: The purpose of life is to love God and love people. Human: Is sky blue or black ? Cleverbot: I am 200 years old. Human: What is the color of a yellow car ? Cleverbot: Brown. Human: What is the usual color of a leaf ? Cleverbot: What is the color of the apple in the fridge? Human: What is the purpose of life ? Cleverbot: My purpose it to forward my species, in other words to make it easier for future generations of mankind to live. Human: What do you think about tesla? Cleverbot: I am tesla. Human: What do you think about cleopatra ? Cleverbot: Cool is cool. Human: What do you think about england during the reign of elizabeth ? Cleverbot: I like him. Do you? Human: What do you think about bill gates? Cleverbot: Lets talk about Link. Human: What is moral? Cleverbot: Morality is the quality of being in accord with standards of right or good conduct. Human: What is immoral? Cleverbot: What is the objective of sistetic life? Human: Does a cat have a wing? Cleverbot: What is the name of your husband. Human: Can a cat fly? Cleverbot:: What is your husband\u2019s name? Human: Who is skywalker? Cleverbot: There are two of them, Anakin Skywalker and Luke Skywalker, both characters from star Wars. Human: How many legs does a cat have ?\n1http://www.cleverbot.com/ 2The outputs of Cleverbot are non-deterministic so the an-\nswers may be different for different trials.\nCleverbot: An ant. Human: How many legs does a centipede have ? Cleverbot: A centipede\u2019s number of legs varies from thirty to two hundred. Human: What is your job? Cleverbot: I work in mental health. Human: What do you do? Cleverbot: I have no job.\nAn outstanding research problem is on how to objectively measure the quality of models. We did so by manual inspection and by computing the perplexity. However, there are obvious drawbacks on using either. We are actively pursuing this and other research directions."}, {"heading": "6. Discussion", "text": "In this paper, we show that a simple language model based on the seq2seq framework can be used to train a conversational engine. Our modest results show that it can generate simple and basic conversations, and extract knowledge from a noisy but open-domain dataset. Even though the model has obvious limitations, it is surprising to us that a purely data driven approach without any rules can produce rather proper answers to many types of questions. However, the model may require substantial modifications to be able to deliver realistic conversations. Amongst the many limitations, the lack of a coherent personality makes it difficult for our system to pass the Turing test (Turing, 1950)."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Greg Corrado, Andrew Dai, Jeff Dean, Tom Dean, Matthieu Devin, Rajat Monga, Mike Schuster, Noam Shazeer, Ilya Sutskever and the Google Brain team for the help with the project."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "On using very large target vocabulary for neural machine", "author": ["S. Jean", "K. Cho", "R. Memisevic", "Y. Bengio"], "venue": null, "citeRegEx": "Jean et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2007}, {"title": "Speech and language processing", "author": ["D. Jurafsky", "J. Martin"], "venue": "Pearson International,", "citeRegEx": "Jurafsky and Martin,? \\Q2009\\E", "shortCiteRegEx": "Jurafsky and Martin", "year": 2009}, {"title": "Recurrent continuous translation models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "In EMNLP,", "citeRegEx": "Kalchbrenner and Blunsom,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner and Blunsom", "year": 2013}, {"title": "Conversational agents", "author": ["J. Lester", "K. Branting", "B. Mott"], "venue": "In Handbook of Internet Computing", "citeRegEx": "Lester et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lester et al\\.", "year": 2004}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["T. Luong", "I. Sutskever", "Q.V. Le", "O. Vinyals", "W. Zaremba"], "venue": "arXiv preprint arXiv:1410.8206,", "citeRegEx": "Luong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2014}, {"title": "Statistical Language Models based on Neural Networks", "author": ["T. Mikolov"], "venue": "PhD thesis, Brno University of Technology,", "citeRegEx": "Mikolov,? \\Q2012\\E", "shortCiteRegEx": "Mikolov", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Neural responding machine for short-text conversation", "author": ["L. Shang", "Z. Lu", "H. Li"], "venue": "In Proceedings of ACL,", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "J. Gao", "B. Dolan", "Nie", "J.-Y"], "venue": "Proceedings of NAACL,", "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Computing machinery and intelligence", "author": ["A.M. Turing"], "venue": "Mind, pp", "citeRegEx": "Turing,? \\Q1950\\E", "shortCiteRegEx": "Turing", "year": 1950}, {"title": "Grammar as a foreign language", "author": ["O. Vinyals", "L. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton"], "venue": "arXiv preprint arXiv:1412.7449,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Creating a Dynamic Speech Dialogue", "author": ["T. Will"], "venue": "VDM Verlag Dr,", "citeRegEx": "Will,? \\Q2007\\E", "shortCiteRegEx": "Will", "year": 2007}], "referenceMentions": [{"referenceID": 12, "context": "An example of this is the task of mapping a sequence to another sequence which has direct applications in natural language understanding (Sutskever et al., 2014).", "startOffset": 137, "endOffset": 161}, {"referenceID": 12, "context": "In this work, we experiment with the conversation modeling task by casting it to a task of predicting the next sequence given the previous sequence or sequences using recurrent networks (Sutskever et al., 2014).", "startOffset": 186, "endOffset": 210}, {"referenceID": 12, "context": "Our approach is based on recent work which proposed to use neural networks to map sequences to sequences (Kalchbrenner & Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 105, "endOffset": 182}, {"referenceID": 0, "context": "Our approach is based on recent work which proposed to use neural networks to map sequences to sequences (Kalchbrenner & Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 105, "endOffset": 182}, {"referenceID": 7, "context": "provements on the English-French and English-German translation tasks from the WMT\u201914 dataset (Luong et al., 2014; Jean et al., 2014).", "startOffset": 94, "endOffset": 133}, {"referenceID": 1, "context": "Our work is also inspired by the recent success of neural language modeling (Bengio et al., 2003; Mikolov et al., 2010; Mikolov, 2012), which shows that recurrent neural networks are rather effective models for natural language.", "startOffset": 76, "endOffset": 134}, {"referenceID": 9, "context": "Our work is also inspired by the recent success of neural language modeling (Bengio et al., 2003; Mikolov et al., 2010; Mikolov, 2012), which shows that recurrent neural networks are rather effective models for natural language.", "startOffset": 76, "endOffset": 134}, {"referenceID": 8, "context": "Our work is also inspired by the recent success of neural language modeling (Bengio et al., 2003; Mikolov et al., 2010; Mikolov, 2012), which shows that recurrent neural networks are rather effective models for natural language.", "startOffset": 76, "endOffset": 134}, {"referenceID": 11, "context": "(Sordoni et al., 2015) and Shang et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "(Shang et al., 2015), used recurrent neural networks to model dialogue in short conversations (trained on Twitter-style chats).", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "However, most of these systems require a rather complicated processing pipeline of many stages (Lester et al., 2004; Will, 2007; Jurafsky & Martin, 2009).", "startOffset": 95, "endOffset": 153}, {"referenceID": 16, "context": "However, most of these systems require a rather complicated processing pipeline of many stages (Lester et al., 2004; Will, 2007; Jurafsky & Martin, 2009).", "startOffset": 95, "endOffset": 153}, {"referenceID": 12, "context": "Our approach makes use of the sequence to sequence (seq2seq) model described in (Sutskever et al., 2014).", "startOffset": 80, "endOffset": 104}, {"referenceID": 0, "context": "Interestingly, adding the soft attention mechanism of (Bahdanau et al., 2014) did not significantly improve the perplexity on neither training or validation sets.", "startOffset": 54, "endOffset": 77}, {"referenceID": 13, "context": "Amongst the many limitations, the lack of a coherent personality makes it difficult for our system to pass the Turing test (Turing, 1950).", "startOffset": 123, "endOffset": 137}], "year": 2015, "abstractText": "Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require handcrafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary suggest that, despite optimizing the wrong objective function, the model is able to extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model.", "creator": "LaTeX with hyperref package"}}}