{"id": "1506.06628", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2015", "title": "Modality-dependent Cross-media Retrieval", "abstract": "In this paper, we investigate the cross-media retrieval between images and text, i.e., using image to search text (I2T) and using text to search images (T2I) for the number of text pages in each group. The study subjects were scanned from one set of 4 4, 4, 5, 6, 7, 8, and 9 of the four-group-trial test (1). We tested the cross-media retrieval in 6- and 10-month-olds for the two-group-trial test, between 1,100 and 3,500 text pages, respectively, in terms of the time between the two trials (a) and 2,200-page-old text page, and 2,400-page-old text page. We analyzed the cross-media retrieval in both groups of subjects, and analyzed the different images to locate the cross-media retrieval.", "histories": [["v1", "Mon, 22 Jun 2015 14:33:39 GMT  (821kb,D)", "https://arxiv.org/abs/1506.06628v1", "in ACM Transactions on Intelligent Systems and Technology"], ["v2", "Tue, 23 Jun 2015 01:34:01 GMT  (889kb,D)", "http://arxiv.org/abs/1506.06628v2", "in ACM Transactions on Intelligent Systems and Technology"]], "COMMENTS": "in ACM Transactions on Intelligent Systems and Technology", "reviews": [], "SUBJECTS": "cs.CV cs.IR cs.LG", "authors": ["yunchao wei", "yao zhao", "zhenfeng zhu", "shikui wei", "yanhui xiao", "jiashi feng", "shuicheng yan"], "accepted": false, "id": "1506.06628"}, "pdf": {"name": "1506.06628.pdf", "metadata": {"source": "CRF", "title": "A Modality-dependent Cross-media Retrieval", "authors": ["YUNCHAO WEI", "YAO ZHAO", "ZHENFENG ZHU", "SHIKUI WEI", "YANHUI XIAO", "JIASHI FENG", "SHUICHENG YAN"], "emails": ["wychao1987@gmail.com,", "shkwei}@bjtu.edu.cn,", "xiaoyanhui@gmail.com.", "jshfeng@gmail.com.", "eleyans@nus.edu.sg."], "sections": [{"heading": null, "text": "A Modality-dependent Cross-media Retrieval\nYUNCHAO WEI, Beijing Jiaotong University YAO ZHAO, Beijing Jiaotong University ZHENFENG ZHU, Beijing Jiaotong University SHIKUI WEI, Beijing Jiaotong University YANHUI XIAO, Beijing Jiaotong University JIASHI FENG, University of California, Berkeley SHUICHENG YAN, National University of Singapore\nIn this paper, we investigate the cross-media retrieval between images and text, i.e., using image to search text (I2T) and using text to search images (T2I). Existing cross-media retrieval methods usually learn one couple of projections, by which the original features of images and text can be projected into a common latent space to measure the content similarity. However, using the same projections for the two different retrieval tasks (I2T and T2I) may lead to a tradeoff between their respective performances, rather than their best performances. Different from previous works, we propose a modality-dependent cross-media retrieval (MDCR) model, where two couples of projections are learned for different cross-media retrieval tasks instead of one couple of projections. Specifically, by jointly optimizing the correlation between images and text and the linear regression from one modal space (image or text) to the semantic space, two couples of mappings are learned to project images and text from their original feature spaces into two common latent subspaces (one for I2T and the other for T2I). Extensive experiments show the superiority of the proposed MDCR compared with other methods. In particular, based the 4,096 dimensional convolutional neural network (CNN) visual feature and 100 dimensional LDA textual feature, the mAP of the proposed method achieves 41.5%, which is a new state-of-the-art performance on the Wikipedia dataset.\nCategories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models\nGeneral Terms: Design, Algorithms, Performance\nAdditional Key Words and Phrases: cross-media retrieval, subspace learning, canonical correlation analysis\nACM Reference Format: ACM Trans. Intell. Syst. Technol. V, N, Article A (January YYYY), 13 pages. DOI:http://dx.doi.org/10.1145/0000000.0000000\nThis work is supported in part by National Basic Research Program of China (No.2012CB316400) and Fundamental Scientific Research Project (No.K15JB00360). Authors\u2019 addresses: Yunchao Wei, Yao Zhao, Zhenfeng Zhu, Shikui Wei and Yanhui Xiao are with the Institute of Information Science, Beijing Jiaotong University, Beijing 100044, China, and with the Beijing Key Laboratory of Advanced Information Science and Network Technology, Beijing 100044, China; email:wychao1987@gmail.com, {yzhao, zhfzhu, shkwei}@bjtu.edu.cn, xiaoyanhui@gmail.com. Shikui Wei is also with Hubei Key Laboratory of Intelligent Vision Based Monitoring for Hydroelectric Engineering, China Three Gorges University, Yichang, Hubei 443002, China. Jiashi Feng is with Department of Electrical Engineering and Computer Sciences, University of California, Berkeley; email:jshfeng@gmail.com. Shuicheng Yan is with Department of Electrical and Computer Engineering, National University of Singapore; eleyans@nus.edu.sg. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212) 869-0481, or permissions@acm.org. c\u00a9 YYYY ACM 2157-6904/YYYY/01-ARTA $15.00 DOI:http://dx.doi.org/10.1145/0000000.0000000\nACM Transactions on Intelligent Systems and Technology, Vol. V, No. N, Article A, Publication date: January YYYY.\nar X\niv :1\n50 6.\n06 62\n8v 2\n[ cs\n.C V\n] 2\n3 Ju\nn 20\n15"}, {"heading": "1. INTRODUCTION", "text": "With the rapid development of information technology, multi-modal data (e.g., image, text, video or audio) have been widely available on the Internet. For example, an image often co-occurs with text on a web page to describe the same object or event. Related research has been conducted incrementally in recent decades, among which the retrieval across different modalities has attracted much attention and benefited many practical applications. However, multi-modal data usually span different feature spaces. This heterogeneous characteristic poses a great challenge to cross-media retrieval tasks. In this work, we mainly focus on addressing the cross-media retrieval between text and images (Fig. 1), i.e., using image (text) to search text documents (images) with the similar semantics.\nTo address this issue, many approaches have been proposed by learning a common representation for the data of different modalities. We observe that most exiting works [Hardoon et al. 2004; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013] focus on learning one couple of mapping matrices to project high-dimensional features from different modalities into a common latent space. By doing this, the correlations of two variables from different modalities can be maximized in the learned common latent subspace. However, only considering pair-wise closeness [Hardoon et al. 2004] is not sufficient for cross-media retrieval tasks, since it is required that multi-modal data from the same semantics should be united in the common latent subspace. Although [Sharma et al. 2012] and [Gong et al. 2013] have proposed to use supervised information to cluster the multi-modal data with the same semantics, learning one couple of projections may only lead to compromised results for each retrieval task.\nIn this paper, we propose a modality-dependent cross-media retrieval (MDCR) method, which recommends different treatments for different retrieval tasks, i.e., I2T and T2I. Specifically, MDCR is a task-specific method, which learns two couples of projections for different retrieval tasks. The proposed method is illustrated in Fig. 2. Fig. 2(a) and Fig. 2(c) are two linear regression operations from the image and the text feature space to the semantic space, respectively. By doing this, multi-modal data with the same semantics can be united in the common latent subspace. Fig. 2(b) is a correla-\nACM Transactions on Intelligent Systems and Technology, Vol. V, No. N, Article A, Publication date: January YYYY.\ntion analysis operation to keep pair-wise closeness of multi-modal data in the common space. We combine Fig. 2(a) and Fig. 2(b) to learn a couple of projections for I2T, and a different couple of projections for T2I is jointly optimized by Fig. 2(b) and Fig. 2(c). The reason why we learn two couples of projections rather than one couple for different retrieval tasks can be explained as follows. For I2T, we argue that the accurate representation of the query (i.e., the image) in the semantic space is more important than that of the text to be retrieved. If the semantics of the query is misjudged, it will be even harder to retrieve the relevant text. Therefore, only the linear regression term from image feature to semantic label vector and the correlation analysis term are considered for optimizing the mapping matrices for I2T. For T2T, the reason is the same as that for I2T. The main contributions of this work are listed as follow:\n\u2022We propose a modality-dependent cross-media retrieval method, which projects data of different modalities into a common space so that similarity measurement such as Euclidean distance could be applied for cross-media retrieval. \u2022To better validate the effectiveness of our proposed MDCR, we compare it with other\nstate-of-the-arts based on more powerful feature representations. In particular, with the 4,096 dimensional CNN visual feature and 100 dimensional LDA textual feature, the mAP of the proposed method reaches 41.5%, which is a new state-of-the-art performance on the Wikipedia dataset as far as we know. \u2022Based on the INRIA-Websearch dataset [Krapac et al. 2010], we construct a new\ndataset for cross-media retrieval evaluation. In addition, all the features utilized in this paper are publicly available1.\nThe remainder of this paper in organized as follows. We briefly review the related work of cross-media retrieval in Section 2. In Section 3, the proposed modalitydependent cross-media retrieval method is described in detail. Then in Section 4, experimental results are reported and analyzed. Finally, Section 5 presents the conclusions."}, {"heading": "2. RELATED WORK", "text": "During the past few years, numerous methods have been proposed to address crossmedia retrieval. Some works [Hardoon et al. 2004; Tenenbaum and Freeman 2000; Rosipal and Kra\u0308mer 2006; Yang et al. 2008; Sharma and Jacobs 2011; Hwang and Grauman 2010; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013; Wei et al. 2014; Zhang et al. 2014] try to learn an optimal common latent subspace for multi-modal data. This kind of methods projects representations of multiple modalities into an isomorphic space, such that similarity measurement can be directly applied between multi-modal data. Two popular approaches, Canonical Correlation Analysis (CCA) [Hardoon et al. 2004] and Partial Least Squares (PLS) [Rosipal and Kra\u0308mer 2006; Sharma and Jacobs 2011], are usually employed to find a couple of mappings to maximize the correlations between two variables. Based on CCA, a number of successful algorithms have been developed for cross-media retrieval tasks [Rashtchian et al. 2010; Hwang and Grauman 2010; Sharma et al. 2012; Gong et al. 2013]. The work [Rashtchian et al. 2010] investigated the cross-media retrieval problem in terms of correlation hypothesis and abstraction hypothesis. Based on the isomorphic feature space obtained from CCA, a multi-class logistic regression is applied to generate a common semantic space for cross-media retrieval tasks. In [Hwang and Grauman 2010], Hwang et al. used KCCA to develop a cross-media retrieval method by modeling the correlation between visual features and textual features. The work [Sharma\n1https://sites.google.com/site/yunchaosite/mdcr\nACM Transactions on Intelligent Systems and Technology, Vol. V, No. N, Article A, Publication date: January YYYY.\net al. 2012] presented a generic framework for multi-modal feature extraction techniques, called Generalized Multiview Analysis (GMA). More recently, the work [Gong et al. 2013] proposed a three-view CCA model by introducing a semantic view to produce a better separation for multi-modal data of different classes in the learned latent subspace.\nTo address the problem of prohibitively expensive nearest neighbor search, some hashing-based approaches [Kumar and Udupa 2011; Wu et al. 2014] to large scale similarity search have drawn much interest from the cross-media retrieval community. In particular, [Kumar and Udupa 2011] proposed a cross view hashing method to generate hash codes by minimizing the distance of hash codes for the similar data and maximizing the distance for the dissimilar data. Recently, [Wu et al. 2014] proposed a sparse multi-modal hashing method, which can obtain sparse codes for the data across different modalities via joint multi-modal dictionary learning, to address cross-modal retrieval. Besides, with the development of deep learning, some deep models [Frome et al. 2013; Wang et al. 2014; Lu et al. 2014; Zhuang et al. 2014] have also been proposed to address cross-media problems. Specifically, [Frome et al. 2013] presented a deep visual-semantic embedding model to identify visual objects using both labeled image data and semantic information obtained from unannotated text documents. [Wang\nACM Transactions on Intelligent Systems and Technology, Vol. V, No. N, Article A, Publication date: January YYYY.\net al. 2014] proposed an effective mapping mechanism, which can capture both intramodal and inter-modal semantic relationships of multi-modal data from heterogeneous sources, based on the stacked auto-encoders deep model.\nBeyond the above mentioned models, some other works [Yang et al. 2009; Yang et al. 2010; Yang et al. 2012; Wu et al. 2013; Zhai et al. 2013; Kang et al. 2014] have also been proposed to address cross-media problems. In particular, [Wu et al. 2013] presented a bi-directional cross-media semantic representation model by optimizing the bi-directional list-wise ranking loss with a latent space embedding. In [Zhai et al. 2013], both the intra-media and the inter-media correlation are explored for crossmedia retrieval. Most recently, [Kang et al. 2014] presented a heterogeneous similarity learning approach based on metric learning for cross-media retrieval. With the convolutional neural network (CNN) visual feature, some new state-of-the-art cross-media retrieval results have been achieved in [Kang et al. 2014]."}, {"heading": "3. MODALITY-DEPENDENT CROSS-MEDIA RETRIEVAL", "text": "In this section, we detail the proposed supervised cross-media retrieval method, which we call modality-dependent cross-media retrieval (MDCR). Each pair of image and text in the training set is accompanied with semantic information (e.g., class labels). Different from [Gong et al. 2013] which incorporates the semantic information as a third view, in this paper, semantic information is employed to determine a common latent space with a fixed dimension where samples with the same label can be clustered.\nSuppose we are given a dataset of n data instances, i.e., G = {(xi, ti)}ni=1, where xi \u2208 Rp and ti \u2208 Rq are original low-level features of image and text document, respectively. Let X = [x1, ...,xn]T \u2208 Rn\u00d7p be the feature matrix of image data, and T = [t1, ..., tn]T \u2208 Rn\u00d7q be the feature matrix of text data. Assume that there are c classes in G. S = [s1, ..., sn]\nT \u2208 Rn\u00d7c is the semantic matrix with the ith row being the semantic vector corresponding to xi and ti. In particular, we set the jth element of si as 1, if xi and ti belong to the jth class. Definition 1: The cross-media retrieval problem is to learn two optimal mapping matrices V \u2208 Rc\u00d7p and W \u2208 Rc\u00d7q from the multi-modal dataset G, which can be formally formulated into the following optimization framework:\nmin V,W\nf(V,W ) = C(V,W ) + L(V,W ) +R(V,W ), (1)\nwhere f is the objective function consisting of three terms. In particular, C(V,W ) is a correlation analysis term used to keep pair-wise closeness of multi-modal data in the common latent subspace. L(V,W ) is a linear regression term from one modal feature space (image or text) to the semantic space, used to centralize the multi-modal data with the same semantics in the common latent subspace.R(V,W ) is the regularization term to control the complexity of the mapping matrices V and W .\nIn the following subsections, we will detail the two algorithms for I2T and T2I based on the optimization framework Eq.(1)."}, {"heading": "3.1. Algorithm for I2T", "text": "This section addresses the cross-media retrieval problem of using an image to retrieve its related text documents. Denote the two optimal mapping matrices for images and text as V1 \u2208 Rc\u00d7p and W1 \u2208 Rc\u00d7q, respectively. Based on the optimization framework Eq.(1), the objective function of I2T is defined as follows:\nmin V1,W1\nf (V1,W1) =\u03bb \u2225\u2225XV T1 \u2212 TWT1 \u2225\u22252F + (1\u2212 \u03bb)\u2225\u2225XV T1 \u2212 S\u2225\u22252F\n+R (V1,W1) , (2)\nACM Transactions on Intelligent Systems and Technology, Vol. V, No. N, Article A, Publication date: January YYYY.\nwhere 0 \u2264 \u03bb \u2264 1 is a tradeoff parameter to balance the importance of the correlation analysis term and the linear regression term, \u2016 \u00b7 \u2016F denotes the Frobenius norm of the matrix, and R (V1,W1) is the regularization function used to regularize the mapping matrices. In this paper, the regularization function is defined as:\nR (V1,W1) = \u03b71 \u2016V1\u20162F + \u03b72 \u2016W1\u2016 2 F ,\nwhere \u03b71 and \u03b72 are nonnegative parameters to balance these two regularization terms."}, {"heading": "3.2. Algorithm for T2I", "text": "This section addresses the cross-media retrieval problem of using text to retrieve its related images. Different from the objective function of I2T, the linear regression term for T2I is a regression operation from the textual space to the semantic space. Denote the two optimal mapping matrices for images and text in T2I as V2 \u2208 Rc\u00d7p and W2 \u2208 Rc\u00d7q, respectively. Based on the optimization framework Eq.(1), the objective function of T2I is defined as follows:\nmin V2,W2\nf (V2,W2) =\u03bb \u2225\u2225XV T2 \u2212 TWT2 \u2225\u22252F + (1\u2212 \u03bb)\u2225\u2225TWT2 \u2212 S\u2225\u22252F\n+R (V2,W2) , (3)\nwhere the setting of the tradeoff parameter \u03bb and the regularization function R (V2,W2) are consistent with the setting presented in Section 3.1."}, {"heading": "3.3. Optimization", "text": "The optimization problems for I2T and T2I are unconstrained optimization with respect to two matrices. Hence, both Eq.(2) and Eq.(3) are non-convex optimization problems and only have many local optimal solutions. For the non-convex problem, we usually design algorithms to seek stationary points. We note that Eq.(2) is convex with respect to either V1 or W1 while fixing the other. Similarly, Eq.(3) is also convex with respect to either V2 or W2 while fixing the other. Specifically, by fixing V1(V2) or W1(W2), the minimization over the other can be finished with the gradient descent method.\nThe partial derivatives of V1 or W1 over Eq.(2) are given as follows: \u2207V1f (V1,W1) = V1XTX + 2 [ \u03b71V1 \u2212 \u03bbW1TTX \u2212 (1\u2212 \u03bb)STX ] , (4)\n\u2207W1f (V1,W1) = 2 [ \u03b72W1 + \u03bb ( W1T TT \u2212 V1XTT )] . (5)\nSimilarly, the partial derivatives of V2 or W2 over Eq.(3) are given as follows: \u2207V2f (V2,W2) = 2 [ \u03b71V2 + \u03bb ( V2X TX \u2212W2TTX )] , (6)\n\u2207W2f (V2,W2) =WTTT + 2 [ \u03b72W2 \u2212 \u03bbV2XTT \u2212 (1\u2212 \u03bb)STT ] . (7)\nA common way to solve this kind of optimization problems is an alternating updating process until the result converges. Algorithm 1 summarizes the optimization procedure of the proposed MDCR method for I2T, which can be easily extended for T2I."}, {"heading": "4. EXPERIMENTAL RESULTS", "text": "To evaluate the proposed MDCR algorithm, we systematically compare it with other state-of-the-art methods on three datasets, i.e., Wikipedia [Rasiwasia et al. 2010], Pascal Sentence [Rashtchian et al. 2010] and a subset of INRIA-Websearch [Krapac et al. 2010].\nACM Transactions on Intelligent Systems and Technology, Vol. V, No. N, Article A, Publication date: January YYYY.\nALGORITHM 1: Optimization for Modality-dependent Cross-media Retrieval Input: The feature matrix of image data X = [x1, ...,xn]T \u2208 Rn\u00d7p, the feature matrix of text\ndata T = [t1, ..., tn]T \u2208 Rn\u00d7q, the semantic matrix corresponding to images and text S = [s1, ..., sn]\nT \u2208 Rn\u00d7c. Initialize V (\u03c5)1 , W (\u03c9) 1 , \u03c5 \u21900 and \u03c9 \u21900. Set the parameters \u03bb, \u03b71, \u03b72, \u00b5 and . \u00b5 is the step size in the alternating updating process and is the convergence condition. repeat\nAlternative optimization process for I2T (Algorithm 2). until Convergence or maximum iteration number achieves.; Output: V (\u03c5)1 , W (\u03c9) 1 .\nALGORITHM 2: Alternative Optimization Process for I2T repeat\nSet value1 = f ( V\n(\u03c5) 1 ,W (\u03c9) 1\n) ;\nUpdate V (\u03c5+1)1 = V (\u03c5) 1 \u2212 \u00b5\u2207V (\u03c5)1\nf ( V\n(\u03c5) 1 ,W (\u03c9) 1\n) ;\nSet value2 = f ( V\n(\u03c5+1) 1 ,W (\u03c9) 1\n) , \u03c5 \u2190 \u03c5 + 1;\nuntil value1\u2212 value2 \u2264 ; repeat Set value1 = f ( V\n(\u03c5) 1 ,W (\u03c9) 1\n) ;\nUpdate W (\u03c9+1)1 =W (\u03c9) 1 \u2212 \u00b5\u2207W (\u03c9)1\nf ( V\n(\u03c5) 1 ,W (\u03c9) 1\n) ;\nSet value2 = f ( V\n(\u03c5) 1 ,W (\u03c9+1) 1\n) , \u03c9 \u2190 \u03c9 + 1;\nuntil value1\u2212 value2 \u2264 ;"}, {"heading": "4.1. Datasets", "text": "Wikipedia2: This dataset contains totally 2,866 image-text pairs from 10 categories. The whole dataset is randomly split into a training set and a test set with 2,173 and 693 pairs. We utilize the publicly available features provided by [Rasiwasia et al. 2010] i.e., 128 dimensional SIFT BoVW for images and 10 dimensional LDA for text, to compare directly with existing results. Besides, we also present the cross-media retrieval results based on the 4,096 dimensional CNN visual features3 and the 100 dimensional Latent Dirichlet Allocation model (LDA) [Blei et al. 2003] textual features (we firstly obtain the textual feature vector based on 500 tokens and then LDA model is used to compute the probability of each document under 100 topics). Pascal Sentence4: This dataset contains 1,000 pairs of image and text descriptions from 20 categories (50 for each category). We randomly select 30 pairs from each category as the training set and the rest are taken as the testing set. We utilize the 4,096 dimensional CNN visual feature for image representation. For textual features, we firstly extract the feature vector based on 300 most frequent tokens (with stop words removed) and then utilize the LDA to compute the probability of each document under 100 topics. The 100 dimensional probability vector is used for textual representation.\n2http://www.svcl.ucsd.edu/projects/crossmodal/ 3The CNN model is pre-trained on ImageNet. We utilize the outputs from the second fully-connected layer as the CNN visual feature in this paper. For more details, please refer to [Krizhevsky et al. 2012]. 4http://vision.cs.uiuc.edu/pascal-sentences/\nACM Transactions on Intelligent Systems and Technology, Vol. V, No. N, Article A, Publication date: January YYYY.\nINRIA-Websearch: This dataset contains 71,478 pairs of image and text annotations from 353 categories. We remove those pairs which are marked as irrelevant, and select those pairs that belong to any one of the 100 largest categories. Then, we get a subset of 14,698 pairs for evaluation. We randomly select 70% pairs from each category as the training set (10,332 pairs), and the rest are treated as the testing set (4,366 pairs). We utilize the 4,096 dimensional CNN visual feature for image representation. For textual features, we firstly obtain the feature vector based on 25,000 most frequent tokens (with stop words removed) and then employ the LDA to compute the probability of each document under 1,000 topics.\nFor semantic representation, the ground-truth labels of each dataset are employed to construct semantic vectors (10 dimensions for Wikipedia dataset, 20 dimensions for Pascal Sentence dataset, and 100 dimensions for INRIA-Websearch dataset) for pairs of image and text."}, {"heading": "4.2. Experimental Settings", "text": "In the experiment, Euclidean distance is used to measure the similarity between features in the embedding latent subspace. Retrieval performance is evaluated by mean average precision (mAP), which is one of the standard information retrieval metrics. Specifically, given a set of queries, the average precision (AP) of each query is defined as:\nAP = \u2211R k=1 P (k)rel(k)\u2211R\nk=1 rel(k) ,\nwhere R is the size of the test dataset. rel(k) = 1 if the item at rank k is relevant, rel(k) = 0 otherwise. P (k) denotes the precision of the result ranked at k. We can get the mAP score by averaging AP for all queries."}, {"heading": "4.3. Results", "text": "In the experiments, we mainly compare the proposed MDCR with six algorithms, including CCA, Semantic Matching (SM) [Rasiwasia et al. 2010], Semantic Correlation Matching (SCM) [Rasiwasia et al. 2010], Three-View CCA (T-V CCA) [Gong et al. 2013], Generalized Multiview Marginal Fisher Analysis (GMMFA) [Sharma et al. 2012] and Generalized Multiview Linear Discriminant Analysis (GMLDA) [Sharma et al. 2012].\nFor the Wikipedia dataset, we firstly compare the proposed MDCR with other methods based on the publicly available features [Rasiwasia et al. 2010], i.e., 128-SIFT BoVW for images and 10-LDA for text. We fix \u00b5 = 0.02 and = 10\u22124, and experimentally set \u03bb = 0.1, \u03b71 = 0.5 and \u03b72 = 0.5 for the optimization of I2T, and the parameters for T2I are set as \u03bb = 0.5, \u03b71 = 0.5 and \u03b72 = 0.5. The mAP scores for each method are shown in Table I. It can be seen that our method is more effective compared with other common space learning methods. To further validate the necessity to be task-specific for cross-media retrieval, we evaluate the proposed method in terms of training a unified V and W by incorporating both two linear regression terms in Eq.(2) and Eq.(3) into a single optimization objective. As shown in Table II, the learned subspaces for I2T and T2I could not be used interchangeably and the unified scheme can only achieve com-\nACM Transactions on Intelligent Systems and Technology, Vol. V, No. N, Article A, Publication date: January YYYY.\npromised performance for each retrieval task, which cannot compare to the proposed modality-dependent scheme.\nAs a very popular dataset, Wikipedia has been employed by many other works for cross-media retrieval evaluation. With a different train/test division, [Wu et al. 2014] achieved an average mAP score of 0.226 (Image Query: 0.227, Text Query: 0.224) through a sparse hash model and [Wang et al. 2014] achieved an average mAP score of 0.183 (Image Query: 0.187, Text Query: 0.179) through a deep auto-encoder model. Besides, some other works utilized their own extracted features (both for images and text) for cross-media retrieval evaluation. To further validate the effectiveness of the proposed method, we also compare MDCR with other methods based on more powerful features, i.e., 4,096-CNN for images and 100-LDA for text. We fix \u00b5 = 0.02 and = 10\u22124, and experimentally set \u03bb = 0.1, \u03b71 = 0.5 and \u03b72 = 0.5 for the optimization of I2T and T2I. The comparison results are shown in Table IV. It can be seen that some new state-of-the-art performances are achieved by these methods based on the new feature representations and the proposed MDCR can also outperform others. In addition, we also compare our method with the recent work [Kang et al. 2014], which utilizes 4,096-CNN for images and 200-LDA for text, in Table III. We can see that the proposed MDCR reaches a new state-of-the-art performance on the Wikipedia dataset. Please refer to Fig. 3 for the comparisons of Precision-Recall curves and Fig. 4 for the mAP score of each category. Figure 5 gives some successful and failure cases of our method. For the image query (the 2nd row), although the query image is categorized into Art, it is prevailingly characterized by the human figure, i.e., a strong man, which has been captured by our method and thus leads to the failure results shown. For the text query (the 4th row), there exist many Warfare descriptions in the document such as war, army and troops, which can be hardly realted to the label of the query text, i.e. Art.\nFor the Pascal Sentence dataset and the INRIA-Websearch dataset, we experimentally set \u03bb = 0.5, \u03b71 = 0.5, \u03b72 = 0.5, \u00b5 = 0.02 and = 10\u22124 during the alternative optimization process for I2T and T2T. The comparison results can be found in Table IV. It can be seen that our method is more effective compared with others even on a more challenging dataset, i.e., INRIA-Websearch (with 14,698 pairs of multi-media data and 100 categories). Please refer to Fig. 3 for the comparisons of Precision-Recall curves for these two datasets and Fig. 4 for the mAP score of each category on the Pascal Sentence dataset."}, {"heading": "5. CONCLUSIONS", "text": "Cross-media retrieval has long been a challenge. In this paper, we focus on designing an effective cross-media retrieval model for images and text, i.e., using image to\nACM Transactions on Intelligent Systems and Technology, Vol. V, No. N, Article A, Publication date: January YYYY.\nsearch text (I2T) and using text to search images (T2I). Different from traditional common space learning algorithms, we propose a modality-dependent scheme which recommends different treatments for I2T and T2I by learning two couples of projections for different cross-media retrieval tasks. Specifically, by jointly optimizing a correlation term (between images and text) and a linear regression term (from one modal space, i.e., image or text to the semantic space), two couples of mappings are gained for different retrieval tasks. Extensive experiments on the Wikipedia dataset, the Pascal Sentence dataset and the INRIA-Websearch dataset show the superiority of the proposed method compared with state-of-the-arts."}], "references": [{"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan."], "venue": "Journal of Machine Learning Research", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "A Multi-View Embedding Space for Modeling Internet Images, Tags, and Their Semantics", "author": ["Yunchao Gong", "Qifa Ke", "Michael Isard", "Svetlana Lazebnik."], "venue": "International Journal of Computer Vision (2013), 1\u201324.", "citeRegEx": "Gong et al\\.,? 2013", "shortCiteRegEx": "Gong et al\\.", "year": 2013}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["D.R. Hardoon", "S. Szedmak", "J. Shawe-Taylor."], "venue": "Neural Computation 16, 12 (2004), 2639\u20132664.", "citeRegEx": "Hardoon et al\\.,? 2004", "shortCiteRegEx": "Hardoon et al\\.", "year": 2004}, {"title": "Accounting for the Relative Importance of Objects in Image Retrieval", "author": ["Sung Ju Hwang", "Kristen Grauman."], "venue": "British Machine Vision Conference. 1\u201312.", "citeRegEx": "Hwang and Grauman.,? 2010", "shortCiteRegEx": "Hwang and Grauman.", "year": 2010}, {"title": "CrossModal Similarity Learning: A Low Rank Bilinear Formulation", "author": ["Cuicui Kang", "Shengcai Liao", "Yonghao He", "Jian Wang", "Shiming Xiang", "Chunhong Pan."], "venue": "arXiv preprint arXiv:1411.4738 (2014).", "citeRegEx": "Kang et al\\.,? 2014", "shortCiteRegEx": "Kang et al\\.", "year": 2014}, {"title": "Improving web-image search results using query-relative classifiers", "author": ["Josip Krapac", "Moray Allan", "Jakob Verbeek", "Fr\u00e9d\u00e9ric Jurie."], "venue": "IEEE Conference on Computer Vision and Pattern Recognition. 1094\u2013 1101. http://lear.inrialpes.fr/pubs/2010/KAVJ10", "citeRegEx": "Krapac et al\\.,? 2010", "shortCiteRegEx": "Krapac et al\\.", "year": 2010}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoff Hinton."], "venue": "Advances in Neural Information Processing Systems. 1106\u20131114.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Learning hash functions for cross-view similarity search", "author": ["Shaishav Kumar", "Raghavendra Udupa."], "venue": "IJCAI Proceedings-International Joint Conference on Artificial Intelligence, Vol. 22. 1360.", "citeRegEx": "Kumar and Udupa.,? 2011", "shortCiteRegEx": "Kumar and Udupa.", "year": 2011}, {"title": "Learning Multimodal Neural Network with Ranking Examples", "author": ["Xinyan Lu", "Fei Wu", "Xi Li", "Yin Zhang", "Weiming Lu", "Donghui Wang", "Yueting Zhuang."], "venue": "Proceedings of the international conference on Multimedia. 985\u2013988.", "citeRegEx": "Lu et al\\.,? 2014", "shortCiteRegEx": "Lu et al\\.", "year": 2014}, {"title": "Collecting image annotations using Amazon\u2019s Mechanical Turk", "author": ["C. Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier."], "venue": "Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk. 139\u2013147.", "citeRegEx": "Rashtchian et al\\.,? 2010", "shortCiteRegEx": "Rashtchian et al\\.", "year": 2010}, {"title": "A new approach to cross-modal multimedia retrieval", "author": ["N. Rasiwasia", "J. Costa Pereira", "E. Coviello", "G. Doyle", "G.R.G. Lanckriet", "R. Levy", "N. Vasconcelos."], "venue": "Proceedings of the international conference on Multimedia. 251\u2013260.", "citeRegEx": "Rasiwasia et al\\.,? 2010", "shortCiteRegEx": "Rasiwasia et al\\.", "year": 2010}, {"title": "Overview and recent advances in partial least squares", "author": ["Roman Rosipal", "Nicole Kr\u00e4mer."], "venue": "Subspace, Latent Structure and Feature Selection. Springer, 34\u201351.", "citeRegEx": "Rosipal and Kr\u00e4mer.,? 2006", "shortCiteRegEx": "Rosipal and Kr\u00e4mer.", "year": 2006}, {"title": "Bypassing synthesis: PLS for face recognition with pose, lowresolution and sketch", "author": ["Abhishek Sharma", "David W Jacobs."], "venue": "IEEE Conference on Computer Vision and Pattern Recognition. 593\u2013600.", "citeRegEx": "Sharma and Jacobs.,? 2011", "shortCiteRegEx": "Sharma and Jacobs.", "year": 2011}, {"title": "Generalized multiview analysis: A discriminative latent space", "author": ["Abhishek Sharma", "Abhishek Kumar", "H Daume", "David W Jacobs."], "venue": "IEEE Conference on Computer Vision and Pattern Recognition. 2160\u2013 2167.", "citeRegEx": "Sharma et al\\.,? 2012", "shortCiteRegEx": "Sharma et al\\.", "year": 2012}, {"title": "Separating style and content with bilinear models", "author": ["Joshua B Tenenbaum", "William T Freeman."], "venue": "Neural computation 12, 6 (2000), 1247\u20131283.", "citeRegEx": "Tenenbaum and Freeman.,? 2000", "shortCiteRegEx": "Tenenbaum and Freeman.", "year": 2000}, {"title": "Effective MultiModal Retrieval based on Stacked Auto-Encoders", "author": ["Wei Wang", "Beng Chin Ooi", "Xiaoyan Yang", "Dongxiang Zhang", "Yueting Zhuang."], "venue": "International Conference on Very Large Data Bases 7, 8 (2014).", "citeRegEx": "Wang et al\\.,? 2014", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Learning a mid-level feature space for cross-media regularization", "author": ["Yunchao Wei", "Yao Zhao", "Zhenfeng Zhu", "Yanhui Xiao", "Shikui Wei."], "venue": "IEEE International Conference on Multimedia and Expo. 1\u20136.", "citeRegEx": "Wei et al\\.,? 2014", "shortCiteRegEx": "Wei et al\\.", "year": 2014}, {"title": "Cross-media semantic representation via bi-directional learning to rank", "author": ["Fei Wu", "Xinyan Lu", "Zhongfei Zhang", "Shuicheng Yan", "Yong Rui", "Yueting Zhuang."], "venue": "Proceedings of the international conference on Multimedia. 877\u2013886.", "citeRegEx": "Wu et al\\.,? 2013", "shortCiteRegEx": "Wu et al\\.", "year": 2013}, {"title": "Sparse Multi-Modal Hashing", "author": ["Fei Wu", "Zhou Yu", "Yi Yang", "Siliang Tang", "Yin Zhang", "Yueting Zhuang."], "venue": "IEEE Transactions on Multimedia 16, 2 (2014), 427\u2013439.", "citeRegEx": "Wu et al\\.,? 2014", "shortCiteRegEx": "Wu et al\\.", "year": 2014}, {"title": "A multimedia retrieval framework based on semi-supervised ranking and relevance feedback", "author": ["Yi Yang", "Feiping Nie", "Dong Xu", "Jiebo Luo", "Yueting Zhuang", "Yunhe Pan."], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 34, 4 (2012), 723\u2013742.", "citeRegEx": "Yang et al\\.,? 2012", "shortCiteRegEx": "Yang et al\\.", "year": 2012}, {"title": "Cross-media retrieval using query dependent search methods", "author": ["Yi Yang", "Fei Wu", "Dong Xu", "Yueting Zhuang", "Liang-Tien Chia."], "venue": "Pattern Recognition 43, 8 (2010), 2927\u20132936.", "citeRegEx": "Yang et al\\.,? 2010", "shortCiteRegEx": "Yang et al\\.", "year": 2010}, {"title": "Ranking with local regression and global alignment for cross media retrieval", "author": ["Yi Yang", "Dong Xu", "Feiping Nie", "Jiebo Luo", "Yueting Zhuang."], "venue": "Proceedings of the international conference on Multimedia. 175\u2013184.", "citeRegEx": "Yang et al\\.,? 2009", "shortCiteRegEx": "Yang et al\\.", "year": 2009}, {"title": "Harmonizing hierarchical manifolds for multimedia document semantics understanding and cross-media retrieval", "author": ["Yi Yang", "Yue-Ting Zhuang", "Fei Wu", "Yun-He Pan."], "venue": "IEEE Transactions on Multimedia 10, 3 (2008), 437\u2013446.", "citeRegEx": "Yang et al\\.,? 2008", "shortCiteRegEx": "Yang et al\\.", "year": 2008}, {"title": "Cross-media retrieval by intra-media and inter-media correlation mining", "author": ["Xiaohua Zhai", "Yuxin Peng", "Jianguo Xiao."], "venue": "Multimedia systems 19, 5 (2013), 395\u2013406.", "citeRegEx": "Zhai et al\\.,? 2013", "shortCiteRegEx": "Zhai et al\\.", "year": 2013}, {"title": "Mining Semantically Consistent Patterns for Cross-View Data", "author": ["Lei Zhang", "Yao Zhao", "Zhenfeng Zhu", "Shikui Wei", "Xindong Wu."], "venue": "IEEE Trans. Knowl. Data Eng. 26, 11 (2014), 2745\u20132758.", "citeRegEx": "Zhang et al\\.,? 2014", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Cross-Media Hashing with Neural Networks", "author": ["Yueting Zhuang", "Zhou Yu", "Wei Wang", "Fei Wu", "Siliang Tang", "Jian Shao."], "venue": "Proceedings of the international conference on Multimedia. 901\u2013904.", "citeRegEx": "Zhuang et al\\.,? 2014", "shortCiteRegEx": "Zhuang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "We observe that most exiting works [Hardoon et al. 2004; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013] focus on learning one couple of mapping matrices to project high-dimensional features from different modalities into a common latent space.", "startOffset": 35, "endOffset": 117}, {"referenceID": 10, "context": "We observe that most exiting works [Hardoon et al. 2004; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013] focus on learning one couple of mapping matrices to project high-dimensional features from different modalities into a common latent space.", "startOffset": 35, "endOffset": 117}, {"referenceID": 13, "context": "We observe that most exiting works [Hardoon et al. 2004; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013] focus on learning one couple of mapping matrices to project high-dimensional features from different modalities into a common latent space.", "startOffset": 35, "endOffset": 117}, {"referenceID": 1, "context": "We observe that most exiting works [Hardoon et al. 2004; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013] focus on learning one couple of mapping matrices to project high-dimensional features from different modalities into a common latent space.", "startOffset": 35, "endOffset": 117}, {"referenceID": 2, "context": "However, only considering pair-wise closeness [Hardoon et al. 2004] is not sufficient for cross-media retrieval tasks, since it is required that multi-modal data from the same semantics should be united in the common latent subspace.", "startOffset": 46, "endOffset": 67}, {"referenceID": 13, "context": "Although [Sharma et al. 2012] and [Gong et al.", "startOffset": 9, "endOffset": 29}, {"referenceID": 1, "context": "2012] and [Gong et al. 2013] have proposed to use supervised information to cluster the multi-modal data with the same semantics, learning one couple of projections may only lead to compromised results for each retrieval task.", "startOffset": 10, "endOffset": 28}, {"referenceID": 5, "context": "\u2022Based on the INRIA-Websearch dataset [Krapac et al. 2010], we construct a new dataset for cross-media retrieval evaluation.", "startOffset": 38, "endOffset": 58}, {"referenceID": 2, "context": "Some works [Hardoon et al. 2004; Tenenbaum and Freeman 2000; Rosipal and Kr\u00e4mer 2006; Yang et al. 2008; Sharma and Jacobs 2011; Hwang and Grauman 2010; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013; Wei et al. 2014; Zhang et al. 2014] try to learn an optimal common latent subspace for multi-modal data.", "startOffset": 11, "endOffset": 248}, {"referenceID": 22, "context": "Some works [Hardoon et al. 2004; Tenenbaum and Freeman 2000; Rosipal and Kr\u00e4mer 2006; Yang et al. 2008; Sharma and Jacobs 2011; Hwang and Grauman 2010; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013; Wei et al. 2014; Zhang et al. 2014] try to learn an optimal common latent subspace for multi-modal data.", "startOffset": 11, "endOffset": 248}, {"referenceID": 10, "context": "Some works [Hardoon et al. 2004; Tenenbaum and Freeman 2000; Rosipal and Kr\u00e4mer 2006; Yang et al. 2008; Sharma and Jacobs 2011; Hwang and Grauman 2010; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013; Wei et al. 2014; Zhang et al. 2014] try to learn an optimal common latent subspace for multi-modal data.", "startOffset": 11, "endOffset": 248}, {"referenceID": 13, "context": "Some works [Hardoon et al. 2004; Tenenbaum and Freeman 2000; Rosipal and Kr\u00e4mer 2006; Yang et al. 2008; Sharma and Jacobs 2011; Hwang and Grauman 2010; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013; Wei et al. 2014; Zhang et al. 2014] try to learn an optimal common latent subspace for multi-modal data.", "startOffset": 11, "endOffset": 248}, {"referenceID": 1, "context": "Some works [Hardoon et al. 2004; Tenenbaum and Freeman 2000; Rosipal and Kr\u00e4mer 2006; Yang et al. 2008; Sharma and Jacobs 2011; Hwang and Grauman 2010; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013; Wei et al. 2014; Zhang et al. 2014] try to learn an optimal common latent subspace for multi-modal data.", "startOffset": 11, "endOffset": 248}, {"referenceID": 16, "context": "Some works [Hardoon et al. 2004; Tenenbaum and Freeman 2000; Rosipal and Kr\u00e4mer 2006; Yang et al. 2008; Sharma and Jacobs 2011; Hwang and Grauman 2010; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013; Wei et al. 2014; Zhang et al. 2014] try to learn an optimal common latent subspace for multi-modal data.", "startOffset": 11, "endOffset": 248}, {"referenceID": 24, "context": "Some works [Hardoon et al. 2004; Tenenbaum and Freeman 2000; Rosipal and Kr\u00e4mer 2006; Yang et al. 2008; Sharma and Jacobs 2011; Hwang and Grauman 2010; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013; Wei et al. 2014; Zhang et al. 2014] try to learn an optimal common latent subspace for multi-modal data.", "startOffset": 11, "endOffset": 248}, {"referenceID": 2, "context": "Two popular approaches, Canonical Correlation Analysis (CCA) [Hardoon et al. 2004] and Partial Least Squares (PLS) [Rosipal and Kr\u00e4mer 2006; Sharma and Jacobs 2011], are usually employed to find a couple of mappings to maximize the correlations between two variables.", "startOffset": 61, "endOffset": 82}, {"referenceID": 9, "context": "Based on CCA, a number of successful algorithms have been developed for cross-media retrieval tasks [Rashtchian et al. 2010; Hwang and Grauman 2010; Sharma et al. 2012; Gong et al. 2013].", "startOffset": 100, "endOffset": 186}, {"referenceID": 13, "context": "Based on CCA, a number of successful algorithms have been developed for cross-media retrieval tasks [Rashtchian et al. 2010; Hwang and Grauman 2010; Sharma et al. 2012; Gong et al. 2013].", "startOffset": 100, "endOffset": 186}, {"referenceID": 1, "context": "Based on CCA, a number of successful algorithms have been developed for cross-media retrieval tasks [Rashtchian et al. 2010; Hwang and Grauman 2010; Sharma et al. 2012; Gong et al. 2013].", "startOffset": 100, "endOffset": 186}, {"referenceID": 9, "context": "The work [Rashtchian et al. 2010] investigated the cross-media retrieval problem in terms of correlation hypothesis and abstraction hypothesis.", "startOffset": 9, "endOffset": 33}, {"referenceID": 1, "context": "More recently, the work [Gong et al. 2013] proposed a three-view CCA model by introducing a semantic view to produce a better separation for multi-modal data of different classes in the learned latent subspace.", "startOffset": 24, "endOffset": 42}, {"referenceID": 18, "context": "To address the problem of prohibitively expensive nearest neighbor search, some hashing-based approaches [Kumar and Udupa 2011; Wu et al. 2014] to large scale similarity search have drawn much interest from the cross-media retrieval community.", "startOffset": 105, "endOffset": 143}, {"referenceID": 18, "context": "Recently, [Wu et al. 2014] proposed a sparse multi-modal hashing method, which can obtain sparse codes for the data across different modalities via joint multi-modal dictionary learning, to address cross-modal retrieval.", "startOffset": 10, "endOffset": 26}, {"referenceID": 15, "context": "Besides, with the development of deep learning, some deep models [Frome et al. 2013; Wang et al. 2014; Lu et al. 2014; Zhuang et al. 2014] have also been proposed to address cross-media problems.", "startOffset": 65, "endOffset": 138}, {"referenceID": 8, "context": "Besides, with the development of deep learning, some deep models [Frome et al. 2013; Wang et al. 2014; Lu et al. 2014; Zhuang et al. 2014] have also been proposed to address cross-media problems.", "startOffset": 65, "endOffset": 138}, {"referenceID": 25, "context": "Besides, with the development of deep learning, some deep models [Frome et al. 2013; Wang et al. 2014; Lu et al. 2014; Zhuang et al. 2014] have also been proposed to address cross-media problems.", "startOffset": 65, "endOffset": 138}, {"referenceID": 21, "context": "Beyond the above mentioned models, some other works [Yang et al. 2009; Yang et al. 2010; Yang et al. 2012; Wu et al. 2013; Zhai et al. 2013; Kang et al. 2014] have also been proposed to address cross-media problems.", "startOffset": 52, "endOffset": 158}, {"referenceID": 20, "context": "Beyond the above mentioned models, some other works [Yang et al. 2009; Yang et al. 2010; Yang et al. 2012; Wu et al. 2013; Zhai et al. 2013; Kang et al. 2014] have also been proposed to address cross-media problems.", "startOffset": 52, "endOffset": 158}, {"referenceID": 19, "context": "Beyond the above mentioned models, some other works [Yang et al. 2009; Yang et al. 2010; Yang et al. 2012; Wu et al. 2013; Zhai et al. 2013; Kang et al. 2014] have also been proposed to address cross-media problems.", "startOffset": 52, "endOffset": 158}, {"referenceID": 17, "context": "Beyond the above mentioned models, some other works [Yang et al. 2009; Yang et al. 2010; Yang et al. 2012; Wu et al. 2013; Zhai et al. 2013; Kang et al. 2014] have also been proposed to address cross-media problems.", "startOffset": 52, "endOffset": 158}, {"referenceID": 23, "context": "Beyond the above mentioned models, some other works [Yang et al. 2009; Yang et al. 2010; Yang et al. 2012; Wu et al. 2013; Zhai et al. 2013; Kang et al. 2014] have also been proposed to address cross-media problems.", "startOffset": 52, "endOffset": 158}, {"referenceID": 4, "context": "Beyond the above mentioned models, some other works [Yang et al. 2009; Yang et al. 2010; Yang et al. 2012; Wu et al. 2013; Zhai et al. 2013; Kang et al. 2014] have also been proposed to address cross-media problems.", "startOffset": 52, "endOffset": 158}, {"referenceID": 17, "context": "In particular, [Wu et al. 2013] presented a bi-directional cross-media semantic representation model by optimizing the bi-directional list-wise ranking loss with a latent space embedding.", "startOffset": 15, "endOffset": 31}, {"referenceID": 23, "context": "In [Zhai et al. 2013], both the intra-media and the inter-media correlation are explored for crossmedia retrieval.", "startOffset": 3, "endOffset": 21}, {"referenceID": 4, "context": "Most recently, [Kang et al. 2014] presented a heterogeneous similarity learning approach based on metric learning for cross-media retrieval.", "startOffset": 15, "endOffset": 33}, {"referenceID": 4, "context": "With the convolutional neural network (CNN) visual feature, some new state-of-the-art cross-media retrieval results have been achieved in [Kang et al. 2014].", "startOffset": 138, "endOffset": 156}, {"referenceID": 1, "context": "Different from [Gong et al. 2013] which incorporates the semantic information as a third view, in this paper, semantic information is employed to determine a common latent space with a fixed dimension where samples with the same label can be clustered.", "startOffset": 15, "endOffset": 33}, {"referenceID": 10, "context": ", Wikipedia [Rasiwasia et al. 2010], Pascal Sentence [Rashtchian et al.", "startOffset": 12, "endOffset": 35}, {"referenceID": 9, "context": "2010], Pascal Sentence [Rashtchian et al. 2010] and a subset of INRIA-Websearch [Krapac et al.", "startOffset": 23, "endOffset": 47}, {"referenceID": 5, "context": "2010] and a subset of INRIA-Websearch [Krapac et al. 2010].", "startOffset": 38, "endOffset": 58}, {"referenceID": 10, "context": "We utilize the publicly available features provided by [Rasiwasia et al. 2010] i.", "startOffset": 55, "endOffset": 78}, {"referenceID": 0, "context": "Besides, we also present the cross-media retrieval results based on the 4,096 dimensional CNN visual features3 and the 100 dimensional Latent Dirichlet Allocation model (LDA) [Blei et al. 2003] textual features (we firstly obtain the textual feature vector based on 500 tokens and then LDA model is used to compute the probability of each document under 100 topics).", "startOffset": 175, "endOffset": 193}, {"referenceID": 6, "context": "For more details, please refer to [Krizhevsky et al. 2012].", "startOffset": 34, "endOffset": 58}, {"referenceID": 10, "context": "Results In the experiments, we mainly compare the proposed MDCR with six algorithms, including CCA, Semantic Matching (SM) [Rasiwasia et al. 2010], Semantic Correlation Matching (SCM) [Rasiwasia et al.", "startOffset": 123, "endOffset": 146}, {"referenceID": 10, "context": "2010], Semantic Correlation Matching (SCM) [Rasiwasia et al. 2010], Three-View CCA (T-V CCA) [Gong et al.", "startOffset": 43, "endOffset": 66}, {"referenceID": 1, "context": "2010], Three-View CCA (T-V CCA) [Gong et al. 2013], Generalized Multiview Marginal Fisher Analysis (GMMFA) [Sharma et al.", "startOffset": 32, "endOffset": 50}, {"referenceID": 13, "context": "2013], Generalized Multiview Marginal Fisher Analysis (GMMFA) [Sharma et al. 2012] and Generalized Multiview Linear Discriminant Analysis (GMLDA) [Sharma et al.", "startOffset": 62, "endOffset": 82}, {"referenceID": 13, "context": "2012] and Generalized Multiview Linear Discriminant Analysis (GMLDA) [Sharma et al. 2012].", "startOffset": 69, "endOffset": 89}, {"referenceID": 10, "context": "For the Wikipedia dataset, we firstly compare the proposed MDCR with other methods based on the publicly available features [Rasiwasia et al. 2010], i.", "startOffset": 124, "endOffset": 147}, {"referenceID": 18, "context": "With a different train/test division, [Wu et al. 2014] achieved an average mAP score of 0.", "startOffset": 38, "endOffset": 54}, {"referenceID": 15, "context": "224) through a sparse hash model and [Wang et al. 2014] achieved an average mAP score of 0.", "startOffset": 37, "endOffset": 55}, {"referenceID": 4, "context": "In addition, we also compare our method with the recent work [Kang et al. 2014], which utilizes 4,096-CNN for images and 200-LDA for text, in Table III.", "startOffset": 61, "endOffset": 79}, {"referenceID": 4, "context": "Cross-media retrieval comparition with results of four methods reported by [Kang et al. 2014] on the Wikipedia dataset.", "startOffset": 75, "endOffset": 93}], "year": 2015, "abstractText": "In this paper, we investigate the cross-media retrieval between images and text, i.e., using image to search text (I2T) and using text to search images (T2I). Existing cross-media retrieval methods usually learn one couple of projections, by which the original features of images and text can be projected into a common latent space to measure the content similarity. However, using the same projections for the two different retrieval tasks (I2T and T2I) may lead to a tradeoff between their respective performances, rather than their best performances. Different from previous works, we propose a modality-dependent cross-media retrieval (MDCR) model, where two couples of projections are learned for different cross-media retrieval tasks instead of one couple of projections. Specifically, by jointly optimizing the correlation between images and text and the linear regression from one modal space (image or text) to the semantic space, two couples of mappings are learned to project images and text from their original feature spaces into two common latent subspaces (one for I2T and the other for T2I). Extensive experiments show the superiority of the proposed MDCR compared with other methods. In particular, based the 4,096 dimensional convolutional neural network (CNN) visual feature and 100 dimensional LDA textual feature, the mAP of the proposed method achieves 41.5%, which is a new state-of-the-art performance on the Wikipedia dataset.", "creator": "TeX"}}}