{"id": "1506.07452", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2015", "title": "Parallel Multi-Dimensional LSTM, With Application to Fast Biomedical Volumetric Image Segmentation", "abstract": "Convolutional Neural Networks (CNNs) can be shifted across 2D images or 3D videos to segment them. They have a fixed input size and typically perceive only small local contexts of the pixels to be classified as foreground or background. In contrast, Multi-Dimensional Recurrent NNs (MD-RNNs) can perceive the entire spatio-temporal context of each pixel in a few sweeps through all pixels, especially when the RNN is a Long Short-Term Memory (LSTM). Despite these theoretical advantages, however, unlike CNNs, previous MD-LSTM variants were hard to parallelize on GPUs. Here we re-arrange the traditional cuboid order of computations in MD-LSTM in pyramidal fashion. The resulting PyraMiD-LSTM is easy to parallelize, especially for 3D data such as stacks of brain slice images. PyraMiD-LSTM achieved best known pixel-wise brain image segmentation results on MRBrainS13 (and competitive results on EM-ISBI12).\n\n\nFigure 3. Normalized MD-LSTM in a 1-dimensional layer (top right: TSM in this chart)\nFigure 3. Normalized MD-LSTM in a 1-dimensional layer (bottom right: TSM in this chart)\nThe MD-LSTM image segmentation result on WMSP3 was generated from the previous MD-LSTM in pyramidal fashion. The following images demonstrate the normalization of MD-LSTM over a 2-dimensional layer:\nFigure 3. Normalized MD-LSTM in a 1-dimensional layer (bottom right: TSM in this chart)\nThe MD-LSTM image segmentation results on MSP3 was generated from the previous MD-LSTM in pyramidal fashion. The following images demonstrate the normalization of MD-LSTM over a 2-dimensional layer:\nFigure 3. Normalized MD-LSTM in a 1-dimensional layer (top right: TSM in this chart)\nThe MD-LSTM image segmentation result on WMSP3 was generated from the previous MD-LSTM in pyramidal fashion. The following images demonstrate the normalization of MD-LSTM over a 2-dimensional layer:\nFigure 3. Normalized MD-LSTM in a 1-dimensional layer (top right: TSM in this chart)\n", "histories": [["v1", "Wed, 24 Jun 2015 16:26:51 GMT  (640kb,D)", "http://arxiv.org/abs/1506.07452v1", "Marijn F. Stollenga and Wonmin Byeon are the shared first authors, both authors contributed equally to this work"]], "COMMENTS": "Marijn F. Stollenga and Wonmin Byeon are the shared first authors, both authors contributed equally to this work", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["marijn f stollenga", "wonmin byeon", "marcus liwicki", "j\u00fcrgen schmidhuber"], "accepted": true, "id": "1506.07452"}, "pdf": {"name": "1506.07452.pdf", "metadata": {"source": "CRF", "title": "Parallel Multi-Dimensional LSTM, With Application to Fast Biomedical Volumetric Image Segmentation", "authors": ["Marijn F. Stollenga", "Wonmin Byeon", "Marcus Liwicki", "Juergen Schmidhuber"], "emails": ["marijn@idsia.ch,", "wonmin.byeon@dfki.de"], "sections": [{"heading": "1 Introduction", "text": "Long Short-Term Memory (LSTM) networks [1, 2] are recurrent neural networks (RNNs) initially designed for sequence processing. They achieved state-of-the-art results on challenging tasks such as handwriting recognition [3], large vocabulary speech recognition [4, 5] and machine translation [6]. Their architecture contains gates to store and read out information from linear units called error carousels that retain information over long time intervals, which is hard for traditional RNNs.\nar X\niv :1\n50 6.\n07 45\n2v 1\nMulti-Dimensional LSTM networks (MD-LSTM [7]) connect hidden LSTM units in grid-like fashion1. Two dimensional MD-LSTM is applicable to image segmentation [7, 8, 9] where each pixel is assigned to a class such as background or foreground. Each LSTM unit sees a pixel and receives input from neighboring LSTM units, thus recursively gathering information about all other pixels in the image.\nThere are many biomedical 3D volumetric data sources, such as computed tomography (CT), magnetic resonance (MR), and electron microscopy (EM). Most previous approaches process each 2D slice separately, using image segmentation algorithms such as snakes [10], random forests [11] and Convolutional Neural Networks [12]. 3DLSTM, however, can process the full context of each pixel in such a volume through 8 sweeps over all pixels by 8 different LSTMs, each sweep in the general direction of one of the 8 directed volume diagonals.\nDue to the sequential nature of RNNs, however, MD-LSTM parallelization was difficult, especially for volumetric data. The novel Pyramidal Multi-Dimensional LSTM (PyraMiD-LSTM) networks introduced in this paper use a rather different topology and update strategy. They are easier to parallelize, need fewer computations overall, and scale well on GPU architectures.\nPyraMiD-LSTM is applied to two challenging tasks involving segmentation of biological volumetric images. Competitive results are achieved on EM-ISBI12 [13]; best known results are achieved on MRBrainS13 [14]."}, {"heading": "2 Method", "text": "We will first describe standard one-dimensional LSTM [2] and MD-LSTM. Then we introduce topology changes to construct the PyraMiD-LSTM, which is formally described and discussed.\nThe original LSTM unit consists of an input gate (i), forget gate2 (f ), output gate (o), and memory cell (c) which control what should be remembered or forgotten over potentially long periods of time. All gates and activations are real-valued vectors: x, i, f, c\u0303, c, o, h \u2208 RT , where T is the length of the input. The gates and activations at discrete time t (t=1,2,...) are computed as follows:\nit = \u03c3(xt \u00b7 \u03b8xi + ht-1 \u00b7 \u03b8hi + \u03b8ibias), (1) ft = \u03c3(xt \u00b7 \u03b8xf + ht-1 \u00b7 \u03b8hf + \u03b8fbias), (2)\nc\u0303t = tanh(xt \u00b7 \u03b8xc\u0303 + ht-1 \u00b7 \u03b8hc\u0303 + \u03b8c\u0303bias), (3) ct = c\u0303t it + ct-1 ft, (4)\not = \u03c3(xt \u00b7 \u03b8xo + ht-1 \u00b7 \u03b8ho + \u03b8obias), (5) ht = ot tanh(ct) (6)\nwhere (\u00b7) is a matrix multiplication, ( ) an element-wise multiplication, and \u03b8 denotes the weights. c\u0303 is the input to the \u2019cell\u2019 c, which is gated by the input gate, and h\n1For example, in two dimensions this yields 4 directions; up, down, left and right. 2Although the forget gate output is inverted and actually \u2018remembers\u2019 when it is on, and forgets when it\nis off, the traditional nomenclature is kept.\nis the output. The non-linear functions \u03c3 and tanh are applied element-wise, where \u03c3(x) = 11+e\u2212x . Equations (1, 2) determine gate activations, Equation (3) cell inputs, Equation (4) the new cell states (here \u2018memories\u2019 are stored or forgotten), Equation (5) output gate activations which appear in Equation (6), the final output."}, {"heading": "2.1 Pyramidal Connection Topology", "text": "In MD-LSTMs, connections are aligned with the grid axes. In 2D, these directions are up, down, left and right. A 2D-LSTM adds the pixel-wise outputs of 4 LSTMs, one scanning the image pixel by pixel from north-west to south-east, one from north-east to south-west, one from south-west to north-east, and one from south-east to north-west.\nIf the connections are rotated by 45\u25e6, all inputs to all units come from either left, right, up, or down (left in case of Figure 1\u2013b). This greatly facilitates parallelization, since all the elements of a whole grid row can be computed independently, which does not work for MD-LSTM simplexes, whose sizes vary. However, this introduces context gaps as in Figure 1\u2013b. By adding an extra input, these gaps are filled as in Figure 1\u2013c.\nA similar connection strategy has been previously used to speed up non-euclidian distance computations on surfaces [15]. There are however important differences:\n\u2022 We can exploit efficient GPU-based CUDA convolution operations, but in a way unlike what is done in CNNs, as will be explained below. \u2022 As a result of these operations, input filters that are bigger than the necessary 3 \u00d7 3 filters arise naturally, creating overlapping contexts. Such redundancy turns out to be beneficial and is used in our experiments. \u2022 We apply several layers of complex processing with multi-channeled outputs and several state-variables for each pixel, instead of having a single value per pixel as in distance computations. \u2022 Our application is focused on volumetric data.\nOne of the striking differences between PyraMiD-LSTM and MD-LSTM is the shape of the scanned contexts. Each LSTM of an MD-LSTM scans rectangle-like contexts in 2D or cuboids in 3D. Each LSTM of a PyraMiD-LSTM scans triangles in 2D and pyramids in 3D (see Figure 2). An MD-LSTM needs 8 LSTMs to scan a volume, while a PyraMiD-LSTM needs only 6, since it takes 8 cubes or 6 pyramids to fill a volume. Given dimension d, the number of LSTMs grows as 2d for an MD-LSTM (exponentially) and 2\u00d7 d for a PyraMiD-LSTM (linearly)."}, {"heading": "2.2 PyraMiD-LSTM", "text": "Here we explain the PyraMiD-LSTM network architecture for 3D volumes (see Figure 3). It consists of six LSTMs with RNN-tailored convolutions (C-LSTM), one for each direction, to create the full context of each pixel. Note that each of these C-LSTMs is a entire LSTM RNN, processing the entire volume in one direction. The directions D are defined over the three axes (x, y, z): D = {(\u00b7, \u00b7, 1), (\u00b7, \u00b7,\u22121), (\u00b7, 1, \u00b7), (\u00b7,\u22121, \u00b7), (1, \u00b7, \u00b7), (\u22121, \u00b7, \u00b7)}.\nEach C-LSTM performs computations in a plane moving in the defined direction. The symbol (\u00b7) in a dimension signifies that the plane is parallel to this axis, and a\n1 or \u22121 implies that the computation is moving along the positive or negative direction of that axis, respectively. The input is x \u2208 RW\u00d7H\u00d7D\u00d7C , where W is the width, H the height, D the depth, and C the number of channels of the input, or hidden units in the case of second- and higher layers. Similarly, we define the volumes fd, id, od, c\u0303d, cd, hd, h \u2208 RW\u00d7H\u00d7D\u00d7O, where d \u2208 D is a direction and O is the number of hidden units (per pixel). Since each direction needs a separate volume, we denote volumes with (\u00b7)d.\nTo keep the equations readable, we omit the time index below: ht becomes h and ht\u22121 becomes h-1. The time index t is bound to the axis along which computations are performed. For instance, for direction d = (\u00b7, \u00b7, 1), vd refers to the plane orthogonal to axis z; i.e. vx,y,z,c for x = 1..X, y = 1..Y, c = 1..C, and z = t. For a negative direction d = (\u00b7, \u00b7,\u22121), the plane is the same but moves in the opposite direction: z = Z \u2212 t. Furthermore, vd-1 refers to the previous plane, in this case vx,y,z,c for x = 1..X, y = 1..Y, c = 1..C, z = t \u2212 1. A special case is the first plane in each direction, which does not have a previous plane, hence we omit the corresponding computation. The following functions are defined for all planes and all directions:\nC-LSTM:\nid = \u03c3(x \u2217 \u03b8dxi + hd-1 \u2217 \u03b8dhi + \u03b8dibias), (7) fd = \u03c3(x \u2217 \u03b8dxf + hd-1 \u2217 \u03b8dhf + \u03b8dfbias), (8)\nc\u0303d = tanh(x \u2217 \u03b8dxc\u0303 + hd-1 \u2217 \u03b8dhc\u0303 + \u03b8dc\u0303bias), (9) cd = c\u0303d id + cd-1 fd, (10)\nod = \u03c3(x \u2217 \u03b8dxo + hd-1 \u2217 \u03b8dho + \u03b8dobias), (11) hd = od tanh(cd), (12)\nwhere (\u2217) is a RNN-tailored convolution3, and h is the output of the layer. Note that in C-LSTMs the convolution operations propagate information \u2018sideways\u2019 over the axis of the data. This is very different from CNNs where the convolution operations propagate information upwards to the next layer. All biases are the same for all LSTM units (i.e., no positional biases are used). The outputs hd for all directions are added, combining all C-LSTMs into one PyraMiD-LSTM:\nh = \u2211 d\u2208D hd. (13)\nFully-Connected Layer: Each PyraMiD-LSTM layer is connected to a fullyconnected layer, and the output is squashed by the hyperbolic tangent (tanh) function. At the end, a softmax function is applied after the last fully-connected layer."}, {"heading": "3 Experiments", "text": "We evaluate our approach on two 3D biomedical image segmentation datasets: electron microscopy (EM) and MR Brain images.\n3In 3D volumes, convolutions are performed in 2D; in general an n-D volume requires n-1-D convolutions. All convolutions have stride 1, and their filter sizes should at least be 3\u00d73 in each dimension to create the full context.\nEM dataset The EM dataset [13] is provided by the ISBI 2012 workshop on Segmentation of Neuronal Structures in EM Stacks [16]. Two stacks consist of 30 slices of 512\u00d7 512 pixels obtained from a 2\u00d7 2\u00d7 1.5 \u00b5m3 microcube with a resolution of 4 \u00d7 4 \u00d7 50 nm3/pixel and binary labels. One stack is used for training, the other for testing. Target data consists of binary labels (membrane and non-membrane).\nMR Brain dataset The MR Brain images are provided by the ISBI 2015 workshop on Neonatal and Adult MR Brain Image Segmentation (ISBI NEATBrainS15) [14]. The dataset consists of twenty fully annotated high-field (3T) multi-sequences: 3D T1weighted scan (T1), T1-weighted inversion recovery scan (IR), and fluid-attenuated inversion recovery scan (FLAIR). The dataset is divided into a training set with five volumes and a test set with fifteen volumes. All scans are bias-corrected and aligned. Each volume includes 48 slices with 240\u00d7240 pixels (3mm slice thickness). The slices are manually segmented through nine labels: cortical gray matter, basal ganglia, white matter, white matter lesions, cerebrospinal fluid in the extracerebral space, ventricles, cerebellum, brainstem, and background. Following the ISBI NEATBrainS15 workshop procedure, all labels are grouped into four classes and background: 1) cortical gray matter and basal ganglia (GM), 2) white matter and white matter lesions (WM), 3) cerebrospinal fluid and ventricles (CSF), and 4) cerebellum and brainstem. Class 4) is ignored for the final evaluation as required.\nSub-volumes and Augmentation The full dataset requires more than the 12 GB of memory provided by our GPU, hence we train and test on sub-volumes. We randomly pick a position in the full data and extract a smaller cube (see the details in Bootstrapping). This cube is possibly rotated at a random angle over some axis and can be flipped over any axis. For EM images, we rotate over the z-axis and flipped sub-volumes with 50% chance along x, y, and z axes. For MR brain images, rotation is disabled; only flipping along the x direction is considered, since brains are (mostly) symmetric in this direction.\nDuring test-time, rotations and flipping are disabled and the results of all subvolumes are stitched together using a Gaussian kernel, providing the final result.\nPre-processing We normalize each input slice towards a mean of zero and variance of one, since the imaging methods sometimes yield large variability in contrast and brightness. We do not apply the complex pre-processing common in biomedical image segmentation [11].\nWe apply simple pre-processing on the three datatypes of the MR Brain dataset, since they contain large brightness changes under the same label (even within one slice; see Figure 5). From all slices we subtract the Gaussian smoothed images (filter size: 31 \u00d7 31, \u03c3 = 5.0), then a Contrast-Limited Adaptive Histogram Equalization (CLAHE) [17] is applied to enhance the local contrast (tile size: 16\u00d716, contrast limit: 2.0). An example of the images after pre-processing is shown in Figure 5. The original and pre-processed images are all used, except the original IR images (Figure 5b), which have high variability.\nTraining We apply RMS-prop [18, 19] with momentum. We define a \u03c1\u2190\u2212 b to be an = \u03c1an + (1 \u2212 \u03c1)bn, where a, b \u2208 RN . The following equations hold for every epoch:\nE = (y\u2217 \u2212 y)2, (14) MSE\n\u03c1MSE\u2190\u2212\u2212\u2212\u2212 \u22072\u03b8E, (15)\nG = \u2207\u03b8E\u221a MSE + , (16)\nM \u03c1M\u2190\u2212\u2212 G, (17)\n\u03b8 = \u03b8 \u2212 \u03bblrM, (18)\nwhere y\u2217 is the target, y is the output from the networks, E is the squared loss, MSE a running average of the variance of the gradient, \u22072 is the element-wise squared gradient, G the normalized gradient, M the smoothed gradient, and \u03b8 the weights. This results in normalized gradients of similar size for all weights, such that even weights with small gradients get updated. This also helps to deal with vanishing gradients [20].\nWe use a decaying learning rate: \u03bblr = 10\u22126 + 10\u22122 ( 100 \u221a 1 2 )epoch , which starts at \u03bblr \u2248 10\u22122 and halves every 100 epochs asymptotically towards \u03bblr = 10\u22126. Other hyper-parameters used are = 10\u22125, \u03c1MSE = 0.9, and \u03c1M = 0.9.\nBootstrapping To speed up training, we run three learning procedures with increasing sub-volume sizes: first, 3000 epochs with size 64 \u00d7 64 \u00d7 8, then 2000 epochs with size 128\u00d7 128\u00d7 15. Finally, for the EM-dataset, we train 1000 epochs with size 256\u00d7 256\u00d7 20, and for the MR Brain dataset 1000 epochs with size 240\u00d7 240\u00d7 25. After each epoch, the learning rate \u03bblr is reset.\nExperimental Setup All experiments are performed on a desktop computer with an NVIDIA GTX TITAN X 12GB GPU. For GPU implementation, the NVIDIA CUDA Deep Neural Network library (cuDNN) [21] is used. On the MR brain dataset, training took around three days, and testing per volume took around 2 minutes.\nWe use exactly the same hyper-parameters and architecture for both datasets. Our networks contain three PyraMiD-LSTM layers. The first PyraMiD-LSTM layer has 16 hidden units followed by a fully-connected layer with 25 hidden units. In the next PyraMiD-LSTM layer, 32 hidden units are connected to a fully-connected layer with 45 hidden units. In the last PyraMiD-LSTM layer, 64 hidden units are connected to the fully-connected output layer whose size equals the number of classes.\nThe convolutional filter size for all PyraMiD-LSTM layers is set to 7\u00d77. The total number of weights is 10,751,549, and all weights are initialized according to a uniform distribution: U(\u22120.1, 0.1)."}, {"heading": "3.1 Neuronal Membrane Segmentation", "text": "Evaluation metrics Three error metrics evaluate the following factors:\nResults Membrane segmentation is evaluated through an online system provided by the ISBI 2012 organizers. Comparisons to other methods are reported in Table 1. The teams IDSIA and DIVE provide membrane probability maps for each pixel, like our method. These maps are adapted by the post-processing technique of the teams SCI [24], which directly optimizes the rand error (DIVE-SCI (top-1) and IDSIA-SCI (top-2)); this is most important in this particular segmentation task. Without postprocessing, PyraMiD-LSTM networks outperform other methods in rand error, and are competitive in wrapping and pixel errors. Of course, performance could be further improved by applying post-processing techniques. Figure 4 shows an example segmentation result."}, {"heading": "3.2 MR Brain Segmentation", "text": "Evaluation metrics The results are compared based on the following three measures:\n\u2022 The DICE overlap (DC) [25]: spatial overlap between the segmented volume and ground truth\n\u2022 The modified Hausdorff distance (MD) [26]: 95th-percentile Hausdorff distance between the segmented volume and ground truth\n\u2022 The absolute volume difference (AVD) [27]: the absolute difference between segmented and ground truth volume, normalized over the whole volume.\nResults MR brain image segmentation results are evaluated by the ISBI NEATBrain15 organizers [14] who provided the extensive comparison to other approaches on http: //mrbrains13.isi.uu.nl/results.php. Table 2 compares our results to those of the top five teams. The organizers compute nine measures in total and rank all teams for each of them separately. These ranks are then summed per team, determining the final ranking (ties are broken using the standard deviation). PyraMiD-LSTM leads the final ranking with a new state-of-the-art result and outperforms other methods for CFS in all metrics.\nWe also tried regularization through dropout [28]. Following earlier work [29], the dropout operator is applied only to non-recurrent connections (50% dropout on fully connected layers and/or 20% on input layer). However, this did not improve performance, but made the system slower."}, {"heading": "4 Conclusion", "text": "Since 2011, GPU-trained max-pooling CNNs have dominated classification contests [30, 31, 32] and segmentation contests [12]. MD-LSTM, however, may pose a serious challenge to such CNNs, at least for segmentation tasks. Unlike CNNs, MD-LSTM has an elegant recursive way of taking each pixel\u2019s entire spatio-temporal context into account,\nin both images and videos. Previous MD-LSTM implementations, however, could not exploit the parallelism of modern GPU hardware. This has changed through our work presented here. Although our novel highly parallel PyraMiD-LSTM has already achieved state-of-the-art segmentation results in challenging benchmarks, we feel we have only scratched the surface of what will become possible with such PyraMiDLSTM and other MD-RNNs."}, {"heading": "5 Acknowledgements", "text": "We would like to thank Klaus Greff and Alessandro Giusti for their valuable discussions, and Jan Koutnik and Dan Ciresan for their useful feedback. We also thank the ISBI NEATBrain15 organizers [14] and the ISBI 2012 organisers, in particular Adrie\u0308nne Mendrik and Ignacio Arganda-Carreras. Lastly we thank NVIDIA for generously providing us with hardware to perform our research. This research was funded by the NASCENCE EU project (EU/FP7-ICT-317662)."}], "references": [{"title": "Long Short-Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Learning to Forget: Continual Prediction with LSTM", "author": ["F.A. Gers", "J. Schmidhuber", "F. Cummins"], "venue": "Proc. ICANN\u201999, Int. Conf. on Artificial Neural Networks. Edinburgh, Scotland: IEE,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "A Novel Connectionist System for Improved Unconstrained Handwriting Recognition", "author": ["A. Graves", "M. Liwicki", "S. Fernandez", "R. Bertolami", "H. Bunke", "J. Schmidhuber"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Sequence Discriminative Distributed Training of Long Short-Term Memory Recurrent Neural Networks", "author": ["H. Sak", "O. Vinyals", "G. Heigold", "A. Senior", "E. McDermott", "R. Monga", "M. Mao"], "venue": "Proc. Interspeech", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling", "author": ["H. Sak", "A. Senior", "F. Beaufays"], "venue": "Proc. Interspeech", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Tech. rep. arXiv:1409.3215 [cs.CL]. NIPS\u20192014. Google,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Multi-dimensional Recurrent Neural Networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "J. Schmidhuber"], "venue": "ICANN", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Scene Labeling With LSTM Recurrent Neural Networks", "author": ["W. Byeon", "T.M. Breuel", "F. Raue", "M. Liwicki"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Snakes: Active contour models", "author": ["M. Kass", "A. Witkin", "D. Terzopoulos"], "venue": "English. In: International Journal of Computer Vision", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1988}, {"title": "LINKS: Learning-based multi-source IntegratioN frameworK for Segmentation of infant brain images", "author": ["L. Wang", "Y. Gao", "F. Shi", "G. Li", "J.H. Gilmore", "W. Lin", "D. Shen"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images", "author": ["D.C. Ciresan", "A. Giusti", "L.M. Gambardella", "J. Schmidhuber"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "An integrated micro-and macroarchitectural analysis of the Drosophila brain by computer-assisted serial section electron microscopy", "author": ["A. Cardona", "S. Saalfeld", "S. Preibisch", "B. Schmid", "A. Cheng", "J. Pulokas", "P. Tomancak", "V. Hartenstein"], "venue": "PLoS biology", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "MRBrainS Challenge: Online Evaluation Framework for Brain Image Segmentation in 3T MRI Scans, http://mrbrains13.isi.uu.nl", "author": ["A.M. Mendrik", "K.L. Vincken", "H.J. Kuijf", "G.J. Biessels", "M.A. Viergever (organizers"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Parallel algorithms for approximation of distance maps on parametric surfaces", "author": ["O. Weber", "Y.S. Devir", "A.M. Bronstein", "M.M. Bronstein", "R. Kimmel"], "venue": "ACM Transactions on Graphics (TOG)", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Adaptive Histogram Equalization and Its Variations", "author": ["S.M. Pizer", "E.P. Amburn", "J.D. Austin", "R. Cromartie", "A. Geselowitz", "T. Greer", "B.T.H. Romeny", "J.B. Zimmerman"], "venue": "In: Comput. Vision Graph. Image Process", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1987}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "In: COURSERA: Neural Networks for Machine Learning", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "RMSProp and equilibrated adaptive learning rates for non-convex optimization", "author": ["Y.N. Dauphin", "H. de Vries", "J. Chung", "Y. Bengio"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Untersuchungen zu dynamischen neuronalen Netzen", "author": ["S. Hochreiter"], "venue": "Diploma thesis, Institut fu\u0308r Informatik, Lehrstuhl Prof. Brauer, Technische Universita\u0308t Mu\u0308nchen. Advisor: J. Schmidhuber", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1991}, {"title": "cuDNN: Efficient Primitives for Deep Learning", "author": ["S. Chetlur", "C. Woolley", "P. Vandermersch", "J. Cohen", "J. Tran", "B. Catanzaro", "E. Shelhamer"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Objective Criteria for the Evaluation of Clustering Methods", "author": ["W.M. Rand"], "venue": "Journal of the American Statistical Association", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1971}, {"title": "Boundary Learning 12  by Optimization with Topological Constraints", "author": ["V. Jain", "B. Bollmann", "M. Richardson", "D. Berger", "M. Helmstaedter", "K. Briggman", "W. Denk", "J. Bowden", "J. Mendenhall", "W. Abraham", "K. Harris", "N. Kasthuri", "K. Hayworth", "R. Schalek", "J. Tapia", "J. Lichtman", "H. Seung"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "A modular hierarchical approach to 3D electron microscopy image segmentation", "author": ["T. Liu", "C. Jones", "M. Seyedhosseini", "T. Tasdizen"], "venue": "Journal of Neuroscience Methods", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Measures of the Amount of Ecologic Association Between Species", "author": ["L.R. Dice"], "venue": "English. In: Ecology", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1945}, {"title": "Comparing images using the Hausdorff distance", "author": ["D. Huttenlocher", "G. Klanderman", "W. Rucklidge"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 15.9", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1993}, {"title": "An evaluation of four automatic methods of segmenting the subcortical structures in the brain", "author": ["K.O. Babalola", "B. Patenaude", "P. Aljabar", "J. Schnabel", "D. Kennedy", "W. Crum", "S. Smith", "T. Cootes", "M. Jenkinson", "D. Rueckert"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Recurrent Neural Network Regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "A Committee of Neural Networks for Traffic Sign Classification", "author": ["D.C. Ciresan", "U. Meier", "J. Masci", "J. Schmidhuber"], "venue": "In: International Joint Conference on Neural Networks (IJCNN)", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I Sutskever", "G. E Hinton"], "venue": "In: NIPS", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Visualizing and Understanding Convolutional Networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Tech. rep. arXiv:1311.2901 [cs.CV]. NYU,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Long Short-Term Memory (LSTM) networks [1, 2] are recurrent neural networks (RNNs) initially designed for sequence processing.", "startOffset": 39, "endOffset": 45}, {"referenceID": 1, "context": "Long Short-Term Memory (LSTM) networks [1, 2] are recurrent neural networks (RNNs) initially designed for sequence processing.", "startOffset": 39, "endOffset": 45}, {"referenceID": 2, "context": "They achieved state-of-the-art results on challenging tasks such as handwriting recognition [3], large vocabulary speech recognition [4, 5] and machine translation [6].", "startOffset": 92, "endOffset": 95}, {"referenceID": 3, "context": "They achieved state-of-the-art results on challenging tasks such as handwriting recognition [3], large vocabulary speech recognition [4, 5] and machine translation [6].", "startOffset": 133, "endOffset": 139}, {"referenceID": 4, "context": "They achieved state-of-the-art results on challenging tasks such as handwriting recognition [3], large vocabulary speech recognition [4, 5] and machine translation [6].", "startOffset": 133, "endOffset": 139}, {"referenceID": 5, "context": "They achieved state-of-the-art results on challenging tasks such as handwriting recognition [3], large vocabulary speech recognition [4, 5] and machine translation [6].", "startOffset": 164, "endOffset": 167}, {"referenceID": 6, "context": "Multi-Dimensional LSTM networks (MD-LSTM [7]) connect hidden LSTM units in grid-like fashion1.", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": "Two dimensional MD-LSTM is applicable to image segmentation [7, 8, 9] where each pixel is assigned to a class such as background or foreground.", "startOffset": 60, "endOffset": 69}, {"referenceID": 7, "context": "Two dimensional MD-LSTM is applicable to image segmentation [7, 8, 9] where each pixel is assigned to a class such as background or foreground.", "startOffset": 60, "endOffset": 69}, {"referenceID": 8, "context": "Two dimensional MD-LSTM is applicable to image segmentation [7, 8, 9] where each pixel is assigned to a class such as background or foreground.", "startOffset": 60, "endOffset": 69}, {"referenceID": 9, "context": "Most previous approaches process each 2D slice separately, using image segmentation algorithms such as snakes [10], random forests [11] and Convolutional Neural Networks [12].", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "Most previous approaches process each 2D slice separately, using image segmentation algorithms such as snakes [10], random forests [11] and Convolutional Neural Networks [12].", "startOffset": 131, "endOffset": 135}, {"referenceID": 11, "context": "Most previous approaches process each 2D slice separately, using image segmentation algorithms such as snakes [10], random forests [11] and Convolutional Neural Networks [12].", "startOffset": 170, "endOffset": 174}, {"referenceID": 12, "context": "Competitive results are achieved on EM-ISBI12 [13]; best known results are achieved on MRBrainS13 [14].", "startOffset": 46, "endOffset": 50}, {"referenceID": 13, "context": "Competitive results are achieved on EM-ISBI12 [13]; best known results are achieved on MRBrainS13 [14].", "startOffset": 98, "endOffset": 102}, {"referenceID": 1, "context": "We will first describe standard one-dimensional LSTM [2] and MD-LSTM.", "startOffset": 53, "endOffset": 56}, {"referenceID": 14, "context": "A similar connection strategy has been previously used to speed up non-euclidian distance computations on surfaces [15].", "startOffset": 115, "endOffset": 119}, {"referenceID": 12, "context": "EM dataset The EM dataset [13] is provided by the ISBI 2012 workshop on Segmentation of Neuronal Structures in EM Stacks [16].", "startOffset": 26, "endOffset": 30}, {"referenceID": 13, "context": "MR Brain dataset The MR Brain images are provided by the ISBI 2015 workshop on Neonatal and Adult MR Brain Image Segmentation (ISBI NEATBrainS15) [14].", "startOffset": 146, "endOffset": 150}, {"referenceID": 10, "context": "We do not apply the complex pre-processing common in biomedical image segmentation [11].", "startOffset": 83, "endOffset": 87}, {"referenceID": 15, "context": "0), then a Contrast-Limited Adaptive Histogram Equalization (CLAHE) [17] is applied to enhance the local contrast (tile size: 16\u00d716, contrast limit: 2.", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "Training We apply RMS-prop [18, 19] with momentum.", "startOffset": 27, "endOffset": 35}, {"referenceID": 17, "context": "Training We apply RMS-prop [18, 19] with momentum.", "startOffset": 27, "endOffset": 35}, {"referenceID": 18, "context": "This also helps to deal with vanishing gradients [20].", "startOffset": 49, "endOffset": 53}, {"referenceID": 19, "context": "For GPU implementation, the NVIDIA CUDA Deep Neural Network library (cuDNN) [21] is used.", "startOffset": 76, "endOffset": 80}, {"referenceID": 11, "context": "IDSIA [12] 0.", "startOffset": 6, "endOffset": 10}, {"referenceID": 20, "context": "\u2022 Rand error [22]: 1 - F-score of rand index, which measures similarity between two segmentations on the foreground.", "startOffset": 13, "endOffset": 17}, {"referenceID": 21, "context": "\u2022 Warping error [23]: topological disagreements (object splits and mergers)", "startOffset": 16, "endOffset": 20}, {"referenceID": 22, "context": "These maps are adapted by the post-processing technique of the teams SCI [24], which directly optimizes the rand error (DIVE-SCI (top-1) and IDSIA-SCI (top-2)); this is most important in this particular segmentation task.", "startOffset": 73, "endOffset": 77}, {"referenceID": 23, "context": "\u2022 The DICE overlap (DC) [25]: spatial overlap between the segmented volume and ground truth", "startOffset": 24, "endOffset": 28}, {"referenceID": 24, "context": "\u2022 The modified Hausdorff distance (MD) [26]: 95th-percentile Hausdorff distance between the segmented volume and ground truth", "startOffset": 39, "endOffset": 43}, {"referenceID": 25, "context": "\u2022 The absolute volume difference (AVD) [27]: the absolute difference between segmented and ground truth volume, normalized over the whole volume.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "Results MR brain image segmentation results are evaluated by the ISBI NEATBrain15 organizers [14] who provided the extensive comparison to other approaches on http: //mrbrains13.", "startOffset": 93, "endOffset": 97}, {"referenceID": 26, "context": "We also tried regularization through dropout [28].", "startOffset": 45, "endOffset": 49}, {"referenceID": 27, "context": "Following earlier work [29], the dropout operator is applied only to non-recurrent connections (50% dropout on fully connected layers and/or 20% on input layer).", "startOffset": 23, "endOffset": 27}, {"referenceID": 28, "context": "Since 2011, GPU-trained max-pooling CNNs have dominated classification contests [30, 31, 32] and segmentation contests [12].", "startOffset": 80, "endOffset": 92}, {"referenceID": 29, "context": "Since 2011, GPU-trained max-pooling CNNs have dominated classification contests [30, 31, 32] and segmentation contests [12].", "startOffset": 80, "endOffset": 92}, {"referenceID": 30, "context": "Since 2011, GPU-trained max-pooling CNNs have dominated classification contests [30, 31, 32] and segmentation contests [12].", "startOffset": 80, "endOffset": 92}, {"referenceID": 11, "context": "Since 2011, GPU-trained max-pooling CNNs have dominated classification contests [30, 31, 32] and segmentation contests [12].", "startOffset": 119, "endOffset": 123}, {"referenceID": 13, "context": "We also thank the ISBI NEATBrain15 organizers [14] and the ISBI 2012 organisers, in particular Adri\u00ebnne Mendrik and Ignacio Arganda-Carreras.", "startOffset": 46, "endOffset": 50}], "year": 2015, "abstractText": "Convolutional Neural Networks (CNNs) can be shifted across 2D images or 3D videos to segment them. They have a fixed input size and typically perceive only small local contexts of the pixels to be classified as foreground or background. In contrast, Multi-Dimensional Recurrent NNs (MD-RNNs) can perceive the entire spatio-temporal context of each pixel in a few sweeps through all pixels, especially when the RNN is a Long Short-Term Memory (LSTM). Despite these theoretical advantages, however, unlike CNNs, previous MD-LSTM variants were hard to parallelize on GPUs. Here we re-arrange the traditional cuboid order of computations in MD-LSTM in pyramidal fashion. The resulting PyraMiD-LSTM is easy to parallelize, especially for 3D data such as stacks of brain slice images. PyraMiD-LSTM achieved best known pixel-wise brain image segmentation results on MRBrainS13 (and competitive results on EM-ISBI12).", "creator": "LaTeX with hyperref package"}}}