{"id": "1506.08189", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jun-2015", "title": "Correlation Clustering and Biclustering with Locally Bounded Errors", "abstract": "We introduce a new agnostic clustering method: minimax correlation clustering. Given a graph whose edges are labeled with $+$ or $-$, we wish to partition the graph into clusters while trying to avoid errors: $+$ edges between clusters or $-$ edges within clusters. Unlike classical correlation clustering, which seeks to minimize the total number of errors, minimax clustering instead seeks to minimize the number of errors at the worst vertex, that is, at the vertex with the greatest number of incident errors. For example, $+$ edges between clusters or $+$ edges will not be split. We want to minimize the amount of errors within clusters and minimize the number of each error.\n\n\n\n\nWe are interested in a small group of people. We are interested in creating clusters that can easily be found in many applications (e.g., applications like social network applications). We are interested in creating clusters that can easily be found in many applications (e.g., applications like social network applications). We are interested in creating clusters that can easily be found in many applications (e.g., applications like social network applications). We are interested in creating clusters that can easily be found in many applications (e.g., applications like social network applications). We are interested in creating clusters that can easily be found in many applications (e.g., applications like social network applications). We are interested in creating clusters that can easily be found in many applications (e.g., applications like social network applications). We are interested in creating clusters that can easily be found in many applications (e.g., applications like social network applications). We are interested in creating clusters that can easily be found in many applications (e.g., applications like social network applications). We are interested in creating clusters that can easily be found in many applications (e.g., applications like social network applications). We are interested in creating clusters that can easily be found in many applications (e.g., applications like social network applications). We are interested in creating clusters that can easily be found in many applications (e.g., applications like social network applications). We are interested in creating clusters that can easily be found in many applications (e.g., applications like social network applications). We are interested in creating clusters that can easily be found in many applications (e.g., applications like social network applications). We are interested in creating clusters that can easily be found in many applications (e.g., applications like social network applications). We are interested in creating", "histories": [["v1", "Fri, 26 Jun 2015 19:46:41 GMT  (19kb)", "https://arxiv.org/abs/1506.08189v1", "16 pages"], ["v2", "Wed, 9 Dec 2015 22:39:19 GMT  (21kb)", "http://arxiv.org/abs/1506.08189v2", "17 pages, corrected several errors in the previous version of the paper and included discussion of more general objective functions"], ["v3", "Tue, 24 May 2016 19:29:33 GMT  (26kb)", "http://arxiv.org/abs/1506.08189v3", "20 pages, reorganized paper to emphasize the key properties of the rounding algorithm and the broader class of possible objective functions"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["gregory j puleo", "olgica milenkovic"], "accepted": true, "id": "1506.08189"}, "pdf": {"name": "1506.08189.pdf", "metadata": {"source": "CRF", "title": "Correlation Clustering and Biclustering with Locally Bounded Errors", "authors": ["Gregory J. Puleo", "Olgica Milenkovic"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 6.\n08 18\n9v 3\n[ cs\n.D S]\n2 4\nM ay"}, {"heading": "1 Introduction", "text": "Correlation clustering is a clustering model first introduced by Bansal, Blum, and Chawla [5, 6]. The basic form of the model is as follows. We are given a collection of objects and, for some pairs of objects, we are given a judgment of whether the objects are similar or dissimilar. This information is represented as a labeled graph, with edges labeled + or \u2212 according to whether the endpoints are similar or dissimilar. Our goal is to cluster the graph so that + edges tend to be within clusters and \u2212 edges tend to go across clusters. The number of clusters is not specified in advance; determining the optimal number of clusters is instead part of the optimization problem.\nGiven a solution clustering, an error is a + edge whose endpoints lie in different clusters or a \u2212 edge whose endpoints lie in the same cluster. In the original formulation of the correlation clustering, the goal is to minimize the total number of errors; this formulation of the optimization problem is called MinDisagree. Finding an exact optimal solution is NP-hard even when the input graph is complete [5, 6]. Furthermore, if the input graph is allowed to be arbitrary, the best known approximation ratio is O(log n), obtained by [9, 10, 14]. Assuming the Unique Games Conjecture of Khot [16], no constant-factor approximation for MinDisagree on arbitrary graphs is possible; this follows from the results of [11, 21] concerning the minimum multicut problem and the connection between correlation clustering and minimum multicut described in [9, 10, 14].\nSince theoretical barriers appear to preclude constant-factor approximations on arbitrary graphs, much research has focused on special graph classes such as complete graphs and complete bipartite graphs, which are the graph classes we consider here. Ailon, Charikar, and Newman [2, 3] gave a very simple randomized 3-approximation algorithm for MinDisagree on complete graphs. This algorithm was derandomized by van Zuylen and Williamson [24], and a parallel version of the algorithm was studied by Pan, Papailiopoulos, Recht, Ramchandran, and Jordan [20]. More recently, a 2.06-approximation algorithm was announced by Chawla, Makarychev, Schramm and Yaroslavtsev [12]. Similar results have been obtained for complete bipartite graphs. The first constant approximation algorithm for correlation clustering on complete bipartite graphs was described by Amit [4], who gave an 11-approximation algorithm. This ratio was improved by Ailon, Avigdor-Elgrabli, Liberty and van Zuylen [1], who obtained a 4-approximation algorithm. Chawla, Makarychev, Schramm and Yaroslavtsev [12] announced a 3-approximation algorithm for correlation clustering on complete k-partite graphs, for arbitrary k, which includes the complete bipartite case. Bipartite clustering has also been studied, outside the correlation-clustering context, by Lim, Chen, and Xu [19].\nWe depart from the classical correlation-clustering literature by considering a broader class of objective functions which also cater to the need of many community-detection applications in machine learning, social sciences, recommender systems and bioinformatics [13, 22, 18]. The technical details of this class of functions can be found in Section 2. As a representative example of this class, we introduce minimax correlation clustering.\nIn minimax clustering, rather than seeking to minimize the total number of errors, we instead seek to minimize the number of errors at the worst-off vertex in the clustering. Put more formally, if for a given clustering each vertex v has yv incident edges that are errors, then we wish to find a clustering that minimizes maxv yv.\nMinimax clustering, like classical correlation clustering, is NP-hard on complete graphs, as we prove in Appendix C. To design approximation algorithms for minimax clustering, it is necessary to bound the growth of errors locally at each vertex when we round from a fractional clustering to a discrete clustering; this introduces new difficulties in the design and analysis of our rounding algorithm. These new technical difficulties cause the algorithm of [2, 3] to fail in the minimax context, and there is no obvious way to adapt that algorithm to this new context; this phenomenon is explored further in Appendix A.\nMinimax correlation clustering on graphs is relevant in detecting communities, such as gene, social network, or voter communities, in which no antagonists are allowed. Here, an antagonist refers to an entity that has properties inconsistent with a large number of members of the community. Alternatively, one may view the minimax constraint as enabling individual vertex quality control within the clusters, which is relevant in biclustering applications such as collaborative filtering for recommender systems, where minimum quality recommendations have to be ensured for each user in a given category. As an illustrative example, one may view a complete bipartite graph as a preference model in which nodes on the left represent viewers and nodes on the right represent movies. A positive edge between a user and a movie indicates that the viewer likes the movie, while a negative edge indicates that they do not like or have not seen the movie. We may be interested in finding communities of viewers for the purpose of providing them with joint recommendations. Using a minimax objective function here allows us to provide a uniform quality of recommendations, as we seek to minimize the number of errors for the user who suffers the most errors.\nA minimax objective function for a graph partitioning problem different from correlation clustering was previously studied by [7]. In that paper, the problem under consideration was to split a graph into k roughlyequal-sized parts, minimizing the total number of edges leaving any part. Thus, the minimum in [7] is being taken over the parts of the solution, rather than minimizing over vertices as we do here.\nAnother idea slightly similar to minimax clustering has previously appeared in the literature on fixedparameter tractability of the Cluster Editing problem, which is an equivalent formulation of Correlation Clustering. In particular, Komusiewicz and Uhlmann [17] proved that the following problem is fixedparameter tractable for the combined parameter (d, t):\n(d, t)-Constrained-Cluster Editing Input: A labeled complete graph G, a function \u03c4 : V (G) \u2192 {0, . . . , t}, and nonnegative integers d and k. Question: Does G admit a clustering into at most d clusters with at most k errors such that every vertex v is incident to at most \u03c4(v) errors?\n(Here, we have translated their original formulation into the language of correlation clustering.) Komusiewicz and Uhlmann also obtained several NP-hardness results related to this formulation of the problem. While their work involves a notion of local errors for correlation clustering, their results are primarily focused on fixed-parameter tractability, rather than approximation algorithms, and are therefore largely orthogonal to the results of this paper.\nThe contributions of this paper are organized as follows. In Section 2, we introduce and formally express our framework for the generalized version of correlation clustering, which includes both classical clustering and minimax clustering as special cases. In Section 3, we give a rounding algorithm which allows the development of constant-factor approximation algorithms for the generalized clustering problem. In Section 4, we give a version of this rounding algorithm for complete bipartite graphs.\nIn Appendix A, we discuss minimax clustering in more detail, and show that algorithms similar to the Ailon\u2013Charikar\u2013Newman algorithm fail in the minimax context. In Appendix B we discuss the approximation properties of the MaxAgree formulation of minimax clustering, where the objective is to maximize the\nnumber of correct edges, rather than minimize the number of incorrect edges, at the worst vertex. In Appendix C and Appendix D we prove that the minimax correlation clustering problem is NP-hard on complete graphs and complete bipartite graphs, respectively. Appendix E contains technical details for various proofs."}, {"heading": "2 Framework and Formal Definitions", "text": "In this section, we formally set up the framework we will use for our broad class of correlation-clustering objective functions.\nDefinition 1. Let G be an edge-labeled graph. A discrete clustering (or just a clustering) of G is a partition of V (G). A fractional clustering of G is a vector x indexed by (\nV (G) 2\n) such that xuv \u2208 [0, 1] for all uv \u2208 ( V (G) 2 )\nand such that xvz \u2264 xvw + xwz for all distinct v, w, z \u2208 V (G).\nIf x is a fractional clustering, we can view xuv as a \u201cdistance\u201d from u to v; the constraints xvz \u2264 xvw+xwz are therefore referred to as triangle inequality constraints. We also adopt the convention that xuu = 0 for all u.\nIn the special case where all coordinates of x are 0 or 1, the triangle inequality constraints guarantee that the relation defined by u \u223c v iff xuv = 0 is an equivalence relation. Such a vector x can therefore naturally be viewed as a discrete clustering, where the clusters are the equivalence classes under \u223c. By viewing a discrete clustering as a fractional clustering with integer coordinates, we see that fractional clusterings are a continuous relaxation of discrete clusterings, which justifies the name. This gives a natural notion of the total weight of errors at a given vertex.\nDefinition 2. Let G be an edge-labeled complete graph, and let x be a fractional clustering of G. The error vector of x with respect to G, written err(x), is a real vector indexed by V (G) whose coordinates are defined by\nerr(x)v = \u2211\nw\u2208N+(v)\nxvw + \u2211\nw\u2208N\u2212(v)\n(1\u2212 xvw).\nIf C is a clustering of G and xC is the natural associated fractional clustering, we define err(C) as err(xC).\nWe are now prepared to formally state the optimization problem we wish to solve. Let Rn\u22650 denote the set of vectors in Rn with all coordinates nonnegative. Our problem is parameterized by a function f : Rn\u22650 \u2192 R.\nf-Correlation Clustering Input: A labeled graph G. Output: A clustering C of G. Objective: Minimize f(err(C)).\nIn order to approximate f -Correlation Clustering, we introduce a relaxed version of the problem.\nFractional f-Correlation Clustering Input: A labeled graph G. Output: A fractional clustering x of G. Objective: Minimize f(err(x)).\nIf f is convex on Rn\u22650, then using standard techniques from convex optimization [8], the Fractional f - Correlation Clustering problem can be approximately solved in polynomial time, as the composite function f \u25e6 err is convex and the constraints defining a fractional clustering are linear inequalities in the variables xe. When G is a complete graph, we then employ a rounding algorithm based on the algorithm of Charikar, Guruswami, and Wirth [9, 10] to transform the fractional clustering into a discrete clustering. Under rather modest conditions on f , we are able to obtain a constant-factor bound on the error growth, that is, we can produce a clustering C such that f(err(C)) \u2264 cf(err(x)), where c is a constant not depending on f or x. In particular, we require the following assumptions on f .\nAssumption A. We assume that f : Rn\u22650 \u2192 R has the following properties.\n(1) f(cy) \u2264 cf(y) for all c \u2265 0 and all y \u2208 Rn, and\n(2) If y, z \u2208 Rn\u22650 are vectors with yi \u2264 zi for all i, then f(y) \u2264 f(z).\nUnder Assumption A, the claim that f(err(C)) \u2264 cf(err(x)) follows if we can show that err(C)v \u2264 c err(x)v for every vertex v \u2208 V (G). This is the property we prove for our rounding algorithms.\nWe will slightly abuse terminology by referring to the constant c as an approximation ratio for the rounding algorithm; this notation is motivated by the fact that when f is linear, the Fractional f -Correlation Clustering problem can be solved exactly in polynomial time, and applying a rounding algorithm with constant c to the fractional solution yields a c-approximation algorithm to the (discrete) f -Correlation Clustering problem. In contrast, when f is nonlinear, we may only be able to obtain a (1+ \u01eb)-approximation for the Fractional f -Correlation Clustering problem, in which case applying the rounding algorithm yields a c(1 + \u01eb)-approximation algorithm for the discrete problem.\nA natural class of convex objective functions obeying Assumption A is the class of \u2113p norms. For all p \u2265 1, the \u2113p-norm on Rn is defined by\n\u2113p(x) =\n(\nn \u2211\ni=1\n|xi| p\n)1/p\n.\nAs p grows larger, the \u2113p-norm puts more emphasis on the coordinates with larger absolute value. This justifies that definition of the \u2113\u221e-norm as\n\u2113\u221e(x) = max{x1, . . . , xn}.\nClassical correlation clustering is the case of f -Correlation Clustering where f(x) = 1n\u2113 1(x), while minimax correlation clustering is the case of f -Correlation Clustering where f(x) = \u2113\u221e(x). Our emphasis on convex f is due to the fact that convex programming techniques allow the Fractional f -Correlation Clustering problem to be approximately solved in polynomial time when f is convex. However, the correctness of our rounding algorithm does not depend on the convexity of f , only on the properties listed in Assumption A. If f is nonconvex and obeys Assumption A, and we produce a \u201cgood\u201d fractional clustering x by some means, then our algorithm still produces a discrete clustering C with f(err(C)) \u2264 cf(err(x))."}, {"heading": "3 A Rounding Algorithm for Complete Graphs", "text": "We now describe a rounding algorithm to transform an arbitrary fractional clustering x of a labeled complete graph G into a clustering C such that err(C)v \u2264 c err(x)v for all v \u2208 V (G).\nOur rounding algorithm is based on the algorithm of Charikar, Guruswami, and Wirth [9, 10] and is shown in Algorithm 1. The main difference between Algorithm 1 and the algorithm of [9, 10] is the new strategy of choosing a pivot vertex that maximizes |T \u2217u |; in [9, 10], the pivot vertex is chosen arbitrarily. Furthermore, the algorithm of [9, 10] always uses \u03b1 = 1/2 as a cutoff for forming \u201ccandidate clusters\u201d, while we express \u03b1 as a parameter which we later choose in order to optimize the approximation ratio.\nUnder the classical objective function, an optimal fractional clustering is the solution to a linear program, which motivates the following notation for the more general case.\nDefinition 3. If uv is an edge of a labeled graph G, we define the LP-cost of uv relative to a fractional clustering x to be xuv if uv \u2208 E+, and 1 \u2212 xuv if uv \u2208 E\u2212. Likewise, the cluster-cost of an edge uv is 1 if uv is an error in the clustering produced by Algorithm 1, and 0 otherwise.\nOur general strategy for obtaining the constant-factor error bound for Algorithm 1 is similar to that of [9, 10]. Each time a cluster is output, we pay for the cluster-cost of the errors incurred by \u201ccharging\u201d the cost of these errors to the LP-costs of the fractional clustering. The main difference between our proof and the proof of [9, 10] is that we must pay for errors locally: for each vertex v, we must pay for all clustering errors incident to v by charging to the LP cost incident to v. In particular, every clustering error must now be paid for at each of its endpoints, while in [9, 10], it was enough to pay for each clustering error at one of its endpoints. For edges which cross between a cluster and its complement, this requires a different analysis\nAlgorithm 1 Round fractional clustering x to obtain a discrete clustering, using threshold parameters \u03b1, \u03b3 with 0 < \u03b3 < \u03b1 < 1/2.\nLet S = V (G). while S 6= \u2205 do For each u \u2208 S, let Tu = {w \u2208 S \u2212 {u} : xuw \u2264 \u03b1} and let T \u2217u = {w \u2208 S \u2212 {u} : xuw \u2264 \u03b3}. Choose a pivot vertex u \u2208 S that maximizes |T \u2217u |. Let T = Tu. if \u2211\nw\u2208T xuw \u2265 \u03b1 |T | /2 then Output the cluster {u}. {Type 1 cluster} Let S = S \u2212 {u}.\nelse\nOutput the cluster {u} \u222a T . {Type 2 cluster} Let S = S \u2212 ({u} \u222a T ).\nend if\nend while\nat each endpoint, a difficulty which was not present in [9, 10]. Our proof emphasizes the solutions to these new technical problems; the parts of the proof that are technically nontrivial but follow earlier work are omitted due to space constraints but can be found in Appendix E.\nObservation 4. Let x be a fractional clustering of a graph G, and let w, z \u2208 V (G). For any vertex u, we have xwz \u2265 xuz \u2212 xuw and 1\u2212 xwz \u2265 1\u2212 xuz \u2212 xuw.\nTheorem 5. Let G be a labeled complete graph, let \u03b1 and \u03b3 be parameters with 0 < \u03b3 < \u03b1 < 1/2, and let x be any fractional clustering of G. If C is the clustering produced by Algorithm 1 with the given input, then for all v \u2208 V (G) we have err(C)v \u2264 c err(x)v , where c is a constant depending only on \u03b1 and \u03b3.\nProof. Let k1, k2, k3 be constants to be determined, with 1/2 < k1 < 1 and 0 < 2k2 \u2264 k3 < 1/2. Also assume that k1\u03b1 > \u03b3 and that k2\u03b1 \u2264 1\u2212 2\u03b1.\nTo prove the approximation ratio, we consider the cluster-costs incurred as each cluster is output, splitting into cases according to the type of cluster. In our analysis, as the algorithm runs, we will mark certain vertices as \u201csafe\u201d, representing the fact that some possible future clustering costs have been paid for in advance. Initially, no vertex is marked as safe.\nCase 1: A Type 1 cluster is output. Let X = S\u2229N+(u), with S as in Algorithm 1. The new cluster-cost incurred at u is |X |, and for each v \u2208 X , a new cluster-cost of 1 is incurred at v.\nFirst we pay for the new cluster cost incurred at u. For each edge uv with v \u2208 T , we have xuv \u2264 \u03b1 and so 1 \u2212 xuv \u2265 1 \u2212 \u03b1 \u2265 xuv. Thus, the total LP cost of edges uv with v \u2208 T is at least \u2211\nv\u2208T xuv, which is at least \u03b1 |T | /2 since {u} is output as a Type 1 cluster. Thus, charging each edge uv with v \u2208 T a total of 2/\u03b1 times its LP-cost pays for the cluster-cost of any positive edges from u to T . On the other hand, if uv is a positive edge with v \u2208 S \u2212 T , then since v /\u2208 T , we have xuv \u2265 \u03b1. Hence, the LP-cost of uv is at least \u03b1, and charging 1/\u03b1 times the LP-cost of uv pays for the cluster-cost of this edge.\nNow let v \u2208 X ; we must pay for the new cluster cost at v. If xuv \u2265 k2\u03b1, then the edge uv already incurs LP cost at least k2\u03b1, so the new cost at v is only 1/(k2\u03b1) times the LP-cost of the edge uv. So assume xuv < k2\u03b1. In this case, we say that u is a bad pivot for v.\nFirst suppose that v is not safe (as is initially the case). We will make a single charge to the edges incident to v that is large enough to pay for both the edge uv and for all possible future bad pivots, and then we will mark v as safe to indicate that we have done this. The basic idea is that if v has many possible bad pivots, then since xuv is \u201csmall\u201d, all of these possible bad pivots are also close to u, thus included in Tu. Since \u2211\nw\u2208Tu xuw \u2265 \u03b1 |Tu| /2, there is a large set B \u2286 Tu of vertices that are \u201cmoderately far\u201d from u,\nand therefore moderately far from v. The number of these vertices grows with the number of bad pivots, so charging all the edges vz for z \u2208 B is sufficient to pay for all bad pivots.\nWe now make this argument rigorous. Let Pv be the set of potential bad pivots for v, defined by\nPv = {p \u2208 S : xvp < k2\u03b1}.\nNote that u \u2208 Pv. Since k2 < 1/4, we have xup \u2264 xuv + xvp < \u03b1/2 for all p \u2208 Pv; hence Pv \u2286 T . Define the vertex set B by\nB = {z \u2208 T : xuz > k3\u03b1}.\nSince xuz \u2264 \u03b1 for all z \u2208 T , we see that \u2211\nz\u2208T\nxuz \u2264 k3\u03b1 |T \u2212B|+ \u03b1 |B| .\nOn the other hand, since {u} is output as a Type 1 cluster, we have \u2211\nz\u2208T\nxuz \u2265 \u03b1 |T | /2.\nCombining these inequalities and rearranging, we obtain |B| \u2265 (1\u2212 2k3) |T \u2212B|. For each vertex z \u2208 B, we have xvz \u2265 xuz \u2212 xuv \u2265 (k3 \u2212 k2)\u03b1; in particular, since k3 \u2265 2k2, we have xvz \u2265 k2\u03b1, so that z /\u2208 Pv. Hence |T \u2212B| \u2265 |Pv|, and we have |B| \u2265 (1\u2212 2k3) |Pv|.\nOn the other hand, for z \u2208 B we also have 1 \u2212 xvz \u2265 1 \u2212 xuv \u2212 xuz \u2265 1 \u2212 (1 + k2)\u03b1. It follows that each edge vz for z \u2208 B has LP-cost at least min((k3 \u2212 k2)\u03b1, 1 \u2212 (1 + k2)\u03b1), independent of whether vz is positive or negative. It is easy to check that since \u03b1 < 1/2 and k3 < 1, this minimum is always achieved by (k3 \u2212 k2)\u03b1. Therefore, we can pay for the (possible) Type-1-cluster cost of all edges vp for p \u2208 Pv by charging each edge vz with z \u2208 B a total of\n1\n(1\u2212 2k3)(k3 \u2212 k2)\u03b1\ntimes its LP-cost. We make all these charges when the cluster {u} is created and put them in a \u201cbank account\u201d to pay for later Type-1-cluster costs for v. Then we mark v as safe. The total charge in the bank account is at least |Pv|, which is enough to pay for all bad pivots for v.\nWe have just described the case where u is a bad pivot and v is not safe. On the other hand, if u is a bad pivot and v is safe, then v already has a bank account large enough to pay for all its bad pivots, and we simply charge 1 to the account to pay for the edge uv.\nCase 2: A Type 2 cluster {u} \u222a T is output. The negative edges within {u} \u222a T are easy to pay for: if vw if a negative edge inside {u}\u222aT , then we have 1\u2212 xvw \u2265 1\u2212 xuv \u2212 xuw \u2265 1\u2212 2\u03b1, so we can pay for each of these edges by charging a factor of 11\u22122\u03b1 times its LP-cost.\nThus, we consider edges joining {u} \u222a T with S \u2212 ({u} \u222a T ). We call these edges cross-edges for their endpoints. A standard argument (see Appendix E) shows that for z \u2208 S \u2212 ({u} \u222a T ), the total cluster-cost of the cross-edges for z is at most max{1/(1 \u2212 2\u03b1), 2/\u03b1} times the LP-cost of those edges, so the vertices outside {u} \u222a T can be dealt with easily.\nHowever, we also must bound the cluster-cost at vertices inside {u} \u222a T . This is where we use the maximality of |T \u2217u |.\nLet w \u2208 {u} \u222a T . First consider the positive cross-edges wz such that xwz \u2265 \u03b3. Any such edge has cluster-cost 1 and already has LP-cost at least \u03b3, so charging 1/\u03b3 times the LP-cost to such an edge pays for its cluster cost. Now let X = {z \u2208 S \u2212 ({u}\u222a T ) : xwz < \u03b3}; we still must pay for the edges wz with z \u2208 X .\nIf xuw \u2264 k1\u03b1, which includes the case u = w, then for all z \u2208 X , we have xwz \u2265 xuz \u2212 xuw \u2265 \u03b1\u2212 k1\u03b1 = (1 \u2212 k1)\u03b1. Hence, for any positive edge wz with z \u2208 X , the LP-cost of wz is at least (1 \u2212 k1)\u03b1, and so the cluster cost of the edge wz is at most 1/((1 \u2212 k1)\u03b1) times the LP cost. Charging this factor to each cross-edge pays for the cluster-cost of each cross-edge.\nNow suppose xuw > k1\u03b1. Since k1\u03b1 > \u03b3, this implies w /\u2208 T \u2217u . In this case, it is possible that w may have many positive neighbors z \u2208 X for which xwz is quite small, so we cannot necessarily pay for the cluster-cost of the edges joining w and X by using their LP-cost. Instead, we charge their cluster-cost to the LP-cost of edges within T .\nObserve that X \u2286 T \u2217w, and hence |T \u2217 w| \u2265 |X |. By the maximality of |T \u2217 u |, this implies that |T \u2217 u | \u2265 |X |.\nNow for any v \u2208 T \u2217u , we have the following bounds:\nxwv \u2265 xuw \u2212 xuv \u2265 k1\u03b1\u2212 \u03b3,\n1\u2212 xwv \u2265 1\u2212 xuw \u2212 xuv \u2265 1\u2212 \u03b1\u2212 \u03b3.\nSince \u03b1 < 1/2 and k1 \u2264 1, we have k1\u03b1 \u2264 \u03b1 < 1 \u2212 \u03b1, so these lower bounds imply that each edge wv with v \u2208 T \u2217u has LP-cost at least k1\u03b1 \u2212 \u03b3, independent of whether wv is a positive or negative edge. Thus, the total LP cost of edges joining w to T \u2217u is at least (k1\u03b1\u2212 \u03b3) |T \u2217 u |.\nSince the total cluster-cost of edges joining w and X is at most |X | and since |T \u2217u | \u2265 |X |, we can pay for these edges by charging each edge wv with v \u2208 T \u2217u a factor of 1 k1\u03b1\u2212\u03b3 times its LP-cost.\nHaving paid for all cluster-costs, we now look at the total charge accrued at each vertex. Fix any vertex v and an edge vw incident to v. We bound the total amount charged to vw by v in terms of the LP-cost of vw. There are three distinct possibilities for the edge vw: either vw ended inside a cluster, or v was clustered before w, or w was clustered before v.\nCase 1: vw ended within a cluster. In this case, v may have made the following charges:\n\u2022 A charge of 1(1\u22122k3)(k3\u2212k2)\u03b1 times the LP-cost, to pay for a \u201cbank account\u201d for v,\n\u2022 A charge of 11\u22122\u03b1 times the LP-cost, to pay for vw itself if vw is a negative edge,\n\u2022 A charge of 1k1\u03b1\u2212\u03b3 times the LP-cost, to pay for positive edges leaving the v-cluster.\nThus, in this case the total cost charged to vw by v is at most c1 times the LP-cost of vw, where\nc1 = 1\n(1\u2212 2k3)(k3 \u2212 k2)\u03b1 +\n1\n1\u2212 2\u03b1 +\n1\nk1\u03b1\u2212 \u03b3 .\nCase 2: v was clustered before w. In this case, v may have made the following charges:\n\u2022 A charge of 1(1\u22122k3)(k3\u2212k2)\u03b1 times the LP-cost, to pay for a \u201cbank account\u201d for v,\n\u2022 A charge of at most 2\u03b1 times the LP-cost, to pay for all cross-edges if v was output as a Type 1 cluster,\n\u2022 A charge of at most max {\n1 (1\u2212k1)\u03b1 , 1\u03b3\n}\ntimes the LP-cost, to pay for vw if v was output in a Type 2\ncluster.\nNote that k1 > 1/2 implies that 1 (1\u2212k1)\u03b1 \u2265 2\u03b1 , so we may disregard the case where v is output as a Type 1 cluster. Thus, in this case the total cost charged to vw by v is at most c2 times the LP-cost of vw, where\nc2 = 1\n(1\u2212 2k3)(k3 \u2212 k2)\u03b1 +max\n{\n1 (1\u2212 k1)\u03b1 , 1 \u03b3\n}\n.\nCase 3: w was clustered before v. In this case, v may have made the following charges:\n\u2022 A charge of at most 1(1\u22122k3)(k3\u2212k2)\u03b1 times the LP-cost, to pay for a \u201cbank account\u201d for v,\n\u2022 A charge of at most 1k2\u03b1 times the LP-cost, to pay for the cluster-cost of vw if vw is a positive edge and w was output as a Type 1 cluster,\n\u2022 A charge of at most\nmax\n{\n1 1\u2212 2\u03b1 , 2 \u03b1\n}\ntimes the LP-cost, to pay for vw if w was output in a Type 2 cluster.\nClearly vw cannot receive both the second and third types of charge. Furthermore, since k2 \u2264 1/4, we have 1 k2\u03b1 \u2265 2\u03b1 . Since k2\u03b1 \u2264 1 \u2212 2\u03b1, we see that 1 k2\u03b1\nis the largest charge that vw could receive from either the second or third type of charge. Thus, in this case the total cost charged to vw by v is at most c3 times the LP-cost, where\nc3 = 1\n(1\u2212 2k3)(k3 \u2212 k2)\u03b1 +\n1\nk2\u03b1.\nThus, the approximation ratio of the algorithm is at most max{c1, c2, c3}. We wish to choose the various parameters to make this ratio as small as possible, subject to the various assumptions on the parameters\nrequired for the correctness of the proof. It seems difficult to obtain an exact solution to this optimization problem. Solving the problem numerically, we obtained the following values for the parameters:\n\u03b1 = 0.465744 \u03b3 = 0.0887449\nk1 = 0.767566 k2 = 0.117219 k3 = 0.308433.\nThese parameters yield an approximation ratio of roughly 48."}, {"heading": "4 A Rounding Algorithm for One-Sided Biclustering", "text": "In this section, we consider a version of the f -Correlation Clustering problem on complete bipartite graphs. Let G be a complete bipartite graph with edges labeled + and \u2212, and let V1 and V2 be its partite sets. We will obtain a rounding algorithm that transforms any fractional clustering x into a discrete clustering C such that err(C)v \u2264 c err(x)v for all v \u2208 V1. Our algorithm is shown in Algorithm 2.\nOur algorithm does not guarantee any upper bound on err(C)v for v \u2208 V2: as the algorithm treats the sides V1 and V2 asymmetrically, it is difficult to control the per-vertex error at V2. Nevertheless, an error guarantee for the vertices in V1 suffices for some applications. Our approach is motivated by applications in recommender systems, where vertices in V1 correspond to users, while vertices in V2 correspond to objects to be ranked. In this context, quality of service conditions only need to be imposed for users, and not for objects.\nAlgorithm 2 Round fractional clustering to obtain a discrete clustering, using threshold parameters \u03b1, \u03b3 with \u03b1 < 1/2 and \u03b3 < \u03b1.\nLet S = V (G). while V1 \u2229 S 6= \u2205 do For each u \u2208 V1 \u2229 S, let Tu = {w \u2208 S \u2212 {u} : xuw \u2264 \u03b1} and let T \u2217u = {w \u2208 V2 \u2229 S : xuw \u2264 \u03b3}. Choose a pivot vertex u \u2208 V1 \u2229 S that maximizes |T \u2217u |. Let T = Tu. if \u2211\nw\u2208V2\u2229T xuw \u2265 \u03b1 |V2 \u2229 T | /2 then\nOutput the singleton cluster {u}. {Type 1 cluster} Let S = S \u2212 {u}.\nelse\nOutput the cluster {u} \u222a T . {Type 2 cluster} Let S = S \u2212 ({u} \u222a T ).\nend if\nend while Output each remaining vertex of V2 \u2229 S as a singleton cluster.\nTheorem 6. Let G be a labeled complete bipartite graph with partite sets V1 and V2, let \u03b1, \u03b3 be parameters as described in Algorithm 2, and let x be any fractional clustering of G. If C is the clustering produced by Algorithm 2 with the given input, then for all v \u2208 V1 we have err(C)v \u2264 c err(x)v, where c is a constant depending only on \u03b1 and \u03b3.\nWe note that the proof of Theorem 6 is actually simpler than the proof of Theorem 5, because the focus on errors only at V1 eliminates the need for the \u201cbad pivots\u201d argument used in Theorem 6. This also leads to a smaller value of c in Theorem 6 than we were able to obtain in Theorem 5.\nProof. As before, we make charges to pay for the new cluster costs at each vertex of V1 as each cluster is output, splitting into cases according to the type of cluster. Let k1 be a constant to be determined, with k1\u03b1 > \u03b3.\nCase 1: A Type 1 cluster {u} is output. In this case, the only cluster costs incurred are the positive edges incident to u, all of which have their other endpoint in V2. The averaging argument used in Case 1 of Section 3 shows that charging every edge incident to u a factor of 2/\u03b1 times its LP cost pays for the cluster cost of all such edges.\nCase 2: A Type 2 cluster {u} \u222a T is output. Negative edges within the cluster are easy to pay for: if w1w2 is a negative edge within the cluster, with wi \u2208 Vi, then we have\n1\u2212 xw1w2 \u2265 1\u2212 xuw1 \u2212 xuw2 \u2265 1\u2212 2\u03b1,\nso we can pay for the cluster-cost of such an edge by charging it a factor of 1/(1\u2212 2\u03b1) times its LP-cost. We still must pay for positive edges joining the cluster with the rest of S; we call such edges cross-edges. Each such edge must be paid for at its endpoint in V1. If z \u2208 V1 is a vertex outside the cluster, then a standard argument (see Appendix E) shows that the cross-edges for z can be paid for by charging each such edge a factor of max{1/(1\u2212 2\u03b1), 2/\u03b1)} times its LP cost.\nNow let w \u2208 V1 be a vertex inside the cluster. We must pay for the cross-edges incident to w using the LP-cost of the edges incident to w. First consider the positive edges from w to vertices z outside the cluster such that xwz \u2265 \u03b3. Any such edge has cluster-cost 1 and LP-cost at least \u03b3, so charging each such edge a factor of 1/\u03b3 times its LP-cost pays for its cluster cost. Let X = {z \u2208 (S \u2229 V2)\u2212 T : xwz < \u03b3}; we must pay for the edges wz with z \u2208 X . Note that xuz > \u03b1 for all z \u2208 X , since z \u2208 X implies z /\u2208 T .\nIf xuw \u2264 k1\u03b1, then for all z \u2208 X , we have\nxwz \u2265 xuz \u2212 xuw \u2265 (1\u2212 k1)\u03b1.\nHence, for any positive cross-edge wz with z \u2208 X , the LP-cost of wz is at least (1\u2212 k1)\u03b1, and so we can pay for the cluster-cost of wz by charging wz a factor of 1(1\u2212k1)\u03b1 times its LP-cost.\nNow suppose xuw > k1\u03b1. As before, we pay for the cross-edges by charging the edges inside the cluster. Observe that |T \u2217w| \u2265 |X |. Since u was chosen to maximize |T \u2217 u |, this implies that |T \u2217 u | \u2265 |X |. For any v \u2208 T \u2217 u , we have xwv \u2265 xuw \u2212 xuv \u2265 k1\u03b1\u2212 \u03b3.\nOn the other hand, for any v \u2208 T \u2217u we also have\n1\u2212 xwv \u2265 1\u2212 xuw \u2212 xuv \u2265 1\u2212 \u03b1\u2212 \u03b3 \u2265 \u03b1\u2212 \u03b3.\nSince k1 \u2264 1, it follows that the edge wv has LP-cost at least k1\u03b1\u2212 \u03b3 independent of whether wv is positive or negative. Thus, the total LP cost of edges joining w to T \u2217u is at least (k1\u03b1\u2212 \u03b3) |T \u2217 u |.\nSince the total cluster-cost of the cross- edges joining w and X is at most |X | and since |T \u2217u | \u2265 |X |, we can pay for the cross-edges by charging each edge wv with v \u2208 T \u2217u a factor of 1 k1\u03b1\u2212\u03b3 times its LP-cost.\nHaving paid for all cluster-costs, we now look at the total charge accrued at each vertex. Fix a vertex v \u2208 V1 and an edge vw incident to v. We bound the total amount charged to vw by v in terms of the LP-cost of vw. There are three distinct possibilities for the edge vw: either vw ended inside a cluster, or v was clustered before w, or w was clustered before v.\nCase 1: vw ended within a cluster. In this case, v may have made the following charges:\n\u2022 A charge of at most 11\u22122\u03b1 times the LP cost, to pay for vw itself if vw is a negative edge,\n\u2022 A charge of 1k1\u03b1\u2212\u03b3 times the LP-cost, to pay for positive edges leaving the v-cluster.\nThus, in this case the total cost charged to vw by v is at most c1 times the LP-cost of vw, where\nc1 = 1\n1\u2212 2\u03b1 +\n1\nk1\u03b1\u2212 \u03b3 .\nCase 2: v was clustered before w. In this case, v may have made the following charges:\n\u2022 A charge of 2/\u03b1 times the LP cost, to pay for vw if v was output as a singleton,\n\u2022 A charge of max{ 1(1\u2212k1)\u03b1 , 1 \u03b3 } times the LP cost, to pay for vw if v was output in a nonsingleton cluster,\nSince v makes at most one of the charges above, the total cost charged to vw by v is at most c2 times the LP-cost of vw, where\nc2 = max\n{\n1 (1\u2212 k1)\u03b1 , 1 \u03b3 , 2 \u03b1\n}\n.\nCase 3: w was clustered before v. In this case, v may have made the following charges:\n\u2022 A charge of at most max{ 11\u22122\u03b1 , 2 \u03b1} times the LP cost, to pay for cross-edges at v if w is output in a\nnonsingleton cluster.\nThus, in this case the total cost charged to vw by v is at most c3 times the LP-cost of vw, where\nc3 = max\n{\n1 1\u2212 2\u03b1 , 2 \u03b1\n}\n.\nThe approximation ratio is max{c1, c2, c3}. Numerically, we obtain an approximation ratio of at most 10 by taking the following parameter values:\n\u03b1 = 0.377 \u03b3 = 0.102 k1 = 0.730"}, {"heading": "5 Acknowledgments", "text": "The authors thank Dimitris Papailiopoulos for helpful discussions that led to the example in Appendix A. The authors also acknowledge funding from the NSF grants IOS 1339388 and CCF 1527636, 1526875, 1117980. Research of the first author was supported by the IC Postdoctoral Program."}, {"heading": "A Minimax Clustering and the Failure of Pivoting Algorithms", "text": "In this appendix, we consider minimax clustering, which is the special case of f -Correlation Clustering where f(y) = maxv\u2208V (G) yv. Thus, in minimax clustering, we seek to minimize the number of errors at the worst vertex in the clustering. Equivalently, we are trying to minimize the \u2113\u221e-norm of the error vector, in contrast to classical correlation clustering, where we are trying to minimize the \u21131-norm.\nMinimax clustering is a representative example of the difficulties which arise in moving from classical correlation clustering to the more general f -Correlation Clustering problem. We will show that some techniques which work well for the classical correlation clustering problem break down in the minimax context.\nAilon, Charikar, and Newman [2, 3] gave a beautifully simple randomized 3-approximation algorithm for classical correlation clustering on complete graphs. Their algorithm is shown in Algorithm 3. Since our rounding clustering in Section 3 is based on the Charikar\u2013Guruswami\u2013Wirth algorithm with a modified pivoting rule, it is natural to ask whether a similar modification to the Ailon\u2013Charikar\u2013Newman algorithm also yields a constant-factor approximation algorithm for minimax clustering.\nAlgorithm 3 Ailon\u2013Charikar\u2013Newman algorithm [2, 3].\nLet S = V (G). while S 6= \u2205 do Pick v \u2208 S uniformly at random. Let T = ({v} \u222aN+(v)) \u2229 S. Output the cluster T . Let S = S \u2212 T . end while\nUnfortunately, it seems that there are severe obstacles to modifying the ACN algorithm in this manner. For any positive integer t, let Mt be a graph on 2t vertices consisting of t pairwise disjoint edges, and let Gt be the labeling of K2t in which the edges of Mt are labeled \u2212 and all other edges are labeled +.\nClearly, if all vertices of Gt are placed in the same cluster (the \u201cgiant clustering\u201d), then there is only 1 error at each vertex of Gt. We show that all other clusterings of Gt have many more errors at some vertex.\nLemma 7. If C is a clustering of Gt with more than 1 cluster, then some vertex of Gt has at least t \u2212 1 errors in C.\nProof. Let X be the smallest cluster in C. Since C has at least 2 clusters, we have |X | \u2264 t. For any v \u2208 X , there is at most one w /\u2208 X such that vw is a negative edge. Hence, each v \u2208 X has at least t\u2212 1 incident errors.\nBy Lemma 7, any constant-factor randomized algorithm for minimax clustering must return the giant clustering for Gt with probability 1 \u2212 O(1/t). On the other hand, if we modify Algorithm 3 by changing the rule for choosing the pivot vertex v, the resulting algorithm still cannot produce the giant clustering. It is difficult to see how Algorithm 3 could sensibly be modified in order to return the giant clustering for Gt with high enough probability.\nWe now consider the behavior of Algorithm 1 on the graph Gt. While the minimax objective function is not linear in the variables xuv, we can still model the f -Fractional Correlation Clustering problem using the linear program L shown in Figure 1.\nSince the algorithm presented in Section 3 yields a constant-factor approximation algorithm for minimax clustering, and since every clustering of Gt other than the giant clustering has t\u2212 1 errors at some vertex, it is necessary that our rounding algorithm, applied to an optimal solution of L, returns the giant clustering for all sufficiently large t. This follows immediately from the following result.\nProposition 8. Let L be the linear program shown in Figure 1, as formulated for Gt. If t \u2265 3, then the unique optimal solution to L has xuv = 0 for all uv \u2208 E(G).\nProof. The dual program to L is shown in Figure 2, with the following variables:\n\u2022 For each v \u2208 V (Gt), a variable \u03c0v corresponding to the constraint \u2211 w\u2208N+(v) xvw + \u2211 w\u2208N\u2212(v)(1 \u2212\nxvw) \u2264 M ,\n\u2022 For each ordered triple (u, v, z) where u, v, z are distinct vertices of V (Gt), a variable \u03c3(u,v,z) corresponding to the constraint xuv \u2264 xuz + xzv.\nFor convenience of notation, we also introduce the abbreviation \u03c3\u0302u,v to stand for \u2211 z\u2208V (G)\u2212{u,v}(\u2212\u03c3u,v,z \u2212 \u03c3v,u,z + \u03c3z,u,v + \u03c3z,v,u + \u03c3u,z,v + \u03c3v,z,u). Observe that there are exactly 2t\u2212 2 choices of z to sum over. Now we define a dual solution. Let u\u2032u\u2032\u2032 be an edge of the negative matching. Consider the dual solution defined below:\n\u03c0u\u2032 = \u03c0u\u2032\u2032 = 1/2, \u03c3u\u2032,u\u2032\u2032,z = 1/(2t\u2212 2) for all z /\u2208 {u \u2032, u\u2032\u2032},\n\u03c0v = 0 for all v /\u2208 {u \u2032, u\u2032\u2032}, \u03c3u,v,z = 0 if (u, v) 6= (u \u2032, u\u2032\u2032).\nClearly this solution has an objective value of 1; we check that it is feasible for t \u2265 2. If uv is an edge containing neither of {u\u2032, u\u2032\u2032}, then \u03c0u = \u03c0v = 0 and \u03c3\u0302u,v = 0, since every term of \u03c3\u0302u,v is 0. The edge u\u2032u\u2032\u2032 is a negative edge with \u03c0u\u2032 = \u03c0u\u2032\u2032 = 1/2, and after eliminating all the zero terms, we have\n\u03c3\u0302u\u2032u\u2032\u2032 = \u2211\nz\u2208V (G)\u2212{u,v}\n(\u2212\u03c3u\u2032,u\u2032\u2032,z) = \u2212 \u2211\nz\u2208V (G)\u2212{u,v}\n1\n2t\u2212 2 = \u22121.\nThus, \u03c0u\u2032 + \u03c0u\u2032\u2032 + \u03c3\u0302u\u2032,u\u2032\u2032 \u2264 0, as required. Finally, if uv is a positive edge with u \u2208 {u\u2032, u\u2032\u2032}, say if u = u\u2032, then the only nonzero term of \u03c3\u0302uv is \u03c3u,u\u2032\u2032,v, and we have \u2212\u03c0u \u2212 \u03c0v + \u03c3\u0302u,v = \u22121/2 + 1/(2t \u2212 2) \u2264 0 as required. The same argument holds if u = u\u2032\u2032.\nSince this solution has an objective value of 1, matching the primal objective when xuv = 0 everywhere, it is clearly optimal. Furthermore, if t \u2265 3, then for each positive edge incident to u\u2032 or u\u2032\u2032, there is slack in the corresponding constraint of the dual problem. By complementary slackness, this implies that in any optimal solution to L, we have xu\u2032v = xu\u2032\u2032v = 0 for all v \u2208 V (G)\u2212 {u\u2032, u\u2032\u2032}. The triangle inequality constraints in L then imply that in an optimal primal solution, xuv = 0 for all uv \u2208 E(G)."}, {"heading": "B MaxAgree for Classical and Minimax Clustering", "text": "In this paper, we have mainly focused on studying theMinDisagree formulation of f -Correlation Clustering, where we seek to minimize an objective function related to the clustering errors in a candidate solution, and\nwhere a c-approximation algorithm is an algorithm whose total error weight is at most c times the optimal weight.\nAn alternative formulation to MinDisagree is MaxAgree, where we instead seek to maximize some function related to the edges that are not errors. In classical correlation clustering, this means that we want to maximize the number of edges which are correct. In minimax clustering, we wish to maximize the number of correct edges at the vertex with the fewest correct edges. In both cases, an optimal solution to MinDisagree is also an optimal solution to MaxAgree, but their approximation properties differ.\nIn the classical case, there is a trivial 2-approximation algorithm for MaxAgree on arbitrary graphs: we can simply choose the better of clustering with all vertices in separate clusters and the clustering with all vertices in the same cluster. All negative edges are correct in the first clustering and all positive edges are correct in the second clustering, so taking the better of the two yields a clustering with at least half the edges correct, which is clearly at least half the value of an optimal clustering. Less trivially, Bansal, Blum, and Chawla [5, 6] gave a PTAS for MaxAgree, so that any approximation ratio greater than 1 is achievable. In contrast, the best approximation ratio known for MinDisagree on arbitrary graphs has a ratio of logn.\nIt is natural to ask whether some algorithm can also be found to approximateMaxAgree in the minimax context. The trivial 2-approximation algorithm no longer works, since if G both has vertices of high positive degree and high negative degree, then each of the \u201cextreme\u201d clusterings will cause a large number of errors at some vertex. We have not been able to find any constant-factor approximation algorithm for the MaxAgree formulation of minimax clustering, even with the additional assumption that G is a labeled complete graph.\nWe now construct a graph which seems to be a good example of the difficulties in designing an algorithm for this problem. For any n, let Gn be the complete graph on n+1 vertices, and fix some vertex u\n\u2217 \u2208 V (Gn). All edges incident to u\u2217 are labeled +, while all other edges are labeled \u2212. Thus, u\u2217 has positive degree n, while all other vertices have positive degree 1.\nIt is clear that only one type of integer clustering could be optimal: cluster u\u2217 with some number t of the remaining vertices, and cluster all other vertices as singletons. This yields t correct edges at u\u2217, n\u2212 t+1 correct edges at each vertex clustered with u\u2217, and n\u2212 1 correct edges at each singleton vertex. Thus, the optimal clustering has \u230a(n+ 1)/2\u230b correct edges at its worst vertex.\nThe following result demonstrates why algorithms based on LP rounding are likely to have trouble finding a good clustering of Gn under the MaxAgree objective. We reuse the LP formulation of MinDisagree shown in Figure 1; this is valid because when we seek an exact solution, minimizing M in Figure 1 is equivalent to maximizing |V (G)| \u2212 1\u2212M , the weight of the correct edges at the worst vertex.\nProposition 9. Let L be the linear program shown in Figure 1, as formulated for Gn. If n \u2265 2, then the unique optimal solution to L has xu\u2217v = 1/3 for all v 6= u\u2217 and xvw = 2/3 for all vw \u2208 E(Gn \u2212 u\u2217).\nProof. In the proposed solution, we have M = n/3. To show that this solution is optimal and unique, we construct a solution to the dual program shown in Figure 2, as in the proof of Proposition 8. Consider the dual solution defined by\n\u03c0u\u2217 = 1\u2212 n\n3(n\u2212 1) , \u03c3v,w,u\u2217 =\n1\n3(n\u2212 1) for all vw \u2208 E(Gn \u2212 u\n\u2217)\n\u03c0v = 1\n3(n\u2212 1) for all v 6= u\u2217, \u03c3v,w,z = 0 if z 6= u \u2217.\nSince d\u2212(u\u2217) = 0 and d\u2212(v) = n\u2212 1 for all v 6= u\u2217, the objective value of this solution is n/3. Thus, if this solution is feasible, then it is optimal.\nTo see that this solution is feasible, we observe that for v, w 6= u\u2217, we have \u03c3\u0302v,w = \u2212\u03c3v,w,u\u2217 \u2212 \u03c3w,v,u\u2217 = \u2212(\u03c0v + \u03c0w), so that \u03c0v + \u03c0w + \u03c3\u0302v,w \u2264 0 for all negative edges vw, as needed. On the other hand, for v 6= u \u2217 we have \u03c3\u0302u\u2217,v = \u2211\nz /\u2208{u\u2217,v}\n(\u03c3v,z,u\u2217 + \u03c3z,v,u\u2217) = 2(n\u2212 1)\u03c0v.\nSince \u03c0u\u2217 = 1\u2212 n\u03c0v, this implies that\n\u2212piu\u2217 \u2212 \u03c0v + \u03c3\u0302u\u2217,v = \u2212(1\u2212 n\u03c0v)\u2212 \u03c0v + 2(n\u2212 1)\u03c0v = (n\u2212 1)\u03c0v \u2212 1 + 2(n\u2212 1)\u03c0v = 0,\nso that \u2212\u03c0u\u2217 \u2212 \u03c0v + \u03c3\u0302u\u2217,v \u2264 0 for all positive edges u\u2217v, as needed. Since also \u2211\nv \u03c0v = 1, we see that the proposed dual solution is feasible, so the given primal solution is optimal.\nNow we argue that the given primal solution is the unique optimal solution. Let x be any optimal primal solution. For each edge vw \u2208 E(Gn \u2212 u\u2217), the dual variable \u03c3v,w,u\u2217 is nonzero in the dual solution above, so by complementary slackness we have xvw = xu\u2217v + xu\u2217w. Furthermore, since each \u03c0v > 0, each v 6= u\n\u2217 must have total error weight equal to M , again by complementary slackness. Therefore, for each v 6= u\u2217, we have\nM = \u2211\nw\u2208N+(v)\nxvw + \u2211\nw\u2208N\u2212(v)\n(1 \u2212 xvw) = xu\u2217v + \u2211\nw/\u2208{v,u\u2217}\n(1\u2212 (xu\u2217v + xu\u2217w))\n= (n\u2212 1)\u2212 (n\u2212 3)xu\u2217v \u2212 \u2211\nw 6=u\u2217\nxu\u2217w.\nThis implies that xu\u2217v = xu\u2217w for all v 6= w. Letting p denote this common value, we have M = (n\u2212 1)\u2212 (n\u2212 3)p\u2212 np = (n\u2212 1)\u2212 (2n\u2212 3)p. On the other hand, since \u03c0u\u2217 > 0, we also have\nM = \u2211\nw\u2208N+(u\u2217)\nxu\u2217w = np.\nThus, (n \u2212 1) \u2212 (2n \u2212 3)p = np, which implies that p = 1/3. Hence, in any optimal solution we have xu\u2217v = 1/3 for all v 6= u \u2217 and xvw = 2/3 for all vw \u2208 E(Gn \u2212 u \u2217), as desired.\nThus, the only optimal solution to the natural LP rounding is highly symmetric, but the natural symmetric clusterings of Gn \u2013 into either all singletons or into one giant cluster \u2013 both have at most 1 correct edge at the worst vertex, which is far short of the optimum value of \u230an/2\u230b correct edges. We note that this does not pose a problem for the MinDisagree formulation: in a c-approximation for MinDisagree, we only promise that the generated clustering has at most c\u2308n/2\u2309 errors at its worst vertex, and if c > 2, then any clustering at all meets this guarantee."}, {"heading": "C NP-Completeness of Minimax Clustering on Complete Graphs", "text": "To show that minimax clustering is NP-hard on complete graphs, we use a reduction from the Partitioninto-Triangles problem, originally stated in [15] and attributed to Schaefer.\nPartition into Triangles Input: A graph G with |V (G)| = 3q for some integer q. Question: Is there a partition of V (G) into q sets V1, . . . , Vq such that each set Vi induces a triangle in G?\nSpecifically, we reduce from the 4-regular case:\nTheorem 10 (van Rooij, van Kooten Niekerk, Bodlaender [23]). Partition into Triangles on 4-regular graphs is NP-complete.\n(Although this is not explicitly stated in [23], it follows immediately from two of their results: that the problem is NP-hard on graphs of maximum degree at most 4, and that every partition-into-triangles instance with maximum degree at most 4 can be transformed in polynomial time into an equivalent 4-regular instance.)\nTo prove that minimax clustering is NP-hard, we use the following reformulation, which is more convenient for our purposes.\nt-Perfect Clustering Input: A labeled complete graph G together with a tolerance tv \u2208 Z+ for each v \u2208 V (G). Question: Does G admit a t-perfect clustering, that is, a clustering such that each vertex v has at most tv incident mistakes?\nTaking \u03bbv = 1/tv, we see that G has a t-perfect clustering if and only if the minimax-clustering value of the resulting weighted graph is at most 1.\nOur NP-completeness proof mimics the proof given by Bansal, Blum, and Chawla for the classical correlation clustering problem. Let G be a 4-regular graph on n vertices, where n \u2265 7, and let G\u2032 be the labeled complete graph on the same vertex set whose positive edges are exactly the edges of G. Observe that G has a partition into triangles if and only if G\u2032 has a clustering with all clusters of size at most 3 and exactly 2 mistakes at each vertex. The idea is to expand G\u2032 into a larger labeled complete graph H such that in an optimal clustering of H , every cluster has at most three G\u2032-vertices.\nWe use essentially the same construction as Bansal\u2013Blum\u2013Chawla. Let H consist of G\u2032, augmented as follows. For every 3-set {u, v, w} \u2286 V (G\u2032), add to H a clique Cuvw with 7 vertices. All edges within Cuvw are positive, all edges from Cuvw to the vertices {u, v, w} are positive, and all other edges incident to Cuvw are negative.\nWe assign the following tolerances: each original vertex u \u2208 G\u2032 has tu = 7( ( n\u22121 2 )\n\u22121)+2, and each added vertex v \u2208 H \u2212G\u2032 has tv = 3.\nLemma 11. If H has a t-perfect clustering C, then every cluster of C contains at most three vertices of G\u2032, and every cluster of C contains vertices from at most exactly one clique of H \u2212G\u2032.\nProof. First suppose that C has a cluster X containing vertices from two different cliques of H \u2212 G\u2032. Let v1, v2 belong to the cliques C1, C2 respectively. If |X \u2229 C1| > 3, then v2 has more than 3 incident mistakes, which exceeds its tolerance. On the other hand, if |X \u2229C1| \u2264 3, then since |C1| = 7, we have |C1 \u2212X | \u2265 4, so v1 has at least 4 incident mistakes, which again exceeds its tolerance. Thus, if C is t-perfect, then every cluster contains vertices from at most one clique.\nNow suppose that C has a cluster X that does not contain vertices from any clique of H \u2212 G\u2032. Since clusters are nonempty, X contains a vertex v \u2208 V (G\u2032). Since v has 7 (\nn\u22121 2\n)\nneighbors in V (H \u2212 G\u2032) and\nis not clustered with any of them, v has at least 7 ( n\u22121 2 ) incident mistakes, which exceeds its tolerance of 7 (\nn\u22121 2\n)\n\u2212 5. Finally, suppose that C has some cluster X with at least four G\u2032-vertices. Since X contains vertices from at most one clique of H \u2212 G\u2032, there is some vertex v \u2208 V (G\u2032) \u2229X does not have any positive neighbors in X \u2229 V (H \u2212 G\u2032). Since v has a total of 7 (\nn\u22121 2\n)\npositive neighbors in H \u2212 G\u2032, it again follows that v has at\nleast 7 ( n\u22121 2 ) incident mistakes, exceeding its tolerance.\nCorollary 12. H has a t-perfect clustering if and only if G has a partition into triangles.\nProof. First suppose that V1, . . . , Vk is a partition of G into triangles. Cluster H as follows: for i \u2208 [k], let Xi = Vi \u222aCVi , where CVi is the clque of H with vertex set Vi. For every clique C that is not equal to some Vi, cluster C on its own.\nEach v \u2208 V (G\u2032) has exactly 7( ( n\u22121 2 ) \u2212 1) + 2 mistakes: among the 7 ( n\u22121 2 )\npostive edges to vertices of H \u2212 G\u2032, it is clustered with exactly 7 of them, and among its 4 positive neighbors in G, it is clustered with exactly 2 of them (and with no negative neighbors), since V1, . . . , Vk is a partition of G into triangles. Furthermore, each v \u2208 V (H\u2212G\u2032) has at most 3 mistakes, since this clustering has no mistakes within H\u2212G\u2032 and does not cluster any w \u2208 V (Cxyz) with a vertex outside of {x, y, z}. Thus, the clustering is t-perfect. Now suppose that H has a t-perfect clustering C. By Lemma 11, every cluster of C contains at most three vertices of G and contains vertices from exactly one cluster Cuvw of V (H\u2212G\u2032). We claim that the restriction of C to V (G\u2032) is a partition of G into triangles. If not, some vertex v \u2208 V (G\u2032) is clustered with fewer than 2 of its positive neighbors, and therefore has at least 3 incident mistakes in G\u2032. Since the cluster containing v contains vertices from only one of the cliques containing v, we see that v also has at least 7( (\nn\u22121 2\n)\n\u2212 1)\nincident mistakes to vertices of V (H \u2032 \u2212G), for at total of at least 7( ( n\u22121 2 )\n\u2212 1) + 3 incident mistakes. This exceeds its tolerance, contradicting the hypothesis that C is t-perfect."}, {"heading": "D NP-Completeness on Complete Bipartite Graphs", "text": "In this section, we show that \u201cone-sided\u201d minimax clustering on complete bipartite graphs is NP-hard. This complements the approximation algorithm given in Section 4 for the same problem. Our proof is similar\nto the proof of Amit [4] which shows that biclustering with the classical objective function is NP-hard, but requires significant modifications to accomodate the new objective function. The proof uses a reduction from the 3-cover problem, which is well-known to be NP-complete [15].\n3-Cover Input: A ground set U = {u1, . . . , u3n} and a family of subsets S = {S1, . . . , Sp} with each |Si| = 3. Question: Is there a subfamily S \u2032 \u2286 S such that each ui lies in exactly one element of S \u2032?\nGiven an instance of 3-cover, we construct an instance of the following problem:\nOne-Sided t-perfect Biclustering Input: A labeled complete bipartite graph G with partite sets V1, V2 and a tolerance tv \u2208 Z+ for each v \u2208 V1. Question: Does G have a clustering such that each vertex v \u2208 V1 has at most tv incident edges that are errors?\nBy the same argument used in Appendix C, any algorithm which exactly determines the optimal one-sided minimax clustering for complete bipartite graphs would also solve the t-perfect biclustering problem. Hence, it suffices to show that t-perfect biclustering is NP-hard. Note also that one-sided minimax clustering can be viewed as the special case of (two-sided) minimax clustering for which tv = |V1| for all v \u2208 V2; thus, the reduction in this section also shows that the two-sided version of the problem is NP-hard.\nGiven a nontrivial instance of 3-cover (that is, an instance with n, p \u2265 1), we construct an instance of t-perfect biclustering as follows. For each ui \u2208 U , construct a pair of vertices xi \u2208 V1, yi \u2208 V2. Call these vertices ground vertices. Each edge xiyj is positive if ui = uj or if ui and uj lie in some common triplet of S, and negative otherwise.\nFor each Si \u2208 S, we create a vertex x(Si) \u2208 V1 and m vertices y1(Si), . . . , ym(Si) \u2208 V2, where each xj(Si) \u2208 V1 and yj(Si) \u2208 V2, where m \u2265 6n+ 3p is some fixed constant. Call these vertices triplet vertices, and let Bi = {x(Si)} \u222a {yj(Si) : j \u2208 {1, . . . ,m}}. All edges x(Si)yk(Si) for a fixed i are positive, and all edges x(Si)yk(S\u2113) for i 6= \u2113 are negative. For ui \u2208 U , if ui \u2208 Sj , then the edges xiyk(Sj) and yix(Sj) are positive, and otherwise these edges are negative.\nFinally, let Z = {z1, . . . , z3n} be new V2-vertices, and for each zi \u2208 Z, add positive edges to all groundvertices in V1 and negative edges to all triplet-vertices in V1. Call these vertices dummy vertices.\nNext we determine the tolerances tv. For Si \u2208 S, let tx(Si) = 3. For ui \u2208 U , the corresponding tolerances are computed more intricately. Let d(ui) be the number of triplets Sj \u2208 S containing ui and let c(ui) be the number of uj \u2208 U \u2212 {ui} such that uj and ui lie in some common triplet Sj . We define\ntxi = m(d(ui)\u2212 1) + (c(ui)\u2212 2) + (|Z| \u2212 3).\nIt is clear that G and t can be constructed in polynomial time.\nLemma 13. Suppose that G has a t-perfect clustering C. For any Si, Sj \u2208 S with i 6= j, the vertices x(Si) and x(Sj) lie in different clusters.\nProof. Suppose that x(Si) and x(Sj) lie in the same cluster X . Since tx(Si) = 3, we see that X contains at least m\u2212 3 vertices from y1(Si), . . . , ym(Si). Since x(Sj) has negative edges to all these vertices, it follows that x(Sj) has at least m \u2212 3 incident errors. Since m \u2212 3 > 3 = tx(Sj), this contradicts the fact that C is t-perfect.\nLemma 14. Suppose that G has a t-perfect clustering C. For any uj \u2208 U , there is a unique Si \u2208 S such that xj is clustered with x(Si). Furthermore, this Si has the following properties:\n1. uj \u2208 Si, and\n2. xj is clustered with each vertex y\u2113 such that u\u2113 \u2208 Si.\nProof. First we prove the existence of a unique Si such that xj is clustered with x(Si), then we show that Si has the desired properties.\nIf yk(Si) is a triplet V2-vertex not clustered with x(Si), call yk(Si) a rogue vertex. It is immediate from the definition of t that in a t-perfect clustering, each Bi contains at most 3 rogue vertices.\nTo prove that xj is clustered with some x(Si), it suffices to show that xj is clustered with some triplet V2-vertex that is not a rogue vertex. Since each Bi contains at most 3 rogue vertices, there are at most 3p rogue vertices in total, where p = |S|. If all triplet vertices clustered with xj are rogue vertices, then since xj has md(uj) positive edges to triplet vertices, it follows that xj has at least md(uj) \u2212 3p incident errors. Now we have\ntxj = m(d(uj)\u2212 1) + (c(uj)\u2212 2) + (|Z| \u2212 3) < md(uj)\u2212m+ 6n \u2264 md(uj)\u2212 3p,\nwhere the last inequality follows from m \u2265 6n + 3p. Thus, there are more than txj errors at xj , contradicting the assumption that C is t-perfect. Thus, xj is clustered with some x(Si). Uniqueness of Si follows immediately from Lemma 13.\nTo see that uj \u2208 Si, suppose that uj /\u2208 Si. Then xj is clustered with at most 3 triplet-vertices that are its positive neighbors, and therefore has at least md(uj) \u2212 3 incident errors. Since md(uj) \u2212 3 > txj , this contradicts the assumption that C is t-perfect.\nNext we prove (2). Let B = N+(xj)\u2212N +(x(Si)). Since tx(Si) = 3, the cluster containing xj contains at\nmost 3 vertices from B. Thus, there are at least |B| \u2212 3 errors from x to the vertices of B, where\n|B| \u2212 3 = |Z|+m(d(uj)\u2212 1) + (c(uj)\u2212 2)\u2212 3 = txj .\nThus, for C to be t-perfect, it is necessary that all errors incident to xj are edges from x to B. In particular, xj is clustered with all vertices in N+(xj)\u2229N +(x(Si)), so that xj is clustered with all y\u2113 such that y\u2113 \u2208 Si.\nCorollary 15. G has a t-perfect clustering if and only if S \u2032 has a 3-cover.\nProof. Given any t-perfect clustering, let S \u2032 be the family of triplets Si such that some vertex of Bi is clustered with some V1-ground-vertex xj . Lemma 14 immediately implies that these triplets cover all of u. Furthemore, Lemma 14 implies that these triplets are pairwise disjoint: if S\u20321 and S \u2032 2 are triplets of S\n\u2032 that both contain uj, then Lemma 14 would force each x(S \u2032 1) and x(S \u2032 2) to both be clustered with yj and hence to be clustered together, which contradicts Lemma 13. Hence, S \u2032 is a 3-cover. Conversely, let S \u2032 be a 3-cover in S. We define a clustering of G. Since S \u2032 is a 3-cover, we have |S \u2032| = n. Let ZS\u2032 1 , . . . , ZS\u2032n be a partition of Z into n disjoint sets of size 3, indexed by the sets of S\n\u2032. Now for each Si \u2208 S, define a cluster Xi by\nXi =\n{\nBi \u222a {xj , yj : uj \u2208 Si} \u222a ZSi , if Si \u2208 S \u2032, Bi, otherwise.\nSince S \u2032 is a 3-cover, the clusters Xi are pairwise disjoint and cover the vertices of G. We claim that this clustering is t-perfect. If x(Si) is a triplet vertex corresponding to some Si /\u2208 S \u2032, then x(Si) has exactly 3 incident errors, namely its edges to the ground-vertices yj with uj \u2208 Si. On the other hand, if x(Si) is a triplet vertex corresponding to some Si \u2208 S \u2032, then x(Si) again has exactly 3 incident errors, namely its edges to the dummy-vertices in ZSi .\nIf xj (or yj) is a ground vertex, then xj has m(d(uj) \u2212 1) incident errors which are positive edges to triplet-vertices, c(uj) \u2212 2 incident errors which are positive edges to ground-vertices, and |Z| \u2212 3 incident errors which are positive edges to dummy-vertices. This is a total of exactly txj incident errors. Hence the clustering is t-perfect."}, {"heading": "E Technical Details", "text": "Lemma 16. Suppose a Type 2 cluster {u}\u222aT has just been output in Algorithm 1. For any z \u2208 S\u2212({u}\u222aT ), the total cluster-cost of the cross-edges for z is at most max{1/(1\u2212 2\u03b1), 2/\u03b1} times the total LP-cost of the cross-edges for z.\nProof. This is essentially the same proof given by Charikar, Guruswami, and Wirth [9, 10]; we repeat it here to keep the paper self-contained. If xuz \u2265 1\u2212 \u03b1, then for each w \u2208 {u} \u222a T , we have\nxwz \u2265 xuz \u2212 xuw \u2265 1\u2212 2\u03b1.\nIf there are p positive cross-edges, this implies that the total LP-cost of the cross-edges for z is at least (1\u2212 2\u03b1)p. Since the total cluster-cost of the cross-edges for z is p, the claim holds.\nNow consider xuz \u2208 (\u03b1, 1 \u2212 \u03b1). Let P = N+(z) \u2229 ({u} \u222a T ) and let Q = N\u2212(z) \u2229 ({u} \u222a T ); the total cluster-cost of the cross-edges for z is just |P |. We have the following lower bound on the total LP-cost of the cross-edges for z:\n\u2211\nw\u2208P\nxwz + \u2211\nw\u2208N\n(1\u2212 xwz) \u2265 \u2211\nw\u2208P\n(xuz \u2212 xuw) + \u2211\nw\u2208Q\n(1\u2212 xuz \u2212 xuw)\n= |P |xuz + |Q| (1 \u2212 xuz)\u2212 \u2211\nw\u2208{u}\u222aT\nxuw\n\u2265 |P |xuz + |Q| (1 \u2212 xuz)\u2212 \u03b1(|P |+ |Q|)\n2 ,\nwhere in the last line we used the inequality \u2211 w\u2208{u}\u222aT xuw \u2264 \u03b1|{u}\u222aT | 2 . This lower bound is linear in xuz , so we study its behavior at the endpoints of (\u03b1, 1\u2212 \u03b1). When xuz = \u03b1, the lower bound rearranges as follows:\n\u03b1 |P |+ (1\u2212 \u03b1) |Q| \u2212 \u03b1(|P |+ |Q|)\n2 =\n\u03b1 2 |P |+ (1\u2212 3\u03b1 2 ) |Q| \u2265 \u03b1 2 |P | .\nWhen xuz = 1\u2212 \u03b1, the lower bound rearranges as follows:\n(1\u2212 \u03b1) |P |+ \u03b1 |Q| \u2212 \u03b1(|P |+ |Q|)\n2 = (1\u2212\n3\u03b1\n2 ) |P |+\n\u03b1 2 |Q| \u2265 \u03b1 2 |P | .\nIn both cases, we used the assumption \u03b1 < 1/2, which implies 1\u2212 3\u03b12 \u2265 \u03b1 2 . It follows that charging 2 \u03b1 times the LP-cost of each cross-edge yields enough charge to pay for the cluster-cost of all cross-edges.\nLemma 17. Suppose that a Type 2 cluster C has just been output in Algorithm 2. For any vertex z \u2208 V1\u2212C, the total cluster-cost of the cross-edges for z is at most max{1/(1\u2212 2\u03b1), 2/\u03b1} times the total LP-cost of the cross-edges for z.\nProof. We essentially repeat the proof of Lemma 16. If xuz \u2265 1\u2212 \u03b1, then for each w \u2208 {u} \u222a T , we have\nxwz \u2265 xuz \u2212 xuw \u2265 1\u2212 2\u03b1.\nIf there are p positive cross-edges, this implies that the total LP-cost of the cross-edges for z is at least (1\u2212 2\u03b1)p. Since the total cluster-cost of the cross-edges for z is p, the claim holds.\nNow consider xuz \u2208 (\u03b1, 1 \u2212 \u03b1). Let P = N+(z) \u2229 ({u} \u222a T ) and let Q = N\u2212(z) \u2229 ({u} \u222a T ); the total cluster-cost of the cross-edges for z is just |P |. Note that P \u222a Q = V2 \u2229 T . We have the following lower bound on the total LP-cost of the cross-edges for z:\n\u2211\nw\u2208P\nxwz + \u2211\nw\u2208N\n(1\u2212 xwz) \u2265 \u2211\nw\u2208P\n(xuz \u2212 xuw) + \u2211\nw\u2208Q\n(1\u2212 xuz \u2212 xuw)\n= |P |xuz + |Q| (1 \u2212 xuz)\u2212 \u2211\nw\u2208V2\u2229T\nxuw\n\u2265 |P |xuz + |Q| (1 \u2212 xuz)\u2212 \u03b1\n2 (|P |+ |Q|),\nwhere in the last line we used the inequality \u2211\nw\u2208V2\u2229T xuw \u2264 \u03b1 2 |{u} \u222a T |. This lower bound is linear in\nxuz , so we study its behavior at the endpoints of (\u03b1, 1 \u2212 \u03b1). When xuz = \u03b1, the lower bound rearranges as follows:\n\u03b1 |P |+ (1\u2212 \u03b1) |Q| \u2212 \u03b1\n2 (|P |+ |Q|) =\n\u03b1 2 |P |+ (1\u2212 3\u03b1 2 ) |Q| \u2265 \u03b1 2 |P | .\nWhen xuz = 1\u2212 \u03b1, the lower bound rearranges as follows:\n(1 \u2212 \u03b1) |P |+ \u03b1 |Q| \u2212 \u03b1\n2 (|P |+ |Q|) \u2265 (1\u2212 \u03b1\u2212\n\u03b1 2 ) |P |+ \u03b1 2 |Q| \u2265 \u03b1 2 |P | .\nIn both cases, we used the assumption \u03b1 < 1/2. It follows that when xuz \u2208 (\u03b1, 1 \u2212 \u03b1), charging 1/(\u03b1\u2212 \u03b2) times the LP-cost of each cross-edge yields enough charge to pay for the cluster-cost of all cross-edges."}], "references": [{"title": "Aggregating inconsistent information: ranking and clustering", "author": ["Nir Ailon", "Moses Charikar", "Alantha Newman"], "venue": "Proceedings of the thirty-seventh annual ACM symposium on Theory of computing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "The bicluster graph editing", "author": ["Noga Amit"], "venue": "Tel Aviv University,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Min-max graph partitioning and small set expansion, 2011", "author": ["Nikhil Bansal", "Uriel Feige", "Robert Krauthgamer", "Konstantin Makarychev", "Viswanath Nagarajan", "Joseph Naor", "Roy Schwartz"], "venue": "IEEE 52nd Annual Symposium on Foundations of Computer Science\u2014FOCS", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Clustering with qualitative information", "author": ["Moses Charikar", "Venkatesan Guruswami", "Anthony Wirth"], "venue": "Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science (Washington, DC, USA),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "On the hardness of approximating multicut and sparsest-cut", "author": ["Shuchi Chawla", "Robert Krauthgamer", "Ravi Kumar", "Yuval Rabani", "D Sivakumar"], "venue": "Computational Complexity", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Biclustering of expression data", "author": ["Yizong Cheng", "George M Church"], "venue": "ISMB, vol", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "Correlation clustering in general weighted graphs", "author": ["Erik D Demaine", "Dotan Emanuel", "Amos Fiat", "Nicole Immorlica"], "venue": "Theoretical Computer Science", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "On the power of unique 2-prover 1-round games", "author": ["Subhash Khot"], "venue": "Proceedings of the thiry-fourth annual ACM symposium on Theory of computing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Cluster editing with locally bounded modifications", "author": ["Christian Komusiewicz", "Johannes Uhlmann"], "venue": "Discrete Appl. Math", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Clustering high-dimensional data: A survey on subspace clustering, pattern-based clustering, and correlation clustering, ACM Transactions on Knowledge Discovery from Data (TKDD", "author": ["Hans-Peter Kriegel", "Peer Kr\u00f6ger", "Arthur Zimek"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "A convex optimization framework for bi-clustering", "author": ["Shiau Hong Lim", "Yudong Chen", "Huan Xu"], "venue": "Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Connections between unique games and multcut, Tech. Report TR09-125", "author": ["D. Steurer", "N. Vishnoi"], "venue": "Electronic Colloquium on Computational Complexity,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Nearest-biclusters collaborative filtering with constant values, Advances in web mining and web usage", "author": ["Panagiotis Symeonidis", "Alexandros Nanopoulos", "Apostolos Papadopoulos", "Yannis Manolopoulos"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Partition into triangles on bounded degree graphs", "author": ["Johan M.M. van Rooij", "Marcel E. van Kooten Niekerk", "Hans L. Bodlaender"], "venue": "Theory Comput. Syst", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}], "referenceMentions": [{"referenceID": 3, "context": "Furthermore, if the input graph is allowed to be arbitrary, the best known approximation ratio is O(log n), obtained by [9, 10, 14].", "startOffset": 120, "endOffset": 131}, {"referenceID": 6, "context": "Furthermore, if the input graph is allowed to be arbitrary, the best known approximation ratio is O(log n), obtained by [9, 10, 14].", "startOffset": 120, "endOffset": 131}, {"referenceID": 7, "context": "Assuming the Unique Games Conjecture of Khot [16], no constant-factor approximation for MinDisagree on arbitrary graphs is possible; this follows from the results of [11, 21] concerning the minimum multicut problem and the connection between correlation clustering and minimum multicut described in [9, 10, 14].", "startOffset": 45, "endOffset": 49}, {"referenceID": 4, "context": "Assuming the Unique Games Conjecture of Khot [16], no constant-factor approximation for MinDisagree on arbitrary graphs is possible; this follows from the results of [11, 21] concerning the minimum multicut problem and the connection between correlation clustering and minimum multicut described in [9, 10, 14].", "startOffset": 166, "endOffset": 174}, {"referenceID": 11, "context": "Assuming the Unique Games Conjecture of Khot [16], no constant-factor approximation for MinDisagree on arbitrary graphs is possible; this follows from the results of [11, 21] concerning the minimum multicut problem and the connection between correlation clustering and minimum multicut described in [9, 10, 14].", "startOffset": 166, "endOffset": 174}, {"referenceID": 3, "context": "Assuming the Unique Games Conjecture of Khot [16], no constant-factor approximation for MinDisagree on arbitrary graphs is possible; this follows from the results of [11, 21] concerning the minimum multicut problem and the connection between correlation clustering and minimum multicut described in [9, 10, 14].", "startOffset": 299, "endOffset": 310}, {"referenceID": 6, "context": "Assuming the Unique Games Conjecture of Khot [16], no constant-factor approximation for MinDisagree on arbitrary graphs is possible; this follows from the results of [11, 21] concerning the minimum multicut problem and the connection between correlation clustering and minimum multicut described in [9, 10, 14].", "startOffset": 299, "endOffset": 310}, {"referenceID": 0, "context": "Ailon, Charikar, and Newman [2, 3] gave a very simple randomized 3-approximation algorithm for MinDisagree on complete graphs.", "startOffset": 28, "endOffset": 34}, {"referenceID": 1, "context": "The first constant approximation algorithm for correlation clustering on complete bipartite graphs was described by Amit [4], who gave an 11-approximation algorithm.", "startOffset": 121, "endOffset": 124}, {"referenceID": 10, "context": "Bipartite clustering has also been studied, outside the correlation-clustering context, by Lim, Chen, and Xu [19].", "startOffset": 109, "endOffset": 113}, {"referenceID": 5, "context": "We depart from the classical correlation-clustering literature by considering a broader class of objective functions which also cater to the need of many community-detection applications in machine learning, social sciences, recommender systems and bioinformatics [13, 22, 18].", "startOffset": 264, "endOffset": 276}, {"referenceID": 12, "context": "We depart from the classical correlation-clustering literature by considering a broader class of objective functions which also cater to the need of many community-detection applications in machine learning, social sciences, recommender systems and bioinformatics [13, 22, 18].", "startOffset": 264, "endOffset": 276}, {"referenceID": 9, "context": "We depart from the classical correlation-clustering literature by considering a broader class of objective functions which also cater to the need of many community-detection applications in machine learning, social sciences, recommender systems and bioinformatics [13, 22, 18].", "startOffset": 264, "endOffset": 276}, {"referenceID": 0, "context": "These new technical difficulties cause the algorithm of [2, 3] to fail in the minimax context, and there is no obvious way to adapt that algorithm to this new context; this phenomenon is explored further in Appendix A.", "startOffset": 56, "endOffset": 62}, {"referenceID": 2, "context": "A minimax objective function for a graph partitioning problem different from correlation clustering was previously studied by [7].", "startOffset": 126, "endOffset": 129}, {"referenceID": 2, "context": "Thus, the minimum in [7] is being taken over the parts of the solution, rather than minimizing over vertices as we do here.", "startOffset": 21, "endOffset": 24}, {"referenceID": 8, "context": "In particular, Komusiewicz and Uhlmann [17] proved that the following problem is fixedparameter tractable for the combined parameter (d, t):", "startOffset": 39, "endOffset": 43}, {"referenceID": 3, "context": "When G is a complete graph, we then employ a rounding algorithm based on the algorithm of Charikar, Guruswami, and Wirth [9, 10] to transform the fractional clustering into a discrete clustering.", "startOffset": 121, "endOffset": 128}, {"referenceID": 3, "context": "Our rounding algorithm is based on the algorithm of Charikar, Guruswami, and Wirth [9, 10] and is shown in Algorithm 1.", "startOffset": 83, "endOffset": 90}, {"referenceID": 3, "context": "The main difference between Algorithm 1 and the algorithm of [9, 10] is the new strategy of choosing a pivot vertex that maximizes |T \u2217 u |; in [9, 10], the pivot vertex is chosen arbitrarily.", "startOffset": 61, "endOffset": 68}, {"referenceID": 3, "context": "The main difference between Algorithm 1 and the algorithm of [9, 10] is the new strategy of choosing a pivot vertex that maximizes |T \u2217 u |; in [9, 10], the pivot vertex is chosen arbitrarily.", "startOffset": 144, "endOffset": 151}, {"referenceID": 3, "context": "Furthermore, the algorithm of [9, 10] always uses \u03b1 = 1/2 as a cutoff for forming \u201ccandidate clusters\u201d, while we express \u03b1 as a parameter which we later choose in order to optimize the approximation ratio.", "startOffset": 30, "endOffset": 37}, {"referenceID": 3, "context": "Our general strategy for obtaining the constant-factor error bound for Algorithm 1 is similar to that of [9, 10].", "startOffset": 105, "endOffset": 112}, {"referenceID": 3, "context": "The main difference between our proof and the proof of [9, 10] is that we must pay for errors locally: for each vertex v, we must pay for all clustering errors incident to v by charging to the LP cost incident to v.", "startOffset": 55, "endOffset": 62}, {"referenceID": 3, "context": "In particular, every clustering error must now be paid for at each of its endpoints, while in [9, 10], it was enough to pay for each clustering error at one of its endpoints.", "startOffset": 94, "endOffset": 101}, {"referenceID": 3, "context": "at each endpoint, a difficulty which was not present in [9, 10].", "startOffset": 56, "endOffset": 63}], "year": 2016, "abstractText": "We consider a generalized version of the correlation clustering problem, defined as follows. Given a complete graph G whose edges are labeled with + or \u2212, we wish to partition the graph into clusters while trying to avoid errors: + edges between clusters or \u2212 edges within clusters. Classically, one seeks to minimize the total number of such errors. We introduce a new framework that allows the objective to be a more general function of the number of errors at each vertex (for example, we may wish to minimize the number of errors at the worst vertex) and provide a rounding algorithm which converts \u201cfractional clusterings\u201d into discrete clusterings while causing only a constant-factor blowup in the number of errors at each vertex. This rounding algorithm yields constant-factor approximation algorithms for the discrete problem under a wide variety of objective functions.", "creator": "LaTeX with hyperref package"}}}