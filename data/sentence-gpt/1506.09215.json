{"id": "1506.09215", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2015", "title": "Unsupervised Learning from Narrated Instruction Videos", "abstract": "We address the problem of automatically learning the main steps to complete a certain task, such as changing a car tire, from a set of narrated instruction videos. The contributions of this paper are three-fold. First, we develop a joint model for video and natural language narration that takes advantage of the complementary nature of the two signals. Second, we collect an annotated dataset of 57 Internet instruction videos containing more than 350,000 frames for two tasks (changing car tire and CardioPulmonary Resuscitation). Third, we experimentally demonstrate that the proposed model automatically discovers, in an unsupervised manner, the main steps to achieve each task and locate them within the input videos. The results further show that the proposed model outperforms single-modality baselines, demonstrating the benefits of joint modeling video and text. Lastly, we apply a third model to a video, which is designed to capture and interpret each video in its own image and to create the unique characteristics of the two signals that allow the network to communicate in one visual direction.", "histories": [["v1", "Tue, 30 Jun 2015 19:55:37 GMT  (4681kb,D)", "http://arxiv.org/abs/1506.09215v1", "13 pages"], ["v2", "Thu, 2 Jul 2015 16:43:36 GMT  (1200kb,D)", "http://arxiv.org/abs/1506.09215v2", "13 pages - corrected typo in reference [13] + added recently appeared arXiv reference [21]"], ["v3", "Mon, 30 Nov 2015 18:10:53 GMT  (6411kb,D)", "http://arxiv.org/abs/1506.09215v3", "improved NLP method and bigger dataset"], ["v4", "Tue, 28 Jun 2016 18:43:37 GMT  (3190kb,D)", "http://arxiv.org/abs/1506.09215v4", "Appears in: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016). 21 pages"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["jean-baptiste alayrac", "piotr bojanowski", "nishant agrawal", "josef sivic", "ivan laptev", "simon lacoste-julien"], "accepted": false, "id": "1506.09215"}, "pdf": {"name": "1506.09215.pdf", "metadata": {"source": "CRF", "title": "Learning from narrated instruction videos", "authors": ["Jean-Baptiste Alayrac", "Piotr Bojanowski", "Nishant Agrawal", "IIIT Hyderabad", "Josef Sivic", "Simon Lacoste-Julien"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Millions of people watch narrated instruction videos1 to learn new tasks such as assembling IKEA furniture or changing a flat car tire. What if machines could learn these tasks from myriads of instruction videos on the Internet? Such automatic cognitive ability would enable constructing virtual assistants and smart robots that would help people to achieve new tasks in unfamiliar situations.\nIn this work, we consider instruction videos and develop a method that learns a sequence of steps, as well as their textual and visual representations, required to achieve a certain task. For example, given a set of narrated instruction videos demonstrating how to change a car tire, our method automatically discovers consecutive steps for this task such as loosen the nuts of the wheel, jack up the car, remove the spare tire and so on as illustrated in Figure 1. In addition, the method learns the visual and linguistic variability of these steps from natural videos.\nDiscovering key steps from instruction videos is a highly challenging task. First, linguistic expressions for the same step can have high variability across videos, for example: \u201c...Loosen up the wheel nut just a little before you start jacking the car...\u201d and \u201c...Start to loosen the lug nuts just enough to make them easy to turn by hand...\u201d. Second, the visual appearance of each step vary greatly between videos as the people and objects are different, the action is captured from a different viewpoint, and the way people perform actions also vary. Finally, there is also a variability of the overall structure of the sequence of steps achieving the certain task. For example, some videos may omit some steps or change slightly their order.\nTo address these challenges, in this paper we develop a joint model of visual appearance and natural language narration. The model benefits from the complementary nature of both modalities to resolve \u2217WILLOW project-team, De\u0301partement d\u2019Informatique de l\u2019Ecole Normale Supe\u0301rieure, ENS/INRIA/CNRS UMR 8548, Paris, France. \u2020SIERRA project-team, De\u0301partement d\u2019Informatique de l\u2019Ecole Normale Supe\u0301rieure, ENS/INRIA/CNRS UMR 8548, Paris, France. 1Some instruction videos on YouTube have tens of millions of views, e.g. www.youtube.com/watch? v=J4-GRH2nDvw.\nar X\niv :1\n50 6.\n09 21\n5v 1\n[ cs\n.C V\n] 3\n0 Ju\nn 20\ntheir ambiguities. We assume that the same sequence of steps (also called script in the NLP literature [19]) is common to all input videos of the same task, but the actual sequence and the individual steps are unknown and are learnt directly from data. This is in contrast to other existing methods for modeling instruction videos [13] that assume a script (recipe) is know and fixed in advance. We address the problem by a temporal clustering of video and text, where the two clustering tasks are linked by joint constraints. The output of our method is the script listing the discovered steps of the task as well as the temporal location of each step in input videos. We validate our method on a new dataset of instruction videos composed of two tasks with the total of 57 videos and more than 350,000 frames."}, {"heading": "2 Related work", "text": "This work relates to unsupervised and weakly-supervised learning methods in computer vision and natural language processing. Particularly related to ours is the work on learning script-like knowledge from natural language descriptions [5, 9, 19]. These methods aim to discover typical events (steps) and their order for particular scenarios (tasks)2 such as \u201ccooking scrambled egg\u201d, \u201ctaking a bus\u201d or \u201cmaking coffee\u201d. While [5] uses large-scale news copora, [19] argues that many events are implicit and are not described in such general-purpose text data. Instead [9, 19] use event sequence descriptions collected for particular scenarios. Differently to this work, we learn sequences of events from narrated instruction videos on the Internet. Such data contains detailed event descriptions but is not structured and contains more noise compared to the input of [9, 19]. Our method also shows the benefit of joint learning from video and text, where action recognition in video helps grouping similar events with different textual descriptions such as \u201clift car\u201d and \u201cjack car\u201d. Similar to [19] we use the method of [10] for multiple sequence alignment.\nInterpretation of narrated instruction videos has been recently addressed in [13]. While this work analyses cooking videos at an impressive scale, it relies on readily-available recipes which may not be available for more general scenarios. Differently from [13], we here aim to learn the steps of instruction videos using a discriminative clustering approach. [11] considers a similar task to ours and apply the latent variable structured perceptron algorithm to align nouns in instruction sentences with objects touched by hands in instruction videos. Similarly to [13], [11] uses laboratory experimental protocols as textual input, whereas here we consider a weaker signal in the form of the speech to text narration of the video.\nIn computer vision, unsupervised action recognition has been explored in simple videos [15]. More recently, weakly supervised learning of actions in video using video scripts or event order has been addressed in [2, 3, 4, 8, 12]. Particularly related to ours is the work [3] which explores the known\n2We here assign the same meaning to terms \u201cevent\u201d and \u201cstep\u201d as well as to terms \u201cscript\u201d and \u201ctask\u201d.\norder of events to localize and learn actions in training data. While [3] uses manually annotated sequences of events, we here discover events jointly from transcribed narrations and corresponding instruction videos. [4] aligns natural text descriptions to video but does not discover events. Methods in [14, 18] learn in an unsupervised manner the temporal structure of actions from video but do not discover textual expressions for actions as we do in this work.\nOur work is also related to video summarization and in particular to the recent work on categoryspecific video summarization [16, 21]. While summarization is a relatively subjective task, we here aim to extract the key steps required to achieve a certain task. Unlike video summarization in [16, 21] we jointly exploit visual and linguistic modalities in our approach.\nRelatively little work exploits vision to improve NLP or to ground linguistic expressions. [17] improves coreference resolution by learning appearance of people in video. Similar to our work, [20] demonstrates that a text-based model of similarity between actions improves when combined with visual information from videos depicting the described actions.\nContributions. The contributions of our work are three-fold: (i) we address the problem of automatically discovering the main steps in narrated instruction videos in an unsupervised manner, (ii) we propose a joint model of video and text to tackle this complicated problem, (iii) we construct a new dataset with two tasks and demonstrate that the joint model improves results in both text and video."}, {"heading": "3 Modeling narrated instruction videos", "text": "We are given a set of N instruction videos all depicting the same task (such as \u201cchanging a tire\u201d). An input video n is composed of a video stream of Tn frames (xnt ) Tn t=1 and an audio stream containing a detailed verbal description of the depicted task. We suppose that the audio description was transcribed to raw text and then processed to a sequence of Sn text tokens (dns ) Sn s=1. Given this data, we want to automatically recover the sequence of K main steps that compose the given task and locate each step within each input video and text transcription. We formulate the problem as two clustering tasks, one for each input modality, linked by joint constraints. This is achieved by solving the following minimization problem:\nmin R\u2208R, Z\u2208Z\ng(R) + h(Z) s.t. c(R,Z) \u2264 0, (1)\nwhere g(R) and h(Z) are the cost of clustering all the text streams {dn}Nn=1 and the video streams {xn}Nn=1, respectively, into a sequence of K steps. The minimization is performed over the set of latent indicator variables Z \u2208 Z := {0, 1}T\u00d7K and R \u2208 R := {0, 1}S\u00d7K where T := \u2211N n=1 Tn\nand S := \u2211N\nn=1 Sn. These latent variables are concatenations of latent variables specific to each instruction video from the collection. They indicate the visual presence of step k at a time interval t (Ztk = 1) or the fact that the s-th text token expresses the instructions for step k (Rsk = 1), respectively.\nWe use the vector of joint constraints c(R,Z) \u2264 0 to encode the structural assumptions about the individual clustering problems as well as their joint relationships. This includes the temporal consistency between video and text for each instruction video, as well as a global ordering within each of the two streams. Details of the two clustering problems and the joint constraints are given next."}, {"heading": "3.1 Clustering transcribed verbal instructions", "text": "The goal here is to cluster the transcribed verbal descriptions of each video into a sequence of main steps necessary to achieve the task. We assume that the important steps are common to many of the transcripts and that the sequence of steps is (roughly) preserved in all transcripts. The main challenge is to deal with the variability in spoken natural language. To overcome this challenge, we take advantage of the fact that completing a certain task usually involves interactions with objects or people and hence we can extract a more structured representation from the input text stream.\nMore specifically, we represent the textual data as a sequence of direct object relations. A direct object relation d is a pair composed of a verb and its direct object complement, such as \u201cremove tire\u201d, extracted from the dependency parser of the input transcribed narration [7]. We denote the set\nof all direct object relations extracted from all narrations as D. The cardinality of D, corresponding to the number of different extracted direct object relations, is denoted by D. For the n-th video, we thus represent the text signal as a sequence of direct object relation tokens: dn = (dn1 , . . . , d n Sn\n), where the length Sn of the sequence varies from one video clip to another. This step is key to the success of our method as it allows us to convert the problem of clustering raw transcribed text into an easier problem of clustering sequences of direct object relations.\nWe formulate the problem of clustering the input sequences of direct object relations as a multiple sequence alignment [19], as illustrated in Figure 2. The input sequences are first aligned together and the K most common steps (that occur most frequently in the aligned transcripts) are kept. The multiple sequence alignment is performed using the Clustal algorithm [10] that maximizes the pairwise similarity between the input sequences, and we use the negative of that score as the text clustering cost g in equation (1). The key component of the multiple sequence alignment is the score of matching (aligning) together two direct object relations. In this work, we measure the similarity of two direct object relations d1 and d2 as\nsd(d1, d2) = 1\n2 (s(v1, v2) + s(o1, o2)), (2)\nwhere s(v1, v2) \u2208 [0, 1] measures the similarity of verbs v1 and v2 and s(o1, o2) \u2208 [0, 1] measures the similarity of direct objects o1 and o2. We measure both verb and object similarities based on their distance in the WordNet tree. If a particular word has multiple senses, we take the most similar sense. We found that this works well in our set-up where we match words only in the constrained scenario of one task (e.g. \u201cchanging a tire\u201d). However, we found that the precision of the score is important for the sequence alignment and conservatively threshold the score to 1 if sd(d1, d2) = 1 and set the score to \u2212\u221e otherwise. We extract the main instruction steps from the resulting alignment by taking the K most common groups of aligned direct object relations. We also require that the number of steps does not exceed K, which can result, in some cases, in less than K output steps. The whole procedure of extracting the main instruction steps from the transcripts is illustrated in Figure 2."}, {"heading": "3.2 Discriminative clustering of videos", "text": "We formulate the problem of clustering the N input video streams (xt) into a sequence of K steps as a discriminative clustering task with sequence constraints. We represent each time interval by a d-dimensional feature vector. To simplify notation and the implementation, we use an extra constant bias feature set to a large value so that it is not regularized. The feature vectors for the n-th video are stacked in a Tn \u00d7 d design matrix denoted by Xn. For each input video n, the goal is to find an optimal assignment matrix Zn \u2208 {0, 1}Tn\u00d7K of the Tn input video intervals xt into one of K clusters. To do so, we learn a linear classifier represented by a d \u00d7 K matrix denoted by W . This model is shared among all videos. Again for succinctness, we eliminate the video index n by stacking all assignment matrices in one matrix Z of size T \u00d7K where T = \u2211N n=1 Tn. Similarly, we denote by X the T \u00d7 d matrix obtained by the concatenation of all Xn matrices. The optimal assignment Z\u0302 is found by minimizing the cost function h, where the clustering cost h(Z) is given\nas in DIFFRAC [1] as:\nh(Z) = min W\u2208RK\u00d7d\n1\n2T \u2016Z \u2212XW\u20162F\ufe38 \ufe37\ufe37 \ufe38\nDiscriminative loss on data\n+ \u03bb\n2 \u2016W\u20162F\ufe38 \ufe37\ufe37 \ufe38 Regularizer . (3)\nThe first term in (3) is the discriminative loss on the data that measures how easy the input data X is separable by the linear classifier W when the target classes are given by assignments Z. For the squared loss considered in eq. (3), the optimal weights W \u2217 minimizing (3) can be found in closed form (eq. (6) in Appendix A), which significantly simplifies the computation. However, to solve minZ\u2208Z h(Z) with respect to our additional constraints, we need to optimize over assignment matrices Z that encode sequences of events and incorporate constraints given by clusters obtained from transcribed textual narrations (section 3.1). These additional constraints on Z are described next."}, {"heading": "3.3 Joint constraints between verbal descriptions and video", "text": "We wish to encode constraints that link the clustering of transcripts (Section 3.1) with the clustering of videos (Section 3.2). We use two types of constraints to link the two clustering tasks. The first constraint encodes the fact that people perform the action approximately at the same time when they talk about it. This is a joint constraint that strongly links together the assignment matrices R and Z of the two clustering tasks. The second constraint encodes the fact that the clustered instruction steps in both the transcript and the video need to respect a global sequence ordering. This is an independent constraint on indicator assignment matrices R and Z. However, as the two assignment matrices are linked together, this results in a single global ordering common to both clusterings. We give details of the two types of constraints next.\nPeople do what they say when they say it. The timing of the closed captions (transcripts) give us an approximate temporal alignment between the video and the narrated speech. We encode this alignment by a binary matrix A in {0, 1}S\u00d7T , where Ast is 1 if the s-th direct object is mentioned in a closed caption, which overlaps with the time interval t in video. Note that the alignment is only approximate as people do not perform the action exactly at the time when they talk about it but often with a delay that changes from action to action. Second, the alignment is noisy as people typically perform the action only once, but often talk about it multiple times (e.g. in a summary at the beginning of the video). To address these issues we, first, consider in matrix A a larger set of possible time intervals [t \u2212\u2206b, t + \u2206f ] rather than the exact time interval t given by the timing of the closed caption. \u2206b and \u2206f are global parameters fixed by cross-validation. Second, we require that the action happens only at least once in the set of all possible video time intervals where the action is mentioned in the transcript. This is encoded by the inequality constraint rTk Azk \u2265 1, where rk is the k-th column of R that encodes with 1 all direct object relations assigned to cluster k in the transcript and zk is the k-th column of Z that encodes with 1 intervals assigned to cluster k in video. Note that rTk Azk encodes the number of aligned video intervals and direct object relations that are matched to step k. Finally, this constraint can be written for all K clusters as RTAZ \u2265 IK , where IK is a K \u00d7K identity matrix. Sequence constraints. Our goal is to recover a set of K clusters in video and text that form a single (global) sequence of steps. This is a strong constraint on the structure of the assignment matrices for the two clustering tasks R and Z. For text clustering, the sequence constraint on R is encoded implicitly by the multiple sequence alignment algorithm, which maintains a single ordering throughout the optimization. For the discriminative clustering of video, we encode the sequence constraints on Z in a similar manner to [4]. In particular, we only predict the most salient time interval in the video that describes a given task. This means that a particular step is assigned to exactly one time interval in each video. See Appendix B for details.\nOptimization. The constraints introduced previously are summarized by c(R,Z) \u2264 0. To minimize the cost of the joint clustering (1), we proceed as follows. We first solve the text clustering task g(R) without joint constraints onR. We then fix the text assignment variablesR and solve the video clustering problem by minimizing h(Z) subject to the constraints from the text transcripts. This is done using the Frank-Wolfe algorithm (see Appendix B for details). We found that one step of this procedure gives satisfactory results, but we show in experiments that the resulting video assignments Z can further improve the text similarity measure for text clustering."}, {"heading": "4 Experimental evaluation", "text": "In this section, we first describe our new annotated dataset of instruction videos followed by the details of the text and video features. We then present results divided into three experiments. Sec. 4.1 evaluates the quality of steps extracted from video narrations. In Sec. 4.2, we evaluate the temporal localization of steps in instruction videos using constraints derived from text. Finally, in Sec. 4.3 we evaluate the benefits of the visual signal for grouping synonymous text descriptions representing the same step.\nDataset and annotation. We have collected a dataset of instruction videos for two tasks: Changing car tire and Performing cardiopulmonary resuscitation (CPR) by searching YouTube with relevant keywords. We have selected videos with English transcripts obtained by YouTube\u2019s automatic speech recognition (ASR). To focus on the main problem in this paper, we have manually corrected errors and punctuations of ASR. We have obtained 30 videos for the task Changing car tire and 27 videos for CPR each with its (curated) speech-to-text transcription. The average length of our videos is about 6,000 frames or 200 seconds. Each word of the transcript is associated with its corresponding (coarse) time interval in the video. For the purpose of evaluation, we have manually annotated the main steps in all videos. We have defined 10 ground truth steps for the task Changing car tire and 6 ground truth steps for the task CPR (see Table 1). We then assigned each time interval in the videos to one of these ground truth steps (or no step). These annotations are used for evaluation purpose only. Some frames from our dataset are illustrated in Figure 3.\nVideo and text features. We represent the transcribed narrations as sequences of direct object relations. For this purpose, we run a dependency parser [7] on each transcript. We lemmatize all direct object relations and keep the ones for which the direct object corresponds to nouns. To represent a video, we use motion and appearance features aiming to capture actions (loosening, jacking-up, giving compressions) and objects (tire, jack, car) respectively. We split each video into\n10-frames time intervals and represent each interval by its motion and appearance features. Our motion features are histograms of local optical flow (HOF) descriptors aggregated over a block of 30 frames into a single bag-of-visual-word vector of 2,000 dimensions [23]. The visual vocabulary is generated by k-means on a large set of training descriptors. To represent appearance, we use VGG-s CNN [6] and obtain 4,096-dimensional descriptors for each video frame corresponding to the \u201cfull7\u201d layer output of the network [22]. The per-frame descriptors are aggregated over time intervals using max-aggregation applied to each descriptor dimension. Motion and appearance descriptors are L2normalized and then concatenated into a single 6,096 dimensional vector representing each time interval."}, {"heading": "4.1 Results of step discovery from text narrations", "text": "Results of discovering the main steps for each task from text narrations are presented in Table 1. We report results of the multiple sequence alignment described in Sec. 3.1 using different values of parameter K (the maximal number of discovered steps). We illustrate the recovered sequences of steps, with each step represented by the most common direct object relation. The recovered sequences of steps are mostly correct for both tasks. We can also see that with the increase K, we recover more complete sequences at the cost of occasional repetitions, e.g. lower car and lower jack that refer to the same step. For CPR and large K, the method recovers fine-grain steps e.g. tilt head, lift chin, and pinch nose, which were not included in our original annotation. To quantify the performance, we measure precision as the proportion of correctly recovered steps appearing in correct order. We also measure recall as the proportion of the recovered ground truth step. The values of precision and recall are given at the bottom of table 1. The decrease of precision for the CPR task is partly caused by the coarse granularity of the steps in our annotation. This brings up the issue of the definition of \u201ccorrect\u201d annotation, which will depend on the chosen granularity of labelling."}, {"heading": "4.2 Results of localizing instruction steps in video", "text": "The discovered sequence of steps and their realizations in each video provide initial time intervals for instruction steps in the video. Such time intervals, however, are usually imprecise since narrations and corresponding events are often misaligned in time. Moreover, the parsing of narrations often results in missing steps due to the variability of natural language expressions. We therefore use text input to constrain the output of vision-based step localization in video. The goal of this section is, hence, to evaluate the benefits of these constraints for the temporal localization of instruction steps.\nExperimental setup. For each task (changing car tire, CPR), we split the dataset into the validation and evaluation sets. We use the validation set only to adjust the hyperparameters of the method (\u03bb, \u2206b and \u2206f ). In practice, we keep 80% of the data in the evaluation set. Note that the optimization is always run on the entire dataset but without access to the ground truth annotations and is thus unsupervised. We perform experiments over 40 random validation/evaluation splits and report average performance.\nEvaluation metric. To evaluate the temporal localization, we manually define a one-to-one mapping between the automatically discovered steps from Table 1 and the ground truth steps. This mapping is only used for evaluation.3 The aim is to evaluate whether an interval for every ground truth step has been correctly detected in all instruction videos, and thus we use recall as our evaluation score. For a given video and a given recovered ground truth step, our video clustering method predicts\n3We ignore the automatically discovered steps that are not associated to a ground truth step in our evaluation.\nexactly one time interval t. This detection is considered correct if the time interval falls inside any of the corresponding ground truth intervals, and incorrect otherwise (giving a false positive for this video). We report the recall across all steps and videos, which we define as the ratio of the number of correct predictions over the total number N \u00b7G of possible ground truth events, where G is the number of ground truth steps for this task (irrespective of the number of recovered steps). A recall of 1 indicates that every ground truth step has been correctly detected across all videos. The score decreases towards 0 when we miss some ground truth steps (missed detections). This happens either because this step was not recovered globally, or because the algorithm detected it in a video at an incorrect location (false positive) because of the exactly one time interval prediction constraint.\nBaselines. We compare results to three baselines. To demonstrate the difficulty of our dataset, we first evaluate a \u201cUniform\u201d baseline, which simply distributes instructions steps uniformly over the entire instruction video. The second baseline \u201cVideo only\u201d does not use any constraints from the narration and performs only discriminative clustering on visual features with the sequence constraints [3].4 The third baseline called \u201cText only\u201d does not look at the video, and only uses the detected instruction steps in the textual narration to produce multiple sequences respecting the given order of instructions and the text constraints (using random scores for the visual features).\nResults. Results of localizing the discovered instruction steps for Changing car tire and CPR are presented in Table 2. We report results for different numbers of steps K. The poor performance of the \u201cUniform\u201d baseline illustrates the difficulty of our dataset. Our method outperforms both the \u201cVideo only\u201d and \u201cText only\u201d baselines by a large margin for all values of K, clearly demonstrating the benefits of jointly modeling videos and corresponding narrations. For Changing car tire, the performance increases for K = 10, which is the number of ground truth instruction steps for this task. For CPR, where the ground truth contains only 6 instruction steps, the performance remains fairly stable with the increasing of the number of discovered steps. This illustrates the robustness of our method to the exact choice of K. Qualitative results of discovered steps in video and text are illustrated in Figure 3."}, {"heading": "4.3 Using visual cues to improve text clustering", "text": "The motivation here is to assess whether we can improve the clustering of text with the results obtained from vision. Recall that in order to maximize the precision over our clustered direct object\n4We use here the improved model on Z from [4] which does not require a \u201cbackground class\u201d and yields a stronger baseline equivalent to our model without the narration constraints.\nrelations, we have only allowed direct objects relations that have a similarity score sd (defined in equation (2)) of 1 to be aligned during the initial MSA (see Section 3.1). In particular, even if they have a high similarity score sd, we wanted to avoid aligning together the direct object relations jack car and lower car. However, even if they do not have a similarity score sd of 1, we would now want to be able to refine our grouping by clustering the direct object relations lower car and lower jack together.\nThe following experiment evaluates how well our vision prediction improves the similarity measure sd for the task of text clustering. To do so, we compared different methods for augmenting the direct object relations clusters obtained in Section 4.1 for the important recovered instruction steps. Recall that at the end of the multiple sequence alignment algorithm, these instruction steps are represented by a list of direct object relations. For example, in Figure 2, the first instruction step is represented by loosen nut and undo bolt. We obtain ground truth clusters for each step by manually finding the possible direct object relations that could be used to meaningfully describe them. Given a clustering similarity score between direct object relations and an instruction step, we can compute a precisionrecall curve by varying a threshold on this similarity measure, and then measure performance by the average precision.\nWe compare the \u201cNLP\u201d baseline to our full method \u201cNLP+Vision\u201d by defining different clustering similarity measures. The clustering similarity measure between a direct object relation d and an instruction event e for the \u201cNLP\u201d baseline is simply defined as the average sd over the pairs (d, d\u2032), where d\u2032 varies over the list of (high precision) direct object relations that compose e and were obtained after the MSA used in Section 4.1. \u201cNLP+Vision\u201d uses detected instruction steps in the video to modify the previous clustering similarity measure. In particular, we remove direct object relations which do not have support in the video signal. This is implemented by setting the clustering similarity measure sd to 0 for cases with no temporal overlap between the instruction step in the video and direct object relation. We also consider a variant of an \u201cupper bound\u201d denoted as \u201cNLP + GT Vision\u201d and evaluate the potential benefits of perfect visual constraints obtained from the manual annotation of instruction steps in the video. For the \u201cNLP+Vision\u201d method, we adjust hyperparameters \u2206b and \u2206f by cross-validation and report an average with the standard deviation over 40 random splits.\nWe give results for 7 of the 9 recovered instruction steps (K = 15) of the Changing car tire in Table 3. We remove two instruction steps from the evaluation (get tools out and put tools back) because of their too high subjectivity in text. For the majority of instruction steps, text clustering benefits from constraints obtained through the visual localization. This again illustrates the benefits of the joint modeling of video and text by our method. Finally, from the results of \u201cNLP + GT Vision\u201d we can conclude that improvements in visual localization should lead to further improvements in text clustering."}, {"heading": "5 Conclusion", "text": "We present a method to discover the main steps of narrated instruction videos obtained from YouTube. We formulate the method as a joint clustering problem of video and narrated text and demonstrate benefits of this joint approach on two tasks. Next we plan to scale our method to more tasks and model person-object interactions in both video and text. Integrating automatic speech recognition into our joint framework could be also beneficial."}, {"heading": "6 Acknowledgments", "text": "This research was supported in part by a Google Research Award, and the ERC grants VideoWorld (no. 267907), Activia (no. 307574) and LEAP (no. 336845)."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We address the problem of automatically learning the main steps to complete a<lb>certain task, such as changing a car tire, from a set of narrated instruction videos.<lb>The contributions of this paper are three-fold. First, we develop a joint model for<lb>video and natural language narration that takes advantage of the complementary<lb>nature of the two signals. Second, we collect an annotated dataset of 57 Internet in-<lb>struction videos containing more than 350,000 frames for two tasks (changing car<lb>tire and CardioPulmonary Resuscitation). Third, we experimentally demonstrate<lb>that the proposed model automatically discovers, in an unsupervised manner, the<lb>main steps to achieve each task and locate them within the input videos. The re-<lb>sults further show that the proposed model outperforms single-modality baselines,<lb>demonstrating the benefits of joint modeling video and text.", "creator": "LaTeX with hyperref package"}}}