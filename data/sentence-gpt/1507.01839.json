{"id": "1507.01839", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jul-2015", "title": "Dependency-based Convolutional Neural Networks for Sentence Embedding", "abstract": "In sentence modeling and classification, convolutional neural network approaches have recently achieved state-of-the-art results, but all such efforts process word vectors sequentially and neglect long-distance dependencies. To exploit both deep learning and linguistic structures, we propose a tree-based convolutional neural network model which exploit various long-distance relationships between words. Our model improves the sequential baselines on all three sentiment and question classification tasks, and achieves the highest published accuracy on TREC. We further propose a model that addresses the need to optimize learning with deep learning. In our models, convolutional neural networks (CTMs) perform similar performance as convolutional neural networks. To avoid problems with classification, we use the model for an analysis of the temporal information of all four semantic clusters. We use the model for an analysis of the first and second semantic clusters. This model provides for the most detailed visualization of language performance across languages: the first and the second semantic clusters. For this analysis, we use the model for a deep learning model that generates a full-scale statistical-fitting matrix. This model also allows for a more detailed description of the language structure and classification capabilities. We further investigate the relative contribution of the posterior and posterior COD-style convolutional neural networks in terms of convolutional neural networks. The result is that convolutional neural networks generate a full-scale statistical-fitting matrix based on the classification accuracy of the first and second semantic clusters. Our model achieves the most comprehensive model with a complete visual representation of the language structure of all four semantic clusters. The model can be run using the following methods:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 7 Jul 2015 15:20:36 GMT  (530kb,D)", "http://arxiv.org/abs/1507.01839v1", "this paper has been accepted by ACL 2015"], ["v2", "Mon, 3 Aug 2015 15:36:45 GMT  (538kb,D)", "http://arxiv.org/abs/1507.01839v2", "this paper has been accepted by ACL 2015"]], "COMMENTS": "this paper has been accepted by ACL 2015", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["mingbo ma", "liang huang", "bowen zhou", "bing xiang"], "accepted": true, "id": "1507.01839"}, "pdf": {"name": "1507.01839.pdf", "metadata": {"source": "CRF", "title": "Tree-based Convolution for Sentence Modeling\u2217", "authors": ["Mingbo Ma", "Liang Huang", "Bing Xiang", "Bowen Zhou"], "emails": ["lhuang@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com"], "sections": [{"heading": null, "text": "1 Introduction Convolutional neural networks (CNNs), originally invented in computer vision (LeCun et al., 1995), has recently attracted much attention in natural language processing (NLP) on problems such as sequence labeling (Collobert et al., 2011), semantic parsing (Yih et al., 2014), and search query retrieval (Shen et al., 2014). In particular, recent work on CNN-based sentence modeling (Kalchbrenner et al., 2014; Kim, 2014) has achieved excellent, often state-of-the-art, results on various classification tasks such as sentiment, subjectivity, and question-type classification. However, despite their celebrated success, there remains a major limitation from the linguistics perspective: CNNs, being invented on pixel matrices in image processing, only consider sequential n-grams that are consecutive on the surface string and neglect longdistance dependencies, while the latter play an important role in many linguistic phenomena such as negation, subordination, and wh-extraction, all of which might dully affect the sentiment, subjectivity, or other categorization of the sentence.\n\u2217This work was done at both IBM and CUNY, and was supported in part by DARPA FA8750-13-2-0041 (DEFT), and NSF IIS-1449278.\nIndeed, in the sentiment analysis literature, researchers have incorporated long-distance information from syntactic parse trees, but the results are somewhat inconsistent: some reported small improvements (Gamon, 2004; Matsumoto et al., 2005), while some otherwise (Dave et al., 2003; Kudo and Matsumoto, 2004). As a result, syntactic features have yet to become popular in the sentiment analysis community. We suspect one of the reasons for this is data sparsity (according to our experiments, tree n-grams are significantly sparser than surface n-grams), but this problem has largely been alleviated by the recent advances in word embedding. Can we combine the advantages of both worlds?\nSo we propose a very simple tree-based convolutional neural networks. We show dependency tree-based convolutional neural networks (DTCNNs) in this paper, but it also could be easily extended to constituency tree-based convolutional neural networks (CTCNNs). Our model is similar to Kim (2014), but while his sequential CNNs put a word in its sequential context, ours considers a word and its parent, grand-parent, great-grandparent, and siblings on the dependency tree. This way we incorporate long-distance information that are otherwise unavailable on the surface string.\nExperiments on three classification tasks demonstrate the superior performance of our DTCNNs over the baseline sequential CNNs. In particular, our accuracy on the TREC dataset outperforms all previously published results in the literature, including those with heavy hand-engineered features.\n2 Dependency Tree-based Convolution\nThe original CNN, first proposed by LeCun et al. (1995), applies convolution kernels on a series of continuous areas of given images, and was adapted to NLP by Collobert et al. (2011). Following Kim (2014), one dimensional convolution operates the convolution kernel in sequential order in Equation 1, where xi \u2208 Rd represents the d dimensional\nar X\niv :1\n50 7.\n01 83\n9v 1\n[ cs\n.C L\n] 7\nJ ul\n2 01\n5\nDespite the film \u2019s shortcomings the stories are quietly moving .\nROOT\nFigure 1: Running example from Movie Reviews dataset.\nword representation for the i-th word in the sentence, and \u2295 is the concatenation operator. Therefore x\u0303i,j refers to concatenated word vector from the i-th word to the (i+ j)-th word in sentence:\nx\u0303i,j = xi \u2295 xi+1 \u2295 \u00b7 \u00b7 \u00b7 \u2295 xi+j (1) Sequential word concatenation x\u0303i,j works as n-gram models which feeds local information into convolution operations. However, this setting can not capture long-distance relationships unless we enlarge the window indefinitely which would inevitably cause the data sparsity problem.\nIn order to capture the long-distance dependencies we propose the dependency tree-based convolution model (DTCNN). Figure 1 illustrates an example from the Movie Reviews (MR) dataset (Pang and Lee, 2005). The sentiment of this sentence is obviously positive, but this is quite difficult for sequential CNNs because many n-gram windows would include the highly negative word \u201cshortcomings\u201d, and the distance between \u201cDespite\u201d and \u201cshortcomings\u201d is quite long. DTCNN, however, could capture the tree-based bigram \u201cDespite \u2013 shortcomings\u201d, thus flipping the sentiment, and the tree-based trigram \u201cROOT \u2013 moving \u2013 stories\u201d, which is highly positive.\n2.1 Convolution on Ancestor Paths We define our concatenation based on the dependency tree for a given modifier xi: xi,k = xi \u2295 xp(i) \u2295 \u00b7 \u00b7 \u00b7 \u2295 xpk\u22121(i) (2) where function pk(i) returns the i-th word\u2019s k-th ancestor index, which is recursively defined as:\npk(i) = { p(pk\u22121(i)) if k > 0 i if k = 0\n(3)\nTable 1 (left) illustrates ancestor paths patterns with various orders. We always start the convolution with xi and concatenate with its ancestors. If the root node is reached, we add \u201cROOT\u201d as dummy ancestors (vertical padding).\nFor a given tree-based concatenated word sequence xi,k, the convolution operation applies a filter w \u2208 Rk\u00d7d to xi,k with a bias term b described in equation 4:\nci = f(w \u00b7 xi,k + b) (4) where f is a non-linear activation function such as rectified linear unit (ReLu) or sigmoid function. The filter w is applied to each word in the sentence, generating the feature map c \u2208 Rl: c = [c1, c2, \u00b7 \u00b7 \u00b7 , cl] (5) where l is the length of the sentence.\n2.2 Max-Over-Tree Pooling and Dropout The filters convolve with different word concatenation in Eq. 4 can be regarded as pattern detection: only the most similar pattern between the words and the filter could return the maximum activation. In sequential CNNs, max-over-time pooling (Collobert et al., 2011; Kim, 2014) operates over the feature map to get the maximum activation c\u0302 = max c representing the entire feature map. Our DTCNNs also pool the maximum activation from feature map to detect the strongest activation over the whole tree (i.e., over the whole sentence). Since the tree no longer defines a sequential \u201ctime\u201d direction, we refer to our pooling as \u201cmax-over-tree\u201d pooling.\nIn order to capture enough variations, we randomly initialize the set of filters to detect different structure patterns. Each filter\u2019s height is the number of words considered and the width is always equal to the dimensionality d of word representation. Each filter will be represented by only one feature after max-over-tree pooling. After a series of convolution with different filter with different heights, multiple features carry different structural information become the final representation of the input sentence. Then, this sentence representation is passed to a fully connected soft-max layer and outputs a distribution over different labels.\nNeural networks often suffer from overtraining. Following Kim (2014), we employ random dropout on penultimate layer (Hinton et al., 2012). in order to prevent co-adaptation of hidden units. In our experiments, we set our drop out rate as 0.5 and learning rate as 0.95 by default. Following Kim (2014), training is done through stochastic gradient descent over shuffled mini-batches with the Adadelta update rule (Zeiler, 2012).\nFigure 1: unning exa ple fro ovie Reviews dataset.\nword representation for the i-th word in the sentence, and \u2295 is the concatenation operator. Therefore x\u0303i,j refers to concatenated word vector from the i-th word to the (i+ j)-th word in sentence:\nx\u0303i,j = xi \u2295 xi+1 \u2295 \u00b7 \u00b7 \u00b7 \u2295 xi+j (1) Sequential word concatenation x\u0303i,j works as n-gram models which feeds local information into convolution operations. However, this setting can not capture long-distance relationships unless we enlarge the window indefinitely which would inevitably cause the data sparsity problem.\nIn order to capture the long-distance dependencies we propose the dependency tree-based convolution model (DTCNN). Figure 1 illustrates an example from the Movie Reviews (MR) dataset (Pang and Lee, 2005). The sentiment of this sentence is obviously positive, but this is quite difficult for sequential CNNs because many n-gram windows would include the highly negative word \u201cshortcomings\u201d, and the distance between \u201cDespite\u201d and \u201cshortcomings\u201d is quite long. DTCNN, however, could capture the tree-based bigram \u201cDespite \u2013 shortcomings\u201d, thus flipping the sentiment, and the tree-based trigram \u201cROOT \u2013 moving \u2013 stories\u201d, which is highly positive.\n2.1 Convolution on Ancestor Paths We define our concatenation based on the dependency tree for a given modifier xi: xi,k = xi \u2295 xp(i) \u2295 \u00b7 \u00b7 \u00b7 \u2295 xpk\u22121(i) (2) where function pk(i) returns the i-th word\u2019s k-th ancestor index, which is recursively defined as:\npk(i) = { p(pk\u22121(i)) if k > 0 i if k = 0\n(3)\nFigure 2 (left) illustrates ancestor paths patterns with various orders. We always start the convolution with xi and concatenate with its ancestors. If the root node is reached, we add \u201cROOT\u201d as dummy ancestors (vertical padding).\nFor a given tree-based concatenated word sequence xi,k, the convolution operation applies a filter w \u2208 Rk\u00d7d to xi,k with a bias term b described in equation 4:\nci = f(w \u00b7 xi,k + b) (4) here f is a non-linear activation function such as rectified linear unit (ReLu) or sigmoid function. The filter w is applied to each word in the sentence, generating the feature map c \u2208 Rl:\nc = [c1, c2, \u00b7 , cl] (5) here l is the length of the sentence.\n2.2 ax-Over-Tr e P oling and Dropout The filters convolve with different word concatenation in Eq. 4 can be regarded as pattern d tection: only the most similar pattern between the\nords and the filter could return the maximum activation. In sequential CNNs, max-over-time pooling (Collobert et al., 20 1; Kim, 2014) operates over the feature map to get the maximum activation c\u0302 = max c representing the entire feature\nap. Our DTCNNs also pool the maximum activation from feature map to detect the strongest activation over the whole tree (i.e., over the whole sentence). Since the tree no longer defines a sequential \u201ctime\u201d direction, we refer to our pooling as \u201cmax-over-tree\u201d pooling.\nIn order to capture enough variations, we randomly initialize the set of filters to detect different structure patterns. Each filter\u2019s height is the number of words considered and the width is always equal to the dimensionality d of word representation. Each filter will be represented by only one feature after max-over-tree pooling. After a series of convolution with different filter with different heights, multiple features carry different structural information become the final representation of the input sentence. Then, this sentence representation is passed to a fully connected soft-max layer and outputs a distribution over different labels.\nNeural networks often suffer from overtraining. Following Kim (2014), we employ random dropout on penultimate layer (Hinton et al., 2012). in order to prevent co-adaptation of hidden units. In our experiments, we set our drop out rate as 0.5 and learning rate as 0.95 by default. Following Kim (2014), training is done through stochastic gradient descent over shuffled mini-batches with the Adadelta update rule (Zeiler, 2012).\nancestor paths siblings n pattern(s) n pattern(s)\n3 m h g\n2 s m\n4 m h g g2\n3 s m h t s m\n5 m h g g2 g3\n4 t s m h s m h g\nTable 1: Tree-based convolution patterns. Word concatenation always starts with m, while h, g, and g2 denote parent, grand parent, and great-grand parent, etc., and \u201c \u201d denotes words excluded in convolution.\n2.3 Convolution on Siblings Ancestor paths alone is not enough to capture many linguistic phenomena such as conjunction. Inspired by higher-order dependency parsing (McDonald and Pereira, 2006; Koo and Collins, 2010), we also incorporate siblings for a given word in various ways. See Table 1 (right) for details.\n2.4 Combined Model Powerful as it is, structural information still does not fully cover sequential information. Also, parsing errors (which are common especially for informal text such as online reviews) directly affect DTCNN performance while sequential n-grams are always correctly observed. To best exploit both information, we want to combine both models. The easiest way of combination is to concatenate these representations together, then feed into fully connected soft-max neural networks. In these cases, combine with different feature from different type of sources could stabilize the performance. The final sentence representation is thus:\nc\u0302 = [c\u0302(1)a , ..., c\u0302 (Na) a\ufe38 \ufe37\ufe37 \ufe38\nancestors\n; c\u0302(1)s , ..., c\u0302 (Ns) s\ufe38 \ufe37\ufe37 \ufe38\nsiblings\n; c\u0302(1), ..., c\u0302(N)\ufe38 \ufe37\ufe37 \ufe38 sequential ]\nwhere Na, Ns, and N are the number of ancestor, sibling, and sequential filters. In practice, we use 100 filters for each template in Table 1. The fully combined representation is 1100-dimensional by contrast to 300-dimensional for sequential CNN.\n3 Experiments We implement our DTCNN on top of the open source CNN code by Kim (2014).1 Table 2 summarizes our results in the context of other high-performing efforts in the literature. We use three benchmark datasets in two categories: sentiment analysis on both Movie Review (MR) (Pang and Lee, 2005) and Stanford Sentiment Treebank\n1https://github.com/yoonkim/CNN sentence\n(SST-1) (Socher et al., 2013) datasets, and question classification on TREC (Li and Roth, 2002).\nFor all datasets, we first obtain the dependency parse tree from Stanford parser (Manning et al., 2014).2 Different window size for different choice of convolution are shown in Table 1. For the dataset without a development set (MR), we randomly choose 10% of the training data to indicate early stopping. In order to have a fare comparison with baseline CNN, we also use 3 to 5 as our window size. Most of our results are generated by GPU due to its efficiency, however CPU potentially could generate better results.3 Our implementation can be found on Github.4\n3.1 Sentiment Analysis Both sentiment analysis datasets (MR and SST1) are based on movie reviews. The differences between them are mainly in the different numbers of categories and whether the standard split is given. There are 10,662 sentences in the MR dataset. Each instance is labeled positive or negative, and in most cases contains one sentence. Since no standard data split is given, following the literature we use 10 fold cross validation to include every sentence in training and testing at least once. Concatenating with sibling and sequential information obviously improves tree-based CNNs, and the final model outperforms the baseline sequential CNNs by 0.4, and ties with Zhu et al. (2015).\nDifferent from MR, the Stanford Sentiment Treebank (SST-1) annotates finer-grained labels, very positive, positive, neutral, negative and very negative, on an extension of the MR dataset. There are 11,855 sentences with standard split. Our model achieves an accuracy of 49.5 which is second only to Irsoy and Cardie (2014). We set batch size to 100 for this task.\n2The phrase-structure trees in SST-1 are actually automatically parsed, and thus can not be used as gold-standard trees.\n3GPU only supports float32 while CPU supports float64. 4https://github.com/cosmmb/DTCNN\nFigure 2: Tree-based convolution patterns. Word concatenation always starts with m, while h, g, and g2 denote parent, grand parent, and great-grand parent, etc., and \u201c \u201d denotes words excluded in convolution.\n2.3 Convolution on Siblings Ancestor paths alone is not enough to capture many linguistic phenomena such as conjuncti . Inspired by higher-order dependency parsing ( Donald and Pe eira, 2006; K o and Co lins, 2 we also incorporate siblings for a given word in various ways. See Figure 2 (right) for details.\n2.4 Combined Model Powerful as it is, structural information still does not fully cover sequential information. Also, parsing errors (which are common especially for informal text such as online reviews) directly affect DTCNN performance while sequential n-grams are always correctly observed. To best exploit both information, we want to combine both models. The easiest way of combination is to c n catenate these representations together, then feed i to fully connected soft-max neural networks. In these cases, combine with different feature from different type of sources could stabilize the performance. The final sentence representation is thus:\nc\u0302 = [c\u0302(1)a , ..., c\u0302 (Na) a\ufe38 \ufe37\ufe37 \ufe38\nancestors\n; c\u0302(1)s , ..., c\u0302 (Ns) s\ufe38 \ufe37\ufe37 \ufe38\nsiblings\n; c\u0302(1), ..., c\u0302(N)\ufe38 \ufe37\ufe37 \ufe38 sequential ]\nwhere Na, Ns, and N are the number of ancestor, sibling, and sequential filters. In practice, we use 100 filters for each template in Figure 2 . The fully combi ed representation is 1100-dimensional by con ra t t 300-dimensional for sequential CNN.\n3 Experiments We implement our DTCNN on top of the open source CNN code by Kim (2014).1 Table 1 summarizes our results in the context of other high-performing efforts in the literature. We use three benchmark datasets in two categories: sentiment analysis on both Movie Review (MR) (Pang\n1https://github.com/yoonkim/CNN sentence\nand Lee, 2005) and Stanford Sentiment Treebank (SST-1) (Socher et al., 2013) datasets, and question classification on TREC (Li and Roth, 2002).\nFor all datasets, we first obtain the dependency parse tree from S anford parser (Manning e al., 2014).2 Differe t window size for different choice of convolution are shown in Figure 2. For the dataset without a development set (MR), we randomly choose 10% of the training data to indicate early stopping. In order to have a fare comparison with baseline CNN, we also use 3 to 5 as our window size. Most of our results are generated by GPU due to its efficiency, however CPU potentially could generate better results.3 Our implementation can be found on Github.4\n3.1 Sentiment Analysis oth sentiment analysis datasets (MR and SST) are based on movie reviews. The differenc s t een them are mainly in the different umrs of categories and whether the standar split is given. There are 10,662 sentences in the MR dataset. Each instance is labeled positive or negative, and in most cases contains one sentence. Since no standard data split is given, following the literature we use 10 fold cross validation to include every sentence in training and testing at least once. Concatenating with sibling and sequential information obviously improves tree-based CNNs, and the final model outperforms the baseline sequential CNNs by 0.4, and ties with Zhu et al. (2015).\nDifferent from MR, the Stanford Sentiment Treebank (SST-1) annotates finer-grained labels, very positive, positive, eutral, negative a d very n gative, on an extension of the MR dataset. There ar 11,855 se t nc s with standard split. Our\n2The phrase-structure trees in SST-1 are actually automatically parsed, and thus can not be used as gold-standard trees.\n3GPU only supports float32 while CPU supports float64. 4https://github.com/cosmmb/DTCNN\nCategory Model MR SST-1 TREC TREC-2 This work DTCNNs: ancestor 80.4\u2020 47.7\u2020 95.4\u2020 88.4\u2020\nDTCNNs: ancestor+sibling 81.7\u2020 48.3\u2020 95.6\u2020 89.0\u2020 DTCNNs: ancestor+sibling+sequential 81.9 49.5 95.4\u2020 88.8\u2020\nCNNs CNNs-non-static (Kim, 2014) \u2013 baseline 81.5 48.0 93.6 86.4\u2217\nCNNs-multichannel (Kim, 2014) 81.1 47.4 92.2 86.0\u2217 Deep CNNs (Kalchbrenner et al., 2014) - 48.5 93.0 -\nRecursive NNs Recursive Autoencoder (Socher et al., 2011) 77.7 43.2 - - Recursive Neural Tensor (Socher et al., 2013) - 45.7 - - Deep Recursive NNs (Irsoy and Cardie, 2014) - 49.8 - - Recurrent NNs LSTM on tree (Zhu et al., 2015) 81.9 48.0 - - Other deep learning Paragraph-Vec (Le and Mikolov, 2014) - 48.7 - - Hand-coded rules SVMS (Silva et al., 2011) - 95.0 90.8\nTable 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets. TREC-2 is TREC with fine grained labels. \u2020Results generated by GPU (all others generated by CPU).\n\u2217Results generated from Kim (2014)\u2019s implementation.\nmodel achieves an accuracy of 49.5 which is second only to Irsoy and Cardie (2014). We set batch size to 100 for this task.\n3.2 Question Classification\nIn the TREC dataset, the entire dataset of 5,952 sentences are classified into the following 6 categories: abbreviation, entity, description, location and numeric. In this experiment, DTCNNs easily outperform any other methods even with ancestor convolution only. DTCNNs with sibling achieve the best performance in the published literature. DTCNNs combined with sibling and sequential information might suffer from overfitting on the training data based on our observation. One thing to note here is that our best result even exceeds SVMS (Silva et al., 2011) with 60 handcoded rules. We set batch size to 210 for this task.\nThe TREC dataset also provides subcategories such as numeric:temperature, numeric:distance, and entity:vehicle. To make our task more realistic and challenging, we also test the proposed model with respect to the 50 subcategories. There are obvious improvements over sequential CNNs from the last column of Table 1. Like ours, Silva et al. (2011) is a tree-based system but it uses constituency trees compared to ours dependency trees. They report a higher fine-grained accuracy of 90.8 but their parser is trained only on the QuestionBank (Judge et al., 2006) while we used the standard Stanford parser trained on both the Penn Treebank and QuestionBank. Moreover, as mentioned above, their approach is rule-based while ours is automatically learned. For this task, we set batch size to 30.\nCategory Model MR SST-1 TREC TREC-2 This work DTCNNs: ancestor 80.4\u2020 47.7\u2020 95.4\u2020 88.4\u2020\nDTCNNs: ancestor+sibling 81.7\u2020 48.3\u2020 95.6\u2020 89.0\u2020 DTCNNs: ancestor+sibling+sequential 81.9 49.5 95.4\u2020 88.8\u2020\nCNNs CNNs-non-static (Kim, 2014) \u2013 baseline 81.5 48.0 93.6 86.4\u2217\nCNNs-multichannel (Kim, 2014) 81.1 47.4 92.2 86.0\u2217 Deep CNNs (Kalchbrenner et al., 2014) - 48.5 93.0 -\nRecursive NNs Recursive Autoencoder (Socher et al., 2011) 77.7 43.2 - - Recursive Neural Tensor (Socher et al., 2013) - 45.7 - - Deep Recursive NNs (Irsoy and Cardie, 2014) - 49.8 - - Recurrent NNs LSTM on tree (Zhu et al., 2015) 81.9 48.0 - - Other deep learning Paragraph-Vec (Le and Mikolov, 2014) - 48.7 - - Hand-coded rules SVMS (Silva et al., 2011) - 95.0 90.8\nTable 2: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets. TREC-2 is TREC with fine grained labels. \u2020Results generated by GPU (all others generated by CPU). \u2217Results generated from Kim (2014)\u2019s implementation.\nWhat is Hawaii \u2019s state flower ?\nroot\n(a) enty\u21d2 loc\nWhat is natural gas composed of ?\nroot\n(b) enty\u21d2 desc\nWhat does a defibrillator do ?\nroot\n(c) desc\u21d2 enty\nNothing plot wise is worth emailing home about\nroot\n(d) mild negative\u21d2 mild positive\n3.2 Question Classification In the TREC dataset, the entire dataset of 5,952 sentences are classified into the following 6 categories: abbreviation, entity, description, location and numeric. In this experiment, DTCNNs easily outperform any other methods even with ancestor convolution only. DTCNNs with sibling achieve the best performance in the published literature. DTCNNs combined with sibling and sequential information might suffer from overfitting on the training data based on our observation. One thing to note here is that our best result even exceeds SVMS (Silva et al., 2011) with 60 handcoded rules. We set batch size to 210 for this task.\nThe TREC dataset also provides subcategories such as numeric:temperature, numeric:distance, and entity:vehicle. To make our task more realistic and challenging, we also test the proposed model with respect to the 50 subcategories. There are obvious improvements over sequential CNNs from the last column of Table 2. Like ours, Silva et al. (2011) is a tree-based system but it uses constituency trees compared to ours dependency trees. They report a higher fine-grained accuracy of 90.8 but their parser is trained only on the QuestionBank (Judge et al., 2006) while we used the standard Stanford parser trained on both the Penn Treebank and QuestionBank. Moreover, as mentioned above, their approach is rule-based while ours is automatically learned. For this task, we set batch size to 30.\n3.3 Discussions and Examples Compared with sentiment analysis, the advantage of our proposed model is obviously more substantial on the TREC dataset. Based on our error anal-\nFigure 3: Examples from TREC (a\u2013c), SST-1 (d)\nand TREC with fine-grained label (e\u2013f) that are\nmisclassified by the baseline CNN but correctly\nlabeled b our DTCNN. For example, (a) should\nbe entity but is labeled location by CNN.\nWhat is the speed hummingbirds fly ? (noun)\nroot\n(a) num\u21d2 enty\nWhat body of water are the Canary Islands in ?\nroot\n(b) loc\u21d2 enty\nWhat position did Willie Davis play in baseball ?\nroot\n(c) hum\u21d2 enty\nFigure 3: Examples from TREC datasets that are misclassified by DTCNN but correctly labeled by baseline CNN. For example, (a) should be numerical but is labeled entity by DTCNN.\nysis, we conclude that this is mainly due to the difference of the parse tree quality between the two tasks. In sentiment analysis, the dataset is collected from the Rotten Tomatoes website which includes many irregular usage of language. Some of the sentences even come from languages other than English. The errors in parse trees inevitably affect the classification accuracy. However, the parser works substantially better on the TREC dataset since all questions are in formal written English, and the training set for Stanford parser5 already includes the QuestionBank (Judge et al., 2006) which includes 2,000 TREC sentences.\nFigure 2 visualizes examples where CNN errs while DTCNN does not. For example, CNN labels (a) as location due to \u201cHawaii\u201d and \u201cstate\u201d, while the long-distance backbone \u201cWhat \u2013 flower\u201d is clearly asking for an entity. Similarly, in (d), DTCNN captures the obviously negative treebased trigram \u201cNothing \u2013 worth \u2013 emailing\u201d. Note that our model also works with non-projective dependency trees such as the one in (b). The last two examples in Figure 2 visualize cases where DTCNN outperforms the baseline CNNs in finegrained TREC. In example (e), the word \u201ctemperature\u201d is at second from the top and is root of a 8 word span \u201cthe ... earth\u201d. When we use a window of size 5 for tree convolution, every words in that span get convolved with \u201ctemperature\u201d and this should be the reason why DTCNN get correct.\n5http://nlp.stanford.edu/software/parser-faq.shtml\nWhat is the melting point of copper ?\nroot\n(a) num\u21d2 enty and desc\nWhat did Jesse Jackson organize ?\nroot\n(b) hum\u21d2 enty and enty\nWhat is the electrical output in Madrid , Spain ?\nroot\n(c) enty\u21d2 num and num\nFigure 4: Examples from TREC datasets that are\nmisclassified by both DTCNN and baseline CNN.\nFor example, (a) should be numerical but is la-\nbeled entity by DTCNN and description by CNN.\nFigure 3 showcases examples where baseline\nCNNs get better results than DTCNNs. Example (a) is misclassified as entity by DTCNN due to parsing/tagging error (the Stanford parser performs its own part-of-speech tagging). The word \u201cfly\u201d at the end of the sentence should be a verb instead of noun, and \u201chummingbirds fly\u201d should be a relative clause modifying \u201cspeed\u201d.\nThere are some sentences that are misclassified by both the baseline CNN and DTCNN. Figure 4 shows three such examples. Example (a) is not classified as numerical by both methods due to the ambiguous meaning of the word \u201cpoint\u201d which is difficult to capture by word embedding. This word can mean location, opinion, etc. Apparently, the numerical aspect is not captured by word embedding. Example (c) might be an annotation error.\nFrom the mistakes made by DTCNNs, we find the performance of DTCNN is mainly limited by two factors: the accuracy of the parser and the quality of word embedding. Future work will focus on these two issues.\n4 Conclusions and Future Work\nWe have presented a very simple dependency treebased convolution framework which outperforms sequential CNN baselines on various classification tasks. Extensions of this model would consider dependency labels and constituency trees. Also, we would evaluate on gold-standard parse trees.\nFigure 4: Examples from TREC datasets that are misclassified by DTCNN but correctly labeled by baseline CNN. For example, (a) should be numerical but is labeled entity by DTCNN.\n3.3 Discussions and Examples Compared with sentiment analysis, the advantage of our proposed model is obviously more substantial on the TREC dataset. Based on our error analysis, we conclude that this is mainly due to the differ nce of the p rse tree quality b tween the two tasks. In sentiment analysis, the dataset is collected from the Rotten Tomatoes website which includes many irregular usage of language. Some of the sentences even come from languages other than English. The errors in parse trees inevitably affect the classification accuracy. However, the pars r works substantially better on the TREC dataset since all questions are in formal ritten English, and the training set for Stanford parser5 already includes the QuestionBank (Judge et al., 2006) which includes 2,000 TREC sentences.\nFigure 3 visualizes examples where CNN errs while DTCNN does not. For example, CNN labels (a) as location due to \u201cHawaii\u201d and \u201cstate\u201d, while the long-distance backbone \u201cWhat \u2013 flower\u201d is clearly asking for an entity. Similarly, in (d), DTCNN captures the obviously negative treebased trigram \u201cNothing \u2013 worth \u2013 emailing\u201d. Note that our model also works with non-projective dependency trees such as the one in (b). The last two examples in Figure 3 visualize cases where DTCNN outperforms the baseline CNNs in finegrained TREC. In example (e), the word \u201ctemperature\u201d is at second from the top and is root of a 8 word span \u201cthe ... earth\u201d. When we use a win-\n5http://nlp.stanford.edu/software/parser-faq.shtml\nWhat is the speed hummingbirds fly ? (noun)\nroot\n(a) num\u21d2 enty\nWhat body of water are the Canary Islands in ?\nroot\n(b) loc\u21d2 enty\nWhat position did Willie Davis play in baseball ?\nroot\n(c) hum\u21d2 enty\nFigure 3: Examples from TREC datasets that are misclassified by DTCNN but correctly labeled by aseline CNN. For example, (a) should be numerical but is labeled entity by DTCNN.\ny is, we conclud that this is mainly due to the difference of the parse tree quality between the two tasks. In sentiment analysis, the dataset is collected from the Rotten Tomatoes website which includes many irregular usage of language. Some of the sentences even come from languages other than English. The errors in parse trees inevitably affect the classification accuracy. However, the parser works substantially better on the TREC dataset sinc all q estions are in formal written English, and the training set for St nford parser5 already includes the QuestionBank (Judge et al., 2006) which includes 2,000 TREC sentences.\nFigure 2 visualizes examples where CNN errs while DTCNN does not. For example, CNN labels (a) as location due to \u201cHawaii\u201d and \u201cstate\u201d, while the long-distance backbone \u201cWhat \u2013 flower\u201d is clearly asking for an entity. Similarly, in (d), DTCNN captures the obviously negative treebased igram \u201cNothing \u2013 worth \u2013 emailing\u201d. Note that our model also works with non-projective dependen y trees such as the one in (b). The last two examples in Figure 2 visualize cases where DTCNN outperforms the baseline CNNs in finegrained TREC. In example (e), the word \u201ctemperature\u201d is at second from the top and is root of a 8 word span \u201cthe ... earth\u201d. When we use a window of size 5 for tree convolution, every words in that pan get convolv d with \u201ctemperature\u201d and this should be the reason why DTCNN get correct.\n5http://nlp.stanford.edu/software/parser-faq.shtml\nWhat is the melting point of copper ?\nroot\n(a) num\u21d2 enty and desc\nWhat did Jesse Jackson organize ?\nroot\n(b) hum\u21d2 enty and enty\nWhat is the electrical output in Madrid , Spain ?\nroot\n(c) enty\u21d2 num and num\nFigure 4: Examples from TREC datasets that are misclassified by both DTCNN and baseline CNN. For example, (a) should be numerical but is labeled entity by DTCNN and description by CNN.\nFigure 3 showcases examples where baseline CNNs get better results than DTCNNs. Example (a) is misclassified as entity by DTCNN due to parsing/tagging error (the Stanford parser performs its own part-of-speech tagging). The word \u201cfly\u201d at the end of the sentence should be a verb instead of noun, and \u201chummingbirds fly\u201d should be a relative clause modifying \u201cspeed\u201d.\nThere are some sentences that are misclassified by both the baseline CNN and DTCNN. Figure 4 shows three such examples. Example (a) is not classified as numerical by both methods due to the ambiguous meaning of the word \u201cpoint\u201d which is difficult to capture by word embedding. This word can mean location, opinion, etc. Apparently, the numerical aspect is not captured by word embedding. Example (c) might be an annotation error.\nFrom the mistakes made by DTCNNs, we find the pe for ance of DTCNN is main y limited by two factor : the accuracy of the parser and the quality of word embedding. Future work will focus on these two issues.\n4 Conclusions and Future Work\nWe have presented a very simple dependency treebased convolution framework which outperforms sequential CNN baselines on various classification tasks. Extensions of this model would consider dependency labels and constituency trees. Also, we would evaluate on gold-standard parse trees.\nFigure 5: Examples from TREC datasets that are misclassified by both DTCNN and baseline CNN. For example, (a) should be numerical but is labeled entity by DTCNN and descriptio by CNN.\ndow of size 5 for tree convolution, every words in that span get convolved with \u201ctemperature\u201d and this should be the reason why DTCNN get correct.\nFigure 4 showcases examples where baseline CNNs get better results than DTCNNs. Example (a) is misclassified as entity by DTCNN due to parsing/tagging error (the Stanford parser performs its own part-of-speech tagging). The word \u201cfly\u201d at the nd of the sente ce should be a verb instead of noun, and \u201chummingbirds fly\u201d should be a relative cla s modifying \u201cspeed\u201d.\nThere are some sentenc s that are misclassified by both the baseline CNN and DTCNN. Figure 5 shows three suc ex mples. Example (a) is not classified as numerical by both methods due to the ambiguous meaning of the word \u201cpoint\u201d which is difficult to capture by word embedding. This word can mean location, opinion, etc. Apparently, the numerical aspect is not captured by word embedding. Example (c) might be an annotation error.\nFrom th mistakes mad by DTCNNs, we find the performance of DTCNN is mainly limited by two factors: the accuracy of the parser and the quality of word embedding. Future work will focu on these two issu s.\n4 Conclusions and Future Work We have presented a very simple depend ncy treebased convolution framework which outperforms sequential CNN baselines on various classification tasks. Extensions of this model would consider dependency labels and constituency trees. Also, we would evaluate on gold-standard parse trees.\nReferences R. Collobert, J. Weston, L. Bottou, M. Karlen,\nK. Kavukcuoglu, and P. Kuksa. 2011. Natural language processing (almost) from scratch. volume 12, pages 2493\u20132537.\nKushal Dave, Steve Lawrence, and David M Pennock. 2003. Mining the peanut gallery: Opinion extraction and semantic classification of product reviews. In Proceedings of the 12th international conference on World Wide Web, pages 519\u2013528. ACM.\nMichael Gamon. 2004. Sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis. In Proceedings of the 20th international conference on Computational Linguistics, page 841.\nGeoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors. volume abs/1207.0580.\nOzan Irsoy and Claire Cardie. 2014. Deep recursive neural networks for compositionality in language. In Advances in Neural Information Processing Systems, pages 2096\u20132104.\nJohn Judge, Aoife Cahill, and Josef van Genabith. 2006. Questionbank: Creating a corpus of parseannotated questions. In Proceedings of the 21st International Conference on Computational Linguistics, pages 497\u2013504.\nNal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 655\u2013665.\nYoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746\u2013 1751, Doha, Qatar, October. Association for Computational Linguistics.\nTerry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1\u201311. Association for Computational Linguistics.\nTaku Kudo and Yuji Matsumoto. 2004. Proceedings of the 2004 conference on empirical methods in natural language processing.\nQuoc V Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents.\nY. LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes, J. Denker, H. Drucker, I. Guyon, U. Mller, E. Sckinger, P. Simard, and V. Vapnik. 1995. Comparison of learning algorithms for handwritten digit\nrecognition. In INTERNATIONAL CONFERENCE ON ARTIFICIAL NEURAL NETWORKS, pages 53\u2013 60.\nXin Li and Dan Roth. 2002. Learning question classifiers. In Proceedings of the 19th International Conference on Computational Linguistics - Volume 1, COLING \u201902, pages 1\u20137. Association for Computational Linguistics.\nChristopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55\u201360.\nShotaro Matsumoto, Hiroya Takamura, and Manabu Okumura. 2005. Sentiment classification using word sub-sequences and dependency sub-trees. In Proceedings of the 9th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining.\nRyan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of EACL.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of ACL, pages 115\u2013124.\nYelong Shen, Xiaodong he, Jianfeng Gao, Li Deng, and Gregoire Mesnil. 2014. Learning semantic representations using convolutional neural networks for web search. WWW 2014, April.\nJ. Silva, L. Coheur, A. C. Mendes, and Andreas Wichert. 2011. From symbolic to sub-symbolic information in question classification. volume 35.\nRichard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011. Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions. In Proceedings of EMNLP 2011.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of EMNLP 2013.\nWen-tau Yih, Xiaodong He, and Christopher Meek. 2014. Semantic parsing for single-relation question answering. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 643\u2013648.\nMattgew Zeiler. 2012. Adadelta: An adaptive learning rate method. arxiv: abs/1212.5701.\nXiaodan Zhu, Parinaz Sobhani, and Hongyu Guo. 2015. Long short-term memory over tree structures. In In Proceedings of ICML."}], "references": [{"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa."], "venue": "volume 12, pages 2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Mining the peanut gallery: Opinion extraction and semantic classification of product reviews", "author": ["Kushal Dave", "Steve Lawrence", "David M Pennock."], "venue": "Proceedings of the 12th international conference on World Wide Web, pages 519\u2013528. ACM.", "citeRegEx": "Dave et al\\.,? 2003", "shortCiteRegEx": "Dave et al\\.", "year": 2003}, {"title": "Sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis", "author": ["Michael Gamon."], "venue": "Proceedings of the 20th international conference on Computational Linguistics, page 841.", "citeRegEx": "Gamon.,? 2004", "shortCiteRegEx": "Gamon.", "year": 2004}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "volume abs/1207.0580.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Ozan Irsoy", "Claire Cardie."], "venue": "Advances in Neural Information Processing Systems, pages 2096\u20132104.", "citeRegEx": "Irsoy and Cardie.,? 2014", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2014}, {"title": "Questionbank: Creating a corpus of parseannotated questions", "author": ["John Judge", "Aoife Cahill", "Josef van Genabith."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics, pages 497\u2013504.", "citeRegEx": "Judge et al\\.,? 2006", "shortCiteRegEx": "Judge et al\\.", "year": 2006}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 655\u2013665.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746\u2013 1751, Doha, Qatar, October. Association for Com-", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Efficient thirdorder dependency parsers", "author": ["Terry Koo", "Michael Collins."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1\u201311. Association for Computational Linguistics.", "citeRegEx": "Koo and Collins.,? 2010", "shortCiteRegEx": "Koo and Collins.", "year": 2010}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov"], "venue": null, "citeRegEx": "Le and Mikolov.,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Comparison of learning algorithms for handwritten digit", "author": ["Y. LeCun", "L. Jackel", "L. Bottou", "A. Brunot", "C. Cortes", "J. Denker", "H. Drucker", "I. Guyon", "U. Mller", "E. Sckinger", "P. Simard", "V. Vapnik"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1995\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1995}, {"title": "Learning question classifiers", "author": ["Xin Li", "Dan Roth."], "venue": "Proceedings of the 19th International Conference on Computational Linguistics - Volume 1, COLING \u201902, pages 1\u20137. Association for Computational Linguistics.", "citeRegEx": "Li and Roth.,? 2002", "shortCiteRegEx": "Li and Roth.", "year": 2002}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computa-", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Sentiment classification using word sub-sequences and dependency sub-trees", "author": ["Shotaro Matsumoto", "Hiroya Takamura", "Manabu Okumura."], "venue": "Proceedings of the 9th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Min-", "citeRegEx": "Matsumoto et al\\.,? 2005", "shortCiteRegEx": "Matsumoto et al\\.", "year": 2005}, {"title": "Online learning of approximate dependency parsing algorithms", "author": ["Ryan McDonald", "Fernando Pereira."], "venue": "Proceedings of EACL.", "citeRegEx": "McDonald and Pereira.,? 2006", "shortCiteRegEx": "McDonald and Pereira.", "year": 2006}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Bo Pang", "Lillian Lee."], "venue": "Proceedings of ACL, pages 115\u2013124.", "citeRegEx": "Pang and Lee.,? 2005", "shortCiteRegEx": "Pang and Lee.", "year": 2005}, {"title": "Learning semantic representations using convolutional neural networks for web search", "author": ["Yelong Shen", "Xiaodong he", "Jianfeng Gao", "Li Deng", "Gregoire Mesnil."], "venue": "WWW 2014, April.", "citeRegEx": "Shen et al\\.,? 2014", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "From symbolic to sub-symbolic information in question classification", "author": ["J. Silva", "L. Coheur", "A.C. Mendes", "Andreas Wichert."], "venue": "volume 35.", "citeRegEx": "Silva et al\\.,? 2011", "shortCiteRegEx": "Silva et al\\.", "year": 2011}, {"title": "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Proceedings of EMNLP 2011.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts."], "venue": "Proceedings of EMNLP 2013.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Semantic parsing for single-relation question answering", "author": ["Wen-tau Yih", "Xiaodong He", "Christopher Meek."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 643\u2013648.", "citeRegEx": "Yih et al\\.,? 2014", "shortCiteRegEx": "Yih et al\\.", "year": 2014}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Mattgew Zeiler."], "venue": "arxiv: abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Long short-term memory over tree structures", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo."], "venue": "In Proceedings of ICML.", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "Convolutional neural networks (CNNs), originally invented in computer vision (LeCun et al., 1995), has recently attracted much attention in natural language processing (NLP) on problems such as sequence labeling (Collobert et al.", "startOffset": 77, "endOffset": 97}, {"referenceID": 0, "context": ", 1995), has recently attracted much attention in natural language processing (NLP) on problems such as sequence labeling (Collobert et al., 2011), semantic parsing (Yih et al.", "startOffset": 122, "endOffset": 146}, {"referenceID": 20, "context": ", 2011), semantic parsing (Yih et al., 2014), and search query retrieval (Shen et al.", "startOffset": 26, "endOffset": 44}, {"referenceID": 16, "context": ", 2014), and search query retrieval (Shen et al., 2014).", "startOffset": 36, "endOffset": 55}, {"referenceID": 6, "context": "In particular, recent work on CNN-based sentence modeling (Kalchbrenner et al., 2014; Kim, 2014) has achieved excellent, often state-of-the-art, results on various classification tasks such as sentiment, subjectivity, and question-type classification.", "startOffset": 58, "endOffset": 96}, {"referenceID": 7, "context": "In particular, recent work on CNN-based sentence modeling (Kalchbrenner et al., 2014; Kim, 2014) has achieved excellent, often state-of-the-art, results on various classification tasks such as sentiment, subjectivity, and question-type classification.", "startOffset": 58, "endOffset": 96}, {"referenceID": 2, "context": "Indeed, in the sentiment analysis literature, researchers have incorporated long-distance information from syntactic parse trees, but the results are somewhat inconsistent: some reported small improvements (Gamon, 2004; Matsumoto et al., 2005), while some otherwise (Dave et al.", "startOffset": 206, "endOffset": 243}, {"referenceID": 13, "context": "Indeed, in the sentiment analysis literature, researchers have incorporated long-distance information from syntactic parse trees, but the results are somewhat inconsistent: some reported small improvements (Gamon, 2004; Matsumoto et al., 2005), while some otherwise (Dave et al.", "startOffset": 206, "endOffset": 243}, {"referenceID": 1, "context": ", 2005), while some otherwise (Dave et al., 2003; Kudo and Matsumoto, 2004).", "startOffset": 30, "endOffset": 75}, {"referenceID": 7, "context": "Our model is similar to Kim (2014), but while his sequential CNNs put a word in its sequential context, ours considers a word and its parent, grand-parent, great-grandparent, and siblings on the dependency tree.", "startOffset": 24, "endOffset": 35}, {"referenceID": 8, "context": "The original CNN, first proposed by LeCun et al. (1995), applies convolution kernels on a series of continuous areas of given images, and was adapted to NLP by Collobert et al.", "startOffset": 36, "endOffset": 56}, {"referenceID": 0, "context": "(1995), applies convolution kernels on a series of continuous areas of given images, and was adapted to NLP by Collobert et al. (2011). Following Kim (2014), one dimensional convolution operates the convolution kernel in sequential order in Equation 1, where xi \u2208 Rd represents the d dimensional ar X iv :1 50 7.", "startOffset": 111, "endOffset": 135}, {"referenceID": 0, "context": "(1995), applies convolution kernels on a series of continuous areas of given images, and was adapted to NLP by Collobert et al. (2011). Following Kim (2014), one dimensional convolution operates the convolution kernel in sequential order in Equation 1, where xi \u2208 Rd represents the d dimensional ar X iv :1 50 7.", "startOffset": 111, "endOffset": 157}, {"referenceID": 15, "context": "Figure 1 illustrates an example from the Movie Reviews (MR) dataset (Pang and Lee, 2005).", "startOffset": 68, "endOffset": 88}, {"referenceID": 0, "context": "In sequential CNNs, max-over-time pooling (Collobert et al., 2011; Kim, 2014) operates over the feature map to get the maximum activation \u0109 = max c representing the entire feature map.", "startOffset": 42, "endOffset": 77}, {"referenceID": 7, "context": "In sequential CNNs, max-over-time pooling (Collobert et al., 2011; Kim, 2014) operates over the feature map to get the maximum activation \u0109 = max c representing the entire feature map.", "startOffset": 42, "endOffset": 77}, {"referenceID": 3, "context": "Following Kim (2014), we employ random dropout on penultimate layer (Hinton et al., 2012).", "startOffset": 68, "endOffset": 89}, {"referenceID": 21, "context": "Following Kim (2014), training is done through stochastic gradient descent over shuffled mini-batches with the Adadelta update rule (Zeiler, 2012).", "startOffset": 132, "endOffset": 146}, {"referenceID": 0, "context": "In sequential CNNs, max-over-time pooling (Collobert et al., 2011; Kim, 2014) operates over the feature map to get the maximum activation \u0109 = max c representing the entire feature map. Our DTCNNs also pool the maximum activation from feature map to detect the strongest activation over the whole tree (i.e., over the whole sentence). Since the tree no longer defines a sequential \u201ctime\u201d direction, we refer to our pooling as \u201cmax-over-tree\u201d pooling. In order to capture enough variations, we randomly initialize the set of filters to detect different structure patterns. Each filter\u2019s height is the number of words considered and the width is always equal to the dimensionality d of word representation. Each filter will be represented by only one feature after max-over-tree pooling. After a series of convolution with different filter with different heights, multiple features carry different structural information become the final representation of the input sentence. Then, this sentence representation is passed to a fully connected soft-max layer and outputs a distribution over different labels. Neural networks often suffer from overtraining. Following Kim (2014), we employ random dropout on penultimate layer (Hinton et al.", "startOffset": 43, "endOffset": 1173}, {"referenceID": 0, "context": "In sequential CNNs, max-over-time pooling (Collobert et al., 2011; Kim, 2014) operates over the feature map to get the maximum activation \u0109 = max c representing the entire feature map. Our DTCNNs also pool the maximum activation from feature map to detect the strongest activation over the whole tree (i.e., over the whole sentence). Since the tree no longer defines a sequential \u201ctime\u201d direction, we refer to our pooling as \u201cmax-over-tree\u201d pooling. In order to capture enough variations, we randomly initialize the set of filters to detect different structure patterns. Each filter\u2019s height is the number of words considered and the width is always equal to the dimensionality d of word representation. Each filter will be represented by only one feature after max-over-tree pooling. After a series of convolution with different filter with different heights, multiple features carry different structural information become the final representation of the input sentence. Then, this sentence representation is passed to a fully connected soft-max layer and outputs a distribution over different labels. Neural networks often suffer from overtraining. Following Kim (2014), we employ random dropout on penultimate layer (Hinton et al., 2012). in order to prevent co-adaptation of hidden units. In our experiments, we set our drop out rate as 0.5 and learning rate as 0.95 by default. Following Kim (2014), training is done through stochastic gradient descent over shuffled mini-batches with the Adadelta update rule (Zeiler, 2012).", "startOffset": 43, "endOffset": 1405}, {"referenceID": 15, "context": "Figure 1 illustrates an example from the Movie Reviews (MR) dataset (Pang and Lee, 2005).", "startOffset": 68, "endOffset": 88}, {"referenceID": 7, "context": "In sequential CNNs, max-over-time pooling (Collobert et al., 20 1; Kim, 2014) operates over the feature map to get the maximum activation \u0109 = max c representing the entire feature ap.", "startOffset": 42, "endOffset": 77}, {"referenceID": 3, "context": "Following Kim (2014), we employ random dropout on penultimate layer (Hinton et al., 2012).", "startOffset": 68, "endOffset": 89}, {"referenceID": 21, "context": "Following Kim (2014), training is done through stochastic gradient descent over shuffled mini-batches with the Adadelta update rule (Zeiler, 2012).", "startOffset": 132, "endOffset": 146}, {"referenceID": 0, "context": "In sequential CNNs, max-over-time pooling (Collobert et al., 20 1; Kim, 2014) operates over the feature map to get the maximum activation \u0109 = max c representing the entire feature ap. Our DTCNNs also pool the maximum activation from feature map to detect the strongest activation over the whole tree (i.e., over the whole sentence). Since the tree no longer defines a sequential \u201ctime\u201d direction, we refer to our pooling as \u201cmax-over-tree\u201d pooling. In order to capture enough variations, we randomly initialize the set of filters to detect different structure patterns. Each filter\u2019s height is the number of words considered and the width is always equal to the dimensionality d of word representation. Each filter will be represented by only one feature after max-over-tree pooling. After a series of convolution with different filter with different heights, multiple features carry different structural information become the final representation of the input sentence. Then, this sentence representation is passed to a fully connected soft-max layer and outputs a distribution over different labels. Neural networks often suffer from overtraining. Following Kim (2014), we employ random dropout on penultimate layer (Hinton et al.", "startOffset": 43, "endOffset": 1172}, {"referenceID": 0, "context": "In sequential CNNs, max-over-time pooling (Collobert et al., 20 1; Kim, 2014) operates over the feature map to get the maximum activation \u0109 = max c representing the entire feature ap. Our DTCNNs also pool the maximum activation from feature map to detect the strongest activation over the whole tree (i.e., over the whole sentence). Since the tree no longer defines a sequential \u201ctime\u201d direction, we refer to our pooling as \u201cmax-over-tree\u201d pooling. In order to capture enough variations, we randomly initialize the set of filters to detect different structure patterns. Each filter\u2019s height is the number of words considered and the width is always equal to the dimensionality d of word representation. Each filter will be represented by only one feature after max-over-tree pooling. After a series of convolution with different filter with different heights, multiple features carry different structural information become the final representation of the input sentence. Then, this sentence representation is passed to a fully connected soft-max layer and outputs a distribution over different labels. Neural networks often suffer from overtraining. Following Kim (2014), we employ random dropout on penultimate layer (Hinton et al., 2012). in order to prevent co-adaptation of hidden units. In our experiments, we set our drop out rate as 0.5 and learning rate as 0.95 by default. Following Kim (2014), training is done through stochastic gradient descent over shuffled mini-batches with the Adadelta update rule (Zeiler, 2012).", "startOffset": 43, "endOffset": 1404}, {"referenceID": 14, "context": "Inspired by higher-order dependency parsing (McDonald and Pereira, 2006; Koo and Collins, 2010), we also incorporate siblings for a given word in various ways.", "startOffset": 44, "endOffset": 95}, {"referenceID": 8, "context": "Inspired by higher-order dependency parsing (McDonald and Pereira, 2006; Koo and Collins, 2010), we also incorporate siblings for a given word in various ways.", "startOffset": 44, "endOffset": 95}, {"referenceID": 15, "context": "We use three benchmark datasets in two categories: sentiment analysis on both Movie Review (MR) (Pang and Lee, 2005) and Stanford Sentiment Treebank", "startOffset": 96, "endOffset": 116}, {"referenceID": 7, "context": "We implement our DTCNN on top of the open source CNN code by Kim (2014).1 Table 2 summarizes our results in the context of other high-performing efforts in the literature.", "startOffset": 61, "endOffset": 72}, {"referenceID": 19, "context": "com/yoonkim/CNN sentence (SST-1) (Socher et al., 2013) datasets, and question classification on TREC (Li and Roth, 2002).", "startOffset": 33, "endOffset": 54}, {"referenceID": 11, "context": ", 2013) datasets, and question classification on TREC (Li and Roth, 2002).", "startOffset": 54, "endOffset": 73}, {"referenceID": 12, "context": "For all datasets, we first obtain the dependency parse tree from Stanford parser (Manning et al., 2014).", "startOffset": 81, "endOffset": 103}, {"referenceID": 22, "context": "4, and ties with Zhu et al. (2015).", "startOffset": 17, "endOffset": 35}, {"referenceID": 4, "context": "5 which is second only to Irsoy and Cardie (2014). We set batch size to 100 for this task.", "startOffset": 26, "endOffset": 50}, {"referenceID": 7, "context": "We implement our DTCNN on top of the open source CNN code by Kim (2014).1 Table 1 summarizes our results in the context of other high-performing efforts in the literature.", "startOffset": 61, "endOffset": 72}, {"referenceID": 19, "context": "com/yoonkim/CNN sentence and Lee, 2005) and Stanford Sentiment Treebank (SST-1) (Socher et al., 2013) datasets, and question classification on TREC (Li and Roth, 2002).", "startOffset": 80, "endOffset": 101}, {"referenceID": 11, "context": ", 2013) datasets, and question classification on TREC (Li and Roth, 2002).", "startOffset": 54, "endOffset": 73}, {"referenceID": 22, "context": "4, and ties with Zhu et al. (2015).", "startOffset": 17, "endOffset": 35}, {"referenceID": 7, "context": "CNNs CNNs-non-static (Kim, 2014) \u2013 baseline 81.", "startOffset": 21, "endOffset": 32}, {"referenceID": 7, "context": "4\u2217 CNNs-multichannel (Kim, 2014) 81.", "startOffset": 21, "endOffset": 32}, {"referenceID": 6, "context": "0\u2217 Deep CNNs (Kalchbrenner et al., 2014) - 48.", "startOffset": 13, "endOffset": 40}, {"referenceID": 18, "context": "Recursive NNs Recursive Autoencoder (Socher et al., 2011) 77.", "startOffset": 36, "endOffset": 57}, {"referenceID": 19, "context": "2 - Recursive Neural Tensor (Socher et al., 2013) - 45.", "startOffset": 28, "endOffset": 49}, {"referenceID": 4, "context": "7 - Deep Recursive NNs (Irsoy and Cardie, 2014) - 49.", "startOffset": 23, "endOffset": 47}, {"referenceID": 22, "context": "Recurrent NNs LSTM on tree (Zhu et al., 2015) 81.", "startOffset": 27, "endOffset": 45}, {"referenceID": 9, "context": "Other deep learning Paragraph-Vec (Le and Mikolov, 2014) - 48.", "startOffset": 34, "endOffset": 56}, {"referenceID": 17, "context": "Hand-coded rules SVMS (Silva et al., 2011) 95.", "startOffset": 22, "endOffset": 42}, {"referenceID": 7, "context": "\u2217Results generated from Kim (2014)\u2019s implementation.", "startOffset": 24, "endOffset": 35}, {"referenceID": 4, "context": "5 which is second only to Irsoy and Cardie (2014). We set batch size to 100 for this task.", "startOffset": 26, "endOffset": 50}, {"referenceID": 17, "context": "ceeds SVMS (Silva et al., 2011) with 60 handcoded rules.", "startOffset": 11, "endOffset": 31}, {"referenceID": 5, "context": "8 but their parser is trained only on the QuestionBank (Judge et al., 2006) while we used the standard Stanford parser trained on both the Penn Treebank and QuestionBank.", "startOffset": 55, "endOffset": 75}, {"referenceID": 16, "context": "Like ours, Silva et al. (2011) is a tree-based system but it uses constituency trees compared to ours dependency trees.", "startOffset": 11, "endOffset": 31}, {"referenceID": 7, "context": "CNNs CNNs-non-static (Kim, 2014) \u2013 baseline 81.", "startOffset": 21, "endOffset": 32}, {"referenceID": 7, "context": "4\u2217 CNNs-multichannel (Kim, 2014) 81.", "startOffset": 21, "endOffset": 32}, {"referenceID": 6, "context": "0\u2217 Deep CNNs (Kalchbrenner et al., 2014) - 48.", "startOffset": 13, "endOffset": 40}, {"referenceID": 18, "context": "Recursive NNs Recursive Autoencoder (Socher et al., 2011) 77.", "startOffset": 36, "endOffset": 57}, {"referenceID": 19, "context": "2 - Recursive Neural Tensor (Socher et al., 2013) - 45.", "startOffset": 28, "endOffset": 49}, {"referenceID": 4, "context": "7 - Deep Recursive NNs (Irsoy and Cardie, 2014) - 49.", "startOffset": 23, "endOffset": 47}, {"referenceID": 22, "context": "Recurrent NNs LSTM on tree (Zhu et al., 2015) 81.", "startOffset": 27, "endOffset": 45}, {"referenceID": 9, "context": "Other deep learning Paragraph-Vec (Le and Mikolov, 2014) - 48.", "startOffset": 34, "endOffset": 56}, {"referenceID": 17, "context": "Hand-coded rules SVMS (Silva et al., 2011) 95.", "startOffset": 22, "endOffset": 42}, {"referenceID": 7, "context": "\u2217Results generated from Kim (2014)\u2019s implementation.", "startOffset": 24, "endOffset": 35}, {"referenceID": 17, "context": "One thing to note here is that our best result even exceeds SVMS (Silva et al., 2011) with 60 handcoded rules.", "startOffset": 65, "endOffset": 85}, {"referenceID": 5, "context": "8 but their parser is trained only on the QuestionBank (Judge et al., 2006) while we used the standard Stanford parser trained on both the Penn", "startOffset": 55, "endOffset": 75}, {"referenceID": 5, "context": "However, the parser works substantially better on the TREC dataset since all questions are in formal written English, and the training set for Stanford parser5 already includes the QuestionBank (Judge et al., 2006) which includes 2,000 TREC sentences.", "startOffset": 194, "endOffset": 214}, {"referenceID": 5, "context": "However, the pars r works substantially better on the TREC dataset since all questions are in formal ritten English, and the training set for Stanford parser5 already includes the QuestionBank (Judge et al., 2006) which includes 2,000 TREC sentences.", "startOffset": 193, "endOffset": 213}, {"referenceID": 5, "context": "However, the parser works substantially better on the TREC dataset sinc all q estions are in formal written English, and the training set for St nford parser5 already includes the QuestionBank (Judge et al., 2006) which includes 2,000 TREC sentences.", "startOffset": 193, "endOffset": 213}], "year": 2017, "abstractText": "In sentence modeling and classification, convolutional neural network approaches have recently achieved state-of-the-art results, but all such efforts process word vectors sequentially and neglect long-distance dependencies. To exploit both deep learning and linguistic structures, we propose a tree-based convolutional neural network model which exploit various long-distance relationships between words. Our model improves the sequential baselines on all three sentiment and question classification tasks, and achieves the highest published accuracy on TREC.", "creator": "TeX"}}}