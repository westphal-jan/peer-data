{"id": "1507.02189", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jul-2015", "title": "Intersecting Faces: Non-negative Matrix Factorization With New Guarantees", "abstract": "Non-negative matrix factorization (NMF) is a natural model of admixture and is widely used in science and engineering. A plethora of algorithms have been developed to tackle NMF, but due to the non-convex nature of the problem, there is little guarantee on how well these methods work. Recently a surge of research have focused on a very restricted class of NMFs, called separable NMF, where provably correct algorithms have been developed to do precisely the same. These are called separate categories. We propose the following general categories for NMF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 8 Jul 2015 15:07:40 GMT  (245kb,D)", "http://arxiv.org/abs/1507.02189v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["rong ge 0001", "james zou"], "accepted": true, "id": "1507.02189"}, "pdf": {"name": "1507.02189.pdf", "metadata": {"source": "CRF", "title": "Intersecting Faces: Non-negative Matrix Factorization With New Guarantees", "authors": ["Rong Ge", "James Zou"], "emails": ["rongge@microsoft.com", "jazo@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "In many settings in science and engineering the observed data are admixtures of multiple latent sources. We would typically want to infer the latent sources as well as the admixture distribution given the observations. Non-negative matrix factorization (NMF) is a natural mathematical framework to model many admixture problems.\nIn NMF we are given an observation matrix M \u2208 Rn\u00d7m, where each row of M corresponds to a data-point in Rm. We assume that there are r latent sources, modeled by the unobserved matrix W \u2208 Rr\u00d7m, where each row of M characterizes one source. Each observed data-point is a linear combination of the r sources and the combination weights are encoded in a matrix A \u2208 Rn\u00d7r. Moreover, in many natural settings, the sources are non-negative and the combinations are additive. The computational problem is then is to factor a given matrix M as M = AW , where all the entries of M,A and W are non-negative. We call r the inner-dimension of the factorization, and the smallest possible r is usually called the nonnegative rank of M . NMF was first purposed by (Lee & Seung, 1999), and has been widely applied in computer vision (Lee & Seung, 2000), document clustering (Xu et al., 2003), hyperspectral unmixing(Nascimento & Dias, 2004; Gomez et al., 2007), computational biology (Devarajan, 2009), etc. We give two concrete examples\nExample 1. In topic modeling, M is the n-by-m word-by-document matrix, where n is the vocabulary size and m is the number of documents. Each column of M corresponds to one document and the entry M(i, j) is the frequency with which word i appears in document j. The topics are the columns of A, and A(i, k) is the probability that topic k uses word i. W is the topic-by-document matrix and captures how much each topic contributes to each document. Since all the entries of M,A and W are frequencies, they are all non-negative. Given M from a corpus of documents, we would like to factor M = AW and recover the relevant topics in these documents. (Note that in this example A is the matrix of \u201csources\u201d and W is the matrix of mixing weights, so it is the transpose of what we just introduced. We use this notation to be consistent with previous works (Arora et al., 2012).)\nExample 2. In many bio-medical applications, we collect samples and for each sample perform multiple measurements (e.g. expression of 104 genes or DNA methylation at 106 positions in the genome; all the values are non-negative). M is the sample-by-measurement matrix, where M(i, j) is the value of the jth measurement in sample\nar X\niv :1\n50 7.\n02 18\n9v 1\n[ cs\n.L G\n] 8\nJ ul\n2 01\n5\ni. Each sample, whether taken from humans or animals, is typically a composition of several cell-types that we do not directly observe. Each row of W corresponds to one cell-type, and W (k, j) is the value of cell-type k in measurement j. The entry A(i, k) is the fraction of sample i that consists of cell-type k. Experiments give us the matrix M , and we would like to factor M = AW to identify the relevant cell-types and their compositions in our samples.\nDespite the simplicity of its formulation, NMF is a challenging problem. First, the NMF problem may not be identifiable, and hence we can not hope to recover the trueA andW . Moreover, even ignoring the identifiabilityVavasis (2009) showed that finding any factorization M = AW with inner-dimension r is an NP -hard problem. Arora et al. (2012) showed under reasonable assumptions we cannot hope to find a factorization in time (mn)o(r), and the best algorithm known is Moitra (2013) that runs in time O(2rmn)O(r\n2). Many heuristic algorithms have been developed for NMF but they do not have guarantees for when they would actually converge to the true factorization (Lee & Seung, 2000; Lin, 2007). More recently, there has been a surge of interest in constructing practical NMF algorithms with strong theoretical guarantees. Most of this activity (e.g. Arora et al. (2012); Bittorf et al. (2012); Kumar et al. (2012); Gillis (2012); Gillis & Vavasis (2014), see more in Gillis (2014)) are based on the notion of separabilityDonoho & Stodden (2003) which is a very strict condition that requires that all the rows of W appear as rows in M . While this might hold in some document corpus, it is unlikely to be true in other engineering and bio-medical applications.\nOur Results In this paper, we develop the notion of subset separability, which is a significantly weaker and more general condition than separability. In topic models, for example, separability states that there is a word that is unique to each topic. Subset separability means that there is a combination of words that is unique to each topic. We show that subset separability arise naturally as a necessary condition when the NMF is identifiable or when we are seeking the minimal volume factorization. We characterize settings when subset-separable NMF can be solved in polynomialtime, and this include the separable setting as a special case. We construct the Face-Intersect algorithm which provably and robustly solves the NMF even in the presence of adversarial noise. We use simulations to explore conditions where our algorithm achieves more accurate inference than current state-of-art algorithms.\nOrganization We first describe the geometric interpretation of NMF (Sec. 2), which leads us to the notion of subsetseparable NMF (Sec. 3). We then develop our Face-Intersect algorithm and analyze its robustness (Sec. 4). Our main result, Theorem 4.2, states that for subset-separable NMF, if the facets are properly filled in a way that depends on the magnitude of the adversarial noise, then Face-Intersect is guaranteed to find a factorization that is close to the true factorization in polynomial time. We discuss the algorithm in more detail in Sections 5 and 6, and analyze a generative model that give rise to properly filled facets in Section 7. Finally we present experiments to explore settings where Face-Intersect outperforms state-of-art NMF algorithms (Sec. 8). Due to space constraints, all the proofs are presented in the appendix. Throughout the paper, we give intuitions behind proofs of the main results."}, {"heading": "2 Geometric intuition", "text": "For a matrix M \u2208 Rn\u00d7m, we use M i \u2208 Rm to denote the i-th row of M , but it is viewed as a column vector. Given a factorization M = AW , without loss of generality we can assume the rows of M,A,W all sum up to 1 (this can always be done, see Arora et al. (2012)). In this way we can view the rows of W as vertices of an unknown simplex, and the rows of M are all in the convex hull of these vertices. The NMF is then equivalent to the following geometric problem:\nNMF, Geometric Interpretation There is an unknown W -simplex whose vertices are the rows of W \u2208 Rm, W 1, ...,W r. We observe n points M1,M2, ...,Mn \u2208 Rm (corresponding to rows of M ) that lie in the W -simplex. The goal is to identify the vertices of the W -simplex.\nWhen clear from context, we also call the W matrix as the simplex, and the goal is to find the vertices of this simplex. There is one setting where it is easy to identify all the vertices.\nDefinition 2.1 (separability). A NMF is separable if all the vertices W j\u2019s appear in the points M i\u2019s that we observe.\nSeparability was introduced in Donoho & Stodden (2003). When the NMF is separable, the problem simplifies as we only need to identify which of the points M j\u2019s are vertices of the simplex. This can be done in time polynomial in n,m and r (Arora et al., 2012). Separability is a highly restrictive condition and it takes advantage of only the 0-dimensional structure (vertices) of the simplex. In this work, we use higher dimensional structures of the simplex to solve the NMF. We use the following standard definition of facets:\nDefinition 2.2 (facet). A facet S \u2282 [r] of the W -simplex is the convex hull of vertices {W j : j \u2208 S}. We call S a filled facet if there is at least one point M i in the interior of S (or if |S| = 1 and there is one point M i that is equal to that vertex; such M i is called an anchor).\nConventions When it\u2019s clear from context, we interchangeably represent a facet S both by the indices of its vertices and by the convex hull of these vertices. A facet also corresponds to a unique linear subspace QS with dimension |S| that is the span of {W j : j \u2208 S}. In the rest of the paper, it\u2019s convenient to use linear algebra to quantify various geometric ideas. We will represent a d-dimensional subspace of Rm using a matrix U \u2208 Rm\u00d7d, the columns of matrix U is an arbitrary orthonormal basis for the subspace (hence the representation is not unique). We use PU = UUT to denote the projection matrix to subspace U , and U\u22a5 \u2208 Rm\u00d7(m\u2212d) to denote an arbitrary representation of the orthogonal subspace. For two subspaces U and V of the same dimension, we define their distance to the the sin of the principle angle between the two subspaces (this is the largest angle between vectors u, v for u \u2208 U and v \u2208 V ). This distance can be computed as the spectral norm \u2016PU\u22a5V \u2016 (and has many equivalent formulations)."}, {"heading": "3 Subset Separability", "text": "NMF is not identifiable up to scalings and permutations of the rows of W . Ignoring such transformations, there can still be multiple non-negative factorizations of the same matrixM . This arise when there are different sets of r vertices in the non-negative orthant that contain all the points M i in its convex hull. For example, suppose M = AW and the A matrix has all positive entries. All the points M i are in the interior of the W -simplex. Then it is possible to perturb the vertices of W while still maintaining all of the M i\u2019s in its convex hull. This give rise to a different factorization M = A\u0302W\u0302 . When the factorization is not unique, we may want find a solution where the W -simplex has minimal volume, in the sense that it is impossible to move a single vertex and shrink the volume while maintaining the validity of the solution.\nIt\u2019s clear that in order for W to be the minimal volume solution to the NMF, there must be some points M i that lie on the boundary of the W -simplex. We show that a necessary condition for W to be volume minimizing is for the filled facets (facets of W with points in its interior) to be subset-separable. Intuitively, this means that each vertex of W is the unique intersection point of a subset of filled facets.\nDefinition 3.1 (subset-separable). A NMFM = AW is subset-separable if there is a set of filled facets S1, ..., Sk \u2282 [r] such that \u2200j \u2208 [r], there is a subset of Sj1 , Sj2 , ..., Sjkj whose intersection is exactly j.\nProposition 3.1. SupposeW is a minimal volume rank r solution of the NMFM = AW . ThenW is subset-separable.\nIt is easy to see that the factorization M = AW is subset-separable is equivalent to the property that for every j1 6= j2 \u2208 [r], there is a row i of A such that Ai,j1 = 0 and Ai,j2 6= 0. The previously proposed separability condition corresponds to the special case where the filled facets S1, ..., Sk correspond to the singleton sets {W 1}, ..., {W r}.\nExample. We illustrate the subset-separable condition in Figure 1. In this figure, the circles correspond to data points M i\u2019s and they are colored according to the facet that they belong. The filled facets are S1 = {1}, S2 = {3}, S3 = {1, 2} and S4 = {2, 3}. The facet {W 1,W 3} is not filled since there are no points in its interior. The singleton facets S1 and S2 are also called anchors. This NMF is subset-separable since W 2 is the unique intersection of S3 and S4, but it is not separable. The figure also illustrates the correspondingAmatrix, where the rows are grouped by facets and the shaded entries denote the support of each row.\nThe geometry of the simplex suggests an intuitive meta-algorithm for solving subset-separable NMFs, which is the basis of our Face-Intersect algorithm.\n1. Identify the filled facets, S1, ..., Sk, r \u2264 k \u2264 n.\n2. Take intersections of the facets to recover all the rows of W (vertices of the simplex).\n3. Use M and W to solve for A."}, {"heading": "4 Robust algorithm for subset-separable NMF", "text": "In order to carry out the meta-algorithm, the key computational challenge is to efficiently and correctly identify the filled facets of the W simplex. Finding filled facets is related to well-studied problems in subspace clustering (Vidal, 2010) and subspace recovery(Hardt & Moitra, 2013). In subspace clustering we are given points in k different subspaces and the goal is to cluster the points according to which subspace it belong to. This problem is in general NP-hard (Elhamifar & Vidal, 2009) and can only be solved under strong assumptions. Subspace recovery tries to find a unique subspace a fraction p of the points. Hardt & Moitra (2013) showed this problem is hard unless p is large compared to the ratio of the dimensions. Techniques and algorithms from subspace clustering and recovery typically make strong assumptions about the independence of subspaces or the generative model of the points, and cannot be directly applied to our problem. Moreover, our filled facets have the useful property that they are on the boundary of the convex hull of the data points, which is not considered in general subspace clustering/discovery methods. We identified a general class of filled facets, called properly filled facets that are computationally efficient to find.\nDefinition 4.1 (properly filled facets). Given a NMF M = AW , a set of facets S1, ..., Sk \u2208 [r] of W is properly filled if it satisfies the following properties:\n1. For any facet |Si| > 1, the rows of A with support equal to Si (i.e. points that lie on this facet) has a |Si| \u2212 1- dimensional convex hull. Moreover, there is at least one row of A that is in the interior of the convex hull.\n2. (General positions property.) For any subspace of dimension 1 < t < r, if it contains more than t rows in M , then the subspace contains at least one Si which is not a singleton facet.\nCondition 1 ensures that each Si has sufficiently many points to be non-degenerate. Condition 2 says that points that are not in the lower dimensional facets S1, ..., Sk are in general positions, so that no random subspace look like a properly filled facet. A set of properly filled facets S1, ..., Sk may contain singleton sets corresponding some of the rows W j if these rows also appear as rows in M . We first state the main results and then state the Face-Intersect algorithm.\nTheorem 4.1. Suppose M = AW is subset separable by S1, ..., Sk and these facets are properly filled, then given M the Face-Intersect algorithm computes A and W in time polynomial in n, m and r (and in particular the factorization is unique).\nIn many applications, we have to deal with noisy NMF M\u0302 = AW + noise where (potentially correlated) noise is added to rows of the data matrix M . Suppose every row is perturbed by a small noise (in `2 norm), we would like the algorithm to be robust to such additive noise. We need a generalization of properly filled facets.\nDefinition 4.2 ((N,H, \u03b3) properly filled facets). Given a NMF M = AW , a set of facets S1, ..., Sk \u2208 [r] of W is (N,H, \u03b3) properly filled if it satisfies the following properties:\n1. In any set |Si| > 1, there is a row i\u2217 in A whose support is equal to Si, and is in the convex hull of other rows of A. There exists a convex combination M i \u2217 = \u2211 i\u2208[n]\\i\u2217 wiM i, such that the matrix \u2211 i\u2208[n]\\i\u2217 wi(M i)(M i)T\nhas rank |Si|, and the smallest nonzero singular value is at least \u03b3. We call this special point M i \u2217\nthe center for this facet.\n2. For any set |Si| > 1, there are at least N rows in A whose support is exactly equal to Si.\n3. For any subspace Q of dimension 1 < t < r, if there are at least N rows of M in an -neighborhood of Q, then there exists a non-singleton set Si with corresponding subspace Qi such that \u2016PQ\u22a5Qi\u2016 \u2264 H .\nIntuitively, if we represent the center point as a convex combination of other points, the only points that have a nonzero contribution must be on the same facet as the center. Condition 1 then ensures there is a \u201cnice\u201d convex combination that allows us to robustly recover the subspace corresponding to the facet even in presence of noise. Condition 2 shows every properly filled facets contain many points, which is why they are different from other subspaces and are the facets of the true solution. Condition 3 is a generalization of the general position propery, which essentially says \u201cevery subspace that contains many points must be close to a properly filled facet\u201d. In Section 7 we show that under a natural generative model, the NMF has (N,H, \u03b3)-properly filled facets with high probability.\nProperly filled facets is a property of how the points M i are distributed on the facets of W . The geometry of the W -simplex itself also affects the accuracy of our Face-Intersect algorithm.\nDefinition 4.3. A matrix W \u2208 Rr\u00d7m(r \u2264 m) is \u03b1-robust if its rows have norm bounded by 1, and its r-th singular value is at least \u03b1.\nUnder these assumptions we prove that Face-Intersect robustly learns the unknown simplex W .\nTheorem 4.2. SupposeM = AW is subset separable by S1, ..., Sk and these facets are (N,H, \u03b3) properly filled, and the matrix W is \u03b1-robust. Then given M\u0302 whose rows are within `2 distance to M , with < o(\u03b14\u03b3/Hr3), Algorithm Face-Intersect finds W\u0302 such that there exists a permutation \u03c0 and for all i \u2016W\u0302i \u2212 W\u03c0(i)\u2016 \u2264 O(Hr2 /\u03b12\u03b3). The running time is polynomial in n,m and r.\nAlgorithm 1 Face-Intersect Run Algorithm 3 to find subspaces that correspond to properly filled facets S1, S2, ..., Sk where |Si| \u2265 2. Run Algorithm 5 to find the intersection vertices P . Run Algorithm 5 (similar to Algorithm 4 in Arora et al. (2013)) to find the singleton points (anchors). Given M\u0302 , W\u0302 , compute A\u0302.\nA vertex j \u2208 [r] is an intersection vertex if there exists a subset of properly filled facets {Sjk : |Sjk | \u2265 2} such that j = \u2229kSjk . Since the first module of Face-Intersect, Algorithm 3, only finds non-singleton facets, the intersection vertices are all the vertices that we could find using these facets. The last module of Face-Intersect finds all the remaining vertices of the simplex.\nOur approach The main idea of our algorithm is to first find the subspaces corresponding properly filled facets, then take the intersections of these facets to find the intersection vertices. Finally we adapt the algorithm from Arora et al. (2013) to find the remaining vertices that correspond to singleton sets. \u2022 Finding facets For each row of M , we try to represent it as the convex combination of other rows of M . We use\nan iterative algorithm to make sure the span of points used in this convex combination is exactly the subspace corresponding to the facet. \u2022 Removing false positives The previous step will generate subspaces that correspond to properly filled facets, but it might also generate false positives (subspaces that do not correspond to any properly filled facets). Condition 3 in Definition 4.2 allows us to filter out these false positives as these subspaces will not contain enough nearby points.\n\u2022 Finding intersection vertices We design an algorithm that systematically tries to take the intersections of subspaces in order to find the intersection vertices. This relies on the subset-separable property and robustness properties of the simplex. This step computes at most O(nr) subspace intersection operations. \u2022 Finding remaining vertices The remaining vertices correspond to the singleton sets. This is similar to the separable case and we use an algorithm from Arora et al. (2013)."}, {"heading": "5 Finding properly filled facets", "text": "In this section we show how to find properly filled facets Si with |Si| \u2265 2. The singleton facets (anchors) are not considered in this section, since they will be found through a separate algorithm. We first show how to find a properly filled facet if we know its center (Condition 1 in Definition 4.2). Then to find all the properly filled facets we enumerate points to be the center and remove false positives.\nFinding one properly filled facet Given the center point, if there is no noise then when we represent this point as convex combinations of other points, all the points with positive weight will be on the same facet. Intuitively the span of these points should be equal to the subspace corresponding to the facet. However there are two key challenges here: first we need to show that when there is noise, points with large weights in the convex combination are close to the true facet; second, it is possible that points with large weights only span a lower dimensional subspace of the facet. Condition 1 in Definition 4.2 guarantees that there exists a nice convex combination that spans the entire subspace (and robustly so because the smallest singular value is large compared to noise). In Algorithm 2, we iteratively improve our convex combination and eventually converge to this nice combination.\nAlgorithm 2 Finding a properly filled facet input points v\u03021, v\u03022, ..., v\u0302n, and center point v\u03020 (Condition 1 in Definition 4.2). output the proper facet containing v\u03020.\n1: Maintain a subspace Q\u0302 (initially empty) 2: Iteratively solve the following optimization program:\nmax tr(PQ\u0302\u22a5 n\u2211 i=1 wiv\u0302 i(v\u0302i)TPQ\u0302\u22a5)\n\u2200i \u2208 [n] wi \u2265 0 n\u2211 i=1 wi = 1\n\u2016v\u03020 \u2212 n\u2211 i=1 wiv\u0302 i\u2016 \u2264 2\ndiag(Q\u0302T (\nn\u2211 i=1 wiv\u0302 i(v\u0302i)T\n) Q\u0302) \u2265 \u03b3/2.\n3: Let Q\u0302 be the top singular space of (\u2211n\ni=1 wiv\u0302 i(v\u0302i)T\n) for singular values larger than \u03b3/2d.\n4: Repeat until the dimension of Q\u0302 does not increase.\nTheorem 5.1. Suppose \u2016v\u0302i \u2212 vi\u2016 \u2264 , v0 is the center point of a properly filled facet S \u2282 [r] with |S| = d, and the unknown simplex W is \u03b1-robust, when d \u221a r /\u03b1\u03b3 1 Algorithm 2 stops within d iterations, and the subspace Q\u0302 is within distance O( \u221a r /\u03b1\u03b3) to the true subspace QS .\nThe intuition of Algorithm 2 is to maintain a convex combination for the center point. We show for any convex combination, the top singular space associated with the combination, Q\u0302, is always close to a subspace of the true\nspace QS . The algorithm then tries to explore other directions by maximizing the projection that is outside the current subspace Q\u0302 (the objective function of the convex optimization), while maintaining that the current subspace have large singular values (the last constraint). In the proof we show since there is a nice solution, the algorithm will always be able to make progress until the final solution is a nice convex combination.\nFinding all subsets Algorithm 2 can find one properly filled facet, if we have its center point (Condition 1 in Definition 4.2). In order to find all the properly filled facets, we enumerate through rows of M and prune false positives using Condition 3 in Definition 4.2\nAlgorithm 3 Finding all proper facets input M\u0302 whose factorization is subset-separable with (N,H, \u03b3)-properly filled facets.\n1: for i = 1 to n do 2: Let v\u03020 = M\u0302 i and v\u03021, ..., v\u0302n\u22121 be the rest of vertices. 3: Run Algorithm 2 to get a subspace Q. 4: If dim(Q) < r, and there are at least N points that are within distance O( \u221a r /\u03b1\u03b3) add it to the collection of subspaces. 5: end for 6: LetQ be a subspace in the collection, removeQ if there is a subspaceQ\u2032 with dim(Q\u2032) < dim(Q) and \u2016PQ\u22a5Q\u2032\u2016 \u2264 O(H \u221a r /\u03b1\u03b3) 7: Merge all subspaces that are within distance O(H \u221a r /\u03b1\u03b3) to each other.\nTheorem 5.2. IfH \u221a r /\u03b1\u03b3 = o(\u03b1), then the output of Algorithm 3 contains only subspaces that are S = O(H \u221a r /\u03b1\u03b3)close to the properly filled facets, and for every properly filled facet there is a subspace in the output that is S close."}, {"heading": "6 Finding intersections", "text": "Given an subset-separable NMF with (N,H, \u03b3)-properly filled facets, let Qi denote the subspace associated with a set Si of vertices: Qi = span(WSi). For all properly filled facets with at least two vertices, Algorithm 3 returns noisy versions of the subspaces Q\u0302i that are S close to the true subspaces. Without loss of generality, assume the first h facets are non-singletons. Our goal is to find all the intersection vertices {W i : i \u2208 P}. Recall that intersection vertices are the unique intersections of subsets of S1, ..., Sh. We can view this as a set intersection problem:\nSet Intersections We are given sets S1, S2, ..., Sh \u2282 [r]. There is an unknown set P \u2282 [r] such that \u2200i \u2208 P there exists {Sik} and i = \u2229kSik . Our goal is to find the set P .\nThis problem is simple if we know the subsets of W j in each facet. However, since what we really have access to are subspaces, it is impossible to identify the vertices unless we have a subspace of dimension 1. On the other hand, we can perform intersection and linear-span for the subspaces, which correspond to intersection and union for the sets. We also know the size of a set by looking at the dimension of the subspace. The main challenge here is that we cannot afford to enumerate all the possible combinations of the sets, and also there are vertices that are not intersection vertices and they may or may not appear in the sets we have. The idea of the algorithm is to keep vertices that we have already found in R, and try to avoid finding the same vertices by making sure S is never a subset of R. We show after every inner-loop one of the two cases can happen: in the first case we find an element in P ; in the second case S is a set that satisfies (S\\R) \u2229 P = \u2205, so by adding S to R we remove some of the vertices that are not in P . Since the size of R increases by at least 1 in every iteration until R = [r], the algorithm always ends in r iterations and finds all the vertices in P . In practice, we implement all the set operations in 4 using the analogous subspace operations (see Algorithm 6 in Appendix). We prove the following :\nTheorem 6.1. When W is \u03b1-robust and S < o(\u03b13/r2.5), Algorithm 6 finds all the intersection vertices of W , with error at most v = 4r1.5 S/\u03b1.\nAlgorithm 4 Finding Intersection input k sets S1, ..., Sh. output A set P that has all the intersection vertices.\nInitialize P = \u2205, R = \u2205. for i = 1 to r do\nLet S = [r] for j = 1 to h do\nif |S \u2229 Sj | < |S| and S \u2229 Sj 6\u2286 R then S = S \u2229 Sj\nend if end for R = R \u222a S Add S to P if |S| = 1.\nend for\nAlgorithm 5 Finding remaining vertices input matrix M\u0302 , intersection vertices W\u0302 1, ..., W\u0302 |P |. output remaining vertices W\u0302 |P |+1, ..., W\u0302 r.\nfor i = |P |+ 1 TO r do Let Q = span{W\u0302 1, ..., W\u0302 i\u22121}. Pick the point M\u0302 j with largest \u2016PQ\u22a5M\u0302 j\u2016, let W\u0302 i = M\u0302 j . end for\nFinding the remaining vertices The remaining vertices correspond to singleton sets in subset-separable assumption. They appear in rows of M . The situation is very similar to the separable NMF and we use an algorithm from Arora et al. (2013) to find the remaining vertices. For completeness we describe the algorithm here. By Lemma 4.5 in Arora et al. (2013) we directly get the following theorem:\nTheorem 6.2. If vertices already found have accuracy v such that v \u2264 \u03b1/20r, Algorithm 5 outputs the remaining vertices with accuracy O( /\u03b12) < v .\nRunning time. Face-Intersect (Algorithm 1) has 3 parts: find facets (Algorithm 3), find intersections (Algorithm 4) and find remaining anchors (Algorithm 5). We discuss the runtime of each part. We first do dimension reduction to map the n points to an r-dimensional subspace to improve the running time of later steps. The dimension reduction takes O(nmr) time, where n,m are the number of rows and columns of M , respectively, and r is the rank of the factorization. Algorithm 3\u2019s runtime is O(nd \u00b7OPT), where d is the max dimension of properly filled facets (typically d < r m). OPT is the time to solve the convex optimization problem in Algorithm 2. OPT is essentially equivalent to solving an LP with n nonnegative variables and r + d constraints. Algorithm 4\u2019s runtime is O(kr4) where k is the number of properly filled facets; typically k n. Algorithm 5\u2019s runtime is O(nr3). The overall runtime of FaceIntersect is O(mnr + nd \u00b7 OPT + kr4 + nr3). Calling the OPT routine is the most expensive part of the algorithm. Empirically, we find that the algorithm converges after \u223c k nd calls to OPT."}, {"heading": "7 Generative model of NMF naturally creates properly filled facets", "text": "To better understand the generality of our approach, we analyzed a simple generative model of subset-separable NMFs and showed that properly filled facets naturally arise with high probability.\nGenerative Model Given a simplex W that is \u03b1-robust and a subset of facets S1, S2, ..., Sk that is subset separable. Let pi be the probability associated with facet i, and let pmin = mini\u2264k pi and d = maxi\u2208[k] |Si|. For convenience,\ndenote S0 = [r] and p0 = 1 \u2212 \u2211k i=1 pi. To generate a sample, first sample facet Si with probability pi, and then uniformly randomly sample a point within the convex hull of the points {W j : j \u2208 Si}. Here we think of d as a small constant or O((log n)/ log log n) (in general d can be much smaller than r). For example, separability assumption implies d = 1, and it is already nontrivial when d = 2.\nTheorem 7.1. Given n = \u2126(max{(4d)d log(d/\u03b7), kr2 log(d/pmin\u03b7)}/pmin) samples from the model, with high probability the facets S1, ..., Sk are (pminn/2, 200r1.5/pmin\u03b1, \u03b12/16d) properly filled.\nThe proof relies on the following two lemmas. The first lemma shows that once we have enough points in a simplex, then there is a center point with high probability.\nLemma 7.2. Given n = \u2126((4d)d log d/\u03b7) uniform points v1, v2, ..., vn in a standard d-dimensional simplex (with vertices e1, e2, ..., ed), with probability 1\u2212\u03b7 there exists a point vi such that vi = \u2211 j 6=i wjv j (wj \u2265 0, \u2211 j 6=i wj = 1),\nand \u03c3min( \u2211 j 6=i wj(v j)(vj)T ) \u2265 1/16d.\nThe next lemma shows unless a subspace contains a properly filled facet, it cannot contain too many points in its neighborhood.\nLemma 7.3. Given n = \u2126(d2 log(d/pmin\u03b7)/pmin) uniform points v1, v2, ..., vn in a standard d-dimensional simplex (with vertices e1, e2, ..., ed), with probability 1\u2212 \u03b7 for all matrices A whose largest column norm is equal to 1, there are at most pminn/4 points with \u2016Avi\u2016 \u2264 pmin/200d."}, {"heading": "8 Experiments", "text": "While our algorithm has strong theoretical guarantees, we additionally performed proof-of-concept experiments to show that when the noise is relatively small, our algorithm can outperform the state-of-art NMF algorithms. We\nsimulated data according to the generative NMF model described in Section 7. We first randomly select r non-negative vectors in Rm as rows of the W matrix. We grouped the vertices W j into r groups, S1, ..., Sr of three elements each, such that each vertex is the unique intersection of two groups. Each Si then corresponds to a 2-dim facet. To generate the A matrix, for each Si, we randomly sampled n1 rows of A with support Si, where each entry is an i.i.d. from Unif(0, 1). An additional n2 rows of A were sampled with full support. These correspond to points in the interior of the simplex. We tested a range of settings with m between 5 to 100, r between 3 to 10, and n1 and n2 between 100 and 500. We generated the true data as M = AW and added i.i.d. Gaussian noise to each entry of M to generate the observed data M\u0303 .\nThere are many algorithms for solving NMF, most of them are either iterative algorithms that have no guarantees, or algorithms that work only under separability condition. We choose two typical algorithms: the Anchor-Words algorithm(Arora et al., 2013) for separable NMF, and Projected Gradient (Lin, 2007) for iterative algorithms. For each simulated NMF, we evaluated the output factors A\u0302, W\u0302 of these algorithms on three criteria: accuracy of the reconstructed anchors to the true anchors, ||W \u2212 W\u0302 ||2; accuracy of the reconstructed data matrix to the observed data, ||M\u0303 \u2212 A\u0302W\u0302 ||2; accuracy of the reconstructed data to the true data, ||M \u2212 A\u0302W\u0302 ||2. In Figure 2, we show the results for the three methods under the setting n1 = 100, n2 = 100,m = 10, r = 5. We grouped the results by the noise level of the experiment, which is defined to be the ratio of the average magnitude of the noise vectors to the average magnitude of the data points in Rm. Face-Intersect is substantially more accurate in reconstructing the W matrix compared to Anchor-Words and Projected Gradient. In terms of reconstructing the M\u0303 and M matrices, Face-Intersect slightly outperforms Anchor-Words (p < 0.05 t-test), and they both were substantially more accurate than Projected Gradient. As noise level increased, the accuracy of Face-Intersect and Anchor-Words degrades and at noise around 12.5%, the accuracy of the three methods converged. In many applications, we are more interested in accurate reconstruction of the latent W than of M . For example, in bio-medical applications, each row of M is a sample and each column is the measurement of that sample at a particular bio-marker. Each sample is typically a mixture of r cell-types, and each cell-type corresponds to a row of W . The A matrix gives the mixture weights of the cell-types into the samples. Given measurement on a set of samples, M , an important problem is to infer the values of the latent cell-types at each bio-marker, W (Zou et al., 2014). To create a more realistic simulation of this setting, we used DNA methylation values measures at 100 markers in 5 cell-types (Monocytes, B-cells, T-cells, NK-cells and Granulocytes) as the true W matrix (Zou et al., 2014). From these 5 anchors we generated 600 samples\u2013which is a typical size of such datasets\u2013using the same procedure as above. Both Face-Intersect and Anchor-Words substantially outperformed Projected Gradient across all three reconstruction criteria. In terms of reconstructing the biomarker matrixW , Face-Intersect was significantly more accurate than Anchor-Words. For reconstructing the data matricesM and M\u0303 , Face-Intersect was statistically more accurate than Anchor-Words when the noise is less than 8% (p < 0.05), though the magnitude of the difference is small.\nDiscussion We have presented the notion of subset separability, which substantially generalizes separable NMFs and is a necessary condition for the factorization to be unique or to have minimal volume. This naturally led us to develop the Face-Intersect algorithm, and we showed that when the NMF is subset separable and have properly filled facets, this algorithm provably recovers the true factorization. Moreover, it is robust to small adversarial noise. We show that the requirements for Face-Intersect to work are satisfied by simple generative models of NMFs. The original theoretical analysis of separable NMF led to a burst of research activity. Several highly efficient NMF algorithms were inspired by the theoretical ideas. We are hopeful that the idea of subset-separability will similarly lead to practical and theoretically sound algorithms for a much larger class of NMFs. Our Face-Intersect algorithm and its analysis is a first proof-ofconcept that this is a promising direction. In exploratory experiments, we showed that under some settings where the relative noise is low, the Face-Intersect algorithm can outperform state-of-art NMF solvers. An important agenda of research will be to develop more robust and scalable algorithms motivated by our subset-separability analysis."}, {"heading": "A Subset Separability and minimal volume", "text": "In this section we prove Proposition 3.1 subset separability condition is necessary for a minimal volume solution.\nProof. Suppose M = AW is a rank-r nonnegative matrix factorization with minimal volume. If this decomposition does not satisfy the subset-separable condition, then there exists i 6= j \u2208 R such that for every row At, the two entries At,i, At,j are either all zero or all nonzero. That is, the columns Ai and Aj have the same support. Consider a new factorization A\u2032W \u2032, where the columns of A\u2032 are the same as columns of A except for columns i, j, and rows of W \u2032 are the same as rows of W \u2032 except for row i. Let A\u2032i = 1 1\u2212 Ai, A \u2032 j = Aj \u2212 1\u2212 Ai, and (W\n\u2032)i = (1\u2212 )W i + W j , it is easy to verify that A\u2032W \u2032 = AW = M , and W \u2032 is still nonnegative for \u2208 [0, 1].\nSince the support of Ai and Aj are the same, there exists a positive such that A\u2032j is still a nonnegative vector. In that case A\u2032W \u2032 is a valid nonnegative matrix factorization where only one row of W \u2032 is different from W . By construction it is clear that the volume of W \u2032 is equal to (1 \u2212 ) times the volume of W , so this contradicts with the assumption that M = AW is a factorization with minimal volume."}, {"heading": "B Detailed analysis for finding properly filled facets", "text": "In this section we analyze Algorithms 2 and 3.\nB.1 Finding one properly filled facet We first prove Theorem 5.1. For this algorithm, it is more natural to use the following robustness condition, which is a corollary of \u03b1-robustness.\nLemma B.1. Suppose the vertices of the unknown simplex are rows of W \u2208 Rr\u00d7n, and W is \u03b1-robust. For any face S of W with corresponding subspace Q, there exists a unit vector h \u22a5 Q, let v be any vector in the simplex and v\u22a5 be its component that is orthogonal to Q, then |h\u00b7v \u22a5|\n\u2016v\u22a5\u2016 \u2265 \u03b1\u221a r .\nProof. Suppose Q has dimension d (we know d < r). Let B be the projection of W to the orthogonal subspace of Q, and remove the 0-columns in B. The matrix B is a n \u00d7 (r \u2212 d) matrix whose smallest singular value is at least \u03b1 (the smallest singular value in a projection is at least the smallest singular value of the matrix). We construct h as h = B\n\u2020~1 \u2016B\u2020~1\u2016 . By the property of B we know h \u00b7Bi = 1 \u2016B\u2020~1\u2016 = \u03b1/\n\u221a r.\nFor any vector v in the simplex, its orthogonal component v\u22a5 is equal to PQ\u22a5( \u2211r i=1 wiW i), which is a nonnega-\ntive combination of columns in B. Therefore |h\u00b7v \u22a5| \u2016v\u22a5\u2016 = \u2211 i wihi\u00b7Bi \u2016 \u2211 i wiBi\u2016 \u2265 maxi hi\u00b7Bi\u2016Bi\u2016 = \u03b1/ \u221a r (here we used the fact that h \u00b7Bi are all positive).\nAs we explained, there are two challenges in proving Theorem 5.1: 1). the observations are noisy. We would like to show even with the noisy v\u0302\u2019s, the subspace Q\u0302 is always close to a subspace of the true space Q; 2). the convex combination may not find the entire space Q, for which we show the dimension of Q\u0302 will increase until it is equal to the dimension of Q. Throughout this section we will use d to denote the dimension of true space Q.\nWe first show that in every step of the algorithm all the vectors in Q\u0302 are close to the subspace Q. We start by proving a general perturbation lemma for singular subspaces:\nLemma B.2. Let F\u0302 = F +E where both F\u0302 and F are positive semidefinite, F is a rank d matrix with column spanQ, andU is the top t (t \u2264 d) singular space of F\u0302 with the t-th singular value \u03c3t(F\u0302 ) > \u2016E\u2016, then \u2016PQ\u22a5U\u2016 \u2264 \u2016E\u2016/\u03c3t(F\u0302 ).\nProof. Let UDUT be the truncated top t SVD of F\u0302 , and U\u0302D\u0302U\u0302T be the full SVD. We know\n\u2016PQ\u22a5U\u0302D\u0302\u2016 = \u2016PQ\u22a5 F\u0302\u2016 = \u2016PQ\u22a5E\u2016 \u2264 \u2016E\u2016,\nwhere the first equality is because U\u0302 is an orthonormal matrix, and the second equality is because the column span of F is inside Q.\nOn the other hand, PQ\u22a5UD is a submatrix of PQ\u22a5U\u0302D\u0302, so we know \u2016PQ\u22a5UD\u2016 \u2264 \u2016PQ\u22a5U\u0302D\u0302\u2016 \u2264 \u2016E\u2016. Since all the entries in D are at least \u03c3t(F\u0302 ), this implies \u2016PQ\u22a5U\u2016 \u2264 \u2016E\u2016/\u03c3t(F\u0302 ).\nIn the later proofs we usually think of F\u0302 as (\u2211n\ni=1 wiv\u0302 i(v\u0302i)T\n) , and F as (\u2211n i=1 wiPQv i(vi)TPQ ) . The next\nlemma shows that for any feasible solution of the optimization program (even just considering the first three constraints), the matrix F\u0302 is close to F :\nLemma B.3. For any feasible solution that satisfies the first three constraint, let F\u0302 = (\u2211n\ni=1 wiv\u0302 i(v\u0302i)T ) and F =(\u2211n\ni=1 wiPQviv T i PQ\n) , we have \u2016E\u2016 = \u2016F\u0302 \u2212 F\u2016 \u2264 O( \u221a r /\u03b1). In fact, even the nuclear norm1 \u2016F\u0302 \u2212 F\u2016\u2217 \u2264\nO( \u221a r /\u03b1).\nProof. Let F\u0303 = (\u2211n\ni=1 wiv i(vi)T ) , we show F\u0303 is close to both F and F\u0302 . Let \u03b4i = v\u0302i \u2212 vi. By assumption we\nknow \u2016\u03b4i\u2016 \u2264 . Also, by assumption \u2016vi\u2016 \u2264 1 (normalization) so \u2016F\u0302 \u2212 F\u0303\u2016 \u2264 \u2211n i=1 wi\u2016\u03b4i(vi)T + vi\u03b4Ti + \u03b4i\u03b4Ti \u2016 \u2264\n(2 + 2) \u2211n i=1 wi = O( ).\nOn the other hand, by the third constraint we know \u2016v\u03020\u2212 \u2211n i=1 wiv\u0302 i\u2016 \u2264 2 , which implies \u2016v0\u2212 \u2211n i=1 wiv\ni\u2016 \u2264 4 (because \u2016vi\u2212 v\u0302i\u2016 \u2264 and wi\u2019s form a probability distribution). Using the robustness condition, let vi\u22a5 = vi\u2212PQvi, then\n4 > \u2016 \u2211 i wiv i \u2212 v0\u2016 \u2265 \u2211 i wihI \u00b7 vi\u22a5 \u2265 4 n\u2211 i=1 wi\u2016vi\u22a5\u2016\nTherefore we know \u2016F\u0303 \u2212 F\u2016 \u2264 \u2211n i=1 wi\u2016vi\u22a5(vi)TPQ + PQvi(vi\u22a5)T + (vi\u22a5)(vi\u22a5)T \u2016 \u2264 O( \u221a r /\u03b1) (note that \u2016vi\u22a5\u2016 \u2264 1 by normalization). The nuclear norm bound follows from exactly the same proof.\nThe previous two lemmas guarantee that at any time of the algorithm, the subspace Q\u0302 is always close to a subspace of Q. In the next lemma we show that the algorithm makes progress\nLemma B.4. If dim(Q\u0302) = t < d, then in the next iteration the dimension of Q\u0302 increases by at least 1. Proof. Since v0 is a center of the facet, we know there exists a \u201cnice\u201d solution w\u2217 such that v0 = \u2211n i=1 w \u2217 i v i and\u2211n\ni=1 w \u2217 i (v i)(vi)T has d-th singular value \u03b3. We first show that this guaranteed good solution w\u2217 is always a feasible solution. Clearly it satisfies the first three constraints (by triangle inequality). For the last constraint, let F \u2217 be the F -matrix constructed be w\u2217 and F\u0302 \u2217 be the corresponding F\u0302 matrix. By assumption we know F\u0302 \u2217 has \u03c3d(F\u0302 \u2217) \u2265 \u03b3, so in particular for any direction u in subspace Q, uTF \u2217u \u2265 \u03b3. Since by previous two lemmas we have \u2016PQ\u22a5Q\u0302\u2016 \u2264 O(d \u221a r /\u03b1\u03b3), in particular every column of Q\u0302 is within O(d \u221a r /\u03b1\u03b3) with its projection in Q, we know diag(Q\u0302TF \u2217Q\u0302) \u2265 34\u03b3 (when \u221a r /\u03b1\u03b3 is smaller than some universal constant). Now by Lemma B.3 F \u2217 and F\u0302 \u2217 are close (in spectral norm) we have diag(Q\u0302T F\u0302 \u2217Q\u0302) \u2265 \u03b3/2. Since the solution w\u2217 is feasible, the optimal solution must have objective value no less than the objective value of w\u2217. By the nuclear norm bound, for any subspace Q\u0302 we know\ntr(PQ\u0302\u22a5FPQ\u0302\u22a5)\u2212 tr(PQ\u0302\u22a5FPQ\u0302\u22a5) = tr(PQ\u0302\u22a5(F \u2212 F\u0302 )PQ\u0302\u22a5) \u2264 \u2016PQ\u0302\u22a5(F \u2212 F\u0302 )PQ\u0302\u22a5\u2016\u2217 \u2264 \u2016F \u2212 F\u0302\u2016\u2217 \u2264 O( \u221a r /\u03b1),\nwhere we used the fact that the trace of a matrix is always bounded by its nuclear norm, and nuclear norm of a projection is always smaller than nuclear norm of the original matrix 2.\nOn the other hand tr(PQ\u0302\u22a5F \u2217PQ\u0302\u22a5) \u2265 tr(F \u2217) \u2212 \u2211t i=1 \u03c3i(F \u2217) \u2265 \u2211d i=t+1 \u03c3i(F\n\u2217) \u2265 \u03b3(d \u2212 t). So the optimal objective value must be at least \u03b3(d\u2212 t)\u2212O( \u221a r /\u03b1).\nLet w be the optimal solution and F , F\u0302 be the corresponding matrices, by the same argument we know\ntr(P\u22a5 Q\u0302 FP\u22a5 Q\u0302 ) \u2265 tr(P\u22a5 Q\u0302 F\u0302P\u22a5 Q\u0302 )\u2212O(\n\u221a r /\u03b1) \u2265 \u03b3(d\u2212 t)\u2212O( \u221a r /\u03b1).\n1Nuclear norm \u2016M\u2016\u2217 is equal to the sum of singular values of M , it is also the dual norm of spectral norm in the sense that \u2016M\u2016\u2217 = max\u2016A\u2016\u22641\u3008A,M\u3009.\n2This follows from the fact that \u2016A\u2016\u2217 = max\u2016B\u2016\u22641\u3008A,B\u3009 and spectral norms do not increase after projection.\nHowever, F is a matrix of rank at most d, therefore \u2016P\u22a5 Q\u0302 FP\u22a5 Q\u0302 \u2016 \u2265 \u03b3/d\u2212O(\n\u221a r /\u03b1d). For the F\u0302 matrix we also have\n\u2016P\u22a5 Q\u0302 F\u0302P\u22a5 Q\u0302 \u2016 \u2265 \u03b3/2d because \u2016F\u0302 \u2212 F\u2016 is small.\nNow for the matrix F\u0302 , there are t + 1 orthogonal directions (t from Q\u0302, and at least one orthogonal to Q\u0302) with singular value at least \u03b3/2d, hence \u03c3t+1(F\u0302 ) \u2265 \u03b3/2d. As a result in the next step the dimension of Q\u0302 increases by at least 1.\nNow we are ready to prove Theorem 5.1.\nProof. (of Theorem 5.1). By Lemma B.4 and Lemma B.3, we know when the algorithm ends we must have dim(Q\u0302) \u2265 dim(Q).\nNow by the last constraint, we know \u03c3r( \u2211n i=1 wiv\u0302\ni(v\u0302i)T ) \u2265 \u03b3/2. Combined with Lemma B.3 and Lemma B.2 this implies the final subspace is within distance O( \u221a r /\u03b1\u03b3).\nB.2 Finding all properly filled facets Theorem 5.2 follows immediately from Theorem 5.1, for completeness we provide the proof here.\nProof. (of Theorem 5.2) By Theorem 5.1, and by Condition 2 in Definition 4.2, when we run Algorithm 2 on a correct center point, the resulting subspace will always be added to the collection. Therefore at the end of the loop for each facet Si with at least two vertices, and its corresponding subspace Qi, there must be a Q\u0302i in the collection that is O( \u221a r /\u03b1\u03b3)-close.\nOn the other hand, by Condition 3 in Definition 4.2 we know every subspace Q\u0302 that is in the collection must satisfy \u2016PQ\u0302\u22a5Qi\u2016 \u2264 O(H \u221a r /\u03b1\u03b3) for some true subspace Qi. If Q\u0302 has dimension larger than Qi, then \u2016PQ\u0302\u22a5Q\u0302i\u2016 \u2264 \u2016PQ\u0302\u22a5Qi\u2016 + \u2016PQ\u22a5Q\u0302i\u2016 \u2264 O(H \u221a r /\u03b1\u03b3)3. Therefore all the false positives with higher dimension are removed. The remaining subspaces must be O(H \u221a r /\u03b1\u03b3)-close to one of the true subspaces.\nBy the \u03b1-robustness condition, two subspaces corresponding to different facets must have distance at least \u03b1, so when H \u221a r /\u03b1\u03b3 \u2264 o(\u03b1) the subspaces Q\u0302 close to a true subset cannot be removed. Also, in the last step it is easy to identify the subspaces Q\u0302 that are close to one true space Qi, any one of those will be S-close to the true subsets."}, {"heading": "C Detailed Analysis for finding intersections", "text": "In this section we first prove Theorem 6.1, then we discuss how to apply Algorithm 5 from Arora et al. (2013) to find the remaining vertices.\nThe main idea of the implementation is that the subspace Z will always be close to the span of {W i : i \u2208 S} where S = \u2229j\u2208USj . The subspace \u0393 will correspond to span of {W i : i \u2208 R} where R is the set of points that we have already found. If \u2016P\u0393\u22a5Z\u2016 is large then it means S is not a subset of R.\nFor this step we also need a particular corollary of the \u03b1-robustness condition.\nLemma C.1. Suppose the vertices of the unknown simplex are rows of W \u2208 Rr\u00d7m and W is \u03b1-robust. Let Q1, Q2, ..., Qt be a set of faces that has intersection S \u2282 [r], andQ\u22a5i be an arbitrary basis for the orthogonal subspace of Qi. The matrix \u03a3 = [Q\u22a51 , Q \u22a5 2 , ..., Q \u22a5 t ] T has a null-space equal to span{W i : i \u2208 S} and \u03c3n\u2212|S|(\u03a3) \u2265 \u03b1/ \u221a r.\nProof. Clearly all the vectors {W i : i \u2208 S} are in the null-space of \u03a3 as W i \u2208 Qj for all j \u2208 [t]. For vectors that are orthogonal to the span of columns of W , they have projection 1 in all of PQ\u22a5j \u2019s, and they do not influence the projections within the row span of W . We only need to prove that within the row span of W , for all the directions orthogonal to {W i : i \u2208 S} the matrix still has large singular values.\nLet Sj be the set of vertices that Qj contains, we define S\u2032j as follows: S \u2032 1 = [r]\\S1, for all j > 1 S\u2032j = [r]\\ ( (\u222aj\u2032<jS\u2032j\u2032) \u222a Sj ) . Since S is the intersection of the verticies, we know \u222aS\u2032j = [r]\\S. Also by construction we know the S\u2032j\u2019s are disjoint. For each S \u2032 j , let Q \u2032 j be the span of rows of W with indices in [r]\\S\u2032j . Since [r]\\S\u2032j is a superset of Sj , we know Qj is a subspace of Q\u2032j and hence P(Q\u2032j)\u22a5 PQ\u22a5j . For each j construct B j to be the matrix\n3This uses the variational characterization of PU\u22a5V = maxu\u2208U,v\u2208V sin \u03b8(u, v) where \u03b8(u, v) is the angle between u, v.\nAlgorithm 6 Finding Intersection input k subspaces Q\u03021, ..., Q\u0302h. output intersection vertices W\u0302 i that corresponds to {W i : i \u2208 P}\nLet v = 4r1.5 S/\u03b1, Y = 2r v/\u03b1. Maintain list of vertices {W\u0302 i}, matrix Y , and subspace \u0393 that correspond to the left singular space of Y with singular values larger than \u03b1/2. for i = 1 TO r do\nMaintain set U \u2282 [k], \u03a3 = [Q\u0302\u22a5i : i \u2208 U ] and Z be the space of left singular vectors of \u03a3 with singular values at most r S . Initialize S = \u2205 for j = 1 TO h do\nLet \u03a3\u2032 = \u03a3 + PQ\u0302\u22a5j , Let Z \u2032 be the space of eigenvectors of \u03a3 with eigenvalues at most r S . if dim(Z \u2032) < dim(Z) and \u2016P\u0393\u22a5Z \u2032\u2016 > \u03b1/2 then let U = U \u222a {j}, replace \u03a3 and Z by \u03a3\u2032, Z \u2032.\nend if end for Append Z to Y (Y = [Y, Z]), update \u0393. If dim(Z) = 1 then add the direction to list of W\u0302\nend for\nthat is an (arbitrary) orthogonal basis of the orthogonal subspace of Q\u2032j in span of W . Let B = [B 1, B2, ..., Bt] \u2208 Rn\u00d7(r\u22121). We know BBT \u2211 j P(Q\u2032j)\u22a5 \u03a3\u03a3\nT . Therefore we only need to show the matrix B has large smallest singular value.\nNow consider the product WB. By construction of B, this is a block diagonal matrix (with blocks correspond to S\u2032j\u2019s). Since W is \u03b1-robust we know each block has smallest singular value \u03b1. Therefore \u03c3min(WB) \u2265 \u03b1, and \u03c3min(B) \u2265 \u03b1/\u2016W\u2016 \u2265 \u03b1/ \u221a r. By the relationship between \u03a3 and B we know \u03c3n\u22121(\u03a3) \u2265 \u03c3min(B) \u2265 \u03b1/ \u221a r.\nThis lemma allows us to take the intersections of subspaces robustly. We prove the theorem by induction. The induction hypothesis is\nClaim C.1. At the end of every outer-loop, W\u0302 i\u2019s are v = 4r1.5 S/\u03b1 close to some vertices in W , \u0393 is Y = 2r v/\u03b1close to a subspace spanned by WR where R \u2282 [r] is a subset of vertices. The set R never contains any vertex in P that is not already close to one of the elements in the list W\u0302 i.\nClearly this hypothesis is true before the first iteration (everything was empty). Next we analyze the inner-loop of the algorithm. During the inner-loop the algorithm maintains the following properties:\nLemma C.2. The set U always has size at most r\u22121, the subspace Z is always v = 4r1.5 S/\u03b1-close to the subspace spanned by {Ai : i \u2208 \u2229j\u2208USj}.\nProof. After the first element is added to U , the dimension of Z is equal to the dimension of some Q\u0302j , which is at most r \u2212 1. Every time we add an element to U the dimension of Z decreases by 1, and when dim(Z) becomes 1 the algorithm stops. So there must be at most r \u2212 1 elements in U . By Lemma C.1 we know if the matrix consist of the true Q\u22a5j , then it has nullspace equal to the span of {W i : i \u2208 \u2229j\u2208USj}, and all the other directions have eigenvalue at least \u03b1/ \u221a r. The difference between \u03a3 and the true matrix is at most 2r S , so when 2r S < \u03b1/4 \u221a r by matrix perturbation bounds (Wedin\u2019s TheoremStewart & Sun (1990)) we know Z is always 4r1.5 S/\u03b1-close.\nAnother property is that in the intersection \u2229j\u2208USj there is always an element that is not already found.\nLemma C.3. \u2016P\u0393\u22a5Z\u2016 > \u03b1/2 if and only if \u2229j\u2208USj contains at least one element outside of R. Further, this ensures S = \u2229j\u2208USj always contains at least one element outside of R during the inner-loop.\nProof. This is because by induction hypothesis \u0393 is Y -close to the row span ofWR. On the other hand by Lemma C.2 we know Z is v close to the row span of W\u2229j\u2208USj . If \u2229j\u2208USj \u2282 R then the row span of W\u2229j\u2208USj is a subspace of the row span of WR, and \u2016P\u0393\u22a5Z\u2016 \u2264 Y + v \u03b1/2.\nOn the other hand, if \u2229j\u2208USj has an element that is outside R, then since W is \u03b1-robust, there is a direction in W\u2229j\u2208USj that has distance at least \u03b1 to the row span of WR. By triangle inequality the distance between P\u0393\u22a5Z \u2265 \u03b1\u2212 Y \u2212 v > \u03b1/2.\nThe last statement of the lemma then follows directly because this is true initially (S = [r] initially) and the conditions in the if-statement ensures this property is preserved.\nUsing these two properties we know whenever the inner-loop adds a point to the list W\u0302 i then it must be v close to one of the unfound W i\u2019s (which is the first part of the induction hypothesis). Next we prove if at the end of the inner-loop dim(Z) is more than 1, then \u2229j\u2208USj does not contain any vertices in P\\R.\nLemma C.4. If dim(Z) is more than 1 after the inner-loop, then \u2229j\u2208USj does not contain any vertices in P\\R.\nProof. Assume towards contradiction that dim(Z) > 1 and there is an element i \u2208 P\\R and i \u2208 \u2229j\u2208USj . Let S = \u2229j\u2208USj after the inner-loop. By assumption and by Lemma C.3 we know S has at least two elements, one of them must be i, and call another i\u2032. By the property of P we know there exists a set Sj where i \u2208 Sj and i\u2032 6\u2208 Sj . Clearly j > p (where p is the initial element) as it contains an element outside or R and Sp must contain i\u2032 (otherwise i\u2032 will not be in S).\nWhen the inner-loop goes to j, by Lemma C.2 the dimension of Z \u2032 will be smaller than Z. Also, by the robustness we know the set at that point contains an element (namely i) that is not in R, so by Lemma C.3 we know \u2016P\u0393\u22a5Z \u2032\u2016 > \u03b1/2. As a result j must be added to U and this contradicts with the fact that in the end i\u2032 is still in S.\nLet S = \u2229j\u2208USj after the inner-loop, finally we show in the next iteration \u0393 will be Y -close to the span of vertices in R \u222a S.\nLemma C.5. Let S = \u2229j\u2208USj after the inner-loop, then in the next iteration, \u0393 is Y close to row span of WR\u222aS .\nProof. Based on the hypothesis all the matrices appended to the matrix Y are v-close to the span of subset of rows of W , and the union of all the previous subsets equal to R. Let B be a matrix that corresponds to the matrix Y with the true spans, then \u2016B \u2212 Y \u2016 \u2264 r v , and on the other hand the span of B is equal to row span of WR\u222aS , with smallest nonzero singular value at least \u03b1 (because W is \u03b1-robust). Therefore by Wedin\u2019s theorem we know since r v \u03b1 \u0393 must be Y = 2r v/\u03b1-close to the row span of WR\u222aS .\nThe last two lemmas proved the second half of induction hypothesis. Finally it is easy to see that the algorithm will not stop as long as P\\R is not empty, and it must stop after r iterations because the size of R increases by at least 1 in every iteration. This concludes the proof of Theorem 6.1\nFinding the remaining vertices The proof of Theorem 6.2 follows directly from Lemma 4.5 in Arora et al. (2013), for completeness we explain the proof here. ( v \u2264 \u03b1/20r, O( /\u03b12))\nProof. (of Theorem 6.2) First observe that \u03b1-robust implies \u03b1-robust in Arora et al. (2013), because for any vertexW i, let v be the direction of W i projected to the orthogonal subspace of W\u2212i (all the other rows). By \u03b1-robust condition of this paper we know \u2016Wv\u2016 \u2265 \u03b1, which in particular implies \u2016P(W\u2212i)\u22a5W i\u2016 \u2265 \u03b1.\nBy Lemma 4.5 in Arora et al. (2013), as long as the previously found vertices are at least \u03b1/20r-close, and all the points are -close, the new vertex found by the algorithm must be O( /\u03b12) close. Since /\u03b12 v we can find all the remaining vertices.\nNote that we are not running the clean-up phase of Algorithm 4 FastAnchorWords, this is because the vertices we find in this phase is already more accurate than the intersection vertices and the clean-up phase cannot improve the quality of the intersection vertices (as they don\u2019t appear in M i)."}, {"heading": "D Generative model for subset-separable NMF", "text": "In this section we prove under natural generative model an NMF problem can have (N,H, \u03b3)-properly filled facets with high probability.\nIn order to prove Theorem 7.1, We use the following two lemmas. The first lemma shows with enough uniform points in a simplex, with high probability one of them will be a center for the simplex.\nLemma D.1 (Restating Lemma 7.2). Given n = \u2126((4d)d log d/\u03b7) uniform points v1, v2, ..., vn in a standard ddimensional simplex (with vertices e1, e2, ..., ed), with probability 1 \u2212 \u03b7 there exists a point vi such that vi =\u2211 j 6=i wjv j (wj \u2265 0, \u2211 j 6=i wj = 1), and \u03c3min( \u2211 j 6=i wj(v j)(vj)T ) \u2265 1/16d.\nProof. Consider d + 1 subsets of the d-dimensional simplex: let S0 be the set of points that satisfy vi \u2265 1/2d for all i \u2208 [d]; let Sj (j \u2208 [d]) be the set of points that satisfy vj \u2265 1 \u2212 1/4d. The volume of these sets are at least (4d)\u2212d. By simple Chernoff bound we know when there are n = \u2126((4d)d log d/\u03b7) samples, with probability at least 1 \u2212 \u03b7 there is a point in each of these sets.\nNext we shall prove the point in S0 is in the convex hull of the points in Sj , and the convex hull satisfies the smallest singular value requirement. First we relabel the points, let v0 be any point in S0 and vj be any point in Sj (j 6= 0). Let V \u2208 Rd\u00d7d be the matrix whose columns are vj\u2019s (j \u2208 [d]). We can apply Gershgorin\u2019s Disk Theorem to the matrix V TV (this is a matrix with diagonal entries at least 1 \u2212 1/2d and off-diagonal entries at most 1/2d), and conclude that \u03c3min(V TV ) \u2265 1/4.\nSince in particular V is full rank, let w = V \u22121v0. Let wi be the smallest entry. If ui < 0 then since \u2211d j=1 wj = 1\n(all the columns of V and v0 sum up to 1), \u2211d j=1 |uj | \u2264 1 \u2212 2dui. The i-th coordinate (V u)i = \u2211d j=1 wjv j i \u2264 (1\u22121/4d)wi+(1\u22122dwi)/4d \u2264 1/4d, which cannot be equal to v0i , thereforewi \u2265 0. In this case since 1/2d \u2264 v0i = (V w)i = \u2211d j=1 wjv j i \u2264 wi + 14d , we know wi \u2265 1/4d. Therefore \u03c3min( \u2211 j 6=i wj(v j)(vj)T ) \u2265 14d\u03c3min(V V T ) \u2265\n1 16d .\nNext lemma shows only subspaces that contains a properly filled facet can have many points.\nLemma D.2 (Restating Lemma 7.3). Given n = \u2126(rd log(d/pmin\u03b7)/pmin) uniform points v1, v2, ..., vn in a standard d-dimensional simplex (with vertices e1, e2, ..., ed), with probability 1 \u2212 \u03b7 for all matrices A \u2208 Rr\u00d7d whose largest column norm is equal to 1, there are at most pminn/4 points with \u2016Avi\u2016 \u2264 pmin/200d.\nProof. We first prove this for a particular matrix A, then we will construct an -net and do union bound over all possible matrices A.\nLet u = Ai where Ai is the column with norm 1. For random v that is uniform in the standard d dimensional simplex, we will show Pr[|uTAv| \u2264 ...] \u2264 pmin/8. By property of uniform distribution on a simplex, we know vi is independent of v\u2212i/(1 \u2212 vi) (where v\u2212i is the vector v with i-th coordinate removed), and vi is distributed as a Beta distribution Beta(1, d\u2212 1). Let q = uTAv\u2212i/(1\u2212 vi), then we know uTAv = vi + (1\u2212 vi)q and q \u2208 [\u22121, 1]. The density function of vi is bounded by d \u2212 1, therefore for any value q, the probability that |uTAv| \u2264 pmin/100d is at most pmin/8. When the number of samples is at least n = \u2126(log(1/\u03b7\u2032)/pmin), with probability 1 \u2212 \u03b7\u2032 there are at most pminn/4 points that satisfy |uTAv| \u2264 pmin/100d.\nNow we construct an -net so that for any matrix A with largest column norm 1, there is a matrix A\u2032 in the - net that is column-wise -close to A. Set = pmin/200d2, by standard construction the number of matrices in the -net is O(d2/pmin)rd). Let \u03b7\u2032 = \u03b7/O(d2/pmin)d 2\n) (and hence n = \u2126(rd log(d/pmin\u03b7)/pmin), by union bound we know with probability 1 \u2212 \u03b7, there are at most pminn/4 points with \u2016Avi\u2016 \u2264 pmin/100d for all matrices A in the -net. For a matrix A that is not in the -net, let A\u2032 be the matrix in the net that is column-wise -close, clearly \u2016Avi\u2016\u2212\u2016A\u2032vi\u2016 \u2264 pmin/200d. If there are more than pminn/4 points with \u2016Avi\u2016 \u2264 pmin/200d then all these points will have \u2016A\u2032vi\u2016 \u2264 pmin/100d and that is impossible.\nWith these two lemmas we can now prove the theorem:\nProof. (of Theorem 7.1) In order to satisfy Condition 1, we apply Lemma 7.2. For any proper facet the points are equal to the rows of WSi multiplied by uniform random points, since \u03c3min(WSi) \u2265 \u03c3min(W ) \u2265 \u03b1, we know if\nthe facet has more than \u2126((4d)d log d/\u03b7) points the convex combination has smallest singular value \u03b12/16d. This is ensured when the number of samples is at least n = \u2126((4d)d log(kd/\u03b7)/pmin) by Chernoff bound.\nCondition 2 is satisfied whenever n = \u2126(log(k/\u03b7)/pmin) by simple Chernoff bound. Condition 3 follows from Lemma 7.3. Suppose Q is a subspace that for any proper facet Qi we have \u2016PQ\u22a5Qi\u2016 >\nH . Since W is \u03b1-robust this means \u2016PQ\u22a5WSi\u2016 \u2265 H\u03b1 , therefore there is always a column that has norm H\u03b1 / \u221a r. By Lemma 7.3 we know no matter which subspace the point is chosen from, with probability at most pmin/8 it will be /2-close to the subspace. Now we can apply union bound to the product of all the -nets constructed for different proper facets, so the size of the net is exp(kdr log d). Therefore we know when n = \u2126(kr2 log(r/pmin\u03b7)/pmin) (here for simplicity we used d = r because in particular the interior points are in a space of dimension r) with high probability there will be at most pminn/4 points for this subspace Q."}], "references": [{"title": "Computing a nonnegative matrix factorization \u2013 provably", "author": ["S. Arora", "R. Ge", "R. Kannan", "A. Moitra"], "venue": "In STOC, pp", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "A practical algorithm for topic modeling with provable guarantees", "author": ["Arora", "Sanjeev", "Ge", "Rong", "Halpern", "Yoni", "Mimno", "David M", "Moitra", "Ankur", "Sontag", "David", "Wu", "Yichen", "Zhu", "Michael"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Arora et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2013}, {"title": "Factoring nonnegative matrices with linear programs", "author": ["V. Bittorf", "B. Recht", "C. Re", "J. Tropp"], "venue": "In NIPS,", "citeRegEx": "Bittorf et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bittorf et al\\.", "year": 2012}, {"title": "Nonnegative matrix factorization: an analytical and interpretive tool in computational biology", "author": ["K. Devarajan"], "venue": "PLoS Comput Biol,", "citeRegEx": "Devarajan,? \\Q2009\\E", "shortCiteRegEx": "Devarajan", "year": 2009}, {"title": "When does non-negative matrix factorization give the correct decomposition into parts", "author": ["D. Donoho", "V. Stodden"], "venue": "In NIPS,", "citeRegEx": "Donoho and Stodden,? \\Q2003\\E", "shortCiteRegEx": "Donoho and Stodden", "year": 2003}, {"title": "Sparse subspace clustering", "author": ["Elhamifar", "Ehsan", "Vidal", "Ren\u00e9"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Elhamifar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Elhamifar et al\\.", "year": 2009}, {"title": "Robustness analysis of hotttopixx, a linear programming model for factoring nonnegative matrices", "author": ["N. Gillis"], "venue": null, "citeRegEx": "Gillis,? \\Q2012\\E", "shortCiteRegEx": "Gillis", "year": 2012}, {"title": "The why and how of nonnegative matrix factorization", "author": ["N. Gillis"], "venue": null, "citeRegEx": "Gillis,? \\Q2014\\E", "shortCiteRegEx": "Gillis", "year": 2014}, {"title": "Fast and robust recursive algorithmsfor separable nonnegative matrix factorization", "author": ["N. Gillis", "S.A. Vavasis"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Gillis and Vavasis,? \\Q2014\\E", "shortCiteRegEx": "Gillis and Vavasis", "year": 2014}, {"title": "N-findr method versus independent component analysis for lithological identification in hyperspectral imagery", "author": ["C. Gomez", "Borgne", "H. Le", "P. Allemand", "C. Delacourt", "P. Ledru"], "venue": "Int. J. Remote Sens.,", "citeRegEx": "Gomez et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gomez et al\\.", "year": 2007}, {"title": "Algorithms and hardness for robust subspace recovery", "author": ["Hardt", "Moritz", "Moitra", "Ankur"], "venue": "In COLT, pp", "citeRegEx": "Hardt et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2013}, {"title": "Fast conical hull algorithms for near-separable non-negative matrix factorization", "author": ["A. Kumar", "V. Sindhwani", "P. Kambadur"], "venue": null, "citeRegEx": "Kumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2012}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D. Lee", "H. Seung"], "venue": "Nature, pp", "citeRegEx": "Lee and Seung,? \\Q1999\\E", "shortCiteRegEx": "Lee and Seung", "year": 1999}, {"title": "Algorithms for non-negative matrix factorization", "author": ["D. Lee", "H. Seung"], "venue": "In NIPS, pp", "citeRegEx": "Lee and Seung,? \\Q2000\\E", "shortCiteRegEx": "Lee and Seung", "year": 2000}, {"title": "Projected gradient methods for nonnegative matrix factorization", "author": ["Lin", "Chih-Jen"], "venue": "Neural computation,", "citeRegEx": "Lin and Chih.Jen.,? \\Q2007\\E", "shortCiteRegEx": "Lin and Chih.Jen.", "year": 2007}, {"title": "An almost optimal algorithm for computing nonnegative rank", "author": ["Moitra", "Ankur"], "venue": "In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Moitra and Ankur.,? \\Q2013\\E", "shortCiteRegEx": "Moitra and Ankur.", "year": 2013}, {"title": "Vertex component analysis: A fast algorithm to unmix hyperspectral data", "author": ["Nascimento", "J.M. P", "J.M.B. Dias"], "venue": "IEEE TRANS. GEOSCI. REM. SENS,", "citeRegEx": "Nascimento et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Nascimento et al\\.", "year": 2004}, {"title": "Matrix perturbation theory, volume 175", "author": ["G.W. Stewart", "J. Sun"], "venue": "Academic press New York,", "citeRegEx": "Stewart and Sun,? \\Q1990\\E", "shortCiteRegEx": "Stewart and Sun", "year": 1990}, {"title": "On the complexity of nonnegative matrix factorization", "author": ["S. Vavasis"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Vavasis,? \\Q2009\\E", "shortCiteRegEx": "Vavasis", "year": 2009}, {"title": "A tutorial on subspace clustering", "author": ["Vidal", "Ren\u00e9"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Vidal and Ren\u00e9.,? \\Q2010\\E", "shortCiteRegEx": "Vidal and Ren\u00e9.", "year": 2010}, {"title": "In this section we first prove Theorem 6.1, then we discuss how to apply Algorithm", "author": ["Arora"], "venue": null, "citeRegEx": "Arora,? \\Q2013\\E", "shortCiteRegEx": "Arora", "year": 2013}, {"title": "The last two lemmas proved the second half of induction hypothesis. Finally it is easy to see that the algorithm will not stop as long as P\\R is not empty, and it must stop after r iterations because the size of R increases by at least 1 in every iteration. This concludes the proof of Theorem 6.1 Finding the remaining vertices The proof of Theorem 6.2 follows directly from Lemma", "author": ["Arora"], "venue": null, "citeRegEx": "Arora,? \\Q2013\\E", "shortCiteRegEx": "Arora", "year": 2013}, {"title": "2013), as long as the previously found vertices are at least \u03b1/20r-close, and all the points are -close, the new vertex found by the algorithm must be O( /\u03b1", "author": ["Arora"], "venue": null, "citeRegEx": "Arora,? \\Q2013\\E", "shortCiteRegEx": "Arora", "year": 2013}], "referenceMentions": [{"referenceID": 9, "context": ", 2003), hyperspectral unmixing(Nascimento & Dias, 2004; Gomez et al., 2007), computational biology (Devarajan, 2009), etc.", "startOffset": 31, "endOffset": 76}, {"referenceID": 3, "context": ", 2007), computational biology (Devarajan, 2009), etc.", "startOffset": 31, "endOffset": 48}, {"referenceID": 0, "context": "We use this notation to be consistent with previous works (Arora et al., 2012).", "startOffset": 58, "endOffset": 78}, {"referenceID": 12, "context": "Moreover, even ignoring the identifiabilityVavasis (2009) showed that finding any factorization M = AW with inner-dimension r is an NP -hard problem.", "startOffset": 43, "endOffset": 58}, {"referenceID": 0, "context": "Arora et al. (2012) showed under reasonable assumptions we cannot hope to find a factorization in time (mn), and the best algorithm known is Moitra (2013) that runs in time O(2mn) 2).", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Arora et al. (2012) showed under reasonable assumptions we cannot hope to find a factorization in time (mn), and the best algorithm known is Moitra (2013) that runs in time O(2mn) 2).", "startOffset": 0, "endOffset": 155}, {"referenceID": 0, "context": "Arora et al. (2012) showed under reasonable assumptions we cannot hope to find a factorization in time (mn), and the best algorithm known is Moitra (2013) that runs in time O(2mn) 2). Many heuristic algorithms have been developed for NMF but they do not have guarantees for when they would actually converge to the true factorization (Lee & Seung, 2000; Lin, 2007). More recently, there has been a surge of interest in constructing practical NMF algorithms with strong theoretical guarantees. Most of this activity (e.g. Arora et al. (2012); Bittorf et al.", "startOffset": 0, "endOffset": 541}, {"referenceID": 0, "context": "Arora et al. (2012) showed under reasonable assumptions we cannot hope to find a factorization in time (mn), and the best algorithm known is Moitra (2013) that runs in time O(2mn) 2). Many heuristic algorithms have been developed for NMF but they do not have guarantees for when they would actually converge to the true factorization (Lee & Seung, 2000; Lin, 2007). More recently, there has been a surge of interest in constructing practical NMF algorithms with strong theoretical guarantees. Most of this activity (e.g. Arora et al. (2012); Bittorf et al. (2012); Kumar et al.", "startOffset": 0, "endOffset": 564}, {"referenceID": 0, "context": "Arora et al. (2012) showed under reasonable assumptions we cannot hope to find a factorization in time (mn), and the best algorithm known is Moitra (2013) that runs in time O(2mn) 2). Many heuristic algorithms have been developed for NMF but they do not have guarantees for when they would actually converge to the true factorization (Lee & Seung, 2000; Lin, 2007). More recently, there has been a surge of interest in constructing practical NMF algorithms with strong theoretical guarantees. Most of this activity (e.g. Arora et al. (2012); Bittorf et al. (2012); Kumar et al. (2012); Gillis (2012); Gillis & Vavasis (2014), see more in Gillis (2014)) are based on the notion of separabilityDonoho & Stodden (2003) which is a very strict condition that requires that all the rows of W appear as rows in M .", "startOffset": 0, "endOffset": 585}, {"referenceID": 0, "context": "Arora et al. (2012) showed under reasonable assumptions we cannot hope to find a factorization in time (mn), and the best algorithm known is Moitra (2013) that runs in time O(2mn) 2). Many heuristic algorithms have been developed for NMF but they do not have guarantees for when they would actually converge to the true factorization (Lee & Seung, 2000; Lin, 2007). More recently, there has been a surge of interest in constructing practical NMF algorithms with strong theoretical guarantees. Most of this activity (e.g. Arora et al. (2012); Bittorf et al. (2012); Kumar et al. (2012); Gillis (2012); Gillis & Vavasis (2014), see more in Gillis (2014)) are based on the notion of separabilityDonoho & Stodden (2003) which is a very strict condition that requires that all the rows of W appear as rows in M .", "startOffset": 0, "endOffset": 600}, {"referenceID": 0, "context": "Arora et al. (2012) showed under reasonable assumptions we cannot hope to find a factorization in time (mn), and the best algorithm known is Moitra (2013) that runs in time O(2mn) 2). Many heuristic algorithms have been developed for NMF but they do not have guarantees for when they would actually converge to the true factorization (Lee & Seung, 2000; Lin, 2007). More recently, there has been a surge of interest in constructing practical NMF algorithms with strong theoretical guarantees. Most of this activity (e.g. Arora et al. (2012); Bittorf et al. (2012); Kumar et al. (2012); Gillis (2012); Gillis & Vavasis (2014), see more in Gillis (2014)) are based on the notion of separabilityDonoho & Stodden (2003) which is a very strict condition that requires that all the rows of W appear as rows in M .", "startOffset": 0, "endOffset": 625}, {"referenceID": 0, "context": "Arora et al. (2012) showed under reasonable assumptions we cannot hope to find a factorization in time (mn), and the best algorithm known is Moitra (2013) that runs in time O(2mn) 2). Many heuristic algorithms have been developed for NMF but they do not have guarantees for when they would actually converge to the true factorization (Lee & Seung, 2000; Lin, 2007). More recently, there has been a surge of interest in constructing practical NMF algorithms with strong theoretical guarantees. Most of this activity (e.g. Arora et al. (2012); Bittorf et al. (2012); Kumar et al. (2012); Gillis (2012); Gillis & Vavasis (2014), see more in Gillis (2014)) are based on the notion of separabilityDonoho & Stodden (2003) which is a very strict condition that requires that all the rows of W appear as rows in M .", "startOffset": 0, "endOffset": 652}, {"referenceID": 0, "context": "Arora et al. (2012) showed under reasonable assumptions we cannot hope to find a factorization in time (mn), and the best algorithm known is Moitra (2013) that runs in time O(2mn) 2). Many heuristic algorithms have been developed for NMF but they do not have guarantees for when they would actually converge to the true factorization (Lee & Seung, 2000; Lin, 2007). More recently, there has been a surge of interest in constructing practical NMF algorithms with strong theoretical guarantees. Most of this activity (e.g. Arora et al. (2012); Bittorf et al. (2012); Kumar et al. (2012); Gillis (2012); Gillis & Vavasis (2014), see more in Gillis (2014)) are based on the notion of separabilityDonoho & Stodden (2003) which is a very strict condition that requires that all the rows of W appear as rows in M .", "startOffset": 0, "endOffset": 716}, {"referenceID": 0, "context": "Given a factorization M = AW , without loss of generality we can assume the rows of M,A,W all sum up to 1 (this can always be done, see Arora et al. (2012)).", "startOffset": 136, "endOffset": 156}, {"referenceID": 0, "context": "This can be done in time polynomial in n,m and r (Arora et al., 2012).", "startOffset": 49, "endOffset": 69}, {"referenceID": 0, "context": "Run Algorithm 5 (similar to Algorithm 4 in Arora et al. (2013)) to find the singleton points (anchors).", "startOffset": 43, "endOffset": 63}, {"referenceID": 0, "context": "Finally we adapt the algorithm from Arora et al. (2013) to find the remaining vertices that correspond to singleton sets.", "startOffset": 36, "endOffset": 56}, {"referenceID": 0, "context": "This is similar to the separable case and we use an algorithm from Arora et al. (2013).", "startOffset": 67, "endOffset": 87}, {"referenceID": 0, "context": "The situation is very similar to the separable NMF and we use an algorithm from Arora et al. (2013) to find the remaining vertices.", "startOffset": 80, "endOffset": 100}, {"referenceID": 0, "context": "The situation is very similar to the separable NMF and we use an algorithm from Arora et al. (2013) to find the remaining vertices. For completeness we describe the algorithm here. By Lemma 4.5 in Arora et al. (2013) we directly get the following theorem:", "startOffset": 80, "endOffset": 217}, {"referenceID": 1, "context": "We choose two typical algorithms: the Anchor-Words algorithm(Arora et al., 2013) for separable NMF, and Projected Gradient (Lin, 2007) for iterative algorithms.", "startOffset": 60, "endOffset": 80}], "year": 2015, "abstractText": "Non-negative matrix factorization (NMF) is a natural model of admixture and is widely used in science and engineering. A plethora of algorithms have been developed to tackle NMF, but due to the non-convex nature of the problem, there is little guarantee on how well these methods work. Recently a surge of research have focused on a very restricted class of NMFs, called separable NMF, where provably correct algorithms have been developed. In this paper, we propose the notion of subset-separable NMF, which substantially generalizes the property of separability. We show that subset-separability is a natural necessary condition for the factorization to be unique or to have minimum volume. We developed the Face-Intersect algorithm which provably and efficiently solves subset-separable NMF under natural conditions, and we prove that our algorithm is robust to small noise. We explored the performance of Face-Intersect on simulations and discuss settings where it empirically outperformed the state-of-art methods. Our work is a step towards finding provably correct algorithms that solve large classes of NMF problems.", "creator": "LaTeX with hyperref package"}}}