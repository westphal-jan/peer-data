{"id": "1507.08449", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jul-2015", "title": "One model, two languages: training bilingual parsers with harmonized treebanks", "abstract": "We introduce an approach to train parsers using bilingual corpora obtained by merging harmonized treebanks of different languages, producing parsers that effectively analyze sentences in any of the learned languages, or even sentences that mix both languages. We test the approach on the Universal Dependency Treebanks, training with MaltParser and MaltOptimizer.The results show that these bilingual parsers are more than competitive, as some combinations not only preserve the performance, but even achieve significant improvements over the corresponding monolingual parsers. Preliminary experiments also show the approach to be promising on texts with code-switching.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 30 Jul 2015 10:53:11 GMT  (90kb)", "https://arxiv.org/abs/1507.08449v1", "6 pages, 2 tables, 1 figure"], ["v2", "Thu, 19 May 2016 14:57:45 GMT  (112kb)", "http://arxiv.org/abs/1507.08449v2", "7 pages, 4 tables, 1 figure"]], "COMMENTS": "6 pages, 2 tables, 1 figure", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["david vilares", "carlos g\u00f3mez-rodr\u00edguez", "miguel a alonso"], "accepted": true, "id": "1507.08449"}, "pdf": {"name": "1507.08449.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["miguel.alonso}@udc.es"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 7.\n08 44\n9v 2\n[ cs\n.C L\n] 1\n9 M\nay 2\n01 6"}, {"heading": "1 Introduction", "text": "The need of frameworks for analyzing content in different languages has been discussed recently (Dang et al., 2014), and multilingual dependency parsing is no stranger to this challenge. Data-driven parsing models (Nivre, 2006) can be trained for any language, given enough annotated data.\nOn languages where treebanks are not available, cross-lingual transfer can be used to train parsers for a target language with data from one or more source languages. Data transfer approaches (e.g. Yarowsky et al. (2001), Tiedemann (2014)) map linguistic annotations across languages through parallel corpora. Instead, model transfer approaches (e.g. Naseem et al. (2012)) rely on crosslinguistic syntactic regularities to learn aspects of the source language that help parse an unseen language, without parallel corpora.\nModel transfer approaches have benefitted from the development of multilingual resources that harmonize annotations. Petrov et al. (2011) proposed a universal tagset, and McDonald et al. (2011) employed it to transfer delexicalized parsers (Zeman and Resnik, 2008). More recently, several projects have presented treebank collections of multiple languages with their annotations standardized at the syntactic level, including HamleDT (Zeman et al., 2012) and the Universal Dependency Treebanks (McDonald et al., 2013).\nIn this paper we also rely on these resources, but with a different goal: we use universal annotations to train bilingual dependency parsers that effectively analyze unseen sentences in any of the learned languages. Unlike delexicalized approaches for model transfer, our parsers exploit lexical features. The results are encouraging: our experiments show that, starting with a monolingual parser, we can \u201cteach\u201d it an additional language for free in terms of accuracy (i.e., without significant accuracy loss on the original language, in spite of learning a more complex task) in the vast majority of cases."}, {"heading": "2 Bilingual training", "text": "Universal Dependency Treebanks v2.0 (McDonald et al., 2013) is a set of CoNLLformatted treebanks for ten languages, annotated with common criteria. They include two versions of PoS tags: universal tags (Petrov et al., 2011) in the CPOSTAG column, and a refined annotation with treebank-specific information in the POSTAG column. Some of the latter tags are not part of the core universal set, and they can denote linguistic phenomena that are language-specific, or phenomena that not all the corpora have annotated in the same way.\nTo train monolingual parsers (our baseline), we used the official training-dev-set splits provided with the corpora. For the bilingual models, for each pair of languages L1, L2; we simply merged their training sets into a single file acting as a training set for L1\u222aL2, and we did the same for the development sets. The test sets were not merged because comparing the bilingual parsers to monolingual ones requires evaluating each bilingual parser on the two corresponding monolingual test sets.\nTo build the models, we relied on MaltParser (Nivre et al., 2007). Due to the large number of language pairs that complicates manual optimization, and to ensure a fair comparison, we used MaltOptimizer (Ballesteros and Nivre, 2012), an automatic optimizer for MaltParser models. This system works in three phases: Phase 1 and 2 choose a parsing algorithm by analyzing the training set, and performing experiments with default features. Phase 3 tunes the feature model and algorithm parameters. We hypothesize that the bilingual models will learn a set of features that fits both languages, and check this hypothesis by evaluating on the test sets.\nWe propose two training configurations: (1) a treebank-dependent tags configuration where we include the information in the POSTAG column and (2) a universal tags only configuration, where we do not use this information, relying only on the CPOSTAG column. Information that could be present in FEATS or LEMMA columns is not used in any case. This methodology plans to answer two research questions: (1) can we train bilingual parsers with good accuracy by merging harmonized training sets?, and (2) is it essential that the tagsets for both languages are the same, or can we still get accuracy gains from fine-grained PoS tags (as in the monolingual case) even if some of them are treebank-specific?\nAll models are freely available.1"}, {"heading": "3 Evaluation", "text": "To ensure a fair comparison between monolingual and bilingual models, we chose to optimize the models from scratch with MaltOptimizer, expecting it to choose the parsing algorithm and feature model which is most likely to obtain good results. We observed that the selection of a bilingual parsing algorithm was not necessarily related with the algorithms selected for the monolingual models.\n1http://grupolys.org/software/PARSERS/\nThe system sometimes chose an algorithm for a bilingual model that was not selected for any of the corresponding monolingual models.\nIn view of this, and as it is known that different parsing algorithms can be more or less competitive depending on the language (Nivre, 2008), we ran a control experiment to evaluate the models setting the same parsing algorithm for all cases, executing only phase 3 of MaltOptimizer. We chose the arc-eager parser for this experiment, as it was the algorithm that MaltOptimizer chose most frequently for the monolingual models in the previous configuration. The aim was to compare the accuracy of the bilingual models with respect to the monolingual ones, when there is no variation on the parsing algorithm between them. The results of this control experiment are not shown for space reasons, but they were very similar to those of the original experiment."}, {"heading": "3.1 Results on the Universal Treebanks", "text": "Table 1 compares the accuracy of bilingual models to that of monolingual ones, under the treebankdependent tags configuration. Each table cell shows the accuracy of a model, in terms of LAS and UAS. Cells in the diagonal correspond to monolingual models (the baseline), with the cell located at row i and column i representing the result obtained by training a monolingual parser on the training set of language Li, and evaluating it on the test set of the same language Li. Each cell outside the diagonal (at row i and column j, with j 6= i) shows the results of training a bilingual model on the training set for Li\u222aLj , evaluated on the test set of Li.\nAs we can see, in a large majority of cases, bilingual parsers learn to parse two languages with no statistically significant accuracy loss with respect to the corresponding monolingual parsers (p < 0.05 with Bikel\u2019s randomized parsing evaluation comparator). This happened in 74 out of 90 cases when measuring UAS, or 69 out of 90 in terms of LAS. Therefore, in most cases where we are applying a parser to texts in a given language, adding a second language comes for free in terms of accuracy.\nMore strikingly, there are many cases where bilingual parsers outperform monolingual ones, even in this evaluation on purely monolingual datasets. In particular, there are 12 cases where a bilingual parser obtains statistically\nsignificant gains in LAS over the monolingual baseline, and 9 cases with significant gains in UAS. This clearly surpasses the amount of significant gains to be expected by chance, and applying the Benjamini-Hochberg procedure (Benjamini and Hochberg, 1995) to correct for multiple comparisons with a maximum false discovery rate of 20% yields 8 significant improvements in LAS and UAS. Therefore, it is clear that there is synergy between datasets: in some cases, adding annotated data in a different language to our training set can actually improve the accuracy that we obtain in the original language. This opens up interesting research potential in using confidence criteria to se-\nlect the data that can help parsing in this way, akin to what is done in self-training approaches (Chen et al., 2008; Goutam and Ambati, 2011).\nComparing the results by language, we note that the accuracy on the English and Spanish datasets almost always improves when adding a second treebank for training. Other languages that tend to get improvements in this way are French and Portuguese. There seems to be a rough trend towards the languages with the largest training corpora benefiting from adding a second language, and those with the smallest corpora (e.g. Indonesian, Italian or Japanese) suffering accuracy loss, likely because the training gets biased towards the second language.\nTraining bilingual models containing a significant number of non-overlapping treebankdependent tags tends to have a positive effect. English and Spanish are two of the clearest examples of this. As shown in Table 3, which shows a complete report of shared PoS tags for each pair of languages under the treebank-dependent tags configuration, English only shares 1 PoS tag with the rest of the corpora under the said configuration, except for Swedish, with up to 5 tags in common; and the en-sv model is the only one suffering a significant loss on the English test set. Similar behavior is observed on Spanish: sv (0), en (1), ja (10) and ko (12) are the four languages with fewest shared PoS tags, and those are the four that obtained a significant improvement on the Spanish evaluation; while with pt-br, with 15 shared PoS tags, we lose accuracy. The validity of this hypothesis is reinforced by an experiment where we differentiate the universal tags by language by appending a language code to them (e.g. EN NOUN for an English noun). An overall improvement was observed with respect to the bilingual parsers with non-disjoint sets of features.\nWhile all these experiments have been performed on sentences with gold PoS tags, preliminary experiments assuming predicted tags instead show analogous results: the absolute values of LAS and UAS are slightly smaller across the board, but the behavior in relative terms is the same, and the bilingual models that improved over the monolingual baseline in the gold experiments keep doing so under this setting.\nOn the other hand, Table 2 shows the performance of the monolingual and bilingual models under the universal tags only configuration. The bilingual parsers are also able to keep an acceptable accuracy with respect to the monolingual models, but significant losses are much more\nprevalent than under the treebank-dependent tags configuration.\nPutting both tables together, our experiments clearly suggest that not only treebank-specific tags do not impair the training of bilingual models, but they are even beneficial, supporting the idea that using partially treebank-dependent tagsets helps multilingual parsing. We hypothesize that this may be because complementing the universal information at the syntactic level with languagespecific information at the lower levels (lexical and morphological) may help the parser identify specific constructions of one language that would not benefit from the knowledge learned from the other, preventing it from trying to exploit spurious similarities between languages. This explanation is coherent with work on delexicalized parser transfer (Lynn et al., 2014) showing that better results can be obtained using disparate languages than closely-related languages, as long as they have common syntactic constructions. Thus, using universal PoS tags to train multilingual parsers can be, surprisingly, counterproductive."}, {"heading": "3.2 Parsing code-switched sentences", "text": "Our bilingual parsers also show robustness on texts exhibiting code-switching. Unfortunately, there are no syntactically annotated codeswitching corpora, so we could not perform a formal evaluation. We did perform informal tests, by running the Spanish-English bilingual parsers on some such sentences. We observed that they were able to parse the English and Spanish parts of the sentences much better than monolingual models. This required training a bilingual tagger, which we did with the free distribution of the Stanford tagger (Toutanova and Manning, 2000); merging the Spanish and English corpora to train a combined bilingual tagger. Under the universal tags only configuration, the multilingual tagger obtained 98.00% and 95.88% over the monolingual test sets. Using treebank-dependent tags instead, it obtained 97.19% and 93.88% over the monolingual test sets. Figure 1 shows an interesting example on how using bilingual parsers (and taggers) affects the parsing accuracy.\nTable 4 shows the performance on a tiny codeswitching treebank built on top of ten normalized tweets.2 This confirms that monolingual pipelines\n2The code-switching treebank follows the Universal Treebank v2.0 annotations. It can be obtained by asking any of the authors.\nperform poorly. Using a bilingual tagger helps improve the performance, thanks to accurate tags for both languages, but a bilingual parser is needed to push both LAS and UAS up to state-of-the-art levels."}, {"heading": "3.3 Adding more languages", "text": "To show that our approach works when more languages are added, we created a quadrilingual parser using the romanic languages and the fine PoS tag set. The results (LAS/UAS) on the monolingual sets were: 80.18/84.64 (es), 79.11/84.29 (fr), 82.16/86.15 (it) and 84.45/86.80 (pt). In all cases, the performance is almost equivalent to the monolingual parser.\nNoah\u2019s ARK group (Ammar et al., 2016) has shown that this idea can be also adapted to universal parsing. Our models are a collection of weights learned from mixing harmonized treebanks, that accurately analyze sentences in any of the learned languages and where it is possible to take advantage of linguistic universals, but they are still dependent on language-specific word forms. Instead, Ammar et al. (2016) rely on multilingual word clusters and multilingual word embeddings, learning a universal representation. They also support incorporating language-specific information (e.g. PoS tags) to keep learning language-specific behavior. To address syntactic differences be-\ntween languages (e.g. noun-adjective vs adjectivenoun structure) they can inform the parser about the input language."}, {"heading": "4 Conclusions and future work", "text": "To our knowledge, this is the first attempt to train purely bilingual parsers to analyze sentences irrespective of which of the two languages they are written in; as existing work on training a parser on two languages (Smith and Smith, 2004) focused on using parallel corpora to transfer linguistic knowledge between languages.\nOur results reflect that bilingual parsers do not lose accuracy with respect to monolingual parsers on their corresponding language, and can even outperform them, especially if fine-grained tags are used. This shows that, thanks to universal dependencies and shared syntactic structures across different languages, using treebank-dependent tag sets is not a drawback, but even an advantage.\nThe applications include parsing sentences of different languages with a single model, improving the accuracy of monolingual parsing with training sets from other languages, and successfully parsing sentences exhibiting code-switching.\nAs future work, our approach could benefit from simple domain adaptation techniques (Daume\u0301 III, 2009), to enrich the training set for a target language by incorporating data from a source language."}, {"heading": "5 Acknowledgments", "text": "This research is supported by the Ministerio de Econom\u0131\u0301a y Competitividad (FFI2014-51978C2). David Vilares is funded by the Ministerio de Educacio\u0301n, Cultura y Deporte (FPU13/01180). Carlos Go\u0301mez-Rodr\u0131\u0301guez is funded by an Opor-\ntunius program grant (Xunta de Galicia). We thank Marcos Garcia for helping with the codeswitching treebank. We also thank the reviewers for their comments and suggestions."}], "references": [{"title": "MaltOptimizer: an optimization tool for MaltParser", "author": ["Ballesteros", "Nivre2012] M. Ballesteros", "J. Nivre"], "venue": "In Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association", "citeRegEx": "Ballesteros et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2012}, {"title": "Controlling the false discovery rate: a practical and powerful approach to multiple testing", "author": ["Benjamini", "Hochberg1995] Y. Benjamini", "Y. Hochberg"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Benjamini et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Benjamini et al\\.", "year": 1995}, {"title": "Learning reliable information for dependency parsing adaptation", "author": ["W. Chen", "Y. Wu", "H. Isahara"], "venue": "In Proceedings of the 22Nd International Conference on Computational Linguistics - Volume", "citeRegEx": "Chen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2008}, {"title": "An integrated framework for analyzing multilingual content in Web 2.0 social media", "author": ["Dang et al.2014] Y. Dang", "Y. Zhang", "Paul J. Hu", "S.A. Brown", "Y. Ku", "J. Wang", "H. Chen"], "venue": "Decision Support Systems,", "citeRegEx": "Dang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dang et al\\.", "year": 2014}, {"title": "Exploring self training for Hindi dependency parsing", "author": ["Goutam", "Ambati2011] R. Goutam", "B.R. Ambati"], "venue": "In Proceedings of 5th International Joint Conference on Natural Language Processing,", "citeRegEx": "Goutam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Goutam et al\\.", "year": 2011}, {"title": "Cross-lingual transfer parsing for low-resourced languages: An Irish case study", "author": ["Lynn et al.2014] T. Lynn", "J. Foster", "M. Dras", "L. Tounsi"], "venue": "CLTW", "citeRegEx": "Lynn et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lynn et al\\.", "year": 2014}, {"title": "Multi-source transfer of delexicalized dependency parsers", "author": ["R. McDonald", "S. Petrov", "K. Hall"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "McDonald et al\\.,? \\Q2011\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2011}, {"title": "Universal dependency annotation", "author": ["R. McDonald", "J. Nivre", "Y. Quirmbach-brundage", "Y. Goldberg", "D. Das", "K. Ganchev", "K. Hall", "S. Petrov", "Hao Zhang", "O. T\u00e4ckstr\u00f6m", "C. Bedini", "N. Castell\u00f3", "J. Lee"], "venue": null, "citeRegEx": "McDonald et al\\.,? \\Q2013\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2013}, {"title": "Selective sharing for multilingual dependency parsing", "author": ["T. Naseem", "R. Barzilay", "A. Globerson"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Naseem et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Naseem et al\\.", "year": 2012}, {"title": "MaltParser: A languageindependent system for data-driven dependency parsing", "author": ["J. Nivre", "J. Hall", "J. Nilsson", "A. Chanev", "G. Eryigit", "S. K\u00fcbler", "S. Marinov", "E. Marsi"], "venue": "Natural Language Engineering,", "citeRegEx": "Nivre et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2007}, {"title": "Two strategies for text parsing", "author": ["J. Nivre"], "venue": "Kaius Sinnema\u0308ki, editors, A Man of Measure: Festschrift", "citeRegEx": "Nivre.,? \\Q2006\\E", "shortCiteRegEx": "Nivre.", "year": 2006}, {"title": "Algorithms for deterministic incremental dependency parsing", "author": ["J. Nivre"], "venue": "Computational Linguistics,", "citeRegEx": "Nivre.,? \\Q2008\\E", "shortCiteRegEx": "Nivre.", "year": 2008}, {"title": "A universal part-of-speech tagset", "author": ["S. Petrov", "D. Das", "R. McDonald"], "venue": "arXiv preprint arXiv:1104.2086", "citeRegEx": "Petrov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2011}, {"title": "Bilingual Parsing with Factored Estimation: Using English to Parse Korean", "author": ["Smith", "Smith2004] D.A. Smith", "N.A. Smith"], "venue": "Proceedings of EMNLP", "citeRegEx": "Smith et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2004}, {"title": "Rediscovering annotation projection for cross-lingual parser induction", "author": ["J. Tiedemann"], "venue": "In Proceedings of COLING 2014: 25th International Conference on Computational Linguistics,", "citeRegEx": "Tiedemann.,? \\Q2014\\E", "shortCiteRegEx": "Tiedemann.", "year": 2014}, {"title": "Enriching the knowledge sources used in a maximum entropy part-of-speech tagger", "author": ["Toutanova", "Manning2000] K. Toutanova", "C. D Manning"], "venue": "In Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language pro-", "citeRegEx": "Toutanova et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2000}, {"title": "Inducing multilingual text analysis tools via robust projection across aligned", "author": ["Yarowsky et al.2001] D. Yarowsky", "G. Ngai", "R. Wicentowski"], "venue": null, "citeRegEx": "Yarowsky et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Yarowsky et al\\.", "year": 2001}, {"title": "Cross-language parser adaptation between related languages", "author": ["Zeman", "Resnik2008] D. Zeman", "P. Resnik"], "venue": "In IJCNLP 2008 Workshop on NLP for Less Privileged Languages,", "citeRegEx": "Zeman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zeman et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 3, "context": "cently (Dang et al., 2014), and multilingual de-", "startOffset": 7, "endOffset": 26}, {"referenceID": 10, "context": "Data-driven parsing models (Nivre, 2006) can be trained for any language, given enough annotated data.", "startOffset": 27, "endOffset": 40}, {"referenceID": 14, "context": "Yarowsky et al. (2001), Tiedemann (2014)) map linguistic annotations across languages through parallel corpora.", "startOffset": 0, "endOffset": 23}, {"referenceID": 13, "context": "(2001), Tiedemann (2014)) map linguistic annotations across languages through parallel corpora.", "startOffset": 8, "endOffset": 25}, {"referenceID": 8, "context": "Naseem et al. (2012)) rely on cross-", "startOffset": 0, "endOffset": 21}, {"referenceID": 7, "context": ", 2012) and the Universal Dependency Treebanks (McDonald et al., 2013).", "startOffset": 47, "endOffset": 70}, {"referenceID": 10, "context": "Petrov et al. (2011) proposed a universal tagset, and McDonald et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 6, "context": "(2011) proposed a universal tagset, and McDonald et al. (2011) employed it to transfer delexicalized parsers (Zeman and Resnik, 2008).", "startOffset": 40, "endOffset": 63}, {"referenceID": 7, "context": "(McDonald et al., 2013) is a set of CoNLLformatted treebanks for ten languages, annotated with common criteria.", "startOffset": 0, "endOffset": 23}, {"referenceID": 12, "context": "They include two versions of PoS tags: universal tags (Petrov et al., 2011) in the CPOSTAG column, and a refined annotation with treebank-specific information in the POSTAG column.", "startOffset": 54, "endOffset": 75}, {"referenceID": 9, "context": "To build the models, we relied on MaltParser (Nivre et al., 2007).", "startOffset": 45, "endOffset": 65}, {"referenceID": 11, "context": "In view of this, and as it is known that different parsing algorithms can be more or less competitive depending on the language (Nivre, 2008), we ran a control experiment to evaluate the models setting the same parsing algorithm for all cases, executing only phase 3 of MaltOptimizer.", "startOffset": 128, "endOffset": 141}, {"referenceID": 2, "context": "This opens up interesting research potential in using confidence criteria to select the data that can help parsing in this way, akin to what is done in self-training approaches (Chen et al., 2008; Goutam and Ambati, 2011).", "startOffset": 177, "endOffset": 221}, {"referenceID": 5, "context": "This explanation is coherent with work on delexicalized parser transfer (Lynn et al., 2014) showing that better results can be obtained using disparate languages than closely-related languages, as long as they have common syntactic constructions.", "startOffset": 72, "endOffset": 91}], "year": 2016, "abstractText": "We introduce an approach to train lexicalized parsers using bilingual corpora obtained by merging harmonized treebanks of different languages, producing parsers that can analyze sentences in either of the learned languages, or even sentences that mix both. We test the approach on the Universal Dependency Treebanks, training with MaltParser and MaltOptimizer. The results show that these bilingual parsers are more than competitive, as most combinations not only preserve accuracy, but some even achieve significant improvements over the corresponding monolingual parsers. Preliminary experiments also show the approach to be promising on texts with code-switching and when more languages are added.", "creator": "LaTeX with hyperref package"}}}