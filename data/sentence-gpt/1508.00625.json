{"id": "1508.00625", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Aug-2015", "title": "Sparse PCA via Bipartite Matchings", "abstract": "We consider the following multi-component sparse PCA problem: given a set of data points, we seek to extract a small number of sparse components with disjoint supports that jointly capture the maximum possible variance. These components can be computed one by one, repeatedly solving the single-component problem and deflating the input data matrix, but as we show this greedy procedure is suboptimal. We present a novel algorithm for sparse PCA that jointly optimizes multiple disjoint components, which requires two distinct components, one for each unique component, and one for each unique value, and one for each unique value, and one for each unique value. For this analysis, we have been able to compute the following dataset and the resulting set of data points. We assume that the single-component problem has a uniform distribution of all the data points for each unique component: 1.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.25 = 4.", "histories": [["v1", "Tue, 4 Aug 2015 00:12:35 GMT  (6911kb)", "http://arxiv.org/abs/1508.00625v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.DS cs.LG math.OC", "authors": ["megasthenis asteris", "dimitris s papailiopoulos", "anastasios kyrillidis", "alexandros g dimakis"], "accepted": true, "id": "1508.00625"}, "pdf": {"name": "1508.00625.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Megasthenis Asteris", "Dimitris Papailiopoulos", "Anastasios Kyrillidis", "Alexandros G. Dimakis"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 8.\n00 62\n5v 1\n[ st\nat .M\nL ]\n4 A"}, {"heading": "1 Introduction", "text": "Principal Component Analysis (PCA) reduces the dimensionality of a data set by projecting it onto principal subspaces spanned by the leading eigenvectors of the sample covariance matrix. Sparse PCA is a useful variant that offers higher data interpretability [1, 2, 3], a property that is sometimes desired even at the cost of statistical fidelity [4]. Furthermore, when the obtained features are used in subsequent learning tasks, sparsity potentially leads to better generalization error [5].\nGiven a real n\u00d7 d data matrix S representing n centered data points supported on d features, the leading sparse principal component of the data set is the sparse vector that maximizes the explained variance:\nx\u22c6 , argmax \u2016x\u20162=1,\u2016x\u20160=s\nx\u22a4Ax, (1)\nwhere A = 1/n \u00b7 S\u22a4S is the d \u00d7 d empirical covariance matrix. The sparsity constraint makes the problem NP-hard and hence computationally intractable in general, and hard to approximate within some small constant [6]. A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].\nIn most practical settings, we tend to go beyond computing a single sparse PC. Contrary to the single-component problem, there has been limited work on computing multiple components. The scarcity is partially attributed to conventional PCA wisdom: multiple components can be computed one-by-one, repeatedly, by solving the single-component sparse PCA problem (1) and deflating the input data to remove information captured by previously extracted components [25]. In fact, the multi-component version of sparse PCA is not uniquely defined in the literature. Different deflation-based approaches can lead to different outputs: extracted components may or may not be orthogonal, while they may have disjoint or overlapping supports [25]. In the statistics literature, where the objective is typically to recover a \u201ctrue\u201d principal subspace, a branch of work has focused on the \u201csubspace row sparsity\u201d [26], an assumption that leads to sparse components all supported on the same set of variables. While in [27], the authors discuss an alternative perspective on the fundamental objective of the sparse PCA problem.\nIn this work, we develop a novel algorithm for the multi-component sparse PCA problem with disjoint supports. Formally, we are interested in finding k components that are s-sparse, have disjoint supports, and jointly maximize the explained variance:\nX\u22c6 , argmax X\u2208Xk\nTr ( X\u22a4AX ) , (2)\nwhere the feasible set is\nXk , { X \u2208 Rd\u00d7k : \u2016Xj\u20162 = 1, \u2016Xj\u20160 = s, supp(Xi) \u2229 supp(Xj) = \u2205, \u2200 j \u2208 [k], i < j } ,\nwith Xj denoting the jth column of X. The number k of the desired components is a user defined parameter and we consider it to be a small constant.\nContrary to the greedy sequential approach that repeatedly uses deflation, our algorithm jointly computes all the vectors in X, and comes with theoretical approximation guarantees. We note that even if one could solve each single-component sparse PCA problem (1) exactly, greedy deflation can be highly suboptimal. We show this through a simple example in Section 7.\nOur Contributions\n1. We develop an algorithm that provably approximates the solution to the sparse PCA problem (2) within a multiplicative factor arbitrarily close to 1. To the best of our knowledge, this is the first algorithm that jointly optimizes multiple components with disjoint supports, provably. Our algorithm is combinatorial; it recasts sparse PCA as multiple instances of bipartite maximum weight matching on graphs determined by the input data.\n2. The computational complexity of our algorithm grows as a low order polynomial in the ambient dimension d, but is exponential in the intrinsic dimension of the input data, i.e., the rank of A. To alleviate the impact of this dependence, our algorithm can be applied on a lowdimensional sketch of the input data to obtain an approximate solution to (2). This extra level of approximation introduces an additional penalty in our theoretical approximation guarantees, which naturally depends on the quality of the sketch and, in turn, the spectral decay of A. We show how these bounds further translate to an additive PTAS (polynomial-time approximation scheme) for sparse PCA. Our additive PTAS outputs an approximate solution with explained variance of at least OPT \u2212 \u01eb \u00b7 s, for any sparsity s \u2208 {1, . . . , n}, any constant error \u01eb > 0 and any k = O(1) number of orthogonal components.1\n1Here, OPT is the explained variance captured by the optimal set of k components that are s sparse and have disjoint supports.\n3. We empirically evaluate our algorithm on real datasets, and compare it against state-of-the-art methods for the single-component sparse PCA problem (1) in conjunction with the appropriate deflation step. In many cases, our algorithm\u2014as a result of jointly optimizing over multiple components\u2014leads to significantly improved results, and outperforms deflation-based approaches."}, {"heading": "2 Sparse PCA through Bipartite Matchings", "text": "Our algorithm approximately solves the constrained maximization (2) on a d \u00d7 d rank-r PSD matrix A within a multiplicative factor arbitrarily close to 1. It operates by recasting the maximization into multiple instances of the bipartite maximum weight matching problem. Each instance ultimately yields a feasible solution: a set of k components that are s-sparse and have disjoint supports. The algorithm examines these solutions, and outputs the one that maximizes the explained variance, i.e., the quadratic objective in (2).\nThe computational complexity of our algorithm grows as a low order polynomial in the ambient dimension d of the input, but exponentially in its rank r. Despite the unfavorable dependence on the rank, it is unlikely that a substantial improvement can be achieved in general [6]. However, decoupling the dependence on the ambient and the intrinsic dimension of the input has an interesting ramification; instead of the original input A, our algorithm can be applied on a low-rank surrogate to obtain an approximate solution, alleviating the dependence on r. We discuss this in Section 3, and present the approximation bound that this allows us to obtain.\nLet A = U\u039bU\u22a4 denote the truncated eigenvalue decomposition of A; \u039b is a diagonal r \u00d7 r whose ith diagonal entry is equal to the ith largest eigenvalue of A, while the columns of U are the corresponding eigenvectors. By the Cauchy-Schwartz inequality, for any x \u2208 Rd,\nx\u22a4Ax = \u2225\u2225\u039b1/2U\u22a4x \u2225\u22252 2 \u2265 \u2329 \u039b1/2U\u22a4x, c \u232a2 , \u2200 c \u2208 Rr : \u2016c\u20162 = 1. (3)\nIn fact, equality in (3) can always be achieved for c colinear to \u039b1/2Ux \u2208 Rr and in turn\nx\u22a4Ax = max c\u2208Sr\u221212\n\u2329 x, U\u039b1/2c \u232a2 ,\nwhere Sr\u221212 denotes the \u21132-unit sphere in r dimensions. More generally, for any X \u2208 Rd\u00d7k,\nTr ( X\u22a4AX ) = k\u2211\nj=1\nXj \u22a4 AXj = max\nC:Cj\u2208Sr\u221212 \u2200j\nk\u2211\nj=1\n\u2329 Xj , U\u039b1/2Cj \u232a2 . (4)\nUnder the variational characterization of the trace objective in (4), the sparse PCA problem (2) can be re-written as a joint maximization over the variables X and C as follows:\nmax X\u2208Xk\nTr ( X\u22a4AX ) = max\nX\u2208Xk max\nC:Cj\u2208Sr\u221212 \u2200j\nk\u2211\nj=1\n\u2329 Xj , U\u039b1/2Cj \u232a2 . (5)\nThe alternative formulation of the sparse PCA problem in (5) takes a step towards decoupling the dependence of the optimization on the ambient and intrinsic dimensions d and r, respectively. The motivation behind the introduction of the auxiliary variable C will become clear in the sequel.\nFor a given C, the value of X \u2208 Xk that maximizes the objective in (5) for that C is\nX\u0302 , argmax X\u2208Xk\nk\u2211\nj=1\n\u2329 Xj,Wj \u232a2 , (6)\nwhere W,U\u039b1/2C is a real d\u00d7 k matrix. The constrained, non-convex maximization (6) plays a central role in our developments. We will later describe a combinatorial O(d \u00b7 (s \u00b7 k)2) procedure to efficiently compute X\u0302, reducing the maximization to an instance of the bipartite maximum weight matching problem. For now, however, let us assume that such a procedure exists.\nLet X\u22c6, C\u22c6 be the pair that attains the maximum in (5); in other words, X\u22c6 is the desired solution to the sparse PCA problem. If the optimal auxiliary variable C\u22c6 was known, then we would be able to recover X\u22c6 by solving the maximization (6) for C = C\u22c6. Of course, C\u22c6 is not known, and it is not possible to exhaustively consider all possible values in the domain of C. Instead, we examine only a finite number of possible values of C over a fine discretization of its domain. In particular, let N\u01eb/2(Sr\u221212 ) denote a finite \u01eb/2-net of the r-dimensional \u21132-unit sphere; for any point in Sr\u221212 , the net contains a point within an \u01eb/2 radius from the former. There are several ways to construct such a net [28]. Further, let [N\u01eb/2(Sr\u221212 )]\u2297k \u2282 Rd\u00d7k denote the kth Cartesian power of the aforementioned \u01eb/2-net. By construction, this collection of points contains a matrix C that is column-wise close to C\u22c6. In turn, it can be shown using the properties of the net, that the candidate solution X \u2208 Xk obtained through (6) at that point C will be approximately as good as the optimal X\u22c6 in terms of the quadratic objective in (2).\nAlgorithm 1 Sparse PCA (Multiple disjoint components)\ninput : PSD d\u00d7 d rank-r matrix A, \u01eb \u2208 (0, 1), k \u2208 Z+. output : X \u2208 Xk {Theorem 1} 1: C \u2190 {} 2: [U,\u039b] \u2190 EIG(A) 3: for each C \u2208 [N\u01eb/2(Sr\u221212 )]\u2297k do 4: W \u2190 U\u039b1/2C {W \u2208 Rd\u00d7k} 5: X\u0302 \u2190 argmaxX\u2208Xk \u2211k j=1 \u2329 Xj ,Wj\n\u232a2 {Alg. 2} 6: C \u2190 C \u222a { X\u0302 } 7: end for 8: X \u2190 argmax X\u2208C Tr ( X\u22a4AX ) All above observations yield a procedure for approximately solving the sparse PCA problem (2). The steps are outlined in Algorithm 1. Given the desired number of components k and an accuracy parameter \u01eb \u2208 (0, 1), the algorithm generates a net [N\u01eb/2(Sr\u221212 )]\u2297k and iterates over its points. At each point C, it computes a feasible solution for the sparse PCA problem \u2013 a set of k ssparse components \u2013 by solving the maximization in (6) via a procedure (Alg. 2) that will be described in the sequel. The algorithm collects the candidate solutions identified at the points of the net. The best among them achieves an objective in (2) that provably lies close to optimal. More formally,\nTheorem 1. For any real d\u00d7 d rank-r PSD matrix A, desired number of components k, number s of nonzero entries per component, and accuracy parameter \u01eb \u2208 (0, 1), Algorithm 1 outputs X \u2208 Xk such that\nTr ( X \u22a4 AX ) \u2265 (1\u2212 \u01eb) \u00b7Tr ( X\u22a4\u22c6 AX\u22c6 ) ,\nwhere X\u22c6, argmaxX\u2208Xk Tr ( X\u22a4AX ) , in time TSVD(r) +O (( 4 \u01eb )r\u00b7k \u00b7 d \u00b7 (s \u00b7 k)2 ) .\nAlgorithm 1 is the first nontrivial algorithm that provably approximates the solution of the sparse PCA problem (2). According to Theorem 1, it achieves an objective value that lies within a multiplicative factor from the optimal, arbitrarily close to 1. Its complexity grows as a low-order polynomial in the dimension d of the input, but exponentially in the intrinsic dimension r. Note, however, that it can be exponentially faster compared to the O(ds\u00b7k) brute force approach that exhaustively considers all candidate supports for the k sparse components. The complexity of our algorithm follows from the cardinality of the net and the complexity of Algorithm 2, the subroutine that solves the constrained maximization (6). The latter is a key ingredient of our algorithm, and is discussed in detail in the next subsection. A formal proof of Theorem 1 is provided in Section 9.2."}, {"heading": "2.1 Sparse Components via Bipartite Matchings", "text": "In the core of Algorithm 1 lies Algorithm 2, a procedure that solves the constrained maximization in (6). The algorithm breaks down the maximization into two stages. First, it identifies the support of the optimal solution X\u0302. Determining the support reduces to an instance of the maximum matching problem on a weighted bipartite graph G. Then, it recovers the exact values of the nonzero entries in X\u0302 based on the Cauchy-Schwarz inequality. In the sequel, we provide a brief description of Algorithm 2, leading up to its guarantees in Lemma 2.1.\nLet Ij,supp(X\u0302j) be the support of the jth column of X\u0302, j = 1, . . . , k. The objective in (6) becomes\nk\u2211\nj=1\n\u2329 X\u0302j ,Wj \u232a2 = k\u2211\nj=1\n(\u2211\ni\u2208Ij\nX\u0302ij \u00b7Wij )2 \u2264 k\u2211\nj=1\n\u2211\ni\u2208Ij\nW 2ij . (7)\nThe last inequality is an application of the Cauchy-Schwarz Inequality and the constraint \u2016Xj\u20162 = 1 \u2200 j \u2208 {1, . . . , k}. In fact, if an oracle reveals the supports Ij, j = 1, . . . , k, the upper bound in (7) can always be achieved by setting the nonzero entries of X\u0302 as in Algorithm 2 (Line 6). Therefore, the key in solving (6) is determining the collection of supports to maximize the right-hand side of (7).\nBy constraint, the sets Ij must be pairwise disjoint, each with cardinality s. Consider a weighted bipartite graph G = ( U = {U1, . . . , Uk}, V,E ) constructed as follows2 (Fig. 1):\n\u2022 V is a set of d vertices v1, . . . , vd, corresponding to the d variables, i.e., the d rows of X\u0302.\n\u2022 U is a set of k \u00b7 s vertices, conceptually partitioned into k disjoint subsets U1, . . . , Uk, each of cardinality s. The jth subset, Uj , is associated with the support Ij; the s vertices u(j)\u03b1 , \u03b1 = 1, . . . , s in Uj serve as placeholders for the variables/indices in Ij.\n\u2022 Finally, the edge set is E = U \u00d7 V . The edge weights are determined by the d\u00d7k matrixW in (6). In particular, the weight of edge (u(j)\u03b1 , vi) is equal to W 2ij . Note that all vertices in Uj are effectively identical; they all share a common neighborhood and edge weights.\nAny feasible support {Ij}kj=1 corresponds to a perfect matching in G and vice-versa. Recall that a matching is a subset of the edges containing no two edges incident to the same vertex, while a perfect matching, in the case of an unbalanced bipartite graph G = (U, V,E) with |U | \u2264 |V |, is a matching that contains at least one incident edge for each vertex in U . Given a perfect matching M \u2286 E, the disjoint neighborhoods of Ujs under M yield a support {Ij}kj=1. Conversely, any valid support yields a unique perfect matching in G (taking into account that all vertices in Uj are isomorphic). Moreover, due to the choice of weights in G, the right-hand side of (7) for a given support {Ij}kj=1 is equal to the weight of the matching M in G induced by the former, i.e.,\n2The construction is formally outlined in Algorithm 4 in Section 8.\nAlgorithm 2 Compute Candidate Solution input Real d\u00d7 k matrix W output X\u0302 = argmaxX\u2208Xk \u2211k j=1 \u2329 Xj ,Wj \u232a2\n1: G ( {Uj}kj=1, V,E ) \u2190 GenBiGraph(W) {Alg. 4} 2: M \u2190 MaxWeightMatch(G) {\u2282 E} 3: X\u0302 \u2190 0d\u00d7k 4: for j = 1, . . . , k do 5: Ij \u2190 {i \u2208 {1, . . . , d} : (u, vi) \u2208 M, u \u2208 Uj} 6: [X\u0302j ]Ij \u2190 [Wj ]Ij/\u2016[Wj ]Ij\u20162 7: end for\n\u2211k j=1 \u2211 i\u2208Ij W 2ij= \u2211\n(u,v)\u2208M w(u, v). It follows that determining the support of the solution in (6), reduces to solving the maximum weight matching problem on the bipartite graph G.\nAlgorithm 2 readily follows. Given W \u2208 Rd\u00d7k, the algorithm generates a weighted bipartite graph G as described, and computes its maximum weight matching. Based on the latter, it first recovers the desired support of X\u0302 (Line 5), and subsequently the exact values of its nonzero entries (Line 6). The running time is dominated by the computation of the matching, which can be done in O ( |E||U |+ |U |2 log |U | ) using a variant of the Hungarian algorithm [29]. Hence, Lemma 2.1. For any W \u2208 Rd\u00d7k, Algorithm 2 computes the solution to (6), in time O ( d \u00b7 (s \u00b7 k)2 ) .\nA more formal analysis and proof of Lemma 2.1 is available in Section 9.1. With Algorithm 2 and Lemma 2.1 in place, we complete the description of our sparse PCA algorithm (Algorithm 1) and the proof sketch of Theorem 1."}, {"heading": "3 Sparse PCA on Low-Dimensional Sketches", "text": "Algorithm 3 Sparse PCA on Low Dim. Sketch input : Real n\u00d7 d S, r \u2208 Z+, \u01eb \u2208 (0, 1), k \u2208 Z+. output X(r) \u2208 Xk. {Thm. 2} 1: S \u2190 Sketch(S, r) 2: A \u2190 S\u22a4S 3: X(r) \u2190 Algorithm 1 (A, \u01eb, k). Algorithm 1 approximately solves the sparse PCA problem (2) on a d \u00d7 d rank-r PSD matrix A, in time that grows as a low-order polynomial in the ambient dimension d, but depends exponentially on r. This dependence can be prohibitive in practice. To mitigate its effect, instead of the original input, we can apply our sparse PCA algorithm on a low-rank approximation of A. Intuitively, the quality of the extracted components should depend on how well that low-rank surrogate approximates the original input.\nMore formally, let S be the real n \u00d7 d data matrix representing n (potentially centered) datapoints in d variables, and A the corresponding d \u00d7 d covariance matrix. Further, let S be a low-dimensional sketch of the original data; an n \u00d7 d matrix whose rows lie in an r-dimensional subspace, with r being an accuracy parameter. Such a sketch can be obtained in several ways, including for example exact or approximate SVD, or online sketching methods [30]. Finally, let A = 1/n \u00b7 S\u22a4S be the covariance matrix of the sketched data. Then, instead of A, we can approximately solve the sparse PCA problem by applying Algorithm 1 on the low-rank surrogate A. The above are formally outlined in Algorithm 3. We note that the covariance matrix A does not need to be explicitly computed; Algorithm 1 can operate directly on the (sketched) input data matrix.\nTheorem 2. For any n \u00d7 d input data matrix S, with corresponding empirical covariance matrix A = 1/n \u00b7 S\u22a4S, any desired number of components k, and accuracy parameters \u01eb \u2208 (0, 1) and r, Algorithm 3 outputs X(r) \u2208 Xk such that\nTr ( X\u22a4(r)AX(r) ) \u2265 (1\u2212 \u01eb) \u00b7Tr ( X\u22a4\u22c6 AX\u22c6 ) \u2212 2 \u00b7 k \u00b7 \u03bb1,s(A\u2212A),\nin time TSKETCH(r) + TSVD(r) + O (( 4 \u01eb )r\u00b7k \u00b7 d \u00b7 (s \u00b7 k)2 ) . Here, X\u22c6, argmaxX\u2208Xk Tr ( X\u22a4AX ) , and \u03bb1,s(A) denotes the sparse eigenvalue, i.e., the eigenvalue that corresponds to the principal s-sparse eigenvector of A.\nThe error \u03bb1,s(A\u2212A) and in turn the tightness of the approximation guarantees hinges on the quality of the sketch A. Higher values of the parameter r (the rank of the sketch) can allow for a more accurate solution and tighter guarantees. That is the case, for example, when the sketch is obtained through exact SVD. In that sense, Theorem 2 establishes a natural trade-off between the running time of Algorithm 3 and the quality of the approximation guarantees. A formal proof of Theorem 2 is provided in Section 9.3. Observe that the error term itself is a sparse eigenvalue that is hard to approximate, however even loose bounds provide tight conditional approximation results, as we see next.\nUsing the main matrix approximation result of [31], the next theorem establishes that Algorithm 3 can be turned into an additive PTAS.\nTheorem 3. Let A be a d \u00d7 d positive semidefinite matrix with entries in [\u22121, 1], V be a d \u00d7 d matrix such that A = VV\u22a4. Further, let R be a random d \u00d7 r matrix with entries drawn i.i.d. according to N (0, 1/r), and define\nA,VRR\u22a4V\u22a4.\nFor any constant \u01eb \u2208 (0, 1], let r = O(\u01eb\u22122 log d). Then, for any desired sparsity s, and number of components k = O(1), Algorithm 1 with input argument A and accuracy parameter \u01eb, outputs X(r) \u2208 Xk such that\nTr ( X\u22a4(r)AX(r) ) \u2265 Tr ( X\u22a4\u22c6 AX\u22c6 ) \u2212 \u01eb \u00b7 s\nwith probability at least 1\u2212 1/poly(d), in time nO(log(1/\u01eb)/\u01eb2)).\nRemark 3.1. Note that \u03bb1(A\u2212A) serves as another elementary upper bound on \u03bb1,s(A\u2212A). If A is a the rank-d SVD approximation of A, then\u2014similar to [32]\u2014we can obtain a multiplicative PTAS for sparse PCA, under the assumption of a decaying spectrum (e.g., under a power-law decay), and for s = \u2126(n)."}, {"heading": "4 Related Work", "text": "We are not aware of any algorithm with provable guarantees for sparse PCA with disjoint supports. Multiple components can be extracted by repeatedly solving (1) using one of the aforementioned methods. To ensure disjoint supports, variables \u201cselected\u201d by a component are removed from the dataset. This greedy approach, however, can result in highly suboptimal objective value (See example in Sec. 7).\nA significant volume of work has focused on the single-component sparse PCA problem (1); we scratch the surface and refer the reader to citations therein. Representative examples range from early heuristics in [2], to the LASSO based techniques in [3], the elastic net \u21131-regression in [4], \u21131 and \u21130 regularized optimization methods such as GPower in [7], a greedy branch-and-bound\ntechnique in [8], or semidefinite programming approaches [9, 10, 11]. The authors of [13] present an approach that uses ideas from an expectation-maximization (EM) formulation of the problem. More recently, [12] presents a simple and very efficient truncated version of the power iteration (TPower). Finally, [15] introduces an exact solver for the low-rank case of the problem; this solver was then used on low-rank sketches in the work of [14] (SpanSPCA), that provides conditional approximation guarantees under spectral assumptions on the input data. Several ideas in this work are inspired by the aforementioned low-rank solvers. In our experiments, we compare against EM, TPower, and SpanSPCA, which all are experimentally achieving state-of-the-art performance.\nParallel to the algorithmic and optimization perspective, there is large line of statistical analysis for sparse PCA that focuses on guarantees pertaining to planted models and the recovery of a \u201ctrue\u201d sparse component [16, 17, 18, 19, 20, 21, 22, 23, 24].\nThere has been some work on the explicit estimation of principal subspaces or multiple components under sparsity constraints. Non-deflation-based algorithms include extensions of the diagonal thresholding algorithm [33] and iterative thresholding approaches [17], while [34] and [35] propose methods that rely on the \u201crow sparsity for subspaces\u201d assumption of [26]. These methods yield components supported on a common set of variables, and hence solve a problem different from (2). Magdon-Ismail and Boutsidis [27] discuss the multiple component Sparse PCA problem, propose an alternative objective function and for that problem obtain interesting theoretical guarantees. Finally, [36] develops a framework for sparse matrix factorizaiton problems, based on a novel atomic norm. That framework captures sparse PCA \u2013 although not explicitly the constraint of disjoint supports \u2013 but the resulting optimization problem, albeit convex, is NP-hard."}, {"heading": "5 Experiments", "text": "We evaluate our algorithm on a series of real datasets, and compare it to deflation-based approaches for sparse PCA using TPower [12], EM [13], and SpanSPCA [14]. The latter are representative of the state of the art for the single-component sparse PCA problem (1). Multiple components are computed one by one. To ensure disjoint supports, the deflation step effectively amounts to removing from the dataset all variables used by previously extracted components. For algorithms that are randomly initialized, we depict best results over multiple random restarts. Additional experimental results are listed in Section 11 of the appendix.\nOur experiments are conducted in a Matlab environment. Due to its nature, our algorithm is easily parallelizable; its prototypical implementation utilizes the Parallel Pool Matlab feature to exploit multicore (or distributed cluster) capabilities. Recall that our algorithm operates on a low-rank approximation of the input data. Unless otherwise specified, it is configured for a rank-4 approximation obtained via truncated SVD. Finally, we put a time barrier in the execution of our algorithm, at the cost of the theoretical approximation guarantees; the algorithm returns best results at the time of termination. This \u201cearly termination\u201d can only hurt the performance of our algorithm.\nLeukemia Dataset. We evaluate our algorithm on the Leukemia dataset [37]. The dataset comprises 72 samples, each consisting of expression values for 12582 probe sets. We extract k = 5 sparse components, each active on s = 50 features. In Fig. 2(a), we plot the cumulative explained variance versus the number of components. Deflation-based approaches are greedy: the leading components capture high values of variance, but subsequent ones contribute less. On the contrary, our algorithm jointly optimizes the k = 5 components and achieves higher total cumulative variance; one cannot identify a top component. We repeat the experiment for multiple values of k. Fig. 2(b) depicts the total cumulative variance capture by each method, for each value of k.\nAdditional Datasets. We repeat the experiment on multiple datasets, arbitrarily selected from [37]. Table 1 lists the total cumulative variance captured by k = 5 components, each with s = 40 nonzero entries, extracted using the four methods. Our algorithm achieves the highest values in most cases.\nBag of Words (BoW) Dataset. [37] This is a collection of text corpora stored under the \u201cbag-of-words\u201d model. For each text corpus, a vocabulary of d words is extracted upon tokenization, and the removal of stopwords and words appearing fewer than ten times in total. Each document is then represented as a vector in that d-dimensional space, with the ith entry corresponding to the number of appearances of the ith vocabulary entry in the document.\nWe solve the sparse PCA problem (2) on the word-by-word cooccurrence matrix, and extract k = 8 sparse components, each with cardinality s = 10. We note that the latter is not explicitly constructed; our algorithm can operate directly on the input word-by-document matrix. Table 2 lists the variance captured by each method; our algorithm consistently outperforms the other\napproaches. Finally, note that here each sparse component effectively selects a small set of words. In turn, the k extracted components can be interpreted as a set of well-separated topics. In Table 3, we list the topics extracted from the NY Times corpus (part of the Bag of Words dataset). The corpus consists of 3 \u00b7 105 news articles and a vocabulary of d = 102660 words."}, {"heading": "6 Conclusions", "text": "We considered the sparse PCA problem for multiple components with disjoint supports. Existing methods for the single component problem can be used along with an appropriate deflation step to compute multiple components one by one, leading to potentially suboptimal results. We presented a novel algorithm for jointly optimizing multiple sparse and disjoint components with provable approximation guarantees. Our algorithm is combinatorial and exploits interesting connections between the sparse PCA and the bipartite maximum weight matching problems. It runs in time that grows as a low-order polynomial in the ambient dimension of the input data, but depends exponentially on its rank. To alleviate this dependency, we can apply the algorithm on a lowdimensional sketch of the input, at the cost of an additional error in our theoretical approximation guarantees. Empirical evaluation of our algorithm demonstrated that in many cases it outperforms deflation-based approaches."}, {"heading": "Acknowledgments", "text": "DP is generously supported by NSF awards CCF-1217058 and CCF-1116404 and MURI AFOSR grant 556016. This research has been supported by NSF Grants CCF 1344179, 1344364, 1407278, 1422549 and ARO YIP W911NF-14-1-0258."}, {"heading": "7 On the sub-optimality of deflation \u2013 An example", "text": "We provide a simple example demonstrating the sub-optimality of deflation based approaches for computing multiple sparse components with disjoint supports. Consider the real 4\u00d7 4 matrix\nA =   1 0 0 \u01eb 0 \u03b4 0 0 0 0 \u03b4 0\n\u01eb 0 0 1\n  ,\nwith \u01eb, \u03b4 > 0 such that \u01eb+ \u03b4 < 1. Note that A is PSD; A = B\u22a4B for\nB =   1 0 0 \u01eb 0 \u221a \u03b4 0 0 0 0 \u221a \u03b4 0\n0 0 0 \u221a 1\u2212 \u01eb2\n  .\nWe seek two 2-sparse components with disjoint supports, i.e., the solution to\nmax X\u2208X\n2\u2211\nj=1\nx\u22a4j Axj, (8)\nwhere\nX, { X \u2208 R4\u00d72 : \u2016xi\u20162 \u2264 1, \u2016xi\u20160 \u2264 2 \u2200 i \u2208 {1, 2}, supp(x1) \u2229 supp(x2) = \u2205 } .\nIterative computation with deflation. Following an iterative, greedy procedure with a deflation step, we compute one component at the time. The first component is\nx1 = argmax \u2016x\u20160=2,\u2016x\u20162=1\nx\u22a4Ax. (9)\nRecall that for any unit norm vector x with support I = supp(x),\nx\u22a4Ax \u2264 \u03bbmax (AI,I) , (10)\nwhere AI,I denotes the principal submatrix of A formed by the rows and columns indexed by I. Equality can be achieved in (10) for x equal to the leading eigenvector of AI,I . Hence, it suffices to determine the optimal support for x1. Due to the small size of the example, it is easy to determine that the set I1 = {1, 4} maximizes the objective in (10) over all sets of two indices, achieving value\nx\u22a41 Ax1 = \u03bbmax\n([ 1 \u01eb\n\u01eb 1\n]) = 1 + \u01eb. (11)\nSince subsequent components must have disjoint supports, it follows that the support of the second 2-sparse component x2 is I2 = {2, 3}, and x2 achieves value\nx\u22a42 Ax2 = \u03bbmax\n([ \u03b4 0\n0 \u03b4\n]) = \u03b4. (12)\nIn total, the objective value in (8) achieved by the greedy computation with a deflation step is\n2\u2211\nj=1\nx\u22a4j Axj = 1 + \u01eb+ \u03b4. (13)\nThe sub-optimality of deflation. Consider an alternative pair of 2-sparse components x\u20321 and x \u2032 2 with support sets I \u20321 = {1, 2} and I \u20322 = {3, 4}, respectively. Based on the above, such a pair achieves objective value in (8) equal to\n\u03bbmax\n([ 1 0\n0 \u03b4\n]) + \u03bbmax ([ \u03b4 0\n0 1\n]) = 1 + 1 = 2,\nwhich clearly outperforms the objective value in (13) (under the assumption \u01eb + \u03b4 < 1), demonstrating the sub-optimality of the x1, x2 pair computed by the deflation-based approach. In fact, for small \u01eb, \u03b4 the objective value in the second case is larger than the former by almost a factor of two."}, {"heading": "8 Construction of Bipartite Graph", "text": "The following algorithm formally outlines the steps for generating the bipartite graphG = ( {Uj}kj=1, V,E ) given a weight d\u00d7 k matrix W.\nAlgorithm 4 Generate Bipartite Graph input Real d\u00d7 k matrix W output Bipartite G = ( {Uj}kj=1, V,E ) {Fig. 1}\n1: for j = 1, . . . , k do 2: Uj \u2190 { u (j) 1 , . . . , u (j) s } 3: end for 4: U \u2190 \u222akj=1Uj {|U | = k \u00b7 s} 5: V \u2190 { 1, . . . , d } 6: E \u2190 U \u00d7 V 7: for i = 1, . . . , d do 8: for j = 1, . . . , k do 9: for each u \u2208 Uj do\n10: w ( u, vi ) \u2190 W 2ij 11: end for 12: end for 13: end for"}, {"heading": "9 Proofs", "text": ""}, {"heading": "9.1 Guarantees of Algorithm 2", "text": "Lemma 2.1. For any real d\u00d7 k matrix W, and Algorithm 2 outputs\nX\u0303 = argmax X\u2208Xk\nk\u2211\nj=1\n\u2329 Xj,Wj \u232a2 (14)\nin time O ( d \u00b7 (s \u00b7 k)2 ) .\nProof. Consider a matrix X \u2208 Xk and let Ij , j = 1 . . . , k denote the support sets of its columns. By the constraints in Xk, those sets are disjoint, i.e., Ij1 \u2229 Ij2 = \u2205 \u2200j1, j2 \u2208 {1, . . . , k}, j1 6= j2, and\nk\u2211\nj=1\n\u2329 Xj , Wj \u232a2 = k\u2211\nj=1\n(\u2211\ni\u2208Ij\nXij \u00b7Wij )2 \u2264 k\u2211\nj=1\n(\u2211\ni\u2208Ij\nW 2ij\n) . (15)\nThe last inequality is due to Cauchy-Schwarz and the fact that \u2016Xj\u20162 \u2264 1, \u2200 j \u2208 {1, . . . , k}. In fact, if the supports sets Ij, j = 1, . . . , k were known, the upper bound in (15) would be achieved by setting XjIj = W j Ij /\u2016WjIj\u20162, i.e., setting the nonzero subvector of the jth column of X colinear to the corresponding subvector of the jth column of W. Hence, the key step towards computing the optimal solution X\u0303 is to determine the support sets Ij , j = 1, . . . , k of its columns.\nConsider the set of binary matrices\nZ, { Z \u2208 {0, 1}d\u00d7k : \u2016Zj\u20160 \u2264 s \u2200 j \u2208 [k], supp(Zi) \u2229 supp(Zj) = \u2205 \u2200 i, j \u2208 [k], i 6= j } .\nThe set represents all possible supports for the members of Xk. Taking into account the previous discussion, the maximization in (14) can be written with respect to Z \u2208 Z:\nmax X\u2208Xk\nk\u2211\nj=1\n\u2329 Xj, Wj \u232a2 = max\nZ\u2208Z\nk\u2211\nj=1\nd\u2211\ni=1\nZijW 2 ij . (16)\nLet Z\u0303 \u2208 Z denote the optimal solution, which corresponds to the (support) indicator of X\u0303. Next, we show that computing Z\u0303 boils down to solving a maximum weight matching problem on the bipartite graph generated by Algorithm 4. Recall that given W \u2208 Rd\u00d7k, Algorithm 4 generates a complete weighted bipartite graph G = (U, V,E) where\n\u2022 V is a set of d vertices v1, . . . , vd, corresponding to the d variables, i.e., the d rows of X\u0302. \u2022 U is a set of k \u00b7 s vertices, conceptually partitioned into k disjoint subsets U1, . . . , Uk, each of\ncardinality s. The jth subset, Uj, is associated with the support Ij; the s vertices u(j)\u03b1 , \u03b1 = 1, . . . , s in Uj serve as placeholders for the variables/indices in Ij. \u2022 Finally, the edge set is E = U \u00d7 V . The edge weights are determined by the d \u00d7 k matrix W in (6). In particular, the weight of edge (u(j)\u03b1 , vi) is equal to W 2 ij . Note that all vertices in Uj are\neffectively identical; they all share a common neighborhood and edge weights.\nIt is straightforward to verify that any Z \u2208 Z corresponds to a perfect matching in G and vice versa; Zij = 1 if and only if vertex vi \u2208 V is matched with a vertex in Uj (all vertices in Uj are equivalent with respect to their neighborhood). Further, the objective value in (16) for a given Z \u2208 Z is equal to the weight of the corresponding matching in G. More formally,\n\u2022 Given a perfect matching M, the support Ij of the jth column of Z is determined by the neighborhood of Uj in the matching:\nIj \u2190 { i \u2208 [d] : (u, vi) \u2208 M, u \u2208 Uj } , j = 1, . . . , k. (17)\nNote that the sets Ij , j = 1, . . . , k are indeed disjoint, and each has cardinality equal to s. The weight of the matching M is\n\u2211\n(u,v)\u2208M\nw(u, v) =\nk\u2211\nj=1\n\u2211\n(u,vi)\u2208M: u\u2208Uj\nw(u, vi) =\nk\u2211\nj=1\n\u2211\ni\u2208Ij\nW 2ij =\nk\u2211\nj=1\nd\u2211\ni=1\nZij \u00b7W 2ij, (18)\nwhich is equal to the objective function in (16). \u2022 Conversely, given an indicator matrix Z \u2208 Z, let Ij,supp(Zj), and let Ij(\u03b1) denote the \u03b1th\nelement in the set, \u03b1 = 1, . . . , s (with an arbitrary ordering). Then,\nM = { (u(j)\u03b1 , vIj(\u03b1)), \u03b1 = 1, . . . , s, j = 1, . . . , k } \u2282 E\nis a perfect matching in G. The objective value achieved by Z is equal to the weight of M: k\u2211\nj=1\nd\u2211\ni=1\nZij \u00b7W 2ij = k\u2211\nj=1\n\u2211\ni\u2208Ij\nW 2ij =\nk\u2211\nj=1\ns\u2211\n\u03b1=1\nW 2Ij(\u03b1),j = \u2211\n(u,v)\u2208M\nw(u, v). (19)\nIt follows from (18) and (19) that to determine Z\u0303, it suffices to compute a maximum weight perfect matching in G. The desired support is then obtained as described in (17) (lines 4-7 of Algorithm 2). This complete the proof of correctness of Algorithm 2 which proceeds in the steps described above to determine the support of X\u0303.\nThe weighted bipartite graph G is generated in O(d \u00b7(s \u00b7k)). The running time of Algorithm 2 is dominated by computing the maximum weight matching of G. For the case of unbalanced bipartite graph with |U | = s \u00b7 k < d = |V | the Hungarian algorithm can be modified [29] to compute the maximum weight bipartite matching in time O ( |E||U |+ |U |2 log |U | ) = O ( d \u00b7 (s \u00b7 k)2 ) . This completes the proof."}, {"heading": "9.2 Guarantees of Algorithm 1 \u2013 Proof of Theorem 1", "text": "We first prove a more general version of Theorem 1 for arbitrary constraint sets. Combining that with the guarantees of Algorithm 2, we prove the Theorem 1.\nLemma 9.2. For any real d \u00d7 d rank-r PSD matrix A and arbitrary set X \u2282 Rd\u00d7k, let X\u22c6, argmaxX\u2208X Tr ( X\u22a4AX ) . Assuming that there exists an operator PX : R\nd\u00d7k \u2192 X such that PX (W), argmaxX\u2208X \u2329 xj , wj \u232a2 , then Algorithm 1 outputs X \u2208 X such that\nTr ( X \u22a4 AX ) \u2265 (1\u2212 \u01eb) \u00b7Tr ( X\u22a4\u22c6 AX\u22c6 ) ,\nin time TSVD(r)+O (( 4 \u01eb )r\u00b7k \u00b7 ( TX +kd )) , where TX is the time required to compute PX (\u00b7) and TSVD(r) the time required to compute the truncated SVD of A.\nProof. Let A = U\u039bU \u22a4 denote the truncated eigenvalue decomposition of A; \u039b is a diagonal r\u00d7 r whose ith diagonal entry \u039bii is equal to the ith largest eigenvalue of A, while the columns of U contain the corresponding eigenvectors. By the Cauchy-Schwartz inequality, for any x \u2208 Rd,\nx\u22a4Ax = \u2225\u2225\u039b1/2U\u22a4x \u2225\u22252 2 \u2265 \u2329 \u039b 1/2 U \u22a4 x, c \u232a2 , \u2200 c \u2208 Rr : \u2016c\u20162 = 1. (20)\nIn fact, equality in (20) is achieved for c colinear to \u039b 1/2 Ux, and hence,\nx\u22a4Ax = max c\u2208Sr\u221212\n\u2329 \u039b 1/2 U \u22a4 x, c \u232a2 . (21)\nIn turn,\nTr ( X\u22a4AX ) = k\u2211\nj=1\nXj \u22a4 AXj = max\nC:Cj\u2208Sr\u221212 \u2200j\nk\u2211\nj=1\n\u2329 \u039b 1/2 U \u22a4 Xj , Cj \u232a2 . (22)\nRecall that X\u22c6 is the optimal solution of the trace maximization on A, i.e.,\nX\u22c6, argmax X\u2208X Tr\n( X\u22a4AX ) .\nLet C\u22c6 be the maximizing value of C in (22) for X = X\u22c6, i.e., C\u22c6 is an r\u00d7k matrix with unit-norm columns such that for all j \u2208 {1, . . . , k},\nXj\u22c6 \u22a4 AXj\u22c6 = \u2329 \u039b 1/2 U \u22a4 Xj\u22c6, C j \u22c6 \u232a2 . (23)\nAlgorithm 1 iterates over the points (r \u00d7 k matrices) C in N\u2297k\u01eb/2 ( S r\u22121 2 ) , the kth cartesian power of a finite \u01eb/2-net of the r-dimensional l2-unit sphere. At each such point C, it computes a candidate\nX\u0303 = argmax X\u2208X\nk\u2211\nj=1\n\u2329 Xj,U\u039b1/2Cj \u232a2\nvia Algorithm 2 (See Lemma 9.1 for the guarantees of Algorithm 2). By construction, the set N\u2297k\u01eb/2 ( S r\u22121 2 ) contains a C\u266f such that\n\u2016C\u266f \u2212C\u22c6\u2016\u221e,2 = max j\u2208{1,...,k} \u2016Cj\u266f \u2212Cj\u22c6\u20162 \u2264 \u01eb/2. (24)\nBased on the above, for all j \u2208 {1, . . . , k}, ( Xj\u22c6 \u22a4 AXj\u22c6 )1/2 = \u2223\u2223\u2329\u039b1/2U\u22a4Xj\u22c6, Cj\u22c6 \u232a\u2223\u2223\n= \u2223\u2223\u2329\u039b1/2U\u22a4Xj\u22c6, Cj\u266f \u232a + \u2329 \u039b 1/2 U \u22a4 Xj\u22c6, ( Cj\u22c6 \u2212Cj\u266f )\u232a\u2223\u2223 \u2264 \u2223\u2223\u2329\u039b1/2U\u22a4Xj\u22c6, Cj\u266f \u232a\u2223\u2223+ \u2223\u2223\u2329\u039b1/2U\u22a4Xj\u22c6, ( Cj\u22c6 \u2212Cj\u266f )\u232a\u2223\u2223 \u2264 \u2223\u2223\u2329\u039b1/2U\u22a4Xj\u22c6, Cj\u266f \u232a\u2223\u2223+ \u2225\u2225\u039b1/2U\u22a4Xj\u22c6 \u2225\u2225 \u00b7 \u2225\u2225Cj\u22c6 \u2212Cj\u266f \u2225\u2225 \u2264 \u2223\u2223\u2329\u039b1/2U\u22a4Xj\u22c6, Cj\u266f \u232a\u2223\u2223+ (\u01eb/2) \u00b7 ( Xj\u22c6 \u22a4 AXj\u22c6 )1/2 . (25)\nThe first step follows by the definition of C\u22c6, the second by the linearity of the inner product, the third by the triangle inequality, the fourth by Cauchy-Schwarz inequality and the last by (24). Rearranging the terms in (25),\n\u2223\u2223\u2329\u039b1/2U\u22a4Xj\u22c6, Cj\u266f \u232a\u2223\u2223 \u2265 ( 1\u2212 \u01eb2 ) \u00b7 ( Xj\u22c6 \u22a4 AXj\u22c6 )1/2 \u2265 0,\nand in turn,\n\u2329 \u039b 1/2 U \u22a4 Xj\u22c6, C j \u266f \u232a2 \u2265 ( 1\u2212 \u01eb2 )2 \u00b7Xj\u22c6 \u22a4 AXj\u22c6 \u2265 (1\u2212 \u01eb) \u00b7Xj\u22c6 \u22a4 AXj\u22c6 (26)\nSumming the terms in (26) over all j \u2208 {1, . . . , k}, k\u2211\nj=1\n\u2329 \u039b 1/2 U\n\u22a4 Xj\u22c6, C j \u266f \u232a2 \u2265 (1\u2212 \u01eb) \u00b7Tr ( X\u22a4\u22c6 AX\u22c6 ) . (27)\nLet X\u266f \u2208 X be the candidate solution produced by the algorithm at C\u266f, i.e.,\nX\u266f, argmax X\u2208X\nk\u2211\nj=1\n\u2329 xj, U\u039b 1/2 Cj\u266f \u232a2 . (28)\nThen,\nTr ( X\u22a4\u266f AX\u266f ) (\u03b1) = max\nC:Cj\u2208Sr\u221212 \u2200j\nk\u2211\nj=1\n\u2329 \u039b 1/2 U\n\u22a4 Xj\u266f , C\nj \u232a2\n(\u03b2) \u2265 k\u2211\nj=1\n\u2329 \u039b 1/2 U\n\u22a4 Xj\u266f , C j \u266f\n\u232a2\n(\u03b3) \u2265 k\u2211\nj=1\n\u2329 Xj\u22c6, U\u039b 1/2 Cj\u266f \u232a2\n(\u03b4) \u2265 (1\u2212 \u01eb) \u00b7Tr ( X\u22a4\u22c6 AX\u22c6 ) , (29)\nwhere (\u03b1) follows from the observation in (22), (\u03b2) from the sub-optimality of C\u266f, (\u03b3) by the definition of X\u266f in (28), while (\u03b4) follows from (27). According to (29), at least one of the candidate solutions produced by Algorithm 1, namely X\u266f, achieves an objective value within a multiplicative factor (1\u2212 \u01eb) from the optimal, implying the guarantees of the lemma.\nFinally, the running time of Algorithm 1 follows immediately from the cost per iteration and the cardinality of the \u01eb/2-net on the unit-sphere. Note that matrix multiplications can exploit the singular value decomposition which is performed once.\nTheorem 1. For any real d\u00d7 d rank-r PSD matrix A, desired number of components k, number s of nonzero entries per component, and accuracy parameter \u01eb \u2208 (0, 1), Algorithm 1 outputs X \u2208 Xk such that\nTr ( X \u22a4 AX ) \u2265 (1\u2212 \u01eb) \u00b7Tr ( X\u22a4\u22c6 AX\u22c6 ) ,\nwhere X\u22c6, argmaxX\u2208Xk Tr ( X\u22a4AX ) , in time TSVD(r) +O (( 4 \u01eb )r\u00b7k \u00b7 d \u00b7 (s \u00b7 k)2 ) . TSVD(r) is the time required to compute the truncated SVD of A.\nProof. Recall that Xk is the set of d\u00d7 k matrices X whose columns have unit length and pairwise disjoint supports. Algorithm 2, given any W \u2208 Rd\u00d7k, computes X \u2208 Xk that optimally solves the constrained maximization in line 5. (See Lemma 9.1 for the guarantee of Algorithm 2). in time O ( d \u00b7 (s \u00b7 k)2 ) . The desired result then follows by Lemma 9.2 for the constrained set Xk."}, {"heading": "9.3 Guarantees of Algorithm 3 \u2013 Proof of Theorem 2", "text": "We prove Theorem 2 with the approximation guarantees of Algorithm 3.\nLemma 9.3. For any d\u00d7 d PSD matrices A and A, and any set X \u2286 Rd\u00d7k let\nX\u22c6, argmax X\u2208X Tr\n( X\u22a4AX ) , and X\u22c6, argmax\nX\u2208X Tr\n( X\u22a4AX ) .\nThen, for any X \u2208 X such that Tr ( X \u22a4 AX ) \u2265 \u03b3 \u00b7Tr ( X\u22a4\u22c6 AX\u22c6 ) for some 0 < \u03b3 < 1,\nTr ( X \u22a4 AX ) \u2265 \u03b3 \u00b7Tr ( X\u22a4\u22c6 AX\u22c6 ) \u2212 2 \u00b7 \u2016A\u2212A\u20162 \u00b7max\nX\u2208X \u2016X\u20162F.\nProof. By the optimality of X\u22c6 for A,\nTr ( X\u22a4\u22c6 AX\u22c6 ) \u2265 Tr ( X\u22a4\u22c6 AX\u22c6 ) .\nIn turn, for any X \u2208 X such that Tr ( X \u22a4 AX ) \u2265 \u03b3 \u00b7Tr ( X\u22a4\u22c6 AX\u22c6 ) for some 0 < \u03b3 < 1,\nTr ( X \u22a4 AX ) \u2265 \u03b3 \u00b7Tr ( X\u22a4\u22c6 AX\u22c6 ) . (30)\nLet E,A\u2212A. By the linearity of the trace,\nTr ( X \u22a4 AX ) = Tr ( X \u22a4 AX ) \u2212Tr ( X \u22a4 EX )\n\u2264 Tr ( X \u22a4 AX ) + \u2223\u2223Tr ( X \u22a4 EX )\u2223\u2223. (31)\nBy Lemma 10.10,\n\u2223\u2223Tr ( X \u22a4 EX )\u2223\u2223 \u2264 \u2016X\u2016F \u00b7 \u2016X\u2016F \u00b7 \u2016E\u20162 \u2264 \u2016E\u20162 \u00b7max X\u2208X \u2016X\u20162F , R. (32)\nContinuing from (31),\nTr ( X \u22a4 AX ) \u2264 Tr ( X \u22a4 AX ) +R. (33)\nSimilarly,\nTr ( X\u22a4\u22c6 AX\u22c6 ) = Tr ( X\u22a4\u22c6 AX\u22c6 ) \u2212Tr ( X\u22a4\u22c6 EX\u22c6 )\n\u2265 Tr ( X\u22a4\u22c6 AX\u22c6 ) \u2212 \u2223\u2223Tr ( X\u22a4\u22c6 EX\u22c6 )\u2223\u2223 \u2265 Tr ( X\u22a4\u22c6 AX\u22c6 ) \u2212R. (34)\nCombining the above, we have\nTr ( X \u22a4 AX ) \u2265 Tr ( X \u22a4 AX ) \u2212R\n\u2265 \u03b3 \u00b7Tr ( X\u22a4\u22c6 AX\u22c6 ) \u2212R \u2265 \u03b3 \u00b7 ( Tr ( X\u22a4\u22c6 AX\u22c6 ) \u2212R ) \u2212R = \u03b3 \u00b7Tr ( X\u22a4\u22c6 AX\u22c6 ) \u2212 (1 + \u03b3) \u00b7R \u2265 \u03b3 \u00b7Tr ( X\u22a4\u22c6 AX\u22c6 ) \u2212 2 \u00b7R,\nwhere the first inequality follows from (33) the second from (30), the third from (34), and the last from the fact that R \u2265 0 and 0 < \u03b3 \u2264 1. This concludes the proof.\nRemark 9.2. If in Lemma 9.3 the PSD matrices A and A \u2208 Rd\u00d7d are such that A \u2212A is also PSD, then the following tighter bound holds:\nTr ( X \u22a4 AX ) \u2265 \u03b3 \u00b7Tr ( X\u22a4\u22c6 AX\u22c6 ) \u2212 k\u2211\ni=1\n\u03bbi ( A\u2212A ) .\nProof. This follows from the fact that if E,A\u2212A is PSD, then\nTr ( X \u22a4 EX ) = d\u2211\nj=1\nx\u22a4j Exj \u2265 0,\nand the bound in (31) can be improved to\nTr ( X \u22a4 AX ) = Tr ( X \u22a4 AX ) \u2212Tr ( X \u22a4 EX ) \u2264 Tr ( X \u22a4 AX ) .\nFurther, by Lemma 10.11, the bound in (32) can be improved to\nTr ( X \u22a4 EX ) \u2264 k\u2211\ni=1\n\u03bbi ( E ) , R.\nThe rest of the proof follows as is.\nTheorem 2. For any n \u00d7 d input data matrix S, with corresponding empirical covariance matrix A = 1/n \u00b7 S\u22a4S, any desired number of components k, and accuracy parameters \u01eb \u2208 (0, 1) and r, Algorithm 3 outputs X(r) \u2208 Xk such that\nTr ( X\u22a4(r)AX(r) ) \u2265 (1\u2212 \u01eb) \u00b7Tr ( X\u22a4\u22c6 AX\u22c6 ) \u2212 2 \u00b7 k \u00b7 \u2016A\u2212A\u20162,\nwhere X\u22c6, argmaxX\u2208Xk Tr ( X\u22a4AX ) , in time TSKETCH(r) + TSVD(r) +O (( 4 \u01eb )r\u00b7k \u00b7 d \u00b7 (s \u00b7 k)2 ) .\nProof. The theorem follows from Lemma 9.3 and the approximation guarantees of Algorithm 1."}, {"heading": "9.4 Proof of Theorem 3", "text": "First, we restate and prove the following Lemma by [31].\nLemma 9.4. Let A \u2208 Rd\u00d7d be an positive semidefinite matrix with entries in [\u22121, 1], and V \u2208 Rd\u00d7d matrix such that A = VV\u22a4. Consider a random matrix R \u2208 Rd\u00d7r with entries drawn according to a Gaussian distribution N(0, 1/r), and define\nA = VRR\u22a4V\u22a4.\nThen, for r = O(\u01eb\u22122 log d),\n\u2223\u2223[A]i,j \u2212 [A]i,j \u2223\u2223 \u2264 \u01eb\nfor all i, j with probability at least 1\u2212 1/d.\nProof. The proof relies on the Johnson-Lindenstrauss (JL) Lemma [38], according to which for any two unit norm vectors x,y \u2208 Rd and R generated as described\nPr { |x\u22a4RR\u22a4y \u2212 x\u22a4y| \u2265 \u01eb } \u2264 2 \u00b7 e\u2212(\u01eb2\u2212\u01eb3)\u00b7r/4.\nObserve that each element of A is in [\u22121, 1], hence can be rewritten as an inner product of two unit-norm vectors:\n[A]i,j = V T :,iV:,j.\nSetting r = O(\u01eb\u22122 log d) and using the JL lemma and a union bound over all O(d2) vector pairs V:,i, V:,j we obtain the desired result.\nNext, we provide the proof of Theorem 3 for the simple case of k = 1; the proof easily generalizes to the multi-component case k > 1. According to Lemma 9.4, choosing d = O ( (\u03b4/6)\u22122 log n ) = O ( \u03b4\u22122 log n ) suffices for all entries of A constructed as described in the lemma to satisfiy\n\u2223\u2223[A]i,j \u2212 [A]i,j \u2223\u2223 \u2264 \u03b4\n6\nwith probability at least 1\u2212 1/d. In turn, for any s-sparse, unit-norm x,\n\u2223\u2223x\u22a4Ax\u2212 x\u22a4Ax \u2223\u2223 = \u2223\u2223\u2223\u2223\u2223\u2223 \u2211\ni,j\nxixj([A]ij \u2212 [A]ij) \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 \u03b4 6 \u00b7 \u2223\u2223\u2223\u2223\u2223\u2223 n\u2211\ni=1\n|xi| n\u2211\nj=1\n|xj| \u2223\u2223\u2223\u2223\u2223\u2223\n\u2264 \u03b4 6 \u00b7 \u2016x\u201621 \u2264 \u03b4 6 \u00b7 (\u221a s \u00b7 \u2016x\u20162 )2 = \u03b4 6 \u00b7 s, (35)\nwhere the second inequality follows from the fact that x is s-sparse and unit norm. We run Algorithm 1 (for k = 1) with input argument the rank-r matrix A, desired sparsity s and accuracy parameter \u01eb = \u03b4/6. Algorithm 1 outputs a s-sparse unit-norm vector x\u0302 which according to Theorem 1 satisfies\n(1\u2212 \u03b4/6) \u00b7 xd\u22a4Axd \u2264 x\u0302\u22a4Ax\u0302 \u2264 xd\u22a4Axd, (36)\nwhere xd is the true s-sparse principal component of A. This, in turn, implies that x\u0302 satisfies\n\u2223\u2223\u2223x\u0302\u22a4Ax\u0302\u2212 xd\u22a4Axd \u2223\u2223\u2223 \u2264 \u03b4\n6 \u00b7 xd\u22a4Axd \u2264\n\u03b4\n6\n( 1 + \u03b4\n6\n) s \u2264 \u03b4\n3 \u00b7 s, (37)\nwhere the second inequality follows from the fact that the entries of A lie in [\u22121\u2212 \u03b46 , 1 + \u03b46 ] and x\u0302 is s-sparse and unit-norm.\nIn the following, we bound the difference of the performance of x\u0302 on the original matrix A from the optimal value. Let x\u22c6 denote the s-sparse principal component of A and define\nOPT,x\u22c6 TAx\u22c6.\nThen,\n|OPT\u2212 x\u0302\u22a4Ax\u0302| = |x\u22a4\u22c6 Ax\u22c6 \u2212 x\u0302\u22a4Ax\u0302| = |x\u22a4\u22c6 Ax\u22c6 \u2212 x\u22a4d Axd + xd\u22a4Axd \u2212 x\u0302\u22a4Ax\u0302| \u2264 |x\u22a4\u22c6 Ax\u22c6 \u2212 xd\u22a4Axd|\ufe38 \ufe37\ufe37 \ufe38\nA\n+ |xd\u22a4Axd \u2212 x\u0302\u22a4Ax\u0302|\ufe38 \ufe37\ufe37 \ufe38 B . (38)\nUtilizing (35) and the triangle inequality, one can verify that\nA = |x\u22c6\u22a4Ax\u22c6 \u2212 xd\u22a4Axd + xd\u22a4Axd \u2212 xd\u22a4Axd| \u2264 |x\u22c6\u22a4Ax\u22c6 \u2212 xd\u22a4Axd|+ |xd\u22a4Axd \u2212 xd\u22a4Axd|\n\u2264 x\u22c6\u22a4Ax\u22c6 \u2212 xd\u22a4Axd\ufe38 \ufe37\ufe37 \ufe38 \u22650\n+ \u03b4\n6 \u00b7 s\n\u2264 x\u22c6\u22a4Ax\u22c6 \u2212 xd\u22a4Axd + \u03b4\n6 \u00b7 s + xd\u22a4Axd \u2212 x\u22c6\u22a4Ax\u22c6\ufe38 \ufe37\ufe37 \ufe38\n\u22650\n\u2264 x\u22c6\u22a4Ax\u22c6 \u2212 x\u22c6\u22a4Ax\u22c6 + \u03b4\n6 \u00b7 s + xd\u22a4Axd \u2212 xd\u22a4Axd\n\u2264 |x\u22c6\u22a4Ax\u22c6 \u2212 x\u22c6\u22a4Ax\u22c6|+ \u03b4\n6 \u00b7 s + |xd\u22a4Axd \u2212 xd\u22a4Axd|\n\u2264 \u03b4 2 \u00b7 s. (39)\nSimilarly,\nB = \u2223\u2223\u2223xd\u22a4Axd \u2212 x\u0302\u22a4Ax\u0302+ x\u0302\u22a4Ax\u0302\u2212 x\u0302\u22a4Ax\u0302 \u2223\u2223\u2223\n= \u2223\u2223\u2223xd\u22a4Axd \u2212 x\u0302\u22a4Ax\u0302 \u2223\u2223\u2223+ \u2223\u2223\u2223x\u0302\u22a4Ax\u0302\u2212 x\u0302\u22a4Ax\u0302 \u2223\u2223\u2223 \u2264 \u2223\u2223\u2223xd\u22a4Axd \u2212 x\u0302\u22a4Ax\u0302\n\u2223\u2223\u2223+ \u03b4 6 \u00b7 s\n(\u03b1) \u2264 2\u03b4 6 \u00b7 s+ \u03b4 6 \u00b7 s \u2264 \u03b4 2 \u00b7 s. (40)\nwhere (\u03b1) follows from (37). Continuing from (38), combining (39) and (40) we obtain\n|OPT \u2212x\u0302\u22a4Ax\u0302 \u2223\u2223\u2223 \u2264 \u03b4 \u00b7 s,\nwhich is the desired result."}, {"heading": "10 Auxiliary Technical Lemmata", "text": "Lemma 10.5. For any real d\u00d7 n matrix M, and any r, k \u2264 min{d, n}, r+k\u2211\ni=r+1\n\u03c3i(M) \u2264 k\u221a r + k \u00b7 \u2016M\u2016F,\nwhere \u03c3i(M) is the ith largest singular value of M.\nProof. By the Cauchy-Schwartz inequality,\nr+k\u2211\ni=r+1\n\u03c3i(M) =\nr+k\u2211\ni=r+1\n|\u03c3i(M)| \u2264 ( r+k\u2211\ni=r+1\n\u03c32i (M) )1/2 \u00b7 \u20161k\u20162 = \u221a k \u00b7 ( r+k\u2211\ni=r+1\n\u03c32i (M)\n)1/2 .\nNote that \u03c3r+1(M), . . . , \u03c3r+k(M) are the k smallest among the r+k largest singular values. Hence,\nr+k\u2211\ni=r+1\n\u03c32i (M) \u2264 k\nr + k\nr+k\u2211\ni=1\n\u03c32i (M) \u2264 k\nr + k\nmin{d,n}\u2211\ni=1\n\u03c32i (M) = k\nr + k \u2016M\u20162F.\nCombining the two inequalities, the desired result follows.\nCorollary 1. For any real d\u00d7 n matrix M and k \u2264 min{d, n}, \u03c3k(M) \u2264 k\u22121/2 \u00b7 \u2016M\u2016F.\nProof. It follows immediately from Lemma 10.5.\nLemma 10.6. Let a1, . . . , an and b1, . . . , bn be 2n real numbers and let p and q be two numbers such that 1/p+ 1/q = 1 and p > 1. We have\n\u2223\u2223 n\u2211\ni=1\naibi \u2223\u2223 \u2264\n( n\u2211\ni=1\n|ai|p )1/p \u00b7 ( n\u2211\ni=1\n|bi|q )1/q .\nLemma 10.7. For any two real matrices A and B of appropriate dimensions,\n\u2016AB\u2016F \u2264 min{\u2016A\u20162\u2016B\u2016F, \u2016A\u2016F\u2016B\u20162} .\nProof. Let bi denote the ith column of B. Then,\n\u2016AB\u20162F = \u2211\ni\n\u2016Abi\u201622 \u2264 \u2211\ni\n\u2016A\u201622\u2016bi\u201622 = \u2016A\u201622 \u2211\ni\n\u2016bi\u201622 = \u2016A\u201622\u2016B\u20162F.\nSimilarly, using the previous inequality,\n\u2016AB\u20162F = \u2016B\u22a4A\u22a4\u20162F \u2264 \u2016B\u22a4\u201622\u2016A\u22a4\u20162F = \u2016B\u201622\u2016A\u20162F.\nCombining the two upper bounds, the desired result follows.\nLemma 10.8. For any A,B \u2208 Rn\u00d7k, \u2223\u2223\u3008A,B\u3009 \u2223\u2223, \u2223\u2223Tr ( A\u22a4B )\u2223\u2223 \u2264 \u2016A\u2016F\u2016B\u2016F.\nProof. The inequality follows from Lemma 10.6 for p = q = 2, treating A and B as vectors.\nLemma 10.9. For any real m\u00d7 n matrix A, and any k \u2264 min{m, n},\nmax Y\u2208Rn\u00d7k Y \u22a4 Y=Ik\n\u2016AY\u2016F = ( k\u2211\ni=1\n\u03c32i (A)\n)1/2 .\nThe maximum is attained by Y coinciding with the k leading right singular vectors of A.\nProof. Let U\u03a3V\u22a4 be the singular value decomposition of A; U and V are m \u00d7 m and n \u00d7 n unitary matrices respectively, while \u03a3 is a diagonal matrix with \u03a3jj = \u03c3j, the jth largest singular value of A, j = 1, . . . , d, where d,min{m,n}. Due to the invariance of the Frobenius norm under unitary multiplication,\n\u2016AY\u20162F = \u2016U\u03a3V\u22a4Y\u20162F = \u2016\u03a3V\u22a4Y\u20162F. (41)\nContinuing from (41),\n\u2016\u03a3V\u22a4Y\u20162F = Tr ( Y\u22a4V\u03a32V\u22a4Y ) = k\u2211\ni=1\ny\u22a4i\n( d\u2211\nj=1\n\u03c32j \u00b7 vjv\u22a4j ) yi = d\u2211\nj=1\n\u03c32j \u00b7 k\u2211\ni=1\n( v\u22a4j yi )2 .\nLet zj, \u2211k\ni=1 ( v\u22a4j yi )2 , j = 1, . . . , d. Note that each individual zj satisfies\n0 \u2264 zj, k\u2211\ni=1\n( v\u22a4j yi )2 \u2264 \u2016vj\u20162 = 1,\nwhere the last inequality follows from the fact that the columns of Y are orthonormal. Further,\nd\u2211\nj=1\nzj =\nd\u2211\nj=1\nk\u2211\ni=1\n( v\u22a4j yi )2 = k\u2211\ni=1\nd\u2211\nj=1\n( v\u22a4j yi )2 = k\u2211\ni=1\n\u2016yi\u20162 = k.\nCombining the above, we conclude that\n\u2016AY\u20162F = d\u2211\nj=1\n\u03c32j \u00b7 zj \u2264 \u03c321 + . . .+ \u03c32k. (42)\nFinally, it is straightforward to verify that if yi = vi, i = 1, . . . , k, then (42) holds with equality.\nLemma 10.10. For any real d\u00d7n matrix A, and pair of d\u00d7 k matrix X and n\u00d7 k matrix Y such that X\u22a4X = Ik and Y \u22a4Y = Ik with k \u2264 min{d, n}, the following holds:\n\u2223\u2223Tr ( X\u22a4AY )\u2223\u2223 \u2264 \u221a k \u00b7 ( k\u2211\ni=1\n\u03c32i (A) )1/2 .\nProof. By Lemma 10.8,\n|\u3008X, AY\u3009| = \u2223\u2223Tr ( X\u22a4AY )\u2223\u2223 \u2264 \u2016X\u2016F \u00b7 \u2016AY\u2016F = \u221a k \u00b7 \u2016AY\u2016F.\nwhere the last inequality follows from the fact that \u2016X\u20162F = Tr ( X\u22a4X ) = Tr(Ik) = k. Combining with a bound on \u2016AY\u2016F as in Lemma 10.9, completes the proof.\nLemma 10.11. For any real d \u00d7 d PSD matrix A, and k \u00d7 d matrix X with k \u2264 d orthonormal columns,\nTr ( X\u22a4AX ) \u2264 k\u2211\ni=1\n\u03bbi(A)\nwhere \u03bbi(A) is the ith largest eigenvalue of A. Equality is achieved for X coinciding with the k leading eigenvectors of A. Proof. LetA = VV\u22a4 be a factorization of the PSDmatrixA. Then,Tr ( X\u22a4AX ) = Tr ( X\u22a4VV\u22a4X ) = \u2016V\u22a4X\u20162F. The desired result follows by Lemma 10.9 and the fact that \u03bbi(A) = \u03c32i (V), i = 1, . . . , d.\n11 Additional Experimental Results"}], "references": [{"title": "The varimax criterion for analytic rotation in factor analysis", "author": ["H.F. Kaiser"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1958}, {"title": "Rotation of principal components: choice of normalization constraints", "author": ["I.T. Jolliffe"], "venue": "Journal of Applied Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}, {"title": "A modified principal component technique based on the lasso", "author": ["I.T. Jolliffe", "N.T. Trendafilov", "M. Uddin"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Sparse principal component analysis", "author": ["Hui Zou", "Trevor Hastie", "Robert Tibshirani"], "venue": "Journal of computational and graphical statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Sparse features for pca-like linear regression", "author": ["Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "On the worst-case approximability of sparse PCA", "author": ["Siu On Chan", "Dimitris Papailiopoulos", "Aviad Rubinstein"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Generalized power method for sparse principal component analysis", "author": ["M. Journ\u00e9e", "Y. Nesterov", "P. Richt\u00e1rik", "R. Sepulchre"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Spectral bounds for sparse pca", "author": ["B. Moghaddam", "Y. Weiss", "S. Avidan"], "venue": "Exact and greedy algorithms. NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Optimal solutions for sparse principal component analysis", "author": ["Alexandre d\u2019Aspremont", "Francis Bach", "Laurent El Ghaoui"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Sparse pca: Convex relaxations, algorithms and applications", "author": ["Y. Zhang", "A. d\u2019Aspremont", "L.E. Ghaoui"], "venue": "Handbook on Semidefinite, Conic and Polynomial Optimization,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "A direct formulation for sparse pca using semidefinite programming", "author": ["A. d\u2019Aspremont", "L. El Ghaoui", "M.I. Jordan", "G.R.G. Lanckriet"], "venue": "SIAM review,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Truncated power method for sparse eigenvalue problems", "author": ["Xiao-Tong Yuan", "Tong Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Expectation-maximization for sparse and nonnegative pca", "author": ["Christian D. Sigg", "Joachim M. Buhmann"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Sparse pca through low-rank approximations", "author": ["Dimitris Papailiopoulos", "Alexandros Dimakis", "Stavros Korokythakis"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "The sparse principal component of a constant-rank matrix", "author": ["Megasthenis Asteris", "Dimitris S. Papailiopoulos", "Georgios N. Karystinos"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "High-dimensional analysis of semidefinite relaxations for sparse principal components", "author": ["Arash Amini", "Martin Wainwright"], "venue": "In Information Theory,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Sparse principal component analysis and iterative thresholding", "author": ["Zongming Ma"], "venue": "The Annals of Statistics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Approximation bounds for sparse principal component analysis", "author": ["A. d\u2019Aspremont", "F. Bach", "L.E. Ghaoui"], "venue": "arXiv preprint arXiv:1205.0121,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Sparse pca: Optimal rates and adaptive estimation", "author": ["T Tony Cai", "Zongming Ma", "Yihong Wu"], "venue": "arXiv preprint arXiv:1211.1309,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Sparse pca via covariance thresholding", "author": ["Yash Deshpande", "Andrea Montanari"], "venue": "arXiv preprint arXiv:1311.5179,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Optimal detection of sparse principal components in high dimension", "author": ["Quentin Berthet", "Philippe Rigollet"], "venue": "Ann. Statist.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Complexity theoretic lower bounds for sparse principal component detection", "author": ["Q. Berthet", "P. Rigollet"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Statistical and computational trade-offs in estimation of sparse principal components", "author": ["Tengyao Wang", "Quentin Berthet", "Richard J. Samworth"], "venue": "arXiv preprint arXiv:1408.5369,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Do semidefinite relaxations solve sparse PCA up to the information limit", "author": ["Robert Krauthgamer", "Boaz Nadler", "Dan Vilenchik"], "venue": "Annals of Probability,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Deflation methods for sparse pca", "author": ["L. Mackey"], "venue": "NIPS, 21:1017\u20131024,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Minimax rates of estimation for sparse pca in high dimensions", "author": ["Vincent Vu", "Jing Lei"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Optimal sparse linear auto-encoders and sparse PCA", "author": ["Malik Magdon-Ismail", "Christos Boutsidis"], "venue": "CoRR, abs/1502.06626,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "On minimum-cost assignments in unbalanced bipartite graphs", "author": ["Lyle Ramshaw", "Robert E Tarjan"], "venue": "HP Labs, Palo Alto, CA, USA, Tech. Rep. HPL-2012-40R1,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Nathan Halko", "Per-Gunnar Martinsson", "Joel A Tropp"], "venue": "SIAM review,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "The approximate rank of a matrix and its algorithmic applications: approximate rank", "author": ["Noga Alon", "Troy Lee", "Adi Shraibman", "Santosh Vempala"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Nonnegative sparse pca with provable guarantees", "author": ["Megasthenis Asteris", "Dimitris Papailiopoulos", "Alexandros Dimakis"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "On consistency and sparsity for principal components analysis in high dimensions", "author": ["Iain M Johnstone", "Arthur Yu Lu"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "Fantope projection and selection: A near-optimal convex relaxation of sparse pca", "author": ["Vincent Q Vu", "Juhee Cho", "Jing Lei", "Karl Rohe"], "venue": "In NIPS,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Nonconvex statistical optimization: minimaxoptimal sparse pca in polynomial time", "author": ["Zhaoran Wang", "Huanran Lu", "Han Liu"], "venue": "arXiv preprint arXiv:1408.5352,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Tight convex relaxations for sparse matrix factorization", "author": ["Emile Richard", "Guillaume R Obozinski", "Jean-Philippe Vert"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Sparse PCA is a useful variant that offers higher data interpretability [1, 2, 3], a property that is sometimes desired even at the cost of statistical fidelity [4].", "startOffset": 72, "endOffset": 81}, {"referenceID": 1, "context": "Sparse PCA is a useful variant that offers higher data interpretability [1, 2, 3], a property that is sometimes desired even at the cost of statistical fidelity [4].", "startOffset": 72, "endOffset": 81}, {"referenceID": 2, "context": "Sparse PCA is a useful variant that offers higher data interpretability [1, 2, 3], a property that is sometimes desired even at the cost of statistical fidelity [4].", "startOffset": 72, "endOffset": 81}, {"referenceID": 3, "context": "Sparse PCA is a useful variant that offers higher data interpretability [1, 2, 3], a property that is sometimes desired even at the cost of statistical fidelity [4].", "startOffset": 161, "endOffset": 164}, {"referenceID": 4, "context": "Furthermore, when the obtained features are used in subsequent learning tasks, sparsity potentially leads to better generalization error [5].", "startOffset": 137, "endOffset": 140}, {"referenceID": 5, "context": "The sparsity constraint makes the problem NP-hard and hence computationally intractable in general, and hard to approximate within some small constant [6].", "startOffset": 151, "endOffset": 154}, {"referenceID": 1, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 2, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 3, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 6, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 7, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 8, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 9, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 10, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 11, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 12, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 13, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 14, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 153}, {"referenceID": 15, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 253, "endOffset": 289}, {"referenceID": 16, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 253, "endOffset": 289}, {"referenceID": 17, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 253, "endOffset": 289}, {"referenceID": 18, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 253, "endOffset": 289}, {"referenceID": 19, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 253, "endOffset": 289}, {"referenceID": 20, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 253, "endOffset": 289}, {"referenceID": 21, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 253, "endOffset": 289}, {"referenceID": 22, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 253, "endOffset": 289}, {"referenceID": 23, "context": "A significant volume of prior work has focused on algorithms that approximately solve the optimization problem [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15], while a large volume of theoretical results has been established under planted statistical models [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 253, "endOffset": 289}, {"referenceID": 24, "context": "The scarcity is partially attributed to conventional PCA wisdom: multiple components can be computed one-by-one, repeatedly, by solving the single-component sparse PCA problem (1) and deflating the input data to remove information captured by previously extracted components [25].", "startOffset": 275, "endOffset": 279}, {"referenceID": 24, "context": "Different deflation-based approaches can lead to different outputs: extracted components may or may not be orthogonal, while they may have disjoint or overlapping supports [25].", "startOffset": 172, "endOffset": 176}, {"referenceID": 25, "context": "In the statistics literature, where the objective is typically to recover a \u201ctrue\u201d principal subspace, a branch of work has focused on the \u201csubspace row sparsity\u201d [26], an assumption that leads to sparse components all supported on the same set of variables.", "startOffset": 163, "endOffset": 167}, {"referenceID": 26, "context": "While in [27], the authors discuss an alternative perspective on the fundamental objective of the sparse PCA problem.", "startOffset": 9, "endOffset": 13}, {"referenceID": 5, "context": "Despite the unfavorable dependence on the rank, it is unlikely that a substantial improvement can be achieved in general [6].", "startOffset": 121, "endOffset": 124}, {"referenceID": 27, "context": "The running time is dominated by the computation of the matching, which can be done in O ( |E||U |+ |U |2 log |U | ) using a variant of the Hungarian algorithm [29].", "startOffset": 160, "endOffset": 164}, {"referenceID": 28, "context": "Such a sketch can be obtained in several ways, including for example exact or approximate SVD, or online sketching methods [30].", "startOffset": 123, "endOffset": 127}, {"referenceID": 29, "context": "Using the main matrix approximation result of [31], the next theorem establishes that Algorithm 3 can be turned into an additive PTAS.", "startOffset": 46, "endOffset": 50}, {"referenceID": 30, "context": "If A is a the rank-d SVD approximation of A, then\u2014similar to [32]\u2014we can obtain a multiplicative PTAS for sparse PCA, under the assumption of a decaying spectrum (e.", "startOffset": 61, "endOffset": 65}, {"referenceID": 1, "context": "Representative examples range from early heuristics in [2], to the LASSO based techniques in [3], the elastic net l1-regression in [4], l1 and l0 regularized optimization methods such as GPower in [7], a greedy branch-and-bound", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "Representative examples range from early heuristics in [2], to the LASSO based techniques in [3], the elastic net l1-regression in [4], l1 and l0 regularized optimization methods such as GPower in [7], a greedy branch-and-bound", "startOffset": 93, "endOffset": 96}, {"referenceID": 3, "context": "Representative examples range from early heuristics in [2], to the LASSO based techniques in [3], the elastic net l1-regression in [4], l1 and l0 regularized optimization methods such as GPower in [7], a greedy branch-and-bound", "startOffset": 131, "endOffset": 134}, {"referenceID": 6, "context": "Representative examples range from early heuristics in [2], to the LASSO based techniques in [3], the elastic net l1-regression in [4], l1 and l0 regularized optimization methods such as GPower in [7], a greedy branch-and-bound", "startOffset": 197, "endOffset": 200}, {"referenceID": 7, "context": "technique in [8], or semidefinite programming approaches [9, 10, 11].", "startOffset": 13, "endOffset": 16}, {"referenceID": 8, "context": "technique in [8], or semidefinite programming approaches [9, 10, 11].", "startOffset": 57, "endOffset": 68}, {"referenceID": 9, "context": "technique in [8], or semidefinite programming approaches [9, 10, 11].", "startOffset": 57, "endOffset": 68}, {"referenceID": 10, "context": "technique in [8], or semidefinite programming approaches [9, 10, 11].", "startOffset": 57, "endOffset": 68}, {"referenceID": 12, "context": "The authors of [13] present an approach that uses ideas from an expectation-maximization (EM) formulation of the problem.", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "More recently, [12] presents a simple and very efficient truncated version of the power iteration (TPower).", "startOffset": 15, "endOffset": 19}, {"referenceID": 14, "context": "Finally, [15] introduces an exact solver for the low-rank case of the problem; this solver was then used on low-rank sketches in the work of [14] (SpanSPCA), that provides conditional approximation guarantees under spectral assumptions on the input data.", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "Finally, [15] introduces an exact solver for the low-rank case of the problem; this solver was then used on low-rank sketches in the work of [14] (SpanSPCA), that provides conditional approximation guarantees under spectral assumptions on the input data.", "startOffset": 141, "endOffset": 145}, {"referenceID": 15, "context": "Parallel to the algorithmic and optimization perspective, there is large line of statistical analysis for sparse PCA that focuses on guarantees pertaining to planted models and the recovery of a \u201ctrue\u201d sparse component [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 219, "endOffset": 255}, {"referenceID": 16, "context": "Parallel to the algorithmic and optimization perspective, there is large line of statistical analysis for sparse PCA that focuses on guarantees pertaining to planted models and the recovery of a \u201ctrue\u201d sparse component [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 219, "endOffset": 255}, {"referenceID": 17, "context": "Parallel to the algorithmic and optimization perspective, there is large line of statistical analysis for sparse PCA that focuses on guarantees pertaining to planted models and the recovery of a \u201ctrue\u201d sparse component [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 219, "endOffset": 255}, {"referenceID": 18, "context": "Parallel to the algorithmic and optimization perspective, there is large line of statistical analysis for sparse PCA that focuses on guarantees pertaining to planted models and the recovery of a \u201ctrue\u201d sparse component [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 219, "endOffset": 255}, {"referenceID": 19, "context": "Parallel to the algorithmic and optimization perspective, there is large line of statistical analysis for sparse PCA that focuses on guarantees pertaining to planted models and the recovery of a \u201ctrue\u201d sparse component [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 219, "endOffset": 255}, {"referenceID": 20, "context": "Parallel to the algorithmic and optimization perspective, there is large line of statistical analysis for sparse PCA that focuses on guarantees pertaining to planted models and the recovery of a \u201ctrue\u201d sparse component [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 219, "endOffset": 255}, {"referenceID": 21, "context": "Parallel to the algorithmic and optimization perspective, there is large line of statistical analysis for sparse PCA that focuses on guarantees pertaining to planted models and the recovery of a \u201ctrue\u201d sparse component [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 219, "endOffset": 255}, {"referenceID": 22, "context": "Parallel to the algorithmic and optimization perspective, there is large line of statistical analysis for sparse PCA that focuses on guarantees pertaining to planted models and the recovery of a \u201ctrue\u201d sparse component [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 219, "endOffset": 255}, {"referenceID": 23, "context": "Parallel to the algorithmic and optimization perspective, there is large line of statistical analysis for sparse PCA that focuses on guarantees pertaining to planted models and the recovery of a \u201ctrue\u201d sparse component [16, 17, 18, 19, 20, 21, 22, 23, 24].", "startOffset": 219, "endOffset": 255}, {"referenceID": 31, "context": "Non-deflation-based algorithms include extensions of the diagonal thresholding algorithm [33] and iterative thresholding approaches [17], while [34] and [35] propose methods that rely on the \u201crow sparsity for subspaces\u201d assumption of [26].", "startOffset": 89, "endOffset": 93}, {"referenceID": 16, "context": "Non-deflation-based algorithms include extensions of the diagonal thresholding algorithm [33] and iterative thresholding approaches [17], while [34] and [35] propose methods that rely on the \u201crow sparsity for subspaces\u201d assumption of [26].", "startOffset": 132, "endOffset": 136}, {"referenceID": 32, "context": "Non-deflation-based algorithms include extensions of the diagonal thresholding algorithm [33] and iterative thresholding approaches [17], while [34] and [35] propose methods that rely on the \u201crow sparsity for subspaces\u201d assumption of [26].", "startOffset": 144, "endOffset": 148}, {"referenceID": 33, "context": "Non-deflation-based algorithms include extensions of the diagonal thresholding algorithm [33] and iterative thresholding approaches [17], while [34] and [35] propose methods that rely on the \u201crow sparsity for subspaces\u201d assumption of [26].", "startOffset": 153, "endOffset": 157}, {"referenceID": 25, "context": "Non-deflation-based algorithms include extensions of the diagonal thresholding algorithm [33] and iterative thresholding approaches [17], while [34] and [35] propose methods that rely on the \u201crow sparsity for subspaces\u201d assumption of [26].", "startOffset": 234, "endOffset": 238}, {"referenceID": 26, "context": "Magdon-Ismail and Boutsidis [27] discuss the multiple component Sparse PCA problem, propose an alternative objective function and for that problem obtain interesting theoretical guarantees.", "startOffset": 28, "endOffset": 32}, {"referenceID": 34, "context": "Finally, [36] develops a framework for sparse matrix factorizaiton problems, based on a novel atomic norm.", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "We evaluate our algorithm on a series of real datasets, and compare it to deflation-based approaches for sparse PCA using TPower [12], EM [13], and SpanSPCA [14].", "startOffset": 129, "endOffset": 133}, {"referenceID": 12, "context": "We evaluate our algorithm on a series of real datasets, and compare it to deflation-based approaches for sparse PCA using TPower [12], EM [13], and SpanSPCA [14].", "startOffset": 138, "endOffset": 142}, {"referenceID": 13, "context": "We evaluate our algorithm on a series of real datasets, and compare it to deflation-based approaches for sparse PCA using TPower [12], EM [13], and SpanSPCA [14].", "startOffset": 157, "endOffset": 161}, {"referenceID": 0, "context": "[1] H.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] I.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] I.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Hui Zou, Trevor Hastie, and Robert Tibshirani.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Siu On Chan, Dimitris Papailiopoulos, and Aviad Rubinstein.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] B.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Alexandre d\u2019Aspremont, Francis Bach, and Laurent El Ghaoui.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Xiao-Tong Yuan and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Christian D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Dimitris Papailiopoulos, Alexandros Dimakis, and Stavros Korokythakis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Megasthenis Asteris, Dimitris S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Arash Amini and Martin Wainwright.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Zongming Ma.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] T Tony Cai, Zongming Ma, and Yihong Wu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Yash Deshpande and Andrea Montanari.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Quentin Berthet and Philippe Rigollet.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Q.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Tengyao Wang, Quentin Berthet, and Richard J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Robert Krauthgamer, Boaz Nadler, and Dan Vilenchik.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Vincent Vu and Jing Lei.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] Malik Magdon-Ismail and Christos Boutsidis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] Lyle Ramshaw and Robert E Tarjan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[30] Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[31] Noga Alon, Troy Lee, Adi Shraibman, and Santosh Vempala.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[32] Megasthenis Asteris, Dimitris Papailiopoulos, and Alexandros Dimakis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[33] Iain M Johnstone and Arthur Yu Lu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34] Vincent Q Vu, Juhee Cho, Jing Lei, and Karl Rohe.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[35] Zhaoran Wang, Huanran Lu, and Han Liu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[36] Emile Richard, Guillaume R Obozinski, and Jean-Philippe Vert.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "For the case of unbalanced bipartite graph with |U | = s \u00b7 k < d = |V | the Hungarian algorithm can be modified [29] to compute the maximum weight bipartite matching in time O ( |E||U |+ |U |2 log |U | ) = O ( d \u00b7 (s \u00b7 k)2 ) .", "startOffset": 112, "endOffset": 116}, {"referenceID": 29, "context": "4 Proof of Theorem 3 First, we restate and prove the following Lemma by [31].", "startOffset": 72, "endOffset": 76}], "year": 2015, "abstractText": "We consider the following multi-component sparse PCA problem: given a set of data points, we seek to extract a small number of sparse components with disjoint supports that jointly capture the maximum possible variance. These components can be computed one by one, repeatedly solving the single-component problem and deflating the input data matrix, but as we show this greedy procedure is suboptimal. We present a novel algorithm for sparse PCA that jointly optimizes multiple disjoint components. The extracted features capture variance that lies within a multiplicative factor arbitrarily close to 1 from the optimal. Our algorithm is combinatorial and computes the desired components by solving multiple instances of the bipartite maximum weight matching problem. Its complexity grows as a low order polynomial in the ambient dimension of the input data matrix, but exponentially in its rank. However, it can be effectively applied on a low-dimensional sketch of the data; this allows us to obtain polynomial-time approximation guarantees via spectral bounds. We evaluate our algorithm on real data-sets and empirically demonstrate that in many cases it outperforms existing, deflationbased approaches.", "creator": "LaTeX with hyperref package"}}}