{"id": "1508.03891", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Aug-2015", "title": "A Refinement-Based Architecture for Knowledge Representation and Reasoning in Robotics", "abstract": "This paper describes an architecture that combines the complementary strengths of probabilistic graphical models and declarative programming to enable robots to represent and reason with qualitative and quantitative descriptions of uncertainty and domain knowledge. The architecture comprises three components: a controller, a logician and an executor. The logician uses a coarse-resolution, qualitative model of the world and logical reasoning to plan a sequence of abstract actions for an assigned goal. For each abstract action to be executed, the executor uses probabilistic information at finer granularity and probabilistic algorithms to execute a sequence of concrete actions, and to report observations of the environment. The controller dispatches goals and relevant information to the logician and executor, collects and processes information from these components, refines the coarse-resolution description to obtain the probabilistic information at finer granularity for each abstract action, and computes the policy for executing the abstract action using probabilistic algorithms. The architecture is evaluated in simulation and on a mobile robot moving objects in an indoor domain, to show that it supports reasoning with violation of defaults, noisy observations and unreliable actions, in complex domains. The architecture is implemented with a set of pre-defined, standardized logicians, and is fully compliant with the code in the source code.", "histories": [["v1", "Mon, 17 Aug 2015 01:17:49 GMT  (2154kb,D)", "https://arxiv.org/abs/1508.03891v1", "27 pages, 9 figures"], ["v2", "Mon, 17 Oct 2016 11:01:57 GMT  (762kb,D)", "http://arxiv.org/abs/1508.03891v2", "47 pages, 13 figures"], ["v3", "Wed, 19 Apr 2017 20:50:13 GMT  (2479kb,D)", "http://arxiv.org/abs/1508.03891v3", "53 pages, 14 figures"]], "COMMENTS": "27 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.LO", "authors": ["mohan sridharan", "michael gelfond", "shiqi zhang", "jeremy wyatt"], "accepted": false, "id": "1508.03891"}, "pdf": {"name": "1508.03891.pdf", "metadata": {"source": "CRF", "title": "A Refinement-Based Architecture for Knowledge Representation and Reasoning in Robotics", "authors": ["Mohan Sridharan", "Michael Gelfond", "Jeremy Wyatt"], "emails": ["m.sridharan@auckland.ac.nz", "michael.gelfond@ttu.edu", "s.zhang9@csuohio.edu", "jlw@cs.bham.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Robots1 are increasingly being used to assist humans in homes, offices and other complex domains. To truly assist humans in such domains, robots need to be re-taskable and robust. We consider a robot to be re-taskable if its reasoning system enables it to achieve a wide range of goals in a wide range of environments. We consider a robot to be robust if it is able to cope with unreliable sensing, unreliable actions, changes in the environment agents, and the existence of atypical environments, by representing and reasoning with different description of knowledge and uncertainty. While there have been many attempts, satisfying these desiderata remains an open research problem.\nRobotics and artificial intelligence researchers have developed many approaches for robot reasoning, drawing on ideas from two very different classes of systems for knowledge representation and reasoning, based on logic and probability theory respectively. Systems based on logic incorporate compositionally structured commonsense knowledge about objects and relations, and support powerful generalization of reasoning to new situations. Systems based on probability reason optimally (or near optimally) about the effects of numerically quantifiable uncertainty in sensing\n1We use the terms \u201crobot\u201d and \u201cagent\u201d interchangeably in this paper.\nar X\niv :1\n50 8.\n03 89\n1v 3\n[ cs\n.R O\nand action. There have been many attempts to combine the benefits of these two classes of systems, including work on joint (i.e., logic-based and probabilistic) representations of state and action, and algorithms for planning and decisionmaking in such formalisms. These approaches provide significant expressive power, but they also impose a significant computational burden. More efficient (and often approximate) reasoning algorithms for such unified probabilisticlogical paradigms are being developed. However, practical robot systems that combine abstract task-level planning with probabilistic reasoning, link, rather than unify, their logic-based and probabilistic representations, primarily because roboticists often need to trade expressivity or correctness guarantees for computational speed. Information close to the sensorimotor level is often represented probabilistically to quantitatively model the uncertainty in sensing and actuation, with the robot\u2019s beliefs including statements such as \u201cthe robotics book is on the shelf with probability 0.9\u201d. At the same time, logic-based systems are used to reason with (more) abstract commonsense knowledge, which may not necessarily be natural or easy to represent probabilistically. This knowledge may include hierarchically organized information about object sorts (e.g., a cookbook is a book), and default information that holds in all but a few exceptional situations (e.g., \u201cbooks are typically found in the library\u201d). These representations are linked, in that the probabilistic reasoning system will periodically commit particular claims about the world being true, with some residual uncertainty, to the logical reasoning system, which then reasons about those claims as if they were true. There are thus languages of different expressive strengths, which are linked within an architecture.\nThe existing work in architectures for robot reasoning has some key limitations. First, many of these systems are driven by the demands of robot systems engineering, and there is little formalization of the corresponding architectures. Second, many systems employ a logical language that is indefeasible, e.g., first order predicate logic, and incorrect commitments can lead to irrecoverable failures. Our proposed architecture addresses these limitations. It represents and reasons about the world, and the robot\u2019s knowledge of it, at two granularities. A fine-resolution description of the domain, close to the data obtained from the robot\u2019s sensors and actuators, is reasoned about probabilistically, while a coarse-resolution description of the domain, including commonsense knowledge, is reasoned about using nonmonotonic logic. Our architecture precisely defines the coupling between the representations at the two granularities, enabling the robot to represent and efficiently reason about commonsense knowledge, what the robot does not know, and how actions change the robot\u2019s knowledge. The interplay between the two types of knowledge is viewed as a conversation between, and the (physical and mental) actions of, a logician and a statistician. Consider, for instance, the following exchange:\nLogician: the goal is to find the robotics book. I do not know where it is, but I know that books are typically in the library and I am in the library. We should first look for the robotics book in the library.\nLogician\u2192 Statistician: look for the robotics book in the library. You only need to reason about the robotics book and the library.\nStatistician: In my representation of the world, the library is a set of grid cells. I shall determine how to locate the book probabilistically in these cells considering the probabilities of movement failures and visual processing failures.\nStatistician: I visually searched for the robotics book in the grid cells of the library, but did not find the book. Although there is a small probability that I missed the book, I am prepared to commit that the robotics book is not in the library.\nStatistician\u2192 Logician: here are my observations from searching the library; the robotics book is not in the library. Logician: the robotics book was not found in the library either because it was not there, or because it was moved to\nanother location. The next default location for books is the bookshelf in the lab. We should go look there next.\nand so on... where the representations used by the logician and the statistician, and the communication of information between them, is coordinated by a controller. This imaginary exchange illustrates key features of our approach:\n\u2022 Reasoning about the states of the domain, and the effects of actions, happens at different levels of granularity, e.g., the logician reasons about rooms, whereas the statistician reasons about grid cells in those rooms.\n\u2022 For any given goal, the logician computes a plan of abstract actions, and each abstract action is executed probabilistically as a sequence of concrete actions planned by the statistician.\n\u2022 The effects of the coarse-resolution (logician\u2019s) actions are non-deterministic, but the statistician\u2019s fine-resolution action effects, and thus the corresponding beliefs, have probabilities associated with them.\n\u2022 The coarse-resolution knowledge base (of the logician) may include knowledge of things that are irrelevant to the current goal. Probabilistic reasoning at fine resolution (by statistician) only considers things deemed relevant to the current coarse-resolution action.\n\u2022 Fine-resolution probabilistic reasoning about observations and actions updates probabilistic beliefs, and highly likely statements (e.g., probability > 0.9) are considered as being completely certain for subsequent coarseresolution reasoning (by the logician)."}, {"heading": "1.1 Technical Contributions", "text": "The design of our architecture is based on tightly-coupled transition diagrams at two levels of granularity. A coarseresolution description includes commonsense knowledge, and the fine-resolution transition diagram is defined as a refinement of the coarse-resolution transition diagram. For any given goal, non-monotonic logical reasoning with the coarse-resolution system description and the system\u2019s recorded history, results in a sequence of abstract actions. Each such abstract action is implemented as a sequence of concrete actions by zooming to a part of the fine-resolution transition diagram relevant to this abstract action, and probabilistically modeling the non-determinism in action outcomes. The technical contributions of this architecture are summarized below.\nAction language extensions. An action language is a formalism used to model action effects, and many action languages have been developed and used in robotics, e.g., STRIPS, PDDL [19], BC [32], and ALd [17]. We extend ALd in two ways to make it more expressive. First, we allow fluents (domain properties that can change) that are nonBoolean, which allows us to compactly model a much wider range of situations. Second, we allow non-deterministic causal laws, which captures the non-deterministic effects of the robot\u2019s actions, not only in probabilistic but also qualitative terms. This extended version of ALd is used to describe the coarse-resolution and fine-resolution transition diagrams of the proposed architecture.\nDefaults, histories and explanations. Our architecture makes three contributions related to reasoning with default knowledge and histories. First, we expand the notion of the history of a dynamic domain, which typically includes a record of actions executed and observations obtained (by the robot), to support the representation of (prioritized) default information. We can, for instance, say that a textbook is typically found in the library and, if it is not there, it is typically found in the auxiliary library. Second, we define the notion of a model of a history with defaults in the initial state, enabling the robot to reason with such defaults. Third, we limit reasoning with such expanded histories to the coarse resolution, and enable the robot to efficiently (a) use default knowledge to compute plans to achieve the desired goal; and (b) reason with history to generate explanations for unexpected observations. For instance, in the absence of knowledge about the locations of a specific object, the robot can construct a plan using the object\u2019s default location to speed up search. Also, the robot can build a revised model of the history to explain subsequent observations that contradict expectations based on initial assumptions.\nTightly-coupled transition diagrams. The next set of contributions are related to the relationship between different models of the domain used by the robot, i.e., the tight coupling between the transition diagrams at two resolutions. First, we provide a formal definition of one transition diagram being a refinement of another, and use this definition to formalize the notion of the coarse-resolution transition diagram being refined to obtain the fine-resolution transition diagram\u2014the fact that both transition diagrams are described in the same language facilitates their construction and this formalization. A coarse-resolution state is, for instance, magnified to provide multiple states at the fineresolution\u2014the corresponding ability to reason about space at two different resolutions is central for scaling to larger environments. We find two resolutions to be practically sufficient for many robot tasks, and leave extensions to other resolutions as an open problem. Second, we define randomization of a fine-resolution transition diagram, replacing deterministic causal laws by non-deterministic ones. Third, we formally define and automate zooming to a part of the\nfine-resolution transition diagram relevant to a specific coarse-resolution transition, allowing the robot, while executing any given abstract action, to avoid considering parts of the fine-resolution diagram irrelevant to this action, e.g., a robot moving between two rooms only considers its location in the cells in those rooms.\nDynamic generation of probabilistic representations. The next set of innovations connect the contributions described so far to quantitative models of action and observation uncertainty. First, we use a semi-supervised algorithm, the randomized fine-resolution transition diagram, prior knowledge (if any), and experimental trials, to collect statistics and compute probabilities of fine-resolution action outcomes and observations. Second, we provide an algorithm that, for any given abstract action, uses these computed probabilities and the zoomed fine-resolution description to automatically construct the data structures for, and thus significantly limit the computational requirements of, probabilistic reasoning. Third, based on the coupling between transition diagrams at the two resolutions, the outcomes of probabilistic reasoning update the coarse-resolution history for subsequent reasoning.\nMethodology and architecture. The final set of contributions are related to the overall architecture. First, for the design of the software components of robots that are re-taskable and robust, we articulate a methodology that is rather general, provides a path for proving correctness of these components, and enables us to predict the robot\u2019s behavior. Second, the proposed knowledge representation and reasoning architecture combines the representation and reasoning methods from action languages, declarative programming, probabilistic state estimation and probabilistic planning, to support reliable and efficient operation. The domain representation for logical reasoning is translated into a program in SPARC [2], an extension of CR-Prolog, and the representation for probabilistic reasoning is translated into a partially observable Markov decision process (POMDP) [27]. CR-Prolog [4] (and thus SPARC) incorporates consistency-restoring rules in Answer Set Prolog (ASP)\u2014in this paper, the terms ASP, CR-Prolog and SPARC are often used interchangeably\u2014and has a close relationship with our action language, allowing us to reason efficiently with hierarchically organized knowledge and default knowledge, and to pose state estimation, planning, and explanation generation within a single framework. Also, using an efficient approximate solver to reason with POMDPs supports a principled and quantifiable trade-off between accuracy and computational efficiency in the presence of uncertainty, and provides a near-optimal solution under certain conditions [27, 37]. Third, our architecture avoids exact, inefficient probabilistic reasoning over the entire fine-resolution representation, while still tightly coupling the reasoning at different resolutions. This intentional separation of non-monotonic logical reasoning and probabilistic reasoning is at the heart of the representational elegance, reliability and inferential efficiency provided by our architecture.\nThe proposed architecture is evaluated in simulation and on a physical robot finding and moving objects in an indoor domain. We show that the architecture enables a robot to reason with violation of defaults, noisy observations, and unreliable actions, in larger, more complex domains, e.g., with more rooms and objects, than was possible before."}, {"heading": "1.2 Structure of the Paper", "text": "The remainder of the paper is organized as follows. Section 2 introduces a domain used as an illustrative example throughout the paper, and Section 3 discusses related work in knowledge representation and reasoning for robots. Section 4 presents the methodology associated with the proposed architecture, and Section 5 introduces definitions of basic notions used to build mathematical models of the domain. Section 5.1 describes the action language used to describe the architecture\u2019s coarse-resolution and fine-resolution transition diagrams, and Section 5.2 introduces histories with initial state defaults as an additional type of record, describes models of system histories, and reduces planning with the coarse-resolution domain representation to computing the answer set of the corresponding ASP program. Section 6 provides the logician\u2019s domain representation base on these definitions. Next, Section 7 describes the (a) refinement of the coarse-resolution transition diagram to obtain the fine-resolution transition diagram; (b) randomization of the fine-resolution system description; (c) collection of statistics to compute the probability of action outcomes and observations; and (d) zooming to the part of the randomized system description relevant to the execution of any given abstract action. Next, Section 8 describes how a POMDP is constructed and solved to obtain a policy that implements the abstract action as a sequence of concrete actions. The overall control loop of the architecture is described in Section 9. Section 10 describes the experimental results in simulation and on a mobile robot, followed by conclusions in Section 11. In what follows, we refer to the functions and abstract actions of the coarse-resolution\ntransition diagram as being \u201chigh level\u201d, using H as the subscript or superscript. Concrete functions and actions of the fine-resolution transition diagram are referred to as being \u201clow level\u201d, using L as the subscript or superscript."}, {"heading": "2 Illustrative Example: Office Domain", "text": "The following domain (with some variants) will be used as an illustrative example throughout the paper.\nExample 1. [Office Domain] Consider a robot that is assigned the goal of moving specific objects to specific places in an office domain. This domain contains:\n\u2022 The sorts: place, thing, robot, and ob ject, with ob ject and robot being subsorts of thing. Sorts textbook, printer and kitchenware, are subsorts of the sort ob ject. Sort names and constants are written in lower-case, while variable names are in uppercase.\n\u2022 Four specific places: o ff ice, main library, aux library, and kitchen. We assume that these places are accessible from each other without the need to navigate any corridors, and that doors between these places are open.\n\u2022 An instance of the sort robot, called rob1. Also, a number of instances of subsorts of the sort ob ject.\nAs an extension of this illustrative example that will be used in the experimental trials on physical robots, consider the robot shown in Figure 1(b) operating in an office building whose map is shown in Figure 1(a). Assume that the robot can (a) build and revise the domain map based on laser range finder data; (b) visually recognize objects of interest; and (c) execute actuation commands, although neither the information extracted from sensor inputs nor the action execution is completely reliable. Next, assume that the robot is in the study corner and is given the goal of fetching the robotics textbook. Since the robot knows that books are typically found in the main library, ASP-based reasoning provides a plan of abstract actions that require the robot to go to the main library, pick up the book and bring it back. For the first abstract action, i.e., for moving to the main library, the robot can focus on just the relevant part of the fine-resolution representation, e.g., the cells through which the robot must pass, but not the robotics book that is irrelevant at this stage of reasoning. It then creates and solves a POMDP for this movement sub-task, and executes a sequence of concrete movement actions until it believes that it has reached the main library with high probability. This information is used to reason at the coarse resolution, prompting the robot to execute the next abstract action to pick up the robotics book. Now, assume that the robot is unable to pick up the robotics book because it fails to find the book in the main library despite a thorough search. This observation violates what the robot expects to see based on default knowledge, but the robot explains this by understanding that the book was not in the main library to begin with, and creates a plan to go to the auxiliary library, the second most likely location for textbooks. In this case, assume that the robot finds the book and completes the task. The proposed architecture enables such robot behavior."}, {"heading": "3 Related Work", "text": "The objective of this paper is to enable robots to represent and reason with logic-based and probabilistic descriptions of domain knowledge and degrees of belief. We review some related work below.\nThere are many recent examples of researchers using probabilistic graphical models such as POMDPs to formulate tasks such as planning, sensing, navigation, and interaction on robots [1, 20, 25, 40]. These formulations, by themselves, are not well-suited for reasoning with commonsense knowledge, e.g., default reasoning and non-monotonic logical reasoning. In parallel, research in classical planning and logic programming has provided many algorithms for knowledge representation and reasoning, which have been used on mobile robots. These algorithms typically require a significant amount of prior knowledge of the domain and the agent\u2019s capabilities, and the preconditions and effects of the actions. Many of these algorithms are based on first-order logic, and do not support capabilities such as nonmonotonic logical reasoning, default reasoning, and the ability to merge new, unreliable information with the current beliefs in a knowledge base. Other logic-based formalisms address some of these limitations. This includes, for instance, theories of reasoning about action and change, as well as Answer Set Prolog (ASP), a non-monotonic logic programming paradigm, which is well-suited for representing and reasoning with commonsense knowledge [8, 18]. An international research community has developed around ASP, with applications in cognitive robotics [15] and other non-robotics domains. For instance, ASP has been used for planning and diagnostics by one or more simulated robot housekeepers [14], and for representation of domain knowledge learned through natural language processing by robots interacting with humans [11]. ASP-based architectures have also been used for the control of unmanned aerial vehicles in dynamic indoor environments [6, 7]. However, ASP does not support quantitative models of uncertainty, whereas a lot of information available to robots is represented probabilistically to quantitatively model the uncertainty in sensor input processing and actuation.\nMany approaches for reasoning about actions and change in robotics and artificial intelligence (AI) are based on action languages, which are formal models of parts of natural language used for describing transition diagrams. There are many different action languages such as STRIPS, PDDL [19], BC [32], and ALd [17], which have been used for different applications [10, 29]. In robotics applications, we often need to represent and reason with recursive state constraints, non-boolean fluents and non-deterministic causal laws. We expanded ALd , which already supports recursive state constraints, to address there requirements. We also expanded the notion of histories to include initial state defaults. Action language BC also supports the desired capabilities but it allows causal laws specifying default values of fluents at arbitrary time steps, and is thus too powerful for our purposes and occasionally poses difficulties with representing all exceptions to such defaults when the domain is expanded.\nRobotics and AI researchers have designed algorithms and architectures based on the understanding that robots interacting with the environment through sensors and actuators need both logical and probabilistic reasoning capabilities. For instance, architectures have been developed to support hierarchical representation of knowledge and axioms in first-order logic, and probabilistic processing of perceptual information [30, 31, 48], while deterministic and probabilistic algorithms have been combined for task and motion planning on robots [28]. Another example is the behavior control of a robot that included semantic maps and commonsense knowledge in a probabilistic relational representation, and then used a continual planner to switch between decision-theoretic and classical planning procedures based on degrees of belief [24]. The performance of such architectures can be sensitive to the choice of threshold for switching between the different planning procedures, and the use of first order logic in these architectures limits the expressiveness and use of commonsense knowledge. More recent work has used a three-layered organization of knowledge (instance, default and diagnostic), with knowledge at the higher level modifying that at the lower levels, and a three-layered architecture (competence layer, belief layer and deliberative layer) for distributed control of information flow, combining first-order logic and probabilistic reasoning for open world planning [23]. Declarative programming has also been combined with continuous-time planners for path planning in mobile robot teams [42]. More recent work has combined a probabilistic extension of ASP with POMDPs for commonsense inference and probabilistic planning in human-robot dialog [54], used a probabilistic extension of ASP to determine some model parameters of POMDPs [50], used ASP-based architecture to support learning of action costs on a robot [29], and combined logic programming and relational reinforcement learning to interactively and cumulatively discover domain axioms [45] and affordances [46].\nCombining logical and probabilistic reasoning is a fundamental problem in AI, and many principled algorithms have been developed to address this problem. For instance, a Markov logic network combines probabilistic graphical\nmodels and first order logic, assigning weights to logic formulas [39]. Bayesian Logic relaxes the unique name constraint of first-order probabilistic languages to provide a compact representation of distributions over varying sets of objects [36]. Other examples include independent choice logic [38], PRISM [21], probabilistic first-order logic [22], first-order relational POMDPs [26, 41], and Plog that assigns probabilities to different possible worlds represented as answer sets of ASP programs [9, 33]. Despite significant prior research, knowledge representation and reasoning for robots collaborating with humans continues to present many open problems. Algorithms based on first-order logic do not support non-monotonic logical reasoning, and do not provide the desired expressiveness for capabilities such as default reasoning\u2014it is not always possible to express degrees of belief and uncertainty quantitatively, e.g., by attaching probabilities to logic statements. Other algorithms based on logic programming do not support one or more of the capabilities such as reasoning about relations as in causal Bayesian networks; incremental addition of probabilistic information; reasoning with large probabilistic components; or dynamic addition of variables to represent open worlds. Our prior work has developed architectures that support different subsets of these capabilities. For instance, we developed an architecture that coupled planning based on a hierarchy of POMDPs [47, 52] with ASPbased inference. The domain knowledge included in the ASP knowledge base of this architecture was incomplete and considered default knowledge, but did not include a model of action effects. ASP-based inference provided priors for POMDP state estimation, and observations and historical data from comparable domains were considered for reasoning about the presence of target objects in the domain [53]. Building on recent work [44, 51], this paper describes a general refinement-based architecture for knowledge representation and reasoning in robotics. The architecture enables robots to represent and reason with descriptions of incomplete domain knowledge and uncertainty at different levels of granularity, tailoring sensing and actuation to tasks to support scaling to larger, complex domains."}, {"heading": "4 Design Methodology", "text": "Our proposed architecture is based on a design methodology. A designer following this methodology will:\n1. Provide a coarse-resolution description of the robot\u2019s domain in action language ALd together with the description of the initial state.\n2. Provide the necessary domain-specific information for, and construct and examine correctness of, the fineresolution refinement of the coarse-resolution description.\n3. Provide domain-specific information and randomize the fine-resolution description of the domain to capture the non-determinism in action execution.\n4. Run experiments and collect statistics to compute probabilities of the outcomes of actions and the reliability of observations.\n5. Provide these components, together with any desired goal, to a reasoning system that directs the robot towards achieving this goal.\nThe reasoning system implements an action loop that can be viewed as an interplay between a logician and statistician (Section 1 and Section 9). In this paper, the reasoning system uses ASP-based non-monotonic logical reasoning, POMDP-based probabilistic reasoning, models and descriptions constructed during the design phase, and records of action execution and observations obtained from the robot. The following sections describe components of the architecture, design methodology steps, and the reasoning system. We first define some basic notions, specifically action description and domain history, which are needed to build mathematical domain models."}, {"heading": "5 Action Language and Histories", "text": "This section first describes extensions to action language ALd to support non-boolean fluents and non-deterministic causal laws (Section 5.1). Next, Section 5.2 expands the notion of the history of a dynamic domain to include initial state defaults, defines models of such histories, and describes how these models can be computed. The subsequent sections describe the use of these models (of action description and history) to provide the coarse-resolution description of the domain, and to build more refined fine-resolution models of the domain.\n5.1 ALd with non-boolean fluents and non-determinism Action languages are formal models of parts of natural language used for describing transition diagrams. In this paper, we extend action language ALd [16, 17, 18] (we preserve the old name for simplicity) to allow non-boolean fluents and non-deterministic causal laws. A system description of ALd consists of a sorted signature containing a collection of basic sorts organized into an inheritance hierarchy, and three special sorts: statics, f luents and actions. Statics are domain properties whose truth values cannot be changed by actions (e.g., locations of walls and doors), fluents are properties whose values can be changed by actions (e.g., location of the robot), and actions are sets of elementary actions that can be executed in parallel. Fluents of ALd are divided into basic and defined. The defined fluents are boolean, do not obey laws of inertia, and are defined in terms of other fluents, whereas basic fluents obey laws of inertia (thus often called inertial fluents in the knowledge representation literature) and are directly changed by actions. There are two types of basic fluents. The first one is related to the physical properties of the domain\u2014fluents of this type can be changed by actions that change the physical state. The second one, called a basic knowledge fluent is changed by knowledge producing actions that only change the agent\u2019s knowledge about the domain. Atoms in ALd are of the form f (x\u0304) = y, where y and elements of x\u0304 are variables or properly typed object constants2\u2014when convenient, we also write this as f (x\u0304,y). If f is boolean, we use the standard notation f (x\u0304) and \u00ac f (x\u0304). Literals are expressions of the form f (x\u0304) = y and f (x\u0304) 6= y. For instance, in our example domain (Example 1), static next to(Pl1,Pl2) says that places Pl1 and Pl2 are next to each other, basic fluent loc(T h) = Pl says that thing T h is located in place Pl, basic fluent in hand(R,Ob) says that robot R is holding object Ob, and action move(R,Pl) moves robot R to place Pl.\nALd allows five types of statements: deterministic causal laws, non-deterministic causal laws, state constraints, definitions, and executability conditions. Deterministic causal laws are of the form:\na causes f (x\u0304) = y if body (1)\nwhere a is an action, f is a basic fluent, and body is a collection of literals. Statement 1 says that if a is executed in a state satisfying body, the value of f in any resulting state would be y. Non-deterministic causal laws are of the form:\na causes f (x\u0304) : {Y : p(Y )} if body (2)\nwhere p is a unary boolean function and p(Y ) is a boolean literal, or:\na causes f (x\u0304) : sort name if body (3)\nwhere Statement 2 says that if a were to be executed in a state satisfying body, f may take on any value from the set {Y : p(Y )}\u2229range( f ) in the resulting state. Statement 3 says that f may take any value from {sort name\u2229range( f )}. If the body of a causal law is empty, the if will be omitted. In the context of Example 1, the deterministic causal law:\nmove(R,Pl) causes loc(R) = Pl\nsays that a robot R moving to place Pl will end up in Pl. Examples of other forms of the causal law are provided later.\nState constraints are of the form: f (x\u0304) = y if body (4)\nwhere f is a basic fluent or static. The state constraint says that f (x\u0304) = y must be true in every state satisfying body. For instance, the constraint:\nloc(Ob) = Pl if loc(R) = Pl, in hand(R,Ob)\nguarantees that the object grasped by a robot shares the robot\u2019s location.\nThe definition of a defined fluent f (x\u0304) is a collection of statements of the form:\nf (x\u0304) if body (5)\n2This representation of relations as functions, in the context of ASP, is based on prior work [5].\nAs in logic programming definitions, f (x\u0304) is true if it follows from the truth of at least one of its defining rules. Otherwise, f (x\u0304) is false.\nExecutability conditions are statements of the form:\nimpossible a0, . . . ,ak if body (6)\nwhich implies that in a state satisfying body, actions a0, . . .ak cannot be executed simultaneously. For instance, the following executability condition:\nimpossible move(R,Pl) if loc(R) = Pl\nimplies that a robot cannot move to a location if it is already there. A collection of statements of ALd forms a system description D . The semantics of D is given by a transition diagram \u03c4(D) whose nodes correspond to possible states of the system. A diagram contains an arc \u3008\u03c31,a,\u03c32\u3009 if, after the execution of action a in a state \u03c31, the system may move into state \u03c32. We define the states and transitions of \u03c4(D) in terms of answer sets of logic programs, as described below; see [16, 18] for more details.\nRecall that an interpretation of the signature of D is an assignment of a value to each f (x\u0304) from the signature. An interpretation can be represented by the collection of atoms of the form f (x\u0304) = y, where y is the value of f (x\u0304). For any interpretation \u03c3 , let \u03c3nd denote the collection of all atoms of \u03c3 formed by basic fluents and statics\u2014\u201dnd\u201d stands for non-defined. Also, let \u03a0c(D), where c stands for constraints, denote the logic program defined as follows:\n1. For every state constraint (Statement 4) and definition (Statement 5), program \u03a0c(D) contains:\nf (x\u0304) = y\u2190 body\n2. For every defined fluent f , \u03a0c(D) contains the closed world assumption (CWA):\n\u00ac f (x\u0304)\u2190 body, not f (x\u0304)\nwhere, unlike classical negation \u201c\u00ac a\u201d that implies \u201ca is believed to be false\u201d, default negation \u201cnot a\u201d only implies that \u201ca is not believed to be true\u201d.\nWe can now define states of \u03c4(D).\nDefinition 1. [State of \u03c4(D)] An interpretation \u03c3 is a state of the transition diagram \u03c4(D) if it is the unique answer set of program \u03a0c(D)\u222a\u03c3nd .\nThe uniqueness of an answer set is guaranteed for a large class of system descriptions that are well-founded. Although well-foundedness is not easy to check, the broad syntactic condition called weak-acyclicity, which is easy to check, is a sufficient condition for well-foundedness [17]. All system descriptions discussed henceforth in this paper are assumed to be well-founded.\nOur definition of transition relation of \u03c4(D) is also based on the notion of the answer set of a logic program.\nDefinition 2. [Transition of \u03c4(D)] To describe a transition \u3008\u03c30,a,\u03c31\u3009, we construct a program \u03a0(D ,\u03c30,a) consisting of:\n\u2022 Logic programming encoding \u03a0(D) of system description D .\n\u2022 Initial state \u03c30.\n\u2022 Set of actions a.\nThe answer sets of this program determine the states the system can move into after the execution of a in \u03c30. The encoding \u03a0(D) of system description D consists of the encoding of the signature of D and rules obtained from statements of D , as described below.\n\u2022 Encoding of the signature: we start with the encoding sig(D) of signature of D .\n\u2013 For each basic sort c, sig(D) contains: sort name(c). \u2013 For each subsort link \u3008c1,c2\u3009 of the hierarchy of basic sorts, sig(D) contains: s link(c1,c2). \u2013 For each membership link \u3008x,c\u3009 of the hierarchy, sig(D) contains: m link(x,c). \u2013 For every function symbol f : c1\u00d7 . . .cn\u2192 c, the signature sig(D) contains the domain: dom( f ,c1, . . . ,cn),\nand range: range( f ,c).\n\u2013 For every static g of D , sig(D) contains: static(g). \u2013 For every f (x\u0304) where f is a basic fluent, sig(D) contains: f luent(basic, f (x\u0304)). \u2013 For every f (x\u0304) where f is a defined fluent, sig(D) contains: f luent(de f ined, f (x\u0304)). \u2013 For every action a of D , sig(D) contains: action(a).\nWe also need axioms describing the hierarchy of basic sorts:\nsubsort(C1,C2)\u2190 s link(C1,C2) subsort(C1,C2)\u2190 s link(C1,C), subsort(C,C2) member(X ,C)\u2190 m link(X ,C)\nmember(X ,C1)\u2190 m link(X ,C0), subsort(C0,C1)\n\u2022 Encoding of statements of D: for this encoding we need two steps that stand for the beginning and the end of a transition. This is sufficient for describing a single transition; however, we later describe longer chains of events and let steps range over [0,n] for some constant n. To allow an easier generalization of the program, we encode steps by using constant n for the maximum number of steps, as follows:\nstep(0..n)\nwhere n will be assigned a specific value based on the goals under consideration, e.g., #const n= 3. We also need a relation val( f (x1, . . . ,xn),y, i), which states that the value of f (x1, . . . ,xn) at step i is y; and relation occurs(a, i), which states that compound action a occurred at step i, i.e., occurs({a0, . . . ,ak}, i) =de f {occurs(ai) : 0\u2264 i\u2264 k}. We use this notation to encode statements of D as follows:\n\u2013 For every causal law (Statements 2-3), where the range of f is {y1, . . . ,yk}, \u03a0(D) contains a rule:\nval( f (x\u0304),y1, I +1) or . . .or val( f (x\u0304),yk, I +1)\u2190val(body, I), occurs(a, I), I < n\nwhere val(body, I) is obtained by replacing every literal fm(x\u0304m) = z from body by val( fm(x\u0304m),z, I). To encode that due to this action, f (x\u0304) only takes a value that satisfies property p, \u03a0(D) contains a constraint:\n\u2190 val( f (x\u0304),Y, I +1), not val(p(Y ), true, I)\nand rules:\nsatis f ied(p, I)\u2190 val(p(Y ), true, I) \u00acoccurs(a, I)\u2190 not satis f ied(p, I)\n\u2013 For every state constraint and definition (Statements 4, 5), \u03a0(D) contains:\nval( f (x\u0304),y, I)\u2190 val(body, I)\n\u2013 \u03a0(D) contains the CWA for defined fluents:\nval(F, f alse, I)\u2190 f luent(de f ined,F), not val(F, true, I)\n\u2013 For every executability condition (Statement 6), \u03a0(D) contains:\n\u00acoccurs(a0, I) or . . . or \u00acoccurs(ak, I)\u2190val(body, I), I < n\n\u2013 \u03a0(D) contains the Inertia Axiom:\nval(F,Y, I +1)\u2190 f luent(basic,F), val(F,Y, I), not \u00acval(F,Y, I +1), I < n\n\u2013 \u03a0(D) contains CWA for actions:\n\u00acoccurs(A, I)\u2190 not occurs(A, I), I < n\n\u2013 Finally, we need the rule: \u00acval(F,Y1, I)\u2190 val(F,Y2, I), Y1 6= Y2\nwhich says that a fluent can only have one value at each time step.\nThis completes the construction of encoding \u03a0(D) of system description D . Please note that the axioms described above are shorthand for the set of ground instances obtained from them by replacing variables by (the available) ground terms from the corresponding sorts.\nTo continue with our definition of transition \u3008\u03c30,a,\u03c31\u3009, we describe the two remaining parts of program \u03a0(D ,\u03c30,a), the encoding val(\u03c30,0) of initial state \u03c30, and the encoding occurs(a,0) of action a:\nval(\u03c30,0) =de f {val( f (x\u0304),y,0) : ( f (x\u0304) = y) \u2208 \u03c30} occurs(a,0) =de f {occurs(ai,0) : ai \u2208 a}\nTo complete program \u03a0(D ,\u03c30,a), we simply gather our description of the system\u2019s laws, together with the description of the initial state and the actions that occur in it:\n\u03a0(D ,\u03c30,a) =de f \u03a0(D)\u222a val(\u03c30,0)\u222aoccurs(a,0)\nNow we are ready to define the notion of transition of \u03c4(D). Let a be a non-empty collection of actions, and \u03c30 and \u03c31 be states of the transition diagram \u03c4(D) defined by a system description D . A state-action-state triple \u3008\u03c30,a,\u03c31\u3009 is a transition of \u03c4(D) iff \u03a0(D ,\u03c30,a) has an answer set AS such that \u03c31 = { f (x\u0304) = y : val( f (x\u0304),y,1) \u2208 AS}."}, {"heading": "5.2 Histories with defaults", "text": "A dynamic domain\u2019s recorded history is usually a collection of records of the form obs(L, I), which says that a literal is observed at step I, e.g., obs(loc(tb1) = o ff ice,0) denotes the observation of textbook tb1 in the o ff ice\u2014when convenient, this will also be written as obs(loc(tb1,o ff ice), true,0); and hpd(action,step), which says that a specific action happened happen at a given step, e.g., hpd(move(rob1,kitchen),1). In addition to the statements described above, we introduce an additional type of historical record:\ninitial default f (x\u0304) = y if body (7)\nwhere f (x\u0304) is a basic fluent. We illustrate the use of such initial state defaults with an example, before providing a formal description.\nExample 2. [Example of defaults] Consider the following statements about the locations of textbooks in the initial state in our illustrative example. Textbooks are typically in the main library. If a textbook is not there, it is typically in the auxiliary library. If a textbook is checked out, it can usually be found in the office. These defaults can be represented as:\ninitial default loc(X) = main library if textbook(X) % Default d1 (8)\ninitial default loc(X) = aux library if textbook(X), % Default d2 (9) loc(X) 6= main library\ninitial default loc(X) = o ff ice if textbook(X), % Default d3 (10) loc(X) 6= main library, loc(X) 6= aux library\nwhere we use the fluent {loc : thing\u2192 place}. Intuitively, a history Ha with the above statements entails: val(loc(tb1)= main library, true,0) for textbook tb1. History Hb that adds obs(loc(tb1) 6= main library,0) as an observation to Ha renders default d1 (Statement 8) inapplicable; it entails: val(loc(tb1) = aux library, true,0) based on default d2 (Statement 9). A history Hc that adds observation: obs(loc(tb1) 6= aux library,0) to Hb should entail: val(loc(tb1) = o ff ice, true,0). Adding observation obs(loc(tb1) 6= main library,1) to Ha results in history Hd that defeats default d1 because, if this default\u2019s conclusion is true in the initial state, it is also true at step 1 (by inertia), which contradicts our observation. Default d2 will conclude that this book is initially in the aux library; the inertia axiom will propagate this information to entail: val(loc(tb1) = aux library, true,1). Figure 2 illustrates the beliefs of a robot corresponding to these four histories. Please see example2.sp at https://github.com/mhnsrdhrn/refine-arch for an example of the complete program in SPARC.\nThe history H of the system will define collection of its models, i.e., trajectories of the system considered possible by the agent recording this history. To define such models, consider program \u03a0(D ,H ) obtained by adding to \u03a0(D):\n\u2022 Record of observations and actions from H .\n\u2022 Rules for every default (Statement 7):\nval( f (x\u0304),y,0)\u2190val(body,0), (11) not \u00acval( f (x\u0304),y,0)\nval( f (x\u0304),Y,0) +\u2190val(body,0), % CR rule (12) range( f ,C),\nmember(Y,C),\nY 6= y is de f ined( f (x\u0304))\u2190val(body,0)\nwhere the second rule is a consistency restoring (CR) rule, which states that to restore consistency of the program one may assume that the conclusion of the default is not the expected one. For more details about CR rules, please see [18].\n\u2022 A rule defining initial values of fluents through observations:\nis de f ined( f (x\u0304))\u2190 obs( f (x\u0304) = y,0) (13)\n\u2022 A rule for every basic fluent:\nval( f (x\u0304),y1,0) or . . . or val( f (x\u0304),yn,0)\u2190 not is de f ined( f (x\u0304)) (14)\nwhere {y1, . . . ,yn} are elements in the range of f not occurring in the head of any initial default of H . This rule states that if a fluent is not defined in the initial state by the head of an initial default, or by an observation, it must take on some possible value from its range.\n\u2022 A reality check [3]: \u2190val(F,Y1, I), obs(F = Y2, I), Y1 6= Y2 (15) \u2190val(F,Y1, I), obs(F 6= Y1, I)\nwhich states that an observation of a fluent shall return the fluent\u2019s expected value.\n\u2022 And a rule: occurs(A, I)\u2190 hpd(A, I) (16)\nTo define a model of the history, we also need the following auxiliary definitions.\nDefinition 3. [Defined Sequence] We say that a set S of literals defines a sequence \u3008\u03c30,a0,\u03c31, . . . ,an\u22121,\u03c3n\u3009 if\n\u2022 For every 0\u2264 i\u2264 n, ( f (x\u0304) = y) \u2208 \u03c3i iff val( f (x\u0304),y, i)) \u2208 S.\n\u2022 For every 0\u2264 i < n, e \u2208 ai iff occurs(e, i) \u2208 S.\nDefinition 4. [Compatible initial states] A state \u03c3 of \u03c4(D) is compatible with a description I of the initial state of history H if:\n\u2022 \u03c3 satisfies all observations of I ; and\n\u2022 \u03c3 contains the closure of the union of statics of D and the set { f = y : obs( f = y,0) \u2208I }\u222a{ f 6= y : obs( f 6= y,0) \u2208I }.\nLet Ik be the description of the initial state of history Hk. States in Example 2 compatible with Ia, Ib, Ic must then contain {loc(tb1) = main library}, {loc(tb1) = aux library}, and {loc(tb1) = o ff ice} respectively. There are multiple such states, which differ by the location of robot. Since Ia = Id , they have the same compatible states.\nNext, we define models of history H , i.e., paths of the transition diagram \u03c4(D) of D compatible with H .\nDefinition 5. [Models] A path M = \u3008\u03c30,a0,\u03c31, . . . ,an\u22121,\u03c3n\u3009 of \u03c4(D) is a model of history H of D , with description I of its initial state, if there is a collection E of obs statements such that:\n1. If obs( f = y,0) \u2208 E then f 6= y is the head of one of the defaults of I . Similarly, for obs( f 6= y,0).\n2. The initial state of M is compatible with the description: IE = I \u222aE.\n3. Path M satisfies all observations of fluents and action occurrences in H .\n4. There is no collection E0 of init statements which has less elements than E and satisfies the conditions above.\nWe will refer to E as an explanation of H . For example, consider the four histories described in Example 2. Models of Ha, Hb, and Hc are paths consisting of initial states compatible with Ia, Ib, and Ic\u2014the corresponding explanations are empty. However, in the case of Hd , the situation is different\u2014the predicted location of tb1 will be different from the observed one. The only explanation of this discrepancy is that tb1 is an exception to the first default. Adding E = {obs(loc(tb1) 6= main library,0)} to Id will resolve this problem.\nWe illustrate this definition by some more examples.\nExample 3. [Examples of Models] Consider a system description Da with basic boolean fluents f and g and a history Ha:\ninitial default \u00acg if f\n{ f ,\u00acg}, {\u00ac f ,g}, and {\u00ac f ,\u00acg} are models of \u3008Da,Ha\u3009 and \u03c3 = { f ,g} is not. The latter is not surprising since even though \u03c3 may be physically possible, the agent, relying on the default, will not consider \u03c3 to be compatible with the default since the history gives no evidence that the default should be violated.\nIf the agent were to record an observation: obs(g,0), the only states compatible with the resulting history Hb (i.e.,the models) would be { f ,g} and {\u00ac f ,g}. Next, we expand our system description by a basic fluent h and a state constraint:\nh if \u00acg\nIn this case, to compute models of a history Hc of a system Db, where Hc consists of the default and an observation: obs(\u00ach,0), we need CR rules (see second rule of Statement 11). The models are { f ,\u00ach,g} and {\u00ac f ,\u00ach,g}.\nNext, consider a system description Dc with basic fluents f , g, and h, the initial default, action a, a causal law:\na causes h if \u00acg\nand a history Hd consisting of obs( f ,0), hpd(a,0); \u3008{ f ,\u00acg,h},a,{ f ,\u00acg,h}\u3009 and \u3008{ f ,\u00acg,\u00ach},a,{ f ,\u00acg,h}\u3009 are the two models of Hd . Finally, history He obtained by adding obs(\u00ach,1) to Hd has a single model \u3008{ f ,g,\u00ach},a,{ f ,g,h}\u3009. The new observation is an indirect exception to the initial default, which is resolved using the corresponding CR rule.\nComputing models of system histories: The definition of models of a history H of a system D (see Definition 5) suggests a simple algorithm for computing a model of H . We only need to use an (existing) answer set solver to compute an answer set AS of a program \u03a0(D ,H ), and check if AS defines a path of \u03c4(D). To do the latter, we need to check that for every 0\u2264 i\u2264 n, the set:\n{ f (x\u0304) = y : val( f (x\u0304),y, i) \u2208 AS}\nis a state of \u03c4(D), and that triples \u3008\u03c3i,ai,\u03c3i+1\u3009 defined by AS are transitions of \u03c4(D). A triple \u3008\u03c3i,a,\u03c3i+1\u3009 is defined by AS if \u03c3i = { f (x\u0304) = y : val( f (x\u0304),y, i)\u2208AS}, \u03c3i+1 = { f (x\u0304) = y : val( f (x\u0304),y, i+1)\u2208AS}, and ai = {e : occurs(e, i)\u2208AS}. Although this check can be done using the definitions of state and transition, the following theorem shows that for a large class of system descriptions, this computation can (fortunately) be avoided.\nProposition 1. [Models and Answer Sets] A path M = \u3008\u03c30,a0,\u03c31, . . . ,\u03c3n\u22121,an\u3009 of \u03c4(D) is a model of history H n iff there is an answer set AS of a program \u03a0(D ,H ) such that:\n1. A fluent literal ( f = y) \u2208 \u03c3i iff val( f ,y, i) \u2208 AS,\n2. A fluent literal ( f 6= y1) \u2208 \u03c3i iff val( f ,y2, i) \u2208 AS,\n3. An action e \u2208 ai iff occurs(e, i) \u2208 AS.\nThe proof of this proposition is in Appendix A. This proposition allows us to reduce the task of planning to computing answer sets of a program obtained from \u03a0(D ,H ) by adding the definition of a goal, a constraint stating that the goal must be achieved, and a rule generating possible future actions of the robot. In other words, the same process of computing answer sets can be used for inference, planning and diagnostics."}, {"heading": "6 Logician\u2019s Domain Representation", "text": "We are now ready for the first step of our design methodology, i.e., specify the transition diagram of the logician.\n1. Specify the transition diagram, \u03c4H , which will be used by the logician for high-level reasoning, including planning and diagnostics.\nThis step is accomplished by providing the signature and ALd axioms of system description DH defining this diagram. We will use standard techniques for representing knowledge in action languages, e.g., [18]. We illustrate this process by describing the domain representation for the office domain described in Example 1).\nExample 4. [Logician\u2019s domain representation] The system description DH of the domain in Example 2 consists of a sorted signature (\u03a3H ) and axioms describing the transition diagram \u03c4H . \u03a3H defines the names of objects and functions available for use by the logician. As described in Example 1, the sorts are: place, thing, robot, and ob ject, with ob ject and robot being subsorts of thing. The sort ob ject has subsorts such as: textbook, printer and kitchenware. The statics include a relation next to(place, place), which describes if two places are next to each other. The signatures of the fluents of the domain are: loc : thing\u2192 place and in hand : robot \u00d7 ob ject \u2192 boolean. The value of loc(T h) = Pl if thing T h is located at place Pl. The fluent in hand(R,Ob) is true if robot R is holding object Ob. These are basic fluents subject to the laws of inertia. The domain has three actions: move(robot, place), grasp(robot,ob ject), and putdown(robot,ob ject). The domain dynamics are defined using axioms that consist of causal laws:\nmove(R,Pl) causes loc(R) = Pl (17) grasp(R,Ob) causes in hand(R,Ob) putdown(R,Ob) causes \u00acin hand(R,Ob)\nstate constraints: loc(Ob) = Pl if loc(R) = Pl, in hand(R,Ob) (18) next to(P1,P2) if next to(P2,P1)\nand executability conditions:\nimpossible move(R,Pl) if loc(R) = Pl (19) impossible move(R,Pl2) if loc(R) = Pl1, \u00acnext to(Pl1,Pl2)\nimpossible A1, A2 if A1 6= A2 impossible grasp(R,Ob) if loc(R) = Pl1, loc(Ob) = Pl2, Pl1 6= Pl2 impossible grasp(R,Ob) if in hand(R,Ob) impossible putdown(R,Ob) if \u00acin hand(R,Ob)\nThe part of \u03a3H described so far, the sort hierarchy and the signatures of functions, is unlikely to undergo substantial changes for any given domain. However, the last step in the constructions of \u03a3H is likely to undergo more frequent revisions\u2014it populates the basic sorts of the hierarchy with specific objects; e.g robot = {rob1}, place = {r1, . . . ,rn} where rs are rooms, textbook = {tb1, . . . tbm}, kitchenware = {cup1,cup2, plate1, plate2} etc. Ground instances of the axioms are obtained by replacing variables by ground terms from the corresponding sorts.\nThe transition diagram \u03c4H described by DH is too large to depict in a picture. The top part of Figure 3(a) shows the transitions of \u03c4H corresponding to a move between two places. The only fluent shown there is the location of the robot rob1\u2014the values of other fluents remain unchanged and are not shown here. The actions of this coarseresolution transition diagram \u03c4H of the logician, as described above, are assumed to be deterministic, and the values of its fluents are assumed to be observable. These assumptions allow the robot to do fast, tentative planning and diagnostics necessary for achieving its assigned goals.\nThe domain representation described above should ideally be tested extensively. This can be done by including various recorded histories of the domain, which may include histories with prioritized defaults (Example 2), and using the resulting programs to solve various reasoning tasks.\nThe logician\u2019s model of the world thus consists of the system description (Example 4), initial state defaults (Example 2), and recorded history of actions and observations. The logician achieves any given goal by first translating the model (of the world) to an ASP program\u2014see Sections 5.1, 5.2. For planning and diagnostics, this program is passed to an ASP solver\u2014we use SPARC, which expands CR-Prolog and provides explicit constructs to specify objects, relations, and their sorts [2]. Please see example4.sp at https://github.com/mhnsrdhrn/refine-arch for the SPARC version of the complete program. The solver returns the answer set of the program. Atoms of the form:\noccurs(action,step)\nbelonging to this answer set, e.g., occurs(a1,1), . . . ,occurs(an,n), represent the shortest sequence of abstract actions, i.e., the shortest plan for achieving the logician\u2019s goal. Prior research results in the theory of action languages and ASP ensure that the plan is provably correct [18]. In a similar manner, suitable atoms in the answer set can be used for diagnostics, e.g., to explain unexpected observations in terms of exogenous actions."}, {"heading": "7 Refinement, Zoom and Randomization", "text": "For any given goal, each abstract action in the plan created by reasoning with the coarse-resolution domain representation is implemented as a sequence of concrete actions by the statistician. To do so, the robot probabilistically reasons about the part of the fine-resolution transition diagram relevant to the abstract action to be executed. This section defines refinement, randomization, and the zoom operation, which are necessary to build the fine-resolution models for such probabilistic reasoning, along with the corresponding steps of the design methodology."}, {"heading": "7.1 Refinement", "text": "The second step of the design methodology corresponds to the construction of a fine-resolution transition diagram \u03c4L from the coarse-resolution transition diagram \u03c4H . We illustrate this step by constructing the signature of \u03c4L and the axioms of its system description DL for the office domain of Example 1. This construction is not entirely algorithmic\u2014 although the signature of \u03c4H and its axioms in Example 4 will play an important role, the construction will also depend on the result of the increase in the resolution, which is domain dependent. Note, however, that any input provided by the designer is during the initial design phase\u2014at run-time, all steps of planning and execution are algorithmic and automated. We begin with some terminology.\nIf examining an object of \u03a3H at higher resolution leads to the discovery of new structure(s), the object is said to have been magnified. Newly discovered parts of the magnified objects are referred to as its refined components. We can now construct the signature of \u03c4L.\n2. Constructing the refinement \u03c4L of \u03c4H . (a) Signature \u03a3L of \u03c4L\nA signature \u03a3L is said to refine signature \u03a3H if:\n\u2022 For every basic sort stH of \u03a3H , whose elements were magnified by the increase in the resolution, \u03a3L contains a (a) coarse-resolution version st\u2217L = stH ; and (b) fine-resolution version stL = {o1, . . . ,om} consisting of the components of magnified elements of stH . We also refer to the fine-resolution version of an original sort as its fine-resolution counterpart.\nFor instance, for the sort place = {r1, . . . ,rn} in \u03a3H (representing rooms), we have the coarse-resolution copy:\nplace\u2217 = {r1, . . . ,rn}\nand its fine-resolution counterpart:\nplace = {c1, . . . ,cm}\nwhere c1, . . . ,cm are newly discovered cells in the rooms. Although the original version and its fine-resolution counterpart have the same name, it does not cause a problem because they belong to different signatures. Instead, it proves to be convenient for the construction of axioms of DL. Also, for basic sort stH of \u03a3H whose elements were not magnified by the increase in resolution, \u03a3L contains stL = stH .\n\u2022 \u03a3L contains static relation component(oi,o), which holds iff object oi \u2208 stL is a newly discovered component of magnified object o from st\u2217L .\nContinuing with Example 1 and Example 4, we have:\ncomponent : place\u00d7 place\u2217\u2192 boolean\nwhere component(c,r) is true iff cell c is part of room r.\n\u2022 For every function symbol f : st1, . . . ,stn\u2192 st0 of \u03a3H :\n\u2013 If signature of f contains magnified sorts, \u03a3L contains function symbol f \u2217 : st \u20321, . . . ,st \u2032 n\u2192 st \u20320, where st \u2032i =\nst\u2217i if sti is magnified and st \u2032 i = sti otherwise.\n\u2013 f : st1, . . . ,stn\u2192 st0 is a function symbol of \u03a3L.\nIn our example, \u03a3L will include, for instance:\nloc\u2217 : thing\u2192 place\u2217\nand loc : thing\u2192 place\nAlthough the second fluent looks identical to its coarse-resolution counterpart in \u03a3H , the meaning of place in it is different\u2014elements of sort place are rooms in \u03a3H , but are cells in \u03a3L. In a similar manner, \u03a3L will include both next to(place, place) and next to\u2217(place\u2217, place\u2217)\u2014the former describes two cells that are next to each other while the latter describes two rooms that are next to each other.\n\u2022 Actions of \u03a3L include (a) every action in \u03a3H with its magnified parameters replaced by their fine-resolution counterparts; and (b) knowledge-producing action:\ntest(robot, f luent,value)\nwhich activates algorithms on the robot R to check if the value of an observable fluent F in a given state is Y . Note that this action only changes basic knowledge fluents (see below).\nIn our example, the sort action of \u03a3L will have (a) original actions grasp and putdown; (b) action move(robot,cell) of a robot moving to an (adjacent) cell; and (c) test action for testing the values of each observable fluent, e.g., instances of test(R, loc(T h),C) and test(R, in hand(R,O), true) for specific cells and objects.\n\u2022 \u03a3L includes basic knowledge fluents directly observed, indirectly observed and can be tested, and defined fluents may discover and observed. These fluents are used to describe observations of the environment and the axioms governing them\u2014details provided below in the context of the axioms in DL.\n2. Constructing the refinement \u03c4L of \u03c4H . (b) Axioms of DL\nAxioms of the refined system description DL include:\n\u2022 All axioms of DH .\nAlthough this set of axioms is syntactically identical to the axioms of DH , their ground instantiations are different because their variables may range over different sorts. Continuing with our example of refining the description in Example 4, while variables for places were ground using names of rooms in DH , they are ground by names of cells in DL. There are also differences in the definition of statics, as discussed further below.\n\u2022 Axioms relating coarse-resolution domain properties and their fine-resolution counterparts. Assuming that the only sort of the signature of the original fluent f influenced by the increase in resolution is its range, the axiom has the form:\nf \u2217(X1, . . . ,Xm) = Y if component(C1,X1), . . . , component(Cm,Xm), component(C,Y ), (20) f (C1, . . . ,Cm) =C\nwhere, for non-magnified object O, component(O,O) holds true. In our example, we have:\nloc\u2217(T h) = Rm if component(C,Rm), loc(T h) =C next to\u2217(Rm1,Rm2) if component(C1,Rm1), component(C2,Rm2), next to(C1,C2)\nIn general, we need to add component(Ci,Xi) to the body of the rule for those Xi that were affected by the increase in resolution, and change the value of the domain property appropriately in the head and the body of the rule. A key requirement is that the head of the axiom holds true in the coarse-resolution system description if and only if the body of the axiom holds in the fine-resolution (i.e., refined) system description.\n\u2022 Axioms for observing the environment.\nIn addition to actions that change domain properties, the refinement includes actions and fluents for observing the environment. As stated above, the values of fluents of DL will be determined by action test(robot, f luent,value). Even though the second parameter of this action is an observable fluent, its value may not always be testable by a sensor, e.g., we assume that a robot cannot check if an object is located in a cell unless the robot is also located in that cell. This is represented by a domain-dependent basic knowledge fluent:\ncan be tested : robot\u00d7 f luent\u00d7 value\u2192 boolean\nwhose definition is supplied by the designer. In our example, the corresponding axioms may be:\ncan be tested(R, loc(T h),C) if loc(R) =C can be tested(R, in hand(R,O),V )\nNext, to model the results of sensing, we use another basic knowledge fluent:\ndirectly observed : robot\u00d7 f luent\u00d7 value\u2192 outcomes\nwhere: outcomes = {true, f alse,undet}\nis a sort with three possible values true, false, and undetermined. The initial value of directly observed, for all its parameters, is undet. The direct effect of test(R,F,Y ) is then described by the following causal laws:\ntest(R,F,Y ) causes directly observed(R,F,Y ) = true if F = Y. (21) test(R,F,Y ) causes directly observed(R,F,Y ) = f alse if F 6= Y.\nand the executability condition:\nimpossible test(R,F,Y ) if \u00accan be tested(R,F,Y ) (22)\nIn the context of the refinement of Example 4, if a robot rob1 located in cell c is checking for an object o, directly observed(rob1, loc(o),c) will be true iff o is in c during testing; it will be false iff o is not in c. These values will be preserved by inertia until the state is observed to have changed when the same cell is tested again. If robot rob1 has not yet tested a cell c for object o, the value of directly observed(r, loc(o),c) will remain undet.\nIn addition to directly observing the fine-resolution fluents using action test, the robot should be able to test the values of observable coarse-resolution fluents inherited from \u03a3H . For instance, although rob1 cannot directly observe if object o is in a room r, it can do so indirectly if o is observed in cell c of room r\u2014o is indirectly observed to not be in r if all the cells in r have been examined without observing o. Such a relationship is assumed to hold for all magnified fluents. This assumption is axiomatized using a basic knowledge fluent:\nindirectly observed : robot\u00d7 f luent\u2217\u00d7 value\u2217\u2192 outcomes\nwhich will initially be set to undet, and a defined fluent:\nmay discover : robot\u00d7 f luent\u2217\u00d7 value\u2217\u2192 boolean.\nwhere may discover(R,F\u2217,Y ) holds if robot R may discover if the value of F\u2217, copy of a coarse-resolution fluent whose value is determined by the value of its components, is Y . The axioms for an indirect observation are:\nindirectly observed(R,F\u2217,Y ) = true if directly observed(R,F,C) = true, (23) component(C,Y )\nindirectly observed(R,F\u2217,Y ) = f alse if indirectly observed(R,F\u2217,Y ) 6= true, \u00acmay discover(R,F\u2217,Y )\nmay discover(R,F\u2217,Y ) if indirectly observed(R,F\u2217,Y ) 6= true, component(C,Y ),\ndirectly observed(R,F,C) = undet\nwhere F and F\u2217 are the fine-resolution counterpart and the coarse-resolution version (respectively) of a fluent in \u03a3H that has been magnified. In our example, the axioms for fluent loc will look as follows:\nindirectly observed(R, loc\u2217(O),Room) = true if directly observed(R, loc(O),C) = true, component(C,Room).\nindirectly observed(R, loc\u2217(O),Room) = f alse if indirectly observed(R, loc\u2217(O),Room) 6= true, \u00acmay discover(R, loc\u2217(O),Room).\nmay discover(R, loc\u2217(O),Room) if indirectly observed(R, loc\u2217(O),Room) 6= true, component(C,Room),\ndirectly observed(R, loc(O),C) = undet.\nFinally, a defined fluent is used to say that any fluent observed directly or indirectly has been observed:\nobserved : robot\u00d7 f luent\u00d7 value\u2192 boolean\nwhere observed(R,F,Y ) is true iff the most recent observation of F returned value Y . The following axioms relate this fluent with the basic knowledge fluents defined earlier:\nobserved(R,F,Y ) = true if directly observed(R,F,Y ) = true (24) observed(R,F,Y ) = true if indirectly observed(R,F,Y ) = true\nThis completes our construction of DL. We are now ready to provide a formal definition of refinement.\nDefinition 6. [Refinement of a state] A state \u03b4 of \u03c4L is said to be a refinement of a state \u03c3 of \u03c4H if:\n\u2022 For every magnified domain property f that is from the signature of \u03a3H :\nf (x) = y \u2208 \u03c3 iff f \u2217(x) = y \u2208 \u03b4\n\u2022 For every other domain property of \u03a3H : f (x) = y \u2208 \u03c3 iff f (x) = y \u2208 \u03b4\nDefinition 7. [Refinement of a system description] Let DL and DH be system descriptions with transition diagrams \u03c4L and \u03c4H respectively. DL is a refinement of DH if:\n1. States of \u03c4L are the refinements of states of \u03c4H .\n2. For every transition \u3008\u03c31,aH ,\u03c32\u3009 of \u03c4H , every fluent f in a set F of observable fluents, and every refinement \u03b41 of \u03c31, there is a path P in \u03c4L from \u03b41 to a refinement \u03b42 of \u03c32 such that:\n(a) Every action of P is executed by the robot which executes aH . (b) Every state of P is a refinement of \u03c31 or \u03c32, i.e., no unrelated fluents are changed. (c) observed(R, f ,Y ) = true \u2208 \u03b42 iff ( f = Y ) \u2208 \u03b42 and observed(R, f ,Y ) = f alse \u2208 \u03b42 iff ( f 6= Y ) \u2208 \u03b42.\nProposition 2. [Refinement] Let DH and DL be the coarse-resolution and fine-resolution system descriptions for the office domain in Example 1. Then DL is a refinement of DH . The proof of this proposition is in Appendix B. As stated in Section 4, it is the designer\u2019s responsibility to establish that the fine-resolution system description of any given domain is a refinement of the coarse-resolution system description."}, {"heading": "7.2 Randomization", "text": "The system description DL of transition diagram \u03c4L, obtained by refining transition diagram \u03c4H , is insufficient to implement a coarse-resolution transition T = \u3008\u03c31,aH ,\u03c32\u3009 \u2208 \u03c4H . We still need to capture the non-determinism in action execution and observations, which brings us to the third step of the design methodology (Section 4).\n3. Provide domain-specific information and randomize the fine-resolution description of the domain to capture the non-determinism in action execution.\nThis step models the non-determinism by first creating DLR, the randomized fine-resolution system description, by:\n\u2022 Replacing each action\u2019s deterministic causal laws in DL by non-deterministic ones; and\n\u2022 Modifying the signature by declaring each affected fluent as a random fluent, i.e., define the set of values the fluent can choose from when the action is executed. A defined fluent may be introduced to describe this set of values in terms of other variables.\nFor instance, consider a robot moving to a specific cell in the o ff ice. During this move, the robot can reach the desired cell or one of the neighboring cells. The causal law for the move action in DL can therefore be (re)stated as:\nmove(R,C2) causes loc(R) = {C : range(loc(R),C)} (25)\nwhere the robot can only move to a cell that is next to its current cell location:\nimpossible move(R,C2) if loc(R) =C1, \u00acnext to(C1,C2) (26)\nand the relation range is a defined fluent that is given by:\nrange(loc(R),C) if loc(R) =C range(loc(R),C) if loc(R) =C1, next to(C,C1)\nwhere the cell the robot is currently in, and the cells next to this cell, are all within the range of the robot. In addition, the fluent affected by the change in this causal law is declared as a random fluent and its definition is changed to:\nloc(X) = {Y : p(Y )}\nwhere a thing\u2019s location is one of a set of values that satisfy a given property\u2014in the current example that considers a robot\u2019s location, this property is range as described above. In a similar manner, the non-deterministic version of the test action used to determine the robot\u2019s cell location in the o ff ice, is given by:\ntest(rob1, loc(rob1),ci) causes directly observed(rob1, loc(rob1),ci) = {true, f alse} if loc(rob1) = ci\nwhich indicates that the result of the test action may not always be as expected, and ci are cells in the o ff ice. Similar to refinement, it is the designer\u2019s responsibility to provide domain-specific information needed for randomization.\nCollecting statistics: Once the fine-resolution system description has been randomized, experiments are run and statistics are collected to compute the probabilities of action outcomes and the reliability of observations. This corresponds to the fourth step of the design methodology (Section 4).\n4. Run experiments, collect statistics, and compute probabilities of action outcomes and the reliability of observations.\nSpecifically, we need to compute the:\n\u2022 Causal probabilities for the outcomes of actions; and\n\u2022 Quantitative model for observations, which provides the probability of the observations being correct.\nThis collection of statistics is typically a one-time process performed in an initial training phase. Also, the statistics are computed separately for each basic fluent in DLR. To collect the statistics, we consider one non-deterministic causal law in DLR at a time. We sample some ground instances of this causal law, e.g., corresponding to different atoms in the causal law. The robot then executes the action corresponding to this sampled instance multiple times, and collects statistics (i.e., counts) of the number of times each possible outcome (i.e., value) is obtained. The robot also collects information about the amount of time taken to execute each action.\nAs an example, consider a ground instance of the non-deterministic causal law for move, considering specific locations of a robot in a specific room:\nmove(rob1,c2) causes loc(R) = {c1,c2,c3}\nwhere rob1 in cell c1 can end up in one of three possible cells when it tries to move to c2. In ten attempts to move to c2, assume that rob1 remains in c1 in one trial, reaches c2 in eight trials, and reaches c3 in one trial. The maximum likelihood estimates of the probabilities of these outcomes are then 0.1, 0.8 and 0.1 respectively\u2014the probability of rob1 moving to other cells is zero. Similar statistics are collected for other ground instances of this causal law, and averaged to compute the statistics for the fluent loc for rob1. The same approach is used to collect statistics for other causal laws and fluents, including those related to knowledge actions and basic knowledge fluents. For instance, assume that the collected statistics indicate that testing for the presence of a textbook in a cell requires twice as much computational time (and thus effort) as testing for the presence of a printer. This information, and the relative accuracy of recognizing textbook and printers, will be used to determine the relative value of executing the corresponding test actions (see Section 8.2).\nThere are some important caveats related to the collection of statistics. First, the collection of statistics depends on the availability of relevant ground truth information, e.g., the actual location of rob1 after executing move(rob1,c2)\u2014 this information is provided by an external high-fidelity sensor during the training phase, or by a human. Second, although we do not do so in our experiments, it is possible to use heuristic functions to model the computational effort, and to update the statistics incrementally over time\u2014if any heuristic functions are used, the designer has to make them available to automate subsequent steps of our control loop. Third, considering all ground instances of one causal law at a time can require a lot of training in complex domains, but this is often unnecessary. For instance, it is often the case that the statistics of moving from a cell to one of its neighbors is the same for cells in a room and any given robot. In a similar manner, if the robot and an object are (are not) in the same cell, the probability of the robot observing (not observing) the object is often the same for any cell. The designer thus only considers representative samples of the distinct cases to collect statistics, e.g., statistics corresponding to moving between cells will be collected in two different rooms only if these statistics are expected to be different because the rooms have different floor coverings."}, {"heading": "7.3 Zoom", "text": "Reasoning probabilistically about the entire randomized fine-resolution system description can become computationally intractable. For any given transition T = \u3008\u03c31,aH ,\u03c32\u3009 \u2208 \u03c4H , this intractability could be offset by limiting fineresolution probabilistic reasoning to the part of transition diagram \u03c4LR whose states are the refinements of \u03c31 and \u03c32. For instance, for the state transition corresponding to a robot moving from the o ff ice to the kitchen in Example 4, i.e.,\naH = move(rob1,kitchen), we could only consider states of \u03c4LR in which the robot\u2019s location is a cell in the o ff ice or the kitchen. However, these states would still contain fluents and actions not relevant to the execution of aH , e.g., locations of domain objects, and the grasp action. What we need is a fine-resolution transition diagram \u03c4LR(T ) whose states contain no information unrelated to the execution of aH , while its actions are limited to those which may be useful for such an execution. In the case of aH = move(rob1,kitchen), for instance, states of \u03c4LR(T ) should not contain any information about domain objects. In the proposed architecture, the controller constructs such a zoomed fineresolution system description DLR(T ) in two steps. First, a new action description is constructed by focusing on the transition T , creating a system description DH(T ) that consists of ground instances of DH built from object constants of \u03a3H relevant to T . In the second step, the refinement of DH(T ) is extracted from DLR to obtain DLR(T ). We first consider the requirements of the zoom operation.\nDefinition 8. [Requirements of zoom] The following are the requirements the zoom operation should satisfy:\n1. Every path in the zoomed transition diagram should correspond to a path in the transition diagram before zooming. In other words, for every path Pz of \u03c4LR(T ) between states \u03b4 z1 \u2286 \u03b41 and \u03b4 z 2 \u2286 \u03b42, where \u03b41 and \u03b42 are\nrefinements of \u03c31 and \u03c32 respectively, there is a path P between states \u03b41 and \u03b42 in \u03c4LR.\n2. Every path in the transition diagram before zooming should correspond to a path in the zoomed transition diagram. In other words, for every path P of \u03c4LR, formed by actions of \u03c4LR(T ), between states \u03b41 and \u03b42 that are refinements of \u03c31 and \u03c32 respectively, there is a path Pz of \u03c4LR(T ) between states \u03b4 z1 \u2286 \u03b41 and \u03b4 z 2 \u2286 \u03b42.\n3. Paths in \u03c4LR(T ) should be of sufficiently high probability for the probabilistic solver to find them.\nTo construct such a zoomed system description DLR(T ) defining transition diagram \u03c4LR(T ), we begin by defining relObConH(T ), the collection of object constants of signature \u03a3H of DH relevant to transition T .\nDefinition 9. [Constants relevant to a transition] For any given (ground) transition T = \u3008\u03c31,aH ,\u03c32\u3009 of \u03c4H , by relObConH(T ) we denote the minimal set of object constants of signature \u03a3H of DH closed under the following rules:\n1. Object constants from aH are in relObConH(T );\n2. If f (x1, . . . ,xn) = y belongs to \u03c31 or \u03c32, but not both, then x1, . . . ,xn,y are in relObConH(T );\n3. If the body B of an executability condition of aH contains an occurrence of a term f (x1, . . . ,xn) and f (x1, . . . ,xn)= y \u2208 \u03c31 then x1, . . . ,xn,y are in relObConH(T ).\nConstants from relObConH(T ) are said to be relevant to T . In the context of Example 4, consider transition T = \u3008\u03c31,grasp(rob1,cup1),\u03c32\u3009 such that loc(rob1) = kitchen and loc(cup1) = kitchen are in \u03c31. Then, relObConH(T ) consists of rob1 of sort robot and cup1 of sort kitchenware (based on the first rule above), and kitchen of sort place (based on the third rule above and fourth axiom in Statement 19 in Example 4). For more details, see Example 6.\nNow we are ready for the first step of the construction of DLR(T ). Object constants of the signature \u03a3H(T ) of the new system description DH(T ) are those of relObConH(T ). Basic sorts of \u03a3H(T ) are non-empty intersections of basic sorts of \u03a3H with relObConH(T ). The domain properties and actions of \u03a3H(T ) are those of \u03a3H restricted to the basic sorts of \u03a3H(T ), and the axioms of DH(T ) are restrictions of axioms of DH to \u03a3H(T ). It is easy to show that the system descriptions DH and DH(T ) satisfy the following requirement\u2014for any transition T = \u3008\u03c31,aH ,\u03c32\u3009 of transition diagram \u03c4H corresponding to system description DH , there exists a transition \u3008\u03c31(T ),aH ,\u03c32(T )\u3009 in transition diagram \u03c4H(T ) corresponding to system description DH(T ), where \u03c31(T ) and \u03c32(T ) are obtained by restricting \u03c31 and \u03c32 (respectively) to the signature \u03a3H(T ).\nIn the second step, the zoomed system description DLR(T ) is constructed by refining the system description DH(T ). Unlike the description of refinement in Section 7.1, which requires the designer to supply domain-specific information, we do not need any additional input from the designer for refining DH(T ) and can automate the entire zoom operation. We now provide a formal definition of the zoomed system description.\nDefinition 10. [Zoomed system description] For a coarse-resolution transition T , DLR(T ) with signature \u03a3LR(T ) is said to be the zoomed fine-resolution system description if:\n1. Basic sorts of \u03a3LR(T ) are those of DLR that are refined counterparts of the basic sorts of DH(T ).\n2. Functions of \u03a3LR(T ) are those of DLR restricted to the basic sorts of \u03a3LR(T ).\n3. Actions of \u03a3LR(T ) are those of DLR restricted to the basic sorts of \u03a3LR(T ).\n4. Axioms of DLR(T ) are those of DLR restricted to the signature \u03a3LR(T ).\nConsider T = \u3008\u03c31,move(rob1,kitchen),\u03c32\u3009 such that loc(rob1) = o ff ice \u2208 \u03c31. The basic sorts of \u03a3LR(T ) include robotzL = {rob1}, place \u2217z L = {o ff ice,kitchen} and place z L = {ci : ci \u2208 kitchen\u222a o ff ice}. The functions of \u03a3LR(T ) include loc\u2217(rob1) taking values from place\u2217zL , loc(rob1) taking values from place z L, range(loc(rob1), place z L), statics next to\u2217(place\u2217zL , place \u2217z L ) and next to(place z L, place z L), properly restricted functions related to testing the values of fluent terms etc. The actions include move(rob1,ci) and test(rob1, loc(rob1),ci), where ci are individual elements of placezL. Finally, restricting the axioms of DLR to the signature \u03a3LR(T ) removes causal laws for grasp and put down, and the first state constraint corresponding to Statement 18 in DLR. Furthermore, in the causal law and executability condition for move, we only consider cells in the kitchen or the o ff ice.\nBased on Definition 7 of refinement and Proposition 2, it is easy to show that the system descriptions DH(T ) and DLR(T ) satisfy the following requirement\u2014for any transition \u3008\u03c31(T ),aH ,\u03c32(T )\u3009 in transition diagram \u03c4H(T ) corresponding to system description DH(T ), where \u03c31(T ) and \u03c32(T ) are obtained by restricting states \u03c31 and \u03c32 (respectively) of DH to signature \u03a3H(T ), there exists a path in \u03c4LR(T ) between every refinement \u03b4 z1 of \u03c31(T ) and a refinement \u03b4 z2 of \u03c32(T ). We now provide two examples of constructing the zoomed system description.\nExample 5. [First example of zoom] As an illustrative example, consider the transition T = \u3008\u03c31,move(rob1,kitchen),\u03c32\u3009 such that loc(rob1) = o ff ice\u2208 \u03c31. In addition to the description in Example 4, assume that the domain includes (a) boolean fluent broken(robot); and (b) fluent color(robot) taking a value from a set of colors\u2014there is also an executability condition:\nimpossible move(Rb,Pl) if broken(Rb)\nIntuitively, color(Rb) and broken(Rb), where Rb 6= rob1, are not relevant to aH , but broken(rob1) is relevant. Specifically, based on Definition 9, relObConH(T ) consists of rob1 of sort robot, and {kitchen,o ff ice} of sort place\u2014basic sorts of \u03a3H(T ) are intersections of these sorts with those of \u03a3H . The domain properties and actions of signature \u03a3H(T ) are restricted to these basic sorts, and axioms of DH(T ) are those of DH restricted to \u03a3H(T ), e.g., they only include suitably ground instances of the first axiom in Statement 17, the second axiom in Statement 18, and the first three axioms in Statement 19.\nNow, the signature \u03a3LR(T ) of the zoomed system description DLR(T ) has the following:\n\u2022 Basic sorts robotzL = {rob1}, place \u2217z L = {o ff ice,kitchen} and place z L = {ci : ci \u2208 kitchen\u222ao ff ice}.\n\u2022 Functions that include (a) fluents loc(robotzL) and loc\u2217(robot z L) that take values from place z L and place \u2217z L respec-\ntively, and range(loc(robotzL), place z L); (b) static relations next to \u2217(place\u2217zL , place \u2217z L ) and next to(place z L, place z L); (c) broken(robotzL); (d) knowledge fluents, e.g., directly observed(robot z L, loc(robot z L), place z L), etc.\n\u2022 Actions that include (a) move(robotzL, place z L); and (b) test(robot z L, loc(robot z L), place z L).\nThe axioms of DLR(T ) are those of DLR restricted to \u03a3LR(T ), e.g., they include:\nmove(rob1,c j) causes loc(rob1) = {C : range(loc(rob1),C)} test(rob1,loc(rob1),c j) causes directly observed(rob1, loc(rob1),c j) = {true, f alse} if loc(rob1) = c j\nimpossible move(rob1,c j) if loc(rob1) = ci, \u00acnext to(c j,ci) impossible move(rob1,c j) if broken(rob1)\nwhere range(loc(rob1),C) may hold for C \u2208 {ci,c j,ck}, which are within the range of the robot\u2019s current location (ci), and are elements of placezL. Assuming the robot is not broken, each state of \u03c4LR(T ) thus includes an atom of the form loc(rob1) = ci, where ci is a cell in the kitchen or the o ff ice, \u00acbroken(rob1), direct observations of this atom, e.g., directly observed(rob1, loc(rob1),ci) = true, and statics such as next to(ci,c j) etc. Specific actions include move(rob1,ci) and test(rob1, loc(rob1),ci).\nAs an extension to this example, if rob1 is holding textbook tb1 before executing aH = move(rob1,kitchen), i.e., in hand(rob1, tb1) \u2208 \u03c31, then \u03a3H(T ) also includes tb1 of sort textbook, and \u03a3LR(T ) includes ob jectzL = {tb1}. The functions of DLR(T ) include basic fluent in hand(robotzL,ob ject z L) and the corresponding knowledge fluents, and the actions and axioms are suitably restricted.\nExample 6. [Second example of zoom] Consider the transition T = \u3008\u03c31,grasp(rob1,cup1),\u03c32\u3009 such that loc(rob1) = kitchen is in \u03c31. Note that this example does not have the additional fluents (e.g., broken) considered in Example 5. Based on Definition 9, relObConH(T ) consists of rob1 of sort robot and cup1 of sort kitchenware, and kitchen of sort place\u2014signature \u03a3H(T ) and system description DH(T ) are constructed in a manner similar to that described in Example 5.\nNow, the signature \u03a3LR(T ) of the zoomed system description DLR(T ) has the following: \u2022 Basic sorts robotzL = {rob1}, place \u2217z L = {kitchen}, place z L = {ci : ci \u2208 kitchen}, and ob ject z L = {cup1}.\n\u2022 Functions that include (a) basic non-knowledge fluents loc(robotzL) and loc(ob ject z L) that take values from\nplacezL, and loc \u2217(robotzL) and loc \u2217(ob jectzL) that take values from place \u2217z L ; (b) fluent range(loc(robot z L), place z L); (c) static relations next to\u2217(place\u2217zL , place \u2217z L ) and next to(place z L, place z L); (d) knowledge fluents restricted to the basic sorts and fluents, etc.\n\u2022 Actions such as (a) move(robotzL, place z L); (b) grasp(robot z L,ob ject z L); (c) putdown(robot z L,ob ject z L); (d) knowledge-\nproducing actions test(robotzL, loc(robot z L), place z L) and test(robot z L, loc(ob ject z L), place z L), etc.\nThe axioms of DLR(T ) are those of DLR restricted to the signature \u03a3LR(T ). These axioms include:\nmove(rob1,c j) causes loc(rob1) = {C : range(loc(rob1),C)} grasp(rob1,cup1) causes in hand(rob1,cup1) = {true, f alse}\ntest(rob1,loc(rob1),c j) causes directly observed(rob1, loc(rob1),c j) = {true, f alse} if loc(rob1) = c j test(rob1,loc(cup1),c j) causes directly observed(rob1, loc(cup1),c j) = {true, f alse} if loc(cup1) = c j\nimpossible move(rob1,c j) if loc(rob1) = ci, \u00acnext to(c j,ci) impossible grasp(rob1,cup1) if loc(rob1) = ci, loc(cup1) = c j, ci 6= c j\nwhere range(loc(rob1),C) may hold for C \u2208 {ci,c j,ck}, which are within the range of the robot\u2019s current location (ci), and are elements of placezL. The states of \u03c4LR(T ) thus include atoms of the form loc(rob1) = ci and loc(cup1) = c j, where ci and c j are values in placezL, in hand(rob1,cup1), observations, e.g., directly observed(rob1, loc(rob1),ci) = true, statics such as next to(ci,c j), etc. Specific actions include move(rob1,ci), grasp(rob1,cup1), putdown(rob1,cup1), test(rob1, loc(rob1),ci) and test(rob1, loc(cup1),ci).\nIn Examples 5 and 6, the statistics collected earlier (Section 7.2) can be used to assign probabilities to the outcomes of actions, e.g., if action move(rob1,c1) is executed, the probabilities of the outcomes may be:\nP(loc(rob1) = c1) = 0.85\nP(loc(rob1) =Cl | range(loc(rob1),Cl),Cl 6= c1) = 0.15 |Cl| ; where Cl = {Cl : range(loc(rob1),Cl),Cl 6= c1}\nSimilarly, if the robot has to search for a textbook cup1 once it reaches the kitchen, and if a test action is executed to determine the location of a textbook cup1 in cell ci in the kitchen, the probabilities of the outcomes may be:\nP ( directly observed(rob1, loc(cup1,ci) = true \u2223\u2223\u2223 loc(cup1) = ci)= 0.9\nP ( directly observed(rob1, loc(cup1),ci) = f alse \u2223\u2223\u2223 loc(cup1) = ci)= 0.1\nGiven DLR(T ) and the probabilistic information, the robot now has to execute a sequence of concrete actions that implement the desired transition T = \u3008\u03c31,aH ,\u03c32\u3009. For instance, a robot searching for cup1 in the kitchen can check cells in the kitchen for cup1 until either the cell location of cup1 is determined with high probability (e.g., \u2265 0.9), or all cells are examined without locating cup1. In the former case, the probabilistic belief can be elevated to a fully certain statement, and the robot reasons about the action outcome and observations to infer that cup1 is in the kitchen, whereas the robot infers that cup1 is not in the kitchen in the latter case. Such a probabilistic implementation of an abstract action as a sequence of concrete actions is accomplished by constructing and solving a POMDP, and repeatedly invoking the corresponding policy to choose actions until termination, as described below."}, {"heading": "8 POMDP Construction and Probabilistic Execution", "text": "In this section, we describe the construction of a POMDP P(T ) as a representation of the zoomed system description DLR(T ) and the learned probabilities of action outcomes (Section 7.2), and the use of P(T ) for the fine-resolution implementation of transition T = \u3008\u03c31,aH ,\u03c32\u3009 of \u03c4H . First, Section 8.1 summarizes the use of a POMDP to compute a policy for selecting one or more concrete actions that implement any given abstract action aH . Section 8.2 then describes the steps of the POMDP construction in more detail."}, {"heading": "8.1 POMDP overview", "text": "A POMDP is described by a tuple \u3008AP,SP,bP0 ,ZP,T P,OP,RP\u3009 for specific goal state(s). This formulation of a POMDP builds on the standard formulation [27], and the tuple\u2019s elements are:\n\u2022 AP: set of concrete actions available to the robot.\n\u2022 SP: set of p-states to be considered for probabilistic implementation of aH . A p-state is a projection of states of DLR(T ) on the set of atoms of the form f (t) = y, where f (t) is a basic non-knowledge fine-resolution fluent term, or a special p-state called the terminal p-state. We use the term \u201cp-state\u201d to differentiate between the states represented by the POMDP and the definition of state we use in this paper.\n\u2022 bP0 : initial belief state, where a belief state is a probability distribution over SP.\n\u2022 ZP: set of observations. An observation is a projection of states of DLR(T ) on the set of atoms of basic knowledge fluent terms corresponding to the robot\u2019s observation of the value of a fine-resolution fluent term, e.g., directly observed(robot, f (t),y) = outcome, where y is a possible outcome of the fluent term f (t). For simplicity, we use the observation none to replace all instances that have undet as the outcome.\n\u2022 T P : SP\u00d7AP\u00d7SP\u2192 [0,1], the transition function, which defines the probability of transitioning to each p-state when particular actions are executed in particular p-states.\n\u2022 OP : SP\u00d7AP\u00d7ZP \u2192 [0,1], the observation function, which defines the probability of each observation in ZP when particular actions are executed in particular p-states.\n\u2022 RP : SP\u00d7AP\u00d7SP\u2192\u211c, the reward specification, which encodes the relative immediate reward of taking specific actions in specific p-states.\nThe p-states are considered to be partially observable because they cannot be observed with complete certainty, and the POMDP reasons with probability distributions over the p-states, called belief states. Note that this formulation is only based on a system description and does not include any history of observations and actions\u2014in a standard POMDP formulation, the current p-state is assumed to be the result of all information obtained in previous time steps, i.e., the p-state is assumed to implicitly include the history of observations and actions.\nThe use of a POMDP has two phases (1) policy computation; and (2) policy execution. The first phase computes policy \u03c0P : BP \u2192 AP that maps belief states to actions, using an algorithm that maximizes the utility (i.e., expected cumulative discounted reward) over a planning horizon\u2014we use a point-based approximate solver [37]. In the second\nphase, the computed policy is used to repeatedly choose an action in the current belief state, updating the belief state after executing the action and receiving an observation. Belief revision is based on Bayesian updates:\nbPt+1(s P t+1) \u221d O(s P t+1,a P t+1,o P t+1)\u2211\nsPt\nT (sPt ,a P t+1,s P t+1) \u00b7bPt (sPt ) (27)\nwhere bPt+1 is the belief state at time t + 1. The belief update continues until policy execution is terminated. In our case, policy execution terminates when doing so has a higher (expected) utility than continuing to execute the policy. This happens when either the belief in a specific p-state is very high (e.g., \u2265 0.8), or none of the p-states have a high probability associated with them after invoking the policy several times\u2014the latter case is interpreted as the failure to execute the coarse-resolution action under consideration. We will use \u201cPOMDP-1\u201d to refer to the process of constructing a POMDP, computing the policy, and using this policy to implement the desired abstract action."}, {"heading": "8.2 POMDP construction", "text": "Next, we describe the construction of POMDP P(T ) for the fine-resolution probabilistic implementation of transition T = \u3008\u03c31,aH ,\u03c32\u3009 \u2208 \u03c4H , using DLR(T ) and the statistics collected in the training phase (Section 7.2). We illustrate these steps using examples based on the domain described in Example 1, including the example described in Appendix C.\nActions: the set AP of actions of P(T ) consists of concrete actions from the signature of DLR(T ) and new terminal actions that terminate policy execution. We use a single terminal action\u2014if AP is to include domain-specific terminal actions, it is the designer\u2019s responsibility to specify them. For the discussion below, it will be useful to partition AP into three subsets (1) AP1 , actions that cause a change in the p-states; (2) A P 2 , knowledge-producing actions for testing the values of fluents; and (3) AP3 , terminal actions that terminate policy execution. The example in Appendix C includes (a) actions from AP1 that move the robot to specific cells, e.g., move-0 and move-1 cause robot to move to cell 0 and 1 respectively, and the grasp action; (b) test actions from AP2 to check if the robot or target object (textbook) are in specific cells; and (c) action finish from AP3 that terminates policy execution. P-states, initial belief state and observations: the following steps are used to construct SP, ZP and bP0 .\n1. Construct ASP program \u03a0c(DLR(T ))\u222a \u03b4 nd . Here, \u03a0c(DLR(T )) is constructed as described in Definition 1 (Section 5.1), and \u03b4 nd is a collection of (a) atoms formed by statics; and (b) disjunctions of atoms formed by basic fluent terms. Each disjunction is of the form { f (t) = y1 \u2228 . . . \u2228 f (t) = yn}, where {y1, . . . ,yn} are possible values of basic fluent term f (t). Observe that AS is an answer set of \u03a0c(DLR(T ))\u222a \u03b4 nd iff it is an answer set of \u03a0c(DLR(T ))\u222ag for some g \u2208G, where G is the collection of sets of literals obtained by assigning unique values to each basic fluent term f (t) in \u03b4 nd . This statement follows from the definition of answer set and the splitting set theorem.\n2. Compute answer set(s) of ASP program \u03a0c(DLR(T ))\u222a\u03b4 nd . Based on the observation in Step-1 above, and the well-foundedness of DLR(T ), it is easy to show that each answer set is unique and is a state of DLR(T ).\n3. From each answer set, extract atoms of the form f (t) = y, where f (t) is a basic non-knowledge fine-resolution fluent term, to obtain an element of SP. Basic fluent terms corresponding to a coarse-resolution domain property, e.g., room location of the robot, are not represented probabilistically and thus not included in SP. We refer to such a projection of a state \u03b4 of DLR(T ) as the p-state defined by \u03b4 . Also include in SP an \u201cabsorbing\u201d terminal p-state absb that is reached when a terminal action from AP3 is executed.\n4. From each answer set, extract atoms formed by basic knowledge fluent terms corresponding to the robot sensing a fine-resolution fluent term\u2019s value, to obtain elements of ZP, e.g., directly observed(robot, f (t),y) = outcome. We refer to such a projection of a state \u03b4 of DLR(T ) as an observation defined by \u03b4 . As described earlier, for simplicity, observation none replaces all instances in ZP that have undet as the outcome.\n5. If all p-states are equally likely, the initial belief state bP0 is a uniform distribution. If some other distribution is to be used as the initial belief state, this information has to be provided by the designer. For instance, the designer may distribute (1\u2212 \u03b5), where \u03b5 is a small number such as 0.1, over p-states known to be more likely a priori, and distribute \u03b5 over the remaining p-states.\nIn the example in Appendix C, abstract action grasp(rob1, tb1) has to be executed in the o ff ice. To do so, the robot has to move and find tb1 in the o ff ice. Example 6 shows the corresponding DLR(T ), and \u03b4 nd includes (a) atoms formed by statics, e.g., next to(c1,c2) where c1 and c2 are neighboring cells in the o ff ice; and (b) disjunctions such as {loc(rob1) = c1 \u2228 . . . \u2228 loc(rob1) = cn} and {loc(tb1) = c1 \u2228 . . . \u2228 loc(tb1) = cn}, where {c1, . . . ,cn} \u2208 o ff ice. In Step 3, p-states such as {loc(rob1) = c1, loc(tb1) = c1, \u00acin hand(rob1,c1)} are extracted from the answer sets. In Step 4, observations such as directly observed(rob1, loc(rob1),c1) = true and directly observed(rob1, loc(tb1),c1) = f alse are extracted from the answer sets. Finally, the initial belief state bP0 is set as a uniform distribution (Step 5).\nTransition function and observation function: a transition between p-states of P(T ) is defined as \u3008si,a,s j\u3009 \u2208 T P iff there is an action a \u2208 AP1 and a transition \u3008\u03b4x,a,\u03b4y\u3009 of DLR(T ) such that si and s j are p-states defined by \u03b4x and \u03b4y respectively. The probability of \u3008si,a,s j\u3009 \u2208 T P equals the probability of \u3008\u03b4x,a,\u03b4y\u3009. In a similar manner, \u3008si,a,z j\u3009 \u2208OP iff there is an action a \u2208 AP2 and a transition \u3008\u03b4x,a,\u03b4y\u3009 of DLR(T ) such that si and z j are a p-state and an observation defined by \u03b4x and \u03b4y respectively. The probability of \u3008si,a,z j\u3009 \u2208 OP equals the probability of \u3008\u03b4x,a,\u03b4y\u3009.\nWe construct T P and OP from DLR(T ) and the statistics collected in the initial training phase (as described in Section 7.2). First, we augment DLR(T ) with causal laws for proper termination:\nf inish causes absb\nimpossible AP if absb\nNext, we note that actions in AP1 cause p-state transitions but provide no observations, while actions in A P 2 do not cause p-state changes but provide observations, and terminal actions in AP3 cause transition to the absorbing state and provide no observations. To use state of the art POMDP solvers, we need to represent T P and OP as a collection of tables, one for each action. More precisely, T Pa [si,s j] = p iff \u3008si,a,s j\u3009 \u2208 T P and its probability is p. In a similar manner, OPa [si,z j] = p iff \u3008si,a,z j\u3009 \u2208 OP and its probability is p. Algorithm 1 describes the construction of T P and OP.\nSome specific steps of Algorithm 1 are elaborated below.\n\u2022 After initialization, Lines 3\u201312 of Algorithm 1 handle special cases. For instance, any terminal action will cause a transition to the terminal p-state and provide no observations (Lines 4-5).\n\u2022 An ASP program of the form \u03a0(DLR(T ),si,Dis j(A)) (Lines 12, 15) is defined as \u03a0(DLR(T ))\u222a val(si,0)\u222a Dis j(A). Here, Dis j(A) is a disjunction of the form {occurs(a1,0)\u2228 . . .\u2228occurs(an,0)}, where {a1, . . . ,an} \u2208A. Lines 14-16 construct and compute answer sets of such a program to identify all possible p-state transitions as a result of actions in AP1 \u2014Lines 17-19 construct and compute answer set of such a program to identify possible observations as a result of actions in AP2 .\n\u2022 Line 16 extracts a statement of the form occurs(ak \u2208 AP1 ,0), and p-state s j \u2208 SP, from each answer set AS, to obtain p-state transition \u3008si,ak,s j\u3009. As stated earlier, a p-state is extracted from an answer set by extracting atoms formed by basic non-knowledge fluent terms.\n\u2022 Line 19 extracts a statement of the form occurs(a j \u2208 AP2 ,0), and observation z j \u2208 ZP, from each answer set AS, to obtain triple \u3008si,ak,z j\u3009. As described earlier, an observation is extracted from an answer set by extracting atoms formed by basic knowledge fluent terms.\n\u2022 Use probabilities obtained experimentally (Section 7.2) to set probabilities of p-state transitions (Line 16) and observations (Line 19).\nIn the example in Appendix C, a robot in the o ff ice has to pick up a textbook tb1 believed to be in the o ff ice. This example assumes that a move action from one cell to a neighboring cell succeeds with probability 0.95\u2014with probability 0.05 the robot remains in its current cell. It is also assumed that with probability 0.95 the robot observes (does not observe) the textbook when it exists (does not exist) in the cell the robot is currently in. The corresponding T P and OP, constructed for this example, are shown in Appendix C.\nThe correctness of the approach used to extract p-state transitions and observations, in Lines 16, 19 of Algorithm 1, is based on the following propositions.\nAlgorithm 1: Constructing POMDP transition function T P and observation function OP\nInput: SP, AP, ZP, DLR(T ); transition probabilities for actions \u2208 AP1 ; observation probabilities for actions \u2208 AP2 . Output: POMDP transition function T P and observation function OP.\n1 Initialize T P as |SP|\u00d7 |SP| identity matrix for each action. 2 Initialize OP as |SP|\u00d7 |ZP| matrix of zeros for each action. /* Handle special cases */ 3 for each a j \u2208 AP3 do 4 T Pa j(\u2217,absb) = 1 5 OPa j(\u2217,none) = 1 6 end 7 for each action a j \u2208 AP1 do 8 OPa j(\u2217,none) = 1 9 end\n10 for each a j \u2208 AP do 11 OPa j(absb,none) = 1 12 end\n/* Handle normal transitions */\n13 for each p-state si \u2208 SP do /* Construct and set probabilities of p-state transitions */ 14 Construct ASP program \u03a0(DLR(T ),si,Dis j(AP1 )). 15 Compute answer sets AS of ASP program. 16 From each AS \u2208 AS, extract p-state transition \u3008si,ak,s j\u3009, and set the probability of T Pak [si,s j].\n/* Construct and set probabilities of observations */\n17 Construct ASP program \u03a0(DLR(T ),si,Dis j(AP2 )). 18 Compute answer sets AS of ASP program. 19 From each AS \u2208 AS, extract triple \u3008si,ak,z j\u3009, and set value of OPak [si,z j]. 20 end 21 return T P and OP.\nProposition 3. [Extracting p-state transitions from answer sets]\n\u2022 If \u3008si,a,s j\u3009 \u2208 T P then there is an answer set AS of program \u03a0(DLR(T ),si,Dis j(AP1 )) such that s j = { f (x\u0304) = y : f (x\u0304) = y \u2208 AS and is basic}.\n\u2022 For every answer set AS of program \u03a0(DLR(T ),si,Dis j(AP1 )) and s j = { f (x\u0304) = y : f (x\u0304) = y \u2208 AS and is basic}, \u3008si,a,s j\u3009 \u2208 T P.\nProposition 4. [Extracting observations from answer sets]\n\u2022 If \u3008si,a,z j\u3009 \u2208 OP then there is an answer set AS of program \u03a0(DLR(T ),si,Dis j(AP2 )) such that z j = { f (x\u0304) = y : f (x\u0304) = y \u2208 AS and is basic}.\n\u2022 For every answer set AS of program \u03a0(DLR(T ),si,Dis j(AP1 )) and z j = { f (x\u0304) = y : f (x\u0304) = y \u2208 AS and is basic}, \u3008si,a,z j\u3009 \u2208 OP.\nIt is possible to show that these propositions are true based on the definition of an answer set, the definition of the zoomed system description DLR(T ), and the definition of the POMDP components.\nReward specification: the reward function RP assigns a real-valued reward to each p-state transition, as described in Algorithm 2. Specifically, for any state transition with a non-zero probability in T P:\nAlgorithm 2: Construction of POMDP reward function RP\nInput: SP, AP, and T P; statistics regarding accuracy and time taken to execute non-terminal actions. Output: Reward function RP.\n/* Consider each possible p-state transition */\n1 for each (s,a,s\u2032) \u2208 SP\u00d7AP\u00d7SP with T P(s,a,s\u2032) 6= 0 do /* Consider terminal actions first */ 2 if a \u2208 AP3 then 3 if s\u2032 is a goal p-state then 4 RP(s,a,s\u2032) = large positive value. 5 else 6 RP(s,a,s\u2032) = large negative value. 7 end\n/* Rewards are costs for non-terminal actions */\n8 else 9 Set RP(s,a,s\u2032) based on relative computational effort and accuracy.\n10 end 11 end 12 return RP\n1. If it involves a terminal action from AP3 , the reward is a large positive (negative) value if this action is chosen after (before) achieving the goal p-state.\n2. If it involves non-terminal actions, reward is a real-valued cost (i.e., negative reward) of action execution.\nHere, any p-state s \u2208 SP defined by state \u03b4 of DLR(T ) that is a refinement of \u03c32 in transition T = \u3008\u03c31,aH ,\u03c32\u3009 is a goal p-state. In Appendix C, we assign large positive reward (100) for executing f inish when textbook tb1 is in the robot\u2019s grasp, and large negative reward (\u2212100) for terminating before tb1 has been grasped. We assign a fixed cost (\u22121) for all other (i.e., non-terminal) actions. However, as stated earlier, this cost can be a heuristic function of both relative computational effort and accuracy, using domain expertise and statistics collected experimentally. For instance, statistics may indicate that a knowledge-producing action that determines an object\u2019s color takes twice as much time as the action that determines the object\u2019s shape, which can be used to set RP(\u2217,shape,\u2217) = \u22121 and RP(\u2217,color,\u2217) =\u22122. The reward function, in turn, influences the (a) rate of convergence during policy computation; and (b) accuracy of results during policy execution. Appendix C describes the reward function for a specific example.\nComputational efficiency: Solving POMDPs can be computationally expensive, even with state of the art approximate solvers. For specific tasks such as path planning, it may also be possible to use specific heuristic or probabilistic algorithms that are more computationally efficient than a POMDP. However, POMDPs provide a (a) principled and quantifiable trade-off between accuracy and computational efficiency in the presence of uncertainty in both sensing and actuation; and (b) near-optimal solution if the POMDP\u2019s components are modeled correctly. With a POMDP, the computational efficiency can be improved further by \u201cfactoring\u201d the p-state estimation problem into sub-problems that model actions and observations influencing one fluent independent of those influencing other fluents. Such a factoring of the problem is not always possible, e.g., when a robot is holding a textbook in hand, the robot\u2019s location and the textbook\u2019s location are not independent. Instead, in our architecture, we preserve such constraints while still constructing a POMDP for the relevant part of the domain to significantly reduce the computational complexity of solving the POMDP. Furthermore, many of the POMDPs required for a given domain can be precomputed, solved and reused. For instance, if the robot has constructed a POMDP for the task of locating a textbook in a room, the POMDP for locating a different book (or even a different object) in the same room may only differ in the values of some transition probabilities, observation probabilities, and rewards. Note that this similarity between tasks may not hold in non-stationary domains, in which the elements of the POMDP tuple (e.g., set of p-states), and the collected statistics (e.g., transition probabilities), may need to be revised over time.\nComputational error: Although the outcomes of POMDP policy execution are non-deterministic, following an optimal policy produced by an exact POMDP solver is most likely (among all such possible policies) to take the robot to a goal p-state if the following conditions hold:\n\u2022 The coarse-resolution transition diagram \u03c4H of the domain has been constructed correctly;\n\u2022 The statistics collected in the initial training phase (Section 7.2) correctly model the domain dynamics; and\n\u2022 The reward function is constructed to suitably reward desired behavior.\nThis statement is based on existing literature [27, 35, 43]. We use an approximate POMDP solver for computational efficiency, and an exact belief update (Equation 27), which provides a bound on the regret (i.e., loss in value) achieved by following the computed policy in comparison with the optimal policy [37]. We can thus only claim that the outcomes of executing of our policy are approximately correct with high probability. We can also provide a bound on the margin of error [37]. For instance, if the probability associated with a statement in the fine-resolution representation is p, the margin of error in a commitment made to the history H (in the coarse-resolution representation) based on this statement is (1\u2212 p). If a set of statements with probabilities pi are used to arrive at a conclusion that is committed to H , (1\u2212\u220fi pi) is the corresponding error."}, {"heading": "9 Reasoning System and Control Loop of Architecture", "text": "Next, we give a more detailed description of the reasoning system and control loop of our architecture for building intelligent robots. For this description, we (once again) view a robot as consisting of a logician and a statistician, who communicate through a controller, as described in Section 1 and shown in Figure 4. For any given goal, the logician takes as input the system description DH that corresponds to a coarse-resolution transition diagram \u03c4H , recorded history H with initial state defaults (see Example 2), and the current coarse-resolution state \u03c31 (potentially inferred from observations). If recent recorded observations differ from the logician\u2019s predictions, the discrepancies are diagnosed and a plan comprising one or more abstract actions is computed to achieve the goal. Planning and diagnostics are reduced to computing answer sets of the CR-Prolog program \u03a0(DH ,H ). For a given goal, the controller uses the\ntransition T corresponding to the next abstract action aH in the computed plan to zoom to DLR(T ), the part of the randomized fine-resolution system description DLR that is relevant to the T . A POMDP is then constructed from DLR(T ) and the learned probabilities, and solved to obtain a policy. The POMDP and the policy are communicated to the statistician who invokes the policy repeatedly to implement the abstract action aH as a sequence of (more) concrete actions. When the POMDP policy is terminated, the corresponding observations are sent to the controller. The controller performs inference in DLR(T ), recording the corresponding coarse-resolution action outcomes and observations in the coarse-resolution history H , which is used by the logician for subsequent reasoning.\nAlgorithm 3: Control loop Input: coarse-resolution system description DH and history H ; randomized fine-resolution system description\nDLR; coarse-resolution description of the goal; coarse-resolution initial state \u03c31. Output: robot is in a state satisfying the goal; reports failure if this is impossible.\n1 while goal is not achieved do 2 Logician uses DH and H to find a possible plan, aH1 , . . . ,a H n to achieve the goal. 3 if no plan exists then 4 return failure 5 end 6 i := 1 7 continue1 := true 8 while continue1 do 9 Check pre-requisites of aHi .\n10 if pre-requisites not satisfied then 11 continue1 := false 12 else 13 Controller zooms to DLR(T ), the part of DLR relevant to transition T = \u3008\u03c31,aHi ,\u03c32\u3009 and constructs a POMDP. 14 Controller solves POMDP to compute a policy to implement aHi . 15 continue2 := true 16 while continue2 do 17 Statistician invokes policy to select and execute an action, obtain observation, and update belief state. 18 if terminal action executed then 19 Statistician communicates observations to the controller. 20 continue2 = false 21 i := i+1 22 continue1 := (i < n+1) 23 end 24 Controller performs fine-resolution inference, recording action outcomes and observations in H . 25 \u03c31 = current coarse-resolution state. 26 end 27 end 28 end\nAlgorithm 3 describes the overall control loop for achieving the assigned goal. Correctness of this algorithm (with a certain margin of error) is ensured by:\n1. Applying the planning and diagnostics algorithm discussed in Section 5.2 for planning with \u03c4H and H ;\n2. Using the formal definitions of refinement and zoom described in Section 7; and\n3. Using a POMDP to probabilistically plan an action sequence and executing it for each aH of the logician\u2019s plan, as discussed in Section 8.\nThe probabilistic planning is also supported by probabilistic state estimation algorithms that process inputs from sensors and actuators. For instance, the robot builds a map of the domain and estimates its position in the map using a Particle Filter algorithm for Simultaneous Localization and Mapping (SLAM) [49]. This algorithm represents the true underlying probability distribution over the possible states using samples drawn from a proposal distribution. Samples more likely to represent the true state, determined based on the degree of match between the expected and actual sensor observations of domain landmarks, are assigned higher (relative) weights and re-sampled to incrementally converge to the true distribution. Implementations of the particle filtering algorithm are used widely in the robotics literature to track multiple hypotheses of system state. A similar algorithm is used to estimate the pose of the robot\u2019s arm. On the physical robot, other algorithms used to process specific sensor inputs. For instance, we use existing implementations of algorithms to process camera images, which are the primary source of information to identify specific domain objects. The robot also uses an existing implementation of a SLAM algorithm to build a domain map and localize itself in the map. These algorithms are summarized in Section 10, when we discuss experiments on physical robots."}, {"heading": "10 Experimental Setup and Results", "text": "This section describes the experimental setup and results of evaluating the architecture\u2019s capabilities."}, {"heading": "10.1 Experimental setup", "text": "The proposed architecture was evaluated in simulation and on a physical robot. As stated in Section 8, statistics of action execution, e.g., observed outcomes of all actions and computation time for knowledge producing actions, are collected in an initial training phase. These statistics are used by the controller to compute the relative utility of different actions, and the probabilities of obtaining different action outcomes and observations. The simulator uses these statistics to simulate the robot\u2019s movement and perception. In addition, the simulator represents objects using probabilistic functions of features extracted from images, with the corresponding models being acquired in an initial training phase\u2014see [52] for more details about such models.\nIn each experimental trial, the robot\u2019s goal was to find and move specific objects to specific places\u2014the robot\u2019s location, the target object, and locations of domain objects were chosen randomly. An action sequence extracted from an answer set of the ASP program provides a plan comprising abstract actions, each of which is executed probabilistically. Our proposed architecture, henceforth referred to as \u201cPA\u201d, was compared with: (1) POMDP-1; and (2) POMDP-2, which revises POMDP-1 by assigning specific probability values to default statements to bias the initial belief. The performance measures were: (a) success, the fraction (or %) of trials in which the robot achieved the assigned goals; (b) planning time, the time taken to compute a plan to achieve the assigned goal; and (c) the average number of actions that were executed to achieve the desired goal. We evaluate the following three key hypotheses:\nH1 PA simplifies design in comparison with architectures based on purely probabilistic reasoning and increases confidence in the correctness of the robot\u2019s behavior;\nH2 PA achieves the assigned goals more reliably and efficiently than POMDP-1; and\nH3 Our representation for defaults improves reliability and efficiency in comparison with not using defaults or assigning specific probability values to defaults.\nWe examine the first hypothesis qualitatively in the context of some execution traces grounded in the illustrative domain described in Example 1 (Section 10.2). We then discuss the quantitative results corresponding to the experimental evaluation of the other two hypotheses in simulation and on physical robots (Section 10.3)."}, {"heading": "10.2 Execution traces", "text": "The following (example) execution traces illustrate some of the key capabilities of the proposed architecture.\nExecution Example 1. [Planning with default knowledge] Consider the scenario in which a robot is assisting with a meeting in the o ff ice, i.e., loc(rob1,o ff ice), and is assigned a goal state that contains:\nloc(cup1,o ff ice)\nwhere the robot\u2019s goal is to move coffee cup cup1 to the o ff ice.\n\u2022 The plan of abstract actions, as created by the logician, is:\nmove(rob1,kitchen)\ngrasp(rob1,cup1)\nmove(rob1,o ff ice)\nputdown(rob1,cup1)\nNote that this plan uses initial state default knowledge that kitchenware are usually found in the kitchen. Each abstract action in this plan is executed by computing and executing a sequence of concrete actions.\n\u2022 To implement move(rob1,kitchen), the controller constructs DLR(T ) by zooming to the part of DLR relevant to this action. For instance, only cells in the kitchen and the o ff ice are possible locations of rob1, and move is the only action that can change the physical state, in the fine-resolution representation.\n\u2022 DLR(T ) is used to construct and solve a POMDP to obtain an action selection policy, which is provided to the statistician. The statistician repeatedly invokes this policy to select actions (until a terminal action is selected) that are executed by the robot. In the context of Figure 3(b), assume that the robot moved from cell c1 \u2208 o ff ice to c5 \u2208 kitchen (through cell c2 \u2208 o ff ice) with high probability.\n\u2022 The direct observation from the POMDP, directly observed(rob1, loc(rob1),c5) = true, is used by the controller for inference in DLR(T ) and DL, e.g., to produce observed(rob1, loc(rob1),kitchen). The controller adds this information to the coarse-resolution history H of the logician, e.g., obs(loc(rob1) = kitchen,1). Since the first abstract action has had the expected outcome, the logician sends the next abstract action in the plan, grasp(rob1,cup1) to the controller for implementation.\n\u2022 A similar sequence of steps is performed for each abstract action in the plan, e.g., to grasp cup1, the robot locates the coffee cup in the kitchen and then picks it up. Subsequent actions cause rob1 to move cup1 to the o ff ice, and put cup1 down to achieve the assigned goal.\nExecution Example 2. [Planning with unexpected failure] Consider the scenario in which a robot in the o ff ice is assigned the goal of fetching textbook tb1, i.e., the initial state includes loc(rob1,o ff ice), and the goal state includes:\nloc(tb1,o ff ice)\nThe coarse-resolution system description DH and history H , along with the goal, are passed on to the logician.\n\u2022 The plan of abstract actions, as created by the logician, is:\nmove(rob1,main library)\ngrasp(rob1, tb1)\nmove(rob1,o ff ice)\nputdown(rob1, tb1)\nThis plan uses the initial state default knowledge that textbooks are typically in the main library (Statement 8). Each abstract action in this plan is executed by computing and executing a sequence of concrete actions.\n\u2022 Assume that loc(rob1,main library), i.e., that the robot is in the main library after successfully executing the first abstract action. To execute the grasp(rob1, tb1) action, the controller constructs DLR(T ) by zooming to the part of DLR relevant to this action. For instance, only cells in the main library are possible locations of rob1 and tb1 in the fine-resolution representation.\n\u2022 DLR(T ) is used to construct and solve a POMDP to obtain a action selection policy, which is provided to the statistician. The statistician repeatedly invokes this policy to select actions (until a terminal action is selected) that are executed by the robot. In the context of Figure 3(b), if r2 is the main library, the robot may move to and search for tb1 in each cell in r2, starting from its current location.\n\u2022 The robot unfortunately does not find tb1 in any of the cells in the main library in the second step. These observations from the POMDP, i.e., directly observed(rob1, loc(tb1),ci) = f alse for each ci \u2208 main library, are used by the controller for inference in DLR(T ) and DL, e.g., to produce observed(rob1, loc(tb1),main library)= f alse. The controller adds this information to the coarse-resolution history H of the logician, e.g., obs(loc(tb1) 6= main library,2).\n\u2022 The inconsistency caused by the observation is resolved by the logician using a CR rule, and the new plan is created based on the second initial state default that a textbook not in the main library is typically in the aux library (Statement 9):\nmove(rob1,aux library)\ngrasp(rob1, tb1)\nmove(rob1,o ff ice)\nputdown(rob1, tb1)\n\u2022 This time, the robot is able to successfully execute each abstract action in the plan, i.e., it is able to move to the aux library, find tb1 and grasp it, move back to the o ff ice, and put tb1 down to achieve the assigned goal.\nBoth these examples illustrate key advantages provided by the formal definitions, e.g., of the different system descriptions and the tight coupling between the system descriptions, which are part of the proposed architecture:\n1. Once the designer has provided the domain-specific information, e.g., for refinement or computing probabilities of action outcomes, no further input is necessary to automate planning, diagnostics, and execution for any given goal.\n2. Attention is automatically directed to only the relevant part of the available knowledge at the appropriate resolution. For instance, reasoning by the logician (statistician) is restricted to a coarse-resolution (fine-resolution) system description. It is thus easier to understand, and to identify and fix errors in, the observed behavior, in comparison with architectures that consider all the available knowledge or only support probabilistic reasoning [53].\n3. There is smooth transfer of control and relevant knowledge between the components of the architecture, and confidence in the correctness of the robot\u2019s behavior. Also, the proposed methodology supports the use of this architecture on different robots in different domains, e.g., Section 10.3 describes the result of using this architecture on mobile robots in two different indoor domains.\nNext, we describe the experimental evaluation of the hypotheses H2 and H3 in simulation and on a mobile robot."}, {"heading": "10.3 Experimental results", "text": "To evaluate hypothesis H2, we first compared PA with POMDP-1 in a set of trials in which the robot\u2019s initial position is known but the position of the object to be moved is unknown. The solver used in POMDP-1 was evaluated with different fixed amounts of time for computing action policies. Figure 5 summarizes the results; each point is the average of 1000 trials, and we set (for ease of interpretation) each room to have four cells. The brown-colored plots\nin Figure 5 represent the ability to successfully achieve the assigned goal (y-axis on the left), as a function of the number of cells in the domain. The blue-colored plots show the number of actions executed before termination. For the plots corresponding to POMDP-1, the number of actions the robot is allowed to execute before it has to terminate is set to 50. We note that PA significantly improves the robot\u2019s ability to achieve the assigned goal in comparison with POMDP-1. As the number of cells (i.e., size of the domain) increases, it becomes computationally difficult to generate good policies with POMDP-1. The robot needs a greater number of actions to achieve the goal and there is a loss in accuracy if the limit on the number of actions the robot can execute before termination is reduced. While using POMDP-1, any incorrect observations (e.g., false positive sightings of objects) significantly impacts the ability to successfully complete the trials. PA, on the other hand, focuses the robot\u2019s attention on relevant regions of the domain (e.g., specific rooms and cells), and it is thus able to recover from errors and operate efficiently.\nNext, we evaluated the time taken by PA to generate a plan as the size of the domain increases. We characterize domain size based on the number of rooms and the number of objects in the domain. We conducted three sets of experiments in which the robot reasons with: (1) all available knowledge of domain objects and rooms; (2) only knowledge relevant to the assigned goal\u2014e.g., if the robot knows an object\u2019s default location, it need not reason about other objects and rooms in the domain to locate this object; and (3) relevant knowledge and knowledge of an additional\n20% of randomly selected domain objects and rooms. Figures 6(a)-6(c) summarize these results. We observe that using just the knowledge relevant to the goal to be accomplished significantly reduces the planning time. PA supports the identification of such knowledge based on the refinement and zooming operations described in Section 7. As a result, robots equipped with PA will be able to generate appropriate plans for domains with a large number of rooms and objects. Furthermore, if we only use a probabilistic approach (POMDP-1), it soon becomes computationally intractable to generate a plan for domains with many objects and rooms. These results are not shown in Figure 6, but they are documented in prior papers evaluating just the probabilistic component of the proposed architecture [47, 52].\nTo evaluate hypothesis H3, i.e., to evaluate our representation and use of default knowledge, we first conducted trials in which PA was compared with PA\u2217, a version that does not include any default knowledge, e.g., when the robot is asked to fetch a textbook, there is no prior knowledge regarding the location of textbooks, and the robot explores the closest location first. Figure 7 summarizes the average number of actions executed per trial as a function of the number of rooms in the domain\u2014each sample point in this figure is the average of 10000 trials. The goal in each trial is (as before) to move a specific object to a specific place. We observe that our (proposed) representation and use of default knowledge significantly reduces the number of actions (and thus time) required to achieve the assigned goal.\nNext PA was compared with POMDP-2, a version of POMDP-1 that assigns specific probability values to default knowledge (e.g., \u201ctextbooks are in the library with probability 0.9\u201d) and suitably revises the initial belief state. The goal (once again) was to find and move objects to specific locations, and we measured the ability to successfully achieve the assigned goal and the number of actions executed before termination. Figures 8-9 summarize the corresponding results under two extreme cases representing a perfect match (mismatch) between the default locations and ground truth locations of objects. In Figure 8, the ground truth locations of target objects (unknown to the robot) match the default locations of the objects, i.e., there are no exceptions to the default statements. We observe that as the probability assigned to the default statement increases, the number of actions executed by the robot decreases and\nthe fraction of trials completed successfully increases. However, for larger values along the x-axis, the difference in the robot\u2019s performance for two different values of the probability (assigned to defaults) is not that significant. In Figure 8, the ground truth locations of the target objects never match the default locations of the objects, i.e., unknown to the robot, all trials correspond to exceptions to the default knowledge. In this case, the robot executes many more actions before termination and succeeds in a smaller fraction of trials as the probability value assigned to default statements increases. We also repeated these experimental trials after varying the extent to which the ground truth locations of objects matched their default locations. We noticed that when the probability assigned to default statements accurately reflects the ground truth, the number of trials in which the robot successfully achieves the goal increases and approaches the performance obtained with PA. However, recall that computing the probabilities of default statements accurately takes a lot of time and effort. Also, these probabilities may change over time and the robot\u2019s ability to achieve the assigned goals may be sensitive to these changes, making it difficult to predict the robot\u2019s behavior with confidence. In addition, it is all the more challenging to accurately represent and efficiently use probabilistic information about prioritized defaults (e.g., Example 2). In general, we observed that the effect of assigning a probability value to defaults is arbitrary depending on factors such as (a) the numerical value chosen; and (b) the degree of match between ground truth and the default information. For instance, if a large probability is assigned to the default knowledge that books are typically in the library, but the book the robot has to move is an exception to the default (e.g., a cookbook), it takes significantly longer for POMDP-2 to revise (and recover from) the initial belief. PA, on the other hand, supports elegant representation of, and reasoning with, defaults and exceptions to these defaults.\nRobot Experiments: In addition to the trials in simulated domains, we implemented and evaluated PA with POMDP1 on physical robots using the Robot Operating System (ROS). We conducted experimental trials with two robot platforms (see Figure 1) in variants of the domain described in Example 1. Visual object recognition is based on learned object models that consist of appearance-based and contextual visual cues [34]. Since, in each trial, the robot\u2019s initial location and the target object(s) are chosen randomly, it is difficult to compute a meaningful estimate of variance, and statistical significance is established through paired trials. In each paired trial, for each approach being compared (e.g., PA or POMDP-1), the target object(s), the robot\u2019s initial location, and the location of domain objects are the same, and the robot has the same initial domain knowledge.\nFirst, we conducted 50 trials on two floors of our Computer Science department building. This domain includes places in addition to those included in our illustrative example, e.g., Figure 1(a) shows a subset of the domain map of the third floor of the building, and Figure 1(b) shows the Peoplebot wheeled robot platform used in these trials. The robot is equipped with a stereo camera, laser range finder, microphone, speaker, and a laptop running Ubuntu Linux that performs all the processing. The domain maps are learned and revised by the robot using laser range finder data and the existing ROS implementation of a SLAM algorithm [13]. This robot has a manipulator arm that can be moved to reachable 3D locations relative to the robot. However, since robot manipulation is not a focus of this work, once the robot is next to the desired object, it extends its gripper and asks for the object to be placed in it. For experimental trials\non the third floor, we considered 15 rooms, which includes faculty offices, research labs, common areas and a corridor. To make it feasible to use POMDP-1 in such large domains, we used our prior work on a hierarchical decomposition of POMDPs for visual sensing and information processing that supports automatic belief propagation across the levels of the hierarchy and model generation in each level of the hierarchy [47, 52]. The experiments included paired trials, e.g., over 15 trials (each), POMDP-1 takes 1.64 as much time as PA (on average) to move specific objects to specific places. For these paired trials, this 39% reduction in execution time provided by PA is statistically significant: p-value = 0.0023 at the 95% significance level.\nConsider a trial in which the robot\u2019s objective is to bring a specific textbook to the place named study corner. The robot uses default knowledge to create a plan of abstract actions that causes the robot to move to and search for the textbook in the main library. When the robot does not find this textbook in the main library after searching using a suitable POMDP policy, replanning by the logician causes the robot to investigate the aux library. The robot finds the desired textbook in the aux library and moves it to the target location. A video of such an experimental trial can be viewed online at http://youtu.be/8zL4R8te6wg\nTo explore the applicability of PA in different domains, we also conducted 40 experimental trials using the Turtlebot wheeled robot platform in Figure 1(c) in a variant of the illustrative domain in Example 1. This domain had three rooms in the Electrical Engineering department building arranged to mimic a robot operating as a robot butler, with additional objects (e.g., tables, chairs, food items etc). The robot was equipped with a Kinect (RGB-D) sensor, a laser range finder, and a laptop running Ubuntu Linux that performs all the processing. As before, the robot used the ROS implementation of a SLAM algorithm, and a hierarchical decomposition of POMDPs for POMDP-1. This robot did not have a manipulator arm\u2014once it reached a location next to the location of the desired object, it asks for the object to be placed on it. The experiments included paired trials, e.g., in 15 paired trials, POMDP-1 takes 2.3 as much time as PA (on average) to move specific objects to specific places\u2014this reduction in execution time by PA is statistically significant at the 95% significance level.\nConsider a trial in which the robot\u2019s goal was to fetch a bag of crisps for a human. The robot uses default knowledge about the location of the bag of crisps, to create a plan of abstract actions that causes the robot to first move to the kitchen and search for the bag of crisps. The robot finds the bag of crisps, asks for the bag to be placed on it (since it has no manipulator), and moves back to table1 in lab1 (the location of the human who wanted the crisps) only to be told that it has brought a bag of chocolates instead. The robot diagnoses the cause for this error (human gave it the incorrect bag in the kitchen), goes back and fetches the correct bag (of crisps) this time. A video of this trial can be viewed online at https://vimeo.com/136990534"}, {"heading": "11 Conclusions", "text": "This paper described a knowledge representation and reasoning architecture that combines the complementary strengths of declarative programming and probabilistic graphical models. The architecture is based on tightly-coupled transition diagrams that represent domain knowledge, and the robot\u2019s abilities and goals, at two levels of granularity. The architecture makes several key contributions.\n\u2022 Action language ALd is extended to support non-Boolean fluents and non-deterministic causal laws, and is used to describe the coarse-resolution and fine-resolution transition diagrams.\n\u2022 The notion of history of a dynamic domain is extended to include default knowledge in the initial state, and a model of this history is defined. These definitions are used to define a notion of explanation of unexpected observations, and to provide an algorithm for coarse-resolution planning and diagnostics. The algorithm is based on the translation of a history into a program of CR-Prolog and computed answer sets of this program. The desired plan and, if necessary, explanations are extracted from this answer set.\n\u2022 A formal definition is provided of one transition diagram being a refinement of another transition diagram, and the fine-resolution diagram is defined as a refinement of the coarse-resolution transition diagram of the domain.\n\u2022 The randomization of the fine-resolution transition diagram is defined, and an algorithm is provided for experimental collection of statistics. These statistics are used to compute the probabilities of action outcomes and observations at the fine-resolution.\n\u2022 A formal definition is provided for zooming to a part of the randomized fine-resolution diagram relevant to the execution of any given coarse-resolution (abstract) action. This definition is used to automate the zoom operation during execution of the coarse-resolution plan.\n\u2022 We also provide an algorithm that uses the computed probabilities and the zoomed part of the fine-resolution transition diagram, to automatically construct data structures appropriate for the probabilistic implementation of any given abstract action. The outcomes of probabilistic reasoning update the coarse-resolution history for subsequent reasoning.\n\u2022 Finally, and possibly one of the major contributions, is that we articulate a general methodology for the design of software components of robots that are re-taskable and robust. It simplifies the use of this architecture in other domains, provides a path to predict the robot\u2019s behavior, and thus increases confidence in the correctness of the robot\u2019s behavior.\nIn this paper, the domain representation for non-monotonic logical reasoning at coarse-resolution is translated to a CRProlog program, while the representation for probabilistic reasoning is translated to a POMDP. These choices allow us to reason reliably and efficiently with hierarchically organized knowledge, and to provide a single framework for inference, planning and explanation generation, and for a quantifiable trade off between accuracy and computational efficiency in the presence of probabilistic models of uncertainty in sensing and actuation. Experimental results in simulation and on physical robots indicate that the architecture supports reasoning at the sensorimotor level and the cognitive level with violation of defaults, noisy observations and unreliable actions, and has the potential to scale well to complex domains.\nThe proposed architecture open up many directions for further research, some of which relax the constraints imposed in the design of our current architecture. First, we will further explore the tight coupling between the transition diagrams, and between logical and probabilistic reasoning, in dynamic domains. We have, for instance, explored different resolutions for reasoning probabilistically [12], and investigated the inference, planning and diagnostics capabilities of architectures that reason at different resolutions [53]. However, we have so far not explored non-stationary domains, a limiting constraint that we seek to relax in future work. Second, our architecture has so far focused on a single robot, although we have instantiated the architecture in different domains. Another direction of further research is to extend the architecture to enable collaboration between a team of robots working towards a shared goal. It is theoretically possible to extend our architecture to work on multiple robots, but it will open up challenging questions and choices regarding communication (between robots) and propagation of beliefs of a robot and its teammates. Third, the proposed architecture has focused on representation and reasoning with incomplete knowledge, but a robot collaborating with humans in a dynamic domain also needs to be able to learn from its experiences. Preliminary work in this direction, e.g., based on combining relational reinforcement learning with declarative programming, has provided some promising results [46, 45], and we seek to further explore this direction of work in the future. The long-term objective is to better understand the coupling between non-monotonic logical reasoning and probabilistic reasoning, and to use this understanding to develop architectures that enable robots to assist humans in complex domains."}, {"heading": "Acknowledgments", "text": "This work was supported in part by the U.S. Office of Naval Research Science of Autonomy Award N00014-13-1-0766 (Mohan Sridharan, Shiqi Zhang), the Asian Office of Aerospace Research and Development award FA2386-16-1-4071 (Mohan Sridharan), and the EC-funded Strands project FP7-IST-600623 (Jeremy Wyatt). Opinions and conclusions in this paper are those of the authors."}, {"heading": "A Proof of Proposition 1", "text": "In this section, we prove Proposition 1, which states that:\nA path M = \u3008\u03c30,a0,\u03c31, . . . ,\u03c3n\u22121,an\u3009 of \u03c4(D) is a model of history H n iff there is an answer set AS of a program \u03a0(D ,H ) such that:\n1. A fluent literal ( f = y) \u2208 \u03c3i iff val( f ,y, i) \u2208 AS,\n2. A fluent literal ( f 6= y1) \u2208 \u03c3i iff val( f ,y2, i) \u2208 AS,\n3. An action e \u2208 ai iff occurs(e, i) \u2208 AS.\nFor our proof, we will need the following Lemma:\nLemma 1. Let \u03c30 be a state of the transition diagram of system description D , \u3008a0, . . . ,an\u22121\u3009 be a sequence of actions from the D\u2019s signature, and\nP =de f \u03a0(D)\u222a val(\u03c30,0)\u222a{occurs(ai, i) : 0\u2264 i < n}\nIf D is well-founded then every answer set of P defines a path of \u03c4(D).\nProof of the proposition3: First, we use the Splitting Set Theorem for CR-Prolog [4] to simplify \u03a0(D ,H ) by eliminating from it occurrences of atoms formed by relations obs and hpd. Let us denote the set of such atoms by U . To apply the theorem it is sufficient to notice that no cr-rule of the program contains atoms from U and that rules whose heads are in U have empty bodies. Hence, U is a splitting set of \u03a0(D ,H ). By Splitting Set Theorem, set AS is an answer set of \u03a0(D ,H ) iff AS = Obs\u222aAS\u2032 where Obs is the collection of atoms formed by obs and hpd from history H , and AS\u2032 is an answer set of the program P1 obtained from \u03a0(D ,H ) by:\n\u2022 Replacing every ground instance: val( f (x\u0304),y,0)\u2190 obs( f (x\u0304) = y,0), such that obs( f (x\u0304) = y,0) \u2208 Obs, of the rule in Statement 13, by: val( f (x\u0304),y,0) and removing all the remaining ground instances of this rule.\n\u2022 Replacing every ground instance:\n\u2190val( f ,y1, i), obs( f = y2, i),\ny1 6= y2.\nof the rule in Statement 15 such that obs( f = y2, i) \u2208 Obs and y1 6= y2 by: \u2190 val( f ,y1, i) and removing all the remaining ground instances of this rule.\n\u2022 Replacing every ground instance: occurs(a, i)\u2190 hpd(a, i), such that hpd(a, i)\u2208Obs, of the rule in Statement 16, by: occurs(a, i) and removing all the remaining ground instances of this rule.\nSet AS defines a path M of \u03c4(D) iff AS\u2032 defines a path M of \u03c4(D). Our goal now is to find a state \u03c30 of \u03c4(D) such that AS\u2032 is an answer set of a program:\nP =de f \u03a0(D)\u222a val(\u03c30,0)\u222a{occurs(ai, i) : hpd(ai, i) \u2208H and 0\u2264 i < n}.\nOnce this is established, the conclusion of the proposition will follow immediately from Lemma 1.\nLet \u03c30 = { f (x\u0304) = y : val( f (x\u0304),y,0) \u2208 AS\u2032}, and consider a series of transformations of \u03a0(D ,H ) such that AS\u2032 is an answer set of each program in this series and that the last program of the series is P .\n3This proof uses the observations that are listed later in this section. They are easy to prove, and either belong to the folklore of the field or appear as lemmas in early ASP papers.\nStep 1: By the definition of answer set of CR-Prolog, AS\u2032 is an answer set of an ASP program P2 obtained from P1 by replacing its CR-rules with a (possibly empty) collection of their ASP counterparts, i.e., rules of the form:\nval( f (x\u0304),yk,0)\u2190val(body,0), (28) range( f ,c),\nmember(yk,c),\nyk 6= y.\nStep 2: From Observation 1, we have that AS\u2032 is an answer set of P2 iff AS\u2032 is an answer set of program P3 obtained from P2 by removing rules of the form\u2190 val( f ,y1, i).\nStep 3: From Observation 2 and definition of \u03c30, we have that AS\u2032 is an answer set of P3 iff AS\u2032 is an answer set of program P4 = P3\u222a val(\u03c30,0).\nStep 4: The fact that AS\u2032 satisfies rules of P4 and the definition of \u03c30 imply that every ground instance of rules in Statements 11 and 28, either has a literal in the body which does not belong to AS\u2032 or a literal in the head which is an atomic statement in P4. Hence, by Observations 3 and 4, AS\u2032 is an answer set of a program P5 obtained from P4 by removing these instances. This completes our transformation.\nNote that program P5 is of the form:\n\u03a0(D)\u222a val(\u03c30,0)\u222a{occurs(ai, i) : 0\u2264 i < n}.\nTo show that P5 is equal to program P from Lemma 1, it remains to be shown that \u03c30 is a state of \u03c4(D), i.e., that \u03c30 is an interpretation of the signature of D that is a unique answer set of program \u03a0c(D)\u222a\u03c3nd0 . To prove that \u03c30 is an interpretation we need to show that for every f (x\u0304) there is y such that f (x\u0304) = y \u2208 \u03c30. Consider three cases:\n1. There is y such that f (x\u0304) = y is the head of a default. In this case, there is a rule r of the form (in Statement 11) in P2 obtained from this default. There are again two possibilities:\n\u2022 The body of r is satisfied by AS\u2032. In this case val( f (x\u0304),y,0) \u2208 AS\u2032 and hence f (x\u0304) = y \u2208 \u03c30 \u2022 There is some literal in the body of r which is not satisfied by AS\u2032. In this case, Statement 14 guarantees\nthat f (x\u0304) = yi for some yi is in AS\u2032 and, hence, in \u03c30.\n2. f (x\u0304) is a basic fluent and there is no atom formed by f (x\u0304) which belong to the head of some default (Statement 7). In this case, the existence of yi such that val( f (x\u0304),yi,0) \u2208 AS\u2032, and hence in \u03c30, is guaranteed by Statement 14.\n3. f (x\u0304) is a defined fluent. In this case, its value is assigned by one of the existing rules (e.g., the CWA).\nTo show that \u03c30 is the unique answer set of R1 = \u03a0c(D)\u222a\u03c3nd0 we first observe that \u03c30 is an answer set of R1 iff it is an answer set of the encoding R2 of R1 which consists of ground instances of rules corresponding to state constraints, definitions and CWA, and facts of the form val( f (x\u0304),y,0) where f (x\u0304) is a basic fluent and f (x\u0304) = y \u2208 \u03c30. To finish proving that \u03c30 is a state, we need to show that val(\u03c30,0) is an answer set of R2. To do that, it is sufficient to notice that val(\u03c30,0) is a splitting set of program P5 and hence, By Splitting Set Theorem, AS\u2032 = ASb \u222aASt where ASb is the answer set of R2 and ASt does not contain atoms with a time step 0. Clearly, ASb = \u03c30. Finally, we need to recall that D is well-founded and, hence, by the definition of well-foundedness, R2 cannot have multiple answer sets.\nObservation 1: If \u03a0 is an ASP program and C is a collection of rules with the empty heads then AS\u2032 is an answer set of \u03a0\u222aC iff AS\u2032 is an answer set of \u03a0 which satisfies rules of C.\nObservation 2: If AS\u2032 is an answer set of an ASP program \u03a0 and S0 \u2286 AS\u2032, AS\u2032 is an answer set of the program obtained from \u03a0 by adding to it the encoding of literals from S0.\nObservation 3: If AS\u2032 is an answer set of an ASP program \u03a0 then AS\u2032 is an answer set of a program obtained from \u03a0 by removing rules whose bodies are not satisfied by AS\u2032.\nObservation 4: If AS\u2032 is an answer set of an ASP program \u03a0 then AS\u2032 is an answer set of a program obtained from \u03a0 by removing rules whose heads have occurrences of atoms which are facts of \u03a0.\nTo formulate the next observation we need some notation and terminology. If \u03b1 is a mapping from atoms to atoms then for any rule r by \u03b1(r) we mean the rule obtained from r by applying \u03b1 to all occurrences of atoms in r. Similarly for a collection of literals.\nASP programs P1 and P2 are called isomorphic if there is a one to one correspondence \u03b1 between literals of P1 and literals of P2 such that a rule r \u2208 P1 iff the rule \u03b1(r) \u2208 P2 (i.e. r \u2208 P1 implies that \u03b1(r) \u2208 P2 and r \u2208 P2 implies that \u03b1\u22121(r) \u2208 P1).\nObservation 5: If P1 and P2 are isomorphic with an isomorphism \u03b1 , then AS\u2032 is an answer set of P1 iff \u03b1(AS\u2032) is an answer set of P2.\nProof of Lemma 1 (sketch): Let D , \u03c30 and P be as in Lemma 1. To simplify the argument we assume that P contain no statics which can be eliminated using the splitting set theorem without any effect on statements formed by relations val and occurs.\nBy Pn we denote the grounding of \u03a0(D) with time steps ranging from 0 to n combined with the set val(\u03c30,0)\u222a {occurs(ai, i) : 1\u2264 i < n}. To prove the Lemma, it is sufficient to show that every answer set AS\u2032 of Pn defines a path of \u03c4(D). This proof is by induction on n.\n1. The base case (n= 0) is immediate since val(\u03c30,0) is a subset of an answer set AS\u2032 of P0 and hence the path \u3008\u03c30\u3009 is defined by AS\u2032.\n2. Assume that the lemma holds for and show that it holds for Pn.\nThe program Pn can be represented as: Pn =P0\u222aR0. Let us denote the set of literals occurring in the heads of rules from P0 by U . It can be checked that U is a splitting set of Pn and therefore, by Splitting Set Theorem, every answer set AS\u2032 of Pn can be represented as AS\u2032 = AS0 \u222aAS1 where AS0 is an answer set of P0 and AS1 is an answer set of partial evaluation of R0 with respect to U and AS0.\nLet us first show that AS0 defines a single transition path \u3008\u03c30,a0,\u03c31\u3009 of \u03c4(D). The definition of transition and the fact that \u03c30 is a state of \u03c4(D) implies that the only thing which needs proving is that \u03c31 = { f (x\u0304) = y : val( f (x\u0304),y,1) \u2208 S0} is a state. The proof consists of the following two steps:\n1. Show that \u03c31 is an interpretation, i.e. for every f (x\u0304) there is y such that f (x\u0304) = y \u2208 \u03c31.\n(a) P0 contains a rule:\nval( f (x\u0304),y1,1) or . . .or val( f (x\u0304),yk,1)\u2190val(body,0), (29) occurs(a,0).\nor a rule:\nval( f (x\u0304),y,1)\u2190 val(body,1) (30)\nwhose body is satisfied by AS0. To satisfy such rules AS0 must contain val( f (x\u0304),y,1) for some value y.\n(b) If case (a) above does not hold then consider two cases: f is basic and f is defined. To deal with the first case note that, since \u03c30 is a state, there is some y such that f (x\u0304) = y\u2208 \u03c30. Then, by Inertia Axiom f (x\u0304) will have the same value at step one, i.e. val( f (x\u0304),y,1) is in AS0. If f is defined then val( f (x\u0304), f alse,1) is in AS0 due to CWA for defined fluents. In both cases, f (x\u0304) = y \u2208 \u03c31.\n2. Uniqueness of an answer set of \u03a0c(D)\u222a\u03c3nd1 follows from the well-foundedness of D . Thus, \u03c31 is a state and \u3008\u03c30,a0,\u03c31\u3009 is a transition defined by AS0.\nTo complete the induction, we notice that, by Splitting Set Theorem, AS\u2032 is an answer set of Pn iff it is an answer set of AS0 \u222aR0. The latter program can be represented as: (AS0 \\ val(\u03c31,1))\u222aQ, where: Q = val(\u03c31,1)\u222aR. Note that Q is isomorphic to Pn\u22121, where the isomorphism, \u03b1 , is obtained by simply replacing a time-step i in each atom of R by i\u2212 1. By inductive hypothesis and Observation 5 (above), we have that an answer set AS1 of Q defines a path \u3008\u03c31,a1, . . . ,\u03c3n\u3009 in \u03c4(D). Since by Splitting Set Theorem AS\u2032 = AS0 \u222aAS1, AS\u2032 defines a path \u3008\u03c30,a0,\u03c31,a1, . . . ,\u03c3n\u3009. This completes the proof of the proposition."}, {"heading": "B Proof of Proposition 2", "text": "In this section, we examine Proposition 2, which states that:\nLet DH and DL be the coarse-resolution and fine-resolution system descriptions for the office domain in Example 1. Then DL is a refinement of DH .\nTo establish this proposition, we need to establish that the relationship between DH (Example 4) and DL (Section 7.1) satisfies the conditions of Definition 7, as stated below.\nSystem description DL is a refinement of system description DH if:\n1. States of \u03c4L are the refinements of states of \u03c4H .\n2. For every transition \u3008\u03c31,aH ,\u03c32\u3009 of \u03c4H , every fluent f in a set F of observable fluents, and every refinement \u03b41 of \u03c31, there is a path P in \u03c4L from \u03b41 to a refinement \u03b42 of \u03c32 such that:\n(a) Every action of P is executed by the robot which executes aH .\n(b) Every state of P is a refinement of \u03c31 or \u03c32, i.e., no unrelated fluents are changed. (c) observed(R, f ,Y ) = true \u2208 \u03b42 iff ( f = Y ) \u2208 \u03b42 and observed(R, f ,Y ) = f alse \u2208 \u03b42 iff ( f 6= Y ) \u2208 \u03b42.\nThe first condition in Definition 7 is that states of \u03c4L be refinements of states of \u03c4H , as specified by Definition 6. To establish this condition, let \u03b4 be a state of \u03c4L, i.e., a unique answer set of a program \u03a0L with statements such as:\ncomponent(c1,o f f ice) ... next to(c1,c2), next to(c2,c5) ... loc(rob1) = c1, loc(cup1) = c6\nwhich include atoms for (a) cells that are components of rooms; (b) adjacent cells that are accessible from each other; and (c) initial locations of the robot and a coffee cup. Program \u03a0L has axioms for loc such as:\nloc(cup1) =C if loc(rob1) =C, in hand(rob1,cup1) loc\u2217(cup1) = Rm if loc(cup1) =C, component(C,Rm) loc\u2217(rob1) = Rm if loc(rob1) =C, component(C,Rm)\ndefined (here) in the context of textbook cup1, and axioms for static relation next to such as:\nnext to(C2,C1) if next to(C1,C2) next to\u2217(Rm1,Rm2) if next to(C1,C2), component(C1,Rm1), component(C2,Rm2), Rm1 6= Rm2\nThe program \u03a0L also includes axioms related to the basic knowledge fluents corresponding to observations such as can be tested(rob1, loc(cup1),C), and the CWA for defined knowledge fluents such as may discover(rob1, loc\u2217(cup1),Rm).\nPlease see refined.sp at https://github.com/mhnsrdhrn/refine-arch for an example of the complete program (in SPARC), with additional axioms for planning.\nWe need to show that there is a state \u03c3 \u2208 \u03c4H such that \u03b4 is a refinement of \u03c3 . We will do so by construction. Let \u03c3 be the union of two sets of atoms:\n\u03c3 = \u03c3\u2217m \u222a \u03c3nm (31) \u03c3\u2217m = { f = y : f is a magnified coarse-resolution domain property, ( f \u2217 = y) \u2208 \u03b4} \u03c3nm = { f = y : f is a non-magnified coarse-resolution domain property, ( f = y) \u2208 \u03b4}\nwhere \u03c3\u2217m is a collection of atoms of domain properties that are magnified during refinement, and \u03c3nm is a collection of atoms of domain properties that are not magnified during refinement. Continuing with our example in the context of Figure 3, \u03c3\u2217m includes {loc\u2217(rob1) = r1, loc\u2217(cup1) = r2, next to\u2217(r1,r2)}, where r1 = o f f ice and r2 = kitchen, and \u03c3nm includes \u00acin hand(rob1,cup1). Based on Definition 6, \u03b4 is a refinement of \u03c3 . We now need to establish that \u03c3 is a state, i.e., an answer set of program \u03a0H that consists of all the facts in \u03c3 , and the axioms of DH , the coarse-resolution system description. To do so, we need to establish that the facts in \u03c3 satisfy the axioms in \u03a0H . This holds true by construction of \u03c3 and the system description DL, thus establishing the first condition of Definition 7.\nNext, we establish the second condition in Definition 7. We do so by construction, and by considering two representative transitions\u2014other transitions can be addressed in a similar manner. In our discussion below, descriptions of states omit negative literals for simplicity. We also often omit atoms formed of statics, fluents unchanged by the transition, knowledge fluents, and fluents whose values are unknown. Furthermore, unless otherwise stated, the set F of observable fluents includes all fluents.\nFirst, consider the transition corresponding to action aH = move(rob1,kitchen) that moves the robot from the o ff ice to the kitchen in Figure 3. Considering just the fluent that changes due to this transition, {loc(rob1) = o f f ice} \u2208 \u03c31 and {loc(rob1) = kitchen} \u2208 \u03c32. Without loss of generality, consider a refinement \u03b41 of \u03c31 that has atoms such as:\nloc\u2217(rob1) = r1 loc(rob1) = c2 directly observed(rob1, loc(rob1),c2)\ndirectly observed(rob1, loc(rob1),c5) = undet\nwhich implies that the robot is in cell c2 in room r1, which is the o ff ice, and the value of directly observed is undetermined for all its parameters other than the current location of the robot. Our description of delta1 omits atoms such as next to(c1,c2), next to(c5,c6), and atoms corresponding to some other fluents. Now, we construct a path P1 from \u03c4L that corresponds to two state transitions. The first transition corresponds to executing action move(rob1,c5), changing the state from \u03b41 to \u03b41,a that has atoms such as:\n\u03b41,a = {loc(rob1) = c5, loc\u2217(rob1) = r2}\nand corresponds to the robot\u2019s movement from cell c2 in room r1 to neighboring cell c5 in room r2. The second transition of the constructed path P1 corresponds to action test(rob1, loc(rob1),c5), which causes a transition to state:\n\u03b42 ={loc(rob1) = c5, loc\u2217(rob1) = r2, directly observed(rob1, loc(rob1),c5), observed(rob1, loc(rob1),c5), observed(rob1, loc\u2217(rob1),r2}\nwhere the newly added atoms correspond to observing the robot\u2019s location. We observe that \u03b42 is a state of \u03c4L and a refinement of state \u03c32 of \u03c4H (see Definition 6). To establish the second condition of Definition 7, we need to show that the chosen coarse-resolution transition, constructed path P1 from \u03c4L, set F of observable fluents, and refinement \u03b41 of \u03c31, satisfy requirements (a)-(c). To establish requirement (a), notice that every action of path P1 is executed by a robot that executes the coarse-resolution action aH . To establish requirement (b), notice that state \u03b41,a is a refinement of \u03c32\u2014we already know that \u03b41 and \u03b42 are refinements of \u03c31 and \u03c32 respectively\u2014i.e., every state of P1 is a refinement\nof \u03c31 or \u03c32 and unrelated fluents remain unchanged. Finally, to establish requirement (c), notice that Statements 21, 23 and 24 (in the construction of DL) ensure that for each f \u2208 F , observed(rob1, f ,Y ) = true \u2208 \u03b42 iff ( f = Y ) \u2208 \u03b42 and observed(rob1, f ,Y ) = f alse \u2208 \u03b42 iff ( f 6= Y ) \u2208 \u03b42.\nAs a second representative example of a transition, consider the robot, which is in the kitchen, executing aH = grasp(rob1,cup1) to pick up the coffee cup cup1 that is known to be in the kitchen. Considering just the fluents that change as a result of this transition, the state \u03c33 \u2208 \u03c4H has {loc(rob1) = kitchen, loc(cup1) = kitchen}. Now, consider a refinement \u03b43 of \u03c33 that has the atoms such as:\nloc\u2217(rob1) = r2, loc(rob1) = c5 loc\u2217(cup1) = r2, loc(cup1) = c6 directly observed(rob1, loc(rob1),c5)\ndirectly observed(rob1, loc(rob1),c6) = undet\nwhich implies that the robot is in cell c5 in room r2, coffee cup cup1 is in cell c6 in r2, and the value of directly observed is undetermined for all its parameters other than the current locations of the robot and the coffee cup. Now, consider path P2 from \u03c4L that corresponds to four state transitions. The first transition involves executing action move(rob1,c6), which changes the state from \u03b43 to \u03b43,a that has atoms such as:\n\u03b41,a ={loc(rob1) = c6, loc\u2217(rob1) = r2, loc(cup1) = c6, loc\u2217(cup1) = r2}\nand corresponds to the robot moving from cell c5 to cell c6 in room r2. The second transition corresponds to executing action test(rob1, loc(rob1),c6), which changes the state to delta3,b that has atoms such as:\n\u03b43,b ={loc(rob1) = c6, loc\u2217(rob1) = r2, loc(cup1) = c6, loc\u2217(cup1) = r2, directly observed(rob1, loc(rob1),c6), observed(rob1, loc(rob1),c6), observed(rob1, loc\u2217(rob1),r2}\nwhere we notice that the newly added atoms correspond to observing the robot\u2019s location. The third transition along path P2 corresponds to action grasp(rob1,cup1), which changes the state to \u03b43,c that has atoms such as:\n\u03b43,c ={loc(rob1) = c6, loc\u2217(rob1) = r2, loc(cup1) = c6, loc\u2217(cup1) = r2, in hand(rob1,cup1), directly observed(rob1, loc(rob1),c6), observed(rob1, loc(rob1),c6), observed(rob1, loc\u2217(rob1),r2}\nwhere we notice that cup1 is now in the robot\u2019s grasp. Finally, action test(rob1, in hand(rob1,cup1), true) causes a transition to state \u03b44 that has atoms such as:\n\u03b44 ={loc(rob1) = c6, loc\u2217(rob1) = r2, loc(cup1) = c6, loc\u2217(cup1) = r2, in hand(rob1,cup1), directly observed(rob1, loc(rob1),c6), observed(rob1, loc(rob1),c6), observed(rob1, loc\u2217(rob1),r2,\ndirectly observed(rob1, in hand(rob1,cup1), true), observed(rob1, in hand(rob1,cup1), true)}\nWe observe that \u03b44 is a state of \u03c4L and a refinement of state \u03c34 of \u03c4H . To establish the second condition of Definition 7, we need to show that the chosen coarse-resolution transition, constructed path P2 from \u03c4L, set F of observable fluents, and refinement \u03b43 of \u03c33, satisfy requirements (a)-(c). To establish requirement (a), notice that every action of P2 has to be executed to implement aH . Next, to establish requirement (b), we notice that \u03b43,a and \u03b43,b are refinements of \u03c33, while \u03b43,c and \u03b44are refinements of \u03c34, i..e, every state of P2 is a refinement of \u03c33 or \u03c34 and unrelated fluents remain unchanged. Finally, to establish requirement (c), notice that Statements 21, 23 and 24 (in the construction of DL) ensure that for each f \u2208 F , observed(rob1, f ,Y ) = true \u2208 \u03b42 iff ( f = Y ) \u2208 \u03b42 and observed(rob1, f ,Y ) = f alse \u2208 \u03b42 iff ( f 6= Y ) \u2208 \u03b42.\nThe steps described above can be repeated for other coarse-resolution transitions. We have thus shown that DL is indeed a refinement of DH . These steps can also be verified by solving refined.sp after suitably revising the initial state and goal state."}, {"heading": "C POMDP Construction Example", "text": "In this section, we provide an illustrative example of constructing a POMDP for a specific abstract action that needs to be implemented as a sequence of concrete actions whose effects are modeled probabilistically.\nExample 7. [Example of POMDP construction] Consider abstract action aH = grasp(rob1, tb1), with the robot and textbook in the o ff ice, in the context of Example 4. The corresponding zoomed system description DLR(T ) is in Example 6. For ease of explanation, assume the following transition probabilities, observation probabilities, and rewards\u2014these values would typically be computed by the robot in the initial training phase (Section 7.2):\n\u2022 Any move from a cell to a neighboring cell succeeds with probability 0.85. Since there are only two cells in this room, the robot remains in the same cell if move does not succeed.\n\u2022 The grasp action succeeds with probability 0.95; otherwise it fails.\n\u2022 If the thing being searched for in a cell exists in the cell, 0.95 is the probability of successfully finding it.\n\u2022 All non-terminal actions have unit cost. A correct answer receives a large positive reward (100), whereas an incorrect answer receives a large negative reward (\u2212100).\nThe elements of the corresponding POMDP are described (below) in the format of the approximate POMDP solver used in our experiments [37]. As described in Section 8.2, please note that:\n\u2022 Executing a terminal action causes a transition to a terminal state.\n\u2022 Actions that change the p-state do not provide any observations.\n\u2022 Knowledge-producing actions do not change the p-state.\ndiscount: 0.99\nvalues: reward\n% States, actions and observations as enumerated lists\nstates: robot-0-object-0-inhand robot-1-object-1-inhand robot-0-object-0-not-inhand\nrobot-0-object-1-not-inhand robot-1-object-0-not-inhand\nrobot-1-object-1-not-inhand absb\nactions: move-0 move-1 grasp test-robot-0 test-robot-1 test-object-0 test-object-1\ntest-inhand finish\nobservations: robot-found robot-not-found object-found object-not-found\ninhand not-inhand none\n% Transition function format.\n% T : action : S x S\u2019 -> [0, 1]\n% Probability of transition from first element of S to that of S\u2019 is\n% in the top left corner of each matrix"}, {"heading": "T: move-0", "text": "1 0 0 0 0 0 0\n0.85 0.15 0 0 0 0 0\n0 0 1 0 0 0 0\n0 0 0 1 0 0 0\n0 0 0.85 0 0.15 0 0\n0 0 0 0.85 0 0.15 0\n0 0 0 0 0 0 1"}, {"heading": "T: move-1", "text": "0.15 0.85 0 0 0 0 0\n0 1 0 0 0 0 0\n0 0 0.15 0 0.85 0 0\n0 0 0 0.15 0 0.85 0\n0 0 0 0 1 0 0\n0 0 0 0 0 1 0\n0 0 0 0 0 0 1"}, {"heading": "T: grasp", "text": "1 0 0 0 0 0 0\n0 1 0 0 0 0 0\n0.95 0 0.05 0 0 0 0\n0 0 0 1 0 0 0\n0 0 0 0 1 0 0\n0 0.95 0 0 0 0.05 0\n0 0 0 0 0 0 1"}, {"heading": "T: test-robot-0 identity", "text": ""}, {"heading": "T: test-robot-1 identity", "text": ""}, {"heading": "T: test-object-0 identity", "text": ""}, {"heading": "T: test-object-1 identity", "text": ""}, {"heading": "T: test-inhand identity", "text": ""}, {"heading": "T: finish uniform", "text": "% Observation function format(s)\n% O : action : s_i : z_i -> [0, 1] (or)\n% : S x Z -> [0, 1]\n% In each matrix, first row provides probability of each possible\n% observation in the first p-state in S"}, {"heading": "O: move-0 : * : none 1", "text": ""}, {"heading": "O: move-1 : * : none 1", "text": "O: grasp : * : none 1"}, {"heading": "O: test-robot-0", "text": "0.95 0.05 0 0 0 0 0\n0.05 0.95 0 0 0 0 0\n0.95 0.05 0 0 0 0 0\n0.95 0.05 0 0 0 0 0\n0.05 0.95 0 0 0 0 0\n0.05 0.95 0 0 0 0 0\n0 0 0 0 0 0 1"}, {"heading": "O: test-robot-1", "text": "0.05 0.95 0 0 0 0 0\n0.95 0.05 0 0 0 0 0\n0.05 0.95 0 0 0 0 0\n0.05 0.95 0 0 0 0 0\n0.95 0.05 0 0 0 0 0\n0.95 0.05 0 0 0 0 0\n0 0 0 0 0 0 1"}, {"heading": "O: test-object-0", "text": "0 0 0.95 0.05 0 0 0\n0 0 0.05 0.95 0 0 0\n0 0 0.95 0.05 0 0 0\n0 0 0.05 0.95 0 0 0\n0 0 0.95 0.05 0 0 0\n0 0 0.05 0.95 0 0 0\n0 0 0 0 0 0 1"}, {"heading": "O: test-object-1", "text": "0 0 0.05 0.95 0 0 0\n0 0 0.95 0.05 0 0 0\n0 0 0.05 0.95 0 0 0\n0 0 0.95 0.05 0 0 0\n0 0 0.05 0.95 0 0 0\n0 0 0.95 0.05 0 0 0\n0 0 0 0 0 0 1"}, {"heading": "O: test-inhand", "text": "0 0 0 0 0.95 0.05 0\n0 0 0 0 0.95 0.05 0\n0 0 0 0 0.05 0.95 0\n0 0 0 0 0.05 0.95 0\n0 0 0 0 0.05 0.95 0\n0 0 0 0 0.05 0.95 0\n0 0 0 0 0 0 1"}, {"heading": "O: finish : * : none 1", "text": "% Reward function format\n% R : action : s_i : s_i\u2019 : real value\nR: finish : robot-0-object-0-inhand : * : -100"}, {"heading": "R: finish : robot-1-object-1-inhand : * : 100", "text": ""}, {"heading": "R: finish : robot-0-object-0-not-inhand : * : -100", "text": ""}, {"heading": "R: finish : robot-0-object-1-not-inhand : * : -100", "text": ""}, {"heading": "R: finish : robot-1-object-0-not-inhand : * : -100", "text": ""}, {"heading": "R: finish : robot-1-object-1-not-inhand : * : -100", "text": ""}, {"heading": "R: move-0 : * : * : -1", "text": ""}, {"heading": "R: move-1 : * : * : -1", "text": ""}, {"heading": "R: grasp : * : * : -1", "text": ""}, {"heading": "R: test-robot-0 : * : * : -1", "text": ""}, {"heading": "R: test-robot-1 : * : * : -1", "text": ""}, {"heading": "R: test-object-0: * : * : -1", "text": ""}, {"heading": "R: test-object-1: * : * : -1", "text": "R: test-inhand : * : * : -1"}], "references": [{"title": "Integrated Perception and Planning in the Continuous Space: A POMDP Approach", "author": ["Haoyu Bai", "David Hsu", "Wee Sun Lee"], "venue": "International Journal of Robotics Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Towards Answer Set Programming with Sorts", "author": ["Evgenii Balai", "Michael Gelfond", "Yuanlin Zhang"], "venue": "In International Conference on Logic Programming and Nonmonotonic Reasoning, Corunna,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Diagnostic Reasoning with A-Prolog", "author": ["Marcello Balduccini", "Michael Gelfond"], "venue": "Theory and Practice of Logic Programming,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Logic Programs with Consistency-Restoring Rules", "author": ["Marcello Balduccini", "Michael Gelfond"], "venue": "In AAAI Spring Symposium on Logical Formalization of Commonsense Reasoning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Language ASPf with Arithmetic Expressions and Consistency-Restoring Rules", "author": ["Marcello Balduccini", "Michael Gelfond"], "venue": "In Workshop on Answer Set Programming and Other Computing Paradigms (ASPOCP) at ICLP,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "An ASP-Based Architecture for Autonomous UAVs in Dynamic Environments: Progress Report", "author": ["Marcello Balduccini", "William C. Regli", "Duc N. Nguyen"], "venue": "In International Workshop on Non-Monotonic Reasoning (NMR), Vienna,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Towards an ASP-Based Architecture for Autonomous UAVs in Dynamic Environments (Extended Abstract)", "author": ["Marcello Balduccini", "William C. Regli", "Duc N. Nguyen"], "venue": "In International Conference on Logic Programming", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Knowledge Representation, Reasoning and Declarative Problem Solving", "author": ["Chitta Baral"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Probabilistic Reasoning with Answer Sets", "author": ["Chitta Baral", "Michael Gelfond", "Nelson Rushton"], "venue": "Theory and Practice of Logic Programming,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Continual Planning and Acting in Dynamic Multiagent Environments", "author": ["Michael Brenner", "Bernhard Nebel"], "venue": "Autonomous Agents and Multiagent Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Toward Open Knowledge Enabling for Human-Robot Interaction", "author": ["Xiaoping Chen", "Jiongkun Xie", "Jianmin Ji", "Zhiqiang Sui"], "venue": "Human-Robot Interaction,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "What Happened and Why? A Mixed Architecture for Planning and Explanation Generation in Robotics", "author": ["Zenon Colaco", "Mohan Sridharan"], "venue": "In Australasian Conference on Robotics and Automation (ACRA),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "A Solution to the Simultaneous Localization and Map Building (SLAM) Problem", "author": ["G. Dissanayake", "P. Newman", "S. Clark"], "venue": "IEEE Transactions on Robotics and Automation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "Answer Set Programming for Collaborative Housekeeping Robotics: Representation, Reasoning, and Execution", "author": ["Esra Erdem", "Erdi Aker", "Volkan Patoglu"], "venue": "Intelligent Service Robotics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Applications of Action Languages to Cognitive Robotics", "author": ["Esra Erdem", "Volkan Patoglu"], "venue": "In Correct Reasoning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Yet Another Modular Action Language", "author": ["Michael Gelfond", "Daniela Inclezan"], "venue": "In International Workshop on Software Engineering for Answer Set Programming,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Some Properties of System Descriptions of ALd", "author": ["Michael Gelfond", "Daniela Inclezan"], "venue": "Journal of Applied Non-Classical Logics, Special Issue on Equilibrium Logic and Answer Set Programming,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Knowledge Representation, Reasoning and the Design of Intelligent Agents", "author": ["Michael Gelfond", "Yulia Kahl"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Automated Planning: Theory and Practice", "author": ["Malik Ghallab", "Dana Nau", "Paolo Traverso"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "A Switching Planner for Combined Task and Observation Planning", "author": ["Moritz G\u00f6belbecker", "Charles Gretton", "Richard Dearden"], "venue": "In National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Model Checking with Probabilistic Tabled Logic Programming", "author": ["Andrey Gorlin", "C.R. Ramakrishnan", "Scott A. Smolka"], "venue": "Theory and Practice of Logic Programming,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Reasoning about Uncertainty", "author": ["Joseph Halpern"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "Robot Task Planning and Explanation in Open and Uncertain Worlds", "author": ["Marc Hanheide", "Moritz Gobelbecker", "Graham Horn", "Andrzej Pronobis", "Kristoffer Sjoo", "Patric Jensfelt", "Charles Gretton", "Richard Dearden", "Miroslav Janicek", "Hendrik Zender", "Geert-Jan Kruijff", "Nick Hawes", "Jeremy Wyatt"], "venue": "Artificial Intelligence,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Exploiting Probabilistic Knowledge under Uncertain Sensing for Efficient Robot Behaviour", "author": ["Marc Hanheide", "Charles Gretton", "Richard Dearden", "Nick Hawes", "Jeremy Wyatt", "Andrzej Pronobis", "Alper Aydemir", "Moritz Gobelbecker", "Hendrik Zender"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Automated Handwashing Assistance for Persons with Dementia using Video and a Partially Observable Markov Decision Process", "author": ["Jesse Hoey", "Pascal Poupart", "Axel Bertoldi", "Tammy Craig", "Craig Boutilier", "Alex Mihailidis"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Integrated Common Sense Learning and Planning in POMDPs", "author": ["Brendan Juba"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Planning and Acting in Partially Observable Stochastic Domains", "author": ["Leslie Kaelbling", "Michael Littman", "Anthony Cassandra"], "venue": "Artificial Intelligence,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1998}, {"title": "Integrated Task and Motion Planning in Belief Space", "author": ["Leslie Kaelbling", "Tomas Lozano-Perez"], "venue": "International Journal of Robotics Research,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Planning in Action Language BC while Learning Action Costs for Mobile Robots", "author": ["Piyush Khandelwal", "Fangkai Yang", "Matteo Leonetti", "Vladimir Lifschitz", "Peter Stone"], "venue": "In International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Extending the Soar Cognitive Architecture", "author": ["John E. Laird"], "venue": "In International Conference on Artificial General Intelligence, Memphis,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "An Unified Cognitive Architecture for Physical Agents", "author": ["Patrick Langley", "Dongkyu Choi"], "venue": "In The Twenty-first National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "Action Language BC: Preliminary Report", "author": ["Joohyun Lee", "Vladimir Lifschitz", "Fangkai Yang"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI), Beijing,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "A Probabilistic Extension of the Stable Model Semantics", "author": ["Joohyung Lee", "Yi Wang"], "venue": "In AAAI Spring Symposium on Logical Formalizations of Commonsense Reasoning,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Move and the Robot will Learn: Vision-based Autonomous Learning of Object Models", "author": ["Xiang Li", "Mohan Sridharan"], "venue": "In International Conference on Advanced Robotics,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Algorithms for Sequential Decision Making", "author": ["Michael Littman"], "venue": "PhD thesis,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1996}, {"title": "BLOG: Probabilistic Models with Unknown Objects. In Statistical Relational Learning", "author": ["Brian Milch", "Bhaskara Marthi", "Stuart Russell", "David Sontag", "Daniel L. Ong", "Andrey Kolobov"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2006}, {"title": "Planning under Uncertainty for Robotic Tasks with Mixed Observability", "author": ["Sylvie C. Ong", "Shao Wei Png", "David Hsu", "Wee Sun Lee"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2010}, {"title": "Abducing through Negation as Failure: Stable Models within the Independent Choice Logic", "author": ["David Poole"], "venue": "Journal of Logic Programming,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2000}, {"title": "Learning Accuracy and Availability of Humans who Help Mobile Robots", "author": ["Stephanie Rosenthal", "Manuela Veloso", "Anind Dey"], "venue": "In National Conference on Artificial Intelligence, San Francisco,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "Symbolic Dynamic Programming for First-order POMDPs", "author": ["Scott Sanner", "Kristian Kersting"], "venue": "In National Conference on Artificial Intelligence (AAAI), July", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2010}, {"title": "Cognitive Factories with Multiple Teams of Heterogeneous Robots: Hybrid Reasoning for Optimal Feasible Global Plans", "author": ["Zeynep G. Saribatur", "Esra Erdem", "Volkan Patoglu"], "venue": "In International Conference on Intelligent Robots and Systems (IROS), Chicago,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "The Optimal Control of Partially Observable Markov Processes", "author": ["Edward J. Sondik"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1971}, {"title": "Using Knowledge Representation and Reasoning Tools in the Design of Robots. In IJCAI Workshop on Knowledge-based Techniques for Problem Solving and Reasoning (KnowProS)", "author": ["Mohan Sridharan", "Michael Gelfond"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Should I do that? Using Relational Reinforcement Learning and Declarative Programming to Discover Domain Axioms", "author": ["Mohan Sridharan", "Ben Meadows"], "venue": "In International Conference on Developmental Learning and Epigenetic Robotics (ICDL-EpiRob),", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2016}, {"title": "What can I not do? Towards An Architecture for Reasoning about and Learning Affordances", "author": ["Mohan Sridharan", "Ben Meadows"], "venue": "In International Conference on Automated Planning and Scheduling (ICAPS), Pittsburgh,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2017}, {"title": "Planning to See: A Hierarchical Aprroach to Planning Visual Actions on a Robot using POMDPs", "author": ["Mohan Sridharan", "Jeremy Wyatt", "Richard Dearden"], "venue": "Artificial Intelligence,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2010}, {"title": "Planning for Human- Robot Teaming in Open Worlds", "author": ["Kartik Talamadupula", "J. Benton", "Subbarao Kambhampati", "Paul Schermerhorn", "Matthias Scheutz"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2010}, {"title": "Probabilistic Robotics", "author": ["Sebastian Thrun", "Wolfram Burgard", "Dieter Fox"], "venue": null, "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2005}, {"title": "Dynamically Constructed (PO)MDPs for Adaptive Robot Planning", "author": ["Shiqi Zhang", "Piyush Khandelwal", "Peter Stone"], "venue": "In AAAI Conference on Artificial Intelligence (AAAI), San Francisco,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2017}, {"title": "Towards An Architecture for Knowledge Representation and Reasoning in Robotics", "author": ["Shiqi Zhang", "Mohan Sridharan", "Michael Gelfond", "Jeremy Wyatt"], "venue": "In International Conference on Social Robotics (ICSR), Sydney, Australia,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2014}, {"title": "Active Visual Planning for Mobile Robot Teams using Hierarchical POMDPs", "author": ["Shiqi Zhang", "Mohan Sridharan", "Christian Washington"], "venue": "IEEE Transactions on Robotics,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2013}, {"title": "Mixed Logical Inference and Probabilistic Planning for Robots in Unreliable Worlds", "author": ["Shiqi Zhang", "Mohan Sridharan", "Jeremy Wyatt"], "venue": "IEEE Transactions on Robotics,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2015}, {"title": "CORPP: Commonsense Reasoning and Probabilistic Planning, as Applied to Dialog with a Mobile Robot", "author": ["Shiqi Zhang", "Peter Stone"], "venue": "In AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": ", STRIPS, PDDL [19], BC [32], and ALd [17].", "startOffset": 15, "endOffset": 19}, {"referenceID": 31, "context": ", STRIPS, PDDL [19], BC [32], and ALd [17].", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": ", STRIPS, PDDL [19], BC [32], and ALd [17].", "startOffset": 38, "endOffset": 42}, {"referenceID": 1, "context": "The domain representation for logical reasoning is translated into a program in SPARC [2], an extension of CR-Prolog, and the representation for probabilistic reasoning is translated into a partially observable Markov decision process (POMDP) [27].", "startOffset": 86, "endOffset": 89}, {"referenceID": 26, "context": "The domain representation for logical reasoning is translated into a program in SPARC [2], an extension of CR-Prolog, and the representation for probabilistic reasoning is translated into a partially observable Markov decision process (POMDP) [27].", "startOffset": 243, "endOffset": 247}, {"referenceID": 3, "context": "CR-Prolog [4] (and thus SPARC) incorporates consistency-restoring rules in Answer Set Prolog (ASP)\u2014in this paper, the terms ASP, CR-Prolog and SPARC are often used interchangeably\u2014and has a close relationship with our action language, allowing us to reason efficiently with hierarchically organized knowledge and default knowledge, and to pose state estimation, planning, and explanation generation within a single framework.", "startOffset": 10, "endOffset": 13}, {"referenceID": 26, "context": "Also, using an efficient approximate solver to reason with POMDPs supports a principled and quantifiable trade-off between accuracy and computational efficiency in the presence of uncertainty, and provides a near-optimal solution under certain conditions [27, 37].", "startOffset": 255, "endOffset": 263}, {"referenceID": 36, "context": "Also, using an efficient approximate solver to reason with POMDPs supports a principled and quantifiable trade-off between accuracy and computational efficiency in the presence of uncertainty, and provides a near-optimal solution under certain conditions [27, 37].", "startOffset": 255, "endOffset": 263}, {"referenceID": 0, "context": "There are many recent examples of researchers using probabilistic graphical models such as POMDPs to formulate tasks such as planning, sensing, navigation, and interaction on robots [1, 20, 25, 40].", "startOffset": 182, "endOffset": 197}, {"referenceID": 19, "context": "There are many recent examples of researchers using probabilistic graphical models such as POMDPs to formulate tasks such as planning, sensing, navigation, and interaction on robots [1, 20, 25, 40].", "startOffset": 182, "endOffset": 197}, {"referenceID": 24, "context": "There are many recent examples of researchers using probabilistic graphical models such as POMDPs to formulate tasks such as planning, sensing, navigation, and interaction on robots [1, 20, 25, 40].", "startOffset": 182, "endOffset": 197}, {"referenceID": 38, "context": "There are many recent examples of researchers using probabilistic graphical models such as POMDPs to formulate tasks such as planning, sensing, navigation, and interaction on robots [1, 20, 25, 40].", "startOffset": 182, "endOffset": 197}, {"referenceID": 7, "context": "This includes, for instance, theories of reasoning about action and change, as well as Answer Set Prolog (ASP), a non-monotonic logic programming paradigm, which is well-suited for representing and reasoning with commonsense knowledge [8, 18].", "startOffset": 235, "endOffset": 242}, {"referenceID": 17, "context": "This includes, for instance, theories of reasoning about action and change, as well as Answer Set Prolog (ASP), a non-monotonic logic programming paradigm, which is well-suited for representing and reasoning with commonsense knowledge [8, 18].", "startOffset": 235, "endOffset": 242}, {"referenceID": 14, "context": "An international research community has developed around ASP, with applications in cognitive robotics [15] and other non-robotics domains.", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "For instance, ASP has been used for planning and diagnostics by one or more simulated robot housekeepers [14], and for representation of domain knowledge learned through natural language processing by robots interacting with humans [11].", "startOffset": 105, "endOffset": 109}, {"referenceID": 10, "context": "For instance, ASP has been used for planning and diagnostics by one or more simulated robot housekeepers [14], and for representation of domain knowledge learned through natural language processing by robots interacting with humans [11].", "startOffset": 232, "endOffset": 236}, {"referenceID": 5, "context": "ASP-based architectures have also been used for the control of unmanned aerial vehicles in dynamic indoor environments [6, 7].", "startOffset": 119, "endOffset": 125}, {"referenceID": 6, "context": "ASP-based architectures have also been used for the control of unmanned aerial vehicles in dynamic indoor environments [6, 7].", "startOffset": 119, "endOffset": 125}, {"referenceID": 18, "context": "There are many different action languages such as STRIPS, PDDL [19], BC [32], and ALd [17], which have been used for different applications [10, 29].", "startOffset": 63, "endOffset": 67}, {"referenceID": 31, "context": "There are many different action languages such as STRIPS, PDDL [19], BC [32], and ALd [17], which have been used for different applications [10, 29].", "startOffset": 72, "endOffset": 76}, {"referenceID": 16, "context": "There are many different action languages such as STRIPS, PDDL [19], BC [32], and ALd [17], which have been used for different applications [10, 29].", "startOffset": 86, "endOffset": 90}, {"referenceID": 9, "context": "There are many different action languages such as STRIPS, PDDL [19], BC [32], and ALd [17], which have been used for different applications [10, 29].", "startOffset": 140, "endOffset": 148}, {"referenceID": 28, "context": "There are many different action languages such as STRIPS, PDDL [19], BC [32], and ALd [17], which have been used for different applications [10, 29].", "startOffset": 140, "endOffset": 148}, {"referenceID": 29, "context": "For instance, architectures have been developed to support hierarchical representation of knowledge and axioms in first-order logic, and probabilistic processing of perceptual information [30, 31, 48], while deterministic and probabilistic algorithms have been combined for task and motion planning on robots [28].", "startOffset": 188, "endOffset": 200}, {"referenceID": 30, "context": "For instance, architectures have been developed to support hierarchical representation of knowledge and axioms in first-order logic, and probabilistic processing of perceptual information [30, 31, 48], while deterministic and probabilistic algorithms have been combined for task and motion planning on robots [28].", "startOffset": 188, "endOffset": 200}, {"referenceID": 46, "context": "For instance, architectures have been developed to support hierarchical representation of knowledge and axioms in first-order logic, and probabilistic processing of perceptual information [30, 31, 48], while deterministic and probabilistic algorithms have been combined for task and motion planning on robots [28].", "startOffset": 188, "endOffset": 200}, {"referenceID": 27, "context": "For instance, architectures have been developed to support hierarchical representation of knowledge and axioms in first-order logic, and probabilistic processing of perceptual information [30, 31, 48], while deterministic and probabilistic algorithms have been combined for task and motion planning on robots [28].", "startOffset": 309, "endOffset": 313}, {"referenceID": 23, "context": "Another example is the behavior control of a robot that included semantic maps and commonsense knowledge in a probabilistic relational representation, and then used a continual planner to switch between decision-theoretic and classical planning procedures based on degrees of belief [24].", "startOffset": 283, "endOffset": 287}, {"referenceID": 22, "context": "More recent work has used a three-layered organization of knowledge (instance, default and diagnostic), with knowledge at the higher level modifying that at the lower levels, and a three-layered architecture (competence layer, belief layer and deliberative layer) for distributed control of information flow, combining first-order logic and probabilistic reasoning for open world planning [23].", "startOffset": 389, "endOffset": 393}, {"referenceID": 40, "context": "Declarative programming has also been combined with continuous-time planners for path planning in mobile robot teams [42].", "startOffset": 117, "endOffset": 121}, {"referenceID": 52, "context": "More recent work has combined a probabilistic extension of ASP with POMDPs for commonsense inference and probabilistic planning in human-robot dialog [54], used a probabilistic extension of ASP to determine some model parameters of POMDPs [50], used ASP-based architecture to support learning of action costs on a robot [29], and combined logic programming and relational reinforcement learning to interactively and cumulatively discover domain axioms [45] and affordances [46].", "startOffset": 150, "endOffset": 154}, {"referenceID": 48, "context": "More recent work has combined a probabilistic extension of ASP with POMDPs for commonsense inference and probabilistic planning in human-robot dialog [54], used a probabilistic extension of ASP to determine some model parameters of POMDPs [50], used ASP-based architecture to support learning of action costs on a robot [29], and combined logic programming and relational reinforcement learning to interactively and cumulatively discover domain axioms [45] and affordances [46].", "startOffset": 239, "endOffset": 243}, {"referenceID": 28, "context": "More recent work has combined a probabilistic extension of ASP with POMDPs for commonsense inference and probabilistic planning in human-robot dialog [54], used a probabilistic extension of ASP to determine some model parameters of POMDPs [50], used ASP-based architecture to support learning of action costs on a robot [29], and combined logic programming and relational reinforcement learning to interactively and cumulatively discover domain axioms [45] and affordances [46].", "startOffset": 320, "endOffset": 324}, {"referenceID": 43, "context": "More recent work has combined a probabilistic extension of ASP with POMDPs for commonsense inference and probabilistic planning in human-robot dialog [54], used a probabilistic extension of ASP to determine some model parameters of POMDPs [50], used ASP-based architecture to support learning of action costs on a robot [29], and combined logic programming and relational reinforcement learning to interactively and cumulatively discover domain axioms [45] and affordances [46].", "startOffset": 452, "endOffset": 456}, {"referenceID": 44, "context": "More recent work has combined a probabilistic extension of ASP with POMDPs for commonsense inference and probabilistic planning in human-robot dialog [54], used a probabilistic extension of ASP to determine some model parameters of POMDPs [50], used ASP-based architecture to support learning of action costs on a robot [29], and combined logic programming and relational reinforcement learning to interactively and cumulatively discover domain axioms [45] and affordances [46].", "startOffset": 473, "endOffset": 477}, {"referenceID": 35, "context": "Bayesian Logic relaxes the unique name constraint of first-order probabilistic languages to provide a compact representation of distributions over varying sets of objects [36].", "startOffset": 171, "endOffset": 175}, {"referenceID": 37, "context": "Other examples include independent choice logic [38], PRISM [21], probabilistic first-order logic [22], first-order relational POMDPs [26, 41], and Plog that assigns probabilities to different possible worlds represented as answer sets of ASP programs [9, 33].", "startOffset": 48, "endOffset": 52}, {"referenceID": 20, "context": "Other examples include independent choice logic [38], PRISM [21], probabilistic first-order logic [22], first-order relational POMDPs [26, 41], and Plog that assigns probabilities to different possible worlds represented as answer sets of ASP programs [9, 33].", "startOffset": 60, "endOffset": 64}, {"referenceID": 21, "context": "Other examples include independent choice logic [38], PRISM [21], probabilistic first-order logic [22], first-order relational POMDPs [26, 41], and Plog that assigns probabilities to different possible worlds represented as answer sets of ASP programs [9, 33].", "startOffset": 98, "endOffset": 102}, {"referenceID": 25, "context": "Other examples include independent choice logic [38], PRISM [21], probabilistic first-order logic [22], first-order relational POMDPs [26, 41], and Plog that assigns probabilities to different possible worlds represented as answer sets of ASP programs [9, 33].", "startOffset": 134, "endOffset": 142}, {"referenceID": 39, "context": "Other examples include independent choice logic [38], PRISM [21], probabilistic first-order logic [22], first-order relational POMDPs [26, 41], and Plog that assigns probabilities to different possible worlds represented as answer sets of ASP programs [9, 33].", "startOffset": 134, "endOffset": 142}, {"referenceID": 8, "context": "Other examples include independent choice logic [38], PRISM [21], probabilistic first-order logic [22], first-order relational POMDPs [26, 41], and Plog that assigns probabilities to different possible worlds represented as answer sets of ASP programs [9, 33].", "startOffset": 252, "endOffset": 259}, {"referenceID": 32, "context": "Other examples include independent choice logic [38], PRISM [21], probabilistic first-order logic [22], first-order relational POMDPs [26, 41], and Plog that assigns probabilities to different possible worlds represented as answer sets of ASP programs [9, 33].", "startOffset": 252, "endOffset": 259}, {"referenceID": 45, "context": "For instance, we developed an architecture that coupled planning based on a hierarchy of POMDPs [47, 52] with ASPbased inference.", "startOffset": 96, "endOffset": 104}, {"referenceID": 50, "context": "For instance, we developed an architecture that coupled planning based on a hierarchy of POMDPs [47, 52] with ASPbased inference.", "startOffset": 96, "endOffset": 104}, {"referenceID": 51, "context": "ASP-based inference provided priors for POMDP state estimation, and observations and historical data from comparable domains were considered for reasoning about the presence of target objects in the domain [53].", "startOffset": 206, "endOffset": 210}, {"referenceID": 42, "context": "Building on recent work [44, 51], this paper describes a general refinement-based architecture for knowledge representation and reasoning in robotics.", "startOffset": 24, "endOffset": 32}, {"referenceID": 49, "context": "Building on recent work [44, 51], this paper describes a general refinement-based architecture for knowledge representation and reasoning in robotics.", "startOffset": 24, "endOffset": 32}, {"referenceID": 15, "context": "In this paper, we extend action language ALd [16, 17, 18] (we preserve the old name for simplicity) to allow non-boolean fluents and non-deterministic causal laws.", "startOffset": 45, "endOffset": 57}, {"referenceID": 16, "context": "In this paper, we extend action language ALd [16, 17, 18] (we preserve the old name for simplicity) to allow non-boolean fluents and non-deterministic causal laws.", "startOffset": 45, "endOffset": 57}, {"referenceID": 17, "context": "In this paper, we extend action language ALd [16, 17, 18] (we preserve the old name for simplicity) to allow non-boolean fluents and non-deterministic causal laws.", "startOffset": 45, "endOffset": 57}, {"referenceID": 4, "context": "2This representation of relations as functions, in the context of ASP, is based on prior work [5].", "startOffset": 94, "endOffset": 97}, {"referenceID": 15, "context": "We define the states and transitions of \u03c4(D) in terms of answer sets of logic programs, as described below; see [16, 18] for more details.", "startOffset": 112, "endOffset": 120}, {"referenceID": 17, "context": "We define the states and transitions of \u03c4(D) in terms of answer sets of logic programs, as described below; see [16, 18] for more details.", "startOffset": 112, "endOffset": 120}, {"referenceID": 16, "context": "Although well-foundedness is not easy to check, the broad syntactic condition called weak-acyclicity, which is easy to check, is a sufficient condition for well-foundedness [17].", "startOffset": 173, "endOffset": 177}, {"referenceID": 17, "context": "For more details about CR rules, please see [18].", "startOffset": 44, "endOffset": 48}, {"referenceID": 2, "context": "\u2022 A reality check [3]: \u2190val(F,Y1, I), obs(F = Y2, I), Y1 6= Y2 (15) \u2190val(F,Y1, I), obs(F 6= Y1, I)", "startOffset": 18, "endOffset": 21}, {"referenceID": 17, "context": ", [18].", "startOffset": 2, "endOffset": 6}, {"referenceID": 1, "context": "For planning and diagnostics, this program is passed to an ASP solver\u2014we use SPARC, which expands CR-Prolog and provides explicit constructs to specify objects, relations, and their sorts [2].", "startOffset": 188, "endOffset": 191}, {"referenceID": 17, "context": "Prior research results in the theory of action languages and ASP ensure that the plan is provably correct [18].", "startOffset": 106, "endOffset": 110}, {"referenceID": 26, "context": "This formulation of a POMDP builds on the standard formulation [27], and the tuple\u2019s elements are: \u2022 AP: set of concrete actions available to the robot.", "startOffset": 63, "endOffset": 67}, {"referenceID": 0, "context": "\u2022 T P : SP\u00d7AP\u00d7SP\u2192 [0,1], the transition function, which defines the probability of transitioning to each p-state when particular actions are executed in particular p-states.", "startOffset": 18, "endOffset": 23}, {"referenceID": 0, "context": "\u2022 OP : SP\u00d7AP\u00d7ZP \u2192 [0,1], the observation function, which defines the probability of each observation in ZP when particular actions are executed in particular p-states.", "startOffset": 18, "endOffset": 23}, {"referenceID": 36, "context": ", expected cumulative discounted reward) over a planning horizon\u2014we use a point-based approximate solver [37].", "startOffset": 105, "endOffset": 109}, {"referenceID": 26, "context": "This statement is based on existing literature [27, 35, 43].", "startOffset": 47, "endOffset": 59}, {"referenceID": 34, "context": "This statement is based on existing literature [27, 35, 43].", "startOffset": 47, "endOffset": 59}, {"referenceID": 41, "context": "This statement is based on existing literature [27, 35, 43].", "startOffset": 47, "endOffset": 59}, {"referenceID": 36, "context": ", loss in value) achieved by following the computed policy in comparison with the optimal policy [37].", "startOffset": 97, "endOffset": 101}, {"referenceID": 36, "context": "We can also provide a bound on the margin of error [37].", "startOffset": 51, "endOffset": 55}, {"referenceID": 47, "context": "For instance, the robot builds a map of the domain and estimates its position in the map using a Particle Filter algorithm for Simultaneous Localization and Mapping (SLAM) [49].", "startOffset": 172, "endOffset": 176}, {"referenceID": 50, "context": "In addition, the simulator represents objects using probabilistic functions of features extracted from images, with the corresponding models being acquired in an initial training phase\u2014see [52] for more details about such models.", "startOffset": 189, "endOffset": 193}, {"referenceID": 51, "context": "It is thus easier to understand, and to identify and fix errors in, the observed behavior, in comparison with architectures that consider all the available knowledge or only support probabilistic reasoning [53].", "startOffset": 206, "endOffset": 210}, {"referenceID": 45, "context": "These results are not shown in Figure 6, but they are documented in prior papers evaluating just the probabilistic component of the proposed architecture [47, 52].", "startOffset": 154, "endOffset": 162}, {"referenceID": 50, "context": "These results are not shown in Figure 6, but they are documented in prior papers evaluating just the probabilistic component of the proposed architecture [47, 52].", "startOffset": 154, "endOffset": 162}, {"referenceID": 33, "context": "Visual object recognition is based on learned object models that consist of appearance-based and contextual visual cues [34].", "startOffset": 120, "endOffset": 124}, {"referenceID": 12, "context": "The domain maps are learned and revised by the robot using laser range finder data and the existing ROS implementation of a SLAM algorithm [13].", "startOffset": 139, "endOffset": 143}, {"referenceID": 45, "context": "To make it feasible to use POMDP-1 in such large domains, we used our prior work on a hierarchical decomposition of POMDPs for visual sensing and information processing that supports automatic belief propagation across the levels of the hierarchy and model generation in each level of the hierarchy [47, 52].", "startOffset": 299, "endOffset": 307}, {"referenceID": 50, "context": "To make it feasible to use POMDP-1 in such large domains, we used our prior work on a hierarchical decomposition of POMDPs for visual sensing and information processing that supports automatic belief propagation across the levels of the hierarchy and model generation in each level of the hierarchy [47, 52].", "startOffset": 299, "endOffset": 307}, {"referenceID": 11, "context": "We have, for instance, explored different resolutions for reasoning probabilistically [12], and investigated the inference, planning and diagnostics capabilities of architectures that reason at different resolutions [53].", "startOffset": 86, "endOffset": 90}, {"referenceID": 51, "context": "We have, for instance, explored different resolutions for reasoning probabilistically [12], and investigated the inference, planning and diagnostics capabilities of architectures that reason at different resolutions [53].", "startOffset": 216, "endOffset": 220}, {"referenceID": 44, "context": ", based on combining relational reinforcement learning with declarative programming, has provided some promising results [46, 45], and we seek to further explore this direction of work in the future.", "startOffset": 121, "endOffset": 129}, {"referenceID": 43, "context": ", based on combining relational reinforcement learning with declarative programming, has provided some promising results [46, 45], and we seek to further explore this direction of work in the future.", "startOffset": 121, "endOffset": 129}, {"referenceID": 3, "context": "Proof of the proposition3: First, we use the Splitting Set Theorem for CR-Prolog [4] to simplify \u03a0(D ,H ) by eliminating from it occurrences of atoms formed by relations obs and hpd.", "startOffset": 81, "endOffset": 84}, {"referenceID": 36, "context": "The elements of the corresponding POMDP are described (below) in the format of the approximate POMDP solver used in our experiments [37].", "startOffset": 132, "endOffset": 136}, {"referenceID": 0, "context": "% T : action : S x S\u2019 -> [0, 1] % Probability of transition from first element of S to that of S\u2019 is % in the top left corner of each matrix T: move-0 1 0 0 0 0 0 0 0.", "startOffset": 25, "endOffset": 31}, {"referenceID": 0, "context": "% Observation function format(s) % O : action : s_i : z_i -> [0, 1] (or) % : S x Z -> [0, 1] % In each matrix, first row provides probability of each possible % observation in the first p-state in S O: move-0 : * : none 1", "startOffset": 61, "endOffset": 67}, {"referenceID": 0, "context": "% Observation function format(s) % O : action : s_i : z_i -> [0, 1] (or) % : S x Z -> [0, 1] % In each matrix, first row provides probability of each possible % observation in the first p-state in S O: move-0 : * : none 1", "startOffset": 86, "endOffset": 92}], "year": 2017, "abstractText": "This paper describes an architecture that combines the complementary strengths of probabilistic graphical models and declarative programming to enable robots to represent and reason with logic-based and probabilistic descriptions of uncertainty and domain knowledge. An action language is extended to support non-boolean fluents and nondeterministic causal laws. This action language is used to describe tightly-coupled transition diagrams at two levels of granularity, refining a coarse-resolution transition diagram of the domain to obtain a fine-resolution transition diagram. The coarse-resolution system description, and a history that includes (prioritized) defaults, are translated into an Answer Set Prolog (ASP) program. For any given goal, inference in the ASP program provides a plan of abstract actions. To implement each such abstract action probabilistically, the part of the fine-resolution transition diagram relevant to this action is identified, and a probabilistic representation of the uncertainty in sensing and actuation is included and used to construct a partially observable Markov decision process (POMDP). The policy obtained by solving the POMDP is invoked repeatedly to implement the abstract action as a sequence of concrete actions, with the corresponding observations being recorded in the coarse-resolution history and used for subsequent reasoning. The architecture is evaluated in simulation and on a mobile robot moving objects in an indoor domain, to show that it supports reasoning with violation of defaults, noisy observations and unreliable actions, in complex domains.", "creator": "LaTeX with hyperref package"}}}