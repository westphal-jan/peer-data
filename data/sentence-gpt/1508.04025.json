{"id": "1508.04025", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Aug-2015", "title": "Effective Approaches to Attention-based Neural Machine Translation", "abstract": "An attentional mechanism has been used in neural machine translation (NMT) to selectively focus on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. A local approach, however, relies on a local approach: a network of individual regions that have different attentional mechanisms. For example, one approach to local attentional memory has been proposed using a local approach: a local approach, rather than relying on an individual region, is used to select all current word samples, which would then be translated into an area of the word's input word: a local approach. For example, an average local approach is used to select a group of words from one or two regions, and the average local approach is used to select each word samples. Finally, while a local approach, for example, focuses solely on finding the first word, the total number of local words that a local approach is focused on is increased, and an average local approach is used to select an individual word sample (n = 12). A local approach is used to select each word sample, but only the local approach does so on the order of magnitude.\n\n\n\nThe goal of this paper was to study the impact of neural machine translation on NMT on learning. It was a preliminary report published by the American Association of Neuroscientists (AANA), a California State University, San Diego, California. It is supported by the National Science Foundation and a National Institute of Health (NIDA). The National Science Foundation is supporting the efforts of the AANA and the National Institute of Health (NIDA).\nNMT is an advanced neurochemical and neurochemical technology designed to provide the ability to provide information about a region that may not always be able to be processed into a whole, with a high number of distinct regions for each word.\nThe first step was to study the impact of neural machine translation on learning. The first step was to study the impact of neural machine translation on learning. The first step was to study the impact of neural machine translation on learning. The first step was to study the impact of neural machine translation on learning. The first step was to study the impact of neural machine translation on learning. The first step was to study the impact of neural machine translation on learning. The first step was to study", "histories": [["v1", "Mon, 17 Aug 2015 13:43:19 GMT  (91kb)", "https://arxiv.org/abs/1508.04025v1", "EMNLP 2015 camera-ready version"], ["v2", "Tue, 18 Aug 2015 10:27:26 GMT  (91kb)", "http://arxiv.org/abs/1508.04025v2", "EMNLP 2015 camera-ready version"], ["v3", "Wed, 19 Aug 2015 08:14:59 GMT  (92kb)", "http://arxiv.org/abs/1508.04025v3", "11 pages, EMNLP 2015 camera-ready version"], ["v4", "Sat, 29 Aug 2015 09:03:04 GMT  (95kb)", "http://arxiv.org/abs/1508.04025v4", "11 pages, 7 figures, EMNLP 2015 camera-ready version, minor fixes"], ["v5", "Sun, 20 Sep 2015 08:25:52 GMT  (73kb)", "http://arxiv.org/abs/1508.04025v5", "11 pages, 7 figures, EMNLP 2015 camera-ready version, more training details"]], "COMMENTS": "EMNLP 2015 camera-ready version", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["thang luong", "hieu pham", "christopher d manning"], "accepted": true, "id": "1508.04025"}, "pdf": {"name": "1508.04025.pdf", "metadata": {"source": "CRF", "title": "Effective Approaches to Attention-based Neural Machine Translation", "authors": ["Minh-Thang Luong Hieu Pham", "Christopher D. Manning"], "emails": ["lmthang@stanford.edu", "hyhieu@stanford.edu", "manning@stanford.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 8.\n04 02\n5v 5\n[ cs\n.C L\n] 2\n0 Se\np 20\n15"}, {"heading": "1 Introduction", "text": "Neural Machine Translation (NMT) achieved state-of-the-art performances in large-scale translation tasks such as from English to French (Luong et al., 2015) and English to German (Jean et al., 2015). NMT is appealing since it requires minimal domain knowledge and is conceptually simple. The model by Luong et al. (2015) reads through all the source words until the end-ofsentence symbol <eos> is reached. It then starts\n1All our code and models are publicly available at http://nlp.stanford.edu/projects/nmt.\nemitting one target word at a time, as illustrated in Figure 1. NMT is often a large neural network that is trained in an end-to-end fashion and has the ability to generalize well to very long word sequences. This means the model does not have to explicitly store gigantic phrase tables and language models as in the case of standard MT; hence, NMT has a small memory footprint. Lastly, implementing NMT decoders is easy unlike the highly intricate decoders in standard MT (Koehn et al., 2003).\nIn parallel, the concept of \u201cattention\u201d has gained popularity recently in training neural networks, allowing models to learn alignments between different modalities, e.g., between image objects and agent actions in the dynamic control problem (Mnih et al., 2014), between speech frames and text in the speech recognition task (?), or between visual features of a picture and its text description in the image caption generation task (Xu et al., 2015). In the context of NMT, Bahdanau et al. (2015) has successfully applied such attentional mechanism to jointly translate and align words. To the best of our knowledge, there has not been any other work exploring the use of attention-based architectures for NMT.\nIn this work, we design, with simplicity and ef-\nfectiveness in mind, two novel types of attentionbased models: a global approach in which all source words are attended and a local one whereby only a subset of source words are considered at a time. The former approach resembles the model of (Bahdanau et al., 2015) but is simpler architecturally. The latter can be viewed as an interesting blend between the hard and soft attention models proposed in (Xu et al., 2015): it is computationally less expensive than the global model or the soft attention; at the same time, unlike the hard attention, the local attention is differentiable almost everywhere, making it easier to implement and train.2 Besides, we also examine various alignment functions for our attention-based models.\nExperimentally, we demonstrate that both of our approaches are effective in the WMT translation tasks between English and German in both directions. Our attentional models yield a boost of up to 5.0 BLEU over non-attentional systems which already incorporate known techniques such as dropout. For English to German translation, we achieve new state-of-the-art (SOTA) results for both WMT\u201914 and WMT\u201915, outperforming previous SOTA systems, backed by NMT models and n-gram LM rerankers, by more than 1.0 BLEU. We conduct extensive analysis to evaluate our models in terms of learning, the ability to handle long sentences, choices of attentional architectures, alignment quality, and translation outputs."}, {"heading": "2 Neural Machine Translation", "text": "A neural machine translation system is a neural network that directly models the conditional probability p(y|x) of translating a source sentence, x1, . . . , xn, to a target sentence, y1, . . . , ym.3 A basic form of NMT consists of two components: (a) an encoder which computes a representation s for each source sentence and (b) a decoder which generates one target word at a time and hence decomposes the conditional probability as:\nlog p(y|x) = \u2211m\nj=1 log p (yj|y<j, s) (1)\nA natural choice to model such a decomposition in the decoder is to use a\n2There is a recent work by Gregor et al. (2015), which is very similar to our local attention and applied to the image generation task. However, as we detail later, our model is much simpler and can achieve good performance for NMT.\n3All sentences are assumed to terminate with a special \u201cend-of-sentence\u201d token <eos>.\nrecurrent neural network (RNN) architecture, which most of the recent NMT work such as (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common. They, however, differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation s.\nKalchbrenner and Blunsom (2013) used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation. On the other hand, both Sutskever et al. (2014) and Luong et al. (2015) stacked multiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit for both the encoder and the decoder. Cho et al. (2014), Bahdanau et al. (2015), and Jean et al. (2015) all adopted a different version of the RNN with an LSTM-inspired hidden unit, the gated recurrent unit (GRU), for both components.4\nIn more detail, one can parameterize the probability of decoding each word yj as:\np (yj|y<j, s) = softmax (g (hj)) (2)\nwith g being the transformation function that outputs a vocabulary-sized vector.5 Here, hj is the RNN hidden unit, abstractly computed as:\nhj = f(hj\u22121, s), (3)\nwhere f computes the current hidden state given the previous hidden state and can be either a vanilla RNN unit, a GRU, or an LSTM unit. In (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015), the source representation s is only used once to initialize the decoder hidden state. On the other hand, in (Bahdanau et al., 2015; Jean et al., 2015) and this work, s, in fact, implies a set of source hidden states which are consulted throughout the entire course of the translation process. Such an approach is referred to as an attention mechanism, which we will discuss next.\nIn this work, following (Sutskever et al., 2014; Luong et al., 2015), we use the stacking LSTM architecture for our NMT systems, as illustrated\n4They all used a single RNN layer except for the latter two works which utilized a bidirectional RNN for the encoder.\n5One can provide g with other inputs such as the currently predicted word yj as in (Bahdanau et al., 2015).\nin Figure 1. We use the LSTM unit defined in (Zaremba et al., 2015). Our training objective is formulated as follows:\nJt = \u2211\n(x,y)\u2208D \u2212 log p(y|x) (4)\nwith D being our parallel training corpus."}, {"heading": "3 Attention-based Models", "text": "Our various attention-based models are classifed into two broad categories, global and local. These classes differ in terms of whether the \u201cattention\u201d is placed on all source positions or on only a few source positions. We illustrate these two model types in Figure 2 and 3 respectively.\nCommon to these two types of models is the fact that at each time step t in the decoding phase, both approaches first take as input the hidden state ht at the top layer of a stacking LSTM. The goal is then to derive a context vector ct that captures relevant source-side information to help predict the current target word yt. While these models differ in how the context vector ct is derived, they share the same subsequent steps.\nSpecifically, given the target hidden state ht and the source-side context vector ct, we employ a simple concatenation layer to combine the information from both vectors to produce an attentional hidden state as follows:\nh\u0303t = tanh(Wc[ct;ht]) (5)\nThe attentional vector h\u0303t is then fed through the softmax layer to produce the predictive distribution formulated as:\np(yt|y<t, x) = softmax(Wsh\u0303t) (6)\nWe now detail how each model type computes the source-side context vector ct."}, {"heading": "3.1 Global Attention", "text": "The idea of a global attentional model is to consider all the hidden states of the encoder when deriving the context vector ct. In this model type, a variable-length alignment vector at, whose size equals the number of time steps on the source side, is derived by comparing the current target hidden state ht with each source hidden state h\u0304s:\nat(s) = align(ht, h\u0304s) (7)\n= exp\n( score(ht, h\u0304s) )\n\u2211 s\u2032 exp ( score(ht, h\u0304s\u2032) )\nHere, score is referred as a content-based function for which we consider three different alternatives:\nscore(ht, h\u0304s)=\n\n \n \nh \u22a4 t h\u0304s dot h \u22a4 t Wah\u0304s general v \u22a4 a tanh ( Wa[ht; h\u0304s] ) concat\nBesides, in our early attempts to build attentionbased models, we use a location-based function in which the alignment scores are computed from solely the target hidden state ht as follows:\nat = softmax(Waht) location (8)\nGiven the alignment vector as weights, the context vector ct is computed as the weighted average over all the source hidden states.6\nComparison to (Bahdanau et al., 2015) \u2013 While our global attention approach is similar in spirit to the model proposed by Bahdanau et al. (2015), there are several key differences which reflect how we have both simplified and generalized from the original model. First, we simply use hidden states at the top LSTM layers in both the encoder and decoder as illustrated in Figure 2. Bahdanau et al. (2015), on the other hand, use the concatenation of the forward and backward source hidden states in the bi-directional encoder\n6Eq. (8) implies that all alignment vectors at are of the same length. For short sentences, we only use the top part of at and for long sentences, we ignore words near the end.\nand target hidden states in their non-stacking unidirectional decoder. Second, our computation path is simpler; we go from ht \u2192 at \u2192 ct \u2192 h\u0303t then make a prediction as detailed in Eq. (5), Eq. (6), and Figure 2. On the other hand, at any time t, Bahdanau et al. (2015) build from the previous hidden state ht\u22121 \u2192 at \u2192 ct \u2192 ht, which, in turn, goes through a deep-output and a maxout layer before making predictions.7 Lastly, Bahdanau et al. (2015) only experimented with one alignment function, the concat product; whereas we show later that the other alternatives are better."}, {"heading": "3.2 Local Attention", "text": "The global attention has a drawback that it has to attend to all words on the source side for each target word, which is expensive and can potentially render it impractical to translate longer sequences, e.g., paragraphs or documents. To address this deficiency, we propose a local attentional mechanism that chooses to focus only on a small subset of the source positions per target word.\nThis model takes inspiration from the tradeoff between the soft and hard attentional models proposed by Xu et al. (2015) to tackle the image caption generation task. In their work, soft attention\n7We will refer to this difference again in Section 3.3.\nrefers to the global attention approach in which weights are placed \u201csoftly\u201d over all patches in the source image. The hard attention, on the other hand, selects one patch of the image to attend to at a time. While less expensive at inference time, the hard attention model is non-differentiable and requires more complicated techniques such as variance reduction or reinforcement learning to train.\nOur local attention mechanism selectively focuses on a small window of context and is differentiable. This approach has an advantage of avoiding the expensive computation incurred in the soft attention and at the same time, is easier to train than the hard attention approach. In concrete details, the model first generates an aligned position pt for each target word at time t. The context vector ct is then derived as a weighted average over the set of source hidden states within the window [pt\u2212D, pt+D]; D is empirically selected.8 Unlike the global approach, the local alignment vector at is now fixed-dimensional, i.e., \u2208 R2D+1. We consider two variants of the model as below.\nMonotonic alignment (local-m) \u2013 we simply set pt = t assuming that source and target sequences are roughly monotonically aligned. The alignment vector at is defined according to Eq. (7).9\nPredictive alignment (local-p) \u2013 instead of assuming monotonic alignments, our model predicts an aligned position as follows:\npt = S \u00b7 sigmoid(v \u22a4 p tanh(Wpht)), (9)\nWp and vp are the model parameters which will be learned to predict positions. S is the source sentence length. As a result of sigmoid, pt \u2208 [0, S]. To favor alignment points near pt, we place a Gaussian distribution centered around pt . Specifically, our alignment weights are now defined as:\nat(s) = align(ht, h\u0304s) exp\n(\n\u2212 (s\u2212 pt)\n2\n2\u03c32\n)\n(10)\nWe use the same align function as in Eq. (7) and the standard deviation is empirically set as \u03c3= D2 . Note that pt is a real nummber; whereas s is an integer within the window centered at pt.10\n8If the window crosses the sentence boundaries, we simply ignore the outside part and consider words in the window.\n9local-m is the same as the global model except that the vector at is fixed-length and shorter.\n10local-p is similar to the local-m model except that we dynamically compute pt and use a truncated Gaussian distribution to modify the original alignment weights align(ht, h\u0304s) as shown in Eq. (10). By utilizing pt to derive at, we can compute backprop gradients for Wp and vp. This model is differentiable almost everywhere.\nComparison to (Gregor et al., 2015) \u2013 have proposed a selective attention mechanism, very similar to our local attention, for the image generation task. Their approach allows the model to select an image patch of varying location and zoom. We, instead, use the same \u201czoom\u201d for all target positions, which greatly simplifies the formulation and still achieves good performance."}, {"heading": "3.3 Input-feeding Approach", "text": "In our proposed global and local approaches, the attentional decisions are made independently, which is suboptimal. Whereas, in standard MT, a coverage set is often maintained during the translation process to keep track of which source words have been translated. Likewise, in attentional NMTs, alignment decisions should be made jointly taking into account past alignment information. To address that, we propose an inputfeeding approach in which attentional vectors h\u0303t are concatenated with inputs at the next time steps as illustrated in Figure 4.11 The effects of having such connections are two-fold: (a) we hope to make the model fully aware of previous alignment choices and (b) we create a very deep network spanning both horizontally and vertically.\nComparison to other work \u2013 Bahdanau et al. (2015) use context vectors, similar to our ct, in building subsequent hidden states, which can also achieve the \u201ccoverage\u201d effect. However, there has not been any analysis of whether such connections are useful as done\n11If n is the number of LSTM cells, the input size of the first LSTM layer is 2n; those of subsequent layers are n.\nin this work. Also, our approach is more general; as illustrated in Figure 4, it can be applied to general stacking recurrent architectures, including non-attentional models.\nXu et al. (2015) propose a doubly attentional approach with an additional constraint added to the training objective to make sure the model pays equal attention to all parts of the image during the caption generation process. Such a constraint can also be useful to capture the coverage set effect in NMT that we mentioned earlier. However, we chose to use the input-feeding approach since it provides flexibility for the model to decide on any attentional constraints it deems suitable."}, {"heading": "4 Experiments", "text": "We evaluate the effectiveness of our models on the WMT translation tasks between English and German in both directions. newstest2013 (3000 sentences) is used as a development set to select our hyperparameters. Translation performances are reported in case-sensitive BLEU (Papineni et al., 2002) on newstest2014 (2737 sentences) and newstest2015 (2169 sentences). Following (Luong et al., 2015), we report translation quality using two types of BLEU: (a) tokenized12 BLEU to be comparable with existing NMT work and (b) NIST13 BLEU to be comparable with WMT results."}, {"heading": "4.1 Training Details", "text": "All our models are trained on the WMT\u201914 training data consisting of 4.5M sentences pairs (116M English words, 110M German words). Similar to (Jean et al., 2015), we limit our vocabularies to be the top 50K most frequent words for both languages. Words not in these shortlisted vocabularies are converted into a universal token <unk>.\nWhen training our NMT systems, following (Bahdanau et al., 2015; Jean et al., 2015), we filter out sentence pairs whose lengths exceed 50 words and shuffle mini-batches as we proceed. Our stacking LSTM models have 4 layers, each with 1000 cells, and 1000-dimensional embeddings. We follow (Sutskever et al., 2014; Luong et al., 2015) in training NMT with similar settings: (a) our parameters are uniformly initialized in [\u22120.1, 0.1], (b) we train for 10 epochs us-\n12All texts are tokenized with tokenizer.perl and BLEU scores are computed with multi-bleu.perl.\n13With the mteval-v13a script as per WMT guideline.\ning plain SGD, (c) a simple learning rate schedule is employed \u2013 we start with a learning rate of 1; after 5 epochs, we begin to halve the learning rate every epoch, (d) our mini-batch size is 128, and (e) the normalized gradient is rescaled whenever its norm exceeds 5. Additionally, we also use dropout with probability 0.2 for our LSTMs as suggested by (Zaremba et al., 2015). For dropout models, we train for 12 epochs and start halving the learning rate after 8 epochs. For local attention models, we empirically set the window size D = 10.\nOur code is implemented in MATLAB. When running on a single GPU device Tesla K40, we achieve a speed of 1K target words per second. It takes 7\u201310 days to completely train a model."}, {"heading": "4.2 English-German Results", "text": "We compare our NMT systems in the EnglishGerman task with various other systems. These include the winning system in WMT\u201914 (Buck et al., 2014), a phrase-based system whose language models were trained on a huge monolingual text, the Common Crawl corpus. For end-to-end NMT systems, to the best of our knowledge, (Jean et al., 2015) is the only work experimenting with this language pair and currently the SOTA system. We only present results for some of our attention models and will later analyze the rest in Section 5.\nAs shown in Table 1, we achieve pro-\ngressive improvements when (a) reversing the source sentence, +1.3 BLEU, as proposed in (Sutskever et al., 2014) and (b) using dropout, +1.4 BLEU. On top of that, (c) the global attention approach gives a significant boost of +2.8 BLEU, making our model slightly better than the base attentional system of Bahdanau et al. (2015) (row RNNSearch). When (d) using the inputfeeding approach, we seize another notable gain of +1.3 BLEU and outperform their system. The local attention model with predictive alignments (row local-p) proves to be even better, giving us a further improvement of +0.9 BLEU on top of the global attention model. It is interesting to observe the trend previously reported in (Luong et al., 2015) that perplexity strongly correlates with translation quality. In total, we achieve a significant gain of 5.0 BLEU points over the non-attentional baseline, which already includes known techniques such as source reversing and dropout.\nThe unknown replacement technique proposed in (Luong et al., 2015; Jean et al., 2015) yields another nice gain of +1.9 BLEU, demonstrating that our attentional models do learn useful alignments for unknown works. Finally, by ensembling 8 different models of various settings, e.g., using different attention approaches, with and without dropout etc., we were able to achieve a new SOTA result of 23.0 BLEU, outperforming the existing\nbest system (Jean et al., 2015) by +1.4 BLEU.\nLatest results in WMT\u201915 \u2013 despite the fact that our models were trained on WMT\u201914 with slightly less data, we test them on newstest2015 to demonstrate that they can generalize well to different test sets. As shown in Table 2, our best system establishes a new SOTA performance of 25.9 BLEU, outperforming the existing best system backed by NMT and a 5-gram LM reranker by +1.0 BLEU."}, {"heading": "4.3 German-English Results", "text": "We carry out a similar set of experiments for the WMT\u201915 translation task from German to English. While our systems have not yet matched the performance of the SOTA system, we nevertheless show the effectiveness of our approaches with large and progressive gains in terms of BLEU as illustrated in Table 3. The attentional mechanism gives us +2.2 BLEU gain and on top of that, we obtain another boost of up to +1.0 BLEU from the input-feeding approach. Using a better alignment function, the content-based dot product one, together with dropout yields another gain of +2.7 BLEU. Lastly, when applying the unknown word replacement technique, we seize an additional +2.1 BLEU, demonstrating the usefulness of attention in aligning rare words."}, {"heading": "5 Analysis", "text": "We conduct extensive analysis to better understand our models in terms of learning, the ability to handle long sentences, choices of attentional architectures, and alignment quality. All results reported here are on English-German newstest2014."}, {"heading": "5.1 Learning curves", "text": "We compare models built on top of one another as listed in Table 1. It is pleasant to observe in Figure 5 a clear separation between non-attentional and attentional models. The input-feeding approach and the local attention model also demonstrate their abilities in driving the test costs lower. The non-attentional model with dropout (the blue\n+ curve) learns slower than other non-dropout models, but as time goes by, it becomes more robust in terms of minimizing test errors."}, {"heading": "5.2 Effects of Translating Long Sentences", "text": "We follow (Bahdanau et al., 2015) to group sentences of similar lengths together and compute a BLEU score per group. Figure 6 shows that our attentional models are more effective than the non-attentional one in handling long sentences: the quality does not degrade as sentences become longer. Our best model (the blue + curve) outperforms all other systems in all length buckets."}, {"heading": "5.3 Choices of Attentional Architectures", "text": "We examine different attention models (global, local-m, local-p) and different alignment functions (location, dot, general, concat) as described in Section 3. Due to limited resources, we cannot run all the possible combinations. However, results in Table 4 do give us some idea about different choices. The location-based function does\nnot learn good alignments: the global (location) model can only obtain a small gain when performing unknown word replacement compared to using other alignment functions.14 For contentbased functions, our implementation concat does not yield good performances and more analysis should be done to understand the reason.15 It is interesting to observe that dot works well for the global attention and general is better for the local attention. Among the different models, the local attention model with predictive alignments (localp) is best, both in terms of perplexities and BLEU."}, {"heading": "5.4 Alignment Quality", "text": "A by-product of attentional models are word alignments. While (Bahdanau et al., 2015) visualized\n14There is a subtle difference in how we retrieve alignments for the different alignment functions. At time step t in which we receive yt\u22121 as input and then compute ht,at, ct, and h\u0303t before predicting yt, the alignment vector at is used as alignment weights for (a) the predicted word yt in the location-based alignment functions and (b) the input word yt\u22121 in the content-based functions.\n15With concat, the perplexities achieved by different models are 6.7 (global), 7.1 (local-m), and 7.1 (local-p). Such high perplexities could be due to the fact that we simplify the matrix Wa to set the part that corresponds to h\u0304s to identity.\nalignments for some sample sentences and observed gains in translation quality as an indication of a working attention model, no work has assessed the alignments learned as a whole. In contrast, we set out to evaluate the alignment quality using the alignment error rate (AER) metric.\nGiven the gold alignment data provided by RWTH for 508 English-German Europarl sentences, we \u201cforce\u201d decode our attentional models to produce translations that match the references. We extract only one-to-one alignments by selecting the source word with the highest alignment weight per target word. Nevertheless, as shown in Table 6, we were able to achieve AER scores comparable to the one-to-many alignments obtained by the Berkeley aligner (Liang et al., 2006).16\nWe also found that the alignments produced by local attention models achieve lower AERs than those of the global one. The AER obtained by the ensemble, while good, is not better than the localm AER, suggesting the well-known observation that AER and translation scores are not well correlated (Fraser and Marcu, 2007). We show some alignment visualizations in Appendix A."}, {"heading": "5.5 Sample Translations", "text": "We show in Table 5 sample translations in both directions. It it appealing to observe the effect of attentional models in correctly translating names such as \u201cMiranda Kerr\u201d and \u201cRoger Dow\u201d. Non-attentional models, while producing sensible names from a language model perspective, lack the direct connections from the source side to make correct translations. We also observed an interesting case in the second example, which requires translating the doubly-negated phrase, \u201cnot incompatible\u201d. The attentional model correctly produces \u201cnicht . . . unvereinbar\u201d; whereas the non-attentional model generates \u201cnicht verein-\n16We concatenate the 508 sentence pairs with 1M sentence pairs from WMT and run the Berkeley aligner.\nbar\u201d, meaning \u201cnot compatible\u201d.17 The attentional model also demonstrates its superiority in translating long sentences as in the last example."}, {"heading": "6 Conclusion", "text": "In this paper, we propose two simple and effective attentional mechanisms for neural machine translation: the global approach which always looks at all source positions and the local one that only attends to a subset of source positions at a time. We test the effectiveness of our models in the WMT translation tasks between English and German in both directions. Our local attention yields large gains of up to 5.0 BLEU over non-attentional\n17The reference uses a more fancy translation of \u201cincompatible\u201d, which is \u201cim Widerspruch zu etwas stehen\u201d. Both models, however, failed to translate \u201cpassenger experience\u201d.\nmodels which already incorporate known techniques such as dropout. For the English to German translation direction, our ensemble model has established new state-of-the-art results for both WMT\u201914 and WMT\u201915, outperforming existing best systems, backed by NMT models and n-gram LM rerankers, by more than 1.0 BLEU.\nWe have compared various alignment functions and shed light on which functions are best for which attentional models. Our analysis shows that attention-based NMT models are superior to nonattentional ones in many cases, for example in translating names and handling long sentences."}, {"heading": "Acknowledgment", "text": "We gratefully acknowledge support from a gift from Bloomberg L.P. and the support of NVIDIA\nCorporation with the donation of Tesla K40 GPUs. We thank Andrew Ng and his group as well as the Stanford Research Computing for letting us use their computing resources. We thank Russell Stewart for helpful discussions on the models. Lastly, we thank Quoc Le, Ilya Sutskever, Oriol Vinyals, Richard Socher, Michael Kayser, Jiwei Li, Panupong Pasupat, Kelvin Guu, members of the Stanford NLP Group and the annonymous reviewers for their valuable comments and feedback."}, {"heading": "A Alignment Visualization", "text": "We visualize the alignment weights produced by our different attention models in Figure 7. The visualization of the local attention model is much sharper than that of the global one. This contrast matches our expectation that local attention is designed to only focus on a subset of words each time. Also, since we translate from English to German and reverse the source English sentence, the white strides at the words \u201creality\u201d and \u201c.\u201d in the global attention model reveals an interesting access pattern: it tends to refer back to the beginning of the source sequence.\nCompared to the alignment visualizations in (Bahdanau et al., 2015), our alignment patterns are not as sharp as theirs. Such difference could possibly be due to the fact that translating from English to German is harder than translating into French as done in (Bahdanau et al., 2015), which is an interesting point to examine in future work."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau et al.2015] D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "N-gram counts and language models from the common crawl", "author": ["Buck et al.2014] Christian Buck", "Kenneth Heafield", "Bas van Ooyen"], "venue": null, "citeRegEx": "Buck et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Buck et al\\.", "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Measuring word alignment quality for statistical machine translation", "author": ["Fraser", "Marcu2007] Alexander Fraser", "Daniel Marcu"], "venue": "Computational Linguistics,", "citeRegEx": "Fraser et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fraser et al\\.", "year": 2007}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["Gregor et al.2015] Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra"], "venue": null, "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Jean et al.2015] S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": null, "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] N. Kalchbrenner", "P. Blunsom"], "venue": "In EMNLP", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Statistical phrase-based translation", "author": ["Koehn et al.2003] Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Alignment by agreement", "author": ["Liang et al.2006] P. Liang", "B. Taskar", "D. Klein"], "venue": null, "citeRegEx": "Liang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Luong et al.2015] M.-T. Luong", "I. Sutskever", "Q.V. Le", "O. Vinyals", "W. Zaremba"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["Mnih et al.2014] Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei jing Zhu"], "venue": null, "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["O. Vinyals", "Q.V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "venue": "In ICML", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["Ilya Sutskever", "Oriol Vinyals"], "venue": "In ICLR", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Neural Machine Translation (NMT) achieved state-of-the-art performances in large-scale translation tasks such as from English to French (Luong et al., 2015) and English to German (Jean et al.", "startOffset": 136, "endOffset": 156}, {"referenceID": 5, "context": ", 2015) and English to German (Jean et al., 2015).", "startOffset": 30, "endOffset": 49}, {"referenceID": 5, "context": ", 2015) and English to German (Jean et al., 2015). NMT is appealing since it requires minimal domain knowledge and is conceptually simple. The model by Luong et al. (2015) reads through all the source words until the end-ofsentence symbol <eos> is reached.", "startOffset": 31, "endOffset": 172}, {"referenceID": 7, "context": "Lastly, implementing NMT decoders is easy unlike the highly intricate decoders in standard MT (Koehn et al., 2003).", "startOffset": 94, "endOffset": 114}, {"referenceID": 10, "context": ", between image objects and agent actions in the dynamic control problem (Mnih et al., 2014), between speech frames and text in the speech recognition task (?), or between visual features of a picture and its text description in the image caption generation task (Xu et al.", "startOffset": 73, "endOffset": 92}, {"referenceID": 13, "context": ", 2014), between speech frames and text in the speech recognition task (?), or between visual features of a picture and its text description in the image caption generation task (Xu et al., 2015).", "startOffset": 178, "endOffset": 195}, {"referenceID": 0, "context": "In the context of NMT, Bahdanau et al. (2015) has successfully applied such attentional mechanism to jointly translate and align words.", "startOffset": 23, "endOffset": 46}, {"referenceID": 0, "context": "The former approach resembles the model of (Bahdanau et al., 2015) but is simpler architecturally.", "startOffset": 43, "endOffset": 66}, {"referenceID": 13, "context": "The latter can be viewed as an interesting blend between the hard and soft attention models proposed in (Xu et al., 2015): it is computationally less expensive than the global model or the soft attention; at the same time, unlike the hard attention, the local attention is differentiable almost everywhere, making it easier to implement and train.", "startOffset": 104, "endOffset": 121}, {"referenceID": 12, "context": "recurrent neural network (RNN) architecture, which most of the recent NMT work such as (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common.", "startOffset": 87, "endOffset": 223}, {"referenceID": 2, "context": "recurrent neural network (RNN) architecture, which most of the recent NMT work such as (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common.", "startOffset": 87, "endOffset": 223}, {"referenceID": 0, "context": "recurrent neural network (RNN) architecture, which most of the recent NMT work such as (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common.", "startOffset": 87, "endOffset": 223}, {"referenceID": 9, "context": "recurrent neural network (RNN) architecture, which most of the recent NMT work such as (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common.", "startOffset": 87, "endOffset": 223}, {"referenceID": 5, "context": "recurrent neural network (RNN) architecture, which most of the recent NMT work such as (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common.", "startOffset": 87, "endOffset": 223}, {"referenceID": 2, "context": "There is a recent work by Gregor et al. (2015), which is very similar to our local attention and applied to the image generation task.", "startOffset": 26, "endOffset": 47}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common. They, however, differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation s. Kalchbrenner and Blunsom (2013) used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation.", "startOffset": 8, "endOffset": 268}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common. They, however, differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation s. Kalchbrenner and Blunsom (2013) used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation. On the other hand, both Sutskever et al. (2014) and Luong et al.", "startOffset": 8, "endOffset": 458}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common. They, however, differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation s. Kalchbrenner and Blunsom (2013) used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation. On the other hand, both Sutskever et al. (2014) and Luong et al. (2015) stacked multiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit for both the encoder and the decoder.", "startOffset": 8, "endOffset": 482}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common. They, however, differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation s. Kalchbrenner and Blunsom (2013) used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation. On the other hand, both Sutskever et al. (2014) and Luong et al. (2015) stacked multiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit for both the encoder and the decoder. Cho et al. (2014), Bahdanau et al.", "startOffset": 8, "endOffset": 621}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common. They, however, differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation s. Kalchbrenner and Blunsom (2013) used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation. On the other hand, both Sutskever et al. (2014) and Luong et al. (2015) stacked multiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit for both the encoder and the decoder. Cho et al. (2014), Bahdanau et al. (2015), and Jean et al.", "startOffset": 8, "endOffset": 645}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common. They, however, differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation s. Kalchbrenner and Blunsom (2013) used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation. On the other hand, both Sutskever et al. (2014) and Luong et al. (2015) stacked multiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit for both the encoder and the decoder. Cho et al. (2014), Bahdanau et al. (2015), and Jean et al. (2015) all adopted a different version of the RNN with an LSTM-inspired hidden unit, the gated recurrent unit (GRU), for both components.", "startOffset": 8, "endOffset": 669}, {"referenceID": 12, "context": "In (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015), the source representation s is only used once to initialize the decoder hidden state.", "startOffset": 3, "endOffset": 97}, {"referenceID": 2, "context": "In (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015), the source representation s is only used once to initialize the decoder hidden state.", "startOffset": 3, "endOffset": 97}, {"referenceID": 9, "context": "In (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015), the source representation s is only used once to initialize the decoder hidden state.", "startOffset": 3, "endOffset": 97}, {"referenceID": 0, "context": "On the other hand, in (Bahdanau et al., 2015; Jean et al., 2015) and this work, s, in fact, implies a set of source hidden states which are consulted throughout the entire course of the translation process.", "startOffset": 22, "endOffset": 64}, {"referenceID": 5, "context": "On the other hand, in (Bahdanau et al., 2015; Jean et al., 2015) and this work, s, in fact, implies a set of source hidden states which are consulted throughout the entire course of the translation process.", "startOffset": 22, "endOffset": 64}, {"referenceID": 12, "context": "In this work, following (Sutskever et al., 2014; Luong et al., 2015), we use the stacking LSTM architecture for our NMT systems, as illustrated", "startOffset": 24, "endOffset": 68}, {"referenceID": 9, "context": "In this work, following (Sutskever et al., 2014; Luong et al., 2015), we use the stacking LSTM architecture for our NMT systems, as illustrated", "startOffset": 24, "endOffset": 68}, {"referenceID": 0, "context": "One can provide g with other inputs such as the currently predicted word yj as in (Bahdanau et al., 2015).", "startOffset": 82, "endOffset": 105}, {"referenceID": 14, "context": "We use the LSTM unit defined in (Zaremba et al., 2015).", "startOffset": 32, "endOffset": 54}, {"referenceID": 0, "context": "Comparison to (Bahdanau et al., 2015) \u2013 While our global attention approach is similar in spirit to the model proposed by Bahdanau et al.", "startOffset": 14, "endOffset": 37}, {"referenceID": 0, "context": "Comparison to (Bahdanau et al., 2015) \u2013 While our global attention approach is similar in spirit to the model proposed by Bahdanau et al. (2015), there are several key differences which reflect how we have both simplified and generalized from the original model.", "startOffset": 15, "endOffset": 145}, {"referenceID": 0, "context": "Comparison to (Bahdanau et al., 2015) \u2013 While our global attention approach is similar in spirit to the model proposed by Bahdanau et al. (2015), there are several key differences which reflect how we have both simplified and generalized from the original model. First, we simply use hidden states at the top LSTM layers in both the encoder and decoder as illustrated in Figure 2. Bahdanau et al. (2015), on the other hand, use the concatenation of the forward and backward source hidden states in the bi-directional encoder", "startOffset": 15, "endOffset": 404}, {"referenceID": 0, "context": "On the other hand, at any time t, Bahdanau et al. (2015) build from the previous hidden state ht\u22121 \u2192 at \u2192 ct \u2192 ht, which, in turn, goes through a deep-output and a maxout layer before making predictions.", "startOffset": 34, "endOffset": 57}, {"referenceID": 0, "context": "On the other hand, at any time t, Bahdanau et al. (2015) build from the previous hidden state ht\u22121 \u2192 at \u2192 ct \u2192 ht, which, in turn, goes through a deep-output and a maxout layer before making predictions.7 Lastly, Bahdanau et al. (2015) only experimented with one alignment function, the concat product; whereas we show later that the other alternatives are better.", "startOffset": 34, "endOffset": 236}, {"referenceID": 13, "context": "This model takes inspiration from the tradeoff between the soft and hard attentional models proposed by Xu et al. (2015) to tackle the image caption generation task.", "startOffset": 104, "endOffset": 121}, {"referenceID": 4, "context": "Comparison to (Gregor et al., 2015) \u2013 have proposed a selective attention mechanism, very similar to our local attention, for the image generation task.", "startOffset": 14, "endOffset": 35}, {"referenceID": 0, "context": "Comparison to other work \u2013 Bahdanau et al. (2015) use context vectors, similar to our ct, in building subsequent hidden states, which can also achieve the \u201ccoverage\u201d effect.", "startOffset": 27, "endOffset": 50}, {"referenceID": 11, "context": "Translation performances are reported in case-sensitive BLEU (Papineni et al., 2002) on newstest2014 (2737 sentences) and newstest2015 (2169 sentences).", "startOffset": 61, "endOffset": 84}, {"referenceID": 9, "context": "Following (Luong et al., 2015), we report translation quality using two types of BLEU: (a) tokenized12 BLEU to be comparable with existing NMT work and (b) NIST13 BLEU to be comparable with WMT results.", "startOffset": 10, "endOffset": 30}, {"referenceID": 5, "context": "Similar to (Jean et al., 2015), we limit our vocabularies to be the top 50K most frequent words for both languages.", "startOffset": 11, "endOffset": 30}, {"referenceID": 0, "context": "When training our NMT systems, following (Bahdanau et al., 2015; Jean et al., 2015), we filter out sentence pairs whose lengths exceed 50 words and shuffle mini-batches as we proceed.", "startOffset": 41, "endOffset": 83}, {"referenceID": 5, "context": "When training our NMT systems, following (Bahdanau et al., 2015; Jean et al., 2015), we filter out sentence pairs whose lengths exceed 50 words and shuffle mini-batches as we proceed.", "startOffset": 41, "endOffset": 83}, {"referenceID": 12, "context": "We follow (Sutskever et al., 2014; Luong et al., 2015) in training NMT with similar settings: (a) our parameters are uniformly initialized in [\u22120.", "startOffset": 10, "endOffset": 54}, {"referenceID": 9, "context": "We follow (Sutskever et al., 2014; Luong et al., 2015) in training NMT with similar settings: (a) our parameters are uniformly initialized in [\u22120.", "startOffset": 10, "endOffset": 54}, {"referenceID": 1, "context": "Winning WMT\u201914 system \u2013 phrase-based + large LM (Buck et al., 2014) 20.", "startOffset": 48, "endOffset": 67}, {"referenceID": 5, "context": "RNNsearch (Jean et al., 2015) 16.", "startOffset": 10, "endOffset": 29}, {"referenceID": 5, "context": "5 RNNsearch + unk replace (Jean et al., 2015) 19.", "startOffset": 26, "endOffset": 45}, {"referenceID": 5, "context": "0 RNNsearch + unk replace + large vocab + ensemble 8 models (Jean et al., 2015) 21.", "startOffset": 60, "endOffset": 79}, {"referenceID": 14, "context": "2 for our LSTMs as suggested by (Zaremba et al., 2015).", "startOffset": 32, "endOffset": 54}, {"referenceID": 1, "context": "These include the winning system in WMT\u201914 (Buck et al., 2014), a phrase-based system whose language models were trained on a huge monolingual text, the Common Crawl corpus.", "startOffset": 43, "endOffset": 62}, {"referenceID": 5, "context": "For end-to-end NMT systems, to the best of our knowledge, (Jean et al., 2015) is the only work experimenting with this language pair and currently the SOTA system.", "startOffset": 58, "endOffset": 77}, {"referenceID": 12, "context": "3 BLEU, as proposed in (Sutskever et al., 2014) and (b) using dropout, +1.", "startOffset": 23, "endOffset": 47}, {"referenceID": 9, "context": "It is interesting to observe the trend previously reported in (Luong et al., 2015) that perplexity strongly correlates with translation quality.", "startOffset": 62, "endOffset": 82}, {"referenceID": 0, "context": "8 BLEU, making our model slightly better than the base attentional system of Bahdanau et al. (2015) (row RNNSearch).", "startOffset": 77, "endOffset": 100}, {"referenceID": 9, "context": "The unknown replacement technique proposed in (Luong et al., 2015; Jean et al., 2015) yields another nice gain of +1.", "startOffset": 46, "endOffset": 85}, {"referenceID": 5, "context": "The unknown replacement technique proposed in (Luong et al., 2015; Jean et al., 2015) yields another nice gain of +1.", "startOffset": 46, "endOffset": 85}, {"referenceID": 5, "context": "best system (Jean et al., 2015) by +1.", "startOffset": 12, "endOffset": 31}, {"referenceID": 0, "context": "We follow (Bahdanau et al., 2015) to group sentences of similar lengths together and compute a BLEU score per group.", "startOffset": 10, "endOffset": 33}, {"referenceID": 0, "context": "While (Bahdanau et al., 2015) visualized", "startOffset": 6, "endOffset": 29}, {"referenceID": 8, "context": "Nevertheless, as shown in Table 6, we were able to achieve AER scores comparable to the one-to-many alignments obtained by the Berkeley aligner (Liang et al., 2006).", "startOffset": 144, "endOffset": 164}], "year": 2015, "abstractText": "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT\u201915 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.1", "creator": "LaTeX with hyperref package"}}}