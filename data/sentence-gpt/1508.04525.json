{"id": "1508.04525", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Aug-2015", "title": "Recognizing Extended Spatiotemporal Expressions by Actively Trained Average Perceptron Ensembles", "abstract": "Precise geocoding and time normalization for text requires that location and time phrases be identified. Many state-of-the-art geoparsers and temporal parsers suffer from low recall. Categories commonly missed by parsers are: nouns used in a non- spatiotemporal sense, adjectival and adverbial phrases, prepositional phrases, and numerical phrases, respectively. A variety of languages can be used to indicate the relative magnitude of semantic error and to avoid confusion when defining an object's semantic errors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 19 Aug 2015 04:17:47 GMT  (5649kb)", "http://arxiv.org/abs/1508.04525v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["wei zhang", "yang yu", "osho gupta", "judith gelernter"], "accepted": false, "id": "1508.04525"}, "pdf": {"name": "1508.04525.pdf", "metadata": {"source": "CRF", "title": "Recognizing Extended Spatiotemporal Expressions by Actively Trained Average Perceptron Ensembles", "authors": ["Wei Zhang", "Yang Yu", "Judith Gelernter", "Osho Gupta"], "emails": ["wynnzh@gmail.com", "yu@us.ibm.com", "gelern@cs.cmu.edu", "osho.gupta.ece11@iitbhu.ac.in"], "sections": [{"heading": null, "text": "Our contributions include (1) an new dataset annotated with named entities and expanded spatiotemporal expressions; (2) a comparison of inference algorithms for ensemble models showing the superior accuracy of Belief Propagation over Viterbi Decoding; (3) a new example re-weighting method for active ensemble learning that \u201cmemorizes\u201d the latest examples trained; (4) a spatiotemporal parser that jointly recognizes expanded spatiotemporal expressions as well as named entities.1\nCategories and Subject Descriptors D.3.3 [Artificial Intelligence]: Natural Language Processing \u2013 language parsing and understanding, text analysis\nGeneral Terms Algorithms\nKeywords temporal parsing, geoparsing, named entity recognition, perceptron algorithm, ensemble learning, active learning, queryby-bagging\n1 Source at https://github.com/weizh/LearnSpatioTemporal and https://github.com/weizh/PerceptronFHMM"}, {"heading": "1. INTRODUCTION", "text": "We consider the problem of recognizing unnamed location and time phrases to ease downstream applications such as geocoding [45] and time normalization [2]. A wider range of expressions parsed could benefit question and answering [11][9][31], web querying toward the semantic web [43], information retrieval as from news messages [32].\nData. The types of location and time expressions that parsers miss are the types not included in standard annotated data sets. So we were forced to create our own labelled data because given location and time tags in most corpora are named entities. To create our corpus, we sampled sentences from the web that include seed phrases of the sort that preliminary experiments had shown are not found by geoparsers and temporal parsers. False positives were also collected by search engine. We used active learning to find sentences that would provide the most information gain for training. These 1701 unlabeled training, and 450 manually labeled testing sentences make up our Expanded Spatiotemporal Data (EST).\nModel. Labeling words in sentences automatically is done sequence labeling models such as Hidden Markov Models (HMM) [28] [46] and Conditional Random Fields (CRF) [16], Average Perceptron [3], Max-Margin Markov Networks [39], SVMstruct [42], SEARN algorithm [5], Max Entropy Markov Models [20] and so on. We are interested in Average Perceptron due to its high accuracy and ease of overfitting control by averaging model parameters. We used an average perceptron algorithm for training a Featurized HMM (an extension of HMM with features, see Section 3). To further reduce overfitting and prediction variance, we used FHMM ensembles, specifically, bagging [6] of FHMM, with the correct result being found by voting among results of the individual models. We also experimented with two different ensemble inference methods called Best Viterbi Sequence and Best BP Sequence which are based on Viterbi Decoding and Belief Propagation (BP) respectively.\nEvaluation. We use commonly used named entity recognition (NER) data sets and a chunking data set to demonstrate the viability of FHMM model in comparison to other methods before running our own EST data.\nWe used five sub-datasets from the OntoNotes2 news data that others had published NER results, the CoNLL 2000 chunking dataset3, and the NLPBA biomedical NER dataset4. Our single FHMM model performed competitively with others on NER tasks and chunking task on those data sets. Thus we could confirm that FHMM is a reliable building block for an ensemble model. We found also that the Belief Propagation works in concert with the ensemble FHMM better than does the Viterbi, and that our active sampling method achieved higher results overall than would a random sample of the data have achieved.\nContributions (1) A general English Expanded Spatiotemporal Dataset, the EST corpus is created. We used seeds to find sentences on the web, and used active learning to find and tag the examples. (2) We adapted single model inference algorithms to ensembles, and found out that Best Belief Propagation Sequence (Belief Propagation variant) won over Best Viterbi Decoding Sequence (Viterbi Decoding variant) for ensembles to a large margin, especially for ensembles. This conclusion also guides the selection of inference algorithm for other sequence labeling models as well. (3) A new training example re-weighting method is proposed to reduce the variance of the ensemble created in active learning process, to eventually speed up the active learning process. (4) A spatiotemporal parser that jointly recognizes expanded spatiotemporal expressions as well as named entities.5\nIn section 2, we survey the related work, section 3 we elaborate on our data collection method. Section 4 introduces the average perceptron trained FHMM model, followed by the FHMM ensemble in Section 5. Active Learning method for FHMM ensemble is explained in Section 6. At last, we show experiment results on FHMM, FHMM ensembles, and Active Learning of FHMM ensembles in Section 7."}, {"heading": "2. RELATED WORK", "text": "No single model for location and time yet. We join the parsing of time and locations in one model (one FHMM ensemble), because we observed linguistic parallels between spatial and temporal expressions, as is shown in Table 2. We believe that so doing could help distinguish between meanings of words that could be used just as frequently in a temporal as in a location sense.\nWhat does a parser typically find, and why? Geoparsers find nouns \u2013 specifically, location words (or toponyms). Temporal parsers find months, dates and holidays. Phrases (that includes valuable modifiers to time and locations) are not always included if ever. The PETAR system ([18] that works on tweets is among the first to change this.\nState-of-the-art spatiotemporal parser accuracy To get a general idea of the precision and recall of existing temporal parsers, we wrote 79 sentences6 (we use the words \u201csentences\u201d, \u201cinstances\u201d and \u201cexamples\u201d interchangeably throughout this paper) using specific and non-specific temporal words and phrases that covers a variety of cases, which is a decent benchmark for evaluating 2 https://catalog.ldc.upenn.edu/LDC2011T03 3 http://www.cnts.ua.ac.be/conll2000/chunking/ 4 http://www.nactem.ac.uk/tsujii/GENIA/ERtask/report.html 5 Code at https://github.com/weizh/LearnSpatioTemporal and https://github.com/weizh/PerceptronFHMM 6 Data available at https://github.com/weizh/PerceptronFHMM\nspatiotemporal parsers. We ran the sentences through temporal taggers in Table 1. For geoparsing (method to find location entities) results, see [45].\nNo corpora tagged with spatiotemporal phrases. Geocoding research that incorporates natural language understanding of adjectival or prepositional phrases has been termed \u201cspatial relations\u201d [44]. Tags for such expressions were defined only recently.7\nEnsemble methods Many temporal parsers are based on heuristic methods, and the geoparsers are based on classifiers with knowledge bases (a world gazetteer). Ensemble methods are generally reserved for problems of higher complexity. [22] and [23] studied ensemble classifiers for continuous values, [33] for gesture recognition, [25] for biomedical data, and [13] on object recognition for computer vision. [19] designed ensembles that are heterogeneous rather than homogeneous.\nActive Learning for Ensembles. Active learning for ensembles is little studied beyond [22] and [23]. However, active learning for sequence labeling models has been investigated extensively [4] [36] [21]."}, {"heading": "3. Web DATA for spatiotemporal corpus (EST)", "text": "Necessity of creating EST Dataset. Among all the data sets that we investigated (CoNLL 20038, OntoNotes, NLPBA, etc.), the occurrence of phrase types in Table 2 are rare, which will undermine our method due to the training data insufficiency. Thus, we made Expanded Spatiotemporal Dataset (EST Dataset) to include examples of spatiotemporal expressions that FHMM ensembles learn to recognize. We used the Microsoft Search API to automatically collect sentences from the web using seed expressions (Figure 1) of the sort missed by state-of-the-art temporal parsers and geoparsers.\n7 http://timexportal.wikidot.com/corpora-tides 8 http://www.cnts.ua.ac.be/conll2003/ner/\nHeideltime 0.827 0.409 0.547 Illinois extractor 0.866 0.396 0.544\nGUTime 0.786 0.36 0.494 SUTime [2] 0.874 0.677 0.763\nStemptag (IntelliGIS) 0.863 0.616 0.719 Gmail (Google) 0.818 0.109 0.194\nMac OS X (Apple) 0.833 0.122 0.213 TIPSem (Univ Alicante) 1 0.25 0.4\nNumerical Quarter to three Quarter mile\nThree hours Three stops on bus 25 minutes 25 light years\nAdj/adv Quite a long time Quite close Every Tuesday Every kilometer Far from the end Far away Prepositions Near dinner time Near the exit At last At the door Over an hour Over the hill\nData collection. Figure 1 shows some seed words used to search the Microsoft search API to make the training dataset. Keyword search results might be biased, but the advantages are that this is an efficient way to collect relevant data. For each returned list of results, we selected the top N snippets. We observed the results, and found that the top results were highly biased toward named entities (because of search engine optimization). For instance, \u201cThe Day After Tomorrow\u201d is a movie name instead of a temporal phrase. To keep the dataset from leaning toward such edge cases, we used N=50, so that more sentences with the general sense for time or location words could be included. We uniformly sampled 2151 sentences from all of those we collected. For evaluation purposes, we randomly split the dataset into 80% training and held 20% for testing. We manually tagged the 450 test sentences according to the tag set in Table 3 and we left the remaining 1701 training sentences untagged for active learning.\nAnnotations. We created the tag set shown in Table 3. In addition to traditional location entity types such as L, we also added the annotation B and ST, for \u201cBuilding\u201d, \u201cStreet\u201d for fine-grained named entities, UL, US, UB for unnamed locations to distinguish them from named ones. We tagged all the words in a time phrase as T that are not named entities, and all location words as G that are not location named entities. For example, given the letter-tags in Table 3, \u201cwest of Boston\u201d is tagged as (G G L), and \u201c3 days before National Day\u201d as (T T T E E). Notice that what is annotated GG is actually a geo-word plus a preposition. The annotation of the preposition as T is exactly what we need in the training data, so that \u201cof\u201d is included as part of the named entity to be used to connect \u201cwest\u201d with \u201cBoston\u201d to more precisely target the location or range."}, {"heading": "4. THE AVERAGE PERCEPTRON TRAINING OF FHMM", "text": "We introduce the Featurized Hidden Markov Models (FHMM) (see Figure 2), the building block of our FHMM ensemble. The model is trained with the average perceptron algorithm [3], which resembles a single linear perceptron in neural networks.\nSince we apply perceptron training, the model turned from Bayesian model into a discriminative model and model bias is reduced. Thus, there is no arrow for the edges in Figure 2. The gray nodes labeled \u2018word feature j\u2019 define the j-th word level feature function for each word in example S, such as the shape, part-of-speech or suffix that does not depend on the context. The blue nodes labeled \u2018global feature k\u2019 under the states layer represent the k-th global feature function defined on context of a word. In the middle \u2018states\u2019 layer, each state (denoted by s) represents a label in Table 3, and the line between states is the second order transition feature. The features are all attached to weights, denoted as \u03b1!, where k indexes feature functions. The probability of a training example is proportional to the linear combination of its feature values. See [3] for details.\nThe perceptron algorithm updates feature weights \ud835\udefc! iteratively by observing the errors made by the current parameter setting and adjusts the parameter according to errors (See Algorithm 1). This idea is similar to the back-propagation algorithm used to train neural networks. Averaging parameters in the perceptron can help reduce FHMM overfitting.\nFeatures. Table 4 shows how we define features used for the FHMM according to different datasets. We need to use different\nAlgorithm 1. The Perceptron training algorithm Initialize: Constant K = total number of features Constant F = total number of feature values All weights \ud835\udc4e! \u2208 \ud835\udc45!\u00d7! with zero, sum of weights \ud835\udc34! with zero. 1 for each iteration \ud835\udc61 \u2208 [1,\ud835\udc47], and training example \ud835\udc52!, \ud835\udc56 \u2208 [1,\ud835\udc41]: 2 \ud835\udc67(\ud835\udc52!) = \ud835\udc39\ud835\udc3b\ud835\udc40\ud835\udc40. \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc61(\ud835\udc52!) 3 for each word position p: 4 for each feature k: 5 \ud835\udefc! !\ud835\udc67!\ud835\udc52!\"!! = \ud835\udefc! !\ud835\udc67!\ud835\udc52!\"!! \u2212 1;\n5 \ud835\udefc! !\ud835\udc66!\ud835\udc52!\"!! = \ud835\udefc! !\ud835\udc66!\ud835\udc52!\"!!+ 1; 7 store a snapshot of \ud835\udefc into \ud835\udc34! 8 output A!/(T\u00d7N)\nacross from, at once, at the end, at the same time, between, classic, close to, contemporary, early, eastward, every mile, ever since, every time, fall, far away, far from, far longer, first week in, formerly, fortnight, frequently, half of an hour, homeward, in the back, into the distance, last kilometer, last moment, long distance, long time, miles before (the station, intersection), minutes before (closing, sunset, the hour, the match), modern quick, quite close, quite late, rarely, right then, right there, second Sabbath, second weekend in, several blocks (ahead, away), since the day, straight away, third weekend in, toward the, very close, very late, very near, walking distance, without delay\nFigure 1. Sample of spatiotemporal keywords used to search Microsoft Search API to collect sentences\nfeature for different sets because each dataset contains different annotations. For example, the NLPBA corpus does not use the Part-of-Speech (POS) tags, so the part of speech feature is not included. Feature engineering is important because perceptron is a single neuron that is linear with respect to its input. Feature engineering could help the model to be \u201cnon-linear\u201d to better fit the NER problem. In Table 4, the features defined in context window [-2,2] are the \u201cglobal features\u201d that increase non-linearity.\nOverfitting. Many machine learning algorithms have a high risk of overfitting, especially discriminative models that are nonBayesian. Several general techniques can be used to reduce overfitting in optimization, such as early stopping [27], L2 regularization [7], Lasso regression [40], cross validation [14],\nand feature selection. But in a perceptron algorithm, parameter averaging is a simple but effective technique used to control overfitting [3]. Additionally, we apply the ensemble learning technique to reduce the variance of the model created, so that overfitting can be further controlled. See section 5 for details.\nInference. We compared two inference (decoding) algorithms: Nbest Viterbi decoding (or Viterbi decoding for short) [10], and the Belief Propagation (BP) algorithm, or the sum-product message passing algorithm [26], [30]. Viterbi decoding can be used when exact inference is feasible, and BP could also be used for exact inference, but can also give approximate results when the joint probability is impossible to compute. The BP algorithm is widely used in graphical models for exact inference in tree graphs [15] and for approximate inference over loopy graphs [24]. Recent work tries to combine the two to achieve the power of both [17]. In our work, we compare Viterbi decoding and BP algorithm in the setting of sequence labeling ensembles. Viterbi decoding generates the probability for each decoding sequence, whereas BP uses forward-backward algorithm to predict each token individually [28]. We noticed that for single FHMM model, the inference accuracy of BP is generally worse than Viterbi decoding, especially when the dataset is large. However, it is different for ensemble models. We will come back to this point in a moment."}, {"heading": "5. BAG OF AVERAGE PERCEPTRONS \u2013 ENSEMBLE FHMM", "text": "Ensemble learning is a meta-learning technique to reduce variance of the training model [1]. The bagging algorithm [1] as a type of ensemble learning algorithm, creates a bag of models (same learning functions) using a subset of training data that are sampled with replacements from the full data. The number of FHMM created is heuristically selected. When the training data size is small, especially during the first several iterations in an active learning procedure, a FHMM model can have high variance on its output (common to other training algorithms as well) due to overfitting, which in turn can lead to imprecise prediction of the next informative sentence to include. To solve this problem, we experiment with candidate inference algorithms so that better predictions can be made and based on the utility function to choose the next instance, in hopes of optimizing selected instances faster.\nDue to the change from using single FHMM to an ensemble of FHMMs, strategies are needed to combine the results from FHMMs. We designed two inference algorithms for the ensemble based on the Viterbi Decoding and BP algorithm respectively. The Best Viterbi Sequence (BVS) strategy is based on Viterbi decoding results from each FHMM member. Formally, assume M = {M!,\u2026 ,M!} is an ensemble of k FHMMs. The total collection of unique output sequences from the ensemble is denoted as S = {S!,\u2026 , S!}, coming from all n-best sequences of all FHMM models. So the best \ud835\udc46! given the ensemble M is described by\n\ud835\udc46!\"#$ = \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc65!\ud835\udc43! \ud835\udc46! = \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc65! \ud835\udc43!!(\ud835\udc46!) !\nWhere P!!(S!) is the probability for \ud835\udc40! to predict sequence \ud835\udc46! as output. This probability is the aggregated probability from all the models in ensemble for \ud835\udc46!. And, the sequence that has the best aggregated probability is the best output \ud835\udc46!\"#$ for ensemble.\nAlgorithm 2. General Query-by-bagging algorithms for sequence labeling ensembles T : set of training examples, initialized with I examples U: set of unlabeled examples Ensemble: the Ensemble learning algorithm Base: The base sequential labeling algorithm Utility: The utility function defined for unlabeled training example 1 repeat t times: 2 Generate Ensemble = learn_Ensemble( Base, T) 3 for each example e in U: \ud835\udc62\ud835\udc61\ud835\udc56\ud835\udc59!= Utility(Ensemble, e) 4 Sort U by \ud835\udc62\ud835\udc61\ud835\udc56\ud835\udc59!, select top K examples A in U 5 Label examples in A, move them from T to U. 6 Return Ensemble\nTable 4: Different feature configurations for different tasks. [-2,2] in first column means the feature is extracted within token window of 5 for each word.\nFeature Chunk NLPBA NER Onto Notes NER EST NER+ unnamed\nCurrent word y y y y Current word lemma y y y y\nCurrent word part of speech y y Current word suffix y y y y\nCurrent word form+suffix y y y y Current word prefix + suffix y y\nCurrent word POS + suffix y y Lemma in window [-2,2] y y y y\nWord form [-2,2] y y Part-of-speech [-2,2] y\nWord suffix [-2,2] y y y Named entity tag [-2,2] y\nPart-of-speech bigram [-2,2] y Suffix bigram [-2,2] y\nWord form bigram [-2,2] y y y\nAlgorithm 3. Selection by sentence re-weighting\nInput: Training data \ud835\udc47! and corresponding weight vector \ud835\udc4a! of iteration t \ud835\udefc: Small value used for sample rate decay I: The new example to add to training data \ud835\udc47! 1 for each example-weight pair(\ud835\udc47!\",\ud835\udc4a!\") in \u00a0(\ud835\udc47!,\ud835\udc4a!), do 2 \ud835\udc64!\" = max(1 \u2212 \ud835\udefc(\ud835\udc61 \u2212 1), \ud835\udc5f) 3 \ud835\udc4a!!! =\ud835\udc4a!\u2a01 1 ,\ud835\udc47!!! = \ud835\udc47! \u222a \ud835\udc3c 4 return(\ud835\udc47!!!,\ud835\udc4a!!!)\nBy contrast, the Best BP Sequence (BPS) that corresponds to the BP algorithm for a single model is the aggregation of individual BP predictions. Formally, we define \ud835\udc64!\"  \u00a0to be the word j in example \ud835\udc52!. The l-th label for \ud835\udc64!\" in label sequence \ud835\udc60!\" is \u00a0\ud835\udc60!\"! . The best sequence is\n\ud835\udc60!\"!\"#$ = \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc65!  \u00a0\ud835\udc43! \ud835\udc60!\"!  \u00a0 \u00a0 \ud835\udc52!) = \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc65!  \u00a0 \ud835\udc43!! \ud835\udc60!\" !  \u00a0 \ud835\udc52!)\n!\n \u00a0 \u00a0\nWhere each \ud835\udc43!! \ud835\udc60!\" !  \u00a0 \ud835\udc52!) \u00a0is generated by the BP algorithm from model \ud835\udc40!. It shows that the BP decoding result for token j in example i is the aggregated probability from the output of all the models. The best label for token j is the one that gives the best aggregated score. The best decoding sequence is just the concatenation of labels from each individual token. Compared to the Best Viterbi Sequence, where the relation among predictions is preserved, the Best BP Sequence does not force the coupling of predictions between tokens. Each individual prediction can change freely given the individual output. We observed from our experiment that when a single model is used and the data is abundant, the Best Viterbi Sequence generates better result. However, when ensembles of models are used in the first several iterations, the Best BP Sequence performs better."}, {"heading": "6. ACTIVE LEARNING FOR FHMM ENSEMBLES", "text": "Active learning for classifier ensembles has been studied on classification problems [22] and [23]. Active learning for sequence labeling models was extensively investigated in [4] [36] [21]. We are the first to use active learning technique to train an ensemble of sequence labeling models. Active learning techniques is used to reduce labeling cost by choosing only the most effective training instances, to achieve a classifier comparable to a model trained with the full dataset. Labeling training examples can be costly.\nThe FHMM ensemble during active learning produces output with lower variance than the single models that comprise it. Biasvariance tradeoff haunts supervised learning algorithms. A model\u2019s expected generalization error depends on the sum of bias and variance. Bias is intrinsic to the learning algorithm, but variance may come from several sources. In our case, the bias is fixed since we do not change the FHMM algorithm, but the variance may be from data insufficiency or overfitting. Averaging model parameters in the perceptron alleviates overfitting. But data insufficiency still contributes to variance, which is the situation for active learning in the early stages. Creating ensembles is an effective means for reducing variance. But reducing member variance is not equal to reducing ensemble variance. Ensemble variance is what we care about ultimately, since it directly relates to the generalization of the ensemble on unseen data. The actively selected examples might be used to reduce single model variance, which might not significantly help reducing ensemble variance. Thus, iterations might be wasted, and active learning slows. We employed a new example sampling method that forces the five FHMM members to be more \u201csimilar\u201d to each other in early iterations, to reduce the selection of edge case examples that could cause variance of the ensemble. The technique is named \u201csentence re-weighting\u201d, introduced below.\nWe introduce the general architecture of a typical active learning routine for ensembles [22] and [23] before we discuss details of active learning. The general framework has been adapted to sequence labeling problems (Algorithm 2).\nIn this work, we configured algorithm 2 by using (1) a sequence labeling utility function Sequence Vote Entropy [36] for Utility(), and (2) a new Sentence Re-weighting method for creating ensembles from T, which corresponds to learn_Ensemble (Base, T) shown in Algorithm 3.\nUtility Function. The purpose of a utility function is to evaluate how much the classifier would gain from the unlabeled training example. To do this, we applied the utility function Sequence Vote Entropy (SVE). We are interested in this method because it evaluates the probability for the whole sequence instead of evaluating each token individually. We did not compare exhaustively all potential utility functions since this is not the purpose of the paper. We used sentence random sampling to compare against it. SVE utility is calculated as\n\ud835\udf19!\"# = \u2212 \ud835\udc43 \ud835\udc66 \ud835\udc65;\ud835\udc40 log\ud835\udc43 \ud835\udc66 \ud835\udc65;\ud835\udc40) !\u2208!!  \u00a0 \u00a0 \u00a0\nWhere \ud835\udc41! is the collection of all N-best predictions from models in the ensemble. Here, x is the input sentence; \ud835\udc66 represents a prediction from \ud835\udc41! , which is the collection of all n-best predictions from models in the ensemble \ud835\udc40 . Each sequence prediction probability is calculated as the average of all the prediction probabilities for each model m by \ud835\udc43 \ud835\udc66 \ud835\udc65;\ud835\udc40 = ! !\n\ud835\udc43(\ud835\udc66 \u00a0|\ud835\udc65;\ud835\udc5a)!!!! , and each probability within the sum is the sequence probability calculated by the Viterbi Decoding algorithm. Also, the probability has been normalized into [0,1] interval being divided by the sum of probabilities.\nSentence Re-weighting. Once an example \ud835\udc86 is chosen by the utility function, it will be added to \ud835\udc7b, the collection of annotated training data. It will be sampled with equal probability as a normal sample in \ud835\udc7b , as in the general ensemble creation algorithm. Imagine the case where \ud835\udc86 is only involved in a small portion of FHMM members in ensemble. For a sentence that is extremely similar to e, say, \ud835\udc1e! in the unlabeled example pool, it might not need to be trained again since e is included in T. But, because e was only included in some of the individual FHMM model members, some of the models will not be able to predict the similar instance \ud835\udc1e\u2032 correctly, just because e is not included in those models. So some members of the ensemble will disagree on predicting \ud835\udc1e\u2032, which will lead to a high utility value of \ud835\udc1e\u2032 that makes it likely to be selected again in the active learning process. Since \ud835\udc1e\u2032 is highly similar to e, the annotation and training of \ud835\udc1e\u2032 is redundant and slows active learning. In order to reduce the selection of redundant examples like \ud835\udc86\u2032, we created a strategy to re-weight the examples in the labeled training data that gives newly labeled examples a higher probability of being sampled in the very next iteration. And then the chance of newly-labeled samples being selected will decrease over iterations. This is analogous to short-term memory mechanism used in Long-Short Term Memory (LSTM) neural networks [12].\nIn algorithm 2, step 2, learn_Ensemble() randomly samples with equal probability each example in T with expectation r, called the sample rate. \ud835\udd3c(|\ud835\udc47!|) = \ud835\udc5f\ud835\udc47 is the expected number of examples fed to model m. We fix the rate r, and in iteration t, we choose the top example i (with highest utility), and set the probability to 1 for the example in next (t+1)-th iteration. This means the newly added example will always be included to build the ensemble in iteration t+1, for every FHMM member. Then in (t+2)-th, the probability for this instance will be max \u00a0(\ud835\udc5f,\ud835\udc64!\"), where \ud835\udc64!\" is the weight of example i at iteration t. The weight decay for example i will stop until \ud835\udc64!\" reduces to r. See Algorithm 3. For any\nexample-weight pair in training data T of time t, we first modify the weight \ud835\udc4a!\" for the example  \u00a0\ud835\udc47!\" by deducting decay factor \ud835\udefc = 2(1 \u2212 \ud835\udc5f)/(\ud835\udc61 \u2212 1) , which is calculated by equating two expectations\n\ud835\udd3c \ud835\udc47!!! = \ud835\udc5f(\ud835\udc61 + 1)\nWhich is the expected number of examples selected by uniform random sampling, and the new expectation\n\ud835\udd3c! \ud835\udc47! + 1 = \ud835\udc61 \u2212 0.5\ud835\udefc\ud835\udc61 \ud835\udc61 \u2212 1 + \ud835\udc5f\ud835\udc3c \u00a0\nWhich is the expected number of examples selected by sentence reweighting. Equating the two guarantees that the sentence reweighting method only changes the weights of the examples in T instead of the total number of training examples. The \ud835\udefc value used to deduct from the probability decreases through iterations. In other words, the rate of weight change will decrease over time. When the weight decreases to a pre-defined rate r, it will stop decreasing, and the corresponding example \ud835\udc47!\" will no longer have a high sampling probability. In the Experiments section, we will see the effect of this example re-weighting strategy."}, {"heading": "7. EXPERIMENTS", "text": "(1) We compare the FHMM model to others on the basis of similar sequence-labeling data sets\u2014a necessity since ours is the only model that is used to handle EST data (2) we consider the active learning process, and (3) the degree to which the inference algorithm affects ensemble FHMM, and (4) we run our ensemble FHMM model on our data set to check precision and recall separately on time expressions and on location expressions."}, {"heading": "7.1 Corpora to compare models", "text": "We chose the following datasets to compare our Perceptron FHMM with several other models. Due to data annotation and others\u2019 published results, we compared on the basis of NER and the chunking task. We use the CoNLL 2000 Chunking dataset, the NLPBA data set for Biomedical NER, and OntoNotes version 5.0 \u2013 the 5 news subsets ABC, MNB, NBC, PRI and VOA therein. We skipped the CoNLL 2003 NER dataset because at the time of the paper we are not able to acquire the dataset. We would leave for our future work. We can use part of it, but it seems unnecessary. PRI and VOA are two datasets that resemble EST dataset in size. ABC, MNB and NBC are three small NER datasets that simulate early stages in active learning. NLPBA is used for testing biomedical NER and CoNLL chunking is used for testing a NER-like task."}, {"heading": "7.2 Perceptron FHMM vs. previous work", "text": "Our task of finding spatiotemporal phrases is essentially the recognition of non-named entities. However, in order to compare our model to others, we compare on task of Named Entity Recognition, and the F1 metric that is quoted in others\u2019 research. We look also at the chunking task that recognizes phrases because of its similarity to recognizing spatiotemporal phrases. We use the F1 measure from our single FHMM model to be consistent with others\u2019 published results on single models (even though [37] uses an ensemble HMM and voting to achieve 95.23% on the chunking task, which was the highest reported). For training the FHMM, we stops training iterations based on whichever is met first: 100 iterations or the 1E-10 training error rate. The decoding algorithm uses Viterbi algorithm. The features used by FHMM model on each dataset were described in Table 4.\nTable 6 shows results on the CoNLL 2000 chunking task. Table 7 shows the F1 results for the OntoNotes news dataset. Each dataset was split into about 80% of the data for training and about 20% for testing. Train test split of those follows [8] to be comparable. Our score is higher than that of Finkel and Manning\u2019s [8] basic NER model which uses features similar to those that we use. Our score is lower than the joint model that uses parsing information. [29] used Average Perceptron training along with rich semantic features, as do [41].\nTable 8 shows how different models perform the NER task on the NLPBA dataset. The NLPBA 2004 dataset is an English language dataset for biomedical NER, which includes PubMed abstracts along with corresponding named entities (such as DNA and RNA). This data differs from the OntoNotes news articles in that the terms are biomedical terms with a high out-of-vocabulary rate and with complicated word forms. Still, it is the Wang et al [49] ensemble method that outperforms other single-model methods to a large extent. The second best result comes from using rich knowledge as features for word matching. Among the results that do not use extended outside knowledge but explore word forms and context only, FHMM is shown to be competitive with [8] and [35]. Tables 7 and 8 suggest that our FHMM model needs improvement to complete with knowledge rich models or the joint model with parsing information. This suggests that additional information helps.\nFigure 3 shows how the best Viterbi sequence (BVS) compares to the Best BP sequence (BPS) on data sets of different sizes, and how different data sampling methods compare for active learning. Experiments are conducted both on the OntoNotes corpora and on\nTable 8. F1 Model comparison on NLPBA Dataset Model F1\nFHMM 71.42 Ensemble of Sequence Labeling algorithms [48] 77.6 Average Perceptron with knowledge [41] 74.27 CRF by Wang et al. [49] 70.1 CRF by Settles et al. [35] 69.8\nTable 7. F1 Model comparison on OntoNotes NER Datasets Model ABC MNB NBC PRI VOA FHMM Single model-Viterbi 74.87 68.9 68.59 81.5 83.14 CRF by Finkel et al. [8] 74.91 66.49 67.96 86.34 88.18 Average Perceptron by Ratinov [29] 72.74 73.1 65.78 79.63 84.93 CRF by Tkachenko [41] 76.45 71.52 67.41 83.72 87.12\nTable 6. Model Comparison on CoNLL 2000 Chunking Model F1 FHMM Single model- Viterbi 94.48\nConditional Random Fields [34] 94.3 Latent Dynamic Conditional Random Fields [38] 94.34\nour own Expanded Spatiotemporal Dataset (see Table 5). We used an ensemble of 5 FHMMs, with a sample rate r=0.8, and compared results with vt (Viterbi Decoding), bp (Belief Propagation), nrw (no example re-weighting for creating ensembles), rw (example re-weighting), utl (utility function to select unlabeled examples), and rnd (random sampling from unlabeled examples--baseline).\n(1) Viterbi (vt) vs. Belief Propagation (bp): Comparing the curves for vt and bp in Figure 3, we can see that the BP algorithm attains higher overall accuracy on almost all datasets to a large margin, and the differences amplify when the iterations increase. We conducted 250 iterations for each dataset, because\nperformance increases slowly afterwards. The difference between using BVS and BPS for inference seems to have roughly 10% F1 difference on our EST data. The phenomenon can be explained by the discussion in section 4. To review, for BVS, we simply aggregate the sequences from each model output, and the sequence from each model is used for voting. By contrast, BPS votes each token individually, so the method has the ability to modify part of the sequence (that is, part of the sentence) when giving outputs. This is an important observation because when we conduct active learning, the data size is small for the first several iterations, and we need a reliable decoding algorithm that tells us how uncertain is each training instance, so that utility values can\n0 50 100 150 200 250\n0. 2\n0. 3\n0. 4\n0. 5\n0. 6\n0. 7\nABC\n# of training examples\nF1\nvt-nrw-utl vt-nrw-rnd vt-rw-utl vt-rw-rnd bp-nrw-utl bp-nrw-rnd bp-rw-utl bp-rw-rnd\n0 50 100 150 200 250\n0. 2\n0. 3\n0. 4\n0. 5\n0. 6\n0. 7\nMNB\n# of training examples\nF1\nvt-nrw-utl vt-nrw-rnd vt-rw-utl vt-rw-rnd bp-nrw-utl bp-nrw-rnd bp-rw-utl bp-rw-rnd\n0 50 100 150 200 250\n0. 2\n0. 3\n0. 4\n0. 5\n0. 6\n0. 7\nNBC\n# of training examples\nF1\nvt-nrw-utl vt-nrw-rnd vt-rw-utl vt-rw-rnd bp-nrw-utl bp-nrw-rnd bp-rw-utl bp-rw-rnd\n0 50 100 150 200 250\n0. 2\n0. 3\n0. 4\n0. 5\n0. 6\n0. 7\nVOA\n# of training examples\nF1\nvt-nrw-utl vt-nrw-rnd vt-rw-utl vt-rw-rnd bp-nrw-utl bp-nrw-rnd bp-rw-utl bp-rw-rnd\n0 50 100 150 200 250\n0. 2\n0. 3\n0. 4\n0. 5\n0. 6\n0. 7\nPRI\n# of training examples\nF1\nvt-nrw-utl vt-nrw-rnd vt-rw-utl vt-rw-rnd bp-nrw-utl bp-nrw-rnd bp-rw-utl bp-rw-rnd\nFigure 3 Comparison of sampling strategies for ensembles during active learning process. ABC, MNB, NBC, PRI, VOA are 5 subsets of the OntoNotes news corpus. Each dataset is split into roughly 80% train 20% test. x-axis is the number of training examples labeled so far, and the y-axis is the F1 value on each corresponding test dataset. We used an ensemble of 5 FHMMs, sample rate r=0.8.\nbe computed more reliably based on those predictions.\n(2) With Utility Functions (utl) vs. Without (rnd). From the Figure 3, by comparing curves with utl and rnd to the other parameters fixed, we can see that using a utility function did not significantly outperform the random sampling baseline. This result is consistent with the [36] result. This proves that for sequence labeling in active learning, choosing the whole sequence performs a kind of smoothing for the learning curve, because the non-informative tokens in the sequence have to be labeled with the informative ones. Better solution could be selecting part of sentences to label instead of the sentence as a whole.\n(3) With Example Re-weighting (rw) vs. without (nrw). From the comparison between the curves with 'rw' and 'nrw', we can see that the re-weighting method was slightly better than the non\u2013reweighting method especially on our EST dataset (see the two entangled curves at the bottom of Fig. 4). This confirms our hypothesis that the short-term memory provided by the weighing scores does help ensembles remember previous states, and make better choices on the example to learn in the next iteration. Note that in the five OntoNotes datasets, this pattern does not show clearly because of the vibration of \u2018rw\u2019 curves.\nWe mentioned above that we used two decoding algorithms for inference, best Viterbi sequence and best BP sequence. In Table 9, we experimented with two inference algorithms using the OntoNotes dataset, which resembles our EST dataset with respect to data size and number of labels (although the EST has a wider variety of label). The Best BP sequence outperformed the best Viterbi sequence by a small margin, and we observed a similar discrepancy in our experiment with the EST dataset, as Figure 4 shows.\nWe saw inaccuracies in Figure 3 that prevented steady improvement in accuracy as training data size increases, because active learning introduces random errors. Errors come from noise in the training data, and from absorbing entire sentences for training regardless of the utility of the single tokens. We could improve active learning by not labeling tokens that are less informative, and labelling only the informative parts of a sentence."}, {"heading": "7.3 Evaluation of spatiotemporal parsing", "text": "The purpose is to evaluate how well our ensemble model does on our defined G and T tags. In Table 10, we used 1700 training sentences on the 5-model ensemble, at a sample rate of 0.8. During preliminary experimentation, we had created 79 spatiotemporal sentences to determine which types of temporal expressions temporal parsers found. These 79 sentences do not contain named entity tags, which can be extremely helpful to nonentity spatiotemporal expressions, because entities co-occur with non- entity spatiotemporal expressions frequently. So we created an FHMM ensemble model on the EST dataset with all tags except for T and G tags, namely FHMM-G, to train a general purpose NER system on location, dates, organization, person, etc.\nThen, FHMM-G is applied to features relevant for the corpus (as shown in Table 4).\nOur model did remarkably well on the precision (89.27) and recall (88.17) for temporal expressions. Naturally, it did less well on location expressions since there were very few in the 79-sentence corpus.\nOur hypothesis that joint modeling would be effective is based on the large number of linguistic parallels between time and place language (as shown in Table 3), which we were able to disambiguate by constructing a model that parses expressions both for location and also for time. The fact that we achieved high recall and also precision for both location and temporal phrases (see Table 10) demonstrates that our joint spatiotemporal parser is successful."}, {"heading": "7.4 Generalizability and future research", "text": "The algorithm is limited in that it was trained using certain seeds. Spatiotemporal expressions not included in the data set still will not be found, nor will numerical sequences with numbers different than those in the training data. We could widen the expressions recognized by using templates for features. For example, we could find \u201cthe x week of y\u201d, rather than only \u201cthe second week of April\u201d. Even so, a larger quantity of data would need to be annotated manually, by crowd-sourcing on Mechanical Turk, for example, so that the patterns would be tagged.\nAlternative algorithms for training that could be tested include neural networks, which have shown excellent results on sequential labeling tasks, especially the long-term short-term memory network [12]. The method lends itself in particular because the sequences that we would recognize are longer than standard Named Entity Recognition tasks.\nTable 9. Comparison of two decoding algorithms on OntoNotes NER Dataset. An ensemble of 5 FHMMs is used. Each FHMM uses 80% of randomly sampled examples\nModel ABC MNB NBC PRI VOA\nFHMM Ensemble \u2013Viterbi FHMM Ensemble - BP 73.74 70.41 66.7 69.0 65.62 68.09 78.33 81.56 81.67 82.93"}, {"heading": "8. CONCLUSION", "text": "We have identified the spatiotemporal expressions missed by state-of-the-art time and location parsers, and have created a tool that will identify such expressions. The recall of our spatiotemporal parser is high, and does not sacrifice precision, which demonstrates the strength of our solution.\nTo create our spatiotemporal parser, we collected and annotated a data set rich in such expressions for training the model. We used active learning to reduce the manual annotation required and chose only the most effective examples for training. We used an average perceptron FHMM as a base model to recognize such expressions and we adopted ensemble learning to create a bag of FHMMs to reduce the model variance. Our dataset is annotated both with named entities and with unnamed entities that are spatiotemporal expressions is available to the research community. Our approach can be applied widely in sequence labeling tasks.\nWe have shown that ensemble methods are effective in reducing the variance of label output. Further, we have found surprisingly that Belief Propagation is a more suitable inference algorithm than Viterbi decoding at the beginning of the active learning periods, owing to the small size of the data. Our re-weighting for active learning sampling outperformed the baseline random sampling, but the utility function does not seem to improve active learning very much."}, {"heading": "9. REFERENCES", "text": "[1] Breiman, L.. (1996). Bagging Predictors, Machine Learning,\n24(2): 123-140.\n[2] Chang, A.X., Manning, C. D. (2012). SUTime: A library for recognizing and normalizing time expressions. LREC. 2012.\n[3] Collins, M. (2002). Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms. Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, 1-8.\n[4] Culotta, A. and McCallum, A. (2005). Reducing labeling effort for structured prediction tasks. AAAI, vol 2 746-751.\n[5] Daum\u00e9 III, H., Langford, J., and Marcu, D. (2009). Searchbased structured prediction. Machine learning, 75(3), 297- 325.\n[6] Dietterich, T. G. (2000). An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization. Machine learning, 40(2), 139-157.\n[7] Evgeniou, T., Pontil, M. and Poggio, T. (2000). Regularization networks and support vector machines. Advances in computational mathematics (13): 1-50.\n[8] Finkel, J. and Manning, C. (2009). Joint parsing and named entity recognition. Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, 323-334.\n[9] Gey, F.C., Kando, N., Larson, R.R. (2010). The crucial role of semantic discovery and markup in geo-temporal search. ESAIR \u201910, Toronto, Ontario, Canada, 5-6.\n[10] Hagenauer, J., & Hoeher, P. (1989). A Viterbi algorithm with soft-decision outputs and its applications. In Global Telecommunications Conference and Exhibition\nCommunications Technology for the 1990s and Beyond'(GLOBECOM), 1989. IEEE (pp. 1680-1686). IEEE.\n[11] Hartrumpf, S., and Leveling, J. (2007). Interpretation and normalization of temporal expressions for question answering. Evaluation of Multilingual and Multi-modal Information Retrieval. Springer Berlin Heidelberg, 2007. 432-439.\n[12] Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural computation 9.8 (1997): 1735-1780.\n[13] Holub, A., Perona, P. and Burl, M. C. (2008). Entropy-based active learning for object recognition. IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPRW'08. 23-28 June 2008, 885- 892\n[14] Kohavi, R. (1995). A study of cross-validation and bootstrap for accuracy estimation and model selection. Proceedings of the International Joint Conference on Artificial Intelligence IJCAI 14(2): 1137-1143\n[15] Kschischang, F.R., Frey, B.J., and Loeliger, H-A. (2001). Factor graphs and the sum-product algorithm. IEEE Transactions on Information Theory 47(2): 498-519.\n[16] Lafferty, John, Andrew McCallum, and Fernando CN Pereira. \"Conditional random fields: Probabilistic models for segmenting and labeling sequence data.\" (2001).\n[17] Lember, J., and Koloydenko, A. A. (2014). Bridging Viterbi and posterior decoding: a generalized risk approach to hidden path inference based on hidden Markov models. The Journal of Machine Learning Research 15(1): 1-58.\n[18] Li, C. and Sun, A. (2014). Fine-grained location extraction from tweets with temporal awareness. SIGIR\u201914, July 6\u201311, 2014, Gold Coast, QLD, Australia, [10 p.].\n[19] Lu, Z., Wu, X., and Bongard, J.C. (2015). Active Learning Through Adaptive Heterogeneous Ensembling. Knowledge and Data Engineering, IEEE Transactions on 27(2): 368- 381.\n[20] McCallum, A., Freitag, D., & Pereira, F. C. (2000, June). Maximum Entropy Markov Models for Information Extraction and Segmentation. In ICML (Vol. 17, pp. 591- 598).\n[21] Marcheggiani, D. and Artieres, T. (2014). An experimental comparison of active learning strategies for partially labeled sequences. EMNLP, October 25-29, Doha Quatar, 898-906.\n[22] Melville, P., and Mooney. Diverse ensembles for active learning. Proceedings of the Twenty-first International Conference on Machine Learning. ACM, 2004, 74-81.\n[23] Melville, P., and Mooney, R. J. (2005). Creating diversity in ensembles using artificial data. Information Fusion 6(1): 99- 111.\n[24] Murphy, K.P., Weiss, Y. and Jordan, M. I. (1999). Loopy belief propagation for approximate inference: An empirical study. Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence. Morgan Kaufmann Publishers Inc., 467-475.\n[25] Oh, S., Lee, M.S. and Zhang, B-T (2011). Ensemble learning with active example selection for imbalanced biomedical data classification. IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB) 8(2): 316-325.\n[26] Pearl, J. (1982). Reverend Bayes on inference engines: A distributed hierarchical approach. AAAI. 1982, 133-136.\n[27] Prechelt, L. (1998). Automatic early stopping using cross validation: quantifying the criteria. Neural Networks 11(4): 761-767.\n[28] Rabiner, L. R., and Juang, B-H. (1986.) An introduction to hidden Markov models. ASSP Magazine, IEEE 3(1): 4-16.\n[29] Ratinov, L. and Roth, D. (2009). Design challenges and misconceptions in Named Entity Recognition. CoNLL 09, [8p.]\n[30] Richardson, T. J., and Urbanke, R.L. (2001). The capacity of low-density parity-check codes under message-passing decoding. IEEE Transactions on Information Theory 47(2): 599-618.\n[31] Santos, D., Cardoso, N., Carvalho, P. Dornescu, I., Hartfrumpf, S. Leveling, J. and Skalban, Y. (2009). GikiP at GeoCLEF 2008: Joining GIR and QA forces for querying Wikipedia. In C. Peters et al. (Eds). Evaluating systems for multilingual and multimodal information access, vol. 5706 LNCS, 894-905. Springer.\n[32] Schilder, F., and Habel, C. (2001). From temporal expressions to temporal information: Semantic tagging of news messages. Proceedings of the workshop on Temporal and Spatial Information Processing-Volume 13. Association for Computational Linguistics, 2001.\n[33] Schumacher, J., et al. (2012) Active learning of ensemble classifiers for gesture recognition. Springer Berlin Heidelberg, 2012.\n[34] Sha, F., and Pereira, F. (2003). Shallow parsing with conditional random fields. Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, 134-141.\n[35] Settles, B. (2004). Biomedical named entity recognition using conditional random fields and rich feature sets. Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications. Association for Computational Linguistics, 2004, 104-107.\n[36] Settles, B., and Craven, M (2008). An analysis of active learning strategies for sequence labeling tasks. Proceedings of the conference on empirical methods in natural language processing. Association for Computational Linguistics, 2008. [10 p.]\n[37] Shen, H., and Sarkar, A. (2005). Voting between multiple data representations for text chunking. B. K\u00e9gl and G. Lapalme (Eds). AI 2005, LNAI 3501, 389-400.\n[38] Sun, X., Morency, L-P, Okanohara, D., and Tsujii, J. (2008). Modeling latent-dynamic in shallow parsing: a latent conditional model with improved inference. Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, 841-848.\n[39] Taskar, Ben. Guestrin, Carlos. Koller, Daphne. \"Max-margin Markov networks.\" Advances in neural information processing systems 16 (2004): 25.\n[40] Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological) (1996): 267-288.\n[41] Tkachenko, M., and Simanovsky, A. (2012). Named entity recognition: Exploring features. Proceedings of KONVENS, 118-127.\n[42] Tsochantaridis, I., Joachims, T., Hofmann, T., & Altun, Y. (2005). Large margin methods for structured and interdependent output variables. In Journal of Machine Learning Research (pp. 1453-1484).\n[43] Vicente-Diez, M.t., and Martinez, P. (2009). Temporal semantics extraction for improving web search. Database and Expert Systems Application, 2009. DEXA'09. 20th International Workshop on. IEEE, 2009\n[44] Yuan, Y. (2011). Extracting spatial relations from document for geographic information retrieval. 19th International Conference on Geoinformatics, 24-26 June, Shanghai, China, 1-5.\n[45] Zhang, W., and Gelernter, J. (2015). Geocoding location expressions in Twitter messages: A preference learning method. Journal of Spatial Information Science 9 (2015): 37- 70.\n[46] Zhou, G., and Su, J. (2002). Named entity recognition using an HMM-based chunk tagger. Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, 2002, 473-480.\n[47] Haochang Wang, Tiejun Zhao, Hongye Tan, and Shu Zhang. 2008. Biomedical named entity recognition based on classifiers ensemble. IJCSA, 5(2):1\u201311.\n[48] Haochang Wang, Tiejun Zhao, Hongye Tan, and Shu Zhang. 2008. Biomedical named entity recognition based on classifiers ensemble. IJCSA, 5(2):1\u201311.\n[49] Finkel, J., Dingare, S., Nguyen, H., Nissim, M., Manning, C., & Sinclair, G. 2004. Exploiting context for biomedical entity recognition: from syntax to the web. In Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (pp. 88-91)."}], "references": [{"title": "SUTime: A library for recognizing and normalizing time expressions", "author": ["A.X. Chang", "C.D. Manning"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms", "author": ["M. Collins"], "venue": "Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Reducing labeling effort for structured prediction", "author": ["A. Culotta", "A. McCallum"], "venue": "tasks. AAAI,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Searchbased structured prediction", "author": ["H. Daum\u00e9 III", "J. Langford", "D. Marcu"], "venue": "Machine learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization", "author": ["T.G. Dietterich"], "venue": "Machine learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Regularization networks and support vector machines. Advances in computational mathematics", "author": ["T. Evgeniou", "M. Pontil", "T. Poggio"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Joint parsing and named entity recognition", "author": ["J. Finkel", "C. Manning"], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "The crucial role of semantic discovery and markup in geo-temporal search", "author": ["F.C. Gey", "N. Kando", "R.R. Larson"], "venue": "ESAIR \u201910,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "A Viterbi algorithm with soft-decision outputs and its applications", "author": ["J. Hagenauer", "P. Hoeher"], "venue": "In Global Telecommunications Conference and Exhibition  Communications Technology for the 1990s and Beyond'(GLOBECOM),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1989}, {"title": "Interpretation and normalization of temporal expressions for question answering. Evaluation of Multilingual and Multi-modal Information Retrieval", "author": ["S. Hartrumpf", "J. Leveling"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Entropy-based active learning for object recognition", "author": ["A. Holub", "P. Perona", "M.C. Burl"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "A study of cross-validation and bootstrap for accuracy estimation and model selection", "author": ["R. Kohavi"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence IJCAI", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1995}, {"title": "Factor graphs and the sum-product algorithm", "author": ["F.R. Kschischang", "B.J. Frey", "Loeliger", "H-A"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence", "author": ["Lafferty", "John", "Andrew McCallum", "Fernando CN Pereira"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Bridging Viterbi and posterior decoding: a generalized risk approach to hidden path inference based on hidden Markov models", "author": ["J. Lember", "A.A. Koloydenko"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Fine-grained location extraction from tweets with temporal awareness. SIGIR\u201914", "author": ["C. Li", "A. Sun"], "venue": "July 6\u201311,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Active Learning Through Adaptive Heterogeneous Ensembling", "author": ["Z. Lu", "X. Wu", "J.C. Bongard"], "venue": "Knowledge and Data Engineering, IEEE Transactions on 27(2):", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Maximum Entropy Markov Models for Information Extraction and Segmentation", "author": ["A. McCallum", "D. Freitag", "Pereira", "F. C", "June"], "venue": "In ICML (Vol", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "An experimental comparison of active learning strategies for partially labeled sequences", "author": ["D. Marcheggiani", "T. Artieres"], "venue": "Doha Quatar,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Diverse ensembles for active learning", "author": ["P. Melville", "Mooney"], "venue": "Proceedings of the Twenty-first International Conference on Machine Learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Creating diversity in ensembles using artificial data", "author": ["P. Melville", "R.J. Mooney"], "venue": "Information Fusion", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Loopy belief propagation for approximate inference: An empirical study", "author": ["K.P. Murphy", "Y. Weiss", "M.I. Jordan"], "venue": "Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1999}, {"title": "Ensemble learning with active example selection for imbalanced biomedical data classification", "author": ["S. Oh", "M.S. Lee", "Zhang", "B-T"], "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB)", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Reverend Bayes on inference engines: A distributed hierarchical approach", "author": ["J. Pearl"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1982}, {"title": "Automatic early stopping using cross validation: quantifying the criteria", "author": ["L. Prechelt"], "venue": "Neural Networks", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1998}, {"title": "An introduction to hidden Markov models", "author": ["L.R. Rabiner", "Juang", "B-H"], "venue": "ASSP Magazine,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1986}, {"title": "Design challenges and misconceptions in Named Entity Recognition", "author": ["L. Ratinov", "D. Roth"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "The capacity of low-density parity-check codes under message-passing decoding", "author": ["T.J. Richardson", "R.L. Urbanke"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2001}, {"title": "GikiP at GeoCLEF 2008: Joining GIR and QA forces for querying Wikipedia", "author": ["D. Santos", "N. Cardoso", "Carvalho", "I.P. Dornescu", "Hartfrumpf", "J.S. Leveling", "Y. Skalban"], "venue": "In C. Peters et al. (Eds). Evaluating systems for multilingual and multimodal information access,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "From temporal expressions to temporal information: Semantic tagging of news messages", "author": ["F. Schilder", "C. Habel"], "venue": "Proceedings of the workshop on Temporal and Spatial Information Processing-Volume 13. Association for Computational Linguistics,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2001}, {"title": "Active learning of ensemble classifiers for gesture recognition", "author": ["J Schumacher"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Shallow parsing with conditional random fields", "author": ["F. Sha", "F. Pereira"], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2003}, {"title": "Biomedical named entity recognition using conditional random fields and rich feature sets", "author": ["B. Settles"], "venue": "Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications. Association for Computational Linguistics,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2004}, {"title": "An analysis of active learning strategies for sequence labeling tasks. Proceedings of the conference on empirical methods in natural language processing", "author": ["B. Settles", "M Craven"], "venue": "Association for Computational Linguistics,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2008}, {"title": "Voting between multiple data representations for text chunking", "author": ["H. Shen", "A. Sarkar"], "venue": "B. Ke\u0301gl and G. Lapalme (Eds). AI 2005,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2005}, {"title": "Modeling latent-dynamic in shallow parsing: a latent conditional model with improved inference", "author": ["X. Sun", "Morency", "L-P", "D. Okanohara", "J. Tsujii"], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics-Volume", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2008}, {"title": "Max-margin Markov networks.\" Advances in neural information processing systems", "author": ["Taskar", "Ben. Guestrin", "Carlos. Koller", "Daphne"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2004}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological)", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1996}, {"title": "Named entity recognition: Exploring features", "author": ["M. Tkachenko", "A. Simanovsky"], "venue": "Proceedings of KONVENS,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2012}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "In Journal of Machine Learning Research (pp. 1453-1484)", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2005}, {"title": "Temporal semantics extraction for improving web search", "author": ["Vicente-Diez", "M.t", "P. Martinez"], "venue": "Database and Expert Systems Application,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2009}, {"title": "Extracting spatial relations from document for geographic information retrieval", "author": ["Y. Yuan"], "venue": "International Conference on Geoinformatics,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2011}, {"title": "Geocoding location expressions in Twitter messages: A preference learning method", "author": ["W. Zhang", "J. Gelernter"], "venue": "Journal of Spatial Information Science", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "Named entity recognition using an HMM-based chunk tagger", "author": ["G. Zhou", "J. Su"], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2002}, {"title": "Biomedical named entity recognition based on classifiers ensemble", "author": ["Haochang Wang", "Tiejun Zhao", "Hongye Tan", "Shu Zhang"], "venue": "IJCSA,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2008}, {"title": "Biomedical named entity recognition based on classifiers ensemble", "author": ["Haochang Wang", "Tiejun Zhao", "Hongye Tan", "Shu Zhang"], "venue": "IJCSA,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2008}, {"title": "Exploiting context for biomedical entity recognition: from syntax to the web", "author": ["J. Finkel", "S. Dingare", "H. Nguyen", "M. Nissim", "C. Manning", "G. Sinclair"], "venue": "In Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (pp. 88-91)", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2004}], "referenceMentions": [{"referenceID": 43, "context": "We consider the problem of recognizing unnamed location and time phrases to ease downstream applications such as geocoding [45] and time normalization [2].", "startOffset": 123, "endOffset": 127}, {"referenceID": 0, "context": "We consider the problem of recognizing unnamed location and time phrases to ease downstream applications such as geocoding [45] and time normalization [2].", "startOffset": 151, "endOffset": 154}, {"referenceID": 9, "context": "A wider range of expressions parsed could benefit question and answering [11][9][31], web querying toward the semantic web [43], information retrieval as from news messages [32].", "startOffset": 73, "endOffset": 77}, {"referenceID": 7, "context": "A wider range of expressions parsed could benefit question and answering [11][9][31], web querying toward the semantic web [43], information retrieval as from news messages [32].", "startOffset": 77, "endOffset": 80}, {"referenceID": 29, "context": "A wider range of expressions parsed could benefit question and answering [11][9][31], web querying toward the semantic web [43], information retrieval as from news messages [32].", "startOffset": 80, "endOffset": 84}, {"referenceID": 41, "context": "A wider range of expressions parsed could benefit question and answering [11][9][31], web querying toward the semantic web [43], information retrieval as from news messages [32].", "startOffset": 123, "endOffset": 127}, {"referenceID": 30, "context": "A wider range of expressions parsed could benefit question and answering [11][9][31], web querying toward the semantic web [43], information retrieval as from news messages [32].", "startOffset": 173, "endOffset": 177}, {"referenceID": 26, "context": "Labeling words in sentences automatically is done sequence labeling models such as Hidden Markov Models (HMM) [28] [46] and Conditional Random Fields (CRF) [16], Average Perceptron [3], Max-Margin Markov Networks [39], SVM [42], SEARN algorithm [5], Max Entropy Markov Models [20] and so on.", "startOffset": 110, "endOffset": 114}, {"referenceID": 44, "context": "Labeling words in sentences automatically is done sequence labeling models such as Hidden Markov Models (HMM) [28] [46] and Conditional Random Fields (CRF) [16], Average Perceptron [3], Max-Margin Markov Networks [39], SVM [42], SEARN algorithm [5], Max Entropy Markov Models [20] and so on.", "startOffset": 115, "endOffset": 119}, {"referenceID": 14, "context": "Labeling words in sentences automatically is done sequence labeling models such as Hidden Markov Models (HMM) [28] [46] and Conditional Random Fields (CRF) [16], Average Perceptron [3], Max-Margin Markov Networks [39], SVM [42], SEARN algorithm [5], Max Entropy Markov Models [20] and so on.", "startOffset": 156, "endOffset": 160}, {"referenceID": 1, "context": "Labeling words in sentences automatically is done sequence labeling models such as Hidden Markov Models (HMM) [28] [46] and Conditional Random Fields (CRF) [16], Average Perceptron [3], Max-Margin Markov Networks [39], SVM [42], SEARN algorithm [5], Max Entropy Markov Models [20] and so on.", "startOffset": 181, "endOffset": 184}, {"referenceID": 37, "context": "Labeling words in sentences automatically is done sequence labeling models such as Hidden Markov Models (HMM) [28] [46] and Conditional Random Fields (CRF) [16], Average Perceptron [3], Max-Margin Markov Networks [39], SVM [42], SEARN algorithm [5], Max Entropy Markov Models [20] and so on.", "startOffset": 213, "endOffset": 217}, {"referenceID": 40, "context": "Labeling words in sentences automatically is done sequence labeling models such as Hidden Markov Models (HMM) [28] [46] and Conditional Random Fields (CRF) [16], Average Perceptron [3], Max-Margin Markov Networks [39], SVM [42], SEARN algorithm [5], Max Entropy Markov Models [20] and so on.", "startOffset": 223, "endOffset": 227}, {"referenceID": 3, "context": "Labeling words in sentences automatically is done sequence labeling models such as Hidden Markov Models (HMM) [28] [46] and Conditional Random Fields (CRF) [16], Average Perceptron [3], Max-Margin Markov Networks [39], SVM [42], SEARN algorithm [5], Max Entropy Markov Models [20] and so on.", "startOffset": 245, "endOffset": 248}, {"referenceID": 18, "context": "Labeling words in sentences automatically is done sequence labeling models such as Hidden Markov Models (HMM) [28] [46] and Conditional Random Fields (CRF) [16], Average Perceptron [3], Max-Margin Markov Networks [39], SVM [42], SEARN algorithm [5], Max Entropy Markov Models [20] and so on.", "startOffset": 276, "endOffset": 280}, {"referenceID": 4, "context": "To further reduce overfitting and prediction variance, we used FHMM ensembles, specifically, bagging [6] of FHMM, with the correct result being found by voting among results of the individual models.", "startOffset": 101, "endOffset": 104}, {"referenceID": 16, "context": "The PETAR system ([18] that works on tweets is among the first to change this.", "startOffset": 18, "endOffset": 22}, {"referenceID": 43, "context": "For geoparsing (method to find location entities) results, see [45].", "startOffset": 63, "endOffset": 67}, {"referenceID": 42, "context": "Geocoding research that incorporates natural language understanding of adjectival or prepositional phrases has been termed \u201cspatial relations\u201d [44].", "startOffset": 143, "endOffset": 147}, {"referenceID": 20, "context": "[22] and [23] studied ensemble classifiers for continuous values, [33] for gesture recognition, [25] for biomedical data, and [13] on object recognition for computer vision.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] and [23] studied ensemble classifiers for continuous values, [33] for gesture recognition, [25] for biomedical data, and [13] on object recognition for computer vision.", "startOffset": 9, "endOffset": 13}, {"referenceID": 31, "context": "[22] and [23] studied ensemble classifiers for continuous values, [33] for gesture recognition, [25] for biomedical data, and [13] on object recognition for computer vision.", "startOffset": 66, "endOffset": 70}, {"referenceID": 23, "context": "[22] and [23] studied ensemble classifiers for continuous values, [33] for gesture recognition, [25] for biomedical data, and [13] on object recognition for computer vision.", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "[22] and [23] studied ensemble classifiers for continuous values, [33] for gesture recognition, [25] for biomedical data, and [13] on object recognition for computer vision.", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "[19] designed ensembles that are heterogeneous rather than homogeneous.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Active learning for ensembles is little studied beyond [22] and [23].", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "Active learning for ensembles is little studied beyond [22] and [23].", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "However, active learning for sequence labeling models has been investigated extensively [4] [36] [21].", "startOffset": 88, "endOffset": 91}, {"referenceID": 34, "context": "However, active learning for sequence labeling models has been investigated extensively [4] [36] [21].", "startOffset": 92, "endOffset": 96}, {"referenceID": 19, "context": "However, active learning for sequence labeling models has been investigated extensively [4] [36] [21].", "startOffset": 97, "endOffset": 101}, {"referenceID": 0, "context": "494 SUTime [2] 0.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "The model is trained with the average perceptron algorithm [3], which resembles a single linear perceptron in neural networks.", "startOffset": 59, "endOffset": 62}, {"referenceID": 1, "context": "See [3] for details.", "startOffset": 4, "endOffset": 7}, {"referenceID": 0, "context": "In Table 4, the features defined in context window [-2,2] are the \u201cglobal features\u201d that increase non-linearity.", "startOffset": 51, "endOffset": 57}, {"referenceID": 25, "context": "Several general techniques can be used to reduce overfitting in optimization, such as early stopping [27], L2 regularization [7], Lasso regression [40], cross validation [14], and feature selection.", "startOffset": 101, "endOffset": 105}, {"referenceID": 5, "context": "Several general techniques can be used to reduce overfitting in optimization, such as early stopping [27], L2 regularization [7], Lasso regression [40], cross validation [14], and feature selection.", "startOffset": 125, "endOffset": 128}, {"referenceID": 38, "context": "Several general techniques can be used to reduce overfitting in optimization, such as early stopping [27], L2 regularization [7], Lasso regression [40], cross validation [14], and feature selection.", "startOffset": 147, "endOffset": 151}, {"referenceID": 12, "context": "Several general techniques can be used to reduce overfitting in optimization, such as early stopping [27], L2 regularization [7], Lasso regression [40], cross validation [14], and feature selection.", "startOffset": 170, "endOffset": 174}, {"referenceID": 1, "context": "But in a perceptron algorithm, parameter averaging is a simple but effective technique used to control overfitting [3].", "startOffset": 115, "endOffset": 118}, {"referenceID": 8, "context": "We compared two inference (decoding) algorithms: Nbest Viterbi decoding (or Viterbi decoding for short) [10], and the Belief Propagation (BP) algorithm, or the sum-product message passing algorithm [26], [30].", "startOffset": 104, "endOffset": 108}, {"referenceID": 24, "context": "We compared two inference (decoding) algorithms: Nbest Viterbi decoding (or Viterbi decoding for short) [10], and the Belief Propagation (BP) algorithm, or the sum-product message passing algorithm [26], [30].", "startOffset": 198, "endOffset": 202}, {"referenceID": 28, "context": "We compared two inference (decoding) algorithms: Nbest Viterbi decoding (or Viterbi decoding for short) [10], and the Belief Propagation (BP) algorithm, or the sum-product message passing algorithm [26], [30].", "startOffset": 204, "endOffset": 208}, {"referenceID": 13, "context": "The BP algorithm is widely used in graphical models for exact inference in tree graphs [15] and for approximate inference over loopy graphs [24].", "startOffset": 87, "endOffset": 91}, {"referenceID": 22, "context": "The BP algorithm is widely used in graphical models for exact inference in tree graphs [15] and for approximate inference over loopy graphs [24].", "startOffset": 140, "endOffset": 144}, {"referenceID": 15, "context": "Recent work tries to combine the two to achieve the power of both [17].", "startOffset": 66, "endOffset": 70}, {"referenceID": 26, "context": "Viterbi decoding generates the probability for each decoding sequence, whereas BP uses forward-backward algorithm to predict each token individually [28].", "startOffset": 149, "endOffset": 153}, {"referenceID": 0, "context": "[-2,2] in first column means the feature is extracted within token window of 5 for each word.", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "Lemma in window [-2,2] y y y y", "startOffset": 16, "endOffset": 22}, {"referenceID": 0, "context": "Word form [-2,2] y y", "startOffset": 10, "endOffset": 16}, {"referenceID": 0, "context": "Part-of-speech [-2,2] y", "startOffset": 15, "endOffset": 21}, {"referenceID": 0, "context": "Word suffix [-2,2] y y y", "startOffset": 12, "endOffset": 18}, {"referenceID": 0, "context": "Named entity tag [-2,2] y", "startOffset": 17, "endOffset": 23}, {"referenceID": 0, "context": "Part-of-speech bigram [-2,2] y", "startOffset": 22, "endOffset": 28}, {"referenceID": 0, "context": "Suffix bigram [-2,2] y", "startOffset": 14, "endOffset": 20}, {"referenceID": 0, "context": "Word form bigram [-2,2] y y y", "startOffset": 17, "endOffset": 23}, {"referenceID": 20, "context": "Active learning for classifier ensembles has been studied on classification problems [22] and [23].", "startOffset": 85, "endOffset": 89}, {"referenceID": 21, "context": "Active learning for classifier ensembles has been studied on classification problems [22] and [23].", "startOffset": 94, "endOffset": 98}, {"referenceID": 2, "context": "Active learning for sequence labeling models was extensively investigated in [4] [36] [21].", "startOffset": 77, "endOffset": 80}, {"referenceID": 34, "context": "Active learning for sequence labeling models was extensively investigated in [4] [36] [21].", "startOffset": 81, "endOffset": 85}, {"referenceID": 19, "context": "Active learning for sequence labeling models was extensively investigated in [4] [36] [21].", "startOffset": 86, "endOffset": 90}, {"referenceID": 20, "context": "We introduce the general architecture of a typical active learning routine for ensembles [22] and [23] before we discuss details of active learning.", "startOffset": 89, "endOffset": 93}, {"referenceID": 21, "context": "We introduce the general architecture of a typical active learning routine for ensembles [22] and [23] before we discuss details of active learning.", "startOffset": 98, "endOffset": 102}, {"referenceID": 34, "context": "In this work, we configured algorithm 2 by using (1) a sequence labeling utility function Sequence Vote Entropy [36] for Utility(), and (2) a new Sentence Re-weighting method for creating ensembles from T, which corresponds to learn_Ensemble (Base, T) shown in Algorithm 3.", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "This is analogous to short-term memory mechanism used in Long-Short Term Memory (LSTM) neural networks [12].", "startOffset": 103, "endOffset": 107}, {"referenceID": 35, "context": "We use the F1 measure from our single FHMM model to be consistent with others\u2019 published results on single models (even though [37] uses an ensemble HMM and voting to achieve 95.", "startOffset": 127, "endOffset": 131}, {"referenceID": 6, "context": "Train test split of those follows [8] to be comparable.", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "Our score is higher than that of Finkel and Manning\u2019s [8] basic NER model which uses features similar to those that we use.", "startOffset": 54, "endOffset": 57}, {"referenceID": 27, "context": "[29] used Average Perceptron training along with rich semantic features, as do [41].", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[29] used Average Perceptron training along with rich semantic features, as do [41].", "startOffset": 79, "endOffset": 83}, {"referenceID": 47, "context": "Still, it is the Wang et al [49] ensemble method that outperforms other single-model methods to a large extent.", "startOffset": 28, "endOffset": 32}, {"referenceID": 6, "context": "Among the results that do not use extended outside knowledge but explore word forms and context only, FHMM is shown to be competitive with [8] and [35].", "startOffset": 139, "endOffset": 142}, {"referenceID": 33, "context": "Among the results that do not use extended outside knowledge but explore word forms and context only, FHMM is shown to be competitive with [8] and [35].", "startOffset": 147, "endOffset": 151}, {"referenceID": 46, "context": "Ensemble of Sequence Labeling algorithms [48] 77.", "startOffset": 41, "endOffset": 45}, {"referenceID": 39, "context": "6 Average Perceptron with knowledge [41] 74.", "startOffset": 36, "endOffset": 40}, {"referenceID": 47, "context": "[49] 70.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[35] 69.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[8] 74.", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "Average Perceptron by Ratinov [29] 72.", "startOffset": 30, "endOffset": 34}, {"referenceID": 39, "context": "CRF by Tkachenko [41] 76.", "startOffset": 17, "endOffset": 21}, {"referenceID": 32, "context": "Conditional Random Fields [34] 94.", "startOffset": 26, "endOffset": 30}, {"referenceID": 36, "context": "3 Latent Dynamic Conditional Random Fields [38] 94.", "startOffset": 43, "endOffset": 47}, {"referenceID": 34, "context": "This result is consistent with the [36] result.", "startOffset": 35, "endOffset": 39}, {"referenceID": 10, "context": "Alternative algorithms for training that could be tested include neural networks, which have shown excellent results on sequential labeling tasks, especially the long-term short-term memory network [12].", "startOffset": 198, "endOffset": 202}], "year": 2015, "abstractText": "Precise geocoding and time normalization for text requires that location and time phrases be identified. Many state-of-the-art geoparsers and temporal parsers suffer from low recall. Categories commonly missed by parsers are: nouns used in a nonspatiotemporal sense, adjectival and adverbial phrases, prepositional phrases, and numerical phrases. We collected and annotated data set by querying commercial web searches API with such spatiotemporal expressions as were missed by state-of-theart parsers. Due to the high cost of sentence annotation, active learning was used to label training data, and a new strategy was designed to better select training examples to reduce labeling cost. For the learning algorithm, we applied an average perceptron trained Featurized Hidden Markov Model (FHMM). Five FHMM instances were used to create an ensemble, with the output phrase selected by voting. Our ensemble model was tested on a range of sequential labeling tasks, and has shown competitive performance. Our contributions include (1) an new dataset annotated with named entities and expanded spatiotemporal expressions; (2) a comparison of inference algorithms for ensemble models showing the superior accuracy of Belief Propagation over Viterbi Decoding; (3) a new example re-weighting method for active ensemble learning that \u201cmemorizes\u201d the latest examples trained; (4) a spatiotemporal parser that jointly recognizes expanded spatiotemporal expressions as well as named entities.", "creator": "Word"}}}