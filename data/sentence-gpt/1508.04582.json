{"id": "1508.04582", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Aug-2015", "title": "Learning to Predict Independent of Span", "abstract": "We consider how to learn multi-step predictions efficiently. Conventional algorithms wait until observing actual outcomes before performing the computations to update their predictions. If predictions are made at a high rate or span over a large amount of time, substantial computation can be required to store all relevant observations and to update all predictions when the outcome is finally observed. We show that the exact same predictions can be learned in a much more computationally congenial way, with uniform per-step computation that does not depend on the span of the predictions. We apply this idea to various settings of increasing generality, repeatedly adding desired properties and each time deriving an equivalent span-independent algorithm for the conventional algorithm that satisfies these desiderata. Interestingly, along the way several known algorithmic constructs emerge spontaneously from our derivations, including dutch eligibility traces, temporal difference errors, and averaging. This allows us to link these constructs one-to-one to the corresponding desiderata, unambiguously connecting the `how' to the `why'. Each step, we make sure that the derived algorithm subsumes the previous algorithms, thereby retaining their properties. Ultimately we arrive at a single general temporal-difference algorithm that is applicable to the full setting of reinforcement learning.", "histories": [["v1", "Wed, 19 Aug 2015 09:37:25 GMT  (153kb,D)", "http://arxiv.org/abs/1508.04582v1", "32 pages"]], "COMMENTS": "32 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hado van hasselt", "richard s sutton"], "accepted": false, "id": "1508.04582"}, "pdf": {"name": "1508.04582.pdf", "metadata": {"source": "CRF", "title": "Learning to Predict Independent of Span", "authors": ["Hado van Hasselt", "Richard S. Sutton"], "emails": [], "sections": [{"heading": "1 Learning long-term predictions", "text": "The span of a multi-step prediction is the number of steps elapsing between when the prediction is made and when its target or ideal value is known. We consider the case in which predictions are made repeatedly, at each of a sequence of discrete time steps. For example, if on each day we predict what a stock market index will be in 30 days, then the span is 30, whereas if we predict at each hour what the stock market index will be in 30 days, then the span is 30\u00d7 24 = 720.\nThe span may vary for individual predictions in a sequence. For example, if we predict on each day what the stock-market index will be at the end of the year, then the span will be much longer for predictions made in January than it is for predictions made in\n\u2217Google DeepMind \u2020Reinforcement Learning and Artificial Intelligence Laboratory\nDepartment of Computing Science, University of Alberta Edmonton, Alberta, Canada T6G 2E8\nar X\niv :1\n50 8.\n04 58\n2v 1\n[ cs\n.L G\n] 1\n9 A\nug 2\nDecember. If the span may vary in this way, then we consider the span of the prediction sequence to be the maximum possible span of any individual prediction in the sequence. For example, the span of a daily end-of-year stock-index prediction is 365. Often the span is infinite. For example, in reinforcement learning we often learn value functions that are predictions of the discounted sum of all future rewards in the potentially infinite future (Sutton and Barto 1998).\nIn this paper we consider computational and algorithmic issues in efficiently learning long-term predictions, defined as predictions of large integer span. Predictions could be long term in this sense either because a great deal of clock time passes, as in predicting something at the end of the year, or because predictions are made very often, with a short time between steps (e.g., as in high-frequency financial trading). The per-step computational complexity of some algorithms for learning accurate predictions depends on the span of the predictions, and this can become a significant concern if the span is large. Therefore, we focus on the construction of learning algorithms whose computational complexity per time step (in both time and memory) is constant (does not scale with time) and independent of span.\nThis paper features two recurring themes, the first of which is the repeated spontaneous emergence of, often well-known, algorithmic constructs, directly from our derivations. We start each derivation by formalizing a desired property and constructing an algorithm that fulfills it, without considering computationally efficiency. Then, we derive a spanindependent algorithm that results on each step in exactly the same predictions. Interestingly, each time a specific algorithmic construct emerges, demonstrating a clear connection between the desideratum (the \u2018why\u2019) and the algorithmic construct (the \u2018how\u2019). For instance, the desire to be independent of span leads to a dutch eligibility trace, which was previously derived only in the more specific context of online temporal difference (TD) learning (van Seijen and Sutton 2014).\nThe second theme is that we unify the algorithms at each step. Each time, we make sure to obtain an algorithm that is strictly more general than the previous ones, so that in the end we obtain one single algorithm that can fulfill all the desiderata while remaining computationally congenial."}, {"heading": "2 Outline of the paper", "text": "In this section, we briefly describe the high-level narrative of the paper, without going into technical detail. In each of the Sections 3 to 8, we describe and formalize one or more desirable properties for our algorithms and then derive a computationally congenial algorithm that achieves this exactly. We build up to the final, most general, algorithm that is ultimately derived in Section 8 to highlight the connections between desired properties and algorithmic constructs. Making these connections clear is one of the main goals of this paper.\nSpecifically, in Section 3 we derive a span-independent algorithm to update the predictions for a single final outcome. The algorithm is offline in the sense that does not change its predictions before observing the outcome. The dutch trace emerges spontaneously, which shows that this trace is closely tied to the requirement of span-independent computation. This emergence is surprising and intriguing because it shows that these traces\nare not specific to online TD learning, for which they were first proposed (van Seijen and Sutton 2014).\nIn Section 4 we derive span-independent updates that update the predictions online, towards interim targets that temporarily stand in for the final outcome. We show that the desire to be online results in the spontaneous emergence of TD errors (Sutton 1984; Sutton 1988). In this paper we are mostly agnostic to the origin of the interim targets. These may for instance be given by external experts or by own online predictions, as in standard TD learning (e.g., see Sutton and Barto 1998).\nIt can be beneficial to be able to switch smoothly between online and offline updates, on a step-by-step basis, for instance when we do not full trust some of the interim targets that we would use for our online updates. This allows us to have the best of both worlds: the online predictions stay trustworthy even if some interim targets are wrong, and we are still able to use any useful information immediately when it is observed. In Section 5 we consider how to do this efficiently and from our derivation an update emerges that averages the online weights in a separate trusted weight vector. This is interesting because such averaging is known to improve the convergence rates of online learning algorithms (Polyak and Juditsky 1992; Bach and Moulines 2013), but seems to only rarely be used in reinforcement learning (as noted, e.g., by Szepesva\u0301ri 2010).\nSome interim targets may be so informative that we want their effect to persist in the predictions even after observing the final outcome. For instance, if the final outcome is stochastic and the interim targets are drawn independently from the same distribution it makes sense to average these instead of committing fully only to the final outcome. In the extreme, we might see an interim target that we trust so much that we do not even care about the actual outcome anymore, for instance because the interim target already takes into account all possible outcomes from that moment rather than only the specific one that will happen to materialize this time, resulting in a more accurate prediction on average than a single final outcome. In Section 6, we formalize these ideas and show they lead naturally to a form of TD(\u03bb) (Sutton 1988; Sutton and Barto 1998).\nThe \u03bb parameter that governs the amount of persistency of the interim targets can be interpreted as representing a degree of trust: if we trust an interim target fully (\u03bb = 0) we do not need to consider later observations, while if we distrust it fully (\u03bb = 1) it will be replaced by later targets and leave no trace in the final predictions. This is a different notion of trust than the one considered for the smooth switching between online and offline updates, where the trust was relative to the actual final outcome rather than the expected outcome. These two forms of trust are compatible and complimentary, and in Section 7 we show how to combine them into a single algorithm.\nUp to Section 7, we have only considered predicting a single final outcome in an episodic setting. In Section 8 we consider how to deal with two important generalizations of the problem setting: cumulative returns, and soft terminations. Cumulative returns allow us to see part of the return on each step, and allow us to start learning from these partial returns immediately in the online setting. Soft terminations allow us to learn about predictions that may conditionally terminate even if the actual process continues, and they allow for non-episodic predictions that may terminal softly on each step rather than completely at a single point in time. This leads to a single final algorithm that subsumes all previous algorithms as special cases. The algorithm is similar to the conventional TD(\u03bb) algorithm but with important differences that ensure that it is exactly equivalent to the desired, but\ninefficient, algorithm and therefore inherits all its desirable properties. Because our final algorithm is novel, it is appropriate to analyze it. In Section 9 we prove that the algorithm is convergent under typical mild conditions, and that it converges to the same solution as similar previous algorithms, including TD(\u03bb).\nWe conclude the paper with a short discussion in Section 10."}, {"heading": "3 Independence of span and the emergence of traces", "text": "We start with a supervised learning setting\u2014predicting the final numeric outcome of an episodic process. An episode of the process starts at time t = 0 and moves stochastically from state to state generating feature vectors \u03c6t until termination with a final numeric outcome Z at final time T . For example, Z could be the price of a particular stock that we want to predict, and each episode may be a year, such that time T corresponds to the end of the year.\nWe consider the general case of multi-step predictions (T > 1), where a prediction is made on each step. The standard supervised learning setting is a special case where in each episode we only make one prediction (such that, without loss of generality, we can take T = 1).\nOur predictions are linear, that is, the prediction at time t is the inner product of \u03c6t and a learned weight vector \u03b8, denoted \u03c6>t \u03b8. The algorithms are indifferent to the origins of the features, which may be handcrafted or learned.1 The weights have an initial value \u03b80 that is presumably due to previous episodes. We analyze how the weights change in a single episode (and thus we do not include the episode number in our notation).\nAt the final time, when Z is observed, we can update all the predictions towards the target as in the classical least mean squares (LMS) algorithm defined by the updates:\n\u03b8t+1 . = \u03b8t + \u03b1t\u03c6t ( Z \u2212 \u03c6>t \u03b8t ) , t = 0, . . . , T \u2212 1, (1)\nwhere \u03b1t > 0 is a step-size parameter that may vary from time step to time step (e.g., as a function of the state at that time). We call this a forward view, because to update the prediction at time t we need to look forward in time to the outcome Z which is observed at the later time T .\nTo perform the updates (1) we have to wait until Z is known and then do the update for all previous time steps t. This requires storing and then computing updates for all the preceding feature vectors. The required computational resources scale with the span of the prediction (the maximum length of an episode), which is what we wish to avoid. We seek incremental computations whose per-time-step complexity is O(n), where n is the number of parameters, and that result in the same weights as (1) at the end of the episode. That is, the incremental updates should compute the same \u03b8T as (1) if they are given the same input (the same \u03b80, the same sequence {\u03c6t}T\u22121t=0 , and the same Z).\nIt may seem that the best we can hope for is to approximate the result computed by the LMS algorithm, because of the strict computational restriction. Such a trade off between computation and accuracy is not uncommon. We will however now derive an algorithm\n1This includes, for instance, the case where \u03c6t is the last hidden layer of a neural network.\nthat finds the exact same final predictions with much more congenial computation, by carefully analyzing the total change to the weight vector due to the LMS algorithm.\nThe final step of the algorithm in (1) can be rewritten as \u03b8T = \u03b8T\u22121 + \u03b1T\u22121\u03c6T\u22121 ( Z \u2212 \u03c6>T\u22121\u03b8T\u22121 ) = \u03b8T\u22121 + \u03b1T\u22121\u03c6T\u22121Z \u2212 \u03b1T\u22121\u03c6T\u22121\u03c6>T\u22121\u03b8T\u22121 = ( I\u2212 \u03b1T\u22121\u03c6T\u22121\u03c6>T\u22121 ) \u03b8T\u22121 + \u03b1T\u22121\u03c6T\u22121Z\n= FT\u22121\u03b8T\u22121 + \u03b1T\u22121\u03c6T\u22121Z .\nHere Ft . = I \u2212 \u03b1t\u03c6t\u03c6>t is a fading matrix that will be important throughout this paper. Now, continuing,\n\u03b8T = FT\u22121 (FT\u22122\u03b8T\u22122 + \u03b1T\u22122\u03c6T\u22122Z) + \u03b1T\u22121\u03c6T\u22121Z (expanding \u03b8T\u22121)\n= FT\u22121FT\u22122\u03b8T\u22122 + (FT\u22121\u03b1T\u22122\u03c6T\u22122 + \u03b1T\u22121\u03c6T\u22121)Z (regrouping)\n= FT\u22121FT\u22122 (FT\u22123\u03b8T\u22123 + \u03b1T\u22123\u03c6T\u22123Z) + (FT\u22121\u03b1T\u22122\u03c6T\u22122 + \u03b1T\u22121\u03c6T\u22121)Z (recursing on \u03b8T\u22122)\n= FT\u22121FT\u22122FT\u22123\u03b8T\u22123 + (FT\u22121FT\u22122\u03b1T\u22123\u03c6T\u22123 + FT\u22121\u03b1T\u22122\u03c6T\u22122 + \u03b1T\u22121\u03c6T\u22121)Z (regrouping)\n... (recursing further)\n= FT\u22121FT\u22122 \u00b7 \u00b7 \u00b7F0\u03b80\ufe38 \ufe37\ufe37 \ufe38 . = aT\u22121 +\n( T\u22121\u2211 t=0 FT\u22121FT\u22122 \u00b7 \u00b7 \u00b7Ft+1\u03b1t\u03c6t ) \ufe38 \ufe37\ufe37 \ufe38\n. = eT\u22121\nZ\n= aT\u22121 + eT\u22121Z , (2)\nwhere at and et are two auxiliary memory vectors. Importantly, the auxiliary vectors can be updated without knowledge of Z, and with complexity independent of span and proportional to the number of features. The at vector stores the effect of the initial weights on the updated weights. It is initialized as a0 = \u03b80 and can then be updated efficiently with\nat . = FtFt\u22121 \u00b7 \u00b7 \u00b7F0\u03b80 = Ft (Ft\u22121 \u00b7 \u00b7 \u00b7F1\u03b81) = Ftat\u22121\n= at\u22121 \u2212 \u03b1t\u03c6t\u03c6>t at\u22121 = at\u22121 + \u03b1t\u03c6t(0\u2212 \u03c6>t at\u22121) , t = 1, . . . , T \u2212 1. (3)\nThe et vector is analogous to the conventional eligibility trace (see: Sutton 1988; Sutton and Barto 1998, and references therein) but has a special form as first proposed by van Seijen and Sutton (2014). It is initialized to e\u22121 = 0 (or, equivalently, to e0 = \u03b10\u03c60) and then updated according to\net . = t\u2211 k=0 FtFt\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6k\n= t\u22121\u2211 k=0 FtFt\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6k + \u03b1t\u03c6t\n= Ft t\u22121\u2211 k=0\nFt\u22121Ft\u22122 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6k\ufe38 \ufe37\ufe37 \ufe38 = et\u22121 + \u03b1t\u03c6t\n= Ftet\u22121 + \u03b1t\u03c6t (4) = et\u22121 \u2212 \u03b1t\u03c6t\u03c6>t et\u22121 + \u03b1t\u03c6t = et\u22121 + \u03b1t\u03c6t(1\u2212 \u03c6>t et\u22121), t = 0, . . . , T \u2212 1. (5)\nAn eligibility trace of this special form is called a dutch trace (van Hasselt, Mahmood, and Sutton 2014). For comparison, the conventional accumulating trace that it replaces can be written as e\u22121 . = 0 and et . = et\u22121 + \u03b1t\u03c6t. 2\nThe emergence of the dutch trace here is surprising and intriguing because, in contrast to previous work (van Seijen and Sutton 2014; van Hasselt, Mahmood, and Sutton 2014), the dutch trace has arisen in a setting without temporal-difference (TD) learning. Eligibility traces are not specific to TD learning at all; they are more fundamental than that. The need for eligibility traces seems to arise whenever one tries to learn long-term predictions in an efficient manner, that is, with computational complexity that is independent of predictive span.\nThe auxiliary vectors at and et are updated on each time step t < T and then, after observing Z at time T , are used to compute \u03b8T = aT\u22121 + eT\u22121Z, as in (2). This way we achieve exactly the same final result as the forward view (1), but with an algorithm whose time and memory complexity per step is uniformly O(n) and independent of span. The complete algorithm can be summarized as:\na0 . = \u03b80 , then at+1 . = at + \u03b1t\u03c6t(0\u2212 \u03c6>t at), t = 1, . . . , T \u2212 1 , e\u22121 . = 0 , then et . = et\u22121 + \u03b1t\u03c6t(1\u2212 \u03c6>t et\u22121), t = 0, . . . , T \u2212 1 , \u03b8T . = aT\u22121 + ZeT\u22121 .\n(6)\nThe vector aT\u22121 can be interpreted as storing the remaining effect of the initial weights \u03b80 after all updates in (1) have concluded. The trace eT\u22121 can be interpreted as storing all we need to know about the feature vectors that were observed during the episode. Together, these vectors allow us to replace all the T updates of the forward view with one fully equivalent update at the end of the episode.\nWe call the span-independent algorithm (6) the backward view corresponding to the forward view defined in (1), because on each step all updates only use information that is available at that time step: we only look backwards in time. The advantage of this is that the updates can be computed immediately and we do not have to wait and store observations until later. Until recently, exactly equivalences between forward and backward views were only known to exist for algorithms that update their predictions in batch (Sutton and Barto 1998). Van Seijen & Sutton (2014) were the first to derive an online backward\n2We incorporate the step size into both trace updates. This is a slight deviation from the way these traces are usually written to allow for time-changing step sizes and increased generality.\nview that was exactly equivalent to its forward view in terms of the learned predictions. The derivation in this section shows that such equivalences exist more generally, including for the LMS update in (1).\nWhen we consider the episode as a whole, there is no gain in total computation time for the span-independent algorithm (6) compared to the conventional algorithm (1): both algorithms use O(nT ) computation for the entire episode. However, in the span-independent algorithm the computation is spread out more evenly with a uniform per-step complexity of O(n), whereas the conventional algorithm performs the bulk of computation at the end, when we finally observe Z.\nAdditionally, there is a gain in terms of required memory. Algorithm (1) needs to store all previously observed features, leading to memory requirements of order O(nT ), whereas algorithm (6) only needs to store a and e and therefore has constant span-independent memory requirements of order O(n). In real-world problems, for instance in robotics, it is not uncommon to extract millions of features from the sensory inputs at each step (e.g., Montemerlo and Thrun 2003), where each step lasts only a fraction of a second. Consider a robot that generates one million features each 10 ms, where each feature is a real number represented with single precision using 4 bytes of memory. Figure 1 shows the resulting memory requirements for the conventional algorithm and the span-independent algorithm for different spans of the predictions. The required storage of the conventional algorithm ranges from 0.4 GB for predictions spanning one second to about 35 Terabytes for predictions spanning one day. While a few Gigabytes of on-board storage is feasible with today\u2019s resources, several Terabits will be a significant burden for an autonomous\nmobile robot. Concretely, this means that with the conventional algorithm we would have to restrict either the number of features, the frequency at which we make predictions, or the maximum amount of wall time a prediction can span. The span-independent algorithm scales much better. In the example in Figure 1 it only needs to store two vectors with 106 components of 4 bytes each, resulting in memory requirements that are constant at 8 Megabytes."}, {"heading": "4 Online updating and the emergence of TD errors", "text": "The algorithms in the previous section do not make any changes to the predictions during the episode; these are offline algorithms. Yet it would sometimes make sense to update the predictions during an episode, especially if the span is very long and we do not want to wait that long before we start learning. In this section we introduce an online forward view and derive a span-independent algorithm (the backward view) that on each time step computes the exact same predictions.\nAn online algorithm cannot update the predictions towards the final outcome Z during the episode, because Z is not yet available. Instead, if we only have observations up to a horizon h < T we may want to move the predictions for all earlier times t < h towards some informed guess of what the final outcome will be. Such a guess plays the role of a target for the updates, like Z in the forward view (1), but it is used prior to Z being available; it is an interim target. We use Zh to denote the interim target at time h, which may be based on all the data available up that horizon. The interim target might be from a human expert or it might be, as in TD learning, the current prediction corresponding to the feature vector \u03c6h. Interim targets at times closer to T might produce more accurate predictions. In the example of the stock market, as we get closer to the end of the year we may be able to more accurately estimate the final stock price. For now we consider the general case and do not specify the source of the interim targets Zh, for h = 1, . . . , T \u2212 1. Notationally it is convenient to define ZT . = Z. Note that the time index on Zh is a superscript rather than a subscript. Our convention is that the superscript position is reserved for the upper limit of the data considered available in an online update. The subscript position is used for the time step whose prediction is being modified.\nTo clarify the notation under online updating, we introduce the notation \u03b8ht for the weights at step t based on all the data up through time h. Using these double subscripts, what we previously called \u03b8t would now be \u03b8 T t , because these weight vectors depend on Z which is considered to arrive at time T . The complete set of online updates is then\n\u03b8h0 . = \u03b80, h = 0, . . . , T ; \u03b8ht+1 . = \u03b8ht + \u03b1t\u03c6t ( Zh \u2212 \u03c6>t \u03b8ht ) , t = 0, . . . , h\u2212 1 , h = 1, . . . , T ;\n= Ft\u03b8 h t + \u03b1t\u03c6tZ\nh. (7)\nThis online algorithm defines a set of h updates for each interim horizon h \u2208 {1, . . . , T}. For each horizon, all predictions are updated towards the latest, and presumably best available, interim target. Although most updates do not involve the final outcome Z, this algorithm is still considered a forward view, because the prediction at some time t (e.g., \u03c6>t \u03b8 h t ) is updated using interim targets that arrive later (e.g., Z h).\nWe can write out all the double-subscripted weight vectors in a triangle as\n\u03b800 \u03b810 \u03b8 1 1\n... ...\n. . .\n\u03b8h0 \u03b8 h 1 . . . \u03b8 h h \u03b8h+10 \u03b8 h+1 1 . . . \u03b8 h+1 h \u03b8 h+1 h+1\n... ...\n... ...\n. . .\n\u03b8T0 \u03b8 T 1 . . . \u03b8 T h \u03b8 T h+1 . . . \u03b8 T T .\n(8)\nThe computation proceeds from top to bottom, one row at a time from left to right. Each row starts with the same initial values \u03b8h0 = \u03b80 and then computes the sequence \u03b8 h 1 , \u03b8 h 2 and so on If we really computed all the different weight vectors in the triangle then the algorithm would be inefficient. With a computational complexity of O(nh) on each time h \u2208 {1, . . . , T}, the computation would not be constant per time step and would not be independent of the span of the prediction; the last row alone has complexity O(nT ) and it can only be computed after observing Z at time T . However, instead of computing the whole triangle, perhaps there is a way to incrementally compute just the diagonal, to somehow obtain \u03b8h+1h+1 from \u03b8 h h efficiently on each step. If this can be done, then the entire computation will be of uniform O(n) complexity per time step, independent of span. To find an efficient update along the diagonal of the triangle, notice first that the forward view (7) already provides a way to efficiently move right one step in the triangle. In other words, we can get to \u03b8h+1h+1 from \u03b8 h+1 h for any h with constant O(n) computation. If we can find an efficient way to step down in the triangle, that is to get to \u03b8h+1h from \u03b8 h h for any h with constant O(n) computation, then we can combine these two steps into a single O(n) update. To see if this is possible, we first write down explicitly how each weight vector in the triangle depends on the initial weights and the interim targets. Similar to our derivation of the final weights in (2) in the previous section, we can unroll the forward-view updates repeatedly starting from \u03b8ht and obtaining\n\u03b8ht = Ft\u22121\u03b8 h t\u22121 + \u03b1t\u22121\u03c6t\u22121Z h (applying (7) to \u03b8ht )\n= Ft\u22121(Ft\u22122\u03b8 h t\u22122 + \u03b1t\u22122\u03c6t\u22122Z h) + \u03b1t\u22121\u03c6t\u22121Z h (applying (7) to \u03b8ht\u22121) = Ft\u22121Ft\u22122\u03b8 h t\u22122 + (Ft\u22121\u03b1t\u22122\u03c6t\u22122 + \u03b1t\u22121\u03c6t\u22121)Z h (regrouping) = Ft\u22121Ft\u22122(Ft\u22123\u03b8 h t\u22123 + \u03b1t\u22123\u03c6t\u22123Z h) + (Ft\u22121\u03b1t\u22122\u03c6t\u22122 + \u03b1t\u22121\u03c6t\u22121)Z h\n(applying (7) to \u03b8ht\u22122)\n= Ft\u22121Ft\u22122Ft\u22123\u03b8 h t\u22123 + (Ft\u22121Ft\u22122\u03b1t\u22123\u03c6t\u22123 + Ft\u22121\u03b1t\u22122\u03c6t\u22122 + \u03b1t\u22121\u03c6t\u22121)Z h\n... (continuing until we reach \u03b8h0 )\n= Ft\u22121 \u00b7 \u00b7 \u00b7F0\u03b80\ufe38 \ufe37\ufe37 \ufe38 at\u22121 + t\u22121\u2211 j=0 Ft\u22121 \u00b7 \u00b7 \u00b7Fj+1\u03b1j\u03c6j  \ufe38 \ufe37\ufe37 \ufe38\net\u22121\nZh\n= at\u22121 + et\u22121Z h . (9)\nNotice that at\u22121 and et\u22121 depend only on time t and not on the data horizon h. We can use this result to find the difference between \u03b8h+1h and \u03b8 h h as\n\u03b8h+1h \u2212 \u03b8 h h = (ah\u22121 + eh\u22121Z h+1)\u2212 (ah\u22121 + eh\u22121Zh) = eh\u22121(Z h+1 \u2212 Zh) . (10)\nHere we see the emergence of a temporal difference error Zh+1 \u2212 Zh. We know from the previous section that eh\u22121 can be computed incrementally with (5) and therefore does not have to be recomputed from scratch for each new observation. Therefore, we now have an efficient way to compute \u03b8h+1h from \u03b8 h h for any h with constant O(n) computation per step that is independent of span. Now that we have \u03b8h+1h we can efficiently compute \u03b8 h+1 h+1 using (7), and we can merge these two steps to compute \u03b8h+1h+1 directly from \u03b8 h h. The complete update can then be written as\n\u03b8h+1h+1 = Fh\u03b8 h+1 h + \u03b1h\u03c6hZ h+1 (using (7)) = Fh ( \u03b8hh + eh\u22121(Z h+1 \u2212 Zh) ) + \u03b1h\u03c6hZ h+1 (using (10))\n= Fh\u03b8 h h + Fheh\u22121(Z h+1 \u2212 Zh) + \u03b1h\u03c6hZh+1 = Fh\u03b8 h h + (eh \u2212 \u03b1h\u03c6h)(Zh+1 \u2212 Zh) + \u03b1h\u03c6hZh+1 (using (4)) = Fh\u03b8 h h + eh(Z h+1 \u2212 Zh)\u2212 \u03b1h\u03c6h(Zh+1 \u2212 Zh) + \u03b1h\u03c6hZh+1 = Fh\u03b8 h h + eh(Z h+1 \u2212 Zh) + \u03b1h\u03c6hZh = (I\u2212 \u03b1h\u03c6h\u03c6>h )\u03b8hh + eh(Zh+1 \u2212 Zh) + \u03b1h\u03c6hZh (by definition of Fh) = \u03b8hh + eh(Z h+1 \u2212 Zh) + \u03b1h\u03c6h ( Zh \u2212 \u03c6>h \u03b8hh ) .\nThis update holds for all h \u2265 1. The update for \u03b811 is given directly by (7) as\n\u03b811 = \u03b8 1 0 + \u03b10\u03c60(Z 1 \u2212 \u03c6>0 \u03b810) .\nFor any Z0 we can rewrite this as\n\u03b811 = \u03b8 0 0 + \u03b10\u03c60(Z 1 \u2212 \u03c6>0 \u03b800) (using \u03b810 . = \u03b80 . = \u03b800)\n= \u03b800 + \u03b10\u03c60(Z 1 \u2212 Z0 + Z0 \u2212 \u03c6>0 \u03b800) = \u03b800 + \u03b10\u03c60(Z 1 \u2212 Z0) + \u03b10\u03c60(Z0 \u2212 \u03c6>0 \u03b800) = \u03b800 + e0(Z 1 \u2212 Z0) + \u03b10\u03c60(Z0 \u2212 \u03c6>0 \u03b800) , (using e0 . = \u03b10\u03c60)\nwhich means that then the update derived above for h \u2265 1 in fact also holds for h = 0. For concreteness, we will define Z0 . = 0, even though this value does not actually affect any of the weights. Now that we have an update that can compute \u03b8t+1t+1 from \u03b8 t t for any t, we can drop the redundant superscript. The resulting algorithm is\ne\u22121 . = 0, then et . = et\u22121 + \u03b1t\u03c6t(1\u2212 \u03c6>t et\u22121), t = 0, . . . , T \u2212 1, \u03b8t+1 . = \u03b8t + et ( Zt+1 \u2212 Zt ) + \u03b1t\u03c6t(Z t \u2212 \u03c6>t \u03b8t), t = 0, . . . , T \u2212 1. (11)\nBy construction this backward view is equivalent to the less efficient forward view (7) in the sense that \u03b8t = \u03b8 t t for all t. In contrast to the offline backward view (6) that we derived\nin the previous section, we no longer need to compute and store the auxiliary vector at. All relevant information that was contained therein is now stored directly in the online weights \u03b8t.\nAlthough the online backward view yields different predictions during the episode, the final weights \u03b8T are exactly equal to those computed by the conventional LMS algorithm (1) that constituted our first, offline, forward view. In terms of the triangle in (8), the online forward view (7) computes the whole triangle, the online backward view (11) efficiently computes only the diagonal, the offline forward view (1) computes only the last row, and the offline backward view (6) from the previous section computes only the final weights. All three algorithms ultimately result in the same final weights."}, {"heading": "5 Unifying online and offline learning and the emer-", "text": "gence of averaging\nThe online algorithms from the previous section do not quite subsume the offline algorithms from Section 3. Although they all reach the same weights by the end of the episode, during the episode their weights are different. The offline algorithm does not change the weights during the episode, and the online algorithm must change them.\nOne might think that the online algorithm is always better because it can immediately use any incoming relevant information, but it is not so. Suppose the interim targets are always wildly wrong (say due to a poor human \u2018expert\u2019). They would cause the weights of the online algorithm to also be wildly wrong for all steps except the last one at the end of the episode. In this case the weights of the online algorithm would be worse than those of the offline algorithm almost all of the time.\nBecause interim targets can sometimes be misleading, we might want to reduce their effect on some steps, based on how much we trust these targets. In this section \u03b8\u0303t denotes the online weights, which are computed by the span-independent backward view (11). The unified weights \u03b8t take into account the degree of trust for each interim target and are used to make our predictions. If we trust Zt fully, we want to obtain the same weights as in the online algorithm, so that \u03b8t = \u03b8\u0303t. If we do not trust Z\nt at all, we want the predictions to remain unchanged, so that \u03b8t = \u03b8t\u22121. For intermediate degrees of trust, \u03b2t \u2208 (0, 1), the algorithm should smoothly move from one extreme to the other, so the final result of the update should be something like\n\u03b8t+1 = (1\u2212 \u03b2t+1)\u03b8t + \u03b2t+1\u03b8\u0303t+1 . (12)\nThe above reasoning may sound plausible, but is it sound? In this section we construct a forward view for partially trusted interim targets, and then derive an equivalent spanindependent backward view. It turns out the resulting update indeed changes the weights precisely as in (12).\nIn the forward view, the online weights that always trust the latest interim target fully will be denoted \u03b8\u0303ht to differentiate them from the trusted interim weights \u03b8 h t . If we have data up to horizon h but we trust the latest interim target Zh only with degree \u03b2h \u2208 [0, 1], then the predictions prior to h should update towards Zh only to this degree and for the rest, with degree (1\u2212\u03b2h), fall back on earlier interim targets. Similarly we update towards\nZh\u22121 only as far as this interim target was trusted, so with total degree (1 \u2212 \u03b2h)\u03b2h\u22121, and then further fall back to Zh\u22122 with total degree (1 \u2212 \u03b2h)(1 \u2212 \u03b2h\u22121)\u03b2h\u22122, and so on. If we do not trust any of the interim targets observed between the time t of making the prediction and the current horizon h, the predictions should remain wherever they were at time t. This can be achieved by updating the prediction at time t towards the then-current prediction \u03c6>t \u03b8 t t with the remaining degree (1 \u2212 \u03b2h)(1 \u2212 \u03b2h\u22121) \u00b7 \u00b7 \u00b7 (1 \u2212 \u03b2t+1). Note that the different multipliers sum to one:\n\u03b2h + (1\u2212 \u03b2h)\u03b2h\u22121 + (1\u2212 \u03b2h)(1\u2212 \u03b2h\u22121)\u03b2h\u22122 + . . . + (1\u2212 \u03b2h) \u00b7 \u00b7 \u00b7 (1\u2212 \u03b2t+1) = 1 .\nThe total forward view for a prediction at time t with a horizon of h is therefore given by\n\u03b8ht+1 = \u03b8 h t + \u03b2h\u03b1t\u03c6t(Z h \u2212 \u03c6>t \u03b8ht ) + (1\u2212 \u03b2h)\u03b2h\u22121\u03b1t\u03c6t(Zh\u22121 \u2212 \u03c6>t \u03b8ht ) + (1\u2212 \u03b2h)(1\u2212 \u03b2h\u22121)\u03b2h\u22122\u03b1t\u03c6t(Zh\u22122 \u2212 \u03c6>t \u03b8ht ) ...\n+ (1\u2212 \u03b2h) \u00b7 \u00b7 \u00b7 (1\u2212 \u03b2t+2)\u03b2t+1\u03b1t\u03c6t(Zt+1 \u2212 \u03c6>t \u03b8ht ) + (1\u2212 \u03b2h) \u00b7 \u00b7 \u00b7 (1\u2212 \u03b2t+2)(1\u2212 \u03b2t+1)\u03b1t\u03c6t(\u03c6>t \u03b8tt \u2212 \u03c6>t \u03b8ht ) .\n(now grouping terms \u03b1t\u03c6t(\u00b7 \u2212 \u03c6>t \u03b8ht ) with total weight equal to one)\n= \u03b8ht + \u03b1t\u03c6t ( \u03b2hZ h+\n(1\u2212 \u03b2h)\u03b2h\u22121Zh\u22121+ (1\u2212 \u03b2h)(1\u2212 \u03b2h\u22121)\u03b2h\u22122Zh\u22122+\n...\n(1\u2212 \u03b2h) \u00b7 \u00b7 \u00b7 (1\u2212 \u03b2t+2)\u03b2t+1Zt+1+ (1\u2212 \u03b2h) \u00b7 \u00b7 \u00b7 (1\u2212 \u03b2t+2)(1\u2212 \u03b2t+1)\u03c6>t \u03b8tt \u2212 \u03c6>t \u03b8ht )\n= \u03b8ht + \u03b1t\u03c6t(Z h t \u2212 \u03c6>t \u03b8ht ) = Ft\u03b8 h t + \u03b1t\u03c6tZ h t , t = 0, . . . , h\u2212 1 ; h = 1, . . . , T, (13)\nwhere Zht . = \u03b2hZ h\n+ (1\u2212 \u03b2h)\u03b2h\u22121Zh\u22121 + (1\u2212 \u03b2h)(1\u2212 \u03b2h\u22121)\u03b2h\u22122Zh\u22122\n+ . . .\n+ (1\u2212 \u03b2h) \u00b7 \u00b7 \u00b7 (1\u2212 \u03b2t+2)\u03b2t+1Zt+1\n+ (1\u2212 \u03b2h) \u00b7 \u00b7 \u00b7 (1\u2212 \u03b2t+2)(1\u2212 \u03b2t+1)\u03c6>t \u03b8t = \u03b2hZ\nh + (1\u2212 \u03b2h)Zh\u22121t , t = 0, . . . , h\u2212 1 ; h = 1, . . . , T , and (14) Ztt . = \u03c6>t \u03b8 t t t = 0, . . . , T \u2212 1 . (15)\nTo derive a span-independent variant of algorithm (13), we first identify how a general weight vector \u03b8ht depends on the combined interim targets and the observed feature vectors by applying the recursive definition in (13) repeatedly, yielding\n\u03b8ht = Ft\u22121\u03b8 h t\u22121 + \u03b1t\u22121\u03c6t\u22121Z h t\u22121 (applying (13) to \u03b8 h t ) = Ft\u22121 ( Ft\u22122\u03b8 h t\u22122 + \u03b1t\u22122\u03c6t\u22122Z h t\u22122 ) + \u03b1t\u22121\u03c6t\u22121Z h t\u22121 (applying (13) to \u03b8 h t\u22121)\n= Ft\u22121Ft\u22122\u03b8 h t\u22122 + Ft\u22121\u03b1t\u22122\u03c6t\u22122Z h t\u22122 + \u03b1t\u22121\u03c6t\u22121Z h t\u22121 = Ft\u22121Ft\u22122(Ft\u22123\u03b8 h t\u22123 + \u03b1t\u22123\u03c6t\u22123Z h t\u22123) + Ft\u22121\u03b1t\u22122\u03c6t\u22122Z h t\u22122 + \u03b1t\u22121\u03c6t\u22121Z h t\u22121\n(applying (13) to \u03b8ht\u22122)\n= Ft\u22121Ft\u22122Ft\u22123\u03b8 h t\u22123 + Ft\u22121Ft\u22122\u03b1t\u22123\u03c6t\u22123Z h t\u22123 + Ft\u22121\u03b1t\u22122\u03c6t\u22122Z h t\u22122 + \u03b1t\u22121\u03c6t\u22121Z h t\u22121\n...\n= Ft\u22121 \u00b7 \u00b7 \u00b7F0\u03b8t0\ufe38 \ufe37\ufe37 \ufe38 . = at\u22121 +\nt\u22121\u2211 k=0 Ft\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6kZhk\n= at\u22121 + t\u22121\u2211 k=0 Ft\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6kZhk . (16)\nThe last step uses the fact that the initial trusted weights \u03b8t0 are equal to the initial online weights, such that \u03b8t0 = \u03b8\u0303 t 0 = \u03b80 for any t, which means at is the same as before. Notice that we have not yet used the definition of Zht in any way; the derivation so far holds for any combined target.\nWe now first examine if we can efficiently go down in the triangle, that is, to get to \u03b8h+1h from \u03b8 h h:\n\u03b8h+1h \u2212 \u03b8 h h = ( ah\u22121 + h\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6kZh+1k ) (\u03b8h+1h from (16))\n\u2212 ( ah\u22121 +\nh\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6kZhk ) (\u03b8hh from (16))\n= h\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6k ( Zh+1k \u2212 Z h k ) (merge sums, cancel ah\u22121)\n= h\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6k ( \u03b2h+1Z h+1 + (1\u2212 \u03b2h+1)Zhk \u2212 Zhk )\n(using (14) on Zh+1k )\n= h\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6k\u03b2h+1(Zh+1 \u2212 Zhk )\n= \u03b2h+1 ( h\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6k ) \ufe38 \ufe37\ufe37 \ufe38\n= eh\u22121\nZh+1 \u2212 \u03b2h+1 h\u22121\u2211 k=0\nFh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6kZhk\ufe38 \ufe37\ufe37 \ufe38 = \u03b8hh \u2212 ah\u22121, from (16)\n= \u03b2h+1eh\u22121Z h+1 \u2212 \u03b2h+1(\u03b8hh \u2212 ah\u22121) = \u03b2h+1 (ah\u22121 + eh\u22121Z h+1)\ufe38 \ufe37\ufe37 \ufe38\n= \u03b8\u0303h+1h , from (9)\n\u2212 \u03b2h+1\u03b8hh\n= \u03b2h+1(\u03b8\u0303 h+1 h \u2212 \u03b8 h h) (17)\nThus \u03b8h+1h can be written as a simple combination of the previous trusted weights \u03b8 h h and the interim online weights \u03b8\u0303h+1h with\n\u03b8h+1h = (1\u2212 \u03b2h+1)\u03b8 h h + \u03b2h+1\u03b8\u0303 h+1 h . (18)\nWe can plug this value into the definition of \u03b8h+1h+1 to find\n\u03b8h+1h+1 = Fh\u03b8 h+1 h + \u03b1hZ h+1 h \u03c6h (using (13))\n= Fh ( (1\u2212 \u03b2h+1)\u03b8hh + \u03b2h+1\u03b8\u0303h+1h ) + \u03b1hZ h+1 h \u03c6h (using (18))\n= Fh ( (1\u2212 \u03b2h+1)\u03b8hh + \u03b2h+1\u03b8\u0303h+1h ) + \u03b1h ( \u03b2h+1Z h+1 + (1\u2212 \u03b2h+1)Zh ) \u03c6h .\n(using (14))\n= Fh ( (1\u2212 \u03b2h+1)\u03b8hh + \u03b2h+1\u03b8\u0303h+1h ) + \u03b1h ( \u03b2h+1Z h+1 + (1\u2212 \u03b2h+1)\u03c6>h \u03b8hh ) \u03c6h .\n(using (15))\nNow we group the terms depending on whether they are trusted (multiplied with \u03b2h+1) or untrusted (multiplied with (1\u2212 \u03b2h+1)) to simply further to\n\u03b8h+1h+1 = (1\u2212 \u03b2h+1) ( Fh\u03b8 h h + \u03b1h\u03c6h\u03c6 > h \u03b8 h h ) \ufe38 \ufe37\ufe37 \ufe38 =\u03b8hh , using Fh=I\u2212\u03b1h\u03c6h\u03c6 > h + \u03b2h+1 ( Fh\u03b8\u0303 h+1 h + \u03b1hZ h+1\u03c6h ) \ufe38 \ufe37\ufe37 \ufe38\n= \u03b8\u0303h+1h+1, using (7)\n= (1\u2212 \u03b2h+1)\u03b8hh + \u03b2h+1\u03b8\u0303h+1h+1 .\nAll superscripts now match their corresponding subscripts and so we can write down an algorithm that is equivalent to the forward view in the sense that \u03b8tt = \u03b8t for all t, with\ne\u22121 . = 0, then et . = et\u22121 + \u03b1t\u03c6t(1\u2212 \u03c6>t et\u22121), t = 0, . . . , T \u2212 1, \u03b8\u0303t+1 . = \u03b8\u0303t + et ( Zt+1 \u2212 Zt ) + \u03b1t\u03c6t(Z\nt \u2212 \u03c6>t \u03b8\u0303t), t = 0, . . . , T \u2212 1, \u03b8t+1 . = \u03b8t + \u03b2t+1(\u03b8\u0303t+1 \u2212 \u03b8t), t = 0, . . . , T \u2212 1,\n(19)\nwhere \u03b2t \u2208 [0, 1] is the degree of trust we place in Zt. The first two lines compute the online weights, and are equal to the online backward view (11) from the previous section.\nThe last line effectively computes a weighted running average the online weights, according to the sequence {\u03b2t}Tt=1.\nAlgorithm (19) subsumes the previous algorithms. If \u03b2t = 1, \u2200t, then the predictions are equal those of the online algorithm on each step. If \u03b2t = 0 for t \u2208 {1, . . . , T \u2212 1} and \u03b2T = 1, then the predictions are equal those of the offline algorithm. As long as we trust the final outcome, such that \u03b2T = 1, then all these algorithms result in exactly the same weights at the end of the episode, and algorithm (19) allows us to be flexible about how much we change the predictions during the episode, without requiring us to commit to either fully online or fully offline updates for the whole episode."}, {"heading": "6 Bootstrapping", "text": "So far we have always fully trusted the actual final outcome. All interim targets have been deemed irrelevant by the end of the episode, leaving no effect on the computed final weights. There are cases in which we do not want to discard all interim targets. For instance, consider a stock that crashes down just before the end of the year. Certainly our updated predictions should include the possibility of such a crash, but we may not want to predict it will always crash just before the year\u2019s end. Similarly, suppose it rains on a certain date for which we want to predict the weather. It then seems wasteful to ignore the sunny weather on the days leading up to that date. These are examples of cases in which the interim targets are almost as informative as the final outcome. In some cases, an interim target may even be more informative. For instance, it may be due to a highly-trusted expert that takes into account all possible outcomes from that point in time. Surely, this expert should not be ignored completely in favor of one random final outcome.\nThe general idea of updating predictions using other predictions, such as the interim targets, is called bootstrapping (see, e.g., Sutton and Barto 1998). One way to obtain persistence of interim targets in our final predictions, and to achieve bootstrapping, is to drop the requirement in the previous section that we trust the final outcome fully, and allow \u03b2T < 1. The eventual target for our updates is then a weighted average of the interim targets and the final outcome. For instance, if \u03b2t = 1/2 for all t \u2208 {1, . . . , T} the final updates will place a weight of \u03b2T = 1/2 on the final outcome, a weight of (1\u2212 \u03b2T )\u03b2T\u22121 = 1/4 on the interim target immediate before then, and so on.\nThe notion of trust from the previous section applies a single degree of trust for an interim target uniformly to all prior predictions, but this is not always desirable. Using this definition of trust, if we fully trust an interim target then it replaces all earlier interim targets. However, even if an interim target if fully trusted for the most recent prediction, it may be inherently less trustworthy for earlier predictions. To illustrate this, consider flipping a coin three times and predicting the total number of heads. The possible final outcomes are 0, 1, 2, and 3 heads. If a trusted expert tells us before the first flip that the coin is fair, that is equivalent to observing a trustworthy interim target of Z1 = 1.5. Suppose then the first two flips both result in heads, such that the only remaining possible final outcomes are 2 and 3. If the coin is indeed fair, an interim target of 2.5 would now be trustworthy target for the prediction made after observing two heads. However, we would probably not want to replace the earlier interim target of 1.5 for the first prediction.\nUnfortunately, this is exactly what happens with the algorithm from the previous section. This suggests a different notion of trust, based on the degree of trust we place in an interim target as a stand-in for the expected final outcome rather than the actual (random) outcome. If an interim target precisely matches the expected outcome at that point in time, then later targets can then only be noisier or more specific but not more informative, and it should never be replaced by later targets.\nSuppose, concretely, that we fully trust Zt under this new notion of trust. Then, the update for the prediction made at time t\u22121 should disregard any targets that arrive later, including even the final outcome. Conversely, if we do not trust Zt at all, then it should leave no trace in the final updates. More generally we can update towards Zt with an intermediate degree of trust \u03b7t \u2208 [0, 1]. If the next interim target Zt+1 is trusted with degree \u03b7t+1, we then update our prediction at time t towards it with a total weight of (1 \u2212 \u03b7t)\u03b7t+1. The update towards Zt+2 will get a total weight of (1 \u2212 \u03b7t)(1 \u2212 \u03b7t+1)\u03b7t+2, and so on until we reach either the final outcome or the current data horizon. The latest interim target (and the final outcome) is always trusted fully until we move to the next time horizon. Therefore, at horizon h we always place any remaining weight on Zh and update towards it with total weight (1\u2212 \u03b7t)(1\u2212 \u03b7t+1) \u00b7 \u00b7 \u00b7 (1\u2212 \u03b7h\u22121).\nThe corresponding total update to the prediction at time t with a current horizon h is then\n\u03b8ht+1 = \u03b8 h t + \u03b7t+1\u03b1t\u03c6t(Z t+1 \u2212 \u03c6>t \u03b8ht ) + (1\u2212 \u03b7t+1)\u03b7t+2\u03b1t\u03c6t(Zt+2 \u2212 \u03c6>t \u03b8ht ) + (1\u2212 \u03b7t+1)(1\u2212 \u03b7t+2)\u03b7t+3\u03b1t\u03c6t(Zt+3 \u2212 \u03c6>t \u03b8ht ) + . . .\n+ (1\u2212 \u03b7t+1) \u00b7 \u00b7 \u00b7 (1\u2212 \u03b7h\u22122)\u03b7h\u22121\u03b1t\u03c6t(Zh\u22121 \u2212 \u03c6>t \u03b8ht ) + (1\u2212 \u03b7t+1) \u00b7 \u00b7 \u00b7 (1\u2212 \u03b7h\u22121)\u03b1t\u03c6t(Zh \u2212 \u03c6>t \u03b8ht )\n. = \u03b8ht + \u03b1t\u03c6t ( Zht \u2212 \u03c6>t \u03b8ht ) . (20)\nwhere we have grouped the updates into a single update towards a combined target Zht , just as in the previous section. This update is perhaps more familiar when we change the notation slightly. For all t, we define \u03bbt . = 1\u2212 \u03b7t such that \u03bbt essentially specifies to what degree we distrust Zt. The combined target is then defined as\nZht . = (1\u2212 \u03bbt+1)Zt+1\n+ \u03bbt+1(1\u2212 \u03bbt+2)Zt+2\n. . .\n+ \u03bbt+1 \u00b7 \u00b7 \u00b7\u03bbh\u22122(1\u2212 \u03bbh\u22121)Zh\u22121 + \u03bbt+1 \u00b7 \u00b7 \u00b7\u03bbh\u22121Zh . (21)\nThis target Zht is known as a \u03bb-return (Sutton and Barto 1998). The version that truncates at the current horizon h was first proposed by van Seijen and Sutton (2014). The total set of updates is\n\u03b8t0 . = \u03b80 , t = 0, . . . , T ;\n\u03b8ht+1 . = \u03b8ht + \u03b1t\u03c6t(Z h t \u2212 \u03c6>t \u03b8ht ) , (22)\n. = Ft\u03b8 h t + \u03b1t\u03c6tZ h t , t = 0, . . . , h\u2212 1 ; h = 1, . . . , T .\nIf we ultimately distrust all interim targets, then \u03bbt = 1 for all t and Z h t = Z h. The algorithm then reduces to the online algorithm from Section 4. Otherwise, at least some interim targets persist and contribute to the final weights. At the other extreme, if we trust all interim targets, then \u03bbt = 0 for all t and Z h t = Z\nt+1. Then, the updates reduce to single-step updates that only use the immediate next interim target. In that case each update depends only on the immediate next time step and we can drop the superscript h and the forward view (22) reduces to an efficient span-independent algorithm\n\u03b8t+1 . = \u03b8t + \u03b1t\u03c6t(Z t+1 \u2212 \u03c6>t \u03b8t) , t = 0, . . . , T .\nApart from this special case, the forward view (22) is computationally inefficient and we desire an efficient span-independent algorithm to get from \u03b8hh to \u03b8 h+1 h+1. In the previous section we derived how a weight vector depends on any sequence of combined targets Zht , independent on the definition of those targets. We repeat the result of that derivation, as first given in (16), here for clarity:\n\u03b8ht = at\u22121 + t\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6kZhk t = 0, . . . , h\u2212 1 ; h = 1, . . . , T .\nBecause this equation holds regardless of the definition of Zht , we can apply it to the current algorithm. In particular we use it to try to find an efficient algorithm to go from \u03b8hh to \u03b8 h+1 h . If this is possible, we can then use the update (22) to go from \u03b8 h+1 h to \u03b8 h+1 h+1. We start by writing out the difference as\n\u03b8h+1h \u2212 \u03b8 h h = h\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6kZh+1k \u2212 h\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1t\u03c6kZhk\n(ah\u22121 cancels)\n= h\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6k ( Zh+1k \u2212 Z h k ) . (23)\nThe combined targets Zh+1k and Z h k share many terms: going back to (21) we can see that all interim targets up to Zh\u22121 will have the exact same multipliers. These terms cancel, and the remaining difference is given by\nZh+1k \u2212 Z h k = \u03bbk+1 \u00b7 \u00b7 \u00b7\u03bbh\u22121(1\u2212 \u03bbh)Zh + \u03bbk+1 \u00b7 \u00b7 \u00b7\u03bbhZh+1\ufe38 \ufe37\ufe37 \ufe38\ndue to Zh+1k\n\u2212\u03bbk+1 \u00b7 \u00b7 \u00b7\u03bbh\u22121Zh\ufe38 \ufe37\ufe37 \ufe38 due to Zhk\n= \u03bbk+1 \u00b7 \u00b7 \u00b7\u03bbh ( Zh+1 \u2212 Zh ) . (24)\nWe can then continue from (23) with\n\u03b8h+1h \u2212 \u03b8 h h = h\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6k ( Zh+1k \u2212 Z h k )\n= h\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6k\u03bbk+1 \u00b7 \u00b7 \u00b7\u03bbh(Zh+1 \u2212 Zh) (using (24))\n= \u03bbh ( h\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03bbk+1 \u00b7 \u00b7 \u00b7\u03bbh\u22121\u03b1k\u03c6k ) \ufe38 \ufe37\ufe37 \ufe38\n. = eh\u22121\n(Zh+1 \u2212 Zh)\n= \u03bbheh\u22121(Z h+1 \u2212 Zh) . (25)\nWe again encounter the TD error Zh+1 \u2212 Zh and, more importantly, a new trace vector et that can be updated efficiently with\net . = t\u2211 k=0 Ft \u00b7 \u00b7 \u00b7Fk+1\u03bbk+1 \u00b7 \u00b7 \u00b7\u03bbt\u03b1k\u03c6k\n= t\u22121\u2211 k=0 Ft \u00b7 \u00b7 \u00b7Fk+1\u03bbk+1 \u00b7 \u00b7 \u00b7\u03bbt\u03b1k\u03c6k + \u03b1t\u03c6t\n= \u03bbtFt t\u22121\u2211 k=0\nFt\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03bbk+1 \u00b7 \u00b7 \u00b7\u03bbt\u22121\u03b1k\u03c6k\ufe38 \ufe37\ufe37 \ufe38 et\u22121 + \u03b1t\u03c6t\n= \u03bbtFtet\u22121 + \u03b1t\u03c6t (26) = \u03bbtet\u22121 + \u03b1t\u03c6t(1\u2212 \u03bbt\u03c6>t et\u22121) .\nThis trace is similar to the one we encountered before, but with the difference that the value of the vector decays towards zero by multiplication with \u03bbt on each step. Predictions made prior to a fully trusted interim target (for which \u03bbt = 0) will never be affected by later interim targets because the trace vector is set to zero. The extent to which the trace extends backward in time depends on the extent to which we have not yet trusted the corresponding interim targets.\nWe now combine the derived update from \u03b8hh to \u03b8 h+1 h with the update from \u03b8 h+1 h to\n\u03b8h+1h+1 to derive a single efficient update, given by\n\u03b8h+1h+1 = Fh\u03b8 h+1 h + \u03b1h\u03c6hZ h+1 (using (22)) = Fh ( \u03b8hh + \u03bbheh\u22121(Z h+1 \u2212 Zh) ) + \u03b1h\u03c6hZ h+1 (using (25))\n= Fh\u03b8 h h + \u03bbhFheh\u22121(Z h+1 \u2212 Zh) + \u03b1h\u03c6hZh+1 = Fh\u03b8 h h + (eh \u2212 \u03b1h\u03c6h) ( Zh+1 \u2212 Zh ) + \u03b1h\u03c6hZ h+1 (using \u03bbhFheh\u22121 = eh \u2212 \u03b1h\u03c6h, from (26)) = Fh\u03b8 h h + eh ( Zh+1 \u2212 Zh ) + \u03b1h\u03c6hZ h\n= (I\u2212 \u03b1h\u03c6h\u03c6>h )\u03b8hh + eh ( Zh+1 \u2212 Zh ) + \u03b1h\u03c6hZ h\n= \u03b8hh + eh ( Zh+1 \u2212 Zh ) + \u03b1h\u03c6h ( Zh \u2212 \u03c6>h \u03b8hh ) .\nThis concludes our derivation because the value of \u03b8h+1h+1 is now defined fully in terms of the previous weights on the diagonal \u03b8hh and other quantities that are either directly\navailable upon reaching our new data horizon h+ 1 or, in the case of the trace vector, can be computed with constant O(n) computation per step. The span-independent algorithm with persistent interim targets is\ne\u22121 . = 0, then et . = \u03bbtet\u22121 + \u03b1t\u03c6t(1\u2212 \u03bbt\u03c6>t et\u22121), t = 1, . . . , T\u22121, \u03b8t+1 . = \u03b8t + et ( Zt+1 \u2212 Zt ) + \u03b1t\u03c6t(Z t \u2212 \u03c6>t \u03b8t) t = 0, . . . , T\u22121. (27)\nCompared to the online backward view (11), the only difference is the appearance of \u03bbt in the update of the trace. If \u03bbt = 1 for all t, we regain the online backward view precisely, demonstrating that the new algorithm is strictly more general. In contrast to the averaging backward view (19), from the previous section, we see that for the notion of trust corresponding to \u03bb we do not need to maintain separate online and trusted weights. Instead, the degree of trust is used to scale the trace vector et down accordingly. If \u03bbt < 1 for any t \u2208 {1, . . . , T}, the corresponding interim targets have a lasting effect on the final weight vector and therefore for the first time we may obtain predictions that differ not just during the episode but also at its end."}, {"heading": "7 Combining two notions of trust and the emergence", "text": "of averaged TD(\u03bb)\nAlgorithm (27) is a strict generalization of the online algorithm (11), but it does not subsume the offline algorithm (6), or the averaging algorithm (19) that switches smoothly between online and offline updates. In this section, we combine the ideas from the last two sections to arrive at an algorithm that generalizes and subsumes all previous algorithms, thereby unifying all that came before into a single, general-purpose algorithm.\nAn offline version of the TD(\u03bb) algorithm can be obtained by using the online algorithm in (27) to update an online weight vector \u03b8\u0303t and then defining the trusted weight vector \u03b8t to remain equal to the initial weights until the last step, at which time we replace them with the online weights. An algorithm that switches smoothly between the offline and online cases can then be obtained similar to before, resulting in\ne\u22121 . = 0, then et . = \u03bbtet\u22121 + \u03b1t\u03c6t(1\u2212 \u03bbt\u03c6>t et\u22121), t = 0, . . . , T \u2212 1, \u03b8\u0303t+1 . = \u03b8\u0303t + et ( Zt+1 \u2212 Zt ) + \u03b1t\u03c6t(Z\nt \u2212 \u03c6>t \u03b8\u0303t), t = 0, . . . , T \u2212 1, \u03b8t+1 . = \u03b8t + \u03b2t+1(\u03b8\u0303t+1 \u2212 \u03b8t), t = 0, . . . , T \u2212 1.\n(28)\nThe first two lines are the online algorithm (27), from the previous section. The last line is equal to the last line in the unified algorithm without persistency of interim targets, as given in (19), but now using the online weights that use persistent interim targets weighted according to \u03bb-returns, as computed in the first two lines. When \u03bbt = 1 for all t we regain the averaging algorithm (19) without persistent interim targets. When \u03b2t = 1 for all t we regain the online algorithm (27) with persistent interim targets. So, we have again successfully unified all previously seemingly difference approaches to trust and have arrived at a single general algorithm that subsumes all that came before.\nThe merits of \u03bb-returns are well known (Sutton 1988; Sutton and Barto 1998) but the \u03b2-weighting of the online weights is novel to this paper, and it is appropriate to discuss it in a little more detail. So far we have considered only a single episode, but a major\npotential benefit of including \u03b2 appears when we consider multiple episodes. For clarity, consider the extreme case where all episodes last only a single step. Then \u03bb plays no role because there is no interim within each episode; there is only a beginning and an end. If we use m to enumerate episodes, such that Zm is the true outcome of episode m, then the updates for single-step episodes are\n\u03b8\u0303m+1 = \u03b8\u0303m + \u03b1m(Zm \u2212 \u03c6>m\u03b8\u0303m)\u03c6m , \u03b8m+1 = \u03b8m + \u03b2m+1(\u03b8\u0303m+1 \u2212 \u03b8m) ,\nwhere we used the fact that the final weights of episode n are the first weights of episode m + 1. Using \u03b2, we can do something here that we cannot do with \u03bb alone: we can weight the relative impact of different episodes. For instance, we can choose to keep the weights and the predictions stationary over multiple episodes, by setting \u03b2m = 0, to reduce the impact of the final outcomes observed in those episodes on our predictions. Another possibility is to decay the trust, for instance according to \u03b2m = 1 m , such that we trust the outcome of the first episode fully (\u03b21 = 1) and then reduce the trust for each subsequent episode. Such a choice of \u03b2 makes sense if we view the trust we place in outcomes as being relative to the trust we place in the predictions we already have. As our predictions improve over time, the outcomes become relatively less trustworthy. With this definition of trust, the trusted weights are the average of the weights of all previous episodes: \u03b8m = 1 m \u2211m i=0 \u03b8\u0303i. This specific algorithm is interesting because the predictions according to the averages \u03b8m are known to converge to the optimal predictions faster than the predictions according to any sequence of online weights \u03b8\u0303m (Polyak and Juditsky 1992; Bach and Moulines 2013). This shows that the notion of trust as provided by \u03b2 gives us something that cannot be obtained with \u03bb alone. To our knowledge, algorithm (28) is the first to generalize this idea of averaging online weights, in a principled fashion, to long-term predictions."}, {"heading": "8 Generalizing to cumulative returns and soft termi-", "text": "nations\nIn this section, we discuss how to extend our algorithms to handle soft terminations and cumulative returns. Both extensions generalize the episodic final-outcome setting considered above, and the algorithm we derive in this section will subsume all previous algorithms.\nOften, we want to predict the cumulation of a signal {Xt}Tt=0 rather than a single final outcome. In the episodic setting, with termination at time T , we then aim to predict\nZTt . = Xt+1 +Xt+2 + . . .+XT ,\nwhere, as before, t is the time of the prediction. In contrast to final outcomes, these cumulative outcomes depend on the time step of the prediction because at later time steps there will be less signal left to accumulate before the episode ends. We call this time-dependent outcome the cumulative return.\nWe may wish to update our predictions online, before observing the full cumulative return. To do so, we need to define interim targets to temporarily take the place of the\nactual return. An interim target Zht up to a horizon h < T should in any case include the part of the signal that was already observed. In addition, we introduce a residual prediction Ph that stands in for the unseen part of the signal, from horizon h to the end of the episode T . The full interim target at time t up to horizon h is then\nZht = Xt+1 +Xt+2 + . . .+Xh + Ph .\nThe residual prediction Ph may for instance be given by an external expert or by our own predictions at time h. For now we are agnostic to its origin and consider the general case. In any case, we define PT = 0 because at termination there is no remaining signal left to predict. If all signals except the one coinciding with termination are zero, we regain the final-outcome setting where the last signal XT = Z takes the role of the final outcome. If we further define the interim target at each horizon to be equal to the residual prediction at that horizon, such that Zh = Ph, we are back in the online final-outcome setting. Therefore, cumulative returns are strictly more general than final outcomes.\nSo far, we have considered episodic predictions where each episode ends with a single final outcome that we wish to predict. The algorithms extend naturally to multiple episodes by using the final weights of the episode as the initial weights of the next episode. However, some predictive questions do not fit nicely into this strictly episodic format because they are better thought of as terminating softly on each time step.\nA soft termination is a conceptual and potentially partial termination of the signal. Such a soft termination could represent a probability of termination, for instance when we want to take into account the probability of a robot breaking while learning in a simulation in which it never actually does. Or the soft termination could represent a desire to trade off the imminence and the magnitude of a signal, for instance when we do not just want more money rather than less, but we also want it sooner rather than later. In both cases the prediction is about a diminishing version of the \u2018raw\u2019 signal (e.g., money). Here, we are not concerned with the potential reasons for using soft terminations and consider the general case where the termination of the prediction can vary per time step and be anywhere between full continuation and full termination. Soft terminations allow us to ask more general predictive questions, and even to simultaneously consider multiple predictions that may resolve at different times.\nSoft terminations can be modeled by using a continuation parameter \u03b3t \u2208 [0, 1] to denote the amount of termination of our prediction upon reaching time t. This quantity is often called a discount factor, because it discounts the impact of later outcomes compared to earlier ones. If \u03b3t = 1, no termination happens at time t; if \u03b3t = 0, the prediction terminates fully, even if the trajectory may continue. We consider general sequences of \u03b3t and only require that eventually every prediction resolves completely, potentially asymptotically, such that \u220f\u221e i=t \u03b3i = 0 for all t. The episodic setting considered in the previous sections is a special case where \u03b3t = 0 only if t = T is the final step of the episode, and \u03b3t = 1 on all other steps.\nWe are now almost ready to formulate the target for a prediction about a discounted cumulative signal. In order to maintain full generality and compatibility with previous sections, we immediately include persistency of the residual predictions according to a sequence of {\u03bbt}\u221et=0 and trust of the resulting combined targets according to a sequence {\u03b2t}\u221et=0. For any horizon h > t, the combined interim target for the prediction at time\nt should include at least the immediate next signal Xt+1. Beyond this first signal we continue with \u03b3t+1 and observe the residual prediction Pt+1. We trust this prediction with degree 1 \u2212 \u03bbt+1 as a stand-in for the expected cumulative discounted return, and so its total multiplier is \u03b3t+1(1 \u2212 \u03bbt+1). If we trust this prediction fully, so that \u03bbt = 0, there is no need to continue further. Otherwise, we continue for as much as we have not yet terminated according to both \u03b3t+1 and \u03bbt+1, so that the next signal Xt+2 gets a total weight of \u03b3t+1\u03bbt+1. This line of reasoning then continues until we reach the horizon of our current data at time h, at which point we place any remaining trust on the most recent residual prediction Ph. All together, this gives us a combined target for the prediction at time t with data up to time h > t:\nZht . = Xt+1 + \u03b3t+1(1\u2212 \u03bbt+1)Pt+1\n+ \u03b3t+1\u03bbt+1(Xt+2 + \u03b3t+2(1\u2212 \u03bbt+2)Pt+2) + \u03b3t+1\u03bbt+1\u03b3t+2\u03bbt+2(Xt+3 + \u03b3t+3(1\u2212 \u03bbt+3)Pt+3) ...\n+ \u03b3t+1 \u00b7 \u00b7 \u00b7 \u03b3h\u22122\u03bbt+1 \u00b7 \u00b7 \u00b7\u03bbh\u22122(Xh\u22121 + \u03b3h\u22121(1\u2212 \u03bbh\u22121)Ph\u22121) + \u03b3t+1 \u00b7 \u00b7 \u00b7 \u03b3h\u22121\u03bbt+1 \u00b7 \u00b7 \u00b7\u03bbh\u22121(Xh + \u03b3hPh) . (29)\nThis can be written recursively as\nZht . = Xt+1 + \u03b3t+1 ( (1\u2212 \u03bbt+1)Pt+1 + \u03bbt+1Zht+1 ) , (30)\nand Ztt . = Pt , (31)\nEach of these interim targets is trusted according to the trust associated with its horizon, \u03b2h. Recall from Section 5 that this trust propagates backward. If we trust the last target Zht fully, there is no need to consider earlier targets. Otherwise, we multiply Z h t with its associated trust \u03b2h and continue to the previous target with degree (1 \u2212 \u03b2h). This then continues until either we find a target we fully trust, or we reach the then-current predictions \u03c6>t \u03b8 t t at time t, when we made the prediction we are currently updating. Our final interim targets are then given by\nZ\u0304ht . = \u03b2hZ h t\n+ (1\u2212 \u03b2h)\u03b2h\u22121Zh\u22121t + (1\u2212 \u03b2h)(1\u2212 \u03b2h\u22121)\u03b2h\u22122Zh\u22122t ...\n+ (1\u2212 \u03b2h) \u00b7 \u00b7 \u00b7 (1\u2212 \u03b2t+2)\u03b2t+1Zt+1t + (1\u2212 \u03b2h) \u00b7 \u00b7 \u00b7 (1\u2212 \u03b2t+1)\u03c6>t \u03b8tt\n= \u03b2hZ h t + (1\u2212 \u03b2h)Z\u0304h\u22121t (32)\nand Z\u0304tt . = \u03c6>t \u03b8 t t (33)\nwhere \u03b8tt are the trusted weights for time t that we will compute. The forward-viewing update is then given by\n\u03b8t0 . = \u03b80 t = 0, . . . , T\n\u03b8ht+1 . = \u03b8ht + \u03b1t\u03c6t(Z\u0304 h t \u2212 \u03c6>t \u03b8ht ) , t = 0, . . . h\u2212 1 ; h = 0, . . . , T , (34)\n. = Ft\u03b8 h t + \u03b1t\u03c6tZ\u0304 h t , t = 0, . . . h\u2212 1 ; h = 0, . . . , T .\nTo find an efficient backward view, again we can start by writing down explicitly the value of \u03b8ht . Following a derivation identical to the one for when we first consider trusted weights, as shown in (16) but with the complex target Z\u0304hk replacing Z h k , we obtain\n\u03b8ht = at\u22121 + t\u22121\u2211 k=0 Ft\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6kZ\u0304hk t = 0, . . . , h\u2212 1 ; h = 1, . . . , T . (35)\nAs before, we can apply this to both \u03b8h+1h and \u03b8 h h to find the difference\n\u03b8h+1h \u2212 \u03b8 h h = h\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6kZ\u0304h+1k \u2212 h\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6kZ\u0304hk\n(ah\u22121 cancels)\n= h\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6k ( Z\u0304h+1k \u2212 Z\u0304 h k )\n= h\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6k ( \u03b2h+1Z h+1 k + (1\u2212 \u03b2h+1)Z\u0304 h k \u2212 Z\u0304hk ) (from (32))\n= h\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6k\u03b2h+1 ( Zh+1k \u2212 Z\u0304 h k )\n= \u03b2h+1 h\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6kZh+1k \u2212 \u03b2h+1 h\u22121\u2211 k=0\nFh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6kZ\u0304hk\ufe38 \ufe37\ufe37 \ufe38 . = \u03b8hh \u2212 ah\u22121, from (35)\n= \u03b2h+1 ( ah\u22121 +\nh\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6kZh+1k ) \u2212 \u03b2h+1\u03b8hh . (36)\nThe first part of this result, within the brackets, looks familiar: notice the similarity to (35). The term can be interpreted as the intermediate result \u03b8\u0303h+1h of a forward view with targets Zht , as defined in (30), and updates defined by\n\u03b8\u0303t0 . = \u03b8\u03030 t = 0, . . . , T \u03b8\u0303ht+1 . = \u03b8\u0303ht + \u03b1t\u03c6t(Z h t \u2212 \u03c6>t \u03b8\u0303ht ) , t = 0, . . . h\u2212 1 ; h = 0, . . . , T .\n(37)\nThis is an online algorithm, comparable to the algorithm for \u03bb-returns derived before, but implicitly including cumulative returns and discounting through the definition of Zht . The complete forward view (34) can then be interpreted as switching smoothly between this online algorithm and an offline algorithm that in the extreme can delay updating\nthe predictions indefinitely. Analogous to the interim weights shown in (35), the interim weights of this online algorithm satisfy\n\u03b8\u0303ht = at\u22121 + t\u22121\u2211 k=0 Ft\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6kZhk t = 0, . . . , h ; h = 1, . . . , T . (38)\nWe can then continue our derivation from (36) with\n\u03b8h+1h \u2212 \u03b8 h h = \u03b2h+1 ( ah\u22121 + h\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6kZh+1k ) \ufe38 \ufe37\ufe37 \ufe38\n= \u03b8\u0303h+1h , using (38)\n\u2212 \u03b2h+1\u03b8hh\n= \u03b2h+1(\u03b8\u0303 h+1 h \u2212 \u03b8 h h) .\nThis implies that if we have \u03b8hh and \u03b8\u0303 h+1 h we can then compute \u03b8 h+1 h efficiently using\n\u03b8h+1h = (1\u2212 \u03b2h+1)\u03b8 h h + \u03b2h+1\u03b8\u0303 h+1 h . (39)\nFor now, we ignore the question of how to obtain \u03b8\u0303h+1h and first focus on how to go from \u03b8h+1h to \u03b8 h+1 h+1 and, resultingly, from \u03b8 h h to \u03b8 h+1 h+1. We can now derive\n\u03b8h+1h+1 = Fh\u03b8 h+1 h + \u03b1h\u03c6hZ\u0304 h+1 h (using (34))\n= Fh ( \u03b2h+1\u03b8\u0303 h+1 h + (1\u2212 \u03b2h+1)\u03b8 h h ) + \u03b1h\u03c6hZ\u0304 h+1 h (using (39))\n= Fh ( \u03b2h+1\u03b8\u0303 h+1 h + (1\u2212 \u03b2h+1)\u03b8 h h ) + \u03b1h\u03c6h ( \u03b2h+1Z h+1 h + (1\u2212 \u03b2h+1)Z\u0304 h h ) (using (32))\n= Fh ( \u03b2h+1\u03b8\u0303 h+1 h + (1\u2212 \u03b2h+1)\u03b8 h h ) + \u03b1h\u03c6h ( \u03b2h+1Z h+1 h + (1\u2212 \u03b2h+1)\u03c6 > h \u03b8 h h ) (using (33))\n= \u03b2h+1 ( Fh\u03b8\u0303 h+1 h + \u03b1h\u03c6hZ h+1 h ) \ufe38 \ufe37\ufe37 \ufe38\n. = \u03b8\u0303h+1h+1, using (37)\n+ (1\u2212 \u03b2h+1) ( Fh\u03b8 h h + \u03b1h\u03c6h\u03c6 > h \u03b8 h h ) \ufe38 \ufe37\ufe37 \ufe38\n= \u03b8hh (regrouping)\n= \u03b2h+1\u03b8\u0303 h+1 h+1 + (1\u2212 \u03b2h+1)\u03b8 h h .\nWe see that \u03b8\u0303h+1h is not needed and instead we can use \u03b8\u0303 h+1 h+1. Therefore, if \u03b8\u0303 h+1 h+1 can be computed with constant computation independent of span, then \u03b8h+1h+1 can be computed from \u03b8hh efficiently as well, using\n\u03b8h+1h+1 = (1\u2212 \u03b2h+1)\u03b8 h h + \u03b2h+1\u03b8\u0303 h+1 h+1 . (40)\nIt now remains to be shown that the online weights \u03b8\u0303h+1h+1 can be computed efficiently.\nWe start with the difference between \u03b8\u0303h+1h and \u03b8\u0303 h h for which we can use (38) to obtain\n\u03b8\u0303h+1h \u2212 \u03b8\u0303 h h = h\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6k ( Zh+1k \u2212 Z h k ) . (41)\nWe can find the difference Zh+1k \u2212Zhk from the definition in (29). All terms not involving Ph, Xh+1 or Ph+1 cancel, leaving us with\nZh+1k \u2212 Z h k = \u03b3k+1 \u00b7 \u00b7 \u00b7 \u03b3h\u03bbk+1 \u00b7 \u00b7 \u00b7\u03bbh\u22121 ((1\u2212 \u03bbh)Ph + \u03bbhXh+1 + \u03bbh\u03b3h+1Ph+1)\ufe38 \ufe37\ufe37 \ufe38\nfrom Zh+1k\n\u2212 \u03b3k+1 \u00b7 \u00b7 \u00b7 \u03b3h\u03bbk+1 \u00b7 \u00b7 \u00b7\u03bbh\u22121Ph\ufe38 \ufe37\ufe37 \ufe38 from Zhk\n= \u03b3k+1 \u00b7 \u00b7 \u00b7 \u03b3h\u03bbk+1 \u00b7 \u00b7 \u00b7\u03bbh\u22121 (\u2212\u03bbhPh + \u03bbhXh+1 + \u03bbh\u03b3h+1Ph+1) = \u03b3k+1 \u00b7 \u00b7 \u00b7 \u03b3h\u03bbk+1 \u00b7 \u00b7 \u00b7\u03bbh (Xh+1 + \u03b3h+1Ph+1 \u2212 Ph) .\nHere we see the emergence of a general form of the classical temporal-difference error for cumulative discounted returns: \u03b4h . = Xh+1 + \u03b3h+1Ph+1 \u2212 Ph.3 Using this, we can now continue from (41) with\n\u03b8\u0303h+1h \u2212 \u03b8\u0303 h h = h\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6k ( Zh+1k \u2212 Z h k ) =\nh\u22121\u2211 k=0 Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6k\u03b3k+1 \u00b7 \u00b7 \u00b7 \u03b3h\u03bbk+1 \u00b7 \u00b7 \u00b7\u03bbh\u03b4h\n= \u03b3h\u03bbh ( h\u22121\u2211 k=0 \u03b3k+1 \u00b7 \u00b7 \u00b7 \u03b3h\u22121\u03bbk+1 \u00b7 \u00b7 \u00b7\u03bbh\u22121Fh\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6k ) \ufe38 \ufe37\ufe37 \ufe38\n. = eh\u22121\n\u03b4h\n= \u03b3h\u03bbheh\u22121\u03b4h , (42)\nwhere, similar to before, the trace vector et can be updated recursively according to\net . = t\u2211 k=0 \u03b3k+1 \u00b7 \u00b7 \u00b7 \u03b3t\u03bbk+1 \u00b7 \u00b7 \u00b7\u03bbtFt \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6k\n= t\u22121\u2211 k=0 \u03b3k+1 \u00b7 \u00b7 \u00b7 \u03b3t\u03bbk+1 \u00b7 \u00b7 \u00b7\u03bbtFt \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6k + \u03b1t\u03c6t\n3If we use our current predictions as interim targets, the TD error is \u03b4t = Xt+1+\u03b3t+1\u03c6>t+1\u03b8t\u2212\u03c6>t \u03b8t\u22121. This TD error uses the weights at two consecutive time steps and is therefore slightlt different from the classic TD error defined, using only the current weights \u03b8t, as \u03b4t = Xt+1 + \u03b3t+1\u03c6>t+1\u03b8t \u2212 \u03c6>t \u03b8t. The difference is important to achieve exact equivalence, although it is also possible to rewrite the new algorithms to use a more standard TD error.\n= \u03b3t\u03bbtFt t\u22121\u2211 k=0 \u03b3k+1 \u00b7 \u00b7 \u00b7 \u03b3t\u22121\u03bbk+1 \u00b7 \u00b7 \u00b7\u03bbt\u22121Ft\u22121 \u00b7 \u00b7 \u00b7Fk+1\u03b1k\u03c6k + \u03b1t\u03c6t\n= \u03b3t\u03bbtFtet\u22121 + \u03b1t\u03c6t (43) = \u03b3t\u03bbtet\u22121 + \u03b1t\u03c6t(1\u2212 \u03b3t\u03bbt\u03c6>t et\u22121) .\nThe trace now decays according to both \u03bb and \u03b3. Using this trace vector, we can compute \u03b8\u0303h+1h efficiently from \u03b8\u0303 h h using (42). We now combine this with the final step to \u03b8\u0303 h+1 h+1 from \u03b8\u0303h+1h and derive\n\u03b8\u0303h+1h+1 = Fh\u03b8\u0303 h+1 h + \u03b1h\u03c6hZ h+1 h (using (37))\n= Fh\u03b8\u0303 h+1 h + \u03b1h\u03c6h (Xh+1 + \u03b3h+1Ph+1) (using (30), (31))\n= Fh ( \u03b8\u0303hh + \u03b3h\u03bbheh\u22121 (Xh+1 + \u03b3h+1Ph+1 \u2212 Ph) ) + \u03b1h\u03c6h (Xh+1 + \u03b3h+1Ph+1)\n(using (42))\n= Fh\u03b8\u0303 h h + \u03b3h\u03bbhFheh\u22121 (Xh+1 + \u03b3h+1Ph+1 \u2212 Ph) + \u03b1h\u03c6h (Xh+1 + \u03b3h+1Ph+1) = Fh\u03b8\u0303 h h + (eh \u2212 \u03b1h\u03c6h) (Xh+1 + \u03b3h+1Ph+1 \u2212 Ph) + \u03b1h\u03c6h (Xh+1 + \u03b3h+1Ph+1) (using \u03b3h\u03bbhFheh\u22121 = eh \u2212 \u03b1h\u03c6h, from (43)) = Fh\u03b8\u0303 h h + eh (Xh+1 + \u03b3h+1Ph+1 \u2212 Ph) + \u03b1h\u03c6hPh = (I\u2212 \u03b1h\u03c6h\u03c6>h )\u03b8\u0303hh + eh (Xh+1 + \u03b3h+1Ph+1 \u2212 Ph) + \u03b1h\u03c6hPh\n= \u03b8\u0303hh + eh (Xh+1 + \u03b3h+1Ph+1 \u2212 Ph) + \u03b1h\u03c6h ( Ph \u2212 \u03c6>h \u03b8\u0303hh ) = \u03b8\u0303hh + eh\u03b4h + \u03b1h\u03c6h ( Ph \u2212 \u03c6>h \u03b8\u0303hh ) .\nAll weights again have matching sub- and superscripts and an equivalent TD algorithm, in the sense that \u03b8\u0303t = \u03b8\u0303 t t and \u03b8t = \u03b8 t t, for the fully general case including cumulative discounted returns is given by\ne\u22121 . = 0, then et . = \u03b3t\u03bbtet\u22121 + \u03b1t\u03c6t(1\u2212 \u03b3t\u03bbt\u03c6>t et\u22121), t = 0, . . . , T \u2212 1, \u03b8\u0303t+1 . = \u03b8\u0303t + et\u03b4t + \u03b1t\u03c6t(Pt \u2212 \u03c6>t \u03b8\u0303t), t = 0, . . . , T \u2212 1, \u03b8t+1 . = \u03b8t + \u03b2t+1(\u03b8\u0303t+1 \u2212 \u03b8t), t = 0, . . . , T \u2212 1.\n(44)\nThe first two lines constitute the online algorithm we just derived. The last line is from (40) and extends this algorithm to include smooth switching between offline and online updates. If \u03b2t = 1 for all t, the algorithm reduces to a variant of TD(\u03bb) known as true online TD(\u03bb) (van Seijen and Sutton 2014), but extended to include general, potentially non-constant, sequences of {\u03b1t}, {\u03b3t} and {\u03bbt}. The extension to averaging according to \u03b2t is new to this paper.\nSoft termination generalizes the episodic setting we considered previously. This means that as far as the learning update is concerned, we do not have to treat steps on which the process actually terminates and restarts for a new episode in any special way. To see how this works, we first renumber the time steps on consecutive episodes: if the first episode ends at time T , the initial time step of the second episode will be taken to be T rather than 0. If the second episode lasts T \u2032 steps, the third episode is then taken to\nbegin on T + T \u2032, and so on. Together with the requirement that \u03b3T = 0 on every actual termination, this is sufficient to get updates that are completely equivalent to treating the subsequent episodes completely separately. Notice that the update on termination at some time T , and resulting in \u03b8T , uses the residual prediction on termination PT only when multiplied with \u03b3T . Previously we required that PT = 0 because there is no further signal to predict. This is now no longer necessary, because \u03b3T = 0 already fulfills the requirement that \u03b3TPT = 0. If for instance we use our current predictions, such that Pt+1 . = \u03c6>t+1\u03b8t for all t, we can simply keep the update as is even though PT will then be the prediction for the cumulative return of the next episode, because \u03c6T is now defined to be its first feature vector. Therefore, both hard and soft termination can be handled seamlessly using algorithm (44), and that will be our final, general algorithm."}, {"heading": "9 Convergence Analysis", "text": "The algorithm (44) differs from related earlier algorithms such as TD(\u03bb) in a few subtle but important ways. The most notable differences are the updates to the traces and the averaging due to \u03b2t. Known results on convergence therefore do not automatically transfer to this new algorithm and it is appropriate to take a moment to analyze it.\nThe convergence of the trusted weights \u03b8 depends on the convergence of the online weights \u03b8\u0303 and so we must investigate these jointly. The online weights, in turn, depend on the sequences of parameters and residual predictions that are supplied. We want our analysis to be general, which means we want to be able to handle general sequences of discounts {\u03b3t}\u221et=1, persistency parameters, {\u03bbt}\u221et=1, and residual predictions {Pt}\u221et=1. Naturally, if any of these can change completely arbitrarily, we can have no hope of converging to any predeterminable solution. Therefore, as in Sutton et al. (2014), we allow the features, discounts and persistency parameters to be stationary functions of an underlying unobserved state, such that \u03c6t . = \u03c6(St), \u03b3t . = \u03b3(St) and \u03bbt . = \u03bb(St) for some fixed functions \u03c6 : S \u2192 Rn, \u03b3 : S \u2192 [0, 1] and \u03bb : S \u2192 [0, 1], where S is a state space and St \u2208 S is the, unobserved, state of the world at time t. We assume there is a steady-state distribution over these states such that all expectations used below are well-defined with respect to a distribution over states defined by limt\u2192\u221e Pr(St = s). This setting generalizes the more standard approach where \u03b3t = \u03b3 and \u03bbt = \u03bb are constants, because now these parameters can still change over time, but it avoids the possibility of arbitrary non-stationarity that would ruin convergence.\nWe first consider convergence when the residual predictions are also due to a fixed function of state, for instance because they are due to otherwise stationary experts or oracles. Theorem 1. Let Xt . = X(St), \u03c6t . = \u03c6(St), \u03b3t . = \u03b3(St), \u03bbt . = \u03bb(St) and Pt . = P (St) all be fixed functions of (unobserved) states St \u2208 S, with a stable steady-state distribution d. Then, if \u2211\u221e t=0 \u03b1t = \u2211\u221e t=0 \u03b2t = \u221e, and \u2211\u221e t=0 \u03b1 2 t < \u221e, algorithm (44) converges almost surely to the fixed-point solution\n\u03b8\u2217 . = E[\u03c6t\u03c6t]\u22121 E[Z\u221et \u03c6t] .\nProof. We start by analyzing the online weights \u03b8\u0303t. Because of the equivalence of the forward and backward views, we can investigate the forward view, which is easier to\nanalyze. In other words, instead of investigating limt\u2192\u221e \u03b8\u0303t as updated through (44), we investigate limt\u2192\u221e \u03b8\u0303 t t as updated through (37). By construction, the end result is exactly the same. The asymptotic forward view as the horizon goes to infinity is\n\u03b8\u0303\u221et+1 = \u03b8\u0303 \u221e t + \u03b1t\u03c6t(Z \u221e t \u2212 \u03c6>t \u03b8\u0303\u221et ) , t = 0, . . . ,\nwhere Z\u221et . = lim h\u2192\u221e Zht , t = 0, . . . .\nBecause Zht does not depend on the weights, this is a standard stochastic gradient-descent update \u03b8\u0303t+1 = \u03b8\u0303t \u2212 \u03b1t\u2207\u03b8\u0303l(\u03b8\u0303)|\u03b8\u0303t on the quadratic loss function\nl(\u03b8\u0303) . = E [ (Z\u221et \u2212 \u03c6>t \u03b8\u0303)2 ] .\nIf the step sizes are suitably chosen, for instance such that \u2211\u221e t=0 \u03b1t =\u221e and \u2211\u221e t=0 \u03b1 2 t <\u221e (Robbins and Monro 1951), and if the means and variances of Z\u221et and \u03c6t are well-defined and bounded for all t, this update converges to the fixed-point solution \u03b8\u2217 that minimizes the quadratic loss (cf. Kushner and Yin 2003), such that\nlim t\u2192\u221e\n\u03b8\u0303t = \u03b8\u2217 . = E [ \u03c6t\u03c6 > t ]\u22121 E[\u03c6tZ\u221et ] . It is straightforward to see that \u03b8t will have the same limit; it suffices to have \u2211\u221e t=0 \u03b2t = \u221e.\nAlthough convergence is already guaranteed when \u03b2t = 1 for all t, recent work has shown that for similar stochastic gradient algorithms the optimal rate of convergence is attained if \u03b2t decreases much faster than \u03b1t, specifically when \u03b2t = O(t\n\u22121) while \u03b1t = \u03b1 for some constant \u03b1 (Bach and Moulines 2013). More generally, it seems likely that convergence also holds if \u2211\u221e t=0 \u03b2 2 t <\u221e and \u2211\u221e t=0 \u03b1 2 t =\u221e. The observation that \u03b2t should perhaps decrease over time for faster learning may seem at odds with our introduction of this parameter as a degree of trust. However, these two views are quite compatible if we consider \u03b2t to be the degree of trust we place in the online updates relative to the trust we place in our current predictions due to the trusted weights. When the trust in the predictions increases over time, the relative trust in the inherently noisy online targets should then decrease.\nAlthough Theorem 1 is already fairly general, it does not cover the important case when the residual predictions additionally depend on the weights we are updating. It makes sense to use the predictions we trust most and therefore we now consider what happens when Pt . = \u03c6>t \u03b8t\u22121. Notice that we have to use \u03b8t\u22121 rather than \u03b8t, because Pt is used in the computation of \u03b8t and so the latter is not yet available when we compute Pt. The analysis of this case is more complex than the previous one, because Pt is no longer a constant function of state. This means the update is no longer a standard gradient-descent update on a quadratic loss, because the target Zht for the online weights itself depends on the trusted weights that we are simultaneously updating.\nThe results by Bach and Moulines (2013) on stochastic gradient descent indicate that perhaps the most interesting case is where \u03b2t decreases faster than \u03b1t, such that limt\u2192\u221e \u03b2t/\u03b1t = 0. This suggests an analysis on two time scales is appropriate.\nTheorem 2. Let Xt . = X(St), \u03c6t . = \u03c6(St), \u03b3t . = \u03b3(St) and \u03bbt . = \u03bb(St) all be fixed bounded functions of (unobserved) states St \u2208 S, with a stable steady-state distribution d. Define Pt . = \u03c6>t \u03b8t\u22121. Then, if \u2211\u221e t=0 \u03b1t = \u2211\u221e t=0 \u03b2t = \u221e, \u2211\u221e t=0 \u03b1 2 t < \u221e, and limt\u2192\u221e \u03b2t \u03b1t\n= 0, algorithm (44) converges almost surely to the TD fixed-point solution \u03b8\u2217 that minimizes the mean-squared projected Bellman error (Sutton, Szepesva\u0301ri, and Maei 2008; Sutton et al. 2009), such that\nE [ (Zt(\u03b8\u2217)\u2212 \u03c6>t \u03b8\u2217)\u03c6>t ] E[\u03c6t\u03c6t]\u22121 E [ \u03c6t(Zt(\u03b8\u2217)\u2212 \u03c6>t \u03b8\u2217) ] = 0 , (45)\nwhere Zt(\u03b8) . = Xt+1 + \u03b3t+1(1\u2212 \u03bbt+1)\u03c6>t \u03b8 + Zt(\u03b8) , \u2200\u03b8 .\nProof. In two-time-scale analyses, we are allowed to analyze the faster updates as if the slower updates have stopped. This means that in analyzing the updates to the online weights \u03b8\u0303t, we can assume the trusted weights \u03b8t are constant to analyze where \u03b8\u0303t converges towards as a function of the stationary \u03b8t. On the other hand, when we analyze the slower updates to the trusted weights \u03b8t we are allowed to assume the faster updates to the online weights converge completely between each two steps. For more detail on analyzing stochastic approximations on two times scales, we refer to Borkar (1997), Borkar (2008), Kushner and Yin (2003), and Konda and Tsitsiklis (2004).\nWe first analyze the convergence of the faster updates to the online weights, where we can assume that the trusted weights are stationary at some value \u03b8. Then, using Pt . = \u03c6>t \u03b8, the targets for the updates of the forward view are\nZtt (\u03b8) = \u03c6 > t \u03b8 , Zht (\u03b8) = Xt+1 + \u03b3t+1(1\u2212 \u03bbt+1)\u03c6>t \u03b8 + \u03b3t+1\u03bbt+1Zht+1(\u03b8) ,\nwhere we have extended the notation slightly to make the dependence of Zht (\u03b8) on \u03b8 explicit. Notice that the residual predictions on each time step depend on the same stationary trusted weights \u03b8. Because of the assumed stationarity of \u03b8, the updates to the weights \u03b8\u0303ht of the forward view can again be considered standard stochastic gradient updates and therefore these weights converge towards the fixed point \u03b8\u0303\u2217(\u03b8), where again we make the dependence on \u03b8 explicit, defined by\n\u03b8\u0303\u2217(\u03b8) = E [ \u03c6t\u03c6 > t ]\u22121 E[\u03c6tZ\u221et (\u03b8)] , where Z\u221et (\u03b8) = limh\u2192\u221e Z h t (\u03b8) denotes the limit of the target of the update as the horizon grows to infinity. In an episodic setting, Z\u221et = Z T t for all t < T , where T denotes the first\ntermination after time k. More in general, Z\u221et is always well-defined because we require\u220f\u221e t=0 \u03b3t = 0. For the analysis of the slower updates to \u03b8t, we can now assume the faster time scale has already converged to its fixed point \u03b8\u0303\u2217(\u03b8t) for the current weights. Therefore, we analyze the update\n\u03b8t+1 = \u03b8t + \u03b2t+1(\u03b8\u0303\u2217(\u03b8t)\u2212 \u03b8t) = \u03b8t + \u03b2t+1(E [ \u03c6k\u03c6 > k ]\u22121 E[\u03c6kZ\u221ek (\u03b8t)]\u2212 \u03b8t) .\nThis is a stochastic-approximation update that, under the conditions that \u2211\u221e t=0 \u03b2t = \u221e\nand \u2211\u221e t=0 \u03b2 2 t <\u221e, converges almost surely to the fixed point \u03b8\u2217 that satisfies\n\u03b8\u2217 = E [ \u03c6t\u03c6 > t ]\u22121 E[\u03c6tZ\u221et (\u03b8\u2217)] . If we multiply both sides with E [ \u03c6t\u03c6 > t ] , this implies that E [ \u03c6t\u03c6 > t \u03b8\u2217 ] = E[\u03c6tZ\u221et (\u03b8\u2217)] and\ntherefore, by moving both terms to the same side and then multiplying with E [ \u03c6t\u03c6 > t ]\u22121 ,\nE [ \u03c6t\u03c6 > t ]\u22121 E[\u03c6t(Z\u221et (\u03b8\u2217)\u2212 \u03c6>t \u03b8\u2217)] = 0 . It follows immediately that \u03b8\u2217 minimizes the mean-squared projected Bellman error completely to zero, as desired."}, {"heading": "10 Discussion", "text": "In this paper, we have considered how to answer predictive questions with algorithms that use constant computation per time step that is proportional to the number of learned weights, and that is independent of the span of the prediction. We considered both final and cumulative outcomes, under online and offline updating, with and without persistency of the residual predictions we encounter during an episode, and with hard and soft termination. In the end, we obtained a single general algorithm that can be used for all these different predictive questions, which is shown in (44). This algorithm is guaranteed to be convergent under typical, fairly mild, technical conditions.\nSome extensions remain for future work. In particular, we have not considered how different policies of behavior can influence our predictions, and as a result have not talked about the problem of control in which the goal is to find the optimal policy for a given (reward) signal. Our analysis already extends naturally to the prediction of action values, from which control policies can be easily distilled. Then, using a form of policy iteration (Bellman 1957; Howard 1960), we can repeatedly switch between estimating and improving the policy to tackle the problem of optimal control. However, to properly and fully include adaptable policies, we would in addition need to carefully consider the problem of learning off-policy, about action-selection policies that differ from the one used to generate the data (Sutton and Barto 1998). This is consistent but orthogonal to the ideas outlined in this paper, and such off-policy predictions (including those about the greedy and, ultimately, optimal policy) are learnable through a proper use of rejection sampling, as in Q-learning, or importance sampling (Precup, Sutton, and Singh 2000; Precup and Sutton 2001; Maei 2011; Sutton et al. 2014; van Hasselt, Mahmood, and Sutton 2014; Mahmood, van Hasselt, and Sutton 2014).\nAll algorithms considered in this paper are in a sense descendent from a linear stochastic gradient, or LMS, update. The main idea of span-independent computation is more general and can be applied quite naturally to other settings, including for instance non-linear functions such as deep neural networks (LeCun, Bengio, and Hinton 2015; Mnih et al. 2015) or to quadratic-time linear-function algorithms as in LSTD (Bradtke and Barto 1996). Not all updates may have fully equivalent span-independent counterparts, but even then it may be more important to be independent of span than to be exactly equivalent."}], "references": [{"title": "Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n)", "author": ["F. Bach", "E. Moulines"], "venue": "Advances in Neural Information Processing Systems 26, pp. 773\u2013781.", "citeRegEx": "Bach and Moulines,? 2013", "shortCiteRegEx": "Bach and Moulines", "year": 2013}, {"title": "Dynamic Programming", "author": ["R. Bellman"], "venue": "Princeton University Press.", "citeRegEx": "Bellman,? 1957", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Stochastic approximation with two time scales", "author": ["V.S. Borkar"], "venue": "Systems & Control Letters 29(5), pp. 291\u2013294.", "citeRegEx": "Borkar,? 1997", "shortCiteRegEx": "Borkar", "year": 1997}, {"title": "Stochastic approximation", "author": ["V.S. Borkar"], "venue": "Cambridge Books.", "citeRegEx": "Borkar,? 2008", "shortCiteRegEx": "Borkar", "year": 2008}, {"title": "Linear least-squares algorithms for temporal difference learning", "author": ["S.J. Bradtke", "A.G. Barto"], "venue": "Machine Learning 22, pp. 33\u201357.", "citeRegEx": "Bradtke and Barto,? 1996", "shortCiteRegEx": "Bradtke and Barto", "year": 1996}, {"title": "Dynamic programming and Markov processes", "author": ["R.A. Howard"], "venue": "MIT Press.", "citeRegEx": "Howard,? 1960", "shortCiteRegEx": "Howard", "year": 1960}, {"title": "Convergence rate of linear two-time-scale stochastic approximation", "author": ["V.R. Konda", "J.N. Tsitsiklis"], "venue": "Annals of applied probability 14(2), pp. 796\u2013819.", "citeRegEx": "Konda and Tsitsiklis,? 2004", "shortCiteRegEx": "Konda and Tsitsiklis", "year": 2004}, {"title": "Stochastic approximation and recursive algorithms and applications", "author": ["H.J. Kushner", "G. Yin"], "venue": "Vol. 35. Springer Science & Business Media.", "citeRegEx": "Kushner and Yin,? 2003", "shortCiteRegEx": "Kushner and Yin", "year": 2003}, {"title": "Gradient temporal-difference learning algorithms", "author": ["H.R. Maei"], "venue": "PhD thesis. University of Alberta.", "citeRegEx": "Maei,? 2011", "shortCiteRegEx": "Maei", "year": 2011}, {"title": "Weighted importance sampling for off-policy learning with linear function approximation", "author": ["A.R. Mahmood", "H.P. van Hasselt", "R.S. Sutton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mahmood et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mahmood et al\\.", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature 518(7540), pp. 529\u2013533.", "citeRegEx": "Mnih et al\\.,? 2015", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Simultaneous localization and mapping with unknown data association using FastSLAM", "author": ["M. Montemerlo", "S. Thrun"], "venue": "In: IEEE International Conference on Robotics and Automation, 2003. Vol. 2. IEEE, pp. 1985\u20131991.", "citeRegEx": "Montemerlo and Thrun,? 2003", "shortCiteRegEx": "Montemerlo and Thrun", "year": 2003}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B.T. Polyak", "A.B. Juditsky"], "venue": "SIAM Journal on Control and Optimization 30(4), pp. 838\u2013855.", "citeRegEx": "Polyak and Juditsky,? 1992", "shortCiteRegEx": "Polyak and Juditsky", "year": 1992}, {"title": "Off-policy temporal-difference learning with function approximation", "author": ["D. Precup", "R.S. Sutton"], "venue": "In: Proceedings of the eighteenth International Conference on Machine Learning. Morgan Kaufmann, pp. 417\u2013424.", "citeRegEx": "Precup and Sutton,? 2001", "shortCiteRegEx": "Precup and Sutton", "year": 2001}, {"title": "Eligibility traces for off-policy policy evaluation", "author": ["D. Precup", "R.S. Sutton", "S.P. Singh"], "venue": "In: Proceedings of the Seventeenth International Conference on Machine Learning. Morgan Kaufmann, pp. 766\u2013773.", "citeRegEx": "Precup et al\\.,? 2000", "shortCiteRegEx": "Precup et al\\.", "year": 2000}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The Annals of Mathematical Statistics 22(3), pp. 400\u2013407.", "citeRegEx": "Robbins and Monro,? 1951", "shortCiteRegEx": "Robbins and Monro", "year": 1951}, {"title": "Temporal credit assignment in reinforcement learning", "author": ["R.S. Sutton"], "venue": "PhD thesis. University of Massachusetts.", "citeRegEx": "Sutton,? 1984", "shortCiteRegEx": "Sutton", "year": 1984}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning 3, pp. 9\u201344. 31", "citeRegEx": "Sutton,? 1988", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "The MIT press, Cambridge MA.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "A convergent O(n) algorithm for off-policy temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "Szepesv\u00e1ri", "Cs.", "H.R. Maei"], "venue": "Advances in Neural Information Processing Systems 21, pp. 1609\u20131616.", "citeRegEx": "Sutton et al\\.,? 2008", "shortCiteRegEx": "Sutton et al\\.", "year": 2008}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "Szepesv\u00e1ri", "Cs.", "E. Wiewiora"], "venue": "In: Proceedings of the 26th Annual International Conference on Machine Learning. ACM, pp. 993\u20131000.", "citeRegEx": "Sutton et al\\.,? 2009", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "A new Q(\u03bb) with interim forward view and Monte Carlo equivalence", "author": ["R.S. Sutton", "A.R. Mahmood", "D. Precup", "H.P. van Hasselt"], "venue": "JMLR W&CP", "citeRegEx": "Sutton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2014}, {"title": "Algorithms for reinforcement learning", "author": ["Szepesv\u00e1ri", "Cs."], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning 4(1), pp. 1\u2013103.", "citeRegEx": "Szepesv\u00e1ri and Cs.,? 2010", "shortCiteRegEx": "Szepesv\u00e1ri and Cs.", "year": 2010}, {"title": "Off-policy TD(\u03bb) with a true online equivalence", "author": ["H.P. van Hasselt", "A.R. Mahmood", "R.S. Sutton"], "venue": "Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Hasselt et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2014}, {"title": "True online TD(\u03bb)", "author": ["H. van Seijen", "R.S. Sutton"], "venue": "JMLR W&CP", "citeRegEx": "Seijen and Sutton,? \\Q2014\\E", "shortCiteRegEx": "Seijen and Sutton", "year": 2014}], "referenceMentions": [{"referenceID": 18, "context": "For example, in reinforcement learning we often learn value functions that are predictions of the discounted sum of all future rewards in the potentially infinite future (Sutton and Barto 1998).", "startOffset": 170, "endOffset": 193}, {"referenceID": 16, "context": "We show that the desire to be online results in the spontaneous emergence of TD errors (Sutton 1984; Sutton 1988).", "startOffset": 87, "endOffset": 113}, {"referenceID": 17, "context": "We show that the desire to be online results in the spontaneous emergence of TD errors (Sutton 1984; Sutton 1988).", "startOffset": 87, "endOffset": 113}, {"referenceID": 12, "context": "This is interesting because such averaging is known to improve the convergence rates of online learning algorithms (Polyak and Juditsky 1992; Bach and Moulines 2013), but seems to only rarely be used in reinforcement learning (as noted, e.", "startOffset": 115, "endOffset": 165}, {"referenceID": 0, "context": "This is interesting because such averaging is known to improve the convergence rates of online learning algorithms (Polyak and Juditsky 1992; Bach and Moulines 2013), but seems to only rarely be used in reinforcement learning (as noted, e.", "startOffset": 115, "endOffset": 165}, {"referenceID": 17, "context": "In Section 6, we formalize these ideas and show they lead naturally to a form of TD(\u03bb) (Sutton 1988; Sutton and Barto 1998).", "startOffset": 87, "endOffset": 123}, {"referenceID": 18, "context": "In Section 6, we formalize these ideas and show they lead naturally to a form of TD(\u03bb) (Sutton 1988; Sutton and Barto 1998).", "startOffset": 87, "endOffset": 123}, {"referenceID": 16, "context": "The et vector is analogous to the conventional eligibility trace (see: Sutton 1988; Sutton and Barto 1998, and references therein) but has a special form as first proposed by van Seijen and Sutton (2014). It is initialized to e\u22121 = 0 (or, equivalently, to e0 = \u03b10\u03c60) and then updated according to", "startOffset": 71, "endOffset": 204}, {"referenceID": 18, "context": "Until recently, exactly equivalences between forward and backward views were only known to exist for algorithms that update their predictions in batch (Sutton and Barto 1998).", "startOffset": 151, "endOffset": 174}, {"referenceID": 16, "context": "Until recently, exactly equivalences between forward and backward views were only known to exist for algorithms that update their predictions in batch (Sutton and Barto 1998). Van Seijen & Sutton (2014) were the first to derive an online backward", "startOffset": 152, "endOffset": 203}, {"referenceID": 18, "context": "This target Z t is known as a \u03bb-return (Sutton and Barto 1998).", "startOffset": 39, "endOffset": 62}, {"referenceID": 16, "context": "This target Z t is known as a \u03bb-return (Sutton and Barto 1998). The version that truncates at the current horizon h was first proposed by van Seijen and Sutton (2014). The total set of updates is", "startOffset": 40, "endOffset": 167}, {"referenceID": 17, "context": "The merits of \u03bb-returns are well known (Sutton 1988; Sutton and Barto 1998) but the \u03b2-weighting of the online weights is novel to this paper, and it is appropriate to discuss it in a little more detail.", "startOffset": 39, "endOffset": 75}, {"referenceID": 18, "context": "The merits of \u03bb-returns are well known (Sutton 1988; Sutton and Barto 1998) but the \u03b2-weighting of the online weights is novel to this paper, and it is appropriate to discuss it in a little more detail.", "startOffset": 39, "endOffset": 75}, {"referenceID": 12, "context": "This specific algorithm is interesting because the predictions according to the averages \u03b8m are known to converge to the optimal predictions faster than the predictions according to any sequence of online weights \u03b8\u0303m (Polyak and Juditsky 1992; Bach and Moulines 2013).", "startOffset": 217, "endOffset": 267}, {"referenceID": 0, "context": "This specific algorithm is interesting because the predictions according to the averages \u03b8m are known to converge to the optimal predictions faster than the predictions according to any sequence of online weights \u03b8\u0303m (Polyak and Juditsky 1992; Bach and Moulines 2013).", "startOffset": 217, "endOffset": 267}, {"referenceID": 16, "context": "Therefore, as in Sutton et al. (2014), we allow the features, discounts and persistency parameters to be stationary functions of an underlying unobserved state, such that \u03c6t .", "startOffset": 17, "endOffset": 38}, {"referenceID": 15, "context": "If the step sizes are suitably chosen, for instance such that \u2211\u221e t=0 \u03b1t =\u221e and \u2211\u221e t=0 \u03b1 2 t <\u221e (Robbins and Monro 1951), and if the means and variances of Z\u221e t and \u03c6t are well-defined and bounded for all t, this update converges to the fixed-point solution \u03b8\u2217 that minimizes the quadratic loss (cf.", "startOffset": 95, "endOffset": 119}, {"referenceID": 0, "context": "Although convergence is already guaranteed when \u03b2t = 1 for all t, recent work has shown that for similar stochastic gradient algorithms the optimal rate of convergence is attained if \u03b2t decreases much faster than \u03b1t, specifically when \u03b2t = O(t \u22121) while \u03b1t = \u03b1 for some constant \u03b1 (Bach and Moulines 2013).", "startOffset": 281, "endOffset": 305}, {"referenceID": 0, "context": "Although convergence is already guaranteed when \u03b2t = 1 for all t, recent work has shown that for similar stochastic gradient algorithms the optimal rate of convergence is attained if \u03b2t decreases much faster than \u03b1t, specifically when \u03b2t = O(t \u22121) while \u03b1t = \u03b1 for some constant \u03b1 (Bach and Moulines 2013). More generally, it seems likely that convergence also holds if \u2211\u221e t=0 \u03b2 2 t <\u221e and \u2211\u221e t=0 \u03b1 2 t =\u221e. The observation that \u03b2t should perhaps decrease over time for faster learning may seem at odds with our introduction of this parameter as a degree of trust. However, these two views are quite compatible if we consider \u03b2t to be the degree of trust we place in the online updates relative to the trust we place in our current predictions due to the trusted weights. When the trust in the predictions increases over time, the relative trust in the inherently noisy online targets should then decrease. Although Theorem 1 is already fairly general, it does not cover the important case when the residual predictions additionally depend on the weights we are updating. It makes sense to use the predictions we trust most and therefore we now consider what happens when Pt . = \u03c6t \u03b8t\u22121. Notice that we have to use \u03b8t\u22121 rather than \u03b8t, because Pt is used in the computation of \u03b8t and so the latter is not yet available when we compute Pt. The analysis of this case is more complex than the previous one, because Pt is no longer a constant function of state. This means the update is no longer a standard gradient-descent update on a quadratic loss, because the target Z t for the online weights itself depends on the trusted weights that we are simultaneously updating. The results by Bach and Moulines (2013) on stochastic gradient descent indicate that perhaps the most interesting case is where \u03b2t decreases faster than \u03b1t, such that limt\u2192\u221e \u03b2t/\u03b1t = 0.", "startOffset": 282, "endOffset": 1709}, {"referenceID": 20, "context": "Then, if \u2211\u221e t=0 \u03b1t = \u2211\u221e t=0 \u03b2t = \u221e, \u2211\u221e t=0 \u03b1 2 t < \u221e, and limt\u2192\u221e \u03b2t \u03b1t = 0, algorithm (44) converges almost surely to the TD fixed-point solution \u03b8\u2217 that minimizes the mean-squared projected Bellman error (Sutton, Szepesv\u00e1ri, and Maei 2008; Sutton et al. 2009), such that", "startOffset": 205, "endOffset": 260}, {"referenceID": 2, "context": "For more detail on analyzing stochastic approximations on two times scales, we refer to Borkar (1997), Borkar (2008), Kushner and Yin (2003), and Konda and Tsitsiklis (2004).", "startOffset": 88, "endOffset": 102}, {"referenceID": 2, "context": "For more detail on analyzing stochastic approximations on two times scales, we refer to Borkar (1997), Borkar (2008), Kushner and Yin (2003), and Konda and Tsitsiklis (2004).", "startOffset": 88, "endOffset": 117}, {"referenceID": 2, "context": "For more detail on analyzing stochastic approximations on two times scales, we refer to Borkar (1997), Borkar (2008), Kushner and Yin (2003), and Konda and Tsitsiklis (2004).", "startOffset": 88, "endOffset": 141}, {"referenceID": 2, "context": "For more detail on analyzing stochastic approximations on two times scales, we refer to Borkar (1997), Borkar (2008), Kushner and Yin (2003), and Konda and Tsitsiklis (2004). We first analyze the convergence of the faster updates to the online weights, where we can assume that the trusted weights are stationary at some value \u03b8.", "startOffset": 88, "endOffset": 174}, {"referenceID": 1, "context": "Then, using a form of policy iteration (Bellman 1957; Howard 1960), we can repeatedly switch between estimating and improving the policy to tackle the problem of optimal control.", "startOffset": 39, "endOffset": 66}, {"referenceID": 5, "context": "Then, using a form of policy iteration (Bellman 1957; Howard 1960), we can repeatedly switch between estimating and improving the policy to tackle the problem of optimal control.", "startOffset": 39, "endOffset": 66}, {"referenceID": 18, "context": "However, to properly and fully include adaptable policies, we would in addition need to carefully consider the problem of learning off-policy, about action-selection policies that differ from the one used to generate the data (Sutton and Barto 1998).", "startOffset": 226, "endOffset": 249}, {"referenceID": 13, "context": "This is consistent but orthogonal to the ideas outlined in this paper, and such off-policy predictions (including those about the greedy and, ultimately, optimal policy) are learnable through a proper use of rejection sampling, as in Q-learning, or importance sampling (Precup, Sutton, and Singh 2000; Precup and Sutton 2001; Maei 2011; Sutton et al. 2014; van Hasselt, Mahmood, and Sutton 2014; Mahmood, van Hasselt, and Sutton 2014).", "startOffset": 269, "endOffset": 434}, {"referenceID": 8, "context": "This is consistent but orthogonal to the ideas outlined in this paper, and such off-policy predictions (including those about the greedy and, ultimately, optimal policy) are learnable through a proper use of rejection sampling, as in Q-learning, or importance sampling (Precup, Sutton, and Singh 2000; Precup and Sutton 2001; Maei 2011; Sutton et al. 2014; van Hasselt, Mahmood, and Sutton 2014; Mahmood, van Hasselt, and Sutton 2014).", "startOffset": 269, "endOffset": 434}, {"referenceID": 21, "context": "This is consistent but orthogonal to the ideas outlined in this paper, and such off-policy predictions (including those about the greedy and, ultimately, optimal policy) are learnable through a proper use of rejection sampling, as in Q-learning, or importance sampling (Precup, Sutton, and Singh 2000; Precup and Sutton 2001; Maei 2011; Sutton et al. 2014; van Hasselt, Mahmood, and Sutton 2014; Mahmood, van Hasselt, and Sutton 2014).", "startOffset": 269, "endOffset": 434}, {"referenceID": 10, "context": "The main idea of span-independent computation is more general and can be applied quite naturally to other settings, including for instance non-linear functions such as deep neural networks (LeCun, Bengio, and Hinton 2015; Mnih et al. 2015) or to quadratic-time linear-function algorithms as in LSTD (Bradtke and Barto 1996).", "startOffset": 189, "endOffset": 239}, {"referenceID": 4, "context": "2015) or to quadratic-time linear-function algorithms as in LSTD (Bradtke and Barto 1996).", "startOffset": 65, "endOffset": 89}], "year": 2015, "abstractText": "We consider how to learn multi-step predictions efficiently. Conventional algorithms wait until observing actual outcomes before performing the computations to update their predictions. If predictions are made at a high rate or span over a large amount of time, substantial computation can be required to store all relevant observations and to update all predictions when the outcome is finally observed. We show that the exact same predictions can be learned in a much more computationally congenial way, with uniform per-step computation that does not depend on the span of the predictions. We apply this idea to various settings of increasing generality, repeatedly adding desired properties and each time deriving an equivalent span-independent algorithm for the conventional algorithm that satisfies these desiderata. Interestingly, along the way several known algorithmic constructs emerge spontaneously from our derivations, including dutch eligibility traces, temporal difference errors, and averaging. This allows us to link these constructs one-to-one to the corresponding desiderata, unambiguously connecting the \u2018how\u2019 to the \u2018why\u2019. Each step, we make sure that the derived algorithm subsumes the previous algorithms, thereby retaining their properties. Ultimately we arrive at a single general temporal-difference algorithm that is applicable to the full setting of reinforcement learning. 1 Learning long-term predictions The span of a multi-step prediction is the number of steps elapsing between when the prediction is made and when its target or ideal value is known. We consider the case in which predictions are made repeatedly, at each of a sequence of discrete time steps. For example, if on each day we predict what a stock market index will be in 30 days, then the span is 30, whereas if we predict at each hour what the stock market index will be in 30 days, then the span is 30\u00d7 24 = 720. The span may vary for individual predictions in a sequence. For example, if we predict on each day what the stock-market index will be at the end of the year, then the span will be much longer for predictions made in January than it is for predictions made in \u2217Google DeepMind \u2020Reinforcement Learning and Artificial Intelligence Laboratory Department of Computing Science, University of Alberta Edmonton, Alberta, Canada T6G 2E8 1 ar X iv :1 50 8. 04 58 2v 1 [ cs .L G ] 1 9 A ug 2 01 5 December. If the span may vary in this way, then we consider the span of the prediction sequence to be the maximum possible span of any individual prediction in the sequence. For example, the span of a daily end-of-year stock-index prediction is 365. Often the span is infinite. For example, in reinforcement learning we often learn value functions that are predictions of the discounted sum of all future rewards in the potentially infinite future (Sutton and Barto 1998). In this paper we consider computational and algorithmic issues in efficiently learning long-term predictions, defined as predictions of large integer span. Predictions could be long term in this sense either because a great deal of clock time passes, as in predicting something at the end of the year, or because predictions are made very often, with a short time between steps (e.g., as in high-frequency financial trading). The per-step computational complexity of some algorithms for learning accurate predictions depends on the span of the predictions, and this can become a significant concern if the span is large. Therefore, we focus on the construction of learning algorithms whose computational complexity per time step (in both time and memory) is constant (does not scale with time) and independent of span. This paper features two recurring themes, the first of which is the repeated spontaneous emergence of, often well-known, algorithmic constructs, directly from our derivations. We start each derivation by formalizing a desired property and constructing an algorithm that fulfills it, without considering computationally efficiency. Then, we derive a spanindependent algorithm that results on each step in exactly the same predictions. Interestingly, each time a specific algorithmic construct emerges, demonstrating a clear connection between the desideratum (the \u2018why\u2019) and the algorithmic construct (the \u2018how\u2019). For instance, the desire to be independent of span leads to a dutch eligibility trace, which was previously derived only in the more specific context of online temporal difference (TD) learning (van Seijen and Sutton 2014). The second theme is that we unify the algorithms at each step. Each time, we make sure to obtain an algorithm that is strictly more general than the previous ones, so that in the end we obtain one single algorithm that can fulfill all the desiderata while remaining computationally congenial. 2 Outline of the paper In this section, we briefly describe the high-level narrative of the paper, without going into technical detail. In each of the Sections 3 to 8, we describe and formalize one or more desirable properties for our algorithms and then derive a computationally congenial algorithm that achieves this exactly. We build up to the final, most general, algorithm that is ultimately derived in Section 8 to highlight the connections between desired properties and algorithmic constructs. Making these connections clear is one of the main goals of this paper. Specifically, in Section 3 we derive a span-independent algorithm to update the predictions for a single final outcome. The algorithm is offline in the sense that does not change its predictions before observing the outcome. The dutch trace emerges spontaneously, which shows that this trace is closely tied to the requirement of span-independent computation. This emergence is surprising and intriguing because it shows that these traces", "creator": "LaTeX with hyperref package"}}}