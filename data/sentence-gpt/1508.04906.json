{"id": "1508.04906", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Aug-2015", "title": "Semi-supervised Learning with Regularized Laplacian", "abstract": "We study a semi-supervised learning method based on the similarity graph and RegularizedLaplacian. We give convenient optimization formulation of the Regularized Laplacian method and establishits various properties. In particular, we show that the kernel of the methodcan be interpreted in terms of discrete and continuous time random walks and possesses several importantproperties of proximity measures. Both optimization and linear algebra methods can be used for efficientcomputation of the classification functions. We demonstrate on numerical examples that theRegularized Laplacian method is competitive with respect to the other state of the art semi-supervisedlearning methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 20 Aug 2015 08:01:42 GMT  (1773kb)", "http://arxiv.org/abs/1508.04906v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["konstantin avrachenkov", "pavel chebotarev", "alexey mishenin"], "accepted": false, "id": "1508.04906"}, "pdf": {"name": "1508.04906.pdf", "metadata": {"source": "CRF", "title": "Semi-supervised Learning with Regularized Laplacian", "authors": ["K. Avrachenkov", "P. Chebotarev", "A. Mishenin"], "emails": ["k.avrachenkov@inria.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 8.\n04 90\n6v 1\n[ cs\n.L G\n] 2\n0 A\nug 2\n01 5\nIS S\nN 02\n49 -6\n39 9\nIS R\nN IN\nR IA\n/R R\n-- 87\n65 --\nF R\n+ E\nN G\nRESEARCH REPORT\nN\u00b0 8765 July 2015\nProject-Team Maestro"}, {"heading": "Semi-supervised", "text": ""}, {"heading": "Learning with", "text": ""}, {"heading": "Regularized Laplacian", "text": "K. Avrachenkov, P. Chebotarev, A. Mishenin\nRESEARCH CENTRE SOPHIA ANTIPOLIS \u2013 M\u00c9DITERRAN\u00c9E\n2004 route des Lucioles - BP 93 06902 Sophia Antipolis Cedex\nSemi-supervised Learning with\nRegularized Laplacian\nK. Avrachenkov\u2217, P. Chebotarev\u2020, A. Mishenin\u2021 \u00a7\nProject-Team Maestro\nResearch Report n\u00b0 8765 \u2014 July 2015 \u2014 19 pages\nAbstract: We study a semi-supervised learning method based on the similarity graph and Regularized Laplacian. We give convenient optimization formulation of the Regularized Laplacian method and establish its various properties. In particular, we show that the kernel of the method can be interpreted in terms of discrete and continuous time random walks and possesses several important properties of proximity measures. Both optimization and linear algebra methods can be used for efficient computation of the classification functions. We demonstrate on numerical examples that the Regularized Laplacian method is competitive with respect to the other state of the art semi-supervised learning methods.\nKey-words: Semi-supervised learning, Graph-based learning, Regularized Laplacian, Proximity measure, Wikipedia article classification\n\u2217 Corresponding author. K. Avrachenkov is with Inria Sophia Antipolis, 2004 Route des Lucioles, 06902, Sophia Antipolis, France k.avrachenkov@inria.fr\n\u2020 P. Chebotarev is with Trapeznikov Institute of Control Sciences of the Russian Academy of Sciences, 65 Profsoyuznaya Str., Moscow, 117997, Russia\n\u2021 A. Mishenin is with St. Petersburg State University, Faculty of Applied Mathematics and Control Processes, Peterhof, 198504, Russia\n\u00a7 This work was partially supported by Campus France, Alcatel-Lucent Inria Joint Lab, EU Project Congas FP7-ICT-2011-8-317672, and RFBR grant No. 13-07-00990."}, {"heading": "L\u2019Apprentissage Semi-supervis\u00e9 avec Laplacian R\u00e9gularis\u00e9", "text": "R\u00e9sum\u00e9 : Nous \u00e9tudions une m\u00e9thode d\u2019apprentissage semi-supervis\u00e9, bas\u00e9 sur le graphe de similarit\u00e9 et Laplacian r\u00e9gularis\u00e9. Nous formalisons la m\u00e9thode comme un probl\u00e8me d\u2019optimisation convexe et quadratique et nous \u00e9tablissons ses diverses propri\u00e9t\u00e9s. En particulier, nous montrons que le noyau de la m\u00e9thode peut \u00eatre interpr\u00e9t\u00e9 en termes des marches al\u00e9atoires en temps discret et continu et poss\u00e8de plusieurs propri\u00e9t\u00e9s importantes des mesures de proximit\u00e9. Les techniques d\u2019optimisation ainsi que les techniques d\u2019alg\u00e9bre lin\u00e9aire peuvent \u00eatre utilis\u00e9 pour un calcul efficace des fonctions de classification. Nous d\u00e9montrons sur des exemples num\u00e9riques que la m\u00e9thode de Laplacian r\u00e9gularis\u00e9 est concurrentiel par rapport aux autres \u00e9tat de l\u2019art m\u00e9thodes d\u2019apprentissage semi-supervis\u00e9.\nMots-cl\u00e9s : Apprentissage Semi-supervis\u00e9, Apprentissage bas\u00e9 sur le graphe de similarit\u00e9, Laplacian r\u00e9gularis\u00e9, mesure de proximit\u00e9, classification des articles Wikipedia"}, {"heading": "Semi-supervised Learning with Regularized Laplacian 3", "text": ""}, {"heading": "1 Introduction", "text": "Graph-based semi-supervised learning methods have the following three principles at their foundation. The first principle is to use a few labelled points (points with known classification) together with the unlabelled data to tune the classifier. In contrast with the supervised machine learning, the semi-supervised learning creates a synergy between the training data and classification data. This drastically reduces the size of the training set and hence significantly reduces the cost of experts\u2019 work. The second principal idea of the semi-supervised learning methods is to use a (weighted) similarity graph. If two data points are connected by an edge, this indicates some similarity of these points. Then, the weight of the edge, if present, reflects the degree of similarity. The result of classification is given in the form of classification functions. Each class has its own classification function defined over all data points. An element of a classification function gives a degree of relevance to the class for each data point. Then, the third principal idea of the semi-supervised learning methods is that the classification function should change smoothly over the similarity graph. Intuitively, nodes of the similarity graph that are closer together in some sense are more likely to belong to the same class. This idea of classification function smoothness can naturally be expressed using graph Laplacian or its modification.\nThe work [37] seems to be the first work where the graph-based semi-supervised learning was introduced. The authors of [37] formulated the semi-supervised learning method as a constrained optimization problem involving graph Laplacian. Then, in [36, 35] the authors proposed optimization formulations based on several variations of the graph Laplacian. In [4] a unifying optimization framework was proposed which gives as particular cases the methods of [35] and [36]. In addition, the general framework in [4] gives as a particular case an interesting PageRank based method, which provides robust classification with respect to the choice of the labelled points [3, 5]. We would like to note that the local graph partitioning problem [2, 20] can be related to graph-based semi-supervised learning. An interested reader can find more details about various semi-supervised learning methods in the surveys and books [9, 23, 38].\nIn the present work we study in detail a semi-supervised learning method based on the Regularized Laplacian. To the best of our knowledge, the idea of using Regularized Laplacian and its kernel for measuring proximity in graphs and application to mathematical sociology goes back to the works [13, 15]. In [23] the authors compared experimentally many graph-based semi-supervised learning methods on several datasets and their conclusion was that the semisupervised learning method based on the Regularized Laplacian kernel demonstrates one of the best performances on nearly all datasets. In [8] the authors studied a semi-supervised learning method based on the Normalized Laplacian graph kernel which also shows good performance. Interestingly, as we show below, if we choose Markovian Laplacian as a weight matrix, several known semi-supervised learning methods reduce to the Regularized Laplacian method. In this work we formulate the Regularized Laplacian method as a convex quadratic optimization problem which helps to design easily parallelizable numerical methods. In fact, the Regularized Laplacian method can be regarded as a Lagrangian relaxation of the method proposed in [37]. Of course, this is a more flexible formulation, since by choosing an appropriate value for the Lagrange multiplier one can always retrieve the method of [37] as a particular case. We establish various properties of the Regularized Laplacian method. In particular, we show that the kernel of the method can be interpreted in terms of discrete and continuous time random walks and possesses several important properties of proximity measures. Both optimization and linear algebra methods can be used for efficient computation of the classification functions. We discuss advantages and disadvantages of various numerical approaches. We demonstrate on numerical examples that the Regularized Laplacian method is competitive with respect to the other state of the art semi-supervised learning methods.\nRR n\u00b0 8765"}, {"heading": "4 K. Avrachenkov & P. Chebotarev & A. Mishenin", "text": "The paper is organized as follows: In the next section we formally define the Regularized Laplacian method. In Section 3 we discuss several related graph-based semi-supervised methods and graph kernels. In Section 4 we present insightful interpretations and properties of the Regularized Laplacian method. We analyse important limiting cases in Section 5. Then, in Section 6 we discuss various numerical approaches to compute the classification functions and show by numerical examples that the performance of the Regularized Laplacian method is better or comparable with the leading semi-supervised methods. Section 7 concludes the paper with directions for future research."}, {"heading": "2 Notations and method formulation", "text": "Suppose one needs to classify N data points (nodes) into K classes and assume P data points are labelled. That is, we know the class to which each labelled point belongs. Denote by Vk the set of labelled points in class k = 1, ...,K. Of course, |V1|+ ...+ |VK | = P .\nThe graph-based semi-supervised learning approach uses a weighted graph G = (V,A) connecting data points, where V , |V | = N , denotes the set of nodes and A denotes the weight (similarity) matrix. In this work we assume that A is symmetric and the underlying graph is connected. Each element aij represents the degree of similarity between data points i and j. Denote by D the diagonal matrix with its (i, i)-element equal to the sum of the i-th row of matrix A: di = \u2211N\nj=1 aij . We denote by L = D \u2212 A the Standard (Combinatorial) Laplacian associated with the graph G.\nDefine an N \u00d7K matrix Y as\nYik =\n{\n1, if i \u2208 Vk, i.e., point i is labelled as a class k point,\n0, otherwise.\nWe refer to each column Y\u2217k of matrix Y as a labeling function. Also define an N \u00d7K matrix F and call its columns F\u2217k classification functions. The general idea of the graph-based semisupervised learning is to find classification functions so that on the one hand they are close to the corresponding labeling function and on the other hand they change smoothly over the graph associated with the similarity matrix. This general idea can be expressed by means of the following particular optimization problem:\nmin F\n{\nK \u2211\nk=1\n(F\u2217k \u2212 Y\u2217k) T (F\u2217k \u2212 Y\u2217k) + \u03b2\nK \u2211\nk=1\nFT\u2217kLF\u2217k\n}\n, (1)\nwhere \u03b2 \u2208 (0,\u221e) is a regularization parameter. The regularization parameter \u03b2 represents a trade-off between the closeness of the classification function to the labeling function and its smoothness.\nSince the Laplacian L is positive-semidefinite and the second term in (1) is strictly convex, the optimization problem (1) has a unique solution determined by the stationarity condition\n2(F\u2217k \u2212 Y\u2217k) T + 2\u03b2FT\u2217kL = 0, k = 1, ...,K,\nwhich gives F\u2217k = (I + \u03b2L) \u22121Y\u2217k, k = 1, ...,K. (2)\nThe matrix Q\u03b2 = (I + \u03b2L) \u22121 is known as Regularized Laplacian kernel of the graph [28, 33] and can be related to the matrix forest theorems [13, 1] and stochastic matrices [1]. The classification functions F\u2217k, k = 1, ...,K, can be obtained either by numerical linear algebra methods\nInria"}, {"heading": "Semi-supervised Learning with Regularized Laplacian 5", "text": "(e.g., power iterations) applied to (2) or by numerical optimization methods applied to (1). We elaborate on numerical methods in Section 6. Once the classification functions are obtained, the points are classified according to the rule\nFik > Fik\u2032 , \u2200k \u2032 6= k \u21d2 Point i is classified into class k.\nThe ties can be broken in arbitrary fashion."}, {"heading": "3 Related approaches", "text": "Let us discuss a number of related approaches. First, we discuss formal relations and in the numerical examples section we compare the approaches on some benchmark examples."}, {"heading": "3.1 Relation to heat kernels", "text": "The authors of [17, 18] first introduced and studied the properties of the heat kernel based on the normalized Laplacian. Specifically, they introduced the kernel\nH(t) = exp(\u2212tL), (3)\nwhere L = D\u22121/2LD\u22121/2\nis the normalized Laplacian. Let us refer to H(t) as the normalized heat kernel. Note that the normalized heat kernel can be obtained as a solution of the following differential equation\nH\u0307(t) = \u2212LH(t),\nwith the initial condition H(0) = I. Then, in [19] the PageRank heat kernel was introduced\n\u03a0(t) = exp(\u2212t(I \u2212 P )), (4)\nwhere P = D\u22121A, (5)\nis the transition probability matrix of the standard random walk on the graph. In [20] the PageRank heat kernel was applied to local graph partitioning.\nIn [28] the heat kernel based on the standard Laplacian\nH(t) = exp(\u2212tL), (6)\nwith L = D \u2212 A, was proposed as a kernel in the support vector machine learning method. Then, in [37] the authors proposed a semi-supervised learning method based on the solution of a heat diffusion equation with Dirichlet boundary conditions. Equivalently, the method of [37] can be viewed as the minimization of the second term in (1) with the values of the classification functions F\u2217k fixed on the labelled points. Thus, the proposed approach (1) is more general as it can be viewed as a Lagrangian relaxation of [37]. The results of the method in [37] can be retrieved with a particular choice of the regularization parameter.\nRR n\u00b0 8765"}, {"heading": "6 K. Avrachenkov & P. Chebotarev & A. Mishenin", "text": ""}, {"heading": "3.2 Relation to the generalized semi-supervised learning method", "text": "In [4] the authors proposed a generalized optimization framework for graph based semi-supervised learning methods\nmin F\n\n\n\nN \u2211\ni=1\nN \u2211\nj=1\nwij\u2016di \u03c3\u22121Fi\u2217 \u2212 dj \u03c3\u22121Fj\u2217\u2016 2 + \u00b5\nN \u2211\ni=1\ndi 2\u03c3\u22121\u2016Fi\u2217 \u2212 Yi\u2217\u2016 2\n\n\n\n, (7)\nwhere wij are the entries of a weight matrix W = (wij) which is a function of A (in particular, one can also take W = A).\nIn particular, with \u03c3 = 1 we retrieve the transductive semi-supervised learning method [35], with \u03c3 = 1/2 we retrieve the semi-supervised learning with local and global consistency [36] and with \u03c3 = 0 we retrieve the PageRank based method [3].\nThe classification functions of the generalized graph based semi-supervised learning are given by\nF\u2217k = \u00b5\n2 + \u00b5\n(\nI \u2212 2\n2 + \u00b5 D\u2212\u03c3WD\u03c3\u22121\n)\u22121\nY\u2217k, k = 1, ...,K.\nNow taking as the weight matrix W = I \u2212 \u03c4L = I \u2212 \u03c4(D\u2212A) (note that with this choice of the weight matrix, the generalized degree matrix D\u2032 = diag(W1) becomes the identity matrix), the above equation transforms to\nF\u2217k =\n(\nI + 2\u03c4\n\u00b5 L\n)\u22121\nY\u2217k, k = 1, ...,K,\nwhich is (2) with \u03b2 = 2\u03c4/\u00b5. It is very interesting to observe that with the proposed choice of the weight matrix all the semi-supervised learning methods defined by various \u03c3\u2019s coincide."}, {"heading": "4 Properties and interpretations of the Regularized Lapla-", "text": "cian method\nThere is a number of interesting interpretations and characterizations which we can provide for the classification functions (2). These interpretations and characterizations will give different insights about the Regularized Laplacian kernel Q\u03b2 and the classification functions (2)."}, {"heading": "4.1 Discrete-time random walk interpretation", "text": "The Regularized Laplacian kernel Q\u03b2 = (I + \u03b2L) \u22121 can be interpreted as the overall transition matrix of a random walk on the similarity graph G with a geometrically distributed number of steps. Namely, consider a Markov chain whose states are our data points and the probabilities of transitions between distinct states are proportional to the corresponding entries of the similarity matrix A:\np\u0302ij = \u03c4aij , i, j = 1, . . . , N, i 6= j, (8)\nwhere \u03c4 > 0 is a sufficiently small parameter. Then the diagonal elements of the transition matrix P\u0302 = (p\u0302ij) are\np\u0302ii = 1\u2212 \u2211\nj 6=i\n\u03c4aij , i = 1, . . . , N (9)\nInria"}, {"heading": "Semi-supervised Learning with Regularized Laplacian 7", "text": "or, in the matrix form,\nP\u0302 = I \u2212 \u03c4L. (10)\nThe matrix P\u0302 determines a random walk on G which differs from the \u201cstandard\u201d one defined by (5) and related to the PageRank heat kernel (4). As distinct from (5), the transition matrix (10) is symmetric for every undirected graph; in general, it has a nonzero diagonal. It is interesting to observe that P\u0302 coincides with the weight matrix W used for transformation of Subsection 3.2.\nConsider a sequence of independent Bernoulli trials indexed by 0, 1, 2, . . . with a certain success probability q. Assume that the number of steps, K, in a random walk is equal to the trial number of the first success. And let Xk be the state of the Markov chain at step k. Then, K is distributed geometrically:\nPr{K = k} = q(1 \u2212 q)k, k = 0, 1, 2, . . . ,\nand the transition matrix of the overall random walk after a random number of stepsK, Z = (zij), zij = Pr{XK = j | X0 = i}, i, j = 1, . . . , N, is given by\nZ = q \u221e \u2211\nk=0\n(1 \u2212 q)kP\u0302 k = q \u221e \u2211\nk=0\n(1\u2212 q)k(I \u2212 \u03c4L)k\n= q (I \u2212 (1 \u2212 q)(I \u2212 \u03c4L)) \u22121 = ( I + \u03c4(q\u22121 \u2212 1)L )\u22121 .\nThus, Z = Q\u03b2 = (I + \u03b2L) \u22121 with \u03b2 = \u03c4(q\u22121 \u2212 1).\nThis means that the i-th component of the classification function can be interpreted as the probability of finding the discrete-time random walk with transition matrix (10) in node i after the geometrically distributed number of steps with parameter q, given the random walk started with the distribution Y\u2217k/(1 TY\u2217k)."}, {"heading": "4.2 Continuous-time random walk interpretation", "text": "Consider the differential equation\nH\u0307(t) = \u2212LH(t), (11)\nwith the initial condition H(0) = I. Also consider the standard continuous-time random walk that spends exponentially distributed time in node k with the expected duration 1/dk and after the exponentially distributed time moves to a new node l with probability akl/dk. Then, the solution hij(t) = exp(\u2212tL) of the differential equation (11) can be interpreted as a probability to find the standard continuous-time random walk in node j given the random walk started from node i. By taking the Laplace transform of (11) we obtain\nH(s) = (sI + L)\u22121 = s\u22121(I + s\u22121L)\u22121. (12)\nThus, the classification function (2) can be interpreted as the Laplace transform divided by 1/s, or equivalently the i-th component of the classification function can be interpreted as a quantity proportional to the probability of finding the random walk in node i after exponentially distributed time with mean \u03b2 = 1/s given the random walk started with the distribution Y\u2217k/(1 TY\u2217k).\nRR n\u00b0 8765"}, {"heading": "8 K. Avrachenkov & P. Chebotarev & A. Mishenin", "text": ""}, {"heading": "4.3 Proximity and distance properties", "text": "As before, let Q\u03b2=(q \u03b2 ij)N\u00d7N be the Regularized Laplacian kernel (I + \u03b2L) \u22121 of (2).\nQ\u03b2 determines a positive 1-proximity measure [14] s(i, j) := q \u03b2 ij , i.e., it satisfies [13] the\nfollowing conditions: (1) for any i \u2208 V, \u2211\nk\u2208V q \u03b2 ik = 1 and\n(2) for any i, j, k \u2208 V, q\u03b2ji + q \u03b2 jk \u2212 q \u03b2 ik \u2264 q \u03b2 jj with a strict inequality whenever i = k and i 6= j\n(the triangle inequality for proximities).\nThis implies [14] the following two important properties: (a) q\u03b2ii > q \u03b2 ij for all i, j \u2208 V such\nthat i 6= j (egocentrism property); (b) \u03c1\u03b2ij := \u03b2(q \u03b2 ii + q \u03b2 jj \u2212 q \u03b2 ij \u2212 q \u03b2 ji) is 1 a distance on V. Because of the forest interpretation of Q\u03b2 (see Section 4.4), it is called the adjusted forest distance. The distances \u03c1\u03b2ij have a twofold connection with the resistance distance \u03c1\u0303ij on G [16]. First, lim\u03b2\u2192\u221e \u03c1 \u03b2 ij = \u03c1\u0303ij , i, j \u2208 V. Second, let G \u03b2 be the weighted graph such that: V (G\u03b2) = V (G)\u222a{0}, the restriction of G\u03b2 to V (G) coincides with G, and G\u03b2 additionally contains an edge (i, 0) of weight 1/\u03b2 for each node i \u2208 V (G). Then it follows that \u03c1\u03b2ij(G) = \u03c1\u0303ij(G \u03b2), i, j \u2208 V. In the electrical interpretation of G, the weight 1/\u03b2 of the edges (i, 0) is treated as conductivity, i.e., the lines connecting each node to the \u201chub\u201d 0 have resistance \u03b2. An interested reader can find more properties of the proximity measures determined by Q\u03b2 in [13].\nFurthermore, every Q\u03b2, \u03b2 > 0 determines a transitional measure on V, which means [12] that:\nq\u03b2ij q \u03b2 jk \u2264 q \u03b2 ik q \u03b2 jj for all i, j, k \u2208 V with q \u03b2 ij q \u03b2 jk = q \u03b2 ik q \u03b2 jj if and only if every path in G from i to k visits j.\nIt follows that d\u03b2ij := \u2212 ln\n(\nq\u03b2ij/ \u221a q\u03b2iiq \u03b2 jj\n)\nprovides a distance on V. This distance is cutpoint\nadditive, that is, d\u03b2ij + d \u03b2 jk = d \u03b2 ik if and only if every path in G from i to k visits j. In the asymptotics, d\u03b2ij becomes proportional to the shortest path distance and the resistance distance as \u03b2 \u2192 0 and \u03b2 \u2192 \u221e, respectively."}, {"heading": "4.4 Matrix forest characterization", "text": "By the matrix forest theorem [13, 1], each entry q\u03b2ij of Q\u03b2 is equal to the specific weight of the spanning rooted forests that connect node i to node j in the weighted graph G whose combinatorial Laplacian is L.\nMore specifically, q\u03b2ij = F \u03b2 i\u22a3j/F \u03b2, where F\u03b2 is the total \u03b2-weight of all spanning rooted forests of G, F\u03b2i\u22a3j being the total \u03b2-weight of such of them that have node i in a tree rooted at j. Here, the \u03b2-weight of a forest stands for the product of its edges weights, each multiplied by \u03b2.\nLet us mention a closely related interpretation of the Regularized Laplacian kernel Q\u03b2 in terms of information dissemination [11]. Suppose that an information unit (an idea) must be transmitted through G. A plan of information transmission is a spanning rooted forest F in G: the information unit is initially injected into the roots of F; after that it comes to the other nodes along the edges of F. Suppose that a plan is chosen at random: the probability of every choice is proportional to the \u03b2-weight of the corresponding forest. Then by the matrix forest theorem, the probability that the information unit arrives at i from root j equals q\u03b2ij = F \u03b2 i\u22a3j/F\n\u03b2. This interpretation is particularly helpful in the context of machine learning for social networks.\n1Cf. the cosine law [21] and the inverse covariance mapping [22, Section 5.2].\nInria"}, {"heading": "Semi-supervised Learning with Regularized Laplacian 9", "text": ""}, {"heading": "4.5 Statistical characterization", "text": "Consider the problem of attribute evaluation from paired comparisons.\nSuppose that each data point (node) i has a value parameter vi, and a series of paired comparisons rij between the points is performed. Let the result of i in a comparison with j obey the Scheff\u00e9 linear statistical model [32]\nE(rij) = vi \u2212 vj , (13)\nwhere E(\u00b7) is the mathematical expectation. The matrix form of (13) applied to an experiment is\nE(r) = Xv,\nwhere v = (v1, . . . , vN ) T , and r is the vector of comparison results, X being the incidence matrix (design matrix , in terms of statistics): if the kth element of r is a comparison result of i confronted to j, then, in accordance with (13), xki = 1, xkj = \u22121, and xkl = 0 for l 6\u2208 {i, j}.\nSuppose that X is known, r being a sample, and the problem is to estimate v up to a shift [10, Section 4]. Then\nv\u0303(\u03bb) = (\u03bbI +XTX)\u22121XTr (14)\nis the well-known ridge estimate of v, where \u03bb > 0 is the ridge parameter. Denoting \u03b2 = \u03bb\u22121 and XTX = L (it is easily verified that XTX is a Laplacian matrix whose (i, j)-entry with j 6= i is minus the number of comparisons between i and j) one has\nv\u0303(\u03bb) = (I + \u03b2L)\u22121\u03b2XTr, (15)\ni.e., the solution is provided by the same transformation based on the Regularized Laplacian kernel as in (2) (cf. also (12)). Here, the weight matrix A of G contains the numbers of comparisons between nodes; s = XTr is the vector of the sums of comparison results of the nodes: si = \u2211 j rij \u2212 \u2211\nj rji, where rij and rji are taken from r, which has one entry (either rij or rji) for each comparison result.\nSuppose now that value parameter vi (belonging to an interval centered at zero) is a positive or negative intensity of some property, and thus, vi can be treated as a signed membership of data point i in the corresponding class. The pairwise comparisons r are performed with respect to this property. Then \u03b2XTr = \u03b2s is a kind of labeling function or a crude correlate of membership in the above class, whereas (15) provides a refined measure of membership which takes into account proximity. Along these lines, (15) can be considered as a procedure of semi-supervised learning.\nA Bayesian version of the model (13) enables one to interpret and estimate the ridge parameter \u03bb = 1/\u03b2. Namely, assume that: (i) the parameters v1, . . . , vN chosen at random from the universal set are independent random variables with zero mean and variance \u03c321 and (ii) for any vector v, the errors in (13) are independent and have zero mean, their unconditional variance being \u03c322 .\nIt can be shown [10, Proposition 4.2] that under these conditions, the best linear predictors for the parameters v are the ridge estimators (15) with \u03b2 = \u03c321/\u03c3 2 2 .\nThe best linear predictors for v are the v\u0303i\u2019s that minimize E(v\u0303i \u2212 vi) 2 among all statistics of\nthe form v\u0303i = ci + C T i r satisfying E(v\u0303i \u2212 vi) = 0.\nThe variances \u03c321 and \u03c3 2 2 can be estimated from the experiment. In fact, there are many\napproaches to choosing the ridge parameter, see, e.g., [24, 29] and the references therein.\nRR n\u00b0 8765"}, {"heading": "10 K. Avrachenkov & P. Chebotarev & A. Mishenin", "text": ""}, {"heading": "5 Limiting cases", "text": "Let us analyse the formula (2) in two limiting cases: \u03b2 \u2192 0 and \u03b2 \u2192 \u221e. If \u03b2 \u2192 0, we have\nF\u2217k = (I \u2212 \u03b2L)Y\u2217k + o(\u03b2).\nThus, for very small values of \u03b2, the method resembles the nearest neighbour method with the weight matrix W = I \u2212\u03b2L. If there are many points situated more than one hop away from any labelled point, the method cannot produce good classification with very small values of \u03b2. This will be illustrated by the numerical experiments in Section 6.\nNow consider the other case \u03b2 \u2192 \u221e. We shall employ the Blackwell series expansion [7, 31] for the resolvent operator (\u03bbI + L)\u22121 with \u03bb = 1/\u03b2\n(I + \u03b2L)\u22121 = \u03bb(\u03bbI + L)\u22121\n= \u03bb\n(\n1\n\u03bb\n1\nN 11T +H \u2212 \u03bbH2 + ...\n)\n, (16)\nwhere H = (L + 1N 11 T )\u22121 \u2212 1N 11 T is the generalized (group) inverse of the Laplacian. Since the first term in (16) gives the same value for all classes if 1TY\u2217k = 1 TY\u2217l, k 6= l (which is typically the case), the classification will depend on the entries of the matrix H and finally, of the matrix (L+ 1N 11\nT )\u22121. Note that the matrix (L+\u03b111T )\u22121, with a sufficiently small positive \u03b1, determines a proximity measure called accessibility via dense forests. Its properties are listed in [15, Proposition 10]. An interpretation of H in terms of spanning forests can be found in [15, Theorem 3]; see also [26].\nThe accessibility via dense forests violates a natural monotonicity condition, as distinct from (I + \u03b2L)\u22121 with a finite \u03b2. Thus, a better performance of the regularized Laplacian proximity measure with finite values of \u03b2 can be expected.\nFor the sake of comparison, let us analyse the limiting behaviour of the heat kernels. For instance, let us consider the Standard Laplacian heat kernel (6), since it is also based on the Standard Laplacian. In fact, it is immediate to see that the Standard Laplacian heat kernel has the same asymptotic as the Regularized Laplacian kernel. Namely, if t \u2192 0,\nH(t) = exp(\u2212tL) = I \u2212 tL+ o(t).\nSimilar expressions hold for the other heat kernels. Thus, for small values of t, the semi-supervised learning methods based on heat kernels should behave as the nearest neighbour method.\nNext consider the Standard Laplacian heat kernel when t \u2192 \u221e. Recall that the Laplacian L = D\u2212A is a positive definite symmetric matrix. Without the loss of generality, we can denote and rearrange the eigenvalues of the Laplacian as 0 = \u03bb1 \u2264 \u03bb2 \u2264 ... and the corresponding eigenvectors as u1, ..., un. Note that u1 = 1. Thus, we can write\nH(t) = u1u T 1 +\nN \u2211\ni=2\nexp(\u2212\u03bbit)uiu T i .\nWe can see that for large values of t the first term in the above expression is non-informative as in the case of the Regularized Laplacian method and we need to look for the second order term. However, in contrast to the Regularized Laplacian kernel, the second order term exp(\u2212\u03bb2t)u2u T 2 is a rank-one term and cannot in principle give correct classification in the case of more than two classes. The second term of the Regularized Laplacian kernel H is not a rank-one matrix and as mentioned above can be interpreted in terms of proximity measures.\nInria"}, {"heading": "Semi-supervised Learning with Regularized Laplacian 11", "text": ""}, {"heading": "6 Numerical methods and examples", "text": "Let us first discuss various approaches for the numerical computation of the classification functions (2). Broadly speaking, the approaches can be divided into linear algebra methods and optimization methods. One of the basic linear algebra methods is the power iteration method. Similarly to the power iteration method described in [6], we can write\nF\u2217k = (I + \u03b2D \u2212 \u03b2A) \u22121Y\u2217k,\nF\u2217k = (I \u2212 \u03b2(I + \u03b2D) \u22121A)\u22121(I + \u03b2D)\u22121Y\u2217k,\nF\u2217k = (I \u2212 \u03b2(I + \u03b2D) \u22121DD\u22121A)\u22121(I + \u03b2D)\u22121Y\u2217k.\nNow denoting B := \u03b2(I + \u03b2D)\u22121D and C := (I + \u03b2D)\u22121, we can propose the following power iteration method to compute the classification functions\nF (s+1) \u2217k = BD \u22121AF (s) \u2217k + CY\u2217k, s = 0, 1, ... , (17)\nwith F (0) \u2217k = Y\u2217k. Since B is a diagonal matrix with the diagonal entries less than one, the matrix BD\u22121A is substochastic with the spectral radius less than one and the power iterations (17) are convergent. However, for large values of \u03b2 and di, the matrix BD\n\u22121A can be very close to stochastic and hence the convergence rate of the power iterations can be very slow. Therefore, unless the value of \u03b2 is small, we recommend to use the other methods from numerical linear algebra for the solution of linear systems with symmetric matrices (recall that L is a symmetric positive semi-definite matrix in the case of undirected graphs). In particular, we tried the Cholesky decomposition method and the conjugate gradient method. Both methods appeared to be very efficient for the problems with tens of thousands of variables. Actually, the conjugate gradient method can also be viewed as an optimization method for the respective convex quadratic optimization problem such as (1) and (7). A very convenient property of optimization formulations (1) and (7) is that the objective, and consequently, the gradient, can be written in terms of a sum over the edges of the underlying graph. This allows a very simple (and with some software packages even automatic) parallelization of the optimization methods based on the gradient. For instance, we have used the parallel implementation of the gradient based methods provided by the NVIDIA CUDA sparse matrix library (cuSPARSE) [39] and it showed excellent performance.\nLet us now illustrate the Regularized Laplacian method and compare it with some other state of the art semi-supervised learning methods on two datasets: Les Miselables and Wikipedia Mathematical Articles.\nThe first dataset represents the network of interactions between major characters in the novel Les Miserables. If two characters participate in one or more scenes, there is a link between these two characters. We consider the links to be unweighted and undirected. The network of the interactions of Les Miserables characters has been compiled by Knuth [27]. There are 77 nodes and 508 edges in the graph. Using the betweenness based algorithm of Newman and Girvan [30] we obtain 6 clusters which can be identified with the main characters: Valjean (17), Myriel (10), Gavroche (18), Cosette (10), Thenardier (12), Fantine (10), where in brackets we give the number of nodes in the respective cluster. First, we generate randomly (100 times) labeled points (two labeled points per class). In Figure 1 we plot average precision as a function of parameter \u03b2. In [4, 5] it was observed that the PageRank based semi-supervised method (obtained by taking \u03c3 = 0 in (7)) is the only method among a large family of semi-supervised methods which is robust to the choice of the labelled data [3, 4, 5]. Thus, we compare the Regularized Laplacian method with the PageRank based method. As we can see for Figure 1.(a), the performance\nRR n\u00b0 8765"}, {"heading": "12 K. Avrachenkov & P. Chebotarev & A. Mishenin", "text": "of the Regularized Laplacian method is comparable to that of the PageRank based method on Les Miserables dataset. The horizontal line in Figure 1.(a) corresponds to the PageRank based method with the best choice of the regularization parameter or the restart probability in the context of PageRank. Since the Regularized Laplacian method is based on graph Laplacian, we also compare it in Figure 1.(b) with the three heat kernel methods derived from variations of the graph Laplacian. Specifically, we consider the three time-domain kernels based on various Laplacians: Standard Heat kernel (6), Normalized Heat kernel (3), and PageRank Heat kernel (4). For instance, in the case of the Standard Heat kernel the classification functions are given by F\u2217k = H(t)Y\u2217k. It turns out that all the three time-domain heat kernels are very sensitive to the value of the chosen time, t. Even though there are parameter settings that give similar performances of Heat kernel methods and the Regularized Laplacian method, the Regularized Laplacian method has a large plateau for values of \u03b2 where the good performance of the method is assured. Thus, the Regularized Laplacian method is more robust with respect to the parameter setting than the heat kernel methods.\nTo see better the behaviour of the heat kernel methods for large values of t, we have chosen a larger interval for t in Figure 2. The performance of the heat kernel methods degrades quite significantly for large values of t. This is actually predicted by the asymptotics given in Section 5. Since we have more than two classes, the heat kernels with rank-one second order asymptotics are not able to distinguish among the classes. All heat kernel methods as well as the Regularized Laplacian method show a deterioration in performance for small values of t and \u03b2. This was predicted in Section 5, as all the methods start to behave like the nearest neighbour method. In particular, as follows from the asymptotics of Section 5 and can be observed in the figures the Standard Laplacian heat kernel method and the Regularized Laplacian method shows exactly the same performance when t \u2192 0 and \u03b2 \u2192 0.\nIt was observed in [5] that taking labelled data points with large (weighted) degree is typically beneficial for the semi-supervised learning methods. Thus, we now label randomly two points out of three points with maximal degree for each class. The average precision is given in Figure 3.(a). We also test heat kernel based methods with the same labelled points, see Figure 3.(b). One can see that if we choose the labelled points with large degree, the Regularized Laplacian Method outperforms the PageRank based method. Some heat kernel based methods with large degree labelled points also outperform the PageRank based method but their performance is much less stable with respect to the value of parameter t.\nInria"}, {"heading": "Semi-supervised Learning with Regularized Laplacian 13", "text": "RR n\u00b0 8765"}, {"heading": "14 K. Avrachenkov & P. Chebotarev & A. Mishenin", "text": "Next, we consider the second dataset consisting of Wikipedia mathematical articles. This dataset is derived from the English language Wikipedia snapshot (dump) from January 30, 20102. The similarity graph is constructed by a slight modification of the hyper-text graph. Each Wikipedia article typically contains links to other Wikipedia articles which are used to explain specific terms and concepts. Thus, Wikipedia forms a graph whose nodes represent articles and whose edges represent hyper-text inter-article links. The links to special pages (categories, portals, etc.) have been ignored. In the present experiment we did not use the information about the direction of links, so the similarity graph in our experiments is undirected. Then we have built a subgraph with mathematics related articles, a list of which was obtained from \u201cList of mathematics articles\u201d page from the same dump. In the present experiments we have chosen the following three mathematical classes: \u201cDiscrete mathematics\u201d (DM), \u201cMathematical analysis\u201d (MA), \u201cApplied mathematics\u201d (AM). With the help of AMS MSC Classification3 and experts we have classified relatedWikipedia mathematical articles into the three above mentioned classes. As a result, we obtained three imbalanced classes DM (106), MA (368) and AM (435). The subgraph induced by these three topics is connected and contains 909 articles. Then, the similarity matrix A is just the adjacency matrix of this subgraph.\nFirst, we have chosen uniformly at random 100 times 5 labeled nodes for each class. The average precisions corresponding to the Regularized Laplacian method and the PageRank based method are plotted in Figure 4.(a). We also provide the results for the three heat kernel based methods in Figure 4.(b). As one can see, the results of Wikipedia Mathematical articles dataset are consistent with the results of Les Miserables dataset.\nThen, for each class out of 10 data points with largest degrees we choose 5 points and average the results. The average precisions for the Regularized Laplacian method, PageRank based method and for the three heat kernel based methods are plotted in Figure 5. The results are again consistent with the corresponding results for Les Miserables dataset. We would like to mention that for the computations in the Wiki Math dataset with many parameter settings and extensive averaging using NVIDIA CUDA sparse matrix library (cuSPARSE) [39] were noticeably faster than using numpy.linalg.solve calling LAPACK routine _gesv.\nFinally, we would like to recall from Subsection 4.5 that a good value of \u03b2 can be provided\n2http://download.wikimedia.org/enwiki/20100130 3http://www.ams.org/mathscinet/msc/msc2010.html\nInria"}, {"heading": "Semi-supervised Learning with Regularized Laplacian 15", "text": "by the ratio \u03c321/\u03c3 2 2 , where \u03c3 2 1 is the variance related to the data points and \u03c3 2 2 is the variance related to the paired comparison between points. We can argue that \u03c321 is naturally large and the paired comparisons between points can be performed with much more certainty, and hence, \u03c322 is small. This gives a statistical explanation why it is good to take relatively large values for the parameter \u03b2 in the Regularized Laplacian method."}, {"heading": "7 Conclusions", "text": "We have studied in detail the semi-supervised learning method based on the Regularized Laplacian. The method admits both linear algebraic and optimization formulations. The optimization formulation appears to be particularly well suited for parallel implementation. We have provided various interpretations and proximity-distance properties of the Regularized Laplacian graph kernel. We have also shown that the method is related to the Scheff\u00e9 linear statistical model. The method was tested and compared with the other state of the art semi-supervised learning methods on two datasets. The results from the two datasets are consistent. In particular, we can conclude that the Regularized Laplacian method is comparable in performance with the PageRank based method and outperforms the related heat kernel based methods in terms of robustness.\nSeveral interesting research directions remain open for investigation. It will be interesting to compare the Regularized Laplacian method with the other semi-supervised methods on a very large dataset. We are currently working in this direction. We observe that there is a large plateau of \u03b2 values for which the Regularized Laplacian method performs very well. It will be very useful to characterize this plateau analytically. Also, it will be interesting to understand analytically why the Regularized Laplacian method performs better when the labelled points with large degree are chosen."}, {"heading": "16 K. Avrachenkov & P. Chebotarev & A. Mishenin", "text": "[2] Andersen, R., Chung, F., and Lang, K. (2006). \u201cLocal graph partitioning using pagerank vectors\u201d. In Proceedings of IEEE FOCS 2006, pp. 475\u2013486.\n[3] Avrachenkov, K., Dobrynin, V., Nemirovsky, D., Pham, S.K., and Smirnova, E. (2008). \u201cPagerank based clustering of hypertext document collections\u201d. In Proceedings of ACM SIGIR 2008, pp. 873\u2013874.\n[4] Avrachenkov, K., Gon\u00e7alves, P., Mishenin, A., and Sokol, M. (2012). \u201cGeneralized optimization framework for graph-based semi-supervised learning\u201d. In Proceedings of SIAM Conference on Data Mining (SDM 2012) (Vol. 9).\n[5] Avrachenkov, K., Gon\u00e7alves, P., and Sokol, M. (2013). \u201cOn the choice of kernel and labelled data in semi-supervised learning methods\u201d. In Algorithms and Models for the Web Graph, WAW 2013, also LNCS, Vol. 8305, pp. 56\u201367.\n[6] Avrachenkov, K., Mazalov, V. and Tsynguev, B. (2015) \u201cBeta Current Flow Centrality for Weighted Networks\u201d. In Proceedings of the 4th International Conference on Computational Social Networks (CSoNet 2015), also LNCS 9197, Chapter 19.\n[7] Blackwell, D. (1962). \u201cDiscrete dynamic programming\u201d. The Annals of Mathematical Statistics, 33, pp. 719\u2013726.\n[8] Callut, J., Fran\u00e7oisse, K., Saerens, M., and Dupont, P. (2008) \u201cSemi-supervised classification from discriminative random walks\u201d. In Machine Learning and Knowledge Discovery in Databases European Conference, ECML PKDD 2008, Antwerp, Belgium, September 15\u2013 19, 2008, Proceedings, Part I, ser. Lecture Notes in Computer Science / Lecture Notes on Artificial Intelligence, W. Daelemans, B. Goethals, and K. Morik, Eds., vol. 5211. BerlinHeidelberg: Springer, pp. 162\u2013177.\n[9] Chapelle, O., Sch\u00f6lkopf, B. and Zien A. (2006). Semi-supervised learning, MIT Press.\n[10] Chebotarev, P. Y. (1994). \u201cAggregation of preferences by the generalized row sum method\u201d. Mathematical Social Sciences, 27, pp. 293\u2013320.\n[11] Chebotarev, P. (2008). \u201cSpanning forests and the golden ratio\u201d. Discrete Applied Mathematics, 156(5), pp. 813\u2013821.\n[12] P. Chebotarev (2011). \u201cThe graph bottleneck identity\u201d. Advances in Applied Mathematics, 47(3), pp. 403\u2013413.\n[13] Chebotarev, P. Yu., and Shamis, E. V. (1997). \u201cThe matrix-forest theorem and measuring relations in small social groups\u201d. Automation and Remote Control, 58(9), pp. 1505\u20131514.\n[14] Chebotarev, P. Yu., and Shamis, E. V. (1998). \u201cOn a duality between metrics and \u03a3proximities\u201d. Automation and Remote Control, 59(4), pp. 608\u2013612.\n[15] Chebotarev, P. Yu., and Shamis, E. V. (1998). \u201cOn proximity measures for graph vertices\u201d. Automation and Remote Control, 59(10), pp. 1443\u20131459.\n[16] Chebotarev, P. Yu., and Shamis, E. V. (2000). \u201cThe forest metrics of a graph and their properties\u201d. Automation and Remote Control, 61(8), pp. 1364\u20131373.\n[17] Chung, F., and Yau, S. T. (1999). \u201cCoverings, heat kernels and spanning trees\u201d. Electronic Journal of Combinatorics, 6, R12.\nInria"}, {"heading": "Semi-supervised Learning with Regularized Laplacian 17", "text": "[18] Chung, F., and Yau, S. T. (2000). \u201cDiscrete Green\u2019s functions\u201d. Journal of Combinatorial Theory, Series A, 91(1), pp. 191\u2013214.\n[19] Chung, F. (2007). \u201cThe heat kernel as the pagerank of a graph\u201d. PNAS, 105(50), pp. 19735\u2013 19740.\n[20] Chung, F. (2009). \u201cA local graph partitioning algorithm using heat kernel pagerank\u201d. In Proceedings of WAW 2009, LNCS 5427, pp. 62\u201375.\n[21] Critchley, F. (1988) \u201cOn certain linear mappings between inner-product and squareddistance matrices\u201d. Linear Algebra and its Applications, 105, pp. 91\u2013107.\n[22] Deza, M. M., and Laurent, M. (1997) Geometry of Cuts and Metrics, volume 15 of Algorithms and Combinatorics. Berlin: Springer.\n[23] Fouss, F., Francoisse, K., Yen, L., Pirotte A., and Saerens, M. (2012) \u201cAn experimental investigation of kernels on graphs for collaborative recommendation and semisupervised classification\u201d. Neural Networks, 31, pp. 53\u201372.\n[24] Dorugade, A. V. (2014) \u201cNew ridge parameters for ridge regression\u201d. Journal of the Association of Arab Universities for Basic and Applied Sciences, 15, pp. 94\u201399.\n[25] Fouss, F., Yen, L., Pirotte, A., and Saerens, M. (2006) \u201cAn experimental investigation of graph kernels on a collaborative recommendation task\u201d. In Sixth International Conference on Data Mining (ICDM\u201906), pp. 863\u2013868.\n[26] Kirkland, S. J., Neumann, M., and Shader, B. L. (1997) \u201cDistances in weighted trees and group inverse of Laplacian matrices\u201d. SIAM J. Matrix Anal. Appl., 18, pp.827\u2013841.\n[27] Knuth, D. E. (1993). The Stanford GraphBase: a platform for combinatorial computing. ACM, New York, NY, USA.\n[28] Kondor, R. I., and Lafferty, J. (2002). \u201cDiffusion kernels on graphs and other discrete input spaces\u201d. In Proceedings of ICML, 2, pp. 315\u2013322.\n[29] Muniz, G. and Kibria, B. M. G. (2009) \u201cOn some ridge regression estimators: An empirical comparisons\u201d. Communications in Statistics \u2013 Simulation and Computation, 38(3), pp. 621\u2013 630.\n[30] Newman, M. E. J. and Girvan, M. (2004). \u201cFinding and evaluating community structure in networks\u201d. Phys. Rev. E, 69(2):026113.\n[31] Puterman, M. L. (1994). Markov decision processes: Discrete stochastic dynamic programming, John Wiley & Sons.\n[32] Scheff\u00e9, H. (1952) \u201cAn analysis of variance for paired comparisons\u201d. Journal of the American Statistical Association, 47(259), pp. 381-400.\n[33] Smola, A. J., and Kondor, R. I. (2003) \u201cKernels and regularization of graphs\u201d. In Proceedings of the 16th Annual Conference on Learning Theory, pp. 144\u2013158.\n[34] Yen, L., Saerens, M., Mantrach, A., and Shimbo, M. (2008) \u201cA family of dissimilarity measures between nodes generalizing both the shortest-path and the commutetime distances\u201d. In 14th ACM SIGKDD Intern. Conf. on Knowledge Discovery and Data Mining, pp. 785\u2013793.\nRR n\u00b0 8765"}, {"heading": "18 K. Avrachenkov & P. Chebotarev & A. Mishenin", "text": "[35] Zhou, D., and Burges, C. J. C. (2007) \u201cSpectral clustering and transductive learning with multiple views\u201d. In Proceedings of ICML 2007, pp. 1159\u20131166.\n[36] Zhou, D., Bousquet, O., Navin Lal, T., Weston, J., Sch\u00f6lkopf, B. (2004). \u201cLearning with local and global consistency\u201d. In: Advances in Neural Information Processing Systems, 16, pp. 321\u2013328.\n[37] Zhu, X., Ghahramani, Z., and Lafferty, J. (2003). \u201cSemi-supervised learning using Gaussian fields and harmonic functions\u201d. In Proceedings of ICML 2003, Vol. 3, pp. 912\u2013919.\n[38] Zhu, X. (2005). \u201cSemi-supervised learning literature survey\u201d. University of WisconsinMadison Research Report TR 1530.\n[39] The NVIDIA CUDA Sparse Matrix library (cuSPARSE), https://developer.nvidia.com/cuSPARSE\nInria"}, {"heading": "Semi-supervised Learning with Regularized Laplacian 19", "text": ""}, {"heading": "Contents", "text": ""}, {"heading": "1 Introduction 3", "text": ""}, {"heading": "2 Notations and method formulation 4", "text": ""}, {"heading": "3 Related approaches 5", "text": "3.1 Relation to heat kernels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 3.2 Relation to the generalized semi-supervised learning\nmethod . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6"}, {"heading": "4 Properties and interpretations of the Regularized Laplacian method 6", "text": "4.1 Discrete-time random walk interpretation . . . . . . . . . . . . . . . . . . . . . . 6 4.2 Continuous-time random walk interpretation . . . . . . . . . . . . . . . . . . . . 7 4.3 Proximity and distance properties . . . . . . . . . . . . . . . . . . . . . . . . . . 8 4.4 Matrix forest characterization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 4.5 Statistical characterization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9"}, {"heading": "5 Limiting cases 10", "text": ""}, {"heading": "6 Numerical methods and examples 11", "text": ""}, {"heading": "7 Conclusions 15", "text": "RR n\u00b0 8765\nRESEARCH CENTRE SOPHIA ANTIPOLIS \u2013 M\u00c9DITERRAN\u00c9E\n2004 route des Lucioles - BP 93 06902 Sophia Antipolis Cedex\nPublisher Inria Domaine de Voluceau - Rocquencourt BP 105 - 78153 Le Chesnay Cedex inria.fr\nISSN 0249-6399\nThis figure \"logo-inria.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/1508.04906v1\nThis figure \"pagei.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/1508.04906v1\nThis figure \"rrpage1.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/1508.04906v1"}], "references": [{"title": "Spanning forests of a digraph and their applications", "author": ["R.P. Agaev", "P.Y. Chebotarev"], "venue": "Automation and Remote Control,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Local graph partitioning using pagerank vectors", "author": ["R. Andersen", "F. Chung", "K. Lang"], "venue": "In Proceedings of IEEE FOCS", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Pagerank based clustering of hypertext document collections", "author": ["K. Avrachenkov", "V. Dobrynin", "D. Nemirovsky", "S.K. Pham", "E. Smirnova"], "venue": "In Proceedings of ACM SI- GIR", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Generalized optimization framework for graph-based semi-supervised learning", "author": ["K. Avrachenkov", "P. Gon\u00e7alves", "A. Mishenin", "M. Sokol"], "venue": "In Proceedings of SIAM Conference on Data Mining (SDM", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "On the choice of kernel and labelled data in semi-supervised learning methods", "author": ["K. Avrachenkov", "P. Gon\u00e7alves", "M. Sokol"], "venue": "WAW 2013, also LNCS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Beta Current Flow Centrality for Weighted Networks", "author": ["K. Avrachenkov", "V. Mazalov", "B. Tsynguev"], "venue": "In Proceedings of the 4th International Conference on Computational Social Networks (CSoNet 2015),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Discrete dynamic programming", "author": ["D. Blackwell"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1962}, {"title": "Semi-supervised classification from discriminative random walks", "author": ["J. Callut", "K. Fran\u00e7oisse", "M. Saerens", "P. Dupont"], "venue": "In Machine Learning and Knowledge Discovery in Databases European Conference,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Aggregation of preferences by the generalized row sum method", "author": ["P.Y. Chebotarev"], "venue": "Mathematical Social Sciences,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "Spanning forests and the golden ratio", "author": ["P. Chebotarev"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "The graph bottleneck identity", "author": ["P. Chebotarev"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "The matrix-forest theorem and measuring relations in small social groups", "author": ["Chebotarev", "P. Yu", "E.V. Shamis"], "venue": "Automation and Remote Control,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "On a duality between metrics and \u03a3proximities", "author": ["Chebotarev", "P. Yu", "E.V. Shamis"], "venue": "Automation and Remote Control,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "On proximity measures for graph vertices", "author": ["Chebotarev", "P. Yu", "E.V. Shamis"], "venue": "Automation and Remote Control,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1998}, {"title": "The forest metrics of a graph and their properties", "author": ["Chebotarev", "P. Yu", "E.V. Shamis"], "venue": "Automation and Remote Control,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Coverings, heat kernels and spanning trees", "author": ["F. Chung", "S.T. Yau"], "venue": "Electronic Journal of Combinatorics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Discrete Green\u2019s functions", "author": ["F. Chung", "S.T. Yau"], "venue": "Journal of Combinatorial Theory, Series A,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "The heat kernel as the pagerank of a graph", "author": ["F. Chung"], "venue": "PNAS, 105(50),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "A local graph partitioning algorithm using heat kernel pagerank", "author": ["F. Chung"], "venue": "In Proceedings of WAW 2009,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "On certain linear mappings between inner-product and squareddistance matrices", "author": ["F. Critchley"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1988}, {"title": "Geometry of Cuts and Metrics, volume 15 of Algorithms and Combinatorics", "author": ["M.M. Deza", "M. Laurent"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "An experimental investigation of kernels on graphs for collaborative recommendation and semisupervised classification", "author": ["F. Fouss", "K. Francoisse", "L. Yen", "Pirotte A", "M. Saerens"], "venue": "Neural Networks,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "New ridge parameters for ridge regression", "author": ["A.V. Dorugade"], "venue": "Journal of the Association of Arab Universities for Basic and Applied Sciences,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "An experimental investigation of graph kernels on a collaborative recommendation task", "author": ["F. Fouss", "L. Yen", "A. Pirotte", "M. Saerens"], "venue": "In Sixth International Conference on Data Mining (ICDM\u201906),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2006}, {"title": "Distances in weighted trees and group inverse of Laplacian matrices", "author": ["S.J. Kirkland", "M. Neumann", "B.L. Shader"], "venue": "SIAM J. Matrix Anal. Appl.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1997}, {"title": "The Stanford GraphBase: a platform for combinatorial computing", "author": ["D.E. Knuth"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1993}, {"title": "Diffusion kernels on graphs and other discrete input spaces", "author": ["R.I. Kondor", "J. Lafferty"], "venue": "In Proceedings of ICML,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2002}, {"title": "On some ridge regression estimators: An empirical comparisons", "author": ["G. Muniz", "B.M.G. Kibria"], "venue": "Communications in Statistics \u2013 Simulation and Computation,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Finding and evaluating community structure in networks", "author": ["M.E.J. Newman", "M. Girvan"], "venue": "Phys. Rev. E,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}, {"title": "Markov decision processes: Discrete stochastic dynamic programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1994}, {"title": "An analysis of variance for paired comparisons", "author": ["H. Scheff\u00e9"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1952}, {"title": "Kernels and regularization of graphs", "author": ["A.J. Smola", "R.I. Kondor"], "venue": "In Proceedings of the 16th Annual Conference on Learning Theory,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2003}, {"title": "A family of dissimilarity measures between nodes generalizing both the shortest-path and the commutetime distances", "author": ["L. Yen", "M. Saerens", "A. Mantrach", "M. Shimbo"], "venue": "In 14th ACM SIGKDD Intern. Conf. on Knowledge Discovery and Data Mining, pp. 785\u2013793", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2008}, {"title": "Spectral clustering and transductive learning with multiple views", "author": ["D. Zhou", "C.J.C. Burges"], "venue": "In Proceedings of ICML", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Learning with local and global consistency", "author": ["D. Zhou", "O. Bousquet", "T. Navin Lal", "J. Weston", "B. Sch\u00f6lkopf"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2004}, {"title": "Semi-supervised learning using Gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J. Lafferty"], "venue": "In Proceedings of ICML 2003,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2003}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "University of Wisconsin- Madison Research Report TR 1530", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2005}], "referenceMentions": [{"referenceID": 35, "context": "The work [37] seems to be the first work where the graph-based semi-supervised learning was introduced.", "startOffset": 9, "endOffset": 13}, {"referenceID": 35, "context": "The authors of [37] formulated the semi-supervised learning method as a constrained optimization problem involving graph Laplacian.", "startOffset": 15, "endOffset": 19}, {"referenceID": 34, "context": "Then, in [36, 35] the authors proposed optimization formulations based on several variations of the graph Laplacian.", "startOffset": 9, "endOffset": 17}, {"referenceID": 33, "context": "Then, in [36, 35] the authors proposed optimization formulations based on several variations of the graph Laplacian.", "startOffset": 9, "endOffset": 17}, {"referenceID": 3, "context": "In [4] a unifying optimization framework was proposed which gives as particular cases the methods of [35] and [36].", "startOffset": 3, "endOffset": 6}, {"referenceID": 33, "context": "In [4] a unifying optimization framework was proposed which gives as particular cases the methods of [35] and [36].", "startOffset": 101, "endOffset": 105}, {"referenceID": 34, "context": "In [4] a unifying optimization framework was proposed which gives as particular cases the methods of [35] and [36].", "startOffset": 110, "endOffset": 114}, {"referenceID": 3, "context": "In addition, the general framework in [4] gives as a particular case an interesting PageRank based method, which provides robust classification with respect to the choice of the labelled points [3, 5].", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": "In addition, the general framework in [4] gives as a particular case an interesting PageRank based method, which provides robust classification with respect to the choice of the labelled points [3, 5].", "startOffset": 194, "endOffset": 200}, {"referenceID": 4, "context": "In addition, the general framework in [4] gives as a particular case an interesting PageRank based method, which provides robust classification with respect to the choice of the labelled points [3, 5].", "startOffset": 194, "endOffset": 200}, {"referenceID": 1, "context": "We would like to note that the local graph partitioning problem [2, 20] can be related to graph-based semi-supervised learning.", "startOffset": 64, "endOffset": 71}, {"referenceID": 18, "context": "We would like to note that the local graph partitioning problem [2, 20] can be related to graph-based semi-supervised learning.", "startOffset": 64, "endOffset": 71}, {"referenceID": 21, "context": "An interested reader can find more details about various semi-supervised learning methods in the surveys and books [9, 23, 38].", "startOffset": 115, "endOffset": 126}, {"referenceID": 36, "context": "An interested reader can find more details about various semi-supervised learning methods in the surveys and books [9, 23, 38].", "startOffset": 115, "endOffset": 126}, {"referenceID": 11, "context": "To the best of our knowledge, the idea of using Regularized Laplacian and its kernel for measuring proximity in graphs and application to mathematical sociology goes back to the works [13, 15].", "startOffset": 184, "endOffset": 192}, {"referenceID": 13, "context": "To the best of our knowledge, the idea of using Regularized Laplacian and its kernel for measuring proximity in graphs and application to mathematical sociology goes back to the works [13, 15].", "startOffset": 184, "endOffset": 192}, {"referenceID": 21, "context": "In [23] the authors compared experimentally many graph-based semi-supervised learning methods on several datasets and their conclusion was that the semisupervised learning method based on the Regularized Laplacian kernel demonstrates one of the best performances on nearly all datasets.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "In [8] the authors studied a semi-supervised learning method based on the Normalized Laplacian graph kernel which also shows good performance.", "startOffset": 3, "endOffset": 6}, {"referenceID": 35, "context": "In fact, the Regularized Laplacian method can be regarded as a Lagrangian relaxation of the method proposed in [37].", "startOffset": 111, "endOffset": 115}, {"referenceID": 35, "context": "Of course, this is a more flexible formulation, since by choosing an appropriate value for the Lagrange multiplier one can always retrieve the method of [37] as a particular case.", "startOffset": 153, "endOffset": 157}, {"referenceID": 26, "context": "(2) The matrix Q\u03b2 = (I + \u03b2L) \u22121 is known as Regularized Laplacian kernel of the graph [28, 33] and can be related to the matrix forest theorems [13, 1] and stochastic matrices [1].", "startOffset": 86, "endOffset": 94}, {"referenceID": 31, "context": "(2) The matrix Q\u03b2 = (I + \u03b2L) \u22121 is known as Regularized Laplacian kernel of the graph [28, 33] and can be related to the matrix forest theorems [13, 1] and stochastic matrices [1].", "startOffset": 86, "endOffset": 94}, {"referenceID": 11, "context": "(2) The matrix Q\u03b2 = (I + \u03b2L) \u22121 is known as Regularized Laplacian kernel of the graph [28, 33] and can be related to the matrix forest theorems [13, 1] and stochastic matrices [1].", "startOffset": 144, "endOffset": 151}, {"referenceID": 0, "context": "(2) The matrix Q\u03b2 = (I + \u03b2L) \u22121 is known as Regularized Laplacian kernel of the graph [28, 33] and can be related to the matrix forest theorems [13, 1] and stochastic matrices [1].", "startOffset": 144, "endOffset": 151}, {"referenceID": 0, "context": "(2) The matrix Q\u03b2 = (I + \u03b2L) \u22121 is known as Regularized Laplacian kernel of the graph [28, 33] and can be related to the matrix forest theorems [13, 1] and stochastic matrices [1].", "startOffset": 176, "endOffset": 179}, {"referenceID": 15, "context": "1 Relation to heat kernels The authors of [17, 18] first introduced and studied the properties of the heat kernel based on the normalized Laplacian.", "startOffset": 42, "endOffset": 50}, {"referenceID": 16, "context": "1 Relation to heat kernels The authors of [17, 18] first introduced and studied the properties of the heat kernel based on the normalized Laplacian.", "startOffset": 42, "endOffset": 50}, {"referenceID": 17, "context": "Then, in [19] the PageRank heat kernel was introduced", "startOffset": 9, "endOffset": 13}, {"referenceID": 18, "context": "In [20] the PageRank heat kernel was applied to local graph partitioning.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "In [28] the heat kernel based on the standard Laplacian H(t) = exp(\u2212tL), (6)", "startOffset": 3, "endOffset": 7}, {"referenceID": 35, "context": "Then, in [37] the authors proposed a semi-supervised learning method based on the solution of a heat diffusion equation with Dirichlet boundary conditions.", "startOffset": 9, "endOffset": 13}, {"referenceID": 35, "context": "Equivalently, the method of [37] can be viewed as the minimization of the second term in (1) with the values of the classification functions F\u2217k fixed on the labelled points.", "startOffset": 28, "endOffset": 32}, {"referenceID": 35, "context": "Thus, the proposed approach (1) is more general as it can be viewed as a Lagrangian relaxation of [37].", "startOffset": 98, "endOffset": 102}, {"referenceID": 35, "context": "The results of the method in [37] can be retrieved with a particular choice of the regularization parameter.", "startOffset": 29, "endOffset": 33}, {"referenceID": 3, "context": "2 Relation to the generalized semi-supervised learning method In [4] the authors proposed a generalized optimization framework for graph based semi-supervised learning methods", "startOffset": 65, "endOffset": 68}, {"referenceID": 33, "context": "In particular, with \u03c3 = 1 we retrieve the transductive semi-supervised learning method [35], with \u03c3 = 1/2 we retrieve the semi-supervised learning with local and global consistency [36] and with \u03c3 = 0 we retrieve the PageRank based method [3].", "startOffset": 87, "endOffset": 91}, {"referenceID": 34, "context": "In particular, with \u03c3 = 1 we retrieve the transductive semi-supervised learning method [35], with \u03c3 = 1/2 we retrieve the semi-supervised learning with local and global consistency [36] and with \u03c3 = 0 we retrieve the PageRank based method [3].", "startOffset": 181, "endOffset": 185}, {"referenceID": 2, "context": "In particular, with \u03c3 = 1 we retrieve the transductive semi-supervised learning method [35], with \u03c3 = 1/2 we retrieve the semi-supervised learning with local and global consistency [36] and with \u03c3 = 0 we retrieve the PageRank based method [3].", "startOffset": 239, "endOffset": 242}, {"referenceID": 12, "context": "Q\u03b2 determines a positive 1-proximity measure [14] s(i, j) := q \u03b2 ij , i.", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": ", it satisfies [13] the following conditions: (1) for any i \u2208 V, \u2211 k\u2208V q \u03b2 ik = 1 and (2) for any i, j, k \u2208 V, q ji + q \u03b2 jk \u2212 q \u03b2 ik \u2264 q \u03b2 jj with a strict inequality whenever i = k and i 6= j (the triangle inequality for proximities).", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "This implies [14] the following two important properties: (a) q ii > q \u03b2 ij for all i, j \u2208 V such that i 6= j (egocentrism property); (b) \u03c1\u03b2ij := \u03b2(q \u03b2 ii + q \u03b2 jj \u2212 q \u03b2 ij \u2212 q \u03b2 ji) is 1 a distance on V.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "The distances \u03c1\u03b2ij have a twofold connection with the resistance distance \u03c1\u0303ij on G [16].", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "An interested reader can find more properties of the proximity measures determined by Q\u03b2 in [13].", "startOffset": 92, "endOffset": 96}, {"referenceID": 10, "context": "Furthermore, every Q\u03b2, \u03b2 > 0 determines a transitional measure on V, which means [12] that: q ij q \u03b2 jk \u2264 q \u03b2 ik q \u03b2 jj for all i, j, k \u2208 V with q \u03b2 ij q \u03b2 jk = q \u03b2 ik q \u03b2 jj if and only if every path in G from i to k visits j.", "startOffset": 81, "endOffset": 85}, {"referenceID": 11, "context": "4 Matrix forest characterization By the matrix forest theorem [13, 1], each entry q ij of Q\u03b2 is equal to the specific weight of the spanning rooted forests that connect node i to node j in the weighted graph G whose combinatorial Laplacian is L.", "startOffset": 62, "endOffset": 69}, {"referenceID": 0, "context": "4 Matrix forest characterization By the matrix forest theorem [13, 1], each entry q ij of Q\u03b2 is equal to the specific weight of the spanning rooted forests that connect node i to node j in the weighted graph G whose combinatorial Laplacian is L.", "startOffset": 62, "endOffset": 69}, {"referenceID": 9, "context": "Let us mention a closely related interpretation of the Regularized Laplacian kernel Q\u03b2 in terms of information dissemination [11].", "startOffset": 125, "endOffset": 129}, {"referenceID": 19, "context": "the cosine law [21] and the inverse covariance mapping [22, Section 5.", "startOffset": 15, "endOffset": 19}, {"referenceID": 30, "context": "Let the result of i in a comparison with j obey the Scheff\u00e9 linear statistical model [32]", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": ", [24, 29] and the references therein.", "startOffset": 2, "endOffset": 10}, {"referenceID": 27, "context": ", [24, 29] and the references therein.", "startOffset": 2, "endOffset": 10}, {"referenceID": 6, "context": "We shall employ the Blackwell series expansion [7, 31] for the resolvent operator (\u03bbI + L) with \u03bb = 1/\u03b2 (I + \u03b2L) = \u03bb(\u03bbI + L) = \u03bb (", "startOffset": 47, "endOffset": 54}, {"referenceID": 29, "context": "We shall employ the Blackwell series expansion [7, 31] for the resolvent operator (\u03bbI + L) with \u03bb = 1/\u03b2 (I + \u03b2L) = \u03bb(\u03bbI + L) = \u03bb (", "startOffset": 47, "endOffset": 54}, {"referenceID": 24, "context": "An interpretation of H in terms of spanning forests can be found in [15, Theorem 3]; see also [26].", "startOffset": 94, "endOffset": 98}, {"referenceID": 5, "context": "Similarly to the power iteration method described in [6], we can write", "startOffset": 53, "endOffset": 56}, {"referenceID": 25, "context": "The network of the interactions of Les Miserables characters has been compiled by Knuth [27].", "startOffset": 88, "endOffset": 92}, {"referenceID": 28, "context": "Using the betweenness based algorithm of Newman and Girvan [30] we obtain 6 clusters which can be identified with the main characters: Valjean (17), Myriel (10), Gavroche (18), Cosette (10), Thenardier (12), Fantine (10), where in brackets we give the number of nodes in the respective cluster.", "startOffset": 59, "endOffset": 63}, {"referenceID": 3, "context": "In [4, 5] it was observed that the PageRank based semi-supervised method (obtained by taking \u03c3 = 0 in (7)) is the only method among a large family of semi-supervised methods which is robust to the choice of the labelled data [3, 4, 5].", "startOffset": 3, "endOffset": 9}, {"referenceID": 4, "context": "In [4, 5] it was observed that the PageRank based semi-supervised method (obtained by taking \u03c3 = 0 in (7)) is the only method among a large family of semi-supervised methods which is robust to the choice of the labelled data [3, 4, 5].", "startOffset": 3, "endOffset": 9}, {"referenceID": 2, "context": "In [4, 5] it was observed that the PageRank based semi-supervised method (obtained by taking \u03c3 = 0 in (7)) is the only method among a large family of semi-supervised methods which is robust to the choice of the labelled data [3, 4, 5].", "startOffset": 225, "endOffset": 234}, {"referenceID": 3, "context": "In [4, 5] it was observed that the PageRank based semi-supervised method (obtained by taking \u03c3 = 0 in (7)) is the only method among a large family of semi-supervised methods which is robust to the choice of the labelled data [3, 4, 5].", "startOffset": 225, "endOffset": 234}, {"referenceID": 4, "context": "In [4, 5] it was observed that the PageRank based semi-supervised method (obtained by taking \u03c3 = 0 in (7)) is the only method among a large family of semi-supervised methods which is robust to the choice of the labelled data [3, 4, 5].", "startOffset": 225, "endOffset": 234}, {"referenceID": 4, "context": "It was observed in [5] that taking labelled data points with large (weighted) degree is typically beneficial for the semi-supervised learning methods.", "startOffset": 19, "endOffset": 22}], "year": 2015, "abstractText": "We study a semi-supervised learning method based on the similarity graph and Regularized Laplacian. We give convenient optimization formulation of the Regularized Laplacian method and establish its various properties. In particular, we show that the kernel of the method can be interpreted in terms of discrete and continuous time random walks and possesses several important properties of proximity measures. Both optimization and linear algebra methods can be used for efficient computation of the classification functions. We demonstrate on numerical examples that the Regularized Laplacian method is competitive with respect to the other state of the art semi-supervised learning methods. Key-words: Semi-supervised learning, Graph-based learning, Regularized Laplacian, Proximity measure, Wikipedia article classification \u2217 Corresponding author. K. Avrachenkov is with Inria Sophia Antipolis, 2004 Route des Lucioles, 06902, Sophia Antipolis, France k.avrachenkov@inria.fr \u2020 P. Chebotarev is with Trapeznikov Institute of Control Sciences of the Russian Academy of Sciences, 65 Profsoyuznaya Str., Moscow, 117997, Russia \u2021 A. Mishenin is with St. Petersburg State University, Faculty of Applied Mathematics and Control Processes, Peterhof, 198504, Russia \u00a7 This work was partially supported by Campus France, Alcatel-Lucent Inria Joint Lab, EU Project Congas FP7-ICT-2011-8-317672, and RFBR grant No. 13-07-00990. L\u2019Apprentissage Semi-supervis\u00e9 avec Laplacian R\u00e9gularis\u00e9 R\u00e9sum\u00e9 : Nous \u00e9tudions une m\u00e9thode d\u2019apprentissage semi-supervis\u00e9, bas\u00e9 sur le graphe de similarit\u00e9 et Laplacian r\u00e9gularis\u00e9. Nous formalisons la m\u00e9thode comme un probl\u00e8me d\u2019optimisation convexe et quadratique et nous \u00e9tablissons ses diverses propri\u00e9t\u00e9s. En particulier, nous montrons que le noyau de la m\u00e9thode peut \u00eatre interpr\u00e9t\u00e9 en termes des marches al\u00e9atoires en temps discret et continu et poss\u00e8de plusieurs propri\u00e9t\u00e9s importantes des mesures de proximit\u00e9. Les techniques d\u2019optimisation ainsi que les techniques d\u2019alg\u00e9bre lin\u00e9aire peuvent \u00eatre utilis\u00e9 pour un calcul efficace des fonctions de classification. Nous d\u00e9montrons sur des exemples num\u00e9riques que la m\u00e9thode de Laplacian r\u00e9gularis\u00e9 est concurrentiel par rapport aux autres \u00e9tat de l\u2019art m\u00e9thodes d\u2019apprentissage semi-supervis\u00e9. Mots-cl\u00e9s : Apprentissage Semi-supervis\u00e9, Apprentissage bas\u00e9 sur le graphe de similarit\u00e9, Laplacian r\u00e9gularis\u00e9, mesure de proximit\u00e9, classification des articles Wikipedia Semi-supervised Learning with Regularized Laplacian 3", "creator": "LaTeX with hyperref package"}}}