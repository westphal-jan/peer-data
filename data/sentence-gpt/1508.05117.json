{"id": "1508.05117", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Aug-2015", "title": "The backtracking survey propagation algorithm for solving random K-SAT problems", "abstract": "Satisfiability of random Boolean expressions built from many clauses with K variables per clause (random K-satisfiability) is a fundamental problem in combinatorial discrete optimization. Here we study random K-satisfiability for K = 3 and K = 4 by the Backtracking Survey Propagation algorithm. This algorithm is able to find, in a time linear in the problem size, solutions within a region never reached before, very close to SAT-UNSAT threshold, and even beyond the freezing threshold.\n\n\nIn this paper, we analyze the effects of the algorithm on C and K values in C (unlikelihoods in all other optimization operations, with random K-satisfiability).\nIn the analysis, we identify a large-scale random k-satisfiability effect in linear-in and exponential K-satisfiability (for example, 0.5 percent of K's is positive). This effect is the result of a small-scale random k-satisfiability effect (with a linear factor of 1) used to construct an exponential K-satisfiability function using the Bayesian General Relativity Principle.\nIn addition to linear LPT, we investigate the general relationship between K and the LPT on multiple regression methods in this paper.\nTo test the probability of each parameter of K, we perform a Bayesian regression, which is a linear k-satisfiability function. In our results, we also report that the number of possible K values in all K-satisfiability can be calculated with a Bayesian General Relativity Principle (BGC). We also report that the K-satisfiability function is also able to be calculated on multiple regression methods, in addition to that of the Bayesian General Relativity Principle.", "histories": [["v1", "Thu, 20 Aug 2015 20:41:29 GMT  (172kb)", "http://arxiv.org/abs/1508.05117v1", "9 pages, 9 figures"], ["v2", "Tue, 13 Oct 2015 17:00:45 GMT  (88kb,D)", "http://arxiv.org/abs/1508.05117v2", "10 pages, 9 figures"], ["v3", "Tue, 19 Apr 2016 22:26:58 GMT  (104kb,D)", "http://arxiv.org/abs/1508.05117v3", "10 pages, 9 figures. v2: data largely improved and manuscript rewritten"], ["v4", "Thu, 6 Oct 2016 07:37:19 GMT  (114kb,D)", "http://arxiv.org/abs/1508.05117v4", "11 pages, 10 figures. v2: data largely improved and manuscript rewritten"]], "COMMENTS": "9 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.CC cs.AI cs.DS", "authors": ["raffaele marino", "giorgio parisi", "federico ricci-tersenghi"], "accepted": false, "id": "1508.05117"}, "pdf": {"name": "1508.05117.pdf", "metadata": {"source": "CRF", "title": "The Backtracking Survey Propagation Algorithm for Solving Random K-SAT Problems", "authors": ["R. Marino", "F. Ricci-Tersenghi"], "emails": [], "sections": [{"heading": null, "text": "Satisfiability of random Boolean expressions built from many clauses with K variables per clause (random K-satisfiability) is a fundamental problem in combinatorial discrete optimization. Here we study random Ksatisfiability for K = 3 and K = 4 by the Backtracking Survey Propagation algorithm. This algorithm is able to find, in a time linear in the problem size, solutions within a region never reached before, very close to SAT-UNSAT threshold, and even beyond the freezing threshold. For K = 3 the algorithmic threshold practically coincides with the SAT-UNSAT threshold. We also study the whitening process on all the solutions found by the Backtracking Survey Propagation algorithm: none contains frozen variables and the whitening procedure is able to remove all variables, following a two-steps process, in a time that diverges approaching the algorithmic threshold.\nThe K-satisfiability (K-SAT) problem is a combinatorial discrete optimization problem of N Boolean variables, submitted to M constraints. Each constraint, called clause, is in the form of an OR logical operator of K literals (variables or their negations), and the problem is solved when exists one configuration of the variables, among the 2N possible ones, that satisfies all constraints. The K-SAT problem for K \u2265 3 is a central problem in combinatorial optimization: it was the first problem to be shown NP -complete [1, 2, 3] and is still very much studied. Efforts from the theoretical computer science community have been especially devoted to the study of the randomK-SAT ensemble [4, 5], where each formula is generated by randomly choosing M = \u03b1N clauses of K literals [6]; indeed formulas from this ensemble become extremely hard to solve when the clause to variable ratio \u03b1 grows [6] and still the locally tree-like structure of the factor graph [7], representing the interaction network among variables, makes the randomK-SAT ensemble a perfect candidate for an analytic solution. The study of random K-SAT problems and of the related solving algorithms is likely to shed light on the origin of the computational complexity and to allow the development of improved algorithms. Both numerical [8] and analytical [9, 10] evidences suggest that a threshold phenomenon, i.e. a 0\u2212 1 law, takes place in random K-SAT ensembles: in the limit of very large formulas, N \u2192 \u221e, for \u03b1 < \u03b1s(K) a typical formula has a solution, while for \u03b1 > \u03b1s(K) a typical formula is unsatisfiable. It has been very recently proved in Ref. [11] that for K large enough the SAT-UNSAT threshold \u03b1s(K) exists in the N \u2192 \u221e limit and coincides with the prediction from the cavity method [12]. A widely accepted conjecture is that the SAT-UNSAT threshold \u03b1s(K) exists for any value ofK. The best prediction on the location of the SATUNSAT threshold \u03b1s(K) for any K value comes from the cavity method [12, 13, 14, 15]: for example, \u03b1s(K = 3) = 4.2667 [14] and \u03b1s(K = 4) = 9.931 [15]. The cavity method also allows us to count clusters of solutions as a function of their internal entropy [16] and from this very detailed description of the space of solutions several phase transition have been identified [17, 15]. Before reaching the SAT-UNSAT threshold \u03b1s, typical formulas undergo at \u03b1d a shattering of the solutions space in a number of distinct clusters growing exponentially with the system sizeN , with vey high barriers (both energetic and entropic) separating the clusters and an even larger\nnumber of metastable states (local minima of the energy potential), that can trap algorithms looking for solutions. Later at \u03b1c a condensation transition takes place and the remaining solutions concentrate in a sub-extensive number of clusters, leading to effective long-range correlations among variables in solutions, which are hard to approximate by any algorithmwith a finite horizon. In general \u03b1d \u2264 \u03b1c \u2264 \u03b1s hold. One more feature of the solution space which has a direct effect on algorithms that actually search for solutions is the presence of frozen variables. A cluster of solutions is said to have frozen variables if there is a finite fraction of variables that take the same value in all the solutions of that cluster. This concept is similar to the idea of the backbone of variables hard to set [18], but restricted to a given cluster of solutions. A widely believed conjecture is that finding solutions in clusters with frozen variables is extremely hard and practically impossible to do in a time growing linearly with the system size. The freezing transition takes place at \u03b1f when the vast majority of solutions belongs to clusters with frozen variables. While forK > 8 it has been rigorously proved [19, 20, 21] that a value \u03b1f , strictly smaller than \u03b1s, exists such that for \u03b1f < \u03b1 < \u03b1s all solutions belongs to clusters with frozen variables, for smallK is again the cavity method that provides the best prediction for \u03b1f [17, 15] and for a statistical description of the freezing transition [22]. Algorithms searching for solutions to randomK-SAT problems can be roughly classified in two main categories: (a) algorithms that search for a solution by performing a biased random walk in the space of configurations and (b) algorithms that try to build the solutions by assigning variables one-by-one, according to some estimated marginals. Belong to the former categoryWalkSat [23], focused Metropolis search [24] and ASAT [25]; while in the latter category we find Belief Propagation Guided Decimation [26] and Survey Inspired Decimation [27]. All these algorithms are very effective in finding solutions to random K-SAT problems. However only Belief Propagation Guided Decimation can be analytically solved [26] to find the algorithmic threshold \u03b1a, i.e. the largest \u03b1 value up to where the algorithm finds a solution with high probability in the N \u2192 \u221e limit. For all the other algorithms we are forced to run extensive numerical simulations in order to measure their performances and algorithmic thresholds. The algorithm achieving the best performances on several constraint satisfaction problems up to now seems to be Survey Inspired Decimation (SID), which has been successfully applied to the randomK-SAT problem [12] and to the coloring problem [28]. The statistical properties of the SID algorithm have been studied in details in the case of the random 3-SAT problem in the limit of very large number of variables in Refs. [29, 27]. Numerical experiments on random 3-SAT problems with a large number of variables, e.g.N = 3\u00d7105, show that in a time that is approximately linear inN the SID algorithm finds solutions up to \u03b1SIDa \u2243 4.2525 [29], that is definite smaller, although very close to, \u03b1s(K = 3) = 4.2667. In other words in the region \u03b1SIDa < \u03b1 < \u03b1s the problem is satisfiable for large N , but we are unable to find a solution to the problem using the SID algorithm. To fill this gap we introduce a new algorithm for finding solutions to random K-SAT problems, the Backtracking Survey Propagation (BSP) algorithm. This algorithm is based, as SID, on the survey propagation equations derived within the cavity method [12, 29, 27], but allows to re-assigned variables previously set in order to improve the partial solution already achieved. The idea of the backtracking [30] is that a choice made at the beginning of the decimation process, when most of the variables are unassigned, later on may turn to be suboptimal, and re-assigning a variable, which is no longer consistent with the current best estimate of its marginal probability, may lead to a better satisfying configuration. We do not expect the backtracking to be essential when correlations between variables are short ranged, but approaching \u03b1s we know that correlations become long ranged and thus the assignment of a single variable may affect a huge number of other variables: this is the situation when we expect the backtracking to be really essential. The BSP algorithm is explained in detail in the Methods section, but it is worth stressing that programming this new algorithm is as difficult as the standard SID algorithm. The only extra parameter is the ratio r between the number of backtracking moves (releasing one variable) and the number of decimation moves (fixing one variable). Obviously we must have r < 1, in order to eventually converge to a solution, for r = 0 we recover the SID algorithm and the running times grow as 1/(1\u2212 r) when r grows, but the algorithm complexity remain linear in the system size for any r < 1. We are going to show that this new kind of algorithm is able to find solutions in all the SAT phase for K = 3, i.e. \u03b1BSPa \u2248 \u03b1s, and for the more difficult K = 4 case, the numerical evidences suggest the BSP algorithm can go beyond the freezing threshold, i.e. \u03b1f < \u03b1BSPa < \u03b1s, and very close to the SAT-UNSAT threshold."}, {"heading": "Results", "text": "In this section we show the results of our numerical simulations. In each plot having \u03b1 on the abscissa, the right end of the plot coincide with the best current estimate of \u03b1s, in order to provide an immediate indication of how close to the SAT-UNSAT threshold the different algo-\nrithms can go. Probability of finding a SAT assignment. We start showing in Fig. 1 the probability that the BSP algorithm run with different values of the r parameter (r = 0, 0.5 and 0.9) on random 4-SAT problems of two different sizes (N = 5 \u00d7 103 and N = 5 \u00d7 104) finds a SAT assignment. We notice that the probability of finding a solution increases both by increasing r and the system size N . Moreover for larger N the behavior of the probability becomes sharper, as expected for a stochastic process having a 0\u22121 law in theN \u2192 \u221e limit. However the extrapolation of the curves in Fig. 1 to the N \u2192 \u221e limit seems difficult, because of biased finite size effects. Order parameter and algorithmic threshold. In order to compute the BSP algorithmic threshold in the N \u2192 \u221ewe need to find an order parameter whose finite size effects are under control, or even better one which is size-independent. We identify this order parameter with the quantity \u03a3res/Nres, where \u03a3res and Nres are respectively the complexity (i.e. log of number of clusters) and the size of the residual problem which has been simplified enough by decimating variables and is ready to be passed to WalkSat for the relatively easy assignment of the last Nres variables. The average, shown in Fig. 2, is performed only on runs that finally found a solution. More precisely \u03a3res is measured in the last step before WalkSat is called, since on calling WalkSat all the survey messages are null (see Methods for a more detailed description of the BSP algorithm) and so is the residual complexity. In general the residual complexity \u03a3res is\nnot always positive, andWalkSat is called anyway. However all the simulations we run provide a solid evidence that if WalkSat is called with \u03a3res < 0 then it is not able to find a solution, while if \u03a3res > 0 WalkSat always return a solution. The case \u03a3res < 0 may happen either because the original problem was unsatisfiable or because the decimation procedure made some error and removed the few available solutions. Taking the average only on problems were a solution is eventually found, i.e.\nfor which \u03a3res \u2265 0, a null value for the mean residual complexity signals the BSP algorithm is not able to find any solution, and provide a good estimate for the algorithmic threshold \u03b1BSPa [29]. As we see in the data shown in the upper panel of Fig. 2 themean value of the intensive residual complexity \u03a3res/Nres is practically size-independent and linear fits provide very good estimates for the algorithmic threshold \u03b1BSPa for both random 3-SAT and 4-SAT (values reported in the caption of Fig. 2). For random 3-SAT the algorithmic threshold of BSP run with r = 0.9 practically coincide with the SAT-UNSAT threshold \u03b1s thus providing a strong evidence that BSP can find solutions in the entire SAT phase of random 3-SAT. For random 4-SAT the inequality \u03b1BSPa < \u03b1s seems to hold strictly, although the distance from the SAT-UNSAT threshold is really tiny and more importantly the algorithmic threshold for BSP run with r = 0.9 is already larger than the freezing threshold \u03b1f = 9.88 [15]. This means that BSP is able to find solutions in a region with \u03b1 > \u03b1f where the majority of solutions is in clusters with frozen variables and thus hard to find. Indeed we will show below that BSP is finding solutions in atypical clusters with no frozen variables, as it has been observed also for other solution searching algorithms [32]. Obviously also for random 3-SAT problems the BSP algorithm can find solutions beyond the freezing threshold, given that it finds solutions in the entire SAT phase. However for random 3-SAT the estimate \u03b1f = 4.254 comes from exhaustive enumeration of small problems [31] and may have strong finite N corrections. Computational complexity. The running time of the BSP algorithm is O(N), since it is proportional to \u03b7\n(1\u2212 r)PSAT N , (1)\nwhere \u03b7 is the mean number of iterations to find a solution to the survey propagation equations, shown in Fig. 3, and PSAT is the probability of finding a solution, shown in Fig. 1. Eventually the computational complexity may become O(N log(N)) if the sorting of the single variable marginal biases is made in a trivial way. Fig.3 clearly show that both for random 3-SAT and random 4-SAT the mean running time for reaching convergence in the survey propagation equations increases only slightly with increasing \u03b1, and it is likely to have a finite value at the algorithmic threshold, where PSAT should go to zero, in the N \u2192 \u221e limit. In the worst case we expect a dependence \u03b7 = O(log(N)) that does not modify the statement that the BSP running time is at most O(N log(N)). Whitening procedure. Given that the BSP algorithm is able to find solutions even beyond the freezing threshold, it is natural to check whether these solutions\nhave frozen variables or not. We concentrate on solutions found for random 3-SAT problems with N = 106, since the large size of these problems makes the analysis very clean, and because we have a good number of solutions above the best estimate for the freezing transition \u03b1f = 4.254(9) [31]. We check for frozen variables in a solution running the whitening procedure starting from that solution. The whitening procedure assigns iteratively a \u22c6 value to variables which belong only to clauses, which are already satisfied by another variable or already contain a \u22c6 variable. The procedure is continued until all variables are \u22c6 or a fixed point is reached: non-\u22c6 variables at the fixed point correspond to frozen variables in the starting solution. We uncover that all solutions found by BSP are converted to all-\u22c6 by running the whitening procedure, thus showing that solutions found by BSP have no frozen variables. This is expected since it is widely believe that find-\ning solutions with a finite fraction of frozen variable is very hard and probably takes a time exponential in the problem size. So the BSP algorithm is actually finding solutions even beyond the freezing threshold \u03b1f by focusing on the sub-dominant clusters with unfrozen solutions. The whitening procedure leads to a relaxation of the number of non-\u22c6 variables as a function of the number of iterations \u03c4 that follows a kind of two steps process [22] with an evident plateau (see upper panel in Fig. 4), that becomes longer increasing \u03b1 towards the algorith-\nmic threshold. The time for leaving the plateau has large fluctuations as shown in the lower panel of Fig. 4, but after having left the plateau the dynamics of the whitening procedure is almost deterministic. Indeed plotting the mean fraction of non-\u22c6 variables as a function of the time remained to reach the all-\u22c6 configuration, \u03c4f \u2212 \u03c4 , we see that fluctuations are strongly suppressed and the relaxation is practically deterministic (see Fig. 5). Critical exponent for the whitening time divergence. Given the clear increase of the whitening time approaching the algorithmic threshold, we study whether such an increase follows any critical behavior, i.e. power law divergence as a function of \u03b1a \u2212\u03b1 or \u03a3res, which are linearly related. In Fig.6 we plot in a double logarithmic scale the mean whitening time \u03c4(f) as a function of the residual complexity \u03a3res, for different choices of the fraction f of non-\u22c6 variables defining the whitening time. Data points are interpolated via the following power law\n\u03c4(f) = C(f)+A(f)(\u03b1a\u2212\u03b1)\u2212\u03bd = C(f)+B(f)\u03a3\u2212\u03bdres , (2)\nwhere the critical exponent \u03bd is forced to be the same for all fractions f , while the other two fitting parameters are allowed to depend on f . Joint interpolations return the following best estimates for the critical exponent \u03bd: \u03bd = 0.269(5) for K = 4 and \u03bd = 0.281(6) for K = 3. The two estimate turn out to be well compatible within\nerrors, thus suggesting a sort of universality for the critical behavior approaching the algorithmic threshold \u03b1BSPa . Nonetheless a word of caution is needed since the solutions we are using as starting points for the whitening procedure are atypical solutions (otherwise they would likely contain frozen variables and would not flow to the all-\u22c6 configuration under the whitening procedure). So, while finding universal critical properties in a dynamical process is definitely a good news, how to relate it to the behavior of the same process on typical solutions it is not obvious (and indeed for the whitening process starting from typical solutions one would expect the naive mean field exponent \u03bd = 1/2, which is much larger than the one we are finding)."}, {"heading": "Discussion", "text": "We have presented the BSP algorithm for finding solutions in random K-SAT problems and provided numerical evidence that it works much better than previously available algorithms. For K = 3 the BSP algorithm can find solution practically up to the SAT-UNSAT threshold, while for K = 4 a tiny gap to the SAT-UNSAT threshold still remains, but the algorithmic threshold seems to be located beyond the freezing threshold \u03b1f . Beating the freezing threshold, i.e. finding solution in a region where the majority of solutions belongs to clusters with frozen variables, is hard, but not impossible, because smart algorithms can look for solutions with no\nfrozen variables. Indeed we have shown that all solutions found by BSP have no frozen variables, even for \u03b1 > \u03b1f . This is in agreement with the most recent conjecture about the location of the algorithmic threshold \u03b1a, that is the threshold unbeatable by any polynomial-time algorithm. \u03b1a is conjectured to be the clauses-to-variables density \u03b1 where the complexity of clusters with only unfrozen variables \u03a3unfrozen goes to zero. Since the complexity of clusters with frozen variables \u03a3frozen vanishes at \u03b1s (close to \u03b1s all clusters contain frozen variables) and \u03b1f is defined such that \u03a3unfrozen = \u03a3frozen, it follows that in general \u03b1f \u2264 \u03b1a \u2264 \u03b1s. Our numerical results are consistent with these inequalities for K = 4 (where the computation of \u03b1f is known analytically) and thus promote the BSP algorithm to an almost ideal algorithm for random K-SAT, at least for small values of K and large problem sizesN . It would be very interesting to run BSP on random hypergraph bicoloring problems, where the threshold values are known [33] and a very recent computation provides an estimate for \u03a3unfrozen [34]. Similarly to the SID algorithm, a smart implementation of the BSP algorithm has running times that approximately linear inN , and we expect in worst case running times O(N log(N)2), due to the sorting of biases and to the growth of the diameter of the random factor graph. It is worth noticing that the BSP algorithm, as also the SID algorithm, is easy to parallelize, since most of the operations are local and do not require any strong centralized control. Obviously the effectiveness of a parallel version of the algorithm would largely depend on the topology of the factor graph representing the specific problem: if the factor graph is an expander, then splitting the problem on several cores may require too much inter-core bandwidth, but in problems having a natural hierarchical structure the parallelization may lead to further performance improvements. The backtracking introduced in the BSP algorithm helps a lot in correcting errors made during the partial assignment of variables and this allow the BSP algorithm to reach solutions at large \u03b1 values. Clearly the price we pay is that a too frequent backtracking makes the algorithm slower. A natural direction to improve this class of algorithms would be to used biased marginals focusing on solutions which are easy to be reached by the algorithm. For example in the region \u03b1 > \u03b1f the measure is concentrated on solutionswith frozen variables, and thus push the algorithm in a direction it can not really follow (because reaching a solution with frozen variables is too hard). The backtracking thus intervenes and corrects the partial assignment until a solution with unfrozen variables is found by pure chance. If the marginals could be computed from the measures which is concentrated on the cluster with no frozen variables, this could push the algorithm immediately in the right direction and much\nless backtracking would be hopefully needed."}, {"heading": "Methods", "text": "The Backtracking Survey Propagation (BSP) algorithm is based on the survey propagation equations and can be viewed as an extension of the Survey Inspired Decimation (SID) algorithm. Survey Inspired Decimation. The SID algorithm has been proposed for finding solutions to random Ksatisfiability problems in the region where solutions are shattered in an exponentially large number of different clusters [12, 13]. A complete description of the algorithm is given by Braunstein et al. [27]. The SID algorithm starts by solving iteratively the survey propagation equations: after convergence to the fixed point, each variable receives some messages (called surveys) that represent a simplified (projected) marginal, carrying information about the fraction of clusters where the variable i is forced to be positive (w+i ), negative (w \u2212 i ) or not forced at all. From these surveys a bias for each variable i can be computed [29]\nP (i) = 1\u2212min(w+i , w \u2212 i ) , (3)\nsuch that assigning a variable of large bias minimizes the probability of doing a wrong choice. The SID algorithm then proceed by assigning variables (decimation), starting from those of largest bias. In order to keep the algorithm efficient, at each step of decimation a small fraction f of variables is assigned to their most probable values (according to the surveys). After each step of decimation, the survey propagation equations are solved again on the subproblem, which is obtained by removing satisfied clauses and by reducing those containing a false literal (unless a zero-length clause is generated, and in that case the algorithm returns a failure). The main idea of the algorithm is that fixing variables which are almost certain to their most probable value, one can reduce the size of the problem without reducing too much the number of solutions. The number of solutions, or better the number of clusters of solutions that we can count via the complexity \u03a3, should always remain positive during the decimation process in order to achieve the goal of finding a solution. At every fixed point of the survey propagation equations (both on the original problem and on all the subproblems generated during decimation) the complexity \u03a3 can be computed from the surveys [27] and its evolution during the SID algorithm can be very informative [29]. Indeed it is found that, if the complexity \u03a3 becomes too small or negative, the SID algorithm is likely to fail, either because the iterative method for solving the survey propagation equations no longer converges to a fixed point or because\na contradiction is generated by assigning variables. In these cases the SID algorithm returns a failure. If the complexity \u03a3 remains always positive, the SID algorithm is likely to reduce suchmuch the problem, that eventually the surveys becomes all null. This is a strong indication that the residual subproblem is easy and can be solved also by other standard and fast SAT solvers. Then WalkSat is run on the residual problem and always returns a solution if the residual complexity \u03a3res computed on the last non-trivial fixed point of the survey propagation equations is positive. A careful analysis of the SID algorithm for random 3-SAT problems of size N = O(105) shows that the algorithmic threshold achievable by SID is \u03b1SIDa = 4.2525 [29], which is close, but definitely smaller than the SAT-UNSAT threshold \u03b1s = 4.2667. The running time of the SID algorithm experimentally measured is O(N log(N)), however it is not excluded than an extra factor log(N) could be present in the convergence time of the iterative solution to survey propagation equations, but for sizes up toN = O(107) has not been observed [27]. Backtracking Survey Propagation. Willing to improve the SID algorithm to find solutions also in the region \u03b1SIDa < \u03b1 < \u03b1s, one has to reconsider the way variables are assigned. It is clear that the fact the SID algorithm assigns each variable only once is a strong limitation, especially in a situation where correlations between variables becomes extremely strong and long-ranged. In difficult problems it can easily happen that one realizes that a variable is taking the wrong value only after having assigned some of its neighbours variables. However the SID algorithm is not able to solve this kind of frustrating situations. The BSP algorithms tries to solve this kind of problematic situations by introducing a new backtracking step, where a variable already assigned can be released and eventually reassigned in a future decimation step. It is not difficult to understand whether a variable is worth releasing. Indeed the bias P (i) can be computed also for a variable i already assigned: if the bias P (i), that was large at the time the variable iwas assigned, gets strongly reduces by the effect of assigning other variables, then it is likely that releasing the variable i may be beneficial in the search for a solution. So both variables to be fixed in the decimation step and variables to be released in the backtracking step are chosen according to the biases P (i): variables to be fixed have large biases and variables to be released have small biases. The BSP algorithm then proceeds by doing decimation steps and backtracking steps (always on a fraction f of variables in order to keep the algorithm efficient), and running the iterativemethod for solving survey propagation equations after each step (either decimation or\nbacktracking one), such that it can update the surveys and the biases. The choice between a decimation or a backtracking step is taken according to a stochastic rule (unless there no variables to release), where the parameter r \u2208 [0, 1) represents the ratio between backtracking steps to decimation steps. Obviously for r = 0 we recover the SID algorithm, since no backtracking step is ever done. Increasing r the algorithm becomes slower by a factor 1/(1 \u2212 r), because several variables are reassigned many times, but it can correct wrong choices made during the decimation, and achieve better results than the SID algorithm. The BSP algorithm can stop for the same reasons the SID algorithm does: either the survey propagation equations can not be solved iteratively or the generated subproblem has a contradiction. Both cases happen when the complexity \u03a3 becomes too small or negative. On the contrary if the complexity remain always positive the BSP eventually generate a subproblem where all survey are null and this subproblem is passed to WalkSat that always finds a solution if the residual complexity \u03a3res is positive."}, {"heading": "Acknowledgements", "text": "WethankK. Freese, R. Eichhorn andE. Aurell for useful discussions. This research is supported by the Swedish Science Council through grant 621-2012-2982."}], "references": [{"title": "The complexity of theorem proving procedures", "author": ["S.A. Cook"], "venue": "Proc. 3rd Ann. ACM Symp. on Theory of Computing, Assoc. Comput. Mach., New York, p. 151. ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1971}, {"title": "Computers and Intractability; A guide to the theory of NP-completeness", "author": ["M. Garey", "D.S. Johnson"], "venue": "Freeman, San Francisco, ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1979}, {"title": "Computational Complexity", "author": ["C.H. Papadimitriou"], "venue": "Addison-Wesley ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1994}, {"title": "Information", "author": ["M. M\u00e9zard", "A. Montanari"], "venue": "Physics, and Computation, Oxford University Press ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "The Nature of Computation", "author": ["C. Moore", "S. Mertens"], "venue": "Oxford University Press ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Finding Hard Instances of the Satisfiability Problem: A Survey", "author": ["S.A. Cook", "D.G. Mitchell"], "venue": "In: Satisfiability Problem: Theory and Applications. J. Du, D. Gu and P. Pardalos (Eds). DIMACS Series in DiscreteMathematics and Theoretical Computer Science, Volume 35 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "Factor Graphs and the Sum-Product Algorithm", "author": ["F.R. Kschischang", "B.J. Frey", "H.-A. Loeliger"], "venue": "IEEE Trans. Infor. Theory 47, 498 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Critical Behaviour in the satisfiability of random Boolean expressions", "author": ["S. Kirkpatrick", "B. Selman"], "venue": "Science 264, 1297 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1994}, {"title": "Typical random 3-SAT formulae and the satisfiability threshold", "author": ["O. Dubois", "Y. Boufkhad", "J. Mandler"], "venue": "Proc. 11th ACM-SIAM Symp. on Discrete Algorithms, 124 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "B", "author": ["O. Dubois", "R. Monasson"], "venue": "Selman and R. Zecchina (Eds.), Phase Transitions in Combinatorial Problems, Theoret. Comp. Sci. 265, ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "Proof of the satisfiability conjecture for large k", "author": ["J. Ding", "A. Sly", "N. Sun"], "venue": "arXiv:1411.0650 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Analytic and algorithmic solution of random satisfiability problems", "author": ["M. M\u00e9zard", "G. Parisi", "R. Zecchina"], "venue": "Science 297, 812 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "The random Ksatisfiability problem: from an analytic solution to an efficient algorithm", "author": ["M. M\u00e9zard", "R. Zecchina"], "venue": "Phys. Rev. E 66, 056126 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Threshold Values of Random K-SAT from the Cavity Method", "author": ["S. Mertens", "M. M\u00e9zard", "R. Zecchina"], "venue": "Random Struct. Alg. 28, 340 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Gibbs States and the Set of Solutions of Random Constraint Satisfaction Problems", "author": ["F. Krzakala", "A. Montanari", "F. Ricci-Tersenghi", "G. Semerjian", "L. Zdeborova"], "venue": "PNAS 104, 10318 ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Phase transitions in the coloring of randomgraphs", "author": ["L. Zdeborova", "F. Krzakala"], "venue": "Phys. Rev. E 76, 031131 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Determining computational complexity from characteristic phase transitions", "author": ["R. Monasson", "R. Zecchina", "S. Kirkpatrick", "B. Selman", "L. Troyansky"], "venue": "Nature 400, 133 ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "On the Solution-Space Geometry of Random Constraint Satisfaction Problems", "author": ["D. Achlioptas", "F. Ricci-Tersenghi"], "venue": "STOC \u201906: Proceedings of the thirtyeighth annual ACM symposium on Theory of computing (Seattle, WA, USA), 130 ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Random FormulasHave FrozenVariables", "author": ["D. Achlioptas", "F. Ricci-Tersenghi"], "venue": "SIAM J. Comput. 39, 260 ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "On the Solution-Space Geometry of Random Constraint Satisfaction Problems", "author": ["D. Achlioptas", "A. Coja-Oghlan", "F. Ricci- Tersenghi"], "venue": "Random Struct. Alg. 38, 251 ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "On the freezing of variables in random constraint satisfaction problem", "author": ["G. Semerjian"], "venue": "J. Stat. Phys. 130, 251 ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Noise strategies for improving local search", "author": ["B. Selman", "H.A. Kautz", "B. Cohen"], "venue": "Proc. AAAI-94, 337 ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1994}, {"title": "Focused local search for random 3-satisfiability", "author": ["S. Sakari", "M. Alava", "P. Orponen"], "venue": "J. Stat. Mech. P06006 ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Behavior of heuristics on large and hard satisfiability problems", "author": ["J. Ardelius andE. Aurell"], "venue": "Phys. Rev. E 74,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2006}, {"title": "On the cavity method for decimated random constraint satisfaction problems and the analysis of belief propagation guided decimation algorithms", "author": ["F. Ricci-Tersenghi", "G. Semerjian"], "venue": "J. Stat. Mech. P09001 ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Survey propagation: An algorithm for satisfiability", "author": ["A. Braunstein", "M. M\u00e9zard", "R. Zecchina"], "venue": "Random Struct. Alg. 27, 201 ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Coloring Random Graphs", "author": ["R. Mulet", "A. Pagnani", "M. Weigt", "R. Zecchina"], "venue": "Phys. Rev. Lett. 89, 268701 ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Some remarks on the survey decimation algorithm for K- satisfiability", "author": ["G. Parisi"], "venue": "arXiv:cs/0301015 ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2003}, {"title": "A backtracking survey propagation algorithm for K-satisfiability", "author": ["G. Parisi"], "venue": "arxiv:cond-mat/0308510 ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "Exhaustive enumeration unveils clustering and freezing in the random 3-satisfiability problem", "author": ["J. Ardelius", "L. Zdeborova"], "venue": "Phys. Rev. E 78, 040101 ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Entropy landscape and non-Gibbs solutions in constraint satisfaction problems", "author": ["L. Dall\u2019Asta", "A. Ramezanpour", "andR. Zecchina"], "venue": "Phys. Rev. E 77,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Bicolouring random hypergraphs", "author": ["T. Castellani", "V. Napolano", "F. Ricci-Tersenghi", "R. Zecchina"], "venue": "J. Phys. A 36, 11037 ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2003}, {"title": "Aurell for useful discussions. This research is supported by the Swedish Science Council through grant 621-2012-2982", "author": ["G. Semerjian"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "The K-SAT problem for K \u2265 3 is a central problem in combinatorial optimization: it was the first problem to be shown NP -complete [1, 2, 3] and is still very much studied.", "startOffset": 130, "endOffset": 139}, {"referenceID": 1, "context": "The K-SAT problem for K \u2265 3 is a central problem in combinatorial optimization: it was the first problem to be shown NP -complete [1, 2, 3] and is still very much studied.", "startOffset": 130, "endOffset": 139}, {"referenceID": 2, "context": "The K-SAT problem for K \u2265 3 is a central problem in combinatorial optimization: it was the first problem to be shown NP -complete [1, 2, 3] and is still very much studied.", "startOffset": 130, "endOffset": 139}, {"referenceID": 3, "context": "Efforts from the theoretical computer science community have been especially devoted to the study of the randomK-SAT ensemble [4, 5], where each formula is generated by randomly choosing M = \u03b1N clauses of K literals [6]; indeed formulas from this ensemble become extremely hard to solve when the clause to variable ratio \u03b1 grows [6] and still the locally tree-like structure of the factor graph [7], representing the interaction network among variables, makes the randomK-SAT ensemble a perfect candidate for an analytic solution.", "startOffset": 126, "endOffset": 132}, {"referenceID": 4, "context": "Efforts from the theoretical computer science community have been especially devoted to the study of the randomK-SAT ensemble [4, 5], where each formula is generated by randomly choosing M = \u03b1N clauses of K literals [6]; indeed formulas from this ensemble become extremely hard to solve when the clause to variable ratio \u03b1 grows [6] and still the locally tree-like structure of the factor graph [7], representing the interaction network among variables, makes the randomK-SAT ensemble a perfect candidate for an analytic solution.", "startOffset": 126, "endOffset": 132}, {"referenceID": 5, "context": "Efforts from the theoretical computer science community have been especially devoted to the study of the randomK-SAT ensemble [4, 5], where each formula is generated by randomly choosing M = \u03b1N clauses of K literals [6]; indeed formulas from this ensemble become extremely hard to solve when the clause to variable ratio \u03b1 grows [6] and still the locally tree-like structure of the factor graph [7], representing the interaction network among variables, makes the randomK-SAT ensemble a perfect candidate for an analytic solution.", "startOffset": 216, "endOffset": 219}, {"referenceID": 5, "context": "Efforts from the theoretical computer science community have been especially devoted to the study of the randomK-SAT ensemble [4, 5], where each formula is generated by randomly choosing M = \u03b1N clauses of K literals [6]; indeed formulas from this ensemble become extremely hard to solve when the clause to variable ratio \u03b1 grows [6] and still the locally tree-like structure of the factor graph [7], representing the interaction network among variables, makes the randomK-SAT ensemble a perfect candidate for an analytic solution.", "startOffset": 329, "endOffset": 332}, {"referenceID": 6, "context": "Efforts from the theoretical computer science community have been especially devoted to the study of the randomK-SAT ensemble [4, 5], where each formula is generated by randomly choosing M = \u03b1N clauses of K literals [6]; indeed formulas from this ensemble become extremely hard to solve when the clause to variable ratio \u03b1 grows [6] and still the locally tree-like structure of the factor graph [7], representing the interaction network among variables, makes the randomK-SAT ensemble a perfect candidate for an analytic solution.", "startOffset": 395, "endOffset": 398}, {"referenceID": 7, "context": "Both numerical [8] and analytical [9, 10] evidences suggest that a threshold phenomenon, i.", "startOffset": 15, "endOffset": 18}, {"referenceID": 8, "context": "Both numerical [8] and analytical [9, 10] evidences suggest that a threshold phenomenon, i.", "startOffset": 34, "endOffset": 41}, {"referenceID": 9, "context": "Both numerical [8] and analytical [9, 10] evidences suggest that a threshold phenomenon, i.", "startOffset": 34, "endOffset": 41}, {"referenceID": 10, "context": "[11] that for K large enough the SAT-UNSAT threshold \u03b1s(K) exists in the N \u2192 \u221e limit and coincides with the prediction from the cavity method [12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[11] that for K large enough the SAT-UNSAT threshold \u03b1s(K) exists in the N \u2192 \u221e limit and coincides with the prediction from the cavity method [12].", "startOffset": 142, "endOffset": 146}, {"referenceID": 11, "context": "The best prediction on the location of the SATUNSAT threshold \u03b1s(K) for any K value comes from the cavity method [12, 13, 14, 15]: for example, \u03b1s(K = 3) = 4.", "startOffset": 113, "endOffset": 129}, {"referenceID": 12, "context": "The best prediction on the location of the SATUNSAT threshold \u03b1s(K) for any K value comes from the cavity method [12, 13, 14, 15]: for example, \u03b1s(K = 3) = 4.", "startOffset": 113, "endOffset": 129}, {"referenceID": 13, "context": "The best prediction on the location of the SATUNSAT threshold \u03b1s(K) for any K value comes from the cavity method [12, 13, 14, 15]: for example, \u03b1s(K = 3) = 4.", "startOffset": 113, "endOffset": 129}, {"referenceID": 13, "context": "2667 [14] and \u03b1s(K = 4) = 9.", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "The cavity method also allows us to count clusters of solutions as a function of their internal entropy [16] and from this very detailed description of the space of solutions several phase transition have been identified [17, 15].", "startOffset": 104, "endOffset": 108}, {"referenceID": 15, "context": "The cavity method also allows us to count clusters of solutions as a function of their internal entropy [16] and from this very detailed description of the space of solutions several phase transition have been identified [17, 15].", "startOffset": 221, "endOffset": 229}, {"referenceID": 16, "context": "This concept is similar to the idea of the backbone of variables hard to set [18], but restricted to a given cluster of solutions.", "startOffset": 77, "endOffset": 81}, {"referenceID": 17, "context": "While forK > 8 it has been rigorously proved [19, 20, 21] that a value \u03b1f , strictly smaller than \u03b1s, exists such that for \u03b1f < \u03b1 < \u03b1s all solutions belongs to clusters with frozen variables, for smallK is again the cavity method that provides the best prediction for \u03b1f [17, 15] and for a statistical description of the freezing transition [22].", "startOffset": 45, "endOffset": 57}, {"referenceID": 18, "context": "While forK > 8 it has been rigorously proved [19, 20, 21] that a value \u03b1f , strictly smaller than \u03b1s, exists such that for \u03b1f < \u03b1 < \u03b1s all solutions belongs to clusters with frozen variables, for smallK is again the cavity method that provides the best prediction for \u03b1f [17, 15] and for a statistical description of the freezing transition [22].", "startOffset": 45, "endOffset": 57}, {"referenceID": 19, "context": "While forK > 8 it has been rigorously proved [19, 20, 21] that a value \u03b1f , strictly smaller than \u03b1s, exists such that for \u03b1f < \u03b1 < \u03b1s all solutions belongs to clusters with frozen variables, for smallK is again the cavity method that provides the best prediction for \u03b1f [17, 15] and for a statistical description of the freezing transition [22].", "startOffset": 45, "endOffset": 57}, {"referenceID": 15, "context": "While forK > 8 it has been rigorously proved [19, 20, 21] that a value \u03b1f , strictly smaller than \u03b1s, exists such that for \u03b1f < \u03b1 < \u03b1s all solutions belongs to clusters with frozen variables, for smallK is again the cavity method that provides the best prediction for \u03b1f [17, 15] and for a statistical description of the freezing transition [22].", "startOffset": 271, "endOffset": 279}, {"referenceID": 20, "context": "While forK > 8 it has been rigorously proved [19, 20, 21] that a value \u03b1f , strictly smaller than \u03b1s, exists such that for \u03b1f < \u03b1 < \u03b1s all solutions belongs to clusters with frozen variables, for smallK is again the cavity method that provides the best prediction for \u03b1f [17, 15] and for a statistical description of the freezing transition [22].", "startOffset": 341, "endOffset": 345}, {"referenceID": 21, "context": "Belong to the former categoryWalkSat [23], focused Metropolis search [24] and ASAT [25]; while in the latter category we find Belief Propagation Guided Decimation [26] and Survey Inspired Decimation [27].", "startOffset": 37, "endOffset": 41}, {"referenceID": 22, "context": "Belong to the former categoryWalkSat [23], focused Metropolis search [24] and ASAT [25]; while in the latter category we find Belief Propagation Guided Decimation [26] and Survey Inspired Decimation [27].", "startOffset": 69, "endOffset": 73}, {"referenceID": 23, "context": "Belong to the former categoryWalkSat [23], focused Metropolis search [24] and ASAT [25]; while in the latter category we find Belief Propagation Guided Decimation [26] and Survey Inspired Decimation [27].", "startOffset": 83, "endOffset": 87}, {"referenceID": 24, "context": "Belong to the former categoryWalkSat [23], focused Metropolis search [24] and ASAT [25]; while in the latter category we find Belief Propagation Guided Decimation [26] and Survey Inspired Decimation [27].", "startOffset": 163, "endOffset": 167}, {"referenceID": 25, "context": "Belong to the former categoryWalkSat [23], focused Metropolis search [24] and ASAT [25]; while in the latter category we find Belief Propagation Guided Decimation [26] and Survey Inspired Decimation [27].", "startOffset": 199, "endOffset": 203}, {"referenceID": 24, "context": "However only Belief Propagation Guided Decimation can be analytically solved [26] to find the algorithmic threshold \u03b1a, i.", "startOffset": 77, "endOffset": 81}, {"referenceID": 11, "context": "The algorithm achieving the best performances on several constraint satisfaction problems up to now seems to be Survey Inspired Decimation (SID), which has been successfully applied to the randomK-SAT problem [12] and to the coloring problem [28].", "startOffset": 209, "endOffset": 213}, {"referenceID": 26, "context": "The algorithm achieving the best performances on several constraint satisfaction problems up to now seems to be Survey Inspired Decimation (SID), which has been successfully applied to the randomK-SAT problem [12] and to the coloring problem [28].", "startOffset": 242, "endOffset": 246}, {"referenceID": 27, "context": "[29, 27].", "startOffset": 0, "endOffset": 8}, {"referenceID": 25, "context": "[29, 27].", "startOffset": 0, "endOffset": 8}, {"referenceID": 27, "context": "2525 [29], that is definite smaller, although very close to, \u03b1s(K = 3) = 4.", "startOffset": 5, "endOffset": 9}, {"referenceID": 11, "context": "This algorithm is based, as SID, on the survey propagation equations derived within the cavity method [12, 29, 27], but", "startOffset": 102, "endOffset": 114}, {"referenceID": 27, "context": "This algorithm is based, as SID, on the survey propagation equations derived within the cavity method [12, 29, 27], but", "startOffset": 102, "endOffset": 114}, {"referenceID": 25, "context": "This algorithm is based, as SID, on the survey propagation equations derived within the cavity method [12, 29, 27], but", "startOffset": 102, "endOffset": 114}, {"referenceID": 28, "context": "The idea of the backtracking [30] is that a choice made at the beginning of the decimation process, when most of the variables are unassigned, later on may turn to be suboptimal, and re-assigning a variable, which is no longer consistent with the current best estimate of its marginal probability, may lead to a better satisfying configuration.", "startOffset": 29, "endOffset": 33}, {"referenceID": 27, "context": "for which \u03a3res \u2265 0, a null value for the mean residual complexity signals the BSP algorithm is not able to find any solution, and provide a good estimate for the algorithmic threshold \u03b1BSP a [29].", "startOffset": 191, "endOffset": 195}, {"referenceID": 30, "context": "Indeed we will show below that BSP is finding solutions in atypical clusters with no frozen variables, as it has been observed also for other solution searching algorithms [32].", "startOffset": 172, "endOffset": 176}, {"referenceID": 29, "context": "254 comes from exhaustive enumeration of small problems [31] and may have strong finite N corrections.", "startOffset": 56, "endOffset": 60}, {"referenceID": 29, "context": "254(9) [31].", "startOffset": 7, "endOffset": 11}, {"referenceID": 20, "context": "The whitening procedure leads to a relaxation of the number of non-\u22c6 variables as a function of the number of iterations \u03c4 that follows a kind of two steps process [22] with an evident plateau (see upper panel in Fig.", "startOffset": 164, "endOffset": 168}, {"referenceID": 31, "context": "It would be very interesting to run BSP on random hypergraph bicoloring problems, where the threshold values are known [33] and a very recent computation provides an estimate for \u03a3unfrozen [34].", "startOffset": 119, "endOffset": 123}, {"referenceID": 32, "context": "It would be very interesting to run BSP on random hypergraph bicoloring problems, where the threshold values are known [33] and a very recent computation provides an estimate for \u03a3unfrozen [34].", "startOffset": 189, "endOffset": 193}, {"referenceID": 11, "context": "satisfiability problems in the region where solutions are shattered in an exponentially large number of different clusters [12, 13].", "startOffset": 123, "endOffset": 131}, {"referenceID": 12, "context": "satisfiability problems in the region where solutions are shattered in an exponentially large number of different clusters [12, 13].", "startOffset": 123, "endOffset": 131}, {"referenceID": 25, "context": "[27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "From these surveys a bias for each variable i can be computed [29]", "startOffset": 62, "endOffset": 66}, {"referenceID": 25, "context": "At every fixed point of the survey propagation equations (both on the original problem and on all the subproblems generated during decimation) the complexity \u03a3 can be computed from the surveys [27] and its evolution during the SID algorithm can be very informative [29].", "startOffset": 193, "endOffset": 197}, {"referenceID": 27, "context": "At every fixed point of the survey propagation equations (both on the original problem and on all the subproblems generated during decimation) the complexity \u03a3 can be computed from the surveys [27] and its evolution during the SID algorithm can be very informative [29].", "startOffset": 265, "endOffset": 269}, {"referenceID": 27, "context": "2525 [29], which is close, but definitely smaller than the SAT-UNSAT threshold \u03b1s = 4.", "startOffset": 5, "endOffset": 9}, {"referenceID": 25, "context": "The running time of the SID algorithm experimentally measured is O(N log(N)), however it is not excluded than an extra factor log(N) could be present in the convergence time of the iterative solution to survey propagation equations, but for sizes up toN = O(107) has not been observed [27].", "startOffset": 285, "endOffset": 289}], "year": 2015, "abstractText": "Satisfiability of random Boolean expressions built from many clauses with K variables per clause (random K-satisfiability) is a fundamental problem in combinatorial discrete optimization. Here we study random Ksatisfiability for K = 3 and K = 4 by the Backtracking Survey Propagation algorithm. This algorithm is able to find, in a time linear in the problem size, solutions within a region never reached before, very close to SAT-UNSAT threshold, and even beyond the freezing threshold. For K = 3 the algorithmic threshold practically coincides with the SAT-UNSAT threshold. We also study the whitening process on all the solutions found by the Backtracking Survey Propagation algorithm: none contains frozen variables and the whitening procedure is able to remove all variables, following a two-steps process, in a time that diverges approaching the algorithmic threshold. The K-satisfiability (K-SAT) problem is a combinatorial discrete optimization problem of N Boolean variables, submitted to M constraints. Each constraint, called clause, is in the form of an OR logical operator of K literals (variables or their negations), and the problem is solved when exists one configuration of the variables, among the 2N possible ones, that satisfies all constraints. The K-SAT problem for K \u2265 3 is a central problem in combinatorial optimization: it was the first problem to be shown NP -complete [1, 2, 3] and is still very much studied. Efforts from the theoretical computer science community have been especially devoted to the study of the randomK-SAT ensemble [4, 5], where each formula is generated by randomly choosing M = \u03b1N clauses of K literals [6]; indeed formulas from this ensemble become extremely hard to solve when the clause to variable ratio \u03b1 grows [6] and still the locally tree-like structure of the factor graph [7], representing the interaction network among variables, makes the randomK-SAT ensemble a perfect candidate for an analytic solution. The study of random K-SAT problems and of the related solving algorithms is likely to shed light on the origin of the computational complexity and to allow the development of improved algorithms. Both numerical [8] and analytical [9, 10] evidences suggest that a threshold phenomenon, i.e. a 0\u2212 1 law, takes place in random K-SAT ensembles: in the limit of very large formulas, N \u2192 \u221e, for \u03b1 < \u03b1s(K) a typical formula has a solution, while for \u03b1 > \u03b1s(K) a typical formula is unsatisfiable. It has been very recently proved in Ref. [11] that for K large enough the SAT-UNSAT threshold \u03b1s(K) exists in the N \u2192 \u221e limit and coincides with the prediction from the cavity method [12]. A widely accepted conjecture is that the SAT-UNSAT threshold \u03b1s(K) exists for any value ofK. The best prediction on the location of the SATUNSAT threshold \u03b1s(K) for any K value comes from the cavity method [12, 13, 14, 15]: for example, \u03b1s(K = 3) = 4.2667 [14] and \u03b1s(K = 4) = 9.931 [15]. The cavity method also allows us to count clusters of solutions as a function of their internal entropy [16] and from this very detailed description of the space of solutions several phase transition have been identified [17, 15]. Before reaching the SAT-UNSAT threshold \u03b1s, typical formulas undergo at \u03b1d a shattering of the solutions space in a number of distinct clusters growing exponentially with the system sizeN , with vey high barriers (both energetic and entropic) separating the clusters and an even larger", "creator": " XeTeX output 2015.08.20:2224"}}}