{"id": "1508.05328", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Aug-2015", "title": "Multi-agent Reinforcement Learning with Sparse Interactions by Negotiation and Knowledge Transfer", "abstract": "Reinforcement learning has significant applications for multi-agent systems, especially in unknown dynamic environments. However, most multi-agent reinforcement learning (MARL) algorithms suffer from such problems as exponential computation complexity in the joint state-action space, which makes it difficult to scale up to realistic multi-agent problems. In this paper, a novel algorithm named negotiation-based MARL with sparse interactions (NegoSI) is presented. In contrast to traditional sparse-interaction based MARL algorithms, NegoSI adopts the equilibrium concept and makes it possible for agents to select the non-strict Equilibrium Dominating Strategy Profile (non-strict EDSP) or Meta equilibrium for their joint actions. The presented NegoSI algorithm consists of four parts: the equilibrium-based framework for sparse interactions, the negotiation for the equilibrium set, the minimum variance method for selecting one joint action and the knowledge transfer of local Q-values. In this integrated algorithm, three techniques, i.e., unshared value functions, equilibrium solutions and sparse interactions are adopted to achieve privacy protection, better coordination and lower computational complexity, respectively. To evaluate the performance of the presented NegoSI algorithm, two groups of experiments are carried out regarding three criteria: steps of each episode (SEE), rewards of each episode (REE) and average runtime (AR). The first group of experiments is conducted using six grid world games and shows fast convergence and high scalability of the presented algorithm. Then in the second group of experiments NegoSI is applied to an intelligent warehouse problem and simulated results demonstrate the effectiveness of the presented NegoSI algorithm compared with other state-of-the-art MARL algorithms. In the third group of experiments the presented NegoSI algorithm performs a combination of a combination of different strategies with the lowest efficiency of the presented NegoSI algorithm to overcome high scalability and low scalability and low scalability. In the third group the algorithm uses the same algorithm as the presented NegoSI algorithm and produces a performance increase based on both the results in the first and second groups, i.e., the observed performance increases of the presented NegoSI algorithm after using three steps of each episode and the maximum of the observed performance increases of the presented NegoSI algorithm after using three step-by-step evaluations.\n\n\n\n\n\n\n\nThe second group of experiments is carried out in four of the present experiments: the first trial of the present experiment with high scalability and low scalability, the second trial of the present experiment with", "histories": [["v1", "Fri, 21 Aug 2015 16:30:25 GMT  (2055kb)", "http://arxiv.org/abs/1508.05328v1", "12 pages, 15 figures"], ["v2", "Thu, 31 Mar 2016 15:44:26 GMT  (2056kb)", "http://arxiv.org/abs/1508.05328v2", "13 pages, 15 figures"]], "COMMENTS": "12 pages, 15 figures", "reviews": [], "SUBJECTS": "cs.MA cs.AI", "authors": ["luowei zhou", "pei yang", "chunlin chen", "yang gao"], "accepted": false, "id": "1508.05328"}, "pdf": {"name": "1508.05328.pdf", "metadata": {"source": "CRF", "title": "Multi-agent Reinforcement Learning with Sparse Interactions by Negotiation and Knowledge Transfer", "authors": ["Luowei Zhou", "Pei Yang", "Chunlin Chen", "Yang Gao"], "emails": ["luozhou@umich.edu).", "yangpei@nju.edu.cn).", "clchen@nju.edu.cn)."], "sections": [{"heading": null, "text": "ar X\niv :1\n50 8.\n05 32\n8v 1\n[ cs\n.M A\n] 2\n1 A\nug 2\n01 5\nIndex Terms\u2014Knowledge transfer, multi-agent reinforcement learning, negotiation, sparse interactions.\nI. INTRODUCTION\nMulti-agent learning is drawing more and more interests from scientists and engineers in multi-agent systems (MAS) and machine learning communities [1]-[4]. One key technique for multi-agent learning is multi-agent reinforcement learning (MARL), which is an extension of reinforcement learning in multi-agent domain [5]. Several mathematical models have\nThis work was supported by the National Natural Science Foundation of China (Nos.61273327 and 61432008).\nL. Zhou is with the Department of Control and Systems Engineering, School of Management and Engineering, Nanjing University, Nanjing 210093, China and with the Robotics Institute, University of Michigan, Ann Arbor, MI 48109, USA (e-mail: luozhou@umich.edu).\nP. Yang is with the Department of Control and Systems Engineering, School of Management and Engineering, Nanjing University, Nanjing 210093, China and with the State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210093, China (e-mail: yangpei@nju.edu.cn).\nC. Chen is with the Department of Control and Systems Engineering, School of Management and Engineering, Nanjing University, Nanjing 210093, China and with the Research Center for Novel Technology of Intelligent Equipments, Nanjing University, Nanjing 210093, China (e-mail: clchen@nju.edu.cn).\nY. Gao is with the State Key Laboratory for Novel Software Technology, Department of Computer Science, Nanjing University, Nanjing 210023, China (e-mail: gaoy@nju.edu.cn).\nbeen built as frameworks of MARL, such as Markov games (MG) [6] and decentralized sparse-interaction Markov decision processes (Dec-SIMDP) [7]. Markov games are based on the assumption of full observability of all agents in the entire joint state-action space. Several well-known equilibrium-based MARL algorithms [6]-[12] are derived from this model. DecSIMDP based algorithms rely on agents\u2019 local observation, i.e., the individual state and action. Agents in Dec-SIMDP are modeled with single-agent MDP when they are outside of the interaction areas, while the multi-agent model such as MG is used when they are inside. Typical Dec-SIMDP based algorithms include LoC [13] and CQ-learning [14]. Besides, other models such as learning automata [2] [15] are also valuable tools for designing MARL algorithms.\nIn spite of the rapid development of MARL theories and algorithms, more efforts are needed for practical applications of MARL when compared with other MAS techniques [16]- [18] due to some limitations of the existing MARL methods. The equilibrium-based MARL relies on the tightly coupled learning process which hinders their applications in practice. Calculating the equilibrium (e.g., Nash equilibrium [19]) for each time step and all joint states are computationally expensive [8], even for relatively small scale environments with two or three agents. In addition, sharing individual states, individual actions or even value functions all the time with other agents is unrealistic in some distributed domains (e.g., streaming processing systems [20], sensor networks [21]) given the agents\u2019 privacy protections and huge real-time communication costs [1]. As for MARL with sparse interactions, agents in this setting have no concept of equilibrium policy and they tend to act aggressively towards their goals, which results in a high probability of collisions.\nTherefore, in this paper we focus on how the equilibrium mechanism can be used in sparse-interaction based algorithms and a negotiation-based MARL algorithm with sparse interactions (NegoSI) is proposed for multi-agent systems in unknown dynamic environments. The NegoSI algorithm consists of four parts: the equilibrium-based framework for sparse interactions, the negotiation for the equilibrium set, the minimum variance method and the knowledge transfer of local Q-values. Firstly, we start with the proposed algorithm based on the MDP model and the assumption that the agents have already obtained the single-agent optimal policy before learning in multi-agent settings. Then, the agents negotiate for pure strategy profiles as their potential set for the joint action at the \u201ccoordination state\u201d and they use the minimum variance method to select the relatively good one. After that, the agents move to the next joint state and receive immediate rewards. Finally, the agents\u2019 Q-values are updated by their rewards and equilibrium utilities. When initializing the Q-value for the\n2 expanded \u201ccoordination state\u201d, the agents with NegoSI utilize both of the environmental information and the prior-trained coordinated Q-values. To test the effectiveness of the proposed algorithm, several benchmarks are adopted to demonstrate the performances in terms of the convergence, scalability, fairness and coordination ability. In addition, aiming at solving realistic MARL problems, the presented NegoSI algorithm is also further tested on an intelligent warehouse simulation platform.\nThe rest of this paper is organized as follows. Section II introduces the basic MARL theory and the MAS with sparse interactions. In Section III, the negotiation-based MARL algorithm with sparse interactions (NegoSI) is presented in details and related issues are discussed. Experimental results are shown in Section IV. Conclusions are given in Section V."}, {"heading": "II. BACKGROUND", "text": "In this section, some important concepts in multi-agent reinforcement learning and typical sparse-interaction based MARL algorithms are introduced."}, {"heading": "A. MDP and Markov Games", "text": "We start with reviewing two standard decision-making models that are relevant to our work, i.e., Markov decision process (MDP) and Markov Games, respectively. MDP is the foundation of Markov Games, while Markov Games adopt game theory in multi-agent MDPs. MDP describes a sequential decision problem as follows [22]:\nDefinition 1: (Markov Decision Process, MDP) A MDP is a tuple \u3008S,A,R,T \u3009, where S is the state space, A is the action space of the agent, R : S\u00d7A\u2192R is the reward function mapping state-action pairs to rewards, T : S\u00d7A\u00d7S \u2192 [0,1] is the transition function.\nAn agent in a MDP is required to find an optimal policy which maximizes some reward-based optimization criteria, such as expected discounted sum of rewards:\nV \u2217(s) = max \u03c0\nE\u03c0{ \u221e\n\u2211 k=0 \u03b3krt+k|st = s}, (1)\nwhere V \u2217(s) stands for the value of a state s under the optimal policy, \u03c0 : S\u00d7A\u2192 [0,1] stands for the policy of an agent, E\u03c0 is the expectation under policy \u03c0 , t is any time step, k represents a future time step, rt+k denotes the reward at the time step (t + k) and \u03b3 \u2208 [0,1] is a parameter called the discount factor. This goal can also be equivalently described using the Q-value for a state-action pair:\nQ\u2217(s,a) = r(s,a)+ \u03b3 \u2211 s\u2032 T (s,a,s\u2032)max a\u2032 Q\u2217(s\u2032,a\u2032), (2)\nwhere Q\u2217(s,a) stands for the value of a state-action pair (s,a) under the optimal policy, s\u2032 is the next state and r(s,a) is the immediate reward when agent adopts the action a at the state s, T (s,a,s\u2032) is the transition possibility for the agent to transit from s to s\u2032 given action a. One classic RL algorithm for estimating Q\u2217(s,a) is Q-learning [23], whose one-step updating rule is as follows:\nQ(s,a)\u2190 (1\u2212\u03b1)Q(s,a)+\u03b1[r(s,a)+ \u03b3 max a\u2032 Q(s\u2032,a\u2032)], (3)\nwhere Q(s,a) denotes the state-action value function at a stateaction pair (s,a) and \u03b1 \u2208 [0,1] is a parameter called the learning rate. Provided that all state-action pairs are visited infinite times with a reasonable learning rate, the estimated Q-value Q(s,a) converges to Q\u2217(s,a) [24].\nMarkov games are widely adopted as a framework for multiagent reinforcement learning (MARL) [6] [10]. It is regarded as multiple MDPs in which the transition probabilities and rewards depend on the joint state-action pairs of all agents. In a certain state, agents\u2019 individual action sets generate a repeated game that could be solved in a game-theoretic way. Therefore, Markov game is a richer framework which generalizes both of the MDP and the repeated game [25].\nDefinition 2: (Markov game) An n-agent (n\u2265 2) Markov game is a tuple \u3008n,{Si}i=1,...,n,{Ai}i=1,...,n,{Ri}i=1,...,n,T \u3009, where n is the number of agents in the system, S = {Si}i=1,...,n is the set of state spaces for all agents, A = {Ai}i=1,...,n is the set of action spaces for all agents, Ri : S\u00d7A \u2192 R is the reward function of agent i, T : S\u00d7A\u00d7S\u2192 [0,1] is the transition function.\nDenote the individual policy of agent i by \u03c0i = S\u00d7Ai \u2192 [0,1] and the joint policy of all agents by \u03c0 = (\u03c01, . . . ,\u03c0n). The Q-value of the join state-action pair for agent i under the joint policy \u03c0 can be formulated by:\nQ\u03c0i (~s,~a) = E\u03c0{ \u221e\n\u2211 k=0 \u03b3krt+k|~st =~s,~at =~a}, (4)\nwhere~s \u2208 S stands for a joint state, ~a\u2208 A for a joint action and rt+ki is the reward received at the time step (t +k). Unlike the optimization goal in a MDP, the objective of a Markov game is to find an equilibrium joint policy \u03c0 rather than an optimal joint policy for all agents. Here, the equilibrium policy concept is usually transferred to finding the equilibrium solution for the one-shot game played in each joint state of a Markov game [8]. Several equilibrium-based MARL algorithms in existing literatures such as NashQ [10] [26] and NegoQ [8] have been proposed, so that the joint state-action pair Q-value can be updated according to the equilibrium:\nQi(~s,~a)\u2190 (1\u2212\u03b1)Qi(~s,~a)+\u03b1(ri(~s,~a)+ \u03b3\u03c6i(~s\u2032))), (5)\nwhere \u03c6i(~s\u2032) denotes the expected value of the equilibrium in the next joint state ~s\u2032 for agent i and can be calculated in the one-shot game at that joint state."}, {"heading": "B. MAS with Sparse Interactions", "text": "The definition of Markov game reveals that all agents need to learn their policies in the full joint state-action space and they are coupled with each other all the time [10] [27]. However, this assumption does not hold in practice. The truth is that the learning agents in many practical multi-agent systems are loosely coupled with some limited interactions in some particular areas [5] [13] [27]. Meanwhile, the interactions\n3 1\n2\n3\n4\n5\n6\n7\n1 2 3 4 5 6 7\nR1 R2\nR3\nG3\nG1\nG2\nFig. 1: A part of an intelligent warehouse with three robots, where Ri (i = 1,2,3) represents for robot i with its corresponding goal Gi and the shaded grids are storage shelves.\nbetween agents may not always involve all the agents. These facts lead to a new mechanism of sparse interactions for MARL research.\nSparse-interaction based algorithms [13] [14] have recently found wide applications in MAS research. An example of MAS with sparse interactions is the intelligent warehouse systems, where autonomous robots only consider other robots when they are close enough to each other [25] (see Fig. 1), i.e., when they meet around the crossroad. Otherwise, they can act independently.\nEarlier works such as the coordinated reinforcement learning [28] [29] and sparse cooperative Q-learning [30] used coordination graphs (CGs) to learn interdependencies between agents and decomposed the joint value function to local value functions. However, these algorithms cannot learn CGs online and only focus on finding specific states where coordination is necessary instead of learning for coordination [13]. Melo and Veloso [13] extended the Q-learning to a two-layer algorithm and made it possible for agents to use an additional COORDINATE action to determine the state when the coordination was necessary. Hauwere and Vrancx proposed Coordinating Qlearning (CQ-learning) [14] and FCQ-learning [31] that helped agents learn from statistical information of rewards and Qvalues where an agent should take other agents\u2019 states into account. However, all these algorithms allow agents to play greedy strategies at a certain joint state rather than equilibrium strategies, which might cause conflicts.\nMore recently, Hu et al [8] proposed an efficient equilibrium-based MARL method, called Negotiation-based Q-learning, by which agents can learn in a Markov game with unshared value functions and unshared joint state-actions. In later work, they applied this method for sparse interactions by knowledge transfer and game abstraction [27], and demonstrated the effectiveness of the equilibrium-based MARL in solving sparse-interaction problems. Nevertheless, as opposed to single Q-learning based approaches like CQ-learning, these equilibrium-based methods for sparse interactions require a great deal of real-time information about the joint states and joint actions of all the agents, which results in huge\namount of communication costs. In this paper, we focus on solving learning problems in complex systems. Tightly coupled equilibrium-based MARL methods discussed above are impractical in these situations while sparse-interaction based algorithms tend to cause many collisions. To this end, we adopt the sparse-interaction based learning framework and each agent selects equilibrium joint actions when they are in coordination areas."}, {"heading": "III. NEGOTIATION-BASED MARL WITH SPARSE INTERACTIONS", "text": "When people work in restricted environments with possible conflicts, they usually learn how to finish their individual tasks first and then learn how to coordinate with others. We apply this commonsense to our sparse-interaction method and decompose the learning process into two distinct sub-processes [5]. First, each agent learns an optimal single-agent policy by itself in the static environment and ignores the existences of other agents. Second, each agent learns when to coordinate with others according to their immediate reward changes, and then learns how to coordinate with others in a game-theoretic way. In this section, the negotiation-based framework for MAS with sparse interactions is first introduced and then related techniques and specific algorithms are described in details."}, {"heading": "A. Negotiation-based Framework for Sparse Interactions", "text": "We assume that agents have learnt their optimal policies and reward models when acting individually in the environment. Two situations might occur when agents are working in a multi-agent setting. If the received immediate rewards for state-action pairs are the same as what they learned by reward models, the agents act independently. Otherwise, they need to expand their state-action space to the joint state-action pairs for better coordination. This negotiation-based framework for sparse interactions is given as shown in Algorithm 1, while the expansion and negotiation process is explained as follows:\n1) Broadcast for joint state. Agents select an action at a certain state, and they detect a change in the immediate rewards. This state-action pair is marked as \u201cdangerous\u201d and these agents are called \u201ccoordinating agents\u201d. Then, \u201ccoordinating agents\u201d broadcast their state-action information to all other agents and receive corresponding state-action information from others. These state-action pairs with reward changes form a joint state-action and is marked as a \u201ccoordination pair\u201d. Also, these states form a joint state called a \u201ccoordination state\u201d, which is included in the state space of each \u201ccoordinating agent\u201d. 2) Negotiation for equilibrium policy. When agents select a \u201cdangerous\u201d state-action pair, they broadcast their stateaction information to each other to determine whether they are staying at a \u201ccoordination pair\u201d. If so, the agents need to find an equilibrium policy rather than their inherent greedy policies to avoid collisions. We propose a negotiation mechanism similar to the work in [8] to find this equilibrium policy. Each \u201ccoordinating agent\u201d broadcasts the set of strategy profiles that are potential Non-strict Equilibrium-Dominating Strategy\n4 Profile (non-strict EDSP) according to its own utilities (See Algorithm 2). If no non-strict EDSP is found, the agents search for a Meta equilibrium set (See Algorithm 3) instead, which is always nonempty [8]. Then, if there are several candidates in the equilibrium set, the agents use the minimum variance method to find the relatively good one (See Algorithm 4).\n3) When an agent selects an action at a certain state, if neither a change in the immediate reward nor a \u201cdangerous\u201d state-action pair is detected, the agent continue to select its action independently.\nRemark 1: If the number of agents n\u2265 2, the agents may have several different expansions for different \u201ccoordination states\u201d in one state. Each expanded joint-state is assigned an index and has a corresponding local Q-value. Once the agents form a \u201ccoordination state\u201d, they search for the joint state in their expanded joint-state pool and obtain the corresponding local Q-value. An example is shown as in Fig. 2. In this example, agent 1-4 has twelve states, four states, five states and three states, respectively. The fourth state of agent 1 is expanded to two joint states and the fifth state is expanded to three joint states.\nRemark 2: Owing to the broadcast mechanism, an agent will add the states of all the \u201ccoordinating agents\u201d to its state\nspace, even though it does not need to coordinate with some of these agents. The agents cannot distinguish the agent they need to coordinate with from other agents, which brings unnecessary calculations. This problem becomes more significant with the growth of the complexity of the environment and the number of the agents. Our tentative solution is setting the communication range of the agents so that they can only add neighbouring agents\u2019 states to their state space and ignore those that do not need to coordinate with."}, {"heading": "B. Negotiation for Equilibrium Set", "text": "One contribution of this paper is to apply the equilibrium solution concept to traditional Q-learning based MARL algorithms. Different from previous work like CQ-learning and Learning of Coordination [14], our approach aims at finding the equilibrium solution for the one-shot game played in each \u201ccoordination state\u201d. As a result, we focus on two pure strategy profiles [8], i.e., Non-strict Equilibrium-Dominating Strategy Profile (non-strict EDSP) and Meta Equilibrium set. The definition of Non-strict EDSP is as follows:\nDefinition 3: (Non-strict EDSP): In an n-agent (n \u2265 2) normal-form game \u0393, ~ei \u2208 A(i = 1,2, . . . ,m) are pure strategy Nash equilibriums. A joint action ~a \u2208 A is a non-strict EDSP if \u2200 j \u2264 n,\nU j(~a)\u2265 min i U j(~ei) (6)\nAlgorithm 1 Negotiation-based Framework for Sparse Interactions\nInput: The agent i, state space Si, action space Ai, learning rate \u03b1 , discount rate \u03b3 , exploration factor \u03b5 for the \u03b5 \u2212Greedy exploration policy. Initialize: Global Q-value Qi with optimal single-agent policy. 1: for each episode do 2: Initialize state si; 3: while true do 4: Select ai \u2208 Ai from Qi with \u03b5 \u2212Greedy; 5: if (si,ai) is \u201cdangerous\u201d then 6: Broadcasts (si,ai) and receives (s\u2212i,a\u2212i), form (~s,~a);\n/* See Section III-D for the definitions of s\u2212i and a\u2212i*/ 7: if (~s,~a) is not \u201ccoordination pair\u201d then 8: Mark (~s,~a) as a \u201ccoordination pair\u201d and ~s as a \u201ccoordination state\u201d, initialize Local Q-value QJi at ~s with transfer knowledge (See Equation 9); 9: end if 10: Negotiate for the equilibrium joint action with Algorithm 2 - Algorithm 4. Select new ~a with \u03b5 \u2212Greedy; 11: else {detected an immediate reward change} 12: Mark (si,ai) as \u201cdangerous\u201d, broadcasts (si,ai) and receives (s\u2212i,a\u2212i), form (~s,~a); 13: Mark (~s,~a) as \u201ccoordination pair\u201d and ~s as \u201ccoordination state\u201d, initialize QJi at ~s with transfer knowledge (See Equation 9); 14: Negotiate for the equilibrium joint action with Algorithm 2 - Algorithm 4. Select new ~a with \u03b5 \u2212Greedy; 15: end if 16: Move to the next state s\u2032i and receive the reward ri; 17: if ~s exists then 18: Update QJi (~s,~a) = (1\u2212\u03b1)Q J i (~s,~a)+\u03b1(ri + \u03b3 max\na\u2032i\nQi(s\u2032i,a \u2032 i));\n19: end if 20: Update Qi(si,ai) = (1\u2212\u03b1)Qi(si,ai)+\u03b1(ri + \u03b3 max\na\u2032i\nQi(s\u2032i,a \u2032 i));\n21: si \u2190 s\u2032i; 22: end while until si is a terminal state. 23: end for\nThe negotiation process of finding the set of Non-strict EDSP is shown as in Algorithm 2, which generally consists of two steps: 1) the agents find the set of strategy profiles that are potentially Non-strict EDSP according to their individual utilities; 2) the agents solve the intersection of all the potentially strategy sets and get the Non-strict EDSP set.\nHowever, given the fact that the pure strategy Nash equilibrium can be non-existent in some cases, the set of Nonstrict EDSP might also be empty. On this occasion, we replace this strategy profile with Meta Equilibrium, which is always nonempty. The sufficient and necessary condition of Meta Equilibrium is defined as follows:\nDefinition 4: (Sufficient and Necessary Condition of\nAlgorithm 2 Negotiation for Non-strict EDSPs Set\nInput: A normal-form game \u3008n,{Ai}i=1,...,n,{Ui}i=1,...,n\u3009. /* To be noted, \u201ccoordinating agent\u201d i only has the knowledge of n, {Ai}i=1,...,n and Ui*/ Initialize: The set of non-strict EDSP candidates for \u201ccoordinating agents\u201d i: JiNS \u2190 /0; Minimum utility value of pure strategy Nash equilibrium (PNE) candidates for \u201ccoordinating agents\u201d i: MinU iPNE \u2190 \u221e; The set of non-strict EDSPs: JNS \u2190 /0.\n1: for each ~a\u2212i \u2208 A\u2212i do 2: if max\na\u2032i Ui(a\u2032i, ~a\u2212i)< MinU i PNE then\n3: MinU iPNE = max a\u2032i Ui(a\u2032i, ~a\u2212i); 4: end if 5: end for 6: for each ~a \u2208 A do 7: if Ui(~a)\u2265 MinU iPNE then 8: JiNS \u2190 J i NS \u222a{~a};\n9: end if 10: end for\n/* Broadcast JiNS and corresponding utilities*/ 11: JNS \u2190 \u22c2n i=1 J i NS\nMeta Equilibrium) [8]: In an n-agent (n \u2265 2) normal-form game \u0393, a joint action ~a is called a Meta equilibrium from a metagame k1k2 . . .kr\u0393 if and only if for any i there holds:\nUi(~a)\u2265 min ~aPi max ai min ~aSi Ui(~aPi ,ai, ~aSi), (7)\nwhere Pi is the set of agents listed before sign i in the prefix k1k2 . . .kr, Si is the set of agents listed after sign i in the prefix.\nFor example, in a three agents metagame 213\u0393, we have P1 = {2},S1 = {3};P2 = /0,S2 = {1,3};P3 = {2,1},S3 = /0. A Meta equilibrium ~a meets the following constraints:\nU1(~a)\u2265 min a2 max a1 min a3 U1(a1,a2,a3), U2(~a)\u2265 max a2 min a1 min a3 U2(a1,a2,a3), U3(~a)\u2265 min a2 min a1 max a3 U3(a1,a2,a3).\n(8)\nHu et al [8] used a negotiation-based method to find Meta Equilibrium set and we simplified the method as shown in Algorithm 3. It is also pointed out that both of these two strategy profiles belong to the set of symmetric meta equilibrium, which to some degree strengthens the convergence of the algorithm."}, {"heading": "C. Minimum Variance Method", "text": "In Section III-B, we presented the process for all \u201ccoordinating agents\u201d to negotiate for an equilibrium joint-action set. However, the obtained set usually contains many strategy profiles and it is difficult for the agents to choose an appropriate one. In this paper a Minimum Variance method is proposed to help the \u201ccoordinating agents\u201d choose the joint action with relatively high total utility and minimum utilities variance to guarantee the cooperation and fairness of the learning process. In addition, if the equilibrium set is nonempty, the best solution defined in the minimum variance method always exists. The minimum variance method is described in Algorithm 4.\nAfter negotiating for one equilibrium, the agents update their states according to their joint actions and receive immediate rewards, which are used to update local Q-values as well\n6 as global Q-values. Unlike other sparse-interaction algorithms (e.g., CQ-learning), we update the global optimal Q-values to avoid possible misleading information. In fact, in some cases, the selected policies in multi-agent setting are totally opposite to the agents\u2019 individual optimal policies due to dynamic coordinating processes. The whole negotiation-based learning algorithm has already been given in Algorithm 1."}, {"heading": "D. Local Q-value transfer", "text": "At the beginning of the learning process, we use the transfer knowledge of agents\u2019 optimal single agent policy to accelerate the learning process. Furthermore, it is possible to improve the algorithm performance by the local Q-value transfer. In most previous literatures [13] [14], the initial local Q-values of the newly expanded joint states are zeros. Recently, Vrancx et al proposed a transfer learning method [32] to initialize these Q-values with prior trained Q-value from the source task, which is reasonable in the real world. When people meet with others on the way to their individual destinations, they usually have prior knowledge of how to avoid collisions. Based on this prior commonsense and the knowledge of how to finish their individual tasks, the agents learn to negotiate with others and obtain fixed coordination strategies suitable for certain environments. However, Vrancx et al only used coordination knowledge to initialize the local Q-values and overlooked the environmental information, which was proved to be less effective in our experiments. In our approach, we initialize the local Q-values of newly detected \u201ccoordination\u201d to hybrid knowledge as follows:\nQJi ((si, ~s\u2212i),(ai, ~a\u2212i))\u2190 Qi(si,ai)+Q CT i (~s,~a), (9)\nwhere ~s\u2212i is the joint-state set except for the state si and ~a\u2212i is the joint-action set except for the action ai, QJi ((si, ~s\u2212i),(ai, ~a\u2212i)) is equal with Q J i (~s,~a), Q CT i (~s,~a) is the\nAlgorithm 3 Negotiation for Meta Equilibrium Set for 3-agent games\nInput: A normal-form game < n,{Ai}i=1,...,n,{Ui}i=1,...,n >. /* To be noted, \u201ccoordinating agents\u201d i only has the knowledge of n, {Ai}i=1,...,n and Ui*/ Initialize: The set of Meta Equilibrium candidates for \u201ccoordinating agents\u201d i: JiMetaE \u2190 /0; Minimum utility value of Meta Equilibrium candidates for \u201ccoordinating agents\u201d i: MinU iMetaE \u2190 \u221e; The set of Meta Equilibrium: JMetaE \u2190 /0;\n1: Randomly initialize the prefix s1s2s3 from the set {123,132,213,231,312,321}. 2: Calculate MinU iMetaE according to Equation 7; 3: for each ~a \u2208 A do 4: if Ui(~a)\u2265 MinU iMetaE then 5: JiMetaE \u2190 J i MetaE \u222a{~a}; 6: end if 7: end for\n/* Broadcast JiMetaE and corresponding utilities*/ 8: JMetaE \u2190 \u22c2n i=1 J i MetaE\nAlgorithm 4 Minimum Variance Method\nInput: The equilibrium set with m elements JNS = { ~a1ns, ~a2ns, . . . , ~amns} and corresponding utilities {U ns i }i=1,...,n. Initialize: threshold value for total utility \u03c4; /* We set the threshold value to the mean value of the sum\nutilities of different joint-action profiles \u2211mj=1 \u2211 n i=1 U ns i ( ~ a jns)\nm */ Minimum variance of these equilibriums MinV \u2190 \u221e; Best equilibrium of the non-strict EDSPs set JBestNS \u2190 /0.\n1: for each ~a jns \u2208 JNS do 2: if \u2211ni=1 Unsi (\n~a jns)< \u03c4 then 3: JNS \u2190 JNS\\{\n~a jns}; 4: end if 5: end for\n/* Minimize the joint action\u2019s utility variance*/\n6: for each ~a jns \u2208 JNS do 7: if \u221a\n1 n \u2211 n k=1[U ns k ( ~a jns)\u2212 1n \u2211 n i=1 U ns i ( ~a jns)]2 < MinV then\n8: MinV = \u221a\n1 n \u2211 n k=1[U ns k ( ~a jns)\u2212 1n \u2211 n i=1 U ns i ( ~a jns)]2;\n9: JBestNS = { ~a jns};\n10: end if 11: end for 12: Output JBestNS as the adopted joint action.\ntransferred Q-value from a blank source task at joint state ~s. In this blank source task, joint action learners (JAL) [33] learn to coordinate with others disregarding the environmental information. They are given a fixed number of learning steps to learn stable Q-value QCTi (~s,~a). Similar to [32], the joint state ~s in QCTi (~s,~a) is presented as the relative position (\u2206x,\u2206y), horizontally and vertically. When agents attempt to move into the same grid location or their previous locations (as shown in Fig. 3), these agents receive a penalty of -10. In other cases, the reward is zero.\nTake a two-agent source task for example (as shown in Fig. 4(a)). The locations of agent 1 and agent 2 are (4,3) and (3,4), respectively. Then we have the joint state ~s = (x1 \u2212 x2,y1 \u2212 y2) = (1,\u22121) and the joint action for this joint state\n~a = ( (a1,a2) ) =\n\n   (\u2191,\u2191) (\u2191,\u2193) (\u2191,\u2190) (\u2191,\u2192) (\u2193,\u2191) (\u2193,\u2193) (\u2193,\u2190) (\u2193,\u2192) (\u2190,\u2191) (\u2190,\u2193) (\u2190,\u2190) (\u2190,\u2192) (\u2192,\u2191) (\u2192,\u2193) (\u2192,\u2190) (\u2192,\u2192)\n\n   ,\nwhere {\u2191,\u2193,\u2190,\u2192} denotes the action set of {up, down, left, right} for each agent. After sufficient learning steps, the agents learn their Q-value matrices at joint state ~s as:\nQCT1 (~s, \u00b7) = Q CT 2 (~s, \u00b7) =\n\n   0 0 \u221210 0 0 0 0 0 0 0 0 0 0 \u221210 0 0\n\n   .\nSuppose that the agents are in the \u201ccoordination state\u201d as shown in Fig. 4(b). When acting independently in the environment, the agents learn their single agent optimal Q-value vectors at state s1 or s2 as: Q1(s1, \u00b7) = (\u22121,\u221210,\u22121,\u22125), Q2(s2, \u00b7) = (\u221210,\u22121,5,\u22121). Then the local Q-value matrices\n7 of this \u201ccoordination state\u201d need to be initialized as:\nQJ1(~s, \u00b7) = Q1(s1, \u00b7) T \u00d7 (1,1,1,1)+QCT1 (~s, \u00b7)\n=\n\n   \u22121 \u22121 \u221211 \u22121 \u221210 \u221210 \u221210 \u221210 \u22121 \u22121 \u22121 \u22121 5 \u22125 5 5\n\n   ,\nQJ2(~s, \u00b7) = (1,1,1,1) T \u00d7Q2(s2, \u00b7)+Q CT 2 (~s, \u00b7)\n=\n\n   \u221210 \u22121 \u22125 \u22121 \u221210 \u22121 5 \u22121 \u221210 \u22121 5 \u22121 \u221210 \u221211 5 \u22121\n\n   .\nSo the pure strategy Nash equilibrium for this joint state is (right, left). If we initialize the local Q-value with the way used in [32] or just initialize them to zeros, the learning process would be much longer.\nFor the \u201ccoordination state\u201d with three agents, the Q-value QCTi (~s,~a) can be calculated in the same way, except for the relative positions (\u2206x1,\u2206y1,\u2206x2,\u2206y2) = (x1 \u2212 x2,y1 \u2212 y2,x2 \u2212 x3,y2 \u2212 y3) and the cubic Q-value for each joint state."}, {"heading": "IV. EXPERIMENTS", "text": "To test the presented NegoSI algorithm, several groups of simulated experiments are implemented and the results are compared with those of three other state-of-the-art MARL algorithms, namely, CQ-learning [14], NegoQ with value function transfer (NegoQ-VFT) [27] and independent learners with value function transfer (IL-VFT). In next two subsections, the presented NegoSI algorithm is applied to six grid world games and an intelligent warehouse problem, which shows that NegoSI is an effective approach for MARL problems compared with other existing MARL algorithms.\nFor all these experiments, each agent has four actions: up, down, left, right. The reward settings are as follows:\n1) When an agent reaches its goal/goals, it receives a reward of 100. Its final goal is an absorbing state. One episode is over when all agents reach their goals. 2) A negative reward of -10 is received if a collision happens or an agent steps out of the border. In these cases, agents will bounce to their previous states. 3) Otherwise, the reward is set to -1 as the power consumption.\nThe settings of other parameters are the same for all algorithms: learning rate \u03b1 = 0.1, discount rate \u03b3 = 0.9, exploration\n1 2 3 4 5\nfactor \u03b5 = 0.01 for \u03b5-greedy strategy. All algorithms run 2000 iterations for the grid world games and 8000 iterations for the intelligent warehouse problem. We use three typical criteria to evaluate these MARL algorithms, i.e., steps of each episode (SEE), rewards of each episode (REE) and average runtime (AR). All the results are averaged over 50 runs."}, {"heading": "A. Tests on grid world games", "text": "The proposed NegoSI algorithm is evaluated in the grid world games presented by Melo and Veloso [13], which are shown in Fig. 5. The first four benchmarks, i.e., ISR, SUNY, MIT and PENTAGON (shown as Fig. 5(a)-(d), respectively), are two-agent games, where the cross symbols denote the initial locations of each agent and the goals of the other agent.\n8 0 5 10 15 20 0 20 40 60 80\nepisodes (\u00d7100)\nst ep\ns of\ne ac\nh ep\nis od\ne\nCQ ILVFT NegoQVFT NegoSI\n(a) ISR\n0 5 10 15 20 10\n15\n20\n25\nepisodes (\u00d7100)\nst ep\ns of\ne ac\nh ep\nis od\ne\nCQ ILVFT NegoQVFT NegoSI\n(b) SUNY\n0 5 10 15 20 0\n20\n40\n60\n80\nepisodes (\u00d7100)\nst ep\ns of\ne ac\nh ep\nis od\ne\nCQ ILVFT NegoQVFT NegoSI\n(c) MIT\n0 5 10 15 20 0\n20\n40\n60\nepisodes (\u00d7100)\nst ep\ns of\ne ac\nh ep\nis od\ne\nCQ ILVFT NegoQVFT NegoSI\n(d) PENTAGON\n0 5 10 15 20 10\n15\n20\n25\n30\nepisodes (\u00d7100)\nst ep\ns of\ne ac\nh ep\nis od\ne\nCQ ILVFT NegoQVFT NegoSI\n(e) GW nju\n0 5 10 15 20 0\n20\n40\n60\n80\nepisodes (\u00d7100)\nst ep\ns of\ne ac\nh ep\nis od\ne\nCQ ILVFT NegoQVFT NegoSI\n(f) GWa3\nFig. 6: SEE (steps of each episode) for each tested benchmark map.\nIn addition, we design two highly competitive games to further test the algorithms, namely, GW nju (Fig. 5(e)) and GWa3 (Fig. 5(f)). The game GW nju has two agents and the game GWa3 has three agents. The initial location and the goal of agent i are represented by the number i and Gi, respectively.\nWe first examine the performances of the tested algorithms regarding the SEE (steps of each episode) for each benchmark map (See Fig. 6). The state-of-the-art value function transfer mechanism helps NegoQ and ILs to converge fast in all games except for SUNY. NegoSI also has good convergence characteristics. CQ-learning, however, has a less stable learning curve especially in PENTAGON, GW nju and GWa3. This is reasonable since in these highly competitive games, the agents\u2019 prior-learned optimal policies always have conflicts and CQ-learning cannot find the equilibrium joint action to coordinate them. When more collisions occur, the agents are frequently bounced to previous states and forced to take more steps before reaching their goals. The learning steps of the final episode for each algorithm in each benchmark map are shown in Table I. In ISR, NegoSI converges to 7.48, which\nis the closest one to the value obtained by the optimal policy. In other cases, NegoSI achieves the learning step of the final episode between 105.1% and 120.4% to the best solution.\nThen we analyze the REE (rewards of each episode) criterion of these tested algorithms (See Fig. 7-12). The results vary for each map. In ISR, NegoSI generally achieves the highest REE through the whole learning process, which shows fewer collisions and more cooperations between the learning agents. Also, the difference between the two agents\u2019 reward values is small, which reflects the fairness of the learning process of NegoSI. The agents are more independent in SUNY. Each of them has three candidates for the single-agent optimal policy. In this setting, ILVFT has good performance. NegoSI shows its fairness and high REE value compared with NegoVFT and CQ-learning. The agents in MIT have more choices of collision avoidances. All the tested algorithms obtain the final reward of around 80. However, the learning curves of CQ-learning are relatively unstable. In PENTAGON, NegoSI proves its fairness and achieves as good performance as algorithms do with the value function transfer.\nOther than these above benchmarks, we give two highly competitive games to test the proposed algorithm. In both games, the agents\u2019 single-agent optimal policies conflict with each other and need appropriate coordination. In GW nju, the learning curves of NegoSI finally converge to 91.39 and 90.09, which are very close to the optimal final REE values as 93 and 91, respectively. Similar to NegoSI, NegoVFT achieves the final REE values of 91.27 and 91.25. However, NegoSI can negotiate for a fixed and safer policy that allows one agent to always move greedily and the other to avoid collision (while NegoVFT cannot), which shows the better coordination ability of NegoSI. For the three-agent grid world GWa3, even though NegoSI does not have the lowest SEE, it achieves the highest REE through the whole learning process. This result shows that the agents using NegoSI have the ability to avoid collisions to obtain more rewards while the agents using traditional MARL methods have less desire to coordinate and therefore lose their rewards. It also demonstrates the scalability of NegoSI in the three-agent setting.\nThe results regarding AR (average runtime) are shown in Table II. ILVFT has the fastest learning speed, which is only five to ten times slower than Q-learning to learn single policy. CQ-learning only considers joint states in \u201ccoordination state\u201d and it also has a relatively small computational complexity. The AR of NegoVFT is five to ten times more than that of ILVFT. This is reasonable since NegoVFT learns in the whole joint state-action space and computes the equilibrium\n10\nfor each joint state. The learning speed of NegoSI is slower than CQ-learning but faster than NegoVFT. Even if NegoSI adopts the sparse-interaction based learning framework and has computational complexity similar to CQ-learning, it needs to search for the equilibrium joint action in the \u201ccoordination state\u201d, which slows down the learning process."}, {"heading": "B. A real-world application: intelligent warehouse systems", "text": "MARL has been widely used in such simulated domains as grid worlds [1], but few applications have been found for realistic problems comparing to single-agent reinforcement learning (RL) algorithms [34]-[38]. In this paper, we apply the proposed NegoSI algorithm to an intelligent warehouse problem. Previously, single agent path planning methods have been successfully used in complex systems [39] [40], however, the intelligent warehouse employs a team of mobile robots to transport objects and single-agent path planning methods frequently cause collisions [41]. So we solve the multi-agent coordination problem in a learning way.\nThe intelligent warehouse is made up of three parts: picking stations, robots and storage shelves (shown as in Fig. 13). There are generally four steps of the order fulfillment process for intelligent warehouse systems:\n1) Input and decomposition: input and decompose orders into separated tasks; 2) Task allocation: the central controller allocates the tasks to corresponding robots using task allocation algorithms (e.g., the Auction method);\n3) Path planning: robots plan their transportation paths with a single-agent path planning algorithm; 4) Transportation and collision avoidance: robots transport their target storage shelves to the picking station and then bring them back to their initial positions. During the transportation process, robots use sensors to detect shelves and other robots to avoid collisions.\nWe focus on solving the multi-robot path planning and collision avoidance in a MARL way and ignore the first two steps of the order fulfillment process by requiring each robot to finish a certain number of random tasks. The simulation platform of an intelligent warehouse system is shown as in Fig. 13, which is a 16\u00d7 21 grid environment and each grid is of size 0.5m\u00d70.5m in real world. Shaded grids are storage shelves which cannot be passed through. The state space of each robot is made up of two parts: the location and the task number. Each robot has 4 actions, namely, \u201cup\u201d, \u201cdown\u201d, \u201cleft\u201d and \u201cright\u201d.\nWe only compare the NegoSI algorithm with CQ-learning for the intelligent warehouse problem. In fact, ILVFT has no guarantee of the convergence characteristics and it is difficult to converge in the intelligent warehouse setting in practice. NegoVFT is also infeasible since its internal memory cost in MATLAB is estimated to be 208\u00d7208\u00d7208\u00d710\u00d710\u00d710\u00d7 4\u00d74\u00d74\u00d78B= 4291GB, while the costs of other algorithms are about 2 MB. Like NegoVFT, other MG-based MARL algorithms cannot solve the intelligent warehouse problem either. Experiments were performed in 2-agent and 3-agent settings, respectively, and the results are shown as in Fig. 14 and Fig. 15.\nIn the 2-agent setting, the initial position and final goal of robot 1 are (1,1) and those of robot 2 are (1,16). Each robot needs to finish 30 randomly assigned tasks. The task set is the same for all algorithms. Robots with NegoSI achieve lower SEE (steps of each episode) than robots with CQ-learning throughout the whole learning process (as shown in Fig. 14). NegoSI finally converges to 449.9 steps and CQ-learning converges to 456.9 steps. In addition, robots with NegoSI have higher and more stable REE (rewards of each episode). Finally, the average runtime for completing all the tasks is 2227s for NegoSI and is 3606s for CQ-learning. Robots used 38% less time to complete all the tasks with NegoSI than that with CQlearning.\nThe performances of NegoSI are also better in the 3-agent setting than that of CQ-learning. The initial position and the final goal of different robots are (1,1), (1,8) and (1,16). Each robot needs to finish 10 randomly assigned tasks. The task\n11\nset is the same for different algorithms. SEE (steps of each episode) for robots with NegoSI finally converges to 168.7 steps and that for robots with CQ-learning converges to 177.3 steps (as shown in Fig. 15). In addition, the learning curves of NegoSI are more stable. Robots with NegoSI have higher and more stable REE (rewards of each episode). Finally, the average runtime for completing all the tasks is 1352s for NegoSI and 2814s for CQ-learning. The robots use 52% less time to complete all the tasks with NegoSI than that with CQlearning.\nRemark 3: The agents with CQ-learning algorithm learn faster than the agents with NegoSI for the benchmark maps in Section IV-A, but slower than those with NegoSI for the intelligent warehouse problem in Section IV-B. The reason is that for agents with CQ-learning, the number of \u201ccoordination state\u201d is several times higher than that with NegoSI. This difference becomes significant with the increase of the task number, the environment scale and the number of agents. Thus, the time used to search for one specific \u201ccoordination state\u201d in the \u201ccoordination state\u201d pool increases faster in CQ-learning than in NegoSI, which results in the increase of the whole learning time. According to all these experimental results, the presented NegoSI algorithm maintains better performances regarding such characteristics as coordination ability, convergence, scalability and computational complexity, especially for practical problems."}, {"heading": "V. CONCLUSIONS", "text": "In this paper a negotiation-based MARL algorithm with sparse interactions (NegoSI) is proposed for the learning\nand coordination problems in multi-agent systems. In this integrated algorithm the knowledge transfer mechanism is also adopted to improve agent\u2019s learning speed and coordination ability. In contrast to traditional sparse-interaction based MARL algorithms, NegoSI adopts the equilibrium concept and makes it possible for agents to select non-strict EDSP or Meta equilibrium for their joint actions, which makes it easy to find near optimal (or even optimal) policy and to avoid collisions as well. The experimental results demonstrate the effectiveness of the presented NegoSI algorithm regarding such characteristics as fast convergence, low computational complexity and high scalability in comparison to the state-ofthe-art MARL algorithms, especially for practical problems. Our future work focuses on further comparison of NegoSI with other existing MARL algorithms and more applications of MARL algorithms to general and realistic problems. In addition, multi-objective reinforcement learning (MORL) [42]- [44] will also be considered to further combine environment information and coordination knowledge for local learning."}, {"heading": "ACKNOWLEDGEMENT", "text": "The authors would like to thank Dr. Yujing Hu and Dr. Jiajia Dou for helpful discussion."}], "references": [{"title": "A comprehensive survey of multi-agent reinforcement learning", "author": ["L. Bu\u015foniu", "R. Babu\u0161ka", "B.D. Schutter"], "venue": "IEEE Transactions on System, Man, and Cybernetics, Part C: Applications and Reviews, vol. 38, no. 2, pp. 156-172, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Heuristicallyaccelerated multiagent reinforcement learning", "author": ["R. Bianchi", "M.F. Martins", "C.H. Ribeiro", "A.H. Costa"], "venue": "IEEE Transactions on Cybernetics, vol. 44, no. 2, pp. 252-265, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Cooperative multiagent congestion control for high-speed networks", "author": ["K. Hwang", "S. Tan", "M. Hsiao", "C. Wu"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 35, no. 2, pp. 255-268, 2005.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Multiagent learning of coordination in loosely coupled multiagent systems", "author": ["C. Yu", "M. Zhang", "F. Ren", "G. Tan"], "venue": "IEEE Transaction on Cybernetics, DOI: 10.1109/TCYB.2014.2387277, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Markov games as a framework for multi-agent reinforcement learning", "author": ["M.L. Littman"], "venue": "Proceedings of the International Conference on Machine Learning (ICML-94), pp.157-163, New Brunswick, NJ, July 10-13, 1994.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1994}, {"title": "Decentralized MDPs with sparse interactions", "author": ["F.S. Melo", "M. Veloso"], "venue": "Artificial Intelligence, vol. 175, no. 11, pp. 1757-1789, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiagent reinforcement learning with unshared value functions", "author": ["Y. Hu", "Y. Gao", "B. An"], "venue": "IEEE Transaction on Cybernetics, vol. 45, no. 4, pp. 647-662, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Accelerating multiagent reinforcement learning by equilibrium transfer", "author": ["Y. Hu", "Y. Gao", "B. An"], "venue": "IEEE Transaction on Cybernetics, vol.45, no. 7, pp. 1289 - 1302, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Nash Q-learning for general-sum stochastic games", "author": ["J. Hu", "M.P. Wellman"], "venue": "The Journal of Machine Learning Research, vol. 4, pp. 1039C1069, 2003.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Friend-or-foe Q-learning in general-sum games", "author": ["M.L. Littman"], "venue": "Proceedings of the International Conference on Machine Learning (ICML-01), pp. 322-328, Williams College, Williamstown, MA, USA, 2001.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Correlated Q-learning", "author": ["A. Greenwald", "K. Hall", "R. Serrano"], "venue": "Proceedings of the International Conference on Machine Learning (ICML-03), pp. 84-89, Washington, DC, USA, 2003.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning of coordination: Exploiting sparse interactions in multiagent systems", "author": ["F.S. Melo", "M. Veloso"], "venue": "Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 773-780, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning multi-agent state space representations", "author": ["Y.D. Hauwere", "P. Vrancx", "A. Now\u00e9"], "venue": "Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), vol. 1, no. 1, pp. 715-722, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Generalized learning automata for multi-agent reinforcement learning", "author": ["Y.D. Hauwere", "P. Vrancx", "A. Now\u00e9"], "venue": "AI Communications, vol. 23, no. 4, pp. 311-324, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Community-aware task allocation for social networked multiagent systems", "author": ["W. Wang", "Y. Jiang"], "venue": "IEEE Transactions on Cybernetics, vol. 44, no. 9, pp. 1529-1543, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimization and coordinated autonomy in mobile fulfillment systems", "author": ["J. Enright", "P.R. Wurman"], "venue": "Automated Action Planning for Autonomous Mobile Robots, pp. 33-38, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiagent systems: a survey from the machine learning perspective", "author": ["P. Stone", "M. Veloso"], "venue": "Autonomous Robots, vol. 8, no. 3, pp. 345-383, 2000.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Equilibrium points in n-person games", "author": ["J.F. Nash"], "venue": "Proceedings of the National Academy of Sciences of the United States of America, vol. 36, no. 1, pp. 48-49, 1950.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1950}, {"title": "Heuristics for negotiation schedules in multi-plan optimization", "author": ["B. An", "F. Douglis", "F. Ye"], "venue": "Proceedings of the International Conference on Autonomous Agents and Multiagent Aystems (AAMAS), vol. 2, pp. 551-558, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Agent-mediated multistep optimization for resource allocation in distributed sensor networks", "author": ["B. An", "V. Lesser", "D. Westbrook", "M. Zink"], "venue": "Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), vol. 2, pp. 609-616, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Reinforcement learning: an introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press, 1998.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning from delayed rewards", "author": ["C. Watkins"], "venue": "PhD thesis, University of Cambridge, 1989.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1989}, {"title": "Asynchronous stochastic approximation and Q-learning", "author": ["J. Tsitsiklis"], "venue": "Machine Learning, vol. 16, no. 3, pp. 185-202, 1994.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1994}, {"title": "Game theory and multiagent reinforcement learning", "author": ["A. Now\u00e9", "P. Vrancx", "Y.D. Hauwere"], "venue": "Reinforcement Learning, Springer Berlin Heidelberg, pp. 441-470, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Simple search methods for finding a Nash equilibirum", "author": ["R. Porter", "E. Nudelman", "Y. Shoham"], "venue": "Games and Economic Behavior, vol. 63, no. 2, pp. 642-662, 2008.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning in Multi-agent Systems with Sparse Interactions by Knowledge Transfer and Game Abstraction", "author": ["Y. Hu", "Y. Gao", "B. An"], "venue": "Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 753-761, Istanbul, Turkey, 4-8 May, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Coordinated reinforcement learning", "author": ["C. Guestrin", "M. Lagoudakis", "R. Parr"], "venue": "Proceedings of the International Conference on Machine Learning (ICML-02), vol. 2, pp. 227-234. 2002.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Context-specific multiagent coordination and planning with factored MDPs", "author": ["C. Guestrin", "S. Venkataraman", "D. Koller"], "venue": "Proceedings of the National Conference on Artificial Intelligence, pp. 253-259. 2002.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "Sparse cooperative Q-learning", "author": ["J.R. Kok", "N.A. Vlassis"], "venue": "Proceedings of the International Conference on Machine Learning (ICML-04), pp. 61-68, Banff, Alberta, Canada, 2004.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "Solving sparse delayed coordination problems in multi-agent reinforcement learning", "author": ["Y.D. Hauwere", "P. Vrancx", "A. Now\u00e9"], "venue": "Adaptive and Learning Agents, Springer Berlin Heidelberg, pp. 114-133, 2012.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Transfer learning for multiagent coordination", "author": ["P. Vrancx", "Y.D. Hauwere", "A. Now\u00e9"], "venue": "Proceedings of the International Conference on Agents and Artificial Intelligence (ICAART), pp. 263-272, 2011.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "The dynamics of reinforcement learning in cooperative multiagent systems", "author": ["C. Claus", "C. Boutilier"], "venue": "Proceedings of the fifteenth national/tenth conference on Artificial intelligence/Innovative applications of artificial intelligence, pp. 746-752, July 26C30, 1998.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1998}, {"title": "Cooperative, hybrid agent architecture for real-time traffic signal control", "author": ["M.C. Choy", "D. Srinivasan", "R.L. Cheu"], "venue": "IEEE Transactions on Systems, Man and Cybernetics, Part A: Systems and Humans, vol. 33, no. 5, pp. 597-607, 2003.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2003}, {"title": "Intercell interference management in ofdma networks: A decentralized approach based onreinforcement learning", "author": ["F. Bernardo", "R.R. Agusti", "J.J. Perez-Romero", "O. Sallent"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews, vol. 41, no. 6, pp. 968-976, 2011.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Intelligent Deflection Routing in Buffer- Less Networks", "author": ["S. Haeri", "L. Trajkovic"], "venue": "IEEE Transactions on Cybernetics, vol. 45, no. 2, pp. 316-327, 2015.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning versus model predictive control: a comparison on a power system problem", "author": ["D. Ernst", "M. Glavic", "F. Capitanescu", "L. Wehenkel"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 39, no. 2, pp. 517-529, 2009.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2009}, {"title": "Improved adaptiveCreinforcement learning control for morphing unmanned air vehicles", "author": ["J. Valasek", "J. Doebbler", "M.D. Tandale", "A.J. Meade"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 38, no. 4, pp. 1014-1020, 2008.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "A balanced heuristic mechanism for multirobot task allocation of intelligent warehouses", "author": ["L. Zhou", "Y. Shi", "J. Wang", "P. Yang"], "venue": "Mathematical Problems in Engineering, vol. 2014, article ID 380480, 2014.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Simulation-based evaluations of reinforcement learning algorithms for autonomous mobile robot path planning", "author": ["H.H. Viet", "P.H. Kyaw", "T. Chung"], "venue": "IT Convergence and Services, Springer Netherlands, pp. 467- 476, 2011.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Coordinating hundreds of cooperative, autonomous vehicles in warehouses", "author": ["P.R. Wurman", "R. D\u2019Andrea", "M. Mountz"], "venue": "AI Magazine, vol. 29, no. 1, pp. 9-20, 2008.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2008}, {"title": "Empirical evaluation methods for multiobjective reinforcement learning algorithms", "author": ["P. Vamplew", "R. Dazeley", "A. Berry", "R. Issabekov", "E. Dekker"], "venue": "Machine Learning, vol. 84, no. 1-2, pp. 51-80, 2011.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "A reinforcement neuro-fuzzy combiner for multiobjective control", "author": ["C. Lin", "I. Chung"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 29, no. 6, pp. 726-744, 1999.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1999}, {"title": "Multiobjective reinforcement learning: A comprehensive overview", "author": ["C. Liu", "X. Xu", "D. Hu"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 45, no. 3, pp. 385-398, 2015.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Multi-agent learning is drawing more and more interests from scientists and engineers in multi-agent systems (MAS) and machine learning communities [1]-[4].", "startOffset": 161, "endOffset": 164}, {"referenceID": 2, "context": "INTRODUCTION Multi-agent learning is drawing more and more interests from scientists and engineers in multi-agent systems (MAS) and machine learning communities [1]-[4].", "startOffset": 165, "endOffset": 168}, {"referenceID": 3, "context": "One key technique for multi-agent learning is multi-agent reinforcement learning (MARL), which is an extension of reinforcement learning in multi-agent domain [5].", "startOffset": 159, "endOffset": 162}, {"referenceID": 4, "context": "been built as frameworks of MARL, such as Markov games (MG) [6] and decentralized sparse-interaction Markov decision processes (Dec-SIMDP) [7].", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "been built as frameworks of MARL, such as Markov games (MG) [6] and decentralized sparse-interaction Markov decision processes (Dec-SIMDP) [7].", "startOffset": 139, "endOffset": 142}, {"referenceID": 4, "context": "Several well-known equilibrium-based MARL algorithms [6]-[12] are derived from this model.", "startOffset": 53, "endOffset": 56}, {"referenceID": 10, "context": "Several well-known equilibrium-based MARL algorithms [6]-[12] are derived from this model.", "startOffset": 57, "endOffset": 61}, {"referenceID": 11, "context": "Typical Dec-SIMDP based algorithms include LoC [13] and CQ-learning [14].", "startOffset": 47, "endOffset": 51}, {"referenceID": 12, "context": "Typical Dec-SIMDP based algorithms include LoC [13] and CQ-learning [14].", "startOffset": 68, "endOffset": 72}, {"referenceID": 13, "context": "Besides, other models such as learning automata [2] [15] are also valuable tools for designing MARL algorithms.", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "In spite of the rapid development of MARL theories and algorithms, more efforts are needed for practical applications of MARL when compared with other MAS techniques [16][18] due to some limitations of the existing MARL methods.", "startOffset": 166, "endOffset": 170}, {"referenceID": 16, "context": "In spite of the rapid development of MARL theories and algorithms, more efforts are needed for practical applications of MARL when compared with other MAS techniques [16][18] due to some limitations of the existing MARL methods.", "startOffset": 170, "endOffset": 174}, {"referenceID": 17, "context": ", Nash equilibrium [19]) for each time step and all joint states are computationally expensive [8], even for relatively small scale environments with two or three agents.", "startOffset": 19, "endOffset": 23}, {"referenceID": 6, "context": ", Nash equilibrium [19]) for each time step and all joint states are computationally expensive [8], even for relatively small scale environments with two or three agents.", "startOffset": 95, "endOffset": 98}, {"referenceID": 18, "context": ", streaming processing systems [20], sensor networks [21]) given the agents\u2019 privacy protections and huge real-time communication costs [1].", "startOffset": 31, "endOffset": 35}, {"referenceID": 19, "context": ", streaming processing systems [20], sensor networks [21]) given the agents\u2019 privacy protections and huge real-time communication costs [1].", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": ", streaming processing systems [20], sensor networks [21]) given the agents\u2019 privacy protections and huge real-time communication costs [1].", "startOffset": 136, "endOffset": 139}, {"referenceID": 20, "context": "MDP describes a sequential decision problem as follows [22]: Definition 1: (Markov Decision Process, MDP) A MDP is a tuple \u3008S,A,R,T \u3009, where S is the state space, A is the action space of the agent, R : S\u00d7A\u2192R is the reward function mapping state-action pairs to rewards, T : S\u00d7A\u00d7S \u2192 [0,1] is the transition function.", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "MDP describes a sequential decision problem as follows [22]: Definition 1: (Markov Decision Process, MDP) A MDP is a tuple \u3008S,A,R,T \u3009, where S is the state space, A is the action space of the agent, R : S\u00d7A\u2192R is the reward function mapping state-action pairs to rewards, T : S\u00d7A\u00d7S \u2192 [0,1] is the transition function.", "startOffset": 283, "endOffset": 288}, {"referenceID": 0, "context": "where V \u2217(s) stands for the value of a state s under the optimal policy, \u03c0 : S\u00d7A\u2192 [0,1] stands for the policy of an agent, E\u03c0 is the expectation under policy \u03c0 , t is any time step, k represents a future time step, rt+k denotes the reward at the time step (t + k) and \u03b3 \u2208 [0,1] is a parameter called the discount factor.", "startOffset": 82, "endOffset": 87}, {"referenceID": 0, "context": "where V \u2217(s) stands for the value of a state s under the optimal policy, \u03c0 : S\u00d7A\u2192 [0,1] stands for the policy of an agent, E\u03c0 is the expectation under policy \u03c0 , t is any time step, k represents a future time step, rt+k denotes the reward at the time step (t + k) and \u03b3 \u2208 [0,1] is a parameter called the discount factor.", "startOffset": 272, "endOffset": 277}, {"referenceID": 21, "context": "One classic RL algorithm for estimating Q(s,a) is Q-learning [23], whose one-step updating rule is as follows: Q(s,a)\u2190 (1\u2212\u03b1)Q(s,a)+\u03b1[r(s,a)+ \u03b3 max a\u2032 Q(s,a)], (3)", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "where Q(s,a) denotes the state-action value function at a stateaction pair (s,a) and \u03b1 \u2208 [0,1] is a parameter called the learning rate.", "startOffset": 89, "endOffset": 94}, {"referenceID": 22, "context": "Provided that all state-action pairs are visited infinite times with a reasonable learning rate, the estimated Q-value Q(s,a) converges to Q(s,a) [24].", "startOffset": 146, "endOffset": 150}, {"referenceID": 4, "context": "Markov games are widely adopted as a framework for multiagent reinforcement learning (MARL) [6] [10].", "startOffset": 92, "endOffset": 95}, {"referenceID": 8, "context": "Markov games are widely adopted as a framework for multiagent reinforcement learning (MARL) [6] [10].", "startOffset": 96, "endOffset": 100}, {"referenceID": 23, "context": "Therefore, Markov game is a richer framework which generalizes both of the MDP and the repeated game [25].", "startOffset": 101, "endOffset": 105}, {"referenceID": 0, "context": ",n is the set of action spaces for all agents, Ri : S\u00d7A \u2192 R is the reward function of agent i, T : S\u00d7A\u00d7S\u2192 [0,1] is the transition function.", "startOffset": 106, "endOffset": 111}, {"referenceID": 0, "context": "Denote the individual policy of agent i by \u03c0i = S\u00d7Ai \u2192 [0,1] and the joint policy of all agents by \u03c0 = (\u03c01, .", "startOffset": 55, "endOffset": 60}, {"referenceID": 6, "context": "Here, the equilibrium policy concept is usually transferred to finding the equilibrium solution for the one-shot game played in each joint state of a Markov game [8].", "startOffset": 162, "endOffset": 165}, {"referenceID": 8, "context": "Several equilibrium-based MARL algorithms in existing literatures such as NashQ [10] [26] and NegoQ [8] have been proposed, so that the joint state-action pair Q-value can be updated according to the equilibrium:", "startOffset": 80, "endOffset": 84}, {"referenceID": 24, "context": "Several equilibrium-based MARL algorithms in existing literatures such as NashQ [10] [26] and NegoQ [8] have been proposed, so that the joint state-action pair Q-value can be updated according to the equilibrium:", "startOffset": 85, "endOffset": 89}, {"referenceID": 6, "context": "Several equilibrium-based MARL algorithms in existing literatures such as NashQ [10] [26] and NegoQ [8] have been proposed, so that the joint state-action pair Q-value can be updated according to the equilibrium:", "startOffset": 100, "endOffset": 103}, {"referenceID": 8, "context": "MAS with Sparse Interactions The definition of Markov game reveals that all agents need to learn their policies in the full joint state-action space and they are coupled with each other all the time [10] [27].", "startOffset": 199, "endOffset": 203}, {"referenceID": 25, "context": "MAS with Sparse Interactions The definition of Markov game reveals that all agents need to learn their policies in the full joint state-action space and they are coupled with each other all the time [10] [27].", "startOffset": 204, "endOffset": 208}, {"referenceID": 3, "context": "The truth is that the learning agents in many practical multi-agent systems are loosely coupled with some limited interactions in some particular areas [5] [13] [27].", "startOffset": 152, "endOffset": 155}, {"referenceID": 11, "context": "The truth is that the learning agents in many practical multi-agent systems are loosely coupled with some limited interactions in some particular areas [5] [13] [27].", "startOffset": 156, "endOffset": 160}, {"referenceID": 25, "context": "The truth is that the learning agents in many practical multi-agent systems are loosely coupled with some limited interactions in some particular areas [5] [13] [27].", "startOffset": 161, "endOffset": 165}, {"referenceID": 11, "context": "Sparse-interaction based algorithms [13] [14] have recently found wide applications in MAS research.", "startOffset": 36, "endOffset": 40}, {"referenceID": 12, "context": "Sparse-interaction based algorithms [13] [14] have recently found wide applications in MAS research.", "startOffset": 41, "endOffset": 45}, {"referenceID": 23, "context": "An example of MAS with sparse interactions is the intelligent warehouse systems, where autonomous robots only consider other robots when they are close enough to each other [25] (see Fig.", "startOffset": 173, "endOffset": 177}, {"referenceID": 26, "context": "Earlier works such as the coordinated reinforcement learning [28] [29] and sparse cooperative Q-learning [30] used coordination graphs (CGs) to learn interdependencies between agents and decomposed the joint value function to local value functions.", "startOffset": 61, "endOffset": 65}, {"referenceID": 27, "context": "Earlier works such as the coordinated reinforcement learning [28] [29] and sparse cooperative Q-learning [30] used coordination graphs (CGs) to learn interdependencies between agents and decomposed the joint value function to local value functions.", "startOffset": 66, "endOffset": 70}, {"referenceID": 28, "context": "Earlier works such as the coordinated reinforcement learning [28] [29] and sparse cooperative Q-learning [30] used coordination graphs (CGs) to learn interdependencies between agents and decomposed the joint value function to local value functions.", "startOffset": 105, "endOffset": 109}, {"referenceID": 11, "context": "However, these algorithms cannot learn CGs online and only focus on finding specific states where coordination is necessary instead of learning for coordination [13].", "startOffset": 161, "endOffset": 165}, {"referenceID": 11, "context": "Melo and Veloso [13] extended the Q-learning to a two-layer algorithm and made it possible for agents to use an additional COORDINATE action to determine the state when the coordination was necessary.", "startOffset": 16, "endOffset": 20}, {"referenceID": 12, "context": "Hauwere and Vrancx proposed Coordinating Qlearning (CQ-learning) [14] and FCQ-learning [31] that helped agents learn from statistical information of rewards and Qvalues where an agent should take other agents\u2019 states into account.", "startOffset": 65, "endOffset": 69}, {"referenceID": 29, "context": "Hauwere and Vrancx proposed Coordinating Qlearning (CQ-learning) [14] and FCQ-learning [31] that helped agents learn from statistical information of rewards and Qvalues where an agent should take other agents\u2019 states into account.", "startOffset": 87, "endOffset": 91}, {"referenceID": 6, "context": "More recently, Hu et al [8] proposed an efficient equilibrium-based MARL method, called Negotiation-based Q-learning, by which agents can learn in a Markov game with unshared value functions and unshared joint state-actions.", "startOffset": 24, "endOffset": 27}, {"referenceID": 25, "context": "In later work, they applied this method for sparse interactions by knowledge transfer and game abstraction [27], and demonstrated the effectiveness of the equilibrium-based MARL in solving sparse-interaction problems.", "startOffset": 107, "endOffset": 111}, {"referenceID": 3, "context": "We apply this commonsense to our sparse-interaction method and decompose the learning process into two distinct sub-processes [5].", "startOffset": 126, "endOffset": 129}, {"referenceID": 6, "context": "We propose a negotiation mechanism similar to the work in [8] to find this equilibrium policy.", "startOffset": 58, "endOffset": 61}, {"referenceID": 6, "context": "If no non-strict EDSP is found, the agents search for a Meta equilibrium set (See Algorithm 3) instead, which is always nonempty [8].", "startOffset": 129, "endOffset": 132}, {"referenceID": 12, "context": "Different from previous work like CQ-learning and Learning of Coordination [14], our approach aims at finding the equilibrium solution for the one-shot game played in each \u201ccoordination state\u201d.", "startOffset": 75, "endOffset": 79}, {"referenceID": 6, "context": "As a result, we focus on two pure strategy profiles [8], i.", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "4: end if 5: end for 6: for each ~a \u2208 A do 7: if Ui(~a)\u2265 MinU i PNE then 8: Ji NS \u2190 J i NS \u222a{~a}; 9: end if 10: end for /* Broadcast Ji NS and corresponding utilities*/ 11: JNS \u2190 \u22c2n i=1 J i NS Meta Equilibrium) [8]: In an n-agent (n \u2265 2) normal-form game \u0393, a joint action ~a is called a Meta equilibrium from a metagame k1k2 .", "startOffset": 211, "endOffset": 214}, {"referenceID": 6, "context": "Hu et al [8] used a negotiation-based method to find Meta Equilibrium set and we simplified the method as shown in Algorithm 3.", "startOffset": 9, "endOffset": 12}, {"referenceID": 11, "context": "In most previous literatures [13] [14], the initial local Q-values of the newly expanded joint states are zeros.", "startOffset": 29, "endOffset": 33}, {"referenceID": 12, "context": "In most previous literatures [13] [14], the initial local Q-values of the newly expanded joint states are zeros.", "startOffset": 34, "endOffset": 38}, {"referenceID": 30, "context": "Recently, Vrancx et al proposed a transfer learning method [32] to initialize these Q-values with prior trained Q-value from the source task, which is reasonable in the real world.", "startOffset": 59, "endOffset": 63}, {"referenceID": 31, "context": "In this blank source task, joint action learners (JAL) [33] learn to coordinate with others disregarding the environmental information.", "startOffset": 55, "endOffset": 59}, {"referenceID": 30, "context": "Similar to [32], the joint state ~s in QCT i (~s,~a) is presented as the relative position (\u2206x,\u2206y), horizontally and vertically.", "startOffset": 11, "endOffset": 15}, {"referenceID": 30, "context": "If we initialize the local Q-value with the way used in [32] or just initialize them to zeros, the learning process would be much longer.", "startOffset": 56, "endOffset": 60}, {"referenceID": 12, "context": "EXPERIMENTS To test the presented NegoSI algorithm, several groups of simulated experiments are implemented and the results are compared with those of three other state-of-the-art MARL algorithms, namely, CQ-learning [14], NegoQ with value function transfer (NegoQ-VFT) [27] and independent learners with value function transfer (IL-VFT).", "startOffset": 217, "endOffset": 221}, {"referenceID": 25, "context": "EXPERIMENTS To test the presented NegoSI algorithm, several groups of simulated experiments are implemented and the results are compared with those of three other state-of-the-art MARL algorithms, namely, CQ-learning [14], NegoQ with value function transfer (NegoQ-VFT) [27] and independent learners with value function transfer (IL-VFT).", "startOffset": 270, "endOffset": 274}, {"referenceID": 11, "context": "Tests on grid world games The proposed NegoSI algorithm is evaluated in the grid world games presented by Melo and Veloso [13], which are shown in Fig.", "startOffset": 122, "endOffset": 126}, {"referenceID": 0, "context": "MARL has been widely used in such simulated domains as grid worlds [1], but few applications have been found for realistic problems comparing to single-agent reinforcement learning (RL) algorithms [34]-[38].", "startOffset": 67, "endOffset": 70}, {"referenceID": 32, "context": "MARL has been widely used in such simulated domains as grid worlds [1], but few applications have been found for realistic problems comparing to single-agent reinforcement learning (RL) algorithms [34]-[38].", "startOffset": 197, "endOffset": 201}, {"referenceID": 36, "context": "MARL has been widely used in such simulated domains as grid worlds [1], but few applications have been found for realistic problems comparing to single-agent reinforcement learning (RL) algorithms [34]-[38].", "startOffset": 202, "endOffset": 206}, {"referenceID": 37, "context": "Previously, single agent path planning methods have been successfully used in complex systems [39] [40], however, the intelligent warehouse employs a team of mobile robots to transport objects and single-agent path planning methods frequently cause collisions [41].", "startOffset": 94, "endOffset": 98}, {"referenceID": 38, "context": "Previously, single agent path planning methods have been successfully used in complex systems [39] [40], however, the intelligent warehouse employs a team of mobile robots to transport objects and single-agent path planning methods frequently cause collisions [41].", "startOffset": 99, "endOffset": 103}, {"referenceID": 39, "context": "Previously, single agent path planning methods have been successfully used in complex systems [39] [40], however, the intelligent warehouse employs a team of mobile robots to transport objects and single-agent path planning methods frequently cause collisions [41].", "startOffset": 260, "endOffset": 264}, {"referenceID": 40, "context": "In addition, multi-objective reinforcement learning (MORL) [42][44] will also be considered to further combine environment information and coordination knowledge for local learning.", "startOffset": 59, "endOffset": 63}, {"referenceID": 42, "context": "In addition, multi-objective reinforcement learning (MORL) [42][44] will also be considered to further combine environment information and coordination knowledge for local learning.", "startOffset": 63, "endOffset": 67}], "year": 2017, "abstractText": "Reinforcement learning has significant applications for multi-agent systems, especially in unknown dynamic environments. However, most multi-agent reinforcement learning (MARL) algorithms suffer from such problems as exponential computation complexity in the joint state-action space, which makes it difficult to scale up to realistic multi-agent problems. In this paper, a novel algorithm named negotiation-based MARL with sparse interactions (NegoSI) is presented. In contrast to traditional sparse-interaction based MARL algorithms, NegoSI adopts the equilibrium concept and makes it possible for agents to select the non-strict Equilibrium Dominating Strategy Profile (non-strict EDSP) or Meta equilibrium for their joint actions. The presented NegoSI algorithm consists of four parts: the equilibrium-based framework for sparse interactions, the negotiation for the equilibrium set, the minimum variance method for selecting one joint action and the knowledge transfer of local Q-values. In this integrated algorithm, three techniques, i.e., unshared value functions, equilibrium solutions and sparse interactions are adopted to achieve privacy protection, better coordination and lower computational complexity, respectively. To evaluate the performance of the presented NegoSI algorithm, two groups of experiments are carried out regarding three criteria: steps of each episode (SEE), rewards of each episode (REE) and average runtime (AR). The first group of experiments is conducted using six grid world games and shows fast convergence and high scalability of the presented algorithm. Then in the second group of experiments NegoSI is applied to an intelligent warehouse problem and simulated results demonstrate the effectiveness of the presented NegoSI algorithm compared with other state-of-the-art MARL algorithms.", "creator": "LaTeX with hyperref package"}}}