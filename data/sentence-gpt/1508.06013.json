{"id": "1508.06013", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Aug-2015", "title": "ERBlox: Combining Matching Dependencies with Machine Learning for Entity Resolution", "abstract": "Entity resolution (ER), an important and common data cleaning problem, is about detecting data duplicate representations for the same external entities, and merging them into single representations. Relatively recently, declarative rules called matching dependencies (MDs) have been proposed for specifying similarity conditions under which attribute values in database records are merged. In this work we show the process and the benefits of integrating three components of ER: (a) Classifiers for duplicate/non-duplicate record pairs built using machine learning (ML) techniques, (b) MDs for supporting both the blocking phase of ML and the merge itself; and (c) The use of the declarative language LogiQL -an extended form of Datalog supported by the LogicBlox platform- for data processing, and the specification and enforcement of MDs. We demonstrate how MDs can enhance the performance of the Datalog system.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 25 Aug 2015 02:35:58 GMT  (320kb,D)", "http://arxiv.org/abs/1508.06013v1", "To appear in Proc. SUM, 2015"]], "COMMENTS": "To appear in Proc. SUM, 2015", "reviews": [], "SUBJECTS": "cs.DB cs.AI cs.LG", "authors": ["zeinab bahmani", "leopoldo bertossi", "nikolaos vasiloglou"], "accepted": false, "id": "1508.06013"}, "pdf": {"name": "1508.06013.pdf", "metadata": {"source": "CRF", "title": "ERBlox: Combining Matching Dependencies with Machine Learning for Entity Resolution", "authors": ["Zeinab Bahmani", "Leopoldo Bertossi", "Nikolaos Vasiloglou"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Entity resolution (ER) is a common and difficult problem in data cleaning that has to do with handling unintended multiple representations in a database of the same external objects. Multiple representations lead to uncertainty in data and the problem of managing it. Cleaning the database reduces uncertainty. In more precise terms, ER is about the identification and fusion of database records (think of rows or tuples in tables) that represent the same real-world entity [8, 15]. As a consequence, ER usually goes through two main consecutive phases: (a) detecting duplicates, and (b) merging them into single representations.\nFor duplicate detection, one must first analyze multiple pairs of records, comparing the two records in them, and discriminating between: pairs of duplicate records and pairs of non-duplicate records. This classification problem is approached with machine learning (ML) methods, to learn from previously known or already made classifications (a training set for supervised learning), building a classification model (a classifier) for deciding about other record pairs [10, 15].\nIn principle, in ER every two records (forming a pair) have to be compared, and then classified. Most of the work on applying ML to ER work at the record level [22, 10, 11], and only some of the attributes, or their features, i.e. numerical values associated to them, may be involved in duplicate detection. The choice of relevant sets of attributes and features is application dependent.\nER may be a task of quadratic complexity since it requires comparing every two records. To reduce the large number two-record comparisons, blocking techniques are\nar X\niv :1\n50 8.\n06 01\n3v 1\n[ cs\n.D B\n] 2\n5 A\nug 2\nused [2, 19, 24]. Commonly, a single record attribute, or a combination of attributes, the so-called blocking key, is used to split the database records into blocks. Next, under the assumption that any two records in different blocks are unlikely to be duplicates, only every two records in a same block are compared for duplicate detection.\nAlthough blocking will discard many record pairs that are obvious non-duplicates, some true duplicate pairs might be missed (by putting them in different blocks), due to errors or typographical variations in attribute values. More interestingly, similarity between blocking keys alone may fail to capture the relationships that naturally hold in the data and could be used for blocking. Thus, entity blocking based only on blocking key similarities may cause low recall. This is a major drawback of traditional blocking techniques.\nIn this work we consider different and coexisting entities. For each of them, there is a collection of records. Records for different entities may be related via attributes in common or referential constraints. Blocking can be performed on each of the participating entities, and the way records for an entity are placed in blocks may influence the way the records for another entity are assigned to blocks. This is called \u201ccollective blocking\u201d. Semantic information, in addition to that provided by blocking keys for single entities, can be used to state relationships between different entities and their corresponding similarity criteria. So, blocking decision making forms a collective and intertwined process involving several entities. In the end, the records for each individual entity will be placed in blocks associated to that entity.\nExample 1. Consider two entities, Author and Paper. For each of them, there is a set of records (for all practical purposes, think of database tuples in a single table). For Author we have records of the form a = \u3008name, . . . , affiliation, . . . , paper title, . . .\u3009, with {name, affiliation} the blocking key; and for Paper, records of the form p = \u3008title, . . . , author name, . . .\u3009, with title the blocking key. We want to group Author and Paper records at the same time, in an entwined process. We block together two Author entities on the basis of the similarities of authors\u2019 names and affiliations.\nAssume that Author entities a1,a2 have similar names, but their affiliations are not. So, the two records would not be put in the same block. However, a1,a2 are authors of papers (in Paper records) p1,p2, resp., which have been put in the same block (of papers) on the basis of similarities of paper titles. In this case, additional semantic knowledge might specify that if two papers are in the same block, then corresponding Author records that have similar author names should be put in the same block too. Then, a1 and a2 would end up in the same block.\nIn this example, we are blocking Author and Paper entities, separately, but collectively and in interaction.\nCollective blocking is based on blocking keys and the enforcement of semantic information about the relational closeness of entities Author and Paper, which is captured by a set of matching dependencies (MDs). So, we propose \u201cMD-based collective blocking\u201d (more on MDs right below).\nAfter records are divided in blocks, the proper duplicate detection process starts, and is carried out by comparing every two records in a block, and classifying the pair as \u201cduplicates\u201d or \u201cnon-duplicates\u201d using the trained ML model at hand. In the end,\nrecords in duplicate pairs are considered to represent the same external entity, and have to be merged into a single representation, i.e. into a single record. This second phase is also application dependent. MDs were originally proposed to support this task.\nMatching dependencies are declarative logical rules that tell us under what conditions of similarity between attribute values, any two records must have certain attribute values merged, i.e. made identical [16, 17]. For example, the MD\nDeptB [dept ] \u2248 DeptB [dept ] \u2192 DeptB [city ] . = DeptB [city ] (1)\ntells us that for any two records for entity (or relation or table) DeptB that have similar values for attribute dept attribute, their values for attribute city should be matched, i.e. made the same.\nMDs as introduced in [17] do not specify how to merge values. In [6, 7], MDs were extended with matching functions (MFs). For a data domain, an MF specifies how to assign a value in common to two values. We adopt MDs with MFs in this work. In the end, the enforcement of MDs with MFs should produce a duplicate-free instance (cf. Section 2 for more details).\nMDs have to be specified in a declarative manner, and at some point enforced, by producing changes on the data. For this purpose, we use the LogicBlox platform, a data management system developed by the LogicBlox1 company, that is centered around its declarative language, LogiQL. LogiQL supports relational data management and, among several other features [1], an extended form of Datalog with stratified negation [9]. This language is expressive enough for the kind of MDs considered in this work.2\nIn this paper, we describe our ERBlox system. It is built on top of the LogicBlox platform, and implements entity resolution (ER) applying to LogiQL, ML techniques, and the specification and enforcement of MDs. More specifically, ERBlox has three main components: (a) MD-based collective blocking, (b) ML-based duplicate detection, and (c) MD-based merging. The sets of MDs are fixed and different for the first and last components. In both cases, the set of MDs are interaction-free [7], which results, for each entity, in the unique set of blocks, and eventually into a single, duplicate-free instance [7]. We use LogicQL to declaratively implement the two MD-based components of ERBlox.\nThe blocking phase uses MDs to specify the blocking strategy. They express conditions in terms of blocking key similarities and also relational closeness (the semantic knowledge) to assign two records to a same block (by making the block identifiers identical). Then, under MD-based collective blocking different records of possibly several related entities are simultaneously assigned to blocks through the enforcement of MDs (cf. Section 5 for details).\nOn the ML side, the problem is about detecting pairs of duplicate records. The ML algorithm is trained using record-pairs known to be duplicates or non-duplicates. We independently used three established classification algorithms: support vector machines (SVMs) [25], k-nearest neighbor (K-NN) [14], and non-parametric Bayes classifier (NBC) [4]. We used the Ismion3 implementations of them due to the in-house expertise\n1 www.logicblox.com 2 For arbitrary sets of MDs, we need higher expressive power [7], such as that provided by\nanswer set programming [3]. 3 http://www.ismion.com\nat LogicBlox. Since the emphasis of this work is on the use of LogiQL and MDs, we will refer only to our use of SVMs.\nWe experimented with our ERBlox system using as dataset a snapshot of Microsoft Academic Search (MAS)4 (as of January 2013) including 250K authors and 2.5M papers. It contains a training set. The experimental results show that our system improves ER accuracy over traditional blocking techniques [18], which we will call standard blocking, where just blocking-key similarities are used. Actually, MD-based collective blocking leads to higher precision and recall on the given datasets.\nThis paper is structured as follows. Section 2 introduces background on matching dependencies and their semantics, and SVMs. A general overview of the ERBlox system is presented in Section 3. The specific components of ERBlox are discussed in Sections 4, 5, and 6. Experimental results are shown in Section 7. Section 8 presents conclusions."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 Matching dependencies", "text": "We consider an application-dependent relational schemaR, with a data domain U . For an attribute A, DomA is its finite domain. We assume predicates do not share attributes, but different attributes may share a domain. An instanceD forR is a finite set of ground atoms of the form R(c1, . . . , cn), with R \u2208 R, ci \u2208 U .\nWe assume that each entity is represented by a relational predicate, and its tuples or rows in its extension correspond to records for the entity. As in [7], we assume records have unique, fixed, global identifiers, rids, which are positive integers. This allows us to trace changes of attribute values in records. Record ids are placed in an extra attribute for R \u2208 R that acts as a key. Then, records take the form R(r, r\u0304), with r the rid, and r\u0304 = (c1, . . . , cn). Sometimes we leave rids implicit, and sometimes we use them to denote whole records: if r is a record identifier in instance D, r\u0304 denotes the record in D identified by r. Similarly, if A is a sublist of the attributes of predicate R, then r[A] denotes the restriction of r\u0304 to A.\nMDs are formulas of the form: R1[X\u03041] \u2248 R2[X\u03042] \u2192 R1[Y\u03041] . = R2[Y\u03042] [16, 17]. Here, R1, R2 \u2208 R (and may be the same); and X\u03041, X\u03042 are lists of attribute names of the same length that are pairwise comparable, that is, Xi1 and X i 2, and also Y\u03041, Y\u03042, share the same domain.5 The MD says that, for every pair of tuples (one in relation R1, the other in relation R2) where the LHS is true, the attribute values in them on the RHS have to be made identical. Symbol \u2248 denotes generic, reflexive, symmetric, and application/domain dependent similarity relations on shared attribute domains.\nA dynamic, chase-based semantics for MDs with matching functions (MFs) was introduced in [7]. Given an initial instance D, the set \u03a3 of MDs is iteratively enforced until they cannot be be applied any further, at which point a resolved instance has been produced. In order to enforce (the RHSs of) MDs, there are binary matching functions (MFs) mA : DomA \u00d7 DomA \u2192 DomA; and mA(a, a\u2032) is used to replace two values a, a\u2032 \u2208 DomA that have to be made identical. MFs are idempotent, commutative, and\n4 http://academic.research.microsoft.com. For comparison, we also tested our system with data from DBLP and Cora.\n5 A more precise notation for the MD would be: \u2200x11 \u00b7 \u00b7 \u00b7 \u2200ym2 ( \u2227 j R1[x j 1] \u2248j R2[x\nj 2] \u2212\u2192\u2227\nk R1[y k 1 ] . = R2[y k 2 ]).\nassociative, and then induce a partial-order structure \u3008DomA, A\u3009, with: a A a\u2032 :\u21d4 mA(a, a\u2032) = a\u2032 [6, 5]. It always holds: a, a\u2032 A mA(a, a\u2032). In this work, MFs are treated as built-in relations.\nThere may be several resolved instances for D and \u03a3. However, when (a) MFs are similarity-preserving (i.e., a \u2248 a\u2032 implies a \u2248 mA(a\u2032, a\u2032\u2032)); or (b) \u03a3 is interaction-free (i.e., each attribute may appear in either the RHS or LHS of MDs in \u03a3), there is a unique resolved instance that is computable in polynomial time in |D| [7]."}, {"heading": "2.2 Support vector machines", "text": "The SVMs technique [25] is a form of kernel-based learning. SVMs can be used for classifying vectors in an inner-product vector space V over R. Vectors are classified in two classes, with a label in {0, 1}. The algorithm learns from a training set, say {(e1, f(e1)), (e2, f(e2)), (e3, f(e3)), . . . , (en, f(en))}. Here, ei \u2208 V , and for the feature (function) f : f(ei) \u2208 {0, 1}.\nSVMs find an optimal hyperplane,H, in V that separates the two classes where the training vectors are classified. Hyperplane H has an equation of the form w \u2022 x + b, where \u2022 denotes the inner product, x is a vector variable, w is a weight vector of real values, and b is a real number. Now, a new vector e in V can be classified as positive or negative depending on the side of H it lies. This is determined by computing h(e) := sign(w \u2022 e + b). If h(e) > 0, e belongs to class 1; otherwise, to class 0.\nIt is possible to compute real numbers \u03b11, . . . , \u03b1n, such that the classifier h can be computed through: h(e) = sign( \u2211 i \u03b1i \u00b7 f(ei) \u00b7 ei \u2022 e + b) (cf. Figure 3).\n3 Overview of ERBlox A high-level description of the components of ERBlox is given in Figure 1. It shows the workflow supported by ERBlox when doing ER. ERBlox\u2019s three main components are: (1) MD-based collective blocking (path 1,3,5, {6,8}), (2) ML-based record duplicate detection (the whole initial workflow up to task 13, inclusive), and (3) MD-based merging (path 14,15). In the figure, all the boxes in light grey are supported by LogiQL. As just done, in the rest of this section, numbers in boldface refer to the edges in this figure.\nThe initial input data is stored in structured text files. (We assume these data are already standardized and free of misspellings, etc., but duplicates may be present.) Our general LogiQL program that\nsupports the whole workflow contains some rules for importing data from the files into the extensions of relational predicates (think of tables, this is edge 1). This results in a relational database instance T containing the training data (edge 2), and the instance D on which ER will be performed (edge 3).\nThe next main task is blocking, which requires similarity computation of pairs of records in D (edge 5). For record pairs \u3008r1, r2\u3009 in T , similarities have to be computed as well (edge 4). Similarity computation is based on similarity functions, Sf i : DomAi \u00d7 DomAi \u2192 [0, 1], each of which assigns a numerical value, called similarity weight, to the comparisons of values for a\nrecord attributeAi (from a pre-chosen subset of attributes) (cf. Figure 2). A weight vector w(r1, r2) = \u3008\u00b7 \u00b7 \u00b7 ,Sf i(r1[Ai], r2[Ai]), \u00b7 \u00b7 \u00b7 \u3009 is formed by similarity weights (edge 7). For more details on similarity computation see Section 4.\nSince some pairs in T are considered to be duplicates and others non-duplicates, the result of this process leads to a \u201csimilarity-enhanced\u201d database Ts of tuples of the form \u3008r1, r2, w(r1, r2), L\u3009, with label L \u2208 {0, 1} indicating if the two records are duplicates (L = 1) or not (L = 0). The labels are consistent with the corresponding weight vectors. The classifier is trained using Ts, leading to a classification model (edges 9,10).\nFor records in D, similarity measures are needed for blocking, to decide if two records r1, r2 go to the same block. Initially, every record has its rid assigned as block (number). To assign two records to the same block, we use matching dependencies that specify and enforce (through their RHSs) that their blocks have to be identical. This happens when certain similarities between pairs of attribute values appearing in the LHSs of the MDs hold. For this reason, similarity computation is also needed before blocking (workflow 5,6,8). This similarity computation process is similar to the one for T . However, in the case of D, this does not lead directly to the same kind of weight vector computation. Instead, the computation of similarity measures is only for the similarity predicates appearing in the LHSs of the blocking-MDs. (So, as the evaluation of the LHS in (1) requires the computation of similarities for dept-string values.)\nNotice that these blocking-MDs may capture semantic knowledge, so they could involve in their LHSs similarities of attribute values in records for different kinds of entities. For example, in relation to Example 1, there could be similarity comparisons involving attributes for entities Author and Paper, e.g.\nAuthor(x1, y1, bl1) \u2227 Paper(y1, z1, bl3) \u2227Author(x2, y2, bl2) \u2227 Paper(y2, z2, bl4) \u2227 x1 \u22481 x2 \u2227 z1 \u22482 z2 \u2192 bl1 . = bl2, (2) expressing that when the similarities on the LHS hold, the blocks bl1, bl2 have to be made identical.6 The similarity comparison atoms on the LHS are considered to be true when the similarity values are above predefined thresholds (edges 5,8).7\n6 These MDs are more general than those introduced in Section 2.1: they may contain regular database atoms, which are used to give context to the similarity atoms in the same antecedent. 7 At this point, since all we want is to do blocking, and not yet decisions about duplicates, we could, in comparison with what is done with pairs in T , compute less similarity measures and and even with low thresholds.\nThis is the MD-based collective blocking stage that results in database D enhanced with information about the blocks to which the records are assigned. Pairs of records with the same block form candidate duplicate record pairs, and any two records with different blocks are simply not tested as possible duplicates (of each other).\nAfter the records have been assigned to blocks, pairs of records \u3008r1, r2\u3009 in the same block are considered for the duplicate test. As this point we proceed as we did for T : the similarity vectors w(r1, r2) have to be computed (edges 11,12).8 Next, tuples \u3008r1, r2, w(r1, r2)\u3009 are used as input for the trained classification algorithm (edge 12).\nThe result of the trained ML-based classifier, in this case obtained through SVMs as a separation hyperplane H, is a set M of record pairs \u3008r1, r2, 1\u3009 that come from the same block and are considered to be duplicates (edge 13).9 The records in these pairs will be merged on the basis of an ad hoc set of MDs (edge 15), different from those used in edges 6,8.\nInformally, the merge-MDs are of the form: r1 \u2248 r2 \u2192 r1 . = r2, where the antecedent is true when \u3008r1, r2, 1\u3009 is an output of the classifier. The RHS is a shorthand for: r1[A1] . = r2[A1]\u2227 \u00b7 \u00b7 \u00b7 \u2227 r1[Am] . = r2[Am], where m is the total number of record attributes. Merge at the attribute level uses the matching functions mAi . We point out that MD-based merging takes care of transitive cases provided by the classifier, e.g. if it returns \u3008r1, r2, 1\u3009, \u3008r2, r3, 1\u3009, but not \u3008r1, r3, 1\u3009, we still merge r1, r3 (even when r1 \u2248 r3 does not hold). Actually, we do this by by merging all the records r1, r2, r3 into the same record. Our system is capable of recognizing this situation and solving it as expected. This relies on the way we store and manage -via our LogiQL program- the positive cases obtained from the classifier (details can be found in Section 6). In essence, this makes our set of merging-MDs interaction-free, and leads to a unique resolved instance [7].\nThe following sections provide more details on ERBlox and our approach to ER."}, {"heading": "4 Initial Data and Similarity Computation", "text": "We describe now some aspects of the MAS dataset, highlighting the input for- and output of each component of the ERBlox system. The data is represented and provided as follows. The Author relation contains authors names and their affiliations. The Paper relation contains paper titles, years, conference IDs, journal IDs, and keywords. The PaperAuthor relation contains papers IDs, authors IDs, authors names, and their affiliations. The Journal and Conference relations contain short names, full names, and home pages of journals and conferences, respectively. By using ERBlox on this dataset, we determine which papers in MAS data are written by a given author. This is clear case of\n8 Similarity computations are kept in appropriate program predicates. So similarity values computed before blocking can be reused at this stage, or whenever needed. 9 The classifier also returns pairs or records that come from the same block, but are not considered to be duplicate. The set thereof in not interesting, at least as a workflow component.\nER since there are many authors who publish under several variations of their names. Also the same paper may appear under slightly different titles, etc.10\nFrom the MAS dataset, which contains the data in structured files, extensions for intentional, relational predicates are computed by LogiQL-rules of the general program, e.g.\nfile in(x1, x2, x3)\u2192 string(x1), string(x2), string(x3). (3) lang : physical : filePath[\u2018 file in] = \u201dauthor .csv\u201d. (4)\n+author(id1, x2, x3)\u2190 file in(x1, x2, x3), string : int64:convert [x1] = id1. (5)\nHere, (3) is a predicate schema declaration (metadata uses \u201c\u2192\u201d), in this case of the \u201c file in\u201d predicate with three string-valued attributes,11 which is used to store the contents extracted from the source file, whose path is specified by (4). Derivation rules, such as (5), use the usual \u201c\u2190\u201d. In this case, it defines the author predicate, and the \u201c+\u201d in the rule head inserts the data into the predicate extension. The first attribute is made an identifier [1]. Figure 4 illustrates a small part of the dataset obtained by importing data into the relational predicates. (There may be missing attributes values.)\nAs described above, in ERBlox, similarity computation generates similarity weights, which are used to: (a) compute the weight vectors for the training data T and the data in D under classification; and (b) do the blocking, where similarity weights are compared with predefined thresholds for the similarity conditions in the LHSs of blocking-MDs.12\nWe used three well-known similarity functions [13], depending on the attribute domains. \u201cTF-IDF cosine similarity\u201d [23] used for computing similarities for text-valued attributes, whose values are string vectors. It assigns low weights to frequent strings and high weights to rare strings. It was used for attribute values that contain frequent strings, such as affiliation. For attributes with short string values, such as author name, we applied \u201cJaro-Winkler similarity\u201d [26]. Finally, for numerical attributes, such as publication year, we used \u201cLevenshtein distance\u201d [21], which computes similarity of\n10 For our experiments, we independently used two other datasets: DBLP and Cora Citation. 11 In LogiQL, each predicate has to be declared, unless it can be inferred from the rest of the\nprogram. 12 As described at the end of Section 3, these similarity computations are not used with the MDs\nthat support the final merging process (cf. Section 6).\ntwo numbers on the basis of the minimum number of operations required to transform one into the other.\nSimilarity computation for ERBlox is supported by LogiQL-rules that define similarity functions. In particular, similarity computations are kept in extensions of program predicates. For example, if the similarity weight of values a1, a2 for attribute Title is above the threshold, a tuple TitleSim(a1, a2) is created by the program."}, {"heading": "5 MD-Based Collective Blocking and Duplicate Detection", "text": "Since every record has an identifier, rid, initially each record uses its rid as its block number, in an extra attribute Bl#. In this way, we create the initial blocking instance from the initial instance D, also denoted with D. Now, blocking strategies are captured by means of (blocking) MDs of the form:\nRi(X\u03041,Bl1) \u2227Ri(X\u03042,Bl2) \u2227 \u03c8(X\u03043) \u2192 Bl1 . = Bl2. (6)\nHere Bl1,Bl2 are variables for block numbers, and Ri is a database (record) predicate. The lists of variables X\u03041, X\u03042 stand for all the attributes in Ri, but Bl#. Formula \u03c8 is a conjunction of relational atoms and comparison atoms via similarity predicates; but it does not contain similarity comparisons of blocking numbers, such as Bl3\u2248 Bl4.13 The variables in the list X\u03043 appear in Ri or in another database predicate or in a similarity atom. It holds that (X\u03041 \u222a X\u03042) \u2229 X\u03043 6= \u2205. For an example, see (2), where Ri is Author.\nIn order to enforce these MDs on two records, we use a binary matching function m\nBl# , to make two block numbers identical: m Bl# (i, j) := i if j \u2264 i. More generally, for the application-dependent set, \u03a3Bl , of blocking-MDs we adopt the chase-based semantics for entity resolution [7]. Since this set of MDs is interaction-free, its enforcement results in a single instance DBl , where now records may share block numbers, in which case they belong to the the same block. Every record is assigned to a single block.\nExample 2. These are some of the blocking-MDs used for the MAS dataset: Paper(pid1, x1, y1, z1, w1, v1, bl1) \u2227 Paper(pid2, x2, y2, z2, w2, v2, bl2) \u2227 (7)\nx1 \u2248Title x2 \u2227 y1 = y2 \u2227 z1 = z2 \u2192 bl1 . = bl2.\nAuthor(aid1, x1, y1, bl1) \u2227 Author(aid2, x2, y2, bl2) \u2227 (8) x1 \u2248Name x2 \u2227 y1 \u2248Aff y2 \u2192 bl1 . = bl2. Paper(pid1, x1, y1, z1, w1, v1, bl1) \u2227 Paper(pid2, x2, y2, z2, w2, v2, bl2) \u2227 (9)\nPaperAuthor(pid1, aid1, x \u2032 1, y \u2032 1) \u2227 PaperAuthor(pid2, aid2, x \u2032 2, y \u2032 2) \u2227\nAuthor(aid1, x \u2032 1, y \u2032 1, bl3) \u2227 Author(aid2, x \u2032 2, y \u2032 2, bl3) \u2227 x1 \u2248Title x2 \u2192 bl1 . = bl2.\nAuthor(aid1, x1, y1, bl1) \u2227 Author(aid2, x2, y2, bl2) \u2227 x1 \u2248Name x2 \u2227 (10) PaperAuthor(pid1, aid1, x1, y1) \u2227 PaperAuthor(pid2, aid2, x2, y2) \u2227\nPaper(pid1, x \u2032 1, y \u2032 1, z \u2032 1, w \u2032 1, v \u2032 1, bl3) \u2227 Paper(pid2, x \u2032 2, y \u2032 2, z \u2032 2, w \u2032 2, v \u2032 2, bl3)\u2192 bl1 . = bl2.\nInformally, (7) tells us that, for every two Paper entities p1,p2 for which the values for attribute Title are similar and with same publication year, conference ID, the values for attribute Bl# must be made the same. By (8), whenever there are similar values for name and affiliation in Author, the corresponding authors should be in the same block. Furthermore, (9) and (10) collectively block Paper and Author entities. For instance, (9)\n13 Actually, this natural condition makes the set of blocking-MDs interaction-free, i.e. for every two blocking-MDs m1,m2, the set of attributes on the RHS of m1 and the set of attributes on the LHS of m2 on which there are similarity predicates, are disjoint [7].\nstates that if two authors are in the same block, their papers p1, p2 having similar titles must be in the same block. Notice that if papers p1 and p2 have similar titles, but they do not have same publication year or conference ID, we cannot block them together using (7) alone.\nWe now show how these MDs are represented in LogiQL, and how we use LogiQL programs for declarative specification of MD-based collective blocking.14 In LogiQL, an MD takes the form: Ri[X\u03041]=Bl2, Ri[X\u03042]=Bl2 \u2190\u2212 Ri[X\u03041] = Bl1, Ri[X\u03042] = Bl2, \u03c8(X\u03043), Bl1 < Bl2, (11) subject to the same conditions as in (6). An atom Ri[X\u0304]=Bl states that predicate Ri is functional on X\u0304 [1]. It means each record in Ri can have only one block number Bl#.\nGiven an initial instance D, a LogiQL program PB(D) that specifies MD-based collective blocking contains the following (kind of) rules:\n1. For every atom R(rid , x\u0304, bl) \u2208 D, the fact R[rid , x\u0304] = bl . (Initially, bl := rid .)\n2. For every attribute A of Ri, facts of the form A-Sim(a1, a2), with a1, a2 \u2208 DomA, the finite attribute domain. They are obtained by similarity computation.\n3. The blocking-MDs as in (11).\n4. Rules to represent the consecutive versions of entities during MD-enforcement: R-OldVersion(r1, x\u03041, bl1) \u2190 R[r1, x\u03041] = bl1, R[r1, x\u03041] = bl2, bl1 < bl2. For each rid, r, there could be several atoms of the form R[r, x\u0304]=bl , corresponding to the evolution of the record identified by r due to MD-enforcement. The rule specifies that versions of records with lower block numbers are old.\n5. Rules that collect the latest versions of records. They are used to form blocks: R-MDBlock [r1, x\u03041] = bl1 \u2190 R[r1, x\u03041] = bl1, ! R-OldVersion(r1, x\u03041, bl1).\nIn LogiQL, \u201c!\u201d, as in the body above, is used for negation [1]. The rule collects Rrecords that are not old versions.\nPrograms PB(D) as above are stratified (there is no recursion involving negation). Then, as expected in relation to the blocking-MDs, they have a single model, which can be used to read the final block number for each record.\nExample 3. (ex. 2 cont.) Considering only MDs (7) and (9), the portion of PB(D) for blocking Paper entities has the following rules:\n2. Facts such as: TitleSim(Illness entities in West Africa, Illness entities in Africa). TitleSim(DLR Simulation Environment m3 ,DLR Simulation Environment).\n3. Paper [pid1, x1, y1, z1, w1, v1] = bl2,Paper [pid2, x2, y2, z2, w2, v2] = bl2 \u2190 Paper [pid1, x1, y1, z1, w1, v1] = bl1,Paper [pid2, x2, y2, z2, w2, v2] = bl2,\nTitleSim(x1, x2), y1 = y2, z1 = z2, bl1 < bl2.\nPaper [pid1, x1, y1, z1, w1, v1] = bl2,Paper [pid2, x2, y2, z2, w2, v2] = bl2 \u2190 Paper [pid1, x1, y1, z1, w1, v1] = bl1,Paper [pid2, x2, y2, z2, w2, v2] = bl2,TitleSim(x1, x2),\nPaperAuthor(pid1, aid1, x \u2032 1, y \u2032 1),PaperAuthor(pid2, aid2, x \u2032 2, y \u2032 2),\nAuthor [aid1, x \u2032 1, y \u2032 1] = bl3,Author [aid2, x \u2032 2, y \u2032 2] = bl3, bl1 < bl2.\n14 Notice that since we have interaction-free sets of blocking-MDs, stratified Datalog programs are expressive enough to express and enforce them [3]. LogiQL supports stratified Datalog.\n4. PaperOldVersion(pid1, x1, y1, z1, w1, v1, bl1)\u2190Paper [pid1, x1, y1, z1, w1, v1] = bl1, Paper [pid1, x1, y1, z1, w1, v1] = bl2, bl1 < bl2. 5. PaperMDBlock [pid, x\u03041] = bl1 \u2190 Paper [pid1, x1, y1, z1, w1, v1] = bl1, PaperOldVersion(pid1, x1, y1, z1, w1, v1, bl1).\nRestricting the model of the program to the relevant attributes of predicate PaperMDBlock returns: {{123, 205}, {195, 769}}, i.e. the papers with pids 123 and 205 are blocked together; similarly for those with pids 195 and 769.\nAs described above, the input to the trained classifier is a set of tuples of the form \u3008r1, r2, w(r1, r2)\u3009, with w(r1, r2) the computed weight vector for records (with ids) r1, r2 in a same block.15\nExample 4. (ex. 3 cont.) Consider the blocks for entity Paper. If the \u201cjournal ID\u201d values are null in both records, but not the \u201cconference ID\u201d values, \u201cjournal ID\u201d is not considered for a feature. Similarly, when the conference ID values are null. However, the values for \u201cjournal ID\u201d and \u201cconference ID\u201d are replaced by \u201cjournal full name\u201d and \u201cconference full name\u201d values, found in Conference and Journal records, resp. In this case then, attributes Title, Year, ConfFullName or JourFullName and Keyword are used for corresponding feature for weight vector computation.\nConsidering the previous Paper records, the input to the classifier consists of: \u3008123, 205, w(123, 205)\u3009, withw(123, 205) = [0.8, 1.0, 1.0, 0.7], and \u3008195, 769, w(195, 769)\u3009, with w(195, 769) = [0.93, 1.0, 1.0, 0.5] (actually the contents of the two square brackets only).\nSeveral ML techniques are accessible from LogicBlox platform through the BloxMLPack library, that provides a generic Datalog interface. Then, ERBlox can call an MLbased record duplicate detection component through the general LogiQL program. In this way, the SVMs package is invoked by ERBlox.\nThe output is a set of tuples of the form \u3008r1, r2, 1\u3009 or \u3008r1, r2, 0\u3009, where r1, r2 are ids for records of entity (table)R. In the former case, a tupleR-Duplicate(r1, r2) is created (as defined by the LogicQL program). In the previous example, the SVMs method return \u3008[0.8, 1.0, 1.0, 0.7], 1\u3009 and \u3008[0.93, 1.0, 1.0, 0.5], 1\u3009, then PaperDuplicate(123, 205) and PaperDuplicate(195, 769) are created."}, {"heading": "6 MD-Based Merging", "text": "When EntityDuplicate(r1, r2) is created, the corresponding full records r\u03041, r\u03042 have to be merged via record-level merge-MDs of the form R[r1] \u2248 R[r2] \u2212\u2192 R[r\u03041] . = R[r\u03042], where R[r1] \u2248 R[r2] is true when R-Duplicate(r1, r2) has been created according to the output of the SVMs classifier. The RHS means that the two records are merged into a new full record r\u0304, with r\u0304[Ai] := mAi(r\u03041[Ai], r\u03042[Ai]) [7]. Example 5. (ex. 4 cont.) We merge duplicate Paper entities enforcing the MD: Paper [pid1] \u2248 Paper [pid2] \u2212\u2192 Paper [Title,Year ,CID ,Keyword ] . = Paper [Title,Year ,CID , Keyword ].\n15 The features considered in a weight vector computation depend on whether they have a strong discrimination power, i.e. do not contain missing values.\nThe portion, PM , of the general LogiQL program that represents MD-based merging contains rules as in 1.-4. below: 1. The atoms of the form R-Duplicate mentioned above, and those representing the matching functions (MFs) m\nA .\n2. For an MD R[r1] \u2248 R[r2] \u2212\u2192 R[r\u03041] . = R[r\u03042], the rule:\nR[r1, x\u03043] = bl , R[r2, x\u03043] = bl \u2190\u2212 R-Duplicate(r1, r2), R[r1, x\u03041] = bl , R[r2, x\u03042] = bl , m(x\u03041, x\u03042) = x\u03043,\nwhich creates two records (one of them can be purged afterwards) with different ids but all the other attribute values the same, and computed componentwise according to the MFs for m . Here, x\u03041, x\u03042, x\u03043 stand each for all attributes of relation R, except for the id and the block number (represented by bl ). (Block numbers play no role in merging.)\n3. As for program PB(D) given in Section 5, rules specify the old versions of a record:\nR-OldVersion(r1, x\u03041) \u2190 R[r1, x\u03041] = bl , R[r1, x\u03042] = bl , x\u03041 \u227a x\u03042.\nHere, x\u03041 stands for all attributes other than the id and the block number; and on the RHS x\u03041 \u227a x\u03042 means componentwise comparison of values according to the partial orders defined by the MFs. 4. Finally, rules to collect the latest version of each record, building the final resolved instance: R-ER(r1, x\u03041) \u2190 R[r1, x\u03041] = bl , ! R-OldVersion(r1, x\u03041).\nNotice that the derived tables R-Duplicate that appear in the LHSs of the MDs (or in the bodies of the corresponding rules) are all computed before (and kept fixed during) the enforcement of the merge-MDs. In particular, a duplicate relationship between any two records is not lost. This has the effect of making the set of merging-MDs interaction-free, which results in a unique resolved instance."}, {"heading": "7 Experimental Evaluation", "text": "We now show that our approach to ER can improve accuracy in comparison with standard blocking. In addition to the MAS, we used datasets from DBLP and Cora Citation.\nIn order to emphasize the importance of semantic knowledge in blocking, we consider standard blocking and two different sets of MDs, (1) and (2), for MDbased collective blocking. Under (1), we define blocking-MDs for all the blocking keys used for standard blocking, but under (2) we have MDs for only some of the used blocking keys. In both cases, in addition to properly collective blocking MDs.\nWe use three measures for the comparisons of blocking techniques. One is reduction ratio, which is the the ratio (minus 1) of the number of candidate record-pairs over the initial number of records. The higher\nthis value, the less candidate record-pairs are being generated, but the quality of the generated candidate record pairs is not taken into account. We also use recall and precision measures. The former is the number of true duplicate candidate record-pairs divided by the number of true duplicate pairs, and precision is the number of true candidate duplicate record-pairs divided by the total number of candidate pairs [12].\nFigures 5, 6 and 7 show the comparative performance of ERBlox. They show that standard blocking has higher reduction ratio than MD-based collective blocking version (1). This means that less candidate record-pairs are being generated by standard blocking. However, the precision and recall of MD-based blocking version (1) are higher than standard blocking, meaning that MD-based blocking version (1) can lead to improved ER results at the cost of larger blocks, and thus more candidate record pairs that need to be compared.\nIn blocking, this is a common tradeoff that needs to be considered. On the one hand, having a large number of smaller blocks will result in fewer candidate record-pairs that will be generated, probably increasing the number of true duplicate record-pairs that are missed. On the other hand, blocking techniques that result in larger blocks generate a higher number of candidate record-pairs that will likely cover more true duplicate pairs, at the cost of having to compare\nmore candidate pairs [12]. The experiments are all done before MD-based merging.\nInterestingly, MD-based blocking version (2) has higher reduction ratio, recall, and precision than standard blocking. This emphasizes the importance of MDs supporting collective blocking, and shows that blocking based on string similarity alone fails to capture the relationships that naturally hold in the data.\nAs expected, the experiments show that different sets of MDs for MD-based\ncollective blocking have different impact on reduction ratio, so as standard blocking depends on the choice of blocking keys. However, the quality of MD-based collective blocking, in its two versions, dominates standard blocking for the three datasets."}, {"heading": "8 Conclusions", "text": "We have shown that matching dependencies, a new class of data quality/cleaning semantic constraints in databases, can be profitably integrated with traditional ML-methods, in our case for entity resolution. They play a role not only in the intended goal of merging duplicate representations, but also in the record blocking process that precedes the\nlearning task. At that stage they allow to declaratively capture semantic information that can be used to enrich the blocking activity. MDs declaration and enforcement, data processing in general, and machine learning can all be integrated using the LogiQL language. Acknowledgments: Part of this research was funded by an NSERC Discovery grant and the NSERC Strategic Network on Business Intelligence (BIN). Z. Bahmani and L. Bertossi are very much grateful for the support from LogicBlox during their internship and sabbatical visit."}], "references": [{"title": "Design and Implementation of the LogicBlox System", "author": ["M. Aref", "B. ten Cate", "T.J. Green", "B. Kimelfeld", "D. Olteanu", "E. Pasalic", "T.L. Veldhuizen", "G. Washburn"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "A Comparison of Fast Blocking Methods for Record Linkage", "author": ["R. Baxter", "P. Christen", "T. Churches"], "venue": "Proc. ACM SIGKDD Workshop on Data Cleaning, Record Linkage, and Object Identification", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Declarative Entity Resolution via Matching Dependencies and Answer Set Programs", "author": ["Z. Bahmani", "L. Bertossi", "S. Kolahi", "L. Lakshmanan"], "venue": "Proc. KR", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Generalized Discriminant Analysis using a Kernel Approach", "author": ["Baudat G", "F. Anouar"], "venue": "Neural Computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Swoosh: A Generic Approach to Entity Resolution", "author": ["O. Benjelloun", "H. Garcia-Molina", "D. Menestrina", "Q. Su", "S. EuijongWhang", "J. Widom"], "venue": "VLDB Journal,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Data Cleaning and Query Answering with Matching Dependencies and Matching Functions", "author": ["L. Bertossi", "S. Kolahi", "L. Lakshmanan"], "venue": "Proc. ICDT", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Data Cleaning and Query Answering with Matching Dependencies and Matching Functions", "author": ["L. Bertossi", "S. Kolahi", "L. Lakshmanan"], "venue": "Th. Comp. Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Quality and Complexity Measures for Data Linkage and Deduplication", "author": ["P. Christen", "K. Goiser"], "venue": "In Quality Measures in Data Mining, ser. Studies in Computational Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Automatic Record Linkage using Seeded Nearest Neighbour and Support Vector Machine Classification", "author": ["P. Christen"], "venue": "Proc. SIGKDD", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "A Survey of Indexing Techniques for Scalable Record Linkage and Deduplication", "author": ["P. Christen"], "venue": "IEEE Transactions in Knowledge and Data Engineering,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "A Comparison of String Metrics for Matching Names and Records", "author": ["W. Cohen", "P. Ravikumar", "S. Fienberg"], "venue": "Proc. Workshop on Data Cleaning and Object Consolidation", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Nearest Neighbor Pattern Classification", "author": ["T.M. Cover", "P.E. Hart"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1967}, {"title": "Duplicate Record Detection: a Survey", "author": ["A. Elmagarmid", "P. Ipeirotis", "V. Verykios"], "venue": "IEEE Transactions in Knowledge and Data Engineering,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Dependencies Revisited for Improving Data Quality", "author": ["W. Fan"], "venue": "Proc. PODS", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "A Theory for Record Linkage", "author": ["I.P. Fellegi", "A.B. Sunter"], "venue": "Journal of the American Statistical Society,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1969}, {"title": "Data Quality and Record Linkage Techniques", "author": ["T.N. Herzog", "F.J. Scheuren", "W.E. Winkler"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "UNIMATCH: A Record Linkage System: User\u2019s Manual", "author": ["M.A. Jaro"], "venue": "Technical Report, U.S. Bureau of the Census,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1976}, {"title": "A Guided Tour to Approximate String Matching", "author": ["G. Navarro"], "venue": "ACM Computing Surveys,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Large-scale Collective Entity Matching", "author": ["V. Rastogi", "N.N. Dalvi", "M.N. Garofalakis"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Term-weighting Approaches in Automatic Text Retrieval", "author": ["G. Salton", "C. Buckley"], "venue": "Information Processing and Management,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1988}, {"title": "Entity Resolution with Iterative Blocking", "author": ["S. Euijong Whang", "D. Menestrina", "G. Koutrika", "M. Theobald", "H. Garcia-Molina"], "venue": "Proc. SIGMOD", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "The State of Record Linkage and Current Research Problems", "author": ["W.E. Winkler"], "venue": "Technical Report, U.S. Census Bureau,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}], "referenceMentions": [{"referenceID": 12, "context": "In more precise terms, ER is about the identification and fusion of database records (think of rows or tuples in tables) that represent the same real-world entity [8, 15].", "startOffset": 163, "endOffset": 170}, {"referenceID": 7, "context": "This classification problem is approached with machine learning (ML) methods, to learn from previously known or already made classifications (a training set for supervised learning), building a classification model (a classifier) for deciding about other record pairs [10, 15].", "startOffset": 268, "endOffset": 276}, {"referenceID": 12, "context": "This classification problem is approached with machine learning (ML) methods, to learn from previously known or already made classifications (a training set for supervised learning), building a classification model (a classifier) for deciding about other record pairs [10, 15].", "startOffset": 268, "endOffset": 276}, {"referenceID": 18, "context": "Most of the work on applying ML to ER work at the record level [22, 10, 11], and only some of the attributes, or their features, i.", "startOffset": 63, "endOffset": 75}, {"referenceID": 7, "context": "Most of the work on applying ML to ER work at the record level [22, 10, 11], and only some of the attributes, or their features, i.", "startOffset": 63, "endOffset": 75}, {"referenceID": 8, "context": "Most of the work on applying ML to ER work at the record level [22, 10, 11], and only some of the attributes, or their features, i.", "startOffset": 63, "endOffset": 75}, {"referenceID": 1, "context": "used [2, 19, 24].", "startOffset": 5, "endOffset": 16}, {"referenceID": 15, "context": "used [2, 19, 24].", "startOffset": 5, "endOffset": 16}, {"referenceID": 20, "context": "used [2, 19, 24].", "startOffset": 5, "endOffset": 16}, {"referenceID": 13, "context": "made identical [16, 17].", "startOffset": 15, "endOffset": 23}, {"referenceID": 5, "context": "In [6, 7], MDs were extended with matching functions (MFs).", "startOffset": 3, "endOffset": 9}, {"referenceID": 6, "context": "In [6, 7], MDs were extended with matching functions (MFs).", "startOffset": 3, "endOffset": 9}, {"referenceID": 0, "context": "LogiQL supports relational data management and, among several other features [1], an extended form of Datalog with stratified negation [9].", "startOffset": 77, "endOffset": 80}, {"referenceID": 6, "context": "In both cases, the set of MDs are interaction-free [7], which results, for each entity, in the unique set of blocks, and eventually into a single, duplicate-free instance [7].", "startOffset": 51, "endOffset": 54}, {"referenceID": 6, "context": "In both cases, the set of MDs are interaction-free [7], which results, for each entity, in the unique set of blocks, and eventually into a single, duplicate-free instance [7].", "startOffset": 171, "endOffset": 174}, {"referenceID": 11, "context": "We independently used three established classification algorithms: support vector machines (SVMs) [25], k-nearest neighbor (K-NN) [14], and non-parametric Bayes classifier (NBC) [4].", "startOffset": 130, "endOffset": 134}, {"referenceID": 3, "context": "We independently used three established classification algorithms: support vector machines (SVMs) [25], k-nearest neighbor (K-NN) [14], and non-parametric Bayes classifier (NBC) [4].", "startOffset": 178, "endOffset": 181}, {"referenceID": 6, "context": "com 2 For arbitrary sets of MDs, we need higher expressive power [7], such as that provided by answer set programming [3].", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "com 2 For arbitrary sets of MDs, we need higher expressive power [7], such as that provided by answer set programming [3].", "startOffset": 118, "endOffset": 121}, {"referenceID": 14, "context": "The experimental results show that our system improves ER accuracy over traditional blocking techniques [18], which we will call standard blocking, where just blocking-key similarities are used.", "startOffset": 104, "endOffset": 108}, {"referenceID": 6, "context": "As in [7], we assume records have unique, fixed, global identifiers, rids, which are positive integers.", "startOffset": 6, "endOffset": 9}, {"referenceID": 13, "context": "= R2[\u02322] [16, 17].", "startOffset": 9, "endOffset": 17}, {"referenceID": 6, "context": "A dynamic, chase-based semantics for MDs with matching functions (MFs) was introduced in [7].", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "associative, and then induce a partial-order structure \u3008DomA, A\u3009, with: a A a\u2032 :\u21d4 mA(a, a\u2032) = a\u2032 [6, 5].", "startOffset": 97, "endOffset": 103}, {"referenceID": 4, "context": "associative, and then induce a partial-order structure \u3008DomA, A\u3009, with: a A a\u2032 :\u21d4 mA(a, a\u2032) = a\u2032 [6, 5].", "startOffset": 97, "endOffset": 103}, {"referenceID": 6, "context": ", each attribute may appear in either the RHS or LHS of MDs in \u03a3), there is a unique resolved instance that is computable in polynomial time in |D| [7].", "startOffset": 148, "endOffset": 151}, {"referenceID": 0, "context": "Similarity computation is based on similarity functions, Sf i : DomAi \u00d7 DomAi \u2192 [0, 1], each of which assigns a numerical value, called similarity weight, to the comparisons of values for a record attributeAi (from a pre-chosen subset of attributes) (cf.", "startOffset": 80, "endOffset": 86}, {"referenceID": 6, "context": "In essence, this makes our set of merging-MDs interaction-free, and leads to a unique resolved instance [7].", "startOffset": 104, "endOffset": 107}, {"referenceID": 0, "context": "The first attribute is made an identifier [1].", "startOffset": 42, "endOffset": 45}, {"referenceID": 10, "context": "12 We used three well-known similarity functions [13], depending on the attribute domains.", "startOffset": 49, "endOffset": 53}, {"referenceID": 19, "context": "\u201cTF-IDF cosine similarity\u201d [23] used for computing similarities for text-valued attributes, whose values are string vectors.", "startOffset": 27, "endOffset": 31}, {"referenceID": 21, "context": "For attributes with short string values, such as author name, we applied \u201cJaro-Winkler similarity\u201d [26].", "startOffset": 99, "endOffset": 103}, {"referenceID": 17, "context": "Finally, for numerical attributes, such as publication year, we used \u201cLevenshtein distance\u201d [21], which computes similarity of", "startOffset": 92, "endOffset": 96}, {"referenceID": 6, "context": "More generally, for the application-dependent set, \u03a3 , of blocking-MDs we adopt the chase-based semantics for entity resolution [7].", "startOffset": 128, "endOffset": 131}, {"referenceID": 6, "context": "for every two blocking-MDs m1,m2, the set of attributes on the RHS of m1 and the set of attributes on the LHS of m2 on which there are similarity predicates, are disjoint [7].", "startOffset": 171, "endOffset": 174}, {"referenceID": 0, "context": "An atom Ri[X\u0304]=Bl states that predicate Ri is functional on X\u0304 [1].", "startOffset": 63, "endOffset": 66}, {"referenceID": 0, "context": "In LogiQL, \u201c!\u201d, as in the body above, is used for negation [1].", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "14 Notice that since we have interaction-free sets of blocking-MDs, stratified Datalog programs are expressive enough to express and enforce them [3].", "startOffset": 146, "endOffset": 149}, {"referenceID": 6, "context": "The RHS means that the two records are merged into a new full record r\u0304, with r\u0304[Ai] := mAi(r\u03041[Ai], r\u03042[Ai]) [7].", "startOffset": 110, "endOffset": 113}, {"referenceID": 9, "context": "The former is the number of true duplicate candidate record-pairs divided by the number of true duplicate pairs, and precision is the number of true candidate duplicate record-pairs divided by the total number of candidate pairs [12].", "startOffset": 229, "endOffset": 233}, {"referenceID": 9, "context": "On the other hand, blocking techniques that result in larger blocks generate a higher number of candidate record-pairs that will likely cover more true duplicate pairs, at the cost of having to compare more candidate pairs [12].", "startOffset": 223, "endOffset": 227}], "year": 2016, "abstractText": "Entity resolution (ER), an important and common data cleaning problem, is about detecting data duplicate representations for the same external entities, and merging them into single representations. Relatively recently, declarative rules called matching dependencies (MDs) have been proposed for specifying similarity conditions under which attribute values in database records are merged. In this work we show the process and the benefits of integrating three components of ER: (a) Classifiers for duplicate/non-duplicate record pairs built using machine learning (ML) techniques, (b) MDs for supporting both the blocking phase of ML and the merge itself; and (c) The use of the declarative language LogiQL -an extended form of Datalog supported by the LogicBlox platformfor data processing, and the specification and enforcement of MDs.", "creator": "LaTeX with hyperref package"}}}