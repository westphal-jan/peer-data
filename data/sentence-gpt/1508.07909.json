{"id": "1508.07909", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2015", "title": "Neural Machine Translation of Rare Words with Subword Units", "abstract": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, so the translation of rare and unknown words is an open problem. Previous work addresses this problem through back-off dictionaries. In this paper, we introduce a simpler and more effective approach, enabling the translation of rare and unknown words by encoding them as sequences of subword units, based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations) via a single word system. Our approach is a step-by-step approach that relies on the first step in this paper. By using these data sets, we hope that in order to further improve the understanding of the underlying concepts of NMT, the transliteration of rare and unknown words is possible.", "histories": [["v1", "Mon, 31 Aug 2015 16:37:31 GMT  (209kb)", "http://arxiv.org/abs/1508.07909v1", null], ["v2", "Fri, 27 Nov 2015 15:41:25 GMT  (122kb)", "http://arxiv.org/abs/1508.07909v2", "new results with improved baseline; more references in related work; cuts to fit conference proceedings"], ["v3", "Thu, 17 Mar 2016 14:56:06 GMT  (125kb)", "http://arxiv.org/abs/1508.07909v3", null], ["v4", "Fri, 3 Jun 2016 15:01:02 GMT  (125kb)", "http://arxiv.org/abs/1508.07909v4", "accepted at ACL 2016"], ["v5", "Fri, 10 Jun 2016 14:45:08 GMT  (197kb)", "http://arxiv.org/abs/1508.07909v5", "accepted at ACL 2016; new in this version: figure 3"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rico sennrich", "barry haddow", "alexandra birch"], "accepted": true, "id": "1508.07909"}, "pdf": {"name": "1508.07909.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["rico.sennrich@ed.ac.uk,", "a.birch@ed.ac.uk,", "bhaddow@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 8.\n07 90\n9v 1\n[ cs\n.C L\n] 3\n1 A\nug 2\n01 5"}, {"heading": "1 Introduction", "text": "Neural machine translation has recently shown impressive results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). However, the translation of rare words is an open problem. The vocabulary of neural models is typically limited to 30 000\u201350 000 words, but translation is an open-vocabulary problem, and especially for languages with productive word for-\nThe research results presented in this publication were conducted in the cooperation with Samsung Electronics Polska sp. z o.o. - Samsung R&D Institute Poland.\nmation processes such as agglutination and compounding, translation models require mechanisms to translate and generate words that are rare or have never occurred during training.\nJean et al. (2015a) recently proposed techniques for efficiently increasing the vocabulary size by keeping only part of the vocabulary in memory during training, and filtering the vocabulary during decoding. They present results with a vocabulary size of 500 000 words. However, we will show that translation accuracy is still low for rare words, even if they are in the network vocabulary.\nThe translation of out-of-vocabulary words is addressed in (Jean et al., 2015a; Luong et al., 2015) through a back-off to a dictionary lookup. We note that such techniques make assumptions that often do not hold true in practice. For instance, there is not always a 1-to-1 correspondence between source and target words because of variance in the degree of morphological synthesis between languages. For example, German compounds such as Sonnensystem corresponds to multiple tokens in English (\u2018solar system\u2019). Also, a back-off dictionary is unable to translate or generate unseen words. Copying unknown words into the target text, as done by (Jean et al., 2015a; Luong et al., 2015), is a reasonable strategy for names, but transliteration is often required, especially if alphabets differ.\nThis paper has two main contributions:\n\u2022 We propose a simple technique to increase the effective vocabulary of neural translation models. Instead of replacing rare words with special tokens which are later used for a backoff translation (Jean et al., 2015a; Luong et al., 2015), we segment them into subword units, allowing the neural networks to learn their translation.\n\u2022 We investigate different algorithms for word segmentation and compare them in terms of\nvocabulary size, text length, and translation quality. We adapt byte pair encoding (Gage, 1994), a simple compression algorithm, to the task of word segmentation.\nWe show that neural translation models with subword units not only eliminate the need for a back-off dictionary, but are also more powerful. The subword models achieve better accuracy for the translation of rare words than a back-off dictionary, and are able to productively generate new words that were not seen at training time. Our analysis shows that the neural networks are able to learn compounding and transliteration from our subword representations."}, {"heading": "2 Neural Machine Translation", "text": "We follow the neural machine translation architecture by Bahdanau et al. (2014), which we will here briefly summarize. However, we note that out approach is not specific to this architecture.\nThe neural machine translation system is implemented as an encoder-decoder network with recurrent neural networks.\nThe encoder is a bidirectional neural network with gated recurrent units (Cho et al., 2014) that reads an input sequence x = (x1, ..., xm) and calculates a forward sequence of hidden states ( \u2212\u2192 h1, ..., \u2212\u2192 hm), and a backward sequence ( \u2190\u2212 h1, ..., \u2190\u2212 hm). The hidden states \u2212\u2192 hj and \u2190\u2212 hj are concatenated to obtain the annotation vector hj .\nThe decoder is a recurrent neural network that predicts a target sequence y = (y1, ..., yn). Each word yi is predicted based on a recurrent hidden state si, the previously predicted word yi\u22121, and a context vector ci. ci is computed as a weighted sum of the annotations hj . The weight of each annotation hj is computed through an alignment model \u03b1ij , which models the probability that yi is aligned to xj . The alignment model is a singlelayer feedforward neural network that is learned jointly with the rest of the network through backpropagation.\nA detailed description can be found in (Bahdanau et al., 2014). Training is performed on a parallel corpus with stochastic gradient descent. For translation, a beam search with small beam size is employed."}, {"heading": "3 Subword Translation", "text": "The main motivation behind this paper is that the translation of some words is transparent in that they are translatable by a competent translator even if they are novel to him or her, based on a translation of known subword units such as morphemes or phonemes. Word categories whose translation is potentially transparent include:\n\u2022 named entities. Between languages that share an alphabet, names can often be copied from source to target text. Transcription or transliteration may be required, especially if the alphabets or syllabaries differ. Example: Barack Obama (English; German) \u0411\u0430\u0440\u0430\u043a \u041e\u0431\u0430\u043c\u0430 (Russian) \u30d0\u30e9\u30af\u30fb\u30aa\u30d0\u30de (ba-ra-ku o-ba-ma) (Japanese)\n\u2022 cognates and loanwords. Cognates and loanwords with a common origin can differ in regular ways between languages, so that character-level translation rules are sufficient (Tiedemann, 2012). Example: claustrophobia (English) Klaustrophobie (German) \u041a\u043b\u0430\u0443\u0441\u0442\u0440\u043e\u0444\u043e\u0431\u0438\u044f (Klaustrofobi\u00e2) (Russian)\n\u2022 morphologically complex words. Words containing multiple morphemes, for instance formed via compounding, affixation, or inflection, may be translatable by translating the morphemes separately. Example: solar system (English) Sonnensystem (Sonne + System) (German) Naprendszer (Nap + Rendszer) (Hungarian)\nIn an analysis of 100 rare tokens (not among the 50 000 most frequent types) in a German training text, the majority of tokens are potentially translatable from English through smaller units. We find 56 compounds, 21 names, 6 loanwords with a common origin (emancipate\u2192emanzipieren), 5 cases of transparent affixation (sweetish \u2018sweet\u2019 + \u2018-ish\u2019 \u2192 s\u00fc\u00dflich \u2018s\u00fc\u00df\u2019 + \u2018-lich\u2019), 1 number and 1 computer language identifier.\nOur hypothesis is that a segmentation of rare words into appropriate subword units is sufficient to allow for the neural translation network to learn transparent translations, and even generalize this knowledge to translate and produce unseen words.1 The rest of this paper is focused on\n1Not every segmentation we produce is transparent. While we expect no performance benefit from opaque seg-\ndiscussing and evaluating different segmentation strategies."}, {"heading": "3.1 Related Work", "text": "For Statistical Machine Translation, the translation of unknown words has been the subject of intensive research. The segmentation of morphologically complex words such as compounds is widely used, and various algorithm for morpheme segmentation have been investigated (Nie\u00dfen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007).\nA large proportion of unknown words are names, which can just be copied into the target text if both languages use the same alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has been successfully used for closely related languages (Tiedemann, 2012).\nMikolov et al. (2012) investigates subword language models, and proposes to use syllables as units. We test a slightly more sophisticated syllable segmentation developed for word hyphenation (Liang, 1983). The best choice of subword units may be task-specific, and for speech recognition, phone-level language models have been used (Bazzi and Glass, 2000).\nVarious techniques have been proposed to produce fixed-length continuous word vectors based on characters or morphemes (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015). These approaches offer no obvious mechanism for producing strings from the continuous word vectors, which is central for NMT decoding. We could theoretically employ such techniques on the source side of the encoder\u2013decoder architecture, but we expect that the attention mechanism benefits from our variable-length representation: the network can learn to place attention on different subword units at each step.\nNeural machine translation differs from phrasebased methods in that there are strong incentives to minimize the vocabulary size of neural models to increase time and space efficiency. At the same time, we also want a compact representation of the text itself, since an increase in text length reduces efficiency and increases the distances over which neural models need to pass information.\nmentations, i.e. segmentations where the units cannot be translated independently, our NMT models seem to be robust towards such oversplitting.\nA simple method to manipulate the trade-off between vocabulary size and text size is to use shortlists of unsegmented words, using subword units only for rare words. As an alternative, we propose a segmentation algorithm based on byte pair encoding (BPE), which lets us learn a vocabulary that provides a good compression rate of the text."}, {"heading": "3.2 Byte Pair Encoding (BPE)", "text": "Byte Pair Encoding (BPE) (Gage, 1994) is a simple data compression technique that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte. We adapt this algorithm for word segmentation. Instead of merging frequent pairs of bytes, we merge characters or character sequences.\nAs starting point, we initialize the symbol vocabulary with the character vocabulary, and represent each word as a sequence of characters, plus a special end-of-word symbol \u2018</w>\u2019, which allows us to restore the original tokenization after translation. We iteratively count all symbol pairs and replace each occurrence of the most frequent pair (\u2018A\u2019, \u2018B\u2019) with a new symbol \u2018AB\u2019. Each merge operation produces a new symbol which represent a character n-gram, resulting in a variable-length encoding of each word. BPE will learn to represent frequent words as a single symbol, and we apply it without a shortlist. The final symbol vocabulary size is equal to the size of the initial vocabulary, plus the number of merge operations. The number of merge operations is the only hyperparameter of the algorithm.\nFor efficiency, we do not consider pairs that cross word boundaries. The algorithm can thus be run on the dictionary extracted from a text, with each word being weighted by its frequency. Algorithm 1 shows a minimal Python implementation of the algorithm. In practice, we increase efficiency by indexing all pairs, and updating the data structures incrementally.\nThe main difference to other compression algorithms, such as Huffman encoding, which have been proposed to produce a variable-length encoding of words for NMT (Chitnis and DeNero, 2015), is that our symbol sequences are still interpretable as subword units, and that the network can generalize to translate and produce new words (unseen at training time) on the basis of these subword units. From the example dictionary {\u2018low\u2019, \u2018lower\u2019, \u2018newest\u2019, \u2018widest\u2019} in algorithm 1, we\nAlgorithm 1 character-level BPE applied to dictionary.\nimport re, collections\ndef get_stats(vocab): pairs = collections.defaultdict(int) for word, freq in vocab.items():\nsymbols = word.split() for i in range(len(symbols)-1): pairs[symbols[i],symbols[i+1]] += freq\nreturn pairs\ndef merge_vocab(pair, v_in): v_out = {} p = re.compile(r'(?<!\\S)' + ' '.join(pair) + r'(?!\\S)') for word in v_in:\nw_out = p.sub(''.join(pair), word) v_out[w_out] = v_in[word]\nreturn v_out\nvocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2, 'n e w e s t </w>' : 6, 'w i d e s t </w>' : 3} num_merges = 10 for i in range(num_merges):\npairs = get_stats(vocab) best = max(pairs, key=pairs.get) vocab = merge_vocab(best, vocab) print(best)\nlearn to merge the character sequences \u2018low\u2019, and \u2018est</w>\u2019, among others. The merge operations can be applied to out-of-vocabulary items such as \"lowest\" to segment it into \u2018low est</w>\u2019.\nWe evaluate two methods of applying BPE: learning two independent encodings, one for the source, one for the target vocabulary, or learning the encoding on the joint vocabulary. The former has the advantage of being more compact in terms of text and vocabulary size, whereas the latter improves consistency between the source and the target segmentation. If we apply BPE independently, the same name may be segmented differently in the two languages, which makes it harder for the neural models to learn a mapping between the subword units. To increase the consistency between English and Russian segmentation despite the differing alphabets, we transliterate the Russian vocabulary into Latin characters with ISO-9 to learn the joint BPE encoding, then transliterate the BPE merge operations back into Cyrillic to apply them to the Russian training text.2"}, {"heading": "4 Evaluation", "text": "We aim to answer the following empirical questions:\n\u2022 Can we improve the translation of rare and unseen words in neural machine translation by representing them via subword units?\n2Since the Russian training text also contains words that use the Latin alphabet, we also apply the Latin BPE operations.\n\u2022 Which segmentation into subword units performs best in terms of vocabulary size, text size, and translation quality?\nWe perform experiments on data from the shared translation task of the 2015 Workshop on Statistical Machine Translation. For English\u2192German, our training set consists of 4.2 million sentence pairs, or approximately 100 million tokens. For English\u2192Russian, the training set consists of 2.6 million sentence pairs, or approximately 50 million tokens. We tokenize and truecase the data with the scripts provided in Moses (Koehn et al., 2007).\nWe use Groundhog3 as the implementation of the NMT system for all experiments (Bahdanau et al., 2014).\nWe report BLEU with the mteval-v13a.pl script for comparability with official WMT results. We use newstest2013 as development set, and report results on newstest2014 and newstest2015.\nWe generally follow settings by previous work (Bahdanau et al., 2014; Jean et al., 2015a). All networks have a hidden layer size of 1000, and an embedding layer size of 620. Following Jean et al. (2015a), we only keep a shortlist of \u03c4 = 30000 words in memory.\nDuring training, we use Adadelta (Zeiler, 2012), a minibatch size of 80, and reshuffle the training set between epochs. We train a network for approximately 7 days, then take the last 4 saved models (models being saved every 12 hours), and continue training each with a fixed embedding layer (as suggested by (Jean et al., 2015a)) for 12 hours. We report results of the system that performed best on our development set (newstest2013), and of an ensemble of all 4 models.\nWe use a beam size of 12 for beam search, with probabilities normalized by sentence length. We use a bilingual dictionary based on fast-align (Dyer et al., 2013). For our baseline, this serves to speed up translation, since only a filtered list of candidate translations is stored in memory (like Jean et al. (2015a), we use K = 30000; K \u2032 = 10), and as back-off dictionary for rare words. For the experimental systems with subword units, the use as back-off dictionary falls away, and the candidate list is of varying importance for speed, depending on the size of the target vocabulary.\n3https://github.com/sebastien-j/LV_ groundhog"}, {"heading": "4.1 Subword statistics", "text": "Apart from translation quality, which we will verify empirically, our objectives are efficiency and model compactness. The time complexity of encoder-decoder architectures is at least4 linear to sequence length, and oversplitting harms efficiency. On the other hand, an algorithm that performs few splits produces large vocabularies, and our aim is to allow for compact networks with small vocabularies, and to encode unknown words through subword units that are known to the network. We compare different segmentation algorithms on the basis of the number of tokens, which affects efficiency, the vocabulary size, which affects model compactness and efficiency, and the number of unknown tokens in a test set.\nWe perform experiments with different segmentation techniques on the German section of the parallel training data. A simple baseline is the segmentation of words into character n-grams.5 Compound splitting (Koehn and Knight, 2003) is a popular technique in SMT. Morfessor (Creutz and Lagus, 2002) implements word segmentation based on minimum description length. Syllabification has been used to produce subword units for language modelling in (Mikolov et al., 2012), and we employ a word hyphenation algorithm (Liang, 1983).\nResults are shown in table 1. Character n-grams allow for different trade-offs between sequence length (# tokens) and vocabulary size (# types), depending on the choice of n. However, the increase in sequence length is substantial for all 1 \u2264 n \u2264 3. Of the more sophisticated segmentation methods, morfessor and BPE offer interesting trade-offs between sequence length and vocabulary size, while we consider compound splitting too conservative, hyphenation too aggressive for our purposes.6 We also note that there is noise in our subword symbol sets, e.g. because of characters from foreign alphabets, and we can use a pruned subset for the neural network. We use the 500 most frequent charac-\n4The attention model of (Bahdanau et al., 2014) has complexity O(m \u00b7 n), m and n being the number of source and target symbols.\n5Our character n-grams do not cross word boundaries. We mark whether a subword is word-final or not with a special character, which allows us to restore the original tokenization.\n6Note that hyperparameters and the choice of training corpus affect morfessor, compound splitting, and BPE. Our overview only shows one possible setting \u2013 typically default settings, 59 500 merge operations for BPE, and 89 500 for BPE applied on the joint vocabulary.\nters, the 10 000 most frequent character bigrams, or the 50 000 most frequent character trigrams, in the NMT systems that use either of these as backoff.\nDifferent segmentation methods can be combined: we perform experiments in which we leave the 50 000 most frequent word types unsegmented, and represent all other words through character ngrams. For character bigrams, this leads to a 29% increase in the number of tokens compared to the baseline without segmentation, reducing the number of types to 69 000. The BPE encoding is more compact, increasing text size only by 12% with a similar vocabulary size.\nWe evaluate various character n-gram segmentations, Morfessor and BPE in terms of translation quality."}, {"heading": "4.2 Translation experiments", "text": "English\u2192German translation results are shown in Table 2. Our internal baseline WDict is a system without segmentation and with a back-off dictionary. The back-off dictionary yields an improvement of about 1 BLEU over the system WUnk, which just represents out-of-vocabulary words as UNK.7 We attempted to reproduce the results by Jean et al. (2015a), but our baseline is slightly worse8; We speculate that the difference is caused by differences in preprocessing and cleanup, training time, different hyperparameters, and/or ran-\n7We use UNK to denote words that are outside the model vocabulary, and OOV to denote words that do not occur in the training text.\n8The published results are not directly comparable because of different evaluation scripts, but WDict obtains a BLEU of 19.0 on newstest2014 with the multi-bleu.pl script, as compared to 19.4 BLEU reported by (Jean et al., 2015a).\ndom variation.\nThe system with morfessor segmentation MDict yields no improvement over the baseline, and was not pursued further. However, we do observe improvements over the baseline from using character n-grams to represent rare words. The subword system closest to WDict is the system C2-3/500k with the same unsegmented vocabulary of 300 000 source and 500 000 target words, with out-of-vocabulary words represented via character bigrams, for which we observe an average improvement of 0.3 BLEU over our baseline. Reducing the size of the shortlist to 50k (C2-50k) yields further improvements, for an average BLEU improvement of 0.8 over the baseline. Character trigrams (C3-50k), and BPE with a jointly learned encoding (BPE-J90k) yield similar improvements over the baseline than character bigrams, which indicates that the Neural network robustly learns from different subword representations. On average, BPE-J90k outperforms BPE with independent encodings (BPE-60k).\nThe performance of our NMT systems is below our syntax-based baseline by Sennrich and Haddow (2015). We note that Jean et al. (2015b) report a score of 24.8 BLEU on newstest2015, using an ensemble of 8 NMT models. We are confident that our improvements to the translation of rare words are orthogonal to improvements achievable through other improvements in the network architecture, training algorithm, or better ensembles.\nWe test a subset of the systems on the language pair English\u2192Russian. Results are shown in Table 3. The back-off dictionary is less effective for this language pair, partially because the back-off\ndictionary is incapable of transliterating unknown names. The quality of the WDict baseline is substantially below the phrase-based state-of-the-art: the English\u2192Russian system by Haddow et al. (2015) outperforms WDict by 4.5 BLEU on newstest2014, and by 3.3 BLEU on newstest2015. The subword models are a step towards closing this gap, and BPE-J90k yields an average improvement of 1.5 BLEU over WDict.\nAs a further comment on our translation results, we want to emphasize that performance variability is still an open problem with NMT. On our development set, we observe differences of up to 1 BLEU between different models. For single systems, we report the results of the model that performs best on dev (out of 4), which has a stabilizing effect, but how to control for randomness deserves further attention in future research. To evaluate the effect of different subword segmentation strategies on the translation of rare and unknown words, we conduct a more focused analysis."}, {"heading": "5 Analysis", "text": ""}, {"heading": "5.1 Unigram accuracy", "text": "Our main claims are that the translation of rare and unknown words is poor in state-of-the-art NMT models, and that subword models improve the translation of these word types. We empirically verify these claims by measuring clipped unigram precision and recall of words of different frequency.9 We report the harmonic mean between precision and recall, f1, and we plot targetside words sorted by their frequency in the training\n9Clipped unigram precision is essentially 1-gram BLEU without brevity penalty.\nset.10\nFigure 1 shows results for the English\u2013German ensemble systems on newstest2013. Unigram f1 of all systems tends to decrease for lowerfrequency words. The baseline system has a spike in f1 for OOVs, i.e. words that do not occur in the training text. This is because a high proportion of OOVs are names, for which a copy from the source to the target text is a good strategy for English\u2192German.\nThe systems with a target vocabulary of 500 000 words mostly differ in how well they translate words with rank > 500 000. A back-off dictionary is an obvious improvement over producing UNK, which results in 0 f1, but the subword system C23/500k achieves better performance. Note that the subword system can also produce new OOVs such as compounds, while the back-off dictionary can only produce those OOVs which can be copied from the source, usually names.\nFor the 50 000 most frequent words, the representation is the same for all neural networks, and all neural networks achieve comparable unigram f1 for this category. For the interval between frequency rank 50 000 and 500 000, the comparison between C2-3/500k and C2-50k unveils an interesting difference. The two systems only differ in the size of the shortlist, with C2-3/500k representing words in this interval as single units, and C250k via subword units. We find that the performance of C2-3/500k degrades heavily up to frequency rank 500 000, at which point the model switches to a subword representation and performance recovers. The performance of C2-50k remains more stable. We attribute this to the fact that subword units are less sparse than words. In\n10We perform binning of words with the same training set frequency, and apply bezier smoothing to the graph.\nour training set, the frequency rank 50 000 corresponds to a frequency of 60 in the training data; the frequency rank 500 000 to a frequency of 2. Because subword representations are less sparse, reducing the size of the network vocabulary, and representing more words via subword units, can lead to better performance.\nNot shown in the graph, but interesting to note, is the fact that the back-off dictionary itself does not benefit from an ensemble of neural networks. In contrast, the subword models improve in the translation of rare words and OOVs when moving from a single model to an ensemble. This is a further argument in favour of modelling the translation of rare words in the main model, rather than backing off to an external model: we hope that future improvements in the NMT architecture and training regime will further increase the magnitude by which subword models outperform backoff approaches.\nTo put the f1 curve in relation to the total BLEU scores reported in Table 2, we note that the 50 000 most frequent word types in the training data make up 92.8% of tokens in the reference translation. 4.7% fall into the range 50 000\u2013500 000, 0.8% into the range 500 000\u2013\u221e, and 1.7% are OOV. While the comparison between WDict and BPEJ90k shows massive improvements in f1 for rare words (from 34% to 38% for the range 50 000\u2013 500 000; from 13% to 34% for the range 500 000\u2013 \u221e), the relative rarity of these categories means that that the total improvement in BLEU is a more modest 0.6 points. However, rare words tend to be content words such as names and compounds, which carry central information in a sentence. We suspect that BLEU underestimates the effect on actual translation quality of improving the translation of these rare words.\nFigure 2 shows unigram f1 for the translation direction English\u2192Russian. The most obvious difference is that the there is no spike in f1 for OOVs because unknown names can only rarely be copied, and usually require transliteration.\nThe f1 graphs hides some qualitative differences between systems. For English\u2192German, the dictionary back-off system produces few OOVs, but with high precision, whereas the subword systems achieve higher recall, but lower precision. Precision, recall and f1 for words of different frequencies are shown in Tables 4 and 5. For common words (category <50k), we attribute differences to random variation between models; differences in output length affect the precision\u2013 recall trade-off. We note that the character bigram model C2-50k produces the most OOV words, and achieves relatively low precision for this category. BPE-60k produces fewer OOVs, but its recall suffers from transliteration (or copy) errors due to segmentation inconsistencies. The joint BPE encoding of BPE-J90k improves the quality of the subword translations, particularly for names in the category OOV."}, {"heading": "5.2 Manual Analysis", "text": "Table 6 shows two translation examples for the translation direction English\u2192German, Table 7 for English\u2192Russian. The baseline system fails for all of the examples, either by deleting content (health, situation), or by copying source words that should be translated or transliterated. The subword translations of health research in-\nstitutes show that the subword systems are capable of learning translations when oversplitting (research\u2192Fo|rs|ch|un|g), or when the segmentation does not match morpheme boundaries: the segmentation Forschungs|instituten would be linguistically more plausible, and simpler to align to the English research institutes, than the segmentation Forsch|ungsinstitu|ten in the BPE-60k system, but still, a correct translation is produced. If the systems have failed to learn a translation due to data sparseness, like for asinine, which should be translated as dumm, we see translations that are wrong, but could be plausible for (partial) loanwords (asinine Situation\u2192Asinin-Situation).\nThe English\u2192Russian examples show that the subword systems are capable of transliteration. However, transliteration errors do occur, either due to ambiguous transliterations, or because of non-consistent segmentations between source and target text which make it hard for the system to learn a transliteration mapping. Note that the BPE-60k system encodes Mirzayeva inconsistently for the two language pairs (Mirz|ayeva\u2192\u041c\u0438\u0440|\u0437\u0430|\u0435\u0432\u0430 Mir|za|eva). This example is still translated correctly, but we observe spurious insertions and deletions of characters in the BPE-60k system. An example is the transliteration of rakfisk, where a \u043f is inserted and a \u043a is deleted. We trace this error back to translation pairs in the training data with inconsistent segmentations, such as (p|rak|ri|ti\u2192\u043f\u0440\u0430|\u043a\u0440\u0438\u0442|\u0438 (pra|krit|i)), from which the translation (rak\u2192\u043f\u0440\u0430) is erroneously learned.\nThe segmentation of the BPE system with a jointly learned segmentation (BPE-J90k) is more consistent (pra|krit|i\u2192\u043f\u0440\u0430|\u043a\u0440\u0438\u0442|\u0438 (pra|krit|i))."}, {"heading": "6 Conclusion", "text": "The main contribution of this paper is that we show that neural network encoder\u2013decoder translation systems are able to translate rare and unseen words by representing them as a sequence of subword units. This is both simpler and more effective than using an external translation system (even just a simple dictionary) as a back-off model. The NMT system is relatively robust regarding the type of subword unit, and we observe similar performance gains over the baseline with\ncharacter bigrams, character trigrams, or variablelength units. In regards to efficiency, we achieve the most compact representation of the text and vocabulary with a word segmentation technique based on byte pair encoding.\nOur analysis shows that not only out-ofvocabulary words, but also rare in-vocabulary words are translated poorly by our baseline NMT system. We find that for rare words, a subword representation improves the model\u2019s ability to learn their translation. We expect the optimal point at which to switch from full-word to subword representation to depend on the language pair and the amount of training data. In this work, our choice of vocabulary size is somewhat arbi-\ntrary, and mainly motivated by comparison to prior work. One avenue of future research is to learn the optimal vocabulary size for a translation task automatically. We also believe there is further potential in bilingually informed segmentation algorithms to create more semantically meaningful and translatable subword units, although the segmentation algorithm cannot rely on the target text at runtime. Despite language-specific differences, we believe that a subword representation of rare words is suitable for NMT for most language pairs, eliminating the need for large network vocabularies or backoff models."}, {"heading": "Acknowledgments", "text": "The research results presented in this publication were conducted in the cooperation with Samsung Electronics Polska sp. z o.o. - Samsung R&D Institute Poland. This project received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement 645452 (QT21)."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Modeling outof-vocabulary words for robust speech recognition", "author": ["Issam Bazzi", "James R. Glass."], "venue": "Sixth International Conference on Spoken Language Processing, ICSLP 2000 / INTERSPEECH 2000, Beijing, China, October 16-20, 2000, pages", "citeRegEx": "Bazzi and Glass.,? 2000", "shortCiteRegEx": "Bazzi and Glass.", "year": 2000}, {"title": "Compositional Morphology for Word Representations and Language Modelling", "author": ["Jan A. Botha", "Phil Blunsom."], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML), Beijing, China.", "citeRegEx": "Botha and Blunsom.,? 2014", "shortCiteRegEx": "Botha and Blunsom.", "year": 2014}, {"title": "VariableLength Word Encodings for Neural Translation Models", "author": ["Rohan Chitnis", "John DeNero."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Chitnis and DeNero.,? 2015", "shortCiteRegEx": "Chitnis and DeNero.", "year": 2015}, {"title": "Learning Phrase Representations using RNN Encoder\u2013 Decoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Pro-", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Unsupervised Discovery of Morphemes", "author": ["Mathias Creutz", "Krista Lagus."], "venue": "Proceedings of the ACL-02 Workshop on Morphological and Phonological Learning, pages 21\u201330. Association for Computational Linguistics.", "citeRegEx": "Creutz and Lagus.,? 2002", "shortCiteRegEx": "Creutz and Lagus.", "year": 2002}, {"title": "Integrating an Unsupervised Transliteration Model into Statistical Machine Translation", "author": ["Nadir Durrani", "Hassan Sajjad", "Hieu Hoang", "Philipp Koehn."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational", "citeRegEx": "Durrani et al\\.,? 2014", "shortCiteRegEx": "Durrani et al\\.", "year": 2014}, {"title": "A Simple, Fast, and Effective Reparameterization of IBM Model 2", "author": ["Chris Dyer", "Victor Chahuneau", "Noah A. Smith."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-", "citeRegEx": "Dyer et al\\.,? 2013", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "A New Algorithm for Data Compression", "author": ["Philip Gage."], "venue": "C Users J., 12(2):23\u201338, February.", "citeRegEx": "Gage.,? 1994", "shortCiteRegEx": "Gage.", "year": 1994}, {"title": "The Edinburgh/JHU Phrase-based Machine Translation Systems for WMT 2015", "author": ["Barry Haddow", "Matthias Huck", "Alexandra Birch", "Nikolay Bogoychev", "Philipp Koehn."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation.", "citeRegEx": "Haddow et al\\.,? 2015", "shortCiteRegEx": "Haddow et al\\.", "year": 2015}, {"title": "On Using Very Large Target Vocabulary for Neural Machine Translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "citeRegEx": "Jean et al\\.,? 2015a", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Montreal Neural Machine Translation Systems for WMT\u201915", "author": ["S\u00e9bastien Jean", "Orhan Firat", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation.", "citeRegEx": "Jean et al\\.,? 2015b", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent Continuous Translation Models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle, October. Association for Computational Linguistics.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Empirical Methods for Compound Splitting", "author": ["Philipp Koehn", "Kevin Knight."], "venue": "EACL \u201903: Proceedings of the Tenth Conference on European Chapter of the Association for Computational Linguistics, pages 187\u2013193, Budapest, Hungary. Asso-", "citeRegEx": "Koehn and Knight.,? 2003", "shortCiteRegEx": "Koehn and Knight.", "year": 2003}, {"title": "Moses: Open Source Toolkit for Statistical Machine Translation", "author": ["Constantin", "Evan Herbst."], "venue": "Proceedings of the ACL-2007 Demo and Poster Sessions, pages 177\u2013180, Prague, Czech Republic. Association for Computational Linguistics.", "citeRegEx": "Constantin and Herbst.,? 2007", "shortCiteRegEx": "Constantin and Herbst.", "year": 2007}, {"title": "Word hy-phen-a-tion by com-put-er", "author": ["Franklin M. Liang."], "venue": "Ph.D. thesis, Stanford University, Department of Linguistics, Stanford, CA.", "citeRegEx": "Liang.,? 1983", "shortCiteRegEx": "Liang.", "year": 1983}, {"title": "Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation", "author": ["Wang Ling", "Chris Dyer", "Alan W. Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis."], "venue": "Proceedings of the", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Better Word Representations with Recursive Neural Networks for Morphology", "author": ["Thang Luong", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning, CoNLL", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Addressing the Rare Word Problem in Neural Machine Translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Subword Language Modeling with Neural Networks", "author": ["Tomas Mikolov", "Ilya Sutskever", "Anoop Deoras", "HaiSon Le", "Stefan Kombrink", "Jan Cernock\u00fd."], "venue": "Unpublished.", "citeRegEx": "Mikolov et al\\.,? 2012", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Improving SMT quality with morpho-syntactic analysis", "author": ["Sonja Nie\u00dfen", "Hermann Ney."], "venue": "18th Int. Conf. on Computational Linguistics, pages 1081\u20131085.", "citeRegEx": "Nie\u00dfen and Ney.,? 2000", "shortCiteRegEx": "Nie\u00dfen and Ney.", "year": 2000}, {"title": "A Joint Dependency Model of Morphological and Syntactic Structure for Statistical Machine Translation", "author": ["Rico Sennrich", "Barry Haddow."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Sennrich and Haddow.,? 2015", "shortCiteRegEx": "Sennrich and Haddow.", "year": 2015}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Character-Based Pivot Translation for Under-Resourced Languages and Domains", "author": ["J\u00f6rg Tiedemann."], "venue": "Proceedings of the 13th Conference of the", "citeRegEx": "Tiedemann.,? 2012", "shortCiteRegEx": "Tiedemann.", "year": 2012}, {"title": "Morphology-Aware Statistical Machine Translation Based on Morphs Induced in an Unsupervised Manner", "author": ["Sami Virpioja", "Jaakko J. V\u00e4yrynen", "Mathias Creutz", "Markus Sadeniemi."], "venue": "Proceedings of the Machine Translation Summit XI, pages", "citeRegEx": "Virpioja et al\\.,? 2007", "shortCiteRegEx": "Virpioja et al\\.", "year": 2007}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Matthew D. Zeiler."], "venue": "CoRR, abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 12, "context": "Neural machine translation has recently shown impressive results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 65, "endOffset": 144}, {"referenceID": 22, "context": "Neural machine translation has recently shown impressive results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 65, "endOffset": 144}, {"referenceID": 0, "context": "Neural machine translation has recently shown impressive results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 65, "endOffset": 144}, {"referenceID": 10, "context": "The translation of out-of-vocabulary words is addressed in (Jean et al., 2015a; Luong et al., 2015) through a back-off to a dictionary lookup.", "startOffset": 59, "endOffset": 99}, {"referenceID": 18, "context": "The translation of out-of-vocabulary words is addressed in (Jean et al., 2015a; Luong et al., 2015) through a back-off to a dictionary lookup.", "startOffset": 59, "endOffset": 99}, {"referenceID": 10, "context": "Jean et al. (2015a) recently proposed techniques for efficiently increasing the vocabulary size by keeping only part of the vocabulary in memory during training, and filtering the vocabulary during decoding.", "startOffset": 0, "endOffset": 20}, {"referenceID": 10, "context": "the target text, as done by (Jean et al., 2015a; Luong et al., 2015), is a reasonable strategy for names, but transliteration is often required, especially if alphabets differ.", "startOffset": 28, "endOffset": 68}, {"referenceID": 18, "context": "the target text, as done by (Jean et al., 2015a; Luong et al., 2015), is a reasonable strategy for names, but transliteration is often required, especially if alphabets differ.", "startOffset": 28, "endOffset": 68}, {"referenceID": 10, "context": "Instead of replacing rare words with special tokens which are later used for a backoff translation (Jean et al., 2015a; Luong et al., 2015), we segment them into subword units, allowing the neural networks to learn", "startOffset": 99, "endOffset": 139}, {"referenceID": 18, "context": "Instead of replacing rare words with special tokens which are later used for a backoff translation (Jean et al., 2015a; Luong et al., 2015), we segment them into subword units, allowing the neural networks to learn", "startOffset": 99, "endOffset": 139}, {"referenceID": 8, "context": "We adapt byte pair encoding (Gage, 1994), a simple compression algorithm, to the task of word segmentation.", "startOffset": 28, "endOffset": 40}, {"referenceID": 0, "context": "We follow the neural machine translation architecture by Bahdanau et al. (2014), which we will here briefly summarize.", "startOffset": 57, "endOffset": 80}, {"referenceID": 4, "context": "The encoder is a bidirectional neural network with gated recurrent units (Cho et al., 2014) that reads an input sequence x = (x1, .", "startOffset": 73, "endOffset": 91}, {"referenceID": 0, "context": "A detailed description can be found in (Bahdanau et al., 2014).", "startOffset": 39, "endOffset": 62}, {"referenceID": 23, "context": "Cognates and loanwords with a common origin can differ in regular ways between languages, so that character-level translation rules are sufficient (Tiedemann, 2012).", "startOffset": 147, "endOffset": 164}, {"referenceID": 20, "context": "The segmentation of morphologically complex words such as compounds is widely used, and various algorithm for morpheme segmentation have been investigated (Nie\u00dfen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007).", "startOffset": 155, "endOffset": 224}, {"referenceID": 13, "context": "The segmentation of morphologically complex words such as compounds is widely used, and various algorithm for morpheme segmentation have been investigated (Nie\u00dfen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007).", "startOffset": 155, "endOffset": 224}, {"referenceID": 24, "context": "The segmentation of morphologically complex words such as compounds is widely used, and various algorithm for morpheme segmentation have been investigated (Nie\u00dfen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007).", "startOffset": 155, "endOffset": 224}, {"referenceID": 6, "context": "If alphabets differ, transliteration is required (Durrani et al., 2014).", "startOffset": 49, "endOffset": 71}, {"referenceID": 23, "context": "Character-based translation has been successfully used for closely related languages (Tiedemann, 2012).", "startOffset": 85, "endOffset": 102}, {"referenceID": 15, "context": "We test a slightly more sophisticated syllable segmentation developed for word hyphenation (Liang, 1983).", "startOffset": 91, "endOffset": 104}, {"referenceID": 1, "context": "The best choice of subword units may be task-specific, and for speech recognition, phone-level language models have been used (Bazzi and Glass, 2000).", "startOffset": 126, "endOffset": 149}, {"referenceID": 17, "context": "Various techniques have been proposed to produce fixed-length continuous word vectors based on characters or morphemes (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015).", "startOffset": 119, "endOffset": 183}, {"referenceID": 2, "context": "Various techniques have been proposed to produce fixed-length continuous word vectors based on characters or morphemes (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015).", "startOffset": 119, "endOffset": 183}, {"referenceID": 16, "context": "Various techniques have been proposed to produce fixed-length continuous word vectors based on characters or morphemes (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015).", "startOffset": 119, "endOffset": 183}, {"referenceID": 8, "context": "Byte Pair Encoding (BPE) (Gage, 1994) is a simple data compression technique that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte.", "startOffset": 25, "endOffset": 37}, {"referenceID": 3, "context": "coding of words for NMT (Chitnis and DeNero, 2015), is that our symbol sequences are still in-", "startOffset": 24, "endOffset": 50}, {"referenceID": 0, "context": "We use Groundhog3 as the implementation of the NMT system for all experiments (Bahdanau et al., 2014).", "startOffset": 78, "endOffset": 101}, {"referenceID": 0, "context": "We generally follow settings by previous work (Bahdanau et al., 2014; Jean et al., 2015a).", "startOffset": 46, "endOffset": 89}, {"referenceID": 10, "context": "We generally follow settings by previous work (Bahdanau et al., 2014; Jean et al., 2015a).", "startOffset": 46, "endOffset": 89}, {"referenceID": 0, "context": "We generally follow settings by previous work (Bahdanau et al., 2014; Jean et al., 2015a). All networks have a hidden layer size of 1000, and an embedding layer size of 620. Following Jean et al. (2015a), we only keep a shortlist of \u03c4 = 30000 words in memory.", "startOffset": 47, "endOffset": 204}, {"referenceID": 25, "context": "During training, we use Adadelta (Zeiler, 2012), a minibatch size of 80, and reshuffle the training set between epochs.", "startOffset": 33, "endOffset": 47}, {"referenceID": 10, "context": "We train a network for approximately 7 days, then take the last 4 saved models (models being saved every 12 hours), and continue training each with a fixed embedding layer (as suggested by (Jean et al., 2015a)) for 12 hours.", "startOffset": 189, "endOffset": 209}, {"referenceID": 7, "context": "We use a bilingual dictionary based on fast-align (Dyer et al., 2013).", "startOffset": 50, "endOffset": 69}, {"referenceID": 7, "context": "We use a bilingual dictionary based on fast-align (Dyer et al., 2013). For our baseline, this serves to speed up translation, since only a filtered list of candidate translations is stored in memory (like Jean et al. (2015a), we use K = 30000; K \u2032 = 10), and as back-off dictionary for rare words.", "startOffset": 51, "endOffset": 225}, {"referenceID": 13, "context": "5 Compound splitting (Koehn and Knight, 2003) is a popular technique in SMT.", "startOffset": 21, "endOffset": 45}, {"referenceID": 5, "context": "Morfessor (Creutz and Lagus, 2002) implements word segmentation based on minimum description length.", "startOffset": 10, "endOffset": 34}, {"referenceID": 19, "context": "Syllabification has been used to produce subword units for language modelling in (Mikolov et al., 2012), and we employ a word hyphenation algorithm (Liang, 1983).", "startOffset": 81, "endOffset": 103}, {"referenceID": 15, "context": ", 2012), and we employ a word hyphenation algorithm (Liang, 1983).", "startOffset": 52, "endOffset": 65}, {"referenceID": 0, "context": "The attention model of (Bahdanau et al., 2014) has complexity O(m \u00b7 n), m and n being the number of source and target symbols.", "startOffset": 23, "endOffset": 46}, {"referenceID": 13, "context": "\u25b3: (Koehn and Knight, 2003); *: (Creutz and Lagus, 2002); \u22c4: (Liang, 1983).", "startOffset": 3, "endOffset": 27}, {"referenceID": 5, "context": "\u25b3: (Koehn and Knight, 2003); *: (Creutz and Lagus, 2002); \u22c4: (Liang, 1983).", "startOffset": 32, "endOffset": 56}, {"referenceID": 15, "context": "\u25b3: (Koehn and Knight, 2003); *: (Creutz and Lagus, 2002); \u22c4: (Liang, 1983).", "startOffset": 61, "endOffset": 74}, {"referenceID": 10, "context": "7 We attempted to reproduce the results by Jean et al. (2015a), but our baseline is slightly worse8; We speculate that the difference is caused by differences in preprocessing and cleanup, training time, different hyperparameters, and/or ran-", "startOffset": 43, "endOffset": 63}, {"referenceID": 10, "context": "4 BLEU reported by (Jean et al., 2015a).", "startOffset": 19, "endOffset": 39}, {"referenceID": 21, "context": "syntax-based (Sennrich and Haddow, 2015) 22.", "startOffset": 13, "endOffset": 40}, {"referenceID": 19, "context": "The performance of our NMT systems is below our syntax-based baseline by Sennrich and Haddow (2015). We note that Jean et al.", "startOffset": 73, "endOffset": 100}, {"referenceID": 10, "context": "We note that Jean et al. (2015b) report a score of 24.", "startOffset": 13, "endOffset": 33}, {"referenceID": 9, "context": "The quality of the WDict baseline is substantially below the phrase-based state-of-the-art: the English\u2192Russian system by Haddow et al. (2015) outperforms WDict by 4.", "startOffset": 122, "endOffset": 143}, {"referenceID": 9, "context": "phrase-based (Haddow et al., 2015) 29.", "startOffset": 13, "endOffset": 34}], "year": 2015, "abstractText": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, so the translation of rare and unknown words is an open problem. Previous work addresses this problem through back-off dictionaries. In this paper, we introduce a simpler and more effective approach, enabling the translation of rare and unknown words by encoding them as sequences of subword units, based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English\u2192German and English\u2192Russian by 0.8 and 1.5 BLEU, respectively.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}