{"id": "1509.01817", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2015", "title": "On collapsed representation of hierarchical Completely Random Measures", "abstract": "The main aim of this paper is to establish the applicability of a broad class of random measures, that includes the gamma process, for mixed membership modelling. We use completely random measures~(CRM) and hierarchical CRM to define a prior for Poisson processes. We derive the marginal distribution of the resultant point process, when the underlying CRM is marginalized out of the original (b) (and the derived points are distributed among the same points in order to maximize distribution, and then the derived points (or the associated points, with different logrithm) are distributed among other points.\n\n\n\n\n\n\n\nAs the point process is only a finite, finite number of points to be generated from the random process, it is possible to derive the expected distribution. We find that there is, indeed, a fundamental principle which is evident in the function of probability (e.g., the distribution of probability), that is required to generate random points:\nThe probability of a randomly generated random process is the probability of the random process.\nThe point process can be used as a way of estimating its probability, but the main aim is to quantify its probability (i.e., if the given probability is not enough, it must be more realistic or simpler to compute it.)\nThe main aim is to identify and test the validity of the statistical method for calculating the true probability.\nThe main goal of this paper is to develop the best-selling statistic used by computer scientists in computer science to evaluate the validity of the statistical method.\nA few years ago, we established a new statistical method for estimating the value of the random process:\nUsing a simple logrithm, we can use a large number of random factors, the random number, to determine the value of the random process.\nThe first important aspect of this paper is that, as an algorithm, the number of random factors is not random, as is the distribution of the probabilities of these random factors.\nWhen it comes to estimating the value of random processes, this is precisely what a regular random procedure is.\nTo derive the probability of a randomly generated random process, we can generate the total random process at an arbitrary time.\nIn this work, we define the number of random factors. The number of random factors is the average of random factors on a random sample of randomly generated random factors, the rate of random effects, the percentage of random effects, the number of random effects on the total random system, and the random system. We have found", "histories": [["v1", "Sun, 6 Sep 2015 14:44:38 GMT  (58kb,D)", "https://arxiv.org/abs/1509.01817v1", "14 pages, 1 figure"], ["v2", "Thu, 2 Jun 2016 06:46:28 GMT  (69kb,D)", "http://arxiv.org/abs/1509.01817v2", "11 pages, 1 figure"]], "COMMENTS": "14 pages, 1 figure", "reviews": [], "SUBJECTS": "math.ST cs.LG stat.TH", "authors": ["gaurav pandey", "ambedkar dukkipati"], "accepted": true, "id": "1509.01817"}, "pdf": {"name": "1509.01817.pdf", "metadata": {"source": "META", "title": "On collapsed representation of hierarchical Completely Random Measures", "authors": ["Gaurav Pandey", "Ambedkar Dukkipati"], "emails": ["GP88@CSA.IISC.ERNET.IN", "AD@CSA.IISC.ERNET.IN"], "sections": [{"heading": "1. Introduction", "text": "Mixed membership modelling is the problem of assigning an object to multiple latent classes/features simultaneously. Depending upon the problem, one can allow a single latent feature to be exhibited single or multiple times by the object. For instance, a document may comprise several topics, with each topic occurring in the document with variable multiplicity. The corresponding problem of mapping the words of a document to topics, is referred to as topic modelling.\nWhile parametric solutions to mixed membership mod-\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nelling have been available in literature since more than a decade (Landauer & Dumais, 1997; Hofmann, 1999; Blei et al., 2001), the first non-parametric approach, that allowed the number of latent classes to be determined as well, was the hierarchical Dirichlet process (HDP) (Teh et al., 2006). Both the approaches model the object as a set of repeated draws from an object-specific distribution, whereby the object specific distribution is itself sampled from a common distribution. On the other hand, recent approaches such as hierarchical beta-negative binomial process (Zhou et al., 2012; Broderick et al., 2015) and hierarchical gamma-Poisson process (Titsias, 2008; Zhou & Carin, 2015) model the object as a point process, sampled from an object specific random measure, which is itself sampled from a common random measure. In some sense, these approaches are more natural for mixed membership modelling, since they model the object as a single entity rather than as a sequence of draws from a distribution.\nA straightforward implementation of any of the above nonparametric models would require sampling the atoms in the non-parametric distribution for the base as well as objectspecific measure. However, since the number of atoms in these distributions are often infinite, a truncation step is required to ensure tractability. Alternatively, for the HDP, a Chinese restaurant franchise scheme (Teh et al., 2006) can be used for collapsed inference in the model (that is, without explicitly instantiating the atoms). Fully collapsed inference scheme has also been proposed for beta-negative binomial process (BNBP) (Heaukulani & Roy, 2013; Zhou, 2014) and Gamma-Gamma-Poisson process (Zhou et al., 2015). Of particular relevance is the work by Roy (2014), whereby a Chinese restaurant fanchise scheme has been proposed for hierarchies of beta proceses (and its generalizations), when coupled with Bernoulli process.\nIn this paper, it is our aim to extend fully collapsed sampling so as to allow any completely random measure (CRM) for the choice of base and object-specific measure. As proposed in Roy (2014) for hierarchies of generalized beta processes, we propose Chinese restaurant franchise schemes for hierarchies of CRMs, when coupled with Pois-\nar X\niv :1\n50 9.\n01 81\n7v 2\n[ m\nat h.\nST ]\n2 J\nun 2\n01 6\nson process. We hope that this will encourage the use of hierarchical random measures, other than HDP and BNBP, for mixed-membership modelling and will lead to further research into an understanding of the applicability of the various random measures. To give an idea about the flexibility that can be obtained by using other measures, we propose the sum of generalized gamma process (SGGP), which allows one to determine the power term in the powerlaw distribution of topics with documents, by defining a prior on the parameters of SGGP. Alternatively, one can also define a prior directly on the discount parameter.\nThe main contributions in this paper are as follows:\n\u2022 We derive marginal distributions of Poisson process, when coupled with CRMs,\n\u2022 We provide an exact approach for generating a Poisson process sampled from a hierarchical CRM, without having to instantiate the infinitely many atoms of the random measure.\n\u2022 We provide a Gibbs sampling approach for sampling a Poisson process from a hierarchical CRM.\n\u2022 In the experiments section, we propose the sum of generalized gamma process (SGGP), and show its applicability for topic-modelling. By defining a prior on the parameters of SGGP, one can determine the power-law distribution of the topics and words in a Bayesian fashion."}, {"heading": "2. Preliminaries and background", "text": "In this section, we fix the notation and recall a few well known results from the theory of point processes."}, {"heading": "2.1. Poisson process", "text": "Let (S,S) be a measurable space and \u03a0 be a random countable collection of points on S. LetN(A) = |\u03a0\u2229A|, for any measurable set A. N is also known as the counting process of \u03a0. \u03a0 is called a Poisson process if N(A) is independent of N(B), whenever A and B are disjoint measurable sets, and N(A) is Poisson distributed with mean \u00b5(A) for a fixed \u03c3-finite measure \u00b5. In sequel, we refer to both the random collection \u03a0 and its counting processN as Poisson process.\nLet (T, T ) be another measurable space and f : S \u2192 T be a measurable function. If the push forward measure of \u00b5 via f , that is, \u00b5 \u25e6 f\u22121 is non-atomic, then f(\u03a0) = {f(x) : x \u2208 \u03a0} is also a Poisson process with mean measure \u00b5 \u25e6 f\u22121. This is also known as the mapping proposition for Poisson processes (Kingman, 1992). Moreover, if \u03a01,\u03a02, . . . is a countable collection of independent Poisson processes with mean measures \u00b51, \u00b52, . . . respectively,\nthen the union \u03a0 = \u222a\u221ei=1\u03a0i is also a Poisson process with mean measure \u00b5 = \u2211\u221e i=1 \u00b5i. This is known as the superposition proposition. Equivalently, if Ni is the counting process of \u03a0i, then N = \u2211\u221e i=1Ni is the counting process\nof a Poisson process with mean measure \u00b5 = \u2211\u221e i=1 \u00b5i.\nFinally, let g be a measurable function from S to R, and \u03a3 = \u2211 x\u2208\u03a0 g(x). By Campbell\u2019s proposition (Kingman, 1992), \u03a3 is absolutely convergent with probability, if and only if \u222b\nS\nmin(|g(x)|, 1)\u00b5(dx ) <\u221e. (1)\nIf this condition holds, then for any t > 0, E[e\u2212t\u03a3] = exp { \u2212 \u222b S (1\u2212 e\u2212tg(x))\u00b5(dx ) } . (2)"}, {"heading": "2.2. Completely random measures", "text": "Let (\u2126,F ,P) be some probability space. Let (M(S),B) be the space of all \u03c3-finite measures on (S,S) supplied with an appropriate \u03c3-algebra. A completely random measure (CRM) \u039b on (S,S), is a measurable mapping from \u2126 to M(S) such that\n1. P{\u039b(\u2205) = 0} = 1,\n2. For any disjoint countable collection of sets A1, A2, . . . , the random variables \u039b(Ai), i = 1, 2, . . . are independent, and \u039b(\u222aAi) = \u2211 i \u039b(Ai), holds\nalmost surely. (the independent increments property)\nAn important characterization of CRMs in terms of Poisson processes is as follows (Kingman, 1967). For any CRM \u039b on (S,S) without any fixed atoms or deterministic component, there exists a Poisson process N on (R+ \u00d7 S,BR+ \u2297 S), such that \u039b(dx ) = \u222b R+ zN(dz ,dx ). Using Campbell\u2019s proposition, the Laplace transform of \u039b(A) for a measurable set A, is given by the following formula:\nE[e\u2212t\u039b(A)] = exp ( \u2212 \u222b R+\u00d7A (1\u2212 e\u2212tz)\u03bd(dz,dx) ) , t \u2265 0, (3) where \u03bd denotes the mean measure of the underlying Poisson processN . \u03bd is also referred to as the Poisson intensity measure of \u039b. If \u03bd(dz,dx) = \u03c1(dz)\u00b5(dx), for a \u03c3-finite measure \u00b5 on S, and a \u03c3-finite measure \u03c1 on R+ that satisfies \u222b R+(1 \u2212 e\n\u2212tz)\u03c1(dz ) < \u221e, then \u039b(.) is known as homogenous CRM. In sequel, we assume \u00b5(.) to be finite. Moreover, unless specified, whenever we refer to CRM, it means a homogeneous completely random measure without any fixed atoms or deterministic component.\nLet N be the Poisson process of the CRM \u039b, that is, \u039b(dx ) = \u222b R+ sN(dz ,dx ). If \u03a0 is the random collection of points corresponding to N , then \u039b can equivalently\nbe written as \u039b = \u2211\n(z,x)\u2208\u03a0 z\u03b4x. {z : (z, x) \u2208 \u03a0} constitute the weights of the CRM \u039b. By the mapping proposition for Poisson processes, they form a Poisson process with mean measure \u00b5\u2217(dz ) = \u00b5 \u25e6 f\u22121(dz ), where f(x, y) = x is the projection map on R+. Hence, the weights of \u039b form a Poisson process on R+ with mean measure \u00b5\u2217(dz ) = \u03bd(dz , S) = \u03c1(dz )\u00b5(S). We formally state this result below.\nLemma 2.1. The weights of a homogenous CRM with no atoms or deterministic component, whose Poisson intensity measure \u03bd(dz ,dx ) = \u03c1(dz )\u00b5(dx ) form a Poisson process with mean measure \u03c1(dz )\u00b5(S).\nNote 1: A completely random measure without any fixed atoms or deterministic component is a purely-atomic random measure.\nNote 2: Every such homogeneous CRM \u039b on (S,S) has an underlying Poisson process N on (R+ \u00d7 S,BR+ \u2297 S), such that\n\u039b(dx ) = \u222b R+ zN(dz ,dx ) (4)\nalmost surely."}, {"heading": "3. The proposed model", "text": "Let X1, . . . , Xn be n observed samples, for instance, n documents. We assume that each sample Xi is generated as follows:\n\u2022 The base measure \u03a6 is CRM(\u03c1, \u00b5), where \u03c1 and \u00b5 are \u03c3-finite and finite (non-atomic) measures on (S,S) respectively.\n\u2022 Object specific measures \u039bi, 1 \u2264 i \u2264 n are CRM(\u03c1\u0304,\u03a6), where \u03c1\u0304 is another \u03c3-finite non-atomic measure on (S,S).\n\u2022 The latent feature set Ni for each object Xi is a Poisson process with mean measure \u039bi.\n\u2022 Finally, the visible features Xi are sampled from Ni.\nNote: For topic modelling, S corresponds to the space of all probability measures on the words in the dictionary, also known as topics. Hence, when we sample \u03a6, we sample a subset of topics, along with the weights for those topics. This follows from the discreteness of \u03a6. Sampling objectspecific random measures \u039bi corresponds to sampling the document specific weights for all the topics in \u03a6. Sampling the latent featuresNi then corresponds to selecting a subset of topics from \u039bi based on the corresponding documentspecific weights. Since, all the \u039b\u2032is have access to the same set of topics, this leads to sharing of topics among Nis. Finally, the words inXi is sampled from the corresponding topic in Ni using categorical distribution.\nOur aim is to infer the latent features Ni, 1 \u2264 i \u2264 n from Xi, 1 \u2264 i \u2264 n. By Bayes\u2019 rule\nP (N1, . . . , Nn|X1, . . . , Xn) \u221d P (X1, . . . , Xn|N1, . . . , Nn)P (N1, . . . , Nn) = \u03a0ni=1P (Xi|Ni)P (N1, . . . , Nn)\nThe conditional distribution of Xi given Ni are often very simple to compute, for instance, in the case of topic modelling, it is simply the product of categorical distributions. Hence, all we need to compute is the prior distribution of the latent features N1, . . . , Nn. This can be obtained by marginalizing out the base and object-specific random measures \u03a6 and \u039bi, 1 \u2264 i \u2264 n. This is what we wish to achieve in the next few sections.\nWe will address the problem of marginalizing out the base and object-specific random measures in two steps. Firstly, in section 3.1, we will derive results for the case when the base measure is held fixed and the object-specific random measure is marginalized out. Next, in section 3.2, we will derive results for the case, when the base random measure \u03a6 is also marginalized out. All the proofs are provided in the appendix."}, {"heading": "3.1. Marginalizing out the object specific measure", "text": "Let \u03c6 be a realization of the base random measure \u03a6. Let \u039bi, 1 \u2264 i \u2264 n, be independent CRM(\u03c1\u0304, \u03c6). It is straightforward to see that if \u03c6 is a finite measure \u039bis will almostsurely be finite. Because of the independence among \u039bis, we can focus on marginalizing out a single object-specific random measure, say \u039b. Although, in our original formulation, only 1 object is sampled from its object-specific random measure, we will present results for the case when n objects, N1, . . . , Nn are sampled from the object specific random measure. This extended result will be needed in the next section when marginalizing the base measure.\nThere are several ways to instantiate the random measure \u039b. For instance, one can use the fact that since the underlying base measure \u03c6 is purely-atomic, the support of CRM(\u03c1\u0304, \u03c6) will be restricted to only those measures whose support is a subset of the support of \u03c6. In particular, if \u03c6 = \u2211\u221e j=1 \u03b2j\u03b4xj , then \u039b will be of the form \u2211\u221e j=1 Lj\u03b4xj , where Lj are independent random variables. The independence of Ljs follows from the complete randomness of the measure.\nHowever, we found that this approach doesn\u2019t lead us far. Hence, we derive the marginal distribution of the Poisson processes N1, . . . , Nn in proposition 3.1 and 3.2, by first assuming \u03c6 to be a continuous measure and then generalizing it to the case where \u03c6 is any finite measure.\nIn the sequel, \u03c8(t) = \u222b R+(1\u2212 e\n\u2212tz)\u03c1\u0304(dz ), and \u03c8(k) is the kth derivative of \u03c8.\nProposition 3.1. Let \u039b be a CRM on (S,S) with Poisson intensity measure \u03c1(dz )\u00b5(dx ), where both \u00b5(.) and \u03c1(.) are non-atomic. Let N1, . . . , Nn be n independent Poisson process with random mean measure \u039b, and M be the distinct points of Ni, 1 \u2264 i \u2264 n. Then, M is a Poisson process with mean measure E[M(dx )] = \u00b5(dx ) \u222b R+(1 \u2212 e\u2212nz)\u03c1(dz ).\nThe above proposition provides the distribution of distinct points of the n point processes, N1, . . . , Nn. In order to complete the description of the distribution of N1, . . . , Nn, we also need to specify the joint distribution of the counts of each distinct feature in each Ni. This distribution is referred to as CRM-Poisson distribution in the rest of the paper. Let M(S) = k and mij be the count of the jth distinct feature in the ith object. Furthermore, let [m\u00b7j ] be the count of the jth distinct feature for each object and [mij ]1\u2264i\u2264n,1\u2264j\u2264k be the set of count vectors for the each latent feature.\nProposition 3.2. The joint distribution of the set of count vectors for the each latent feature [mij ](n,k) is given by\nP ([mij ](n,k)) = (\u22121) m\u00b7\u00b7\u2212k \u03b8ke\u2212\u03b8\u03c8(n)\u220fn i=1(mi\u00b7)! k\u220f j=1 \u03c8 (m\u00b7j)(n) , (5) where mi\u00b7 = \u2211k j=1mij , m\u00b7j = \u2211n i=1mij , m\u00b7\u00b7 =\u2211n\ni=1 \u2211k j=1mij , \u03b8 = \u00b5(S) and \u03c8(t) = \u222b R+(1 \u2212 e\u2212tz)\u03c1(dz ) is the Laplace exponent of \u039b, and \u03c8(l)(t) is the lth derivative of \u03c8(t). This distribution will be referred to as CRM-Poisson(\u00b5(S), \u03c1, n).\nCorollary 3.3. Conditioned on M(S) = k, the set of count vectors for the each latent feature [mij ](n,k) is distributed as\nP ([mij ](n,k)|M(S) = k) (6)\n= \u03b8k(\u22121)m\u00b7\u00b7\u2212kk!\u220fn i=1(mi\u00b7)! k\u220f j=1 \u03c8 (mi\u00b7)(n) \u03c8(n)\nNote that both \u03c8(k) and \u03c8 contain a multiple involving \u00b5(S), which cancels out when they are divided in (6). Hence, conditioned on the number of points in the Poisson process M , the distribution of the set of counts for each latent feature [mij ](n,k) does not depend on the measure \u00b5. In sequel, this distribution will be referred to as conditional CRM-Poisson(\u03c1, n, k) or CCRM-Poisson(\u03c1, n, k).\nExample 1: The Gamma-Poisson process The Poisson-intensity measure of gamma process is given by \u03c1(dz ) = e\u2212zz\u22121 dz . The corresponding Laplace exponent is \u03c8(t) = ln(1 + t). Replacing it in equation (5), we\nget\nP ([mij ](n,k)) = \u03b8k \u220fk j=1 \u0393(m\u00b7j)\u220fn\ni=1mi\u00b7!(1 + n)m\u00b7\u00b7+\u03b8 (7)\nNext, we generalize these results for the case when \u03c6 is an atomic measure.\nProposition 3.4. Let \u039b be a completely random measure with Poisson intensity measure \u03bd(dz ,dx ) = \u03c6(dx )\u03c1\u0304(dz ), where \u03c1\u0304 is non-atomic. Let N be a Poisson process with mean measure \u039b. Then, N can be obtained by sampling a Poisson process with mean measure \u03c6(dx )\u03c8(1), say M , and then sampling the count of each feature inM using the conditional CRM-Poisson distribution.\nNote: The points in M won\u2019t be distinct anymore, since the underlying mean measure is non-atomic."}, {"heading": "3.2. Marginalizing out the base measure", "text": "The previous section derived the marginal distribution of the Poisson processes, for a fixed realization \u03c6 of the base random measure \u03a6. In this section, we want to marginalize the CRM \u03a6 as well. Marginalizing \u03a6 does away with the independence among the latent features Nis, hence, we need to model the joint distribution of N1, . . . , Nn.\nThe model under study is\n\u03a6 \u223c CRM(\u03c1, \u00b5) , \u039bi|\u03a6 \u223c CRM(\u03c1\u2032,\u03a6), 1 \u2264 i \u2264 n , Ni|\u039bi \u223c Poisson Process(\u039bi), 1 \u2264 i \u2264 n .\n(8)\nWe use Proposition 3.4 to marginalize out \u039bi from the above description. Thus Ni can equivalently be obtained by sampling a Poisson processes with mean measure \u03a6(dx ) \u222b R+(1\u2212 e\n\u2212z)\u03c1(dz ), and then sampling the count of each feature in Mi for each point process Ni using Corollary 3.3. In particular, let Mi be the corresponding Poisson process, and mij be the count of the jth feature in Mi for the point process Ni and ri\u00b7 = Mi(S). The reason for the symbol ri\u00b7 will become clear, when we have a picture of the entire generative model. Let [mij ]\u00b7,ri\u00b7 be the set of counts of the latent features for the ith individual. The distribution of the set of counts [mij ]\u00b7,ri\u00b7 conditioned on Mi(S) does not depend on \u03a6. Hence, an alternative description of the Ni via Mi and mij , 1 \u2264 j \u2264 ri\u00b7 is as\nfollows: Mi|\u03a6 \u223c Poisson Process ( \u03a6(.) \u222b R+ (1\u2212 e\u2212z)\u03c1\u0304(dz ) ) ,\n[mij ](\u00b7,ri\u00b7)|{Mi(S) = ri\u00b7} \u223c CCRM-Poisson(\u03c1\u0304, 1, ri\u00b7)\nNi = ri\u00b7\u2211 i=1 mij\u03b4Mij ,\n(9)\nwhere Mij are the points in the point process Mi.\nMi, 1 \u2264 i \u2264 n are independent Poisson processes, whose mean measure is a scaled CRM, and hence, also a CRM. Hence, we are again in the domain of CRM-Poisson models. Let \u03c8\u0304(1) = \u222b R+(1 \u2212 e\n\u2212z)\u03c1\u0304(dz ). If we define \u03a6\u2032(dx ) = \u03c8\u0304(1)\u03a6(dx ), then\nE[e\u2212t\u03a6 \u2032(A)] = E[e\u2212t\u03c8\u0304(1)\u03a6(A)]\n= exp { \u2212\u00b5(A) \u222b R+ (1\u2212 e\u2212t\u03c8\u0304(1)z)\u03c1(dz) }\n= exp { \u2212\u00b5(A) \u222b R+ (1\u2212 e\u2212tz \u2032 )\u03c1(d(z\u2032/\u03c8\u0304(1))) } Hence, the Poisson intensity measure of the scaled CRM \u03a6\u2032 is given by \u03c1(d(z/\u03c8\u0304(1)))\u00b5(dx ). Applying Proposition 3.4 to marginalize out \u03a6, we get that Mi\u2019s can be obtained by sampling a Poisson process R with mean measure\nE[R(dx )] = \u00b5(dx ) \u222b R+ (1\u2212 e\u2212nz \u2032 )\u03c1(d(z\u2032/\u03c8\u0304(1)))\n= \u00b5(dx ) \u222b R+ (1\u2212 e\u2212\u03c8\u0304(1)nz)\u03c1(dz) .\nThe count of each feature in R for each point process Mi can then be obtained by using Corollary 3.3. In particular, let rik be the count of the kth point in R for the point process Mi and p = R(S).\nA complete generative model for generating the point processes Ni, 1 \u2264 i \u2264 n is as follows:\nR \u223c Poisson Process ( \u00b5(.) \u222b R+ (1\u2212 e\u2212\u03c8\u0304(1)nz)\u03c1(dz ) ) , [rik](n,p)| {R(S) = p} \u223c CCRM-Poisson(\u03c1, \u03c8\u0304(1)n, p) (10)\nMi = p\u2211 k=1 rik\u03b4Rk\n[mij ](\u00b7,ri\u00b7)|{Mi(S) = ri\u00b7} \u223c CCRM-Poisson(\u03c1\u0304, 1, ri\u00b7)\nNi = ri\u00b7\u2211 j=1 mij\u03b4Mij ,\nSince R is again a Poisson process, it is straightforward to extend this hierarchy further by sampling \u00b5(.) again from a CRM."}, {"heading": "4. Implementation via Gibbs sampling", "text": "Section 3 provided an approach for sampling a Poisson process, when sampled from a hierarchical CRM, without having to instantiate the infinitely many atoms of the base or object-specific CRM. However, it is not clear how the above derivations can be used for determining the latent features N1, . . . , Nn for the objects X1, . . . , Xn, which is the aim of this work.\nIn this section, we provide a Gibbs sampling approach for sampling the latent features from its prior distribution that is P (N1, . . . , Nn). In order to sample from the posterior, one simply needs to multiply the equations in this section with the likelihood of the latent feature. In order to be able to perform MCMC sampling in hierarchical CRM-Poisson models, we need to marginalize out R(S) and Mi(S) from distributions of [rik](n,p) and [mij ](\u00b7,ri\u00b7) respectively. By marginalizing out the Poisson distributed random variable R(S) from (10), we get that\n[rik](n,p) \u223c CRM-Poisson(\u00b5(S), \u03c1, \u03c8\u0304(1)n) .\nThe marginal distribution of the set of counts of each latent feature for the ith individual [mij ](\u00b7,ri\u00b7) (where ri\u00b7 is also random) is given by the following lemma.\nLemma 4.1. Let h(u) = E[e\u2212u\u03c8(S)] = exp { \u2212\u00b5(S) \u222b R+ (1\u2212 e\u2212uz)\u03c1(dz ) } .\nFurthermore, if we let\n\u03c8(u) = \u222b R+ (1\u2212 e\u2212uz)\u03c1(dz )\n\u03c8\u0304(u) = \u222b R+ (1\u2212 e\u2212uz)\u03c1\u0304(dz ) ,\nthen, [mij ](\u00b7,ri\u00b7) is marginally distributed as\nP ([mij ](\u00b7,ri\u00b7)) = (\u22121) mi\u00b7h(ri\u00b7) (\u03c8\u0304(1)) \u220fri\u00b7j=1 \u03c8\u0304(mij)(1)\nmi\u00b7! ,\n(11)\nIn the case of topic-modelling, the number of latent features, #Ni is equal to the number of observed features #Xi. Hence, let Xil be the lth observed feature associated with the ith object and Nil be the corresponding latent feature. Here, we discuss the MCMC approach for sampling from the prior distribution of Nil, 1 \u2264 l \u2264 mi\u00b7. As discussed in (Neal, 2000), it is more efficient to sample the index of the latent feature, rather than the latent feature itself. Hence, let Til be the index of the point in Mi associated with Nil, and Dij be the index of the point in R associated with Mij . In an analny with the Chinese restaurant franchise model (Teh et al., 2006), one can think of Til\nto be the index of the table assigned to the lth customer in the ith restaurant, and Dij to be the index of the dish associated with the jth table in ith restaurant. Moreover, mij refers to the number of customers sitting on the jth table in ith restaurant, and rik refers to the number of tables in the ith restaurant with the kth dish. Hence ri\u00b7 = \u2211p k=1 rik is the number of tables in the ith restaurant.\nThe distribution of the number of customers per table in the ith restaurant, [mij ](\u00b7,ri\u00b7) follows from Lemma 4.1. Hence, in order to sample the table of lth customer, Til, given the indices of the tables of all the other customers in ith restaurant, we treat it as the table corresponding to the last customer of the ith restaurant. Let m\u2212(il)i\u2032j be the number of customers sitting on the jth table in the i\u2032th restaurant, excluding the lth customer. The probability that the lth customer in the ith restaurant occupies the jth table is proportional to P (m\u2212(il)ij\u2032 + 1j\u2032=j , 1 \u2264 j\u2032 \u2264 ri\u00b7) as given in (11). We divide the expression by P (m\u2212(il)ij\u2032 , 1 \u2264 j\u2032 \u2264 ri\u00b7) to get a simpler form for the unnormalized probability distribution. Hence, the probability of assigning an existing table with index j is given by\nP (Til = j|T\u2212(il)) \u221d \u2212 \u03c8\u0304(m\n\u2212(il) ij +1)(1)\n\u03c8\u0304(m \u2212(il) ij )(1)\n, (12)\nand the probability of sampling a new table for the customer is given by\nP (Til = ri\u00b7 + 1|T\u2212(il)) \u221d= \u2212 h (ri\u00b7+1)(\u03c8\u0304(1)) h (ri\u00b7)(\u03c8\u0304(1)) \u03c8\u0304 (1)(1) ,\n(13) where \u03c8\u0304(t) = \u222b R+(1 \u2212 e \u2212tz)\u03c1\u0304(dz ) and h(k) is the kth derivative of h.\nMoreover, whenever a new table is sampled for a customer, a dish is sampled for the table from the distribution on tables per dish. By the discussion in the beginning of this section, the number of tables per dish [rik](n,p) follow a CRM-Poisson(\u00b5(S), \u03c1, \u03c8\u0304(1)n) distribution. Hence, in order to sample the dish at jth table, Dij , given the indices of the dishes at all the other tables, we treat it as the dish corresponding to the last table of the last restaurant. Let r \u2212(ij) \u00b7k be the total number of tables served with the kth dish, excluding the jth table of ith restaurant. The probability that the kth dish is served at the jth table in the ith restaurant is proportional to P (r\u2212(ij)i\u2032k\u2032 + 1i\u2032=i\u2032,k\u2032=k, 1 \u2264 i\u2032 \u2264 n, 1 \u2264 k\u2032 \u2264 p) as given in (11). We divide the expression by P (r\u2212(ij)i\u2032k\u2032 , 1 \u2264 i\u2032 \u2264 n, 1 \u2264 k\u2032 \u2264 p) to get a simpler form for the unnormalized probability distribution. Hence, the probability of serving an existing dish with index k is\ngiven by\nP (Dij = k|D\u2212(ij)) \u221d \u2212 \u03c8\n(r \u2212(ij)\u00b7k +1)(\u03c8\u0304(1)n) \u03c8 (r \u2212(ij)\u00b7k )(\u03c8\u0304(1)n) , (14)\nand the probability of sampling a new dish for the table is given by\nP (Dij = p+ 1|D\u2212(ij)) \u221d \u03b8\u03c8(1)(\u03c8\u0304(1)n) , (15) where \u03c8(t) = \u222b R+(1\u2212 e \u2212tz)\u03c1(dz ) and \u03b8 = \u00b5(S).\nHence, a complete description of one iteration of MCMC sampling, from the prior distribution, in hierarchical CRMPoisson models is as follows:\n1. For each customer in each restaurant, sample his table index conditioned on the indices of table of other customers, according to equations (12) and (13).\n2. If the table selected is a new table, sample the index of dish corresponding to that table from equations (14) and (15).\n3. Sample the index of dish for each table, conditioned on the indices of dishes at the other tables, according to equations (14) and (15).\nExample 2: The Gamma-Gamma-Poisson process We compute the dish and table sampling probabilities for the Gamma-Gamma-Poisson process using the above equations. The Poisson intensity measure for both the base and object specific measures \u03a6 and \u039bi, 1 \u2264 i \u2264 n is z\u22121e\u2212z dz . The corresponding Laplace exponent is given by \u03c8(t) = \u03c8\u0304(t) = ln(1 + t). Moreover, let the mean measure for the base measure \u03a6 be \u00b5(.) and \u00b5(S) = \u03b8. Then, h(u) = Ee\u2212u\u03a6(S) = 1\n(1+u)\u03b8 . The corresponding deriva-\ntives are given by\n\u03c8(k) = \u03c8\u0304(k)(t) = (\u22121)k\u22121\u0393(k)\n(1 + t)k (16)\nh(k)(u) = (\u22121)k\u0393(k + \u03b8) (1 + u)k+\u03b8\u0393(\u03b8)\n(17)\nThe corresponding dish sampling probabilities are given by\nP (Dij = k|D\u2212(ij)) \u221d r \u2212(ij) \u00b7k\n1 + n ln 2 (18)\nP (Dij = p+ 1|D\u2212(ij)) \u221d \u03b8\n1 + n ln 2 (19)\nfor an existing and new dish respectively. Normalizing these probabilities, we get\nP (Dij = k|D\u2212(ij)) = r \u2212(ij) \u00b7k\u2211p\nk=1 r\u00b7k + \u03b8 (20)\nP (Dij = p+ 1|D\u2212(ij)) = \u03b8\u2211p\nk=1 r\u00b7k + \u03b8 (21)\nThe table sampling probabilities are given by\nP (Til = j|T\u2212(il)) \u221d m \u2212(il) ij\n1 + ln(2) (22)\nP (Til = ri\u00b7 + 1|T\u2212(il)) \u221d \u03b8 + ri\u00b7\n(1 + ln(2))2 (23)\nfor an existing and new table respectively. Normalizing these probabilities, we get\nP (Til = j|T\u2212(il)) = m \u2212(il) ij\u2211ri\u00b7\nj=1m \u2212(il) ij + \u03b8+ri\u00b7 1+ln(2)\n(24)\nP (Til = ri\u00b7 + 1|T\u2212(il)) = (\u03b8 + ri\u00b7)/(1 + ln(2))\u2211ri\u00b7 j=1m \u2212(il) ij + \u03b8+ri\u00b7 1+ln(2)\n(25)\nExample 3: The Gamma-Generalized Gamma-Poisson process In this scenario, the base random measure has \u03c1(dz ) = e\u2212zz\u22121 dz , whereas the object specific measure has \u03c1\u0304(dz ) = e\u2212zz\u2212d\u22121 dz , where 0 < d < 1 is known as the discount parameter. The corresponding Laplace exponents are given by \u03c8(t) = ln(1 + t) and \u03c8\u0304(t) = (1+t)\nd\u22121 d\nrespectively. The derivative of \u03c8\u0304 is given by\n\u03c8\u0304(k)(t) = (\u22121)k\u22121\u0393(k \u2212 d)\n(1 + t)k\u2212d\u0393(1\u2212 d) (26)\n(27)\nOther derivatives remain same as in the previous example. Moreover, the dish sampling probabilities remain same. The table sampling probabilities are given by\nP (Til = j|T\u2212(il)) = m \u2212(il) ij \u2212 d\u2211ri\u00b7\nj=1(m \u2212(il) ij \u2212 d) + \u03b8+ri\u00b7 1+lnd(2)\n(28)\nP (Til = ri\u00b7 + 1|T\u2212(il)) = (\u03b8 + ri\u00b7)/(1 + lnd(2))\u2211ri\u00b7 j=1(m \u2212(il) ij \u2212 d) + \u03b8+ri\u00b7 1+lnd(2)\n(29)\nwhere lnd(2) = 2 d\u22121 d ."}, {"heading": "5. Experimental results", "text": "We use hierarchical CRM-Poisson models for learning topics from the NIPS corpus 1.\n1The dataset can be downloaded from http: //psiexp.ss.uci.edu/research/programs_data/ toolbox.htm"}, {"heading": "5.1. Evaluation", "text": "For evaluating the different models, we divide each document into a training section and a test section by independently sampling a boolean random variable for each word. The probability of sending the word to the training section is varied from 0.3 to 0.7. We run 2000 iterations of Gibbs sampling. The first 500 iterations are discarded, and every sample in every 5 iterations afterwards is used to update the document-specific distribution on topics and the topic specific distribution on words. In particular, letW be the number of words, K be the number of topics, (\u03b2dk)1\u2264k\u2264K be the document specific distribution on topics for the document d, and (\u03c4kw)1\u2264w\u2264W be the topic specific distribution on words for the kth topic. Then, the probability of observing a word w in document d is given by \u2211K k=1 \u03b2dk\u03c4kw. For the evaluation metric, we use perplexity, which is simply the inverse of the geometric mean of the probability of all the words in the test set."}, {"heading": "5.2. Varying the Common CRM", "text": "In our experiments, we fix the object specific random measure \u039bi in (8) to be the gamma process, with \u03c1\u0304(dz ) = e\u2212zz\u22121 dz . For the base CRM \u03a6, we consider two specific choices of random measures.\n\u2022 Generalized gamma process (GGP): The Poisson intensity measure of \u03a6 is given by \u03bd(dz ,dx ) = \u03c1(dz )\u00b5(dx ), where \u03c1(dz ) =\n\u03b8 \u0393(1\u2212d)e \u2212zz\u2212d\u22121 dz , 0 \u2264 d < 1, \u03b8 > 0 and \u00b5(S) = 1. The corresponding Laplace exponent is given by \u03b8((1 + t)d \u2212 1)/d.\n\u2022 Sum of Generalized gamma processes (SGGP): The Poisson intensity measure of the CRM is given by \u03bd(dz ,dx ) = \u03c1(dz )\u00b5(dx ), where\n\u03c1(dz ) = m\u2211 q=1 \u03b8q \u0393(1\u2212 dq) e\u2212zz\u2212dq\u22121 dz (30)\nand \u00b5(S) = 1. The corresponding Laplace exponent is given by\n\u03c8(t) = ( m\u2211 q=1 \u03b8q (1 + t)dq \u2212 1 dq ) . (31)\nFor the case of GGP, the value of the discount parameter d is chosen from the set {0, .1, .2, .3, .4}. Furthermore, a gamma prior with rate parameter 2 and shape parameter 4 is defined on \u03b8.\nNote: The generalized gamma process with discount parameter 0 corresponds to the Gamma process. Using a gamma process prior for the base and object-specific CRM\ncorresponds exactly to the hierarchical Dirichlet process with a gamma prior on the concentration parameter of the object specific Dirichlet process. We did not add comparison results with HDP separately, because the same perplexity is obtained in both the models.\nFor the case of SGGP, we consider m = 5, and d1 = 0, d2 = .1 . . . , d5 = .4. Furthermore, independent gamma priors with rate parameter 2 and shape parameter 4 are defined for each \u03b8q, 1 \u2264 q \u2264 5. The posterior of each parameter \u03b8q is sampled via uniform sampling. We use equations (12)-(15) to compute the dish sampling and table sampling probabilities. The probability of sampling an existing dish is given by\nP (Dij = k|D\u2212(ij))\n\u221d \u2211m q=1 \u03b8q \u0393(r \u2212(ij)\u00b7k +1\u2212dq) \u0393(1\u2212dq) (1 + \u03c8\u0304(1)n) dq\n\u2211m q=1 \u03b8q \u0393(r \u2212(ij)\u00b7k \u2212dq) \u0393(1\u2212dq) (1 + \u03c8\u0304(1)n) dq ,\nwhere \u03c8\u0304(1) = \u222b R+(1\u2212 e\n\u2212z)\u03c1\u0304(dz ) = ln(2). Similarly, the probability of a new dish is given by\nP (Dij = p+ 1|D\u2212(ij)) \u221d m\u2211 q=1 \u03b8q(1 + \u03c8\u0304(1)n) dq .\nThe table-sampling probabilities can be computed similarly. We approximated the Laplace transform of \u03a6(S) (h in (13)), by a weighted sum of exponential functions to simplify the computation of its derivatives. The perplexity for the hierarchical CRM-Poisson models as a function of training percentage is plotted in Figure 1. Note that Figure 1 doesn\u2019t necessarily imply that SGGM-based models will always outperform GGM based models as the results have been obtained by defining a specific gamma prior for each hyperparameter, as mentioned above."}, {"heading": "6. Conclusion", "text": "For years, hierarchical Dirichlet processes have been the standard tool for nonparametric topic modelling, since collapsed inference in HDP can be performed using the Chinese restaurant franchise scheme. In this paper, our aim was to show that collapsed Gibbs sampling can be extended to a much larger set of hierarchical random measures using the same Chinese restaurant franchise scheme, thereby opening doors for further research into the efficacy of various hierarchical priors. We hope that this will encourage a better understanding of applicability of various hierarchical CRM priors. Furthermore, the results of the paper can be used to prove results for hierarchical CRMs in other contexts, for instance, nonparametric hidden Markov models."}, {"heading": "Acknowledgement", "text": "Gaurav Pandey is supported by IBM PhD Fellowship for the academic year 2015-2016."}, {"heading": "Appendix", "text": ""}, {"heading": "Proof of proposition 3.1", "text": "Proof. Let N = \u2211n i=1Ni. Since, conditioned on \u039b, N1, . . . , Nn are independent Poisson process with mean measure \u039b, by the superposition proposition for Poisson processes, N is a Poisson process conditioned on \u039b with mean measure n\u039b. Since, E[et1N(A)+t2N(B)|\u039b] = E[et1N(A)|\u039b(A)]E[et2N(B)|\u039b(B)], and \u039b(A) and \u039b(B) are independent, hence, N(A) and N(B) are also independent and therefore, N is a CRM. Hence, N(dx ) =\u222b R+ sN\u0304(dz ,dx ), for a Poisson process N\u0304 on S \u00d7 R\n+. Moreover,\nE[e\u2212tN(A)] = E [ E[e\u2212tN(A)|\u039b] ] = E [ exp ( \u2212n\u039b(A)(1\u2212 e\u2212t)\n)] = exp ( \u2212\u00b5(A)\n\u222b R+ (1\u2212 e\u2212n(1\u2212e \u2212t)z)\u03c1(dz ) ) = exp ( \u2212\u00b5(A)\n\u222b R+ ( \u221e\u2211 k=0 e\u2212nz(nz)k k!\n\u2212 \u221e\u2211 k=0 e\u2212nze\u2212kt(nz)k k!\n) \u03c1(dz ) )\nwhere, we have used the fact that 1 = \u2211\u221e k=0 e\u2212nz(nz)k\nk! . Rearranging the terms in the above equation, we get\nE[e\u2212tN(A)]\n= exp ( \u2212\u00b5(A)\n\u221e\u2211 k=1 (1\u2212 e\u2212kt) \u222b R+ e\u2212nz(nz)k k! \u03c1(dz ) ) .\nHence, the Poisson intensity measure of N , when viewed as a CRM is given by\n\u03bd\u0304(dk ,dx ) = \u00b5(dx ) \u222b R+ e\u2212nz(nz)k k! \u03c1(dz )\nwhen k \u2208 {1, 2, 3, . . . }, and 0 otherwise. The distinct points of N can be obtained by projecting N on S. Hence, by the mapping proposition for Poisson processes (Kingman, 1992), the distinct points ofN form a Poisson process with mean measure \u00b5\u2217(dx ) = \u03bd\u0304(f\u22121(dx )), where f is the projection map on S. Hence f\u22121(dx ) = (R+,dx ), and\n\u00b5\u2217(dx ) = \u03bd\u0304(R+,dx )\n= \u00b5(dx ) \u222b R+ \u221e\u2211 k=1 e\u2212nz(nz)k k! \u03c1(dz )\n= \u00b5(dx ) \u222b R+ (1\u2212 e\u2212nz)\u03c1(dz ) .\nThus, the result follows."}, {"heading": "Proof of proposition 3.2", "text": "Proof. The proof relies on the simple fact, that conditioned on the number of points to be sampled, the points of a Poisson process are independent (Kingman, 1992). Thus, n point processes can be sampled from a measure \u039b, by first sampling the number of points in each point process from a Poisson distribution with mean \u039b(S), and then sampling the points independently. Let \u039b = \u2211n i=1 \u2206i\u03b4Xi . Let (Xl1 , . . . , Xlk) be the features discovered by the n Poisson processes. Let the ith point process Ni consist of mi1 occurrences of Xl1 , mi2 occurrences of Xl2 and mik occurrences of Xlk . Then, the joint distribution of the n point processes conditioned on \u039b is given by\nP(N1, . . . , Nn|\u039b)\n= n\u220f i=1 exp(\u2212T )T \u2211k j=1 mij ( \u2211k j=1mij)! k\u220f j=1 ( \u2206lj T )mij ,\nwhere T = \u039b(S) = \u2211\u221e i=1 \u2206i\u03b4Xi(S) = \u2211\u221e i=1 \u2206i. Readjusting the outermost product in the above equation, we get,\nP(N1, . . . , Nn|\u039b) = exp(\u2212nT )\u220fn i=1( \u2211k j=1mij)! k\u220f j=1 \u2206 \u2211n i=1 mij lj .\nSince, we are not interested in the actual points Xli \u2019s, but only the number of occurrences of the different points in the point processes, that is, [mij ](n,k), we can sum over every k-tuple of distinct atoms in the random measure \u039b. Hence,\nP ([mij ](n,k)|\u039b)\n= exp(\u2212nT )\u220fn i=1( \u2211k j=1mij)! \u2211 \u2206l1 6=\u2206l2 6=\u00b7\u00b7\u00b76=\u2206lk k\u220f j=1 \u2206 \u2211n i=1 mij lj ,\nwhere the sum is over all subsets of length k of the set {\u22061,\u22062, . . . }. Finally, in order to compute the result, we need to take expectation with respect to the distribution of \u039b. Towards that end, we note that only the weights of \u039b appear in the above equation. From section 2.2, we know that the weights of a CRM with Poisson intensity measure \u03c1(dz )\u00b5(dx )form a Poisson process with mean measure \u00b5(S)\u03c1(dz ). Hence, it is enough to take the expectation with respect to the Poisson process.\nP ([mij ](n,k)) (32)\n= 1\u220fn i=1( \u2211k j=1mij)! E\n[ exp (\u2212nT ) (33)\n\u2211 \u2206l1 6=\u2206l2 6=\u00b7\u00b7\u00b76=\u2206lk k\u220f j=1 \u2206 \u2211n i=1 mij lj\n , (34)\nThe expectation can further be simplified by applying Proposition 2.1 of (James, 2005).\nProposition 6.1 ((James, 2005)). Let N be the space of all \u03c3\u2212finite counting measures on R+, equipped with an appropriate \u03c3-field. Let f : R+ \u2192 R+ and g : N \u2192 R+ be measurable with respect to their \u03c3-fields. Then, for a Poisson processN with mean measure E[N(dx )] = \u03c1(dx ),\nE [ g(N)e\u2212 \u2211 \u2206\u2208N f(\u2206) ] = E [ e\u2212 \u2211 \u2206\u2208N f(\u2206) ] E[g(N\u0304)]\nwhere N\u0304 is a Poisson process with mean measure E[N(dx )] = e\u2212f(x)\u03c1(dx ).\nApplying the above proposition to (32), we get P ([mij ](n,k)) = E [ e\u2212 \u2211\u221e i=1 n\u2206i ]\u220fn i=1( \u2211k j=1mij)!\n\u00d7 E  \u2211 \u2206l1 6=\u2206l2 6=\u00b7\u00b7\u00b76=\u2206lk\u2208N\u0304 k\u220f j=1 \u2206 \u2211n i=1 mij lj  where N\u0304 is a Poisson process with mean measure E[N(dz )] = e\u2212nz\u03c1(dz )\u03b8. The first expectation can be evaluated using Campbell\u2019s proposition and is given by exp ( \u2212\u03b8 \u222b R+(1\u2212 e \u2212nz)\u03c1(dz ) ) . In order to evaluate the second expectation, we construct a new point process from N\u2217 on R+k by concatenating every set of k distinct points in N\u0304 . The expression in the second expectation can then be rewritten as\n\u2211 (\u2206l1 ,...,\u2206lk )\u2208N \u2217 k\u220f j=1 \u2206 \u2211n i=1 mij lj\nBy Campbell\u2019s proposition for point processes,\nE [\u2211 \u2206\u2208N f(\u2206) ] = \u222b z\u2208R+ f(z)\u03c1(dz ) ,\nwhere \u03c1(dz ) = E[N(dz )]. Moreover, since the point process N\u2217 is obtained by concatenating distinct points in N , E[N\u2217(dz 1, . . . ,dzk)] = \u220fk j=1 E[N\u0304(dz j)] =\u220fk\nj=1 \u03b8e \u2212nz\u03c1(dz j), whenever zj\u2019s are distinct. Hence,\nE  \u2211 \u2206l1 6=\u2206l2 6=\u00b7\u00b7\u00b76=\u2206lk\u2208N\u0304 k\u220f j=1 \u2206 \u2211n i=1 mij lj  =\nk\u220f j=1 \u222b z\u2208R+ \u03b8e\u2212nzz \u2211n i=1 mij\u03c1(dz ) .\nHence, the final expression for the marginal distribution of the set of counts for each latent feature is given by\nP ([mij ](n,k)) = exp\n( \u2212\u03b8 \u222b R+(1\u2212 e \u2212nz)\u03c1(dz ) )\u220fn\ni=1( \u2211k j=1mij)!\n\u00d7 k\u220f j=1 \u222b z\u2208R+ \u03b8e\u2212nzz \u2211n i=1 mij\u03c1(dz )\nThe above expression can be simplified by letting \u03c8(t) = \u03b8 \u222b R+(1 \u2212 e\n\u2212tz)\u03c1(dz ). Hence, \u03c8(l)(t) = (\u22121)l\u22121 \u222b R+ \u03b8e\n\u2212tzzl\u03c1(dz ). Hence, the above expression can be rewritten as P ([mij ](n,k)) =(\u22121) \u2211n i=1 \u2211k j=1 mij\u2212k \u03b8\nke\u2212\u03b8\u03c8(n)\u220fn i=1( \u2211k j=1mij)!\n\u00d7 k\u220f j=1 \u03c8( \u2211n i=1 mij)(n)"}, {"heading": "Proof of Corollary 3.3", "text": "Proof. From proposition 3.1, the distinct points in the point processes Ni, 1 \u2264 i \u2264 n, form a Poisson process with mean measure \u00b5(dx)\u00b5(S) \u03c8(n). Hence, the total number of distinct points k is distributed as Poisson(\u03c8(n)). Hence, conditioning equation (5) with respect to k, we get the desired result."}, {"heading": "Proof of Proposition 3.4", "text": "Proof. Let N = \u2211n i=1Ni. From the arguments of proposition 3.1, N is a CRM, and hence, can be written as N(dx ) = \u222b R+ zN\u0304(dz ,dx ) for some Poisson process N\u0304 . Let \u03a0 be the random collection of points corrsponding to N\u0304 . Now define a map f : R+ \u00d7 S \u2192 S as the projection map on S, that is, f(x, y) = y and M = f(\u03a0) = {{f(x, y) : (x, y) \u2208 \u03a0}}, where the double brackets indicate that M is a multiset. The rest of the arguments remain same as in proposition 3.1 and proposition 3.2."}, {"heading": "Proof of Lemma 4.1", "text": "Proof. Using Proposition 3.4 to marginalize \u039bi from 8, we get that [mij ]1\u2264j\u2264ri\u00b7 is distributed as CRMPoisson(\u03a6(S), \u03c1, 1), that is,\nP ([mij ]1\u2264j\u2264ri\u00b7|\u03a6(S))\n= exp\n( \u2212\u03a6(S) \u222b R+(1\u2212 e \u2212z)\u03c1\u0304(dz ) )\n( \u2211ri\u00b7 j=1mij)!\n\u00d7 ri\u00b7\u220f j=1 \u222b z\u2208R+ \u03a6(S)e\u2212zzmij \u03c1\u0304(dz ) (35)\nLet mi\u00b7 = \u2211ri\u00b7 j=1mij . Taking expectation with respect to \u03a6(S), we get the marginal distribution of [mij ]1\u2264j\u2264ri\u00b7 , where ri\u00b7 is also random. P ([mij ]1\u2264j\u2264ri\u00b7) = E [ exp ( \u2212\u03a6(S)\n\u222b R+ (1\u2212 e\u2212z)\u03c1\u0304(dz ) ) \u03a6(S) ri\u00b7 ]\n\u00d7 \u220fri\u00b7 j=1 \u222b z\u2208R+ e \u2212zzmij \u03c1\u0304(dz )\nmi\u00b7! (36)\nIt is given that h(u) = E[e\u2212u\u03a6(S)]\n\u03c8\u0304(u) = \u222b R+ (1\u2212 e\u2212uz)\u03c1\u0304(dz ) ,\nHence\ndri\u00b7 duri\u00b7 h(u) = (\u22121)\nri\u00b7E [ \u03a6(S) ri\u00b7e\u2212u\u03a6(S) ]\n\u03c8\u0304(mij)(u) = (\u22121)mij\u22121 \u222b R+ e\u2212uzzmij \u03c1\u0304(dz )\nUsing the above results with u = \u03c8\u0304(1), equation (36) can be rewritten as\nP ([mij ]1\u2264j\u2264ri\u00b7) = (\u22121)mi\u00b7h(ri\u00b7)(\u03c8\u0304(1)) \u220fri\u00b7 j=1 \u03c8\u0304 (mij)(1)\nmi\u00b7! (37)"}], "references": [{"title": "Latent Dirichlet Allocation", "author": ["Blei", "David M", "Ng", "Andrew Y", "Jordan", "Michael I"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Blei et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2001}, {"title": "Combinatorial Clustering and the Beta Negative Binomial Process", "author": ["Broderick", "Tamara", "Mackey", "Lester", "Paisley", "John", "Jordan", "Michael I"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Broderick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Broderick et al\\.", "year": 2015}, {"title": "The combinatorial structure of beta negative binomial processes", "author": ["Heaukulani", "Creighton", "Roy", "Daniel M"], "venue": "arXiv preprint arXiv:1401.0062,", "citeRegEx": "Heaukulani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Heaukulani et al\\.", "year": 2013}, {"title": "Probabilistic latent semantic analysis", "author": ["Hofmann", "Thomas"], "venue": "In Proceedings of the Fifteenth conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Hofmann and Thomas.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann and Thomas.", "year": 1999}, {"title": "Bayesian Poisson process Partition Calculus with an application to Bayesian L\u00e9vy Moving Averages", "author": ["James", "Lancelot F"], "venue": "Annals of Statistics,", "citeRegEx": "James and F.,? \\Q2005\\E", "shortCiteRegEx": "James and F.", "year": 2005}, {"title": "Completely Random Measures", "author": ["Kingman", "John"], "venue": "Pacific Journal of Mathematics,", "citeRegEx": "Kingman and John.,? \\Q1967\\E", "shortCiteRegEx": "Kingman and John.", "year": 1967}, {"title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Landauer", "Thomas K", "Dumais", "Susan T"], "venue": "Psychological review,", "citeRegEx": "Landauer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1997}, {"title": "Markov Chain Sampling Methods for Dirichlet Process Mixture Models", "author": ["Neal", "Radford M"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Neal and M.,? \\Q2000\\E", "shortCiteRegEx": "Neal and M.", "year": 2000}, {"title": "The continuum-of-urns scheme, generalized beta and indian buffet processes, and hierarchies thereof", "author": ["Roy", "Daniel M"], "venue": "arXiv preprint arXiv:1501.00208,", "citeRegEx": "Roy and M.,? \\Q2014\\E", "shortCiteRegEx": "Roy and M.", "year": 2014}, {"title": "Hierarchical Dirichlet Processes", "author": ["Teh", "Yee Whye", "Jordan", "Michael I", "Beal", "Matthew J", "Blei", "David M"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Teh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "The Infinite Gamma-Poisson Feature Model", "author": ["Titsias", "Michalis K"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Titsias and K.,? \\Q2008\\E", "shortCiteRegEx": "Titsias and K.", "year": 2008}, {"title": "Beta-negative binomial process and exchangeable random partitions for mixed-membership modeling", "author": ["Zhou", "Mingyuan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhou and Mingyuan.,? \\Q2014\\E", "shortCiteRegEx": "Zhou and Mingyuan.", "year": 2014}, {"title": "Negative Binomial Process Count and Mixture Modelling", "author": ["Zhou", "Mingyuan", "Carin", "Lawrence"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "Beta-Negative Binomial Process and Poisson Factor Analysis", "author": ["Zhou", "Mingyuan", "Hannah", "Lauren A", "Dunson", "David B", "Carin", "Lawrence"], "venue": "In AISTATS,", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}, {"title": "Priors for random count matrices derived from a family of negative binomial processes", "author": ["Zhou", "Mingyuan", "Padilla", "Oscar Hernan Madrid", "Scott", "James G"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "elling have been available in literature since more than a decade (Landauer & Dumais, 1997; Hofmann, 1999; Blei et al., 2001), the first non-parametric approach, that allowed the number of latent classes to be determined as well, was the hierarchical Dirichlet process (HDP) (Teh et al.", "startOffset": 66, "endOffset": 125}, {"referenceID": 9, "context": ", 2001), the first non-parametric approach, that allowed the number of latent classes to be determined as well, was the hierarchical Dirichlet process (HDP) (Teh et al., 2006).", "startOffset": 157, "endOffset": 175}, {"referenceID": 13, "context": "On the other hand, recent approaches such as hierarchical beta-negative binomial process (Zhou et al., 2012; Broderick et al., 2015) and hierarchical gamma-Poisson process (Titsias, 2008; Zhou & Carin, 2015) model the object as a point process, sampled from an object specific random measure, which is itself sampled from a common random measure.", "startOffset": 89, "endOffset": 132}, {"referenceID": 1, "context": "On the other hand, recent approaches such as hierarchical beta-negative binomial process (Zhou et al., 2012; Broderick et al., 2015) and hierarchical gamma-Poisson process (Titsias, 2008; Zhou & Carin, 2015) model the object as a point process, sampled from an object specific random measure, which is itself sampled from a common random measure.", "startOffset": 89, "endOffset": 132}, {"referenceID": 9, "context": "Alternatively, for the HDP, a Chinese restaurant franchise scheme (Teh et al., 2006) can be used for collapsed inference in the model (that is, without explicitly instantiating the atoms).", "startOffset": 66, "endOffset": 84}, {"referenceID": 12, "context": "Fully collapsed inference scheme has also been proposed for beta-negative binomial process (BNBP) (Heaukulani & Roy, 2013; Zhou, 2014) and Gamma-Gamma-Poisson process (Zhou et al., 2015).", "startOffset": 167, "endOffset": 186}, {"referenceID": 0, "context": "elling have been available in literature since more than a decade (Landauer & Dumais, 1997; Hofmann, 1999; Blei et al., 2001), the first non-parametric approach, that allowed the number of latent classes to be determined as well, was the hierarchical Dirichlet process (HDP) (Teh et al., 2006). Both the approaches model the object as a set of repeated draws from an object-specific distribution, whereby the object specific distribution is itself sampled from a common distribution. On the other hand, recent approaches such as hierarchical beta-negative binomial process (Zhou et al., 2012; Broderick et al., 2015) and hierarchical gamma-Poisson process (Titsias, 2008; Zhou & Carin, 2015) model the object as a point process, sampled from an object specific random measure, which is itself sampled from a common random measure. In some sense, these approaches are more natural for mixed membership modelling, since they model the object as a single entity rather than as a sequence of draws from a distribution. A straightforward implementation of any of the above nonparametric models would require sampling the atoms in the non-parametric distribution for the base as well as objectspecific measure. However, since the number of atoms in these distributions are often infinite, a truncation step is required to ensure tractability. Alternatively, for the HDP, a Chinese restaurant franchise scheme (Teh et al., 2006) can be used for collapsed inference in the model (that is, without explicitly instantiating the atoms). Fully collapsed inference scheme has also been proposed for beta-negative binomial process (BNBP) (Heaukulani & Roy, 2013; Zhou, 2014) and Gamma-Gamma-Poisson process (Zhou et al., 2015). Of particular relevance is the work by Roy (2014), whereby a Chinese restaurant fanchise scheme has been proposed for hierarchies of beta proceses (and its generalizations), when coupled with Bernoulli process.", "startOffset": 107, "endOffset": 1764}, {"referenceID": 0, "context": "elling have been available in literature since more than a decade (Landauer & Dumais, 1997; Hofmann, 1999; Blei et al., 2001), the first non-parametric approach, that allowed the number of latent classes to be determined as well, was the hierarchical Dirichlet process (HDP) (Teh et al., 2006). Both the approaches model the object as a set of repeated draws from an object-specific distribution, whereby the object specific distribution is itself sampled from a common distribution. On the other hand, recent approaches such as hierarchical beta-negative binomial process (Zhou et al., 2012; Broderick et al., 2015) and hierarchical gamma-Poisson process (Titsias, 2008; Zhou & Carin, 2015) model the object as a point process, sampled from an object specific random measure, which is itself sampled from a common random measure. In some sense, these approaches are more natural for mixed membership modelling, since they model the object as a single entity rather than as a sequence of draws from a distribution. A straightforward implementation of any of the above nonparametric models would require sampling the atoms in the non-parametric distribution for the base as well as objectspecific measure. However, since the number of atoms in these distributions are often infinite, a truncation step is required to ensure tractability. Alternatively, for the HDP, a Chinese restaurant franchise scheme (Teh et al., 2006) can be used for collapsed inference in the model (that is, without explicitly instantiating the atoms). Fully collapsed inference scheme has also been proposed for beta-negative binomial process (BNBP) (Heaukulani & Roy, 2013; Zhou, 2014) and Gamma-Gamma-Poisson process (Zhou et al., 2015). Of particular relevance is the work by Roy (2014), whereby a Chinese restaurant fanchise scheme has been proposed for hierarchies of beta proceses (and its generalizations), when coupled with Bernoulli process. In this paper, it is our aim to extend fully collapsed sampling so as to allow any completely random measure (CRM) for the choice of base and object-specific measure. As proposed in Roy (2014) for hierarchies of generalized beta processes, we propose Chinese restaurant franchise schemes for hierarchies of CRMs, when coupled with Poisar X iv :1 50 9.", "startOffset": 107, "endOffset": 2118}, {"referenceID": 9, "context": "In an analny with the Chinese restaurant franchise model (Teh et al., 2006), one can think of Til", "startOffset": 57, "endOffset": 75}], "year": 2016, "abstractText": "The aim of the paper is to provide an exact approach for generating a Poisson process sampled from a hierarchical CRM, without having to instantiate the infinitely many atoms of the random measures. We use completely random measures (CRM) and hierarchical CRM to define a prior for Poisson processes. We derive the marginal distribution of the resultant point process, when the underlying CRM is marginalized out. Using well known properties unique to Poisson processes, we were able to derive an exact approach for instantiating a Poisson process with a hierarchical CRM prior. Furthermore, we derive Gibbs sampling strategies for hierarchical CRM models based on Chinese restaurant franchise sampling scheme. As an example, we present the sum of generalized gamma process (SGGP), and show its application in topicmodelling. We show that one can determine the power-law behaviour of the topics and words in a Bayesian fashion, by defining a prior on the parameters of SGGP.", "creator": "LaTeX with hyperref package"}}}