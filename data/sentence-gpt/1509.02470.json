{"id": "1509.02470", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2015", "title": "Deep Attributes from Context-Aware Regional Neural Codes", "abstract": "Recently, many researches employ middle-layer output of convolutional neural network models (CNN) as features for different visual recognition tasks. Although promising results have been achieved in some empirical studies, such type of representations still suffer from the well-known issue of semantic gap. This paper proposes so-called deep attribute framework to alleviate this issue from three aspects. First, we introduce object region proposals as intermedia to represent target images, and extract features from region proposals. Second, we study aggregating features from different CNN layers for all region proposals. The aggregation yields a holistic yet compact representation of input images. Results show that cross-region max-pooling of soft-max layer output outperform all other layers. As soft-max layer directly corresponds to semantic concepts, this representation is named \"deep attributes\". Third, we observe that only a small portion of generated regions by object proposals algorithm are correlated to classification target. Therefore, we introduce context-aware region refining algorithm to pick out contextual regions and build context-aware classifiers. Finally, we analyze network network performance for the same task (task-related) as the rest of the network, and determine the validity of the network performance. Finally, we develop a unified network network analysis algorithm to represent the task and the context (task-related).\n\n\n\n\n\n\n\n\nThis article is a part of the J-Machine, a company that aims to improve the security and efficiency of the J-Machine.", "histories": [["v1", "Tue, 8 Sep 2015 17:53:54 GMT  (1789kb,D)", "http://arxiv.org/abs/1509.02470v1", "10 pages, 8 figures"]], "COMMENTS": "10 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["jianwei luo", "jianguo li", "jun wang", "zhiguo jiang", "yurong chen"], "accepted": false, "id": "1509.02470"}, "pdf": {"name": "1509.02470.pdf", "metadata": {"source": "CRF", "title": "Deep Attributes from Context-Aware Regional Neural Codes", "authors": ["Jianwei Luo", "Jianguo Li", "Jun Wang", "Zhiguo Jiang", "Yurong Chen"], "emails": [], "sections": [{"heading": null, "text": "We apply the proposed deep attributes framework for various vision tasks. Extensive experiments are conducted on standard benchmarks for three visual recognition tasks, i.e., image classification, fine-grained recognition and visual instance retrieval. Results show that deep attribute approaches achieve state-of-the-art results, and outperforms existing peer methods with a significant margin, even though some benchmarks have little overlap of concepts with the pre-trained CNN models."}, {"heading": "1. Introduction", "text": "Since the breakthrough work of Krizhevsky et al. [20] on ImageNet [8], researches on convolutional neural networks (CNN) have been exploding. Among them, a lot of\n\u2217This work was done when the first author was working as intern at Intel Labs China under the supervision of the second author.\nresearches adopt pre-trained CNN models as feature extractor for various visual recognition tasks like object detection [12], object recognition [9, 25, 6], image retrieval [13, 25], etc. Features are usually from different CNN layer activations or outputs. To achieve advanced and robust performance, people either fine-tune the pre-trained CNN models on their own tasks, or make extensively data augmentation to get robust classifiers. These developed techniques have shown promising results in comparison to conventional methods using standard feature representations like bag-of-words [29], sparse-coding [34], etc. However, there are two limitations of these kind of methods. First, neural codes from middle-layer are difficult to explain. Second, neural code extraction from whole image definitely loss many context information. These two are summarized to be the well-known semantic gap [30].\nMeanwhile, region features are appealing on recognition task. For instance, Gu et al. [14] employ mid-level features like contour shape, edge shape, color and texture to describe each region for visual recognition tasks. It is well known that region features can naturally preserve more mid-level semantic information like materials, textures, shapes, etc of objects [11]. However, traditional region representations either highly depend on segmentation algorithm [3, 23], or lack of a generic semantic representation for regions for various visual recognition tasks.\nTo address these challenges and leverage the power from\n1\nar X\niv :1\n50 9.\n02 47\n0v 1\n[ cs\n.C V\n] 8\nS ep\n2 01\nboth richness and semantics of these two types of feature representations, in this paper, we propose to integrate the semantic output, i.e., the output from the soft-max layer of CNN models, as well as region proposals to achieve compact yet effective visual representations, namely deep attribute (DA). Since the soft-max layer neural codes are the probability response to the categories on which CNNs are trained, it is fairly compact, semantic, and sparse due to insignificant responses to most categories. Briefly, the proposed method contains four key components.\n(1) We introduce region proposals using algorithm like selective search [31] or edge-box [35] from each input image.\n(2) We feed each extracted regions to CNN models to compute neural codes from soft-max layer.\n(3) We perform cross-region-pooling of regional neural codes to obtain a holistic yet compact representation of the image.\n(4) We observe that only a small portion of region proposals are correlated to classification target, thus we impose learned classifier feedback to pick out a few contextual regions and re-pooling to get a context-aware classifier.\nThe major difference to traditional off-the-shelf CNN methods are illustrated in Figure 1, while the proposed approach is further illustrated in details in Figure 2. To capture more context information related to scale, we also study different pooling layout schemes like multi-scale-pooling and spatial pyramid-pooling extensions.\nTo demonstrate the capacity and robustness of the proposed technique, we employ deep attributes on three visual recognition tasks: image classification, fine-grained object recognition, and visual instance retrieval. Experimental results on several standard benchmark datasets show that deep attributes can achieve state-of-the-arts performance. Especially, the context-aware region refining (CARR) algorithm\nclearly outperforms peer methods with a significant margin. In summary, the major contributions of this paper include: \u2022 We propose a deep attribute approach for visual recognition\ntasks, which is equipped with the semantic power from the outputs of pre-trained CNN models, as well as the compactness of region proposals.\n\u2022 We present schemes to utilize contextual information among region proposals. The context-aware region refining algorithm yield large performance gains.\n\u2022 Experimental results on three different visual recognition tasks clearly show the superiority of the proposed method, as well as the generalization ability to different vision tasks.\nIn the rest of the paper, we will first give a brief survey on related works in Section 2, and then present the details of the proposed deep attribute approach in Section 3. In Section 4, we conduct experiments to study different aspects of setting which will impact the performance of the algorithm. We show experimental results on three visual recognition tasks on Section 5. Conclusions and discussions are given in Section 6."}, {"heading": "2. Related Works", "text": "In this section, we will briefly revisit related works from the following four aspects. CNN methods:. Since the breakthrough success of CNN models on ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 [20], employing CNN models to other vision tasks becomes popular in the computer vision community. Razavian et al. [25] evaluate the performance of CNN features on several vision tasks, including object recognition, fine-grained object recognition, and image retrieval. Meanwhile, DeCAF [9] also shows that CNN features work surprisingly well on image classification. Subsequently, Babenko et al [5] present a similar idea on im-\nage retrieval with fine-tuning on self-collected datasets to further improve retrieval accuracy. In addition, they adopt PCA to compress neural-codes for efficient search. All these methods adopt the neural code activation from the first full-connected layer.\nAttribute methods: Attribute methods adopt discriminative outputs of multi-class classifiers as mid-level features for visual recognition tasks. In face recognition, Kuma et al. [21] consider the labels of reference faces and facecomponents as attributes to describe other faces. Farhadi et al. [11] describe objects using 64 explicitly semantic attribute classifiers. However, it remains as an open challenge to exploit the discriminative output of CNNs to solve generic vision tasks.\nPooling strategy: Pooling is a general strategy to augment features. As one of the most well known work, spatial pyramid matching performs pooling over pyramid of regular grids [22, 34]. Gong et al. [13] encodes the activations of CNN fully connected layer by VLAD [18], and then concatenates the encoded features over windows at three scale levels. Most of these pooling methods simply concatenate features from different grids of scales. On the contrary, decision-level cross-region pooling has been applied when there are multiple region/patch candidates [27, 32]. In our work, since we use the semantic output of CNNs as regional features, it is fairly straightforward to perform pooling across different region proposals.\nRegion proposals: Methods for detecting region proposal are used in object detection to avoid exhaustive sliding window search across images and speed up the detection without noticeable loss of recall rates [12]. In general, region proposal detection is based on low-level features and visual cues to measure objectness of local regions to generate relatively fewer candidate windows. In the past few years, there have been extensive studies on this topic and many techniques are invented, including selective search [31], edge-boxes [35], BING [7], multiscale combinatorial grouping (MCG) [4], and so on. Recently, Jan Hosang et al. [16] evaluates ten region proposal methods, in which selective search and edge-boxes achieved consistently better performance in terms of ground truth recall, repeatability, and detection speed. Hence, we may employ them to produce region proposals as the first step of our deep attribute method."}, {"heading": "3. Deep Attribute with Regional Neural Codes", "text": "The proposed deep attribute method consists of the following four steps.\n(1) Region proposals extraction: We use advanced techniques like selective search [31] or edge-boxes [35] to extract semantical regions, as both of which show satisfactory performance in benchmarks [16].\n(2) Computing of neural codes for region proposals: We use CNN models trained from 1000 categories of ILSVRC 2012 to produce 1000 dimensional semantic output for each extracted region proposals. Such computed neural codes will serve as the semantic input for the next pooling stage.\n(3) Cross-region-pooling: We make pooling per dimension across extracted regions to get a 1000-dimensional holistic representations. Different pooling layout schemes are applicable for further possible performance improvement.\n(4) Context-aware region refining and classifier build: A linear classifier can be trained over deep attribute representations for visual recognition tasks. We observed that only a small portion of regions are correlated to classification targets. We thus impose classifier feedback to pick out a few contextual regions and re-pooling to get a context-aware classifier.\nThe first two steps are easy to understand. We will describe the latter two items in details below."}, {"heading": "3.1. Cross Region Pooling (CRP)", "text": "Given image I , a set of object region proposals {R1, R2, ..., RN} are extracted by region detection algorithms like [31, 35]. Then the regions are wrapped and feed to CNN models. Each region Ri is thus represented by the output from soft-max layer as Fi = (fi1, \u00b7 \u00b7 \u00b7 , fiK), where fik is the k-th dimensional neural code of Fi. The computed neural codes of all the regions are then aggregated with a pooling operation to construct a holistic representation of the input image I . The pooling could be either maxpooling or average-pooling. In our practice, we find maxpooling works better than average-pooling when do pooling over all region proposals. Thus, the final code for the k-th dimension f\u0302k is obtained as f\u0302k = maxNi=1{fik}. Figure 2 illustrates the details of the pooling scheme. Since the CNN models are trained on 1000 categories, we derive a 1000 dimensional deep attribute after this pooling procedure.\nAs the single-scale cross-region-pooling does not consider the layout of regions across scales (like the person and the horse in Figure 2), we may further enhance it with different pooling layout scheme. This paper studies two layout schemes: multi-scale pooling scheme or spatial pyramid pooling scheme. In multi-scale pooling, we divide the regions into different scale-interval groups, perform pooling over the regions within each group independently, and then concatenate the deep attributes from each group together, yielding a final holistical representation of the image. Particularly, the scale of a region is defined as the ratio of its area proportional to the area of the whole image, or the area of the image bounding box when available. In our experiments, we use five scale-intervals, which are (0, 116 ],( 1 16 , 1 8 ],( 1 8 , 1 4 ],( 1 4 , 1 2 ] and ( 1 2 , 1]. Therefore, the final representation is a 5000-dimension deep attribute feature. In contrast, spatial pyramid pooling (SPP) divides the whole image into 1x1, 2x2 and 4x4 grids, and make cross-region-\npooling in each grid, and then concatenate features together for a holistic representation."}, {"heading": "3.2. Context Aware Region Refining (CARR)", "text": "Region proposal algorithms usually produce thousands of regions for input images to ensure high recall rate [16]. For a certain classification task, we observed that extracted regions could be divided into three categories. First, some regions are directly related to the classification target. Second, some regions can be viewed as useful context information. Third, some regions belong to background clutter, which has little relationship to the classification task. For instance, for a horse classification in Figure 3, the horse region is target region, and the human region can be viewed as a context region, while the car region is certainly background noise. To improve the accuracy, we should maximally exploit the context information, while suppress the clutter information. Note that the background clutter regions are category specific. Again in Figure 3, for car classification, the horse region is background noise. This inspires us imposing classifier feedback information to pick out category-specific context regions. We therefore propose a context-aware region refining (CARR) algorithm based on this observation.\nSuppose linear classifier hc(x) = W c \u2217 x is trained from features x by cross-region-pooling for category c. Given image I , region proposal algorithm extracted N regions, and region Rk is represented by semantical neural codes Fk. We define a score for region Rk as\nSck = h c(Fk) = w c \u00b7 Fk. (1)\nThe larger the score is, the higher the region correlated to category c. A region is defined as category-c contextual region if only if Sck > \u03b8.\nIn practice, for each image I and category c, we rank the regions according to their score Sck. We then pick top-K regions as contextual regions, and apply cross-region-pooling on these top-K regions again to get a new holistic representation for each category separately. A refined linear classifier for category c is then learned on these updated repre-\nsentations. This procedure can be run iteratively to refine both context regions and linear classifiers.\nSuppose we run the procedure with T iterations, we thus have T classifiers {hct(x)}Tt=1 for each category c. Intuitively, we can directly use the final classifier hcT (x). However, classifier ensemble may bring more accuracy gains. We can fuse hct(x) together as\nHcT (x) = \u2211T\nt=1 \u03b1th\nc t(x) = \u2211T t=1 \u03b1tw c t \u00b7 x, (2)\nwhere \u03b1t is a weight coefficient for t-th classifier. Empirically, \u03b1t can be estimated by grid search in a crossvalidation manner. Since this fusion is similar to boosting, we give \u03b1t estimation followed by the AdaBoost rule as\n\u03b1t = log 1\u2212 Et Et , (3)\nwhere Et is the classification error on training set at t-th iteration. Algorithm 1 lists the training procedure.\nIn the testing phase, we applyHcT (x) on each image, and do context-aware region refining based on classifier label. Algorithm 2 lists the detailed testing procedure.\nAlgorithm 1: Learning CARR Classifier Data: For each image I , given regions {Rk}I and\ncorresponding soft-max output {Fk}I for t = 1:T do\nfor each category c do for each image I do\nfor each region Rk do Compute Sck(Fk) = H c t\u22121(Fk); end Sort Sck in descending order; Pick top-K regions from all N regions; Cross-region-pooling on top-K regions; Get new representation FI ;\nend Train new linear classifier hct(x) based on {FI}; Compute error-rate Et for hct(x); Update Hct according to Eq. 2;\nend end Result: Output fused classifier HcT (x)."}, {"heading": "4. Implementation and Performance Study", "text": "This section will give implementation details and study performance impact of different choices. Through this section, we study the performance on PASCAL VOC 2007 image classification benchmark with standard protocol. PASCAL VOC [10] is a very challenging benchmark for object recognition. It contains images of 20 categories including animals, handmade objects and natural objects. The objects are at different locations and scales\nAlgorithm 2: Prediction with CARR Classifier Data: Input testing image I and region proposals {Rk} for each classifier Hc(x) do\nfor t = 1:T do for each region Rk do\nCompute score Sck = H c t (Fk);\nend Sort Sck in descending order; Pick top-K regions from all N regions; Cross-region-pooling on top-K regions; Get new representation FI ; Predict image score with ScI = H c t (FI);\nend end Result: Category label c\u0302 = argmax\nc ScI .\nwith clutter background. Even if objects are annotated with bounding box, this study does not use this information. The accuracy is measured by mean average-precision (mAP) over 20 categories on the benchmark.\nIn this paper, we adopt the CNN models trained on ImageNet dataset [26], which contains 1.2 million images associated with 1000 semantic categories. We make experiments based on the Caffe deep learning framework [19]. It should be highlighted here that we do not make any data augmentation at all. For each region, we only feed the single center cropping into CNN for computing neural-codes output. Besides, we did not fine-tune the CNN model on the given datasets. After deep attribute feature is extracted, we process the feature with a RootSIFT trick normalization as in [2]. Linear SVM classifier is trained by choosing the best cost parameter C in SVM on the train/val split."}, {"heading": "4.1. CNN Models: Alex\u2019s vs VGG\u2019s", "text": "We first compare two CNN models: Alex\u2019s net [20] vs VGG\u2019s net [28]. There are several different VGG CNN models public available, in which we used the 16 layer VGG model. In this study, we only apply the single-scale crossproposal max-pooling without context refining. We adopt selective search as region proposal generation algorithms. Alex\u2019s net achieves 80.8% mAP on 20 categories of VOC 2007, while VGG\u2019s net achieves 85.6% mAP. It is obvious that VGG\u2019s net outperforms Alex\u2019s net with a big margin. This consistent with the factor that VGG\u2019s net performs better than Alex\u2019s net on ImageNet. Hence, we adopt VGG\u2019s net in all the remaining studies."}, {"heading": "4.2. Selective Search vs Edge Box", "text": "We then compare two best region proposal generation algorithms according to [16], i.e., selective search [31] and edge-box [35]. Figure 5(a) illustrates recall rate with respect to number of top-regions on PASCAL VOC 2007 by\nboth selective search and edge box. This graph is generated in the following way. Since PASCAL VOC 2007 dataset gives object bounding box annotations, we employ selective search or edge-box to generate a set of region proposals and sort them according to region scores. Then, we pick topK regions, and compute the intersection-over-union (IoU) rate (IoU) using top-K regions with the ground truth. One region is counted as a recall when the IoU is larger than 0.5. Changing the value of K, we got the graph. From this graph, we can see that edge-box works slightly better than selective search.\nA single scale VGG\u2019s net is adopted in this study with max-pooling for region aggregation. All the generated region proposals are used in cross-region-pooling. Selective search achieves 85.6% mAP on VOC 2007, while edge box achieves 86.1% mAP. This shows edge box works slightly better than selective search, and is consistent with results in Figure 5(a). Specially, edge box is more than 20 times faster than selective search. Therefore, we adopt edge box in all the remaining studies."}, {"heading": "4.3. Different Pooling Schemes", "text": "We also compare different pooling schemes. First, we compare pooling layout schemes under cross-proposal maxpooling. Single-scale makes cross-region-pooling (CRP) over the whole image. Spatial-pyramid-pooling divides the whole image into 1x1 grids, 2x2 grids plus one center grid. It makes CRP on each grid, and then concatenates features from each grid together. Multi-scale-pooling makes CRP over five different region scales according to region size proportion to image size. Features from different scales are concatenated together to a holistic representation. In this study, we adopt VGG\u2019s net with max-pooling. Experiments show that single-scale achieves 86.1% mAP, spatialpyramid-pooling achieves 87.2% mAP, and the multi-scale pooling achieves 89.7% mAP. Hence, multi-scale pooling outperforms other pooling scheme with a big margin. As spatial-pyramid-pooling does no show advantages over multi-scale case, we will not take it into consideration in future studies.\nSecond, we compare max-pooling to average-pooling\nunder both single-scale layout and multi-scale layout. In single-scale case, max-pooling achieves 86.1% mAP, while average-pooling achieves 87.3% mAP. In the multi-scale case, max-pooling achieves 89.7% mAP, while average pooling achieves 89.3% mAP. That means, max-pooling is worse than average-pooling in single-scale case, while it is better than average-pooling in multi-scale case."}, {"heading": "4.4. Different CNN Layers", "text": "We aggregate output from soft-max layer with crossregion-pooling as feature representations. What if we used output of other CNN layers as features? In this study, we compare 5 different CNN layers (pool5, fc1, fc2, fc3, soft-max) on their performance with cross-region-pooling. Figure 4 illustrates the results on both single-scale case and multi-scale case. We can conclude that (1) In multiscale case, max-pooling consistently outperform averagingpooling on different layers, while DA (soft-max) layer achieves 89.7% mAP, which beats all the other layers on both max-pooling and average-pooling; (2) In single-scale case, max-pooling outperforms averaging-pooling on all layers except for the soft-max layer. Soft-max layer with average-pooling performs the best over all the other layers in this case.\nAs superb accuracy reached by soft-max layers and relative lower dimensionality (1000 for single scale, 5000 for multi-scale), we choose soft-max layer and multi-scale pooling in the proposed framework."}, {"heading": "4.5. How Many Regions Are Required?", "text": "People may doubt whether cross-proposal pooling are useful, and how many regions are enough? This study will answer these two questions. We use edge-box to generate region proposals. We pick top-K regions according to their region score for the multi-scale cross-region-pooling. For different K value, we get the accuracy on VOC 2007. The results are shown in Figure 5(b). It shows that more regions is better, and when the number of regions exceed 500, the\naccuracy is saturated. That means we can just feed top 500 regions rather than all 1500+ regions to CNN, which can save a lot of computation cost."}, {"heading": "4.6. Parameters in CARR", "text": "In the context-aware region refining, we need to determine parameter K and number of iterations. Instead of directly set K, we define the context region ratio \u03b2 = KN , where N is the total number of region proposals extracted. Figure 6 illustrates the mAP curves on VOC 2007 with different \u03b2 and different iteration number. It shows that \u03b2 = 0.025 gives the best results. As the number of total regions N usually larger than 1500, that means there are usually about 40 context regions picked out for each image and each category. Besides, we observed that only one iteration of context region refining will bring 3.6% accuracy improvement for single scale deep attributes (from 86.1% to 89.7%), while bring 1.1% accuracy improvement for the multi-scale case (from 89.7% to 90.8%). Further iterations do not bring notable gains. Therefore, we set \u03b2=0.025, iteration number = 2 for all the remaining experiments."}, {"heading": "4.7. How About using Other Layers in Refining?", "text": "In section 4.4, we showed that soft-max layer (DA) is the best with cross-region-pooling. We therefore fixed the first step (global CRP step) with DA features. In section 4.6, we adopted DA features in the region refining step. Here we\nstudy other CNN layers in the refining step. We compared DA+DA (aka, DA2) to DA+Pool5, DA+FC1, DA+FC2, DA+FC3 on VOC 2007, in which the first DA indicates the global CRP step, while the 2nd item (after \u2018+\u2019 sign) indicates the layer used in the region refining step. We also study the impact from max-pooling and average-pooling in the refining step. Figure 7 illustrates the comparison results.\nIt is obvious that average-pooling works better than maxpooling in the refining step. This is different from the global CRP step. The reason is that the global CRP step contains all the regions which may contain many background clutter noise regions; while in the refining step, most noise regions are suppressed. Also we find that DA+FC1 works the best (91.4% mAP). For the fusion of two steps, we still adopted the rule by Eq.2 with \u03b1t given by Eq.3."}, {"heading": "5. Experimental Results", "text": "We thoroughly evaluate the proposed approach on three vision tasks, i.e., image classification, fine-grained object recognition and visual instance retrieval.\nAccording to previous study, we fixed parameters of the proposed framework as VGG-16 CNN model, edge-box for region extraction, multi-scale pooling layout, and one CARR step with \u03b2=0.025. We report results both by DA2 and DA+FC1 for all the recognition tasks."}, {"heading": "5.1. Image Classification Task", "text": "The image classification task is evaluated on the PASCAL VOC 2007 and 2012 benchmarks.\nWe first report the mean Average Precision (mAP) for the PASCAL VOC 2007 dataset with per-category results. Table 1 shows per-category results in comparison to the state-\nof-the-art methods w.r.t each category on VOC 2007, while Table 2 shows per-category results on VOC 2012. Note that we listed one results by DA+FC1\u2217, which is trained on a combination training set from VOC 2007 and 2012. We further listed CNN-related methods on their training configurations at Table 3 on two benchmarks.\nWe can see that our method is fairly simple, without fine-tuning and data augmentation. It outperforms current state-of-the-art method with a large margin. Specially, we should mention two methods here. First is the very-deep CNN method (aka, VGG-16-19 fusion). The result of verydeep is by fusion both VGG-16 model and VGG-19 models with sophisticated multi-scale and multi-crop data augmentation. The proposed approach is based on VGG-16 model with just center-crop region proposal inputs, yielding a 2.8% margin over very-deep. Second is the multiview multi-instance framework by [33] (FV+LV-20-VD), which takes FC layers output from region proposals as feature view and ground truth bounding box as label view, and combine them under a Fisher-vector framework. This method achieves 90.7% accuracy. In comparison, the proposed approach outperforms FV+LV-20-VD with a margin 1.8% with out using ground-truth bounding-box information and without fine-tuning."}, {"heading": "5.2. Fine-Grained Recognition Task", "text": "The fine-grained recognition task is evaluated on the Oxford flower dataset, which contains 102 categories of flowers. Each category contains 40 to 258 of images. The flowers appear at different scales, pose and lighting conditions. The dataset provides segmentation for all the images. However, we do not use this information in our experiment.\nOur evaluation follows the standard protocol of this\nbenchmark. We report mean Accuracy on the Oxford 102 flowers dataset in Table 4. DA2 achieve 90.1% accuracy, which is significantly higher than all existing methods. When replacing refining step DA feature with FC1, it (DA+FC1) achieved 95.1% accuracy. Note that these results are obtained without using segmentation information, fine-tuning CNN networks. More interestingly, the flower dataset has little concepts overlap with the pre-trained 1000- category CNN models."}, {"heading": "5.3. Visual Instance Retrieval Task", "text": "The visual instance retrieval task is evaluated on two datasets: (1) Holidays [17] consists of 1491 vacation photographs distributed among 500 groups based on same object or scene. One image is selected from each group and is acted as a query image. This dataset is challenge due to viewpoint and scale variations. (2) University of Kentucky Benchmark dataset [24] (UKB) includes 10,200 indoor photographs uniformly from 2550 objects, and each image is used to query the rest. It is challenging due to viewpoint variations. In this experiment, we use the cosine similarity as the metric.\nWe report the mean Average Precision (mAP) for the Holidays dataset, and accuracy of top-4 retrieval results for the UKB dataset according to standard evaluation protocol on these two datasets. Note that this task is for image retrieval, which does not require to train a classifier. We just adopt single-scale and multi-scale max-pooling DA features for retrieval, without any special tricks. Table 5 illustrates comparison results on these two datasets. It shows that the\nproposed methods outperform the state-of-the-art methods with a notable margin."}, {"heading": "6. Conclusions and Discussions", "text": "To handle semantic gap of global CNN feature representation, this paper propose the deep attribute framework to alleviate the issue from three aspects. First, we introduce semantic region proposals as an intermedia to represent images. Second, we show that aggregating soft-max output from region proposals with cross-region max-pooling yields best accuracy among all different CNN feature layers. The soft-max output (aka, deep attributes) is interpretable yet compact. Third, we introduce context-aware region refining algorithm to pick out classification target related regions, and build context-aware classifiers.\nTo corroborate the effectiveness, we use the deep attribute as generic feature representation on various vision tasks. Our empirical studies show that the proposed approach outperforms the competing methods with a large margin. The reason for success is due to at least three factors. First, the region proposals are good alignment to target objects. Second, cross-region max-pooling will suppress most noise regions, while keep most meaningful regions. Third, the context region refining will keep only those those highly correlated regions while suppress most possible background noise regions.\nDeep attributes have several good properties. Here we discussion some of them. First, the deep attribute representation can be easily interpreted. We can backtrack each attribute (per feature dimension) to region proposal which produce this attribute, as the aggregation is done by maxpooling. We can also identify the most discriminant region\nfor each image and category by sorting region score Sk from Eq.1. Figure 8 shows more examples of the most discriminative region for each image in a red bounding box.\nSecond, deep attribute can be applied to a wide range of compute vision tasks. Especially, even the deep attribute is obtained from CNN model trained from ImageNet, it can be employed to those tasks which has fairly different object concepts from that of ImageNet. For instance, in the experiment for fine grained flower recognition, the flower dataset has little category overlap with the ImageNet 1000 categories; as well as the Holiday and UKB dataset in image retrieval task. As discussed above, Figure 8 illustrates the most discriminative region associated with each semantic category for the samples from both the PASCAL VOC and flower datasets. It is obvious that regions in PASCAL VOC samples reflect more semantic meanings, while this is not true for flower samples. This is due to the fact that PASCAL VOC has more semantic concepts overlap with that of ImageNet than flowers dataset. This phenomenon also inspires us to explore more things behind deep attributes. For instance, how many attributes are sufficient to support generic visual recognition tasks? Could we find a minimum supporting attribute set for visual recognition? Future works will make further exploration on it.\nThird, in Figures 2, we can see that the extracted deep attribute representation is somewhat sparse. We could make the representation even sparse by shrinking the deep attribute with a pre-defined threshold. Such a property is extremely useful for large-scale vision system, because a sparse representation can bring benefits in both storage and execution speed.\nThe proposed approach has several aspects to be im-\nproved. First, there are large space for execution efficiency improvement. Currently, it runs about 5\u223c10s per image on NVidia Titan X GPU. Second, the accuracy and robustness could be further improved with fine-tuning on given dataset. An interesting future direction is to design an unified framework to consider these two points together."}], "references": [{"title": "Efficient object detection and segmentation for fine-grained recognition", "author": ["A. Angelova", "S. Zhu"], "venue": "CVPR, pages 811\u2013 818", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Three things everyone should know to improve object retrieval", "author": ["R. Arandjelovi\u0107", "A. Zisserman"], "venue": "CVPR", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "From contours to regions: An empirical evaluation", "author": ["P. Arbelaez", "M. Maire", "C. Fowlkes", "J. Malik"], "venue": "CVPR, pages 2294\u20132301", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "and et al", "author": ["P. Arbelaez", "J. Pont-Tuset"], "venue": "Multiscale combinatorial grouping. In CVPR", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural codes for image retrieval", "author": ["A. Babenko", "A. Slesarev", "A. Chigorin", "V. Lempitsky"], "venue": "arXiv:1404.1777", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "BMVC", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Bing: Binarized normed gradients for objectness estimation at 300fps", "author": ["M.-M. Cheng", "Z. Zhang", "W.-Y. Lin", "P. Torr"], "venue": "CVPR", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "CVPR, pages 248\u2013255", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "and et al", "author": ["J. Donahue", "Y. Jia", "O. Vinyals"], "venue": "Decaf: A deep convolutional activation feature for generic visual recognition. In ICML", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "venue": "IJCV, 88(2):303\u2013338", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth"], "venue": "CVPR, pages 1778\u20131785", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "arXiv:1311.2524", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-scale orderless pooling of deep convolutional activation features", "author": ["Y. Gong", "L. Wang", "R. Guo", "S. Lazebnik"], "venue": "arXiv:1403.1840", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Recognition using regions", "author": ["C. Gu", "J.J. Lim", "P. Arbelaez", "J. Malik"], "venue": "CVPR, pages 1030\u20131037", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Combining efficient object localization and image classification", "author": ["H. Harzallah", "F. Jurie", "C. Schmid"], "venue": "CVPR, pages 237\u2013244", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "How good are detection proposals", "author": ["J. Hosang", "R. Benenson", "B. Schiele"], "venue": "really? BMVC", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Hamming embedding and weak geometric consistency for large scale image search", "author": ["H. Jegou", "M. Douze", "C. Schmid"], "venue": "In ECCV,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Aggregating local descriptors into a compact image representation", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid", "P. P\u00e9rez"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv:1408.5093", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, pages 1097\u20131105", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Attribute and simile classifiers for face verification", "author": ["N. Kumar", "A.C. Berg", "P.N. Belhumeur", "S.K. Nayar"], "venue": "CVPR, pages 365\u2013372", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "CVPR, volume 2, pages 2169\u20132178", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Attributes make sense on segmented objects", "author": ["Z. Li", "E. Gavves", "T. Mensink", "C.G. Snoek"], "venue": "In ECCV,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Scalable recognition with a vocabulary tree", "author": ["D. Nister", "H. Stewenius"], "venue": "CVPR, volume 2, pages 2161\u20132168", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "arXiv:1403.6382", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "CoRR, abs/1409.0575", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Overfeat: Integrated recognition", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "localization and detection using convolutional networks. In International Conference on Learning Representations", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Video google: A text retrieval approach to object matching in videos", "author": ["J. Sivic", "A. Zisserman"], "venue": "CVPR, pages 1470\u2013 1477. IEEE", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2003}, {"title": "Content-based image retrieval at the end of the early years", "author": ["A.W. Smeulders", "M. Worring", "S. Santini", "A. Gupta", "R. Jain"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 22(12):1349\u20131380", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2000}, {"title": "K", "author": ["J.R. Uijlings"], "venue": "E. van de Sande, T. Gevers, and A. W. Smeulders. Selective search for object recognition. IJCV, 104(2):154\u2013171", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "CNN: single-label to multi-label", "author": ["Y. Wei", "W. Xia", "J. Huang", "B. Ni", "J. Dong", "Y. Zhao", "S. Yan"], "venue": "CoRR, abs/1406.5726", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "and J", "author": ["H. Yang", "J.T. Zhou", "Y. Zhang", "B.-B. Gao", "J. Wu"], "venue": "Cai. Can partial strong labels boost multi-label object recognition? arXiv preprint arXiv:1504.05843", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": "CVPR, pages 1794\u20131801", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Edge boxes: Locating object proposals from edges", "author": ["C.L. Zitnick", "P. Dollar"], "venue": "ECCV", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 19, "context": "[20] on ImageNet [8], researches on convolutional neural networks (CNN) have been exploding.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[20] on ImageNet [8], researches on convolutional neural networks (CNN) have been exploding.", "startOffset": 17, "endOffset": 20}, {"referenceID": 11, "context": "researches adopt pre-trained CNN models as feature extractor for various visual recognition tasks like object detection [12], object recognition [9, 25, 6], image retrieval [13, 25], etc.", "startOffset": 120, "endOffset": 124}, {"referenceID": 8, "context": "researches adopt pre-trained CNN models as feature extractor for various visual recognition tasks like object detection [12], object recognition [9, 25, 6], image retrieval [13, 25], etc.", "startOffset": 145, "endOffset": 155}, {"referenceID": 24, "context": "researches adopt pre-trained CNN models as feature extractor for various visual recognition tasks like object detection [12], object recognition [9, 25, 6], image retrieval [13, 25], etc.", "startOffset": 145, "endOffset": 155}, {"referenceID": 5, "context": "researches adopt pre-trained CNN models as feature extractor for various visual recognition tasks like object detection [12], object recognition [9, 25, 6], image retrieval [13, 25], etc.", "startOffset": 145, "endOffset": 155}, {"referenceID": 12, "context": "researches adopt pre-trained CNN models as feature extractor for various visual recognition tasks like object detection [12], object recognition [9, 25, 6], image retrieval [13, 25], etc.", "startOffset": 173, "endOffset": 181}, {"referenceID": 24, "context": "researches adopt pre-trained CNN models as feature extractor for various visual recognition tasks like object detection [12], object recognition [9, 25, 6], image retrieval [13, 25], etc.", "startOffset": 173, "endOffset": 181}, {"referenceID": 28, "context": "These developed techniques have shown promising results in comparison to conventional methods using standard feature representations like bag-of-words [29], sparse-coding [34], etc.", "startOffset": 151, "endOffset": 155}, {"referenceID": 33, "context": "These developed techniques have shown promising results in comparison to conventional methods using standard feature representations like bag-of-words [29], sparse-coding [34], etc.", "startOffset": 171, "endOffset": 175}, {"referenceID": 29, "context": "These two are summarized to be the well-known semantic gap [30].", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "[14] employ mid-level features like contour shape, edge shape, color and texture to describe each region for visual recognition tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "It is well known that region features can naturally preserve more mid-level semantic information like materials, textures, shapes, etc of objects [11].", "startOffset": 146, "endOffset": 150}, {"referenceID": 2, "context": "However, traditional region representations either highly depend on segmentation algorithm [3, 23], or lack of a generic semantic representation for regions for various visual recognition tasks.", "startOffset": 91, "endOffset": 98}, {"referenceID": 22, "context": "However, traditional region representations either highly depend on segmentation algorithm [3, 23], or lack of a generic semantic representation for regions for various visual recognition tasks.", "startOffset": 91, "endOffset": 98}, {"referenceID": 30, "context": "(1) We introduce region proposals using algorithm like selective search [31] or edge-box [35] from each input image.", "startOffset": 72, "endOffset": 76}, {"referenceID": 34, "context": "(1) We introduce region proposals using algorithm like selective search [31] or edge-box [35] from each input image.", "startOffset": 89, "endOffset": 93}, {"referenceID": 19, "context": "Since the breakthrough success of CNN models on ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 [20], employing CNN models to other vision tasks becomes popular in the computer vision community.", "startOffset": 112, "endOffset": 116}, {"referenceID": 24, "context": "[25] evaluate the performance of CNN features on several vision tasks, including object recognition, fine-grained object recognition, and image retrieval.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Meanwhile, DeCAF [9] also shows that CNN features work surprisingly well on image classification.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "Subsequently, Babenko et al [5] present a similar idea on im-", "startOffset": 28, "endOffset": 31}, {"referenceID": 20, "context": "[21] consider the labels of reference faces and facecomponents as attributes to describe other faces.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] describe objects using 64 explicitly semantic attribute classifiers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "As one of the most well known work, spatial pyramid matching performs pooling over pyramid of regular grids [22, 34].", "startOffset": 108, "endOffset": 116}, {"referenceID": 33, "context": "As one of the most well known work, spatial pyramid matching performs pooling over pyramid of regular grids [22, 34].", "startOffset": 108, "endOffset": 116}, {"referenceID": 12, "context": "[13] encodes the activations of CNN fully connected layer by VLAD [18], and then concatenates the encoded features over windows at three scale levels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[13] encodes the activations of CNN fully connected layer by VLAD [18], and then concatenates the encoded features over windows at three scale levels.", "startOffset": 66, "endOffset": 70}, {"referenceID": 26, "context": "On the contrary, decision-level cross-region pooling has been applied when there are multiple region/patch candidates [27, 32].", "startOffset": 118, "endOffset": 126}, {"referenceID": 31, "context": "On the contrary, decision-level cross-region pooling has been applied when there are multiple region/patch candidates [27, 32].", "startOffset": 118, "endOffset": 126}, {"referenceID": 11, "context": "Region proposals: Methods for detecting region proposal are used in object detection to avoid exhaustive sliding window search across images and speed up the detection without noticeable loss of recall rates [12].", "startOffset": 208, "endOffset": 212}, {"referenceID": 30, "context": "In the past few years, there have been extensive studies on this topic and many techniques are invented, including selective search [31], edge-boxes [35], BING [7], multiscale combinatorial grouping (MCG) [4], and so on.", "startOffset": 132, "endOffset": 136}, {"referenceID": 34, "context": "In the past few years, there have been extensive studies on this topic and many techniques are invented, including selective search [31], edge-boxes [35], BING [7], multiscale combinatorial grouping (MCG) [4], and so on.", "startOffset": 149, "endOffset": 153}, {"referenceID": 6, "context": "In the past few years, there have been extensive studies on this topic and many techniques are invented, including selective search [31], edge-boxes [35], BING [7], multiscale combinatorial grouping (MCG) [4], and so on.", "startOffset": 160, "endOffset": 163}, {"referenceID": 3, "context": "In the past few years, there have been extensive studies on this topic and many techniques are invented, including selective search [31], edge-boxes [35], BING [7], multiscale combinatorial grouping (MCG) [4], and so on.", "startOffset": 205, "endOffset": 208}, {"referenceID": 15, "context": "[16] evaluates ten region proposal methods, in which selective search and edge-boxes achieved consistently better performance in terms of ground truth recall, repeatability, and detection speed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "(1) Region proposals extraction: We use advanced techniques like selective search [31] or edge-boxes [35] to extract semantical regions, as both of which show satisfactory performance in benchmarks [16].", "startOffset": 82, "endOffset": 86}, {"referenceID": 34, "context": "(1) Region proposals extraction: We use advanced techniques like selective search [31] or edge-boxes [35] to extract semantical regions, as both of which show satisfactory performance in benchmarks [16].", "startOffset": 101, "endOffset": 105}, {"referenceID": 15, "context": "(1) Region proposals extraction: We use advanced techniques like selective search [31] or edge-boxes [35] to extract semantical regions, as both of which show satisfactory performance in benchmarks [16].", "startOffset": 198, "endOffset": 202}, {"referenceID": 30, "context": ", RN} are extracted by region detection algorithms like [31, 35].", "startOffset": 56, "endOffset": 64}, {"referenceID": 34, "context": ", RN} are extracted by region detection algorithms like [31, 35].", "startOffset": 56, "endOffset": 64}, {"referenceID": 15, "context": "Region proposal algorithms usually produce thousands of regions for input images to ensure high recall rate [16].", "startOffset": 108, "endOffset": 112}, {"referenceID": 9, "context": "PASCAL VOC [10] is a very challenging benchmark for object recognition.", "startOffset": 11, "endOffset": 15}, {"referenceID": 25, "context": "In this paper, we adopt the CNN models trained on ImageNet dataset [26], which contains 1.", "startOffset": 67, "endOffset": 71}, {"referenceID": 18, "context": "We make experiments based on the Caffe deep learning framework [19].", "startOffset": 63, "endOffset": 67}, {"referenceID": 1, "context": "After deep attribute feature is extracted, we process the feature with a RootSIFT trick normalization as in [2].", "startOffset": 108, "endOffset": 111}, {"referenceID": 19, "context": "We first compare two CNN models: Alex\u2019s net [20] vs VGG\u2019s net [28].", "startOffset": 44, "endOffset": 48}, {"referenceID": 27, "context": "We first compare two CNN models: Alex\u2019s net [20] vs VGG\u2019s net [28].", "startOffset": 62, "endOffset": 66}, {"referenceID": 15, "context": "We then compare two best region proposal generation algorithms according to [16], i.", "startOffset": 76, "endOffset": 80}, {"referenceID": 30, "context": ", selective search [31] and edge-box [35].", "startOffset": 19, "endOffset": 23}, {"referenceID": 34, "context": ", selective search [31] and edge-box [35].", "startOffset": 37, "endOffset": 41}, {"referenceID": 14, "context": "INRIA [15] 77.", "startOffset": 6, "endOffset": 10}, {"referenceID": 5, "context": "5 CNN S\u2217 [6] 95.", "startOffset": 9, "endOffset": 12}, {"referenceID": 24, "context": "4 CNNaug-SVM [25] 90.", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "2 HCP-1000C\u2217 [32] 95.", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "5 HCP-2000C\u2217 [32] 96.", "startOffset": 13, "endOffset": 17}, {"referenceID": 27, "context": "2 VGG-16-19-Fusion [28] 98.", "startOffset": 19, "endOffset": 23}, {"referenceID": 32, "context": "7 FV+LV-20-VD [33] 97.", "startOffset": 14, "endOffset": 18}, {"referenceID": 31, "context": "HCP-1000C [32] 97.", "startOffset": 10, "endOffset": 14}, {"referenceID": 31, "context": "7 HCP-2000C [32] 97.", "startOffset": 12, "endOffset": 16}, {"referenceID": 27, "context": "2 VGG-16 [28] 99.", "startOffset": 9, "endOffset": 13}, {"referenceID": 27, "context": "0 VGG-16-19-Fusion [28] 99.", "startOffset": 19, "endOffset": 23}, {"referenceID": 32, "context": "3 FV+LV-20-VD [33] 98.", "startOffset": 14, "endOffset": 18}, {"referenceID": 32, "context": "Second is the multiview multi-instance framework by [33] (FV+LV-20-VD), which takes FC layers output from region proposals as feature view and ground truth bounding box as label view, and combine them under a Fisher-vector framework.", "startOffset": 52, "endOffset": 56}, {"referenceID": 24, "context": "CNNaug-SVM [25] OverFeat [27] No Yes FC 77.", "startOffset": 11, "endOffset": 15}, {"referenceID": 26, "context": "CNNaug-SVM [25] OverFeat [27] No Yes FC 77.", "startOffset": 25, "endOffset": 29}, {"referenceID": 5, "context": "2 NA CNN S [6] CNN-S [6] Yes Yes FC 82.", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "2 NA CNN S [6] CNN-S [6] Yes Yes FC 82.", "startOffset": 21, "endOffset": 24}, {"referenceID": 31, "context": "2 HCP-1000C [32] Alex\u2019s [20] Yes No FC 81.", "startOffset": 12, "endOffset": 16}, {"referenceID": 19, "context": "2 HCP-1000C [32] Alex\u2019s [20] Yes No FC 81.", "startOffset": 24, "endOffset": 28}, {"referenceID": 31, "context": "7 HCP-2000C [32] Alex\u2019s [20] Yes Yes FC 85.", "startOffset": 12, "endOffset": 16}, {"referenceID": 19, "context": "7 HCP-2000C [32] Alex\u2019s [20] Yes Yes FC 85.", "startOffset": 24, "endOffset": 28}, {"referenceID": 27, "context": "2 VGG-16-19-Fusion [28] VGG-16+19 [28] No Yes FC 89.", "startOffset": 19, "endOffset": 23}, {"referenceID": 27, "context": "2 VGG-16-19-Fusion [28] VGG-16+19 [28] No Yes FC 89.", "startOffset": 34, "endOffset": 38}, {"referenceID": 32, "context": "3 FV+LV-20-VD [33] CNN-S/M [6] Yes Yes FC 90.", "startOffset": 14, "endOffset": 18}, {"referenceID": 5, "context": "3 FV+LV-20-VD [33] CNN-S/M [6] Yes Yes FC 90.", "startOffset": 27, "endOffset": 30}, {"referenceID": 27, "context": "7 DA VGG-16 [28] No No Soft-max 90.", "startOffset": 12, "endOffset": 16}, {"referenceID": 27, "context": "2 DA+FC1 VGG-16 [28] No No Soft-max+FC 91.", "startOffset": 16, "endOffset": 20}, {"referenceID": 27, "context": "2 DA+FC1\u2217 VGG-16 [28] No No Soft-max+FC 92.", "startOffset": 17, "endOffset": 21}, {"referenceID": 0, "context": "Dense HOG+Coding+Pooling w/o seg [1] 76.", "startOffset": 33, "endOffset": 36}, {"referenceID": 0, "context": "7 Seg+Dense HOG+Coding+Pooling [1] 80.", "startOffset": 31, "endOffset": 34}, {"referenceID": 24, "context": "7 CNN-SVM w/o seg [25] 74.", "startOffset": 18, "endOffset": 22}, {"referenceID": 24, "context": "7 CNNaug-SVM w/o seg [25] 86.", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "The visual instance retrieval task is evaluated on two datasets: (1) Holidays [17] consists of 1491 vacation photographs distributed among 500 groups based on same object or scene.", "startOffset": 78, "endOffset": 82}, {"referenceID": 23, "context": "(2) University of Kentucky Benchmark dataset [24] (UKB) includes 10,200 indoor photographs uniformly from 2550 objects, and each image is used to query the rest.", "startOffset": 45, "endOffset": 49}, {"referenceID": 12, "context": "MOP-CNN [13] 80.", "startOffset": 8, "endOffset": 12}, {"referenceID": 4, "context": "2 Neural Codes [5] 74.", "startOffset": 15, "endOffset": 18}, {"referenceID": 4, "context": "8 Neural Codes+ retrain [5] 79.", "startOffset": 24, "endOffset": 27}, {"referenceID": 24, "context": "3 CNNaug-ss [25] 84.", "startOffset": 12, "endOffset": 16}], "year": 2015, "abstractText": "Recently, many researches employ middle-layer output of convolutional neural network models (CNN) as features for different visual recognition tasks. Although promising results have been achieved in some empirical studies, such type of representations still suffer from the well-known issue of semantic gap. This paper proposes so-called deep attribute framework to alleviate this issue from three aspects. First, we introduce object region proposals as intermedia to represent target images, and extract features from region proposals. Second, we study aggregating features from different CNN layers for all region proposals. The aggregation yields a holistic yet compact representation of input images. Results show that cross-region max-pooling of soft-max layer output outperform all other layers. As softmax layer directly corresponds to semantic concepts, this representation is named \u201cdeep attributes\u201d. Third, we observe that only a small portion of generated regions by object proposals algorithm are correlated to classification target. Therefore, we introduce context-aware region refining algorithm to pick out contextual regions and build contextaware classifiers. We apply the proposed deep attributes framework for various vision tasks. Extensive experiments are conducted on standard benchmarks for three visual recognition tasks, i.e., image classification, fine-grained recognition and visual instance retrieval. Results show that deep attribute approaches achieve state-of-the-art results, and outperforms existing peer methods with a significant margin, even though some benchmarks have little overlap of concepts with the pre-trained CNN models.", "creator": "LaTeX with hyperref package"}}}