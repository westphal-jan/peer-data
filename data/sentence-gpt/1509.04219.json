{"id": "1509.04219", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Sep-2015", "title": "Twitter Sentiment Analysis", "abstract": "This project addresses the problem of sentiment analysis in twitter; that is classifying tweets according to the sentiment expressed in them: positive, negative or neutral. Twitter is an online micro-blogging and social-networking platform which allows users to write short status updates of maximum length 140 characters. It is a rapidly expanding service with over 200 million registered users - out of which 100 million are active users and half of them log on twitter on a daily basis - generating nearly 250 million tweets per day and the best growth in Twitter's traffic. Twitter is currently the most popular user-driven platform of all time (about a third of all users in the US) and is currently the best user-oriented platform on the market. Twitter is also a great example of Twitter's unique brand and the success of its social-networking platform for free. The social media platform was founded in 1989 in the hope that individuals will have the tools to express their opinions and emotions with the tools necessary to communicate their opinions and emotions with other users. The company has around 100 million followers and a user base of over 9 billion followers. For further information on Twitter, please visit https://twitter.com/Twitter_Company\n\n\nAbout Twitter\n\nTwitter is a social network of social and media companies which includes Twitter, LinkedIn, Tumblr, Instagram, Twitter, Twitter, and many more. As a company, Twitter is a social network of social and media companies which includes Twitter, LinkedIn, Tumblr, Instagram, Twitter, and many more. We are proud of the success of its social-networking platform on social networks and as a company, we have reached its strategic goal to attract more and more users. We have now reached the target of our growing customers and will continue to strive to reach millions of users. We have more than 40 million users and we have achieved our goal of bringing more users to our platform, and the goal of our platform is to achieve as many as we can on the platform and to continue to support the many millions of users who come to our platform. We believe that as people and companies, we can continue to offer our platform, providing an alternative way to express themselves.\n\n\nAbout Twitter\nTwitter is a social network of social and media companies which includes Twitter, LinkedIn, Tumblr, Instagram, Twitter, Twitter, and many more. It is the only social media service in the world, and we do not expect any market in the US to be an open marketplace of opinion and information. We focus on making a living in our social and", "histories": [["v1", "Mon, 14 Sep 2015 17:39:37 GMT  (690kb)", "http://arxiv.org/abs/1509.04219v1", "Bachelors Thesis Report"]], "COMMENTS": "Bachelors Thesis Report", "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.SI", "authors": ["afroze ibrahim baqapuri"], "accepted": false, "id": "1509.04219"}, "pdf": {"name": "1509.04219.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Muhammad Usman", "Ali Mustafa Qamar"], "emails": [], "sections": [{"heading": "Twitter Sentiment Analysis", "text": "By\nAfroze Ibrahim Baqapuri (NUST-BEE-310)"}, {"heading": "A Project report submitted in fulfilment", "text": "of the requirement for the degree of\nBachelors in Electrical (Electronics) Engineering"}, {"heading": "Department of Electrical Engineering", "text": ""}, {"heading": "School of Electrical Engineering & Computer Science", "text": ""}, {"heading": "National University of Sciences & Technology", "text": "Islamabad, Pakistan\n2012"}, {"heading": "CERTIFICATE", "text": "It is certified that the contents and form of thesis entitled \u201cTwitter Sentiment Analysis\u201d submitted by Afroze Ibrahim Baqapuri (NUST-BEE-310) have been found satisfactory for the requirement of the degree.\nAdvisor: ______________________________\n(Dr. Muhammad Usman Ilyas)\nCo-Advisor: ______________________________\n(Dr. Ali Mustafa Qamar)"}, {"heading": "DEDICATION", "text": "To Allah the Almighty\n&\nTo my Parents and Faculty"}, {"heading": "ACKNOWLEDGEMENTS", "text": "I am deeply thankful to my advisor and Co-Advisor, Dr. Muhammad Usman\nIlyas, Dr. Ali Mustafa Qamar for helping me throughout the course in accomplishing my final project. Their guidance, support and motivation enabled me in achieving the objectives of the project."}, {"heading": "TABLE OF CONTENTS", "text": "Twitter Sentiment Analysis ............................................................................................. 1 CERTIFICATE ............................................................................................................... 2 ACKNOWLEDGEMENTS ............................................................................................ 4 ABSTRACT .................................................................................................................... 8 INTRODUCTION .......................................................................................................... 9\nMotivation ................................................................................................................... 9 Domain Introduction ................................................................................................. 10 LITERATURE REVIEW ............................................................................................. 13\nLimitations of Prior Art ............................................................................................. 13 Related Work ............................................................................................................. 13 FUNCTIONALITY AND DESIGN ............................................................................. 22\nData Acquisition: ....................................................................................................... 22 Human Labelling: ...................................................................................................... 24\n\u2022 Positive ........................................................................................................... 25\n\u2022 Negative ......................................................................................................... 25\n\u2022 Neutral/Objective ........................................................................................... 25\n\u2022 Ambiguous ..................................................................................................... 25\n\u2022 <Blank> .......................................................................................................... 25 Feature Extraction: .................................................................................................... 28 Classification: ............................................................................................................ 37 TweetMood Web Application: .................................................................................. 39\nTweetScore: .......................................................................................................... 41 TweetCompare: ..................................................................................................... 42 TweetStats: ............................................................................................................ 42\nIMPLEMENTATION AND RESULT DISCUSSION ................................................ 43 CONCLUSION AND FUTURE RECOMMENDATIONS ......................................... 47 REFERENCES ............................................................................................................. 50"}, {"heading": "LIST OF FIGURES", "text": "Figure 1: Using POS Tagging as features for objectivity/subjectivity classification ... 19 Figure 2: Using POS Tagging as features in positive/negative classification .............. 20 Figure 3: Information Gain of Objectivity / Subjectivity Features ............................... 35 Figure 4: Information Gain of Positive / Negative (Polarity) Features......................... 36 Figure 5: 2-d Scater Plot after Step 1 ............................................................................ 39 Figure 6: TweetMood web-application logo ................................................................. 40"}, {"heading": "LIST OF TABLES", "text": "Table 1: A Typical 2x2 Confusion Matrix .................................................................... 12 Table 2: Step 1 results for Objective / Subjective Classification in [16] ...................... 18 Table 3: Step 2 results for Polarity Classification in [16] ............................................. 18 Table 4: Human-Human Agreement in Tweet Labelling ............................................. 26 Table 5: Human- Human Agreement in Verbs / Adjectives Labelling [6] ................... 27 Table 6: Results from Objective / Subjective Classification ........................................ 43 Table 7: Results from Polarity Classification (Positive / Negative) ............................. 44 Table 8: Positive / Negative Classification Results presented by (1-9) ........................ 44 Table 9: Final Results using SVM at Step 2 and Naive Bayes at Step 1 ...................... 45"}, {"heading": "ABSTRACT", "text": "This project addresses the problem of sentiment analysis in twitter; that is classifying tweets according to the sentiment expressed in them: positive, negative or neutral. Twitter is an online micro-blogging and social-networking platform which allows users to write short status updates of maximum length 140 characters. It is a rapidly expanding service with over 200 million registered users [24] - out of which 100 million are active users and half of them log on twitter on a daily basis - generating nearly 250 million tweets per day [20]. Due to this large amount of usage we hope to achieve a reflection of public sentiment by analysing the sentiments expressed in the tweets. Analysing the public sentiment is important for many applications such as firms trying to find out the response of their products in the market, predicting political elections and predicting socioeconomic phenomena like stock exchange. The aim of this project is to develop a functional classifier for accurate and automatic sentiment classification of an unknown tweet stream.\nChapter 1\nINTRODUCTION"}, {"heading": "Motivation", "text": "We have chosen to work with twitter since we feel it is a better approximation of public sentiment as opposed to conventional internet articles and web blogs. The reason is that the amount of relevant data is much larger for twitter, as compared to traditional blogging sites. Moreover the response on twitter is more prompt and also more general (since the number of users who tweet is substantially more than those who write web blogs on a daily basis). Sentiment analysis of public is highly critical in macro-scale socioeconomic phenomena like predicting the stock market rate of a particular firm. This could be done by analysing overall public sentiment towards that firm with respect to time and using economics tools for finding the correlation between public sentiment and the firm\u2019s stock market value. Firms can also estimate how well their product is responding in the market, which areas of the market is it having a favourable response and in which a negative response (since twitter allows us to download stream of geo-tagged tweets for particular locations. If firms can get this information they can analyze the reasons behind geographically differentiated response, and so they can market their product in a more optimized manner by looking for appropriate solutions like creating suitable market segments. Predicting the results of popular political elections and polls is also an emerging application to sentiment analysis. One such study was conducted by Tumasjan et al. in Germany for predicting the outcome of federal elections in which concluded that twitter is a good reflection of offline sentiment [4].\nDomain Introduction This project of analyzing sentiments of tweets comes under the domain of \u201cPattern Classification\u201d and \u201cData Mining\u201d. Both of these terms are very closely related and intertwined, and they can be formally defined as the process of discovering \u201cuseful\u201d patterns in large set of data, either automatically (unsupervised) or semiautomatically (supervised). The project would heavily rely on techniques of \u201cNatural Language Processing\u201d in extracting significant patterns and features from the large data set of tweets and on \u201cMachine Learning\u201d techniques for accurately classifying individual unlabelled data samples (tweets) according to whichever pattern model best describes them.\nThe features that can be used for modeling patterns and classification can be\ndivided into two main groups: formal language based and informal blogging based. Language based features are those that deal with formal linguistics and include prior sentiment polarity of individual words and phrases, and parts of speech tagging of the sentence. Prior sentiment polarity means that some words and phrases have a natural innate tendency for expressing particular and specific sentiments in general. For example the word \u201cexcellent\u201d has a strong positive connotation while the word \u201cevil\u201d possesses a strong negative connotation. So whenever a word with positive connotation is used in a sentence, chances are that the entire sentence would be expressing a positive sentiment. Parts of Speech tagging, on the other hand, is a syntactical approach to the problem. It means to automatically identify which part of speech each individual word of a sentence belongs to: noun, pronoun, adverb, adjective, verb, interjection, etc. Patterns can be extracted from analyzing the frequency distribution of these parts of speech (ether individually or collectively with some other part of speech) in a particular class of labeled tweets. Twitter based features are more informal and relate with how people express themselves on online social platforms and compress their sentiments in the limited space of 140 characters offered by twitter. They include twitter hashtags, retweets, word capitalization, word\nlengthening [13], question marks, presence of url in tweets, exclamation marks, internet emoticons and internet shorthand/slangs.\nClassification techniques can also be divided into a two categories: Supervised\nvs. unsupervised and non-adaptive vs. adaptive/reinforcement techniques. Supervised approach is when we have pre-labeled data samples available and we use them to train our classifier. Training the classifier means to use the pre-labeled to extract features that best model the patterns and differences between each of the individual classes, and then classifying an unlabeled data sample according to whichever pattern best describes it. For example if we come up with a highly simplified model that neutral tweets contain 0.3 exclamation marks per tweet on average while sentiment-bearing tweets contain 0.8, and if the tweet we have to classify does contain 1 exclamation mark then (ignoring all other possible features) the tweet would be classified as subjective, since 1 exclamation mark is closer to the model of 0.8 exclamation marks. Unsupervised classification is when we do not have any labeled data for training. In addition to this adaptive classification techniques deal with feedback from the environment. In our case feedback from the environment can be in form of a human telling the classifier whether it has done a good or poor job in classifying a particular tweet and the classifier needs to learn from this feedback. There are two further types of adaptive techniques: Passive and active. Passive techniques are the ones which use the feedback only to learn about the environment (in this case this could mean improving our models for tweets belonging to each of the three classes) but not using this improved learning in our current classification algorithm, while the active approach continuously keeps changing its classification algorithm according to what it learns at real-time.\nThere are several metrics proposed for computing and comparing the results of\nour experiments. Some of the most popular metrics include: Precision, Recall, Accuracy, F1-measure, True rate and False alarm rate (each of these metrics is calculated individually for each class and then averaged for the overall classifier\nperformance.) A typical confusion table for our problem is given below along with illustration of how to compute our required metric."}, {"heading": "Human says yes tp fn", "text": ""}, {"heading": "Human says no fp tn", "text": "Precision(P) = \ud835\udc95\ud835\udc91 \ud835\udc95\ud835\udc91+\ud835\udc87\ud835\udc91\nRecall(R) = \ud835\udc95\ud835\udc91 \ud835\udc95\ud835\udc91+\ud835\udc87\ud835\udc8f\nAccuracy(A) = \ud835\udc95\ud835\udc91+\ud835\udc95\ud835\udc8f \ud835\udc95\ud835\udc91+\ud835\udc95\ud835\udc8f+\ud835\udc87+\ud835\udc87\ud835\udc91+\ud835\udc87\ud835\udc8f\nF1 = \ud835\udfd0.\ud835\udc77.\ud835\udc79 \ud835\udc77+\ud835\udc79\nTrue Rate(T) = \ud835\udc95\ud835\udc91 \ud835\udc95\ud835\udc91+\ud835\udc87\ud835\udc8f\nFalse-alarm Rate(F) = \ud835\udc87\ud835\udc91 \ud835\udc95\ud835\udc91+\ud835\udc87\ud835\udc8f\nChapter 2"}, {"heading": "LITERATURE REVIEW", "text": ""}, {"heading": "Limitations of Prior Art", "text": "Sentiment analysis of in the domain of micro-blogging is a relatively new research topic so there is still a lot of room for further research in this area. Decent amount of related prior work has been done on sentiment analysis of user reviews [x], documents, web blogs/articles and general phrase level sentiment analysis [16]. These differ from twitter mainly because of the limit of 140 characters per tweet which forces the user to express opinion compressed in very short text. The best results reached in sentiment classification use supervised learning techniques such as Naive Bayes and Support Vector Machines, but the manual labelling required for the supervised approach is very expensive. Some work has been done on unsupervised (e.g., [11] and [13]) and semi-supervised (e.g., [3] and [10]) approaches, and there is a lot of room of improvement. Various researchers testing new features and classification techniques often just compare their results to base-line performance. There is a need of proper and formal comparisons between these results arrived through different features and classification techniques in order to select the best features and most efficient classification techniques for particular applications."}, {"heading": "Related Work", "text": "The bag-of-words model is one of the most widely used feature model for almost all text classification tasks due to its simplicity coupled with good performance. The model represents the text to be classified as a bag or collection of individual words with no link or dependence of one word with the other, i.e. it completely disregards grammar and order of words within the text. This model is also very popular in\nsentiment analysis and has been used by various researchers. The simplest way to incorporate this model in our classifier is by using unigrams as features. Generally speaking n-grams is a contiguous sequence of \u201cn\u201d words in our text, which is completely independent of any other words or grams in the text. So unigrams is just a collection of individual words in the text to be classified, and we assume that the probability of occurrence of one word will not be affected by the presence or absence of any other word in the text. This is a very simplifying assumption but it has been shown to provide rather good performance (for example in [7] and [2]). One simple way to use unigrams as features is to assign them with a certain prior polarity, and take the average of the overall polarity of the text, where the overall polarity of the text could simply be calculated by summing the prior polarities of individual unigrams. Prior polarity of the word would be positive if the word is generally used as an indication of positivity, for example the word \u201csweet\u201d; while it would be negative if the word is generally associated with negative connotations, for example \u201cevil\u201d. There can also be degrees of polarity in the model, which means how much indicative is that word for that particular class. A word like \u201cawesome\u201d would probably have strong subjective polarity along with positivity, while the word \u201cdecent\u201d would although have positive prior polarity but probably with weak subjectivity.\nThere are three ways of using prior polarity of words as features. The simpler\nun-supervised approach is to use publicly available online lexicons/dictionaries which map a word to its prior polarity. The Multi-Perspective-Question-Answering (MPQA) is an online resource with such a subjectivity lexicon which maps a total of 4,850 words according to whether they are \u201cpositive\u201d or \u201cnegative\u201d and whether they have \u201cstrong\u201d or \u201cweak\u201d subjectivity [25]. The SentiWordNet 3.0 is another such resource which gives probability of each word belonging to positive, negative and neutral classes [15]. The second approach is to construct a custom prior polarity dictionary from our training data according to the occurrence of each word in each particular class. For example if a certain word is occurring more often in the positive labelled phrases in our training dataset (as compared to other classes) then we can calculate the\nprobability of that word belonging to positive class to be higher than the probability of occurring in any other class. This approach has been shown to give better performance, since the prior polarity of words is more suited and fitted to a particular type of text and is not very general like in the former approach. However, the latter is a supervised approach because the training data has to be labelled in the appropriate classes before it is possible to calculate the relative occurrence of a word in each of the class. Kouloumpis et al. noted a decrease in performance by using the lexicon word features along with custom n-gram word features constructed from the training data, as opposed to when the n-grams were used alone [7].\nThe third approach is a middle ground between the above two approaches. In\nthis approach we construct our own polarity lexicon but not necessarily from our training data, so we don\u2019t need to have labelled training data. One way of doing this as proposed by Turney et al. is to calculate the prior semantic orientation (polarity) of a word or phrase by calculating it\u2019s mutual information with the word \u201cexcellent\u201d and subtracting the result with the mutual information of that word or phrase with the word \u201cpoor\u201d [11]. They used the number of result hit counts from online search engines of a relevant query to compute the mutual information. The final formula they used is as follows:\n\ud835\udc77\ud835\udc90\ud835\udc8d\ud835\udc82\ud835\udc93\ud835\udc8a\ud835\udc95\ud835\udc9a(\ud835\udc91\ud835\udc89\ud835\udc93\ud835\udc82\ud835\udc94\ud835\udc86) = \ud835\udc8d\ud835\udc90\ud835\udc88\ud835\udfd0 \ud835\udc89\ud835\udc8a\ud835\udc95\ud835\udc94(\ud835\udc91\ud835\udc89\ud835\udc93\ud835\udc82\ud835\udc94\ud835\udc86 \ud835\udc75\ud835\udc6c\ud835\udc68\ud835\udc79 \"excellent\").\ud835\udc89\ud835\udc8a\ud835\udc95\ud835\udc94(\"poor\") \ud835\udc89\ud835\udc8a\ud835\udc95\ud835\udc94(\ud835\udc91\ud835\udc89\ud835\udc93\ud835\udc82\ud835\udc94\ud835\udc86 \ud835\udc75\ud835\udc6c\ud835\udc68\ud835\udc79 \"poor\").\ud835\udc89\ud835\udc8a\ud835\udc95\ud835\udc94(\"\ud835\udc86\ud835\udc99\ud835\udc84\ud835\udc86\ud835\udc8d\ud835\udc8d\ud835\udc86\ud835\udc8f\ud835\udc95\")\nWhere hits(phrase NEAR \u201cexcellent\u201d) means the number documents returned by the search engine in which the phrase (whose polarity is to be calculated) and word \u201cexcellent\u201d are co-occurring. While hits(\u201cexcellent\u201d) means the number of documents retuned which contain the word \u201cexcellent\u201d. Prabowo et al. have gone ahead with this idea and used a seed of 120 positive words and 120 negative to perform the internet searches [12]. So the overall semantic orientation of the word under consideration can be found by calculating the closeness of that word with each one of the seed words and\ntaking and average of it. Another graphical way of calculating polarity of adjectives has been discussed by Hatzivassiloglou et al. [8]. The process involves first identifying all conjunctions of adjectives from the corpus and using a supervised algorithm to mark every pair of adjectives as belonging to the same semantic orientation or different. A graph is constructed in which the nodes are the adjectives and links indicate same or different semantic orientation. Finally a clustering algorithm is applied which divides the graph into two subsets such that nodes within a subset mainly contain links of same orientation and links between the two subsets mainly contain links of different orientation. One of the subsets would contain positive adjectives and the other would contain negative.\nMany of the researchers in this field have used already constructed\npublicly available lexicons of sentiment bearing words (e.g., [7], [12] and [16]) while many others have also explored building their own prior polarity lexicons (e.g., [3], [10] and [11]).\nThe basic problem with the approach of prior polarity approach has been\nidentified by Wilson et al. who distinguish between prior polarity and contextual polarity [16]. They say that the prior polarity of a word may in fact be different from the way the word has been used in the particular context. The paper presented the following phrase as an example:\nPhilip Clapp, president of the National Environment Trust, sums up well the general thrust of the reaction of environmental movements: \u201cThere is no reason at all to believe that the polluters are suddenly going to become reasonable.\u201d\nIn this example all of the four underlined words \u201ctrust\u201d, \u201cwell\u201d, \u201creason\u201d and \u201creasonable\u201d have positive polarities when observed without context to the phrase, but here they are not being used to express a positive sentiment. This concludes that even though generally speaking a word like \u201ctrust\u201d may be used in positive sentences, but this doesn\u2019t rule out the chances of it appearing in non-positive sentences as well.\nHenceforth prior polarities of individual words (whether the words generally carry positive or negative connotations) may alone not enough for the problem. The paper explores some other features which include grammar and syntactical relationships between words to make their classifier better at judging the contextual polarity of the phrase.\nThe task of twitter sentiment analysis can be most closely related to phrase-\nlevel sentiment analysis. A seminal paper on phrase level sentiment analysis was presented in 2005 by Wilson et al. [16] which identified a new approach to the problem by first classifying phrases according to subjectivity (polar) and objectivity (neutral) and then further classifying the subjective-classified phrases as either positive or negative. The paper noticed that many of the objective phrases used prior sentiment bearing words in them, which led to poor classification of especially objective phrases. It claims that if we use a simple classifier which assumes that the contextual polarity of the word is merely equal to its prior polarity gives a result of about 48%. The novel classification process proposed by this paper along with the list of ingenious features which include information about contextual polarity resulted in significant improvement in performance (in terms of accuracy) of the classification process. The results from this paper are presented in the table below:\nFeatures Accuracy Subjective F. Objective F.\nWord tokens 73.6 55.7 81.2\nWords + prior\npolarity\n74.2 60.6 80.7\n28 features 75.9 63.6 82.1\nTable 2: Step 1 results for Objective / Subjective Classification in [16]\nFeatures Accuracy Positive F. Negative F. Both F. Objective F.\nWord tokens"}, {"heading": "61.7 61.2 73.1 14.6 37.7", "text": "Word +\nprior\n63.0 61.6 75.5 14.6 40.7\n10 features 65.7 65.1 77.2 16.1 46.2\nTable 3: Step 2 results for Polarity Classification in [16]\nOne way of alleviating the condition of independence and including partial\ncontext in our word models is to use bigrams and trigrams as well besides unigrams. Bigrams are collection of two contiguous words in a text, and similarly trigrams are collection of three contiguous words. So we could calculate the prior polarity of the bigram / trigram - or the prior probability of that bigram / trigram belonging to a certain class \u2013 instead of prior polarity of individual words. Many researchers have experimented with them with the general conclusion that if we have to use one of them alone unigrams perform the best, while unigrams along with bigrams may give better results with certain classifiers [2], [3]. However trigrams usually result in poor performance as reported by Pak et al. [3]. The reduction in performance by using trigrams is because there is a compromise between capturing more intricate patterns and word coverage as one goes to higher-numbered grams. Besides from this some researchers have tried to incorporate negation into the unigram word models. Pang et al. and Pakl et al. used a model in which the prior polarity of the word was reversed if there was a negation (like \u201cnot\u201d, \u201cno\u201d, \u201cdon\u2019t\u201d, etc.) next to that word [5], [3]. In this way some contextual information is included in the word models.\nGrammatical features (like \u201cParts of Speech Tagging\u201d or POS tagging)\nare also commonly used in this domain. The concept is to tag each word of the tweet in terms of what part of speech it belongs to: noun, pronoun, verb, adjective, adverb, interjections, intensifiers etc. The concept is to detect patterns based on these POS and use them in the classification process. For example it has been reported that objective tweets contain more common nouns and third-person verbs than subjective tweets [3], so if a tweet to be classified has a proportionally large usage of common nouns and verbs in third person, that tweet would have a greater probability of being objective (according to this particular feature). Similarly subjective tweets contain more adverbs, adjectives and interjections [3]. These relationships are demonstrated in the figures below:\nHowever there is still conflict whether Parts-of-Speech are a useful feature for sentiment classification or not. Some researchers argue in favour of good POS features (e.g., [10]) while others not recommending them (e.g., [7]).\nBesides from these much work has been done in exploring a class of features\npertinent only to micro blogging domain. Presence of URL and number of capitalized words/alphabets in a tweet have been explored by Koulompis et al. [7] and Barbosa et al. [10]. Koulmpis also reports positive results for using emoticons and internet slang words as features. Brody et al. does study on word lengthening as a sign of subjectivity in a tweet [13]. The paper reports positive results for their study that the more number of cases a word has of lengthening, the more chance there of that word being a strong indication of subjectivity.\nThe most commonly used classification techniques are the Naive Bayes\nClassifier and State Vector Machines. Some researchers like Barbosa et al. publish better results for SVMs [10] while others like Pak et al. support Naive Bayes [3]. (1-9) and (2-6) also report good results for Maximum Entropy classifier.\nIt has been observed that having a larger training sample pays off to a certain\ndegree, after which the accuracy of the classifier stays almost constant even if we keep adding more labelled tweets in the training data [10]. Barbosa et al. used tweets labelled by internet resources (e.g., [28]), instead of labelling them by hand, for training the classifier. Although there is loss of accuracy of the labelled samples in doing so (which is modelled as increase in noise) but it has been observed that if the accuracy of training labels is greater than 50%, the more the labels, the higher the accuracy of the resulting classifier. So in this way if there are an extremely large number of tweets, the fact that our labels are noisy and inaccurate can be compensated for [10]. On the other hand Pak et al. and Go et al. [2] use presence of positive or negative emoticons to assign labels to the tweets [3]. Like in the above case they used large number of tweets to reduce effect of noise in their training data.\nSome of the earliest work in this field classified text only as positive or\nnegative, assuming that all the data provided is subjective (for example in [2] and [5]). While this is a good assumption for something like movie reviews but when analyzing tweets and blogs there is a lot of objective text we have to consider, so incorporating neutral class into the classification process is now becoming a norm. Some of the work which has included neutral class into their classification process includes [7], [10], [3] and [16].\nThere has also been very recent research of classifying tweets according to the\nmood expressed in them, which goes one step further. Bollen et al. explores this area and develops a technique to classify tweets into six distinct moods: tension, depression, anger, vigour, fatigue and confusion [9]. They use an extended version of Profile of Mood States (POMS): a widely accepted psychometric instrument. They generate a word dictionary and assign them weights corresponding to each of the six mood states, and then they represented each tweet as a vector corresponding to these six dimensions. However not much detail has been provided into how they built their customized lexicon and what technique did they use for classification.\nChapter 3"}, {"heading": "FUNCTIONALITY AND DESIGN", "text": "The process of designing a functional classifier for sentiment analysis can be\nbroken down into five basic categories. They are as follows:\nI. Data Acquisition\nII. Human Labelling\nIII. Feature Extraction IV. Classification\nV. TweetMood Web Application"}, {"heading": "Data Acquisition:", "text": "Data in the form of raw tweets is acquired by using the python library \u201ctweestream\u201d which provides a package for simple twitter streaming API [26]. This API allows two modes of accessing tweets: SampleStream and FilterStream. SampleStream simply delivers a small, random sample of all the tweets streaming at a real time. FilterStream delivers tweet which match a certain criteria. It can filter the delivered tweets according to three criteria:\n\u2022 Specific keyword(s) to track/search for in the tweets\n\u2022 Specific Twitter user(s) according to their user-id\u2019s\n\u2022 Tweets originating from specific location(s) (only for geo-tagged tweets).\nA programmer can specify any single one of these filtering criteria or a multiple combination of these. But for our purpose we have no such restriction and will thus stick to the SampleStream mode.\nSince we wanted to increase the generality of our data, we acquired it in\nportions at different points of time instead of acquiring all of it at one go. If we used the latter approach then the generality of the tweets might have been compromised since a significant portion of the tweets would be referring to some certain trending topic and would thus have more or less of the same general mood or sentiment. This phenomenon has been observed when we were going through our sample of acquired tweets. For example the sample acquired near Christmas and New Year\u2019s had a significant portion of tweets referring to these joyous events and were thus of a generally positive sentiment. Sampling our data in portions at different points in time would thus try to minimize this problem. Thus forth, we acquired data at four different points which would be 17th of December 2011, 29th of December 2011, 19th of January 2012 and 8th of February 2012.\nA tweet acquired by this method has a lot of raw information in it which we\nmay or may not find useful for our particular application. It comes in the form of the python \u201cdictionary\u201d data type with various key-value pairs. A list of some key-value pairs are given below:\n\u2022 Whether a tweet has been favourited\n\u2022 User ID\n\u2022 Screen name of the user\n\u2022 Original Text of the tweet\n\u2022 Presence of hashtags\n\u2022 Whether it is a re-tweet\n\u2022 Language under which the twitter user has registered their account\n\u2022 Geo-tag location of the tweet\n\u2022 Date and time when the tweet was created\nSince this is a lot of information we only filter out the information that we need and discard the rest. For our particular application we iterate through all the tweets in our sample and save the actual text content of the tweets in a separate file given that\nlanguage of the twitter is user\u2019s account is specified to be English. The original text content of the tweet is given under the dictionary key \u201ctext\u201d and the language of user\u2019s account is given under \u201clang\u201d.\nSince human labelling is an expensive process we further filter out the tweets\nto be labelled so that we have the greatest amount of variation in tweets without the loss of generality. The filtering criteria applied are stated below:\n\u2022 Remove Retweets (any tweet which contains the string \u201cRT\u201d)\n\u2022 Remove very short tweets (tweet with length less than 20 characters)\n\u2022 Remove non-English tweets (by comparing the words of the tweets with a list\nof 2,000 common English words, tweets with less than 15% of content matching threshold are discarded)\n\u2022 Remove similar tweets (by comparing every tweet with every other tweet,\ntweets with more than 90% of content matching with some other tweet is discarded)\nAfter this filtering roughly 30% of tweets remain for human labelling on average per sample, which made a total of 10,173 tweets to be labelled."}, {"heading": "Human Labelling:", "text": "For the purpose of human labelling we made three copies of the tweets so that they can be labelled by four individual sources. This is done so that we can take average opinion of people on the sentiment of the tweet and in this way the noise and inaccuracies in labelling can be minimized. Generally speaking the more copies of labels we can get the better it is, but we have to keep the cost of labelling in our mind, hence we reached at the reasonable figure of three.\nWe labelled the tweets in four classes according to sentiments\nexpressed/observed in the tweets: positive, negative, neutral/objective and ambiguous. We gave the following guidelines to our labellers to help them in the labelling process:\n\u2022 Positive: If the entire tweet has a positive/happy/excited/joyful attitude or if\nsomething is mentioned with positive connotations. Also if more than one sentiment is expressed in the tweet but the positive sentiment is more dominant. Example: \u201c4 more years of being in shithole Australia then I move to the USA! :D\u201d.\n\u2022 Negative: If the entire tweet has a negative/sad/displeased attitude or if\nsomething is mentioned with negative connotations. Also if more than one sentiment is expressed in the tweet but the negative sentiment is more dominant. Example: \u201cI want an android now this iPhone is boring :S\u201d.\n\u2022 Neutral/Objective: If the creator of tweet expresses no personal\nsentiment/opinion in the tweet and merely transmits information. Advertisements of different products would be labelled under this category. Example: \u201cUS House Speaker vows to stop Obama contraceptive rule... http://t.co/cyEWqKlE\u201d.\n\u2022 Ambiguous: If more than one sentiment is expressed in the tweet which are\nequally potent with no one particular sentiment standing out and becoming more obvious. Also if it is obvious that some personal opinion is being expressed here but due to lack of reference to context it is difficult/impossible to accurately decipher the sentiment expressed. Example: \u201cI kind of like heroes and don\u2019t like it at the same time...\u201d. Finally if the context of the tweet is not apparent from the information available. Example: \u201cThat\u2019s exactly how I feel about avengers haha\u201d.\n\u2022 <Blank>: Leave the tweet unlabelled if it belongs to some language other than\nEnglish so that it is ignored in the training data.\nBesides this labellers were instructed to keep personal biases out of labelling and make no assumptions, i.e. judge the tweet not from any past extra personal information and only from the information provided in the current individual tweet.\nOnce we had labels from four sources our next step was to combine opinions of\nthree people to get an averaged opinion. The way we did this is through majority vote.\nSo for example if a particular tweet had to two labels in agreement, we would label the overall tweet as such. But if all three labels were different, we labelled the tweet as \u201cunable to reach a majority vote\u201d. We arrived at the following statistics for each class after going through majority voting.\n\u2022 Positive: 2543 tweets\n\u2022 Negative: 1877 tweets\n\u2022 Neutral: 4543 tweets\n\u2022 Ambiguous: 451 tweets\n\u2022 Unable to reach majority vote: 390 tweets\n\u2022 Unlabelled non-English tweets: 369 tweets\nSo if we include only those tweets for which we have been able to achieve a positive, negative or neutral majority vote, we are left with 8963 tweets for our training set. Out of these 4543 are objective tweets and 4420 are subjective tweets (sum of positive and negative tweets).\nWe also calculated the human-human agreement for our tweet labelling task,\nresults of which are as follows:\nIn the above matrix the \u201cstrict\u201d measure of agreement is where all the label assigned by both human beings should match exactly in all cases, while the \u201clenient\u201d measure is in which if one person marked the tweet as \u201cambiguous\u201d and the other marked it as\nsomething else, then this would not count as a disagreement. So in case of the \u201clenient\u201d measure, the ambiguous class could map to any other class. So since the human-human agreement lies in the range of 60-70% (depending upon our definition of agreement), this shows us that sentiment classification is inherently a difficult task even for human beings. We will now look at another table presented by Kim et al. which shows human-human agreement in case labelling individual adjectives and verbs. [14]\nOver here the strict measure is when classification is between the three categories of positive, negative and neutral, while the lenient measure the positive and negative classes into one class, so now humans are only classifying between neutral and subjective classes. These results reiterate our initial claim that sentiment analysis is an inherently difficult task. These results are higher than our agreement results because in this case humans are being asked to label individual words which is an easier task than labelling entire tweets."}, {"heading": "Feature Extraction:", "text": "Now that we have arrived at our training set we need to extract useful features from it which can be used in the process of classification. But first we will discuss some text formatting techniques which will aid us in feature extraction:\n\u2022 Tokenization: It is the process of breaking a stream of text up into words,\nsymbols and other meaningful elements called \u201ctokens\u201d. Tokens can be separated by whitespace characters and/or punctuation characters. It is done so that we can look at tokens as individual components that make up a tweet [19].\n\u2022 Url\u2019s and user references (identified by tokens \u201chttp\u201d and \u201c@\u201d) are removed if\nwe are interested in only analyzing the text of the tweet.\n\u2022 Punctuation marks and digits/numerals may be removed if for example we\nwish to compare the tweet to a list of English words.\n\u2022 Lowercase Conversion: Tweet may be normalized by converting it to\nlowercase which makes it\u2019s comparison with an English dictionary easier.\n\u2022 Stemming: It is the text normalizing process of reducing a derived word to its\nroot or stem [28]. For example a stemmer would reduce the phrases \u201cstemmer\u201d, \u201cstemmed\u201d, \u201cstemming\u201d to the root word \u201cstem\u201d. Advantage of stemming is that it makes comparison between words simpler, as we do not need to deal with complex grammatical transformations of the word. In our case we employed the algorithm of \u201cporter stemming\u201d on both the tweets and the dictionary, whenever there was a need of comparison.\n\u2022 Stop-words removal: Stop words are class of some extremely common words\nwhich hold no additional information when used in a text and are thus claimed to be useless [19]. Examples include \u201ca\u201d, \u201can\u201d, \u201cthe\u201d, \u201che\u201d, \u201cshe\u201d, \u201cby\u201d, \u201con\u201d, etc. It is sometimes convenient to remove these words because they hold no additional information since they are used almost equally in all classes of text, for example when computing prior-sentiment-polarity of words in a tweet according to their frequency of occurrence in different classes and using this\npolarity to calculate the average sentiment of the tweet over the set of words used in that tweet.\n\u2022 Parts-of-Speech Tagging: POS-Tagging is the process of assigning a tag to\neach word in the sentence as to which grammatical part of speech that word belongs to, i.e. noun, verb, adjective, adverb, coordinating conjunction etc.\nNow that we have discussed some of the text formatting techniques employed\nby us, we will move to the list of features that we have explored. As we will see below a feature is any variable which can help our classifier in differentiating between the different classes. There are two kinds of classification in our system (as will be discussed in detail in the next section), the objectivity / subjectivity classification and the positivity / negativity classification. As the name suggests the former is for differentiating between objective and subjective classes while the latter is for differentiating between positive and negative classes.\nThe list of features explored for objective / subjective classification is as below:\n\u2022 Number of exclamation marks in a tweet\n\u2022 Number of question marks in a tweet\n\u2022 Presence of exclamation marks in a tweet\n\u2022 Presence of question marks in a tweet\n\u2022 Presence of url in a tweet\n\u2022 Presence of emoticons in a tweet\n\u2022 Unigram word models calculated using Naive Bayes\n\u2022 Prior polarity of words through online lexicon MPQA\n\u2022 Number of digits in a tweet\n\u2022 Number of capitalized words in a tweet\n\u2022 Number of capitalized characters in a tweet\n\u2022 Number of punctuation marks / symbols in a tweet\n\u2022 Ratio of non-dictionary words to the total number of words in the tweet\n\u2022 Length of the tweet\n\u2022 Number of adjectives in a tweet\n\u2022 Number of comparative adjectives in a tweet\n\u2022 Number of superlative adjectives in a tweet\n\u2022 Number of base-form verbs in a tweet\n\u2022 Number of past tense verbs in a tweet\n\u2022 Number of present participle verbs in a tweet\n\u2022 Number of past participle verbs in a tweet \u2022 Number of 3rd person singular present verbs in a tweet \u2022 Number of non-3rd person singular present verbs in a tweet\n\u2022 Number of adverbs in a tweet\n\u2022 Number of personal pronouns in a tweet\n\u2022 Number of possessive pronouns in a tweet\n\u2022 Number of singular proper noun in a tweet\n\u2022 Number of plural proper noun in a tweet\n\u2022 Number of cardinal numbers in a tweet\n\u2022 Number of possessive endings in a tweet\n\u2022 Number of wh-pronouns in a tweet\n\u2022 Number of adjectives of all forms in a tweet\n\u2022 Number of verbs of all forms in a tweet\n\u2022 Number of nouns of all forms in a tweet\n\u2022 Number of pronouns of all forms in a tweet\nThe list of features explored for positive / negative classification are given below:\n\u2022 Overall emoticon score (where 1 is added to the score in case of positive\nemoticon, and 1 is subtracted in case of negative emoticon)\n\u2022 Overall score from online polarity lexicon MPQA (where presence of strong\npositive word in the tweet increases the score by 1.0 and the presence of weak negative word would decrease the score by 0.5)\n\u2022 Unigram word models calculated using Naive Bayes\n\u2022 Number of total emoticons in the tweet\n\u2022 Number of positive emoticons in a tweet\n\u2022 Number of negative emoticons in a tweet\n\u2022 Number of positive words from MPQA lexicon in tweet\n\u2022 Number of negative words from MPQA lexicon in tweet\n\u2022 Number of base-form verbs in a tweet\n\u2022 Number of past tense verbs in a tweet\n\u2022 Number of present participle verbs in a tweet\n\u2022 Number of past participle verbs in a tweet \u2022 Number of 3rd person singular present verbs in a tweet \u2022 Number of non-3rd person singular present verbs in a tweet\n\u2022 Number of plural nouns in a tweet\n\u2022 Number of singular proper nouns in a tweet\n\u2022 Number of cardinal numbers in a tweet\n\u2022 Number of prepositions or coordinating conjunctions in a tweet\n\u2022 Number of adverbs in a tweet\n\u2022 Number of wh-adverbs in a tweet\n\u2022 Number of verbs of all forms in a tweet\nNext we will give mathematical reasoning of how we calculate the unigram word models using Naive Bayes. The basic concept is to calculate the probability of a word belonging to any of the possible classes from our training sample. Using mathematical formulae we will demonstrate an example of calculating probability of word belong to\nobjective and subjective class. Similar steps would need to be taken for positive and negative classes as well.\nWe will start by calculating the probability of a word in our training data for belonging to a particular class:\nWe now state the Bayes\u2019 rule [19]. According to this rule, if we need to find the probability of whether a tweet is objective, we need to calculate the probability of tweet given the objective class and the prior probability of objective class. The term P(tweet) can be substituted with P(tweet | obj) + P(tweet | subj).\nNow if we assume independence of the unigrams inside the tweet (i.e. the occurrence of a word in a tweet will not affect the probability of occurrence of any other word in the tweet) we can approximate the probability of tweet given the objective class to a mere product of the probability of all the words in the tweet belonging to objective class. Moreover, if we assume equal class sizes for both objective and subjective class we can ignore the prior probability of the objective class. Henceforth we are left with the following formula, in which there are two distinct terms and both of them are easily calculated through the formula mention above.\nNow that we have the probability of objectivity given a particular tweet, we can easily calculate the probability of subjectivity given that same tweet by simply subtracting the earlier term from 1. This is because probabilities must always add to 1. So if we have information of P(obj | tweet) we automatically know P(subj | tweet).\nFinally we calculate P(obj | tweet) for every tweet and use this term as a single feature in our objectivity / subjectivity classification.\nThere are two main potential problems with this approach. First being that if\nwe include every unique word used in the data set then the list of words will be too large making the computation too expensive and time-consuming. To solve this we only include words which have been used at least 5 times in our data. This reduces the size of our dictionary for objective / subjective classification from 11,216 to 2,320. While for positive / negative classification unigram dictionary size is reduced from 6,502 to 1,235 words.\nThe second potential problem is if in our training set a particular word only\nappears in a certain class only and does not appear at all in the other class (for example if the word is misspelled only once). If we have such a scenario then our classifier will always classify a tweet to that particular class (regardless of any other features present in the tweet) just because of the presence of that single word. This is a very harsh approach and results in over-fitting. To avoid this we make use of the technique known as \u201cLaplace Smoothing\u201d. We replace the formula for calculating the probability of a word belonging to a class with the following formula:\nIn this formula \u201cx\u201d is a constant factor called the smoothing factor, which we have arbitrarily selected to be 1. How this works is that even if the count of a word in a particular class is zero, the numerator still has a small value so the probability of a word belonging to some class will never be equal to zero. Instead if the probability would have been zero according to the earlier formula, it would be replace by a very small non-zero probability.\nThe final issue we have in feature selection is choosing the best features from a\nlarge number of features. Our ultimate aim is to achieve the greatest accuracy of our classifier while using least number of features. This is because adding new feature add to the dimensionality of our classification problem and thus add to the complexity of our classifier. This increase in complexity may not necessarily be linear and may even be quadratic so it is preferred to keep the features at a minimum low. Another issue we have with too many features is that our training data may be over-fit and it may confuse the classifier when doing classification on an unknown test set, so the accuracy of the classifier may even decrease. To solve this issue we select the most pertinent features by computing the information-gain of all the features under exploration and then selecting the features with highest information gain. We used WEKA machine learning tool for this task of feature selection [17].\nWe explored a total of 33 features for objectivity / subjectivity classification\nand used WEKA to calculate the information gain from each of these features. The resulting graph is shown below:\nThis graph is basically the super-imposition of 10 different graphs, each one arrived through one fold out of the 10-fold cross validation we performed. Since we see that all the graphs are nicely overlapping so the results each fold are almost the same which shows us that the features we select will perform best in all the scenarios. We selected the best 5 features from this graph which are as follows:\n1. Unigram word models (for prior probabilities of words belonging to objective /\nsubjective classes)\n2. Presence of URL in tweet 3. Presence of emoticons in tweet 4. Number of personal pronouns in tweet 5. Number of exclamation marks in tweet\nSimilarly we explored 22 features for positive / negative classification and used\nWEKA to calculate the information gain from each of these features. The resulting graph is shown below:\nThis graph is basically the super-imposition of 10 different graphs, each one arrived through one fold out of the 10-fold cross validation we performed. Since we see that all the graphs are nicely overlapping so the results each fold are almost the same which shows us that the features we select will perform best in all the scenarios. We selected the best 5 features out of which 2 were redundant features and we were left with only 3 features for our positive / negative classification which are as follows:\n1. Unigram word models (for prior probabilities of words belonging to positive or\nnegative classes)\n2. Number of positive emoticons in tweet 3. Number of negative emoticons in tweet\nThe redundant features we chose ignore because they posed no extra information in presence of the above features are as follows:\n\u2022 Emoticon score for the tweet\n\u2022 MPQA score for the tweet"}, {"heading": "Classification:", "text": "Pattern classification is the process through which data is divided into different classes according to some common patterns which are found in one class which differ to some degree with the patterns found in the other classes. The ultimate aim of our project is to design a classifier which accurately classifies tweets in the following four sentiment classes: positive, negative, neutral and ambiguous.\nThere can be two kinds of sentiment classifications in this area: contextual\nsentiment analysis and general sentiment analysis. Contextual sentiment analysis deals with classifying specific parts of a tweet according to the context provided, for example for the tweet \u201c4 more years of being in shithole Australia then I move to the USA :D\u201d a contextual sentiment classifier would identify Australia with negative sentiment and USA with a positive sentiment. On the other hand general sentiment analysis deals with the general sentiment of the entire text (tweet in this case) as a whole. Thus for the tweet mentioned earlier since there is an overall positive attitude, an accurate general sentiment classifier would identify it as positive. For our particular project we will only be dealing with the latter case, i.e. of general (overall) sentiment analysis of the tweet as a whole.\nThe classification approach generally followed in this domain is a two-step\napproach. First Objectivity Classification is done which deals with classifying a tweet or a phrase as either objective or subjective. After this we perform Polarity Classification (only on tweets classified as subjective by the objectivity classification) to determine whether the tweet is positive, negative or both (some researchers include the both category and some don\u2019t). This was presented by Wilson et al. and reports enhanced accuracy than a simple one-step approach [16].\nWe propose a novel approach which is slightly different from the approach\nproposed by Wilson et al. [16]. We propose that in first step each tweet should undergo two classifiers: the objectivity classifier and the polarity classifier. The former would\ntry to classify a tweet between objective and subjective classes, while latter would do so between the positive and negative classes. We use the short-listed features for these classifications and use the Naive Bayes algorithm so that after the first step we have two numbers from 0 to 1 representing each tweet. One of these numbers is the probability of tweet belonging to objective class and the other number is probability of tweet belonging to positive class. Since we can easily calculate the two remaining probabilities of subjective and negative by simple subtraction by 1, we don\u2019t need those two probabilities.\nSo in the second step we would treat each of these two numbers as separate\nfeatures for another classification, in which the feature size would be just 2. We use WEKA and apply the following Machine Learning algorithms for this second classification to arrive at the best result:\n\u2022 K-Means Clustering\n\u2022 Support Vector Machine\n\u2022 Logistic Regression\n\u2022 K Nearest Neighbours\n\u2022 Naive Bayes\n\u2022 Rule Based Classifiers\nTo better understand how this works we show a plot of actual test set from one of our cross-validations on the 2-dimensional space mentioned above:\nIn this figure the labels are the actual ground truth and the distribution shows how the classified data points are actually scattered throughout the space. As we go right the tweet starts becoming increasingly objective and as we go up the tweet starts becoming more positive. The results for our classification approach are mentioned in the next section of this report."}, {"heading": "TweetMood Web Application:", "text": "We designed a web application which performed real-time sentiment analysis on Twitter on tweets that matched particular keywords provided by the user. For example if a user is interested in performing sentiment analysis on tweets which contain the word \u201cObama\u201d he / she will enter that keyword and the web application will perform the appropriate sentiment analysis and display the results for the user.\nThe url of the website is www.tweet-mood-check.appspot.com and its logo is given below:\nThe web application has been implemented using the Google App Engine\nservice [21] because it can be used as a free web hosting service and it provides a layer of abstraction to the developer from the low level web operations so it is easier to learn. We implemented our algorithm in python and integrated it with GUI for our website using HTML and Javascript using the jinja2 template [23]. We used the Google Visualization Chart API for presenting our results in a graphical, easy-tounderstand manner [22].\nFor acquiring tweets from Twitter we used the REST API in this case [27].\nTwitter REST API provides access to tweets up to around 5 days in past according to the search query we specify. If we used the Twitter Streaming API and the user specified a keyword which is not very common in Twitter, the web application may have to wait for a long time to acquire enough tweets to display reasonable results. In contrast to this it is much simpler to acquire the tweets in a couple of simple URL calls to the Twitter REST API. One limitation of The REST API however is that one call can only give us a maximum of 100 results. Since we apply sentiment analysis on the past 1,000 tweets on any search query (given that there are that many tweets matching with the keyword available), so we have to basically call the API 10 times to get the required number of tweets. This is the basic source of processing delay in our web application.\nWe have three ways of performing sentiment analysis on our website and we\nwill discuss each of them one by one:\n\u2022 TweetScore\n\u2022 TweetCompare\n\u2022 TweetStats"}, {"heading": "TweetScore:", "text": "This feature calculates the popularity score of the keyword which is a number\nfrom 100 to -100. The more positive popularity score suggests that the keyword is highly positively popular on Twitter, while the more negative popularity score suggests that the keyword is highly negatively popular on Twitter. A popularity score close to 0 suggests that the keyword has either mixed opinions or is not a popular topic on Twitter. The popularity score is dependent on two ratios:\n\u2022 Number of positive classified tweets / Number of negative classified tweets\n\u2022 Number of tweets acquired / Time in past needed to explore the REST API\nThe first ratio suggests if the number of positive tweets is larger than negative tweets on a particular keyword, the keyword would have overall popular opinion and vice versa. The second ratio suggests that the lesser time in past we need to explore the REST API to get the 1,000 tweets means that the more number of people are talking about the keyword on Twitter, hence the keyword is popular on Twitter. However it gives no information about the positivity or negativity of the keyword and so higher the second ratio is, the more popularity score from the first ratio is shifted to the extreme ends (away from zero) may it be in positive or negative direction depends on whether there are more number of positive or negative tweets. Finally a maximum of 10 tweets are displayed for each class (positive, negative and neutral) so that the user develops confidence in our classifier."}, {"heading": "TweetCompare:", "text": "This feature compares the popularity score of two or three different keywords\nand replies with which keyword is currently most popular on Twitter. This can have many interesting applications for example having our web application recommend users between movies, songs and products/brands."}, {"heading": "TweetStats:", "text": "This feature is for long term sentiment analysis. We input a number of popular\nkeywords on Twitter on which a backend operation runs after every hour, calculates the popularity score for the tweets generated on that keyword within an hour time frame and stores the results against every hour in a database. We can have a maximum of about 300 such keywords as per Google\u2019s bandwidth requirements. So once we have a reasonable amount of data we can use it to plot graphs of popularity score against time and visualize the effect of change in popularity score with respect to certain events. Once we have collected enough data we can also use it to predict correlation with socio-economic phenomena like stock exchange rates and political elections. Work on this has been done before by Tumasjan et al. [4] and Bollen et al. [9].\nChapter 4"}, {"heading": "IMPLEMENTATION AND RESULT DISCUSSION", "text": "We will first present our results for the objective / subjective and positive /\nnegative classifications. These results act as the first step of our classification approach. We only use the short-listed features for both of these results. This means that for the objective / subjective classification we have 5 features and for positive / negative classification we have 3 features. For both of these results we use the Na\u00efve Bayes classification algorithm, because that is the algorithm we are employing in our actual classification approach at the first step. Furthermore all the figures reported are the result of 10-fold cross validation. We take an average of each of the 10 values we get from the cross validation.\nClasses True\nPositive\nFalse\nPositive\nRecall Precision F-measure\nPositive 0.84 0.19 0.86 0.84 0.85\nNegative 0.81 0.16 0.79 0.81 0.80\nIn addition to the above information, we make a condition while reporting the\nresults of polarity classification (which differentiates between positive and negative classes) that only subjective labelled tweets are used to calculate these results. However, in case of final classification approach, any such condition is removed and basically both objectivity and polarity classifications are applied to all tweets regardless of whether they are labelled objective or subjective.\nIf we compare these results to those provided by Wilson et al. [16] (results are\ndisplayed in Table 2 and Table 3 of this report) we see that although the accuracy of neutral class falls from 82.1% to 73% if we use our classification instead of theirs. However, for all other classes we report significantly greater results. Although the results presented by Wilson et al. are not from Twitter data they are of phrase level sentiment analysis which is very close in concept to Twitter sentiment analysis.\nNext we will compare our results with those presented by Go et al. [2]. The\nresults presented by this paper are as follows:\nIf we compare these results to ours, we see that they are more or less similar.\nHowever, we arrive at comparable results with just 10 features and about 9,000 training data. In contrast to this, they used about 1.6 million noisy labels. Their labels were noisy in the sense that the tweets that contained positive emoticons were labelled as positive, while those with negative emoticons were labelled negative. The rest of the tweets (which did not contain any emoticon) were discarded from the data set. So in this way they hoped to achieve high results without human labelling but at the cost of using humongous large number amount of data set.\nNext we will present our results for the complete classification. We note that\nthe best results are reached through Support Vector Machine being applied at the second stage of the classification process. Hence the results below will only pertain to those of SVM. These results use a total of two features: P(objectivity | tweet) and P(positivity | tweet). But if we include all the features employed in step 1 of the classification process, we have a list of 8 shortlisted features (3 for polarity classification and 5 for objectivity classification). The following results are reported after conducting 10-fold cross validation:\nIn comparison with these results, Koulompis et al. [7] reports average F-\nmeasure of 68%. However when they include another portion of their data into their classification process (which they call the HASH data), their average F-measure drops to 65%. In contrast to this we achieve average F-measure of more than 70% which shows better performance than either of these results. Moreover we make use of only 8 features and 9,000 labelled tweets, while their process involves about 15 features in total and more than 220,000 tweets in their training set. Our unigram word models are also simpler than theirs, because they incorporate negation into their word models. However like in the case of (1-9) their tweets are not labelled by humans, but rather undergo noisy labelling in two ways: labels acquired from positive and negative emoticons and hashtags.\nFinally we conclude that our classification approach provides improvement in\naccuracy by using even the simplest features and small amount of data set. However there are still a number of things we would like to consider as future work which we mention in the next section.\nChapter 5"}, {"heading": "CONCLUSION AND FUTURE RECOMMENDATIONS", "text": "The task of sentiment analysis, especially in the domain of micro-bloging, is\nstill in the developing stage and far from complete. So we propose a couple of ideas which we feel are worth exploring in the future and may result in further improved performance.\nRight now we have worked with only the very simplest unigram models; we\ncan improve those models by adding extra information like closeness of the word with a negation word. We could specify a window prior to the word (a window could for example be of 2 or 3 words) under consideration and the effect of negation may be incorporated into the model if it lies within that window. The closer the negation word is to the unigram word whose prior polarity is to be calculated, the more it should affect the polarity. For example if the negation is right next to the word, it may simply reverse the polarity of that word and farther the negation is from the word the more minimized ifs effect should be.\nApart from this, we are currently only focusing on unigrams and the effect of\nbigrams and trigrams may be explored. As reported in the literature review section when bigrams are used along with unigrams this usually enhances performance.\nHowever for bigrams and trigrams to be an effective feature we need a much more labeled data set than our meager 9,000 tweets.\nRight now we are exploring Parts of Speech separate from the unigram models,\nwe can try to incorporate POS information within our unigram models in future. So say instead of calculating a single probability for each word like P(word | obj) we could instead have multiple probabilities for each according to the Part of Speech the word belongs to. For example we may have P(word | obj, verb), P(word | obj, noun) and P(word | obj, adjective). Pang et al. [5] used a somewhat similar approach and claims that appending POS information for every unigram results in no significant change in performance (with Naive Bayes performing slightly better and SVM having a slight decrease in performance), while there is a significant decrease in accuracy if only adjective unigrams are used as features. However these results are for classification of reviews and may be verified for sentiment analysis on micro blogging websites like Twitter.\nOne more feature we that is worth exploring is whether the information about\nrelative position of word in a tweet has any effect on the performance of the classifier. Although Pang et al. explored a similar feature and reported negative results, their results were based on reviews which are very different from tweets and they worked on an extremely simple model.\nOne potential problem with our research is that the sizes of the three classes are\nnot equal. The objective class which contains 4,543 tweets is about twice the sizes of positive and negative classes which contain 2,543 and 1,877 tweets respectively. The problem with unequal classes is that the classifier tries to increase the overall accuracy of the system by increasing the accuracy of the majority class, even if that comes at the cost of decrease in accuracy of the minority classes. That is the very reason why we report significantly higher accuracies for objective class as opposed to positive or negative classes. To overcome this problem and have the classifier exhibit no bias\ntowards any of the classes, it is necessary to label more data (tweets) so that all three of our classes are almost equal.\nIn this research we are focussing on general sentiment analysis. There is\npotential of work in the field of sentiment analysis with partially known context. For example we noticed that users generally use our website for specific types of keywords which can divided into a couple of distinct classes, namely: politics/politicians, celebrities, products/brands, sports/sportsmen, media/movies/music. So we can attempt to perform separate sentiment analysis on tweets that only belong to one of these classes (i.e. the training data would not be general but specific to one of these categories) and compare the results we get if we apply general sentiment analysis on it instead.\nLast but not the least, we can attempt to model human confidence in our\nsystem. For example if we have 5 human labellers labelling each tweet, we can plot the tweet in the 2-dimensional objectivity / subjectivity and positivity / negativity plane while differentiating between tweets in which all 5 labels agree, only 4 agree, only 3 agree or no majority vote is reached. We could develop our custom cost function for coming up with optimized class boundaries such that highest weightage is given to those tweets in which all 5 labels agree and as the number of agreements start decreasing, so do the weights assigned. In this way the effects of human confidence can be visualized in sentiment analysis."}], "references": [{"title": "Sentiment Knowledge Discovery in Twitter Streaming Data", "author": ["Albert Biffet", "Eibe Frank"], "venue": "Discovery Science, Lecture Notes in Computer Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Twitter Sentiment Classification using Distant Supervision", "author": ["Alec Go", "Richa Bhayani", "Lei Huang"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Twitter as a Corpus for Sentiment Analysis and Opinion Mining", "author": ["Alexander Pak", "Patrick Paroubek"], "venue": "In Proceedings of international conference on Language Resources and Evaluation (LREC),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Predicting Elections with Twitter: What 140 Characters Reveal about Political Sentiment", "author": ["Andranik Tumasjan", "Timm O. Sprenger", "Philipp G. Sandner", "Isabell M. Welpe"], "venue": "In Proceedings of AAAI Conference on Weblogs and Social Media (ICWSM),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Thumbs up? Sentiment Classification using Machine Learning Techniques", "author": ["Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "User Level Sentiment Analysis Incorporating Social Networks", "author": ["Chenhao Tan", "Lilian Lee", "Jie Tang", "Long Jiang", "Ming Zhou", "Ping Li"], "venue": "In Proceedings of ACM Special Interest Group on Knowledge Discovery and Data Mining (SIGKDD),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Twitter Sentiment Analysis: The Good the Bad and the OMG", "author": ["Efthymios Kouloumpis", "Theresa Wilson", "Johanna Moore"], "venue": "In Proceedings of AAAI Conference on Weblogs and Social Media (ICWSM),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Predicting the semantic orientation of adjectives", "author": ["V. Hatzivassiloglou", "McKeown", "K.R"], "venue": "In Proceedings of the 35th Annual Meeting of the ACL and the 8th Conference of the European Chapter of the ACL,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Modelling Public Mood and Emotion: Twitter Sentiment and socio-economic phenomena", "author": ["Johann Bollen", "Alberto Pepe", "Huina Mao"], "venue": "In Proceedings of AAAI Conference on Weblogs and Social Media (ICWSM),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Robust Sentiment Detection on Twitter from Biased and Noisy Data", "author": ["Luciano Barbosa", "Junlan Feng"], "venue": "In Proceedings of the international conference on Computational Linguistics (COLING),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews", "author": ["Peter D. Turney"], "venue": "In Proceedings of the Annual Meeting of the Association of Computational Linguistics (ACL),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Sentiment Analysis: A Combined Approach", "author": ["Rudy Prabowo", "Mike Thelwall"], "venue": "Journal of Infometrics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using Word Lengthening to Detect  Project Thesis Report 52  Sentiment in Microblogs", "author": ["Samuel Brody", "Nicholas Diakopoulus"], "venue": "In Proceedings of Empirical Methods on Natural Language Processing (EMNLP),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Determining the Sentiment of Opinions", "author": ["Soo-Min Kim", "Eduard Hovy"], "venue": "In Proceedings of International Conference on Computational Linguistics (ICCL),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "SENTIWORDNET 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining", "author": ["Stefano Baccianella", "Andrea Esuli", "Fabrizio Sebastiani"], "venue": "In Proceedings of international conference on Language Resources and Evaluation (LREC),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis", "author": ["Theresa Wilson", "Janyce Wiebe", "Paul Hoffmann"], "venue": "In the Annual Meeting of Association of Computational Linguistics: Human Language Technologies (ACL-HLT),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}], "referenceMentions": [{"referenceID": 15, "context": "12 Table 2: Step 1 results for Objective / Subjective Classification in [16] .", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "18 Table 3: Step 2 results for Polarity Classification in [16] .", "startOffset": 58, "endOffset": 62}, {"referenceID": 5, "context": "26 Table 5: Human- Human Agreement in Verbs / Adjectives Labelling [6] .", "startOffset": 67, "endOffset": 70}, {"referenceID": 3, "context": "in Germany for predicting the outcome of federal elections in which concluded that twitter is a good reflection of offline sentiment [4].", "startOffset": 133, "endOffset": 136}, {"referenceID": 12, "context": "11 lengthening [13], question marks, presence of url in tweets, exclamation marks, internet emoticons and internet shorthand/slangs.", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "Decent amount of related prior work has been done on sentiment analysis of user reviews [x], documents, web blogs/articles and general phrase level sentiment analysis [16].", "startOffset": 167, "endOffset": 171}, {"referenceID": 10, "context": ", [11] and [13]) and semi-supervised (e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 12, "context": ", [11] and [13]) and semi-supervised (e.", "startOffset": 11, "endOffset": 15}, {"referenceID": 2, "context": ", [3] and [10]) approaches, and there is a lot of room of improvement.", "startOffset": 2, "endOffset": 5}, {"referenceID": 9, "context": ", [3] and [10]) approaches, and there is a lot of room of improvement.", "startOffset": 10, "endOffset": 14}, {"referenceID": 6, "context": "This is a very simplifying assumption but it has been shown to provide rather good performance (for example in [7] and [2]).", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "This is a very simplifying assumption but it has been shown to provide rather good performance (for example in [7] and [2]).", "startOffset": 119, "endOffset": 122}, {"referenceID": 14, "context": "0 is another such resource which gives probability of each word belonging to positive, negative and neutral classes [15].", "startOffset": 116, "endOffset": 120}, {"referenceID": 6, "context": "noted a decrease in performance by using the lexicon word features along with custom n-gram word features constructed from the training data, as opposed to when the n-grams were used alone [7].", "startOffset": 189, "endOffset": 192}, {"referenceID": 10, "context": "is to calculate the prior semantic orientation (polarity) of a word or phrase by calculating it\u2019s mutual information with the word \u201cexcellent\u201d and subtracting the result with the mutual information of that word or phrase with the word \u201cpoor\u201d [11].", "startOffset": 242, "endOffset": 246}, {"referenceID": 11, "context": "have gone ahead with this idea and used a seed of 120 positive words and 120 negative to perform the internet searches [12].", "startOffset": 119, "endOffset": 123}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": ", [7], [12] and [16]) while many others have also explored building their own prior polarity lexicons (e.", "startOffset": 2, "endOffset": 5}, {"referenceID": 11, "context": ", [7], [12] and [16]) while many others have also explored building their own prior polarity lexicons (e.", "startOffset": 7, "endOffset": 11}, {"referenceID": 15, "context": ", [7], [12] and [16]) while many others have also explored building their own prior polarity lexicons (e.", "startOffset": 16, "endOffset": 20}, {"referenceID": 2, "context": ", [3], [10] and [11]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 9, "context": ", [3], [10] and [11]).", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": ", [3], [10] and [11]).", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": "who distinguish between prior polarity and contextual polarity [16].", "startOffset": 63, "endOffset": 67}, {"referenceID": 15, "context": "[16] which identified a new approach to the problem by first classifying phrases according to subjectivity (polar) and objectivity (neutral) and then further classifying the subjective-classified phrases as either positive or negative.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "18 Table 2: Step 1 results for Objective / Subjective Classification in [16]", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "Table 3: Step 2 results for Polarity Classification in [16]", "startOffset": 55, "endOffset": 59}, {"referenceID": 1, "context": "Many researchers have experimented with them with the general conclusion that if we have to use one of them alone unigrams perform the best, while unigrams along with bigrams may give better results with certain classifiers [2], [3].", "startOffset": 224, "endOffset": 227}, {"referenceID": 2, "context": "Many researchers have experimented with them with the general conclusion that if we have to use one of them alone unigrams perform the best, while unigrams along with bigrams may give better results with certain classifiers [2], [3].", "startOffset": 229, "endOffset": 232}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": ") next to that word [5], [3].", "startOffset": 20, "endOffset": 23}, {"referenceID": 2, "context": ") next to that word [5], [3].", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "For example it has been reported that objective tweets contain more common nouns and third-person verbs than subjective tweets [3], so if a tweet to be classified has a proportionally large usage of common nouns and verbs in third person, that tweet would have a greater probability of being objective (according to this particular feature).", "startOffset": 127, "endOffset": 130}, {"referenceID": 2, "context": "Similarly subjective tweets contain more adverbs, adjectives and interjections [3].", "startOffset": 79, "endOffset": 82}, {"referenceID": 9, "context": ", [10]) while others not recommending them (e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 6, "context": ", [7]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 6, "context": "[7] and Barbosa et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "does study on word lengthening as a sign of subjectivity in a tweet [13].", "startOffset": 68, "endOffset": 72}, {"referenceID": 9, "context": "publish better results for SVMs [10] while others like Pak et al.", "startOffset": 32, "endOffset": 36}, {"referenceID": 2, "context": "support Naive Bayes [3].", "startOffset": 20, "endOffset": 23}, {"referenceID": 9, "context": "21 It has been observed that having a larger training sample pays off to a certain degree, after which the accuracy of the classifier stays almost constant even if we keep adding more labelled tweets in the training data [10].", "startOffset": 221, "endOffset": 225}, {"referenceID": 9, "context": "So in this way if there are an extremely large number of tweets, the fact that our labels are noisy and inaccurate can be compensated for [10].", "startOffset": 138, "endOffset": 142}, {"referenceID": 1, "context": "[2] use presence of positive or negative emoticons to assign labels to the tweets [3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[2] use presence of positive or negative emoticons to assign labels to the tweets [3].", "startOffset": 82, "endOffset": 85}, {"referenceID": 1, "context": "Some of the earliest work in this field classified text only as positive or negative, assuming that all the data provided is subjective (for example in [2] and [5]).", "startOffset": 152, "endOffset": 155}, {"referenceID": 4, "context": "Some of the earliest work in this field classified text only as positive or negative, assuming that all the data provided is subjective (for example in [2] and [5]).", "startOffset": 160, "endOffset": 163}, {"referenceID": 6, "context": "Some of the work which has included neutral class into their classification process includes [7], [10], [3] and [16].", "startOffset": 93, "endOffset": 96}, {"referenceID": 9, "context": "Some of the work which has included neutral class into their classification process includes [7], [10], [3] and [16].", "startOffset": 98, "endOffset": 102}, {"referenceID": 2, "context": "Some of the work which has included neutral class into their classification process includes [7], [10], [3] and [16].", "startOffset": 104, "endOffset": 107}, {"referenceID": 15, "context": "Some of the work which has included neutral class into their classification process includes [7], [10], [3] and [16].", "startOffset": 112, "endOffset": 116}, {"referenceID": 8, "context": "explores this area and develops a technique to classify tweets into six distinct moods: tension, depression, anger, vigour, fatigue and confusion [9].", "startOffset": 146, "endOffset": 149}, {"referenceID": 13, "context": "[14]", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Table 5: Human- Human Agreement in Verbs / Adjectives Labelling [6]", "startOffset": 64, "endOffset": 67}, {"referenceID": 15, "context": "and reports enhanced accuracy than a simple one-step approach [16].", "startOffset": 62, "endOffset": 66}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] and Bollen et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16] (results are displayed in Table 2 and Table 3 of this report) we see that although the accuracy of neutral class falls from 82.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] reports average Fmeasure of 68%.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] used a somewhat similar approach and claims that appending POS information for every unigram results in no significant change in performance (with Naive Bayes performing slightly better and SVM having a slight decrease in performance), while there is a significant decrease in accuracy if only adjective unigrams are used as features.", "startOffset": 0, "endOffset": 3}], "year": 2012, "abstractText": null, "creator": "Acrobat PDFMaker 10.1 for Word"}}}