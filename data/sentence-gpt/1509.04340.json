{"id": "1509.04340", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Sep-2015", "title": "Voted Kernel Regularization", "abstract": "This paper presents an algorithm, Voted Kernel Regularization , that provides the flexibility of using potentially very complex kernel functions such as predictors based on much higher-degree polynomial kernels, while benefitting from strong learning guarantees. The success of our algorithm arises from derived bounds that suggest a new regularization penalty in terms of the Rademacher complexities of the corresponding families of kernel maps. In a series of experiments we demonstrate the improved performance of our algorithm as compared to baselines. We show that Voted Kernel Regularization (Voted Kernel Regularization ) is now available from a searchable GitHub repository.\n\n\n\nThis paper describes the performance of Voted Kernel Regularization using a variety of functions and the benefits of Voted Kernel Regularization.\nAs a result, the Voted Kernel Regularization algorithm can now be used as an efficient and flexible replacement of the R2-based lambda functions that were introduced in 2002 by JSF. We provide an alternative for those using the existing R2-based lambda functions (the R2-based lambda functions, R3-based lambda functions, R4-based lambda functions, R5-based lambda functions, R6-based lambda functions). This paper discusses the improved performance of the R2-based lambda functions, with the potential for a more efficient and flexible replacement of the R2-based lambda functions.\nThe following papers demonstrate the improved performance of our algorithm as compared to baselines.", "histories": [["v1", "Mon, 14 Sep 2015 21:58:43 GMT  (21kb)", "http://arxiv.org/abs/1509.04340v1", "16 pages"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["corinna cortes", "prasoon goyal", "vitaly kuznetsov", "mehryar mohri"], "accepted": false, "id": "1509.04340"}, "pdf": {"name": "1509.04340.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["corinna@google.com,", "pg1338@nyu.edu,", "vitaly@cims.nyu.edu,", "mohri@cs.nyu.edu."], "sections": [{"heading": null, "text": "ar X\niv :1\n50 9.\n04 34\n0v 1\n[ cs\n.L G\n] 1\n4 Se\np 20\n15"}, {"heading": "1. INTRODUCTION", "text": "The hypothesis returned by learning algorithms such as SVMs [Cortes and Vapnik, 1995] and other algorithms for which the representer theorem holds is a linear combination of functions K(x, \u00b7), where K is the kernel function used and x is a training sample. The generalization guarantees for SVMs depend on the sample size and the margin, but also on the complexity of the kernel function K used, measured by its trace [Koltchinskii and Panchenko, 2002].\nThese guarantees suggest that, for a moderate margin, learning with very complex kernels, such as sums of polynomial kernels of degree up to some large d may lead to overfitting, which frequently is observed empirically. Thus, in practice, simpler kernels are typically used, that is small ds for sums of polynomial kernels. On the other hand, to achieve a sufficiently high performance in challenging learning tasks, it may be necessary to augment a linear combination of such functions K(x, \u00b7) with a function K \u2032(x, \u00b7), where K \u2032 is possibly a substantially more complex kernel, such as a polynomial kernel of degree d\u2032 \u226b d. This flexibility is not available when using SVMs or other learning algorithms such as kernel Perceptron [Aizerman et al., 1964, Rosenblatt, 1958] with the same solution form: either a complex kernel function K \u2032 is used and then there is risk of overfitting, or a potentially too simple kernel K is used limiting the performance that could be achieved in some tasks.\nThis paper presents an algorithm, Voted Kernel Regularization , that precisely provides the flexibility of using potentially very complex kernel functions such as predictors based on much higherdegree polynomial kernels, while benefitting from strong learning guarantees. In a series of experiments we demonstrate the improved performance of our algorithm.\nGOOGLE RESEARCH, 111 8TH AVENUE, NEW YORK, NY 10011 COURANT INSTITUTE OF MATHEMATICAL SCIENCES, 251 MERCER STREET, NEW YORK, NY 10012 COURANT INSTITUTE OF MATHEMATICAL SCIENCES, 251 MERCER STREET, NEW YORK, NY 10012 COURANT INSTITUTE AND GOOGLE RESEARCH, 251 MERCER STREET, NEW YORK, NY 10012 E-mail addresses: corinna@google.com, pg1338@nyu.edu, vitaly@cims.nyu.edu,\nmohri@cs.nyu.edu. 1\nWe present data-dependent learning bounds for this algorithm that are expressed in terms of the Rademacher complexities of the reproducing kernel Hilbert spaces (RHKS) of the kernel functions used. These results are based on the framework of Voted Risk Minimization originally introduced by Cortes et al. [2014] for ensemble methods. We further extend these results using local Rademacher complexity analysis to show that faster convergence rates are possible when the spectrum of the kernel matrix is controlled. The success of our algorithm arises from these bounds that suggest a new regularization penalty in terms of the Rademacher complexities of the corresponding families of kernel maps. Therefore, it becomes crucial to have a good estimate of these complexity measures. We provide a thorough theoretical analysis of these complexities for several commonly used kernel classes.\nBesides the improved performance and the theoretical guarantees Voted Kernel Regularization admits a number of additional favorable properties. Our formulation leads to a convex optimization problem that can be solved either via Linear Programming or using Coordinate Descent. Voted Kernel Regularization does not require the kernel functions to be positive-definite or even symmetric. This enables the use of much richer families of kernel functions. In particular, some standard distances known not to be PSD such as the edit-distance and many others can be used with this algorithm.\nYet another advantage of our algorithm is that it produces highly sparse solutions providing greater efficiency and less memory needs. In that respect, Voted Kernel Regularization is similar to so-called norm-1 SVM [Vapnik, 1998, Zhu et al., 2003] and Any-Norm-SVM [Dekel and Singer, 2007] which all use a norm-penalty to reduce the number of support vectors. However, to the best of our knowledge these regularization terms on their own has not led to performance improvement over regular SVMs [Zhu et al., 2003, Dekel and Singer, 2007]. In contrast, our experimental results show that Voted Kernel Regularization algorithm can outperform both regular SVM and norm-1 SVM, and at the same time significantly reduce the number of support vectors. In other work hybrid regularization schemes are combined to obtain a performance improvement [Zou, 2007]. Possibly this technique could be applied to our Voted Kernel Regularization algorithm as well resulting in additional performance improvements.\nSomewhat related algorithms are learning kernels or multiple kernel learning and has been extensively investigated over the last decade by both algorithmic and theoretical studies [Lanckriet et al., 2004, Argyriou et al., 2005, 2006, Srebro and Ben-David, 2006, Lewis et al., 2006, Zien and Ong, 2007, Micchelli and Pontil, 2005, Jebara, 2004, Bach, 2008, Ong et al., 2005, Ying and Campbell, 2009, Cortes et al., 2010]. In learning kernels, training data is used to select a single kernel out of the family of convex combinations of p base kernels and to learn a predictor based on just one kernel. In contrast in Voted SVM, every training point can be thought of as representing a different kernel. Another related approach is Ensemble SVM [Cortes et al., 2011], where a predictor for each base kernel is used and these predictors are combined in to define a single predictor, these two tasks being performed in a single stage or in two subsequent stages. The algorithm where the task is performed in a single stage bears the most resemblance with our Voted Kernel Regularization . However the regularization is different and most importantly not capacity-dependent.\nThe rest of the paper is organized as follows. Some preliminary definitions and notation are introduced in Section 2. The Voted Kernel Regularization algorithm is presented in Section 3 and in Section 4 we provide strong data-dependent learning guarantees for this algorithm showing that it is possible to learn with highly complex kernel classes and yet not overfit. In Section 4, we also prove local complexity bounds that detail how faster convergence rates are possible provided that the spectrum of the kernel matrix is controlled. Section 5 discusses the implementation of the Voted\nKernel Regularization algorithm including optimization procedures and analysis of Rademacher complexities. We conclude with experimental results in Section 6."}, {"heading": "2. PRELIMINARIES", "text": "Let X denote the input space. We consider the familiar supervised learning scenario. We assume that training and test points are drawn i.i.d. according to some distribution D over X \u00d7 {\u22121,+1} and denote by S = ((x1, y1), . . . , (xm, ym)) a training sample of size m drawn according to Dm.\nLet \u03c1 > 0. For a function f taking values in R, we denote by R(f) its binary classification error, by R\u0302S(f) its empirical error, and by R\u0302S,\u03c1(f) its empirical margin error for the sample S:\nR(f) = E (x,y)\u223cD [1yf(x)\u22640], R\u0302S(f) = E (x,y)\u223cS [1yf(x)\u22640], and R\u0302\u03c1(f) = E (x,y)\u223cS [1yf(x)\u2264\u03c1],\nwhere the notation (x, y) \u223c S indicates that (x, y) is drawn according to the empirical distribution defined by S. We will denote by R\u0302S(H) the empirical Rademacher complexity of a hypothesis set H on the set S of functions mapping X to R, and by Rm(H) the Rademacher complexity [Koltchinskii and Panchenko, 2002, Bartlett and Mendelson, 2002]:\nR\u0302S(H) = 1\nm E \u03c3 [ sup h\u2208H m\u2211\ni=1\n\u03c3ih(xi) ] Rm(H) = E\nS\u223cDm\n[ R\u0302S(H) ] ,\nwhere the random variables \u03c3i are independent and uniformly distributed over {\u22121,+1}."}, {"heading": "3. THE VOTED KERNEL REGULARIZATION ALGORITHM", "text": "In this section, we introduce the Voted Kernel Regularization algorithm. Let K1, . . . , Kp be p positive semi-definite (PSD) kernel functions with \u03bak = supx\u2208X \u221a Kk(x, x) for all k \u2208 [1, p]. We consider p corresponding families of functions mapping from X to R, H1, . . . , Hp, defined by Hk = {x 7\u2192 \u00b1Kk(x, x\u2032) : x\u2032 \u2208 X}, where the sign accounts for two possible ways of classifying a point x\u2032 \u2208 X . The general form of a hypothesis f returned by the algorithm is the following:\nf = m\u2211\nj=1\np\u2211\nk=1\n\u03b1k,jKk(\u00b7, xj),\nwhere \u03b1k,j \u2208 R for all j and k. Thus, f is a linear combination of hypotheses in Hks. This form with many \u03b1s per point is distinctly different from that of learning kernels with only one \u03b1 per point. Since the families Hk are symmetric, this linear combination can be made a non-negative combination. Our algorithm consists of minimizing the Hinge loss on the training sample, as with SVMs, but with a different regularization term that tends to penalize hypotheses drawn from more complex Hks more than those selected from simpler ones and to minimize the norm-1 of the coefficients \u03b1k,j. Let rk denote the empirical Rademacher complexity of Hk: rk = R\u0302S(Hk). Then, the following is the objective function of Voted Kernel Regularization :\nF (\u03b1)= 1\nm\nm\u2211\ni=1\nmax ( 0, 1\u2212yiyj m\u2211\nj=1\np\u2211\nk=1\n\u03b1k,jKk(xi, xj)\n) + m\u2211\nj=1\np\u2211\nk=1\n(\u03bbrk + \u03b2)|\u03b1k,j|, (1)\nwhere \u03bb \u2265 0 and \u03b2 \u2265 0 are parameters of the algorithm. We will adopt the notation \u039bk = \u03bbrk + \u03b2 to simplify the presentation in what follows.\nNote that the objective function F is convex: the Hinge loss is convex thus its composition with an affine function is also convex, which shows that the first term is convex; the second term is\nconvex as the absolute value terms with non-negative coefficients; and F is convex as the sum of these two convex terms. Thus, the optimization problem admits a global minimum. Voted Kernel Regularization returns the function f defined by (3) with coefficients \u03b1 = (\u03b1k,j)k,j minimizing F .\nThis formulation admits several benefits. First, it enables us to learn with very complex hypothesis sets and yet not overfit, thanks to Rademacher complexity-based penalties assigned to coefficients associated to different Hks. We will see later that the algorithm thereby defined benefits from strong learning guarantees. Notice further that the penalties assigned are data-dependent, which is a key feature of the algorithm. Second, observe that the objective function (6) does not require the kernels Kk to be positive-definite or even symmetric. Function F is convex regardless of the kernel properties. This is a significant benefit of the algorithm which enables to extend its use beyond what algorithms such as SVMs require. In particular, some standard distances known not to be PSD such as the edit-distance and many others could be used with this algorithm. Another advantage of this algorithm compared to standard SVM and other \u21132-regularized methods is that \u21131-norm regularization used for Voted Kernel Regularization leads sparse solutions. The solution \u03b1 is typically sparse, which significantly reduces prediction time and the memory needs.\nNote that hypotheses h \u2208 Hk are defined by h(x) = Kk(x, x\u2032) where x\u2032 is an arbitrary element of the input space X . However, our objective only includes those xj that belong to the observed sample. We show that in the case of a PDS kernel, there is no loss of generality in that as we now show. Indeed, observe that for x\u2032 \u2208 X we can write \u03a6k(x\u2032) = w +w\u22a5, where \u03a6k is a feature map associated with the kernel Kk and where w lies in the span of \u03a6k(x1), . . . ,\u03a6k(xm) and w\u22a5 is in orthogonal compliment of this subspace. Therefore, for any sample point xi\nKk(xi, x \u2032) = \u3008\u03a6(xi),\u03a6(x\u2032)\u3009Hk = \u3008\u03a6(xi),w\u3009Hk + \u3008\u03a6(xi),w\u22a5\u3009Hk\n=\nm\u2211\nj=1\n\u03b2j\u3008\u03a6(xi),\u03a6(xj)\u3009Hk = m\u2211\nj=1\n\u03b2jKk(xi, xj),\nwhich leads to objective (1). Note that since selecting \u2212Kk(\u00b7, xj) with weight \u03b1k,j is equivalent to selecting Kk(\u00b7, xj) with \u2212\u03b1k,j , which accounts for the absolute value on the \u03b1k,js in the regularization term.\nThe Voted Kernel Regularization algorithm has some connections with other algorithms previously described in the literature. In the absence of any regularization, that is \u03bb = 0 and \u03b2 = 0, it reduces to the minimization of the Hinge loss and is therefore of course close to the SVM algorithm [Cortes and Vapnik, 1995]. For \u03bb = 0, that is when discarding our regularization based on the different complexity of the hypothesis sets, the algorithm coincides with an algorithm originally described by Vapnik [1998][pp. 426-427], later by several other authors starting with [Zhu et al., 2003], and sometimes referred to as the norm-1 SVM."}, {"heading": "4. LEARNING GUARANTEES", "text": "In this section, we provide strong data-dependent learning guarantees for the Voted Kernel Regularization algorithm.\nLet F denote conv(\u22c3pk=1Hk), that is the family of functions f of the form f = \u2211T\nt=1 \u03b1tht, where \u03b1 = (\u03b11, . . . , \u03b1T ) is in the simplex \u2206 and where, for each t \u2208 [1, T ], Hkt denotes the hypothesis set containing ht, for some kt \u2208 [1, p]. Then, the following learning guarantee holds for all f \u2208 F [Cortes et al., 2014]. Theorem 1. Assume p > 1. Fix \u03c1 > 0. Then, for any \u03b4 > 0, with probability at least 1 \u2212 \u03b4 over the choice of a sample S of size m drawn i.i.d. according to Dm, the following inequality holds for\nall f = \u2211T\nt=1 \u03b1tht \u2208 F :\nR(f) \u2264 R\u0302S,\u03c1(f) + 4\n\u03c1\nT\u2211\nt=1\n\u03b1tRm(Hkt) + 2\n\u03c1\n\u221a log p\nm +\n\u221a\u2308 4\n\u03c12 log [ \u03c12m log p ]\u2309 log p m + log 2 \u03b4 2m .\nThus, R(f) \u2264 R\u0302S,\u03c1(f) + 4\u03c1 \u2211T t=1 \u03b1tRm(Hkt) +O (\u221a log p \u03c12m log [ \u03c12m log p ]) .\nTheorem 1 can be used to derive VKR objective and we provide full details of this derivation in Appendix B. Furthermore, the results of Theorem 1 can further be improved using local Rademacher complexity analysis showing that faster rates of convergence are possible.\nTheorem 2. Assume p > 1. Fix \u03c1 > 0. Then, for any \u03b4 > 0, with probability at least 1 \u2212 \u03b4 over the choice of a sample S of size m drawn i.i.d. according to Dm, the following inequality holds for all f = \u2211T t=1 \u03b1tht \u2208 F for any K > 1:\nR(f)\u2212 K K \u2212 1R\u0302S,\u03c1(f) \u2264 6K 1 \u03c1\nT\u2211\nt=1\n\u03b1tRm(Hkt)\n+ 40 K \u03c12 log p m + 5K\nlog 2 \u03b4\nm + 5K\n\u2308 8\n\u03c12 log\n\u03c12(1 + K K\u22121 )m\n40K log p\n\u2309 log p\nm .\nThus, for K = 2, R(f) \u2264 2R\u0302S,\u03c1(f) + 12\u03c1 \u2211T t=1 \u03b1tRm(Hkt) +O ( log p \u03c12m log ( \u03c1m log p ) + log 1 \u03b4 m ) .\nThe proof of this result is given in Appendix A. Note that O(logm/ \u221a m) in Theorem 1 is replaced with O(logm/m) in Theorem 2. For full hypothesis classes Hks, Rm(Hk) may be on the order of O(1/ \u221a m) and will dominate the bound. However, if we use localized classes Hk(r) = {h \u2208 Hk : E[h2] < r} then for certain values of r\u2217 local Rademacher complexities Rm(Hk(r\u2217)) \u2208 O(1/m) leading to even stronger learning guarantees. Furthermore, this result leads to an extension of Voted Kernel Regularization objective:\nF (\u03b1)= 1\nm\nm\u2211\ni=1\nmax ( 0, 1\u2212yiyj m\u2211\nj=1\np\u2211\nk=1\n\u03b1k,jKk(xi, xj)\n) + m\u2211\nj=1\np\u2211\nk=1\n(\u03bbRm(Hk(s)) + \u03b2)|\u03b1k,j|, (2)\nwhich is optimized over \u03b1 and parameter s is set via cross-validation. In Section 5.3, we provide an explicit expression for the local Rademacher complexities of PDS kernel functions."}, {"heading": "5. OPTIMIZATION SOLUTIONS", "text": "In this section, we propose two different algorithmic approaches to solve the optimization problem (1): a linear programming (LP) and a coordinate descent (CD) approach.\n5.1. Linear Programming (LP) formulation. This section presents a linear programming approach for solving the Voted Kernel Regularization optimization problem (1). Observe that by introducing slack variables \u03bei the optimization can be equivalently written as follows:\nmin \u03b1,\u03be\n1\nm\nm\u2211\ni=1\n\u03bei +\nm\u2211\nj=1\np\u2211\nk=1\n\u039bk|\u03b1k,j| s.t. \u03bei \u2265 1\u2212 m\u2211\nj=1\np\u2211\nk=1\n\u03b1k,jyiyjKk(xi, xj), \u2200i \u2208 [1, m].\nNext, we introduce new variables \u03b1+k,j \u2265 0 and \u03b1\u2212k,j \u2265 0 such that \u03b1k,j = \u03b1+k,j \u2212 \u03b1\u2212k,j . Then, for any k and j, |\u03b1k,j| can be rewritten as |\u03b1k,j| \u2264 \u03b1+k,j + \u03b1\u2212k,j. The optimization problem is therefore equivalent to the following:\nmin \u03b1 +\u22650,\u03b1\u2212\u22650,\u03be\n1\nm\nm\u2211\ni=1\n\u03bei + m\u2211\nj=1\np\u2211\nk=1\n\u039bk(\u03b1 + k,j + \u03b1 \u2212 k,j)\ns.t. \u03bei \u2265 1\u2212 m\u2211\nj=1\np\u2211\nk=1\n(\u03b1+k,j \u2212 \u03b1\u2212k,j)yiyjKk(xi, xj), \u2200i \u2208 [1, m],\nsince conversely, a solution with \u03b1k,j = \u03b1 + k,j \u2212 \u03b1\u2212k,j verifies the condition \u03b1+k,j = 0 or \u03b1\u2212k,j = 0 for any k and j, thus \u03b1k,j = \u03b1 + k,j when \u03b1k,j \u2265 0 and \u03b1k,j = \u03b1\u2212k,j when \u03b1k,j \u2264 0. This is because if \u03b4 = min(\u03b1+k,j, \u03b1 \u2212 k,j) > 0, then replacing \u03b1 + k,j with \u03b1 + k,j \u2212 \u03b4 and \u03b1\u2212k,j with \u03b1\u2212k,j \u2212 \u03b4 would not affect \u03b1+k,j \u2212 \u03b1\u2212k,j but would reduce \u03b1+k,j + \u03b1\u2212k,j . Note that the resulting optimization problem is an LP problem since the objective function is linear in both \u03beis and \u03b1+, \u03b1\u2212, and since the constraints are affine. There is a battery of wellestablished methods to solve this LP problem including interior-point methods and the simplex algorithm. An additional advantage of this formulation of the Voted Kernel Regularization algorithm is that there is a large number of generic software packages for solving LPs making the Voted Kernel Regularization algorithm easier to implement.\n5.2. Coordinate Descent (CD) formulation. An alternative approach for solving the Voted Kernel Regularization optimization problem (1) consists of using a coordinate descent method. The advantage of such a formulation over the LP formulation is that there is no need to explicitly store the whole vector of \u03b1s but rather only non-zero entries. This enables learning with very large number of base hypotheses including scenarios in which the number of base hypotheses is infinite. The full description of the algorithm is given in Appendix C.\n5.3. Complexity penalties. An additional benefit of the learning bounds presented in Section 4 is that they are data-dependent. They are based on the Rademacher complexity rks of the base hypothesis sets Hk, which in some cases can be well estimated from the training sample. Our formulation directly inherits this advantage. However, in certain cases computing or estimating complexities r1, . . . , rj may be costly. In this section, we discuss various upper bounds on these complexities that be can used in practice for efficient implementation of the Voted Kernel Regularization algorithm.\nNote that the hypothesis set Hk = {x 7\u2192 \u00b1Kk(x, x\u2032) : x\u2032 \u2208 X} is of course distinct from the RKHS Hk of the kernel Kk. Thus, we cannot use the known upper bound on R\u0302S(Hk) to bound R\u0302S(Hk). Nevertheless our proof of the upper bound is similar and leads to a similar upper bound.\nLemma 3. Let Kk be the kernel matrix of the PDS kernel function Kk for the sample S and let \u03bak = supx\u2208X \u221a Kk(x, x). Then, the following inequality holds:\nR\u0302S(Hk) \u2264 \u03bak \u221a Tr[Kk]\nm .\nWe present the full proof of this result in Appendix A. Observe that the expression given by the lemma can be precomputed and used as the parameter rk of the optimization procedure.\nThe upper bound just derived is not fine enough to distinguish between different normalized kernels since for any normalized kernel Kk, \u03ba = 1 and Tr[Kk] = m. In that case, finer bounds in\nterms of localized complexities can be used. In particular, local Rademacher complexity of a set of functions H id defined as Rlocm (H, r) = Rm({h \u2208 H : E[h2] \u2264 r}). If (\u03bbi)\u221ei=1 is a sequence of eigenvalues associated with the kernel Kk then once can show [Mendelson, 2003, Bartlett et al.,\n2005] that for every r > 0,Rlocm (H, r) \u2264 \u221a 2 m min\u03b8\u22650 ( \u03b8r + \u2211 j>\u03b8 \u03bbj ) = \u221a 2 m \u2211\u221e j=1min(r, \u03bbj).\nFurthermore, there is an absolute constant c such that if \u03bb1 \u2265 1m , then for every r \u2265 1m , c\u221a m \u221e\u2211\nj=1\n(r, \u03bbj) \u2264 Rlocm (H, r).\nNote that taking r = \u221e recovers earlier bound Rm(Hk) \u2264 \u221a\nTr[Kk]/m. On the other hand one can show that for instance in the case of Gaussian kernels Rlocm (H, r) = O( \u221a r m log(1/r)) and using the fixed point of this function leads to Rlocm (H, r) = O( logm m\n). These results can be used in conjunction with the local Rademacher complexity extension of Voted Kernel Regularization discussed in Section 4.\nIf all of the kernels belong to the same family such as, for example, polynomial or Gaussian kernels it may be desirable to use measures of complexity that would account for specific properties of the given family of kernels such polynomial degree or bandwidth of the Gaussian. Below we discuss several additional upper bounds that aim to address these questions.\nFor instance, if Kk is a polynomial kernel of degree k, then we can use an upper bound on the Rademacher complexity of Hk in terms of the square-root of its pseudo-dimension Pdim(Hk), which coincides with the dimension dk of the feature space corresponding to a polynomial kernel of degree k, which is given by\ndk =\n( N + k\nk\n) \u2264 (N + k) k\nk! \u2264\n( (N + k)e\nk\n)k . (3)\nLemma 4. Let Kk be a polynomial kernel of degree k. Then, the empirical Rademacher complexity\nof Hk can be upper bounded as R\u0302S(Hk) \u2264 12\u03ba2k \u221a \u03c0dk m .\nThe proof of this result is in Appendix A Thus, in view of the lemma, we can use rk = \u03ba2k \u221a dk as a complexity penalty in the formulation of the Voted Kernel Regularization algorithm with polynomial kernels, with dk given by the expression (3)."}, {"heading": "6. EXPERIMENTS", "text": "We experimented with several benchmark datasets from the UCI repository, specifically breastcancer, climate, diabetes,german(numeric),ionosphere,musk, ocr49, phishing,retinopathy, vertebral and waveform01. Here, ocr49 refers to the subset of the OCR dataset with classes 4 and 9, and similarly waveform01 refers to the subset of waveform dataset with classes 0 and 1. More details on all the datasets are given in Table 2 in Appendix D.\nOur experiments compared Voted Kernel Regularization to regular SVM, that we refer to as L2-SVM, and to norm-1 SVM, called L1-SVM. In all of our experiments, we used lp solve, an off-the-shelf LP solver, to solve the Voted Kernel Regularization and L1-SVM optimization problems. For L2-SVM, we used LibSVM.\nIn each of the experiments, we used standard 5-fold cross-validation for performance evaluation and model selection. In particular, each dataset was randomly partitioned into 5 folds, and each algorithm was run 5 times, with a different assignment of folds to the training set, validation set\nand test set for each run. Specifically, for each i \u2208 {0, . . . , 4}, fold i was used for testing, fold i + 1 (mod 5) was used for validation, and the remaining folds were used for training. For each setting of the parameters, we computed the average validation error across the 5 folds, and selected the parameter setting with minimum average validation error. The average error across the 5 folds was then computed for this particular parameter setting.\nIn the first set of experiments we used polynomial kernels of the form Kk(x,y) = (xTy + 1)k. We report the results in Table 6. For Voted Kernel Regularization , we optimized over \u03bb \u2208 {10\u2212i : i = 0, . . . , 6} and \u03b2 \u2208 {10\u2212i : i = 0, . . . , 6} The family of kernel functions Hk for k \u2208 [1, 10] was chosen to be the set of polynomial kernels of degree k. In our experiments we compared the bounds of both Lemma 3 and Lemma 4 used as an estimate of the Rademacher complexity. For L1-SVM, we cross-validated over degrees in range 1 through 10 and \u03b2 in the same range as for Voted Kernel Regularization . Cross-validation for L2-SVM was also done over the degree and regularization parameter C \u2208 {10i : i = \u22124, . . . , 7}.\nOn 5 out of 11 datasets Voted Kernel Regularization outperformed L2-SVM and L1-SVM with a considerable improvement on 3 data sets. On the rest of the datasets, there was no statistical difference between these algorithms. Note that our results are also consistent with previous studies that indicated that L1-SVM and L2-SVM often have comparable performance. Observe that solutions obtained by Voted Kernel Regularization are often up to 10 times sparser then those of L2-SVM. In other words, Voted Kernel Regularization has a benefit of sparse solutions and often\nan improved performance, which provides strong empirical evidence in the support of our formulation. In a second set of experiments we used families of Gaussian kernels based on distinct values of the parameter \u03b3 \u2208 {10i : i = \u22126, . . . , 0}. We used the bound of Lemma 3 as an estimate of the Rademacher complexity. In our cross-validation we used the same range for \u03bb and \u03b2 parameters of Voted Kernel Regularization and L1-SVM algorithms. For L2-SVM we increased the range of the regularization parameter: C \u2208 {10i : i = \u22124, . . . , 7}. The results of our experiments are comparable to the results with polynomial kernels, however, improvements obtained by Voted Kernel Regularization are not always as significant in this case. The sparseness of the solutions are comperable to those observed with polynomial kernels."}, {"heading": "7. CONCLUSION", "text": "In this paper we presented a new support vector algorithm - Voted Kernel Regularization . Our algorithm benefits from strong data-dependent learning guarantees that enable learning with highly complex feature maps and yet not overfit. We further improved these learning guarantees using local complexity analysis leading to an extension of Voted Kernel Regularization algorithm. The key ingredient of our algorithm is a new regularization term that makes use of the Rademacher complexities of different families of kernel functions used by the Voted Kernel Regularization algorithm. We provide a thorough analysis of several different alternatives that can be used for this approximation. We also provide two practical implementations of our algorithm based on linear programming and coordinate descent. Finally, we presented results of extensive experiments that show that our algorithm always finds solutions that are much sparse than those of the other support vector algorithms and at the same time often outperforms other formulations."}, {"heading": "APPENDIX A. PROOFS OF LEARNING GUARANTEES", "text": "Theorem 2. Assume p > 1. Fix \u03c1 > 0. Then, for any \u03b4 > 0, with probability at least 1 \u2212 \u03b4 over the choice of a sample S of size m drawn i.i.d. according to Dm, the following inequality holds for all f = \u2211T t=1 \u03b1tht \u2208 F for any K > 1:\nR(f)\u2212 K K \u2212 1R\u0302S,\u03c1(f) \u2264 6K 1 \u03c1\nT\u2211\nt=1\n\u03b1tRm(Hkt)\n+ 40 K \u03c12 log p m + 5K\nlog 2 \u03b4\nm + 5K\n\u2308 8\n\u03c12 log\n\u03c12(1 + K K\u22121 )m\n40K log p\n\u2309 log p\nm .\nThus, for K = 2, R(f) \u2264 2R\u0302S,\u03c1(f) + 12\u03c1 \u2211T t=1 \u03b1tRm(Hkt) +O ( log p \u03c12m log ( \u03c1m log p ) + log 1 \u03b4 m ) .\nProof. For a fixed h = (h1, . . . , hT ), any \u03b1 \u2208 \u2206 defines a distribution over {h1, . . . , hT}. Sampling from {h1, . . . , hT} according to \u03b1 and averaging leads to functions g of the form g = 1 n \u2211T i=1 ntht for some n = (n1, . . . , nT ), with \u2211T t=1 nt = n, and ht \u2208 Hkt .\nFor any N = (N1, . . . , Np) with |N| = n, we consider the family of functions\nGF ,N =\n{ 1\nn\np\u2211\nk=1\nNk\u2211\nj=1\nhk,j |\u2200(k, j) \u2208 [p]\u00d7 [Nk], hk,j\u2208Hk } ,\nand the union of all such families GF ,n = \u22c3\n|N|=nGF ,N. Fix \u03c1 > 0. We define a class \u03a6 \u25e6GF ,N = {\u03a6\u03c1(g) : g \u2208 GF ,N} and Gr = G\u03a6,F ,N,r = {r\u2113g/max(r,E[\u2113g] : \u2113g \u2208 \u03a6 \u25e6GF ,N} for r to be chosen later. Observe that for vg \u2208 G\u03a6,F ,N,r Var[vg] \u2264 r. Indeed, if r > E[\u2113g] then vg = \u2113g. Otherwise, Var[vg] = r 2Var[\u2113g]/(E[\u2113g]) 2 \u2264 r(E[\u21132g])/E[\u2113g] \u2264 r.\nBy Theorem 2.1 in Bartlett et al. [2005], for any \u03b4 > 0 with probability at least 1 \u2212 \u03b4, for any 0 < \u03b2 < 1,\nV \u2264 2(1 + \u03b2)Rm(G\u03a6,F ,N,r) +\n\u221a 2r log 1\n\u03b4 m + (1 3 + 1 \u03b2 ) log 1 \u03b4 m ,\nwhere V = supv\u2208Gr (E[v]\u2212 En[v]) and \u03b2 is a free parameter. Next we observe that if Rm(G\u03a6,F ,N,r) \u2264 Rm({\u03b1\u2113g : g \u2208 \u03a6\u25e6GF ,N, \u03b1 \u2208 [0, 1]}) = Rm(\u03a6\u25e6GF ,N). Therefore, using Talagrand\u2019s contraction lemma and convexity we have that Rm(G\u03a6,F ,N,r) \u2264 1\u03c1 \u2211p k=1 Nk n Rm(Hk). It follows that for any \u03b4 > 0 with probability at least 1\u2212 \u03b4, for all 0 < \u03b2 < 1\nV \u2264 2(1 + \u03b2)1 \u03c1\np\u2211\nk=1\nNk n Rm(Hk) +\n\u221a 2r log 1\n\u03b4 m + (1 3 + 1 \u03b2 ) log 1 \u03b4 m .\nSince there are at most pn possible p-tuples N with |N| = n, by the union bound, for any \u03b4 > 0, with probability at least 1\u2212 \u03b4,\nV \u2264 2(1 + \u03b2)1 \u03c1\np\u2211\nk=1\nNk n Rm(Hk) +\n\u221a r log p n\n\u03b4 m + (1 3 + 1 \u03b2 ) log pn \u03b4 m .\nThus, with probability at least 1\u2212\u03b4, for all functions g = 1 n \u2211T i=1 ntht with ht \u2208 Hkt, the following inequality holds\nV \u2264 2(1 + \u03b2)1 \u03c1\nT\u2211\nt=1\nnt n Rm(Hkt) +\n\u221a r log p n\n\u03b4 m + (1 3 + 1 \u03b2 ) log pn \u03b4 m .\nTaking the expectation with respect to \u03b1 and using E\u03b1[nt/n] = \u03b1t, we obtain that for any \u03b4 > 0, with probability at least 1\u2212 \u03b4, for all h, we can write\nE \u03b1 [V ] \u2264 2(1 + \u03b2)1 \u03c1\nT\u2211\nt=1\n\u03b1tRm(Hkt) +\n\u221a r log p n\n\u03b4 m + (1 3 + 1 \u03b2 ) log pn \u03b4 m .\nWe now show that r can be chosen in such a way that E\u03b1[V ] \u2264 r/K. The right hand side of the above bound is of the form A \u221a r + B. Note that solution of r/K = C + A \u221a r is bounded by K2A2 + 2KC and hence by Lemma 5 in [Bartlett et al., 2002] the following bound holds\nE \u03b1\n[R\u03c1/2(g)\u2212 K K \u2212 1R\u0302S,\u03c1(g)] \u2264 4K(1 + \u03b2) 1 \u03c1\nT\u2211\nt=1\n\u03b1tRm(Hkt) + ( 2K2 + 2K (1 3 + 1 \u03b2 )) log 1 \u03b4 m .\nSet \u03b2 = 1/2, then we have that\nE \u03b1\n[R\u03c1/2(g)\u2212 K K \u2212 1R\u0302S,\u03c1(g)] \u2264 6K 1 \u03c1\nT\u2211\nt=1\n\u03b1tRm(Hkt) + 5K log 1 \u03b4\nm .\nThen, for any \u03b4n > 0, with probability at least 1\u2212 \u03b4n,\nE \u03b1\n[R\u03c1/2(g)\u2212 K K \u2212 1R\u0302S,\u03c1(g)] \u2264 6K 1 \u03c1\nT\u2211\nt=1\n\u03b1tRm(Hkt) + 5K log p\nn\n\u03b4n\nm .\nChoose \u03b4n = \u03b42pn\u22121 for some \u03b4 > 0, then for p \u2265 2, \u2211 n\u22651 \u03b4n = \u03b4 2(1\u22121/p) \u2264 \u03b4. Thus, for any \u03b4 > 0 and any n \u2265 1, with probability at least 1\u2212 \u03b4, the following holds for all h:\nE \u03b1\n[R\u03c1/2(g)\u2212 K K \u2212 1R\u0302S,\u03c1(g)] \u2264 6K 1 \u03c1\nT\u2211\nt=1\n\u03b1tRm(Hkt) + 5K log 2p\n2n\u22121\n\u03b4\nm . (4)\nNow, for any f = \u2211T t=1 \u03b1tht \u2208 F and any g = 1n \u2211T\ni=1 ntht, we can upper bound R(f) = Pr(x,y)\u223cD[yf(x) \u2264 0], the generalization error of f , as follows: R(f) = Pr\n(x,y)\u223cD [yf(x)\u2212 yg(x) + yg(x) \u2264 0] \u2264 Pr[yf(x)\u2212 yg(x) < \u2212\u03c1/2] + Pr[yg(x) \u2264 \u03c1/2]\n= Pr[yf(x)\u2212 yg(x) < \u2212\u03c1/2] +R\u03c1/2(g). We can also write\nR\u0302\u03c1(g) = R\u0302S,\u03c1(g \u2212 f + f) \u2264 P\u0302r[yg(x)\u2212 yf(x) < \u2212\u03c1/2] + R\u0302S,3\u03c1/2(f). Combining these inequalities yields\nPr (x,y)\u223cD [yf(x) \u2264 0]\u2212 K K \u2212 1R\u0302S,3\u03c1/2(f) \u2264 Pr[yf(x)\u2212 yg(x) < \u2212\u03c1/2]\n+ K K \u2212 1P\u0302r[yg(x)\u2212 yf(x) < \u2212\u03c1/2] +R\u03c1/2(g)\u2212 K K \u2212 1R\u0302S,\u03c1(g).\nTaking the expectation with respect to \u03b1 yields\nR(f)\u2212 R\u0302S,3\u03c1/2(f) \u2264 E x\u223cD,\u03b1 [1yf(x)\u2212yg(x)<\u2212\u03c1/2]\n+ K K \u2212 1 Ex\u223cD,\u03b1[1yg(x)\u2212yf(x)<\u2212\u03c1/2] + E\u03b1[R\u03c1/2(g)\u2212 K K \u2212 1R\u0302S,\u03c1(g)].\nSince f = E\u03b1[g], by Hoeffding\u2019s inequality, for any x,\nE \u03b1 [1yf(x)\u2212yg(x)<\u2212\u03c1/2]=Pr \u03b1\n[yf(x)\u2212yg(x)<\u2212\u03c1/2] \u2264 e\u2212n\u03c1 2 8\nE \u03b1 [1yg(x)\u2212yf(x)<\u2212\u03c1/2]=Pr \u03b1\n[yg(x)\u2212yf(x)<\u2212\u03c1/2] \u2264 e\u2212n\u03c1 2 8 .\nThus, for any fixed f \u2208 F , we can write\nR(f)\u2212 R\u0302S,3\u03c1/2(f) \u2264 ( 1 + K K \u2212 1 ) e\u2212n\u03c1 2/8 + E \u03b1 [R\u03c1/2(g)\u2212 K\nK \u2212 1R\u0302S,\u03c1(g)]. Thus, the following inequality holds:\nsup f\u2208F\n( R(f)\u2212 K K \u2212 1R\u0302S,\u03c1(f) ) \u2264 ( 1 + K K \u2212 1 ) e\u2212n\u03c1 2/8 + sup h E \u03b1 [R\u03c1/2(g)\u2212 K K \u2212 1R\u0302S,\u03c1/2(g)].\nTherefore, in view of (4), for any \u03b4 > 0 and any n \u2265 1, with probability at least 1\u2212\u03b4, the following holds for all f \u2208 F :\nR(f)\u2212 K K \u2212 1R\u0302S,\u03c1(f) \u2264\n( 1 + K K \u2212 1 ) e\u2212n\u03c1 2/8 + 6K 1 \u03c1 T\u2211\nt=1\n\u03b1tRm(Hkt) + 5K log 2p\n2n\u22121\n\u03b4\nm .\nTo conclude the proof we optimize over n, f : n 7\u2192 v1e\u2212nu+v2n, which leads to n = (1/u) log(uv2/v1). Therefore, we set\nn =\n\u2308 8\n\u03c12 log\n\u03c12(1 + K K\u22121 )m\n40K log p\n\u2309\nto obtain that the following bound\nR(f)\u2212 K K \u2212 1R\u0302S,\u03c1(f) \u2264 6K 1 \u03c1\nT\u2211\nt=1\n\u03b1tRm(Hkt)\n+ 40 K \u03c12 log p m + 5K\nlog 2 \u03b4\nm + 5K\n\u2308 8\n\u03c12 log\n\u03c12(1 + K K\u22121 )m\n40K log p\n\u2309 log p\nm .\nThus, taking K = 2, simply yields\nR(f) \u2264 2R\u0302S,\u03c1(f) + 12\n\u03c1\nT\u2211\nt=1\n\u03b1tRm(Hkt) +O\n( log p\n\u03c12m log ( \u03c1m log p ) + log 1 \u03b4 m\n)\nand the proof is complete.\nLemma 3. Let Kk be the kernel matrix of the kernel function Kk for the sample S and let \u03bak = supx\u2208X \u221a Kk(x, x). Then, the following inequality holds:\nR\u0302S(Hk) \u2264 \u03bak \u221a Tr[Kk]\nm .\nProof. R\u0302S(Hk) can be upper bounded as follows using the Cauchy-Schwarz inequality:\nR\u0302S(Hk) = 1\nm E \u03c3\n[ sup\nx\u2032\u2208X ,s\u2208{\u22121,+1}\nm\u2211\ni=1\n\u03c3isKk(xi, x \u2032) ] = 1\nm E \u03c3 [ sup x\u2032\u2208X \u2223\u2223\u2223 m\u2211\ni=1\n\u03c3isKk(xi, x \u2032) \u2223\u2223\u2223 ]\n= 1\nm E \u03c3 [ sup x\u2032\u2208X \u2223\u2223\u2223 m\u2211\ni=1\n\u03c3i\u03a6k(xi) \u00b7 \u03a6k(x\u2032) \u2223\u2223\u2223 ] \u2264 1\nm E \u03c3 [ sup x\u2032\u2208X \u2016\u03a6k(x\u2032)\u2016Hk \u2225\u2225\u2225 m\u2211\ni=1\n\u03c3i\u03a6k(xi) \u2225\u2225\u2225 Hk\n]\n= \u03bak m E \u03c3\n[\u2225\u2225\u2225 m\u2211\ni=1\n\u03c3i\u03a6k(xi) \u2225\u2225\u2225 Hk ] \u2264 \u03bak m \u221a\u221a\u221a\u221aE \u03c3 [ m\u2211\ni,j=1\n\u03c3i\u03c3j\u03a6k(xi) \u00b7 \u03a6k(xj) ] = \u03bak \u221a Tr[Kk]\nm ,\nwhere we used in the last line Jensen\u2019s inequality.\nLemma 4. Let Kk be a polynomial kernel of degree k. Then, the empirical Rademacher complexity of Hk can be upper bounded as follows:\nR\u0302S(Hk) \u2264 12\u03ba2k \u221a\n\u03c0dk m .\nProof. By the proof of Lemma 3, we can write\nR\u0302S(Hk) \u2264 \u03bak m E \u03c3\n[\u2225\u2225\u2225 m\u2211\ni=1\n\u03c3i\u03a6k(xi) \u2225\u2225\u2225 Hk ] = 2\u03ba2k R\u0302S(H 1 k),\nwhere H1k is the family of linear functions H 1 k = {w 7\u2192 w \u00b7 \u03a6k(x) : \u2016w\u2016Hk \u2264 12\u03bak }. By Dudley\u2019s formula [Dudley, 1989], we can write\nR\u0302S(H 1 k) \u2264 12\n\u222b \u221e\n0\n\u221a logN (\u01eb,H1k , L2(D\u0302))\nm d\u01eb,\nwhere D\u0302 is the empirical distribution. Since H1k can be viewed as a subset of a dk-dimensional linear space and since |w \u00b7\u03a6k(x)| \u2264 12 for all x \u2208 X and w \u2208 H1k , we have logN (\u01eb,H1k , L2(D\u0302)) \u2264 log [ (1 \u01eb )dk ] . Thus, we can write\nR\u0302S(H 1 k) \u2264 12\n\u222b 1\n0\n\u221a dk log 1 \u01eb\nm d\u01eb = 12 \u221a dk m \u222b 1\n0\n\u221a log 1\n\u01eb d\u01eb = 12 \u221a dk m \u221a \u03c0 2 ,\nwhich completes the proof."}, {"heading": "APPENDIX B. OPTIMIZATION PROBLEM", "text": "This section provides the derivation for VKR optimization problem. We will assume that H1, . . . , Hp are p families of functions with increasing Rademacher complexities Rm(Hk), k \u2208 [1, p], and, for any hypothesis h \u2208 \u222apk=1Hk, denote by d(h) the index of the hypothesis set it belongs to, that is h \u2208 Hd(h). The bound of Theorem 1 holds uniformly for all \u03c1 > 0 and functions f \u2208 conv(\u22c3pk=1Hk) at the price of an additional term that is in O (\u221a log log 2 \u03c1 m ) . The condition \u2211T t=1 \u03b1t = 1 of Theorem 1 can be relaxed to \u2211T t=1 \u03b1t \u2264 1. To see this, use for example a null\nhypothesis (ht = 0 for some t). Since the last term of the bound does not depend on \u03b1, it suggests selecting \u03b1 to minimize\nG(\u03b1) = 1\nm\nm\u2211\ni=1\n1yi \u2211T t=1 \u03b1tht(xi)\u2264\u03c1 +\n4\n\u03c1\nT\u2211\nt=1\n\u03b1trt,\nwhere rt = Rm(Hd(ht)). Since for any \u03c1 > 0, f and f/\u03c1 admit the same generalization error, we can instead search for \u03b1 \u2265 0 with \u2211Tt=1 \u03b1t \u2264 1/\u03c1 which leads to\nmin \u03b1\u22650\n1\nm\nm\u2211\ni=1\n1yi \u2211T t=1\u03b1tht(xi)\u22641 +4\nT\u2211\nt=1\n\u03b1trt s.t. T\u2211\nt=1\n\u03b1t \u2264 1\n\u03c1 .\nThe first term of the objective is not a convex function of \u03b1 and its minimization is known to be computationally hard. Thus, we will consider instead a convex upper bound based on the Hinge loss: let \u03a6(\u2212u) = max(0, 1\u2212u), then 1\u2212u \u2264 \u03a6(\u2212u). Using this upper bound yields the following convex optimization problem:\nmin \u03b1\u22650\n1\nm\nm\u2211\ni=1\n\u03a6 ( 1\u2212 yi T\u2211\nt=1\n\u03b1tht(xi) ) + \u03bb T\u2211\nt=1\n\u03b1trt s.t. T\u2211\nt=1\n\u03b1t \u2264 1\n\u03c1 , (5)\nwhere we introduced a parameter \u03bb \u2265 0 controlling the balance between the magnitude of the values taken by function \u03a6 and the second term. Introducing a Lagrange variable \u03b2 \u2265 0 associated to the constraint in (5), the problem can be equivalently written as\nmin \u03b1\u22650\n1\nm\nm\u2211\ni=1\n\u03a6 ( 1\u2212 yi T\u2211\nt=1\n\u03b1tht(xi) ) + T\u2211\nt=1\n(\u03bbrt + \u03b2)\u03b1t.\nHere, \u03b2 is a parameter that can be freely selected by the algorithm since any choice of its value is equivalent to a choice of \u03c1 in (5). Let (hk,j)k,j be the set of distinct base functions x 7\u2192 Kk(\u00b7, xj). Then, the problem can be rewritten as F be the objective function based on that collection:\nmin \u03b1\u22650\n1\nm\nm\u2211\ni=1\n\u03a6 ( 1\u2212yi N\u2211\nj=1\n\u03b1jhj(xi) ) + N\u2211\nt=1\n\u039bj\u03b1j , (6)\nwith \u03b1 = (\u03b11, . . . , \u03b1N) \u2208 RN and \u039bj = \u03bbrj + \u03b2, for all j \u2208 [1, N ]. This coincides precisely with the optimization problem min\u03b1\u22650 F (\u03b1) defining Voted Kernel Regularization . Since the problem was derived by minimizing a Hinge loss upper bound on the generalization bound, this shows that the solution returned by Voted Kernel Regularization benefits from the strong data-dependent learning guarantees of Theorem 1."}, {"heading": "APPENDIX C. COORDINATE DESCENT (CD) FORMULATION", "text": "An alternative approach for solving the Voted Kernel Regularization optimization problem (1) consists of using a coordinate descent method. A coordinate descent method proceeds in rounds. At each round, it maintains a parameter vector \u03b1. Let \u03b1t = (\u03b1t,k,j)\u22a4k,j denote the vector obtained after t \u2265 1 iterations and let \u03b10 = 0. Let ek,j denote the unit vector in direction (k, j) in Rp\u00d7m . Then, the direction ek,j and the step \u03b7 selected at the tth round are those minimizing F (\u03b1t\u22121 +\nTABLE 2. Dataset statistics.\nData set Examples Features breastcancer 699 9 climate 540 18 diabetes 768 8 german 1000 24 ionosphere 351 34 musk 476 166 ocr49 2000 196 phishing 2456 30 retinopathy 1151 19 vertebral 310 6 waveform01 3304 21\n\u03b7ek,j), that is\nF (\u03b1)= 1\nm\nm\u2211\ni=1\nmax ( 0, 1\u2212yift\u22121 \u2212 yiyj\u03b7Kk(xi, xj) ) + m\u2211\nj=1\np\u2211\nk=1\n\u039bk|\u03b1t\u22121,j,k|+ \u039bk|\u03b7 + \u03b1t\u22121,k,j|,\nwhere ft\u22121 = \u2211m\nj=1 \u2211p k=1 \u03b1t\u22121,j,kyjKk(\u00b7, xj). To find the best descent direction, a coordinate\ndescent method computes the sub-gradient in the direction (k, j) for each (k, j) \u2208 [1, p] \u00d7 [1, m]. The sub-gradient is given by\n\u03b4F (\u03b1t\u22121, ej) =    1 m \u2211m i=1 \u03c6t,j,k,i + sgn(\u03b1t\u22121,k,j)\u039bk if \u03b1t\u22121,k,j 6= 0 0 else if \u2223\u2223\u2223 1m \u2211m i=1 \u03c6t,j,k,i \u2223\u2223\u2223 \u2264 \u039bk 1 m \u2211m i=1 \u03c6t,j,k,i \u2212 sgn ( 1 m \u2211m i=1 \u03c6t,j,k,i ) \u039bk otherwise .\nwhere \u03c6t,j,k,i = \u2212yiKk(xi, xj) if \u2211p\nk=1 \u2211m j=1 \u03b1t\u22121,k,jyiyjK(xi, xj) < 1 and 0 otherwise. Once\nthe optimal direction ek,j is determined, the step size \u03b7t can be found using a line search or other numerical methods.\nThe advantage of the coordinate descent formulation over the LP formulation is that there is no need to explicitly store the whole vector of \u03b1s but rather only no-zero entries. This enables learning with very large number of base hypotheses including scenarios in which the number of base hypotheses is infinite."}, {"heading": "APPENDIX D. DATASET STATISTICS", "text": "The dataset statistics are provided in Table 2"}], "references": [{"title": "Ensembles of kernel predictors", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": "In UAI,", "citeRegEx": "2010", "shortCiteRegEx": "2010", "year": 2011}, {"title": "Multi-task feature and kernel selection for SVMs", "author": ["T. Jebara"], "venue": "In ICML,", "citeRegEx": "Jebara.,? \\Q2004\\E", "shortCiteRegEx": "Jebara.", "year": 2004}, {"title": "Nonstationary kernel combination", "author": ["D.P. Lewis", "T. Jebara", "W.S. Noble"], "venue": "In ICML,", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": "In COLT,", "citeRegEx": "Vapnik.,? \\Q2006\\E", "shortCiteRegEx": "Vapnik.", "year": 2006}, {"title": "Multiclass multiple kernel learning", "author": ["A. Zien", "C.S. Ong"], "venue": "ICML", "citeRegEx": "Zien and Ong.,? \\Q2007\\E", "shortCiteRegEx": "Zien and Ong.", "year": 2007}, {"title": "2005], for any \u03b4 > 0 with probability at least 1 \u2212 \u03b4", "author": ["Bartlett"], "venue": null, "citeRegEx": "Bartlett,? \\Q2005\\E", "shortCiteRegEx": "Bartlett", "year": 2005}], "referenceMentions": [{"referenceID": 3, "context": "In the absence of any regularization, that is \u03bb = 0 and \u03b2 = 0, it reduces to the minimization of the Hinge loss and is therefore of course close to the SVM algorithm [Cortes and Vapnik, 1995]. For \u03bb = 0, that is when discarding our regularization based on the different complexity of the hypothesis sets, the algorithm coincides with an algorithm originally described by Vapnik [1998][pp.", "startOffset": 178, "endOffset": 385}], "year": 2015, "abstractText": "This paper presents an algorithm, Voted Kernel Regularization , that provides the flexibility of using potentially very complex kernel functions such as predictors based on much higher-degree polynomial kernels, while benefitting from strong learning guarantees. The success of our algorithm arises from derived bounds that suggest a new regularization penalty in terms of the Rademacher complexities of the corresponding families of kernel maps. In a series of experiments we demonstrate the improved performance of our algorithm as compared to baselines. Furthermore, the algorithm enjoys several favorable properties. The optimization problem is convex, it allows for learning with non-PDS kernels, and the solutions are highly sparse, resulting in improved classification speed and memory requirements.", "creator": "LaTeX with hyperref package"}}}