{"id": "1509.04355", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Sep-2015", "title": "Towards Making High Dimensional Distance Metric Learning Practical", "abstract": "In this work, we study distance metric learning (DML) for high dimensional data. A typical approach for DML with high dimensional data is to perform the dimensionality reduction first before learning the distance metric. The main shortcoming of this approach is that it may result in a suboptimal solution due to the subspace removed by the dimensionality reduction method. In this work, we present a dual random projection frame for DML with high dimensional data that explicitly addresses the limitation of dimensionality reduction for DML. The key idea is to first project all the data points into a low dimensional space by random projection, and compute the dual variables using the projected vectors. It then reconstructs the distance metric in the original space using the estimated dual variables. The proposed method, on one hand, enjoys the light computation of random projection, and on the other hand, alleviates the limitation of most dimensionality reduction methods. We verify both empirically and theoretically the effectiveness of the proposed algorithm for high dimensional DML. This work is supported by the MSPX.", "histories": [["v1", "Tue, 15 Sep 2015 00:04:24 GMT  (43kb)", "http://arxiv.org/abs/1509.04355v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["qi qian", "rong jin", "lijun zhang", "shenghuo zhu"], "accepted": false, "id": "1509.04355"}, "pdf": {"name": "1509.04355.pdf", "metadata": {"source": "CRF", "title": "Towards Making High Dimensional Distance Metric Learning Practical", "authors": ["Qi Qian", "Rong Jin", "Lijun Zhang", "Shenghuo Zhu", "QI QIAN", "RONG JIN", "LIJUN ZHANG", "SHENGHUO ZHU"], "emails": ["QIANQI@CSE.MSU.EDU", "RONGJIN@CSE.MSU.EDU", "ZLJ@CSE.MSU.EDU", "ZSH@NEC-LABS.COM"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 9.\n04 35\n5v 1\n[ cs\n.L G\nKeywords: Distance Metric Learning, Dual Random Projection"}, {"heading": "1. Introduction", "text": "Distance metric learning (DML) is essential to many machine learning tasks, including ranking (Chechik et al., 2010; Lim et al., 2013), k-nearest neighbor (k-NN) classification (Weinberger and Saul, 2009) and k-means clustering (Xing et al., 2002). It finds a good metric by minimizing the distance between data pairs in the same classes and maximizing the distance between data pairs from different classes (Xing et al., 2002; Globerson and Roweis, 2005; Yang and Jin, 2006; Davis et al., 2007; Weinberger and Saul, 2009; Shaw et al., 2011). The main computational challenge of DML arises from the constraint that the learned matrix has to be positive semi-definite (PSD). It is computationally demanding even with a stochastic gradient descent (SGD) because it has to project the intermediate solutions onto the PSD cone at every iteration. In a recent study (Chechik et al., 2010), the authors show empirically that it is possible to learn a good distance metric using online learning without having to perform the projection at each iteration. In fact, only one projection into the PSD\nc\u00a9 Qi Qian, Rong Jin, Lijun Zhang and Shenghuo Zhu.\ncone is performed at the end of online learning to ensure that the resulting matrix is PSD 1. Our study of DML follows the same paradigm, to which we refer as one-projection paradigm.\nAlthough the one-projection paradigm resolves the computational challenge from projection onto the PSD cone, it still suffers from a high computational cost when each data point is described by a large number of features. This is because, for d dimensional data points, the size of learned matrix will be O(d2), and as a result, the cost of computing the gradient, the fundamental operation for any first order optimization method, will also be O(d2). The focus of this work is to develop an efficient first order optimization method for high dimensional DML that avoids O(d2) computational cost per iteration.\nSeveral approaches have been proposed to reduce the computation cost for high dimensional DML. In (Davis and Dhillon, 2008), the authors assume that the learned metric M is of low rank, and write it as M = LL\u22a4, where L \u2208 Rd\u00d7r with r \u226a d. Instead of learning M , the authors proposed to learn L directly, which reduces the cost of computing the gradient from O(d2) to O(dr). A similar idea was studied in (Weinberger and Saul, 2009). The main problem with this approach is that it will result in non-convex optimization. An alternative approach is to reduce the dimensionality of data using dimensionality reduction methods such as principal component analysis (PCA) (Weinberger and Saul, 2009) or random projection (RP) (Tsagkatakis and Savakis, 2010). Although RP is computationally more efficient than PCA, it often yields significantly worse performance than PCA unless the number of random projections is sufficiently large (Fradkin and Madigan, 2003). We note that although RP has been successfully applied to many machine learning tasks, e.g., classification (Rahimi and Recht, 2007), clustering (Boutsidis et al., 2010) and regression (Maillard and Munos, 2012), only a few studies examined the application of RP to DML, and most of them with limited success.\nIn this paper, we propose a dual random projection approach for high dimensional DML. Our approach, on one hand, enjoys the light computation of random projection, and on the other hand, significantly improves the effectiveness of random projection. The main limitation of using random projection for DML is that all the columns/rows of the learned metric will lie in the subspace spanned by the random vectors. We address this limitation of random projection by\n\u2022 first estimating the dual variables based on the random projected vectors and, \u2022 then reconstructing the distance metric using the estimated dual variables and data vectors in\nthe original space. Since the final distance metric is computed using the original vectors, not the randomly projected vectors, the column/row space of the learned metric will NOT be restricted to the subspace spanned by the random projection, thus alleviating the limitation of random projection. We verify the effectiveness of the proposed algorithms both empirically and theoretically.\nWe finally note that our work is built upon the recent work (Zhang et al., 2013) on random projection where a dual random projection algorithm is developed for linear classification. Our work differs from (Zhang et al., 2013) in that we apply the theory of dual random projection to DML. More importantly, we have made an important progress in advancing the theory of dual random projection. Unlike the theory in (Zhang et al., 2013) where the data matrix is assumed to be low rank or approximately low rank, our new theory of dual random projection is applicable to any\n1. We note that this is different from the algorithms presented in (Hazan and Kale, 2012; Mahdavi et al., 2012). Although these two algorithms only need either one or no projection step, they introduce additional mechanisms to prevent the intermediate solutions from being too far away from the PSD cone, which could result in a significant overhead per iteration.\ndata matrix even when it is NOT approximately low rank. This new analysis significantly broadens the application domains where dual random projection is applicable, which is further verified by our empirical study.\nThe rest of the paper is organized as follows: Section 2 introduces the methods that are related to the proposed method. Section 3 describes the proposed dual random projection approach for DML and the detailed algorithm for solving the dual problem in the subspace spanned by random projection. Section 4 summarizes the results of the empirical study, and Section 5 concludes this work with future directions."}, {"heading": "2. Related Work", "text": "Many algorithms have been developed for DML (Xing et al., 2002; Globerson and Roweis, 2005; Davis et al., 2007; Weinberger and Saul, 2009). Exemplar DML algorithms are MCML (Globerson and Roweis, 2005), ITML (Davis et al., 2007), LMNN (Weinberger and Saul, 2009) and OASIS (Chechik et al., 2010). Besides algorithms, several studies were devoted to analyzing the generalization performance of DML (Jin et al., 2009; Bellet and Habrard, 2012). Survey papers (Yang and Jin, 2006; Kulis, 2013) provide detailed investigation about the topic. Although numerous studies were devoted to DML, only a limited progress is made to address the high dimensional challenge in DML (Davis and Dhillon, 2008; Weinberger and Saul, 2009; Qi et al., 2009; Lim et al., 2013). In (Davis and Dhillon, 2008; Weinberger and Saul, 2009), the authors address the challenge of high dimensionality by enforcing the distance metric to be a low rank matrix. (Qi et al., 2009; Lim et al., 2013) alleviate the challenge of learning a distance metric M from high dimensional data by assuming M to be a sparse matrix. The main shortcoming of these approaches is that they have to place strong assumption on the learned metric, significantly limiting their application. In addition, these approaches will result in non-convex optimization problems that are usually difficult to solve. In contrast, the proposed DML algorithm does not have to make strong assumption regarding the learned metric.\nRandom projection is widely used for dimension reduction in various learning tasks (Rahimi and Recht, 2007; Boutsidis et al., 2010; Maillard and Munos, 2012). Unfortunately, it requires a large amount of random projections for the desired result (Fradkin and Madigan, 2003), and this limits its application in DML, where the computational cost is proportion to the square of dimensions. Dual random projection is first introduced for linear classification task (Zhang et al., 2013) and following aspects make our work significantly different from the initial study (Zhang et al., 2013): First, we apply dual random projection for DML, where the number of variables is quadratic to the dimension and the dimension crisis is more serious than linear classifier. Second, we optimize the dual problem directly rather than the primal problem in the subspace as the previous work. Consequently, nonsmoothed loss (e.g., hinge loss) could be used for the proposed method. Last, we give the theoretical guarantee when the dataset is not low rank, which is an important assumption for the study (Zhang et al., 2013). All of these efforts try to efficiently learn a distance metric for high dimensional datasets and sufficient empirical study verifies the success of our method."}, {"heading": "3. Dual Random Projection for Distance Metric Learning", "text": "Let X = (x1, \u00b7 \u00b7 \u00b7 ,xn) \u2208 Rd\u00d7n denote the collection of training examples. Given a PSD matrix M , the distance between two examples xi and xj is given as\ndM (xi,xj) = (xi \u2212 xj)\u22a4M(xi \u2212 xj).\nThe proposed framework for DML will be based on triplet constraints, not pairwise constraints. This is because several previous studies have suggested that triplet constraints are more effective than pairwise constraints (Weinberger and Saul, 2009; Chechik et al., 2010; Shaw et al., 2011). Let D = {(x1i ,x1j ,x1k), . . . , (xNi ,xNj ,xNk )} be the set of triplet constraints used for training, where xti is expected to be more similar to xtj than to x t k. Our goal is to learn a metric function M that is consistent with most of the triplet constraints in D, i.e.\n\u2200(xti,xtj ,xtk) \u2208 D, (xti\u2212xtj)\u22a4M(xti\u2212xtj)+1 \u2264 (xti\u2212xtk)\u22a4M(xti\u2212xtk)\nFollowing the empirical risk minimization framework, we cast the triplet constraints based DML into the following optimization problem:\nmin M\u2208Sd\n\u03bb 2 \u2016M\u20162F + 1 N\nN\u2211\nt=1\n\u2113(\u3008M,At\u3009) (1)\nwhere Sd stands for the symmetric matrix of size d \u00d7 d, \u03bb > 0 is the regularization parameter, \u2113(\u00b7) is a convex loss function, At = (xti\u2212xtk)(xti\u2212xtk)\u22a4\u2212 (xti\u2212xtj)(xti\u2212xtj)\u22a4, and \u3008\u00b7, \u00b7\u3009 stands for the dot product between two matrices. We note that we did not enforce M in (1) to be PSD because we follow the one-projection paradigm proposed in (Chechik et al., 2010) that first learns a symmetric matrix M by solving the optimization problem in (1) and then projects the learned matrix M onto the PSD cone. We emphasize that unlike (Zhang et al., 2013), we did not assume \u2113(\u00b7) to be smooth, making it possible to apply the proposed approach to the hinge loss.\nLet \u2113\u2217(\u00b7) be the convex conjugate of \u2113(\u00b7). The dual problem of (1) is given by\nmax \u03b11,...,\u03b1N \u2212 1 N\nN\u2211\nt=1\n\u2113\u2217(\u03b1t)\u2212 1\n2\u03bbN2 \u2225\u2225\u2225\u2225\u2225 N\u2211\nt=1\n\u03b1tAt \u2225\u2225\u2225\u2225\u2225 2\nF\nwhich is equivalent to\nmax \u03b1\u2208[\u22121,0]N\n\u2212 N\u2211\nt=1\n\u2113\u2217(\u03b1t)\u2212 1\n2\u03bbN \u03b1\n\u22a4G\u03b1 (2)\nwhere \u03b1 = (\u03b11, \u00b7 \u00b7 \u00b7 , \u03b1N )\u22a4 and G = [Ga,b]N\u00d7N is a matrix of N \u00d7N with Ga,b = \u3008Aa, Ab\u3009. We denote by M\u2217 \u2208 Rd\u00d7d the optimal primal solution to (1), and by \u03b1\u2217 \u2208 RN the optimal dual solution to (2). Using the first order condition for optimality, we have\nM\u2217 = \u2212 1\n\u03bbN\nN\u2211\nt=1\n\u03b1t\u2217At (3)"}, {"heading": "3.1 Dual Random Projection for Distance Metric Learning", "text": "Directly solving the primal problem in (1) or the dual problem in (2) could be computational expensive when the data is of high dimension and the number of training triplets is very large. We address this challenge by inducing a random matrix R \u2208 Rd\u00d7m, where m \u226a d and Ri,j \u223c N (0, 1/m), and projecting all the data points into the low dimensional space using the random matrix, i.e., x\u0302i = R\n\u22a4xi. As a result, At, after random projection, becomes A\u0302t = R\u22a4AtR. A typical approach of using random projection for DML is to obtain a matrix Ms of size m\u00d7m\nby solving the primal problem with the randomly projected vectors {x\u0302i}ni=1, i.e.\nmin M\u2208Sm\n\u03bb 2 \u2016M\u20162F + 1 N\nN\u2211\nt=1\n\u2113(\u3008M, A\u0302t\u3009) (4)\nGiven the learned metric Ms, for any two data points x and x\u2032, their distance is measured by (x \u2212 x\u2032)\u22a4RMsR\u22a4(x \u2212 x\u2032) = (x \u2212 x\u2032)\u22a4M \u2032(x \u2212 x\u2032), where M \u2032 = RMsR\u22a4 \u2208 Rd\u00d7d is the effective metric in the original space Rd. The key limitation of this random projection approach is that both the column and row space of M \u2032 are restricted to the subspace spanned by vectors in random matrix R.\nInstead of solving the primal problem, we proposed to solve the dual problem using the randomly projected data points {x\u0302i}ni=1, i.e.\nmax \u03b1\u2208RN\n\u2212 N\u2211\nt=1\n\u2113\u2217(\u03b1t)\u2212 1\n2\u03bbN \u03b1\n\u22a4G\u0302\u03b1 (5)\nwhere G\u0302a,b = \u3008R\u22a4AaR,R\u22a4AbR\u3009. After obtaining the optimal solution \u03b1\u0302\u2217 for (5), we reconstruct the metric by using the dual variables \u03b1\u0302\u2217 and data matrix X in the original space, i.e.\nM\u0302\u2217 = \u2212 1\n\u03bbN\nN\u2211\nt=1\n\u03b1\u0302t\u2217At (6)\nIt is important to note that unlike the random projection approach, the recovered metric M\u0302\u2217 in (6) is not restricted by the subspace spanned by the random vectors, a key to the success of the proposed algorithm.\nAlg. 1 summarizes the key steps for the proposed dual random projection method for DML. Following one-projection paradigm (Chechik et al., 2010), we project the learned symmetric matrix M onto the PSD cone at the end of the algorithm. The key component of Alg. 1 is to solve the optimization problem in (2) at Step 4 accurately. We choose stochastic dual coordinate ascent (SDCA) method for solving the dual problem (5) because it enjoys a linear convergence when the loss function is smooth, and is shown empirically to be significantly faster than the other stochastic optimization methods (Shalev-Shwartz and Zhang, 2012). We use the combination strategy recommended in (Shalev-Shwartz and Zhang, 2012), denoted by CSDCA, which uses SGD for the first epoch and then applies SDCA for the rest epochs."}, {"heading": "3.2 Main Theoretical Results", "text": "First, similar to (Zhang et al., 2013), we consider the case when the data matrix X is of low rank. The theorem below shows that under the low rank assumption, with a high probability, the distance metric recovered by Algorithm 1 is nearly optimal.\nAlgorithm 1 Dual Random Projection Method (DuRP) for DML 1: Input: the triplet constraints D and the number of random projections m. 2: Generate a random matrix R \u2208 Rd\u00d7m and Ri,j \u223c N (0, 1/m). 3: Project each example as x\u0302 = R\u22a4x. 4: Solve the optimization problem (5) and obtain the optimal solution \u03b1\u0302\u2217 5: Recover the solution in the original space by M\u0302\u2217 = \u2212 1\u03bbN \u2211 t \u03b1\u0302 t \u2217At\n6: Output: \u03a0PSD(M\u0302\u2217)\nTheorem 1 Let M\u2217 be the optimal solution to (1). Let \u03b1\u0302\u2217 be the optimal solution for (5), and let M\u0302\u2217 be the solution recovered from \u03b1\u0302\u2217 using (6). Under the assumption that all the data points lie in the subspace of r-dimension, for any 0 < \u03b5 \u2264 1/6, with a probability at least 1\u2212 \u03b4, we have\n\u2016\u03a0PSD(M\u2217)\u2212\u03a0PSD(M\u0302\u2217)\u2016F \u2264 3\u03b5\n1\u2212 3\u03b5\u2016M\u2217\u2016F\nprovided m \u2265 (r+1) log(2r/\u03b4)c\u03b52 and constant c is at least 1/3.\nThe proof of Theorem 1 can be found in appendix. Theorem 1 indicates that if the number of random projections is sufficiently large (i.e. m = \u2126(r log r)), we can recover the optimal solution in the original space with a small error. It is important to note that our analysis, unlike (Zhang et al., 2013), can be applied to non-smooth loss such as the hinge loss.\nIn the second case, we assume the loss function \u2113(\u00b7) is \u03b3-smooth (i.e., |\u2113\u2032(z)\u2212\u2113\u2032(z\u2032)| \u2264 \u03b3|z\u2212z\u2032|). The theorem below shows that the dual variables obtained by solving the optimization problem in (5) can be close to the optimal dual variables, even when the data matrix X is NOT low rank or approximately low rank. For the presentation of theorem, we first define a few important quantities. Define matrices M1 \u2208 RN\u00d7N , M2 \u2208 RN\u00d7N , M3 \u2208 RN\u00d7N , and M4 \u2208 RN\u00d7N as\nM1a,b = \u2016(xai \u2212 xak)\u201622 \u00d7 \u2016(xbi \u2212 xbk)\u201622 M2a,b = \u2016(xai \u2212 xaj )\u201622 \u00d7 \u2016(xbi \u2212 xbj)\u201622 M3a,b = \u2016(xai \u2212 xak)\u201622 \u00d7 \u2016(xbi \u2212 xbj)\u201622 M4a,b = \u2016(xai \u2212 xaj )\u201622 \u00d7 \u2016(xbi \u2212 xbk)\u201622\nDefine \u03ba the maximum of the spectral norm of the four matrices, i.e.\n\u03ba = max ( \u2016M1\u20162, \u2016M2\u20162, \u2016M3\u20162, \u2016M4\u20162 ) (7)\nwhere \u2016 \u00b7 \u20162 stands for the spectral norm of matrices.\nTheorem 2 Assume \u2113(z) is \u03b3-smooth. Let \u03b1\u2217 be the optimal solution to the dual problem in (2), and let \u03b1\u0302\u2217 be the approximately optimal solution for (5) with suboptimality \u03b7. Then, with a probability at least 1\u2212 \u03b4, we have\n\u2016\u03b1\u2217 \u2212 \u03b1\u0302\u2217\u20162 \u2264 max ( 8\u01eb\u03b3\u03ba\u2016\u03b1\u2217\u20162, \u221a 2\u03b3\u03b7 )\nwhere \u03ba is define in (7), provided m \u2265 8 \u01eb2 ln 8N\u03b4 .\nThe proof of Theorem 2 can be found in the appendix. Unlike Theorem 1 where the data matrix X is assumed to be low rank, Theorem 2 holds without any prior assumption about the data matrix. It shows that despite the random projection, the dual solution can be recovered approximately using the randomly projected vectors, provided that the number of random projections m is sufficiently large, \u03ba is small, and the approximately optimal solution \u03b1\u0302\u2217 is sufficiently accurate. In the case when most of the training examples are not linear dependent, we could have \u03ba = \u0398(N/d), which could be a modest number when d is very large. The result in Theorem 2 essentially justifies the key idea of our approach, i.e. computing the dual variables first and recovering the distance metric later. Finally, since \u2016\u03b1\u2217 \u2212 \u03b1\u0302\u2217\u20162, the approximation error in the recovered dual variables, is proportional to the square root of the suboptimality \u03b7, an accurate solution for (5) is needed to ensure a small approximation error. We note that given Theorem 2, it is straightforward to bound \u2016M\u2217 \u2212 M\u0302\u2217\u20162 using the relationship between the dual variables and the primal variables in (3)."}, {"heading": "4. Experiments", "text": "We will first describe the experimental setting, and then present our empirical study for ranking and classification tasks on various datesets."}, {"heading": "4.1 Experimental Setting", "text": "Data sets Six datasets are used to validate the effectiveness of the proposed algorithm for DML. Table 1 summarizes the information of these datasets. caltech30 is a subset of Caltech256 image dataset (Griffin et al., 2007) and we use the version pre-processed by (Chechik et al., 2010). tdt30 is a subset of tdt2 dataset (Cai et al., 2009). Both caltech30 and tdt30 are comprised of the examples from the 30 most popular categories. All the other datasets are downloaded from LIBSVM (Chang and Lin, 2011), where rcv30 is a subset of the original dataset consisted of documents from the 30 most popular categories. For datasets tdt30, 20news and rcv30, they are comprised of documents represented by vectors of \u223c 50, 000 dimensions. Since it is expensive to compute and maintain a matrix of 50, 000 \u00d7 50, 000, for these three datasets, we follow the procedure in (Chechik et al., 2010) that maps all documents to a space of 1, 000 dimension. More specifically, we first keep the top 20, 000 most popular words for each collection, and then reduce their dimensionality to 1, 000 by using PCA. We emphasize that for several data sets in our test beds, their data matrices can not\nbe well approximated by low rank matrices. Fig. 2 summarizes the eigenvalue distribution of the six datasets used in our experiment. We observe that four out of these datasets (i.e., caltech20, tdt30, 20news, rcv30) have a flat eigenvalue distribution, indicating that the associated data matrices can not be well approximated by a low rank matrix. This justifies the importance of removing the low rank assumption from the theory of dual random projection, an important contribution of this work.\nFor most datasets used in this study, we use the standard training/testing split provided by the original datasets, except for datasets tdt30, caltech30 and rcv30. For tdt30 and caltech30, we randomly select 70% of the data for training and use the remaining 30% for testing; for rcv30, we switch the training and test sets defined by the original package to ensure that the number of training examples is sufficiently large.\nEvaluation metrics To measure the quality of learned distance metrics, two types of evaluations are adopted in our study. First, we follow the evaluation protocol in (Chechik et al., 2010) and evaluate the learned metric by its ranking performance. More specifically, we treat each test instance q as a query, and rank the other test instances in the ascending order of their distance to q using the learned metric. The mean-average-precision(mAP) given below is used to evaluate the quality of the ranking list\nmAP = 1\n|Q|\n|Q|\u2211\ni=1\n1\nri\nri\u2211\nj=1\nP (xi\u223cj)\nwhere |Q| is the size of query set, ri is the number of relevant instances for i-th query and P (xi\u223cj) is the precision for the first j ranked instances when the instance ranked at the j-th position is relevant to the query q. Here, an instance x is relevant to a query q if they belong to the same class. Second, we evaluate the learned metric by its classification performance with k-nearest neighbor classifier.\nMore specifically, for each test instance q, we apply the learned metric to find the first k training examples with the shortest distance, and predict the class assignment for k by taking the majority vote among the k nearest neighbors. Finally, we also evaluate the computational efficiency of the proposed algorithm for DML by its efficiency.\nBaselines Besides the Euclidean distance that is used as a baseline similarity measure, six stateof-the-art DML methods are compared in our empirical study:\n\u2022 DuOri: This algorithm first applies Combined Stochastic Dual Coordinate Ascent (CSDCA) (ShalevShwartz and Zhang, 2012) to solve the dual problem in (2) and then computes the distance metric using the learned dual variables. \u2022 DuRP: This is the proposed algorithm for DML (i.e. Algorithm 1). \u2022 SRP: This algorithm applies random projection to project data into low dimensional space,\nand then it employs CSDCA to learn the distance metric in this subspace. \u2022 SPCA: This algorithm uses PCA as the initial step to reduce the dimensionality, and then\napplies CSDCA to learn the distance metric in the subspace generated by PCA. \u2022 OASIS (Chechik et al., 2010): A state-of-art online learning algorithm for DML that learns\nthe optimal distance metric directly from the original space without any dimensionality reduction. \u2022 LMNN (Weinberger and Saul, 2009): A state-of-art batch learning algorithm for DML. It performs the dimensionality reduction using PCA before starting DML.\nImplementation details We randomly select N = 100, 000 active triplets (i.e., incur the positive hinge loss by Euclidean distance) and set the number of epochs to be 3 for all stochastic methods (i.e., DuOri, DuRP, SRP, SPCA and OASIS), which yields sufficiently accurate solutions in our experiments and is also consistent with the observation in (Shalev-Shwartz and Zhang, 2012). We search \u03bb in {10\u22125, 10\u22124, 10\u22123, 10\u22122} and fix it as 1/N since it is insensitive. The step size of CSDCA is set according to the analysis in (Shalev-Shwartz and Zhang, 2012). For all stochastic optimization methods, we follow the one-projection paradigm by projecting the learned metric onto the PSD cone. The hinge loss is used in the implementation of the proposed algorithm. Both OASIS and LMNN use the implementation provided by the original authors and parameters are tuned based on the recommendation by the original authors. All methods are implemented in Matlab, except for LMNN, whose core part is implemented in C, which is shown to be more efficient than our Matlab implementation. All stochastic optimization methods are repeated five times and the average result over five trials is reported. All experiments are implemented on a Linux Server with 64GB memory and 12\u00d7 2.4GHz CPUs and only single thread is permitted for each experiment."}, {"heading": "4.2 Efficiency of the Proposed Method", "text": "In this experiment, we set the number of random projection to be 10, which according to experimental results in Section 4.3 and 4.4, yields almost the optimal performance for the proposed algorithm. For fair comparison, the number of reduced dimension is also set to be 10 for LMNN.\nTable. 2 compares the CPUtime (in minutes) of different methods. Notice that the time of sampling triplets is not taken into account as it is consumed by all the methods, and all the other operators (e.g., random projection and PCA) are included. It is not surprising to observe that DuRP, SRP and SPCA have similar CPUtimes, and are significantly more efficient than the other methods due to the effect of dimensionality reduction. Since DuRP and SRP share the same procedure for\ncomputing the dual variables in the subspace, the only difference between them lies in the procedure for reconstructing the distance metric from the estimated dual variables, a computational overhead that makes DuRP slightly slower than SRP. For all datasets, we observe that DuRP is at least 200 times faster than DuOri and 20 times faster than OASIS. Compared to the stochastic optimization methods, LMNN is the least efficient on three datasets (i.e., protein, caltech30 and 20news), mostly due to the fact that it is a batch learning algorithm."}, {"heading": "4.3 Evaluation by Ranking", "text": "In first experiment, we set the number of random projections used by SRP, SPCA and the proposed DuRP algorithm to be 10, which is roughly 1% of the dimensionality of the original space. For fair comparison, the number of reduced dimension for LMNN is also set to be 10. We measure the quality of learned metrics by its ranking performance using the metric of mAP.\nTable. 3 summarizes the performance of different methods for DML. First, we observe that DuRP significantly outperforms SRP and SPCA for all datasets. In fact, SRP is worse than Euclidean distance which computes the distance in the original space. SPCA is only able to perform better than the Euclidean distance, and is outperformed by all the other DML algorithms. Second, we observe that for all the datasets, DuRP yields similar performance as DuOri. The only difference\nbetween DuRP and DuOri is that DuOri solves the dual problem without using random projection. The comparison between DuRP and DuOri indicates that the random projection step has minimal impact on the learned distance metric, justifying the design of the proposed algorithm. Third, compared to OASIS, we observe that DuRP performs significantly better on two datasets (i.e., tdt30 and 20news) and has the comparable performance on the other datasets. Finally, we observe that for all datasets, the proposed DuRP method significantly outperforms LMNN, a state-of-the-art batch learning algorithm for DML. We also note that because of limited memory, we are unable to run LMNN on datasets rcv30.\nIn the second experiment, we vary the number of random projections from 10 to 50. All stochastic methods are run with five trails and Fig. 2 reports the average results with standard deviation. Note that the performance of OASIS and DuOri remain unchanged with varied number of projections because they do not use projection. It is surprising to observe that DuRP almost achieves its best performance with only 10 projections for all datasets. This is in contrast to SRP and SPCA, whose performance usually improves with increasing number of projections except for the data set usps where the performance of SPCA declines when the number of random projections is increased from 10 to 30. A detailed examination shows that the strange behavior for SPCA is due to its extreme low rank at 30 projections after the learned matrix is projected onto the PSD cone. More investigation is needed for this strange case. We also observe that DuRP outperforms DuOri for several datasets (i.e. protein, caltech30, tdt30 and 20news). We suspect that the better performance of DuRP is because of the implicit regularization due to the random projection. We plan to investigate more about the regularization capability of random projection in the future. We finally point out that with sufficiently large number of projections, SPCA is able to outperform OASIS on 3 datasets (i.e., protein, tdt30 and 20news), indicating that the comparison result may be sensitive to the number of projections."}, {"heading": "4.4 Evaluation by Classification", "text": "In this experiment, we evaluate the learned metric by its classification accuracy with k-NN (k = 5) classifier. We emphasize that the purpose of this experiment is to evaluate the metrics learned by different DML algorithms, not to demonstrate that the learned metric will result in the state-of-art classification performance2 . Similar to the evaluation by ranking, all experiments are run five times and the results averaged over five trials with standard deviation are reported in Fig. 3. We essentially have the same observation as that for the ranking experiments reported in Section 4.3 except that for most datasets, the three methods DuRP, DuOri, and OASIS yield very similar performance.\nNote the main concern of this paper is time efficiency and the size of learned metric is d\u00d7 d. It is straightforward to store the learned metric efficiently by keeping a low-rank approximation of it."}, {"heading": "5. Conclusion", "text": "In this paper, we propose a dual random projection method to learn the distance metric for largescale high-dimensional datasets. The main idea is to solve the dual problem in the subspace spanned by random projection, and then recover the distance metric in the original space using the estimated dual variables. We develop the theoretical guarantee that with a high probability, the proposed method can accurately recover the optimal solution with small error when the data matrix is of low rank, and the optimal dual variables even when the data matrix cannot be well approximated by a low rank matrix. Our empirical study confirms both the effectiveness and efficiency of the\n2. Many studies (e.g., (Weinberger and Saul, 2009; Xu et al., 2012)) have shown that metric learning do not yield better classification accuracy than the standard classification algorithms (e.g., SVM) given a sufficiently large number of training data.\nproposed algorithm for DML by comparing it to the state-of-the-art algorithms for DML. In the future, we plan to further improve the efficiency of our method by exploiting the scenario when optimal distance metric can be well approximated by a low rank matrix."}, {"heading": "Appendix A. Proof of Theorem 1", "text": "First, we want to prove that G\u0302 is a good estimation for G. We rewrite Ga,b by Kronecker product:\nGa,b = \u3008Aa, Ab\u3009 =\u3008(xai \u2212xak)(xai \u2212xak)\u22a4\u2212(xai \u2212xaj )(xai \u2212xaj )\u22a4, (xbi\u2212xbk)(xbi\u2212xbk)\u22a4\u2212(xbi\u2212xbj)(xbi\u2212xbj)\u22a4\u3009 =\u3008(xai \u2212xak)\u2297(xai \u2212xak)\u2212(xai \u2212xaj )\u2297(xai \u2212xaj ), (xbi\u2212xbk)\u2297(xbi\u2212xbk)\u2212(xbi\u2212xbj)\u2297(xbi\u2212xbj)\u3009 = \u3008za, zb\u3009\nwhere zt = (xti \u2212 xtk) \u2297 (xti \u2212 xtk) \u2212 (xti \u2212 xtj) \u2297 (xti \u2212 xtj). Define Z = [z1, \u00b7 \u00b7 \u00b7 , zN ], we have G = Z\u22a4Z .\nUnder the low rank assumption that all training examples lie in the subspace of r-dimension, the dataset X = [x1, \u00b7 \u00b7 \u00b7 ,xn] can be decomposed as:\nX = U\u03a3V =\nr\u2211\ni=1\n\u03bbiuiv \u22a4 i\nwhere \u03bbi is the i-th singular value of X, and ui and vi are the corresponding left and right singular vectors of X. Given the property of Kronecker product that (A \u2297 B)(C \u2297 D) = (AC) \u2297 (BD), we have:\nzt = (x t i \u2212 xtk)\u2297 (xti \u2212 xtk)\u2212 (xti \u2212 xtj)\u2297 (xti \u2212 xtj)\n= [U(x\u0303ti \u2212 x\u0303tk)]\u2297 [U(x\u0303ti \u2212 x\u0303tk)]\u2212 [U(x\u0303ti \u2212 x\u0303tj)]\u2297 [U(x\u0303ti \u2212 x\u0303tj)] = (U \u2297 U) [ (x\u0303ti \u2212 x\u0303tk)\u2297 (x\u0303ti \u2212 x\u0303tk)\u2212 (x\u0303ti \u2212 x\u0303tj)\u2297 (x\u0303ti \u2212 x\u0303tj) ]\nwhere x\u0303ti = U \u22a4xti. Define Z\u0303 = (z\u03031, . . . , z\u0303n), where z\u0303t = (x\u0303 t i \u2212 x\u0303tj) \u2297 (x\u0303ti \u2212 x\u0303tj) \u2212 (x\u0303ti \u2212 x\u0303tk) \u2297 (x\u0303ti \u2212 x\u0303tk), we have:\nG = Z\u0303\u22a4(U\u22a4 \u2297 U\u22a4)(U \u2297 U)Z\u0303 = Z\u0303\u22a4(Ir \u2297 Ir)Z\u0303 = Z\u0303\u22a4Z\u0303\nwhere Ir \u2297 Ir equals to the identity operator of r2 \u00d7 r2. With the random projection approximation, we have:\nzt = (R \u22a4(xti \u2212 xtk))\u2297 (R\u22a4(xti \u2212 xtk))\u2212 (R\u22a4(xti \u2212 xtj))\u2297 (R\u22a4(xti \u2212 xtj))\n= [R\u22a4U(x\u0303ti \u2212 x\u0303tk)]\u2297 [R\u22a4U(x\u0303ti \u2212 x\u0303tk)]\u2212 [R\u22a4U(x\u0303ti \u2212 x\u0303tj)]\u2297 [R\u22a4U(x\u0303ti \u2212 x\u0303tj)] = (R\u22a4 \u2297R\u22a4)(U \u2297 U) [ (x\u0303ti \u2212 x\u0303tk)\u2297 (x\u0303ti \u2212 x\u0303tk)\u2212 (x\u0303ti \u2212 x\u0303tj)\u2297 (x\u0303ti \u2212 x\u0303tj) ]\nSo,\nG\u0302 = Z\u0303\u22a4(U\u22a4 \u2297 U\u22a4)(RR\u22a4 \u2297RR\u22a4)(U \u2297 U)Z\u0303 = Z\u0303([U\u22a4RR\u22a4U ]\u2297 [U\u22a4RR\u22a4U ])Z\u0303\nIn order to bound the difference between G and G\u0302, we need the following corollary:\nCorollary 3 (Zhang et al., 2013) Let S \u2208 Rr\u00d7m be a standard Gaussian random matrix. Then, for any 0 < \u03b5 \u2264 1/2, with a probability 1\u2212 \u03b4, we have\n\u2225\u2225\u2225\u2225 1\nm SS\u22a4 \u2212 I \u2225\u2225\u2225\u2225 2 \u2264 \u03b5\nprovided\nm \u2265 (r + 1) log(2r/\u03b4) c\u03b52\nwhere constant c is at least 1/4.\nDefine \u2206 = U\u22a4RR\u22a4U \u2212 Ir. Using Corollary. 3, with a probability 1 \u2212 \u03b4, we have \u2016\u2206\u20162 \u2264 \u03b5. Using the notation \u2206, we have the following expression for G\u0302\u2212G\nG\u0302\u2212G = Z\u0303\u22a4 ((Ir +\u2206)\u2297 (Ir +\u2206)\u2212 Ir \u2297 Ir) Z\u0303 = Z\u0303\u22a4 (\u2206\u2297 Ir + Ir \u2297\u2206+\u2206\u2297\u2206) Z\u0303 = Z\u0303\u22a4\u0393Z\u0303\nwhere \u0393 = \u2206\u2297 Ir + Ir \u2297\u2206+\u2206\u2297\u2206. Using the fact that the eigenvalue values of A\u2297B is given by \u03bbi(A)\u03bbj(B), it is easy to verify that,\n\u2016\u0393\u20162 \u2264 \u2016\u2206\u20162 + \u2016\u2206\u20162 + \u2016\u2206\u20162\u2016\u2206\u20162 Using the fact that \u2016\u2206\u20162 \u2264 \u03b5 and taking \u01eb \u2264 1/6 which results in c \u2265 1/3, with a probability 1\u2212 \u03b4, we have\n\u2016\u0393\u20162 \u2264 3\u03b5 (8)\nDefine L(\u03b1) and L\u0302(\u03b1) as\nL(\u03b1) = \u2212 n\u2211\ni=1\n\u2113\u2217(\u03b1i)\u2212 1\n2\u03bbN \u03b1\n\u22a4G\u03b1,\nL\u0302(\u03b1) = \u2212 n\u2211\ni=1\n\u2113\u2217(\u03b1i)\u2212 1\n2\u03bbN \u03b1\n\u22a4G\u0302\u03b1\nWe are now ready to give the proof for Theorem 1. The basic logic is straightforward. Since G\u0302 is close to G, we would expect \u03b1\u0302\u2217, the optimal solution to L\u0302(\u03b1), to be close to \u03b1\u2217, the optimal solution to L(\u03b1). Since both M\u2217 and M\u0302\u2217 are linear in the dual variables \u03b1\u2217 and \u03b1\u0302\u2217, we would expect M\u0302\u2217 to be close to M\u2217.\nSince \u03b1\u0302\u2217 maximizes L\u0302(\u03b1) over its domain, which means (\u03b1\u2217 \u2212 \u03b1\u0302\u2217)\u22a4\u2207L\u0302(\u03b1\u0302\u2217) \u2264 0, we have\nL\u0302(\u03b1\u0302\u2217) \u2265 L\u0302(\u03b1\u2217) + 1\n2\u03bbN (\u03b1\u0302\u2217 \u2212\u03b1\u2217)\u22a4G\u0302(\u03b1\u0302\u2217 \u2212\u03b1\u2217) (9)\nUsing the concaveness of L\u0302(\u03b1) and the fact that \u03b1\u2217 maximizes L(\u03b1) over its domain, we have:\nL\u0302(\u03b1\u0302\u2217) \u2264 L\u0302(\u03b1\u2217) + (\u03b1\u0302\u2217 \u2212\u03b1\u2217)\u22a4\u2207L\u0302(\u03b1\u2217)\u2212 1\n2\u03bbN (\u03b1\u0302\u2217 \u2212\u03b1\u2217)\u22a4G\u0302(\u03b1\u0302\u2217 \u2212\u03b1\u2217)\n= L\u0302(\u03b1\u2217)\u2212 1\n2\u03bbN (\u03b1\u0302\u2217 \u2212\u03b1\u2217)\u22a4G\u0302(\u03b1\u0302\u2217 \u2212\u03b1\u2217) + (\u03b1\u0302\u2217 \u2212\u03b1\u2217)\u22a4\n( \u2207L\u0302(\u03b1\u2217)\u2212\u2207L(\u03b1\u2217) +\u2207L(\u03b1\u2217) )\n\u2264 L\u0302(\u03b1\u2217) + 1\n\u03bbN (\u03b1\u0302\u2217 \u2212\u03b1\u2217)\u22a4(G\u2212 G\u0302)(\u03b1\u2217)\u2212\n1\n2\u03bbN (\u03b1\u0302\u2217 \u2212\u03b1\u2217)\u22a4G\u0302(\u03b1\u0302\u2217 \u2212\u03b1\u2217) (10)\nCombining the inequalities in (11) and (12), we have\n1\n\u03bbN (\u03b1\u0302\u2212\u03b1\u2217)\u22a4(G\u2212 G\u0302)\u03b1\u2217 \u2265\n1\n\u03bbN (\u03b1\u0302\u2212\u03b1\u2217)\u22a4G\u0302(\u03b1\u0302\u2212\u03b1\u2217)\nor\n(\u03b1\u2217 \u2212 \u03b1\u0302\u2217)\u22a4(G\u0302\u2212G)\u03b1\u2217 \u2265 (\u03b1\u0302\u2217 \u2212\u03b1\u2217)\u22a4G(\u03b1\u0302\u2217 \u2212\u03b1\u2217) + (\u03b1\u0302\u2217 \u2212\u03b1\u2217)\u22a4(G\u0302\u2212G)(\u03b1\u0302\u2217 \u2212\u03b1\u2217)\nDefine p\u2217 = Z\u0303\u03b1\u2217, p\u0302\u2217 = Z\u0303\u03b1\u0302\u2217, we have:\n(p\u2217 \u2212 p\u0302\u2217)\u22a4\u0393p\u2217 \u2265 \u2016p\u0302\u2217 \u2212 p\u2217\u201622 + (p\u0302\u2217 \u2212 p\u2217)\u22a4\u0393(p\u0302\u2217 \u2212 p\u2217)\nUsing the bound given in (8), with a probability 1\u2212 \u03b4, we have\n\u2016p\u0302\u2217 \u2212 p\u2217\u20162 \u2264 3\u03b5\n1\u2212 3\u03b5\u2016p\u2217\u20162\nWe complete the proof by using the fact\n\u2016M\u2217 \u2212 M\u0302\u2217\u2016F = 1\n\u03bbN \u2016p\u2217 \u2212 p\u0302\u2217\u20162, \u2016M\u2217\u2016F =\n1\n\u03bbN \u2016p\u2217\u20162\nand (Stark et al., 1998)\n\u2016\u03a0PSD(M\u2217)\u2212\u03a0PSD(M\u0302\u2217)\u2016F \u2264 \u2016M\u2217 \u2212 M\u0302\u2217\u2016F"}, {"heading": "Appendix B. Proof of Theorem 2", "text": "Our analysis is based on the following two theorems. Theorem 4 (Theorem 2 (Blum, 2005)) Let x \u2208 Rd, and x\u0302 = R\u22a4x/\u221am, where R \u2208 Rd\u00d7m is a random matrix whose entries are chosen independently from N (0, 1). Then:\nPr { (1\u2212 \u03b5)\u2016x\u201622 \u2264 \u2016x\u0302\u201622 \u2264 (1 + \u03b5)\u2016x\u201622 } \u2265 1\u2212 2 exp ( \u2212m\n4 (\u03b52 \u2212 \u03b53)\n) .\nTheorem 5 (Lemma B-1 (Karoui, 2010)) Suppose M is a real symmetric matrix with non-negative entries, and E is a real symmetric matrix such that maxi,j |Ei,j| \u2264 \u03be. Then, \u2016E \u25e6M\u20162 \u2264 \u03be\u2016M\u20162, where \u2016 \u00b7 \u20162 stands for the spectral norm of matrix and E \u25e6M is the element-wise product between matrices E and M .\nDefine L(\u03b1) and L\u0302(\u03b1) as\nL(\u03b1) = \u2212 N\u2211\ni=1\n\u2113\u2217(\u03b1i)\u2212 1\n2\u03bbN \u03b1\n\u22a4G\u03b1,\nL\u0302(\u03b1) = \u2212 N\u2211\ni=1\n\u2113\u2217(\u03b1i)\u2212 1\n2\u03bbN \u03b1\n\u22a4G\u0302\u03b1\nSince \u2113(z) is \u03b3-smooth, we have \u2113\u2217(\u03b1) be \u03b3\u22121-strongly-convex. Using the fact that \u03b1\u0302\u2217 approximately maximizes L\u0302(\u03b1) with \u03b7-suboptimality and \u2113\u2217(\u00b7) is \u03b3\u22121-strongly-convex, we have\nL\u0302(\u03b1\u0302\u2217) \u2265 L\u0302(\u03b1\u2217)+ 1\n2\u03bbN (\u03b1\u0302\u2217 \u2212\u03b1\u2217)\u22a4(\u03b3\u22121I + G\u0302)(\u03b1\u0302\u2217 \u2212\u03b1\u2217)\u2212\u03b7 (11)\nUsing the concaveness of L\u0302(\u03b1) and the fact that \u03b1\u2217 maximizes L(\u03b1) over its domain, we have:\nL\u0302(\u03b1\u0302\u2217) \u2264 L\u0302(\u03b1\u2217) + 1\n\u03bbN (\u03b1\u0302\u2217 \u2212\u03b1\u2217)\u22a4(G\u2212 G\u0302)\u03b1\u2217 \u2212\n1\n2\u03bbN (\u03b1\u0302\u2217 \u2212\u03b1\u2217)\u22a4(\u03b3\u22121I + G\u0302)(\u03b1\u0302\u2217 \u2212\u03b1\u2217) (12)\nCombining the inequalities in (11) and (12), we have\n\u03b7 + (\u03b1\u2217 \u2212 \u03b1\u0302\u2217)\u22a4(G\u0302\u2212G)\u03b1\u2217 \u2265 1\n\u03b3 \u2016\u03b1\u0302\u2217 \u2212\u03b1\u2217\u201622\nwhen we set \u03bb = 1/N . Using the fact (\u03b1\u2217 \u2212 \u03b1\u0302\u2217)(G\u0302\u2212G)\u03b1\u2217 \u2264 \u2016\u03b1\u2217 \u2212 \u03b1\u0302\u2217\u20162\u2016\u03b1\u2217\u20162\u2016G\u0302 \u2212G\u20162, we have\n\u2016\u03b1\u2217 \u2212 \u03b1\u0302\u2217\u201622 \u2264 \u03b3\u2016\u03b1\u2217 \u2212 \u03b1\u0302\u2217\u20162\u2016\u03b1\u2217\u20162\u2016G\u0302\u2212G\u20162 + \u03b3\u03b7,\nimplying that\n\u2016\u03b1\u2217 \u2212 \u03b1\u0302\u2217\u20162 \u2264 max ( 2\u03b3\u2016G\u0302\u2212G\u20162\u2016\u03b1\u2217\u20162, \u221a 2\u03b3\u03b7 ) . (13)\nTo bound \u2016\u03b1\u2217 \u2212 \u03b1\u0302\u2217\u20162, we need to bound \u2016G\u0302\u2212G\u20162. To this end, we write the Ga,b as\nGa,b = \u3008Aa, Ab\u3009\n= [ (xai \u2212xak)\u22a4(xbi\u2212xbk) ]2 + [ (xai \u2212xaj )\u22a4(xbi\u2212xbj) ]2 \u2212 [ (xai \u2212xak)\u22a4(xbi\u2212xbj) ]2 \u2212 [ (xai \u2212xaj )\u22a4(xbi\u2212xbk) ]2\nSimilarly, we write G\u0302a,b as\nG\u0302a,b = \u3008R\u22a4AaR,R\u22a4AbR\u3009 = [ (xai \u2212xak)\u22a4RR\u22a4(xbi\u2212xbk) ]2 + [ (xai \u2212xaj )\u22a4RR\u22a4(xbi\u2212xbj) ]2 \u2212 [ (xai \u2212xak)\u22a4RR\u22a4(xbi\u2212xbj) ]2\u2212 [ (xai \u2212xaj )\u22a4RR\u22a4(xbi\u2212xbk) ]2\nHence, we can write G\u0302\u2212G = B1 +B2 +B3 +B4, where B1, B2, B3, and B4 are defined as\nB1a,b = [ (xai \u2212xak)\u22a4RR\u22a4(xbi\u2212xbk) ]2 \u2212 [ (xai \u2212xak)\u22a4(xbi\u2212xbk) ]2 B2a,b = [ (xai \u2212xaj )\u22a4RR\u22a4(xbi\u2212xbj) ]2 \u2212 [ (xai \u2212xaj )\u22a4(xbi\u2212xbj) ]2 B3a,b = [ (xai \u2212xak)\u22a4(xbi\u2212xbj) ]2 \u2212 [ (xai \u2212xak)\u22a4RR\u22a4(xbi\u2212xbj) ]2 B4a,b = [ (xai \u2212xaj )\u22a4(xbi\u2212xbk) ]2 \u2212 [ (xai \u2212xaj )\u22a4RR\u22a4(xbi\u2212xbk) ]2\nUsing the result from Theorem 4 and the definition of matrices M1, M2, M3, M4, we have, with a probability 1\u2212 \u03b4, for any a, b,\n|Bia,b| \u2264 \u01ebM ia,b, i = {1, 2, 3, 4}\nprovided that \u01eb \u2264 1/2 and\nm \u2265 8 \u01eb2 ln 8N \u03b4 (14)\nUsing Theorem 5, under the condition in (14), we have, with a probability 1\u2212 \u03b4,\n\u2016G\u0302\u2212G\u20162\u2264\u01eb ( \u2016M1\u2016+\u2016M2\u2016+\u2016M3\u2016+\u2016M4\u2016 ) \u22644\u03ba\u01eb\nwhere the last step uses the definition of \u03ba. We complete the proof by plugging the bound for \u2016G\u0302\u2212G\u20162 into (13)."}], "references": [{"title": "Robustness and generalization for metric learning", "author": ["Aur\u00e9lien Bellet", "Amaury Habrard"], "venue": "CoRR, abs/1209.1086,", "citeRegEx": "Bellet and Habrard.,? \\Q2012\\E", "shortCiteRegEx": "Bellet and Habrard.", "year": 2012}, {"title": "Random projection, margins, kernels, and feature-selection", "author": ["Avrim Blum"], "venue": "In SLSFS, pages 52\u201368,", "citeRegEx": "Blum.,? \\Q2005\\E", "shortCiteRegEx": "Blum.", "year": 2005}, {"title": "Random projections for k-means clustering", "author": ["Christos Boutsidis", "Anastasios Zouzias", "Petros Drineas"], "venue": "In NIPS,", "citeRegEx": "Boutsidis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2010}, {"title": "Probabilistic dyadic data analysis with local and global consistency", "author": ["Deng Cai", "Xuanhui Wang", "Xiaofei He"], "venue": "In ICML,", "citeRegEx": "Cai et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2009}, {"title": "Libsvm: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM TIST,", "citeRegEx": "Chang and Lin.,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin.", "year": 2011}, {"title": "Large scale online learning of image similarity through ranking", "author": ["Gal Chechik", "Varun Sharma", "Uri Shalit", "Samy Bengio"], "venue": "JMLR, 11:1109\u20131135,", "citeRegEx": "Chechik et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chechik et al\\.", "year": 2010}, {"title": "Structured metric learning for high dimensional problems", "author": ["Jason V. Davis", "Inderjit S. Dhillon"], "venue": "In KDD,", "citeRegEx": "Davis and Dhillon.,? \\Q2008\\E", "shortCiteRegEx": "Davis and Dhillon.", "year": 2008}, {"title": "Information-theoretic metric learning", "author": ["Jason V. Davis", "Brian Kulis", "Prateek Jain", "Suvrit Sra", "Inderjit S. Dhillon"], "venue": "In ICML,", "citeRegEx": "Davis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2007}, {"title": "Experiments with random projections for machine learning", "author": ["Dmitriy Fradkin", "David Madigan"], "venue": "In KDD,", "citeRegEx": "Fradkin and Madigan.,? \\Q2003\\E", "shortCiteRegEx": "Fradkin and Madigan.", "year": 2003}, {"title": "Metric learning by collapsing classes", "author": ["Amir Globerson", "Sam T. Roweis"], "venue": "In NIPS,", "citeRegEx": "Globerson and Roweis.,? \\Q2005\\E", "shortCiteRegEx": "Globerson and Roweis.", "year": 2005}, {"title": "Projection-free online learning", "author": ["Elad Hazan", "Satyen Kale"], "venue": "In ICML,", "citeRegEx": "Hazan and Kale.,? \\Q2012\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2012}, {"title": "Regularized distance metric learning: Theory and algorithm", "author": ["Rong Jin", "Shijun Wang", "Yang Zhou"], "venue": "In NIPS,", "citeRegEx": "Jin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2009}, {"title": "The spectrum of kernel random matrices", "author": ["Noureddine El Karoui"], "venue": "In The Annals of Statistics,", "citeRegEx": "Karoui.,? \\Q2010\\E", "shortCiteRegEx": "Karoui.", "year": 2010}, {"title": "Metric learning: A survey", "author": ["Brian Kulis"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Kulis.,? \\Q2013\\E", "shortCiteRegEx": "Kulis.", "year": 2013}, {"title": "Robust structural metric learning", "author": ["Daryl Lim", "Gert Lanckriet", "Brian McFee"], "venue": "In ICML,", "citeRegEx": "Lim et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lim et al\\.", "year": 2013}, {"title": "Stochastic gradient descent with only one projection", "author": ["M. Mahdavi", "T. Yang", "R. Jin", "S. Zhu", "J. Yi"], "venue": "In NIPS,", "citeRegEx": "Mahdavi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mahdavi et al\\.", "year": 2012}, {"title": "Linear regression with random projections", "author": ["Odalric-Ambrym Maillard", "Remi Munos"], "venue": "In JMLR,", "citeRegEx": "Maillard and Munos.,? \\Q2012\\E", "shortCiteRegEx": "Maillard and Munos.", "year": 2012}, {"title": "An efficient sparse metric learning in high-dimensional space via l1-penalized log-determinant regularization", "author": ["Guo-Jun Qi", "Jinhui Tang", "Zheng-Jun Zha", "Tat-Seng Chua", "Hong-Jiang Zhang"], "venue": null, "citeRegEx": "Qi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Qi et al\\.", "year": 2009}, {"title": "Random features for large-scale kernel machines", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": "In NIPS,", "citeRegEx": "Rahimi and Recht.,? \\Q2007\\E", "shortCiteRegEx": "Rahimi and Recht.", "year": 2007}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "CoRR, abs/1209.1873,", "citeRegEx": "Shalev.Shwartz and Zhang.,? \\Q2012\\E", "shortCiteRegEx": "Shalev.Shwartz and Zhang.", "year": 2012}, {"title": "Learning a distance metric from a network", "author": ["Blake Shaw", "Bert C. Huang", "Tony Jebara"], "venue": "In NIPS, pages 1899\u20131907,", "citeRegEx": "Shaw et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shaw et al\\.", "year": 2011}, {"title": "Vector space projections: a numerical approach to signal and image processing, neural nets, and optics", "author": ["Henry Stark", "Yongi Yang", "Yongyi Yang"], "venue": null, "citeRegEx": "Stark et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Stark et al\\.", "year": 1998}, {"title": "Manifold modeling with learned distance in random projection space for face recognition", "author": ["Grigorios Tsagkatakis", "Andreas E. Savakis"], "venue": "In ICPR,", "citeRegEx": "Tsagkatakis and Savakis.,? \\Q2010\\E", "shortCiteRegEx": "Tsagkatakis and Savakis.", "year": 2010}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["Kilian Q. Weinberger", "Lawrence K. Saul"], "venue": "JMLR, 10:207\u2013244,", "citeRegEx": "Weinberger and Saul.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul.", "year": 2009}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["Eric P. Xing", "Andrew Y. Ng", "Michael I. Jordan", "Stuart J. Russell"], "venue": "In NIPS,", "citeRegEx": "Xing et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2002}, {"title": "Distance metric learning for kernel machines", "author": ["Zhixiang Eddie Xu", "Kilian Q. Weinberger", "Olivier Chapelle"], "venue": "CoRR, abs/1208.3422,", "citeRegEx": "Xu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2012}, {"title": "Distance metric learning: a comprehensive survery", "author": ["Liu Yang", "Rong Jin"], "venue": null, "citeRegEx": "Yang and Jin.,? \\Q2006\\E", "shortCiteRegEx": "Yang and Jin.", "year": 2006}, {"title": "Recovering optimal solution by dual random projection", "author": ["Lijun Zhang", "Mehrdad Mah-davi", "Rong Jin", "Tian-Bao Yang", "Shenghuo Zhu"], "venue": "In arXiv:1211.3046,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "Introduction Distance metric learning (DML) is essential to many machine learning tasks, including ranking (Chechik et al., 2010; Lim et al., 2013), k-nearest neighbor (k-NN) classification (Weinberger and Saul, 2009) and k-means clustering (Xing et al.", "startOffset": 107, "endOffset": 147}, {"referenceID": 14, "context": "Introduction Distance metric learning (DML) is essential to many machine learning tasks, including ranking (Chechik et al., 2010; Lim et al., 2013), k-nearest neighbor (k-NN) classification (Weinberger and Saul, 2009) and k-means clustering (Xing et al.", "startOffset": 107, "endOffset": 147}, {"referenceID": 23, "context": ", 2013), k-nearest neighbor (k-NN) classification (Weinberger and Saul, 2009) and k-means clustering (Xing et al.", "startOffset": 50, "endOffset": 77}, {"referenceID": 24, "context": ", 2013), k-nearest neighbor (k-NN) classification (Weinberger and Saul, 2009) and k-means clustering (Xing et al., 2002).", "startOffset": 101, "endOffset": 120}, {"referenceID": 24, "context": "It finds a good metric by minimizing the distance between data pairs in the same classes and maximizing the distance between data pairs from different classes (Xing et al., 2002; Globerson and Roweis, 2005; Yang and Jin, 2006; Davis et al., 2007; Weinberger and Saul, 2009; Shaw et al., 2011).", "startOffset": 159, "endOffset": 292}, {"referenceID": 9, "context": "It finds a good metric by minimizing the distance between data pairs in the same classes and maximizing the distance between data pairs from different classes (Xing et al., 2002; Globerson and Roweis, 2005; Yang and Jin, 2006; Davis et al., 2007; Weinberger and Saul, 2009; Shaw et al., 2011).", "startOffset": 159, "endOffset": 292}, {"referenceID": 26, "context": "It finds a good metric by minimizing the distance between data pairs in the same classes and maximizing the distance between data pairs from different classes (Xing et al., 2002; Globerson and Roweis, 2005; Yang and Jin, 2006; Davis et al., 2007; Weinberger and Saul, 2009; Shaw et al., 2011).", "startOffset": 159, "endOffset": 292}, {"referenceID": 7, "context": "It finds a good metric by minimizing the distance between data pairs in the same classes and maximizing the distance between data pairs from different classes (Xing et al., 2002; Globerson and Roweis, 2005; Yang and Jin, 2006; Davis et al., 2007; Weinberger and Saul, 2009; Shaw et al., 2011).", "startOffset": 159, "endOffset": 292}, {"referenceID": 23, "context": "It finds a good metric by minimizing the distance between data pairs in the same classes and maximizing the distance between data pairs from different classes (Xing et al., 2002; Globerson and Roweis, 2005; Yang and Jin, 2006; Davis et al., 2007; Weinberger and Saul, 2009; Shaw et al., 2011).", "startOffset": 159, "endOffset": 292}, {"referenceID": 20, "context": "It finds a good metric by minimizing the distance between data pairs in the same classes and maximizing the distance between data pairs from different classes (Xing et al., 2002; Globerson and Roweis, 2005; Yang and Jin, 2006; Davis et al., 2007; Weinberger and Saul, 2009; Shaw et al., 2011).", "startOffset": 159, "endOffset": 292}, {"referenceID": 5, "context": "In a recent study (Chechik et al., 2010), the authors show empirically that it is possible to learn a good distance metric using online learning without having to perform the projection at each iteration.", "startOffset": 18, "endOffset": 40}, {"referenceID": 6, "context": "In (Davis and Dhillon, 2008), the authors assume that the learned metric M is of low rank, and write it as M = LL\u22a4, where L \u2208 Rd\u00d7r with r \u226a d.", "startOffset": 3, "endOffset": 28}, {"referenceID": 23, "context": "A similar idea was studied in (Weinberger and Saul, 2009).", "startOffset": 30, "endOffset": 57}, {"referenceID": 23, "context": "An alternative approach is to reduce the dimensionality of data using dimensionality reduction methods such as principal component analysis (PCA) (Weinberger and Saul, 2009) or random projection (RP) (Tsagkatakis and Savakis, 2010).", "startOffset": 146, "endOffset": 173}, {"referenceID": 22, "context": "An alternative approach is to reduce the dimensionality of data using dimensionality reduction methods such as principal component analysis (PCA) (Weinberger and Saul, 2009) or random projection (RP) (Tsagkatakis and Savakis, 2010).", "startOffset": 200, "endOffset": 231}, {"referenceID": 8, "context": "Although RP is computationally more efficient than PCA, it often yields significantly worse performance than PCA unless the number of random projections is sufficiently large (Fradkin and Madigan, 2003).", "startOffset": 175, "endOffset": 202}, {"referenceID": 18, "context": ", classification (Rahimi and Recht, 2007), clustering (Boutsidis et al.", "startOffset": 17, "endOffset": 41}, {"referenceID": 2, "context": ", classification (Rahimi and Recht, 2007), clustering (Boutsidis et al., 2010) and regression (Maillard and Munos, 2012), only a few studies examined the application of RP to DML, and most of them with limited success.", "startOffset": 54, "endOffset": 78}, {"referenceID": 16, "context": ", 2010) and regression (Maillard and Munos, 2012), only a few studies examined the application of RP to DML, and most of them with limited success.", "startOffset": 23, "endOffset": 49}, {"referenceID": 27, "context": "We finally note that our work is built upon the recent work (Zhang et al., 2013) on random projection where a dual random projection algorithm is developed for linear classification.", "startOffset": 60, "endOffset": 80}, {"referenceID": 27, "context": "Our work differs from (Zhang et al., 2013) in that we apply the theory of dual random projection to DML.", "startOffset": 22, "endOffset": 42}, {"referenceID": 27, "context": "Unlike the theory in (Zhang et al., 2013) where the data matrix is assumed to be low rank or approximately low rank, our new theory of dual random projection is applicable to any", "startOffset": 21, "endOffset": 41}, {"referenceID": 10, "context": "We note that this is different from the algorithms presented in (Hazan and Kale, 2012; Mahdavi et al., 2012).", "startOffset": 64, "endOffset": 108}, {"referenceID": 15, "context": "We note that this is different from the algorithms presented in (Hazan and Kale, 2012; Mahdavi et al., 2012).", "startOffset": 64, "endOffset": 108}, {"referenceID": 24, "context": "Many algorithms have been developed for DML (Xing et al., 2002; Globerson and Roweis, 2005; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 44, "endOffset": 138}, {"referenceID": 9, "context": "Many algorithms have been developed for DML (Xing et al., 2002; Globerson and Roweis, 2005; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 44, "endOffset": 138}, {"referenceID": 7, "context": "Many algorithms have been developed for DML (Xing et al., 2002; Globerson and Roweis, 2005; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 44, "endOffset": 138}, {"referenceID": 23, "context": "Many algorithms have been developed for DML (Xing et al., 2002; Globerson and Roweis, 2005; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 44, "endOffset": 138}, {"referenceID": 9, "context": "Exemplar DML algorithms are MCML (Globerson and Roweis, 2005), ITML (Davis et al.", "startOffset": 33, "endOffset": 61}, {"referenceID": 7, "context": "Exemplar DML algorithms are MCML (Globerson and Roweis, 2005), ITML (Davis et al., 2007), LMNN (Weinberger and Saul, 2009) and OASIS (Chechik et al.", "startOffset": 68, "endOffset": 88}, {"referenceID": 23, "context": ", 2007), LMNN (Weinberger and Saul, 2009) and OASIS (Chechik et al.", "startOffset": 14, "endOffset": 41}, {"referenceID": 5, "context": ", 2007), LMNN (Weinberger and Saul, 2009) and OASIS (Chechik et al., 2010).", "startOffset": 52, "endOffset": 74}, {"referenceID": 11, "context": "Besides algorithms, several studies were devoted to analyzing the generalization performance of DML (Jin et al., 2009; Bellet and Habrard, 2012).", "startOffset": 100, "endOffset": 144}, {"referenceID": 0, "context": "Besides algorithms, several studies were devoted to analyzing the generalization performance of DML (Jin et al., 2009; Bellet and Habrard, 2012).", "startOffset": 100, "endOffset": 144}, {"referenceID": 26, "context": "Survey papers (Yang and Jin, 2006; Kulis, 2013) provide detailed investigation about the topic.", "startOffset": 14, "endOffset": 47}, {"referenceID": 13, "context": "Survey papers (Yang and Jin, 2006; Kulis, 2013) provide detailed investigation about the topic.", "startOffset": 14, "endOffset": 47}, {"referenceID": 6, "context": "Although numerous studies were devoted to DML, only a limited progress is made to address the high dimensional challenge in DML (Davis and Dhillon, 2008; Weinberger and Saul, 2009; Qi et al., 2009; Lim et al., 2013).", "startOffset": 128, "endOffset": 215}, {"referenceID": 23, "context": "Although numerous studies were devoted to DML, only a limited progress is made to address the high dimensional challenge in DML (Davis and Dhillon, 2008; Weinberger and Saul, 2009; Qi et al., 2009; Lim et al., 2013).", "startOffset": 128, "endOffset": 215}, {"referenceID": 17, "context": "Although numerous studies were devoted to DML, only a limited progress is made to address the high dimensional challenge in DML (Davis and Dhillon, 2008; Weinberger and Saul, 2009; Qi et al., 2009; Lim et al., 2013).", "startOffset": 128, "endOffset": 215}, {"referenceID": 14, "context": "Although numerous studies were devoted to DML, only a limited progress is made to address the high dimensional challenge in DML (Davis and Dhillon, 2008; Weinberger and Saul, 2009; Qi et al., 2009; Lim et al., 2013).", "startOffset": 128, "endOffset": 215}, {"referenceID": 6, "context": "In (Davis and Dhillon, 2008; Weinberger and Saul, 2009), the authors address the challenge of high dimensionality by enforcing the distance metric to be a low rank matrix.", "startOffset": 3, "endOffset": 55}, {"referenceID": 23, "context": "In (Davis and Dhillon, 2008; Weinberger and Saul, 2009), the authors address the challenge of high dimensionality by enforcing the distance metric to be a low rank matrix.", "startOffset": 3, "endOffset": 55}, {"referenceID": 17, "context": "(Qi et al., 2009; Lim et al., 2013) alleviate the challenge of learning a distance metric M from high dimensional data by assuming M to be a sparse matrix.", "startOffset": 0, "endOffset": 35}, {"referenceID": 14, "context": "(Qi et al., 2009; Lim et al., 2013) alleviate the challenge of learning a distance metric M from high dimensional data by assuming M to be a sparse matrix.", "startOffset": 0, "endOffset": 35}, {"referenceID": 18, "context": "Random projection is widely used for dimension reduction in various learning tasks (Rahimi and Recht, 2007; Boutsidis et al., 2010; Maillard and Munos, 2012).", "startOffset": 83, "endOffset": 157}, {"referenceID": 2, "context": "Random projection is widely used for dimension reduction in various learning tasks (Rahimi and Recht, 2007; Boutsidis et al., 2010; Maillard and Munos, 2012).", "startOffset": 83, "endOffset": 157}, {"referenceID": 16, "context": "Random projection is widely used for dimension reduction in various learning tasks (Rahimi and Recht, 2007; Boutsidis et al., 2010; Maillard and Munos, 2012).", "startOffset": 83, "endOffset": 157}, {"referenceID": 8, "context": "Unfortunately, it requires a large amount of random projections for the desired result (Fradkin and Madigan, 2003), and this limits its application in DML, where the computational cost is proportion to the square of dimensions.", "startOffset": 87, "endOffset": 114}, {"referenceID": 27, "context": "Dual random projection is first introduced for linear classification task (Zhang et al., 2013) and following aspects make our work significantly different from the initial study (Zhang et al.", "startOffset": 74, "endOffset": 94}, {"referenceID": 27, "context": ", 2013) and following aspects make our work significantly different from the initial study (Zhang et al., 2013): First, we apply dual random projection for DML, where the number of variables is quadratic to the dimension and the dimension crisis is more serious than linear classifier.", "startOffset": 91, "endOffset": 111}, {"referenceID": 27, "context": "Last, we give the theoretical guarantee when the dataset is not low rank, which is an important assumption for the study (Zhang et al., 2013).", "startOffset": 121, "endOffset": 141}, {"referenceID": 23, "context": "This is because several previous studies have suggested that triplet constraints are more effective than pairwise constraints (Weinberger and Saul, 2009; Chechik et al., 2010; Shaw et al., 2011).", "startOffset": 126, "endOffset": 194}, {"referenceID": 5, "context": "This is because several previous studies have suggested that triplet constraints are more effective than pairwise constraints (Weinberger and Saul, 2009; Chechik et al., 2010; Shaw et al., 2011).", "startOffset": 126, "endOffset": 194}, {"referenceID": 20, "context": "This is because several previous studies have suggested that triplet constraints are more effective than pairwise constraints (Weinberger and Saul, 2009; Chechik et al., 2010; Shaw et al., 2011).", "startOffset": 126, "endOffset": 194}, {"referenceID": 5, "context": "We note that we did not enforce M in (1) to be PSD because we follow the one-projection paradigm proposed in (Chechik et al., 2010) that first learns a symmetric matrix M by solving the optimization problem in (1) and then projects the learned matrix M onto the PSD cone.", "startOffset": 109, "endOffset": 131}, {"referenceID": 27, "context": "We emphasize that unlike (Zhang et al., 2013), we did not assume l(\u00b7) to be smooth, making it possible to apply the proposed approach to the hinge loss.", "startOffset": 25, "endOffset": 45}, {"referenceID": 5, "context": "Following one-projection paradigm (Chechik et al., 2010), we project the learned symmetric matrix M onto the PSD cone at the end of the algorithm.", "startOffset": 34, "endOffset": 56}, {"referenceID": 19, "context": "We choose stochastic dual coordinate ascent (SDCA) method for solving the dual problem (5) because it enjoys a linear convergence when the loss function is smooth, and is shown empirically to be significantly faster than the other stochastic optimization methods (Shalev-Shwartz and Zhang, 2012).", "startOffset": 263, "endOffset": 295}, {"referenceID": 19, "context": "We use the combination strategy recommended in (Shalev-Shwartz and Zhang, 2012), denoted by CSDCA, which uses SGD for the first epoch and then applies SDCA for the rest epochs.", "startOffset": 47, "endOffset": 79}, {"referenceID": 27, "context": "2 Main Theoretical Results First, similar to (Zhang et al., 2013), we consider the case when the data matrix X is of low rank.", "startOffset": 45, "endOffset": 65}, {"referenceID": 27, "context": "It is important to note that our analysis, unlike (Zhang et al., 2013), can be applied to non-smooth loss such as the hinge loss.", "startOffset": 50, "endOffset": 70}, {"referenceID": 5, "context": ", 2007) and we use the version pre-processed by (Chechik et al., 2010).", "startOffset": 48, "endOffset": 70}, {"referenceID": 3, "context": "tdt30 is a subset of tdt2 dataset (Cai et al., 2009).", "startOffset": 34, "endOffset": 52}, {"referenceID": 4, "context": "All the other datasets are downloaded from LIBSVM (Chang and Lin, 2011), where rcv30 is a subset of the original dataset consisted of documents from the 30 most popular categories.", "startOffset": 50, "endOffset": 71}, {"referenceID": 5, "context": "Since it is expensive to compute and maintain a matrix of 50, 000 \u00d7 50, 000, for these three datasets, we follow the procedure in (Chechik et al., 2010) that maps all documents to a space of 1, 000 dimension.", "startOffset": 130, "endOffset": 152}, {"referenceID": 5, "context": "First, we follow the evaluation protocol in (Chechik et al., 2010) and evaluate the learned metric by its ranking performance.", "startOffset": 44, "endOffset": 66}, {"referenceID": 5, "context": "\u2022 OASIS (Chechik et al., 2010): A state-of-art online learning algorithm for DML that learns the optimal distance metric directly from the original space without any dimensionality reduction.", "startOffset": 8, "endOffset": 30}, {"referenceID": 23, "context": "\u2022 LMNN (Weinberger and Saul, 2009): A state-of-art batch learning algorithm for DML.", "startOffset": 7, "endOffset": 34}, {"referenceID": 19, "context": ", DuOri, DuRP, SRP, SPCA and OASIS), which yields sufficiently accurate solutions in our experiments and is also consistent with the observation in (Shalev-Shwartz and Zhang, 2012).", "startOffset": 148, "endOffset": 180}, {"referenceID": 19, "context": "The step size of CSDCA is set according to the analysis in (Shalev-Shwartz and Zhang, 2012).", "startOffset": 59, "endOffset": 91}, {"referenceID": 23, "context": ", (Weinberger and Saul, 2009; Xu et al., 2012)) have shown that metric learning do not yield better classification accuracy than the standard classification algorithms (e.", "startOffset": 2, "endOffset": 46}, {"referenceID": 25, "context": ", (Weinberger and Saul, 2009; Xu et al., 2012)) have shown that metric learning do not yield better classification accuracy than the standard classification algorithms (e.", "startOffset": 2, "endOffset": 46}, {"referenceID": 27, "context": "Corollary 3 (Zhang et al., 2013) Let S \u2208 Rr\u00d7m be a standard Gaussian random matrix.", "startOffset": 12, "endOffset": 32}, {"referenceID": 21, "context": "Combining the inequalities in (11) and (12), we have 1 \u03bbN (\u03b1\u0302\u2212\u03b1\u2217)(G\u2212 \u011c)\u03b1\u2217 \u2265 1 \u03bbN (\u03b1\u0302\u2212\u03b1\u2217)\u011c(\u03b1\u0302\u2212\u03b1\u2217) or (\u03b1\u2217 \u2212 \u03b1\u0302\u2217)(\u011c\u2212G)\u03b1\u2217 \u2265 (\u03b1\u0302\u2217 \u2212\u03b1\u2217)G(\u03b1\u0302\u2217 \u2212\u03b1\u2217) + (\u03b1\u0302\u2217 \u2212\u03b1\u2217)(\u011c\u2212G)(\u03b1\u0302\u2217 \u2212\u03b1\u2217) Define p\u2217 = Z\u0303\u03b1\u2217, p\u0302\u2217 = Z\u0303\u03b1\u0302\u2217, we have: (p\u2217 \u2212 p\u0302\u2217)\u0393p\u2217 \u2265 \u2016p\u0302\u2217 \u2212 p\u2217\u20162 + (p\u0302\u2217 \u2212 p\u2217)\u0393(p\u0302\u2217 \u2212 p\u2217) Using the bound given in (8), with a probability 1\u2212 \u03b4, we have \u2016p\u0302\u2217 \u2212 p\u2217\u20162 \u2264 3\u03b5 1\u2212 3\u03b5\u2016p\u2217\u20162 We complete the proof by using the fact \u2016M\u2217 \u2212 M\u0302\u2217\u2016F = 1 \u03bbN \u2016p\u2217 \u2212 p\u0302\u2217\u20162, \u2016M\u2217\u2016F = 1 \u03bbN \u2016p\u2217\u20162 and (Stark et al., 1998) \u2016\u03a0PSD(M\u2217)\u2212\u03a0PSD(M\u0302\u2217)\u2016F \u2264 \u2016M\u2217 \u2212 M\u0302\u2217\u2016F", "startOffset": 444, "endOffset": 464}, {"referenceID": 1, "context": "Theorem 4 (Theorem 2 (Blum, 2005)) Let x \u2208 Rd, and x\u0302 = R\u22a4x/\u221am, where R \u2208 Rd\u00d7m is a random matrix whose entries are chosen independently from N (0, 1).", "startOffset": 21, "endOffset": 33}, {"referenceID": 12, "context": "Theorem 5 (Lemma B-1 (Karoui, 2010)) Suppose M is a real symmetric matrix with non-negative entries, and E is a real symmetric matrix such that maxi,j |Ei,j| \u2264 \u03be.", "startOffset": 21, "endOffset": 35}], "year": 2015, "abstractText": "In this work, we study distance metric learning (DML) for high dimensional data. A typical approach for DML with high dimensional data is to perform the dimensionality reduction first before learning the distance metric. The main shortcoming of this approach is that it may result in a suboptimal solution due to the subspace removed by the dimensionality reduction method. In this work, we present a dual random projection frame for DML with high dimensional data that explicitly addresses the limitation of dimensionality reduction for DML. The key idea is to first project all the data points into a low dimensional space by random projection, and compute the dual variables using the projected vectors. It then reconstructs the distance metric in the original space using the estimated dual variables. The proposed method, on one hand, enjoys the light computation of random projection, and on the other hand, alleviates the limitation of most dimensionality reduction methods. We verify both empirically and theoretically the effectiveness of the proposed algorithm for high dimensional DML.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}