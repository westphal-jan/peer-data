{"id": "1509.04397", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Sep-2015", "title": "Exponential Family Matrix Completion under Structural Constraints", "abstract": "We consider the matrix completion problem of recovering a structured matrix from noisy and partial measurements. Recent works have proposed tractable estimators with strong statistical guarantees for the case where the underlying matrix is low--rank, and the measurements consist of a subset, either of the exact individual entries, or of the entries perturbed by additive Gaussian noise, which is thus implicitly suited for thin--tailed continuous data. Arguably, common applications of matrix completion require estimators for (a) heterogeneous data--types, such as skewed--continuous, count, binary, etc. In the past few years, recent work has demonstrated that this approach can be generalized to discrete and robust models. These are the first in this series of analyses of statistical approaches.", "histories": [["v1", "Tue, 15 Sep 2015 04:49:57 GMT  (1204kb)", "http://arxiv.org/abs/1509.04397v1", "20 pages, 9 figures"]], "COMMENTS": "20 pages, 9 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["suriya gunasekar", "pradeep ravikumar", "joydeep ghosh"], "accepted": true, "id": "1509.04397"}, "pdf": {"name": "1509.04397.pdf", "metadata": {"source": "CRF", "title": "Exponential Family Matrix Completion under Structural Constraints", "authors": ["Suriya Gunasekar", "Pradeep Ravikumar", "Joydeep Ghosh"], "emails": ["SURIYA@UTEXAS.EDU", "PRADEEPR@CS.UTEXAS.EDU", "GHOSH@ECE.UTEXAS.EDU"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 9.\n04 39\n7v 1\n[ st\nat .M\nL ]\n1 5\nSe p\n20 15\nKeywords: Matrix Completion, Exponential Families, High Dimensional Prediction, Low Rank Approximation, Nuclear Norm Minimization"}, {"heading": "1. Introduction", "text": "In the general problem of matrix completion, we seek to recover a structured matrix from noisy and partial measurements. This problem class encompasses a wide range of practically important applications such as recommendation systems, recovering gene\u2013protein interactions, and analyzing document collections in language processing, among others. In recent years, leveraging developments in sparse estimation and compressed sensing, there has been a surge of work on computationally tractable estimators with strong statistical guarantees, specifically for the setting where a subset of entries of a low\u2013rank matrix are observed either deterministically, or perturbed by additive noise that is Gaussian Candes and Plan (2010), or more generally sub\u2013Gaussian Keshavan et al. (2010b); Negahban and Wainwright (2012). While such a Gaussian noise model is amenable to the subtle statistical analyses required for the ill\u2013posed problem of matrix completion, it is not always practically suitable for all data settings encountered in matrix completion problems. For instance, such a Gaussian error model might not be appropriate in recommender systems based on movie ratings that are either binary (likes or dislikes), or range over the integers one through five. The first question we ask in this paper is whether we can generalize the statistical estimators for matrix completion as well as their analyses to general noise models? Note that a noise model captures the uncertainty underlying the matrix measurements, and is thus an important component of the problem specification\ngiven any application; and it is thus vital for broad applicability of the class of matrix completion estimators to extend to general noise models.\nThough this might seem like a narrow technical, although important question, it is related to a broader issue. A Gaussian observation model implicitly assumes the matrix values are continuous\u2013 valued (and that they are thin\u2013tail\u2013distributed). But in modern applications, matrix data span the gamut of heterogeneous data\u2013types, for instance, skewed\u2013continuous, categorical\u2013discrete including binary, count\u2013valued, among others. This, thus gives rise to the second question of whether we can generalize the standard matrix completion estimators and statistical analyses, suited for thin\u2013 tailed continuous data, to more heterogeneous data\u2013types? Note that there has been some recent work for the specific case of binary data by Davenport et al. (2012), but generalizations to other data\u2013types and distributions is largely unexplored.\nThe recent line of work on matrix completion, moreover, enforces the constraint that the underlying matrix be either exactly or approximately low\u2013rank. Aside from the low\u2013rank constraints, further assumptions to eliminate overly \u201cspiky\u201d matrices are required for well\u2013posed recovery under partial measurements Candes and Recht (2009). Early work provided generalization error bounds for various low\u2013rank matrix completion algorithms, including algorithms based on nuclear norm minimization Candes and Recht (2009); Candes and Tao (2010); Candes and Plan (2010); Recht (2011), max\u2013margin matrix factorization Srebro et al. (2004), spectral algorithms Keshavan et al. (2010a,b), and alternating minimization Jain et al. (2013). These work made stringent matrix incoherence assumptions to avoid \u201cspiky\u201d matrices. These assumptions have been made less stringent in more recent results Negahban and Wainwright (2012), which moreover extend the guarantees to approximately low\u2013rank matrices. Such (approximate) low\u2013rank structure is one instance of general structural constraints which are now understood to be necessary for consistent statistical estimation under high\u2013dimensional settings (with very large number of parameters and very few observations). Note that the high\u2013dimensional matrix completion problem is particularly ill\u2013posed, since the measurements are typically both very local (e.g. individual matrix entries), and partial (e.g. covering a decaying fraction of entries of the entire matrix). However, the specific (approximately) low\u2013rank structural constraint imposed in the past work on matrix completion does not capture the rich variety of other qualitatively different structural constraints such as row\u2013sparseness, column\u2013sparseness, or a superposition structure of low\u2013rank plus elementwise sparseness, among others. For instance, in the classical introductory survey on matrix completion Laurent (2009), the authors discuss structural constraints of a contraction matrix, and a Euclidean distance matrix. Thus, the third question we ask in this paper is whether we can generalize the recent line of work on low\u2013rank matrix completion to the more general structurally constrained case.\nIn this paper, we answer all of the three questions above in the affirmative, and provide a vastly unified framework for generalized matrix completion. We address the first two questions by considering a general matrix completion setting wherein observed matrix entries are sampled from any member of a rich family of natural exponential family distributions. Note that this family of distributions encompass a wide variety of popular distributions including Gaussian, Poisson, binomial, negative\u2013binomial, Bernoulli, etc. Moreover, the choice of the exponential family distribution can be made depending on the form of the data. For instance, thin\u2013tailed continuous data is typically modeled using the Gaussian distribution; count\u2013data is modeled through an appropriate distribution over integers (Poisson, binomial, etc.), binary data through Bernoulli, categorical\u2013discrete through multinomial, etc. We address the last question by considering general structural constraints upon the underlying matrix, as captured by a general regularization function R(.). Our general matrix\ncompletion setting thus captures heterogeneous noise\u2013channels, for heterogeneous data\u2013types, and heterogeneous structural constraints.\nIn a key contribution, we propose a simple regularized convex M\u2013estimator for recovering the structurally constrained underlying matrix in this general setting; and moreover provide a unified and novel statistical analysis for our general matrix completion problem. Following a standard approach Negahban (2012), we (a) first showed that the negative log\u2013likelihood of the subset of observed entries satisfies a form of Restricted Strong Convexity (RSC) (Definition 4); and (b) under this RSC condition, our proposed M\u2013estimator satisfies strong statistical guarantees. We note that proving these individual components for our general matrix completion problem under general structural constraints required a fairly delicate and novel analysis, particularly the first component of showing the RSC condition, which we believe would be of independent interest. A key corollary of our general framework is matrix completion under sub\u2013Gaussian samples and low\u2013rank constraints, where we show that our theorem recovers results comparable to the existing literature Candes and Plan (2010); Keshavan et al. (2010b); Negahban and Wainwright (2012). Finally, we corroborate our theoretical findings via simulated experiments."}, {"heading": "1.1 Notations and Preliminaries", "text": "In this subsection we describe the notations and definitions frequently used throughout the paper. Matrices are denoted by capital letters, X, \u0398, M , etc. For a matrix M , Mj and M (i) are the jth column and ith row of M respectively, and Mij denotes the (i, j)th entry of M . The transpose, trace, and rank of a matrix M are denoted by M \u2020, tr(M), and rk(M), respectively. The inner product between two matrices is given by \u3008X,Y \u3009 = tr(X\u2020Y ) =\u2211(i,j)XijYij .\nFor a matrix M \u2208 Rm\u00d7n of rank r, with singular values \u03c31 \u2265 \u03c32 \u2265 . . . \u03c3r, commonly used matrix norms include the nuclear norm \u2016M\u2016\u2217 = \u2211 i \u03c3i, the spectral norm \u2016M\u20162 = \u03c31, the Frobenius\nnorm \u2016M\u2016F = \u221a\u2211 i \u03c3 2 i , and the maximum norm \u2016M\u2016max = max(i,j) Mij .\nGiven any matrix norm \u2016 \u00b7 \u2016, the dual norm, \u2016 \u00b7 \u2016\u2217 is given by \u2016X\u2016\u2217 = sup\u2016Y \u2016\u22641\u3008X,Y \u3009. Definition 1 (Natural Exponential Family). A distribution of a random variable X, in a normed vector space is said to belong to the natural exponential family, if its probability density function, characterized by the parameter \u0398 in the dual vector space, is given by:\nP (X|\u0398) = h(X) exp ( \u3008X,\u0398\u3009 \u2212G(\u0398) ) ,\nwhere G(\u0398) = log \u222b X h(X)e\n\u3008X,\u0398\u3009dX, called the log\u2013partition function, is strictly convex, and analytic.\nDefinition 2 (Bregman Divergence). Let \u03c6 : dom(\u03c6) \u2192 R be a strictly convex function differentiable in the relative interior of dom(\u03c6). The Bregman divergence (associated with \u03c6) between x \u2208 dom(\u03c6) and y \u2208 ri(dom(\u03c6)) is defined as:\nB\u03c6(x, y) = \u03c6(x)\u2212 \u03c6(y)\u2212 \u3008\u2207\u03c6(y), x\u2212 y\u3009.\nDefinition 3 (Subspace compatibility constants). Given a matrix norm R(.), we define the following maximum and minimum subspace compatibility constants of R(.) w.r.t the subspace M:\n\u03a8(M;R) = sup \u0398\u2208M\\{0} R(\u0398) \u2016\u0398\u2016F , \u03a8min(R) = inf \u0398 6={0} R(\u0398) \u2016\u0398\u2016F .\nThus, \u2200\u0398 \u2208 M \u03a8min(R)\u2016\u0398\u2016F \u2264 R(\u0398) \u2264 \u03a8(M,R)\u2016\u0398\u2016F .\nIn the rest of the paper, to avoid notational clutter, the dependence of \u03a8 and \u03a8min on R is suppressed. Thus, \u03a8(M;R) and \u03a8min(R) are denoted as \u03a8(M) and \u03a8min, respectively. Definition 4 (Restricted Strong Convexity). A loss function L is said to satisfy Restricted Strong Convexity with respected to a subspace S, if for some \u00b5L > 0,\nL(\u0398 +\u2206)\u2212 L(\u0398)\u2212 \u3008\u2207L(\u0398),\u2206\u3009 \u2265 \u00b5L\u2016\u2206\u20162F ,\u2200\u2206 \u2208 S.\nDefinition 5 (Sub\u2013Gaussian Distributions). A mean zero random variable X is said to have a sub\u2013 Gaussian distribution with parameter b if \u2200 s > 0, the distribution satisfies E[esX ] \u2264 es2b2/2. Further, if X is sub\u2013Gaussian with parameter b and E[X] = 0, then Var(X) \u2264 b2 (Vershynin (2010))."}, {"heading": "2. Exponential Family Matrix Completion", "text": "Denote the underlying target matrix by \u0398\u2217 \u2208 Rm\u00d7n. We then assume that individual entries \u0398\u2217ij are observed indirectly via a noisy channel: specifically, via a sample drawn from the corresponding member of natural exponential family (see Definition 1):\nP (Xij |\u0398\u2217ij) = h(Xij) exp ( Xij\u0398 \u2217 ij \u2212G(\u0398\u2217ij) ) , (1)\nwhere G : R \u2192 R is a strictly convex, and analytic function called the log\u2013partition function. Consider the random matrix X \u2208 Rm\u00d7n, where each entry Xij is drawn independently from the corresponding distribution in (1); it can be seen that: P (X|\u0398\u2217) = \u220f\nij\n( h(Xij) exp ( Xij\u0398 \u2217 ij \u2212G(\u0398\u2217ij) ))\n= h(X) exp (\u3008X,\u0398\u2217\u3009 \u2212G(\u0398\u2217)) , (2) where we overload the notation to denote G : Rm\u00d7n \u2192 R as G(\u0398) = \u2211ij G(\u0398ij), and the base measure h(X) as h(X) = \u220f ij h(xij).\nUniformly Sampled Observations: In a \u201cfully observed\u201d setting, we would observe all the entries of the observation matrix X \u2208 Rm\u00d7n. However, we consider a partially observed setting, where we observe entries over a subset of indices \u2126 \u2282 [m] \u00d7 [n]. We assume a uniform sampling model, so that \u2200 (i, j) \u2208 \u2126, i \u223c uniform([m]), j \u223c uniform([n]). (3) Note that, under the above described sampling scheme, an index (i, j) can be sampled multiple times, in such cases we include the multiple instances of (i, j) in \u2126 (and not just the unique indices in \u2126). Given \u2126, we define the following matrix P\u2126(X) as\nP\u2126(X) = \u2211\n(i,j)\u2208\u2126 Xijeie\n\u2020 j.\nThe matrix completion task can then be stated as the estimation of \u0398\u2217 from (\u2126,P\u2126(X)), where X \u223c P (X|\u0398\u2217). As noted earlier, this problem is ill\u2013posed in general. However, as we will show, under structural constraints imposed on the parameter matrix \u0398\u2217, we are able to design an M\u2013 estimator with a near optimal deviation from \u0398\u2217."}, {"heading": "2.1 Applications", "text": "Gaussian (fixed \u03c32) is typically used to model continuous data, x \u2208 R, such as measurements with additive errors, affinity datasets. Here, G(\u03b8) = 12\u03c3 2\u03b82.\nBernoulli is a popular distribution of choice to model binary data, x \u2208 {0, 1}, with G(\u03b8) = log (1 + e\u03b8). Some examples of data suitable for Bernoulli model include social networks, gene protein interactions, etc. Binomial (fixed N ) is used to model number of successes in N trials. Here, x \u2208 {0, 1, 2, . . . , N}, and G(\u03b8) = N log (1 + e\u03b8). Some applications include predicting success/failure rate, survey outcomes, etc. Poisson is used to model count data x \u2208 {0, 1, 2, . . .}, such as arrival times, events per unit time, click\u2013throughs among others. Here, G(\u03b8) = e\u03b8. Exponential is often used to model positive valued continuous data x \u2208 R+, specially inter arrival times between events. Here, G(\u03b8) = \u2212 log (\u2212\u03b8)."}, {"heading": "2.2 Log\u2013likelihood", "text": "Denote the gradient map:\ng(\u0398) , \u2207G(\u0398) \u2208 Rm\u00d7n, where g(\u0398)ij = \u2202G(\u0398)\n\u2202\u03b8ij .\nIt can then be verified that the mean and variance of the distribution P (X|\u0398\u2217) are given by E[X] = g(\u0398\u2217), and Var(X) = \u22072G(\u0398\u2217), respectively. The Fenchel conjugate of the log partition function G, is denoted by: F (X) , sup\u0398 \u3008X,\u0398\u3009 \u2212G(\u0398).\nA useful consequence of the exponential family is that the negative log\u2013likelihood is convex in the natural parameters \u0398\u2217, and moreover has a bijection with a large class of Bregman divergences (Definition 2). The following relationship was first noted by Forster and Warmuth (2002), and later established by Banerjee et al. (2005) [Theorem 4]:\n\u2212 logP (X|\u0398) \u221d BF (X, g(\u0398)), \u2200X \u2208 dom(F). (4)"}, {"heading": "2.3 Discussion and directions for future work", "text": "We consider the standard matrix\u2013completion setting where the distribution of the observation matrix X in (2) corresponds to its entries Xij being drawn independently from the other entries. Further, the probability of observing a specific entry Xij , under uniform sampling is independent of the noise channel or the distribution P (Xij |\u0398\u2217ij). However, in some applications, it might be beneficial to have a sampling scheme involving dependencies; for instance, when a user gives a movie a bad rating, we might want to vary the sampling scheme to sample an entirely different region of the matrix. It would be interesting to extend the analysis of this paper to such a dependent sampling setting.\nThe form of the observation random matrix distribution in (2), given the individual observations in (1), can be seen to have connotations of multi\u2013task learning: here recovering each individual matrix entry \u0398\u2217ij together with its corresponding noise model forms a single task, and all these tasks can be performed jointly given the shared structural constraint on \u0398\u2217. It would be interesting to generalize this to form a more general statistical framework for partial multi-task learning.\nWe use the general class of exponential family distributions as the underlying probabilistic model capturing the measurement uncertainties. The richness of the class of exponential family distributions has been used in other settings to provide general statistical frameworks. Kakade et al. (2010) provide a generalization of compressed sensing problem to general exponential family distributions. Note however that results from compressed sensing cannot be immediately extended to matrix completion case, since the sampling operator P\u2126 does not satisfy\nthe typical assumptions (restricted isometry or restricted eigenvalues) made in such settings; see (Candes and Recht, 2009) for additional discussion. There have been extensions of classical probabilistic PCA Tipping and Bishop (1999) from Gaussian noise models to exponential family distributions Collins et al. (2001); Mohamed et al. (2008); Gordon (2002). There have also been recent extensions of probabilistic graphical model classes, beyond Gaussian and Ising models, to multivariate extensions of exponential family distributions (Yang et al., 2012, 2013).\nMore complicated probabilistic models have also been proposed in the context of collaborative filtering Mnih and Salakhutdinov (2007); Salakhutdinov and Mnih (2008), but these typically involve non\u2013convex optimization, and it is difficult to extend the rigorous statistical analyses of the form in this paper (and in the matrix completion literature) to these models."}, {"heading": "3. Main Result and Consequences", "text": "As noted in the introduction, we consider the matrix completion setting with general structural constraints on the underlying target matrix \u0398\u2217. To formalize the notion of such structural constraints, we follow (Negahban, 2012), and assume that \u0398\u2217 satisfies \u0398\u2217 \u2208 M \u2286 M \u2282 Rm\u00d7n, for some subspace M \u2286 M, which contains parameter matrices that are structured similar to the target (the corresponding structural constraints such as low rankness, low rankness+sparsity etc); we also allow the flexibility of working with a superset M of the model subspace that is potentially easier to analyze. Moreover, we use their definition of a decomposable norm regularization function, R(.) : Rm\u00d7n \u2192 R+, which suitably captures these structural constraints: Assumption 1. (Decomposable Norm Regularizer) We assume that R(.) is a matrix norm, and is decomposable over (M,M\u22a5), i.e. if X \u2208 M, Y \u2208 M\u22a5, then,\nR(X + Y ) = R(X) +R(Y ). We provide some examples of such decomposable regularizers and structural constraint subspaces, and refer to (Negahban, 2012) for more examples and discussion. Example 1. Low\u2013rank: This is a common structure assumed in numerous matrix estimation problems, particularly those in collaborative filtering, PCA, spectral clustering, etc. The corresponding structural constraint subspaces (M,M\u22a5) in this case correspond to a linear span of specific one\u2013rank matrices; we discuss these in further detail in Section 3.2, where we derive a corollary of our general theorem to the specific case of recovery guarantees for low\u2013rank constrained matrix completion. The nuclear norm R(\u0398) = \u2016\u0398\u2016\u2217 = \u2211 k \u03c3k, has been shown to be decomposable with respect to these constraint subspaces Fazel et al. (2001). Example 2. Block sparsity: Another important structural constraint for a matrix is block\u2013 sparsity, where each row is either all zeros or mostly non\u2013zero, and the number of non\u2013zero rows is small. The structural constraint subspaces in this case correspond to a linear span of specific Frobenius\u2013norm\u2013one matrices that are non\u2013zero in a small subset of the rows (dependent on \u0398\u2217); it has been shown that \u21131/\u2113q (q > 1) norms Negahban and Wainwright (2008); Obozinski et al. (2011) are decomposable with respect to such structural constraint subspaces. Recalling that \u0398(i) is the ith row of \u0398, the \u21131/\u2113q norm is defined as:\n\u2016\u0398\u20161,q = m\u2211\ni=1\n\u2016\u0398(i)\u2016q = m\u2211\ni=1\n[( n\u2211\nj=1\n|\u0398ij |q )1/q] .\nExample 3. Low\u2013rank plus sparse: This structure is often used to model low\u2013rank matrices which are corrupted by a sparse outlier noise matrix. The structural constraint subspaces corre-\nsponding to these consist of the linear span of weighted sum of specific rank\u2013one matrices and sparse matrices with non\u2013zero components on specified positions; and appropriate regularization function decomposable with respect to such structural constraints is the infimum convolution of the weighted nuclear norm with weighted elementwise \u21131 norm, \u2016M\u20161,1 = \u2211 ij |Mij | Candes et al. (2011); Yang and Ravikumar (2013): R(\u0398) = inf{\u03bb1\u2016S\u20161,1 + \u03bb2\u2016L\u2016\u2217 : \u0398 = S + L}.\nThe second assumption we make is on the curvature of the log\u2013partition function. This is required to establish a form of RSC (Definition 4) for the loss function.\nAssumption 2. The second derivative of the log\u2013partition function G : R \u2192 R has atmost an exponential decay, i.e,\n\u22072G(u) \u2265 e\u2212\u03b7|u|, \u2200 u \u2208 R, for some \u03b7 > 0 It can be verified that commonly used members of natural exponential family obey this assump-\ntion.\nFinally, we make an assumption to avoid \u201cspiky\u201d target matrices. As Candes and Recht (2009) show with numerous examples, low\u2013rank and presumably other such structural constraints as above, by themselves are not sufficient for accurate recovery, in part due to the infeasibility of recovering overly \u201cspiky\u201d matrices with very few large entries. Early work Candes and Plan (2010); Keshavan et al. (2010a,b), assumed stringent matrix incoherence conditions to preclude such matrices, while more recent work Davenport et al. (2012); Negahban and Wainwright (2012), relax these assumptions to restricting the spikiness ratio, defined as follows:\n\u03b1sp(\u0398) = \u221a mn\u2016\u0398\u2016max \u2016\u0398\u2016F . (5)\nAssumption 3. There exists a known \u03b1\u2217 > 0, such that\n\u2016\u0398\u2217\u2016max = \u03b1sp(\u0398 \u2217)\u221a mn \u2016\u0398\u2217\u2016F \u2264 \u03b1\u2217\u221a mn ."}, {"heading": "3.1 M\u2013estimator for Generalized Matrix Completion", "text": "We propose a regularized M\u2013estimate as our candidate parameter matrix \u0398\u0302. The norm regularizer R(.) used is a convex surrogate for the structural constraints, and is assumed to satisfy A 1. For a suitable \u03bb > 0,\n\u0398\u0302 = argmin \u2016\u0398\u2016max\u2264 \u03b1\u2217\u221amn\nmn |\u2126| [\u2211\nij\u2208\u2126 \u2212 log P (Xij |\u0398ij)\n] + \u03bbR(\u0398)\n= argmin \u2016\u0398\u2016max\u2264 \u03b1 \u2217\u221a mn\nmn |\u2126| [\u2211\nij\u2208\u2126 G(\u0398ij)\u2212Xij\u0398ij\n] + \u03bbR(\u0398). (6)\nIn the above estimator, for simplicity we have assumed that the domain of the minimizing function spans all or Rm\u00d7n. In cases where this is violated, additional constraints to restrict \u0398 to the domain could be imposed on the estimator and the results and analysis in the following section still hold. The above optimization problem is a convex program, and can be solved by any off\u2013the shelf convex solvers."}, {"heading": "3.2 Main Results", "text": "Without loss of generality, suppose that m \u2264 n. Let R\u2217(.) = supR(X)\u22641\u3008X, .\u3009 be the dual norm of the regularizer R(.). Further, let \u03a8(M) and \u03a8min be the maximum and minimum subspace compatibility constants of R w.r.t the model subspace M (Definition 3)\u2217. We next define the following quantity:\n\u03baR(n, |\u2126|) := E [\u221amn |\u2126| R \u2217 (\u2211\nij\u2208\u2126 \u01ebijeie\n\u2217 j\n)] ,\nwhere the expectation is over the random sampling index set \u2126, and over the Rademacher sequence {\u01ebij : \u2200(i, j) \u2208 \u2126}; here {ei \u2208 Rm}, {ej \u2208 Rn} are the standard basis. This quantity \u03baR(n, |\u2126|) captures the interaction between the sampling scheme and the structural constraint as captured by the regularizer (specifically its dual R\u2217). Note that it depends only on n (n \u2265 m), and on the size |\u2126| of \u2126.\nTheorem 1. Let \u0398\u0302 be the estimate from (6) with \u03bb2 \u2265 mn|\u2126|R\u2217(P\u2126(X \u2212 g(\u0398\u2217)). Under the Assumptions 1\u20133, if |\u2126| \u2265 c0\u03a82(M)n log n) for large enough c0, then given a constant \u03b2 > 0, \u2203 a constant K\u03b2 > 0 such that, using \u00b5L := e \u2212 2\u03b7\u03b1 \u2217 \u221a mn ( K\u03b2 \u2212 64c0 \u221a |\u2126|\u03ba2R(n,|\u2126|) n logn ) , the following holds with probability > 1\u2212 4e\u2212(1+\u03b2)\u03a84min log2 n:\n\u2016\u0398\u0302\u2212\u0398\u2217\u20162F \u2264 \u03a82(M)max { 3\u03bb2\n2\u00b52 L\n, c20\u03b1 \u22172n logn\n|\u2126|\n} ,\nprovided \u00b5L > 0.\nIn the above theorem, \u03b7 and \u03b1\u2217 \u2265 \u03b1sp(\u0398\u2217)\u2016\u0398\u2217\u2016F are constants from Assumptions 2 and 3, respectively. Our proof uses elements from Negahban (2012), as well as Negahban and Wainwright (2012) where they analyze the case of low\u2013rank structure and additive noise, and establish a form of restricted strong convexity (RSC) for squared loss over subset of matrix entries (closely relates to the special case, when the exponential family distribution assumed in (2) is Gaussian). However, showing such an RSC condition for our general loss function over a subset of structured matrix entries involved some delicate arguments. Further, we provide a much simpler proof that moreover only required a low\u2013spikiness condition rather than a multiplicative spikiness and structural constraint. Moreover, we are able to provide an RSC condition broadly for general structures, and the negative log\u2013likelihood loss associated with general exponential family distribution."}, {"heading": "3.3 Corollary", "text": "An important special case of the problem is when the parameter matrix \u0398\u2217, is assumed to be of a low\u2013rank r \u226a m. The most commonly used convex regularizer to enforce low\u2013rank is the nuclear norm. The intuition behind the low\u2013rank assumption on the parameter matrix is as follows: the parameter \u0398\u2217ij , can be thought of as the true measure of affinity between the entities corresponding to row i and column j, respectively; and the data Xij , is a sample from a noisy channel parametrized by \u0398ij . It is hypothesized that {\u0398\u2217ij}, are obtained from a weighted interaction of a small number of latent variables as, \u0398\u2217ij = \u2211r k=1 \u03c3kuikvjk.\n\u2217We suppress the dependence of \u03a8 and \u03a8min on R in our notation to avoid notational clutter\nLet {uk \u2208 Rm} and {vk \u2208 Rn}, k \u2208 [r] be the left and right singular vectors, respectively of \u0398\u2217. Let the column and row span of \u0398\u2217 be U\u2217 , col(\u0398\u2217) = span{ui} and V \u2217 , row(\u0398\u2217) = span{vj}, respectively. Define:\nM := {\u0398 : row(\u0398) \u2286 V \u2217, col(\u0398) \u2286 U\u2217}, and M\u22a5 := {\u0398 : row(\u0398) \u2286 V \u2217\u22a5, col(\u0398) \u2286 U\u2217\u22a5}.\n(7)\nIt can be verified that, M 6= M, however, M \u2282 M. Corollary 1. Let \u0398\u2217 be a low\u2013rank matrix of rank atmost r \u226a m. Further, let \u2200(i, j), (Xij \u2212 g(\u0398\u2217ij)) are sub\u2013Gaussian (Definition 5) with parameter b, and |\u2126| > c0rn log n for large enough constant c0. Given any \u03b2 > 0, there exists constants c\u03b2 > 0, C\u03b2 > 0 and K\u03b2 > 0, such that using R(.) = \u2016.\u2016\u2217 and \u03bb2 := c\u03b2 \u221a mnb \u221a n logn |\u2126| in (6), w.p. > 1\u2212 4e\u2212(1+\u03b2) log\n2 n \u2212 e\u2212(1+\u03b2) log(n), 1\nmn \u2016\u0398\u0302\u2212\u0398\u2217\u20162F \u2264 C\u03b2 max{b2, \u03b1\u22172/mn} \u00b52L\n( rn log n\n|\u2126|\n) ,\nwhere \u00b5L = K \u2032\u03b2e \u2212 2\u03b7\u03b1\n\u2217 \u221a\nmn > 0.\nRemark 1: Note that the above results hold for the minimizer \u0398\u0302 of the convex program in (6), optimized for any \u03b1\u2217 \u2265 \u03b1sp(\u0398\u2217)\u2016\u0398\u2217\u2016F ; in particular it holds with \u03b1\u2217 = \u03b1sp(\u0398\u2217)\u2016\u0398\u2217\u2016F , where 1 \u2264 \u03b1sp(\u0398\u2217) \u2264 \u221a mn. While in practice \u03b1\u2217 is chosen through cross\u2013validation, the theoretical bound in Corollary 1 can be tightened to the following (if \u2016\u0398\u2016F \u2265 1): \u2016\u0398\u0302 \u2212\u0398\u2217\u20162F\n\u2016\u0398\u2217\u20162F \u2264 C\u03b2\n\u03b12sp(\u0398 \u2217)max {b2, 1}\n\u00b52L\n( rn log n\n|\u2126|\n) . (8)\nSimilar bound can be obtained for Theorem 1. Remark 2: b2 is a measure of noise per entry; \u2200(i, j),Var(Xij \u2212 g(\u0398\u2217ij)) \u2264 b2. Note that as we do not make stronger matrix incoherence assumptions, only an approximate recovery is guaranteed even as b \u2192 0."}, {"heading": "4. Proof", "text": "In this section we provide key steps in the proofs of the main results (Section 3.2-3.3). Proofs of intermediate lemmata are deferred to the appendix."}, {"heading": "4.1 Proof of Theorem 1", "text": "The proof of our main theorem involves two key steps: \u2022 We first show that, under assumptions Assumption 1\u20133, RSC of the form in Definition 4 holds\nfor the loss function in (6) over a large subset of the solution space. \u2022 When the RSC condition holds, the result follows from a few simple calculations; we handle\nthe case where RSC does not hold separately. Let \u2206\u0302 = \u0398\u0302\u2212\u0398\u2217. We state two results of interest.\nLemma 1. We define the following subset: V = {\u0398 \u2208 Rm\u00d7n : R(\u0398M\u22a5) \u2264 3R(\u0398M)}, where recall (M,M\u22a5) from Assumption 1, and \u0398M is the projection of \u0398 onto the subspace M. If \u0398\u0302 is the minimizer of (6), and \u03bb2 \u2265 mn|\u2126|R\u2217(P\u2126(X \u2212 g(\u0398\u2217)), then \u2206\u0302 = \u0398\u0302\u2212\u0398\u2217 \u2208 V . The proof follows from Lemma 1 of Negahban (2012).\nLemma 2. Let \u0398\u0302 be the minimizer of (6). If \u03bb2 \u2265 mn|\u2126|R\u2217(P\u2126(X \u2212 g(\u0398\u2217)), then: mn\n|\u2126| \u2211\n(i,j)\u2208\u2126 BG(\u0398\u0302ij ,\u0398\n\u2217 ij) \u2264 3\u03bb\u03a8(M) 2 \u2016\u0398\u2217 \u2212 \u0398\u0302\u2016F\nThe proof is provided in Appendix A.2.\nRecall the notation \u03b1sp(\u2206) = \u221a mn\u2016\u2206\u2016max \u2016\u2206\u2016F . We now consider two cases, depending on whether\nthe following condition holds for the constant c0 > 0 dictated by Theorem 1:\n\u03b1sp(\u2206\u0302) \u2264 1\nc0\u03a8(M)\n\u221a |\u2126|\nn log n . (9)\nCase 1: Suppose condition in (9) does not hold; so that \u03b1sp(\u2206\u0302) > 1c0\u03a8(M)\n\u221a |\u2126|\nn logn . From the\nconstraints of the optimization problem (6), we have that \u2016\u2206\u0302\u2016max \u2264 \u2016\u0398\u0302\u2016max + \u2016\u0398\u2217\u2016max \u2264 (2\u03b1\u2217/ \u221a mn). Thus,\n\u2016\u2206\u0302\u2016F = \u221a mn\u2016\u2206\u0302\u2016max \u03b1sp(\u2206\u0302) \u2264 2c0\u03b1\u2217 \u221a \u03a82(M)n logn |\u2126| .\n(10)\nCase 2: Suppose condition in (9) does hold. Then, the following theorem shows that an RSC condition of the form in Definition 4 holds.\nTheorem 2 (Restricted Strong Convexity). For c0 given by Theorem 1, let \u03b1sp(\u2206\u0302) \u2264 1 c0\u03a8(M) \u221a |\u2126| n logn . For large enough c0, given any constant \u03b2 > 0, there exists constant K\u03b2 > 0 such that, under the assumptions in Theorem 1, w.p. > 1\u2212 4e\u2212(1+\u03b2)\u03a84min log2 n:\nmn |\u2126| \u2211\nij\u2208\u2126 BG(\u0398\u0302ij ,\u0398\n\u2217 ij) \u2265 \u00b5L\u2016\u2206\u0302\u20162F ,\nwhere \u00b5L = e \u2212 2\u03b7\u03b1\n\u2217 \u221a\nmn ( K\u03b2 \u2212 64c0 \u221a |\u2126|\u03ba2R(n,|\u2126|) n logn ) .\nAs noted earlier, such an RSC result for the special case of squared loss under low\u2013rank constraints was shown in Negahban and Wainwright (2012). We prove this theorem in Section 4.3.\nRemaining steps of the proof of Theorem 1: Thus, if \u03b1sp(\u2206\u0302) \u2264 1c0\u03a8(M) \u221a |\u2126| n logn , and \u00b5L > 0, from Theorem 2 and Lemma 2, w.h.p.:\n\u00b5L\u2016\u2206\u0302\u20162F \u2264 mn |\u2126| \u2211\nij\u2208\u2126\nBG(\u0398\u0302ij ,\u0398 \u2217 ij) \u2264 3\u03bb\u03a8(M) 2 \u2016\u2206\u0302\u2016F (11)\nFrom (10) and (11), under assumptions of Theorem 1, w.p. > 1\u2212 4e\u2212(1+\u03b2)\u03a84min log2 n, we have:\n\u2016\u2206\u0302\u20162F \u2264 \u03a82(M)max { 3\u03bb2\n2\u00b52L , \u03b1\u22172c20n log n |\u2126|\n} ."}, {"heading": "4.2 Proof of Corollary 1", "text": "From the definition of M\u22a5 in (7), we have M = span{uix\u2020, yvj\u2020 : x \u2208 Rn, y \u2208 Rm}. Let PU\u2217 \u2208 Rm\u00d7m and PV \u2217 \u2208 Rn\u00d7n, be the projection matrices onto the column and row spaces (U\u2217, V \u2217) of \u0398\u2217, respectively. We have, \u2200X \u2208 Rm\u00d7n, XM = PU\u2217X + XPV \u2217 \u2212 PU\u2217XPV \u2217 . Also, rk(PU\u2217) = rk(PV \u2217) = rk(\u0398\u2217) = r. Thus, \u2200\u03a6 \u2208 M, rk(\u03a6) \u2264 2r; and hence,\n\u03a8(M) = sup \u03a6\u2208M\\{0} \u2016\u03a6\u2016\u2217 \u2016\u03a6\u2016F \u2264 \u221a 2r. Further, \u03a8min = 1.\nNext, we use the following proposition by Negahban and Wainwright (2012), to bound \u03baR(n, |\u2126|) in Theorem 1.\nLemma 3. If \u2126 \u2282 [m] \u00d7 [n] is sampled using uniform sampling and |\u2126| > n log n, then for a Rademacher sequence {\u01ebij ,\u2200(i, j) \u2208 \u2126},\nE [ 1 |\u2126|\u2016 \u2211\nij\u2208\u2126\n\u221a mn\u01ebijeie \u2217 j\u20162 ] \u2264 10\n\u221a n log n\n|\u2126| .\nThis follows from Lemma 6 of Negahban and Wainwright (2012), using |\u2126| > n log n.\nThus, for large enough c0 > 640, using \u03baR(n, |\u2126|) = 10 \u221a n logn |\u2126| in Theorem 2, for some\nK \u2032\u03b2 > 0 we have:\n\u00b5L = e \u2212 2\u03b7\u03b1\n\u2217 \u221a\nmn ( K\u03b2 \u2212 640\nc0\n) = K \u2032\u03b2e \u2212 2\u03b7\u03b1 \u2217 \u221a mn . (12)\nFinally, to prove the corollary, we derive a bound on \u2016P\u2126(X \u2212 g(\u0398\u2217))\u20162 using the Ahlswede\u2013 Winter Matrix bound (Appendix A.3). Let \u03c6(x) = \u03c82(x) = ex\n2 \u2212 1; and let Y (ij) , \u221amn(Xij \u2212 g(\u0398\u2217ij))eie \u2020 j , such that, \u221a mn |\u2126| \u2016P\u2126(X \u2212 g(\u0398\u2217))\u20162 = \u2016\n1 |\u2126| \u2211 ij\u2208\u2126 Y (ij)\u20162.\nFrom the equivalence of sub-Gaussian definitions in Lemma 5.5 of Vershynin (2010), there exists a constant c1 such that \u2016Xij \u2212 g(\u0398\u2217ij)\u2016\u03c6 \u2264 c1b, \u2200ij. Since, Y (ij) has a single element,\u221a mn(Xij \u2212 g(\u0398\u2217ij)), we have, \u2016Y (ij)\u2016\u03c82 \u2264 c1 \u221a mnb. Further,\nE[Y (ij) T Y (ij)] = E[mn(Xij \u2212 g(\u0398\u2217ij))2eje\u2217j ] (a) = mnE(ij\u2208\u2126)[EX [(Xij \u2212 g(\u0398\u2217ij))2]eje\u2217j ]\n(b) \u2264 mnb2E(ij\u2208\u2126)[eje\u2217j ] (c) = mnb2 1\nn In\u00d7n, (13)\nwhere (a) follows from Fubini\u2019s Theorem, (b) follows as (Xij \u2212 g(\u0398\u2217ij)) is b-sub\u2013Gaussian, and (c) follows from the uniform sampling model. Similarly, E[Y (ij)Y (ij) T ] = mnb2Im\u00d7m. Define \u03c32ij := max{E[Y (ij) T Y (ij)],E[Y (ij)Y (ij) T ]} \u2264 nb2\nIn Lemma 5, using \u03c32 := \u2211\nij\u2208\u2126 \u03c3 2 ij = n|\u2126|b2, M = c1 \u221a mnb \u2264 c1nb, and t = |\u2126|\u03b4, we have:\nP (\u2225\u2225 1 |\u2126| \u2211\nij\u2208\u2126 Y (ij)\n\u2225\u2225 2 \u2265 \u03b4 ) \u2264 n2max { e\u2212 \u03b42|\u2126| 4nb2 , e \u2212 \u03b4|\u2126| 2c1nb } .\nIf |\u2126| > cn log n for large enough c > 0, then for any constant C , using \u03b4 = Cb \u221a\nn logn |\u2126| ,\nP\n(\u221a mn\n|\u2126| \u2016P\u2126(X \u2212 g(\u0398 \u2217))\u20162 \u2265 Cb\n\u221a n log n\n|\u2126|\n) \u2264 n2e\u2212C 2 4 logn. (14)\nRe-parameterizing the constants, we have for \u03b2 > 0, \u2203C\u03b2 > 0 such that w.p. > 1 \u2212 e(1+\u03b2) logn, \u221a mn |\u2126| \u2016P\u2126(X \u2212 g(\u0398\u2217))\u20162 \u2264 C\u03b2b \u221a n logn |\u2126| . Thus, using \u03a8min \u2265 1, \u00b5L = K \u2032\u03b2e \u2212 2\u03b7\u03b1 \u2217 \u221a mn (from (12)), and \u03bb2 := C\u03b2 \u221a mnb \u221a n logn |\u2126| in Theorem 1 leads to the corollary."}, {"heading": "4.3 Proof of Theorem 2", "text": "This proof uses symmetrization arguments and contractions (Ledoux and Talagrand (1991) Ch.4& 6). We observe that, \u2200 (i, j) \u2208 \u2126, \u2203vij \u2208 [0, 1], s.t.\nBG(\u0398\u0302ij ,\u0398 \u2217 ij) = G(\u0398\u0302ij)\u2212G(\u0398\u2217ij)\u2212 g(\u0398\u2217ij)(\u0398\u0302ij \u2212\u0398\u2217ij)\n= \u22072G((1 \u2212 vij)\u0398\u2217ij + vij\u0398\u0302ij)\u2206\u03022ij (a) \u2265 e\u2212 2\u03b7\u03b1 \u2217 \u221a mn \u2206\u03022ij . (15)\nwhere (a) holds as |(1 \u2212 vij)\u0398\u2217ij + vij\u0398\u0302ij| \u2264 \u2016\u0398\u2217\u2016max + \u2016\u0398\u0302\u2016max \u2264 2\u03b1 \u2217\u221a mn , and \u22072G(u) \u2265 e\u2212\u03b7|u| (Assumption 2).\nLemma 4. Under Theorem 2, consider the subset\nE = { \u2206 \u2208 V : \u03b1sp(\u2206) \u2264\n1\nc0\u03a8(M)\n\u221a |\u2126|\nn logn , \u2016\u2206\u2016F = 1\n} .\nGiven any constant \u03b2 > 0, there exists a constant k\u03b2 > 0, such that w.p. > 1\u2212 4e\u2212(1+\u03b2)\u03a8 4 min log2 n, \u2200 \u2206 \u2208 E: \u2223\u2223\u2223mn|\u2126| \u2211\nij\u2208\u2126\n\u22062ij \u2212 1 \u2223\u2223\u2223 \u2264 16R(\u2206)\nc0\u03a8(M)\n\u221a |\u2126|\u03ba2\nR (n, |\u2126|)\nn logn + k\u03b2R(\u2206) c20\u03a8(M) .\nThe proof is provided in Appendix A.1.\nFrom the assumptions in Theorem 2 and Proposition 1, \u2206\u0302\u2016\u2206\u0302\u2016F \u2208 E . Further, as \u2206\u0302 \u2208 V , R(\u2206\u0302) \u2264 R(\u2206\u0302M) + R(\u2206\u0302M\u22a5) \u2264 4R(\u2206\u0302M) \u2264 4\u03a8(M)\u2016\u2206\u0302\u2016F . Using Lemma 4, and (15), and choosing |\u2126| > c0\u03a82(M)n log n, for large enough c0, we have K\u03b2 := 1 \u2212 4k\u03b2c2\n0\n> 0. Finally, using \u00b5L :=\ne \u2212 2\u03b7\u03b1\n\u2217 \u221a\nmn ( K\u03b2 \u2212 64c0 \u221a |\u2126|\u03ba2R(n,|\u2126|) n logn ) ; if \u00b5L > 0, then w.h.p.,\nmn |\u2126| \u2211\nij\u2208\u2126 BG(\u0398\u0302ij ,\u0398\n\u2217 ij) \u2265 \u00b5L\u2016\u2206\u0302\u20162F . (16)"}, {"heading": "5. Experiments", "text": "We provide simulated experiments to corroborate our theoretical guarantees, focusing specifically on Corollary 1, where we consider the special case where the underlying parameter matrix is low\u2013 rank, but the underlying noise model for the matrix elements could be any of the general class of exponential family distributions. We look at three well known members of exponential family suitable for different data\u2013types, namely Gaussian, Bernoulli, and binomial, which are popular choices for modeling continuous valued, binary, and count valued data, respectively."}, {"heading": "5.1 Experimental Setup", "text": "We create low\u2013rank ground truth parameter matrices, \u0398\u2217 \u2208 Rm\u00d7n of sizes n \u2208 {50, 100, 150, 200} (for simplicity we considered square matrices, m = n); we set the rank to r = 2 log n. The observation matrices, X, are then sampled from the different members of exponential family distributions parameterized by \u0398\u2217. For each n, we uniformly sample a subset \u2126 entries of the observation matrix X, and estimate \u0398\u0302 from (6). Evaluation: For each member of the exponential family of distributions considered, we can measure the performance of our M\u2013estimator in parameter space as \u2016\u0398\u0302\u2212\u0398\u2217\u20162F \u2016\u0398\u2217\u20162F , or in observation space using an appropriate error metric err(X\u0302,X), where X\u0302 is the maximum likelihood estimate of the recovered distribution, X\u0302 = argmaxXP (X|\u0398\u0302) (we use RMSE, MAE in our plots). From our corollary, we require |\u2126| = O(rn log n) samples for consistent recovery, so we plot the error metric against the the \u201cnormalized\u201d sample size, |\u2126|rn logn . For reasons of space, we only provide results for the error metric in observations space plotted against the the \u201cnormalized\u201d sample size. The remainder of the results are provided in Appendix B. It can be seen from the plots that the error decays with increasing sample size, corroborating our consistency results; indeed |\u2126| > 1.5rn log n samples suffice for the errors to decay to a very small value. Further, the aligning of the curves (for different n) given the \u201cnormalized\u201d sample size corroborates the convergence rates as well."}, {"heading": "Acknowledgments", "text": "The research was funded by NSF Grants IIS-0713142 and IIS-1017614. Pradeep Ravikumar acknowledges the support of ARO via W911NF-12-1-0390 and NSF via IIS-1149803, IIS-1320894, DMS-1264033."}, {"heading": "Appendix A. Proofs of Lemma", "text": "A.1 Proof of Lemma 4\nRecall that V = {\u2206 : R(\u2206M\u22a5) \u2264 3R(\u2206M)}. To prove Lemma 4, consider the nuclear norm ball SR(t) = {\u2206 : R(\u2206) \u2264 t}.\n1. Show that, P ( sup\n\u2206\u2208E\u2229SR(t)\n\u2223\u2223\u2223mn|\u2126| \u2211 ij\u2208\u2126\u2206 2 ij \u2212 1 \u2223\u2223\u2223 > 8t c0\u03a8(M) \u221a |\u2126|\u03ba2(n,|\u2126|) n logn + k\u03b2t 2c2 0 \u03a8(M)\n) is small;\nwhere \u03ba(n, |\u2126|) is a quantity that depends only on the dimensions n and |\u2126|. This is done by:\n(a) Bounding the expectation, E [\nsup \u2206\u2208E\u2229SR(t)\n\u2223\u2223\u2223mn|\u2126| \u2211 ij\u2208\u2126\u2206 2 ij \u2212 1 \u2223\u2223\u2223 ]\n(b) Showing an exponential decay of the tail.\n2. Then use a peeling argument Raskutti et al. (2010) to derive at the result in Lemma 4.\nA.1.1 BOUNDING EXPECTATION Note that \u2200 \u2206 \u2208 E , E[mn|\u2126| \u2211 ij\u2208\u2126\u2206 2 ij] = \u2016\u2206\u20162F = 1. Thus, by using standard symmetrization argument (Lemma 6.3 of Ledoux and Talagrand (1991), with a Rademacher sequence, {\u01ebij ,\u2200 ij \u2208 \u2126}, we have:\nE [ sup\n\u2206\u2208E\u2229SR(t)\n\u2223\u2223\u2223mn|\u2126| \u2211\nij\u2208\u2126 \u22062ij \u2212 1\n\u2223\u2223\u2223 ] \u2264 2mn|\u2126| E [ sup\n\u2206\u2208E\u2229SR(t)\n\u2223\u2223\u2223 \u2211\nij\u2208\u2126 \u01ebij\u2206\n2 ij \u2223\u2223\u2223 ]\n(17)\nAlso, \u2200\u2206 \u2208 E , \u03c6ij(\u2206) , \u22062ij\n2 sup \u2206\u2208E\n\u2016\u2206\u2016max is a contraction, and \u2200\u2206 \u2208 E , \u2016\u2206\u2016max = \u03b1sp(\u2206)\u221a mn \u2264\n1 c0\u03a8(M) \u221a mn\n\u221a |\u2126|\nn logn . Thus, using Theorem 4.12 of Ledoux and Talagrand (1991) in Equation 17,\nwe have:\nE [ sup\n\u2206\u2208E\u2229SR(t)\n\u2223\u2223\u2223mn|\u2126| \u2211\nij\u2208\u2126 \u22062ij \u2212 1\n\u2223\u2223\u2223 ] \u2264 8\nc0\u03a8(M)\n\u221a |\u2126|\nn log n E\n[ sup\n\u2206\u2208E\u2229SR(t)\n\u2223\u2223\u2223 \u221a mn |\u2126| \u2329\u2211\nij\u2208\u2126 \u01ebijeie\n\u2217 j ,\u2206 \u232a\u2223\u2223\u2223 ]\n(a) \u2264 8t c0\u03a8(M)\n\u221a |\u2126|\nn log n E\n[\u221a mn\n|\u2126| R \u2217 (\u2211\nij\u2208\u2126 \u01ebijeie\n\u2217 j\n)]\n(18)\nwhere (a) follows from Cauchy\u2013Schwartz and as R(\u2206) \u2264 t. Note that R\u2217 (\u2211\nij\u2208\u2126 \u01ebijeie \u2217 j\n) is\nindependent of \u2206 and depends only on n and |\u2126|. Let \u03ba(n, |\u2126|) \u2265 E [ \u221a mn |\u2126| R\u2217 (\u2211 ij\u2208\u2126 \u01ebijeie \u2217 j )]\nbe a suitable upper bound.\nE [ sup\n\u2206\u2208E\u2229SR(t)\n\u2223\u2223\u2223mn|\u2126| \u2211\nij\u2208\u2126 \u22062ij \u2212 1 \u2223\u2223\u2223 ] \u2264 8t c0\u03a8(M)\n\u221a |\u2126|\u03ba2(n, |\u2126|)\nn log n (19)\nA.1.2 TAIL BEHAVIOR\nLet Gt(\u2126) , sup \u2206\u2208E\u2229SR(t)\n\u2223\u2223\u2223mn|\u2126| \u2211 ij\u2208\u2126\u2206 2 ij \u2212 1 \u2223\u2223\u2223. Let \u2126\u2032 \u2282 [m] \u00d7 [n] be another set of indices that\ndiffer from \u2126 in exactly one element. We then have:\nGt(\u2126)\u2212Gt(\u2126\u2032) = sup \u2206\u2208E\u2229SR(t) \u2223\u2223\u2223mn|\u2126| \u2211 ij\u2208\u2126 \u22062ij \u2212 1 \u2223\u2223\u2223\u2212 sup \u2206\u2208E\u2229SR(t) \u2223\u2223\u2223mn|\u2126| \u2211 kl\u2208\u2126\u2032 \u22062kl \u2212 1 \u2223\u2223\u2223\n\u2264 mn|\u2126| sup\u2206\u2208E\u2229SR(t)\n(\u2223\u2223\u2223 \u2211\nij\u2208\u2126 \u22062ij \u2212 1\n\u2223\u2223\u2223\u2212 \u2223\u2223\u2223 \u2211\nkl\u2208\u2126\u2032 \u22062kl \u2212 1\n\u2223\u2223\u2223 )\n\u2264 mn|\u2126| sup\u2206\u2208E\u2229SR(t)\n(\u2223\u2223\u2223 \u2211\nij\u2208\u2126 \u22062ij \u2212\n\u2211\nkl\u2208\u2126\u2032 \u22062kl\n\u2223\u2223\u2223 )\n\u2264 2mn|\u2126| sup\u2206\u2208E\u2229SR(t) \u2016\u2206\u20162max \u2264\n2\nc20\u03a8 2(M)n log n\nBy similar arguments on Gt(\u2126\u2032) \u2212 Gt(\u2126), we conclude that |Gt(\u2126) \u2212 Gt(\u2126\u2032)| \u2264 2c2 0 \u03a82(M)n logn . Therefore, using Mc Diarmid\u2019s inequality, we have P (|Gt(\u2126) \u2212 E[Gt(\u2126)]| > \u03b4) \u2264 2 exp ( \u2212 c 4 0 \u03b42\u03a84(M)n2 log2 n\n2|\u2126|\n) . Fix \u03b4 = 2k1t\nc2 0 \u03a8(M) for appropriate constant k1. Recall that \u03a8min =\ninf X\\{0} R(X) \u2016X\u2016F \u2264 \u03a8(M). Using |\u2126| \u2264 n 2,\nP ( Gt(\u2126) >\n8t\nc0\u03a8(M)\n\u221a |\u2126|\u03ba2(n, |\u2126|)\nn log n +\n2k1t\nc20\u03a8(M)\n) \u2264 2 exp ( \u22122k21t2\u03a82min log2 n )\nA.1.3 PEELING ARGUMENT\nConsider the following sets, S\u2113 = {\u2206 \u2208 E : 2\u2113\u22121\u03a8min \u2264 R(\u2206) \u2264 2\u2113\u03a8min}, for all (integers) \u2113 \u2265 1. Since, \u2200\u2206 \u2208 E , R(\u2206) \u2265 \u03a8min\u2016\u2206\u2016F = \u03a8min, for each \u2206 \u2208 E , \u2206 \u2208 S\u2113 for some \u2113 \u2265 1. Further, if for some \u2206 \u2208 E , \u2223\u2223\u2223mn|\u2126| \u2211 ij\u2208\u2126\u2206 2 ij \u2212 1 \u2223\u2223\u2223 > 16R(\u2206) c0\u03a8(M) \u221a |\u2126|\u03ba2(n,|\u2126|) n logn + 4k1R(\u2206) c2 0 \u03a8(M) , then for some \u2113:\n\u2223\u2223\u2223mn|\u2126| \u2211\nij\u2208\u2126 \u22062ij \u2212 1\n\u2223\u2223\u2223 > 16(2 \u2113\u22121)\u03a8min\nc0\u03a8(M)\n\u221a |\u2126|\u03ba2(n, |\u2126|)\nn log n +\n4k12 \u2113\u22121\u03a8min\nc20\u03a8(M)\n= 8(2\u2113\u03a8min)\nc0\u03a8(M)\n\u221a |\u2126|\u03ba2(n, |\u2126|)\nn log n +\n2k1(2 \u2113\u03a8min)\nc20\u03a8(M)\n(20)\nThus,\nP ( sup \u2206\u2208E \u2223\u2223\u2223\u2223 mn |\u2126| \u2211 ij\u2208\u2126 \u22062ij \u2212 1 \u2223\u2223\u2223\u2223 > 16R(\u2206) c0\u03a8(M) \u221a |\u2126|\u03ba2(n, |\u2126|) n log n + 4k1R(\u2206) c20\u03a8(M) )\n\u2264 \u221e\u2211\n\u2113=1\nP ( G2\u2113(\u2126) > 8(2\u2113\u03a8min)\nc0\u03a8(M)\n\u221a |\u2126|\u03ba2(n, |\u2126|)\nn log n +\n2k1(2 \u2113\u03a8min)\nc20\u03a8(M)\n) \u2264 \u221e\u2211\n\u2113=1\n2 exp (\u22122k2122l\u03a84min log2 n)\n(a) \u2264 \u221e\u2211\n\u2113=1\n2 exp(\u22124 log 2 k21\u2113\u03a84min log2 n) \u2264 2e\u22124k 2 1 \u03a84 min log2 n\n1\u2212 e\u22124k21\u03a84min log2 n \u2264 4e\u22124k21\u03a84min log2 n\n(21)\nwhere (a) follows as x \u2265 log x for x > 1, and the last step holds for n > 1. The lemma follows by re-parametrization of constants in terms of \u03b2.\nA.2 Proof of Lemma 2\nLet \u2206\u0302 = \u0398\u0302\u2212\u0398\u2217.\nR(\u0398\u0302) = R ( \u0398\u2217 + \u2206\u0302M + \u2206\u0302M\u22a5 ) \u2265 R ( \u0398\u2217 + \u2206\u0302M\u22a5 ) \u2212R ( \u2206\u0302M ) = R ( \u0398\u2217 ) +R ( \u2206\u0302M\u22a5 ) \u2212R ( \u2206\u0302M )\n(22)\nThe above inequalities hold due to triangle inequality, and decomposability of R over \u0398\u2217 \u2208 M and \u2206M\u22a5 \u2208 M \u22a5 .\nmn |\u2126| \u2211\n(i,j)\u2208\u2126 BG(\u0398\u0302ij,\u0398\n\u2217 ij) =\nmn |\u2126| [ \u2211\n(i,j)\u2208\u2126 G(\u0398\u0302ij)\u2212Xij\u0398\u0302ij \u2212G(\u0398\u2217ij) +Xij\u0398\u2217ij + \u3008P\u2126(X \u2212 g(\u0398\u2217), \u2206\u0302\u3009\n]\n(a) \u2264 \u03bbR(\u0398\u2217)\u2212 \u03bbR(\u0398\u0302) + mn|\u2126| R \u2217(P\u2126(X \u2212 g(\u0398\u2217)))R(\u2206\u0302)\n(b) \u2264 \u03bbR(\u2206\u0302M)\u2212 \u03bbR(\u2206\u0302M\u22a5) + \u03bb 2 R(\u2206\u0302M + \u2206\u0302M\u22a5) (c) \u2264 3\u03bb 2 R(\u2206\u0302M)\u2212 \u03bb 2 R(\u2206\u0302M\u22a5) \u2264 3\u03bb\u03a8(M) 2 \u2016\u0398\u2217 \u2212 \u0398\u0302M\u2016F \u2264 3\u03bb\u03a8(M) 2 \u2016\u0398\u2217 \u2212 \u0398\u0302\u2016F (23)\nwhere (a) follows as \u0398\u0302 is the minimizer of (6) and using Cauchy Schwartz, (b) follows from 0(22) and using mn|\u2126|R\u2217(P\u2126(X \u2212 g(\u0398\u2217)) \u2264 \u03bb 2 , and (c) follows from triangle inequality.\nA.3 Ahlswede\u2013Winter Matrix Bound (Extension)\nThe Orlicz norm of a random matrix Z \u2208 Rm\u00d7n w.r.t to a convex, differentiable and monotonically increasing function, \u03c6(x) : R+ \u2192 R as follows:\n\u2016Z\u2016\u03c6 ,inf{t \u2265 0 : E [ \u03c6 ( |\u3008Z,Z \u2032\u3009|/t) )] \u2264 1,\n\u2200 Z \u2032 \u2208 Rm\u00d7n, and Z \u2032ij \u2208 [0, 1]}\nLemma 5 (Ahlswede-Winter Matrix Bound). Let Z(1), Z(2), . . . , Z(K) be random matrices of dimensions m \u00d7 n. Let \u2016Z(i)\u2016\u03c6 \u2264 M , \u2200i. Further, \u03c32i = max{\u2016E[Z(i) T Z(i)]\u20162, \u2016E[Z(i)Z(i) T ]\u20162},\nand \u03c32 = \u2211K\ni=1 \u03c3 2 i , then:\nP ( \u2016 K\u2211\ni=1\nZ(i)\u20162 \u2265 t ) \u2264 mnmax { e\u2212 t2 4\u03c32 , e\u2212 t 2M }\nThe above lemma is an extension noted by Vershynin (2009) (Theorem 1 and a later remark) for the matrix bounds resulting from Ahlswede and Winter (2002)."}, {"heading": "Appendix B. Additional Experimental Results", "text": "We provide the additional experimental results where we compare the error of the estimate in the parameter space. We plot the results first against the proportion of the total entries sampled, |\u2126|mn (Figure on left), and then against the \u201cnormalized\u201d sample size, |\u2126|rn logn (Figures on right). We observe trends similar to those observed in Section 5. Again, we find that the curves (for different n) given the \u201cnormalized\u201d sample size, align and converge (left), corroborating the theoretical results. Note that, the curves do not align when plotted against, unnormalized sample size (right). Further, as with errors in observation space, with |\u2126| > 1.5rn log n samples, the errors parameter space also decay to a sufficiently small value."}], "references": [{"title": "Strong converse for identification via quantum channels", "author": ["R. Ahlswede", "A. Winter"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Ahlswede and Winter.,? \\Q2002\\E", "shortCiteRegEx": "Ahlswede and Winter.", "year": 2002}, {"title": "Clustering with bregman divergences", "author": ["A. Banerjee", "S. Merugu", "I.S. Dhillon", "J. Ghosh"], "venue": "JMLR, 6:1705\u20131749,", "citeRegEx": "Banerjee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2005}, {"title": "Matrix completion with noise", "author": ["E.J. Candes", "Y. Plan"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Candes and Plan.,? \\Q2010\\E", "shortCiteRegEx": "Candes and Plan.", "year": 2010}, {"title": "Exact matrix completion via convex optimization", "author": ["E.J. Candes", "B. Recht"], "venue": "Foundations of Computational mathematics,", "citeRegEx": "Candes and Recht.,? \\Q2009\\E", "shortCiteRegEx": "Candes and Recht.", "year": 2009}, {"title": "The power of convex relaxation: near-optimal matrix completion", "author": ["E.J. Candes", "T. Tao"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Candes and Tao.,? \\Q2010\\E", "shortCiteRegEx": "Candes and Tao.", "year": 2010}, {"title": "Robust principal component analysis", "author": ["E.J. Candes", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Candes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2011}, {"title": "A generalization of principal components analysis to the exponential family", "author": ["M. Collins", "S. Dasgupta", "R.E. Schapire"], "venue": "In NIPS,", "citeRegEx": "Collins et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2001}, {"title": "1-bit matrix completion", "author": ["M.A. Davenport", "Y. Plan", "E. Berg", "M. Wootters"], "venue": "arXiv preprint arXiv:1209.3672,", "citeRegEx": "Davenport et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Davenport et al\\.", "year": 2012}, {"title": "A rank minimization heuristic with application to minimum order system approximation", "author": ["M. Fazel", "H Hindi", "S.P. Boyd"], "venue": "In American Control Conference,", "citeRegEx": "Fazel et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Fazel et al\\.", "year": 2001}, {"title": "Relative expected instantaneous loss bounds", "author": ["J. Forster", "M. Warmuth"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Forster and Warmuth.,? \\Q2002\\E", "shortCiteRegEx": "Forster and Warmuth.", "year": 2002}, {"title": "Generalized\u02c6 2 linear\u02c6 2 models", "author": ["G.J. Gordon"], "venue": "In NIPS, pages 577\u2013584,", "citeRegEx": "Gordon.,? \\Q2002\\E", "shortCiteRegEx": "Gordon.", "year": 2002}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["P. Jain", "P. Netrapalli", "S. Sanghavi"], "venue": "In STOC,", "citeRegEx": "Jain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2013}, {"title": "Learning exponential families in highdimensions: Strong convexity and sparsity", "author": ["S.M. Kakade", "O. Shamir", "K. Sridharan", "A. Tewari"], "venue": "In AISTATS, JMLR Workshop and Conference Proceedings,", "citeRegEx": "Kakade et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2010}, {"title": "Matrix completion from a few entries", "author": ["R.H. Keshavan", "A. Montanari", "S. Oh"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Keshavan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Keshavan et al\\.", "year": 2010}, {"title": "Matrix completion from noisy entries", "author": ["R.H. Keshavan", "A. Montanari", "S. Oh"], "venue": null, "citeRegEx": "Keshavan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Keshavan et al\\.", "year": 2010}, {"title": "Matrix completion problems", "author": ["M. Laurent"], "venue": "Encyclopedia of Optimization,", "citeRegEx": "Laurent.,? \\Q2009\\E", "shortCiteRegEx": "Laurent.", "year": 2009}, {"title": "Probability in Banach Spaces: isoperimetry and processes, volume 23", "author": ["M. Ledoux", "M. Talagrand"], "venue": null, "citeRegEx": "Ledoux and Talagrand.,? \\Q1991\\E", "shortCiteRegEx": "Ledoux and Talagrand.", "year": 1991}, {"title": "Probabilistic matrix factorization", "author": ["A. Mnih", "R. Salakhutdinov"], "venue": "In NIPS, pages 1257\u20131264,", "citeRegEx": "Mnih and Salakhutdinov.,? \\Q2007\\E", "shortCiteRegEx": "Mnih and Salakhutdinov.", "year": 2007}, {"title": "Bayesian exponential family pca", "author": ["S. Mohamed", "Z. Ghahramani", "K.A. Heller"], "venue": "In NIPS,", "citeRegEx": "Mohamed et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mohamed et al\\.", "year": 2008}, {"title": "Structured Estimation in High-Dimensions", "author": ["S. Negahban"], "venue": "PhD thesis, EECS Department,", "citeRegEx": "Negahban.,? \\Q2012\\E", "shortCiteRegEx": "Negahban.", "year": 2012}, {"title": "Joint support recovery under high-dimensional scaling: Benefits and perils of l1,-regularization", "author": ["S. Negahban", "M.J. Wainwright"], "venue": null, "citeRegEx": "Negahban and Wainwright.,? \\Q2008\\E", "shortCiteRegEx": "Negahban and Wainwright.", "year": 2008}, {"title": "Restricted strong convexity and weighted matrix completion: Optimal bounds with noise", "author": ["S. Negahban", "M.J. Wainwright"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Negahban and Wainwright.,? \\Q2012\\E", "shortCiteRegEx": "Negahban and Wainwright.", "year": 2012}, {"title": "Support union recovery in high-dimensional multivariate regression", "author": ["G. Obozinski", "M.J. Wainwright", "M.I. Jordan"], "venue": "The Annals of Statistics,", "citeRegEx": "Obozinski et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Obozinski et al\\.", "year": 2011}, {"title": "Restricted eigenvalue properties for correlated gaussian designs", "author": ["G. Raskutti", "M. J Wainwright", "B. Yu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Raskutti et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Raskutti et al\\.", "year": 2010}, {"title": "A simpler approach to matrix completion", "author": ["B. Recht"], "venue": "JMLR, 12:3413\u20133430,", "citeRegEx": "Recht.,? \\Q2011\\E", "shortCiteRegEx": "Recht.", "year": 2011}, {"title": "Bayesian probabilistic matrix factorization using markov chain monte carlo", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "In ICML,", "citeRegEx": "Salakhutdinov and Mnih.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov and Mnih.", "year": 2008}, {"title": "Maximum-margin matrix factorization", "author": ["N. Srebro", "J. Rennie", "T.S. Jaakkola"], "venue": "In NIPS,", "citeRegEx": "Srebro et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2004}, {"title": "Probabilistic principal component analysis", "author": ["M.E. Tipping", "C.M. Bishop"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Tipping and Bishop.,? \\Q1999\\E", "shortCiteRegEx": "Tipping and Bishop.", "year": 1999}, {"title": "A note on sums of independent random matrices after ahlswede-winter", "author": ["R. Vershynin"], "venue": "Lecture Notes,", "citeRegEx": "Vershynin.,? \\Q2009\\E", "shortCiteRegEx": "Vershynin.", "year": 2009}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": "arXiv preprint arXiv:1011.3027,", "citeRegEx": "Vershynin.,? \\Q2010\\E", "shortCiteRegEx": "Vershynin.", "year": 2010}, {"title": "Dirty statistical models", "author": ["E. Yang", "P. Ravikumar"], "venue": "In NIPS,", "citeRegEx": "Yang and Ravikumar.,? \\Q2013\\E", "shortCiteRegEx": "Yang and Ravikumar.", "year": 2013}, {"title": "Graphical models via generalized linear models", "author": ["E. Yang", "G. Allen", "Z. Liu", "P. Ravikumar"], "venue": "In NIPS,", "citeRegEx": "Yang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2012}, {"title": "Conditional random fields via univariate exponential families", "author": ["E. Yang", "P. Ravikumar", "G.I. Allen", "Z. Liu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}, {"title": "Then use a peeling argument Raskutti et al. (2010) to derive at the result in Lemma 4", "author": ["E E"], "venue": "BOUNDING EXPECTATION Note", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "In recent years, leveraging developments in sparse estimation and compressed sensing, there has been a surge of work on computationally tractable estimators with strong statistical guarantees, specifically for the setting where a subset of entries of a low\u2013rank matrix are observed either deterministically, or perturbed by additive noise that is Gaussian Candes and Plan (2010), or more generally sub\u2013Gaussian Keshavan et al.", "startOffset": 356, "endOffset": 379}, {"referenceID": 2, "context": "In recent years, leveraging developments in sparse estimation and compressed sensing, there has been a surge of work on computationally tractable estimators with strong statistical guarantees, specifically for the setting where a subset of entries of a low\u2013rank matrix are observed either deterministically, or perturbed by additive noise that is Gaussian Candes and Plan (2010), or more generally sub\u2013Gaussian Keshavan et al. (2010b); Negahban and Wainwright (2012).", "startOffset": 356, "endOffset": 435}, {"referenceID": 2, "context": "In recent years, leveraging developments in sparse estimation and compressed sensing, there has been a surge of work on computationally tractable estimators with strong statistical guarantees, specifically for the setting where a subset of entries of a low\u2013rank matrix are observed either deterministically, or perturbed by additive noise that is Gaussian Candes and Plan (2010), or more generally sub\u2013Gaussian Keshavan et al. (2010b); Negahban and Wainwright (2012). While such a Gaussian noise model is amenable to the subtle statistical analyses required for the ill\u2013posed problem of matrix completion, it is not always practically suitable for all data settings encountered in matrix completion problems.", "startOffset": 356, "endOffset": 467}, {"referenceID": 4, "context": "This, thus gives rise to the second question of whether we can generalize the standard matrix completion estimators and statistical analyses, suited for thin\u2013 tailed continuous data, to more heterogeneous data\u2013types? Note that there has been some recent work for the specific case of binary data by Davenport et al. (2012), but generalizations to other data\u2013types and distributions is largely unexplored.", "startOffset": 299, "endOffset": 323}, {"referenceID": 2, "context": "Aside from the low\u2013rank constraints, further assumptions to eliminate overly \u201cspiky\u201d matrices are required for well\u2013posed recovery under partial measurements Candes and Recht (2009). Early work provided generalization error bounds for various low\u2013rank matrix completion algorithms, including algorithms based on nuclear norm minimization Candes and Recht (2009); Candes and Tao (2010); Candes and Plan (2010); Recht (2011), max\u2013margin matrix factorization Srebro et al.", "startOffset": 158, "endOffset": 182}, {"referenceID": 2, "context": "Aside from the low\u2013rank constraints, further assumptions to eliminate overly \u201cspiky\u201d matrices are required for well\u2013posed recovery under partial measurements Candes and Recht (2009). Early work provided generalization error bounds for various low\u2013rank matrix completion algorithms, including algorithms based on nuclear norm minimization Candes and Recht (2009); Candes and Tao (2010); Candes and Plan (2010); Recht (2011), max\u2013margin matrix factorization Srebro et al.", "startOffset": 158, "endOffset": 362}, {"referenceID": 2, "context": "Aside from the low\u2013rank constraints, further assumptions to eliminate overly \u201cspiky\u201d matrices are required for well\u2013posed recovery under partial measurements Candes and Recht (2009). Early work provided generalization error bounds for various low\u2013rank matrix completion algorithms, including algorithms based on nuclear norm minimization Candes and Recht (2009); Candes and Tao (2010); Candes and Plan (2010); Recht (2011), max\u2013margin matrix factorization Srebro et al.", "startOffset": 158, "endOffset": 385}, {"referenceID": 2, "context": "Early work provided generalization error bounds for various low\u2013rank matrix completion algorithms, including algorithms based on nuclear norm minimization Candes and Recht (2009); Candes and Tao (2010); Candes and Plan (2010); Recht (2011), max\u2013margin matrix factorization Srebro et al.", "startOffset": 203, "endOffset": 226}, {"referenceID": 2, "context": "Early work provided generalization error bounds for various low\u2013rank matrix completion algorithms, including algorithms based on nuclear norm minimization Candes and Recht (2009); Candes and Tao (2010); Candes and Plan (2010); Recht (2011), max\u2013margin matrix factorization Srebro et al.", "startOffset": 203, "endOffset": 240}, {"referenceID": 2, "context": "Early work provided generalization error bounds for various low\u2013rank matrix completion algorithms, including algorithms based on nuclear norm minimization Candes and Recht (2009); Candes and Tao (2010); Candes and Plan (2010); Recht (2011), max\u2013margin matrix factorization Srebro et al. (2004), spectral algorithms Keshavan et al.", "startOffset": 203, "endOffset": 294}, {"referenceID": 2, "context": "Early work provided generalization error bounds for various low\u2013rank matrix completion algorithms, including algorithms based on nuclear norm minimization Candes and Recht (2009); Candes and Tao (2010); Candes and Plan (2010); Recht (2011), max\u2013margin matrix factorization Srebro et al. (2004), spectral algorithms Keshavan et al. (2010a,b), and alternating minimization Jain et al. (2013). These work made stringent matrix incoherence assumptions to avoid \u201cspiky\u201d matrices.", "startOffset": 203, "endOffset": 390}, {"referenceID": 2, "context": "Early work provided generalization error bounds for various low\u2013rank matrix completion algorithms, including algorithms based on nuclear norm minimization Candes and Recht (2009); Candes and Tao (2010); Candes and Plan (2010); Recht (2011), max\u2013margin matrix factorization Srebro et al. (2004), spectral algorithms Keshavan et al. (2010a,b), and alternating minimization Jain et al. (2013). These work made stringent matrix incoherence assumptions to avoid \u201cspiky\u201d matrices. These assumptions have been made less stringent in more recent results Negahban and Wainwright (2012), which moreover extend the guarantees to approximately low\u2013rank matrices.", "startOffset": 203, "endOffset": 577}, {"referenceID": 2, "context": "Early work provided generalization error bounds for various low\u2013rank matrix completion algorithms, including algorithms based on nuclear norm minimization Candes and Recht (2009); Candes and Tao (2010); Candes and Plan (2010); Recht (2011), max\u2013margin matrix factorization Srebro et al. (2004), spectral algorithms Keshavan et al. (2010a,b), and alternating minimization Jain et al. (2013). These work made stringent matrix incoherence assumptions to avoid \u201cspiky\u201d matrices. These assumptions have been made less stringent in more recent results Negahban and Wainwright (2012), which moreover extend the guarantees to approximately low\u2013rank matrices. Such (approximate) low\u2013rank structure is one instance of general structural constraints which are now understood to be necessary for consistent statistical estimation under high\u2013dimensional settings (with very large number of parameters and very few observations). Note that the high\u2013dimensional matrix completion problem is particularly ill\u2013posed, since the measurements are typically both very local (e.g. individual matrix entries), and partial (e.g. covering a decaying fraction of entries of the entire matrix). However, the specific (approximately) low\u2013rank structural constraint imposed in the past work on matrix completion does not capture the rich variety of other qualitatively different structural constraints such as row\u2013sparseness, column\u2013sparseness, or a superposition structure of low\u2013rank plus elementwise sparseness, among others. For instance, in the classical introductory survey on matrix completion Laurent (2009), the authors discuss structural constraints of a contraction matrix, and a Euclidean distance matrix.", "startOffset": 203, "endOffset": 1587}, {"referenceID": 16, "context": "Following a standard approach Negahban (2012), we (a) first showed that the negative log\u2013likelihood of the subset of observed entries satisfies a form of Restricted Strong Convexity (RSC) (Definition 4); and (b) under this RSC condition, our proposed M\u2013estimator satisfies strong statistical guarantees.", "startOffset": 30, "endOffset": 46}, {"referenceID": 2, "context": "A key corollary of our general framework is matrix completion under sub\u2013Gaussian samples and low\u2013rank constraints, where we show that our theorem recovers results comparable to the existing literature Candes and Plan (2010); Keshavan et al.", "startOffset": 201, "endOffset": 224}, {"referenceID": 2, "context": "A key corollary of our general framework is matrix completion under sub\u2013Gaussian samples and low\u2013rank constraints, where we show that our theorem recovers results comparable to the existing literature Candes and Plan (2010); Keshavan et al. (2010b); Negahban and Wainwright (2012).", "startOffset": 201, "endOffset": 249}, {"referenceID": 2, "context": "A key corollary of our general framework is matrix completion under sub\u2013Gaussian samples and low\u2013rank constraints, where we show that our theorem recovers results comparable to the existing literature Candes and Plan (2010); Keshavan et al. (2010b); Negahban and Wainwright (2012). Finally, we corroborate our theoretical findings via simulated experiments.", "startOffset": 201, "endOffset": 281}, {"referenceID": 28, "context": "Further, if X is sub\u2013Gaussian with parameter b and E[X] = 0, then Var(X) \u2264 b2 (Vershynin (2010)).", "startOffset": 79, "endOffset": 96}, {"referenceID": 8, "context": "The following relationship was first noted by Forster and Warmuth (2002), and later established by Banerjee et al.", "startOffset": 46, "endOffset": 73}, {"referenceID": 1, "context": "The following relationship was first noted by Forster and Warmuth (2002), and later established by Banerjee et al. (2005) [Theorem 4]: \u2212 logP (X|\u0398) \u221d BF (X, g(\u0398)), \u2200X \u2208 dom(F).", "startOffset": 99, "endOffset": 122}, {"referenceID": 12, "context": "Kakade et al. (2010) provide a generalization of compressed sensing problem to general exponential family distributions.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "the typical assumptions (restricted isometry or restricted eigenvalues) made in such settings; see (Candes and Recht, 2009) for additional discussion.", "startOffset": 99, "endOffset": 123}, {"referenceID": 3, "context": "the typical assumptions (restricted isometry or restricted eigenvalues) made in such settings; see (Candes and Recht, 2009) for additional discussion. There have been extensions of classical probabilistic PCA Tipping and Bishop (1999) from Gaussian noise models to exponential family distributions Collins et al.", "startOffset": 100, "endOffset": 235}, {"referenceID": 3, "context": "the typical assumptions (restricted isometry or restricted eigenvalues) made in such settings; see (Candes and Recht, 2009) for additional discussion. There have been extensions of classical probabilistic PCA Tipping and Bishop (1999) from Gaussian noise models to exponential family distributions Collins et al. (2001); Mohamed et al.", "startOffset": 100, "endOffset": 320}, {"referenceID": 3, "context": "the typical assumptions (restricted isometry or restricted eigenvalues) made in such settings; see (Candes and Recht, 2009) for additional discussion. There have been extensions of classical probabilistic PCA Tipping and Bishop (1999) from Gaussian noise models to exponential family distributions Collins et al. (2001); Mohamed et al. (2008); Gordon (2002).", "startOffset": 100, "endOffset": 343}, {"referenceID": 3, "context": "the typical assumptions (restricted isometry or restricted eigenvalues) made in such settings; see (Candes and Recht, 2009) for additional discussion. There have been extensions of classical probabilistic PCA Tipping and Bishop (1999) from Gaussian noise models to exponential family distributions Collins et al. (2001); Mohamed et al. (2008); Gordon (2002). There have also been recent extensions of probabilistic graphical model classes, beyond Gaussian and Ising models, to multivariate extensions of exponential family distributions (Yang et al.", "startOffset": 100, "endOffset": 358}, {"referenceID": 3, "context": "the typical assumptions (restricted isometry or restricted eigenvalues) made in such settings; see (Candes and Recht, 2009) for additional discussion. There have been extensions of classical probabilistic PCA Tipping and Bishop (1999) from Gaussian noise models to exponential family distributions Collins et al. (2001); Mohamed et al. (2008); Gordon (2002). There have also been recent extensions of probabilistic graphical model classes, beyond Gaussian and Ising models, to multivariate extensions of exponential family distributions (Yang et al., 2012, 2013). More complicated probabilistic models have also been proposed in the context of collaborative filtering Mnih and Salakhutdinov (2007); Salakhutdinov and Mnih (2008), but these typically involve non\u2013convex optimization, and it is difficult to extend the rigorous statistical analyses of the form in this paper (and in the matrix completion literature) to these models.", "startOffset": 100, "endOffset": 698}, {"referenceID": 3, "context": "the typical assumptions (restricted isometry or restricted eigenvalues) made in such settings; see (Candes and Recht, 2009) for additional discussion. There have been extensions of classical probabilistic PCA Tipping and Bishop (1999) from Gaussian noise models to exponential family distributions Collins et al. (2001); Mohamed et al. (2008); Gordon (2002). There have also been recent extensions of probabilistic graphical model classes, beyond Gaussian and Ising models, to multivariate extensions of exponential family distributions (Yang et al., 2012, 2013). More complicated probabilistic models have also been proposed in the context of collaborative filtering Mnih and Salakhutdinov (2007); Salakhutdinov and Mnih (2008), but these typically involve non\u2013convex optimization, and it is difficult to extend the rigorous statistical analyses of the form in this paper (and in the matrix completion literature) to these models.", "startOffset": 100, "endOffset": 729}, {"referenceID": 19, "context": "To formalize the notion of such structural constraints, we follow (Negahban, 2012), and assume that \u0398\u2217 satisfies \u0398\u2217 \u2208 M \u2286 M \u2282 Rm\u00d7n, for some subspace M \u2286 M, which contains parameter matrices that are structured similar to the target (the corresponding structural constraints such as low rankness, low rankness+sparsity etc); we also allow the flexibility of working with a superset M of the model subspace that is potentially easier to analyze.", "startOffset": 66, "endOffset": 82}, {"referenceID": 19, "context": "We provide some examples of such decomposable regularizers and structural constraint subspaces, and refer to (Negahban, 2012) for more examples and discussion.", "startOffset": 109, "endOffset": 125}, {"referenceID": 8, "context": "The nuclear norm R(\u0398) = \u2016\u0398\u2016\u2217 = \u2211 k \u03c3k, has been shown to be decomposable with respect to these constraint subspaces Fazel et al. (2001). Example 2.", "startOffset": 116, "endOffset": 136}, {"referenceID": 8, "context": "The nuclear norm R(\u0398) = \u2016\u0398\u2016\u2217 = \u2211 k \u03c3k, has been shown to be decomposable with respect to these constraint subspaces Fazel et al. (2001). Example 2. Block sparsity: Another important structural constraint for a matrix is block\u2013 sparsity, where each row is either all zeros or mostly non\u2013zero, and the number of non\u2013zero rows is small. The structural constraint subspaces in this case correspond to a linear span of specific Frobenius\u2013norm\u2013one matrices that are non\u2013zero in a small subset of the rows (dependent on \u0398\u2217); it has been shown that l1/lq (q > 1) norms Negahban and Wainwright (2008); Obozinski et al.", "startOffset": 116, "endOffset": 592}, {"referenceID": 8, "context": "The nuclear norm R(\u0398) = \u2016\u0398\u2016\u2217 = \u2211 k \u03c3k, has been shown to be decomposable with respect to these constraint subspaces Fazel et al. (2001). Example 2. Block sparsity: Another important structural constraint for a matrix is block\u2013 sparsity, where each row is either all zeros or mostly non\u2013zero, and the number of non\u2013zero rows is small. The structural constraint subspaces in this case correspond to a linear span of specific Frobenius\u2013norm\u2013one matrices that are non\u2013zero in a small subset of the rows (dependent on \u0398\u2217); it has been shown that l1/lq (q > 1) norms Negahban and Wainwright (2008); Obozinski et al. (2011) are decomposable with respect to such structural constraint subspaces.", "startOffset": 116, "endOffset": 617}, {"referenceID": 5, "context": "sponding to these consist of the linear span of weighted sum of specific rank\u2013one matrices and sparse matrices with non\u2013zero components on specified positions; and appropriate regularization function decomposable with respect to such structural constraints is the infimum convolution of the weighted nuclear norm with weighted elementwise l1 norm, \u2016M\u20161,1 = \u2211 ij |Mij | Candes et al. (2011); Yang and Ravikumar (2013): R(\u0398) = inf{\u03bb1\u2016S\u20161,1 + \u03bb2\u2016L\u2016\u2217 : \u0398 = S + L}.", "startOffset": 369, "endOffset": 390}, {"referenceID": 5, "context": "sponding to these consist of the linear span of weighted sum of specific rank\u2013one matrices and sparse matrices with non\u2013zero components on specified positions; and appropriate regularization function decomposable with respect to such structural constraints is the infimum convolution of the weighted nuclear norm with weighted elementwise l1 norm, \u2016M\u20161,1 = \u2211 ij |Mij | Candes et al. (2011); Yang and Ravikumar (2013): R(\u0398) = inf{\u03bb1\u2016S\u20161,1 + \u03bb2\u2016L\u2016\u2217 : \u0398 = S + L}.", "startOffset": 369, "endOffset": 417}, {"referenceID": 2, "context": "As Candes and Recht (2009) show with numerous examples, low\u2013rank and presumably other such structural constraints as above, by themselves are not sufficient for accurate recovery, in part due to the infeasibility of recovering overly \u201cspiky\u201d matrices with very few large entries.", "startOffset": 3, "endOffset": 27}, {"referenceID": 2, "context": "Early work Candes and Plan (2010); Keshavan et al.", "startOffset": 11, "endOffset": 34}, {"referenceID": 2, "context": "Early work Candes and Plan (2010); Keshavan et al. (2010a,b), assumed stringent matrix incoherence conditions to preclude such matrices, while more recent work Davenport et al. (2012); Negahban and Wainwright (2012), relax these assumptions to restricting the spikiness ratio, defined as follows: \u03b1sp(\u0398) = \u221a mn\u2016\u0398\u2016max \u2016\u0398\u2016F .", "startOffset": 11, "endOffset": 184}, {"referenceID": 2, "context": "Early work Candes and Plan (2010); Keshavan et al. (2010a,b), assumed stringent matrix incoherence conditions to preclude such matrices, while more recent work Davenport et al. (2012); Negahban and Wainwright (2012), relax these assumptions to restricting the spikiness ratio, defined as follows: \u03b1sp(\u0398) = \u221a mn\u2016\u0398\u2016max \u2016\u0398\u2016F .", "startOffset": 11, "endOffset": 216}, {"referenceID": 19, "context": "Our proof uses elements from Negahban (2012), as well as Negahban and Wainwright (2012) where they analyze the case of low\u2013rank structure and additive noise, and establish a form of restricted strong convexity (RSC) for squared loss over subset of matrix entries (closely relates to the special case, when the exponential family distribution assumed in (2) is Gaussian).", "startOffset": 29, "endOffset": 45}, {"referenceID": 19, "context": "Our proof uses elements from Negahban (2012), as well as Negahban and Wainwright (2012) where they analyze the case of low\u2013rank structure and additive noise, and establish a form of restricted strong convexity (RSC) for squared loss over subset of matrix entries (closely relates to the special case, when the exponential family distribution assumed in (2) is Gaussian).", "startOffset": 29, "endOffset": 88}, {"referenceID": 19, "context": "The proof follows from Lemma 1 of Negahban (2012).", "startOffset": 34, "endOffset": 50}, {"referenceID": 19, "context": "As noted earlier, such an RSC result for the special case of squared loss under low\u2013rank constraints was shown in Negahban and Wainwright (2012). We prove this theorem in Section 4.", "startOffset": 114, "endOffset": 145}, {"referenceID": 19, "context": "Next, we use the following proposition by Negahban and Wainwright (2012), to bound \u03baR(n, |\u03a9|) in Theorem 1.", "startOffset": 42, "endOffset": 73}, {"referenceID": 19, "context": "This follows from Lemma 6 of Negahban and Wainwright (2012), using |\u03a9| > n log n.", "startOffset": 29, "endOffset": 60}, {"referenceID": 28, "context": "5 of Vershynin (2010), there exists a constant c1 such that \u2016Xij \u2212 g(\u0398ij)\u2016\u03c6 \u2264 c1b, \u2200ij.", "startOffset": 5, "endOffset": 22}, {"referenceID": 16, "context": "3 Proof of Theorem 2 This proof uses symmetrization arguments and contractions (Ledoux and Talagrand (1991) Ch.", "startOffset": 80, "endOffset": 108}], "year": 2015, "abstractText": "We consider the matrix completion problem of recovering a structured matrix from noisy and partial measurements. Recent works have proposed tractable estimators with strong statistical guarantees for the case where the underlying matrix is low\u2013rank, and the measurements consist of a subset, either of the exact individual entries, or of the entries perturbed by additive Gaussian noise, which is thus implicitly suited for thin\u2013tailed continuous data. Arguably, common applications of matrix completion require estimators for (a) heterogeneous data\u2013types, such as skewed\u2013continuous, count, binary, etc., (b) for heterogeneous noise models (beyond Gaussian), which capture varied uncertainty in the measurements, and (c) heterogeneous structural constraints beyond low\u2013rank, such as block\u2013sparsity, or a superposition structure of low\u2013rank plus elementwise sparseness, among others. In this paper, we provide a vastly unified framework for generalized matrix completion by considering a matrix completion setting wherein the matrix entries are sampled from any member of the rich family of exponential family distributions; and impose general structural constraints on the underlying matrix, as captured by a decomposable norm regularizer R(.). We propose a simple convex regularized M\u2013estimator for this generalized framework, and provide a unified and novel statistical analysis for this class of estimators. We finally corroborate our theoretical results on simulated datasets.", "creator": "LaTeX with hyperref package"}}}