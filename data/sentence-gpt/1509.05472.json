{"id": "1509.05472", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Sep-2015", "title": "Learning to Hash for Indexing Big Data - A Survey", "abstract": "The explosive growth in big data has attracted much attention in designing efficient indexing and search methods recently. In many critical applications such as large-scale search and pattern matching, finding the nearest neighbors to a query is a fundamental research problem. However, the straightforward solution using exhaustive comparison is infeasible due to the prohibitive computational complexity and memory requirement. In response, Approximate Nearest Neighbor (ANN) search based on hashing techniques has become popular due to its promising performance in both efficiency and accuracy. Prior randomized hashing methods, e.g., Locality-Sensitive Hashing (LSH), explore data-independent hash functions with random projections or permutations. Although having elegant theoretic guarantees on the search quality in certain metric spaces, performance of randomized hashing has been shown insufficient in many real-world applications. As a remedy, new approaches incorporating data-driven learning methods in development of advanced hash functions have emerged. Such learning to hash methods exploit information such as data distributions or class labels when optimizing the hash codes or functions. Importantly, the learned hash codes are able to preserve the proximity of neighboring data in the original feature spaces in the hash code spaces. The goal of this paper is to provide readers with systematic understanding of insights, pros and cons of the emerging techniques. We provide a comprehensive survey of the learning to hash framework and representative techniques of various types, including unsupervised, semi-supervised, and supervised. In addition, we also summarize recent hashing approaches utilizing the deep learning models. Finally, we discuss the future direction and trends of research in this area.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 17 Sep 2015 23:19:07 GMT  (2730kb)", "http://arxiv.org/abs/1509.05472v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jun wang", "wei liu", "sanjiv kumar", "shih-fu chang"], "accepted": false, "id": "1509.05472"}, "pdf": {"name": "1509.05472.pdf", "metadata": {"source": "CRF", "title": "Learning to Hash for Indexing Big Data - A Survey", "authors": ["Jun Wang"], "emails": ["j.wang@alibaba-inc.com.", "weiliu@us.ibm.com.", "sanjivk@google.com."], "sections": [{"heading": null, "text": "ar X\niv :1\n50 9.\n05 47\n2v 1\n[ cs\n.L G\n] 1\n7 Se\np 20\n15 PROCEEDINGS OF THE IEEE 1\nIndex Terms\u2014Learning to hash, approximate nearest neighbor search, unsupervised learning, semi-supervised learning, supervised learning, deep learning.\nI. Introduction\nThe advent of Internet has resulted in massive information overloading in the recent decades. Nowadays, the World Wide Web has over 366 million accessible websites, containing more than 1 trillion webpages1. For instance, Twitter receives over 100 million tweets per day, and Yahoo! exchanges over 3 billion messages per day. Besides the overwhelming textual data, the photo sharing website Flickr has more than 5 billion images available, where images are still being uploaded at the rate of over 3, 000\nJun Wang is with the Institute of Data Science and Technology at Alibaba Group, Seattle, WA, 98101, USA. Phone: `1 917 945\u20132619, e-mail: j.wang@alibaba-inc.com.\nWei Liu is with IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA. Phone: `1 917 945\u20131274, e-mail: weiliu@us.ibm.com.\nSanjiv Kumar is with Google Research, New York, NY 10011, USA. Phone: `1 212 865\u20132214, e-mail: sanjivk@google.com.\nShih-Fu Chang is with the Departments of Electrical Engineering and Computer Science, Columbia University, New York, NY 10027, USA. Phone: `1 212 854\u20136894, fax: `1 212 932\u20139421, e-mail: sfchang@ee.columbia.edu.\n1 The number webpages is estimated based on the number of indexed links by Google in 2008.\nimages per minute. Another rich media sharing website YouTube receives more than 100 hours of videos uploaded per minute. Due to the dramatic increase in the size of the data, modern information technology infrastructure has to deal with such gigantic databases. In fact, compared to the cost of storage, searching for relevant content in massive databases turns out to be even a more challenging task. In particular, searching for rich media data, such as audio, images, and videos, remains a major challenge since there exist major gaps between available solutions and practical needs in both accuracy and computational costs. Besides the widely used text-based commercial search engines such as Google and Bing, content-based image retrieval (CBIR) has attracted substantial attention in the past decade [1]. Instead of relying on textual keywords based indexing structures, CBIR requires efficiently indexing media content in order to directly respond to visual queries.\nSearching for similar data samples in a given database essentially relates to the fundamental problem of nearest neighbor search [2]. Exhaustively comparing a query point q with each sample in a database X is infeasible because the linear time complexity Op|X |q tends to be expensive in realistic large-scale settings. Besides the scalability issue, most practical large-scale applications also suffer from the curse of dimensionality [3], since data under modern analytics usually contains thousands or even tens of thousands of dimensions, e.g., in documents and images. Therefore, beyond the infeasibility of the computational cost for exhaustive search, the storage constraint originating from loading original data into memory also becomes a critical bottleneck. Note that retrieving a set of Approximate Nearest Neighbors (ANNs) is often sufficient for many practical applications. Hence, a fast and effective indexing method achieving sublinear (op|X |q), logarithmic (Oplog |X |q), or even constant (Op1q) query time is desired for ANN search. Tree-based indexing approaches, such as KD tree [4], ball tree [5], metric tree [6], and vantage point tree [7], have been popular during the past several decades. However, tree-based approaches require significant storage costs (sometimes more than the data itself). In addition, the performance of tree-based indexing methods dramatically degrades when handling high-dimensional data [8]. More recently, product quantization techniques have been proposed to encode high-dimensional data vectors via subspace decomposition for efficient ANN search [9][10].\nUnlike the recursive partitioning used by tree-based indexing methods, hashing methods repeatedly partition the entire dataset and derive a single hash \u2019bit\u20192 from each par-\n2 Depending on the type of the hash function used, each hash may return either an integer or simply a binary bit. In this survey we\ntitioning. In binary partitioning based hashing, input data is mapped to a discrete code space called Hamming space, where each sample is represented by a binary code. Specifically, given N D-dim vectors X P RD\u02c6N , the goal of hashing is to derive suitable K-bit binary codes Y P BK\u02c6N . To\ngenerate Y, K binary hash functions hk : R D \u00de\u00d1 B\n(K\nk\u201c1 are needed. Note that hashing-based ANN search techniques can lead to substantially reduced storage as they usually store only compact binary codes. For instance, 80 million tiny images (32\u02c6 32 pixels, double type) cost around 600G bytes [11], but can be compressed into 64-bit binary codes requiring only 600M bytes! In many cases, hash codes are organized into a hash table for inverse table lookup, as shown in Figure 1. One advantage of hashingbased indexing is that hash table lookup takes only constant query time. In fact, in many cases, another alternative way of finding the nearest neighbors in the code space by explicitly computing Hamming distance with all the database items can be done very efficiently as well. Hashing methods have been intensively studied and widely used in many different fields, including computer graphics, computational geometry, telecommunication, computer vision, etc., for several decades [12]. Among these methods, the randomized scheme of LocalitySensitive Hashing (LSH) is one of the most popular choices [13]. A key ingredient in LSH family of techniques is a hash function that, with high probabilities, returns the same bit for the nearby data points in the original metric space. LSH provides interesting asymptotic theoretical properties leading to performance guarantees. However, LSH based randomized techniques suffer from several crucial drawbacks. First, to achieve desired search precision, LSH often needs to use long hash codes, which reduces the recall. Multiple hash tables are used to alleviate this issue, but it dramatically increases the storage cost as well as the query time. Second, the theoretical guarantees of LSH only apply to certain metrics such as \u2113p (p P p0,2s) and Jaccard [14]. However, returning ANNs in such metric spaces may not lead to good search performance when semantic similarity is represented in a complex way instead of a simple distance or similarity metric. This discrepancy between semantic and metric spaces has been recognized in the computer vision and machine learning communities, namely as semantic gap [15]. To tackle the aforementioned issues, many hashing methods have been proposed recently to leverage machine learning techniques to produce more effective hash codes [16]. The goal of learning to hash is to learn datadependent and task-specific hash functions that yield compact binary codes to achieve good search accuracy [17]. In order to achieve this goal, sophisticated machine learning tools and algorithms have been adapted to the procedure of hash function design, including the boosting algorithm [18], distance metric learning [19], asymmetric binary embedding [20], kernel methods [21][22], compressed sensing [23], maximum margin learning [24], sequential\nprimarily focus on binary hashing techniques as they are used most commonly due to their computational and storage efficiency.\nlearning [25], clustering analysis [26], semi-supervised learning [14], supervised learning [27][22], graph learning [28], and so on. For instance, in the specific application of image search, the similarity (or distance) between image pairs is usually not defined via a simple metric. Ideally, one would like to provide pairs of images that contain \u201csimilar\u201d or \u201cdissimilar\u201d images. From such pairwise labeled information, a good hashing mechanism should be able to generate hash codes which preserve the semantic consistency, i.e., semantically similar images should have similar codes. Both the supervised and semi-supervised learning paradigms have been explored using such pairwise semantic relationships to learn semantically relevant hash functions [11][29][30][31]. In this paper, we will survey important representative hashing approaches and also discuss the future research directions. The remainder of this article is organized as follows. In Section II, we will present necessary background information, prior randomized hashing methods, and the motivations of studying hashing. Section III gives a high-level overview of emerging learning-based hashing methods. In Section IV, we survey several popular methods that fall into the learning to hash framework. In addition, Section V describes the recent development of using neural networks to perform deep learning of hash codes. Section VI discusses advanced hashing techniques and large-scale applications of hashing. Several open issues and future directions are described in Section VII."}, {"heading": "II. Notations and Background", "text": "In this section, we will first present the notations, as summarized in Table I. Then we will briefly introduce the conceptual paradigm of hashing-based ANN search. Finally, we will present some background information on hashing methods, including the introduction of two wellknown randomized hashing techniques."}, {"heading": "A. Notations", "text": "Given a sample point x P RD, one can employ a set of hash functions H \u201c th1, \u00a8 \u00a8 \u00a8 , hKu to compute a K-bit binary code y \u201c ty1, \u00a8 \u00a8 \u00a8 ,yKu for x as\ny \u201c th1pxq, \u00a8 \u00a8 \u00a8 ,h2pxq, \u00a8 \u00a8 \u00a8 ,hKpxqu, (1)\nwhere the kth bit is computed as yk \u201c hkpxq. The hash function performs the mapping as hk : R\nD \u00dd\u00d1 B. Such a binary encoding process can also be viewed as mapping the original data point to a binary valued space, namely Hamming space:\nH : x\u00d1th1pxq, \u00a8 \u00a8 \u00a8 ,hKpxqu. (2)\nGiven a set of hash functions, we can map all the items in the database X \u201c txnu N n\u201c1 P R\nD\u02c6N to the corresponding binary codes as\nY \u201c HpXq \u201c th1pXq, h2pXq, \u00a8 \u00a8 \u00a8 , hKpXqu,\nwhere the hash codes of the data X are Y P BK\u02c6N .\nAfter computing the binary codes, one can perform ANN search in Hamming space with significantly reduced computation. Hamming distance between two binary codes yi and yj is defined as\ndHpyi,yjq \u201c |yi \u00b4yj | \u201c K \u00ff\nk\u201c1\n|hkpxiq\u00b4 hkpxjq|, (3)\nwhere yi \u201c rh1pxiq, \u00a8 \u00a8 \u00a8 ,hkpxiq, \u00a8 \u00a8 \u00a8 ,hKpxiqs and yj \u201c rh1pxjq, \u00a8 \u00a8 \u00a8 ,hkpxjq, \u00a8 \u00a8 \u00a8 ,hKpxjqs. Note that the Hamming distance can be calculated in an efficient way as a bitwise logic operation. Thus, even conducting exhaustive search in the Hamming space can be significantly faster than doing the same in the original space. Furthermore, through designing a certain indexing structure, the ANN search with hashing methods can be even more efficient. Below we describe the pipeline of a typical hashing-based ANN search system."}, {"heading": "B. Pipeline of Hashing-based ANN Search", "text": "There are three basic steps in ANN search using hashing techniques: designing hash functions, generating hash codes and indexing the database items, and online querying using hash codes. These steps are described in detail below.\nB.1 Designing Hash Functions\nThere exist a number of ways of designing hash functions. Randomized hashing approaches often use random projections or permutations. The emerging learning to hash framework exploits the data distribution and often various levels of supervised information to determine optimal parameters of the hash functions. The supervised information includes pointwise labels, pairwise relationships, and ranking orders. Due to their efficiency, the most commonly used hash functions are of the form of a generalized linear projection:\nhkpxq \u201c sgn ` fpwJk x` bkq \u02d8 . (4)\nHere fp\u00a8q is a prespecified function which can be possibly nonlinear. The parameters to be determined are twk, bku K k\u201c1, representing the projection vector wk and the corresponding intercept bk. During the training procedure, the data X, sometimes along with supervised information, is used to estimate these parameters. In addition, different choices of fp\u00a8q yield different properties of the hash functions, leading to a wide range of hashing approaches. For example, LSH keeps fp\u00a8q to be an identity function, while shift-invariant kernel-based hashing and spectral hashing choose fp\u00a8q to be a shifted cosine or sinusoidal function [32][33]. Note that, the hash functions given by (4) generate the\ncodes as hkpxq P t\u00b41,1u. One can easily convert them into binary codes from t0,1u as\nyk \u201c 1\n2 p1` hkpxqq . (5)\nWithout loss of generality, in this survey we will use the term hash codes to refer to either t0,1u or t\u00b41,1u form, which should be clear from the context.\nB.2 Indexing Using Hash Tables\nWith a learned hash function, one can compute the binary codes Y for all the items in a database. For K hash functions, the codes for the entire database cost onlyNK{8 bytes. Assuming the original data to be stored in doubleprecision floating-point format, the original storage costs 8ND bytes. Since the massive datasets are often associated with thousands of dimensions, the computed hash codes significantly reduce the storage cost by hundreds and even thousands of times. In practice, the hash codes of the database are organized as an inverse-lookup, resulting in a hash table or a hash map. For a set of K binary hash functions, one can have at most 2K entries in the hash table. Each entry, called a hash bucket, is indexed by a K-bit hash code. In the hash table, one keeps only those buckets that contains at least one database item. Figure 1 shows an example of using binary hash functions to index the data and construct a hash table. Thus, a hash table can be seen as an inverse-lookup table, which can return all the database items corresponding to a certain code in constant time. This procedure is key to achieving speedup by many hashing based ANN search techniques. Since most of the buckets from 2K possible choices are typically empty, creating an inverse lookup can be a very efficient way of even storing the codes if multiple database items end up with the same codes.\nB.3 Online Querying with Hashing\nDuring the querying procedure, the goal is to find the nearest database items to a given query. The query is first converted into a code using the same hash functions that mapped the database items to codes. One way to find nearest neighbors of the query is by computing the Hamming distance between the query code to all the database codes.\nNote that the Hamming distance can be rapidly computed using logical xor operation between binary codes as\ndHpyi,yjq \u201c yi \u2018yj. (6)\nOn modern computer architectures, this is achieved efficiently by running xor instruction followed by popcount. With the computed Hamming distance between the query and each database item, one can perform exhaustive scan to extract the approximate nearest neighbors of the query. Although this is much faster than the exhaustive search in the original feature space, the time complexity is still linear. An alternative way of searching for the neighbors is by using the inverse-lookup in the hash table and returning the data points within a small Hamming distance r of the query. Specifically, given a query point q, and its corresponding hash code yq \u201cHpqq, all the database points y\u0303 whose hash codes fall within the Hamming ball of radius r centered at yq, i.,e. dHpy\u0303,Hpqqq \u010f r. As shown in Figure 2, for a K-bit binary code, a total of \u0159r\nl\u201c0\n`\nK l\n\u02d8\npossible codes will be within Hamming radius of r. Thus one needs to search OpKrq buckets in the hash table. The union of all the items falling into the corresponding hash buckets is returned as the search result. The inverse-lookup in a hash table has constant time complexity independent of the database size N . In practice, a small value of r (r \u201c 1,2 is commonly used) is used to avoid the exponential growth in the possible code combinations that need to be searched."}, {"heading": "C. Randomized Hashing Methods", "text": "Randomized hashing, e.g. locality sensitive hash family, has been a popular choice due to its simplicity. In addition, it has interesting proximity preserving properties. A binary hash function hp\u00a8q from LSH family is chosen such that the probability of two points having the same bit is proportional to their (normalized) similarity, i.e.,\nP thpxiq \u201c hpxjqu \u201c simpxi,xjq. (7)\nHere simp\u00a8, \u00a8q represents similarity between a pair of points in the input space, e.g., cosine similarity or Jaccard similarity [34]. In this section, we briefly review two categories of randomized hashing methods, i.e. random projection based and random permutation based approaches.\nC.1 Random Projection Based Hashing\nAs a representative member of the LSH family, random projection based hash (RPH) functions have been widely used in different applications. The key ingredient of RPH is to map nearby points in the original space to the same hash bucket with a high probability. This equivalently preserves the locality in the original space in the Hamming space. Typical examples of RPH functions consist of a random projection w and a random shift b as\nhkpxq \u201c sgnpw J k x` bkq, (8)\nThe random vectorw is constructed by sampling each component of w randomly from a standard Gaussian distribution for cosine distance [34]. It is easy to show that the collision probability of two samples xi,xj falling into the same hash bucket is determined by the angle \u03b8ij between these two sample vectors, as shown in Figure 3. One can show that\nPr rhkpxiq \u201c hkpxjqs \u201c 1\u00b4 \u03b8ij\n\u03c0 \u201c 1\u00b4\n1 \u03c0 cos\u00b41\nxJi xj\n}xi}}xj} (9)\nThe above collision probability gives the asymptotic theoretical guarantees for approximating the cosine similarity defined in the original space. However, long hash codes are required to achieve sufficient discrimination for high precision. This significantly reduces the recall if hash table based inverse lookup is used for search. In order to balance the tradeoff of precision and recall, one has to construct multiple hash tables with long hash codes, which increases both storage and computation costs. In particular, with hash codes of length K, it is required to construct a sufficient number of hash tables to ensure the desired performance bound [35]. Given l K-bit tables, the collision probability is given as:\nP tHpxiq \u201cHpxjqu9 l \u00a8\n\u201e\n1\u00b4 1\n\u03c0 cos\u00b41\nxJi xj\n}xi}}xj}\nK\n(10)\nTo balance the search precision and recall, the length of hash codes should be long enough to reduce false collisions (i.e., non-neighbor samples falling into the same bucket).\nMeanwhile, the number of hash tables l should be sufficiently large to increase the recall. However, this is inefficient due to extra storage cost and longer query time.\nTo overcome these drawbacks, many practical systems adapt various strategies to reduce the storage overload and to improve the efficiency. For instance, a self-tuning indexing technique, called LSH forest was proposed in [36], which aims at improving the performance without additional storage and query overhead. In [37][38], a technique called MultiProbe LSH was developed to reduce the number of required hash tables through intelligently probing multiple buckets in each hash table. In [39], nonlinear randomized Hadamard transforms were explored to speed up the LSH based ANN search for Euclidean distance. In [40], BayesLSH was proposed to combine Bayesian inference with LSH in a principled manner, which has probabilistic guarantees on the quality of the search results in terms of accuracy and recall. However, the random projections based hash functions ignore the specific properties of a given data set and thus the generated binary codes are data-independent, which leads to less effective performance compared to the learning based methods to be discussed later.\nIn machine learning and data mining community, recent methods tend to leverage data-dependent and taskspecific information to improve the efficiency of random projection based hash functions [16]. For example, incorporating kernel learning with LSH can help generalize ANN search from a standard metric space to a wide range of similarity functions [41][42]. Furthermore, metric learning has been combined with randomized LSH functions to explore a set of pairwise similarity and dissimilarity constraints [19]. Other variants of locality sensitive hashing techniques include super-bit LSH [43], boosted LSH [18], as well as non-metric LSH [44]\nC.2 Random Permutation based Hashing\nAnother well-known paradigm from the LSH family is min-wise independent permutation hashing (MinHash), which has been widely used for approximating Jaccard similarity between sets or vectors. Jaccard is a popular choice for measuring similarity between documents or images. A typical application is to index documents and then identify near-duplicate samples from a corpus of documents [45][46]. The Jaccard similarity between two sets Si and Sj is defined as JpSi,Sjq \u201c SiXSj SiYSj . Since a collection of sets tSiu N i\u201c1 can be represented as a characteristic matrix C P BM\u02c6N , where M is the cardinality of the universal set S1 Y \u00a8\u00a8 \u00a8 YSN . Here the rows of C represents the elements of the universal set and the columns correspond to the sets. The element cdi \u201c 1 indicates the d-th element is a member of the i-th set, cdi \u201c 0 otherwise. Assume a random permutation \u03c0kp\u00a8q that assigns the index of the d-th element as \u03c0kpdq P t1, \u00a8 \u00a8 \u00a8 ,Du. It is easy to see that the random permutation satisfies two properties: \u03c0kpdq \u2030 \u03c0kplq and Prr\u03c0kpdq \u0105 \u03c0kplqs \u201c 0.5. A random permutation based min-hash signature of a set Si is defined\nas the minimum index of the non-zero element after performing permutation using \u03c0k\nhkpSiq \u201c min dPt1,\u00a8\u00a8\u00a8 ,Du,c\u03c0kpdqi\u201c1 \u03c0kpdq. (11)\nNote that such a hash function holds a property that the chance of two sets having the same MinHash values is equal to the Jaccard similarity between them [47]\nPrrhkpSiq \u201c hkpSjqs \u201c JpSi,Sjq. (12)\nThe definition of the Jaccard similarity can be extended to two vectors xi \u201c txi1, \u00a8 \u00a8 \u00a8 , xid, \u00a8 \u00a8 \u00a8 , xiDu and xj \u201c txj1, \u00a8 \u00a8 \u00a8 ,xjd, \u00a8 \u00a8 \u00a8 ,xjDu as\nJpxi,xjq \u201c\n\u0159D\nd\u201c1 minpxid, xjdq \u0159D\nd\u201c1 maxpxid, xjdq .\nSimilar min-hash functions can be defined for the above vectors and the property of the collision probability shown in Eq. 12 still holds [48]. Compared to the random projection based LSH family, the min-hash functions generate non-binary hash values that can be potentially extended to continuous cases. In practice, the min-hash scheme has shown powerful performance for high-dimensional and sparse vectors like the bag-of-word representation of documents or feature histograms of images. In a large scale evaluation conducted by Google Inc., the min-hash approach outperforms other competing methods for the application of webpage duplicate detection [49]. In addition, the minhash scheme is also applied for Google news personalization [50] and near duplicate image detection [51] [52]. Some recent efforts have been made to further improve the min-hash technique, including b-bit minwise hashing [53] [54], one permutation approach [55], geometric min-Hashing [56], and a fast computing technique for image data [57]."}, {"heading": "III. Categories of Learning Based Hashing Methods", "text": "Among the three key steps in hashing-based ANN search, design of improved data-dependent hash functions has been the focus in learning to hash paradigm. Since the proposal of LSH in [58], many new hashing techniques have been developed. Note that most of the emerging hashing methods are focused on improving the search performance using a single hash table. The reason is that these techniques expect to learn compact discriminative codes such that searching within a small Hamming ball of the query or even exhaustive scan in Hamming space is both fast and accurate. Hence, in the following, we primarily focus on various techniques and algorithms for designing a single hash table. In particular, we provide different perspectives such as the learning paradigm and hash function characteristics to categorize the hashing approaches developed recently. It is worth mentioning that a few recent studies have shown that exploring the power of multiple hash tables can sometimes generate superior performance. In\norder to improve precision as well as recall, Xu et al., developed multiple complementary hash tables that are sequentially learned using a boosting-style algorithm [31]. Also, in cases when the code length is not very large and the number of database points is large, exhaustive scan in Hamming space can be done much faster by using multitable indexing as shown by Norouzi et al. [59]."}, {"heading": "A. Data-Dependent vs. Data-Independent", "text": "Based on whether design of hash functions requires analysis of a given dataset, there are two high-level categories of hashing techniques: data-independent and datadependent. As one of the most popular data-independent approaches, random projection has been used widely for designing data-independent hashing techniques such as LSH and SIKH mentioned earlier. LSH is arguably the most popular hashing method and has been applied to a variety of problem domains, including information retrieval and computer vision. In both LSH and SIKH, the projection vector w and intersect b, as defined in Eq. 4, are randomly sampled from certain distributions. Although these methods have strict performance guarantees, they are less efficient since the hash functions are not specifically designed for a certain dataset or search task. Based on the random projection scheme, there have been several efforts to improve the performance of the LSH method [37] [39] [41]. Realizing the limitation of data-independent hashing approaches, many recent methods use data and possibly some form of supervision to design more efficient hash functions. Based on the level of supervision, the datadependent methods can be further categorized into three subgroups as described below."}, {"heading": "B. Unsupervised, Supervised, and Semi-Supervised", "text": "Many emerging hashing techniques are designed by exploiting various machine learning paradigms, ranging from unsupervised and supervised to semi-supervised settings. For instance, unsupervised hashing methods attempt to integrate the data properties, such as distributions and manifold structures to design compact hash codes with improved accuracy. Representative unsupervised methods include spectral hashing [32], graph hashing [28], manifold hashing [60], iterative quantization hashing [61], kernalized locality sensitive hashing [41][21], isotropic hashing [62], angular quantization hashing [63], and spherical hashing [64]. Among these approaches, spectral hashing explores the data distribution and graph hashing utilizes the underlying manifold structure of data captured by a graph representation. In addition, supervised learning paradigms ranging from kernel learning to metric learning to deep learning have been exploited to learn binary codes, and many supervised hashing methods have been proposed recently [19][22][65][66][67][68]. Finally, semi-supervised learning paradigm was employed to design hash functions by using both labeled and unlabeled data. For instance, Wang et. al proposed a regularized objective to achieve accurate yet balanced hash codes to avoid overfitting [14].\nIn [69][70], authors proposed to exploit the metric learning and locality sensitive hashing to achieve fast similarity based search. Since the labeled data is used for deriving optimal metric distance while the hash function design uses no supervision, the proposed hashing technique can be regarded as a semi-supervised approach."}, {"heading": "C. Pointwise, Pairwise, Triplet-wise and Listwise", "text": "Based on the level of supervision, the supervised or semisupervised hashing methods can be further grouped into several subcategories, including pointwise, pairwise, tripletwise, and listwise approaches. For example, a few existing approaches utilize the instance level semantic attributes or labels to design the hash functions [71][18][72]. Additionally, learning methods based on pairwise supervision have been extensively studied, and many hashing techniques have been proposed [19][22][65][66][14][69][70][73]. As demonstrated in Figure 4(a), the pair px2,x3q contains similar points and the other two pairs px1,x2q and px1,x3q contain dissimilar points. Such relations are considered in the learning procedure to preserve the pairwise label information in the learned Hamming space. Since the ranking information is not fully utilized, the performance of pairwise supervision based methods could be sub-optimal for nearest neighbor search. More recently, a triplet ranking that encodes the pairwise proximity comparison among three data points is exploited to design hash codes [74][65][75]. As shown in Figure 4(b), the point x` is more similar to the query point q than the point x\u00b4. Such a triplet ranking information, i.e., simpq,x`q \u0105 simpq,x\u00b4q is expected to be encoded in the learned binary hash codes. Finally, the listwise information indicates the rank order of a set of points with respect to the query point. In Figure 4(c), for the query point q, the rank list px4,x1,x2,x3q shows the ranking order of their similarities to the query point q, where x4 is the nearest point and x3 is the farthest one. By converting rank lists to a triplet tensor matrix, listwise hashing is designed to preserve the ranking in the Hamming space [76]."}, {"heading": "D. Linear vs. Nonlinear", "text": "Based on the form of function fp\u00a8q in Eq. 4, hash functions can also be categorized in two groups: linear and nonlinear. Due to their computational efficiency, linear functions tend to be more popular, which include random projection based LSH methods. The learning based methods derive optimal projections by optimizing different types of objectives. For instance, PCA hashing performs principal component analysis on the data to derive large variance projections [63][77][78], as shown in Figure 5(a). In the same league, supervised methods have used Linear Discriminant Analysis to design more discriminative hash codes [79][80]. Semi-supervised hashing methods estimate the projections that have minimum empirical loss on pair-wise labels while partitioning the unlabeled data in a balanced way [14]. Techniques that use variance of the projections as the underlying objective, also tend to use orthogonality constraints for computational ease. However, these constraints lead to a significant drawback since the variance for most real-world data decays rapidly with most of the variance contained only in top few directions. Thus, in order to generate more bits in the code, one is forced to use progressively low-variance directions due to orthogonality constraints. The binary codes derived from these low-variance projections tend to have significantly lower performance. Two types of solutions based on relaxation of the orthogonality constraints or random/learned rotation of the data have been proposed in the literature to address these issues [14][61]. Isotropic hashing is proposed to derive projections with equal variances and is shown to be superior to anisotropic variances based projections [62]. Instead of performing one-shot learning, sequential projection learning derives correlated projections with the goal of correcting errors from previous hash bits [25]. Finally, to reduce the computational complexity of full projection, circulant binary embedding was recently proposed to significantly speed up the encoding process using the circulant convolution [81].\nDespite its simplicity, linear hashing often suffers from insufficient discriminative power. Thus, nonlinear methods have been developed to override such limitations. For instance, spectral hashing first extracts the principal projections of the data, and then partitions the projected data by a sinusoidal function (nonlinear) with a specific angular frequency. Essentially, it prefers to partition projec-\ntions with large spread and small spatial frequency such that the large variance projections can be reused. As illustrated in Figure 5(b), the fist principal component can be reused in spectral hashing to divide the data into four parts while being encoded with only one bit. In addition, shift-invariant kernel-based hashing chooses fp\u00a8q to be a shifted cosine function and samples the projection vector in the same way as standard LSH does [33]. Another category of nonlinear hashing techniques employs kernel functions [21][22][28][82]. Anchor graph hashing proposed by Liu et al. [28] uses a kernel function to measure similarity of each points with a set of anchors resulting in nonlinear hashing. Kernerlized LSH uses a sparse set of datapoints to compute a kernel matrix and preform random projection in the kernel space to compute binary codes [21]. Based on similar representation of kernel metric, Kulis and Darrell propose learning of hash functions by explicitly minimizing the reconstruction error in the kernel space and Hamming space [27]. Liu et al. applies kernel representation but optimizes the hash functions by exploring the equivalence between optimizing the code inner products and the Hamming distances to achieve scale invariance [22]."}, {"heading": "E. Single-Shot Learning vs. Multiple-Shot Learning", "text": "For learning based hashing methods, one first formulates an objective function reflecting desired characteristics of the hash codes. In a single-shot learning paradigm, the optimal solution is derived by optimizing the objective function in a single-shot. In such a learning to hash framework, the K hash functions are learned simultaneously. In contrast, the multiple-shot learning procedure considers a global objective, but optimizes a hash function considering the bias generated by the previous hash functions. Such a procedure sequentially trains hash functions one bit at a time [83][25][84]. The multiple-shot hash function learning is often used in supervised or semi-supervised settings since the given label information can be used to assess the quality of the hash functions learned in previous steps. For instance, the sequential projection based hashing aims to incorporate the bit correlations by iteratively updating the pairwise label matrix, where higher weights are imposed on point pairs violated by the previous hash functions [25]. In the complementary projection learning approach [84], the authors present a sequential learning procedure to obtain a series of hash functions that cross the sparse data region, as well as generate balanced hash buckets. Column generation hashing learns the best hash function during each iteration and updates the weights of hash functions accordingly. Other interesting learning ideas include twostep learning methods which treat hash bit learning and hash function learning separately [85][86]."}, {"heading": "F. Non-Weighted vs. Weighted Hashing", "text": "Given the Hamming embedding defined in Eq. 2, traditional hashing based indexing schemes map the original data into a non-weighted Hamming space, where each bit contributes equally. Given such a mapping, the Hamming distance is calculated by counting the number of different\nbits. However, it is easy to observe that different bits often behave differently [14][32]. In general, for linear projection based hashing methods, the binary code generated from large variance projection tends to perform better due to its superior discriminative power. Hence, to improve discrimination among hash codes, techniques were designed to learn a weighted hamming embedding as\nH : X \u00d1 t\u03b11h1pxq, \u00a8 \u00a8 \u00a8 ,\u03b1KhKpxqu. (13)\nHence the conventional hamming distance is replaced by a weighted version as\ndWH \u201c K \u00ff\nk\u201c1\n\u03b1k|hkpxiq\u00b4 hkpxjq|. (14)\nOne of the representative approaches is Boosted Similarity Sensitive Coding (BSSC) [18]. By learning the hash functions and the corresponding weights t\u03b11, \u00a8 \u00a8 \u00a8 , \u03b1ku jointly, the objective is to lower the collision probability of nonneighbor pair pxi,xjq P C while improving the collision probability of neighboring pair pxi,xjq PM. If one treats each hash function as a decision stump, the straightforward way of learning the weights is to directly apply adaptive boosting algorithm [87] as described in [18]. In [88], a boosting-style method called BoostMAP is proposed to map data points to weighted binary vectors that can leverage both metric and semantic similarity measures. Other weighted hashing methods include designing specific bit-level weighting schemes to improve the search accuracy [74][89][90][91][92]. In addition, a recent work about designing a unified bit selection framework can be regarded as a special case of weighted hashing approach, where the weights of hash bits are binary [93]. Another effective hash code ranking method is the query-sensitive hashing, which explores the raw feature of the query sample and learns query-specific weights of hash bits to achieve accurate \u01ebnearest neighbor search [94]."}, {"heading": "IV. Methodology Review and Analysis", "text": "In this section, we will focus on review of several representative hashing methods that explore various machine learning techniques to design data-specific indexing schemes. The techniques consist of unsupervised, semisupervised, as well as supervised approaches, including spectral hashing, anchor graph hashing, angular quantization, binary reconstructive embedding based hashing, metric learning based hashing, semi-supervised hashing, column generation hashing, and ranking supervised hashing. Table II summarizes the surveyed hashing techniques, as well as their technical merits. Note that this section mainly focuses on describing the intuition and formulation of each method, as well as discussing their pros and cons. The performance of each individual method highly depends on practical settings, including learning parameters and dataset itself. In general, the nonlinear and supervised techniques tend to generate better performance than linear and unsupervised methods, while being more computationally costly [14][19][21][22][27]."}, {"heading": "A. Spectral Hashing", "text": "In the formulation of spectral hashing, the desired properties include keeping neighbors in input space as neighbors in the hamming space and requiring the codes to be balanced and uncorrelated [32]. Hence, the objective of spectral hashing is formulated as:\nmin \u00ff\nij\n1 2 Aij}yi\u00b4yj} 2 \u201c 1 2 trpYJLYq (15)\nsubject to: Y P t\u00b41,1uN\u02c6K\n1Jyk\u00a8 \u201c 0, k \u201c 1, \u00a8 \u00a8 \u00a8 ,K\nYJY \u201c nIK\u02c6K ,\nwhere A \u201c tAiju N i,j\u201c1 is a pairwise similarity matrix and the Laplacian matrix is calculated as L \u201c diagpA1q \u00b4A. The constraint 1Jyk \u201c 0 ensures that the hash bit yk reaches a balanced partitioning of the data and the constraint YJY \u201c nIK\u02c6K imposes orthogonality between hash bits to minimize the redundancy. The direct solution for the above optimization is nontrivial for even a single bit since it is essentially a balanced graph partition problem, which is NP hard. The orthogonality constraints for K-bit balanced partitioning make the above problem even harder. Motivated by the wellknown spectral graph analysis [95], the authors suggest to minimize the cost function with relaxed constraints. In particular, with the assumption of uniform data distribution, the spectral solution can be efficiently computed using 1D-Laplacian eigenfunctions [32]. The final solution for spectral hashing equals to apply a sinusoidal function with pre-computed angular frequency to partition data along PCA directions. Note that the projections are computed using data but learned in an unsupervised manner. As most of the orthogonal projection based hashing methods, spectral hashing suffers from the low-quality binary coding using low-variance projections. Hence, a \u201ckernel trick\u201d is used to alleviate the degraded performance when using long hash bits [96]. Moreover, the assumption of uniform data distribution usually hardly hold for real-world data."}, {"heading": "B. Anchor Graph Hashing", "text": "Following the similar objective as spectral hashing, anchor graph hashing was designed to solve the problem from a different perspective without the assumption of uniform distribution [28]. Note that the critical bottleneck for solving Eq. 15 is the cost of building a pariwsie similarity graph A, the computation of associated graph Laplacian, as well as solving the corresponding eigen-system, which at least has a quadratic complexity. The key idea is to use a small set of MpM !Nq anchor points to approximate the graph structure represented by the matrix A such that the similarity between any pair of points can be approximated using point-to-anchor similarities [97]. In particular, the truncated point-to-anchor similarity Z P RN\u02c6M gives the similarities between N database points to the M anchor points. Thus, the approximated similarity matrix A\u0302 can be calculated as A\u0302 \u201c Z\u039bZJ, where \u039b \u201c diagpZ1q is the degree matrix of the anchor graph Z. Based on such an approximation, instead of solving the eigen-system of the matrix A\u0302\u201c Z\u039bZJ, one can alternatively solve a much smaller eigen-system with an M \u02c6M matrix \u039b1{2ZJZ\u039b1{2. The final binary codes can be obtained through calculating the sign function over a spectral embedding as\nY \u201c sgnpZ\u039b1{2V\u03a31{2q, (16)\nHere we have the matrices V \u201c rv1, \u00a8 \u00a8 \u00a8 , vk, \u00a8 \u00a8 \u00a8 ,vK s P R M\u02c6K and \u03a3 \u201c diagp\u03c31, \u00a8 \u00a8 \u00a8 , \u03c3k, \u00a8 \u00a8 \u00a8 , \u03c3Kq P R K\u02c6K , where tvk,\u03c3ku are the eigenvector-eigenvalue pairs [28]. Figure 6 shows the two-bit partitioning on a synthetic data with nonlinear structure using different hashing methods, including spectral hashing, exact graph hashing, and anchor graph hashing. Note that since spectral hashing computes two smoothest pseudo graph Laplacian eigenfunctions instead of performing real spectral embedding, it can not handle such type of nonlinear data structures. The exact graph hashing method first constructs an exact neighborhood graph, e.g., kNN graph, and then performs partitioning with spectral techniques to solve the optimization problem in Eq.15. The anchor graph hashing archives a\ngood separation (by the first bit) of the nonlinear manifold and balancing partitioning, even performs better than the exact graph hashing, which loses the property of balancing partitioning for the second bit. The anchor graph hashing approach was recently further improved by leveraging a discrete optimization technique to directly solve binary hash codes without any relaxation [98]."}, {"heading": "C. Angular Quantization Based Hashing", "text": "Since similarity is often measured by the cosine of the angle between pairs of samples, angular quantization is thus proposed to map non-negative feature vectors onto a vertex of the binary hypercube with the smallest angle [63]. In such a setting, the vertices of the hypercube is treated as quantization landmarks that grow exponentially with the data dimensionality D. As shown in Figure 7, the nearest binary vertex b in a hypercube to the data point x is given by\nb\u02da \u201c argmax b\nbJx }b}2 (17)\nsubject to: b P t0,1uK ,\nAlthough it is an integer programming problem, its global maximum can be found with a complexity of OpD logDq. The optimal binary vertices will be used as the binary hash codes for data points as y \u201c b\u02da. Based on this angular quantization framework, a data-dependent extension is designed to learn a rotation matrix R P RD\u02c6D to align the projected data RJx to the binary vertices without changing the similarity between point pairs. The objective is\nformulated as the following\npb\u02dai ,R \u02daq \u201c argmax\nbi,R\n\u00ff\ni\nbJi }bi}2 RJxi (18)\nsubject to: b P t0,1uK\nRJR\u201c ID\u02c6D\nNote that the above formulation still generates a D-bit binary code for each data point, while compact codes are often desired in many real-world applications [14]. To generate a K=bit code, a projection matrix S P RD\u02c6K with orthogonal columns can be used to replace the rotation matrix R in the above objective with additional normalization, as discussed in [63]. Finally, the optimal binary codes and the projection/rotation matrix are learned using an alternating optimization scheme."}, {"heading": "D. Binary Reconstructive Embedding", "text": "Instead of using data-independent random projections as in LSH or principal components as in SH, Kulis and Darrell [27] proposed data-dependent and bit-correlated hash functions as:\nhkpxq \u201c sgn\n\u02dc\ns \u00ff\nq\u201c1\nWkq\u03bapxkq ,xq\n\u00b8\n(19)\nThe sample set txkqu, q \u201c 1, \u00a8 \u00a8 \u00a8 , s is the training data for learning hash function hk and \u03bap\u00a8q is a kernel function, and W is a weight matrix. Based on the above formulation, a method called Binary Reconstructive Embedding (BRE) was designed to minimize a cost function measuring the difference between the metric and reconstructed distance in hamming space. The Euclidean metric dM and the binary reconstruction distance dR are defined as:\ndMpxi,xjq \u201c 1\n2 }xi \u00b4xj}\n2 (20)\ndRpxi,xjq \u201c 1\nK\nK \u00ff\nk\u201c1\nphkpxiq\u00b4 hkpxjqq 2\nThe objective is to minimize the following reconstruction error to derive the optimal W:\nW\u02da \u201c argmin W\n\u00ff\npxi,xjqPN\nrdMpxi,xjq\u00b4 dRpxi,xjqs 2 , (21)\nwhere the set of sample pairs N is the training data. Optimizing the above objective function is difficult due to the non-differentiability of sgnp\u00a8q function. Instead, a coordinate-descent algorithm was applied to iteratively update the hash functions to a local optimum. This hashing method can be easily extended to a supervised scenario by setting pairs with same labels to have zero distance and pairs with different labels to have a large distance. However, since the binary reconstruction distance dR is bounded in r0,1s while the metric distance dM has no upper bound, the minimization problem in Eq. (21) is only meaningful when input data is appropriately normalized. In practice, the original data point x is often mapped to a hypersphere with unit length so that 0 \u010f dM \u010f 1. This normalization removes the scale of data points, which is often not negligible for practical applications of nearest neighbor search. In addition, Hamming distance based objective is hard to optimize due to its nonconvex and nonsmooth properties. Hence, Liu et al. proposed to utilize the equivalence between code inner products and the Hamming distances to design supervised and kernel-based hash functions [22]. The objective is to ensure the inner product of hash codes consistent with the given pairwise supervision. Such a strategy of optimizing the hash code inner product in KSH rather than the Hamming distance like what\u2019s done in BRE pays off nicely and leads to major performance gains in similarity-based retrieval consistently confirmed in extensive experiments reported in [22] and recent studies [99]."}, {"heading": "E. Metric Learning based Hashing", "text": "The key idea for metric learning based hashing method is to learn a parameterized Mahalanobis metric using pairwise label information. Such learned metrics are then employed to the standard random projection based hash functions [19]. The goal is to preserve the pairwise relationship in the binary code space, where similar data pairs are more\nlikely to collide in the same hash buck and dissimilar pairs are less likely to share the same hash codes, as illustrated in Figure 8. The parameterized inner product is defined as\nsimpxi,xjq \u201c x J i Mxj ,\nwhere M is a positive-definite d\u02c6 d matrix to be learned from the labeled data. Note that this similarity measure corresponds to the parameterized squared Mahalanobis distance dM. Assume that M can be factorized as M \u201c GJG. Then the parameterized squared Mahalanobis distance can be written as\ndMpxi,xjq \u201c pxi \u00b4xjq JMpxi \u00b4xjq (22)\n\u201c pGxi \u00b4Gxiq JpGxi \u00b4Gxiq.\nBased on the above equation, the distance dMpxi,xjq can be interpreted as the Euclidian distance between the projected data points Gxi and Gxj . Note that the matrix M can be learned through various metric learning method such as information-theoretic metric learning [100]. To accommodate the learned distance metric, the randomized hash function is given as\nhkpxq \u201c sgnpwkG Jxq. (23)\nIt is easy to see that the above hash function generates the hash codes which preserve the parameterized similarity measure in the Hamming space. Figure 8 demonstrates the difference between standard random projection based LSH and the metric learning based LSH, where it is easy to see that the learned metric help assign the same hash bit to the similar sample pairs. Accordingly, the collision probability is given as\nPr rhkpxiq \u201c hkpxiqs \u201c 1\u00b4 1\n\u03c0 cos\u00b41\nxJi G JGxj\n}Gxi}}Gxj} (24)\nRealizing that the pairwise constraints often come to be available incrementally, Jain et al exploit an efficient online locality-sensitive hashing with gradually learned distance metrics [70]."}, {"heading": "F. Semi-Supervised Hashing", "text": "Supervised hashing techniques have been shown to be superior than unsupervised approaches since they leverage the supervision information to design task-specific hash codes. However, for a typical setting of large scale problem, the human annotation process is often costly and the labels can be noisy and sparse, which could easily lead to overfitting. Considering a small set of pairswise labels and a large amount of unlabled data, semi-supervised hashing aims in designing hash functions with minimum empirical loss while maintaining maximum entropy over the entire dataset. Specifically, assume the pairwise labels are given as two type of sets M and C. A pair of data point pxi,xjq P M indicates that xi and xj are similar and pxi,xjq P C means that xi and xj . Hence, the empirical accuracy on the labeled data for a family of hash functions H\u201c rh1, \u00a8 \u00a8 \u00a8 ,hKs is given as\nJpHq\u201c \u00ff\nk\n\u00bb\n\u2013\n\u00ff\npxi,xjqPM\nhkpxiqhkpxjq\u00b4 \u00ff\npxi,xjqPC\nhkpxiqhkpxjq\nfi\nfl . (25)\nIf define a matrix S P Rl\u02c6l incorporating the pairwise labeled information from Xl as:\nSij \u201c\n$\n&\n% 1 : pxi,xjq PM \u00b41 : pxi,xjq P C 0 : otherwise. , (26)\nThe above empirical accuracy can be written in a compact matrix form after dropping off the sgnp\u00a8q function\nJpHq \u201c 1\n2 tr\n`\nWJXl S W JXJl\n\u02d8\n. (27)\nHowever, only considering the empirical accuracy during the design of the hash function can lead to undesired results. As illustrated in Figure 9(b), although such a hash bit partitions the data with zero error over the pairwise labeled data, it results in imbalanced separation of the unlabeled data, thus being less informative. Therefore, an information theoretic regularization is suggested to maximize the entropy of each hash bit. After relaxation, the final objective is formed as\nW\u02da \u201c argmax W\n1 2 tr ` WJXlSX J l W \u02d8 ` \u03b7 2 tr ` WJXXJW \u02d8 ,\n(28)\nwhere the first part represents the empirical accuracy and the second component encourages partitioning along large variance projections. The coefficient \u03b7 weighs the contribution from these two components. The above objective can be solved using various optimization strategies, resulting in orthogonal or correlated binary codes, as described in [14][25]. Figure 9 illustrates the comparison of one-bit linear partition using different learning paradigms, where the semi-supervised method tends to produce balanced yet accurate data separation. Finally, Xu et al. employs similar semi-supervised formulation to sequentially learn multiple complementary hash tables to further improve the performance [31]."}, {"heading": "G. Column Generation Hashing", "text": "Beyond pairwise relationship, complex supervision like ranking triplets and ranking lists has been exploited to learn hash functions with the property of ranking preserving. In many real applications such as image retrieval and recommendation system, it is often easier to receive the relative comparison instead of instance-wise or pari-wise labels. For a general claim, such relative comparison information is given in a triplet form. Formally, a set of triplets are represented as:\nE \u201c tpqi,x ` i ,x \u00b4 i q|simpqi,x ` i q \u0105 simpqi,x \u00b4 i qu,\nwhere the the function simp\u00a8q could be an unknown similarity measure. Hence, the triplet pqi,x ` i ,x \u00b4 i q indicates that the sample point x`i is more semantically similar or closer to a query point q than the point x\u00b4i , as demonstrated in Figure 4(b). As one of the representative methods falling into this category, column generation hashing explores the largemargin framework to leverage such type of proximity comparison information to design weighted hash functions [74]. In particular, the relative comparison information simpqi,x ` i q \u0105 simpqi,x \u00b4 i q will be preserved in a weighted Hamming space as dWHpqi,x ` i q \u0103 dWHpqi,x \u00b4 i q, where dWH is the weighted Hamming distance as defined in Eq. 14. To impose a large-margin, the constraint dWHpqi,x ` i q \u0103 dWHpqi,x \u00b4 i q should be satisfied as well as possible. Thus, a typical large-margin objective with \u21131 norm regularization can be formulated as\nargmin w,\u03be\n|E| \u00ff\ni\u201c1\n\u03bei `C}w}1 (29)\nsubject to: w \u013e 0,\u03be \u013e 0;\ndWHpqi,x \u00b4 i q\u00b4 dWHpqi,x \u00b4 i q \u011b 1\u00b4 \u03bei,@i,\nwhere w is the random projections for computing the hash codes. To solve the above optimization problem, the authors proposed using column generation technique to learn the hash function and the associated bit weights iteratively. For each iteration, the best hash function is generated and the weight vector is updated accordingly. In addition, different loss functions with other regularization terms such as \u21138 are also suggested as alternatives in the above formulation."}, {"heading": "H. Ranking Supervised Hashing", "text": "Different from other methods that explore the triplet relationship [74][75][65], the ranking supervised hashing method attempt to preserve the ranking order of a set of database points corresponding to the query point [76]. Assume that the training dataset X \u201c txnu hasN points with xn P R\nD. In addition, a query set is given as Q \u201c tqmu, and qm P R\nD,m\u201c 1, \u00a8 \u00a8 \u00a8 ,M . For any specific query point qm, we can derive a ranking list over X , which can be written as a vector as rpqm,X q \u201c pr m 1 , \u00a8 \u00a8 \u00a8 , r m n , \u00a8 \u00a8 \u00a8 , r m N q. Each element rmn falls into the integer range r1,N s and no two elements share the same value for the exact ranking case. If\nrmi \u0103 r m j (i, j \u201c 1, \u00a8 \u00a8 \u00a8 ,N), it indicates sample xi has higher rank than xj , which means xi is more relevant or similar to qm than xj . To represent such a discrete ranking list, a ranking triplet matrix S P RN\u02c6N is defined as\nSpqm;xi,xjq \u201c\n$\n&\n%\n1 : rqi \u0103 r q j \u00b41 : rqi \u0105 r q j\n0 : rqi \u201c r q j .\n(30)\nHence for a set of query points Q\u201c tqmu, we can derive a triplet tensor, i.e., a set of triplet matrices\nS \u201c tSpqmqu P R M\u02c6N\u02c6N .\nIn particular, the element of the triplet tensor is defined as Smij \u201c Spqmqpi, jq \u201c Spqm; xi, xjq, m \u201c 1, \u00a8 \u00a8 \u00a8 , M , i, j \u201c 1, \u00a8 \u00a8 \u00a8 , N . The objective is to preserve the ranking lists in the mapped Hamming space. In other words, if Spqm; xi, xjq \u201c 1, we tend to ensure dHpqm, xiq \u0103 dHpqm, xiq, otherwise dHpqm,xiq \u0105 dHpqm,xiq. Assume the hash code has the value as t\u00b41,1u, such ranking order is equivalent to the similarity measurement using the inner products of the binary codes, i.e.,\ndHpqm,xiq\u0103dHpqm,xiq\u00f4Hpqmq JHpxiq\u0105Hpqmq JHpxjq.\nThen the empirical loss function LH over the ranking list can be represented as\nLH \u201c \u00b4 \u00ff\nm\n\u00ff\ni,j\nHpqmq J rHpxiq \u00b4HpxjqsSmij .\nAssume we utilize linear hash functions, the final objective is formed as the following constrained quadratic problem\nW\u02da \u201c argmax W LH \u201c argmax W trpWWJBq (31)\ns.t. WJW\u201c I,\nwhere the constant matrix B is computed as B \u201c \u0159\nm pmq J m with pm \u201c\n\u0159\ni,j rxi \u00b4xjsSmij . The orthogonality constraint is utilized to minimize the redundancy between different hash bits. Intuitively, the above formulation is to preserve the ranking list in the Hamming space, as shown in the conceptual diagram in Figure 10. The augmented Lagrangian multiplier method was introduced to derive feasible solutions for the above constrained problem, as discussed in [76]."}, {"heading": "I. Circulant Binary Embedding", "text": "Realizing that most of the current hashing techniques rely on linear projections, which could suffer from very high computational and storage costs for high-dimensional data, circulant binary embedding was recently developed to handle such a challenge using the circulant projection [81]. Briefly, given a vector r \u201c tr0, \u00a8 \u00a8 \u00a8 , rd\u00b41u, we can generate its corresponding circulant matrix R \u201c circprq [101]. Therefore, the binary embedding with the circulant projection is defined as:\nhpxq \u201c sgnpRxq \u201c sgnpcircprq \u00a8xq. (32)\nSince the circulant projection circprqx is equivalent to circular convolution rgx, the computation of linear projection can be eventually realized using fast Fourier transform as\ncircprqx\u201c rgx\u201c F\u00b41 pFprq \u02ddFpxqq . (33)\nThus, the time complexity is reduced from d2 to d logd. Finally, one could randomly select the circulant vector r or design specific ones using supervised learning methods."}, {"heading": "V. Deep Learning for Hashing", "text": "During the past decade (since around 2006), Deep Learning [102], also known as Deep Neural Networks, has drawn increasing attention and research efforts in a variety of artificial intelligence areas including speech recognition, computer vision, machine learning, text mining, etc. Since one main purpose of deep learning is to learn robust and powerful feature representations for complex data, it is very natural to leverage deep learning for exploring compact hash codes which can be regarded as binary representations of data. In this section, we briefly introduce several recently proposed hashing methods that employ deep learning. In Table III, we compare eight deep learning based hashing methods in terms of four key characteristics that can be used to differentiate the approaches. The earliest work in deep learning based hashing may be Semantic Hashing [103]. This method builds a deep generative model to discover hidden binary units (i.e., latent topic features) which can model input text data (i.e.,\nword-count vectors). Such a deep model is made as a stack of Restricted Boltzmann Machines (RBMs) [104]. After learning a multi-layer RBM through pre-training and finetuning on a collection of documents, the hash code of any document is acquired by simply thresholding the output of the deepest layer. Such hash codes provided by the deep RBM were shown to preserve semantically similar relationships among input documents into the code space, in which each hash code (or hash key) is used as a memory address to locate corresponding documents. In this way, semantically similar documents are mapped to adjacent memory addresses, thereby enabling efficient search via hash table lookup. To enhance the performance of deep RBMs, a supervised version was proposed in [66], which borrows the idea of nonlinear Neighbourhood Component Analysis (NCA) embedding [105]. The supervised information stems from given neighbor/nonneighbor relationships between training examples. Then, the objective function of NCA is optimized on top of a deep RBM, making the deep RBM yield discriminative hash codes. Note that supervised deep RBMs can be applied to broad data domains other than text data. In [66], supervised deep RBMs using a Gaussian distribution to model visible units in the first layer were successfully applied to handle massive image data.\nA recent work named Sparse Similarity-Preserving Hashing [99] tried to address the low recall issue pertaining to relatively long hash codes, which affect most of previous hashing techniques. The idea is enforcing sparsity into the hash codes to be learned from training examples with pairwise supervised information, that is, similar and dissimilar pairs of examples (also known as side information in the machine learning literature). The relaxed hash functions, actually nonlinear embedding functions, are learned by training a Tailored Feed-Forward Neural Network. Within this architecture, two ISTA-type networks [110] that share the same set of parameters and conduct fast approximations of sparse coding are coupled in the training phase. Since each output of the neural network is continuous albeit sparse, a hyperbolic tangent function is applied to the output followed by a thresholding operation, leading to the final binary hash code. In [99], an extension to hashing multimodal data, e.g., web images with textual tags, was also presented.\nAnother work named Deep Hashing [106] developed a deep neural network to learn a multiple hierarchical nonlinear transformation which maps original images to compact binary hash codes and hence supports large-scale image retrieval with the learned binary image representation. The deep hashing model is established under three constraints which are imposed on the top layer of the deep neural network: 1) the reconstruction error between an original realvalued image feature vector and the resulting binary code is minimized, 2) each bit of binary codes has a balance, and 3) all bits are independent of each other. Similar constraints have been adopted in prior unsupervised hashing or binary coding methods such as Iterative Quantization (ITQ) [111]. A supervised version called Supervised Deep\nHashing3 was also presented in [106], where a discriminative term incorporating pairwise supervised information is added to the objective function of the deep hashing model. The authors of [106] showed the superiority of the supervised deep hashing model over its unsupervised counterpart. Both of them produce hash codes through thresholding the output of the top layer in the neural network, where all activation functions are hyperbolic tangent functions. It is worthwhile to point out that the above methods, including Sparse Similarity-Preserving Hashing, Deep Hashing and Supervised Deep Hashing, did not include a pretraining stage during the training of the deep neural networks. Instead, the hash codes are learned from scratch using a set of training data. However, the absence of pre-training may make the generated hash codes less effective. Specifically, the Sparse Similarity-Preserving Hashing method is found to be inferior to the state-of-the-art supervised hashing method Kernel-Based Supervised Hashing (KSH) [22] in terms of search accuracy on some image datasets [99]; the Deep Hashing method and its supervised version are slightly better than ITQ and its supervised version CCA+ITQ, respectively [111][106]. Note that KSH, ITQ and CCA+ITQ exploit relatively shallow learning frameworks. Almost all existing hashing techniques including the aforementioned ones relying on deep neural networks take a vector of hand-crafted visual features extracted from an image as input. Therefore, the quality of produced hash codes heavily depends on the quality of hand-crafted features. To remove this barrier, a recent method called Convolutional Neural Network Hashing [107] was developed to integrate image feature learning and hash value learning into a joint learning model. Given pairwise supervised information, this model consists of a stage of learning approximate hash codes and a stage of training a deep Convolutional Neural Network (CNN) [112] that outputs continuous hash values. Such hash values can be generated by activation functions like sigmoid, hyperbolic tangent or softmax, and then quantized into binary hash codes through appropriate thresholding. Thanks to the power of CNNs, the joint model is capable of simultaneously learning image features and hash values, directly working on raw image pixels. The deployed CNN is composed of three convolution-pooling layers that involve rectified linear activation, max pooling, and local contrast normalization, a standard fully-connected layer, and an output layer with softmax activation functions. Also based on CNNs, a latest method called as Deep Semantic Ranking Hashing [108] was presented to learn hash values such that multilevel semantic similarities among multi-labeled images are preserved. Like the Convolutional Neural Network Hashing method, this method takes image pixels as input and trains a deep CNN, by which image feature representations and hash values are jointly learned. The deployed CNN consists of five convolution-\n3 It is essentially semi-supervised as abundant unlabeled examples are used for training the deep neural network.\npooling layers, two fully-connected layers, and a hash layer (i.e., output layer). The key hash layer is connected to both fully-connected layers and in the function expression as\nhpxq \u201c 2\u03c3 ` wJrf1pxq; f2pxqs \u02d8 \u00b4 1,\nin which x represents an input image, hpxq represents the vector of hash values for image x, f1pxq and f2pxq respectively denote the feature representations from the outputs of the first and second fully-connected layers, w is the weight vector, and \u03c3pq is the logistic function. The Deep Semantic Ranking Hashing method leverages listwise supervised information to train the CNN, which stems from a collection of image triplets that encode the multilevel similarities, i.e., the first image in each triplet is more similar to the second one than the third one. The hash code of image x is finally obtained by thresholding the output hpxq of the hash layer at zero. The above Convolutional Neural Network Hashing method [107] requires separately learning approximate hash codes to guide the subsequent learning of image representation and finer hash values.A latest method called Deep Neural Network Hashing [109] goes beyond, in which the image representation and hash values are learned in one stage so that representation learning and hash learning are tightly coupled to benefit each other. Similar to the Deep Semantic Ranking Hashing method [108], the Deep Neural Network Hashing method incorporates listwise supervised information to train a deep CNN, giving rise to a currently deepest architecture for supervised hashing. The pipeline of the deep hashing architecture includes three building blocks: 1) a triplet of images (the first image is more similar to the second one than the third one) which are fed to the CNN, and upon which a triplet ranking loss is designed to characterize the listwise supervised information; 2) a shared sub-network with a stack of eight convolution layers to generate the intermediate image features; 3) a divide-and-encode module to divide the intermediate image features into multiple channels, each of which is encoded into a single hash bit. Within the divide-and-encode module, there are one fully-connected layer and one hash layer. The former uses sigmoid activation, while the latter uses a piecewise thresholding scheme to produce a nearly discrete hash values. Eventually, the hash code of any im-\nage is yielded by thresholding the output of the hash layer at 0.5. In [109], the Deep Neural Network Hashing method was shown to surpass the Convolutional Neural Network Hashing method as well as several shallow learning based supervised hashing methods in terms of image search accuracy. Last, we a few observations are worth mentioning deep learning based hashing methods introduced in this section. 1. The majority of those methods did not report the time of hash code generation. In real-world search scenarios, the speed for generating hashes should be substantially fast. There might be concern about the hashing speed of those deep neural network driven approaches, especially the approaches involving image feature learning, which may take much longer time to hash an image compared to shallow learning driven approaches like ITQ and KSH.\n2. Instead of employing deep neural networks to seek hash codes, another interesting problem is to design a proper hashing technique to accelerate deep neural network training or save memory space. The latest work [113] presented a hashing trick named HashedNets, which shrinks the storage costs of neural networks significantly while mostly preserving the generalization performance in image classification tasks."}, {"heading": "VI. Advanced Methods and Related Applications", "text": "In this section, we further extend the survey scope to cover a few more advanced hashing methods that are developed for specific settings and applications, such as pointto-hyperplane hashing, subspace hashing, and multimodality hashing."}, {"heading": "A. Hyperplane Hashing", "text": "Distinct from the previously surveyed conventional hashing techniques all of which address the problem of fast point-to-point nearest neighbor search (see Figure 12(a)), a new scenario \u201cpoint-to-hyperplane\u201d hashing emerges to tackle fast point-to-hyperplane nearest neighbor search (see Figure 12(b)), where the query is a hyperplane instead\n4 The illustration figure is from http://vision.cs.utexas.edu/projects/activehash/\nof a data point. Such a new scenario requires hashing the hyperplane query to near database points, which is difficult to accomplish because point-to-hyperplane distances are quite different from routine point-to-point distances in terms of the computation mechanism. Despite the bulk of research on point-to-point hashing, this special hashing paradigm is rarely touched. For convenience, we call point-to-hyperplane hashing as Hyperplane Hashing. Hyperplane hashing is actually fairly important for many machine learning applications such as large-scale active learning with SVMs [114]. In SVM-based active learning [115], the well proven sample selection strategy is to search in the unlabeled sample pool to identify the sample closest to the current hyperplane decision boundary, thus providing the most useful information for improving the learning model. When making such active learning scalable to gigantic databases, exhaustive search for the point nearest to the hyperplane is not efficient for the online sample selection requirement. Hence, novel hashing methods that can principally handle hyperplane queries are called for. A conceptual diagram using hyperplane hashing to scale up active learning process is demonstrated in Figure 11. We demonstrate the geometric relationship between a data point x and a hyperplane Pw with the vector normal as w in Figure 13(a). Given a hyperplane query Pw and a set of points X , the target nearest neighbor is\nx\u02da \u201c argmin xPX Dpx,Pwq,\nwhere Dpx,Pwq \u201c |wJx| }w} is the point-to-hyperplane distance. The existing hyperplane hashing methods [116][117] all attempt to minimize a slightly modified \u201cdistance\u201d |wJx| }w}}x} , i.e., the sine of the point-to-hyperplane angle \u03b1x,w \u201c \u02c7 \u02c7\u03b8x,w \u00b4 \u03c0 2 \u02c7\n\u02c7. Note that \u03b8x,w P r0,\u03c0s is the angle between x and w. The angle measure \u03b1x,w P r0,\u03c0{2s between a database point and a hyperplane query turns out to be reflected into the design of hash functions. As shown in Figure 13(b), the goal of hyperplane hashing is to hash a hyperplane query Pw and the desired neighbors (e.g., x1,x2) with narrow \u03b1x,w into the same or nearby hash buckets, meanwhile avoiding to return the unde-\nsired nonneighbors (e.g., x3,x4) with wide \u03b1x,w. Because \u03b1x,w \u201c \u02c7 \u02c7\u03b8x,w \u00b4 \u03c0 2 \u02c7\n\u02c7, the point-to-hyperplane search problem can be equivalently transformed to a specific point-to-point search problem where the query is the hyperplane normal w and the desired nearest neighbor to the raw query Pw is the one whose angle \u03b8x,w from w is closest to \u03c0{2, i.e., most closely perpendicular to w (we write \u201cperpendicular tow\u201d as Kw for brevity). This is very different from traditional point-to-point nearest neighbor search which returns the most similar point to the query point. In the following, several existing hyperplane hashing methods will be briefly discussed\nJain et al. [117] devised two different families of randomized hash functions to attack the hyperplane hashing problem. The first one is Angle-Hyperplane Hash (AHHash) A, of which one instance function is\nhApzq \u201c \"\nrsgnpuJzq,sgnpvJzqs,z is a database point rsgnpuJzq,sgnp\u00b4vJzqs, z is a hyperplane normal\n(34)\nwhere z P Rd represents an input vector, and u and v are both drawn independently from a standard d-variate Gaussian, i.e., u,v \u223cN p0, Id\u02c6dq. Note that h\nA is a two-bit hash function which leads to the probability of collision for a hyperplane normal w and a database point x:\nPr \u201c hApwq \u201c hApxq \u2030 \u201c 1\n4 \u00b4\n\u03b12x,w\n\u03c02 . (35)\nThis probability monotonically decreases as the point-tohyperplane angle \u03b1x,w increases, ensuring angle-sensitive hashing.\nThe second family proposed by Jain et al. is EmbeddingHyperplane Hash (EH-Hash) function family E of which\none instance is\nhEpzq \u201c\n\"\nsgn ` UJVpzzJq \u02d8\n,z is a database point sgn ` \u00b4UJVpzzJq \u02d8 , z is a hyperplane normal\n(36)\nwhere VpAq returns the vectorial concatenation of matrix A, and U\u223cN p0, Id2\u02c6d2q. The EH hash function h E yields hash bits on an embedded space Rd 2\nresulting from vectorizing rank-one matrices zzJ and \u00b4zzJ. Compared with hA, hE gives a higher probability of collision:\nPr \u201c hEpwq \u201c hEpxq \u2030 \u201c cos\u00b41 sin2p\u03b1x,wq\n\u03c0 , (37)\nwhich also bears the angle-sensitive hashing property. However, it is much more expensive to compute than AHHash. More recently, Liu et al. [117] designed a randomized function family with bilinear Bilinear-Hyperplane Hash (BH-Hash) as:\nB \u201c hBpzq \u201c sgnpuJzzJvq, i.i.d. u,v \u223cN p0, Id\u02c6dq ( .\n(38)\nAs a core finding, Liu et al. proved in [117] that the probability of collision for a hyperplane query Pw and a database point x under hB is\nPr \u201c hBpPwq \u201c h Bpxq \u2030 \u201c 1\n2 \u00b4 2\u03b12x,w \u03c02 . (39)\nSpecifically, hBpPwq is prescribed to be \u00b4h Bpwq. Eq. (39) endows hB with the angle-sensitive hashing property. It is important to find that the collision probability given by the BH hash function hB is always twice of the collision probability by the AH hash function hA, and also greater than the collision probability by the EH hash function hE . As illustrated in Figure 14, for any fixed r, BH-Hash accomplishes the highest probability of collision, which indicates that the BH-Hash has a better angle-sensitive property. In terms of the formulation, the bilinear hash function hB is correlated with yet different from the linear hash\nfunctions hA and hE . (1) hB produces a single hash bit which is the product of the two hash bits produced by hA. (2) hB may be a rank-one special case of hE in algebra if we write uJzzJv \u201c trpzzJvuJq and UJVpzzJq \u201c trpzzJUq. (3) hB appears in a universal form, while both hA and hE treat a query and a database item in a distinct manner. The computation time of hB is \u0398p2dq which is the same as that of hA and one order of magnitude faster than \u0398p2d2q of hE . Liu et al. further improved the performance of hB through learning the bilinear projection directions u,v in hB from the data. Gong et al. extended the bilinear formulation to the conventional point-to-point hashing scheme through designing compact binary codes for highdimensional visual descriptors [118]."}, {"heading": "B. Subspace Hashing", "text": "Beyond the aforementioned conventional hashing which tackles searching in a database of vectors, subspace hashing [119], which has been rarely explored in the literature, attempts to efficiently search through a large database of subspaces. Subspace representation is very common in many computer vision, pattern recognition, and statistical learning problems, such as subspace representations of image patches, image sets, video clips, etc. For example, face images of the same subject with fixed poses but different illuminations are often assumed to reside near linear subspaces. A common use scenario is to use a single face image to find the subspace (and the corresponding subject ID) closest to the query image [120]. Given a query in the form of vector or subspace, searching for a nearest subspace in a subspace database is frequently encountered in a variety of practical applications including example-based image synthesis, scene classification, speaker recognition, face recognition, and motion-based action recognition [120].\nHowever, hashing and searching for subspaces are both different from the schemes used in traditional vector hashing and the latest hyperplane hashing. [119] presented a general framework to the problem of Approximate Nearest Subspace (ANS) search, which uniformly deals with the cases that query is a vector or subspace, query and database elements are subspaces of fixed dimension, query and database elements are subspaces of different dimension, and database elements are subspaces of varying di-\nmension. The critical technique exploited by [119] is twostep: 1) a simple mapping that maps both query and database elements to \u201cpoints\u201d in a new vector space, and 2) doing approximate nearest neighbor search using conventional vector hashing algorithms in the new space. Consequently, the main contribution of [119] is reducing the difficult subspace hashing problem to a regular vector hashing task. [119] used LSH for the vector hashing task. While simple, the hashing technique (mapping + LSH) of [119] perhaps suffers from the high dimensionality of the constructed new vector space. More recently, [120] exclusively addressed the point-tosubspace query where query is a vector and database items are subspaces of arbitrary dimension. [120] proposed a rigorously faster hashing technique than that of [119]. Its hash function can hash D-dimensional vectors (D is the ambient dimension of the query) or D\u02c6r-dimensional subspaces (r is arbitrary) in a linear time complexity OpDq, which is computationally more efficient than the hash functions devised in [119]. [120] further proved the search time under the OpDq hashes to be sublinear in the database size. Based on the nice finding of [120], we would like to achieve faster hashing for the subspace-to-subspace query by means of crafted novel hash functions to handle subspaces in varying dimension. Both theoretical and practical explorations in this direction will be beneficial to the hashing area."}, {"heading": "C. MultiModality Hashing", "text": "Note that the majority of the hash learning methods are designed for constructing the Hamming embedding for a single modality or representation. Some recent advanced methods are proposed to design the hash functions for more complex settings, such as that the data are represented by multimodal features or the data are formed in a heterogeneous way [121]. Such type of hashing methods are closely related to the applications in social network, whether multimodality and heterogeneity are often observed. Below we survey several representative methods that are proposed recently. Realizing that data items like webpage can be described from multiple information sources, composing hashing was recently proposed to design hashing schme using several information sources [122]. Besides the intuitive way of concatenating multiple features to derive hash functions, the author also presented an iterative weighting scheme and formulated convex combination of multiple features. The objective is to ensure the consistency between the semantic similarity and Hamming similarity of the data. Finally, a joint optimization strategy is employed to learn the importance of individual type of features and the hash functions. Co-regularized hashing was proposed to investigate the hashing learning across multiparity data in a supervised setting, where similar and dissimilar pairs of intra-modality points are given as supervision information [123]. One of such a typical setting is to index images and the text jointly to preserve the semantic relations\nbetween image and text. The authors formulate their objective as a boosted co-regularization framework with the cost component as a weighted sum of the intra-modality and inter-modality loss. The learning process of the hash functions is performed via a boosting procedure to that the bias introduced by previous hash function can be sequentially minimized. Dual-view hashing attempts to derive a hidden common Hamming embedding of data from two views, while maintaining the predictability of the binary codes [124]. A probabilistic model called multimodal latent binary embedding was recently presented to derive binary latent factors in a common Hamming space for indexing multimodal data [125]. Other closely related hashing methods include the design of multiple feature hashing for near-duplicate duplicate detection [126], submodular hashing for video indexing [127], and Probabilistic Attributed Hashing for integrating low-level features and semantic attributes [128]."}, {"heading": "D. Applications with Hashing", "text": "Indexing massive multimedia data, such as images and video, are the natural applications for learning based hashing. Especially, due to the well-known semantic gap, supervised and semi-supervised hashing methods have been extensively studied for image search and retrieval [69][29][41][129][130][131], mobile product search[132]. Other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [134][135], pose estimation [71], object tracking [136], and duplicate detection [126][137][52][51][138]. In addition, this emerging hash learning framework can be exploited for some general machine learning and data mining tasks, including crossmodality data fusion [139], large scale optimization [140], large scale classification and regression [141], collaborative filtering [142], and recommendation [143]. For indexing video sequences, a straightforward method is to independently compute binary codes for each key frames and use a set of hash code to represent video index. More recently, Ye et al. proposed a structure learning framework to derive a video hashing technique that incorporates both temporal and spatial structure information [144]. In addition, advanced hashing methods are also developed for document search and retrieval. For instance, Wang et al. proposed to leverage both tag information and semantic topic modeling to achieve more accurate hash codes [145]. Li et al. designed a two-stage unsupervised hashing framework for fast document retrieval [146].\nHashing techniques have also been applied to the active learning framework to cope with big data applications. Without performing exhaustive test on all the data points, hyperplane hashing can help significantly speed up the interactive training sample selection procedure [114][117][116]. In addition, a two-stage hashing scheme is developed to achieve fast query pair selection for large scale active learning to rank [147]."}, {"heading": "VII. Open Issues and Future Directions", "text": "Despite the tremendous progress in developing a large array of hashing techniques, several major issues remain open. First, unlike the locality sensitive hashing family, most of the learning based hashing techniques lack the theoretical guarantees on the quality of returned neighbors. Although several recent techniques have presented theoretical analysis of the collision probability, they are mostly based on randomized hash functions [116][117][19]. Hence, it is highly desired to further investigate such theoretical properties. Second, compact hash codes have been mostly studied for large scale retrieval problems. Due to their compact form, the hash codes also have great potential in many other large scale data modeling tasks such as efficient nonlinear kernel SVM classifiers [148] and rapid kernel approximation [149]. A bigger question is: instead of using the original data, can one directly use compact codes to do generic unsupervised or supervised learning without affecting the accuracy? To achieve this, theoretically sound practical methods need to be devised. This will make efficient large-scale learning possible with limited resources, for instance on mobile devices. Third, most of the current hashing technicals are designed for given feature representations that tend to suffer from the semnatic gap. One of the possible future directions is to integrate representation learning with binary code learning using advanced learning schemes such as deep neural network. Finally, since heterogeneity has been an important characteristics of the big data applications, one of the future trends will be to design efficient hashing approaches that can leverage heterogeneous features and multi-modal data to improve the overall indexing quality. Along those lines, developing new hashing techniques for composite distance measures, i.e., those based on combinations of different distances acting on different types of features will be of great interest."}], "references": [{"title": "Image retrieval: Ideas, influences, and trends of the new age", "author": ["R. Datta", "D. Joshi", "J. Li", "J.Z. Wang"], "venue": "ACM Computing Surveys, vol. 40, no. 2, pp. 1\u201360, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Nearest-neighbor methods in learning and vision: theory and practice", "author": ["G. Shakhnarovich", "T. Darrell", "P. Indyk"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Dynamic programming, ser. Rand Corporation research study", "author": ["R. Bellman"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1957}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["J. Bentley"], "venue": "Communications of the ACM, vol. 18, no. 9, p. 517, 1975.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1975}, {"title": "Efficient algorithms with neural network behavior", "author": ["S. Omohundro"], "venue": "Complex Systems, vol. 1, no. 2, pp. 273\u2013347, 1987.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1987}, {"title": "Satisfying general proximity/similarity queries with metric trees", "author": ["J. Uhlmann"], "venue": "Information Processing Letters, vol. 40, no. 4, pp. 175\u2013179, 1991.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1991}, {"title": "Data structures and algorithms for nearest neighbor search in general metric spaces", "author": ["P. Yianilos"], "venue": "Proc. of the fourth annual ACM-SIAM Symposium on Discrete algorithms, 1993, pp. 311\u2013321.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1993}, {"title": "Nearest-neighbor searching in high dimensions", "author": ["P. Indyk"], "venue": "Handbook of discrete and computational geometry, J. E. Goodman and J. O\u2019Rourke, Eds. Boca Raton, FL: CRC Press LLC, 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Product quantization for nearest neighbor search", "author": ["H. Jegou", "M. Douze", "C. Schmid"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 1, pp. 117\u2013128, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Optimized product quantization for approximate nearest neighbor search", "author": ["T. Ge", "K. He", "Q. Ke", "J. Sun"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Portland, Oregon, USA, 2013, pp. 2946\u20132953.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["A. Torralba", "R. Fergus", "W. Freeman"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 30, no. 11, pp. 1958\u20131970, 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1958}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "Proc. of 25th International Conference on Very Large Data Bases, 1999, pp. 518\u2013529.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Semi-supervised hashing for large scale search", "author": ["J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Content-based image retrieval at the end of the early years", "author": ["A. Smeulders", "M. Worring", "S. Santini", "A. Gupta", "R. Jain"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 22, no. 12, pp. 1349\u20131380, 2000.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "A learning framework for nearest neighbor search", "author": ["L. Cayton", "S. Dasgupta"], "venue": "Advances in Neural Information Processing Systems 20, J. Platt, D. Koller, Y. Singer, and S. Roweis, Eds. Cambridge, MA: MIT Press, 2008, pp. 233\u2013240.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Compact hashing with joint optimization of search accuracy and time", "author": ["J. He", "S.-F. Chang", "R. Radhakrishnan", "C. Bauer"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, Colorado Springs, CO, USA, 2011, pp. 753\u2013760.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning task-specific similarity", "author": ["G. Shakhnarovich"], "venue": "Ph.D. dissertation, Massachusetts Institute of Technology, 2005.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast similarity search for learned metrics", "author": ["B. Kulis", "P. Jain", "K. Grauman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 12, pp. 2143\u20132157, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Asymmetric distances for binary embeddings", "author": ["A. Gordo", "F. Perronnin"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, Colorado Springs, CO, USA, 2011, pp. 729\u2013736.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Kernelized locality-sensitive hashing", "author": ["B. Kulis", "K. Grauman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Supervised hashing with kernels", "author": ["W. Liu", "J. Wang", "R. Ji", "Y.-G. Jiang", "S.-F. Chang"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, 2012, pp. 2074\u20132081.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Compressed hashing", "author": ["Y. Lin", "R. Jin", "D. Cai", "S. Yan", "X. Li"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Portland, Oregon, USA, 2013, pp. 446\u2013451.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Random maximum margin hashing", "author": ["A. Joly", "O. Buisson"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, Colorado Springs, CO, USA, 2011, pp. 873\u2013880.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Sequential projection learning for hashing with compact codes", "author": ["J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "Proceedings of the 27th International Conference on Machine Learning, 2010, pp. 1127\u20131134.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "K-means hashing: An affinitypreserving quantization method for learning binary compact codes", "author": ["K. He", "F. Wen", "J. Sun"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Portland, Oregon, USA, 2013, pp. 2938\u20132945.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to Hash with Binary Reconstructive Embeddings", "author": ["B. Kulis", "T. Darrell"], "venue": "Proc. of Advances in Neural Information Processing Systems, Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, Eds., 2009, vol. 20, pp. 1042\u20131050.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Hashing with graphs", "author": ["W. Liu", "J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "Proc. of ICML, Bellevue, Washington, USA, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Semi-supervised hashing for scalable image retrieval", "author": ["J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition, San Francisco, USA, 2010, pp. 3424\u20133431.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Minimal loss hashing for compact binary codes", "author": ["M. Norouzi", "D. Fleet"], "venue": "Proceedings of the 27th International Conference on Machine Learning, 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Complementary hashing for approximate nearest neighbor search", "author": ["H. Xu", "J. Wang", "Z. Li", "G. Zeng", "S. Li", "N. Yu"], "venue": " 20  PROCEEDINGS OF THE IEEE Proceedings of the IEEE International Conference on Computer Vision, 2011, pp. 1631\u20131638.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "Proc. of Advances in Neural Information Processing Systems, D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, Eds. MIT Press, 2008, vol. 21, pp. 1753\u20131760.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Locality-sensitive binary codes from shift-invariant kernels", "author": ["M. Raginsky", "S. Lazebnik"], "venue": "Advances in Neural Information Processing Systems 22, Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, Eds. MIT Press, 2009, pp. 1509\u20131517.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M.S. Charikar"], "venue": "Proceedings of the thiry-fourth annual ACM symposium on Theory of computing. ACM, 2002, pp. 380\u2013388.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2002}, {"title": "Localitysensitive hashing scheme based on p-stable distributions", "author": ["M. Datar", "N. Immorlica", "P. Indyk", "V. Mirrokni"], "venue": "Proceedings of the twentieth annual Symposium on Computational Geometry, 2004, pp. 253\u2013262.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2004}, {"title": "LSH forest: self-tuning indexes for similarity search", "author": ["M. Bawa", "T. Condie", "P. Ganesan"], "venue": "Proceedings of the 14th international conference on World Wide Web, Chiba, Japan, 2005, pp. 651\u2013660.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2005}, {"title": "Multiprobe LSH: efficient indexing for high-dimensional similarity search", "author": ["Q. Lv", "W. Josephson", "Z. Wang", "M. Charikar", "K. Li"], "venue": "Proceedings of the 33rd international conference on Very large data bases, 2007, pp. 950\u2013961.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2007}, {"title": "Modeling lsh for performance tuning", "author": ["W. Dong", "Z. Wang", "W. Josephson", "M. Charikar", "K. Li"], "venue": "Proceedings of the 17th ACM Conference on Information and Knowledge Management, 2008, pp. 669\u2013678.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast locality-sensitive hashing", "author": ["A. Dasgupta", "R. Kumar", "T. Sarlos"], "venue": "Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2011, pp. 1073\u20131081.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Bayesian locality sensitive hashing for fast similarity search", "author": ["V. Satuluri", "S. Parthasarathy"], "venue": "Proceedings of the VLDB Endowment, vol. 5, no. 5, pp. 430\u2013441, 2012.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Kernelized locality-sensitive hashing for scalable image search", "author": ["B. Kulis", "K. Grauman"], "venue": "IEEE International Conference on Computer Vision, kyoto, Japan, 2009, pp. 2130 \u2013 2137.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Weakly-supervised hashing in kernel space", "author": ["Y. Mu", "J. Shen", "S. Yan"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition, San Francisco, USA, 2010, pp. 3344\u20133351.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "Super-bit localitysensitive hashing", "author": ["J. Ji", "J. Li", "S. Yan", "B. Zhang", "Q. Tian"], "venue": "inAdvances in Neural Information Processing Systems 25, 2012, pp. 108\u2013116.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2012}, {"title": "Non-metric locality-sensitive hashing.", "author": ["Y. Mu", "S. Yan"], "venue": "Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2010}, {"title": "On the resemblance and containment of documents", "author": ["A. Broder"], "venue": "Compression and Complexity of Sequences Proceedings, 1997, pp. 21\u201329.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1997}, {"title": "Min-wise independent permutations", "author": ["A.Z. Broder", "M. Charikar", "A.M. Frieze", "M. Mitzenmacher"], "venue": "Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, 1998, pp. 327\u2013336.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1998}, {"title": "Mining of massive datasets", "author": ["A. Rajaraman", "J.D. Ullman"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2012}, {"title": "Improved consistent sampling, weighted minhash and l1 sketching", "author": ["S. Ioffe"], "venue": "IEEE International Conference on Data Mining. IEEE, 2010, pp. 246\u2013255.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2010}, {"title": "Finding near-duplicate web pages: a large-scale evaluation of algorithms", "author": ["M. Henzinger"], "venue": "Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, 2006, pp. 284\u2013291.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2006}, {"title": "Google news personalization: scalable online collaborative filtering", "author": ["A.S. Das", "M. Datar", "A. Garg", "S. Rajaram"], "venue": "Proceedings of the 16th international conference on World Wide Web, 2007, pp. 271\u2013280.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2007}, {"title": "Near duplicate image detection: min-hash and tf-idf weighting.", "author": ["O. Chum", "J. Philbin", "A. Zisserman"], "venue": "Proceedings of the the British Machine Vision Conference,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2008}, {"title": "Partition min-hash for partial duplicate image discovery", "author": ["D.C. Lee", "Q. Ke", "M. Isard"], "venue": "European Conference on Computer Vision, Heraklion, Crete, Greece, 2012, pp. 648\u2013662.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2012}, {"title": "b-bit minwise hashing", "author": ["P. Li", "C. K\u00f6nig"], "venue": "Proceedings of the 19th International Conference on World Wide Web, 2010, pp. 671\u2013680.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2010}, {"title": "b-bit minwise hashing for estimating three-way similarities", "author": ["P. Li", "A. Konig", "W. Gui"], "venue": "Advances in Neural Information Processing Systems 23, J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, Eds., 2010, pp. 1387\u20131395.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2010}, {"title": "One permutation hashing", "author": ["P. Li", "A. Owen", "C.-H. Zhang"], "venue": "Advances in Neural Information Processing Systems 25, P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, Eds., 2012, pp. 3122\u20133130.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2012}, {"title": "Geometric min-hashing: Finding a (thick) needle in a haystack", "author": ["O. Chum", "M. Perdoch", "J. Matas"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Miami, Florida, USA, 2009, pp. 17\u201324.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast computation of min-hash signatures for image collections", "author": ["O. Chum", "J. Matas"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, 2012, pp. 3077\u20133084.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2012}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "Proc. of 30th ACM Symposium on Theory of Computing, 1998, pp. 604\u2013613.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 1998}, {"title": "Fast search in hamming space with multi-index hashing", "author": ["M. Norouzi", "A. Punjani", "D.J. Fleet"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, 2012, pp. 3108\u20133115.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2012}, {"title": "Inductive hashing on manifolds", "author": ["F. Shen", "C. Shen", "Q. Shi", "A. van den Hengel", "Z. Tang"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Portland, Oregon, USA, 2013, pp. 1562\u20131569.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2013}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Y. Gong", "S. Lazebnik"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, Colorado Springs, CO, USA, 2011, pp. 817\u2013824.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2011}, {"title": "Isotropic hashing", "author": ["W. Kong", "W.-J. Li"], "venue": "Advances in Neural Information Processing Systems 25, 2012, pp. 1655\u2013 1663.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2012}, {"title": "Angular quantization based binary codes for fast similarity search", "author": ["Y. Gong", "S. Kumar", "V. Verma", "S. Lazebnik"], "venue": "Advances in Neural Information Processing Systems 25, 2012, pp. 1205\u20131213.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2012}, {"title": "Spherical hashing", "author": ["J.-P. Heo", "Y. Lee", "J. He", "S.-F. Chang", "S.-E. Yoon"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, 2012, pp. 2957\u2013 2964.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2012}, {"title": "Hamming distance metric learning", "author": ["M. Norouzi", "D. Fleet", "R. Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 1070\u20131078.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2012}, {"title": "Small codes and large image databases for recognition", "author": ["A. Torralba", "R. Fergus", "Y. Weiss"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, Anchorage, Alaska, USA, 2008, pp. 1\u20138.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2008}, {"title": "Supervise binary hash code learning with jensen shannon divergence", "author": ["L. Fan"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2013}, {"title": "Supervised discrete hashing", "author": ["F. Shen", "C. Shen", "W. Liu", "H.T. Shen"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Boston, Massachusetts, USA, 2015.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast image search for learned metrics", "author": ["P. Jain", "B. Kulis", "K. Grauman"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, Anchorage, Alaska, USA, 2008, pp. 1\u20138.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2008}, {"title": "Online metric learning and fast similarity search", "author": ["P. Jain", "B. Kulis", "I.S. Dhillon", "K. Grauman"], "venue": "Advances in neural information processing systems, 2008, pp. 761\u2013768.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast pose estimation with parameter-sensitive hashing", "author": ["G. Shakhnarovich", "P. Viola", "T. Darrell"], "venue": "IEEE International Conference on Computer Vision, Nice, France, 2003, pp. 750\u2013 757.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2003}, {"title": "Attribute discovery via predictable discriminative binary codes", "author": ["M. Rastegari", "A. Farhadi", "D.A. Forsyth"], "venue": "European Conference on Computer Vision, Florence, Italy, 2012, pp. 876\u2013889.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2012}, {"title": "Non-transitive hashing with latent similarity components", "author": ["M. Ou", "P. Cui", "F. Wang", "J. Wang", "W. Zhu"], "venue": "Proceedings of  WANG, LIU, KUMAR, AND CHANG: LEARNING TO HASH FOR INDEXING BIG DATA - A SURVEY  21 the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 895\u2013904.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning hash functions using column generation", "author": ["X. Li", "G. Lin", "C. Shen", "A.V. den Hengel", "A. Dick"], "venue": "Proceedings of the 30th International Conference on Machine Learning, 2013, pp. 142\u2013150.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2013}, {"title": "Order preserving hashing for approximate nearest neighbor search", "author": ["J. Wang", "J. Wang", "N. Yu", "S. Li"], "venue": "Proceedings of the 21st ACM international conference on Multimedia, 2013, pp. 133\u2013142.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning hash codes with listwise supervision", "author": ["J. Wang", "W. Liu", "A.X. Sun", "Y.-G. Jiang"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2013}, {"title": "Aggregating local descriptors into a compact image representation", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid", "P. P\u00e9rez"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2010, pp. 3304\u20133311.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2010}, {"title": "On the difficulty of nearest neighbor search", "author": ["J. He", "S. Kumar", "S.-F. Chang"], "venue": "Proceedings of the 29th international conference on Machine learning, 2012.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2012}, {"title": "Ldahash: Improved matching with smaller descriptors", "author": ["C. Strecha", "A.M. Bronstein", "M.M. Bronstein", "P. Fua"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 1, pp. 66\u201378, 2012.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient discriminative projections for compact binary descriptors", "author": ["T. Trzcinski", "V. Lepetit"], "venue": "European Conference on Computer Vision, Florence, Italy, 2012, pp. 228\u2013242.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2012}, {"title": "Circulant binary embedding", "author": ["F. Yu", "S. Kumar", "Y. Gong", "S.-F. Chang"], "venue": "Proc. of ICML, Beijing, China, 2014, pp. 946\u2013 954.", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2014}, {"title": "Scalable similarity search with optimized kernel hashing", "author": ["J. He", "W. Liu", "S.-F. Chang"], "venue": "Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2010, pp. 1129\u20131138.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2010}, {"title": "Spec hashing: Similarity preserving algorithm for entropy-based coding", "author": ["R.-S. Lin", "D.A. Ross", "J. Yagnik"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition, San Francisco, USA, 2010, pp. 848\u2013854.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2010}, {"title": "Complementary projection hashing", "author": ["Z. Jin", "Y. Hu", "Y. Lin", "D. Zhang", "S. Lin", "D. Cai", "X. Li"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2013}, {"title": "A general two-step approach to learning-based hashing", "author": ["G. Lin", "C. Shen", "A.V. den Hengel", "D. Suter"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2013}, {"title": "Self-taught hashing for fast similarity search", "author": ["D. Zhang", "J. Wang", "D. Cai", "J. Lu"], "venue": "Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, 2010, pp. 18\u201325.", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2010}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R. Schapire"], "venue": "Computational Learning Theory, 1995, pp. 23\u201337.", "citeRegEx": "87", "shortCiteRegEx": null, "year": 1995}, {"title": "Boostmap: An embedding method for efficient nearest neighbor retrieval", "author": ["V. Athitsos", "J. Alon", "S. Sclaroff", "G. Kollios"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 30, no. 1, pp. 89\u2013104, Jan. 2008.", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2008}, {"title": "Queryadaptive image search with hash codes", "author": ["Y.-G. Jiang", "J. Wang", "X. Xue", "S.-F. Chang"], "venue": "IEEE Transactions on Multimedia, vol. 15, no. 2, pp. 442\u2013453, 2013.", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2013}, {"title": "Lost in binarization: Query-adaptive ranking for similar image search with compact codes", "author": ["Y.-G. Jiang", "J. Wang", "S.-F. Chang"], "venue": "Proceedings of ACM International Conference on Multimedia Retrieval, 2011.", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2011}, {"title": "Weighted hashing for fast large scale similarity search", "author": ["Q. Wang", "D. Zhang", "L. Si"], "venue": "Proceedings of the 22nd ACM Conference on information and knowledge management, 2013, pp. 1185\u20131188.", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2013}, {"title": "Binary code ranking with weighted hamming distance", "author": ["L. Zhang", "Y. Zhang", "X. Gu", "Q. Tian"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Portland, Oregon, USA, 2013, pp. 1586\u2013159.", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2013}, {"title": "Hash bit selection: a unified solution for selection problems in hashing", "author": ["X. Liu", "J. He", "B. Lang", "S.-F. Chang"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Portland, Oregon, USA, 2013, pp. 1570\u20131577.", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2013}, {"title": "Qsrank: Querysensitive hash code ranking for efficient \u01eb-neighbor search", "author": ["X. Zhang", "L. Zhang", "H.-Y. Shum"], "venue": " IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, 2012, pp. 2058\u20132065.", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2012}, {"title": "Spectral grouping using the Nystr\u00f6m method", "author": ["C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, no. 2, pp. 214\u2013225, 2004.", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2004}, {"title": "Multidimensional spectral hashing", "author": ["Y. Weiss", "R. Fergus", "A. Torralba"], "venue": "European Conference on Computer Vision, Florence, Italy, 2012, pp. 340\u2013353.", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2012}, {"title": "Large graph construction for scalable semi-supervised learning", "author": ["W. Liu", "J. He", "S.-F. Chang"], "venue": "Proceedings of the 27th International Conference on Machine Learning, 2010, pp. 679\u2013 686.", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2010}, {"title": "Discrete graph hashing", "author": ["W. Liu", "C. Mu", "S. Kumar", "S.-F. Chang"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 3419\u20133427.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse similarity-preserving hashing", "author": ["J. Masci", "A. Bronstein", "M. Bronstein", "P. Sprechmann", "G. Sapiro"], "venue": "Proc. International Conference on Learning Representations, 2014.", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2014}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "Proceedings of the 24th international conference on Machine learning, 2007, pp. 209\u2013216.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2007}, {"title": "Toeplitz and circulant matrices: A review", "author": ["R.M. Gray"], "venue": "now publishers inc,", "citeRegEx": "101", "shortCiteRegEx": "101", "year": 2006}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, pp. 436\u2013444, 2015.", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic hashing", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "International Journal of Approximate Reasoning, vol. 50, no. 7, pp. 969\u2013978, 2009.", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2009}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "104", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning a nonlinear embedding by preserving class neighbourhood structure", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "Proc. International Conference on Artificial Intelligence and Statistics, 2007, pp. 412\u2013419.", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2007}, {"title": "Deep hashing for compact binary codes learning", "author": ["V.E. Liong", "J. Lu", "G. Wang", "P. Moulin", "J. Zhou"], "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2015.", "citeRegEx": "106", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised hashing for image retrieval via image representation learning", "author": ["R. Xia", "Y. Pan", "H. Lai", "C. Liu", "S. Yan"], "venue": "Proc. AAAI Conference on Artificial Intelligence, 2014, pp. 2156\u2013 2162.", "citeRegEx": "107", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep semantic ranking based hashing for multi-label image retrieval", "author": ["F. Zhao", "Y. Huang", "L. Wang", "T. Tan"], "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2015.", "citeRegEx": "108", "shortCiteRegEx": null, "year": 2015}, {"title": "Simultaneous feature learning and hash coding with deep neural networks", "author": ["H. Lai", "Y. Pan", "Y. Liu", "S. Yan"], "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2015.", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning fast approximations of sparse coding", "author": ["K. Gregor", "Y. LeCun"], "venue": "Proc. International Conference on Machine Learning, 2010, pp. 399\u2013406.", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2010}, {"title": "Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval", "author": ["Y. Gong", "S. Lazebnik", "A. Gordo", "F. Perronnin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 12, pp. 2916\u2013 2929, 2013.", "citeRegEx": "111", "shortCiteRegEx": null, "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Proc. Advances in Neural Information Processing Systems, vol. 25, 2012, pp. 1106\u20131114.", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2012}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "Proc. International Conference on Machine Learning, 2015, pp. 2285\u20132294.", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2015}, {"title": "Hashing hyperplane queries to near points with applications to large-scale active learning", "author": ["S. Vijayanarasimhan", "P. Jain", "K. Grauman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013.", "citeRegEx": "114", "shortCiteRegEx": null, "year": 2013}, {"title": "Support vector machine active learning with applications to text classification", "author": ["S. Tong", "D. Koller"], "venue": "Journal of Machine Learning Research, vol. 2, pp. 45\u201366, 2001.", "citeRegEx": "115", "shortCiteRegEx": null, "year": 2001}, {"title": "Hashing hyperplane queries to near points with applications to large-scale  22  PROCEEDINGS OF THE IEEE active learning", "author": ["P. Jain", "S. Vijayanarasimhan", "K. Grauman"], "venue": "Advances in Neural Information Processing Systems 23, J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, Eds. MIT Press, 2010, pp. 928\u2013936.", "citeRegEx": "116", "shortCiteRegEx": null, "year": 2010}, {"title": "Compact hyperplane hashing with bilinear functions", "author": ["W. Liu", "J. Wang", "Y. Mu", "S. Kumar", "S.-F. Chang"], "venue": "Proceedings of the 29th international conference on Machine learning, 2012.", "citeRegEx": "117", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning binary codes for high-dimensional data using bilinear projections", "author": ["Y. Gong", "S. Kumar", "H. Rowley", "S. Lazebnik"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Portland, Oregon, USA, 2013, pp. 484\u2013491.", "citeRegEx": "118", "shortCiteRegEx": null, "year": 2013}, {"title": "Approximate nearest subspace search", "author": ["R. Basri", "T. Hassner", "L. Zelnik-Manor"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 2, pp. 266\u2013278, 2011.", "citeRegEx": "119", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast subspace search via grassmannian based hashing", "author": ["X. Wang", "S. Atev", "J. Wright", "G. Lerman"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013.", "citeRegEx": "120", "shortCiteRegEx": null, "year": 2013}, {"title": "Sequential spectral learning to hash with multiple representations", "author": ["S. Kim", "Y. Kang", "S. Choi"], "venue": "European Conference on Computer Vision, Florence, Italy, 2012, pp. 538\u2013551.", "citeRegEx": "121", "shortCiteRegEx": null, "year": 2012}, {"title": "Composite hashing with multiple information sources", "author": ["D. Zhang", "F. Wang", "L. Si"], "venue": "Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, 2011, pp. 225\u2013234.", "citeRegEx": "122", "shortCiteRegEx": null, "year": 2011}, {"title": "Co-regularized hashing for multimodal data", "author": ["Y. Zhen", "D.-Y. Yeung"], "venue": "Advances in Neural Information Processing Systems 25, P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, Eds. MIT Press, 2012, pp. 1385\u20131393.", "citeRegEx": "123", "shortCiteRegEx": null, "year": 2012}, {"title": "Predictable dual-view hashing", "author": ["M. Rastegari", "J. Choi", "S. Fakhraei", "D. Hal", "L. Davis"], "venue": "Proceedings of the 30th International Conference on Machine Learning, 2013, pp. 1328\u2013 1336.", "citeRegEx": "124", "shortCiteRegEx": null, "year": 2013}, {"title": "A probabilistic model for multimodal hash function learning", "author": ["Y. Zhen", "D.-Y. Yeung"], "venue": "Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2012, pp. 940\u2013948.", "citeRegEx": "125", "shortCiteRegEx": null, "year": 2012}, {"title": "Multiple feature hashing for real-time large scale near-duplicate video retrieval", "author": ["J. Song", "Y. Yang", "Z. Huang", "H.T. Shen", "R. Hong"], "venue": "Proceedings of the 19th ACM international conference on Multimedia, 2011, pp. 423\u2013432.", "citeRegEx": "126", "shortCiteRegEx": null, "year": 2011}, {"title": "Submodular video hashing: a unified framework towards video pooling and indexing", "author": ["L. Cao", "Z. Li", "Y. Mu", "S.-F. Chang"], "venue": "Proceedings of the 20th ACM international conference on Multimedia, 2012, pp. 299\u2013308.", "citeRegEx": "127", "shortCiteRegEx": null, "year": 2012}, {"title": "Probabilistic attributed hashing", "author": ["M. Ou", "P. Cui", "J. Wang", "F. Wang", "W. Zhu"], "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015, pp. 2894\u20132900.", "citeRegEx": "128", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning binary hash codes for large-scale image search", "author": ["K. Grauman", "R. Fergus"], "venue": "Machine Learning for Computer Vision. Springer, 2013, pp. 49\u201387.", "citeRegEx": "129", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficiently searching for similar images", "author": ["K. Grauman"], "venue": "Commun. ACM, vol. 53, no. 6, 2010.", "citeRegEx": "130", "shortCiteRegEx": null, "year": 2010}, {"title": "Manhattan hashing for largescale image retrieval", "author": ["W. Kong", "W.-J. Li", "M. Guo"], "venue": "Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, 2012, pp. 45\u201354.", "citeRegEx": "131", "shortCiteRegEx": null, "year": 2012}, {"title": "Mobile product search with bag of hash bits and boundary reranking", "author": ["J. He", "J. Feng", "X. Liu", "T. Cheng", "T.-H. Lin", "H. Chung", "S.-F. Chang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, 2012, pp. 3005\u20133012.", "citeRegEx": "132", "shortCiteRegEx": null, "year": 2012}, {"title": "Coherency sensitive hashing", "author": ["S. Korman", "S. Avidan"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2011, pp. 1607\u20131614.", "citeRegEx": "133", "shortCiteRegEx": null, "year": 2011}, {"title": "Rapid face recognition using hashing", "author": ["Q. Shi", "H. Li", "C. Shen"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition, San Francisco, USA, 2010, pp. 2753\u20132760.", "citeRegEx": "134", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimal bayesian hashing for efficient face recognition", "author": ["Q. Dai", "J. Li", "J. Wang", "Y. Chen", "Y.-G. Jiang"], "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intellige, 2015, pp. 3430\u20133437.", "citeRegEx": "135", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning compact binary codes for visual tracking", "author": ["X. Li", "C. Shen", "A. Dick", "A. van den Hengel"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Portland, Oregon, USA, 2013, pp. 2419\u2013 2426.", "citeRegEx": "136", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient mining of repetitions in large-scale tv streams with product quantization hashing", "author": ["J. Yuan", "G. Gravier", "S. Campion", "X. Liu", "H. Jgou"], "venue": "European Conference on Computer Vision, Florence, Italy, 2012, pp. 271\u2013280.", "citeRegEx": "137", "shortCiteRegEx": null, "year": 2012}, {"title": "Detecting nearduplicates for web crawling", "author": ["G.S. Manku", "A. Jain", "A. Das Sarma"], "venue": "Proceedings of the 16th international conference on World Wide Web, 2007, pp. 141\u2013150.", "citeRegEx": "138", "shortCiteRegEx": null, "year": 2007}, {"title": "Data fusion through cross-modality metric learning using similarity-sensitive hashing", "author": ["M.M. Bronstein", "A.M. Bronstein", "F. Michel", "N. Paragios"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition, San Francisco, USA, 2010, pp. 3594\u20133601.", "citeRegEx": "139", "shortCiteRegEx": null, "year": 2010}, {"title": "Accelerated large scale optimization by concomitant hashing", "author": ["Y. Mu", "J. Wright", "S.-F. Chang"], "venue": "European Conference on Computer Vision, Florence, Italy, 2012, pp. 414\u2013427.", "citeRegEx": "140", "shortCiteRegEx": null, "year": 2012}, {"title": "Hashing algorithms for large-scale learning", "author": ["P. Li", "A. Shrivastava", "J.L. Moore", "A.C. Knig"], "venue": "Advances in Neural Information Processing Systems 24, J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Weinberger, Eds., 2011, pp. 2672\u20132680.", "citeRegEx": "141", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning binary codes for collaborative filtering", "author": ["K. Zhou", "H. Zha"], "venue": "Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2012, pp. 498\u2013506.", "citeRegEx": "142", "shortCiteRegEx": null, "year": 2012}, {"title": "Comparing apples to oranges: A scalable solution with heterogeneous hashing", "author": ["M. Ou", "P. Cui", "F. Wang", "J. Wang", "W. Zhu", "S. Yang"], "venue": "Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2013, pp. 230\u2013238.", "citeRegEx": "143", "shortCiteRegEx": null, "year": 2013}, {"title": "Large-scale video hashing via structure learning", "author": ["G. Ye", "D. Liu", "J. Wang", "S.-F. Chang"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013.", "citeRegEx": "144", "shortCiteRegEx": null, "year": 2013}, {"title": "Semantic hashing using tags and topic modeling", "author": ["Q. Wang", "D. Zhang", "L. Si"], "venue": "Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2013, pp. 213\u2013222.", "citeRegEx": "145", "shortCiteRegEx": null, "year": 2013}, {"title": "Two-stage hashing for fast document retrieval", "author": ["H. Li", "W. Liu", "H. Ji"], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics, Baltimore, Maryland, USA, 2014.", "citeRegEx": "146", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast pairwise query selection for large-scale active learning to rank", "author": ["B. Qian", "X. Wang", "J. Wang", "WeifengZhi", "H. Li", "I. Davidson"], "venue": "Proceedings of the IEEE International Conference on Data Mining, 2013.", "citeRegEx": "147", "shortCiteRegEx": null, "year": 2013}, {"title": "Hash-svm: Scalable kernel machines for large-scale visual classification", "author": ["Y. Mu", "G. Hua", "W. Fan", "S.-F. Chang"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Columbus, Ohio, USA, 2014, pp. 446\u2013451.", "citeRegEx": "148", "shortCiteRegEx": null, "year": 2014}, {"title": "Hash kernels for structured data", "author": ["Q. Shi", "J. Petterson", "G. Dror", "J. Langford", "A. Smola", "S. Vishwanathan"], "venue": "The Journal of Machine Learning Research, vol. 10, pp. 2615\u20132637, 2009.", "citeRegEx": "149", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Besides the widely used text-based commercial search engines such as Google and Bing, content-based image retrieval (CBIR) has attracted substantial attention in the past decade [1].", "startOffset": 178, "endOffset": 181}, {"referenceID": 1, "context": "Searching for similar data samples in a given database essentially relates to the fundamental problem of nearest neighbor search [2].", "startOffset": 129, "endOffset": 132}, {"referenceID": 2, "context": "Besides the scalability issue, most practical large-scale applications also suffer from the curse of dimensionality [3], since data under modern analytics usually contains thousands or even tens of thousands of dimensions, e.", "startOffset": 116, "endOffset": 119}, {"referenceID": 3, "context": "Tree-based indexing approaches, such as KD tree [4], ball tree [5], metric tree [6], and vantage point tree [7], have been popular during the past several decades.", "startOffset": 48, "endOffset": 51}, {"referenceID": 4, "context": "Tree-based indexing approaches, such as KD tree [4], ball tree [5], metric tree [6], and vantage point tree [7], have been popular during the past several decades.", "startOffset": 63, "endOffset": 66}, {"referenceID": 5, "context": "Tree-based indexing approaches, such as KD tree [4], ball tree [5], metric tree [6], and vantage point tree [7], have been popular during the past several decades.", "startOffset": 80, "endOffset": 83}, {"referenceID": 6, "context": "Tree-based indexing approaches, such as KD tree [4], ball tree [5], metric tree [6], and vantage point tree [7], have been popular during the past several decades.", "startOffset": 108, "endOffset": 111}, {"referenceID": 7, "context": "In addition, the performance of tree-based indexing methods dramatically degrades when handling high-dimensional data [8].", "startOffset": 118, "endOffset": 121}, {"referenceID": 8, "context": "More recently, product quantization techniques have been proposed to encode high-dimensional data vectors via subspace decomposition for efficient ANN search [9][10].", "startOffset": 158, "endOffset": 161}, {"referenceID": 9, "context": "More recently, product quantization techniques have been proposed to encode high-dimensional data vectors via subspace decomposition for efficient ANN search [9][10].", "startOffset": 161, "endOffset": 165}, {"referenceID": 10, "context": "For instance, 80 million tiny images (32\u02c6 32 pixels, double type) cost around 600G bytes [11], but can be compressed into 64-bit binary codes requiring only 600M bytes! In many cases, hash codes are organized into a hash table for inverse table lookup, as shown in Figure 1.", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "Among these methods, the randomized scheme of LocalitySensitive Hashing (LSH) is one of the most popular choices [13].", "startOffset": 113, "endOffset": 117}, {"referenceID": 12, "context": "Second, the theoretical guarantees of LSH only apply to certain metrics such as lp (p P p0,2s) and Jaccard [14].", "startOffset": 107, "endOffset": 111}, {"referenceID": 13, "context": "This discrepancy between semantic and metric spaces has been recognized in the computer vision and machine learning communities, namely as semantic gap [15].", "startOffset": 152, "endOffset": 156}, {"referenceID": 14, "context": "To tackle the aforementioned issues, many hashing methods have been proposed recently to leverage machine learning techniques to produce more effective hash codes [16].", "startOffset": 163, "endOffset": 167}, {"referenceID": 15, "context": "The goal of learning to hash is to learn datadependent and task-specific hash functions that yield compact binary codes to achieve good search accuracy [17].", "startOffset": 152, "endOffset": 156}, {"referenceID": 16, "context": "In order to achieve this goal, sophisticated machine learning tools and algorithms have been adapted to the procedure of hash function design, including the boosting algorithm [18], distance metric learning [19], asymmetric binary embedding [20], kernel methods [21][22], compressed sensing [23], maximum margin learning [24], sequential", "startOffset": 176, "endOffset": 180}, {"referenceID": 17, "context": "In order to achieve this goal, sophisticated machine learning tools and algorithms have been adapted to the procedure of hash function design, including the boosting algorithm [18], distance metric learning [19], asymmetric binary embedding [20], kernel methods [21][22], compressed sensing [23], maximum margin learning [24], sequential", "startOffset": 207, "endOffset": 211}, {"referenceID": 18, "context": "In order to achieve this goal, sophisticated machine learning tools and algorithms have been adapted to the procedure of hash function design, including the boosting algorithm [18], distance metric learning [19], asymmetric binary embedding [20], kernel methods [21][22], compressed sensing [23], maximum margin learning [24], sequential", "startOffset": 241, "endOffset": 245}, {"referenceID": 19, "context": "In order to achieve this goal, sophisticated machine learning tools and algorithms have been adapted to the procedure of hash function design, including the boosting algorithm [18], distance metric learning [19], asymmetric binary embedding [20], kernel methods [21][22], compressed sensing [23], maximum margin learning [24], sequential", "startOffset": 262, "endOffset": 266}, {"referenceID": 20, "context": "In order to achieve this goal, sophisticated machine learning tools and algorithms have been adapted to the procedure of hash function design, including the boosting algorithm [18], distance metric learning [19], asymmetric binary embedding [20], kernel methods [21][22], compressed sensing [23], maximum margin learning [24], sequential", "startOffset": 266, "endOffset": 270}, {"referenceID": 21, "context": "In order to achieve this goal, sophisticated machine learning tools and algorithms have been adapted to the procedure of hash function design, including the boosting algorithm [18], distance metric learning [19], asymmetric binary embedding [20], kernel methods [21][22], compressed sensing [23], maximum margin learning [24], sequential", "startOffset": 291, "endOffset": 295}, {"referenceID": 22, "context": "In order to achieve this goal, sophisticated machine learning tools and algorithms have been adapted to the procedure of hash function design, including the boosting algorithm [18], distance metric learning [19], asymmetric binary embedding [20], kernel methods [21][22], compressed sensing [23], maximum margin learning [24], sequential", "startOffset": 321, "endOffset": 325}, {"referenceID": 23, "context": "learning [25], clustering analysis [26], semi-supervised learning [14], supervised learning [27][22], graph learning [28], and so on.", "startOffset": 9, "endOffset": 13}, {"referenceID": 24, "context": "learning [25], clustering analysis [26], semi-supervised learning [14], supervised learning [27][22], graph learning [28], and so on.", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "learning [25], clustering analysis [26], semi-supervised learning [14], supervised learning [27][22], graph learning [28], and so on.", "startOffset": 66, "endOffset": 70}, {"referenceID": 25, "context": "learning [25], clustering analysis [26], semi-supervised learning [14], supervised learning [27][22], graph learning [28], and so on.", "startOffset": 92, "endOffset": 96}, {"referenceID": 20, "context": "learning [25], clustering analysis [26], semi-supervised learning [14], supervised learning [27][22], graph learning [28], and so on.", "startOffset": 96, "endOffset": 100}, {"referenceID": 26, "context": "learning [25], clustering analysis [26], semi-supervised learning [14], supervised learning [27][22], graph learning [28], and so on.", "startOffset": 117, "endOffset": 121}, {"referenceID": 10, "context": "Both the supervised and semi-supervised learning paradigms have been explored using such pairwise semantic relationships to learn semantically relevant hash functions [11][29][30][31].", "startOffset": 167, "endOffset": 171}, {"referenceID": 27, "context": "Both the supervised and semi-supervised learning paradigms have been explored using such pairwise semantic relationships to learn semantically relevant hash functions [11][29][30][31].", "startOffset": 171, "endOffset": 175}, {"referenceID": 28, "context": "Both the supervised and semi-supervised learning paradigms have been explored using such pairwise semantic relationships to learn semantically relevant hash functions [11][29][30][31].", "startOffset": 175, "endOffset": 179}, {"referenceID": 29, "context": "Both the supervised and semi-supervised learning paradigms have been explored using such pairwise semantic relationships to learn semantically relevant hash functions [11][29][30][31].", "startOffset": 179, "endOffset": 183}, {"referenceID": 30, "context": "For example, LSH keeps fp \u0308q to be an identity function, while shift-invariant kernel-based hashing and spectral hashing choose fp \u0308q to be a shifted cosine or sinusoidal function [32][33].", "startOffset": 180, "endOffset": 184}, {"referenceID": 31, "context": "For example, LSH keeps fp \u0308q to be an identity function, while shift-invariant kernel-based hashing and spectral hashing choose fp \u0308q to be a shifted cosine or sinusoidal function [32][33].", "startOffset": 184, "endOffset": 188}, {"referenceID": 32, "context": ", cosine similarity or Jaccard similarity [34].", "startOffset": 42, "endOffset": 46}, {"referenceID": 32, "context": "The random vectorw is constructed by sampling each component of w randomly from a standard Gaussian distribution for cosine distance [34].", "startOffset": 133, "endOffset": 137}, {"referenceID": 33, "context": "In particular, with hash codes of length K, it is required to construct a sufficient number of hash tables to ensure the desired performance bound [35].", "startOffset": 147, "endOffset": 151}, {"referenceID": 34, "context": "For instance, a self-tuning indexing technique, called LSH forest was proposed in [36], which aims at improving the performance without additional storage and query overhead.", "startOffset": 82, "endOffset": 86}, {"referenceID": 35, "context": "In [37][38], a technique called MultiProbe LSH was developed to reduce the number of required hash tables through intelligently probing multiple buckets in each hash table.", "startOffset": 3, "endOffset": 7}, {"referenceID": 36, "context": "In [37][38], a technique called MultiProbe LSH was developed to reduce the number of required hash tables through intelligently probing multiple buckets in each hash table.", "startOffset": 7, "endOffset": 11}, {"referenceID": 37, "context": "In [39], nonlinear randomized Hadamard transforms were explored to speed up the LSH based ANN search for Euclidean distance.", "startOffset": 3, "endOffset": 7}, {"referenceID": 38, "context": "In [40], BayesLSH was proposed to combine Bayesian inference with LSH in a principled manner, which has probabilistic guarantees on the quality of the search results in terms of accuracy and recall.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "In machine learning and data mining community, recent methods tend to leverage data-dependent and taskspecific information to improve the efficiency of random projection based hash functions [16].", "startOffset": 191, "endOffset": 195}, {"referenceID": 39, "context": "For example, incorporating kernel learning with LSH can help generalize ANN search from a standard metric space to a wide range of similarity functions [41][42].", "startOffset": 152, "endOffset": 156}, {"referenceID": 40, "context": "For example, incorporating kernel learning with LSH can help generalize ANN search from a standard metric space to a wide range of similarity functions [41][42].", "startOffset": 156, "endOffset": 160}, {"referenceID": 17, "context": "Furthermore, metric learning has been combined with randomized LSH functions to explore a set of pairwise similarity and dissimilarity constraints [19].", "startOffset": 147, "endOffset": 151}, {"referenceID": 41, "context": "Other variants of locality sensitive hashing techniques include super-bit LSH [43], boosted LSH [18], as well as non-metric LSH [44]", "startOffset": 78, "endOffset": 82}, {"referenceID": 16, "context": "Other variants of locality sensitive hashing techniques include super-bit LSH [43], boosted LSH [18], as well as non-metric LSH [44]", "startOffset": 96, "endOffset": 100}, {"referenceID": 42, "context": "Other variants of locality sensitive hashing techniques include super-bit LSH [43], boosted LSH [18], as well as non-metric LSH [44]", "startOffset": 128, "endOffset": 132}, {"referenceID": 43, "context": "A typical application is to index documents and then identify near-duplicate samples from a corpus of documents [45][46].", "startOffset": 112, "endOffset": 116}, {"referenceID": 44, "context": "A typical application is to index documents and then identify near-duplicate samples from a corpus of documents [45][46].", "startOffset": 116, "endOffset": 120}, {"referenceID": 45, "context": "Note that such a hash function holds a property that the chance of two sets having the same MinHash values is equal to the Jaccard similarity between them [47]", "startOffset": 155, "endOffset": 159}, {"referenceID": 46, "context": "12 still holds [48].", "startOffset": 15, "endOffset": 19}, {"referenceID": 47, "context": ", the min-hash approach outperforms other competing methods for the application of webpage duplicate detection [49].", "startOffset": 111, "endOffset": 115}, {"referenceID": 48, "context": "In addition, the minhash scheme is also applied for Google news personalization [50] and near duplicate image detection [51] [52].", "startOffset": 80, "endOffset": 84}, {"referenceID": 49, "context": "In addition, the minhash scheme is also applied for Google news personalization [50] and near duplicate image detection [51] [52].", "startOffset": 120, "endOffset": 124}, {"referenceID": 50, "context": "In addition, the minhash scheme is also applied for Google news personalization [50] and near duplicate image detection [51] [52].", "startOffset": 125, "endOffset": 129}, {"referenceID": 51, "context": "Some recent efforts have been made to further improve the min-hash technique, including b-bit minwise hashing [53] [54], one permutation approach [55], geometric min-Hashing [56], and a fast computing technique for image data [57].", "startOffset": 110, "endOffset": 114}, {"referenceID": 52, "context": "Some recent efforts have been made to further improve the min-hash technique, including b-bit minwise hashing [53] [54], one permutation approach [55], geometric min-Hashing [56], and a fast computing technique for image data [57].", "startOffset": 115, "endOffset": 119}, {"referenceID": 53, "context": "Some recent efforts have been made to further improve the min-hash technique, including b-bit minwise hashing [53] [54], one permutation approach [55], geometric min-Hashing [56], and a fast computing technique for image data [57].", "startOffset": 146, "endOffset": 150}, {"referenceID": 54, "context": "Some recent efforts have been made to further improve the min-hash technique, including b-bit minwise hashing [53] [54], one permutation approach [55], geometric min-Hashing [56], and a fast computing technique for image data [57].", "startOffset": 174, "endOffset": 178}, {"referenceID": 55, "context": "Some recent efforts have been made to further improve the min-hash technique, including b-bit minwise hashing [53] [54], one permutation approach [55], geometric min-Hashing [56], and a fast computing technique for image data [57].", "startOffset": 226, "endOffset": 230}, {"referenceID": 56, "context": "Since the proposal of LSH in [58], many new hashing techniques have been developed.", "startOffset": 29, "endOffset": 33}, {"referenceID": 29, "context": ", developed multiple complementary hash tables that are sequentially learned using a boosting-style algorithm [31].", "startOffset": 110, "endOffset": 114}, {"referenceID": 57, "context": "[59].", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "Based on the random projection scheme, there have been several efforts to improve the performance of the LSH method [37] [39] [41].", "startOffset": 116, "endOffset": 120}, {"referenceID": 37, "context": "Based on the random projection scheme, there have been several efforts to improve the performance of the LSH method [37] [39] [41].", "startOffset": 121, "endOffset": 125}, {"referenceID": 39, "context": "Based on the random projection scheme, there have been several efforts to improve the performance of the LSH method [37] [39] [41].", "startOffset": 126, "endOffset": 130}, {"referenceID": 30, "context": "Representative unsupervised methods include spectral hashing [32], graph hashing [28], manifold hashing [60], iterative quantization hashing [61], kernalized locality sensitive hashing [41][21], isotropic hashing [62], angular quantization hashing [63], and spherical hashing [64].", "startOffset": 61, "endOffset": 65}, {"referenceID": 26, "context": "Representative unsupervised methods include spectral hashing [32], graph hashing [28], manifold hashing [60], iterative quantization hashing [61], kernalized locality sensitive hashing [41][21], isotropic hashing [62], angular quantization hashing [63], and spherical hashing [64].", "startOffset": 81, "endOffset": 85}, {"referenceID": 58, "context": "Representative unsupervised methods include spectral hashing [32], graph hashing [28], manifold hashing [60], iterative quantization hashing [61], kernalized locality sensitive hashing [41][21], isotropic hashing [62], angular quantization hashing [63], and spherical hashing [64].", "startOffset": 104, "endOffset": 108}, {"referenceID": 59, "context": "Representative unsupervised methods include spectral hashing [32], graph hashing [28], manifold hashing [60], iterative quantization hashing [61], kernalized locality sensitive hashing [41][21], isotropic hashing [62], angular quantization hashing [63], and spherical hashing [64].", "startOffset": 141, "endOffset": 145}, {"referenceID": 39, "context": "Representative unsupervised methods include spectral hashing [32], graph hashing [28], manifold hashing [60], iterative quantization hashing [61], kernalized locality sensitive hashing [41][21], isotropic hashing [62], angular quantization hashing [63], and spherical hashing [64].", "startOffset": 185, "endOffset": 189}, {"referenceID": 19, "context": "Representative unsupervised methods include spectral hashing [32], graph hashing [28], manifold hashing [60], iterative quantization hashing [61], kernalized locality sensitive hashing [41][21], isotropic hashing [62], angular quantization hashing [63], and spherical hashing [64].", "startOffset": 189, "endOffset": 193}, {"referenceID": 60, "context": "Representative unsupervised methods include spectral hashing [32], graph hashing [28], manifold hashing [60], iterative quantization hashing [61], kernalized locality sensitive hashing [41][21], isotropic hashing [62], angular quantization hashing [63], and spherical hashing [64].", "startOffset": 213, "endOffset": 217}, {"referenceID": 61, "context": "Representative unsupervised methods include spectral hashing [32], graph hashing [28], manifold hashing [60], iterative quantization hashing [61], kernalized locality sensitive hashing [41][21], isotropic hashing [62], angular quantization hashing [63], and spherical hashing [64].", "startOffset": 248, "endOffset": 252}, {"referenceID": 62, "context": "Representative unsupervised methods include spectral hashing [32], graph hashing [28], manifold hashing [60], iterative quantization hashing [61], kernalized locality sensitive hashing [41][21], isotropic hashing [62], angular quantization hashing [63], and spherical hashing [64].", "startOffset": 276, "endOffset": 280}, {"referenceID": 17, "context": "In addition, supervised learning paradigms ranging from kernel learning to metric learning to deep learning have been exploited to learn binary codes, and many supervised hashing methods have been proposed recently [19][22][65][66][67][68].", "startOffset": 215, "endOffset": 219}, {"referenceID": 20, "context": "In addition, supervised learning paradigms ranging from kernel learning to metric learning to deep learning have been exploited to learn binary codes, and many supervised hashing methods have been proposed recently [19][22][65][66][67][68].", "startOffset": 219, "endOffset": 223}, {"referenceID": 63, "context": "In addition, supervised learning paradigms ranging from kernel learning to metric learning to deep learning have been exploited to learn binary codes, and many supervised hashing methods have been proposed recently [19][22][65][66][67][68].", "startOffset": 223, "endOffset": 227}, {"referenceID": 64, "context": "In addition, supervised learning paradigms ranging from kernel learning to metric learning to deep learning have been exploited to learn binary codes, and many supervised hashing methods have been proposed recently [19][22][65][66][67][68].", "startOffset": 227, "endOffset": 231}, {"referenceID": 65, "context": "In addition, supervised learning paradigms ranging from kernel learning to metric learning to deep learning have been exploited to learn binary codes, and many supervised hashing methods have been proposed recently [19][22][65][66][67][68].", "startOffset": 231, "endOffset": 235}, {"referenceID": 66, "context": "In addition, supervised learning paradigms ranging from kernel learning to metric learning to deep learning have been exploited to learn binary codes, and many supervised hashing methods have been proposed recently [19][22][65][66][67][68].", "startOffset": 235, "endOffset": 239}, {"referenceID": 12, "context": "al proposed a regularized objective to achieve accurate yet balanced hash codes to avoid overfitting [14].", "startOffset": 101, "endOffset": 105}, {"referenceID": 67, "context": "In [69][70], authors proposed to exploit the metric learning and locality sensitive hashing to achieve fast similarity based search.", "startOffset": 3, "endOffset": 7}, {"referenceID": 68, "context": "In [69][70], authors proposed to exploit the metric learning and locality sensitive hashing to achieve fast similarity based search.", "startOffset": 7, "endOffset": 11}, {"referenceID": 69, "context": "For example, a few existing approaches utilize the instance level semantic attributes or labels to design the hash functions [71][18][72].", "startOffset": 125, "endOffset": 129}, {"referenceID": 16, "context": "For example, a few existing approaches utilize the instance level semantic attributes or labels to design the hash functions [71][18][72].", "startOffset": 129, "endOffset": 133}, {"referenceID": 70, "context": "For example, a few existing approaches utilize the instance level semantic attributes or labels to design the hash functions [71][18][72].", "startOffset": 133, "endOffset": 137}, {"referenceID": 17, "context": "Additionally, learning methods based on pairwise supervision have been extensively studied, and many hashing techniques have been proposed [19][22][65][66][14][69][70][73].", "startOffset": 139, "endOffset": 143}, {"referenceID": 20, "context": "Additionally, learning methods based on pairwise supervision have been extensively studied, and many hashing techniques have been proposed [19][22][65][66][14][69][70][73].", "startOffset": 143, "endOffset": 147}, {"referenceID": 63, "context": "Additionally, learning methods based on pairwise supervision have been extensively studied, and many hashing techniques have been proposed [19][22][65][66][14][69][70][73].", "startOffset": 147, "endOffset": 151}, {"referenceID": 64, "context": "Additionally, learning methods based on pairwise supervision have been extensively studied, and many hashing techniques have been proposed [19][22][65][66][14][69][70][73].", "startOffset": 151, "endOffset": 155}, {"referenceID": 12, "context": "Additionally, learning methods based on pairwise supervision have been extensively studied, and many hashing techniques have been proposed [19][22][65][66][14][69][70][73].", "startOffset": 155, "endOffset": 159}, {"referenceID": 67, "context": "Additionally, learning methods based on pairwise supervision have been extensively studied, and many hashing techniques have been proposed [19][22][65][66][14][69][70][73].", "startOffset": 159, "endOffset": 163}, {"referenceID": 68, "context": "Additionally, learning methods based on pairwise supervision have been extensively studied, and many hashing techniques have been proposed [19][22][65][66][14][69][70][73].", "startOffset": 163, "endOffset": 167}, {"referenceID": 71, "context": "Additionally, learning methods based on pairwise supervision have been extensively studied, and many hashing techniques have been proposed [19][22][65][66][14][69][70][73].", "startOffset": 167, "endOffset": 171}, {"referenceID": 72, "context": "More recently, a triplet ranking that encodes the pairwise proximity comparison among three data points is exploited to design hash codes [74][65][75].", "startOffset": 138, "endOffset": 142}, {"referenceID": 63, "context": "More recently, a triplet ranking that encodes the pairwise proximity comparison among three data points is exploited to design hash codes [74][65][75].", "startOffset": 142, "endOffset": 146}, {"referenceID": 73, "context": "More recently, a triplet ranking that encodes the pairwise proximity comparison among three data points is exploited to design hash codes [74][65][75].", "startOffset": 146, "endOffset": 150}, {"referenceID": 74, "context": "By converting rank lists to a triplet tensor matrix, listwise hashing is designed to preserve the ranking in the Hamming space [76].", "startOffset": 127, "endOffset": 131}, {"referenceID": 61, "context": "For instance, PCA hashing performs principal component analysis on the data to derive large variance projections [63][77][78], as shown in Figure 5(a).", "startOffset": 113, "endOffset": 117}, {"referenceID": 75, "context": "For instance, PCA hashing performs principal component analysis on the data to derive large variance projections [63][77][78], as shown in Figure 5(a).", "startOffset": 117, "endOffset": 121}, {"referenceID": 76, "context": "For instance, PCA hashing performs principal component analysis on the data to derive large variance projections [63][77][78], as shown in Figure 5(a).", "startOffset": 121, "endOffset": 125}, {"referenceID": 77, "context": "In the same league, supervised methods have used Linear Discriminant Analysis to design more discriminative hash codes [79][80].", "startOffset": 119, "endOffset": 123}, {"referenceID": 78, "context": "In the same league, supervised methods have used Linear Discriminant Analysis to design more discriminative hash codes [79][80].", "startOffset": 123, "endOffset": 127}, {"referenceID": 12, "context": "Semi-supervised hashing methods estimate the projections that have minimum empirical loss on pair-wise labels while partitioning the unlabeled data in a balanced way [14].", "startOffset": 166, "endOffset": 170}, {"referenceID": 12, "context": "Two types of solutions based on relaxation of the orthogonality constraints or random/learned rotation of the data have been proposed in the literature to address these issues [14][61].", "startOffset": 176, "endOffset": 180}, {"referenceID": 59, "context": "Two types of solutions based on relaxation of the orthogonality constraints or random/learned rotation of the data have been proposed in the literature to address these issues [14][61].", "startOffset": 180, "endOffset": 184}, {"referenceID": 60, "context": "Isotropic hashing is proposed to derive projections with equal variances and is shown to be superior to anisotropic variances based projections [62].", "startOffset": 144, "endOffset": 148}, {"referenceID": 23, "context": "Instead of performing one-shot learning, sequential projection learning derives correlated projections with the goal of correcting errors from previous hash bits [25].", "startOffset": 162, "endOffset": 166}, {"referenceID": 79, "context": "Finally, to reduce the computational complexity of full projection, circulant binary embedding was recently proposed to significantly speed up the encoding process using the circulant convolution [81].", "startOffset": 196, "endOffset": 200}, {"referenceID": 31, "context": "In addition, shift-invariant kernel-based hashing chooses fp \u0308q to be a shifted cosine function and samples the projection vector in the same way as standard LSH does [33].", "startOffset": 167, "endOffset": 171}, {"referenceID": 19, "context": "Another category of nonlinear hashing techniques employs kernel functions [21][22][28][82].", "startOffset": 74, "endOffset": 78}, {"referenceID": 20, "context": "Another category of nonlinear hashing techniques employs kernel functions [21][22][28][82].", "startOffset": 78, "endOffset": 82}, {"referenceID": 26, "context": "Another category of nonlinear hashing techniques employs kernel functions [21][22][28][82].", "startOffset": 82, "endOffset": 86}, {"referenceID": 80, "context": "Another category of nonlinear hashing techniques employs kernel functions [21][22][28][82].", "startOffset": 86, "endOffset": 90}, {"referenceID": 26, "context": "[28] uses a kernel function to measure similarity of each points with a set of anchors resulting in nonlinear hashing.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Kernerlized LSH uses a sparse set of datapoints to compute a kernel matrix and preform random projection in the kernel space to compute binary codes [21].", "startOffset": 149, "endOffset": 153}, {"referenceID": 25, "context": "Based on similar representation of kernel metric, Kulis and Darrell propose learning of hash functions by explicitly minimizing the reconstruction error in the kernel space and Hamming space [27].", "startOffset": 191, "endOffset": 195}, {"referenceID": 20, "context": "applies kernel representation but optimizes the hash functions by exploring the equivalence between optimizing the code inner products and the Hamming distances to achieve scale invariance [22].", "startOffset": 189, "endOffset": 193}, {"referenceID": 81, "context": "Such a procedure sequentially trains hash functions one bit at a time [83][25][84].", "startOffset": 70, "endOffset": 74}, {"referenceID": 23, "context": "Such a procedure sequentially trains hash functions one bit at a time [83][25][84].", "startOffset": 74, "endOffset": 78}, {"referenceID": 82, "context": "Such a procedure sequentially trains hash functions one bit at a time [83][25][84].", "startOffset": 78, "endOffset": 82}, {"referenceID": 23, "context": "For instance, the sequential projection based hashing aims to incorporate the bit correlations by iteratively updating the pairwise label matrix, where higher weights are imposed on point pairs violated by the previous hash functions [25].", "startOffset": 234, "endOffset": 238}, {"referenceID": 82, "context": "In the complementary projection learning approach [84], the authors present a sequential learning procedure to obtain a series of hash functions that cross the sparse data region, as well as generate balanced hash buckets.", "startOffset": 50, "endOffset": 54}, {"referenceID": 83, "context": "Other interesting learning ideas include twostep learning methods which treat hash bit learning and hash function learning separately [85][86].", "startOffset": 134, "endOffset": 138}, {"referenceID": 84, "context": "Other interesting learning ideas include twostep learning methods which treat hash bit learning and hash function learning separately [85][86].", "startOffset": 138, "endOffset": 142}, {"referenceID": 12, "context": "However, it is easy to observe that different bits often behave differently [14][32].", "startOffset": 76, "endOffset": 80}, {"referenceID": 30, "context": "However, it is easy to observe that different bits often behave differently [14][32].", "startOffset": 80, "endOffset": 84}, {"referenceID": 16, "context": "One of the representative approaches is Boosted Similarity Sensitive Coding (BSSC) [18].", "startOffset": 83, "endOffset": 87}, {"referenceID": 85, "context": "If one treats each hash function as a decision stump, the straightforward way of learning the weights is to directly apply adaptive boosting algorithm [87] as described in [18].", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "If one treats each hash function as a decision stump, the straightforward way of learning the weights is to directly apply adaptive boosting algorithm [87] as described in [18].", "startOffset": 172, "endOffset": 176}, {"referenceID": 86, "context": "In [88], a boosting-style method called BoostMAP is proposed to map data points to weighted binary vectors that can leverage both metric and semantic similarity measures.", "startOffset": 3, "endOffset": 7}, {"referenceID": 72, "context": "Other weighted hashing methods include designing specific bit-level weighting schemes to improve the search accuracy [74][89][90][91][92].", "startOffset": 117, "endOffset": 121}, {"referenceID": 87, "context": "Other weighted hashing methods include designing specific bit-level weighting schemes to improve the search accuracy [74][89][90][91][92].", "startOffset": 121, "endOffset": 125}, {"referenceID": 88, "context": "Other weighted hashing methods include designing specific bit-level weighting schemes to improve the search accuracy [74][89][90][91][92].", "startOffset": 125, "endOffset": 129}, {"referenceID": 89, "context": "Other weighted hashing methods include designing specific bit-level weighting schemes to improve the search accuracy [74][89][90][91][92].", "startOffset": 129, "endOffset": 133}, {"referenceID": 90, "context": "Other weighted hashing methods include designing specific bit-level weighting schemes to improve the search accuracy [74][89][90][91][92].", "startOffset": 133, "endOffset": 137}, {"referenceID": 91, "context": "In addition, a recent work about designing a unified bit selection framework can be regarded as a special case of weighted hashing approach, where the weights of hash bits are binary [93].", "startOffset": 183, "endOffset": 187}, {"referenceID": 92, "context": "Another effective hash code ranking method is the query-sensitive hashing, which explores the raw feature of the query sample and learns query-specific weights of hash bits to achieve accurate \u01ebnearest neighbor search [94].", "startOffset": 218, "endOffset": 222}, {"referenceID": 12, "context": "In general, the nonlinear and supervised techniques tend to generate better performance than linear and unsupervised methods, while being more computationally costly [14][19][21][22][27].", "startOffset": 166, "endOffset": 170}, {"referenceID": 17, "context": "In general, the nonlinear and supervised techniques tend to generate better performance than linear and unsupervised methods, while being more computationally costly [14][19][21][22][27].", "startOffset": 170, "endOffset": 174}, {"referenceID": 19, "context": "In general, the nonlinear and supervised techniques tend to generate better performance than linear and unsupervised methods, while being more computationally costly [14][19][21][22][27].", "startOffset": 174, "endOffset": 178}, {"referenceID": 20, "context": "In general, the nonlinear and supervised techniques tend to generate better performance than linear and unsupervised methods, while being more computationally costly [14][19][21][22][27].", "startOffset": 178, "endOffset": 182}, {"referenceID": 25, "context": "In general, the nonlinear and supervised techniques tend to generate better performance than linear and unsupervised methods, while being more computationally costly [14][19][21][22][27].", "startOffset": 182, "endOffset": 186}, {"referenceID": 30, "context": "In the formulation of spectral hashing, the desired properties include keeping neighbors in input space as neighbors in the hamming space and requiring the codes to be balanced and uncorrelated [32].", "startOffset": 194, "endOffset": 198}, {"referenceID": 93, "context": "Motivated by the wellknown spectral graph analysis [95], the authors suggest to minimize the cost function with relaxed constraints.", "startOffset": 51, "endOffset": 55}, {"referenceID": 30, "context": "In particular, with the assumption of uniform data distribution, the spectral solution can be efficiently computed using 1D-Laplacian eigenfunctions [32].", "startOffset": 149, "endOffset": 153}, {"referenceID": 94, "context": "Hence, a \u201ckernel trick\u201d is used to alleviate the degraded performance when using long hash bits [96].", "startOffset": 96, "endOffset": 100}, {"referenceID": 26, "context": "Following the similar objective as spectral hashing, anchor graph hashing was designed to solve the problem from a different perspective without the assumption of uniform distribution [28].", "startOffset": 184, "endOffset": 188}, {"referenceID": 95, "context": "The key idea is to use a small set of MpM !Nq anchor points to approximate the graph structure represented by the matrix A such that the similarity between any pair of points can be approximated using point-to-anchor similarities [97].", "startOffset": 230, "endOffset": 234}, {"referenceID": 26, "context": "R M\u02c6K and \u03a3 \u201c diagp\u03c31,  \u0308  \u0308  \u0308 , \u03c3k,  \u0308  \u0308  \u0308 , \u03c3Kq P R K\u02c6K , where tvk,\u03c3ku are the eigenvector-eigenvalue pairs [28].", "startOffset": 114, "endOffset": 118}, {"referenceID": 96, "context": "The anchor graph hashing approach was recently further improved by leveraging a discrete optimization technique to directly solve binary hash codes without any relaxation [98].", "startOffset": 171, "endOffset": 175}, {"referenceID": 61, "context": "Since similarity is often measured by the cosine of the angle between pairs of samples, angular quantization is thus proposed to map non-negative feature vectors onto a vertex of the binary hypercube with the smallest angle [63].", "startOffset": 224, "endOffset": 228}, {"referenceID": 61, "context": "method [63].", "startOffset": 7, "endOffset": 11}, {"referenceID": 61, "context": "b4 \u201c r0 1 1sJ in the illustrated example [63].", "startOffset": 41, "endOffset": 45}, {"referenceID": 12, "context": "Note that the above formulation still generates a D-bit binary code for each data point, while compact codes are often desired in many real-world applications [14].", "startOffset": 159, "endOffset": 163}, {"referenceID": 61, "context": "To generate a K=bit code, a projection matrix S P R with orthogonal columns can be used to replace the rotation matrix R in the above objective with additional normalization, as discussed in [63].", "startOffset": 191, "endOffset": 195}, {"referenceID": 25, "context": "Instead of using data-independent random projections as in LSH or principal components as in SH, Kulis and Darrell [27] proposed data-dependent and bit-correlated hash functions as:", "startOffset": 115, "endOffset": 119}, {"referenceID": 17, "context": "figure in [19]).", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "proposed to utilize the equivalence between code inner products and the Hamming distances to design supervised and kernel-based hash functions [22].", "startOffset": 143, "endOffset": 147}, {"referenceID": 20, "context": "Such a strategy of optimizing the hash code inner product in KSH rather than the Hamming distance like what\u2019s done in BRE pays off nicely and leads to major performance gains in similarity-based retrieval consistently confirmed in extensive experiments reported in [22] and recent studies [99].", "startOffset": 265, "endOffset": 269}, {"referenceID": 97, "context": "Such a strategy of optimizing the hash code inner product in KSH rather than the Hamming distance like what\u2019s done in BRE pays off nicely and leads to major performance gains in similarity-based retrieval consistently confirmed in extensive experiments reported in [22] and recent studies [99].", "startOffset": 289, "endOffset": 293}, {"referenceID": 17, "context": "Such learned metrics are then employed to the standard random projection based hash functions [19].", "startOffset": 94, "endOffset": 98}, {"referenceID": 98, "context": "Note that the matrix M can be learned through various metric learning method such as information-theoretic metric learning [100].", "startOffset": 123, "endOffset": 128}, {"referenceID": 68, "context": "Realizing that the pairwise constraints often come to be available incrementally, Jain et al exploit an efficient online locality-sensitive hashing with gradually learned distance metrics [70].", "startOffset": 188, "endOffset": 192}, {"referenceID": 12, "context": "The above objective can be solved using various optimization strategies, resulting in orthogonal or correlated binary codes, as described in [14][25].", "startOffset": 141, "endOffset": 145}, {"referenceID": 23, "context": "The above objective can be solved using various optimization strategies, resulting in orthogonal or correlated binary codes, as described in [14][25].", "startOffset": 145, "endOffset": 149}, {"referenceID": 29, "context": "employs similar semi-supervised formulation to sequentially learn multiple complementary hash tables to further improve the performance [31].", "startOffset": 136, "endOffset": 140}, {"referenceID": 72, "context": "As one of the representative methods falling into this category, column generation hashing explores the largemargin framework to leverage such type of proximity comparison information to design weighted hash functions [74].", "startOffset": 218, "endOffset": 222}, {"referenceID": 72, "context": "Different from other methods that explore the triplet relationship [74][75][65], the ranking supervised hashing method attempt to preserve the ranking order of a set of database points corresponding to the query point [76].", "startOffset": 67, "endOffset": 71}, {"referenceID": 73, "context": "Different from other methods that explore the triplet relationship [74][75][65], the ranking supervised hashing method attempt to preserve the ranking order of a set of database points corresponding to the query point [76].", "startOffset": 71, "endOffset": 75}, {"referenceID": 63, "context": "Different from other methods that explore the triplet relationship [74][75][65], the ranking supervised hashing method attempt to preserve the ranking order of a set of database points corresponding to the query point [76].", "startOffset": 75, "endOffset": 79}, {"referenceID": 74, "context": "Different from other methods that explore the triplet relationship [74][75][65], the ranking supervised hashing method attempt to preserve the ranking order of a set of database points corresponding to the query point [76].", "startOffset": 218, "endOffset": 222}, {"referenceID": 74, "context": "The augmented Lagrangian multiplier method was introduced to derive feasible solutions for the above constrained problem, as discussed in [76].", "startOffset": 138, "endOffset": 142}, {"referenceID": 79, "context": "Realizing that most of the current hashing techniques rely on linear projections, which could suffer from very high computational and storage costs for high-dimensional data, circulant binary embedding was recently developed to handle such a challenge using the circulant projection [81].", "startOffset": 283, "endOffset": 287}, {"referenceID": 99, "context": "Briefly, given a vector r \u201c tr0,  \u0308  \u0308  \u0308 , rd \u03011u, we can generate its corresponding circulant matrix R \u201c circprq [101].", "startOffset": 115, "endOffset": 120}, {"referenceID": 100, "context": "During the past decade (since around 2006), Deep Learning [102], also known as Deep Neural Networks, has drawn increasing attention and research efforts in a variety of artificial intelligence areas including speech recognition, computer vision, machine learning, text mining, etc.", "startOffset": 58, "endOffset": 63}, {"referenceID": 101, "context": "The earliest work in deep learning based hashing may be Semantic Hashing [103].", "startOffset": 73, "endOffset": 78}, {"referenceID": 102, "context": "Such a deep model is made as a stack of Restricted Boltzmann Machines (RBMs) [104].", "startOffset": 77, "endOffset": 82}, {"referenceID": 64, "context": "To enhance the performance of deep RBMs, a supervised version was proposed in [66], which borrows the idea of nonlinear Neighbourhood Component Analysis (NCA) embedding [105].", "startOffset": 78, "endOffset": 82}, {"referenceID": 103, "context": "To enhance the performance of deep RBMs, a supervised version was proposed in [66], which borrows the idea of nonlinear Neighbourhood Component Analysis (NCA) embedding [105].", "startOffset": 169, "endOffset": 174}, {"referenceID": 64, "context": "In [66], supervised deep RBMs using a Gaussian distribution to model visible units in the first layer were successfully applied to handle massive image data.", "startOffset": 3, "endOffset": 7}, {"referenceID": 97, "context": "A recent work named Sparse Similarity-Preserving Hashing [99] tried to address the low recall issue pertaining to relatively long hash codes, which affect most of previous hashing techniques.", "startOffset": 57, "endOffset": 61}, {"referenceID": 108, "context": "Within this architecture, two ISTA-type networks [110] that share the same set of parameters and conduct fast approximations of sparse coding are coupled in the training phase.", "startOffset": 49, "endOffset": 54}, {"referenceID": 97, "context": "In [99], an extension to hashing multimodal data, e.", "startOffset": 3, "endOffset": 7}, {"referenceID": 104, "context": "Another work named Deep Hashing [106] developed a deep neural network to learn a multiple hierarchical nonlinear transformation which maps original images to compact binary hash codes and hence supports large-scale image retrieval with the learned binary image representation.", "startOffset": 32, "endOffset": 37}, {"referenceID": 109, "context": "Similar constraints have been adopted in prior unsupervised hashing or binary coding methods such as Iterative Quantization (ITQ) [111].", "startOffset": 130, "endOffset": 135}, {"referenceID": 104, "context": "A supervised version called Supervised Deep Hashing was also presented in [106], where a discriminative term incorporating pairwise supervised information is added to the objective function of the deep hashing model.", "startOffset": 74, "endOffset": 79}, {"referenceID": 104, "context": "The authors of [106] showed the superiority of the supervised deep hashing model over its unsupervised counterpart.", "startOffset": 15, "endOffset": 20}, {"referenceID": 20, "context": "Specifically, the Sparse Similarity-Preserving Hashing method is found to be inferior to the state-of-the-art supervised hashing method Kernel-Based Supervised Hashing (KSH) [22] in terms of search accuracy on some image datasets [99]; the Deep Hashing method and its supervised version are slightly better than ITQ and its supervised version CCA+ITQ, respectively [111][106].", "startOffset": 174, "endOffset": 178}, {"referenceID": 97, "context": "Specifically, the Sparse Similarity-Preserving Hashing method is found to be inferior to the state-of-the-art supervised hashing method Kernel-Based Supervised Hashing (KSH) [22] in terms of search accuracy on some image datasets [99]; the Deep Hashing method and its supervised version are slightly better than ITQ and its supervised version CCA+ITQ, respectively [111][106].", "startOffset": 230, "endOffset": 234}, {"referenceID": 109, "context": "Specifically, the Sparse Similarity-Preserving Hashing method is found to be inferior to the state-of-the-art supervised hashing method Kernel-Based Supervised Hashing (KSH) [22] in terms of search accuracy on some image datasets [99]; the Deep Hashing method and its supervised version are slightly better than ITQ and its supervised version CCA+ITQ, respectively [111][106].", "startOffset": 365, "endOffset": 370}, {"referenceID": 104, "context": "Specifically, the Sparse Similarity-Preserving Hashing method is found to be inferior to the state-of-the-art supervised hashing method Kernel-Based Supervised Hashing (KSH) [22] in terms of search accuracy on some image datasets [99]; the Deep Hashing method and its supervised version are slightly better than ITQ and its supervised version CCA+ITQ, respectively [111][106].", "startOffset": 370, "endOffset": 375}, {"referenceID": 105, "context": "To remove this barrier, a recent method called Convolutional Neural Network Hashing [107] was developed to integrate image feature learning and hash value learning into a joint learning model.", "startOffset": 84, "endOffset": 89}, {"referenceID": 110, "context": "Given pairwise supervised information, this model consists of a stage of learning approximate hash codes and a stage of training a deep Convolutional Neural Network (CNN) [112] that outputs continuous hash values.", "startOffset": 171, "endOffset": 176}, {"referenceID": 106, "context": "Also based on CNNs, a latest method called as Deep Semantic Ranking Hashing [108] was presented to learn hash values such that multilevel semantic similarities among multi-labeled images are preserved.", "startOffset": 76, "endOffset": 81}, {"referenceID": 101, "context": "Semantic Hashing [103] text unsupervised no 4", "startOffset": 17, "endOffset": 22}, {"referenceID": 64, "context": "Restricted Boltzmann Machine [66] text and image supervised no 4 and 5", "startOffset": 29, "endOffset": 33}, {"referenceID": 97, "context": "Tailored Feed-Forward Neural Network [99] text and image supervised no 6", "startOffset": 37, "endOffset": 41}, {"referenceID": 104, "context": "Deep Hashing [106] image unsupervised no 3", "startOffset": 13, "endOffset": 18}, {"referenceID": 104, "context": "Supervised Deep Hashing [106] image supervised no 3", "startOffset": 24, "endOffset": 29}, {"referenceID": 105, "context": "Convolutional Neural Network Hashing [107] image supervised yes 5", "startOffset": 37, "endOffset": 42}, {"referenceID": 106, "context": "Deep Semantic Ranking Hashing [108] image supervised yes 8", "startOffset": 30, "endOffset": 35}, {"referenceID": 107, "context": "Deep Neural Network Hashing [109] image supervised yes 10", "startOffset": 28, "endOffset": 33}, {"referenceID": 105, "context": "The above Convolutional Neural Network Hashing method [107] requires separately learning approximate hash codes to guide the subsequent learning of image representation and finer hash values.", "startOffset": 54, "endOffset": 59}, {"referenceID": 107, "context": "A latest method called Deep Neural Network Hashing [109] goes beyond, in which the image representation and hash values are learned in one stage so that representation learning and hash learning are tightly coupled to benefit each other.", "startOffset": 51, "endOffset": 56}, {"referenceID": 106, "context": "Similar to the Deep Semantic Ranking Hashing method [108], the Deep Neural Network Hashing method incorporates listwise supervised information to train a deep CNN, giving rise to a currently deepest architecture for supervised hashing.", "startOffset": 52, "endOffset": 57}, {"referenceID": 107, "context": "In [109], the Deep Neural Network Hashing method was shown to surpass the Convolutional Neural Network Hashing method as well as several shallow learning based supervised hashing methods in terms of image search accuracy.", "startOffset": 3, "endOffset": 8}, {"referenceID": 111, "context": "The latest work [113] presented a hashing trick named HashedNets, which shrinks the storage costs of neural networks significantly while mostly preserving the generalization performance in image classification tasks.", "startOffset": 16, "endOffset": 21}, {"referenceID": 112, "context": "Hyperplane hashing is actually fairly important for many machine learning applications such as large-scale active learning with SVMs [114].", "startOffset": 133, "endOffset": 138}, {"referenceID": 113, "context": "In SVM-based active learning [115], the well proven sample selection strategy is to search in the unlabeled sample pool to identify the sample closest to the current hyperplane decision boundary, thus providing the most useful information for improving the learning model.", "startOffset": 29, "endOffset": 34}, {"referenceID": 114, "context": "The existing hyperplane hashing methods [116][117] all attempt to minimize a slightly modified \u201cdistance\u201d |wx| }w}}x} , i.", "startOffset": 40, "endOffset": 45}, {"referenceID": 115, "context": "The existing hyperplane hashing methods [116][117] all attempt to minimize a slightly modified \u201cdistance\u201d |wx| }w}}x} , i.", "startOffset": 45, "endOffset": 50}, {"referenceID": 115, "context": "[117] devised two different families of randomized hash functions to attack the hyperplane hashing problem.", "startOffset": 0, "endOffset": 5}, {"referenceID": 115, "context": "[117] designed a randomized function family with bilinear Bilinear-Hyperplane Hash (BH-Hash) as:", "startOffset": 0, "endOffset": 5}, {"referenceID": 115, "context": "proved in [117] that the probability of collision for a hyperplane query Pw and a database point x under h is", "startOffset": 10, "endOffset": 15}, {"referenceID": 116, "context": "extended the bilinear formulation to the conventional point-to-point hashing scheme through designing compact binary codes for highdimensional visual descriptors [118].", "startOffset": 162, "endOffset": 167}, {"referenceID": 117, "context": "Beyond the aforementioned conventional hashing which tackles searching in a database of vectors, subspace hashing [119], which has been rarely explored in the literature, attempts to efficiently search through a large database of subspaces.", "startOffset": 114, "endOffset": 119}, {"referenceID": 118, "context": "A common use scenario is to use a single face image to find the subspace (and the corresponding subject ID) closest to the query image [120].", "startOffset": 135, "endOffset": 140}, {"referenceID": 118, "context": "Given a query in the form of vector or subspace, searching for a nearest subspace in a subspace database is frequently encountered in a variety of practical applications including example-based image synthesis, scene classification, speaker recognition, face recognition, and motion-based action recognition [120].", "startOffset": 308, "endOffset": 313}, {"referenceID": 117, "context": "[119] presented a general framework to the problem of Approximate Nearest Subspace (ANS) search, which uniformly deals with the cases that query is a vector or subspace, query and database elements are subspaces of fixed dimension, query and database elements are subspaces of different dimension, and database elements are subspaces of varying di-", "startOffset": 0, "endOffset": 5}, {"referenceID": 117, "context": "The critical technique exploited by [119] is twostep: 1) a simple mapping that maps both query and database elements to \u201cpoints\u201d in a new vector space, and 2) doing approximate nearest neighbor search using conventional vector hashing algorithms in the new space.", "startOffset": 36, "endOffset": 41}, {"referenceID": 117, "context": "Consequently, the main contribution of [119] is reducing the difficult subspace hashing problem to a regular vector hashing task.", "startOffset": 39, "endOffset": 44}, {"referenceID": 117, "context": "[119] used LSH for the vector hashing task.", "startOffset": 0, "endOffset": 5}, {"referenceID": 117, "context": "While simple, the hashing technique (mapping + LSH) of [119] perhaps suffers from the high dimensionality of the constructed new vector space.", "startOffset": 55, "endOffset": 60}, {"referenceID": 118, "context": "More recently, [120] exclusively addressed the point-tosubspace query where query is a vector and database items are subspaces of arbitrary dimension.", "startOffset": 15, "endOffset": 20}, {"referenceID": 118, "context": "[120] proposed a rigorously faster hashing technique than that of [119].", "startOffset": 0, "endOffset": 5}, {"referenceID": 117, "context": "[120] proposed a rigorously faster hashing technique than that of [119].", "startOffset": 66, "endOffset": 71}, {"referenceID": 117, "context": "Its hash function can hash D-dimensional vectors (D is the ambient dimension of the query) or D\u02c6r-dimensional subspaces (r is arbitrary) in a linear time complexity OpDq, which is computationally more efficient than the hash functions devised in [119].", "startOffset": 246, "endOffset": 251}, {"referenceID": 118, "context": "[120] further proved the search time under the OpDq hashes to be sublinear in the database size.", "startOffset": 0, "endOffset": 5}, {"referenceID": 118, "context": "Based on the nice finding of [120], we would like to achieve faster hashing for the subspace-to-subspace query by means of crafted novel hash functions to handle subspaces in varying dimension.", "startOffset": 29, "endOffset": 34}, {"referenceID": 119, "context": "Some recent advanced methods are proposed to design the hash functions for more complex settings, such as that the data are represented by multimodal features or the data are formed in a heterogeneous way [121].", "startOffset": 205, "endOffset": 210}, {"referenceID": 120, "context": "Realizing that data items like webpage can be described from multiple information sources, composing hashing was recently proposed to design hashing schme using several information sources [122].", "startOffset": 189, "endOffset": 194}, {"referenceID": 121, "context": "Co-regularized hashing was proposed to investigate the hashing learning across multiparity data in a supervised setting, where similar and dissimilar pairs of intra-modality points are given as supervision information [123].", "startOffset": 218, "endOffset": 223}, {"referenceID": 122, "context": "Dual-view hashing attempts to derive a hidden common Hamming embedding of data from two views, while maintaining the predictability of the binary codes [124].", "startOffset": 152, "endOffset": 157}, {"referenceID": 123, "context": "A probabilistic model called multimodal latent binary embedding was recently presented to derive binary latent factors in a common Hamming space for indexing multimodal data [125].", "startOffset": 174, "endOffset": 179}, {"referenceID": 124, "context": "Other closely related hashing methods include the design of multiple feature hashing for near-duplicate duplicate detection [126], submodular hashing for video indexing [127], and Probabilistic Attributed Hashing for integrating low-level features and semantic attributes [128].", "startOffset": 124, "endOffset": 129}, {"referenceID": 125, "context": "Other closely related hashing methods include the design of multiple feature hashing for near-duplicate duplicate detection [126], submodular hashing for video indexing [127], and Probabilistic Attributed Hashing for integrating low-level features and semantic attributes [128].", "startOffset": 169, "endOffset": 174}, {"referenceID": 126, "context": "Other closely related hashing methods include the design of multiple feature hashing for near-duplicate duplicate detection [126], submodular hashing for video indexing [127], and Probabilistic Attributed Hashing for integrating low-level features and semantic attributes [128].", "startOffset": 272, "endOffset": 277}, {"referenceID": 67, "context": "Especially, due to the well-known semantic gap, supervised and semi-supervised hashing methods have been extensively studied for image search and retrieval [69][29][41][129][130][131], mobile product search[132].", "startOffset": 156, "endOffset": 160}, {"referenceID": 27, "context": "Especially, due to the well-known semantic gap, supervised and semi-supervised hashing methods have been extensively studied for image search and retrieval [69][29][41][129][130][131], mobile product search[132].", "startOffset": 160, "endOffset": 164}, {"referenceID": 39, "context": "Especially, due to the well-known semantic gap, supervised and semi-supervised hashing methods have been extensively studied for image search and retrieval [69][29][41][129][130][131], mobile product search[132].", "startOffset": 164, "endOffset": 168}, {"referenceID": 127, "context": "Especially, due to the well-known semantic gap, supervised and semi-supervised hashing methods have been extensively studied for image search and retrieval [69][29][41][129][130][131], mobile product search[132].", "startOffset": 168, "endOffset": 173}, {"referenceID": 128, "context": "Especially, due to the well-known semantic gap, supervised and semi-supervised hashing methods have been extensively studied for image search and retrieval [69][29][41][129][130][131], mobile product search[132].", "startOffset": 173, "endOffset": 178}, {"referenceID": 129, "context": "Especially, due to the well-known semantic gap, supervised and semi-supervised hashing methods have been extensively studied for image search and retrieval [69][29][41][129][130][131], mobile product search[132].", "startOffset": 178, "endOffset": 183}, {"referenceID": 130, "context": "Especially, due to the well-known semantic gap, supervised and semi-supervised hashing methods have been extensively studied for image search and retrieval [69][29][41][129][130][131], mobile product search[132].", "startOffset": 206, "endOffset": 211}, {"referenceID": 131, "context": "Other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [134][135], pose estimation [71], object tracking [136], and duplicate detection [126][137][52][51][138].", "startOffset": 80, "endOffset": 85}, {"referenceID": 116, "context": "Other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [134][135], pose estimation [71], object tracking [136], and duplicate detection [126][137][52][51][138].", "startOffset": 108, "endOffset": 113}, {"referenceID": 132, "context": "Other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [134][135], pose estimation [71], object tracking [136], and duplicate detection [126][137][52][51][138].", "startOffset": 132, "endOffset": 137}, {"referenceID": 133, "context": "Other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [134][135], pose estimation [71], object tracking [136], and duplicate detection [126][137][52][51][138].", "startOffset": 137, "endOffset": 142}, {"referenceID": 69, "context": "Other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [134][135], pose estimation [71], object tracking [136], and duplicate detection [126][137][52][51][138].", "startOffset": 160, "endOffset": 164}, {"referenceID": 134, "context": "Other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [134][135], pose estimation [71], object tracking [136], and duplicate detection [126][137][52][51][138].", "startOffset": 182, "endOffset": 187}, {"referenceID": 124, "context": "Other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [134][135], pose estimation [71], object tracking [136], and duplicate detection [126][137][52][51][138].", "startOffset": 213, "endOffset": 218}, {"referenceID": 135, "context": "Other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [134][135], pose estimation [71], object tracking [136], and duplicate detection [126][137][52][51][138].", "startOffset": 218, "endOffset": 223}, {"referenceID": 50, "context": "Other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [134][135], pose estimation [71], object tracking [136], and duplicate detection [126][137][52][51][138].", "startOffset": 223, "endOffset": 227}, {"referenceID": 49, "context": "Other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [134][135], pose estimation [71], object tracking [136], and duplicate detection [126][137][52][51][138].", "startOffset": 227, "endOffset": 231}, {"referenceID": 136, "context": "Other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [134][135], pose estimation [71], object tracking [136], and duplicate detection [126][137][52][51][138].", "startOffset": 231, "endOffset": 236}, {"referenceID": 137, "context": "In addition, this emerging hash learning framework can be exploited for some general machine learning and data mining tasks, including crossmodality data fusion [139], large scale optimization [140], large scale classification and regression [141], collaborative filtering [142], and recommendation [143].", "startOffset": 161, "endOffset": 166}, {"referenceID": 138, "context": "In addition, this emerging hash learning framework can be exploited for some general machine learning and data mining tasks, including crossmodality data fusion [139], large scale optimization [140], large scale classification and regression [141], collaborative filtering [142], and recommendation [143].", "startOffset": 193, "endOffset": 198}, {"referenceID": 139, "context": "In addition, this emerging hash learning framework can be exploited for some general machine learning and data mining tasks, including crossmodality data fusion [139], large scale optimization [140], large scale classification and regression [141], collaborative filtering [142], and recommendation [143].", "startOffset": 242, "endOffset": 247}, {"referenceID": 140, "context": "In addition, this emerging hash learning framework can be exploited for some general machine learning and data mining tasks, including crossmodality data fusion [139], large scale optimization [140], large scale classification and regression [141], collaborative filtering [142], and recommendation [143].", "startOffset": 273, "endOffset": 278}, {"referenceID": 141, "context": "In addition, this emerging hash learning framework can be exploited for some general machine learning and data mining tasks, including crossmodality data fusion [139], large scale optimization [140], large scale classification and regression [141], collaborative filtering [142], and recommendation [143].", "startOffset": 299, "endOffset": 304}, {"referenceID": 142, "context": "proposed a structure learning framework to derive a video hashing technique that incorporates both temporal and spatial structure information [144].", "startOffset": 142, "endOffset": 147}, {"referenceID": 143, "context": "proposed to leverage both tag information and semantic topic modeling to achieve more accurate hash codes [145].", "startOffset": 106, "endOffset": 111}, {"referenceID": 144, "context": "designed a two-stage unsupervised hashing framework for fast document retrieval [146].", "startOffset": 80, "endOffset": 85}, {"referenceID": 112, "context": "Without performing exhaustive test on all the data points, hyperplane hashing can help significantly speed up the interactive training sample selection procedure [114][117][116].", "startOffset": 162, "endOffset": 167}, {"referenceID": 115, "context": "Without performing exhaustive test on all the data points, hyperplane hashing can help significantly speed up the interactive training sample selection procedure [114][117][116].", "startOffset": 167, "endOffset": 172}, {"referenceID": 114, "context": "Without performing exhaustive test on all the data points, hyperplane hashing can help significantly speed up the interactive training sample selection procedure [114][117][116].", "startOffset": 172, "endOffset": 177}, {"referenceID": 145, "context": "In addition, a two-stage hashing scheme is developed to achieve fast query pair selection for large scale active learning to rank [147].", "startOffset": 130, "endOffset": 135}, {"referenceID": 114, "context": "Although several recent techniques have presented theoretical analysis of the collision probability, they are mostly based on randomized hash functions [116][117][19].", "startOffset": 152, "endOffset": 157}, {"referenceID": 115, "context": "Although several recent techniques have presented theoretical analysis of the collision probability, they are mostly based on randomized hash functions [116][117][19].", "startOffset": 157, "endOffset": 162}, {"referenceID": 17, "context": "Although several recent techniques have presented theoretical analysis of the collision probability, they are mostly based on randomized hash functions [116][117][19].", "startOffset": 162, "endOffset": 166}, {"referenceID": 146, "context": "Due to their compact form, the hash codes also have great potential in many other large scale data modeling tasks such as efficient nonlinear kernel SVM classifiers [148] and rapid kernel approximation [149].", "startOffset": 165, "endOffset": 170}, {"referenceID": 147, "context": "Due to their compact form, the hash codes also have great potential in many other large scale data modeling tasks such as efficient nonlinear kernel SVM classifiers [148] and rapid kernel approximation [149].", "startOffset": 202, "endOffset": 207}], "year": 2015, "abstractText": "The explosive growth in big data has attracted much attention in designing efficient indexing and search methods recently. In many critical applications such as large-scale search and pattern matching, finding the nearest neighbors to a query is a fundamental research problem. However, the straightforward solution using exhaustive comparison is infeasible due to the prohibitive computational complexity and memory requirement. In response, Approximate Nearest Neighbor (ANN) search based on hashing techniques has become popular due to its promising performance in both efficiency and accuracy. Prior randomized hashing methods, e.g., Locality-Sensitive Hashing (LSH), explore data-independent hash functions with random projections or permutations. Although having elegant theoretic guarantees on the search quality in certain metric spaces, performance of randomized hashing has been shown insufficient in many real-world applications. As a remedy, new approaches incorporating data-driven learning methods in development of advanced hash functions have emerged. Such learning to hash methods exploit information such as data distributions or class labels when optimizing the hash codes or functions. Importantly, the learned hash codes are able to preserve the proximity of neighboring data in the original feature spaces in the hash code spaces. The goal of this paper is to provide readers with systematic understanding of insights, pros and cons of the emerging techniques. We provide a comprehensive survey of the learning to hash framework and representative techniques of various types, including unsupervised, semi-supervised, and supervised. In addition, we also summarize recent hashing approaches utilizing the deep learning models. Finally, we discuss the future direction and trends of research in this area.", "creator": "LaTeX with hyperref package"}}}