{"id": "1509.05488", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2015", "title": "TransG : A Generative Mixture Model for Knowledge Graph Embedding", "abstract": "Recently, knowledge graph embedding, which projects symbolic entities and relations into continuous vector space, has become a new, hot topic in artificial intelligence. This paper addresses a new issue of \\textbf{multiple relation semantics} that a relation may have multiple meanings revealed by the entity pairs associated with the corresponding triples, and proposes a novel Gaussian mixture model for embedding, \\textbf{TransG}. The new model can discover latent semantics for a relation and leverage a mixture of relation component vectors for embedding a fact triple (see paper on \\textbf{Multiple relationship semantics} ), and a function that is based on a model for integrating a fact triple and a Gaussian result. We also investigate the effects of this Gaussian combination model on multiple relations in real-time computation. As well as analyzing the results, we also demonstrate that the model can perform large and accurate test-related tests for the validity of the results. In addition, we have found that using an integration model, we can predict if the value of the combination is different, when one has a multiple relation to the given data, or when the result of both relationships is different.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Fri, 18 Sep 2015 02:30:17 GMT  (1019kb)", "http://arxiv.org/abs/1509.05488v1", null], ["v2", "Mon, 28 Sep 2015 02:17:33 GMT  (390kb,D)", "http://arxiv.org/abs/1509.05488v2", null], ["v3", "Mon, 21 Dec 2015 01:32:17 GMT  (343kb,D)", "http://arxiv.org/abs/1509.05488v3", null], ["v4", "Sun, 27 Dec 2015 05:46:51 GMT  (339kb,D)", "http://arxiv.org/abs/1509.05488v4", null], ["v5", "Tue, 13 Jun 2017 06:03:20 GMT  (344kb,D)", "http://arxiv.org/abs/1509.05488v5", null], ["v6", "Sat, 17 Jun 2017 03:54:46 GMT  (377kb,D)", "http://arxiv.org/abs/1509.05488v6", null], ["v7", "Fri, 8 Sep 2017 12:55:14 GMT  (394kb,D)", "http://arxiv.org/abs/1509.05488v7", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["han xiao", "minlie huang", "yu hao", "xiaoyan zhu"], "accepted": true, "id": "1509.05488"}, "pdf": {"name": "1509.05488.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 9.\n05 48\n8v 1\n[ cs\n.C L\n] 1\n8 Se\np 20"}, {"heading": "Introduction", "text": "Abstract or real-world knowledge is always a major topic in Artificial Intelligence. Knowledge bases such as Wordnet (Miller 1995) and Freebase (Bollacker et al. 2008) have been shown very useful to AI tasks including question answering, knowledge inference, and so on. However, traditional knowledge bases are symbolic and logic, thus numerical machine learning methods cannot be leveraged to support the computation over the knowledge bases. To this end, knowledge graph embedding, which projects entities and relations into continuous vector spaces, has been proposed. Among various embedding models, there is a line of translationbased models such as TransE (Bordes et al. 2013), TransH (Wang et al. 2014) and TransR (Lin et al. 2015).\nA fact of knowledge base can usually be represented by a triple (h, r, t) where h, r, t indicates a head entity, a relation, and a tail entity, respectively. All translation-based models are almost following the same principle hr + r \u2248 tr where hr, r, tr indicate the embedding vectors of triple (h, r, t), with the head and tail entity vector projected with respect to the relation space.\nIn spite of the success of these models, none of the previous models can deal with the multiple relation semantics that a relation may have multiple meanings revealed by the entity pairs associated with the corresponding triples. As can\nCopyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nbe seen from Fig. 1, clustering results on embedding vectors obtained from TransE (Bordes et al. 2013) show that, there are different clusters for a specific relation, and different clusters indicate different latent semantics. For example, the relation HasPart has at least two latent semantics: composition-related as (Table,HasPart,Leg) and locationrelated as (Atlantics,HasPart,NewYorkBay). This phenomenon is quite common in knowledge bases for two reasons: artificial simplification and nature of knowledge. On one hand, knowledge base curators could not involve too many similar relations, so abstracting multiple similar relations into one specific relation is a common trick. On the other hand, both language and knowledge representations often involve ambiguous information. The ambiguity of knowledge means a semantic mixture. For example, when we mention \u201cExpert\u201d, we may refer to scientist, businessman or writer, so the concept \u201cExpert\u201d may be ambiguous in a specific situation, or generally a semantic mixture of these cases.\nHowever, since previous translation-based models adopt hr + r \u2248 tr, they assign only one translation vector for one relation, and these models cannot alleviate the issue of multiple relation semantics. To illustrate more clearly, as showed in Fig.2, there is only one unique representation for relation HasPart in traditional models, thus the models made more errors when embedding the triples of the relation. Instead, in our proposed model, we leverage a Gaussian mixture embedding model to handle multiple relation semantics by mixing multiple translation components for a relation. Thus, different semantics are characterised by different components in our mixture model. For example, we can distinguish the two clusters HasPart 1 orHasPart 2, where the relation semantics are automatically clustered to represent the meaning of associated entity pairs.\nTo summarize, our contributions are as follows:\n\u2022 We address a new issue in knowledge graph embedding, multiple relation semantics that a relation in knowledge base may have different meanings revealed by the associated entity pairs, which has never been studied previously.\n\u2022 We propose a novel Gaussian mixture embedding model, TransG, to address this issue. The model can automatically discover semantic clusters of a relation, and leverage a mixture of multiple relation components for translating\n\u2022 Extensive experiments show that our proposed model obtains substantial improvements against the state-of-the-art baselines."}, {"heading": "Related Work", "text": "Prior studies are classified into two branches: translationbased embedding methods and the others."}, {"heading": "Translation-Based Embedding Methods", "text": "Existing translation-based embedding methods share the same translation principle h+ r \u2248 t and the score function is designed as:\nfr(h, t) = ||hr + r\u2212 tr|| 2 2\nwhere hr, tr are entity embedding vectors projected in the relation-specific space. TransE (Bordes et al. 2013), lays the entities in the original entity space: hr = h, tr = t. TransH (Wang et al. 2014), projects entities into a hyperplane for addressing the issue of complex relation embedding: hr = h\u2212w\u22a4r hwr, tr = t\u2212w \u22a4 r twr. To address the same issue, TransR (Lin et al. 2015), transforms the entity embeddings by the same matrix:hr = Mrh, tr = Mrt.\nFigure 2: Visualization of multiple relation semantics. The data is selected from Wordnet. The dots are correct triples that belong to HasPart relation, while the circles are incorrect ones. The point coordinate is the difference vector between tail and head entity, which should be near to the centre. (a) The correct triples are hard to be distinguished from the incorrect ones. (b) By applying multiple semantic components, our proposed model could discriminate the correct triples from the wrong ones.\nTransR also proposes an ad-hoc clustering-based method, CTransR, where the entity pairs for a relation are clustered into different groups, and the pairs in the same group share the same relation vector. In comparison, our model is more elegant to address such an issue theoretically, and does not require a pre-process of clustering. Furthermore, our model has much better performance than CTransR, as we expect. TransM (Fan et al. 2014) leverages the structure of the knowledge graph via pre-calculating the distinct weight for each training triple to enhance embedding."}, {"heading": "Other Embedding Methods", "text": "There list other embedding approaches:\nStructured Embedding (SE). The SE model (Bordes et al. 2011) transforms the entity space with the head-specific and tail-specific matrices. The score function is defined as fr(h, t) = ||Mh,rh\u2212Mt,rt||. According to (Socher et al. 2013), this model cannot capture the relationship between entities.\nSemantic Matching Energy (SME). The SME model (Bordes et al. 2012) (Bordes et al. 2014) can handle the correlations between entities and relations by matrix product and Hadamard product. In some recent work (Bordes et al. 2014), the score function is re-defined with 3-way tensors instead of matrices.\nSingle Layer Model (SLM). SLM applies neural network to knowledge graph embedding. The score function is defined as fr(h, t) = u\u22a4r g(Mr,1h+Mr,2t) where Mr,1,Mr,2 are relation-specific weight matrices. Collobert had applied a similar method into the language model, (Collobert and Weston 2008).\nLatent Factor Model (LFM). The LFM (Jenatton et al. 2012), (Sutskever, Tenenbaum, and Salakhutdinov 2009) attempts to capture the second-order correlations between entities by a quadratic form. The score function is as fr(h, t) = h \u22a4 Wrt.\nNeural Tensor Network (NTN). The NTN model (Socher et al. 2013) defines a very expressive score function to combine the SLM and LFM: fr(h, t) = u \u22a4 r g(h \u22a4 W\u00b7\u00b7rt+Mr,1h+Mr,2t+ br), where ur is a relation-specific linear layer, g(\u00b7) is the tanh function, W \u2208 R\nd\u00d7d\u00d7k is a 3-way tensor. Unstructured Model (UM). The UM (Bordes et al. 2012) may be a simplified version of TransE without considering any relation-related information. The score function is directly defined as fr(h, t) = ||h\u2212 t||22.\nRESCAL. This is a collective matrix factorization model which is also a common method in knowledge base embedding (Nickel, Tresp, and Kriegel 2011), (Nickel, Tresp, and Kriegel 2012).\nSemantically Smooth Embedding (SSE). (Guo et al. 2015) aims at further discovering the geometric structure of the embedding space to make it semantically smooth.\n(Wang et al. 2014) jointly embeds knowledge and texts. (Wang, Wang, and Guo 2015) incorporates the rules. (Lin, Liu, and Sun 2015) is a path-based embedding model."}, {"heading": "Methods", "text": "In this section, we first review current translation-based methods from the generative perspective, and then introduce TransG, a generative mixture embedding model to capture multiple relation semantics."}, {"heading": "A Generative Perspective for Embedding", "text": "First of all, let\u2019s review previous translation-based embedding models. Existing methods obey the same rule r \u2248 t\u2212 h and apply Euclidean distance as loss metric. The score function is defined as follows:\nfr(h, t) = ||h+ r\u2212 t|| 2 2 (1)\nTherefore, the generative process could be regarded that the difference vector between tail and head (t\u2212 h) is drawn from a Gaussian Distribution with r as mean and an identical matrix E as covariance.\nt\u2212 h | r \u223c N (r,E) (2)\nP{(h, r, t)} = 1\nZ e\u2212||h+r\u2212t||\n2 2 (3)\nwhere Z is a normalization constant. If maximizing the data likelihood of the training set, we derive the TransE model. As discussed in \u201cRelated Work\u201d, different translation-based methods project entities into different vector spaces. Obviously, TransH and TransR have a similar generative process\nas below:\n(t\u2212w\u22a4 r twr)\u2212 (h\u2212w \u22a4 r hwr) | r \u223c N (r,E) (4)\nMrt\u2212Mrh | r \u223c N (r,E) (5)"}, {"heading": "TransG: A Generative Mixture Model for Embedding", "text": "As stated in \u201cIntroduction\u201d, only one single translation vector for a relation may be incompetent to model multiple relation semantics. This means, Eq.(2) should be generalised to a mixture model, such as Gaussian Mixture Model (Yu 2012). Thus, our TransG is proposed as follows:\nt\u2212 h | r \u223c M \u2211\nm=1\n\u03c0r,mN (ur,m,E) (6)\nP{(h, r, t)} = M \u2211\nm=1\n\u03c0r,me \u2212||h+ur,m\u2212t|| 2 2 (7)\nwhere M indicates how many components a relation should have. ur,m is the m-th component translation vector of relation r. \u03c0r,m is the mixing factor, indicating the weight of m-th component. Note that, \u03c0 could be treated as the component prior in Bayesian Statistics, but TransG learns these coefficients from the data.\nInspired by Fig.1, TransG leverages a mixture of relation component vectors for a specific relation. Each component represents a specific latent semantics. By this way, TransG could distinguish multiple relation semantics. From the clustering perspective, TransG attempts to cluster the triples by their latent semantics automatically and then characterise each semantic component for a given entity pair < h, t >. Note that, our score function is the same as other translationbased methods, which is the probability of generating the triple P{(h, r, t)} and the optimization objective is the data likelihood."}, {"heading": "Perspective from Geometry", "text": "Previous studies always have geometric explanations, and so does TransG. In the previous methods, when the relation r of triple (h, r, t) is given, the geometric representations are fixed, as h+ r \u2248 t. However, TransG generalises this geometric principle to:\nm\u2217(h,r,t) = argmaxm=1...M\n(\n\u03c0r,me \u2212||h+ur,m\u2212t||\n2 2\n)\n(8)\nh+ ur,m\u2217 (h,r,t) \u2248 t (9)\nwhere m\u2217(h,r,t) is the index of primary component. Though all the components contribute to the model, the primary one contributes the most due to the exponential effect (exp(\u00b7)). When a triple (h, r, t) is given, TransG works out the index of primary component then translates the head entity to the tail one with this worked-out primary translation vector.\nFor most triples, there should be only one component\nthat have significant non-zero ( \u03c0r,me \u2212||h+ur,m\u2212t|| 2 2 ) value\nand the others would be small enough, due to the exponential decay. This property reduces the noise from the other\nsemantic components to better characterise multiple relation semantics. In detail, (t\u2212 h) is almost around only one translation vector ur,m\u2217\n(h,r,t) in TransG. Under the condition\nm 6= m\u2217(h,r,t), ||h+ ur,m \u2212 t|| 2 2 is very large so that the exponential function value is very small. This is why the primary component could represent the primary semantics.\nTo summarize, previous studies make translation identically for all the triples of the same relation, but TransG automatically selects the best translation vector according to the specific semantics of a triple. Therefore, TransG could focus on the specific semantic embedding to avoid much noise from the other unrelated semantic components and result in promising improvements than existing methods. Note that, all the components in TransG have their own contributions, but the primary one makes the most."}, {"heading": "Training Algorithm", "text": "The maximum data likelihood principle is applied for training. To better distinguish the true triples from the false ones, we maximize the ratio of likelihood of the true triples to that of the false ones. Notably, too many redundant components may do harm to embedding, and thus we impose a sparsity regularization term on \u03c0r,m. Note that embedding vectors are initialized by (Glorot and Bengio 2010). Taking into account all the other constraints, the final objective function is obtained, as follows:\nmin \u2212 \u2211\n(h,r,t)\u2208\u2206\nln\n(\nM \u2211\nm=1\n\u03c0r,me \u2212||h+ur,m\u2212t||\n2 2\n)\n+\n\u2211\n(h\u2032,r\u2032,t\u2032)\u2208\u2206\u2032\nln\n(\nM \u2211\nm=1\n\u03c0r\u2032,me \u2212||h\u2032+u r\u2032,m \u2212t\u2032||22\n)\n+\u2126\ns.t. \u03c0r,m \u2265 0, r \u2208 R, m = 1...M\n\u2126 = C\n(\n\u2211\nr\u2208R\nM \u2211\nm=1\n||ur,m|| 2 2 +\n\u2211\ne\u2208E\n||e||22\n)\n+\u03bb\n(\n\u2211\nr\u2208R\nM \u2211\nm=1\n||\u03c0r,m||1\n)\n(10)\nwhere \u2206 is the set of golden triples and \u2206\u2032 is the set of false triples. C controls the scaling degree and \u03bb controls the sparsity of components. \u2126 is the overall regularization term. E is the set of entities and R is the set of relations.\nSGD is applied to solve this optimization problem. In addition, we apply a trick to control the parameter update process during training. For those very impossible triples, the\nupdate process should be skipped. Hence, we introduce a similar condition as TransE (Bordes et al. 2013) adopts: the training algorithm will update the embedding vectors only if the below condition is satisfied: P{(h, r, t)} P{(h\u2032, r\u2032, t\u2032)} = \u2211M m=1 \u03c0r,me \u2212||h+ur,m\u2212t|| 2 2 \u2211M\nm=1 \u03c0r\u2032,me \u2212||h\u2032+u r\u2032,m \u2212t\u2032||22\n\u2264 Me\u03b3 (11)\nwhere (h, r, t) \u2208 \u2206 and (h\u2032, r\u2032, t\u2032) \u2208 \u2206\u2032. \u03b3 controls the updating condition.\nExperiments Our experiments are conducted on four public benchmark datasets that are the subsets of Wordnet and Freebase, respectively. The statistics of these datasets are listed in Tab.1. Experiments are conducted on two tasks : Link Prediction and Triples Classification. To further demonstrate how the proposed model approaches multiple relation semantics, we present semantic component analysis in the last of this section."}, {"heading": "Link Prediction", "text": "In order to testify the performance of knowledge graph completion, link prediction task is conducted. When given an entity and a relation, the embedding models predict the other missing entity. More specifically, in this task, we predict t given (h, r, \u2217), or predict h given (\u2217, r, t). The WN18 and FB15K are two benchmark datasets for this task. Note that many AI tasks could be enhanced by Link Prediction such as relation extraction (Hoffmann et al. 2011).\nEvaluation Protocol. We adopt the same protocol used in previous studies. For each testing triple (h, r, t), we corrupt it by replacing the tail t (or the head h) with every entity e in the knowledge graph and calculate a probabilistic score of this corrupted triple (h, r, e) (or (e, r, t)) with the score function fr(h, e). By ranking these scores in descending order, we then obtain the rank of the original triple. There are two metrics for evaluation: the averaged rank (Mean Rank) and the proportion of testing triple whose rank is not larger than 10 (HITS@10). This is called \u201cRaw\u201d setting. When we filter out the corrupted triples that exist in the training, validation, or test datasets, this is the\u201cFilter\u201d setting. If a corrupted triple exists in the knowledge graph, ranking it ahead the original triple is also acceptable. To eliminate this case, the \u201cFilter\u201d setting is more preferred. In both settings, a lower Mean Rank and a higher HITS@10 mean better performance.\nImplementation. As the datasets are the same, we directly reproduce the experimental results of several baselines from the literature, as in (Bordes et al. 2013), (Wang et al. 2014) and (Lin et al. 2015). We have attempted several settings on the validation dataset to get the best configuration. Under the \u201cbern.\u201d sampling strategy, the optimal configurations are: learning rate \u03b1 = 0.001, embedding dimension k = 50, \u03b3 = 0.2, M = 10, \u03bb = 0.01 on WN18; \u03b1 = 0.0015, k = 400, \u03b3 = 0.25, M = 10, \u03bb = 0.01 on FB15K. Note that all the symbols are introduced in \u201cMethods\u201d. We train the model until it converges, but at most 10,000 rounds (usually converges at about 1,000 rounds).\nResults. Evaluation results on WN18 and FB15K are reported in Tab.2 and Tab.4. We observe that:\n1. TransG outperforms all the baselines obviously. Compared to TransR, TransG makes improvements by 2.9% on WN18 and 23.0% on FB15K, and the averaged semantic component number on WN18 is 2.61 and that on FB15K is 5.39. This result demonstrates capturing multiple relation semantics would benefit embedding.\n2. The model has a bad Mean Rank score on the WN18 dataset. Further analysis shows that there are 24 testing triples (0.5% of the testing set) whose ranks are more than 30,000, and these few cases would lead to about 150 mean rank loss. Among these triples, there are 23 triples whose tail or head entities have never been co-occurring with the corresponding relations in the training set. In one word, there is no sufficient training data for those relations and entities. Since TransG applies multiple relation components and the clustering centers are more scattered in the vector space while other models just apply single relation vector, TransG is less competitive to handle these extreme cases.\n3. Compared to CTransR, TransG solves the multiple relation semantics problem much better for two reasons. Firstly, CTransR clusters the entity pairs for a specific relation then performs embedding for each cluster, but TransG deals with embedding and multiple relation semantics at the same time, where the two processes can be enhanced by each other. Secondly, CTransR models a triple by only one cluster, but TransG applies a mixture to refine the embedding."}, {"heading": "Triples Classification", "text": "In order to testify the discriminative capability between true and false facts, triples classification task is conducted. This is a classical task in knowledge base embedding, which aims at predicting whether a given triple (h, r, t) is correct or not. WN11 and FB13 are the benchmark datasets for this task. Note that evaluation of classification needs negative samples, and the datasets have already been built with negative triples.\nEvaluation Protocol. The decision process is very simple as follows: for a triple (h, r, t), if fr(h, t) is below a threshold \u03c3r, then positive; otherwise negative. The thresholds {\u03c3r} are determined on the validation dataset.\nImplementation. As all methods use the same datasets, we directly re-use the results of different methods from the literature. We have attempted several settings on the validation dataset to find the best configuration. The optimal configurations of TransG are as follows: \u201cbern\u201d sampling, learning rate \u03b1 = 0.001, embedding dimension k = 50, \u03b3 = 6.0, M = 10, \u03bb = 0.01 on WN11, and \u201cbern\u201d sampling, \u03b1 = 0.002, k = 400, \u03b3 = 0.2, M = 10, \u03bb = 0.01 on FB13. We limit the maximum number of epochs to 500 but the algorithm usually converges at around 100 epochs.\nResults. Accuracies are reported in Tab.3 and Fig.3. We observe that:\n1. TransG outperforms all the baselines remarkably. Compared to TransR, TransG improves by 1.7% on WN11 and 5.8% on FB13, and the averaged semantic component number on WN11 is 2.72 and that on FB13 is 3.08. This result shows the benefit of capturing multiple relation semantics for a relation.\n2. The relations, such as \u201cSynset Domain\u201d and \u201cType Of\u201d, which hold more semantic components, are improved much more. Conversely, the relation \u201cSimilar\u201d holds only one semantic component and is almost not promoted. This further demonstrates that capturing multiple relation semantics can benefit embedding.\nFigure 3: Accuracies of each relations in WN11 for triples classification. We count the components with significant non-zero \u03c0r,m as \u201cSemantic Component Number\u201d for each relation.\nFigure 4: Semantic component number on WN18 (left) and FB13 (right). For each relation, we only count the components with non-zero \u03c0r,m."}, {"heading": "Semantic Component Analysis", "text": "In this subsection, we analyse the number of semantic components for different relations. Since sparsity is considered in our approach, we list the component number on the dataset WN18 and FB13 in Fig.4 where only the component with non-zero \u03c0m is counted.\nResults. As Fig. 4 and Tab. 5 illustrates, we observe that:\n1. Multiple semantic components are indeed necessary for most relations. Except for relations such as \u201cAlso See\u201d, \u201cSynset Usage\u201d and \u201cGender\u201d, all other relations have more than one semantic component.\n2. Different components indeed correspond to different semantics, justifying the theoretical analysis and effectiveness of TransG. For example, \u201cProfession\u201d has at least three significant semantics: sciencerelated as (ElLissitzky,Architect), business-related as (EnochPratt,Entrepreneur) and writer-related as (Vlad.Gardin, ScreenWriter).\n3. WN11 and WN18 are the different subsets of Wordnet. As we know, the semantic component number is decided on the triples in the dataset. Therefore, It\u2019s reasonable that similar relations, such as \u201cSynset Region\u201d and \u201cSynset Usage\u201d may hold different semantic numbers for WN11 and WN18."}, {"heading": "Conclusion", "text": "In this paper, we address a new issue, multiple relation semantics, and propose TransG, a generative mixture embedding model for knowledge graph embedding. TransG can discover the latent semantics of a relation automatically and leverage a mixture of relation components for embedding. Extensive experiments show our method achieves substantial improvements against the state-of-the-art baselines. To reproduce our results, experimental codes and data will be published in github1."}], "references": [{"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor"], "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management of data, 1247\u20131250. ACM.", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Learning structured embeddings of knowledge bases", "author": ["A. Bordes", "J. Weston", "R. Collobert", "Y Bengio"], "venue": "In Proceedings of the Twenty-fifth AAAI Conference on Artificial Intelligence", "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "Joint learning of words and meaning representations for open-text semantic parsing", "author": ["A. Bordes", "X. Glorot", "J. Weston", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics, 127\u2013135.", "citeRegEx": "Bordes et al\\.,? 2012", "shortCiteRegEx": "Bordes et al\\.", "year": 2012}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["A. Bordes", "N. Usunier", "A. Garcia-Duran", "J. Weston", "O. Yakhnenko"], "venue": "Advances in Neural Information Processing Systems, 2787\u20132795.", "citeRegEx": "Bordes et al\\.,? 2013", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "A semantic matching energy function for learning with multirelational data", "author": ["A. Bordes", "X. Glorot", "J. Weston", "Y. Bengio"], "venue": "Machine Learning 94(2):233\u2013259.", "citeRegEx": "Bordes et al\\.,? 2014", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th international conference on Machine learning, 160\u2013167. ACM.", "citeRegEx": "Collobert and Weston,? 2008", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Transition-based knowledge graph embedding with relational mapping properties", "author": ["M. Fan", "Q. Zhou", "E. Chang", "T.F. Zheng"], "venue": "Proceedings of the 28th Pacific Asia Conference on Language, Information, and Computation, 328\u2013337.", "citeRegEx": "Fan et al\\.,? 2014", "shortCiteRegEx": "Fan et al\\.", "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International conference on artificial intelligence and statistics, 249\u2013256.", "citeRegEx": "Glorot and Bengio,? 2010", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "Semantically smooth knowledge graph embedding", "author": ["S. Guo", "Q. Wang", "B. Wang", "L. Wang", "L. Guo"], "venue": "Proceedings of ACL.", "citeRegEx": "Guo et al\\.,? 2015", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["R. Hoffmann", "C. Zhang", "X. Ling", "L. Zettlemoyer", "D.S. Weld"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language", "citeRegEx": "Hoffmann et al\\.,? 2011", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "A latent factor model for highly multi-relational data", "author": ["R. Jenatton", "N.L. Roux", "A. Bordes", "G.R. Obozinski"], "venue": "Advances in Neural Information Processing Systems, 3167\u20133175.", "citeRegEx": "Jenatton et al\\.,? 2012", "shortCiteRegEx": "Jenatton et al\\.", "year": 2012}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Y. Lin", "Z. Liu", "M. Sun", "Y. Liu", "X. Zhu"], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence.", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Modeling relation paths for representation learning of knowledge bases", "author": ["Y. Lin", "Z. Liu", "M. Sun"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics.", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Wordnet: a lexical database for english", "author": ["G.A. Miller"], "venue": "Communications of the ACM 38(11):39\u201341.", "citeRegEx": "Miller,? 1995", "shortCiteRegEx": "Miller", "year": 1995}, {"title": "A threeway model for collective learning on multi-relational data", "author": ["M. Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), 809\u2013816.", "citeRegEx": "Nickel et al\\.,? 2011", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Factorizing yago: scalable machine learning for linked data", "author": ["M. Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "Proceedings of the 21st international conference on World Wide Web, 271\u2013280. ACM.", "citeRegEx": "Nickel et al\\.,? 2012", "shortCiteRegEx": "Nickel et al\\.", "year": 2012}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A. Ng"], "venue": "Advances in Neural Information Processing Systems, 926\u2013934.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Modelling relational data using bayesian clustered tensor", "author": ["I. Sutskever", "J.B. Tenenbaum", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2009}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen"], "venue": "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, 1112\u20131119.", "citeRegEx": "Wang et al\\.,? 2014", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Knowledge base completion using embeddings and rules", "author": ["Q. Wang", "B. Wang", "L. Guo"], "venue": "Proceedings of the 24th International Joint Conference on Artificial Intelligence.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "A nonlinear kernel gaussian mixture model based inferential monitoring approach for fault detection and diagnosis of chemical processes", "author": ["J. Yu"], "venue": "Chemical Engineering Science 68(1):506\u2013519.", "citeRegEx": "Yu,? 2012", "shortCiteRegEx": "Yu", "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "Knowledge bases such as Wordnet (Miller 1995) and Freebase (Bollacker et al.", "startOffset": 32, "endOffset": 45}, {"referenceID": 0, "context": "Knowledge bases such as Wordnet (Miller 1995) and Freebase (Bollacker et al. 2008) have been shown very useful to AI tasks including question answering, knowledge inference, and so on.", "startOffset": 59, "endOffset": 82}, {"referenceID": 3, "context": "Among various embedding models, there is a line of translationbased models such as TransE (Bordes et al. 2013), TransH (Wang et al.", "startOffset": 90, "endOffset": 110}, {"referenceID": 18, "context": "2013), TransH (Wang et al. 2014) and TransR (Lin et al.", "startOffset": 14, "endOffset": 32}, {"referenceID": 11, "context": "2014) and TransR (Lin et al. 2015).", "startOffset": 17, "endOffset": 34}, {"referenceID": 3, "context": "1, clustering results on embedding vectors obtained from TransE (Bordes et al. 2013) show that, there are different clusters for a specific relation, and different clusters indicate different latent semantics.", "startOffset": 64, "endOffset": 84}, {"referenceID": 3, "context": "TransE (Bordes et al. 2013), lays the entities in the original entity space: hr = h, tr = t.", "startOffset": 7, "endOffset": 27}, {"referenceID": 18, "context": "TransH (Wang et al. 2014), projects entities into a hyperplane for addressing the issue of complex relation embedding: hr = h\u2212w r hwr, tr = t\u2212w \u22a4 r twr.", "startOffset": 7, "endOffset": 25}, {"referenceID": 11, "context": "To address the same issue, TransR (Lin et al. 2015), transforms the entity embeddings by the same matrix:hr = Mrh, tr = Mrt.", "startOffset": 34, "endOffset": 51}, {"referenceID": 6, "context": "TransM (Fan et al. 2014) leverages the structure of the knowledge graph via pre-calculating the distinct weight for each training triple to enhance embedding.", "startOffset": 7, "endOffset": 24}, {"referenceID": 1, "context": "The SE model (Bordes et al. 2011) transforms the entity space with the head-specific and tail-specific matrices.", "startOffset": 13, "endOffset": 33}, {"referenceID": 16, "context": "According to (Socher et al. 2013), this model cannot capture the relationship between entities.", "startOffset": 13, "endOffset": 33}, {"referenceID": 2, "context": "The SME model (Bordes et al. 2012) (Bordes et al.", "startOffset": 14, "endOffset": 34}, {"referenceID": 4, "context": "2012) (Bordes et al. 2014) can handle the correlations between entities and relations by matrix product and Hadamard product.", "startOffset": 6, "endOffset": 26}, {"referenceID": 4, "context": "In some recent work (Bordes et al. 2014), the score function is re-defined with 3-way tensors instead of matrices.", "startOffset": 20, "endOffset": 40}, {"referenceID": 5, "context": "Collobert had applied a similar method into the language model, (Collobert and Weston 2008).", "startOffset": 64, "endOffset": 91}, {"referenceID": 10, "context": "The LFM (Jenatton et al. 2012), (Sutskever, Tenenbaum, and Salakhutdinov 2009) attempts to capture the second-order correlations between entities by a quadratic form.", "startOffset": 8, "endOffset": 30}, {"referenceID": 16, "context": "The NTN model (Socher et al. 2013) defines a very expressive score function to combine the SLM and LFM: fr(h, t) =", "startOffset": 14, "endOffset": 34}, {"referenceID": 2, "context": "The UM (Bordes et al. 2012) may be a simplified version of TransE without considering any relation-related information.", "startOffset": 7, "endOffset": 27}, {"referenceID": 8, "context": "(Guo et al. 2015) aims at further discovering the geometric structure of the embedding space to make it semantically smooth.", "startOffset": 0, "endOffset": 17}, {"referenceID": 18, "context": "(Wang et al. 2014) jointly embeds knowledge and texts.", "startOffset": 0, "endOffset": 18}, {"referenceID": 20, "context": "(2) should be generalised to a mixture model, such as Gaussian Mixture Model (Yu 2012).", "startOffset": 77, "endOffset": 86}, {"referenceID": 7, "context": "Note that embedding vectors are initialized by (Glorot and Bengio 2010).", "startOffset": 47, "endOffset": 71}, {"referenceID": 3, "context": "Hence, we introduce a similar condition as TransE (Bordes et al. 2013) adopts: the training algorithm will update the embedding vectors only if the below condition is satisfied:", "startOffset": 50, "endOffset": 70}, {"referenceID": 9, "context": "Note that many AI tasks could be enhanced by Link Prediction such as relation extraction (Hoffmann et al. 2011).", "startOffset": 89, "endOffset": 111}, {"referenceID": 3, "context": "As the datasets are the same, we directly reproduce the experimental results of several baselines from the literature, as in (Bordes et al. 2013), (Wang et al.", "startOffset": 125, "endOffset": 145}, {"referenceID": 18, "context": "2013), (Wang et al. 2014) and (Lin et al.", "startOffset": 7, "endOffset": 25}, {"referenceID": 11, "context": "2014) and (Lin et al. 2015).", "startOffset": 10, "endOffset": 27}, {"referenceID": 1, "context": "Metric Mean Rank HITS@10(%) Mean Rank HITS@10(%) Raw Filter Raw Filter Raw Filter Raw Filter SE(Bordes et al. 2011) 1,011 985 68.", "startOffset": 95, "endOffset": 115}, {"referenceID": 2, "context": "8 SME(linear) (Bordes et al. 2012) 545 533 65.", "startOffset": 14, "endOffset": 34}, {"referenceID": 2, "context": "8 SME(bilinear) (Bordes et al. 2012) 526 509 54.", "startOffset": 16, "endOffset": 36}, {"referenceID": 10, "context": "3 LFM (Jenatton et al. 2012) 469 456 71.", "startOffset": 6, "endOffset": 28}, {"referenceID": 3, "context": "1 TransE (Bordes et al. 2013) 263 251 75.", "startOffset": 9, "endOffset": 29}, {"referenceID": 18, "context": "1 TransH (Wang et al. 2014) 401 388 73.", "startOffset": 9, "endOffset": 27}, {"referenceID": 11, "context": "4 TransR (Lin et al. 2015) 238 225 79.", "startOffset": 9, "endOffset": 26}, {"referenceID": 11, "context": "7 CTransR (Lin et al. 2015) 231 218 79.", "startOffset": 10, "endOffset": 27}, {"referenceID": 1, "context": "Relation Category 1-1 1-N N-1 N-N 1-1 1-N N-1 N-N SE(Bordes et al. 2011) 35.", "startOffset": 52, "endOffset": 72}, {"referenceID": 2, "context": "3 SME(linear) (Bordes et al. 2012) 35.", "startOffset": 14, "endOffset": 34}, {"referenceID": 2, "context": "3 SME(bilinear) (Bordes et al. 2012) 30.", "startOffset": 16, "endOffset": 36}, {"referenceID": 3, "context": "8 TransE (Bordes et al. 2013) 43.", "startOffset": 9, "endOffset": 29}, {"referenceID": 18, "context": "0 TransH (Wang et al. 2014) 66.", "startOffset": 9, "endOffset": 27}, {"referenceID": 11, "context": "2 TransR (Lin et al. 2015) 78.", "startOffset": 9, "endOffset": 26}, {"referenceID": 11, "context": "1 CTransR (Lin et al. 2015) 81.", "startOffset": 10, "endOffset": 27}], "year": 2017, "abstractText": "Recently, knowledge graph embedding, which projects symbolic entities and relations into continuous vector space, has become a new, hot topic in artificial intelligence. This paper addresses a new issue of multiple relation semantics that a relation may have multiple meanings revealed by the entity pairs associated with the corresponding triples, and proposes a novel Gaussian mixture model for embedding, TransG. The new model can discover latent semantics for a relation and leverage a mixture of relation component vectors for embedding a fact triple. To the best of our knowledge, this is the first generative model for knowledge graph embedding, which is able to deal with multiple relation semantics. Extensive experiments show that the proposed model achieves substantial improvements against the state-of-the-art baselines.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}