{"id": "1509.06041", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2015", "title": "Robust Image Sentiment Analysis Using Progressively Trained and Domain Transferred Deep Networks", "abstract": "Sentiment analysis of online user generated content is important for many social media analytics tasks. Researchers have largely relied on textual sentiment analysis to develop systems to predict political elections, measure economic indicators, and so on. Recently, social media users are increasingly using images and videos to express their opinions and share their experiences. Sentiment analysis of such large scale visual content can help better extract user sentiments toward events or topics, such as those in image tweets, so that prediction of sentiment from visual content is complementary to textual sentiment analysis. Motivated by the needs in leveraging large scale yet noisy training data to solve the extremely challenging problem of image sentiment analysis, we employ Convolutional Neural Networks (CNN). We first design a suitable CNN architecture for image sentiment analysis. We obtain half a million training samples by using a baseline sentiment algorithm to label Flickr images. To make use of such noisy machine labeled data, we employ a progressive strategy to fine-tune the deep network. Furthermore, we improve the performance on Twitter images by inducing domain transfer with a small number of manually labeled Twitter images. We have conducted extensive experiments on manually labeled Twitter images. The results show that the proposed CNN can achieve better performance in image sentiment analysis than competing algorithms. Our goal is to provide robust, robust algorithms that improve performance and improve overall prediction. We hope that our data can be used to support the development of a robust neural network.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Sun, 20 Sep 2015 18:36:01 GMT  (5382kb,D)", "http://arxiv.org/abs/1509.06041v1", "9 pages, 5 figures, AAAI 2015"]], "COMMENTS": "9 pages, 5 figures, AAAI 2015", "reviews": [], "SUBJECTS": "cs.CV cs.IR cs.LG", "authors": ["quanzeng you", "jiebo luo", "hailin jin", "jianchao yang"], "accepted": true, "id": "1509.06041"}, "pdf": {"name": "1509.06041.pdf", "metadata": {"source": "CRF", "title": "Robust Image Sentiment Analysis Using Progressively Trained and Domain Transferred Deep Networks", "authors": ["Quanzeng You", "Jiebo Luo", "Hailin Jin", "Jianchao Yang"], "emails": ["jluo}@cs.rochester.edu", "jiayang}@adobe.com"], "sections": [{"heading": "Introduction", "text": "Online social networks are providing more and more convenient services to their users. Today, social networks have grown to be one of the most important sources for people to acquire information on all aspects of their lives. Meanwhile, every online social network user is a contributor to such large amounts of information. Online users love to share their experiences and to express their opinions on virtually all events and subjects.\nAmong the large amount of online user generated data, we are particularly interested in people\u2019s opinions or sentiments towards specific topics and events. There have been many\nCopyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nworks on using online users\u2019 sentiments to predict boxoffice revenues for movies (Asur and Huberman 2010), political elections (O\u2019Connor et al. 2010; Tumasjan et al. 2010) and economic indicators (Bollen, Mao, and Zeng 2011; Zhang, Fuehres, and Gloor 2011). These works have suggested that online users\u2019 opinions or sentiments are closely correlated with our real-world activities. All of these results hinge on accurate estimation of people\u2019s sentiments according to their online generated content. Currently all of these works only rely on sentiment analysis from textual content. However, multimedia content, including images and videos, has become prevalent over all online social networks. Indeed, online social network providers are competing with each other by providing easier access to their increasingly powerful and diverse services. Figure 1 shows example images related to the 2012 United States presidential election. Clearly, images in the top and bottom rows convey opposite sentiments towards the two candidates.\nA picture is worth a thousand words. People with different backgrounds can easily understand the main content of an image or video. Apart from the large amount of easily available visual content, today\u2019s computational infrastructure is also much cheaper and more powerful to make the analysis of computationally intensive visual content analysis feasible. In this era of big data, it has been shown that the integration of visual content can provide us more reliable or complementary online social signals (Jin et al. 2010; Yuan et al. 2013).\nTo the best of our knowledge, little attention has been paid to the sentiment analysis of visual content. Only a few recent works attempted to predict visual sentiment using features\nar X\niv :1\n50 9.\n06 04\n1v 1\n[ cs\n.C V\n] 2\n0 Se\np 20\n15\nfrom images (Siersdorfer et al. 2010; Borth et al. 2013b; Borth et al. 2013a; Yuan et al. 2013) and videos (Morency, Mihalcea, and Doshi 2011). Visual sentiment analysis is extremely challenging. First, image sentiment analysis is inherently more challenging than object recognition as the latter is usually well defined. Image sentiment involves a much higher level of abstraction and subjectivity in the human recognition process (Joshi et al. 2011), on top of a wide variety of visual recognition tasks including object, scene, action and event recognition. In order to use supervised learning, it is imperative to collect a large and diverse labeled training set perhaps on the order of millions of images. This is an almost insurmountable hurdle due to the tremendous labor required for image labeling. Second, the learning schemes need to have high generalizability to cover more different domains. However, the existing works use either pixel-level features or a limited number of predefined attribute features, which is difficult to adapt the trained models to images from a different domain.\nThe deep learning framework enables robust and accurate feature learning, which in turn produces the state-of-the-art performance on digit recognition (LeCun et al. 1989; Hinton, Osindero, and Teh 2006), image classification (Cires\u0327an et al. 2011; Krizhevsky, Sutskever, and Hinton 2012), musical signal processing (Hamel and Eck 2010) and natural language processing (Maas et al. 2011). Both the academia and industry have invested a huge amount of effort in building powerful neural networks. These works suggested that deep learning is very effective in learning robust features in a supervised or unsupervised fashion. Even though deep neural networks may be trapped in local optima (Hinton 2010; Bengio 2012), using different optimization techniques, one can achieve the state-of-the-art performance on many challenging tasks mentioned above.\nInspired by the recent successes of deep learning, we are interested in solving the challenging visual sentiment analysis task using deep learning algorithms. For images related tasks, Convolutional Neural Network (CNN) are widely used due to the usage of convolutional layers. It takes into consideration the locations and neighbors of image pixels, which are important to capture useful features for visual tasks. Convolutional Neural Networks (LeCun et al. 1998; Cires\u0327an et al. 2011; Krizhevsky, Sutskever, and Hinton 2012) have been proved very powerful in solving computer vision related tasks. We intend to find out whether applying CNN to visual sentiment analysis provides advantages over using a predefined collection of low-level visual features or visual attributes, which have been done in prior works.\nTo that end, we address in this work two major challenges: 1) how to learn with large scale weakly labeled training data, and 2) how to generalize and extend the learned model across domains. In particular, we make the following contributions.\n\u2022 We develop an effective deep convolutional network architecture for visual sentiment analysis. Our architecture employs two convolutional layers and several fully connected layers for the prediction of visual sentiment labels.\n\u2022 Our model attempts to address the weakly labeled nature\nof the training image data, where such labels are machine generated, by leveraging a progressive training strategy and a domain transfer strategy to fine-tune the neural network. Our evaluation results suggest that this strategy is effective for improving the performance of neural network in terms of generalizability.\n\u2022 In order to evaluate our model as well as competing algorithms, we build a large manually labeled visual sentiment dataset using Amazon Mechanical Turk. This dataset will be released to the research community to promote further investigations on visual sentiment."}, {"heading": "Related Work", "text": "In this section, we review literature closely related to our study on visual sentiment analysis, particularly in sentiment analysis and Convolutional Neural Networks."}, {"heading": "Sentiment Analysis", "text": "Sentiment analysis is a very challenging task (Liu et al. 2003; Li et al. 2010). Researchers from natural language processing and information retrieval have developed different approaches to solve this problem, achieving promising or satisfying results (Pang and Lee 2008). In the context of social media, there are several additional unique challenges. First, there are huge amounts of data available. Second, messages on social networks are by nature informal and short. Third, people use not only textual messages, but also images and videos to express themselves.\nTumasjan et al. (2010) and Bollen et al. (2011) employed pre-defined dictionaries for measuring the sentiment level of Tweets. The volume or percentage of sentiment-bearing words can produce an estimate of the sentiment of one particular tweet. Davidov et al. (2010) used the weak labels from a large amount of Tweets. In contrast, they manually selected hashtags with strong positive and negative sentiments and ASCII smileys are also utilized to label the sentiments of tweets. Furthermore, Hu et al. (2013) incorporated social signals into their unsupervised sentiment analysis framework. They defined and integrated both emotion indication and correlation into a framework to learn parameters for their sentiment classifier.\nThere are also several recent works on visual sentiment analysis. Siersdorfer et al. (2010) proposes a machine learning algorithm to predict the sentiment of images using pixellevel features. Motivated by the fact that sentiment involves high-level abstraction, which may be easier to explain by objects or attributes in images, both (Borth et al. 2013a) and (Yuan et al. 2013) propose to employ visual entities or attributes as features for visual sentiment analysis. In (Borth et al. 2013a), 1200 adjective noun pairs (ANP), which may correspond to different levels of different emotions, are extracted. These ANPs are used as queries to crawl images from Flickr. Next, pixel-level features of images in each ANP are employed to train 1200 ANP detectors. The responses of these 1200 classifiers can then be considered as mid-level features for visual sentiment analysis. The work in (Yuan et al. 2013) employed a similar mechanism. The main difference is that 102 scene attributes are used instead."}, {"heading": "Convolutional Neural Networks", "text": "Convolutional Neural Networks (CNN) have been very successful in document recognition (LeCun et al. 1998). CNN typically consists of several convolutional layers and several fully connected layers. Between the convolutional layers, there may also be pooling layers and normalization layers. CNN is a supervised learning algorithm, where parameters of different layers are learned through back-propagation. Due to the computational complexity of CNN, it has only be applied to relatively small images in the literature. Recently, thanks to the increasing computational power of GPU, it is now possible to train a deep convolutional neural network on a large scale image dataset (Krizhevsky, Sutskever, and Hinton 2012). Indeed, in the past several years, CNN has been successfully applied to scene parsing (Grangier, Bottou, and Collobert 2009), feature learning (LeCun, Kavukcuoglu, and Farabet 2010), visual recognition (Kavukcuoglu et al. 2010) and image classification (Krizhevsky, Sutskever, and Hinton 2012). In our work, we intend to use CNN to learn features which are useful for visual sentiment analysis."}, {"heading": "Visual Sentiment Analysis", "text": "We propose to develop a suitable convolutional neural network architecture for visual sentiment analysis. Moreover, we employ a progressive training strategy that leverages the training results of convolutional neural network to further filter out (noisy) training data. The details of the proposed framework will be described in the following sections.\nVisual Sentiment Analysis with regular CNN CNN has been proven to be effective in image classification tasks, e.g., achieving the state-of-the-art performance in ImageNet Challenge (Krizhevsky, Sutskever, and Hinton 2012). Visual sentiment analysis can also be treated as an image classification problem. It may seem to be a much easier problem than image classification from ImageNet (2 classes vs. 1000 classes in ImageNet). However, visual sentiment analysis is quite challenging because sentiments or opinions correspond to high level abstractions from a given image. This type of high level abstraction may require viewer\u2019s knowledge beyond the image content itself. Meanwhile, images in the same class of ImageNet mainly\ncontain the same type of object. In sentiment analysis, each class contains much more diverse images. It is therefore extremely challenging to discover features which can distinguish much more diverse classes from each other. In addition, people may have totally different sentiments over the same image. This adds difficulties to not only our classification task, but also the acquisition of labeled images. In other words, it is nontrivial to obtain highly reliable labeled instances, let alone a large number of them. Therefore, we need a supervised learning engine that is able to tolerate a significant level of noise in the training dataset.\nThe architecture of the CNN we employ for sentiment analysis is shown in Figure 2. Each image is resized to 256 \u00d7 256 (if needed, we employ center crop, which first resizes the shorter dimension to 256 and then crops the middle section of the resized image). The resized images are processed by two convolutional layers. Each convolutional layer is also followed by max-pooling layers and normalization layers. The first convolutional layer has 96 kernels of size 11 \u00d7 11 \u00d7 3 with a stride of 4 pixels. The second convolutional layer has 256 kernels of size 5 \u00d7 5 with a stride of 2 pixels. Furthermore, we have four fully connected layers. Inspired by (C\u0327aglar Gu\u0308lc\u0327ehre et al. 2013), we constrain the second to last fully connected layer to have 24 neurons. According to the Plutchik\u2019s wheel of emotions (Plutchik 1984), there are a total of 24 emotions belonging to two categories: positive emotions and negative emotions. Intuitively. we hope these 24 nodes may help the network to learn the 24 emotions from a given image and then classify each image into positive or negative class according to the responses of these 24 emotions.\nThe last layer is designed to learn the parameter w by maximizing the following conditional log likelihood function (xi and yi are the feature vector and label for the i-th instance respectively):\nl(w) = n\u2211 i=1 ln p(yi = 1|xi, w) + (1\u2212 yi) ln p(yi = 0|xi, w) (1) where\np(yi|xi, w) = exp(w0 +\n\u2211k j=1 wjxij) yi\n1 + exp(w0 + \u2211k j=1 wjxij) yi\n(2)\n... ..."}, {"heading": "Visual Sentiment Analysis with Progressive CNN", "text": "Since the images are weakly labeled, it is possible that the neural network can get stuck in a bad local optimum. This may lead to poor generalizability of the trained neural network. On the other hand, we found that the neural network is still able to correctly classify a large proportion of the training instances. In other words, the neural network has learned knowledge to distinguish the training instances with relatively distinct sentiment labels. Therefore, we propose to progressively select a subset of the training instances to reduce the impact of noisy training instances. Figure 3 shows the overall flow of the proposed progressive CNN (PCNN). We first train a CNN on Flickr images. Next, we select training samples according to the prediction score of the trained model on the training data itself. Instead of training from the beginning, we further fine-tune the trained model using these newly selected, and potentially cleaner training instances. This fine-tuned model will be our final model for visual sentiment analysis.\nAlgorithm 1 Progressive CNN training for Visual Sentiment Analysis Input: X = {x1, x2, . . . , xn} a set of images of size 256\u00d7\n256 Y = {y1, y2, . . . , yn} sentiment labels of X\n1: Train convolutional neural network CNN with input X and Y 2: Let S \u2208 Rn\u00d72 be the sentiment scores of X predicted using CNN 3: for si \u2208 S do 4: Delete xi from X with probability pi (Eqn.(3)) 5: end for 6: Let X \u2032 \u2282 X be the remaining training images, Y \u2032 be\ntheir sentiment labels 7: Fine-tune CNN with input X \u2032 and Y \u2032 to get PCNN 8: return PCNN\nIn particular, we employ a probabilistic sampling algorithm to select the new training subset. The intuition is that we want to keep instances with distinct sentiment scores\nbetween the two classes with a high probability, and conversely remove instances with similar sentiment scores for both classes with a high probability. Let si = (si1, si2) be the prediction sentiment scores for the two classes of instance i. We choose to remove the training instance i with probability pi given by Eqn.(3). Algorithm 1 summarizes the steps of the proposed framework.\npi = max (0, 2\u2212 exp(|si1 \u2212 si2|)) (3)\nWhen the difference between the predicted sentiment scores of one training instance are large enough, this training instance will be kept in the training set. Otherwise, the smaller the difference between the predicted sentiment scores become, the larger the probability of this instance being removed from the training set."}, {"heading": "Experiments", "text": "We choose to use the same half million Flickr images from SentiBank1 to train our Convolutional Neural Network. These images are only weakly labeled since each image belongs to one adjective noun pair (ANP). There are a total of 1200 ANPs. According to the Plutchik\u2019s Wheel of Emotions (Plutchik 1984), each ANP is generated by the combination of adjectives with strong sentiment values and nouns from tags of images and videos (Borth et al. 2013b). These ANPs are then used as queries to collect related images for each ANP. The released SentiBank contains 1200 ANPs with about half million Flickr images. We train our convolutional neural network mainly on this image dataset. We implement the proposed architecture of CNN on the publicly available implementation Caffe (Jia 2013). All of our experiments are evaluated on a Linux X86 64 machine with 32G RAM and two NVIDIA GTX Titan GPUs."}, {"heading": "Comparisons of different CNN architectures", "text": "The architecture of our model is shown in Figure 2. However, we also evaluate other architectures for the visual sentiment analysis task. Table 1 summarizes the performance of different architectures on a randomly chosen Flickr testing dataset. In Table 1, iCONV-jFC indicates that there are\n1http://visual-sentiment-ontology.appspot.com/\ni convolutional layers and j fully connected layers in the architecture. The model in Figure 2 shows slightly better performance than other models in terms of F1 and accuracy. In the following experiments, we mainly focus on the evaluation of CNN using the architecture in Figure 2."}, {"heading": "Baselines", "text": "We compare the performance of PCNN with three other baselines or competing algorithms for image sentiment classification.\nLow-level Feature-based Siersdorfer et al. (2010) defined both global and local visual features. Specifically, the global color histograms (GCH) features consist of 64-bin RGB histogram. The local color histogram features (LCH) first divided the image into 16 blocks and used the 64-bin RGB histogram for each block. They also employed SIFT features to learn a visual word dictionary. Next, they defined bag of visual word features (BoW) for each image.\nMid-level Feature-based Damian et al. (2013a; 2013b) proposed a framework to build visual sentiment ontology and SentiBank according to the previously discussed 1200 ANPs. With the trained 1200 ANP detectors, they are able to generate 1200 responses for any given test image using these pre-trained 1200 ANP detectors. A sentiment classifier is built on top of these mid-level features according to the sentiment label of training images. Sentribute (Yuan et al. 2013) also employed mid-level features for sentiment prediction. However, instead of using adjective noun pairs, they employed scene-based attributes (Patterson and Hays 2012) to define the mid-level features."}, {"heading": "Deep Learning on Flickr Dataset", "text": "We randomly choose 90% images from the half million Flickr images as our training dataset. The remaining 10% images are our testing dataset. We train the convolutional neural network with 300,000 iterations of mini-batches (each mini-batch contains 256 images). We employ the sampling probability in Eqn.(3) to filter the training images according to the prediction score of CNN on its training data. In the fine-tuning stage of PCNN, we run another 100,000 iterations of mini-batches using the filtered training dataset. Table 2 gives a summary of the number of data instances in our experiments. Figure 4 shows the filters learned in the first convolutional layer of CNN and PCNN, respectively. There are some differences between 4(a) and 4(b). While it is somewhat inconclusive that the neural networks have reached a better local optimum, at least we can conclude that the fine-tuning stage using a progressively cleaner training\ndataset has prompted the neural networks to learn different knowledge. Indeed, the evaluation results suggest that this fine-tuning leads to the improvement of performance.\nTable 3 shows the performance of both CNN and PCNN on the 10% randomly chosen testing data. PCNN outperformed CNN in terms of Precision, Recall, F1 and Accuracy. The results in Table 3 and the filters from Figure 4 shows that the fine-tuning stage of PCNN can help the neural network to search for a better local optimum."}, {"heading": "Twitter Testing Dataset", "text": "We also built a new image dataset from image tweets. Image tweets refer to those tweets that contain images. We built a total of 1269 images as our candidate testing images. We employed crowd intelligence, Amazon Mechanical Turk (AMT), to generate sentiment labels for these testing images, in a similar fashion to (Borth et al. 2013b). We recruited 5 AMT workers for each of the candidate image. Table 4 shows the statistics of the labeling results from the Amazon Mechanical Turk. In the table, \u201cfive agree\u201d indicates that all the 5 AMT workers gave the same sentiment label for a given image. Only a small portion of the images, 153 out of 1269, had significant disagreements between the\nTable 5: Performance of different algorithms on the Twitter image dataset (Acc stands for Accuracy).\nAlgorithms Five Agree At Least Four Agree At Least Three AgreePrecision Recall F1 Acc Precision Recall F1 Acc Precision Recall F1 Acc CNN 0.749 0.869 0.805 0.722 0.707 0.839 0.768 0.686 0.691 0.814 0.747 0.667 PCNN 0.77 0.878 0.821 0.747 0.733 0.845 0.785 0.714 0.714 0.806 0.757 0.687\n5 workers (3 vs. 2). We evaluate the performance of Convolutional Neural Networks on this manually labeled image dataset according to the model trained on Flickr images. Table 5 shows the performance of the two frameworks. Not surprisingly, both models perform better on the less ambiguous image set (\u201cfive agree\u201d by AMT). Meanwhile, PCNN shows better performance than CNN on all the three labeling sets in terms of both F1 and accuracy. This suggests that the fine-tuning stage of CNN effectively improves the generalizability extensibility of the neural networks."}, {"heading": "Transfer Learning", "text": "Half million Flickr images are used in our CNN training. The features learned are generic features on these half million images. Table 5 shows that these generic features also have the ability to predict visual sentiment of images from other domains. The question we ask is whether we can further improve the performance of visual sentiment analysis on Twitter images by inducing transfer learning. In this section, we conduct experiments to answer this question.\nThe users of Flickr are more likely to spend more time on taking high quality pictures. Twitter users are likely to share the moment with the world. Thus, most of the Twitter images are casually taken snapshots. Meanwhile, most of the images are related to current trending topics and personal experiences, making the images on Twitter much diverse in content as well as quality.\nIn this experiment, we fine-tune the pre-trained neural network model in the following way to achieve transfer learning. We randomly divide the Twitter images into 5 equal partitions. Every time, we use 4 of the 5 partitions to fine-tune our pre-trained model from the half million Flickr images and evaluate the new model on the remaining partition. The averaged evaluation results are reported. The algorithm is detailed in Algorithm 2.\nSimilar to (Borth et al. 2013b), we also employ 5-fold cross-validation to evaluate the performance of all the baseline algorithms. Table 6 summarizes the averaged performance results of different baseline algorithms and our two CNN models. Overall, both CNN models outperform the baseline algorithms. In the baseline algorithms, Sentribute gives slightly better results than the other two baseline al-\nFigure 5: Positive (top block) and Negative (bottom block) examples. Each column shows the negative example images for each algorithm (PCNN, CNN, Sentribute, Sentibank, GCH, LCH, GCH+BoW, LCH+BoW). The images are ranked by the prediction score from top to bottom in a decreasing order.\nAlgorithm 2 Transfer Learning to fine-tune CNN Input: X = {x1, x2, . . . , xn} a set of images of size 256\u00d7\n256 Y = {y1, y2, . . . , yn} sentiment labels of X Pre-trained CNN model M\n1: Randomly partition X and Y into 5 equal groups {(X1, Y1), . . . , (X5, Y5)}. 2: for i from 1 to 5 do 3: Let (X \u2032, Y \u2032) = (X,Y )\u2212 (Xi, Yi) 4: Fine-tune M with input (X \u2032, Y \u2032) to obtain model Mi\n5: Evaluate the performance of Mi on (Xi, Yi) 6: end for 7: return The averaged performance of Mi on (Xi, Yi) (i\nfrom 1 to 5)\ngorithms. Interestingly, even the combination of using low-\nlevel features local color histogram (LCH) and bag of visual words (BoW) shows better results than SentiBank on our Twitter dataset. Both fine-tuned CNN models have been improved. This improvement is significant given that we only use four fifth of the 1269 images for domain adaptation. Both neural network models have similar performance on all the three sets of the Twitter testing data. This suggests that the fine-tuning stage helps both models to find a better local minimum. In particular, the knowledge from the Twitter images starts to determine the performance of both neural networks. The previously trained model only determines the start position of the fine-tuned model.\nMeanwhile, for each model, we respectively select the top 5 positive and top 5 negative examples from the 1269 Twitter images according to the evaluation scores. Figure show those examples for each model. In both figures, each column contains the images for one model. A green solid box means the prediction label of the image agrees with the human label. Otherwise, we use a red dashed box. The labels of top ranked images in both neural network models are all correctly predicted. However, the images are not all the same. This on the other hand suggests that even though the two models achieve similar results after fine-tuning, they may have arrived at somewhat different local optima due to the different starting positions, as well as the transfer learning process. For all the baseline models, it is difficult to say which kind of images are more likely to be correctly classified according to these images. However, we observe that there are several mistakenly classified images in common among the models using low-level features (the four rightmost columns in Figure ). Similarly, for Sentibank and Sentribute, several of the same images are also in the top ranked samples. This indicates that there are some common learned knowledge in the low-level feature models and mid-level feature models."}, {"heading": "Conclusions", "text": "Visual sentiment analysis is a challenging and interesting problem. In this paper, we adopt the recent developed convolutional neural networks to solve this problem. We have designed a new architecture, as well as new training strategies to overcome the noisy nature of the large-scale training samples. Both progressive training and transfer learning inducted by a small number of confidently labeled images\nfrom the target domain have yielded notable improvements. The experimental results suggest that convolutional neural networks that are properly trained can outperform both classifiers that use predefined low-level features or mid-level visual attributes for the highly challenging problem of visual sentiment analysis. Meanwhile, the main advantage of using convolutional neural networks is that we can transfer the knowledge to other domains using a much simpler finetuning technique than those in the literature e.g., (Duan et al. 2012).\nIt is important to reiterate the significance of this work over the state-of-the-art (Siersdorfer et al. 2010; Borth et al. 2013b; Yuan et al. 2013). We are able to directly leverage a much larger weakly labeled data set for training, as well as a larger manually labeled dataset for testing. The larger data sets, along with the proposed deep CNN and its training strategies, give rise to better generalizability of the trained model and higher confidence of such generalizability. In the future, we plan to develop robust multimodality models that employ both the textual and visual content for social media sentiment analysis. We also hope our sentiment analysis results can encourage further research on online user generated content.\nWe believe that sentiment analysis on large scale online user generated content is quite useful since it can provide more robust signals and information for many data analytics tasks, such as using social media for prediction and forecasting. In the future, we plan to develop robust multimodality models that employ both the textual and visual content for social media sentiment analysis. We also hope our sentiment analysis results can encourage further research on online user generated content."}, {"heading": "Acknowledgments", "text": "This work was generously supported in part by Adobe Research. We would like to thank Digital Video and Multimedia (DVMM) Lab at Columbia University for providing the half million Flickr images and their machine-generated labels."}], "references": [{"title": "B", "author": ["S. Asur", "Huberman"], "venue": "A.", "citeRegEx": "Asur and Huberman 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Modeling public mood and emotion: Twitter sentiment and socio-economic phenomena", "author": ["Mao Bollen", "J. Pepe 2011] Bollen", "H. Mao", "A. Pepe"], "venue": null, "citeRegEx": "Bollen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bollen et al\\.", "year": 2011}, {"title": "Twitter mood predicts the stock market", "author": ["Mao Bollen", "J. Zeng 2011] Bollen", "H. Mao", "X. Zeng"], "venue": "Journal of Computational Science", "citeRegEx": "Bollen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bollen et al\\.", "year": 2011}, {"title": "Sentibank: large-scale ontology and classifiers for detecting sentiment and emotions in visual content", "author": ["Borth"], "venue": "In ACM MM,", "citeRegEx": "Borth,? \\Q2013\\E", "shortCiteRegEx": "Borth", "year": 2013}, {"title": "Large-scale visual sentiment ontology and detectors using adjective noun pairs", "author": ["Borth"], "venue": "In ACM MM,", "citeRegEx": "Borth,? \\Q2013\\E", "shortCiteRegEx": "Borth", "year": 2013}, {"title": "Learned-norm pooling for deep neural networks. CoRR abs/1311.1780", "author": ["\u00c7aglar G\u00fcl\u00e7ehre"], "venue": null, "citeRegEx": "G\u00fcl\u00e7ehre,? \\Q2013\\E", "shortCiteRegEx": "G\u00fcl\u00e7ehre", "year": 2013}, {"title": "L", "author": ["D.C. Cire\u015fan", "U. Meier", "J. Masci", "Gambardella"], "venue": "M.; and Schmidhuber, J.", "citeRegEx": "Cire\u015fan et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Enhanced sentiment learning using twitter hashtags and smileys", "author": ["Tsur Davidov", "D. Rappoport 2010] Davidov", "O. Tsur", "A. Rappoport"], "venue": "In ICL,", "citeRegEx": "Davidov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Davidov et al\\.", "year": 2010}, {"title": "Visual event recognition in videos by learning from web data", "author": ["Duan"], "venue": "IEEE PAMI 34(9):1667\u20131680", "citeRegEx": "Duan,? \\Q2012\\E", "shortCiteRegEx": "Duan", "year": 2012}, {"title": "Deep convolutional networks for scene parsing", "author": ["Bottou Grangier", "D. Collobert 2009] Grangier", "L. Bottou", "R. Collobert"], "venue": "In ICML 2009 Deep Learning Workshop,", "citeRegEx": "Grangier et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Grangier et al\\.", "year": 2009}, {"title": "and Eck", "author": ["P. Hamel"], "venue": "D.", "citeRegEx": "Hamel and Eck 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "G", "author": ["Hinton"], "venue": "E.; Osindero, S.; and Teh, Y.-W.", "citeRegEx": "Hinton. Osindero. and Teh 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Unsupervised sentiment analysis with emotional signals. In WWW, 607\u2013618", "author": ["Hu"], "venue": "International World Wide Web Conferences Steering Committee", "citeRegEx": "Hu,? \\Q2013\\E", "shortCiteRegEx": "Hu", "year": 2013}, {"title": "The wisdom of social multimedia: using flickr for prediction and forecast", "author": ["Jin"], "venue": "In ACM MM,", "citeRegEx": "Jin,? \\Q2010\\E", "shortCiteRegEx": "Jin", "year": 2010}, {"title": "J", "author": ["D. Joshi", "R. Datta", "E. Fedorovskaya", "Q.-T. Luong", "Wang"], "venue": "Z.; Li, J.; and Luo, J.", "citeRegEx": "Joshi et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning convolutional feature hierarchies for visual recognition", "author": ["Kavukcuoglu"], "venue": "In NIPS,", "citeRegEx": "Kavukcuoglu,? \\Q2010\\E", "shortCiteRegEx": "Kavukcuoglu", "year": 2010}, {"title": "G", "author": ["A. Krizhevsky", "I. Sutskever", "Hinton"], "venue": "E.", "citeRegEx": "Krizhevsky. Sutskever. and Hinton 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "L", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "Jackel"], "venue": "D.", "citeRegEx": "LeCun et al. 1989", "shortCiteRegEx": null, "year": 1989}, {"title": "P", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "Haffner"], "venue": "1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11):2278\u2013", "citeRegEx": "LeCun et al. 1998", "shortCiteRegEx": null, "year": 2324}, {"title": "Convolutional networks and applications in vision", "author": ["Kavukcuoglu LeCun", "Y. Farabet 2010] LeCun", "K. Kavukcuoglu", "C. Farabet"], "venue": "In ISCAS,", "citeRegEx": "LeCun et al\\.,? \\Q2010\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2010}, {"title": "S", "author": ["Li, G.", "Hoi"], "venue": "C.; Chang, K.; and Jain, R.", "citeRegEx": "Li et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "P", "author": ["B. Liu", "Y. Dai", "X. Li", "W.S. Lee", "Yu"], "venue": "S.", "citeRegEx": "Liu et al. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "A", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "Ng"], "venue": "Y.; and Potts, C.", "citeRegEx": "Maas et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Towards multimodal sentiment analysis: Harvesting opinions from the web", "author": ["Mihalcea Morency", "L.-P. Doshi 2011] Morency", "R. Mihalcea", "P. Doshi"], "venue": "In ICMI,", "citeRegEx": "Morency et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Morency et al\\.", "year": 2011}, {"title": "From tweets to polls: Linking text sentiment to public opinion time series", "author": ["O\u2019Connor"], "venue": null, "citeRegEx": "O.Connor,? \\Q2010\\E", "shortCiteRegEx": "O.Connor", "year": 2010}, {"title": "and Lee", "author": ["B. Pang"], "venue": "L.", "citeRegEx": "Pang and Lee 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "and Hays", "author": ["G. Patterson"], "venue": "J.", "citeRegEx": "Patterson and Hays 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Analyzing and predicting sentiment of images on the social web", "author": ["Siersdorfer"], "venue": "In ACM MM,", "citeRegEx": "Siersdorfer,? \\Q2010\\E", "shortCiteRegEx": "Siersdorfer", "year": 2010}, {"title": "I", "author": ["A. Tumasjan", "T.O. Sprenger", "P.G. Sandner", "Welpe"], "venue": "M.", "citeRegEx": "Tumasjan et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Sentribute: image sentiment analysis from a mid-level perspective", "author": ["Yuan"], "venue": "In Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining,", "citeRegEx": "Yuan,? \\Q2013\\E", "shortCiteRegEx": "Yuan", "year": 2013}, {"title": "P", "author": ["X. Zhang", "H. Fuehres", "Gloor"], "venue": "A.", "citeRegEx": "Zhang. Fuehres. and Gloor 2011", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [], "year": 2015, "abstractText": "Sentiment analysis of online user generated content is important for many social media analytics tasks. Researchers have largely relied on textual sentiment analysis to develop systems to predict political elections, measure economic indicators, and so on. Recently, social media users are increasingly using images and videos to express their opinions and share their experiences. Sentiment analysis of such large scale visual content can help better extract user sentiments toward events or topics, such as those in image tweets, so that prediction of sentiment from visual content is complementary to textual sentiment analysis. Motivated by the needs in leveraging large scale yet noisy training data to solve the extremely challenging problem of image sentiment analysis, we employ Convolutional Neural Networks (CNN). We first design a suitable CNN architecture for image sentiment analysis. We obtain half a million training samples by using a baseline sentiment algorithm to label Flickr images. To make use of such noisy machine labeled data, we employ a progressive strategy to fine-tune the deep network. Furthermore, we improve the performance on Twitter images by inducing domain transfer with a small number of manually labeled Twitter images. We have conducted extensive experiments on manually labeled Twitter images. The results show that the proposed CNN can achieve better performance in image sentiment analysis than competing algorithms.", "creator": "LaTeX with hyperref package"}}}