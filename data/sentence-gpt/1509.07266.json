{"id": "1509.07266", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Sep-2015", "title": "CRDT: Correlation Ratio Based Decision Tree Model for Healthcare Data Mining", "abstract": "The phenomenal growth in the healthcare data has inspired us in investigating robust and scalable models for data mining. For classification problems Information Gain(IG) based Decision Tree is one of the popular choices. However, depending upon the nature of the dataset, IG based Decision Tree may not always perform well as it prefers the attribute with more number of distinct values as the splitting attribute, and it also may not always perform well for a full set of data.\n\n\nIn order to understand the data mining method, we first need to understand the problem of choice. It can be summarized with a simple example:\nOne thing you need to understand is that a single variable in the data can be divided into two values. When one is a different value, one can compute the following:\nYou can choose to choose to classify all data for one of the two values as one of the two values. You can choose the value of each of those values to use the same attributes.\nTo do this, let's compare the results of the results using the DataGrowth.numerical. In order to evaluate the dataset, we have to look at the output of the dataset as shown below:\nTo find the data, we must look at the data with the highest accuracy in all data and compare the results of the dataset with the highest accuracy in all data.\nThe data are also of great value. In this example, we have to compare the results of the dataset with the lowest accuracy in all data and compare the results of the dataset with the highest accuracy in all data.\nThe data are also of great value. In this example, we have to look at the data with the highest accuracy in all data and compare the results of the dataset with the highest accuracy in all data.\nHere we have the data using the DataGrowth.numerical. In this example, we have to look at the data with the highest accuracy in all data and compare the results of the dataset with the highest accuracy in all data.\nHere we have the data using the DataGrowth.numerical. In this example, we have to look at the data with the highest accuracy in all data and compare the results of the dataset with the highest accuracy in all data.\nThe data are also of great value. In this example, we have to look at the data with the highest accuracy in all data and compare the results of the dataset with the highest accuracy in all data.\nIt is important to note that the DataGrowth.numerical", "histories": [["v1", "Thu, 24 Sep 2015 07:57:27 GMT  (13kb,D)", "http://arxiv.org/abs/1509.07266v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.DB", "authors": ["smita roy", "samrat mondal", "asif ekbal"], "accepted": false, "id": "1509.07266"}, "pdf": {"name": "1509.07266.pdf", "metadata": {"source": "CRF", "title": "CRDT: Correlation Ratio Based Decision Tree Model for Healthcare Data Mining", "authors": ["Smita Roy", "Samrat Mondal", "Asif Ekbal"], "emails": ["smitaroy@cub.ac.in", "samrat@iitp.ac.in", "asif@iitp.ac.in"], "sections": [{"heading": null, "text": "Index Terms\nData Mining, Healthcare, Decision Tree, Information Gain, Correlation Ratio.\nI. INTRODUCTION\nDue to the growth of Internet technology and healthcare software, data are available in abundance in unstructured form as all as structured form over the Internet. We are rich in information but lack of knowledge. So this led to the path to healthcare data mining. Healthcare data mining techniques find hidden but useful patterns from healthcare datasets.\nDiseases like heart-diseases, hepatitis, diabetes are very common among all ages of patients. Some alarming statistics regarding those diseases are given below:\n\u2022 One-third of all deaths in India will be caused by cardiovascular disease by the year 2020.[1]\n\u2022 In middle-income countries, diabetes is one among the top 10 life threatening diseases.[2].\nAll these observations are made after some careful analysis of different healthcare datasets. And such observations often change the focus of the government policies.\nThere are different data mining algorithms - such as Decision Tree algorithm[3], Naive Bayes Classifier [4][5][3], Neural Network model [6][3], k-Nearest Neighbour algorithm[3], Support Vector Machine [7][3], K-means [8][3], Bisecting K-means algorithm [9], Association Rule mining algorithms [10][3] etc. Among them some are used for classification, some for clustering purposes and some others for finding easily interpretable rules for taking proper decision. Decision Tree algorithm is a very popular algorithm for classifications. It generally uses Information Gain (IG) as the criterion for splitting on an attribute. The attribute with the highest IG is chosen as the splitting attribute at each level. But Information Gain has some disadvantages like it prefers the attribute which has large number of distinct values. So if there is an attribute in the dataset like product-ID, then Information Gain approach will prefer to split on product-ID as because this attribute can uniquely identify each tuple in the set and this would result in a large number of partitions (as many as there are values), each one will have just one tuple. Since there will be no records with different class labels in each partition, the required information to classify data set D based on this partitioning based on Information Gain [3] principle would be Info product-ID(D)=0. Therefore, the information gained by partitioning on this attribute is highest. Therefore, such a partitioning is useless for classification [3]. So it will not generalize the model. So, IG based approach is not effective for all types of datasets. For instance, if a dataset has different attributes with different numbers of distinct values, it prefers the attributes with more numbers of distinct values as splitting attributes though some other attributes with less number of distinct values may be more significant for classification.\nDue to this reason for some dataset IG based approach does not provide adequate accuracy. To overcome this we have proposed an approach which uses the concept of Correlation Ratio [11] or CR as the splitting criterion. This method has no such biasness.\nar X\niv :1\n50 9.\n07 26\n6v 1\n[ cs\n.A I]\n2 4\nSe p\n20 15\nIt considers that attribute for classification which is significant enough to identify at least one outcome class. The general CR method is suitable for quantative data. But our proposed CR based approach is applicable to nominal or categorical attributes also.\nThe organization of the paper is as follows: Section II describes the related work done in this area. Section III illustrates our proposed approach. The result and analysis of the proposed approach is shown in Section IV. Section V concludes the paper and gives future direction of the work."}, {"heading": "II. RELATED WORK", "text": "Decision Tree technique has been popularly used for finding interesting patterns in health care datasets. We will discuss next some of the relevant works demonstrating this fact.\nPolat et al.[12] proposed a hybrid model for classification of multiclass dataset. For each class separate models based on C4.5 Decision Tree algorithm has been constructed and the class for which the model is built is given positive class label and the rest of the classes are assigned negative class label. The proposed model showed significant performance improvement over the traditional C4.5 Decision Tree based model. As optimization of dataset can improve classification accuracy, some more methods like Homogeneity-based algorithm (HBA) etc have been proposed by Pham et al.[13]. This algorithm was used in association with standard classification algorithms as SVM, DT and ANN to enhance their performances. The four parameters of HBA were then optimized by Genetic Algorithm. The proposed approach showed significant performance improvement over standard approaches. Decision Tree induction method has a wide variety of applications as discussed previously. Changala et al. [14] have discussed different aspects of the Decision Tree induction method in the paper. Since most of the learning algorithms require the dataset to be in memory, it is a matter of concern for huge datasets. So, in this paper the scalability issues have also been discussed.\nKaraolis et al.[15] used Decision Tree based models to find out the risk factors for three types of Coronary heart disease events - myocardial infarction (MI), percutaneous coronary intervention (PCI), and coronary artery bypass graft surgery (CABG). The models for PCI and CABG have performed well with 75% classification accuracy compared to the model for MI. A predictive model for determining the ability of the persons affected with dementia to take help of a technology based on mobile phone based video streaming system has been developed by Zhang et al.[16]. The dataset was having two classes : Adopter and Non-Adopter. Popular classification algorithms were used for building models. Experimental results shows that among all the models DT, NN, SVM and kNN based models performed well. G. Sathyadevi [17] proposed to build intelligent decision support system for Hepatitis disease diagnosis. At first most relevant attributes were selected based on some threshold value or some condition. Then CART(Classification And Regression Trees) Decision Tree algorithm was applied on the dataset which showed 83.2% accuracy and it was relatively higher as compared to the accuracy obtained from models using ID3 and C4.5 Decision Tree algorithms. Some significant rules were extracted after constructing the Decision Tree using CART. Decision Tree as a prediction model has also been used by the author in [18] to predict hepatitis C virus(HCV) polyprotein cleavage sites. The challenge was to collect accurate data. The model gave a very good result with a 96% accuracy which was slightly lesser than the accuracy(97%) of the model based on Support Vector Machine(SVM).\nOverall the above survey demonstrates the effectiveness of Decision Tree technique for efficient healthcare."}, {"heading": "III. PROPOSED APPROACH", "text": "In this section, we give the details of our proposed Correlation Ratio based Decision Tree construction approach."}, {"heading": "A. Overview of Correlation Ratio", "text": "Sometimes, the expected outcome of our learning algorithm is some categorical values like \u201cyes/no\u201d. The Correlation Coefficient method is suitable for applications where the outcome is quantitative, thus it cannot be applied in those cases where categorical outcome is desired. To sort out this problem, the CR[11] method can be applied.\nThe CR method can be employed to partition the sample dataset into different categories according to the observed outcome. A significant attribute is one which can identify at least one outcome class where the average value of the attribute and the average on all classes are remarkably different, otherwise that attribute would not be useful to identify any outcome. Table I provides a summary of the different notations used in our proposed approach and the corresponding meanings.\nSuppose that there is a set of ` tuples in a dataset. Let the number of times that outcome y \u2208 Y occurs is `y , so that the dataset can be partitioned by their outcome as follows:\nwhere Sy is the set of all tuples with outcome y and x (i) jy is the value of the i -th attribute of the j-th tuple among all the `y tuples with outcome y . The average of the i-th attribute from all tuples within each outcome class is given by:\n\u2200y \u2208 Y |x(i)y = \u2211`y j=1 x (i) jy\n`y (2)\nand the overall average of the i-th attribute from all tuples is :\nx(i) =\n\u2211 y\u2208Y \u2211`y j=1 x (i) jy\n` =\n\u2211 y\u2208Y `yx (i) y\n` (3)\nThe square of CR[11] between the i-th attribute of the dataset and the outcome or the class attribute is given by\nCr2i =\n\u2211 y\u2208Y `y(x\n(i) y \u2212 x(i))2\u2211 y\u2208Y \u2211`y j=1 (x (i) jy \u2212 x(i))2\n(4)\nIf the value of the i-th attribute of the dataset and value of the outcome are linearly related, then both the Correlation Coefficient and the CR will have same value which is equal to the slope of the dependence.\nThe CR is able to capture non-linear dependencies in all other cases."}, {"heading": "B. Example for Computation of Correlation Ratio", "text": "The computation of CR can be illustrated using the following example where we have considered a dataset (shown in Table II) of 15 patients of different age groups - teenager, middle-aged, old and the class attribute(Y) for the dataset is Age-group.\nThe labels of attribute Age-group(Y) is denoted as y where y \u2208teenager, middle-aged, old. Let the i-th attribute (where i=1) in the dataset is BP. The values of the attribute Blood pressure (BP) for these different sets of patients are given as:\nFor y = teenager, Steenager = (x (i) jy ); j = 1, \u00b7 \u00b7 \u00b7 , 5\n= 60, 75, 70, 80, 65 (BP values of 5 teenager patients)\nFor y = middle\u2212 aged, Smiddle\u2212aged = (x(i)jy ); j = 1, \u00b7 \u00b7 \u00b7 , 4\n= {80, 75, 85, 72} (BP values of 4 middle-aged patients)\nFor y = old, Sold = (x (i) jy ); j = 1, \u00b7 \u00b7 \u00b7 , 6\n= {90, 80, 120, 100, 95, 85} (BP values of 6 old patients)\nFor y = teenager, average of BP(i-th attribute),\nx (BP ) teenager = 70\nFor y = middle\u2212 aged, average of BP(i-th attribute),\nx (BP ) middle\u2212aged = 78\nFor y = old, average of BP(i-th attribute),\nx (BP ) old = 95\nOverall average of BP for these three different age-group patients, x(BP ) = 82\nThe weighted sum of square of the differences between the average BP of each group of patients and the overall average is\n= 5(70\u2212 82)2 + 4(78\u2212 82)2 + 6(95\u2212 82)2\n= 1798\nwhereas the overall sum of the squares of the differences between the individual BP and the overall average BP is:\n(60\u2212 82)2 + (75\u2212 82)2 + (70\u2212 82)2 + (80\u2212 82)2 + (65\u2212 82)2 + (80\u2212 82)2 + (75\u2212 82)2 + (85\u2212 82)2 + (72\u2212 82)2 + (90\u2212 82)2 + (80\u2212 82)2 + (120\u2212 82)2 + (100\u2212 82)2 + (95\u2212 82)2 + (85\u2212 82)2 = 3146\nThus, from equation (7),\nCr2BP = 1798 3146 = 0.572 CrBP = 0.756"}, {"heading": "C. Proposed Algorithms", "text": "The CR approach that has been discussed above is basically applicable to quantitative data. So, we propose an approach based on the concept of CR which will be applicable on dataset having nominal or categorical attributes.\nAlgorithm 1 Constructing CR based Decision Tree(D,N`) 1: //Inputs: Dataset D and node N` 2: Let A = A1, A2, \u00b7 \u00b7 \u00b7 , An be the set of n attributes for the tuples in D 3: if all the tuples in D have the same class label then 4: Return N` as a leaf node labelled with the class label 5: else 6: for each attribute Ai do 7: Cri = Correlation ratio(Ai, Y ) , where Y is the class attribute 8: Insert Cri in set CR, where CR is the set of Correlation ratios. 9: end for\n10: r = max(CR) 11: if multiple Ai have Cri == r then 12: choose the attribute Ai as the splitting attribute which has most of the possible distinct values present in D 13: else 14: Choose the attribute Ai as the splitting attribute which has Cri == r 15: end if 16: Label node N` with attribute Ai 17: Let attribute Ai has m distinct values Ai,1,Ai,2,...,Ai,m 18: Divide D into m partitions D = D1, D2, . . . , Dm corresponding to each distinct value of Ai respectively, and create a\nchild node N`j corresponding to each partition from node N` with corresponding distinct value of attribute Ai as the label on the branch\n19: for each partition Dj in D do 20: if that partition is empty then 21: Label node N`j as a leaf node with the majority class in D 22: else 23: Call DecisionTree(Dj ,N`j) 24: end if 25: end for 26: end if\nAlgorithm 2 Compute CR(Ai,Y ) 1: //Inputs: Ai, Y 2: Let attribute Ai has m distinct values with respect to D 3: Let class attribute Y has l distinct labels Y1,Y2,.....Yl 4: for each class label Yj of Y do 5: for each distinct value Ai,k of Ai with class label Yj do 6: fk = frequency(A Yj i,k)\n7: Insert fk in set FYj 8: end for 9: mYj = max(FYj )\n10: x\u0304iYj = mYj tYj\n, where tYj is the total occurrence of records in the dataset D with class label Yj . 11: end for 12: for each attribute Ai do 13: Call Avg(Y1,Y2,....,Yl,mY1 ,mY2 ,...mYl ,tY1 ,tY2 ,...,tYl ) // This function returns the value x\u0304 i = \u2211 Yj\u2208Y\nmj\u2211 Yj\u2208Y tYj ,where\nx\u0304i is the overall average of the i-th attribute 14: end for 15: Call Disin(Y1,Y2,....,Yl,tY2 ,...,tYl ,x\u0304 i Y1 , x\u0304iY2 ,... x\u0304 i Yl\n,x\u0304i) //which returns the dispersion among individual classes as din = \u2211 Yj\u2208Y tYj (x\u0304 i Yj \u2212 x\u0304i)2 , where x\u0304iYj is the average of the i-th attribute within each outcome class Yj .\n16: Call Disov(Y1,Y2,....,Yl,m,xi(1,Y1),x i (2,Y1),...,x i (m,Y1),x i (1,Y2),x i (2,Y2),...,x i (m,Y2),...,x i (1,Yl),x i (2,Yl), ...,x i (m,Yl),x\u0304 i) // which returns the dispersion across whole population as dov = \u2211 Yj\u2208Y \u2211m a=1(x i (a,Yj) \u2212 x\u0304i)2, where xia,Yj is the\nfrequency of occurrence of the a\u2212 th distinct value of Ai with class label Yj . 17: Compute Correlation Ratio square Cr2Ai as the ratio of din and dov 18: Return square-root of Cr2Ai as Correlation Ratio CrAi\nWe show in Algorithm 1 how to create a Decision Tree using CR as the splitting criterion. A root node is built corresponding to the whole dataset. CR based approach is used to split the dataset further. CR between each attribute and the class attribute is calculated at each level of Decision Tree construction and the attribute with the highest CR value with the Class attribute is chosen as the partitioning attribute for the dataset. The root node is labelled with the corresponding splitting attribute. The subtrees of the root node are built using the different distinct values of the splitting attribute as the branch labels and child nodes are created from the root node for each splitted sub-dataset respectively. If in any partition, all the tuples have the same class label, then label the corresponding leaf node with the corresponding class label. On the other hand, if any partition is empty, then mark the corresponding leaf node with the majority class label in its parent\u2019s partition. Iterate the same process until for each partition, all the data points have the same class labels.\nIn Algorithm 2 we have shown various steps to compute CR. When calculating the average value of the i-th attribute within each outcome class we have taken the ratio of the highest frequency value of occurrence for a distinct value of i-th attribute for that class and the total occurrence of records with that outcome class. Thereafter the overall average of the i-th attribute is calculated. Then the ratio of the dispersion among individual classes and the dispersion across the whole population for the i-th attribute is calculated which is actually the square of the CR for the i-th attribute from which the square root is calculated to get the actual CR value.\nThe computation of significance of an attribute using Algorithm 2 is illustrated using the following example in Table III:\nIn Table III, we consider Temperature as the first attribute of a dataset and it has three possible values - Hot, Mild, Cool. There are two classes - No and Yes. The frequencies of the attribute values for each of the class is shown using the numeric values in each cell of the table. The maximum frequency value for the attribute Temperature is used to calculate the average weight of the attribute in each class. The overall average weight x(1) of the attribute is the ratio of the summation of the maximum frequencies of the two classes and the total number of instances in the two classes. The significance of the attribute Temperature for predicting class Y, CrTemperature is calculated as shown in the example. The continuous attributes in the datasets taken from UCI machine learning repository [19] were discretized. In each level while constructing the Decision Tree, we have considered the attribute which has the highest value of CR with the class attribute."}, {"heading": "IV. OBSERVATIONS ON BENCHMARK HEALTHCARE DATASETS", "text": "Several datasets like Pima Indian Diabetes dataset, Liver Disorder dataset, Mammography Masses dataset, Breast Cancer dataset, Hepatitis dataset, Post-operative dataset, ILPD dataset and Spect-heart datasets from UCI machine learning repository were considered for performance evaluation of our proposed approach."}, {"heading": "A. About the datasets", "text": "Table IV shows the characteristics of the datasets considered here.\nThe Pima Indian Diabetes dataset has overall 9 attributes including the class attribute which is categorical (tested positive for diabetes(1), tested negative for diabetes(0)). All the attributes are numeric-valued except the class attribute. There were 768 instances. Among these 500 instances are having class value 0 and 268 instances are having class value 1.\nThe Mammography Masses dataset consists of 961 instances and 6 attributes in which one of the attribute is the class attribute (possible values - 0 and 1). One attribute was of type integer and the other attributes are nominal or ordinal. The integer attribute was discretized into 5 different values. Two of the ordinal attributes have 5 different values and remaining two ordinal attributes have 4 different values. There were some missing values in almost all of the attributes. Class distribution was like : benign(0)- 516, malignant(1)- 445.\nThe Breast Cancer dataset has 699 instances and total 10 attributes. The class attribute takes two possibles values - 2 for benign and 4 for malignant. Except the class attribute and another attribute which is indicating the id number, rest of the attributes can take values in the the range 1-10. There are 458 instances which are Benign and 248 Malignant instances. Missing values were present in 16 instances.\nThe Hepatitis dataset has overall 155 instances and 20 attributes of different types like categorical, integer and real. Six of the attributes are of integer type and were discretized into five discrete values. Thirteen attributes are categorical each having two possible values. The class attribute is categorical and can take two possible values - DIE(32 instances) and LIVE(123 instances). There are missing values in many attributes.\nThe Post-operative dataset is having 9 attributes (including the class attribute) and 90 instances. Attributes are of different types like - categorical and integer. There are seven categorical attributes and one integer attribute. The class attribute is categorical and can take three possible values - I (patient sent to Intensive Care Unit): 2 instances, S (patient prepared to go home): 24 instances and A (patient sent to general hospital floor): 64 instances. There were no missing values.\nThe Indian Liver Patient Dataset(ILPD) contains 583 instances and 10 attributes of different categories like integer and real. Among 10 attributes 9 are integer type and 1 attribute is categorical. Each of the integer type attributes are discretised into five distinct categorical values. The class label attribute can take two possible values \u20191\u2019 and \u20192\u2019. There are 416 instances with class label \u20191\u2019 and \u2019167\u2019 instances with class label \u20192\u2019.\nThe Spect Heart dataset consists of 267 instances and 23 attributes including the class attribute. All the attributes are binary. The class attribute can have 2 possible values \u20190\u2019 and \u20191\u2019 and class distribution is - 55 examples with class label \u20190\u2019 and 212 examples with class labels \u20191\u2019. It has no missing values.\nThe Statlog (heart) dataset has 13 attributes where one of the attributes is the class attribute. The attributes are of different types like Real, Ordered, Nominal and Binary and there are total 270 observations and two classes. Out of six real attributes, five are transformed into five categorical values and the remaining one real attribute is transformed into four categorical values. Three attributes are binary. One attribute is ordered and has three different values. Three attributes are nominal out of which one attribute is having four different values and two attributes is having three different values.\nThe continuous attributes of different datasets are discretized. The natures of the datasets after discretization has been shown in Table IV. For most of the datasets k-fold cross validation has been used where the dataset is divided into k disjoint subsets and (k-1) subsets are used for training and the remaining subset is used for testing. This process is repeated k-number of times and the results of all the iterations are combined together[3]. For the Post-Operative dataset, it is divided into training and test sets in the ratio of 70:30. In this case, as the dataset is very small, so cross validation is not used. The Spect-heart dataset is already divided into separate training (80 instances) and test sets(187 instances)."}, {"heading": "B. Result and Analysis", "text": "Next we have applied our proposed technique on the datasets discussed in Subsection IV-A.\nTable V shows that for Pima Indian Diabetes, Mammography and Breast Cancer datasets the IG based approach has performed slightly better than CR based approach. For two of these datasets (Pima Indian Diabetes, Breast Cancer) more number of attributes are there and all of the attributes have same number of distinct values. One dataset (Mammography) has different numbers of distinct values for the attributes but less number of attributes.\nFor the Spect-heart dataset both IG and CR approaches have given same performance (74.33% accuracy). This dataset also has same number of distinct values for all the attributes and has lots of attributes compared to the number of instances. For the Post-operative patient dataset both the approaches have given 62.96% accuracy. The reason behind the not-so-good performance can be attributed to the fact that there were less number of instances (90), more number of attributes (9) and more number of classes (3) compared to the other datasets. Also this dataset has different number of distinct values for different types of attributes.\nFor the ILPD dataset, almost all the attributes have same number of distinct values except one attribute. The CR based approach performed slightly better than the IG based approach. The CR approach outperformed the IG based approach for the Hepatitis dataset. 6 attributes have same number of distinct values and rest 13 attributes have less number of distinct values. The CR approach has given slightly better result for the Statlog (heart) dataset where the attributes have different numbers of distinct values and there are total 270 observations and two classes.\nIt is therefore observed from the analysis of the result that generally our proposed approach can handle datasets with same as well as different numbers of distinct valued attributes almost equally well because it is not biased towards attributes with large number of distinct values. On the other hand IG approach prefers attributes with many distinct values. In healthcare datasets where there are large number of attributes and different attributes have different numbers of distinct values, the IG based approach prefers attributes with more number of distinct values rather than attributes which have less number of distinct values even if those attributes may be more significant for classification. This gives a reason for lesser performance of the IG based than our proposed approach in those cases. Also another observation that has been made out of the results is that for smaller datasets where less number of instances and comparatively more number of attributes are there like Hepatitis and Statlog (heart)datasets, the proposed approach performs slightly better than the IG based approach. For this to be taken as a proven fact some more datasets need to be analysed in future."}, {"heading": "V. CONCLUSION", "text": "Healthcare datasets are available in plenty. The nature or the distribution patterns of such healthcare datasets may also vary. However we observe that such datasets generally have large number number of attributes and different attributes have different numbers of distinct values. Thus, applying existing IG based splitting criterion may not give good accuracy for all cases. So, in this paper, CR based Decision Tree learner is proposed. This technique serves as a complement of IG based technique i.e., when IG fails, CR based technique succeeds. We demonstrated this fact using some benchmark healthcare datasets. In future we would like to explore some more such datasets and apply our proposed technique.\nREFERENCES\n[1] http://neocardiabcare.com/alarming-statistics-india.htm. [2] Seyed Mohammad Kalantar Motamedi, Reza Majdzadeh, Fatemeh Ardeshir Larijani, Fakher Raheem, Zahra Koleini, and\nBagher Larijani. Potentially preventable incidence of diabetes due to risk factor modification. Journal of diabetes or metabolic disorders, Vol(11), 24th August 2012. [3] J. Han, M. Kamber, and J. Pei. Data Mining Concepts and Techniques. Morgan Kaufman, 2006. [4] G. H. John and P. Langley. Estimating continuous distributions in bayesian classifers. pages 338\u2013345, 1995. [5] Domingos P and Pazzani M. On the optimality of the simple bayesian classifier under zero-one loss. pages 29:103\u2013130,\n1997. [6] N. Murata, S. Yoshizawa, and S.-I. Amari. Network information criterion-determining the number of hidden units for an\nartificial neural network model. Vol(5):865 \u2013 872, November 1994. [7] V. N. Vapnik. The nature of statistical learning theory. Springer, 1995. [8] J. MacQueen. Some methods for classification and analysis of multivariate observations. In Proceedings of the Fifth\nSymposium on Math, Statistics and Probability, pages 281\u2013297. Berkeley,CA:University of California Press, 1967. [9] Steinbach M, Karypis G, and Kumar V. A comparison of document clustering techniques. In In: Proceedings of the\nKDD Workshop on Text Mining. Berkeley,CA:University of California Press, 2000. [10] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. In In: Proceedings of the 20th VLDB conference, pages 487\u2013499. Morgan Kaufmann Publishers Inc. San Francisco, CA, USA, 1994. [11] R. Battiti, M. Brunato, and F. Mascia. Reactive search and intelligent optimization, 2007. [12] K. Polat and S. Gunes. A novel hybrid intelligent method based on c4.5 decision tree classifier and one-against-all approach for multi-class classification problems. Expert Systems with Applications, Vol(36):1587\u20131592, March 2009. [13] Huy Nguyen Anh Pham and Evangelos Triantaphyllou. Prediction of diabetes by employing a new data mining approach which balances fitting and generalization. Computer and Information Science, SCI 131, pages 11\u201326, 2008. [14] R. Changala, A. Gummadi, G Yedukondalu, and UNPG Raju. Classification by decision tree induction algorithm to learn\ndecision trees from the class-labeled training tuples. International Journal of Advanced Research in Computer Science and Software Engineering, Vol(2):427\u2013434, April 2012.\n[15] D.Hadjipanayi M.A. Karaolis, J.A. Moutiris and C.S. Pattichis. Assessment of the risk factors of coronary heart events based on data mining with decision trees. IEEE Transactions On Information Technology In Biomedicine, Vol(14):559\u2013 566, May 2010. [16] S. Zhang, S.I. McClean, C.D. Nugent, M.P. Donnelly, L. Galway, B.W. Scotney, and I. Cleland. A predictive model for assistive technology adoption for people with dementia. IEEE Journal Of Biomedical And Health Informatics, Vol(18):375\u2013 383, January 2014. [17] G. Sathyadevi. Application of cart algorithm in hepatitis disease diagnosis. In IEEE-International Conference on Recent Trends in Information Technology,ICRTIT 2011, pages 1283\u20131287, June3-5 2011. [18] A. mohamed samir ali gamal eldin. A data mining approach for the prediction of hepatitis c virus protease cleavage sites. International Journal of Advanced Computer Science and Applications, Vol(2)(No.12), December 2011. [19] M. Lichman. UCI machine learning repository, 2013."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "The phenomenal growth in the healthcare data has inspired us in investigating robust and scalable models for data mining.<lb>For classification problems Information Gain(IG) based Decision Tree is one of the popular choices. However, depending upon<lb>the nature of the dataset, IG based Decision Tree may not always perform well as it prefers the attribute with more number of<lb>distinct values as the splitting attribute. Healthcare datasets generally have many attributes and each attribute generally has many<lb>distinct values. In this paper, we have tried to focus on this characteristics of the datasets while analysing the performance of our<lb>proposed approach which is a variant of Decision Tree model and uses the concept of Correlation Ratio(CR). Unlike IG based<lb>approach, this CR based approach has no biasness towards the attribute with more number of distinct values. We have applied<lb>our model on some benchmark healthcare datasets to show the effectiveness of the proposed technique.<lb>Index Terms Data Mining, Healthcare, Decision Tree, Information Gain, Correlation Ratio.", "creator": "LaTeX with hyperref package"}}}