{"id": "1509.07612", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2015", "title": "Sentiment Uncertainty and Spam in Twitter Streams and Its Implications for General Purpose Realtime Sentiment Analysis", "abstract": "State of the art benchmarks for Twitter Sentiment Analysis do not consider the fact that for more than half of the tweets from the public stream a distinct sentiment cannot be chosen. This paper provides a new perspective on Twitter Sentiment Analysis by highlighting the necessity of explicitly incorporating uncertainty. Moreover, a dataset of high quality to evaluate solutions for this new problem is introduced and made publicly available.\n\n\n\n\n\n\n\n\n\nThe findings of the project are described below and are intended to provide an overview of Twitter Sentiment Analysis (TAM) that would be useful to the general public.\nThe TAM is a project of the National Research Council, a nonpartisan think tank, founded in 2007. The goal of the project is to provide the following analyses of Twitter Sentiment Analysis (TAM).\nTo investigate the effect of TAM on the overall performance of Twitter Sentiment Analysis, please see the results of this paper\nA few observations in this paper have been made.\n1. The tAM is a subset of the Twitter Sentiment Analysis dataset, which is based on multiple analyses of tweets from the public.\n2. The dataset is a collection of 10 tweets. Each tweet contains at least two tAMs each, with each TAM in the dataset. Each tweet has at least one TAM in the dataset.\n3. The dataset contains at least one TAM in the dataset. All tweets have at least two TAMs each (tearches are also available) in the dataset, which in turn accounts for the difference between tweets with at least one TAM in the dataset.\n4. We will focus on a few things that we consider to be a bit lacking:\n1. Twitter Sentiment Analysis is not a single data-set. It is a whole collection of 10 tweets, with each TAM in the dataset.\n2. We will focus on an important element of Twitter Sentiment Analysis:\n1. The data-set includes the tweets the tweets the average user sees.\n2. Twitter Sentiment Analysis contains the tweets the average user sees.\n3. The dataset contains tweets the average user sees.\n4. The dataset contains tweets the average user sees.\n5. The dataset contains tweets the average user sees.\n6. The dataset contains tweets the average user sees.\n7. The dataset contains tweets the average user sees.\n8. The dataset contains tweets the average user sees.\n9. The dataset contains tweets the average user sees.\nIn each case,", "histories": [["v1", "Fri, 25 Sep 2015 07:55:26 GMT  (41kb,D)", "http://arxiv.org/abs/1509.07612v1", "3 pages, 1 figure, accepted at GSCL '15"]], "COMMENTS": "3 pages, 1 figure, accepted at GSCL '15", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["nils haldenwang", "oliver vornberger"], "accepted": false, "id": "1509.07612"}, "pdf": {"name": "1509.07612.pdf", "metadata": {"source": "CRF", "title": "Sentiment Uncertainty and Spam in Twitter Streams and Its Implications for General Purpose Realtime Sentiment Analysis", "authors": ["Nils Haldenwang", "Oliver Vornberger"], "emails": ["nils.haldenwang@uos.de", "oliver@uos.de"], "sections": [{"heading": "1 Introduction", "text": "As a field of research Twitter Sentiment Analysis has gained much attention recently. For a multitude of applications such as sales prediction (Asur and Huberman, 2010), stock market prediction (Bollen et al., 2011) or political debate analysis (Diakopoulos and Shamma, 2010) it has been shown to generate practical value. Twitter Sentiment Analysis denotes the task of assigning a given tweet a sentiment label of either positive or negative and is an integral part of many practical applications. Few methods consider neutral as a third class. However, defining a neutral class is a hard task. Pak and Paroubek (2010) for example label tweets of popular news sites as neutral. This assumption is not always true. The headline \u201cMultiple children were killed in the attack.\u201d would be labeled as negative by most human labelers. Thus, we propose an alternative approach to this problem. Its basic idea is the explicit incorporation of sentiment uncertainty."}, {"heading": "2 The State of the Art and Its Shortcomings", "text": "SemEval-2014 Task 9 (Rosenthal et al., 2014) provides a widely used state of the art benchmark for\n1http://project2.cs.uos.de/TweeDOS\nTwitter Sentiment Analysis and compares the performance of many current approaches. From a dataset collected from January 2012 to January 2013, popular topics have been extracted through identification of frequently mentioned named entities. Only tweets scoring above a certain polarity threshold determined by a sentiment lexicon were considered to ensure the inclusion of a sentiment. The labels included in the dataset are positive, negative and neutral, determined by a majority vote of five labelers who were told to vote for the sentiment they perceive as strongest, when in doubt. This assigns tweets to the classes positive and negative which do not carry a distinct sentiment. Methods performing well on this dataset are shown to be able to distinguish between positive and negative sentiment under the assumption that all tweets can be assigned one of these labels. Moreover, all test tweets include popular named entities of the time. As the authors themselves noted: The dataset is biased. Moreover, the majority vote along with the treatment of ambiguity adds noise to the dataset. While providing a dataset of high quality for the desired purpose, the general composition of the public Twitter stream is not represented by the dataset. Hence, only part of the problems arising in practical analysis of the live stream are addressed with the related research."}, {"heading": "3 A General Purpose Dataset", "text": "When analysing the Twitter stream we are interested in the \u201cElectronic Word of Mouth\u201d(Jansen et al., 2009), i.e. the personal opinions of private Twitter users. While labeling tweets, we noticed that a relatively high percentage of tweets are spam, advertising or marketing messages which we are not interested in. Those tweets shall be labeled spam. Moreover, it became obvious that for the remaining tweets only a small fraction can be distinctly labeled as positive or negative. The remaining tweets may still include polarity and can often\nar X\niv :1\n50 9.\n07 61\n2v 1\n[ cs\n.C L\n] 2\n5 Se\np 20\n15\nnot be labeled neutral while being neither positive nor negative. Hence, we propose the new category uncertain. Tweets labeled as neutral can be assigned to the class uncertain too, as they provide no additional information for sentiment analysis and can be treated in the same way as tweets of uncertain sentiment. This approach reduces the noise for the sentiment bearing classes which is a desirable feature if political or business decisions are supposed to be supported by the analysis results.\nTo acquire a representative view on the label composition of the public Twitter stream, we randomly sampled our dataset from a collection of about 43 million tweets with their creation dates ranging from June 2012 to August 2013 to minimize topical bias. Each tweet was labeled by two human labelers who had to assign it one of the labels positive, negative, uncertain or spam. In total 14506 tweets have been labeled by 27 labelers. The labelers consisted of master\u2019s students from the University of Osnabru\u0308ck, Germany and researchers from our group.\nThe distribution of labels is shown in figure 1. There is a total of 9356 (64.5% of total tweets labeled) tweets to which both human labelers assigned the same label. Of these tweets 15% are spam and 55% are labeled uncertain. A definite sentiment label could only be assigned to 30% of tweets with 13% being positive and 17% being negative.\nFundraiser Results by Salesperson\nPARTICIPANT UNITS SOLD PERCENTAGE positive 749 12% negative 1.023 16% uncertain 3.450 55% sp m 1.042 17%\nN um\nbe r o\nf T w\nee ts\n0\n1.000\n2.000\n3.000\n4.000\nLabel\npositive negative uncertain spam\n1.042\n3.450\n1.023 749\n12% 16%\n55%\n17%\nFundraiser Results by Salesperson-1\nPARTICIPANT UNITS SOLD PERCENTAGE positive 1.176 13% negative 1.620 17% uncertain 5.138 55% spam 1.422 15%\nThese results provide evidence for our claim that one has to deal with uncertainty in sentiment analysis when working with the public Twitter stream.\nTo assess the inter annotator agreement we computed Fleiss\u2019 Kappa (Fleiss, 1971) resulting in a value of \u03ba = 0.45 which can be interpreted as mod-\nerate agreement (Landis and Koch, 1977). At first sight this value seems to be rather low but when considering the disagreement matrix shown in table 1 the claim of the necessity to deal with uncertainty is further strengthened.\nLabelers seem to have a very good understanding of what distinguishes the classes positive and negative, only 106 tweets have been assigned both these labels. The disagreement for positive/spam and negative/spam is of similar or even smaller magnitude. Looking at these tweets we noticed that the disagreement is mainly related to misunderstanding of the labeling instructions or probably accidentally clicking the wrong label. Hence, these tweets should be omitted from the test set when evaluating methods for reliable Twitter Sentiment Analysis.\nHowever, the disagreement between positive/negative and uncertain is relatively large. These tweets make up about 76% of the tweets to which the two labelers assigned different labels. This indicates that in many cases not even two humans can agree upon whether a tweet contains a distinct sentiment or should be labeled uncertain. Systems aiming to perform reliable sentiment analysis of the public Twitter stream should be able to deal with these tweets. While not strictly belonging to the category uncertain they should still be labeled as such or at least not be considered for sentiment analysis. Another possible approach can be to interpret them as rather positive or rather negative, depending on the amount of reliability the respective application requires.\nModerate disagreement (914 tweets) can be noted for the classes uncertain and spam. Since these tweets may still contain useful information in the sense of answering the question \u201cWhat do people talk about?\u201d they probably should not be considered spam. However, they also should not be assigned a sentiment. A system labeling these as uncertain will still produce reliable results with regard to sentiment analysis.\nAs a first approach one can make use of just the tweets with two identical labels to asses methods for reliable sentiment analysis of the public Twitter stream. However, it should be considered that in practice the tweets upon which the labelers disagreed can also appear in the stream and have to be handled to provide reliable sentiment results. To enable researchers to develop systems which meet all the aforementioned requirements the complete dataset including the tweets disagreed upon is publicly available."}, {"heading": "4 Conclusion and Outlook", "text": "When performing analysis on the public live stream of Twitter with regard to sentiment, it needs to be considered that more than half of the tweets cannot be assigned a distinct sentiment. These tweets have to be filtered or explicitly dealt with before sentiment analysis takes place. Moreover, one has to deal with spam tweets. Spam adds unwanted noise by polluting topics with artificially injected tweets. Most of the work on spam detection on Twitter focusses on catching the users generating the spam by looking at the accounts\u2019 behaviour over time (Grier et al., 2010; Lin and Huang, 2013). When performing realtime analysis, a given tweet has to be determined to be spam or no spam by looking at its content and meta data only as there is no time to examine the author\u2019s account in detail. New methods have to be developed which are able to deal with sentiment uncertainty and spam if reliable representations of the public opinion are to be acquired from the Twitter stream. The dataset presented in this paper can be used to develop and evaluate methods for reliable Twitter Sentiment Analysis."}], "references": [{"title": "Predicting the future with social media", "author": ["Asur", "Huberman2010] Sitaram Asur", "Bernardo A Huberman"], "venue": "In 2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT),", "citeRegEx": "Asur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Asur et al\\.", "year": 2010}, {"title": "Twitter mood predicts the stock market", "author": ["Bollen et al.2011] Johan Bollen", "Huina Mao", "Xiaojun Zeng"], "venue": "Journal of Computational Science,", "citeRegEx": "Bollen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bollen et al\\.", "year": 2011}, {"title": "Characterizing debate performance via aggregated twitter", "author": ["Diakopoulos", "David A Shamma"], "venue": null, "citeRegEx": "Diakopoulos et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Diakopoulos et al\\.", "year": 2010}, {"title": "Measuring nominal scale agreement among many raters", "author": ["Joseph L Fleiss"], "venue": "Psychological bulletin,", "citeRegEx": "Fleiss.,? \\Q1971\\E", "shortCiteRegEx": "Fleiss.", "year": 1971}, {"title": "spam: the underground on 140 characters or less", "author": ["Grier et al.2010] Chris Grier", "Kurt Thomas", "Vern Paxson", "Michael Zhang"], "venue": "In Proceedings of the 17th ACM conference on Computer and communications security,", "citeRegEx": "Grier et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Grier et al\\.", "year": 2010}, {"title": "Twitter power: Tweets as electronic word of mouth", "author": ["Mimi Zhang", "Kate Sobel", "Abdur Chowdury"], "venue": "Journal of the American society for information science and technology,", "citeRegEx": "Jansen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jansen et al\\.", "year": 2009}, {"title": "The measurement of observer agreement for categorical data", "author": ["Landis", "Koch1977] J Richard Landis", "Gary G Koch"], "venue": null, "citeRegEx": "Landis et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Landis et al\\.", "year": 1977}, {"title": "A study of effective features for detecting long-surviving twitter spam accounts", "author": ["Lin", "Huang2013] Po-Ching Lin", "Po-Min Huang"], "venue": "In Advanced Communication Technology (ICACT),", "citeRegEx": "Lin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2013}, {"title": "Twitter as a corpus for sentiment analysis and opinion mining", "author": ["Pak", "Paroubek2010] Alexander Pak", "Patrick Paroubek"], "venue": "In Proceedings of the 7th International Conference on Language Resources,", "citeRegEx": "Pak et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pak et al\\.", "year": 2010}, {"title": "Semeval-2014 task 9: Sentiment analysis in twitter", "author": ["Alan Ritter", "Preslav Nakov", "Veselin Stoyanov"], "venue": "In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Rosenthal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rosenthal et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "For a multitude of applications such as sales prediction (Asur and Huberman, 2010), stock market prediction (Bollen et al., 2011) or political debate analysis (Diakopoulos and Shamma, 2010) it has been shown to generate practical value.", "startOffset": 108, "endOffset": 129}, {"referenceID": 1, "context": "For a multitude of applications such as sales prediction (Asur and Huberman, 2010), stock market prediction (Bollen et al., 2011) or political debate analysis (Diakopoulos and Shamma, 2010) it has been shown to generate practical value. Twitter Sentiment Analysis denotes the task of assigning a given tweet a sentiment label of either positive or negative and is an integral part of many practical applications. Few methods consider neutral as a third class. However, defining a neutral class is a hard task. Pak and Paroubek (2010) for example label tweets of popular news sites as neutral.", "startOffset": 109, "endOffset": 534}, {"referenceID": 9, "context": "SemEval-2014 Task 9 (Rosenthal et al., 2014) provides a widely used state of the art benchmark for", "startOffset": 20, "endOffset": 44}, {"referenceID": 5, "context": "When analysing the Twitter stream we are interested in the \u201cElectronic Word of Mouth\u201d(Jansen et al., 2009), i.", "startOffset": 85, "endOffset": 106}, {"referenceID": 3, "context": "To assess the inter annotator agreement we computed Fleiss\u2019 Kappa (Fleiss, 1971) resulting in a value of \u03ba = 0.", "startOffset": 66, "endOffset": 80}, {"referenceID": 4, "context": "the spam by looking at the accounts\u2019 behaviour over time (Grier et al., 2010; Lin and Huang, 2013).", "startOffset": 57, "endOffset": 98}], "year": 2015, "abstractText": "State of the art benchmarks for Twitter Sentiment Analysis do not consider the fact that for more than half of the tweets from the public stream a distinct sentiment cannot be chosen. This paper provides a new perspective on Twitter Sentiment Analysis by highlighting the necessity of explicitly incorporating uncertainty. Moreover, a dataset of high quality to evaluate solutions for this new problem is introduced and made publicly available1.", "creator": "LaTeX with hyperref package"}}}