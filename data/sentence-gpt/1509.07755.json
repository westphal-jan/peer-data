{"id": "1509.07755", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2015", "title": "A Mathematical Theory for Clustering in Metric Spaces", "abstract": "Clustering is one of the most fundamental problems in data analysis and it has been studied extensively in the literature. Though many clustering algorithms have been proposed, clustering theories that justify the use of these clustering algorithms are still unsatisfactory. In particular, one of the fundamental challenges is to address the following question: how does the clustering algorithm work? One of the problems with clustering algorithms is that in most cases it can be arbitrarily distributed, or at most in any case, multiple datasets can be distributed as clusters in different regions.\n\n\n\nIn a paper in Applied Structures and Applications, J. M. Dornhuyser and colleagues have demonstrated the ability to perform an algorithm for clustering by clustering only the cluster with only one dataset (the most recent example is the number of units in the cluster). This enables clustering to be performed with all the data and for many cases it is difficult to perform the algorithm in large amounts of data. The technique, shown by M. Dornhuyser and colleagues, is the most straightforward and efficient clustering algorithm, and it is relatively easy to perform even with an average cost of approximately $4,000 (assuming a cost of approximately $3,000). This approach can also be applied to the most common cluster, namely the cluster size. It is therefore important to note that even with many clusters, the amount of data is significantly larger than in previous work.\nThe clustering algorithm is able to be run on a variety of operating systems. In addition to running on many systems, it is possible to make a significant contribution to reducing the overhead of data extraction and analysis of clustering algorithms by using clustering. In fact, the concept of clustering algorithms is much more popular among scientists. This means that, as a result, it is far less efficient and more difficult to perform large-scale statistical analysis of small clusters. Moreover, this is not an ideal approach for clustering. In many cases, many clusters can be run on multiple architectures or in clusters with the smallest cost (typically ~1/2) depending on the size and complexity of the cluster. In order to efficiently compute clusters in the large-scale distributed computing environment, the researchers were able to use the techniques shown in this paper.\nThis paper examines the clustering algorithm's properties by using clustering algorithms. This technique is particularly useful when dealing with distributed data from single data systems. When performing large-scale distributed statistical analyses in many systems, such as Oracle's Oracle cloud cluster", "histories": [["v1", "Fri, 25 Sep 2015 15:30:18 GMT  (650kb)", "http://arxiv.org/abs/1509.07755v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["cheng-shang chang", "wanjiun liao", "yu-sheng chen", "li-heng liou"], "accepted": false, "id": "1509.07755"}, "pdf": {"name": "1509.07755.pdf", "metadata": {"source": "CRF", "title": "A Mathematical Theory for Clustering in Metric Spaces", "authors": ["Cheng-Shang Chang"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 9.\n07 75\n5v 1\n[ cs\n.L G\n] 2\n5 Se\np 20\n15\nWhat is a cluster in a set of data points? In this paper, we make an attempt to address such a question by considering a set of data points associated with a distance measure (metric). We first propose a new cohesion measure in terms of the distance measure. Using the cohesion measure, we define a cluster as a set of points that are cohesive to themselves. For such a definition, we show there are various equivalent statements that have intuitive explanations. We then consider the second question: How do we find clusters and good partitions of clusters under such a definition? For such a question, we propose a hierarchical agglomerative algorithm and a partitional algorithm. Unlike standard hierarchical agglomerative algorithms, our hierarchical agglomerative algorithm has a specific stopping criterion and it stops with a partition of clusters. Our partitional algorithm, called the K-sets algorithm in the paper, appears to be a new iterative algorithm. Unlike the Lloyd iteration that needs two-step minimization, our K-sets algorithm only takes one-step minimization.\nOne of the most interesting findings of our paper is the duality result between a distance measure and a cohesion measure. Such a duality result leads to a dual K-sets algorithm for clustering a set of data points with a cohesion measure. The dual K-sets algorithm converges in the same way as a sequential version of the classical kernel K-means algorithm. The key difference is that a cohesion measure does not need to be positive semi-definite.\nIndex Terms\nClustering, hierarchical algorithms, partitional algorithms, convergence, K-sets, duality\nI. INTRODUCTION\nClustering is one of the most fundamental problems in data analysis and it has a lot of applications in various fields, including Internet search for information retrieval, social network analysis for community detection, and computation biology for clustering protein sequences. The problem of clustering has been studied extensively in the literature (see e.g., the books [1], [2] and the historical review papers [3], [4]). For a clustering problem, there is a set of data points (or objects) and a similarity (or dissimilarity) measure that measures how similar two data points are. The aim of a clustering algorithm is to cluster these data points so that data points within the same cluster are similar to each other and data points in different clusters are dissimilar.\nAs stated in [4], clustering algorithms can be divided into two groups: hierarchical and partitional. Hierarchical algorithms can further be divided into two subgroups: agglomerative and divisive. Agglomerative hierarchical algorithms, starting from each data point as a sole cluster, recursively merge two similar clusters into a new cluster. On the other hand, divisive hierarchical algorithms, starting from the whole set as a single cluster, recursively divide a cluster into two dissimilar clusters. As such, there is a hierarchical structure of clusters from either a hierarchical agglomerative clustering algorithm or a hierarchical divisive clustering algorithm.\nPartitional algorithms do not have a hierarchical structure of clusters. Instead, they find all the clusters as a partition of the data points. The K-means algorithm is perhaps the simplest and the most widely used partitional algorithm for data points in an Euclidean space, where the Euclidean distance serves as the natural dissimilarity measure. The K-means algorithm starts from an initial partition of the data points into K clusters. It then repeatedly carries out the Lloyd iteration [5] that consists of the following two steps: (i) generate a new partition by assigning each data point to the closest cluster center, and (ii) compute the new cluster centers. The Lloyd iteration is known to reduce the sum of squared distance of each data point to its cluster center in each iteration and thus the K-means algorithm converges to a local minimum. The new cluster centers can be easily found if the data points are in a Euclidean space (or an inner product space). However, it is in general much more difficult to find the representative points for clusters, called medoids, if data points are in a non-Euclidean space. The refined K-means algorithms are commonly referred as the K-medoids algorithm (see e.g., [6], [1], [7], [8]). As the K-means algorithm (or the K-medoids algorithm) converges to a local optimum, it is quite sensitive to the initial choice of the partition. There are\nC. S. Chang and L. H. Liou are with the Institute of Communications Engineering, National Tsing Hua University, Hsinchu 300, Taiwan, R.O.C. email: cschang@ee.nthu.edu.tw, dacapo1142@gmail.com\nW. Liao and Y.-S. Chen are with Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan, R.O.C. email: {wjliao,r01921042}@ntu.edu.tw.\nsome recent works that provide various methods for selecting the initial partition that might lead to performance guarantees [9], [10], [11], [12], [13]. Instead of using the Lloyd iteration to minimize the sum of squared distance of each data point to its cluster center, one can also formulate a clustering problem as an optimization problem with respect to a certain objective function and then solve the optimization problem by other methods. This then leads to kernel and spectral clustering methods (see e.g., [14], [15], [16], [17], [18] and [19], [20] for reviews of the papers in this area). Solving the optimization problems formulated from the clustering problems are in general NP-hard and one has to resort to approximation algorithms [21]. In [21], Balcan et al. introduced the concept of approximation stability that assumes all the partitions (clusterings) that have the objective values close to the optimum ones are close to the target partition. Under such an assumption, they proposed efficient algorithms for clustering large data sets.\nThough there are already many clustering algorithms proposed in the literature, clustering theories that justify the use of these clustering algorithms are still unsatisfactory. As pointed out in [22], there are three commonly used approaches for developing a clustering theory: (i) an axiomatic approach that outlines a list of axioms for a clustering function (see e.g., [23], [24], [25], [26], [27], [28]), (ii) an objective-based approach that provides a specific objective for a clustering function to optimize (see e.g., [29], [21], and (iii) a definition-based approach that specifies the definition of clusters (see e.g, [30], [31], [32]). In [26], Kleinberg adopted an axiomatic approach and showed an impossibility theorem for finding a clustering function that satisfies the following three axioms:\n(i) Scale invariance: if we scale the dissimilarity measure by a constant factor, then the clustering function still outputs the same partition of clusters. (ii) Richness: for any specific partition of the data points, there exists a dissimilarity measure such that the clustering function outputs that partition. (iii) Consistency: for a partition from the clustering function with respect to a specific dissimilarity measure, if we increase the dissimilarity measure between two points in different clusters and decrease the dissimilarity measure between two points in the same cluster, then the clustering function still outputs the same partition of clusters. Such a change of a dissimilarity measure is called a consistent change.\nThe impossibility theorem is based on the fundamental result that the output of any clustering function satisfying the scale invariance property and the consistency property is in a collection of antichain partitions, i.e., there is no partition in that collection that in turn is a refinement of another partition in that collection. As such, the richness property cannot be satisfied. In [29], it was argued that the impossibility theorem is not an inherent feature of clustering. The key point in [29] is that\nthe consistency property may not be a desirable property for a clustering function. This can be illustrated by considering a consistent change of 5 clusters in Figure 1. The figure is redrawn from Figure 1 in [29] that originally consists of 6 clusters. On the left hand side of Figure 1, it seems reasonable to have a partition of 5 clusters. However, after the consistent change, a new partition of 3 clusters might be a better output than the original partition of 5 clusters. As such, they abandoned the three axioms for clustering functions and proposed another three similar axioms for Clustering-Quality Measures (CGM) (for measuring the quality of a partition). They showed the existence of a CGM that satisfies their three axioms for CGMs.\nAs for the definition-based approach, most of the definitions of a single cluster in the literature are based on loosely defined terms [1]. One exception is [30], where Ester et al. provided a precise definition of a single cluster based on the concept of density-based reachability. A point p is said to be directly density-reachable from another point q if point p lies within the \u01eb-neighborhood of point q and the \u01eb-neighborhood of point q contains at least a minimum number of points. A point is said to be density-reachable from another point if they are connected by a sequence of directly density-reachable points. Based on the concept of density-reachability, a cluster is defined as a maximal set of points that are density-reachable from each other. An intuitive way to see such a definition for a cluster in a set of data points is to convert the data set into a graph. Specifically, if we put a directed edge from one point p to another point q if point p is directly density-reachable from point q, then a cluster simply corresponds to a strongly connected component in the graph. One of the problems for such a definition is that it requires specifying two parameters, \u01eb and the minimum number of points in a \u01eb-neighborhood. As pointed out in [30], it is not an easy task to determine these two parameters.\nIn this paper, we make an attempt to develop a clustering theory in metric spaces. In Section II, we first address the question:\nWhat is a cluster in a set of data points in metric spaces?\nFor this, we first propose a new cohesion measure in terms of the distance measure. Using the cohesion measure, we define a cluster as a set of points that are cohesive to themselves. For such a definition, we show in Theorem 7 that there are various equivalent statements and these statements can be explained intuitively. We then consider the second question:\nHow do we find clusters and good partitions of clusters under such a definition?\nFor such a question, we propose a hierarchical agglomerative algorithm in Section III and a partitional algorithm Section IV. Unlike standard hierarchical agglomerative algorithms, our hierarchical agglomerative algorithm has a specific stopping criterion. Moreover, we show in Theorem 9 that our hierarchical agglomerative algorithm returns a partition of clusters when it stops. Our partitional algorithm, called the K-sets algorithm in the paper, appears to be a new iterative algorithm. Unlike the Lloyd iteration that needs two-step minimization, our K-sets algorithm only takes one-step minimization. We further show in Theorem 14 that the K-sets algorithm converges in a finite number of iterations. Moreover, for K = 2, the K-sets algorithm returns two clusters when the algorithm converges.\nOne of the most interesting findings of our paper is the duality result between a distance measure and a cohesion measure. In Section V, we first provide a general definition of a cohesion measure. We show that there is an induced distance measure, called the dual distance measure, for each cohesion measure. On the other hand, there is also an induced cohesion measure, called the dual cohesion measure, for each distance measure. In Theorem 18, we further show that the dual distance measure of a dual cohesion measure of a distance measure is the distance measure itself. Such a duality result leads to a dual K-sets algorithm for clustering a set of data points with a cohesion measure. The dual K-sets algorithm converges in the same way as a sequential version of the classical kernel K-means algorithm. The key difference is that a cohesion measure does not need to be positive semi-definite.\nIn Table I, we provide a list of notations used in this paper.\nII. CLUSTERS IN METRIC SPACES\nA. What is a cluster?\nAs pointed out in [4], one of the fundamental challenges associated with clustering is to address the following question:\nWhat is a cluster in a set of data points?\nIn this paper, we will develop a clustering theory that formally define a cluster for data points in a metric space. Specifically, we consider a set of n data points, \u2126 = {x1, x2, . . . , xn} and a distance measure d(x, y) for any two points x and y in \u2126. The distance measure d(\u00b7, \u00b7) is assumed to a metric and it satisfies\n(D1) d(x, y) \u2265 0; (D2) d(x, x) = 0; (D3) (Symmetric) d(x, y) = d(y, x); (D4) (Triangular inequality) d(x, y) \u2264 d(x, z) + d(z, y).\nSuch a metric assumption is stronger than the usual dissimilarity (similarity) measures [33], where the triangular inequality in general does not hold. We also note that (D2) is usually stated as a necessary and sufficient condition in the literature, i.e., d(x, y) = 0 if and only if x = y. However, we only need the sufficient part in this paper. Our approach begins with a definition-based approach. We first give a specific definition of what a cluster is (without the need of specifying any parameters) and show those axiom-like properties are indeed satisfied under our definition of a cluster.\nB. Relative distance and cohesion measure\nOne important thing that we learn from the consistent change in Figure 1 is that a good partition of clusters should be looked at a global level and the relative distances among clusters should be considered as an important factor. The distance measure between any two points only gives an absolute value and it does not tell us how close these two points are relative to the whole set of data points. The key idea of defining the relative distance from one point x to another point y is to choose another random point z as a reference point and compute the relative distance as the average of d(x, y) \u2212 d(x, z) for all the points z in \u2126. This leads to the following definition of relative distance.\nDefinition 1 (Relative distance) The relative distance from a point x to another point y, denoted by RD(x||y), is defined as follows:\nRD(x||y) = 1\nn\n\u2211\nz\u2208\u2126\n(d(x, y) \u2212 d(x, z))\n= d(x, y) \u2212 1\nn\n\u2211\nz\u2208\u2126\nd(x, z). (1)\nThe relative distance (from a random point) to a point y, denoted by RD(y), is defined as the average relative distance from a random point to y, i.e.,\nRD(y) = 1\nn\n\u2211\nz\u2208\u2126\nRD(z||y)\n= 1\nn\n\u2211\nz2\u2208\u2126\nd(z2, y)\u2212 1\nn2\n\u2211\nz2\u2208\u2126\n\u2211\nz1\u2208\u2126\nd(z2, z1). (2)\nNote from (1) that in general RD(x||y) is not symmetric, i.e., RD(x||y) 6= RD(y||x). Also, RD(x||y) may not be nonnegative. In the following, we extend the notion of relative distance from one point to another point to the relative distance from one set to another set.\nDefinition 2 (Relative distance) The relative distance from a set of points S1 to another set of points S2, denoted by RD(S1||S2), is defined as the average relative distance from a random point in S1 to another random point in S2, i.e.,\nRD(S1||S2) = 1\n|S1| \u00b7 |S2|\n\u2211\nx\u2208S1\n\u2211\ny\u2208S2\nRD(x||y). (3)\nBased on the notion of relative distance, we define a cohesion measure for two points x and y below.\nDefinition 3 (Cohesion measure between two points) Define the cohesion measure between two points x and y, denoted by \u03b3(x, y), as the difference of the relative distance to y and the relative distance from x to y, i.e.,\n\u03b3(x, y) = RD(y)\u2212 RD(x||y). (4)\nTwo points x and y are said to be cohesive (resp. incohesive) if \u03b3(x, y) \u2265 0 (resp. \u03b3(x, y) \u2264 0).\nIn view of (4), two points x and y are cohesive if the relative distance from x to y is not larger than the relative distance (from a random point) to y.\nNote from (1) and (2) that\n\u03b3(x, y) = RD(y)\u2212 RD(x||y)\n= 1\nn\n\u2211\nz2\u2208\u2126\nd(z2, y) + 1\nn\n\u2211\nz1\u2208\u2126\nd(x, z1)\n\u2212 1\nn2\n\u2211\nz2\u2208\u2126\n\u2211\nz1\u2208\u2126\nd(z2, z1)\u2212 d(x, y) (5)\n= 1\nn2\n\u2211\nz2\u2208\u2126\n\u2211\nz1\u2208\u2126\n(\nd(x, z1) + d(z2, y)\n\u2212d(z1, z2)\u2212 d(x, y) ) . (6)\nThough there are many ways to define a cohesion measure for a set of data points in a metric space, our definition of the cohesion measure in Definition 3 has the following four desirable properties. Its proof is based on the representations in (5) and (6) and it is given in Appendix A.\nProposition 4 (i) (Symmetry) The cohesion measure is symmetric, i.e., \u03b3(x, y) = \u03b3(y, x). (ii) (Self-cohesiveness) Every data point is cohesive to itself, i.e., \u03b3(x, x) \u2265 0. (iii) (Self-centredness) Every data point is more cohesive to itself than to another point, i.e., \u03b3(x, x) \u2265 \u03b3(x, y) for all\ny \u2208 \u2126. (iv) (Zero-sum) The sum of the cohesion measures between a data point to all the points in \u2126 is zero, i.e., \u2211\ny\u2208\u2126 \u03b3(x, y) = 0.\nThese four properties can be understood intuitively by viewing a cohesion measure between two points as a \u201cbinding force\u201d between those two points. The symmetric property ensures that the binding force is reciprocated. The self-cohesiveness property ensures that each point is self-binding. The self-centredness property further ensures that the self binding force is always stronger than the binding force to the other points. In view of the zero-sum property, we know for every point x there are points that are incohesive to x and each of these points has a negative binding force to x. Also, there are points that are cohesive to x (including x itself from the self-cohesiveness property) and each of these points has a positive force to x. As such, the binding force will naturally \u201cpush\u201d points into \u201cclusters.\u201d\nTo further understand the intuition of the cohesion measure, we can think of z1 and z2 in (6) as two random points that are used as reference points. Then two points x and y are cohesive if d(x, z1) + d(z2, y) \u2265 d(z1, z2) + d(x, y) for two reference points z1 and z2 that are randomly chosen from \u2126. In Figure 2, we show an illustrating example for such an intuition in R2. In Figure 2(a), point x is close to one reference point z1 and point y is close to the other reference point z2. As such, d(x, z1) + d(z2, y) \u2264 d(z1, z2) and thus these two points x and y are incohesive. In Figure 2(b), point x is not that close to z1 and point y is not that close to z2. However, x and y are on the two opposite sides of the segment between the two reference points z1 and z2. As such, there are two triangles in this graph: the first triangle consists of the three points x, z1, and w, and the second triangle consists of the three points y, z2, and w. From the triangular inequality, we then have d(w, z1) + d(x,w) \u2265 d(x, z1) and d(y, w) + d(w, z2) \u2265 d(y, z2). Since d(w, z1) + d(w, z2) = d(z1, z2) and d(x,w) + d(y, w) = d(x, y), it then follows that d(z1, z2) + d(x, y) \u2265 d(x, z1) + d(y, z2). Thus, points x and y are also incohesive in Figure 2(b). In Figure 2(c), point x is not that close to z1 and point y is not that close to z2 as in Figure 2(b). Now x and y are on the same side of the segment between the two reference points z1 and z2. There are two triangles in this graph: the first triangle consists of the three points x, y, and w, and the second triangle consists of the three points z1, z2, and w. From the triangular inequality, we then have d(x,w) + d(w, y) \u2265 d(x, y) and d(w, z1) + d(w, z2) \u2265 d(z1, z2). Since d(x,w) + d(w, z1) = d(x, z1) and d(w, y) + d(w, z2) = d(z2, y), it then follows that d(x, z1) + d(y, z2) \u2265 d(z1, z2) + d(x, y). Thus, points x and y are cohesive in Figure 2(c). In view of Figure 2(c), it is intuitive to see that two points x and y are cohesive if they both are far away from the two reference points and they both are close to each other.\nThe notions of relative distance and cohesion measure are also related to the notion of relative centrality in our previous work [34]. To see this, suppose that we sample two points x and y from \u2126 according to the following bivariate distribution:\np(x, y) = e\u2212\u03b8d(x,y) \u2211\nu\u2208\u2126\n\u2211\nv\u2208\u2126 e \u2212\u03b8d(u,v)\n, \u03b8 > 0. (7)\nLet PX(x) = \u2211 y\u2208\u2126 p(x, y) and PY (y) = \u2211\nx\u2208\u2126 p(x, y) be the two marginal distributions. Then one can verify that the covariance p(x, y)\u2212PX(x)PY (y) is proportional to the cohesion measure \u03b3(x, y) when \u03b8 \u2193 0. Intuitively, two points x and y are cohesive if they are positively correlated according to the sampling in (7) when \u03b8 is very small.\nNow we extend the cohesion measure between two points to the cohesion measure between two sets.\nDefinition 5 (Cohesion measure between two sets) Define the cohesion measure between two sets S1 and S2, denoted by \u03b3(S1, S2), as the sum of the cohesion measures of all the pairs of two points (with one point in S1 and the other point in S2), i.e.,\n\u03b3(S1, S2) = \u2211\nx\u2208S1\n\u2211\ny\u2208S2\n\u03b3(x, y). (8)\nTwo sets S1 and S2 are said to be cohesive (resp. incohesive) if \u03b3(S1, S2) \u2265 0 (resp. \u03b3(S1, S2) \u2264 0).\nC. Equivalent statements of clusters\nNow we define what a cluster is in terms of the cohesion measure.\nDefinition 6 (Cluster) A nonempty set S is called a cluster if it is cohesive to itself, i.e.,\n\u03b3(S, S) \u2265 0. (9)\nIn the following, we show the first main theorem of the paper. Its proof is given in Appendix B.\nTheorem 7 Consider a nonempty set S that is not equal to \u2126. Let Sc = \u2126\\S be the set of points that are not in S. Also, let d\u0304(S1, S2) be the average distance between two randomly selected points with one point in S1 and another point in S2, i.e.,\nd\u0304(S1, S2) = 1\n|S1| \u00d7 |S2|\n\u2211\nx\u2208S1\n\u2211\ny\u2208S2\nd(x, y). (10)\nThe following statements are equivalent.\n(i) The set S is a cluster, i.e., \u03b3(S, S) \u2265 0. (ii) The set Sc is a cluster, i.e., \u03b3(Sc, Sc) \u2265 0. (iii) The two sets S and Sc are incohesive, i.e., \u03b3(S, Sc) \u2264 0. (iv) The set S is more cohesive to itself than to Sc, i.e., \u03b3(S, S) \u2265 \u03b3(S, Sc).\nALGORITHM 1: The Hierarchical Agglomerative Algorithm\nInput: A data set \u2126 = {x1, x2, . . . , xn} and a distance measure d(\u00b7, \u00b7). Output: A partition of clusters {S1, S2, . . . , SK}. Initially, K = n; Si = {xi}, i = 1, 2, . . . , n; Compute the cohesion measures \u03b3(Si, Sj) = \u03b3(xi, xj) for all i, j = 1, 2, . . . , n; while there exists some i and j with \u03b3(Si, Sj) > 0 do\nMerge Si and Sj into a new set Sk, i.e., Sk = Si \u222a Sj ; \u03b3(Sk, Sk) = \u03b3(Si, Si) + 2\u03b3(Si, Sj) + \u03b3(Sj , Sj); for each \u2113 6= k do\n\u03b3(Sk, S\u2113) = \u03b3(S\u2113, Sk) = \u03b3(Si, S\u2113) + \u03b3(Sj , S\u2113); end K = K \u2212 1;\nend Reindex the K remaining sets to {S1, S2, . . . , SK};\n(v) 2d\u0304(S,\u2126)\u2212 d\u0304(\u2126,\u2126)\u2212 d\u0304(S, S) \u2265 0. (vi) The relative distance from \u2126 to S is not smaller than the relative distance from S to S, i.e., RD(\u2126||S) \u2265 RD(S||S). (vii) The relative distance from Sc to S is not smaller than the relative distance from S to S, i.e., RD(Sc||S) \u2265 RD(S||S). (viii) 2d\u0304(S, Sc)\u2212 d\u0304(S, S)\u2212 d\u0304(Sc, Sc) \u2265 0. (ix) The relative distance from S to Sc is not smaller than the relative distance from \u2126 to Sc, i.e., RD(S||Sc) \u2265 RD(\u2126||Sc). (x) The relative distance from Sc to S is not smaller than the relative distance from \u2126 to S, i.e., RD(Sc||S) \u2265 RD(\u2126||S).\nOne surprise finding in Theorem 7(ii) is that the set Sc is also a cluster. This shows that the points inside S are cohesive and the points outside S are also cohesive. Thus, there seems a boundary between S and Sc from the cohesion measure. Another surprise finding is in Theorem 7(viii). One usually would expect that a cluster S should satisfy d\u0304(S, S) \u2264 d\u0304(S, Sc). But it seems our definition of a cluster is much weaker than that. Regarding the scale invariance property, it is easy to see from Theorem 7(viii) that the inequality there is still satisfied if we scale the distance measure by a constant factor. Thus, a cluster of data points is still a cluster after scaling the distance measure by a constant factor. Regarding the richness property, we argue that there exists a distance measure such that any subset of points in \u2126 is a cluster. To see this, we simple let the distance between any two points in the subset be equal to 0 and the distance between a point outside the subset to a point in the subset be equal to 1. Since a point x itself is a cluster, i.e., \u03b3(x, x) \u2265 0, we then have \u03b3(x, y) = \u03b3(x, x) \u2265 0 for any two points x and y in the subset. From (9), the subset is a cluster under such a choice of the distance measure. Furthermore, one can also see from Theorem 7(vii) that for a cluster S, if we decrease the relative distance between two points in S and increase the relative distance between one point in S and another point in Sc, then the set S is still a cluster under such a \u201dconsistent\u201d change.\nWe also note that in our proof of Theorem 7 we only need d(\u00b7, \u00b7) to be symmetric. As such, the results in Theorem 7 also hold even when the triangular inequality is not satisfied."}, {"heading": "III. A HIERARCHICAL AGGLOMERATIVE ALGORITHM", "text": "Once we define what a cluster is, our next question is\nHow do we find clusters and good partitions of clusters?\nFor this, we turn to an objective-based approach. We will show that clusters can be found by optimizing two specific objective functions by a hierarchical algorithm in Section III and a partitional algorithm in Section IV.\nIn the following, we first define a quality measure for a partition of \u2126.\nDefinition 8 (Modularity) Let Sk, k = 1, 2, . . . ,K , be a partition of \u2126 = {x1, x2, . . . , xn}, i.e., Sk \u2229Sk\u2032 is an empty set for k 6= k\u2032 and \u222aKk=1Sk = \u2126. The modularity index Q with respect to the partition Sk, k = 1, 2, . . . ,K , is defined as follows:\nQ = K \u2211\nk=1\n\u03b3(Sk, Sk). (11)\nBased on such a quality measure, we can thus formulate the clustering problem as an optimization problem for finding a partition S1, S2, . . . , SK (for some unknown K) that maximizes the modularity index Q. Note that\nQ =\nK \u2211\nk=1\n\u03b3(Sk, Sk) =\nK \u2211\nk=1\n\u2211\nx\u2208Sk\n\u2211\ny\u2208Sk\n\u03b3(x, y)\n= \u2211\nx\u2208\u2126\n\u2211\ny\u2208\u2126\n\u03b3(x, y)\u03b4c(x),c(y), (12)\nwhere c(x) is the cluster of x and \u03b4c(x),c(y) = 1 if x and y are in the same cluster. In view of (12), another way to look at the optimization problem is to find the assignment of each point to a cluster. However, it was shown in [35] that finding the optimal assignment for modularity maximization is NP-complete in the strong sense and thus heuristic algorithms, such as hierarchical algorithms and partitional algorithms are commonly used in the literature for solving the modularity maximization problem.\nIn Algorithm 1, we propose a hierarchical agglomerative clustering algorithm that converges to a local optimum of this objective. The algorithm starts from n clusters with each point itself as a cluster. It then recursively merges two disjoint cohesive clusters to form a new cluster until either there is a single cluster left or all the remaining clusters are incohesive. There are two main differences between a standard hierarchical agglomerative clustering algorithm and ours:\n(i) Stopping criterion: in a standard hierarchical agglomerative clustering algorithm, such as single linkage or complete linkage, there is no stopping criterion. Here our algorithm stops when all the remaining clusters are incohesive. (ii) Greedy selection: our algorithm only needs to select a pair of cohesive clusters to merge. It does not need to be the most cohesive pair. This could potentially speed up the algorithm in a large data set.\nIn the following theorem, we show that the modularity index Q in (11) is non-decreasing in every iteration of the hierarchical agglomerative clustering algorithm and it indeed produces clusters. Its proof is given in Appendix C.\nTheorem 9 (i) Every set returned by the hierarchical agglomerative clustering algorithm is indeed a cluster. (ii) For the hierarchical agglomerative clustering algorithm, the modularity index is non-decreasing in every iteration\nand thus converges to a local optimum.\nAs commented before, our algorithm only requires to find a pair of cohesive clusters to merge in each iteration. This is different from the greedy selection in [1], Chapter 13, and [36]. Certainly, our hierarchical agglomerative clustering algorithm can also be operated in a greedy manner. As in [36], in each iteration we can merge the two clusters that result in the largest increase of the modularity index, i.e., the most cohesive pair. It is well-known (see e.g., the book [2]) that a na\u0308ive implementation of a greedy hierarchical agglomerative clustering algorithm has O(n3) computational complexity and the computational complexity can be further reduced to O(n2 log(n)) if priority queues are implemented for the greedy selection. We also note that there are several hierarchical agglomerative clustering algorithms proposed in the literature for community detection in networks (see e.g., [37], [38], [39], [40]). These algorithms are also based on \u201cmodularity\u201d maximization. Among them, the fast unfolding algorithm in [38] is the fast one as there is a second phase of building a new (and much smaller) network whose nodes are the communities found during the previous phase. The Newman and Girvan modularity in [37] is based on a probability measure from a random selection of an edge in a network (see [34] for more detailed discussions) and this is different from the distance metric used in this paper.\nIn the following, we provide an illustrating example for our hierarchical agglomerative clustering algorithm by using the greedy selection of the most cohesive pair.\nExample 10 (Zachary\u2019s karate club) As in [37], [42], we consider the Zachary\u2019s karate club friendship network [41] in Figure 3. The set of data was observed by Wayne Zachary [41] over the course of two years in the early 1970s at an American university. During the course of the study, the club split into two groups because of a dispute between its administrator (node 34 in Figure 3) and its instructor (node 1 in Figure 3).\nIn Figure 6, we show the dendrogram generated by using our hierarchical agglomerative clustering algorithm with the greedy selection of the most cohesive pair in each iteration. The distance measure is the geodesic distance of the graph in Figure 3. The algorithm stops when there are three incohesive clusters left, one led by the administrator (node 34), one led by the instructor (node 1), and person number 9 himself. According to [41], there was an interesting story for person number 9. He was a weak supporter for the administrator. However, he was only three weeks away from a test for black belt (master status) when the split of the club occurred. He would have had to give up his rank if he had joined the administrator\u2019s club. He ended up with the instructor\u2019s club. We also run an additional step for our algorithm (to merge the pair with the largest cohesive measure) even though the remaining three clusters are incohesive. The additional step reveals that person number 9 is clustered into the instructor\u2019s club.\nIV. A PARTITIONAL ALGORITHM\nA. Triangular distance\nIn this section, we consider another objective function.\nDefinition 11 (normalized modularity) Let Sk, k = 1, 2, . . . ,K , be a partition of \u2126 = {x1, x2, . . . , xn}, i.e., Sk \u2229 Sk\u2032 is an empty set for k 6= k\u2032 and \u222aKk=1Sk = \u2126. The normalized modularity index R with respect to the partition Sk, k = 1, 2, . . . ,K , is defined as follows:\nR =\nK \u2211\nk=1\n1\n|Sk| \u03b3(Sk, Sk). (13)\nUnlike the hierarchical agglomerative clustering algorithm in the previous section, in this section we assume that K is fixed and known in advance. As such, we may use an approach similar to the classical K-means algorithm by iteratively assigning each point to the nearest set (until it converges). Such an approach requires a measure that can measure how close a point x to a set S is. In the K-means algorithm, such a measure is defined as the square of the distance between x and the centroid of S. However, there is no centroid for a set in a non-Euclidean space and we need to come up with another measure.\nOur idea for measuring the distance from a point x to a set S is to randomly choose two points z1 and z2 from S and consider the three sides of the triangle x, z1 and z2. Note that the triangular inequality guarantees that d(x, z1)+d(x, z2)\u2212d(z1, z2) \u2265 0. Moreover, if x is close to z1 and z2, then d(x, z1) + d(x, z2)\u2212 d(z1, z2) should also be small. We illustrate such an intuition\nin Figure 5, where there are two points x and y and a set S in R2. Such an intuition leads to the following definition of triangular distance from a point x to a set S.\nDefinition 12 (Triangular distance) The triangular distance from a point x to a set S, denoted by \u2206(x, S), is defined as follows:\n\u2206(x, S) = 1\n|S|2\n\u2211\nz1\u2208S\n\u2211\nz2\u2208S\n( d(x, z1) + d(x, z2)\u2212 d(z1, z2) ) . (14)\nIn the following lemma, we show several properties of the triangular distance and its proof is given in Appendix D.\nLemma 13 (i) \u2206(x, S) = 2d\u0304({x}, S)\u2212 d\u0304(S, S) \u2265 0. (15)\n(ii)\n\u2206(x, S) = \u03b3(x, x)\u2212 2\n|S| \u03b3({x}, S) +\n1\n|S|2 \u03b3(S, S). (16)\n(iii) Let Sk, k = 1, 2, . . . ,K , be a partition of \u2126 = {x1, x2, . . . , xn}. Then\nK \u2211\nk=1\n\u2211\nx\u2208Sk\n\u2206(x, Sk) = \u2211\nx\u2208\u2126\n\u03b3(x, x) \u2212R. (17)\n(iv) Let Sk, k = 1, 2, . . . ,K , be a partition of \u2126 = {x1, x2, . . . , xn} and c(x) be the index of the set to which x belongs, i.e., x \u2208 Sc(x). Then\nK \u2211\nk=1\n\u2211\nx\u2208Sk\n\u2206(x, Sk) = K \u2211\nk=1\n\u2211\nx\u2208Sk\nd\u0304({x}, Sk)\n= \u2211\nx\u2208\u2126\nd\u0304({x}, Sc(x)). (18)\nThe first property of this lemma is to represent triangular distance by the average distance. The second property is to represent the triangular distance by the cohesion measure. Such a property plays an important role for the duality result in Section V. The third property shows that the optimization problem for maximizing the normalized modularity R is equivalent to the optimization problem that minimizes the sum of the triangular distance of each point to its set. The fourth property further shows that such an optimization problem is also equivalent to the optimization problem that minimizes the sum of the average distance of each point to its set. Note that d\u0304({x}, Sk) = 1|Sk| \u2211 y\u2208Sk d(x, y). The objective for maximizing the normalized modularity R is also equivalent to minimize\nK \u2211\nk=1\n1\n|Sk|\n\u2211\nx\u2208Sk\n\u2211\ny\u2208Sk\nd(x, y).\nThis is different from the K-median objective, the K-means objective and the min-sum objective addressed in [21].\nALGORITHM 2: The K-sets Algorithm\nInput: A data set \u2126 = {x1, x2, . . . , xn}, a distance measure d(\u00b7, \u00b7), and the number of sets K . Output: A partition of sets {S1, S2, . . . , SK}. (0) Initially, choose arbitrarily K disjoint nonempty sets S1, . . . , SK as a partition of \u2126. (1) for i = 1, 2, . . . , n do\nCompute the triangular distance \u2206(xi, Sk) for each set Sk by using (15). Find the set to which the point xi is closest in terms of the triangular distance. Assign point xi to that set.\nend (2) Repeat from (1) until there is no further change.\nB. The K-sets algorithm\nIn the following, we propose a partitional clustering algorithm, called the K-sets algorithm in Algorithm 2, based on the triangular distance. The algorithm is very simple. It starts from an arbitrary partition of the data points that contains K disjoint sets. Then for each data point, we assign the data point to the closest set in terms of the triangular distance. We repeat the process until there is no further change. Unlike the Lloyd iteration that needs two-step minimization, the K-sets algorithm only takes one-step minimization. This might give the K-sets algorithm the computational advantage over the K-medoids algorithms [6], [1], [7], [8].\nIn the following theorem, we show the convergence of the K-sets algorithm. Moreover, for K = 2, the K-sets algorithm yields two clusters. Its proof is given in Appendix E.\nTheorem 14 (i) In the K-sets algorithm based on the triangular distance, the normalized modularity is increasing when there is a change, i.e., a point is moved from one set to another. Thus, the algorithm converges to a local optimum of the normalized modularity.\n(ii) Let S1, S2, . . . , SK be the K sets when the algorithm converges. Then for all i 6= j, the two sets Si and Sj are two clusters if these two sets are viewed in isolation (by removing the data points not in Si \u222a Sj from \u2126).\nAn immediate consequence of Theorem 14 (ii) is that for K = 2, the two sets S1 and S2 are clusters when the algorithm converges. However, we are not able to show that for K \u2265 3 the K sets, S1, S2, . . . , SK , are clusters in \u2126. On the other hand, we are not able to find a counterexample either. All the numerical examples that we have tested for K \u2265 3 yield K clusters.\nC. Experiments\nIn this section, we report several experimental results for the K-sets algorithm: including the dataset with two rings in Section IV-C1, the stochastic block model in Section IV-C2, and the mixed National Institute of Standards and Technology dataset in Section IV-C3.\n1) Two rings: In this section, we first provide an illustrating example for the K-sets algorithm. In Figure 6, we generate two rings by randomly placing 500 points in R2. The outer (resp. inner) ring consists of 300 (resp. 200) points. The radius of a point in the outer (resp. inner) ring is uniformly distributed between 20 and 22 (resp. 10 and 12). The angle of each point is uniformly distributed between 0 and 2\u03c0. In Figure 6(a), we show a typical clustering result by using the classical K-means algorithm with K = 2. As the centroids of these two rings are very close to each other, it is well-known that the K-means algorithm does not perform well for the two rings. Instead of using the Euclidean distance as the distance measure for our K-sets algorithm, we first convert the two rings into a graph by adding an edge between two points with the Euclidean distance less than 5. Then the distance measure between two points is defined as the geodesic distance of these two points in the graph. By doing so, we can then easily separate these rings by using the K-sets algorithm with K = 2 as shown in Figure 6(b).\nThe purpose of this example is to show the limitation of the applicability of the K-means algorithm. The data points for the K-means algorithm need to be in some Euclidean space. On the other hand, the data points for the K-sets algorithms only need to be in some metric space. As such, the distance matrix constructed from a graph cannot be directly applied by the K-means algorithm while it is still applicable for the K-sets algorithm.\n2) Stochastic block model: The stochastic block model (SBM), as a generalization of the Erdo\u0308s-Re\u0301nyi random graph [43], is a commonly used method for generating random graphs that can be used for benchmarking community detection algorithms [44], [45]. In a stochastic block model with q blocks (communities), the total number of nodes in the random graph are evenly distributed to these q blocks. The probability that there is an edge between two nodes within the same block is pin and the probability that there is an edge between two nodes in two different blocks is pout. These edges are generated independently. Let cin = n \u00b7 pin , cout = n \u00b7 pout. Then it is known [44] that these q communities can be detected (in theory for a large network) if\n|cin \u2212 cout| > q \u221a mean degree. (19)\nIn this paper, we use MODE-NET [45] to run SBM. Specifically, we consider a stochastic block model with two blocks. The number of nodes in the stochastic block model is 1,000 with 500 nodes in each of these two blocks. The average degree of a node is set to be 3. The values of cin \u2212 cout of these graphs are in the range from 2.5 to 5.9 with a common step of 0.1. We generate 20 graphs for each cin \u2212 cout. Isolated vertices are removed. Thus, the exact numbers of vertices used in this experiment are slightly less than than 1,000.\nWe compare our K-sets algorithm with some other community detection algorithms, such as OSLOM2 [46], infomap [47], [48], and fast unfolding [38]. The metric used for the K-sets algorithm for each sample of the random graph is the resistance distance, and this is pre-computed by NumPy [49]. The resistance distance matrix (denoted by R = (Ri,j)) can be derived from the pseudo inverse of the adjacency matrix (denoted by \u0393 = (\u0393i,j)) as follows: [50]:\nRi,j =\n{\n0, if i = j,\n\u0393i,i + \u0393j,j \u2212 \u0393i,j \u2212 \u0393j,i, otherwise.\nThe K-sets algorithm and OSLOM2 are implemented in C++, and the others are all taken from igraph [51] and are implemented in C with python wrappers. In Table II, we show the average running times for these four algorithms over 700 trials. The pre-computation time for the K-sets algorithm is the time to compute the distance matrix. Except infomap, the other three algorithms are very fast. In Figure 7, we compute the normalized mutual information measure (NMI) by using a built-in function in igraph [51] for the results obtained from these four algorithms. Each point is averaged over 20 random graphs from the stochastic block model. The error bars are the 95% confidence intervals. In this stochastic block model, the theoretical phase transition threshold from (19) is cin \u2212 cout = 3.46. It seems that the K-sets algorithm is able to detect these two blocks when cin \u2212 cout \u2265 4.5. Its performance in that range is better than infomap [47], [48], fast unfolding [38] and OSLOM2 [46]. We note that the comparison is not exactly fair as the other three algorithms do not have the information of the number of blocks (communities).\n3) Mixed National Institute of Standards and Technology dataset: In this section, we consider a real-world dataset, the mixed National Institute of Standards and Technology dataset (the MNIST dataset) [52]. The MNIST dataset contains 60,000 samples of hand-written digits. These samples are 28\u00d728 pixels grayscale images (i.e., each of the image is a 784 dimensional data point). For our experiments, we select the first 1,000 samples from each set of the digit 0 to 9 to create a total number of 10,000 samples.\nTo fairly evaluate the performance of the K-sets algorithm, we compare the K-sets algorithm with two clustering algorithms in which the number of clusters is also known a priori, i.e., the K-means++ algorithm [53] and the K-medoids algorithm [54]. For the MNIST dataset, the number of clusters is 10 (for the ten digits, 0, 1, 2, . . . , 9). The K-means++ algorithm is an improvement of the standard K-means algorithm with a specific method to choose the initial centroids of the K clusters. Like the K-sets algorithm, the K-medoids algorithm is also a clustering algorithm that uses a distance measure. The key difference between the K-sets algorithm and the K-medoids algorithm is that we use the triangular distance to a set for the assignment of each data point and the K-medoids algorithm uses the distance to a medoid for such an assignment. The Euclidean distance between two data points (samples from the MNIST dataset) for the K-medoids algorithm and the K-sets algorithm are pre-computed by NumPy [49]. The K-sets algorithm is implemented in C++, and the others are implemented in C with python wrappers. All the programs are executed on an Acer Altos-T350-F2 machine with two Intel(R) Xeon(R)\nCPU E5-2690 v2 processors. In order to have a fair comparison of their running times, the parallelization of each program is disabled, i.e., only one core is used in these experiments. We assume that the input data is already stored in the main memory and the time consumed for I/O is not recorded.\nIn Table III, we show the average running times for these three algorithms over 100 trials. Both the K-medoids algorithm and the K-sets algorithm need to compute the distance matrix and this is shown in the row marked with the pre-computation time. The total running times for these three algorithms are roughly the same for this experiment. In Figure 8, we compute the normalized mutual information measure (NMI) by using a built-in function in igraph [51] for the results obtained from these three algorithms. Each point is averaged over 100 trials. The error bars are the 95% confidence intervals. In view of Figure 8, the K-sets algorithm outperforms the K-means++ algorithm and the K-medoids algorithm for the MNIST dataset. One possible explanation for this is that both the the K-means++ algorithm and the K-medoids algorithm only select a single representative data point for a cluster and that representative data point may not be able to represent the whole cluster well enough. On the other hand, the K-sets algorithm uses the triangular distance that takes the distance to every point in a cluster into account."}, {"heading": "V. DUALITY BETWEEN A COHESION MEASURE AND A DISTANCE MEASURE", "text": "A. The duality theorem\nIn this section, we show the duality result between a cohesion measure and a distance measure. In the following, we first provide a general definition for a cohesion measure.\nDefinition 15 A measure between two points x and y, denoted by \u03b2(x, y), is called a cohesion measure for a set of data points \u2126 if it satisfies the following three properties:\n(C1) (Symmetry) \u03b2(x, y) = \u03b2(y, x) for all x, y \u2208 \u2126. (C2) (Zero-sum) For all x \u2208 \u2126, \u2211\ny\u2208\u2126 \u03b2(x, y) = 0. (C3) (Triangular inequality) For all x, y, z in \u2126,\n\u03b2(x, x) + \u03b2(y, z)\u2212 \u03b2(x, z)\u2212 \u03b2(x, y) \u2265 0. (20)\nIn the following lemma, we show that the specific cohesion measure defined in Section II indeed satisfies (C1)\u2013(C3) in Definition 15. Its proof is given in Appendix F.\nLemma 16 Suppose that d(\u00b7, \u00b7) is a distance measure for \u2126, i.e., d(\u00b7, \u00b7) satisfies (D1)\u2013(D4). Let\n\u03b2(x, y) = 1\nn\n\u2211\nz2\u2208\u2126\nd(z2, y) + 1\nn\n\u2211\nz1\u2208\u2126\nd(x, z1)\n\u2212 1\nn2\n\u2211\nz2\u2208\u2126\n\u2211\nz1\u2208\u2126\nd(z2, z1)\u2212 d(x, y). (21)\nThen \u03b2(x, y) is a cohesion measure for \u2126.\nWe know from (5) that the cohesion measure \u03b3(\u00b7, \u00b7) defined in Section II has the following representation:\n\u03b3(x, y) = 1\nn\n\u2211\nz2\u2208\u2126\nd(z2, y) + 1\nn\n\u2211\nz1\u2208\u2126\nd(x, z1)\n\u2212 1\nn2\n\u2211\nz2\u2208\u2126\n\u2211\nz1\u2208\u2126\nd(z2, z1)\u2212 d(x, y).\nAs a result of Lemma 16, it also satisfies (C1)\u2013(C3) in Definition 15. As such, we call the cohesion measure \u03b3(\u00b7, \u00b7) defined in Section II the dual cohesion measure of the distance measure d(\u00b7, \u00b7).\nOn the other hand, if \u03b2(x, y) is a cohesion measure for \u2126, then there is an induced distance measure and it can be viewed as the dual distance measure of the cohesion measure \u03b2(x, y). This is shown in the following lemma and its proof is given in Appendix G.\nLemma 17 Suppose that \u03b2(\u00b7, \u00b7) is a cohesion measure for \u2126. Let\nd(x, y) = (\u03b2(x, x) + \u03b2(y, y))/2\u2212 \u03b2(x, y). (22)\nThen d(\u00b7, \u00b7) is a distance measure that satisfies (D1)\u2013(D4).\nIn the following theorem, we show the duality result. Its proof is given in Appendix H.\nTheorem 18 Consider a set of data points \u2126. For a distance measure d(\u00b7, \u00b7) that satisfies (D1)\u2013(D4), let\nd\u2217(x, y) = 1\nn\n\u2211\nz2\u2208\u2126\nd(z2, y) + 1\nn\n\u2211\nz1\u2208\u2126\nd(x, z1)\n\u2212 1\nn2\n\u2211\nz2\u2208\u2126\n\u2211\nz1\u2208\u2126\nd(z2, z1)\u2212 d(x, y) (23)\nbe the dual cohesion measure of d(\u00b7, \u00b7). On the other hand, For a cohesion measure \u03b2(\u00b7, \u00b7) that satisfies (C1)\u2013(C3), let\n\u03b2\u2217(x, y) = (\u03b2(x, x) + \u03b2(y, y))/2\u2212 \u03b2(x, y) (24)\nbe the dual distance measure of \u03b2(\u00b7, \u00b7). Then d\u2217\u2217(x, y) = d(x, y) and \u03b2\u2217\u2217(x, y) = \u03b2(x, y) for all x, y \u2208 \u2126.\nALGORITHM 3: The dual K-sets Algorithm\nInput: A data set \u2126 = {x1, x2, . . . , xn}, a cohesion measure \u03b3(\u00b7, \u00b7), and the number of sets K . Output: A partition of sets {S1, S2, . . . , SK}. (0) Initially, choose arbitrarily K disjoint nonempty sets S1, . . . , SK as a partition of \u2126. (1) for i = 1, 2, . . . , n do\nCompute the triangular distance \u2206(xi, Sk) for each set Sk by using (26). Find the set to which the point xi is closest in terms of the triangular distance. Assign point xi to that set.\nend (2) Repeat from (1) until there is no further change.\nB. The dual K-sets algorithm\nFor the K-sets algorithm, we need to have a distance measure. In view of the duality theorem between a cohesion measure and a distance measure, we propose the dual K-sets algorithm in Algorithm 3 that uses a cohesion measure. As before, for a cohesion measure \u03b3(\u00b7, \u00b7) between two points, we define the cohesion measure between two sets S1 and S2 as\n\u03b3(S1, S2) = \u2211\nx\u2208S1\n\u2211\ny\u2208S2\n\u03b3(x, y). (25)\nAlso, note from (16) that the triangular distance from a point x to a set S can be computed by using the cohesion measure as follows:\n\u2206(x, S) = \u03b3(x, x) \u2212 2\n|S| \u03b3({x}, S) +\n1\n|S|2 \u03b3(S, S). (26)\nAs a direct result of the duality theorem in Theorem 18 and the convergence result of the K-sets algorithm in Theorem 14, we have the following convergence result for the dual K-sets algorithm.\nCorollary 19 As in (13), we define the normalized modularity as \u2211K k=1 1 |Sk| \u03b3(Sk, Sk). For the dual K-sets algorithm, the normalized modularity is increasing when there is a change, i.e., a point is moved from one set to another. Thus, the algorithm converges to a local optimum of the normalized modularity. Moreover, for K = 2, the dual K-sets algorithm yields two clusters when the algorithm converges.\nC. Connections to the kernel K-means algorithm\nIn this section, we show the connection between the dual K-sets algorithm and the kernel K-means algorithm in the literature (see e.g., [55]). Let us consider the n\u00d7n matrix \u0393 = (\u03b3i,j) with \u03b3i,j = \u03b3(xi, xj) being the cohesion measure between xi and xj . Call the matrix \u0393 the cohesion matrix (corresponding to the cohesion measure \u03b3(\u00b7, \u00b7)). Since \u03b3(xi, xj) = \u03b3(xj , xi), the matrix \u0393 is symmetric and thus have real eigenvalues \u03bbk, k = 1, 2, . . . , n. Let I be the n\u00d7n identity matrix and \u03c3 \u2265 \u2212min1\u2264k\u2264n \u03bbk. Then the matrix \u0393\u0303 = \u03c3I + \u0393 is positive semi-definite as its n eigenvalues \u03bb\u0303k = \u03c3 + \u03bbk, k = 1, 2, . . . , N are all nonnegative. Let vk = (vk,1, vk,2, . . . , vk,n)T , k = 1, 2, . . . , n be the eigenvector of \u0393 corresponding to the eigenvalue \u03bbk. Then vk is also the eigenvector of \u0393\u0303 corresponding to the eigenvalue \u03bb\u0303k. Thus, we can decompose the matrix \u0393\u0303 as follows:\n\u0393\u0303 =\nn \u2211\nk=1\n\u03bb\u0303kvkv T k , (27)\nwhere vTk is the transpose of vk. Now we choose the mapping \u03c6 : \u2126 7\u2192 R n as follows:\n\u03c6(xi) = (\n\u221a\n\u03bb\u03031v1,i,\n\u221a\n\u03bb\u03032v2,i, . . . ,\n\u221a\n\u03bb\u0303nvn,i\n)T\n, (28)\nfor i = 1, 2 . . . , n. Note that\n\u03c6(xi) T \u00b7 \u03c6(xj) =\nn \u2211\nk=1\n\u03bb\u0303kvk,ivk,j\n= (\u0393\u0303)i,j = \u03c3\u03b4i,j + \u03b3(xi, xj), (29)\nwhere \u03b4i,j = 1 if i = j and 0 otherwise. The \u201ccentroid\u201d of a set S can be represented by the corresponding centroid in Rn, i.e.,\n1\n|S|\n\u2211\ny\u2208S\n\u03c6(y), (30)\nand the square of the \u201cdistance\u201d between a point x and the \u201ccentroid\u201d of a set S is (\n\u03c6(x) \u2212 1\n|S|\n\u2211\ny\u2208S\n\u03c6(y) )T \u00b7 ( \u03c6(x) \u2212 1\n|S|\n\u2211\ny\u2208S\n\u03c6(y) )\n= \u03c6(x)T \u00b7 \u03c6(x) \u2212 2\n|S|\n\u2211\ny\u2208S\n\u03c6(x)T \u00b7 \u03c6(y)\n+ 1\n|S|2\n\u2211\ny1\u2208S\n\u2211\ny2\u2208S\n\u03c6(y1) T \u00b7 \u03c6(y2)\n= \u03b3(x, x) + \u03c3 \u2212 2\n|S|\n\u2211\ny\u2208S\n\u03b3(x, y)\u2212 2\u03c3\n|S| 1{x\u2208S}\n+ 1\n|S|2\n\u2211\ny1\u2208S\n\u2211\ny2\u2208S\n\u03b3(y1, y2) + |S|\u03c3\n|S|2\n= (1 \u2212 2\n|S| 1{x\u2208S} +\n1\n|S| )\u03c3 + \u03b3(x, x)\u2212\n2\n|S| \u03b3({x}, S)\n+ 1\n|S|2 \u03b3(S, S), (31)\nwhere 1{x\u2208S} is the indicator function that has value 1 if x is in S and 0 otherwise. In view of (16), we then have (\n\u03c6(x) \u2212 1\n|S|\n\u2211\ny\u2208S\n\u03c6(y) )T \u00b7 ( \u03c6(x) \u2212 1\n|S|\n\u2211\ny\u2208S\n\u03c6(y) )\n= (1\u2212 2\n|S| 1{x\u2208S} +\n1\n|S| )\u03c3 +\u2206(x, S), (32)\nwhere \u2206(x, S) is the triangular distance from a point x to a set S. Thus, the square of the \u201cdistance\u201d between a point x and the \u201ccentroid\u201d of a set S is (1\u2212 1|S|)\u03c3+\u2206(x, S) for a point x \u2208 S and (1+ 1 |S|)\u03c3+\u2206(x, S) for a point x 6\u2208 S. In particular, when \u03c3 = 0, the dual K-sets algorithm is the same as the sequential kernel K-means algorithm for the kernel \u0393\u0303. Unfortunately, the matrix \u0393\u0303 may not be positive semi-definite if \u03c3 is chosen to be 0. As indicated in [55], a large \u03c3 decreases (resp. increases) the distance from a point x to a set S that contains (resp. does not contain) that point. As such, a point is more unlikely to move from one set to another set and the kernel K-means algorithm is thus more likely to be trapped in a local optimum.\nTo summarize, the dual K-sets algorithm operates in the same way as a sequential version of the classical kernel K-means algorithm by viewing the matrix \u0393 as a kernel. However, there are two key differences between the dual K-sets algorithm and the classical kernel K-means algorithm: (i) the dual K-sets algorithm guarantees the convergence even though the matrix \u0393 from a cohesion measure is not positive semi-definite, and (ii) the dual K-sets algorithm can only be operated sequentially and the kernel K-means algorithm can be operated in batches. To further illustrate the difference between these two algorithms, we show in the following two examples that a cohesion matrix may not be positive semi-definite and a positive semi-definite matrix may not be a cohesion matrix.\nExample 20 In this example, we show there is a cohesion matrix \u0393 that is not a positive semi-definite matrix.\n\u0393 =\n\n     0.44 0.04 0.04 0.04 \u22120.56 0.04 0.64 \u22120.36 \u22120.36 0.04 0.04 \u22120.36 0.64 \u22120.36 0.04 0.04 \u22120.36 \u22120.36 0.64 0.04\n\u22120.56 0.04 0.04 0.04 0.44\n\n     . (33)\nThe eigenvalues of this matrix are \u22120.2, 0, 1, 1, and 1.\nExample 21 In this example, we show there is a positive semi-definite matrix M = (mi,j) that is not an cohesion matrix.\nM =\n\n   0.375 \u22120.025 \u22120.325 \u22120.025 \u22120.025 0.875 \u22120.025 \u22120.825 \u22120.325 \u22120.025 0.375 \u22120.025 \u22120.025 \u22120.825 \u22120.025 0.875\n\n   . (34)\nThe eigenvalues of this matrix are 0, 0.1, 0.7, and 1.7. Even though the matrix M is symmetric and has all its row sums and column sums being 0, it is still not a cohesion matrix as m1,1 \u2212m1,2 \u2212m1,4 +m2,4 = \u22120.4 < 0.\nD. Constructing a cohesion measure from a similarity measure\nA similarity measure is in general defined as a bivariate function of two distinct data points and it is often characterized by a square matrix without specifying the diagonal elements. In the following, we show how one can construct a cohesion measure from a symmetric bivariate function by further specifying the diagonal elements. Its proof is given in Appendix I.\nProposition 22 Suppose a bivariate function \u03b20 : \u2126\u00d7\u2126 7\u2192 R is symmetric, i.e., \u03b20(x, y) = \u03b20(y, x). Let \u03b21(x, y) = \u03b20(x, y) for all x 6= y and specify \u03b21(x, x) such that\n\u03b21(x, x) \u2265 max x 6=y 6=z [\u03b21(x, z) + \u03b21(x, y) \u2212 \u03b21(y, z)]. (35)\nAlso, let\n\u03b2(x, y) = \u03b21(x, y)\u2212 1\nn\n\u2211\nz1\u2208\u2126\n\u03b21(z1, y)\n\u2212 1\nn\n\u2211\nz2\u2208\u2126\n\u03b21(x, z2) + 1\nn2\n\u2211\nz1\u2208\u2126\n\u2211\nz2\u2208\u2126\n\u03b21(z1, z2). (36)\nThen \u03b2(x, y) is a cohesion measure for \u2126.\nWe note that one simple choice for specifying \u03b21(x, x) in (35) is to set\n\u03b21(x, x) = 2\u03b2max \u2212 \u03b2min, (37)\nwhere \u03b2max = max\nx 6=y \u03b2(x, y), (38)\nand \u03b2min = min\nx 6=y \u03b2(x, y). (39)\nIn particular, if the similarity measure \u03b2(x, y) only has values 0 and 1 as in the adjacency matrix of a simple undirected graph, then one can simply choose \u03b21(x, x) = 2 for all x.\nExample 23 (A cohesion measure for a graph) As an illustrating example, suppose A = (ai,j) is the n \u00d7 n adjacency matrix of a simple undirected graph with ai,j = 1 if there is an edge between node i and node j and 0 otherwise. Let ki = \u2211n j=1 ai,j be the degree of node i and m = 1 2 \u2211n\ni=1 ki be the total number of edges in the graph. Then one can simply let \u03b21(i, j) = 2\u03b4i,j +ai,j , where \u03b4i,j = 1 if i = j and 0 otherwise. By doing so, we then have the following cohesion measure\n\u03b2(i, j) = 2\u03b4i,j + ai,j \u2212 2 + ki n \u2212 2 + kj n + 2m+ 2n n2 . (40)\nWe note that such a cohesion measure is known as the deviation to indetermination null model in [56]."}, {"heading": "VI. CONCLUSIONS", "text": "In this paper, we developed a mathematical theory for clustering in metric spaces based on distance measures and cohesion measures. A cluster is defined as a set of data points that are cohesive to themselves. The hierarchical agglomerative algorithm in Algorithm 1 was shown to converge with a partition of clusters. Our hierarchical agglomerative algorithm differs from a standard hierarchical agglomerative algorithm in two aspects: (i) there is a stopping criterion for our algorithm, and (ii) there is no need to use the greedy selection. We also proposed the K-sets algorithm in Algorithm 2 based on the concept of triangular distance. Such an algorithm appears to be new. Unlike the Lloyd iteration, it only takes one-step minimization in each iteration and that might give the K-sets algorithm the computational advantage over the K-medoids algorithms. The K-sets algorithm was shown to converge with a partition of two clusters when K = 2. Another interesting finding of the paper is the duality result between a distance measure and a cohesion measure. As such, one can perform clustering either by a distance measure or a cohesion measure. In particular, the dual K-sets algorithm in Algorithm 3 converges in the same way as a sequential version of the kernel K-means algorithm without the need for the cohesion matrix to positive semi-definite.\nThere are several possible extensions for our work: (i) Asymmetric distance measure: One possible extension is to remove the symmetric property in (D3) for a distance measure. Our preliminary result shows that one only needs d(x, x) = 0 in (D2) and the triangular inequality in (D4) for the K-sets algorithm to converge. The key insight for this is that one can replace the original distance measure d(x, y) by a new distance measure d\u0303(x, y) = d(x, y) + d(y, x). By doing so, the new distance measure is symmetric.\n(ii) Distance measure without the triangular inequality: Another possible extension is to remove the triangular inequality in (D4). However, the K-sets algorithm does not work properly in this setting as the triangular distance is no longer nonnegative. In order for the K-sets algorithm to converge, our preliminary result shows that one can adjust the value of the triangular distance based on a weaker notion of cohesion measure. Results along this line will be reported separately. (iii) Performance guarantee: Like the K-means algorithm, the output of the K-sets algorithm also depends on the initial partition. It would be of interest to see if it is possible to derive performance guarantee for the K-sets algorithm (or the optimization problem for the normalized modularity). In particular, the approach by approximation stability in [21] might be applicable as their threshold graph lemma seems to hold when one replaces the distance from a point x to its center c, i.e., d(x, c), by the average distance of a point x to its set, i.e., d\u0304(x, S). (iv) Local clustering: The problem of local clustering is to find a cluster that contains a specific point x. Since we already define what a cluster is, we may use the hierarchical agglomerative algorithm in Algorithm 1 to find a cluster that contains x. One potential problem of such an approach is the output cluster might be very big. Analogous to the concept of community strength in [34], it would be of interest to define a concept of cluster strength and stop the agglomerative process when the desired cluster strength can no longer be met. (v) Reduction of computational complexity: Note that the computation complexity for each iteration within the FOR loop of the K-sets algorithm is O(Kn2) as it takes O(Kn) steps to compute the triangular distance for each point and there are n points that need to be assigned in each iteration. To further reduce the computational complexity for such an algorithm, one might exploit the idea of \u201csparsity\u201d and this can be done by the transformation of distance measure."}, {"heading": "ACKNOWLEDGEMENT", "text": "This work was supported in part by the Excellent Research Projects of National Taiwan University, under Grant Number AE00-00-04, and in part by National Science Council (NSC), Taiwan, under Grant Numbers NSC102-2221-E-002-014-MY2 and 102-2221-E-007 -006 -MY3."}, {"heading": "APPENDIX E", "text": "In this section, we prove Theorem 14. For this, we need to prove the following two inequalities.\nLemma 28 For any set S and any point x that is not in S, \u2211\ny\u2208S\u222a{x}\n\u2206(y, S \u222a {x}) \u2264 \u2211\ny\u2208S\u222a{x}\n\u2206(y, S), (62)\nand \u2211\ny\u2208S\n\u2206(y, S) \u2264 \u2211\ny\u2208S\n\u2206(y, S \u222a {x}). (63)\nProof. We first show that for any set S and any point x that is not in S,\nd\u0304(S \u222a {x}, S \u222a {x})\u2212 2d\u0304(S \u222a {x}, S) + d\u0304(S, S) \u2264 0. (64)\nFrom the symmetric property in Fact 24(ii) and the weighted average property in Fact 24(iii), we have\nd\u0304(S \u222a {x}, S \u222a {x}) = |S|2\n(|S|+ 1)2 d\u0304(S, S)\n+ 2|S|\n(|S|+ 1)2 d\u0304({x}, S) +\n1\n(|S|+ 1)2 d\u0304({x}, {x}),\nand\nd\u0304(S \u222a {x}, S) = |S|\n|S|+ 1 d\u0304(S, S) +\n1\n|S|+ 1 d\u0304({x}, S).\nNote that d\u0304({x}, {x}) = d(x, x) = 0.\nThus,\nd\u0304(S \u222a {x}, S \u222a {x})\u2212 2d\u0304(S \u222a {x}, S) + d\u0304(S, S) = 1\n(|S|+ 1)2\n( d\u0304(S, S)\u2212 2d\u0304({x}, S) ) \u2264 0,\nwhere we use (15) in the last inequality. Note from (15) that\n\u2211\ny\u2208S2\n\u2206(y, S1) = \u2211\ny\u2208S2\n(2d\u0304({y}, S1)\u2212 d\u0304(S1, S1))\n= |S2| \u00b7 ( 2d\u0304(S1, S2)\u2212 d\u0304(S1, S1) ) . (65)\nUsing (65) yields \u2211\ny\u2208S\u222a{x}\n\u2206(y, S \u222a {x})\u2212 \u2211\ny\u2208S\u222a{x}\n\u2206(y, S)\n= |S \u222a {x}| \u00b7 ( 2d\u0304(S \u222a {x}, S \u222a {x})\n\u2212d\u0304(S \u222a {x}, S \u222a {x}) )\n\u2212|S \u222a {x}| \u00b7 ( 2d\u0304(S, S \u222a {x})\u2212 d\u0304(S, S) )\n= |S \u222a {x}| \u00b7 ( d\u0304(S \u222a {x}, S \u222a {x})\n\u22122d\u0304(S, S \u222a {x}) + d\u0304(S, S) ) . (66)\nAs a result of (64), we then have \u2211\ny\u2208S\u222a{x}\n\u2206(y, S \u222a {x})\u2212 \u2211\ny\u2208S\u222a{x}\n\u2206(y, S) \u2264 0.\nSimilarly, using (65) and (64) yields \u2211\ny\u2208S\n\u2206(y, S)\u2212 \u2211\ny\u2208S\n\u2206(y, S \u222a {x})\n= |S| \u00b7 ( 2d\u0304(S, S)\u2212 d\u0304(S, S) )\n\u2212|S| \u00b7 ( 2d\u0304(S \u222a {x}, S)\u2212 d\u0304(S \u222a {x}, S \u222a {x}) )\n= |S| \u00b7 ( d\u0304(S \u222a {x}, S \u222a {x})\n\u22122d\u0304(S \u222a {x}, S) + d\u0304(S, S) )\n\u2264 0 (67)\nProof. (Theorem 14) (i) Let Sk (resp. S\u2032k), k = 1, 2, . . . ,K , be the partition before (resp. after) the change. Also let c(x) be the index of the set to which x belongs. Suppose that \u2206(xi, Sk\u2217) < \u2206(xi, Sc(xi)) and xi is moved from Sc(xi) to Sk\u2217 for some point xi and some k\u2217. In this case, we have S\u2032k\u2217 = Sk\u2217 \u222a {xi}, S \u2032 c(xi) = S\u2032c(xi)\\{x} and S \u2032 k = Sk for all k 6= c(xi), k \u2217.\nIt then follows from \u2206(xi, Sk\u2217) < \u2206(xi, Sc(xi)) that\nK \u2211\nk=1\n\u2211\nx\u2208Sk\n\u2206(x, Sk)\n= \u2211\nk 6=c(xi),k\u2217\n\u2211\nx\u2208Sk\n\u2206(x, Sk)\n+ \u2211\nx\u2208Sc(xi)\n\u2206(x, Sc(xi)) + \u2211\nx\u2208Sk\u2217\n\u2206(x, Sk\u2217)\n= \u2211\nk 6=c(xi),k\u2217\n\u2211\nx\u2208Sk\n\u2206(x, Sk) + \u2211\nx\u2208Sc(xi)\\{xi}\n\u2206(x, Sc(xi))\n+\u2206(xi, Sc(xi)) + \u2211\nx\u2208Sk\u2217\n\u2206(x, Sk\u2217)\n> \u2211\nk 6=c(xi),k\u2217\n\u2211\nx\u2208Sk\n\u2206(x, Sk) + \u2211\nx\u2208Sc(xi)\\{xi}\n\u2206(x, Sc(xi))\n+\u2206(xi, Sk\u2217) + \u2211\nx\u2208Sk\u2217\n\u2206(x, Sk\u2217)\n= \u2211\nk 6=c(xi),k\u2217\n\u2211\nx\u2208S\u2032 k\n\u2206(x, S\u2032k) + \u2211\nx\u2208Sc(xi)\\{xi}\n\u2206(x, Sc(xi))\n+ \u2211\nx\u2208Sk\u2217\u222a{xi}\n\u2206(x, Sk\u2217 ), (68)\nwhere we use the fact that S\u2032k = Sk for all k 6= c(xi), k \u2217, in the last equality. From (62) and S\u2032k\u2217 = Sk\u2217 \u222a{xi}, we know that\n\u2211\nx\u2208Sk\u2217\u222a{xi}\n\u2206(x, Sk\u2217) \u2265 \u2211\nx\u2208Sk\u2217\u222a{xi}\n\u2206(x, Sk\u2217 \u222a {xi})\n= \u2211\nx\u2208S\u2032 k\u2217\n\u2206(x, S\u2032k\u2217). (69)\nAlso, it follows from (63) and S\u2032c(xi) = S \u2032 c(xi) \\{xi} that \u2211\nx\u2208Sc(xi)\\{xi}\n\u2206(x, Sc(xi)) \u2265 \u2211\nx\u2208Sc(xi)\\{xi}\n\u2206(x, Sc(xi)\\{xi})\n= \u2211\nx\u2208S\u2032 c(xi)\n\u2206(x, S\u2032c(xi)). (70)\nUsing (69) and (70) in (68) yields\nK \u2211\nk=1\n\u2211\nx\u2208Sk\n\u2206(x, Sk)\n> \u2211\nk 6=c(xi),k\u2217\n\u2211\nx\u2208S\u2032 k\n\u2206(x, S\u2032k) + \u2211\nx\u2208S\u2032 k\u2217\n\u2206(x, S\u2032k\u2217)\n+ \u2211\nx\u2208S\u2032 c(xi)\n\u2206(x, S\u2032c(xi))\n=\nK \u2211\nk=1\n\u2211\nx\u2208S\u2032 k\n\u2206(x, S\u2032k). (71)\nIn view of (17) and (71), we then conclude that the normalized modularity is increasing when there is a change. Since there is only a finite number of partitions for \u2126, the algorithm thus converges to a local optimum of the normalized modularity.\n(ii) The algorithm converges when there are no further changes. As such, we know for any x \u2208 Si and j 6= i, \u2206(x, Si) \u2264 \u2206(x, Sj). Summing up all the points x \u2208 Si and using (15) yields\n0 \u2265 \u2211\nx\u2208Si\n( \u2206(x, Si)\u2212\u2206(x, Sj) )\n= \u2211\nx\u2208Si\n( 2d\u0304({x}, Si)\u2212 d\u0304(Si, Si) )\n\u2212 \u2211\nx\u2208Si\n( 2d\u0304({x}, Sj)\u2212 d\u0304(Sj , Sj) )\n= |Si| \u00b7 ( d\u0304(Si, Si)\u2212 2d\u0304(Si, Sj) + d\u0304(Sj , Sj) ) . (72)\nWhen the two sets Si and Sj are viewed in isolation (by removing the data points not in Si \u222a Sj from \u2126), we have Sj = Sci . Thus,\nd\u0304(Si, Si)\u2212 2d\u0304(Si, S c i ) + d\u0304(S c i , S c i ) \u2264 0.\nAs a result of Theorem 7(viii), we conclude that Si is a cluster when the two sets Si and Sj are viewed in isolation. Also, Theorem 7(ii) implies that Sj = Sci is also a cluster when the two sets Si and Sj are viewed in isolation.\nAPPENDIX F\nIn this section, we prove Lemma 16. We first show (C1). Since d(x, y) = d(y, x) for all x 6= y, we have from (21) that \u03b2(x, y) = \u03b2(y, x) for all x 6= y. To verify (C2), note from (21) that\n\u2211\ny\u2208\u2126\n\u03b2(x, y) = 1\nn\n\u2211\ny\u2208\u2126\n\u2211\nz2\u2208\u2126\nd(z2, y) + \u2211\nz1\u2208\u2126\nd(x, z1)\n\u2212 1\nn\n\u2211\nz2\u2208\u2126\n\u2211\nz1\u2208\u2126\nd(z2, z1)\u2212 \u2211\ny\u2208\u2126\nd(x, y)\n= 0. (73)\nNow we show (C3). Note from (21) that\n\u03b2(x, x) + \u03b2(y, z)\u2212 \u03b2(x, z)\u2212 \u03b2(x, y)\n= \u2212d(x, x)\u2212 d(y, z) + d(x, z) + d(x, y). (74)\nSince d(x, x) = 0, it then follows from the triangular inequality for d(\u00b7, \u00b7) that\n\u03b2(x, x) + \u03b2(y, z)\u2212 \u03b2(x, z)\u2212 \u03b2(x, y) \u2265 0.\nAPPENDIX G\nIn this section, we prove Lemma 17. Clearly, d(x, x) = 0 from (22) and thus (D2) holds trivially. That (D3) holds follows from the symmetric property in (C1)\nof Definition 15. To see (D1), choosing z = y in (20) yields\n0 \u2264 \u03b2(x, x) + \u03b2(y, y)\u2212 \u03b2(x, y) \u2212 \u03b2(x, y) = 2d(x, y).\nFor the triangular inequality in (D4), note from (22) and (20) in (C3) that\nd(x, z) + d(z, y)\u2212 d(x, y)\n= (\u03b2(x, x) + \u03b2(z, z))\n2 \u2212 \u03b2(x, z) +\n(\u03b2(z, z) + \u03b2(y, y))\n2\n\u2212\u03b2(z, y)\u2212 (\u03b2(x, x) + \u03b2(y, y))\n2 + \u03b2(x, y)\n= \u03b2(z, z) + \u03b2(x, y)\u2212 \u03b2(z, x)\u2212 \u03b2(z, y) \u2265 0.\nAPPENDIX H\nIn this section, we prove Theorem 18. We first show that d\u2217\u2217(x, y) = d(x, y) for a distance measure d(\u00b7, \u00b7). Note from (23) and d(x, x) = 0 that\nd\u2217(x, x) = 1\nn\n\u2211\nz2\u2208\u2126\nd(z2, x) + 1\nn\n\u2211\nz1\u2208\u2126\nd(x, z1)\n\u2212 1\nn2\n\u2211\nz2\u2208\u2126\n\u2211\nz1\u2208\u2126\nd(z2, z1). (75)\nFrom the symmetric property of d(\u00b7, \u00b7), it then follows that\nd\u2217(x, x) = 2\nn\n\u2211\nz1\u2208\u2126\nd(x, z1)\u2212 1\nn2\n\u2211\nz2\u2208\u2126\n\u2211\nz1\u2208\u2126\nd(z2, z1). (76)\nSimilarly,\nd\u2217(y, y) = 2\nn\n\u2211\nz2\u2208\u2126\nd(z2, y)\u2212 1\nn2\n\u2211\nz2\u2208\u2126\n\u2211\nz1\u2208\u2126\nd(z2, z1). (77)\nUsing (23), (76) and (77) in (24) yields\nd\u2217\u2217(x, y) = (d\u2217(x, x) + d\u2217(y, y))/2\u2212 d\u2217(x, y) = d(x, y). (78)\nNow we show that \u03b2\u2217\u2217(x, y) = \u03b2(x, y) for a cohesion measure \u03b2(\u00b7, \u00b7). Note from (24) that\n\u03b2\u2217(z2, y) + \u03b2 \u2217(x, z1)\u2212 \u03b2 \u2217(z1, z2)\u2212 \u03b2 \u2217(x, y)\n= \u2212\u03b2(z2, y)\u2212 \u03b2(x, z1) + \u03b2(z1, z2) + \u03b2(x, y). (79)\nAlso, we have from (23) that\n\u03b2\u2217\u2217(x, y) = 1\nn\n\u2211\nz2\u2208\u2126\n\u03b2\u2217(z2, y) + 1\nn\n\u2211\nz1\u2208\u2126\n\u03b2\u2217(x, z1)\n\u2212 1\nn2\n\u2211\nz2\u2208\u2126\n\u2211\nz1\u2208\u2126\n\u03b2\u2217(z2, z1)\u2212 \u03b2 \u2217(x, y)\n= 1\nn2\n\u2211\nz2\u2208\u2126\n\u2211\nz1\u2208\u2126\n(\n\u03b2\u2217(z2, y) + \u03b2 \u2217(x, z1)\n\u2212\u03b2\u2217(z1, z2)\u2212 \u03b2 \u2217(x, y)\n)\n. (80)\nUsing (79) in (80) yields\n\u03b2\u2217\u2217(x, y)\n= 1\nn2\n\u2211\nz2\u2208\u2126\n\u2211\nz1\u2208\u2126\n(\n\u03b2(x, y) + \u03b2(z1, z2)\n\u2212\u03b2(x, z1)\u2212 \u03b2(z2, y) )\n= \u03b2(x, y) + 1\nn2\n\u2211\nz2\u2208\u2126\n\u2211\nz1\u2208\u2126\n\u03b2(z1, z2)\u2212 1\nn\n\u2211\nz1\u2208\u2126\n\u03b2(x, z1)\n\u2212 1\nn\n\u2211\nz1\u2208\u2126\n\u03b2(z2, y). (81)\nSince \u03b2(\u00b7, \u00b7) is a cohesion measure that satisfies (C1)\u2013(C3), we have from (C1) and (C2) that the last three terms in (81) are all equal to 0. Thus, \u03b2\u2217\u2217(x, y) = \u03b2(x, y).\nAPPENDIX I\nIn this section, we prove Proposition 22. We first show (C1). Since \u03b21(x, y) = \u03b20(x, y) for all x 6= y, we have from the symmetric property of \u03b20(\u00b7, \u00b7) that\n\u03b21(x, y) = \u03b21(y, x) for all x 6= y. In view of (36), we then also have \u03b2(x, y) = \u03b2(y, x) for all x 6= y. To verify (C2), note from (36) that\n\u2211\ny\u2208\u2126\n\u03b2(x, y) = \u2211\ny\u2208\u2126\n\u03b21(x, y)\u2212 1\nn\n\u2211\ny\u2208\u2126\n\u2211\nz1\u2208\u2126\n\u03b21(z1, y)\n\u2212 \u2211\nz2\u2208\u2126\n\u03b21(x, z2) + 1\nn\n\u2211\nz1\u2208\u2126\n\u2211\nz2\u2208\u2126\n\u03b21(z1, z2)\n= 0. (82)\nNow we show (C3). Note from (36) that\n\u03b2(x, x) + \u03b2(y, z)\u2212 \u03b2(x, z)\u2212 \u03b2(x, y)\n= \u03b21(x, x) + \u03b21(y, z)\u2212 \u03b21(x, z)\u2212 \u03b21(x, y). (83)\nIt then follows from (35) that for all x 6= y 6= z that\n\u03b2(x, x) + \u03b2(y, z)\u2212 \u03b2(x, z)\u2212 \u03b2(x, y) \u2265 0. (84)\nIf either x = y or x = z, we also have\n\u03b2(x, x) + \u03b2(y, z)\u2212 \u03b2(x, z)\u2212 \u03b2(x, y) = 0.\nThus, it remains to show the case that y = z and x 6= y. For this case, we need to show that\n\u03b2(x, x) + \u03b2(y, y)\u2212 \u03b2(x, y)\u2212 \u03b2(x, y) \u2265 0. (85)\nNote from (84) that\n\u03b2(y, y) + \u03b2(x, z)\u2212 \u03b2(y, z)\u2212 \u03b2(y, x) \u2265 0. (86)\nSumming the two inequalities in (84) and (86) and using the symmetric property of \u03b2(\u00b7, \u00b7) yields the desired inequality in (85)."}], "references": [{"title": "Mining of massive datasets", "author": ["Anand Rajaraman", "Jure Leskovec", "Jeffrey David Ullman"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Data clustering: a review", "author": ["Anil K Jain", "M Narasimha Murty", "Patrick J Flynn"], "venue": "ACM computing surveys (CSUR),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Data clustering: 50 years beyond k-means", "author": ["Anil K Jain"], "venue": "Pattern Recognition Letters,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Least squares quantization in pcm", "author": ["Stuart Lloyd"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1982}, {"title": "Finding groups in data: an introduction to cluster analysis, volume 344", "author": ["Leonard Kaufman", "Peter J Rousseeuw"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "A new partitioning around medoids algorithm", "author": ["Mark Van der Laan", "Katherine Pollard", "Jennifer Bryan"], "venue": "Journal of Statistical Computation and Simulation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "A simple and fast algorithm for k-medoids clustering", "author": ["Hae-Sang Park", "Chi-Hyuck Jun"], "venue": "Expert Systems with Applications,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "How fast is the k-means method? Algorithmica", "author": ["Sariel Har-Peled", "Bardia Sadri"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "k-means++: The advantages of careful seeding", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Scalable k-means++", "author": ["Bahman Bahmani", "Benjamin Moseley", "Andrea Vattani", "Ravi Kumar", "Sergei Vassilvitskii"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Analysis of k-means++ for separable data. In Approximation, Randomization, and Combinatorial Optimization", "author": ["Ragesh Jaiswal", "Nitin Garg"], "venue": "Algorithms and Techniques,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "k-means++ under approximation stability", "author": ["Manu Agarwal", "Ragesh Jaiswal", "Arindam Pal"], "venue": "In Theory and Applications of Models of Computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Normalized cuts and image segmentation", "author": ["Jianbo Shi", "Jitendra Malik"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2000}, {"title": "Support vector clustering", "author": ["Asa Ben-Hur", "David Horn", "Hava T Siegelmann", "Vladimir Vapnik"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "K-means clustering via principal component analysis", "author": ["Chris Ding", "Xiaofeng He"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "A novel kernel method for clustering", "author": ["Francesco Camastra", "Alessandro Verri"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Parallel spectral clustering in distributed systems", "author": ["Wen-Yen Chen", "Yangqiu Song", "Hongjie Bai", "Chih-Jen Lin", "Edward Y Chang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "A tutorial on spectral clustering", "author": ["Ulrike Von Luxburg"], "venue": "Statistics and computing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "A survey of kernel and spectral methods for clustering", "author": ["Maurizio Filippone", "Francesco Camastra", "Francesco Masulli", "Stefano Rovetta"], "venue": "Pattern recognition,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Clustering under approximation stability", "author": ["Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Towards a statistical theory of clustering", "author": ["Ulrike Von Luxburg", "Shai Ben-David"], "venue": "In Pascal workshop on statistics and optimization of clustering. Citeseer,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "A formalization of cluster analysis", "author": ["William E Wright"], "venue": "Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1973}, {"title": "A theory of proximity based clustering: Structure detection by optimization", "author": ["Jan Puzicha", "Thomas Hofmann", "Joachim M Buhmann"], "venue": "Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2000}, {"title": "An impossibility theorem for clustering", "author": ["Jon Kleinberg"], "venue": "Advances in neural information processing systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2003}, {"title": "A uniqueness theorem for clustering", "author": ["Reza Bosagh Zadeh", "Shai Ben-David"], "venue": "In Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Hierarchical quasi-clustering methods for asymmetric networks", "author": ["Gunnar Carlsson", "Facundo M\u00e9moli", "Alejandro Ribeiro", "Santiago Segarra"], "venue": "arXiv preprint arXiv:1404.4655,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Measures of clustering quality: A working set of axioms for clustering", "author": ["Shai Ben-David", "Margareta Ackerman"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise", "author": ["Martin Ester", "Hans-Peter Kriegel", "J\u00f6rg Sander", "Xiaowei Xu"], "venue": "In Kdd,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1996}, {"title": "Cluster analysis: a further approach based on density estimation", "author": ["Antonio Cuevas", "Manuel Febrero", "Ricardo Fraiman"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2001}, {"title": "A density-based cluster validity approach using multi-representatives", "author": ["Maria Halkidi", "Michalis Vazirgiannis"], "venue": "Pattern Recognition Letters,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "The link prediction problem for social networks", "author": ["David Liben-Nowell", "Jon Kleinberg"], "venue": "In Proceedings of the twelfth international conference on Information and knowledge management,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2003}, {"title": "Relative centrality and local community detection", "author": ["Cheng-Shang Chang", "Chih-Jung Chang", "Wen-Ting Hsieh", "Duan-Shin Lee", "Li-Heng Liou", "Wanjiun Liao"], "venue": "Network Science, FirstView:1\u201335,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "On modularity clustering", "author": ["Ulrik Brandes", "Daniel Delling", "Marco Gaertler", "Robert Gorke", "Martin Hoefer", "Zoran Nikoloski", "Dorothea Wagner"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "Fast algorithm for detecting community structure in networks", "author": ["Mark EJ Newman"], "venue": "Physical review E,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2004}, {"title": "Finding and evaluating community structure in networks", "author": ["Mark EJ Newman", "Michelle Girvan"], "venue": "Physical review E,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2004}, {"title": "Fast unfolding of communities in large networks", "author": ["Vincent D Blondel", "Jean-Loup Guillaume", "Renaud Lambiotte", "Etienne Lefebvre"], "venue": "Journal of Statistical Mechanics: Theory and Experiment,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2008}, {"title": "Multi-scale modularity in complex networks. In Modeling and Optimization in Mobile, Ad Hoc and Wireless Networks (WiOpt), 2010", "author": ["Renaud Lambiotte"], "venue": "Proceedings of the 8th International Symposium on,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "Sophia N Yaliraki", "author": ["J-C Delvenne"], "venue": "and Mauricio Barahona. Stability of graph communities across time scales. Proceedings of the National Academy of Sciences, 107(29):12755\u201312760", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "An information flow model for conflict and fission in small groups", "author": ["Wayne W Zachary"], "venue": "Journal of anthropological research,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1977}, {"title": "A general probabilistic framework for detecting community structure in networks", "author": ["Cheng-Shang Chang", "Chin-Yi Hsu", "Jay Cheng", "Duan-Shin Lee"], "venue": "In IEEE INFOCOM", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2011}, {"title": "On random graphs", "author": ["P. Erd\u00f6s", "A. R\u00e9nyi"], "venue": "Publ. Math. Debrecen, 6:290\u2013297", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1959}, {"title": "Spectral clustering of graphs with the bethe hessian", "author": ["Alaa Saade", "Florent Krzakala", "Lenka Zdeborov\u00e1"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "Mode-net: Modules detection", "author": ["Lenka Zdeborova Aurelien Decelle", "Florent Krzakala", "Pan Zhang"], "venue": "in networks,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2012}, {"title": "Finding statistically significant communities in networks", "author": ["Andrea Lancichinetti", "Filippo Radicchi", "Jos\u00e9 J Ramasco", "Santo Fortunato"], "venue": "PloS one,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2011}, {"title": "Maps of information flow reveal community structure in complex networks", "author": ["M. Rosvall", "C.T. Bergstrom"], "venue": "Technical report, Citeseer", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2007}, {"title": "The map equation", "author": ["Martin Rosvall", "Daniel Axelsson", "Carl T Bergstrom"], "venue": "The European Physical Journal Special Topics,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2010}, {"title": "The numpy array: a structure for efficient numerical computation", "author": ["Stefan Van Der Walt", "S Chris Colbert", "Gael Varoquaux"], "venue": "Computing in Science & Engineering,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2011}, {"title": "A simple method for computing resistance", "author": ["Ravindra B Bapat", "Ivan Gutmana", "Wenjun Xiao"], "venue": "distance. Zeitschrift fu\u0308r Naturforschung A,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2003}, {"title": "The igraph software package for complex network research", "author": ["Gabor Csardi", "Tamas Nepusz"], "venue": "InterJournal, Complex Systems:1695,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2006}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research, 12:2825\u2013 2830", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2011}, {"title": "Msmbuilder2: Modeling conformational dynamics on the picosecond to millisecond scale", "author": ["Kyle A Beauchamp", "Gregory R Bowman", "Thomas J Lane", "Lutz Maibaum", "Imran S Haque", "Vijay S Pande"], "venue": "Journal of chemical theory and computation,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2011}, {"title": "A unified view of kernel k-means, spectral clustering and graph cuts", "author": ["Inderjit Dhillon", "Yuqiang Guan", "Brian Kulis"], "venue": null, "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2004}, {"title": "A generalized and adaptive method for community detection", "author": ["Romain Campigotto", "Patricia Conde C\u00e9spedes", "Jean-Loup Guillaume"], "venue": "arXiv preprint arXiv:1406.2518,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": ", the books [1], [2] and the historical review papers [3], [4]).", "startOffset": 17, "endOffset": 20}, {"referenceID": 1, "context": ", the books [1], [2] and the historical review papers [3], [4]).", "startOffset": 54, "endOffset": 57}, {"referenceID": 2, "context": ", the books [1], [2] and the historical review papers [3], [4]).", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "As stated in [4], clustering algorithms can be divided into two groups: hierarchical and partitional.", "startOffset": 13, "endOffset": 16}, {"referenceID": 3, "context": "It then repeatedly carries out the Lloyd iteration [5] that consists of the following two steps: (i) generate a new partition by assigning each data point to the closest cluster center, and (ii) compute the new cluster centers.", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": ", [6], [1], [7], [8]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": ", [6], [1], [7], [8]).", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": ", [6], [1], [7], [8]).", "startOffset": 17, "endOffset": 20}, {"referenceID": 7, "context": "A MATHEMATICAL THEORY FOR CLUSTERING IN METRIC SPACES 2 some recent works that provide various methods for selecting the initial partition that might lead to performance guarantees [9], [10], [11], [12], [13].", "startOffset": 181, "endOffset": 184}, {"referenceID": 8, "context": "A MATHEMATICAL THEORY FOR CLUSTERING IN METRIC SPACES 2 some recent works that provide various methods for selecting the initial partition that might lead to performance guarantees [9], [10], [11], [12], [13].", "startOffset": 186, "endOffset": 190}, {"referenceID": 9, "context": "A MATHEMATICAL THEORY FOR CLUSTERING IN METRIC SPACES 2 some recent works that provide various methods for selecting the initial partition that might lead to performance guarantees [9], [10], [11], [12], [13].", "startOffset": 192, "endOffset": 196}, {"referenceID": 10, "context": "A MATHEMATICAL THEORY FOR CLUSTERING IN METRIC SPACES 2 some recent works that provide various methods for selecting the initial partition that might lead to performance guarantees [9], [10], [11], [12], [13].", "startOffset": 198, "endOffset": 202}, {"referenceID": 11, "context": "A MATHEMATICAL THEORY FOR CLUSTERING IN METRIC SPACES 2 some recent works that provide various methods for selecting the initial partition that might lead to performance guarantees [9], [10], [11], [12], [13].", "startOffset": 204, "endOffset": 208}, {"referenceID": 12, "context": ", [14], [15], [16], [17], [18] and [19], [20] for reviews of the papers in this area).", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": ", [14], [15], [16], [17], [18] and [19], [20] for reviews of the papers in this area).", "startOffset": 8, "endOffset": 12}, {"referenceID": 14, "context": ", [14], [15], [16], [17], [18] and [19], [20] for reviews of the papers in this area).", "startOffset": 14, "endOffset": 18}, {"referenceID": 15, "context": ", [14], [15], [16], [17], [18] and [19], [20] for reviews of the papers in this area).", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": ", [14], [15], [16], [17], [18] and [19], [20] for reviews of the papers in this area).", "startOffset": 26, "endOffset": 30}, {"referenceID": 17, "context": ", [14], [15], [16], [17], [18] and [19], [20] for reviews of the papers in this area).", "startOffset": 35, "endOffset": 39}, {"referenceID": 18, "context": ", [14], [15], [16], [17], [18] and [19], [20] for reviews of the papers in this area).", "startOffset": 41, "endOffset": 45}, {"referenceID": 19, "context": "Solving the optimization problems formulated from the clustering problems are in general NP-hard and one has to resort to approximation algorithms [21].", "startOffset": 147, "endOffset": 151}, {"referenceID": 19, "context": "In [21], Balcan et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "As pointed out in [22], there are three commonly used approaches for developing a clustering theory: (i) an axiomatic approach that outlines a list of axioms for a clustering function (see e.", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": ", [23], [24], [25], [26], [27], [28]), (ii) an objective-based approach that provides a specific objective for a clustering function to optimize (see e.", "startOffset": 8, "endOffset": 12}, {"referenceID": 22, "context": ", [23], [24], [25], [26], [27], [28]), (ii) an objective-based approach that provides a specific objective for a clustering function to optimize (see e.", "startOffset": 14, "endOffset": 18}, {"referenceID": 23, "context": ", [23], [24], [25], [26], [27], [28]), (ii) an objective-based approach that provides a specific objective for a clustering function to optimize (see e.", "startOffset": 20, "endOffset": 24}, {"referenceID": 24, "context": ", [23], [24], [25], [26], [27], [28]), (ii) an objective-based approach that provides a specific objective for a clustering function to optimize (see e.", "startOffset": 26, "endOffset": 30}, {"referenceID": 25, "context": ", [23], [24], [25], [26], [27], [28]), (ii) an objective-based approach that provides a specific objective for a clustering function to optimize (see e.", "startOffset": 32, "endOffset": 36}, {"referenceID": 26, "context": ", [29], [21], and (iii) a definition-based approach that specifies the definition of clusters (see e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 19, "context": ", [29], [21], and (iii) a definition-based approach that specifies the definition of clusters (see e.", "startOffset": 8, "endOffset": 12}, {"referenceID": 27, "context": "g, [30], [31], [32]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "g, [30], [31], [32]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 29, "context": "g, [30], [31], [32]).", "startOffset": 15, "endOffset": 19}, {"referenceID": 23, "context": "In [26], Kleinberg adopted an axiomatic approach and showed an impossibility theorem for finding a clustering function that satisfies the following three axioms: (i) Scale invariance: if we scale the dissimilarity measure by a constant factor, then the clustering function still outputs the same partition of clusters.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "In [29], it was argued that the impossibility theorem is not an inherent feature of clustering.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "The key point in [29] is that", "startOffset": 17, "endOffset": 21}, {"referenceID": 26, "context": "The figure is redrawn from Figure 1 in [29] that originally consists of 6 clusters.", "startOffset": 39, "endOffset": 43}, {"referenceID": 27, "context": "One exception is [30], where Ester et al.", "startOffset": 17, "endOffset": 21}, {"referenceID": 27, "context": "As pointed out in [30], it is not an easy task to determine these two parameters.", "startOffset": 18, "endOffset": 22}, {"referenceID": 2, "context": "What is a cluster? As pointed out in [4], one of the fundamental challenges associated with clustering is to address the following question: What is a cluster in a set of data points? In this paper, we will develop a clustering theory that formally define a cluster for data points in a metric space.", "startOffset": 37, "endOffset": 40}, {"referenceID": 30, "context": "Such a metric assumption is stronger than the usual dissimilarity (similarity) measures [33], where the triangular inequality in general does not hold.", "startOffset": 88, "endOffset": 92}, {"referenceID": 31, "context": "The notions of relative distance and cohesion measure are also related to the notion of relative centrality in our previous work [34].", "startOffset": 129, "endOffset": 133}, {"referenceID": 32, "context": "However, it was shown in [35] that finding the optimal assignment for modularity maximization is NP-complete in the strong sense and thus heuristic algorithms, such as hierarchical algorithms and partitional algorithms are commonly used in the literature for solving the modularity maximization problem.", "startOffset": 25, "endOffset": 29}, {"referenceID": 33, "context": "This is different from the greedy selection in [1], Chapter 13, and [36].", "startOffset": 68, "endOffset": 72}, {"referenceID": 33, "context": "As in [36], in each iteration we can merge the two clusters that result in the largest increase of the modularity index, i.", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": ", the book [2]) that a n\u00e4ive implementation of a greedy hierarchical agglomerative clustering algorithm has O(n) computational complexity and the computational complexity can be further reduced to O(n log(n)) if priority queues are implemented for the greedy selection.", "startOffset": 11, "endOffset": 14}, {"referenceID": 34, "context": ", [37], [38], [39], [40]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 35, "context": ", [37], [38], [39], [40]).", "startOffset": 8, "endOffset": 12}, {"referenceID": 36, "context": ", [37], [38], [39], [40]).", "startOffset": 14, "endOffset": 18}, {"referenceID": 37, "context": ", [37], [38], [39], [40]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 35, "context": "Among them, the fast unfolding algorithm in [38] is the fast one as there is a second phase of building a new (and much smaller) network whose nodes are the communities found during the previous phase.", "startOffset": 44, "endOffset": 48}, {"referenceID": 34, "context": "The Newman and Girvan modularity in [37] is based on a probability measure from a random selection of an edge in a network (see [34] for more detailed discussions) and this is different from the distance metric used in this paper.", "startOffset": 36, "endOffset": 40}, {"referenceID": 31, "context": "The Newman and Girvan modularity in [37] is based on a probability measure from a random selection of an edge in a network (see [34] for more detailed discussions) and this is different from the distance metric used in this paper.", "startOffset": 128, "endOffset": 132}, {"referenceID": 34, "context": "Example 10 (Zachary\u2019s karate club) As in [37], [42], we consider the Zachary\u2019s karate club friendship network [41] in Figure 3.", "startOffset": 41, "endOffset": 45}, {"referenceID": 39, "context": "Example 10 (Zachary\u2019s karate club) As in [37], [42], we consider the Zachary\u2019s karate club friendship network [41] in Figure 3.", "startOffset": 47, "endOffset": 51}, {"referenceID": 38, "context": "Example 10 (Zachary\u2019s karate club) As in [37], [42], we consider the Zachary\u2019s karate club friendship network [41] in Figure 3.", "startOffset": 110, "endOffset": 114}, {"referenceID": 38, "context": "The set of data was observed by Wayne Zachary [41] over the course of two years in the early 1970s at an American university.", "startOffset": 46, "endOffset": 50}, {"referenceID": 38, "context": "According to [41], there was an interesting story for person number 9.", "startOffset": 13, "endOffset": 17}, {"referenceID": 38, "context": "The network of friendships between individuals in the karate club study of Zachary[41].", "startOffset": 82, "endOffset": 86}, {"referenceID": 19, "context": "This is different from the K-median objective, the K-means objective and the min-sum objective addressed in [21].", "startOffset": 108, "endOffset": 112}, {"referenceID": 4, "context": "This might give the K-sets algorithm the computational advantage over the K-medoids algorithms [6], [1], [7], [8].", "startOffset": 95, "endOffset": 98}, {"referenceID": 5, "context": "This might give the K-sets algorithm the computational advantage over the K-medoids algorithms [6], [1], [7], [8].", "startOffset": 105, "endOffset": 108}, {"referenceID": 6, "context": "This might give the K-sets algorithm the computational advantage over the K-medoids algorithms [6], [1], [7], [8].", "startOffset": 110, "endOffset": 113}, {"referenceID": 40, "context": "2) Stochastic block model: The stochastic block model (SBM), as a generalization of the Erd\u00f6s-R\u00e9nyi random graph [43], is a commonly used method for generating random graphs that can be used for benchmarking community detection algorithms [44], [45].", "startOffset": 113, "endOffset": 117}, {"referenceID": 41, "context": "2) Stochastic block model: The stochastic block model (SBM), as a generalization of the Erd\u00f6s-R\u00e9nyi random graph [43], is a commonly used method for generating random graphs that can be used for benchmarking community detection algorithms [44], [45].", "startOffset": 239, "endOffset": 243}, {"referenceID": 42, "context": "2) Stochastic block model: The stochastic block model (SBM), as a generalization of the Erd\u00f6s-R\u00e9nyi random graph [43], is a commonly used method for generating random graphs that can be used for benchmarking community detection algorithms [44], [45].", "startOffset": 245, "endOffset": 249}, {"referenceID": 41, "context": "Then it is known [44] that these q communities can be detected (in theory for a large network) if |cin \u2212 cout| > q \u221a", "startOffset": 17, "endOffset": 21}, {"referenceID": 42, "context": "(19) In this paper, we use MODE-NET [45] to run SBM.", "startOffset": 36, "endOffset": 40}, {"referenceID": 43, "context": "We compare our K-sets algorithm with some other community detection algorithms, such as OSLOM2 [46], infomap [47], [48], and fast unfolding [38].", "startOffset": 95, "endOffset": 99}, {"referenceID": 44, "context": "We compare our K-sets algorithm with some other community detection algorithms, such as OSLOM2 [46], infomap [47], [48], and fast unfolding [38].", "startOffset": 109, "endOffset": 113}, {"referenceID": 45, "context": "We compare our K-sets algorithm with some other community detection algorithms, such as OSLOM2 [46], infomap [47], [48], and fast unfolding [38].", "startOffset": 115, "endOffset": 119}, {"referenceID": 35, "context": "We compare our K-sets algorithm with some other community detection algorithms, such as OSLOM2 [46], infomap [47], [48], and fast unfolding [38].", "startOffset": 140, "endOffset": 144}, {"referenceID": 46, "context": "The metric used for the K-sets algorithm for each sample of the random graph is the resistance distance, and this is pre-computed by NumPy [49].", "startOffset": 139, "endOffset": 143}, {"referenceID": 47, "context": "The resistance distance matrix (denoted by R = (Ri,j)) can be derived from the pseudo inverse of the adjacency matrix (denoted by \u0393 = (\u0393i,j)) as follows: [50]: Ri,j = {", "startOffset": 154, "endOffset": 158}, {"referenceID": 48, "context": "The K-sets algorithm and OSLOM2 are implemented in C++, and the others are all taken from igraph [51] and are implemented in C with python wrappers.", "startOffset": 97, "endOffset": 101}, {"referenceID": 48, "context": "In Figure 7, we compute the normalized mutual information measure (NMI) by using a built-in function in igraph [51] for the results obtained from these four algorithms.", "startOffset": 111, "endOffset": 115}, {"referenceID": 44, "context": "Its performance in that range is better than infomap [47], [48], fast unfolding [38] and OSLOM2 [46].", "startOffset": 53, "endOffset": 57}, {"referenceID": 45, "context": "Its performance in that range is better than infomap [47], [48], fast unfolding [38] and OSLOM2 [46].", "startOffset": 59, "endOffset": 63}, {"referenceID": 35, "context": "Its performance in that range is better than infomap [47], [48], fast unfolding [38] and OSLOM2 [46].", "startOffset": 80, "endOffset": 84}, {"referenceID": 43, "context": "Its performance in that range is better than infomap [47], [48], fast unfolding [38] and OSLOM2 [46].", "startOffset": 96, "endOffset": 100}, {"referenceID": 49, "context": ", the K-means++ algorithm [53] and the K-medoids algorithm [54].", "startOffset": 26, "endOffset": 30}, {"referenceID": 50, "context": ", the K-means++ algorithm [53] and the K-medoids algorithm [54].", "startOffset": 59, "endOffset": 63}, {"referenceID": 46, "context": "The Euclidean distance between two data points (samples from the MNIST dataset) for the K-medoids algorithm and the K-sets algorithm are pre-computed by NumPy [49].", "startOffset": 159, "endOffset": 163}, {"referenceID": 44, "context": "Comparison of infomap [47], [48], fast unfolding [38], OSLOM2 [46] and K-sets for the stochastic block model with two blocks.", "startOffset": 22, "endOffset": 26}, {"referenceID": 45, "context": "Comparison of infomap [47], [48], fast unfolding [38], OSLOM2 [46] and K-sets for the stochastic block model with two blocks.", "startOffset": 28, "endOffset": 32}, {"referenceID": 35, "context": "Comparison of infomap [47], [48], fast unfolding [38], OSLOM2 [46] and K-sets for the stochastic block model with two blocks.", "startOffset": 49, "endOffset": 53}, {"referenceID": 43, "context": "Comparison of infomap [47], [48], fast unfolding [38], OSLOM2 [46] and K-sets for the stochastic block model with two blocks.", "startOffset": 62, "endOffset": 66}, {"referenceID": 49, "context": "Comparison of K-means++ [53], K-medoids [54] and K-sets for the MNIST dataset.", "startOffset": 24, "endOffset": 28}, {"referenceID": 50, "context": "Comparison of K-means++ [53], K-medoids [54] and K-sets for the MNIST dataset.", "startOffset": 40, "endOffset": 44}, {"referenceID": 48, "context": "In Figure 8, we compute the normalized mutual information measure (NMI) by using a built-in function in igraph [51] for the results obtained from these three algorithms.", "startOffset": 111, "endOffset": 115}, {"referenceID": 51, "context": ", [55]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 51, "context": "As indicated in [55], a large \u03c3 decreases (resp.", "startOffset": 16, "endOffset": 20}, {"referenceID": 52, "context": "(40) We note that such a cohesion measure is known as the deviation to indetermination null model in [56].", "startOffset": 101, "endOffset": 105}], "year": 2015, "abstractText": "Clustering is one of the most fundamental problems in data analysis and it has been studied extensively in the literature. Though many clustering algorithms have been proposed, clustering theories that justify the use of these clustering algorithms are still unsatisfactory. In particular, one of the fundamental challenges is to address the following question: What is a cluster in a set of data points? In this paper, we make an attempt to address such a question by considering a set of data points associated with a distance measure (metric). We first propose a new cohesion measure in terms of the distance measure. Using the cohesion measure, we define a cluster as a set of points that are cohesive to themselves. For such a definition, we show there are various equivalent statements that have intuitive explanations. We then consider the second question: How do we find clusters and good partitions of clusters under such a definition? For such a question, we propose a hierarchical agglomerative algorithm and a partitional algorithm. Unlike standard hierarchical agglomerative algorithms, our hierarchical agglomerative algorithm has a specific stopping criterion and it stops with a partition of clusters. Our partitional algorithm, called the K-sets algorithm in the paper, appears to be a new iterative algorithm. Unlike the Lloyd iteration that needs two-step minimization, our K-sets algorithm only takes one-step minimization. One of the most interesting findings of our paper is the duality result between a distance measure and a cohesion measure. Such a duality result leads to a dual K-sets algorithm for clustering a set of data points with a cohesion measure. The dual K-sets algorithm converges in the same way as a sequential version of the classical kernel K-means algorithm. The key difference is that a cohesion measure does not need to be positive semi-definite. Index Terms Clustering, hierarchical algorithms, partitional algorithms, convergence, K-sets, duality", "creator": "LaTeX with hyperref package"}}}