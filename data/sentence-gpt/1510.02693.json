{"id": "1510.02693", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2015", "title": "Feedforward Sequential Memory Neural Networks without Recurrent Feedback", "abstract": "We introduce a new structure for memory neural networks, called feedforward sequential memory networks (FSMN), which can learn long-term dependency without using recurrent feedback. The proposed FSMN is a standard feedforward neural networks equipped with learnable sequential memory blocks in the hidden layers. In this work, we have applied FSMN to several language modeling (LM) tasks. Experimental results have shown that the memory blocks in FSMN can learn effective representations of long history. Experiments have shown that FSMN based language models can significantly outperform not only feedforward neural network (FNN) based LMs but also the popular recurrent neural network (RNN) LMs. By performing a few tasks during training, the learning of long history can yield useful results. This study is a first for the FSMN. This is the first in the FSMN. In our previous work we presented the first FSMN to a wider range of FSMN users. We hope that FSMN can be applied to many applications in the context of more recent memory management. For example, in the context of an individual network, a given training task can learn from the current training training session that there is a significant change in the memory block, and a specific time and weight on which it can learn from the current training session.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Fri, 9 Oct 2015 15:04:11 GMT  (184kb,D)", "http://arxiv.org/abs/1510.02693v1", "4 pages, 1figure"]], "COMMENTS": "4 pages, 1figure", "reviews": [], "SUBJECTS": "cs.NE cs.CL cs.LG", "authors": ["shiliang zhang", "hui jiang", "si wei", "lirong dai"], "accepted": false, "id": "1510.02693"}, "pdf": {"name": "1510.02693.pdf", "metadata": {"source": "CRF", "title": "Feedforward Sequential Memory Neural Networks without Recurrent Feedback", "authors": ["Shiliang Zhang", "Hui Jiang", "Si Wei", "Lirong Dai"], "emails": ["zsl2008@mail.ustc.edu.cn,", "hj@cse.yorku.ca,", "siwei@iflytek.com,", "lrdai@ustc.edu.cn"], "sections": [{"heading": null, "text": "We introduce a new structure for memory neural networks, called feedforward sequential memory networks (FSMN), which can learn long-term dependency without using recurrent feedback. The proposed FSMN is a standard feedforward neural networks equipped with learnable sequential memory blocks in the hidden layers. In this work, we have applied FSMN to several language modeling (LM) tasks. Experimental results have shown that the memory blocks in FSMN can learn effective representations of long history. Experiments have shown that FSMN based language models can significantly outperform not only feedforward neural network (FNN) based LMs but also the popular recurrent neural network (RNN) LMs."}, {"heading": "1 Introduction", "text": "When machine learning methods are applied to model sequential data such as text, speech and video, it is very important to take advantage of the long-term dependency. Traditional approaches have explored to capture the long-term structure within the sequential data using recurrent feedbacks such as in regular recurrent neural networks (RNNs) or LSTM-based models. [1, 2, 3, 4]. RNNs can learn and carry out complicated transformations of data over extended periods of time and store the memory in the weights of the network. Therefore, they are gaining more and more popular in sequential data modeling tasks. More recently, different from RNNs, there has also been a surge in constructing neural computing models with varying forms of explicit memory units [5, 6, 7, 8] . For example, in [6], the proposed memory networks employ a memory component that can be read from and written to. In [5], the proposed neural turing machines (NTM) improve the memory of neural networks by coupling with external memory resources, which can learn to sort a small set of numbers as well as other symbolic manipulation tasks.\nIn this work, we have proposed a simpler structure for memory neural networks, namely feedforward sequential memory networks (FSMN), which can learn long-term dependency in sequential data without using the recurrent feedback. For FSMN, we extend the standard feedforward neural networks by introducing memory blocks in the hidden layers. Different from RNNs, the overall FSMNs remain as a feed-forward structure so that they can be learned in much more efficient and stable ways than RNNs and LSTMs. In our work, we have evaluated the performance of FSMN on two language modeling tasks: Penn Tree Bank (PTB) and English wiki in Large Text Compression Benchmark (LTCB). In both tasks, FSMN based language models can significantly outperform not only the standard FNN-LMs but also the popular recurrent neural network (RNN) LMs by a significant margin.\nar X\niv :1\n51 0.\n02 69\n3v 1\n[ cs\n.N E\n] 9"}, {"heading": "2 Our Approach", "text": ""}, {"heading": "2.1 Feedforward Sequential Memory Neural Networks", "text": "Feedforward sequential memory network (FSMN) is a standard feedforward neural network with single or multiple memory blocks in the hidden layer. For instance, Figure 1 (a) shows an FSMN with one memory block added into its second hidden layer. Given a sequence, X = {x1,x2, \u00b7 \u00b7 \u00b7 ,xT}, each xt \u2208 RD\u00d71 represents an input data at time instance t. The corresponding hidden layer outputs are denoted as H = {h1,h2, \u00b7 \u00b7 \u00b7 ,hT}, each ht \u2208 RD`\u00d71. As shown in Figure 1 (b), we can use a tapped-delay structure to encode ht and its previous N histories into a fixed-sized representation in the memory block:\nh\u0303t = f( N\u2211 i=0 ai \u00b7 ht\u2212i) (1)\nwhere all coefficients form an N-dimension learnable vector a = {a0, a1, a2, \u00b7 \u00b7 \u00b7 , aN}, and f(\u00b7) is the activation function (sigmoid or RELU). Furthermore, as shown in Figure 1 (a), h\u0303t may be fed into next hidden layer in the same way as ht.\nFrom the viewpoint of signal processing, the memory block in FSMN can be regarded as a highorder finite impulse response (FIR) filter while the recurrent layer in RNNs, namely h\u0303t = f(ht + W \u00b7 h\u0303t\u22121), may be viewed as a first-order infinite impulse response (IIR) filter, see Figure 1 (c). Obviously, the vector a may be regarded as the coefficients of an N-order FIR filter. We know IIR filters are more compact than FIR filters. However, IIR filters may be difficult to implement. In some cases, IIR filters may become unstable but FIR filters are always stable. Moreover, the learning of IIR-filter-like RNNs requires to use the so-called back-propagation through time (BPTT) which significantly increases the computational complexity of the learning and also causes the problems of gradient vanishing and exploding [10]. On the other hand, the proposed FIR-filter-like FSMNs can be efficiently learned using the standard back-propagation procedure. Therefore, the learning of FSMNs is more stable and efficient than that of RNNs."}, {"heading": "2.2 Implement FSMN for language models", "text": "The goal in language modeling is to predict the next word in a text sequence given all previous words. We now explain how to implement FSMNs for this task. FSMN is a standard feedforward neural network (FNN) except the additional memory blocks. We will show that the memory block can be efficiently implemented as sentence-by-sentence matrix multiplications, which are suitable for the mini-batch based stochastic gradient descent (SGD) method running on GPUs.\nSuppose the N-order FIR filter coefficients in the memory block are denoted as a = {a0, a1, a2, \u00b7 \u00b7 \u00b7 , aN}. For a given sentence X consisting of T words, we may construct a T \u00d7 T\nsquare matrix M as follow:\nM =  a0 a1 \u00b7 \u00b7 \u00b7 aN 0 \u00b7 \u00b7 \u00b7 0 \u00b7 \u00b7 \u00b7 0 a0 a1 \u00b7 \u00b7 \u00b7 aN 0 \u00b7 \u00b7 \u00b7 ... \u00b7 \u00b7 \u00b7 0 . . . . . . ... 0 \u00b7 \u00b7 \u00b7 a0 \u00b7 \u00b7 \u00b7 aN ... \u00b7 \u00b7 \u00b7 . . .\n... 0 \u00b7 \u00b7 \u00b7 0 a0  Therefore, the sequential memory operations in eq.(1) for the whole sequence can be computed with one matrix multiplication as: H\u0303 = f(H \u00b7M). Similarly, we may extend the idea to a mini-batch composed of K sentences,L = {X1 X2 \u00b7 \u00b7 \u00b7XK}, we can compute the sequential memory representation for all sentences in the mini-batch as follows:\nH\u0303 = f([H1,H2 \u00b7 \u00b7 \u00b7HK] \u00b7  M1 M2\n. . . MK ) = f(H\u0304 \u00b7 M\u0304) (2) In the backward pass, except the weights in the network, we also need to calculate the gradients of M\u0304, which is then used to update the filter coefficients a. We can calculate the gradients using the standard back-propagation (BP) algorithm. Therefore, all computation in FSMNs can be formulated as large matrix multiplications, which can be efficiently conducted in GPUs. As a result, FSMN based LMs have the same computational complexity as the standard NN LMs in training, which is much more efficient than RNN-LMs."}, {"heading": "3 Experiments", "text": "We have evaluated FSMNs on two benchmark LM tasks: i) the Penn Treebank (PTB) corpus of about 1M words, following the same setup as [11]. ii) The Large Text Compression Benchmark (LTCB) [12]. In LTCB, we use the enwik9 dataset, which is composed of the first 109 bytes of enwiki20060303-pages-articles.xml. We split it into three parts: training (153M), validation (8.9M) and test (8.9M) sets. We limit the vocabulary size to 80k for LTCB and replace all out-of-vocabulary words by <UNK>.\nFor FSMNs, the hidden units employ the rectified linear activation function, i.e., f(x) = max(0, x). The nets are initialized based on the normalized initialization in [13], without using any pre-training. We use SGD with a mini-batch size of 200 and 500 for PTB and LTCB tasks respectively. The initial learning rate is 0.4 and 0.002 for the weights and filter coefficients respectively, which is kept fixed as long as the perplexity on the validation set decreases by at least 1. After that, we continue six more epochs of training, where the learning rate is halved after each epoch. In PTB task, we also use momentum (0.9) and weight decay (0.00004) to avoid overfitting."}, {"heading": "3.1 Results", "text": "We have first evaluated the performance of FSMN-LMs on the PTB task. We have trained FSMN with an input context window of two, where the previous two words are used to predict the next word. The FSMN contains a linear projection layer (of 200 nodes), two hidden layers (of 400 nodes pre layer) and a memory block in the first hidden layer. For the memory block, we use a 20-order FIR filter. In Table 1, we have summarized the perplexities on the PTB test set for various language models.\nFor the LTCB task, we have trained several baseline systems: i) two n-gram LMs (3-gram and 5-gram) using the modified Kneser-Ney smoothing without count cutoffs; ii) several traditional feedforward NNLMs with different model sizes and input context windows (bigram, trigram, 4- gram and 5-gram); iii) an RNN-LM with one hidden layer of 600 nodes using the toolkit in [15]; iv)\n2nd-order FOFE based FNNLM [14] with different hidden layer sizes. Moreover, we have examined our FSMN based LMs with different architectures. We have trained a 3-hidden-layer FSMN with a memory block in the first hidden layer, second hidden layer or both. The order of the FIR filter is 30 in these experiments. In Table 2, we have summarized the perplexities on the LTCB test set for various models.\nExperimental results in Table 1 and Table 2 have shown that the FSMN based LMs can significantly outperform the baseline higher-order feedforward neural network (FNN) LMs, FOFE-based FNN LMs as well as the popular RNN-based LMs by a quite significant margin."}, {"heading": "4 Conclusions and Future Work", "text": "In this work, we have proposed a novel neural network architecture, namely feedforward sequential memory networks (FSMN), which use FIR-filter-like memory blocks in the hidden layer of standard feedforward neural networks. Experimental results on language modeling tasks have shown that the FSMN can effectively learn the long term history. For the future work, we will try to apply this model to other sequential data modeling tasks, such as acoustic modeling in speech recognition."}], "references": [{"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv:1308.0850", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["J. Chung", "C. Gulcehre", "K.H. Cho"], "venue": "Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv:1412.3555", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["T. Mikolov", "A. Joulin", "S. Chopra"], "venue": "Learning longer memory in recurrent neural networks. arXiv:1412.7753", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural Turing Machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv:1410.5401", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "arXiv:1410.3916", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "End-To-End Memory Networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston"], "venue": "arXiv:1503.08895", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets", "author": ["A. Joulin", "T. Mikolov"], "venue": "arXiv:1503.01007", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Large-scale Simple Question Answering with Memory Networks", "author": ["A. Bordes", "N. Usunier", "S. Chopra", "J. Weston"], "venue": "arXiv:1506.02075", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks. volume 5, no 2, pages 157-166", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1994}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J. Cernocky", "S. Khudanpur"], "venue": "Proc. of International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5528-5531", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Large Text Compression Benchmark", "author": ["M. Mahoney"], "venue": "http://mattmahoney.net/dc/textdata.html", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["G. Xavier", "Y. Bengio"], "venue": "Proc. of AISTATS", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "The Fixed-Size Ordinally-Forgetting Encoding Method for Neural Network Language Models", "author": ["S. Zhang", "H. Jiang", "M. Xu", "J. Hou", "L.R. Dai"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL), pp.495-500", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "Proc. of Interspeech, pages 1045-1048", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "[1, 2, 3, 4].", "startOffset": 0, "endOffset": 12}, {"referenceID": 1, "context": "[1, 2, 3, 4].", "startOffset": 0, "endOffset": 12}, {"referenceID": 2, "context": "[1, 2, 3, 4].", "startOffset": 0, "endOffset": 12}, {"referenceID": 3, "context": "[1, 2, 3, 4].", "startOffset": 0, "endOffset": 12}, {"referenceID": 4, "context": "More recently, different from RNNs, there has also been a surge in constructing neural computing models with varying forms of explicit memory units [5, 6, 7, 8] .", "startOffset": 148, "endOffset": 160}, {"referenceID": 5, "context": "More recently, different from RNNs, there has also been a surge in constructing neural computing models with varying forms of explicit memory units [5, 6, 7, 8] .", "startOffset": 148, "endOffset": 160}, {"referenceID": 6, "context": "More recently, different from RNNs, there has also been a surge in constructing neural computing models with varying forms of explicit memory units [5, 6, 7, 8] .", "startOffset": 148, "endOffset": 160}, {"referenceID": 7, "context": "More recently, different from RNNs, there has also been a surge in constructing neural computing models with varying forms of explicit memory units [5, 6, 7, 8] .", "startOffset": 148, "endOffset": 160}, {"referenceID": 5, "context": "For example, in [6], the proposed memory networks employ a memory component that can be read from and written to.", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "In [5], the proposed neural turing machines (NTM) improve the memory of neural networks by coupling with external memory resources, which can learn to sort a small set of numbers as well as other symbolic manipulation tasks.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "Moreover, the learning of IIR-filter-like RNNs requires to use the so-called back-propagation through time (BPTT) which significantly increases the computational complexity of the learning and also causes the problems of gradient vanishing and exploding [10].", "startOffset": 254, "endOffset": 258}, {"referenceID": 10, "context": "We have evaluated FSMNs on two benchmark LM tasks: i) the Penn Treebank (PTB) corpus of about 1M words, following the same setup as [11].", "startOffset": 132, "endOffset": 136}, {"referenceID": 11, "context": "ii) The Large Text Compression Benchmark (LTCB) [12].", "startOffset": 48, "endOffset": 52}, {"referenceID": 12, "context": "The nets are initialized based on the normalized initialization in [13], without using any pre-training.", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "For the LTCB task, we have trained several baseline systems: i) two n-gram LMs (3-gram and 5-gram) using the modified Kneser-Ney smoothing without count cutoffs; ii) several traditional feedforward NNLMs with different model sizes and input context windows (bigram, trigram, 4gram and 5-gram); iii) an RNN-LM with one hidden layer of 600 nodes using the toolkit in [15]; iv)", "startOffset": 365, "endOffset": 369}, {"referenceID": 10, "context": "Model Test PPL KN 5-gram [11] 141 RNNLM [11] 123 LSTM [2] 117 MemN2N[7] 111 trigram FNNLM[14] 131 6-gram FNNLM[14] 113 FOFE-FNNLM[14] 108 FSMN-LM 102 Table 2: Perplexities on LTCB for various language models.", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "Model Test PPL KN 5-gram [11] 141 RNNLM [11] 123 LSTM [2] 117 MemN2N[7] 111 trigram FNNLM[14] 131 6-gram FNNLM[14] 113 FOFE-FNNLM[14] 108 FSMN-LM 102 Table 2: Perplexities on LTCB for various language models.", "startOffset": 40, "endOffset": 44}, {"referenceID": 1, "context": "Model Test PPL KN 5-gram [11] 141 RNNLM [11] 123 LSTM [2] 117 MemN2N[7] 111 trigram FNNLM[14] 131 6-gram FNNLM[14] 113 FOFE-FNNLM[14] 108 FSMN-LM 102 Table 2: Perplexities on LTCB for various language models.", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "Model Test PPL KN 5-gram [11] 141 RNNLM [11] 123 LSTM [2] 117 MemN2N[7] 111 trigram FNNLM[14] 131 6-gram FNNLM[14] 113 FOFE-FNNLM[14] 108 FSMN-LM 102 Table 2: Perplexities on LTCB for various language models.", "startOffset": 68, "endOffset": 71}, {"referenceID": 13, "context": "Model Test PPL KN 5-gram [11] 141 RNNLM [11] 123 LSTM [2] 117 MemN2N[7] 111 trigram FNNLM[14] 131 6-gram FNNLM[14] 113 FOFE-FNNLM[14] 108 FSMN-LM 102 Table 2: Perplexities on LTCB for various language models.", "startOffset": 89, "endOffset": 93}, {"referenceID": 13, "context": "Model Test PPL KN 5-gram [11] 141 RNNLM [11] 123 LSTM [2] 117 MemN2N[7] 111 trigram FNNLM[14] 131 6-gram FNNLM[14] 113 FOFE-FNNLM[14] 108 FSMN-LM 102 Table 2: Perplexities on LTCB for various language models.", "startOffset": 110, "endOffset": 114}, {"referenceID": 13, "context": "Model Test PPL KN 5-gram [11] 141 RNNLM [11] 123 LSTM [2] 117 MemN2N[7] 111 trigram FNNLM[14] 131 6-gram FNNLM[14] 113 FOFE-FNNLM[14] 108 FSMN-LM 102 Table 2: Perplexities on LTCB for various language models.", "startOffset": 129, "endOffset": 133}, {"referenceID": 13, "context": "2nd-order FOFE based FNNLM [14] with different hidden layer sizes.", "startOffset": 27, "endOffset": 31}], "year": 2015, "abstractText": "We introduce a new structure for memory neural networks, called feedforward sequential memory networks (FSMN), which can learn long-term dependency without using recurrent feedback. The proposed FSMN is a standard feedforward neural networks equipped with learnable sequential memory blocks in the hidden layers. In this work, we have applied FSMN to several language modeling (LM) tasks. Experimental results have shown that the memory blocks in FSMN can learn effective representations of long history. Experiments have shown that FSMN based language models can significantly outperform not only feedforward neural network (FNN) based LMs but also the popular recurrent neural network (RNN) LMs.", "creator": "LaTeX with hyperref package"}}}