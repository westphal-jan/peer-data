{"id": "1510.02892", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2015", "title": "Survey on Feature Selection", "abstract": "Feature selection plays an important role in the data mining process. It is needed to deal with the excessive number of features, which can become a computational burden on the learning algorithms. It is also necessary, even when computational resources are not scarce, since it improves the accuracy of the machine learning tasks, as we will see in the upcoming sections. In this review, we discuss the different feature selection approaches, and the relation between them and the various machine learning algorithms. We explain why the model selection of a dataset from a single dataset was particularly important when it comes to the ability of the computational tasks in this review. We also explain why the model selection can be useful in modeling the problem of an individual dataset and why that can change in the future.\n\n\n\n\nThe problem of using the model selection of a dataset from a single dataset for each dataset is not only difficult, but also is also necessary in order to have a more realistic model selection algorithm for each dataset. In order to achieve this, we will present a number of algorithms that are able to use the model selection algorithms on an average basis.\n1. Machine learning\nMachine learning is a multi-level system of machine learning that has been able to simulate multiple different types of data. It is used in many systems such as high-speed computer simulation and machine learning. Machine learning is developed in several different domains as part of the LITV (LITV) initiative, but it is also useful for improving model prediction. Machine learning algorithms can also be used to create an optimal model selection algorithm that combines several datasets and generates a model-specific model selection algorithm. Machine learning algorithms are highly complex and need a simple analysis of its features. This is especially important when considering a very complex and highly complex model selection algorithm. To overcome this challenge, we have devised a new model selection algorithm which allows us to use machine learning algorithms as a model-specific model-specific model-specific model-specific model.\n2. Model selection\nThe machine learning algorithm is an approach for machine learning and model selection for all types of data. Machine learning algorithms can be used to provide a realistic simulation of data using several different methods. The model selection methods for the model selection algorithms are very well-known among machine learning algorithms, especially the LITV (LITV) project, but they are usually very complicated, so we would not recommend them to people who have a hard time working on the machine learning algorithm. The LITV (LITV) project is based on a combination", "histories": [["v1", "Sat, 10 Oct 2015 08:54:48 GMT  (10kb)", "http://arxiv.org/abs/1510.02892v1", "Report, for Data Mining class during KDD Masters Degree at University of East Anglia"]], "COMMENTS": "Report, for Data Mining class during KDD Masters Degree at University of East Anglia", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tarek amr abdallah", "beatriz de la iglesia"], "accepted": false, "id": "1510.02892"}, "pdf": {"name": "1510.02892.pdf", "metadata": {"source": "CRF", "title": "Survey on Feature Selection", "authors": ["Tarek Amr", "Beatriz de La Iglesia"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n51 0.\n02 89\n2v 1\n[ cs\n.L G\n] 1\n0 O\nct 2\n01 5\nFeature selection plays an important role in the data mining process. It is needed to deal with the excessive number of features, which can become a computational burden on the learning algorithms. It is also necessary, even when computational resources are not scarce, since it improves the accuracy of the machine learning tasks, as we will see in the upcoming sections. In this review, we discuss the different feature selection approaches, and the relation between them and the various machine learning algorithms."}, {"heading": "1 Introduction", "text": "According to Dunham (2002), machine learning tasks can be seen as predictive or descriptive ones. Classification is an example of predictive models. Friedman (1997) described it as a model where discrete output values (class labels) are learnt from the different variables (features) of the input data. Clustering, on the other hand, is categorised by Dunham (2002) as a descriptive task. The features of the input data are used to categorize it without supervised training. In both cases, the choice of the feature-set plays an important role in the performance of the data mining problem. Liu et al. (2010) listed three advantages for removing irrelevant and redundant features: it makes the data mining task more efficient, improves its accuracy and simplifies the inferred model, making it more comprehensible.\nFor an accurate classifier, it is needed to reduce both bias and variance of the model (Friedman, 1997). As described by Domingos (1999), the bias is a systematic error that occurs when inferring a more generalized model for the data, hence increasing the training data will not improve it. Variance, on the other hand, results when the model tries to cope with the variations of the noisy data sample. Increasing the sample size in this case can balance the effect of the noise and reduce the variance accordingly. Nevertheless, Friedman (1997) stresses that during the training process, the more sensitive the model is to the training data, the lower the bias in exchange for a higher variance, and vice versa. This is known as \u201cbias-variance trade-off\u201d. Hence, as noted by Kohavi and John (1997), classifiers faced with limited data has to find an optimum point where they can actually estimate the statistical distribution of fewer features (variance reduction) versus less accurate estimation of more features (bias reduction); ergo, Munson and Caruana (2009) summarized the feature selection process as the process of finding the best bias-variance trade-off point.\nWhen it comes to unsupervised learning algorithms, such as clustering, Janecek et al. (2008) highlighted that the problem with high dimensional data (more features) is that it makes the proximity measures between the records more uniform, hence metrics such as distance and density become harder to obtain.\nIn the next sections we explain the different feature selection approaches."}, {"heading": "2 The selection process", "text": "In its simplest form, the feature selection process can evaluate individual features and rank them based on their correlation with class labels (Yu and Liu, 2004). However, Hall (1999) reported that studies had proven a good feature subset to be the one whose features are not correlated to each other, besides them being correlated to class labels. Hence, they are better evaluated as subset rather than individually. Liu et al. (2010) summarized subset feature selection process into three main steps:\n\u2022 Search: Generating a subset of the available features to be evaluated.\n\u2022 Evaluation: Evaluating the utility of the generated subset\n\u2022 Stop: Deciding whether to stop or continue the search till a stopping criterion is reached\nFor N-dimension feature-space, there are 2N possible subsets. Thus, the generation step uses different approaches to traverse the available subsets. Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003).\nJohn et al. (1994) listed two search algorithms. The forward selection algorithms starts with an empty set and keeps adding features, while the backward elimination starts with all the features and keeps on removing ones. Pintelas (2004) explained that in the two algorithms, once a feature is added it cannot be removed and vice versa. Thus, they are described by Yang and Honavar (1998); Hall (1999) as greedy hill-climbing algorithms, where they assume monotonicity of the whole process. Kohavi and John (1997) added that dealing with smaller subset in the beginning makes the forward selection faster than the backward elimination algorithm and can reach relatively fewer features more quickly, yet the latter usually selects more interactive features. This makes Forward Selection preferable with high dimensional data. Generally, hill-climbing algorithms might get caught in local minima and fail to include useful features, or exclude irrelevant ones.\nHall (1999) mentioned another algorithm; best first search. Unlike the hill climbing algorithms, at each step it generates all possible moves and allow for backtracking once the path it is traversing is not adding any improvement. Kohavi and John (1997) highlighted the importance of having a stoppage criterion, to limit the generation of possible moves at each step, in order to prevent the algorithm from traversing the entire search space.\nYang and Honavar (1998) argued against the monotonicity assumption, presenting genetic algorithms as an alternative to escape the local minimas. Pintelas (2004) explained it as follows: The features are represented as a binary string where zeros represent the absence of features and ones represent their presence. Genetic operations such as mutation (adding or removing a feature by reversing the value of the bit representing it) and crossover (combining two subsets together) take place on the strings and better feature subsets have more chance to produce newer subsets via more mutations and crossovers.\nOne idea proposed by Yu and Liu (2004), is to start with individual feature selection first, to eliminating irrelevant features Then subset selection is performed later to remove redundant features. By decoupling the two processes, they downsize the search space for the subset selection, hence improving its performance. However, this contradicts with\nwhat Kohavi-1997 warned of, where an irrelevant feature on its own can still form, among others, an optimal subset.\nIn each iteration, the generated subset has to be evaluated. Dash and Liu (2003) explained that this step compares the new subset with the previously acquired ones, or with predefined optimum threshold to decide (i) whether the new subset should replace the previous best subset, and also (ii) whether a stopping criterion has been reached to prevent the algorithm from doing exhaustive search.\nThe way evaluation is done is what subdivides feature selection into to two main categories: filters and wrappers (Hall, 1999; Liu et al., 2010). The two approaches are discussed in the next section."}, {"heading": "3 Filters and Wrappers", "text": "Filters and wrappers are two evaluation strategies. In filters, individual features or subsets are evaluated independently of the learning algorithms, while wrappers use the learning algorithm to evaluate feature subsets (Este\u0301vez et al., 2009).\nGheyas and Smith (2010) listed some filter methods such as: mutual information (Lewis, 1992; Peng et al., 2005; Este\u0301vez et al., 2009), chi-square test (Jin et al., 2006) and Pearson correlation coefficients (Biesiada and Duch, 2007).\nFor individual selection, Lewis (1992) measures the mutual information (MI) between each feature and the target class label. Then features are ranked accordingly, selecting the top n features. Hamming (1986) stated the following equation to calculate the MI between two variables, A = [a1, a\u2212 2, ..an] and B = [b1, b2, ..bn]:\nI(A,B) = \u03a3i\u03a3jPr(ai, bj) log Pr(ai, bj)\nPr(ai) \u2217 Pr(bj)\nIt is clear from the previous equation that for features with equal conditional probability with a class, the rare ones get higher scores than common ones (Yang and Pedersen, 1997). On contrary, Yang and Pedersen (1997) added that Chi-Squared values are normalized, but it is not suitably for rare features, since they hardly follow X2 distribution. Linear correlation coefficient is another option, however Yu and Liu (2004); Go\u0301mez-Verdejo et al. (2009) warned that the assumption of linear relation between variables and classes is not usually valid; therefore, MI is still widely used.\nIt worth mentioning here that some papers, such as Yang and Pedersen (1997), discriminate between Information Gain (IG) and Mutual Information (MI), however Cover and Thomas (2006, p. 21) showed that the MI formula mentioned above is the same one referred to by Quinlan (1986); Hall (1999) as IG.\nTraditionally, filter methods select features individually. One idea is to calculate MI between class labels and subsets instead of individual features. However, Ding and Peng (2005) explained that the more variables in our joint probabilities, the harder it is for our limited sample to cover the multivariate density. Hence, they proposed a \u201cminimalredundancy-maximal-relevance\u201d (mRMR) formula, which accounts for both inter-features and feature-to-class MI. Both Peng et al. (2005) and Este\u0301vez et al. (2009) built on this idea. Similarly, Torkkola (2003) proposed the use of Renyi\u2019s entropy as an alternative to Shannon et al. (1949)\u2019s entropy to solve the multivariate issue, whereas Markov blanket,\npresented Koller and Sahami (1996), is one other solution.\nThe absence of target labels in unsupervised learning encouraged He et al. (2006) to use Laplacian Score (LS). LS assumes that a relevant feature is the one where neighbouring records across the whole feature space are also close across this feature vector (He et al., 2006). They added that LS yields to Fisher Criterion Score (FCS) when target labels are available. Yan et al. (2007); Fu et al. (2008) highlighted that these methods assume classes to be normally distribution across the data-space. Hence, Dhir and Lee (2009) proposed a hybrid measurement based on FCS and MI\nAlthough filters are normally faster than wrappers, John et al. (1994) warned that it doesn\u2019t take into its consideration the biases of the learning algorithm during subset selection, after showing the wrappers effectiveness. Wrapper methods use the learning algorithm, during the evaluation step, to determine the utility of a certain features subset based on the algorithm\u2019s accuracy while using that specific subset (John et al., 1994). Hall (1999) added that the training data is usually divided into folds and accuracy is determined using cross-validation. Compared to filters, Gheyas and Smith (2010) stated that wrapper\u2019s effectiveness comes at the expense of their computational cost. Because of their cost, Pintelas (2004) noticed that the forward selection algorithms (mentioned earlier) might be more common with wrappers, even if it is less effective than the backward selection. He et al. (2006) also added that wrappers are common in unsupervised learning scenarios, since filters, other than Laplacian Score, usually rely on class labels to calculate the correlations between features and those labels."}, {"heading": "4 FS and Learning Algorithms", "text": "We have stated earlier that good features are not only the ones highly correlated with the target class, but also the ones not correlated with each other. Kohavi and John (1997) highlighted that the accuracy of instance-based algorithms is vulnerable to the former, while Naive-Bayes is more robust when faced with the former yet vulnerable to the latter. Nearest neighbour (NN) algorithm is an examples of instance based learning. (Witten and Frank, 2005, p. 116) added that the adoption of k-NN, where (k > 1), can smooth the effect of noisy data a bit, hence variance.\nLal et al. (2006) remarked that some learning algorithms, such as decision trees (DT), select relevant features implicitly. Guyon and Elisseeff (2003) added that in those embedded methods of feature selection, the selection process takes place during the training phase, rather than in preprocessing step. Nevertheless, decision tree still need earlier feature selection, as noted by Kohavi and John (1997).\nAdditionally, Kohavi and John (1997) noticed in their experiments that different search algorithms work better with different learning algorithms as well as datasets. Similarly, experiments by Hua et al. (2009) proved different feature-selection methods to give various accuracy across different sample sizes and data nature."}, {"heading": "5 Conclusion", "text": "We have seen that wrappers are generally more accurate then filters, yet the latter is more computational efficient. Similar trade-offs exist between selecting the features individually or as a subset, as well as between the different search algorithms. However, experiments\nshowed that the nature of the dataset, the robustness of the classifier and the nature of the learning problem dictates our choices between those trade-offs. Additionally, there are efforts being put to make filter methods suitable to subset selection and unsupervised learning scenarios."}], "references": [{"title": "Feature selection for high-dimensional data, a pearson redundancy based filter", "author": ["J. Biesiada", "W. Duch"], "venue": "Computer Recognition Systems", "citeRegEx": "Biesiada and Duch.,? \\Q2007\\E", "shortCiteRegEx": "Biesiada and Duch.", "year": 2007}, {"title": "Consistency-based search in feature selection", "author": ["M. Dash", "H. Liu"], "venue": "Artificial intelligence,", "citeRegEx": "Dash and Liu.,? \\Q2003\\E", "shortCiteRegEx": "Dash and Liu.", "year": 2003}, {"title": "Hybrid feature selection: combining fisher criterion and mutual information for efficient feature selection", "author": ["C. Dhir", "S. Lee"], "venue": "Advances in Neuro-Information Processing,", "citeRegEx": "Dhir and Lee.,? \\Q2009\\E", "shortCiteRegEx": "Dhir and Lee.", "year": 2009}, {"title": "Minimum redundancy feature selection from microarray gene expression data", "author": ["C. Ding", "H. Peng"], "venue": "Journal of bioinformatics and computational biology,", "citeRegEx": "Ding and Peng.,? \\Q2005\\E", "shortCiteRegEx": "Ding and Peng.", "year": 2005}, {"title": "The role of occam\u2019s razor in knowledge discovery", "author": ["P. Domingos"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "Domingos.,? \\Q1999\\E", "shortCiteRegEx": "Domingos.", "year": 1999}, {"title": "Data Mining: Introductory and Advanced Topics", "author": ["Margaret H. Dunham"], "venue": null, "citeRegEx": "Dunham.,? \\Q2002\\E", "shortCiteRegEx": "Dunham.", "year": 2002}, {"title": "Normalized mutual information feature selection", "author": ["P.A. Est\u00e9vez", "M. Tesmer", "C.A. Perez", "J.M. Zurada"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Est\u00e9vez et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Est\u00e9vez et al\\.", "year": 2009}, {"title": "On bias, variance, 0/1loss, and the curse-of-dimensionality", "author": ["J.H. Friedman"], "venue": "Data mining and knowledge discovery,", "citeRegEx": "Friedman.,? \\Q1997\\E", "shortCiteRegEx": "Friedman.", "year": 1997}, {"title": "Classification and feature extraction by simplexization", "author": ["Y. Fu", "S. Yan", "T.S. Huang"], "venue": "Information Forensics and Security, IEEE Transactions on,", "citeRegEx": "Fu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fu et al\\.", "year": 2008}, {"title": "Feature subset selection in large dimensionality domains", "author": ["I.A. Gheyas", "L.S. Smith"], "venue": "Pattern Recognition,", "citeRegEx": "Gheyas and Smith.,? \\Q2010\\E", "shortCiteRegEx": "Gheyas and Smith.", "year": 2010}, {"title": "Information-theoretic feature selection for functional data classification", "author": ["V. G\u00f3mez-Verdejo", "M. Verleysen", "J. Fleury"], "venue": null, "citeRegEx": "G\u00f3mez.Verdejo et al\\.,? \\Q2009\\E", "shortCiteRegEx": "G\u00f3mez.Verdejo et al\\.", "year": 2009}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Guyon and Elisseeff.,? \\Q2003\\E", "shortCiteRegEx": "Guyon and Elisseeff.", "year": 2003}, {"title": "Correlation-based feature selection for machine learning", "author": ["M.A. Hall"], "venue": "PhD thesis, The University of Waikato,", "citeRegEx": "Hall.,? \\Q1999\\E", "shortCiteRegEx": "Hall.", "year": 1999}, {"title": "Coding and information theory", "author": ["R.W. Hamming"], "venue": "Prentice-Hall, Inc.,", "citeRegEx": "Hamming.,? \\Q1986\\E", "shortCiteRegEx": "Hamming.", "year": 1986}, {"title": "Laplacian score for feature selection", "author": ["X. He", "D. Cai", "P. Niyogi"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "He et al\\.,? \\Q2006\\E", "shortCiteRegEx": "He et al\\.", "year": 2006}, {"title": "Performance of feature-selection methods in the classification of high-dimension data", "author": ["J. Hua", "W.D. Tembe", "E.R. Dougherty"], "venue": "Pattern Recognition,", "citeRegEx": "Hua et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hua et al\\.", "year": 2009}, {"title": "On the relationship between feature selection and classification accuracy", "author": ["A.G.K. Janecek", "W.N. Gansterer", "M. Demel", "G.F. Ecker"], "venue": "In JMLR: Workshop and Conference Proceedings,", "citeRegEx": "Janecek et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Janecek et al\\.", "year": 2008}, {"title": "Machine learning techniques and chi-square feature selection for cancer classification using sage gene expression profiles", "author": ["X. Jin", "A. Xu", "R. Bie", "P. Guo"], "venue": "Data Mining for Biomedical Applications,", "citeRegEx": "Jin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2006}, {"title": "Irrelevant features and the subset selection problem", "author": ["G.H. John", "R. Kohavi", "K. Pfleger"], "venue": "In Proceedings of the eleventh international conference on machine learning,", "citeRegEx": "John et al\\.,? \\Q1994\\E", "shortCiteRegEx": "John et al\\.", "year": 1994}, {"title": "Wrappers for feature subset selection", "author": ["R. Kohavi", "G.H. John"], "venue": "Artificial intelligence,", "citeRegEx": "Kohavi and John.,? \\Q1997\\E", "shortCiteRegEx": "Kohavi and John.", "year": 1997}, {"title": "Toward optimal feature selection", "author": ["D. Koller", "M. Sahami"], "venue": null, "citeRegEx": "Koller and Sahami.,? \\Q1996\\E", "shortCiteRegEx": "Koller and Sahami.", "year": 1996}, {"title": "Feature selection and feature extraction for text categorization", "author": ["D.D. Lewis"], "venue": "In Proceedings of the workshop on Speech and Natural Language,", "citeRegEx": "Lewis.,? \\Q1992\\E", "shortCiteRegEx": "Lewis.", "year": 1992}, {"title": "Feature selection: An ever evolving frontier in data mining", "author": ["H. Liu", "H. Motoda", "R. Setiono", "Z. Zhao"], "venue": "In Proc. The Fourth Workshop on Feature Selection in Data Mining,", "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "On feature selection, bias-variance, and bagging", "author": ["M. Munson", "R. Caruana"], "venue": "Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Munson and Caruana.,? \\Q2009\\E", "shortCiteRegEx": "Munson and Caruana.", "year": 2009}, {"title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy", "author": ["H. Peng", "F. Long", "C. Ding"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Peng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2005}, {"title": "On the selection of classifier-specific feature selection algorithms", "author": ["S.B.K.P.E. Pintelas"], "venue": "In Proceedings of International Conference on Intelligent Knowledge Systems (IKS-2004),", "citeRegEx": "Pintelas.,? \\Q2004\\E", "shortCiteRegEx": "Pintelas.", "year": 2004}, {"title": "Induction of decision trees", "author": ["J.R. Quinlan"], "venue": "Machine learning,", "citeRegEx": "Quinlan.,? \\Q1986\\E", "shortCiteRegEx": "Quinlan.", "year": 1986}, {"title": "The mathematical theory of communication, volume 117", "author": ["C.E. Shannon", "W. Weaver", "R.E. Blahut", "B. Hajek"], "venue": "University of Illinois press Urbana,", "citeRegEx": "Shannon et al\\.,? \\Q1949\\E", "shortCiteRegEx": "Shannon et al\\.", "year": 1949}, {"title": "Feature extraction by non parametric mutual information maximization", "author": ["K. Torkkola"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Torkkola.,? \\Q2003\\E", "shortCiteRegEx": "Torkkola.", "year": 2003}, {"title": "Data Mining: Practical machine learning tools and techniques", "author": ["I.H. Witten", "E. Frank"], "venue": null, "citeRegEx": "Witten and Frank.,? \\Q2005\\E", "shortCiteRegEx": "Witten and Frank.", "year": 2005}, {"title": "Graph embedding and extensions: A general framework for dimensionality reduction", "author": ["S. Yan", "D. Xu", "B. Zhang", "H.J. Zhang", "Q. Yang", "S. Lin"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Yan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2007}, {"title": "Feature subset selection using a genetic algorithm", "author": ["J. Yang", "V. Honavar"], "venue": "Intelligent Systems and Their Applications, IEEE,", "citeRegEx": "Yang and Honavar.,? \\Q1998\\E", "shortCiteRegEx": "Yang and Honavar.", "year": 1998}, {"title": "A comparative study on feature selection in text categorization", "author": ["Y. Yang", "J.O. Pedersen"], "venue": "In MACHINE LEARNING-INTERNATIONAL WORKSHOP THEN CONFERENCE-,", "citeRegEx": "Yang and Pedersen.,? \\Q1997\\E", "shortCiteRegEx": "Yang and Pedersen.", "year": 1997}, {"title": "Efficient feature selection via analysis of relevance and redundancy", "author": ["L. Yu", "H. Liu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Yu and Liu.,? \\Q2004\\E", "shortCiteRegEx": "Yu and Liu.", "year": 2004}], "referenceMentions": [{"referenceID": 7, "context": "For an accurate classifier, it is needed to reduce both bias and variance of the model (Friedman, 1997).", "startOffset": 87, "endOffset": 103}, {"referenceID": 4, "context": "1 Introduction According to Dunham (2002), machine learning tasks can be seen as predictive or descriptive ones.", "startOffset": 28, "endOffset": 42}, {"referenceID": 4, "context": "1 Introduction According to Dunham (2002), machine learning tasks can be seen as predictive or descriptive ones. Classification is an example of predictive models. Friedman (1997) described it as a model where discrete output values (class labels) are learnt from the different variables (features) of the input data.", "startOffset": 28, "endOffset": 180}, {"referenceID": 4, "context": "1 Introduction According to Dunham (2002), machine learning tasks can be seen as predictive or descriptive ones. Classification is an example of predictive models. Friedman (1997) described it as a model where discrete output values (class labels) are learnt from the different variables (features) of the input data. Clustering, on the other hand, is categorised by Dunham (2002) as a descriptive task.", "startOffset": 28, "endOffset": 381}, {"referenceID": 4, "context": "1 Introduction According to Dunham (2002), machine learning tasks can be seen as predictive or descriptive ones. Classification is an example of predictive models. Friedman (1997) described it as a model where discrete output values (class labels) are learnt from the different variables (features) of the input data. Clustering, on the other hand, is categorised by Dunham (2002) as a descriptive task. The features of the input data are used to categorize it without supervised training. In both cases, the choice of the feature-set plays an important role in the performance of the data mining problem. Liu et al. (2010) listed three advantages for removing irrelevant and redundant features: it makes the data mining task more efficient, improves its accuracy and simplifies the inferred model, making it more comprehensible.", "startOffset": 28, "endOffset": 624}, {"referenceID": 4, "context": "As described by Domingos (1999), the bias is a systematic error that occurs when inferring a more generalized model for the data, hence increasing the training data will not improve it.", "startOffset": 16, "endOffset": 32}, {"referenceID": 4, "context": "As described by Domingos (1999), the bias is a systematic error that occurs when inferring a more generalized model for the data, hence increasing the training data will not improve it. Variance, on the other hand, results when the model tries to cope with the variations of the noisy data sample. Increasing the sample size in this case can balance the effect of the noise and reduce the variance accordingly. Nevertheless, Friedman (1997) stresses that during the training process, the more sensitive the model is to the training data, the lower the bias in exchange for a higher variance, and vice versa.", "startOffset": 16, "endOffset": 441}, {"referenceID": 4, "context": "As described by Domingos (1999), the bias is a systematic error that occurs when inferring a more generalized model for the data, hence increasing the training data will not improve it. Variance, on the other hand, results when the model tries to cope with the variations of the noisy data sample. Increasing the sample size in this case can balance the effect of the noise and reduce the variance accordingly. Nevertheless, Friedman (1997) stresses that during the training process, the more sensitive the model is to the training data, the lower the bias in exchange for a higher variance, and vice versa. This is known as \u201cbias-variance trade-off\u201d. Hence, as noted by Kohavi and John (1997), classifiers faced with limited data has to find an optimum point where they can actually estimate the statistical distribution of fewer features (variance reduction) versus less accurate estimation of more features (bias reduction); ergo, Munson and Caruana (2009) summarized the feature selection process as the process of finding the best bias-variance trade-off point.", "startOffset": 16, "endOffset": 694}, {"referenceID": 4, "context": "As described by Domingos (1999), the bias is a systematic error that occurs when inferring a more generalized model for the data, hence increasing the training data will not improve it. Variance, on the other hand, results when the model tries to cope with the variations of the noisy data sample. Increasing the sample size in this case can balance the effect of the noise and reduce the variance accordingly. Nevertheless, Friedman (1997) stresses that during the training process, the more sensitive the model is to the training data, the lower the bias in exchange for a higher variance, and vice versa. This is known as \u201cbias-variance trade-off\u201d. Hence, as noted by Kohavi and John (1997), classifiers faced with limited data has to find an optimum point where they can actually estimate the statistical distribution of fewer features (variance reduction) versus less accurate estimation of more features (bias reduction); ergo, Munson and Caruana (2009) summarized the feature selection process as the process of finding the best bias-variance trade-off point.", "startOffset": 16, "endOffset": 960}, {"referenceID": 4, "context": "As described by Domingos (1999), the bias is a systematic error that occurs when inferring a more generalized model for the data, hence increasing the training data will not improve it. Variance, on the other hand, results when the model tries to cope with the variations of the noisy data sample. Increasing the sample size in this case can balance the effect of the noise and reduce the variance accordingly. Nevertheless, Friedman (1997) stresses that during the training process, the more sensitive the model is to the training data, the lower the bias in exchange for a higher variance, and vice versa. This is known as \u201cbias-variance trade-off\u201d. Hence, as noted by Kohavi and John (1997), classifiers faced with limited data has to find an optimum point where they can actually estimate the statistical distribution of fewer features (variance reduction) versus less accurate estimation of more features (bias reduction); ergo, Munson and Caruana (2009) summarized the feature selection process as the process of finding the best bias-variance trade-off point. When it comes to unsupervised learning algorithms, such as clustering, Janecek et al. (2008) highlighted that the problem with high dimensional data (more features) is that it makes the proximity measures between the records more uniform, hence metrics such as distance and density become harder to obtain.", "startOffset": 16, "endOffset": 1160}, {"referenceID": 33, "context": "In its simplest form, the feature selection process can evaluate individual features and rank them based on their correlation with class labels (Yu and Liu, 2004).", "startOffset": 144, "endOffset": 162}, {"referenceID": 12, "context": "However, Hall (1999) reported that studies had proven a good feature subset to be the one whose features are not correlated to each other, besides them being correlated to class labels.", "startOffset": 9, "endOffset": 21}, {"referenceID": 12, "context": "However, Hall (1999) reported that studies had proven a good feature subset to be the one whose features are not correlated to each other, besides them being correlated to class labels. Hence, they are better evaluated as subset rather than individually. Liu et al. (2010) summarized subset feature selection process into three main steps:", "startOffset": 9, "endOffset": 273}, {"referenceID": 1, "context": "Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003).", "startOffset": 203, "endOffset": 223}, {"referenceID": 1, "context": "Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003). John et al. (1994) listed two search algorithms.", "startOffset": 204, "endOffset": 244}, {"referenceID": 1, "context": "Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003). John et al. (1994) listed two search algorithms. The forward selection algorithms starts with an empty set and keeps adding features, while the backward elimination starts with all the features and keeps on removing ones. Pintelas (2004) explained that in the two algorithms, once a feature is added it cannot be removed and vice versa.", "startOffset": 204, "endOffset": 463}, {"referenceID": 1, "context": "Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003). John et al. (1994) listed two search algorithms. The forward selection algorithms starts with an empty set and keeps adding features, while the backward elimination starts with all the features and keeps on removing ones. Pintelas (2004) explained that in the two algorithms, once a feature is added it cannot be removed and vice versa. Thus, they are described by Yang and Honavar (1998); Hall (1999) as greedy hill-climbing algorithms, where they assume monotonicity of the whole process.", "startOffset": 204, "endOffset": 614}, {"referenceID": 1, "context": "Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003). John et al. (1994) listed two search algorithms. The forward selection algorithms starts with an empty set and keeps adding features, while the backward elimination starts with all the features and keeps on removing ones. Pintelas (2004) explained that in the two algorithms, once a feature is added it cannot be removed and vice versa. Thus, they are described by Yang and Honavar (1998); Hall (1999) as greedy hill-climbing algorithms, where they assume monotonicity of the whole process.", "startOffset": 204, "endOffset": 627}, {"referenceID": 1, "context": "Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003). John et al. (1994) listed two search algorithms. The forward selection algorithms starts with an empty set and keeps adding features, while the backward elimination starts with all the features and keeps on removing ones. Pintelas (2004) explained that in the two algorithms, once a feature is added it cannot be removed and vice versa. Thus, they are described by Yang and Honavar (1998); Hall (1999) as greedy hill-climbing algorithms, where they assume monotonicity of the whole process. Kohavi and John (1997) added that dealing with smaller subset in the beginning makes the forward selection faster than the backward elimination algorithm and can reach relatively fewer features more quickly, yet the latter usually selects more interactive features.", "startOffset": 204, "endOffset": 739}, {"referenceID": 1, "context": "Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003). John et al. (1994) listed two search algorithms. The forward selection algorithms starts with an empty set and keeps adding features, while the backward elimination starts with all the features and keeps on removing ones. Pintelas (2004) explained that in the two algorithms, once a feature is added it cannot be removed and vice versa. Thus, they are described by Yang and Honavar (1998); Hall (1999) as greedy hill-climbing algorithms, where they assume monotonicity of the whole process. Kohavi and John (1997) added that dealing with smaller subset in the beginning makes the forward selection faster than the backward elimination algorithm and can reach relatively fewer features more quickly, yet the latter usually selects more interactive features. This makes Forward Selection preferable with high dimensional data. Generally, hill-climbing algorithms might get caught in local minima and fail to include useful features, or exclude irrelevant ones. Hall (1999) mentioned another algorithm; best first search.", "startOffset": 204, "endOffset": 1196}, {"referenceID": 1, "context": "Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003). John et al. (1994) listed two search algorithms. The forward selection algorithms starts with an empty set and keeps adding features, while the backward elimination starts with all the features and keeps on removing ones. Pintelas (2004) explained that in the two algorithms, once a feature is added it cannot be removed and vice versa. Thus, they are described by Yang and Honavar (1998); Hall (1999) as greedy hill-climbing algorithms, where they assume monotonicity of the whole process. Kohavi and John (1997) added that dealing with smaller subset in the beginning makes the forward selection faster than the backward elimination algorithm and can reach relatively fewer features more quickly, yet the latter usually selects more interactive features. This makes Forward Selection preferable with high dimensional data. Generally, hill-climbing algorithms might get caught in local minima and fail to include useful features, or exclude irrelevant ones. Hall (1999) mentioned another algorithm; best first search. Unlike the hill climbing algorithms, at each step it generates all possible moves and allow for backtracking once the path it is traversing is not adding any improvement. Kohavi and John (1997) highlighted the importance of having a stoppage criterion, to limit the generation of possible moves at each step, in order to prevent the algorithm from traversing the entire search space.", "startOffset": 204, "endOffset": 1438}, {"referenceID": 1, "context": "Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003). John et al. (1994) listed two search algorithms. The forward selection algorithms starts with an empty set and keeps adding features, while the backward elimination starts with all the features and keeps on removing ones. Pintelas (2004) explained that in the two algorithms, once a feature is added it cannot be removed and vice versa. Thus, they are described by Yang and Honavar (1998); Hall (1999) as greedy hill-climbing algorithms, where they assume monotonicity of the whole process. Kohavi and John (1997) added that dealing with smaller subset in the beginning makes the forward selection faster than the backward elimination algorithm and can reach relatively fewer features more quickly, yet the latter usually selects more interactive features. This makes Forward Selection preferable with high dimensional data. Generally, hill-climbing algorithms might get caught in local minima and fail to include useful features, or exclude irrelevant ones. Hall (1999) mentioned another algorithm; best first search. Unlike the hill climbing algorithms, at each step it generates all possible moves and allow for backtracking once the path it is traversing is not adding any improvement. Kohavi and John (1997) highlighted the importance of having a stoppage criterion, to limit the generation of possible moves at each step, in order to prevent the algorithm from traversing the entire search space. Yang and Honavar (1998) argued against the monotonicity assumption, presenting genetic algorithms as an alternative to escape the local minimas.", "startOffset": 204, "endOffset": 1652}, {"referenceID": 1, "context": "Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003). John et al. (1994) listed two search algorithms. The forward selection algorithms starts with an empty set and keeps adding features, while the backward elimination starts with all the features and keeps on removing ones. Pintelas (2004) explained that in the two algorithms, once a feature is added it cannot be removed and vice versa. Thus, they are described by Yang and Honavar (1998); Hall (1999) as greedy hill-climbing algorithms, where they assume monotonicity of the whole process. Kohavi and John (1997) added that dealing with smaller subset in the beginning makes the forward selection faster than the backward elimination algorithm and can reach relatively fewer features more quickly, yet the latter usually selects more interactive features. This makes Forward Selection preferable with high dimensional data. Generally, hill-climbing algorithms might get caught in local minima and fail to include useful features, or exclude irrelevant ones. Hall (1999) mentioned another algorithm; best first search. Unlike the hill climbing algorithms, at each step it generates all possible moves and allow for backtracking once the path it is traversing is not adding any improvement. Kohavi and John (1997) highlighted the importance of having a stoppage criterion, to limit the generation of possible moves at each step, in order to prevent the algorithm from traversing the entire search space. Yang and Honavar (1998) argued against the monotonicity assumption, presenting genetic algorithms as an alternative to escape the local minimas. Pintelas (2004) explained it as follows: The features are represented as a binary string where zeros represent the absence of features and ones represent their presence.", "startOffset": 204, "endOffset": 1789}, {"referenceID": 1, "context": "Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003). John et al. (1994) listed two search algorithms. The forward selection algorithms starts with an empty set and keeps adding features, while the backward elimination starts with all the features and keeps on removing ones. Pintelas (2004) explained that in the two algorithms, once a feature is added it cannot be removed and vice versa. Thus, they are described by Yang and Honavar (1998); Hall (1999) as greedy hill-climbing algorithms, where they assume monotonicity of the whole process. Kohavi and John (1997) added that dealing with smaller subset in the beginning makes the forward selection faster than the backward elimination algorithm and can reach relatively fewer features more quickly, yet the latter usually selects more interactive features. This makes Forward Selection preferable with high dimensional data. Generally, hill-climbing algorithms might get caught in local minima and fail to include useful features, or exclude irrelevant ones. Hall (1999) mentioned another algorithm; best first search. Unlike the hill climbing algorithms, at each step it generates all possible moves and allow for backtracking once the path it is traversing is not adding any improvement. Kohavi and John (1997) highlighted the importance of having a stoppage criterion, to limit the generation of possible moves at each step, in order to prevent the algorithm from traversing the entire search space. Yang and Honavar (1998) argued against the monotonicity assumption, presenting genetic algorithms as an alternative to escape the local minimas. Pintelas (2004) explained it as follows: The features are represented as a binary string where zeros represent the absence of features and ones represent their presence. Genetic operations such as mutation (adding or removing a feature by reversing the value of the bit representing it) and crossover (combining two subsets together) take place on the strings and better feature subsets have more chance to produce newer subsets via more mutations and crossovers. One idea proposed by Yu and Liu (2004), is to start with individual feature selection first, to eliminating irrelevant features Then subset selection is performed later to remove redundant features.", "startOffset": 204, "endOffset": 2276}, {"referenceID": 12, "context": "The way evaluation is done is what subdivides feature selection into to two main categories: filters and wrappers (Hall, 1999; Liu et al., 2010).", "startOffset": 114, "endOffset": 144}, {"referenceID": 22, "context": "The way evaluation is done is what subdivides feature selection into to two main categories: filters and wrappers (Hall, 1999; Liu et al., 2010).", "startOffset": 114, "endOffset": 144}, {"referenceID": 1, "context": "Dash and Liu (2003) explained that this step compares the new subset with the previously acquired ones, or with predefined optimum threshold to decide (i) whether the new subset should replace the previous best subset, and also (ii) whether a stopping criterion has been reached to prevent the algorithm from doing exhaustive search.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "In filters, individual features or subsets are evaluated independently of the learning algorithms, while wrappers use the learning algorithm to evaluate feature subsets (Est\u00e9vez et al., 2009).", "startOffset": 169, "endOffset": 191}, {"referenceID": 21, "context": "Gheyas and Smith (2010) listed some filter methods such as: mutual information (Lewis, 1992; Peng et al., 2005; Est\u00e9vez et al., 2009), chi-square test (Jin et al.", "startOffset": 79, "endOffset": 133}, {"referenceID": 24, "context": "Gheyas and Smith (2010) listed some filter methods such as: mutual information (Lewis, 1992; Peng et al., 2005; Est\u00e9vez et al., 2009), chi-square test (Jin et al.", "startOffset": 79, "endOffset": 133}, {"referenceID": 6, "context": "Gheyas and Smith (2010) listed some filter methods such as: mutual information (Lewis, 1992; Peng et al., 2005; Est\u00e9vez et al., 2009), chi-square test (Jin et al.", "startOffset": 79, "endOffset": 133}, {"referenceID": 17, "context": ", 2009), chi-square test (Jin et al., 2006) and Pearson correlation coefficients (Biesiada and Duch, 2007).", "startOffset": 25, "endOffset": 43}, {"referenceID": 0, "context": ", 2006) and Pearson correlation coefficients (Biesiada and Duch, 2007).", "startOffset": 45, "endOffset": 70}, {"referenceID": 5, "context": "In filters, individual features or subsets are evaluated independently of the learning algorithms, while wrappers use the learning algorithm to evaluate feature subsets (Est\u00e9vez et al., 2009). Gheyas and Smith (2010) listed some filter methods such as: mutual information (Lewis, 1992; Peng et al.", "startOffset": 170, "endOffset": 217}, {"referenceID": 0, "context": ", 2006) and Pearson correlation coefficients (Biesiada and Duch, 2007). For individual selection, Lewis (1992) measures the mutual information (MI) between each feature and the target class label.", "startOffset": 46, "endOffset": 111}, {"referenceID": 0, "context": ", 2006) and Pearson correlation coefficients (Biesiada and Duch, 2007). For individual selection, Lewis (1992) measures the mutual information (MI) between each feature and the target class label. Then features are ranked accordingly, selecting the top n features. Hamming (1986) stated the following equation to calculate the MI between two variables, A = [a1, a\u2212 2, .", "startOffset": 46, "endOffset": 280}, {"referenceID": 32, "context": "I(A,B) = \u03a3i\u03a3jPr(ai, bj) log Pr(ai, bj) Pr(ai) \u2217 Pr(bj) It is clear from the previous equation that for features with equal conditional probability with a class, the rare ones get higher scores than common ones (Yang and Pedersen, 1997).", "startOffset": 210, "endOffset": 235}, {"referenceID": 24, "context": "I(A,B) = \u03a3i\u03a3jPr(ai, bj) log Pr(ai, bj) Pr(ai) \u2217 Pr(bj) It is clear from the previous equation that for features with equal conditional probability with a class, the rare ones get higher scores than common ones (Yang and Pedersen, 1997). On contrary, Yang and Pedersen (1997) added that Chi-Squared values are normalized, but it is not suitably for rare features, since they hardly follow X distribution.", "startOffset": 211, "endOffset": 275}, {"referenceID": 24, "context": "I(A,B) = \u03a3i\u03a3jPr(ai, bj) log Pr(ai, bj) Pr(ai) \u2217 Pr(bj) It is clear from the previous equation that for features with equal conditional probability with a class, the rare ones get higher scores than common ones (Yang and Pedersen, 1997). On contrary, Yang and Pedersen (1997) added that Chi-Squared values are normalized, but it is not suitably for rare features, since they hardly follow X distribution. Linear correlation coefficient is another option, however Yu and Liu (2004); G\u00f3mez-Verdejo et al.", "startOffset": 211, "endOffset": 480}, {"referenceID": 8, "context": "Linear correlation coefficient is another option, however Yu and Liu (2004); G\u00f3mez-Verdejo et al. (2009) warned that the assumption of linear relation between variables and classes is not usually valid; therefore, MI is still widely used.", "startOffset": 77, "endOffset": 105}, {"referenceID": 8, "context": "Linear correlation coefficient is another option, however Yu and Liu (2004); G\u00f3mez-Verdejo et al. (2009) warned that the assumption of linear relation between variables and classes is not usually valid; therefore, MI is still widely used. It worth mentioning here that some papers, such as Yang and Pedersen (1997), discriminate between Information Gain (IG) and Mutual Information (MI), however Cover and Thomas (2006, p.", "startOffset": 77, "endOffset": 315}, {"referenceID": 8, "context": "Linear correlation coefficient is another option, however Yu and Liu (2004); G\u00f3mez-Verdejo et al. (2009) warned that the assumption of linear relation between variables and classes is not usually valid; therefore, MI is still widely used. It worth mentioning here that some papers, such as Yang and Pedersen (1997), discriminate between Information Gain (IG) and Mutual Information (MI), however Cover and Thomas (2006, p. 21) showed that the MI formula mentioned above is the same one referred to by Quinlan (1986); Hall (1999) as IG.", "startOffset": 77, "endOffset": 516}, {"referenceID": 8, "context": "Linear correlation coefficient is another option, however Yu and Liu (2004); G\u00f3mez-Verdejo et al. (2009) warned that the assumption of linear relation between variables and classes is not usually valid; therefore, MI is still widely used. It worth mentioning here that some papers, such as Yang and Pedersen (1997), discriminate between Information Gain (IG) and Mutual Information (MI), however Cover and Thomas (2006, p. 21) showed that the MI formula mentioned above is the same one referred to by Quinlan (1986); Hall (1999) as IG.", "startOffset": 77, "endOffset": 529}, {"referenceID": 3, "context": "However, Ding and Peng (2005) explained that the more variables in our joint probabilities, the harder it is for our limited sample to cover the multivariate density.", "startOffset": 9, "endOffset": 30}, {"referenceID": 3, "context": "However, Ding and Peng (2005) explained that the more variables in our joint probabilities, the harder it is for our limited sample to cover the multivariate density. Hence, they proposed a \u201cminimalredundancy-maximal-relevance\u201d (mRMR) formula, which accounts for both inter-features and feature-to-class MI. Both Peng et al. (2005) and Est\u00e9vez et al.", "startOffset": 9, "endOffset": 332}, {"referenceID": 3, "context": "However, Ding and Peng (2005) explained that the more variables in our joint probabilities, the harder it is for our limited sample to cover the multivariate density. Hence, they proposed a \u201cminimalredundancy-maximal-relevance\u201d (mRMR) formula, which accounts for both inter-features and feature-to-class MI. Both Peng et al. (2005) and Est\u00e9vez et al. (2009) built on this idea.", "startOffset": 9, "endOffset": 358}, {"referenceID": 3, "context": "However, Ding and Peng (2005) explained that the more variables in our joint probabilities, the harder it is for our limited sample to cover the multivariate density. Hence, they proposed a \u201cminimalredundancy-maximal-relevance\u201d (mRMR) formula, which accounts for both inter-features and feature-to-class MI. Both Peng et al. (2005) and Est\u00e9vez et al. (2009) built on this idea. Similarly, Torkkola (2003) proposed the use of Renyi\u2019s entropy as an alternative to Shannon et al.", "startOffset": 9, "endOffset": 405}, {"referenceID": 3, "context": "However, Ding and Peng (2005) explained that the more variables in our joint probabilities, the harder it is for our limited sample to cover the multivariate density. Hence, they proposed a \u201cminimalredundancy-maximal-relevance\u201d (mRMR) formula, which accounts for both inter-features and feature-to-class MI. Both Peng et al. (2005) and Est\u00e9vez et al. (2009) built on this idea. Similarly, Torkkola (2003) proposed the use of Renyi\u2019s entropy as an alternative to Shannon et al. (1949)\u2019s entropy to solve the multivariate issue, whereas Markov blanket,", "startOffset": 9, "endOffset": 484}, {"referenceID": 14, "context": "LS assumes that a relevant feature is the one where neighbouring records across the whole feature space are also close across this feature vector (He et al., 2006).", "startOffset": 146, "endOffset": 163}, {"referenceID": 18, "context": "Wrapper methods use the learning algorithm, during the evaluation step, to determine the utility of a certain features subset based on the algorithm\u2019s accuracy while using that specific subset (John et al., 1994).", "startOffset": 193, "endOffset": 212}, {"referenceID": 14, "context": "presented Koller and Sahami (1996), is one other solution.", "startOffset": 10, "endOffset": 35}, {"referenceID": 10, "context": "The absence of target labels in unsupervised learning encouraged He et al. (2006) to use Laplacian Score (LS).", "startOffset": 65, "endOffset": 82}, {"referenceID": 10, "context": "The absence of target labels in unsupervised learning encouraged He et al. (2006) to use Laplacian Score (LS). LS assumes that a relevant feature is the one where neighbouring records across the whole feature space are also close across this feature vector (He et al., 2006). They added that LS yields to Fisher Criterion Score (FCS) when target labels are available. Yan et al. (2007); Fu et al.", "startOffset": 65, "endOffset": 386}, {"referenceID": 7, "context": "(2007); Fu et al. (2008) highlighted that these methods assume classes to be normally distribution across the data-space.", "startOffset": 8, "endOffset": 25}, {"referenceID": 2, "context": "Hence, Dhir and Lee (2009) proposed a hybrid measurement based on FCS and MI Although filters are normally faster than wrappers, John et al.", "startOffset": 7, "endOffset": 27}, {"referenceID": 2, "context": "Hence, Dhir and Lee (2009) proposed a hybrid measurement based on FCS and MI Although filters are normally faster than wrappers, John et al. (1994) warned that it doesn\u2019t take into its consideration the biases of the learning algorithm during subset selection, after showing the wrappers effectiveness.", "startOffset": 7, "endOffset": 148}, {"referenceID": 2, "context": "Hence, Dhir and Lee (2009) proposed a hybrid measurement based on FCS and MI Although filters are normally faster than wrappers, John et al. (1994) warned that it doesn\u2019t take into its consideration the biases of the learning algorithm during subset selection, after showing the wrappers effectiveness. Wrapper methods use the learning algorithm, during the evaluation step, to determine the utility of a certain features subset based on the algorithm\u2019s accuracy while using that specific subset (John et al., 1994). Hall (1999) added that the training data is usually divided into folds and accuracy is determined using cross-validation.", "startOffset": 7, "endOffset": 529}, {"referenceID": 2, "context": "Hence, Dhir and Lee (2009) proposed a hybrid measurement based on FCS and MI Although filters are normally faster than wrappers, John et al. (1994) warned that it doesn\u2019t take into its consideration the biases of the learning algorithm during subset selection, after showing the wrappers effectiveness. Wrapper methods use the learning algorithm, during the evaluation step, to determine the utility of a certain features subset based on the algorithm\u2019s accuracy while using that specific subset (John et al., 1994). Hall (1999) added that the training data is usually divided into folds and accuracy is determined using cross-validation. Compared to filters, Gheyas and Smith (2010) stated that wrapper\u2019s effectiveness comes at the expense of their computational cost.", "startOffset": 7, "endOffset": 684}, {"referenceID": 2, "context": "Hence, Dhir and Lee (2009) proposed a hybrid measurement based on FCS and MI Although filters are normally faster than wrappers, John et al. (1994) warned that it doesn\u2019t take into its consideration the biases of the learning algorithm during subset selection, after showing the wrappers effectiveness. Wrapper methods use the learning algorithm, during the evaluation step, to determine the utility of a certain features subset based on the algorithm\u2019s accuracy while using that specific subset (John et al., 1994). Hall (1999) added that the training data is usually divided into folds and accuracy is determined using cross-validation. Compared to filters, Gheyas and Smith (2010) stated that wrapper\u2019s effectiveness comes at the expense of their computational cost. Because of their cost, Pintelas (2004) noticed that the forward selection algorithms (mentioned earlier) might be more common with wrappers, even if it is less effective than the backward selection.", "startOffset": 7, "endOffset": 809}, {"referenceID": 2, "context": "Hence, Dhir and Lee (2009) proposed a hybrid measurement based on FCS and MI Although filters are normally faster than wrappers, John et al. (1994) warned that it doesn\u2019t take into its consideration the biases of the learning algorithm during subset selection, after showing the wrappers effectiveness. Wrapper methods use the learning algorithm, during the evaluation step, to determine the utility of a certain features subset based on the algorithm\u2019s accuracy while using that specific subset (John et al., 1994). Hall (1999) added that the training data is usually divided into folds and accuracy is determined using cross-validation. Compared to filters, Gheyas and Smith (2010) stated that wrapper\u2019s effectiveness comes at the expense of their computational cost. Because of their cost, Pintelas (2004) noticed that the forward selection algorithms (mentioned earlier) might be more common with wrappers, even if it is less effective than the backward selection. He et al. (2006) also added that wrappers are common in unsupervised learning scenarios, since filters, other than Laplacian Score, usually rely on class labels to calculate the correlations between features and those labels.", "startOffset": 7, "endOffset": 986}, {"referenceID": 17, "context": "Kohavi and John (1997) highlighted that the accuracy of instance-based algorithms is vulnerable to the former, while Naive-Bayes is more robust when faced with the former yet vulnerable to the latter.", "startOffset": 0, "endOffset": 23}, {"referenceID": 17, "context": "Kohavi and John (1997) highlighted that the accuracy of instance-based algorithms is vulnerable to the former, while Naive-Bayes is more robust when faced with the former yet vulnerable to the latter. Nearest neighbour (NN) algorithm is an examples of instance based learning. (Witten and Frank, 2005, p. 116) added that the adoption of k-NN, where (k > 1), can smooth the effect of noisy data a bit, hence variance. Lal et al. (2006) remarked that some learning algorithms, such as decision trees (DT), select relevant features implicitly.", "startOffset": 0, "endOffset": 435}, {"referenceID": 11, "context": "Guyon and Elisseeff (2003) added that in those embedded methods of feature selection, the selection process takes place during the training phase, rather than in preprocessing step.", "startOffset": 0, "endOffset": 27}, {"referenceID": 11, "context": "Guyon and Elisseeff (2003) added that in those embedded methods of feature selection, the selection process takes place during the training phase, rather than in preprocessing step. Nevertheless, decision tree still need earlier feature selection, as noted by Kohavi and John (1997). Additionally, Kohavi and John (1997) noticed in their experiments that different search algorithms work better with different learning algorithms as well as datasets.", "startOffset": 0, "endOffset": 283}, {"referenceID": 11, "context": "Guyon and Elisseeff (2003) added that in those embedded methods of feature selection, the selection process takes place during the training phase, rather than in preprocessing step. Nevertheless, decision tree still need earlier feature selection, as noted by Kohavi and John (1997). Additionally, Kohavi and John (1997) noticed in their experiments that different search algorithms work better with different learning algorithms as well as datasets.", "startOffset": 0, "endOffset": 321}, {"referenceID": 11, "context": "Guyon and Elisseeff (2003) added that in those embedded methods of feature selection, the selection process takes place during the training phase, rather than in preprocessing step. Nevertheless, decision tree still need earlier feature selection, as noted by Kohavi and John (1997). Additionally, Kohavi and John (1997) noticed in their experiments that different search algorithms work better with different learning algorithms as well as datasets. Similarly, experiments by Hua et al. (2009) proved different feature-selection methods to give various accuracy across different sample sizes and data nature.", "startOffset": 0, "endOffset": 495}], "year": 2015, "abstractText": "Feature selection plays an important role in the data mining process. It is needed to deal with the excessive number of features, which can become a computational burden on the learning algorithms. It is also necessary, even when computational resources are not scarce, since it improves the accuracy of the machine learning tasks, as we will see in the upcoming sections. In this review, we discuss the different feature selection approaches, and the relation between them and the various machine learning algorithms.", "creator": "LaTeX with hyperref package"}}}