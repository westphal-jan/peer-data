{"id": "1510.03820", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2015", "title": "A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification", "abstract": "Convolutional Neural Networks (CNNs) have recently achieved remarkably strong performance on sentence classification tasks (Kim, 2014; Kalchbrenner et al.,2014; Wang et al., 2015) compared to a network where the accuracy was not only at its most low (<10%, or a very high (50%, or 50%) compared to a network where it was able to do more without a good performance task (Finn and Geller, 2015; Dreyer, 2006a; Kornel, 2015; and Cresson, 2010).\n\n\n\n\n\nIn the model where the accuracy was significantly higher than predicted, the network of training task, task and task performance were able to learn a single language using a network that has a network of learning networks, such as the Learning Learning Network (LNN), to classify the neural networks that encode speech, language and sentences to the task. These network learning networks are very simple and often use learning networks.\nFigure 1. Training task and task performance in training task and task performance.\nFigure 2. Training task and task performance.\nIn the model where the accuracy was significantly higher than predicted, the network of training task, task and task performance were able to learn a single language using a network that has a network of learning networks, such as the Learning Network (LNN), to classify the neural networks that encode speech, language and sentences to the task.\nLearning networks are very simple and often use learning networks.\nIn contrast, learning networks do not have neural networks that encode speech, language and sentences. Thus, learning networks may have no learning networks that encode speech, language and sentences.\nUsing learning networks, the LNN provides a training-related model for the learning network that uses learning networks to classify neural networks that encode words. In general, training networks train neural networks to learn a single language using the LNN, and training neural networks to learn a single language using the LNN.\nThe training-related model is the same in practice as training-related models for learning networks. In fact, it is very similar in training-related models for learning networks. The LNN training network training network training network training network training network is used by the researchers and teachers of language and sentences as the learning networks and learning networks.\nTraining learning networks\nTraining networks used in training-related models like LNN and LNN training have been proposed previously.\nTraining-related models, like LNN and LNN training have been proposed previously. In", "histories": [["v1", "Tue, 13 Oct 2015 19:00:57 GMT  (141kb,D)", "http://arxiv.org/abs/1510.03820v1", null], ["v2", "Mon, 19 Oct 2015 19:26:44 GMT  (142kb,D)", "http://arxiv.org/abs/1510.03820v2", "Corrected typos and add some more references"], ["v3", "Sat, 20 Feb 2016 07:01:52 GMT  (152kb,D)", "http://arxiv.org/abs/1510.03820v3", null], ["v4", "Wed, 6 Apr 2016 23:20:27 GMT  (148kb,D)", "http://arxiv.org/abs/1510.03820v4", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["ye zhang", "byron wallace"], "accepted": false, "id": "1510.03820"}, "pdf": {"name": "1510.03820.pdf", "metadata": {"source": "CRF", "title": "A Sensitivity Analysis of (and Practitioners\u2019 Guide to) Convolutional Neural Networks for Sentence Classification", "authors": ["Ye Zhang", "Byron C. Wallace"], "emails": ["yezhang@utexas.edu", "byron.wallace@utexas.edu"], "sections": [{"heading": "1 Introduction", "text": "Our focus in this work is on the practically important task of sentence categorization. Convolutional Neural Networks (CNNs) have recently been shown to perform quite well for this task (Kim, 2014; Kalchbrenner et al., 2014; Wang et al., 2015; Goldberg, 2015). Such models capitalize on distributed representations of words by first converting the tokens comprising each instance into a vector, forming a matrix to be used\nas input to the CNN (Figure 1). Empirical results have been impressive. And the models need not be complex to realize strong results: e.g., Kim (2014) proposed a straight forward one-layer CNN architecture that achieved consistent state of the art (or comparable) results across several tasks. Thus there is now compelling support to prefer CNNs over sparse linear models for sentence classification tasks.\nHowever, a downside to CNNs is that they require practitioners to specify the exact model architecture to be used and to set the accompanying hyper-parameters. To the uninitiated, making such decisions can seem like something of a black art, especially because there are many \u2018free parameters\u2019 in the model that one could explore. This is in contrast to the linear models widely used for text classification, such as regularized logistic regression and linear-kernel Support Vector Machines (SVMs) (Joachims, 1998). Such models are typically induced over sparse \u2018bag-of-words\u2019 representations of text and require comparatively little tuning: often one needs only to set the parameter encoding the regularization strength (i.e., the model bias). Conducting a line-search to identify this parameter using training data provides a practical means of setting this hyper-parameter.\nThe recent work on CNNs for sentence classification alluded to above have provided the settings used to achieve reported results. However these configurations were selected via an unspecified tuning procedure performed using a development set. But in practice, exploring the space of possible configurations for CNNs is extremely expensive, for at least two reasons: (1) training these models is relatively slow, even using GPUs. For example, it takes about 1 hour to run 10-fold cross validation on SST-1 dataset (Socher et al., ) using a similar configuration to that described in (Kim, 2014).1 (2) the space of possible model architec-\n1We ran all experiments reported in this paper using\nar X\niv :1\n51 0.\n03 82\n0v 1\n[ cs\n.C L\n] 1\n3 O\nct 2\n01 5\ntures and hyper-parameter settings is vast. For example, the simple CNN architecture we consider requires, at a minimum, specifying the following: the input word vector representations; the filter region size(s); the number of feature maps; the activation function(s); the pooling strategy/operation; the dropout rate (if any); and the l2-norm constraint (if any).\nIn practice, tuning all of these parameters is simply not feasible, especially in light of the runtime required for parameter estimation. Our aim is thus to identify empirically the settings that practitioners should expend effort tuning, and those that are either inconsequential with respect to performance or that seem to have a \u2018best\u2019 setting independent of the specific dataset. We take inspiration from previous empirical analyses of neural models due to Coates et al. (2011) and Breuel (Breuel, 2015), which investigated factors that effect performance of unsupervised feature learning and the effects of Stochastic Gradient Descent (SGD) hyper-parameters on training, respectively. Here we consider the effects of configurations of the model architecture and hyper-parameter values of one-layer CNNs specifically for the task of sentence categorization. We report the results of a large number of experiments exploring different configurations of this model, run over seven sentence classification datasets.\nFor those interested in only the punchlines, we summarize our empirical findings and derive from these practical advice presented in Section 6."}, {"heading": "2 Background and Preliminaries", "text": "Deep learning methods are now well established in machine learning (LeCun et al., 2015; Bengio, 2009). They have been especially successful (and popular) for image and speech processing tasks. However, recently such methods have begun to overtake traditional sparse, linear models for natural language processing (NLP) tasks (Goldberg, 2015). Much of the interest in this space has been focused on inducing distributed representations of words (Bengio et al., 2003; Mikolov et al., 2013) and jointly embedding such \u2018internal\u2019 representations into models for token classification (Collobert and Weston, 2008; Collobert et al., 2011) or sentence modeling (Kalchbrenner et al., 2014; Socher et al., ).\nIn (Kalchbrenner et al., 2014), the authors con-\nTheano package, on an NVIDIA K20 GPU.\nstructed a CNN architecture with multiple convolution layers. Their model used dynamic k-max pooling. Their model posits latent, dense, low dimensional word vectors (initialized to random values prior to inference).\nKim (2014) defined a much simpler architecture that achieves comparable results to (Kalchbrenner et al., 2014) on the same datasets. This model also represents each word as a dense, low dimensional vector (Mikolov et al., 2013). They used pre-trained word vectors, and considered two approaches: static and non-static. In the former approach, word vectors are treated as static inputs, while in the latter one dynamically adjusts (or \u2018tunes\u2019) the word vectors for a specific task.\nElsewhere, Johnson and Zhang (2014) introduced a similar model, but swapped in high dimensional one-hot vector representations of words. They considered two variants of this approach, seq-CNN and bow-CNN. The former completely preserves sequential structure (at the cost of operating in a very high dimensional input space) while the latter preserves some sequence, but loses order within small regions. Their focus was on classification of longer texts, rather than sentences (but of course the model could be used for sentence classification).\nThe relative simplicity of Kim\u2018s architecture \u2013 which is largely the same as that proposed by Johnson and Zhang (2014), modulo the word vectors \u2013 coupled with observed strong empirical performance across several datasets makes this an appealing approach for sentence classification. However, in practice one is faced with making several model architecture decisions and setting various hyper-parameters. At present, very little empirical data is available to guide such decisions; addressing this gap is our aim here."}, {"heading": "2.1 CNN Architecture", "text": "We first describe the relatively simple CNN architecture we use in this paper. We begin with a tokenized sentence which we then convert to a sentence matrix, the rows of which are inferred word vectors for each token. For example, these might be outputs from the word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014) models. We denote the dimensionality of the word vectors by d. If the length of a given sentence (i.e., token count) is s, then the dimensionality of the sen-\ntence matrix is s \u00d7 d.2 Following (Collobert and Weston, 2008), we can then effectively treat the sentence matrix as an \u2018image\u2019, and perform convolution on it via linear filters. In NLP applications there is inherent sequential structure to the data. Intuitively, because rows represent discrete symbols (namely, words), it is reasonable to use filters with widths equal to the dimensionality of the word vectors (i.e., d). We can then think of varying only the \u2018height\u2019 of the filter, which refers to the number of adjacent rows (word vectors) considered jointly. From this point on, we\u2019ll refer to the height of the filter as the region size of the filter.\nSuppose that there is a filter parameterized by the weight vector w with region size h; w will contain h \u00b7 d parameters to be estimated. We denote the sentence matrix by A, and use A[i : j] to represent the sub-matrix of A from row i to row j. The output sequence of the convolution operator is obtained by repeatedly applying the filter on sub-matrices of A:\noi = w \u00b7A[i : i+ h\u2212 1], (1)\nwhere i = 1 . . . s \u2212 h + 1, and \u00b7 is the dot product between the sub-matrix and the filter, and the length of the output sequence o is s \u2212 h + 1. We include a bias term b and an activation function f to each oi, inducing the feature map c of length s\u2212 h+ 1 for this filter:\nci = f(oi + b). (2)\nNote that one may use multiple filters for the same region size, with the aim being that each filter learns complementary features from the same regions. One may also specify multiple kinds of filters with different region sizes (i.e., \u2018heights\u2019).\nThe dimensionality of the feature map generated by each filter will very as a function of the sentence length and the region size. Therefore, a pooling function must be applied to each feature map to generate a fixed-length feature vector for the final classification layer. Commonly the pooling operator is the max-pooling function (Boureau et al., 2010b), which generates a unidimensional feature from each feature map. Alternatively, the pooling strategy can be modified to operate within equal-sized regions in the feature map, encoding salient features corresponding\n2We use the same zero-padding strategy as in (Kim, 2014).\nto each region. Together, the fixed-length outputs generated from each filter map can be concatenated into a \u2018top-level\u2019 feature vector, the size of which will be independent of individual sentence lengths.\nAt this level, one may opt to apply a \u2018dropout strategy\u2019 (Hinton et al., 2012) as means of regularization. This entails randomly setting some values in the vector to 0. We may also choose to impose an l2 norm constraint, i.e., linearly scale the l2 norm of the vector to a specified threshold when it exceed this.\nThis representation is then fed through a softmax function to generate the final classification. During training, the objective to minimized is the categorical cross-entropy loss, and the parameters to be estimated include the weight vector(s) of the filter(s), the bias term in the activation function, and the weight vector of the softmax function. Note that one may either opt to treat the word vectors as fixed (we will refer to this as \u2018static\u2019) or as additional parameters of the model, to be tuned (we will refer to this approach as \u2018non-static\u2019). We explore both variants.\nFigure 1 provides a simple schematic to illustrate the model architecture just described."}, {"heading": "3 Datasets", "text": "We use the same seven datasets as in (Kim, 2014), summarized briefly as follows:\n\u2022 MR: Sentence polarity dataset from (Pang and Lee, 2005).\n\u2022 SST-1: Stanford Sentiment Treebank (Socher et al., ). Note that to make input representations consistent across tasks, we only train and test on sentences. This is in contrast to (Kim, 2014), wherein the authors trained models on both phrases and sentences.\n\u2022 SST-2: Derived from SST-1, but paring to only two classes. We again only train and test models on sentences, excluding phrases.\n\u2022 Subj: Subjectivity dataset from (Pang and Lee, 2005).\n\u2022 TREC: Question classification dataset from (Li and Roth, 2002).\n\u2022 CR: Customer review dataset (Hu and Liu, 2004).\n\u2022 MPQA: Opinion polarity dataset (Wiebe et al., 2005)\nWe give the average length and the maximum length of the tokenized sentences for all seven datasets in Table1. For more details on these datasets, please refer to (Kim, 2014)."}, {"heading": "4 Performance of Baseline Models", "text": "To provide a point of reference for the CNN results, we first report the performance achieved using sparse regularized linear models for text classification: specifically Support Vector Machine (SVM) and logistic regression. We used uni-gram and bi-gram features, keeping only the most frequent 30k n-grams for all datasets.\nWe also wanted to explore the relative gains achieved by navely incorporating embedding information directly into these models. To this end, we augmented this representation with averaged word vectors (from word2vec or GloVe) calculated over the words comprising the sentence. We then use an RBF-kernel SVM as the classifier operating in this dense feature space. We also experiment with combining the uni-gram, bi-gram and word2vec as the feature of the sentence and use linear SVM as the classifier.\nWe tuned the regularization hyper-parameters via nested cross-fold validation, optimizing for accuracy. We report means from 10-folds over all datasets in Table 2.3 For consistency, we use the same pre-processing steps for the data as described in previous work (Kim, 2014).\nOne thing that is immediately notable from these results is the consistent performance increase realized by even navely incorporating word2vec output into feature vectors.\n3Note that when folds are fixed, parameter estimation for SVM via QP is deterministic, which is why we report only means here."}, {"heading": "5 Sensitivity Analysis of CNNs", "text": "We now report results from our main analysis, which aims to interrogate the sensitivity of CNNs for sentence classification as a function of specific architecture and hyper-parameter settings. To this end, we take as our starting point a baseline configuration (described below) which has been shown to work well in previous work (Kim, 2014). We then explore the effects of modifying components of this baseline configuration in turn, holding other settings constant.\nWe performed experiments using both \u2018static\u2019 and \u2018non-static\u2019 word vectors; in the former case, word vectors are not updated during inference, while in the latter case the vectors are \u2018tuned\u2019 to the task at hand. The non-static configuration uniformly outperformed the static variant. Therefore, we report only non-static results in this paper, although we provide results for the static configuration in the Appendix."}, {"heading": "5.1 Baseline Configuration", "text": "We now consider the performance of a baseline model configuration of CNN. Specifically, we start with the architectural decisions and hyperparameters used in previous work (Kim, 2014). To contextualize the variance in performance attributable to various architecture decisions and hyper-parameter settings, it is critical to assess the variance due strictly to the parameter estimation procedure. Most prior work, unfortunately, has not reported such variance, despite a highly stochastic inference procedure. This variance is attributable to estimation via Stochastic Gradient Descent (SGD), random dropout, and random weight parameter initialization. We show that mean performance calculated via 10-fold cross validation exhibits relatively high variance over repeated runs.\nWe first use the original parameter settings depicted in Table 3 and replicate the experiments\n100 times for each dataset, in which each replication is a 10-fold CV5, and the folds over the replications are fixed. \u2018ReLU\u2019 in Table 3 refers to rectified linear unit(Maas et al., 2013), which is a commonly used activation function in CNN. We record the average accuracy over the 10-folds for each replication and report the mean, minimum and maximum mean values observed over 100 replications. We do this for both static and non-static methods. This provides a sense of the variance we might observe without any changes to the model. Results are shown in Table 4. Figure 2 provides density plots of the mean accuracy of 10- fold CV over the 100 replications for both methods on all the datasets. For clarity in presentation, we exclude SST-1, because accuracy is substantially lower on this dataset (results can be found in the tables, however). Since we split and process some datasets differently from the previous work as we have described previously, the results are also different from the original ones. And since in this work, we\u2019re only interested in the sensitivity and\n5We run 10-fold CV for all datasets, which is different from (Kim, 2014)\neffect of each component of CNN on the performance, we don\u2019t care much about the absolute accuracy and won\u2019t compare the results we got with the ones in previous works.\nHaving established a baseline performance for CNNs, we now consider the effect of different architecture decisions and hyper-parameter settings. To this end, we hold all other settings constant (as\nper Table 3) and vary only the component of interest. For every configuration that we consider, we replicate the experiment 10 times, where each replication constitutes a run of 10-fold CV.6 Like the 100 replications of the original parameter settings, we also report the average mean, minimum mean, and maximum mean of 10 fold CV over 10 replications. For all experiments, we use the same preprocessing steps for the data as in (Kim, 2014). Similarly, we use the ADADELTA update rule for SGD (Zeiler, 2012), and set the minibatch size as 50 ."}, {"heading": "5.2 Effect of input word vectors", "text": "A nice property of sentence classification models that start with distributed representations of words as inputs is the flexibility the architecture affords to swap in different pre-trained word vectors. Therefore, we first explore the sensitivity of CNNs for sentence classification with respect to the input representations used. In particular, we replace Google word2vec with GloVe representations (pre-trained on 840 billion tokens of web data from Common Crawl (Pennington et al., 2014)). We keep all other settings the same as in the original configuration. We report results in Table 5. (Note that we also report results for SVM augmented with average GloVe vectors in Table 2.)\nAs a potentially simple means of realizing the best performance across all datasets, we also considered an approach that capitalizes jointly on both of these pre-trained representations. Specifically, we concatenated word2vec and GloVe vectors for each word, resulting in 600-dimensional word vectors, and we used these as input for the CNN. Pre-trained vectors may not always be available for specific words (either in word2vec or GloVe, or both); in such cases, we randomly initialized the corresponding sub-vectors, as described above.\n6Running 100 replications for every configuration that we consider was simply not feasible.\nResults are shown Table 6. Here we report results only for the non-static variant, given its general superiority.\nFrom these results, one can see that the relative performance when using GloVe versus word2vec depends on the dataset, and, unfortunately, that simply concatenating these representations is not necessarily helpful. Practically, when faced with a new dataset, it is probably worth experimenting with different pre-trained word vectors using the training data.\nWe also experimented with using long, sparse one-hot vectors as input word representations, in the spirit of (Johnson and Zhang, 2014). In this strategy, each word is encoded as a one-hot vector, which is a sparse high dimensional vector . In this case, the width of the sentence matrix is equal to the vocabulary size. The one-hot vector is fixed during the training, since this method acts like it searches each word in a pre-built dictionary. The performance is shown in Table 7.\nComparing the result with the ones with word2vec and GloVe, , we can see that under the same basic CNN configuration, one-hot vector uniformly performs worse than word2vec or GloVe. We do not exclude the possibility that with specific configurations, one-hot CNN may be able to outperform other input representations for sentence classification. But our evidence here suggests that one-hot CNN may not be suitable for sentence classification. This may be due to sparsity; the sentences are perhaps too brief to provide\nenough information for this high-dimensional encoding (whereas this may be less of a problem for longer documents)."}, {"heading": "5.3 Effect of filter region size", "text": "We first explore the effect of filter region size when using only one region size, and we set the number of feature maps for this region size as 100 (as in the original configuration). We consider region sizes of 1,3,5,7,10,15,20,25, and 30, and record the mean, minimum, and maximum accuracies over 10 replications of 10-fold CV for each region size, and show the performance in Table 8.\nFigure 3 shows the difference between mean accuracy of each region size and the region size 3. Because we are only interested in the trend of the accuracy as we alter the region size or other components of the CNN (and not the absolute performance on each task), we show only the change in accuracy from an arbitrary baseline point (here, a region-size of 3). We follow this convention for all figures in this paper to ease interpretation.\nFrom the figure, we can conclude that each dataset has its own optimal range of filter region size. Practically, this suggests performing a coarse grid search over specified range; the data here suggests that a reasonable range for sentence classification might be from 2 to 25. However, for datasets comprising longer sentences, such as CR (maximum sentence length is 105), the optimal region size might be even larger. This might also due to the fact that in CR, it\u2019s easier to predict the positive/negative customer reviews given a larger context.\nWe also explore the effect of combining sev-\neral different filter region sizes, while keeping the number of feature maps for each region size fixed at 100. Here we find that combining several filters with region size close to the optimal single region size can improve the performance, but adding region sizes far outside the optimal range may hurt performance. For example, from Figure 3, one can observe that the optimal single region size for the MR dataset is 7. We therefore combine several different filter region sizes of close to this optimal range, and compare this to approaches that use region sizes outside of this range. From Table 9, we can see that using (5,6,7),and (7,8,9) and (6,7,8,9)\u2013 the sets near the best single region size \u2013 produce the best results. The difference is especially pronounced when comparing to the baseline setting of (3,4,5). Note that even only using a single good filter region size 7 results in better performance than combining different sizes (3,4,5). The best performing strategy here is to simply use many feature maps (here, 400) all with region sizes equal to 7, i.e., the single best region size.\nWe then give another empirical result using several region sizes on TREC dataset in Table 10. From Fig 3, we see that the best single filter region size for TREC is 3 and 5, so we explore the region size around them, and compare with the multiple region sizes far away from them. From TREC result, we see that (3,3,3) and (3,3,3,3) are worse than (2,3,4) and (3,4,5). However, the result still shows that combination of the region sizes near the optimal single region size can perform better than the region sizes far away from the optimal single region size, and a single good region size (3) outperforms combination of several bad region sizes (7,8,9) and (14,15,16).\nIn light of these observations, we believe it advisable to first perform a coarse line-search over a\nsingle filter region size to find the \u2018best\u2019 size for the dataset under consideration, and then explore the combining filters with region sizes nearby this single best size."}, {"heading": "5.4 Effect of number of feature maps for each filter region size", "text": "We explore the effect of number of feature maps for each filter region size. Again, we keep other configurations the same, where there are three filter region sizes: 3, 4 and 5, and only the number of feature maps for each of these region size is changed. We keep the number of feature maps for 3, 4, and 5 the same. The results for non-static CNN are shown in Table 11. The change in accuracy from the baseline 100 is shown in Fig 4.\nOne can see from the \u2018best\u2019 number of feature maps for each filter region size depends on the dataset. However, as a practical observation, it would seem that increasing the number of maps beyond 600 yields at best very marginal returns, and often hurts performs (likely due to overfitting). Another point to notice is that it takes a longer time to train the model when the number of feature maps is increased. We provide results pertaining to running time as a function of the number of feature maps in the Appendix. In practice, the evidence here suggests perhaps searching over a range of 50 to 600."}, {"heading": "5.5 Effect of activation function", "text": "We explore the effect of seven different activation functions in the convolution layer, including: ReLU (as per the original setting), hyperbolic tangent (tanh), Sigmoid function, (Maas et al., 2013), SoftPlus function (Dugas et al., 2001), Cube function(Chen and Manning, 2014), and tanh cube function (Pei et al., 2015). We use \u2018Iden\u2019 to denote the identity function, which means not using any activation function. The effect of different activation functions in non-static CNN is reported in Table 12, and the change in accuracy from baseline \u2018ReLU\u2019 is shown in Fig 5.\nFrom the figure, we can see that for 6 out of 7 datasets, the best activation function is one of Iden, ReLU and tanh. Softplus and Sigmoid functions outperform these only on one dataset (MPQA). Practically, we therefore suggest experimenting with each of Iden, ReLU and tanh."}, {"heading": "5.6 Effect of pooling strategy", "text": "We investigate the effect of the pooling strategy and the pooling region size. We keep the filter region size the same as (3,4,5) as in the baseline configuration, and we use 100 feature maps for each\nregion size, thus we change only the pooling strategy and the pooling region size.\nIn the original configuration, 1-max pooling was performed over entire feature maps, inducing a feature vector of length 1 for each filter. Alternatively, however, pooling may also be performed over small equal sized local regions rather than over the entire feature map (Boureau et al., 2011). Each small local region on the feature map will generate a single number from pooling, and all of these numbers can be concatenated together to form a feature vector for one feature map. The following step is the same as 1-max pooling: we concatenate all the feature vectors together to form a single feature vector for the classification layer. Figure 6 illustrates an example of max pooling performed over local regions (with size 3) on a feature map of length 9. This feature map will generate a feature vector of length 3. Here we use the same pooling strategy (max pooling or average\npooling) for all of the feature maps. Because we have 100 feature maps for each of the three filter region sizes, if each feature map generates a feature vector of length n, then the final feature vector will have length 300 \u2217 n rather than 300, as in the case 1-max pooling. When the length of the feature map is not a multiple of the pooling region size, we simply take the maximum value from the rest of the feature map.\nThe results for non-static CNN are shown in Table 13. Here \u2018max,3\u2019 means that we perform max pooling within local region of sizes of 3 on the feature map for all of the filter region sizes. Interestingly, 1-max pooling (\u2018max,all\u2019; the last column) uniformly outperforms local max pooling.\nWe also consider a k-max pooling strategy similar to (Kalchbrenner et al., 2014), in which the maximum k values are extracted from the entire\nfeature map, and the relative order of these values is preserved. We explore the effect of k on the performance, and show the results for non-static CNN in Table 14. Again we see 1-max pooling fares best, consistently outperforming k-max pooling.\nNext, we replace the max operation in the local max pooling with the average operation (Boureau et al., 2010a), that is, we take the average value within a region size rather than the maximum value. The rest of the architecture is the same. We find that the average pooling uniformly performs (much) worse than max pooling, at least on the CR and TREC dataset as shown in Table 15 and in\nTable 16.7\nAverage pooling region size Accuracy (%) 3 81.01 (80.73,81.28)\n10 80.74 (80.36,81.09) 20 80.69 (79.72,81.32) 30 81.13 (80.16,81.76)\nall (1-average) 80.17 (79.97,80.84)\nTable 15: Performance of average pooling on CR using non-static word2vec-CNN\nThe take-away from our analysis of pooling strategies is that 1-max pooling consistently performs better than alternative strategies for the task of sentence classification. This may be because the location of predictive contexts does not matter, and certain n-grams in the sentence can be more predictive on their own than the entire sentence considered jointly.\n7Due to the substantially worse performance and slow running time when using average versus max pooling, we did not repeat experiments on all datasets.\nNone 0.0 0.1 0.3 0.5 0.7 0.9 Dropout rate\n4\n3\n2\n1\n0\n1\nC h a n g e i n a\ncc u ra\ncy (\n% )\nMR SST-1 SST-2 Subj TREC CR MPQA\nFigure 7: Effect of dropout rate in non-static word2vec-CNN"}, {"heading": "5.7 Effect of regularization", "text": "As mentioned above, \u2018dropout\u2019 is one means of regularization, and is applied to the input to the penultimate layer. We experiment with varying the dropout rate from 0.0 to 0.9, while fixing the l2 norm constraint to 3, as per the baseline configuration. The results for non-static CNN are shown in Table 17. We also report the accuracy achieved when we remove both dropout and the l2 norm constraint (i.e., when no regularization is performed); thi sis dentoed as \u2018None\u2019. The change in accuracy from baseline 0.5 is shown in Fig 7.\nSeparately, we considered the effect of the l2 norm imposed on the weight vectors that parametrize the softmax function. Recall that the l2 norm of a weight vector is linearly scaled to a constraint c when it exceeds this threshold, so smaller c implies stronger regularization. (Note that this strategy also applies only to the penultimate layer like the dropout.) We show the relative effect of varying c on non-static CNN in Table 18 and Figure 8, where we have fixed the dropout rate to 0.5; 3 is the baseline here (again, arbitrarily).\nFrom Figures 7 and 8, one can see that nonzero dropout rates can help at some points from\n0.1 to 0.5, depending on datasets. But the l2 norm constraint either does not help much, and even adversely effects performance on at leastone dataset (CR). This observation is in contrast to results reported for image and speech tasks (Srivastava et al., 2014). We would suggest not using strong regularization: perhaps using a dropout rate ranging from 0 to 0.5, and l2 norm constraint no less than 3. Further, it is probably worth exploring dropping reuglarization entirely."}, {"heading": "6 Conclusions", "text": ""}, {"heading": "6.1 Summary of Main Empirical Findings", "text": "From our experimental analysis we draw several conclusions that we hope will guide future work and be useful for researchers new to using CNNs for sentence classification.\n\u2022 Prior work has tended to report only the mean\nperformance on datasets achieved by models. But this overlooks variance due solely to the stochastic inference procedure used. This can be substantial: holding everything constant (including the folds), so that variance is due exclusively to the stochastic inference procedure, we find that mean performance (calculated via 10 fold cross-validation) has a range of up to 1.5 points (Table 4). More replication should be performed in future work, and ranges/variances should be reported, to prevent potential\n\u2022 Surprisingly (in our view) regularization \u2013 i.e., drop-out and l2 constraints on parameter weights \u2013 seems to have little effect on model performance on some datasets.\n\u2022 We find that, even when tuning them to the task at hand, the choice of input word vector representation (e.g., between word2vec and GloVe) has an impact on performance, however different representations perform better for different tasks. At least for sentence classification, both seem to perform better than using one-hot vectors directly.\n\u2022 The filter region size can have a large effect on performance, and should be tuned on a per-task basis.\n\u2022 The number of features can also play an important role in the performance, however, trying to find a good number might be time consuming.\n\u2022 1-max pooling uniformly beats other pooling strategies."}, {"heading": "6.2 Advice to practitioners", "text": "Drawing from our empirical results, we provide the following guidance regarding CNN architecture and hyper-parameters for practitioners looking to apply CNNs to a new sentence classification task.\n\u2022 Consider starting with the basic configuration described in Table 3 and use the non-static word2vec or GloVe rather than one-hot vector CNN.\n\u2022 Line-search over the single filter region size to find the \u2018best\u2019 single region size. A reasonable range might be 2\u223c10. However, for datasets with very long sentences like CR, it may be worth exploring larger filter region sizes. Once this \u2018best\u2019 region size is identified, it may be worth exploring combining multiple filters using regions sizes near this single best size, given that empirically multiple \u2018good\u2019 region sizes always outperformed using only the single best region size.\n\u2022 Alter the number of feature maps for each filter region size from 50 to 600. Note that increasing the number of feature maps will increase the running time, so there is a trade-off to consider.\n\u2022 Consider different activation functions if possible: ReLU and tanh are the best overall candidates. And it might also be worth trying no activation function at all.\n\u2022 Use 1-max pooling; it does not seem necessary to expend resources evaluating alternative strategies.\n\u2022 Regarding regularization: If using regularization, perform a line search over the dropout rate parameter from 0.0 to 0.5. Using an l2 norm constraint is probably not necessary; but if one is to be used, it should be small;\nit may also be worth trying no regularization at all.\n\u2022 When summarizing the performance of a model (or a particular configuration thereof), it is imperative to consider variance. Therefore, replications of the cross-fold validation procedure should be performed and variances and ranges reported.\nOf course, the above suggestions are applicable only to datasets comprising sentences with similar properties to the seven considered in this work. And there may be examples that run counter to our findings here. Nonetheless, we believe these suggestions are likely to provide a reasonable starting point for researchers or practitioners looking to apply a simple one-layer CNNs to a new short sentence classification task."}, {"heading": "7 Appendix", "text": "Performance of logistic regression In Table 19, we additionally report results achieved using logistic regression (regularized via an l2 norm penalty on the coefficients, weighted proportionally to a hyper-parameter that we again tuned) using the same feature sets.\nEffect of single filter region size The result of single filter region size using static CNN is in Table 20.\nEffect of number of feature maps The effect of number of feature maps for each filter region size using static CNN is shown in Table 21.\nIn Table 22, we give the average running time of one 10-fold CV in sequential on TREC dataset as we increase the feature maps. 8.\nEffect of activation function. The effect of activation using static CNN is shown in Table 23.\nEffect of pooling The result of pooling strategy using static CNN is shown in Table 24 and Table 25.\nEffect of dropout rate The effect of dropout rate using static CNN is shown in Table 26.\nEffect of l2 norm constraint on weight vector The effect of l2 norm constraint using static CNN is shown in Table 27.\n8If people can parallelize the 10-fold CV, the running time will be greatly decreased."}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Learning deep architectures for ai. Foundations and trends in Machine Learning, 2(1):1\u2013127", "author": ["Yoshua Bengio"], "venue": null, "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "Learning mid-level features for recognition", "author": ["Francis Bach", "Yann LeCun", "Jean Ponce"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Boureau et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boureau et al\\.", "year": 2010}, {"title": "A theoretical analysis of feature pooling in visual recognition", "author": ["Jean Ponce", "Yann LeCun"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Boureau et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boureau et al\\.", "year": 2010}, {"title": "Ask the locals: multi-way local pooling for image recognition", "author": ["Nicolas Le Roux", "Francis Bach", "Jean Ponce", "Yann LeCun"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "Boureau et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boureau et al\\.", "year": 2011}, {"title": "The effects of hyperparameters on sgd training of neural networks. arXiv preprint arXiv:1508.02788", "author": ["Thomas M Breuel"], "venue": null, "citeRegEx": "Breuel.,? \\Q2015\\E", "shortCiteRegEx": "Breuel.", "year": 2015}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher D Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Coates et al.2011] Adam Coates", "Andrew Y Ng", "Honglak Lee"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Incorporating second-order functional knowledge for better option pricing", "author": ["Dugas et al.2001] Charles Dugas", "Yoshua Bengio", "Fran\u00e7ois B\u00e9lisle", "Claude Nadeau", "Ren\u00e9 Garcia"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Dugas et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Dugas et al\\.", "year": 2001}, {"title": "A primer on neural network models for natural language processing", "author": ["Yoav Goldberg"], "venue": null, "citeRegEx": "Goldberg.,? \\Q2015\\E", "shortCiteRegEx": "Goldberg.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Mining and summarizing customer reviews", "author": ["Hu", "Liu2004] Minqing Hu", "Bing Liu"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Hu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2004}, {"title": "Text categorization with support vector machines: Learning with many relevant", "author": ["Thorsten Joachims"], "venue": null, "citeRegEx": "Joachims.,? \\Q1998\\E", "shortCiteRegEx": "Joachims.", "year": 1998}, {"title": "Effective use of word order for text categorization with convolutional neural networks. arXiv preprint arXiv:1412.1058", "author": ["Johnson", "Zhang2014] Rie Johnson", "Tong Zhang"], "venue": null, "citeRegEx": "Johnson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882", "author": ["Yoon Kim"], "venue": null, "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Learning question classifiers", "author": ["Li", "Roth2002] Xin Li", "Dan Roth"], "venue": "In Proceedings of the 19th international conference on Computational linguistics-Volume", "citeRegEx": "Li et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Li et al\\.", "year": 2002}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Maas et al.2013] Andrew L Maas", "Awni Y Hannun", "Andrew Y Ng"], "venue": "In Proc. ICML,", "citeRegEx": "Maas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Pang", "Lee2005] Bo Pang", "Lillian Lee"], "venue": "In Proceedings of the ACL", "citeRegEx": "Pang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2005}, {"title": "An effective neural network model for graph-based dependency parsing", "author": ["Pei et al.2015] Wenzhe Pei", "Tao Ge", "Baobao Chang"], "venue": "In Proc. of ACL", "citeRegEx": "Pei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pei et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Semantic clustering and convolutional neural network for short text categorization", "author": ["Wang et al.2015] Peng Wang", "Jiaming Xu", "Bo Xu", "Chenglin Liu", "Heng Zhang", "Fangyuan Wang", "Hongwei Hao"], "venue": "In Proceedings of the 53rd Annual Meet-", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Annotating expressions of opinions and emotions in language. Language resources and evaluation, 39(2-3):165\u2013210", "author": ["Wiebe et al.2005] Janyce Wiebe", "Theresa Wilson", "Claire Cardie"], "venue": null, "citeRegEx": "Wiebe et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wiebe et al\\.", "year": 2005}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 17, "context": "performance on sentence classification tasks (Kim, 2014; Kalchbrenner et al., 2014; Wang et al., 2015).", "startOffset": 45, "endOffset": 102}, {"referenceID": 16, "context": "performance on sentence classification tasks (Kim, 2014; Kalchbrenner et al., 2014; Wang et al., 2015).", "startOffset": 45, "endOffset": 102}, {"referenceID": 25, "context": "performance on sentence classification tasks (Kim, 2014; Kalchbrenner et al., 2014; Wang et al., 2015).", "startOffset": 45, "endOffset": 102}, {"referenceID": 17, "context": "We focus on one-layer CNNs (to the exclusion of more complex models) due to their comparative simplicity and strong empirical performance (Kim, 2014).", "startOffset": 138, "endOffset": 149}, {"referenceID": 17, "context": "Convolutional Neural Networks (CNNs) have recently been shown to perform quite well for this task (Kim, 2014; Kalchbrenner et al., 2014; Wang et al., 2015; Goldberg, 2015).", "startOffset": 98, "endOffset": 171}, {"referenceID": 16, "context": "Convolutional Neural Networks (CNNs) have recently been shown to perform quite well for this task (Kim, 2014; Kalchbrenner et al., 2014; Wang et al., 2015; Goldberg, 2015).", "startOffset": 98, "endOffset": 171}, {"referenceID": 25, "context": "Convolutional Neural Networks (CNNs) have recently been shown to perform quite well for this task (Kim, 2014; Kalchbrenner et al., 2014; Wang et al., 2015; Goldberg, 2015).", "startOffset": 98, "endOffset": 171}, {"referenceID": 11, "context": "Convolutional Neural Networks (CNNs) have recently been shown to perform quite well for this task (Kim, 2014; Kalchbrenner et al., 2014; Wang et al., 2015; Goldberg, 2015).", "startOffset": 98, "endOffset": 171}, {"referenceID": 11, "context": ", 2015; Goldberg, 2015). Such models capitalize on distributed representations of words by first converting the tokens comprising each instance into a vector, forming a matrix to be used as input to the CNN (Figure 1). Empirical results have been impressive. And the models need not be complex to realize strong results: e.g., Kim (2014) proposed a straight forward one-layer CNN architecture that achieved consistent state of the art (or comparable) results across several tasks.", "startOffset": 8, "endOffset": 338}, {"referenceID": 14, "context": "contrast to the linear models widely used for text classification, such as regularized logistic regression and linear-kernel Support Vector Machines (SVMs) (Joachims, 1998).", "startOffset": 156, "endOffset": 172}, {"referenceID": 17, "context": ", ) using a similar configuration to that described in (Kim, 2014).", "startOffset": 55, "endOffset": 66}, {"referenceID": 5, "context": "(2011) and Breuel (Breuel, 2015), which investigated factors that effect performance of unsupervised feature learning and the effects of Stochastic Gradient Descent", "startOffset": 18, "endOffset": 32}, {"referenceID": 6, "context": "tion from previous empirical analyses of neural models due to Coates et al. (2011) and Breuel (Breuel, 2015), which investigated factors that effect performance of unsupervised feature learning and the effects of Stochastic Gradient Descent", "startOffset": 62, "endOffset": 83}, {"referenceID": 1, "context": "Deep learning methods are now well established in machine learning (LeCun et al., 2015; Bengio, 2009).", "startOffset": 67, "endOffset": 101}, {"referenceID": 11, "context": "However, recently such methods have begun to overtake traditional sparse, linear models for natural language processing (NLP) tasks (Goldberg, 2015).", "startOffset": 132, "endOffset": 148}, {"referenceID": 0, "context": "Much of the interest in this space has been focused on inducing distributed representations of words (Bengio et al., 2003; Mikolov et al., 2013) and jointly embedding such \u2018internal\u2019 representations into models for token classification (Collobert and Weston, 2008; Collobert et al.", "startOffset": 101, "endOffset": 144}, {"referenceID": 20, "context": "Much of the interest in this space has been focused on inducing distributed representations of words (Bengio et al., 2003; Mikolov et al., 2013) and jointly embedding such \u2018internal\u2019 representations into models for token classification (Collobert and Weston, 2008; Collobert et al.", "startOffset": 101, "endOffset": 144}, {"referenceID": 9, "context": ", 2013) and jointly embedding such \u2018internal\u2019 representations into models for token classification (Collobert and Weston, 2008; Collobert et al., 2011) or sentence modeling (Kalchbrenner et al.", "startOffset": 99, "endOffset": 151}, {"referenceID": 16, "context": "In (Kalchbrenner et al., 2014), the authors con-", "startOffset": 3, "endOffset": 30}, {"referenceID": 16, "context": "Kim (2014) defined a much simpler architecture that achieves comparable results to (Kalchbrenner et al., 2014) on the same datasets.", "startOffset": 83, "endOffset": 110}, {"referenceID": 20, "context": "This model also represents each word as a dense, low dimensional vector (Mikolov et al., 2013).", "startOffset": 72, "endOffset": 94}, {"referenceID": 20, "context": "For example, these might be outputs from the word2vec (Mikolov et al., 2013) or GloVe (Pennington et al.", "startOffset": 54, "endOffset": 76}, {"referenceID": 23, "context": ", 2013) or GloVe (Pennington et al., 2014) models.", "startOffset": 17, "endOffset": 42}, {"referenceID": 17, "context": "We use the same zero-padding strategy as in (Kim, 2014).", "startOffset": 44, "endOffset": 55}, {"referenceID": 12, "context": "At this level, one may opt to apply a \u2018dropout strategy\u2019 (Hinton et al., 2012) as means of regularization.", "startOffset": 57, "endOffset": 78}, {"referenceID": 17, "context": "We use the same seven datasets as in (Kim, 2014), summarized briefly as follows:", "startOffset": 37, "endOffset": 48}, {"referenceID": 17, "context": "This is in contrast to (Kim, 2014), wherein the authors trained models on both phrases and sentences.", "startOffset": 23, "endOffset": 34}, {"referenceID": 26, "context": "\u2022 MPQA: Opinion polarity dataset (Wiebe et al., 2005)", "startOffset": 33, "endOffset": 53}, {"referenceID": 17, "context": "For more details on these datasets, please refer to (Kim, 2014).", "startOffset": 52, "endOffset": 63}, {"referenceID": 17, "context": "3 For consistency, we use the same pre-processing steps for the data as described in previous work (Kim, 2014).", "startOffset": 99, "endOffset": 110}, {"referenceID": 17, "context": "To this end, we take as our starting point a baseline configuration (described below) which has been shown to work well in previous work (Kim, 2014).", "startOffset": 137, "endOffset": 148}, {"referenceID": 17, "context": "Specifically, we start with the architectural decisions and hyperparameters used in previous work (Kim, 2014).", "startOffset": 98, "endOffset": 109}, {"referenceID": 19, "context": "\u2018ReLU\u2019 in Table 3 refers to rectified linear unit(Maas et al., 2013), which", "startOffset": 49, "endOffset": 68}, {"referenceID": 17, "context": "We run 10-fold CV for all datasets, which is different from (Kim, 2014) effect of each component of CNN on the performance, we don\u2019t care much about the absolute accuracy and won\u2019t compare the results we got with the ones in previous works.", "startOffset": 60, "endOffset": 71}, {"referenceID": 17, "context": "For all experiments, we use the same preprocessing steps for the data as in (Kim, 2014).", "startOffset": 76, "endOffset": 87}, {"referenceID": 27, "context": "Similarly, we use the ADADELTA update rule for SGD (Zeiler, 2012), and set the minibatch size as 50 .", "startOffset": 51, "endOffset": 65}, {"referenceID": 23, "context": "we replace Google word2vec with GloVe representations (pre-trained on 840 billion tokens of web data from Common Crawl (Pennington et al., 2014)).", "startOffset": 119, "endOffset": 144}, {"referenceID": 19, "context": "gent (tanh), Sigmoid function, (Maas et al., 2013), SoftPlus function (Dugas et al.", "startOffset": 31, "endOffset": 50}, {"referenceID": 10, "context": ", 2013), SoftPlus function (Dugas et al., 2001), Cube function(Chen and Manning, 2014), and tanh cube function (Pei et al.", "startOffset": 27, "endOffset": 47}, {"referenceID": 22, "context": ", 2001), Cube function(Chen and Manning, 2014), and tanh cube function (Pei et al., 2015).", "startOffset": 71, "endOffset": 89}, {"referenceID": 16, "context": "We also consider a k-max pooling strategy similar to (Kalchbrenner et al., 2014), in which the maximum k values are extracted from the entire", "startOffset": 53, "endOffset": 80}, {"referenceID": 24, "context": "This observation is in contrast to results reported for image and speech tasks (Srivastava et al., 2014).", "startOffset": 79, "endOffset": 104}], "year": 2015, "abstractText": "Convolutional Neural Networks (CNNs) have recently achieved remarkably strong performance on sentence classification tasks (Kim, 2014; Kalchbrenner et al., 2014; Wang et al., 2015). However, these models require practitioners to specify the exact model architecture and accompanying hyper-parameters, e.g., the choice of filter region size, regularization parameters, and so on. It is currently unknown how sensitive model performance is to changes in these configurations for the task of sentence classification. We thus conduct an empirical sensitivity analysis of one-layer CNNs to explore the effect of each part of the architecture on the performance; our aim is to assess the robustness of the model and to distinguish between important and comparatively inconsequential design decisions for sentence classification. We focus on one-layer CNNs (to the exclusion of more complex models) due to their comparative simplicity and strong empirical performance (Kim, 2014). We derive practical advice from our extensive empirical results for those interested in getting the most out of CNNs for sentence classification.", "creator": "LaTeX with hyperref package"}}}