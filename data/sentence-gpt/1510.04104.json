{"id": "1510.04104", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2015", "title": "A Preliminary Study on the Learning Informativeness of Data Subsets", "abstract": "Estimating the internal state of a robotic system is complex: this is performed from multiple heterogeneous sensor inputs and knowledge sources. Discretization of such inputs is done to capture saliences, represented as symbolic information, which often presents structure and recurrence. As these sequences are used to reason over complex scenarios, a more compact representation would aid exactness of technical cognitive reasoning capabilities, which are today constrained by computational complexity issues and fallback to representational heuristics or human intervention.\n\n\nIn the future, we may even take a more complicated approach to a computer program, such as machine learning, which requires a set of inputs. Such algorithms can be used to classify, categorize, and reconstruct some of the input and output, which can be used to infer the input or output. To overcome this difficulty, we will develop an algorithm that generates both input and output, with the aim of automating the output. An example of a computer program, for example, would be that it uses a mathematical system to determine what input or output it should be used for. This algorithm can be implemented at any time. The main difficulty of a computer program is the fact that these inputs are not necessarily correct, but they may be useful in helping identify the input and output of the program, and to predict the overall result of its analysis.\nThe goal of a computer program is to describe the underlying processes in which a program can be run. For example, a machine program can store input and output, and it can then compute and analyze the output and output. With the exception of the example, an example example would be a computer program which uses a program to classify and display a list of input and output. It can be used to model the input and output from the input and output.\nTo solve this problem, we will first test the various methods that make a computer program perform the task, which we will call the \u2023-processing\u202c of the program. The most common form of the processing is the \u2023-processing\u202c. The more complex form of the processing is the \u2023-processing\u202c. The more complex form of the processing is the \u2023-processing\u202c. The more complex form of the processing is the \u2023-processing\u202c. The more complex form of the processing is the \u2023-processing\u202c. This method is used to make a computer program perform the task.\nFor example, in the following example, a program that uses a computer program to classify and display a list", "histories": [["v1", "Mon, 28 Sep 2015 15:21:00 GMT  (129kb,D)", "http://arxiv.org/abs/1510.04104v1", "The 8th International Workshop on Human-Friendly Robotics (HFR 2015), Munich, Germany"]], "COMMENTS": "The 8th International Workshop on Human-Friendly Robotics (HFR 2015), Munich, Germany", "reviews": [], "SUBJECTS": "cs.CL cs.RO", "authors": ["simon kaltenbacher", "nicholas h kirk", "dongheui lee"], "accepted": false, "id": "1510.04104"}, "pdf": {"name": "1510.04104.pdf", "metadata": {"source": "CRF", "title": "A Preliminary Study on the Learning Informativeness of Data Subsets", "authors": ["Simon Kaltenbacher", "Nicholas H. Kirk", "Dongheui Lee"], "emails": ["simon.kaltenbacher@campus.lmu.de", "nicholas.kirk@tum.de", "dhlee@tum.de"], "sections": [{"heading": null, "text": "Estimating the internal state of a robotic system is complex: this is performed from multiple heterogeneous sensor inputs and knowledge sources. Discretization of such inputs is done to capture saliences, represented as symbolic information, which often presents structure and recurrence. As these sequences are used to reason over complex scenarios [1], a more compact representation would aid exactness of technical cognitive reasoning capabilities, which are today constrained by computational complexity issues and fallback to representational heuristics or human intervention [1], [2]. Such problems need to be addressed to ensure timely and meaningful human-robot interaction.\nOur work is towards understanding the variability of learning informativeness when training on subsets of a given input dataset. This is in view of reducing the training size while retaining the majority of the symbolic learning potential. We prove the concept on human-written texts, and conjecture this work will reduce training data size of sequential instructions, while preserving semantic relations, when gathering information from large remote sources [3].\nPosterior Evaluation Distribution of Subsets\nWe computed multiple random subsets of sentences from the UMBC WEBBASE CORPUS (\u223c 17.13GB) via a custom implementation using the SPARK distributed framework. We evaluated the learning informativess of such sets in terms of semantic word-sense classification accuracy (with WORD2VEC [4]), and of n-gram perplexity. Previous literature inform us that corpus size and posterior quality do not follow linear correlation for some learning tasks (e.g. semantic measures) [5]. In our semantic tests, on average 85% of the quality can be obtained by training on a random \u223c 4% subset of the original corpus (e.g. as in Fig. 1, 5 random million lines yield 64.14% instead of 75.14%).\nOur claims are that i) such evaluation posteriors are Normally distributed (Tab. I), and that ii) the variance is inversely proportional to the subset size (Tab. II). It is therefore possible to select the best random subset for a given size, if an information criterion is known. Such metric is currently under investigation. Within the robotics domain, in order to reduce computational complexity of the training phase, cardinality reduction of human-written instructions is particularly important for non-recursive online training algorithms, such as current symbol-based probabilistic reasoning systems [1], [3], [6].\n1S.K. is with Ludwig Maximilian University of Munich, Germany simon.kaltenbacher@campus.lmu.de\n2N.H.K. and D.L. are with the Technical University of Munich, Germany {nicholas.kirk,dhlee}@tum.de"}, {"heading": "CHI-SQUARE AND ANDERSON-DARLING TESTS SHOWING THERE IS NO GAUSSIAN NULL HYPOTHESIS REJECTION FOR WORD2VEC AND PERPLEXITY ACCURACY VALUES OF RANDOM SUBSETS (10% SIGNIFICANCE LEVEL).", "text": ""}, {"heading": "RANDOM SUBSETS.", "text": ""}], "references": [{"title": "Online prediction of activities with structure: Exploiting contextual associations and sequences", "author": ["N.H. Kirk", "K. Ramirez-Amaro", "E. Dean-Leon", "M. Saveriano", "G. Cheng"], "venue": "2015 IEEE-RAS International Conference on Humanoid Robots, IEEE, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Controlled natural languages for language generation in artificial cognition", "author": ["N.H. Kirk", "D. Nyga", "M. Beetz"], "venue": "2014 IEEE International Conference on Robotics and Automation (ICRA), pp. 6667\u20136672, IEEE, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Understanding and executing instructions for everyday manipulation tasks from the world wide web", "author": ["M. Tenorth", "D. Nyga", "M. Beetz"], "venue": "2010 IEEE International Conference on Robotics and Automation (ICRA), pp. 1486\u20131491, IEEE, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pp. 3111\u20133119, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Scaling to very very large corpora for natural language disambiguation", "author": ["M. Banko", "E. Brill"], "venue": "Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, pp. 26\u201333, Association for Computational Linguistics, 2001.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Towards learning object affordance priors from technical texts", "author": ["N.H. Kirk"], "venue": "\u201dActive Learning in Robotics\u201d Workshop, 2014 IEEE-RAS International Conference on Humanoid Robots, IEEE, 2014.  ar  X  iv  :1  51 0.  04  10  4v  1 [  cs  .C  L  ]  2  8  Se  p  20  15", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "As these sequences are used to reason over complex scenarios [1], a more compact representation would aid exactness of technical cognitive reasoning capabilities, which are today constrained by computational complexity issues and fallback to representational heuristics or human intervention [1], [2].", "startOffset": 61, "endOffset": 64}, {"referenceID": 0, "context": "As these sequences are used to reason over complex scenarios [1], a more compact representation would aid exactness of technical cognitive reasoning capabilities, which are today constrained by computational complexity issues and fallback to representational heuristics or human intervention [1], [2].", "startOffset": 292, "endOffset": 295}, {"referenceID": 1, "context": "As these sequences are used to reason over complex scenarios [1], a more compact representation would aid exactness of technical cognitive reasoning capabilities, which are today constrained by computational complexity issues and fallback to representational heuristics or human intervention [1], [2].", "startOffset": 297, "endOffset": 300}, {"referenceID": 2, "context": "We prove the concept on human-written texts, and conjecture this work will reduce training data size of sequential instructions, while preserving semantic relations, when gathering information from large remote sources [3].", "startOffset": 219, "endOffset": 222}, {"referenceID": 3, "context": "We evaluated the learning informativess of such sets in terms of semantic word-sense classification accuracy (with WORD2VEC [4]), and of n-gram perplexity.", "startOffset": 124, "endOffset": 127}, {"referenceID": 4, "context": "semantic measures) [5].", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "Within the robotics domain, in order to reduce computational complexity of the training phase, cardinality reduction of human-written instructions is particularly important for non-recursive online training algorithms, such as current symbol-based probabilistic reasoning systems [1], [3], [6].", "startOffset": 280, "endOffset": 283}, {"referenceID": 2, "context": "Within the robotics domain, in order to reduce computational complexity of the training phase, cardinality reduction of human-written instructions is particularly important for non-recursive online training algorithms, such as current symbol-based probabilistic reasoning systems [1], [3], [6].", "startOffset": 285, "endOffset": 288}, {"referenceID": 5, "context": "Within the robotics domain, in order to reduce computational complexity of the training phase, cardinality reduction of human-written instructions is particularly important for non-recursive online training algorithms, such as current symbol-based probabilistic reasoning systems [1], [3], [6].", "startOffset": 290, "endOffset": 293}], "year": 2015, "abstractText": "Estimating the internal state of a robotic system is complex: this is performed from multiple heterogeneous sensor inputs and knowledge sources. Discretization of such inputs is done to capture saliences, represented as symbolic information, which often presents structure and recurrence. As these sequences are used to reason over complex scenarios [1], a more compact representation would aid exactness of technical cognitive reasoning capabilities, which are today constrained by computational complexity issues and fallback to representational heuristics or human intervention [1], [2]. Such problems need to be addressed to ensure timely and meaningful human-robot interaction. Our work is towards understanding the variability of learning informativeness when training on subsets of a given input dataset. This is in view of reducing the training size while retaining the majority of the symbolic learning potential. We prove the concept on human-written texts, and conjecture this work will reduce training data size of sequential instructions, while preserving semantic relations, when gathering information from large remote sources [3].", "creator": "LaTeX with hyperref package"}}}