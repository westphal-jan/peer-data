{"id": "1510.04709", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2015", "title": "Multilingual Image Description with Neural Sequence Models", "abstract": "In this paper we present an approach to multi-language image description bringing together insights from neural machine translation and neural image description. To create a description of an image for a given target language, our sequence generation models condition on feature vectors from the image, the description from the source language, and/or a multimodal vector computed over the image and a description in the source language. In image description experiments on the IAPR-TC12 dataset of images aligned with English and German sentences, we find significant and substantial improvements in BLEU4 and Meteor scores for models trained over multiple languages, compared to a monolingual baseline for only one encoding category (see Table 1 for details). The main difference between the two data sets is that compared to the monolingual baseline, the IAPR-TC12 dataset was trained with a monolingual baseline. The monolingual baseline was comparable to the monolingual baseline, and the monolingual baseline was equivalent to the monolingual baseline, but it was less significant. For example, while the monolingual baseline was significantly higher for a given language than the monolingual baseline, the monolingual baseline was even more significant for a given language than for a given language. As a result, there was a significant increase in the monolingual baseline to the monolingual baseline. This decrease occurred over a period of 2 weeks following the monolingual baseline. Furthermore, the monolingual baseline was significantly smaller in the monolingual baseline. However, these differences in monolingual baseline were inversely correlated with the monolingual baseline (see Table 1).\n\n\n\n\n\nThe present-day model for the monolingual and monolingual data set for the monolingual and monolingual dataset was the first to investigate the association between a model and a multilingual text representation in the IAPR-TC12 dataset. The similarity of both models was greater in the monolingual and monolingual data sets (see Table 1 for details). The similarity of the monolingual and monolingual data sets of the IAPR-TC12 dataset was also greater in the monolingual and monolingual datasets (see Table 1 for details). This similarity of the monolingual and monolingual data sets of the IAPR-TC12 dataset was also greater in the monolingual and monolingual datasets (see Table 1 for details). The similarity of the monolingual and monolingual data sets of the", "histories": [["v1", "Thu, 15 Oct 2015 20:29:21 GMT  (3153kb)", "http://arxiv.org/abs/1510.04709v1", null], ["v2", "Wed, 18 Nov 2015 17:04:35 GMT  (1036kb)", "http://arxiv.org/abs/1510.04709v2", "Under review as a conference paper at ICLR 2016"]], "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.LG cs.NE", "authors": ["desmond elliott", "stella frank", "eva hasler"], "accepted": false, "id": "1510.04709"}, "pdf": {"name": "1510.04709.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["d.elliott@uva.nl", "s.c.frank@uva.nl", "ech57@cam.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 0.\n04 70\n9v 1\n[ cs\n.C L\n] 1"}, {"heading": "1 INTRODUCTION", "text": "Automatic image description \u2014 the task of generating natural language sentences for an image \u2014 has thus far been exclusively performed in English, due to the availability of English datasets. However, the reasons for doing automatic image description, such as text-based image search or providing alt-texts for the visually impaired, also hold for other languages.\nCurrent image description models are not per se English-language specific, implying a straightforward approach to generating for a new language: collect new annotations and train a model for that language. However, the wealth of image description resources for English suggest a cross-language resource transfer approach, which is what we explore here. In other words: How can we best use resources for Language A when generating descriptions for Language B?\nIn this paper, we present a multilingual multimodal image description model. Multilingual image description can be viewed as a form of visually-grounded machine translation, in which parallel sentences are grounded against the image features. This grounding can be particularly useful when the source sentence contains ambiguities that need to be disambiguated in the target sentence. For example, Finnish pronouns are not gendered, so when translating \u201cha\u0308n\u201d to English, visual input could be vital to determine whether to generate \u201che\u201d or \u201cshe\u201d.\nOur multilingual multimodal language model extends a monolingual neural image description model (Vinyals et al., 2015) with features extracted from the source language, in an architecture similar to sequence-to-sequence machine translation (Sutskever et al., 2014). Figure 1 depicts the overall approach: a multimodal language model is used as a source language encoder. The representations learned by the source model are treated as fixed input, which acts as additional conditioning features\n\u2217Authors contributed equally to this paper.\nfor the multimodal target language decoder. We also study a number of sensible variants, such as using only linguistic source features in the encoder, or removing the image features in the decoder.\nIn a series of experiments on the IAPR-TC12 dataset of images described in English and German, we find that all models that use source features outperform monolingual image description models. The best English-language model improves upon the state-of-the-art by 2.3 BLEU4 points for this dataset. In the first results reported on German image description, our model achieves a 8.8 Meteor point improvement compared to a monolingual baseline. The implication is that linguistic and visual features offer orthogonal improvements in multi-modal modelling (a point also made by Silberer & Lapata (2014) and Kiela & Bottou (2014)). Models including visual features also improve over neural MT baselines, although to a lesser extent; this is likely due to the dataset being translations rather than independently elicited descriptions. Our analyses show that the additional features improve mainly lower-quality sentences, indicating that our best models successfully combine multiple noisy input modalities."}, {"heading": "2 MODELS", "text": "Our models are neural sequence generation models, with additional inputs from either visual or linguistic modalities, or both. We draw inspiration from the CNN-to-RNN decoding models (Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Mao et al., 2015), used for image description, as well as the sequence-to-sequence models (Sutskever et al., 2014) developed for MT. Our main modelling contribution is the connection between these architectures.\nWe present the models in sequence of increasing complexity to make their compositional character clear, beginning with a simple sequence model over words (a neural language model) and concluding with the full models using both image and source features. See Figure 2 for a depiction of the model architecture."}, {"heading": "2.1 RECURRENT LANGUAGE MODEL (LM)", "text": "The core of our model is a Recurrent Neural Network model over word sequences, i.e., a neural language model (LM). The model is trained to predict the next word in the sequence, given the current sequence seen so far. At each timestep i for input sequence w0...n, the input word wi, represented as a one-hot vector over the vocabulary, is embedded into a high-dimensional continuous vector using the learned embedding matrix E (Eqn 1). A nonlinear function f is applied to the embedding combined with the previous hidden state to generate the hidden state hi (Eqn 2). At the output layer, the next word oi is predicted via the softmax function over the vocabulary (Eqn 3). Throughout, W s are learned weight matrices, where the subscripts indicate which variables they\ncorrespond to.\nei = Ewi (1)\nhi = f(Whhhi\u22121 +Wheei) (2)\noi = softmax(tanh(hi)) (3)\nIn the simplest RNNs, f can be the tanh or sigmoid function. We use an LSTM to avoid problems with longer sequences (Hochreiter & Schmidhuber, 1997). The sentence is buffered at timestep 0 with a special beginning-of-sentence marker, and predicts a special end-of-sequence marker at timestep n. The first hidden state values h\u22121 are learned, together with the weights."}, {"heading": "2.2 MULTIMODAL LANGUAGE MODEL (MLM)", "text": "The recurrent LM above generates sequences of words conditioned only on the previously seen words, and thus cannot use visual input to do image description. In the multimodal language model (MLM), however, sequence generation is additionally conditioned on image features, leading to a model that generates word sequences corresponding to the image. The image features v (for visual) are input to the model at h0 at the first timestep:\nh0 = f(Whhh\u22121 +Whee0 +Whvv) (4)"}, {"heading": "2.3 TRANSLATION MODEL (SOURCE-LM \u2192 TARGET-LM)", "text": "Instead of conditioning on an image vector to generate a sentence in a given target language, we can condition on a vector representing a sentence in a different language. This vector s is the final hidden state extracted from a sequence model over the source language, the SOURCE-LM. This is essentially the sequence-to-sequence model presented in Sutskever et al. (2014), except that the encoder state is given as input to the model, rather than being used as a special initial h0 state. Thus the initial state for the TARGET-LM is now defined as:\nh0 = f(Whhh\u22121 +Whee0 +Whss) (5)"}, {"heading": "2.4 MULTIMODAL TRANSLATION MODEL (SOURCE-MLM \u2192 TARGET-MLM)", "text": "Finally, we can use both the image as well as source language features in a combined multimodal translation model. Image features can be added on both the source and the target side (SOURCEMLM \u2192 TARGET-MLM), or to only source or target (SOURCE-MLM \u2192 TARGET-LM or SOURCE-LM\na yellow building\nwith white columns in\nthe background\nein gelbes Geba\u0308ude\nmit wei\u00dfen Sa\u0308ulen im\nHintergrund\nFigure 3: Image 00/25 from the IAPR-TC12 dataset with its English and German description.\n\u2192 TARGET-MLM, respectively). The initial state of the TARGET-MLM, regardless of source model type, is:\nh0 = f(Whhh\u22121 +Whee0 +Whss+Whvv) (6)"}, {"heading": "2.5 GENERATING SENTENCES", "text": "We use the same process for generating from all of the proposed models. First, the model is initialised with the special beginning-of-sentence token and any visual or source features. The maximum probability word oi is sampled from the model output, and used as the input token at timestep i+1. This process continues until the model generates the end-of-sentence token, or a pre-defined number of timesteps (30)."}, {"heading": "3 METHODOLOGY", "text": ""}, {"heading": "3.1 DATA", "text": "We use the IAPR-TC12 dataset, originally introduced in the ImageCLEF shared task for object segmentation and later expanded with complete image descriptions (Grubinger et al., 2006). This dataset contains 20,000 images with multiple sentence descriptions in both English and German. We perform experiments using the standard splits of 17,665 images for training, from which we reserve 10% for hyperparameter estimation, and 1,962 for evaluation. We use only the first description of each image because in this dataset the different descriptions cover different aspects of the image. Note that the English descriptions are the originals; the German data was professionally translated from English. Figure 3 shows an example image-bitext tuple from the dataset.\nThe descriptions are lowercased and tokenised using the ptbtokenizer.py script from the MSCOCO evaluation tools1. All words in the training data observed fewer than 3 times are discarded. This leaves a total of 272,172 training tokens for English over a vocabulary of 1,763 types; and 223,137 tokens for German over 2,374 types.\nCompared to the commonly used Flickr8/30K or MS COCO datasets, the English descriptions in the IAPR-TC12 dataset are long, with an average length of 23 words. The lack of multiple reference descriptions also increases the difficulty of training models for image description."}, {"heading": "3.2 BASELINES", "text": "MLM: the first baseline is a monolingual image description model, i.e. multimodal language model with no source language features. The visual input consists of the CNN image features.\nSOURCE-LM \u2192TARGET-LM: the second baseline is a sequence-to-sequence Neural Machine Translation model, trained on only source and target descriptions without visual features. The final hidden state of the SOURCE-LM, after it has generated the source sentence, is input to the TARGET-LM, as described in Section 2.\n1 https://github.com/tylin/coco-caption"}, {"heading": "3.3 MULTILINGUAL MULTIMODAL MODEL VARIANTS", "text": "SOURCE-MLM \u2192TARGET-MLM: This model replaces the LMs in the MT baseline with multimodal language models on both source and target sides. This results in the image features being input twice, with two separately parameterised weight matrices.\nSOURCE-LM \u2192TARGET-MLM: In this model, the source language features are generated by a language-only LM; visual features are input only during target language generation.\nSOURCE-MLM \u2192TARGET-LM: Here, the visual input is given only to the SOURCE-MLM and the TARGET-LM uses a single input vector from the SOURCE-MLM. This source vector combines both linguistic and visual cues, to the extent that the visual features are being represented within the SOURCE-MLM\u2019s hidden layer."}, {"heading": "3.4 HYPERPARAMETERS", "text": "We use an LSTM (Hochreiter & Schmidhuber, 1997) as f in the recurrent language model. The hidden layer size |h| is set to 256 dimensions. The word embeddings are 256-dimensionsal and learned along with other model parameters. We also experimented with larger hidden layers (as well as with deeper architectures), and while that did result in improvements, they also took longer to train. The image features v are the 4096-dimension penultimate layer of the VGG-16 object recognition network (Simonyan & Zisserman, 2015), in line with recent work in this area."}, {"heading": "3.5 TRAINING AND OPTIMISATION", "text": "We trained the models towards the objective function (cross-entropy of the predicted words) using the ADAM optimiser (Kingma & Ba, 2014). It is known that language model perplexity is only weakly correlated with BLEU4 for MT (Auli et al., 2013), and that BLEU4 is only moderately correlated with human judgements for image description (Elliott & Keller, 2014). Here, we do early stopping for model selection based on BLEU4: if validation BLEU4 has not increased for 10 epochs, and validation language-model perplexity has stopped decreasing, training is halted.\nWe apply dropout over the image features, source features, and word representations with p = 0.5 to discourage overfitting (Srivastava et al., 2014). The objective function includes an L2 regularisation term with \u03bb=1e\u22128.\nAll results reported are averages over three runs with different Glorot-style uniform weight initialisations (Glorot & Bengio, 2010). We report image description quality using BLEU4 (Papineni et al., 2002), Meteor (Denkowski & Lavie, 2014), and language-model perplexity. The BLEU4 and Meteor scores are calculated using MultEval (Clark et al., 2011)."}, {"heading": "4 RESULTS", "text": "The results for image description in both German and English are presented in Tables 1 and 2. Example output can be seen in Figure 42. To our knowledge, these are the first published results for German image description. Overall, we note that image description in German results in lower BLEU4 and Meteor scores than English. This is most likely due to the more complex German morphology, which results in a larger vocabulary.\nThe baseline monolingual multimodal result for English (En-MLM) is in line to other state-of-theart models, which report results on the Flickr8K dataset. We obtain a BLEU4 score of 15.8 on the Flickr8K dataset, comparable to the 16.0 from Karpathy & Fei-Fei (2015), which uses an ensemble of models and beam search decoding. On the IAPR-TC12 dataset, the En MLM performs worse than Mao et al. (2015), but their model is trained and evaluated on all reference descriptions.\nAll multilingual models beat the monolingual image description baseline, by a margin of up to 8.9 BLEU4 and 8.8 Meteor points for the best models. The best model variant outperforms Mao et al. (2015) by 2.3 BLEU4. Recall that in this translation-type setting, the source features are produced from gold descriptions. Clearly this results in extremely useful features for the TARGET-LM or TARGET-MLM description generator, despite the switch in languages.\nThe neural MT baseline without visual features performs very well3. This indicates the effectiveness of the overall sequence-to-sequence approach, but is also an artifact of the dataset. A different dataset with independently elicited image descriptions (rather than translations of English descriptions) may result in worse performance for a pure MT system that is not visually grounded.\nOverall, the multilingual models that use a MLM to encode the source sentence outperform the SOURCE-LM models. On the target side, simple LM models perform better. To an extent this can be explained by the smaller number of parameters used in models that do not input the visual features twice, and it seems to be easier to incorporate the visual features on the source side, with fixed gold input. More generally, the MLM variants tend to be worse sentence generators than the LM models, indicating that while visual features lead to useful hidden state values, there is room for improving their role during generation."}, {"heading": "5 DISCUSSION", "text": "Why is it useful to condition on source language features when generating descriptions? The initial step of a monolingual image description model is to \u2018compress\u2019 the image vector into the same number of dimensions as the hidden layer in the recurrent network. This can be thought of as distilling the image down to the essential features that correspond to the words in the description. If this step of the model is prone to mistakes, the resulting descriptions will be of poor quality. A model\n2Visit https://staff.fnwi.uva.nl/d.elliott/GroundedTranslation/ to see the descriptions generated by each model variant for the validation data.\n3The BLEU4 and Meteor scores in Table 2 for En LM \u2192 De LM and En MLM \u2192 De LM are not significantly different according to the MultEval approximate randomization significance test.\nthat is also conditioned on a source feature vector have additional, and hopefully useful, evidence about how to describe the image.\nQualitatively, we can find evidence for this explanation using Barnes-Hut t-SNE projections of the initial hidden representations (van der Maaten, 2014). Figure 5 compares the t-SNE projection of an example from Figure 4 using a En MLM (left) and a De MLM \u2192 En MLM (right). In the monolingual example, the nearest neighbours of the target image are desert scenes with groups of people. If we condition on the source multimodalfeatures, the nearest neighbours are mountainous snowy images.\nWhen does it help to add a source description or an image? Figure 6 shows that the second input type (language or image features) is most helpful when the baseline model yields low quality descriptions. The additional modality adds useful information, resulting in increased sentence-BLEU4 scores. On the other hand, the additional input can also lower the quality of an already good baseline description. This risk seems to be higher when adding an image than when adding source language features, which is unsurprising given the larger distance between visual and linguistic space, versus moving from one language to another. This is also consistent with the lower performance of MLM baseline models compared to LM\u2192LM models.\nSimilar analysis of the lower performing LM\u2192MLM model (not shown) shows similar behaviour to the MLM\u2192LM model above: the sentences with lower BLEU4 baseline scores (the vast majority) are improved by the additional features. However, for this model the decreasing performance starts earlier: the LM\u2192MLM model is unable (on average) to improve sentences with lower baseline BLEU4 scores, particularly with regard to the LM\u2192LM baseline. Adding the image features at the source side, rather than the target side, seems to filter out some of the noise and complexity of the image features, while the essential source language features are retained. Conversely, merging the source language features with image features on the target side, in the TARGET-MLM models, leads to a less helpful entangling of linguistic and (noisier) image input, which can\u2019t be learned sufficiently well on our small dataset.\nThe examples in Figure 4 demonstrate that image and word features can also contribute additively to a good image description. The second picture showing people climbing up a snowface is described by the MLM model as grey rock but the MLM \u2192 MLM model correctly describes the scene type, based on the features extracted from the German example. In the third image, only the MLM \u2192 LM and LM \u2192 LM models produce accurate descriptions. The German descriptions are always good in this instance, but the English descriptions benefit from the extra source features."}, {"heading": "6 RELATED WORK", "text": "The past few years have seen numerous results showing how relatively standard neural network model architectures can be applied to a variety of tasks. The flexibility of the application of these architectures can be seen as a strong point, indicating that the representations learned in these general models are sufficiently powerful to lead to good performance. Another advantage, which we have exploited in the work presented here, is that it becomes relatively straightforward to make connections between models for different tasks, in this case image description and machine translation.\nDeep neural networks for automatic image description typically estimate a joint image-sentence representation in a multimodal recurrent neural network (RNN) (Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Mao et al., 2015). The main difference between these models and discrete tuple-based representations for image description (Farhadi et al., 2010; Yang et al., 2011; Li et al., 2011; Mitchell et al., 2012; Elliott & Keller, 2013; Yatskar et al., 2014; Elliott & de Vries, 2015) is that it is not necessary to explicitly define the joint representation; the structure of the neural network can be used to estimate the optimal joint representation for the description task. As in our MLM, the image\u2013sentence representation in the multimodal RNN is initialised with image features from the penultimate fully-connected layer of a convolutional neural network (CNN) trained for multi-class object recognition (Krizhevsky et al., 2012). Alternative formulations input the image features into the model at each timestep (Mao et al., 2015), or first detect the objects in an image and generate sentences using a maximum-entropy language model (Fang et al., 2015).\nIn the domain of machine translation, a greater variety of neural models have been used for subtasks within the MT pipeline, such as neural network language models (Schwenk, 2012) or joint translation and language models for re-ranking in phrase-based translation models (Le et al., 2012; Auli et al., 2013) or directly during decoding (Devlin et al., 2014).. More recently, end-to-end neural MT systems using Long Short-Term Memory Networks and Gated Recurrent Units have been proposed as Encoder-Decoder models for translation (Sutskever et al., 2014; Bahdanau et al., 2015), and have proven to be highly effective (Bojar et al., 2015; Jean et al., 2015).\nIn the multi-modal modelling literature, there are related approaches using visual and textual information to build representations for word similarity and word categorization tasks (Silberer & Lapata, 2014; Kiela & Bottou, 2014). Silberer & Lapata combine textual and visual modalities by jointly training stacked autoencoders, while Kiela & Bottou construct multi-modal representations by concatenating distributed linguistic and visual feature vectors. In both cases, the results show that the bimodal representations are superior to their unimodal counterparts."}, {"heading": "7 CONCLUSIONS", "text": "In this paper we studied multi-language image description, the task of generating descriptions of an image given a corpus of parallel texts. The main difference between this task and monolingual image description is the availability of references in more than one language compared to multiple reference descriptions.\nImage description datasets typically include multiple reference sentences, which are essential for capturing linguistic diversity (Rashtchian et al., 2010; Elliott & Keller, 2013; Hodosh et al., 2013; Chen et al., 2015). In our experiments, we have shown that useful image description diversity can be found in other languages instead of in multiple monolingual references. It is now an open question whether the benefits of multiple monolingual references extend to multiple multi-language references.\nOur approach to exploiting multiple-language training data essentially acts as an encoder-decoder model. The encoder captures a multimodal representation of the image and the source-language words, which is used as an additional conditioning vector for the decoder, which produces descriptions in the target language. This type of model significantly improves the quality of the descriptions in both directions compared to monolingual baselines.\nThe dataset used in this paper consists of translated descriptions, leading to high performance for the MT baseline. However, we believe that multilingual image description should be based on independently elicited descriptions in multiple languages, rather than literal translations. Linguistic and cultural differences may lead to very different descriptions being appropriate for different languages, in which case image features will provide stronger cues.\nIn the future, we plan to reframe the learning algorithm as a complete joint model over the image\u2013 source\u2013target language, in order to update the source-language representations based on what we learn in the target language. It would also be interesting to explore attention-based recurrent neural networks, which have recently been used for machine translation (Bahdanau et al., 2015; Jean et al., 2015) and image description (Xu et al., 2015). We would like to apply these models to other language pairs, such as the recently released PASCAL 1K Japanese Translations dataset containing professionally translated texts (Funaki & Nakayama, 2015). Lastly, applying these types of models to a multi-language video description dataset (Chen & Dolan, 2011) would be an exciting challenge."}, {"heading": "ACKNOWLEDGMENTS", "text": "D. Elliott was supported by an Alain Bensoussain Career Development Fellowship. S. Frank is supported by funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement Nr. 645452.\nWe thank Philip Schulz and Khalil Sima\u2019an for discussions and feedback on the work. We built the models using the Keras library, which is built on-top of Theano. We are grateful to the Database Architectures Group at CWI for access to their K20x GPUs."}], "references": [{"title": "Joint language and translation modeling with recurrent neural networks", "author": ["Auli", "Michael", "Galley", "Michel", "Quirk", "Chris", "Zweig", "Geoffrey"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Auli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Proceedings of ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Findings of the 2015 workshop on statistical machine translation", "author": ["Bojar", "Ond\u0159ej", "Chatterjee", "Rajen", "Federmann", "Christian", "Haddow", "Barry", "Huck", "Matthias", "Hokamp", "Chris", "Koehn", "Philipp", "Logacheva", "Varvara", "Monz", "Christof", "Negri", "Matteo", "Post", "Matt", "Scarton", "Carolina", "Specia", "Lucia", "Turchi", "Marco"], "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation,", "citeRegEx": "Bojar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2015}, {"title": "Building a persistent workforce on Mechanical Turk for multilingual data collection", "author": ["Chen", "David L", "Dolan", "William B"], "venue": "In Proceedings of The 3rd Human Computation Workshop (HCOMP", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Microsoft COCO captions: Data collection and evaluation", "author": ["Chen", "Xinlei", "Fang", "Hao", "Lin", "Tsung-Yi", "Vedantam", "Ramakrishna", "Gupta", "Saurabh", "Doll\u00e1r", "Piotr", "Zitnick", "C. Lawrence"], "venue": "server. CoRR,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Better hypothesis testing for statistical machine translation: Controlling for optimizer instability", "author": ["Clark", "Jonathan H", "Dyer", "Chris", "Lavie", "Alon", "Smith", "Noah A"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume", "citeRegEx": "Clark et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2011}, {"title": "Meteor Universal: Language Specific Translation Evaluation for Any Target Language", "author": ["M. Denkowski", "A. Lavie"], "venue": "In EACL Workshop on Statistical Machine Translation,", "citeRegEx": "Denkowski and Lavie,? \\Q2014\\E", "shortCiteRegEx": "Denkowski and Lavie", "year": 2014}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin", "Jacob", "Zbib", "Rabih", "Huang", "Zhongqiang", "Lamar", "Thomas", "Schwartz", "Richard", "Makhoul", "John"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CoRR, abs/1411.4389,", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Describing images using inferred visual dependency representations", "author": ["Elliott", "Desmond", "de Vries", "Arjen P"], "venue": "In ACL,", "citeRegEx": "Elliott et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2015}, {"title": "Image Description using Visual Dependency Representations", "author": ["Elliott", "Desmond", "Keller", "Frank"], "venue": "In EMNLP,", "citeRegEx": "Elliott et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2013}, {"title": "Comparing Automatic Evaluation Measures for Image Description", "author": ["Elliott", "Desmond", "Keller", "Frank"], "venue": "In ACL,", "citeRegEx": "Elliott et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2014}, {"title": "From captions to visual concepts and back", "author": ["Fang", "Hao", "Gupta", "Saurabh", "Iandola", "Forrest", "Srivastava", "Rupesh K", "Deng", "Li", "Dollar", "Piotr", "Gao", "Jianfeng", "He", "Xiaodong", "Mitchell", "Margaret", "Platt", "John C", "C. Lawrence Zitnick", "Zweig", "Geoffrey"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Fang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fang et al\\.", "year": 2015}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["Farhadi", "Ali", "M Hejrati", "Sadeghi", "Mohammad Amin", "P Young", "C Rashtchian", "J Hockenmaier", "Forsyth", "David"], "venue": "In ECCV,", "citeRegEx": "Farhadi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Farhadi et al\\.", "year": 2010}, {"title": "Image-mediated learning for zero-shot cross-lingual document retrieval", "author": ["Funaki", "Ruka", "Nakayama", "Hideki"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "Funaki et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Funaki et al\\.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "The IAPR TC-12 benchmark: A new evaluation resource for visual information systems", "author": ["M. Grubinger", "P.D. Clough", "H. Muller", "D. Thomas"], "venue": "In LREC,", "citeRegEx": "Grubinger et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Grubinger et al\\.", "year": 2006}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics", "author": ["Hodosh", "Micah", "P Young", "J. Hockenmaier"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hodosh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Montreal neural machine translation systems for wmt\u201915", "author": ["Jean", "S\u00e9bastien", "Firat", "Orhan", "Cho", "Kyunghyun", "Memisevic", "Roland", "Bengio", "Yoshua"], "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation,", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Andrej", "Fei-Fei", "Li"], "venue": null, "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-Modal Semantics", "author": ["Kiela", "Douwe", "Bottou", "L\u00e9on"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-14),", "citeRegEx": "Kiela et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Multimodal neural language models", "author": ["Kiros", "Ryan", "Salakhutdinov", "Ruslan", "Zemel", "Rich"], "venue": "In ICML, pp", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Continuous space translation models with neural networks", "author": ["Le", "Hai-Son", "Allauzen", "Alexandre", "Yvon", "Francois"], "venue": "In Proceedings of NAACL HLT,", "citeRegEx": "Le et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Le et al\\.", "year": 2012}, {"title": "Composing simple image descriptions using web-scale n-grams", "author": ["S Li", "G Kulkarni", "T L Berg", "A C Berg", "Choi", "Y Young"], "venue": "In CoNLL,", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Deep captioning with multimodal recurrent neural networks (m-RNN)", "author": ["Mao", "Junhua", "Xu", "Wei", "Yang", "Yi", "Wang", "Jiang", "Huang", "Zhiheng", "Yuille", "Alan"], "venue": null, "citeRegEx": "Mao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Midge: generating image descriptions from computer vision detections", "author": ["Mitchell", "Margaret", "Han", "Xufeng", "Dodge", "Jesse", "Mensch", "Alyssa", "Goyal", "Amit", "A C Berg", "Yamaguchi", "Kota", "T L Berg", "Stratos", "Karl", "III Daume", "Hal", "III"], "venue": "In EACL,", "citeRegEx": "Mitchell et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2012}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "Zhu", "W.-J"], "venue": "In ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Collecting image annotations using Amazon\u2019s Mechanical Turk", "author": ["C. Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier"], "venue": "In NAACLHLT Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk,", "citeRegEx": "Rashtchian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rashtchian et al\\.", "year": 2010}, {"title": "Continuous space translation models for phrase-based statistical machine translation", "author": ["Schwenk", "Holger"], "venue": "Proceedings of COLING (Posters),", "citeRegEx": "Schwenk and Holger.,? \\Q2012\\E", "shortCiteRegEx": "Schwenk and Holger.", "year": 2012}, {"title": "Learning grounded meaning representations with autoencoders", "author": ["Silberer", "Carina", "Lapata", "Mirella"], "venue": "In Proceedings of ACL,", "citeRegEx": "Silberer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silberer et al\\.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In ICLR \u201915,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V. V"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Accelerating t-sne using tree-based algorithms", "author": ["L.J.P. van der Maaten"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten,? \\Q2014\\E", "shortCiteRegEx": "Maaten", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Cho", "Kyunghyun", "Courville", "Aaron C", "Salakhutdinov", "Ruslan", "Zemel", "Richard S", "Bengio", "Yoshua"], "venue": "CoRR, abs/1502.03044,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Corpus-guided sentence generation of natural images", "author": ["Y Yang", "C L Teo", "III Daume", "Hal", "Y. Aloimonos"], "venue": "In EMNLP,", "citeRegEx": "Yang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2011}, {"title": "See no evil, say no evil: Description generation from densely labeled", "author": ["M Yatskar", "L Vanderwende", "L. Zettlemoyer"], "venue": "images. SEM,", "citeRegEx": "Yatskar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yatskar et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 37, "context": "Our multilingual multimodal language model extends a monolingual neural image description model (Vinyals et al., 2015) with features extracted from the source language, in an architecture similar to sequence-to-sequence machine translation (Sutskever et al.", "startOffset": 96, "endOffset": 118}, {"referenceID": 35, "context": ", 2015) with features extracted from the source language, in an architecture similar to sequence-to-sequence machine translation (Sutskever et al., 2014).", "startOffset": 129, "endOffset": 153}, {"referenceID": 23, "context": "We draw inspiration from the CNN-to-RNN decoding models (Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Mao et al., 2015), used for image description, as well as the sequence-to-sequence models (Sutskever et al.", "startOffset": 56, "endOffset": 164}, {"referenceID": 8, "context": "We draw inspiration from the CNN-to-RNN decoding models (Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Mao et al., 2015), used for image description, as well as the sequence-to-sequence models (Sutskever et al.", "startOffset": 56, "endOffset": 164}, {"referenceID": 37, "context": "We draw inspiration from the CNN-to-RNN decoding models (Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Mao et al., 2015), used for image description, as well as the sequence-to-sequence models (Sutskever et al.", "startOffset": 56, "endOffset": 164}, {"referenceID": 27, "context": "We draw inspiration from the CNN-to-RNN decoding models (Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Mao et al., 2015), used for image description, as well as the sequence-to-sequence models (Sutskever et al.", "startOffset": 56, "endOffset": 164}, {"referenceID": 35, "context": ", 2015), used for image description, as well as the sequence-to-sequence models (Sutskever et al., 2014) developed for MT.", "startOffset": 80, "endOffset": 104}, {"referenceID": 35, "context": "This is essentially the sequence-to-sequence model presented in Sutskever et al. (2014), except that the encoder state is given as input to the model, rather than being used as a special initial h0 state.", "startOffset": 64, "endOffset": 88}, {"referenceID": 16, "context": "We use the IAPR-TC12 dataset, originally introduced in the ImageCLEF shared task for object segmentation and later expanded with complete image descriptions (Grubinger et al., 2006).", "startOffset": 157, "endOffset": 181}, {"referenceID": 27, "context": "1 Mao et al. (2015) 20.", "startOffset": 2, "endOffset": 20}, {"referenceID": 0, "context": "It is known that language model perplexity is only weakly correlated with BLEU4 for MT (Auli et al., 2013), and that BLEU4 is only moderately correlated with human judgements for image description (Elliott & Keller, 2014).", "startOffset": 87, "endOffset": 106}, {"referenceID": 34, "context": "5 to discourage overfitting (Srivastava et al., 2014).", "startOffset": 28, "endOffset": 53}, {"referenceID": 29, "context": "We report image description quality using BLEU4 (Papineni et al., 2002), Meteor (Denkowski & Lavie, 2014), and language-model perplexity.", "startOffset": 48, "endOffset": 71}, {"referenceID": 5, "context": "The BLEU4 and Meteor scores are calculated using MultEval (Clark et al., 2011).", "startOffset": 58, "endOffset": 78}, {"referenceID": 27, "context": "On the IAPR-TC12 dataset, the En MLM performs worse than Mao et al. (2015), but their model is trained and evaluated on all reference descriptions.", "startOffset": 57, "endOffset": 75}, {"referenceID": 27, "context": "On the IAPR-TC12 dataset, the En MLM performs worse than Mao et al. (2015), but their model is trained and evaluated on all reference descriptions. All multilingual models beat the monolingual image description baseline, by a margin of up to 8.9 BLEU4 and 8.8 Meteor points for the best models. The best model variant outperforms Mao et al. (2015) by 2.", "startOffset": 57, "endOffset": 348}, {"referenceID": 23, "context": "Deep neural networks for automatic image description typically estimate a joint image-sentence representation in a multimodal recurrent neural network (RNN) (Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Mao et al., 2015).", "startOffset": 157, "endOffset": 265}, {"referenceID": 8, "context": "Deep neural networks for automatic image description typically estimate a joint image-sentence representation in a multimodal recurrent neural network (RNN) (Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Mao et al., 2015).", "startOffset": 157, "endOffset": 265}, {"referenceID": 37, "context": "Deep neural networks for automatic image description typically estimate a joint image-sentence representation in a multimodal recurrent neural network (RNN) (Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Mao et al., 2015).", "startOffset": 157, "endOffset": 265}, {"referenceID": 27, "context": "Deep neural networks for automatic image description typically estimate a joint image-sentence representation in a multimodal recurrent neural network (RNN) (Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Mao et al., 2015).", "startOffset": 157, "endOffset": 265}, {"referenceID": 13, "context": "The main difference between these models and discrete tuple-based representations for image description (Farhadi et al., 2010; Yang et al., 2011; Li et al., 2011; Mitchell et al., 2012; Elliott & Keller, 2013; Yatskar et al., 2014; Elliott & de Vries, 2015) is that it is not necessary to explicitly define the joint representation; the structure of the neural network can be used to estimate the optimal joint representation for the description task.", "startOffset": 104, "endOffset": 257}, {"referenceID": 39, "context": "The main difference between these models and discrete tuple-based representations for image description (Farhadi et al., 2010; Yang et al., 2011; Li et al., 2011; Mitchell et al., 2012; Elliott & Keller, 2013; Yatskar et al., 2014; Elliott & de Vries, 2015) is that it is not necessary to explicitly define the joint representation; the structure of the neural network can be used to estimate the optimal joint representation for the description task.", "startOffset": 104, "endOffset": 257}, {"referenceID": 26, "context": "The main difference between these models and discrete tuple-based representations for image description (Farhadi et al., 2010; Yang et al., 2011; Li et al., 2011; Mitchell et al., 2012; Elliott & Keller, 2013; Yatskar et al., 2014; Elliott & de Vries, 2015) is that it is not necessary to explicitly define the joint representation; the structure of the neural network can be used to estimate the optimal joint representation for the description task.", "startOffset": 104, "endOffset": 257}, {"referenceID": 28, "context": "The main difference between these models and discrete tuple-based representations for image description (Farhadi et al., 2010; Yang et al., 2011; Li et al., 2011; Mitchell et al., 2012; Elliott & Keller, 2013; Yatskar et al., 2014; Elliott & de Vries, 2015) is that it is not necessary to explicitly define the joint representation; the structure of the neural network can be used to estimate the optimal joint representation for the description task.", "startOffset": 104, "endOffset": 257}, {"referenceID": 40, "context": "The main difference between these models and discrete tuple-based representations for image description (Farhadi et al., 2010; Yang et al., 2011; Li et al., 2011; Mitchell et al., 2012; Elliott & Keller, 2013; Yatskar et al., 2014; Elliott & de Vries, 2015) is that it is not necessary to explicitly define the joint representation; the structure of the neural network can be used to estimate the optimal joint representation for the description task.", "startOffset": 104, "endOffset": 257}, {"referenceID": 24, "context": "As in our MLM, the image\u2013sentence representation in the multimodal RNN is initialised with image features from the penultimate fully-connected layer of a convolutional neural network (CNN) trained for multi-class object recognition (Krizhevsky et al., 2012).", "startOffset": 232, "endOffset": 257}, {"referenceID": 27, "context": "Alternative formulations input the image features into the model at each timestep (Mao et al., 2015), or first detect the objects in an image and generate sentences using a maximum-entropy language model (Fang et al.", "startOffset": 82, "endOffset": 100}, {"referenceID": 12, "context": ", 2015), or first detect the objects in an image and generate sentences using a maximum-entropy language model (Fang et al., 2015).", "startOffset": 111, "endOffset": 130}, {"referenceID": 25, "context": "In the domain of machine translation, a greater variety of neural models have been used for subtasks within the MT pipeline, such as neural network language models (Schwenk, 2012) or joint translation and language models for re-ranking in phrase-based translation models (Le et al., 2012; Auli et al., 2013) or directly during decoding (Devlin et al.", "startOffset": 271, "endOffset": 307}, {"referenceID": 0, "context": "In the domain of machine translation, a greater variety of neural models have been used for subtasks within the MT pipeline, such as neural network language models (Schwenk, 2012) or joint translation and language models for re-ranking in phrase-based translation models (Le et al., 2012; Auli et al., 2013) or directly during decoding (Devlin et al.", "startOffset": 271, "endOffset": 307}, {"referenceID": 7, "context": ", 2013) or directly during decoding (Devlin et al., 2014).", "startOffset": 36, "endOffset": 57}, {"referenceID": 35, "context": "More recently, end-to-end neural MT systems using Long Short-Term Memory Networks and Gated Recurrent Units have been proposed as Encoder-Decoder models for translation (Sutskever et al., 2014; Bahdanau et al., 2015), and have proven to be highly effective (Bojar et al.", "startOffset": 169, "endOffset": 216}, {"referenceID": 1, "context": "More recently, end-to-end neural MT systems using Long Short-Term Memory Networks and Gated Recurrent Units have been proposed as Encoder-Decoder models for translation (Sutskever et al., 2014; Bahdanau et al., 2015), and have proven to be highly effective (Bojar et al.", "startOffset": 169, "endOffset": 216}, {"referenceID": 2, "context": ", 2015), and have proven to be highly effective (Bojar et al., 2015; Jean et al., 2015).", "startOffset": 48, "endOffset": 87}, {"referenceID": 19, "context": ", 2015), and have proven to be highly effective (Bojar et al., 2015; Jean et al., 2015).", "startOffset": 48, "endOffset": 87}, {"referenceID": 30, "context": "Image description datasets typically include multiple reference sentences, which are essential for capturing linguistic diversity (Rashtchian et al., 2010; Elliott & Keller, 2013; Hodosh et al., 2013; Chen et al., 2015).", "startOffset": 130, "endOffset": 219}, {"referenceID": 18, "context": "Image description datasets typically include multiple reference sentences, which are essential for capturing linguistic diversity (Rashtchian et al., 2010; Elliott & Keller, 2013; Hodosh et al., 2013; Chen et al., 2015).", "startOffset": 130, "endOffset": 219}, {"referenceID": 4, "context": "Image description datasets typically include multiple reference sentences, which are essential for capturing linguistic diversity (Rashtchian et al., 2010; Elliott & Keller, 2013; Hodosh et al., 2013; Chen et al., 2015).", "startOffset": 130, "endOffset": 219}, {"referenceID": 1, "context": "It would also be interesting to explore attention-based recurrent neural networks, which have recently been used for machine translation (Bahdanau et al., 2015; Jean et al., 2015) and image description (Xu et al.", "startOffset": 137, "endOffset": 179}, {"referenceID": 19, "context": "It would also be interesting to explore attention-based recurrent neural networks, which have recently been used for machine translation (Bahdanau et al., 2015; Jean et al., 2015) and image description (Xu et al.", "startOffset": 137, "endOffset": 179}, {"referenceID": 38, "context": ", 2015) and image description (Xu et al., 2015).", "startOffset": 30, "endOffset": 47}], "year": 2017, "abstractText": "In this paper we present an approach to multi-language image description bringing together insights from neural machine translation and neural image description. To create a description of an image for a given target language, our sequence generation models condition on feature vectors from the image, the description from the source language, and/or a multimodal vector computed over the image and a description in the source language. In image description experiments on the IAPR-TC12 dataset of images aligned with English and German sentences, we find significant and substantial improvements in BLEU4 and Meteor scores for models trained over multiple languages, compared to a monolingual baseline.", "creator": "LaTeX with hyperref package"}}}