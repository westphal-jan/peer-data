{"id": "1510.05970", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2015", "title": "Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches", "abstract": "We present a method for extracting depth information from a rectified image pair. Our approach focuses on the first stage of many stereo algorithms: the matching cost computation. We approach the problem by learning a similarity measure on small image patches using a convolutional neural network. This technique allows us to estimate the average difference in a stereo image pair for each position, thus showing how a stereoscopic image pair is performed and which regions to classify as best fit. Finally, we also demonstrate the use of a number of dynamic gradient learning algorithms for the image-related problems. We have used these algorithms for large-scale stereo filtering algorithms for image analysis. This method allows us to estimate the average difference in a stereo image pair for each position, thus showing how a stereo image pair is performed and which regions to classify as best fit.", "histories": [["v1", "Tue, 20 Oct 2015 17:15:05 GMT  (2585kb,D)", "http://arxiv.org/abs/1510.05970v1", "Submitted to the Journal of Machine Learning Research"], ["v2", "Wed, 18 May 2016 19:53:41 GMT  (2591kb,D)", "http://arxiv.org/abs/1510.05970v2", null]], "COMMENTS": "Submitted to the Journal of Machine Learning Research", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["jure \\v{z}bontar", "yann lecun"], "accepted": false, "id": "1510.05970"}, "pdf": {"name": "1510.05970.pdf", "metadata": {"source": "CRF", "title": "Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches", "authors": ["Jure \u017dbontar", "Yann LeCun"], "emails": ["ZBONTAR@CS.NYU.EDU", "YANN@CS.NYU.EDU"], "sections": [{"heading": "1. Introduction", "text": "Consider the following problem: given two images taken from cameras at different horizontal positions, we wish to compute the disparity d for each pixel in the left image. Disparity refers to the difference in horizontal location of an object in the left and right image\u2014an object at position (x, y) in the left image appears at position (x \u2212 d, y) in the right image. If we know the disparity of an object we can compute its depth z using the following relation:\nz = fB\nd ,\nwhere f is the focal length of the camera and B is the distance between the camera centers. Figure 1 depicts the input to and the output from our method.\nThe described problem of stereo matching is important in many fields such as autonomous driving, robotics, intermediate view generation, and 3D scene reconstruction. According to the taxonomy of Scharstein and Szeliski (2002), a typical stereo algorithm consists of four steps: matching\n\u2217. Jure Z\u030cbontar is also with the Faculty of Computer and Information Science, University of Ljubljana, Vec\u030cna pot 113, SI-1001 Ljubljana, Slovenia. \u2020. Yann LeCun is also with Facebook AI Research, 770 Broadway, New York, NY 10003, USA.\nar X\niv :1\n51 0.\n05 97\n0v 1\n[ cs\n.C V\n] 2\n0 O\nct 2\ncost computation, cost aggregation, optimization, and disparity refinement. Following Hirschmu\u0308ller and Scharstein (2009) we refer to the first two steps as computing the matching cost and the last two steps as the stereo method. The focus of this work is on computing a good matching cost.\nWe propose training a convolutional neural network (LeCun et al., 1998) on pairs of small image patches where the true disparity is known (for example, obtained by a LIDAR sensor or structured light). The output of the network is used to initialize the matching cost. We proceed with a number of post-processing steps that are not novel, but are necessary to achieve good results. Matching costs are combined between neighboring pixels with similar image intensities using cross-based cost aggregation. Smoothness constraints are enforced by semiglobal matching, and a left-right consistency check is used to detect and eliminate errors in occluded regions. We perform subpixel enhancement and apply a median filter and a bilateral filter to obtain the final disparity map.\nThe contributions of this paper are\n\u2022 a description of two architectures based on convolutional neural networks for computing the stereo matching cost;\n\u2022 a method, accompanied by its source code, with the lowest error rate on the KITTI 2012, KITTI 2015, and Middlebury stereo data sets; and\n\u2022 experiments analyzing the importance of data set size, the error rate compared with other methods, and the trade-off between accuracy and runtime for different settings of the hyperparameters.\nThis paper extends our previous work (Z\u030cbontar and LeCun, 2015) by including a description of a new architecture, results on two new data sets, lower error rates, and more thorough experiments."}, {"heading": "2. Related Work", "text": "Before the introduction of large stereo data sets like KITTI and Middlebury, relatively few stereo algorithms used ground truth information to learn parameters of their models; in this section, we\nreview the ones that did. For a general overview of stereo algorithms see Scharstein and Szeliski (2002).\nKong and Tao (2004) used the sum of squared distances to compute an initial matching cost. They then trained a model to predict the probability distribution over three classes: the initial disparity is correct, the initial disparity is incorrect due to fattening of a foreground object, and the initial disparity is incorrect due to other reasons. The predicted probabilities were used to adjust the initial matching cost. Kong and Tao (2006) later extend their work by combining predictions obtained by computing normalized cross-correlation over different window sizes and centers. Peris et al. (2012) initialized the matching cost with AD-Census (Mei et al., 2011), and used multiclass linear discriminant analysis to learn a mapping from the computed matching cost to the final disparity.\nGround-truth data was also used to learn parameters of probabilistic graphical models. Zhang and Seitz (2007) used an alternative optimization algorithm to estimate optimal values of Markov random field hyperparameters. Scharstein and Pal (2007) constructed a new data set of 30 stereo pairs and used it to learn parameters of a conditional random field. Li and Huttenlocher (2008) presented a conditional random field model with a non-parametric cost function and used a structured support vector machine to learn the model parameters.\nRecent work (Haeusler et al., 2013; Spyropoulos et al., 2014) focused on estimating the confidence of the computed matching cost. Haeusler et al. (2013) used a random forest classifier to combine several confidence measures. Similarly, Spyropoulos et al. (2014) trained a random forest classifier to predict the confidence of the matching cost and used the predictions as soft constraints in a Markov random field to decrease the error of the stereo method.\nA related problem to computing the matching cost is learning local image descriptors (Brown et al., 2011; Trzcinski et al., 2012; Simonyan et al., 2014; Han et al., 2015; Fischer et al., 2015). The two problems share a common subtask: to measure the similarity between image patches. Brown et al. (2011) introduced a general framework for learning image descriptors and used Powell\u2019s method to select good hyperparameters. Several methods have been suggested for solving the problem of learning local image descriptors, such as boosting (Trzcinski et al., 2012), convex optimization (Simonyan et al., 2014), and convolutional neural networks (Zagoruyko and Komodakis, 2015; Han et al., 2015). Works of Zagoruyko and Komodakis (2015) and Han et al. (2015), in particlar, are very similar to our own, differing mostly in the architecture of the network; concretely, the inclusion of pooling and subsampling to account for larger patch sizes and larger variation in viewpoint."}, {"heading": "3. Matching Cost", "text": "A typical stereo algorithm begins by computing a matching cost at each position p for all disparities d under consideration. A simple method for computing the matching cost is the sum of absolute differences:\nCSAD(p, d) = \u2211\nq\u2208Np\n|IL(q)\u2212 IR(qd)|, (1)\nwhere IL(p) and IR(p) are image intensities at position p in the left and right image and Np is the set of locations within a fixed rectangular window centered at p.\nWe use bold lowercase letters (p, q, and r) to denote image locations. Appending a lowercase d to a location has the following meaning: if p = (x, y) then pd = (x\u2212 d, y). We use typewriter\nfont for the names of hyperparameters. For example, we would use patch size to denote the size of the neighbourhood area Np.\nEquation (1) can be interpreted as measuring the cost associated with matching a patch from the left image, centered at position p, with a patch from the right image, centered at position pd. We want the cost to be low when the two patches are centered around the same 3D point, and high when they are not.\nSince examples of good and bad matches can be constructed from publicly available data sets (for example, the KITTI and Middlebury stereo data sets), we can attempt to solve the matching problem by a supervised learning approach. Inspired by the successful applications of convolutional neural networks to vision problems, we used them to assess how well two small image patches match."}, {"heading": "3.1 Constructing the Data Set", "text": "One training example consists of two image patches, one from the left and one from the right image:\n< PLn\u00d7n(p),PRn\u00d7n(q) >,\nwhere PLn\u00d7n(p) denotes an n\u00d7n patch from the left image centered at p = (x, y). For each location where the true disparity d is known, we extract one negative and one positive example.\nA negative example is obtained by setting the center of the right patch to\nq = (x\u2212 d+ oneg, y),\nwhere oneg is chosen from either the interval [dataset neg low,dataset neg high] or, its origin reflected counterpart, [\u2212dataset neg high,\u2212dataset neg low]. The random offset oneg insures that the resulting image patches are not centered around the same 3D point.\nA positive example is derived by setting\nq = (x\u2212 d+ opos, y),\nwhere opos is chosen randomly from the interval [\u2212dataset pos,dataset pos]. The reason for including opos, instead of setting it to zero, has to do with the stereo method used later on. In particular, we found that cross-based cost aggregation performs better when the network assigns low matching costs to good matches as well as near matches."}, {"heading": "3.2 Network Architectures", "text": "We describe two network architectures for learning a similarity measure on image patches. The first architecture is faster than the second, but produces disparity maps that are slightly less accurate. In both cases, the input to the network is a pair of small image patches and the output is a measure of similarity between them."}, {"heading": "3.2.1 FAST ARCHITECTURE", "text": "The first architecture is a siamese network, that is, two shared-weight sub-networks joined at the head (Bromley et al., 1993). The sub-networks are composed of a number of convolutional layers with rectified linear units following all but the last layer. Both sub-networks output a vector capturing the properties of the input patch. The resulting two vectors are compared using the cosine\nsimilarity measure to produce the final output of the network. Figure 2 provides an overview of the architecture.\nThe network is trained to minimize a hinge loss. The loss is computed by considering pairs of examples centered around the same image position, with one example belonging to the positive and one to the negative class. Let s+ be the output of the network for the positive example, s\u2212 be the output of the network for the negative example, and let m, the margin, be a positive real number. The hinge loss for that pair of examples is defined as\nmax(0,m+ s\u2212 \u2212 s+). (2)\nThe loss for a batch of examples is computed by summing the terms in Equation (2) over all example pairs comprising the batch. The loss is zero when the similarity of the positive example is greater than the similarity of the negative example by at least the margin m. We set the margin to 0.2 for our experiments.\nThe hyperparameters of this architecture are the number of convolutional layers in each subnetwork (num conv layers), the size of the convolution kernels (conv kernel size), the number of feature maps in each layer (num conv feature maps), and the size of the input patch (input patch size)."}, {"heading": "3.2.2 ACCURATE ARCHITECTURE", "text": "The second architecture is derived from the first by replacing the cosine similarity with a number of fully-connected layers (see Figure 3). This architectural change increased the running time, but decreased the error rate. The two sub-networks comprise a number of convolutional layers, with a rectified linear unit following each layer. The resulting two vectors are concatenated and forwardpropagated through a number of fully-connected layers followed by rectified linear units. The last\nfully-connected layer produces a single number which, after being transformed with the sigmoid function, is interpreted as the similarity score between the input patches.\nWe use the binary cross-entropy loss for training. We would have preferred to use the same loss for both architectures, but decided against it. We could have used either loss for the accurate architecture, but chose the binary cross-entropy loss as it performed slightly better than the hinge loss. On the other hand, since the last step of the fast architecture is the cosine similarity computation, only the hinge loss seemed appropriate.\nThe hyperparameters for the accurate architecture are the number of convolutional layers in each sub-network (num conv layers), the size of the convolution kernels (conv kernel size), the number of feature maps in each layer (num conv feature maps), the size of the input patch (input patch size), the number of fully-connected layers (num fc layers), and the number of units in each fully-connected layer (num fc units)."}, {"heading": "3.3 Computing the Matching Cost", "text": "The output of the network is used to initialize the matching cost:\nCCNN(p, d) = \u2212s(< PL(p),PR(pd) >),\nwhere s(< PL(p),PR(pd) >) is the output of the network when run on input patches PL(p) and PR(pd). The minus sign converts the similarity score to a matching cost. To compute the entire matching cost tensor CCNN(p, d) we would, naively, have to perform the forward pass for each image location and each disparity under consideration. The following three implementation details kept the running time manageable:\n\u2022 The outputs of the two sub-networks need to be computed only once per location, and do not need to be recomputed for every disparity under consideration.\n\u2022 The output of the two sub-networks can be computed for all pixels in a single forward pass by propagating full-resolution images, instead of small image patches. Performing a single forward pass on the entire w \u00d7 h image is faster than performing w \u00b7 h forward passes on small patches because many intermediate results can be reused.\n\u2022 The output of the fully-connected layers in the accurate architecture can also be computed in a single forward pass. This is done by replacing each fully-connected layer with a convolutional layer with 1 \u00d7 1 kernels. We still need to perform the forward pass for each disparity under consideration; the maximum disparity d is 228 for the KITTI data set and 400 for the Middlebury data set. As a result, the fully-connected part of the network needs to be run d times, and is a bottleneck for the accurate architecture."}, {"heading": "4. Stereo Method", "text": "The raw outputs of the convolutional neural network are not enough to produce accurate disparity maps, with errors particularly apparent in low-texture regions and occluded areas. The quality of the disparity maps can be improved by applying a series of post-processing steps referred to as the stereo method. The stereo method we used was influenced by Mei et al. (2011) and comprises crossbased cost aggregation, semiglobal matching, a left-right consistency check, subpixel enhancement, a median, and a bilateral filter."}, {"heading": "4.1 Cross-based Cost Aggregation", "text": "Information from neighboring pixels can be combined by averaging the matching cost over a fixed window. This approach fails near depth discontinuities, where the assumption of constant depth within a window is violated. We might prefer a method that adaptively selects the neighborhood for each pixel, so that support is collected only from pixels of the same physical object. In crossbased cost aggregation (Zhang et al., 2009) we build a local neighborhood around each location comprising pixels with similar image intensity values with the hope that these pixels belong to the same object.\nThe method begins by constructing an upright cross at each position; this cross is used to define the local support region. The left arm pl at position p extends left as long as the following two conditions hold:\n\u2022 |I(p)\u2212I(pl)| < cbca intensity; the image intensities at positions p and pl are less than cbca intensity apart.\n\u2022 \u2016p\u2212pl\u2016 < cbca distance; the horizontal distance (or vertical distance in case of top and bottom arms) between positions p and pl is less than cbca distance pixels.\nThe right, bottom, and top arms are constructed analogously. Once the four arms are known, we can compute the support region U(p) as the union of horizontal arms of all positions q laying on p\u2019s vertical arm (see Figure 4).\nZhang et al. (2009) suggest that aggregation should consider the support regions of both images in a stereo pair. Let UL and UR denote the support regions in the left and right image. We define the combined support region Ud as\nUd(p) = {q|q \u2208 UL(p),qd \u2208 UR(pd)}.\nThe matching cost is averaged over the combined support region:\nC0CBCA(p, d) = CCNN(p, d), CiCBCA(p, d) = 1 |Ud(p)| \u2211\nq\u2208Ud(p)\nCi\u22121CBCA(q, d),\nwhere i is the iteration number. We repeat the averaging a number of times. Since the support regions are overlapping, the results can change at each iteration."}, {"heading": "4.2 Semiglobal Matching", "text": "We refine the matching cost by enforcing smoothness constraints on the disparity image. Following Hirschmu\u0308ller (2008), we define an energy function E(D) that depends on the disparity image D:\nE(D) = \u2211\np\n( C4CBCA(p, D(p)) + \u2211 q\u2208Np P1 \u00b7 1{|D(p)\u2212D(q)| = 1}\n+ \u2211\nq\u2208Np\nP2 \u00b7 1{|D(p)\u2212D(q)| > 1} ) ,\nwhere 1{\u00b7} denotes the indicator function. The first term penalizes disparities with high matching costs. The second term adds a penalty P1 when the disparity of neighboring pixels differ by one. The third term adds a larger penalty P2 when the neighboring disparities differ by more than one.\nRather than minimizing E(D) in all directions simultaneously, we could perform the minimization in a single direction with dynamic programming. This solution would introduce unwanted\nstreaking effects, since there would be no incentive to make the disparity image smooth in the directions we are not optimizing over. In semiglobal matching we minimize the energy in a single direction, repeat for several directions, and average to obtain the final result. Although Hirschmu\u0308ller (2008) suggested choosing sixteen direction, we only optimized along the two horizontal and the two vertical directions; adding the diagonal directions did not improve the accuracy of our system. To minimize E(D) in direction r, we define a matching cost Cr(p, d) with the following recurrence relation:\nCr(p, d) = C4CBCA(p, d)\u2212min k\nCr(p\u2212 r, k) + min { Cr(p\u2212 r, d), Cr(p\u2212 r, d\u2212 1) + P1,\nCr(p\u2212 r, d+ 1) + P1,min k\nCr(p\u2212 r, k) + P2 } .\nThe second term is subtracted to prevent values of Cr(p, d) from growing too large and does not affect the optimal disparity map.\nThe penalty parameters P1 and P2 are set according to the image gradient so that jumps in disparity coincide with edges in the image. Let D1 = |IL(p) \u2212 IL(p \u2212 r)| and D2 = |IR(pd) \u2212 IR(pd\u2212 r)| be the difference in image intensity between two neighboring positions in the direction we are optimizing over. We set P1 and P2 according to the following rules:\nP1 = sgm P1, P2 = sgm P2 if D1 < sgm D, D2 < sgm D; P1 = sgm P1/sgm Q2, P2 = sgm P2/sgm Q2 if D1 \u2265 sgm D, D2 \u2265 sgm D; P1 = sgm P1/sgm Q1, P2 = sgm P2/sgm Q1 otherwise.\nThe hyperparameters sgm P1 and sgm P2 set a base penalty for discontinuities in the disparity map. The base penalty is reduced by a factor of sgm Q1 if one of D1 or D2 indicate a strong image gradient or by a larger factor of sgm Q2 if both D1 and D2 indicate a strong image gradient. The value of P1 is further reduced by a factor of sgm V when considering the two vertical directions; in the ground truth, small changes in disparity are much more frequent in the vertical directions than in the horizontal directions and should be penalised less.\nThe final cost CSGM(p, d) is computed by taking the average across all four directions:\nCSGM(p, d) = 1\n4 \u2211 r Cr(p, d).\nAfter semiglobal matching we repeat cross-based cost aggregation, as described in the previous section. The hyperparameters cbca num iterations 1 and cbca num iterations 2 determine the number of cross-based cost aggregation iterations before and after semiglobal matching."}, {"heading": "4.3 Computing the Disparity Image", "text": "The disparity image D(p) is computed by the winner-take-all strategy, that is, by finding the disparity d that minimizes C(p, d):\nD(p) = argmin d C(p, d)."}, {"heading": "4.3.1 INTERPOLATION", "text": "The interpolation steps attempt to resolve the conflicts between the disparity map predicted for the left image and the disparity map predicted for the right image. Let DL denote the disparity map obtained by treating the left image as the reference image\u2014this was the case so far, that is, DL(p) = D(p)\u2014and let DR denote the disparity map obtained by treating the right image as the reference image. DL and DR sometimes disagree on what the correct disparity at a particular position should be. We detect these conflicts by performing a left-right consistency check. We label each position p by applying the following rules in turn:\ncorrect if |d\u2212DR(pd)| \u2264 1 for d = DL(p), mismatch if |d\u2212DR(pd)| \u2264 1 for any other d, occlusion otherwise.\nFor positions marked as occlusion, we want the new disparity value to come from the background. We interpolate by moving left until we find a position labeled correct and use its value. For positions marked as mismatch, we find the nearest correct pixels in 16 different directions and use the median of their disparities for interpolation. We refer to the interpolated disparity map as DINT."}, {"heading": "4.3.2 SUBPIXEL ENHANCEMENT", "text": "Subpixel enhancement provides an easy way to increase the resolution of a stereo algorithm. We fit a quadratic curve through the neighboring costs to obtain a new disparity image:\nDSE(p) = d\u2212 C+ \u2212 C\u2212\n2(C+ \u2212 2C + C\u2212) ,\nwhere d = DINT(p), C\u2212 = CSGM(p, d\u2212 1), C = CSGM(p, d), and C+ = CSGM(p, d+ 1)."}, {"heading": "4.3.3 REFINEMENT", "text": "The final steps of the stereo method consist of a 5\u00d75 median filter and the following bilateral filter:\nDBF(p) = 1\nW (p) \u2211 q\u2208Np DSE(q) \u00b7 g(\u2016p\u2212 q\u2016) \u00b7 1{|IL(p)\u2212 IL(q)| < blur threshold},\nwhere g(x) is the probability density function of a zero mean normal distribution with standard deviation blur sigma and W (p) is the normalizing constant:\nW (p) = \u2211\nq\u2208Np\ng(\u2016p\u2212 q\u2016) \u00b7 1{|IL(p)\u2212 IL(q)| < blur threshold}.\nThe role of the bilateral filter is to smooth the disparity map without blurring the edges. DBF is the final output of our stereo method."}, {"heading": "5. Experiments", "text": "We used three stereo data sets in our experiments: KITTI 2012, KITTI 2015, and Middlebury. The test set error rates reported in Tables 1, 2, and 4 were obtained by submitting the generated disparity maps to the online evaluation server. All other error rates were computed by splitting the data set in two, using one part for training and the other for validation."}, {"heading": "5.1 KITTI Stereo Data Set", "text": "The KITTI stereo data set (Geiger et al., 2013; Menze and Geiger, 2015) is a collection of rectified image pairs taken from two video cameras mounted on the roof of a car, roughly 54 centimeters apart. The images were recorded while driving in and around the city of Karlsruhe, in sunny and cloudy weather, at daytime. The images were taken at a resolution of 1240\u00d7 376. A rotating laser scanner mounted behind the left camera recorded ground truth depth, labeling around 30 % of the image pixels.\nThe ground truth disparities for the test set are withheld and an online leaderboard is provided where researchers can evaluate their method on the test set. Submissions are allowed once every three days. Error is measured as the percentage of pixels where the true disparity and the predicted disparity differ by more than three pixels. Translated into distance, this means that, for example, the error tolerance is 3 centimeters for objects 2 meters from the camera and 80 centimeters for objects 10 meters from the camera.\nTwo KITTI stereo data sets exist: KITTI 20121 and, the newer, KITTI 20152. For the task of computing stereo they are nearly identical, with the newer data set improving some aspects of the optical flow task. The 2012 data set contains 194 training and 195 testing images, while the 2015 data set contains 200 training and 200 testing images. There is a subtle but important difference\n1. The KITTI 2012 scoreboard: http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php? benchmark=stereo. 2. The KITTI 2015 scoreboard: http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php? benchmark=stereo.\nintroduced in the newer data set: vehicles in motion are densely labeled and car glass is included in the evaluation. This emphasizes the method\u2019s performance on reflective surfaces.\nThe best performing methods on the KITTI 2012 data set are listed in Table 1. Our accurate architecture ranks first with an error rate of 2.43 %. Third place on the leaderboard is held by our previous work (Z\u030cbontar and LeCun, 2015) with an error rate of 2.61 %. The two changes that reduced the error from 2.61 % to 2.43 % were augmenting the data set (see Section 5.4) and doubling the number of convolution layers while reducing the kernel size from 5 \u00d7 5 to 3 \u00d7 3. The method in second place (Gu\u0308ney and Geiger, 2015) uses the matching cost computed by our previous work (Z\u030cbontar and LeCun, 2015). The test error rate of the fast architecture is 2.82 %, which would be enough for fifth place had the method been allowed to appear in the public leaderboard. The running time for processing a single image pair is 67 seconds for the accurate architecture and 0.8 seconds for the fast architecture. Figure 5 contains a pair of examples from the KITTI 2012 data set, together with the predictions of our method.\nTable 2 presents the frontrunners on the KITTI 2015 data sets. The error rates of our methods are 3.89 % for the accurate architecture and 4.46 % for the fast architecture, occupying first and second place on the leaderboard. Since one submission per paper is allowed, only the result of the accurate architecture appears on the public leaderboard. See Figure 6 for the disparity maps produced by our method on the KITTI 2015 data set."}, {"heading": "5.2 Middlebury Stereo Data Set", "text": "The image pairs of the Middlebury stereo data set are indoor scenes taken under controlled lighting conditions. Structured light was used to measure the true disparities with higher density and precision than in the KITTI data set. The data sets were published in five separates works in the years 2001, 2003, 2005, 2006, and 2014 (Scharstein and Szeliski, 2002, 2003; Scharstein and Pal, 2007; Hirschmu\u0308ller and Scharstein, 2007; Scharstein et al., 2014). In this paper, we refer to the\nMiddlebury data set as the concatenation of all five data sets; a summary of each is presented in Table 3.\nEach scene in the 2005, 2006, and 2014 data set was taken under a number of lighting conditions and shutter exposures, with a typical image pair taken under four lighting conditions and seven exposure settings for a total of 28 images of the same scene.\nAn online leaderboard3, similar to the one provided by KITTI, displays a ranked list of all submitted methods. Participants have only one opportunity to submit their results on the test set to the public leaderboard. This rule is stricter than the one on the KITTI data set, where submissions are allowed every three days. The test set contains 15 images borrowed from the 2005 and 2014 data sets.\nThe data set is provided in full, half, and quarter resolution. The error is computed at full resolution; if the method outputs half or quarter resolution disparity maps, they are upsampled before the error is computed. We chose to run our method on half resolution images because of the limited size of the graphic card\u2019s memory available.\nRectifying a pair of images using standard calibration procedures, like the ones present in the OpenCV library, results in vertical disparity errors of up to nine pixels on the Middlebury data set (Scharstein et al., 2014). Each stereo pair in the 2014 data set is rectified twice: once using a standard, imperfect approach, and once using precise 2D correspondences for perfect rectification (Scharstein et al., 2014). We train the network on imperfectly rectified image pairs, since only two of the fifteen test images (Australia and Crusade) are rectified perfectly.\nThe error is measured as the percentage of pixels where the true disparity and the predicted disparity differ by more than two pixels; this corresponds to an error tolerance of one pixel at half resolution. The error on the evaluation server is, by default, computed only on non-occluded pixels. The final error reported online is the weighted average over the fifteen test images, with the weights set by the authors of the data set.\nTable 4 contains a snapshot of the third, and newest, version of the Middlebury leaderboard. Our method ranks first with an error rate of 8.29 % and a substantial lead over the second placed MeshStereo method, whose error rate is 13.4 %. See Figure 7 for disparity maps produced by our method on one image pair from the Middlebury data set.\n3. The Middlebury scoreboard: http://vision.middlebury.edu/stereo/eval3/."}, {"heading": "5.3 Details of Learning", "text": "We construct the binary classification data set from all available image pairs in the training set. The data set contains 25 million examples on the KITTI 2012, 17 million examples on the KITTI 2015, and 38 million examples on the Middlebury data set.\nAt training time, the input to the network was a batch of 128 pairs of image patches. At test time, the input was the entire left and right image. We could have used entire images during training as well, as it would allow us to implement the speed optimizations described in Section 3.3. There were several reasons why we preferred to train on image patches: it was easier to control the batch size, the examples could be shuffled so that one batch contained patches from several different images, and it was easier to maintain the same number of positive and negative examples within a batch.\nWe minimized the loss using mini-batch gradient descent with the momentum term set to 0.9. We trained for 14 epochs with the learning rate initially set to 0.003 for the accurate architecture and 0.001 for the fast architecture. The learning rate was decreased by a factor of 10 on the 11th epoch. Each image was preprocessed by subtracting the mean and dividing by the standard deviation of its pixel intensity values. The left and right image of a stereo pair were preprocessed separately. Our initial experiments suggested that using color information does not improve the quality of the disparity maps; therefore, we converted all color images to grayscale. The post-processing steps of the stereo method were implemented in CUDA (Nickolls et al., 2008), the network training was done with the Torch7 environment (Collobert et al., 2011) using the fast convolution routines of the cuDNN v2 library (Chetlur et al., 2014). The OpenCV library (Bradski, 2000) was used for the affine transformation in the data augmetntation step.\nThe hyperparameters where chosen with manual search and simple scripts that helped automate the process. The hyperparameters we selected are shown in Table 5."}, {"heading": "5.4 Data Set Augmentation", "text": "Augmenting the data set by repeatedly transforming the training examples is a commonly employed technique to reduce the network\u2019s generalization error. The transformations are applied at training time and do not affect the runtime performance. We randomly rotate, scale and shear the training patches; we also change their brightness and contrast.\nThe parameters of the transformation are chosen randomly for each pair of patches, and after one epoch of training, when the same example is being presented to the network for the second time, new random parameters are selected. We choose slightly different transformation parameters for the left and right image; for example, we would rotate the left patch by 10 degrees and the right by 14. Different data sets benefited from different types of transformations and, in some cases, using the wrong transformations increased the error.\nOn the Middlebury data set we took advantage of the fact that the images were taken under different lighting conditions and different shutter exposures by training on all available images.\nThe same data set augmentation parameters were used for the KITTI 2012 and KITTI 2015 data sets.\nThe Middlebury data sets contains two test images worth mentioning: Classroom, where the right image is underexposed and, therefore, darker than the left; and Djembe, where the left and right images were taken under different light conditions. To handle these two cases we train, 20 % of the time, on images where either the shutter exposure or the arrangements of lights are different for the left and right image.\nWe combat imperfect rectification on the Middlebury data set by including a small vertical disparity between the left and right image patches.\nBefore describing the steps of data augmentation, let us introduce some notation: in the following, a word in typewriter is used to denote the name of a hyperparameter defining a set, while the same word in italc is used to denote a number drawn randomly from that set. For example, rotate is a hyperparameter defining the set of possible rotations and rotate is a number drawn randomly from that set.\nThe steps of data augmentation are presented in the following list:\n\u2022 Rotate the left patch by rotate degrees and the right patch by rotate + rotate diff degrees.\n\u2022 Scale the left patch by scale and the right patch by scale \u00b7 scale diff.\n\u2022 Scale the left patch in the horizontal direction by horizontal scale and the right patch by horizontal scale \u00b7 horizontal scale diff.\n\u2022 Shear the left patch in the horizontal direction by horizontal shear and the right patch by horizontal shear + horizontal shear diff.\n\u2022 Translate the right patch in the vertical direction by vertical disparity.\n\u2022 Adjust the brightness and contrast by setting the left and right image patches to:\nPL \u2190 PL \u00b7 contrast + brightness and PR \u2190 PR \u00b7 (contrast \u00b7 contrast diff) + (brightness + brightness diff),\nwith addition and multiplication carried out element-wise where appropriate.\nTable 6 contains the hyperparameters used and measures how each data augmentation step affected the validation error.\nData augmentation reduced the validation error from 2.73 % to 2.61 % on the KITTI 2012 data set and from 8.75 % to 7.91 % on the Middlebury data set."}, {"heading": "5.5 Runtime", "text": "We measure the runtime of our implementation on a computer with a NVIDIA Titan X graphics processor unit. Table 7 contains the runtime measurements across a range of hyperparameter settings for three data sets: KITTI, Middlebury half resolution, and a new, fictitious data set, called Tiny, which we use to demonstrate the performance of our method on the kind of images typically used for autonomous driving or robotics. The sizes of images we measured the runtime on were: 1242 \u00d7 350 with 228 disparity levels for the KITTI data set, 1500 \u00d7 1000 with 200 disparity levels for the Middlebury data set, and 320 \u00d7 240 with 32 disparity levels for the Tiny data set.\nTable 7 reveals that the fast architecture is up to 90 times faster than the accurate architecture. Furthermore, the running times of the fast architecture are 0.78 seconds on KITTI, 2.03 seconds on Middlebury, and 0.06 seconds on the Tiny data set. We can also see that the fully-connected layers are responsible for most of the runtime in the accurate architecture, as the hyperparameters controlling the number of convolutional layer and the number of feature maps have only a small effect on the runtime.\nTraining times depended on the size of the data set and the architecture, but never exceeded two days."}, {"heading": "5.6 Matching Cost", "text": "We argue that the low error rate of our method is due to the convolutional neural network and not a superior stereo method. We verify this claim by replacing the convolutional neural network with two standard approaches for computing the matching cost:\n\u2022 The sum of absolute differences, which computes the matching cost according to Equation (1), that is, the matching cost between two image patches is computed by summing the absolute differences in image intensities between corresponding locations. We used 9 \u00d7 9 patches.\n\u2022 The census transform, which represents each image position as a bit vector. The size of this vector is a hyperparameter whose value, after experimenting with several, we set to 81. The vector is computed by cropping a 9 \u00d7 9 image patch centered around the position of interest and comparing the intensity values of each pixel in the patch to the intensity value of the pixel in the center. When the center pixel is brighter the corresponding bit is set. The matching cost is computed as the hamming distance between two census transformed vectors.\nThe \u201csad\u201d and \u201ccens\u201d columns of Table 8 contain the results of the sum of absolute differences and the census transform on the KITTI 2012, KITTI 2015, and Middlebury data sets. The validation errors in the last row of Table 8 should be used to compare the four methods. On all three data sets the ranking of the methods is the same: the accurate architecture performs best, followed by the fast architecture, the census transform, and the sum of absolute differences. Concretely, the error rates of the four methods are 2.61 %, 3.02 %, 4.90 %, and 8.16 % on KITTI 2012; 3.25 %, 3.99 %, 5.03 %, and 9.44 % on KITTI 2015; and 7.91 %, 9.87 %, 16.72 %, and 41.86 % on Middlebury.\nFor a visual comparison of our method and the census transform see Figures 5, 6, and 7."}, {"heading": "5.7 Stereo Method", "text": "The stereo method includes a number of post-processing steps: cross-based cost aggregation, semiglobal matching, interpolation, subpixel enchancement, a median, and a bilateral filter. We ran a set of experiments in which we exclude each of the aforementioned steps and recorded the validation error (see Table 8).\nThe last two rows of Table 8 allude to the importance of the post-processing steps of the stereo method. We see that, if all post-processing steps are removed, the validation error of the accurate architecture increases from 2.61 % to 13.49 % on KITTI 2012, from 3.25 % to 13.38 % on KITTI 2015, and from 7.91 % to 28.33 % on Middlebury.\nOut of all post-processing steps of the stereo method, semiglobal matching affects the validation error the strongest. If we remove it, the validation error increases from 2.61 % to 4.26 % on KITTI 2012, from 3.25 % to 4.51 % on KITTI 2015, and from 7.91 % to 11.99 % on Middlebury.\nWe did not use the left-right consistency check to eliminate errors in occluded regions on the Middlebury data set. The error rate increased from 7.91 % to 8.22 % using the left-right consistency check on the accurate architecture, which is why we decided to remove it."}, {"heading": "5.8 Data Set Size", "text": "We are using a supervised learning approach to measure the similarity between image patches. It is, therefore, natural to ask how does the size of the data set affect the quality of the disparity maps. To answer this question, we retrain our networks on smaller training sets obtained by selecting a random set of examples (see Table 9).\nWe observe that the validation error decreases as we increase the number of training examples. These experiments suggest a simple strategy for improving the results of our stereo method: collect a larger data set."}, {"heading": "20 3.17 2.84 4.13 3.53 11.14 9.73", "text": ""}, {"heading": "40 3.11 2.75 4.10 3.40 10.35 8.71", "text": ""}, {"heading": "60 3.09 2.67 4.05 3.34 10.14 8.36", "text": ""}, {"heading": "80 3.05 2.65 4.02 3.29 10.09 8.21", "text": ""}, {"heading": "5.9 Hyperparameters", "text": "Searching for a good set of hyperparameters is a daunting task\u2014with the search space growing exponentially with the number of hyperparameters and no gradient to guide us. To better understand the effect of each hyperparameter on the validation error, we conduct a series of experiments where we vary the value of a one hyperparameter while keeping the others fixed to their default values. The results are shown in Table 10 and can be summarised by observing that increasing the size of the network improves the generalization performance, but only up to a point, when presumably, because of the size of the data set, the generalization performance starts do decrease.\nNote that the num conv layers hyperparameter implicitly controls the size of the image patches. For example, a network with one convolutional layer with 3 \u00d7 3 kernels compares image\npatches of size 3 \u00d7 3, while a network with five convolutional layers compares patches of size 11\u00d7 11."}, {"heading": "6. Conclusion", "text": "We presented two convolutional neural network architectures for learning a similarity measure on image patches and applied them to the problem of stereo matching.\nThe source code of our implementation is available at https://github.com/jzbontar/ mc-cnn. The online repository contains procedures for computing the disparity map, training the network, as well as the post-processing steps of the stereo method.\nThe accurate architecture produces disparity maps with lower error rates than any previously published method on the KITTI 2012, KITTI 2015, and Middlebury data sets. The fast architecture computes the disparity maps up to 90 times faster than the accurate architecture with only a small increase in error. These results suggest that convolutional neural networks are well suited for computing the stereo matching cost even for applications that require real-time performance."}], "references": [{"title": "The OpenCV library", "author": ["Gary Bradski"], "venue": "Dr. Dobb\u2019s Journal of Software Tools,", "citeRegEx": "Bradski.,? \\Q2000\\E", "shortCiteRegEx": "Bradski.", "year": 2000}, {"title": "Signature verification using a siamese time delay neural network", "author": ["Jane Bromley", "James W Bentz", "L\u00e9on Bottou", "Isabelle Guyon", "Yann LeCun", "Cliff Moore", "Eduard S\u00e4ckinger", "Roopak Shah"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Bromley et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bromley et al\\.", "year": 1993}, {"title": "Discriminative learning of local image descriptors", "author": ["Matthew Brown", "Gang Hua", "Simon Winder"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Brown et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Brown et al\\.", "year": 2011}, {"title": "Large displacement optical flow: descriptor matching in variational motion estimation", "author": ["Thomas Brox", "Jitendra Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Brox and Malik.,? \\Q2011\\E", "shortCiteRegEx": "Brox and Malik.", "year": 2011}, {"title": "Low-level vision by consensus in a spatial hierarchy of regions", "author": ["Ayan Chakrabarti", "Ying Xiong", "Steven J. Gortler", "Todd Zickler"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Chakrabarti et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chakrabarti et al\\.", "year": 2015}, {"title": "A deep visual correspondence embedding model for stereo matching costs", "author": ["Zhuoyuan Chen", "Xun Sun", "Yinan Yu", "Liang Wang", "Chang Huang"], "venue": "IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "cuDNN: Efficient primitives for deep learning", "author": ["Sharan Chetlur", "Cliff Woolley", "Philippe Vandermersch", "Jonathan Cohen", "John Tran", "Bryan Catanzaro", "Evan Shelhamer"], "venue": "CoRR, abs/1410.0759,", "citeRegEx": "Chetlur et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chetlur et al\\.", "year": 2014}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "A two-stage correlation method for stereoscopic depth estimation", "author": ["Nils Einecke", "Julian Eggert"], "venue": "In Digital Image Computing: International Conference on Techniques and Applications (DICTA),", "citeRegEx": "Einecke and Eggert.,? \\Q2010\\E", "shortCiteRegEx": "Einecke and Eggert.", "year": 2010}, {"title": "Flownet: Learning optical flow with convolutional networks", "author": ["Philipp Fischer", "Alexey Dosovitskiy", "Eddy Ilg", "Philip H\u00e4usser", "Caner Haz\u0131rba\u015f", "Vladimir Golkov", "Patrick van der Smagt", "Daniel Cremers", "Thomas Brox"], "venue": "CoRR, abs/1504.06852,", "citeRegEx": "Fischer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fischer et al\\.", "year": 2015}, {"title": "Efficient large-scale stereo matching", "author": ["Andreas Geiger", "Martin Roser", "Raquel Urtasun"], "venue": "In Proceedings of the 10th Asian Conference on Computer Vision - Volume Part I,", "citeRegEx": "Geiger et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Geiger et al\\.", "year": 2011}, {"title": "Vision meets robotics: the KITTI dataset", "author": ["Andreas Geiger", "Philip Lenz", "Christoph Stiller", "Raquel Urtasun"], "venue": "International Journal of Robotics Research (IJRR),", "citeRegEx": "Geiger et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Geiger et al\\.", "year": 2013}, {"title": "Displets: Resolving stereo ambiguities using object knowledge", "author": ["Fatma G\u00fcney", "Andreas Geiger"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "G\u00fcney and Geiger.,? \\Q2015\\E", "shortCiteRegEx": "G\u00fcney and Geiger.", "year": 2015}, {"title": "Ensemble learning for confidence measures in stereo vision", "author": ["Ralf Haeusler", "Rahul Nair", "Daniel Kondermann"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Haeusler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Haeusler et al\\.", "year": 2013}, {"title": "MatchNet: Unifying feature and metric learning for patch-based matching", "author": ["Xufeng Han", "Thomas Leung", "Yangqing Jia", "Rahul Sukthankar", "Alexander C Berg"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Stereo processing by semiglobal matching and mutual information", "author": ["Heiko Hirschm\u00fcller"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Hirschm\u00fcller.,? \\Q2008\\E", "shortCiteRegEx": "Hirschm\u00fcller.", "year": 2008}, {"title": "Evaluation of cost functions for stereo matching", "author": ["Heiko Hirschm\u00fcller", "Daniel Scharstein"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Hirschm\u00fcller and Scharstein.,? \\Q2007\\E", "shortCiteRegEx": "Hirschm\u00fcller and Scharstein.", "year": 2007}, {"title": "Evaluation of stereo matching costs on images with radiometric differences", "author": ["Heiko Hirschm\u00fcller", "Daniel Scharstein"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Hirschm\u00fcller and Scharstein.,? \\Q2009\\E", "shortCiteRegEx": "Hirschm\u00fcller and Scharstein.", "year": 2009}, {"title": "SphereFlow: 6 DoF scene flow from RGB-D pairs", "author": ["Michael Hornacek", "Andrew Fitzgibbon", "Carsten Rother"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Hornacek et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hornacek et al\\.", "year": 2014}, {"title": "A method for learning matching errors for stereo computation", "author": ["Dan Kong", "Hai Tao"], "venue": "British Machine Vision Conference,", "citeRegEx": "Kong and Tao.,? \\Q2004\\E", "shortCiteRegEx": "Kong and Tao.", "year": 2004}, {"title": "Stereo matching via learning multiple experts behaviors", "author": ["Dan Kong", "Hai Tao"], "venue": "British Machine Vision Conference,", "citeRegEx": "Kong and Tao.,? \\Q2006\\E", "shortCiteRegEx": "Kong and Tao.", "year": 2006}, {"title": "Stratified dense matching for stereopsis in complex scenes", "author": ["Jana Kostkov\u00e1", "Radim S\u00e1ra"], "venue": "British Machine Vision Conference,", "citeRegEx": "Kostkov\u00e1 and S\u00e1ra.,? \\Q2003\\E", "shortCiteRegEx": "Kostkov\u00e1 and S\u00e1ra.", "year": 2003}, {"title": "Real-time stereo matching on CUDA using an iterative refinement method for adaptive support-weight correspondences", "author": ["Jedrzej Kowalczuk", "Eric T Psota", "Lance C Perez"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology,", "citeRegEx": "Kowalczuk et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kowalczuk et al\\.", "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Learning for stereo vision using the structured support vector machine", "author": ["Yunpeng Li", "Daniel P Huttenlocher"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Li and Huttenlocher.,? \\Q2008\\E", "shortCiteRegEx": "Li and Huttenlocher.", "year": 2008}, {"title": "On building an accurate stereo matching system on graphics hardware", "author": ["Xing Mei", "Xun Sun", "Mingcai Zhou", "Haitao Wang", "Xiaopeng Zhang"], "venue": "IEEE International Conference on Computer Vision Workshops (ICCV Workshops),", "citeRegEx": "Mei et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mei et al\\.", "year": 2011}, {"title": "Object scene flow for autonomous vehicles", "author": ["Moritz Menze", "Andreas Geiger"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Menze and Geiger.,? \\Q2015\\E", "shortCiteRegEx": "Menze and Geiger.", "year": 2015}, {"title": "Scalable parallel programming with CUDA", "author": ["John Nickolls", "Ian Buck", "Michael Garland", "Kevin Skadron"], "venue": null, "citeRegEx": "Nickolls et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nickolls et al\\.", "year": 2008}, {"title": "Towards a simulation driven stereo vision system", "author": ["Martin Peris", "Atsuto Maki", "Sara Martull", "Yasuhiro Ohkawa", "Kazuhiro Fukui"], "venue": "In 21st International Conference on Pattern Recognition (ICPR),", "citeRegEx": "Peris et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Peris et al\\.", "year": 2012}, {"title": "Learning conditional random fields for stereo", "author": ["Daniel Scharstein", "Chris Pal"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Scharstein and Pal.,? \\Q2007\\E", "shortCiteRegEx": "Scharstein and Pal.", "year": 2007}, {"title": "A taxonomy and evaluation of dense two-frame stereo correspondence algorithms", "author": ["Daniel Scharstein", "Richard Szeliski"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Scharstein and Szeliski.,? \\Q2002\\E", "shortCiteRegEx": "Scharstein and Szeliski.", "year": 2002}, {"title": "High-accuracy stereo depth maps using structured light", "author": ["Daniel Scharstein", "Richard Szeliski"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Scharstein and Szeliski.,? \\Q2003\\E", "shortCiteRegEx": "Scharstein and Szeliski.", "year": 2003}, {"title": "High-resolution stereo datasets with subpixel-accurate ground truth", "author": ["Daniel Scharstein", "Heiko Hirschm\u00fcller", "York Kitajima", "Greg Krathwohl", "Nera Ne\u0161i\u0107", "Xi Wang", "Porter Westling"], "venue": "German Conference on Pattern Recognition (GCPR),", "citeRegEx": "Scharstein et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Scharstein et al\\.", "year": 2014}, {"title": "Learning local feature descriptors using convex optimisation", "author": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Efficient high-resolution stereo matching using local plane sweeps", "author": ["Sudipta N Sinha", "Daniel Scharstein", "Richard Szeliski"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Sinha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sinha et al\\.", "year": 2014}, {"title": "Learning to detect ground control points for improving the accuracy of stereo matching", "author": ["Aristotle Spyropoulos", "Nikos Komodakis", "Philippos Mordohai"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Spyropoulos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Spyropoulos et al\\.", "year": 2014}, {"title": "A quantitative analysis of current practices in optical flow estimation and the principles behind them", "author": ["Deqing Sun", "Stefan Roth", "Michael J Black"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Sun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2014}, {"title": "Learning image descriptors with the boosting-trick", "author": ["Tomasz Trzcinski", "Mario Christoudias", "Vincent Lepetit", "Pascal Fua"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Trzcinski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Trzcinski et al\\.", "year": 2012}, {"title": "Piecewise rigid scene flow", "author": ["Christoph Vogel", "Konrad Schindler", "Stefan Roth"], "venue": "IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Vogel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vogel et al\\.", "year": 2013}, {"title": "View-consistent 3D scene flow estimation over multiple frames", "author": ["Christoph Vogel", "Stefan Roth", "Konrad Schindler"], "venue": "European Conference on Computer Vision (ECCV),", "citeRegEx": "Vogel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vogel et al\\.", "year": 2014}, {"title": "3D scene flow estimation with a piecewise rigid scene model", "author": ["Christoph Vogel", "Konrad Schindler", "Stefan Roth"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Vogel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vogel et al\\.", "year": 2015}, {"title": "Efficient joint segmentation, occlusion labeling, stereo and flow estimation", "author": ["Koichiro Yamaguchi", "David McAllester", "Raquel Urtasun"], "venue": "European Conference on Computer Vision (ECCV),", "citeRegEx": "Yamaguchi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yamaguchi et al\\.", "year": 2014}, {"title": "Learning to compare image patches via convolutional neural networks", "author": ["Sergey Zagoruyko", "Nikos Komodakis"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Zagoruyko and Komodakis.,? \\Q2015\\E", "shortCiteRegEx": "Zagoruyko and Komodakis.", "year": 2015}, {"title": "Computing the stereo matching cost with a convolutional neural network", "author": ["Jure \u017dbontar", "Yann LeCun"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "\u017dbontar and LeCun.,? \\Q2015\\E", "shortCiteRegEx": "\u017dbontar and LeCun.", "year": 2015}, {"title": "Cross-based local stereo matching using orthogonal integral images", "author": ["Ke Zhang", "Jiangbo Lu", "Gauthier Lafruit"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology,", "citeRegEx": "Zhang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2009}, {"title": "Estimating optimal parameters for MRF stereo from a single image pair", "author": ["Li Zhang", "Steven M Seitz"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Zhang and Seitz.,? \\Q2007\\E", "shortCiteRegEx": "Zhang and Seitz.", "year": 2007}], "referenceMentions": [{"referenceID": 30, "context": "According to the taxonomy of Scharstein and Szeliski (2002), a typical stereo algorithm consists of four steps: matching", "startOffset": 29, "endOffset": 60}, {"referenceID": 23, "context": "We propose training a convolutional neural network (LeCun et al., 1998) on pairs of small image patches where the true disparity is known (for example, obtained by a LIDAR sensor or structured light).", "startOffset": 51, "endOffset": 71}, {"referenceID": 43, "context": "This paper extends our previous work (\u017dbontar and LeCun, 2015) by including a description of a new architecture, results on two new data sets, lower error rates, and more thorough experiments.", "startOffset": 37, "endOffset": 62}, {"referenceID": 15, "context": "Following Hirschm\u00fcller and Scharstein (2009) we refer to the first two steps as computing the matching cost and the last two steps as the stereo method.", "startOffset": 10, "endOffset": 45}, {"referenceID": 25, "context": "(2012) initialized the matching cost with AD-Census (Mei et al., 2011), and used multiclass linear discriminant analysis to learn a mapping from the computed matching cost to the final disparity.", "startOffset": 52, "endOffset": 70}, {"referenceID": 13, "context": "Recent work (Haeusler et al., 2013; Spyropoulos et al., 2014) focused on estimating the confidence of the computed matching cost.", "startOffset": 12, "endOffset": 61}, {"referenceID": 35, "context": "Recent work (Haeusler et al., 2013; Spyropoulos et al., 2014) focused on estimating the confidence of the computed matching cost.", "startOffset": 12, "endOffset": 61}, {"referenceID": 2, "context": "A related problem to computing the matching cost is learning local image descriptors (Brown et al., 2011; Trzcinski et al., 2012; Simonyan et al., 2014; Han et al., 2015; Fischer et al., 2015).", "startOffset": 85, "endOffset": 192}, {"referenceID": 37, "context": "A related problem to computing the matching cost is learning local image descriptors (Brown et al., 2011; Trzcinski et al., 2012; Simonyan et al., 2014; Han et al., 2015; Fischer et al., 2015).", "startOffset": 85, "endOffset": 192}, {"referenceID": 33, "context": "A related problem to computing the matching cost is learning local image descriptors (Brown et al., 2011; Trzcinski et al., 2012; Simonyan et al., 2014; Han et al., 2015; Fischer et al., 2015).", "startOffset": 85, "endOffset": 192}, {"referenceID": 14, "context": "A related problem to computing the matching cost is learning local image descriptors (Brown et al., 2011; Trzcinski et al., 2012; Simonyan et al., 2014; Han et al., 2015; Fischer et al., 2015).", "startOffset": 85, "endOffset": 192}, {"referenceID": 9, "context": "A related problem to computing the matching cost is learning local image descriptors (Brown et al., 2011; Trzcinski et al., 2012; Simonyan et al., 2014; Han et al., 2015; Fischer et al., 2015).", "startOffset": 85, "endOffset": 192}, {"referenceID": 37, "context": "Several methods have been suggested for solving the problem of learning local image descriptors, such as boosting (Trzcinski et al., 2012), convex optimization (Simonyan et al.", "startOffset": 114, "endOffset": 138}, {"referenceID": 33, "context": ", 2012), convex optimization (Simonyan et al., 2014), and convolutional neural networks (Zagoruyko and Komodakis, 2015; Han et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 42, "context": ", 2014), and convolutional neural networks (Zagoruyko and Komodakis, 2015; Han et al., 2015).", "startOffset": 43, "endOffset": 92}, {"referenceID": 14, "context": ", 2014), and convolutional neural networks (Zagoruyko and Komodakis, 2015; Han et al., 2015).", "startOffset": 43, "endOffset": 92}, {"referenceID": 20, "context": "For a general overview of stereo algorithms see Scharstein and Szeliski (2002). Kong and Tao (2004) used the sum of squared distances to compute an initial matching cost.", "startOffset": 48, "endOffset": 79}, {"referenceID": 15, "context": "Kong and Tao (2004) used the sum of squared distances to compute an initial matching cost.", "startOffset": 0, "endOffset": 20}, {"referenceID": 15, "context": "Kong and Tao (2004) used the sum of squared distances to compute an initial matching cost. They then trained a model to predict the probability distribution over three classes: the initial disparity is correct, the initial disparity is incorrect due to fattening of a foreground object, and the initial disparity is incorrect due to other reasons. The predicted probabilities were used to adjust the initial matching cost. Kong and Tao (2006) later extend their work by combining predictions obtained by computing normalized cross-correlation over different window sizes and centers.", "startOffset": 0, "endOffset": 443}, {"referenceID": 15, "context": "Kong and Tao (2004) used the sum of squared distances to compute an initial matching cost. They then trained a model to predict the probability distribution over three classes: the initial disparity is correct, the initial disparity is incorrect due to fattening of a foreground object, and the initial disparity is incorrect due to other reasons. The predicted probabilities were used to adjust the initial matching cost. Kong and Tao (2006) later extend their work by combining predictions obtained by computing normalized cross-correlation over different window sizes and centers. Peris et al. (2012) initialized the matching cost with AD-Census (Mei et al.", "startOffset": 0, "endOffset": 604}, {"referenceID": 15, "context": "Kong and Tao (2004) used the sum of squared distances to compute an initial matching cost. They then trained a model to predict the probability distribution over three classes: the initial disparity is correct, the initial disparity is incorrect due to fattening of a foreground object, and the initial disparity is incorrect due to other reasons. The predicted probabilities were used to adjust the initial matching cost. Kong and Tao (2006) later extend their work by combining predictions obtained by computing normalized cross-correlation over different window sizes and centers. Peris et al. (2012) initialized the matching cost with AD-Census (Mei et al., 2011), and used multiclass linear discriminant analysis to learn a mapping from the computed matching cost to the final disparity. Ground-truth data was also used to learn parameters of probabilistic graphical models. Zhang and Seitz (2007) used an alternative optimization algorithm to estimate optimal values of Markov random field hyperparameters.", "startOffset": 0, "endOffset": 903}, {"referenceID": 15, "context": "Kong and Tao (2004) used the sum of squared distances to compute an initial matching cost. They then trained a model to predict the probability distribution over three classes: the initial disparity is correct, the initial disparity is incorrect due to fattening of a foreground object, and the initial disparity is incorrect due to other reasons. The predicted probabilities were used to adjust the initial matching cost. Kong and Tao (2006) later extend their work by combining predictions obtained by computing normalized cross-correlation over different window sizes and centers. Peris et al. (2012) initialized the matching cost with AD-Census (Mei et al., 2011), and used multiclass linear discriminant analysis to learn a mapping from the computed matching cost to the final disparity. Ground-truth data was also used to learn parameters of probabilistic graphical models. Zhang and Seitz (2007) used an alternative optimization algorithm to estimate optimal values of Markov random field hyperparameters. Scharstein and Pal (2007) constructed a new data set of 30 stereo pairs and used it to learn parameters of a conditional random field.", "startOffset": 0, "endOffset": 1039}, {"referenceID": 15, "context": "Kong and Tao (2004) used the sum of squared distances to compute an initial matching cost. They then trained a model to predict the probability distribution over three classes: the initial disparity is correct, the initial disparity is incorrect due to fattening of a foreground object, and the initial disparity is incorrect due to other reasons. The predicted probabilities were used to adjust the initial matching cost. Kong and Tao (2006) later extend their work by combining predictions obtained by computing normalized cross-correlation over different window sizes and centers. Peris et al. (2012) initialized the matching cost with AD-Census (Mei et al., 2011), and used multiclass linear discriminant analysis to learn a mapping from the computed matching cost to the final disparity. Ground-truth data was also used to learn parameters of probabilistic graphical models. Zhang and Seitz (2007) used an alternative optimization algorithm to estimate optimal values of Markov random field hyperparameters. Scharstein and Pal (2007) constructed a new data set of 30 stereo pairs and used it to learn parameters of a conditional random field. Li and Huttenlocher (2008) presented a conditional random field model with a non-parametric cost function and used a structured support vector machine to learn the model parameters.", "startOffset": 0, "endOffset": 1175}, {"referenceID": 11, "context": "Recent work (Haeusler et al., 2013; Spyropoulos et al., 2014) focused on estimating the confidence of the computed matching cost. Haeusler et al. (2013) used a random forest classifier to combine several confidence measures.", "startOffset": 13, "endOffset": 153}, {"referenceID": 11, "context": "Recent work (Haeusler et al., 2013; Spyropoulos et al., 2014) focused on estimating the confidence of the computed matching cost. Haeusler et al. (2013) used a random forest classifier to combine several confidence measures. Similarly, Spyropoulos et al. (2014) trained a random forest classifier to predict the confidence of the matching cost and used the predictions as soft constraints in a Markov random field to decrease the error of the stereo method.", "startOffset": 13, "endOffset": 262}, {"referenceID": 2, "context": "A related problem to computing the matching cost is learning local image descriptors (Brown et al., 2011; Trzcinski et al., 2012; Simonyan et al., 2014; Han et al., 2015; Fischer et al., 2015). The two problems share a common subtask: to measure the similarity between image patches. Brown et al. (2011) introduced a general framework for learning image descriptors and used Powell\u2019s method to select good hyperparameters.", "startOffset": 86, "endOffset": 304}, {"referenceID": 2, "context": "A related problem to computing the matching cost is learning local image descriptors (Brown et al., 2011; Trzcinski et al., 2012; Simonyan et al., 2014; Han et al., 2015; Fischer et al., 2015). The two problems share a common subtask: to measure the similarity between image patches. Brown et al. (2011) introduced a general framework for learning image descriptors and used Powell\u2019s method to select good hyperparameters. Several methods have been suggested for solving the problem of learning local image descriptors, such as boosting (Trzcinski et al., 2012), convex optimization (Simonyan et al., 2014), and convolutional neural networks (Zagoruyko and Komodakis, 2015; Han et al., 2015). Works of Zagoruyko and Komodakis (2015) and Han et al.", "startOffset": 86, "endOffset": 733}, {"referenceID": 2, "context": "A related problem to computing the matching cost is learning local image descriptors (Brown et al., 2011; Trzcinski et al., 2012; Simonyan et al., 2014; Han et al., 2015; Fischer et al., 2015). The two problems share a common subtask: to measure the similarity between image patches. Brown et al. (2011) introduced a general framework for learning image descriptors and used Powell\u2019s method to select good hyperparameters. Several methods have been suggested for solving the problem of learning local image descriptors, such as boosting (Trzcinski et al., 2012), convex optimization (Simonyan et al., 2014), and convolutional neural networks (Zagoruyko and Komodakis, 2015; Han et al., 2015). Works of Zagoruyko and Komodakis (2015) and Han et al. (2015), in particlar, are very similar to our own, differing mostly in the architecture of the network; concretely, the inclusion of pooling and subsampling to account for larger patch sizes and larger variation in viewpoint.", "startOffset": 86, "endOffset": 755}, {"referenceID": 1, "context": "1 FAST ARCHITECTURE The first architecture is a siamese network, that is, two shared-weight sub-networks joined at the head (Bromley et al., 1993).", "startOffset": 124, "endOffset": 146}, {"referenceID": 25, "context": "The stereo method we used was influenced by Mei et al. (2011) and comprises crossbased cost aggregation, semiglobal matching, a left-right consistency check, subpixel enhancement, a median, and a bilateral filter.", "startOffset": 44, "endOffset": 62}, {"referenceID": 44, "context": "In crossbased cost aggregation (Zhang et al., 2009) we build a local neighborhood around each location comprising pixels with similar image intensity values with the hope that these pixels belong to the same object.", "startOffset": 31, "endOffset": 51}, {"referenceID": 15, "context": "Following Hirschm\u00fcller (2008), we define an energy function E(D) that depends on the disparity image D:", "startOffset": 10, "endOffset": 30}, {"referenceID": 15, "context": "Although Hirschm\u00fcller (2008) suggested choosing sixteen direction, we only optimized along the two horizontal and the two vertical directions; adding the diagonal directions did not improve the accuracy of our system.", "startOffset": 9, "endOffset": 29}, {"referenceID": 10, "context": "43 67 2 Displets G\u00fcney and Geiger (2015) 2.", "startOffset": 17, "endOffset": 41}, {"referenceID": 10, "context": "43 67 2 Displets G\u00fcney and Geiger (2015) 2.47 265 3 MC-CNN \u017dbontar and LeCun (2015) 2.", "startOffset": 17, "endOffset": 84}, {"referenceID": 10, "context": "43 67 2 Displets G\u00fcney and Geiger (2015) 2.47 265 3 MC-CNN \u017dbontar and LeCun (2015) 2.61 100 4 PRSM Vogel et al. (2015) F, MV 2.", "startOffset": 17, "endOffset": 120}, {"referenceID": 10, "context": "43 67 2 Displets G\u00fcney and Geiger (2015) 2.47 265 3 MC-CNN \u017dbontar and LeCun (2015) 2.61 100 4 PRSM Vogel et al. (2015) F, MV 2.78 300 MC-CNN-fst Fast architecture 2.82 0.8 5 SPS-StFl Yamaguchi et al. (2014) F, MS 2.", "startOffset": 17, "endOffset": 208}, {"referenceID": 10, "context": "43 67 2 Displets G\u00fcney and Geiger (2015) 2.47 265 3 MC-CNN \u017dbontar and LeCun (2015) 2.61 100 4 PRSM Vogel et al. (2015) F, MV 2.78 300 MC-CNN-fst Fast architecture 2.82 0.8 5 SPS-StFl Yamaguchi et al. (2014) F, MS 2.83 35 6 VC-SF Vogel et al. (2014) F, MV 3.", "startOffset": 17, "endOffset": 250}, {"referenceID": 4, "context": "05 300 7 Deep Embed Chen et al. (2015) 3.", "startOffset": 20, "endOffset": 39}, {"referenceID": 4, "context": "05 300 7 Deep Embed Chen et al. (2015) 3.10 3 8 JSOSM Unpublished work 3.15 105 9 OSF Menze and Geiger (2015) F 3.", "startOffset": 20, "endOffset": 110}, {"referenceID": 4, "context": "28 3000 10 CoR Chakrabarti et al. (2015) 3.", "startOffset": 15, "endOffset": 41}, {"referenceID": 11, "context": "1 KITTI Stereo Data Set The KITTI stereo data set (Geiger et al., 2013; Menze and Geiger, 2015) is a collection of rectified image pairs taken from two video cameras mounted on the roof of a car, roughly 54 centimeters apart.", "startOffset": 50, "endOffset": 95}, {"referenceID": 26, "context": "1 KITTI Stereo Data Set The KITTI stereo data set (Geiger et al., 2013; Menze and Geiger, 2015) is a collection of rectified image pairs taken from two video cameras mounted on the roof of a car, roughly 54 centimeters apart.", "startOffset": 50, "endOffset": 95}, {"referenceID": 30, "context": "8 2 SPS-St Yamaguchi et al. (2014) 5.", "startOffset": 11, "endOffset": 35}, {"referenceID": 20, "context": "31 2 3 OSF Menze and Geiger (2015) F 5.", "startOffset": 11, "endOffset": 35}, {"referenceID": 20, "context": "31 2 3 OSF Menze and Geiger (2015) F 5.79 3000 4 PR-Sceneflow Vogel et al. (2013) F 6.", "startOffset": 11, "endOffset": 82}, {"referenceID": 12, "context": "24 150 5 SGM+C+NL Hirschm\u00fcller (2008); Sun et al.", "startOffset": 18, "endOffset": 38}, {"referenceID": 12, "context": "24 150 5 SGM+C+NL Hirschm\u00fcller (2008); Sun et al. (2014) F 6.", "startOffset": 18, "endOffset": 57}, {"referenceID": 12, "context": "24 150 5 SGM+C+NL Hirschm\u00fcller (2008); Sun et al. (2014) F 6.84 270 6 SGM+LDOF Hirschm\u00fcller (2008); Brox and Malik (2011) F 6.", "startOffset": 18, "endOffset": 99}, {"referenceID": 3, "context": "84 270 6 SGM+LDOF Hirschm\u00fcller (2008); Brox and Malik (2011) F 6.", "startOffset": 39, "endOffset": 61}, {"referenceID": 3, "context": "84 270 6 SGM+LDOF Hirschm\u00fcller (2008); Brox and Malik (2011) F 6.84 86 7 SGM+SF Hirschm\u00fcller (2008); Hornacek et al.", "startOffset": 39, "endOffset": 100}, {"referenceID": 3, "context": "84 270 6 SGM+LDOF Hirschm\u00fcller (2008); Brox and Malik (2011) F 6.84 86 7 SGM+SF Hirschm\u00fcller (2008); Hornacek et al. (2014) F 6.", "startOffset": 39, "endOffset": 124}, {"referenceID": 3, "context": "84 270 6 SGM+LDOF Hirschm\u00fcller (2008); Brox and Malik (2011) F 6.84 86 7 SGM+SF Hirschm\u00fcller (2008); Hornacek et al. (2014) F 6.84 2700 8 ELAS Geiger et al. (2011) 9.", "startOffset": 39, "endOffset": 164}, {"referenceID": 3, "context": "84 270 6 SGM+LDOF Hirschm\u00fcller (2008); Brox and Malik (2011) F 6.84 86 7 SGM+SF Hirschm\u00fcller (2008); Hornacek et al. (2014) F 6.84 2700 8 ELAS Geiger et al. (2011) 9.72 0.3 9 OCV-SGBM Hirschm\u00fcller (2008) 10.", "startOffset": 39, "endOffset": 204}, {"referenceID": 3, "context": "84 270 6 SGM+LDOF Hirschm\u00fcller (2008); Brox and Malik (2011) F 6.84 86 7 SGM+SF Hirschm\u00fcller (2008); Hornacek et al. (2014) F 6.84 2700 8 ELAS Geiger et al. (2011) 9.72 0.3 9 OCV-SGBM Hirschm\u00fcller (2008) 10.86 1.1 10 SDM Kostkov\u00e1 and S\u00e1ra (2003) 11.", "startOffset": 39, "endOffset": 246}, {"referenceID": 43, "context": "Third place on the leaderboard is held by our previous work (\u017dbontar and LeCun, 2015) with an error rate of 2.", "startOffset": 60, "endOffset": 85}, {"referenceID": 12, "context": "The method in second place (G\u00fcney and Geiger, 2015) uses the matching cost computed by our previous work (\u017dbontar and LeCun, 2015).", "startOffset": 27, "endOffset": 51}, {"referenceID": 43, "context": "The method in second place (G\u00fcney and Geiger, 2015) uses the matching cost computed by our previous work (\u017dbontar and LeCun, 2015).", "startOffset": 105, "endOffset": 130}, {"referenceID": 29, "context": "The data sets were published in five separates works in the years 2001, 2003, 2005, 2006, and 2014 (Scharstein and Szeliski, 2002, 2003; Scharstein and Pal, 2007; Hirschm\u00fcller and Scharstein, 2007; Scharstein et al., 2014).", "startOffset": 99, "endOffset": 222}, {"referenceID": 16, "context": "The data sets were published in five separates works in the years 2001, 2003, 2005, 2006, and 2014 (Scharstein and Szeliski, 2002, 2003; Scharstein and Pal, 2007; Hirschm\u00fcller and Scharstein, 2007; Scharstein et al., 2014).", "startOffset": 99, "endOffset": 222}, {"referenceID": 32, "context": "The data sets were published in five separates works in the years 2001, 2003, 2005, 2006, and 2014 (Scharstein and Szeliski, 2002, 2003; Scharstein and Pal, 2007; Hirschm\u00fcller and Scharstein, 2007; Scharstein et al., 2014).", "startOffset": 99, "endOffset": 222}, {"referenceID": 32, "context": "Rectifying a pair of images using standard calibration procedures, like the ones present in the OpenCV library, results in vertical disparity errors of up to nine pixels on the Middlebury data set (Scharstein et al., 2014).", "startOffset": 197, "endOffset": 222}, {"referenceID": 32, "context": "Each stereo pair in the 2014 data set is rectified twice: once using a standard, imperfect approach, and once using precise 2D correspondences for perfect rectification (Scharstein et al., 2014).", "startOffset": 169, "endOffset": 194}, {"referenceID": 20, "context": "1 2435 5 IDR Kowalczuk et al. (2013) Half 18.", "startOffset": 13, "endOffset": 37}, {"referenceID": 14, "context": "49 6 SGM Hirschm\u00fcller (2008) Half 18.", "startOffset": 9, "endOffset": 29}, {"referenceID": 14, "context": "49 6 SGM Hirschm\u00fcller (2008) Half 18.7 9.90 7 LPS Sinha et al. (2014) Half 19.", "startOffset": 9, "endOffset": 70}, {"referenceID": 14, "context": "49 6 SGM Hirschm\u00fcller (2008) Half 18.7 9.90 7 LPS Sinha et al. (2014) Half 19.4 9.52 8 LPS Sinha et al. (2014) Full 20.", "startOffset": 9, "endOffset": 111}, {"referenceID": 14, "context": "49 6 SGM Hirschm\u00fcller (2008) Half 18.7 9.90 7 LPS Sinha et al. (2014) Half 19.4 9.52 8 LPS Sinha et al. (2014) Full 20.3 25.8 9 SGM Hirschm\u00fcller (2008) Quarter 21.", "startOffset": 9, "endOffset": 152}, {"referenceID": 8, "context": "48 10 SNCC Einecke and Eggert (2010) Half 22.", "startOffset": 11, "endOffset": 37}, {"referenceID": 27, "context": "The post-processing steps of the stereo method were implemented in CUDA (Nickolls et al., 2008), the network training was done with the Torch7 environment (Collobert et al.", "startOffset": 72, "endOffset": 95}, {"referenceID": 7, "context": ", 2008), the network training was done with the Torch7 environment (Collobert et al., 2011) using the fast convolution routines of the cuDNN v2 library (Chetlur et al.", "startOffset": 67, "endOffset": 91}, {"referenceID": 6, "context": ", 2011) using the fast convolution routines of the cuDNN v2 library (Chetlur et al., 2014).", "startOffset": 68, "endOffset": 90}, {"referenceID": 0, "context": "The OpenCV library (Bradski, 2000) was used for the affine transformation in the data augmetntation step.", "startOffset": 19, "endOffset": 34}], "year": 2015, "abstractText": "We present a method for extracting depth information from a rectified image pair. Our approach focuses on the first stage of many stereo algorithms: the matching cost computation. We approach the problem by learning a similarity measure on small image patches using a convolutional neural network. Training is carried out in a supervised manner by constructing a binary classification data set with examples of similar and dissimilar pairs of patches. We examine two network architectures for this task: one tuned for speed, the other for accuracy. The output of the convolutional neural network is used to initialize the stereo matching cost. A series of post-processing steps follow: cross-based cost aggregation, semiglobal matching, a left-right consistency check, subpixel enhancement, a median filter, and a bilateral filter. We evaluate our method on the KITTI 2012, KITTI 2015, and Middlebury stereo data sets and show that it outperforms other approaches on all three data sets.", "creator": "LaTeX with hyperref package"}}}