{"id": "1510.08418", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2015", "title": "Fast k-best Sentence Compression", "abstract": "A popular approach to sentence compression is to formulate the task as a constrained optimization problem and solve it with integer linear programming (ILP) tools. Unfortunately, dependence on ILP may make the compressor prohibitively slow, and thus approximation techniques have been proposed which are often complex and offer a moderate gain in speed. As an alternative solution, we introduce a novel compression algorithm which generates k-best compressions relying on local deletion decisions. Our algorithm is two orders of magnitude faster than a recent ILP-based method while producing better compressions. Moreover, an extensive evaluation demonstrates that the quality of compressions does not degrade much as we move from single best to top-five results. The result is that the compression algorithm can be used as a single function of the original code.\n\n\n\n\nThe first approach is to perform the final evaluation and evaluate the optimal compression algorithm, which can be easily used in the following cases:\nThe algorithm will be computed by adding the following parameters:\nThe algorithm will be determined by comparing the algorithm to the algorithm using a method of calculating the size of each one in the input space (the output size is a fixed number of parameters).\nIf the input size exceeds the current size, the algorithm will have to be computed by subtracting the input size from the resulting input size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output size from the output", "histories": [["v1", "Wed, 28 Oct 2015 19:04:30 GMT  (86kb,D)", "http://arxiv.org/abs/1510.08418v1", "11 pages"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["katja filippova", "enrique alfonseca"], "accepted": false, "id": "1510.08418"}, "pdf": {"name": "1510.08418.pdf", "metadata": {"source": "CRF", "title": "Fast k-best Sentence Compression", "authors": ["Katja Filippova", "Enrique Alfonseca"], "emails": ["katjaf@google.com", "ealfonseca@google.com"], "sections": [{"heading": "1 Introduction", "text": "There has been a surge in sentence compression research in the past decade because of the promise it holds for extractive text summarization and the utility it has in the age of mobile devices with small screens. Similar to text summarization, extractive approaches which do not introduce new words into the result have been particularly popular. There, the main challenge lies in knowing which words can be deleted without negatively affecting the information content or grammaticality of the sentence. Given the complexity of the compression task (the number of possible outputs is exponential), many systems frame it, sometimes combined with summarization, as an ILP problem which is then solved\nwith off-the-shelf tools (Martins & Smith, 2009; Berg-Kirkpatrick et al., 2011; Thadani & McKeown, 2013). While ILP formulations are clear and the translation to an ILP problem is often natural (Clarke & Lapata, 2008), they come with a high solution cost and prohibitively long processing times (Woodsend & Lapata, 2012; Almeida & Martins, 2013). Thus, robust algorithms capable of generating informative and grammatically correct compressions at much faster running times are still desirable.\nTowards this goal, we propose a novel supervised sentence compression method which combines local deletion decisions with a recursive procedure of getting most probable compressions at every node in the tree. To generate the top-scoring compression a single tree traversal is required. To extend the k-best list with a k + 1th compression, the algorithm needs O(m \u00d7 n) comparisons where n is the node count and m is the average branching factor in the tree. Importantly, approximate search techniques like beam search (Galanis & Androutsopoulos, 2010; Wang et al., 2013), are not required.\nCompared with a recent ILP method (Filippova & Altun, 2013), our algorithm is two orders of magnitude faster while producing shorter compressions of equal quality. Both methods are supervised and use the same training data and features. The results indicate that good readability and informativeness, as perceived by human raters, can be achieved without impairing algorithm efficiency. Furthermore, both scores remain high as one moves from the top result to the top five. To our knowledge we are the first to report evaluation results beyond single best output.\nTo address cases where local decisions may be in-\nar X\niv :1\n51 0.\n08 41\n8v 1\n[ cs\n.C L\n] 2\n8 O\nct 2\nsufficient, we present an extension to the algorithm where we tradeoff the guarantee of obtaining the top scoring solution for the benefit of scoring a node subset as a whole. This extension only moderately affects the running time while eliminating a source of suboptimal compressions.\nComparison to related work Many compression systems have been introduced since the very first approaches by Grefenstette (1998), Jing & McKeown (2000) and Knight & Marcu (2000). Almost all of them make use of syntactic information (e.g., Clarke & Lapata (2006), McDonald (2006), Toutanova et al. (2007)), and our system is not an exception. Like Nomoto (2009), Wang et al. (2013) we operate on syntactic trees provided by a stateof-the-art parser. The benefit of modifying a given syntactic structure is that the space of possible compressions is significantly constrained: instead of all possible token subsequences, the search space is restricted to all the subtrees of the input parse. While some methods rewrite the source tree and produce an alternative derivation at every consituent (Knight & Marcu, 2000; Galley & McKeown, 2007), others prune edges in the source tree (Filippova & Strube, 2008; Galanis & Androutsopoulos, 2010; Wang et al., 2013). Most of these approaches are supervised in that they learn from a parallel compression corpus either the rewrite operations, or deletion decisions. In our work we also adopt the pruning approach and use parallel data to estimate the probability of deleting an edge given context.\nSeveral text-to-text generation systems use ILP as an optimization tool to generate new sentences by combining pieces from the input (Clarke & Lapata, 2008; Martins & Smith, 2009; Woodsend et al., 2010; Filippova & Altun, 2013). While off-theshelf general purpose LP solvers are designed to be fast, in practice they may make the compressor prohibitively slow, in particular if compression is done jointly with summarization (Berg-Kirkpatrick et al., 2011; Qian & Liu, 2013; Thadani, 2014). Recent improvements to the ILP-based methods have been significant but not dramatic. For example, Thadani (2014) presents an approximation technique resulting in a 60% reduction in average inference time. Compared with this work, the main practical advantage of our system is that it is very fast with-\nout trading compression quality for speed improvements. On the modeling side, it demonstrates that local decisions are sufficient to produce an informative and grammatically correct sentence.\nOur recursive procedure of generating k best compressions at every node is partly inspired by frame semantics (Fillmore et al., 2003) and its extension from predicates to any node type (Titov & Klementiev, 2011). The core idea is that there are two components to a high-quality compression at every node in the tree: (1) it should keep all the essential arguments of that node; (2) these arguments should themselves be good compressions. This motivates an algorithm with a recursively defined scoring function which allows us to obtain k-best compressions nearly as fast as the single best one. In this respect our algorithm is similar to the k-best parsing algorithm by Huang & Chiang (2005)."}, {"heading": "2 The Top-down Approach", "text": "Our approach is syntax-driven and operates on dependency trees (Sec. 2.1). The input tree is pruned to obtain a valid subtree from which a compression is read off. The pruning decisions are carried out based on predictions of a maximum entropy classifier which is trained on a parallel corpora with a rich feature set (Sec. 2.2). Section 2.3 explains how to generate the single, top-scoring compression; Section 2.4 extends the idea to arbitrary k."}, {"heading": "2.1 Preprocessing", "text": "Similar to previous work, we have a special treatment for function words like determiners, prepositions, auxiliary verbs. Unsurprisingly, dealing with function words is much easier than deciding whether a content word can be removed. Approaches which use a constituency parser and prune edges pointing to constituents, deal with function words implicitly (Berg-Kirkpatrick et al., 2011; Wang et al., 2013). Approaches which use a dependency representation either formulate hard constraints (Almeida & Martins, 2013), or collapse function words with their heads. We use the latter approach and transform every input tree (Nivre, 2006) following Filippova & Strube (2008) and also add edges from the dummy root to finite verbs. Finally, we run an entity tagger and collapse nodes referring to entities.\nFigure 1 provides an example of a transformed tree with extra edges from the dummy root node and an undirected coreference edge for the following sentence to which we will refer throughout this section:\n(1) The police said the man who robbed a bank in Arizona was arrested at his home late Friday."}, {"heading": "2.2 Estimating deletion probabilities", "text": "The supervised component of our system is a binary maximum entropy classifier (Berger et al., 1996) which is trained to estimate the probability of deleting an edge given its local context. In what follows, we are going to refer to two probabilities, pdel(e) and pret(e):\npdel(en,m) + pret(en,m) = 1, (1)\nwhere del stands for deleting, ret stands for retaining edge e from node n to node m, and pdel(en,m) is estimated with MaxEnt.\nThe features we use are inspired by most recent work (Almeida & Martins, 2013; Filippova & Altun, 2013; Wang et al., 2013) and are as follows:\nsyntactic: edge labels for the child and its siblings; NE type and PoS tags;\nlexical: head and child lemmas; negation; concatenation of parent lemmas and labels;\nnumeric: depth; node length in words and characters; children count for the parent and the child.\nNote that no feature refers to the compression generated so far and therefore the probability of removing an edge needs to be calculated only once on a first tree traversal.\nAssuming that we have a training set comprising pairs of a transformed tree, like the one in Figure 1, and a compression subtree (e.g., the subtree covering all the nodes from the man to at his home),\nthe compression subtrees provide all the negative items for training (blue edges in Fig. 1). The positive items are all other edges originating from the nodes in the compression (red edges). The remaining edges (black) cannot be used for training.\nAlthough we chose to implement hard constraints for function words (see Sec. 2.1 above), we could also apply no tree transformations and instead expect the classifier to learn that, e.g., the probability of deleting an edge pointing to a determiner is zero. However, given the universality of these rules, it made more sense to us to encode them as preprocessing transformations."}, {"heading": "2.3 Obtaining top-scoring compression", "text": "To find the best compression of the sentence we start at the dummy root node and select a child n with the highest pret(eroot,n). The root of the example tree in Figure 1 has three children (said2, robbed6, was arrested12). Assuming that pret\u2019s for the three predicates are .07, .5, .9, the third child is selected. From there, we recursively continue in a top-down manner and at every node nwhose children areM = {m1,m2, ...} search for a children subset Cn \u2286 M maximizing\nscore(Cn) = \u2211\nm\u2208M\\Cn\nlog pdel(en,m)\n+ \u2211\nm\u2208Cn\nlog pret(en,m). (2)\nSince pdel and pret sum to one, this implies that every edge with pret < 0.5 is deleted. However, we can take any \u03c1 \u2208 [0, 1] to be a threshold for deciding between keeping vs. deleting an edge and linearly scale pdel and pret so that after scaling p\u0302del < 0.5 if and only if pdel < \u03c1. Of course, finding a single \u03c1 value that would be universally optimal is hardly possible and we will return to this point in Sec. 3.\nConsider the node was arrested in Figure 1 and its three children listed in Table 1 with pret given in brackets.\nWith \u03c1 = 0.5, the top scoring subset is C12 = {4}, its score being 0 + log .78 + log .95. The next step is to decide whether node 4 (the man) should retain its relative clause modifier or not. There is no need to go further down the Friday node and consider the score of its sole argument (late).\n2.4 From top-scoring to k-best\nA single best compression may appear too long or too short, or fail to satisfy some other requirement. In many cases it is desirable to have a pool of k-best results to choose from and in this subsection we will present our algorithm for efficiently generating a kbest list (summarized in Fig. 2).\nFirst, let us slightly modify the notation used up to this point to be able to refer to the kth best result at node n. Instead of Cn \u2286 M , we are going to use Ckn, where k \u2208 N \u222a {\u22121}. Unlike Cn, every Ckn is an ordered sequence of exactly |M | elements, corresponding to n\u2019s children:\nCkn = [C k1 m1 , C k2 m2 , ..., C k|M| m|M| ]. (3)\nFor every child mi not retained in the compression, the superscript ki is -1. For example, for the singleton subset C12 containing only node 4 in the previous subsection the corresponding best result C012 is:\nC012 = [C 0 4 , C \u22121 15 , C \u22121 17 ]. (4)\nNote that at this point we do not need to know what C04 actually is. We simply state that the best result for node 12 must include the best result for node 4.\nThe scoring function for Ckn is the averaged sum of the scores of n\u2019s chlidren and must be decreasing over k \u2265 0 (score(Ck+an ) \u2264 score(Ckn), a > 0):\nscore(Ckn) = 1 |M | \u2211\nC ki mi \u2208Ckn\nscore(Ckimi). (5)\nWhen k \u2208 {\u22121, 0}, i.e., when we either delete a child or take its best compression, the score is the familiar probabilities:\nscore(Ckm) = { log pdel(en,m) if k = \u22121 log pret(en,m) if k = 0 (6)\nGreater values of k correspond to k+1\u2019th best result at node n. Consider again node 12 from Table 1. The k-best results at that node may include any of the following variants (the list is not complete): [C04 , C \u22121 15 , C \u22121 17 ], [C 0 4 , C 0 15, C \u22121 17 ], [C 2 4 , C \u22121 15 , C 0 17],\n[C14 , C \u22121 15 , C 1 17], [C 0 4 , C 0 15, C 1 17] [C \u22121 4 , C 0 15, C 0 17].\nHow should these be scored so that high quality compressions are ranked higher? Our assumption is that the quality of a compression at any node is subject to the following two conditions:\n1. The child subset includes essential arguments and does not include those that can be omitted.\n2. The variants for the children retained in the compression are themselves high-quality compressions.\nFor example, a compression at node 12 which deletes the first child (the man) is of a poor quality because it misses the subject and thus violates the first condition. A compression which retains the first node but with a misleading compression, like the man robbed in Arizona (Ck4 = [C l 6], C l 6 = [C\u221215 , C \u22121 8 , C 0 10]), is not good either because it violates the second condition, which is in turn due to the first condition being violated in C l6. Hence, a robust scoring function should balance these two considerations and promote variants with good compression at every node retained. Note that for finding the single best result it is sufficient to focus on the first condition only, ignoring the second one, because the best possible result is returned for every child, and the scoring function in Eq. 2 does exactly that. However, once we begin to generate more than a single best result, we start including compressions which may no longer be optimal. So the main challenge in extending the scoring function lies in how to propagate the scores from node\u2019s descendants so that both conditions are satisfied.\nGiven the best result at node n, which is obtained in a single pass (Sec. 2.3), the second best result must be one of the following:\n\u2022 The next best scoring child subset whose score we know how compute from Eq. (5-6) (e.g., for node 12 it would be [C04 , C 0 15, C \u22121 17 ], see Eq. 4).\n\u2022 A subset of the same children as the best one but with one of ki\u2019s which were 0 in the best result increased to 1 (e.g., for node 12 it would be [C14 , C \u22121 15 , C \u22121 17 ], see Eq. 4):\nNo other variant can have a higher score than either of these. Unless there is a tie in the scores, there is a single new second-best subset. And it follows from the decreasing property and the definition of the scoring function that if more than a single ki is increased from zero, the score is lower than when only one of the ki\u2019s is modified. For example, score([C24 , C \u22121 15 , C 1 17]) \u2264 score([C04 , C \u22121 15 , C 0 17]) \u2264 score([C04 , C 0 15, C \u22121 17 ]), the latter comparison is between two new subsets whose scores can be computed directly from Eq. (5-6). Hence, the second best result C1n is either the next best subset, or one of the at most |M | candidates.\nAssuming that kj = 0 in the best result, the score of candidate C k\u2217j n generated from C0n by incrementing kj is defined as\nscore(C k\u2217j n ) = score(C 0 n) +\nscore(C0+1mj )\n|M | . (7)\nGeneralizing to an arbitrary k, the k+1\u2019th result is also either an unseen subset, whose score is defined in Eq. 5, or it can be obtained by increasing a ki from a non-zero value in one of the k-best results generated so far. Given a Ckn, the score of a candidate generated by incrementing the value of kj is:\nscore(C k\u2217j n ) = score(C k n) + 1\n|M | score(Ckj+1mj )\n\u2212 { 0 if kj = 0, 1 |M |score(C kj mj ) if kj > 0. (8)\nNotice the similarity between Eq. 7 and Eq. 8. The difference is that when we explore candidates of kj\u2019s greater than zero, we replace the contribution of mj\u2019th child: the kj\u2019th best score is replaced with kj +1\u2019th best score. However, the edge score (C0mj ) is never taken out of the total score of Ckn. This is motivated by the first of the two conditions above. As an illustratation to this point, consider the predicate from Table 1 one more time and assume that\npret(e17,16) = 0.4, i.e., the probability of late being the argument of Friday is 0.4. The information that the temporal modifier (node 17) is an argument with a very low score should not disappear from the subsequent scorings of node 12\u2019s candidates. Otherwise a subsequent result may get a higher score than the best one, violating the decreasing property of the scoring function, as the final line below shows:\n[C04 , C \u22121 15 , C \u22121 17 ] (0 + log .78 + log .95)/3 [C04 , C \u22121 15 , C 0 17] (0 + log .78 + log .05)/3 [C04 , C \u22121 15 , C 1 17] (0 + log .78 + log .05 + log .4)/3 [C04 , C \u22121 15 , C 1 17] \u2217 (0 + log .78 + log .4)/3.\nTo sum up, we have defined a monotonically decreasing scoring function and outlined our algorithm for generating k-best compressions (see Fig. 2). As at every request the pool of candidates for node n is extended by not more than |M | + 1 candidates, the complexity of the algorithm is O(k \u00d7 N \u00d7 m) (k times node count times the average branching factor)."}, {"heading": "3 Adding a Node Subset Scorer", "text": "On the first pass, the top-down compressor attempts to find the best possible children subset of every node by considering every child separately and making the retain-or-delete decisions independently of one another. How conservative or aggressive the algorithm is, is determined by a single parameter \u03c1 \u2208 [0, 1] which places a boundary between the two decisions. With smaller values of \u03c1 a low probability of deletion (pdel) would suffice for a node child to be removed. Conversely, a greater value of \u03c1 would mean that only children about which the classifier is fairly certain that they must be deleted would be removed.\nUnsurprisingly, the value of \u03c1 is hard to optimize as it may be too low or too high, depending on a node. While retaining a child which could be dropped would not result in an ungrammatical sentence, omitting an important argument may make the compression incomprehensible. When doing an error analysis on a development set, we did not encounter many cases where the compression was clearly ungrammatical due to a wrongly omitted argument. However, results like that do have a high cost and thus need to be addressed. Consider the following example:\n(2) Yesterday the world was ablaze with the news that the CEO will step down.\nIn this sentence, ablaze is analyzed as an adverbial modifier of the verb to be and the classifier assigns a score of 0.35 to the edge pointing to ablaze. With a decision boundary above 0.35, the meaningful part of the predicate is deleted and the compression becomes incomplete. With the boundary at 0.5, the top scoring subset is a singleton containing only the subject. However, there are hardly any cases where the verb to be has a single argument, and our algorithm could benefit from this knowledge.\nIn the extended model, the score of a children subset (Eq. 5) gets an additional summand, log p(|Ckn|), where |Ckn| refers to the number of n\u2019s children actually retained in Ckn, i.e., with ki \u2265 0:\nscore\u2217(Ckn) = log p(|Ckn|)+ 1 |M | \u2211\nC ki mi \u2208Ckn\nscore(Ckimi).\n(9) Unfortunately, with the updated formula, we can no longer generate k-best compressions as efficiently as before. However, we can keep a beam of b subset candidates for every node and select the one maximizing the new score.\nTo estimate the probability of a children subset size after compression, p(|Ckn|), we use an averaged perceptron implementation (Freund & Shapire, 1999) and the features described in Sec. 2.2. We do not differentiate between sizes greater than four and have five classes in total (0, 1, 2, 3, 4+)."}, {"heading": "4 Evaluation", "text": "The purpose of the evaluation is to validate the following two hypotheses, when comparing the new algorithm with a competitive ILP-based sentence compressor (Filippova & Altun, 2013):\n1. The top-down algorithm was designed to perform local decisions at each node in the parse tree, as compared to the global optimization carried out by the ILP-based compressor. We want to verify whether the local model can attain similar accuracy levels or even outperform the global model, and do so not only for the single best but the top k results.\n2. Automatic ILP optimization can be quite slow when the number of candidates that need to be evaluated for any given input is large. We want to quantify the speed-up that can be attained without a loss in accuracy by taking simpler, local decisions in the input parse tree."}, {"heading": "4.1 Evaluation settings", "text": "Training, development and test set The aligned sentences and compressions were collected using the procedure described in Filippova & Altun (2013). The training set comprises 1,800,000 items, each item consisting of two elements: the first sentence in a news article and an extractive compression obtained by matching content words from the sentence with those from the headline (see Filippova & Altun (2013) for the technical details). A part of this set was held out for classifiers evaluation and development. For testing, we use the dataset released by Filippova & Altun (2013)1. This test set contains 10,000 items, each of which includes the original sentence and the extractive compression and the URL of the source document. From this set, we used the first 1,000 items only, leaving the remaining 9,000 items unseen, reserved for possible future experiments. We made sure that our training set does not include any of the sentences from the test set.\nThe training set provided us with roughly 16 million edges for training MaxEnt with 40% of positive examples (deleted edges). For training the perceptron classifier we had about 6 million nodes at our disposal with the instances distributed over the five classes as follows:\n0 1 2 3 4+"}, {"heading": "19.5% 40.6% 31.2% 7.9% 1%", "text": "Baseline We used the recent ILP-based algorithm of Filippova & Altun (2013) as a baseline. We trained the compressor with all the same features as our model (Sec. 2.2) on the same training data using an averaged perceptron (Collins, 2002). To make this system comparable to ours, when training the model, we did not provide the ILP decoder with the oracle compression length so that the model learned to produce compressions in the absense of length argument. Thus, both methods accept the same input\n1http://storage.googleapis.com/sentencecomp/compressiondata.json\nand are comparable."}, {"heading": "4.2 Automatic evaluation", "text": "To measure the quality of the two classifiers (MaxEnt from Sec. 2.2 and perceptron from Sec. 3), we performed a first, direct evaluation of each of them on a small held out portion of the training set. The MaxEnt classifier predicts the probability of deleting an edge and outputs a score between zero and one. Figure 3 plots precision, recall and F1-score at different threshold values. The highest F1-score is obtained at 0.45. Regarding the perceptron classifier that predicts the number of children that we should retain for each node, its accuracy and per-class precision and recall values are given in Table 2.\nFor an automatic evaluation of the quality of the sentence compressions, we followed the same approach as (Riezler et al., 2003; Filippova & Altun, 2013) and measured F1-score by comparing the trees of the generated compressions to the golden, extractive compression. Table 3 shows the results of the ILP baseline and the two variants of the Topdown approach on the test data (TOP-DOWN + NSS is the extended variant described in Sec. 3). The NSS version, which incorporates a prediction on the number of children to keep for each node, is slightly better than the original Top-down approach, but the\nresults are not statistically significant.\nIt is important to point out the difference in compression rates between ILP and TOP-DOWN: 47% vs. 38% (the average compression rate on the test set is 40.5%). Despite a significant advantage due to compression rate (Napoles et al., 2011, see next subsection), ILP performs slightly worse than the proposed methods.\nFinally, Table 4 shows the results when computing the F1-score for each of the top-5 compressions as generated by the Top-down algorithms. As can be seen, in both cases there is a sharp drop between the top two compressions but further scores are very close. Since the test set only contains a single oracle compression for every sentence, to understand how big the gap in quality really is, we need an evaluation with human raters."}, {"heading": "4.3 Manual evaluation", "text": "The first 100 items in the test data were manually rated by humans. We asked raters to rate both readability and informativeness of the compressions for the golden output, the baseline and our systems2. For both metrics a 5-point Likert scale was used, and three ratings were collected for every item. Note that in a human evaluation between ILP and TOP-DOWN (+ NSS) the baseline has an advantage because (1) it prunes less aggressively and thus has more chances of producing a grammaticaly correct and informative outputs, and (2) it gets a hint to the optimal compression length in edges. We have used IntraClass Correlation (ICC) (Shrout & Fleiss, 1979;\n2The evaluation template and rated sentences are included in the supplementary material.\nCicchetti, 1994) as a measure of inter-judge agreement. ICC for readability was 0.59 (95% confidence interval [0.56, 0.62]) and for informativeness it was 0.51 (95% confidence interval [0.48, 0.54]), indicating fair reliability in both cases.\nResults are shown in Tables 5 and 6. As in the automatic evaluations, the two Top-down systems produced indistinguishable results, but both are significantly better than the ILP baseline at 95% confidence. The top-down results are also now indistinguishable from the extractive compressions."}, {"heading": "4.4 Efficiency", "text": "The average per-sentence processing time is 32,074 microseconds (Intel Xeon machine with 2.67 GHz CPU) using ILP, 929 using TOP-DOWN + NSS, and 678 using TOP-DOWN. This means that we have obtained almost a 50x performance increase over ILP. Figure 4 shows the processing time for each of the 1,000 sentences in the test set with sentence length measured in tokens.\nFor obtaining k-best solutions, the decrease in time is even more remarkable: the average time for generating each of the top-5 compressions using ILP is 42,213 microseconds, greater than that of the single best result. Conversely, the average time for each of the top-5 results decreases to 143 microsec-\nonds using TOP-DOWN, and 195 microseconds using TOP-DOWN + NSS, which means a 300x improvement. The reason is that the Top-down methods, in order to produce the top-ranked compression, have already computed all the per-edge predictions (and the per-node NSS predictions in the case of TOP-DOWN + NSS), and generating the next best solutions is cheap."}, {"heading": "5 Conclusions", "text": "We presented a fast and accurate supervised algorithm for generating k-best compressions of a sentence. Compared with a competitive ILP-based system, our method is 50x faster in generating the best result and 300x faster for subsequent k-best compressions. Quality-wise it is better both in terms of readability and informativeness. Moreover, an evaluation with human raters demonstrates that the quality of the output remains high for the top-5 results."}], "references": [{"title": "Jointly learning to extract and compress", "author": ["T. Berg-Kirkpatrick", "D. Gillick", "D. Klein"], "venue": "In Proc. of ACL-11", "citeRegEx": "Berg.Kirkpatrick et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2011}, {"title": "A maximum entropy approach to natural language processing", "author": ["A. Berger", "S.A. Della Pietra", "V.J. Della Pietra"], "venue": null, "citeRegEx": "Berger et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Berger et al\\.", "year": 1996}, {"title": "Guidelines, criteria, and rules of thumb for evaluating normed and standardized assessment instruments in psychology", "author": ["D.V. Cicchetti"], "venue": "Psychological Assessment,", "citeRegEx": "Cicchetti,? \\Q1994\\E", "shortCiteRegEx": "Cicchetti", "year": 1994}, {"title": "Constraint-based sentence compression: An integer programming approach", "author": ["J. Clarke", "M. Lapata"], "venue": "In Proc. of COLING-ACL-06 Poster Session,", "citeRegEx": "Clarke and Lapata,? \\Q2006\\E", "shortCiteRegEx": "Clarke and Lapata", "year": 2006}, {"title": "Global inference for sentence compression: An integer linear programming approach", "author": ["J. Clarke", "M. Lapata"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Clarke and Lapata,? \\Q2008\\E", "shortCiteRegEx": "Clarke and Lapata", "year": 2008}, {"title": "Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms", "author": ["M. Collins"], "venue": "In Proc. of EMNLP-02,", "citeRegEx": "Collins,? \\Q2002\\E", "shortCiteRegEx": "Collins", "year": 2002}, {"title": "Overcoming the lack of parallel data in sentence compression", "author": ["K. Filippova", "Y. Altun"], "venue": "In Proc. of EMNLP-13,", "citeRegEx": "Filippova and Altun,? \\Q2013\\E", "shortCiteRegEx": "Filippova and Altun", "year": 2013}, {"title": "Dependency tree based sentence compression", "author": ["K. Filippova", "M. Strube"], "venue": "In Proc. of INLG08,", "citeRegEx": "Filippova and Strube,? \\Q2008\\E", "shortCiteRegEx": "Filippova and Strube", "year": 2008}, {"title": "An extractive supervised two-stage method for sentence compression", "author": ["D. Galanis", "I. Androutsopoulos"], "venue": "In Proc. of NAACL-HLT-10,", "citeRegEx": "Galanis and Androutsopoulos,? \\Q2010\\E", "shortCiteRegEx": "Galanis and Androutsopoulos", "year": 2010}, {"title": "Lexicalized Markov grammars for sentence compression", "author": ["M. Galley", "K.R. McKeown"], "venue": "In Proc. of NAACL-HLT-07,", "citeRegEx": "Galley and McKeown,? \\Q2007\\E", "shortCiteRegEx": "Galley and McKeown", "year": 2007}, {"title": "Producing intelligent telegraphic text reduction to provide an audio scanning service for the blind", "author": ["G. Grefenstette"], "venue": "In Working Notes of the Workshop on Intelligent Text Summarization, Palo Alto, Cal.,", "citeRegEx": "Grefenstette,? \\Q1998\\E", "shortCiteRegEx": "Grefenstette", "year": 1998}, {"title": "Better k-best parsing", "author": ["L. Huang", "D. Chiang"], "venue": "Technical Report MS-CIS-05-08: University of Pennsylvania", "citeRegEx": "Huang and Chiang,? \\Q2005\\E", "shortCiteRegEx": "Huang and Chiang", "year": 2005}, {"title": "Cut and paste based text summarization", "author": ["H. Jing", "K. McKeown"], "venue": "In Proc. of NAACL-00,", "citeRegEx": "Jing and McKeown,? \\Q2000\\E", "shortCiteRegEx": "Jing and McKeown", "year": 2000}, {"title": "Statistics-based summarization \u2013 step one: Sentence compression", "author": ["K. Knight", "D. Marcu"], "venue": "In Proc. of AAAI-00,", "citeRegEx": "Knight and Marcu,? \\Q2000\\E", "shortCiteRegEx": "Knight and Marcu", "year": 2000}, {"title": "Summarization with a joing model for sentence extraction and compression", "author": ["A.F.T. Martins", "N.A. Smith"], "venue": "In ILP for", "citeRegEx": "Martins and Smith,? \\Q2009\\E", "shortCiteRegEx": "Martins and Smith", "year": 2009}, {"title": "Discriminative sentence compression with soft syntactic evidence", "author": ["R. McDonald"], "venue": "In Proc. of EACL-06,", "citeRegEx": "McDonald,? \\Q2006\\E", "shortCiteRegEx": "McDonald", "year": 2006}, {"title": "Evaluating sentence compression: Pitfalls and suggested remedies", "author": ["C. Napoles", "C. Callison-Burch", "B. Van Durme"], "venue": "In Proceedings of the Workshop on Monolingual Text-to-text Generation,", "citeRegEx": "Napoles et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Napoles et al\\.", "year": 2011}, {"title": "Inductive Dependency Parsing", "author": ["J. Nivre"], "venue": null, "citeRegEx": "Nivre,? \\Q2006\\E", "shortCiteRegEx": "Nivre", "year": 2006}, {"title": "A comparison of model free versus model intensive approaches to sentence compression", "author": ["T. Nomoto"], "venue": "In Proc. of EMNLP-09,", "citeRegEx": "Nomoto,? \\Q2009\\E", "shortCiteRegEx": "Nomoto", "year": 2009}, {"title": "Fast joint compression and summarization via graph cuts", "author": ["X. Qian", "Y. Liu"], "venue": "In Proc. of EMNLP-13,", "citeRegEx": "Qian and Liu,? \\Q2013\\E", "shortCiteRegEx": "Qian and Liu", "year": 2013}, {"title": "Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for Lexical-Functional Grammar", "author": ["S. Riezler", "T.H. King", "R. Crouch", "A. Zaenen"], "venue": "In Proc. of HLT-NAACL-03,", "citeRegEx": "Riezler et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Riezler et al\\.", "year": 2003}, {"title": "Intraclass correlations: Uses in assessing rater reliability", "author": ["P.E. Shrout", "J.L. Fleiss"], "venue": "Psychological bulletin,", "citeRegEx": "Shrout and Fleiss,? \\Q1979\\E", "shortCiteRegEx": "Shrout and Fleiss", "year": 1979}, {"title": "Approximating strategies for multi-structure sentence compression", "author": ["K. Thadani"], "venue": "In Proc. of ACL-14,", "citeRegEx": "Thadani,? \\Q2014\\E", "shortCiteRegEx": "Thadani", "year": 2014}, {"title": "Sentence compression with joint structural inference", "author": ["K. Thadani", "K. McKeown"], "venue": "In Proc. of CoNLL-13,", "citeRegEx": "Thadani and McKeown,? \\Q2013\\E", "shortCiteRegEx": "Thadani and McKeown", "year": 2013}, {"title": "A Bayesian model for unsupervised semantic parsing", "author": ["I. Titov", "A. Klementiev"], "venue": "In Proc. of ACL-11,", "citeRegEx": "Titov and Klementiev,? \\Q2011\\E", "shortCiteRegEx": "Titov and Klementiev", "year": 2011}, {"title": "The Pythy summarization system: Microsoft Research at DUC", "author": ["K. Toutanova", "C. Brockett", "M. Gamon", "J. Jagarlamundi", "H. Suzuki", "L. Vanderwende"], "venue": "In Proc. of DUC-07", "citeRegEx": "Toutanova et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2007}, {"title": "A sentence compression based framework to query-focused multidocument summarization", "author": ["L. Wang", "H. Raghavan", "V. Castelli", "R. Florian", "C. Cardie"], "venue": "In Proc. of ACL-13,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Title generation with Quasi-Synchronous Grammar", "author": ["K. Woodsend", "Y. Feng", "M. Lapata"], "venue": "In Proc. of EMNLP-10,", "citeRegEx": "Woodsend et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Woodsend et al\\.", "year": 2010}, {"title": "Multiple aspect summarization using Integer Linear Programming", "author": ["K. Woodsend", "M. Lapata"], "venue": "In Proc. of EMNLP-12,", "citeRegEx": "Woodsend and Lapata,? \\Q2012\\E", "shortCiteRegEx": "Woodsend and Lapata", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Given the complexity of the compression task (the number of possible outputs is exponential), many systems frame it, sometimes combined with summarization, as an ILP problem which is then solved with off-the-shelf tools (Martins & Smith, 2009; Berg-Kirkpatrick et al., 2011; Thadani & McKeown, 2013).", "startOffset": 220, "endOffset": 299}, {"referenceID": 26, "context": "Importantly, approximate search techniques like beam search (Galanis & Androutsopoulos, 2010; Wang et al., 2013), are not required.", "startOffset": 60, "endOffset": 112}, {"referenceID": 26, "context": "While some methods rewrite the source tree and produce an alternative derivation at every consituent (Knight & Marcu, 2000; Galley & McKeown, 2007), others prune edges in the source tree (Filippova & Strube, 2008; Galanis & Androutsopoulos, 2010; Wang et al., 2013).", "startOffset": 187, "endOffset": 265}, {"referenceID": 27, "context": "Several text-to-text generation systems use ILP as an optimization tool to generate new sentences by combining pieces from the input (Clarke & Lapata, 2008; Martins & Smith, 2009; Woodsend et al., 2010; Filippova & Altun, 2013).", "startOffset": 133, "endOffset": 227}, {"referenceID": 0, "context": "While off-theshelf general purpose LP solvers are designed to be fast, in practice they may make the compressor prohibitively slow, in particular if compression is done jointly with summarization (Berg-Kirkpatrick et al., 2011; Qian & Liu, 2013; Thadani, 2014).", "startOffset": 196, "endOffset": 260}, {"referenceID": 22, "context": "While off-theshelf general purpose LP solvers are designed to be fast, in practice they may make the compressor prohibitively slow, in particular if compression is done jointly with summarization (Berg-Kirkpatrick et al., 2011; Qian & Liu, 2013; Thadani, 2014).", "startOffset": 196, "endOffset": 260}, {"referenceID": 0, "context": "Approaches which use a constituency parser and prune edges pointing to constituents, deal with function words implicitly (Berg-Kirkpatrick et al., 2011; Wang et al., 2013).", "startOffset": 121, "endOffset": 171}, {"referenceID": 26, "context": "Approaches which use a constituency parser and prune edges pointing to constituents, deal with function words implicitly (Berg-Kirkpatrick et al., 2011; Wang et al., 2013).", "startOffset": 121, "endOffset": 171}, {"referenceID": 17, "context": "We use the latter approach and transform every input tree (Nivre, 2006) following Filippova & Strube (2008) and also add edges from the dummy root to finite verbs.", "startOffset": 58, "endOffset": 71}, {"referenceID": 9, "context": "Comparison to related work Many compression systems have been introduced since the very first approaches by Grefenstette (1998), Jing & McKeown (2000) and Knight & Marcu (2000).", "startOffset": 108, "endOffset": 128}, {"referenceID": 9, "context": "Comparison to related work Many compression systems have been introduced since the very first approaches by Grefenstette (1998), Jing & McKeown (2000) and Knight & Marcu (2000).", "startOffset": 108, "endOffset": 151}, {"referenceID": 9, "context": "Comparison to related work Many compression systems have been introduced since the very first approaches by Grefenstette (1998), Jing & McKeown (2000) and Knight & Marcu (2000). Almost all of them make use of syntactic information (e.", "startOffset": 108, "endOffset": 177}, {"referenceID": 9, "context": "Comparison to related work Many compression systems have been introduced since the very first approaches by Grefenstette (1998), Jing & McKeown (2000) and Knight & Marcu (2000). Almost all of them make use of syntactic information (e.g., Clarke & Lapata (2006), McDonald (2006), Toutanova et al.", "startOffset": 108, "endOffset": 261}, {"referenceID": 9, "context": "Comparison to related work Many compression systems have been introduced since the very first approaches by Grefenstette (1998), Jing & McKeown (2000) and Knight & Marcu (2000). Almost all of them make use of syntactic information (e.g., Clarke & Lapata (2006), McDonald (2006), Toutanova et al.", "startOffset": 108, "endOffset": 278}, {"referenceID": 9, "context": "Comparison to related work Many compression systems have been introduced since the very first approaches by Grefenstette (1998), Jing & McKeown (2000) and Knight & Marcu (2000). Almost all of them make use of syntactic information (e.g., Clarke & Lapata (2006), McDonald (2006), Toutanova et al. (2007)), and our system is not an exception.", "startOffset": 108, "endOffset": 303}, {"referenceID": 9, "context": "Comparison to related work Many compression systems have been introduced since the very first approaches by Grefenstette (1998), Jing & McKeown (2000) and Knight & Marcu (2000). Almost all of them make use of syntactic information (e.g., Clarke & Lapata (2006), McDonald (2006), Toutanova et al. (2007)), and our system is not an exception. Like Nomoto (2009), Wang et al.", "startOffset": 108, "endOffset": 360}, {"referenceID": 9, "context": "Comparison to related work Many compression systems have been introduced since the very first approaches by Grefenstette (1998), Jing & McKeown (2000) and Knight & Marcu (2000). Almost all of them make use of syntactic information (e.g., Clarke & Lapata (2006), McDonald (2006), Toutanova et al. (2007)), and our system is not an exception. Like Nomoto (2009), Wang et al. (2013) we operate on syntactic trees provided by a stateof-the-art parser.", "startOffset": 108, "endOffset": 380}, {"referenceID": 0, "context": "While off-theshelf general purpose LP solvers are designed to be fast, in practice they may make the compressor prohibitively slow, in particular if compression is done jointly with summarization (Berg-Kirkpatrick et al., 2011; Qian & Liu, 2013; Thadani, 2014). Recent improvements to the ILP-based methods have been significant but not dramatic. For example, Thadani (2014) presents an approximation technique resulting in a 60% reduction in average inference time.", "startOffset": 197, "endOffset": 375}, {"referenceID": 0, "context": "While off-theshelf general purpose LP solvers are designed to be fast, in practice they may make the compressor prohibitively slow, in particular if compression is done jointly with summarization (Berg-Kirkpatrick et al., 2011; Qian & Liu, 2013; Thadani, 2014). Recent improvements to the ILP-based methods have been significant but not dramatic. For example, Thadani (2014) presents an approximation technique resulting in a 60% reduction in average inference time. Compared with this work, the main practical advantage of our system is that it is very fast without trading compression quality for speed improvements. On the modeling side, it demonstrates that local decisions are sufficient to produce an informative and grammatically correct sentence. Our recursive procedure of generating k best compressions at every node is partly inspired by frame semantics (Fillmore et al., 2003) and its extension from predicates to any node type (Titov & Klementiev, 2011). The core idea is that there are two components to a high-quality compression at every node in the tree: (1) it should keep all the essential arguments of that node; (2) these arguments should themselves be good compressions. This motivates an algorithm with a recursively defined scoring function which allows us to obtain k-best compressions nearly as fast as the single best one. In this respect our algorithm is similar to the k-best parsing algorithm by Huang & Chiang (2005). 2 The Top-down Approach Our approach is syntax-driven and operates on dependency trees (Sec.", "startOffset": 197, "endOffset": 1448}, {"referenceID": 0, "context": "While off-theshelf general purpose LP solvers are designed to be fast, in practice they may make the compressor prohibitively slow, in particular if compression is done jointly with summarization (Berg-Kirkpatrick et al., 2011; Qian & Liu, 2013; Thadani, 2014). Recent improvements to the ILP-based methods have been significant but not dramatic. For example, Thadani (2014) presents an approximation technique resulting in a 60% reduction in average inference time. Compared with this work, the main practical advantage of our system is that it is very fast without trading compression quality for speed improvements. On the modeling side, it demonstrates that local decisions are sufficient to produce an informative and grammatically correct sentence. Our recursive procedure of generating k best compressions at every node is partly inspired by frame semantics (Fillmore et al., 2003) and its extension from predicates to any node type (Titov & Klementiev, 2011). The core idea is that there are two components to a high-quality compression at every node in the tree: (1) it should keep all the essential arguments of that node; (2) these arguments should themselves be good compressions. This motivates an algorithm with a recursively defined scoring function which allows us to obtain k-best compressions nearly as fast as the single best one. In this respect our algorithm is similar to the k-best parsing algorithm by Huang & Chiang (2005). 2 The Top-down Approach Our approach is syntax-driven and operates on dependency trees (Sec. 2.1). The input tree is pruned to obtain a valid subtree from which a compression is read off. The pruning decisions are carried out based on predictions of a maximum entropy classifier which is trained on a parallel corpora with a rich feature set (Sec. 2.2). Section 2.3 explains how to generate the single, top-scoring compression; Section 2.4 extends the idea to arbitrary k. 2.1 Preprocessing Similar to previous work, we have a special treatment for function words like determiners, prepositions, auxiliary verbs. Unsurprisingly, dealing with function words is much easier than deciding whether a content word can be removed. Approaches which use a constituency parser and prune edges pointing to constituents, deal with function words implicitly (Berg-Kirkpatrick et al., 2011; Wang et al., 2013). Approaches which use a dependency representation either formulate hard constraints (Almeida & Martins, 2013), or collapse function words with their heads. We use the latter approach and transform every input tree (Nivre, 2006) following Filippova & Strube (2008) and also add edges from the dummy root to finite verbs.", "startOffset": 197, "endOffset": 2610}, {"referenceID": 1, "context": "2 Estimating deletion probabilities The supervised component of our system is a binary maximum entropy classifier (Berger et al., 1996) which is trained to estimate the probability of deleting an edge given its local context.", "startOffset": 114, "endOffset": 135}, {"referenceID": 26, "context": "The features we use are inspired by most recent work (Almeida & Martins, 2013; Filippova & Altun, 2013; Wang et al., 2013) and are as follows: syntactic: edge labels for the child and its siblings; NE type and PoS tags; lexical: head and child lemmas; negation; concatenation of parent lemmas and labels; numeric: depth; node length in words and characters; children count for the parent and the child.", "startOffset": 53, "endOffset": 122}, {"referenceID": 5, "context": "2) on the same training data using an averaged perceptron (Collins, 2002).", "startOffset": 58, "endOffset": 73}, {"referenceID": 20, "context": "For an automatic evaluation of the quality of the sentence compressions, we followed the same approach as (Riezler et al., 2003; Filippova & Altun, 2013) and measured F1-score by comparing the trees of the generated compressions to the golden, extractive compression.", "startOffset": 106, "endOffset": 153}], "year": 2015, "abstractText": "A popular approach to sentence compression is to formulate the task as a constrained optimization problem and solve it with integer linear programming (ILP) tools. Unfortunately, dependence on ILP may make the compressor prohibitively slow, and thus approximation techniques have been proposed which are often complex and offer a moderate gain in speed. As an alternative solution, we introduce a novel compression algorithm which generates k-best compressions relying on local deletion decisions. Our algorithm is two orders of magnitude faster than a recent ILPbased method while producing better compressions. Moreover, an extensive evaluation demonstrates that the quality of compressions does not degrade much as we move from single best to top-five results.", "creator": "TeX"}}}