{"id": "1510.08865", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2015", "title": "Mixed Robust/Average Submodular Partitioning: Fast Algorithms, Guarantees, and Applications to Parallel Machine Learning and Multi-Label Image Segmentation", "abstract": "We investigate two novel mixed robust/average-case submodular data partitioning problems that we collectively call \\emph{Submodular Partitioning}. These problems generalize purely robust instances of the problem, namely \\emph{max-min submodular fair allocation} (SFA) and \\emph{min-max submodular load balancing} (SLB), and also average-case instances, that is the \\emph{submodular welfare problem} (SWP) and \\emph{submodular multiway partition} (SMP). While the robust versions have been studied in the theory community, existing work has focused on tight approximation guarantees, and the resultant algorithms are not generally scalable to large real-world applications. We present a framework to guide implementation of this framework by using a single system for determining whether we expect large-scale SFP to be applied in future cases. Using this framework we report an implementation of SFP that allows for a large-scale SFP for scaling down the SFP for a large-scale SFP for larger-scale SFP. For example, the scaling limit in the SCFS for large SFP, for small SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for large SFP, for", "histories": [["v1", "Thu, 29 Oct 2015 20:07:32 GMT  (3652kb,D)", "https://arxiv.org/abs/1510.08865v1", "To appear NIPS 2015"], ["v2", "Tue, 16 Aug 2016 04:00:41 GMT  (4171kb,D)", "http://arxiv.org/abs/1510.08865v2", null]], "COMMENTS": "To appear NIPS 2015", "reviews": [], "SUBJECTS": "cs.DS cs.DM cs.LG", "authors": ["kai wei", "rishabh iyer", "shengjie wang", "wenruo bai", "jeff bilmes"], "accepted": false, "id": "1510.08865"}, "pdf": {"name": "1510.08865.pdf", "metadata": {"source": "CRF", "title": "Mixed Robust/Average Submodular Partitioning: Fast Algorithms, Guarantees, and Applications to Parallel Machine Learning and Multi-Label Image Segmentation", "authors": ["Kai Wei", "Rishabh Iyer", "Shengjie Wang"], "emails": ["kaiwei@uw.edu", "rkiyer@uw.edu", "wangsj@uw.edu", "wrbai@uw.edu", "bilmes@uw.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 0.\npartitioning and load balancing for distributed machine algorithms on parallel machines; 2) data clustering; and 3) multi-label image segmentation with (only) Boolean submodular functions via pixel partitioning. We empirically demonstrate the efficacy of our algorithms on real-world problems involving data partitioning for distributed optimization of standard machine learning objectives (including both convex and deep neural network objectives), and also on purely unsupervised (i.e., no supervised or semi-supervised learning, and no interactive segmentation) image segmentation.\nKeywords: Submodular optimization, submodular partitioning, data partitioning, parallel computing, greedy algorithm, multi-label image segmentation, data science"}, {"heading": "1. Introduction", "text": "The problem of set partitioning arises in many machine learning (ML) and data science applications. Given a set V of items, an m-partition \u03c0 = (A\u03c01 , A \u03c0 2 , . . . , A \u03c0 m) is a size m set of subsets of V (called blocks) that are non-intersecting (i.e., A\u03c0i \u2229 A\u03c0j = \u2205 for all i 6= j) and covering (i.e., \u22c3m i=1A \u03c0 i = V ). The goal of a partitioning algorithm is to produce a partitioning that is measurably good in some way, often based on an aggregation of the judgements of the internal goodness of the resulting blocks.\nIn data science and machine learning applications, a partitioning is almost always the end result of a clustering (although in some cases a clustering might allow for intersecting subsets) in which V is partitioned into m clusters (which we, in this paper, refer to as blocks). Most clustering problems are based on optimizing an objective that is either a sum, or an average-case, utility where the goal is to optimize the sum of individual cluster costs \u2014 this includes the ubiquitous k-means procedure Lloyd (1982); Arthur and Vassilvitskii (2007) where the goal is to construct a partitioning based on choosing a set of cluster centers that minimizes the total sum of squared distances between each point and its closest center. More rarely, clustering algorithms may be based on robust objective functions Garc\u0301\u0131a-Escudero et al. (2010), where the goal is to optimize the worst-case internal cluster cost (i.e., quality of the clustering is based solely on the quality of the worst internal cluster cost within the clustering). The average case vs. worst case cluster cost assessment distinction are extremes along a continuum, although all existing algorithms operate only at, rather than in between, these extremes.\nThere is another way of categorizing clustering algorithms, and that based on the goal of each resultant cluster. Most of the time, clustering algorithms attempt to produce clusters containing items similar to or near each other (e.g., with k-means, a cluster consists of a centroid and a set of nearby points), and dissimilarity exists, ideally, only between different clusters. An alternate possible goal of clustering is to have each block contains as diverse a set of items as possible, where similarity exist between rather than within clusters. For example, partitioning the vertices of a graph into a set of k independent (or, equivalently, stable) sets would fall into this later category, assuming the graph has edges only between similar vertices. This subsumes graph k-colorability problems, one of the most well-known of the NP-complete problems, although in some special cases it is solvable in polynomial time Brandsta\u0308dt (1996); Feder et al. (1999); Hell et al. (2004). This general idea, where the clusters are as internally diverse as possible using some given measure of diversity, has been called anticlustering Valev (1983, 1998); Spa\u0308th (1986) in the past and, as can be seen from the above, comprises in general some difficult computational problems.\nA third way of further delineating cluster problems is based on how each cluster\u2019s internal quality is measured. In most clustering procedures, each cluster A\u03c0i for i \u2208 {1, 2, . . . ,m} is internally judged based on the same function f regardless of which cluster is being evaluated. The overall clustering is then an aggregation of the vector of m scores f(A\u03c01 ), f(A \u03c0 2 ), . . . , f(A \u03c0 m). We call this strategy a homogeneous clustering evaluation since every cluster is internally evaluated using the same underlying function. An alternate strategy to evaluate a clustering uses a different function for each cluster. For example, in assignment problems, there are m individuals each of whom is assigned a cluster, and each individual might not judge the same cluster identically. In such cases, we have m functions f1, f2, . . . , fm that, respectively, internally evaluate the corresponding cluster, and the overall clustering evaluation is based on an aggregation of the vector of scores f1(A \u03c0 1 ), f2(A \u03c0 2 ), . . . , fm(A \u03c0 m). We call this a heterogeneous clustering evaluation.\nThe above three strategies to determine the form of cluster evaluation (i.e., average vs. robust case, internally similar vs. diverse clusters, and homogeneous vs. heterogeneous internal cluster evaluation) combined with the various ways of judging the internal cluster evaluations, and ways to aggregate the scores, leads to a plethora of possible clustering objectives. Even in the simple cases, however (such as k-means, or independent sets of graphs), the problems are generally hard and/or difficult to approximate.\nThis paper studies partitioning problems that span the range within and across the aforementioned three strategies, and all from the perspective of submodular function based internal cluster evaluations. In particular, we study problems of the following form:\nProblem 1: max \u03c0\u2208\u03a0\n[ (1\u2212 \u03bb) min\ni fi(A\n\u03c0 i ) +\n\u03bb\nm m\u2211 j=1 fj(A \u03c0 j ) ] , (1)\nand\nProblem 2: min \u03c0\u2208\u03a0\n[ (1\u2212 \u03bb) max\ni fi(A\n\u03c0 i ) +\n\u03bb\nm m\u2211 j=1 fj(A \u03c0 j ) ] , (2)\nwhere 0 \u2264 \u03bb \u2264 1, the set of sets \u03c0 = (A\u03c01 , A\u03c02 , \u00b7 \u00b7 \u00b7 , A\u03c0m) is an ordered partition of a finite set V (i.e, \u222aiA\u03c0i = V and \u2200i 6= j, A\u03c0i \u2229A\u03c0j = \u2205), and \u03a0 refers to the set of all ordered partitions of V into m blocks. In contrast to the notion of the partition often used in the computer science and mathematical communities, we clarify that an ordered partition \u03c0 is fully characterized by both its constituent blocks {A\u03c0i }mi=1 as well as the ordering of the blocks \u2014 this allows us to cover assignment problems, where the ith block is assigned to the ith, potentially distinct, individual. The parameter \u03bb controls the objective: \u03bb = 1 corresponds to the average case, \u03bb = 0 to the robust case, and 0 < \u03bb < 1 is a mixed case. We are unaware, however, of any previous work (other than our own) that allows for a mixing between worst- and average-case objectives in the context of any form of set partitioning. For convenience, we also define \u03bb\u0304 , 1 \u2212 \u03bb in the below. In general, Problems 1 and 2 are hopelessly intractable, even to approximate, but we assume that the f1, f2, \u00b7 \u00b7 \u00b7 , fm are all monotone non-decreasing (i.e., fi(S) \u2264 fi(T ) whenever S \u2286 T ), normalized (fi(\u2205) = 0), and submodular Fujishige (2005) (i.e., \u2200S, T \u2286 V , fi(S) + fi(T ) \u2265 fi(S \u222a T ) + fi(S \u2229 T )). These assumptions allow us to develop fast, simple, and scalable algorithms that have approximation guarantees, as is\ndone in this paper. These assumptions, moreover, allow us to retain the naturalness and applicability of Problems 1 and 2 to a wide variety of practical problems.\nSubmodularity is a natural property in many real-world ML applications Wei et al. (2013); Zheng et al. (2014); Nagano et al. (2010); Wei et al. (2014b); Krause et al. (2008a); Jegelka and Bilmes (2011); Das and Kempe (2011); Krause et al. (2008b); Wei et al. (2015). When minimizing, submodularity naturally model notions of interacting costs and complexity, while when maximizing it readily models notions of diversity, summarization quality, and information. Hence, Problem 1 asks for a partition whose blocks each (to the extent that \u03bb is close to 0) and that collectively (to the extent that \u03bb is close to 1) are diverse, as represented by the submodular functions. The reason for this is that submodular functions are natural at representing diversity, and a subset A \u2286 V is considered diverse, relative to V and considered amongst other sets also of size |A|, if f(A) is as large as possible. Problem 2, on the other hand, asks for a partition whose blocks each (to the extent that \u03bb is close to 0) and that collectively (to the extent that \u03bb is close to 1) are internally similar (as is typical in clustering) or that are redundant, as measured by the set of submodular functions. Taken together, we call Problems 1 and 2 Submodular Partitioning. As described above, we further categorize these problems depending on if the fi\u2019s are identical to each other (the homogeneous case) or not (the heterogeneous case).1 The heterogeneous case clearly generalizes the homogeneous setting, but as we will see, the additional homogeneous structure can be exploited to provide more efficient and/or tighter algorithms.\nThe paper is organized as follows. We start by motivating Problems 1 and 2 in the context of machine learning, where there are a number of relevant applications, two of which are expounded upon in Sections 1.1 and 1.2, and some of which we ultimately utilize (in Section 4) to evaluate on real-world data. We then provide algorithms for the robust (\u03bb = 0) versions of Problem 1 and Problem 2 in Section 2. Algorithms for Problems 1 and 2 with general \u03bb are given in Section 3. We further explore the applications given section 1.1 and 1.2 and provide an empirical validation in Section 4. We conclude in Section 5."}, {"heading": "1.1 Data Partitioning for Parallel Machine Learning", "text": "Many of today\u2019s statistical learning methods can take advantage of the vast and unprecedented amount of training data that now exists and is readily available, as indeed \u201cthere is no data like more data.\u201d On the other hand, big data presents significant computational challenges to machine learning since, while big data is still getting bigger, it is expected that we are nearing the end of Moore\u2019s law Thompson and Parthasarathy (2006), and single threaded computing speed has unfortunately not significantly improved since about 2003. In light of the ever increasing flood of data that is becoming available, it is hence imperative to develop efficient and scalable methods for large scale training of statistical models. One strategy is to develop smarter and more efficient algorithms, and indeed this fervently is being pursued in the machine learning community. Another natural and complementary strategy, also being widely pursued, is via parallel and distributed computing.\n1. Similar sub-categorizations have been called the \u201cuniform\u201d vs. the \u201cnon-uniform\u201d case in the past Svitkina and Fleischer (2008); Goemans et al. (2009), but we utilize the names homogeneous and heterogeneous to avoid implying that we desire the final set of submodular function valuations be uniform, which they do not need to be in our case.\nSince machine learning procedures are performed over sets of data, one simple way to achieve parallelism is to split the data into chunks each of which resides on a compute node. This is the idea behind many parallel learning approaches such as ADMM Boyd et al. (2011) and distributed neural network training Povey et al. (2014), to name only a few. Such parallel schemes are often performed where the data samples are distributed to their compute nodes in an arbitrary or random fashion. However, there has apparently been very little work on how to intelligently split the data to ensure that the resultant model can be learned in an efficient, or a particularly good, manner.\nOne way to approach this problem is to consider a class of \u201cutility\u201d functions on the training data. Given a set V = {v1, . . . , vn} of training data items, suppose that we have a set function f : 2V \u2192 R+ that measures the utility of subsets of the data set V . That is, given any A \u2286 V , f(A) measures the utility of the training data subset A for producing a good resulting trained model. Given a parallel training scheme (e.g., ADMM) with m compute nodes, we consider an m-partition \u03c0 = (A\u03c01 , A \u03c0 2 , . . . , A \u03c0 m) of the entire training data V , where we send the ith block A\u03c0i of the partition \u03c0 to the i th compute node. If each compute node i has a block of the data A\u03c0i that is non-representative of the utility of the whole (i.e., f(Ami ) f(V )), the parallel learning algorithm, at each iteration, might result in the compute nodes deducing models that are widely different from each other. Any subsequent aggregation of the models could then result in a joint model that is non-representative of the whole, especially in a non-convex case like deep neural network models. On the other hand, suppose that an intelligent partition \u03c0 of the training data V is achieved such that each block A\u03c0i is highly representative of the whole (i.e., f(A m i ) \u2248 f(V ),\u2200i). In this case, the models deduced at the compute nodes will tend to be close to a model trained on the entire data, and any aggregation of the resulting models (say via ADMM) will likely be better. An intelligent data partition, in fact, may have a positive effect in two ways: 1) it can lead to a better final solution (in the non-convex case), and 2) faster convergence may be achieved even in the convex case thanks to the fact that any \u201coscillations\u201d between a distributed stage (where the compute nodes are operating on local data) and an aggregation stage (where some form of model average is computed) could be dampened and hence reduced.\nIt should be noted that the above process should ideally also be done in the context of any necessary parallel load balancing, where data is distributed so that each processing node has roughly the same amount of compute needing to be performed. Our approach above, by contrast, might be instead called \u201cinformation balancing,\u201d, where we ensure that each node has a representative amount of information so that reasonable consensus solutions are constructed.\nIn this work, based on the above intuition, we mathematically describe this goal as obtaining an m-partition of the data set V such that the worst-case or the average-case utility among all blocks in the partition is maximized. More precisely, this can be formulated Problem 1 above, where the set of sets \u03c0 = (A\u03c01 , A \u03c0 2 , \u00b7 \u00b7 \u00b7 , A\u03c0m) forms a partition of the training data V . The parameter \u03bb controls the nature of the objective. For example, \u03bb = 0 is the robust case, which is particularly important in mission critical applications, such as parallel and distributed computing, where one single poor partition block can significantly slow down an entire parallel machine (as all compute nodes might need to spin block while waiting for a slow node to complete its round of computation). \u03bb = 1 is the average case. Taking\na weighted combination of the both robust and average case objectives (the mixed case, 0 < \u03bb < 1) allows one to balance between optimizing worst-case and overall performance.\nA critical aspect of Problem 1 is that submodular functions are an ideal class of functions for modeling information over training data sets. For example, Wei et al. (2015) show that the utility functions of data subsets for training certain machine learning classifiers can be derived as submodular functions. If f is selected as the class of submodular functions that appropriately model the notion of utility for a given machine learning setting (which could be different depending, say, on what form of training one is doing), solving the homogeneous instance of Problem 1 with f as the objective, then, addresses a core goal in modern machine learning on big data sets, namely how to intelligently partition and distribute training data to multiple compute nodes.\nIt should be noted that a random partition might have a high probability of performing well, and might have exponentially small probability of producing a partition that performs very poorly. This could be used as an argument for utilizing just a random data partitioning procedure. On the other hand, there are also quite likely a small number of partitions that perform exceedingly well and a random partition also has a very small probability of achieving on one of these high quality partitions. Our long-term quest is to develop methods that increase the likelihood that we can discover one of these rare but high performing partitions. Moreover, there are other applications (i.e., locality maximization via bipartite graphs, where the goal is to organize the data so as to improve data locality, as in the work of Li et al. (2015); Alistarh et al. (2015) where a random partition is quite likely to perform very poorly. This again is an additional potential application of our work."}, {"heading": "1.2 Multi-label Image Segmentation", "text": "Segmenting images into different regions is one of the more important problems in computer vision. Historically, the problem was addressed by finding a MAP solution to to a Markov random field (MRF), where one constructs a distribution p(y, x\u0304) = 1Z exp(\u2212E(y, x\u0304)). Here, y = (y1, y2, . . . , y|V |) is a set of pixel labels for a set of pixels V , x\u0304 = (x\u03041, x\u03042, . . . , x\u0304|V |) is a set of pixel values, and where E(y, x\u0304) is an energy function. For binary image segmentation, yi \u2208 0, 1, and for multi-label instance, yi \u2208 {0, 1, . . . ,m\u2212 1}, and the image labeling problem computes maxy p(y|x\u0304). In many instances Boykov et al. (2001); Boykov and Kolmogorov (2004); Boykov and Veksler (2006), it was found that performing this maximization can be performed by a graph cut algorithm, in particular when the energy function takes the form E(y, x\u0304) = \u2211 v\u2208V \u03c6v(yv, x\u0304v) + \u2211 C\u2208C \u03c6C(yC) where C are the set of cliques of a graph. When the order of the cliques is two, and when the \u03c6C functions are submodular, the MAP inference problem can be solved via a minimum graph cut algorithm, and if the order if \u03c6C is larger, then general submodular function minimization Fujishige (2005) (which runs in polynomial time in |V |) may be used. This corresponds, however, only to the binary image segmentation problem. In order to perform multi-label image segmentation, there have been various elaborate extensions of the energy functions to support this (e.g., Jegelka and Bilmes (2011); Shelhamer et al. (2014); Heng et al. (2015); Taniai et al. (2015); Kohli et al. (2013); Silberman et al. (2014) to name just a few), but in all cases the energy function E(y, x\u0304) has to be extended to, at the very least, a non-pseudo Boolean function.\nSupposing that we know the number of labels m in advance, an alternate strategy for performing multi-label image segmentation that still utilizes a pseudo-Boolean function is to perform a partition of the pixels. Each block of the partition corresponds to one segment, and for a pixel partitioning to make sense in an image segmentation context, it should be the case that each block consists of pixels that are as non-diverse as possible. For example, the pixels consisting of one object (e.g., a car, a tree, a person) tend to have much less diversity than do a set pixels chosen randomly across the image. A natural set partitioning objective to address multi-label image segmentation is then Problem 2, the reason being that submodular functions, when minimized, tend to choose sets that are non-diverse and, when forced also to be large, are redundant. Problem 2 finds a partition that simultaneously minimizes the worst case diversity of any block, and the average case diversity over all blocks. Note that in this case, the energy function E(y, x\u0304) takes on a different form and in order to define it, we need a bit of additional information. The vector y \u2208 {0, 1, . . . ,m \u2212 1}V corresponds to the multi-label labeling of all the pixels in the image. For i \u2208 {1, 2, . . . ,m\u22121} define Ai(y) = {v \u2208 V : yv = i} \u2286 V be the subset of V corresponding to the pixels with label i in vector y. The multi-label energy function then becomes:\nE(y, x\u0304) = \u2211 v\u2208V \u03c6v(yv, x\u0304v) + [ (1\u2212 \u03bb) max i fi(Ai(y)) + \u03bb m m\u2211 j=1 fj(Aj(y)) ] , (3)\nand Problem 2 minimizes it. After solving Problem 2 and given the result \u03c0 of such a partition procedure, the pixels corresponding to block A\u03c0i should, intuitively, correspond to an object. What is interesting about this approach to multi-label segmentation, however, is that the underlying submodular function (or set thereof) is still binary (i.e., it operates on subsets of sets, or equivalently, on vectors Boolean values corresponding to the characteristic vectors of sets). Instead of changing the function being optimized, Problem 2 changes the way that the function is being used to be more appropriate for the multi-label setting. We evaluate Problem 2 for image segmentation in Section 4."}, {"heading": "1.3 Sub-categorizations and Related Previous Work", "text": "Problem 1: Special cases of Problem 1 have appeared previously in the literature. Problem 1 with \u03bb = 0 is called submodular fair allocation (SFA), which has been studied mostly in the heterogeneous setting. When fi\u2019s are all modular, the tightest algorithm, so far, is to iteratively round an LP solution achieving O(1/( \u221a m log3m)) approximation Asadpour and Saberi (2010), whereas the problem is NP-hard to 1/2 + approximate for any > 0 Golovin (2005). When fi\u2019s are submodular, Golovin (2005) gives a matching-based algorithm with a factor 1/(n\u2212m+ 1) approximation that performs poorly when m n. Khot and Ponnuswami (2007) propose a binary search algorithm yielding an improved factor of 1/(2m \u2212 1). Another approach approximates each submodular function by its ellipsoid approximation (non-scalable) and reduces SFA to its modular version leading to an approximation factor of O( \u221a nm1/4 log n log3/2m). These approaches are theoretically interesting, but they either do not fully exploit the problem structure or cannot scale to large problems. On the other hand, Problem 1 for \u03bb = 1 is called submodular welfare. This problem has been extensively studied in the literature and can be equivalently formulated as submodular maximization under a partition matroid constraint Vondra\u0301k (2008). It\nadmits a scalable greedy algorithm that achieves a 1/2 approximation Fisher et al. (1978). More recently a multi-linear extension based algorithm nicely solves the submodular welfare problem with a factor of (1\u22121/e) matching the hardness of this problem Vondra\u0301k (2008). It is worth noting that the homogeneous instance of the submodular welfare withm = 2 generalizes the well-known max-cut problem (Goemans and Williamson, 1995), where the submodular objective is defined as the submodular neighborhood function (Iyer and Bilmes, 2013; Schrijver, 2003). Given a set of vertices X \u2282 V , of a graph G = (V,E), the neighborhood function f(X) counts the number of edges that are incident to at least one vertex in X, a monotone non-decreasing submodular function. The function h(X) = f(X)+f(V \\X)\u2212f(V ) is then the cut function, and since the last term is a constant, maximizing h corresponds to the submodular welfare problem. As far as we know, Problem 1 for general 0 < \u03bb < 1 has not been studied in the literature.\nProblem 2: When \u03bb = 0, Problem 2 is studied as submodular load balancing (SLB). When fi\u2019s are all modular, SLB is called minimum makespan scheduling. In the homogeneous setting, Hochbaum and Shmoys (1988) give a PTAS scheme ((1+ )-approximation algorithm which runs in polynomial time for any fixed ), while an LP relaxation algorithm provides a 2-approximation for the heterogeneous setting Lenstra et al. (1990). When the objectives are submodular, the problem becomes much harder. Even in the homogeneous setting, Svitkina and Fleischer (2008) show that the problem is information theoretically hard to approximate within o( \u221a n/ log n). They provide a balanced partitioning algorithm yielding a factor of min{m,n/m} under the homogeneous setting. They also give a sampling-based algorithm achieving O( \u221a n/ log n) for the homogeneous setting. However, the sampling-based algorithm is not practical and scalable since it involves solving, in the worst-case, O(n3 log n) instances of submodular function minimization each of which in general currently requires O(n5\u03b3+n6)\ncomputation Orlin (2009), where \u03b3 is the cost of a function valuation. Similar to Submodular Fair Allocation, Goemans et al. (2009) applies the same ellipsoid approximation techniques leading to a factor of O( \u221a n log n) Goemans et al. (2009). When \u03bb = 1, Problem 2 becomes the submodular multiway partition (SMP) for which one can obtain a relaxation based 2- approximation Chekuri and Ene (2011a) in the homogeneous case. In the non-homogeneous case, the guarantee is O(log n) Chekuri and Ene (2011b). Similarly, Zhao et al. (2004); Narasimhan et al. (2005) propose a greedy splitting 2-approximation algorithm for the homogeneous setting. To the best of our knowledge, there does not exist any work on Problem 2 with general 0 < \u03bb < 1.\nWe illustrate both the special and the general cases of Problem 1 and 2 in Figure 1."}, {"heading": "1.4 Our Contributions", "text": "In contrast to Problems 1 and 2 in the average case (i.e., \u03bb = 1), existing algorithms for the worst case (\u03bb = 0) are not scalable. This paper closes this gap, by proposing three new classes of algorithmic frameworks to solve SFA and SLB: (1) greedy algorithms; (2) semigradient-based algorithms; and (3) a Lova\u0301sz extension based relaxation algorithm.\nFor SFA, when m = 2, we formulate the problem as non-monotone submodular maximization, which can be approximated up to a factor of 1/2 with O(n) function evaluations Buchbinder et al. (2012). For general m, we give a simple and scalable greedy algorithm (GreedMax), and show a factor of 1/m in the homogeneous setting, improving the state-ofthe-art factor of 1/(2m\u2212 1) under the heterogeneous setting Khot and Ponnuswami (2007). For the heterogeneous setting, we propose a \u201csaturate\u201d greedy algorithm (GreedSat) that iteratively solves instances of submodular welfare problems. We show GreedSat has a bi-criterion guarantee of (1/2\u2212 \u03b4, \u03b4/(1/2 + \u03b4)), which ensures at least dm(1/2\u2212 \u03b4)e blocks receive utility at least \u03b4/(1/2 + \u03b4)OPT for any 0 < \u03b4 < 1/2. For SLB, we first generalize\n3. Results obtained in this paper are marked as \u2217. Methods for only the homogeneous setting are marked as \u2020.\nthe hardness result in Svitkina and Fleischer (2008) and show that it is hard to approximate better than m for any m = o( \u221a n/ log n) even in the homogeneous setting. We then give a Lova\u0301sz extension based relaxation algorithm (Lova\u0301sz Round) yielding a tight factor of m for the heterogeneous setting. As far as we know, this is the first algorithm achieving a factor of m for SLB in this setting. For both SFA and SLB, we also obtain more efficient algorithms with bounded approximation factors, which we call majorization-minimization (MMin) and minorization-maximization (MMax).\nNext we show algorithms to handle Problems 1 and 2 with general 0 < \u03bb < 1. We first give two simple and generic schemes (CombSfaSwp and CombSlbSmp), both of which efficiently combines an algorithm for the worst-case problem (special case with \u03bb = 0), and an algorithm for the average case (special case with \u03bb = 1) to provide a guarantee interpolating between the two bounds. Given the efficient algorithms proposed in this paper for the robust (worst case) problems (with guarantee \u03b1), and an existing algorithm for the average case (say, with a guarantee \u03b2), we can obtain a combined guarantee in terms of \u03b1, \u03b2 and \u03bb. We then generalize the proposed algorithms for SLB and SFA to give more practical algorithmic frameworks to solve Problems 1 and 2 for general \u03bb. In particular we generalize GreedSat leading to GeneralGreedSat, whose guarantee smoothly interpolates in terms of \u03bb between the bi-criterion factor by GreedSat in the case of \u03bb = 0 and the constant factor of 1/2 by the greedy algorithm in the case of \u03bb = 1. For Problem 2 we generalize Lova\u0301sz Round to obtain a relaxation algorithm (GeneralLova\u0301sz Round) that achieves an m-approximation for general \u03bb. Motivated by the computational limitation of GeneralLova\u0301sz Round we also give a simple and efficient greedy heuristic called GeneralGreedMin that works for the homogeneous setting of Problem 2.\nLastly we demonstrate a number of applications of submodular partitioning in real-world machine learning problems. In particular, corresponding to Sections 1.1 and 1.2, we show Problem 1 is applicable in distributed training of statistical models. Problem 2 is useful for data clustering, image segmentation, and computational load balancing. In the experiments we empirically evaluate Problem 1 on data partitioning for ADMM and distributed deep neural network training. The efficacy of Problem 2 is tested on an unsupervised image segmentation task."}, {"heading": "2. Robust Submodular Partitioning (Problems 1 and 2 when \u03bb = 0)", "text": "Notation: we define f(j|S) , f(S \u222a j)\u2212 f(S) as the gain of j \u2208 V in the context of S \u2286 V . Then, f is submodular if and only if f(j|S) \u2265 f(j|T ) for all S \u2286 T and j /\u2208 T . Also, f is monotone iff f(j|S) \u2265 0,\u2200j /\u2208 S, S \u2286 V . We assume w.l.o.g. that the ground set is V = {1, 2, \u00b7 \u00b7 \u00b7 , n}."}, {"heading": "2.1 Approximation Algorithms for SFA (Problem 1 with \u03bb = 0)", "text": "We first investigate a special case of SFA with m = 2. When m = 2, the problem becomes\nmax A\u2286V g(A), (4)\nwhere g(A) = min{f1(A), f2(V \\A)} and is submodular thanks to Theorem 1.\nTheorem 1 If f1 and f2 are monotone submodular, min{f1(A), f2(V \\A)} is also submodular.\nAll proofs for the theoretical results are given in Appendix. Interestingly SFA for m = 2 can be equivalently formulated as unconstrained submodular maximization. This problem has been well studied in the literature Buchbinder et al. (2012); Dobzinski and Mor (2015); Feige et al. (2011). A simple bi-directional randomized greedy algorithm Buchbinder et al. (2012) solves Eqn 4 with a tight factor of 1/2. Applying this randomized algorithm to solve SFA then achieves a guarantee of 1/2 matching the problem\u2019s hardness. However, the same idea does not apply to the general case of m > 2.\nFor general m, we approach SFA from the perspective of the greedy algorithms. Greedy is often the algorithm that practitioners use for combinatorial optimization problems since they are intuitive, simple to implement, and often lead to very good solutions. In this work we introduce two variants of a greedy algorithm \u2013 GreedMax (Alg. 1) and GreedSat (Alg. 2), suited to the homogeneous and heterogeneous settings, respectively.\nAlgorithm 1: GreedMax\n1: Input: f , m, V . 2: Let A1 =, . . . ,= Am = \u2205; R = V . 3: while R 6= \u2205 do 4: j\u2217 \u2208 argminj f(Aj); 5: a\u2217 \u2208 argmaxa\u2208R f(a|Aj\u2217) 6: Aj\u2217 \u2190 Aj\u2217 \u222a {a\u2217}; R\u2190 R \\ a\u2217 7: end while 8: Output {Ai}mi=1.\nGreedMax: The key idea of GreedMax (see Alg. 1) is to greedily add an item with the maximum marginal gain to the block whose current solution is minimum. Initializing {Ai}mi=1 with the empty sets, the greedy flavor also comes from that it incrementally grows\nthe solution by greedily improving the overall objective mini=1,...,m fi(Ai) until {Ai}mi=1 forms a partition. Besides its simplicity, Theorem 2 offers the optimality guarantee.\nTheorem 2 Under the homogeneous setting (fi = f for all i), GreedMax is guaranteed to find a partition \u03c0\u0302 such that\nmin i=1,...,m\nf(A\u03c0\u0302i ) \u2265 1\nm max \u03c0\u2208\u03a0 min i=1,...,m\nf(A\u03c0i ). (5)\nBy assuming the homogeneity of the fi\u2019s, we obtain a very simple 1/m-approximation algorithm improving upon the state-of-the-art factor 1/(2m \u2212 1) Khot and Ponnuswami (2007). Thanks to the lazy evaluation trick as described in Minoux (1978), Line 5 in Alg. 1 need not to recompute the marginal gain for every item in each round, leading GreedMax to scale to large data sets.\nAlgorithm 2: GreedSat\n1: Input: , {fi}mi=1, m, V , \u03b1. 2: Let F\u0304 c(\u03c0) = 1m \u2211m i=1 min{fi(A\u03c0i ), c}. 3: Let cmin = 0, cmax = mini fi(V ) 4: while cmax \u2212 cmin \u2265 do 5: c = 12(cmax + cmin) 6: \u03c0\u0302c \u2208 argmax\u03c0\u2208\u03a0 F\u0304 c(\u03c0) // solved by GreedSWP (Alg 3) 7: if F\u0304 c(\u03c0\u0302c) < \u03b1c then 8: cmax = c 9: else\n10: cmin = c; \u03c0\u0302 \u2190 \u03c0\u0302c 11: end if 12: end while 13: Output: \u03c0\u0302.\nAlgorithm 3: GreedSWP\nInput: {fi}mi=1, c, V Initialize: A1 =, . . . ,= Am = \u2205, and R\u2190 V while R 6= \u2205 do for i = 1, . . . ,m do\n\u03b4i = maxr\u2208R min{fi(Ai \u222a r), c} \u2212min{fi(Ai), c} ai \u2208 argmaxr\u2208R min{fi(Ai \u222a r), c} \u2212min{fi(Ai), c}\nj \u2208 argmaxi \u03b4(i) Aj \u2190 Aj \u222a {aj} R\u2190 R \\ aj\nOutput \u03c0\u0302c = (A1, . . . , Am).\nGreedSat: Though simple and effective in the homogeneous setting, GreedMax performs arbitrarily poorly under the heterogeneous setting.Consider the following example: V = {v1, v2}, f1(v1) = 1, f1(v2) = 0, f1({v1, v2}) = 1, f2(v1) = 1 + , f2(v2) = 1,\nf2({v1, v2}) = 2 + . f1 and f2 are monotone submodular. The optimal partition is to assign v1 to f1 and v2 to f2 leading to a solution of value 1. However, GreedMax may assign v1 to f2 and v2 to f1 leading to a solution of value 0. Therefore, GreedMax performs arbitrarily poorly on this example.\nTo this end we provide another algorithm \u2013 \u201cSaturate\u201d Greedy (GreedSat, see Alg. 2). The key idea of GreedSat is to relax SFA to a much simpler problem \u2013 Submodular Welfare (SWP), i.e., Problem 1 with \u03bb = 0. Similar in flavor to the one proposed in Krause et al. (2008b) GreedSat defines an intermediate objective F\u0304 c(\u03c0) = \u2211m i=1 f c i (A \u03c0 i ), where f ci (A) = 1 m min{fi(A), c} (Line 2). The parameter c controls the saturation in each block. It is easy to verify that f ci satisfies submodularity for each i. Unlike SFA, the combinatorial optimization problem max\u03c0\u2208\u03a0 F\u0304\nc(\u03c0) (Line 6) is much easier and is an instance of SWP. In this work, we solve Line 6 using the greedy algorithm as described in Alg 3, which attains a constant 1/2-approximation Fisher et al. (1978). Moreover the lazy evaluation trick also applies for Alg 3 enabling the wrapper algorithm GreedSat scalable to large data sets. One can also use a more computationally expensive multi-linear relaxation algorithm as given in Vondra\u0301k (2008) to solve Line 6 with a tight factor \u03b1 = (1\u22121/e). Setting the input argument \u03b1 as the approximation factor for Line 6, the essential idea of GreedSat is to perform a binary search over the parameter c to find the largest c\u2217 such that the returned solution \u03c0\u0302c \u2217 for the instance of SWP satisfies F\u0304 c \u2217 (\u03c0\u0302c \u2217 ) \u2265 \u03b1c\u2217. GreedSat terminates after solving O(log(mini fi(V ) )) instances of SWP. Theorem 3 gives a bi-criterion optimality guarantee.\nTheorem 3 Given > 0, 0 \u2264 \u03b1 \u2264 1 and any 0 < \u03b4 < \u03b1, GreedSat finds a partition such that at least dm(\u03b1\u2212 \u03b4)e blocks receive utility at least \u03b41\u2212\u03b1+\u03b4 (max\u03c0\u2208\u03a0 mini fi(A\u03c0i )\u2212 ).\nFor any 0 < \u03b4 < \u03b1 Theorem 3 ensures that the top dm(\u03b1\u2212 \u03b4)e valued blocks in the partition returned by GreedSat are (\u03b4/(1\u2212 \u03b1+ \u03b4)\u2212 )-optimal. \u03b4 controls the trade-off between the number of top valued blocks to bound and the performance guarantee attained for these blocks. The smaller \u03b4 is, the more top blocks are bounded, but with a weaker guarantee. We set the input argument \u03b1 = 1/2 (or \u03b1 = 1\u2212 1/e) as the worst-case performance guarantee for solving SWP so that the above theoretical analysis follows. However, the worst-case is often achieved only by very contrived submodular functions. For the ones used in practice, the greedy algorithm often leads to near-optimal solution (Krause et al. (2008b) and our own observations). Setting \u03b1 as the actual performance guarantee for SWP (often very close to 1) can improve the empirical bound, and we, in practice, typically set \u03b1 = 1 to good effect.\nMMax: In parallel to GreedSat, we also introduce a semi-gradient based approach for solving SFA under the heterogeneous setting. We call this algorithm minorizationmaximization (MMax, see Alg. 4). Similar to the ones proposed in Iyer et al. (2013a); Iyer and Bilmes (2013, 2012b), the idea is to iteratively maximize tight lower bounds of the submodular functions. Submodular functions have tight modular lower bounds, which are related to the subdifferential \u2202f (Y ) of the submodular set function f at a set Y \u2286 V , which is defined Fujishige (2005) as:\n\u2202f (Y ) = {y \u2208 Rn : f(X)\u2212 y(X) \u2265 f(Y )\u2212 y(Y ) for all X \u2286 V }. (6)\nFor a vector x \u2208 RV and X \u2286 V , we write x(X) = \u2211j\u2208X x(j). Denote a subgradient at Y by hY \u2208 \u2202f (Y ), the extreme points of \u2202f (Y ) may be computed via a greedy algorithm: Let\nAlgorithm 4: MMax\n1: Input: {fi}mi=1, m, V , partition \u03c00. 2: Let t = 0. 3: repeat 4: for i = 1, . . . ,m do 5: Pick a subgradient hi at A \u03c0t i for fi. 6: end for 7: \u03c0t+1 \u2208 argmax\u03c0\u2208\u03a0 mini hi(A\u03c0i ) 8: t = t+ 1; 9: until \u03c0t = \u03c0t\u22121\n10: Output: \u03c0\u0302 \u2208 argmax\u03c0=\u03c01,...,\u03c0t mini fi(A\u03c0i ).\n\u03c3 be a permutation of V that assigns the elements in Y to the first |Y | positions (\u03c3(i) \u2208 Y if and only if i \u2264 |Y |). Each such permutation defines a chain with elements S\u03c30 = \u2205, S\u03c3i = {\u03c3(1), \u03c3(2), . . . , \u03c3(i)}, and S\u03c3|Y | = Y . An extreme point h\u03c3Y of \u2202f (Y ) has each entry as\nh\u03c3Y (\u03c3(i)) = f(S \u03c3 i )\u2212 f(S\u03c3i\u22121). (7)\nDefined as above, h\u03c3Y forms a lower bound of f , tight at Y \u2014 i.e., h \u03c3 Y (X) = \u2211 j\u2208X h \u03c3 Y (j) \u2264 f(X),\u2200X \u2286 V and h\u03c3Y (Y ) = f(Y ). The idea of MMax is to consider a modular lower bound tight at the set corresponding to each block of a partition. In other words, at iteration t+ 1, for each block i, we approximate fi with its modular lower bound tight at A\u03c0 t\ni and solve a modular version of Problem 1 (Line 7), which admits efficient approximation algorithms Asadpour and Saberi (2010). MMax is initialized with a partition \u03c00, which is obtained by solving Problem 1, where each fi is replaced with a simple modular function f \u2032i(A) = \u2211 a\u2208A fi(a). The following worst-case bound holds:\nTheorem 4 MMax achieves a worst-case guarantee of\nO(min i 1 + (|A\u03c0\u0302i | \u2212 1)(1\u2212 \u03bafi(A\u03c0\u0302i )) |A\u03c0\u0302i | \u221a m log3m ),\nwhere \u03c0\u0302 = (A\u03c0\u03021 , \u00b7 \u00b7 \u00b7 , A\u03c0\u0302m) is the partition obtained by the algorithm, and\n\u03baf (A) = 1\u2212min v\u2208V f(v|A \\ v) f(v) \u2208 [0, 1]\nis the curvature of a submodular function f at A \u2286 V .\nWhen each submodular function fi is modular, i.e., \u03bafi(A) = 0,\u2200A \u2286 V, i, the approximation guarantee of MMax becomes O( 1\u221a\nm log3m ), which matches the performance of the approx-\nimation algorithm for the modular problem. When each fi is fully curved, i.e., \u03bafi = 1, we still obtain a bounded guarantee of O( 1\nn \u221a m log3m ). Theorem 4 suggests that the performance\nof MMax improves as the curvature \u03bafi of each objective fi decreases. This is natural since MMax essentially uses the modular lower bounds as the proxy for each objective and\noptimizes with respect to the proxy functions. Lower the curvature of the objectives, the better the modular lower bounds approximate, hence better performance guarantee.\nSince the modular version of SFA is also NP-hard and cannot be exactly solved in polynomial time, we cannot guarantee that successive iterations of MMax improves upon the overall objective. However we still obtain the following Theorem giving a bounded performance gap between the successive iterations.\nTheorem 5 Suppose modular version of SFA is solved with an approximation factor \u03b1 \u2264 1, we have for each iteration t that\nmin i fi(A \u03c0t i ) \u2265 \u03b1mini fi(A \u03c0t\u22121 i ). (8)"}, {"heading": "2.2 Approximation Algorithms for SLB (Problem 2 with \u03bb = 0)", "text": "In this section, we investigate the problem of submodular load balancing (SLB). It is a special case of Problem 2 with \u03bb = 0. We first analyze the hardness of SLB. We then show a Lova\u0301sz extension-based algorithm with a guarantee matching the problem\u2019s hardness. Lastly we describe a more efficient supergradient based algorithm.\nExisting hardness for SLB is shown to be o( \u221a n/ log n) Svitkina and Fleischer (2008).\nHowever it is independent of m, and Svitkina and Fleischer (2008) assumes m = \u0398( \u221a n/ log n) in their analysis. In most of the applications of SLB, we find that the parameter m is such that m n and can sometimes be treated as a constant w.r.t. n. To this end we offer a more general hardness analysis that is dependent directly on m.\nTheorem 6 For any > 0, SLB cannot be approximated to a factor of (1\u2212 )m for any m = o( \u221a n/ log n) with polynomial number of queries even under the homogeneous setting.\nThough the proof technique for Theorem 6 mostly carries over from Svitkina and Fleischer (2008), the result strictly generalizes the analysis in Svitkina and Fleischer (2008). For any choice of m = o( \u221a n/ log n) Theorem 6 implies that it is information theoretically hard to approximate SLB better than m even for the homogeneous setting. For the rest of the paper, we assume m = o( \u221a n/ log n) for SLB, unless stated otherwise. It is worth pointing out that arbitrary partition \u03c0 \u2208 \u03a0 already achieves the best approximation factor of m that one can hope for under the homogeneous setting. Denote \u03c0\u2217 as the optimal partitioning for SLB, i.e., \u03c0\u2217 \u2208 argmin\u03c0\u2208\u03a0 maxi f(A\u03c0i ). This can be verified by considering the following:\nmax i f(A\u03c0i ) \u2264 f(V ) \u2264 m\u2211 i=1 f(A\u03c0 \u2217 i ) \u2264 mmax i f(A\u03c0 \u2217 i ). (9)\nIt is therefore theoretically interesting to consider only the heterogeneous setting. Lova\u0301sz Round: Next we propose a tight algorithm \u2013 Lova\u0301sz Round (see Alg. 5) for the heterogeneous setting of SLB. The algorithm proceeds as follows: (1) apply the Lova\u0301sz extension of submodular functions to relax SLB to a convex program, which is exactly solved to a fractional solution (Line 2); (2) map the fractional solution to a partition using the \u03b8-rounding technique as proposed in Iyer et al. (2014) (Line 3 - 6). The Lova\u0301sz extension, which naturally connects a submodular function f with its convex relaxation f\u0303 , is defined as follows: given any x \u2208 [0, 1]n, we obtain a permutation \u03c3x by ordering\nAlgorithm 5: Lova\u0301sz Round\n1: Input: {fi}mi=1, {f\u0303i}mi=1, m, V . 2: Solve for {x\u2217i }mi=1 via convex relaxation. 3: Rounding: Let A1 =, . . . ,= Am = \u2205. 4: for j = 1, . . . , n do 5: i\u0302 \u2208 argmaxi x\u2217i (j); Ai\u0302 = Ai\u0302 \u222a j 6: end for 7: Output \u03c0\u0302 = {Ai}mi=1.\nits elements in non-increasing order, and thereby a chain of sets S\u03c3x0 \u2282, . . . ,\u2282 S\u03c3xn with S\u03c3xj = {\u03c3x(1), . . . , \u03c3x(j)} for j = 1, . . . , n. The Lova\u0301sz extension f\u0303 for f is the weighted sum of the ordered entries of x:\nf\u0303(x) = n\u2211 j=1 x(\u03c3x(j))(f(S \u03c3x j )\u2212 f(S\u03c3xj\u22121)). (10)\nGiven the convexity of the f\u0303i\u2019s , SLB is relaxed to the following convex program:\nmin x1,...,xm\u2208[0,1]n max i f\u0303i(xi), s.t m\u2211 i=1 xi(j) \u2265 1, for j = 1, . . . , n (11)\nDenoting the optimal solution for Eqn 11 as {x\u22171, . . . , x\u2217m}, the \u03b8-rounding step simply maps each item j \u2208 V to a block i\u0302 such that i\u0302 \u2208 argmaxi x\u2217i (j) (ties broken arbitrarily). The bound for Lova\u0301sz Round is as follows:\nTheorem 7 Lova\u0301sz Round is guaranteed to find a partition \u03c0\u0302 \u2208 \u03a0 such that\nmax i fi(A\n\u03c0\u0302 i ) \u2264 mmin\n\u03c0\u2208\u03a0 max i fi(A\n\u03c0 i )\n.\nWe remark that, to the best of our knowledge, Lova\u0301szRound is the first algorithm that is tight and that gives an approximation in terms of m for the heterogeneous setting.\nMMin: Similar to MMax for SFA, we propose Majorization-Minimization (MMin, see Alg. 6) for SLB. Here, we iteratively choose modular upper bounds, which are defined via superdifferentials \u2202f (Y ) of a submodular function Jegelka and Bilmes (2011) at Y :\n\u2202f (Y ) = {y \u2208 Rn : f(X)\u2212 y(X) \u2264 f(Y )\u2212 y(Y ); for all X \u2286 V }. (12)\nMoreover, there are specific supergradients Iyer and Bilmes (2012a); Iyer et al. (2013a) that define the following two modular upper bounds (when referring to either one, we use mfX):\nmfX,1(Y ) , f(X)\u2212 \u2211\nj\u2208X\\Y\nf(j|X\\j) + \u2211\nj\u2208Y \\X\nf(j|\u2205),\nmfX,2(Y ) , f(X)\u2212 \u2211\nj\u2208X\\Y\nf(j|V \\j) + \u2211\nj\u2208Y \\X\nf(j|X).\nAlgorithm 6: MMin\n1: Input: {fi}mi=1, m, V , partition \u03c00. 2: Let t = 0 3: repeat 4: for i = 1, . . . ,m do 5: Pick a supergradient mi at A \u03c0t i for fi. 6: end for 7: \u03c0t+1 \u2208 argmin\u03c0\u2208\u03a0 maximi(A\u03c0i ) 8: t = t+ 1; 9: until \u03c0t = \u03c0t\u22121\n10: Output: \u03c0t.\nThen mfX,1(Y ) \u2265 f(Y ) and m f X,2(Y ) \u2265 f(Y ), \u2200Y \u2286 V and m f X,1(X) = m f X,2(X) = f(X). At iteration t+ 1, for each block i, MMin replaces fi with a choice of its modular upper bound mi tight at A \u03c0t i and solves a modular version of Problem 2 (Line 7), for which there exists an efficient LP relaxation based algorithm Lenstra et al. (1990). Similar to MMax, the initial partition \u03c00 is obtained by solving Problem 2, where each fi is substituted with f \u2032i(A) = \u2211 a\u2208A fi(a). The following worst-case bound holds:\nTheorem 8 MMin achieves a worst-case guarantee of (2 maxi |A\u03c0\u2217i |\n1+(|A\u03c0\u2217i |\u22121)(1\u2212\u03bafi (A \u03c0\u2217 i ))\n), where\n\u03c0\u2217 = (A\u03c0 \u2217 1 , \u00b7 \u00b7 \u00b7 , A\u03c0 \u2217 m ) denotes the optimal partition.\nSimilar to MMax, we can show that MMin has bounded performance gaps in successive iterations.\nTheorem 9 Suppose the modular version of SLB can be solved with an approximation factor \u03b1 \u2265 1, we have for each iteration t that\nmax i fi(A \u03c0t i ) \u2264 \u03b1maxi fi(A \u03c0t\u22121 i ). (13)"}, {"heading": "3. General Submodular Partitioning (Problems 1 and 2 when 0 < \u03bb < 1)", "text": "In this section we study Problem 1 and Problem 2, in the most general case, i.e., 0 < \u03bb < 1. We use the proposed algorithms for the special cases of Problems 1 and 2 as the building blocks to design algorithms for the general scenarios (0 < \u03bb < 1). We first propose a simple and generic scheme that provides performance guarantee in terms of \u03bb for both problems. We then generalize the proposed GreedSat to obtain a more practically interesting algorithm for Problem 1. For Problem 2 we generalize Lova\u0301sz Round to obtain a relaxation based algorithm.\nExtremal Combination Scheme: First we describe the scheme that works for both problem 1 and 2. It naturally combines an algorithm for solving the worst-case problem (\u03bb = 0) with an algorithm for solving the average case (\u03bb = 1). We use Problem 1 as an example, but the same scheme easily works for Problem 2. Denote AlgWC as the algorithm for the worst-case problem (i.e. SFA), and AlgAC as the algorithm for the average case (i.e., SWP).\nThe scheme is to first obtain a partition \u03c0\u03021 by running AlgWC on the instance of Problem 1 with \u03bb = 0 and a second partition \u03c0\u03022 by running AlgAC with \u03bb = 1. Then we output one of \u03c0\u03021 and \u03c0\u03022, with which the higher valuation for Problem 1 is achieved. We call this scheme CombSfaSwp. Suppose AlgWC solves the worst-case problem with a factor \u03b1 \u2264 1 and AlgAC for the average case with \u03b2 \u2264 1. When applied to Problem 2 we refer to this scheme as CombSlbSmp (\u03b1 \u2265 1 and \u03b2 \u2265 1). The following guarantee holds for both schemes:\nTheorem 10 For any \u03bb \u2208 (0, 1) CombSfaSwp solves Problem 1 with a factor max{ \u03b2\u03b1 \u03bb\u0304\u03b2+\u03b1 , \u03bb\u03b2} in the heterogeneous case, and max{min{\u03b1, 1m}, \u03b2\u03b1 \u03bb\u0304\u03b2+\u03b1\n, \u03bb\u03b2} in the homogeneous case. Similarly, CombSlbSmp solves Problem 2 with a factor min{ m\u03b1\nm\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} in the hetero-\ngeneous case, and min{m, m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} in the homogeneous case.\nAlgorithm 7: GeneralGreedSat\n1: Input: , {fi}mi=1, m, V , \u03bb, \u03b1. 2: Let F\u0304 c\u03bb(\u03c0) = 1 m \u2211m i=1 min{\u03bb\u0304fi(A\u03c0i ) + \u03bb 1m \u2211m j=1 fj(A \u03c0 j ), c}.\n3: Let cmin = 0, cmax = \u2211m\ni=1 fi(V ) 4: while cmax \u2212 cmin \u2265 do 5: c = 12(cmax + cmin) 6: \u03c0\u0302c \u2208 argmax\u03c0\u2208\u03a0 F\u0304 c\u03bb(\u03c0) // solved by GreedSwp (Alg. 3) 7: if F\u0304 c(\u03c0\u0302c) < \u03b1c then 8: cmax = c 9: else\n10: cmin = c; \u03c0\u0302 \u2190 \u03c0\u0302c 11: end if 12: end while 13: Output: \u03c0\u0302.\nGeneralGreedSat: The drawback of CombSfaSwp and CombSlbSmp is that they do not explicitly exploit the trade-off between the average-case and worst-case in terms of \u03bb. To obtain more practically interesting algorithms, we first give GeneralGreedSat (See Alg. 7) that generalizes GreedSat to solve Problem 1 for general \u03bb. The key idea of GeneralGreedSat is again to relax Problem 1 to a simpler submodular welfare problem (SWP). Similar to GreedSat we define an intermediate objective:\nF\u0304 c\u03bb(\u03c0) = 1\nm m\u2211 i=1 min{\u03bb\u0304fi(A\u03c0i ) + \u03bb 1 m m\u2211 j=1 fj(A \u03c0 j ), c}. (14)\nIt is easy to verify that the combinatorial optimization problem max\u03c0\u2208\u03a0 F\u0304 c \u03bb(\u03c0) (Line 6) can be formulated as the submodular welfare problem, for which we can solve efficiently with GreedSwp (see Alg. 3). Defining \u03b1 as the optimality guarantee of the algorithm for solving Line 6 GeneralGreedSat solves Problem 1 with the following bounds:\nTheorem 11 Given > 0, 0 \u2264 \u03b1 \u2264 1, and 0 \u2264 \u03bb \u2264 1, GeneralGreedSat finds a partition \u03c0\u0302 that satisfies the following:\n\u03bb\u0304min i fi(A\n\u03c0\u0302 i ) + \u03bb\n1\nm m\u2211 i=1 fi(A \u03c0\u0302 i ) \u2265 \u03bb\u03b1(OPT \u2212 ) (15)\nwhere OPT = max\u03c0\u2208\u03a0 \u03bb\u0304mini fi(A \u03c0 i ) + \u03bb 1 m \u2211m i=1 fi(A \u03c0 i ).\nMoreover, let F\u03bb,i(\u03c0) = \u03bb\u0304fi(A \u03c0 i ) + \u03bb 1 m \u2211m j=1 fj(A \u03c0 j ). Given any 0 < \u03b4 < \u03b1, there is a set\nI \u2286 {1, . . . ,m} such that |I| \u2265 dm(\u03b1\u2212 \u03b4)e and\nFi,\u03bb(\u03c0\u0302) \u2265 max{ \u03b4\n1\u2212 \u03b1+ \u03b4 , \u03bb\u03b1}(OPT \u2212 ),\u2200i \u2208 I. (16)\nEqn 16 in Theorem 11 reduces to Theorem 3 when \u03bb = 0, i.e., it recovers the bi-criterion guarantee in Theorem 3 for the worst-case scenario (\u03bb = 0). Eqn 15 in Theorem 11 implies that \u03b1-approximation for the average-case objective can almost be recovered by GeneralGreedSat if \u03bb = 1. Moreover Theorem 11 shows that the guarantee of GeneralGreedSat improves as \u03bb increases suggesting that Problem 1 becomes easier as the mixed objective weights more on the average-case objective. We also point out that the approximation guarantee of GeneralGreedSat smoothly interpolates the two extreme cases in terms of \u03bb.\nAlgorithm 8: GeneralLova\u0301sz Round\n1: Input: {fi}mi=1, {f\u0303i}mi=1, \u03bb, m, V . 2: Solve\nmin x1,...,xm\u2208[0,1]n max i \u03bb\u0304f\u0303i(xi) + \u03bb\n1\nm m\u2211 j=1 f\u0303j(xj), s.t m\u2211 i=1 xi(j) \u2265 1, for j = 1, . . . , n (17)\nfor {x\u2217i }mi=1 via convex relaxation. 3: Rounding: Let A1 =, . . . ,= Am = \u2205. 4: for j = 1, . . . , n do 5: i\u0302 \u2208 argmaxi x\u2217i (j); Ai\u0302 = Ai\u0302 \u222a j 6: end for 7: Output \u03c0\u0302 = {Ai}mi=1.\nGeneralLova\u0301sz Round: Next we focus on designing practically more interesting algorithms for Problem 2 with general \u03bb. In particular we generalize Lova\u0301sz Round leading to GeneralLova\u0301sz Round as shown in Alg. 8. Sharing the same idea with Lova\u0301sz Round, GeneralLova\u0301sz Round first relaxes Problem 2 as a convex program (defined in Eqn 17) using the Lova\u0301sz extension of each submodular objective. Given the fractional solution to the convex program {x\u2217i }mi=1, the algorithm then rounds it to a partition using the \u03b8-rounding technique (Line 3- 6). Note the rounding technique used for GeneralLova\u0301sz Round is the same as in Lova\u0301sz Round. The following Theorem holds:\nTheorem 12 GeneralLova\u0301sz Round is guaranteed to find a partition \u03c0\u0302 \u2208 \u03a0 such that\nmax i \u03bb\u0304fi(A\n\u03c0\u0302 i ) + \u03bb\n1\nm m\u2211 j=1 fj(A \u03c0\u0302 j ) \u2264 mmin \u03c0\u2208\u03a0 max i \u03bb\u0304fi(A \u03c0 i ) + \u03bb 1 m m\u2211 j=1 fj(A \u03c0 j ). (18)\nTheorem 12 generalizes Theorem 7 when \u03bb = 0. Moreover we achieve a factor of m by GeneralLova\u0301sz Round for any \u03bb. Though the approximation guarantee is independent of \u03bb the algorithm naturally exploits the trade-off between the worst-case and average-case objectives in terms of \u03bb. The drawback of GeneralLova\u0301sz Round is that it requires high order polynomial queries of the Lova\u0301sz extension of the submodular objectives, hence is not computationally feasible for even medium sized tasks. Moreover, if we restrict ourselves to the homogeneous setting (fi\u2019s are identical), it is easy to verify that arbitrary partitioning already achieves a guarantee of m while Problem 2, in general, cannot be approximated better than m as shown in Theorem 6.\nGeneralGreedMin: In this case, we should resort to intuitive heuristics that are scalable to large-scale applications to solve Problem 2 with general \u03bb. To this end we design a greedy heuristic called GeneralGreedMin (see Alg. 9).\nAlgorithm 9: GeneralGreedMin\n1: Input: f , m, V , 0 \u2264 \u03bb \u2264 1; 2: Solve Sseed \u2208 argmaxS\u2286V ;|S|=m f(S) for m seeds with Sseed = {s1, . . . , sm}. 3: Initialize each block i by the seeds as Ai \u2190 {si},\u2200i. 4: Initialize a counter as k = m and R = V \\ Sseed. 5: while R 6= \u2205 do 6: if k \u2264 (1\u2212 \u03bb)|V | then 7: j\u2217 \u2208 argminj f(Aj) 8: a\u2217 \u2208 mina\u2208R f(a|Aj\u2217) 9: Aj\u2217 \u2190 Aj\u2217 \u222a a\u2217;R\u2190 R \\ a\u2217\n10: else 11: for i = 1, . . . ,m do 12: a\u2217i \u2208 argmina\u2208R f(a|Ai) 13: end for 14: j\u2217 \u2208 argmini=1,...,m f(a\u2217i |Ai); 15: Aj\u2217 \u2190 Aj\u2217 \u222a a\u2217; R\u2190 R \\ a\u2217j\u2217 16: end if 17: k = k + 1; 18: end while 19: Output {Ai}mi=1.\nThe algorithm first solves a constrained submodular maximization on f to obtain a set Sseed of m seeds. Since f is submodular, maximizing f always leads to a set of diverse seeds, where the diversity is measured by the objective f . We initialize each block Ai with one seed from Sseed. Defining k as the number of items that have already been assigned. The main algorithm consists of two phases. In the first phase (k \u2264 (1 \u2212 \u03bb)|V |), we, for each iteration, assign the item that has the smallest marginal gain to the block whose valuation is\nthe smallest. Since the functions are all monotone, any additions to a block can (if anything) only increase its value. Such procedure inherently minimizes the worst-case objective, since it chooses the minimum valuation block to add to in order to keep the maximum valuation block from growing further. In the second phase (k > \u03bb|V |), we assign an item such that its marginal gain is the smallest among all remaining items and all blocks. The greedy procedure in this phase, on the hand, is suitable for minimizing the average-case objective, since it, in each iteration, assigns an item so that the valuation of the average-case objective increases the least. The trade-off between the worse-case and the average-case objectives is controlled by \u03bb, which is used as the input argument to the algorithm. In particular, \u03bb controls the fraction of the iterations in the algorithm to optimize the average-case objective. When \u03bb = 1, the algorithm solely focuses on the average-case objective, while only the worst-case objective is minimized if \u03bb = 0.\nIn general GeneralGreedMin requires O(m|V |2) function valuations, which may still be computationally difficult for large-scale applications. In practice, one can relax the condition in Line 8 and 12. Instead of searching among all items in R, one can, in each round, randomly select a subset R\u0302 \u2286 R and choose an item with the smallest marginal gain from only the subset R\u0302. The resultant computational complexity is reduced to O(m|R\u0302||V |) function valuations. Empirically we observe that GeneralGreedMin can be sped up more than 100 times by this trick without much performance loss."}, {"heading": "4. Experiments", "text": "In this section we empirically evaluate the algorithms proposed for Problems 1 and 2. We first compare the performance of the various algorithms discussed in this paper on a synthetic data set. We then evaluate some of the scalable algorithms proposed for Problems 1 and 2 on large-scale real-world data partitioning applications including distributed ADMM, distributed neural network training, and lastly unsupervised image segmentation tasks."}, {"heading": "4.1 Experiments on Synthetic Data", "text": "In this section we evaluate separately on four different cases: Problem 1 with \u03bb = 0 (SFA), Problem 2 with \u03bb = 0 (SLB), Problem 1 with 0 < \u03bb < 1, and Problem 2 with 0 < \u03bb < 1. Since some of the algorithms, such as the Ellipsoidal Approximations Goemans et al. (2009) and Lova\u0301sz relaxation algorithms, are computationally intensive, we restrict ourselves to only 40 data instances, i.e., |V | = 40. For simplicity we only evaluate on the homogeneous setting (fi\u2019s are identical). For each case we test with two types of submodular functions: facility location function, and the set cover function. The facility location function is defined as follows:\nffac(A) = \u2211 v\u2208V max a\u2208A sv,a, (19)\nwhere sv,a is the similarity between item v and a and is symmetric, i.e., sv,a = sa,v for any pair of v and a. We define ffac on a complete similarity graph with each edge weight sv,a sampled uniformly and independently from [0, 1]. The set cover function fsc is defined by a bipartite graph between V and U (|U | = 40), where we define an edge between an item v \u2208 V and a key u \u2208 U independently with probability p = 0.2.\n(a) Problem 1 on ffac with \u03bb = 0\n(b) Problem 1 on fsc with \u03bb = 0\nProblem 1 For \u03bb = 0, i.e., SFA, we compare among 6 algorithms: GreedMax, GreedSat, MMax, Balanced Partition (BP), Ellipsoid Approximation (EA) Goemans et al. (2009), and Binary Search algorithm (BS) Khot and Ponnuswami (2007). Balanced Partition method simply partitions the ground set V into m blocks such that the size of each block is balanced and is either d |V |m e or b |V | m c. We run 100 randomly generated instances of the balanced partition method. GreedSat is implemented with the choice of the hyperparameter \u03b1 = 1. We compare the performance of these algorithms in Figure 2a and 2b, where we vary the number of blocks m from 2 to 14. The three proposed algorithms (GreedMax, GreedSat, and MMax) significantly and consistently outperform all baseline methods for both ffac and fsc. Among the proposed algorithms we observe that GreedMax, in general, yields the superior performance. Given the empirical success, computational efficiency, and tight theoretical guarantee, we suggest GreedMax as the first choice of algorithm to solve SFA under the homogeneous setting.\nNext we evaluate Problem 1 with general 0 < \u03bb < 1. Baseline algorithms for SFA such as Ellipsoidal Approximations, Binary Search do not apply to the mixed scenario. Similarly the proposed algorithms such as GreedMax, MMax do not simply generalize to this scenario. We therefore only compare GeneralGreedSat with the Balanced Partition as a baseline.\nThe results are summarized in Figure 2c and 2d. We observe that GeneralGreedSat consistently and significantly outperform even the best of 100 instances of the baseline method for all cases of \u03bb.\n(a) Problem 2 on ffac with \u03bb = 0\n(b) Problem 2 on fsc with \u03bb = 0\nProblem 2 For \u03bb = 0, i.e., SLB, we compare among 5 algorithms: Lova\u0301sz Round, MMin, GeneralGreedMin, Ellipsoid Approximation (EA) Goemans et al. (2009), and Balanced Partition Svitkina and Fleischer (2011). We implement GeneralGreedMin with the input argument \u03bb = 0. We also run 100 randomly generated instances of the Balanced Partition method as a baseline. We show the results in Figure 3a and 3b. Among all five algorithms MMin and GeneralGreedMin, in general, perform the best. Between MMin and GeneralGreedMin we observe that GeneralGreedMin performs marginally better, especially on fsc. The computationally intensive algorithms, such as Ellipsoid Approximation and Lova\u0301szRound, do not perform well, though they carry better worst-case approximation factors for the heterogeneous setting.\nLastly we evaluate Problem 2 with general 0 < \u03bb < 1. Since MMin and Ellipsoid Approximation do not apply for the mixed scenario, we test only on GeneralLova\u0301sz Round, GeneralGreedMin, and Balanced Partition. Again we test on 100 instances of\nrandomly generated balanced partitions. We vary \u03bb in this experiment. The results are shown in Figure 3c and 3d. The best performance is consistently achieved by GeneralGreedMin."}, {"heading": "4.2 Problem 1 for Distributed Training", "text": "In this section we focus on applications of Problem 1 to real-wold machine learning problems. In particular we examine how a partition obtained by solving Problem 1 with certain instances of submodular functions perform for distributed training of various statistical models.\nDistributed Convex Optimization: We first consider data partitioning for distributed convex optimization. We evaluate the distributed convex optimization on a text categorization task. We use 20 Newsgroup data set 4, which consists of 18,774 articles divided almost evenly across 20 classes. The text categorization task is to classify an article into one newsgroup (of twenty) to which it was posted. We randomly split 2/3 and 1/3 of the whole data as the training and test data. The task is solved as a multi-class classification problem, which we formulate as an `2 regularized logistic regression. We solve this convex optimization problem in a distributive fashion, where the data samples are partitioned and distributed across multiple machines. In particular we implement an ADMM algorithm as described in Boyd et al. (2011) to solve the distributed convex optimization problem. Given a partition \u03c0 of the training data into m separate clients, in each iteration, ADMM first assigns each client i to solve an `2 regularized logistic regression on its block of data A \u03c0 i using L-BFGS, aggregate the solutions from all m clients according to the ADMM update rules, and then sends the aggregated solution back to each client. This iterative procedure is carried out so that solutions on all clients converge to a consensus, which is the global solution of the overall large-scale convex optimization problem.\nWe formulate the data partitioning problem as an instance of SFA (Problem 1 with \u03bb = 0) under the homogeneous setting. In the experiment, we solve the data partitioning using GreedMax, since it is efficient and attains the tightest guarantee among all algorithms proposed for this setting. Note, however, GreedSat and MMax may also be used in the experiment. We model the utility of a data subset using the feature-based submodular function Wei et al. (2014b, 2015); Tschiatschek et al. (2014), which has the form:\nffea(A) = \u2211 u\u2208U mu(V ) logmu(A), (20)\nwhere U is the set of \u201cfeatures\u201d, mu(A) = \u2211\na\u2208Amu(a) with mu(a) measuring the degree that the article a possesses the feature u \u2208 U . In the experiments, we define U as the set of all words occurred in the entire data set and mu(a) as the number of occurrences of the word u \u2208 U in the article a. ffea is in the form of a sum of concave over modular functions, hence is monotone submodular Stobbe and Krause (2010). The class of feature-based submodular function has been widely applied to model the utility of a data subset on a number of tasks, including speech data subset selection Wei et al. (2014b,c), and image summarization Tschiatschek et al. (2014). Moreover ffea has been shown in Wei et al. (2015) to model the log-likelihood of a data subset for a Na\u0308\u0131ve Bayes classifier.\n4. Data set is obtained at http://qwone.com/\u223cjason/20Newsgroups/\nWe compare the submodular partitioning with the random partitioning for m = 5 and m = 10. We test with 10 instances of random partitioning. We first examine how balanced the sizes of the blocks yielded by submodular partitioning are. This is important since if the block sizes vary a lot in a partition, the computational loads across the blocks are imbalanced, and the actual efficiency of the parallel system is significantly reduced. Fortunately we observe that submodular partitioning yields very balanced partition. The sizes of all blocks in the resultant partitioning for m = 5 range between 2, 225 and 2, 281. In the case of m = 10 the maximum block is of size 1, 140, while the smallest block has 1, 109 items.\nThe comparison between submodular partitioning and random partitioning in terms of the accuracy of the model attained at any iteration is shown in Fig 4. For m = 10 we also run an instance on an adversarial partitioning, where each block is formed by grouping every two of the 20 classes in the training data. We observe submodular partitioning converges faster than the random partitioning, both of which perform significantly better than the adversarial partition. In particular significant and consistent improvement over the best of 10 random instances is achieved by the submodular partition across all iterations when m = 5.\nDistributed Deep Neural Network Training: Next we evaluate our framework on distributed neural network training. We test on two tasks: 1) handwritten digit recognition on the MNIST database 5; 2) phone classification on the TIMIT data.\nThe data for the handwritten digit recognition task consists of 60,000 training and 10,000 test samples. Each data sample is an image of handwritten digit. The training and test data are almost evenly divided into 10 different classes. For the phone classification task, the data consists of 1,124,823 training and 112,487 test samples. Each sample is a frame of speech. The training data is divided into 50 classes, each of which corresponds to a phoneme. The goal of this task to classify each speech sample into one of the 50 phone classes.\nA 4-layer DNN model is applied to the MNIST experiments, and we train a 5-layered DNN for the TIMIT experiments. We apply the same distributed training procedure for both tasks. Given a partitioning of the training data, we distributively solve m instances of\n5. Data set is obtained at yann.lecun.com/exdb/mnist\nsub-problems in each iteration. We define each sub-problem on a separate block of the data. We employ the stochastic gradient descent as the solver on each instance of the sub-problem. In the first iteration we use a randomly generated model as the initial model shared among the m sub-problems. Each sub-problem is solved with 10 epochs of the stochastic gradient decent training. We then average the weights in the m resultant models to obtain a consensus model, which is used as the initial model for each sub-problem in the successive iteration. Note that this distributed training scheme is similar to the ones presented in Povey et al. (2014).\nThe submodular partitioning for both tasks is obtained by solving the homogeneous case of Problem 1 (\u03bb = 0) using GreedMax on a form of clustered facility location, as proposed and used in Wei et al. (2015). The function is defined as follows:\nfc-fac(A) = \u2211 y\u2208Y \u2211 v\u2208V y max a\u2208A\u2229V y sv,a (21)\nwhere sv,a is the similarity measure between sample v and a, Y is the set of class labels, and V y is the set of samples in V with label y \u2208 Y. Note {V y}y\u2208Y forms a disjoint partitioning of the ground set V . In both the MNIST and TIMIT experiments we compute the similarity sv,a as the RBF kernel between the feature representation of v and a. Wei et al. (2015) show that fc-fac models the log-likelihood of a data subset for a Nearest Neighbor classifier. They also empirically demonstrate the efficacy of fc-fac in the case of neural network based classifiers.\nSimilar to the ADMM experiment we also observe that submodular partitioning yields very balanced partitions in all cases of this experiment. In the cases of m = 5 and m = 10 for the MNIST data set the sizes of the blocks in the resultant submodular partitioning are within the range [11991, 12012] and [5981, 6019], respectively. For the TIMIT data set, the block sizes range between 37, 483 and 37510 in the case of m = 30, and the range of [28121, 28122] is observed for m = 40.\nWe also run 10 instances of random partitioning as a baseline. The comparison between submodular partitioning and random partitioning in terms of the accuracy of the model attained at any iteration is shown in Fig 5 and 6. The adversarial partitioning, which is formed by grouping items with the same class, cannot even be trained in both cases. Consistent and significant improvement is again achieved by submodular partitioning for all cases."}, {"heading": "4.3 Problem 2 for Unsupervised Image Segmentation", "text": "Lastly we test the efficacy of Problem 2 on the task of unsupervised image segmentation. We evaluate on the Grab-Cut data set, which consists of 30 color images. Each image has ground truth foreground/background labels. By \u201cunsupervised\u201d, we mean that no labeled data at any time in supervised or semi-supervised training, nor any kind of interactive segmentation, was used in forming or optimizing the objective. In our experiments, the image segmentation task is solved as unsupervised clustering of the pixels, where the goal is to obtain a partitioning of the pixels such that the majority of the pixels in each block share either the same foreground or the background labels.\nLet V be the ground set of pixels of an image, \u03c0 be an m-partition of the image, and {yv}v\u2208V as the pixel-wise ground truth label (yv = {0, 1} with 0 being background and 1\nNumber of iterations 5 10 15 20\nT es\nt a cc\nur ac\ny (%\n)\n98.3\n98.4\n98.5\n98.6\n98.7\n98.8\n98.9\n99\n99.1\n5-Partition on MNIST with Distributed NN\nSubmodular partition Random partition\nNumber of iterations 5 10 15 20\nT es\nt a cc\nur ac\ny (%\n)\n97.8\n98\n98.2\n98.4\n98.6\n98.8\n99\n99.2 10-Partition on MNIST with Distributed NN\nSubmodular partition Random partition\nFigure 5: MNIST\nNumber of iterations 5 10 15 20 25 30 35 40 45 50 55\nT es\nt a cc\nur ac\ny (%\n)\n15\n20\n25\n30\n35\n40\n45\n50\n30-Partition on TIMIT\nSubmodular partition Random partition\nNumber of iterations 5 10 15 20 25 30 35 40 45 50 55\nT es\nt a cc\nur ac\ny (%\n)\n10\n15\n20\n25\n30\n35\n40\n45\n50\n40-Block Partition on TIMIT with Distributed NN\nSubmodular partition Random partition\nFigure 6: TIMIT\nbeing foreground). We measure the performance of the partition \u03c0 in the following two steps: (1) for each block i, predict y\u0302v for all the pixels v \u2208 A\u03c0i in the block as either 0 or 1 having larger intersection with the ground truth labels, i.e., predict y\u0302v = 1,\u2200v \u2208 A\u03c0i , if\u2211\nv\u2208A\u03c0i 1{yv = 1} \u2265 \u2211 v\u2208A\u03c0i\n1{yv = 0}, and predict y\u0302v = 0, \u2200v \u2208 A\u03c0i otherwise. (2) report the performance of the partition \u03c0 as the F-measure of the predicted labels {y\u0302v}v\u2208V relative to the ground truth label {yv}v\u2208V .\nIn the experiment we first preprocess the data by downsampling each image by a factor 0.25 for testing efficiency. We represent each pixel v as 5-dimensional features xv \u2208 R5, including the RGB values and pixel positions. We normalize each feature within [0, 1]. To obtain a segmentation of each image we solve an instance of Problem 2 (0 < \u03bb < 1) under the homogeneous setting using GeneralGreedMin (Alg. 9). We use the facility location function ffac as the objective for Problem 2. The similarity sv,a between the pixels v and a is computed as sv,a = C \u2212 \u2016xv \u2212 xa\u20162 with C = maxv,v\u2032\u2208V \u2016xv \u2212 x\u2032v\u20162 being the maximum pairwise Euclidean distance. Since the facility location function ffac is defined on a pairwise similarity graph, which requires O(|V |2) memory complexity. It becomes computationally infeasible for medium sized images. Fortunately a facility location function that is defined on a sparse k-nearest neighbor similarity graph performs just as well with k being very\nsparse Wei et al. (2014a). In the experiment, we instantiate ffac by a sparse 10-nearest neighbor sparse graph, where each item v is connected only to its 10 closest neighbors.\nA number of unsupervised methods are tested as baselines in the experiment, including k-means, k-medoids, graph cuts Boykov and Kolmogorov (2004) and spectral clustering Von Luxburg (2007). We use the RBF kernel sparse similarity matrix as the input for spectral clustering. The sparsity of the similarity matrix is k and the width parameter of the RBF kernel \u03c3. We test with various choices of \u03c3 and k and find that the setting of \u03c3 = 1 and k = 20 performs the best, with which we report the results. For graph cuts, we use the MATLAB implementation Bagon (2006), which has a smoothness parameter \u03b1. We tune \u03b1 = 0.3 to achieve the best performance and report the result of graph cuts using this choice.\nlambda\n0 0.2 0.4 0.6 0.8 1\na v e ra\ng e d F\n-m e a s u re\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95 Comparison with various lambda\nm=5 m=10 m=15 m=20 m=25\nFigure 7: m 5 10 15 20 25\na v e ra\ng e d F\n-m e a s u re\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95 Comparison with various m\nSubmodular partition K-means K-medoids Spectral clustering Graph cut\nFigure 8\nThe proposed image segmentation method involves a hyperparameter \u03bb, which controls the trade-off between the worst-case objective and the average-case objective. First we examine how the performance of our method varies with different choices of \u03bb in Figure 7. The performance is measured as the averaged F -measure of a partitioning method over all images in the data set. Interestingly we observe that the performance smoothly varies as \u03bb increases from 0 to 1. In particular the best performance is achieved when \u03bb is within the range [0.7, 0.9]. It suggests that using only the worst-case or the average-case objective does not suffice for the unsupervised image segmentation / clustering task, and an improved result is achieved by mixing these two extreme cases. In the subsequent experiments we show only the result of our method with \u03bb = 0.2. Next we compare the proposed approach with baseline methods on various m in Figure 8. In general, each method improves as m increases. Submodular partitioning method performs the best on almost all cases of m. Lastly we show in Figure 9 example segmentation results on several example images as well as averaged F-measure in the case of m = 15. We observe that submodular partitioning, in general, leads to less noisy and more coherent segmentation in comparison to the baselines."}, {"heading": "5. Conclusions", "text": "In this paper, we considered two novel mixed robust/average-case submodular partitioning problems, which generalize four well-known problems: submodular fair allocation (SFA),\nsubmodular load balancing (SLB), submodular welfare problem (SWP), and submodular multiway partition (SMP). While the average case problems, i.e., SWP and SMP, admit efficient and tight algorithms, existing approaches for the worst case problems, i.e., SFA and SLB, are, in general, not scalable. We bridge this gap by providing several new algorithms that not only scale to large data sets but that also achieve comparable theoretical guarantees. Moreover we provide a number of efficient frameworks for solving the general mixed robust/average-case submodular partitioning problems. We also demonstrate that submodular partitioning is applicable in a number of machine learning problems involving distributed optimization, computational load balancing, and unsupervised image segmentation. Lastly we empirically show the effectiveness of the proposed algorithms on these machine learning tasks."}, {"heading": "Acknowledgments", "text": "This material is based upon work supported by the National Science Foundation under Grant No. IIS-1162606, the National Institutes of Health under award R01GM103544, and by a Google, a Microsoft, and an Intel research award. R. Iyer acknowledges support from a Microsoft Research Ph.D Fellowship. This work was supported in part by TerraSwarm, one of six centers of STARnet, a Semiconductor Research Corporation program sponsored by MARCO and DARPA."}, {"heading": "Appendix", "text": ""}, {"heading": "Proof for Theorem 1", "text": "Theorem If f1 and f2 are monotone submodular, min{f1(A), f2(V \\A)} is also submodular. Proof To prove the Theorem, we show a more general result: Let f and h be submodular, and f \u2212 h be either monotone increasing or decreasing, then g(S) = min{f(S), h(S)} is also submodular. The Theorem follows by this result, since f(S) = f1(S) and h(S) = f2(V \\ S) are both submodular and f(S)\u2212 h(S) = f1(S)\u2212 f2(V \\ S) is monotone increasing.\nIn order to show g is submodular, we prove that g satisfies the following:\ng(S) + g(T ) \u2265 g(S \u2229 T ) + g(S \u222a T ), \u2200S, T \u2286 V (22) If g agrees with either f or h on both S and T , and since\nf(S) + f(T ) \u2265 f(S \u222a T ) + f(S \u2229 T ) (23) h(S) + h(T ) \u2265 h(S \u222a T ) + h(S \u2229 T ), (24)\n(25)\nEqn 22 follows. Otherwise, w.l.o.g. we consider g(S) = f(S) and g(T ) = h(T ). For the case where f \u2212 h is monotone non-decreasing, consider the following:\ng(S) + g(T ) = f(S) + h(T ) (26)\n\u2265 f(S \u222a T ) + f(S \u2229 T )\u2212 (f(T )\u2212 h(T )) // submodularity of f (27) \u2265 f(S \u222a T ) + f(S \u2229 T )\u2212 (f(S \u222a T )\u2212 h(S \u222a T )) // monotonicity of f \u2212 h\n(28)\n= f(S \u2229 T ) + h(S \u222a T ) (29) \u2265 g(S \u2229 T ) + g(S \u222a T ). (30)\nSimilarly, for the case where f \u2212 h is monotone non-increasing, consider the following: g(S) + g(T ) = f(S) + h(T ) (31)\n\u2265 h(S \u2229 T ) + h(S \u222a T ) + (f(S)\u2212 h(S)) // submodularity of h (32) \u2265 h(S \u222a T ) + h(S \u2229 T ) + (f(S \u222a T )\u2212 h(S \u222a T )) // monotonicity of f \u2212 h\n(33)\n= h(S \u2229 T ) + f(S \u222a T ) (34) \u2265 g(S \u2229 T ) + g(S \u222a T ). (35)"}, {"heading": "Proof for Theorem 2", "text": "Theorem Under the homogeneous setting (fi = f for all i), GreedMax is guaranteed to find a partition \u03c0\u0302 such that\nmin i=1,...,m\nf(A\u03c0\u0302i ) \u2265 1\nm max \u03c0\u2208\u03a0 min i=1,...,m\nf(A\u03c0i ). (36)\nProof We prove that the guarantee of 1/m, in fact, even holds for a streaming version of the greedy algorithm (StreamGreed, see Alg. 10). In particular, we show that StreamGreed provides a factor of 1/m for SFA under the homogeneous setting. Theorem 2 then follows since GreedMax can be seen as running StreamGreed with a specific order.\nAlgorithm 10: StreamGreed\nInput: V = {v1, v2, . . . , vn}, f , m Initialize: A1 =, . . . ,= Am = \u2205, k = 1 while k \u2264 n do\ni\u2217 \u2208 argminj f(Aj) Ai\u2217 \u2190 Ai\u2217 \u222a {vk} k \u2190 k + 1\nTo prove the guarantee for StreamGreed, we consider the resulting partitioning after an instance of StreamGreed: \u03c0 = (A\u03c01 \u222aA\u03c02 , . . . , A\u03c0m). For simplicity of notation, we write A\u03c0i as Ai for each i in the remaining proof. We refer OPT to the optimal solution, i.e., OPT = max\u03c0 mini f(Ai). W.l.o.g., we assume f(A1) = mini f(Ai). Let ai be the last item to be chosen in block Ai for i = 2, . . . ,m.\nClaim 1: OPT \u2264 f(V \\ {a2, . . . , am}) (37)\nTo show this claim, consider the following: If we enlarge the singleton value of ai, i = 2, . . . ,m, we obtain a new submodular function:\nf \u2032(A) = f(A) + \u03b1 m\u2211 i=2 |A \u2229 ai|, (38)\nwhere \u03b1 is sufficiently large. Then running StreamGreed on f \u2032 with the same ordering of the incoming items leads to the same solution, since only the gain of the last added item for each block is changed.\nNote that f \u2032(A) \u2265 f(A), \u2200A \u2286 V , we then have max\u03c0 mini f \u2032(A\u03c0i ) \u2265 OPT . The optimal partitioning for f \u2032 can be easily obtained as \u03c0\u2032 = (V \\ {a2, . . . , am}, a2, . . . , am). Therefore, we have that\nOPT \u2264 max \u03c0 min i f \u2032(A\u03c0i ) = f \u2032(V \\ {a2, . . . , am}) = f(V \\ {a2, . . . , am}). (39)\nLastly, we have that f(A1) \u2265 f(Ai \\ ai) for any i = 2, . . . ,m due to the procedure of StreamGreed. Therefore we have the following:\nf(A1) \u2265 1\nm (f(A1) + m\u2211 i=2 f(Ai \\ ai)) (40) \u2265 1 m f(V \\ {a2, . . . , am}) // submodularity of f (41) \u2265 1 m OPT // Claim 1 (42)"}, {"heading": "Proof for Theorem 3", "text": "Theorem Given , \u03b1 and any 0 < \u03b4 < \u03b1, GreedSat finds a partition such that at least dm(\u03b1\u2212 \u03b4)e blocks receive utility at least \u03b41\u2212\u03b1+\u03b4 (max\u03c0 mini fi(A\u03c0i )\u2212 ). Proof When GreedSat terminates, it identifies a cmin such that the returned solution \u03c0\u0302cmin satisfies F\u0304 cmin(\u03c0\u0302cmin) \u2265 \u03b1cmin. Also it identifies a cmax such that the returned solution \u03c0\u0302cmax satisfies F\u0304 cmax(\u03c0\u0302cmax) < \u03b1cmax. The gap between cmax and cmin is bounded by , i.e., cmax \u2212 cmin \u2264 .\nNext, we prove that there does not exist any partitioning \u03c0 that satisfies mini fi(A \u03c0 i ) \u2265 cmax, i.e., cmax \u2265 max\u03c0\u2208\u03a0 mini fi(A\u03c0i ). Suppose otherwise, i.e., \u2203\u03c0\u2217 : mini fi(A\u03c0\u2217i ) = cmax + \u03b3 with \u03b3 \u2265 0. Let c = cmax + \u03b3,\nconsider the intermediate objective F\u0304 c(\u03c0) = 1m \u2211m\ni=1 min{fi(A\u03c0i ), c}, we have that F\u0304 c(\u03c0\u2217) = c. An instance of the algorithm for SWP on F\u0304 c is guaranteed to lead to a solution \u03c0\u0302c such that F\u0304 c(\u03c0\u0302c) \u2265 \u03b1c. Since c \u2265 cmax, it should follow that the returned solution \u03c0\u0302cmax for the value cmax also satisfies F\u0304\ncmax(\u03c0\u0302) \u2265 \u03b1cmax. However it contradicts with the termination criterion of GreedSat. Therefore, we prove that cmax \u2265 max\u03c0\u2208\u03a0 mini fi(A\u03c0i ), which indicates that cmin \u2265 cmax \u2212 \u2265 max\u03c0\u2208\u03a0 mini fi(A\u03c0i )\u2212 .\nLet c\u2217 = cmax+cmin2 and the partitioning returned by running for c \u2217 be \u03c0\u0302 (the final output partitioning from GreedSat). We have that F\u0304 c \u2217 (\u03c0\u0302) \u2265 \u03b1c\u2217, we are going to show that for any 0 < \u03b4 < \u03b1, at least a dm(\u03b1 \u2212 \u03b4)e blocks given by \u03c0\u0302 receive utility larger or equal to \u03b4\n1\u2212\u03b1+\u03b4 c \u2217.\nJust to restate the problem: we say that the ith block is (\u03b1, \u03b4)-good if fi(A \u03c0\u0302 i ) \u2265 \u03b41\u2212\u03b1+\u03b4 c\u2217.\nThen the statement becomes: Given 0 < \u03b4 < \u03b1, there is at least md\u03b1\u2212 \u03b4e blocks that are (\u03b1, \u03b4)-good.\nTo prove this statement, we assume, by contradiction, that there is strictly less than dm(\u03b1\u2212 \u03b4)e (\u03b1, \u03b4)-good blocks. Denote the number of (\u03b1, \u03b4)-good blocks as mgood. Then we have that mgood \u2264 dm(\u03b1\u2212 \u03b4)e \u2212 1 < m(\u03b1\u2212 \u03b4). Let \u03b8 = mgoodm be the fraction of (\u03b1, \u03b4) good blocks, then we have that 0 \u2264 \u03b8 < (\u03b1\u2212 \u03b4) < 1. The remaining fraction (1\u2212 \u03b8) of blocks are not good, i.e., they have valuation strictly less than \u03b41\u2212\u03b1+\u03b4 c \u2217. Then, consider the following:\nF\u0304 c \u2217 (\u03c0\u0302) =\n1\nm m\u2211 i=1 min{fi(A\u03c0\u0302i ), c\u2217} (43)\n(a) < \u03b8c\u2217 + (1\u2212 \u03b8) \u03b4\n1\u2212 \u03b1+ \u03b4 c \u2217 (44)\n= \u03b4\n1\u2212 \u03b1+ \u03b4 c \u2217 + 1\u2212 \u03b1 1\u2212 \u03b1+ \u03b4 \u03b8c \u2217 (45)\n(b) <\n\u03b4\n1\u2212 \u03b1+ \u03b4 c \u2217 + 1\u2212 \u03b1 1\u2212 \u03b1+ \u03b4 (\u03b1\u2212 \u03b4)c \u2217 (46) = \u03b1c\u2217 (47)\nInequality (a) follows since good blocks are upper bounded by c\u2217, and not good blocks have values upper bounded by \u03b41\u2212\u03b1+\u03b4 c \u2217. Inequality (b) follows by the assumption on \u03b8. This therefore contradicts the assumption that F\u0304 c \u2217 (\u03c0\u0302) \u2265 \u03b1c\u2217, hence the statement is true.\nThis statement can also be proved using a different strategy. Let f c \u2217 i = min{fi(A\u03c0\u0302i ), c\u2217}\nand fi = fi(A \u03c0\u0302 i ) for all i. For any 0 \u2264 \u03b2 \u2264 1 the following holds:\n\u03b1c\u2217 \u2264 F\u0304 c\u2217(\u03c0\u0302) = 1 m \u2211 i f c \u2217 i \u2264 1 m fi = 1 m \u2211 i:fi<\u03b2c\u2217 fi + 1 m \u2211 i:fi\u2265\u03b2c\u2217 fi < 1 m mbad\u03b2c \u2217 + 1 m mgoodc \u2217\n(48)\nwhere m = mbad + mgood and mgood are the number that are \u03b2-good (i.e., i is \u03b2-good if fi \u2265 \u03b2c\u2217). The goal is to place a lower bound on mgood. From the above\n\u03b1 < (1\u2212 mgood m )\u03b2 + mgood m\n(49)\nwhich means\nmgood \u2265 d \u03b1\u2212 \u03b2 1\u2212 \u03b2 me (50)\nLet \u03b2 = \u03b41\u2212\u03b1+\u03b4 , the statement immediately follows.\nNote c\u2217 = cmin+cmax2 \u2265 cmax \u2212 \u2265 max\u03c0\u2208\u03a0 mini fi(A\u03c0i )\u2212 . Combining pieces together, we have shown that at least dm(\u03b1\u2212 \u03b4)e blocks given by \u03c0\u0302 receive utility larger or equal to\n\u03b4 1\u2212\u03b1+\u03b4 (max\u03c0\u2208\u03a0 mini fi(A \u03c0 i )\u2212 )."}, {"heading": "Proof for Theorem 4", "text": "Theorem MMax achieves a worst-case guarantee of O(mini 1+(|A\u03c0\u0302i |\u22121)(1\u2212\u03bafi (A \u03c0\u0302 i ))\n|A\u03c0\u0302i | \u221a m log3m\n), where\n\u03c0\u0302 = (A\u03c0\u03021 , \u00b7 \u00b7 \u00b7 , A\u03c0\u0302m) is the partition obtained by the algorithm, and \u03baf (A) = 1\u2212minv\u2208V f(v|A\\v)f(v) \u2208 [0, 1] is the curvature of a submodular function f at A \u2286 V . Proof We assume the approximation factor of the algorithm for solving the modular version of Problem 1 is \u03b1 = O( 1\u221a\nm log3m ) Asadpour and Saberi (2010). For notation simplicity,\nwe write \u03c0\u0302 = (A\u03021, . . . , A\u0302m) as the resulting partition after the first iteration of MMax, and \u03c0\u2217 = (A\u22171, . . . , A \u2217 m) as its optimal solution. Note that first iteration suffices to yield the performance guarantee, and the subsequent iterations are designed so as to improve the empirical performance. Since the proxy function for each function fi used for the first iteration are the simple modular upper bound with the form: hi(X) = \u2211 j\u2208X fi(j).\nGiven the curvature of each submodular function fi, we can tightly bound a submodular function fi in the following form Iyer et al. (2013b):\nfi(X) \u2264 hi(X) \u2264 |X|\n1 + (|X| \u2212 1)(1\u2212 \u03bafi(X)) fi(X), \u2200X \u2286 V (51)\nConsider the following:\nmin i fi(A\u0302i) \u2265 min i\n1\n|A\u0302i| 1+(|A\u0302i|\u22121)(1\u2212\u03bafi (A\u0302i))\nhi(A\u0302i) (52)\n\u2265 min i\n1\n|A\u0302i| 1+(|A\u0302i|\u22121)(1\u2212\u03bafi (A\u0302i))\nmin i hi(A\u0302i) (53)\n\u2265 \u03b1min i 1 + (|A\u0302i| \u2212 1)(1\u2212 \u03bafi(A\u0302i)) |A\u0302i| min i hi(A \u2217 i ) (54)\n\u2265 \u03b1min i 1 + (|A\u0302i| \u2212 1)(1\u2212 \u03bafi(A\u0302i)) |A\u0302i| min i fi(A \u2217 i ) (55)\n= O(min i 1 + (|A\u0302i| \u2212 1)(1\u2212 \u03bafi(A\u0302i)) |A\u0302i| \u221a m log3m ) min i fi(A \u2217 i ). (56)"}, {"heading": "Proof for Theorem 5", "text": "Theorem Suppose there exists an algorithm for solving the modular version of SFA with an approximation factor \u03b1 \u2264 1, we have that\nmin i fi(A \u03c0t i ) \u2265 \u03b1mini fi(A \u03c0t\u22121 i ). (57)\nProof Consider the following:\nmin i fi(A \u03c0t\u22121 i ) = mini hi(A \u03c0t\u22121 i )// tightness of modular lower bound. (58)\n\u2264 \u03b1min i hi(A \u03c0t i )// approximation factor of the modular SFA. (59) \u2264 \u03b1hj(A\u03c0tj )//j \u2208 argmin i fi(A \u03c0t i ) (60) \u2264 \u03b1fj(A\u03c0tj )//hj(A \u03c0t\u22121 j ) upper bounds fj everywhere. (61)\n= \u03b1min i fi(A\n\u03c0t i ) (62)"}, {"heading": "Proof for Theorem 6", "text": "Theorem For any > 0, SLB cannot be approximated to a factor of (1 \u2212 )m for any m = o( \u221a n/ log n) with polynomial number of queries even under the homogeneous setting.\nProof We use the same proof techniques as in Svitkina and Fleischer (2008). Consider two submodular functions:\nf1(S) = min{|S|, \u03b1}; (63)\nf2(S) = min{ m\u2211 i=1 min{\u03b2, |S \u2229 Vi|}, \u03b1}; (64)\nwhere {Vi}mi=1 is a uniformly random partitioning of V into m blocks, \u03b1 = nm and \u03b2 = n m2(1\u2212 ) . To be more precise about the uniformly random partitioning, we assign each item into any one of the m blocks with probability 1/m. It can be easily verified that OPT1 = min\u03c0\u2208\u03a0 maxi f1(A \u03c0 i ) = n/m and OPT2 = min\u03c0\u2208\u03a0 maxi f2(A \u03c0 i ) = n m2(1\u2212 ) . The gap is then OPT1OPT2 = m(1\u2212 ). Next, we show that f1 and f2 cannot be distinguished with n\n\u03c9(1) number of queries. Since f1(S) \u2265 f2(S) holds for any S, this is equivalent as showing P{f1(S) > f2(S)} <\nn\u2212\u03c9(1). As shown in Svitkina and Fleischer (2008), P{f1(S) > f2(S)} is maximized when |S| = \u03b1. It suffices to consider only the case of |S| = \u03b1 as follows:\nP{f1(S) > f2(S) : |S| = \u03b1} = P{ m\u2211 i=1 min{\u03b2, |S \u2229 Vi|} < \u03b1 : |S| = \u03b1} (65)\nThe necessary condition for \u2211m\ni=1 min{\u03b2, |S\u2229Vi|} < \u03b1 is that |S\u2229Vi| > \u03b2 is satisfied for some i. Using the Chernoff bound, we have that for any i, it holds P{|S\u2229Vi| > \u03b2} \u2264 e\u2212 2n 3m2 = n\u2212\u03c9(1)\nwhen m = o( \u221a n/ log n). Using the union bound, it holds that the probability for any one block Vi such that |S \u2229 Vi| > \u03b2 is also upper bounded by n\u2212\u03c9(1). Combining all pieces together, we have the following:\nP{f1(S) > f2(S)} \u2264 n\u2212\u03c9(1). (66)\nFinally, we come to prove the Theorem. Suppose the goal is to solve an instance of SLB with f2. Since f1 and f2 are hard to distinguish with polynomial number of function queries, any polynomial time algorithm for solving f2 is equivalent to solving for f1. However, the optimal solution for f1 is \u03b1 = n m , whereas the optimal solution for f2 is \u03b2 = n m2(1\u2212 ) . Therefore, no polynomial time algorithm can find a solution with a factor m(1\u2212 ) for SLB in this case."}, {"heading": "Proof for Theorem 7", "text": "Theorem Lova\u0301szRound is guaranteed to find a partition \u03c0\u0302 \u2208 \u03a0 such that maxi fi(A\u03c0\u0302i ) \u2264 mmin\u03c0\u2208\u03a0 maxi fi(A \u03c0 i ). Proof It suffices to bound the performance loss at the step of rounding the fractional solution {x\u2217i }mi=1, or equivalently, the following:\nmax i f\u0303i(x\n\u2217 i ) \u2265\n1\nm max i fi(Ai), (67)\nwhere {Ai}mi=1 is the resulting partitioning after the rounding. To show Eqn 67, it suffices to show that f\u0303i(x \u2217 i ) \u2265 1mfi(Ai) for all i = 1, . . . ,m. Next, consider the following:\nfi(Ai) = f\u0303i(1Ai) = mf\u0303i( 1\nm 1Ai)// positive homogeneity of Lova\u0301sz extension (68)\nFor any item vj \u2208 Ai, we have x\u2217i (j) \u2265 1m , since \u2211m i=1 x \u2217 i (j) \u2265 1 and x\u2217i (j) = maxi\u2032 xi\u2032(j). Therefore, we have 1m1Ai \u2264 x\u2217i . Since fi is monotone, its extension f\u0303i is also monotone. As a result, fi(Ai) = mf\u0303i( 1 m1Ai) \u2264 mf\u0303i(x\u2217i )."}, {"heading": "Proof for Theorem 8", "text": "Theorem MMin achieves a worst-case guarantee of (2 maxi |A\u03c0\u2217i |\n1+(|A\u03c0\u2217i |\u22121)(1\u2212\u03bafi (A \u03c0\u2217 i ))\n), where\n\u03c0\u2217 = (A\u03c0 \u2217 1 , \u00b7 \u00b7 \u00b7 , A\u03c0 \u2217 m ) denotes the optimal partition. Proof Let \u03b1 = 2 be the approximation factor of the algorithm for solving the modular version of Problem 2 Lenstra et al. (1990). For notation simplicity, we write \u03c0\u0302 = (A\u03021, . . . , A\u0302m) as the resulting partition after the first iteration of MMin, and \u03c0\u2217 = (A\u22171, . . . , A \u2217 m) as its optimal solution. Again the first iteration suffices to yield the performance guarantee, and the subsequent iterations are designed so as to improve the empirical performance. Since the supergradients for each function fi used for the first iteration are the simple modular upper bound with the form: hi(X) = \u2211 j\u2208X fi(j), we can again tightly bound a submodular function fi in the following form:\nfi(X) \u2264 hi(X) \u2264 |X|\n1 + (|X| \u2212 1)(1\u2212 \u03bafi(X)) fi(X), \u2200X \u2286 V (69)\nConsider the following:\nmax i fi(A\u0302i) \u2264 max i hi(A\u0302i) (70)\n\u2264 \u03b1max i hi(A \u2217 i ) (71)\n\u2264 \u03b1max i |A\u2217i | 1 + (|A\u2217i | \u2212 1)(1\u2212 \u03bafi(A\u2217i )) fi(A \u2217 i ) (72)\n\u2264 \u03b1max i |A\u2217i | 1 + (|A\u2217i | \u2212 1)(1\u2212 \u03bafi(A\u2217i )) max i fi(A \u2217 i ) (73)"}, {"heading": "Proof for Theorem 9", "text": "Theorem Suppose there exists an algorithm for solving the modular version of SLB with an approximation factor \u03b1 \u2265 1, we have for each iteration t that\nmax i fi(A \u03c0t i ) \u2264 \u03b1maxi fi(A \u03c0t\u22121 i ). (74)\nProof The proof is symmetric to the one for Theorem 5."}, {"heading": "Proof for Theorem 10", "text": "We prove separately for Problem 1 and Problem 2. Theorem Given an instance of Problem 1 with 0 < \u03bb < 1, CombSfaSwp provides an approximation guarantee of max{min{\u03b1, 1m}, \u03b2\u03b1 \u03bb\u0304\u03b2+\u03b1 , \u03bb\u03b2} in the homogeneous case, and a factor of max{ \u03b2\u03b1 \u03bb\u0304\u03b2+\u03b1\n, \u03bb\u03b2} in the heterogeneous case, where \u03b1 and \u03b2 are the approximation factors of AlgWC and AlgAC for SFA and SWP respectively. Proof We first prove the result for heterogeneous setting. For notation simplicity we write the worst-case objective as F1(\u03c0) = mini=1,...,m f(A \u03c0 i ) and the average-case objective as F2(\u03c0) = 1 m \u2211 i=1,...,m f(A \u03c0 i ).\nSuppose AlgWC outputs a partition \u03c0\u03021 and AlgAC outputs a partition \u03c0\u03022. Let \u03c0\u2217 \u2208 arg max\u03c0\u2208\u03a0 \u03bb\u0304F1(\u03c0) + \u03bbF2(\u03c0) be the optimal partition.\nWe use the following facts: Fact1\nF1(\u03c0\u03021) \u2265 \u03b1F1(\u03c0) (75)\nFact2\nF2(\u03c02) \u2265 \u03b2F2(\u03c0) (76)\nFact3\nF1(\u03c0) \u2264 F2(\u03c0) (77)\nThen we have that\n\u03bb\u0304F1(\u03c0\u03022) + \u03bbF2(\u03c0\u03022) \u2265 \u03bbF2(\u03c0\u03022) (78) \u2265 \u03bb\u03b2F2(\u03c0\u2217) (79) \u2265 \u03bb\u03b2 [ \u03bb\u0304F1(\u03c0 \u2217) + \u03bbF2(\u03c0 \u2217) ] (80)\nand\n\u03bb\u0304F1(\u03c0\u03021) + \u03bbF2(\u03c0\u03021) \u2265 \u00b5 [ \u03bb\u0304F1(\u03c0\u03021) + \u03bbF2(\u03c0\u03021) ] + (1\u2212 \u00b5) [ \u03bb\u0304F1(\u03c0\u03021) + \u03bbF2(\u03c0\u03021) ] (81)\n\u2265 \u00b5 [ \u03bb\u0304\u03b1F1(\u03c0 \u2217) + \u03bb\u03b1F1(\u03c0 \u2217) ]\n+ (1\u2212 \u00b5) [0 + \u03bb\u03b2F2(\u03c0\u2217)] (82) \u2265 \u00b5\u03b1\n\u03bb\u0304 \u03bb\u0304F1(\u03c0\n\u2217) + (1\u2212 \u00b5)\u03b2\u03bbF2(\u03c0\u2217) (83)\n\u2265 min{\u00b5\u03b1 \u03bb\u0304 , (1\u2212 \u00b5)\u03b2}\n[ \u03bb\u0304F1(\u03c0 \u2217) + \u03bbF2(\u03c0 \u2217) ]\n(84)\nmin{\u00b5\u03b1 \u03bb\u0304 , (1 \u2212 \u00b5)\u03b2} is a function over 0 \u2264 \u00b5 \u2264 1 and \u00b5\u2217 \u2208 arg max\u00b5 min{\u00b5\u03b1\u03bb\u0304 , (1 \u2212 \u00b5)\u03b2}.\nIt is easy to show\n\u00b5\u2217 = \u03bb\u0304\u03b2\n\u03bb\u0304\u03b2 + \u03b1 (85)\nmax \u00b5 min{\u00b5\u03b1 \u03bb\u0304 , (1\u2212 \u00b5)\u03b2} = \u03b2\u03b1 \u03bb\u0304\u03b2 + \u03b1 (86)\n\u03bb\u0304F1(\u03c0\u03021) + \u03bbF2(\u03c0\u03021) \u2265 \u03b2\u03b1\n\u03bb\u0304\u03b2 + \u03b1\n[ \u03bb\u0304F1(\u03c0 \u2217) + \u03bbF2(\u03c0 \u2217) ]\n(87)\nTaking the max over the two bounds leads to\nmax{\u03bb\u0304F1(\u03c0\u03021) + \u03bbF2(\u03c0\u03021), \u03bb\u0304F1(\u03c0\u03022) + \u03bbF2(\u03c0\u03022)} \u2265 max{ \u03b2\u03b1\n\u03bb\u0304\u03b2 + \u03b1 , \u03bb\u03b2}max \u03c0\u2208\u03a0 \u03bb\u0304F1(\u03c0) + \u03bbF2(\u03c0)\n(88)\nNext we are going to show the result for the homogeneous setting. We have the following facts that hold for arbitrary partition \u03c0:\nF1(\u03c0\u03021) \u2265 \u03b1F1(\u03c0), F2(\u03c0\u03022) \u2265 \u03b2F2(\u03c0) (89)\nF1(\u03c0) \u2264 F2(\u03c0), F2(\u03c01) \u2265 1\nm F2(\u03c0) (90)\nConsider the following:\n\u03bb\u0304F1(\u03c0\u03021) + \u03bbF2(\u03c0\u03021) \u2265 \u03b1\u03bb\u0304F1(\u03c0\u2217) + \u03bb\nm F2(\u03c0\n\u2217) (91)\n\u2265 min{\u03b1, 1 m } [ \u03bb\u0304F1(\u03c0 \u2217) + \u03bbF2(\u03c0 \u2217) ]\n(92)\nand\n\u03bb\u0304F1(\u03c0\u03022) + \u03bbF2(\u03c0\u03022) \u2265 \u03bbF2(\u03c0\u03022) (93) \u2265 \u03bb\u03b2F2(\u03c0\u2217) (94) \u2265 \u03bb\u03b2 [ \u03bb\u0304F1(\u03c0 \u2217) + \u03bbF2(\u03c0 \u2217) ] (95)\nTaking the max over the two bounds and the result shown in Eqn 87 gives the following:\nmax{\u03bb\u0304F1(\u03c0\u03021) + \u03bbF2(\u03c0\u03021), \u03bb\u0304F1(\u03c0\u03022) + \u03bbF2(\u03c0\u03022)} \u2265 max{min{\u03b1, 1 m }, \u03b2\u03b1 \u03bb\u0304\u03b2 + \u03b1 , \u03bb\u03b2}max \u03c0\u2208\u03a0 \u03bb\u0304F1(\u03c0) + \u03bbF2(\u03c0).\n(96)\nTheorem CombSlbSmp provides an approximation guarantee of min{m, m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+\u03bb)} in the homogeneous case, and a factor of min{ m\u03b1\nm\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} in the heterogeneous case,\nfor Problem 2 with 0 \u2264 \u03bb \u2264 1.\nProof Let \u03c0\u03021 be the solution of AlgWC and \u03c0\u03022 be the solution of AlgAC. Let \u03c0 \u2217 \u2208 arg min\u03c0\u2208\u03a0 \u03bb\u0304F1(\u03c0) +\u03bbF2(\u03c0) be the optimal partition. The following facts hold for all \u03c0 \u2208 \u03a0: Fact1\nF1(\u03c0\u03021) \u2264 \u03b1F1(\u03c0) (97)\nFact2\nF2(\u03c0\u03022) \u2264 \u03b2F2(\u03c0) (98)\nFact3\nF2(\u03c0) \u2264 F1(\u03c0) \u2264 mF2(\u03c0) (99)\nThen we have the following:\n\u03bb\u0304F1(\u03c0\u03021) + \u03bbF2(\u03c0\u03021) \u2264 F1(\u03c0\u03021) (100) \u2264 \u03b1F1(\u03c0\u2217) (101)\n\u2264 \u03b1 \u03bb\u0304+ \u03bbm\n[ \u03bb\u0304F1(\u03c0 \u2217) + \u03bb\nm F1(\u03c0\n\u2217) ] (102)\n\u2264 m\u03b1 m\u03bb\u0304+ \u03bb\n[ \u03bb\u0304F1(\u03c0 \u2217) + \u03bbF2(\u03c0 \u2217) ]\n(103)\nand\n\u03bb\u0304F1(\u03c0\u03022) + \u03bbF2(\u03c0\u03022) \u2264 \u03b2m\u03bb\u0304F2(\u03c0\u2217) + \u03b2\u03bbF2(\u03c0\u2217) (104) \u2264 (m\u03bb\u0304+ \u03bb)\u03b2F2(\u03c0\u2217) (105) \u2264 (m\u03bb\u0304+ \u03bb)\u03b2 [ \u03bb\u0304F1(\u03c0 \u2217) + \u03bbF2(\u03c0 \u2217) ] (106)\n(107)\nTaking the minimum over the two leads to the following:\nmin{\u03bb\u0304F1(\u03c0\u03021) + \u03bbF2(\u03c0\u03021), \u03bb\u0304F1(\u03c0\u03022) + \u03bbF2(\u03c0\u03022)} \u2264 min{ m\u03b1\nm\u03bb\u0304+ \u03bb , \u03b2(m\u03bb\u0304+ \u03bb)}min \u03c0\u2208\u03a0 \u03bb\u0304F1(\u03c0) + \u03bbF2(\u03c0)\n(108)\nEquation 108 gives us a bound for both the homogeneous setting and the heterogeneous settings.\nFurthermore, in the homogeneous setting, for arbitrary partition \u03c0, we have\n\u03bb\u0304F1(\u03c0) + \u03bbF2(\u03c0) \u2264 mmin \u03c0\u2208\u03a0 \u03bb\u0304F1(\u03c0) + \u03bbF2(\u03c0) (109)\nand we can tighten the bound for the homogeneous setting as follows:\nmin{\u03bb\u0304F1(\u03c0\u03021) + \u03bbF2(\u03c0\u03021), \u03bb\u0304F1(\u03c0\u03022) + \u03bbF2(\u03c0\u03022)} \u2264 min{m, m\u03b1\nm\u03bb\u0304+ \u03bb , \u03b2(m\u03bb\u0304+ \u03bb)}max \u03c0\u2208\u03a0 \u03bb\u0304F1(\u03c0) + \u03bbF2(\u03c0)\n(110)"}, {"heading": "Proof for Theorem 11", "text": "Theorem Given , \u03b1, and, 0 \u2264 \u03bb \u2264 1, GeneralGreedSat finds a partition \u03c0\u0302 that satisfies the following:\n\u03bb\u0304min i fi(A\n\u03c0\u0302 i ) + \u03bb\n1\nm m\u2211 i=1 fi(A \u03c0\u0302 i ) \u2265 \u03bb\u03b1(OPT \u2212 ), (111)\nwhere OPT = max\u03c0\u2208\u03a0 \u03bb\u0304mini fi(A \u03c0 i ) + \u03bb 1 m \u2211m i=1 fi(A \u03c0 i ).\nMoreover, let F\u03bb,i(\u03c0) = \u03bb\u0304fi(A \u03c0 i ) + \u03bb 1 m \u2211m j=1 fj(A \u03c0 j ). Given any 0 < \u03b4 < \u03b1, there is a set\nI \u2286 {1, . . . ,m} such that |I| \u2265 dm(\u03b1\u2212 \u03b4)e and\nFi,\u03bb(\u03c0\u0302) \u2265 max{ \u03b4\n1\u2212 \u03b1+ \u03b4 , \u03bb\u03b1}(OPT \u2212 ),\u2200i \u2208 I. (112)\nProof Denote intermediate objective F\u0304 c(\u03c0) = 1m \u2211m i=1 min{\u03bb\u0304fi(A\u03c0i ) + \u03bb 1m \u2211m j=1 fj(A \u03c0 j ), c}. Also we define the overall objective as F (\u03c0) = \u03bb\u0304mini fi(A \u03c0 i ) + \u03bb 1 m \u2211m i=1 fi(A \u03c0 i ). When the algorithm terminates, it identifies a cmin such that the returned solution \u03c0\u0302 cmin satisfies F\u0304 cmin(\u03c0\u0302cmin) \u2265 \u03b1cmin. Also it identifies a cmax such that the returned solution \u03c0\u0302cmax satisfies F\u0304 cmax(\u03c0\u0302cmax) < \u03b1cmax. The gap between cmax and cmin is bounded by , i.e., cmax\u2212 cmin \u2264 .\nNext, we prove that there does not exist any partitioning \u03c0 that satisfies F (\u03c0) \u2265 cmax, i.e., cmax \u2265 OPT .\nSuppose otherwise, i.e., \u2203\u03c0\u2217 : F (\u03c0\u2217) = cmax + \u03b3 with \u03b3 \u2265 0. Let c = cmax + \u03b3, consider the intermediate objective F\u0304 c(\u03c0), we have that F\u0304 c(\u03c0\u2217) = c. An instance of the algorithm for SWP on F\u0304 c is guaranteed to lead to a solution \u03c0\u0302c such that F\u0304 c(\u03c0\u0302c) \u2265 \u03b1c. Since c \u2265 cmax, it should follow that the returned solution \u03c0\u0302cmax for the value cmax also satisfies F\u0304 cmax(\u03c0\u0302) \u2265 \u03b1cmax. However it contradicts with the termination criterion of GreedSat. Therefore, we prove that cmax \u2265 OPT , which indicates that cmin \u2265 cmax \u2212 \u2265 OPT \u2212 .\nLet c\u2217 = cmax+cmin2 and the partitioning returned by running for c \u2217 be \u03c0\u0302 (the final output\npartitioning from the algorithm). We have that F\u0304 c \u2217 (\u03c0\u0302) \u2265 \u03b1c\u2217.\nNext we are ready to prove the Theorem: F (\u03c0\u0302) \u2265 \u03bb\u03b1. For simplicity of notation, we rewrite yi = \u03bb\u0304fi(A \u03c0\u0302 i ) + \u03bb 1 m \u2211m j=1 fj(A \u03c0\u0302 j ) and xi = min{\u03bb\u0304fi(A\u03c0\u0302i ) + \u03bb 1m \u2211m j=1 fj(A \u03c0\u0302 j ), c\n\u2217} = min{yi, c\u2217} for each i. Furthermore, we denote the sample mean as x\u0304 = 1m \u2211m i=1 xi and\ny\u0304 = 1m \u2211m i=1 yi. Then, we have F (\u03c0\u0302) = mini yi and F\u0304 c\u2217(\u03c0\u0302) = x\u0304. We list the following facts to facilitate the analysis:\n1. 0 \u2264 xi \u2264 c\u2217 holds for all i;\n2. yi \u2265 \u03bby\u0304 holds for all i;\n3. xi \u2265 \u03bbx\u0304 holds for all i;\n4. x\u0304 \u2265 \u03b1c\u2217;\n5. xi = min{yi, c\u2217},\u2200i.\nThe second fact follows since\ny\u0304 = 1\nm m\u2211 i=1 yi (113)\n= 1\nm m\u2211 i=1 {\u03bb\u0304fi(A\u03c0\u0302i ) + \u03bb 1 m m\u2211 j=1 fj(A \u03c0\u0302 j )} (114)\n= 1\nm m\u2211 j=1 fj(A \u03c0\u0302 j ) \u2264 yi \u03bb\n(115)\nGiven the second fact, we can prove the third fact as follows. Let i\u2217 \u2208 argmini yi. By definition xi = min{yi, c\u2217}, then i\u2217 \u2208 argmini xi. We consider the two cases:\n(1) yi\u2217 \u2264 c\u2217: In this case, we have that xi\u2217 = yi\u2217 . Since xi \u2264 yi, \u2200i, it holds that x\u0304 \u2264 y\u0304. The third fact follows as xi\u2217 = yi\u2217 \u2265 \u03bby\u0304 \u2265 \u03bbx\u0304.\n(2) yi\u2217 > c \u2217: In this case, yi \u2265 c\u2217 holds for all i. As a result, we have xi = c\u2217,\u2200i.\nTherefore, xi = x\u0304 = c \u2217 \u2265 \u03bbc\u2217.\nCombining fact 3 and 4, it follows for each i:\n\u03bb\u0304fi(A \u03c0\u0302 i ) + \u03bb\n1\nm m\u2211 j=1 fj(A \u03c0\u0302 j ) = yi \u2265 xi \u2265 \u03bbx\u0304 \u2265 \u03b1\u03bbc\u2217 \u2265 \u03b1\u03bb(OPT \u2212 ). (116)\nThe first part of the Theorem is then proved.\nThe second part of the Theorem simply follows from the proof in Theorem 3 and Eqn 116."}, {"heading": "Proof for Theorem 12", "text": "Theorem Define F \u03bb(\u03c0) = \u03bb\u0304maxi fi(A \u03c0 i ) + \u03bb 1 m \u2211m i=1 fi(A \u03c0 i ) for any 0 \u2264 \u03bb \u2264 1. GeneralLova\u0301sz Round is guaranteed to find a partition \u03c0\u0302 \u2208 \u03a0 such that\nF \u03bb(\u03c0\u0302) \u2264 mmin \u03c0\u2208\u03a0 F \u03bb(\u03c0) (117)\nProof We essentially use the same proof technique in Theorem 7 to show this result. After solving for the continuous solution {x\u2217i \u2208 Rn}mi=1, the rounding step simply chooses for each j = 1, . . . , n, assigns the item to the block i\u2217 \u2208 argmaxi=1,...,m x\u2217i (j). We denote the resulting partitioning as \u03c0\u0302 = {A\u03c0\u0302i }mi=1.\nIt suffices to bound the performance loss at the step of rounding the fractional solution {x\u2217i }mi=1, or equivalently, the following:\nf\u0303i(x \u2217 i ) \u2265\n1\nm fi(A\n\u03c0\u0302 i ), (118)\nGiven Eqn 118, the Theorem follows since\nF \u03bb(\u03c0\u2217) \u2265 \u03bb\u0304max i f\u0303i(x \u2217 i ) + \u03bb\n1\nm m\u2211 j=1 f\u0303j(x \u2217 j ) (119)\n\u2265 1 m [\u03bb\u0304max i fi(A \u03c0\u0302 i ) + \u03bb m m\u2211 j=1 fj(A \u03c0\u0302 j )] (120) \u2265 1 m F \u03bb(\u03c0\u0302). (121)\nTo prove Eqn 67, consider the following:\nfi(A \u03c0\u0302 i ) = f\u0303i(1A\u03c0\u0302i\n) = mf\u0303i( 1\nm 1A\u03c0\u0302i )// positive homogeneity of Lova\u0301sz extension (122)\nFor any item vj \u2208 A\u03c0\u0302i , we have x\u2217i (j) \u2265 1m , since \u2211m i=1 x \u2217 i (j) \u2265 1 and x\u2217i (j) = maxi\u2032 xi\u2032(j).\nTherefore, we have 1m1Ai \u2264 x\u2217i . Since fi is monotone, its extension f\u0303i is also monotone. As a result, fi(Ai) = mf\u0303i( 1 m1Ai) \u2264 mf\u0303i(x\u2217i )."}], "references": [{"title": "Streaming min-max hypergraph partitioning", "author": ["Dan Alistarh", "Jennifer Iglesias", "Milan Vojnovic"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Alistarh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Alistarh et al\\.", "year": 2015}, {"title": "k-means++: The advantages of careful seeding", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "In SODA,", "citeRegEx": "Arthur and Vassilvitskii.,? \\Q2007\\E", "shortCiteRegEx": "Arthur and Vassilvitskii.", "year": 2007}, {"title": "An approximation algorithm for max-min fair allocation of indivisible goods", "author": ["Arash Asadpour", "Amin Saberi"], "venue": "In SICOMP,", "citeRegEx": "Asadpour and Saberi.,? \\Q2010\\E", "shortCiteRegEx": "Asadpour and Saberi.", "year": 2010}, {"title": "Matlab wrapper for graph cut", "author": ["Shai Bagon"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bagon.,? \\Q2006\\E", "shortCiteRegEx": "Bagon.", "year": 2006}, {"title": "Handbook of Mathematical Models in Computer Vision, chapter Graph Cuts in Vision and Graphics: Theories and Applications", "author": ["Y. Boykov", "O. Veksler"], "venue": null, "citeRegEx": "Boykov and Veksler.,? \\Q2006\\E", "shortCiteRegEx": "Boykov and Veksler.", "year": 2006}, {"title": "An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision", "author": ["Yuri Boykov", "Vladimir Kolmogorov"], "venue": "IEEE transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Boykov and Kolmogorov.,? \\Q2004\\E", "shortCiteRegEx": "Boykov and Kolmogorov.", "year": 2004}, {"title": "Efficient approximate energy minimization via graph cuts", "author": ["Yuri Boykov", "Olga Veksler", "Ramin Zabih"], "venue": "IEEE transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Boykov et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Boykov et al\\.", "year": 2001}, {"title": "Partitions of graphs into one or two independent sets and cliques", "author": ["Andreas Brandst\u00e4dt"], "venue": "Discrete Mathematics,", "citeRegEx": "Brandst\u00e4dt.,? \\Q1996\\E", "shortCiteRegEx": "Brandst\u00e4dt.", "year": 1996}, {"title": "A tight linear time (1/2)-approximation for unconstrained submodular maximization", "author": ["Niv Buchbinder", "Moran Feldman", "Joseph Naor", "Roy Schwartz"], "venue": "In FOCS,", "citeRegEx": "Buchbinder et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Buchbinder et al\\.", "year": 2012}, {"title": "Approximation algorithms for submodular multiway partition", "author": ["Chandra Chekuri", "Alina Ene"], "venue": "In FOCS,", "citeRegEx": "Chekuri and Ene.,? \\Q2011\\E", "shortCiteRegEx": "Chekuri and Ene.", "year": 2011}, {"title": "Submodular cost allocation problem and applications", "author": ["Chandra Chekuri", "Alina Ene"], "venue": "In Automata, Languages and Programming,", "citeRegEx": "Chekuri and Ene.,? \\Q2011\\E", "shortCiteRegEx": "Chekuri and Ene.", "year": 2011}, {"title": "Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection", "author": ["A. Das", "D. Kempe"], "venue": "arXiv preprint arXiv:1102.3975,", "citeRegEx": "Das and Kempe.,? \\Q2011\\E", "shortCiteRegEx": "Das and Kempe.", "year": 2011}, {"title": "A deterministic algorithm for maximizing submodular functions", "author": ["Shahar Dobzinski", "Ami Mor"], "venue": "arXiv preprint arXiv:1507.07237,", "citeRegEx": "Dobzinski and Mor.,? \\Q2015\\E", "shortCiteRegEx": "Dobzinski and Mor.", "year": 2015}, {"title": "Local distribution and the symmetry gap: Approximability of multiway partitioning problems", "author": ["Alina Ene", "Jan Vondr\u00e1k", "Yi Wu"], "venue": "In SODA,", "citeRegEx": "Ene et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ene et al\\.", "year": 2013}, {"title": "Complexity of graph partition problems", "author": ["Tomas Feder", "Pavol Hell", "Sulamita Klein", "Rajeev Motwani"], "venue": "In Proceedings of the thirty-first annual ACM symposium on Theory of computing,", "citeRegEx": "Feder et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Feder et al\\.", "year": 1999}, {"title": "Maximizing non-monotone submodular functions", "author": ["Uriel Feige", "Vahab Mirrokni", "Jan Vondr\u00e1k"], "venue": "SIAM J. COMPUT.,", "citeRegEx": "Feige et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Feige et al\\.", "year": 2011}, {"title": "An analysis of approximations for maximizing submodular set functions\u2014II", "author": ["M.L. Fisher", "G.L. Nemhauser", "L.A. Wolsey"], "venue": "In Polyhedral combinatorics,", "citeRegEx": "Fisher et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Fisher et al\\.", "year": 1978}, {"title": "Submodular functions and optimization, volume 58", "author": ["Satoru Fujishige"], "venue": null, "citeRegEx": "Fujishige.,? \\Q2005\\E", "shortCiteRegEx": "Fujishige.", "year": 2005}, {"title": "A review of robust clustering methods", "author": ["L.A. Gar\u0107\u0131a-Escudero", "A. Gordaliza", "C. Matr\u00e1n", "A. Mayo-Iscar"], "venue": "Advances in Data Analysis and Classification,", "citeRegEx": "Gar\u0107\u0131a.Escudero et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gar\u0107\u0131a.Escudero et al\\.", "year": 2010}, {"title": "Approximating submodular functions everywhere", "author": ["Michel Goemans", "Nicholas Harvey", "Satoru Iwata", "Vahab Mirrokni"], "venue": "In SODA,", "citeRegEx": "Goemans et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goemans et al\\.", "year": 2009}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["Michel X Goemans", "David P Williamson"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Goemans and Williamson.,? \\Q1995\\E", "shortCiteRegEx": "Goemans and Williamson.", "year": 1995}, {"title": "Max-min fair allocation of indivisible goods", "author": ["Daniel Golovin"], "venue": "Technical Report CMU-CS-05144,", "citeRegEx": "Golovin.,? \\Q2005\\E", "shortCiteRegEx": "Golovin.", "year": 2005}, {"title": "Partitioning chordal graphs into independent sets and cliques", "author": ["Pavol Hell", "Sulamita Klein", "Loana Tito Nogueira", "F\u00e1bio Protti"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "Hell et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hell et al\\.", "year": 2004}, {"title": "Efficient visual exploration and coverage with a micro aerial vehicle in unknown environments", "author": ["L. Heng", "A. Gotovos", "A. Krause", "M. Pollefeys"], "venue": "In IEEE Int. Conf. on Robotics and Automation (ICRA),", "citeRegEx": "Heng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Heng et al\\.", "year": 2015}, {"title": "A polynomial approximation scheme for scheduling on uniform processors: Using the dual approximation approach", "author": ["Dorit S Hochbaum", "David B Shmoys"], "venue": "SICOMP,", "citeRegEx": "Hochbaum and Shmoys.,? \\Q1988\\E", "shortCiteRegEx": "Hochbaum and Shmoys.", "year": 1988}, {"title": "The submodular Bregman and Lov\u00e1sz-Bregman divergences with applications", "author": ["R. Iyer", "J. Bilmes"], "venue": "In NIPS,", "citeRegEx": "Iyer and Bilmes.,? \\Q2012\\E", "shortCiteRegEx": "Iyer and Bilmes.", "year": 2012}, {"title": "Algorithms for approximate minimization of the difference between submodular functions, with applications", "author": ["R. Iyer", "J. Bilmes"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Iyer and Bilmes.,? \\Q2012\\E", "shortCiteRegEx": "Iyer and Bilmes.", "year": 2012}, {"title": "Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints", "author": ["R. Iyer", "J. Bilmes"], "venue": "In NIPS,", "citeRegEx": "Iyer and Bilmes.,? \\Q2013\\E", "shortCiteRegEx": "Iyer and Bilmes.", "year": 2013}, {"title": "Fast semidifferential based submodular function optimization", "author": ["R. Iyer", "S. Jegelka", "J. Bilmes"], "venue": "In ICML,", "citeRegEx": "Iyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Iyer et al\\.", "year": 2013}, {"title": "Curvature and Efficient Approximation Algorithms for Approximation and Minimization of Submodular Functions", "author": ["Rishabh Iyer", "Stefanie Jegelka", "Jeff Bilmes"], "venue": null, "citeRegEx": "Iyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Iyer et al\\.", "year": 2013}, {"title": "Monotone closure of relaxed constraints in submodular optimization: Connections between minimization and maximization", "author": ["Rishabh Iyer", "Stefanie Jegelka", "Jeff Bilmes"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Iyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Iyer et al\\.", "year": 2014}, {"title": "Submodularity beyond submodular energies: coupling edges in graph cuts", "author": ["S. Jegelka", "J. Bilmes"], "venue": "In CVPR,", "citeRegEx": "Jegelka and Bilmes.,? \\Q2011\\E", "shortCiteRegEx": "Jegelka and Bilmes.", "year": 2011}, {"title": "Approximation algorithms for the max-min allocation problem", "author": ["Subhash Khot", "Ashok Ponnuswami"], "venue": "In APPROX,", "citeRegEx": "Khot and Ponnuswami.,? \\Q2007\\E", "shortCiteRegEx": "Khot and Ponnuswami.", "year": 2007}, {"title": "A principled deep random field for image segmentation", "author": ["P. Kohli", "A. Osokin", "S. Jegelka"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Kohli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kohli et al\\.", "year": 2013}, {"title": "Near-optimal sensor placements in Gaussian processes: Theory, efficient algorithms and empirical studies", "author": ["A. Krause", "A. Singh", "C. Guestrin"], "venue": "In JMLR,", "citeRegEx": "Krause et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2008}, {"title": "Robust submodular observation selection", "author": ["Andreas Krause", "Brendan McMahan", "Carlos Guestrin", "Anupam Gupta"], "venue": "In JMLR,", "citeRegEx": "Krause et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2008}, {"title": "Approximation algorithms for scheduling unrelated parallel machines", "author": ["Jan Karel Lenstra", "David B Shmoys", "\u00c9va Tardos"], "venue": "In Mathematical programming,", "citeRegEx": "Lenstra et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Lenstra et al\\.", "year": 1990}, {"title": "Graph partitioning via parallel submodular approximation to accelerate distributed machine learning", "author": ["Mu Li", "Dave Andersen", "Alexander Smola"], "venue": "In arXiv preprint arXiv:1505.04636,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Least squares quantization in pcm", "author": ["S. Lloyd"], "venue": "IEEE Transactions on IT,", "citeRegEx": "Lloyd.,? \\Q1982\\E", "shortCiteRegEx": "Lloyd.", "year": 1982}, {"title": "Accelerated greedy algorithms for maximizing submodular set functions", "author": ["M. Minoux"], "venue": "In Optimization Techniques,", "citeRegEx": "Minoux.,? \\Q1978\\E", "shortCiteRegEx": "Minoux.", "year": 1978}, {"title": "Minimum average cost clustering", "author": ["Kiyohito Nagano", "Yoshinobu Kawahara", "Satoru Iwata"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Nagano et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nagano et al\\.", "year": 2010}, {"title": "A faster strongly polynomial time algorithm for submodular function minimization", "author": ["J.B. Orlin"], "venue": "Mathematical Programming,", "citeRegEx": "Orlin.,? \\Q2009\\E", "shortCiteRegEx": "Orlin.", "year": 2009}, {"title": "Parallel training of deep neural networks with natural gradient and parameter averaging", "author": ["Daniel Povey", "Xiaohui Zhang", "Sanjeev Khudanpur"], "venue": "arXiv preprint arXiv:1410.7455,", "citeRegEx": "Povey et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Povey et al\\.", "year": 2014}, {"title": "Combinatorial optimization: polyhedra and efficiency, volume 24", "author": ["Alexander Schrijver"], "venue": null, "citeRegEx": "Schrijver.,? \\Q2003\\E", "shortCiteRegEx": "Schrijver.", "year": 2003}, {"title": "Communal cuts: Sharing cuts across images", "author": ["E. Shelhamer", "S. Jegelka", "T. Darrell"], "venue": "In NIPS workshop on Discrete Optimization in Machine Learning,", "citeRegEx": "Shelhamer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shelhamer et al\\.", "year": 2014}, {"title": "A contour completion model for augmenting surface reconstructions", "author": ["Nathan Silberman", "Lior Shapira", "Ran Gal", "Pushmeet Kohli"], "venue": "In Europ. Conf. on Computer Vision (ECCV),", "citeRegEx": "Silberman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silberman et al\\.", "year": 2014}, {"title": "Anticlustering: maximizing the variance criterion", "author": ["H Sp\u00e4th"], "venue": "Control and Cybernetics,", "citeRegEx": "Sp\u00e4th.,? \\Q1986\\E", "shortCiteRegEx": "Sp\u00e4th.", "year": 1986}, {"title": "Efficient minimization of decomposable submodular functions", "author": ["P. Stobbe", "A. Krause"], "venue": "In NIPS,", "citeRegEx": "Stobbe and Krause.,? \\Q2010\\E", "shortCiteRegEx": "Stobbe and Krause.", "year": 2010}, {"title": "Submodular approximation: Sampling-based algorithms and lower bounds", "author": ["Z. Svitkina", "L. Fleischer"], "venue": "In FOCS,", "citeRegEx": "Svitkina and Fleischer.,? \\Q2008\\E", "shortCiteRegEx": "Svitkina and Fleischer.", "year": 2008}, {"title": "Submodular approximation: Sampling-based algorithms and lower bounds", "author": ["Zoya Svitkina", "Lisa Fleischer"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Svitkina and Fleischer.,? \\Q2011\\E", "shortCiteRegEx": "Svitkina and Fleischer.", "year": 2011}, {"title": "Superdifferential cuts for binary energies", "author": ["T. Taniai", "Y. Matsushita", "T. Naemura"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Taniai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Taniai et al\\.", "year": 2015}, {"title": "Moore\u2019s law: the future of si microelectronics", "author": ["Scott E Thompson", "Srivatsan Parthasarathy"], "venue": "materials today,", "citeRegEx": "Thompson and Parthasarathy.,? \\Q2006\\E", "shortCiteRegEx": "Thompson and Parthasarathy.", "year": 2006}, {"title": "Learning mixtures of submodular functions for image collection summarization", "author": ["Sebastian Tschiatschek", "Rishabh K Iyer", "Haochen Wei", "Jeff A Bilmes"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Tschiatschek et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tschiatschek et al\\.", "year": 2014}, {"title": "Set partition principles", "author": ["Venceslav Valev"], "venue": "In Transactions of the Ninth Prague Conference on Information Theory, Statistical Decision Functions, and Random Processes,(Prague,", "citeRegEx": "Valev.,? \\Q1982\\E", "shortCiteRegEx": "Valev.", "year": 1982}, {"title": "Set partition principles revisited", "author": ["Ventzeslav Valev"], "venue": null, "citeRegEx": "Valev.,? \\Q1983\\E", "shortCiteRegEx": "Valev.", "year": 1983}, {"title": "A tutorial on spectral clustering", "author": ["Ulrike Von Luxburg"], "venue": "Statistics and computing,", "citeRegEx": "Luxburg.,? \\Q2007\\E", "shortCiteRegEx": "Luxburg.", "year": 2007}, {"title": "Optimal approximation for the submodular welfare problem in the value oracle model", "author": ["Jan Vondr\u00e1k"], "venue": "In STOC,", "citeRegEx": "Vondr\u00e1k.,? \\Q2008\\E", "shortCiteRegEx": "Vondr\u00e1k.", "year": 2008}, {"title": "Using document summarization techniques for speech data subset selection", "author": ["Kai Wei", "Yuzong Liu", "Katrin Kirchhoff", "Jeff Bilmes"], "venue": "In North American Chapter of the Association for Computational Linguistics/Human Language Technology Conference", "citeRegEx": "Wei et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2013}, {"title": "Fast multi-stage submodular maximization", "author": ["Kai Wei", "Rishabh Iyer", "Jeff Bilmes"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Wei et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2014}, {"title": "Submodular subset selection for large-scale speech training data", "author": ["Kai Wei", "Yuzong Liu", "Katrin Kirchhoff", "Chris Bartels", "Jeff Bilmes"], "venue": "In Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, Florence,", "citeRegEx": "Wei et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2014}, {"title": "Unsupervised submodular subset selection for speech data", "author": ["Kai Wei", "Yuzong Liu", "Katrin Kirchhoff", "Jeff Bilmes"], "venue": "In IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Florence,", "citeRegEx": "Wei et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2014}, {"title": "Submodularity in data subset selection and active learning", "author": ["Kai Wei", "Rishabh Iyer", "Jeff Bilmes"], "venue": "In ICML,", "citeRegEx": "Wei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2015}, {"title": "On generalized greedy splitting algorithms for multiway partition problems", "author": ["Liang Zhao", "Hiroshi Nagamochi", "Toshihide Ibaraki"], "venue": "Discrete applied mathematics,", "citeRegEx": "Zhao et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2004}, {"title": "Submodular attribute selection for action recognition in video", "author": ["Jingjing Zheng", "Zhuolin Jiang", "Rama Chellappa", "Jonathon P Phillips"], "venue": "In NIPS,", "citeRegEx": "Zheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 18, "context": "These problems generalize both purely robust instances of the problem (namely max-min submodular fair allocation (SFA) Golovin (2005) and min-max submodular load balancing (SLB) Svitkina and Fleischer (2008)) and also generalize average-case instances (that is the submodular welfare problem (SWP) Vondr\u00e1k (2008) and submodular multiway partition (SMP) Chekuri and Ene (2011a)).", "startOffset": 119, "endOffset": 134}, {"referenceID": 18, "context": "These problems generalize both purely robust instances of the problem (namely max-min submodular fair allocation (SFA) Golovin (2005) and min-max submodular load balancing (SLB) Svitkina and Fleischer (2008)) and also generalize average-case instances (that is the submodular welfare problem (SWP) Vondr\u00e1k (2008) and submodular multiway partition (SMP) Chekuri and Ene (2011a)).", "startOffset": 119, "endOffset": 208}, {"referenceID": 18, "context": "These problems generalize both purely robust instances of the problem (namely max-min submodular fair allocation (SFA) Golovin (2005) and min-max submodular load balancing (SLB) Svitkina and Fleischer (2008)) and also generalize average-case instances (that is the submodular welfare problem (SWP) Vondr\u00e1k (2008) and submodular multiway partition (SMP) Chekuri and Ene (2011a)).", "startOffset": 119, "endOffset": 313}, {"referenceID": 9, "context": "These problems generalize both purely robust instances of the problem (namely max-min submodular fair allocation (SFA) Golovin (2005) and min-max submodular load balancing (SLB) Svitkina and Fleischer (2008)) and also generalize average-case instances (that is the submodular welfare problem (SWP) Vondr\u00e1k (2008) and submodular multiway partition (SMP) Chekuri and Ene (2011a)).", "startOffset": 353, "endOffset": 377}, {"referenceID": 9, "context": "These problems generalize both purely robust instances of the problem (namely max-min submodular fair allocation (SFA) Golovin (2005) and min-max submodular load balancing (SLB) Svitkina and Fleischer (2008)) and also generalize average-case instances (that is the submodular welfare problem (SWP) Vondr\u00e1k (2008) and submodular multiway partition (SMP) Chekuri and Ene (2011a)). While the robust versions have been studied in the theory community Goemans et al. (2009); Golovin (2005); Khot and Ponnuswami (2007); Svitkina and Fleischer (2008); Vondr\u00e1k (2008), existing work has focused on tight approximation guarantees, and the resultant algorithms are not, in general, scalable to very large real-world applications.", "startOffset": 353, "endOffset": 469}, {"referenceID": 9, "context": "These problems generalize both purely robust instances of the problem (namely max-min submodular fair allocation (SFA) Golovin (2005) and min-max submodular load balancing (SLB) Svitkina and Fleischer (2008)) and also generalize average-case instances (that is the submodular welfare problem (SWP) Vondr\u00e1k (2008) and submodular multiway partition (SMP) Chekuri and Ene (2011a)). While the robust versions have been studied in the theory community Goemans et al. (2009); Golovin (2005); Khot and Ponnuswami (2007); Svitkina and Fleischer (2008); Vondr\u00e1k (2008), existing work has focused on tight approximation guarantees, and the resultant algorithms are not, in general, scalable to very large real-world applications.", "startOffset": 353, "endOffset": 485}, {"referenceID": 9, "context": "These problems generalize both purely robust instances of the problem (namely max-min submodular fair allocation (SFA) Golovin (2005) and min-max submodular load balancing (SLB) Svitkina and Fleischer (2008)) and also generalize average-case instances (that is the submodular welfare problem (SWP) Vondr\u00e1k (2008) and submodular multiway partition (SMP) Chekuri and Ene (2011a)). While the robust versions have been studied in the theory community Goemans et al. (2009); Golovin (2005); Khot and Ponnuswami (2007); Svitkina and Fleischer (2008); Vondr\u00e1k (2008), existing work has focused on tight approximation guarantees, and the resultant algorithms are not, in general, scalable to very large real-world applications.", "startOffset": 353, "endOffset": 513}, {"referenceID": 9, "context": "These problems generalize both purely robust instances of the problem (namely max-min submodular fair allocation (SFA) Golovin (2005) and min-max submodular load balancing (SLB) Svitkina and Fleischer (2008)) and also generalize average-case instances (that is the submodular welfare problem (SWP) Vondr\u00e1k (2008) and submodular multiway partition (SMP) Chekuri and Ene (2011a)). While the robust versions have been studied in the theory community Goemans et al. (2009); Golovin (2005); Khot and Ponnuswami (2007); Svitkina and Fleischer (2008); Vondr\u00e1k (2008), existing work has focused on tight approximation guarantees, and the resultant algorithms are not, in general, scalable to very large real-world applications.", "startOffset": 353, "endOffset": 544}, {"referenceID": 9, "context": "These problems generalize both purely robust instances of the problem (namely max-min submodular fair allocation (SFA) Golovin (2005) and min-max submodular load balancing (SLB) Svitkina and Fleischer (2008)) and also generalize average-case instances (that is the submodular welfare problem (SWP) Vondr\u00e1k (2008) and submodular multiway partition (SMP) Chekuri and Ene (2011a)). While the robust versions have been studied in the theory community Goemans et al. (2009); Golovin (2005); Khot and Ponnuswami (2007); Svitkina and Fleischer (2008); Vondr\u00e1k (2008), existing work has focused on tight approximation guarantees, and the resultant algorithms are not, in general, scalable to very large real-world applications.", "startOffset": 353, "endOffset": 560}, {"referenceID": 33, "context": "Most clustering problems are based on optimizing an objective that is either a sum, or an average-case, utility where the goal is to optimize the sum of individual cluster costs \u2014 this includes the ubiquitous k-means procedure Lloyd (1982); Arthur and Vassilvitskii (2007) where the goal is to construct a partitioning based on choosing a set of cluster centers that minimizes the total sum of squared distances between each point and its closest center.", "startOffset": 227, "endOffset": 240}, {"referenceID": 1, "context": "Most clustering problems are based on optimizing an objective that is either a sum, or an average-case, utility where the goal is to optimize the sum of individual cluster costs \u2014 this includes the ubiquitous k-means procedure Lloyd (1982); Arthur and Vassilvitskii (2007) where the goal is to construct a partitioning based on choosing a set of cluster centers that minimizes the total sum of squared distances between each point and its closest center.", "startOffset": 241, "endOffset": 273}, {"referenceID": 1, "context": "Most clustering problems are based on optimizing an objective that is either a sum, or an average-case, utility where the goal is to optimize the sum of individual cluster costs \u2014 this includes the ubiquitous k-means procedure Lloyd (1982); Arthur and Vassilvitskii (2007) where the goal is to construct a partitioning based on choosing a set of cluster centers that minimizes the total sum of squared distances between each point and its closest center. More rarely, clustering algorithms may be based on robust objective functions Gar\u0107\u0131a-Escudero et al. (2010), where the goal is to optimize the worst-case internal cluster cost (i.", "startOffset": 241, "endOffset": 563}, {"referenceID": 1, "context": "Most clustering problems are based on optimizing an objective that is either a sum, or an average-case, utility where the goal is to optimize the sum of individual cluster costs \u2014 this includes the ubiquitous k-means procedure Lloyd (1982); Arthur and Vassilvitskii (2007) where the goal is to construct a partitioning based on choosing a set of cluster centers that minimizes the total sum of squared distances between each point and its closest center. More rarely, clustering algorithms may be based on robust objective functions Gar\u0107\u0131a-Escudero et al. (2010), where the goal is to optimize the worst-case internal cluster cost (i.e., quality of the clustering is based solely on the quality of the worst internal cluster cost within the clustering). The average case vs. worst case cluster cost assessment distinction are extremes along a continuum, although all existing algorithms operate only at, rather than in between, these extremes. There is another way of categorizing clustering algorithms, and that based on the goal of each resultant cluster. Most of the time, clustering algorithms attempt to produce clusters containing items similar to or near each other (e.g., with k-means, a cluster consists of a centroid and a set of nearby points), and dissimilarity exists, ideally, only between different clusters. An alternate possible goal of clustering is to have each block contains as diverse a set of items as possible, where similarity exist between rather than within clusters. For example, partitioning the vertices of a graph into a set of k independent (or, equivalently, stable) sets would fall into this later category, assuming the graph has edges only between similar vertices. This subsumes graph k-colorability problems, one of the most well-known of the NP-complete problems, although in some special cases it is solvable in polynomial time Brandst\u00e4dt (1996); Feder et al.", "startOffset": 241, "endOffset": 1886}, {"referenceID": 1, "context": "Most clustering problems are based on optimizing an objective that is either a sum, or an average-case, utility where the goal is to optimize the sum of individual cluster costs \u2014 this includes the ubiquitous k-means procedure Lloyd (1982); Arthur and Vassilvitskii (2007) where the goal is to construct a partitioning based on choosing a set of cluster centers that minimizes the total sum of squared distances between each point and its closest center. More rarely, clustering algorithms may be based on robust objective functions Gar\u0107\u0131a-Escudero et al. (2010), where the goal is to optimize the worst-case internal cluster cost (i.e., quality of the clustering is based solely on the quality of the worst internal cluster cost within the clustering). The average case vs. worst case cluster cost assessment distinction are extremes along a continuum, although all existing algorithms operate only at, rather than in between, these extremes. There is another way of categorizing clustering algorithms, and that based on the goal of each resultant cluster. Most of the time, clustering algorithms attempt to produce clusters containing items similar to or near each other (e.g., with k-means, a cluster consists of a centroid and a set of nearby points), and dissimilarity exists, ideally, only between different clusters. An alternate possible goal of clustering is to have each block contains as diverse a set of items as possible, where similarity exist between rather than within clusters. For example, partitioning the vertices of a graph into a set of k independent (or, equivalently, stable) sets would fall into this later category, assuming the graph has edges only between similar vertices. This subsumes graph k-colorability problems, one of the most well-known of the NP-complete problems, although in some special cases it is solvable in polynomial time Brandst\u00e4dt (1996); Feder et al. (1999); Hell et al.", "startOffset": 241, "endOffset": 1907}, {"referenceID": 1, "context": "Most clustering problems are based on optimizing an objective that is either a sum, or an average-case, utility where the goal is to optimize the sum of individual cluster costs \u2014 this includes the ubiquitous k-means procedure Lloyd (1982); Arthur and Vassilvitskii (2007) where the goal is to construct a partitioning based on choosing a set of cluster centers that minimizes the total sum of squared distances between each point and its closest center. More rarely, clustering algorithms may be based on robust objective functions Gar\u0107\u0131a-Escudero et al. (2010), where the goal is to optimize the worst-case internal cluster cost (i.e., quality of the clustering is based solely on the quality of the worst internal cluster cost within the clustering). The average case vs. worst case cluster cost assessment distinction are extremes along a continuum, although all existing algorithms operate only at, rather than in between, these extremes. There is another way of categorizing clustering algorithms, and that based on the goal of each resultant cluster. Most of the time, clustering algorithms attempt to produce clusters containing items similar to or near each other (e.g., with k-means, a cluster consists of a centroid and a set of nearby points), and dissimilarity exists, ideally, only between different clusters. An alternate possible goal of clustering is to have each block contains as diverse a set of items as possible, where similarity exist between rather than within clusters. For example, partitioning the vertices of a graph into a set of k independent (or, equivalently, stable) sets would fall into this later category, assuming the graph has edges only between similar vertices. This subsumes graph k-colorability problems, one of the most well-known of the NP-complete problems, although in some special cases it is solvable in polynomial time Brandst\u00e4dt (1996); Feder et al. (1999); Hell et al. (2004). This general idea, where the clusters are as internally diverse as possible using some given measure of diversity, has been called anticlustering Valev (1983, 1998); Sp\u00e4th (1986) in the past and, as can be seen from the above, comprises in general some difficult computational problems.", "startOffset": 241, "endOffset": 1927}, {"referenceID": 1, "context": "Most clustering problems are based on optimizing an objective that is either a sum, or an average-case, utility where the goal is to optimize the sum of individual cluster costs \u2014 this includes the ubiquitous k-means procedure Lloyd (1982); Arthur and Vassilvitskii (2007) where the goal is to construct a partitioning based on choosing a set of cluster centers that minimizes the total sum of squared distances between each point and its closest center. More rarely, clustering algorithms may be based on robust objective functions Gar\u0107\u0131a-Escudero et al. (2010), where the goal is to optimize the worst-case internal cluster cost (i.e., quality of the clustering is based solely on the quality of the worst internal cluster cost within the clustering). The average case vs. worst case cluster cost assessment distinction are extremes along a continuum, although all existing algorithms operate only at, rather than in between, these extremes. There is another way of categorizing clustering algorithms, and that based on the goal of each resultant cluster. Most of the time, clustering algorithms attempt to produce clusters containing items similar to or near each other (e.g., with k-means, a cluster consists of a centroid and a set of nearby points), and dissimilarity exists, ideally, only between different clusters. An alternate possible goal of clustering is to have each block contains as diverse a set of items as possible, where similarity exist between rather than within clusters. For example, partitioning the vertices of a graph into a set of k independent (or, equivalently, stable) sets would fall into this later category, assuming the graph has edges only between similar vertices. This subsumes graph k-colorability problems, one of the most well-known of the NP-complete problems, although in some special cases it is solvable in polynomial time Brandst\u00e4dt (1996); Feder et al. (1999); Hell et al. (2004). This general idea, where the clusters are as internally diverse as possible using some given measure of diversity, has been called anticlustering Valev (1983, 1998); Sp\u00e4th (1986) in the past and, as can be seen from the above, comprises in general some difficult computational problems.", "startOffset": 241, "endOffset": 2107}, {"referenceID": 17, "context": ", fi(S) \u2264 fi(T ) whenever S \u2286 T ), normalized (fi(\u2205) = 0), and submodular Fujishige (2005) (i.", "startOffset": 74, "endOffset": 91}, {"referenceID": 52, "context": "Submodularity is a natural property in many real-world ML applications Wei et al. (2013); Zheng et al.", "startOffset": 71, "endOffset": 89}, {"referenceID": 52, "context": "Submodularity is a natural property in many real-world ML applications Wei et al. (2013); Zheng et al. (2014); Nagano et al.", "startOffset": 71, "endOffset": 110}, {"referenceID": 36, "context": "(2014); Nagano et al. (2010); Wei et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 36, "context": "(2014); Nagano et al. (2010); Wei et al. (2014b); Krause et al.", "startOffset": 8, "endOffset": 49}, {"referenceID": 32, "context": "(2014b); Krause et al. (2008a); Jegelka and Bilmes (2011); Das and Kempe (2011); Krause et al.", "startOffset": 9, "endOffset": 31}, {"referenceID": 30, "context": "(2008a); Jegelka and Bilmes (2011); Das and Kempe (2011); Krause et al.", "startOffset": 9, "endOffset": 35}, {"referenceID": 11, "context": "(2008a); Jegelka and Bilmes (2011); Das and Kempe (2011); Krause et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 11, "context": "(2008a); Jegelka and Bilmes (2011); Das and Kempe (2011); Krause et al. (2008b); Wei et al.", "startOffset": 36, "endOffset": 80}, {"referenceID": 11, "context": "(2008a); Jegelka and Bilmes (2011); Das and Kempe (2011); Krause et al. (2008b); Wei et al. (2015). When minimizing, submodularity naturally model notions of interacting costs and complexity, while when maximizing it readily models notions of diversity, summarization quality, and information.", "startOffset": 36, "endOffset": 99}, {"referenceID": 51, "context": "\u201d On the other hand, big data presents significant computational challenges to machine learning since, while big data is still getting bigger, it is expected that we are nearing the end of Moore\u2019s law Thompson and Parthasarathy (2006), and single threaded computing speed has unfortunately not significantly improved since about 2003.", "startOffset": 201, "endOffset": 235}, {"referenceID": 47, "context": "the \u201cnon-uniform\u201d case in the past Svitkina and Fleischer (2008); Goemans et al.", "startOffset": 35, "endOffset": 65}, {"referenceID": 19, "context": "the \u201cnon-uniform\u201d case in the past Svitkina and Fleischer (2008); Goemans et al. (2009), but we utilize the names homogeneous and heterogeneous to avoid implying that we desire the final set of submodular function valuations be uniform, which they do not need to be in our case.", "startOffset": 66, "endOffset": 88}, {"referenceID": 42, "context": "(2011) and distributed neural network training Povey et al. (2014), to name only a few.", "startOffset": 47, "endOffset": 67}, {"referenceID": 57, "context": "For example, Wei et al. (2015) show that the utility functions of data subsets for training certain machine learning classifiers can be derived as submodular functions.", "startOffset": 13, "endOffset": 31}, {"referenceID": 36, "context": ", locality maximization via bipartite graphs, where the goal is to organize the data so as to improve data locality, as in the work of Li et al. (2015); Alistarh et al.", "startOffset": 135, "endOffset": 152}, {"referenceID": 0, "context": "(2015); Alistarh et al. (2015) where a random partition is quite likely to perform very poorly.", "startOffset": 8, "endOffset": 31}, {"referenceID": 4, "context": "In many instances Boykov et al. (2001); Boykov and Kolmogorov (2004); Boykov and Veksler (2006), it was found that performing this maximization can be performed by a graph cut algorithm, in particular when the energy function takes the form E(y, x\u0304) = \u2211 v\u2208V \u03c6v(yv, x\u0304v) + \u2211 C\u2208C \u03c6C(yC) where C are the set of cliques of a graph.", "startOffset": 18, "endOffset": 39}, {"referenceID": 4, "context": "(2001); Boykov and Kolmogorov (2004); Boykov and Veksler (2006), it was found that performing this maximization can be performed by a graph cut algorithm, in particular when the energy function takes the form E(y, x\u0304) = \u2211 v\u2208V \u03c6v(yv, x\u0304v) + \u2211 C\u2208C \u03c6C(yC) where C are the set of cliques of a graph.", "startOffset": 8, "endOffset": 37}, {"referenceID": 4, "context": "(2001); Boykov and Kolmogorov (2004); Boykov and Veksler (2006), it was found that performing this maximization can be performed by a graph cut algorithm, in particular when the energy function takes the form E(y, x\u0304) = \u2211 v\u2208V \u03c6v(yv, x\u0304v) + \u2211 C\u2208C \u03c6C(yC) where C are the set of cliques of a graph.", "startOffset": 38, "endOffset": 64}, {"referenceID": 4, "context": "(2001); Boykov and Kolmogorov (2004); Boykov and Veksler (2006), it was found that performing this maximization can be performed by a graph cut algorithm, in particular when the energy function takes the form E(y, x\u0304) = \u2211 v\u2208V \u03c6v(yv, x\u0304v) + \u2211 C\u2208C \u03c6C(yC) where C are the set of cliques of a graph. When the order of the cliques is two, and when the \u03c6C functions are submodular, the MAP inference problem can be solved via a minimum graph cut algorithm, and if the order if \u03c6C is larger, then general submodular function minimization Fujishige (2005) (which runs in polynomial time in |V |) may be used.", "startOffset": 38, "endOffset": 548}, {"referenceID": 4, "context": "(2001); Boykov and Kolmogorov (2004); Boykov and Veksler (2006), it was found that performing this maximization can be performed by a graph cut algorithm, in particular when the energy function takes the form E(y, x\u0304) = \u2211 v\u2208V \u03c6v(yv, x\u0304v) + \u2211 C\u2208C \u03c6C(yC) where C are the set of cliques of a graph. When the order of the cliques is two, and when the \u03c6C functions are submodular, the MAP inference problem can be solved via a minimum graph cut algorithm, and if the order if \u03c6C is larger, then general submodular function minimization Fujishige (2005) (which runs in polynomial time in |V |) may be used. This corresponds, however, only to the binary image segmentation problem. In order to perform multi-label image segmentation, there have been various elaborate extensions of the energy functions to support this (e.g., Jegelka and Bilmes (2011); Shelhamer et al.", "startOffset": 38, "endOffset": 845}, {"referenceID": 4, "context": "(2001); Boykov and Kolmogorov (2004); Boykov and Veksler (2006), it was found that performing this maximization can be performed by a graph cut algorithm, in particular when the energy function takes the form E(y, x\u0304) = \u2211 v\u2208V \u03c6v(yv, x\u0304v) + \u2211 C\u2208C \u03c6C(yC) where C are the set of cliques of a graph. When the order of the cliques is two, and when the \u03c6C functions are submodular, the MAP inference problem can be solved via a minimum graph cut algorithm, and if the order if \u03c6C is larger, then general submodular function minimization Fujishige (2005) (which runs in polynomial time in |V |) may be used. This corresponds, however, only to the binary image segmentation problem. In order to perform multi-label image segmentation, there have been various elaborate extensions of the energy functions to support this (e.g., Jegelka and Bilmes (2011); Shelhamer et al. (2014); Heng et al.", "startOffset": 38, "endOffset": 870}, {"referenceID": 4, "context": "(2001); Boykov and Kolmogorov (2004); Boykov and Veksler (2006), it was found that performing this maximization can be performed by a graph cut algorithm, in particular when the energy function takes the form E(y, x\u0304) = \u2211 v\u2208V \u03c6v(yv, x\u0304v) + \u2211 C\u2208C \u03c6C(yC) where C are the set of cliques of a graph. When the order of the cliques is two, and when the \u03c6C functions are submodular, the MAP inference problem can be solved via a minimum graph cut algorithm, and if the order if \u03c6C is larger, then general submodular function minimization Fujishige (2005) (which runs in polynomial time in |V |) may be used. This corresponds, however, only to the binary image segmentation problem. In order to perform multi-label image segmentation, there have been various elaborate extensions of the energy functions to support this (e.g., Jegelka and Bilmes (2011); Shelhamer et al. (2014); Heng et al. (2015); Taniai et al.", "startOffset": 38, "endOffset": 890}, {"referenceID": 4, "context": "(2001); Boykov and Kolmogorov (2004); Boykov and Veksler (2006), it was found that performing this maximization can be performed by a graph cut algorithm, in particular when the energy function takes the form E(y, x\u0304) = \u2211 v\u2208V \u03c6v(yv, x\u0304v) + \u2211 C\u2208C \u03c6C(yC) where C are the set of cliques of a graph. When the order of the cliques is two, and when the \u03c6C functions are submodular, the MAP inference problem can be solved via a minimum graph cut algorithm, and if the order if \u03c6C is larger, then general submodular function minimization Fujishige (2005) (which runs in polynomial time in |V |) may be used. This corresponds, however, only to the binary image segmentation problem. In order to perform multi-label image segmentation, there have been various elaborate extensions of the energy functions to support this (e.g., Jegelka and Bilmes (2011); Shelhamer et al. (2014); Heng et al. (2015); Taniai et al. (2015); Kohli et al.", "startOffset": 38, "endOffset": 912}, {"referenceID": 4, "context": "(2001); Boykov and Kolmogorov (2004); Boykov and Veksler (2006), it was found that performing this maximization can be performed by a graph cut algorithm, in particular when the energy function takes the form E(y, x\u0304) = \u2211 v\u2208V \u03c6v(yv, x\u0304v) + \u2211 C\u2208C \u03c6C(yC) where C are the set of cliques of a graph. When the order of the cliques is two, and when the \u03c6C functions are submodular, the MAP inference problem can be solved via a minimum graph cut algorithm, and if the order if \u03c6C is larger, then general submodular function minimization Fujishige (2005) (which runs in polynomial time in |V |) may be used. This corresponds, however, only to the binary image segmentation problem. In order to perform multi-label image segmentation, there have been various elaborate extensions of the energy functions to support this (e.g., Jegelka and Bilmes (2011); Shelhamer et al. (2014); Heng et al. (2015); Taniai et al. (2015); Kohli et al. (2013); Silberman et al.", "startOffset": 38, "endOffset": 933}, {"referenceID": 4, "context": "(2001); Boykov and Kolmogorov (2004); Boykov and Veksler (2006), it was found that performing this maximization can be performed by a graph cut algorithm, in particular when the energy function takes the form E(y, x\u0304) = \u2211 v\u2208V \u03c6v(yv, x\u0304v) + \u2211 C\u2208C \u03c6C(yC) where C are the set of cliques of a graph. When the order of the cliques is two, and when the \u03c6C functions are submodular, the MAP inference problem can be solved via a minimum graph cut algorithm, and if the order if \u03c6C is larger, then general submodular function minimization Fujishige (2005) (which runs in polynomial time in |V |) may be used. This corresponds, however, only to the binary image segmentation problem. In order to perform multi-label image segmentation, there have been various elaborate extensions of the energy functions to support this (e.g., Jegelka and Bilmes (2011); Shelhamer et al. (2014); Heng et al. (2015); Taniai et al. (2015); Kohli et al. (2013); Silberman et al. (2014) to name just a few), but in all cases the energy function E(y, x\u0304) has to be extended to, at the very least, a non-pseudo Boolean function.", "startOffset": 38, "endOffset": 958}, {"referenceID": 2, "context": "When fi\u2019s are all modular, the tightest algorithm, so far, is to iteratively round an LP solution achieving O(1/( \u221a m logm)) approximation Asadpour and Saberi (2010), whereas the problem is NP-hard to 1/2 + approximate for any > 0 Golovin (2005).", "startOffset": 139, "endOffset": 166}, {"referenceID": 2, "context": "When fi\u2019s are all modular, the tightest algorithm, so far, is to iteratively round an LP solution achieving O(1/( \u221a m logm)) approximation Asadpour and Saberi (2010), whereas the problem is NP-hard to 1/2 + approximate for any > 0 Golovin (2005). When fi\u2019s are submodular, Golovin (2005) gives a matching-based algorithm with a factor 1/(n\u2212m+ 1) approximation that performs poorly when m n.", "startOffset": 139, "endOffset": 246}, {"referenceID": 2, "context": "When fi\u2019s are all modular, the tightest algorithm, so far, is to iteratively round an LP solution achieving O(1/( \u221a m logm)) approximation Asadpour and Saberi (2010), whereas the problem is NP-hard to 1/2 + approximate for any > 0 Golovin (2005). When fi\u2019s are submodular, Golovin (2005) gives a matching-based algorithm with a factor 1/(n\u2212m+ 1) approximation that performs poorly when m n.", "startOffset": 139, "endOffset": 288}, {"referenceID": 2, "context": "When fi\u2019s are all modular, the tightest algorithm, so far, is to iteratively round an LP solution achieving O(1/( \u221a m logm)) approximation Asadpour and Saberi (2010), whereas the problem is NP-hard to 1/2 + approximate for any > 0 Golovin (2005). When fi\u2019s are submodular, Golovin (2005) gives a matching-based algorithm with a factor 1/(n\u2212m+ 1) approximation that performs poorly when m n. Khot and Ponnuswami (2007) propose a binary search algorithm yielding an improved factor of 1/(2m \u2212 1).", "startOffset": 139, "endOffset": 418}, {"referenceID": 2, "context": "When fi\u2019s are all modular, the tightest algorithm, so far, is to iteratively round an LP solution achieving O(1/( \u221a m logm)) approximation Asadpour and Saberi (2010), whereas the problem is NP-hard to 1/2 + approximate for any > 0 Golovin (2005). When fi\u2019s are submodular, Golovin (2005) gives a matching-based algorithm with a factor 1/(n\u2212m+ 1) approximation that performs poorly when m n. Khot and Ponnuswami (2007) propose a binary search algorithm yielding an improved factor of 1/(2m \u2212 1). Another approach approximates each submodular function by its ellipsoid approximation (non-scalable) and reduces SFA to its modular version leading to an approximation factor of O( \u221a nm1/4 log n logm). These approaches are theoretically interesting, but they either do not fully exploit the problem structure or cannot scale to large problems. On the other hand, Problem 1 for \u03bb = 1 is called submodular welfare. This problem has been extensively studied in the literature and can be equivalently formulated as submodular maximization under a partition matroid constraint Vondr\u00e1k (2008). It", "startOffset": 139, "endOffset": 1082}, {"referenceID": 29, "context": "Problem 1 (Max-(Min+Avg)) Approximation factor \u03bb = 0, BinSrch Khot and Ponnuswami (2007) 1/(2m\u2212 1) \u03bb = 0, Matching Golovin (2005) 1/(n\u2212m+ 1) \u03bb = 0, Ellipsoid Goemans et al.", "startOffset": 62, "endOffset": 89}, {"referenceID": 19, "context": "Problem 1 (Max-(Min+Avg)) Approximation factor \u03bb = 0, BinSrch Khot and Ponnuswami (2007) 1/(2m\u2212 1) \u03bb = 0, Matching Golovin (2005) 1/(n\u2212m+ 1) \u03bb = 0, Ellipsoid Goemans et al.", "startOffset": 115, "endOffset": 130}, {"referenceID": 18, "context": "Problem 1 (Max-(Min+Avg)) Approximation factor \u03bb = 0, BinSrch Khot and Ponnuswami (2007) 1/(2m\u2212 1) \u03bb = 0, Matching Golovin (2005) 1/(n\u2212m+ 1) \u03bb = 0, Ellipsoid Goemans et al. (2009) O( \u221a nm1/4 log n logm) \u03bb = 1, GreedWelfare Fisher et al.", "startOffset": 158, "endOffset": 180}, {"referenceID": 16, "context": "(2009) O( \u221a nm1/4 log n logm) \u03bb = 1, GreedWelfare Fisher et al. (1978) 1/2 \u03bb = 0, GreedSat\u2217 (1/2\u2212 \u03b4, \u03b4 1/2+\u03b4 ) \u03bb = 0, MMax\u2217 O(min i 1+(|Ai |\u22121)(1\u2212\u03bafi (A \u03c0\u0302 i )) |Ai | \u221a m logm ) \u03bb = 0, GreedMax\u2020\u2217 1/m 0 < \u03bb < 1, GeneralGreedSat\u2217 \u03bb/2 0 < \u03bb < 1, CombSfaSwp\u2217 max{ \u03b2\u03b1 \u03bb\u0304\u03b2+\u03b1 , \u03bb\u03b2} 0 < \u03bb < 1, CombSfaSwp\u2020\u2217 max{min{\u03b1, 1 m}, \u03b2\u03b1 \u03bb\u0304\u03b2+\u03b1 , \u03bb\u03b2} \u03bb = 0, Hardness 1/2 Golovin (2005) \u03bb = 1, Hardness 1\u2212 1/e Vondr\u00e1k (2008) Table 1: Summary of our contributions and existing work on Problem 1.", "startOffset": 50, "endOffset": 71}, {"referenceID": 16, "context": "(2009) O( \u221a nm1/4 log n logm) \u03bb = 1, GreedWelfare Fisher et al. (1978) 1/2 \u03bb = 0, GreedSat\u2217 (1/2\u2212 \u03b4, \u03b4 1/2+\u03b4 ) \u03bb = 0, MMax\u2217 O(min i 1+(|Ai |\u22121)(1\u2212\u03bafi (A \u03c0\u0302 i )) |Ai | \u221a m logm ) \u03bb = 0, GreedMax\u2020\u2217 1/m 0 < \u03bb < 1, GeneralGreedSat\u2217 \u03bb/2 0 < \u03bb < 1, CombSfaSwp\u2217 max{ \u03b2\u03b1 \u03bb\u0304\u03b2+\u03b1 , \u03bb\u03b2} 0 < \u03bb < 1, CombSfaSwp\u2020\u2217 max{min{\u03b1, 1 m}, \u03b2\u03b1 \u03bb\u0304\u03b2+\u03b1 , \u03bb\u03b2} \u03bb = 0, Hardness 1/2 Golovin (2005) \u03bb = 1, Hardness 1\u2212 1/e Vondr\u00e1k (2008) Table 1: Summary of our contributions and existing work on Problem 1.", "startOffset": 50, "endOffset": 366}, {"referenceID": 16, "context": "(2009) O( \u221a nm1/4 log n logm) \u03bb = 1, GreedWelfare Fisher et al. (1978) 1/2 \u03bb = 0, GreedSat\u2217 (1/2\u2212 \u03b4, \u03b4 1/2+\u03b4 ) \u03bb = 0, MMax\u2217 O(min i 1+(|Ai |\u22121)(1\u2212\u03bafi (A \u03c0\u0302 i )) |Ai | \u221a m logm ) \u03bb = 0, GreedMax\u2020\u2217 1/m 0 < \u03bb < 1, GeneralGreedSat\u2217 \u03bb/2 0 < \u03bb < 1, CombSfaSwp\u2217 max{ \u03b2\u03b1 \u03bb\u0304\u03b2+\u03b1 , \u03bb\u03b2} 0 < \u03bb < 1, CombSfaSwp\u2020\u2217 max{min{\u03b1, 1 m}, \u03b2\u03b1 \u03bb\u0304\u03b2+\u03b1 , \u03bb\u03b2} \u03bb = 0, Hardness 1/2 Golovin (2005) \u03bb = 1, Hardness 1\u2212 1/e Vondr\u00e1k (2008) Table 1: Summary of our contributions and existing work on Problem 1.", "startOffset": 50, "endOffset": 404}, {"referenceID": 20, "context": "It is worth noting that the homogeneous instance of the submodular welfare withm = 2 generalizes the well-known max-cut problem (Goemans and Williamson, 1995), where the submodular objective is defined as the submodular neighborhood function (Iyer and Bilmes, 2013; Schrijver, 2003).", "startOffset": 128, "endOffset": 158}, {"referenceID": 27, "context": "It is worth noting that the homogeneous instance of the submodular welfare withm = 2 generalizes the well-known max-cut problem (Goemans and Williamson, 1995), where the submodular objective is defined as the submodular neighborhood function (Iyer and Bilmes, 2013; Schrijver, 2003).", "startOffset": 242, "endOffset": 282}, {"referenceID": 43, "context": "It is worth noting that the homogeneous instance of the submodular welfare withm = 2 generalizes the well-known max-cut problem (Goemans and Williamson, 1995), where the submodular objective is defined as the submodular neighborhood function (Iyer and Bilmes, 2013; Schrijver, 2003).", "startOffset": 242, "endOffset": 282}, {"referenceID": 16, "context": "admits a scalable greedy algorithm that achieves a 1/2 approximation Fisher et al. (1978). More recently a multi-linear extension based algorithm nicely solves the submodular welfare problem with a factor of (1\u22121/e) matching the hardness of this problem Vondr\u00e1k (2008).", "startOffset": 69, "endOffset": 90}, {"referenceID": 16, "context": "admits a scalable greedy algorithm that achieves a 1/2 approximation Fisher et al. (1978). More recently a multi-linear extension based algorithm nicely solves the submodular welfare problem with a factor of (1\u22121/e) matching the hardness of this problem Vondr\u00e1k (2008). It is worth noting that the homogeneous instance of the submodular welfare withm = 2 generalizes the well-known max-cut problem (Goemans and Williamson, 1995), where the submodular objective is defined as the submodular neighborhood function (Iyer and Bilmes, 2013; Schrijver, 2003).", "startOffset": 69, "endOffset": 269}, {"referenceID": 24, "context": "In the homogeneous setting, Hochbaum and Shmoys (1988) give a PTAS scheme ((1+ )-approximation algorithm which runs in polynomial time for any fixed ), while an LP relaxation algorithm provides a 2-approximation for the heterogeneous setting Lenstra et al.", "startOffset": 28, "endOffset": 55}, {"referenceID": 24, "context": "In the homogeneous setting, Hochbaum and Shmoys (1988) give a PTAS scheme ((1+ )-approximation algorithm which runs in polynomial time for any fixed ), while an LP relaxation algorithm provides a 2-approximation for the heterogeneous setting Lenstra et al. (1990). When the objectives are submodular, the problem becomes much harder.", "startOffset": 28, "endOffset": 264}, {"referenceID": 24, "context": "In the homogeneous setting, Hochbaum and Shmoys (1988) give a PTAS scheme ((1+ )-approximation algorithm which runs in polynomial time for any fixed ), while an LP relaxation algorithm provides a 2-approximation for the heterogeneous setting Lenstra et al. (1990). When the objectives are submodular, the problem becomes much harder. Even in the homogeneous setting, Svitkina and Fleischer (2008) show that the problem is information theoretically hard to approximate within o( \u221a n/ log n).", "startOffset": 28, "endOffset": 397}, {"referenceID": 43, "context": "Problem 2 (Min-(Max+Avg)) Approximation factor \u03bb = 0, Balanced\u2020 Svitkina and Fleischer (2008) min{m,n/m} \u03bb = 0, Sampling Svitkina and Fleischer (2008) O( \u221a n log n) \u03bb = 0, Ellipsoid Goemans et al.", "startOffset": 64, "endOffset": 94}, {"referenceID": 43, "context": "Problem 2 (Min-(Max+Avg)) Approximation factor \u03bb = 0, Balanced\u2020 Svitkina and Fleischer (2008) min{m,n/m} \u03bb = 0, Sampling Svitkina and Fleischer (2008) O( \u221a n log n) \u03bb = 0, Ellipsoid Goemans et al.", "startOffset": 64, "endOffset": 151}, {"referenceID": 16, "context": "Problem 2 (Min-(Max+Avg)) Approximation factor \u03bb = 0, Balanced\u2020 Svitkina and Fleischer (2008) min{m,n/m} \u03bb = 0, Sampling Svitkina and Fleischer (2008) O( \u221a n log n) \u03bb = 0, Ellipsoid Goemans et al. (2009) O( \u221a n log n) \u03bb = 1, GreedSplit\u2020 Zhao et al.", "startOffset": 182, "endOffset": 204}, {"referenceID": 16, "context": "Problem 2 (Min-(Max+Avg)) Approximation factor \u03bb = 0, Balanced\u2020 Svitkina and Fleischer (2008) min{m,n/m} \u03bb = 0, Sampling Svitkina and Fleischer (2008) O( \u221a n log n) \u03bb = 0, Ellipsoid Goemans et al. (2009) O( \u221a n log n) \u03bb = 1, GreedSplit\u2020 Zhao et al. (2004); Narasimhan et al.", "startOffset": 182, "endOffset": 256}, {"referenceID": 16, "context": "Problem 2 (Min-(Max+Avg)) Approximation factor \u03bb = 0, Balanced\u2020 Svitkina and Fleischer (2008) min{m,n/m} \u03bb = 0, Sampling Svitkina and Fleischer (2008) O( \u221a n log n) \u03bb = 0, Ellipsoid Goemans et al. (2009) O( \u221a n log n) \u03bb = 1, GreedSplit\u2020 Zhao et al. (2004); Narasimhan et al. (2005) 2 \u03bb = 1, Relax Chekuri and Ene (2011a) O(log n) \u03bb = 0, MMin\u2217 max i 2|A\u03c0 i | 1+(|A\u03c0 i |\u22121)(1\u2212\u03bafi (A \u03c0\u2217 i )) \u03bb = 0, Lov\u00e1sz Round\u2217 m 0 < \u03bb < 1 GeneralLov\u00e1sz Round\u2217 m 0 < \u03bb < 1, CombSlbSmp\u2217 min{ m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} 0 < \u03bb < 1, CombSlbSmp\u2020\u2217 min{m, m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} \u03bb = 0, Hardness\u2217 m \u03bb = 1, Hardness 2\u2212 2/m Ene et al.", "startOffset": 182, "endOffset": 282}, {"referenceID": 9, "context": "(2005) 2 \u03bb = 1, Relax Chekuri and Ene (2011a) O(log n) \u03bb = 0, MMin\u2217 max i 2|A\u03c0 i | 1+(|A\u03c0 i |\u22121)(1\u2212\u03bafi (A \u03c0\u2217 i )) \u03bb = 0, Lov\u00e1sz Round\u2217 m 0 < \u03bb < 1 GeneralLov\u00e1sz Round\u2217 m 0 < \u03bb < 1, CombSlbSmp\u2217 min{ m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} 0 < \u03bb < 1, CombSlbSmp\u2020\u2217 min{m, m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} \u03bb = 0, Hardness\u2217 m \u03bb = 1, Hardness 2\u2212 2/m Ene et al.", "startOffset": 22, "endOffset": 46}, {"referenceID": 9, "context": "(2005) 2 \u03bb = 1, Relax Chekuri and Ene (2011a) O(log n) \u03bb = 0, MMin\u2217 max i 2|A\u03c0 i | 1+(|A\u03c0 i |\u22121)(1\u2212\u03bafi (A \u03c0\u2217 i )) \u03bb = 0, Lov\u00e1sz Round\u2217 m 0 < \u03bb < 1 GeneralLov\u00e1sz Round\u2217 m 0 < \u03bb < 1, CombSlbSmp\u2217 min{ m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} 0 < \u03bb < 1, CombSlbSmp\u2020\u2217 min{m, m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} \u03bb = 0, Hardness\u2217 m \u03bb = 1, Hardness 2\u2212 2/m Ene et al. (2013) Table 2: Summary of our contributions and existing work on Problem 23.", "startOffset": 22, "endOffset": 333}, {"referenceID": 9, "context": "(2005) 2 \u03bb = 1, Relax Chekuri and Ene (2011a) O(log n) \u03bb = 0, MMin\u2217 max i 2|A\u03c0 i | 1+(|A\u03c0 i |\u22121)(1\u2212\u03bafi (A \u03c0\u2217 i )) \u03bb = 0, Lov\u00e1sz Round\u2217 m 0 < \u03bb < 1 GeneralLov\u00e1sz Round\u2217 m 0 < \u03bb < 1, CombSlbSmp\u2217 min{ m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} 0 < \u03bb < 1, CombSlbSmp\u2020\u2217 min{m, m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} \u03bb = 0, Hardness\u2217 m \u03bb = 1, Hardness 2\u2212 2/m Ene et al. (2013) Table 2: Summary of our contributions and existing work on Problem 23. computation Orlin (2009), where \u03b3 is the cost of a function valuation.", "startOffset": 22, "endOffset": 429}, {"referenceID": 9, "context": "(2005) 2 \u03bb = 1, Relax Chekuri and Ene (2011a) O(log n) \u03bb = 0, MMin\u2217 max i 2|A\u03c0 i | 1+(|A\u03c0 i |\u22121)(1\u2212\u03bafi (A \u03c0\u2217 i )) \u03bb = 0, Lov\u00e1sz Round\u2217 m 0 < \u03bb < 1 GeneralLov\u00e1sz Round\u2217 m 0 < \u03bb < 1, CombSlbSmp\u2217 min{ m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} 0 < \u03bb < 1, CombSlbSmp\u2020\u2217 min{m, m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} \u03bb = 0, Hardness\u2217 m \u03bb = 1, Hardness 2\u2212 2/m Ene et al. (2013) Table 2: Summary of our contributions and existing work on Problem 23. computation Orlin (2009), where \u03b3 is the cost of a function valuation. Similar to Submodular Fair Allocation, Goemans et al. (2009) applies the same ellipsoid approximation techniques leading to a factor of O( \u221a n log n) Goemans et al.", "startOffset": 22, "endOffset": 536}, {"referenceID": 9, "context": "(2005) 2 \u03bb = 1, Relax Chekuri and Ene (2011a) O(log n) \u03bb = 0, MMin\u2217 max i 2|A\u03c0 i | 1+(|A\u03c0 i |\u22121)(1\u2212\u03bafi (A \u03c0\u2217 i )) \u03bb = 0, Lov\u00e1sz Round\u2217 m 0 < \u03bb < 1 GeneralLov\u00e1sz Round\u2217 m 0 < \u03bb < 1, CombSlbSmp\u2217 min{ m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} 0 < \u03bb < 1, CombSlbSmp\u2020\u2217 min{m, m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} \u03bb = 0, Hardness\u2217 m \u03bb = 1, Hardness 2\u2212 2/m Ene et al. (2013) Table 2: Summary of our contributions and existing work on Problem 23. computation Orlin (2009), where \u03b3 is the cost of a function valuation. Similar to Submodular Fair Allocation, Goemans et al. (2009) applies the same ellipsoid approximation techniques leading to a factor of O( \u221a n log n) Goemans et al. (2009). When \u03bb = 1, Problem 2 becomes the submodular multiway partition (SMP) for which one can obtain a relaxation based 2approximation Chekuri and Ene (2011a) in the homogeneous case.", "startOffset": 22, "endOffset": 647}, {"referenceID": 9, "context": "(2005) 2 \u03bb = 1, Relax Chekuri and Ene (2011a) O(log n) \u03bb = 0, MMin\u2217 max i 2|A\u03c0 i | 1+(|A\u03c0 i |\u22121)(1\u2212\u03bafi (A \u03c0\u2217 i )) \u03bb = 0, Lov\u00e1sz Round\u2217 m 0 < \u03bb < 1 GeneralLov\u00e1sz Round\u2217 m 0 < \u03bb < 1, CombSlbSmp\u2217 min{ m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} 0 < \u03bb < 1, CombSlbSmp\u2020\u2217 min{m, m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} \u03bb = 0, Hardness\u2217 m \u03bb = 1, Hardness 2\u2212 2/m Ene et al. (2013) Table 2: Summary of our contributions and existing work on Problem 23. computation Orlin (2009), where \u03b3 is the cost of a function valuation. Similar to Submodular Fair Allocation, Goemans et al. (2009) applies the same ellipsoid approximation techniques leading to a factor of O( \u221a n log n) Goemans et al. (2009). When \u03bb = 1, Problem 2 becomes the submodular multiway partition (SMP) for which one can obtain a relaxation based 2approximation Chekuri and Ene (2011a) in the homogeneous case.", "startOffset": 22, "endOffset": 801}, {"referenceID": 9, "context": "(2005) 2 \u03bb = 1, Relax Chekuri and Ene (2011a) O(log n) \u03bb = 0, MMin\u2217 max i 2|A\u03c0 i | 1+(|A\u03c0 i |\u22121)(1\u2212\u03bafi (A \u03c0\u2217 i )) \u03bb = 0, Lov\u00e1sz Round\u2217 m 0 < \u03bb < 1 GeneralLov\u00e1sz Round\u2217 m 0 < \u03bb < 1, CombSlbSmp\u2217 min{ m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} 0 < \u03bb < 1, CombSlbSmp\u2020\u2217 min{m, m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} \u03bb = 0, Hardness\u2217 m \u03bb = 1, Hardness 2\u2212 2/m Ene et al. (2013) Table 2: Summary of our contributions and existing work on Problem 23. computation Orlin (2009), where \u03b3 is the cost of a function valuation. Similar to Submodular Fair Allocation, Goemans et al. (2009) applies the same ellipsoid approximation techniques leading to a factor of O( \u221a n log n) Goemans et al. (2009). When \u03bb = 1, Problem 2 becomes the submodular multiway partition (SMP) for which one can obtain a relaxation based 2approximation Chekuri and Ene (2011a) in the homogeneous case. In the non-homogeneous case, the guarantee is O(log n) Chekuri and Ene (2011b). Similarly, Zhao et al.", "startOffset": 22, "endOffset": 905}, {"referenceID": 9, "context": "(2005) 2 \u03bb = 1, Relax Chekuri and Ene (2011a) O(log n) \u03bb = 0, MMin\u2217 max i 2|A\u03c0 i | 1+(|A\u03c0 i |\u22121)(1\u2212\u03bafi (A \u03c0\u2217 i )) \u03bb = 0, Lov\u00e1sz Round\u2217 m 0 < \u03bb < 1 GeneralLov\u00e1sz Round\u2217 m 0 < \u03bb < 1, CombSlbSmp\u2217 min{ m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} 0 < \u03bb < 1, CombSlbSmp\u2020\u2217 min{m, m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} \u03bb = 0, Hardness\u2217 m \u03bb = 1, Hardness 2\u2212 2/m Ene et al. (2013) Table 2: Summary of our contributions and existing work on Problem 23. computation Orlin (2009), where \u03b3 is the cost of a function valuation. Similar to Submodular Fair Allocation, Goemans et al. (2009) applies the same ellipsoid approximation techniques leading to a factor of O( \u221a n log n) Goemans et al. (2009). When \u03bb = 1, Problem 2 becomes the submodular multiway partition (SMP) for which one can obtain a relaxation based 2approximation Chekuri and Ene (2011a) in the homogeneous case. In the non-homogeneous case, the guarantee is O(log n) Chekuri and Ene (2011b). Similarly, Zhao et al. (2004); Narasimhan et al.", "startOffset": 22, "endOffset": 936}, {"referenceID": 9, "context": "(2005) 2 \u03bb = 1, Relax Chekuri and Ene (2011a) O(log n) \u03bb = 0, MMin\u2217 max i 2|A\u03c0 i | 1+(|A\u03c0 i |\u22121)(1\u2212\u03bafi (A \u03c0\u2217 i )) \u03bb = 0, Lov\u00e1sz Round\u2217 m 0 < \u03bb < 1 GeneralLov\u00e1sz Round\u2217 m 0 < \u03bb < 1, CombSlbSmp\u2217 min{ m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} 0 < \u03bb < 1, CombSlbSmp\u2020\u2217 min{m, m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} \u03bb = 0, Hardness\u2217 m \u03bb = 1, Hardness 2\u2212 2/m Ene et al. (2013) Table 2: Summary of our contributions and existing work on Problem 23. computation Orlin (2009), where \u03b3 is the cost of a function valuation. Similar to Submodular Fair Allocation, Goemans et al. (2009) applies the same ellipsoid approximation techniques leading to a factor of O( \u221a n log n) Goemans et al. (2009). When \u03bb = 1, Problem 2 becomes the submodular multiway partition (SMP) for which one can obtain a relaxation based 2approximation Chekuri and Ene (2011a) in the homogeneous case. In the non-homogeneous case, the guarantee is O(log n) Chekuri and Ene (2011b). Similarly, Zhao et al. (2004); Narasimhan et al. (2005) propose a greedy splitting 2-approximation algorithm for the homogeneous setting.", "startOffset": 22, "endOffset": 962}, {"referenceID": 8, "context": "For SFA, when m = 2, we formulate the problem as non-monotone submodular maximization, which can be approximated up to a factor of 1/2 with O(n) function evaluations Buchbinder et al. (2012). For general m, we give a simple and scalable greedy algorithm (GreedMax), and show a factor of 1/m in the homogeneous setting, improving the state-ofthe-art factor of 1/(2m\u2212 1) under the heterogeneous setting Khot and Ponnuswami (2007).", "startOffset": 166, "endOffset": 191}, {"referenceID": 8, "context": "For SFA, when m = 2, we formulate the problem as non-monotone submodular maximization, which can be approximated up to a factor of 1/2 with O(n) function evaluations Buchbinder et al. (2012). For general m, we give a simple and scalable greedy algorithm (GreedMax), and show a factor of 1/m in the homogeneous setting, improving the state-ofthe-art factor of 1/(2m\u2212 1) under the heterogeneous setting Khot and Ponnuswami (2007). For the heterogeneous setting, we propose a \u201csaturate\u201d greedy algorithm (GreedSat) that iteratively solves instances of submodular welfare problems.", "startOffset": 166, "endOffset": 428}, {"referenceID": 48, "context": "the hardness result in Svitkina and Fleischer (2008) and show that it is hard to approximate better than m for any m = o( \u221a n/ log n) even in the homogeneous setting.", "startOffset": 23, "endOffset": 53}, {"referenceID": 8, "context": "This problem has been well studied in the literature Buchbinder et al. (2012); Dobzinski and Mor (2015); Feige et al.", "startOffset": 53, "endOffset": 78}, {"referenceID": 8, "context": "This problem has been well studied in the literature Buchbinder et al. (2012); Dobzinski and Mor (2015); Feige et al.", "startOffset": 53, "endOffset": 104}, {"referenceID": 8, "context": "This problem has been well studied in the literature Buchbinder et al. (2012); Dobzinski and Mor (2015); Feige et al. (2011). A simple bi-directional randomized greedy algorithm Buchbinder et al.", "startOffset": 53, "endOffset": 125}, {"referenceID": 8, "context": "This problem has been well studied in the literature Buchbinder et al. (2012); Dobzinski and Mor (2015); Feige et al. (2011). A simple bi-directional randomized greedy algorithm Buchbinder et al. (2012) solves Eqn 4 with a tight factor of 1/2.", "startOffset": 53, "endOffset": 203}, {"referenceID": 32, "context": "By assuming the homogeneity of the fi\u2019s, we obtain a very simple 1/m-approximation algorithm improving upon the state-of-the-art factor 1/(2m \u2212 1) Khot and Ponnuswami (2007). Thanks to the lazy evaluation trick as described in Minoux (1978), Line 5 in Alg.", "startOffset": 147, "endOffset": 174}, {"referenceID": 32, "context": "By assuming the homogeneity of the fi\u2019s, we obtain a very simple 1/m-approximation algorithm improving upon the state-of-the-art factor 1/(2m \u2212 1) Khot and Ponnuswami (2007). Thanks to the lazy evaluation trick as described in Minoux (1978), Line 5 in Alg.", "startOffset": 147, "endOffset": 241}, {"referenceID": 26, "context": "Similar in flavor to the one proposed in Krause et al. (2008b) GreedSat defines an intermediate objective F\u0304 c(\u03c0) = \u2211m i=1 f c i (A \u03c0 i ), where f c i (A) = 1 m min{fi(A), c} (Line 2).", "startOffset": 41, "endOffset": 63}, {"referenceID": 16, "context": "In this work, we solve Line 6 using the greedy algorithm as described in Alg 3, which attains a constant 1/2-approximation Fisher et al. (1978). Moreover the lazy evaluation trick also applies for Alg 3 enabling the wrapper algorithm GreedSat scalable to large data sets.", "startOffset": 123, "endOffset": 144}, {"referenceID": 16, "context": "In this work, we solve Line 6 using the greedy algorithm as described in Alg 3, which attains a constant 1/2-approximation Fisher et al. (1978). Moreover the lazy evaluation trick also applies for Alg 3 enabling the wrapper algorithm GreedSat scalable to large data sets. One can also use a more computationally expensive multi-linear relaxation algorithm as given in Vondr\u00e1k (2008) to solve Line 6 with a tight factor \u03b1 = (1\u22121/e).", "startOffset": 123, "endOffset": 383}, {"referenceID": 16, "context": "In this work, we solve Line 6 using the greedy algorithm as described in Alg 3, which attains a constant 1/2-approximation Fisher et al. (1978). Moreover the lazy evaluation trick also applies for Alg 3 enabling the wrapper algorithm GreedSat scalable to large data sets. One can also use a more computationally expensive multi-linear relaxation algorithm as given in Vondr\u00e1k (2008) to solve Line 6 with a tight factor \u03b1 = (1\u22121/e). Setting the input argument \u03b1 as the approximation factor for Line 6, the essential idea of GreedSat is to perform a binary search over the parameter c to find the largest c\u2217 such that the returned solution \u03c0\u0302c \u2217 for the instance of SWP satisfies F\u0304 c \u2217 (\u03c0\u0302c \u2217 ) \u2265 \u03b1c\u2217. GreedSat terminates after solving O(log(i fi(V ) )) instances of SWP. Theorem 3 gives a bi-criterion optimality guarantee. Theorem 3 Given > 0, 0 \u2264 \u03b1 \u2264 1 and any 0 < \u03b4 < \u03b1, GreedSat finds a partition such that at least dm(\u03b1\u2212 \u03b4)e blocks receive utility at least \u03b4 1\u2212\u03b1+\u03b4 (max\u03c0\u2208\u03a0 mini fi(Ai )\u2212 ). For any 0 < \u03b4 < \u03b1 Theorem 3 ensures that the top dm(\u03b1\u2212 \u03b4)e valued blocks in the partition returned by GreedSat are (\u03b4/(1\u2212 \u03b1+ \u03b4)\u2212 )-optimal. \u03b4 controls the trade-off between the number of top valued blocks to bound and the performance guarantee attained for these blocks. The smaller \u03b4 is, the more top blocks are bounded, but with a weaker guarantee. We set the input argument \u03b1 = 1/2 (or \u03b1 = 1\u2212 1/e) as the worst-case performance guarantee for solving SWP so that the above theoretical analysis follows. However, the worst-case is often achieved only by very contrived submodular functions. For the ones used in practice, the greedy algorithm often leads to near-optimal solution (Krause et al. (2008b) and our own observations).", "startOffset": 123, "endOffset": 1699}, {"referenceID": 16, "context": "In this work, we solve Line 6 using the greedy algorithm as described in Alg 3, which attains a constant 1/2-approximation Fisher et al. (1978). Moreover the lazy evaluation trick also applies for Alg 3 enabling the wrapper algorithm GreedSat scalable to large data sets. One can also use a more computationally expensive multi-linear relaxation algorithm as given in Vondr\u00e1k (2008) to solve Line 6 with a tight factor \u03b1 = (1\u22121/e). Setting the input argument \u03b1 as the approximation factor for Line 6, the essential idea of GreedSat is to perform a binary search over the parameter c to find the largest c\u2217 such that the returned solution \u03c0\u0302c \u2217 for the instance of SWP satisfies F\u0304 c \u2217 (\u03c0\u0302c \u2217 ) \u2265 \u03b1c\u2217. GreedSat terminates after solving O(log(i fi(V ) )) instances of SWP. Theorem 3 gives a bi-criterion optimality guarantee. Theorem 3 Given > 0, 0 \u2264 \u03b1 \u2264 1 and any 0 < \u03b4 < \u03b1, GreedSat finds a partition such that at least dm(\u03b1\u2212 \u03b4)e blocks receive utility at least \u03b4 1\u2212\u03b1+\u03b4 (max\u03c0\u2208\u03a0 mini fi(Ai )\u2212 ). For any 0 < \u03b4 < \u03b1 Theorem 3 ensures that the top dm(\u03b1\u2212 \u03b4)e valued blocks in the partition returned by GreedSat are (\u03b4/(1\u2212 \u03b1+ \u03b4)\u2212 )-optimal. \u03b4 controls the trade-off between the number of top valued blocks to bound and the performance guarantee attained for these blocks. The smaller \u03b4 is, the more top blocks are bounded, but with a weaker guarantee. We set the input argument \u03b1 = 1/2 (or \u03b1 = 1\u2212 1/e) as the worst-case performance guarantee for solving SWP so that the above theoretical analysis follows. However, the worst-case is often achieved only by very contrived submodular functions. For the ones used in practice, the greedy algorithm often leads to near-optimal solution (Krause et al. (2008b) and our own observations). Setting \u03b1 as the actual performance guarantee for SWP (often very close to 1) can improve the empirical bound, and we, in practice, typically set \u03b1 = 1 to good effect. MMax: In parallel to GreedSat, we also introduce a semi-gradient based approach for solving SFA under the heterogeneous setting. We call this algorithm minorizationmaximization (MMax, see Alg. 4). Similar to the ones proposed in Iyer et al. (2013a); Iyer and Bilmes (2013, 2012b), the idea is to iteratively maximize tight lower bounds of the submodular functions.", "startOffset": 123, "endOffset": 2143}, {"referenceID": 16, "context": "In this work, we solve Line 6 using the greedy algorithm as described in Alg 3, which attains a constant 1/2-approximation Fisher et al. (1978). Moreover the lazy evaluation trick also applies for Alg 3 enabling the wrapper algorithm GreedSat scalable to large data sets. One can also use a more computationally expensive multi-linear relaxation algorithm as given in Vondr\u00e1k (2008) to solve Line 6 with a tight factor \u03b1 = (1\u22121/e). Setting the input argument \u03b1 as the approximation factor for Line 6, the essential idea of GreedSat is to perform a binary search over the parameter c to find the largest c\u2217 such that the returned solution \u03c0\u0302c \u2217 for the instance of SWP satisfies F\u0304 c \u2217 (\u03c0\u0302c \u2217 ) \u2265 \u03b1c\u2217. GreedSat terminates after solving O(log(i fi(V ) )) instances of SWP. Theorem 3 gives a bi-criterion optimality guarantee. Theorem 3 Given > 0, 0 \u2264 \u03b1 \u2264 1 and any 0 < \u03b4 < \u03b1, GreedSat finds a partition such that at least dm(\u03b1\u2212 \u03b4)e blocks receive utility at least \u03b4 1\u2212\u03b1+\u03b4 (max\u03c0\u2208\u03a0 mini fi(Ai )\u2212 ). For any 0 < \u03b4 < \u03b1 Theorem 3 ensures that the top dm(\u03b1\u2212 \u03b4)e valued blocks in the partition returned by GreedSat are (\u03b4/(1\u2212 \u03b1+ \u03b4)\u2212 )-optimal. \u03b4 controls the trade-off between the number of top valued blocks to bound and the performance guarantee attained for these blocks. The smaller \u03b4 is, the more top blocks are bounded, but with a weaker guarantee. We set the input argument \u03b1 = 1/2 (or \u03b1 = 1\u2212 1/e) as the worst-case performance guarantee for solving SWP so that the above theoretical analysis follows. However, the worst-case is often achieved only by very contrived submodular functions. For the ones used in practice, the greedy algorithm often leads to near-optimal solution (Krause et al. (2008b) and our own observations). Setting \u03b1 as the actual performance guarantee for SWP (often very close to 1) can improve the empirical bound, and we, in practice, typically set \u03b1 = 1 to good effect. MMax: In parallel to GreedSat, we also introduce a semi-gradient based approach for solving SFA under the heterogeneous setting. We call this algorithm minorizationmaximization (MMax, see Alg. 4). Similar to the ones proposed in Iyer et al. (2013a); Iyer and Bilmes (2013, 2012b), the idea is to iteratively maximize tight lower bounds of the submodular functions. Submodular functions have tight modular lower bounds, which are related to the subdifferential \u2202f (Y ) of the submodular set function f at a set Y \u2286 V , which is defined Fujishige (2005) as:", "startOffset": 123, "endOffset": 2446}, {"referenceID": 2, "context": "In other words, at iteration t+ 1, for each block i, we approximate fi with its modular lower bound tight at A\u03c0 t i and solve a modular version of Problem 1 (Line 7), which admits efficient approximation algorithms Asadpour and Saberi (2010). MMax is initialized with a partition \u03c00, which is obtained by solving Problem 1, where each fi is replaced with a simple modular function f \u2032 i(A) = \u2211 a\u2208A fi(a).", "startOffset": 215, "endOffset": 242}, {"referenceID": 48, "context": "Existing hardness for SLB is shown to be o( \u221a n/ log n) Svitkina and Fleischer (2008). However it is independent of m, and Svitkina and Fleischer (2008) assumes m = \u0398( \u221a n/ log n) in their analysis.", "startOffset": 56, "endOffset": 86}, {"referenceID": 48, "context": "Existing hardness for SLB is shown to be o( \u221a n/ log n) Svitkina and Fleischer (2008). However it is independent of m, and Svitkina and Fleischer (2008) assumes m = \u0398( \u221a n/ log n) in their analysis.", "startOffset": 56, "endOffset": 153}, {"referenceID": 48, "context": "Though the proof technique for Theorem 6 mostly carries over from Svitkina and Fleischer (2008), the result strictly generalizes the analysis in Svitkina and Fleischer (2008).", "startOffset": 66, "endOffset": 96}, {"referenceID": 48, "context": "Though the proof technique for Theorem 6 mostly carries over from Svitkina and Fleischer (2008), the result strictly generalizes the analysis in Svitkina and Fleischer (2008). For any choice of m = o( \u221a n/ log n) Theorem 6 implies that it is information theoretically hard to approximate SLB better than m even for the homogeneous setting.", "startOffset": 66, "endOffset": 175}, {"referenceID": 28, "context": "The algorithm proceeds as follows: (1) apply the Lov\u00e1sz extension of submodular functions to relax SLB to a convex program, which is exactly solved to a fractional solution (Line 2); (2) map the fractional solution to a partition using the \u03b8-rounding technique as proposed in Iyer et al. (2014) (Line 3 - 6).", "startOffset": 276, "endOffset": 295}, {"referenceID": 31, "context": "Here, we iteratively choose modular upper bounds, which are defined via superdifferentials \u2202f (Y ) of a submodular function Jegelka and Bilmes (2011) at Y : \u2202 (Y ) = {y \u2208 R : f(X)\u2212 y(X) \u2264 f(Y )\u2212 y(Y ); for all X \u2286 V }.", "startOffset": 124, "endOffset": 150}, {"referenceID": 25, "context": "Moreover, there are specific supergradients Iyer and Bilmes (2012a); Iyer et al.", "startOffset": 44, "endOffset": 68}, {"referenceID": 25, "context": "Moreover, there are specific supergradients Iyer and Bilmes (2012a); Iyer et al. (2013a) that define the following two modular upper bounds (when referring to either one, we use mfX): mfX,1(Y ) , f(X)\u2212 \u2211", "startOffset": 44, "endOffset": 89}, {"referenceID": 36, "context": "At iteration t+ 1, for each block i, MMin replaces fi with a choice of its modular upper bound mi tight at A \u03c0t i and solves a modular version of Problem 2 (Line 7), for which there exists an efficient LP relaxation based algorithm Lenstra et al. (1990). Similar to MMax, the initial partition \u03c00 is obtained by solving Problem 2, where each fi is substituted with f \u2032 i(A) = \u2211 a\u2208A fi(a).", "startOffset": 232, "endOffset": 254}, {"referenceID": 19, "context": "Since some of the algorithms, such as the Ellipsoidal Approximations Goemans et al. (2009) and Lov\u00e1sz relaxation algorithms, are computationally intensive, we restrict ourselves to only 40 data instances, i.", "startOffset": 69, "endOffset": 91}, {"referenceID": 19, "context": ", SFA, we compare among 6 algorithms: GreedMax, GreedSat, MMax, Balanced Partition (BP), Ellipsoid Approximation (EA) Goemans et al. (2009), and Binary Search algorithm (BS) Khot and Ponnuswami (2007).", "startOffset": 118, "endOffset": 140}, {"referenceID": 19, "context": ", SFA, we compare among 6 algorithms: GreedMax, GreedSat, MMax, Balanced Partition (BP), Ellipsoid Approximation (EA) Goemans et al. (2009), and Binary Search algorithm (BS) Khot and Ponnuswami (2007). Balanced Partition method simply partitions the ground set V into m blocks such that the size of each block is balanced and is either d |V | m e or b |V | m c.", "startOffset": 118, "endOffset": 201}, {"referenceID": 19, "context": ", SLB, we compare among 5 algorithms: Lov\u00e1sz Round, MMin, GeneralGreedMin, Ellipsoid Approximation (EA) Goemans et al. (2009), and Balanced Partition Svitkina and Fleischer (2011).", "startOffset": 104, "endOffset": 126}, {"referenceID": 19, "context": ", SLB, we compare among 5 algorithms: Lov\u00e1sz Round, MMin, GeneralGreedMin, Ellipsoid Approximation (EA) Goemans et al. (2009), and Balanced Partition Svitkina and Fleischer (2011). We implement GeneralGreedMin with the input argument \u03bb = 0.", "startOffset": 104, "endOffset": 180}, {"referenceID": 52, "context": "(2014b, 2015); Tschiatschek et al. (2014), which has the form:", "startOffset": 15, "endOffset": 42}, {"referenceID": 47, "context": "ffea is in the form of a sum of concave over modular functions, hence is monotone submodular Stobbe and Krause (2010). The class of feature-based submodular function has been widely applied to model the utility of a data subset on a number of tasks, including speech data subset selection Wei et al.", "startOffset": 93, "endOffset": 118}, {"referenceID": 47, "context": "ffea is in the form of a sum of concave over modular functions, hence is monotone submodular Stobbe and Krause (2010). The class of feature-based submodular function has been widely applied to model the utility of a data subset on a number of tasks, including speech data subset selection Wei et al. (2014b,c), and image summarization Tschiatschek et al. (2014). Moreover ffea has been shown in Wei et al.", "startOffset": 93, "endOffset": 362}, {"referenceID": 47, "context": "ffea is in the form of a sum of concave over modular functions, hence is monotone submodular Stobbe and Krause (2010). The class of feature-based submodular function has been widely applied to model the utility of a data subset on a number of tasks, including speech data subset selection Wei et al. (2014b,c), and image summarization Tschiatschek et al. (2014). Moreover ffea has been shown in Wei et al. (2015) to model the log-likelihood of a data subset for a N\u00e4\u0131ve Bayes classifier.", "startOffset": 93, "endOffset": 413}, {"referenceID": 42, "context": "Note that this distributed training scheme is similar to the ones presented in Povey et al. (2014). The submodular partitioning for both tasks is obtained by solving the homogeneous case of Problem 1 (\u03bb = 0) using GreedMax on a form of clustered facility location, as proposed and used in Wei et al.", "startOffset": 79, "endOffset": 99}, {"referenceID": 42, "context": "Note that this distributed training scheme is similar to the ones presented in Povey et al. (2014). The submodular partitioning for both tasks is obtained by solving the homogeneous case of Problem 1 (\u03bb = 0) using GreedMax on a form of clustered facility location, as proposed and used in Wei et al. (2015). The function is defined as follows:", "startOffset": 79, "endOffset": 307}, {"referenceID": 57, "context": "Wei et al. (2015) show that fc-fac models the log-likelihood of a data subset for a Nearest Neighbor classifier.", "startOffset": 0, "endOffset": 18}, {"referenceID": 54, "context": "sparse Wei et al. (2014a). In the experiment, we instantiate ffac by a sparse 10-nearest neighbor sparse graph, where each item v is connected only to its 10 closest neighbors.", "startOffset": 7, "endOffset": 26}, {"referenceID": 4, "context": "A number of unsupervised methods are tested as baselines in the experiment, including k-means, k-medoids, graph cuts Boykov and Kolmogorov (2004) and spectral clustering Von Luxburg (2007).", "startOffset": 117, "endOffset": 146}, {"referenceID": 4, "context": "A number of unsupervised methods are tested as baselines in the experiment, including k-means, k-medoids, graph cuts Boykov and Kolmogorov (2004) and spectral clustering Von Luxburg (2007). We use the RBF kernel sparse similarity matrix as the input for spectral clustering.", "startOffset": 117, "endOffset": 189}, {"referenceID": 3, "context": "For graph cuts, we use the MATLAB implementation Bagon (2006), which has a smoothness parameter \u03b1.", "startOffset": 49, "endOffset": 62}, {"referenceID": 2, "context": "Proof We assume the approximation factor of the algorithm for solving the modular version of Problem 1 is \u03b1 = O( 1 \u221a m logm ) Asadpour and Saberi (2010). For notation simplicity, we write \u03c0\u0302 = (\u00c21, .", "startOffset": 126, "endOffset": 153}, {"referenceID": 2, "context": "Proof We assume the approximation factor of the algorithm for solving the modular version of Problem 1 is \u03b1 = O( 1 \u221a m logm ) Asadpour and Saberi (2010). For notation simplicity, we write \u03c0\u0302 = (\u00c21, . . . , \u00c2m) as the resulting partition after the first iteration of MMax, and \u03c0\u2217 = (A1, . . . , A \u2217 m) as its optimal solution. Note that first iteration suffices to yield the performance guarantee, and the subsequent iterations are designed so as to improve the empirical performance. Since the proxy function for each function fi used for the first iteration are the simple modular upper bound with the form: hi(X) = \u2211 j\u2208X fi(j). Given the curvature of each submodular function fi, we can tightly bound a submodular function fi in the following form Iyer et al. (2013b):", "startOffset": 126, "endOffset": 770}, {"referenceID": 48, "context": "Proof We use the same proof techniques as in Svitkina and Fleischer (2008). Consider two submodular functions:", "startOffset": 45, "endOffset": 75}, {"referenceID": 48, "context": "As shown in Svitkina and Fleischer (2008), P{f1(S) > f2(S)} is maximized when |S| = \u03b1.", "startOffset": 12, "endOffset": 42}, {"referenceID": 36, "context": "Proof Let \u03b1 = 2 be the approximation factor of the algorithm for solving the modular version of Problem 2 Lenstra et al. (1990). For notation simplicity, we write \u03c0\u0302 = (\u00c21, .", "startOffset": 106, "endOffset": 128}], "year": 2016, "abstractText": "We study two mixed robust/average-case submodular partitioning problems that we collectively call Submodular Partitioning. These problems generalize both purely robust instances of the problem (namely max-min submodular fair allocation (SFA) Golovin (2005) and min-max submodular load balancing (SLB) Svitkina and Fleischer (2008)) and also generalize average-case instances (that is the submodular welfare problem (SWP) Vondr\u00e1k (2008) and submodular multiway partition (SMP) Chekuri and Ene (2011a)). While the robust versions have been studied in the theory community Goemans et al. (2009); Golovin (2005); Khot and Ponnuswami (2007); Svitkina and Fleischer (2008); Vondr\u00e1k (2008), existing work has focused on tight approximation guarantees, and the resultant algorithms are not, in general, scalable to very large real-world applications. This is in contrast to the average case, where most of the algorithms are scalable. In the present paper, we bridge this gap, by proposing several new algorithms (including those based on greedy, majorization-minimization, minorization-maximization, and relaxation algorithms) that not only scale to large sizes but that also achieve theoretical approximation guarantees close to the state-of-the-art, and in some cases achieve new tight bounds. We also provide new scalable algorithms that apply to additive combinations of the robust and average-case extreme objectives. We show that these problems have many applications in machine learning (ML). This includes: 1) data 1 ar X iv :1 51 0. 08 86 5v 2 [ cs .D S] 1 6 A ug 2 01 6 Wei, Iyer, Wang, Bai, Bilmes partitioning and load balancing for distributed machine algorithms on parallel machines; 2) data clustering; and 3) multi-label image segmentation with (only) Boolean submodular functions via pixel partitioning. We empirically demonstrate the efficacy of our algorithms on real-world problems involving data partitioning for distributed optimization of standard machine learning objectives (including both convex and deep neural network objectives), and also on purely unsupervised (i.e., no supervised or semi-supervised learning, and no interactive segmentation) image segmentation.", "creator": "LaTeX with hyperref package"}}}