{"id": "1510.09079", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2015", "title": "SentiWords: Deriving a High Precision and High Coverage Lexicon for Sentiment Analysis", "abstract": "Deriving prior polarity lexica for sentiment analysis - where positive or negative scores are associated with words out of context - is a challenging task. Usually, a trade-off between precision and coverage is hard to find, and it depends on the methodology used to build the lexicon. Manually annotated lexica provide a high precision but lack in coverage, whereas automatic derivation from pre-existing knowledge guarantees high coverage at the cost of a lower precision. This is particularly true in terms of the ability of lexical analysis to generate and analyze all three sentences and can also be done at a cost of less than $20. Therefore, there are a number of options to achieve the same feat. For example, you can define the term 'words out of context' with different conditions, such as a verb of interest, or simply a verb of interest. In theory, however, lexical analysis is a bit like following a word out of context, or in the words out of context.\n\nLemetic analysis and the distribution of the word\nThis is a lot of work to complete, but I would encourage you to make the most of it. In the case of lexical analysis, the term 'words out of context' is usually used as an alias for words that are not related in any way to other words or to other words. For example, if you have a language that contains a number of nouns in the same order, it is important to use it to indicate that your words belong to another language. If the following sentence is followed by 'words out of context' in the same order, you must do this using both 'words out of context' and 'words out of context' in the same order. For example, if you have a language that contains a number of nouns in the same order, it is important to use it to indicate that your words belong to another language. If the following sentence is followed by 'words out of context' in the same order, you must do this using both 'words out of context' and 'words out of context' in the same order. For example, if you have a language that contains a number of nouns in the same order, it is important to use it to indicate that your words belong to another language. If the following sentence is followed by 'words out of context' in the same order, you must do this using both 'words out of context' and 'words out of context' in the same order. For example, if you have", "histories": [["v1", "Fri, 30 Oct 2015 13:19:47 GMT  (276kb)", "http://arxiv.org/abs/1510.09079v1", "in Affective Computing, IEEE Transactions on (2015)"]], "COMMENTS": "in Affective Computing, IEEE Transactions on (2015)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lorenzo gatti", "marco guerini", "marco turchi"], "accepted": false, "id": "1510.09079"}, "pdf": {"name": "1510.09079.pdf", "metadata": {"source": "CRF", "title": "SentiWords: Deriving a High Precision and High Coverage Lexicon for Sentiment Analysis", "authors": ["Lorenzo Gatti", "Marco Guerini", "Marco Turchi"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n51 0.\n09 07\n9v 1\n[ cs\n.C L\n] 3\n0 O\nct 2\n01 5\nIndex Terms\u2014Natural Language Processing, Text analysis, Machine learning\n\u2726\n\u00a92015 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. DOI: 10.1109/TAFFC.2015.2476456"}, {"heading": "1 INTRODUCTION", "text": "IN sentiment analysis many approaches employ spe-cialized lexica \u2013 i.e. lists of positive and negative words \u2013 often in conjunction with other methods (usually machine learning based) [1], to assign sentiment scores to texts. In most of these lexica, words are associated with their prior polarity, i.e. if that word out of context evokes something positive or something negative. For example, wonderful has a positive connotation \u2013 prior polarity \u2013 while horrible has a negative one. These approaches, based on prior polarity lexica, are so popular because they do not need word sense disambiguation to assign an affective score to a word, and they are often largely domainindependent. Prior polarity lexica can be roughly divided into two groups: those that are manually built (either hiring expert annotators such as linguists or by crowdsourcing the annotation on web platforms such as Mechanical Turk), and those that are automatically derived from pre-existing knowledge. While the first kind of lexica has a high precision but a low coverage, the opposite holds for the second kind. In this paper, we aim to understand if blending\nboth approaches we can build a lexicon that has\n\u2022 L. Gatti, M. Guerini and M. Turchi are with FBK, Trento, Italy. E-mail: {l.gatti, guerini, turchi}@fbk.eu.\nboth a high coverage and a high precision. We focus on SentiWordNet (henceforth SWN), a resource that has been widely adopted since it provides a broadcoverage lexicon \u2013 built in a semi-automatic manner \u2013 for English [2]. Given that SWN provides polarity scores for each word sense (also called \u2018posterior polarities\u2019), it is necessary to derive prior polarities from the posteriors.\nSeveral formulae to compute prior polarities starting from posterior polarity scores have been proposed in the literature. By comparing the formulae against manually built prior polarity lexica we show that some of these formulae are better than others at estimating prior polarities and can represent a fairer state-of-the-art approach using SWN. On top of this, we attempt to outperform the state-of-theart formula using an \u2018ensemble\u2019 learning framework that combines the various formulae together and takes advantage of manually built prior polarity lexica to better predict the value of unseen words. In this way we construct a sentiment lexicon that has both a high coverage and a high precision.\nIn detail, the first part of the paper \u2013 that is based on our previous work, presented in [3] \u2013 addresses three main research questions about words prior polarity computation: (i) is there any relevant difference in the posterior-to-prior polarity formulae performance (both in regression and classification tasks)? (ii) Is there any relevant variation in prior polarity values if we use different releases of SWN (i.e. SWN1 or\nThis is the author\u2019s version of an article that has been published in IEEE Transactions on Affective Computing. Changes were made to this version by the publisher prior to publication. The final version is available at http://dx.doi.org/10.1109/TAFFC.2015.2476456\nSWN3)? (iii) Can a learning framework boost the performance of such formulae? In the second part of the paper \u2013 that represents the novel contribution of the present work \u2013 we introduce SentiWords1, a prior polarity lexicon produced according to the lesson learned from the first part of the paper, and we answer an additional set of questions regarding sentiment analysis of sentences using words prior polarities: (i) does SentiWords still have better performance compared to the posteriorto-prior polarity formulae? (ii) How important is the coverage of the lexicon compared to other handmade lexica (more precise but smaller)? (iii) How well does SentiWords perform across datasets compared to a specialized posterior-polarities lexicon? In the following two sections, we present a series of experiments, both in regression and classification tasks, that give an answer to the aforementioned research questions. The results support the hypothesis that using a learning framework can improve on the state-of-the-art performance in posterior-to-prior computation and that using SentiWords in sentiment analysis provides better results than other available lexica."}, {"heading": "2 RELATED WORK", "text": "The quest for a high precision and high coverage lexicon, where words are associated with either sentiment or emotion scores, has several reasons. First, it is fundamental for tasks such as affective modification of existing texts, where words polarity together with their scores are necessary for creating multiple versions of a text, varying its affective dimension [4], [5], [6]. Second, while in sentiment analysis compositionality (i.e. methods to compute the score of a sentence by combining the scores of the words in its syntactic tree) plays a crucial role, list of words associated with their sentiment score are still a fundamental prerequisite for this task. Works using compositional approaches worth mentioning are: [7], that uses recursive neural networks to learn compositional rules for sentiment analysis, while [8], [9], [10] exploit hand-coded rules. In this respect, compositional approaches represent a promising new trend, since all other approaches, either using semantic similarity or Bag-of-Words (BOW) based machine-learning, cannot handle, for example, cases of texts with the same wording but different word order: \u201cThe dangerous killer escaped one month ago, but lately he was arrested\u201d (positive) vs. \u201cThe dangerous killer was arrested one month ago, but lately he escaped\u201d (negative). The work in [11] partially accounts for this problem arguing that using word bigrams allows improvement over BOW based methods, where words are taken as features in isolation. This way it is\n1. The resource can be downloaded at https://hlt.fbk.eu/technologies/sentiwords.\npossible to capture simple compositional phenomena such as polarity reversing in \u201ckilling cancer\u201d. Finally, tasks such as copywriting, where evocative names are a key element to a successful product [12], [13] require exhaustive lists of emotion related words. In such cases no context is given and the brand name alone, with its perceived prior polarity, is responsible for stating the area of competition and evoking semantic associations. Evoking emotions is also fundamental for a successful name: consider names of a perfume such asObsession, or technological products such as MacBook Air. We now provide a review focusing on research efforts put towards building sentiment and emotion lexica, regardless of the approach in which such lists are then used (machine learning, rule based or deep learning). A general overview can be found in [14], [1], [15], [16].\nSentiment Lexica. In recent years there has been an increasing focus on producing lists of words with affective polarities, to be used in sentiment analysis. When building such lists, a trade-off between coverage and precision of the resource has to be found. The highest precision is obtained with manually annotated lexica, but these are usually smaller due to the time and costs associated with the annotation task. Automatically created resources are usually larger, but their precision is highly dependent on the annotation algorithm [17] and, in general, not as accurate as manual resources. One of the most well-known resources is SentiWordNet (SWN) [2], [18], in which each entry is a set of lemma#PoS#sense-number sharing the same meaning, called synset. Starting from SWN, several prior polarities for words in the form lemma#PoS, can be computed (e.g. considering only the first-sense or averaging on all the senses). These approaches, detailed in [3], produce a list of approximately 155,000 words, where the lower precision given by the automatic scoring of SWN is compensated by the high coverage. SWN and formulae for prior computation will be thoroughly described in Section 3. Another widely used resource is the Affective Norms for English Words (ANEW) [19], providing valence scores for roughly 1,000 words, which were manually assigned by several annotators. This resource has a low coverage, but the precision is very high. Similarly, the SO-CAL entries [20] consist of roughly 4,000 words manually tagged by a small number of linguists with a multi-class label (from very_negative to very_positive). These ratings were further validated through crowdsourcing. The Dictionary of Affect in Language (DAL) contains roughly 9,000 words manually rated along the dimensions \u2018pleasantness\u2019, \u2018activation\u2019 and \u2018imagery\u2019 [21]. More recently, a resource replicating the ANEW annotation approach using crowdsourcing was released by Warriner and colleagues [22], providing sentiment scores for approxi-\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nThis is the author\u2019s version of an article that has been published in IEEE Transactions on Affective Computing. Changes were made to this version by the publisher prior to publication. The final version is available at http://dx.doi.org/10.1109/TAFFC.2015.2476456\nmately 14,000 words (this lexicon will be referred to as Warr henceforth). Interestingly, this resource includes the most frequently used English words, so \u2013 even if its coverage is still far lower than SWN \u2013 it grants a high coverage, with human precision, of language use. Finally, the General Inquirer lexicon [23] provides a binary classification (positive/negative) of approximately 4,000 sentiment-bearing words manually annotated, while the resource presented in [24] expands the General Inquirer to 6,000 words.\nEmotion Lexica. One of the most used resources is WordNetAffect [25] which contains manually assigned affective labels to WordNet synsets (ANGER, JOY, FEAR, etc.). It currently provides 900 annotated synsets and 1,600 words in the form lemma#PoS#sense, corresponding to roughly 1,000 lemma-PoS. AffectNet, part of the SenticNet project [26], contains approximately 10,000 words (out of 23,000 entries) taken from ConceptNet and aligned with WordNetAffect. This resource extends WordNetAffect labels to concepts such as \u2018have breakfast\u2019. Fuzzy Affect Lexicon [27] contains roughly 4,000 lemma-PoS manually annotated by one linguist using 80 emotion labels. EmoLex [28] contains almost 10,000 lemmas annotated with an intensity label for each emotion using Mechanical Turk. Finally Affect database is an extension of SentiFul [29] and contains 2,500 words in the form lemma#PoS, while DepecheMood [30] contains about 37,000 words also in the lemma#PoS format, and was automatically built by harvesting crowd-sourced affective annotation from a social news network. These latter two lexica are the only ones providing words annotated with emotion scores, rather than just with labels."}, {"heading": "3 PROPOSED APPROACH", "text": "In the broad field of Sentiment Analysis we will first focus on the specific problem of words posterior-toprior polarity assessment, using SWN both in regression and classification experiments. For the regression task, we tackle the problem of assigning affective scores (along a continuum between -1 and 1) to words, using posterior-to-prior polarity formulae. For the classification task (assessing whether a word is either positive or negative) we use the same formulae, but considering just the sign of the result. In these experiments we also use an ensemble method which combines the various formulae together. The underlying hypothesis is that by blending these formulae, and looking at the same information from different perspectives (i.e. the posterior polarities provided by SWN combined in various ways), we can obtain a better prediction. In the second part of the paper we will validate the improvement we can obtain in a simple sentiment analysis task with the lexicon produced by our ensemble method over the single SWN metrics and\nover other widely used handmade lexica. To this end, we run an extensive series of experiments on two different datasets of sentences that represents different forms of language use, i.e. news headlines, with simplified syntax and lexicon, and sentences extracted from movie reviews, with normal language use. Also in this case, we face both regression and classification tasks."}, {"heading": "3.1 SentiWordNet", "text": "SentiWordNet [2] is a lexical resource composed of \u201csynsets\u201d, i.e. sets of lemma#PoS#sense-number tuples (where the smallest sense-number corresponds to the most frequent sense of the lemma) sharing the same meaning. Each synset s is associated with the numerical scores Pos(s) and Neg(s), which range from 0 to 1. These scores represent the positive and negative valence (or posterior polarity) of the synset, and are shared by each entry in the synset. The scores were automatically assigned by a classifier committee trained on the glosses of three subsets of WordNet: one composed of positive synsets, one of negative synsets and one containing \u201cneutral\u201d synsets, i.e. synsets that are neither positive nor negative. The positive and negative subsets were constructed by (i) finding the synsets containing 14 \u201cparadigmatic\u201d positive and negative words (e.g. good#a#1), and (ii) automatically expanded by traversing the WordNet hierarchy to find \u201crelated\u201d synsets, using the method described in [25]. Neutral synsets are those that do not belong to the other two subsets and that do not contain terms marked as Positive or Negative in the General Inquirer lexicon. Obviously, different senses of a lemma#PoS can have different polarities. In Table 1, the first 5 senses of cold#a present all possible combinations, including mixed scores (cold#a#4), where positive and negative valences are assigned to the same sense. Intuitively, mixed scores for the same sense are acceptable, as in \u201ccold beer\u201d (positive) vs. \u201ccold pizza\u201d (negative).\nIn our experiments we use two different versions of SWN: SentiWordNet 1.0 (SWN1), the first release of SWN, and its updated version SentiWordNet 3.0 [18] (SWN3). The latter differs from the former because (i) it annotates WordNet 3.0 instead of WordNet 2.0; (ii) it \u201ccorrects\u201d the classifiers scores with a randomwalk process, where the glosses are used to adjust the\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nThis is the author\u2019s version of an article that has been published in IEEE Transactions on Affective Computing. Changes were made to this version by the publisher prior to publication. The final version is available at http://dx.doi.org/10.1109/TAFFC.2015.2476456\nnegativity and positivity scores of the synsets (iii) it uses different, manually annotated glosses, both for training the classifiers and for the previous step. This new annotation algorithm led to an increase in the accuracy of posterior polarities over the first version, as reported by the authors."}, {"heading": "3.2 Prior Polarities Formulae", "text": "In this section, we review the strategies for computing prior polarities from SWN used in previous studies. All the proposed approaches try to estimate the prior polarity from the posterior polarities of all the senses for a single lemma-PoS. Given a lemmaPoS with n senses (lemma#PoS#n), every formula f is independently applied to posScore and negScore (which are the ordered sets of all the Pos(s) and all the Neg(s) for that lemma-PoS, respectively). This produces two scores in the range [0, 1], f(posScore) and f(negScore), for each lemma-PoS. To obtain a unique prior polarity, f(posScore) and f(negScore) can be mapped according to different strategies:\nfm =\n{\nf(posScore) if f(posScore) \u2265 f(negScore)\n\u2212f(negScore) otherwise\nfd = f(posScore)\u2212 f(negScore)\nwhere fm computes the absolute maximum of the two scores, while fd computes the difference between them. Both numbers are in the range [\u22121, 1]. So, considering the first 5 senses of cold#a in Table 1, f(posScore) will be derived from the Pos(s) values <0.0, 0.0, 0.0, 0.125, 0.625>, while f(negScore) from <0.750, 0.750, 0.0, 0.375, 0.0>. Then, the final polarity strength will be either fm or fd. The formulae (f ) we tested are the following:\nfs. In this formula only the first (and thus most frequent) sense is considered for the given lemma#PoS. This is equivalent to considering just the SWN score for lemma#PoS#1. Based on [31], [32], [5], [33], this is the most basic form of prior polarities.\nmean. It calculates the mean of the positive and negative scores for all the senses of the given lemma#PoS. It was used in [34], [35], [36], [37].\nuni. Based on [31], it considers only senses having a Pos(s) greater than or equal to the corresponding Neg(s), and greater than 0 (the stronglyPos set). In the case where posScore is equal to negScore (thus also f(posScore) = f(negScore)), the one with the highest weight is returned, where weights are defined as the cardinality of stronglyPos divided by the total number of senses. The same applies for the negative senses. This is the only method, together with rnd, for which we cannot apply fd, as it returns a positive or negative score according to the weight.\nuniw. Like uni but without the weighting system. w1. This formula weights each sense with a geometric series of ratio 1/2. The rationale behind this choice is based on the assumption that more frequent senses should bear more \u201caffective weight\u201d than rare senses when computing the prior polarity of a word.\nThe system presented in [38] uses a similar approach of weighted mean.\nw2. Similar to the w1, this formula weigths each lemma with a harmonic series, see for example [39] (where w2 appears with the fd variant).\nOn top of these formulae, we implemented some new formulae that were relevant to our task and, to our knowledge, have not been proposed in the literature. These formulae mimic those discussed previously, but they are built under a different assumption: that the saliency of a word prior polarity might be more related to its posterior scores, rather than to sense frequencies. Thus we ordered posScore and negScore by strength, giving more relevance to \u201cstrongly valenced\u201d senses. For instance, in Table 1, posScore and negScore for cold#a become <0.625, 0.125, 0.0, 0.0, 0.0> and <0.750, 0.750, 0.375, 0.0, 0.0> respectively.\nw1s and w2s. These are similar to w1 and w2, but senses are ordered by strength (sorting Pos(s) and Neg(s) independently).\nw1n and w2n. The same as w1 and w2 respectively, but without considering senses that have a 0 score for both Pos(s) and Neg(s). Our motivation is that null senses constitute noise for the purposes of lexicon bootstrapping.\nw1sn and w2sn. The same as w1s and w2s, but without considering senses that have a 0 score for both Pos(s) and Neg(s) respectively.\nmedian. Returns the median of the senses ordered by polarity score.\nmax. Returns max(posScore) and max(negScore), i.e. it returns the highest positive and negative values among all senses.\nAll these prior polarities formulae are compared to two gold standards sentiment lexica (one for regression, one for classification) both separately, as in the works mentioned above, and combined together in a learning framework (to see whether combining these features \u2013 that capture different aspect of prior polarities \u2013 can further improve the results).\nFinally, we implemented two variants of a prior polarity random baseline to assess possible advantages of approaches using SWN:\nrnd. This formula represents the basic baseline random approach. It simply returns a random number between -1 and 1 for any given lemma#PoS.\nswnrnd. This is an advanced random approach that incorporates some \u201cknowledge\u201d from SWN. It takes the scores of a random sense for the given lemma#PoS. We believe this is a fairer baseline than rnd since SWN information can possibly constrain the values. A similar approach has been used in [40].\nmajority class. For the classification experiments we considered an additional baseline that always outputs the class with the higher number of instances, to account for imbalanced datasets.\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nThis is the author\u2019s version of an article that has been published in IEEE Transactions on Affective Computing. Changes were made to this version by the publisher prior to publication. The final version is available at http://dx.doi.org/10.1109/TAFFC.2015.2476456"}, {"heading": "3.3 Learning Algorithms", "text": "All the proposed formulae try to estimate the prior polarity score from the posterior polarities of all the senses for a single lemma-PoS. Each formula has its own partial view of all the information available in the senses, and different formulae can identify complementary information, e.g. some consider only the first sense (fs), others only the highest positive and negative values among all senses (max). An extension to the use of each formula in isolation consists in taking all the predicted scores produced by each formula and defining ensemble methods that, given the formulae prior polarity predictions, fuse them and emit a unique prior polarity. The most used ensemble method is the majority voting schema, that assigns to an unseen lemma-PoS the label with the highest number of votes received from the formulae. While it is quite straightforward for classification problems (see [41], chapter 3), combining regression scores can require ad-hoc decisions. To propose a solution that can be easily applied to both regression and classification, we take advantage of the classic fusion learning framework, where a regressor/classifier is fed with the output of several regressors/classifiers (in our context these are the formulae outputs) and learns from the training data the optimal way to combine them into a single score (prior polarity). For this purpose, we used two non-parametric learning approaches, Support Vector Machines (SVMs) [42] and Gaussian Processes (GPs) [43], to test the performance of all the metrics in conjunction. SVMs are non-parametric deterministic algorithms that have been widely used in several fields. GPs, on the other hand, are an extremely flexible nonparametric probabilistic framework able to explicitly model uncertainty, that only recently have been receiving increased attention in the NLP community. An exhaustive explanation of the two methodologies can be found in [42], [44] and [43]. In the SVM experiments, we use C-SVM and \u01ebSVM implemented in the LIBSVM toolbox [45]. The selection of the kernel (linear, polynomial, radial basis function and sigmoid) and the optimization of the parameters are carried out through grid search in 10- fold cross-validation. As demonstrated in [46], SVMs can benefit from the application of feature selection techniques. For this purpose, Randomized Lasso, or stability selection [47] is applied before training the SVM learner. In our experiments we set the fraction of the data to be sampled at each iteration to 75%, the selection threshold to 25% and the number of resamples to 1,000. We refer to these as SVMfs. GP2 regression models with Gaussian noise are a rare exception where the exact inference with like-\n2. More details on the differences between GPs for regression and classification and the GP kernels are available in \u00a72, \u00a73, \u00a74 in [43]\nlihood functions is tractable. Unfortunately, this is not valid for the classification task where an approximation method (Laplace [48] in our experiments) is required. Different kernels are tested (covariance for constant functions, linear with and without automatic relevance determination (ARD)3, Matern, neural network, etc.) and the linear logistic (lll) and probit regression (prl) likelihood functions are evaluated in classification. All the GP models were implemented using the GPML Matlab toolbox, and the optimization of kernel parameters is performed iteratively maximizing the marginal likelihood (or in classification, the Laplace approximation of the marginal likelihood). The maximum number of iterations was set to 100. A property of GPs is their capability of weighting the features differently according to their importance in the data. This is referred to as the automatic relevance determination kernel (ARD)."}, {"heading": "4 HUMAN-ANNOTATED SENTIMENT LEXICA", "text": "To assess how well prior polarity formulae perform, a gold standard with word polarities provided by human annotators is needed. In the following we describe in detail the two resources we used for our experiments, namely ANEW for the regression experiments and the General Inquirer for the classification."}, {"heading": "4.1 ANEW", "text": "ANEW [19] is a resource developed to provide a set of normative emotional ratings for a large number of words (roughly 1,000, half of them taken from similar previous experiments [49], [50]) in the English language. It contains a set of words that have been rated in terms of pleasure (affective valence), arousal, and dominance. The ratings were collected from students, divided in groups balanced for gender, using the \u201cSelf-Assessment Manikin\u201d, an affective rating system that uses graphic representations to depict values (e.g. happy/unhappy, excited/calm, controlled/in-control) along different emotional dimensions. Students were asked to select which image represents how they felt when reading each word. Words were shown in different order between the groups, and they were presented in isolation (i.e. no context was provided). This means that this resource represents a human validation of prior polarity scores for the given words, and can be used as a gold standard. For each word ANEW provides two main metrics: anew\u00b5, which correspond to the average score of the annotators, and anew\u03c3 , which gives the variance in annotators scores for the given word. For our task we only considered the valence rating, i.e. the degree of positivity or negativity of a word.\n3. linone and linard in the result tables, respectively.\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nThis is the author\u2019s version of an article that has been published in IEEE Transactions on Affective Computing. Changes were made to this version by the publisher prior to publication. The final version is available at http://dx.doi.org/10.1109/TAFFC.2015.2476456"}, {"heading": "4.2 General Inquirer", "text": "The Harvard General Inquirer dictionary (henceforth GI) is a widely used resource, built for automatic text analysis [23]. Its latest revision4 contains 11,789 words, tagged with 182 semantic and pragmatic labels, as well as with their part of speech. Words and their categories were initially taken from the Harvard IV4 Psychosociological Dictionary [51] and the Lasswell Value Dictionary [52]. The GI categories were defined to be used in social-science content-analysis research applications, but this resource has extensively been used for sentiment analysis too. For this paper we consider the Positive and Negative categories (1,915 words and 2,291 words respectively, for a total of 4,206 affective words), which indicate words with a positive or negative valence. As with ANEW, since these words do not have a context, we consider the labels as binary human-assigned prior polarities, thus suitable to be used as a gold standard."}, {"heading": "5 PRIOR POLARITIES EXPERIMENTS", "text": "In order to use the ANEW dataset to measure the performance of prior polarities formulae, we had to assign a PoS to all the words to obtain the SWN lemma#PoS format. To do so, we proceeded as follows: for each word, check if it is present among both SWN1 and SWN3 lemmas; if not, lemmatize the word with the TextPro tool suite [53] and check if the lemma is present instead5. If it is not found (i.e., the word cannot be aligned automatically), remove the word from the list (this was the case for 30 words of the 1,034 present in ANEW). The remaining 1,004 lemmas were then associated with all the PoS present in SWN to get the final lemma#PoS. Note that a lemma can have more than one PoS, for example, writer is present only as a noun (writer#n), while yellow is present as a verb, a noun and an adjective (yellow#v, yellow#n, yellow#a). This gave us a list of 1,484 words in the lemma#PoS format. In a similar way we pre-processed the GI words that uses the generic modif label to indicate either adjective or adverb (noun and verb PoS were consistently used instead). Finally, all the sense-disambiguated words in the lemma#PoS#n format were discarded (1,114 words out of the 4,206 words with positive or negative valence). After the two datasets were pre-processed this way, we removed the words for which the posScore and negScore contained all 0 in both SWN1 and SWN3 (523 lemma-PoS for ANEW and 484 for the GI dataset), since these words are not informative for our experiments. The final dataset included 961 entries for ANEW and 2,557 for GI. For each lemma-PoS in\n4. www.wjh.harvard.edu/\u223cinquirer/ 5. We did not lemmatize everything to avoid duplications (for example, if we lemmatize the ANEW entry addicted, we obtain addict, which is already present in ANEW).\nGI and ANEW, we then applied the prior polarity formulae described in Section 3.2, using both SWN1 and SWN3 and annotated the results. According to the nature of the human labels (real numbers or -1/1), we ran several regression and classification experiments. In both cases, each dataset was randomly split into 70% for training and the remaining for test. This process was repeated 5 times to generate different splits. For each partition, optimization of the learning algorithm parameters was performed on the training data (in 10-fold cross-validation for SVMs). Training and test sets were normalized using z-scores. To evaluate the performance of our regression experiments on ANEW we used the Mean Absolute Error (MAE) and Pearson correlation coefficient. Accuracy and Cohen\u2019s kappa were used for the classification experiments on GI instead. We opted for accuracy \u2013 rather than F1 \u2013 since for us True Negatives have the same importance as True Positives. For each experiment we reported the average performance and the standard deviation over the 5 random splits. In the following sections, we used Student\u2019s t-test to check if there were statistically significant differences in the results of regression experiments. An approximate randomization test [54] was used for the classification experiments instead. In Tables 2 and 3, the results of the regression experiments over the ANEW dataset, using SWN1 and SWN3, are presented. The results of the classification experiments over the GI dataset, using SWN1 and SWN3 are shown in Tables 4 and 5. For the sake of interpretability, results are divided according to the main approaches: randoms, posterior-to-prior formulae, learning algorithms. Note that for classification we report the generics f and not the fm and fd variants. In fact, both versions always return the same classification answer (we are classifying according to the sign of f result and not its strength). For the GPs, we report the two best configurations only."}, {"heading": "5.1 Discussion", "text": "In this section, we sum up the main results of our analysis, providing an answer to the various questions we introduced at the beginning of the paper (since results are largely consistent across the measurements both in regression and classification, in the following we will discuss MAE and accuracy only):\nSentiWordNet improves over random. One of the first things worth noting \u2013 in Tables 2, 3, 4 and 5 \u2013 is that the random approach (rnd), as expected, is the worst performing metric, while all other approaches, based on SWN, have statistically significant improvements both for MAE and for accuracy (p < 0.001).\nSWN3 is better than SWN1. With respect to SWN1, using SWN3 improves the results, both in regression (MAE\u00b5 0.398 vs. 0.366, p < 0.001) and classification\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nThis is the author\u2019s version of an article that has been published in IEEE Transactions on Affective Computing. Changes were made to this version by the publisher prior to publication. The final version is available at http://dx.doi.org/10.1109/TAFFC.2015.2476456\n(accuracy\u00b5 0.710 vs. 0.771, p < 0.001) tasks. Since many of the approaches described in the literature use SWN1 their results should be revised and SWN3 should be used as standard. This difference in performance can be partially explained by the fact that, even after pre-processing, for the ANEW dataset 137 lemma-PoS have all senses equal to 0 in SWN1, while in SWN3 they are just 48. In the GI lexicon the same occurs for 223 lemma-PoS of SWN1 and 69 of SWN3.\nNot all formulae are created equal. The formulae described in Section 3.2 have very different results, along a continuum. While inspecting every difference in performance is out of the scope of the present paper, we found that there is a strong difference between best and worst performing formulae both in regression and classification and these differences are all statistically significant (p < 0.001). Furthermore, the new formulae we introduced, based on the \u201cposterior polarities saliency\u201d hypothesis, proved to be among the best performing in all experiments. This suggests that there is room for inspecting new formulae variants other than those already proposed in the literature.\nSelecting one sense is not a good choice. On a side note, the approaches that rely on the polarity\nTABLE 2 MAE results for regression using SWN1\nApproach MAE\u00b5 MAE\u03c3 \u03c1\u00b5 \u03c1\u03c3\nrnd 0.652 0.026 -0.002 0.123 swnrndm 0.427 0.011 0.350 0.041 swnrndd 0.426 0.009 0.354 0.015\nuniwm 0.420 0.009 0.362 0.035 maxm 0.419 0.009 0.407 0.027 fsd 0.413 0.011 0.404 0.031 fsm 0.412 0.009 0.393 0.028 uni 0.410 0.010 0.372 0.044 uniwd 0.406 0.007 0.392 0.037 w1snm 0.405 0.011 0.415 0.033 maxd 0.404 0.005 0.422 0.036 w2snm 0.402 0.011 0.415 0.033 mediand 0.401 0.014 0.430 0.029 w1d 0.401 0.010 0.443 0.034 w1nd 0.399 0.008 0.428 0.034 meand 0.398 0.010 0.445 0.034 w2d 0.398 0.010 0.449 0.034 medianm 0.397 0.015 0.423 0.031 w1snd 0.397 0.008 0.428 0.034 w2snd 0.397 0.008 0.428 0.034 w2nd 0.397 0.008 0.431 0.034 w1sm 0.396 0.010 0.431 0.034 w1m 0.396 0.010 0.438 0.034 w1nm 0.394 0.009 0.432 0.036 meanm 0.393 0.011 0.443 0.038 w2sd 0.393 0.008 0.449 0.035 w1sd 0.393 0.009 0.447 0.035 w2sm 0.392 0.010 0.435 0.034 w2m 0.391 0.011 0.452 0.030 w2nm 0.391 0.012 0.439 0.034\nGPlinard 0.398 0.014 0.424 0.075 GPlinone 0.398 0.014 0.426 0.071 SVM 0.367 0.010 0.496 0.030 SVMfs 0.366 0.011 0.503 0.032\nof a single sense (namely fs, median and max) have similar results which do not differ significantly from swnrnd. These same approaches are also far from the best performing formulae: the difference between the corresponding best performing formula and the single senses formulae is always significant in the various tables (at least p < 0.01). Among other things, this finding shows that taking the first sense of a lemmaPoS in some cases has no improvement over taking a random sense, and that in all cases it is one of the worst approaches with SWN. This is surprising since in many NLP tasks, such as word sense disambiguation, algorithms based on the most frequent sense represent a very strong baseline6.\nLearning improvements. Combining the formulae in a learning framework with our ensemble methods further improves the results over the best performing formulae, both in regression (MAE\u00b5 with SWN1 0.366 vs. 0.391, p < 0.001;MAE\u00b5 with SWN3 0.333 vs. 0.359, p < 0.001) and in classification (accuracy\u00b5 for SWN1 is 0.743 vs. 0.719, p < 0.001; accuracy\u00b5 for SWN3 is 0.792 vs. 0.781, not significant p = 0.07). Another\n6. In SemEval2010, only 5 participants out of 29 performed better than the most frequent threshold [55].\nTABLE 3 MAE results for regression using SWN3\nApproach MAE\u00b5 MAE\u03c3 \u03c1\u00b5 \u03c1\u03c3\nrnd 0.652 0.026 -0.002 0.123 swnrndd 0.404 0.013 0.395 0.018 swnrndm 0.402 0.010 0.399 0.036\nmaxm 0.393 0.009 0.517 0.039 fsd 0.382 0.008 0.544 0.029 uniwm 0.382 0.015 0.490 0.049 fsm 0.381 0.010 0.540 0.031 medianm 0.377 0.008 0.502 0.024 uniwd 0.377 0.012 0.522 0.036 mediand 0.377 0.011 0.530 0.013 uni 0.376 0.010 0.493 0.030 maxd 0.372 0.011 0.549 0.028 meand 0.371 0.010 0.548 0.017 w1snm 0.371 0.011 0.527 0.040 w2snm 0.369 0.010 0.531 0.038 w1d 0.368 0.010 0.567 0.020 w2d 0.367 0.010 0.567 0.018 meanm 0.367 0.010 0.527 0.029 w1m 0.365 0.010 0.552 0.034 w2snd 0.364 0.011 0.554 0.026 w1snd 0.364 0.010 0.554 0.027 w1sm 0.363 0.009 0.533 0.038 w1nd 0.362 0.009 0.563 0.030 w2sd 0.362 0.010 0.562 0.020 w2m 0.362 0.010 0.554 0.032 w1sd 0.362 0.009 0.561 0.022 w1nm 0.362 0.007 0.549 0.045 w2nd 0.361 0.010 0.563 0.030 w2sm 0.360 0.009 0.540 0.035 w2nm 0.359 0.009 0.551 0.043\nGPlinone 0.356 0.008 0.533 0.034 GPlinard 0.355 0.008 0.533 0.032 SVM 0.333 0.004 0.569 0.027 SVMfs 0.333 0.003 0.568 0.027\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nThis is the author\u2019s version of an article that has been published in IEEE Transactions on Affective Computing. Changes were made to this version by the publisher prior to publication. The final version is available at http://dx.doi.org/10.1109/TAFFC.2015.2476456\nthing worth noting is that, in regression, GPs are outperformed by both versions of SVM (p < 0.001), see Tables 2 and 3. This is in contrast to the results presented in [56], where GPs on the single task are on average better than SVMs. In classification, GPs have similar performance to SVM without feature selection, and in some cases (see Table 5) even slightly better. In all our experiments, SVM with feature selection leads to the best performance. This is not surprising due to the high level of redundancy in the formulae scores. Interestingly, inspecting the most frequently\nselected features by SVMfs, we see that features from different groups are selected, and even the worst performing formulae can add information. This confirms the idea that viewing the same information from different perspectives (i.e. the posterior polarities provided by SWN combined in various ways using ensemble methods) can obtain better predictions. To sum up, according to our results SVMfs using SWN3 outperforms all other methods for priorpolarity computation starting from SentiWordNet."}, {"heading": "6 ERROR ANALYSIS", "text": "As a next step we wanted to understand why the learning algorithms perform better than the formulae. We inspected the errors of the best performing classifier (SVMfs) and of the best performing formula (w2) in the classification task, for a total of 652 misclassified lemma-PoS. In particular, 67% of the words are mislabeled by both methods, while w2 and SVMfs mislabel 19% and 14% respectively. A manual inspection shows that errors are mostly due to discrepancies between posterior polarity values in SWN and the gold label provided by GI. For example, pretty#a has two senses, the first one being positive and the second negative. To explore the nature of such discrepancies, we asked two annotators to inspect a subsample (50 elements) of the errors\u2019 dataset and classify whether the SWN values are correct or not, by looking at each lemmaPoS-sense value and comparing it with the synset gloss. This analysis revealed that 76% of the errors are determined by incorrect values in SWN (with a good annotators agreement, Cohen\u2019s kappa = .75). For example, the synset overjoyed#a has only one sense, with Pos(s) = 0, Neg(s) = 0.75, and this means that both SVMfs and w2 rate the word as negative even though it is positive. On the other hand, the second sense of pretty#a refers to the ironical use of the word, so its negative value is fine. Given such discrepancies, we identified how they affect w2 and SVMfs: (i) when there is an error in a lemma-PoS with only one sense (e.g. the aforementioned overjoyed#a), or errors are distributed over all senses, both methods will fail to find the correct label. This is the case for about 28% of the misclassified words in our dataset. (ii) When the first sense has a posterior polarity different from the gold label, w2 usually gives an incorrect label, as the first sense is weighted much more than all the others. Instead, SVMfs can still find the correct prior since it is less sensitive to noisy data, and it considers all the other senses in a more grounded way. For example, wickedness#a has a positive value in the first sense (0.75) and mostly negative values for the remaining 4 senses, but it is nevertheless classified as positive by w2, while it is correctly labelled as negative by SVMfs.\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nThis is the author\u2019s version of an article that has been published in IEEE Transactions on Affective Computing. Changes were made to this version by the publisher prior to publication. The final version is available at http://dx.doi.org/10.1109/TAFFC.2015.2476456\n(iii) When the first sense has the same polarity as the gold label, and most other senses have the opposite sign, SVMfs usually assigns the wrong label, while w2 does not. Albeit less common this happens for words such as confident#a, whose Pos(s) are +0.375, +0.125 and +0.125, while the Neg(s) are 0, -0.375 and -0.625. The classifier is affected by the strong negativity of the last two senses and incorrectly classifies it as negative. We also did a similar error analysis for the regression task, by defining as an error a MAE that is greater than 2 standard deviations of the overall MAE distribution. The results are in line with the previous analysis, in particular the fact that SVMfs can recover from errors or incoherent values in SWN scores better than simple formulae. Finally, in Figure 1 we report the MAE of SVMfs according to ANEW bins (the horizontal line being the average MAE on the whole dataset), in order to understand how the errors are distributed. On average, our method is less precise on extreme values, where the number of training samples from ANEW is lower."}, {"heading": "7 SentiWords", "text": "In the previous sections we have shown how an ensemble method (SVMfs) can be used to calculate more accurate prior polarities, starting from the posterior polarities scores of SWN3. We used these results to create SentiWords, a lexicon that maximizes both precision and coverage. To obtain this result, we trained our classifier on a larger dataset, the 13,915 entries of Warr [22], and used it to annotate all the lemma-PoS of SWN3. In particular, we processed Warr as we did with ANEW (see Section 5). This way, we obtained a list of 18,154 lemma-PoS, each one associated with the valence score given by human annotators, paired with the scores given by the formulae selected with randomized lasso as features. We used this as training, to create a more precise SVMfs regression model. All the lemma-PoS of SWN3 for which we had at least one non-0 value (roughly 40,000) were thus scored using\nthe SVMfs model. Finally we merged this list with Warr to obtain SentiWords. In a similar way we also created, for our experiments, SentiWordsbin, using the complete list of GI as a training set."}, {"heading": "8 PRIOR POLARITIES AND SENTIMENT ANALYSIS", "text": "To validate the improvement we can obtain in sentiment analysis with SentiWords over the single metrics and over other widely used handmade lexica like ANEW \u2013 that are more precise but have a much smaller coverage \u2013 we ran an extensive series of experiments. In these experiments we considered 2 datasets of sentences annotated both with sentiment values (ranging from -1 to 1) and sentiment labels (NEGATIVE or POSITIVE). As a comparison with SentiWords, we considered also 4 human-annotated lexica (ANEW, Warr and Stanf as gold standards for regression, the same for classification but with GI instead of ANEW) to test the importance of coverage and precision of our newly built lists. In particular:\n\u2022 ANEW represents a gold standard with low coverage on a continuous scale. \u2022 GI represents a gold standard with low coverage in a binary format. \u2022 Warr represents a gold standard with high coverage (at present the highest coverage available for prior polarities). \u2022 Stanf represents a gold standard with high coverage but with \u201dposterior-polarities\u201d.7\nTo be able to compare the results of the experiments, all these lexica were transformed to a lemma#PoS format as described in Section 5. The final size of each lexicon is reported in Table 6."}, {"heading": "8.1 Datasets", "text": "To assess how well the use of prior polarities performs on the specific task of text based sentiment analysis, we tested our resource and the gold standards lexica on two different datasets, that represents different\n7. The Stanf lexicon is not available per se, we created it by extracting all the single words present in the Stanford Sentiment Treebank (see section 8.1.2 for a description of the dataset), with their manually annotated affective score.\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nThis is the author\u2019s version of an article that has been published in IEEE Transactions on Affective Computing. Changes were made to this version by the publisher prior to publication. The final version is available at http://dx.doi.org/10.1109/TAFFC.2015.2476456\nform of language use, i.e. news headlines, with simplified syntax and lexicon, and sentences extracted from movie reviews, with normal language use. These two datasets are used in regression and classification tasks."}, {"heading": "8.1.1 SemEval", "text": "The public dataset provided for the SemEval2007 task on \u2018Affective Text\u2019 [57] is focused on emotion recognition in 1,000 news headlines, both in regression and classification settings. Headlines typically consist of a few words and are often written with the intention of \u2018provoking\u2019 emotions to attract the readers\u2019 attention. An example of a headline from the dataset is the following: \u201cIraq car bombings kill 22 People, wound more than 60\u201d. For the regression task the value provided is -0.98, while for the classification task the label provided is NEGATIVE. This dataset (which will be referred to as SemEval henceforth) is of interest to us since the \u2018compositional\u2019 problem is less prominent given the simplified syntax of news headlines, containing, for example, fewer adverbs (like negations or intensifiers) than normal sentences [58]. Each headline of the dataset was lemmatized and PoS tagged, keeping only those lemma-PoS that have a PoS mappable to WordNet. The average length of headlines is 7.21 words, (5.4 lemma-PoS). Only one headline contained just words not present in SentiWords, further indicating the highcoverage nature of our resource. In Table 7 we report the coverage of the Sentiment Lexica on the SemEval dataset (i.e. percentage of words in the sentences recognized by the lexica). Of particular interest here is the fact that, since Warr was built starting from the most commonly used English words, it grants a high coverage \u2013 higher than the Stanf Lexicon that has more entries but was built starting from a specific dataset. On the contrary, ANEW and GI show a very poor coverage and for almost half of the sentences there was no sentiment word recognized."}, {"heading": "8.1.2 Sentiment Treebank", "text": "The Stanford Sentiment Treebank (STB) is a corpus with fully labelled parse trees, that allows for a complete analysis of the compositional effects of sentiment in language [7]. The corpus is based on the dataset introduced in [59] and consists of 11,855 single sentences extracted from movie reviews. It was parsed\nwith the Stanford parser [60] and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges (using Mechanical Turk). An example of a movie review sentence is: \u201cOne of the finest, most humane and important Holocaust movies ever made.\u201d. For the regression task the value provided is +0.97, while for the classification task the label provided is POSITIVE. For our experiments we took the 11,855 sentences of the STB dataset and lemmatized and PoS tagged all the words, keeping only those lemma-PoS that had a PoS mappable on WordNet, as was done with the SemEval dataset. The average length of a sentence in STB is 20.4 words (11 lemma-PoS). This dataset is somehow complementary to the previous one, since here the syntax is not simplified and represents \u201cnatural\u201d language use. In Table 8 we report the coverage of our Sentiment Lexica on the STB dataset. Results are similar to the previous case, with ANEW and GI showing a very poor coverage: for about 40% of the sentences there was no sentiment word recognized. Note that Stanf has the same coverage as SentiWords \u2013 even if it is much smaller \u2013 since it was built starting from the words present in the STB itself and discarding those that could not be aligned with SWN entries."}, {"heading": "9 SENTIMENT ANALYSIS EXPERIMENTS", "text": "The experiments presented in this section are performed both by using sentences as present in the datasets and by filtering stop words from them. The rationale for this choice is given by the fact that prior-polarity scores can also be given to words that, for the task of text-based sentiment analysis, are not \u201crelevant\u201d, like auxiliary verbs, biasing the results. Still, it is not an error per se, to give a score to such stop words: if people perceive that they convey an affective meaning when taken in isolation, this information can be very useful for other sentiment-related tasks. Going back to the example of naming described in the introduction, let us consider the paradigmatic example of perfumes, that tend to use evocative names \u2013 since their smell cannot be \u201dshown\u201d in advertisement: we have \u201dMust\u201d from Cartier, or \u201dBe\u201d from CalvinKlein, which are both auxiliary verbs. Both examples have a\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nThis is the author\u2019s version of an article that has been published in IEEE Transactions on Affective Computing. Changes were made to this version by the publisher prior to publication. The final version is available at http://dx.doi.org/10.1109/TAFFC.2015.2476456\npositive score (usually in advertising we want a positive feeling associated with the brand) and according to Warr: be#v +0.300, must#v +0.1138. To have a fair comparison among the lexica, we rely on a standard list of stop words (the MySQL stopword list for MyISAM search indexes, consisting of 543 tokens) rather than creating one specifically tailored to our datasets or task. Stop words in this list are thus removed from the datasets in the corresponding experimental setting. Furthermore, to have a fair comparison of resources performance (i.e. without any syntactic or compositional reasoning that can boost the performance) we used a na\u0131\u0308ve approach that averages over all the word scores in a sentence, similar, for example, to the approaches used in [61] and [30]. In particular for the regression experiments we use the \u201daverage\u201d of the corresponding affective scores \u2013 obtained from the lexicon under inspection \u2013 of all lemma-PoS recognized in the text, so the sentence \u201cFamilies celebrate return of sons\u201d (i) gets PoS-tagged to \u201cfamily#n celebrate#v return#n son#n, (ii) for each resource to test, the word scores are found and averaged. For example, for SentiWords the result will be (0.562 + 0.710 + 0.237 + 0.477)/4 = 0.497, while for Stanf it will be [0.333 + 0.667 + (\u22120.055) + 0]/4 = 0.236. In classification experiments a majority vote over the single words is used to predict sentiment (e.g. \u201cMassive mud traps dozens of families\u201d will become massive#a mud#n trap#v family#n, which through SentiWordsbin gets assigned the value 0+(\u22121)+(\u22121)+1 = \u22121, i.e. a negative label). For the sake of conciseness in the following we report only the result of the best and the worst performing prior formulae \u2013 using SWN3 \u2013 for each experiment (fbest and fworst respectively). In general, the results for these formulae are consistent with the experiments carried out on prior-polarity computations, discussed in Section 5.1 (e.g. fs being one of the worst approaches also in sentiment analysis). Moreover, to test the importance of sample size for learning prior polarities, together with SentiWords results, we also report the regression results of the best learning model that was built using ANEW (SVMfs). To give a comparison we also report, separately, the results obtained by CLaC [62], the best performing system at SemEval 2007 (indicated in the tables with SemEvalbest). CLaC is an unsupervised system, i.e. without prior knowledge of this dataset. To detect headline sentiment, it uses a list of \u201csentiment-bearing\n8. A similar example can be drawn for downtoners or intensifiers (i.e. words \u2013 such as slightly, somewhat or very, completely \u2013 that decreases or increase the effect of a modified item). These words are usually adverbs or adjectives (like small or big) and while for the task of sentiment analysis they need to be considered as special linguistic objects for compositional purposes, when taken in isolation they can have their own affective score. Consider the vodka brand \u201cAbsolut\u201d (pronounced as the intensifier adjective absolute#a with a positive score of +0.108.)\nunigrams\u201d, constructed by expanding a small set of human-annotated positive and negative words using WordNet synonymy and antonymy relations, and adding G.I. Positive and Negative words too. In total, 10,809 sentiment-bearing words with different PoS are used. CLaC also uses a list of 490 valence shifters (e.g. negations, intensifiers, etc.) and rules for defining the scope and the results of the combination of sentimentbearing words and value shifters.\nIn Tables 9 and 10, the results of the regression experiments \u2013 over the SemEval and the STB datasets respectively \u2013 are presented. In this case we chose to use Pearson\u2019s correlation coefficient instead of MAE since (i) it is the official measurement of SemEval2007 and (ii) it is not sensitive to data scaling/normalization, unlike MAE, so we can directly compare the averages returned by our na\u0131\u0308ve approach with the gold standard scores.\nLexicon \u03c1\nANEW 0.175 fworst 0.268 SVMfs 0.321 fbest 0.328 Warr 0.359 SentiWords 0.377 Stanf 0.495\nLexicon (removing stop words) \u03c1\nANEW 0.177 fworst 0.284 fbest 0.335 SVMfs 0.350 Warr 0.384 SentiWords 0.402 Stanf 0.496\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nThis is the author\u2019s version of an article that has been published in IEEE Transactions on Affective Computing. Changes were made to this version by the publisher prior to publication. The final version is available at http://dx.doi.org/10.1109/TAFFC.2015.2476456\nThe results of the classification experiments over the SemEval and STB datasets are shown in Tables 11 and 12 respectively. In the SemEval2007 task, sentences in the range [\u22121,\u22120.5] were considered negative, while those in the range [0.5, 1] were labelled as positive. We took this division for our classification experiments, discarding neutral sentences (i.e. those ranging from -0.5 to 0.5), thus obtaining 410 entries with a binary polarity score, 62% of which were negative and the remaining 38% positive. We applied to the STB dataset the same \u201cbinarization\u201d that we used for SemEval, thus filtering out neutral sentences. The final dataset for the classification experiments consisted of 5,365 sentences, of which 54% were positive and 46% were negative. We also ran the same experiments on a dataset created with stricter positivity and negativity threshold (i.e., considering the sentences that fall in the range [\u22121,\u22120.2] negative and those which fall in the range [0.2, 1.0] positive, as suggested by the STB instruction file). Since the results are consistent for both datasets, we present those relating to the dataset created using the SemEval technique. In the following section, to check if there is a statistically significant difference in the results, we used Fisher\u2019s z-transformation for the correlations, and the approximate randomization test for classification experiments."}, {"heading": "9.1 Discussion", "text": "In this section, we sum up the main results of our experiments, providing an answer to the questions we introduced at the beginning of the paper:\nSize matters (learning). The use of Warr gives a boost in performance to SentiWords compared to the scores returned by SVMfs, which is based on ANEW learning sample (\u03c1 values are more than double in\n9. In the following tables, we use these abbreviations: fworst is the worst performing SWN3 formula, fbest is the best performing SWN3 formula, SVMfs refers to the SVM trained on ANEW and SemEvalbest is the best performing system at SemEval 2007.\nboth SemEval and STB datasets, both with and without stop words, p < 0.001). In fact both resources cover the whole SWN list of 155,000 lemma-PoS but since SentiWords was built starting from a resource (Warr) that contains 12 times the examples of ANEW, we can conclude that this doubling in performance is given by the initial learning sample size.\nSize matters (coverage). ANEW is a very precise lexicon but, due to the small size, its coverage is very low, with many cases of \u201cundecidable\u201d sentences (i.e. sentences for which there are no words in the lexicon). This leads to poor performance when compared to SentiWords (\u03c1 values are more than double in all regression experiments, p < 0.001), being in some cases even worse than the worst SWN formula, see Table 10. In classification, the same holds for GI versus SentiWordsbin on both datasets (p < 0.001).\nPriors, less precise but portable. The comparison with the Stanf lexicon (which is \u201cover-fitted\u201d on the STB dataset) shows that using posterior polarities can yield better results when used on specific datasets, see Table 10 and 12. Still, when used on different datasets and in different scenarios, the performance drastically decreases, see Table 9 and 11. The average correlation on the datasets is higher for SentiWords as compared to Stanf (\u03c1\u00b5 0.480 vs. 0.462 in the stop words setting). In classification the difference is less marked (accuracy\u00b5 0.599 vs 0.595), but while SentiWordsbin performance is almost identical across datasets, Stanf has a drop on the SemEval dataset. We can reasonably conclude that, if we were to consider additional datasets and domains, the difference between SentiWords and posterior lexica (i.e. Stanf ) would increase.\nStop words. The removal of stop words significantly increases the performance of our Lexica in the regression task, especially on the STB dataset (p < 0.05 for SVMfs, Warr and SentiWords). While the SemEval headlines use a simplified language (also with less stop words), the movie reviews use a plain language. In particular, 12% of lemma-PoS recognized by SentiWords were discarded from SemEval because they were in the stop words list, while in STB this\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nThis is the author\u2019s version of an article that has been published in IEEE Transactions on Affective Computing. Changes were made to this version by the publisher prior to publication. The final version is available at http://dx.doi.org/10.1109/TAFFC.2015.2476456\nnumber was more than double accounting for 26% of the SentiWords lemma-PoS discarded. As we could have expected, Stanf is less sensitive to stop words removal since it is composed of posterior polarities.\nPrecision vs. Coverage: the Losers. SWN formulae only beat ANEW in regression (and SVMfs trained on ANEW beats the formulae on average, consistently with results in section 5.1). The same holds for SWN formulae and GI in classification. That is to say: SWN metrics are better because of the high coverage compared to the two gold-prior lexica, but their precision is very low compared to Warr and SentiWords.\nPrecision vs. Coverage: the Winners. Warr and SentiWords performance is comparable on SemEval headlines, even if the Warr lexicon is much smaller. On STB, SentiWords performs better in the stop words removal setting (\u03c1 0.402 vs. 0.384, p = 0.05) and slightly better without removing them. This makes sense: on SemEval \u2013 that has a simplified language, as news headlines use only very frequent terms \u2013 the way Warr was built (i.e. considering the most frequent words in English) grants that only few words are not covered, so performance is comparable. On Stanf, where there are more words not covered by Warr, what we learnt using ML for other words helps to improve performance. In general, SentiWords is of help in any dataset for which Warr has lower coverage. Finally, for the sake of comparison, we consider also SemEvalbest (the best performing system at SemEval 2007). In our experiments, this system scored worse than Warr and SentiWords in regression, and worse than fbest and SentiWords in classification. These results give further evidence of the importance of a precise and high-coverage lexicon, in fact SemEvalbest uses elaborated compositional strategies but with a poor lexicon as compared to SentiWords. To sum up: according to our results, and to the best of our knowledge, SentiWords represents a new stateof-the-art prior-polarity lexicon for sentiment analysis. It outperforms other SWN posterior-to-prior formulae and handmade lexica thanks to its wide coverage and to the Warr lexicon it was built on."}, {"heading": "10 CONCLUSIONS", "text": "In this paper, we have presented a study on Prior Polarity lexica for sentiment analysis. While manually annotated lexica provide a high precision but lack of coverage, automatic derivation from pre-existing knowledge guarantees high coverage at the cost of a lower precision. Starting from the experience of automatic derivation of prior polarities from the SentiWordNet resource, we used an ensemble learning framework that \u2013 taking advantage of manually built lexica \u2013 is able to better predict the prior value of unseen words. We concluded by demonstrating that it is possible to use this technique to create a resource (SentiWords) with a very high coverage and a good\nprecision. Using our lexicon in sentiment analysis tasks, we were able to outperform both the single metrics derived from SentiWordNet and popular manually annotated sentiment lexica."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank Jose\u0301 Camargo De Souza for his help with feature selection. This work has been partially supported by the Trento RISE PerTe project."}], "references": [{"title": "A survey of opinion mining and sentiment analysis", "author": ["B. Liu", "L. Zhang"], "venue": "Mining Text Data, C. C. Aggarwal and C. Zhai, Eds. Springer US, 2012, pp. 415\u2013463.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "SentiWordNet: A publicly available lexical resource for opinion mining", "author": ["A. Esuli", "F. Sebastiani"], "venue": "Proc. 5th Int\u2019l Conf. Language Resources and Evaluation (LREC 06), 2006, pp. 417\u2013 422.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Sentiment analysis: How to derive prior polarities from SentiWordNet", "author": ["M. Guerini", "L. Gatti", "M. Turchi"], "venue": "Proc. 2013 Conf. Empirical Methods on Natural Language Processing (EMNLP 13), 2013, pp. 1259\u20131269.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Generating morepositive and more-negative text", "author": ["D.Z. Inkpen", "O. Feiguina", "G. Hirst"], "venue": "Computing Attitude and Affect in Text: Theory and Applications. Springer, 2006, pp. 187\u2013 198.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Valentino: A tool for valence shifting of natural language texts", "author": ["M. Guerini", "O. Stock", "C. Strapparava"], "venue": "Proc. 6th Int\u2019l Conf. Language Resources and Evaluation (LREC 08), 2008, pp. 243\u2013246.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Generating shifting sentiment for a conversational agent", "author": ["S. Whitehead", "L. Cavedon"], "venue": "Proc. North Am. Chapter of the Assoc. Computational Linguistics: Human Language Technologies 2010 (NAACL HLT 10) Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, June 2010, pp. 89\u201397.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "Proc. 2013 Conf. Empirical Methods on Natural Language Processing (EMNLP 13), 2013, pp. 1631\u20131642.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Contextual valence shifters", "author": ["L. Polanyi", "A. Zaenen"], "venue": "Computing attitude and affect in text: Theory and applications. Springer, 2006, pp. 1\u201310.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Sentiment composition", "author": ["K. Moilanen", "S. Pulman"], "venue": "Proceedings of the Recent Advances in Natural Language Processing International Conference, 2007, pp. 378\u2013382.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Affect analysis model: Novel rule-based approach to affect sensing from text", "author": ["A. Neviarouskaya", "H. Prendinger", "M. Ishizuka"], "venue": "Natural Language Eng., vol. 17, no. 1, pp. 95\u2013135, Jan. 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["S. Wang", "C. Manning"], "venue": "Proc. 50th Ann. Meeting of the Assoc. for Computational Linguistics (ACL 12), 2012, pp. 90\u201394.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "A computational approach to the automation of creative naming", "author": ["G. \u00d6zbal", "C. Strapparava"], "venue": "Proc. 50th Ann. Meeting of the Assoc. for Computational Linguistics (ACL 12), 2012, pp. 703\u2013711.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Brand Pitt: A corpus to explore the art of naming", "author": ["G. \u00d6zbal", "C. Strapparava", "M. Guerini"], "venue": "Proc. 8th Int\u2019l Conf. Language Resources and Evaluation (LREC 12), 2012, pp. 1822\u2013 1828.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Opinion mining and sentiment analysis", "author": ["B. Pang", "L. Lee"], "venue": "Foundations and Trends in Information Retrieval, vol. 2, no. 1-2, pp. 1\u2013135, 2008.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Just how mad are you? Finding strong and weak opinion clauses", "author": ["T. Wilson", "J. Wiebe", "R. Hwa"], "venue": "Proc. 19th Nat\u2019l Conf. Artificial Intelligence (AAAI 04), 2004, pp. 761\u2013769. Copyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org. This is the author\u2019s version of an article that has been published in IEEE Transactions on Affective Computing. Changes were made to this version by the publisher prior to publication. The final version is available at http://dx.doi.org/10.1109/TAFFC.2015.2476456", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Online textual communications annotated with grades of emotion strength", "author": ["G. Paltoglou", "M. Thelwall", "K. Buckley"], "venue": "Proc. 3rd Int\u2019l Workshop of Emotion: Corpora for research on Emotion and Affect (satellite of LREC 10), 2010, pp. 25\u201331.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Sentiment lexicon creation from lexical resources", "author": ["B. Heerschop", "A. Hogenboom", "F. Frasincar"], "venue": "Business Information Systems, ser. Lecture Notes in Business Information Processing, W. Abramowicz, Ed. Springer Berlin Heidelberg, 2011, vol. 87, pp. 185\u2013196. [Online]. Available: http://dx.doi.org/10.1007/978-3-642-21863-7 16", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining", "author": ["S. Baccianella", "A. Esuli", "F. Sebastiani"], "venue": "Proc. 7th Conf. Int\u2019l Language Resources and Evaluation (LREC 10), 2010, pp. 2200\u20132204.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Affective norms for English words (ANEW): Instruction manual and affective ratings", "author": ["M. Bradley", "P. Lang"], "venue": "Univ. of Florida, tech. report C-1, 1999.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}, {"title": "Lexicon-based methods for sentiment analysis", "author": ["M. Taboada", "J. Brooke", "M. Tofiloski", "K. Voll", "M. Stede"], "venue": "Computational linguistics, vol. 37, no. 2, pp. 267\u2013307, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "The dictionary of affect in language", "author": ["C. Whissell"], "venue": "Emotion: Theory, research, and experience, vol. 4, no. 113-131, p. 94, 1989.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1989}, {"title": "Norms of valence, arousal, and dominance for 13,915 english lemmas", "author": ["A. Warriner", "V. Kuperman", "M. Brysbaert"], "venue": "Behavior research methods, vol. 45, no. 4, pp. 1\u201317, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "The General Inquirer: A Computer Approach to Content Analysis", "author": ["P. Stone", "D. Dunphy", "M. Smith"], "venue": "MIT press,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1966}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["T. Wilson", "J. Wiebe", "P. Hoffmann"], "venue": "Proc. Conf. Human Language Technology and Empirical Methods in Natural Language Processing (HLT/EMNLP 05), 2005, pp. 347\u2013354.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "WordNet-Affect: an affective extension of WordNet", "author": ["C. Strapparava", "A. Valitutti"], "venue": "Proc. 4th Int\u2019l Conf. Language Resources and Evaluation (LREC 04), 2004, pp. 1083 \u2013 1086.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "Affect analysis of text using fuzzy semantic typing", "author": ["P. Subasic", "A. Huettner"], "venue": "IEEE Trans. Fuzzy Systems, vol. 9, no. 4, pp. 483\u2013496, 2001.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2001}, {"title": "Crowdsourcing a word\u2013 emotion association lexicon", "author": ["S.M. Mohammad", "P.D. Turney"], "venue": "Computational Intelligence, vol. 29, no. 3, pp. 436\u2013465, 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Textual affect sensing for sociable and expressive online communication", "author": ["A. Neviarouskaya", "H. Prendinger", "M. Ishizuka"], "venue": "Affective Computing and Intelligent Interaction, ser. Lecture Notes in Computer Science, A. Paiva, R. Prada, and R. Picard, Eds. Springer Berlin Heidelberg, 2007, vol. 4738, pp. 218\u2013229.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "DepecheMood: a lexicon for emotion analysis from crowd-annotated news", "author": ["J. Staiano", "M. Guerini"], "venue": "Proc. 52nd Ann. Meeting of the Assoc. for Computational Linguistics (ACL 14), pp. 427\u2013433, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Sentiful: A lexicon for sentiment analysis", "author": ["A. Neviarouskaya", "H. Prendinger", "M. Ishizuka"], "venue": "IEEE Trans. Affective Computing, vol. 2, no. 1, pp. 22\u201336, 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Using syntactic and contextual information for sentiment polarity analysis", "author": ["S. Agrawal", "T. Siddiqui"], "venue": "Proc. 2nd Int\u2019l Conf. Interaction Sciences: Information Technology, Culture and Human (ICIS 09), 2009, pp. 620\u2013623.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "FBK: Sentiment analysis in twitter with tweetsted", "author": ["F. Chowdhury", "M. Guerini", "S. Tonelli", "A. Lavelli"], "venue": "Proc. 7th Int\u2019l Workshop on Semantic Evaluation (SemEval \u201913), vol. 2, June 2013, pp. 466\u2013470.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "Sentiment analysis of movie reviews on discussion boards using a linguistic approach", "author": ["T. Thet", "J. Na", "C. Khoo", "S. Shakthikumar"], "venue": "Proc. 1st Int\u2019l CIKM Workshop on Topic-sentiment analysis for mass opinion (TSA 09), 2009, pp. 81\u201384.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Are SentiWordNet scores suited for multidomain sentiment classification?", "author": ["K. Denecke"], "venue": "Proc. 4th Int\u2019l Conf. Digital Information Management (ICDIM", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Sentiment polarity identification in financial news: A cohesion-based approach", "author": ["A. Devitt", "K. Ahmad"], "venue": "Proc. 45th Ann. Meeting of the Assoc. for Computational Linguistics (ACL 07), 2007, pp. 984\u2013991.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "Development of a novel algorithm for sentiment analysis based on adverb-adjectivenoun combinations", "author": ["J. Sing", "S. Sarkar", "T. Mitra"], "venue": "Proc. 3rd Nat\u2019l Conf. Emerging Trends  and Applications in Computer Science (NCETACS 12), 2012, pp. 38\u201340.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "UPAR7: A knowledge-based system for headline sentiment tagging", "author": ["F. Chaumartin"], "venue": "Proc. 4th Int\u2019l Workshop on Semantic Evaluations (IWSE 07), 2007, pp. 422\u2013425.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2007}, {"title": "Endorsements and rebuttals in blog distillation", "author": ["G. Berardi", "A. Esuli", "F. Sebastiani", "F. Silvestri"], "venue": "Information Sciences, vol. 249, pp. 38\u201347, 2013.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Sentence level subjectivity and sentiment analysis experiments in NTCIR-7 MOAT challenge", "author": ["L. Qu", "C. Toprak", "N. Jakob", "I. Gurevych"], "venue": "Proc. 7th NII Test Collection for IR Systems Workshop Meeting (NTCIR-7), 2008, pp. 210\u2013217.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2008}, {"title": "Pattern classification using ensemble methods", "author": ["L. Rokach"], "venue": "World Scientific,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2010}, {"title": "Kernel methods for pattern analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": "Cambridge Univ. press,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2004}, {"title": "Gaussian processes for machine learning", "author": ["C. Rasmussen", "C. Williams"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2006}, {"title": "Support vector machines", "author": ["A. Mammone", "M. Turchi", "N. Cristianini"], "venue": "Wiley Interdisciplinary Reviews: Computational Statistics, vol. 1, no. 3, pp. 283\u2013289, 2009.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2009}, {"title": "LIBSVM: A library for support vector machines", "author": ["C. Chang", "C. Lin"], "venue": "ACM Trans. Intelligent Systems and Technology, vol. 2, no. 3, pp. 27:1\u201327:27, 2011.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Feature selection for SVMs", "author": ["J. Weston", "S. Mukherjee", "O. Chapelle", "M. Pontil", "T. Poggio", "V. Vapnik"], "venue": "Proc. 14th Conf. Neural Information Processing Systems (NIPS 00), 2000, pp. 668\u2013 674.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2000}, {"title": "Stability selection", "author": ["N. Meinshausen", "P. B\u00fchlmann"], "venue": "J. of the Royal Statistical Soc.: Series B (Statistical Methodology), vol. 72, no. 4, pp. 417\u2013473, 2010.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}, {"title": "Bayesian classification with gaussian processes", "author": ["C. Williams", "D. Barber"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 20, no. 12, pp. 1342\u20131351, 1998.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1998}, {"title": "An approach to environmental psychology", "author": ["A. Mehrabian", "J.A. Russell"], "venue": null, "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1974}, {"title": "Words high and low in pleasantness as rated by male and female college students", "author": ["F.S. Bellezza", "A.G. Greenwald", "M.R. Banaji"], "venue": "Behavior Research Methods, Instruments, & Computers, vol. 18, no. 3, pp. 299\u2013303, 1986.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1986}, {"title": "Validation of the General Inquirer Harvard IV Dictionary", "author": ["D. Dunphy", "C. Bullard", "E. Crossing"], "venue": "Proc. Pisa Conf. Content Analysis, 1974.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1974}, {"title": "The Lasswell value dictionary", "author": ["H. Lasswell", "J. Namenwirth"], "venue": "New Haven, 1969.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1969}, {"title": "The TextPro tool suite", "author": ["E. Pianta", "C. Girardi", "R. Zanoli"], "venue": "Proc. 6th Int\u2019l Conf. Language Resources and Evaluation (LREC 08), 2008, pp. 2603\u20132607.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2008}, {"title": "More accurate tests for the statistical significance of result differences", "author": ["A. Yeh"], "venue": "Proc. 18th Int\u2019l Conf. Computational Linguistics (COLING 00), 2000, pp. 947\u2013953.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2000}, {"title": "Semeval-2010 task 17: All-words word sense disambiguation on a specific domain", "author": ["E. Agirre", "O. De Lacalle", "C. Fellbaum", "S. Hsieh", "M. Tesconi", "M. Monachini", "P. Vossen", "R. Segers"], "venue": "Proc. 5th Int\u2019l Workshop on Semantic Evaluation (IWSE 10), 2010, pp. 75\u201380.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2010}, {"title": "Modelling annotator bias with multitask gaussian processes: An application to machine translation quality estimation", "author": ["T. Cohn", "L. Specia"], "venue": "Proc. 51th Ann. Meeting of the Assoc. for Computational Linguistics (ACL 13), 2013, pp. 32\u201342.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2013}, {"title": "SemEval-2007 task 14: Affective text", "author": ["C. Strapparava", "R. Mihalcea"], "venue": "Proc. 4th Int\u2019l Workshop on Semantic Evaluations (SemEval 07), 2007, pp. 70\u201374.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2007}, {"title": "Onts: optima news translation system", "author": ["M. Turchi", "M. Atkinson", "A. Wilcox", "B. Crawley", "S. Bucci", "R. Steinberger", "E. Van der Goot"], "venue": "Proc. Demonstrations at the 13th Conf. European Chapter of the Assoc. for Computational Linguistics (EACL 12), 2012, pp. 25\u201330.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2012}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["B. Pang", "L. Lee"], "venue": "Proc. 43rd Ann. Meeting of the Assoc. for Computational Linguistics (ACL 05), 2005, pp. 115\u2013124.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning to identify emotions in text", "author": ["C. Strapparava", "R. Mihalcea"], "venue": "Proc. 23rd Ann. ACM Symp. Applied computing (SAC 08), 2008, pp. 1556\u20131560.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "lists of positive and negative words \u2013 often in conjunction with other methods (usually machine learning based) [1], to assign sentiment scores to texts.", "startOffset": 112, "endOffset": 115}, {"referenceID": 1, "context": "We focus on SentiWordNet (henceforth SWN), a resource that has been widely adopted since it provides a broadcoverage lexicon \u2013 built in a semi-automatic manner \u2013 for English [2].", "startOffset": 174, "endOffset": 177}, {"referenceID": 2, "context": "In detail, the first part of the paper \u2013 that is based on our previous work, presented in [3] \u2013 addresses three main research questions about words prior polarity computation: (i) is there any relevant difference in the posterior-to-prior polarity formulae performance (both in regression and classification tasks)? (ii) Is there any relevant variation in prior polarity values if we use different releases of SWN (i.", "startOffset": 90, "endOffset": 93}, {"referenceID": 3, "context": "First, it is fundamental for tasks such as affective modification of existing texts, where words polarity together with their scores are necessary for creating multiple versions of a text, varying its affective dimension [4], [5], [6].", "startOffset": 221, "endOffset": 224}, {"referenceID": 4, "context": "First, it is fundamental for tasks such as affective modification of existing texts, where words polarity together with their scores are necessary for creating multiple versions of a text, varying its affective dimension [4], [5], [6].", "startOffset": 226, "endOffset": 229}, {"referenceID": 5, "context": "First, it is fundamental for tasks such as affective modification of existing texts, where words polarity together with their scores are necessary for creating multiple versions of a text, varying its affective dimension [4], [5], [6].", "startOffset": 231, "endOffset": 234}, {"referenceID": 6, "context": "Works using compositional approaches worth mentioning are: [7], that uses recursive neural networks to learn compositional rules for sentiment analysis, while [8], [9], [10] exploit hand-coded rules.", "startOffset": 59, "endOffset": 62}, {"referenceID": 7, "context": "Works using compositional approaches worth mentioning are: [7], that uses recursive neural networks to learn compositional rules for sentiment analysis, while [8], [9], [10] exploit hand-coded rules.", "startOffset": 159, "endOffset": 162}, {"referenceID": 8, "context": "Works using compositional approaches worth mentioning are: [7], that uses recursive neural networks to learn compositional rules for sentiment analysis, while [8], [9], [10] exploit hand-coded rules.", "startOffset": 164, "endOffset": 167}, {"referenceID": 9, "context": "Works using compositional approaches worth mentioning are: [7], that uses recursive neural networks to learn compositional rules for sentiment analysis, while [8], [9], [10] exploit hand-coded rules.", "startOffset": 169, "endOffset": 173}, {"referenceID": 10, "context": "The work in [11] partially accounts for this problem arguing that using word bigrams allows improvement over BOW based methods, where words are taken as features in isolation.", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "Finally, tasks such as copywriting, where evocative names are a key element to a successful product [12], [13] require exhaustive lists of emotion related words.", "startOffset": 100, "endOffset": 104}, {"referenceID": 12, "context": "Finally, tasks such as copywriting, where evocative names are a key element to a successful product [12], [13] require exhaustive lists of emotion related words.", "startOffset": 106, "endOffset": 110}, {"referenceID": 13, "context": "A general overview can be found in [14], [1], [15], [16].", "startOffset": 35, "endOffset": 39}, {"referenceID": 0, "context": "A general overview can be found in [14], [1], [15], [16].", "startOffset": 41, "endOffset": 44}, {"referenceID": 14, "context": "A general overview can be found in [14], [1], [15], [16].", "startOffset": 46, "endOffset": 50}, {"referenceID": 15, "context": "A general overview can be found in [14], [1], [15], [16].", "startOffset": 52, "endOffset": 56}, {"referenceID": 16, "context": "Automatically created resources are usually larger, but their precision is highly dependent on the annotation algorithm [17] and, in general, not as accurate as manual resources.", "startOffset": 120, "endOffset": 124}, {"referenceID": 1, "context": "One of the most well-known resources is SentiWordNet (SWN) [2], [18], in which each entry is a set of lemma#PoS#sense-number sharing the same meaning, called synset.", "startOffset": 59, "endOffset": 62}, {"referenceID": 17, "context": "One of the most well-known resources is SentiWordNet (SWN) [2], [18], in which each entry is a set of lemma#PoS#sense-number sharing the same meaning, called synset.", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "These approaches, detailed in [3], produce a list of approximately 155,000 words, where the lower precision given by the automatic scoring of SWN is compensated by the high coverage.", "startOffset": 30, "endOffset": 33}, {"referenceID": 18, "context": "Another widely used resource is the Affective Norms for English Words (ANEW) [19], providing valence scores for roughly 1,000 words, which were manually assigned by several annotators.", "startOffset": 77, "endOffset": 81}, {"referenceID": 19, "context": "Similarly, the SO-CAL entries [20] consist of roughly 4,000 words manually tagged by a small number of linguists with a multi-class label (from very_negative to very_positive).", "startOffset": 30, "endOffset": 34}, {"referenceID": 20, "context": "The Dictionary of Affect in Language (DAL) contains roughly 9,000 words manually rated along the dimensions \u2018pleasantness\u2019, \u2018activation\u2019 and \u2018imagery\u2019 [21].", "startOffset": 151, "endOffset": 155}, {"referenceID": 21, "context": "More recently, a resource replicating the ANEW annotation approach using crowdsourcing was released by Warriner and colleagues [22], providing sentiment scores for approxi-", "startOffset": 127, "endOffset": 131}, {"referenceID": 22, "context": "Finally, the General Inquirer lexicon [23] provides a binary classification (positive/negative) of approximately 4,000 sentiment-bearing words manually annotated, while the resource presented in [24] expands the General Inquirer to 6,000 words.", "startOffset": 38, "endOffset": 42}, {"referenceID": 23, "context": "Finally, the General Inquirer lexicon [23] provides a binary classification (positive/negative) of approximately 4,000 sentiment-bearing words manually annotated, while the resource presented in [24] expands the General Inquirer to 6,000 words.", "startOffset": 195, "endOffset": 199}, {"referenceID": 24, "context": "One of the most used resources is WordNetAffect [25] which contains manually assigned affective labels to WordNet synsets (ANGER, JOY, FEAR, etc.", "startOffset": 48, "endOffset": 52}, {"referenceID": 25, "context": "Fuzzy Affect Lexicon [27] contains roughly 4,000 lemma-PoS manually annotated by one linguist using 80 emotion labels.", "startOffset": 21, "endOffset": 25}, {"referenceID": 26, "context": "EmoLex [28] contains almost 10,000 lemmas annotated with an intensity label for each emotion using Mechanical Turk.", "startOffset": 7, "endOffset": 11}, {"referenceID": 27, "context": "Finally Affect database is an extension of SentiFul [29] and contains 2,500 words in the form lemma#PoS, while DepecheMood [30] contains about 37,000 words also in the lemma#PoS format, and was automatically built by harvesting crowd-sourced affective annotation from a social news network.", "startOffset": 52, "endOffset": 56}, {"referenceID": 28, "context": "Finally Affect database is an extension of SentiFul [29] and contains 2,500 words in the form lemma#PoS, while DepecheMood [30] contains about 37,000 words also in the lemma#PoS format, and was automatically built by harvesting crowd-sourced affective annotation from a social news network.", "startOffset": 123, "endOffset": 127}, {"referenceID": 1, "context": "SentiWordNet [2] is a lexical resource composed of \u201csynsets\u201d, i.", "startOffset": 13, "endOffset": 16}, {"referenceID": 24, "context": "good#a#1), and (ii) automatically expanded by traversing the WordNet hierarchy to find \u201crelated\u201d synsets, using the method described in [25].", "startOffset": 136, "endOffset": 140}, {"referenceID": 17, "context": "0 [18] (SWN3).", "startOffset": 2, "endOffset": 6}, {"referenceID": 0, "context": "This produces two scores in the range [0, 1], f(posScore) and f(negScore), for each lemma-PoS.", "startOffset": 38, "endOffset": 44}, {"referenceID": 29, "context": "Based on [31], [32], [5], [33], this is the most basic form of prior polarities.", "startOffset": 9, "endOffset": 13}, {"referenceID": 30, "context": "Based on [31], [32], [5], [33], this is the most basic form of prior polarities.", "startOffset": 15, "endOffset": 19}, {"referenceID": 4, "context": "Based on [31], [32], [5], [33], this is the most basic form of prior polarities.", "startOffset": 21, "endOffset": 24}, {"referenceID": 31, "context": "Based on [31], [32], [5], [33], this is the most basic form of prior polarities.", "startOffset": 26, "endOffset": 30}, {"referenceID": 32, "context": "It was used in [34], [35], [36], [37].", "startOffset": 15, "endOffset": 19}, {"referenceID": 33, "context": "It was used in [34], [35], [36], [37].", "startOffset": 21, "endOffset": 25}, {"referenceID": 34, "context": "It was used in [34], [35], [36], [37].", "startOffset": 27, "endOffset": 31}, {"referenceID": 35, "context": "It was used in [34], [35], [36], [37].", "startOffset": 33, "endOffset": 37}, {"referenceID": 29, "context": "Based on [31], it considers only senses having a Pos(s) greater than or equal to the corresponding Neg(s), and greater than 0 (the stronglyPos set).", "startOffset": 9, "endOffset": 13}, {"referenceID": 36, "context": "The system presented in [38] uses a similar approach of weighted mean.", "startOffset": 24, "endOffset": 28}, {"referenceID": 37, "context": "Similar to the w1, this formula weigths each lemma with a harmonic series, see for example [39] (where w2 appears with the fd variant).", "startOffset": 91, "endOffset": 95}, {"referenceID": 38, "context": "A similar approach has been used in [40].", "startOffset": 36, "endOffset": 40}, {"referenceID": 39, "context": "While it is quite straightforward for classification problems (see [41], chapter 3), combining regression scores can require ad-hoc decisions.", "startOffset": 67, "endOffset": 71}, {"referenceID": 40, "context": "For this purpose, we used two non-parametric learning approaches, Support Vector Machines (SVMs) [42] and Gaussian Processes (GPs) [43], to test the performance of all the metrics in conjunction.", "startOffset": 97, "endOffset": 101}, {"referenceID": 41, "context": "For this purpose, we used two non-parametric learning approaches, Support Vector Machines (SVMs) [42] and Gaussian Processes (GPs) [43], to test the performance of all the metrics in conjunction.", "startOffset": 131, "endOffset": 135}, {"referenceID": 40, "context": "An exhaustive explanation of the two methodologies can be found in [42], [44] and [43].", "startOffset": 67, "endOffset": 71}, {"referenceID": 42, "context": "An exhaustive explanation of the two methodologies can be found in [42], [44] and [43].", "startOffset": 73, "endOffset": 77}, {"referenceID": 41, "context": "An exhaustive explanation of the two methodologies can be found in [42], [44] and [43].", "startOffset": 82, "endOffset": 86}, {"referenceID": 43, "context": "In the SVM experiments, we use C-SVM and \u01ebSVM implemented in the LIBSVM toolbox [45].", "startOffset": 80, "endOffset": 84}, {"referenceID": 44, "context": "As demonstrated in [46], SVMs can benefit from the application of feature selection techniques.", "startOffset": 19, "endOffset": 23}, {"referenceID": 45, "context": "For this purpose, Randomized Lasso, or stability selection [47] is applied before training the SVM learner.", "startOffset": 59, "endOffset": 63}, {"referenceID": 41, "context": "More details on the differences between GPs for regression and classification and the GP kernels are available in \u00a72, \u00a73, \u00a74 in [43] lihood functions is tractable.", "startOffset": 128, "endOffset": 132}, {"referenceID": 46, "context": "Unfortunately, this is not valid for the classification task where an approximation method (Laplace [48] in our experiments) is required.", "startOffset": 100, "endOffset": 104}, {"referenceID": 18, "context": "ANEW [19] is a resource developed to provide a set of normative emotional ratings for a large number of words (roughly 1,000, half of them taken from similar previous experiments [49], [50]) in the English language.", "startOffset": 5, "endOffset": 9}, {"referenceID": 47, "context": "ANEW [19] is a resource developed to provide a set of normative emotional ratings for a large number of words (roughly 1,000, half of them taken from similar previous experiments [49], [50]) in the English language.", "startOffset": 179, "endOffset": 183}, {"referenceID": 48, "context": "ANEW [19] is a resource developed to provide a set of normative emotional ratings for a large number of words (roughly 1,000, half of them taken from similar previous experiments [49], [50]) in the English language.", "startOffset": 185, "endOffset": 189}, {"referenceID": 22, "context": "The Harvard General Inquirer dictionary (henceforth GI) is a widely used resource, built for automatic text analysis [23].", "startOffset": 117, "endOffset": 121}, {"referenceID": 49, "context": "Words and their categories were initially taken from the Harvard IV4 Psychosociological Dictionary [51] and the Lasswell Value Dictionary [52].", "startOffset": 99, "endOffset": 103}, {"referenceID": 50, "context": "Words and their categories were initially taken from the Harvard IV4 Psychosociological Dictionary [51] and the Lasswell Value Dictionary [52].", "startOffset": 138, "endOffset": 142}, {"referenceID": 51, "context": "To do so, we proceeded as follows: for each word, check if it is present among both SWN1 and SWN3 lemmas; if not, lemmatize the word with the TextPro tool suite [53] and check if the lemma is present instead.", "startOffset": 161, "endOffset": 165}, {"referenceID": 52, "context": "An approximate randomization test [54] was used for the classification experiments instead.", "startOffset": 34, "endOffset": 38}, {"referenceID": 53, "context": "In SemEval2010, only 5 participants out of 29 performed better than the most frequent threshold [55].", "startOffset": 96, "endOffset": 100}, {"referenceID": 54, "context": "This is in contrast to the results presented in [56], where GPs on the single task are on average better than SVMs.", "startOffset": 48, "endOffset": 52}, {"referenceID": 21, "context": "To obtain this result, we trained our classifier on a larger dataset, the 13,915 entries of Warr [22], and used it to annotate all the lemma-PoS of SWN3.", "startOffset": 97, "endOffset": 101}, {"referenceID": 55, "context": "The public dataset provided for the SemEval2007 task on \u2018Affective Text\u2019 [57] is focused on emotion recognition in 1,000 news headlines, both in regression and classification settings.", "startOffset": 73, "endOffset": 77}, {"referenceID": 56, "context": "This dataset (which will be referred to as SemEval henceforth) is of interest to us since the \u2018compositional\u2019 problem is less prominent given the simplified syntax of news headlines, containing, for example, fewer adverbs (like negations or intensifiers) than normal sentences [58].", "startOffset": 277, "endOffset": 281}, {"referenceID": 6, "context": "The Stanford Sentiment Treebank (STB) is a corpus with fully labelled parse trees, that allows for a complete analysis of the compositional effects of sentiment in language [7].", "startOffset": 173, "endOffset": 176}, {"referenceID": 57, "context": "The corpus is based on the dataset introduced in [59] and consists of 11,855 single sentences extracted from movie reviews.", "startOffset": 49, "endOffset": 53}, {"referenceID": 58, "context": "without any syntactic or compositional reasoning that can boost the performance) we used a na\u0131\u0308ve approach that averages over all the word scores in a sentence, similar, for example, to the approaches used in [61] and [30].", "startOffset": 209, "endOffset": 213}, {"referenceID": 28, "context": "without any syntactic or compositional reasoning that can boost the performance) we used a na\u0131\u0308ve approach that averages over all the word scores in a sentence, similar, for example, to the approaches used in [61] and [30].", "startOffset": 218, "endOffset": 222}], "year": 2015, "abstractText": "Deriving prior polarity lexica for sentiment analysis \u2013 where positive or negative scores are associated with words out of context \u2013 is a challenging task. Usually, a trade-off between precision and coverage is hard to find, and it depends on the methodology used to build the lexicon. Manually annotated lexica provide a high precision but lack in coverage, whereas automatic derivation from pre-existing knowledge guarantees high coverage at the cost of a lower precision. Since the automatic derivation of prior polarities is less time consuming than manual annotation, there has been a great bloom of these approaches, in particular based on the SentiWordNet resource. In this paper, we compare the most frequently used techniques based on SentiWordNet with newer ones and blend them in a learning framework (a so called \u2018ensemble method\u2019). By taking advantage of manually built prior polarity lexica, our ensemble method is better able to predict the prior value of unseen words and to outperform all the other SentiWordNet approaches. Using this technique we have built SentiWords, a prior polarity lexicon of approximately 155,000 words, that has both a high precision and a high coverage. We finally show that in sentiment analysis tasks, using our lexicon allows us to outperform both the single metrics derived from SentiWordNet and popular manually annotated sentiment lexica.", "creator": "LaTeX with hyperref package"}}}