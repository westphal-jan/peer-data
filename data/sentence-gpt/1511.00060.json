{"id": "1511.00060", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2015", "title": "Top-down Tree Long Short-Term Memory Networks", "abstract": "In this paper we develop a recurrent neural network (TreeRNN), which is designed to predict a tree rather than a linear sequence as is the case in conventional recurrent neural networks. Our model defines the probability of a sentence by estimating the generation probability of its dependency tree. We construct the tree incrementally by generating the left and right dependents of a node whose probability is computed using recurrent neural networks with shared hidden layers. Application of our model to two language modeling tasks shows that it outperforms or performs on par with related models. This work is supported by a number of papers, including the LSE paper (see \"In this paper we have not received enough funding). In addition, our model is derived from this paper.\n\n\n\nThe model was designed to be a more robust recurrent neural network, based on an approach to the neural networks. It has the potential to change the order of learning in both recurrent and recurrent neural networks and is the cornerstone of this paper.\nWe believe that in this paper we have a clear goal, which is to improve computational power in recurrent neural networks and to improve our learning curve. This goal is to improve on any and all learning problems. The goal is to improve our learning curve and to improve our learning curve. The goal is to improve our learning curve by working in a nonlinear neural network in our network.", "histories": [["v1", "Sat, 31 Oct 2015 02:05:28 GMT  (140kb,D)", "http://arxiv.org/abs/1511.00060v1", null], ["v2", "Thu, 7 Jan 2016 00:48:42 GMT  (108kb,D)", "http://arxiv.org/abs/1511.00060v2", null], ["v3", "Sun, 3 Apr 2016 23:30:17 GMT  (109kb,D)", "http://arxiv.org/abs/1511.00060v3", "to appear in NAACL 2016; code available atthis https URL"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["xingxing zhang", "liang lu", "mirella lapata"], "accepted": true, "id": "1511.00060"}, "pdf": {"name": "1511.00060.pdf", "metadata": {"source": "CRF", "title": "Tree Recurrent Neural Networks with Application to Language Modeling", "authors": ["Xingxing Zhang", "Liang Lu", "Mirella Lapata"], "emails": ["x.zhang@ed.ac.uk,", "liang.lu@ed.ac.uk,", "mlap@inf.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Statistical language modeling plays an important role in many areas of natural language processing including speech recognition, machine translation, and information retrieval. The prototypical use of language models is to assign probabilities to sequences of words. By invoking the chain rule, these probabilities are generally estimated as the product of conditional probabilities P (wi|hi) of a word wi given the history of preceding words hi \u2261 wi\u221211 . In theory, the history could span any number of words up towi, however it has proven challenging in practice to deal with the combinatorial growth in the number of possible histories. A simple and effective strategy is to truncate the chain rule to include only the n \u2212 1 preceding words. The simplification reduces the number of free parameters however, at the expense of being able to model long-range dependencies. The literature offers many examples of how to overcome this limitation, such as cache language models (Kuhn and de Mori, 1990), trigger models (Rosenfeld, 1996), and notably structured language models (Chelba et al., 1997; Chelba and Je-\nlinek, 2000; Roark, 2001). The latter go beyond the representation of history as a linear sequence of words to capture the syntactic constructions in which these words are embedded.\nNeural language models have been gaining increasing attention in recent years as a competitive alternative to n-grams. The main idea is to represent each word using a real-valued feature vector capturing the contexts in which it occurs. The conditional probability of the next word is then modeled as a smooth function of the feature vectors of the preceding words and the next word. In essence, similar representations are learned for words found in similar contexts resulting in similar predictions for the next word. Previous approaches have mainly employed feed-forward (Bengio et al., 2003; Mnih and Hinton, 2007) and recurrent neural networks (Mikolov et al., 2010; Mikolov et al., 2011c; Mikolov et al., 2011b; Auli et al., 2013; Mikolov, 2012) in order to map the feature vectors of the context words to the distribution for the next word.\nDespite superior performance in a many applications ranging from machine translation (Cho et al., 2014), to speech recognition (Mikolov et al., 2011c; Chen et al., 2015), image description generation (Vinyals et al., 2015), and language understanding (Yao et al., 2013), standard neural language models essentially predict sequences of words. Many NLP models, however, exploit syntactic information and therefore operate over tree structures (e.g., dependency or constituent trees). In this paper we develop a novel neural network model (TreeRNN) which combines the advantages of recurrent neural network language models and syntactic structure. Our model estimates the probability of a sentence by estimating the generation probability of its dependency tree. Instead of explicitly encoding tree structure as a set of features, we use four recurrent neural networks (RNNs) to model four types of dependency edges which alar X iv :1 51 1.\n00 06\n0v 1\n[ cs\n.C L\n] 3\n1 O\nct 2\n01 5\ntogether specify how the tree is built. At each time step, one RNN is activated which predicts the next word conditioned on the sub-tree generated so far. To learn the representations of the conditioned sub-tree, we force the four RNNs to share their hidden layers. Besides estimating the probability of a tree or sub-tree, our model is also capable of generating trees just by sampling from a trained model and can be seamlessly integrated with text generation applications, e.g., performing machine translation (Sutskever et al., 2014; Cho et al., 2014) or image description (Kiros et al., 2014; Vinyals et al., 2015).\nWe train our model on large vocabularies (in the scale of 65K) using noise-contrastive estimation (Gutmann and Hyva\u0308rinen, 2012) and apply it to two language modeling benchmark datasets. We experimentally show that it is superior or comparable to other state-of-the-art systems."}, {"heading": "2 Related Work", "text": "Our work combines two strands of research: recurrent neural network-based language models (RNNLMs) and syntax-based language models. RNNLMs typically model linear word sequences without taking syntactic information into account. Contrary to feedforward neural networks which consider only the direct (n\u22121) predecessor words for predicting the probability of the next word, RNNLMs can in theory take the entire sequence of preceding words into account. However, in practice it has proven difficult due to gradient exploding and gradient vanishing problems (Bengio et al., 1994; Hochreiter, 1998).\nSimilar to RNNLMs, our model can also capture longer-range dependencies across the entire history of preceding words. Importantly, we can estimate the probability of a tree, which a standard RNNLM cannot. Moreover, we argue that compared to a conventional RNNLM, our tree-structured model is easier to train since it reduces the dependency range of the language model. Intuitively, for a sentence with n words, the farthest dependent will be at length n \u2212 1, whereas for a dependency tree with n nodes the farthest dependent will not be longer. In a balanced tree, it is roughly log(n) \u2212 1. Empirically, we also observed (in the Penn Treebank and APNews datasets; see Section 5 for details) that on average the farthest dependent is for TreeRNN at length 10, while for RNNLM it is at length 24.\nRecursive Neural Networks (Pollack, 1990) are\na related class of models which operate on structured inputs. Given the structural representation of a sentence (e.g., a binary parse tree), they recursively generate parent representations in a bottom-up fashion, by combining tokens to produce representations for phrases, and eventually the whole sentence. The learned representations can be then used in classification tasks such as sentiment analysis (Socher et al., 2011b) and paraphrase detection (Socher et al., 2011a). The recently proposed tree-structured long shortterm memory network model (Tai et al., 2015) models sentential meaning whilst taking syntactic structure into account. It generalizes the standard Long Short-Term Memory (LSTM) architecture (Hochreiter and Schmidhuber, 1997) to treestructured network topologies. Recursive neural networks and the tree-LSTM both learn semantic representations over syntactic trees but cannot predict their structure or estimate their probability.\nThe idea to inject long distance syntactic information into a language model dates back to Chelba and Jelinek (2000). Their model conditions the probability of the next word on the linear trigram context and some part of the dependency tree relating to the word\u2019s left antecedents. Other work develops dependency-based language models for specific applications such as machine translation (Shen et al., 2008; Zhang, 2009; Sennrich, 2015) or sentence completion (Gubbins and Vlachos, 2013). All instances of these models apply Markov assumptions on the dependency tree, and adopt standard n-gram smoothing methods for reliable parameter estimation. Emami et al. (2003) and Sennrich (2015) estimate the parameters of a structured language model using feed-forward neural networks (Bengio et al., 2003).\nOur model shares with other structured-based language models the ability to take dependency information into account. It differs in the following respects: (a) it does not artificially restrict the depth of the dependencies it considers and can thus be viewed as an infinite order dependency language model; (b) it not only estimates the probability of a string but is also capable of generating dependency trees simply by sampling from a trained model; this explicit generation mechanism sets our model apart from recursive neural networks (and related models) which learn representations of phrases and sentences in a continuous space without an underlying generation model; (c) finally, contrary to previous dependency-based\nlanguage models which encode syntactic information as features, our model takes tree structure into account more directly via representing different types of dependency edges explicitly using RNNs. Therefore, there is no need to manually determine which dependency tree features should be used or how large the feature embeddings should be."}, {"heading": "3 Recurrent Neural Network Language Model", "text": "In this section we briefly describe RNNLMs and then proceed to introduce our model. Let S = w1, w2, . . . , wm denote a linear sequence of words. We estimate its probability as:\nP (S) = m\u220f i=2 P (wi|w1:i\u22121) (1)\nEach term in Equation (1) corresponds to the output of a recurrent neural network at one time step (Mikolov et al., 2010). Let e(wt) \u2208 R|V |\u00d71 denote the input layer at time t (with e(wt) being the one hot vector of wt and |V | the vocabulary size), ht denotes the hidden layer, and yt the output layer. Wih \u2208 Rr\u00d7|V | is the weight matrix between the input layer and the hidden layer (r is the hidden unit size), Whh \u2208 Rr\u00d7r is the weight matrix between the hidden layer of the last time step and the \u201ccurrent\u201d hidden layer, and Who \u2208 R|V |\u00d7r the weight matrix between the hidden layer and the output layer. The input, hidden, and output layers are computed as:\nh0 = 0 (2a) ht = f(Wih \u00b7 e(wt) +Whh \u00b7 ht\u22121) (2b) yt = softmax(Who \u00b7 ht) (2c)\nwhere f is a non-linear function (e.g. sigmoid) and e(wt+1)T \u00b7 yt a term in Equation (1) (i.e., P (wt+1|w1:t)). The RNN is trained by minimizing the negative log likelihood with Stochastic Gradient Descent."}, {"heading": "4 Tree Recurrent Neural Network Language Model", "text": "We seek to model the probability of a sentence by estimating the generation probability of its dependency tree. We will first present our model when using unlabeled dependencies and then show how it extends to labeled dependencies (Section 4.3).\nWe assume a dependency tree is built in breadth-first manner. Generation starts at the\nROOT node, the only node at level zero. For each node at each level, we first generate left dependents (from closest to farthest) and then right dependents (again from closet to farthest). Figure 1 illustrates the breadth-first traversal of the dependency tree for the sentence A little girl is climbing into a lovely wooden playhouse. As can be seen, we first visit climbing, then its left dependents (is, girl ), and then its right dependents (into, .); next, we visit girl which has only left dependents (little, a), and so on.\nWe further assume that each word w in a tree (only) depends on its dependency path, D(w), which is essentially a sub-tree (see Section 4.1 for details on how we define dependency paths). Therefore, the probability of a sentence S given its dependency tree T is:\nP (S|T ) = \u220f\nw\u2208BFS(T )\\ROOT\nP (w|D(w)) (3)\nwhere the ordering of words w corresponds to a breadth-first enumeration of the dependency tree (BFS(T )) and the probability of ROOT is ignored since every tree has a ROOT node. The role of ROOT in a dependency tree is the same as the begin of sentence token (BOS) in a sentence. When computing P (S|T ), the probability of ROOT and BOS are not taken into account (we assume they always exist), but they are both used to predict other words."}, {"heading": "4.1 Dependency Path", "text": "A dependency path can be broadly described as the path between ROOT and w, consisting of the words and edges connecting them. To represent a dependency path, we define four types of edges. Let w0 denote a node in a dependency tree and w1, w2, . . . , wn its left dependents. As shown in Figure 2, LEFT edge is the edge between w0 and its first left dependent denoted as (w0, w1). Assume wk (with 1 < k \u2264 n) is a non-first left dependent of w0. The edge from wk\u22121 to wk is a NX-LEFT edge, where wk\u22121 is the right adjacent sibling of wk. We use NX as a shorthand for NEXT. In our model, the NXLEFT edge (wk\u22121, wk) replaces edge (w0, wk) (illustrated with a dashed line in Figure 2) in the original dependency tree. We do this because we want information to flow from w0 to wk throughw1, . . . , wk\u22121 rather than directly fromw0 to wk. RIGHT and NX-RIGHT edges are defined analogously for right dependents.\nGiven these four types of edges, we can now define dependency paths. Recall that the only dependent of ROOT is its first right dependent. Let wp denote the parent of w.\n(1) if w is ROOT, then D(w) = \u2205 (2) if w is a left dependent of wp\n(a) if w is the first left dependent, then D(w) = D(wp) \u22c3 {\u3008wp, LEFT\u3009} (b) ifw is not the first left dependent andws\nis its right adjacent sibling, then D(w) = D(ws) \u22c3 {\u3008ws,NX-LEFT\u3009}\n(3) if w is a right dependent of wp\n(a) if w is the first right dependent, then D(w) = D(wp) \u22c3 {\u3008wp,RIGHT\u3009}\n(b) if w is not the first right dependent and ws is its left adjacent sibling, then D(w) = D(ws) \u22c3 {\u3008ws,NX-RIGHT\u3009}\nTo provide examples of dependency paths, consider again Figure 1. Here, D(climbing) =\n{\u3008ROOT,RIGHT\u3009} (see definitions (1) and (3a)), D(is) =D(climbing) \u22c3 {\u3008climbing, LEFT\u3009} (according to definition (2a)), whereas D(girl) = D(is) \u22c3 {\u3008is,NX-LEFT\u3009} (see (2b)),\nD(into) = D(climbing) \u22c3 {\u3008climbing,RIGHT\u3009}\n(see (3a)), D(.) = D(into) \u22c3 {\u3008into,NX-RIGHT\u3009} (according to (3b)). A dependency tree can be represented by the set of its dependency paths which in turn can be used to reconstruct the original dependency graph."}, {"heading": "4.2 Generation with four RNNs", "text": "In a fashion analogous to Equation (1), we estimate the probability P (w|D(w)) (Equation (3)) using RNNs. While it is straightforward to model a word sequence in an RNN, we are trying to modelD(w), which is a sub-tree or more precisely a sequence of \u3008word, edge-type\u3009 tuples. To do so, we use four RNNs (GEN-L, GEN-R, GEN-NX-L and GEN-NX-R)1 each corresponding to the four types of edges (LEFT, RIGHT, NX-LEFT, and NXRIGHT). For each \u3008word, edge-type\u3009 tuple, we choose an RNN according to edge-type, feed the word to the RNN, and generate/predict its dependent. Specifically, RNNs GEN-L and GEN-R generate the first left and right dependents (see w1 and w4 in Figure 3). So, they are responsible for going deeper in a tree. RNNs GEN-NX-L and GEN-NX-R generate the remaining left/right dependents, and thus go wider in a tree. In Figure 3, RNN GEN-NX-L generates w2 and w3, whereas RNN GEN-NX-R generates w5 and w6. Note that our model can handle any number of left or right dependents (through successive application of GEN-NX-L or GEN-NX-R). Also note that the four RNNs exchange information by sharing their hidden layers. That is the output of the (updated) hidden layer of one RNN might be used as the input (previous) hidden layer of another RNN.\nWith a standard RNN (see Section 3) modeling a linear sequence, at time step t we need to know the input wt, the previous hidden layer ht\u22121, and the output wt+1 in order to compute the new hidden layer ht. Our model estimates P (wt|D(wt)), the probability of wt given its dependency path D(wt) represented as a sequence of \u3008word, edgetype\u3009 tuples. So, we need to know at time step t the dependent wt to be predicted and the last tuple \u3008w\u2032t, at\u3009 in its dependency pathD(wt) where at is the type of the edge between w\u2032t and wt. Since\n1GEN stands for GENERATE, NX stands for NEXT, L for LEFT, and R for RIGHT.\nour model is recurrent, information about other tuples in the sequence is recorded in the RNNs\u2019 hidden layers. Moreover, we need to know where to get the hidden layer of the last time step from, and where to position the new hidden layer. In a tree, hidden layers are not accessed sequentially as in a sentence. To store the shared hidden layers, we create matrix H \u2208 Rr\u00d7(n+1) (with r denoting the hidden unit size). The first column in this matrix corresponds to the initial hidden layer and each of the remaining n columns corresponds to the representation of a tree node\u2019s dependency path. Let at \u2208 {0, 1, 2, 3} denote the four types of edges defined in Section 4.1 (0 corresponds to LEFT, 1 to RIGHT, 2 to NX-LEFT, and 3 to NX-RIGHT). Time steps t in our case are the steps taken by breadth-first search while traversing the dependency tree and are represented as:\n(w\u2032t at t \u2032 t wt) t = 1, 2, . . . , n (4)\nwhere t\u2032 is the breath-first search id of w\u2032t. We are now ready to define our TreeRNN model. Our current formulation uses basic recurrent neural networks (Elman, 1990) as a backbone. Other variants such as Long Short Term Memory Recurrent Neural Network (Hochreiter and Schmidhuber, 1997) or Bidirectional Recurrent Neural Networks (Schuster and Paliwal, 1997) can be applied, however we leave this to future work. The parameters of TreeRNN are as follows. We \u2208 Rs\u00d7|V | is the word embedding matrix (s is the word embedding size and |V | the vocabulary\nsize); Wih \u2208 R4\u00d7r\u00d7s denotes four weight matrices (one for each RNN) between the input layer and the hidden layer, and r is the hidden unit size; Whh \u2208 R4\u00d7r\u00d7r denotes the four weight matrices between the previous hidden layer and the current hidden layer; finally, Who \u2208 R4\u00d7|V |\u00d7r denotes the four weight matrices between the hidden layer and the output layer.\nLet e(wt) \u2208 R|V |\u00d71 denote the one hot vector of wt. Given the representation in Equation (4), the weight matrices used at time step t are:\nW cih =Wih[at, :, :] (5a) W chh =Whh[at, :, :] (5b) W cho =Who[at, :, :] (5c)\nBased on the edge type at, we select an RNN (GEN-L, GEN-R, GEN-NX-L, or GEN-NX-R) accordingly. Computation of TreeRNN proceeds as follows:\nH[:, 0] = 0 (6a) xt =We \u00b7 e(w\u2032t) (6b) ht = f(W c ih \u00b7 xt +W chh \u00b7H[:, t\u2032]) (6c) H[:, t] = ht (6d) yt = softmax(W c ho \u00b7 ht) (6e)\nwhere f is the hidden layer activation function (we use tanh in our experiments). ht is the hidden layer and yt is the output layer. e(wt)T \u00b7 yt corresponds to the P (wt|D(wt)) term in Equation (3). Note that H[:, 0], the initial hidden layer in Equation (6a), can be also initialized to a vector with a small value such as 0.01. The training objective is negative log-likelihood (NLL):\nJNLL(\u03b8) = \u2212 1 |S| \u2211 S\u2208S logP (S|T ) (7)\nwhere S refers to the training set (or a mini batch) and S to a sentence in S and T to the dependency tree of S (see Equation (3)). Finally, we should also point out that although our model consists of four jointly trained RNNs, for a single training example the training and inference complexity is the same as a regular RNN, because at each time step only one RNN is working."}, {"heading": "4.3 Labeled Dependency Model (LTreeRNN)", "text": "The model in the previous section does not take dependency labels into account. Labels provide valuable information with regard to the meaning\nof sentences for at least two reasons. Firstly, they can potentially discriminate amongst dependency trees with otherwise very similar structure. To give an example, there are 28 unique incoming dependency labels for the word mind in one of our datasets.2 The most frequent labels are POBJ, DOBJ, NSUB, ROOT, and CCOMP indicating that mind is often used as a verb in the corpus, but without explicit label information it would be difficult to distinguish between the verb and noun usages of mind. Secondly, trees with different structures, may be deemed similar if they have similar dependency labels.\nTo model the intuition above, we modify TreeRNN so as to consider dependency labels (in addition to words). The probability of a sentence S given a labeled tree TL now becomes:\nP (S|TL) = \u220f\nw\u2208BFS(TL)\\ROOT\nP (w|D(w),B(w))\n(8) where B(w) denotes incoming dependency labels for words in D(w). We represent labeled trees by modifying Equation (4) as follows:\n(l\u2032t w \u2032 t at t \u2032 t wt) t = 1, . . . , n (9) where l\u2032t are the incoming labels of w \u2032 t. Wl \u2208 Rq\u00d7|L| denotes a dependency label embedding matrix (q is the label embedding size and |L| is the size of the label set L). Wlh \u2208 R4\u00d7r\u00d7q denotes the the four weight matrices between the label embedding layer and the hidden layer for the four RNNs. The weight matrices at time t are the same shown in Equation (5) with the addition of:\nW clh =Wlh[at, :, :] (10a)\nThe labeled model is similar to the unlabeled TreeRNN (see Equation (6)), modulo the computation of the hidden layer ht which additionally takes the label embedding bt into account:\nbt =Wl \u00b7 e(l\u2032t) (11a) ht =f(W c ih \u00b7 xt +W clh \u00b7 bt +W chh \u00b7H[:, t\u2032])\n(11b)\nwhere e(l\u2032t) is the one hot vector of l \u2032 t.\nAnalogously to the unlabeled model in Equation (8), the probability P (w|D(w),B(w)) corresponds to e(wt)T \u00b7yt (see Equation (6)). The training objective for the labeled model is also negative log likelihood. We simply replace P (S|T ) in Equation (7) with P (S|TL).\n2The MSR Sentence Completion corpus (see Section 5.3 for details)."}, {"heading": "4.4 Model Training", "text": "Computing the full softmax of the output layer can be very expensive, when a large vocabulary is used (e.g., more than 30K words). Class-based methods (Mikolov et al., 2011c) and noise-contrastive estimation (NCE; Gutmann and Hyva\u0308rinen (2012)) are often used to speedup training time. For largescale experiments, we employ NCE which does not require repeated summations over the whole vocabulary and has been previously shown to work well for neural language models (Mnih and Teh, 2012; Vaswani et al., 2013).\nThe intuition behind NCE is to perform binary classification to discriminate between samples from the data distribution and samples from a noise distribution. The variance of the normalization term is minimized during training. We thus replace the normalized probability P (w|D(wt)) with the unnormalized probability P\u0302 (w|D(wt)) and treat Z\u0302 as a constant (w can be any word in V ):\nP (w|D(wt)) = e(w)T \u00b7 yt (12a)\n= exp(W cho[w, :] \u00b7 ht)\u2211|V | i=1 exp(W c ho[i, :] \u00b7 ht) (12b) \u2248 P\u0302 (w|D(wt)) (12c)\nP\u0302 (w|D(wt)) = exp(W cho[w, :] \u00b7 ht)\nZ\u0302 (13)\nwhere P\u0302 (w|D(wt)) is the estimated word distribution. Let Pn(w) denote the noise word distribution. We assume that noise words are k times more frequent than real words (Mnih and Teh, 2012; Vaswani et al., 2013). Thus, words come from the distribution 1k+1 P\u0302 (w|D(wt))+ k k+1Pn(w). Finally, the posterior probability of a word w being generated from the from the TreeRNN distribution (P (D = 1|w,D(wt))) or the noise distribution (P (D = 0|w,D(wt))) is:\nP (D = 1|w,D(wt)) = P\u0302 (w|D(wt))\nP\u0302 (w|D(wt)) + kPn(w)\nP (D = 0|w,D(wt)) = kPn(w)\nP\u0302 (w|D(wt)) + kPn(w) And the training objective now becomes:\nJNCE(\u03b8) =\u2212 1 |S| \u2211 T\u2208S |T |\u2211 t=1 ( logP (D = 1|wt,D(wt))\n+ k\u2211 j=1 logP (D = 0|w\u0303t,j ,D(wt)) )\nwhere w\u0303t,j is a word drawn from the noise distribution Pn(w). As in Mikolov et al. (2013b), we use smoothed unigram frequencies (exponentiating by 0.75) as the noise distribution Pn(w). For simplicity, we set ln Z\u0302 = 9.5 rather than learn it (Mnih and Teh, 2012; Vaswani et al., 2013; Chen et al., 2015), and use 20 negative samples."}, {"heading": "4.5 Tree Generation", "text": "Previous work has shown that it is relatively straightforward to jointly train neural language models with other types of neural networks to perform tasks involving natural language generation such as translating sentences and generating descriptions for images (Kalchbrenner and Blunsom, 2013; Kiros et al., 2014; Zhang and Lapata, 2014; Sutskever et al., 2014; Cho et al., 2014; Vinyals et al., 2015). Typically, the output of a neural sentence model or neural image model serves as additional input (other than preceding words) to the neural language model. The generation starts with the beginning-of-sentence (BOS) token, and at each step, a word is sampled from the language model\u2019s distribution. Generation terminates when the end-of-sentence (EOS) token is predicted.\nIn a similar fashion, TreeRNN can be jointly trained with other neural network models to perform any of the aforementioned generation tasks. TreeRNN generates a dependency tree which can be easily converted to a sentence by doing inorder traversal. Generation starts at the ROOT node. Then at each step, TreeRNN can generate a left/right dependent with GEN-L/GEN-R RNN or a left/right sibling with GEN-NX-L/GEN-NX-R RNN. Problematically, at generation time we do not know whether a node has dependents or siblings! We could easily address this at training time, by adding two artificial children for each node indicating the end of generation (EOG) on the left and right, respectively. At generation time, we would then stop generating dependent or siblings when the EOG token is predicted. Unfortunately this approach would render training computationally prohibitive for large datasets (a sentence with N words would correspond to a tree with 3N + 1 nodes).\nInstead, we use four binary classifiers to predict whether we should GENERATE-LEFT, GENERATE-RIGHT, GENERATE-NEXT-LEFT, or GENERATE-NEXT-RIGHT at each node using the TreeRNN hidden units ht (see Equation (6c)) as features. Specifically, we use a trained TreeRNN\nmodel to go through the training corpus and generate hidden units as input features; the corresponding class labels (i.e., 0 for generating a dependent and 1 for not generating one) are \u201cread off\u201d the dependency trees present in the training data. We use two-layer neural networks as the four classifiers (details on the classifiers are given in Section 5.4). Note that the predictions of the GENERATELEFT and GENERATE-RIGHT classifiers do not influence the predictions of the GENERATE-NEXTLEFT and GENERATE-NEXT-RIGHT classifiers; the former classifiers determine whether to add a left/right dependent to the current node, whereas the latter determine whether to add a left/right dependent to the parent of the current node."}, {"heading": "5 Experiments", "text": "In this section we present details on how our models were trained and experimental results on two evaluation tasks commonly used to assess the performance of language models. Specifically, in Section 5.2 we perform perplexity experiments on the well-known Penn Treebank (PTB; Marcus et al. (1993)) and the APNews dataset (Bengio et al., 2003). Section 5.3 presents our results on the Microsoft Research (MSR) Sentence Completion Challenge dataset (Zweig and Burges, 2012). We also illustrate the tree generation capabilities of our model in Section 5.4."}, {"heading": "5.1 Training Details", "text": "In all our experiments, we trained labeled and unlabeled versions of TreeRNN with stochastic gradient descent without momentum on an Nvidia GTX 980 Graphic Card. We used a mini-batch size of 50 to 100. We initialized all parameters of our model with the uniform distribution between \u22120.2 to 0.2. We used an initial learning rate of 0.3 or 0.1 for all experiments and validated the model per epoch on the validation set. In cases where there was no significant improvement in log-likelihood, we divided the learning rate by 2 or 1.5 per epoch until there was no improvement in log-likelihood again. It is well known that RNNs suffer from problems of exploding gradients; as a result, we rescaled the gradient g when the gradient norm ||g|| > 5 and set g = 5g||g|| (Pascanu et al., 2013; Sutskever et al., 2014). The word embedding size was set to s = min(100, r/2) where r is the hidden unit size. Label embedding size q = 30 when s = 100, otherwise q = 15."}, {"heading": "5.2 Perplexity Evaluation", "text": "For the Penn Treebank dataset, following common practice, we trained on PTB sections 0\u201320 (1M words), used sections 21\u201322 for validation (80K words), and sections 23\u201324 (90K words) for testing. We report experiments on gold standard dependency trees. Specifically, we used the Stanford CoreNLP toolkit (Manning et al., 2014) to convert gold PTB phrase structure trees to dependencies. Previous work (Mikolov et al., 2011a) on this corpus follows a preprocessing regime with speech recognition in mind where punctuation is removed, words are lower-cased, numbers are normalized, and so on. We replaced words attested 5 times or less with UNK and normalized to lower case but did not remove punctuation. We obtained a vocabulary of 10K words. With regard to the APNews corpus (Bengio et al., 2003), the training set contains 14M words (sampled from the APW 1996 partition of the English Gigaword3), the validation set contains 1M words (sampled from APW 199501 to 199506) and the test set contains 1M words (sampled from APW 199507 to 199512). All our experiments on this corpus were conducted on automatic parse trees which we obtained with the Stanford CoreNLP toolkit. We used a vocabulary size of 22K.\nAside from experimenting with gold and automatic parse trees, we evaluated several other versions of our model: with and without dependency labels (LTreeRNN vs. TreeRNN), and using the full softmax during training or noise contrastive estimation (NCE). We compared these variants against modified Kneser-Ney 5-gram model (KN5), a representative n-gram backoff model, and a Recurrent Neural Network Language Model (Mikolov et al., 2011a), which has previously achieved substantial perplexity reductions over a wide range of language models. We used SRILM (Stolcke and others, 2002) and the RNNLM toolkit (Mikolov et al., 2011c) to implement these models.\nWe used perplexity to evaluate model performance. Perplexity computations typically take end of sentence (EOS) tokens into account, however there is no EOS in TreeRNN4 (see Equation (3)). For a fair comparison, EOS markers\n3https://catalog.ldc.upenn.edu/ LDC2003T05\n4As mentioned in Section 4.5, we could have added an end of left and right child token to each node; however, we refrained from doing this as it would significantly increase computational complexity in the model.\nwere not included in any of the datasets used in our experiments. This concerns both PTB and APNews in all partitions (training, validation, and test set), and all comparison models (TreeRNN, LTreeRNN, RNN, and KN5).\nAnother issue concerns the hidden unit size of the RNN models under consideration. There is no one-size fits all solution, and simply keeping the number of parameters identical across models does not necessarily entail a fair comparison (the same hidden unit size may be optimal for one model but lead to overfitting or underfitting for another). We therefore report test set results for the RNN and TreeRNN models with hidden unit sizes deemed optimal on the validation set. Specifically, we start from a small hidden unit size (i.e., 50 on the PTB and 100 on the APNews corpora) and progressively attempt to make the model larger until performance on the validation set no longer improves (perplexity reduction is less than 2). An exhaustive list of the hidden unit sizes we experimented with together with model performance on the validation set is presented in Appendix A.\nTable 1 summarizes our perplexity results with models trained on PTB (1M words; gold dependencies) and APNews (14M words; automatic dependencies). As can be seen, hidden unit sizes vary with different amounts of training data. We generally observe that when models have access to more data, larger hidden units improve performance (Mikolov, 2012). The best performing RNN on PTB has 250 hidden units, and 500 on\nAPNews. TreeRNN obtains best results on PTB with 100 hidden units; on APNews, the hidden units increase to 200 (or 300 when NCE is employed). TreeRNN has approximately 2.26 times more parameters compared to RNN, assuming the hidden unit size is equal (see Appendix B for details on how we compute the number of parameters). Thus, the optimal RNN and TreeRNN models have comparable number of parameters on both PTB and APNews datasets (250 vs. 100 and 500 vs. 200; see Table 1).\nOverall, TreeRNN and LTreeRNN obtain substantial perplexity reductions over KN5 and RNN. LTreeRNN yields slightly better perplexity which indicates that dependency labels help predict the target word. Perplexity reductions are also observed when using noise contrastive estimation (NCE rows in Table 1). NCE speeds up training (on PTB by a factor of 1.21 and on APNews by a factor of 1.95), at the expense of a slight degradation in perplexity. Interestingly, TreeRNN and LTreeRNN maintain a lead in perplexity over RNN and KN5 even when trained on automatically parsed trees which unavoidably contain errors. Our results give rise to two questions: (1) why is TreeRNN better than standard RNN, and (2) why would one expect TreeRNN to outperform5 other syntax-based language models?\nThere are at least two reasons for the superiority of TreeRNN over RNN. Firstly, as mentioned in Section 2, the average sequence length in TreeRNN is shorter compared to RNN, which makes the learning task easier. Secondly, as noted in other syntax-based language modeling work (Chelba and Jelinek, 2000; Roark, 2001; Chelba et al., 1997), in a tree-based model the context used to predict a word is more informative compared to a word-based model. For this reason, TreeRNN can more accurately capture important co-occurrences and transitions between words. With respect to the second question, a common trend in previous syntactic language modeling work was to manually engineer features representing tree structure, which may or may not have been optimal. In our model, feature engineering is delegated to the four RNNs which learn feature representations directly from data. Besides, the fact that these representations are continuous\n5Although not strictly comparable, Emami et al. (2003), obtain a perplexity of 131 on PTB using a structured language model (Chelba and Jelinek, 2000) and feed-forward neural networks for parameter estimation.\nmight also be advantageous."}, {"heading": "5.3 MSR Sentence Completion Challenge", "text": "We also applied our models to the MSR Sentence Completion Challenge dataset (Zweig and Burges, 2012). The challenge for a hypothetical system is to select the correct missing word for 1,040 SAT-style test sentences when presented with five candidate completions. The training set contains 522 novels from the Project Gutenberg which we preprocessed as follows. After removing the project Gutenberg headers and footers from the files, we tokenized and parse the dataset into dependency trees with the Stanford Core NLP toolkit (Manning et al., 2014). The resulting training set contains 49M words. We converted all words to lower case and replaced those occurring five times or less with UNK. The resulting vocabulary size was 65K. We randomly sampled 4,000 sentences from the training set as our validation set. We used our model to compute the probability of each test sentence. We then picked the candidate completion which produced the highest scoring sentence as our answer.\nSeveral techniques have been already benchmarked on the MSR sentence completion dataset (Zweig and Burges, 2012; Zweig et al., 2012; Mikolov, 2012; Mnih and Teh, 2012). A combination of recurrent neural networks and the skipgram model holds the state of the art achieving an accuracy of 58.9% (Mikolov et al., 2013a). The combination involves models trained on the original and filtered training data where frequent words are discarded. Models generally fare much worse than humans who achieve over 90% accuracy on this task (Zweig and Burges, 2012).\nTable 2 presents a summary of our results together with previously published results. The top of the table presents models which employ techniques other than neural networks. These include a modified Kneser-Ney 5-gram model (Mikolov, 2012), LSA (Zweig and Burges, 2012), and two variants of the structured language model presented in Gubbins and Vlachos (2013). The LSA-based approach performs dimensionality reduction on the training data to obtain a 300-dimensional representation of each word. To decide which option to select, the average similarity of the candidate to every other word in the sentence is computed and the word with the greatest overall similarity is selected. The models presented in Gubbins and Vlachos (2013) use\nmaximum likelihood to estimate the probability of words given labeled and unlabeled dependency paths (LDepNgram and UDepNgram in Table 2) in combination with backoff smoothing (Brants et al., 2007). These models and KN5 calculate the probabilities assigned to each candidate sentence and choose the completion with the highest scores.\nIn the middle of Table 2 we present neural language models using a hidden unit size of 300. The comparison includes Mikolov et al.\u2019s (2011a) recurrent neural network language model (RNN) and a variant which is jointly trained with a maximum entropy model with n-gram features (RNNME). The log-bilinear model (LBL; Mnih and Teh (2012)) first predicts the representation of the next word by linearly combining the representations of the context words; the distribution for the next word is computed based on the similarity between the predicted representation and the representations of all words in the vocabulary. As in Section 5.2 we report results with two variants of our model using unlabeled and labeled dependencies (see TreeRNN and LTreeRNN in Table 2). Since we are dealing with a large vocabulary, we apply noise contrastive estimation (NCE) to all our models. Moreover, we only use automatically predicted dependency trees (there are no gold parses for the MSR dataset).\nThe bottom of Table 2 compares larger models and model combinations. Specifically, the skip-gram model (Mikolov et al., 2013a) learns distributed representations for words using a loglinear classifier to predict which words will appear\nbefore and after the current word; it uses a hidden unit size of 640. We also present a labeled and unlabeled version of our model with a hidden unit size of 400 and their combination (U+LTreeRNN).\nAs can be seen from Table 2, the best performing individual model on this task is LBL. TreeRNN and LTreeRNN with 300 hidden units outperform KN5 and RNN, but fare slightly worse when compared to other models such as LSA, UDepNgram, and skip-gram. We conjecture that the hidden unit size is not big enough to adequately represent our training sample. Indeed, increasing the hidden unit size to 400 improves accuracy, at least for TreeRNN which is now close to LSA, and outperforms skip-gram and UDepNgram. We did not use any regularization technique during training and this may be the reason why the larger LTreeRNN (with 400 hidden units) did not outperform its smaller counterpart (with 300 hidden units). TreeRNN performs worse compared to RNNME (either with 300 or 400 units). In principle, our model can also be jointly trained with a ME model, however we leave this to future work. Finally, a combination of TreeRNN and LTreeRNN obtains accuracy superior to LSA, RNNME, UDepNgram, and comparable to LDepNgram. We combined TreeRNN and LTreeRNN by linearly interpolating the log probabilities they assigned to the candidate sentences. The interpolation weight was tuned on the development set. In sum, we observe that our model performs competitively against other neural language models modulo differences in datasets and training regimes. Also notice that no comparison model is able to perform tree generation of any kind."}, {"heading": "5.4 Tree Generation Evaluation", "text": "In this section we demonstrate our model\u2019s ability to generate dependency trees. We sampled depen-\ndency trees from the PTB and MSR datasets using an unlabeled model (TreeRNN) trained with 100 hidden units on the former corpus and 400 on the latter. In both cases we used noise contrastive estimation during training.\nAs explained in Section 4.5, we trained four binary classifiers (LEFT-EOG, RIGHT-EOG, NEXT-LEFT-EOG, and NEXT-RIGHT-EOG) to predict if a node can continue to generate given the current hidden state (1 for can and 0 for cannot). In our case each classifier was a two-layer neural network.6 The hidden unit size of the two layers was 100 when generating trees from the PTB and 400 when the trees were sampled from the MSR dataset. The accuracy of the individual classifiers on the test set is shown in Table 3. For comparison we also show the performance of a baseline which always predicts 0, i.e., cannot generate (see Base column in Table 3).\nOur classifiers outperform the baseline on both datasets, and achieve comparable accuracies despite being trained on corpora of different sizes, domains, and genres. Examples of generated trees from the PTB and the MSR models are shown in Figure 4. Trees (a)\u2013(c) were sampled from the PTB, whereas trees (d)\u2013(f) were sampled from the MSR corpus. As mentioned earlier, the model manages to capture the stylistic conventions of the training corpus. Sentences sampled from the MSR model tend to be more literary, whereas PTB sentences resemble the writing style of newspaper texts. Most of the dependencies in tree (a) in Figure 4 are correct, save package which is the\n6The networks were trained for 20 epochs each; we fixed the learning rate to 0.001 on PTB and 0.0003 on MSR.\ndependent of deliver rather than buy. Also note that deliver ought to have been a noun, (i.e., delivery) rather than a verb. Tree (b) is grammatical and the dependencies have also been correctly assigned. Tree (c) is an example of coordination; most dependencies have been identified correctly, except and which should have been the dependent of knows. MSR tree (d) is in direct speech and tree (e) in first person mimicking dialogue conventions in literary text. Tree (e) contains several dependency mistakes, especially in the subordinate clause when men were about to play. Finally, tree (f) is an example of a copula construction. The MSR model tends to end sentences with a full-stop followed by double quotation marks typically used by fiction writers to signal dialogue."}, {"heading": "6 Conclusions", "text": "In this paper we proposed a recurrent neural network model, which is is designed to predict a tree rather than a linear sequence. Experimental results on two language modeling tasks indicate that our model yields performance superior to conventional RNNs and is competitive to a range of neural language models some of which were specifically designed for the modeling tasks at hand. The ability of our model to generate dependency trees holds promise for text generation applications as well as for tasks operating over structural input. Although our experiments have focused exclusively on dependency trees, there is nothing inherent in our formulation that disallows its application to other types of tree structure (e.g., constituent trees or even taxonomies).\nRecently, RNNs with Long Short-Term Mem-\nory (LSTM) units (Hochreiter and Schmidhuber, 1997) have been applied to a variety of sequence modeling and prediction tasks demonstrating results superior to RNNs (Sak et al., 2014; Mikolov et al., 2014). Adapting our model so that it can work with LSTMs is an obvious next step. We would also like to extend TreeRNN to a bidirectional neural network (Schuster and Paliwal, 1997) which we believe would improve tree generation. For example, when generating the right dependents of a node, the information in the left dependents may be helpful and vice versa. Finally, we plan to use our model in a variety of applications such as sentence compression and simplification (Filippova et al., 2015), image description generation (Kiros et al., 2014; Vinyals et al., 2015), and notably machine translation (Sutskever et al., 2014; Cho et al., 2014)."}, {"heading": "Appendix A", "text": "Tables 4 and 5 show perplexity results (on the validation set) for RNN, TreeRNN, and LTreeRNN with varying hidden unit sizes. Results are reported on the PTB and APNews datasets."}, {"heading": "Appendix B", "text": "In this section demonstrate how we compute the number of parameters for the RNN and TreeRNN. We used the RNNLM toolkit (Mikolov et al.,\n2011c) with word classes which speeds up computation when working with large vocabularies. The number of parameters in the RNN is:\nNRNNpara. =r \u00d7 |V |+ r2 + r \u00d7Nclass + r \u00d7 |V | =2\u00d7 r \u00d7 |V |+ r2 + r \u00d7Nclass\nwhere r is the hidden unit size, |V | is the vocabulary size and Nclass is the number of classes for the output vocabulary (Nclass = 100 in all our experiments).\nThe Number of parameters in TreeRNN is:\nNTreeRNNpara. =s\u00d7 |V |+ 4\u00d7 s\u00d7 r + 4\u00d7 r2\n+ 4\u00d7 r \u00d7 |V | \u22644.5\u00d7 r \u00d7 |V |+ 6\u00d7 r2\nwhere s is the word embedding size and s \u2264 r2 (see Section 5.1). Therefore, NTreeRNNpara. \u2248 2.26 \u00d7 NRNNpara. , when |V | \u2208 {10K, 22K, 65K} and 100 \u2264 r \u2264 400."}], "references": [{"title": "Joint Language and Translation Modeling with Recurrent Neural Networks", "author": ["Auli et al.2013] Michael Auli", "Michel Galley", "Chris Quirk", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language", "citeRegEx": "Auli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2013}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Large language models in machine translation", "author": ["Ashok C. Popat", "Peng Xu", "Franz J. Och", "Jeffrey Dean"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Brants et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Brants et al\\.", "year": 2007}, {"title": "Structured language modeling", "author": ["Chelba", "Jelinek2000] Ciprian Chelba", "Frederick Jelinek"], "venue": "Computer Speech and Language,", "citeRegEx": "Chelba et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2000}, {"title": "Structure and performance of a dependency language model", "author": ["David Engle", "Frederick Jelinek", "Victor Jimenez", "Sanjeev Khudanpur", "Lidia Mangu", "Harry Printz", "Eric Ristad", "Ronald Rosenfeld", "Andreas Stolcke"], "venue": null, "citeRegEx": "Chelba et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 1997}, {"title": "Recurrent neural network language model training with noise contrastive estimation for speech recognition", "author": ["Chen et al.2015] X Chen", "X Liu", "MJF Gales", "PC Woodland"], "venue": "IEEE International Conference on Accoustics,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Using a connectionist model in a syntactical based language model", "author": ["Emami et al.2003] Ahmad Emami", "Peng Xu", "Frederick Jelinek"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Emami et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Emami et al\\.", "year": 2003}, {"title": "Sentence compression by deletion with lstms", "author": ["Enrique Alfonseca", "Carlos A. Colmenares", "Lukasz Kaiser", "Oriol Vinyals"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language", "citeRegEx": "Filippova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Filippova et al\\.", "year": 2015}, {"title": "Dependency language models for sentence completion", "author": ["Gubbins", "Vlachos2013] Joseph Gubbins", "Andreas Vlachos"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Gubbins et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gubbins et al\\.", "year": 2013}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["Gutmann", "Hyv\u00e4rinen2012] Michael U Gutmann", "Aapo Hyv\u00e4rinen"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Gutmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Vanishing gradient problem during learning recurrent neural nets and problem solutions", "author": ["Sepp Hochreiter"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-based Systems,", "citeRegEx": "Hochreiter.,? \\Q1998\\E", "shortCiteRegEx": "Hochreiter.", "year": 1998}, {"title": "Recurrent Continuous Translation Models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Multimodal neural language models", "author": ["Kiros et al.2014] Ryan Kiros", "Ruslan Salakhutdinov", "Rich Zemel"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "A cache based natural language model for speech recognition", "author": ["Kuhn", "de Mori1990] Roland Kuhn", "Renato de Mori"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Kuhn et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Kuhn et al\\.", "year": 1990}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J Bethard", "David McClosky"], "venue": "In Proceedings of 52nd Annual Meeting of the Association", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Building a large annotated corpus of english: the penn treebank", "author": ["Marcus et al.1993] Mitch Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Recurrent Neural Network based Language Model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In Proceedings of INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Empirical evaluation and combination", "author": ["Anoop Deoras", "Stefan Kombrink", "Lukas Burget", "Jan Cernock\u1ef3"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Strategies for Training Large Scale Neural Network Language Models", "author": ["Anoop Deoras", "Daniel Povey", "Lukas Burget", "Jan Cernocky"], "venue": "In Proceedings of ASRU", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "2011c. Extensions of Recurrent Neural Network Language Model", "author": ["Stefan Kombrink", "Lukas Burget", "JH Cernocky", "Sanjeev Khudanpur"], "venue": "In Proceedings of the 2011 IEEE International Conference on Acoustics,", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Proceedings of the 2013 International Conference on Learning Representations,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning longer memory in recurrent neural networks. arXiv preprint arXiv:1412.7753", "author": ["Armand Joulin", "Sumit Chopra", "Michael Mathieu", "Marc\u2019Aurelio Ranzato"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2014}, {"title": "Statistical Language Models based on", "author": ["Tomas Mikolov"], "venue": "Neural Networks. Ph.D. thesis, Brno University of Technology", "citeRegEx": "Mikolov.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov.", "year": 2012}, {"title": "Three new graphical models for statistical language modelling", "author": ["Mnih", "Hinton2007] Andriy Mnih", "Geoffrey Hinton"], "venue": "In Proceedings of the 24th International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2007}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Mnih", "Teh2012] Andriy Mnih", "Yee Whye Teh"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2012}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Recursive distributed representations", "author": ["Jordan B. Pollack"], "venue": "Artificial Intelligence,", "citeRegEx": "Pollack.,? \\Q1990\\E", "shortCiteRegEx": "Pollack.", "year": 1990}, {"title": "Robust Probabilistic Predictive Syntactic Processing: Motivations, Models, and Applications", "author": ["Brian Edward Roark"], "venue": "Ph.D. thesis,", "citeRegEx": "Roark.,? \\Q2001\\E", "shortCiteRegEx": "Roark.", "year": 2001}, {"title": "A maximum entropy approach to adaptive statistical language modeling", "author": ["Roni Rosenfeld"], "venue": "Computer Speech and Language,", "citeRegEx": "Rosenfeld.,? \\Q1996\\E", "shortCiteRegEx": "Rosenfeld.", "year": 1996}, {"title": "Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition", "author": ["Sak et al.2014] Ha\u015fim Sak", "Andrew Senior", "Fran\u00e7oise Beaufays"], "venue": "arXiv preprint arXiv:1402.1128", "citeRegEx": "Sak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "Bidirectional recurrent neural networks", "author": ["Schuster", "Paliwal1997] Mike Schuster", "Kuldip K Paliwal"], "venue": "Signal Processing, IEEE Transactions", "citeRegEx": "Schuster et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 1997}, {"title": "Modelling and optimizing on syntactic n-grams for statistical machine translation. Transactions of the Association for Computational Linguistics, 3:169\u2013182", "author": ["Rico Sennrich"], "venue": null, "citeRegEx": "Sennrich.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich.", "year": 2015}, {"title": "A new string-to-dependency machine translation algorithm with a target dependency language model", "author": ["Shen et al.2008] Libin Shen", "Jinxi Xu", "Ralph Weischedel"], "venue": "In Proceedings of ACL-08: HLT,", "citeRegEx": "Shen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2008}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Eric H. Huang", "Jeffrey Pennington", "Christopher D. Manning", "Andrew Ng"], "venue": "In Advances in Neural Information Processing Sys-", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Proceedings of the 2011 Conference", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Srilm-an extensible language modeling toolkit", "author": ["Andreas Stolcke"], "venue": "In Proceedings of the 7th International Conference on Spoken Language Processing,", "citeRegEx": "Stolcke,? \\Q2002\\E", "shortCiteRegEx": "Stolcke", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075v3", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["Yinggong Zhao", "Victoria Fossum", "David Chiang"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language", "citeRegEx": "Vaswani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "Show and tell: A neural image caption generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Recurrent neural networks for language understanding", "author": ["Yao et al.2013] Kaisheng Yao", "Geoffrey Zweig", "MeiYuh Hwang", "Yangyang Shi", "Dong Yu"], "venue": "In Interspeech,", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Chinese poetry generation with recurrent neural networks", "author": ["Zhang", "Lapata2014] Xingxing Zhang", "Mirella Lapata"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Structured language models for statistical machine translation", "author": ["Ying Zhang"], "venue": null, "citeRegEx": "Zhang.,? \\Q2009\\E", "shortCiteRegEx": "Zhang.", "year": 2009}, {"title": "A challenge set for advancing language modeling", "author": ["Zweig", "Burges2012] Geoffrey Zweig", "Chris J.C. Burges"], "venue": "In Proceedings of the NAACL-HLT", "citeRegEx": "Zweig et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zweig et al\\.", "year": 2012}, {"title": "Computational approaches to sentence completion", "author": ["Zweig et al.2012] Geoffrey Zweig", "John C. Platt", "Christopher Meek", "Christopher J.C. Burges", "Ainur Yessenalina", "Qiang Liu"], "venue": "In Proceedings of the 50th Annual Meeting of the Association", "citeRegEx": "Zweig et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zweig et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 32, "context": "The literature offers many examples of how to overcome this limitation, such as cache language models (Kuhn and de Mori, 1990), trigger models (Rosenfeld, 1996), and notably structured language models (Chelba et al.", "startOffset": 143, "endOffset": 160}, {"referenceID": 4, "context": "The literature offers many examples of how to overcome this limitation, such as cache language models (Kuhn and de Mori, 1990), trigger models (Rosenfeld, 1996), and notably structured language models (Chelba et al., 1997; Chelba and Jelinek, 2000; Roark, 2001).", "startOffset": 201, "endOffset": 261}, {"referenceID": 31, "context": "The literature offers many examples of how to overcome this limitation, such as cache language models (Kuhn and de Mori, 1990), trigger models (Rosenfeld, 1996), and notably structured language models (Chelba et al., 1997; Chelba and Jelinek, 2000; Roark, 2001).", "startOffset": 201, "endOffset": 261}, {"referenceID": 1, "context": "Previous approaches have mainly employed feed-forward (Bengio et al., 2003; Mnih and Hinton, 2007) and recurrent neural networks (Mikolov et al.", "startOffset": 54, "endOffset": 98}, {"referenceID": 19, "context": ", 2003; Mnih and Hinton, 2007) and recurrent neural networks (Mikolov et al., 2010; Mikolov et al., 2011c; Mikolov et al., 2011b; Auli et al., 2013; Mikolov, 2012) in order to map the feature vectors of the context words to the distribution for the next word.", "startOffset": 61, "endOffset": 163}, {"referenceID": 0, "context": ", 2003; Mnih and Hinton, 2007) and recurrent neural networks (Mikolov et al., 2010; Mikolov et al., 2011c; Mikolov et al., 2011b; Auli et al., 2013; Mikolov, 2012) in order to map the feature vectors of the context words to the distribution for the next word.", "startOffset": 61, "endOffset": 163}, {"referenceID": 26, "context": ", 2003; Mnih and Hinton, 2007) and recurrent neural networks (Mikolov et al., 2010; Mikolov et al., 2011c; Mikolov et al., 2011b; Auli et al., 2013; Mikolov, 2012) in order to map the feature vectors of the context words to the distribution for the next word.", "startOffset": 61, "endOffset": 163}, {"referenceID": 6, "context": "Despite superior performance in a many applications ranging from machine translation (Cho et al., 2014), to speech recognition (Mikolov et al.", "startOffset": 85, "endOffset": 103}, {"referenceID": 5, "context": ", 2014), to speech recognition (Mikolov et al., 2011c; Chen et al., 2015), image description generation (Vinyals et al.", "startOffset": 31, "endOffset": 73}, {"referenceID": 43, "context": ", 2015), image description generation (Vinyals et al., 2015), and language understanding (Yao et al.", "startOffset": 38, "endOffset": 60}, {"referenceID": 44, "context": ", 2015), and language understanding (Yao et al., 2013), standard neural language models essentially predict sequences of words.", "startOffset": 36, "endOffset": 54}, {"referenceID": 40, "context": ", performing machine translation (Sutskever et al., 2014; Cho et al., 2014) or image description (Kiros et al.", "startOffset": 33, "endOffset": 75}, {"referenceID": 6, "context": ", performing machine translation (Sutskever et al., 2014; Cho et al., 2014) or image description (Kiros et al.", "startOffset": 33, "endOffset": 75}, {"referenceID": 15, "context": ", 2014) or image description (Kiros et al., 2014; Vinyals et al., 2015).", "startOffset": 29, "endOffset": 71}, {"referenceID": 43, "context": ", 2014) or image description (Kiros et al., 2014; Vinyals et al., 2015).", "startOffset": 29, "endOffset": 71}, {"referenceID": 13, "context": "However, in practice it has proven difficult due to gradient exploding and gradient vanishing problems (Bengio et al., 1994; Hochreiter, 1998).", "startOffset": 103, "endOffset": 142}, {"referenceID": 30, "context": "Recursive Neural Networks (Pollack, 1990) are a related class of models which operate on structured inputs.", "startOffset": 26, "endOffset": 41}, {"referenceID": 41, "context": "The recently proposed tree-structured long shortterm memory network model (Tai et al., 2015) models sentential meaning whilst taking syntactic structure into account.", "startOffset": 74, "endOffset": 92}, {"referenceID": 36, "context": "Other work develops dependency-based language models for specific applications such as machine translation (Shen et al., 2008; Zhang, 2009; Sennrich, 2015) or sentence completion (Gubbins and Vlachos, 2013).", "startOffset": 107, "endOffset": 155}, {"referenceID": 46, "context": "Other work develops dependency-based language models for specific applications such as machine translation (Shen et al., 2008; Zhang, 2009; Sennrich, 2015) or sentence completion (Gubbins and Vlachos, 2013).", "startOffset": 107, "endOffset": 155}, {"referenceID": 35, "context": "Other work develops dependency-based language models for specific applications such as machine translation (Shen et al., 2008; Zhang, 2009; Sennrich, 2015) or sentence completion (Gubbins and Vlachos, 2013).", "startOffset": 107, "endOffset": 155}, {"referenceID": 1, "context": "(2003) and Sennrich (2015) estimate the parameters of a structured language model using feed-forward neural networks (Bengio et al., 2003).", "startOffset": 117, "endOffset": 138}, {"referenceID": 7, "context": "Emami et al. (2003) and Sennrich (2015) estimate the parameters of a structured language model using feed-forward neural networks (Bengio et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 7, "context": "Emami et al. (2003) and Sennrich (2015) estimate the parameters of a structured language model using feed-forward neural networks (Bengio et al.", "startOffset": 0, "endOffset": 40}, {"referenceID": 19, "context": "Each term in Equation (1) corresponds to the output of a recurrent neural network at one time step (Mikolov et al., 2010).", "startOffset": 99, "endOffset": 121}, {"referenceID": 7, "context": "Our current formulation uses basic recurrent neural networks (Elman, 1990) as a backbone.", "startOffset": 61, "endOffset": 74}, {"referenceID": 42, "context": "For largescale experiments, we employ NCE which does not require repeated summations over the whole vocabulary and has been previously shown to work well for neural language models (Mnih and Teh, 2012; Vaswani et al., 2013).", "startOffset": 181, "endOffset": 223}, {"referenceID": 19, "context": "Class-based methods (Mikolov et al., 2011c) and noise-contrastive estimation (NCE; Gutmann and Hyv\u00e4rinen (2012)) are often used to speedup training time.", "startOffset": 21, "endOffset": 112}, {"referenceID": 42, "context": "frequent than real words (Mnih and Teh, 2012; Vaswani et al., 2013).", "startOffset": 25, "endOffset": 67}, {"referenceID": 42, "context": "5 rather than learn it (Mnih and Teh, 2012; Vaswani et al., 2013; Chen et al., 2015), and use 20 negative samples.", "startOffset": 23, "endOffset": 84}, {"referenceID": 5, "context": "5 rather than learn it (Mnih and Teh, 2012; Vaswani et al., 2013; Chen et al., 2015), and use 20 negative samples.", "startOffset": 23, "endOffset": 84}, {"referenceID": 18, "context": "As in Mikolov et al. (2013b), we use smoothed unigram frequencies (exponentiating by 0.", "startOffset": 6, "endOffset": 29}, {"referenceID": 15, "context": "Previous work has shown that it is relatively straightforward to jointly train neural language models with other types of neural networks to perform tasks involving natural language generation such as translating sentences and generating descriptions for images (Kalchbrenner and Blunsom, 2013; Kiros et al., 2014; Zhang and Lapata, 2014; Sutskever et al., 2014; Cho et al., 2014; Vinyals et al., 2015).", "startOffset": 262, "endOffset": 402}, {"referenceID": 40, "context": "Previous work has shown that it is relatively straightforward to jointly train neural language models with other types of neural networks to perform tasks involving natural language generation such as translating sentences and generating descriptions for images (Kalchbrenner and Blunsom, 2013; Kiros et al., 2014; Zhang and Lapata, 2014; Sutskever et al., 2014; Cho et al., 2014; Vinyals et al., 2015).", "startOffset": 262, "endOffset": 402}, {"referenceID": 6, "context": "Previous work has shown that it is relatively straightforward to jointly train neural language models with other types of neural networks to perform tasks involving natural language generation such as translating sentences and generating descriptions for images (Kalchbrenner and Blunsom, 2013; Kiros et al., 2014; Zhang and Lapata, 2014; Sutskever et al., 2014; Cho et al., 2014; Vinyals et al., 2015).", "startOffset": 262, "endOffset": 402}, {"referenceID": 43, "context": "Previous work has shown that it is relatively straightforward to jointly train neural language models with other types of neural networks to perform tasks involving natural language generation such as translating sentences and generating descriptions for images (Kalchbrenner and Blunsom, 2013; Kiros et al., 2014; Zhang and Lapata, 2014; Sutskever et al., 2014; Cho et al., 2014; Vinyals et al., 2015).", "startOffset": 262, "endOffset": 402}, {"referenceID": 1, "context": "(1993)) and the APNews dataset (Bengio et al., 2003).", "startOffset": 31, "endOffset": 52}, {"referenceID": 17, "context": "2 we perform perplexity experiments on the well-known Penn Treebank (PTB; Marcus et al. (1993)) and the APNews dataset (Bengio et al.", "startOffset": 74, "endOffset": 95}, {"referenceID": 29, "context": "It is well known that RNNs suffer from problems of exploding gradients; as a result, we rescaled the gradient g when the gradient norm ||g|| > 5 and set g = 5g ||g|| (Pascanu et al., 2013; Sutskever et al., 2014).", "startOffset": 166, "endOffset": 212}, {"referenceID": 40, "context": "It is well known that RNNs suffer from problems of exploding gradients; as a result, we rescaled the gradient g when the gradient norm ||g|| > 5 and set g = 5g ||g|| (Pascanu et al., 2013; Sutskever et al., 2014).", "startOffset": 166, "endOffset": 212}, {"referenceID": 17, "context": "Specifically, we used the Stanford CoreNLP toolkit (Manning et al., 2014) to convert gold PTB phrase structure trees to dependencies.", "startOffset": 51, "endOffset": 73}, {"referenceID": 1, "context": "With regard to the APNews corpus (Bengio et al., 2003), the training set contains 14M words (sampled from the APW 1996 partition of the English Gigaword3), the validation set contains 1M words (sampled from APW 199501 to 199506) and the test set contains 1M words (sampled from APW 199507 to 199512).", "startOffset": 33, "endOffset": 54}, {"referenceID": 26, "context": "We generally observe that when models have access to more data, larger hidden units improve performance (Mikolov, 2012).", "startOffset": 104, "endOffset": 119}, {"referenceID": 31, "context": "Secondly, as noted in other syntax-based language modeling work (Chelba and Jelinek, 2000; Roark, 2001; Chelba et al., 1997), in a tree-based model the context used to predict a word is more informative compared to a word-based model.", "startOffset": 64, "endOffset": 124}, {"referenceID": 4, "context": "Secondly, as noted in other syntax-based language modeling work (Chelba and Jelinek, 2000; Roark, 2001; Chelba et al., 1997), in a tree-based model the context used to predict a word is more informative compared to a word-based model.", "startOffset": 64, "endOffset": 124}, {"referenceID": 8, "context": "Although not strictly comparable, Emami et al. (2003), obtain a perplexity of 131 on PTB using a structured language model (Chelba and Jelinek, 2000) and feed-forward neural networks for parameter estimation.", "startOffset": 34, "endOffset": 54}, {"referenceID": 17, "context": "After removing the project Gutenberg headers and footers from the files, we tokenized and parse the dataset into dependency trees with the Stanford Core NLP toolkit (Manning et al., 2014).", "startOffset": 165, "endOffset": 187}, {"referenceID": 47, "context": "Several techniques have been already benchmarked on the MSR sentence completion dataset (Zweig and Burges, 2012; Zweig et al., 2012; Mikolov, 2012; Mnih and Teh, 2012).", "startOffset": 88, "endOffset": 167}, {"referenceID": 26, "context": "Several techniques have been already benchmarked on the MSR sentence completion dataset (Zweig and Burges, 2012; Zweig et al., 2012; Mikolov, 2012; Mnih and Teh, 2012).", "startOffset": 88, "endOffset": 167}, {"referenceID": 26, "context": "These include a modified Kneser-Ney 5-gram model (Mikolov, 2012), LSA (Zweig and Burges, 2012), and two variants of the structured language model presented in Gubbins and Vlachos (2013).", "startOffset": 49, "endOffset": 64}, {"referenceID": 26, "context": "These include a modified Kneser-Ney 5-gram model (Mikolov, 2012), LSA (Zweig and Burges, 2012), and two variants of the structured language model presented in Gubbins and Vlachos (2013). The LSA-based approach performs dimensionality reduction on the training data to obtain a 300-dimensional representation of each word.", "startOffset": 50, "endOffset": 186}, {"referenceID": 26, "context": "These include a modified Kneser-Ney 5-gram model (Mikolov, 2012), LSA (Zweig and Burges, 2012), and two variants of the structured language model presented in Gubbins and Vlachos (2013). The LSA-based approach performs dimensionality reduction on the training data to obtain a 300-dimensional representation of each word. To decide which option to select, the average similarity of the candidate to every other word in the sentence is computed and the word with the greatest overall similarity is selected. The models presented in Gubbins and Vlachos (2013) use", "startOffset": 50, "endOffset": 558}, {"referenceID": 2, "context": "maximum likelihood to estimate the probability of words given labeled and unlabeled dependency paths (LDepNgram and UDepNgram in Table 2) in combination with backoff smoothing (Brants et al., 2007).", "startOffset": 176, "endOffset": 197}, {"referenceID": 19, "context": "The comparison includes Mikolov et al.\u2019s (2011a) recurrent neural network language model (RNN) and a variant which is jointly trained with a maximum entropy model with n-gram features (RNNME).", "startOffset": 24, "endOffset": 49}, {"referenceID": 19, "context": "The comparison includes Mikolov et al.\u2019s (2011a) recurrent neural network language model (RNN) and a variant which is jointly trained with a maximum entropy model with n-gram features (RNNME). The log-bilinear model (LBL; Mnih and Teh (2012)) first predicts the representation of the next word by linearly combining the representations of the context words; the distribution for the next word is computed based on the similarity between the predicted representation and the representations of all words in the vocabulary.", "startOffset": 24, "endOffset": 242}, {"referenceID": 33, "context": "ory (LSTM) units (Hochreiter and Schmidhuber, 1997) have been applied to a variety of sequence modeling and prediction tasks demonstrating results superior to RNNs (Sak et al., 2014; Mikolov et al., 2014).", "startOffset": 164, "endOffset": 204}, {"referenceID": 25, "context": "ory (LSTM) units (Hochreiter and Schmidhuber, 1997) have been applied to a variety of sequence modeling and prediction tasks demonstrating results superior to RNNs (Sak et al., 2014; Mikolov et al., 2014).", "startOffset": 164, "endOffset": 204}, {"referenceID": 9, "context": "Finally, we plan to use our model in a variety of applications such as sentence compression and simplification (Filippova et al., 2015), image description generation (Kiros et al.", "startOffset": 111, "endOffset": 135}, {"referenceID": 15, "context": ", 2015), image description generation (Kiros et al., 2014; Vinyals et al., 2015), and notably machine translation (Sutskever et al.", "startOffset": 38, "endOffset": 80}, {"referenceID": 43, "context": ", 2015), image description generation (Kiros et al., 2014; Vinyals et al., 2015), and notably machine translation (Sutskever et al.", "startOffset": 38, "endOffset": 80}, {"referenceID": 40, "context": ", 2015), and notably machine translation (Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 41, "endOffset": 83}, {"referenceID": 6, "context": ", 2015), and notably machine translation (Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 41, "endOffset": 83}], "year": 2015, "abstractText": "In this paper we develop a recurrent neural network (TreeRNN), which is designed to predict a tree rather than a linear sequence as is the case in conventional recurrent neural networks. Our model defines the probability of a sentence by estimating the generation probability of its dependency tree. We construct the tree incrementally by generating the left and right dependents of a node whose probability is computed using recurrent neural networks with shared hidden layers. Application of our model to two language modeling tasks shows that it outperforms or performs on par with related models.", "creator": "LaTeX with hyperref package"}}}