{"id": "1511.00213", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2015", "title": "Large-scale probabilistic predictors with and without guarantees of validity", "abstract": "This paper studies theoretically and empirically a method of turning machine-learning algorithms into probabilistic predictors that automatically enjoys a property of validity (perfect calibration) and is computationally efficient. The price to pay for perfect calibration is that these probabilistic predictors produce imprecise (in practice, almost precise for large data sets) probabilities. When these imprecise probabilities are merged into precise probabilities, the resulting predictors, while losing the theoretical property of perfect calibration, are consistently more accurate than the existing methods in empirical studies.\n\n\n\nHowever, these results are in no way new to the field. The results are highly speculative in nature, and they may not even offer much of a benefit. In fact, these results suggest that there is little new knowledge about the methods to make predictions. While this may be interesting, the results are a good one.\nAlthough these results are a promising paper, the paper is an interesting one. The paper is a first in a long series of fields that investigate artificial intelligence. Although it will be interesting to see if this paper can be replicated, I hope to keep it in the discussion as I have done before.\nFor more on the paper, see this blog post.", "histories": [["v1", "Sun, 1 Nov 2015 07:16:04 GMT  (118kb,D)", "https://arxiv.org/abs/1511.00213v1", "26 pages, 5 figures, to appear in Advances in Neural Information Processing Systems 28 (NIPS 2015)"], ["v2", "Fri, 13 Nov 2015 09:28:34 GMT  (193kb,D)", "http://arxiv.org/abs/1511.00213v2", "38 pages, 14 figures, to appear in Advances in Neural Information Processing Systems 28 (NIPS 2015). As compared with the previous version (v1), the MATLAB code (the 5 files with extension .m) and results of new empirical studies have been added"]], "COMMENTS": "26 pages, 5 figures, to appear in Advances in Neural Information Processing Systems 28 (NIPS 2015)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["vladimir vovk", "ivan petej", "valentina fedorova"], "accepted": true, "id": "1511.00213"}, "pdf": {"name": "1511.00213.pdf", "metadata": {"source": "CRF", "title": "Large-scale probabilistic prediction with and without validity guarantees", "authors": ["Vladimir Vovk", "Ivan Petej", "Valentina Fedorova"], "emails": [], "sections": [{"heading": null, "text": "The conference version of this paper is to appear in Advances in Neural Information Processing Systems 28, 2015."}, {"heading": "Contents", "text": ""}, {"heading": "1 Introduction 1", "text": ""}, {"heading": "2 Inductive Venn\u2013Abers predictors (IVAPs) 2", "text": ""}, {"heading": "3 Cross Venn\u2013Abers predictors (CVAPs) 10", "text": ""}, {"heading": "4 Making probability predictions out of multiprobability ones 10", "text": ""}, {"heading": "5 Comparison with other calibration methods 11", "text": "5.1 Platt\u2019s method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 5.2 Isotonic regression . . . . . . . . . . . . . . . . . . . . . . . . . . 14"}, {"heading": "6 Empirical studies 15", "text": ""}, {"heading": "7 Conclusion 34", "text": "References 34"}, {"heading": "1 Introduction", "text": "Prediction algorithms studied in this paper belong to the class of Venn\u2013Abers predictors, introduced in [19]. They are based on the method of isotonic regression [1] and prompted by the observation that when applied in machine learning the method of isotonic regression often produces miscalibrated probability predictions (see, e.g., [8, 9]); it has also been reported ([3], Section 1) that isotonic regression is more prone to overfitting than Platt\u2019s scaling [13] when data is scarce. The advantage of Venn\u2013Abers predictors is that they are a special case of Venn predictors ([18], Chapter 6), and so ([18], Theorem 6.6) are always wellcalibrated (cf. Proposition 1 below). They can be considered to be a regularized version of the procedure used by [20], which helps them resist overfitting.\nThe main desiderata for Venn (and related conformal, [18], Chapter 2) predictors are validity, predictive efficiency, and computational efficiency. This paper introduces two computationally efficient versions of Venn\u2013Abers predictors, which we refer to as inductive Venn\u2013Abers predictors (IVAPs) and cross-Venn\u2013 Abers predictors (CVAPs). The ways in which they achieve the three desiderata are:\n\u2022 Validity (in the form of perfect calibration) is satisfied by IVAPs automatically, and the experimental results reported in this paper suggest that it is inherited by CVAPs.\n\u2022 Predictive efficiency is determined by the predictive efficiency of the underlying learning algorithms (so that the full arsenal of methods of modern machine learning can be brought to bear on the prediction problem at hand).\n\u2022 Computational efficiency is, again, determined by the computational efficiency of the underlying algorithm; the computational overhead of extracting probabilistic predictions consists of sorting (which takes time O(n log n), where n is the number of observations) and other computations taking time O(n).\nAn advantage of Venn prediction over conformal prediction, which also enjoys validity guarantees, is that Venn predictors output probabilities rather than pvalues, and probabilities, in the spirit of Bayesian decision theory, can be easily combined with utilities to produce optimal decisions.\nIn Sections 2 and 3 we discuss IVAPs and CVAPs, respectively. Section 4 is devoted to minimax ways of merging imprecise probabilities into precise probabilities and thus making IVAPs and CVAPs precise probabilistic predictors.\nIn this paper we concentrate on binary classification problems, in which the objects to be classified are labelled as 0 or 1. Most of machine learning algorithms are scoring algorithms, in that they output a real-valued score for each test object, which is then compared with a threshold to arrive at a categorical prediction, 0 or 1. As precise probabilistic predictors, IVAPs and CVAPs are ways of converting the scores for test objects into numbers in the range [0, 1] that\ncan serve as probabilities, or calibrating the scores. In Section 5 we discuss two existing calibration methods, Platt\u2019s [13] and the method [20] based on isotonic regression, and compare them with IVAPs and CVAPs theoretically. Section 6 is devoted to experimental comparisons and shows that CVAPs consistently outperform the two existing methods."}, {"heading": "2 Inductive Venn\u2013Abers predictors (IVAPs)", "text": "In this paper we consider data sequences (usually loosely referred to as sets) consisting of observations z = (x, y), each observation consisting of an object x and a label y \u2208 {0, 1}; we only consider binary labels. We are given a training set whose size will be denoted l.\nThis section introduces inductive Venn\u2013Abers predictors. Our main concern is how to implement them efficiently, but as functions, an IVAP is defined in terms of a scoring algorithm (see the last paragraph of the previous section) as follows:\n\u2022 Divide the training set of size l into two subsets, the proper training set of size m and the calibration set of size k, so that l = m+ k.\n\u2022 Train the scoring algorithm on the proper training set.\n\u2022 Find the scores s1, . . . , sk of the calibration objects x1, . . . , xk.\n\u2022 When a new test object x arrives, compute its score s. Fit isotonic regression to (s1, y1), . . . , (sk, yk), (s, 0) obtaining a function f0. Fit isotonic regression to (s1, y1), . . . , (sk, yk), (s, 1) obtaining a function f1. The multiprobability prediction for the label y of x is the pair (p0, p1) := (f0(s), f1(s)) (intuitively, the prediction is that the probability that y = 1 is either f0(s) or f1(s)).\nNotice that the multiprobability prediction (p0, p1) output by an IVAP always satisfies p0 < p1, and so p0 and p1 can be interpreted as the lower and upper probabilities, respectively; in practice, they are close to each other for large training sets.\nFirst we state formally the property of validity of IVAPs (adapting the approach of [19] to IVAPs). A random variable P taking values in [0, 1] is perfectly calibrated (as a predictor) for a random variable Y taking values in {0, 1} if E(Y | P ) = P a.s. A selector is a random variable taking values in {0, 1}. As a general rule, in this paper random variables are denoted by capital letters (e.g., X are random objects and Y are random labels).\nProposition 1. Let (P0, P1) be an IVAP\u2019s prediction for X based on a training sequence (X1, Y1), . . . , (Xl, Yl). There is a selector S such that PS is perfectly calibrated for Y provided the random observations (X1, Y1), . . . , (Xl, Yl), (X,Y ) are i.i.d.\nOur next proposition concerns the computational efficiency of IVAPs; both propositions will be proved later in the section.\nProposition 2. Given the scores s1, . . . , sk of the calibration objects, the prediction rule for computing the IVAP\u2019s predictions can be computed in time O(k log k) and space O(k). Its application to each test object takes time O(log k). Given the sorted scores of the calibration objects, the prediction rule can be computed in time and space O(k).\nProofs of both statements rely on the geometric representation of isotonic regression as the slope of the GCM (greatest convex minorant) of the CSD (cumulative sum diagram): see [2], pages 9\u201313 (especially Theorem 1.1). To make our exposition more self-contained, we define both GCM and CSD below.\nFirst we explain how to fit isotonic regression to (s1, y1), . . . , (sk, yk) (without necessarily assuming that si are the calibration scores and yi are the calibration labels, which will be needed to cover the use of isotonic regression in IVAPs). We start from sorting all scores s1, . . . , sk in the increasing order and removing the duplicates. (This is the most computationally expensive step in our calibration procedure, O(k log k) in the worst case.) Let k\u2032 \u2264 k be the number of distinct elements among s1, . . . , sk, i.e., the cardinality of the set {s1, . . . , sk}. Define s\u2032j , j = 1, . . . , k\u2032, to be the jth smallest element of {s1, . . . , sk}, so that s\u20321 < s\u20322 < \u00b7 \u00b7 \u00b7 < s\u2032k\u2032 . Define wj :=\n\u2223\u2223{i = 1, . . . , k : si = s\u2032j}\u2223\u2223 to be the number of times s\u2032j occurs among s1, . . . , sk. Finally, define\ny\u2032j := 1\nwj \u2211 i=1,...,k:si=s\u2032j yi\nto be the average label corresponding to si = s\u2032j . The CSD of (s1, y1), . . . , (sk, yk) is the set of points\nPi :=  i\u2211 j=1 wj , i\u2211 j=1 y\u2032jwj  , i = 0, 1, . . . , k\u2032; (1) in particular, P0 = (0, 0). The GCM is the greatest convex minorant of the CSD. The value at s\u2032i, i = 1, . . . , k\u2032, of the isotonic regression fitted to (s1, y1), . . . , (sk, yk) is defined to be the slope of the GCM between \u2211i\u22121 j=1 wj\nand \u2211i j=1 wj ; the values at other s are somewhat arbitrary (namely, the value at s \u2208 (s\u2032i, s\u2032i+1) can be set to anything between the left and right slopes of the GCM at \u2211i j=1 wj) but are never needed in this paper (unlike in the standard use of isotonic regression in machine learning, [20]): e.g., f1(s) is the value of the isotonic regression fitted to a sequence that already contains (s, 1).\nProof of Proposition 1. Set S := Y . The statement of the proposition even holds conditionally on knowing the values of (X1, Y1), . . . , (Xm, Ym) and the multiset *(Xm+1, Ym+1), . . . , (Xl, Yl), (X,Y )+; this knowledge allows us to compute the scores *s1, . . . , sk, s+ of the calibration objects Xm+1, . . . , Xl and the\ntest object X. The only remaining randomness is over the equiprobable permutations of (Xm+1, Ym+1), . . . , (Xl, Yl), (X,Y ); in particular, (s, Y ) is drawn randomly from the multiset *(s1, Ym+1), . . . , (sk, Yl), (s, Y )+. It remains to notice that, according to the GCM construction, the average label of the calibration and test observations corresponding to a given value of PS is equal to PS .\nThe idea behind computing the pair (f0(s), f1(s)) efficiently is to precompute two vectors F 0 and F 1 storing f0(s) and f1(s), respectively, for all possible values of s. Let k\u2032 and s\u2032i be as defined above in the case where s1, . . . , sk are the calibration scores and y1, . . . , yk are the corresponding labels. The vectors F 0 and F 1 are of length k\u2032, and for all i = 1, . . . , k\u2032 and both \u2208 {0, 1}, F i is the value of f (s) when s = s\u2032i. Therefore, for all i = 1, . . . , k\u2032:\n\u2022 F 1i is also the value of f1(s) when s is just to the left of s\u2032i;\n\u2022 F 0i is also the value of f0(s) when s is just to the right of s\u2032i.\nSince f0 and f1 can change their values only at the points s\u2032i, the vectors F 0 and F 1 uniquely determine the functions f0 and f1, respectively.\nRemark. There are several algorithms for performing isotonic regression on a partially, rather than linearly, ordered set: see, e.g., [2], Section 2.3 (although one of the algorithms described in that section, the Minimax Order Algorithm, was later shown to be defective [10, 12]). Therefore, IVAPs (and CVAPs below) can be defined in the situation where scores take values only in a partially ordered set; moreover, Proposition 1 will continue to hold. (For the reader familiar with the notion of Venn predictors we could also add that Venn\u2013Abers predictors will continue to be Venn predictors, which follows from the isotonic regression being the average of the original function over certain equivalence classes.) The importance of partially ordered scores stems from the fact that they enable us to benefit from a possible \u201csynergy\u201d between two or more prediction algorithms [16]. Suppose, e.g., that one prediction algorithm outputs (scalar) scores s11, . . . , s1k for the calibration objects x1, . . . , xk and another outputs s21, . . . , s2k for the same calibration objects; we would like to use both sets of scores. We could merge the two sets of scores into composite vector scores, si := (s 1 i , s 2 i ), i = 1, . . . , k, and then classify a new object x as described earlier using its composite score s := (s1, s2), where s1 and s2 are the scalar scores computed by the two algorithms and the partial order between composite scores is defined as usual,\n(s1, s2) (t1, t2)\u21d0\u21d2 (s1 \u2264 t1) & (s2 \u2264 t2).\nPreliminary results reported in [16] in a related context suggest that the resulting predictor can outperform predictors based on the individual scalar scores. However, we will not pursue this idea further in this paper."}, {"heading": "Computational details of IVAPs", "text": "Let k\u2032, s\u2032i, and wi be as defined above in the case where s1, . . . , sk and y1, . . . , yk are the calibration scores and labels. The corners of a GCM are the points on the GCM where the slope of the GCM changes. It is clear that the corners belong to the CSD, and we also add the extreme points (P0 and Pk\u2032 in the case of (1)) of the CSD to the list of corners.\nWe will only explain in detail how to compute F 1; the computation of F 0 is analogous and will be explained only briefly. First we explain how to compute F 11 .\nExtend the CSD as defined above (in the case where s1, . . . , sk and y1, . . . , yk are the calibration scores and labels) by adding the point P\u22121 := (\u22121,\u22121). The corresponding GCM will be referred to as the initial GCM ; it has at most k\u2032+2 corners. Algorithm 1, which operates with a stack S (initially empty), computes the corners; it is a trivial modification of Graham\u2019s scan ([6]; [4], Section 33.3). The corners are returned on the stack S, and they are ordered from left to right (P\u22121 being at the bottom of S and Pk\u2032 at the top). The operator \u201cand\u201d in line 4 is, as usual, short circuiting. The expression \u201cthe angle formed by points a, b, and c makes a nonleft (resp. nonright) turn\u201d may be taken to mean that (b \u2212 a) \u00d7 (c \u2212 b) \u2264 0 (resp. \u2265 0), where \u00d7 stands for cross product of planar vectors; this avoids computing angles and divisions (see, e.g., [4], Section 33.1).\nAlgorithm 1 allows us to compute F 11 as the slope of the line between the two bottom corners in S, but this will be done by the next algorithm.\nThe rest of the procedure for computing the vector F 1 is shown as Algorithm 2. The main data structure in Algorithm 2 is a stack S\u2032, which is initialized (in lines 1\u20132) by putting in it all corners of the initial GCM in reverse order as compared with S (so that P\u22121 = (\u22121,\u22121) is initially at the top of S\u2032).\nAt each point in the execution of Algorithm 2 we will have a length-1 active interval and the active corner, which will nearly always be at the top of the stack S\u2032. The initial CSD can be visualized by connecting each pair of adjacent points: P\u22121 and P0, P0 and P1, etc. It stretches over the interval [\u22121, k\u2032] of the horizontal axis; the subinterval [\u22121, 0] corresponds to the test score s (assumed to be to the left of all s\u2032i) and each subinterval [\u2211i\u22121 j=1 wj , \u2211i j=1 wj ]\nAlgorithm 1 Initializing the corners for computing F 1\n1: Push(P\u22121, S) 2: Push(P0, S) 3: for i \u2208 {1, 2, . . . , k\u2032} 4: while S.size > 1 and the angle formed by points\nNext-To-Top(S), Top(S), and Pi makes a nonleft turn\n5: Pop(S) 6: Push(Pi, S) 7: return S\nAlgorithm 2 Computing F 1\n1: while \u00acStack-Empty(S) 2: Push(Pop(S), S\u2032) 3: for i \u2208 {1, 2, . . . , k\u2032} 4: set F 1i to the slope of\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 Top(S\u2032),Next-To-Top(S\u2032) 5: Pi\u22121 = Pi\u22122 + Pi \u2212 Pi\u22121 6: if Pi\u22121 is at or above \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 Top(S\u2032),Next-To-Top(S\u2032) 7: continue 8: Pop(S\u2032) 9: while S\u2032.size > 1 and the angle formed by points\nPi\u22121, Top(S\u2032), and Next-To-Top(S\u2032) makes a nonleft turn\n10: Pop(S\u2032)"}, {"heading": "11: Push(Pi\u22121, S\u2032)", "text": "12: return F 1\ncorresponds to the calibration score s\u2032i, i = 1, . . . , k\u2032. The active corner is initially at P\u22121 = (\u22121,\u22121); the corners to the left of the active corner are irrelevant and ignored (not remembered in S\u2032). The active interval is always between the first coordinate of Top(S\u2032) and the first coordinate of Next-To-Top(S\u2032). At each iteration i = 1, . . . , k\u2032 of the main loop 3\u201311 we are computing F 1i , i.e., f1(s) for the situation where s is between s\u2032i\u22121 and s\u2032i (meaning to the left of s\u20321 if i = 1), and after that we swap the active interval (corresponding to s) and the interval corresponding to s\u2032i; of course, after swapping pieces of CSD are adjusted vertically in order to make the CSD as a whole continuous.\nAt the beginning of each iteration i of the loop 3\u201311 we have the CSD\nP\u22121, P0, P1, . . . , Pk\u2032 (2)\ncorresponding to\nthe points s\u20321, . . . , s \u2032 i\u22121, s, s \u2032 i, s \u2032 i+1, . . . , s \u2032 k\u2032\nwith the weights w1, . . . , wi\u22121, 1, wi, wi+1, . . . , wk\u2032\n(respectively); the active interval is the projection of \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 Pi\u22122, Pi\u22121 (onto the horizontal axis, here and later). At the end of that iteration we have the CSD which looks identical to (2) but in fact contains a different point Pi\u22121 (cf. line 5 of the algorithm) and corresponds to\nthe points s\u20321, . . . , s \u2032 i\u22121, s \u2032 i, s, s \u2032 i+1, . . . , s \u2032 k\u2032\nwith the weights w1, . . . , wi\u22121, wi, 1, wi+1, . . . , wk\u2032\n(respectively); the active interval becomes the projection of \u2212\u2212\u2212\u2212\u2212\u2192 Pi\u22121, Pi. To achieve this, in line 5 we redefine Pi\u22121 to be the reflection of the old Pi\u22121 across the\nAlgorithm 3 Initializing the corners for computing F 0\n1: Push(Pk\u2032+1, S) 2: Push(Pk\u2032 , S) 3: for i \u2208 {k\u2032 \u2212 1, k\u2032 \u2212 2, . . . , 0} 4: while S.size > 1 and the angle formed by points Next-To-Top(S), Top(S), and Pi makes a nonright turn 5: Pop(S) 6: Push(Pi, S) 7: return S\nmid-point (Pi\u22122 + Pi)/2. The stack S\u2032 always consists of corners of the GCM of the current CSD, and it contains all the corners to the right of the active interval (plus one more corner, which is the active corner).\nAt each iteration i of the loop 3\u201311:\n\u2022 We report the slope of the GCM over the active interval as F 1i (line 4).\n\u2022 We then swap the fragments of the CSD corresponding to the active interval and to s\u2032i leaving the rest of the CSD intact. This way the active interval moves to the right (from the projection of \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 Pi\u22122, Pi\u22121 to the pro-\njection of \u2212\u2212\u2212\u2212\u2212\u2192 Pi\u22121, Pi).\n\u2022 If the point Pi\u22121 above the left end-point of the active interval is above (or at) the GCM, move to the next iteration of the loop. (The active corner does not change.) The rest of this description assumes that Pi\u22121 is strictly below.\n\u2022 Make Pi\u22121 the active corner. Redefine the GCM to the right of the active corner by connecting the active corner to the right-most corner C such that the slope of the line connecting the active corner and that corner is minimal; all the corners between the active corner and that right-most corner C are then forgotten.\nLemma 1. The worst-case computation time of Algorithms 1 and 2 is O(k\u2032).\nProof. In the case of Algorithm 1, see [4], Section 33.3. In the case of Algorithm 2, it suffices to notice that the total number of iterations for the while loop does not exceed the total number of elements pushed onto S\u2032 (since at each iteration we pop an element off S\u2032); and the total number of elements pushed onto S\u2032 is at most k\u2032 (in the first for loop) plus k\u2032 (in the second for loop).\nFor convenience of the reader wishing to program IVAPs and CVAPs, we also give the counterparts of Algorithms 1 and 2 for computing F 0: see Algorithms 3 and 4 below. In those algorithms, we do not need the point P\u22121 anymore; however, we need a new point Pk\u2032+1 := Pk\u2032 + (1, 1). The stacks S and S\u2032 that they use are initially empty.\nAlgorithm 4 Computing F 0\n1: while \u00acStack-Empty(S) 2: Push(Pop(S), S\u2032) 3: for i \u2208 {k\u2032, k\u2032 \u2212 1, . . . , 1} 4: set F 0i to the slope of \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 Top(S\u2032),Next-To-Top(S\u2032) 5: Pi = Pi\u22121 + Pi+1 \u2212 Pi 6: if Pi is at or above \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 Top(S\u2032),Next-To-Top(S\u2032) 7: continue 8: Pop(S\u2032) 9: while S\u2032.size > 1 and the angle formed by points Pi,\nTop(S\u2032), and Next-To-Top(S\u2032) makes a nonright turn 10: Pop(S\u2032) 11: Push(Pi, S\u2032) 12: return F 0\nAlternatively, we could use the algorithm for computing F 1 in order to compute F 0, since, for all i \u2208 {1, . . . , k\u2032},\nF 0i (s \u2032 1, . . . , s \u2032 k\u2032 , w1, . . . , wk\u2032 , y \u2032 1, . . . , y \u2032 k\u2032) = 1\u2212 F 1i ( \u2212s\u20321, . . . ,\u2212s\u2032k\u2032 , w1, . . . , wk\u2032 , 1\u2212 y\u20321, . . . , 1\u2212 y\u2032k\u2032 ) ,\nwhere the dependence on various parameters is made explicit. After computing F 0 and F 1 we can arrange the calibration scores s\u20321, . . . , s\u2032k\u2032 into a binary search tree: see Algorithm 5, where F 00 is defined to be 0 and F 1k\u2032+1 is defined to be 1; we will refer to s\u2032i as the keys of the corresponding nodes (only internal nodes will have keys). Algorithm 5 is in fact more general than what we need: it computes the binary search tree for the scores s\u2032a, s\u2032a+1, . . . , s\u2032b for a \u2264 b; therefore, we need to run BST(1, k\u2032). The size of the binary search tree is 2k\u2032 + 1; k\u2032 of its nodes are internal nodes corresponding to different values of s\u2032i, i = 1, . . . , k\u2032, and the other k\u2032 + 1 of its nodes are leaves corresponding to the k\u2032 + 1 intervals formed by the points s\u20321, . . . , s\u2032k\u2032 .\nOnce we have the binary search tree it is easy to compute the prediction for a test object x in time logarithmic in k\u2032: see Algorithm 6, which passes x through the tree and uses N to denote the current node. Formally, we give the test object x, the proper training set T \u2032, and the calibration set T \u2032\u2032 as the inputs of Algorithm 6; however, the algorithm uses for prediction the binary search tree built from T \u2032 and T \u2032\u2032, and the bulk of work is done in Algorithms 1\u20135.\nThe worst-case computational complexity of the overall procedure involves the following components:\n\u2022 Training the algorithm on the proper training set, computing the scores of the calibration objects, and computing the scores of the test objects; at this stage the computation time is determined by the underlying algorithm.\nAlgorithm 5 BST(a, b) (to create the binary search tree, run BST(1, k\u2032)) 1: if b = a 2: construct the binary tree\nwhose root has key s\u2032a and payload {F 0a , F 1a }, left child is a leaf with payload {F 0a\u22121, F 1a }, and right child is a leaf with payload {F 0a , F 1a+1}\n3: return its root 4: else if b = a+ 1 5: construct the binary tree\nwhose root has key s\u2032a and payload {F 0a , F 1a }, left child is a leaf with payload {F 0a\u22121, F 1a }, and right child is BST(b, b)\n6: return its root 7: else if 8: c = b(a+ b)/2c 9: construct the binary tree\nwhose root has key s\u2032c and payload {F 0c , F 1c }, left child is BST(a, c\u2212 1), and right child is BST(c+ 1, b)\n10: return its root\n\u2022 Sorting the scores of the calibration objects takes time O(k log k).\n\u2022 Running our procedure for pre-computing f0 and f1 takes time O(k) (by Lemma 1).\n\u2022 Processing each test object takes an additional time of O(log k) (using binary search).\nIn principle, using binary search does not require an explicit construction of a binary search tree (cf. [4], Exercise 2.3-5), but once we have a binary search tree we can easily transform it into a red-black tree, which allows us to add new observations to (and remove old observations from) the calibration set in time\nAlgorithm 6 IVAP(T \u2032, T \u2032\u2032, x) // inductive Venn\u2013Abers predictor 1: set N to the root of the binary search tree and compute the score s of x 2: while N is not a leaf 3: if s < key(N) 4: set N to N \u2019s left child 5: else if s > key(N) 6: set N to N \u2019s right child 7: else // if s = key(N) 8: return payload(N) 9: return payload(N)\nAlgorithm 7 CVAP(T, x) // cross-Venn\u2013Abers predictor for training set T 1: split the training set T into K folds T1, . . . , TK 2: for k \u2208 {1, . . . ,K} 3: (pk0 , p k 1) := IVAP(T \\ Tk, Tk, x)\n4: return GM(p1)/(GM(1\u2212 p0) + GM(p1))\nO(log k) ([4], Chapter 13)."}, {"heading": "3 Cross Venn\u2013Abers predictors (CVAPs)", "text": "A CVAP is just a combination of K IVAPs, where K is the parameter of the algorithm. It is described as Algorithm 7, where IVAP(A,B, x) stands for the output of IVAP applied to A as proper training set, B as calibration set, and x as test object, and GM stands for geometric mean (so that GM(p1) is the geometric mean of p11, . . . , pK1 and GM(1\u2212p0) is the geometric mean of 1\u2212p10, . . . , 1\u2212pK0 ). The folds should be of approximately equal size, and usually the training set is split into folds at random (although we choose contiguous folds in Section 6 to facilitate reproducibility). One way to obtain a random assignment of the training observations to folds (see line 1) is to start from a regular array in which the first l1 observations are assigned to fold 1, the following l2 observations are assigned to fold 2, up to the last lK observations which are assigned to fold K, where |lk \u2212 l/K| < 1 for all k, and then to apply a random permutation. Remember that the procedure Randomize-in-Place ([4], Section 5.3) can do the last step in timeO(l). See the next section for a justification of the expression GM(p1)/(GM(1\u2212 p0) + GM(p1)) used for merging the IVAPs\u2019 outputs."}, {"heading": "4 Making probability predictions out of multiprobability ones", "text": "In CVAP (Algorithm 7) we merge the K multiprobability predictions output by K IVAPs. In this section we design a minimax way for merging them, essentially following [19]. For the log-loss function the result is especially simple, GM(p1)/(GM(1\u2212 p0) + GM(p1)).\nRemark. Notice that the probability interval (1 \u2212GM(1 \u2212 p0),GM(p1)) (formally, a pair of numbers) is narrower than the corresponding interval for the arithmetic means; this follows from the fact that a geometric mean never exceeds the corresponding arithmetic mean and that we always have p0 < p1.\nLet us check that GM(p1)/(GM(1 \u2212 p0) + GM(p1)) is indeed the minimax expression under log loss. Suppose the pairs of lower and upper probabilities to be merged are (p10, p11), . . . , (pK0 , pK1 ) and the merged probability is p. The extra cumulative loss suffered by p over the correct members p11, . . . , pK1 of the pairs\nwhen the true label is 1 is\nlog p11 p + \u00b7 \u00b7 \u00b7+ log p K 1 p , (3)\nand the extra cumulative loss of p over the correct members of the pairs when the true label is 0 is\nlog 1\u2212 p10 1\u2212 p + \u00b7 \u00b7 \u00b7+ log 1\u2212 p K 0 1\u2212 p . (4)\nEqualizing the two expressions we obtain\np11 \u00b7 \u00b7 \u00b7 pK1 pK = (1\u2212 p10) \u00b7 \u00b7 \u00b7 (1\u2212 pK0 ) (1\u2212 p)K ,\nwhich gives the required minimax expression for the merged probability (since (3) is decreasing and (4) is increasing in p).\nIn the case of the Brier loss function, we solve the linear equation\n(1\u2212 p)2 \u2212 (1\u2212 p11)2 + \u00b7 \u00b7 \u00b7+ (1\u2212 p)2 \u2212 (1\u2212 pK1 )2 = p2 \u2212 (p10)2 + \u00b7 \u00b7 \u00b7+ p2 \u2212 (pK0 )2\nin p; the result is\np = 1\nK K\u2211 k=1 ( pk1 + 1 2 (pk0) 2 \u2212 1 2 (pk1) 2 ) .\nThis expression is more natural than it looks: see [19], the discussion after (11); notice that it reduces to arithmetic mean when p0 = p1.\nThe argument above (\u201cconditioned\u201d on the proper training set) is also applicable to IVAP, in which case we need to set K := 1; the probability predictor obtained from an IVAP by replacing (p0, p1) with p := p1/(1\u2212 p0 + p1) will be referred to as the log-minimax IVAP. (And CVAP is log-minimax by definition.)"}, {"heading": "5 Comparison with other calibration methods", "text": "The two alternative calibration methods that we consider in this paper are Platt\u2019s [13] and isotonic regression [20]."}, {"heading": "5.1 Platt\u2019s method", "text": "Platt\u2019s [13] method uses sigmoids\ng(s) := 1\n1 + exp(As+B) ,\nwhere A < 0 and B are parameters, to calibrate the scores. Platt discusses two approaches:\n\u2022 run the scoring algorithm and fit the parameters A and B on the full training set,\n\u2022 or run the scoring algorithm on a subset (called the proper training set in this paper) and fit A and B on the rest (the calibration set).\nPlatt recommends the second approach, especially that he is interested in SVM, and for SVM the scores for the training set tend to cluster around \u00b11. (In fact, this is also true for the calibration scores, as discussed below.)\nPlatt\u2019s recommended method of fitting A and B is\n\u2212 k\u2211 i=1 (ti log pi + (1\u2212 ti) log(1\u2212 pi))\u2192 min, (5)\nwhere, in the simplest case, ti := yi are the labels of the calibration observations (so that (5) minimizes the log loss on the calibration set). To obtain even better results, Platt recommends regularization:\nti = t+ := k+ + 1\nk+ + 2 (6)\nfor the calibration observations labelled 1 (if there are k+ of them) and\nti = t\u2212 := 1\nk\u2212 + 2 (7)\nfor the calibration observations labelled 0 (if there are k\u2212 of them). We can see from (6) and (7) that the predictions of Platt\u2019s predictor are always in the range (\n1 k\u2212 + 2 , k+ + 1 k+ + 2\n) . (8)\nLet us check that the predictions output by the log-minimax IVAP are in the same range as those for Platt\u2019s method (except that the end-points are now allowed):\nLemma 2. In the case of IVAP, p1 \u2265 1/(k\u2212 + 1) and p0 \u2264 1 \u2212 1/(k+ + 1), where k\u2212 and k+ are the numbers of positive and negative observations in the calibration set, respectively. In the case of log-minimax IVAP, p \u2208 [1/(k\u2212 + 2), 1 \u2212 1/(k+ + 2)] (i.e., p is in the closure of (8)). In the case of CVAP, p \u2208 [1/(k + 2), 1\u2212 1/(k + 2)], where k is the size of the largest fold.\nProof. The statement about IVAP is obvious, and we will only check that it implies the two other statements. For concreteness, we will consider the lower bounds. The lower bound 1/(k\u2212 + 2) for log-minimax IVAP can be deduced from p1 \u2265 1/(k\u2212 + 1) using the isotonicity of t/(c+ t) in t > 0 for c > 0:\np1 (1\u2212 p0) + p1 \u2265 1/(k\u2212 + 1) (1\u2212 p0) + 1/(k\u2212 + 1) \u2265 1/(k\u2212 + 1) 1 + 1/(k\u2212 + 1) = 1 k\u2212 + 2 .\nIn the same way the lower bound 1/(k + 2) for CVAP follows from GM(p1) \u2265 1/(k + 1):\nGM(p1) GM(1\u2212 p0) + GM(p1) \u2265 1/(k + 1) GM(1\u2212 p0) + 1/(k + 1) \u2265 1/(k + 1) 1 + 1/(k + 1) = 1 k + 2 .\nIt is clear that the end-points of the interval (8) can be approached arbitrarily closely in the case of Platt\u2019s predictor and attained in the case of IVAPs.\nThe main disadvantage of Platt\u2019s method is that the optimal calibration curve g is quite often far from being a sigmoid; and if the training set is very big, we will suffer, since in this case we can learn the best shape of the calibrator g. This is particularly serious in asymptotics as the amount of data tends to infinity.\nZhang [21] (Section 3.3) observes that in the case of SVM and universal [14] kernels the scores tend to cluster around \u00b11 at \u201cnon-trivial\u201d objects, i.e., objects that are labelled 1 with non-trivial (not close to 0 or 1) probability. This means that any sigmoid will be a poor calibrator unless the prediction problem is very easy. Formally, we have the following statement (a trivial corollary of known results), which uses the notation \u03b7(x) for the conditional probability that the label of an object x \u2208 X is 1 and assumes that the labels take values in {\u22121, 1}, yi \u2208 {\u22121, 1} (rather than yi \u2208 {0, 1}, as in the rest of this paper).\nProposition 3. Suppose that the probability of each of the events \u03b7(X) = 0, \u03b7(X) = 1/2, and \u03b7(X) = 1 is 0. Let fm be the SVM for a training set of size m, i.e., the solution to the optimization problem\nCm \u2016f\u20162H + m\u2211 i=1 \u03c6(f(xi)yi)\u2192 min, (9)\nwhere \u03c6(v) := (1\u2212 v)+ and H is a universal RKHS ([15], Definition 4.52). As m\u2192\u221e,\nfm(X)\u2192 f(X) := { \u22121 if \u03b7(X) \u2208 [0, 1/2] 1 if \u03b7(X) \u2208 (1/2, 1]\nin probability provided Cm \u2192\u221e and Cm = o(m).\nProof. This follows immediately from Theorem 4.4 in [21] for a natural class of universal kernels related to neural networks. In general, see the proof of Theorem 8.1 in [15].\nThe intuition behind the SVM decision values clustering around \u00b11 is very simple. SVM solves the optimization problem (9); asymptotically as m \u2192 \u221e and under natural assumptions (such as Cm \u2192\u221e and Cm = o(m)), this solves\nE\u03c6(f(X)Y )\u2192 min .\nWe can optimize separately for different values of \u03b7(x). Given \u03b7(x) = \u03b7\u2217, we have the optimization problem\n\u03b7\u2217\u03c6(f) + (1\u2212 \u03b7\u2217)\u03c6(\u2212f)\u2192 min,\nwhose solutions are\nf(x) \u2208  (\u2212\u221e,\u22121] if \u03b7(x) = 0 {\u22121} if \u03b7(x) \u2208 (0, 1/2) [\u22121, 1] if \u03b7(x) = 1/2 {1} if \u03b7(x) \u2208 (1/2, 1) [1,\u221e) if \u03b7(x) = 1.\nAssuming that the probability of each of the events \u03b7(X) = 0, \u03b7(X) = 1/2, and \u03b7(X) = 1 is 0, it is easy to check that asymptotically the best achievable excess log loss of a sigmoid over the Bayes algorithm is\nE ( KL (\u03b7 || E(\u03b7 | \u03b7 > 1/2))1\u03b7>1/2 +KL (\u03b7 || E(\u03b7 | \u03b7 < 1/2))1\u03b7<1/2 ) , (10)\nwhere KL is Kullback\u2013Leibler divergence defined in terms of base 2 logarithm log2, and the conditional expectation E(\u03b7 | E) is defined to be E(\u03b7 1E)/P(E).\nOn the other hand, there are no apparent obstacles to it approaching 0 in the case of isotonic regression, considered in the next subsection.\nFor illustration, suppose \u03b7 := \u03b7(X) is distributed uniformly in [0, 1]. It is easy to see that\nE(\u03b7 | \u03b7 > 1/2) = 3/4 E(\u03b7 | \u03b7 < 1/2) = 1/4;\ntherefore, the excess loss (10) is\nE ( KL (\u03b7 || 3/4)1\u03b7>1/2 +KL (\u03b7 || 1/4)1\u03b7<1/2 ) = E ( \u03b7 log2 \u03b7+(1\u2212\u03b7) log2(1\u2212\u03b7) ) + 2E ( \u03b71\u03b7>1/2 log2 4\n3 + \u03b71\u03b7<1/2 log2 4\n) \u2248 \u22120.7213 + 0.8113 = 0.09.\nWe can see that the Bayes log loss is 72.13%, whereas the best loss achievable by a sigmoid is 81.13%, 9 percentage points worse."}, {"heading": "5.2 Isotonic regression", "text": "There are two standard uses of isotonic regression: we can train the scoring algorithm using what we call a proper training set, and then use the scores of the observations in a disjoint calibration (also called validation) set for calibrating the scores of test objects (as in [3]); alternatively, we can train the scoring algorithm on the full training set and also use the full training set for calibration\n(it appears that this was done in [20]). In both cases, however, we can expect to get an infinite log loss when the test set becomes large enough. Indeed, suppose that we have fixed proper training and calibration sets (not necessarily disjoint, so that both cases mentioned above are covered) such that the score s(X) of a random object X is below the smallest score of the calibration objects with a positive probability; suppose also that the distribution of the label of a random observation is concentrated at 0 with probability zero. Under these realistic assumptions the probability that the average log loss on the test set is \u221e can be made arbitrarily close to one by making the size of the test set large enough: indeed, with a high probability there will be an observation (x, y) in the test set such that the score s(x) is below the smallest score of the calibration objects but y = 1; the log loss on such an observation will be infinite.\nThe presence of regularization is an advantage of Platt\u2019s method: e.g., it never suffers an infinite loss when using the log loss function. There is no standard method of regularization for isotonic regression, and we do not apply one1."}, {"heading": "6 Empirical studies", "text": "The main loss function (cf., e.g., [17]) that we use in our empirical studies is the log loss\n\u03bblog(p, y) := { \u2212 log p if y = 1 \u2212 log(1\u2212 p) if y = 0,\n(11)\nwhere log is binary logarithm, p \u2208 [0, 1] is a probability prediction, and y \u2208 {0, 1} is the true label. Another popular loss function is the Brier loss\n\u03bbBr(p, y) := 4(y \u2212 p)2. (12)\nWe choose the coefficient 4 in front of (y \u2212 p)2 in (12) and the base 2 of the logarithm in (11) in order for the minimax no-information predictor that always predicts p := 1/2 to suffer loss 1. An advantage of the Brier loss function is that it still makes it possible to compare the quality of prediction in cases when prediction algorithms (such as isotonic regression) give a categorical but wrong prediction (and so are simply regarded as infinitely bad when using log loss).\nThe loss of a probability predictor on a test set will be measured by the arithmetic average of the losses it suffers on the test set, namely, by the mean log loss (MLL) and the mean Brier loss (MBL)\nMLL := 1\nn n\u2211 i=1 \u03bblog(pi, yi), MBL := 1 n n\u2211 i=1 \u03bbBr(pi, yi), (13)\n1One of the reviewers of the conference version of this paper proposed complementing the calibration set used in isotonic regression by two dummy observations: one with score +\u221e and labelled by 0 and the other with score \u2212\u221e and labelled by 1.\nwhere yi are the test labels and pi are the probability predictions for them. We will not be checking directly whether various calibration methods produce wellcalibrated predictions, since it is well known that lack of calibration increases the loss as measured by loss functions such as log loss and Brier loss (see, e.g., [11] for the most standard decomposition of the latter into the sum of the calibration error and refinement error).\nIn this section we compare log-minimax IVAPs (i.e., IVAPs whose outputs are replaced by probability predictions, as explained in Section 4) and CVAPs with Platt\u2019s method [13] and the standard method [20] based on isotonic regression; the latter two will be referred to as \u201cPlatt\u201d and \u201cIsotonic\u201d in our tables and figures. (Even though for both IVAPs and CVAPs we use the log-minimax procedure for merging multiprobability predictions, the Brier-minimax procedure leads to virtually identical empirical results.) We use the same underlying algorithms as in [19], namely J48 decision trees (abbreviated to \u201cJ48\u201d), J48 decision trees with bagging (\u201cJ48 bagging\u201d), logistic regression (sometimes abbreviated to \u201clogistic\u201d), naive Bayes, neural networks, and support vector machines (SVM), as implemented in Weka [7] (University of Waikato, New Zealand). The underlying algorithms (except for SVM) produce scores in the interval [0, 1], which can be used directly as probability predictions (referred to as \u201cUnderlying\u201d in our tables and figures) or can be calibrated using the methods of [13, 20] or the methods proposed in this paper (\u201cIVAP\u201d or \u201cCVAP\u201d in the tables and figures).\nWe start our empirical studies with the adult data set available from the UCI repository [5] (this is the main data set used in [13] and one of the data sets used in [20]); however, as we will see later, the picture that we observe is typical for other data sets as well. We use the original split of the data set into a training set of Ntrain = 32, 561 observations and a test set of Ntest = 16, 281 observations. The results of applying the four calibration methods (plus the vacuous one, corresponding to just using the underlying algorithm) to the six underlying algorithms for this data set are shown in Figures 1 and 2. Figures 1 reports results for the log loss (namely, MLL, as defined in (13)) and Figure 2 for the Brier loss (namely, MBL). The underlying algorithms are given in the titles of the plots and the calibration methods are represented by different line styles, as explained in the legends. The marks on the horizontal axis are the ratios of the size of the proper training set to the size of the calibration set (except for the label all, which will be explained later); in the case of CVAPs, the number K of folds can be expressed as the sum of the two numbers forming the ratio (therefore, column 4:1 corresponds to the standard choice of 5 folds in the method of cross-validation). Missing curves or points on curves mean that the corresponding values either are too big and would squeeze unacceptably the interesting parts of the plot if shown or are infinite (such as many results for isotonic regression and neural networks under log loss). In the case of CVAPs, the training set is split into K equal (or as close to being equal as possible) contiguous folds: the first dNtrain/Ke training observations are included in the first fold, the next dNtrain/Ke (or bNtrain/Kc) in the second fold, etc. (first d\u00b7e and then b\u00b7c is used unless Ntrain is divisible by K). In the case of the other calibration methods, we used the first dK\u22121K Ntraine training observation as the\nproper training set (used for training the scoring algorithm) and the rest of the training observations are used as the calibration set.\nIn the case of log loss, isotonic regression often suffers infinite losses, which is indicated by the absence of the round marker for isotonic regression; e.g., only one of the log losses for SVM is finite. We are not trying to use ad hoc solutions, such as clipping predictions to the interval [ , 1\u2212 ] for a small > 0, since we are also using the bounded Brier loss function. The CVAP lines tend to be at the bottom in all plots; experiments with other data sets also confirm this.\nThe column all in the plots of Figures 1 and 2 refers to using the full training set as both the proper training set and calibration set. (In our official definition of IVAP we require that the last two sets be disjoint, but in this section we continue to refer to IVAPs modified in this way simply as IVAPs; in [19], such prediction algorithms were referred to as SVAPs, simplified Venn\u2013Abers predictors.) Using the full training set as both the proper training set and calibration set might appear naive (and is never used in the extensive empirical study [3]), but it often leads to good empirical results on larger data sets. However, it can also lead to very poor results, as in the case of \u201cJ48 bagging\u201d (for IVAP, Platt, and Isotonic), the underlying algorithm that achieves the best performance in Figures 1 and 2.\nA natural question is whether CVAPs perform better than the alternative calibration methods in Figures 1 and 2 (and our other experiments) because of applying cross-over (in moving from IVAP to CVAP) or because of the extra regularization used in IVAPs. The first reason is undoubtedly important for both loss functions and the second for the log loss function. The second reason plays a smaller role for Brier loss for relatively large data sets (in Figure 2 the curves for Isotonic and IVAP are very close to each other), but IVAPs are consistently better for smaller data sets even when using Brier loss. In Tables 1 and 2 we apply the four calibration methods and six underlying algorithms to a much smaller training set, namely to the first 5, 000 observations of the adult data set as the new training set, following [3]; the first 4, 000 training observations are used as the proper training set, the following 1, 000 training observations as the calibration set, and all other observations (the remaining training and all test observations) are used as the new test set. The results are shown in Tables 1 for log loss and 2 for Brier loss. They are consistently better for IVAP than for IR (isotonic regression). Results for nine very small data sets are given in Tables 1 and 2 of [19], where the results for IVAP (with the full training set used as both proper training and calibration sets, labelled \u201cSVA\u201d in the tables in [19]) are consistently (in 52 cases out of the 54 using Brier loss) better, usually significantly better, than for isotonic regression (referred to as DIR in the tables in [19]).\nThe following information might help the reader in reproducing our results (in addition to our code being posted on arXiv together with this paper). For each of the standard prediction algorithms within Weka that we use, we optimise the parameters by minimising the Brier loss on the calibration set, apart from the column labelled all. (We cannot use the log loss since it is often infinite in"}, {"heading": "Adult: log loss", "text": ""}, {"heading": "Adult: Brier loss", "text": "the case of isotonic regression.) We then use the trained algorithm to generate the scores for the calibration and test sets, which allows us to compute probability predictions using Platt\u2019s method, isotonic regression, IVAP, and CVAP. All the scores apart from SVM are already in the [0, 1] range and can be used as probability predictions. Most of the parameters are set to their default values, and the only parameters that are optimised are C (pruning confidence) for J48 and J48 bagging, R (ridge) for logistic regression, L (learning rate) and M (momentum) for neural networks (MultilayerPerceptron), and C (complexity constant) for SVM (SMO, with the linear kernel); naive Bayes does not involve any parameters. Notice that none of these parameters are \u201chyperparameters\u201d, in that they do not control the flexibility of the fitted prediction rule directly; this allows us to optimize the parameters on the training set for the all column. In the case of CVAPs, we optimise the parameters by minimising the cumulative Brier loss over all folds (so that the same parameters are used for all folds). To apply Platt\u2019s method to calibrate the scores generated by the underlying algorithms we use logistic regression, namely the function mnrfit within MATLAB\u2019s Statistics toolbox. For isotonic regression calibration we use the implementation of the PAVA in the R package fdrtool (namely, the function monoreg). Missing values are handled using the Weka filter ReplaceMissingValues, which replaces all missing values for nominal and numeric attributes with the modes and means from the training set."}, {"heading": "Additional experimental results", "text": "Figures 3 and 4 show our results for the covertype data set (available from the UCI repository [5] and also known as forest). In converting this multiclass classification problem to binary we follow [3]: treat the largest class as 1 and the rest as 0, and only consider a random and randomly permuted subset consisting of 30, 000 observations; the first 5000 of those observations are used as the training set and the remaining 25, 000 as the test set. The CVAP results are still at the bottom of the plots and very stable; and the values at the all column are still particularly unstable.\nSimilar results for the insurance, Bank Marketing, Spambase, and Statlog German Credit Data data sets are shown in Figures 5\u201312. The data sets are split into training and test sets in proportion 2:1, without randomization. Since the values for the all column are so unstable, the reader might prefer to disregard them in the case of IVAP, Platt, and Isotonic. In Figures 5\u201310 the CVAP results tend to be at the bottom of the plots. The Statlog German Credit Data data set is much more difficult, and all results in Figures 11\u201312 are poor and somewhat mixed; however, they still demonstrate that CVAPs and IVAPs produce stable results and avoid the occasional bad failures characteristic of the alternative calibration methods.\nAnd finally, Figures 13 and 14 show the results for log loss and Brier loss, respectively, for the adult data set and for a wide range of the ratios of the size of the proper training set to the calibration set. The left-most column of each plot is 1 : 9, which means, in the case of Platt\u2019s method, isotonic regression, and IVAPs, that 10% of the training set was allocated to the proper training set and the rest to the calibration set. In the case of CVAPs, 1 : 9 means that the training set was split into 10 folds, each of them in turn was used as the proper training set, and the rest were used as the calibration set; the results were merged using the minimax procedure as described in Section 4. In the case of the underlying algorithm, 1 : 9 means that only 10% of the training set was in fact used for training (the same 10% as for the first three calibration methods). The other columns are 1 : 8, 1 : 7,. . . , 1 : 2, 1 : 1 (which corresponds to 1 : 1 in Figures 1 and 2),. . . , 4 : 1 (which corresponds to 4 : 1 in Figures 1 and 2, i.e., to the standard procedure of 5-fold cross-validation), 5 : 1,. . . , 9 : 1 (the latter corresponds to the other standard cross-validation procedure, that of 10-fold cross-validation); the results in those columns are analogous to those in the column 1 : 9. In order not to duplicate the information we gave earlier for the adult data set, we give the results for a randomly permuted adult data set. There is not much difference between 5 and 10 folds for most underlying algorithms (logistic regression behaves unusually in that its performance deteriorates as the size of the proper training set increases, perhaps because less data are available for calibration)."}, {"heading": "Covertype: log loss", "text": ""}, {"heading": "Covertype: Brier loss", "text": ""}, {"heading": "Insurance: log loss", "text": ""}, {"heading": "Insurance: Brier loss", "text": ""}, {"heading": "Bank Marketing: log loss", "text": ""}, {"heading": "Bank Marketing: Brier loss", "text": ""}, {"heading": "Spambase: log loss", "text": ""}, {"heading": "Spambase: Brier loss", "text": ""}, {"heading": "Statlog (German Credit): log loss", "text": ""}, {"heading": "Statlog (German Credit): Brier loss", "text": ""}, {"heading": "Adult: log loss", "text": ""}, {"heading": "Adult: Brier loss", "text": ""}, {"heading": "7 Conclusion", "text": "This paper introduces two new computationally efficient algorithms for probabilistic prediction, IVAP, which can be regarded as a regularised form of the calibration method based on isotonic regression, and CVAP, which is built on top of IVAP using the idea of cross-validation. Whereas IVAPs are automatically perfectly calibrated, the advantage of CVAPs is in their good empirical performance.\nThis paper does not study empirically upper and lower probabilities produced by IVAPs and CVAPs, whereas the distance between them provides information about the reliability of the merged probability prediction. Finding interesting ways of using this extra information is one of the directions of further research."}, {"heading": "Acknowledgments", "text": "We are grateful to the conference reviewers for numerous helpful comments and observations, to Vladimir Vapnik for sharing his ideas about exploiting synergy between different learning algorithms, and to participants in the conference Machine Learning: Prospects and Applications (October 2015, Berlin) for their questions and comments. The first author has been partially supported by EPSRC (grant EP/K033344/1) and AFOSR (grant \u201cSemantic Completions\u201d). The second and third authors are grateful to their home institutions for funding their trips to Montre\u0301al to attend NIPS 2015."}], "references": [{"title": "An empirical distribution function for sampling with incomplete information", "author": ["Miriam Ayer", "H. Daniel Brunk", "George M. Ewing", "W.T. Reid", "Edward Silverman"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1955}, {"title": "Statistical Inference under Order Restrictions: The Theory and Application of Isotonic Regression", "author": ["Richard E. Barlow", "D.J. Bartholomew", "J.M. Bremner", "H. Daniel Brunk"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1972}, {"title": "An empirical comparison of supervised learning algorithms", "author": ["Rich Caruana", "Alexandru Niculescu-Mizil"], "venue": "In Proceedings of the Twenty Third International Conference on Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Introduction to Algorithms", "author": ["Thomas H. Cormen", "Charles E. Leiserson", "Ronald L. Rivest", "Clifford Stein"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "An efficient algorithm for determining the convex hull of a finite planar set", "author": ["Ronald L. Graham"], "venue": "Information Processing Letters,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1972}, {"title": "The WEKA data mining software: an update", "author": ["Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H. Witten"], "venue": "SIGKDD Explorations,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Smooth isotonic regression: a new method to calibrate predictive models", "author": ["Xiaoqian Jiang", "Melanie Osl", "Jihoon Kim", "Lucila Ohno-Machado"], "venue": "AMIA Summits on Translational Science Proceedings,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Reliable probability estimates based on support vector machines for large multiclass datasets", "author": ["Antonis Lambrou", "Harris Papadopoulos", "Ilia Nouretdinov", "Alex Gammerman"], "venue": "Proceedings of the AIAI 2012 Workshop on Conformal Prediction and its Applications,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "The Min-Max algorithm and isotonic regression", "author": ["Chu-In Charles Lee"], "venue": "Annals of Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1983}, {"title": "A new vector partition of the probability score", "author": ["Allan H. Murphy"], "venue": "Journal of Applied Meteorology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1973}, {"title": "Probabilities for SV machines", "author": ["John C. Platt"], "venue": "Advances in Large Margin Classifiers,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "On the influence of the kernel on the consistency of support vector machines", "author": ["Ingo Steinwart"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "Support Vector Machines", "author": ["Ingo Steinwart", "Andreas Christmann"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Intelligent learning: Similarity control and knowledge transfer", "author": ["Vladimir N. Vapnik"], "venue": "Talk at the 2015 Yandex School of Data Analysis Conference Machine Learning: Prospects and Applications,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "The fundamental nature of the log loss function", "author": ["Vladimir Vovk"], "venue": "editors, Fields of Logic and Computation II: Essays Dedicated to Yuri Gurevich on the Occasion of His 75th Birthday,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Algorithmic Learning in a Random World", "author": ["Vladimir Vovk", "Alex Gammerman", "Glenn Shafer"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Venn\u2013Abers predictors, On-line Compression Modelling project (New Series), http://alrw.net", "author": ["Vladimir Vovk", "Ivan Petej"], "venue": "Working Paper", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers", "author": ["Bianca Zadrozny", "Charles Elkan"], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "Statistical behavior and consistency of classification methods based on convex risk minimization", "author": ["Tong Zhang"], "venue": "Annals of Statistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}], "referenceMentions": [{"referenceID": 16, "context": "Prediction algorithms studied in this paper belong to the class of Venn\u2013Abers predictors, introduced in [19].", "startOffset": 104, "endOffset": 108}, {"referenceID": 0, "context": "They are based on the method of isotonic regression [1] and prompted by the observation that when applied in machine learning the method of isotonic regression often produces miscalibrated probability predictions (see, e.", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": ", [8, 9]); it has also been reported ([3], Section 1) that isotonic regression is more prone to overfitting than Platt\u2019s scaling [13] when data is scarce.", "startOffset": 2, "endOffset": 8}, {"referenceID": 7, "context": ", [8, 9]); it has also been reported ([3], Section 1) that isotonic regression is more prone to overfitting than Platt\u2019s scaling [13] when data is scarce.", "startOffset": 2, "endOffset": 8}, {"referenceID": 2, "context": ", [8, 9]); it has also been reported ([3], Section 1) that isotonic regression is more prone to overfitting than Platt\u2019s scaling [13] when data is scarce.", "startOffset": 38, "endOffset": 41}, {"referenceID": 10, "context": ", [8, 9]); it has also been reported ([3], Section 1) that isotonic regression is more prone to overfitting than Platt\u2019s scaling [13] when data is scarce.", "startOffset": 129, "endOffset": 133}, {"referenceID": 15, "context": "The advantage of Venn\u2013Abers predictors is that they are a special case of Venn predictors ([18], Chapter 6), and so ([18], Theorem 6.", "startOffset": 91, "endOffset": 95}, {"referenceID": 15, "context": "The advantage of Venn\u2013Abers predictors is that they are a special case of Venn predictors ([18], Chapter 6), and so ([18], Theorem 6.", "startOffset": 117, "endOffset": 121}, {"referenceID": 17, "context": "They can be considered to be a regularized version of the procedure used by [20], which helps them resist overfitting.", "startOffset": 76, "endOffset": 80}, {"referenceID": 15, "context": "The main desiderata for Venn (and related conformal, [18], Chapter 2) predictors are validity, predictive efficiency, and computational efficiency.", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "As precise probabilistic predictors, IVAPs and CVAPs are ways of converting the scores for test objects into numbers in the range [0, 1] that", "startOffset": 130, "endOffset": 136}, {"referenceID": 10, "context": "In Section 5 we discuss two existing calibration methods, Platt\u2019s [13] and the method [20] based on isotonic regression, and compare them with IVAPs and CVAPs theoretically.", "startOffset": 66, "endOffset": 70}, {"referenceID": 17, "context": "In Section 5 we discuss two existing calibration methods, Platt\u2019s [13] and the method [20] based on isotonic regression, and compare them with IVAPs and CVAPs theoretically.", "startOffset": 86, "endOffset": 90}, {"referenceID": 16, "context": "First we state formally the property of validity of IVAPs (adapting the approach of [19] to IVAPs).", "startOffset": 84, "endOffset": 88}, {"referenceID": 0, "context": "A random variable P taking values in [0, 1] is perfectly calibrated (as a predictor) for a random variable Y taking values in {0, 1} if E(Y | P ) = P a.", "startOffset": 37, "endOffset": 43}, {"referenceID": 1, "context": "Proofs of both statements rely on the geometric representation of isotonic regression as the slope of the GCM (greatest convex minorant) of the CSD (cumulative sum diagram): see [2], pages 9\u201313 (especially Theorem 1.", "startOffset": 178, "endOffset": 181}, {"referenceID": 17, "context": ", (sk, yk) is defined to be the slope of the GCM between \u2211i\u22121 j=1 wj and \u2211i j=1 wj ; the values at other s are somewhat arbitrary (namely, the value at s \u2208 (si, si+1) can be set to anything between the left and right slopes of the GCM at \u2211i j=1 wj) but are never needed in this paper (unlike in the standard use of isotonic regression in machine learning, [20]): e.", "startOffset": 356, "endOffset": 360}, {"referenceID": 1, "context": ", [2], Section 2.", "startOffset": 2, "endOffset": 5}, {"referenceID": 8, "context": "3 (although one of the algorithms described in that section, the Minimax Order Algorithm, was later shown to be defective [10, 12]).", "startOffset": 122, "endOffset": 130}, {"referenceID": 13, "context": ") The importance of partially ordered scores stems from the fact that they enable us to benefit from a possible \u201csynergy\u201d between two or more prediction algorithms [16].", "startOffset": 164, "endOffset": 168}, {"referenceID": 13, "context": "Preliminary results reported in [16] in a related context suggest that the resulting predictor can outperform predictors based on the individual scalar scores.", "startOffset": 32, "endOffset": 36}, {"referenceID": 4, "context": "Algorithm 1, which operates with a stack S (initially empty), computes the corners; it is a trivial modification of Graham\u2019s scan ([6]; [4], Section 33.", "startOffset": 131, "endOffset": 134}, {"referenceID": 3, "context": "Algorithm 1, which operates with a stack S (initially empty), computes the corners; it is a trivial modification of Graham\u2019s scan ([6]; [4], Section 33.", "startOffset": 136, "endOffset": 139}, {"referenceID": 3, "context": ", [4], Section 33.", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": "In the case of Algorithm 1, see [4], Section 33.", "startOffset": 32, "endOffset": 35}, {"referenceID": 3, "context": "[4], Exercise 2.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "O(log k) ([4], Chapter 13).", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "Remember that the procedure Randomize-in-Place ([4], Section 5.", "startOffset": 48, "endOffset": 51}, {"referenceID": 16, "context": "In this section we design a minimax way for merging them, essentially following [19].", "startOffset": 80, "endOffset": 84}, {"referenceID": 16, "context": "This expression is more natural than it looks: see [19], the discussion after (11); notice that it reduces to arithmetic mean when p0 = p1.", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "The two alternative calibration methods that we consider in this paper are Platt\u2019s [13] and isotonic regression [20].", "startOffset": 83, "endOffset": 87}, {"referenceID": 17, "context": "The two alternative calibration methods that we consider in this paper are Platt\u2019s [13] and isotonic regression [20].", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "1 Platt\u2019s method Platt\u2019s [13] method uses sigmoids", "startOffset": 25, "endOffset": 29}, {"referenceID": 18, "context": "Zhang [21] (Section 3.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "3) observes that in the case of SVM and universal [14] kernels the scores tend to cluster around \u00b11 at \u201cnon-trivial\u201d objects, i.", "startOffset": 50, "endOffset": 54}, {"referenceID": 12, "context": "where \u03c6(v) := (1\u2212 v) and H is a universal RKHS ([15], Definition 4.", "startOffset": 48, "endOffset": 52}, {"referenceID": 18, "context": "4 in [21] for a natural class of universal kernels related to neural networks.", "startOffset": 5, "endOffset": 9}, {"referenceID": 12, "context": "1 in [15].", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": "For illustration, suppose \u03b7 := \u03b7(X) is distributed uniformly in [0, 1].", "startOffset": 64, "endOffset": 70}, {"referenceID": 2, "context": "2 Isotonic regression There are two standard uses of isotonic regression: we can train the scoring algorithm using what we call a proper training set, and then use the scores of the observations in a disjoint calibration (also called validation) set for calibrating the scores of test objects (as in [3]); alternatively, we can train the scoring algorithm on the full training set and also use the full training set for calibration", "startOffset": 300, "endOffset": 303}, {"referenceID": 17, "context": "(it appears that this was done in [20]).", "startOffset": 34, "endOffset": 38}, {"referenceID": 14, "context": ", [17]) that we use in our empirical studies is the log loss", "startOffset": 2, "endOffset": 6}, {"referenceID": 0, "context": "where log is binary logarithm, p \u2208 [0, 1] is a probability prediction, and y \u2208 {0, 1} is the true label.", "startOffset": 35, "endOffset": 41}, {"referenceID": 9, "context": ", [11] for the most standard decomposition of the latter into the sum of the calibration error and refinement error).", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": ", IVAPs whose outputs are replaced by probability predictions, as explained in Section 4) and CVAPs with Platt\u2019s method [13] and the standard method [20] based on isotonic regression; the latter two will be referred to as \u201cPlatt\u201d and \u201cIsotonic\u201d in our tables and figures.", "startOffset": 120, "endOffset": 124}, {"referenceID": 17, "context": ", IVAPs whose outputs are replaced by probability predictions, as explained in Section 4) and CVAPs with Platt\u2019s method [13] and the standard method [20] based on isotonic regression; the latter two will be referred to as \u201cPlatt\u201d and \u201cIsotonic\u201d in our tables and figures.", "startOffset": 149, "endOffset": 153}, {"referenceID": 16, "context": ") We use the same underlying algorithms as in [19], namely J48 decision trees (abbreviated to \u201cJ48\u201d), J48 decision trees with bagging (\u201cJ48 bagging\u201d), logistic regression (sometimes abbreviated to \u201clogistic\u201d), naive Bayes, neural networks, and support vector machines (SVM), as implemented in Weka [7] (University of Waikato, New Zealand).", "startOffset": 46, "endOffset": 50}, {"referenceID": 5, "context": ") We use the same underlying algorithms as in [19], namely J48 decision trees (abbreviated to \u201cJ48\u201d), J48 decision trees with bagging (\u201cJ48 bagging\u201d), logistic regression (sometimes abbreviated to \u201clogistic\u201d), naive Bayes, neural networks, and support vector machines (SVM), as implemented in Weka [7] (University of Waikato, New Zealand).", "startOffset": 298, "endOffset": 301}, {"referenceID": 0, "context": "The underlying algorithms (except for SVM) produce scores in the interval [0, 1], which can be used directly as probability predictions (referred to as \u201cUnderlying\u201d in our tables and figures) or can be calibrated using the methods of [13, 20] or the methods proposed in this paper (\u201cIVAP\u201d or \u201cCVAP\u201d in the tables and figures).", "startOffset": 74, "endOffset": 80}, {"referenceID": 10, "context": "The underlying algorithms (except for SVM) produce scores in the interval [0, 1], which can be used directly as probability predictions (referred to as \u201cUnderlying\u201d in our tables and figures) or can be calibrated using the methods of [13, 20] or the methods proposed in this paper (\u201cIVAP\u201d or \u201cCVAP\u201d in the tables and figures).", "startOffset": 234, "endOffset": 242}, {"referenceID": 17, "context": "The underlying algorithms (except for SVM) produce scores in the interval [0, 1], which can be used directly as probability predictions (referred to as \u201cUnderlying\u201d in our tables and figures) or can be calibrated using the methods of [13, 20] or the methods proposed in this paper (\u201cIVAP\u201d or \u201cCVAP\u201d in the tables and figures).", "startOffset": 234, "endOffset": 242}, {"referenceID": 10, "context": "We start our empirical studies with the adult data set available from the UCI repository [5] (this is the main data set used in [13] and one of the data sets used in [20]); however, as we will see later, the picture that we observe is typical for other data sets as well.", "startOffset": 128, "endOffset": 132}, {"referenceID": 17, "context": "We start our empirical studies with the adult data set available from the UCI repository [5] (this is the main data set used in [13] and one of the data sets used in [20]); however, as we will see later, the picture that we observe is typical for other data sets as well.", "startOffset": 166, "endOffset": 170}, {"referenceID": 16, "context": "(In our official definition of IVAP we require that the last two sets be disjoint, but in this section we continue to refer to IVAPs modified in this way simply as IVAPs; in [19], such prediction algorithms were referred to as SVAPs, simplified Venn\u2013Abers predictors.", "startOffset": 174, "endOffset": 178}, {"referenceID": 2, "context": ") Using the full training set as both the proper training set and calibration set might appear naive (and is never used in the extensive empirical study [3]), but it often leads to good empirical results on larger data sets.", "startOffset": 153, "endOffset": 156}, {"referenceID": 2, "context": "In Tables 1 and 2 we apply the four calibration methods and six underlying algorithms to a much smaller training set, namely to the first 5, 000 observations of the adult data set as the new training set, following [3]; the first 4, 000 training observations are used as the proper training set, the following 1, 000 training observations as the calibration set, and all other observations (the remaining training and all test observations) are used as the new test set.", "startOffset": 215, "endOffset": 218}, {"referenceID": 16, "context": "Results for nine very small data sets are given in Tables 1 and 2 of [19], where the results for IVAP (with the full training set used as both proper training and calibration sets, labelled \u201cSVA\u201d in the tables in [19]) are consistently (in 52 cases out of the 54 using Brier loss) better, usually significantly better, than for isotonic regression (referred to as DIR in the tables in [19]).", "startOffset": 69, "endOffset": 73}, {"referenceID": 16, "context": "Results for nine very small data sets are given in Tables 1 and 2 of [19], where the results for IVAP (with the full training set used as both proper training and calibration sets, labelled \u201cSVA\u201d in the tables in [19]) are consistently (in 52 cases out of the 54 using Brier loss) better, usually significantly better, than for isotonic regression (referred to as DIR in the tables in [19]).", "startOffset": 213, "endOffset": 217}, {"referenceID": 16, "context": "Results for nine very small data sets are given in Tables 1 and 2 of [19], where the results for IVAP (with the full training set used as both proper training and calibration sets, labelled \u201cSVA\u201d in the tables in [19]) are consistently (in 52 cases out of the 54 using Brier loss) better, usually significantly better, than for isotonic regression (referred to as DIR in the tables in [19]).", "startOffset": 385, "endOffset": 389}, {"referenceID": 0, "context": "All the scores apart from SVM are already in the [0, 1] range and can be used as probability predictions.", "startOffset": 49, "endOffset": 55}, {"referenceID": 2, "context": "In converting this multiclass classification problem to binary we follow [3]: treat the largest class as 1 and the rest as 0, and only consider a random and randomly permuted subset consisting of 30, 000 observations; the first 5000 of those observations are used as the training set and the remaining 25, 000 as the test set.", "startOffset": 73, "endOffset": 76}], "year": 2015, "abstractText": "This paper studies theoretically and empirically a method of turning machinelearning algorithms into probabilistic predictors that automatically enjoys a property of validity (perfect calibration) and is computationally efficient. The price to pay for perfect calibration is that these probabilistic predictors produce imprecise (in practice, almost precise for large data sets) probabilities. When these imprecise probabilities are merged into precise probabilities, the resulting predictors, while losing the theoretical property of perfect calibration, are consistently more accurate than the existing methods in empirical studies. The conference version of this paper is to appear in Advances in Neural Information Processing Systems 28, 2015.", "creator": "LaTeX with hyperref package"}}}