{"id": "1511.00360", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2015", "title": "Automatic Prosody Prediction for Chinese Speech Synthesis using BLSTM-RNN and Embedding Features", "abstract": "Prosody affects the naturalness and intelligibility of speech. However, automatic prosody prediction from text for Chinese speech synthesis is still a great challenge and the traditional conditional random fields (CRF) based method always heavily relies on feature engineering. In this paper, we propose to use neural networks to predict prosodic boundary labels directly from Chinese characters without any feature engineering. Experimental results show that stacking feed-forward and bidirectional long short-term memory (BLSTM) recurrent network layers achieves superior performance over the CRF-based method. The embedding features learned from raw text further enhance the performance. The results provide additional insights into how the networks in question are used in future work.\n\n\n\n\n\n\nThe first step in the development of neural networks is to develop a single set of neural networks for our task-directed tasks, as shown in the first paragraph of this paper. After we apply a standard set of algorithms, we can identify the relevant patterns that lie on top of each other. This is a promising technique because it eliminates the time commitment involved in the tasks we do. The important component of a network-based approach is that it uses a number of features, like the number of characters on each page of a given page, the number of input points and how many characters in a given sentence span a given sentence, and the size of each sentence.\n\nThe following neural networks are implemented using the following algorithms:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\nLayers:\n", "histories": [["v1", "Mon, 2 Nov 2015 02:34:12 GMT  (657kb,D)", "http://arxiv.org/abs/1511.00360v1", "5 pages, 4 figures, ASRU 2015"]], "COMMENTS": "5 pages, 4 figures, ASRU 2015", "reviews": [], "SUBJECTS": "cs.CL cs.SD", "authors": ["chuang ding", "lei xie", "jie yan", "weini zhang", "yang liu"], "accepted": false, "id": "1511.00360"}, "pdf": {"name": "1511.00360.pdf", "metadata": {"source": "CRF", "title": "AUTOMATIC PROSODY PREDICTION FOR CHINESE SPEECH SYNTHESIS USING BLSTM-RNN AND EMBEDDING FEATURES", "authors": ["Chuang Ding", "Lei Xie", "Jie Yan", "Weini Zhang", "Yang Liu"], "emails": ["yangliu}@nwpu-aslp.org"], "sections": [{"heading": null, "text": "Index Terms\u2014 automatic prosody prediction, speech synthesis, neural network, BLSTM, embedding features"}, {"heading": "1. INTRODUCTION", "text": "Prosody refers to the rhythm, stress and intonation of speech, including variations in duration, loudness and pitch. It is well known that speech prosody plays an important perceptual role in human speech communication [1]. Specifically, perception of prosodic boundaries is essential for listeners. In Chinese speech synthesis systems, typical prosody boundary labels consist of prosodic word (PW), prosodic phrase (PPH) and intonational phrase (IPH), which construct a threelayer prosody structure tree [2], as shown in Fig. 1. The leaf nodes of tree structure are lexical words that can be derived from a lexical-based word segmentation module. Whether the prosody labels are properly predicted will directly affect the naturalness and intelligibility of the synthesized speech.\nPrevious studies have investigated a great number of features, their relevance to prosody generation in speech production and various prosodic modeling methods. Some syntactic cues like part-of-speech (POS), syllable identity, syllable stress and their contextual counterparts are commonly used for prosody boundary prediction [3, 4, 5]. Many statistical methods have been investigated to model speech prosody, including classification and regression tree [6], hidden Markov model [7], maximum entropy model [8] and conditional ran-\ndom fields (CRF) [9]. To our knowledge, the best reported results were achieved with CRF due to its ability of relaxing strong model independence assumption and solving the label bias problem [1, 10].\nDespite years of research, it is still a great challenge to predict correct prosodic labels from unrestricted text for a text-to-speech (TTS) system. Obviously, there are two major drawbacks of the CRF-based prosody prediction in Chinese speech synthesis. First, it heavily relies on the performances of Chinese word segmentation (CWS) and POS tagging [11]. Second, the particle size and the inevitable segmentation errors in CWS have negative effects on the subsequent prosodic boundary prediction task. Moreover, the choice of effective features, from a broad set of feature templates, is critical to the success of such systems [12]. Much of the effort goes into feature engineering, which is notoriously labor-intensive, mainly based on the experience of an annotator.\nRecently, deep neural networks (DNN) have been increasingly investigated in order to minimize the effort of feature engineering in sequential labeling tasks. Zheng et al. [12] applied neural networks to CWS and POS tagging and proposed a perceptron-style algorithm to speed up the training process with negligible loss in performance. Pei et al. [13] proposed a max-margin tensor neural network for CWS to model interactions between tags and context characters by exploiting tag embeddings and tensor-based transformation. These researches have proved that DNN is able to achieve similar or even superior performance over CRF-based method with minimal feature engineering in sequential labeling tasks. Therefore, it is promising to apply DNN architectures to automatic prosody prediction. However, we notice that the neural net-\nar X\niv :1\n51 1.\n00 36\n0v 1\n[ cs\n.C L\n] 2\nN ov\n2 01\nworks used in previous researches are feed-forward structures that keep the assumption of sample independence and provide only limited context modeling ability by operating on a fixedsize window of input samples. Instead, bidirectional recurrent neural networks (BRNN) are able to incorporate contextual information from both past and future inputs [14]. Specifically, BRNN with long short-term memory (LSTM) cells, namely BLSTM-RNN, has become a popular model [15].\nIn this paper, we address the prosodic boundary prediction problem using neural networks. There are three main contributions. (1) We propose a neural network approach to predict prosody labels directly from Chinese characters without any feature engineering. (2) We show that superior performance is achieved by stacking feed-forward and bidirectional long short-term memory (BLSTM) recurrent layers. (3) We leverage a large raw text corpus to obtain useful character embedding features. Both objective and subjective evaluations show that the proposed architecture achieves superior performance over the CRF-based method and the embedding features further enhance the performance."}, {"heading": "2. THE PROPOSED APPROACH", "text": "Just like CWS and POS tagging, automatic prosody prediction can be treated as a sequential labeling task that assigns boundary labels to characters of an input sentence. In order to make the prediction models less dependent on the feature engineering, we choose to use a variant of the neural network architecture proposed by [16] for probabilistic language model. This architecture was subsequently used for CWS and POS tagging [12]. As shown in Fig. 2, the architecture takes raw text as input and maps each Chinese character into a basic feature vector. The following layers are two types of neural networks, FFNN and BLSTM-RNN, used to discover multiple levels of feature representations from the basic feature vectors. The output layer is a graph over which tag inference is achieved by the Viterbi algorithm."}, {"heading": "2.1. Feature Vectors", "text": "The characters fed into network are transformed into feature vectors by a mapping operation. Typically, a character dictionary D of size |D| is extracted from the training set and unknown characters are mapped to a special symbol that is not used elsewhere. Each Chinese character can be typically represented by a one-hot vector, the size of which is |D|, and all dimensions are marked as 0 except the location of the character in D, which is marked as 1. However, the one-hot representation, with high dimensions, fails to model the semantic similarity between the ideographic characters. In contrast, the distributed representation or embedding feature, in form of a low dimensional continuous-valued vector learned using neural networks from raw text in a fully unsupervised way, is assumed to carry important syntactic and semantic information [17] [18]. Recently, Mansur et al. [19] have shown superior performance in Chinese word segmentation by the use of embedding features based on a neural language model [16]. Besides [16], Mikolov et al. [17] proposed a faster skip-gram model called word2vec1. As our preliminary experiments do not show much performance difference among various embedding features, we simply choose word2vec in this study because it can be trained much faster."}, {"heading": "2.2. Network Structures and Training", "text": "Two types of neural networks are investigated in this paper: FFNN and BLSTM-RNN. FFNN, trained with a backpropagation learning algorithm [20], is widely used in many practical applications. In a typical FFNN, every unit in a layer is connected with all the units in the previous layer, which takes in the output of the previous layer and computes a new set of non-linear activations for next layer. However, the assumption of sample independence brings in only limited context modeling ability.\nResearchers have proposed RNN to solve the limitation of FFNN. However, conventional RNN is only able to make use of previous context information. This is not accurate in modeling speech prosody that is highly related with both past and future contexts. Instead, bidirectional RNN can access both the preceding and succeeding input contexts with two separate hidden layers, which are then fed to the same output layer. The activation functionH of RNN is usually a sigmoid or hyperbolic tangent function, which often causes the gradient vanishing problem that prevents RNN from modeling the long-span relations in sequence features. An LSTM architecture, which uses purpose-built memory cells to store information, can overcome this problem and model longer contexts. Fig. 3 illustrates a single LSTM memory cell. For LSTM, H is implemented by the following functions:\nit = \u03c3(Wxixt +Whiht\u22121 +Wcict\u22121 + bi)\n1https://code.google.com/p/word2vec/\nft = \u03c3(Wxfxt +Whfht\u22121 +Wcfct\u22121 + bf )\nct = ftct\u22121 + ittanh(Wxcxt +Whcht\u22121 + bc)\not = \u03c3(Wxoxt +Whoht\u22121 +Wcoct + bo)\nht = ottanh(ct)\nwhere x = (x1, x2, ...xt..., xT ) is the input feature sequence, \u03c3 is the logistic function, and i, f , o and c are the input gate, forget gate, output gate and cell memory, respectively. W is the weight matrix and the subscript indicates it is the matrix between two different gates.\nBLSTM-RNN is a combination of LSTM and BRNN. Deep bidirectional LSTM-RNN can be established by stacking multiple BLSTM-RNN hidden layers on top of each other. The output sequence of one layer is used as the input sequence of the next layer. The hidden state sequences, hn, consist of forward and backward sequences \u2212\u2192 h n and \u2190\u2212 h n, iteratively computed from n = 1 to N and t = 1 to T as follows: \u2212\u2192 h nt = H(W\u2212\u2192h n\u22121\u2212\u2192h n \u2212\u2192 h n\u22121t +W\u2212\u2192h n\u2212\u2192h n \u2212\u2192 h nt\u22121 + b n\u2212\u2192 h ),\n\u2190\u2212 h nt = H(W\u2190\u2212h n\u22121\u2190\u2212h n \u2190\u2212 h n\u22121t +W\u2190\u2212h n\u2190\u2212h n \u2190\u2212 h nt\u22121 + b n\u2190\u2212 h ), yt =W\u2212\u2192hNy \u2212\u2192 h Nt +W\u2190\u2212hNy \u2190\u2212 h Nt + by.\nwhere y = (y1, y2, ...yt..., yT ) is the output prosodic boundary sequence.\nIn our study, the feed-forward layers are trained with typical backpropagation (BP) algorithm and the back-propagation through time (BPTT) method is used for training of BLSTM layers. BPTT is applied to both forward and backward hidden nodes and back-propagates layer by layer. The weight gradients are computed over the entire utterance [21]. The neural networks can be trained effectively in a layer-wised training manner, which makes it convenient to stack different types of neural network layers on top of each other to form a deep architecture. The deep architecture is able to build up progressively higher level representations of the input data, which is a crucial factor of the recent success of hybrid systems [22]."}, {"heading": "2.3. Tag Inference", "text": "To model the tag dependency and infer the tag sequence globally, given a set of tags G = {B,NB,O}, a transition score\nSab is introduced for jumping from tag a \u2208 G to tag b \u2208 G. For the input character sequence of a sentence c[1:T ] with a tag sequence tag[1:T ], a sentence-level score is then given by the sum of transition and network scores [12, 23]:\nl(c[1:T ], tag[1:T ], \u03b8) = T\u2211 t=1 (Stagt\u22121tagt + f\u03b8(tagt|ct))\nwhere f\u03b8(tagt|ct)) indicates the score output for tagt at the t-th character by the networks. Given a sentence c[1:T ], we can find the best tag path tag\u2217[1:T ] by maximizing the sentence score:\ntag\u2217[1:T ] = arg max\u2200l[1:T ] l(c[1:T ], tag[1:T ], \u03b8).\nThe Viterbi algorithm can be used for tag inference. The description above shows that it is easy to stack feature vectors, neural networks and tag inference together. Thus, the proposed architecture can be trained in a layer-wised fashion."}, {"heading": "3. EXPERIMENTS", "text": "Totally 48210 sentences randomly selected from People\u2019s Daily were used in our experiments. Prosodic boundaries (PW, PPH and IPH) were labelled by professional annotators with corresponding speech and labeling consistency is ensured. Word segmentation and POS tagging were carried out by a front-end preprocessing tool. The accuracy of word segmentation is 97% and the accuracy of POS tagging is 96%. The corpus was partitioned into three parts: a training set with 43390 utterances, a validation set with 2410 utterances for parameter tuning and a testing set with another 2410 utterances. A character dictionary D of size 4030 was extracted from the training set. A large set of raw texts was also collected from People\u2019s Daily for unsupervised embedding feature learning. All texts were preprocessed with text normalization.\nIn the experiments, PW, PPH and IPH were predicted separately. That is to say, three separate neural network models were trained independently for PW, PPH and IPH using the CURRENNT toolkit [24]. Each character in a sentence was assigned to one of the following three boundary tags: B for a prosodic boundary, NB for a non-boundary, and O for other symbols such as punctuation. Precision (P), recall (R) and F-score (F) were calculated as standard objective evaluation criteria.\nA CRF-based prosodic boundary prediction approach was used as baseline and boundary prediction (B, NB and O) was operated at word level. Atomic features in the CRF approach include word identity, POS tags, the length of word and the predicted tag from the previous boundary level. A linear statistical model was applied to optimize the feature templates. Parameters grid search was adopted to achieve the best performance of the CRF model. The CRF++ toolkit2 was used for the CRF-based prosodic boundary prediction. The baseline results are shown in Table 1.\nWe investigated the performance of neural network architecture with different topologies, as described in Table 2, where F and B denote a feed-forward layer and a BLSTM layer, respectively. The number of the nodes were kept the same for all hidden layers in every tested network architecture. Specifically, the network input is an M -dimensional feature vector, where M=4030 for the PW prediction and M=4031 for the PPH and IPH prediction 3. The network output corresponds to the three boundary tags (B, NB and O). All networks were trained with a momentum of 0.9, a learning rate of 1e-3 for PW and 1e-4 for PPH and IPH. BPTT was performed using stochastic gradient descent (SGD) with 32 parallel sentences. The training stops if no lower error on the validation set can be achieved within the last 10 epochs. The best performances for different prosodic boundary levels are shown in Table 3. We interestingly discover that the best performances at different levels are all obtained with a topology of FBB. When we compare Table 3 with the CRF-baseline Table 1, we find that the proposed neural network approach achieves competitive performance at the PW level and significant improvements at the PPH and IPH levels.\nWe also studied the effectiveness of the character embedding features. Different sizes of unsupervised training data (400M, 800M, 1200M, 1600M and 2000M text) and embedding feature sizes (100, 200, 300 and 400) were tested. The best network architectures, as shown in Table 3, were used in the experiments. Please note that the dimension of feature vector is greatly reduced as compared with the one-hot representation. The results shown in Table 4 indicate that the embedding features can further improve the performance of automatic prosodic boundary prediction.\nWe further conducted an A/B preference test on the naturalness of the synthesized speech. A set of 100 sentences were randomly selected from the test set and the prosodic boundary labels were achieved by:\n(1) CRF-based model in Table 1;\n(2) NN with one-hot representation in Table 3;\n(3) NN with embedding features in Table 4.\n2http://taku910.github.io/crfpp/ 3The predicted tag from the previous level was used as a feature.\nWe carried out two sessions of comparative evaluations: (1) vs (2) and (2) vs (3). A set of 20 sentence pairs of each session was randomly selected from the 100 pairs with different prosody prediction results and speech was generated through a typical HMM-based TTS system. A group of 10 subjects were asked to choose which one was better in terms of the naturalness of synthesis speech. The percentage preference is shown in Figure 4. We can clearly see that the NN architecture with one-hot representation can achieve better naturalness of synthesized speech as compared with CRF, while the use of embedding features further improves the natrualness."}, {"heading": "4. CONCLUSION AND FUTURE WORK", "text": "In this paper, we propose to use neural network architectures to predict prosodic boundary labels directly from Chinese characters without feature engineering. We show that superior performance is achieved by stacking feed-forward and bidirectional long short-term memory (BLSTM) recurrent layers. We obtain useful character embedding features from raw text. Both objective and subjective evaluations show that the proposed neural network approch achieves superior performance over the CRF-based approach and the use of embedding features can further boost the performance. For future work, it is promising to predict PW, PPH and IPH labels in a unified neural network and n-gram character embedding features can be further investigated."}, {"heading": "5. ACKNOWLEDGEMENTS", "text": "This work was supported by the National Natural Science Foundation of China (61175018 and 61571363)."}, {"heading": "6. REFERENCES", "text": "[1] Yao Qian, Zhizheng Wu, Xuezhe Ma, and Frank Soong, \u201cAutomatic prosody prediction and detection with conditional random field (crf) models,\u201d in Proceedings of ISCSLP, 2010, pp. 135\u2013138.\n[2] Jingwei Sun, Jing Yang, Jianping Zhang, and Yonghong Yan, \u201cChinese prosody structure prediction based on conditional random fields,\u201d in Proceedings of ICNC, 2009, vol. 3, pp. 602\u2013606.\n[3] Je Hun Jeon and Yang Liu, \u201cAutomatic prosodic events detection using syllable-based acoustic and syntactic features,\u201d in Proceedings of ICASSP, 2009, pp. 4565\u2013 4568.\n[4] Vivek Rangarajan, Shrikanth Narayanan, and Srinivas Bangalore, \u201cExploiting acoustic and syntactic features for prosody labeling in a maximum entropy framework,\u201d in Proceedings of NAACL HLT, 2007, pp. 1\u20138.\n[5] Philipp Koehn, Steven Abney, Julia Hirschberg, and Michael Collins, \u201cImproving intonational phrasing with syntactic information,\u201d in Proceedings of ICASSP, 2000, pp. 1289\u20131290.\n[6] Min Chu and Yao Qian, \u201cLocating boundaries for prosodic constituents in unrestricted mandarin texts,\u201d Computational linguistics and Chinese language processing, pp. 61\u201382, 2001.\n[7] Xin Nie and Zuo-ying Wang, \u201cAutomatic phrase break prediction in chinese sentences,\u201d Journal of Chinese information Processing, pp. 39\u201344, 2003.\n[8] Jian-Feng Li, Guoping Hu, and Ren-hua Wang, \u201cChinese prosody phrase break prediction based on maximum entropy model,\u201d in Proceedings of INTERSPEECH, 2004, pp. 729\u2013732.\n[9] Gina-Anne Levow, \u201cAutomatic prosodic labeling with conditional random fields and rich acoustic features,\u201d in Proceedings of IJCNLP, 2008, pp. 217\u2013224.\n[10] John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira, \u201cConditional random fields: Probabilistic models for segmenting and labeling sequence data,\u201d in Proceedings of 18th ICML, 2001, pp. 282\u2013289.\n[11] Zhao Sheng, Tao Jianhua, and Cai Lianhong, \u201cLearning rules for chinese prosodic phrase prediction,\u201d in Proceedings of ACL, 2002, pp. 1\u20137.\n[12] Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu, \u201cDeep learning for chinese word segmentation and pos tagging.,\u201d in Proceedings of EMNLP, 2013, pp. 647\u2013657.\n[13] Wenzhe Pei, Tao Ge, and Chang Baobao, \u201cMaxmargin tensor neural network for chinese word segmentation,\u201d in Proceedings of ACL, 2014, pp. 293\u2013303.\n[14] Mike Schuster and Kuldip K Paliwal, \u201cBidirectional recurrent neural networks,\u201d IEEE Transactions on Signal Processing, vol. 45, no. 11, pp. 2673\u20132681, 1997.\n[15] Sepp Hochreiter and Ju\u0308rgen Schmidhuber, \u201cLong shortterm memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[16] Yoshua Bengio, Re\u0301jean Ducharme, Pascal Vincent, and Christian Janvin, \u201cA neural probabilistic language model,\u201d The Journal of Machine Learning Research, vol. 3, pp. 1137\u20131155, 2003.\n[17] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean, \u201cDistributed representations of words and phrases and their compositionality,\u201d in Proceedings of NIPS, 2013, pp. 3111\u20133119.\n[18] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig, \u201cLinguistic regularities in continuous space word representations.,\u201d in Proceedings of HLT-NAACL, 2013, pp. 746\u2013751.\n[19] Mairgup Mansur, Wenzhe Pei, and Baobao Chang, \u201cFeature-based neural language model and chinese word segmentation,\u201d Proceedings of 6th IJCNLP, vol. 1, no. 2.3, pp. 2\u20133, 2013.\n[20] Shin-ichi Horikawa, Takeshi Furuhashi, and Yoshiki Uchikawa, \u201cOn fuzzy modeling using fuzzy neural networks with the back-propagation algorithm,\u201d IEEE transactions on Neural Networks, vol. 3, no. 5, pp. 801\u2013 806, 1992.\n[21] Ronald J Williams and David Zipser, \u201cGradient-based learning algorithms for recurrent networks and their computational complexity,\u201d Back-propagation: Theory, architectures and applications, pp. 433\u2013486, 1995.\n[22] Alex Graves, Navdeep Jaitly, and A-R Mohamed, \u201cHybrid speech recognition with deep bidirectional lstm,\u201d in Proceedings of ASRU, 2013, pp. 273\u2013278.\n[23] Ronan Collobert, Jason Weston, Le\u0301on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa, \u201cNatural language processing (almost) from scratch,\u201d The Journal of Machine Learning Research, vol. 12, pp. 2493\u2013 2537, 2011.\n[24] Felix Weninger, Johannes Bergmann, and Bjo\u0308rn Schuller, \u201cIntroducing currennt\u2013the munich opensource cuda recurrent neural network toolkit,\u201d Journal of Machine Learning Research, vol. 15, 2014."}], "references": [{"title": "Automatic prosody prediction and detection with conditional random field (crf) models", "author": ["Yao Qian", "Zhizheng Wu", "Xuezhe Ma", "Frank Soong"], "venue": "Proceedings of ISCSLP, 2010, pp. 135\u2013138.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Chinese prosody structure prediction based on conditional random fields", "author": ["Jingwei Sun", "Jing Yang", "Jianping Zhang", "Yonghong Yan"], "venue": "Proceedings of ICNC, 2009, vol. 3, pp. 602\u2013606.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Automatic prosodic events detection using syllable-based acoustic and syntactic features", "author": ["Je Hun Jeon", "Yang Liu"], "venue": "Proceedings of ICASSP, 2009, pp. 4565\u2013 4568.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Exploiting acoustic and syntactic features for prosody labeling in a maximum entropy framework", "author": ["Vivek Rangarajan", "Shrikanth Narayanan", "Srinivas Bangalore"], "venue": "Proceedings of NAACL HLT, 2007, pp. 1\u20138.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Improving intonational phrasing with syntactic information", "author": ["Philipp Koehn", "Steven Abney", "Julia Hirschberg", "Michael Collins"], "venue": "Proceedings of ICASSP, 2000, pp. 1289\u20131290.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Locating boundaries for prosodic constituents in unrestricted mandarin texts", "author": ["Min Chu", "Yao Qian"], "venue": "Computational linguistics and Chinese language processing, pp. 61\u201382, 2001.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Automatic phrase break prediction in chinese sentences", "author": ["Xin Nie", "Zuo-ying Wang"], "venue": "Journal of Chinese information Processing, pp. 39\u201344, 2003.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Chinese prosody phrase break prediction based on maximum entropy model", "author": ["Jian-Feng Li", "Guoping Hu", "Ren-hua Wang"], "venue": "Proceedings of INTER- SPEECH, 2004, pp. 729\u2013732.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Automatic prosodic labeling with conditional random fields and rich acoustic features", "author": ["Gina-Anne Levow"], "venue": "Proceedings of IJCNLP, 2008, pp. 217\u2013224.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira"], "venue": "Proceedings of 18th ICML, 2001, pp. 282\u2013289.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning rules for chinese prosodic phrase prediction", "author": ["Zhao Sheng", "Tao Jianhua", "Cai Lianhong"], "venue": "Proceedings of ACL, 2002, pp. 1\u20137.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "Deep learning for chinese word segmentation and pos tagging", "author": ["Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu"], "venue": "Proceedings of EMNLP, 2013, pp. 647\u2013657.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Maxmargin tensor neural network for chinese word segmentation", "author": ["Wenzhe Pei", "Tao Ge", "Chang Baobao"], "venue": "Proceedings of ACL, 2014, pp. 293\u2013303.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal"], "venue": "IEEE Transactions on Signal Processing, vol. 45, no. 11, pp. 2673\u20132681, 1997.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "Long shortterm memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1997}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "The Journal of Machine Learning Research, vol. 3, pp. 1137\u20131155, 2003.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "Proceedings of NIPS, 2013, pp. 3111\u20133119.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "Proceedings of HLT-NAACL, 2013, pp. 746\u2013751.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Feature-based neural language model and chinese word segmentation", "author": ["Mairgup Mansur", "Wenzhe Pei", "Baobao Chang"], "venue": "Proceedings of 6th IJCNLP, vol. 1, no. 2.3, pp. 2\u20133, 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "On fuzzy modeling using fuzzy neural networks with the back-propagation algorithm", "author": ["Shin-ichi Horikawa", "Takeshi Furuhashi", "Yoshiki Uchikawa"], "venue": "IEEE transactions on Neural Networks, vol. 3, no. 5, pp. 801\u2013 806, 1992.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1992}, {"title": "Gradient-based learning algorithms for recurrent networks and their computational complexity", "author": ["Ronald J Williams", "David Zipser"], "venue": "Back-propagation: Theory, architectures and applications, pp. 433\u2013486, 1995.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["Alex Graves", "Navdeep Jaitly", "A-R Mohamed"], "venue": "Proceedings of ASRU, 2013, pp. 273\u2013278.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 2493\u2013 2537, 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Introducing currennt\u2013the munich opensource cuda recurrent neural network toolkit", "author": ["Felix Weninger", "Johannes Bergmann", "Bj\u00f6rn Schuller"], "venue": "Journal of Machine Learning Research, vol. 15, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "It is well known that speech prosody plays an important perceptual role in human speech communication [1].", "startOffset": 102, "endOffset": 105}, {"referenceID": 1, "context": "In Chinese speech synthesis systems, typical prosody boundary labels consist of prosodic word (PW), prosodic phrase (PPH) and intonational phrase (IPH), which construct a threelayer prosody structure tree [2], as shown in Fig.", "startOffset": 205, "endOffset": 208}, {"referenceID": 2, "context": "Some syntactic cues like part-of-speech (POS), syllable identity, syllable stress and their contextual counterparts are commonly used for prosody boundary prediction [3, 4, 5].", "startOffset": 166, "endOffset": 175}, {"referenceID": 3, "context": "Some syntactic cues like part-of-speech (POS), syllable identity, syllable stress and their contextual counterparts are commonly used for prosody boundary prediction [3, 4, 5].", "startOffset": 166, "endOffset": 175}, {"referenceID": 4, "context": "Some syntactic cues like part-of-speech (POS), syllable identity, syllable stress and their contextual counterparts are commonly used for prosody boundary prediction [3, 4, 5].", "startOffset": 166, "endOffset": 175}, {"referenceID": 5, "context": "Many statistical methods have been investigated to model speech prosody, including classification and regression tree [6], hidden Markov model [7], maximum entropy model [8] and conditional ranIPH IPH", "startOffset": 118, "endOffset": 121}, {"referenceID": 6, "context": "Many statistical methods have been investigated to model speech prosody, including classification and regression tree [6], hidden Markov model [7], maximum entropy model [8] and conditional ranIPH IPH", "startOffset": 143, "endOffset": 146}, {"referenceID": 7, "context": "Many statistical methods have been investigated to model speech prosody, including classification and regression tree [6], hidden Markov model [7], maximum entropy model [8] and conditional ranIPH IPH", "startOffset": 170, "endOffset": 173}, {"referenceID": 8, "context": "dom fields (CRF) [9].", "startOffset": 17, "endOffset": 20}, {"referenceID": 0, "context": "To our knowledge, the best reported results were achieved with CRF due to its ability of relaxing strong model independence assumption and solving the label bias problem [1, 10].", "startOffset": 170, "endOffset": 177}, {"referenceID": 9, "context": "To our knowledge, the best reported results were achieved with CRF due to its ability of relaxing strong model independence assumption and solving the label bias problem [1, 10].", "startOffset": 170, "endOffset": 177}, {"referenceID": 10, "context": "First, it heavily relies on the performances of Chinese word segmentation (CWS) and POS tagging [11].", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "Moreover, the choice of effective features, from a broad set of feature templates, is critical to the success of such systems [12].", "startOffset": 126, "endOffset": 130}, {"referenceID": 11, "context": "[12] applied neural networks to CWS and POS tagging and proposed a perceptron-style algorithm to speed up the training process with negligible loss in performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] proposed a max-margin tensor neural network for CWS to model interactions between tags and context characters by exploiting tag embeddings and tensor-based transformation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Instead, bidirectional recurrent neural networks (BRNN) are able to incorporate contextual information from both past and future inputs [14].", "startOffset": 136, "endOffset": 140}, {"referenceID": 14, "context": "Specifically, BRNN with long short-term memory (LSTM) cells, namely BLSTM-RNN, has become a popular model [15].", "startOffset": 106, "endOffset": 110}, {"referenceID": 15, "context": "In order to make the prediction models less dependent on the feature engineering, we choose to use a variant of the neural network architecture proposed by [16] for probabilistic language model.", "startOffset": 156, "endOffset": 160}, {"referenceID": 11, "context": "This architecture was subsequently used for CWS and POS tagging [12].", "startOffset": 64, "endOffset": 68}, {"referenceID": 16, "context": "In contrast, the distributed representation or embedding feature, in form of a low dimensional continuous-valued vector learned using neural networks from raw text in a fully unsupervised way, is assumed to carry important syntactic and semantic information [17] [18].", "startOffset": 258, "endOffset": 262}, {"referenceID": 17, "context": "In contrast, the distributed representation or embedding feature, in form of a low dimensional continuous-valued vector learned using neural networks from raw text in a fully unsupervised way, is assumed to carry important syntactic and semantic information [17] [18].", "startOffset": 263, "endOffset": 267}, {"referenceID": 18, "context": "[19] have shown superior performance in Chinese word segmentation by the use of embedding features based on a neural language model [16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[19] have shown superior performance in Chinese word segmentation by the use of embedding features based on a neural language model [16].", "startOffset": 132, "endOffset": 136}, {"referenceID": 15, "context": "Besides [16], Mikolov et al.", "startOffset": 8, "endOffset": 12}, {"referenceID": 16, "context": "[17] proposed a faster skip-gram model called word2vec1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "FFNN, trained with a backpropagation learning algorithm [20], is widely used in many practical applications.", "startOffset": 56, "endOffset": 60}, {"referenceID": 20, "context": "The weight gradients are computed over the entire utterance [21].", "startOffset": 60, "endOffset": 64}, {"referenceID": 21, "context": "The deep architecture is able to build up progressively higher level representations of the input data, which is a crucial factor of the recent success of hybrid systems [22].", "startOffset": 170, "endOffset": 174}, {"referenceID": 11, "context": "For the input character sequence of a sentence c[1:T ] with a tag sequence tag[1:T ], a sentence-level score is then given by the sum of transition and network scores [12, 23]:", "startOffset": 167, "endOffset": 175}, {"referenceID": 22, "context": "For the input character sequence of a sentence c[1:T ] with a tag sequence tag[1:T ], a sentence-level score is then given by the sum of transition and network scores [12, 23]:", "startOffset": 167, "endOffset": 175}, {"referenceID": 23, "context": "That is to say, three separate neural network models were trained independently for PW, PPH and IPH using the CURRENNT toolkit [24].", "startOffset": 127, "endOffset": 131}], "year": 2015, "abstractText": "Prosody affects the naturalness and intelligibility of speech. However, automatic prosody prediction from text for Chinese speech synthesis is still a great challenge and the traditional conditional random fields (CRF) based method always heavily relies on feature engineering. In this paper, we propose to use neural networks to predict prosodic boundary labels directly from Chinese characters without any feature engineering. Experimental results show that stacking feed-forward and bidirectional long short-term memory (BLSTM) recurrent network layers achieves superior performance over the CRF-based method. The embedding features learned from raw text further enhance the performance.", "creator": "LaTeX with hyperref package"}}}