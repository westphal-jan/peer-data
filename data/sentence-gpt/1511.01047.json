{"id": "1511.01047", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2015", "title": "Detecting Clusters of Anomalies on Low-Dimensional Feature Subsets with Application to Network Traffic Flow Data", "abstract": "In a variety of applications, one desires to detect groups of anomalous data samples, with a group potentially manifesting its atypicality (relative to a reference model) on a low-dimensional subset of the full measured set of features. Samples may only be weakly atypical individually, whereas they may be strongly atypical when considered jointly. What makes this group anomaly detection problem quite challenging is that it is a priori unknown which subset of features jointly manifests a particular group of anomalies, so if there are other members of an anomaly that is unique to the group, then it would be difficult to test for its uniqueness in a single dataset. Such an approach is often described as \"a generalization of a non-group analysis of anomalies and to consider any possible alternative methods\". In an open-source application we are currently testing the ability of one subset of anomalous data samples to identify more than one anomalous dataset with one feature. However, a majority of results are not available in this regard. A group of anomalous data samples that appear to share unique characteristics and characteristics and may also contain a subset of anomalous data samples with a single one feature is likely to be useful. To see how this works, consider using this approach to assess how anomalous data samples are identified in the same dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 10 Jun 2015 15:13:59 GMT  (85kb)", "http://arxiv.org/abs/1511.01047v1", null]], "reviews": [], "SUBJECTS": "cs.NI cs.CR cs.LG", "authors": ["zhicong qiu", "david j miller", "george kesidis"], "accepted": false, "id": "1511.01047"}, "pdf": {"name": "1511.01047.pdf", "metadata": {"source": "CRF", "title": "Detecting Clusters of Anomalies on Low-Dimensional Feature Subsets with Application to Network Traffic Flow Data", "authors": ["Zhicong Qiu", "David J. Miller"], "emails": ["zzq101@psu.edu", "djm25@psu.edu", "gik2@psu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n01 04\n7v 1\n[ cs\n.N I]\nIn a variety of applications, one desires to detect groups of anomalous data samples, with a group potentially manifesting its atypicality (relative to a reference model) on a low-dimensional subset of the full measured set of features. Samples may only be weakly atypical individually, whereas they may be strongly atypical when considered jointly. What makes this group anomaly detection problem quite challenging is that it is a priori unknown which subset of features jointly manifests a particular group of anomalies. Moreover, it is unknown how many anomalous groups are present in a given data batch. In this work, we develop a group anomaly detection (GAD) scheme to identify the subset of samples and subset of features that jointly specify an anomalous cluster. We apply our approach to network intrusion detection to detect BotNet and peer-to-peer flow clusters. Unlike previous studies, our approach captures and exploits statistical dependencies that may exist between the measured features. Experiments on real world network traffic data demonstrate the advantage of our proposed system, and highlight the importance of exploiting feature dependency structure, compared to the feature (or test) independence assumption made in previous studies.\nIndex Terms\nBonferroni correction, group anomaly detection, Gaussian Mixture Model, p-value, network intru-\nsion detection, BotNet, dependence tree\nThis work supported in part by a Cisco Systems URP gift.\nNovember 4, 2015 DRAFT\n2 I. INTRODUCTION\nGroup anomaly detection has recently attracted much attention, with applications in astronomy [14], social media [15], disease/custom control [9][3] and network intrusion detection [11][5][4]. In this work, we focus on group anomaly detection applied to network intrusion detection, where the anomalous groups are either distributed Botnet (Zeus) or peer-to-peer (P2P) nodes generating traffic that deviates from the normal (Web traffic) behavior. Many existing intrusion detection systems (IDSs) only make sample-wise anomaly detections, e.g., in [12], the samples which deviate most from a normal (reference) model are flagged as anomalies/outliers. However, such an approach does not identify anomalous groups (e.g., a collection of BotNet flows), whose samples all exhibit similar behavior. Identifying such groups could be essential for mounting some form of system response or defense. Moreover, individual samples may only be weakly atypical. Thus, a sample-wise IDS may either fail to detect most of the anomalous samples, or may incur high false positives when a low detection threshold is used. By contrast, (weakly) anomalous samples whose anomalies are all \u201csimilar to each other\u201d may be strongly atypical when considered in aggregate, i.e. jointly. For example, for an N = 100-dimensional feature space, suppose there is a sizeable collection of samples in the captured data batch that are all (even only weakly) atypical with respect to the same feature or the same (small) feature subset. There is a low probability that this occurs by chance, (i.e.,under the null). Thus, such clusters of anomalies, each defined by a sample subset and a feature subset, may be strongly atypical, and hence more convincing anomalies, than individual sample anomalies. It should be noted that there is an enormous number of candidate anomalous clusters, considering the conjoining of all possible sample subsets and all possible feature subsets. Thus, a GAD scheme will require some type of heuristic search over this huge space, aiming to detect the most statistically significant cluster candidates. In the sequel, we propose such a GAD scheme. Rather than assuming individual features or outlier events are statistically independent under the null as in [9], [5], in our approach, as in [10], we capture and exploit statistical dependencies amongst the features defining a candidate cluster. Compared to previous works, as shown in our experiments, the proposed scheme is more effective in detecting group anomalies.\nThe paper is organized as follows. Section II defines the problem and elaborates on related works. Section III describes the proposed model. Section IV evaluates the system performance,\nNovember 4, 2015 DRAFT\n3 and compares with some recent works. We then discuss some extensions of our system and future works in section V, followed by conclusions."}, {"heading": "II. PROBLEM DEFINITION AND RELATED WORK", "text": "We assume there is a batch of normal web traffic available at the outset as training set, i.e. Xl = {x\u0303i, i = 1, ..., Tl, x\u0303i \u2208 R D}, where x\u0303i is a D-dimensional feature vector representing the i-th training traffic flow1, and where we assume the number of training flows Tl is large enough to learn an accurate reference model (null hypothesis). These traffic flows can either be generated and captured in a sandbox environment, or sampled from a domain of interest (data warehouse, enterprise network) in real time under normal operating conditions. Given a model of normal network traffic learned based on Xl, our goal is to interrogate a capture batch of unknown traffic flows Xu = {xi, i = 1, ..., Tu, xi \u2208 R D}2, seeking to identify latent groups of Botnet or P2P traffic, with the flows in each such group exhibiting similar behavior. This has been previously considered in [5], where the authors used the samples in Xl to estimate bivariate Gaussian Mixture Models (GMMs), on all feature pairs, representing the null hypothesis. These bivariate GMMs were used to evaluate mixture-based p-values3 for all pairs of features. Assuming the features (tests) are statistically independent, a joint significance score function was defined for a given candidate cluster, specified by its sample subset and feature subset, with a Bonferroni correction used to account for multiple testing. Instead of exhaustively searching over feature subset candidates at order K 4, the authors proposed to trial-add individual features only to the top-ranking candidate feature subsets (in terms of the Bonferroni corrected score) at order K \u2212 1. Furthermore, the authors showed that the computational complexity of determining the optimal (in terms of the joint score) sample subset given the feature subset fixed is linear in Tu, once the samples in a given feature subset are ranked by their aggregate p-values. However, the independent test assumption used in [5] becomes grossly invalid as more and more features are included in a cluster, which limits the proposed model\u2019s detection accuracy for increasing K. A related framework was also proposed in [9], albeit assuming categorical attributes. Here, the\n1A flow is a bidirectional communication sequence between a pair of nodes in a network. 2Unknown in the sense that we do not know which if any of these flows represent outliers or attacks. 3A p-value is the probability that an event is more extreme than the given observation. 4We use \u201corder\u201d to denote the maximum feature dimension considered.\nNovember 4, 2015 DRAFT\n4 authors built a single, global null hypothesis Bayesian network based on Xl. They then assigned categorical-based p-values to samples in Xu, with a cross entropy based scoring criterion used to efficiently search for the best feature and sample subset candidates. A limitation of this approach is that the statistical tests are again assumed to be independent.\nWe herein describe and experiment with a method of anomaly detection that extends [9], [5] and is closely related to [10]. The method captures dependencies between the features in a candidate cluster by a dependence tree structure, and uses this model to help evaluate joint p-values for cluster candidates. As in [5], the Bonferroni corrected score is used as the objective function for evaluating the best cluster candidates (defined by their sample and feature subsets). The candidate with the best such score is detected as a cluster of anomalies. Whereas in [9] a single global (null model) Bayesian network is used to assess candidate clusters, in [10] and in the current work a local, customized cluster-specific dependence tree model is used to assess each candidate cluster."}, {"heading": "III. PROPOSED MODEL", "text": ""}, {"heading": "A. Mixture-based P-values for Singletons and Feature Pairs", "text": "Consider a (sample, feature) index pair (i, j) and let I(j)i be an indicator variable for the event that the jth feature value of the ith sample, x(j)i , is an outlier with respect to the null distribution for feature X(j). Let O(j)(x(j)i ) be a subset of the real line such that, \u2200y (j) \u2208 O(j)(x (j) i ), y (j) is \u201cmore extreme\u201d than the given observation x(j)i . One good definition for this set, consistent with evaluating a 2-sided p-value for a unimodal, symmetric null for X(j), is:\nO(j)(x (j) i ;\u00b5 (j)) = {y(j) : |y(j) \u2212 \u00b5(j)| \u2265 |x (j) i \u2212 \u00b5 (j)|},\nwhere \u00b5(j) is a representative (mean) value for feature X(j). Given the component means \u00b5(j)l , l = 1, ..., Lj , of an Lj-component Gaussian mixture null, let M (j)(x) be a function that maps x to the mixture component index set {1, 2, ..., Lj}, i.e., it indicates which mixture component generated x. Also, let Yj be a random variable distributed according to the mixture density fXj (x). Then, for a given observation x(j)i , we define the binary random variable I (j) i , where I (j) i = 1 if Yj is\nNovember 4, 2015 DRAFT\n5 more extreme under the null than x(j)i . Then, we can write the singleton mixture p-value as:\nP [I (j) i = 1]\n= P [Yj \u2208 \u222a L l=1((O (j)(x (j) i ;\u00b5 (j) l )) \u2229 (M (j)(x (j) i ) = l))]\n=\nL \u2211\nl=1\nP [Yj \u2208 O (j)(x (j) i ;\u00b5 (j) l )]P [M (j)(x (j) i ) = l]. (1)\nHere, an extreme outlier event is conditioned on x(j)i having been generated by component density l. The probability P [Yj \u2208 O(j)(x (j) i ;\u00b5 (j) l )] is the two-sided Gaussian p-value, integrating over the region |y \u2212 \u00b5(j)l | \u2265 |x (j) i \u2212 \u00b5 (j) l |, while P [M (j)(x (j) i ) = l] is the a posteriori probability that x(j)i was generated by component l.\nSimilarly, for a pair of observations (x(j)i , x (k) i ), we have the second order mixture p-value:\nP [I (j) i = 1, I (k) i = 1]\n= L \u2211\nl=1\nP [Yj \u2208 O (j)(x (j) i ;\u00b5 (j) l ), Yk \u2208 O (k)(x (k) i ;\u00b5 (k) l )]\n\u00b7 P [M (j,k)(x (j) i , x (k) i ) = l].\nHere, P [Yj \u2208 O(j)(x (j) i ;\u00b5 (j) l ), Yk \u2208 O (k)(x (k) i ;\u00b5 (k) l )] integrates the l-th component bivariate Gaussian density over the region\n{(yj , yk) : |yj \u2212 \u00b5 (j) l | \u2265 |x (j) i \u2212 \u00b5 (j) l |, |yk \u2212 \u00b5 (k) l | \u2265 |x (k) i \u2212 \u00b5 (k) l |}.\nThis region consists of the union of four unbounded rectangular regions in the plane, as illustrated in Figure 1.\nIn this work, a sample\u2019s anomalousness on a given feature subset is estimated by a joint p-value, with statistical dependencies between features accounted for by a dependence tree (DT) structure [2]. Since the dependence tree [2] is based on first and second order probabilities, the joint p-value will be based on the singleton and second order mixture p-values, as given above. A smaller joint p-value indicates a sample is more anomalous under the given feature subset."}, {"heading": "B. Scoring Clusters", "text": "Let {Ic, Jc} denote cluster candidate c, Ic its sample subset and Jc its feature subset. Let Tc = |Ic|, Nc = |Jc|. Note that p-values are uniformly distributed on [0, 1] under the null. Thus,\nNovember 4, 2015 DRAFT\n6\ngiven a cluster with feature subset Jc, from a test batch of size Tu, the probability that at least one cluster with Tc samples has a smaller p-value than P [ \u2229 j\u2208Jc (I (j) i ) = 1] is:\n1\u2212 (1\u2212 \u220f\ni\nP [ \u2229 j\u2208Jc\n(I (j) i ) = 1]) C(Tu,Tc) (2)\nHere, C(Tu, Tc) =\n(\nTu Tc\n)\n, i.e. it is the number of combinations and implements multiple testing\ncorrection, accounting for all possible sample subset configurations in a cluster with Tc samples, from a test batch of size Tu. In principle, (2) provides a sound basis at least for directly comparing all cluster candidates with the same feature subset Jc. However, it does not allow comparing pairs of cluster candidates with any configurations of (Tc, Nc), because all possible feature subset configurations at a given order, Nc, have not yet been properly multiple-testing corrected. Also, (2) requires evaluation of the joint p-value P [ \u2229 j\u2208Jc (I (j) i ) = 1]), \u2200i \u2208 Ic, which in general depends on the joint density function for (Xj1, Xj2, ..., XjNc ), jm \u2208 Jc, m = 1, ..., Nc. When D is large, it is not practically feasible to learn and store these ( D\nNc\n)\njoint null density functions, i.e., for all\npossible combinations of features up to order Nc. Thus, it appears some tractable representation of P [ \u2229 j\u2208Jc (I (j) i ) = 1]) is needed. An obvious temptation is to assume that I (j) i and I (j\u2032) i are statistically independent \u2200j, j\u2032 \u2208 Jc, j\u2032 6= j. But this is a very poor assumption, consistent with assuming the features are independent.\nTo address the above problems, we seek to modify (2) in two respects. First, we propose to multiple test correct both for the different sample and the different feature subsets, given\nNovember 4, 2015 DRAFT\n7 a cluster candidate with (Tc, Nc). In this approach, instead of the exponent being the number of combinations, it becomes the product of combinations on samples and combinations on features. Based on the Bonferroni approximation of (2), we have the joint score function\nS(Ic, Jc) =\n(\nD Nc\n)(\nT Tc\n)\n\u220f\ni\u2208Ic P [ \u2229 j\u2208Jc (I\n(j) i ) = 1]). For this joint significance measure, we can\nefficiently determine the optimal sample subset, given a fixed feature subset, by greedy sequential sample inclusion, in sorted joint p-value order. This is due to the unimodality of this Bonferroni approximated joint significance measure, as a function of the number of samples included in a cluster\u2019s sample subset (see next subsection).\nSecond, a rich, tractable, joint probability mass function model that does capture statistical dependencies is a restricted form of Bayesian network, based exclusively on first and second order distributions, i.e., the dependence tree (DT), which factorizes the joint distribution P [ \u2229 j\u2208Jc (I (j) i ) = 1]) as a product of first and second order probabilities [2]. In [2], it was shown that, even though there is an enormous number of unique dependence tree structures, one can efficiently find the globally optimal dependence tree, over all such structures, maximizing the dataset\u2019s loglikelihood, by realizing that this can be recast as a maximum weight spanning tree problem, with the pairwise weights defined as the mutual information between the pairs of random variables. The maximum weight spanning tree can be efficiently solved via Kruskal\u2019s algorithm, with complexity O(N2c log(Nc)). Hence, given any candidate feature subset Jc, Kruskal\u2019s algorithm can be applied to determine the DT that maximizes the likelihood measured on Xl, i.e., the null hypothesis is determined, consistent with the given candidate feature subset Jc.\nBased on a given DT structure, P [ \u2229 j\u2208Jc\n(I (j) i ) = 1]) factorizes as a product of first and second\norder distributions, i.e., \u2200i \u2208 Ic:\nP [ \u2229 j\u2208Jc\n(I (j) i )] = P [I (j1) i ]P [I (j2) i |I (j1) i ]...P [I (jNc) i |I (jNc\u22121) i ], (3)\nwhere we use j1 to denote the root node of the DT representing Jc.\nIt is apparent from (3) that, for any feature subset, one can represent the joint p-value of a given sample by its first and second order mixture p-values. That is, for any feature pair (j, k), P [I (j) i |I (k) i ] = P [I (j) i ,I (k) i ]\nP [I (k) i ]\n. The numerator and denominator are, respectively, the second and first\norder mixture-based p-values that we defined earlier. Also note that, in order to evaluate the first order mixture p-value P [I(k)i ], we marginalize feature j from the bivariate GMM for the feature pair (j, k). This gives us the GMM for feature k.\nNovember 4, 2015 DRAFT\n8 C. Identifying the Optimal Sample Subset Ic, Given Fixed Jc\nGiven a fixed Jc and associated DT, we would like to choose the sample subset Ic to minimize (2). Applying the Bonferroni correction, this is essentially equivalent to choosing Ic to minimize the joint score function:\nS(Ic, Jc) =\n(\nD Nc\n)(\nTu Tc\n)\n\u220f\ni\u2208Ic\nP [ \u2229 j\u2208Jc\n(I (j) i ) = 1]). (4)\nIt is in fact easily shown that this objective function is globally minimized by the following\nprocedure: i) sort the samples in increasing order of their joint p-values P [ \u2229 j\u2208Jc\n(I (j) i ) = 1]; ii)\nsequentially include the samples on the sorted list into Ic, until the objective function no longer decreases. This procedure globally minimizes over Ic given fixed Jc."}, {"heading": "D. Overall Search Algorithm", "text": "First, using the normal samples in Xl, all the first and second order null GMMs are separately trained 5. Mutual information for all feature pairs is then calculated based on the bivariate GMMs. This is achieved by generating M = 106 samples from a given bivariate GMM distribution, and then estimating the mutual information by 1 M M \u2211\nn=1\nlog( fX1X2(x\n(n) 1 ,x (n) 2 )\nfX1(x (n) 1 )fX2 (x (n) 2 )\n. We then detect clusters in\nXu sequentially, in a rank-prioritized fashion, according to the joint score S(Ic, Jc). The algorithm operates on an enormous space of candidate clusters even if the feature space itself is only\n5Separately learning each marginal and pairwise feature GMM using the common training set Xl will not ensure consistency with respect to feature marginalizations. Specifically, a marginal-consistent collection of univariate and bivariate density functions should satisfy the following: if we consider any feature pairs (i, j) and (j, k), marginalizing out feature i from the (i, j) bivariate density and marginalizing out feature k from the (j, k) bivariate density should lead to the same marginal density for feature j. However, when the univariate and bivariate distributions are Gaussian mixtures, with a non-convex log-likelihood function (and with BIC-based model order selection separately applied to choose the number of components for each GMM), separate application of EM-plus-BIC to learn each GMM density function does not ensure a set of marginal-consistent distributions. This property is not centrally important here, however, since our main concern is only to learn marginal and pairwise density functions that allow accurate assessment of p-values. Accordingly, in this work we will apply EM-plus-BIC separately, to learn each low-order GMM.\nOne approach to obtain marginal-consistent low-order distributions is to simply learn the single GMM for the joint distribution on the full feature vector, X . This determines (via marginalization) all lower-order distributions (which are also GMMs, and which are guaranteed to be marginal-consistent). However, this strategy suffers from the curse of dimensionality. Alternatively, we refer the interested reader to [10], where a procedure for directly, jointly learning a marginal-consistent set of low-order GMMs is elaborated.\nNovember 4, 2015 DRAFT\n9 modestly sized (D). We start by sweeping over feature subset candidates at low orders and, for tractability, only the \u201cmost promising\u201d candidates at higher orders, with candidate feature subsets at order K formed by \u201caccreting\u201d new features to the best-scoring candidates at order K \u2212 1. For each candidate feature subset Jc, its DT is first learned and its associated, optimal subset Ic is then determined using the method described in section III.C. Evaluating all candidates at all feature subset orders, the one with the best score function value at each order Nc is recorded. The cluster with smallest Bonferroni-corrected score S(Ic, Jc) is then forwarded as detected. Its samples are then removed from the test batch. Subsequent cluster detections can then be made following the same procedure. Cluster detections are thus made (in general) in order of decreasing joint significance."}, {"heading": "IV. EXPERIMENTAL SETUP AND RESULTS", "text": "Our experiments focus on detecting Zeus botnet and P2P traffic among normal Web traffic. The Web packet-flows are obtained from the LBNL repository [6]. This dataset contains Web traffic on TCP port 80, with specified time-of-day information. Specifically, the experiments in this paper are based on three datasets named \u201c200412215-0510.port008\u201d, \u201c20041215-1343.port008\u201d and \u201c20041215-1443.port010\u201d. The protocols to obtain normal, P2P and BotNet network traffic are the same as in [5], i.e., we used the port-mapper in [16] to identify P2P traffic in these files by a C4.5 decision tree pre-trained in another domain (the Cambridge dataset [7]). The Zeus Botnet traffic are obtained from another domain [13]."}, {"heading": "A. Feature Space Selection and Representation", "text": "Firstly, we did not use layer-4 port number features for purposes of detection [16], [1]. Also, we did not consider timing information herein because the Zeus activity was recorded on another domain [1]. In [1], previous efforts were made to detect BotNet and P2P traffic using the wellknown feature representation for network intrusion detection from [8]. The authors found that these features, though able to detect some attack activity, could not successfully discriminate BotNet or P2P from normal Web traffic, i.e., BotNet and P2P traffic appear as \u201cnormal\u201d Web activity according to the features of [8], [1].\nTo capture the intrinsic behavior of BotNet and P2P packet-traffic, we note that most Zeus BotNet traffic involves masters giving command (control) messages, while slaves execute the\nNovember 4, 2015 DRAFT\n10\ngiven commands. In the case of P2P, nodes often communicate in a bidirectional manner, exchanging relatively large packets in both directions. Normal/background Web traffic, on the other hand, tends to involve server-to-client communications.\nHence, we seek to preserve the bidirectional packet size sequence information as feature representation for different traffic flows. This feature representation was previously considered in [5], [11]. The authors used the first N (we set N = 10 in our experiments) packets after the threeway hand shake of each TCP flow. Then a feature vector of dimension 2N is defined, specified by the sizes and directionalities of these N packets. Traffic are assumed to be alternating between client-to-server (CS) and server-to-client (SC). A zero packet size is thus inserted between two consecutive packets in the same direction to indicate an absence of a packet in the other direction. For example, if the bidirectional traffic is strictly SC, a zero will be inserted after each SC packet size. This 2N-dimensional feature representation preserves bidirectional information of a given TCP flow, which is essential for discriminating between P2P, Zeus and normal Web traffic."}, {"heading": "B. Performance Metrics", "text": "Our algorithm detects clusters (groups) in a sequential fashion. For each extracted group, we rank the samples in the group by their associated joint p-values on the given feature subset. These samples will be sequentially removed from the test batch, with the system then continuing to extract groups until the test set is depleted. Then we sweep out an ROC curve based on these rank-ordered detected samples. A larger area under the ROC curve indicates earlier detections of anomalous groups, which implies the effectiveness of the intrusion detection system. We compare our system\u2019s performance with a GMM based anomaly detector, trained by normal samples, on the whole feature space. For this detector, we rank the test samples based on their data likelihood under the GMM, and sweep out an ROC curve. We also compare with the approach presented in [5], which assumes significance tests are independent (denoted \u201cIndependence tests\u201d), and with the recent work presented in [9] with a slight modification \u2013 instead of discretizing feature values consistent with [9], we use a single dependence tree null distribution learned on Xl and our proposed joint p-value for continuous features, P [ \u2229 j\u2208Jc (I (j) i ) = 1]. We denote this variation on the approach in [9] by \u201csingle Bayesian Net.\u201d There are two generalization performance measures of interest on the test set: one is the aforementioned ROC area under curve (ROC AUC) as a function of the maximum feature subset size for a cluster, Kmax. The other is the top\nNovember 4, 2015 DRAFT\n11\n100 precision rate, defined as the fraction of anomalous samples amongst the first 100 detected samples. Lastly, instead of exhaustively searching over all feature subsets at order K, we trialadd individual features to the top candidate feature subsets from order K \u2212 1. At each order K, starting from order 2, we only consider the top 500 candidates from order K \u2212 1.\nTwo different sets of experiments were performed, one on synthetic data, and the other on the network data mentioned earlier. In the synthetic dataset experiment, we used one unimodal Gaussian with 10 dimensions to generate normal samples and two additional unimodal Gaussians to generate two distinct anomalous clusters. The two anomalous clusters use the same distribution as the normal distribution for nine of the ten features. Thus, they deviate from the normal (null) distribution only on a single feature dimension (this \u201cinformative\u201d feature dimension was different for the two clusters). Their corresponding sample subsets consist of 2.5% of the whole data batch Xu (so the proportion of anomalous samples in Xu is 5% of the total). The variance of the informative features was chosen to be the same as that of the normal features, \u03c32n. Moreover the mean of the informative feature under an anomalous cluster was chosen to be two standard deviations away from the mean under the normal class, i.e. |\u00b5n \u2212 \u00b5a| = 2\u03c3n, where we use subscripts n and a to denote \u2018normal\u2019 and \u2018anomalous\u2019, respectively. Thus, if we consider only the informative feature dimension, the Bayes error rate in discriminating normal from anomalous is 15.87%. After generating the synthetic data batch (with a size of ten thousand samples), we randomly chose 20% of normal samples as ground-truth and used them to train the null hypothesis. The remaining normal samples were used as part of the test batch, along with the samples from the two anomalous clusters. This was repeated 10 times, with the performance averaged.\nFor the network data, all the normal web flows from the three files were combined, making nearly ten thousand normal web flows. We randomly selected 20% of these flows as groundtruth normal samples to train the null, and treated the remaining normal flows as part of the test batch, combined with either P2P or Zeus anomalous flows. We separately experimented with P2P and Zeus flows. There were roughly 5 % of either P2P or Zeus flows in a given test batch. Experiments for each scenario were averaged over 10 random train-test splits.\nNovember 4, 2015 DRAFT\n12"}, {"heading": "C. Experimental results", "text": "In Figure 2, we show the performance on the synthetic data. Note that both the proposed scheme and [5] effectively capture groups of anomalies when the maximum feature subset order is two. The first captured cluster (sample subset) consists of more than 95% anomalous samples on average. However, as the maximum feature subset order increases, the \u201cindependence tests\u201d approach drops significantly in performance. This is because too many (assumed to be independent) pairwise tests create many redundant features that are all used to evaluate cluster anomalousness; use of these redundant features de-emphasizes, within the score function, the important (low-order) feature subset. Also, we see an early advantage of using cluster-specific DTs, compared to the single Bayesian Net approach. It appears that if an anomalous process is strictly generated from a low order subspace and normal in other feature dimensions (as is the case in this experiment) our cluster-specific DT approach outperforms a single Bayesian Net approach.\nIn Figure 3 a), we show the performance for normal-P2P discrimination. Compared to [5], which degrades in performance as more and more tests are included, we see superior performance for the proposed method. There is a large batch of anomalous samples captured at maximum order 6 by the proposed method, but both [9] and [5] did not capture this group effectively, as seen in the top 100 precision figure. Also, both of these methods are outperformed by the GMM baseline method. In Figure 3 b), we show the performance for normal-Zeus discrimination. Again, at maximum feature subset order 6 the proposed method captures a large portion of the anomalous flows \u2013 more than 50 Zeus flows were captured out of the first 100 flows detected\nNovember 4, 2015 DRAFT\n13\nby the proposed method. [5] performs poorly in this experiment, and again we observed that as the number of tests increase, the independence assumption degrades the detection performance. The single Bayesian Net approach in [9] also performs relatively poorly on this dataset."}, {"heading": "V. EXTENSIONS AND FUTURE WORK", "text": "In this work, we used the Bonferroni corrected score function to directly evaluate cluster candidates. Alternatively, we could try to evaluate empirical p-values for this decision statistic, by applying our detection strategy to (many) bootstrap test batches drawn from the null distribution. It would be interesting to see whether such an approach gives comparable (or even better) detection accuracy than use of the Bonferroni corrected score by itself. Such an approach could also be used to determine whether any detected clusters are truly statistically significant. In this work we showed detection accuracy as a function of the maximum feature subset size for a cluster. As the maximum feature subset size continues to increase, we observed that false positives also increase in the first detected cluster, and the objective in (4) tends to favor the maximum feature dimension over use of fewer dimensions. In future, we should propose and investigate criteria for choosing this maximum feature subset size.\nNovember 4, 2015 DRAFT\n14"}, {"heading": "VI. CONCLUSION", "text": "In this work, we proposed a GAD scheme to identify anomalous sample and feature subsets, accounting for dependencies between the features in a given subset. The proposed model outperforms previous works that assume statistical tests are independent under the null. We demonstrated the effectiveness of our proposed system on both synthetic and real world data, with the latter drawn from the network intrusion detection domain, aiming to discriminate between normal and P2P/Zeus traffic. Our future work includes empirical p-value assessment and automatic determination of the maximum feature subset size of a cluster."}], "references": [{"title": "Salting public traces with attack traffic to test flow classifiers", "author": ["Z.B. Celik", "J. Raghuram", "G. Kesidis", "D.J. Miller"], "venue": "In Proc. USENIX CSET,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Approximating discrete probability distributions with dependence trees", "author": ["C. Chow", "C. Liu"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1968}, {"title": "Anomaly pattern detection in categorical datasets", "author": ["K. Das", "J. Schneider", "D.B. Neill"], "venue": "In Proc. 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Anomaly detection over noisy data using learned probability distributions", "author": ["E. Eskin"], "venue": "In Proc. 17th International Conference on Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Detecting anomalous latent classes in a batch of network traffic flows", "author": ["F. Kocak", "D.J. Miller", "G. Kesidis"], "venue": "In Proc. IEEE Conf. on Information Sciences and Systems (CISS),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Efficient application identification and the temporal and spatial stability of classification schema", "author": ["W. Li", "M. Canini", "A.W. Moore", "R. Bolla"], "venue": "Computer Networks (Elsevier),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "A machine learning approach for efficient traffic classification", "author": ["W. Li", "A.W. Moore"], "venue": "In Proc. IEEE Int\u2019l Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Fast generalized subset scan for anomalous pattern detection", "author": ["E. McFowland", "S. Speakman", "D.B. Neill"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Detecting clusters of anomalies in a data batch that manifest on low-dimensional feature subsets via dependence-tree based evaluation of joint statistical significance", "author": ["D.J. Miller", "G. Kesidis"], "venue": "Provisional United States Patent Filing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Sequential anomaly detection in a batch with growing number of tests: Application to network intrusion detection", "author": ["D.J. Miller", "F. Kocak", "G. Kesidis"], "venue": "In Proc. IEEE Int\u2019l Workshop on Machine Learning for Signal Processing (MLSP),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Outside the closed world: On using machine learning for network intrusion detection", "author": ["R. Sommer", "V. Paxson"], "venue": "In IEEE Symposium on Security and Privacy,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Hierarchical probabilistic models for group anomaly detection", "author": ["L. Xiong", "B. P\u00f3czos", "J.G. Schneider", "A. Connolly", "J. VanderPlas"], "venue": "In Proc. International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "GLAD: group anomaly detection in social media analysis", "author": ["R. Yu", "X. He", "Y. Liu"], "venue": "In Proc. 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "A flow classifier with tamper-resistant features and an evaluation of its portability to new domains", "author": ["G. Zou", "G. Kesidis", "D.J. Miller"], "venue": "IEEE Journal on Selected Areas in Communications,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}], "referenceMentions": [{"referenceID": 11, "context": "Group anomaly detection has recently attracted much attention, with applications in astronomy [14], social media [15], disease/custom control [9][3] and network intrusion detection [11][5][4].", "startOffset": 94, "endOffset": 98}, {"referenceID": 12, "context": "Group anomaly detection has recently attracted much attention, with applications in astronomy [14], social media [15], disease/custom control [9][3] and network intrusion detection [11][5][4].", "startOffset": 113, "endOffset": 117}, {"referenceID": 7, "context": "Group anomaly detection has recently attracted much attention, with applications in astronomy [14], social media [15], disease/custom control [9][3] and network intrusion detection [11][5][4].", "startOffset": 142, "endOffset": 145}, {"referenceID": 2, "context": "Group anomaly detection has recently attracted much attention, with applications in astronomy [14], social media [15], disease/custom control [9][3] and network intrusion detection [11][5][4].", "startOffset": 145, "endOffset": 148}, {"referenceID": 9, "context": "Group anomaly detection has recently attracted much attention, with applications in astronomy [14], social media [15], disease/custom control [9][3] and network intrusion detection [11][5][4].", "startOffset": 181, "endOffset": 185}, {"referenceID": 4, "context": "Group anomaly detection has recently attracted much attention, with applications in astronomy [14], social media [15], disease/custom control [9][3] and network intrusion detection [11][5][4].", "startOffset": 185, "endOffset": 188}, {"referenceID": 3, "context": "Group anomaly detection has recently attracted much attention, with applications in astronomy [14], social media [15], disease/custom control [9][3] and network intrusion detection [11][5][4].", "startOffset": 188, "endOffset": 191}, {"referenceID": 10, "context": ", in [12], the samples which deviate most from a normal (reference) model are flagged as anomalies/outliers.", "startOffset": 5, "endOffset": 9}, {"referenceID": 7, "context": "Rather than assuming individual features or outlier events are statistically independent under the null as in [9], [5], in our approach, as in [10], we capture and exploit statistical dependencies amongst the features defining a candidate cluster.", "startOffset": 110, "endOffset": 113}, {"referenceID": 4, "context": "Rather than assuming individual features or outlier events are statistically independent under the null as in [9], [5], in our approach, as in [10], we capture and exploit statistical dependencies amongst the features defining a candidate cluster.", "startOffset": 115, "endOffset": 118}, {"referenceID": 8, "context": "Rather than assuming individual features or outlier events are statistically independent under the null as in [9], [5], in our approach, as in [10], we capture and exploit statistical dependencies amongst the features defining a candidate cluster.", "startOffset": 143, "endOffset": 147}, {"referenceID": 4, "context": "This has been previously considered in [5], where the authors used the samples in Xl to estimate bivariate Gaussian Mixture Models (GMMs), on all feature pairs, representing the null hypothesis.", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "However, the independent test assumption used in [5] becomes grossly invalid as more and more features are included in a cluster, which limits the proposed model\u2019s detection accuracy for increasing K.", "startOffset": 49, "endOffset": 52}, {"referenceID": 7, "context": "A related framework was also proposed in [9], albeit assuming categorical attributes.", "startOffset": 41, "endOffset": 44}, {"referenceID": 7, "context": "We herein describe and experiment with a method of anomaly detection that extends [9], [5] and is closely related to [10].", "startOffset": 82, "endOffset": 85}, {"referenceID": 4, "context": "We herein describe and experiment with a method of anomaly detection that extends [9], [5] and is closely related to [10].", "startOffset": 87, "endOffset": 90}, {"referenceID": 8, "context": "We herein describe and experiment with a method of anomaly detection that extends [9], [5] and is closely related to [10].", "startOffset": 117, "endOffset": 121}, {"referenceID": 4, "context": "As in [5], the Bonferroni corrected score is used as the objective function for evaluating the best cluster candidates (defined by their sample and feature subsets).", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "Whereas in [9] a single global (null model) Bayesian network is used to assess candidate clusters, in [10] and in the current work a local, customized cluster-specific dependence tree model is used to assess each candidate cluster.", "startOffset": 11, "endOffset": 14}, {"referenceID": 8, "context": "Whereas in [9] a single global (null model) Bayesian network is used to assess candidate clusters, in [10] and in the current work a local, customized cluster-specific dependence tree model is used to assess each candidate cluster.", "startOffset": 102, "endOffset": 106}, {"referenceID": 1, "context": "In this work, a sample\u2019s anomalousness on a given feature subset is estimated by a joint p-value, with statistical dependencies between features accounted for by a dependence tree (DT) structure [2].", "startOffset": 195, "endOffset": 198}, {"referenceID": 1, "context": "Since the dependence tree [2] is based on first and second order probabilities, the joint p-value will be based on the singleton and second order mixture p-values, as given above.", "startOffset": 26, "endOffset": 29}, {"referenceID": 0, "context": "Note that p-values are uniformly distributed on [0, 1] under the null.", "startOffset": 48, "endOffset": 54}, {"referenceID": 1, "context": ", the dependence tree (DT), which factorizes the joint distribution P [ \u2229 j\u2208Jc (I (j) i ) = 1]) as a product of first and second order probabilities [2].", "startOffset": 149, "endOffset": 152}, {"referenceID": 1, "context": "In [2], it was shown that, even though there is an enormous number of unique dependence tree structures, one can efficiently find the globally optimal dependence tree, over all such structures, maximizing the dataset\u2019s loglikelihood, by realizing that this can be recast as a maximum weight spanning tree problem, with the pairwise weights defined as the mutual information between the pairs of random variables.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "Alternatively, we refer the interested reader to [10], where a procedure for directly, jointly learning a marginal-consistent set of low-order GMMs is elaborated.", "startOffset": 49, "endOffset": 53}, {"referenceID": 4, "context": "The protocols to obtain normal, P2P and BotNet network traffic are the same as in [5], i.", "startOffset": 82, "endOffset": 85}, {"referenceID": 13, "context": ", we used the port-mapper in [16] to identify P2P traffic in these files by a C4.", "startOffset": 29, "endOffset": 33}, {"referenceID": 5, "context": "5 decision tree pre-trained in another domain (the Cambridge dataset [7]).", "startOffset": 69, "endOffset": 72}, {"referenceID": 13, "context": "Firstly, we did not use layer-4 port number features for purposes of detection [16], [1].", "startOffset": 79, "endOffset": 83}, {"referenceID": 0, "context": "Firstly, we did not use layer-4 port number features for purposes of detection [16], [1].", "startOffset": 85, "endOffset": 88}, {"referenceID": 0, "context": "Also, we did not consider timing information herein because the Zeus activity was recorded on another domain [1].", "startOffset": 109, "endOffset": 112}, {"referenceID": 0, "context": "In [1], previous efforts were made to detect BotNet and P2P traffic using the wellknown feature representation for network intrusion detection from [8].", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In [1], previous efforts were made to detect BotNet and P2P traffic using the wellknown feature representation for network intrusion detection from [8].", "startOffset": 148, "endOffset": 151}, {"referenceID": 6, "context": ", BotNet and P2P traffic appear as \u201cnormal\u201d Web activity according to the features of [8], [1].", "startOffset": 86, "endOffset": 89}, {"referenceID": 0, "context": ", BotNet and P2P traffic appear as \u201cnormal\u201d Web activity according to the features of [8], [1].", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": "This feature representation was previously considered in [5], [11].", "startOffset": 57, "endOffset": 60}, {"referenceID": 9, "context": "This feature representation was previously considered in [5], [11].", "startOffset": 62, "endOffset": 66}, {"referenceID": 4, "context": "We also compare with the approach presented in [5], which assumes significance tests are independent (denoted \u201cIndependence tests\u201d), and with the recent work presented in [9] with a slight modification \u2013 instead of discretizing feature values consistent with [9], we use a single dependence tree null distribution learned on Xl and our proposed joint p-value for continuous features, P [ \u2229 j\u2208Jc (I (j) i ) = 1].", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "We also compare with the approach presented in [5], which assumes significance tests are independent (denoted \u201cIndependence tests\u201d), and with the recent work presented in [9] with a slight modification \u2013 instead of discretizing feature values consistent with [9], we use a single dependence tree null distribution learned on Xl and our proposed joint p-value for continuous features, P [ \u2229 j\u2208Jc (I (j) i ) = 1].", "startOffset": 171, "endOffset": 174}, {"referenceID": 7, "context": "We also compare with the approach presented in [5], which assumes significance tests are independent (denoted \u201cIndependence tests\u201d), and with the recent work presented in [9] with a slight modification \u2013 instead of discretizing feature values consistent with [9], we use a single dependence tree null distribution learned on Xl and our proposed joint p-value for continuous features, P [ \u2229 j\u2208Jc (I (j) i ) = 1].", "startOffset": 259, "endOffset": 262}, {"referenceID": 7, "context": "We denote this variation on the approach in [9] by \u201csingle Bayesian Net.", "startOffset": 44, "endOffset": 47}, {"referenceID": 4, "context": "Note that both the proposed scheme and [5] effectively capture groups of anomalies when the maximum feature subset order is two.", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "Compared to [5], which degrades in performance as more and more tests are included, we see superior performance for the proposed method.", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "There is a large batch of anomalous samples captured at maximum order 6 by the proposed method, but both [9] and [5] did not capture this group effectively, as seen in the top 100 precision figure.", "startOffset": 105, "endOffset": 108}, {"referenceID": 4, "context": "There is a large batch of anomalous samples captured at maximum order 6 by the proposed method, but both [9] and [5] did not capture this group effectively, as seen in the top 100 precision figure.", "startOffset": 113, "endOffset": 116}, {"referenceID": 4, "context": "[5] performs poorly in this experiment, and again we observed that as the number of tests increase, the independence assumption degrades the detection performance.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "The single Bayesian Net approach in [9] also performs relatively poorly on this dataset.", "startOffset": 36, "endOffset": 39}], "year": 2015, "abstractText": "In a variety of applications, one desires to detect groups of anomalous data samples, with a group potentially manifesting its atypicality (relative to a reference model) on a low-dimensional subset of the full measured set of features. Samples may only be weakly atypical individually, whereas they may be strongly atypical when considered jointly. What makes this group anomaly detection problem quite challenging is that it is a priori unknown which subset of features jointly manifests a particular group of anomalies. Moreover, it is unknown how many anomalous groups are present in a given data batch. In this work, we develop a group anomaly detection (GAD) scheme to identify the subset of samples and subset of features that jointly specify an anomalous cluster. We apply our approach to network intrusion detection to detect BotNet and peer-to-peer flow clusters. Unlike previous studies, our approach captures and exploits statistical dependencies that may exist between the measured features. Experiments on real world network traffic data demonstrate the advantage of our proposed system, and highlight the importance of exploiting feature dependency structure, compared to the feature (or test) independence assumption made in previous studies.", "creator": "LaTeX with hyperref package"}}}