{"id": "1511.01432", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2015", "title": "Semi-supervised Sequence Learning", "abstract": "We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a \"pretraining\" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.\n\n\nIn order to learn more about recurrent networks using unlabeled data, we first have to introduce two approaches that use unlabeled data to improve sequence learning with recurrent networks. One is a deterministic learning algorithm, which attempts to learn what the input sequence actually comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The third approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The fourth approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The fourth approach is to predict what comes next in a sequence, which is a traditional language model in natural language processing.\nIn order to learn more about recurrent networks using unlabeled data, we first have to introduce two approaches that use unlabeled data to improve sequence learning with recurrent networks. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models.\nThis model is designed to learn the input sequence of a word.\nThe first approach is to do a task that is specific to a task. For example, we can teach a sentence", "histories": [["v1", "Wed, 4 Nov 2015 18:48:36 GMT  (21kb)", "http://arxiv.org/abs/1511.01432v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["andrew m dai", "quoc v le"], "accepted": true, "id": "1511.01432"}, "pdf": {"name": "1511.01432.pdf", "metadata": {"source": "CRF", "title": "Semi-supervised Sequence Learning", "authors": ["Andrew M. Dai"], "emails": ["adai@google.com", "qvl@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n01 43\n2v 1\n[ cs\n.L G\n] 4\nN ov"}, {"heading": "1 Introduction", "text": "Recurrent neural networks (RNNs) are powerful tools for modeling sequential data, yet training them by back-propagation through time [36, 26] can be difficult [8]. For that reason, RNNs have rarely been used for natural language processing tasks such as text classification despite their powers in representing sequential structures.\nOn a number of document classification tasks, we find that it is possible to train Long Short-Term Memory recurrent networks (LSTM RNNs) [9] to achieve good performance with careful tuning of hyperparameters. Furthermore, a simple pretraining step can significantly stabilize the training of LSTMs. For example, we can use a next step prediction model, i.e., a recurrent language model in NLP, as an unsupervised method. Another method is to use a sequence autoencoder, which uses a RNN to read a long input sequence into a single vector. This vector will then be used to reconstruct the original sequence. The weights obtained from these two pretraining methods can then be used as an initialization for standard LSTM RNNs to improve training and generalization.\nIn our experiments on document classification with 20 Newsgroups [16] and DBpedia [19], and sentiment analysis with IMDB [21] and Rotten Tomatoes [25], LSTMs pretrained by recurrent language models or sequence autoencoders are usually better than LSTMs initialized randomly. This pretraining helps LSTMs reach or surpass previous baselines on these datasets without additional data.\nAnother important result from our experiments is that using more unlabeled data from related tasks in the pretraining can improve the generalization of a subsequent supervised model. For example, using unlabeled data from Amazon reviews to pretrain the sequence autoencoders can improve classification accuracy on Rotten Tomatoes from 79.7% to 83.3%, an equivalence of adding substantially more labeled data. This evidence supports the thesis that it is possible to use unsupervised learning with more unlabeled data to improve supervised learning. With sequence autoencoders, and outside unlabeled data, LSTMs are able to match or surpass previously reported results.\nWe believe our semi-supervised approach (as also argued by [1]) has some advantages over other unsupervised sequence learning methods, e.g., Paragraph Vectors [18], because it can allow for easy fine-tuning. Our semi-supervised learning approach is related to Skip-Thought vectors [13], with two differences. The first difference is that Skip-Thought is a harder objective, because it predicts adjacent sentences. The second is that Skip-Thought is a pure unsupervised learning algorithm, without fine-tuning."}, {"heading": "2 Sequence autoencoders and recurrent language models", "text": "Our approach to sequence autoencoding is inspired by the work in sequence to sequence learning (also known as seq2seq) by Sutskever et al. [31], which has been successfully used for machine translation [20, 10], text parsing [32], image captioning [34], video analysis [30], speech recognition [4, 3] and conversational modeling [27, 33]. Key to their approach is the use of a recurrent network as an encoder to read in an input sequence into a hidden state, which is the input to a decoder recurrent network that predicts the output sequence.\nThe sequence autoencoder is similar to the above concept, except that it is an unsupervised learning model. The objective is to reconstruct the input sequence itself. That means we replace the output sequence in the seq2seq framework with the input sequence. In our sequence autoencoders, the weights for the decoder network and the encoder network are the same (see Figure 1).\nWe find that the weights obtained from the sequence autoencoder can be used as an initialization of another supervised network, one which tries to classify the sequence. We hypothesize that this is because the network can already memorize the input sequence. This reason, and the fact that the gradients have shortcuts, are our hypothesis of why the sequence autoencoder is a good and stable approach in initializing recurrent networks.\nA significant property of the sequence autoencoder is that it is unsupervised, and thus can be trained with large quantities of unlabeled data to improve its quality. Our result is that additional unlabeled data can improve the generalization ability of recurrent networks. This is especially useful for tasks that have limited labeled data.\nWe also find that recurrent language models [23] can be used as a pretraining method for LSTMs. This is equivalent to removing the encoder part of the sequence autoencoder in Figure 1. Our experimental results show that this approach works better than LSTMs with random initialization."}, {"heading": "3 Overview of methods", "text": "In our experiments, we use LSTM recurrent networks [9] because they are generally better than RNNs. Our LSTM implementation is standard and has input, forget, and output gates [6, 7]. We compare basic LSTMs against LSTMs initialized with the sequence autoencoder method. When LSTMs are initialized with a sequence autoencoder, the methods are called SA-LSTMs in our experiments. When LSTMs are initialized with a language model, the method is called LM-LSTMs.\nIn most of our experiments our output layer predicts the document label from the LSTM output at the last timestep. We also experiment with the approach of putting the label at every timestep and linearly increasing the weights of the prediction objectives from 0 to 1 [24]. This way we can inject gradients to earlier steps in the recurrent networks. We call this approach linear label gain.\nLastly, we also experiment with the method of jointly training the supervised learning task with the sequence autoencoder and call this method joint training."}, {"heading": "4 Experiments", "text": "In our experiments with LSTMs, we follow the basic recipes as described in [31] by clipping the cell outputs and gradients. The benchmarks of focus are text understanding tasks, with all datasets being publicly available. The tasks are sentiment analysis (IMDB and Rotten Tomatoes) and text classification (20 Newsgroups and DBpedia). Commonly used methods on these datasets, such as bag-of-words or n-grams, typically ignore long-range ordering information (e.g., modifiers and their objects may be separated by many unrelated words); so one would expect recurrent methods which preserve ordering information to perform well. Nevertheless, due to the difficulty in optimizing these networks, recurrent models are not the method of choice for document classification.\nIn our experiments with the sequence autoencoder, we train it to reproduce the full document after reading all the input words. In other words, we do not perform any truncation or windowing. We add an end of sentence marker to the end of each input sequence and train the network to start reproducing the sequence after that marker. To speed up performance and reduce GPU memory usage, we perform truncated backpropagation up to 400 timesteps from the end of the sequence. We preprocess the text so that punctuation is treated as separate tokens and we ignore any non-English characters and words in the DBpedia text. We also remove words that only appear once in each dataset and do not perform any term weighting or stemming.\nAfter training the recurrent language model or the sequence autoencoder for roughly 500K steps with a batch size of 128, we use both the word embedding parameters and the LSTM weights to initialize the LSTM for the supervised task. We then train on that task while fine tuning both the embedding parameters and the weights and use early stopping when the validation error starts to increase. We choose the dropout parameters based on a validation set.\nUsing SA-LSTMs, we are able to match or surpass reported results for all datasets. It is important to emphasize that previous best results come from various different methods. So it is significant that one method achieves strong results for all datasets, presumably because such a method can be used as a general model for any similar task. A summary of results in the experiments are shown in Table 1. More details of the experiments are as follows."}, {"heading": "4.1 Sentiment analysis experiments with IMDB", "text": "In this first set of experiments, we benchmark our methods on the IMDB movie sentiment dataset, proposed by Maas et al. [21].1 There are 25,000 labeled and 50,000 unlabeled documents in the training set and 25,000 in the test set. We use 15% of the labeled training documents as a validation set. The average length of each document is 241 words and the maximum length of a document is 2,526 words. The previous baselines are bag-of-words, ConvNets [12] or Paragraph Vectors [18].\nSince the documents are long, one might expect that it is difficult for recurrent networks to learn. We however find that with tuning, it is possible to train LSTM recurrent networks to fit the training set. For example, if we set the size of hidden state to be 512 units and truncate the backprop to be 400, an LSTM can do fairly well. With random embedding dimension dropout [37] and random word dropout (not published previously), we are able to reach performance of around 86.5% accuracy in the test set, which is approximately 5% worse than most baselines.\n1http://ai.Stanford.edu/amaas/data/sentiment/index.html\nFundamentally, the main problem with this approach is that it is unstable: if we were to increase the number of hidden units or to increase the number of backprop steps, the training breaks down very quickly: the objective function explodes even with careful tuning of the gradient clipping. This is because LSTMs are sensitive to the hyperparameters for long documents. In contrast, we find that the SA-LSTM works better and is more stable. If we use the sequence autoencoders, changing the size of the hidden state or the number of backprop steps hardly affects the training of LSTMs. This is important because the models become more practical to train.\nUsing sequence autoencoders, we overcome the optimization instability in LSTMs in such a way that it is fast and easy to achieve perfect classification on the training set. To avoid overfitting, we again use input dimension dropout, with the dropout rate chosen on a validation set. We find that dropping out 80% of the input embedding dimensions works well for this dataset. The results of our experiments are shown in Table 2 together with previous baselines. We also add an additional baseline where we initialize a LSTM with word2vec embeddings on the training set.\nThe results confirm that SA-LSTM with input embedding dropout can be as good as previous best results on this dataset. In contrast, LSTMs without sequence autoencoders have trouble in optimizing the objective because of long range dependencies in the documents.\nUsing language modeling (LM-LSTM) as an initialization works well, achieving 8.98%, but less well compared to the SA-LSTM. This is perhaps because language modeling is a short-term objective, so that the hidden state only captures the ability to predict the next few words.\nIn the above table, we use 1,024 units for memory cells, 512 units for the input embedding layer in the LM-LSTM and SA-LSTM. We also use a hidden layer 512 units with dropout of 50% between the last hidden state and the classifier. We continue to use these settings in the following experiments except with 30 units in the final hidden layer.\nIn Table 3, we present some examples from the IMDB dataset that are correctly classified by SALSTM but not by a bigram NBSVM model. These examples often have long-term dependencies or have sarcasm that is difficult to find by solely looking at short phrases."}, {"heading": "4.2 Sentiment analysis experiments with Rotten Tomatoes and the positive effects of additional unlabeled data", "text": "The success on the IMDB dataset convinces us to test our methods on another sentiment analysis task to see if similar gains can be obtained. The benchmark of focus in this experiment is the Rotten Tomatoes dataset [25].2 The dataset has 10,662 documents, which are randomly split into 80% for training, 10% for validation and 10% for test. The average length of each document is 22 words and the maximum length is 52 words. Thus compared to IMDB, this dataset is smaller both in terms of the number of documents and the number of words per document.\n2http://www.cs.cornell.edu/people/pabo/movie-review-data/\nOur first observation is that it is easier to train LSTMs on this dataset than on the IMDB dataset and the gaps between LSTMs, LM-LSTMs and SA-LSTMs are smaller than before. This is because movie reviews in Rotten Tomatoes are sentences whereas reviews in IMDB are paragraphs.\nAs this dataset is small, our methods tend to severely overfit the training set. Combining SA-LSTMs with 95% input embedding and 50% word dropout improves generalization and allows the model to achieve 20.3% test set error. Further tuning the hyperparameters on the validation set yields 19.3% test error.\nTo better the performance, we add unlabeled data from the IMDB dataset in the previous experiment and Amazon movie reviews [22] to the autoencoder training stage.3 We also run a control experiment where we use the pretrained word vectors trained by word2vec from Google News.\nThe results for this set of experiments are shown in Table 4. Our observation is that if we use the word vectors from word2vec, there is only a small gain of 0.5%. This is perhaps because the recurrent weights play an important role in our model and are not initialized properly in this experiment. However, if we use IMDB to pretrain the sequence autoencoders, the error decreases from 20.5% to 18.6%, nearly a 2% gain in accuracy; if we use Amazon reviews, a larger unlabeled dataset (7.9\n3The dataset is available at http://snap.stanford.edu/data/web-Amazon.html, which has 34 million general product reviews, but we only use 7.9 million movie reviews in our experiments.\nmillion movie reviews), to pretrain the sequence autoencoders, the error goes down to 16.7% which is another 2% gain in accuracy.\nThis brings us to the question of how well this method of using unlabeled data fares compared to adding more labeled data. As argued by Socher et al. [29], a reason of why the methods are not perfect yet is the lack of labeled training data, they proposed to use more labeled data by labeling an addition of 215,154 phrases created by the Stanford Parser. The use of more labeled data allowed their method to achieve around 15% error in the test set, an improvement of approximately 5% over older methods with less labeled data.\nWe compare our method to their reported results [29] on sentence-level classification. As our method does not have access to valuable labeled data, one might expect that our method is severely disadvantaged and should not perform on the same level. However, with unlabeled data and sequence autoencoders, we are able to obtain 16.7%, ranking second amongst many other methods that have access to a much larger corpus of labeled data. The fact that unlabeled data can compensate for the lack of labeled data is very significant as unlabeled data are much cheaper than labeled data. The results are shown in Table 5."}, {"heading": "4.3 Text classification experiments with 20 newsgroups", "text": "The experiments so far have been done on datasets where the number of tokens in a document is relatively small, a few hundred words. Our question becomes whether it is possible to use SALSTMs for tasks that have a substantial number of words. For that purpose, we carry out the next experiments on the 20 newsgroups dataset [16].4 There are 11,293 documents in the training set and 7,528 in the test set. We use 15% of the training documents as a validation set. Each document is an email with an average length of 267 words and a maximum length of 11,925 words. Attachments, PGP keys, duplicates and empty messages are removed. As the newsgroup documents are long, it was previously considered improbable for recurrent networks to learn anything from the dataset. The best methods are often simple bag-of-words.\nWe repeat the same experiments with LSTMs and SA-LSTMs on this dataset. Similar to observations made in previous experiments, SA-LSTMs are generally more stable to train than LSTMs. To improve generalization of the models, we again use input embedding dropout and word dropout chosen on the validation set. With 70% input embedding dropout and 75% word dropout, SA-LSTM achieves 15.6% test set error which is much better than previous classifiers in this dataset. Results are shown in Table 6."}, {"heading": "4.4 Character-level document classification experiments with DBpedia", "text": "In this set of experiments, we turn our attention to another challenging task of categorizing Wikipedia pages by reading character-by-character inputs. The dataset of attention is the DBpedia\n4http://qwone.com/\u02dcjason/20Newsgroups/\ndataset [19], which was also used to benchmark convolutional neural nets in Zhang and LeCun [38]. DBpedia had no duplication or tainting issues from the outset so we compare experimental results on this dataset. DBpedia is a crowd-sourced effort to extract information from Wikipedia and categorize it into an ontology.\nFor this experiment, we follow the same procedure suggested in Zhang and LeCun [38]. The task is to classify DBpedia abstracts into one of 14 categories after reading the character-by-character input. The dataset is split into 560,000 training examples and 70,000 test examples. A DBpedia document has an average of 300 characters while the maximum length of all documents is 13,467 characters. As this dataset is large, overfitting is not an issue and thus we do not perform any dropout on the input or recurrent layers. For this dataset, we use a two-layered LSTM, each layer has 512 hidden units and and the input embedding has 128 units.\nIn this dataset, we find that the linear label gain as described in Section 3 is an effective mechanism to inject gradients to earlier steps in LSTMs. This linear gain method works well and achieves 1.32% test set error, which is better than SA-LSTM. Combining SA-LSTM and the linear gain method achieves 1.19% test set error, a significant improvement from the results of convolutional networks as shown in Table 7."}, {"heading": "4.5 Object classification experiments with CIFAR-10", "text": "In these experiments, we attempt to see if our pre-training methods extend to non-textual data. To do this, we train a LSTM on the CIFAR-10 image dataset, consisting of 60,000 32x32 colour images divided into 10 classes. The input at each timestep of the LSTM is an entire row of pixels and we predict the class of the image after reading the final row. We use the same method as in [15] to perform data augmentation. We also trained a LSTM to do next row prediction given the current row (we denote this as LM-LSTM) and a LSTM to autoencode the image by rows (SA-LSTM). The loss function during unsupervised learning is the Euclidean L2 distance between the predicted and\nthe target row. We then fine-tune these on the classification task and present the classification results in Table 8. While we do not achieve the results attained by state of the art convolutional networks, our 2-layer pretrained LM-LSTM is able to exceed the results of the baseline convolutional DBN model [14] despite not using any convolutions and outperforms the non pre-trained LSTM."}, {"heading": "5 Discussion", "text": "In this paper, we showed that it is possible to use LSTM recurrent networks for NLP tasks such as document classification. Further, we demonstrated that a language model or a sequence autoencoder can help stabilize the learning in LSTM recurrent networks. On five benchmarks that we tried, LSTMs can reach or surpass the performance levels of all previous baselines.\nAcknowledgements: We thank Oriol Vinyals, Ilya Sutskever, Greg Corrado, Vijay Vasudevan, Manjunath Kudlur, Rajat Monga, Matthieu Devin, and the Google Brain team for their help."}], "references": [{"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R.K. Ando", "T. Zhang"], "venue": "J. Mach. Learn. Res., 6:1817\u20131853, December", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Datasets for single-label text categorization", "author": ["A. Cardoso-Cachopo"], "venue": "http://web.ist.utl.pt/acardoso/datasets/,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Listen, attend and spell", "author": ["William Chan", "Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1508.01211,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "End-to-end continuous speech recognition using attention-based recurrent nn: First results", "author": ["J. Chorowski", "D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.1602,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic ratio matching of RBMs for sparse high-dimensional inputs", "author": ["Y. Dauphin", "Y. Bengio"], "venue": "NIPS,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["F.A. Gers", "J. Schmidhuber", "F. Cummins"], "venue": "Neural Computation,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "LSTM: A search space odyssey", "author": ["K. Greff", "R.K. Srivastava", "J. Koutn\u0131\u0301k", "B.R. Steunebrink", "J. Schmidhuber"], "venue": "In ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": "A Field Guide to Dynamical Recurrent Neural Networks,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S. Jean", "K. Cho", "R. Memisevic", "Y. Bengio"], "venue": "ICML,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["R. Johnson", "T. Zhang"], "venue": "NAACL,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "EMNLP,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Skipthought vectors", "author": ["R. Kiros", "Y. Zhu", "R. Salakhutdinov", "R.S. Zemel", "A. Torralba", "R. Urtasun", "S. Fidler"], "venue": "NIPS,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional deep belief networks on CIFAR-10", "author": ["A. Krizhevsky"], "venue": "Technical report, University of Toronto,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Newsweeder: Learning to filter netnews", "author": ["K. Lang"], "venue": "ICML,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1995}, {"title": "Learning algorithms for the classification restricted boltzmann machine", "author": ["H. Larochelle", "M. Mandel", "R. Pascanu", "Y. Bengio"], "venue": "JMLR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "ICML,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "DBpedia \u2013 a large-scale, multilingual knowledge base extracted from wikipedia", "author": ["J. Lehmann", "R. Isele", "M. Jakob", "A. Jentzsch", "D. Kontokostas", "P.N. Mendes", "S. Hellmann", "M. Morsey", "P. van Kleef", "S. Auer"], "venue": "Semantic Web,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["T. Luong", "I. Sutskever", "Q.V. Le", "O. Vinyals", "W. Zaremba"], "venue": "arXiv preprint arXiv:1410.8206,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning word vectors for sentiment analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "ACL,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Hidden factors and hidden topics: understanding rating dimensions with review text", "author": ["J. McAuley", "J. Leskovec"], "venue": "RecSys, pages 165\u2013172. ACM,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Beyond short snippets: Deep networks for video classification", "author": ["J.Y.H. Ng", "M.J. Hausknecht", "S. Vijayanarasimhan", "O. Vinyals", "R. Monga", "G. Toderici"], "venue": "CVPR,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["B. Pang", "L. Lee"], "venue": "ACL,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning representations by back-propagating errors", "author": ["D. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1986}, {"title": "Neural responding machine for short-text conversation", "author": ["L. Shang", "Z. Lu", "H. Li"], "venue": "EMNLP,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C.D. Manning", "A.Y. Ng"], "venue": "EMNLP,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "EMNLP,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised learning of video representations using LSTMs", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "ICML,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Grammar as a foreign language", "author": ["O. Vinyals", "L. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton"], "venue": "NIPS,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "A neural conversational model", "author": ["O. Vinyals", "Q.V. Le"], "venue": "ICML Deep Learning Workshop,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["S.I. Wang", "C.D. Manning"], "venue": "ACL,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Beyond regression: New tools for prediction and analysis in the behavioral sciences", "author": ["P.J. Werbos"], "venue": "PhD thesis, Harvard,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1974}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Character-level convolutional networks for text classification", "author": ["X. Zhang", "Y. LeCun"], "venue": "NIPS,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 35, "context": "Recurrent neural networks (RNNs) are powerful tools for modeling sequential data, yet training them by back-propagation through time [36, 26] can be difficult [8].", "startOffset": 133, "endOffset": 141}, {"referenceID": 25, "context": "Recurrent neural networks (RNNs) are powerful tools for modeling sequential data, yet training them by back-propagation through time [36, 26] can be difficult [8].", "startOffset": 133, "endOffset": 141}, {"referenceID": 7, "context": "Recurrent neural networks (RNNs) are powerful tools for modeling sequential data, yet training them by back-propagation through time [36, 26] can be difficult [8].", "startOffset": 159, "endOffset": 162}, {"referenceID": 8, "context": "On a number of document classification tasks, we find that it is possible to train Long Short-Term Memory recurrent networks (LSTM RNNs) [9] to achieve good performance with careful tuning of hyperparameters.", "startOffset": 137, "endOffset": 140}, {"referenceID": 15, "context": "In our experiments on document classification with 20 Newsgroups [16] and DBpedia [19], and sentiment analysis with IMDB [21] and Rotten Tomatoes [25], LSTMs pretrained by recurrent language models or sequence autoencoders are usually better than LSTMs initialized randomly.", "startOffset": 65, "endOffset": 69}, {"referenceID": 18, "context": "In our experiments on document classification with 20 Newsgroups [16] and DBpedia [19], and sentiment analysis with IMDB [21] and Rotten Tomatoes [25], LSTMs pretrained by recurrent language models or sequence autoencoders are usually better than LSTMs initialized randomly.", "startOffset": 82, "endOffset": 86}, {"referenceID": 20, "context": "In our experiments on document classification with 20 Newsgroups [16] and DBpedia [19], and sentiment analysis with IMDB [21] and Rotten Tomatoes [25], LSTMs pretrained by recurrent language models or sequence autoencoders are usually better than LSTMs initialized randomly.", "startOffset": 121, "endOffset": 125}, {"referenceID": 24, "context": "In our experiments on document classification with 20 Newsgroups [16] and DBpedia [19], and sentiment analysis with IMDB [21] and Rotten Tomatoes [25], LSTMs pretrained by recurrent language models or sequence autoencoders are usually better than LSTMs initialized randomly.", "startOffset": 146, "endOffset": 150}, {"referenceID": 0, "context": "We believe our semi-supervised approach (as also argued by [1]) has some advantages over other unsupervised sequence learning methods, e.", "startOffset": 59, "endOffset": 62}, {"referenceID": 17, "context": ", Paragraph Vectors [18], because it can allow for easy fine-tuning.", "startOffset": 20, "endOffset": 24}, {"referenceID": 12, "context": "Our semi-supervised learning approach is related to Skip-Thought vectors [13], with two differences.", "startOffset": 73, "endOffset": 77}, {"referenceID": 30, "context": "[31], which has been successfully used for machine translation [20, 10], text parsing [32], image captioning [34], video analysis [30], speech recognition [4, 3] and conversational modeling [27, 33].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[31], which has been successfully used for machine translation [20, 10], text parsing [32], image captioning [34], video analysis [30], speech recognition [4, 3] and conversational modeling [27, 33].", "startOffset": 63, "endOffset": 71}, {"referenceID": 9, "context": "[31], which has been successfully used for machine translation [20, 10], text parsing [32], image captioning [34], video analysis [30], speech recognition [4, 3] and conversational modeling [27, 33].", "startOffset": 63, "endOffset": 71}, {"referenceID": 31, "context": "[31], which has been successfully used for machine translation [20, 10], text parsing [32], image captioning [34], video analysis [30], speech recognition [4, 3] and conversational modeling [27, 33].", "startOffset": 86, "endOffset": 90}, {"referenceID": 33, "context": "[31], which has been successfully used for machine translation [20, 10], text parsing [32], image captioning [34], video analysis [30], speech recognition [4, 3] and conversational modeling [27, 33].", "startOffset": 109, "endOffset": 113}, {"referenceID": 29, "context": "[31], which has been successfully used for machine translation [20, 10], text parsing [32], image captioning [34], video analysis [30], speech recognition [4, 3] and conversational modeling [27, 33].", "startOffset": 130, "endOffset": 134}, {"referenceID": 3, "context": "[31], which has been successfully used for machine translation [20, 10], text parsing [32], image captioning [34], video analysis [30], speech recognition [4, 3] and conversational modeling [27, 33].", "startOffset": 155, "endOffset": 161}, {"referenceID": 2, "context": "[31], which has been successfully used for machine translation [20, 10], text parsing [32], image captioning [34], video analysis [30], speech recognition [4, 3] and conversational modeling [27, 33].", "startOffset": 155, "endOffset": 161}, {"referenceID": 26, "context": "[31], which has been successfully used for machine translation [20, 10], text parsing [32], image captioning [34], video analysis [30], speech recognition [4, 3] and conversational modeling [27, 33].", "startOffset": 190, "endOffset": 198}, {"referenceID": 32, "context": "[31], which has been successfully used for machine translation [20, 10], text parsing [32], image captioning [34], video analysis [30], speech recognition [4, 3] and conversational modeling [27, 33].", "startOffset": 190, "endOffset": 198}, {"referenceID": 22, "context": "We also find that recurrent language models [23] can be used as a pretraining method for LSTMs.", "startOffset": 44, "endOffset": 48}, {"referenceID": 8, "context": "In our experiments, we use LSTM recurrent networks [9] because they are generally better than RNNs.", "startOffset": 51, "endOffset": 54}, {"referenceID": 5, "context": "Our LSTM implementation is standard and has input, forget, and output gates [6, 7].", "startOffset": 76, "endOffset": 82}, {"referenceID": 6, "context": "Our LSTM implementation is standard and has input, forget, and output gates [6, 7].", "startOffset": 76, "endOffset": 82}, {"referenceID": 23, "context": "We also experiment with the approach of putting the label at every timestep and linearly increasing the weights of the prediction objectives from 0 to 1 [24].", "startOffset": 153, "endOffset": 157}, {"referenceID": 30, "context": "In our experiments with LSTMs, we follow the basic recipes as described in [31] by clipping the cell outputs and gradients.", "startOffset": 75, "endOffset": 79}, {"referenceID": 20, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The previous baselines are bag-of-words, ConvNets [12] or Paragraph Vectors [18].", "startOffset": 50, "endOffset": 54}, {"referenceID": 17, "context": "The previous baselines are bag-of-words, ConvNets [12] or Paragraph Vectors [18].", "startOffset": 76, "endOffset": 80}, {"referenceID": 36, "context": "With random embedding dimension dropout [37] and random word dropout (not published previously), we are able to reach performance of around 86.", "startOffset": 40, "endOffset": 44}, {"referenceID": 20, "context": "Full+Unlabeled+BoW [21] 11.", "startOffset": 19, "endOffset": 23}, {"referenceID": 20, "context": "11% WRRBM + BoW (bnc) [21] 10.", "startOffset": 22, "endOffset": 26}, {"referenceID": 34, "context": "77% NBSVM-bi (Na\u0131\u0308ve Bayes SVM with bigrams) [35] 8.", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "78% seq2-bown-CNN (ConvNet with dynamic pooling) [11] 7.", "startOffset": 49, "endOffset": 53}, {"referenceID": 17, "context": "67% Paragraph Vectors [18] 7.", "startOffset": 22, "endOffset": 26}, {"referenceID": 24, "context": "The benchmark of focus in this experiment is the Rotten Tomatoes dataset [25].", "startOffset": 73, "endOffset": 77}, {"referenceID": 21, "context": "To better the performance, we add unlabeled data from the IMDB dataset in the previous experiment and Amazon movie reviews [22] to the autoencoder training stage.", "startOffset": 123, "endOffset": 127}, {"referenceID": 27, "context": "MV-RNN [28] 21.", "startOffset": 7, "endOffset": 11}, {"referenceID": 34, "context": "0% NBSVM-bi [35] 20.", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "6% CNN-rand [12] 23.", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "5% CNN-non-static (ConvNet with vectors from word2vec Google News) [12] 18.", "startOffset": 67, "endOffset": 71}, {"referenceID": 28, "context": "[29], a reason of why the methods are not perfect yet is the lack of labeled training data, they proposed to use more labeled data by labeling an addition of 215,154 phrases created by the Stanford Parser.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "We compare our method to their reported results [29] on sentence-level classification.", "startOffset": 48, "endOffset": 52}, {"referenceID": 28, "context": "NB [29] 18.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "2% SVM [29] 20.", "startOffset": 7, "endOffset": 11}, {"referenceID": 28, "context": "6% BiNB [29] 16.", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": "9% VecAvg [29] 19.", "startOffset": 10, "endOffset": 14}, {"referenceID": 28, "context": "9% RNN [29] 17.", "startOffset": 7, "endOffset": 11}, {"referenceID": 28, "context": "6% MV-RNN [29] 17.", "startOffset": 10, "endOffset": 14}, {"referenceID": 28, "context": "1% RNTN [29] 14.", "startOffset": 8, "endOffset": 12}, {"referenceID": 15, "context": "For that purpose, we carry out the next experiments on the 20 newsgroups dataset [16].", "startOffset": 81, "endOffset": 85}, {"referenceID": 16, "context": "Hybrid Class RBM [17] 23.", "startOffset": 17, "endOffset": 21}, {"referenceID": 4, "context": "8% RBM-MLP [5] 20.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "5% SVM + Bag-of-words [2] 17.", "startOffset": 22, "endOffset": 25}, {"referenceID": 1, "context": "1% Na\u0131\u0308ve Bayes [2] 19.", "startOffset": 16, "endOffset": 19}, {"referenceID": 18, "context": "dataset [19], which was also used to benchmark convolutional neural nets in Zhang and LeCun [38].", "startOffset": 8, "endOffset": 12}, {"referenceID": 37, "context": "dataset [19], which was also used to benchmark convolutional neural nets in Zhang and LeCun [38].", "startOffset": 92, "endOffset": 96}, {"referenceID": 37, "context": "For this experiment, we follow the same procedure suggested in Zhang and LeCun [38].", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "We use the same method as in [15] to perform data augmentation.", "startOffset": 29, "endOffset": 33}, {"referenceID": 13, "context": "While we do not achieve the results attained by state of the art convolutional networks, our 2-layer pretrained LM-LSTM is able to exceed the results of the baseline convolutional DBN model [14] despite not using any convolutions and outperforms the non pre-trained LSTM.", "startOffset": 190, "endOffset": 194}, {"referenceID": 13, "context": "Convolution DBNs [14] 21.", "startOffset": 17, "endOffset": 21}], "year": 2015, "abstractText": "We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a \u201cpretraining\u201d step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.", "creator": "LaTeX with hyperref package"}}}