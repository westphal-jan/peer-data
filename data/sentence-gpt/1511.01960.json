{"id": "1511.01960", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2015", "title": "An Action Language for Multi-Agent Domains: Foundations", "abstract": "In multi-agent domains (MADs), an agent's action may not just change the world and the agent's knowledge and beliefs about the world, but also may change other agents' knowledge and beliefs about the world and their knowledge and beliefs about other agents' knowledge and beliefs about the world. The goals of an agent in a multi-agent world may involve manipulating the knowledge and beliefs of other agents' and again, not just their knowledge/belief about the world, but also their knowledge about other agents' knowledge about the world. Our goal is to present an action language (mA+) that has the necessary features to address the above aspects in representing and RAC in MADs.\n\n\nIn the same article, we will present our work in different ways in addition to showing how we can improve the tools that we offer to manage and manipulate the language and beliefs of our clients and clients by improving tools available to us as a whole.\nWe will continue to share our work and tools with our clients through other methods, like improving the tools that we offer to manage and manipulate the language and beliefs of our clients and clients by improving the tools available to us as a whole. We will also use the tools available to manage and manipulate the language and beliefs of our clients and clients by improving the tools available to us as a whole. We will use the tools available to manage and manipulate the language and beliefs of our clients and clients by improving the tools available to us as a whole. We will also use the tools available to manage and manipulate the language and beliefs of our clients and clients by improving the tools available to us as a whole. We will also use the tools available to manage and manipulate the language and beliefs of our clients and clients by improving the tools available to us as a whole. We will use the tools available to manage and manipulate the language and beliefs of our clients and clients by improving the tools available to us as a whole.\nWe will continue to use the tools available to manage and manipulate the language and beliefs of our clients and clients by improving the tools available to us as a whole. We will use the tools available to manage and manipulate the language and beliefs of our clients and clients by improving the tools available to us as a whole. We will use the tools available to manage and manipulate the language and beliefs of our clients and clients by improving the tools available to us as a whole.\nWe will continue to use the tools available to manage and manipulate the language and beliefs of our clients and clients by improving the tools available", "histories": [["v1", "Fri, 6 Nov 2015 00:16:19 GMT  (439kb,D)", "http://arxiv.org/abs/1511.01960v1", "55 pages, 18 figures"]], "COMMENTS": "55 pages, 18 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["chitta baral", "gregory gelfond", "enrico pontelli", "tran cao son"], "accepted": false, "id": "1511.01960"}, "pdf": {"name": "1511.01960.pdf", "metadata": {"source": "CRF", "title": "An Action Language for Multi-Agent Domains: Foundations", "authors": ["Chitta Baral", "Gregory Gelfond", "Enrico Pontelli", "Tran Cao Son"], "emails": ["chitta@asu.edu", "ggelfond@asu.edu", "epontell@cs.nmsu.edu", "tson@cs.nmsu.edu"], "sections": [{"heading": null, "text": "about the world, but also their knowledge about other agents\u2019 knowledge about the world. The goal of this paper is to present an action language, called mA+, that has the necessary features to address the above aspects in representing and reasoning about actions and change in multi-agent domains.\nThis action language can be viewed as a generalization of the single-agent action languages extensively studied in the literature, to the case of multi-agent domains. The language allows the representation of and reasoning about different types of actions that an agent can perform in a domain where many other agents might be present\u2014such as world-altering actions, sensing actions, and announcement/communication actions. The action language also allows the specification of agents\u2019 dynamic awareness of action occurrences which has future implications on what agents\u2019 know about the world and other agents\u2019 knowledge about the world. The language mA+ considers three different types of awareness: full awareness, partial awareness, and complete oblivion of an action occurrence and its effects. This keeps the language simple, yet powerful enough to address a large variety of knowledge manipulation scenarios in multi-agent domains.\nThe semantics of the language relies on the notion of state, which is described by a pointed Kripke model and is used to encode the agent\u2019s knowledge and the real state of the world. The semantics is defined by a transition function that maps pairs of actions and states into sets of states. The paper illustrates properties of the action theories, including properties that guarantee finiteness of the set of initial states and their practical implementability. Finally, the paper relates mA+ to other related formalisms that contribute to reasoning about actions in multi-agent domains.\nKeywords: Action Languages, Multi-Agent Domains, Planning."}, {"heading": "1 Introduction and Motivation", "text": "Reasoning about actions and change has been a research focus since the early days of artificial intelligence [1]; languages for representing actions and their effects were proposed soon after [2]. Although the early papers on this [2] did not include formal semantics, papers with formal semantics came some years later [3]. The approach adopted in this paper is predominantly influenced by the methodology for representing and reasoning about actions and change (RAC) proposed by Gelfond and Lifschitz [4]. In this approach, actions of agents are described in a high-level language, with an English-like syntax and a transition function based semantics. Among other things, action languages provide a succinct way for representing dynamic domains. The approach proposed in this paper is also related to action description languages developed for planning, such as [5, 6].\nOver the years, several action languages (e.g., A, B, and C) have been developed [7]. Each of these languages addresses some important problems in RAC (e.g., the ramification problem, concurrency, actions with duration, knowledge of agents). Action languages have also provided the foundations for several successful approaches to automated\n1Often, we will simply say \u201cknowledge\u201d to mean both \u201cknowledge\u201d and \u201cbeliefs.\u201d This will be clear from the context.\nar X\niv :1\n51 1.\n01 96\n0v 1\n[ cs\n.A I]\n6 N\nov 2\n01 5\nplanning; for example, the language C is used in the planner C-PLAN [8] and the language B is used in CPA [9]. However, the main focus of these research efforts has been about reasoning within single agent domains.\nIn single agent domains, reasoning about actions and change mainly involves reasoning about what is true in the world, what the agent knows about the world, how the agent can manipulate the world (using world-changing actions) to reach particular states, and how the agent (using sensing actions) can learn unknown aspects of the world. In multi-agent domains an agent\u2019s action may not just change the world and the agent\u2019s knowledge about the world, but also may change other agents\u2019 knowledge about the world and their knowledge about other agents\u2019 knowledge about the world. Similarly, goals of an agent in a multi-agent world may involve manipulating the knowledge of other agents\u2014in particular, this may involve not just their knowledge about the world, but also their knowledge about other agents\u2019 knowledge about the world. Although there is a large body of research on multi-agent planning [10, 11, 12, 13, 14, 15, 16, 17, 18], very few efforts address the above aspects of multi-agent domains which pose a number of new research challenges in representing and reasoning about actions and change. The following example illustrates some of these issues.\nExample 1 (Three Agents and the Coin Box) Three agents, A, B, and C, are in a room. In the middle of the room there is a box containing a coin. It is common knowledge that:\n\u2022 None of the agents knows whether the coin lies heads or tails up; \u2022 The box is locked and one needs to have a key to open it; only agent A has the key of the box; \u2022 In order to learn whether the coin lies heads or tails up, an agent can peek into the box. This, however, requires\nthe box to be open; \u2022 If an agent is looking at the box and someone peeks into the box, then the first agent will notice this fact and\nwill be able to conclude that the second agent knows the status of the coin; yet, the first agent\u2019s knowledge about which face of the coin is up does not change; \u2022 Distracting an agent causes that agent to not look at the box; \u2022 Signaling an agent to look at the box causes this agent to look at the box; \u2022 Announcing that the coin lies heads or tails up will make this a common knowledge among the agents that are\nlistening. Suppose that the agent A would like to know whether the coin lies heads or tails up. She would also like to let the agent B know that she knows this fact. However, she would like to keep this information secret from C. (Note that the last two sentences express goals that are about agents\u2019 knowledge about other agents\u2019 knowledge.) Intuitively, she could achieve her goal by:\n1. Distracting C from looking at the box;\n2. Signaling B to look at the box;\n3. Opening the box; and\n4. Peeking into the box. 2\nThis simple story presents a number of challenges for research in representing and reasoning about actions and their effects in multi-agent domains. In particular:\n\u2022 The domain contains several types of actions:\n\u2013 Actions that allow the agents to change the state of the world (e.g., opening the box); \u2013 Actions that change the knowledge of the agents (e.g, peeking into the box, announcing head/tail); \u2013 Actions that manipulate the beliefs of other agents (e.g., peeking while other agents are looking); and \u2013 Actions that change the observability of agents with respect to awareness about future actions (e.g., distract\nand signal actions before peeking into the box).\nWe observe that the third and fourth types of actions are not considered in single agent systems.\n\u2022 The reasoning process that helpsA to realize that steps (1)-(4) will achieve her goal requiresA\u2019s ability to reason about the effects of her actions on several entities:\n\u2013 The state of the world\u2014e.g., opening the box causes the box to become open; \u2013 The agents\u2019 awareness of the environment and of other agents\u2019 actions\u2014e.g., distracting or signaling an\nagent causes this agent not to look or to look at the box; and\n\u2013 The knowledge of other agents about her own knowledge\u2014e.g., someone following her actions would know what she knows.\nWhile the first requirement is the same as for an agent in single agent domains, the last two are specific to multi-agent domains.\nWith respect to planning, the above specifics of multi-agent systems raise an interesting question:\n\u201cHow can one generate a plan for the agent A to achieve her goal, given the description in Example 1?\u201d\nTo the best of our knowledge, except a dynamic epistemic modeling system called DEMO [19], there exists no automated planner that can address the complete range of issues as exemplified in Example 1. This is in stark contrast to the landscape of automated planning for single agent domains, where we can find several automated planners capable of generating plans consisting of hundreds of actions within seconds\u2014especially building on recent advances in search-based planning.\nAmong the main reasons for the lack of planning systems capable of dealing with the issues like those shown in Example 1 are: (i) lack of action-based formalisms that can address the above mentioned issues and that can actually be orchestrated, and (ii) the fact that logical approaches to reasoning about knowledge of agents in multi-agent domains are mostly model-theoretical, and not as amenable to an implementation in search-based planning systems. This is not the case for single-agent domains, where the de-facto standard Planning Domain Description Language (PDDL) provides a transition-based semantics which allows the creation of highly efficient heuristic search based planners.\nIn terms of related work, multi-agent actions have been explored in Dynamic Epistemic Logics (DEL) (e.g., [20, 21, 22, 23, 24]). However, as discussed later in the paper, DEL does not offer an intuitive view of how to orchestrate or execute a single multi-agent action. In addition, the complex representation of multi-agent actions (akin to Kripke structures) drastically increases the number of possible multi-agent actions\u2014thus, making it difficult to search for multi-agent action sequences. The research in DEL has also not addressed some critical aspects of multi-agent searchbased planning, such as the determination of the initial state of a planning domain instance. Moreover, research in DEL did not explore the link between the state of the world and the observability encoded in multi-agent actions, and hence preventing the dynamic evolution of the observational capabilities and awareness of agents with respect to future actions. In some ways, the DEL approach is similar to the formulations of belief updates (e.g., [25, 26, 27]), and most of the differences and similarities between belief updates and reasoning about actions carry over to the differences and similarities between DEL and our formulation of RAC in multi-agent domains. We will elaborate on these differences in a later section of the paper."}, {"heading": "1.1 Contributions and Assumptions", "text": "Our goal in this paper is to develop a framework that allows reasoning about actions and their effects in a multi-agent domain; the framework is expected to address the above-mentioned issues, e.g., actions\u2019 capability to modify agents\u2019 knowledge about other agents\u2019 knowledge. To this end, we propose a high-level action language for representing and reasoning about actions in multi-agent domains. The language provides an initial set of elements of a planning domain description language for multi-agent systems. The main contributions of the paper are:\n\u2022 The action language mA+, which considers different types of actions\u2014such as world-altering actions, announcement actions, and sensing actions\u2014for formalizing multi-agent domains, along with actions that affect the dynamic awareness and observation capabilities of the agents;\n\u2022 A transition function based semantics formA+, that enables hypothetical reasoning and planning in multi-agent domains;\n\u2022 The notion of definite action theories, that characterizes action theories for which the computation of the initial state is possible\u2014which is an essential component in the implementation of a heuristic search-based planner for domains described in mA+; and\n\u2022 Several theorems relating the semantics of mA+ to multi-agent actions characterizations using update models in DEL of [20].\nIn developing mA+, we make several assumptions and design decisions. The key decision is that actions in our formalism can be effectively executed.2 We also assume that actions are deterministic. This assumption can be lifted in a relatively simple manner by generalizing the treatment of non-deterministic actions, state constraints, and parallel actions studied in the context of single-agent domains.\nNext, although we have mentioned both knowledge and beliefs, in this paper we will follow [24, 20] and focus only on formalizing the changes of beliefs of agents after the execution of actions. Following the considerations in [22], the epistemic operators used in this paper can be read as \u201cto the best my information.\u201d Note that, in a multi-agent system, there may be a need to distinguish between knowledge and beliefs of an agent about the world. For example, let us consider Example 1 and let us denote with p the proposition \u201cnobody knows whether the coin lies heads or tails up.\u201d Initially, all of the three agents know that p is true. However, after agent A executes the sequence of actions (1)-(4), A will know that p is false. Furthermore, B also knows that p is false, thanks to her awareness of A\u2019s execution of the actions of opening the box and looking into it. However, C\u2014being unaware of the execution of the actions performed byA\u2014will still believe that p is true. If this were considered as a part ofC\u2019s knowledge, then C would result in having false knowledge.\nThe investigation of the discrepancy between knowledge and beliefs has been an intense research topic in dynamic epistemic logic and in reasoning about knowledge, which has lead to the development of several modal logics (e.g., [28, 24]). Since our main aim is the development of an action language for hypothetical reasoning and planning, we will be concerned mainly with beliefs of agents. A consequence of this choice is that we will not consider situations where sensing actions, or other knowledge acquisition actions, are employed to correct a wrong belief of agents. For example, an agent will not be able to use a sensing action to realize that her beliefs about a certain property of the world is wrong; or an agent will not be able to use an announcement action to correct an incorrect belief of other agents. Some preliminary steps in this direction have been explored in the context of the DEL framework [21, 29]. We leave the development of an action based formalism that takes into consideration the differences between beliefs and knowledge as future work."}, {"heading": "1.2 Paper Organization", "text": "The rest of the paper is organized as follows. Section 2 reviews the basics definitions and notation of a modal logic with belief operators and the update model based approach to reasoning about actions in multi-agent domains. This section also describes a set of operators on Kripke structures, that will be used in defining the semantics of mA+. Section 3 presents the syntax of mA+. Section 4 defines the transition function of mA+ which maps pairs of actions and states into states; the section also presents the entailment relation between mA+ action theories and queries. Section 5 explores the modeling of the semantics of mA+ using the update models approach. Section 6 identifies a class of mA+ theories whose initial states can be finitely characterized and effectively computed. Section 7 provides an analysis of mA+ with respect to the existing literature, including a comparison with DEL. Section 8 provide some concluding remarks and directions for future work.\nFor simplicity of presentation, the proofs of the main lemmas, propositions, and theorems are placed in the appendix."}, {"heading": "2 Preliminaries", "text": "We begin with a review of the basic notions from the literature on formalizing knowledge and reasoning about effects of actions in multi-agent systems. Subsection 2.1 presents the notion of Kripke structures. Subsection 2.2 reviews the\n2This is not the case in DEL [24], where actions are complex graph structures, similar to Kripke structures, possibly representing a multi-modal formula, and it is not clear if and how such actions can be executed.\nnotion of update models developed by the dynamic epistemic logic community for reasoning about effects of actions in multi-agent systems. In Subsection 2.3, we investigate a collection of basic operators for modifying Kripke structures, that will be used in our semantic formalization in the following sections."}, {"heading": "2.1 Belief Formulae and Kripke Structures", "text": "Let us consider an environment with a set AG of n agents. The real state of the world (or real state, for brevity) is described by a set F of propositional variables, called fluents. We are concerned with the beliefs of agents about the environment and about the beliefs of other agents. For this purpose, we adapt the logic of knowledge and the notations used in [28, 24]. We associate to each agent i \u2208 AG a modal operator Bi and represent the beliefs of an agent as belief formulae in a logic extended with these operators. Formally, we define belief formulae as follows:\n\u2022 Fluent formulae: a fluent formula is a propositional formula built using the propositional variables in F and the traditional propositional operators \u2228, \u2227, \u2192, \u00ac, etc. In particular, a fluent atom is a formula containing just an element f \u2208 F , while a fluent literal is either a fluent atom f \u2208 F or its negation \u00acf . We will use > and \u22a5 to denote true and false, respectively.\n\u2022 Belief formulae: a belief formula is a formula in one of the following forms:\n\u2013 a fluent formula; \u2013 a formula of the form Bi\u03d5 where \u03d5 is a belief formula; \u2013 a formula of the form \u03d51 \u2228 \u03d52, \u03d51 \u2227 \u03d52, \u03d51 \u21d2 \u03d52, or \u00ac\u03d51 where \u03d51, \u03d52 are belief formulae; \u2013 a formula of the form E\u03b1\u03d5 or C\u03b1\u03d5, where \u03d5 is a formula and \u2205 6= \u03b1 \u2286 AG.\nFormulae of the form E\u03b1\u03d5 and C\u03b1\u03d5 are referred to as group formulae. Whenever \u03b1 = AG, we simply write E\u03d5 and C\u03d5 to denote E\u03b1\u03d5 and C\u03b1\u03d5, respectively. Let us denote with LAG the language of the belief formulae over F and AG.\nIntuitively, belief formulae are used to describe the beliefs of one agent concerning the state of the world as well as about the beliefs of other agents. For example, the formula B1B2p expresses the fact that \u201cAgent 1 believes that agent 2 believes that p is true,\u201d while B1f states that \u201cAgent 1 believes that f is true.\u201d\nIn what follows, we will simply talk about \u201cformulae\u201d instead of \u201cbelief formulae,\u201d whenever there is no risk of confusion. The notion of a Kripke structure is defined next.\nDefinition 1 (Kripke Structure) A Kripke structure is a tuple \u3008S, \u03c0,B1, . . . ,Bn\u3009, where\n\u2022 S is a set of worlds,\n\u2022 \u03c0 : S 7\u2192 2F is a function that associates an interpretation of F to each element of S, and\n\u2022 For 1 \u2264 i \u2264 n, Bi \u2286 S \u00d7 S is a binary relation over S.\nA pointed Kripke structure is a pair (M, s) where M = \u3008S, \u03c0,B1, . . . ,Bn\u3009 is a Kripke structure and s \u2208 S. In a pointed Kripke structure (M, s), we refer to s as the real (or actual) world.\nIntuitively, a Kripke structure describes the possible worlds envisioned by the agents\u2014and the presence of multiple worlds identifies uncertainty and presence of different beliefs. The relation (s1, s2) \u2208 Bi denotes that the belief of agent i about the real state of the world is insufficient for her to distinguish between the world described by s1 and the one described by s2. The world s in the state (M, s) identifies the world in M [S] that corresponds to the actual world.\nWe will often view a Kripke structure M = \u3008S, \u03c0,B1, . . . ,Bn\u3009 as a directed labeled graph, whose set of nodes is S and whose set of edges contains (s, i, t)3 if and only if (s, t) \u2208 Bi. (s, i, t) is referred to as an edge coming out of (resp. into) the world s (resp. t).\n3(s, i, t) denotes the edge from node s to node t, labeled by i.\nFor the sake of readability, we use M [S], M [\u03c0], and M [i] to denote the components S, \u03c0, and Bi of M , respectively. We write M [\u03c0](u) to denote the interpretation associated to u via \u03c0 and M [\u03c0](u)(\u03d5) to denote the truth value of a fluent formula \u03d5 with respect to the interpretation M [\u03c0](u).\nFollowing [24], we will refer to a pointed Kripke structure (M, s) as a state and often use these two terms interchangeably.\nIn keeping with the tradition of action languages, we will often refer to \u03c0(u) as the set of fluent literals defined by\n{f | f \u2208 F ,M [\u03c0](u)(f) = >} \u222a {\u00acf | f \u2208 F ,M [\u03c0](u)(f) = \u22a5}.\nGiven a consistent and complete set of literals S, i.e., |{f,\u00acf} \u2229 S| = 1 for every f \u2208 F , we write \u03c0(u) = S or M [\u03c0](u) = S to indicate that the interpretation M [\u03c0](u) is defined in such a way that \u03c0(u) = S.\nThe satisfaction relation between belief formulae and a state is defined as next.\nDefinition 2 Given a formula \u03d5, a Kripke structure M = \u3008S, \u03c0,B1, . . . ,Bn\u3009, and a world s \u2208 S:\n(i) (M, s) |= \u03d5 if \u03d5 is a fluent formula and \u03c0(s) |= \u03d5;\n(ii) (M, s) |= Bi\u03d5 if for each t such that (s, t) \u2208 Bi, (M, t) |= \u03d5;\n(iii) (M, s) |= \u00ac\u03d5 if (M, s) 6|= \u03d5;\n(iv) (M, s) |= \u03d51 \u2228 \u03d52 if (M, s) |= \u03d51 or (M, s) |= \u03d52;\n(v) (M, s) |= \u03d51 \u2227 \u03d52 if (M, s) |= \u03d51 and (M, s) |= \u03d52.\n(vi) (M, s) |= E\u03b1\u03d5 if (M, s) |= Bi\u03d5 for every i \u2208 \u03b1.\n(vii) (M, s) |= C\u03b1\u03d5 if (M, s) |= Ek\u03b1\u03d5 for every k \u2265 0, where E0\u03b1\u03d5 = \u03d5 and Ek+1\u03b1 = E\u03b1(Ek\u03b1\u03d5).\nFor a Kripke structure M and a formula \u03d5, M |= \u03d5 denotes the fact that (M, s) |= \u03d5 for each s \u2208 M [S], while |= \u03d5 denotes the fact that M |= \u03d5 for every Kripke structure M .\nExample 2 (State) Let us consider a simplified version of Example 1 in which the agents are concerned only with the status of the coin. The three agents A,B,C do not know whether the coin has \u2018heads\u2019 or \u2018tails\u2019 up and this is a common belief. Let us assume that the coin is heads up. The beliefs of the agents about the world and about the beliefs of other agents can be captured by the state of Figure 1.\nIn the figure, a circle represents a world. The name and interpretation of the world are written in the circle. Labeled edges between worlds denote the belief relations of the structure. A double circle identifies the real world.\nWe will occasionally be interested in Kripke structures that satisfy certain conditions. In particular, given a Kripke structure M = \u3008S, \u03c0,B1, . . . ,Bn\u3009 we identify the following properties:\n\u2022 K: for each agent i and formulae \u03d5,\u03c8, we have that M |= (Bi\u03d5 \u2227Bi(\u03d5\u21d2 \u03c8))\u21d2 Bi\u03c8.\n\u2022 T: for each agent i and formula \u03c8, we have that M |= Bi\u03c8 \u21d2 \u03c8.\n\u2022 4: for each agent i and formula \u03c8, we have that M |= Bi\u03c8 \u21d2 BiBi\u03c8.\n\u2022 5: for each agent i and formula \u03c8, we have that M |= \u00acBi\u03c8 \u21d2 Bi\u00acBi\u03c8.\n\u2022 D: for each agent i we have that M |= \u00acBi \u22a5.\nA Kripke structure is said to be a T-Kripke (4-Kripke, K-Kripke, 5-Krikpe, D-Kripke, respectively) structure if it satisfies property T (4, K, 5, D, respectively). A Kripke structure is said to be a S5 structure if it satisfies the properties K, T, 4, and 5. Consistency of a set of formulae is defined next.\nDefinition 3 A set of belief formulae X is said to be p-satisfiable (or p-consistent) for p \u2208 {S5,K,T,4,5} if there exists a p-Kripke structure M and a world s \u2208 M [S] such that (M, s) |= \u03c8 for every \u03c8 \u2208 X . In this case, (M, s) is referred to as a p-model of X.\nFinally, let us introduce a notion of equivalence between states.\nDefinition 4 A state (M, s) is equivalent to a state (M \u2032, s\u2032) if (M, s) |= \u03d5 iff (M \u2032, s\u2032) |= \u03d5 for every formula \u03d5 \u2208 LAG ."}, {"heading": "2.2 Update Models", "text": "Update models are used to describe transformations of (pointed) Kripke structures according to a predetermined transformation pattern. An update model uses structures similar to pointed Kripke structures and they describe the effects of a transformation on states using an update operator [20, 23].\nLet us start with some preliminary definitions. An LAG-substitution is a set {p1 \u2192 \u03d51, . . . , pk \u2192 \u03d5k}, where each pi is a distinct fluent in F and each \u03d5i \u2208 LAG . We will implicitly assume that for each p \u2208 F \\ {p1, . . . , pk}, the substitution contains p\u2192 p. SUBLAG denotes the set of all LAG-substitutions.\nDefinition 5 (Update Model) Given a set AG of n agents, an update model \u03a3 is a tuple \u3008\u03a3, R1, . . . , Rn, pre, sub\u3009 where\n(i) \u03a3 is a set, whose elements are called events;\n(ii) each Ri is a binary relation on \u03a3;\n(iii) pre : \u03a3\u2192 LAG is a function mapping each event e \u2208 \u03a3 to a formula in LAG; and\n(iv) sub : \u03a3\u2192 SUBLAG is a function mapping each event e \u2208 \u03a3 to a substitution in SUBLAG .\nAn update instance \u03c9 is a pair (\u03a3, e) where \u03a3 is an update model \u3008\u03a3, R1, . . . , Rn, pre, sub\u3009 and e, referred to as a designated event, is a member of \u03a3.\nIntuitively, an update model represents different views of an action occurrence which are associated with the observability of agents. Each view is represented by an event in \u03a3. The designated event is the one that agents who are aware of the action occurrence will observe. The relation Ri describes agent i\u2019s uncertainty on action execution\u2014i.e., if (\u03c3, \u03c4) \u2208 Ri and event \u03c3 is performed, then agent i may believe that event \u03c4 is executed instead. pre defines the action precondition and sub specifies the changes of fluent values after the execution of an action.\nDefinition 6 (Updates by an Update Model) LetM be a Kripke structure and \u03a3 = \u3008\u03a3, R1, . . . , Rn, pre, sub\u3009 be an update model. The update operator induced by \u03a3 defines a Kripke structure M \u2032 = M \u2297\u03a3, where:\n(i) M \u2032[S] = {(s, \u03c4) | s \u2208M [S], \u03c4 \u2208 \u03a3, (M, s) |= pre(\u03c4)};\n(ii) ((s, \u03c4), (s\u2032, \u03c4 \u2032)) \u2208M \u2032[i] iff (s, \u03c4), (s\u2032, \u03c4 \u2032) \u2208M \u2032[S], (s, s\u2032) \u2208M [i] and (\u03c4, \u03c4 \u2032) \u2208 Ri;\n(iii) For all f \u2208 F , M \u2032[\u03c0]((s, \u03c4)) |= f iff f \u2192 \u03d5 \u2208 sub(\u03c4) and (M, s)|=\u03d5.\nThe structure M \u2032 is obtained from the component-wise cross-product of the old structure M and the update model \u03a3, by (i) removing pairs (s, \u03c4) such that (M, s) does not satisfy the action precondition (checking for satisfaction of action\u2019s precondition), and (ii) removing links of the form ((s, \u03c4), (s\u2032, \u03c4 \u2032)) from the cross product of M [i] and Ri if (s, s\u2032) 6\u2208 M [i] or (\u03c4, \u03c4 \u2032) 6\u2208 Ri (ensuring that each agent\u2019s accessibility relation is updated according to the update model).\nAn update template is a pair (\u03a3,\u0393) where \u03a3 is an update model with the set of events \u03a3 and \u0393 \u2286 \u03a3. The update of a state (M, s) given an update template (\u03a3,\u0393) is a set of states, denoted by (M, s)\u2297 (\u03a3,\u0393), where\n(M, s)\u2297 (\u03a3,\u0393) = {(M \u2297\u03a3, (s, \u03c4)) | \u03c4 \u2208 \u0393, (M, s) |= pre(\u03c4)}\nRemark 1 In the following, we will often represent an update instance by a graph with rectangles representing events, double rectangles designated events, and labeled links between rectangles representing the relation of agents as in the graphical representation of a Kripke model."}, {"heading": "2.3 Kripke Structures Operators", "text": "The execution of an action in a multi-agent environment will change the real state of the world and/or the beliefs of agents. As we will see later, we will employ the notion of a pointed Kripke structure in representing the real state of the world and the beliefs of agents. As such, the execution of an action in a pointed Kripke structure will modify it. Such a change can be a combination of the following basic changes:4\n\u2022 Removing some worlds and/or edges from the structure;\n\u2022 Adding some worlds and/or edges to the structure; or\n\u2022 Replicating a structure or merging different structures into one.\nIn this subsection, we describe some basic operators on Kripke structures that will satisfy the above needs. We will assume that a Kripke structure M is given. The first operator on Kripke structures is to remove a set of worlds from M .\nDefinition 7 (World Subtraction) Given a set of worlds U \u2286M [S], M s U is a Kripke structure M \u2032 defined by:\n(i) M \u2032[S] = M [S] \\ U ; (ii) M \u2032[\u03c0](s)(f) = M [\u03c0](s)(f) for every s \u2208M \u2032[S] and f \u2208 F; and (iii) M \u2032[i] = M [i] \\ {(t, v) | (t, v) \u2208M [i], {t, v} \u2229 U 6= \u2205}.\nIntuitively, M s U is the Kripke structure obtained from M by removing the worlds in U and all edges connected to these worlds. The next two operators are used to remove edges from or introduce edges into a given structure.\nDefinition 8 (Edge Subtraction) For a set of edges X in M , M a X is a Kripke structure, M \u2032, defined by:\n(i) M \u2032[S] = M [S]; (ii) M \u2032[\u03c0] = M [\u03c0]; and (iii) M \u2032[i] = M [i] \\ {(u, i, v) | (u, i, v) \u2208 X} for every i \u2208 AG.\nM a X is the Kripke structure obtained from M by removing from M all the edges in X . Given a state (M, s) and a set of agents \u03b1 \u2286 AG, we denote with (M |\u03b1, s) the state (M a X, s) where\nX = \u22c3\ni\u2208AG\\\u03b1\n{(u, i, v) | (u, v) \u2208M [i]}.\nDefinition 9 (Edge Addition) For a set of edges X in M , M a \u2295X is a Kripke structure, M \u2032, defined by:\n4We will focus on viewing a (pointed) Kripke structure as a labeled graph.\n(i) M \u2032[S] = M [S]; (ii) M \u2032[\u03c0] = M [\u03c0]; and (iii) M \u2032[i] = M [i] \u222a {(u, v) | (u, i, v) \u2208 X} for every i \u2208 AG.\nWhile a removes edges from the structure, a \u2295 is used to add edges to an existing structure. Recall that the nodes/edges in a Kripke structure indicate uncertainty in the agents\u2019 beliefs. As such, these operators are useful for updating the beliefs of the agents in a domain when the beliefs of the agents change.\nDefinition 10 (Equivalence Modulo c) Given two Kripke structures M1 and M2, we say that M1 is c-equivalent5 to M2 (denoted as M1\nc\u223cM2) if there exists a bijective function c : M1[S]\u2192M2[S], such that for every u \u2208M1[S] and f \u2208 F , we have that:\n1. M1[\u03c0](u)(f) = > if and only if M2[\u03c0](c(u))(f) = >; and\n2. for every i \u2208 AG and u, v \u2208M1[S], (u, v) \u2208M1[i] if and only if (c(u), c(v)) \u2208M2[i].\nThe execution of several types of actions leads to the creation of \u201cduplicates\u201d of an original Kripke structure\u2014e.g., to represent how different groups of agents maintain different perspectives after the execution of an action.\nDefinition 11 (Replica) Given a Kripke structure M , a c-replica of M is a Kripke structure M \u2032 such that c is a bijection between M [S] and M \u2032[S], M [S] \u2229M \u2032[S] = \u2205, and M \u2032 c\u223cM .\nGiven a state (M, s), a c-replica of (M, s) is a state (M \u2032, s\u2032) where M \u2032 is a c-replica of M and s\u2032 = c(s).\nWe say that M1 and M2 are compatible if for every s \u2208 M1[S] \u2229 M2[S] and every f \u2208 F , M1[\u03c0](s)(f) = M2[\u03c0](s)(f).\nDefinition 12 (Knowledge Updates) For two compatible Kripke structures M1 and M2, we define M1 \u03ba \u222aM2 as the Kripke structure, M \u2032, where: (i) M \u2032[S] = M1[S] \u222aM2[S]; (ii) M \u2032[\u03c0](s) = M1[\u03c0](s) for s \u2208M1[S] and M \u2032[\u03c0](s) = M2[\u03c0](s) for s \u2208M2[S] \\M1[S]; and (iii) M \u2032[i] = M1[i] \u222aM2[i].\nFor a pair of Kripke structures M1 and M2 such that M1[S] \u2229M2(S) = \u2205, |M1[S]| = |M2(S)|, \u03b1 \u2286 AG, and a one-to-one function \u03bb : M2[S]\u2192M1[S], we define M1 ]\u03bb\u03b1M2 as the Kripke structure, M \u2032, where:\n(i) M \u2032[S] = M1[S] \u222aM2[S]; (ii) M \u2032[\u03c0](s) = M1[\u03c0](s) for s \u2208M1[S] and M \u2032[\u03c0](s) = M2[\u03c0](s) for s \u2208M2[S]; and (iii) for each i \u2208 \u03b1, we have that M \u2032[i] = M1[i] \u222aM2[i], and for each i \u2208 AG \\ \u03b1 we have that\nM \u2032[i] = M1[i] \u222aM2[i] \u222a {(u, v) | u \u2208M2[S], v \u2208M1[S], (\u03bb(u), v) \u2208M1[i]}.\nGiven two states (M1, s1) and (M2, s2) such thatM1[S]\u2229M2[S] = \u2205, |M1[S]| = |M2(S)|, \u03b1 \u2286 AG, and a one-to-one function \u03bb : M2[S]\u2192M1[S]:\n(M1, s1) ]\u03bb\u03b1 (M2, s2) = (M1 ]\u03bb\u03b1M2, s2).\nIntuitively, the operators \u03ba \u222a and ]\u03bb\u03b1 allow us to combine different Kripke structures representing the beliefs of different groups of agents, thereby creating a structure representing the beliefs of \u201call agents.\u201d The \u03ba \u222a operation effectively unions two Kripke structures (that might have already some worlds in common). The ]\u03bb\u03b1 also combines two Kripke structures albeit in a more specific way. It relies on a function \u03bb to create additional edges between the two structures for agents in AG \\ \u03b1.\n5Similar to the notion of bisimulation in [20].\n3 The language mA+: Syntax In this paper, we consider multi-agent domains in which the agents are truthful and no false information may be announced or observed. Furthermore, the underlying assumptions guiding the semantics of our language are the rationality principle and the idea that beliefs of an agent are inertial. In other words, agents believe nothing which they are not forced to believe, and the beliefs of an agent remain the same unless something causes them to change.\nIn this section and the next section, we introduce the language mA+ for describing actions and their effects in multi-agent environment. The language builds over a signature \u3008AG,F ,A\u3009, where AG is a finite set of agent identifiers, F is a set of fluents, and A is a set of actions. Each action in A is an action the agents in the domain are capable of performing.\nSimilar to any action language developed for single-agent environments,mA+ consists of three components which will be used in describing the actions and their effects, the initial state, and the query language (see, e.g., [7]). We will next present each of these components. Before we do so, let us denote the multi-agent domain in Example 1 by D1. For this domain, we have that AG = {A,B,C}. The set of fluents F for this domain consists of:\n\u2022 tail: the coin lies tails up (head is often used in place of \u00actail);\n\u2022 has key(X): agent X has the key of the box;\n\u2022 opened: the box is open; and\n\u2022 looking(X): agent X is looking at the box.\nThe set of actions for D1 consists of:\n\u2022 open(X): agent X opens the box;\n\u2022 peek(X): agent X peeks into the box;\n\u2022 signal(X,Y ): agent X signals agent Y (to look at the box);\n\u2022 distract(X,Y ): agent X distracts agent Y (so that Y does not look at the box); and\n\u2022 shout tail(X): agent X announces that the coin lies tail up.\nwhere X,Y \u2208 {A,B,C} and X 6= Y . We start with the description of actions and their effects."}, {"heading": "3.1 Actions and effects", "text": "We envision three types of actions that an agent can perform: world-altering actions (also known as ontic actions), sensing actions, and announcement actions. Intuitively,\n\u2022 A world-altering action is used to explicitly modify certain properties of the world\u2014e.g., the agent A opens the box in Example 1, or the agent A distracts the agent B so that B does not look at the box (also in Example 1);\n\u2022 A sensing action is used by an agent to refine its beliefs about the world, by making direct observations\u2014e.g., an agent peeks into the box; the effect of the sensing action is to reduce the amount of uncertainty of the agent;\n\u2022 An announcement action is used by an agent to affect the beliefs of the agents receiving the communication\u2014we operate under the assumption that agents receiving an announcement always believe what is being announced.\nFor the sake of simplicity, we assume that each action a \u2208 A falls in exactly one of the three categories.6 In general, an action can be executed only under certain conditions, called its executability conditions. For example, the statement \u201cto open the box, an agent must have its key\u201d in Example 1 describes the executability condition of the action of opening a box. The first type of statements inmA+ is used to describe the executability conditions of actions and is of the following form:\n6It is easy to relax this condition, but it would make the presentation more tedious.\nexecutable a if \u03c8 (1)\nwhere a \u2208 A and \u03c8 is a belief formula. A statement of type (1) will be referred to as the executability condition of a. \u03c8 is referred as the precondition of a. For simplicity of the representation, we will assume that each action a is associated with exactly one executability condition. When \u03c8 = >, the statement will be omitted.\nFor a world-altering action a, such as the action of opening the box, we have statements of the following type that express the change that may be caused by such actions:\na causes ` if \u03c8 (2)\nwhere ` is a fluent literal and \u03c8 is a belief formula. Intuitively, if the real state of the world and of the beliefs match the condition described by \u03c8, then the real state of the world is affected by the change that makes the literal ` true after the execution of a. When \u03c8 = >, the part \u201c if \u03c8\u201d will be omitted from (2). We also use\na causes \u03a6 if \u03c8\nwhere \u03a6 is a set of fluent literals as a shorthand for the set {a causes ` if \u03c8 | ` \u2208 \u03a6}. Sensing actions, such as the action of looking into the box, allow agents to learn about the value of a fluent in the real state of the world (e.g., learn whether the coin lies head or tail up). We use statements of the following kind to represent sensing actions:\na determines f (3)\nwhere f is a fluent. Statements of type (3) encode a sensing action a which enables the agent(s) to learn the value of the fluent f . f is referred to as a sensed fluent of the sensing action a.\nFor actions such as an agent telling another agent that the coin lies heads up, we have statements of the following kind, that express the change that may be caused by such actions:\na announces \u03d5 (4)\nwhere \u03d5 is a fluent formula. a is called an announcement action. Let us illustrate the use of statements of type (1)-(4) to represent the actions of the domain D1.\nExample 3 The actions of domain D1 can be specified by the following statements:\nexecutable open(X) if has key(X),BX(has key) executable peek(X) if opened, looking(X),BX(opened), BX(looking(X)) executable shout tail(X) if BX(tail), tail executable signal(X,Y ) if looking(X),\u00aclooking(Y ),BX(\u00aclooking(Y )),BX(looking(X)) executable distract(X,Y ) if looking(X), looking(Y ),BX(looking(X)),BX(looking(Y ))\nopen(X) causes opened signal(X,Y ) causes looking(Y ) distract(X,Y ) causes \u00aclooking(Y ) peek(X) determines tail shout tail(X) announces tail\nwhere X and Y are different agents in {A,B,C}. The first five statements encode the executability conditions of the five actions in the domain. The next three statements describe the effects of the three world-altering actions. peek(X) is an example of a sensing action. Finally, shout tail(X) is an example of an announcement action."}, {"heading": "3.2 Observability: observers, partial observers, and others", "text": "One of the key differences between single-agent and multi-agent domains lies in how the execution of an action changes the beliefs of agents. This is because, in multi-agent domains, an agent can be oblivious about the occurrence of an action or unable to observe the effect of an action. For example, watching another agent open the box allows the agent to know that the box is open after the execution of the action; however, the agent would still believe that the box is closed if she is not aware of the action occurrence. On the other hand, watching another agent peeking into the box does not help the observer in learning whether the coin lies heads or tails up; the only thing she would learn is that the agent who is peeking into the box has knowledge of the status of the coin.\nmA+ needs to have a component for representing the fact that not all the agents may be completely aware of the presence of actions being executed. Depending on the action and the current situation, we can categorize agents in three classes:\n\u2022 Full observers,\n\u2022 Partial observers, and\n\u2022 Oblivious (or others).\nThis categorization is dynamic: changes in the state of the world may lead to changes to the agent\u2019s category w.r.t. each action. In this paper, we will consider the possible observability of agents for different action types as detailed in Table 1.\nThe first row indicates that, for a world-altering action, an agent can either be a full observer, i.e., completely aware of the occurrence of that action, or oblivious of the occurrence of the action. In the second case, the observability of the agent is categorized as other. Note that we assume that the observer agents know about each others\u2019 status and they are also aware of the fact that the other agents are oblivious. The oblivious agents have no clue of anything.\nFor a sensing action, an agent can either be a full observer\u2014i.e., the agent is aware of the occurrence of that action and of its results\u2014a partial observer\u2014i.e., gaining knowledge that the full observers have performed a sensing action but without knowledge of the result of the observation\u2014or oblivious of the occurrence of the action (i.e., other). Once again, we assume that the observer agents know about each others\u2019 status and they also know about the agents partially observing the action and about the agents that are oblivious. The partially observing agents know about each others\u2019 status, and they also know about the observing agents and the agents that are oblivious. The oblivious agents have no clue of anything. The behavior is analogous for the case of announcement actions.\nThe dynamic nature of the agents observability can be manipulated and this is specified using agent observability statements of the following form:7\nX observes a if \u03d5 (5) X aware of a if \u03c8 (6)\nwhere X \u2208 AG is the name of an agent, a \u2208 A, and \u03d5 and \u03c8 are fluent formulae. Statements of type (5) indicate that X is a full observer of the execution of a if \u03d5 holds. Statements of type (6) state that X is a partial observer of the execution of a if \u03c8 holds. X , a, and \u03d5 (resp. \u03c8) are referred to as the agent, the action, and the condition of (5) (resp. (6)).\n7As discussed earlier, the \u201c if >\u201d are omitted from the statements.\nIn the following, we will assume that, for every agent X \u2208 AG and for every action a \u2208 A, if X occurs in a statement of the form (5), then it will not occur in a statement of the form (6) such that \u03d5 \u2227 \u03c8 is consistent and vice versa.\nDefinition 13 An mA+ domain is a collection of statements of the forms (1)-(6).\n3.3 More Examples of mA+ Domains In this section, we illustrate the use of mA+ in representing multi-agent domains. We start by completing the specification of the domain D1.\nExample 4 (Observability in D1) The actions of D1 are described in Example 3. The observability of agents in D1 can be described by the set O1 of statements\nX observes open(X) X observes peek(X) Y observes open(X) if looking(Y ) Y aware of peek(X) if looking(Y ) Y observes shout tail(X) X observes shout tail(X) X observes distract(X,Y ) X observes signal(X,Y ) Y observes distract(X,Y ) Y observes signal(X,Y )\nwhereX and Y denote different agents in {A,B,C}. The above statements say that agentX is a fully observant agent when open(X), peek(X), distract(X,Y ), signal(X,Y ), or shout tail(X) are executed; Y is a fully observant agent if it is looking (at the box) when open(X) is executed. Y is a partially observant agent if it is looking when peek(X) is executed. An agent different from X or Y is oblivious in all cases.\nThe next example represents a domain in which the agent who executes an action might not be a full observer of the action occurrence.\nExample 5 Let us consider a domain with two agentsA = {A,B}. The two agents are operating in a room; agent A is blind while B is not. Both agents are aware that by flipping a switch it is possible to change the status of the light in the room, and both agents can perform such action. On the other hand, the effect of the execution of the action will be visible only to B. This action can be described by the following statements of mA+:\nflip causes on if \u00acon flip causes \u00acon if on B observes flip\nWe will next describe several examples that contain common actions that are typical to multi-agent domains in mA+. We refer to these actions as reference setting actions, such as the action of distracting another agent. These types of actions are interesting, because it is often necessary for agents to execute them in order to allow subsequent actions to achieve their intended effects (e.g., sharing a secret).\nExample 6 (Distraction) Agent A wishes to access some files on C\u2019s computer without C\u2019s knowledge. Agent C is rather observant therefore, in order for A to succeed, A will need to first cause a distraction (such as pulling the fire alarm) in order to pull C\u2019s attention away from the computer. Once C is distracted, A may access the file on the computer. This can be described by the following statements:\ndistract(A,C) causes distracted(C) A observes distract(A,C) C observes distract(A,C) executable accessFile(A,C) if distracted(C)\nThe action that helps an agent to form or dissolve a group is also frequently needed in multi-agent domains. Groups enable, for example, the execution of communications that are only local to the group (e.g., a secret conversation).\nExample 7 (Group Formation/Dissolution and Secret Communication) Continuing with Example 6, now that A has access to the file, she needs the assistance of agent B to learn its contents because the file is encrypted and the expertise of B is required for decryption. In order to read the file, A must first establish a connection with B\u2014agent A must open/invite a communications channel to B. Let linked(X,Y ) denote that X and Y are connected and distracted(Z) denote that Z is distracted. This action of A inviting B to connect via some communication channel can be represented using the action openChannel(A,B) with the following following specification:\nopenChannel(A,B) causes linked(A,B) A observes openChannel(A,B) B observes openChannel(A,B) C observes openChannel(A,B) if \u00acdistracted(C) executable openChannel(A,B) if \u00aclinked(A,B)\nOnce a channel has been opened, agents A and B are linked and they may together read the file. Once they have read the file, they disconnect the channel in order to leave no trace of their activity. This action can be represented using the action closeChannel(A,B) with the following specification:\ncloseChannel(A,B) causes \u00aclinked(A,B) A observes closeChannel(A,B) B observes closeChannel(A,B) C observes closeChannel(A,B) if \u00acdistracted(C) executable closeChannel(A,B) if linked(A,B)\nReading the file allows A and B to understand its content. Let us assume that the file indicates whether tomorrow is the date set for a cyber-attack against A\u2019s organization. This can be represented as follows.\nreadFile(A) determines attackDate(tomorrow) A observes readFile(A) B observes readFile(A) if linked(A,B) C aware of readFile(A) if \u00acdistracted(C) executable readFile(A) if linked(A,B)\nIf a channel is open, it can be used to share the knowledge of an impeding attack. However, the communication is secure only if the third party is distracted. This action is an announcement action and can be represented using the following statements.\nwarnOfAttack(A,B) announces attackDate(tomorrow) A observes warnOfAttack(A,B) B observes warnOfAttack(A,B) C observes warnOfAttack(A,B) if \u00acdistracted(C) executable warnOfAttack(A,B) if linked(A,B) \u2227 attackDate(tomorrow)\nA more general version of the actions of secretly creating/dissolving a group is given in the next example.\nExample 8 Consider an agent X joining an agent Y to gain visibility of everything that the agent X does; this can be modeled by the action join(Y,X), where Y joins X at the same level of visibility of X\u2019s actions:\njoin(Y,X) causes group member(Y, group(X)) X observes join(Y,X) Y observes join(Y,X)\nThis could be refined by adding the need to be invited to join X:\nexecutable join(Y,X) if invited(Y,X)\nThe effect of gaining visibility of the actions of X can be described by\nY observes aX if group member(Y, group(X))\nwhere aX is any action that is executed by agent X . The symmetrical operation is the operation of leaving the group, leading the agent Y to completely loose visibility of what agent X is doing:\nleave(Y,X) causes \u00acgroup member(Y, group(X)) X observes leave(Y,X) Y observes leave(Y,X)\nThe agent Y may also decide to take the action of separating from the group, through the action separate(Y,X), where the agent Y will observeX from \u201ca distance\u201d, with the consequent loss of the intimate knowledge ofX actions\u2019 effects:\nseparate(Y,X) causes \u00acgroup member(Y, group(X)) \u2227 group observer(Y, group(X)) X observes separate(Y,X) Y observes separate(Y,X) Y aware of aX if group observer(Y, group(X))\nDistracting an agent is not only necessary for secure communication, it is also important for certain world-altering actions to achieve their intended effects, as in Example 6. Another example that highlights the importance of this type of actions can be seen next.\nExample 9 Agent D is a prisoner, having been captured by C. Agent A is in charge of rescuing agent D. In order to do so, he must first distract C, and then trigger a release on C\u2019s computer. Once the release has been triggered, D may escape. The distract(A,C) action has already been presented in Example 6. Let us consider the other actions. Rescuing an agent means that the rescued agent is released. We use the following statements:\nrescue(A,D) causes released(D) A observes rescue(A,D) D observes rescue(A,D) C observes rescue(A,D) if \u00acdistracted(C) executable rescue(A,D) if distracted(C)\nThe action of escaping can have different effects.\nescape(D) causes dead(D) if \u00acdistracted(C) escape(D) causes free(D) if distracted(C) A observes escape(D) D observes escape(D) C observes escape(D) if \u00acdistracted(C)"}, {"heading": "3.4 Initial State", "text": "A domain specification encodes the actions and their effects and the observability of agents in each situation. The initial state, that encodes both the initial state of the world and the initial beliefs of the agents, is specified in mA+ using initial statements of the following form:\ninitially \u03d5 (7)\nwhere \u03d5 is a formula. Intuitively, this statement says that \u03d5 is true in the initial state. We will later discuss restrictions on the formula \u03d5 to ensure the computability of the Kripke structures describing the initial state.\nExample 10 (Representing Initial State of D1) Let us reconsider Example 1. The initial state of D1 can be expressed by the following statements:\ninitially C(has key(A)) initially C(\u00achas key(B)) initially C(\u00achas key(C)) initially C(\u00acopened) initially C(\u00acBXtail \u2227 \u00acBX\u00actail) for X \u2208 {A,B,C} initially C(looking(X)) for X \u2208 {A,B,C}\nThese statements indicate that everyone knows that A has the key and B and C do not have the key, the box is closed, no one knows whether the coin lies head or tail up, and everyone is looking at the box.\nThe notion of an action theory in mA+ is defined next.\nDefinition 14 (Action Theory) A mA+-action theory is a pair (I,D) where D is a mA+ domain and I is a set of initially statements.\n4 Transition Function for mA+ Domains A mA+ domain D specifies a transition system, whose nodes are \u201cstates\u201d that encode the description of the state of the world and of the agents\u2019 beliefs. This transition system will be described by a transition function \u03a6D, which maps pairs of actions and states to states. For simplicity of the presentation, we assume that only one action is executed at each point in time\u2014it is relatively simple to extend it to cases where concurrent actions are present, and this is left as future work. As we have mentioned in Section 2, we will use pointed Kripke structures to represent states in mA+ action theories. A pointed Kripke structure encodes three components:\n\u2022 The actual world;\n\u2022 The state of beliefs of each agent about the real state of the world; and\n\u2022 The state of beliefs of each agent about the beliefs of other agents.\nThese components are affected by the execution of actions. Observe that the notion of a state in mA+ action theories is more complex than the notion of state used in single-agent domains (i.e., a complete set of fluent literals).\nLet us consider a state (M, s). We say that an action a \u2208 A is executable in (M, s) if the executability condition of a, given by a statement of the type (1) in D, is satisfied by (M, s), i.e., (M, s) |= \u03c8. The effect of executing an action a in (M, s) is, in general, a set of possible states denoted by \u03a6D(a, (M, s)). Since each type of action will impact (M, s) in a different manner, we define\n\u03a6D(a, (M, s)) =  \u03a6 w D(a, (M, s)) if a is a world-altering action\n\u03a6sD(a, (M, s)) if a is a sensing action \u03a6aD(a, (M, s)) if a is an announcement action\nIn other words, we will define \u03a6D for each type of actions separately. As we have mentioned earlier, the occurrence of an action can change the state of the world and/or the state of the agents\u2019 beliefs. The change in an agent\u2019s beliefs depends on the degree of awareness of the agent about the action occurrence, which in turn depends on the current state. Therefore, we need to first introduce the notion of a frame of reference in order to define the function \u03a6D."}, {"heading": "4.1 Actions Visibility", "text": "Given a state (M, s) and an action a \u2208 A, let us define\nFD(a,M, s) = {X \u2208 AG | [X observes a if \u03d5] \u2208 D such that (M, s) |= \u03d5} PD(a,M, s) = {X \u2208 AG | [X aware of a if \u03d5] \u2208 D such that (M, s) |= \u03d5} OD(a,M, s) = AG \\ (FD(a,M, s) \u222a PD(a,M, s))\nWe will refer to the tuple (FD(a,M, s),PD(a,M, s),OD(a,M, s)) as the frame of reference for the execution of a in (M, s). Intuitively, FD(a,M, s) (resp. PD(a,M, s) and OD(a,M, s)) are the agents that are fully observant (resp. partially observant and oblivious/other) of the execution of action a in the pointed Kripke structure (M, s). For the sake of simplicity, we will assume that the set of agent observability statements in D is consistent, in the sense that for each pair of an action a \u2208 A and a pointed Kripke structure (M, s), the sets FD(a,M, s) and PD(a,M, s) are disjoint. Thus, the domain specification D and the pointed Kripke structure (M, s) determine a unique frame of reference for each action occurrence.\nThe introduction of frames of reference allows us to elegantly model several types of actions that are aimed at modifying the frame of reference (referred to as reference setting actions). Some possibilities are illustrated in the following examples.\nExample 11 (Reference Setting Actions) Example 4 shows two reference setting actions: signal(X,Y ) and distract(X,Y ). The action signal(X,Y ) allows agent X to promote agent Y into a higher level of observation for the effect of the action peek(X). On the other hand, the action distract(X,Y ) allows agent X to demote agent Y into a lower level of observation. The net effect of executing these actions is a change of frame.\nLet us consider the action signal(X,Y ) and a state (M, s). Furthermore, let us assume that (M \u2032, s\u2032) is a state resulting from the execution of signal(X,Y ) in (M, s). The frames of reference for the execution of the action a = peek(X) in these two states are detailed in Figure 2.\nIntuitively, after the execution of signal(X,Y ), looking(Y ) becomes true because of the statement\nsignal(X,Y ) causes looking(Y )\nin D1. By definition, the statement Y aware of peek(X) if looking(Y )\nindicates that Y is partially observant. Similar argument shows that distract(X,Y ) demotes Y to the lowest level of visibility, i.e., it will cause agent Y to become oblivious of the successive peek(X) action. Again, for a = peek(X), we have the change of frame of reference summarized in Figure 3.\nReference setting actions can be used in modeling group formation activities.\nExample 12 Consider the join and leave actions from Example 8. The frame setting changes of these actions can be summarized as in Figure 4, where a(X) denotes an arbitrary action ofX and (M \u2032, s\u2032) denotes the state resulting from the execution of the corresponding action in (M, s)."}, {"heading": "4.2 World-Altering Actions: \u03a6wD", "text": "Intuitively, the execution of a world-altering (a.k.a. ontic) action will take place in the actual world\u2014thus, it will affect the interpretation associated to the world s of the state (M, s) in which the action is executed. The definition of \u03a6wD must describe this change as well as the change in beliefs of agents with different degrees of awareness. Recall that for a world-altering action a, the frame of reference for its execution in a state (M, s) consists of two elements: the fully observers (FD(a,M, s)) and the oblivious ones (OD(a,M, s)). Intuitively, an agent in FD(a,M, s) should believe that the effects of a are true in the successor state. In addition, she should also be able to remove from consideration those worlds in which the action is non-executable. On the other hand, an agent in OD(a,M, s) will not be able to observe the effects of the execution of a and, hence, will believe that there are no changes, i.e., its accessible possible worlds remain unchanged. We can conclude that the successor state \u03a6w(a, (M, s)) will contain two substructures, one for each group of agents.\nLet us construct the structure for the group of agents FD(a,M, s). As each agent in this group is fully aware of the effects of a, the possible worlds that are present after the occurrence of a will be those resulting from the execution of a in the possible worlds belonging to M . To give a precise definition, we need the following notations.\nFor a fluent literal `, let ` denote its complement, i.e., for f \u2208 F , f = \u00acf and \u00acf = f . For a set of literals S, S = {` | ` \u2208 S}. For a world-altering action a, the effects of the action in the Kripke structure M w.r.t. the world u \u2208M [S] are eD(a,M, u) = {` | [a causes \u03c8 if \u03d5] \u2208 D such that (M,u) |= \u03d5, ` \u2208 \u03c8} In the following, we will assume that for every world-altering action a and state (M, s), eD(a,M, u) is a consistent set of literals for every u \u2208M [s] (i.e., the domain specification is consistent). It is easy to see that, given a, M and u, eD(a,M, u) is unique.\nAn agent observing the execution of the action will learn the outcomes of the execution; nevertheless, the agents have uncertainty about the actual world\u2014which implies that the agent may have to maintain such uncertainty after the execution of the action, by applying the effects of the action in each world that is considered possible by the agent. This leads us to the following construction of the substructure for the agents in FD(a,M, s). Let us denote with Res(a,M, s) the Kripke structure defined as follows\n1. Res(a,M, s)[S] = {r(a, u) | u \u2208M [S], a is executable in (M,u)} where each r(a, u) is a new world;\n2. Res(a,M, s)[\u03c0](r(a, u)) = M [\u03c0](u) \\ eD(a,M, u) \u222a eD(a,M, u);\n3. (r(a, u), r(a, v)) \u2208 Res(a,M, s)[i] iff\n\u25e6 (u, v) \u2208M [i]\n\u25e6 {r(a, u), r(a, v)} \u2286 Res(a,M, s)[S] \u25e6 i \u2208 FD(a,M, s)\nIntuitively, this is a copy of the original Kripke structure M , where:\n\u2022 The worlds receive new names (of the form r(a, u)) and are kept only if a is executable in them (item 1),\n\u2022 Only the edges labeled by fully observant agents are kept (item 3), and\n\u2022 The interpretations attached to the worlds are updated to reflect the consequences of action a (item 2).\nObserve that this construction could be optimized\u2014e.g., the execution of the action might completely remove the uncertainty of agents, thus removing the need for multiple worlds.\nSince the agents in O(a,M, s) are not aware of the action occurrence and its effects, the structure encoding their beliefs is exactly the original structure. So, the successor state \u03a6wD(a, (M, s)) is obtained by combining Res(a,M, s) and (M, s). This combination should maintain the beliefs of agents in O(a,M, s). This is achieved by creating edges of the form (r(a, u), i, v) for each (u, v) \u2208 M [i] and i \u2208 O(a,M, s). The complete definition of \u03a6wD(a, (M, s)) is as follows.\nDefinition 15 (Step Transition for World-Altering Actions) Let a be a world-altering action and (M, s) be a state. Then\n\u03a6wD(a, (M, s)) =  \u2205 a is not executable in (M, s){(M \u2032, s\u2032)} a is executable in (M, s) (8) where\n\u25e6 s\u2032 = r(a, s)\n\u25e6 M \u2032[S] = M [S] \u222aRes(a,M, s)[S]\n\u25e6 M \u2032[\u03c0](u) = M [\u03c0](u) for each u \u2208M [S] and M \u2032[\u03c0](u) = Res(a,M, s)[\u03c0](u) for each u \u2208 Res(a,M, s)[S]\n\u25e6 M \u2032[i] = M [i] \u222aRes(a,M, s)[i] \u222a Link(a,M, s, i), where:\n\u2013 for each i \u2208 FD(a,M, s) we have that Link(a,M, s, i) = \u2205; \u2013 for each i \u2208 OD(a,M, s) we have that\nLink(a,M, s, i) = {(r(a, u), v) | r(a, u) \u2208 Res(a,M, s)[S], u, v \u2208M [S], (u, v) \u2208M [i]}.\nWe illustrate the definition with some simple examples. First, let us continue with the example presented in the introduction.\nExample 13 The action open is a world-altering action. The observability of agents with respect to its execution is given in Example 4. If we assume that agent A opens the box when agent B is looking at it while C is not, then the transformation of a state is shown in Figure 5 (for the sake of readability and the simplicity of the graph, we omit the other fluents, such as has key(X), in the world representations). The frame of reference for open(A) is FD(open(A),M, s) = {A,B} and OD(open(A),M, s) = {C}. Note that the two worlds in the upper row represent the same configuration of beliefs as the starting situation (due to the fact that agent C is oblivious of any change), while the two worlds in the bottom row reflect the new perspective of the agents A and B\u2014in particular, in the actual world, the box is now opened.\nThe next example shows the effect of the execution of the action flip from Example 5.\nExample 14 Let us consider the state (M, s1) shown on top in Figure 6. More precisely, we have that M [S] = {s1, s2}, M [\u03c0](s1)(on) = \u22a5, M [\u03c0](s2)(on) = >, and M [A] = {(s1, s1), (s1, s2), (s2, s1), (s2, s2)} and M [B] = {(s1, s1), (s2, s2)}. Observe that (M, s1) identifies a state where the light is off, but agent A is unaware of that (while agent B is). The bottom part of Figure 6 illustrates the resulting state after the execution of flip in (M, s1).\nThe top part of the resulting state (worlds s1 and s2) represent an exact copy of the original state\u2014this reflects the view of agent A, which is unaware of the effects of the action flip being executed. The bottom part reflects instead the view that agents B has after executing the action, including the fact that the new state of the world (denoted by r(flip, s1)) has a different interpretation w.r.t. s1. The arrows labeled A leaving the state r(flip, s1) indicate that for agent A nothing has changed (since they point back to the original copy of the Kripke structure).\nWe can prove the following lemma.\nLemma 1 Given a consistent domain D, a state (M, s), and a world-altering action a executable in (M, s), we have that \u03a6wD(a, (M, s)) is a singleton.\nProof. This result follows trivially from the construction of (M \u2032, s\u2032), the fact that Res(a,M, s) is unique, and that for each u \u2208 Res(a,M, s)[S], the interpretation of r(a, u) is also unique. 2\nThe next proposition shows that the execution of a world-altering action changes the beliefs of agents according to their classification.\nProposition 2 Let D be a consistent domain, (M, s) be a state, a be a world-altering action that is executable in (M, s) and \u03a6wD(a, (M, s)) = {(M \u2032, s\u2032)}. The following holds:\n1. For every world r(a, u) \u2208M \u2032[S]\\M [S] and for every literal ` \u2208 eD(a,M, u), we have that (M \u2032, r(a, u)) |= `;\n2. For every world r(a, u) \u2208 M \u2032[S] \\M [S] and for every literal ` \u2208 F \\ {`, \u00af\u0300 | ` \u2208 eD(a,M, u)}, we have that (M,u) |= ` iff (M \u2032, r(a, u)) |= `;\n3. For every i \u2208 OD(a,M, s) and formula \u03d5, (M \u2032, s\u2032) |= Bi\u03d5 iff (M, s) |= Bi\u03d5.\nProof. See Appendix. 2\nThe first case of the proposition indicates that the effects of the action are happening in each world where the action is executable (and visible). The second case indicates that inertia is in effect on those worlds where the action is performed (and visible). Finally, the last case indicates that no changes are observed by the other agents."}, {"heading": "4.3 Sensing Actions: \u03a6sD", "text": "Sensing actions differ from world-altering actions in that they do not change the actual state of the world. Rather, they change the beliefs of the agents who can observe the effects of the actions. Let us consider the action peek(A) in the domain of Example 1. If A were to peek into the box then A should know whether the coin lies head or tail up. Let us consider instead B, who is looking at the box (or at A) when A is peeking into the box. We expect that B should know that A knows whether the coin lies head or tail up. However, B\u2019s uncertainty about whether or not the coin lies head up still remains. Now consider C, who is not looking at the box (or at A) when A is peeking into the box. We expect that C will not see any changes in its beliefs about either A or B; C will also not modify its beliefs about the status of the coin.\nIn general, for a sensing action a such that\na determines f\nbelongs to D, the execution of a will have different effects on different groups of agents:\n\u2022 Each agent i \u2208 FD(a,M, s) will have no uncertainty about the truth value of f , i.e., Bif \u2228Bi\u00acf is true in the successor state;\n\u2022 Each agent i \u2208 PD(a,M, s) will be aware that the agents in FD(a,M, s) know the truth value of f , but i itself will not know such value; that is Bi(Bjf \u2228Bj\u00acf) is true in the successor state for j \u2208 FD(a,M, s);\n\u2022 Each agent i \u2208 OD(a,M, s) will not change its beliefs.\nGiven a sensing action a, the set of fluents being sensed by a is defined as follows.\nSensedD(a) = {f | [a determines f ] \u2208 D}.\nLet us proceed with the development of the step transition function for sensing actions. Similarly to the construction of the successor state for world-altering actions, we can see that the new state will consist of two substructures. The first structure encodes the beliefs of agents whose beliefs change after the execution of a. The second one represents agents who are oblivious. Since sensing actions do not change the world, the first structure can be obtained from the original structure M by:\n(i) Removing all worlds in M such that the executability condition of a is not satisfied with respect to them, i.e., the set\nRemstatesa (M) = {u | (M,u) 6|= \u03d5}\nwhere \u03d5 is the executability condition of a;\n(ii) Removing all links related to oblivious agents (OD(a,M, s)); and\n(iii) Removing all links of the form (u, i, v) where i \u2208 FD(a,M, s) such that the interpretation of some f \u2208 SensedD(a) in u and v is different, i.e., the set\nRemlinksa (M) = {(u, i, v) | i \u2208 FD(a,M, s), f \u2208 SensedD(a), (u, v) \u2208M [i],M [\u03c0](u)(f) 6= M [\u03c0](v)(f)}.\nUsing the operations on Kripke structures (Subsection 2.3), we can formulate this process as follows:\n\u2022 Take a replica (Mr, c(s)) of (M, s) and restrict the structure to contain only information related to agents in FD(a,M, s) \u222a PD(a,M, s) which is (Mr s Remstatesa (Mr), c(s)) |FD(a,M,s)\u222aPD(a,M,s);\n\u2022 Remove all links related to the uncertainty about fluents in SensedD(a) for agents in FD(a,M, s) from this structure which is ((Mr s Remstatesa (Mr)) |FD(a,M,s)\u222aPD(a,M,s) a Remlinksa (Mr), c(s)); and\n\u2022 Link the states in the replica to the states of the original Kripke structure via the links of agents in OD(a,M, s) which is represented by the operator ]c\u22121FD(a,M,s)\u222aPD(a,M,s) on the two structures.\nThis leads us to the following definition of \u03a6sD.\nDefinition 16 (Step Transition for Sensing Actions) Let a be a sensing action and (M, s) be a state. Then,\n\u03a6sD(a, (M, s)) = { \u2205 a is not executable in (M, s) {(M \u2032, s\u2032)} a is executable in (M, s) (9)\nwhere (M \u2032, s\u2032) is given by (M \u2032, s\u2032) = (M, s) ]c \u22121\nFD(a,M,s)\u222aPD(a,M,s) (M \u2032\u2032, s\u2032\u2032)\nwith\n\u2022 (M \u2032\u2032, s\u2032\u2032) = ((Mr s Remstatesa (Mr)) |FD(a,M,s)\u222aPD(a,M,s) a Remlinksa (Mr), c(s)) and\n\u2022 (Mr, c(s)) is a c-replica of (M, s).\nWe illustrate the definition in the next example.\nExample 15 Let us consider a slight modification of the scenario used in Example 1. Assume that it is common belief that the box is open and C is not looking at the box while A and B both look at the box. Assume that A peeks into the box. Starting from the state in Figure 1 (we omit all other fluent literals from the world representations, in this case: opened, has key(A), \u00achas key(B), \u00achas key(C), looking(A), looking(B), and \u00aclooking(C)), the execution of the action peek(A) will lead to the state in Figure 7.\nWe can recognize three components in this resulting state:\n\u2022 Agent A (see the bottom-left box in Figure 7, the green box) has exact belief of the property \u00actail being true, without any uncertainty\u2014i.e., there are no possible worlds accessible by A where tail is true.\n\u2022 Agent B operates on the two possible worlds in the last row of Figure 7 (the bottom/blue box); the agent itself does not know whether tail is true or false, but the agent knows thatA is aware of the value of tail, i.e., the agent B sees the formula BAtail \u2228BA\u00actail as valid.\n\u2022 Agent C operates on the two possible worlds in the middle row of Figure 7 (the middle/red box), denoting complete ignorance about tail as well as about the other agents\u2019 beliefs about tail.\nObserve that the bottom structure of the successor state (blue area) is obtained from a copy (Mr, c(s)) of the original structure (top), by removing Remstatespeek(A)(M\nr) = \u2205, restricting the links to those labeled by agents in the set {A,B}, and removing the set of links labeled A that connect worlds with different interpretation of the fluent tail, i.e., Remlinkspeek(A)(M r) = {(s3, A, s4), (s4, A, s3)}.\nHaving defined the step transition function for sensing actions, let us focus on its properties. The first aspect we want to mention is the need to understand the restriction imposed in the introduction\u2014i.e., we intend to use sensing actions to remove uncertainty but not to correct wrong beliefs. The next example shows that even a fully observant agent with a wrong belief could become \u201cignorant\u201d after the execution of a sensing action.\nExample 16 Consider a simple action a that is always executable and determines the fluent f and an agent A who is fully observant of the execution of a in the state given in the left of Figure 8.\nIt is easy to see that the execution of a results in the state on the right of Figure 8. Observe thatA becomes ignorant in s0, the actual world, after the execution of a. This is because A has the wrong belief about the fluent f in the state in which a is executed and the execution of a does not update her belief.\nThe example shows that in general, the execution of a sensing action in a multi-agent environment might have different effects on the agent\u2019s knowledge and beliefs. Taking the difference between knowledge and belief in formalizing actions in multi-agent environment is a challenging issue that will need to be addressed. We have taken the first step into to address this issue in a separate work [30]. We observe that under a certain condition, called consistency preservation, the execution of a sensing action will not result in agents being completely ignorant as in Example 16.\nDefinition 17 A state (M, s) is consistency preserving for a sensing action a if (M,u) 6|= Bif \u2228 Bi\u00acf for every u \u2208M [S], i \u2208 FD(a,M, s) \u222a PD(a,M, s), and f \u2208 SensedD(a).\nWe will next present a proposition which is similar to Proposition 2 for sensing action which states that after the execution of a sensing action in a consistency preserving state, fully observant agents have no uncertainty about the sensed fluents, partially observant agents are aware that the beliefs of fully observant agents are changed, and other agents\u2019 beliefs do not change.\nProposition 3 LetD be a consistent domain, a be a sensing action, and (M, s) be a state. Suppose that a is executable in (M, s) and (M, s) is consistency preserving for a. Assume that (M \u2032, s\u2032) \u2208 \u03a6sD(a, (M, s)). Then, the following holds\n1. for every f \u2208 SensedD(a) and ` \u2208 {f,\u00acf}, if (M, s) |= ` then (M \u2032, s\u2032) |= CFD(a,M,s)`;\n2. for every f \u2208 SensedD(a), (M \u2032, s\u2032) |= CPD(a,M,s)(CFD(a,M,s)f \u2228CFD(a,M,s)\u00acf); and\n3. for every i \u2208 OD(a,M, s) and formula \u03c8, (M \u2032, s\u2032) |= Bi\u03c8 iff (M, s) |= Bi\u03c8.\nProof. See Appendix. 2"}, {"heading": "4.4 Announcement Actions: \u03a6aD", "text": "The announcement actions are used to communicate a piece of information to a group of agents. In this paper, we make the following assumptions:\n\u2022 The announcement is truthful\u2014i.e., the properties being announced are true in the real state of the world; thus, if the announcement of \u03d5 is made in the state (M, s), we expect M [\u03c0](s) |= \u03d5.\n\u2022 The agents receiving the announcement believe the property being announced.\n\u2022 The announcement action occurs in at most one statement of the form (4).\n\u2022 The execution of an announcement action occurs only in states in which fully observant agents do not have wrong beliefs about the announced formula (this is similar to the requirement for the execution of a sensing action).\nSimilar to sensing actions, an announcement action will not change the world. Agents who are aware of the action occurrence, will be able to reason that the current state of the world must satisfy the executability condition of the action. Furthermore, an occurrence of an announcement action only changes the beliefs of agents who are aware or partially aware of the effects of the action. Finally,\n\u2022 Agents who are fully observant will know the truth value of the formula being announced;\n\u2022 Agents who are partially observant will not know the truth value of the formula being announced, but they will know that fully observant agents are knowledgeable about it;\n\u2022 Oblivious agents will have the same beliefs as before the execution of the action.\nThe construction of the successor state \u03a6aD(a, (M, s)) for a state (M, s) and announcement action a is very similar to the one employed for the sensing actions.\nDefinition 18 (Step Transition for Announcements) Let a be an announcement action with\na announces \u03c8\nin D and the executability condition \u03d5 and (M, s) be a state. Then,\n\u03a6aD(a, (M, s)) = { \u2205 a is not executable in (M, s) or (M, s) 6|= \u03d5 {(M \u2032, s\u2032)} a is executable in (M, s) and (M, s) |= \u03d5 (10)\nwhere (M \u2032, s\u2032) is given by (M \u2032, s\u2032) = (M, s) ]c \u22121\nFD(a,M,s)\u222aPD(a,M,s) (M \u2032\u2032, s\u2032\u2032)\nwith\n\u2022 (Mr, c(s)) is a c-replica of (M, s);\n\u2022 Remstatesa (Mr) = {c(u) | u \u2208M [S], (M,u) 6|= \u03d5} \u2022 Remlinksa (M) = { (u, i, v) i \u2208 FD(a,M, s), (u, v) \u2208Mr[i], (Mr[\u03c0](u) |= \u03c8 and Mr[\u03c0](v) |= \u00ac\u03c8 ) or (Mr[\u03c0](u) |= \u00ac\u03c8 and Mr[\u03c0](v) |= \u03c8 ) } \u2022 (M \u2032\u2032, s\u2032\u2032) = ((Mr s Remstatesa (Mr)) |FD(a,M,s)\u222aPD(a,M,s) a Remlinksa (M), c(s))\nIn the next example, we illustrate the definition with the execution of a public announcement action.\nExample 17 Let consider the initial situation described in Figure 9 along with the action shout tail(A) from Example 3, whose observability is given in Example 4. Intuitively, an announcement of the truth of tail is made to all agents. This is an example of a \u201cpublic announcement,\u201d after which all agents are aware of the truth value of the announced formula (tail). This results in the successor state as in Figure 9. Observe that the bottom row looses one of the states as the executability condition does not hold.\nThe next example considers a private announcement action.\nExample 18 Let us assume that A and B have agreed to a scheme of informing each other if the coin lies heads up by raising an hand. B can only observe A if B is looking at the box (or looking at A). C is completely ignorant about the meaning of A\u2019s raising her hand. This can be modeled by the following statements:\nexecutable raising hand(X) if BX(\u00actail),\u00actail raising hand(A) announces \u00actail A observes raising hand(A) if true B observes raising hand(A) if looking(B)\nIf A knows the coin lies heads up and raises her hand, B will be aware that the head is up and C is completely ignorant about this. The execution of raising hand(A) in the state where A knows that the coin lies heads up and B is looking is given in Figure 10.\nThe execution of an announcement action will have similar effects on agents\u2019 beliefs if it is executed in a consistency preserving structure. First, we extend Definition 17 for announcement actions as follows.\nDefinition 19 A state (M, s) is called consistency preserving for an announcement action a which announces \u03d5 if, for every u \u2208M [S] and i \u2208 FD(a,M, s) \u222a PD(a,M, s), we have that (M,u) 6|= Bi\u00ac\u03d5.\nThe next proposition is similar to Proposition 3.\nProposition 4 Let D be a consistent mA+ action theory, (M, s) be a state, and\na announces \u03d5\nin D. Assume that (M, s) is consistency preserving for a, a is executable in (M, s), and \u03a6aD(a, (M, s)) = {(M \u2032, s\u2032)}. Then, the following holds\n\u2022 (M \u2032, s\u2032) |= CFD(a,M,s)\u03d5;\n\u2022 (M \u2032, s\u2032) |= CPD(a,M,s)(CFD(a,M,s)\u03d5 \u2228CFD(a,M,s)\u00ac\u03d5); and\n\u2022 for every i \u2208 OD(a,M, s) and formula \u03c8, (M \u2032, s\u2032) |= Bi\u03c8 iff (M, s) |= Bi\u03c8.\nProof. Because of the assumption that announcement action is truthful, we have that a is executable only if (M, s) |= \u03d5. The proof of this proposition is then similar to that of Proposition 3 and is omitted for brevity. 2\nObserve that our formalization does not distinguish between public announcement and private announcement, as done in the several of the previously published works. This distinction is already captured by the observability statements.\n4.5 Entailment in mA+ Action Theories We are now ready to define the notion of entailment in mA+ action theories. It will be defined between mA+ action theories and queries of the following form:\n\u03d5 after \u03b4 (11)\nwhere \u03d5 is a belief formula and \u03b4 is a sequence of actions a1; . . . ; an (n \u2265 0)\u2014referred to as a plan. Let us observe that the entailment can be easily extended to consider more general forms of conditional plans, that include conditional statements (e.g., if-then) or even loops (e.g., while)\u2014as discussed in [31, 32]. We leave these relatively simple extensions for future work.\nThe description of an evolution of a system will deal with sets of states. We refer to a set of states as a belief state (or a b-state). We need the following definitions. For a b-state B and an action a, we say that a is executable in B if \u03a6D(a, (M, s)) 6= \u2205 for every state (M, s) in B. With a slight abuse of notation, we define\n\u03a6D(a,B) =\n{ {\u22a5} if \u03a6D(a, (M, s)) = \u2205 in some state (M, s) in B or B = {\u22a5}\u22c3\n(M,s)\u2208B \u03a6D(a, (M, s)) otherwise\nwhere {\u22a5} denotes that the execution of a in B fails. Note that we assume that no action is executable in \u22a5. Let \u03b4 be a plan andB be a b-state. The set of b-states resulting from the execution of \u03b4 inB, denoted by \u03a6\u2217D(\u03b4,B), is defined as follows:\n\u2022 If \u03b4 is the empty plan [ ] then \u03a6\u2217D([ ], B) = B;\n\u2022 If \u03b4 is a plan of the form a; \u03b4\u2032 (with a \u2208 A), then \u03a6\u2217D(a; \u03b4\u2032, B) = \u03a6\u2217D(\u03b4\u2032,\u03a6D(a,B)).\nIntuitively, the execution of \u03b4 in B can go through several paths, each path might finish in a set of states. It is easy to see that if one of the states reached on a path during the execution of \u03b4 is \u22a5 (the failed state) then the final result of the execution of \u03b4 in B is {\u22a5}. \u03a6\u2217D(\u03b4,B) = {\u22a5} indicates that the execution of \u03b4 in B fails.\nTo complete the definition of the notion of entailment, we need the following definition.\nDefinition 20 (Initial State/b-State) Let (I,D) be an action theory. An initial state of (I,D) is a state (M, s) such that for every statement\ninitially \u03d5\nin I , (M, s) |= \u03d5. (M, s) is an initial S5-state if it is an initial state and M is a S5 Kripke structure. The initial b-state of (I,D) is the collection of all initial states of (I,D). The initial S5-b-state of (I,D) is the collection of all initial S5-states of (I,D).\nWe are now ready to define the notion of entailment.\nDefinition 21 (Entailment) An action theory (I,D) entails the query\n\u03d5 after \u03b4,\ndenoted by (I,D) |= \u03d5 after \u03b4, if\n(a) \u03a6\u2217D(\u03b4, I0) 6= {\u22a5} and\n(b) (M, s) |= \u03d5 for each (M, s) \u2208 \u03a6\u2217D(\u03b4, I0)\nwhere I0 is the initial b-state of (I,D). We say that (I,D) S5-entails the query \u03d5 after \u03b4, denoted by (I,D) |=S5 \u03d5 after \u03b4, if the two conditions (a)-(b) are satisfied with respect to I0 being the initial S5-b-state of (I,D).\nThe next example illustrates these definitions.\nExample 19 Let D1 be the domain specification given in Examples 3 and 4 and I1 be the set of initial statements given in Example 10. Furthermore, let \u03b4A be the sequence of actions:\n\u03b4A = distract(A,C); signal(A,B); open(A); peek(A).\nWe can show that\n(I1, D1) |= (BAtail \u2228BA\u00actail) \u2227BA(BB(BAtail \u2228BA\u00actail)) after \u03b4A (I1, D1) |= BB(BAtail \u2228BA\u00actail) \u2227 (\u00acBBtail \u2227 \u00acBB\u00actail) after \u03b4A (I1, D1) |= BC [ \u2227 i\u2208{A,B,C}(\u00acBitail \u2227 \u00acBA\u00actail)] after \u03b4A\nunder the assumptions that initially, the beliefs of the agents satisfy the S5-axioms. To see how the above conclusions hold, let us construct an initial state for (I1, D1) (see also Figure 11). Since the truth values of all fluents but tail are known to every agent, we have that M0 = \u3008{s0, s1}, \u03c00,B0A,B0B ,B0C\u3009 where\n\u03c00(s0) = {\u00acopened, has key(A),\u00achas key(B),\u00achas key(C), looking(A), looking(B), looking(C),\u00actail}\nand\n\u03c00(s1) = {\u00acopened, has key(A),\u00achas key(B),\u00achas key(C), looking(A), looking(B), looking(C), tail}.\nFurthermore, B0A = B0B = B0C = {(s0, s0), (s0, s1), (s1, s0), (s1, s1)}. The execution of the action distract(A,C) in (M0, s0) results in the state (M1, s2) as shown in Figure 12 where M1 = \u3008{s0, s1, s2, s3}, \u03c01,B1A,B1B ,B1C\u3009. The top part of the new structure is a replica of (M0, s0), encoding the beliefs of B, who is ignorant of the action occurrence. The bottom part encodes the beliefs of A and C, who are observers of the action occurrence. It includes two new worlds, s2 and s3, which represent the result of the execution of distract(A,C) in s0 and s1 respectively, where \u03c01(s0) = \u03c00(s0), \u03c01(s1) = \u03c00(s1),\n\u03c01(s2) = \u03c00(s0) \\ {looking(C)} \u222a {\u00aclooking(C)}\nand \u03c01(s3) = \u03c00(s1) \\ {looking(C)} \u222a {\u00aclooking(C)}.\nS0 S1\nA, B, C A, B, C\nA, B, C\nFigure 11: (M0, s0): an initial state of (I1, D1)\nS2 S3\nA, C A, C\nA, C\nS0 S1\nA, B, C A, B, C\nA, B, C\nB B\nB B\nFigure 12: (M1, s2): result of execution of distract(A,C) in (M0, s0)\nFinally, B1A = B0A \u222a {(s2, s2), (s2, s3), (s3, s2), (s3, s3)}, B1C = B0C \u222a {(s2, s2), (s2, s3), (s3, s2), (s3, s3)}, and B1B = B0B \u222a {(s2, s0), (s2, s1), (s3, s0), (s3, s1)}. The execution of signal(A,B) in (M1, s2) results in a new state (M2, s6) where\nM2 = \u3008{s0, . . . , s7}, \u03c02,B2A,B2B ,B2C\u3009\nwhere:\n\u2022 For i \u2208 {0, . . . , 3}, the state si+4 is the result of executing signal(A,B) in si. We have that \u03c02(si+4) = \u03c01(si) because B is looking at the box already and thus the execution of this action does not change the state;\n\u2022 B2A = B1A \u222a {(si+4, sj+4) | 0 \u2264 i, j \u2264 3 and (si, sj) \u2208 B1A};\n\u2022 B2B = B1B \u222a {(si+4, sj+4) | 0 \u2264 i, j \u2264 3 and (si, sj) \u2208 B1B}; and\n\u2022 B2C = B1C \u222a {(si+4, sj) | 0 \u2264 i, j \u2264 3 and (si, sj) \u2208 B1C}.\nThe execution of open(A) in (M2, s6) will result in a new state (M3, s14) where\nM3 = \u3008{s0, . . . , s15}, \u03c03,B3A,B3B ,B3C\u3009\nwhere:\n\u2022 For i \u2208 {0, . . . , 7}, the state si+8 is the result of executing open(A) in si. We have that \u03c03(si+8) = \u03c02(si) \\ {\u00acopened} \u222a {opened};\n\u2022 B3A = B2A \u222a {(si+8, sj+8) | 0 \u2264 i, j \u2264 7 and (si, sj) \u2208 B2A};\n\u2022 B3B = B2B \u222a {(si+8, sj+8) | 0 \u2264 i, j \u2264 7 and (si, sj) \u2208 B2B}; and\n\u2022 B3C = B2C \u222a {(si+8, sj) | 0 \u2264 i, j \u2264 7 and (si, sj) \u2208 B2C}.\nFinally, the execution of peek(A) in (M3, s14) results in a new state (M4, s30) where\nM4 = \u3008{si | 0 \u2264 i \u2264 31}, \u03c04,B4A,B4B ,B4C\u3009\nwhere:\n\u2022 For i \u2208 {0, . . . , 15}, the interpretation \u03c04(si+15) = \u03c03(si) since the execution of a sensing action does not change the state of the world;\n\u2022 B4A = B3A \u222a {(si+16, sj+16) | 0 \u2264 i, j \u2264 15 and (si, sj) \u2208 B3A and ((\u00actail \u2208 \u03c03(si) \u2229 \u03c03(sj)) \u2228 (tail \u2208 \u03c03(si) \u2229 \u03c03(sj))};\n\u2022 B4B = B3B \u222a {(si+16, sj+16) | 0 \u2264 i, j \u2264 15 and (si, sj) \u2208 B3B}; and\n\u2022 B4C = B3C \u222a {(si+16, sj) | 0 \u2264 i, j \u2264 15 and (si, sj) \u2208 B3C}.\nWe will finally show that (M4, s30) |= BAtail \u2228 BA\u00actail. In fact, since (M0, s0) |= \u00actail, we will show that (M4, s30) |= BA\u00actail. To prove this, we need to show that (M4, sj) |= \u00actail for every sj \u2208 M4[S], (s30, sj) \u2208 B4A. Since s30 6\u2208 M3[S], we have that (s30, sj) \u2208 B4A iff j \u2265 16, (s14, sj\u221216) \u2208 B3A, and \u00actail \u2208 \u03c03(s14) \u2229 \u03c03(sj\u221216) (because \u00actail \u2208 \u03c03(s14)). This implies that (M4, s30) |= BA\u00actail. The proof of other conclusions is similar.\nWe conclude the example with the observation that another initial state for (I1, D1) is (M0, s1) and the execution of \u03b4A in this state results in a new state (M \u2032, s\u2032) with the property that (M \u2032, s\u2032) |= BAtail. Theoretically, this fact and the fact that (M4, s30) |= BA\u00actail are insufficient for us to conclude that (I1, D1) |=S5 BAtail \u2228 BA\u00actail holds. This, however, holds under some additional assumption and thanks to Proposition 15 (see Section 6).\n5 Update Model Based Transitions for mA+ Domains The previous section presented a transition function formA+ domains in the style of action languages. In this section, we will develop an alternative definition of transitions between states in mA+ domains that has its roots in dynamic epistemic logic. For each action a and state (M, s), we will describe an update instance whose application to (M, s) results in states equivalent to states belonging to \u03a6D(a, (M, s)). Since an update instance describes the effect of a particular action occurrence, it will depend not only on the action specifications but also on a frame of reference. As discussed in the previous section, a frame of reference in a mA+ domain over the signature \u3008AG,F ,A\u3009 is a tuple (F, P,O) where F \u222a P \u222aO = AG and F , P , and O are pairwise disjoint.\nLet us start by considering the world-altering actions.\nDefinition 22 (Update Model/Instance for World-Altering Actions) Given a world-altering action a with the precondition \u03c8 and a frame of reference \u03c1 = (F, P,O), the update model for a and \u03c1, denoted by \u03c9(a, \u03c1), is defined by \u3008\u03a3, R1, . . . , Rn, pre, sub\u3009 where\n\u25e6 \u03a3 = {\u03c3, };\n\u25e6 Ri = {(\u03c3, \u03c3), ( , )} for i \u2208 F and Ri = {(\u03c3, ), ( , )} for i \u2208 O;\n\u25e6 pre(\u03c3) = \u03c8 and pre( ) = >; and\n\u25e6 sub( ) = \u2205 and sub(\u03c3) = {p\u2192 \u03a8+(p, a) \u2228 (p \u2227 \u00ac\u03a8\u2212(p, a)) | p \u2208 F}, where \u03a8+(p, a) = \u2228 {\u03d5 | [a causes p if \u03d5] \u2208 D} and \u03a8\u2212(p, a) = \u2228 {\u03d5 | [a causes \u00acp if \u03d5] \u2208 D}.\nThe update instance for the world-altering action a and the frame of reference \u03c1 is (\u03c9(a, \u03c1), {\u03c3}).\nObserve that the update model of the world-altering action a has only two events. Each event corresponds to a group of agents. The links in the update model for each group of agents reflect the state of beliefs each group would have after the execution of the action. For example, fully observant agents (in F ) will have no uncertainty. The next example illustrates this definition.\nExample 20 Going back to our original example, the action open(A) assumes that everyone is aware that C is not looking at the box while B and A are. Figure 13 (top right) depicts the state. For simplicity, in the worlds we report only the components of the interpretation related to the opened and tail fluents. The frame of reference for open(A) in this situation is ({A,B}, \u2205, {C}). The corresponding update instance for open(A) and the frame of reference ({A,B}, \u2205, {C}) is given in Figure 13 (top left). The bottom part of Figure 13 shows the result of the application of the update instance to the state on the top right.\nIn the next definition, we provide the update instance for a sensing action given a frame of reference. For simplicity of presentation, we will assume that the set of sensed fluents of the action is a singleton.\nDefinition 23 (Update Model/Instance for Sensing Actions) Let a be a sensing action with SensedD(a) = {f}, let its precondition be \u03c8, and let \u03c1 = (F, P,O) be a frame of reference. The update model for a and \u03c1, \u03c9(a, \u03c1), is defined by \u3008\u03a3, R1, . . . , Rn, pre, sub\u3009 where:\n\u25e6 \u03a3 = {\u03c3, \u03c4, };\n\u25e6 Ri is given by\nRi =  {(\u03c3, \u03c3), (\u03c4, \u03c4), ( , )} if i \u2208 F{(\u03c3, \u03c3), (\u03c4, \u03c4), ( , ), (\u03c3, \u03c4), (\u03c4, \u03c3)} if i \u2208 P{(\u03c3, ), (\u03c4, ), ( , )} if i \u2208 O \u25e6 The preconditions pre are defined by\npre(x) =  \u03c8 \u2227 f if x = \u03c3\u03c8 \u2227 \u00acf if x = \u03c4> if x = \u25e6 sub(x) = \u2205 for each x \u2208 \u03a3.\nThe update instance for the sensing action a and the frame of reference \u03c1 is (\u03c9(a, \u03c1), {\u03c3, \u03c4}).\nObserve that an update instance of a sensing action has three events, each one corresponding to a group of agents. Each event is associated with a group of states in which the truth value of sensed fluent is either known to be true, known to be false, or unknown.\nExample 21 Let us consider the occurrence of the action peek(A) in the state described in Figure 14 (top right). The frame of reference for this occurrence of peek(A) is ({A}, {B}, {C}). The corresponding update instance is given in Figure 14 (top left). Its update on the given state results in the same state as in Example 15 (bottom, Figure 14).\nWe will conclude the section with a discussion on the update model of for announcement actions.\nDefinition 24 (Update Model/Instance for Announcement Actions) Given an announcement action a \u2208 A that announces \u03d5 with the precondition \u03c8 and a frame of reference \u03c1 = (F, P,O), the update model for a and \u03c1, \u03c9(a, \u03c1), is defined by \u3008\u03a3, R1, . . . , Rn, pre, sub\u3009 where:\n\u25e6 \u03a3 = {\u03c3, \u03c4, };\n\u25e6 Ri is defined by\nRi =  {(\u03c3, \u03c3), (\u03c4, \u03c4), ( , )} if i \u2208 F{(\u03c3, \u03c3), (\u03c4, \u03c4), ( , ), (\u03c3, \u03c4), (\u03c4, \u03c3)} if i \u2208 P{(\u03c3, ), (\u03c4, ), ( , )} if i \u2208 O \u25e6 pre is defined by\npre(x) =  \u03c8 \u2227 \u03d5 if x = \u03c3\u03c8 \u2227 \u00ac\u03d5 if x = \u03c4> if x = \u25e6 sub is defined as sub(x) = \u2205 for any x \u2208 \u03a3.\nThe update instance for the announcement action a with respect to the frame of reference \u03c1 is (\u03c9(a, \u03c1), {\u03c3}).\nAs we can see, an update model for an announcement action and a frame of reference is structure-wise identical to the update model for a sensing action and a frame of reference. The main distinction lies in the set of designated events in the update instance for each type of actions. There is only a single designated event for announcement actions while there are two for sensing actions.\nExample 22 Let us consider the two versions of the action raising hand(A) described in Example 17 and the state in which B is looking at the box and both A and B are aware of it.\nFor the first version of the action the frame of reference for its occurrence is \u03c1 = ({A}, {B}, {C}). The update instance (\u03c9(raising hand(A), \u03c1), {\u03c3}) is illustrated in Figure 15. Similarly, the update instance for the second version of the action in the same state, which results in the frame of reference is ({A,B}, \u2205, {C}), is shown in Figure 16.\n\u03c3\n\u03c4\n! pre: T\npre: T \u2227 \u00actail\npre: T \u2227 tail\nA,B\nA,B\nB A,B,C\nC\nC\nFigure 15: Update instance for the raising hand(A) action and \u03c1 = ({A}, {B}, {C})\nDefinitions 22-24 allow us to define the following notion.\nDefinition 25 Let a be an action and (M, s) be a state. The update instance of a in (M, s), denoted by \u2126(a, (M, s)), is defined as follows:\n\u2022 \u2126(a, (M, s)) = \u2205 if a is not executable in (M, s);\n\u2022 \u2126(a, (M, s)) is the update instance for a and the frame of reference (FD(a,M, s), PD(a,M, s), OD(a,M, s)) as defined in the Definitions 22-24.\nThe following proposition relates the two approaches to define the successor states of an action occurrence in mA+.\nProposition 5 Given an action a and a state (M, s) such that \u03a6D(a, (M, s) 6= \u2205, we have that for each (M \u2032, s\u2032) \u2208 (M, s)\u2297 \u2126(a, (M, s)) there exists an equivalent state in \u03a6D(a, (M, s)) and vice versa, where (M, s)\u2297 \u2205 = \u2205."}, {"heading": "6 Definite Action Theories", "text": "In this section, we identify a class of mA+ action theories that can be each described by a finite number of initial states, up to a notion of equivalence (Definition 4). The motivation for this task lies in the desire to use available technologies (e.g., answer set solvers and/or forward search planners) in the computation of the entailment of mA+ theories\u2014which requires the presence of a finite number of initial states. We observe that this problem does not arise in single-agent domains, where the size of the state space is bounded by 2|F|. On the other hand, theoretically, there could be infinitely many initial states for an arbitrary mA+ theory. For example, given a state (M, s) and a set of formulae \u03a3 such that (M, s) |= \u03a3, a new state (M \u2032, s) that also satisfies \u03a3 can be constructed from M by adding a new world and keeping everything else unchanged.\nOne way to cope with the aforementioned problem is to limit the type of formulae occurring in the initial statements of the action theory. Another way to deal with it is to limit the type of Kripke structures considered as initial states.\nIndeed, we can observe that several examples found in the literature have the following properties: (i) the initial state can be described by a set of statements involving the common knowledge among all agents; (ii) the common knowledge relates to whether a particular agent is aware of (or not aware of) a property of the world; and (iii) the beliefs of the agents coincide with their knowledge about the world. This suggests that several interesting problem domains can be captured by limiting our attention to specific types of initial Kripke structures and to specific types of initial statements.\nSpecifically, we will focus on S5-initial states and consider initial statements of the following forms:\ninitially \u03d5 (12) initially C\u03d5 (13)\ninitially C(Bi\u03d5) (14) initially C(Bi\u03d5 \u2228Bi\u00ac\u03d5) (15)\ninitially C(\u00acBi\u03d5 \u2227 \u00acBi\u00ac\u03d5) (16)\nwhere \u03d5 is a fluent formula. Intuitively, statements of type (12) indicate properties that are true in the real state of the world; statements of type (13) denote properties that all agents believe to be true (and all agents know about the other agents\u2019 beliefs about such property); statements of type (14)-(15) indicate that all agents believe that agent i is aware of whether \u03d5 is true or false; statements of type (16) indicate that all agents believe that agent i is not aware of whether \u03d5 is true or false.\nWe will now formally define the notion of a definite action theory. For an action theory (I,D), let\nTI = {\u03d5 | [initially \u03d5] \u2208 I}.\nWe will say that (I,D) is consistent if TI and D are consistent.\nDefinition 26 (Definite Action Theory) An action theory (I,D) is said to be definite if\n\u2022 Each initial statement in I is of the form (12)-(16); and\n\u2022 For each fluent formula \u03d5 and agent i, I contains an initial statement of the form (14), (15), or (16) in which i and \u03d5 occurs.\nWe prove that S5-initial states of definite action theories are finitely computable in the next proposition.\nProposition 6 For a consistent definite action theory (I,D), there exists a finite number of initial S5-states (M1, s1), . . . , (Mk, sk) such that every initial S5-state (M, s) of (I,D) is equivalent to some (Mi, si). Furthermore, for each pair of i 6= j and u \u2208Mi[S] there exists some v \u2208Mj [S] such that Mi[\u03c0](u) \u2261Mj [\u03c0](v).\nNote that the last part of the proposition indicates that the set of interpretations used in each S5-state is the same. The proof of the above proposition relies on a series of lemmas. Let us start by introducing some useful notations. Given a state (M, s) and u, v \u2208 M [S], we will refer to a path between u and v as a sequence of worlds u = u0, u1, . . . , un = v in M [S], where for j = 0, . . . , n \u2212 1, there exists some ij \u2208 AG such that (uj , uj+1) \u2208 M [ij ]. We say that v is reachable from u if there exists a path from u to v. We will also often make use of the fact that, for a S5-structure, the relations Bi is symmetric, transitive, and reflective (Theorem 3.1.5, [28]). We list these lemmas below. Proofs are provided in the appendix.\nLemma 7 Every S5-state (M, s) is equivalent to a S5-state (M \u2032, s) such that, for every world u \u2208 M \u2032[S], we have that u is reachable from s.\nThe above lemma indicates that for a S5-state (M, s), worlds that are unreachable from s can be removed. The next lemma deals with initial statements of the form (12)-(14).\nLemma 8 Let (M, s) be a S5-state such that every world u \u2208 M [S] is reachable from s. Let \u03c8 be a formula. Then, (M, s) |= C(\u03c8) iff M [\u03c0](u) |= \u03c8 for every world u \u2208M [S].\nThe lemma shows that for a S5-state (M, s) that satisfies a statement of the form (12)-(14), the literal ` or the formula \u03d5 appearing in the statement must be satisfied at every world in M . The next lemma characterizes S5-states satisfying initial statements of the form (15).\nLemma 9 Let (M, s) be a S5-state such that every world u \u2208 M [S] is reachable from s. Let \u03c8 be a fluent formula. Then:\n(M, s) |= C(Bi\u03c8 \u2228Bi\u00ac\u03c8) iff ( \u2200u, v \u2208M [S] : (u, v) \u2208M [i]\u21d2 ( M [\u03c0](u) |= \u03c8 iff M [\u03c0](v) |= \u03c8 )) The lemma proves that for each S5-state (M, s) satisfying a statement of the form (15), every pair of worlds related by Bi either both satisfy or both do not satisfy the formula \u03d5 appearing in the statement. The next lemma deals with initial statements of the form (16).\nLemma 10 Let (M, s) be a S5-state such that every u \u2208 M [S] is reachable from s. Let \u03c8 be a fluent formula. Then, (M, s) |= C(\u00acBi\u03c8 \u2227 \u00acBi\u00ac\u03c8) iff for every u \u2208 M [S] there exists some v \u2208 M [S] such that (u, v) \u2208 M [i], and M [\u03c0](u)(\u03c8) 6= M [\u03c0](v)(\u03c8).\nThe lemma proves that for a S5-state (M, s) that satisfies a statement of the form (16), there must be at least one pair of worlds in which the formula in the statement is not satisfied in both worlds. Observe that Lemmas 7-10 provide a first characterization of S5-states satisfying initial statements of the forms (12)-(16). These lemmas do not take into consideration the second condition in Definition 26. The next lemma, together with Lemmas 7-10, focuses on this condition and allows us to determine properties of interpretations associated to the worlds in an initial S5-states of a definite action theory.\nLemma 11 Let (M, s) be an initial S5-state of a definite action theory, such that every u \u2208 M [S] is reachable from s. Let \u03d5 be a fluent formula and i \u2208 AG. Then,\n\u2022 If (M,u) |= Bi\u03d5 for some u \u2208M [S] then (M, s) |= C(Bi\u03d5) or (M, s) |= C(Bi\u03d5 \u2228Bi\u00ac\u03d5);\n\u2022 If (M,u) |= \u00acBi\u03d5 for some u \u2208M [S] then (M, s) |= C(\u00acBi\u03d5 \u2227Bi\u00ac\u03d5).\nIn order to complete the proof of Proposition 6, we need some additional notation. Consider a Kripke structure M = \u3008S, \u03c0,B1, . . . ,Bn\u3009. We define a relation \u223c among worlds of M as follows. For each u, v \u2208 M [S], u \u223c v iff \u03c0(u) = \u03c0(v). Thus, u \u223c v indicates that the interpretations associated to u and v (i.e., M [\u03c0](u) and M [\u03c0](v)) are identical. It is easy to see that \u223c is an equivalence relation over M [S]. Let u\u0303 denote the equivalence class of u with respect to the relation \u223c (i.e., u\u0303 = [u]\u223c). The next lemma is critical for the proof of Proposition 6.\nLemma 12 Let (M, s) be an initial S5-state of a definite action theory, such that every u \u2208 M [S] is reachable from s. Let u, v \u2208 M [S] such that u \u223c v. Then, for every i \u2208 AG and x \u2208 M [S] such that (u, x) \u2208 M [i] there exists y \u2208M [S] such that (v, y) \u2208M [i] and x \u223c y.\nThe above lemma allows us to collapse all worlds with the same interpretation into a single world. This is made precise as follows. Given a structure M , let M\u0303 be the structure constructed as follows:\n\u2022 M\u0303 [S] = {u\u0303 | u \u2208 S}\n\u2022 For every u \u2208M [S] and f \u2208 F , M\u0303 [\u03c0](u\u0303)(f) = M [\u03c0](u)(f)\n\u2022 For each i \u2208 AG, (u\u0303, v\u0303) \u2208 M\u0303 [i] if there exists (u\u2032, v\u2032) \u2208M [i] for some u\u2032 \u2208 u\u0303 and v\u2032 \u2208 v\u0303.\nIntuitively, M\u0303 is obtained from M by replacing each equivalence class in M with a single world. We will call (M\u0303, s\u0303) the reduced state of (M, s). Thanks to the previously considered properties, the following holds:\nLemma 13 Let (M, s) be a S5-state such that every u \u2208M [S] is reachable from s. Then, the reduced state (M\u0303, s\u0303) of (M, s) is a finite S5-state.\nThe next lemma shows that a reduced state of an initial S5-state of a definite action theory is also an initial S5-state of the action theory.\nLemma 14 Let (M, s) be a S5-state such that every state u in M [S] is reachable from s. Furthermore, let (M\u0303, s\u0303) be the reduced state of (M, s). It holds that\n1. (M, s) |= \u03c8 iff (M\u0303, s\u0303) |= \u03c8;\n2. (M, s) |= C(\u03c8) iff (M\u0303, s\u0303) |= C(\u03c8);\n3. (M, s) |= C(Bi\u03c8) iff (M\u0303, s\u0303) |= C(Bi\u03c8);\n4. (M, s) |= C(Bi\u03c8 \u2228Bi\u00ac\u03c8) iff (M\u0303, s\u0303) |= C(Bi\u03c8 \u2228Bi\u00ac\u03c8));\n5. (M, s) |= C(\u00acBi\u03c8 \u2227 \u00acBi\u00ac\u03c8) iff (M\u0303, s\u0303) |= C(\u00acBi\u03c8 \u2227 \u00acBi\u00ac\u03c8));\nwhere i \u2208 AG and \u03c8 is a fluent formula.\nLemmas 14 and 13, together with the result on the existence of an initial S5-state with finite number of worlds in [28], allow us to prove Proposition 6. Observe that Proposition 6 opens the door to the use of well-known propositional tools, such as answer set solvers, to compute the entailment relation in definite action theories. On the other hand, the number of initial Kripke structures could still be prohibitively large. We are next interested in domains which determine, modulo equivalence among the Kripke structures, a single initial S5-state. Observe that a state (M, s) has two components, the Kripke structure M and the actual world s. In order for a definite action theory (I,D) to have a unique initial S5-state, the possible interpretation of the real state of the world must be unique. ForM to be unique, we observe that in the singe-agent case (e.g., in action languageA, B, etc.), the initial state is unique when the knowledge of the agent is complete. We generalize this to the case of multi-agent domains as follows.\nDefinition 27 A definite action theory (I,D) is complete if\n{` | ` appears in one of the statements of the form (12)\u2014(14) in I}\nis a complete interpretation of F .\nIntuitively, a complete definite action theory is such that the knowledge of each agent is known to all other agents and the actual world is completely specified.\nProposition 15 For a consistent and complete action theory (I,D), there is a unique initial S5-state (M0, s0) with |M0[S]| \u2264 2|F| such that every initial S5-state (M, s) of (I,D) is equivalent to (M0, s0).\nPropositions 6-15 show that definite action theories have finitely many (or unique) initial S5-states. The second condition of Definition 26, on the other hand, indicates that the size of the set of initial statements of a definite action theory is infinite as there are infinitely many fluent formulae. Clearly, this is not desirable. To address this issue, we will next propose a simplification of Definition 26. This simplification is similar to the use of the Closed World Assumption (CWA) to represent incomplete information in databases. More specifically, we propose that initial statements of the form (16) are given implicitly, i.e., by representing the information that the agents do not know implicitly. This can be realized as follows.\nFor a set of fluents F , a disjunction \u03d5 over F is called a complete clause if for every f \u2208 F , either f or \u00acf appears in \u03d5 but not both. For a set of initial statements I and an agent i \u2208 AG, let\nC(I, i) = { \u03d5 \u03d5 is a complete clause,TI 6|= C(Bi\u03d5) and TI 6|= C(Bi\u03d5 \u2228Bi\u00ac\u03d5) } Let neg(I) = \u22c3 i\u2208AG{initially C(\u00acBi\u03d5\u2227\u00acBi\u00ac\u03d5) | \u03d5 \u2208 C(I, i)}. The completion of I is comp(I) = I\u222aneg(I). It is easy to see that for any action theory (I,D) such that I contains only statements of the form (12)-(16), (comp(I), D) is a definite action theory. This means that we can specify a definite action theory by specifying only statements of the form (12)-(15). Such a specification is obviously finite. Under this consideration, we can define a definite action theory as follows.\nDefinition 28 (Definite Action Theory under CWA) An action theory (I,D) is called a definite action theory under the CWA if I contains only initial statements of the form (12)-(16).\nAn initial state of a definite action theory under the CWA (I,D) if it is an initial state of (comp(I), D).\nObserve that if we consider the action theory (I1, D1) in Example 19 under the CWA, (M0, s0) and (M0, s1) are the only two initial S5-states of (comp(I1), D1) and thus the entailment proved in Example 19 indeed hold.\nIt is easy to see that S5-entailment of definite action theories under the CWA can be computed thanks to Proposition 6. Studying the complexity of this problem and developing algorithms for computing the S5-entailment in definite action theories under the CWA are interesting research topics and the focus of our future work. O"}, {"heading": "7 Related Work and Discussion", "text": "In this section, we connect our work to related efforts in reasoning about actions and their effects in multi-agent domains. The background literature spans multiple areas. We will give a quick introduction and focus our attention on the most closely related works."}, {"heading": "7.1 Related Logics", "text": "The research discussed in this paper relates to a broad variety of logics and languages used to deal with reasoning about actions and their effects in multi-agent domains\u2014e.g., classical logic, non-monotonic logics, causal logics, high level action languages, modal logics, epistemic logics, dynamic logics and dynamic epistemic logics. Here, we provide a very brief overview of their relevance to reasoning about actions and their effects in multi-agent domains.\nClassical logics, in particular, propositional and first-order logic, are often used to specify the physical state of the world. For example, the propositional formula on table a\u2227 on table b\u2227\u00acon table c expresses that the blocks a and b are on the table and the block c is not of the table. Similarly, the first order formula on table(a) \u2227 on table(b) \u2227 \u00acon table(b)\u2227\u2200X(ontable(X)\u21d2 color(X, red)), describes a situation where the blocks a and b are on the table, the block c is not on the table, and all blocks on the table are red. Propositional logic and first-order logic can be used to represent the effects of actions and to reason about them. However, straightforward encodings require a large number of axioms, especially to represent the inertia axioms\u2014the properties of the world that do not change when a particular action is performed. Two approaches can be considered to address this problem: (i) By using non-monotonic logics, that can naturally express statements of the type \u201cNormally an action does not affect a property\u201d and can express exceptions to this statement; (ii) By an alternative approach, used for example in [33], where the effects of actions on various properties of the world are expressed using a high-level logic, which is then translated, using sophisticated compilation techniques, into succinct encodings of inertia axioms in a classical logic.\nWhile reasoning about the effect of actions, the relationship between some properties of the world may give rise to \u201cqualification\u201d and \u201cramification\u201d. Expressing this in classical logic leads to problems in many cases, especially when the relationship between the properties are causal in nature. For example consider the two statements:\n(a) A person cannot be in two places at the same time and\n(b) A person cannot be married to two persons at the same time.\nIn classical logic, their representations are very similar: at(X) \u2227 at(Y ) \u21d2 X = Y and married to(X) \u2227 married to(Y ) \u21d2 X = Y . But let us assume that, initially, a person is at location p and he performs the action of moving to a location q different from p. The statement (a) causes a ramification\u2014since we would like to infer that, after the execution of the action, the person is at p and not at q. On the other hand, let us assume that initially a person is married to p and he (tries to) perform the action of marrying q (in a courthouse with marriage records) different from p. The statement (b), this time, introduces a qualification, because of which the person is unable to perform the action of marrying q.\nCausal logic allows us to express (a) and (b) in a different way: at(X) \u2227 X 6= Y causes \u00acat(Y ) and married to(X) \u2227 married to(Y ) \u2227 X 6= Y causes FALSE. Such causal relationships can be expressed using logic programming, which has a non-classical connective \u201c\u2190.\u201d In logic programming (a) and (b) can be expressed as: \u00acat(Y )\u2190 at(X), X 6= Y and\u2190 married to(X),married to(Y ), X 6= Y .\nHigh-level action languages were introduced to (a) provide a English-like specification language to describe the effects of actions on properties of the world and the relationships between these properties, and to provide a semantics that uses simple set theoretical notations; and (b) provide a framework that can be used to prove correctness of encodings in various logics for reasoning about effect of actions. For example, specifications in the language A [4] are of the forms: (i) a causes f if p1, . . . , pn; and (ii) f after a1, . . . an.\nModal logics add modal operators to various logics and their semantics is often defined using Kripke structures. Two simple modal logics that are relevant to reasoning about actions are temporal logics and epistemic logics. One of the simplest temporal logic is the forward linear temporal logic; it includes the operators 2, \u00a9, 3, and U , meaning \u201calways in the future\u201d, \u201cnext time step\u201d, \u201csometime in the future\u201d and \u201cuntil.\u201d Formulae in this logic are often used to express goals that specify how we want the world to evolve. Reasoning is commonly focused on action-plans, to verify if an action-plan satisfies a desired temporal specification, or to derive action-plans that satisfy a goal, given as a temporal specification. Epistemic logics use the modal operators Ki, where Kif means that agent i knows that f is true. In this paper, we used belief logics which are similar to epistemic logics, the main difference being that we use the modal operators Bi, where Bif means that the agent i believes that f is true.\nThe community working on reasoning about actions and change initially used only the sequencing constructs to build action-plans as sequence of actions and reason about such sequences. This has been extended [34, 32] to allow \u201cif-then\u201d and other procedural features, that become necessary when sensing actions are needed to be part of plans. GOLOG [31], an acronym for \u201calGOl in LOGic\u201d, borrows programming constructs from procedural languages to express complex plans which can be \u201cevaluated\u201d to generate valid action sequences. The evaluation of GOLOG programs, as well as other action-plans, is realized on top of action theories that allow one to reason about a single action and its impact on the world\u2014and, in the process, take into account the issues regarding inertia, qualification and ramification.\nA related area of research is focused on the exploration of Dynamic logics. Dynamic logics were originally developed to reason about program correctness. In the programming domain, the basic actions are assignments of values to variables. Reasoning about the effects of such assignment actions and the associated inertia is straightforward, since all variables retain their old values except for the variables being assigned. In traditional procedural programming languages, one does not have to worry about qualification and ramification, since assigning a value to a variable does not have any implicit impact on other variables.8 Any needed impact is explicitly written as new assignment statements. The original focus of dynamic logics is to reason about programs that are built by composing simple assignment statements. Such programs are built using constructs such as: (i) \u03b1 \u222a \u03b2 (i.e., execute \u03b1 or \u03b2), (ii) \u03b1;\u03b2 (i.e., execute \u03b1 followed by \u03b2), (iii) \u03b1\u2217 (i.e., iterate \u03b1 a finite number of times), (iv) p? (i.e., test whether \u03b1 holds), and (v) \u03bb (i.e., no-op).\nTo reason about programs in dynamic logics, programs are used as modal operators; various axioms and inference rules have been developed that allow one to reason about the programs. The modal constructs used are of the form: (i) [a]p, meaning that, after performing a, it is necessarily the case that p is true in the world, and (ii) \u3008a\u3009p, meaning that, after performing a, it is possible that p is true in the world. These modal constructs allow one to specify effects of arbitrary actions. For example, to express that an action a makes the formula \u03c6 true, one can write [a]\u03c6. Similarly, to express that a makes the formula \u03c6 true only when executed in a state where \u03c8 is true, one can write [\u03bb](\u03c8 \u21d2 [a]\u03c6). However, when we go beyond assignment actions, one needs to account for inertia. Similarly, when one goes beyond the programming environment and variable assignments, one needs to worry about qualification and ramification. The inertia axioms can be expressed in dynamic logic as shown in [35], which is similar to writing inertia axioms using classical logic\u2014i.e., they are encoded using a large number of formulae, that list one by one what properties are not affected by an action. Such approach to account for ramification is tedious and involves the use of logic programming, pre-compilation and then generating dynamic logic formulae based on such pre-compilation. In [36], where dynamic logic is used for reasoning about actions and change, inertia is avoided by referring to it as an undesirable overcommitment; however, the authors admit that their formulation cannot address qualification, and they indicate that \u201cnon-monotonic logics are clearly superior\u201d in that regard. Overall, when reasoning about actions and change (beyond simple variable assignments) using dynamic logics, one still needs to worry about inertia and the frame problem, qualification and ramification issues.\n8If we ignore issues of aliasing, e.g., through pointers.\nMany early works about action and change, as well as dynamic logics, reason about the world and do not worry about agent\u2019s knowledge about the world. When sensing actions are considered, one has to distinguish between the world and a single agent\u2019s knowledge about the world. This leads to the use of epistemic logics and Kripke structures, and frame axioms need to be developed with respect to knowledge formulae. This has been achieved by having frame axioms for the accessibility relations used in the Kripke structures. High level languages that can express sensing actions and their effects have been developed and matched with logical encodings. This paper extends such approach to the case of multi-agent domains, where other knowledge actions (besides sensing) are considered. A new dimension that emerges is the observability of the various agents as part of an action; some may have full observability, some others may have partial observability, and the rest have no observability. The result of such actions and such varied observability is that different agents have different knowledge and beliefs about the world and about each other\u2019s knowledge and beliefs.\nSeveral researchers have considered reasoning about actions in a multi-agent domain using the dynamic logics approach. These proposals are focused on extending dynamic logics to Dynamic Epistemic Logics (DEL), to reason about the agent\u2019s knowledge about the world and about other agents\u2019 knowledge in presence of multi-agent actions. The extensions are twofold. In particular, in the formula [\u03b1]p: (i) In DEL, p is a formula in epistemic logic, while in dynamic logic p is a classical logic formula, and (ii) In DEL, \u03b1 is a more general action than in dynamic logic. In [24], \u03b1 has the usual dynamic logic constructs for complex actions, plus constructs such as: (a) LA?\u03d5 whose intuitive meaning is that the agents in the group A learn that \u03d5 is true; and (b) (\u03b1!\u03b1\u2032) whose intuitive meaning is that between \u03b1 and \u03b1\u2032, we choose \u03b1. The latter is often written as (!\u03b1 \u222a \u03b1\u2032). Using these actions, the authors of [24] show that one can express sensing actions (refereed to as \u201cread\u201d actions) in a multi-agent setting. Specifically, the fact that we have two agents a and b, a senses the value of p as being true, and it is common knowledge between a and b that b observes a sensing, can be modeled as the complex action Lab(!La?p\u222aLa?\u00acp). However, the language of update models [20] is more general and is currently preferred by the DEL community to express such multi-agent actions. In the language of update models, the action discussed above can be expressed as in Figure 17.\n7.2 Relating mA+ and DEL with Update Models: Differences The similarities between mA+ and the update models based approach has been discussed in detail in Section 5. We next detail the differences between the two approaches."}, {"heading": "7.2.1 Multi-Agent Actions", "text": "A key difference between the formalism proposed in this paper and DEL with update models is with respect to the simplest of actions in presence of multiple agents.\nLet us consider the simplified version of the coin in a box problem as presented in Example 2\u2014with three agents A, B, and C, a box containing a coin, and initially it is common knowledge that none of the agents knows whether the coin lies heads up or tails up. Let us assume that agent A peeks into the box. In our formalism, we express this action as peek(A). In DEL, the update model for the same action, as given in Figure 14, will also include additional information about all three agents A, B, and C encoding their \u201croles\u201d or \u201cperspectives\u201d while A is peeking into the box. By roles or perspectives we mean information about what the agents are doing, in terms of who is watching whom and who knows about that.\nIt is evident that our representation of the action simply as peek(A) is much simpler than the DEL representation in Figure 14. But our representation does not include the information about what else the agents A, B, and C are\ndoing while A is peeking into the box. In our formulation, such information is part of the state, and is expressed by using perspective fluents, such as looking(B)\u2014that encodes the information that B is looking at the box\u2014and group member(B,group(A))\u2014that encodes the information that B and A are together in the same group.\nThus, it appears that a critical difference between the mA+ approach to representing multi-agent actions and the approach used in DEL with update models lies in the way we encode the information about agents roles and perspectives\u2014as part of the action in DEL with update models and as part of the state in mA+. At first glance, this difference may not appear far reaching. However, there are some far reaching implications of such difference, as discussed in the following subsections.\nNarratives and Dynamic Evolution of Multi-agent Actions: Let us consider a scenario with two agents A and B. Initially, agentB is looking at agentA. AgentA lifts a block and, after some time, agentA puts down the block. Some time later, agent B is distracted while agent A again lifts the block.\nIn our formulation, this narrative can be formalized by first describing the initial situation, and then describing the sequence of actions that occurred, which for this example is:\nliftBlock(A); putDown(A); distract(B); liftBlock(A).\nThe description of this evolution of scenario in DEL is not as simple: each action occurrence will have to be described as an update model containing information about both agents A and B. In addition, such a description (in DEL) will be partly superfluous, as it will have to record information about B looking (or not looking) at A in the update model, when that information is already part of the state. Thus, the approach used in mA+ to describe this narrative is more natural than the representation in DEL.\nObserve that, in our narrative, the action liftBlock(A) appears twice. However, due to the difference in the roles and perspectives over time, the two occurrences of liftBlock(A) correspond to two different update models. This shows how, using the mA+ formulation, we can support the dynamic evolution of update models, as result of changes in perspective fluents in the state. In DEL, the two update models are distinct and there is no direct connection between them and neither one does evolve from the other.\nIn order to further reinforce this point, let us consider another narrative example. Let us consider a scenario with three agents, A, B, and C. Initially, it is common knowledge that none of the agents knows whether the coin in the box is lying heads up or tails up. In addition, let us assume that initially A and B are looking at the box, while C is looking away. Let us consider the narrative where A peeks into the box; afterwards, A realizes that C is distracted and signals C to look at the box as well; finally A peeks into the box one more time. In mA+, this situation can be described again by a sequence of actions:\npeek(A); signal(C); peek(A)\nThe two occurrences of peek(A) correspond to two different update models; the second occurrence is an evolution of the first caused by the execution of signal(C). In DEL, the relevance of the intermediate action signal(C), and its impact on the second occurrence of peek(A), is mostly lost\u2014and this results in the use of two distinct update models for peek(A) with complete information about the whole action scenario.\nThe key aspect that allows a natural representation of narratives and evolution of update models in mA+ is the presence of the agents\u2019 perspectives and roles encoded as perspective fluents of a state, and their use to dynamically generate the update models of the actions. While DEL can include perspective fluents as part of the states as well, it does not have a way to take advantage of them in a similar way as mA+.\nSeparation of Specification of Actions and their Effects and the Observability of an Agent: As alluded in the previous two sections, a core difference between mA+ and the DEL specification of actions in multi-agent domains, lies in that mA+ separates the specification of actions and action effects from the description of observability of the action occurrence by each agent. In both [24] and [20], the observability of agents is hard-coded in the specification of each complex action. We discuss this difference in more detail using an example.\nLet us reconsider the domain D1. In order to describe the possible histories of the domain, we need to develop the update models for every action occurrence. Let us consider, for example, the action peek(A); we need to have an update model for all of the following cases:\n\u2022 Both B and C are looking; \u2022 Either B or C is looking but not both; and \u2022 Both B C are not looking.\nIn our approach, the above example is specified in a very different way: the action is about sensing tail. The agents who sense it, who observe the sensing take place, and who are oblivious can be specified directly or can be specified indirectly in terms of conditions, such as which agents are near the sensing, which ones are watching from far, and which ones are looking away, respectively. Actions can be planned and executed to change the observers and partial observers. In other words, in mA+, if we have a complex action \u03b1;\u03b2, by executing \u03b1 we may be able to change the world, in terms of who is fully observing, who is partially observing, and who is oblivious with respect to the next action \u03b2. This is not possible in DEL. Thus, while mA+ allows us to develop plans where an agent can manipulate the observability of other agents, such planning cannot be done in straightforward manner in DEL.\nSimplicity by Design: The formulation adopted in this paper is limited in expressivity to ensure simplicity. It is limited by the (perspective) fluents we have and how we use them. On the other hand, DEL is more complex and also more expressive.\nOne advantage of our simplicity is that it limits the number of possible plans of a particular length, contributing to the feasibility of multi-agent planning. In DEL, since update models are analogous to Kripke models, even in presence of a small number fluents, it is possible to generate an infinite number of update models. One can limit the number of update models by imposing restrictions on their structure. Nevertheless, as discussed earlier, an update model is much more complex than actions mA+, which are often9 single units.\nAs an example,10 Figure 18 displays an update model that cannot be represented in mA+. The intuition behind this update model is as follows. When A executes the action peek(A), A believes that both A and B can see the outcome of sensing tail\u2014i.e., A and B are fully observant. In reality, B is oblivious. This shows that, in multi-agent domains, an agent\u2019s observability could also be considered as beliefs, and as such affect the beliefs of an agent about other agents\u2019 beliefs after the execution of an action. The presentmA+ language does not allow for such specification.\nThe simplicity of our formulation is by design, and not an inherent flaw of our approach. Indeed, one could envision developing a complete encoding of the complex graph structure of an update model as part of state, using an extended collection of perspective fluents\u2014but, at this time, we do not have a corresponding theory of change to guide us in using these more expressive perspective fluents to capture the full expressive power of update models in DEL. Hence, our current formalism is less expressive than DEL. However, the higher expressiveness of update models provides us with a target to expand mA+ and capture more general actions. Expanding mA+ to express actions as the one in Fig. 18 will be one of our immediate future goal.\nOn the other hand, as remarked earlier, the research on DEL with update models lacks at present an exploration of how update models can evolve as result of action executions.\n9We could allow parallel actions. 10 We thank an anonymous reviewer of an earlier version of this paper who suggested a similar example.\nAnalogy with Belief Update: Another approach to explore the differences between mA+ and DEL builds on the analogy to the corresponding differences between belief updates and the treatment of actions and change in early action languages [7].\nPapers on belief updates define and study the problem of updating a formula \u03c6 with a formula \u03c8. In contrast, in reasoning about actions and change, the focus is on defining the resulting state of the world after a particular action is performed in a particular world, given a description of (i) how the action may change the world, (ii) when the action can be executed; and (iii) how the fluents in the world may be (possibly causally) related to each other. In such a context, given a state s and an action a, it is possible to see the the determination of the resulting state as the update of s by a formula \u03d5; But, what is important to consider is that the \u03d5 is not just the collection of effects of the action a, but incorporates several other components, that take into account the static causal laws as well as which conditions (part of the conditional effects of a) are true in s.\nThis situation is not dissimilar to the distinction between DEL update models and mA+. An update model can be encoded by an action formula, and the resulting state can be obtained by updating the starting state with such formula. In DEL, such action formula has to be given directly. Instead, our considerations in mA+ are in the spirit of the early research in reasoning about actions and change\u2014where we focus on describing actions and their effects, their executability conditions, and where a resulting \u201cstate\u201d is determined by applying these descriptions to the \u201cstate\u201d where a particular action is performed. Thus, the action formula in this latter case is not explicitly given, but derived from the description of the actions, their effects, and executability conditions.\nTaking the analogy further, an important application of reasoning about actions is to determine action sequences or plan structures that achieve a given goal. This is different from searching for a formula \u03c8 which, if used to update a given initial state, will generate a goal state; the difference is partly due to the fact that the the space of formulae is infinite. Similarly, the space of update models is infinite, and it is not viable to look for an update model (or a sequence of update models) that will cause a transition from a given initial state to a goal state. Instead, mA+ supports the traditional way of planning by finding action sequences or plan structures that achieve a goal.\nExecuting Actions: The notion of actions adopted inmA+ is designed to enable their executions by one or multiple agents. For example, the action peek(A) can be executed by agent A, by peeking into the box. On the other hand, an action modeled using update models in DEL is not executable, in the normal sense. For example, how does the action expressed in Figure 14 get executed? Who does execute such action? What does executing the various edges of Figure 14 mean? The answers to these questions are not clear.\nFurthermore, let us turn around such questions and focus on the perspective fluents: how does one execute the perspective fluents, such as looking(B)? The answer is that they are fluents, and they are not required or supposed to be executed. A more appropriate question would be: how do they become true? The answer is that, while our formulation could have some actions that make them true, when describing a state we need not worry about how exactly the facts in the state came about to be true. This is not the case when describing actions: when describing actions we need to describe something that can be executed. In summary, actions, or parts of actions, are supposed to be something that can be executed, while states, or parts of states, do not have such requirement.\nHence, our representation of actions is more appropriate, and follows the common meaning of an action,11 than the representation of action in DEL.\nValue of Update Models: Having discussed the differences between mA+ and update models, we would like to point out that update models present a very good technical tool for the understanding of effects of actions in multiagent domains. In fact, the transition function \u03a6 formA+ action theories can be effectively characterized using update models, as described in Section 5."}, {"heading": "7.2.2 Specifying the Initial State", "text": "An important aspect of many algorithms for reasoning about actions and change (including planning) is to have a finite set of possible \u201cinitial states\u201d. Although most of the examples in DEL papers show a finite number of possible initial\n11For example, the relevant dictionary meaning of \u201caction is (1) something done or performed; act; deed. (2) an act that one consciously wills and that may be characterized by physical or mental activity.\nstates (often a single Kripke structure), they do not focus on constraining the knowledge specified about the initial state to guarantee the finiteness of the set of possible initial states. This is an important concern of our paper, and we propose a restricted class of knowledge about the initial states that guarantees finiteness and yet is able to capture most of the examples in the literature. Observe that this condition identifies a class of epistemic planning problems as defined in [37, 38, 39] whose solutions can be computed using heuristic forward search.\nWe note that this problem is related to the finite model property in modal logics\u2014which defines when a theory has (at least) one finite model [40, 41]. The problem addressed in Section 6 could be viewed as the problem of identifying a class of epistemic theories that has (up to equivalence) finitely many finite models and is, thus, a stronger problem than the finite model property problem. To the best of our knowledge, this more complex problem has not been addressed in multi-modal logics before. We believe that this is an important contribution of our development of mA+."}, {"heading": "7.3 Previous Work by the Authors", "text": "Early attempts to adapt action languages to formalize multi-agent domains can be found in [42, 43, 44]. In these works, the action languages A, B, and C have been extended to formalize multi-agent domains.\nThe works in [43, 44] investigate the use of action language in multi-agent planning context and focus on the generation of decentralized plans for multiple agents, to either jointly achieve a goal or individual goals.\nIn [42], we show that several examples found in the literature\u2014created to address certain aspect in multi-agent systems (e.g., [45, 46, 47, 48, 49, 50])\u2014can be formalized using an extension of the action language C. Yet, most of the extensions considered in [42, 43, 44] are inadequate for formalizing multi-agent domains in which reasoning about knowledge of other agents is critical. To address this shortcoming, we have developed and investigated several preliminary versions of mA+ [51, 52, 53]. We started with an attempt to formulate knowledge of multiple agents in [51]; we successively extended this preliminary version of mA+ with the use of static observability specifications in [52]. The language developed in this paper subsumes that of [52]. In [53], we demonstrated the use of update models to describe the transition function for the action language mA+ of [52].\n7.4 mA+ and Action Languages for Single-Agent Domains mA+ is a high-level action language for multi-agent domains. It is therefore instructive to discuss the connection between mA+ and action languages for single-agent domains. First, let us observe that mA+ has the following multi-agent domain specific features:\n\u2022 it includes announcement actions; and\n\u2022 it includes specification of the agents\u2019 observability of action occurrences.\nAs it turns out, if we remove all features that are specific to multi-agent domains from mA+, and consider the S5entailment as its semantics, then the language is equivalent to the language AK from [32]. Formally, let us consider a mA+ definite action theory (I,D) over the signature \u3008AG,F ,A\u3009 such that |AG| = 1 and D does not contain statements of the form (4) (announcement actions) and statements of the form (5)-(6). Let us define\nIAK = {\u03d5 | \u03d5 appears in a statement of the form (12), (13), or (14) in I}.\nThen, the following holds\n(comp(I), D) |=S5 \u03d5 after \u03b4 iff (IAK , D) |=AK \u03d5 after \u03b4.\nThis shows thatmA+ is indeed a generalization of action languages for single-agent domains to multi-agent domains. This also supports the claim that other elements that have been considered in action languages of single-agent domains, such as static causal laws, non-deterministic actions, or parallel actions could potentially be generalized tomA+. This is one of our goals for future work."}, {"heading": "8 Conclusions and Future Works", "text": "In this paper, we developed an action language for representing and reasoning about effects of actions in multi-agent domains. The language considers world-altering actions, sensing actions, and announcement actions. It also allows the dynamic specification of agents\u2019 observability with respect to action occurrences, enabling varying degrees of visibility of action occurrences and action effects. The semantics of the language relies on the notion of states (pointed Kripke structures), used as representations of the states of the world and states of agents\u2019 knowledge and beliefs; the semantics builds on a transition function, which maps pairs of states and actions to sets of states.\nWe discussed several properties of the transition function and identified a class of theories (definite action theories) whose set of initial S5-states is finite, thus allowing for the development of algorithms for the S5-entailment relation that is critical in applications such as planning and temporal reasoning. We also relate the proposed language to the update model based approaches for representing and reasoning about effects of actions in multi-agent domains.\nThe development of mA+ is our first step towards our goal of developing automated reasoning and planning systems in multi-agent domains. This will be our focus in the near future. In addition, we plan to extend the language to deal with lying and/or misleading actions, refine the distinction between knowledge and beliefs of the agents, expand the language to include non-deterministic actions and static causal laws, and specify more general models of agents\u2019 observability, to capture some of the capabilities of update models that are missing from mA+."}, {"heading": "Acknowledgments", "text": "The work has been partially supported by NSF grants HRD-1345232 and DGE-0947465."}, {"heading": "Appendix: Proofs", "text": "Proposition 2 Let D be a consistent domain, (M, s) be a state, a be a world-altering action that is executable in (M, s) and \u03a6wD(a, (M, s)) = {(M \u2032, s\u2032)}. Then, the following holds\n1. For every pair of a world r(a, u) \u2208M \u2032[S] \\M [S] and literal ` \u2208 eD(a,M, u), (M \u2032, r(a, u)) |= `;\n2. For every pair of a world r(a, u) \u2208 M \u2032[S] \\M [S] and literal ` \u2208 F \\ {`, \u00af\u0300| ` \u2208 eD(a,M, u)}, (M,u) |= ` iff (M \u2032, r(a, u)) |= `;\n3. For every i \u2208 OD(a,M, s) and formula \u03d5, (M \u2032, s\u2032) |= Bi\u03d5 iff (M, s) |= Bi\u03d5.\nProof.\n1. We have that r(a, u) \u2208M \u2032[S] \\M [S] implies that r(a, u) \u2208 Res(a,M, s) and thus there exits some u \u2208M [S] such that a is executable in (M,u). Because of the definition ofRes(a,M, s), we have that for ` \u2208 eD(a,M, u), M \u2032[\u03c0](r(a, u))(`) = >, and thus (M \u2032, r(a, u)) |= `.\n2. Similar to the above item, the conclusion for this item follows immediately from the definition of Res(a,M, s)[\u03c0].\n3. Assume that i \u2208 OD(a,M, s) and \u03d5 an arbitrary formula. By the definition of M \u2032, we have that (s\u2032, v) \u2208M \u2032[i] iff v \u2208 M [S] and (s, v) \u2208 M [i]. Furthermore, for each u \u2208 M [S] and j \u2208 AG we have that (v, u) \u2208 M [j] iff (v, u) \u2208M \u2032[j]. This shows that (M \u2032, s\u2032) |= Bi\u03d5 iff (M, s) |= Bi\u03d5.\n2\nLemma 15 Let (M, s) be a state, \u03b1 a group of agents, and f a fluent. Let us assume that for every i \u2208 \u03b1 and (u, v) \u2208 M [i], M [\u03c0](u)(f) = M [\u03c0](v)(f). Furthermore, for each u \u2208 M [S] and i \u2208 \u03b1, {v | (u, v) \u2208 M [i]} 6= \u2205. Then, (M, s) |= C\u03b1f if (M, s) |= f and (M, s) |= C\u03b1\u00acf if (M, s) |= \u00acf .\nProof. Without loss of generality, let us assume that (M, s) |= f . We say that a world u \u2208 M [S] is reachable from s via \u03b1 if there exists a sequence of agents i1, . . . , it and world s = v0, v1, . . . , vt = u such that ij \u2208 \u03b1 and (vj\u22121, vj) \u2208M [ij for j = 1, . . . , t. It is easy to see that (M,u) |= f for every u reachable from s via \u03b1. Let us prove by induction on k that (M,u) |= Ek\u03b1f for every u \u2208 M [S] reachable from s via \u03b1. The base case is trivial, since (M,u) |= f and f = E0\u03b1f . Let us now assume that (M,u) |= Ek\u03b1f for every u reachable from s via \u03b1. We will show that (M,u) |= BiEk\u03b1f for i \u2208 \u03b1 and u is reachable via \u03b1. Consider v such that (u, v) \u2208M [i]. Clearly, v is reachable from s via \u03b1. By inductive hypothesis, we have that (M, v) |= Ek\u03b1f . Since this holds for every v \u2208 M [S] such that (u, v) \u2208 M [i] and there exists at least on such world v, we can conclude that (M,u) |= BiEk\u03b1f . This implies the conclusion for the inductive step and proves the lemma for the case (M, s) |= f .\nThe proof for the case (M, s) |= \u00acf is analogous. 2\nProposition 3 LetD be a consistent domain, a be a sensing action, and (M, s) be a state. Suppose that a is executable in (M, s) and (M, s) is consistency preserving for a. Assume that (M \u2032, s\u2032) \u2208 \u03a6sD(a, (M, s)). Then, the following holds\n1. for every f \u2208 SensedD(a), if (M, s) |= ` then (M \u2032, s\u2032) |= CFD(a,M,s)`, where ` \u2208 {f,\u00acf};\n2. for every f \u2208 SensedD(a), (M \u2032, s\u2032) |= CPD(a,M,s)(CFD(a,M,s)f \u2228CFD(a,M,s)\u00acf); and\n3. for every i \u2208 OD(a,M, s) and formula \u03d5, (M \u2032, s\u2032) |= Bi\u03d5 iff (M, s) |= Bi\u03d5.\nProof. Recall that (M \u2032, s\u2032) = (M, s) ]c \u22121\nFD(a,M,s)\u222aPD(a,M,s) (M \u2032\u2032, s\u2032\u2032)\nwhere\n\u2022 (M \u2032\u2032, s\u2032\u2032) = (Mr |FD(a,M,s)\u222aPD(a,M,s) a Rema(Mr), c(s)) and\n\u2022 (Mr, c(s)) is a c-replica of (M, s).\nWe observe that for every i \u2208 FD(a,M, s) \u222a PD(a,M, s),\n(u, v) \u2208M \u2032[i] iff (u, v) \u2208M \u2032\u2032[i] (17)\nand for each u \u2208M \u2032\u2032[i], it holds that {v | (u, v) \u2208M \u2032\u2032[i]} 6= \u2205 because M is consistency preserving for a.\n1. Consider the structure (M \u2032\u2032, s\u2032\u2032) and some f \u2208 SensedD(a). It holds thatM \u2032\u2032[\u03c0](u)(f) = M \u2032\u2032[\u03c0](v)(f) for i \u2208 FD(a,M, s) and (u, v) \u2208M \u2032\u2032[i]. Thus, (M \u2032\u2032, s\u2032\u2032) |= CFD(a,M,s)f or (M \u2032\u2032, s\u2032\u2032) |= CFD(a,M,s)\u00acf (Lemma 15). Because of (17), we have that (M \u2032, s\u2032) |= CFD(a,M,s)` iff (M \u2032\u2032, s\u2032\u2032) |= CFD(a,M,s)` for ` \u2208 {f,\u00acf}. This proves the first item.\n2. Observe that for a sequence of links (s\u2032, j1, s1), . . . , (sk\u22121, jk, sk) in M \u2032 such that jl \u2208 PD(a,M, s) for l = 1, . . . , k, we have that si \u2208M \u2032\u2032[S]. This, together with the fact that (M \u2032\u2032, u) |= CFD(a,M,s)f \u2228CFD(a,M,s)\u00acf for every u \u2208M \u2032\u2032[S] and (17), implies the second item.\n3. The third item is similar to the third item of Proposition 2.\n2\nProposition 5 Given an action a and a state (M, s), we have that for each (M \u2032, s\u2032) \u2208 (M, s) \u2297 \u2126(a, (M, s)) there exists an equivalent element in \u03a6D(a, (M, s)) and vice versa.\nProof. If a is not executable in (M, s) then (M, s)\u2297 \u2126(a, (M, s)) = \u03a6D(a, (M, s)) = \u2205 so the proposition is trivial for this case.\nLet us assume that a is executable in (M, s) and the precondition of a is \u03c8. Furthermore, let \u03c1 = (FD(a,M, s), PD(a,M, s), OD(a,M, s)). We consider three cases:\n1. a is a world-altering action. Following Definitions 15 and 25, we have that \u03a6D(a, (M, s)) = {(M \u2032, s\u2032)} and \u2126(a, (M, s)) = (\u03c9(a, \u03c1), {\u03c3}) with \u03c9(a, \u03c1) as in Definition 22. By the definition of \u2297 operator between a state and an update instance we have that (M, s) \u2297 \u2126(a, (M, s)) is a singleton. Thus, let us denote the unique element in (M, s)\u2297 \u2126(a, (M, s)) with (W,w) where each world in W [S] is of the form (u, \u03c3) or (u, ) for some u \u2208M [S] and w = (s, \u03c3). We will show that (W,w) is equivalent to (M \u2032, s\u2032). We define the function h : W [S]\u2192M \u2032[S] as follows:\nh((u, \u03b3)) = { r(a, u) if \u03b3 = \u03c3 u if \u03b3 =\nObserve that (u, \u03c3) \u2208 W [S] iff (M,u) |= pre(\u03c3) = \u03c8 iff a is executable in u iff r(a, u) \u2208 Res(a,M, s)[S] iff r(a, u) \u2208M \u2032[S]. Furthermore, (u, ) \u2208 W [S] iff u \u2208 M [S] iff u \u2208 M \u2032[S] \\ Res(a,M, s)[S]. This allows us to conclude that h is a bijective function f from W [S] to M \u2032[S]. (*)\nWe will show next that W [\u03c0]((u, \u03b3)) \u2261M \u2032[\u03c0](h((u, \u03b3))). (**) Consider \u03b3 = . For each u \u2208 M [S], we have that W [\u03c0]((u, )) \u2261 M [\u03c0](u) \u2261 M \u2032[\u03c0](u) = M \u2032[\u03c0](f((u, ))). We now show that W [\u03c0]((u, \u03c3)) \u2261M \u2032[\u03c0](r(a, u)). Consider p \u2208 F . We have that W [\u03c0]((u, \u03c3))(p) = > iff p\u2192 \u03a8+(p, a) \u2228 (p \u2227 \u00ac\u03a8\u2212(p, a)) \u2208 sub(\u03c3) and (M,u) |= \u03a8+(p, a) \u2228 (p \u2227 \u00ac\u03a8\u2212(p, a)) iff either (a) there exists a statement \u201ca causes p if \u03d5\u201d in D such that (M,u) |= \u03d5; or (b) (M,u) |= p and (M,u) |= \u00ac\u03d5 for every statement \u201ca causes \u00acp if \u03d5\u201d in D iff either (a) p \u2208 eD(a,M, u) or (b) p 6\u2208 eD(a,M, u) and \u00acp 6\u2208 eD(a,M, u) iff M \u2032[\u03c0](r(a, u))(p) = >. This implies that (**) holds.\nGiven (*) and (**), it is easy to see that the equivalence between (W,w) and (M \u2032, s\u2032) will be proved if we have that ((u, \u03b31), (v, \u03b32)) \u2208W [i] iff (h(u, \u03b31), h(v, \u03b32)) \u2208M \u2032[i]. Consider two cases:\n\u2022 i \u2208 FD(a,M, s). There are two cases: \u2013 ((u, \u03c3), (v, \u03c3)) \u2208 W [i] iff (u, v) \u2208 M [i] and (M,u) |= pre(\u03c3) and (M,v) |= pre(\u03c3) iff (u, v) \u2208 M [i] and r(a, u), r(a, v) \u2208 Res(a,M, s)[S] iff (r(a, u), r(a, v)) \u2208M \u2032[i];\n\u2013 ((u, ), (v, )) \u2208 W [i] iff (u, v) \u2208 M [i] iff (u, v) \u2208 M [i] and u, v \u2208 M \u2032[S] \\ Res(a,M, s)[S] iff (u, v) \u2208M \u2032[i].\n\u2022 i \u2208 OD(a,M, s). There are two cases: \u2013 ((u, \u03c3), (v, )) \u2208 W [i] iff (u, v) \u2208 M [i] and (M,u) |= pre(\u03c3) iff (u, v) \u2208 M [i] and r(a, u) \u2208 Res(a,M, s)[S] and v \u2208M \u2032[S] \\Res(a,M, s)[S] iff (r(a, u), v) \u2208M \u2032[i];\n\u2013 ((u, ), (v, )) \u2208 W [i] iff (u, v) \u2208 M [i] iff (u, v) \u2208 M [i] and u, v \u2208 M \u2032[S] \\ Res(a,M, s)[S] iff (u, v) \u2208M \u2032[i].\nThe above two cases and (*) and (**) show that (M \u2032, s\u2032) is equivalent to (W,w), i.e., the proposition is proved for this case.\n2. a is a sensing action. Without the loss of generality, assume that a senses a single fluent f and the executability condition of a is \u03c8. By Definition 23, (\u03c9(a, \u03c1), {\u03c3, \u03c4}) is the update instance for a and \u03c1 = (FD(a,M, s), PD(a,M, s), OD(a,M, s)). Furthermore, for each u \u2208 M [S], the set of worlds of (M, s) \u2297 \u2126(a, (M, s)) contains either (u, \u03c3) or (u, \u03c4) but not both. Thus, (M, s) \u2297 \u2126(a, (M, s)) is again a singleton. Let us denote the unique element of (M, s)\u2297\u2126(a, (M, s)) by (W,w). Let (Mr, c(s)) be the replica of (M, s) that is used in the construction of (M \u2032, s\u2032) \u2208 \u03a6D(a,M, s). Let h be the function\nh : W [S]\u2192M \u2032[S]\nwith\nh((u, \u03bb)) = { c(u) if (u, \u03bb) \u2208W [S], \u03bb \u2208 {\u03c3, \u03c4} u if (u, \u03bb) \u2208W [S], \u03bb =\nThe rest of the proof follows exactly the steps of the proof for the case of a world-altering action.\n3. a is an announcement action. The proof is analogous to the case of a sensing action.\n2\nThe remaining part of this appendix is devoted to prove Propositions 6-15. The proofs will be organized in a collection of lemmas. Let us start with a lemma that allows us to ignore in a state those worlds that are not reachable from the real state of the word.\nLemma 7 Every S5-state (M, s) is equivalent to a S5-state (M \u2032, s) such that, for every state u \u2208 M \u2032[S], we have that u is reachable from s.\nProof. The result derives from the fact that, if there is a world u which is not reachable from s, then for each formula \u03c8 we have that (M, s) |= \u03c8 iff (M \u2032, s) |= \u03c8, where M \u2032 is defined as follows: (i) M \u2032[S] = M [S] \\ {u} (i.e., we remove the unreachable world u); (ii) M \u2032[\u03c0](v) \u2261 M [\u03c0](v) for every v \u2208 M \u2032[S] (i.e., all interpretations associated to the worlds remain the same); and (iii) M \u2032[i] = M [i] \\ {(p, q) | (p, q) \u2208 M [i], p 6\u2208 M \u2032[S] or q 6\u2208 M \u2032[S]}, (i.e., we maintain the same belief relations except for removing all cases related to the world u). 2 The next lemma characterizes the accessibility relations for states satisfying a common knowledge formula.\nLemma 8 Let (M, s) be a S5-state such that every state u \u2208 M [S] is reachable from s. Let \u03c8 be a formula. Then, (M, s) |= C(\u03c8) iff M [\u03c0](u) |= \u03c8 for every world u \u2208M [S].\nProof. Because of the reflexive, transitive, and symmetric properties of the relations Bi in M and the definition of the satisfaction of C(\u03c8), we have that (M, s) |= C(\u03c8) implies that (M,u) |= \u03c8 for each u \u2208 M [S] (since each u is reachable from s), which implies M [\u03c0](u) |= \u03c8. The converse is obvious from the fact that each state is reachable from s. 2\nLemma 8, in particular, indicates that for each i \u2208 AG and for each fluent formula we have: \u03c8 (M, s) |= C(Bi\u03c8) iff \u2200u \u2208M [S]. M [\u03c0](u) |= \u03c8.\nThe following lemmas allow us to characterize the topological properties of the Kripke structures implied by the satisfaction of the different types of statements in a definite action theory.\nLemma 9 Let (M, s) be a S5-state such that every world u \u2208 M [S] is reachable from s. Let \u03c8 be a fluent formula. Then:\n(M, s) |= C(Bi\u03c8 \u2228Bi\u00ac\u03c8) iff ( \u2200u, v \u2208M [S] : (u, v) \u2208M [i]\u21d2 ( M [\u03c0](u) |= \u03c8 iff M [\u03c0](v) |= \u03c8 )) Proof. Thanks to the Lemma 8, we have that (M, s) |= C(Bi\u03c8\u2228Bi\u00ac\u03c8) holds if and only if (M,u) |= Bi\u03c8\u2228Bi\u00ac\u03c8 for every u \u2208M [S]. The proof of the lemma then follows immediately from the fact that M is a S5-structure. 2\nA consequence of Lemma 9 is the following lemma, that provides a characterization of S5-states with respect to formulae of the form C(\u00acBi\u03c8 \u2227 \u00acBi\u00ac\u03c8). For simplicity of presentation, for a structure M and u, v \u2208 M [S] we write M [\u03c0](u)(\u03c8) 6= M [\u03c0](v)(\u03c8) to denote either (M [\u03c0](u) |= \u03c8 and M [\u03c0](v) 6|= \u03c8) or (M [\u03c0](u) 6|= \u03c8 and M [\u03c0](v) |= \u03c8), i.e., the value of \u03c8 at u is different from the value of \u03c8 at v.\nLemma 10 Let (M, s) be a S5-state such that every u \u2208 M [S] is reachable from s. Let \u03c8 be a fluent formula. Then, (M, s) |= C(\u00acBi\u03c8 \u2227 \u00acBi\u00ac\u03c8) iff for every u \u2208 M [S] there exists some v \u2208 M [S] such that (u, v) \u2208 M [i], and M [\u03c0](u)(\u03c8) 6= M [\u03c0](v)(\u03c8).\nLet us prove a few properties of initial S5-states of a definite action theory.\nLemma 11 Let (M, s) be an initial S5-state of a definite action theory, such that every u \u2208 M [S] is reachable from s. Let \u03d5 be a fluent formula and i \u2208 AG. Then,\n\u2022 If (M,u) |= Bi\u03d5 for some u \u2208M [S] then (M, s) |= C(Bi\u03d5) or (M, s) |= C(Bi\u03d5 \u2228Bi\u00ac\u03d5);\n\u2022 If (M,u) |= \u00acBi\u03d5 for some u \u2208M [S] then (M, s) |= C(\u00acBi\u03d5 \u2227Bi\u00ac\u03d5).\nProof. This result can be proved by contradiction. Let us consider the first item. If the consequence of the first item does not hold then (M, s) |= C(\u00acBi\u03d5 \u2227 Bi\u00ac\u03d5)\u2014since (M, s) is an initial state of a definite action theory. Thus, (M,u) 6|= Bi\u03d5 due to Lemma 10 (since there are two states in which the formula \u03d5 has a different truth value). This leads to a contradiction.\nLet us consider the second item. If the consequence of the second item does not hold then we have that (M, s) |= C(Bi\u03d5) or (M, s) |= C(Bi\u03d5 \u2228 Bi\u00ac\u03d5) (since we are dealing with definite action theories). This implies (M,u) |= Bi\u03d5 or (M,u) |= Bi\u00ac\u03d5 (Lemma 8 and Lemma 9). This leads to a contradiction. 2\nIn the following, for u \u2208M [S], let\nstate(u) = \u2227\nf\u2208F, M [\u03c0](u)(f)=>\nf \u2227 \u2227\nf\u2208F, M [\u03c0](u)(f)=\u22a5\n\u00acf\nLemma 12 Let (M, s) be an initial S5-state of a definite action theory, such that every u \u2208 M [S] is reachable from s. Let u, v \u2208 M [S] such that u \u223c v. Then, for every i \u2208 AG and x \u2208 M [S] such that (u, x) \u2208 M [i] there exists y \u2208M [S] such that (v, y) \u2208M [i] and x \u223c y.\nProof. Let K(p, i) = {q | q \u2208 M [S], (p, q) \u2208 M [i]}\u2014i.e., the set of worlds immediately related to p via M [i]. We consider two cases:\n\u2022 Case 1: K(u, i) \u2229 K(v, i) 6= \u2205. Because of the transitivity of M [i], we can conclude that K(u, i) = K(v, i) and the lemma is trivial (by taking x = y).\n\u2022 Case 2: K(u, i) \u2229 K(v, i) = \u2205. Let us assume that there exists some x \u2208 K(u, i) such that there exists no y \u2208 K(v, i) with x \u223c y. This means that (M,y) |= \u00acstate(x) for each y \u2208 K(v, i). In other words, (M,v) |= Bi\u00acstate(x). This implies that (by Lemma 11):\n(M, s) |= C(Bi\u00acstate(x)) or (M, s) |= C(Bi\u00acstate(x) \u2228Bistate(x)) (18)\nOn the other hand, (M,u) 6|= Bi\u00acstate(x), since x \u2208 K(u, i) and (M,x) |= state(x). This implies (M, s) |= C(\u00acBi\u00acstate(x) \u2227 \u00acBistate(x)) by Lemma 11. This contradicts (18). This means that our assumption is incorrect, i.e., the lemma is proved.\n2\nLemma 13 Let (M, s) be a S5-state such that every u \u2208M [S] is reachable from s. Then, the reduced state (M\u0303, s\u0303) of (M, s) is also a finite S5-state.\nProof. Let us start by observing that, for each u, v \u2208 M [S] such that u \u223c v, we have: u\u0303 = v\u0303. Since the number of possible interpretations of F is finite (being F itself finite), and since there can be no two distinct worlds u\u0303 and v\u0303 such that M [\u03c0](u) = M [\u03c0](v), then we can conclude that the number of worlds in M\u0303 [S] is finite. (1) Let us consider an agent i \u2208 AG and worlds u\u0303, v\u0303, w\u0303 \u2208 M\u0303 [S].\n\u2022 Clearly, (u\u0303, u\u0303) \u2208 M\u0303 [i] since (u, u) \u2208M [i] for every u \u2208M [S]. This implies that M\u0303 [i] is reflexive. (2)\n\u2022 By construction of M\u0303 [i], if (u\u0303, v\u0303) \u2208 M\u0303 [i] then there exists (u\u2032, v\u2032) \u2208M [i] for some u\u2032 \u2208 u\u0303 and v\u2032 \u2208 v\u0303. Because M is a S5-structure, and thus M [i] is symmetric, we have that (v\u2032, u\u2032) \u2208M [i]. This implies that (v\u0303, u\u0303) \u2208 M\u0303 [i], i.e., M\u0303 [i] is symmetric (3)\n\u2022 Now assume that (u\u0303, v\u0303) \u2208 M\u0303 [i] and (v\u0303, w\u0303) \u2208 M\u0303 [i]. The former implies that there exists (u\u2032, v\u2032) \u2208 M [i] for some u\u2032 \u2208 u\u0303 and v\u2032 \u2208 v\u0303. The latter implies that there exists (x\u2032, w\u2032) \u2208 M [i] for some x\u2032 \u2208 v\u0303 and w\u2032 \u2208 w\u0303. Thanks to the construction of M\u0303 [S], we have that v\u2032 \u223c x\u2032\u2014since they belong to the same equivalence class for \u223c. Lemma 12 implies that there exists some w\u2032\u2032 \u223c w\u2032 such that (v\u2032, w\u2032\u2032) \u2208 M [i] which implies that, by transitivity of M [i], (u\u2032, w\u2032\u2032) \u2208M [i]. Thus, (u\u0303, w\u0303) \u2208 M\u0303 [i], i.e., M\u0303 [i] is transitive. (4)\n(1)-(4) prove the conclusion of the lemma. 2 The next lemma shows that a reduced state of an initial S5-state of a definite action theory is also an initial state of\nthe action theory.\nLemma 14 Let (M, s) be a S5-state such that every state u in M [S] is reachable from s. Furthermore, let (M\u0303, s\u0303) be the reduced state of (M, s). It holds that\n1. (M, s) |= \u03c8 iff (M\u0303, s\u0303) |= \u03c8;\n2. (M, s) |= C(\u03c8) iff (M\u0303, s\u0303) |= C(\u03c8);\n3. (M, s) |= C(Bi\u03c8) iff (M\u0303, s\u0303) |= C(Bi\u03c8);\n4. (M, s) |= C(Bi\u03c8 \u2228Bi\u00ac\u03c8) iff (M\u0303, s\u0303) |= C(Bi\u03c8 \u2228Bi\u00ac\u03c8));\n5. (M, s) |= C(\u00acBi\u03c8 \u2227 \u00acBi\u00ac\u03c8) iff (M\u0303, s\u0303) |= C(\u00acBi\u03c8 \u2227 \u00acBi\u00ac\u03c8));\nwhere i \u2208 AG and \u03c8 is a fluent formula.\nProof.\n1. (M, s) |= \u03c8 iff M [\u03c0](s) |= \u03c8 (by definition) iff M\u0303 [\u03c0](s\u0303) |= \u03c8 (by construction of (M\u0303, s\u0303)) iff (M\u0303, s\u0303) |= \u03c8.\n2. (M, s) |= C(\u03c8) iff (M,u) |= \u03c8 for each u \u2208M [S] (by Lemma 8 with respect to (M, s)) iff (M\u0303, u\u0303) |= \u03c8 for each u\u0303 \u2208 M\u0303 [S] (by construction of (M\u0303, s\u0303)) iff (M\u0303, s\u0303) |= C(Bi\u03c8) (by Lemma 8 with respect to (M\u0303, s\u0303)).\n3. (M, s) |= C(Bi\u03c8) iff (M,u) |= \u03c8 for each u \u2208M [S] (by Lemma 8 with respect to (M, s)) iff (M\u0303, u\u0303) |= \u03c8 for each u\u0303 \u2208 M\u0303 [S] (by construction of (M\u0303, s\u0303)) iff (M\u0303, s\u0303) |= C(Bi\u03c8) (by Lemma 8 with respect to (M\u0303, s\u0303)).\n4. (M, s) |= C(Bi\u03c8 \u2228Bi\u00ac\u03c8) iff for every pair of u and v in M [S], (u, v) \u2208 M [i] implies M [\u03c0](u) |= \u03c8 iff M [\u03c0](v) |= \u03c8 (by Lemma 9 with respect to (M, s)) iff for every pair of states p\u0303 and q\u0303 in M\u0303 [S], u \u2208 p\u0303 and v \u2208 q\u0303, (p\u0303, q\u0303) \u2208 M\u0303 [i] implies M\u0303 [\u03c0](p\u0303) |= \u03c8 iff M\u0303 [\u03c0](q\u0303) |= \u03c8 (by construction of (M\u0303, s\u0303)) iff (M\u0303, s\u0303) |= C(Bi\u03c8 \u2228Bi\u00ac\u03c8) (by Lemma 9 with respect to (M\u0303, s\u0303)).\n5. (M, s) |= C(\u00acBi\u03c8 \u2227 \u00acBi\u00ac\u03c8) iff for every u \u2208 M [S] there exists some v \u2208M [S] such that (u, v) \u2208 M [i] and M [\u03c0](u) |= \u03c8 and M [\u03c0](v) 6|= \u03c8 (by Lemma 10 with respect to (M, s)) iff for every u\u0303 \u2208 M\u0303 [S] there exists v\u0303 \u2208 M\u0303 [S] such that (u\u0303, v\u0303) \u2208 M\u0303 [i], M\u0303 [\u03c0](u\u0303) |= \u03c8, and M\u0303 [\u03c0](v\u0303) 6|= \u03c8 (by construction of (M\u0303, s\u0303)) iff (M\u0303, s\u0303) |= C(\u00acBi\u03c8 \u2227 \u00acBi\u00ac\u03c8)) (by Lemma 10 with respect to (M\u0303, s\u0303)).\n2\nLemma 16 Let I be a set of initial statements of a definite action theory. Every S5-state satisfying I is equivalent to a S5 state (M \u2032, s\u2032) such that |M \u2032[S]| \u2264 2|F|.\nProof. By Lemma 7, we can assume that every world in M is reachable from s. Lemma 14 shows that (M\u0303, s\u0303) is also a S5-state satisfying I . Obviously, |M\u0303 [S]| \u2264 2|F|\u2014since each interpretation M [\u03c0](v) can be modeled as a subset of F , and |M\u0303 [S]| is bounded by the number of distinct interpretations. We will show that the reduced state (M\u0303, s\u0303) of (M, s) satisfies the conclusions of the lemma. To complete the proof, we need to show that for an arbitrary formula \u03d5 and u \u2208M [S], the following holds:\n(M,u) |= \u03d5 iff (M\u0303, u\u0303) |= \u03d5. (19)\nWe will prove (19) by induction over the depth of \u03d5.\n\u2022 Base: depth(\u03d5) = 0 implies that \u03d5 is a fluent formula. (19) is trivial from the construction of (M\u0303, s\u0303).\n\u2022 Step: Assume that (19) holds for \u03d5 with depth(\u03d5) \u2264 k. Consider the case depth(\u03d5) = k + 1. There are four cases:\n\u2013 \u03d5 = Bi\u03c8. (M,u) |= \u03d5 iff for every v \u2208M [S] s.t. (u, v) \u2208M [i] we have that (M, v) |= \u03c8 iff for every v\u0303 \u2208 M\u0303 [S], (u\u0303, v\u0303) \u2208 M\u0303 [i], (M\u0303, v\u0303) |= \u03c8 (inductive hypothesis and construction of (M\u0303, s\u0303)) iff (M\u0303, u\u0303) |= Bi\u03c8.\n\u2013 The proof for other cases, when \u03d5 is either \u00ac\u03c8, \u03c6 \u2228 \u03c8, or \u03c6 \u2227 \u03c8 is similar.\nSince the truth value of group formulae of the form E\u03b1\u03d5 and C\u03b1\u03d5 is defined over the truth value of Bi\u03d5 for i \u2208 \u03b1, we can easily show that (19) holds for an arbitrary formula in the language LAG . This proves the lemma. 2\nThe above lemma leads to the following proposition.\nProposition 6 For each consistent definite action theory (I,D), there exists a finite number of initial S5-states (M1, s1), . . . , (Mk, sk) such that every initial S5-state (M, s) of (I,D) is equivalent to some (Mi, si). Furthermore, for each pair of i 6= j and u \u2208Mi[S] there exists some v \u2208Mj [S] such that Mi[\u03c0](u) \u2261Mj [\u03c0](v).\nProof. Observe that the existence of a S5-state satisfying I is proved in [28]. By Lemma (16), we know that each initial S5-state (M, s) of (I,D) is equivalent to its reduced state (M\u0303, s\u0303). The proof of the first conclusion of the proposition follows trivially from the fact that |M\u0303 [S]| \u2264 2|F| and there are only finitely many Kripke structures whose size is at most 2|F|.\nTo prove the second conclusion of the proposition, let us recall the following definition given earlier: for s \u2208M [S], let state(s) = \u2227\nf\u2208F, M [\u03c0](s)(f)=>\nf \u2227 \u2227\nf\u2208F, M [\u03c0](s)(f)=\u22a5\n\u00acf\nConsider two initial S5-states (Mi, si) and (Mj , sj). Let us assume, by contradiction, that there is some u \u2208 Mi[S] such that for every v \u2208 Mj [S], Mi[\u03c0](u) 6\u2261 Mj [\u03c0](v). Let \u03d5 = \u00acstate(u). Because of our assumption, we have that \u03d5 is false in every state of Mj ; thanks to Lemma 8, we can infer that (Mj , sj) |= C(Bi\u03d5). Since (D, I) is a definite action theory and (Mj , sj) is an initial S5-state of (D, I), we have that \u201cinitially C(Bi\u03d5)\u201d is in I; on the other hand, (Mj , u) |= \u00ac\u03d5, which implies that (Mj , sj) is not an initial S5-state of (I,D). The conclusion is proved by contradiction. 2\nProposition 6 and the definition of complete definite action theories give raise to the following proposition.\nProposition 7 For a consistent and complete action theory (I,D), there exists a unique initial S5-state (M0, s0) with |M0[S]| \u2264 2|F| such that every initial S5-state (M, s) of (I,D) is equivalent to some (M0, s0).\nProof. Let us consider two arbitrary S5-states (M1, s1) and (M2, s2) satisfying I . From Lemma 16, we can assume that both M1 and M2 are in reduced form; this guarantees that the number of worlds in M1 and M2 is bounded by the number of possible interpretations, which is 2|F|. Furthermore, from proposition 6 we can also assume that M1 and M2 have the same worlds (since each interpretation in M1 appears in M2 and vice versa, and no interpretation can be associated to two distinct worlds, since the states are in reduced form).\nFor the sake of simplicity, let us assume that M1[S] = M2[S]; let us assume, by contradiction, that there are u, v \u2208 M1[S] such that (u, v) \u2208 M1[i] and (u, v) 6\u2208 M2[i]. Let \u03d5 = \u00acstate(v). Because of the construction, we can see that (M2, u) |= Bi\u03d5. Since the two states are initial states for I , then this means that they are models of one of the following formulate: C(Bi\u03d5) or C(Bi\u03d5 \u2228Bistate(v)). On the other hand, since (u, v) \u2208M1[i], it is easy to see that (M1, u) 6|= Bi\u03d5 and (M1, u) 6|= Bistate(v). Thus, this contradicts the fact that both (M1, s1) and (M2, s2) are models of I (a complete definite theory). 2"}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>In multi-agent domains, an agent\u2019s action may not just change the world and the agent\u2019s knowledge and beliefs<lb>about the world, but also may change other agents\u2019 knowledge and beliefs about the world and their knowledge and<lb>beliefs about other agents\u2019 knowledge and beliefs about the world. Similarly, the goals of an agent in a multi-agent<lb>world may involve manipulating the knowledge and beliefs of other agents\u2019 and again, not just their knowledge<lb>about the world, but also their knowledge about other agents\u2019 knowledge about the world. The goal of this paper is to<lb>present an action language, called mA+, that has the necessary features to address the above aspects in representing<lb>and reasoning about actions and change in multi-agent domains.<lb>This action language can be viewed as a generalization of the single-agent action languages extensively studied<lb>in the literature, to the case of multi-agent domains. The language allows the representation of and reasoning about<lb>different types of actions that an agent can perform in a domain where many other agents might be present\u2014such as<lb>world-altering actions, sensing actions, and announcement/communication actions. The action language also allows<lb>the specification of agents\u2019 dynamic awareness of action occurrences which has future implications on what agents\u2019<lb>know about the world and other agents\u2019 knowledge about the world. The language mA+ considers three different<lb>types of awareness: full awareness, partial awareness, and complete oblivion of an action occurrence and its effects.<lb>This keeps the language simple, yet powerful enough to address a large variety of knowledge manipulation scenarios<lb>in multi-agent domains.<lb>The semantics of the language relies on the notion of state, which is described by a pointed Kripke model and<lb>is used to encode the agent\u2019s knowledge and the real state of the world. The semantics is defined by a transition<lb>function that maps pairs of actions and states into sets of states. The paper illustrates properties of the action theories,<lb>including properties that guarantee finiteness of the set of initial states and their practical implementability. Finally,<lb>the paper relates mA+ to other related formalisms that contribute to reasoning about actions in multi-agent domains.", "creator": "LaTeX with hyperref package"}}}