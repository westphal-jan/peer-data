{"id": "1511.02619", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2015", "title": "Decomposition Bounds for Marginal MAP", "abstract": "Marginal MAP inference involves making MAP predictions in systems defined with latent variables or missing information. It is significantly more difficult than pure marginalization and MAP tasks, for which a large class of efficient and convergent variational algorithms, such as dual decomposition, exist. In this work, we generalize dual decomposition to a generic power sum inference task, which includes marginal MAP, along with pure marginalization and MAP, as special cases. Our method is based on a block coordinate descent algorithm on a new convex decomposition bound, that is guaranteed to converge monotonically, and can be parallelized efficiently. We demonstrate our approach on marginal MAP queries defined on real-world problems from the UAI approximate inference challenge, showing that our framework is faster and more reliable than previous methods. We suggest that an approximate model based on these problems can be generalized in terms of the total degree of convergence that we describe with differential models. In this work, we identify a single (local) local component, a local component, a generalized derivative map, and a generalized derivative map that is more general and highly optimized than previous methods. Using this approach, we create an approximate model with a global representation of the sum of the sum of the local component, which is an approximate representation of all (local) local components, and a generalized derivative map that is more general and highly optimized than previous methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 9 Nov 2015 10:21:39 GMT  (1610kb,D)", "http://arxiv.org/abs/1511.02619v1", "NIPS 2015 (full-length)"]], "COMMENTS": "NIPS 2015 (full-length)", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.IT math.IT stat.ML", "authors": ["wei ping", "qiang liu", "alexander t ihler"], "accepted": true, "id": "1511.02619"}, "pdf": {"name": "1511.02619.pdf", "metadata": {"source": "CRF", "title": "Decomposition Bounds for Marginal MAP", "authors": ["Wei Ping", "Qiang Liu", "Alexander Ihler"], "emails": ["wping@ics.uci.edu", "ihler@ics.uci.edu", "qliu@cs.dartmouth.edu"], "sections": [{"heading": null, "text": "Marginal MAP inference involves making MAP predictions in systems defined with latent variables or missing information. It is significantly more difficult than pure marginalization and MAP tasks, for which a large class of efficient and convergent variational algorithms, such as dual decomposition, exist. In this work, we generalize dual decomposition to a generic power sum inference task, which includes marginal MAP, along with pure marginalization and MAP, as special cases. Our method is based on a block coordinate descent algorithm on a new convex decomposition bound, that is guaranteed to converge monotonically, and can be parallelized efficiently. We demonstrate our approach on marginal MAP queries defined on real-world problems from the UAI approximate inference challenge, showing that our framework is faster and more reliable than previous methods."}, {"heading": "1 Introduction", "text": "Probabilistic graphical models such as Bayesian networks and Markov random fields provide a useful framework and powerful tools for machine learning. Given a graphical model, inference refers to answering probabilistic queries about the model. There are three common types of inference tasks. The first are max-inference or maximum a posteriori (MAP) tasks, which aim to find the most probable state of the joint probability; exact and approximate MAP inference is widely used in structured prediction [26]. Sum-inference tasks include calculating marginal probabilities and the normalization constant of the distribution, and play a central role in many learning tasks (e.g., maximum likelihood). Finally, marginal MAP tasks are \u201cmixed\u201d inference problems, which generalize the first two types by marginalizing a subset of variables (e.g., hidden variables) before optimizing over the remainder.1 These tasks arise in latent variable models [e.g., 29, 25] and many decision-making problems [e.g., 13]. All three inference types are generally intractable; as a result, approximate inference, particularly convex relaxations or upper bounding methods, are of great interest.\nDecomposition methods provide a useful and computationally efficient class of bounds on inference problems. For example, dual decomposition methods for MAP [e.g., 31] give a class of easy-toevaluate upper bounds which can be directly optimized using coordinate descent [37, 6], subgradient updates [14], or other methods [e.g., 22]. It is easy to ensure both convergence, and that the objective is monotonically decreasing (so that more computation always provides a better bound). The resulting bounds can be used either as stand-alone approximation methods [6, 14], or as a component of search [11]. In summation problems, a notable decomposition bound is tree-reweighted BP (TRW), which bounds the partition function with a combination of trees [e.g., 34, 21, 12, 3]. These bounds are useful in joint inference and learning (or \u201cinferning\u201d) frameworks, allowing learning with approximate inference to be framed as a joint optimization over the model parameters and decomposition bound, often leading to more efficient learning [e.g., 23]. However, far fewer methods have been developed for marginal MAP problems.\n1In some literature [e.g., 28], marginal MAP is simply called MAP, and the joint MAP task is called MPE.\nar X\niv :1\n51 1.\n02 61\n9v 1\n[ cs\n.L G\n] 9\nN ov\n2 01\nIn this work, we deveop a decomposition bound that has a number of desirable properties: (1) Generality: our bound is sufficiently general to be applied easily to marginal MAP. (2) Any-time: it yields a bound at any point during the optimization (not just at convergence), so it can be used in an anytime way. (3) Monotonic and convergent: more computational effort gives strictly tighter bounds; note that (2) and (3) are particularly important for high-width approximations, which are expensive to represent and update. (4) Allows optimization over all parameters, including the \u201cweights\u201d, or fractional counting numbers, of the approximation; these parameters often have a significant effect on the tightness of the resulting bound. (5) Compact representation: within a given class of bounds, using fewer parameters to express the bound reduces memory and typically speeds up optimization.\nWe organize the rest of the paper as follows. Section 2 gives some background and notation, followed by connections to related work in Section 3. We derive our decomposed bound in Section 4, and present a (block) coordinate descent algorithm for monotonically tightening it in Section 5. We report experimental results in Section 6 and conclude the paper in Section 7."}, {"heading": "2 Background", "text": "Here, we review some background on graphical models and inference tasks. A Markov random field (MRF) on discrete random variables x = [x1, . . . , xn] \u2208 Xn is a probability distribution,\np(x; \u03b8) = exp [ \u2211 \u03b1\u2208F \u03b8\u03b1(x\u03b1)\u2212 \u03a6(\u03b8) ] ; \u03a6(\u03b8) = log \u2211 x\u2208Xn exp [ \u2211 \u03b1\u2208F \u03b8\u03b1(x\u03b1) ] , (1)\nwhere F is a set of subsets of the variables, each associated with a factor \u03b8\u03b1, and \u03a6(\u03b8) is the log partition function. We associate an undirected graph G = (V,E) with p(x) by mapping each xi to a node i \u2208 V , and adding an edge ij \u2208 E iff there exists \u03b1 \u2208 F such that {i, j} \u2286 \u03b1. We say node i and j are neighbors if ij \u2208 E. Then, F is the subset of cliques (fully connected subgraphs) of G. The use and evaluation of a given MRF often involves different types of inference tasks. Marginalization, or sum-inference tasks perform a sum over the configurations to calculate the log partition function \u03a6 in (1), marginal probabilities, or the probability of some observed evidence. On the other hand, the maximum a posteriori (MAP), or max-inference tasks perform joint maximization to find configurations with the highest probability, that is, \u03a60(\u03b8) = maxx \u2211 \u03b1\u2208F \u03b8\u03b1(x\u03b1).\nA generalization of max- and sum- inference is marginal MAP, or mixed-inference, in which we are interested in first marginalizing a subsetA of variables (e.g., hidden variables), and then maximizing the remaining variables B (whose values are of direct interest), that is,\n\u03a6AB(\u03b8) = max xB Q(xB) = max xB log \u2211 xA exp [ \u2211 \u03b1\u2208F \u03b8\u03b1(x\u03b1) ] , (2)\nwhere A \u222aB = V (all the variables) and A \u2229B = \u2205. Obviously, both sum- and max- inference are special cases of marginal MAP when A = V and B = V , respectively.\nIt will be useful to define an even more general inference task, based on a power sum operator: \u03c4i\u2211 xi f(xi) = [\u2211 xi f(xi) 1/\u03c4i ]\u03c4i ,\nwhere f(xi) is any non-negative function and \u03c4i is a temperature or weight parameter. The power sum reduces to a standard sum when \u03c4i = 1, and approaches maxx f(x) when \u03c4i \u2192 0+, so that we define the power sum with \u03c4i = 0 to equal the max operator.\nThe power sum is helpful for unifying max- and sum- inference [e.g., 36], as well as marginal MAP [15]. Specifically, we can apply power sums with different weights \u03c4i to each variable xi along a predefined elimination order (e.g., [x1, . . . , xn]), to define the weighted log partition function:\n\u03a6\u03c4 (\u03b8) = log \u03c4\u2211 x exp(\u03b8(x)) = log \u03c4n\u2211 xn . . . \u03c41\u2211 x1 exp(\u03b8(x)), (3)\nwhere we note that the value of (3) depends on the elimination order unless all the weights are equal. Obviously, (3) includes marginal MAP (2) as a special case by setting weights \u03c4A = 1 and \u03c4B = 0. This representation provides a useful tool for understanding and deriving new algorithms for general inference tasks, especially marginal MAP, for which relatively few efficient algorithms exist."}, {"heading": "3 Related Work", "text": "Variational upper bounds on MAP and the partition function, along with algorithms for providing fast, convergent optimization, have been widely studied in the last decade. In MAP, dual decomposition and linear programming methods have become a dominating approach, with numerous optimization techniques [37, 6, 32, 14, 38, 30, 22], and methods to tighten the approximations [33, 14].\nFor summation problems, most upper bounds are derived from the tree-reweighted (TRW) family of convex bounds [34], or more generally conditional entropy decompositions [5]. TRW bounds can be framed as optimizing over a convex combination of tree-structured models, or in a dual representation as a message-passing, TRW belief propagation algorithm. This illustrates a basic tension in the resulting bounds: in its primal form 2 (combination of trees), TRW is inefficient: it maintains a weight and O(|V |) parameters for each tree, and a large number of trees may be required to obtain a tight bound; this uses memory and makes optimization slow. On the other hand, the dual, or free energy, form uses onlyO(|E|) parameters (the TRW messages) to optimize over the set of all possible spanning trees \u2013 but, the resulting optimization is only guaranteed to be a bound at convergence, 3 making it difficult to use in an anytime fashion. Similarly, the gradient of the weights is only correct at convergence, making it difficult to optimize over these parameters; most implementations [e.g., 24] simply adopt fixed weights.\nThus, most algorithms do not satisfy all the desirable properties listed in the introduction. For example, many works have developed convergent message-passing algorithms for convex free energies [e.g., 9, 10]. However, by optimizing the dual they do not provide a bound until convergence, and the representation and constraints on the counting numbers do not facilitate optimizing the bound over these parameters. To optimize counting numbers, [8] adopt a more restrictive free energy form requiring positive counting numbers on the entropies; but this cannot represent marginal MAP, whose free energy involves conditional entropies (equivalent to the difference between two entropy terms).\nOn the other hand, working in the primal domain ensures a bound, but usually at the cost of enumerating a large number of trees. [12] heuristically select a small number of trees to avoid being too inefficient, while [21] focus on trying to speed up the updates on a given collection of trees. Another primal bound is weighted mini-bucket (WMB, [16]), which can represent a large collection of trees compactly and is easily applied to marginal MAP using the weighted log partition function viewpoint [15, 18]; however, existing optimization algorithms for WMB are non-monotonic, and often fail to converge, especially on marginal MAP tasks.\nWhile our focus is on variational bounds [16, 17], there are many non-variational approaches for marginal MAP as well. [27, 40] provide upper bounds on marginal MAP by reordering the order in which variables are eliminated, and using exact inference in the reordered join-tree; however, this is exponential in the size of the (unconstrained) treewidth, and can easily become intractable. [20] give an approximation closely related to mini-bucket [2] to bound the marginal MAP; however, unlike (weighted) mini-bucket, these bounds cannot be improved iteratively. The same is true for the algorithm of [19], which also has a strong dependence on treewidth. Other examples of marginal MAP algorithms include local search [e.g., 28] and Markov chain Monte Carlo methods [e.g., 4, 41]."}, {"heading": "4 Fully Decomposed Upper Bound", "text": "In this section, we develop a new general form of upper bound and provide an efficient, monotonically convergent optimization algorithm. Our new bound is based on fully decomposing the graph into disconnected cliques, allowing very efficient local computation, but can still be as tight as WMB or the TRW bound with a large collection of spanning trees once the weights and shifting variables are chosen or optimized properly. Our bound reduces to dual decomposition for MAP inference, but is applicable to more general mixed-inference settings.\nOur main result is based on the following generalization of the classical Ho\u0308lder\u2019s inequality [7]: 2Despite the term \u201cdual decomposition\u201d used in MAP tasks, in this work we refer to decomposition bounds as \u201cprimal\u201d bounds, since they can be viewed as directly bounding the result of variable elimination. This is in contrast to, for example, the linear programming relaxation of MAP, which bounds the result only after optimization.\n3See an example in Supplement A.\nTheorem 4.1. For a given graphical model p(x; \u03b8) in (1) with cliques F = {\u03b1} and a set of nonnegative weights \u03c4 = {\u03c4i \u2265 0, i \u2208 V }, we define a set of \u201csplit weights\u201d w\u03b1 = {w\u03b1i \u2265 0, i \u2208 \u03b1} on each variable-clique pair (i, \u03b1), that satisfies \u2211 \u03b1|\u03b13i w \u03b1 i = \u03c4i. Then we have\n\u03c4\u2211 x \u220f \u03b1\u2208F exp [ \u03b8\u03b1(x\u03b1) ] \u2264 \u220f \u03b1\u2208F w\u03b1\u2211 x\u03b1 exp [ \u03b8\u03b1(x\u03b1) ] , (4)\nwhere the left-hand side is the powered-sum along order [x1, . . . , xn] as defined in (3), and the right-hand side is the product of the powered-sums on subvector x\u03b1 with weights w\u03b1 along\nthe same elimination order; that is, \u2211w\u03b1 x\u03b1 exp [ \u03b8\u03b1(x\u03b1) ] = \u2211w\u03b1kc xkc \u00b7 \u00b7 \u00b7 \u2211w\u03b1k1 xk1 exp [ \u03b8\u03b1(x\u03b1) ] , where x\u03b1 = [xk1 , . . . , xkc ] should be ranked with increasing index, consisting with the elimination order [x1, . . . , xn] as used in the left-hand side.\nProof details can be found in Section E of the supplement. A key advantage of the bound (4) is that it decomposes the joint power sum on x into a product of independent power sums over smaller cliques x\u03b1, which significantly reduces computational complexity and enables parallel computation."}, {"heading": "4.1 Including Cost-shifting Variables", "text": "In order to increase the flexibility of the upper bound, we introduce a set of cost-shifting or reparameterization variables \u03b4 = {\u03b4\u03b1i (xi) | \u2200(i, \u03b1), i \u2208 \u03b1} on each variable-factor pair (i, \u03b1), which can be optimized to provide a much tighter upper bound. Note that \u03a6\u03c4 (\u03b8) can be rewritten as,\n\u03a6\u03c4 (\u03b8) = log \u03c4\u2211 x exp [\u2211 i\u2208V \u2211 \u03b1\u2208Ni \u03b4\u03b1i (xi) + \u2211 \u03b1\u2208F ( \u03b8\u03b1(x\u03b1)\u2212 \u2211 i\u2208\u03b1 \u03b4\u03b1i (xi) )] ,\nwhere Ni = {\u03b1 | \u03b1 3 i} is the set of cliques incident to i. Applying inequality (4), we have that\n\u03a6\u03c4 (\u03b8) \u2264 \u2211 i\u2208V log wi\u2211 xi exp [ \u2211 \u03b1\u2208Ni \u03b4\u03b1i (xi) ] + \u2211 \u03b1\u2208F log w\u03b1\u2211 x\u03b1 exp [ \u03b8\u03b1(x\u03b1)\u2212 \u2211 i\u2208\u03b1 \u03b4\u03b1i (xi) ] def == L(\u03b4,w), (5)\nwhere the nodes i \u2208 V are also treated as cliques within inequality (4), and a new weight wi is introduced on each variable i; the new weights w = {wi, w\u03b1i | \u2200(i, \u03b1), i \u2208 \u03b1} should satisfy\nwi + \u2211 \u03b1\u2208Ni w\u03b1i = \u03c4i, wi \u2265 0, w\u03b1i \u2265 0, \u2200(i, \u03b1). (6)\nThe bound L(\u03b4,w) is convex w.r.t. the cost-shifting variables \u03b4 and weights w, enabling an efficient optimization algorithm that we present in Section 5. As we will discuss in Section 5.1, these shifting variables correspond to Lagrange multipliers that enforce a moment matching condition."}, {"heading": "4.2 Dual Form and Connection With Existing Bounds", "text": "It is straightforward to see that our bound in (5) reduces to dual decomposition [31] when applied on MAP inference with all \u03c4i = 0, and hence wi = w\u03b1i = 0. On the other hand, its connection with sum-inference bounds such as WMB and TRW is seen more clearly via a dual representation of (5):\nTheorem 4.2. The tightest upper bound obtainable by (5), that is,\nmin w min \u03b4 L(\u03b4,w) = min w max b\u2208L(G)\n{ \u3008\u03b8, b\u3009+ \u2211 i\u2208V wiH(xi; bi) + \u2211 \u03b1\u2208F \u2211 i\u2208\u03b1 w\u03b1i H(xi|xpa\u03b1i ; b\u03b1) } , (7)\nwhere b = {bi(xi), b\u03b1(x\u03b1) | \u2200(i, \u03b1), i \u2208 \u03b1} is a set of pseudo-marginals (or beliefs) defined on the singleton variables and the cliques, and L is the corresponding local consistency polytope defined by L(G) = {b | bi(xi) = \u2211 x\u03b1\\i b\u03b1(x\u03b1), \u2211 xi bi(xi) = 1}. Here, H(\u00b7) are their corresponding marginal or conditional entropies, and pa\u03b1i is the set of variables in \u03b1 that rank later than i, that is, for the global elimination order [x1, . . . , xn], pa\u03b1i = {j \u2208 \u03b1 | j i}. The proof details can be found in Section F of the supplement. It is useful to compare Theorem 4.2 with other dual representations. As the sum of non-negatively weighted conditional entropies, the bound is clearly convex and within the general class of conditional entropy decompositions (CED) [5], but unlike generic CED it has a simple and efficient primal form (5). 4 Comparing\n4The primal form derived in [5] (a geometric program) is computationally infeasible.\nto the dual form of WMB in Theorem 4.2 of [16], our bound is as tight as WMB, and hence the class of TRW / CED bounds attainable by WMB [16]. Most duality-based forms [e.g., 9, 10] are expressed in terms of joint entropies, \u3008\u03b8, b\u3009+ \u2211 \u03b2 c\u03b2H(b\u03b2), rather than conditional entropies; while the two can be converted, the resulting counting numbers c\u03b2 will be differences of weights {w\u03b1i }, 5 which obfuscates its convexity, makes it harder to maintain the relative constraints on the counting numbers during optimization, and makes some counting numbers negative (rendering some methods inapplicable [8]). Finally, like most variational bounds in dual form, the RHS of (7) has a inner maximization and hence guaranteed to bound \u03a6\u03c4 (\u03b8) only at its optimum.\nIn contrast, our Eq. (5) is a primal bound (hence, a bound for any \u03b4). It is similar to the primal form of TRW, except that (1) the individual regions are single cliques, rather than spanning trees of the graph, 6 and (2) the fraction weights w\u03b1 associated with each region are vectors, rather than a single scalar. The representation\u2019s efficiency can be seen with an example in Figure 1, which shows a 3\u00d73 grid model and three relaxations that achieve the same bound. Assuming d states per variable and ignoring the equality constraints, our decomposition in Figure 1(c) uses 24d cost-shifting parameters (\u03b4), and 24 weights. WMB (Figure 1(b)) is slightly more efficient, with only 8d parameters for \u03b4 and and 8 weights, but its lack of decomposition makes parallel and monotonic updates difficult. On the other hand, the equivalent primal TRW uses 16 spanning trees, shown in Figure 1(d), for 16 \u00b7 8 \u00b7 d2 parameters, and 16 weights. The increased dimensionality of the optimization slows convergence, and updates are non-local, requiring full message-passing sweeps on the involved trees (although this cost can be amortized in some cases [21])."}, {"heading": "5 Monotonically Tightening the Bound", "text": "In this section, we propose a block coordinate descent algorithm (Algorithm 1) to minimize the upper bound L(\u03b4,w) in (5) w.r.t. the shifting variables \u03b4 and weights w. Our algorithm has a monotonic convergence property, and allows efficient, distributable local computation due to the full decomposition of our bound. Our framework allows generic powered-sum inference, including max-, sum-, or mixed-inference as special cases by setting different weights."}, {"heading": "5.1 Moment Matching and Entropy Matching", "text": "We start with deriving the gradient of L(\u03b4,w) w.r.t. \u03b4 and w. We show that the zero-gradient equation w.r.t. \u03b4 has a simple form of moment matching that enforces a consistency between the singleton beliefs with their related clique beliefs, and that of weights w enforces a consistency of marginal and conditional entropies.\nTheorem 5.1. (1) For L(\u03b4,w) in (5), its zero-gradient w.r.t. \u03b4\u03b1i (xi) is \u2202L\n\u2202\u03b4\u03b1i (xi) = \u00b5i(xi)\u2212 \u2211 x\u03b1\\i \u00b5\u03b1(x\u03b1) = 0, (8)\n5See more details of this connection in Section F.3 of the supplement. 6While non-spanning subgraphs can be used in the primal TRW form, doing so leads to loose bounds; in\ncontrast, our decomposition\u2019s terms consist of individual cliques.\nAlgorithm 1 Generalized Dual-decomposition (GDD) Input: weights {\u03c4i | i \u2208 V }, elimination order o. Output: the optimal \u03b4\u2217,w\u2217 giving tightest upper bound L(\u03b4\u2217,w\u2217) for \u03a6\u03c4 (\u03b8) in (5). initialize \u03b4 = 0 and weights w = {wi, w\u03b1i }. repeat\nfor node i (in parallel with node j, (i, j) 6\u2208 E) do if \u03c4i = 0 then\nupdate \u03b4Ni = {\u03b4\u03b1i |\u2200\u03b1 \u2208 Ni} with the closed-form update (11); else if \u03c4i 6= 0 then\nupdate \u03b4Ni and wNi with gradient descent (8) and(12), combined with line search; end if\nend for until convergence \u03b4\u2217 \u2190 \u03b4, w\u2217 \u2190 w, and evaluate L(\u03b4\u2217,w\u2217) by (5); Remark. GDD solves max-, sum- and mixed-inference by setting different values of weights {\u03c4i}.\nwhere \u00b5i(xi) \u221d exp [\n1 wi \u2211 \u03b1\u2208Ni \u03b4 \u03b1 i (xi) ] can be interpreted as a singleton belief on xi, and \u00b5\u03b1(x\u03b1)\ncan be viewed as clique belief on x\u03b1, defined with a chain rule (assuming x\u03b1 = [x1, . . . , xc]), \u00b5\u03b1(x\u03b1) = \u220fc i=1 \u00b5\u03b1(xi|xi+1:c); \u00b5\u03b1(xi|xi+1:c) = (Zi\u22121(xi:c)/Zi(xi+1:c))1/w \u03b1 i , where Zi is the partial powered-sum up to x1:i on the clique, that is,\nZi(xi+1:c) = w\u03b1i\u2211 xi \u00b7 \u00b7 \u00b7 w\u03b11\u2211 x1 exp [ \u03b8\u03b1(x\u03b1)\u2212 \u2211 i\u2208\u03b1 \u03b4\u03b1i (xi) ] , Z0(x\u03b1) = exp [ \u03b8\u03b1(x\u03b1)\u2212 \u2211 i\u2208\u03b1 \u03b4\u03b1i (xi) ] ,\nwhere the summation order should be consistent with the global elimination order o = [x1, . . . , xn].\n(2) The gradients of L(\u03b4,w) w.r.t. the weights {wi, w\u03b1i } are marginal and conditional entropies defined on the beliefs {\u00b5i, \u00b5\u03b1}, respectively,\n\u2202L \u2202wi = H(xi;\u00b5i),\n\u2202L\n\u2202w\u03b1i = H(xi|xi+1:c;\u00b5\u03b1) = \u2212 \u2211 x\u03b1 \u00b5\u03b1(x\u03b1) log\u00b5\u03b1(xi|xi+1:c). (9)\nTherefore, the optimal weights should satisfy the following KKT condition wi ( H(xi;\u00b5i)\u2212 H\u0304i ) = 0, w\u03b1i ( H(xi|xi+1:c;\u00b5\u03b1)\u2212 H\u0304i ) = 0, \u2200(i, \u03b1) (10)\nwhere H\u0304i = ( wiH(xi;\u00b5i) + \u2211 \u03b1 w \u03b1 i H(xi|xi+1:c;\u00b5\u03b1) ) /\u03c4i is the average entropy on node i.\nThe proof details can be found in Section G of the supplement. The matching condition (8) enforces that \u00b5 = {\u00b5i, \u00b5\u03b1 | \u2200(i, \u03b1)} belong to the local consistency polytope L as defined in Theorem 4.2; similar moment matching results appear commonly in variational inference algorithms [e.g., 34]. [34] also derive a gradient of the weights, but it is based on the free energy form and is correct only after optimization; our form holds at any point, enabling efficient joint optimization of \u03b4 and w."}, {"heading": "5.2 Block Coordinate Descent", "text": "We derive a block coordinate descent method in Algorithm 1 to minimize our bound, in which we sweep through all the nodes i and update each block \u03b4Ni = {\u03b4\u03b1i (xi) | \u2200\u03b1 \u2208 Ni} and wNi = {wi, w\u03b1i | \u2200\u03b1 \u2208 Ni} with the neighborhood parameters fixed. Our algorithm applies two update types, depending on whether the variables have zero weight: (1) For nodes with \u03c4i = 0 (corresponding to max nodes i \u2208 B in marginal MAP), we derive a closed-form coordinate descent rule for the associated shifting variables \u03b4Ni ; these nodes do not require to optimize wNi since it is fixed to be zero. (2) For nodes with \u03c4i 6= 0 (e.g., sum nodes i \u2208 A in marginal MAP), we lack a closed form update for \u03b4Ni and wNi , and optimize by local gradient descent combined with line search.\nThe lack of a closed form coordinate update for nodes \u03c4i 6= 0 is mainly because the order of power sums with different weights cannot be exchanged. However, the gradient descent inner loop is still efficient, because each gradient evaluation only involves the local variables in clique \u03b1.\nClosed-form Update. For any node i with \u03c4i = 0 (i.e., max nodes i \u2208 B in marginal MAP), and its associated \u03b4Ni = {\u03b4\u03b1i (xi) | \u2200\u03b1 \u2208 Ni}, the following update gives a closed form solution for the zero (sub-)gradient equation in (8) (keeping the other {\u03b4\u03b1j |j 6= i,\u2200\u03b1 \u2208 Ni} fixed):\n\u03b4\u03b1i (xi)\u2190 |Ni| |Ni|+ 1 \u03b3\u03b1i (xi)\u2212 1 |Ni|+ 1 \u2211\n\u03b2\u2208Ni\\\u03b1\n\u03b3\u03b2i (xi), (11)\nwhere |Ni| is the number of neighborhood cliques, and \u03b3\u03b1i (xi) = log \u2211w\u03b1\\i x\u03b1\\i exp [ \u03b8\u03b1(x\u03b1) \u2212\u2211\nj\u2208\u03b1\\i \u03b4 \u03b1 j (xj) ] . Note that the update in (11) works regardless of the weights of nodes {\u03c4j | \u2200j \u2208 \u03b1, \u2200\u03b1 \u2208 Ni} in the neighborhood cliques; when all the neighboring nodes also have zero weight (\u03c4j = 0 for \u2200j \u2208 \u03b1, \u2200\u03b1 \u2208 Ni), it is analogous to the \u201cstar\u201d update of dual decomposition for MAP [31]. The detailed derivation is shown in Proposition H.1 and H.2 in the supplement.\nThe update in (11) can be calculated with a cost of only O(|Ni| \u00b7 d|\u03b1|), where d is the number of states of xi, and |\u03b1| is the clique size, by computing and saving all the shared {\u03b3\u03b1i (xi)} before updating \u03b4Ni . Furthermore, the updates of \u03b4Ni for different nodes i are independent if they are not directly connected by some clique \u03b1; this makes it easy to parallelize the coordinate descent process by partitioning the graph into independent sets, and parallelizing the updates within each set.\nLocal Gradient Descent. For nodes with \u03c4i 6= 0 (or i \u2208 A in marginal MAP), there is no closedform solution for {\u03b4\u03b1i (xi)} and {wi, w\u03b1i } to minimize the upper bound. However, because of the fully decomposed form, the gradient w.r.t. \u03b4Ni and wNi , (8)\u2013(9), can be evaluated efficiently via local computation with O(|Ni| \u00b7 d|\u03b1|), and again can be parallelized between nonadjacent nodes. To handle the normalization constraint (6) on wNi , we use an exponential gradient descent: let wi = exp(vi)/ [ exp(vi) + \u2211 \u03b1 exp(v \u03b1 i ) ] and w\u03b1i = exp(v \u03b1 i )/ [ exp(vi) + \u2211 \u03b1 exp(v \u03b1 i ) ] ; taking the gradient w.r.t. vi and v\u03b1i and transforming back gives the following update wi \u221d wi exp [ \u2212 \u03b7wi ( H(xi;\u00b5i)\u2212 H\u0304i )] , w\u03b1i \u221d w\u03b1i exp [ \u2212 \u03b7w\u03b1i ( H(xi|xpa\u03b1i ;\u00b5\u03b1)\u2212 H\u0304i )] , (12) where \u03b7 is the step size and pa\u03b1i ={j\u2208\u03b1 | j i}. In our implementation, we find that a few gradient steps (e.g., 5) with a backtracking line search using the Armijo rule works well in practice. Other more advanced optimization methods, such as L-BFGS and Newton\u2019s method are also applicable."}, {"heading": "6 Experiments", "text": "In this section, we demonstrate our algorithm on a set of real-world graphical models from recent UAI inference challenges, including two diagnostic Bayesian networks with 203 and 359 variables and max domain sizes 7 and 6, respectively, and several MRFs for pedigree analysis with up to 1289 variables, max domain size of 7 and clique size 5.7 We construct marginal MAP problems on these models by randomly selecting half of the variables to be max nodes, and the rest as sum nodes.\nWe implement several algorithms that optimize the same primal marginal MAP bound, including our GDD (Algorithm 1), the WMB algorithm in [16] with ibound = 1, which uses the same cliques and a fixed point heuristic for optimization, and an off-the-shelf L-BFGS implementation that directly optimizes our decomposed bound. For comparison, we also computed several related primal bounds, including standard mini-bucket [2] and elimination reordering [27, 40], limited to the same computational limits (ibound = 1). We also tried MAS [20] but found its bounds extremely loose.8\nDecoding (finding a configuration x\u0302B) is more difficult in marginal MAP than in joint MAP. We use the same local decoding procedure that is standard in dual decomposition [31]. However, evaluating the objective Q(x\u0302B) involves a potentially difficult sum over xA, making it hard to score each decoding. For this reason, we evaluate the score of each decoding, but show the most recent decoding rather than the best (as is standard in MAP) to simulate behavior in practice.\nFigure 2 and Figure 3 compare the convergence of the different algorithms, where we define the iteration of each algorithm to correspond to a full sweep over the graph, with the same order of time complexity: one iteration for GDD is defined in Algorithm 1; for WMB is a full forward and backward message pass, as in Algorithm 2 of [16]; and for L-BFGS is a joint quasi-Newton step on all variables. The elimination order that we use is obtained by a weighted-min-fill heuristic [1] constrained to eliminate the sum nodes first.\nDiagnostic Bayesian Networks. Figure 2(a)-(b) shows that our GDD converges quickly and monotonically on both the networks, while WMB does not converge without proper damping; we\n7See http://graphmod.ics.uci.edu/uai08/Evaluation/Report/Benchmarks. 8The instances tested have many zero probabilities, which make finding lower bounds difficult; since MAS\u2019\nbounds are symmetrized, this likely contributes to its upper bounds being loose.\nexperimented different damping ratios for WMB, and found that it is slower than GDD even with the best damping ratio found (e.g., in Figure 2(a), WMB works best with damping ratio 0.035 (WMB0.035), but is still significantly slower than GDD). Our GDD also gives better decoded marginal MAP solution xB (obtained by rounding the singleton beliefs). Both WMB and our GDD provide a much tighter bound than the non-iterative mini-bucket elimination (MBE) [2] or reordered elimination [27, 40] methods.\nGenetic Pedigree Instances. Figure 3 shows similar results on a set of pedigree instances. Again, GDD outperforms WMB even with the best possible damping, and out-performs the non-iterative bounds after only one iteration (pass through the graph)."}, {"heading": "7 Conclusion", "text": "In this work, we propose a new class of decomposition bounds for general powered-sum inference, which is capable of representing a large class of primal variational bounds but is much more computationally efficient. Unlike previous primal sum bounds, our bound decomposes into computations on small, local cliques, increasing efficiency and enabling parallel and monotonic optimization. We derive a block coordinate descent algorithm for optimizing our bound over both the cost-shifting parameters (reparameterization) and weights (fractional counting numbers), which generalizes dual decomposition and enjoy similar monotonic convergence property. Taking the advantage of its monotonic convergence, our new algorithm can be widely applied as a building block for improved heuristic construction in search, or more efficient learning algorithms."}, {"heading": "Acknowledgments", "text": "This work is sponsored in part by NSF grants IIS-1065618 and IIS-1254071. Alexander Ihler is also funded in part by the United States Air Force under Contract No. FA8750-14-C-0011 under the DARPA PPAML program."}, {"heading": "A Experiment on Ising grid", "text": "Our GDD directly optimizes a primal bound, and is thus guaranteed to be an upper bound of the partition function even before the algorithm converges, enabling a desirable \u201cany-time\u201d property. In contrast, typical implementations of tree reweighted (TRW) belief propagation optimize the dual free energy function [34], and are not guaranteed to be a bound before convergence. We illustrate this point using an experiment on a toy 5\u00d75 Ising grid, with parameters generated by normal ditribution N(0, 2) and half nodes selected as max-nodes for marginal MAP. Figure 4(a)-(b) shows the TRW free energy objective and GDD, WMB upper bounds across iterations; we can see that TRW does violate the upper bound property before convergence, while GDD and WMB always give valid upper bounds."}, {"heading": "B More Results on Diagnostic Bayesian Networks", "text": "In addtion to the marginal MAP results on BN-1 and BN-2 in main text, we vary the percentage of max-nodes when generating the marginal MAP problems; the reported results in Figure 5(a)-(b) are the best bound obtained by the different algorithms with the first 20 iterations. In all cases, GDD\u2019s results are as good or better than WMB. WMB-0.5 (WMB with damping ratio 0.5) appears to work well on sum-only and max-only (MAP) problems, i.e., when the percentage of max-nodes equals 0% and 100% respectively, but performs very poorly on intermediate settings. The far more heavily damped WMB-0.04 or WMB-0.02 work better on average, but have much slower convergence."}, {"heading": "C More Results on Pedigree Linkage Analysis", "text": "We test our algorithm on additional 6 models of pedigree linkage analysis from the UAI08 inference challenge. We construct marginal MAP problems by randomly selected 50% of nodes to be max-nodes, and report all the results in Figure 6. We find that our algorithm consistently outperforms WMB with the best possible damping ratio."}, {"heading": "D Extensions to Junction Graph", "text": "Our bound (5) in the main text uses a standard \u201cfactor graph\u201d representation in which the cost-shifts {\u03b4\u03b1i } are defined for each variable-factor pair (i, \u03b1), and are functions of single variables xi. We can extend our bound to use more general shifting parameters using a junction graph representation; this allows us to exploit higher order clique structures, leading to better performance.\nLet (C,S) be a junction graph of p(x; \u03b8) where C = {c | c \u2282 V } is the set of clusters, and S = {s = ck \u2229 cl | ck, cl \u2208 C} is the set of separators. Assume p(x; \u03b8) can be reparameterized into the form,\np(x; \u03b8) = exp [\u2211 c\u2208C \u03b8c(xc)\u2212 \u03a6(\u03b8) ] , (13)\nand the weighted log partition function is rewritten as \u03a6\u03c4 (\u03b8) = log \u2211\u03c4 x exp [\u2211 c\u2208C \u03b8c(xc) ] . Similar to the derivation of bound (5) in the main text, we can apply Theorem 4.1, but with a set of more general cost-shifting variables \u03b4cs , defined on each adjacent separator-cluster pair (s, c); this gives the more general upper bound,\n\u03a6\u03c4 (\u03b8) \u2264 \u2211 s\u2208S log ws\u2211 xs exp [\u2211 c\u2287s \u03b4cs(xs) ] + \u2211 c\u2208C log wc\u2211 xc exp [ \u03b8c(xc)\u2212 \u2211 s\u2286c \u03b4cs(xs) ] , (14)\nwhere we introduce the set of non-negative weights ws = {wsi | i \u2208 s} on each separator and wc = {wci | i \u2208 c} on each cluster, which should satisfy \u2211 s\u2208Nsei wsi + \u2211 c\u2208Nci wci = \u03c4i, where N se i = {s | i \u2208 s} are all the separators that include node i, and Nci = {c | i \u2208 c} are all the clusters that include node i. Obviously, our earlier bound (5) in the main text can be viewed as a special case of (14) with a special junction graph whose separators consist of only single variables, that is, S = V .\nA block coordinate descent algorithm similar to Algorithm 1 can be derived to optimize the junction graph bound. In this case, we sweep through all the separators s and perform block coordinate update on all {\u03b4cs|\u2200c \u2287 s} at each iteration. Similarly to Algorithm 1, we can derive a close form update for separators with all-zero weights (that is, \u03c4i = 0, \u2200i \u2208 s, corresponding to s \u2286 B in marginal MAP), and perform local gradient descent otherwise."}, {"heading": "E Proof of Thereom 4.1", "text": "Proof. Note the Ho\u0308lder\u2019s inequality is[\u2211\nx \u220f j fj(x) 1/\u03be0 ]\u03be0 \u2264\u220f j [\u2211 x fj(x) 1/\u03bej ]\u03bej , where {fj(x)} are arbitrary positive functions, and {\u03bej} are non-negative numbers that satisfy \u2211 j \u03bej = \u03be0. Note we extend the inequality by defining power sum with \u03bej = 0 to equal the max operator. Our result follows by applying Ho\u0308lder\u2019s inequality on each xi sequentially along the elimination order [x1, x2, \u00b7 \u00b7 \u00b7 , xn]."}, {"heading": "F Dual Representations", "text": "F.1 Background\nThe log-partition function \u03a6(\u03b8) has the following variational (dual) form \u03a6(\u03b8) = log \u2211 x exp(\u03b8(x)) = max b\u2208M(G) { \u3008\u03b8, b\u3009+H(x; b) } where M(G) is the marginal polytope [35]. Then, for any scalar \u03b5 > 0 (including \u03b5\u2192 0+), we have\n\u03a6\u03b5(\u03b8) = \u03b5 log \u2211 x exp( \u03b8(x) \u03b5 ) = \u03b5max b\u2208M { \u3008\u03b8 \u03b5 , b\u3009+H(x; b) } = max b\u2208M { \u3008\u03b8, b\u3009+ \u03b5H(x; b) } .\nAs stated in [15, 16], we can further generalize the variational form of above scalar-weighted log partition function to the vector-weighted log partition function (3) in the main text,\n\u03a6\u03c4 (\u03b8) = log \u03c4n\u2211 xn . . . \u03c41\u2211 x1 exp(\u03b8(x)) = max b\u2208M(G) { \u3008\u03b8, b\u3009+ \u2211 i \u03c4iH(xi|xi+1:n; b) } , (15)\nwhere H(xi|xi+1:n; b) is the conditional entropy on b(x), and is defined as H(xi|xi+1:n; b) = \u2212 \u2211 x b(x) log(b(xi|xi+1:n)). See more details of its derivation in Theorem 4.1 within [15].\nA notable special case of (15) is the dual form of marginal MAP\n\u03a6AB(\u03b8) = max xB log \u2211 xA exp ( \u03b8(x) ) = max b\u2208M(G) { \u3008\u03b8, b\u3009+H(xA|xB ; b) } , (16)\nby setting weights \u03c4A = 1 and \u03c4B = 0.\nF.2 Proof of Thereom 4.2\nWe now prove the following dual representation of our bound,\nmin \u03b4 L(\u03b4,w) = max b\u2208L(G)\n{ \u3008\u03b8, b\u3009+ \u2211 i\u2208V wiH(xi; bi) + \u2211 \u03b1\u2208F \u2211 i\u2208\u03b1 w\u03b1i H(xi|xpa\u03b1i ; b\u03b1) } , (17)\nwhere L(G) = {b | bi(xi) = \u2211 x\u03b1\\i b\u03b1(x\u03b1), \u2211 xi bi(xi) = 1} is the local consistency polytope, and pa\u03b1i = {j \u2208 \u03b1|j i}. Thereom 4.2 follows directly from (17). Proof. In our primal bound L(\u03b4,w) (5) in the main text, we let \u03b8\u0303i(xi) = \u03b8i(xi) + \u2211 \u03b1\u2208Ni \u03b4 \u03b1 i (xi) (we add\ndummy singleton \u03b8i(xi) \u2261 0), and \u03b8\u0303\u03b1(x\u03b1) = \u03b8\u03b1(x\u03b1)\u2212 \u2211 i\u2208\u03b1 \u03b4 \u03b1 i (xi), then the bound can be rewritten as,\nL(\u03b8\u0303,w) = \u2211 i\u2208V log wi\u2211 xi exp [ \u03b8\u0303i(xi) ] + \u2211 \u03b1\u2208F log w\u03b1\u2211 x\u03b1 exp [ \u03b8\u0303\u03b1(x\u03b1) ] .\nNote, for any assignment x, we have \u2211 i \u03b8\u0303i(xi) + \u2211 \u03b1 \u03b8\u0303\u03b1(x\u03b1) = \u2211 \u03b1 \u03b8\u03b1(x\u03b1).\nBy applying the dual form of the powered sum (15) on each node and clique respectively, we have L(\u03b8\u0303,w) = \u2211 i\u2208V max bi\u2208M(Gi) { \u3008\u03b8\u0303i, bi\u3009+ wiH(xi; bi) } + \u2211 \u03b1\u2208F max b\u03b1\u2208M(G\u03b1) { \u3008\u03b8\u0303\u03b1, b\u03b1\u3009+ \u2211 i\u2208\u03b1 w\u03b1i H(xi|xpa\u03b1i ; b\u03b1) } ,\nwhere pa\u03b1i is the set of variables in \u03b1 that are summed out later than i, M(Gi) and M(G\u03b1) are the marginal polytopes on singleton node i and clique \u03b1 respectively, which enforce {bi, b\u03b1} to be properly normalized. The above equation can be more compactly rewritten as,\nL(\u03b8\u0303,w) = max b\u2208M\u0303\n{ \u3008\u03b8\u0303, b\u3009+ \u2211 i\u2208V wiH(xi; bi) + \u2211 \u03b1\u2208F \u2211 i\u2208\u03b1 w\u03b1i H(xi|xpa\u03b1i ; b\u03b1) } ,\nwhere M\u0303 = {M(Gi),M(G\u03b1) | \u2200 i \u2208 V, \u03b1 \u2208 F}, and the elements {bi, b\u03b1} of b are independently optimized.\nThen, by tightening reparameterization \u03b8\u0303 = {\u03b8\u0303i, \u03b8\u0303\u03b1}, we have\nmin \u03b8\u0303 L(\u03b8\u0303,w) = max b\u2208M\u0303 min \u03b8\u0303\n{ \u3008\u03b8\u0303, b\u3009+ \u2211 i\u2208V wiH(xi; bi) + \u2211 \u03b1\u2208F \u2211 i\u2208\u03b1 w\u03b1i H(xi|xpa\u03b1i ; b\u03b1) }\nwhere the order of min and max are commuted according to the strong duality (it\u2019s convex with \u03b8\u0303, and concave with b). The inner minimization min\u03b8\u0303\u3008\u03b8\u0303, b\u3009 is a linear program, and it turns out can be solved analytically. To see this, given the relationship between \u03b8\u0303 and \u03b4, we rewrite the linear program as\nmin \u03b8\u0303 \u3008\u03b8\u0303, b\u3009 = min \u03b4\n{ \u3008\u03b8, b\u3009+ \u2211 i\u2208V \u2211 xi \u2211 \u03b1\u2208Ni \u03b4\u03b1i (xi)bi(xi)\u2212 \u2211 \u03b1\u2208F \u2211 x\u03b1 \u2211 i\u2208\u03b1 \u03b4\u03b1i (xi)b\u03b1(x\u03b1) } ,\n= min \u03b4\n{ \u3008\u03b8, b\u3009+ \u2211 (i,\u03b1) \u2211 xi \u03b4\u03b1i (xi) ( bi(xi)\u2212 \u2211 x\u03b1\\i b\u03b1(x\u03b1) )} .\nThen, it is easy to observe that the linear program is either equal to \u3008\u03b8, b\u3009 only if b satisfy the marginalization constraint \u2211 x\u03b1\\i\nb\u03b1(x\u03b1) = bi(xi) for \u2200(i, \u03b1), or it will become negative infinity. Considering the outer maximization, we have\nmin \u03b8\u0303 L(\u03b8\u0303,w) = max b\u2208L(G)\n{ \u3008\u03b8, b\u3009+ \u2211 i\u2208V wiH(xi; bi) + \u2211 \u03b1\u2208F \u2211 i\u2208\u03b1 w\u03b1i H(xi|xpa\u03b1i ; b\u03b1) } ,\nwhere L(G) is the local consistency polytope that is obtained by enforcing both M\u0303 and the marginalization constraint.\nF.3 Connection with Existing Free Energy Forms\nMost variational forms are expresssed in the following linear combination of local entropies [39, 10], \u3008\u03b8, b\u3009+ \u2211 \u03b2 c\u03b2H(b\u03b2), (18)\nwhere \u03b2 refers the region, c\u03b2 refers the general counting number, b\u03b2(x\u03b2) is the local belief.\nWe can rewrite our dual representations (17) as, \u3008\u03b8, b\u3009+ \u2211 i\u2208V wiH(xi; bi) + \u2211 \u03b1\u2208F \u2211 i\u2208\u03b1 w\u03b1i ( H(xi, xpa\u03b1i ; b\u03b1)\u2212H(xpa\u03b1i ; b\u03b1) ) ,\nwhere pa\u03b1i is the set of variables in \u03b1 that rank later than i. Without loss of generality, assuming x\u03b1 = [x1, \u00b7 \u00b7 \u00b7 , xi, xj , \u00b7 \u00b7 \u00b7xc], i.e. xi and xj are adjacent in the order, we can get\n\u3008\u03b8, b\u3009+ \u2211 i\u2208V wiH(xi; bi) + \u2211 \u03b1\u2208F { w\u03b11H(x\u03b1; b\u03b1) + \u2211 [i,j]v\u03b1 (w\u03b1j \u2212 w\u03b1i )H(xpa\u03b1i ; bpa\u03b1i ) }\n(19)\nwhere belief bpa\u03b1i is defined by bpa\u03b1i (xpa\u03b1i ) = \u2211 x\u03b1\\pa\u03b1\ni\nb\u03b1(x\u03b1).\nOne can view (19) in terms of (18), by selecting the region \u03b2 \u2208 {i \u2208 V } \u222a {\u03b1 \u2208 F} \u222a {pa\u03b1i | \u2200(i, \u03b1)}; some counting numbers c\u03b2 will be the differences of weights w\u03b1j \u2212 w\u03b1i .\nF.4 Matching Our Bound to WMB\nAfter the weights are optimized, our GDD bound matches to WMB bound with optimum weights. A simple weight initialization method matches our bound to WMB with uniform weights on each mini-bucket, which often gives satisfactory result; a similar procedure can be used to match the bound with more general weights as in Section D. We first set wi = 0 for all nodes i. We then visit the nodes xi along the elimination order o = [x1, x2, \u00b7 \u00b7 \u00b7 , xn], and divide xi\u2019s neighborhood cliquesNi = {\u03b1|\u03b1 3 i} into two groups: (1) the children cliques in which all x\u03b1\\i have already been eliminated, that is, Nchi = {\u03b1 | \u2200j \u2208 \u03b1\\i, j \u227a i in o}; (2) the other, parent cliques Npai = {\u03b1 | \u2203j \u2208 \u03b1\\i, j i in o}. We set w \u03b1 i = 0 for all the children cliques (\u03b1 \u2208 Nchi ), and uniformly split the weights, that is, w\u03b1i = \u03c4i/|Npai |, across the parent cliques."}, {"heading": "G Proof of Therom 5.1", "text": "Proof. For each \u03b4\u03b1i (xi), the involved terms in L(\u03b4,w) are L \u03b1 i (\u03b4) = \u03a6wi(\u03b4) + \u03a6w\u03b1(\u03b4), where\n\u03a6wi(\u03b4) = log wi\u2211 xi exp [ \u2211 \u03b1\u2208Ni \u03b4\u03b1i (xi) ] , \u03a6w\u03b1(\u03b4) = log w\u03b1\u2211 x\u03b1 exp [ \u03b8\u03b1(x\u03b1)\u2212 \u2211 i\u2208\u03b1 \u03b4\u03b1i (xi) ] .\nOur result follows by showing that\n\u2202\u03a6wi(\u03b4) \u2202\u03b4\u03b1i (xi) = \u00b5i(xi) and \u2202\u03a6wi(\u03b4) \u2202wi = H(xi;\u00b5i),\n\u2202\u03a6w\u03b1(\u03b4)\n\u2202\u03b4\u03b1i (xi) = \u2212 \u2211 x\u03b1\\i \u00b5\u03b1(x\u03b1) and \u2202\u03a6w\u03b1(\u03b4) \u2202w\u03b1i = H(xi|xi+1:c;\u00b5\u03b1).\nThe gradient of \u03a6wi(\u03b4) is straightforward to calculate,\n\u2202\u03a6wi \u2202\u03b4\u03b1i (xi) = \u2202 \u2202\u03b4\u03b1i (xi)\n( wi log \u2211 xi exp [\u2211 \u03b1\u2208Ni \u03b4 \u03b1 i (xi) wi ]) = exp [\u2211 \u03b1\u2208Ni \u03b4\u03b1i (xi) wi ] Zwi = \u00b5i(xi),\nwhere Zwi = \u2211 xi exp [\u2211 \u03b1\u2208Ni \u03b4\u03b1i (xi) wi ] , and\n\u2202\u03a6wi \u2202wi = logZwi + wi \u00b7 1 Zwi \u00b7 \u2211 xi { exp [\u2211 \u03b1\u2208Ni \u03b4 \u03b1 i (xi) wi ] \u00b7 \u2211 \u03b1\u2208Ni \u03b4 \u03b1 i (xi) \u2212w2i } = logZwi \u2212\n\u2211 xi { \u00b5i(xi) \u00b7 \u2211 \u03b1\u2208Ni \u03b4 \u03b1 i (xi) wi } = \u2212\n\u2211 xi { \u00b5i(xi) \u00b7 [\u2211 \u03b1\u2208Ni \u03b4 \u03b1 i (xi) wi \u2212 logZwi ]} = H(xi;\u00b5i).\nThe gradient of \u03a6w\u03b1(\u03b4) is more involved; see Proposition I.1 for a detailed derivation.\nGiven the gradients, the moment matching condition (8) in Therom 5.1 obviously holds. We now prove the entropy matching condition in (10). The constraint optimization is\nmin w L(w), s.t. wi \u2265 0, w\u03b1i \u2265 0, wi + \u2211 \u03b1 w\u03b1i = \u03c4i.\nNote, when \u03c4i = 0, the optimization is trival, so we simply assume \u03c4i > 0. We frame the Lagrangian as\nK(w,\u03bb, g) def == L(w) + \u2211 i \u03bbi ( wi + + \u2211 \u03b1 w\u03b1i \u2212 \u03c4i ) + \u2211 i giwi + \u2211 (i,\u03b1) g\u03b1i w \u03b1 i .\nNote g \u2264 0 (dual feasibility), otherwise maxg,\u03bbK(w,\u03bb, g) will approach infinity. The KKT conditions are\nstationarity: \u2202K\n\u2202wi = H(xi;\u00b5i) + \u03bbi + gi = 0, (20)\n\u2202K \u2202w\u03b1i = H(xi|xi+1:c;\u00b5\u03b1) + \u03bbi + g\u03b1i = 0, (21)\ncomplementary slackness: giwi = 0, g\u03b1i w \u03b1 i = 0. (22)\nWe multiply wi and w\u03b1i to (20) and (21) respectively, then we can eliminate the KKT multipliers gi and g \u03b1 i by applying the complementary slackness (22),\nwiH(xi;\u00b5i) + wi\u03bbi = 0, (23) w\u03b1i H(xi|xi+1:c;\u00b5\u03b1) + w\u03b1i \u03bbi = 0. (24)\nBy summing (24) over all \u03b1 \u2208 Ni and adding (23), we can solve the multiplier \u03bbi as\n\u03bbi = \u2212 wiH(xi;\u00b5i) +\n\u2211 \u03b1 w \u03b1 i H(xi|xi+1:c;\u00b5\u03b1)\n\u03c4i = \u2212H\u0304i.\nWe plug it into (23) and (24), and obtain the entropy matching condition (10) in the Therom 5.1."}, {"heading": "H Derivations of Closed-form Update", "text": "We first derive the closed-form update rule for \u03b4\u03b1i (xi) in Proposition H.1. We derive the closed-form update rule for the block \u03b4Ni = {\u03b4\u03b1i (xi) | \u2200\u03b1 \u2208 Ni} in Proposition H.2.\nProposition H.1. Given max node i in marginal MAP (i.e., \u03c4i = 0 ) and one clique \u03b1 3 i (i.e. i \u2208 \u03b1), keeping all \u03b4 fixed except \u03b4\u03b1i (xi), there is a closed-form update rule,\n\u03b4\u03b1i (xi)\u2190 1\n2 log w\u03b1\\i\u2211 x\u03b1\\i exp [ \u03b8\u03b1(x\u03b1)\u2212 \u2211 j\u2208\u03b1\\i \u03b4\u03b1j (xj) ] \u2212 1 2 \u2211 \u03b2\u2208Ni\\\u03b1 \u03b4\u03b2i (xi), (25)\nwhere x\u03b1\\i = {xj : j \u2208 \u03b1, j 6= i}, w\u03b1\\i = {w\u03b1j : j \u2208 \u03b1, j 6= i}, and Ni = {\u03b1|\u03b1 3 i} is the set of all clique factors in the neighborhood of node i. Futhermore, this update will monotonically decrease the upper bound.\nProof. The terms within the bound L(\u03b4,w) that depend on \u03b4\u03b1i (xi) are,\nmax xi [ \u2211 \u03b1\u2208Ni \u03b4\u03b1i (xi) ] + max xi log w\u03b1\\i\u2211 x\u03b1\\i exp [ \u03b8\u03b1(x\u03b1)\u2212 \u2211 i\u2208\u03b1 \u03b4\u03b1i (xi) ] . (26)\nThe sub-gradient of (26) w.r.t. \u03b4\u03b1i (xi) equal to zero if and only if,\nx\u2217i = argmax xi [ \u2211 \u03b1\u2208Ni \u03b4\u03b1i (xi) ] = argmax xi log w\u03b1\\i\u2211 x\u03b1\\i exp [ \u03b8\u03b1(x\u03b1)\u2212 \u2211 i\u2208\u03b1 \u03b4\u03b1i (xi) ] ,\nwhich is \u201cargmax\u201d matching. One sufficient condition of this matching is,\n\u2211 \u03b1\u2208Ni \u03b4\u03b1i (xi) = log w\u03b1\\i\u2211 x\u03b1\\i exp [ \u03b8\u03b1(x\u03b1)\u2212 \u2211 i\u2208\u03b1 \u03b4\u03b1i (xi) ]\nwhich impllies matching of \u201cpseudo marginals\u201d. Then, one can pull \u03b4\u03b1i (xi) outside from the operator log \u2211w\u03b1\\i x\u03b1\\i exp, and get the closed-form equation\n\u03b4\u03b1i (xi) = 1\n2 log w\u03b1\\i\u2211 x\u03b1\\i exp [ \u03b8\u03b1(x\u03b1)\u2212 \u2211 j\u2208\u03b1\\i \u03b4\u03b1j (xj) ] \u2212 1 2 \u2211 \u03b2\u2208Ni\\\u03b1 \u03b4\u03b2i (xi).\nTo prove monotonicity, we substitute above update equation of \u03b4\u03b1i (xi) into (26); then we get,\nmax xi { \u2211 \u03b2\u2208Ni\\\u03b1 \u03b4\u03b2i (xi) + log w\u03b1\\i\u2211 x\u03b1\\i exp [ \u03b8\u03b1(x\u03b1)\u2212 \u2211 j\u2208\u03b1\\i \u03b4\u03b1j (xj) ]} . (27)\nClearly, (27) \u2264 (26) by using the fact that maxx[f(x) + g(x)] \u2264 maxx f(x) + maxx g(x).\nProposition H.2. Given node i \u2208 B (i.e., a max node) and all neighborhood cliques Ni = {\u03b1|\u03b1 3 i}, we can jointly optimize \u03b4Ni = {\u03b4\u03b1i (xi) | \u2200\u03b1 \u2208 Ni} in closed-form by keeping the other {\u03b4\u03b1j | j 6= i,\u2200\u03b1 \u2208 Ni} fixed. The update rule is,\n\u03b4\u03b1i (xi)\u2190 |Ni| |Ni|+ 1 \u03b3\u03b1i (xi)\u2212 1 |Ni|+ 1 \u2211\n\u03b2\u2208Ni\\\u03b1\n\u03b3\u03b2i (xi), (28)\nwhere |Ni| is the number of neighborhood cliques, and {\u03b3\u03b1i (xi) | \u2200\u03b1 \u2208 Ni} are defined by\n\u03b3\u03b1i (xi) = log w\u03b1\\i\u2211 x\u03b1\\i exp [ \u03b8\u03b1(x\u03b1)\u2212 \u2211 j\u2208\u03b1\\i \u03b4\u03b1j (xj) ] . (29)\nFuthermore, this upate will monotonically decrease the upper bound.\nProof. For \u2200\u03b1 \u2208 Ni, we have closed-form solutions for \u03b4\u03b1i (xi) as Proposition H.1. We rewrite it as,\n\u2200\u03b1 \u2208 Ni, 2\u03b4\u03b1i (xi) + \u2211\n\u03b2\u2208Ni\\\u03b1\n\u03b4\u03b2i (xi) = log w\u03b1\\i\u2211 x\u03b1\\i exp [ \u03b8\u03b1(x\u03b1)\u2212 \u2211 j\u2208\u03b1\\i \u03b4\u03b1j (xj) ] . (30)\nNote, for \u2200\u03b1, \u03b2 \u2208 Ni, there is a linear relationship between \u03b4\u03b1i (xi) and \u03b4\u03b2i (xi).\nWe denote column vector \u03b3i(xi) filled \u03b1-th element with\n\u03b3\u03b1i (xi) = log w\u03b1\\i\u2211 x\u03b1\\i exp [ \u03b8\u03b1(x\u03b1)\u2212 \u2211 j\u2208\u03b1\\i \u03b4\u03b1j (xj) ] .\nWe also frame all {\u03b4\u03b1i (xi) | \u03b1 \u2208 Ni} into a column vector \u03b4Ni(xi), and denote |Ni| \u00d7 |Ni| matrix A\nA =  2 1 \u00b7 \u00b7 \u00b7 1 1 2 \u00b7 \u00b7 \u00b7 1 ... ... . . .\n... 1 1 \u00b7 \u00b7 \u00b7 2\n , and note A\u22121 =  |Ni| |Ni|+1 \u2212 1|Ni|+1 \u00b7 \u00b7 \u00b7 \u2212 1 |Ni|+1 \u2212 1|Ni|+1 |Ni| |Ni|+1 \u00b7 \u00b7 \u00b7 \u2212 1|Ni|+1 ... ... . . .\n... \u2212 1|Ni|+1 \u2212 1 |Ni|+1 \u00b7 \u00b7 \u00b7 |Ni||Ni|+1 .  It is easy to verify A\u03b4Ni(xi) = \u03b3i(xi). from (30). Since A is invertable, one can solve\n\u03b4Ni(xi) = A \u22121\u03b3i(xi).\nThen, one can read out the closed-form update rule (28). The monotonicity holds directly by noticing that the update rule (28) are solutions which jointly satisfy equation (25)."}, {"heading": "I Derivations of Gradient", "text": "Proposition I.1. Given a weight vector w\u03b1 = [w\u03b11 , \u00b7 \u00b7 \u00b7 , w\u03b1i , \u00b7 \u00b7 \u00b7 , w\u03b1c ] associated with variables x\u03b1 = {x1, \u00b7 \u00b7 \u00b7 , xi, \u00b7 \u00b7 \u00b7 , xc} on clique \u03b1, where c = |\u03b1| the power sum over clique \u03b1 is,\n\u03a6w\u03b1(\u03b4) = log w\u03b1\u2211 x\u03b1 exp [ \u03b8\u03b1(x\u03b1)\u2212 \u2211 i\u2208\u03b1 \u03b4\u03b1i (xi) ] = log w\u03b1c\u2211 xc \u00b7 \u00b7 \u00b7 w\u03b1i\u2211 xi \u00b7 \u00b7 \u00b7 w\u03b11\u2211 x1 exp [ \u03b8\u03b1(x\u03b1)\u2212 \u2211 i\u2208\u03b1 \u03b4\u03b1i (xi) ] .\nWe recursively denote Zi as the partial power sum up to x1:i,\nZ0(x\u03b1) = exp [ \u03b8\u03b1(x\u03b1)\u2212 \u2211 i\u2208\u03b1 \u03b4\u03b1i (xi) ] and Zi(xi+1:c) = w\u03b1i\u2211 xi Zi\u22121(xi:c), (31)\nthus logZc = \u03a6w\u03b1 . We also denote the \u201cpseudo marginal\u201d (or, belief) on x\u03b1,\n\u00b5\u03b1(x\u03b1) = c\u220f i=1 \u00b5\u03b1(xi|xi+1:c); \u00b5\u03b1(xi|xi+1:c) = (Zi\u22121(xi:c) Zi(xi+1:c) )1/w\u03b1i ,\nand it is easy to verify that \u00b5\u03b1(xi|xi+1:c) and \u00b5\u03b1(x\u03b1) are normalized.\nThen, the derivative of \u03a6w\u03b1 w.r.t. \u03b4\u03b1i (xi) can be written by beliefs,\n\u2202\u03a6w\u03b1\n\u2202\u03b4\u03b1i (xi) = \u2212\u00b5\u03b1(xi) = \u2212 \u2211 x\u03b1\\i \u00b5\u03b1(x\u03b1) = \u2212 \u2211 xc \u00b7 \u00b7 \u00b7 \u2211 xi+1 c\u220f j=i \u00b5\u03b1(xj |xj+1:c) (32)\nIn addition, the derivative of \u03a6w\u03b1 w.r.t. w\u03b1i is the conditional entropy, \u2202\u03a6w\u03b1\n\u2202w\u03b1i = H(xi|xi+1:c;\u00b5\u03b1(x\u03b1)) = \u2212 \u2211 x\u03b1 \u00b5\u03b1(x\u03b1) log\u00b5\u03b1(xi|xi+1:c) (33)\nProof. Denote the reparameterization on clique \u03b1 as \u03b8\u0303\u03b1(x\u03b1) = \u03b8\u03b1(x\u03b1)\u2212 \u2211 i\u2208\u03b1 \u03b4 \u03b1 i (xi). From the recursive definition of Zi(xi+1:c) (31), we have the following recursive rule for gradient, \u2202 logZi(xi+1:c)\n\u2202\u03b8\u0303\u03b1(x\u03b1) =\n\u2202\n\u2202\u03b8\u0303\u03b1(x\u03b1)\n( w\u03b1i log \u2211 xi [ Zi\u22121(xi:c) ]1/w\u03b1i )\n= w\u03b1i \u00b7 1 w\u03b1i \u00b7 Zi\u22121(xi:c) 1 w\u03b1 i\u2211\nxi\n[ Zi\u22121(xi:c) ] 1 w\u03b1c \u00b7 Zi\u22121(xi:c)\u22121 \u00b7 \u2202Zi\u22121(xi:c) \u2202\u03b8\u0303\u03b1(x\u03b1)\n= \u00b5\u03b1(xi|xi+1:c) \u00b7 \u2202 logZi\u22121(xi:c)\n\u2202\u03b8\u0303\u03b1(x\u03b1) . (34)\nIt should be noted, implicitly, xi+1:c within \u03b8\u0303\u03b1(x\u03b1) should take the same value as xi+1:c in logZi(xi+1:c), otherwise, the derivative will equal 0.\nAs a result, we can calculate the derivatives of \u03a6w\u03b1(\u03b8\u0303\u03b1) w.r.t. \u03b8\u0303\u03b1(x\u03b1) recursively as,\n\u2202\u03a6w\u03b1\n\u2202\u03b8\u0303\u03b1(x\u03b1) =\n\u2202 logZc \u2202\u03b8\u0303\u03b1(x\u03b1) = \u00b5\u03b1(xc) \u00b7 \u2202 logZc\u22121(xc) \u2202\u03b8\u0303\u03b1(x\u03b1) = \u00b7 \u00b7 \u00b7 = c\u220f i=1 \u00b5\u03b1(xi|xi+1:c) = \u00b5\u03b1(x\u03b1). (35)\nBy the chain rule,\n\u2202\u03a6w\u03b1 \u2202\u03b4\u03b1i (xi) = \u2211 x\u03b1\\i\n\u2202\u03a6w\u03b1 \u2202\u03b8\u0303\u03b1(xi, x\u03b1\\i) \u00b7 \u2202\u03b8\u0303\u03b1(xi, x\u03b1\\i) \u2202\u03b4\u03b1i (xi) = \u2212 \u2211 x\u03b1\\i \u00b5\u03b1(x\u03b1),\nthen (32) has been proved.\nApplying the variational form of powered-sum (15) to \u03a6w\u03b1 , we have\n\u03a6w\u03b1(\u03b8\u0303\u03b1) = max b\u03b1\u2208M\u03b1(G)\n{ \u3008\u03b8\u0303\u03b1, b\u03b1\u3009+ \u2211 i w\u03b1i H(xi|xi+1:n; b\u03b1) } .\nAccording to Danskin\u2019s theorem, the derivative \u2202\u03a6w\u03b1 \u2202\u03b8\u0303\u03b1(x\u03b1) = b\u2217\u03b1(x\u03b1), which is the optimum of RHS. Combined with (35), we have b\u2217\u03b1 = \u00b5\u03b1 immediately, and the derivative w.r.t. w\u03b1i is,\n\u2202\u03a6w\u03b1\n\u2202w\u03b1i = H(xi|xi+1:c;\u00b5\u03b1(x\u03b1)),\nthen (33) has been proved."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Marginal MAP inference involves making MAP predictions in systems defined<lb>with latent variables or missing information. It is significantly more difficult than<lb>pure marginalization and MAP tasks, for which a large class of efficient and con-<lb>vergent variational algorithms, such as dual decomposition, exist. In this work, we<lb>generalize dual decomposition to a generic power sum inference task, which in-<lb>cludes marginal MAP, along with pure marginalization and MAP, as special cases.<lb>Our method is based on a block coordinate descent algorithm on a new convex<lb>decomposition bound, that is guaranteed to converge monotonically, and can be<lb>parallelized efficiently. We demonstrate our approach on marginal MAP queries<lb>defined on real-world problems from the UAI approximate inference challenge,<lb>showing that our framework is faster and more reliable than previous methods.", "creator": "LaTeX with hyperref package"}}}