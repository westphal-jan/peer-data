{"id": "1511.03766", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2015", "title": "Sparse Learning for Large-scale and High-dimensional Data: A Randomized Convex-concave Optimization Approach", "abstract": "In this paper, we develop a randomized algorithm and theory for learning a sparse model from large-scale and high-dimensional data, which is usually formulated as an empirical risk minimization problem with a sparsity-inducing regularizer. Under the assumption that there exists a (approximately) sparse solution with high classification accuracy, we argue that the dual solution is also sparse or approximately sparse. The fact that both primal and dual solutions are sparse motivates us to develop a randomized approach for a general convex-concave optimization problem.\n\n\n\nThe following results represent the first randomized randomized trial of a multivariable linear model (i.e., a) using the following equation:\nA n = 3.\nGiven that the same single parameter is available (i.e., a) for all training, the results show that all training is sparse:\nA n = 2.\nFor a model that contains only four trials (i.e., a), we use a linear model (i.e., a) that contains only four trials (i.e., a) for all training. For a model that is a subset of trials (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training (i.e., a) for all training", "histories": [["v1", "Thu, 12 Nov 2015 03:11:48 GMT  (21kb)", "http://arxiv.org/abs/1511.03766v1", null], ["v2", "Sun, 16 Oct 2016 14:36:01 GMT  (24kb)", "http://arxiv.org/abs/1511.03766v2", "Proceedings of the 27th International Conference on Algorithmic Learning Theory (ALT 2016)"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lijun zhang", "tianbao yang", "rong jin", "zhi-hua zhou"], "accepted": false, "id": "1511.03766"}, "pdf": {"name": "1511.03766.pdf", "metadata": {"source": "CRF", "title": "Sparse Learning for Large-scale and High-dimensional Data: A Randomized Convex-concave Optimization Approach", "authors": ["Lijun Zhang", "Tianbao Yang", "Rong Jin", "Zhi-Hua Zhou"], "emails": ["zhanglj@lamda.nju.edu.cn", "tianbao-yang@uiowa.edu", "rongjin@cse.msu.edu", "zhouzh@lamda.nju.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n03 76\n6v 1\n[ cs\nKeywords: random projection, sparse learning, convex-concave optimization, primal solution, dual solution"}, {"heading": "1. Introduction", "text": "Learning the sparse representation of a predictive model has received considerable attention in recent years (Bach et al., 2012). Given a set of training examples {(xi,yi)}ni=1 with xi \u2208 Rd and yi \u2208 R, the optimization problem is generally formulated as\nmin w\u2208\u2126\n1\nn\nn\u2211\ni=1\n\u2113(yix \u22a4 i w) + \u03b3\u03c8(w) (1)\nwhere \u2113(\u00b7) is a convex function such as the logistic loss to measure the empirical error, and \u03c8(\u00b7) is a sparsity-inducing regularizer such as the elastic net (Zou and Hastie, 2005) to\navoid overfitting (Hastie et al., 2009). When both d and n are very large, directly solving (1) could be computational expensive. To address this challenge, a straightforward way is to first reduce the dimensionality of the data, then solve a low-dimensional problem, and finally map the solution back to the original space. The limitation of this approach is that the final solution, after mapping from the low-dimensional space to the original high-dimensional space, may not be sparse.\nThe goal of this paper is to develop an efficient algorithm for solving the problem in (1), and at the same time preserve the (approximate) sparsity of the solution. Our approach is motivated by the following simple observation:\nIf there exists a sparse model with high prediction accuracy, the dual solution to (1) is also sparse or at least approximately sparse.\nTo see this, let us formulate (1) as a convex-concave optimization problem. By writing \u2113(z) in its convex conjugate form, i.e.,\n\u2113(z) = max \u03bb\u2208\u0393\n\u03bbz \u2212 \u2113\u2217(\u03bb),\nwhere \u2113\u2217(\u00b7) is the Fenchel conjugate of \u2113(\u00b7) (Rockafellar, 1997) and \u0393 is the domain of the dual variable, we get the following convex-concave formulation:\nmax \u03bb\u2208\u0393n min w\u2208\u2126\n\u03b3n\u03c8(w)\u2212 n\u2211\ni=1\n\u2113\u2217(\u03bbi) + n\u2211\ni=1\n\u03bbiyix \u22a4 i w. (2)\nDenote the optimal solutions to (2) by (w\u2217,\u03bb\u2217). By the Fenchel conjugate theory (CesaBianchi and Lugosi, 2006, Lemma 11.4), we have\n[\u03bb\u2217]i = \u2113 \u2032(yix \u22a4 i w\u2217).\nLet us consider the squared hinge loss for classification, where \u2113(z) = max(0, 1\u2212z)2. Therefore, yix \u22a4 i w\u2217 \u2265 1 indicates that [\u03bb\u2217]i = 0. As a result, when most of the examples can be classified by a large margin (which is likely to occur in large-scale and high-dimensional setting), it is reasonable to assume that the dual solution is sparse. Similarly, for logistic regression, we can argue the dual solution is approximately sparse.\nAbstracting (2) slightly, in the following, we will study a general convex-concave optimization problem:\nmax \u03bb\u2208\u2206 min w\u2208\u2126\ng(w) \u2212 h(\u03bb)\u2212w\u22a4A\u03bb (3)\nwhere \u2206 \u2286 Rn and \u2126 \u2286 Rd are the domains for \u03bb and w, respectively, g(\u00b7) and h(\u00b7) are two convex functions, and A \u2208 Rd\u00d7n is a matrix. The benefit of analyzing (3) instead of (1) is that the convex-concave formulation allows us to exploit the prior knowledge that both w\u2217 and \u03bb\u2217 are sparse or approximately sparse. The problem in (3) has been widely studied in the optimization community, and when n and d are medium size, it can be solved iteratively by gradient based methods (Nesterov, 2005; Nemirovski, 2005).\nWe assume the two convex functions g(\u00b7) and h(\u00b7) are relatively simple such that evaluating their values or gradients take O(d) and O(n) complexities, respectively. The bottleneck is the computations involving the bilinear term w\u22a4A\u03bb, which have O(nd) complexity in\nboth time and space. To overcome this difficulty, we develop a randomized algorithm which solves (3) approximately but at a significantly low cost. The proposed algorithm combines two well-known techniques\u2014random projection and \u21131-norm regularization in a principled way. Specifically, random projection is used to find a low-rank approximation of A, which not only reduces the storage requirement but also accelerates the computations. The role of \u21131-norm regularization is twofold. One one hand, it is introduced to compensate for the distortion caused by randomization, and on the other hand it enforces the sparsity of the final solutions. Under mild assumptions about the optimization problem in (3), the proposed algorithm has a small recovery error provided the optimal solutions to (3) are sparse or approximately sparse.\nNotations For a vector x \u2208 Rd and a set D \u2286 [d], we denote by xD the vector which coincides with x on D and has zero coordinates outside D."}, {"heading": "2. Related Work", "text": "Random projection has been widely used as an efficient algorithm for dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001). In the case of unsupervised learning, it has been proved that random projection is able to preserve the distance (Dasgupta and Gupta, 2003), inner product (Arriaga and Vempala, 2006), volumes and distance to affine spaces (Magen, 2002). In the case of supervised learning, random projection is generally used as a pre-processing step to find a low-dimensional representation of the data, and thus reduce the computational cost of training. For classification, theoretical studies mainly focus on examining the generalization error or the preservation of classification margin in the low-dimensional space (Balcan et al., 2006; Shi et al., 2012; Paul et al., 2013). For regression, there do exist theoretical guarantees for the recovery error, but they only hold for the least squares problem (Mahoney, 2011).\nOur work is closely related to Dual Random Projection (DRP) (Zhang et al., 2013) and Dual-sparse Regularized Randomized Reduction (DSRR) (Yang et al., 2015), which also investigate random projection from the perspective of optimization. However, both DRP and DSRR are limited to the special case that \u03c8(w) = \u2016w\u201622, which leads to a simple dual problem and can be analyzed easily. In contrast, our algorithm is designed for the case that \u03c8(\u00b7) is a sparsity-inducing regularizer, and built upon the convex-concave formulation. Similar to DSRR, our algorithm makes use of the sparsity of the dual solution, but we further exploit the sparsity of the primal solution. A noticeable advantage of our analysis is the mild assumption about the data matrix A. In order to recover the primal solution, DRP assumes the data matrix is low-rank and DSRR assumes it satisfies the restricted eigenvalue condition, on the other hand, our algorithm only requires columns or rows of A are well-bounded."}, {"heading": "3. Algorithm", "text": "To reduce the computational cost of (3), we first generate a random matrix R \u2208 Rn\u00d7m, where m \u226a min(d, n). Define A\u0302 = AR \u2208 Rd\u00d7m, we propose to solve the following problem\nmax \u03bb\u2208\u2206 min w\u2208\u2126\ng(w)\u2212 h(\u03bb)\u2212w\u22a4A\u0302R\u22a4\u03bb+ \u03b3w\u2016w\u20161 \u2212 \u03b3\u03bb\u2016\u03bb\u20161 (4)\nwhere \u03b3w and \u03b3\u03bb are two regularization parameters. The construction of the random matrix R, as well as the values of the two regularization parameters \u03b3w and \u03b3\u03bb will be discussed later. The optimization problem in (4) can be solved by the algorithm designed for composite convex-concave problems (He and Monteiro, 2014).\nCompared to (3), the main advantage of (4) is that it only needs to load A\u0302 and R into the memory, making it convenient to deal with large-scale problems. With the help of random projection, the computational complexity for evaluating the value and gradient is reduced from O(dn) to O(dm+ nm). Compared to previous randomized algorithms (Balcan et al., 2006; Zhang et al., 2013; Yang et al., 2015), (4) has two new features: i) the optimization is still performed in the original space; and ii) the \u21131-norm is introduced to regularize both primal and dual solutions. As we will prove later, the combination of these two features will ensure the solutions to (4) are approximately sparse. Finally, note that in (4) RR\u22a4 is inserted at the right side of A, it can also be put at the left side of A.\nLet (w\u2217,\u03bb\u2217) and (w\u0302, \u03bb\u0302) be the optimal solution to the convex-concave optimization problem in (3) and (4), respectively. Under suitable conditions, we will show that\n\u2016w\u0302 \u2212w\u2217\u20162 \u2264 O (\u221a\n\u2016w\u2217\u20160\u2016\u03bb\u2217\u20160 log n m\n) and \u2016\u03bb\u0302\u2212 \u03bb\u2217\u20162 \u2264 O (\u221a \u2016w\u2217\u20160\u2016\u03bb\u2217\u20160 log d\nm\n)\nimplying a small recovery error when w\u2217 and \u03bb\u2217 are sparse. A similar recovery guarantee also holds when the optimal solutions to (3) are approximately sparse, i.e., when they can be well-approximated by sparse vectors."}, {"heading": "4. Main Results", "text": "We first introduce assumptions that we make, and then present the theoretical guarantees of (4)."}, {"heading": "4.1 Assumptions", "text": "Assumptions about (3) We make the following assumptions about (3).\n\u2022 g(w) is \u03b1-strongly convex with respect to the Euclidean norm. Let\u2019s take the optimization problem in (2) as an example. (2) will satisfy this assumption if some strongly convex function (e.g., \u2016w\u201622) is a part of the regularizer \u03c8(w).\n\u2022 h(\u03bb) is \u03b2-strongly convex with respect to the Euclidean norm. For the problem in (2), if \u2113(\u00b7) is a smooth function (e.g., the logistic loss), then its convex conjugate \u2113\u2217(\u00b7) will be strongly convex (Rockafellar, 1997; Kakade et al., 2009).\n\u2022 Either columns or rows of A have bounded \u21132-norm. Without loss of generality, we assume\n\u2016Ai\u2217\u20162 \u2264 1, \u2200i \u2208 [d], (5) \u2016A\u2217j\u20162 \u2264 1, \u2200j \u2208 [n]. (6)\nThe above assumption can be satisfied by normalizing rows or columns of A. In a non-essential theorem (Theorem 6), which is included to enrich this study, we need a stronger assumption that both columns and rows of A are bounded.\nAssumptions about R We assume the random matrix R \u2208 Rn\u00d7m has the following property.\n\u2022 With a high probability, the linear operator R\u22a4 : Rn 7\u2192 Rm is able to preserve the \u21132-norm of its input. In mathematical terms, we need the following theorem.\nTheorem 1 There exists a constant c > 0, such that\nPr { (1\u2212 \u03b5)\u2016x\u201622 \u2264 \u2016R\u22a4x\u201622 \u2264 (1 + \u03b5)\u2016x\u201622 } \u2265 1\u2212 2 exp(\u2212m\u03b52/c)\nfor any fixed x \u2208 Rd and 0 < \u01eb \u2264 1/2.\nThe above theorem is widely used to prove the famous Johnson\u2013Lindenstrauss lemma (Dasgupta and Gupta, 2003). Let R = 1\u221a\nm S. Previous studies (Achlioptas, 2003; Arriaga and\nVempala, 2006) have proved that Theorem 1 is true if {Sij} are independent random variables sampled from the Gaussian distribution N (0, 1), uniform distribution over {\u00b11}, or the following database-friendly distribution\nX =    \u221a 3, with probability 1/6; 0, with probability 2/3;\n\u2212 \u221a 3, with probability 1/6.\nMore generally, a sufficient condition for Theorem 1 is that columns of R are independent, isotropic, and subgaussian vectors (Mendelson et al., 2008)."}, {"heading": "4.2 Theoretical Guarantees", "text": ""}, {"heading": "4.2.1 Sparse Solutions", "text": "We first consider the case that both w\u2217 and \u03bb\u2217 are sparse. Define\nrw = \u2016w\u2217\u20160, and r\u03bb = \u2016\u03bb\u2217\u20160.\nWe have the following theorem.\nTheorem 2 Set\n\u03b3\u03bb \u2265 2\u2016A\u22a4w\u2217\u20162 \u221a c\nm log\n4n\n\u03b4 , (7)\n\u03b3w \u2265 2\u2016\u03bb\u2217\u20162 \u221a c\nm log\n4d\n\u03b4 +\n6\u03b3\u03bb \u221a r\u03bb\n\u03b2\n( 1 + 7 \u221a c\nm\n( log 4d\n\u03b4 + 16r\u03bb log\n9n\n8r\u03bb\n)) . (8)\nWith a probability at least 1\u2212 3\u03b4, we have\n\u2016w\u0302 \u2212w\u2217\u20162 \u2264 3\u03b3w\n\u221a rw\n\u03b1 , \u2016w\u0302 \u2212w\u2217\u20161 \u2264 12\u03b3wrw \u03b1 , and \u2016w\u0302 \u2212w\u2217\u20161 \u2016w\u0302 \u2212w\u2217\u20162 \u2264 4\u221arw\nprovided\nm \u2265 4c log 4 \u03b4 . (9)\nNotice that \u2016w\u0302 \u2212 w\u2217\u20161/\u2016w\u0302 \u2212 w\u2217\u20162 \u2264 4 \u221a rw indicates that w\u0302 \u2212 w\u2217 is approximately sparse (Plan and Vershynin, 2013a,b). Combining with the fact w\u2217 is sparse, we conclude that w\u0302 is also approximately sparse.\nThen, we discuss the recovery guarantee for the sparse learning problem in (1) or (2). Generally speaking, the domain \u2126 \u2286 Rd of the primal variable belongs to certain \u21132-norm ball, thus we can take \u2016w\u2217\u20162 = O(1). So, all the elements of A\u22a4w\u2217 \u2208 Rn are bounded, which implies \u2016A\u22a4w\u2217\u20162 = O( \u221a n). Since the domain \u0393 \u2286 R of the dual variable is always bounded and \u2016\u03bb\u2217\u20160 = r\u03bb, we can take \u2016\u03bb\u2217\u20162 = O( \u221a r\u03bb). According to the theoretical analysis of regularized empirical risk minimization (Wu and Zhou, 2005; Sridharan et al., 2009; Koltchinskii, 2011), the optimal \u03b3, that minimizes the generalization error, can be chosen as \u03b3 = O(1/ \u221a n), and thus \u03b1 = O(\u03b3n) = O( \u221a n). When the loss \u2113(\u00b7) is smooth, we have \u03b2 = O(1). The following corollary provides a simplified result based on the above discussions.\nCorollary 1 Assume \u2016A\u22a4w\u2217\u20162 = O( \u221a n), \u2016\u03bb\u2217\u20162 = O( \u221a r\u03bb), \u03b1 = O( \u221a n), and \u03b2 = O(1). We can choose\n\u03b3\u03bb = O\n(\u221a n log n\nm\n) and \u03b3w = O (\u221a r\u03bb log d\nm + \u03b3\u03bb\n\u221a r\u03bb ) = O (\u221a nr\u03bb log n\nm\n)\nsuch that with a high probability\n\u2016w\u0302 \u2212w\u2217\u20162 = O ( \u03b3w \u221a rw\u221a n ) = O (\u221a rwr\u03bb log n m ) and \u2016w\u0302 \u2212w\u2217\u20161 \u2016w\u0302 \u2212w\u2217\u20162 \u2264 4\u221arw.\nA natural question to ask is whether similar recovery guarantees for \u03bb\u0302 can be proved under the conditions in Theorem 2. Unfortunately, we are not able to give a positive answer, and only have the following theorem.\nTheorem 3 Assume \u03b3\u03bb satisfies the condition in (7). With a probability at least 1\u2212 \u03b4, we have\n\u2016\u03bb\u0302\u2212 \u03bb\u2217\u20162 \u2264 3\u03b3\u03bb\n\u221a r\u03bb \u03b2 + 2 \u03b2 ( 1 + \u2016RR\u22a4 \u2212 I\u20162 ) \u2016A\u22a4(w\u0302 \u2212w\u2217)\u20162\nprovided (9) holds.\nThe upper bound in the above theorem is quite loose, because \u2016RR\u22a4 \u2212 I\u20162 is roughly on the order of n log n/m (Tropp, 2012).\nTo obtain a tight bound for \u03bb\u0302, we can solve a different problem given by\nmax \u03bb\u2208\u2206 min w\u2208\u2126\ng(w) \u2212 h(\u03bb)\u2212w\u22a4RA\u0302\u03bb+ \u03b3w\u2016w\u20161 \u2212 \u03b3\u03bb\u2016\u03bb\u20161 (10)\nwhere R \u2208 Rd\u00d7m is a random matrix, and A\u0302 = R\u22a4A \u2208 Rm\u00d7n. Due to the symmetry between \u03bb and w, we directly have a theoretical guarantee for \u03bb\u0302 by replacing w\u2217 in Theorem 2 with \u03bb\u2217, w\u0302 with \u03bb\u0302, n with d, and so on. To be specific, we have the following theorem.\nTheorem 4 Set\n\u03b3w \u2265 2\u2016A\u03bb\u2217\u20162 \u221a c\nm log\n4d\n\u03b4 ,\n\u03b3\u03bb \u2265 2\u2016w\u2217\u20162 \u221a c\nm log\n4n\n\u03b4 +\n6\u03b3w \u221a rw\n\u03b1\n( 1 + 7 \u221a c\nm\n( log 4n\n\u03b4 + 16rw log\n9d\n8rw\n)) .\nWith a probability at least 1\u2212 3\u03b4, we have\n\u2016\u03bb\u0302\u2212 \u03bb\u2217\u20162 \u2264 3\u03b3\u03bb\n\u221a r\u03bb \u03b2 , \u2016\u03bb\u0302\u2212 \u03bb\u2217\u20161 \u2264 12\u03b3\u03bbr\u03bb \u03b2 , and \u2016\u03bb\u0302\u2212 \u03bb\u2217\u20161 \u2016\u03bb\u0302\u2212 \u03bb\u2217\u20162 \u2264 4\u221ar\u03bb\nprovided (9) holds.\nTo simplify the above theorem, we need to estimate the order of \u2016A\u03bb\u2217\u20162. Since \u2016\u03bb\u2217\u20160 = r\u03bb, A\u03bb\u2217 is a linear combination of r\u03bb columns of A. Under the condition that columns of A are well-bounded, \u2016A\u03bb\u2217\u20162 = O( \u221a r\u03bb) if the selected columns of A are orthogonal to each\nother. Since r\u03bb \u226a n, we take a relaxed condition \u2016A\u03bb\u2217\u20162 = O( \u221a n). 1 Finally, we have the following corollary.\nCorollary 2 Assume \u2016A\u03bb\u2217\u20162 = O( \u221a n), \u2016w\u2217\u20162 = O(1), \u03b1 = O( \u221a n), and \u03b2 = O(1). We can choose\n\u03b3w = O\n(\u221a n log d\nm\n) and \u03b3\u03bb = O (\u221a log n\nm + \u03b3w \u221a rw n ) = O (\u221a rw log d m )\nsuch that with a high probability\n\u2016\u03bb\u0302\u2212 \u03bb\u2217\u20162 = O (\u03b3\u03bb \u221a r\u03bb) = O\n(\u221a rwr\u03bb log d\nm\n) and\n\u2016\u03bb\u0302\u2212 \u03bb\u2217\u20161 \u2016\u03bb\u0302\u2212 \u03bb\u2217\u20162 \u2264 4\u221ar\u03bb."}, {"heading": "4.2.2 Approximately Sparse Solutions", "text": "We now proceed to study the case that the optimal solutions to (3) are only approximately sparse. With a slight abuse of notation, we assume w\u2217 and \u03bb\u2217 are two sparse vectors, with \u2016w\u2217\u20160 = rw and \u2016\u03bb\u2217\u20160 = r\u03bb, that solve (3) approximately in the sense that\n\u2016\u2207g(w\u2217)\u2212A\u03bb\u2217\u2016\u221e \u2264 \u03c2, (11) \u2016\u2207h(\u03bb\u2217) +A\u22a4w\u2217\u2016\u221e \u2264 \u03c2, (12)\nfor some small constant \u03c2 > 0. The above conditions can be considered as sub-optimality conditions (Boyd and Vandenberghe, 2004) of w\u2217 and \u03bb\u2217 measured in the \u2113\u221e-norm. After a similar analysis, we get the following theorem.\n1. Even the selected columns of A are linearly dependent, \u2016A\u03bb\u2217\u20162 cannot be larger than O(r\u03bb). Then, \u2016A\u03bb\u2217\u20162 = O( \u221a n) always true if r\u03bb \u2264 O( \u221a n).\nTheorem 5 Assume (11) and (12) hold. Set\n\u03b3\u03bb \u2265 2\u2016A\u22a4w\u2217\u20162 \u221a c\nm log\n4n\n\u03b4 + 2\u03c2,\n\u03b3w \u2265 2\u2016\u03bb\u2217\u20162 \u221a c\nm log\n4d\n\u03b4 +\n6\u03b3\u03bb \u221a r\u03bb\n\u03b2\n( 1 + 7 \u221a c\nm\n( log 4d\n\u03b4 + 16r\u03bb log\n9n\n8r\u03bb\n)) + 2\u03c2.\nWith a probability at least 1\u2212 3\u03b4, we have\n\u2016w\u0302 \u2212w\u2217\u20162 \u2264 3\u03b3w\n\u221a rw\n\u03b1 , \u2016w\u0302 \u2212w\u2217\u20161 \u2264 12\u03b3wrw \u03b1 , and \u2016w\u0302 \u2212w\u2217\u20161 \u2016w\u0302 \u2212w\u2217\u20162 \u2264 4\u221arw\nprovided (9) holds.\nWhen \u03c2 is small enough, the upper bound in Theorem 6 is on the same order as that in Theorem 2. To be specific, we have the following corollary. Corollary 3 Assume \u2016A\u22a4w\u2217\u20162 = O( \u221a n), \u2016\u03bb\u2217\u20162 = O( \u221a r\u03bb), \u03b1 = O( \u221a n), \u03b2 = O(1), and\n\u03c2 = O\n(\u221a n log n\nm\n) .\nWe can choose \u03b3\u03bb and \u03b3w as in Corollary 1 such that with a high probability\n\u2016w\u0302 \u2212w\u2217\u20162 = O ( \u03b3w \u221a rw\u221a n ) = O (\u221a rwr\u03bb log n m ) and \u2016w\u0302 \u2212w\u2217\u20161 \u2016w\u0302 \u2212w\u2217\u20162 \u2264 4\u221arw.\nWhen the optimal solutions to (3) are allowed to be approximately sparse, g(\u00b7) could be certain smooth regularizer, such as a mixture of \u2016 \u00b7 \u201622 and \u2016 \u00b7 \u2016pp for 1 < p < 2. In the following, we provide a supporting theorem for this special case. we denote by w\u2032\u2217 and \u03bb \u2032 \u2217 the optimal solutions to (3). To quantify the approximate sparsity of w\u2032\u2217 and \u03bb \u2032 \u2217, we assume there exist two sparse vectors w\u2217 and \u03bb\u2217, with \u2016w\u2217\u20160 = rw and \u2016\u03bb\u2217\u20160 = r\u03bb such that\n\u2016w\u2217 \u2212w\u2032\u2217\u20162 \u2264 \u03c4 and \u2016\u03bb\u2217 \u2212 \u03bb\u2032\u2217\u20162 \u2264 \u03c4 (13)\nfor some small constant \u03c4 > 0. Furthermore, we assume both g and h are \u00b5-smooth, i.e.,\n\u2016\u2207g(w1)\u2212\u2207g(w2)\u20162 \u2264 \u00b5\u2016w1 \u2212w2\u2016, \u2200w1,w2, (14) \u2016\u2207h(\u03bb1)\u2212\u2207h(\u03bb2)\u20162 \u2264 \u00b5\u2016\u03bb1 \u2212 \u03bb2\u2016, \u2200\u03bb1,\u03bb2. (15)\nTheorem 6 Assume (13), (14), and (15) hold. Suppose w\u2032\u2217 and \u03bb \u2032 \u2217 lie in the interior of \u2126 and \u2206, respectively. Set\n\u03b3\u03bb \u2265 2\u2016A\u22a4w\u2217\u20162 \u221a c\nm log\n4n\n\u03b4 + 2(1 + \u00b5)\u03c4,\n\u03b3w \u2265 2\u2016\u03bb\u2217\u20162 \u221a c\nm log\n4d\n\u03b4 +\n6\u03b3\u03bb \u221a r\u03bb\n\u03b2\n( 1 + 7 \u221a c\nm\n( log 4d\n\u03b4 + 16r\u03bb log\n9n\n8r\u03bb\n)) + 2(1 + \u00b5)\u03c4.\nWith a probability at least 1\u2212 3\u03b4, we have\n\u2016w\u0302 \u2212w\u2217\u20162 \u2264 3\u03b3w\n\u221a rw\n\u03b1 , \u2016w\u0302 \u2212w\u2217\u20161 \u2264 12\u03b3wrw \u03b1 , and \u2016w\u0302 \u2212w\u2217\u20161 \u2016w\u0302 \u2212w\u2217\u20162 \u2264 4\u221arw\nprovided (9) holds. Similarly, if (1 + \u00b5)\u03c4 = O (\u221a n log n/m ) , the conclusion in Corollary 1 also holds here."}, {"heading": "5. Analysis", "text": "In this section, we provide proofs of main theorems, and others can be found in the appendix."}, {"heading": "5.1 Proof of Theorem 2", "text": "To facilitate the analysis, we introduce a pseudo optimization problem\nmax \u03bb\u2208\u2206\n\u2212h(\u03bb)\u2212w\u22a4\u2217 A\u0302R\u22a4\u03bb\u2212 \u03b3\u03bb\u2016\u03bb\u20161\nwhose optimal solution is denoted by \u03bb\u0303. In the following, we will first discuss how to bound the difference between \u03bb\u0303 and \u03bb\u2217, and then bound the difference between w\u0302 and w\u2217 in a similar way.\nFrom the optimality of \u03bb\u0303 and \u03bb\u2217, we derive the following lemma to bound their difference.\nLemma 1 Denote \u03c1\u03bb = \u2225\u2225\u2225(RR\u22a4 \u2212 I)A\u22a4w\u2217 \u2225\u2225\u2225 \u221e . (16) By choosing \u03b3\u03bb \u2265 2\u03c1\u03bb, we have\n\u2016\u03bb\u0303\u2212 \u03bb\u2217\u20162 \u2264 3\u03b3\u03bb\n\u221a r\u03bb \u03b2 , \u2016\u03bb\u0303\u2212 \u03bb\u2217\u20161 \u2264 12\u03b3\u03bbr\u03bb \u03b2 , and \u2016\u03bb\u0303\u2212 \u03bb\u2217\u20161 \u2016\u03bb\u0303\u2212 \u03bb\u2217\u20162 \u2264 4\u221ar\u03bb.\nBased on the property of the random matrix R described in Theorem 1, we have the following lemma to bound \u03c1\u03bb in (16).\nLemma 2 With a probability at least 1\u2212 \u03b4, we have\n\u03c1\u03bb = \u2225\u2225\u2225(RR\u22a4 \u2212 I)A\u22a4w\u2217 \u2225\u2225\u2225 \u221e \u2264 \u2016A\u22a4w\u2217\u20162 \u221a c m log 4n \u03b4\nprovided (9) holds.\nCombining Lemma 1 with Lemma 2, we immediately obtain the following lemma.\nLemma 3 Set\n\u03b3\u03bb \u2265 2\u2016A\u22a4w\u2217\u20162 \u221a c\nm log\n4n\n\u03b4 .\nWith a probability at least 1\u2212 \u03b4, we have\n\u2016\u03bb\u0303\u2212 \u03bb\u2217\u20162 \u2264 3\u03b3\u03bb\n\u221a r\u03bb \u03b2 , \u2016\u03bb\u0303\u2212 \u03bb\u2217\u20161 \u2264 12\u03b3\u03bbr\u03bb \u03b2 , and \u2016\u03bb\u0303\u2212 \u03bb\u2217\u20161 \u2016\u03bb\u0303\u2212 \u03bb\u2217\u20162 \u2264 4\u221ar\u03bb\nprovided (9) holds.\nWe are now in a position to formulate the key lemmas that lead to Theorem 2. Similar to Lemma 1, we introduce the following lemma to characterize the relation between w\u0302 and w\u2217.\nLemma 4 Denote\n\u03c1w = \u2225\u2225\u2225A(I \u2212RR\u22a4)\u03bb\u2217 \u2225\u2225\u2225 \u221e + \u2225\u2225\u2225ARR\u22a4(\u03bb\u2217 \u2212 \u03bb\u0303) \u2225\u2225\u2225 \u221e . (17)\nBy choosing \u03b3w \u2265 2\u03c1w, we have\n\u2016w\u0302 \u2212w\u2217\u20162 \u2264 3\u03b3w\n\u221a rw\n\u03b1 , \u2016w\u0302 \u2212w\u2217\u20161 \u2264 12\u03b3wrw \u03b1 , and \u2016w\u0302 \u2212w\u2217\u20161 \u2016w\u0302 \u2212w\u2217\u20162 \u2264 4\u221arw.\nThe last step of the proof is to derive an upper bound for \u03c1w based on Theorem 1 and Lemma 3.\nLemma 5 Assume the conclusion in Lemma 3 happens. With a probability at least 1\u2212 2\u03b4, we have\n\u03c1w \u2264 \u2016\u03bb\u2217\u20162 \u221a c\nm log\n4d\n\u03b4 +\n3\u03b3\u03bb \u221a r\u03bb\n\u03b2\n( 1 + 7 \u221a c\nm\n( log 4d\n\u03b4 + 16r\u03bb log\n9n\n8r\u03bb\n))\nprovided (9) holds."}, {"heading": "5.2 Proof of Lemma 1", "text": "Let \u2126\u03bb include the subset of non-zeros entries in \u03bb\u2217 and \u2126\u0304\u03bb = [n] \\ \u2126\u03bb. Define\nL(\u03bb) = \u2212h(\u03bb) + min w\u2208\u2126 g(w)\u2212w\u22a4A\u03bb, L\u0303(\u03bb) = \u2212h(\u03bb)\u2212w\u22a4\u2217 A\u0302R\u22a4\u03bb\u2212 \u03b3\u03bb\u2016\u03bb\u20161.\nLet v \u2208 \u2202\u2016\u03bb\u2217\u20161 be any subgradient of \u2016 \u00b7 \u20161 at \u03bb\u2217. Then, we have\nu = \u2212\u2207h(\u03bb\u2217)\u2212RR\u22a4A\u22a4w\u2217 \u2212 \u03b3\u03bbv \u2208 \u2202L\u0303(\u03bb\u2217). 2\nUsing the fact that \u03bb\u0303 maximizes L\u0303(\u00b7) over the domain \u2206 and h(\u00b7) is \u03b2-strongly convex, we have\n0 \u2265 L\u0303(\u03bb\u2217)\u2212 L\u0303(\u03bb\u0303) \u2265 \u3008\u2212(\u03bb\u0303\u2212 \u03bb\u2217),u\u3009 + \u03b2\n2 \u2016\u03bb\u2217 \u2212 \u03bb\u0303\u201622\n= \u2329 \u03bb\u0303\u2212 \u03bb\u2217,\u2207h(\u03bb\u2217) +RR\u22a4A\u22a4w\u2217 + \u03b3\u03bbv \u232a + \u03b2\n2 \u2016\u03bb\u2217 \u2212 \u03bb\u0303\u201622.\n(18)\nBy setting vi = sign(\u03bb\u0303i), \u2200i \u2208 \u2126\u0304\u03bb, we have \u3008\u03bb\u0303\u2126\u0304\u03bb ,v\u2126\u0304\u03bb\u3009 = \u2016\u03bb\u0303\u2126\u0304\u03bb\u20161. As a result,\n\u3008\u03bb\u0303\u2212 \u03bb\u2217,v\u3009 = \u3008\u03bb\u0303\u2126\u0304\u03bb ,v\u2126\u0304\u03bb\u3009+ \u3008\u03bb\u0303\u2126\u03bb \u2212 \u03bb\u2217,v\u2126\u03bb\u3009 \u2265 \u2016\u03bb\u0303\u2126\u0304\u03bb\u20161 \u2212 \u2016\u03bb\u0303\u2126\u03bb \u2212 \u03bb\u2217\u20161. (19)\n2. In the case that h(\u00b7) is non-smooth, \u2207h(\u03bb\u2217) refers to a subgradient of h(\u00b7) at \u03bb\u2217. In particular, we choose the subgradient that satisfies (21).\nCombining (18) with (19), we have\n\u2329 \u03bb\u0303\u2212 \u03bb\u2217,\u2207h(\u03bb\u2217) +RR\u22a4A\u22a4w\u2217 \u232a + \u03b2\n2 \u2016\u03bb\u2217 \u2212 \u03bb\u0303\u201622 + \u03b3\u03bb\u2016\u03bb\u0303\u2126\u0304\u03bb\u20161 \u2264 \u03b3\u03bb\u2016\u03bb\u0303\u2126\u03bb \u2212 \u03bb\u2217\u20161. (20)\nFrom the fact that \u03bb\u2217 maximizes L(\u00b7) over the domain \u2206, we have \u3008\u2207L(\u03bb\u2217),\u03bb\u2212 \u03bb\u2217\u3009 = \u3008\u2212\u2207h(\u03bb\u2217)\u2212A\u22a4w\u2217,\u03bb\u2212 \u03bb\u2217\u3009 \u2264 0, \u2200\u03bb \u2208 \u2206. (21)\nThen, \u2329 \u03bb\u0303\u2212 \u03bb\u2217,\u2207h(\u03bb\u2217) +RR\u22a4A\u22a4w\u2217 \u232a\n= \u2329 \u03bb\u0303\u2212 \u03bb\u2217,\u2207h(\u03bb\u2217) +A\u22a4w\u2217 \u232a + \u2329 \u03bb\u0303\u2212 \u03bb\u2217, (RR\u22a4 \u2212 I)A\u22a4w\u2217 \u232a\n(21) \u2265 \u2212 \u2016\u03bb\u0303\u2212 \u03bb\u2217\u20161 \u2225\u2225\u2225(RR\u22a4 \u2212 I)A\u22a4w\u2217 \u2225\u2225\u2225 \u221e (16) = \u2212 \u03c1\u03bb\u2016\u03bb\u0303\u2212 \u03bb\u2217\u20161 = \u2212\u03c1\u03bb ( \u2016\u03bb\u0303\u2126\u0304\u03bb\u20161 + \u2016\u03bb\u0303\u2126\u03bb \u2212 \u03bb\u2217\u20161 ) .\n(22)\nFrom (20) and (22), we have\n\u03b2 2 \u2016\u03bb\u0303\u2212 \u03bb\u2217\u201622 + (\u03b3\u03bb \u2212 \u03c1\u03bb)\u2016\u03bb\u0303\u2126\u0304\u03bb\u20161 \u2264 (\u03b3\u03bb + \u03c1\u03bb)\u2016\u03bb\u0303\u2126\u03bb \u2212 \u03bb\u2217\u20161.\nSince \u03b3\u03bb \u2265 2\u03c1\u03bb, we have \u03b2\n2 \u2016\u03bb\u0303\u2212 \u03bb\u2217\u201622 + \u03b3\u03bb 2 \u2016\u03bb\u0303\u2126\u0304\u03bb\u20161 \u2264 3\u03b3\u03bb 2 \u2016\u03bb\u0303\u2126\u03bb \u2212 \u03bb\u2217\u20161.\nAnd thus,\n\u03b2 2 \u2016\u03bb\u0303\u2212 \u03bb\u2217\u201622 \u2264 3\u03b3\u03bb 2 \u2016\u03bb\u0303\u2126\u03bb \u2212 \u03bb\u2217\u20161 \u2264 3\u03b3\u03bb\n\u221a r\u03bb 2 \u2016\u03bb\u0303\u2126\u03bb \u2212 \u03bb\u2217\u20162 \u21d2 \u2016\u03bb\u0303\u2212 \u03bb\u2217\u20162 \u2264 3\u03b3\u03bb \u221a r\u03bb \u03b2 ,\n\u03b2\n2r\u03bb \u2016\u03bb\u0303\u2126\u03bb \u2212 \u03bb\u2217\u201621 \u2264\n\u03b2 2 \u2016\u03bb\u0303\u2212 \u03bb\u2217\u201622 \u2264 3\u03b3\u03bb 2 \u2016\u03bb\u0303\u2126\u03bb \u2212 \u03bb\u2217\u20161 \u21d2 \u2016\u03bb\u0303\u2126\u03bb \u2212 \u03bb\u2217\u20161 \u2264 3\u03b3\u03bbr\u03bb \u03b2 ,\n\u03b3\u03bb 2 \u2016\u03bb\u0303\u2126\u0304\u03bb\u20161 \u2264 3\u03b3\u03bb 2 \u2016\u03bb\u0303\u2126\u03bb \u2212 \u03bb\u2217\u20161 \u21d2 \u2016\u03bb\u0303\u2126\u0304\u03bb\u20161 \u2264 3\u2016\u03bb\u0303\u2126\u03bb \u2212 \u03bb\u2217\u20161 \u21d2 \u2016\u03bb\u0303\u2212 \u03bb\u2217\u20161 \u2264 12\u03b3\u03bbr\u03bb \u03b2 ,\n\u2016\u03bb\u0303\u2212 \u03bb\u2217\u20161 \u2016\u03bb\u0303\u2212 \u03bb\u2217\u20162 \u2264 \u2016\u03bb\u0303\u2126\u03bb \u2212 \u03bb\u2217\u20161 + \u2016\u03bb\u0303\u2126\u0304\u03bb\u20161 \u2016\u03bb\u0303\u2212 \u03bb\u2217\u20162 \u2264 4\u2016\u03bb\u0303\u2126\u03bb \u2212 \u03bb\u2217\u20161 \u2016\u03bb\u0303\u2212 \u03bb\u2217\u20162 \u2264 4 \u221a r\u03bb\u2016\u03bb\u0303\u2126\u03bb \u2212 \u03bb\u2217\u20162 \u2016\u03bb\u0303\u2212 \u03bb\u2217\u20162 \u2264 4\u221ar\u03bb."}, {"heading": "5.3 Proof of Lemma 2", "text": "We first introduce one lemma that is central to our analysis. From the property that R preserves the \u21132-norm, it is easy to verify that it also preserves the inner product (Arriaga and Vempala, 2006). Specifically, we have the following lemma.\nLemma 6 Assume R satisfies Theorem 1. For any two fixed vectors u \u2208 Rn and v \u2208 Rn, with a probability at least 1\u2212 \u03b4, we have\n\u2223\u2223\u2223u\u22a4RR\u22a4v \u2212 u\u22a4v \u2223\u2223\u2223 \u2264 \u2016u\u20162\u2016v\u20162\n\u221a c\nm log\n4 \u03b4 .\nprovided (9) holds.\nFrom Lemma 6, we have with a probability at least 1\u2212 \u03b4, \u2223\u2223\u2223\u2223 [ (RR\u22a4 \u2212 I)A\u22a4w\u2217 ] j \u2223\u2223\u2223\u2223 = \u2223\u2223\u2223ej(RR\u22a4 \u2212 I)A\u22a4w\u2217 \u2223\u2223\u2223 \u2264 \u2016A\u22a4w\u2217\u20162 \u221a c m log 4 \u03b4\nfor each j \u2208 [n]. We complete the proof by taking the union bound over all j \u2208 [n]."}, {"heading": "5.4 Proof of Lemma 4", "text": "Let \u2126w include the subset of non-zeros entries in w\u2217 and \u2126\u0304w = [d] \\ \u2126w. Define G(w) = g(w) + max\n\u03bb\u2208\u2206 \u2212h(\u03bb)\u2212w\u22a4A\u03bb,\nG\u0302(w) = g(w) + \u03b3w\u2016w\u20161 +max \u03bb\u2208\u2206 \u2212h(\u03bb)\u2212w\u22a4A\u0302R\u22a4\u03bb\u2212 \u03b3\u03bb\u2016\u03bb\u20161.\nLet v \u2208 \u2202\u2016w\u2217\u20161 be any subgradient of \u2016 \u00b7 \u20161 at w\u2217. Then, we have u = \u2207g(w\u2217)\u2212ARR\u22a4\u03bb\u0303+ \u03b3wv \u2208 \u2202G\u0302(w\u2217). 3\nUsing the fact that w\u0302 minimizes G\u0302(\u00b7) over the domain \u2126 and g(\u00b7) is \u03b1-strongly convex, we have\n0 \u2265 G\u0302(w\u0302)\u2212 G\u0302(w\u2217) \u2265 \u3008w\u0302 \u2212w\u2217,u\u3009+ \u03b1\n2 \u2016w\u0302 \u2212w\u2217\u201622\n= \u2329 w\u0302 \u2212w\u2217,\u2207g(w\u2217)\u2212ARR\u22a4\u03bb\u0303+ \u03b3wv \u232a + \u03b1\n2 \u2016w\u0302 \u2212w\u2217\u201622.\n(23)\nBy setting vi = sign(w\u0302i), \u2200i \u2208 \u2126\u0304w, we have \u3008w\u0302\u2126\u0304w ,v\u2126\u0304w\u3009 = \u2016w\u0302\u2126\u0304w\u20161. As a result, \u3008w\u0302 \u2212w\u2217,v\u3009 = \u3008w\u0302\u2126\u0304w ,v\u2126\u0304w\u3009+ \u3008w\u0302\u2126w \u2212w\u2217,v\u2126w\u3009 \u2265 \u2016w\u0302\u2126\u0304w\u20161 \u2212 \u2016w\u0302\u2126w \u2212w\u2217\u20161. (24)\nCombining (23) with (24), we have \u2329 w\u0302 \u2212w\u2217,\u2207g(w\u2217)\u2212ARR\u22a4\u03bb\u0303 \u232a + \u03b1\n2 \u2016w\u0302 \u2212w\u2217\u201622 + \u03b3w\u2016w\u0302\u2126\u0304w\u20161 \u2264 \u03b3w\u2016w\u0302\u2126w \u2212w\u2217\u20161. (25)\nFrom the fact that w\u2217 minimizes G(\u00b7) over the domain \u2126, we have \u3008\u2207G(w\u2217),w \u2212w\u2217\u3009 = \u3008\u2207g(w\u2217)\u2212A\u03bb\u2217,w \u2212w\u2217\u3009 \u2265 0, \u2200w \u2208 \u2126. (26)\nThen, \u2329 w\u0302 \u2212w\u2217,\u2207g(w\u2217)\u2212ARR\u22a4\u03bb\u0303 \u232a\n= \u3008w\u0302 \u2212w\u2217,\u2207g(w\u2217)\u2212A\u03bb\u2217\u3009+ \u2329 w\u0302 \u2212w\u2217, A(I \u2212RR\u22a4)\u03bb\u2217 \u232a + \u2329 w\u0302 \u2212w\u2217, ARR\u22a4(\u03bb\u2217 \u2212 \u03bb\u0303) \u232a\n(26) \u2265 \u2212 \u2016w\u0302 \u2212w\u2217\u20161 (\u2225\u2225\u2225A(I \u2212RR\u22a4)\u03bb\u2217 \u2225\u2225\u2225 \u221e + \u2225\u2225\u2225ARR\u22a4(\u03bb\u2217 \u2212 \u03bb\u0303) \u2225\u2225\u2225 \u221e ) (17) = \u2212 \u03c1w\u2016w\u0302 \u2212w\u2217\u20161 = \u2212\u03c1w ( \u2016w\u0302\u2126\u0304w\u20161 + \u2016w\u0302\u2126w \u2212w\u2217\u20161 ) .\n(27)\nFrom (25) and (27), we have\n\u03b1 2 \u2016w\u0302 \u2212w\u2217\u201622 + (\u03b3w \u2212 \u03c1w)\u2016w\u0302\u2126\u0304w\u20161 \u2264 (\u03b3w + \u03c1w)\u2016w\u0302\u2126w \u2212w\u2217\u20161.\nThe rest proof is identical to that of Lemma 1.\n3. In the case that g(\u00b7) is non-smooth, \u2207g(w\u2217) refers to a subgradient of g(\u00b7) at w\u2217. In particular, we choose the subgradient that satisfies (26)."}, {"heading": "5.5 Proof of Lemma 5", "text": "We first upper bound \u03c1w as\n\u03c1w \u2264 \u2225\u2225\u2225A(I \u2212RR\u22a4)\u03bb\u2217 \u2225\u2225\u2225 \u221e\ufe38 \ufe37\ufe37 \ufe38\n:=U1\n+ \u2225\u2225\u2225A(\u03bb\u2217 \u2212 \u03bb\u0303) \u2225\u2225\u2225 \u221e\ufe38 \ufe37\ufe37 \ufe38\n:=U2\n+ \u2225\u2225\u2225A(RR\u22a4 \u2212 I)(\u03bb\u2217 \u2212 \u03bb\u0303) \u2225\u2225\u2225 \u221e\ufe38 \ufe37\ufe37 \ufe38\n:=U3\n.\nBounding U1 From Lemma 6, we have with a probability at least 1\u2212 \u03b4, \u2223\u2223\u2223 [ A(I \u2212RR\u22a4)\u03bb\u2217\n] i \u2223\u2223\u2223 = \u2223\u2223\u2223Ai\u2217(I \u2212RR\u22a4)\u03bb\u2217 \u2223\u2223\u2223 \u2264 max i\u2208[d] \u2016Ai\u2217\u20162\u2016\u03bb\u2217\u20162 \u221a c m log 4 \u03b4 (5) \u2264 \u2016\u03bb\u2217\u20162 \u221a c m log 4 \u03b4\nfor each i \u2208 [d]. Taking the union bound over all i \u2208 [d], we have with a probability at least 1\u2212 \u03b4,\n\u2225\u2225\u2225A(I \u2212RR\u22a4)\u03bb\u2217 \u2225\u2225\u2225 \u221e \u2264 \u2016\u03bb\u2217\u20162 \u221a c m log 4d \u03b4 .\nBounding U2 From our assumption, we have \u2225\u2225\u2225A(\u03bb\u2217 \u2212 \u03bb\u0303)\n\u2225\u2225\u2225 \u221e \u2264 max i\u2208[d] \u2016Ai\u2217\u20162\u2016\u03bb\u2217 \u2212 \u03bb\u0303\u20162 (5) \u2264 \u2016\u03bb\u2217 \u2212 \u03bb\u0303\u20162.\nBounding U3 Notice that the arguments for bounding U1 cannot be used to upper bound U3, that is because \u03bb\u2217 \u2212 \u03bb\u0303 is a random variable that depends on R and thus we cannot apply Lemma 6 directly. To overcome this challenge, we will exploit the fact that \u03bb\u2217 \u2212 \u03bb\u0303 is approximately sparse to decouple the dependence. Define\nKn,16r\u03bb = {x \u2208 Rn : \u2016x\u20162 \u2264 1, \u2016x\u20161 \u2264 4 \u221a r\u03bb} .\nWhen the conclusion in Lemma 3 happens, we have\n\u03bb\u0303\u2212 \u03bb\u2217 \u2016\u03bb\u0303\u2212 \u03bb\u2217\u20162 \u2208 Kn,16r\u03bb (28)\nand thus\nU3 = \u2016\u03bb\u2217 \u2212 \u03bb\u0303\u20162 \u2225\u2225\u2225\u2225\u2225A(RR \u22a4 \u2212 I) \u03bb\u2217 \u2212 \u03bb\u0303 \u2016\u03bb\u2217 \u2212 \u03bb\u0303\u20162 \u2225\u2225\u2225\u2225\u2225 \u221e (28) \u2264 \u2016\u03bb\u2217 \u2212 \u03bb\u0303\u20162 sup z\u2208Kn,16r\u03bb \u2225\u2225\u2225A(RR\u22a4 \u2212 I)z \u2225\u2225\u2225 \u221e\n\ufe38 \ufe37\ufe37 \ufe38 :=U4\n.\nThen, we will utilize the techniques of covering number to provide an upper bound for U4. Lemma 7 With a probability at least 1\u2212 \u03b4, we have\nsup z\u2208Kn,16r\u03bb\n\u2225\u2225\u2225A(RR\u22a4 \u2212 I)z \u2225\u2225\u2225 \u221e \u2264 2(2 + \u221a 2)\n\u221a c\nm\n( log 4d\n\u03b4 + 16r\u03bb log\n9n\n8r\u03bb\n) .\nPutting everything together, we have\n\u03c1w \u2264\u2016\u03bb\u2217\u20162 \u221a c\nm log\n4d\n\u03b4 + \u2016\u03bb\u2217 \u2212 \u03bb\u0303\u20162\n( 1 + 2(2 + \u221a 2) \u221a c\nm\n( log 4d\n\u03b4 + 16r\u03bb log\n9n\n8r\u03bb\n))\n\u2264\u2016\u03bb\u2217\u20162 \u221a c\nm log\n4d\n\u03b4 +\n3\u03b3\u03bb \u221a r\u03bb\n\u03b2\n( 1 + 7 \u221a c\nm\n( log 4d\n\u03b4 + 16r\u03bb log\n9n\n8r\u03bb\n)) ."}, {"heading": "5.6 Proof of Lemma 6", "text": "First, we assume \u2016u\u20162 = \u2016v\u20162 = 1. Following the proof of Corollary 2 in Arriaga and Vempala (2006), we apply Theorem 1 to vectors u+v and u\u2212v. Then, with a probability at least 1\u2212 4 exp(\u2212m\u03b52/c), we have\n(1\u2212 \u03b5)\u2016u + v\u201622 \u2264 \u2016R\u22a4(u+ v)\u201622 \u2264 (1 + \u03b5)\u2016u + v\u201622, (29) (1\u2212 \u03b5)\u2016u \u2212 v\u201622 \u2264 \u2016R\u22a4(u\u2212 v)\u201622 \u2264 (1 + \u03b5)\u2016u \u2212 v\u201622, (30)\nprovided \u01eb \u2264 1/2. From (29) and (30), it is straightforward to show that \u2223\u2223\u2223u\u22a4RR\u22a4v\u2212 u\u22a4v \u2223\u2223\u2223 \u2264 \u03b5.\nThus, with a probability at least 1\u2212 \u03b4, we have \u2223\u2223\u2223u\u22a4RR\u22a4v\u2212 u\u22a4v \u2223\u2223\u2223 \u2264 \u221a c\nm log\n4\n\u03b4\nprovided (9) holds.\nWe complete the proof by noticing\n\u2223\u2223\u2223u\u22a4RR\u22a4v\u2212 u\u22a4v \u2223\u2223\u2223 = \u2016u\u20162\u2016v\u20162 \u2223\u2223\u2223\u2223 u\u22a4\n\u2016u\u20162 RR\u22a4\nv \u2016v\u20162 \u2212 u\n\u22a4v\n\u2016u\u20162\u2016v\u20162\n\u2223\u2223\u2223\u2223 ."}, {"heading": "5.7 Proof of Lemma 7", "text": "First, we define Sn,16r\u03bb = {x \u2208 Rn : \u2016x\u20162 \u2264 1, \u2016x\u20160 \u2264 16r\u03bb} . Using Lemma 3.1 from Plan and Vershynin (2013a), we have\nKn,16r\u03bb \u2282 2 conv(Sn,16r\u03bb).\nand therefore\nU4 \u2264 2 sup z\u2208conv(Sn,16r\u03bb )\n\u2225\u2225\u2225A(RR\u22a4 \u2212 I)z \u2225\u2225\u2225 \u221e\n= 2 sup z\u2208Sn,16r\u03bb\n\u2225\u2225\u2225A(RR\u22a4 \u2212 I)z \u2225\u2225\u2225 \u221e\n\ufe38 \ufe37\ufe37 \ufe38 :=\u03b8\n(31)\nwhere the last equality follows from the fact that the maximum of a convex function over a convex set generally occurs at some extreme point of the set (Rockafellar, 1997).\nLet Sn,s(\u01eb) be a proper \u01eb-net for Sn,s with the smallest cardinality, and |Sn,s(\u01eb)| be the covering number for Sn,s. We have the following lemma for bounding |Sn,s(\u01eb)|.\nLemma 8 (Plan and Vershynin, 2013a, Lemma 3.3) For \u01eb \u2208 (0, 1) and s \u2264 n, we have\nlog |Sn,s(\u01eb)| \u2264 s log ( 9n\n\u01ebs\n) .\nLet Sd,16r\u03bb(\u01eb) be a \u01eb-net of Sd,16r\u03bb with smallest cardinality. With the help of Sd,16r\u03bb(\u01eb), we define a discretized version of \u03b8 in (31) as\n\u03b8(\u01eb) = sup {\u2225\u2225\u2225A(RR\u22a4 \u2212 I)z \u2225\u2225\u2225 \u221e : z \u2208 Sn,16r\u03bb(\u01eb) } .\nThe following lemma relates \u03b8 with \u03b8(\u01eb). Lemma 9 (Koltchinskii, 2011, Lemma 9.2) For \u01eb \u2208 (0, 1/ \u221a 2), we have\n\u03b8 \u2264 \u03b8(\u01eb) 1\u2212 \u221a 2\u01eb .\nBy choosing \u01eb = 1/2, we have \u03b8 \u2264 (2 + \u221a 2)\u03b8(1/2). Combining with (31), we obtain\nU4 \u2264 2(2 + \u221a 2) sup {\u2225\u2225\u2225A(RR\u22a4 \u2212 I)z \u2225\u2225\u2225 \u221e : z \u2208 Sn,16r\u03bb(1/2) }\n\ufe38 \ufe37\ufe37 \ufe38 \u03b8(1/2)\nFurthermore, Lemma 8 implies\nlog |Sn,16r\u03bb(1/2)| \u2264 16r\u03bb log ( 9n\n8r\u03bb\n) .\nWe proceed by providing an upper bound for \u03b8(1/2). Following the arguments for bounding U1 in the proof of Lemma 5, we have with a probability at least 1\u2212 \u03b4,\n\u2225\u2225\u2225A ( RR\u22a4 \u2212 I ) z \u2225\u2225\u2225 \u221e \u2264 \u221a c m log 4d \u03b4\nfor each z \u2208 Sn,16r\u03bb(1/2). We complete the proof by taking the union bound over all z \u2208 Sn,16r\u03bb(1/2)."}, {"heading": "6. Conclusion and Future Work", "text": "In this paper, a randomized algorithm is proposed to solve the convex-concave optimization problem in (3). Compared to previous studies, a distinctive feature of the proposed algorithm is that sparse regularization is introduced to control the damage cased by random projection. Under mild assumptions about the optimization problem, we demonstrate that it is able to accurately recover the optimal solutions to (3) provided they are sparse or approximately sparse.\nFrom the current analysis, we need to solve two different problems if our goal is to recover both w\u2217 and \u03bb\u2217 accurately. It is unclear whether this is an artifact of the proof technique or actually unavoidable. We will investigate this issue in the future. Since the proposed algorithm is designed for the case that the optimal solutions are (approximately) sparse, it is practically important to develop a pre-precessing procedure that can estimate the sparsity of solutions before applying our algorithm. We plan to utilize random sampling to address this problem. Last but not least, we will investigate the empirical performance of the proposed algorithm."}, {"heading": "Appendix A. Proof of Theorem 3", "text": "The analysis here is similar to that for Lemma 1. Recall that in the proof of Theorem 2, we have proved that\n\u03b3\u03bb \u2265 2\u2016A\u22a4w\u2217\u20162 \u221a c\nm log\n4n\n\u03b4 \u2265 2 \u2225\u2225\u2225(RR\u22a4 \u2212 I)A\u22a4w\u2217 \u2225\u2225\u2225 \u221e\n(32)\nholds with a probability at least 1\u2212 \u03b4. Define L\u0302(\u03bb) = \u2212h(\u03bb)\u2212 w\u0302\u22a4A\u0302R\u22a4\u03bb\u2212 \u03b3\u03bb\u2016\u03bb\u20161. Using the fact that \u03bb\u0302 maximizes L\u0302(\u00b7) over the domain \u2206 and h(\u00b7) is \u03b2-strongly convex, we have\n\u2329 \u03bb\u0302\u2212 \u03bb\u2217,\u2207h(\u03bb\u2217) +RR\u22a4A\u22a4w\u0302 \u232a + \u03b2\n2 \u2016\u03bb\u2217 \u2212 \u03bb\u0302\u201622 + \u03b3\u03bb\u2016\u03bb\u0302\u2126\u0304\u03bb\u20161 \u2264 \u03b3\u03bb\u2016\u03bb\u0302\u2126\u03bb \u2212 \u03bb\u2217\u20161. (33)\nOn the other hand, we have \u2329 \u03bb\u0302\u2212 \u03bb\u2217,\u2207h(\u03bb\u2217) +RR\u22a4A\u22a4w\u0302 \u232a\n=\u3008\u03bb\u0302\u2212 \u03bb\u2217,\u2207h(\u03bb\u2217) +A\u22a4w\u2217\u3009+ \u2329 \u03bb\u0302\u2212 \u03bb\u2217, (RR\u22a4 \u2212 I)A\u22a4w\u2217 \u232a + \u2329 \u03bb\u0302\u2212 \u03bb\u2217, RR\u22a4A\u22a4(w\u0302 \u2212w\u2217) \u232a\n(21) \u2265 \u2212 \u2016\u03bb\u0302\u2212 \u03bb\u2217\u20161 \u2225\u2225\u2225(RR\u22a4 \u2212 I)A\u22a4w\u2217 \u2225\u2225\u2225 \u221e \u2212 \u2016\u03bb\u0302\u2212 \u03bb\u2217\u20162 \u2225\u2225\u2225RR\u22a4A\u22a4(w\u0302 \u2212w\u2217) \u2225\u2225\u2225 2 (32)\n\u2265 \u2212 \u03b3\u03bb 2 \u2016\u03bb\u0302\u2212 \u03bb\u2217\u20161 \u2212 \u2016\u03bb\u0302\u2212 \u03bb\u2217\u20162 \u2225\u2225\u2225RR\u22a4A\u22a4(w\u0302 \u2212w\u2217) \u2225\u2225\u2225 2 .\n(34)\nFrom (33) and (34), we have\n\u03b2 2 \u2016\u03bb\u2217 \u2212 \u03bb\u0302\u201622 + \u03b3\u03bb 2 \u2016\u03bb\u0302\u2126\u0304\u03bb\u20161\n\u22643\u03b3\u03bb 2\n\u2016\u03bb\u0303\u2126\u03bb \u2212 \u03bb\u2217\u20161 + \u2016\u03bb\u0302\u2212 \u03bb\u2217\u20162 \u2225\u2225\u2225RR\u22a4A\u22a4(w\u0302 \u2212w\u2217) \u2225\u2225\u2225 2\n\u22643\u03b3\u03bb \u221a r\u03bb\n2 \u2016\u03bb\u0303\u2126\u03bb \u2212 \u03bb\u2217\u20162 + \u2016\u03bb\u0302\u2212 \u03bb\u2217\u20162 \u2225\u2225\u2225RR\u22a4A\u22a4(w\u0302 \u2212w\u2217) \u2225\u2225\u2225 2\n\u2264\u2016\u03bb\u0302\u2212 \u03bb\u2217\u20162 ( 3\u03b3\u03bb \u221a r\u03bb 2 + \u2225\u2225\u2225RR\u22a4A\u22a4(w\u0302 \u2212w\u2217) \u2225\u2225\u2225 2 )\nwhich implies\n\u2016\u03bb\u2217 \u2212 \u03bb\u0302\u20162\n\u2264 2 \u03b2\n( 3\u03b3\u03bb \u221a r\u03bb\n2 + \u2225\u2225\u2225RR\u22a4A\u22a4(w\u0302 \u2212w\u2217) \u2225\u2225\u2225 2\n)\n\u2264 2 \u03b2\n( 3\u03b3\u03bb \u221a r\u03bb\n2 + \u2016A\u22a4(w\u0302 \u2212w\u2217)\u20162 + \u2225\u2225\u2225(RR\u22a4 \u2212 I)A\u22a4(w\u0302 \u2212w\u2217) \u2225\u2225\u2225 2\n)\n\u2264 2 \u03b2\n( 3\u03b3\u03bb \u221a r\u03bb\n2 +\n( 1 + \u2016RR\u22a4 \u2212 I\u20162 ) \u2016A\u22a4(w\u0302 \u2212w\u2217)\u20162 ) ."}, {"heading": "Appendix B. Proof of Theorem 5", "text": "The proof is almost identical to that of Theorem 2. We just need to replace Lemmas (1) and (4) with the following ones.\nLemma 10 Denote\n\u03c1\u03bb = \u2225\u2225\u2225(RR\u22a4 \u2212 I)A\u22a4w\u2217 \u2225\u2225\u2225 \u221e + \u03c2. (35)"}, {"heading": "By choosing \u03b3\u03bb \u2265 2\u03c1\u03bb, then the conclusion of Lemma (1) still holds.", "text": "Lemma 11 Denote\n\u03c1w = \u2225\u2225\u2225A ( I \u2212RR\u22a4 ) \u03bb\u2217 \u2225\u2225\u2225 \u221e + \u2225\u2225\u2225ARR\u22a4(\u03bb\u2217 \u2212 \u03bb\u0303) \u2225\u2225\u2225 \u221e + \u03c2. (36)"}, {"heading": "By choosing \u03b3w \u2265 2\u03c1w, then the conclusion of Lemma (4) still holds.", "text": ""}, {"heading": "Appendix C. Proof of Lemma 10", "text": "From the assumption, we have\n\u2329 \u03bb\u0303\u2212 \u03bb\u2217,\u2207h(\u03bb\u2217) +RR\u22a4A\u22a4w\u2217 \u232a\n= \u2329 \u03bb\u0303\u2212 \u03bb\u2217,\u2207h(\u03bb\u2217) +A\u22a4w\u2217 \u232a + \u2329 \u03bb\u0303\u2212 \u03bb\u2217, (RR\u22a4 \u2212 I)A\u22a4w\u2217 \u232a\n(12) \u2265 \u2212 \u2016\u03bb\u0303\u2212 \u03bb\u2217\u20161 (\u2225\u2225\u2225(RR\u22a4 \u2212 I)A\u22a4w\u2217 \u2225\u2225\u2225 \u221e + \u03c2 ) (35) = \u2212 \u03c1\u03bb\u2016\u03bb\u0303\u2212 \u03bb\u2217\u20161 = \u2212\u03c1\u03bb ( \u2016\u03bb\u0303\u2126\u0304\u03bb\u20161 + \u2016\u03bb\u0303\u2126\u03bb \u2212 \u03bb\u2217\u20161 ) .\nSubstituting the above inequality into (20), and the rest proof is identical to that of Lemma 1."}, {"heading": "Appendix D. Proof of Lemma 11", "text": "Similarly, we have\n\u2329 w\u0302 \u2212w\u2217,\u2207g(w\u2217)\u2212ARR\u22a4\u03bb\u0303 \u232a\n= \u3008w\u0302 \u2212w\u2217,\u2207g(w\u2217)\u2212A\u03bb\u2217\u3009+ \u2329 w\u0302 \u2212w\u2217, A(I \u2212RR\u22a4)\u03bb\u2217 \u232a + \u2329 w\u0302 \u2212w\u2217, ARR\u22a4(\u03bb\u2217 \u2212 \u03bb\u0303) \u232a\n(11) \u2265 \u2212 \u2016w\u0302 \u2212w\u2217\u20161 (\u2225\u2225\u2225A(I \u2212RR\u22a4)\u03bb\u2217 \u2225\u2225\u2225 \u221e + \u2225\u2225\u2225ARR\u22a4(\u03bb\u2217 \u2212 \u03bb\u0303) \u2225\u2225\u2225 \u221e + \u03c2 ) (36) = \u2212 \u03c1w\u2016w\u0302 \u2212w\u2217\u20161 = \u2212\u03c1w ( \u2016w\u0302\u2126\u0304w\u20161 + \u2016w\u0302\u2126w \u2212w\u2217\u20161 ) .\nSubstituting the above inequality into (25), and the rest proof is identical to that of Lemma 4."}, {"heading": "Appendix E. Proof of Theorem 6", "text": "The proof is almost identical to that of Theorem 2. We just need to replace Lemmas (1) and (4) with the following ones.\nLemma 12 Denote\n\u03c1\u03bb = \u2225\u2225\u2225(RR\u22a4 \u2212 I)A\u22a4w\u2217 \u2225\u2225\u2225 \u221e + (1 + \u00b5)\u03c4. (37)\nBy choosing \u03b3\u03bb \u2265 2\u03c1\u03bb, we have\n\u2016\u03bb\u0303\u2212 \u03bb\u2217\u20162 \u2264 3\u03b3\u03bb\n\u221a r\u03bb \u03b2 , \u2016\u03bb\u0303\u2212 \u03bb\u2217\u20161 \u2264 12\u03b3\u03bbr\u03bb \u03b2 , and \u2016\u03bb\u0303\u2212 \u03bb\u2217\u20161 \u2016\u03bb\u0303\u2212 \u03bb\u2217\u20162 \u2264 4\u221ar\u03bb.\nLemma 13 Denote\n\u03c1w = \u2225\u2225\u2225A ( I \u2212RR\u22a4 ) \u03bb\u2217 \u2225\u2225\u2225 \u221e + \u2225\u2225\u2225ARR\u22a4(\u03bb\u2217 \u2212 \u03bb\u0303) \u2225\u2225\u2225 \u221e + (1 + \u00b5)\u03c4. (38)\nBy choosing \u03b3w \u2265 2\u03c1w, we have\n\u2016w\u0302 \u2212w\u2217\u20162 \u2264 3\u03b3w\n\u221a rw\n\u03b1 , \u2016w\u0302 \u2212w\u2217\u20161 \u2264 12\u03b3wrw \u03b1 , and \u2016w\u0302 \u2212w\u2217\u20161 \u2016w\u0302 \u2212w\u2217\u20162 \u2264 4\u221arw."}, {"heading": "Appendix F. Proof of Lemma 12", "text": "Recall the definition of L(\u00b7) in Section 5.2. From the fact that the optimal solution \u03bb\u2032\u2217 lies in the interior of \u2206, we have\n\u2207L(\u03bb\u2032\u2217) = \u2212\u2207h(\u03bb\u2032\u2217)\u2212A\u22a4w\u2032\u2217 = 0. (39)\nThen,\n\u2329 \u03bb\u0303\u2212 \u03bb\u2217,\u2207h(\u03bb\u2217) +RR\u22a4A\u22a4w\u2217 \u232a\n= \u2329 \u03bb\u0303\u2212 \u03bb\u2217,\u2207h(\u03bb\u2032\u2217) +A\u22a4w\u2032\u2217 \u232a + \u2329 \u03bb\u0303\u2212 \u03bb\u2217,\u2207h(\u03bb\u2217)\u2212\u2207h(\u03bb\u2032\u2217) \u232a\n+ \u2329 \u03bb\u0303\u2212 \u03bb\u2217, A\u22a4w\u2217 \u2212A\u22a4w\u2032\u2217 \u232a + \u2329 \u03bb\u0303\u2212 \u03bb\u2217, (RR\u22a4 \u2212 I)A\u22a4w\u2217 \u232a\n(39) \u2265 \u2212 \u2016\u03bb\u0303\u2212 \u03bb\u2217\u20161 ( \u2016\u2207h(\u03bb\u2217)\u2212\u2207h(\u03bb\u2032\u2217)\u2016\u221e + \u2225\u2225\u2225A\u22a4w\u2217 \u2212A\u22a4w\u2032\u2217 \u2225\u2225\u2225 \u221e + \u2225\u2225\u2225(RR\u22a4 \u2212 I)A\u22a4w\u2217 \u2225\u2225\u2225 \u221e ) .\n(40)\nFrom our assumptions, we have\n\u2016\u2207h(\u03bb\u2217)\u2212\u2207h(\u03bb\u2032\u2217)\u2016\u221e \u2264 \u2016\u2207h(\u03bb\u2217)\u2212\u2207h(\u03bb\u2032\u2217)\u20162 (15) \u2264 \u00b5\u2016\u03bb\u2217 \u2212 \u03bb\u2032\u2217\u20162 (13)\n\u2264 \u00b5\u03c4, (41) \u2225\u2225\u2225A\u22a4(w\u2217 \u2212w\u2032\u2217) \u2225\u2225\u2225 \u221e \u2264 max j\u2208[n] \u2016A\u2217j\u20162\u2016w\u2217 \u2212w\u2032\u2217\u20162 (6) \u2264 \u2016w\u2217 \u2212w\u2032\u2217\u20162 (13) \u2264 \u03c4. (42)\nFrom (40), (41) and (42), we have\n\u2329 \u03bb\u0303\u2212 \u03bb\u2217,\u2207h(\u03bb\u2217) +RR\u22a4A\u22a4w\u2217 \u232a \u2265\u2212 \u2016\u03bb\u0303\u2212 \u03bb\u2217\u20161 (\u2225\u2225\u2225(RR\u22a4 \u2212 I)A\u22a4w\u2217\n\u2225\u2225\u2225 \u221e + (1 + \u00b5)\u03c4 )\n(37) = \u2212 \u03c1\u03bb\u2016\u03bb\u0303\u2212 \u03bb\u2217\u20161 = \u2212\u03c1\u03bb ( \u2016\u03bb\u0303\u2126\u0304\u03bb\u20161 + \u2016\u03bb\u0303\u2126\u03bb \u2212 \u03bb\u2217\u20161 ) .\nSubstituting the above inequality into (20), and the rest proof is identical to that of Lemma 1."}, {"heading": "Appendix G. Proof of Lemma 13", "text": "Recall the definition of G(\u00b7) in Section 5.4. From the fact that the optimal solution w\u2032\u2217 lies in the interior of \u2126, we have\n\u2207G(w\u2032\u2217) = \u2207g(w\u2032\u2217)\u2212A\u03bb\u2032\u2217 = 0. (43)\nThen,\n\u2329 w\u0302 \u2212w\u2217,\u2207g(w\u2217)\u2212ARR\u22a4\u03bb\u0303 \u232a\n= \u2329 w\u0302 \u2212w\u2217,\u2207g(w\u2032\u2217)\u2212A\u03bb\u2032\u2217 \u232a + \u3008w\u0302 \u2212w\u2217,\u2207g(w\u2217)\u2212\u2207g(w\u2032\u2217)\u3009+ \u2329 w\u0302 \u2212w\u2217, A\u03bb\u2032\u2217 \u2212A\u03bb\u2217 \u232a\n+ \u2329 w\u0302 \u2212w\u2217, A(I \u2212RR\u22a4)\u03bb\u2217 \u232a + \u2329 w\u0302 \u2212w\u2217, ARR\u22a4(\u03bb\u2217 \u2212 \u03bb\u0303) \u232a\n(43) \u2265 \u2212 \u2016w\u0302 \u2212w\u2217\u20161 (\u2225\u2225\u2207g(w\u2217)\u2212\u2207g(w\u2032\u2217) \u2225\u2225 \u221e + \u2225\u2225A\u03bb\u2032\u2217 \u2212A\u03bb\u2217 \u2225\u2225 \u221e )\n\u2212 \u2016w\u0302 \u2212w\u2217\u20161 (\u2225\u2225\u2225A(I \u2212RR\u22a4)\u03bb\u2217 \u2225\u2225\u2225 \u221e + \u2225\u2225\u2225ARR\u22a4(\u03bb\u2217 \u2212 \u03bb\u0303) \u2225\u2225\u2225 \u221e ) .\n(44)\nFrom our assumptions, we have\n\u2016\u2207g(w\u2217)\u2212\u2207g(w\u2032\u2217)\u2016\u221e \u2264 \u2016\u2207g(w\u2217)\u2212\u2207g(w\u2032\u2217)\u20162 (14) \u2264 \u00b5\u2016w\u2217 \u2212w\u2032\u2217\u20162 (13) \u2264 \u00b5\u03c4, (45)\n\u2016A(\u03bb\u2032\u2217 \u2212 \u03bb\u2217)\u2016\u221e \u2264 max i\u2208[d]\n\u2016Ai\u2217\u20162\u2016\u03bb\u2032\u2217 \u2212 \u03bb\u2217\u20162 (5) \u2264 \u2016\u03bb\u2032\u2217 \u2212 \u03bb\u2217\u20162 (13) \u2264 \u03c4. (46)\nFrom (44), (45) and (46), we have\n\u2329 w\u0302 \u2212w\u2217,\u2207g(w\u2217)\u2212ARR\u22a4\u03bb\u0303 \u232a\n\u2265\u2212 \u2016w\u0302 \u2212w\u2217\u20161 (\u2225\u2225\u2225A(I \u2212RR\u22a4)\u03bb\u2217 \u2225\u2225\u2225 \u221e + \u2225\u2225\u2225ARR\u22a4(\u03bb\u2217 \u2212 \u03bb\u0303) \u2225\u2225\u2225 \u221e + (1 + \u00b5)\u03c4 )\n(38) = \u2212 \u03c1w\u2016w\u0302 \u2212w\u2217\u20161 = \u2212\u03c1w ( \u2016w\u0302\u2126\u0304w\u20161 + \u2016w\u0302\u2126w \u2212w\u2217\u20161 ) .\nSubstituting the above inequality into (25), and the rest proof is identical to that of Lemma 4."}], "references": [{"title": "Database-friendly random projections: Johnson-lindenstrauss with binary coins", "author": ["Dimitris Achlioptas"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Achlioptas.,? \\Q2003\\E", "shortCiteRegEx": "Achlioptas.", "year": 2003}, {"title": "An algorithmic theory of learning: Robust concepts and random projection", "author": ["Rosa I. Arriaga", "Santosh Vempala"], "venue": "Machine Learning,", "citeRegEx": "Arriaga and Vempala.,? \\Q2006\\E", "shortCiteRegEx": "Arriaga and Vempala.", "year": 2006}, {"title": "Optimization with sparsity-inducing penalties", "author": ["Francis Bach", "Rodolphe Jenatton", "Julien Mairal", "Guillaume Obozinski"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bach et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2012}, {"title": "Kernels as features: On kernels, margins, and low-dimensional mappings", "author": ["Maria-Florina Balcan", "Avrim Blum", "Santosh Vempala"], "venue": "Machine Learning,", "citeRegEx": "Balcan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2006}, {"title": "Random projection in dimensionality reduction: applications to image and text data", "author": ["Ella Bingham", "Heikki Mannila"], "venue": "In Proceedings of the 7th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Bingham and Mannila.,? \\Q2001\\E", "shortCiteRegEx": "Bingham and Mannila.", "year": 2001}, {"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe.,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe.", "year": 2004}, {"title": "Prediction, Learning, and Games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "An elementary proof of a theorem of johnson and lindenstrauss", "author": ["Sanjoy Dasgupta", "Anupam Gupta"], "venue": "Random Structures & Algorithms,", "citeRegEx": "Dasgupta and Gupta.,? \\Q2003\\E", "shortCiteRegEx": "Dasgupta and Gupta.", "year": 2003}, {"title": "The Elements of Statistical Learning", "author": ["Trevor Hastie", "Robert Tibshirani", "Jerome Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "An accelerated hpe-type algorithm for a class of composite convex-concave saddle-point problems", "author": ["Yunlong He", "Renato D.C. Monteiro"], "venue": "Technical report, Georgia Institute of Technology,,", "citeRegEx": "He and Monteiro.,? \\Q2014\\E", "shortCiteRegEx": "He and Monteiro.", "year": 2014}, {"title": "On the duality of strong convexity and strong smoothness: Learning applications and matrix regularization", "author": ["Sham M. Kakade", "Shai Shalev-Shwartz", "Ambuj Tewari"], "venue": "Technical report, Toyota Technological Institute at Chicago,", "citeRegEx": "Kakade et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2009}, {"title": "Dimensionality reduction by random mapping: fast similarity computation for clustering", "author": ["Samuel Kaski"], "venue": "In Proceedings of the 1998 IEEE International Joint Conference on Neural Networks,", "citeRegEx": "Kaski.,? \\Q1998\\E", "shortCiteRegEx": "Kaski.", "year": 1998}, {"title": "Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems", "author": ["Vladimir Koltchinskii"], "venue": null, "citeRegEx": "Koltchinskii.,? \\Q2011\\E", "shortCiteRegEx": "Koltchinskii.", "year": 2011}, {"title": "Dimensionality reductions that preserve volumes and distance to affine spaces, and their algorithmic applications", "author": ["Avner Magen"], "venue": "In Randomization and Approximation Techniques in Computer Science,", "citeRegEx": "Magen.,? \\Q2002\\E", "shortCiteRegEx": "Magen.", "year": 2002}, {"title": "Randomized algorithms for matrices and data", "author": ["Michael W. Mahoney"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Mahoney.,? \\Q2011\\E", "shortCiteRegEx": "Mahoney.", "year": 2011}, {"title": "Uniform uncertainty principle for bernoulli and subgaussian ensembles", "author": ["Shahar Mendelson", "Alain Pajor", "Nicole Tomczak-Jaegermann"], "venue": "Constructive Approximation,", "citeRegEx": "Mendelson et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mendelson et al\\.", "year": 2008}, {"title": "Prox-method with rate of convergence O(1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems", "author": ["Arkadi Nemirovski"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nemirovski.,? \\Q2005\\E", "shortCiteRegEx": "Nemirovski.", "year": 2005}, {"title": "Smooth minimization of non-smooth functions", "author": ["Yu. Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "Nesterov.,? \\Q2005\\E", "shortCiteRegEx": "Nesterov.", "year": 2005}, {"title": "Random projections for support vector machines", "author": ["Saurabh Paul", "Christos Boutsidis", "Malik Magdon-Ismail", "Petros Drineas"], "venue": "In Proceedings of the 16th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Paul et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Paul et al\\.", "year": 2013}, {"title": "One-bit compressed sensing by linear programming", "author": ["Yaniv Plan", "Roman Vershynin"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Plan and Vershynin.,? \\Q2013\\E", "shortCiteRegEx": "Plan and Vershynin.", "year": 2013}, {"title": "Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach", "author": ["Yaniv Plan", "Roman Vershynin"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Plan and Vershynin.,? \\Q2013\\E", "shortCiteRegEx": "Plan and Vershynin.", "year": 2013}, {"title": "Convex Analysis", "author": ["Ralph Tyrell Rockafellar"], "venue": null, "citeRegEx": "Rockafellar.,? \\Q1997\\E", "shortCiteRegEx": "Rockafellar.", "year": 1997}, {"title": "Is margin preserved after random projection", "author": ["Qinfeng Shi", "Chunhua Shen", "Rhys Hill", "Anton van den Hengel"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Shi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2012}, {"title": "Fast rates for regularized objectives", "author": ["Karthik Sridharan", "Shai Shalev-shwartz", "Nathan Srebro"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sridharan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sridharan et al\\.", "year": 2009}, {"title": "User-friendly tail bounds for sums of random matrices", "author": ["Joel A. Tropp"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Tropp.,? \\Q2012\\E", "shortCiteRegEx": "Tropp.", "year": 2012}, {"title": "Svm soft margin classifiers: Linear programming versus quadratic programming", "author": ["Qiang Wu", "Ding-Xuan Zhou"], "venue": "Neural Computation,", "citeRegEx": "Wu and Zhou.,? \\Q2005\\E", "shortCiteRegEx": "Wu and Zhou.", "year": 2005}, {"title": "Theory of dual-sparse regularized randomized reduction", "author": ["Tianbao Yang", "Lijun Zhang", "Rong Jin", "Shenghuo Zhu"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Recovering the optimal solution by dual random projection", "author": ["Lijun Zhang", "Mehrdad Mahdavi", "Rong Jin", "Tianbao Yang", "Shenghuo Zhu"], "venue": "In Proceedings of the 26th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Regularization and variable selection via the elastic net", "author": ["Hui Zou", "Trevor Hastie"], "venue": "Journal of the Royal Statistical Society. Series B (Statistical Methodology),", "citeRegEx": "Zou and Hastie.,? \\Q2005\\E", "shortCiteRegEx": "Zou and Hastie.", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "Introduction Learning the sparse representation of a predictive model has received considerable attention in recent years (Bach et al., 2012).", "startOffset": 122, "endOffset": 141}, {"referenceID": 28, "context": "where l(\u00b7) is a convex function such as the logistic loss to measure the empirical error, and \u03c8(\u00b7) is a sparsity-inducing regularizer such as the elastic net (Zou and Hastie, 2005) to", "startOffset": 158, "endOffset": 180}, {"referenceID": 8, "context": "avoid overfitting (Hastie et al., 2009).", "startOffset": 18, "endOffset": 39}, {"referenceID": 21, "context": "where l\u2217(\u00b7) is the Fenchel conjugate of l(\u00b7) (Rockafellar, 1997) and \u0393 is the domain of the dual variable, we get the following convex-concave formulation:", "startOffset": 45, "endOffset": 64}, {"referenceID": 17, "context": "The problem in (3) has been widely studied in the optimization community, and when n and d are medium size, it can be solved iteratively by gradient based methods (Nesterov, 2005; Nemirovski, 2005).", "startOffset": 163, "endOffset": 197}, {"referenceID": 16, "context": "The problem in (3) has been widely studied in the optimization community, and when n and d are medium size, it can be solved iteratively by gradient based methods (Nesterov, 2005; Nemirovski, 2005).", "startOffset": 163, "endOffset": 197}, {"referenceID": 11, "context": "Related Work Random projection has been widely used as an efficient algorithm for dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001).", "startOffset": 107, "endOffset": 147}, {"referenceID": 4, "context": "Related Work Random projection has been widely used as an efficient algorithm for dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001).", "startOffset": 107, "endOffset": 147}, {"referenceID": 7, "context": "In the case of unsupervised learning, it has been proved that random projection is able to preserve the distance (Dasgupta and Gupta, 2003), inner product (Arriaga and Vempala, 2006), volumes and distance to affine spaces (Magen, 2002).", "startOffset": 113, "endOffset": 139}, {"referenceID": 1, "context": "In the case of unsupervised learning, it has been proved that random projection is able to preserve the distance (Dasgupta and Gupta, 2003), inner product (Arriaga and Vempala, 2006), volumes and distance to affine spaces (Magen, 2002).", "startOffset": 155, "endOffset": 182}, {"referenceID": 13, "context": "In the case of unsupervised learning, it has been proved that random projection is able to preserve the distance (Dasgupta and Gupta, 2003), inner product (Arriaga and Vempala, 2006), volumes and distance to affine spaces (Magen, 2002).", "startOffset": 222, "endOffset": 235}, {"referenceID": 3, "context": "For classification, theoretical studies mainly focus on examining the generalization error or the preservation of classification margin in the low-dimensional space (Balcan et al., 2006; Shi et al., 2012; Paul et al., 2013).", "startOffset": 165, "endOffset": 223}, {"referenceID": 22, "context": "For classification, theoretical studies mainly focus on examining the generalization error or the preservation of classification margin in the low-dimensional space (Balcan et al., 2006; Shi et al., 2012; Paul et al., 2013).", "startOffset": 165, "endOffset": 223}, {"referenceID": 18, "context": "For classification, theoretical studies mainly focus on examining the generalization error or the preservation of classification margin in the low-dimensional space (Balcan et al., 2006; Shi et al., 2012; Paul et al., 2013).", "startOffset": 165, "endOffset": 223}, {"referenceID": 14, "context": "For regression, there do exist theoretical guarantees for the recovery error, but they only hold for the least squares problem (Mahoney, 2011).", "startOffset": 127, "endOffset": 142}, {"referenceID": 27, "context": "Our work is closely related to Dual Random Projection (DRP) (Zhang et al., 2013) and Dual-sparse Regularized Randomized Reduction (DSRR) (Yang et al.", "startOffset": 60, "endOffset": 80}, {"referenceID": 26, "context": ", 2013) and Dual-sparse Regularized Randomized Reduction (DSRR) (Yang et al., 2015), which also investigate random projection from the perspective of optimization.", "startOffset": 64, "endOffset": 83}, {"referenceID": 9, "context": "The optimization problem in (4) can be solved by the algorithm designed for composite convex-concave problems (He and Monteiro, 2014).", "startOffset": 110, "endOffset": 133}, {"referenceID": 3, "context": "Compared to previous randomized algorithms (Balcan et al., 2006; Zhang et al., 2013; Yang et al., 2015), (4) has two new features: i) the optimization is still performed in the original space; and ii) the l1-norm is introduced to regularize both primal and dual solutions.", "startOffset": 43, "endOffset": 103}, {"referenceID": 27, "context": "Compared to previous randomized algorithms (Balcan et al., 2006; Zhang et al., 2013; Yang et al., 2015), (4) has two new features: i) the optimization is still performed in the original space; and ii) the l1-norm is introduced to regularize both primal and dual solutions.", "startOffset": 43, "endOffset": 103}, {"referenceID": 26, "context": "Compared to previous randomized algorithms (Balcan et al., 2006; Zhang et al., 2013; Yang et al., 2015), (4) has two new features: i) the optimization is still performed in the original space; and ii) the l1-norm is introduced to regularize both primal and dual solutions.", "startOffset": 43, "endOffset": 103}, {"referenceID": 21, "context": ", the logistic loss), then its convex conjugate l\u2217(\u00b7) will be strongly convex (Rockafellar, 1997; Kakade et al., 2009).", "startOffset": 78, "endOffset": 118}, {"referenceID": 10, "context": ", the logistic loss), then its convex conjugate l\u2217(\u00b7) will be strongly convex (Rockafellar, 1997; Kakade et al., 2009).", "startOffset": 78, "endOffset": 118}, {"referenceID": 7, "context": "The above theorem is widely used to prove the famous Johnson\u2013Lindenstrauss lemma (Dasgupta and Gupta, 2003).", "startOffset": 81, "endOffset": 107}, {"referenceID": 0, "context": "Previous studies (Achlioptas, 2003; Arriaga and Vempala, 2006) have proved that Theorem 1 is true if {Sij} are independent random variables sampled from the Gaussian distribution N (0, 1), uniform distribution over {\u00b11}, or the following database-friendly distribution", "startOffset": 17, "endOffset": 62}, {"referenceID": 1, "context": "Previous studies (Achlioptas, 2003; Arriaga and Vempala, 2006) have proved that Theorem 1 is true if {Sij} are independent random variables sampled from the Gaussian distribution N (0, 1), uniform distribution over {\u00b11}, or the following database-friendly distribution", "startOffset": 17, "endOffset": 62}, {"referenceID": 15, "context": "More generally, a sufficient condition for Theorem 1 is that columns of R are independent, isotropic, and subgaussian vectors (Mendelson et al., 2008).", "startOffset": 126, "endOffset": 150}, {"referenceID": 25, "context": "According to the theoretical analysis of regularized empirical risk minimization (Wu and Zhou, 2005; Sridharan et al., 2009; Koltchinskii, 2011), the optimal \u03b3, that minimizes the generalization error, can be chosen as \u03b3 = O(1/ \u221a n), and thus \u03b1 = O(\u03b3n) = O( \u221a n).", "startOffset": 81, "endOffset": 144}, {"referenceID": 23, "context": "According to the theoretical analysis of regularized empirical risk minimization (Wu and Zhou, 2005; Sridharan et al., 2009; Koltchinskii, 2011), the optimal \u03b3, that minimizes the generalization error, can be chosen as \u03b3 = O(1/ \u221a n), and thus \u03b1 = O(\u03b3n) = O( \u221a n).", "startOffset": 81, "endOffset": 144}, {"referenceID": 12, "context": "According to the theoretical analysis of regularized empirical risk minimization (Wu and Zhou, 2005; Sridharan et al., 2009; Koltchinskii, 2011), the optimal \u03b3, that minimizes the generalization error, can be chosen as \u03b3 = O(1/ \u221a n), and thus \u03b1 = O(\u03b3n) = O( \u221a n).", "startOffset": 81, "endOffset": 144}, {"referenceID": 24, "context": "The upper bound in the above theorem is quite loose, because \u2016RR\u22a4 \u2212 I\u20162 is roughly on the order of n log n/m (Tropp, 2012).", "startOffset": 109, "endOffset": 122}, {"referenceID": 5, "context": "The above conditions can be considered as sub-optimality conditions (Boyd and Vandenberghe, 2004) of w\u2217 and \u03bb\u2217 measured in the l\u221e-norm.", "startOffset": 68, "endOffset": 97}, {"referenceID": 1, "context": "From the property that R preserves the l2-norm, it is easy to verify that it also preserves the inner product (Arriaga and Vempala, 2006).", "startOffset": 110, "endOffset": 137}, {"referenceID": 1, "context": "Following the proof of Corollary 2 in Arriaga and Vempala (2006), we apply Theorem 1 to vectors u+v and u\u2212v.", "startOffset": 38, "endOffset": 65}, {"referenceID": 19, "context": "1 from Plan and Vershynin (2013a), we have", "startOffset": 7, "endOffset": 34}, {"referenceID": 21, "context": "where the last equality follows from the fact that the maximum of a convex function over a convex set generally occurs at some extreme point of the set (Rockafellar, 1997).", "startOffset": 152, "endOffset": 171}], "year": 2017, "abstractText": "In this paper, we develop a randomized algorithm and theory for learning a sparse model from large-scale and high-dimensional data, which is usually formulated as an empirical risk minimization problem with a sparsity-inducing regularizer. Under the assumption that there exists a (approximately) sparse solution with high classification accuracy, we argue that the dual solution is also sparse or approximately sparse. The fact that both primal and dual solutions are sparse motivates us to develop a randomized approach for a general convex-concave optimization problem. Specifically, the proposed approach combines the strength of random projection with that of sparse learning: it utilizes random projection to reduce the dimensionality, and introduces l1-norm regularization to alleviate the approximation error caused by random projection. Theoretical analysis shows that under favored conditions, the randomized algorithm can accurately recover the optimal solutions to the convex-concave optimization problem (i.e., recover both the primal and dual solutions). Furthermore, the solutions returned by our algorithm are guaranteed to be approximately sparse.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}