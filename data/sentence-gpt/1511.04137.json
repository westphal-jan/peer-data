{"id": "1511.04137", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Nov-2015", "title": "Seeing the Unseen Network: Inferring Hidden Social Ties from Respondent-Driven Sampling", "abstract": "Learning about the social structure of hidden and hard-to-reach populations --- such as drug users and sex workers --- is a major goal of epidemiological and public health research on risk behaviors and disease prevention. Respondent-driven sampling (RDS) is a peer-referral process widely used by many health organizations, where research subjects recruit other subjects from their social network. In such surveys, researchers observe who recruited whom, along with the time of recruitment and the total number of acquaintances (network degree) of respondents. However, due to privacy concerns, the identities of acquaintances are not disclosed. In this work, we show how to reconstruct the underlying network structure through which the subjects are recruited. We formulate the dynamics of RDS as a continuous-time diffusion process over the underlying graph and derive the likelihood for the recruitment time series under an arbitrary recruitment time distribution. This way, we develop an efficient stochastic optimization algorithm called RENDER (REspoNdent-Driven nEtwork Reconstruction) that finds the network that best explains the collected data. We support our analytical results through an exhaustive set of experiments on both synthetic and real data. RespoNdent-Driven NEtwork Reconstruction is used to extract the network structures that have been exposed. The result is an average of five out of 10 RDS that can be used to extract the networks and to analyze their distribution (the number of friends and acquaintances is calculated using the same set of statistics as a function of the network structure). The analysis can be performed by performing a task where participants are recruited by another group (by other means, by other means, by another group of peers). The analysis can be performed through the following methods:\n\n\n1. An initial pool of random random samples was performed with random sampling.\n2. Random sampling is not done on a single individual target, or a single target of a particular social group.\n3. An estimate of how many members of a particular social group can be selected (in each population, using the same set of statistics as a function of the network structure) is performed. The estimates are based on the total number of connections the participants have made during the study. A representative sample of the sample of the population, with each group of members, is randomly selected.\n4. The distribution of random samples based on a subset of data is used.\n5. Random sampling is not done on a single individual target, or a single target of a particular social group.\n6", "histories": [["v1", "Fri, 13 Nov 2015 01:59:35 GMT  (565kb,D)", "https://arxiv.org/abs/1511.04137v1", "Accepted by AAAI 2016"], ["v2", "Wed, 2 Dec 2015 01:02:17 GMT  (440kb,D)", "http://arxiv.org/abs/1511.04137v2", "A full version with technical proofs. Accepted by AAAI-16"]], "COMMENTS": "Accepted by AAAI 2016", "reviews": [], "SUBJECTS": "cs.SI cs.AI cs.LG", "authors": ["lin chen 0003", "forrest w crawford", "amin karbasi"], "accepted": true, "id": "1511.04137"}, "pdf": {"name": "1511.04137.pdf", "metadata": {"source": "CRF", "title": "Seeing the Unseen Network: Inferring Hidden Social Ties from Respondent-Driven Sampling", "authors": ["Lin Chen", "Amin Karbasi"], "emails": ["amin.karbasi}@yale.edu"], "sections": [{"heading": "Introduction", "text": "Random sampling is an effective way for researchers to learn about a population of interest. Characteristics of the sample can be generalized to the population of interest using standard statistical theory. However, traditional random sampling may be impossible when the target population is hidden, e.g. drug users, men who have sex with men, sex workers and homeless people. Due to concerns like privacy, stigmatization, discrimination, and criminalization, members of hidden populations may be reluctant to participate in a survey. In addition to the hidden populations for which no sampling \u201cframe\u201d exists, there are rare populations of great research interest, marked by their tiny fractions in the entire population. It is unlikely that random sampling from the general population would accrue a reasonable sample from a very rare population. However, it is often the case that members of hidden or rare populations know each other socially. This suggests that social referral would be an effective method for accruing a large sample. To this end, researchers have developed a variety of social link-tracing survey designs. The most popular is respondent-driven sampling (RDS) (Heckathorn 1997).\nIn RDS, a small set of initial subjects, known as \u201cseeds\u201d are selected, not necessarily simultaneously, from the target population. Subjects are given a few coupons, each tagged with a unique identifier, which they can use to recruit other eligible subjects. Participants are given a reward for being interviewed and for each eligible subject they recruit using their coupons. Each subject reports their degree, the number of others whom they know in the target population. No subject is permitted to enter the study twice and the date and time of each interview is recorded.\nWhile RDS can be an effective recruitment method, it reveals only incomplete social network data to researchers. Any ties between recruited subjects along which no recruitment took place remain unobserved. The social network of recruited subjects is of great interest to sociologists, epidemiologists and public health researchers since it may induce dependencies in the outcomes of sampled individuals. Fortunately, RDS reveals information about the underlying social network that can be used to (approximately) reconstruct it. By leveraging the time series of recruitments, the degrees of recruited subjects, coupon information, and who recruited whom, it is possible to interpret the induced subgraph of RDS respondents as a simple random graph model (Crawford 2016).\nIn this paper, we introduce a flexible and expressive stochastic model of RDS recruitment on a partially observed network structure. We derive the likelihood of the observed time series; the model admits any edgewise inter-recruitment time distribution. We propose a stochastic optimization algorithm RENDER (REspoNdent-Driven nEtwork Reconstruction) to estimate unknown parameters and the underlying social network. We conduct extensive experiments, on synthetic and real data, to confirm the accuracy and the reconstruction performance of RENDER. In particular, we apply RENDER to reconstruct the network of injection drug users from an RDS study in St. Petersburg, Russia."}, {"heading": "Related Work", "text": "RDS has been modeled as a random walk with replacement on a graph at its equilibrium distribution (Heckathorn 1997; Salganik and Heckathorn 2004; Volz and Heckathorn 2008; Goel and Salganik 2009; Gile and Handcock 2010), and under this argument the sampling probability is proportional to degree, which is the basis for an estimator of the popula-\nar X\niv :1\n51 1.\n04 13\n7v 2\n[ cs\n.S I]\n2 D\nec 2\n01 5\ntion mean (Heckathorn 2002; Salganik 2006). In this paper, we adopt an approach that focuses on the network structure estimable from the RDS sample using recruitment information (Crawford 2016). Crawford (2016) assumes that edgewise inter-recruitment times follow the exponential distribution, but this approach is relatively inflexible. In many other contexts, dynamic or random processes can reveal structural information of an underlying, partially observed, network (Kramer et al. 2009; Shandilya and Timme 2011; Linderman and Adams 2014). Network reconstruction and the edge prediction problem have been studied for diffusion processes where multiple realizations of the process on the same network are available (Liben-Nowell and Kleinberg 2007; Gomez Rodriguez, Leskovec, and Krause 2010; Gomez Rodriguez et al. 2011). In the case of RDS, reconstruction is particularly challenging because only a single realization of the diffusion process is observed. Furthermore, we must account for the role of coupons in recruitment as they intricately introduce bias. However, in contrast to general diffusion processes over graphs, some important network topological information is revealed by RDS. In this study, we leverage all the available data routinely collected during real-world RDS studies."}, {"heading": "Problem Formulation", "text": "We conform to the following notation throughout the paper. Suppose that f is a real-valued function and that v is a vector. Then f(v) is a vector of the same size as vector v and the i-th entry of f(v) is denoted by f(v)i, whose value is given by f(vi). The transposes of matrix A and vector v are written as A\u2032 and v\u2032, respectively. And let 1 be the allones column vector."}, {"heading": "Dynamics of Respondent-Driven Sampling", "text": "We characterize the social network of the hidden population as a finite undirected simple graph G = (V,E) with no parallel edges or self-loops. Members of the hidden population are vertices; {i, j} \u2208 E implies that i \u2208 V and j \u2208 V know each other. Using RDS, researchers recruit members from the hidden population into the study. The time-varying recruitment-induced subgraph {GS(t) = (VS(t), ES(t)) : 0 \u2264 t \u2264 tF } is a nested collection of subgraphs of G, where tF is the termination time of the study. For all 0 \u2264 t \u2264 tF , GS(t) is a subgraph of G. Here, GS(0) is the null graph since there are no subjects in the study initially. For simplicity, we write GS = (VS , ES) for GS(tF ) = (VS(tF ), ES(tF )), and call this the recruitmentinduced subgraph or induced subgraph unless we explicitly specify the time t. The vertex set of the time-varying recruitment-induced subgraph at time t (i.e., GS(t)) denotes the members in the hidden population that are known to the study by time t. The subgraphGS(t) is induced by the vertex set VS(t); i.e., ES(t) = {{i, j}|i, j \u2208 VS(t), {i, j} \u2208 E}. The time-varying recruitment-induced subgraph evolves in the following way (Crawford 2016).\n1. At time t\u0303, researchers recruit a subject in the population as a seed. This subject is included in the vertex set VS(t) of\nGS(t) for all t \u2265 t\u0303. Researchers may provide this subject with coupons to recruit its neighbors.\n2. Once recruited into this study (either by researchers or its neighbors) at time t\u0303, subjects currently holding coupons will attempt to recruit their yet-unrecruited neighbors. The inter-recruitment time along each edge connecting a recruiter with an unrecruited neighbor is i.i.d. with cdf F (t). Recruitment happens when a neighbor is recruited into the study and is provided with a number of coupons. A successful recruitment costs the recruiter one coupon.\nThe directed recruitment graph is GR = (VR, ER), where VR = VS(tF ) is the set of members in the study at the final stage. For two subjects i, j \u2208 VR, (i, j) \u2208 ER if and only if i recruits j. Note that the subjects recruited by researchers (i.e., the seeds) have zero in-degree in GR. Let n denote the cardinality of VR (equivalently VS(tF )). For simplicity, we label the subject recruited in the ith recruitment event by i. The labels of the subjects in the study are 1, 2, 3, . . . , n. The vector of recruitment times is t = (t1, t2, t3, . . . , tn), where ti is the recruitment time of subject i. In shorthand, we write \u03c4(i; j) = tj\u22121 \u2212 ti for i < j. Let C be the n \u00d7 n coupon matrix whose element Cij = 1 if subject i has at least one coupon just before the jth recruitment event, and zero otherwise. The rows and columns of the coupon matrix are ordered by subjects\u2019 recruitment time. The degree vector is d = (d1, d2, d3, . . . , dn)\u2032, where di is the degree of i in G. At time t (where t 6= ti for i = 1, 2, 3, . . . , n), if a subject has at least one coupon and at least one neighbor not in the study, we term it a recruiter at time t; if a subject has not entered the study and has at least one neighbor with at least one coupon, we term it a potential recruitee at time t. We assume that the observed data from a RDS recruitment process consist of Y = (GR,d, t,C)."}, {"heading": "Likelihood of Recruitment Time Series", "text": "We assume that the inter-recruitment time along an edge connecting a recruiter and potential recruitee is i.i.d. with cdf F (t). Let Fs(t) = Pr [W \u2264 t |W > s], where W is the random inter-recruitment time and Pr [W \u2264 t] = F (t). We write fs(t) = F \u2032s(t) for the corresponding conditional pdf. Let Ss(t) = 1 \u2212 Fs(t) be the conditional survival function and Hs(t) = fs(t)/Ss(t) be the conditional hazard function.\nWe now derive a closed-form expression for the likelihood of the recruitment time series L(t|GS , \u03b8). In what follows, let M be the set of seeds, and let A denote the adjacency matrix representation of the recruitment-induced subgraph at the final stage GS ; thus we use GS and A interchangeably throughout this paper.\nTheorem 1. (Proof in Appendix). Let R(i) and I(i) be the recruiter set and potential recruitee set just before time ti, respectively, and M be the set of seeds. The following statements with respect to the likelihood of the recruitment time series hold.\n1. The likelihood of the recruitment time series is given by\nL(t|GS , \u03b8)\n= n\u220f i=1  \u2211 u\u2208R(i) |Iu(i)|H\u03c4(u;i)(ti \u2212 tu) 1{i/\u2208M}\n\u00d7 \u220f\nj\u2208R(i)\nS |Ij | \u03c4(j;i)(ti \u2212 tj),\nwhere \u03c4(u; i) = ti\u22121 \u2212 tu. 2. Let m and u be column vectors of size n defined as mi =\n1{i /\u2208 M} and u = d \u2212 A \u00b7 1, and let H and S be n \u00d7 n matrices, defined as Hui = H\u03c4(u;i)(ti \u2212 tu) and Sji = logS\u03c4(j;i)(ti\u2212tj). Furthermore, we form matrices B = (C \u25e6 H) and D = (C \u25e6 S), where \u25e6 denotes the Hadamard (entrywise) product. We let\n\u03b2 = log(B\u2032u + LowerTri(AB)\u2032 \u00b7 1) \u03b4 = D\u2032u + LowerTri(AD)\u2032 \u00b7 1,\nwhere LowerTri(\u00b7) denotes the lower triangular part (diagonal elements inclusive) of a square matrix. Then, the log-likelihood of the recruitment time series can be written as\nl(t|GS , \u03b8) = m\u2032\u03b2 + 1\u2032\u03b4."}, {"heading": "Network Reconstruction Problem", "text": "Given the observed time series, we seek to reconstruct the n\u00d7 n binary symmetric, zero-diagonal adjacency matrix A of GS and the parameter \u03b8 \u2208 \u0398 (\u0398 is the parameter space) that maximizes Pr(A, \u03b8|t). Recall that we use GS and A interchangeably throughout this paper. We have\nPr(A, \u03b8|t) \u221d L(t|A, \u03b8) Pr(A, \u03b8),\nwhere Pr(A, \u03b8) is the prior distribution for (A, \u03b8). The constraint for the parameter \u03b8 is obvious: \u03b8must reside in the parameter space \u0398; i.e., \u03b8 \u2208 \u0398. Now we discuss the constraint for the adjacency matrix A\u2014we require that the adjacency matrix A must be compatible.\nWe seek the matrix A that maximizes the probability Pr(A, \u03b8|t). However, we know that the directed recruitment subgraph, if viewed as an undirected graph, must be a subgraph of the true recruitment-induced subgraph. Let AR be the adjacency matrix of the recruitment subgraph when it is viewed as an undirected graph; i.e., the (i, j) entry of AR is 1 if a direct recruitment event occurs between i and j (either i recruits j or j recruits i), and is 0 otherwise. We require that A be greater than or equal to AR entrywise. Recall that every subject in the study reports its degree; thus the adjacency matrix should also comply with the degree constraints. Following (Crawford 2016), we say that a symmetric, binary and zero-diagonal matrix A is a compatible adjacency matrix if A \u2265 AR entrywise, and A \u00b7 1 \u2264 d entrywise."}, {"heading": "Problem Statement", "text": "Now we formulate this problem as an optimization problem.\nAlgorithm 1 RENDER: Alternating inference of GS and \u03b8 Input: the observed data Y = (GR,d, t,C); the initial guess for the distribution parameter \u03b8, denoted by \u03b8\u03020; the maximum number of iterations, \u03b9max. Output: the estimated adjacency matrix A (denoted by A\u0302) and the estimated parameter \u03b8 (denoted by \u03b8\u0302) \u03b9\u2190 0 while \u03b9 < \u03b9max\nA\u0302\u03b9 \u2190 arg maxA is compatible L(t|A, \u03b8\u0302\u03b9) Pr(A, \u03b8\u0302\u03b9) (Astep, we use Algorithm 2 here.)\n\u03b8\u0302\u03b9+1 \u2190 arg max\u03b8\u2208\u0398 L(t|A\u0302\u03b9, \u03b8) Pr(A\u0302\u03b9, \u03b8) (\u03b8-step) \u03b9\u2190 \u03b9+ 1\nend while A\u0302\u2190 A\u0302\u03b9max\u22121 \u03b8\u0302 \u2190 \u03b8\u0302\u03b9max\nProblem. Given the observed data Y = (GS ,d, t,C), we seek an n \u00d7 n adjacency matrix A (symmetric, binary and zero-diagonal) and a parameter value \u03b8 \u2208 \u0398 that\nmaximizes L(t|A, \u03b8) Pr(A, \u03b8) subject to A \u2265 AR (entrywise)\nA \u00b7 1 \u2264 d (entrywise)\nAlternating Inference of A and \u03b8 Given the observed data Y, we wish to infer the adjacency matrix A of the recruitment-induced graph and the distribution parameter \u03b8. Given A, the maximum likelihood estimator (MLE) for \u03b8 is\n\u03b8\u0302 = arg max\u03b8\u2208\u0398 L(t|A, \u03b8) Pr(A, \u03b8). Similarly, given the true parameter \u03b8, the MLE for the adjacency matrix A is given by\nA\u0302 = arg maxA is compatible L(t|A, \u03b8) Pr(A, \u03b8).\nIn practice, both the parameter \u03b8 and the true recruitmentinduced subgraph GS are unknown and need estimation. However, we can alternately estimate A and \u03b8. This is what RENDER (presented in Algorithm 1) does. Each iteration is divided into two steps: A-step and \u03b8-step.\nEstimation of A using simulated annealing The A-step of RENDER solves\nmax A is compatible\nL(t|A, \u03b8\u0302\u03b9) Pr(A, \u03b8\u0302\u03b9);\nequivalently, it suffices to solve\nmax A is compatible\n(l(t|A, \u03b8\u0302\u03b9) + log Pr(A, \u03b8\u0302\u03b9)).\nWe employ a simulated-annealing-based method to estimate the adjacency matrix A (presented in Algorithm 2). Let the energy function be\n\u039b\u03b3(A; t, \u03b8\u0302\u03b9) , exp [ \u2212 ( l(t|A, \u03b8\u0302\u03b9) + log Pr(A, \u03b8\u0302\u03b9) ) /\u03b3 ] ,\nwhere \u03b3 is the temperature. We specify a cooling schedule, which is a sequence of positive numbers {\u03b3}\u22651 that satisfy\nlim\u2192\u221e \u03b3 = 0, where \u03b3 is the temperature in the -th iteration. Note that the -th iteration of the simulated annealing procedure has a compatible adjacency matrix A() as its state. Algorithm 3 specifies which state (compatible ad-\nAlgorithm 2 Simulated-annealing-based optimization Input: the number of iterations, max; the cooling schedule {\u03b3}\u22651; initial compatible adjacency matrix A(1); estimated parameter \u03b8\u0302\u03b9. Output: the estimated adjacency matrix A\u0302\u03b9 for  = 1 to max do\nUse Algorithm 3 to propose a compatible adjacency matrix A\u0303(+ 1) based on A(). \u03c8 \u2190 min { 1, \u039b\u03b3 (A\u0303(+1);t,\u03b8\u0302\u03b9)\n\u039b\u03b3 (A();t,\u03b8\u0302\u03b9) \u00b7 Pr(A()|A\u0303(+1)) Pr(A\u0303(+1)|A())\n} .\nA(+ 1)\u2190 { A\u0303(+ 1) with probability \u03c8; A() with probability 1\u2212 \u03c8.\nend for A\u0302\u03b9 \u2190 A(max + 1).\njacency matrix) the algorithm should transition into in the next iteration. Concretely, in each iteration of Algorithm 2, it randomly proposes an edge that connects vertices i and j. If the edge does not appear in A() and it still conforms to the degree constraint if we add the edge, then we simply add it to A\u0303( + 1). In contrast, if the edge appears in A() and it still conforms to the subgraph constraint if we remove the edge, then we simply remove it from A\u0303( + 1). If neither condition is satisfied, the algorithm tries again with a new proposal. We prove in the appendix that the space of compatible adjacency matrices is connected by the proposal method.\nThe proposed compatible adjacency matrix A\u0303( + 1) is accepted as the state for the next iteration with probability \u03c8, where \u03c8 equals\nmin { 1, \u039b\u03b3(A\u0303(+ 1); t, \u03b8\u0302\u03b9)\n\u039b\u03b3(A(); t, \u03b8\u0302\u03b9) \u00b7 Pr(A()|A\u0303(+ 1)) Pr(A\u0303(+ 1)|A())\n} ;\notherwise, the matrix A() remains the state for the next iter-\nation. The term \u039b\u03b3 (A\u0303(+1);t,\u03b8\u0302\u03b9)\n\u039b\u03b3 (A();t,\u03b8\u0302\u03b9) is called the likelihood ratio\nand the term Pr(A()|A\u0303(+1)) Pr(A\u0303(+1)|A()) is called the proposal ratio. To implement Algorithm 2, we have to compute the likelihood ratio and the proposal ratio efficiently. In fact, they can be evaluated efficiently in a recursive manner. Theorem 2. (Proof in Appendix). The proposal ratio Pr(A()|A\u0303(+1)) Pr(A\u0303(+1)|A()) is given by\nAdd(A()) + Remove(A())\nAdd(A\u0303(+ 1)) + Remove(A\u0303(+ 1)) , where Add(A) = \u2211 1\u2264i<j\u2264n 1{Aij = 0, \u2211n k=1 Aik <\ndi, \u2211n k=1 Ajk < dj}, and Remove(A) =\u2211\n1\u2264i<j\u2264n 1{Aij = 1 and A (ij) R = 0}. Here, we let\nA (ij) R denote the (i, j)-entry of the matrix AR.\nAlgorithm 3 Proposal of compatible adjacency matrix Input: the compatible adjacency matrix A() in the -th iteration. Output: a compatible adjacency matrix A\u0303(+1), which will be a candidate for the state in the (+ 1)-th iteration. loop\ni, j \u2190 two distinct random integers in [1, n] \u2229 N if Aij() = 0 and \u2211n k=1 Aik() < di and\u2211n\nk=1 Ajk() < dj then A\u0303+(+ 1)\u2190 A() A\u0303+ij(+ 1)\u2190 1 and A\u0303 + ji(+ 1)\u2190 1\nA\u0303(+ 1)\u2190 A\u0303+(+ 1) and exit loop else\nif Aij() = 1 and A (ij) R = 0 (A (ij) R is the (i, j)-\nentry of the matrix AR) then A\u0303\u2212(+ 1)\u2190 A() A\u0303\u2212ij(+ 1)\u2190 0 and A\u0303 \u2212 ji(+ 1)\u2190 0\nA\u0303(+ 1)\u2190 A\u0303\u2212(+ 1) and exit loop end if\nend if end loop return A\u0303(+ 1)\nThe same way, we can find the likelihood ratio in a recursive manner. Theorem 3. (Proof in Appendix). If we view \u03b2 in Theorem 1 as a function of the adjacency matrix A, denoted by \u03b2(A), then the recurrence relation between \u03b2(A\u0303(+1)) and \u03b2(A()) is as follows:\n(e\u03b2(A\u0303(+1)) \u2212 e\u03b2(A()))j = \u00b1 (Bbj1{a < j} \u2212Baj1{b < j}) . (1)\nHere, we assign the minus sign \u201c\u2212\u201d in \u201c\u00b1\u201d if A\u0303( + 1) = A\u0303+( + 1), and assign the plus sign \u201c+\u201d in \u201c\u00b1\u201d if A\u0303( + 1) = A\u0303\u2212( + 1). By the same convention, the likelihood ratio \u039b\u03b3(A\u0303(+ 1)|t, \u03b8\u0302\u03b9)/\u039b\u03b3(A()|t, \u03b8\u0302\u03b9) is given by\nexp { \u2212\u03b3\u22121 [ \u2212 log Pr(A\u0303(+ 1))\nPr(A()) + m\u2032\n( \u03b2(A\u0303(+ 1))\n\u2212\u03b2(A()))\u00b1 n\u2211 j=1 (Dbj1{a < j} \u2212Daj1{b < j})  , Estimation of distribution parameter In a \u03b8-step in Algorithm 1, we have to solve the optimization problem \u03b8\u0302\u03b9+1 \u2190 arg max\u03b8\u2208\u0398 L(\u03b8|A\u0302\u03b9, t) Pr(A\u0302\u03b9, \u03b8). If the parameter space \u0398 is a subset of the p-dimensional Euclidean space Rp, this problem can be solved using off-the-shelf solvers, e.g., FMINSEARCH in MATLAB."}, {"heading": "Experiments", "text": "We evaluated the proposed method in two aspects, the reconstruction performance of the recruitment-induced sub-\ngraph and the parameter estimation of the edgewise interrecruitment time model. Let A be the adjacency matrix of the true recruitment-induced subgraph GS and A\u0302 be the estimate. We define the true and false positive rates (TPR and FPR) as TPR(A\u0302,A) = \u2211 i<j 1{A\u0302ij = 1 and Aij =\n1}/ ( n 2 ) and FPR(A\u0302,A) = \u2211 i<j 1{A\u0302ij = 1 and Aij =\n0}/ ( n 2 ) . Fig. 1 illustrates an example of the reconstruction procedure. We simulated a RDS process over the Project 90 graph (Woodhouse et al. 1994) with power-law edgewise inter-recruitment time distribution, whose shape parameter \u03b1 = 2 and scale parameter xmin = 0.5. Fifty subjects are recruited in this process. We show clockwise from top left: the true recruitment-induced subgraphGS , the observed recruitment graph GR, the estimated network with recruitments as blue arrows and dashed lines as inferred edges, and the estimated recruitment-induced subgraph G\u0302S . The TPR equals 0.769 and the FPR equals 0.106. The estimated parameter \u03b1\u0302 = 1.95 and x\u0302min = 0.49."}, {"heading": "Reconstruction Performance", "text": "Impact of distribution parameter We simulated 50 RDS over the Project 90 graph with inter-recruitment time distribution Gamma(\u03b1, \u03b1) (parametrized by the shape and scale) for each \u03b1 = 0.01, 0.1, 1, 10, and 100. Thus a total number of 250 RDS processes are simulated and the mean interrecruitment time is fixed to be 1. The reconstruction performance is illustrated in Fig. 2. Each point on the receiver operating characteristic (ROC) plane represents a reconstruction accuracy performance of a simulated RDS process.\n\u22120.1\n0.0\n0.1\n0.5 0.75 1 1.25 1.5 \u03b1\nPR (E\nxp ) \u2212\nP R\n(G am\nm a)\nPR FPR TPR\nPR Comparison between true and exponential model\nFigure 3: Difference between true and false positive rates (green and red boxplots, respectively) under the exponential model and the true Gamma model (the vertical axis) with the true shape parameter \u03b1 (the horizontal axis).\nPoints with the same marker have the same inter-recruitment time distribution parameter. From the figure, we can observe that there is no significant sign of separation of points with different inter-recruitment time distribution parameters. Reconstruction accuracy is robust to the distribution parameter.\nImpact of distribution model We simulated 50 RDS process over the Project 90 graph with inter-recruitment time distribution Gamma(0.5, 0.5). The recruitment-induced subgraph is reconstructed via the model of the true inter-\nrecruitment distribution Gamma(0.5, 0.5) and the exponential distribution Exp(1), respectively. The TPR and FPR of each dataset are presented in Fig. 3. The TPR is always higher than the FPR, which reaffirms the effectiveness of our proposed reconstruction method. For each dataset, the TPR and FPR under the true and the exponential models are very close to each other. We observe that there is no significant reconstruction skewness incurred by mis-specification of the inter-recruitment time distribution model."}, {"heading": "Parameter Estimation", "text": "We simulated 200 RDS processes over the Project 90 graph with edgewise inter-recruitment time distribution Gamma(\u03b1, \u03b1) (parametrized by the shape and scale parameters) for each \u03b1 = 0.5, 0.75, 1, 1.25, and 1.5. We used the method in the \u201cEstimation of distribution parameter\u201d section. We assess the bias of the estimated shape parameter \u03b1\u0302, which is given by \u03b1\u0302\u2212\u03b1. Fig. 4 shows the distribution of the bias using Tukey boxplots. In Fig. 4, the red boxes depict the distribution of the biases of the estimated shape parameters inferred through the estimated adjacency matrix, while the green boxes illustrate the distribution of those inferred given the true adjacency matrix. The horizontal axis shows the value of the true shape parameter.\nWith respect to the red boxes (those based on the estimated adjacency matrices), The middle line of each box is very close to zero and thus the estimation is highly accurate. The interquartile range (IQR) , which measures the deviation of the biases, declines as the shape parameter decreases. Even for the box with largest deviation (i.e., the box with \u03b1 = 1.5), the IQR is approximately [\u22120.125, 0.02]\nand 99.3% of the biases reside in the interval [\u22120.27, 0.25]. Compared with the parameter estimation via the true adjacency matrix, this estimator based on the estimated adjacency matrix is biased to some degree. In Fig. 4, we can observe that it underestimates \u03b1.\nThen consider the green boxes (those based on the true adjacency matrix). Similar to those based on the estimated adjacency matrix, the deviation of this estimator declines as the value of the shape parameter \u03b1 decreases. The middle line of each box is noted to coincide perfectly with zero bias line, which suggests that this estimator is unbiased given the true adjacency matrix."}, {"heading": "Experiments on Real Data", "text": "We also apply RENDER to data from an RDS study of n = 813 drug users in St. Petersburg, Russian Federation. We use RENDER to infer the underlying social network structure of the drug users in this study. Since it could be confusing to visualize the whole inferred network, we only show the inferred subgraph of the largest community of the network, as presented in Fig. 5. The blue arrows represent the edges in the recruitment subgraph that indicates the recruiter and recruitee. Gray dashed edges are inferred from the data."}, {"heading": "Conclusion", "text": "In this paper, we precisely formulated the dynamics of RDS as a continuous-time diffusion process over the underlying graph. We derived the likelihood for the recruitment time series under an arbitrarily recruitment time distribution. As a result, we develop an efficient stochastic optimization algorithm, RENDER, that identifies the optimum network that best explains the collected data. We then supported the performance of RENDER through an exhaustive set of experiments on both synthetic and real data."}, {"heading": "Acknowledgements", "text": "FWC was supported by NIH/NCATS grant KL2 TR000140 and NIMH grant P30MH062294. LC would like to thank Zeya Zhang and Yukun Sun for their encouragement."}, {"heading": "Appendix", "text": ""}, {"heading": "Proof of Theorem 1", "text": "Proof. (1) We consider the recruitment of subject i. Let Ru(i) be the set of recruiters of subject u just before time ti and Iu(i) be the set of recruitees of recruiter u just before time ti. Suppose that i /\u2208M . The inter-recruitment time between i and its potential recruiter u is denotedWui = ti\u2212tu and is greater than ti\u22121\u2212 tu conditional on previous recruitment of i. Let U be the random variable of next recruiter and X be the random variable of next recruitee. Let J denote the event \u2200j \u2208 R(i), k \u2208 I(i),Wjk > ti\u22121 \u2212 tj . We have\nPr [U = u,X = x,Wux \u2265 t\u2212 tu | J ] = Pr [Wux \u2265 t\u2212 tu, tj +Wjk > tu +Wux, \u2200j \u2208 R(i), k \u2208 I(i), {u, x} 6= {j, k} | J ] ,\nand\nPr [WUi \u2265 t\u2212 tU | J ] = \u2211 x\u2208I(i) \u2211 u\u2208Rx(i) Pr [Wux \u2265 t\u2212 tu, tj +Wjk > tu\n+Wux,\u2200j \u2208 R(i), k \u2208 I(i), {u, x} 6= {j, k} | J ] = \u2211 x\u2208I(i) \u2211 u\u2208Rx(i) \u02c6 \u221e t\u2212tu f\u03c4(u;i)(s)dsPr [Wjk > s+ tu\n\u2212tj ,\u2200j \u2208 R(i), k \u2208 I(i), {u, x} 6= {j, k} | J ] = \u2211 x\u2208I(i) \u2211 u\u2208Rx(i) \u02c6 \u221e t\u2212tu\nf\u03c4(u;i)(s) \u00b7\u220f j\u2208R(i) \u220f k\u2208Ij(i) ( 1\u2212 F\u03c4(j;i)(s+ tu \u2212 tj)\n) 1\u2212 F\u03c4(u;i)(s) ds\n= \u2211 x\u2208I(i) \u2211 u\u2208Rx(i) \u02c6 \u221e t\u2212tu\nH\u03c4(u;i)(s) \u00b7\u220f j\u2208R(i) S |Ij(i)| \u03c4(j;i) (s+ tu \u2212 tj)ds\nThus the likelihood is\u2211 x\u2208I(i) \u2211 u\u2208Rx(i) H\u03c4(u;i)(ti \u2212 tu) \u220f j\u2208R(i) S |Ij(i)| \u03c4(j;i) (ti \u2212 tj)\n= \u220f\nj\u2208R(i)\nS |Ij(i)| \u03c4(j;i) (ti \u2212 tj) \u2211 u\u2208R(i) |Iu(i)|H\u03c4(u;i)(ti \u2212 tu)\nNow suppose that i \u2208M .\nPr [Wjk \u2265 t\u2212 tj ,\u2200j \u2208 R(i), k \u2208 Ij(i) | J ] = \u220f\nj\u2208R(i) \u220f k\u2208Ij(i) (1\u2212 F\u03c4(j;i)(t\u2212 tj))\n= \u220f\nj\u2208R(i)\nS |Ij(i)| \u03c4(j;i) (t\u2212 tj).\nThus the likelihood is\u220f j\u2208R(i) S |Ij(i)| \u03c4(j;i) (ti \u2212 tj).\nTherefore the entire likelihood is\nn\u220f i=1  \u2211 u\u2208R(i) |Iu(i)|H\u03c4(u;i)(ti \u2212 tu) 1{i/\u2208M} \u00b7 \u220f\nj\u2208R(i)\nS |Ij | \u03c4(j;i)(ti \u2212 tj).\n(2) The log-likelihood is\nn\u2211 i=1 1{i /\u2208M} log  \u2211 u\u2208R(i) |Iu(i)|H\u03c4(u;i)(ti \u2212 tu)  + \u2211 j\u2208R(i) |Ij(i)| logS\u03c4(j;i)(ti \u2212 tj)\n . And the number of recruitees of recruiter u just before time ti is given by\n|Ij(i)| = Cji ( n\u2211 k=i Ajk + uj ) .\nThe term \u2211 u\u2208R(i) |Iu(i)|H\u03c4(u;i)(ti \u2212 tu) in the loglikelihood is given by\n\u2211 u\u2208R(i) |Iu(i)|H\u03c4(u;i)(ti \u2212 tu)\n= \u2211 u Cui ( n\u2211 k=i Auk + uu ) Hui =(B\u2032u + LowerTri(AB)\u2032 \u00b7 1)i.\nThe term \u2211 j\u2208R(i) |Ij(i)| logS\u03c4(j;i)(ti \u2212 tj) in the loglikelihood is given by\n\u2211 j\u2208R(i) |Ij(i)| logS\u03c4(j;i)(ti \u2212 tj)\n= \u2211 j\u2208R(i) Cji ( n\u2211 k=i Ajk + uj ) Sji\n=(D\u2032u + LowerTri(AD)\u2032 \u00b7 1)i.\nThus the log-likelihood is n\u2211 i=1 [1{i /\u2208M} log (B\u2032u + LowerTri(AB)\u2032 \u00b7 1)i\n+ (D\u2032u + LowerTri(AD)\u2032 \u00b7 1)i] = m \u2032\u03b2 + 1\u2032\u03b4.\nProof of connectedness of the space of compatible adjacency matrices Proof. Consider two compatible adjacency matrices A1 and A2. Their Hadamard product A1\u25e6A2, which corresponds to\nthe maximum common subgraph of A1 and A2, is also compatible. We can remove the edges in A1 that do not appear in A1\u25e6A2 one at a time, and thus arrive at state A1\u25e6A2. Then we add the edges in A2 that do not appear in A1 \u25e6A2 once at a time, and thus finally arrive at state A2. All intermediate adjacency matrices in this procedure are compatible."}, {"heading": "Proof of Theorem 2", "text": "Proof. In fact, the total number of possible edges that can be added for A() is given by Add(A()) and the total number of possible edges that can be removed for A() is given by Remove(A()). Thus we have the proposal distribution\nPr(A\u0303(+ 1)|A()) = 1 Add(A()) + Remove(A()) .\nSimilarly, we have\nPr(A()|A\u0303(+ 1)) = 1\nAdd(A\u0303(+ 1)) + Remove(A\u0303(+ 1)) . (2)\nThus we obtain the proposal ratio:\nPr(A()|A\u0303(+ 1)) Pr(A\u0303(+ 1)|A())\n= 1/ ( Add(A\u0303(+ 1)) + Remove(A\u0303(+ 1)) )\n1/ (Add(A()) + Remove(A()))\n= Add(A()) + Remove(A())\nAdd(A\u0303(+ 1)) + Remove(A\u0303(+ 1)) ."}, {"heading": "Proof of Theorem 3", "text": "Proof. Let Jab denote the n \u00d7 n matrix whose (a, b)-entry is one and all other entries are zeros. We have\nA\u0303(+ 1)\u2212A() = \u00b1 ( Jab + Jba ) ,\nwhere we assign the minus sign \u201c\u2212\u201d in \u201c\u00b1\u201d if the edge that connects subjects a and b is added to A\u0303(+1) and we assign the plus sign \u201c+\u201d in \u201c\u00b1\u201d if that edge is removed from A(). And e\u03b2(A) can be re-written as\ne\u03b2(A)\n=B\u2032u + LowerTri(AB)\u2032 \u00b7 1 =B\u2032(d\u2212A \u00b7 1) + LowerTri(AB)\u2032 \u00b7 1 =B\u2032d\u2212B\u2032A \u00b7 1 + LowerTri(AB)\u2032 \u00b7 1 =B\u2032d\u2212B\u2032A\u2032 \u00b7 1 + LowerTri(AB)\u2032 \u00b7 1 (3) =B\u2032d\u2212 (AB)\u2032 \u00b7 1 + LowerTri(AB)\u2032 \u00b7 1 =B\u2032d\u2212 StrictlyUpperTri(AB)\u2032 \u00b7 1,\nwhere in (3) we use the fact that A is symmetric and StrictlyUpperTri(\u00b7) denotes the strictly upper triangular part (diagonal entries exclusive) of a matrix. Thus\ne\u03b2(A)\u00b1J ab \u2212 e\u03b2(A) = \u00b1StrictlyUpperTri(JabB)\u2032 \u00b7 1.\nThe (i, j)-entry of JabB is\n(JabB)ij\n= n\u2211 k=1 JabikBkj\n=JabibBbj .\nThus ( StrictlyUpperTri(JabB)\u2032 \u00b7 1 ) j\n= \u2211 i:i<j JabibBbj\n=Bbj1{a < j}.\nHence we have\n(e\u03b2(A\u0303(+1)) \u2212 e\u03b2(A()))j = \u00b1 (Bbj1{a < j} \u2212Baj1{b < j}) . (4)\nSimilarly, if we view \u03b4 in Theorem 1 as a function of the adjacency matrix A, denoted by \u03b4(A), we have\n\u03b4(A\u0303(+ 1))\u2212 \u03b4(\u03b2(A())) = \u00b1 (Dbj1{a < j} \u2212Daj1{b < j}) . (5)\nIf we plug in Equations (4) and (5) in the likelihood ratio \u039b\u03b3 (A\u0303(+1);t,\u03b8\u0302\u03b9)\n\u039b\u03b3 (A();t,\u03b8\u0302\u03b9) , we obtain the expression in the theorem\nstatement."}], "references": [{"title": "F", "author": ["Chen, L.", "Crawford"], "venue": "W.; and Karbasi, A.", "citeRegEx": "Chen. Crawford. and Karbasi 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "F", "author": ["Crawford"], "venue": "W.", "citeRegEx": "Crawford 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "M", "author": ["K.J. Gile", "Handcock"], "venue": "S.", "citeRegEx": "Gile and Handcock 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "M", "author": ["S. Goel", "Salganik"], "venue": "J.", "citeRegEx": "Goel and Salganik 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "G", "author": ["M. Gomez Rodriguez", "D. Balduzzi", "B. Sch\u00f6lkopf", "Scheffer"], "venue": "T.; et al.", "citeRegEx": "Gomez Rodriguez et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Inferring networks of diffusion and influence", "author": ["Leskovec Gomez Rodriguez", "M. Krause 2010] Gomez Rodriguez", "J. Leskovec", "A. Krause"], "venue": "In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Rodriguez et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rodriguez et al\\.", "year": 2010}, {"title": "D", "author": ["Heckathorn"], "venue": "D.", "citeRegEx": "Heckathorn 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "D", "author": ["Heckathorn"], "venue": "D.", "citeRegEx": "Heckathorn 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "E", "author": ["M.A. Kramer", "U.T. Eden", "S.S. Cash", "Kolaczyk"], "venue": "D.", "citeRegEx": "Kramer et al. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "and Kleinberg", "author": ["D. Liben-Nowell"], "venue": "J.", "citeRegEx": "Liben.Nowell and Kleinberg 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "and Adams", "author": ["S. Linderman"], "venue": "R.", "citeRegEx": "Linderman and Adams 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "D", "author": ["M.J. Salganik", "Heckathorn"], "venue": "D.", "citeRegEx": "Salganik and Heckathorn 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "M", "author": ["Salganik"], "venue": "J.", "citeRegEx": "Salganik 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "and Timme", "author": ["S.G. Shandilya"], "venue": "M.", "citeRegEx": "Shandilya and Timme 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "D", "author": ["E. Volz", "Heckathorn"], "venue": "D.", "citeRegEx": "Volz and Heckathorn 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "J", "author": ["D.E. Woodhouse", "R.B. Rothenberg", "J.J. Potterat", "W.W. Darrow", "S.Q. Muth", "A.S. Klovdahl", "H.P. Zimmerman", "H.L. Rogers", "T.S. Maldonado", "Muth"], "venue": "B.; et al.", "citeRegEx": "Woodhouse et al. 1994", "shortCiteRegEx": null, "year": 1994}], "referenceMentions": [], "year": 2015, "abstractText": "Learning about the social structure of hidden and hard-toreach populations \u2014 such as drug users and sex workers \u2014 is a major goal of epidemiological and public health research on risk behaviors and disease prevention. Respondentdriven sampling (RDS) is a peer-referral process widely used by many health organizations, where research subjects recruit other subjects from their social network. In such surveys, researchers observe who recruited whom, along with the time of recruitment and the total number of acquaintances (network degree) of respondents. However, due to privacy concerns, the identities of acquaintances are not disclosed. In this work, we show how to reconstruct the underlying network structure through which the subjects are recruited. We formulate the dynamics of RDS as a continuous-time diffusion process over the underlying graph and derive the likelihood of the recruitment time series under an arbitrary inter-recruitment time distribution. We develop an efficient stochastic optimization algorithm called RENDER (REspoNdent-Driven nEtwork Reconstruction) that finds the network that best explains the collected data. We support our analytical results through an exhaustive set of experiments on both synthetic and real data. Introduction Random sampling is an effective way for researchers to learn about a population of interest. Characteristics of the sample can be generalized to the population of interest using standard statistical theory. However, traditional random sampling may be impossible when the target population is hidden, e.g. drug users, men who have sex with men, sex workers and homeless people. Due to concerns like privacy, stigmatization, discrimination, and criminalization, members of hidden populations may be reluctant to participate in a survey. In addition to the hidden populations for which no sampling \u201cframe\u201d exists, there are rare populations of great research interest, marked by their tiny fractions in the entire population. It is unlikely that random sampling from the general population would accrue a reasonable sample from a very rare population. However, it is often the case that members of hidden or rare populations know each other socially. This suggests that social referral would be an effective method for accruing a large sample. To this end, researchers have developed a variety of social link-tracing survey designs. The most popular is respondent-driven sampling (RDS) (Heckathorn 1997). In RDS, a small set of initial subjects, known as \u201cseeds\u201d are selected, not necessarily simultaneously, from the target population. Subjects are given a few coupons, each tagged with a unique identifier, which they can use to recruit other eligible subjects. Participants are given a reward for being interviewed and for each eligible subject they recruit using their coupons. Each subject reports their degree, the number of others whom they know in the target population. No subject is permitted to enter the study twice and the date and time of each interview is recorded. While RDS can be an effective recruitment method, it reveals only incomplete social network data to researchers. Any ties between recruited subjects along which no recruitment took place remain unobserved. The social network of recruited subjects is of great interest to sociologists, epidemiologists and public health researchers since it may induce dependencies in the outcomes of sampled individuals. Fortunately, RDS reveals information about the underlying social network that can be used to (approximately) reconstruct it. By leveraging the time series of recruitments, the degrees of recruited subjects, coupon information, and who recruited whom, it is possible to interpret the induced subgraph of RDS respondents as a simple random graph model (Crawford 2016). In this paper, we introduce a flexible and expressive stochastic model of RDS recruitment on a partially observed network structure. We derive the likelihood of the observed time series; the model admits any edgewise inter-recruitment time distribution. We propose a stochastic optimization algorithm RENDER (REspoNdent-Driven nEtwork Reconstruction) to estimate unknown parameters and the underlying social network. We conduct extensive experiments, on synthetic and real data, to confirm the accuracy and the reconstruction performance of RENDER. In particular, we apply RENDER to reconstruct the network of injection drug users from an RDS study in St. Petersburg, Russia. Related Work RDS has been modeled as a random walk with replacement on a graph at its equilibrium distribution (Heckathorn 1997; Salganik and Heckathorn 2004; Volz and Heckathorn 2008; Goel and Salganik 2009; Gile and Handcock 2010), and under this argument the sampling probability is proportional to degree, which is the basis for an estimator of the populaar X iv :1 51 1. 04 13 7v 2 [ cs .S I] 2 D ec 2 01 5 tion mean (Heckathorn 2002; Salganik 2006). In this paper, we adopt an approach that focuses on the network structure estimable from the RDS sample using recruitment information (Crawford 2016). Crawford (2016) assumes that edgewise inter-recruitment times follow the exponential distribution, but this approach is relatively inflexible. In many other contexts, dynamic or random processes can reveal structural information of an underlying, partially observed, network (Kramer et al. 2009; Shandilya and Timme 2011; Linderman and Adams 2014). Network reconstruction and the edge prediction problem have been studied for diffusion processes where multiple realizations of the process on the same network are available (Liben-Nowell and Kleinberg 2007; Gomez Rodriguez, Leskovec, and Krause 2010; Gomez Rodriguez et al. 2011). In the case of RDS, reconstruction is particularly challenging because only a single realization of the diffusion process is observed. Furthermore, we must account for the role of coupons in recruitment as they intricately introduce bias. However, in contrast to general diffusion processes over graphs, some important network topological information is revealed by RDS. In this study, we leverage all the available data routinely collected during real-world RDS studies.", "creator": "LaTeX with hyperref package"}}}