{"id": "1511.04326", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2015", "title": "ICON Challenge on Algorithm Selection", "abstract": "We present the results of the ICON Challenge on Algorithm Selection. We will be using a high-quality, low-cost, efficient, and efficient algorithm design to make use of these results in an effective way to reduce the risk of errors and improve the quality of our services. We will also use this method to develop an efficient, cost-effective, efficient, and efficient system that helps users develop and improve their overall experience. The ICON Challenge was started in 2008 by a group of engineers and engineers from the University of Southern California, Los Angeles. This project is focused on improving the user experience in a way that makes it easier for the user to work with the ICON Challenge system as well. We are also partnering with the University of California San Diego, and in order to accelerate the process to a more sustainable and efficient use of our services, we are expanding the ICON Challenge system into the new system, creating a new, efficient, and efficient system that helps consumers and businesses grow and improve their experience.", "histories": [["v1", "Thu, 12 Nov 2015 20:04:31 GMT  (199kb,D)", "http://arxiv.org/abs/1511.04326v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["lars kotthoff"], "accepted": false, "id": "1511.04326"}, "pdf": {"name": "1511.04326.pdf", "metadata": {"source": "CRF", "title": "ICON Challenge on Algorithm Selection", "authors": [], "emails": [], "sections": [{"heading": null, "text": "ICON Challenge on Algorithm Selection\nhttp://challenge.icon-fet.eu\nLars Kotthoff\nNovember 16, 2015"}, {"heading": "1 Submissions", "text": "The challenge received a total of 8 submissions from 4 different groups of researchers comprising 15 people. Participants were based in 4 different countries on 2 continents. Table 1 gives an overview of all submissions.\nAll submissions were submitted for evaluation on all ASlib scenarios. Most systems used a presolver and specified a subset of features to use for each scenario."}, {"heading": "2 Evaluation", "text": "The evaluation was performed as follows. For each scenario, 10 bootstrap samplings of the entire data were used to create 10 different train/test splits. No stratification was used. The training part was left unmodified. For the test part, algorithm performances were set to 0 and runstatus to \u201cok\u201d for all algorithms and all instances \u2013 the ASlib specification requires algorithm performance data to be part of a scenario. A cv.arff file was generated for both training and\nSystem name Presolving/feature selection used?\nASAP kNN no ASAP RF no autofolio yes flexfolio-schedules yes sunny no sunny-presolv yes zilla yes zillafolio yes\nTable 1: Systems submitted to the challenge.\nar X\niv :1\n51 1.\n04 32\n6v 1\n[ cs\n.A I]\n1 2\nN ov\n2 01\n5\ntesting with 10 folds and the instances assigned to folds by the order in which they appeared in the original scenario.\nFor systems that specified a presolver, the instances that were solved by the presolver within the specified time were removed from the training set. If a subset of features was specified, only these features (and only the costs associated with these features) were left in both training and test set, with all other feature values removed.\nEach system was trained on each train scenario and predicted on each test scenario. In total, 130 evaluations (10 for each of the 13 scenarios) per submitted system were performed. The total CPU time spent was 4685.11 hours.\nThe predictions were evaluated as follows. If a presolver was specified, it was \u201crun\u201d for the specified time. If the instance was solved within this time, the time to solve the instance was taken as the performance on that instance and the instance recorded as solved.\nOtherwise, the time limit given for the presolving run was added to the time required to compute all features specified for the particular scenario. For any instances that were solved during feature computation, the instance was recorded as solved at this point and the time for the presolving run plus feature computation recorded as the performance. The misclassification penalty was set to 0 in this case regardless of the performance of the best solver.\nFor instances not solved during feature computation, the solvers specified in the prediction schedule of the system were \u201crun\u201d. For each instance, the predicted solvers were ordered by the runID specified. If a run was unable to solve an instance, the smaller of time the schedule specified to run it for and the time it actually took to run on the instance was added to the total. If a run solved the respective instance, the actual time required by the algorithm was added to the total and the instance recorded as solved. If the total time exceeded the time limit for the scenario, an instance was recorded as not solved.\nEach system was evaluated in terms of mean PAR10 score, mean misclassification penalty, and mean number of instances solved for each of the 130 evaluations on each scenario and split.\nTo facilitate comparison of the different measures across the different scenarios, all measures were normalised by the performance of the virtual best (VBS) and the single best (SB) solver. The single best solver was determined as the solver with the smallest overall runtime across all instances. Equation 1 defines the normalisation of a score s.\nsnorm = s\u2212 sV BS\nsSB \u2212 sV BS (1)\nThis normalises the score to the interval 0 (VBS) to 1 (SB), with smaller values being better. The number denotes how much of the gap between single best and virtual best solver was left by the system.\nTo determine the overall winner, the mean across all of the normalised measurements was taken. For each submitted system, 390 scores were taken into account for this (13 scenarios times 10 splits times 3 measures)."}, {"heading": "3 Results", "text": "Table 2 shows the final ranking. The first and second placed entries are very close. All systems perform well on average, closing more than half of the gap between virtual and single best solver.\nFor comparison, we show three other systems. Autofolio-48 is identical to Autofolio, but was allowed 48 hours training time to assess the impact of additional exploration of the hyperparameter space. Llama-regrPairs and llama-regr are simple llama models (see Appendix A).\nTo assess how significant the difference are and how stable the ranking is, we took 1 000 bootstrap samples from the scenario-split combinations and computed the scores and ranks on each of them. The mean average of the total score averages over the bootstrap samples and the confidence intervals are show in Table 3.\nThe ranking is the same as the final ranking in Table 2. The confidence intervals show that the rankings are relatively stable."}, {"heading": "3.1 Winner \u2013 zilla", "text": "The winner of the ICON Challenge on Algorithm Selection is zilla by Chris Cameron, Alex Fre\u0301chette, Holger Hoos, Frank Hutter, and Kevin Leyton-Brown."}, {"heading": "3.2 Honourable mention \u2013 ASAP RF", "text": "ASAP RF by Fran\u0327cois Gonard, Marc Schoenauer, and Miche\u0300le Sebag receives an honourable mention as a submission that has not been described in the literature before and showed respectable performance, beating all other approaches in some cases."}, {"heading": "3.3 Alternative rank aggregations", "text": "An alternative (and probably fairer) way of determining the winner is to see the ranking of systems induced by each measure on each split of each scenario as a ballot (for a total of 260 ballots) and aggregate the ranks in those ballots. Here, we optimise the aggregated Spearman coefficient between candidate rankings and ballot rankings. That is, the final ranking has the optimal Spearman coefficient with respect to the ballots.\nThere are significant changes however when averaging the performance across all measures, splits, and scenarios by median rather than mean. Table 5 shows this ranking. Zilla is now in second position, beat by ASAP RF."}, {"heading": "3.4 Detailed results", "text": "Tables 6 through 8 show the rankings by mean score across all splits and scenarios, but separately for each measure.\nTable 9 shows the ranks for the different scenarios for all systems by mean across all measures and splits.\nFigures 1 through 3 give a more detailed overview of the performance of the systems on the different scenarios. The colour of each boxplot denotes the system, the mean performance of which is shown in the legend (this corresponds to the number in the respective table above). The boxplot shows the variation of performance across the 10 different splits for each scenario. The solid black line denotes the performance of the single best solver; anything above is worse.\nTwo of the SAT scenarios are hard for all systems in the sense that the performance they deliver on at least one of the splits is worse than the performance of the single best solver. For most other scenarios, using any algorithm selection system gives a significant performance improvement compared to the single best solver though."}, {"heading": "3.5 Time required to run", "text": "The time required to train the models and make the predictions varied significantly across systems and scenarios, with some completing in minutes and others requiring hours. Figure 4 presents a summary."}, {"heading": "4 Acknowledgements", "text": "We would like to thank all the participants for taking the time to prepare submissions and their help in getting them to run; in alphabetical order: Alex Fre\u0301chette, Chris Cameron, David Bergdoll, Fabio Biselli, Fran\u0327cois Gonard, Frank Hutter, Holger Hoos, Jacopo Mauro, Kevin Leyton-Brown, Marc Schoenauer, Marius Lindauer, Miche\u0300le Sebag, Roberto Amadini, Tong Liu, and Torsten Schaub. We thank Barry Hurley for setting up and maintaining the submission website and Luc De Raedt, Siegfried Nijssen, Benjamin Negrevergne, Behrouz Babaki, Bernd Bischl, and Marius Lindauer for feedback on the design of the challenge.\nAll data, code and results from the challenge are available at http://4c. ucc.ie/~larsko/downloads/challenge.tar.gz."}, {"heading": "A Llama models used for comparison", "text": "A.1 llama-regrPairs\nsuppressMessages ({ l ibrary ( optparse ) l ibrary ( a s l i b ) l ibrary ( l lama ) l ibrary ( p ly r ) })\no l = l i s t (make opt ion (c ( \u201d\u2212t \u201d , \u201d\u2212\u2212t r a i n \u201d ) , help = \u201dAS s c e n a r i o f o r t r a i n i n g \u201d ) , make opt ion (c ( \u201d\u2212p\u201d , \u201d\u2212\u2212p r e d i c t i o n \u201d ) , help = \u201dAS s c e n a r i o f o r p r e d i c t i o n s \u201d ) ) opts = parse args ( OptionParser ( opt ion l i s t = o l ) )\nsuppressWarnings ({ trainAS = parseASScenario ( opts$ t r a i n )} ) suppressWarnings ({ suppressMessages ({ l d f = convertToLlama ( trainAS )} )} ) suppressWarnings ({ testAS = parseASScenario ( opts$p r e d i c t i o n )} ) suppressWarnings ({ suppressMessages ({ l d f t = convertToLlama ( testAS )} )} )\n# some f e a t u r e s are removed by the convers ion , make sure t ha t we use on ly the # i n t e r s e c t i o n f e a t s = intersect ( l d f $ f e a tu r e s , l d f t $ f e a t u r e s ) l d f $ f e a t u r e s = f e a t s l d f t $ f e a t u r e s = f e a t s\nt t = t ra inTes t ( l d f ) model = r e g r e s s i o n P a i r s ( makeLearner ( \u201d r eg r . randomForest\u201d ) , t t )\npreds = model$p r e d i c t o r ( l d f t $data [ , f e a t s ] )\nsched = ddply ( preds , c ( \u201d id \u201d ) , function ( s s ) { data . frame ( instanceID = testAS$ f e a t u r e . va lue s [ s s$ id [ 1 ] , \u201d i n s t ance id \u201d ] ,\nrunID = 1 , s o l v e r = s s$a lgor i thm [ 1 ] , t imeLimit = testAS$desc$a lgor i thm c u t o f f time )\n})\nwrite . csv ( sched [ , c ( \u201d instanceID \u201d , \u201drunID\u201d , \u201d s o l v e r \u201d , \u201d t imeLimit \u201d ) ] , f i l e = stdout ( ) , quote = FALSE, row .names = FALSE)\nA.2 llama-regr\nsuppressMessages ({ l ibrary ( optparse ) l ibrary ( a s l i b ) l ibrary ( l lama ) l ibrary ( p ly r ) })\no l = l i s t (make opt ion (c ( \u201d\u2212t \u201d , \u201d\u2212\u2212t r a i n \u201d ) , help = \u201dAS s c e n a r i o f o r t r a i n i n g \u201d ) , make opt ion (c ( \u201d\u2212p\u201d , \u201d\u2212\u2212p r e d i c t i o n \u201d ) , help = \u201dAS s c e n a r i o f o r p r e d i c t i o n s \u201d ) ) opts = parse args ( OptionParser ( opt ion l i s t = o l ) )\nsuppressWarnings ({ trainAS = parseASScenario ( opts$ t r a i n )} ) suppressWarnings ({ suppressMessages ({ l d f = convertToLlama ( trainAS )} )} ) suppressWarnings ({ testAS = parseASScenario ( opts$p r e d i c t i o n )} ) suppressWarnings ({ suppressMessages ({ l d f t = convertToLlama ( testAS )} )} )\n# some f e a t u r e s are removed by the convers ion , make sure t ha t we use on ly the # i n t e r s e c t i o n f e a t s = intersect ( l d f $ f e a tu r e s , l d f t $ f e a t u r e s ) l d f $ f e a t u r e s = f e a t s l d f t $ f e a t u r e s = f e a t s\nt t = t ra inTes t ( l d f ) model = r e g r e s s i o n ( makeLearner ( \u201d r eg r . randomForest\u201d ) , t t )\npreds = model$p r e d i c t o r ( l d f t $data [ , f e a t s ] )\nsched = ddply ( preds , c ( \u201d id \u201d ) , function ( s s ) { data . frame ( instanceID = testAS$ f e a t u r e . va lue s [ s s$ id [ 1 ] , \u201d i n s t ance id \u201d ] ,\nrunID = 1 , s o l v e r = s s$a lgor i thm [ 1 ] , t imeLimit = testAS$desc$a lgor i thm c u t o f f time )\n})\nwrite . csv ( sched [ , c ( \u201d instanceID \u201d , \u201drunID\u201d , \u201d s o l v e r \u201d , \u201d t imeLimit \u201d ) ] , f i l e = stdout ( ) , quote = FALSE, row .names = FALSE)"}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "The evaluation was performed as follows. For each scenario, 10 bootstrap samplings of the entire data were used to create 10 different train/test splits. No stratification was used. The training part was left unmodified. For the test part, algorithm performances were set to 0 and runstatus to \u201cok\u201d for all algorithms and all instances \u2013 the ASlib specification requires algorithm performance data to be part of a scenario. A cv.arff file was generated for both training and", "creator": "LaTeX with hyperref package"}}}