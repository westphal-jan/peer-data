{"id": "1511.04747", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2015", "title": "Learning Representations of Affect from Speech", "abstract": "There has been a lot of prior work on representation learning for speech recognition applications, but not much emphasis has been given to an investigation of effective representations of affect from speech, where the paralinguistic elements of speech are separated out from the verbal content. In this paper, we explore denoising autoencoders for learning paralinguistic attributes i.e., for learning the difference between the two paralinguistic components of the speech component and the non-paralinguistic elements.\n\n\n\n\nThe paralinguistic element, e.g., is a feature-specific component, where the two paralinguistic elements have the same properties of the individual. It is also referred to as an intrinsic aspect, because it is the structural component of speech. In the first case, it is called a congruent component. The two paralinguistic elements can be understood as the two paralinguistic components of speech: an intrinsic component, a congruent component and an intrinsic component. The two paralinguistic elements can be explained as the two paralinguistic elements of speech.\nTo show the similarities between the two paralinguistic components of speech, we use the following paralinguistic elements:\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\nAn intrinsic component\n", "histories": [["v1", "Sun, 15 Nov 2015 18:16:20 GMT  (596kb,D)", "http://arxiv.org/abs/1511.04747v1", "This is a submission for the ICLR (International Conference on learning Representations). The submission will be updated and revised by 11/19/2016, after the arXiv identifier is obtained"], ["v2", "Fri, 20 Nov 2015 01:37:01 GMT  (571kb)", "http://arxiv.org/abs/1511.04747v2", "This is a submission for the ICLR (International Conference on Learning Representations). The submission will be updated and revised by 11/19/2016, after the arXiv identifier is obtained"], ["v3", "Mon, 11 Jan 2016 20:44:51 GMT  (3088kb,D)", "http://arxiv.org/abs/1511.04747v3", "This is a submission for the ICLR (International Conference on Learning Representations). Final modified draft will be updated shortly"], ["v4", "Mon, 18 Jan 2016 20:36:36 GMT  (3088kb,D)", "http://arxiv.org/abs/1511.04747v4", "This is a submission for the ICLR (International Conference on Learning Representations). Final modified draft will be updated shortly"], ["v5", "Tue, 19 Jan 2016 04:05:50 GMT  (3088kb,D)", "http://arxiv.org/abs/1511.04747v5", "This is a submission for the ICLR (International Conference on Learning Representations). Final modified draft will be updated shortly"], ["v6", "Sun, 14 Feb 2016 18:11:46 GMT  (3088kb,D)", "http://arxiv.org/abs/1511.04747v6", "This is a submission for the ICLR (International Conference on Learning Representations) Workshop 2016"]], "COMMENTS": "This is a submission for the ICLR (International Conference on learning Representations). The submission will be updated and revised by 11/19/2016, after the arXiv identifier is obtained", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["sayan ghosh", "eugene laksana", "louis-philippe morency", "stefan scherer"], "accepted": false, "id": "1511.04747"}, "pdf": {"name": "1511.04747.pdf", "metadata": {"source": "CRF", "title": "SPEECH / CONFERENCE SUBMISSIONS", "authors": ["Sayan Ghosh", "Eugene Laksana", "Stefan Scherer"], "emails": ["sghosh@ict.usc.edu", "elaksana@ict.usc.edu", "scherer@ict.usc.edu", "morency@cs.cmu.edu"], "sections": [{"heading": null, "text": "There has been a lot of prior work on representation learning for speech recognition applications, but not much emphasis has been given to an investigation of effective representations of affect from speech, where the paralinguistic elements of speech are separated out from the verbal content. In this paper, we explore denoising autoencoders for learning paralinguistic attributes i.e. dimensional affective traits from speech. We show that the representations learnt by the bottleneck layer of the autoencoder are highly discriminative at separating out negative sentiments (sadness and anger) from positive sentiments (happiness). We also learn utterance specific representations by a combination of denoising autoencoders and LSTM based recurrent autoencoders, and perform emotion classification with the learnt temporal/dynamic representations. Experiments on a well-established reallife speech dataset (IEMOCAP) show that the utterance representations are comparable to state of the art feature extractors (such as voice quality features and MFCCs) at emotion and affect recognition."}, {"heading": "1 INTRODUCTION", "text": "The field of representation learning has witnessed a huge progress over recent years, particularly in the automated machine perception of images, text and speech. This has been facilitated by developments in deep learning, where it has been observed that stacking layers in neural networks, with large amounts of data has yielded classification accuracies which have beaten the state-of-the-art in large scale recognition problems, such as ILSVRC challenge (Russakovsky et al.), and large scale speech recognition (Hinton et al., 2012). However, there has been limited progress in exploring representation learning for non-phonetic and affective attributes from speech. Current research is mainly on the use of off-the-self feature extractors, such as Mel-Frequency Cepstral Coefficients (MFCCs), and voice quality features (Scherer et al., 2013) such as normalized amplitude quotient, and fundamental frequency). With this view in mind, we wish to explore the effectiveness of representation learning for affect from speech. We primarily focus on two models - (1) Denoising autoencoder learnt from the speech spectrograms, where each frame is a windowed frame in time, and (2) The recurrent neural network as an autoencoder. In this paper, we show that the denoising autoencoder learns features which are very discriminative of affective features. We further train an RNN as an autoencoder over the activations obtained from the denoising autoencoder, and learn utterance based representations from speech. We show that these features have phonetic information filtered out, and are thus very discriminative of paralinguistic attributes.\n\u2217Use footnote for providing further information about author (webpage, alternative address)\u2014not for acknowledging funding agencies. Funding acknowledgements go at the end of the paper.\nar X\niv :1\n51 1.\n04 74\n7v 1\n[ cs\n.C L\n] 1\n5 N\nov 2"}, {"heading": "2 PRIOR WORK IN REPRESENTATION LEARNING FROM SPEECH", "text": "Jaitly & Hinton (2011) model speech waveforms using RBMs (Restricted Boltzmann Machines) and use the extracted features for speech emotion recognition. They obtain phoneme recognition performance close to state-of-the-art (with Mel- cepstrum coefficients). Graves et al. (2013) investigated deep recurrent neural networks for speech recognition, achieving a test set error of 17.7% on TIMIT. Sainath et al. (2013) worked on filterbank learning in a deep neural network framework, where it was found that filterbanks similar to the Mel scale were learnt. They improved on their work in Sainath et al. (2014) by incorporating delta learning and speaker adaptation. Han et al. (2014) performed speech emotion recognition using a combination of DNN (Deep Neural Network) and Extreme Learning Machines. They obtained 20% relative accuracy improvement compared to state-of-the-art approaches. Lee & Tashev (2015) use recurrent neural networks for speech emotion recognition, where the label of each frame is modeled as a sequence of random variables. They obtained a weighted accuracy improvement of 12% compared to a DNN-ELM baseline."}, {"heading": "3 MODEL AND EXPERIMENTAL SETUP", "text": ""}, {"heading": "3.1 DENOISING AUTOENCODER", "text": "The autoencoder is a neural network, where the input data set {xi}i=Ni=1 is passed into a feedforward neural network of two layers, where the middle layer is a bottleneck layer with activations {zi}i=Ni=1 , where the activations are obtained as:\nzi = tanh (W1xi)\nThe reconstructed input x\u0302i is obtained from the autoencoder activations as:\nx\u0302i = tanh (W2zi)\nThus the autoencoder is a feedforward neural network, and it is trained using backpropagation. Vincent et al. (2008) introduce denoising autoencoders, where the data point x\u0302i is corrupted with Gaussian noise to produce x\u0303i, from which the original clean data point xi is reconstructed by the autoencoder. This enables the autoencoder to learn not only latent information in the data, but also structural information among the elements in xi. When stacking autoencoders, we have multiple layers with weights Wk\u22121 and Wk for the k-th layer, where the autoencoder activations at the k-th layer is :\nzik = tanh (Wk\u22121zik\u22121)"}, {"heading": "3.2 DATASET USED", "text": "For our experiments, we used the IEMOCAP dataset (Busso et al., 2008), which is a well-known acted dataset for speech emotion recognition, consists of multimodal interactions of dyadic sessions between actors, where conversations are scripted to elicit emotional expressions. The dataset consist of around 12 hours of speech from 10 human subjects, and are labeled by three annotators for emotions such as Happy,Sad,Angry,Excitement, Neutral and Surprise, along with dimensional labels such as Valence, Activation and Dominance. We use all the datapoints for pre-training the autoencoder, but perform classification experiments only on four emotions - Neutral, Angry, Sad and Happy. Each utterance is approximately 2-5 seconds in duration, with short periods of silence before and after speech in all utterances. Prior to the classification experiments, we split the dataset into seven subjects (training set), and three subjects (validation set). Autoencoder pre-training is performed on the training set, while we present visualizations of the learned representations on the validation set."}, {"heading": "3.3 FEATURES EXTRACTED", "text": "We extracted spectrograms from the utterances, with a frame width of 25 ms, frame overlap of 10 ms and 513 FFT (Fast Fourier Transform) bins. We used a log-scale in the frequency domain, since a higher emphasis in lower frequencies has been shown to be more significant for auditory perception. Contrary to other works in the domain of speech emotion recognition, we do not extract\nany off-the-shelf features, such as Mel-Frequency Cepstral Coefficients (MFCCs), or voice quality features (Scherer et al., 2013), since we wish to investigate if the autoencoder is useful at discovering affective features directly from the frequency representation of the speech signal. However, we use off-the-shelf features for comparison with the learned representations in an emotion recognition task."}, {"heading": "4 CORRELATION OF REPRESENTATIONS WITH AFFECTIVE ATTRIBUTES", "text": "In this section, we describe our experiments with representation learning of affective features using the stacked denoising autoencoder. We initially present results with a single bottleneck layer of 256, following which we greedily pre-train by adding more layers. The bottleneck size was chosen based on performance in a subsequent emotion classification task. The activations learnt at each layer of the autoencoder are dimensionally reduced using tSNE (Van der Maaten & Hinton, 2008) and are visualized at frame level, with different emotion categories. We also examine the variation of the bottleneck representations with paralinguistic attributes such as valence, activation and dominance."}, {"heading": "4.1 EMOTIONS", "text": "In Figure 1,we present scatter plots of the visualizations for four primary emotions - (1) Happy (2) Sad (3) Angry and (4) Neutral. From an examination of the plots we conclude that one layer of the autoencoder can separate out the angry and sad emotions. The features learnt are not so discriminative of the happy and neutral categories. We expect that higher layers of the network will be more discriminative of these emotions. It is also interesting to note that a cluster corresponding to pauses and silence in speech has been formed, which has entirely different acoustics from speech."}, {"heading": "4.2 VALENCE, DOMINANCE AND ACTIVATION", "text": "We also explore whether the autoencoder can learn affective attributes from the speech such as activation (intensity of emotions), valence(positive or negative sentiments) and dominance(dominant and submissive emotions). Figure 2 show a scatter plot of learned representations, colored for each attribute according to intensity (blue for lowest and red for highest). We find that the autoencoder is\nmost discriminative of activation, followed by valence, and dominance. The sensitivity to activation may also be the reason for separating out anger and sad emotions."}, {"heading": "5 EMOTION CLASSIFICATION WITH LEARNT REPRESENTATIONS", "text": "We use the extracted features from the autoencoder for emotion classification. We use a two layer MLP (Multi-layer perceptron) with tanh activations, where the lower layer is initialized to the weights in the pretrained autoencoder. Let us represent the dataset by speech frames xij where i denotes the utterance index, and j, the time frame in the i-th utterance. If yij = f(xij) is the activation of the top layer in K emotion category dimensions predicted by the MLP, then the predicted emotion category Ci for the i-the utterance is given by:\nCi = argmaxi j=Ti\u2211 j=1 yij(k)\nWe split the dataset into a training set of seven subjects and a testing set of three subjects. The validation accuracy on this held-out set (at utterance level) is 47.63%. Emotions Happy and Sad were the most accurately classified, followed by Angry and Neutral emotions."}, {"heading": "6 FUTURE WORK AND CONCLUSIONS", "text": "In this paper, we have explored the denoising autoencoder for representation learning of affective and paralinguistic attributes from speech. We find that the representations learnt by the autoencoder are sensitive to anger and sadness as emotions, and to activation as a dimension. Emotion classification experiments show that a multi-layer perceptron, along with pre-trained layers of the autoencoder achieve a classification accuracy of 47.63% on a held-out validation set. For future work, we wish to explore convolutional autoencoders for emotion recognition from speech."}], "references": [{"title": "Iemocap: Interactive emotional dyadic motion capture database", "author": ["Busso", "Carlos", "Bulut", "Murtaza", "Lee", "Chi-Chun", "Kazemzadeh", "Abe", "Mower", "Emily", "Kim", "Samuel", "Chang", "Jeannette N", "Sungbok", "Narayanan", "Shrikanth S"], "venue": "Language resources and evaluation,", "citeRegEx": "Busso et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Busso et al\\.", "year": 2008}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alan", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Speech emotion recognition using deep neural network and extreme learning machine", "author": ["Han", "Kun", "Yu", "Dong", "Tashev", "Ivan"], "venue": "Proceedings of INTERSPEECH, ISCA, Singapore,", "citeRegEx": "Han et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Han et al\\.", "year": 2014}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Hinton", "Geoffrey", "Deng", "Li", "Yu", "Dong", "Dahl", "George E", "Mohamed", "Abdel-rahman", "Jaitly", "Navdeep", "Senior", "Andrew", "Vanhoucke", "Vincent", "Nguyen", "Patrick", "Sainath", "Tara N"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "A new way to learn acoustic events", "author": ["Jaitly", "Navdeep", "Hinton", "Geoffrey E"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Jaitly et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jaitly et al\\.", "year": 2011}, {"title": "High-level feature representation using recurrent neural network for speech emotion recognition", "author": ["Lee", "Jinkyu", "Tashev", "Ivan"], "venue": "In Sixteenth Annual Conference of the International Speech Communication Association,", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Learning filter banks within a deep neural network framework", "author": ["Sainath", "Tara N", "Kingsbury", "Brian", "Mohamed", "Abdel-rahman", "Ramabhadran", "Bhuvana"], "venue": "In Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Sainath et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2013}, {"title": "Improvements to filterbank and delta learning within a deep neural network framework", "author": ["Sainath", "Tara N", "Kingsbury", "Brian", "Mohamed", "Abdel-rahman", "Saon", "George", "Ramabhadran", "Bhuvana"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Sainath et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2014}, {"title": "Investigating fuzzy-input fuzzy-output support vector machines for robust voice quality classification", "author": ["Scherer", "Stefan", "Kane", "John", "Gobl", "Christer", "Schwenker", "Friedhelm"], "venue": "Computer Speech & Language,", "citeRegEx": "Scherer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Scherer et al\\.", "year": 2013}, {"title": "Visualizing data using t-sne", "author": ["Van der Maaten", "Laurens", "Hinton", "Geoffrey"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 3, "context": "), and large scale speech recognition (Hinton et al., 2012).", "startOffset": 38, "endOffset": 59}, {"referenceID": 8, "context": "Current research is mainly on the use of off-the-self feature extractors, such as Mel-Frequency Cepstral Coefficients (MFCCs), and voice quality features (Scherer et al., 2013) such as normalized amplitude quotient, and fundamental frequency).", "startOffset": 154, "endOffset": 176}, {"referenceID": 1, "context": "Graves et al. (2013) investigated deep recurrent neural networks for speech recognition, achieving a test set error of 17.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Graves et al. (2013) investigated deep recurrent neural networks for speech recognition, achieving a test set error of 17.7% on TIMIT. Sainath et al. (2013) worked on filterbank learning in a deep neural network framework, where it was found that filterbanks similar to the Mel scale were learnt.", "startOffset": 0, "endOffset": 157}, {"referenceID": 1, "context": "Graves et al. (2013) investigated deep recurrent neural networks for speech recognition, achieving a test set error of 17.7% on TIMIT. Sainath et al. (2013) worked on filterbank learning in a deep neural network framework, where it was found that filterbanks similar to the Mel scale were learnt. They improved on their work in Sainath et al. (2014) by incorporating delta learning and speaker adaptation.", "startOffset": 0, "endOffset": 350}, {"referenceID": 1, "context": "Graves et al. (2013) investigated deep recurrent neural networks for speech recognition, achieving a test set error of 17.7% on TIMIT. Sainath et al. (2013) worked on filterbank learning in a deep neural network framework, where it was found that filterbanks similar to the Mel scale were learnt. They improved on their work in Sainath et al. (2014) by incorporating delta learning and speaker adaptation. Han et al. (2014) performed speech emotion recognition using a combination of DNN (Deep Neural Network) and Extreme Learning Machines.", "startOffset": 0, "endOffset": 424}, {"referenceID": 1, "context": "Graves et al. (2013) investigated deep recurrent neural networks for speech recognition, achieving a test set error of 17.7% on TIMIT. Sainath et al. (2013) worked on filterbank learning in a deep neural network framework, where it was found that filterbanks similar to the Mel scale were learnt. They improved on their work in Sainath et al. (2014) by incorporating delta learning and speaker adaptation. Han et al. (2014) performed speech emotion recognition using a combination of DNN (Deep Neural Network) and Extreme Learning Machines. They obtained 20% relative accuracy improvement compared to state-of-the-art approaches. Lee & Tashev (2015) use recurrent neural networks for speech emotion recognition, where the label of each frame is modeled as a sequence of random variables.", "startOffset": 0, "endOffset": 650}, {"referenceID": 10, "context": "Vincent et al. (2008) introduce denoising autoencoders, where the data point x\u0302i is corrupted with Gaussian noise to produce x\u0303i, from which the original clean data point xi is reconstructed by the autoencoder.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "For our experiments, we used the IEMOCAP dataset (Busso et al., 2008), which is a well-known acted dataset for speech emotion recognition, consists of multimodal interactions of dyadic sessions between actors, where conversations are scripted to elicit emotional expressions.", "startOffset": 49, "endOffset": 69}, {"referenceID": 8, "context": "any off-the-shelf features, such as Mel-Frequency Cepstral Coefficients (MFCCs), or voice quality features (Scherer et al., 2013), since we wish to investigate if the autoencoder is useful at discovering affective features directly from the frequency representation of the speech signal.", "startOffset": 107, "endOffset": 129}], "year": 2017, "abstractText": "There has been a lot of prior work on representation learning for speech recognition applications, but not much emphasis has been given to an investigation of effective representations of affect from speech, where the paralinguistic elements of speech are separated out from the verbal content. In this paper, we explore denoising autoencoders for learning paralinguistic attributes i.e. dimensional affective traits from speech. We show that the representations learnt by the bottleneck layer of the autoencoder are highly discriminative at separating out negative sentiments (sadness and anger) from positive sentiments (happiness). We also learn utterance specific representations by a combination of denoising autoencoders and LSTM based recurrent autoencoders, and perform emotion classification with the learnt temporal/dynamic representations. Experiments on a well-established reallife speech dataset (IEMOCAP) show that the utterance representations are comparable to state of the art feature extractors (such as voice quality features and MFCCs) at emotion and affect recognition.", "creator": "LaTeX with hyperref package"}}}