{"id": "1511.04773", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2015", "title": "Large-Scale Approximate Kernel Canonical Correlation Analysis", "abstract": "Kernel Canonical correlation analysis (KCCA) is a fundamental method with broad applicability in statistics and machine learning. Although there exist closed-form solution to the KCCA objective by solving an $N\\times N$ eigenvalue system where $N$ is the training set size, the computational requirements of this approach in both memory and time prohibit its usage in the large scale. Various approximation techniques have been developed for KCCA, including the SPM-SPM-2 and the SPM-RAM-2. However, the KCCA approach provides a more accurate way to obtain a larger dataset of the same magnitude as the SPM-SPM-SPM-RAM-2. As an example, KCCA, for example, is an optimization of the two sets of data to produce a set of 1 and 2, which corresponds to a given set of values. The latter optimization also requires a more complex method, such as an optimised method (called the Eigenvalue Algorithm). The OSP-SPM-2 implementation offers two approaches to the problem of training a dataset. Both the OSP-SPM-RAM-2 and the SPM-RAM-2 have been shown to have a better estimate of the total training time in the KCCA dataset (Fig. 2). One example of this approach is that the OSP-SPM-2 and the SPM-RAM-2 are both optimised over multiple iterations (Fig. 2). The OSP-SPM-RAM-2 is more efficient at generating the same set of values for each set, which is a crucial component of learning the OSP-SPM-RAM-2. However, the latter optimization is not sufficient for predicting the maximum training time in the KCCA dataset (Fig. 2). However, the OSP-SPM-RAM-2 and the SPM-RAM-2 also show similar performance with different implementations of Eigenvalue algorithms (Fig. 2). This is because the OSP-SPM-RAM-2 and the SPM-RAM-2 are optimized over multiple iterations (Fig. 2). However, the OSP-SPM-RAM-2 and the SPM-RAM-2 are optimised over multiple iterations (Fig. 2). This may be a limitation in the KCCA approach to training a dataset. In a similar manner, in this case, the O", "histories": [["v1", "Sun, 15 Nov 2015 22:20:02 GMT  (46kb)", "https://arxiv.org/abs/1511.04773v1", "Submission to ICLR 2016"], ["v2", "Tue, 17 Nov 2015 16:31:14 GMT  (46kb)", "http://arxiv.org/abs/1511.04773v2", "Submission to ICLR 2016"], ["v3", "Thu, 7 Jan 2016 00:27:20 GMT  (49kb)", "http://arxiv.org/abs/1511.04773v3", "Submission to ICLR 2016"], ["v4", "Mon, 29 Feb 2016 16:04:46 GMT  (49kb)", "http://arxiv.org/abs/1511.04773v4", "Published as a conference paper at International Conference on Learning Representations (ICLR) 2016"]], "COMMENTS": "Submission to ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["weiran wang", "karen livescu"], "accepted": true, "id": "1511.04773"}, "pdf": {"name": "1511.04773.pdf", "metadata": {"source": "CRF", "title": "LARGE-SCALE APPROXIMATE KERNEL CANONICAL CORRELATION ANALYSIS", "authors": ["Weiran Wang"], "emails": ["weiranwang@ttic.edu", "klivescu@ttic.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n04 77\n3v 4\n[ cs\n.L G\n] 2\n9 Fe\nb 20\n16 Published as a conference paper at ICLR 2016"}, {"heading": "1 INTRODUCTION", "text": "Canonical correlation analysis (CCA, Hotelling, 1936) and its extensions are ubiquitous techniques in scientific research areas for revealing the common sources of variability in multiple views of the same phenomenon, including meteorology (Anderson, 2003), chemometrics (Montanarella et al., 1995), genomics (Witten et al., 2009), computer vision (Kim et al., 2007; Socher & Li, 2010), speech recognition (Rudzicz, 2010; Arora & Livescu, 2013; Wang et al., 2015a), and natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011; Hodosh et al., 2013; Faruqui & Dyer, 2014; Lu et al., 2015a). CCA seeks linear projections of two random vectors (views), such that the resulting low-dimensional vectors are maximally correlated. Given a dataset of N pairs of observations (x1,y1), . . . , (xN ,yN ) of the random variables, where xi \u2208 Rdx and yi \u2208 Rdy for i = 1, . . . , N , the objective of CCA for L-dimensional projections can be written as1 (see, e.g., Borga, 2001)\nmax U\u2208Rdx\u00d7L,V\u2208Rdy\u00d7L\ntr ( U\u22a4\u03a3xyV )\n(1)\ns.t. U\u22a4\u03a3xxU = V\u22a4\u03a3yyV = I, u\u22a4i \u03a3xyvj = 0, for i 6= j,\nwhere (U,V) are the projection matrices for each view, \u03a3xy = 1N \u2211N i=1 xiy \u22a4 i , \u03a3xx = 1 N \u2211N i=1 xix \u22a4 i + rxI, \u03a3yy = 1 N \u2211N i=1 yiy \u22a4 i + ryI are the cross- and auto-covariance matrices, and (rx, ry) \u2265 0 are regularization parameters (Vinod, 1976; Bie & Moor, 2003). There exists a\n1In this paper, we assume that the inputs are centered at the origin for notational simplicity; if they are not, we can center them as a pre-processing operation.\nclosed-form solution to (1) as follows. Let the rank-L singular value decomposition (SVD) of the whitened covariance matrix T = \u03a3 \u2212 1\n2 xx \u03a3xy\u03a3\n\u2212 1 2\nyy \u2208 Rdx\u00d7dy be U\u0303\u039bV\u0303\u22a4, where \u039b contains the top L singular values \u03c31 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3L on its diagonal, and (U\u0303, V\u0303) are the corresponding singular vectors. Then the optimal projection matrices (U,V) in (1) are (\u03a3 \u2212 1 2 xx U\u0303,\u03a3 \u2212 1 2\nyy V\u0303), and the optimal objective value, referred to as the canonical correlation, is\n\u2211L l=1 \u03c3l. 2 The theoretical properties of CCA (Kakade & Foster, 2007; Chaudhuri et al., 2009; Foster et al., 2009) and its connection to other methods (Borga, 2001; Bach & Jordan, 2005; Chechik et al., 2005) have also been studied.\nOne limitation of CCA is its restriction to linear mappings, which are often insufficient to reveal the highly nonlinear relationships in many real-world applications. To overcome this issue, kernel CCA (KCCA) was proposed indepedently by several researchers (Lai & Fyfe, 2000; Akaho, 2001; Melzer et al., 2001) and has become a common technique in statistics and machine learning (Bach & Jordan, 2002; Hardoon et al., 2004). KCCA extends CCA by mapping the original inputs in both views into reproducing kernel Hilbert spaces (RKHS) and solving linear CCA in the RKHS. By the representer theorem of RKHS (Scho\u0308lkopf & Smola, 2001), one can conveniently work with the kernel functions instead of the high-dimensional (possibly infinite-dimensional) RKHS, and the projection mapping is a linear combination of kernel functions evaluated at the training samples. KCCA has been successfully used for cross-modality retrieval (Hardoon et al., 2004; Li & Shawe-Taylor, 2005; Socher & Li, 2010; Hodosh et al., 2013), acoustic feature learning (Arora & Livescu, 2013), computational biology (Yamanishi et al., 2004; Hardoon et al., 2007; Blaschkoa et al., 2011), and statistical independence measurement (Bach & Jordan, 2002; Fukumizu et al., 2007; Lopez-Paz et al., 2013).\nKCCA also has a closed-form solution, via an N \u00d7 N eigenvalue system (see Sec. 2). However, this solution does not scale up to datasets of more than a few thousand training samples, due to the time complexity of solving the eigenvalue system (O(N3) for a naive solution) and the memory cost of storing the kernel matrices. As a result, various approximation techniques have been developed, most of which are based on low-rank approximations of the kernel matrices. With rank-M approximations of the kernel matrices, the cost of solving approximate KCCA reduces to O(M2N) (see, e.g., Bach & Jordan, 2002; Lopez-Paz et al., 2014). Thus if M \u226a N , the approximation leads to significant computational savings. Typically, ranks of a few hundred to a few thousand are used for the low-rank kernel approximations (Yang et al., 2012; Le et al., 2013; Lopez-Paz et al., 2014). In more challenging real-world applications, however, it is observed that the rank M needed for an approximate kernel method to work well can be quite large, on the order of tens or hundreds of thousands (see Huang et al., 2014; Lu et al., 2015b for classification tasks, and Wang et al., 2015a for KCCA). In such scenarios, it then becomes challenging to solve even approximate KCCA.\nIn this paper, we focus on the computational challenges of scaling up approximate kernel CCA using low-rank kernel approximations when both the training set size N and the approximation rank M are large. The particular variant of approximate KCCA we use, called randomized CCA (Lopez-Paz et al., 2014), transforms the original inputs to an M -dimensional feature space using random features (Rahimi & Recht, 2008; 2009) so that inner products in the new feature space approximate the kernel function. This approach thus turns the original KCCA problem into a very high-dimensional linear CCA problem of the form (1). We then make use of a stochastic optimization algorithm, recently proposed for linear CCA and its deep neural network extension deep CCA (Ma et al., 2015; Wang et al., 2015c), to reduce the memory requirement for solving the resulting linear CCA problem. This algorithm updates parameters iteratively based on small minibatches of training samples. This approach allows us to run approximate KCCA on an 8\u2212million sample dataset of MNIST digits, and on a speech dataset with 1.4 million training samples and rank (dimensionality of random feature space) M = 100000 on a normal workstation. Using this approach we achieve encouraging results for multi-view learning of acoustic transformations for speech recognition.3\nIn the following sections we review approximate KCCA and random features (Sec. 2), present the stochastic optimization algorithm (Sec. 3), discuss related work (Sec. 4), and demonstrate our algorithm on two tasks (Sec. 5).\n2Alternatively, one could also solve some equivalent M \u00d7M eigenvalue system instead of the SVD of T, at a similar cost.\n3Our MATLAB implementation is available at http://ttic.uchicago.edu/\u02dcwwang5/knoi.html"}, {"heading": "2 APPROXIMATE KCCA", "text": ""}, {"heading": "2.1 KCCA SOLUTION", "text": "In KCCA, we transform the inputs {xi}Ni=1 of view 1 and {yi} N i=1 of view 2 using feature mappings \u03c6x and \u03c6y associated with some positive semi-definite kernels kx and ky respectively, and then solve the linear CCA problem (1) for the feature-mapped inputs (Lai & Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach & Jordan, 2002; Hardoon et al., 2004). The key property of such kernels is that kx(x,x\u2032) =< \u03c6x(x), \u03c6x(x\u2032) > (similarly for view 2) Scho\u0308lkopf & Smola, 2001. Even though the feature-mapped inputs live in possibly infinite-dimensional RKHS, replacing the original inputs (xi,yi) with (\u03c6x(xi), \u03c6y(yi)) in (1), and using the KKT theorem (Nocedal & Wright, 2006), one can show that the solution has the form U =\n\u2211N i=1 \u03c6x(xi)\u03b1 \u22a4 i and V = \u2211N i=1 \u03c6y(yi)\u03b2 \u22a4 i where\n\u03b1i,\u03b2i \u2208 R L, i = 1, . . . , N , as a result of the representer theorem (Scho\u0308lkopf & Smola, 2001). The final KCCA projections can therefore be written as f(x) = \u2211N\ni=1 \u03b1ikx(x,xi) \u2208 R L and\ng(y) = \u2211N i=1 \u03b2iky(y,yi) \u2208 R L for view 1 and view 2 respectively.\nDenote by Kx the N \u00d7N kernel matrix for view 1, i.e., (Kx)ij = kx(xi,xj), and similarly denote by Ky the kernel matrix for view 2. Then (1) can be written as a problem in the coefficient matrices A = [\u03b11, . . . ,\u03b1N ] \u22a4 \u2208 RN\u00d7L and B = [\u03b21, . . . ,\u03b2N ] \u22a4 \u2208 RN\u00d7L. One can show that the optimal coefficients A correspond to the top L eigenvectors of the N \u00d7N matrix (Kx+NrxI)\u22121Ky(Ky+ NryI)\n\u22121Kx, and a similar result holds for B (see, e.g., Hardoon et al., 2004). This involves solving an eiaylorgenvalue problem of size N \u00d7N , which is expensive both in memory (storing the kernel matrices) and in time (solving the N \u00d7N eigenvalue systems naively costs O(N3)).\nVarious kernel approximation techniques have been proposed to scale up KCCA, including Cholesky decomposition (Bach & Jordan, 2002), partial Gram-Schmidt (Hardoon et al., 2004), and incremental SVD (Arora & Livescu, 2012). Another widely used approximation technique for kernel matrices is the Nystro\u0308m method (Williams & Seeger, 2001). In the Nystro\u0308m method, we select M (random or otherwise) training samples x\u03031, . . . , x\u0303M and construct the M \u00d7M kernel matrix K\u0303x based on these samples, i.e. (K\u0303x)ij = kx(x\u0303i, x\u0303j). We compute the eigenvalue decomposition K\u0303x = R\u0303\u039b\u0303R\u0303\u22a4, and then the N \u00d7 N kernel matrix for the entire training set can be approximated as Kx \u2248 CK\u0303\u22121x C \u22a4 where C contains the columns of Kx corresponding to the selected subset, i.e., Cij = kx(xi, x\u0303j). This means Kx \u2248 (CR\u0303\u039b\u0303 \u2212 1 2 )(CR\u0303\u039b\u0303 \u2212 1 2 )\u22a4, so we can use the M \u00d7 N matrix (CR\u0303\u039b\u0303 \u2212 1\n2 )\u22a4 as the new feature representation for view 1 (similarly for view 2), where inner products between samples approximate kernel similarities. We can extract such features for both views, and apply linear CCA to them to approximate the KCCA solution (Yang et al., 2012; Lopez-Paz et al., 2014). Notice that using the Nystro\u0308m method has a time complexity (for view 1) of O(M2dx+M3+NMdx+M2N), where the four terms account for the costs of forming K\u0303x \u2208 RM\u00d7M , computing the eigenvalue decomposition of K\u0303x, forming C, and computing CR\u0303\u039b\u0303 \u2212 1\n2 , respectively, and a space complexity of O(M2) for saving the eigenvalue systems of K\u0303x and K\u0303y , which are expensive for large M . Although there have been various sampling/approximation strategies for the Nystro\u0308m method (Li et al., 2010; Zhang & Kwok, 2009; 2010; Kumar et al., 2012; Gittens & Mahoney, 2013), their constructions are more involved."}, {"heading": "2.2 APPROXIMATION VIA RANDOM FEATURES", "text": "We now describe another approximate KCCA formulation that is particularly well-suited to largescale problems.\nIt is known from harmonic analysis that a shift-invariant kernel of the form k(x,x\u2032) = \u03ba(x\u2212 x\u2032) is a positive definite kernel if and only if \u03ba(\u2206) is the Fourier transform of a non-negative measure (this is known as Bochner\u2019s theorem; see Rudin, 1994; Rahimi & Recht, 2008). Thus we can write the kernel function as an expectation over sinusoidal functions over the underlying probability measures and approximate it with sample averages. Taking as a concrete example the Gaussian radial basis function (RBF) kernel k(x,x\u2032) = e\u2212\u2016x\u2212x \u2032\u20162/2s2 where s is the kernel width, it can be approximated\nas (Lopez-Paz et al., 2014)\nk(x,x\u2032) =\n\u222b\ne\u2212jw \u22a4(x\u2212x\u2032)p(w) dw \u2248\n1\nM\nM \u2211\ni=1\n2 cos(w\u22a4i x+ bi) cos(w \u22a4 i x \u2032 + bi),\nwhere p(w) is the multivariate Gaussian distribution N (0, 1s2 I), obtained from the inverse Fourier transform of \u03ba(\u2206) = e\u2212 \u2016\u2206\u20162\n2s2 , and bi is drawn from a uniform distribution over [0, 2\u03c0]. Approximations for other shift-invariant kernels (Laplacian, Cauchy) can be found in Rahimi & Recht (2008). This approach has been extended to other types of kernels (Kar & Karnick, 2012; Hamid et al., 2014; Pennington et al., 2015). A careful quasi-Monte Carlo scheme for sampling from p(w) (Yang et al., 2014), and structured feature transformation for accelerating the computation of w\u22a4i x (Le et al., 2013), have also been studied.\nLeveraging this result, Rahimi & Recht (2009) propose to first extract M -dimensional random Fourier features for input x as (with slight abuse of notation)\n\u03c6(x) =\n\u221a\n2\nM\n[\ncos(w\u22a41 x+ b1), . . . , cos(w \u22a4 Mx+ bM )\n]\n\u2208 RM ,\nso that \u03c6(x)\u22a4\u03c6(x\u2032) \u2248 k(x,x\u2032), and then apply linear methods on these features. The computational advantage of this approach is that it turns nonlinear learning problems into convex linear learning problems, for which empirical risk minimization is much more efficient (e.g, Lu et al., 2015b used the recently proposed stochastic gradient method by Roux et al. 2012 for the task of multinomial logistic regression with random Fourier features). Rahimi & Recht (2009) showed that it allows us to effectively learn nonlinear models and still obtain good learning guarantees.\nLopez-Paz et al. (2014) have recently applied the random feature idea to KCCA, by extracting M - dimensional random Fourier features {(\u03c6x(xi), \u03c6y(yi))}Ni=1 for both views and solving exactly a linear CCA on the transformed pairs. They also provide an approximation guarantee for this approach (see Theorem 4 of Lopez-Paz et al., 2014). Comparing random features with the Nystro\u0308m method described previously, when both techniques use rank-M approximations, the cost of computing the solution to (1) is the same and of orderO(M2N). But using random features, we generate the M -dimensional features in a data-independent fashion with a minimal cost O(NMdx) (for view 1), which is negligible compared to that of the Nystro\u0308m method. Furthermore, random features do not require saving any kernel matrix and the random features can be generated on the fly by saving the random seeds. Although the Nystro\u0308m approximation can be more accurate at the same rank (Yang et al., 2012), the computational efficiency and smaller memory cost of random features make them more appealing for large-scale problems in practice."}, {"heading": "3 STOCHASTIC OPTIMIZATION OF APPROXIMATE KCCA", "text": "When the dimensionality M of the random Fourier features is very large, solving the resulting linear CCA problem is still very costly as one needs to save the M \u00d7M matrix T\u0303 = \u03a3\u0303 \u2212 1 2 xx \u03a3\u0303xy\u03a3\u0303 \u2212 1 2 yy and compute its SVD, where the covariance matrices are now computed on {(\u03c6x(xi), \u03c6y(yi))}Ni=1 instead of {(xi,yi)}Ni=1. It is thus desirable to develop memory-efficient stochastic optimization algorithms for CCA, where each update of the projection mappings depends only on a small minibatch of b examples, thus reducing the memory cost to O(bM). Notice, however, in contrast to the classification or regression objectives that are more commonly used with random Fourier features (Rahimi & Recht, 2009; Huang et al., 2014; Lu et al., 2015b), the CCA objective (1) can not be written as an unconstrained sum or expectation of losses incurred at each training sample (in fact all training samples are coupled together through the constraints). As a result, stochastic gradient descent, which requires unbiased gradient estimates computed from small minibatches, is not directly applicable here.\nFortunately, Ma et al. (2015); Wang et al. (2015c) have developed stochastic optimization algorithms, referred to as AppGrad (Augmented Approximate Gradient) and NOI (Nonlinear Orthogonal Iterations) respectively, for linear CCA and its deep neural network extension deep CCA (Andrew et al., 2013). Their algorithms are essentially equivalent other than the introduction in (Wang et al., 2015c) of a time constant for smoothing the covariance estimates over time.\nAlgorithm 1 KNOI: Stochastic optimization for approximate KCCA.\nInput: Initialization U \u2208 RM\u00d7L,V \u2208 RM\u00d7L, time constant \u03c1, minibatch size b, learning rate \u03b7, momentum \u00b5. \u2206U \u2190 0, \u2206V \u2190 0 Randomly choose a minibatch (Xb0 ,Yb0)\nSxx \u2190 1\n|b0|\n\u2211\ni\u2208b0\n( U\u22a4\u03c6x(xi) ) ( U\u22a4\u03c6x(xi) )\u22a4 ,\nSyy \u2190 1\n|b0|\n\u2211\ni\u2208b0\n( V\u22a4\u03c6y(yi) ) ( V\u22a4\u03c6y(yi) )\u22a4\nfor t = 1, 2, . . . , T do Randomly choose a minibatch (Xbt ,Ybt) of size b\nSxx \u2190 \u03c1Sxx + (1\u2212 \u03c1) 1 b\n\u2211\ni\u2208bt\n( U\u22a4\u03c6x(xi) ) ( U\u22a4\u03c6x(xi) )\u22a4\nSyy \u2190 \u03c1Syy + (1\u2212 \u03c1) 1 b\n\u2211\ni\u2208bt\n( V\u22a4\u03c6y(yi) ) ( V\u22a4\u03c6y(yi) )\u22a4\nCompute the gradient \u2202U of the objective\nmin U\n1\nb\n\u2211\ni\u2208bt\n\u2225 \u2225 \u2225 U\u22a4\u03c6x(xi)\u2212 S \u2212 1 2 yy V \u22a4\u03c6y(yi) \u2225 \u2225 \u2225\n2\nas \u2202U \u2190 1b \u2211 i\u2208bt \u03c6x(xi)\n(\nU\u22a4\u03c6x(xi)\u2212 S \u2212 1 2 yy V \u22a4\u03c6y(yi) )\u22a4\nCompute the gradient \u2202V of the objective\nmin V\n1\nb\n\u2211\ni\u2208bt\n\u2225 \u2225 \u2225 V\u22a4\u03c6y(yi)\u2212 S \u2212 1 2 xx U \u22a4\u03c6x(xi) \u2225 \u2225 \u2225\n2\nas \u2202V \u2190 1b \u2211 i\u2208bt \u03c6y(yi)\n(\nV\u22a4\u03c6y(yi)\u2212 S \u2212 1 2 xx U \u22a4\u03c6x(xi) )\u22a4\n\u2206U \u2190 \u00b5\u2206U \u2212 \u03b7\u2202U, \u2206V \u2190 \u00b5\u2206V \u2212 \u03b7\u2202V U \u2190 U+\u2206U, V \u2190 V +\u2206V\nend for Output: The updated (U,V).\nThe idea originates from the alternating least squares (ALS) formulation of CCA (Golub & Zha, 1995; Lu & Foster, 2014), which computes the SVD of T using orthogonal iterations (a generalization of power iterations to multiple eigenvalues/eigenvectors, Golub & van Loan, 1996) on TT\u22a4 and T\u22a4T. Due to the special form of TT\u22a4 and T\u22a4T, two least squares problems arise in this iterative approach (see, e.g., Wang et al., 2015c, Section III. A for more details). With this observation, Lu & Foster (2014) solve these least squares problems using randomized PCA (Halko et al., 2011) and a batch gradient algorithm. Ma et al. (2015); Wang et al. (2015c) take a step further and replace the exact solutions to the least squares problems with efficient stochastic gradient descent updates. Although unbiased gradient estimates of these subproblems do not lead to unbiased gradient estimates of the original CCA objective, local convergence results (that the optimum of CCA is a fixed point of AppGrad, and the AppGrad iterate converges linearly to the optimal solution when started in its neighborhood) have been established for AppGrad (Ma et al., 2015). It has also been observed that the stochastic algorithms converge fast to approximate solutions that are on par with the exact solution or solutions by batch-based optimizers.\nWe give our stochastic optimization algorithm for approximate KCCA, named KNOI (Kernel Nonlinear Orthogonal Iterations), in Algorithm 1. Our algorithm is adapted from the NOI algorithm of Wang et al. (2015c), which allows the use of smaller minibatches (through the time constant \u03c1) than does the AppGrad algorithm of Ma et al. (2015). In each iteration, KNOI adaptively estimates the covariance of the projections of each view (\u2208 RL) using a convex combination (with \u03c1 \u2208 [0, 1)) of the previous estimate and the estimate based on the current minibatch,4 uses them to whiten the targets of the cross-view least squares regression problems, derives gradients from these problems,5 and finally updates the projection matrices (U,V) with momentum. Notice that \u03c1 controls how fast we forget the previous estimate; larger \u03c1 may be necessary for the algorithm to work well if the mini-\n4In practice we also adaptively estimate the mean of the projections and center each minibatch. 5We also use small weight decay regularization (\u223c 10\u22125) for (U,V) in the least squares problems.\nbatch size b is small (e.g., due to memory constraints), in which case the covariance estimates based on the current minibatch are noisier (see discussions in Wang et al., 2015c). Empirically, we find that using momentum \u00b5 \u2208 [0, 1) helps the algorithm to make rapid progress in the objective with a few passes over the training set, as observed by the deep learning community (Sutskever et al., 2013). Although we have specifically use random Fourier features in Algorithm 1, in principle other low-rank kernel approximations can be used as well.\nIn each iteration of KNOI, the main cost comes from evaluating the random Fourier features and the projections for a minibatch, and computing the gradients. Since we usually look for low-dimensional projections (L is small), it costs little memory and time to compute the covariance estimates Sxx and Syy (of size L \u00d7 L) and their eigenvalue decompositions (for S \u2212 1 2 xx and S \u2212 1 2 yy ). Overall, KNOI has a memory complexity of O(Mb) (excluding the O(ML) cost for saving U and V in memory) and a time complexity of O(bM(dx + dy + 4L)) per iteration.\nThe (U,V) we obtain from Algorithm 1 do not satisfy the constraints U\u22a4\u03a3\u0303xxU = V\u22a4\u03a3\u0303yyV; one can enforce the constraints via another linear CCA in RL on {(U\u22a4\u03c6x(xi),V\u22a4\u03c6y(yi))}Ni=1, which does not change the canonical correlation between the projections. To evaluate the projection of a view 1 test sample x, we generate the random Fourier features \u03c6x(x) using the same random seed for the training set, and compute U\u22a4\u03c6x(x) for it.\nFinally, we comment on the choice of hyperparameters in KNOI. Empirically, we find that larger b tends to give more rapid progress in the training objective, in which case \u03c1 can be set to small values or to 0 as there is sufficient covariance information in a large minibatch (also shown by Ma et al., 2015; Wang et al., 2015c). Therefore, we recommend using larger b and \u03c1 = 0 if one can afford the memory cost. For large-scale problems with millions of training examples, we set b to be a small portion of the training set (a few thousands) and enjoy the fast convergence of stochastic training algorithms (Bottou & Bousquet, 2008). In our experiments we initialize (U,V) with values sampled from a Gaussian distribution with standard deviation 0.1, and tune the learning rate \u03b7 and momentum \u00b5 on small grids."}, {"heading": "4 RELATED WORK", "text": "There have been continuous efforts to scale up classical methods such as principal component analysis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015). The CCA objective is more challenging due to the constraints, as also pointed out by Arora et al. (2012).\nAvron et al. (2013) propose an algorithm for selecting a subset of training samples that retain the most information for accelerating linear CCA, when there are many more training samples (large N ) than features (small M in our case). While this approach effectively reduces the training set size N , it provides no remedy for the large M scenario we face in approximate KCCA.\nIn terms of online/stochastic CCA, Yger et al. (2012) propose an adaptive CCA algorithm with efficient online updates based on matrix manifolds defined by the constraints (and they use a similar form of adaptive estimates for the covariance matrices). However, the goal of their algorithm is anomaly detection for streaming data with a varying distribution, rather than to perform CCA for a given dataset. Regarding the stochastic CCA algorithms of Ma et al. (2015); Wang et al. (2015c) we use here, an intuitively similar approach is proposed in the context of alternating conditional expectation (Makur et al., 2015).\nAnother related approach is that of Xie et al. (2015), who propose the Doubly Stochastic Gradient Descent (DSGD) algorithm for approximate kernel machines (including KCCA) based on random Fourier features. KNOI and DSGD are different in several respects. First, the stochastic update rule of DSGD for (U,V) is derived from the Lagrangian of an eigenvalue formulation of CCA and is different from ours, e.g., DSGD does not have any whitening steps while KNOI does. Second, DSGD gradually increases the number of random Fourier features (or cycles through blocks of random Fourier features) and updates the corresponding portions of (U,V) as it sees more training samples. While this potentially further reduces the memory cost of the algorithm, it is not essential as we could also process the random Fourier features in minibatches (blocks) within KNOI."}, {"heading": "5 EXPERIMENTS", "text": "In this section, we demonstrate the KNOI algorithm on two large-scale problems and compare it to several alternatives:\n\u2022 CCA, solved exactly by SVD.\n\u2022 FKCCA, low-rank approximation of KCCA using random Fourier features, with the CCA step solved exactly by SVD.\n\u2022 NKCCA, low-rank approximation of KCCA using the Nystro\u0308m method, with the CCA step solved exactly by SVD.\nWe implement KNOI in MATLAB with GPU support. Since our algorithm mainly involves simple matrix operations, running it on a GPU provides significant speedup."}, {"heading": "5.1 MNIST 8M", "text": "In the first set of experiments, we demonstrate the scability and efficiency of KNOI on the MNIST8M dataset (Loosli et al., 2007). The dataset consists of 8.1 million 28 \u00d7 28 grayscale images of the digits 0-9. We divide each image into the left and right halves and use them as the two views in KCCA, so the input dimensionality is 392 for both views. The dataset is randomly split into training/test sets of size 8M/0.1M. The task is to learn L = 50 dimensional projections using KCCA, and the evaluation criterion is the total canonical correlation achieved on the test set (upperbounded by 50). The same task is used by Xie et al. (2015), although we use a different training/test split.\nAs in Xie et al. (2015), we fix the kernel widths using the \u201cmedian\u201d trick6 for all algorithms. We vary the rank M for FKCCA and NKCCA from 256 to 6000. For comparison, we use the same hyperparamters as those of Xie et al. (2015)7: data minibatch size b = 1024, feature minibatch size 2048, total number of random Fourier features M = 20480,8 and a decaying step size schedule. For KNOI, we tune hyperparameters on a rough grid based on total canonical correlation obtained on a random subset of the training set with 0.1M samples, and set the minibatch size b = 2500, time constant \u03c1 = 0, learning rate \u03b7 = 0.01, and momentum \u00b5 = 0.995. We run the iterative algorithms DSGD and KNOI for one pass over the data, so that they see the same number of samples as FKCCA/NKCCA. We run each algorithm 5 times using different random seeds and report the mean results.\nThe total canonical correlations achieved by each algorithm on the test set, together with the run times measured on a workstation with 6 3.6GHz CPUs and 64G main memory, are reported in Table 1. As expected, all algorithms improve monotonically as M is increased. FKCCA and NKCCA achieve competitive results with a reasonably large M , with NKCCA consistently outperforming FKCCA at the cost of longer run times. KNOI outperforms the other iterative algorithm DSGD, and overall achieves the highest canonical correlation with a larger M . We show the learning curve of KNOI with M = 40960 (on the test set) in Figure 1. We can see that KNOI achieves steep improvement in the objective in the beginning, and already outperforms the exact solutions of FKCCA and NKCCA with M = 4096 after seeing only 1/4 to 1/2 of the training set. We also run KNOI on an NVIDIA Tesla K40 GPU with 12G memory, and report the run times in parentheses in Table 1; the GPU provides a speedup of more than 12 times. For this large dataset, the KNOI algorithm itself requires less memory (less than 12G) than loading the training data in main memory (\u223c25G)."}, {"heading": "5.2 X-RAY MICROBEAM SPEECH DATA", "text": "In the second set of experiments, we apply approximate KCCA to the task of learning acoustic features for automatic speech recognition. We use the Wisconsin X-ray microbeam (XRMB)\n6Following Xie et al. (2015), kernel widths are estimated from the median of pairwise distances between 4000 randomly selected training samples.\n7We thank the authors for providing their MATLAB implementation of DSGD. 8Xie et al. (2015) used a version of random Fourier features with both cos and sin functions, so the number\nof learnable parameters (in U and V) of DSGD is twice that of KNOI for the same M .\ncorpus (Westbury, 1994) of simultaneously recorded speech and articulatory measurements from 47 American English speakers. It has previously been shown that multi-view feature learning via CCA/KCCA greatly improves phonetic recognition performance given audio input alone (Arora & Livescu, 2013; Wang et al., 2015a;b).\nWe follow the setup of Wang et al. (2015a;b) and use the learned features (KCCA projections) for speaker-independent phonetic recognition.9 The two input views are acoustic features (39D features consisting of mel frequency cepstral coefficients (MFCCs) and their first and second derivatives) and articulatory features (horizontal/vertical displacement of 8 pellets attached to several parts of the vocal tract) concatenated over a 7-frame window around each frame, giving 273D acoustic inputs and 112D articulatory inputs for each view. The XRMB speakers are split into disjoint sets of 35/8/2/2\n9Unlike Wang et al. (2015a;b), who used the HTK toolkit (Young et al., 1999), we use the Kaldi speech recognition toolkit (Povey et al., 2011) for feature extraction and recognition with hidden Markov models. Our results therefore don\u2019t match those in Wang et al. (2015a;b) for the same types of features, but the relative merits of different types of features are consistent.\nspeakers for feature learning/recognizer training/tuning/testing. The 35 speakers for feature learning are fixed; the remaining 12 are used in a 6-fold experiment (recognizer training on 8 speakers, tuning on 2 speakers, and testing on the remaining 2 speakers). Each speaker has roughly 50K frames, giving 1.43M training frames for KCCA training. We remove the per-speaker mean and variance of the articulatory measurements for each training speaker. All of the learned feature types are used in a \u201ctandem\u201d speech recognition approach (Hermansky et al., 2000), i.e., they are appended to the original 39D features and used in a standard hidden Markov model (HMM)-based recognizer with Gaussian mixture observation distributions.\nFor each fold, we select the hyperparameters based on recognition accuracy on the tuning set. For each algorithm, the feature dimensionality L is tuned over {30, 50, 70}, and the kernel widths for each view are tuned by grid search. We initially set M = 5000 for FKCCA/NKCCA, and also test FKCCA at M = 30000 (the largest M at which we could afford to obtain an exact SVD solution on a workstation with 64G main memory) with kernel widths tuned at M = 5000; we could not obtain results for NKCCA with M = 30000 in 48 hours. For KNOI, we set M = 100000 and tune the optimization parameters on a rough grid. The tuned KNOI uses minibatch size b = 2500, time constant \u03c1 = 0, fixed learning rate \u03b7 = 0.01, and momentum \u00b5 = 0.995. For this combination of b and M , we are able to run the algorithm on a Tesla K40 GPU (with 12G memory), and each epoch (one pass over the 1.43M training samples) takes only 7.3 minutes. We run KNOI for 5 epochs and use the resulting acoustic view projection for recognition. We have also tried to run KNOI for 10 epochs and the recognition performance does not change, even though the total canonical correlation keeps improving on both training and tuning sets.\nFor comparison, we report the performance of a baseline recognizer that uses only the original MFCC features, and the performance of deep CCA (DCCA) as described in Wang et al. (2015b), which uses 3 hidden layers of 1500ReLU units followed by a linear output layer in the acoustic view, and only a linear output layer in the articulatory view. With this architecture, each epoch of DCCA takes about 8 minutes on a Tesla K40 GPU, on par with KNOI. Note that this DCCA architecture was tuned carefully for low PER rather than high canonical correlation. This architecture produces a total correlation of about 25 (out of a maximum of L = 70) on tuning data, while KNOI achieves 46.7. DCCA using deeper nonlinear networks for the second view can achieve even better total canonical correlation, but its PER performance then becomes significantly worse.\nPhone error rates (PERs) obtained by different algorithms are given in Table 2, where smaller PER indicates better recognition performance. It is clear that all CCA-based features significantly improve over the baseline. Also, a large M is necessary for KCCA to be competitive with deep neural network methods, which is consistent with the findings of Huang et al. (2014); Lu et al. (2015b) when using random Fourier features for speech data (where the task is frame classification). Overall, KNOI outperforms the other approximate KCCA algorithms, although DCCA is still the best performer."}, {"heading": "6 CONCLUSION", "text": "We have proposed kernel nonlinear orthogonal iterations (KNOI), a memory-efficient approximate KCCA algorithm based on random Fourier features and stochastic training of linear CCA. It scales\nbetter to large data and outperforms previous approximate KCCA algorithms in both the objective values (total canonical correlation) and running times (with GPU support).\nIt is straightforward to incorporate in our algorithm the faster random features of Le et al. (2013) which can be generated (for view 1) in time O(NM log dx) instead of O(NMdx), or the Taylor features of Cotter et al. (2011) which is preferable for sparse inputs, and random features for dot product or polynomial kernels (Kar & Karnick, 2012; Hamid et al., 2014; Pennington et al., 2015), which have proven to be useful for different domains. It is also worth exploring parallelization and multiple kernel learning strategies of Lu et al. (2015b) with random Fourier features to further bridge the gap between kernel methods and deep neural network methods.\nFinally, as noted before, our algorithm does not use unbiased estimates of the gradient of the CCA objective. However, unbiased gradient estimates are not necessary for convergence of stochastic algorithms in general; a prominent example is the popular Oja\u2019s rule for stochastic PCA (see discussions in Balsubramani et al., 2013; Shamir, 2015). Deriving global convergence properties for our algorithm is a challenging topic and the subject of ongoing work."}, {"heading": "ACKNOWLEDGEMENT", "text": "This research was supported by NSF grant IIS-1321015. The opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency. The Tesla K40 GPUs used for this research were donated by NVIDIA Corporation. We thank Bo Xie for providing his implementation of the doubly stochastic gradient algorithm for approximate KCCA, and Nati Srebro for helpful discussions."}], "references": [{"title": "A kernel method for canonical correlation analysis", "author": ["Akaho", "Shotaro"], "venue": "In Proceedings of the International Meeting of the Psychometric Society (IMPS2001),", "citeRegEx": "Akaho and Shotaro.,? \\Q2001\\E", "shortCiteRegEx": "Akaho and Shotaro.", "year": 2001}, {"title": "An Introduction to Multivariate Statistical Analysis", "author": ["T.W. Anderson"], "venue": null, "citeRegEx": "Anderson,? \\Q2003\\E", "shortCiteRegEx": "Anderson", "year": 2003}, {"title": "Deep canonical correlation analysis", "author": ["Andrew", "Galen", "Arora", "Raman", "Bilmes", "Jeff", "Livescu", "Karen"], "venue": "In Proc. of the 30th Int. Conf. Machine Learning (ICML", "citeRegEx": "Andrew et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Andrew et al\\.", "year": 2013}, {"title": "Kernel CCA for multi-view learning of acoustic features using articulatory measurements", "author": ["Arora", "Raman", "Livescu", "Karen"], "venue": "In Symposium on Machine Learning in Speech and Language Processing (MLSLP),", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "Multi-view CCA-based acoustic features for phonetic recognition across speakers and domains", "author": ["Arora", "Raman", "Livescu", "Karen"], "venue": "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP\u201913),", "citeRegEx": "Arora et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2013}, {"title": "Stochastic optimization for PCA and PLS", "author": ["Arora", "Raman", "Cotter", "Andy", "Livescu", "Karen", "Srebro", "Nati"], "venue": "In 50th Annual Allerton Conference on Communication,", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "Stochastic optimization of PCA with capped MSG", "author": ["Arora", "Raman", "Cotter", "Andy", "Srebro", "Nati"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Arora et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2013}, {"title": "Efficient dimensionality reduction for canonical correlation analysis", "author": ["Avron", "Haim", "Boutsidis", "Christos", "Toledo", "Sivan", "Zouzias", "Anastasios"], "venue": "In Proc. of the 30th Int. Conf. Machine Learning (ICML", "citeRegEx": "Avron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Avron et al\\.", "year": 2013}, {"title": "Kernel independent component analysis", "author": ["Bach", "Francis R", "Jordan", "Michael I"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bach et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2002}, {"title": "A probabilistic interpretation of canonical correlation analysis", "author": ["Bach", "Francis R", "Jordan", "Michael I"], "venue": "Technical Report 688,", "citeRegEx": "Bach et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2005}, {"title": "The fast convergence of incremental PCA", "author": ["Balsubramani", "Akshay", "Dasgupta", "Sanjoy", "Freund", "Yoav"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Balsubramani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Balsubramani et al\\.", "year": 2013}, {"title": "On the regularization of canonical correlation analysis", "author": ["Bie", "Tijl De", "Moor", "Bart De"], "venue": "www.esat.kuleuven.ac.be/sista-cosic-docarch/,", "citeRegEx": "Bie et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bie et al\\.", "year": 2003}, {"title": "Semi-supervised kernel canonical correlation analysis with application to human fMRI", "author": ["Blaschkoa", "Matthew B", "Sheltonb", "Jacquelyn A", "Bartelsc", "Andreas", "Lamperte", "Christoph H", "Gretton", "Arthur"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Blaschkoa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Blaschkoa et al\\.", "year": 2011}, {"title": "Canonical correlation: A tutorial", "author": ["Borga", "Magnus"], "venue": null, "citeRegEx": "Borga and Magnus.,? \\Q2001\\E", "shortCiteRegEx": "Borga and Magnus.", "year": 2001}, {"title": "The tradeoffs of large scale learning", "author": ["Bottou", "Leon", "Bousquet", "Olivier"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Bottou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2008}, {"title": "Multi-view clustering via canonical correlation analysis", "author": ["Chaudhuri", "Kamalika", "Kakade", "Sham M", "Livescu", "Karen", "Sridharan", "Karthik"], "venue": "In Proc. of the 26th Int. Conf. Machine Learning", "citeRegEx": "Chaudhuri et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2009}, {"title": "Information bottleneck for Gaussian variables", "author": ["Chechik", "Gal", "Globerson", "Amir", "Tishby", "Naftali", "Weiss", "Yair"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Chechik et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chechik et al\\.", "year": 2005}, {"title": "Explicit approximations of the gaussian kernel", "author": ["Cotter", "Andrew", "Keshet", "Joseph", "Srebro", "Nathan"], "venue": "[cs.AI], September", "citeRegEx": "Cotter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cotter et al\\.", "year": 2011}, {"title": "Multi-view learning of word embeddings via CCA", "author": ["Dhillon", "Paramveer", "Foster", "Dean", "Ungar", "Lyle"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Dhillon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2011}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Faruqui", "Manaal", "Dyer", "Chris"], "venue": "In Proceedings of EACL,", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Multi-view dimensionality reduction via canonical correlation analysis", "author": ["Foster", "Dean P", "Johnson", "Rie", "Kakade", "Sham M", "Zhang", "Tong"], "venue": null, "citeRegEx": "Foster et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Foster et al\\.", "year": 2009}, {"title": "Statistical consistency of kernel canonical correlation analysis", "author": ["Fukumizu", "Kenji", "Bach", "Francis R", "Gretton", "Arthur"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fukumizu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2007}, {"title": "Revisiting the nystrom method for improved large-scale machine learning", "author": ["Gittens", "Alex", "Mahoney", "Michael"], "venue": "In Proc. of the 30th Int. Conf. Machine Learning (ICML", "citeRegEx": "Gittens et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gittens et al\\.", "year": 2013}, {"title": "Linear Algebra for Signal Processing, volume 69 of The IMA Volumes in Mathematics and its Applications, chapter The Canonical Correlations of Matrix Pairs", "author": ["Golub", "Gene H", "Zha", "Hongyuan"], "venue": null, "citeRegEx": "Golub et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Golub et al\\.", "year": 1995}, {"title": "Learning bilingual lexicons from monolingual corpora", "author": ["Haghighi", "Aria", "Liang", "Percy", "Berg-Kirkpatrick", "Taylor", "Klein", "Dan"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Haghighi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Haghighi et al\\.", "year": 2008}, {"title": "An algorithm for the principal component analysis of large data sets", "author": ["Halko", "Nathan", "Martinsson", "Per-Gunnar", "Shkolnisky", "Yoel", "Tygert", "Mark"], "venue": "SIAM J. Sci. Comput.,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "Compact random feature maps", "author": ["Hamid", "Raffay", "Xiao", "Ying", "Gittens", "Alex", "Decoste", "Dennis"], "venue": "In Proc. of the 31st Int. Conf. Machine Learning (ICML", "citeRegEx": "Hamid et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hamid et al\\.", "year": 2014}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["Hardoon", "David R", "Szedmak", "Sandor", "Shawe-Taylor", "John"], "venue": "Neural Computation,", "citeRegEx": "Hardoon et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hardoon et al\\.", "year": 2004}, {"title": "Unsupervised analysis of fMRI data using kernel canonical correlation", "author": ["Hardoon", "David R", "Mour\u00e4o-Miranda", "Janaina", "Brammer", "Michael", "Shawe-Taylor", "John"], "venue": null, "citeRegEx": "Hardoon et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hardoon et al\\.", "year": 2007}, {"title": "Tandem connectionist feature extraction for conventional HMM systems", "author": ["Hermansky", "Hynek", "Ellis", "Daniel P. W", "Sharma", "Sangita"], "venue": "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc", "citeRegEx": "Hermansky et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Hermansky et al\\.", "year": 2000}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Hodosh", "Micah", "Young", "Peter", "Hockenmaier", "Julia"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hodosh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Relations between two sets of variates", "author": ["Hotelling", "Harold"], "venue": "Biometrika, 28(3/4):321\u2013377,", "citeRegEx": "Hotelling and Harold.,? \\Q1936\\E", "shortCiteRegEx": "Hotelling and Harold.", "year": 1936}, {"title": "Kernel methods match deep neural networks on TIMIT: Scalable learning in high-dimensional random Fourier spaces", "author": ["Huang", "Po-Sen", "Avron", "Haim", "Sainath", "Tara", "Sindhwani", "Vikas", "Ramabhadran", "Bhuvana"], "venue": "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP\u201914),", "citeRegEx": "Huang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2014}, {"title": "Multi-view regression via canonical correlation analysis", "author": ["Kakade", "Sham M", "Foster", "Dean P"], "venue": "In Proc. of the 20th Annual Conference on Learning Theory (COLT\u201907),", "citeRegEx": "Kakade et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2007}, {"title": "Random feature maps for dot product kernels", "author": ["Kar", "Purushottam", "Karnick", "Harish"], "venue": "In Proc. of the 15th Int. Workshop on Artificial Intelligence and Statistics (AISTATS", "citeRegEx": "Kar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kar et al\\.", "year": 2012}, {"title": "Discriminative learning and recognition of image set classes using canonical correlations", "author": ["Kim", "Tae-Kyun", "Kittler", "Josef", "Cipolla", "Roberto"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Kim et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2007}, {"title": "A method of stochastic approximation for the determination of the least eigenvalue of a symmetric matrix", "author": ["T.P. Krasulina"], "venue": "USSR Computational Mathematics and Mathematical Physics,", "citeRegEx": "Krasulina,? \\Q1969\\E", "shortCiteRegEx": "Krasulina", "year": 1969}, {"title": "Sampling methods for the nystr\u00f6m method", "author": ["Kumar", "Sanjiv", "Mohri", "Mehryar", "Talwalkar", "Ameet"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2012}, {"title": "Kernel and nonlinear canonical correlation analysis", "author": ["P.L. Lai", "C. Fyfe"], "venue": "Int. J. Neural Syst.,", "citeRegEx": "Lai and Fyfe,? \\Q2000\\E", "shortCiteRegEx": "Lai and Fyfe", "year": 2000}, {"title": "Fastfood - computing Hilbert space expansions in loglinear time", "author": ["Le", "Quoc", "Sarlos", "Tamas", "Smola", "Alexander"], "venue": "In Proc. of the 30th Int. Conf. Machine Learning (ICML", "citeRegEx": "Le et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "Making large-scale Nystr\u00f6m approximation possible", "author": ["M. Li", "J.T. Kwok", "B. Lu"], "venue": "In Proc. of the 27th Int. Conf. Machine Learning (ICML", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Using KCCA for japanese-english cross-language information retrieval and classification", "author": ["Li", "Yaoyong", "Shawe-Taylor", "John"], "venue": "Journal of Intelligent Information Systems,", "citeRegEx": "Li et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Li et al\\.", "year": 2005}, {"title": "Training invariant support vector machines using selective sampling", "author": ["Loosli", "Ga\u00eblle", "Canu", "St\u00e9phane", "Bottou", "L\u00e9on"], "venue": "In Large Scale Kernel Machines,", "citeRegEx": "Loosli et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Loosli et al\\.", "year": 2007}, {"title": "The randomized dependence coefficient", "author": ["Lopez-Paz", "David", "Hennig", "Philipp", "Sch\u00f6lkopf", "Bernhard"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Lopez.Paz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lopez.Paz et al\\.", "year": 2013}, {"title": "Randomized nonlinear component analysis", "author": ["Lopez-Paz", "David", "Sra", "Suvrit", "Smola", "Alex", "Ghahramani", "Zoubin", "Schoelkopf", "Bernhard"], "venue": "In Proc. of the 31st Int. Conf. Machine Learning (ICML", "citeRegEx": "Lopez.Paz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lopez.Paz et al\\.", "year": 2014}, {"title": "Deep multilingual correlation for improved word embeddings", "author": ["Lu", "Ang", "Wang", "Weiran", "Bansal", "Mohit", "Gimpel", "Kevin", "Livescu", "Karen"], "venue": "In The 2015 Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL-HLT 2015),", "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Large scale canonical correlation analysis with iterative least squares", "author": ["Lu", "Yichao", "Foster", "Dean P"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Lu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2014}, {"title": "How to scale up kernel methods to be as good as deep neural nets", "author": ["Lu", "Zhiyun", "May", "Avner", "Liu", "Kuan", "Garakani", "Alireza Bagheri", "Guo", "Dong", "Bellet", "Aur\u00e9lien", "Fan", "Linxi", "Collins", "Michael", "Kingsbury", "Brian", "Picheny", "Sha", "Fei"], "venue": "[cs.LG],", "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Finding linear structure in large datasets with scalable canonical correlation analysis", "author": ["Ma", "Zhuang", "Lu", "Yichao", "Foster", "Dean"], "venue": "In Proc. of the 32st Int. Conf. Machine Learning (ICML", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "An efficient algorithm for information decomposition and extraction", "author": ["Makur", "Anuran", "Kozynski", "Fab\u0131\u0301an", "Huang", "Shao-Lun", "Zheng", "Lizhong"], "venue": "In 53nd Annual Allerton Conference on Communication, Control and Computing,", "citeRegEx": "Makur et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Makur et al\\.", "year": 2015}, {"title": "Nonlinear feature extraction using generalized canonical correlation analysis", "author": ["Melzer", "Thomas", "Reiter", "Michael", "Bischof", "Horst"], "venue": "In Proc. of the 11th Int. Conf. Artificial Neural Networks", "citeRegEx": "Melzer et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Melzer et al\\.", "year": 2001}, {"title": "Memory limited, streaming PCA", "author": ["Mitliagkas", "Ioannis", "Caramanis", "Constantine", "Jain", "Prateek"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Mitliagkas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mitliagkas et al\\.", "year": 2013}, {"title": "Chemometric classification of some European wines using pyrolysis mass spectrometry", "author": ["Montanarella", "Luca", "Bassani", "Maria Rosa", "Br\u00e9as", "Olivier"], "venue": "Rapid Communications in Mass Spectrometry,", "citeRegEx": "Montanarella et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Montanarella et al\\.", "year": 1995}, {"title": "On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix", "author": ["Oja", "Erkki", "Karhunen", "Juha"], "venue": "J. Math. Anal. Appl.,", "citeRegEx": "Oja et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Oja et al\\.", "year": 1985}, {"title": "Spherical random features for polynomial kernels", "author": ["Pennington", "Jeffrey", "Yu", "Felix", "Kumar", "Sanjiv"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Pennington et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2015}, {"title": "The Kaldi speech recognition toolkit", "author": ["Povey", "Daniel", "Ghoshal", "Arnab", "Boulianne", "Gilles", "Burget", "Lukas", "Glembek", "Ondrej", "Goel", "Nagendra", "Hannemann", "Mirko", "Motlicek", "Petr", "Qian", "Yanmin", "Schwarz", "Silovsky", "Jan", "Stemmer", "Georg", "Vesely", "Karel"], "venue": "In Proc. of the 2011 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU", "citeRegEx": "Povey et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Povey et al\\.", "year": 2011}, {"title": "Random features for large-scale kernel machines", "author": ["Rahimi", "Ali", "Recht", "Ben"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Rahimi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rahimi et al\\.", "year": 2008}, {"title": "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning", "author": ["Rahimi", "Ali", "Recht", "Benjamin"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Rahimi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rahimi et al\\.", "year": 2009}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["Roux", "Nicolas Le", "Schmidt", "Mark", "Bach", "Francis"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Roux et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2012}, {"title": "Correcting errors in speech recognition with articulatory dynamics", "author": ["Rudzicz", "Frank"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Rudzicz and Frank.,? \\Q2010\\E", "shortCiteRegEx": "Rudzicz and Frank.", "year": 2010}, {"title": "Learning with Kernels. Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["Sch\u00f6lkopf", "Bernhard", "Smola", "Alexander J"], "venue": "Adaptive Computation and Machine Learning Series", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2001}, {"title": "A stochastic PCA and SVD algorithm with an exponential convergence rate", "author": ["Shamir", "Ohad"], "venue": "In Proc. of the 32st Int. Conf. Machine Learning (ICML", "citeRegEx": "Shamir and Ohad.,? \\Q2015\\E", "shortCiteRegEx": "Shamir and Ohad.", "year": 2015}, {"title": "Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora", "author": ["Socher", "Richard", "Li", "Fei-Fei"], "venue": "In Proc. of the 2010 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Socher et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2010}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George", "Hinton", "Geoffrey"], "venue": "In Proc. of the 30th Int. Conf. Machine Learning (ICML", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Canonical ridge and econometrics of joint production", "author": ["H.D. Vinod"], "venue": "Journal of Econometrics,", "citeRegEx": "Vinod,? \\Q1976\\E", "shortCiteRegEx": "Vinod", "year": 1976}, {"title": "Inferring a semantic representation of text via cross-language correlation analysis", "author": ["Vinokourov", "Alexei", "Cristianini", "Nello", "Shawe-Taylor", "John"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Vinokourov et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Vinokourov et al\\.", "year": 2003}, {"title": "Unsupervised learning of acoustic features via deep canonical correlation analysis", "author": ["Wang", "Weiran", "Arora", "Raman", "Livescu", "Karen", "Bilmes", "Jeff"], "venue": "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP\u201915),", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "On deep multi-view representation learning", "author": ["Wang", "Weiran", "Arora", "Raman", "Livescu", "Karen", "Bilmes", "Jeff"], "venue": "In Proc. of the 32st Int. Conf. Machine Learning (ICML", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Stochastic optimization for deep cca via nonlinear orthogonal iterations", "author": ["Wang", "Weiran", "Arora", "Raman", "Srebro", "Nati", "Livescu", "Karen"], "venue": "In 53nd Annual Allerton Conference on Communication, Control and Computing,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Randomized online PCA algorithms with regret bounds that are logarithmic in the dimension", "author": ["Warmuth", "Manfred K", "Kuzmin", "Dima"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Warmuth et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Warmuth et al\\.", "year": 2008}, {"title": "X-Ray Microbeam Speech Production", "author": ["Westbury", "John R"], "venue": "Database User\u2019s Handbook Version", "citeRegEx": "Westbury and R.,? \\Q1994\\E", "shortCiteRegEx": "Westbury and R.", "year": 1994}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["Williams", "Christopher K. I", "Seeger", "Matthias"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Williams et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2001}, {"title": "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis", "author": ["Witten", "Daniela M", "Tibshirani", "Robert", "Hastie", "Trevor"], "venue": null, "citeRegEx": "Witten et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Witten et al\\.", "year": 2009}, {"title": "Scale up nonlinear component analysis with doubly stochastic gradients", "author": ["Xie", "Bo", "Liang", "Yingyu", "Song", "Le"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Xie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2015}, {"title": "Kernel Methods in Computational Biology, chapter Heterogeneous Data Comparison and Gene Selection with Kernel Canonical Correlation Analysis, pp. 209\u2013229", "author": ["Yamanishi", "Yoshihiro", "Vert", "Jean-Philippe", "Kanehisa", "Minoru"], "venue": null, "citeRegEx": "Yamanishi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Yamanishi et al\\.", "year": 2004}, {"title": "Quasi-Monte Carlo feature maps for shift-invariant kernels", "author": ["Yang", "Jiyan", "Sindhwani", "Vikas", "Avron", "Haim", "Mahoney", "Michael"], "venue": "In Proc. of the 31st Int. Conf. Machine Learning (ICML", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Nystr\u00f6m method vs random Fourier features: A theoretical and empirical comparison", "author": ["Yang", "Tianbao", "Li", "Yu-Feng", "Mahdavi", "Mehrdad", "Jin", "Rong", "Zhou", "Zhi-Hua"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Yang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2012}, {"title": "Adaptive canonical correlation analysis based on matrix manifolds", "author": ["Yger", "Florian", "Berar", "Maxime", "Gasso", "Gilles", "Rakotomamonjy", "Alain"], "venue": "In Proc. of the 29th Int. Conf. Machine Learning (ICML", "citeRegEx": "Yger et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yger et al\\.", "year": 2012}, {"title": "The HTK book version 2.2", "author": ["Young", "Steve J", "Kernshaw", "Dan", "Odell", "Julian", "Ollason", "Dave", "Valtchev", "Valtcho", "Woodland", "Phil"], "venue": "Technical report, Entropic,", "citeRegEx": "Young et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Young et al\\.", "year": 1999}, {"title": "Density-weighted Nystr\u00f6m method for computing large kernel eigen-systems", "author": ["Zhang", "Kai", "Kwok", "James T"], "venue": "Neural Computation,", "citeRegEx": "Zhang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2009}, {"title": "Clustered Nystr\u00f6m method for large scale manifold learning and dimension reduction", "author": ["Zhang", "Kai", "Kwok", "James T"], "venue": "IEEE Trans. Neural Networks,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "Canonical correlation analysis (CCA, Hotelling, 1936) and its extensions are ubiquitous techniques in scientific research areas for revealing the common sources of variability in multiple views of the same phenomenon, including meteorology (Anderson, 2003), chemometrics (Montanarella et al.", "startOffset": 240, "endOffset": 256}, {"referenceID": 52, "context": "Canonical correlation analysis (CCA, Hotelling, 1936) and its extensions are ubiquitous techniques in scientific research areas for revealing the common sources of variability in multiple views of the same phenomenon, including meteorology (Anderson, 2003), chemometrics (Montanarella et al., 1995), genomics (Witten et al.", "startOffset": 271, "endOffset": 298}, {"referenceID": 72, "context": ", 1995), genomics (Witten et al., 2009), computer vision (Kim et al.", "startOffset": 18, "endOffset": 39}, {"referenceID": 35, "context": ", 2009), computer vision (Kim et al., 2007; Socher & Li, 2010), speech recognition (Rudzicz, 2010; Arora & Livescu, 2013; Wang et al.", "startOffset": 25, "endOffset": 62}, {"referenceID": 65, "context": ", 2015a), and natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011; Hodosh et al., 2013; Faruqui & Dyer, 2014; Lu et al., 2015a).", "startOffset": 42, "endOffset": 173}, {"referenceID": 24, "context": ", 2015a), and natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011; Hodosh et al., 2013; Faruqui & Dyer, 2014; Lu et al., 2015a).", "startOffset": 42, "endOffset": 173}, {"referenceID": 18, "context": ", 2015a), and natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011; Hodosh et al., 2013; Faruqui & Dyer, 2014; Lu et al., 2015a).", "startOffset": 42, "endOffset": 173}, {"referenceID": 30, "context": ", 2015a), and natural language processing (Vinokourov et al., 2003; Haghighi et al., 2008; Dhillon et al., 2011; Hodosh et al., 2013; Faruqui & Dyer, 2014; Lu et al., 2015a).", "startOffset": 42, "endOffset": 173}, {"referenceID": 64, "context": "U\u03a3xxU = V\u03a3yyV = I, ui \u03a3xyvj = 0, for i 6= j, where (U,V) are the projection matrices for each view, \u03a3xy = 1 N \u2211N i=1 xiy \u22a4 i , \u03a3xx = 1 N \u2211N i=1 xix \u22a4 i + rxI, \u03a3yy = 1 N \u2211N i=1 yiy \u22a4 i + ryI are the cross- and auto-covariance matrices, and (rx, ry) \u2265 0 are regularization parameters (Vinod, 1976; Bie & Moor, 2003).", "startOffset": 282, "endOffset": 313}, {"referenceID": 15, "context": "2 The theoretical properties of CCA (Kakade & Foster, 2007; Chaudhuri et al., 2009; Foster et al., 2009) and its connection to other methods (Borga, 2001; Bach & Jordan, 2005; Chechik et al.", "startOffset": 36, "endOffset": 104}, {"referenceID": 20, "context": "2 The theoretical properties of CCA (Kakade & Foster, 2007; Chaudhuri et al., 2009; Foster et al., 2009) and its connection to other methods (Borga, 2001; Bach & Jordan, 2005; Chechik et al.", "startOffset": 36, "endOffset": 104}, {"referenceID": 16, "context": ", 2009) and its connection to other methods (Borga, 2001; Bach & Jordan, 2005; Chechik et al., 2005) have also been studied.", "startOffset": 44, "endOffset": 100}, {"referenceID": 50, "context": "To overcome this issue, kernel CCA (KCCA) was proposed indepedently by several researchers (Lai & Fyfe, 2000; Akaho, 2001; Melzer et al., 2001) and has become a common technique in statistics and machine learning (Bach & Jordan, 2002; Hardoon et al.", "startOffset": 91, "endOffset": 143}, {"referenceID": 27, "context": ", 2001) and has become a common technique in statistics and machine learning (Bach & Jordan, 2002; Hardoon et al., 2004).", "startOffset": 77, "endOffset": 120}, {"referenceID": 27, "context": "KCCA has been successfully used for cross-modality retrieval (Hardoon et al., 2004; Li & Shawe-Taylor, 2005; Socher & Li, 2010; Hodosh et al., 2013), acoustic feature learning (Arora & Livescu, 2013), computational biology (Yamanishi et al.", "startOffset": 61, "endOffset": 148}, {"referenceID": 30, "context": "KCCA has been successfully used for cross-modality retrieval (Hardoon et al., 2004; Li & Shawe-Taylor, 2005; Socher & Li, 2010; Hodosh et al., 2013), acoustic feature learning (Arora & Livescu, 2013), computational biology (Yamanishi et al.", "startOffset": 61, "endOffset": 148}, {"referenceID": 74, "context": ", 2013), acoustic feature learning (Arora & Livescu, 2013), computational biology (Yamanishi et al., 2004; Hardoon et al., 2007; Blaschkoa et al., 2011), and statistical independence measurement (Bach & Jordan, 2002; Fukumizu et al.", "startOffset": 82, "endOffset": 152}, {"referenceID": 28, "context": ", 2013), acoustic feature learning (Arora & Livescu, 2013), computational biology (Yamanishi et al., 2004; Hardoon et al., 2007; Blaschkoa et al., 2011), and statistical independence measurement (Bach & Jordan, 2002; Fukumizu et al.", "startOffset": 82, "endOffset": 152}, {"referenceID": 12, "context": ", 2013), acoustic feature learning (Arora & Livescu, 2013), computational biology (Yamanishi et al., 2004; Hardoon et al., 2007; Blaschkoa et al., 2011), and statistical independence measurement (Bach & Jordan, 2002; Fukumizu et al.", "startOffset": 82, "endOffset": 152}, {"referenceID": 21, "context": ", 2011), and statistical independence measurement (Bach & Jordan, 2002; Fukumizu et al., 2007; Lopez-Paz et al., 2013).", "startOffset": 50, "endOffset": 118}, {"referenceID": 43, "context": ", 2011), and statistical independence measurement (Bach & Jordan, 2002; Fukumizu et al., 2007; Lopez-Paz et al., 2013).", "startOffset": 50, "endOffset": 118}, {"referenceID": 44, "context": "With rank-M approximations of the kernel matrices, the cost of solving approximate KCCA reduces to O(MN) (see, e.g., Bach & Jordan, 2002; Lopez-Paz et al., 2014).", "startOffset": 105, "endOffset": 161}, {"referenceID": 76, "context": "Typically, ranks of a few hundred to a few thousand are used for the low-rank kernel approximations (Yang et al., 2012; Le et al., 2013; Lopez-Paz et al., 2014).", "startOffset": 100, "endOffset": 160}, {"referenceID": 39, "context": "Typically, ranks of a few hundred to a few thousand are used for the low-rank kernel approximations (Yang et al., 2012; Le et al., 2013; Lopez-Paz et al., 2014).", "startOffset": 100, "endOffset": 160}, {"referenceID": 44, "context": "Typically, ranks of a few hundred to a few thousand are used for the low-rank kernel approximations (Yang et al., 2012; Le et al., 2013; Lopez-Paz et al., 2014).", "startOffset": 100, "endOffset": 160}, {"referenceID": 44, "context": "The particular variant of approximate KCCA we use, called randomized CCA (Lopez-Paz et al., 2014), transforms the original inputs to an M -dimensional feature space using random features (Rahimi & Recht, 2008; 2009) so that inner products in the new feature space approximate the kernel function.", "startOffset": 73, "endOffset": 97}, {"referenceID": 48, "context": "We then make use of a stochastic optimization algorithm, recently proposed for linear CCA and its deep neural network extension deep CCA (Ma et al., 2015; Wang et al., 2015c), to reduce the memory requirement for solving the resulting linear CCA problem.", "startOffset": 137, "endOffset": 174}, {"referenceID": 50, "context": "In KCCA, we transform the inputs {xi}i=1 of view 1 and {yi} N i=1 of view 2 using feature mappings \u03c6x and \u03c6y associated with some positive semi-definite kernels kx and ky respectively, and then solve the linear CCA problem (1) for the feature-mapped inputs (Lai & Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach & Jordan, 2002; Hardoon et al., 2004).", "startOffset": 257, "endOffset": 352}, {"referenceID": 27, "context": "In KCCA, we transform the inputs {xi}i=1 of view 1 and {yi} N i=1 of view 2 using feature mappings \u03c6x and \u03c6y associated with some positive semi-definite kernels kx and ky respectively, and then solve the linear CCA problem (1) for the feature-mapped inputs (Lai & Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach & Jordan, 2002; Hardoon et al., 2004).", "startOffset": 257, "endOffset": 352}, {"referenceID": 27, "context": "Various kernel approximation techniques have been proposed to scale up KCCA, including Cholesky decomposition (Bach & Jordan, 2002), partial Gram-Schmidt (Hardoon et al., 2004), and incremental SVD (Arora & Livescu, 2012).", "startOffset": 154, "endOffset": 176}, {"referenceID": 76, "context": "We can extract such features for both views, and apply linear CCA to them to approximate the KCCA solution (Yang et al., 2012; Lopez-Paz et al., 2014).", "startOffset": 107, "endOffset": 150}, {"referenceID": 44, "context": "We can extract such features for both views, and apply linear CCA to them to approximate the KCCA solution (Yang et al., 2012; Lopez-Paz et al., 2014).", "startOffset": 107, "endOffset": 150}, {"referenceID": 40, "context": "Although there have been various sampling/approximation strategies for the Nystr\u00f6m method (Li et al., 2010; Zhang & Kwok, 2009; 2010; Kumar et al., 2012; Gittens & Mahoney, 2013), their constructions are more involved.", "startOffset": 90, "endOffset": 178}, {"referenceID": 37, "context": "Although there have been various sampling/approximation strategies for the Nystr\u00f6m method (Li et al., 2010; Zhang & Kwok, 2009; 2010; Kumar et al., 2012; Gittens & Mahoney, 2013), their constructions are more involved.", "startOffset": 90, "endOffset": 178}, {"referenceID": 44, "context": "as (Lopez-Paz et al., 2014)", "startOffset": 3, "endOffset": 27}, {"referenceID": 26, "context": "This approach has been extended to other types of kernels (Kar & Karnick, 2012; Hamid et al., 2014; Pennington et al., 2015).", "startOffset": 58, "endOffset": 124}, {"referenceID": 54, "context": "This approach has been extended to other types of kernels (Kar & Karnick, 2012; Hamid et al., 2014; Pennington et al., 2015).", "startOffset": 58, "endOffset": 124}, {"referenceID": 75, "context": "A careful quasi-Monte Carlo scheme for sampling from p(w) (Yang et al., 2014), and structured feature transformation for accelerating the computation of w i x (Le et al.", "startOffset": 58, "endOffset": 77}, {"referenceID": 39, "context": ", 2014), and structured feature transformation for accelerating the computation of w i x (Le et al., 2013), have also been studied.", "startOffset": 89, "endOffset": 106}, {"referenceID": 26, "context": "This approach has been extended to other types of kernels (Kar & Karnick, 2012; Hamid et al., 2014; Pennington et al., 2015). A careful quasi-Monte Carlo scheme for sampling from p(w) (Yang et al., 2014), and structured feature transformation for accelerating the computation of w i x (Le et al., 2013), have also been studied. Leveraging this result, Rahimi & Recht (2009) propose to first extract M -dimensional random Fourier features for input x as (with slight abuse of notation)", "startOffset": 80, "endOffset": 374}, {"referenceID": 76, "context": "Although the Nystr\u00f6m approximation can be more accurate at the same rank (Yang et al., 2012), the computational efficiency and smaller memory cost of random features make them more appealing for large-scale problems in practice.", "startOffset": 73, "endOffset": 92}, {"referenceID": 43, "context": "g, Lu et al., 2015b used the recently proposed stochastic gradient method by Roux et al. 2012 for the task of multinomial logistic regression with random Fourier features). Rahimi & Recht (2009) showed that it allows us to effectively learn nonlinear models and still obtain good learning guarantees.", "startOffset": 3, "endOffset": 195}, {"referenceID": 43, "context": "Lopez-Paz et al. (2014) have recently applied the random feature idea to KCCA, by extracting M dimensional random Fourier features {(\u03c6x(xi), \u03c6y(yi))}i=1 for both views and solving exactly a linear CCA on the transformed pairs.", "startOffset": 0, "endOffset": 24}, {"referenceID": 32, "context": "Notice, however, in contrast to the classification or regression objectives that are more commonly used with random Fourier features (Rahimi & Recht, 2009; Huang et al., 2014; Lu et al., 2015b), the CCA objective (1) can not be written as an unconstrained sum or expectation of losses incurred at each training sample (in fact all training samples are coupled together through the constraints).", "startOffset": 133, "endOffset": 193}, {"referenceID": 2, "context": "(2015c) have developed stochastic optimization algorithms, referred to as AppGrad (Augmented Approximate Gradient) and NOI (Nonlinear Orthogonal Iterations) respectively, for linear CCA and its deep neural network extension deep CCA (Andrew et al., 2013).", "startOffset": 233, "endOffset": 254}, {"referenceID": 31, "context": "Notice, however, in contrast to the classification or regression objectives that are more commonly used with random Fourier features (Rahimi & Recht, 2009; Huang et al., 2014; Lu et al., 2015b), the CCA objective (1) can not be written as an unconstrained sum or expectation of losses incurred at each training sample (in fact all training samples are coupled together through the constraints). As a result, stochastic gradient descent, which requires unbiased gradient estimates computed from small minibatches, is not directly applicable here. Fortunately, Ma et al. (2015); Wang et al.", "startOffset": 156, "endOffset": 576}, {"referenceID": 31, "context": "Notice, however, in contrast to the classification or regression objectives that are more commonly used with random Fourier features (Rahimi & Recht, 2009; Huang et al., 2014; Lu et al., 2015b), the CCA objective (1) can not be written as an unconstrained sum or expectation of losses incurred at each training sample (in fact all training samples are coupled together through the constraints). As a result, stochastic gradient descent, which requires unbiased gradient estimates computed from small minibatches, is not directly applicable here. Fortunately, Ma et al. (2015); Wang et al. (2015c) have developed stochastic optimization algorithms, referred to as AppGrad (Augmented Approximate Gradient) and NOI (Nonlinear Orthogonal Iterations) respectively, for linear CCA and its deep neural network extension deep CCA (Andrew et al.", "startOffset": 156, "endOffset": 597}, {"referenceID": 25, "context": "With this observation, Lu & Foster (2014) solve these least squares problems using randomized PCA (Halko et al., 2011) and a batch gradient algorithm.", "startOffset": 98, "endOffset": 118}, {"referenceID": 48, "context": "Although unbiased gradient estimates of these subproblems do not lead to unbiased gradient estimates of the original CCA objective, local convergence results (that the optimum of CCA is a fixed point of AppGrad, and the AppGrad iterate converges linearly to the optimal solution when started in its neighborhood) have been established for AppGrad (Ma et al., 2015).", "startOffset": 347, "endOffset": 364}, {"referenceID": 64, "context": ", Wang et al., 2015c, Section III. A for more details). With this observation, Lu & Foster (2014) solve these least squares problems using randomized PCA (Halko et al.", "startOffset": 2, "endOffset": 98}, {"referenceID": 25, "context": "With this observation, Lu & Foster (2014) solve these least squares problems using randomized PCA (Halko et al., 2011) and a batch gradient algorithm. Ma et al. (2015); Wang et al.", "startOffset": 99, "endOffset": 168}, {"referenceID": 25, "context": "With this observation, Lu & Foster (2014) solve these least squares problems using randomized PCA (Halko et al., 2011) and a batch gradient algorithm. Ma et al. (2015); Wang et al. (2015c) take a step further and replace the exact solutions to the least squares problems with efficient stochastic gradient descent updates.", "startOffset": 99, "endOffset": 189}, {"referenceID": 25, "context": "With this observation, Lu & Foster (2014) solve these least squares problems using randomized PCA (Halko et al., 2011) and a batch gradient algorithm. Ma et al. (2015); Wang et al. (2015c) take a step further and replace the exact solutions to the least squares problems with efficient stochastic gradient descent updates. Although unbiased gradient estimates of these subproblems do not lead to unbiased gradient estimates of the original CCA objective, local convergence results (that the optimum of CCA is a fixed point of AppGrad, and the AppGrad iterate converges linearly to the optimal solution when started in its neighborhood) have been established for AppGrad (Ma et al., 2015). It has also been observed that the stochastic algorithms converge fast to approximate solutions that are on par with the exact solution or solutions by batch-based optimizers. We give our stochastic optimization algorithm for approximate KCCA, named KNOI (Kernel Nonlinear Orthogonal Iterations), in Algorithm 1. Our algorithm is adapted from the NOI algorithm of Wang et al. (2015c), which allows the use of smaller minibatches (through the time constant \u03c1) than does the AppGrad algorithm of Ma et al.", "startOffset": 99, "endOffset": 1073}, {"referenceID": 25, "context": "With this observation, Lu & Foster (2014) solve these least squares problems using randomized PCA (Halko et al., 2011) and a batch gradient algorithm. Ma et al. (2015); Wang et al. (2015c) take a step further and replace the exact solutions to the least squares problems with efficient stochastic gradient descent updates. Although unbiased gradient estimates of these subproblems do not lead to unbiased gradient estimates of the original CCA objective, local convergence results (that the optimum of CCA is a fixed point of AppGrad, and the AppGrad iterate converges linearly to the optimal solution when started in its neighborhood) have been established for AppGrad (Ma et al., 2015). It has also been observed that the stochastic algorithms converge fast to approximate solutions that are on par with the exact solution or solutions by batch-based optimizers. We give our stochastic optimization algorithm for approximate KCCA, named KNOI (Kernel Nonlinear Orthogonal Iterations), in Algorithm 1. Our algorithm is adapted from the NOI algorithm of Wang et al. (2015c), which allows the use of smaller minibatches (through the time constant \u03c1) than does the AppGrad algorithm of Ma et al. (2015). In each iteration, KNOI adaptively estimates the covariance of the projections of each view (\u2208 R) using a convex combination (with \u03c1 \u2208 [0, 1)) of the previous estimate and the estimate based on the current minibatch,4 uses them to whiten the targets of the cross-view least squares regression problems, derives gradients from these problems,5 and finally updates the projection matrices (U,V) with momentum.", "startOffset": 99, "endOffset": 1200}, {"referenceID": 63, "context": "Empirically, we find that using momentum \u03bc \u2208 [0, 1) helps the algorithm to make rapid progress in the objective with a few passes over the training set, as observed by the deep learning community (Sutskever et al., 2013).", "startOffset": 196, "endOffset": 220}, {"referenceID": 36, "context": "There have been continuous efforts to scale up classical methods such as principal component analysis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015).", "startOffset": 159, "endOffset": 332}, {"referenceID": 3, "context": "There have been continuous efforts to scale up classical methods such as principal component analysis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015).", "startOffset": 159, "endOffset": 332}, {"referenceID": 51, "context": "There have been continuous efforts to scale up classical methods such as principal component analysis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015).", "startOffset": 159, "endOffset": 332}, {"referenceID": 10, "context": "There have been continuous efforts to scale up classical methods such as principal component analysis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015).", "startOffset": 159, "endOffset": 332}, {"referenceID": 73, "context": "There have been continuous efforts to scale up classical methods such as principal component analysis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015).", "startOffset": 159, "endOffset": 332}, {"referenceID": 49, "context": "(2015c) we use here, an intuitively similar approach is proposed in the context of alternating conditional expectation (Makur et al., 2015).", "startOffset": 119, "endOffset": 139}, {"referenceID": 3, "context": "There have been continuous efforts to scale up classical methods such as principal component analysis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015). The CCA objective is more challenging due to the constraints, as also pointed out by Arora et al. (2012). Avron et al.", "startOffset": 223, "endOffset": 439}, {"referenceID": 3, "context": "There have been continuous efforts to scale up classical methods such as principal component analysis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015). The CCA objective is more challenging due to the constraints, as also pointed out by Arora et al. (2012). Avron et al. (2013) propose an algorithm for selecting a subset of training samples that retain the most information for accelerating linear CCA, when there are many more training samples (large N ) than features (small M in our case).", "startOffset": 223, "endOffset": 460}, {"referenceID": 3, "context": "There have been continuous efforts to scale up classical methods such as principal component analysis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015). The CCA objective is more challenging due to the constraints, as also pointed out by Arora et al. (2012). Avron et al. (2013) propose an algorithm for selecting a subset of training samples that retain the most information for accelerating linear CCA, when there are many more training samples (large N ) than features (small M in our case). While this approach effectively reduces the training set size N , it provides no remedy for the large M scenario we face in approximate KCCA. In terms of online/stochastic CCA, Yger et al. (2012) propose an adaptive CCA algorithm with efficient online updates based on matrix manifolds defined by the constraints (and they use a similar form of adaptive estimates for the covariance matrices).", "startOffset": 223, "endOffset": 872}, {"referenceID": 3, "context": "There have been continuous efforts to scale up classical methods such as principal component analysis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015). The CCA objective is more challenging due to the constraints, as also pointed out by Arora et al. (2012). Avron et al. (2013) propose an algorithm for selecting a subset of training samples that retain the most information for accelerating linear CCA, when there are many more training samples (large N ) than features (small M in our case). While this approach effectively reduces the training set size N , it provides no remedy for the large M scenario we face in approximate KCCA. In terms of online/stochastic CCA, Yger et al. (2012) propose an adaptive CCA algorithm with efficient online updates based on matrix manifolds defined by the constraints (and they use a similar form of adaptive estimates for the covariance matrices). However, the goal of their algorithm is anomaly detection for streaming data with a varying distribution, rather than to perform CCA for a given dataset. Regarding the stochastic CCA algorithms of Ma et al. (2015); Wang et al.", "startOffset": 223, "endOffset": 1284}, {"referenceID": 3, "context": "There have been continuous efforts to scale up classical methods such as principal component analysis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015). The CCA objective is more challenging due to the constraints, as also pointed out by Arora et al. (2012). Avron et al. (2013) propose an algorithm for selecting a subset of training samples that retain the most information for accelerating linear CCA, when there are many more training samples (large N ) than features (small M in our case). While this approach effectively reduces the training set size N , it provides no remedy for the large M scenario we face in approximate KCCA. In terms of online/stochastic CCA, Yger et al. (2012) propose an adaptive CCA algorithm with efficient online updates based on matrix manifolds defined by the constraints (and they use a similar form of adaptive estimates for the covariance matrices). However, the goal of their algorithm is anomaly detection for streaming data with a varying distribution, rather than to perform CCA for a given dataset. Regarding the stochastic CCA algorithms of Ma et al. (2015); Wang et al. (2015c) we use here, an intuitively similar approach is proposed in the context of alternating conditional expectation (Makur et al.", "startOffset": 223, "endOffset": 1305}, {"referenceID": 3, "context": "There have been continuous efforts to scale up classical methods such as principal component analysis and partial least squares with stochastic/online updates (Krasulina, 1969; Oja & Karhunen, 1985; Warmuth & Kuzmin, 2008; Arora et al., 2012; 2013; Mitliagkas et al., 2013; Balsubramani et al., 2013; Shamir, 2015; Xie et al., 2015). The CCA objective is more challenging due to the constraints, as also pointed out by Arora et al. (2012). Avron et al. (2013) propose an algorithm for selecting a subset of training samples that retain the most information for accelerating linear CCA, when there are many more training samples (large N ) than features (small M in our case). While this approach effectively reduces the training set size N , it provides no remedy for the large M scenario we face in approximate KCCA. In terms of online/stochastic CCA, Yger et al. (2012) propose an adaptive CCA algorithm with efficient online updates based on matrix manifolds defined by the constraints (and they use a similar form of adaptive estimates for the covariance matrices). However, the goal of their algorithm is anomaly detection for streaming data with a varying distribution, rather than to perform CCA for a given dataset. Regarding the stochastic CCA algorithms of Ma et al. (2015); Wang et al. (2015c) we use here, an intuitively similar approach is proposed in the context of alternating conditional expectation (Makur et al., 2015). Another related approach is that of Xie et al. (2015), who propose the Doubly Stochastic Gradient Descent (DSGD) algorithm for approximate kernel machines (including KCCA) based on random Fourier features.", "startOffset": 223, "endOffset": 1492}, {"referenceID": 42, "context": "In the first set of experiments, we demonstrate the scability and efficiency of KNOI on the MNIST8M dataset (Loosli et al., 2007).", "startOffset": 108, "endOffset": 129}, {"referenceID": 42, "context": "In the first set of experiments, we demonstrate the scability and efficiency of KNOI on the MNIST8M dataset (Loosli et al., 2007). The dataset consists of 8.1 million 28 \u00d7 28 grayscale images of the digits 0-9. We divide each image into the left and right halves and use them as the two views in KCCA, so the input dimensionality is 392 for both views. The dataset is randomly split into training/test sets of size 8M/0.1M. The task is to learn L = 50 dimensional projections using KCCA, and the evaluation criterion is the total canonical correlation achieved on the test set (upperbounded by 50). The same task is used by Xie et al. (2015), although we use a different training/test split.", "startOffset": 109, "endOffset": 642}, {"referenceID": 42, "context": "In the first set of experiments, we demonstrate the scability and efficiency of KNOI on the MNIST8M dataset (Loosli et al., 2007). The dataset consists of 8.1 million 28 \u00d7 28 grayscale images of the digits 0-9. We divide each image into the left and right halves and use them as the two views in KCCA, so the input dimensionality is 392 for both views. The dataset is randomly split into training/test sets of size 8M/0.1M. The task is to learn L = 50 dimensional projections using KCCA, and the evaluation criterion is the total canonical correlation achieved on the test set (upperbounded by 50). The same task is used by Xie et al. (2015), although we use a different training/test split. As in Xie et al. (2015), we fix the kernel widths using the \u201cmedian\u201d trick6 for all algorithms.", "startOffset": 109, "endOffset": 716}, {"referenceID": 42, "context": "In the first set of experiments, we demonstrate the scability and efficiency of KNOI on the MNIST8M dataset (Loosli et al., 2007). The dataset consists of 8.1 million 28 \u00d7 28 grayscale images of the digits 0-9. We divide each image into the left and right halves and use them as the two views in KCCA, so the input dimensionality is 392 for both views. The dataset is randomly split into training/test sets of size 8M/0.1M. The task is to learn L = 50 dimensional projections using KCCA, and the evaluation criterion is the total canonical correlation achieved on the test set (upperbounded by 50). The same task is used by Xie et al. (2015), although we use a different training/test split. As in Xie et al. (2015), we fix the kernel widths using the \u201cmedian\u201d trick6 for all algorithms. We vary the rank M for FKCCA and NKCCA from 256 to 6000. For comparison, we use the same hyperparamters as those of Xie et al. (2015)7: data minibatch size b = 1024, feature minibatch size 2048, total number of random Fourier features M = 20480,8 and a decaying step size schedule.", "startOffset": 109, "endOffset": 922}, {"referenceID": 73, "context": "We use the Wisconsin X-ray microbeam (XRMB) Following Xie et al. (2015), kernel widths are estimated from the median of pairwise distances between 4000 randomly selected training samples.", "startOffset": 54, "endOffset": 72}, {"referenceID": 73, "context": "We use the Wisconsin X-ray microbeam (XRMB) Following Xie et al. (2015), kernel widths are estimated from the median of pairwise distances between 4000 randomly selected training samples. We thank the authors for providing their MATLAB implementation of DSGD. Xie et al. (2015) used a version of random Fourier features with both cos and sin functions, so the number of learnable parameters (in U and V) of DSGD is twice that of KNOI for the same M .", "startOffset": 54, "endOffset": 278}, {"referenceID": 78, "context": "(2015a;b), who used the HTK toolkit (Young et al., 1999), we use the Kaldi speech recognition toolkit (Povey et al.", "startOffset": 36, "endOffset": 56}, {"referenceID": 55, "context": ", 1999), we use the Kaldi speech recognition toolkit (Povey et al., 2011) for feature extraction and recognition with hidden Markov models.", "startOffset": 53, "endOffset": 73}, {"referenceID": 29, "context": "All of the learned feature types are used in a \u201ctandem\u201d speech recognition approach (Hermansky et al., 2000), i.", "startOffset": 84, "endOffset": 108}, {"referenceID": 29, "context": "All of the learned feature types are used in a \u201ctandem\u201d speech recognition approach (Hermansky et al., 2000), i.e., they are appended to the original 39D features and used in a standard hidden Markov model (HMM)-based recognizer with Gaussian mixture observation distributions. For each fold, we select the hyperparameters based on recognition accuracy on the tuning set. For each algorithm, the feature dimensionality L is tuned over {30, 50, 70}, and the kernel widths for each view are tuned by grid search. We initially set M = 5000 for FKCCA/NKCCA, and also test FKCCA at M = 30000 (the largest M at which we could afford to obtain an exact SVD solution on a workstation with 64G main memory) with kernel widths tuned at M = 5000; we could not obtain results for NKCCA with M = 30000 in 48 hours. For KNOI, we set M = 100000 and tune the optimization parameters on a rough grid. The tuned KNOI uses minibatch size b = 2500, time constant \u03c1 = 0, fixed learning rate \u03b7 = 0.01, and momentum \u03bc = 0.995. For this combination of b and M , we are able to run the algorithm on a Tesla K40 GPU (with 12G memory), and each epoch (one pass over the 1.43M training samples) takes only 7.3 minutes. We run KNOI for 5 epochs and use the resulting acoustic view projection for recognition. We have also tried to run KNOI for 10 epochs and the recognition performance does not change, even though the total canonical correlation keeps improving on both training and tuning sets. For comparison, we report the performance of a baseline recognizer that uses only the original MFCC features, and the performance of deep CCA (DCCA) as described in Wang et al. (2015b), which uses 3 hidden layers of 1500ReLU units followed by a linear output layer in the acoustic view, and only a linear output layer in the articulatory view.", "startOffset": 85, "endOffset": 1653}, {"referenceID": 29, "context": "All of the learned feature types are used in a \u201ctandem\u201d speech recognition approach (Hermansky et al., 2000), i.e., they are appended to the original 39D features and used in a standard hidden Markov model (HMM)-based recognizer with Gaussian mixture observation distributions. For each fold, we select the hyperparameters based on recognition accuracy on the tuning set. For each algorithm, the feature dimensionality L is tuned over {30, 50, 70}, and the kernel widths for each view are tuned by grid search. We initially set M = 5000 for FKCCA/NKCCA, and also test FKCCA at M = 30000 (the largest M at which we could afford to obtain an exact SVD solution on a workstation with 64G main memory) with kernel widths tuned at M = 5000; we could not obtain results for NKCCA with M = 30000 in 48 hours. For KNOI, we set M = 100000 and tune the optimization parameters on a rough grid. The tuned KNOI uses minibatch size b = 2500, time constant \u03c1 = 0, fixed learning rate \u03b7 = 0.01, and momentum \u03bc = 0.995. For this combination of b and M , we are able to run the algorithm on a Tesla K40 GPU (with 12G memory), and each epoch (one pass over the 1.43M training samples) takes only 7.3 minutes. We run KNOI for 5 epochs and use the resulting acoustic view projection for recognition. We have also tried to run KNOI for 10 epochs and the recognition performance does not change, even though the total canonical correlation keeps improving on both training and tuning sets. For comparison, we report the performance of a baseline recognizer that uses only the original MFCC features, and the performance of deep CCA (DCCA) as described in Wang et al. (2015b), which uses 3 hidden layers of 1500ReLU units followed by a linear output layer in the acoustic view, and only a linear output layer in the articulatory view. With this architecture, each epoch of DCCA takes about 8 minutes on a Tesla K40 GPU, on par with KNOI. Note that this DCCA architecture was tuned carefully for low PER rather than high canonical correlation. This architecture produces a total correlation of about 25 (out of a maximum of L = 70) on tuning data, while KNOI achieves 46.7. DCCA using deeper nonlinear networks for the second view can achieve even better total canonical correlation, but its PER performance then becomes significantly worse. Phone error rates (PERs) obtained by different algorithms are given in Table 2, where smaller PER indicates better recognition performance. It is clear that all CCA-based features significantly improve over the baseline. Also, a large M is necessary for KCCA to be competitive with deep neural network methods, which is consistent with the findings of Huang et al. (2014); Lu et al.", "startOffset": 85, "endOffset": 2690}, {"referenceID": 29, "context": "All of the learned feature types are used in a \u201ctandem\u201d speech recognition approach (Hermansky et al., 2000), i.e., they are appended to the original 39D features and used in a standard hidden Markov model (HMM)-based recognizer with Gaussian mixture observation distributions. For each fold, we select the hyperparameters based on recognition accuracy on the tuning set. For each algorithm, the feature dimensionality L is tuned over {30, 50, 70}, and the kernel widths for each view are tuned by grid search. We initially set M = 5000 for FKCCA/NKCCA, and also test FKCCA at M = 30000 (the largest M at which we could afford to obtain an exact SVD solution on a workstation with 64G main memory) with kernel widths tuned at M = 5000; we could not obtain results for NKCCA with M = 30000 in 48 hours. For KNOI, we set M = 100000 and tune the optimization parameters on a rough grid. The tuned KNOI uses minibatch size b = 2500, time constant \u03c1 = 0, fixed learning rate \u03b7 = 0.01, and momentum \u03bc = 0.995. For this combination of b and M , we are able to run the algorithm on a Tesla K40 GPU (with 12G memory), and each epoch (one pass over the 1.43M training samples) takes only 7.3 minutes. We run KNOI for 5 epochs and use the resulting acoustic view projection for recognition. We have also tried to run KNOI for 10 epochs and the recognition performance does not change, even though the total canonical correlation keeps improving on both training and tuning sets. For comparison, we report the performance of a baseline recognizer that uses only the original MFCC features, and the performance of deep CCA (DCCA) as described in Wang et al. (2015b), which uses 3 hidden layers of 1500ReLU units followed by a linear output layer in the acoustic view, and only a linear output layer in the articulatory view. With this architecture, each epoch of DCCA takes about 8 minutes on a Tesla K40 GPU, on par with KNOI. Note that this DCCA architecture was tuned carefully for low PER rather than high canonical correlation. This architecture produces a total correlation of about 25 (out of a maximum of L = 70) on tuning data, while KNOI achieves 46.7. DCCA using deeper nonlinear networks for the second view can achieve even better total canonical correlation, but its PER performance then becomes significantly worse. Phone error rates (PERs) obtained by different algorithms are given in Table 2, where smaller PER indicates better recognition performance. It is clear that all CCA-based features significantly improve over the baseline. Also, a large M is necessary for KCCA to be competitive with deep neural network methods, which is consistent with the findings of Huang et al. (2014); Lu et al. (2015b) when using random Fourier features for speech data (where the task is frame classification).", "startOffset": 85, "endOffset": 2709}, {"referenceID": 26, "context": "(2011) which is preferable for sparse inputs, and random features for dot product or polynomial kernels (Kar & Karnick, 2012; Hamid et al., 2014; Pennington et al., 2015), which have proven to be useful for different domains.", "startOffset": 104, "endOffset": 170}, {"referenceID": 54, "context": "(2011) which is preferable for sparse inputs, and random features for dot product or polynomial kernels (Kar & Karnick, 2012; Hamid et al., 2014; Pennington et al., 2015), which have proven to be useful for different domains.", "startOffset": 104, "endOffset": 170}, {"referenceID": 36, "context": "It is straightforward to incorporate in our algorithm the faster random features of Le et al. (2013) which can be generated (for view 1) in time O(NM log dx) instead of O(NMdx), or the Taylor features of Cotter et al.", "startOffset": 84, "endOffset": 101}, {"referenceID": 16, "context": "(2013) which can be generated (for view 1) in time O(NM log dx) instead of O(NMdx), or the Taylor features of Cotter et al. (2011) which is preferable for sparse inputs, and random features for dot product or polynomial kernels (Kar & Karnick, 2012; Hamid et al.", "startOffset": 110, "endOffset": 131}, {"referenceID": 16, "context": "(2013) which can be generated (for view 1) in time O(NM log dx) instead of O(NMdx), or the Taylor features of Cotter et al. (2011) which is preferable for sparse inputs, and random features for dot product or polynomial kernels (Kar & Karnick, 2012; Hamid et al., 2014; Pennington et al., 2015), which have proven to be useful for different domains. It is also worth exploring parallelization and multiple kernel learning strategies of Lu et al. (2015b) with random Fourier features to further bridge the gap between kernel methods and deep neural network methods.", "startOffset": 110, "endOffset": 454}], "year": 2016, "abstractText": "Kernel canonical correlation analysis (KCCA) is a nonlinear multi-view representation learning technique with broad applicability in statistics and machine learning. Although there is a closed-form solution for the KCCA objective, it involves solving an N \u00d7 N eigenvalue system where N is the training set size, making its computational requirements in both memory and time prohibitive for large-scale problems. Various approximation techniques have been developed for KCCA. A commonly used approach is to first transform the original inputs to an M -dimensional random feature space so that inner products in the feature space approximate kernel evaluations, and then apply linear CCA to the transformed inputs. In many applications, however, the dimensionality M of the random feature space may need to be very large in order to obtain a sufficiently good approximation; it then becomes challenging to perform the linear CCA step on the resulting very high-dimensional data matrices. We show how to use a stochastic optimization algorithm, recently proposed for linear CCA and its neuralnetwork extension, to further alleviate the computation requirements of approximate KCCA. This approach allows us to run approximate KCCA on a speech dataset with 1.4 million training samples and a random feature space of dimensionality M = 100000 on a typical workstation.", "creator": "LaTeX with hyperref package"}}}