{"id": "1511.05077", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2015", "title": "Diversity Networks: Neural Network Compression Using Determinantal Point Processes", "abstract": "We introduce Divnet, a flexible technique for learning networks with diverse neurons. Divnet models neuronal diversity by placing a Determinantal Point Process (DPP) over neurons in a given layer. It uses this DPP to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 16 Nov 2015 18:28:10 GMT  (585kb,D)", "http://arxiv.org/abs/1511.05077v1", null], ["v2", "Wed, 18 Nov 2015 02:22:30 GMT  (669kb,D)", "http://arxiv.org/abs/1511.05077v2", null], ["v3", "Wed, 6 Jan 2016 17:55:06 GMT  (716kb,D)", "http://arxiv.org/abs/1511.05077v3", null], ["v4", "Tue, 19 Jan 2016 18:14:16 GMT  (717kb,D)", "http://arxiv.org/abs/1511.05077v4", null], ["v5", "Wed, 3 Feb 2016 16:37:39 GMT  (716kb,D)", "http://arxiv.org/abs/1511.05077v5", null], ["v6", "Tue, 18 Apr 2017 20:33:53 GMT  (716kb,D)", "http://arxiv.org/abs/1511.05077v6", "This paper appeared under the shorter title Diversity Networks at ICLR 2016 (this http URL)"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["zelda mariet", "suvrit sra"], "accepted": false, "id": "1511.05077"}, "pdf": {"name": "1511.05077.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "We introduce Divnet, a flexible technique for learning networks with diverse neurons. Divnet models neuronal diversity by placing a Determinantal Point Process (DPP) over neurons in a given layer. It uses this DPP to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones. Compared with previous approaches, Divnet offers a more principled, flexible technique for capturing neuronal diversity. This enables effective auto-tuning of network architecture and leads to smaller network sizes without hurting performance. Moreover, through its focus on diversity and neuron fusing, Divnet remains compatible with other procedures that seek to reduce memory footprints of networks. We present experimental results to corroborate our claims: for pruning neural networks, Divnet is seen to be notably superior to competing approaches."}, {"heading": "1 Introduction", "text": "Training neural networks requires setting several hyper-parameters to adequate values, for instance number of hidden layers, number of neurons per hidden layer, learning rate, momentum, dropout rate. Although tuning such hyper-parameters via parameter search has been recently investigated by Maclaurin et al. (2015), doing so remains extremely costly, which makes it imperative to develop new more preferable techniques.\nOf the many hyper-parameters, those that determine the network\u2019s architecture are among the hardest to tune, especially because changing them during training is much more difficult than adjusting more dynamic parameters such as learning rate or momentum. Typically, the architecture parameters are set once and for all before training begins. Thus, assigning them correctly is paramount: if the network is too small, it will not learn well; if it is too large, it may take inordinately longer to train while running the risk of overfitting. Networks are therefore usually trained with more parameters than necessary, and pruned once the training is complete. This reduces their memory footprint and deployment size (possibly benefiting generalization too).\nIn light of the above motivation, we introduce Divnet, a flexible technique for cutting down network size. Divnet reduces redundancy (hence size) of a neural network in two steps: first it samples a diverse subset of neurons; second it merges the remaining neurons with those selected. Specifically, Divnet models neuronal diversity in a layer by defining a Determinantal Point Process (DPP) over the neurons; it uses this DPP to reduce redundancy by selecting a small subset of neurons. Subsequently, it \u201cfuses\u201d the information from the dropped neurons into the selected ones. Both steps together help Divnet substantially reduce network size, without requiring any further training and without hurting performance. Divnet runs fast and takes time negligible in comparision to the network\u2019s prior training time. It is agnostic to other network parameters such as activation functions, number of hidden layers, and learning rates.\nWe describe and analyze Divnet in terms of simple feed-forward neural networks, as it operates over a layer that is fully connected to the following one in the network\u2019s hierarchy. However, as such layers are present in many different types of neural networks, Divnet can be applied without any further modification to Deep Belief Nets and to the fully-connected\nar X\niv :1\n51 1.\n05 07\n7v 1\n[ cs\n.L G\n] 1\n6 N\nov 2\n01 5\nlayers in Convolutional Neural Networks. As these layers are typically responsible for the large majority of the CNNs\u2019 memory footprint (Yang et al., 2014), Divnet is particularly adapted these types of networks."}, {"heading": "1.1 Contributions", "text": "The key contributions of this paper are the following:\n\u2022 We show how to use Determinantal Point Processes (DPPs) as a flexible, powerful tool for modeling layerwise neuronal diversity (Section 2.1). In particular, we present a practical method for creating DPPs over neurons that enables diversity promoting sampling of neurons and leads to smaller network sizes.\n\u2022 We show that the potential negative impacts of pruning on classification error can be minimized by \u201cfusing\u201d redundant neurons. To this end, we introduce a reweighting procedure for a neuron\u2019s connections that transfers the contributions of the pruned neurons to the ones retained (Section 2.2).\nWe name the combination of both these ideas as Divnet, and through several experiments we compare Divnet against of previous approaches to neuron pruning (Section 3.1,3.2)."}, {"heading": "1.2 Related work", "text": "Due to their large number of parameters, deep neural networks typically have a heavy memory footprint. Moreover, in many deep neural network models, these parameters show a significant amount of redundancy (Denil et al., 2013), encouraging different approaches to reducing their size without damaging their performance.\nA common approach to reducing the number of parameters in a network is to remove connections between layers. In (Cun et al., 1990) and (Hassibi et al., 1993), connections are deleted using information drawn from the Hessian of the network\u2019s error function. Sainath et al. (2013) reduce the number of parameters in the network by analyzing the weight matrices, this time by applying low-rank matrix factorization to the final weight layer. Han et al. (2015) remove connections with weights smaller than a given threshold before retraining. Whereas these methods focus on deleting parameters whose removal will influence the network the least, Divnet enforces diversity by merging similar parameters; these methods can thus be used in conjunction with ours.\nConvolutional Neural Networks (Lecun et al., 1998) replace fully-connected layers with convolutional and subsampling layers, significantly decreasing the number of parameters in the model. However, CNNs still maintain fully-connected layers, and can therefore also benefit from Divnet.\nCloser to our approach of shrinking the network by reducing the number of hidden neurons1, He et al. (2014) evaluate each hidden neuron\u2019s importance and delete neurons with the smaller importance measures. This technique also can easily be combined with Divnet.\nRecent approaches have investigated network compression without pruning: in (Hinton et al., 2015), a new, smaller network is trained on the outputs of the large network; Chen et al. (2015) use hashing to reduce the size of the weight matrices\u2019 representations by forcing all connections within a same hash bucket to have the same weight. Courbariaux et al. (2014) and Gupta et al. (2015) show that networks can be trained and run using limited precision to store the network\u2019s parameters, thus reducing the network\u2019s overall memory footprint.\nDivnet\u2019s focus on diversity selection among neurons is thus orthogonal to prior network compression techniques. As a consequence, Divnet can be combined, in most cases trivially, with previous approaches to memory footprint reduction.\n1The previous methods can in effect also prune a neuron by removing all of its outgoing connections."}, {"heading": "2 Eliminating redundancy in networks", "text": "As shown in (Denil et al., 2013), many deep neural network models show significant redundancy in their parameters. By detecting and removing redundant elements, we can thus decrease the size of the network without impacting its performance.\nHenceforth we denote by T the training set, ` a given layer in the network, aij the activation of a neuron ni on input tj , and vi = (ai1, . . . , ai|`|)\n> neuron ni\u2019s activation vector, obtained by feeding the training data through the network."}, {"heading": "2.1 Pruning", "text": "In order to enforce diversity in a layer `, we must determine which neurons are computing redundant information and remove them. This can theoretically be done by finding a largest subset of linearly independent activation vectors in a layer and keeping only the corresponding neurons. In practice, however, the number of items in the training set (or the number of batches) may be significantly larger than the numbers of neurons in a layer: the activation vectors v1, . . . , v|`| will likely be linearly independent."}, {"heading": "2.1.1 Determinantal Point Processes", "text": "DPPs model probability distributions over subsets of items so as to assign a higher probability to diverse subsets of items are of good quality. First introduced to model fermions (Macchi, 1975), they have since then been used in many different machine-learning applications (Kulesza and Taskar, 2012).\nMore recently, they have also been applied to modeling inter-neuron inhibitions in neural spiking behavior in the rat hippocampus (Snoek et al., 2013).\nFormally, over a ground set of N items Y = {1, . . . , N}, the probability of a subset Y \u2286 Y is given by\nP(Y ) = det(LY ) det(L+ I)\n(1)\nwhere L is a N -by-N positive definite matrix called the DPP kernel. LY indicates the submatrix of L indexed by the elements of Y .\nWhen sampling from a DPP, the size of the subset can either be set to a specific integer k (in which case the sampling is said to be done via a k-DPP) or left as a free parameter. DPP sampling can therefore be applied to sampling a subset of neurons with a corresponding diverse set of activation vectors."}, {"heading": "2.1.2 Setting the DPP kernel", "text": "Setting the kernel of a DPP can be done using several different approaches. Experimentally, we found that tuning a Gaussian kernel provided the best results, compared both to simpler approaches such as the outer product of the activation vectors and to more complex Gaussian kernels with additional parameters.\nGiven the |`| activations vectors v1, . . . , v|`|, we first create a |`|\u00d7|`| Gaussian kernel L\u2032 with bandwidth \u03b2 by setting\nL\u2032ij = exp(\u2212\u03b2\u2016vi \u2212 vj\u20162). (2)\nIn order to guarantee the positive definiteness of the kernel matrix L\u2032, we add \u03b5I to L\u2032 (\u03b5 = 0.01). Experimentally, we chose \u03b2 = 10/|T |. In the context of setting the size of a neural network, we typically wish to control the size k of the sampled subset. Thus, L\u2032+\u03b5I is then multiplied by an additional parameter \u03b3 in order to adjust the expected sample size to the desired value k. \u03b3 was computed approximately by setting\n\u03b3 = k |`| \u2212 k \u00b7 |`| \u2212 k\n\u2032\nk\u2032 ,\nwhere k\u2032 is the expected sample size for the kernel L\u2032 + \u03b5I, given by the formula\nE[|Y |] = Tr(L(I + L)\u22121).\nWe found experimentally that if this step was omitted, the sample quality was much poorer when using a kernel with E[|Y |] significantly smaller than the desired sample size k. Finally, generating and then sampling from L = \u03b3(L\u2032 + \u03b5I) runs in O(|`|3 + |`|2|T |)."}, {"heading": "2.2 Merging redundant neurons", "text": "Simply deleting neurons that were not sampled by the DPP will drastically alter the input values of the neurons on the next layer. We present here a reweighting procedure that rebalances the network after removing the redundant neurons by fusing them into those that remain.\nLet without loss of generality n1, . . . , nk be the neurons sampled by the DPP, and let v1, . . . , vk be the corresponding activation vectors. We note wil the weight connecting the i-th neuron in the current layer to the l-th neuron in the following layer, and w\u0303ij = \u03b4ij +wij the weights connecting neurons after removing the redundant neurons.\nIn order to minimize the impact of removing |`|\u2212k neurons in layer ` for a neuron nl in the following layer, we wish to solve\nmin w\u0303ij\u2208R \u2225\u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 w\u0303ilvi \u2212 |`|\u2211 i=1 wilvi \u2225\u2225\u2225\u2225\u2225\u2225 2 = min \u03b4ij\u2208R \u2225\u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 \u03b4ilvi \u2212 |`|\u2211 i=k+1 wilvi \u2225\u2225\u2225\u2225\u2225\u2225 2\n(3)\nin order to minimize the difference in inputs to the l-th neuron in the next layer over all training data. Eq. 3 is minimized by projecting \u2211 i>k wilvi on to the linear space generated by {v1, . . . , vk}. To do so, we obtain the coefficients \u03b1ij that for j > k minimize\u2225\u2225\u2225\u2225\u2225vj \u2212 k\u2211 i=1 \u03b1ijvi \u2225\u2225\u2225\u2225\u2225 2\nand then update the weights by setting\n\u2200i, 1 \u2264 i \u2264 k, w\u0303il = wil + |`|\u2211\nj=k+1\n\u03b1ijwjl (4)\nUsing ordinary least squares to obtain the values for \u03b1, the reweighting procedure runs in O(|T ||`|2 + |`|3)."}, {"heading": "3 Experimental results", "text": "In order to quantify the performance of our algorithm, we present below the results of experiments2 run on common datasets for neural network evaluation: the MNIST dataset (LeCun and Cortes, 2010), the MNIST ROT dataset (Larochelle et al., 2007) and the CIFAR-10 dataset (Krizhevsky, 2009).\nAll networks were trained up until a certain training error threshold, using the softmax activation function on the output layer and sigmoid on other layers; see Table 1 for more details. In all following plots, error bars represent standard deviations\n2Experiments were run in MATLAB, based on the code from DeepLearnToolBox (https:// github.com/rasmusbergpalm/DeepLearnToolbox) and Alex Kulesza\u2019s code for DPPs (http://web. eecs.umich.edu/~kulesza/), on a Linux Mint system with 16GB of RAM and an i7-4710HQ CPU @ 2.50GHz."}, {"heading": "3.1 Pruning and reweighting analysis", "text": "Figure 1 shows the activation of neurons in the first hidden layer of a network trained on the MNIST dataset. Each column in the heat maps represents the activation of a neuron on instances of digits 0 through 9. Figure 1a shows the activations of the 50 neurons sampled using a k-DPP (k = 50) placed over the first hidden layer, whereas Figure 1b simply shows the activations of the first 50 neurons of the same layer. Figure 1b contains multiple similar columns: for example, there are 3 entirely green columns, corresponding to three neurons who saturate to 1 on each of the 10 instances. However, as the DPP samples neurons with diverse activations, Figure 1a shows no similar redundancy.\nFigure 2 shows the impact of pruning on the networks\u2019 error, using both DPP pruning and random pruning (where a fixed number of neurons are selected uniformly at random and removed from the network). DPP-pruned networks have consistently better training and test errors than networks pruned at random for the same final size. As expected, the more neurons are maintained the less the error suffers from the pruning procedure; however, the pruning is in both cases destructive, significantly increasing the error rate.\nThis phenomenon can be mitigated by the reweighting procedure, shown in Figure 3. By reweighting the network after pruning, the training and test errors are considerably reduced, even when all but 10% of the layer\u2019s computational units were removed. The pruning also reduces variability of the results: the standard deviation for the results of the reweighted networks is significantly smaller than for the non-reweighted networks.\nFinally, Figure 4 illustrates the performance of Divnet (DPP pruning and reweighting) compared to random pruning with reweighting. Although the reweighting procedure im-\nproves the performance of both pruning methods, the performance of the DPP pruning and reweighting is ultimately better.\ntrained nets\nThese experiments were also run on the networks when shrinking the second layer while maintaining the first layer intact and led to similar results; they are presented in Appendix A."}, {"heading": "3.2 Performance analysis", "text": "Much attention has been given to reducing the size of neural networks in order to reduce memory consumption. When using neural nets locally on devices with limited memory, it is crucial that their memory footprint be as small as possible.\nNode importance-based pruning (henceforth \u201cimportance pruning\u201d) is one of the most intuitive ways to cut down on network size. Introduced to deep networks by He et al. (2014), this method removes the neurons whose calculations impact least the network. Among the three solutions to estimating a neuron\u2019s importance discussed in He et al. (2014), the sum the output weights of each neuron (the onorm function) provided the best results:\nonorm(ni) = 1\n|`+ 1| |`|\u2211 j=1 |w`+1ij |\nFigure 5 shows the relative error of networks after being pruned using importance pruning with the onorm function as a measure of relevance versus the DPP and reweighting procedure.\nSince importance pruning deletes neurons that contribute the least to the next layer\u2019s computations, it performs well up to a certain point; however, when pruning a significant amount of neurons, this pruning procedure removes neurons performing essential calculations, thus hurting the network\u2019s performance significantly. However, since Divnet is fuses redundant neurons instead of merely deleting them, the resulting network maintains a much better performance even with large amounts of pruning.\nTables 2 and 3 show the network training and test errors under various compression rates, without additional retraining."}, {"heading": "4 Discussion", "text": ""}, {"heading": "5 Future work and conclusion", "text": "Divnet leverages the similarities between the behaviors of neurons in a layer to detect redundant parameters and merge them, thus enforcing neuronal diversity within each hidden layer. Using Divnet, large, redundant networks can be shrunk to much smaller structures without impacting their performance nor requiring further training.\nMany parameters in this procedure can be tuned to suit the needs of the user: the number of remaining neurons per layer can be fixed manually; the precision of the reweighting and the sampling procedure can be tuned by choosing how many training instances are used to generate the DPP kernel and reweighting coefficients, creating a trade-off between accuracy, memory management and computational time.\nMoreover, Divnet is agnostic to most parameters of the network, as they only require knowledge of the activation vectors; as a consequence, Divnet can be easily used simultaneously\nwith other pruning/memory management methods to reduce network size. Additionally, the reweighting procedure is also agnostic to how the pruning is done \u2013 results show that it performs surprisingly well even with random pruning.\nAlthough Divnet requires the input of the user to tune the size of the final network, we believe that a method where no parameter should explicitly need to be tuned worth investigating. The fact that DPPs can be augmented to also reflect different distributions over the sampled set sizes (Kulesza and Taskar, 2012, \u00a75.1.1) might be leveraged to remove the burden of choosing the layer\u2019s size from the user.\nMoreover, we believe that investigating the behavior of DPP pruning with different kernels may provide some noteworthy insight into which interactions between neurons of a layer contain the information necessary for a good representation and classification of the data. For example, using a kernel that would consider that two activation vectors are identical \u2013 or at least very similar \u2013 might lead to better sampling and, as a consequence, better performance with and without reweighting."}, {"heading": "A Pruning the second layer", "text": "trained nets"}], "references": [{"title": "Compressing neural networks with the hashing", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "trick. CoRR,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Low precision arithmetic for deep learning", "author": ["M. Courbariaux", "Y. Bengio", "J. David"], "venue": "CoRR, abs/1412.7024,", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "Optimal brain damage", "author": ["Y.L. Cun", "J.S. Denker", "S.A. Solla"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Cun et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Cun et al\\.", "year": 1990}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "M. Ranzato", "N. de Freitas"], "venue": "CoRR, abs/1306.0543,", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Deep learning with limited numerical precision", "author": ["S. Gupta", "A. Agrawal", "K. Gopalakrishnan", "P. Narayanan"], "venue": "CoRR, abs/1502.02551,", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural networks", "author": ["S. Han", "J. Pool", "J. Tran", "W.J. Dally"], "venue": "CoRR, abs/1506.02626,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Second order derivatives for network pruning: Optimal brain surgeon", "author": ["B. Hassibi", "D.G. Stork", "S.C.R. Com"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hassibi et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hassibi et al\\.", "year": 1993}, {"title": "Reshaping deep neural network for fast decoding by node-pruning", "author": ["T. He", "Y. Fan", "Y. Qian", "T. Tan", "K. Yu"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "He et al\\.,? \\Q2014\\E", "shortCiteRegEx": "He et al\\.", "year": 2014}, {"title": "Distilling the knowledge in a neural network", "author": ["G.E. Hinton", "O. Vinyals", "J. Dean"], "venue": "CoRR, abs/1503.02531,", "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Technical report,", "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "Determinantal point processes for machine learning", "author": ["A. Kulesza", "B. Taskar"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Kulesza and Taskar.,? \\Q2012\\E", "shortCiteRegEx": "Kulesza and Taskar.", "year": 2012}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "In Proceedings of the 24th International Conference on Machine Learning,", "citeRegEx": "Larochelle et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2007}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "Lecun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lecun et al\\.", "year": 1998}, {"title": "The coincidence approach to stochastic point processes", "author": ["O. Macchi"], "venue": "Advances in Applied Probability,", "citeRegEx": "Macchi.,? \\Q1975\\E", "shortCiteRegEx": "Macchi.", "year": 1975}, {"title": "Gradient-based hyperparameter optimization through reversible learning", "author": ["D. Maclaurin", "D. Duvenaud", "R.P. Adams"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Maclaurin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Maclaurin et al\\.", "year": 2015}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["T.N. Sainath", "B. Kingsbury", "V. Sindhwani", "E. Arisoy", "B. Ramabhadran"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Sainath et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2013}, {"title": "A determinantal point process latent variable model for inhibition in neural spiking data", "author": ["J. Snoek", "R. Zemel", "R.P. Adams"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Snoek et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 14, "context": "Although tuning such hyper-parameters via parameter search has been recently investigated by Maclaurin et al. (2015), doing so remains extremely costly, which makes it imperative to develop new more preferable techniques.", "startOffset": 93, "endOffset": 117}, {"referenceID": 3, "context": "Moreover, in many deep neural network models, these parameters show a significant amount of redundancy (Denil et al., 2013), encouraging different approaches to reducing their size without damaging their performance.", "startOffset": 103, "endOffset": 123}, {"referenceID": 2, "context": "In (Cun et al., 1990) and (Hassibi et al.", "startOffset": 3, "endOffset": 21}, {"referenceID": 6, "context": ", 1990) and (Hassibi et al., 1993), connections are deleted using information drawn from the Hessian of the network\u2019s error function.", "startOffset": 12, "endOffset": 34}, {"referenceID": 12, "context": "Convolutional Neural Networks (Lecun et al., 1998) replace fully-connected layers with convolutional and subsampling layers, significantly decreasing the number of parameters in the model.", "startOffset": 30, "endOffset": 50}, {"referenceID": 8, "context": "Recent approaches have investigated network compression without pruning: in (Hinton et al., 2015), a new, smaller network is trained on the outputs of the large network; Chen et al.", "startOffset": 76, "endOffset": 97}, {"referenceID": 0, "context": "In (Cun et al., 1990) and (Hassibi et al., 1993), connections are deleted using information drawn from the Hessian of the network\u2019s error function. Sainath et al. (2013) reduce the number of parameters in the network by analyzing the weight matrices, this time by applying low-rank matrix factorization to the final weight layer.", "startOffset": 4, "endOffset": 170}, {"referenceID": 0, "context": "In (Cun et al., 1990) and (Hassibi et al., 1993), connections are deleted using information drawn from the Hessian of the network\u2019s error function. Sainath et al. (2013) reduce the number of parameters in the network by analyzing the weight matrices, this time by applying low-rank matrix factorization to the final weight layer. Han et al. (2015) remove connections with weights smaller than a given threshold before retraining.", "startOffset": 4, "endOffset": 348}, {"referenceID": 0, "context": "In (Cun et al., 1990) and (Hassibi et al., 1993), connections are deleted using information drawn from the Hessian of the network\u2019s error function. Sainath et al. (2013) reduce the number of parameters in the network by analyzing the weight matrices, this time by applying low-rank matrix factorization to the final weight layer. Han et al. (2015) remove connections with weights smaller than a given threshold before retraining. Whereas these methods focus on deleting parameters whose removal will influence the network the least, Divnet enforces diversity by merging similar parameters; these methods can thus be used in conjunction with ours. Convolutional Neural Networks (Lecun et al., 1998) replace fully-connected layers with convolutional and subsampling layers, significantly decreasing the number of parameters in the model. However, CNNs still maintain fully-connected layers, and can therefore also benefit from Divnet. Closer to our approach of shrinking the network by reducing the number of hidden neurons, He et al. (2014) evaluate each hidden neuron\u2019s importance and delete neurons with the smaller importance measures.", "startOffset": 4, "endOffset": 1040}, {"referenceID": 0, "context": ", 2015), a new, smaller network is trained on the outputs of the large network; Chen et al. (2015) use hashing to reduce the size of the weight matrices\u2019 representations by forcing all connections within a same hash bucket to have the same weight.", "startOffset": 80, "endOffset": 99}, {"referenceID": 0, "context": ", 2015), a new, smaller network is trained on the outputs of the large network; Chen et al. (2015) use hashing to reduce the size of the weight matrices\u2019 representations by forcing all connections within a same hash bucket to have the same weight. Courbariaux et al. (2014) and Gupta et al.", "startOffset": 80, "endOffset": 274}, {"referenceID": 0, "context": ", 2015), a new, smaller network is trained on the outputs of the large network; Chen et al. (2015) use hashing to reduce the size of the weight matrices\u2019 representations by forcing all connections within a same hash bucket to have the same weight. Courbariaux et al. (2014) and Gupta et al. (2015) show that networks can be trained and run using limited precision to store the network\u2019s parameters, thus reducing the network\u2019s overall memory footprint.", "startOffset": 80, "endOffset": 298}, {"referenceID": 3, "context": "2 Eliminating redundancy in networks As shown in (Denil et al., 2013), many deep neural network models show significant redundancy in their parameters.", "startOffset": 49, "endOffset": 69}, {"referenceID": 13, "context": "First introduced to model fermions (Macchi, 1975), they have since then been used in many different machine-learning applications (Kulesza and Taskar, 2012).", "startOffset": 35, "endOffset": 49}, {"referenceID": 10, "context": "First introduced to model fermions (Macchi, 1975), they have since then been used in many different machine-learning applications (Kulesza and Taskar, 2012).", "startOffset": 130, "endOffset": 156}, {"referenceID": 16, "context": "More recently, they have also been applied to modeling inter-neuron inhibitions in neural spiking behavior in the rat hippocampus (Snoek et al., 2013).", "startOffset": 130, "endOffset": 150}, {"referenceID": 11, "context": "In order to quantify the performance of our algorithm, we present below the results of experiments run on common datasets for neural network evaluation: the MNIST dataset (LeCun and Cortes, 2010), the MNIST ROT dataset (Larochelle et al., 2007) and the CIFAR-10 dataset (Krizhevsky, 2009).", "startOffset": 219, "endOffset": 244}, {"referenceID": 9, "context": ", 2007) and the CIFAR-10 dataset (Krizhevsky, 2009).", "startOffset": 33, "endOffset": 51}, {"referenceID": 7, "context": "Introduced to deep networks by He et al. (2014), this method removes the neurons whose calculations impact least the network.", "startOffset": 31, "endOffset": 48}, {"referenceID": 7, "context": "Introduced to deep networks by He et al. (2014), this method removes the neurons whose calculations impact least the network. Among the three solutions to estimating a neuron\u2019s importance discussed in He et al. (2014), the sum the output weights of each neuron (the onorm function) provided the best results:", "startOffset": 31, "endOffset": 218}], "year": 2017, "abstractText": "We introduce Divnet, a flexible technique for learning networks with diverse neurons. Divnet models neuronal diversity by placing a Determinantal Point Process (DPP) over neurons in a given layer. It uses this DPP to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones. Compared with previous approaches, Divnet offers a more principled, flexible technique for capturing neuronal diversity. This enables effective auto-tuning of network architecture and leads to smaller network sizes without hurting performance. Moreover, through its focus on diversity and neuron fusing, Divnet remains compatible with other procedures that seek to reduce memory footprints of networks. We present experimental results to corroborate our claims: for pruning neural networks, Divnet is seen to be notably superior to competing approaches.", "creator": "LaTeX with hyperref package"}}}