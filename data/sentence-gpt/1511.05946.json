{"id": "1511.05946", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "ACDC: A Structured Efficient Linear Layer", "abstract": "The linear layer is one of the most pervasive modules in deep learning representations. However, it requires $O(N^2)$ parameters and $O(N^2)$ operations. These costs can be prohibitive in mobile applications or prevent scaling in many domains. Here, we introduce a deep, differentiable, fully-connected neural network module composed of diagonal matrices of parameters, $\\mathbf{A}$ and $\\mathbf{D}$, and the discrete cosine transform $\\mathbf{C}$. The core module, structured as $\\mathbf{ACDC^{-1}}$, has $O(N)$ parameters and incurs $O(N log N )$ operations. We present theoretical results showing how deep cascades of ACDC layers approximate linear layers. ACDC is, however, a stand-alone module and can be used in combination with any other types of module. In our experiments, we show that it can indeed be successfully interleaved with ReLU modules in convolutional neural networks for image recognition. Our experiments also study critical factors in the training of these structured modules, including initialization and depth. Finally, this paper also provides a connection between structured linear transforms used in deep learning and the field of Fourier optics, illustrating how ACDC could in principle be implemented with lenses and diffractive elements. This article discusses the limitations of a deep neural network that is not yet fully-connected. In our experiments, we use layers of $\\mathbf{ACDC^{-1}}$ which can be re-connected with ReLU modules in convolutional neural networks, such as our Likuli-Corsic (3D) network.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 18 Nov 2015 20:52:17 GMT  (62kb)", "http://arxiv.org/abs/1511.05946v1", null], ["v2", "Fri, 8 Jan 2016 01:37:57 GMT  (371kb,D)", "http://arxiv.org/abs/1511.05946v2", null], ["v3", "Mon, 8 Feb 2016 02:27:52 GMT  (392kb,D)", "http://arxiv.org/abs/1511.05946v3", null], ["v4", "Tue, 1 Mar 2016 03:37:23 GMT  (394kb,D)", "http://arxiv.org/abs/1511.05946v4", null], ["v5", "Sat, 19 Mar 2016 23:31:15 GMT  (398kb,D)", "http://arxiv.org/abs/1511.05946v5", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["marcin moczulski", "misha denil", "jeremy appleyard", "nando de freitas"], "accepted": true, "id": "1511.05946"}, "pdf": {"name": "1511.05946.pdf", "metadata": {"source": "CRF", "title": "ACDC: A STRUCTURED EFFICIENT LINEAR LAYER", "authors": ["Marcin Moczulski", "Misha Denil", "Jeremy Appleyard", "Nando de Freitas"], "emails": ["marcin.moczulski@stcatz.ox.ac.uk", "misha.denil@gmail.com", "jappleyard@nvidia.com", "nando.de.freitas@cs.ox.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n05 94\n6v 1\n[ cs\n.L G\n] 1\n8 N\nov 2"}, {"heading": "1 INTRODUCTION", "text": "The linear layer is the central building block of nearly all modern neural network models. A notable exception to this is the convolutional layer, which has been extremely successful in computer vision; however, even convolutional networks typically feed into one or more linear layers after processing by convolutions. Other specialized network modules including LSTMs; Gated Recurrent Units; the attentional mechanisms used for image captioning (Xu et al., 2015), and also for reading and writing in the Neural Turing Machine (Graves et al., 2015) and Memory Networks (Sukhbaatar et al., 2015) are all built from compositions of linear layers and nonlinear modules, such as sigmoid, softmax and ReLU layers.\nThe linear layer is essentially a matrix-vector operation, where the input x is scaled with a matrix of parameters W as follows:\ny = xW (1)\nWhen the number of inputs and outputs is N , the number of parameters stored in W is O(N2). It also takes O(N2) operations to compute the output y.\nIn spite of the ubiquity and convenience of linear layers, their O(N2) size is extremely wasteful. Indeed, several studies focusing on feedforward perceptrons and convolutional networks have shown that the parametrisation of linear layers is extremely wasteful, with up to 95% of the parameters being redundant (Denil et al., 2013; Gong et al., 2014; Sainath et al., 2013).\nGiven the importance of this research topic, we have witnessed a recent explosion of works introducing structured efficient linear layers (SELLs). We adopt the following notation to describe SELLs within a common framework:\ny = x\u03a6 = x\u03a6(D,P,S,B) (2)\nWe reserve the capital bold symbol D for diagonal matrices, P for permutations, S for sparse matrices, and B \u2208 {F,H,C} for bases such as Fourier, Hadamard and Cosine transforms respectively. In this setup, the parameters are typically in the diagonal or sparse entries of the matrices D and S. Sparse matrices aside, the computational cost of most SELLs is O(N logN), while the number of parameters is reduced from O(N2) to a mere O(N). These costs are a consequence of the facts that we only need to store the diagonal matrices, and that the Fourier, Hadamard or Discrete Cosine transforms can be efficiently computed in O(N logN) steps.\nOften the diagonal and sparse matrices have fixed random entries. When this is the case, we will use tildes to indicate this fact (e.g., D\u0303).\nOur first SELL example is the Fast Random Projections method of Ailon & Chazelle (2009):\n\u03a6 = D\u0303HS\u0303 (3)\nHere, the sparse matrix S\u0303 has Gaussian entries, the diagonal D\u0303 has {+1,\u22121} entries drawn independently with probability 1/2, and H is the Hadamard matrix. The embeddings generated by this SELL preserve metric information with high probability, as formalized by the theory of random projections.\nFastfood (Le et al., 2013), our second SELL example, extends fast random projections as follows:\n\u03a6 = D\u03031HPD\u03032HD\u03033. (4)\nIn (Yang et al., 2015), the authors introduce an adaptive variant of Fastfood, with the random diagonal matrices replaced by diagonal matrices of parameters, and show that it outperforms the random counterpart when applied to the problem of replacing one of the fully connected layers of a convolutional neural network for ImageNet (Jia et al., 2014). Interestingly, while the random variant is competitive in simple applications (MNIST), the adaptive variant has a considerable advantage in more demanding applications (ImageNet).\nThe adaptive SELLs, including Adaptive Fastfood and the alternatives discussed subsequently, are end to end differentiable. They require only O(N) parameters and O(N logN) operations in both the forward and backward passes of backpropagation. These benefits can be achieved both at train and test time.\nCheng et al. (2015) introduced a SELL consisting of the product of a circulant matrix (R) and a random diagonal matrix (D\u03031). Since circulant matrices can be diagonalized with the discrete Fourier transform (Golub & Van Loan, 1996), this SELL falls within our general notation:\n\u03a6 = D\u03031R = D\u03031FD2F \u22121. (5)\nSindhwani et al. (2015) introduced a Toeplitz-like structured transform, within the framework of displacement operators. Since Toeplitz matrices can be \u201cembedded\u201d in circulant matrices, they can also be diagonalized with the discrete Fourier transform (Golub & Van Loan, 1996).\nIn this work, we introduce a SELL that could be thought of as an adaptive variant of the method of Cheng et al. (2015). In addition, instead of using a (single) shallow SELL as in previous works (Yang et al., 2015; Cheng et al., 2015; Sindhwani et al., 2015), we consider deep SELLs:\n\u03a6 = K \u220f\nk=1\nAkFDkF \u22121. (6)\nHere, A is also a diagonal matrix of parameters, but we use a different symbol to emphasize that A scales the signal in the original domain while D scales it in the Fourier domain.\nWhile adaptive SELLs perform better than their random counterparts in practice, there is a lack of theory for adaptive SELLs. Moreover, the empirical studies of recent adaptive SELLs have many deficiencies. For instance, it is often not clear how performance varies depending on implementation, and many critical details such as initialization and the treatment of biases are typically obviated. In addition, the gains are often demonstrated in models of different size, making objective comparison very difficult.\nIn addition to demonstrating good performance replacing the fully connected layers of CaffeNet, we present a theoretical approximation guarantee for our deep SELL in Section 3. We also discuss\nthe crucial issue of implementing deep SELLs efficiently in modern GPU architectures in Section 5 and release this software with this paper. This engineering contribution is important as many of the recently proposed methods for accelerating linear layers often fail to take into account the attributes and limitations of GPUs, and hence fail to be adopted."}, {"heading": "1.1 LIGHTNING FAST DEEP SELL", "text": "Our deep SELL, equation (6), has very interesting connections with optical information processing. The two-dimensional Fourier transform can be carried out with one lens in free space, while the operation of a diffractive element can be described by a diagonal matrix (Reif & Tyagi, 1997; Mu\u0308ller-Quade et al., 1998; Huhtanen, 2008; Schmid et al., 2000). So, in principle, it is possible to envision an optical implementation of our deep SELL.\nVery recently, Hermans & Vaerenbergh (2015) considered using waves for neural network learning. Their cascaded system consisting of products of diagonal matrices of phase shifts and unitary matrices can be thought of as another futuristic avenue for implementing deep SELL.\nEven more recently, Saade et al. (2015) disclosed an invention that peforms optical analog random projections."}, {"heading": "2 FURTHER RELATED WORKS", "text": "The literature on this topic is vast, and consequently this section only aims to capture some of the significant trends. We refer readers to the related work sections of the papers cited in the previous and present section for further details.\nAs mentioned earlier, many studies have shown that the parametrisation of linear layers is extremely wasteful (Denil et al., 2013; Gong et al., 2014; Sainath et al., 2013). In spite of this redundancy, there has been little success in improving the linear layer, since natural extensions, such as low rank factorizations, lead to poor performance when trained end to end. For instance, Sainath et al. (2013) demonstrate significant improvements in reducing the number of parameters of the output softmax layers, but only modest improvements for the hidden linear layers.\nSeveral methods based on low-rank decomposition and sparseness have been proposed to eliminate parameter redundancy at test time, but they provide only a partial solution as the full network must be instantiated during training (Collins & Kohli, 2014; Xue et al., 2013; Blundell et al., 2015; Liu et al., 2015; Han et al., 2015b). That is, these approaches require training the original full model. Hashing techniques have been proposed to reduce the number of parameters (Chen et al., 2015; Bakhtiary et al., 2015). Hashes have irregular memory access patterns and, consequently, good performance on large GPU-based platforms is an open problem. Distillation (Hinton et al., 2015; Romero et al., 2015) also offers a way of compressing neural networks, as a post-processing step.\nNovikov et al. (2015) use a multi-linear transform (Tensor-Train decomposition) to attain significant reductions in the number of parameters in some of the linear layers of convolutional networks."}, {"heading": "3 DEEP SELL", "text": "We define a single component of deep SELL as AFDF(x) = xAFDF\u22121, where F is the Fourier transform and A,D are complex diagonal matrices. It is straightforward to see that the AFDF transform is not sufficient to express an arbitrary linear operator W \u2208 Cn\u00d7n. An AFDF transform has 2n degrees of freedom, whereas an arbitrary linear operator has n2 degrees of freedom.\nTo this end, we turn our attention to studying compositions of AFDF transforms. By composing AFDF transforms we can boost the number of degrees of freedom, and we might expect that any linear operator could be constructed as a composition of sufficiently many AFDF transforms. In the following we show that this is indeed possible, and that a bounded number of AFDF transforms is sufficient.\nDefinition 1. The order-K AFDF transformation is the composition of K consecutive AFDF operations with (optionally) different A and D matrices. We write an order-K complex AFDF transfor-\nmation as follows\ny = AFDFK(x) = x\n[\nK \u220f\nk=1\nAkFDkF \u22121\n]\n. (7)\nWe also assume, without loss of generality, that A1 = I so that AFDF1(x) = xFD1F\u22121.\nFor the analysis it will be convenient to rewrite the AFDF transformation in a different way, which we refer to as the optical presentation.\nDefinition 2. If y = AFDFK(x) then we define the optical presentation of an order-K AFDF transform as\ny\u0302 = x\u0302\n[\nK\u22121 \u220f\nk=1\nDkRk+1\n]\nDK\nwhere x\u0302 and y\u0302 are the Fourier transforms of x and y, and Rk+1 = F\u22121Ak+1F.\nRemark 3. The matrix R = F\u22121AF is circulant. This follows from the duality between convolution in the spatial domain and pointwise multiplication in the Fourier domain.\nThe optical presentation shows how the spectrum of x is related to the spectrum of y. Importantly, it shows that we can express an order-K AFDF transform as a linear operator in Fourier space that is composed of a product of circulant and diagonal matrices. Transformations of this type are well studied in the Fourier optics literature, as they can be realized with cascades of lenses.\nOf particular relevance to us is the main result of Huhtanen & Permki (2015) which states that almost all (in the Lebesgue sense) matrices M \u2208 CN\u00d7N can be factored as\nM =\n[\nN\u22121 \u220f\ni=1\nD2i\u22121R2i\n]\nD2N\u22121\nwhere D2j\u22121 is diagonal and R2j is circulant. This factorization corresponds exactly to the optical presentation of an order-N AFDF transform, therefore we conclude the following:\nTheorem 4. An order-N AFDF transform is sufficient to approximate any linear operator in CN\u00d7N to arbitrary precision.\nProof. Every AFDF transform has an optical presentation, and by the main result of Huhtanen & Permki (2015) operators of this type are dense in CN\u00d7N ."}, {"heading": "4 ACDC: A PRACTICAL DEEP SELL", "text": "Thus far we have focused on a complex SELL, where theoretical guarantees can be obtained. In practice we find it useful to consider instead a real SELL. The real version of AFDFK , denoted ACDCK has the same form as Equation (7), with complex diagonals replaced with real diagonals, and Fourier transforms replaced with Cosine Transforms.\nThe reasons for considering ACDC over AFDF are purely practical.\n1. Most existing deep learning frameworks support only real numbers, and thus working with real valued transformations simplifies the interface between our SELL and the rest of the network.\n2. Working with complex numbers effectively doubles the memory footprint of of the transform itself, and more importantly, of the activations that interact with it.\nThe importance of the second point should not be underestimated, since the computational complexity of our SELL is quite low, a typical GPU implementation will be bottlenecked by the overhead of moving data through the GPU memory hierarchy. Reducing the amount of data to be moved allows for a significantly faster implementation. Using ACDC retains the favorable asymptotic properties\nof AFDF while allowing more opportunity to reduce constant factors with a careful implementation. We discuss these concerns in more detail in Section 5.\nIn this work, we use the DCT (type II) matrix with entries\ncnk =\n\u221a\n2\nN\n[\n\u01ebk cos\n(\n\u03c0(2n+ 1)k\n2N\n)]\n(8)\nfor n, k = 0, 1, . . . , N , and where \u01ebk = 1/ \u221a 2 for k = 0 or k = N and \u01ebk = 1 otherwise. DCT matrices are real and orthogonal: C\u22121 = CT . Moreover, the DCTs are separable transforms. That is, the DCT of a multi-dimensional signal can be decomposed in terms of successive DCTs of the appropriate one-dimensional components of the signal. The DCT can be computed efficiently using the Fast Fourier Transform (FFT) algorithm (or the specialized fast cosine transform).\nDenoting h1 = xiA, h2 = h1C, h3 = h2D, yi = h3C\u22121, and A = diag(a),D = diag(d) we have the following derivatives in the backward pass:\n\u2202L \u2202d = \u2202yi \u2202d \u2202L \u2202yi = \u2202h2D \u2202d \u2202h3C \u22121 \u2202h3 \u2202L \u2202yi = diag(h2)C \u2202L \u2202yi = h2 \u2299C \u2202L \u2202yi (9)\n\u2202L \u2202D = diag( \u2202L \u2202d ) (10)\n\u2202L \u2202a = \u2202yi \u2202a \u2202L \u2202yi = \u2202xiA \u2202a \u2202h1C \u2202h1 \u2202h2D \u2202h2 \u2202L \u2202h3 = xi \u2299C\u22121d\u2299C \u2202L \u2202yi (11)\n\u2202L \u2202A = diag( \u2202L \u2202a ) (12)\n\u2202L \u2202xi = \u2202yi \u2202xi \u2202L \u2202yi = \u2202xiA \u2202xi \u2202L \u2202h1 = a\u2299C\u22121d\u2299C \u2202L \u2202yi (13)"}, {"heading": "5 EFFICIENT IMPLEMENTATION OF ACDC", "text": "The processor used to benchmark the ACDC layer was an NVIDIA Titan X. The peak floating point throughput of the Titan X is 6605 GFLOPs, and the peak memory bandwidth is 336.5GB/s1. This gives an arithmetic intensity (FLOPs per byte) of approximately 20. In the ideal case, where there is enough parallelism for the GPU to hide all latencies, an algorithm with a higher arithmetic intensity than this would be expected to be floating point throughput bound, while an algorithm with lower arithmetic intensity would be expected to be memory throughput bound.\nThe forward pass of a single example through a size-N ACDC layer when calculated using 32-bit floating point arithmetic requires at least 24N bytes to be moved to and from main memory. Eight bytes per element for each of A and D, four bytes per element for the input, and four bytes per element for the output. It also requires approximately 4N +5N log2(N) floating point operations\n2. When batching, the memory transfers for A and D are expected to be cached as they are reused for each example in the batch, so for the purposes of calculating arithmetic intensity in the batched case it is reasonable to discount them. The arithmetic intensity of a minibatch passing through an ACDC layer is therefore approximately:\nAI = (4 + 5 log2(N))/8\nFor the values of N we are interested in (128 \u2212\u2192 16, 384) this arithmetic intensity varies between 4.9 and 9.3, indicating that the peak performance of a large ACDC layer with a large batch size is expected to be limited by the peak memory throughput of the GPU (336.5GB/s), and that optimization of an ACDC implementation should concentrate on removing any extraneous memory operations.\nTwo versions of ACDC have been implemented. One performs the ACDC in a single call, with the minimum of 8N bytes moved per layer (assuming perfect caching of A and D). The other performs ACDC with multiple calls, with significantly more than 8N bytes moved per layer.\n1 http://www.geforce.co.uk/hardware/desktop-gpus/geforce-gtx-titan-x/specifications 2 http://www.fftw.org/speed/method.html"}, {"heading": "5.1 SINGLE CALL IMPLEMENTATION", "text": "To minimize traffic to and from main memory intermediate loads or stores during the layer must be eliminated. To accomplish this kernel fusion is used to fuse all of the operations of ACDC into a single call, with intermediate values being stored in temporary low-level memory instead of main memory. This presents two challenges to the implementation.\nFirstly, the size of the ACDC layer is limited by the availability of temporary memory on the GPU. This limits the size of the ACDC layer that can be calculated. It also has performance implications: the temporary memory used to store intermediate values in the computation is shared with the registers required for basic calculation, such as loop indices. The more of this space that is used by data, the fewer threads can fit on the GPU at once, limiting parallelism.\nSecondly, the DCT and IDCT layers must be written by hand so that they can be efficiently fused with the linear layers. Implementations of DCT and IDCT are non-trivial, and a generic implementation able to handle any input size would be a large project in itself. For this reason, the implementation is constrained to power-of-two and multiples of large power-of-two layer sizes."}, {"heading": "5.2 MULTIPLE CALL IMPLEMENTATION", "text": "While expected to be less efficient a multiple call implementation is both much simpler programmatically, and much more generically usable. Using the method of ? it is possible to perform size-N DCTs and IDCTs using size-N FFTs. As such, the NVIDIA library cuFFT can be used to greatly simplify the code required, as well as achieve reasonable performance across a wide range of ACDC sizes. The procedure is as follows:\n1. Multiply input by A and set up C1\n2. Perform C1 using a C2C cuFFT call\n3. Finalize C1, multiply by D and setup C2\n4. Perform C2 using a C2C cuFFT call\n5. Finalize C2\nThe total memory moved for this implementation is significantly higher as each call requires a load and a store for each element. The performance trade-off with the single call method is therefore one of parallelism against memory traffic."}, {"heading": "5.3 PERFORMANCE COMPARISON", "text": "Figure 1 compares the speed of the single and multiple call implementations of ACDC against dense matrix-matrix multiplication for a variety of layer sizes.\nIt is clear that in both the forward and backward pass ACDC layers have a significantly lower runtime than fully connected layers using dense matrices. Even if the matrix-matrix operations were running at peak, ACDC still would outperform them by up to 10 times.\nAs expected, the single call version of ACDC outperforms the multiple call version, although for smaller layer sizes the gap is larger. When the layer size increases the multiple call version suffers significantly more from small per-call overheads. Both single and multiple call versions of ACDC perform significantly worse on non power-of-two layer sizes. This is because they rely on FFT operations, which are known to be more efficient when the input sizes are of lengths zn, where z is a small integer3.\nWhile the backward pass of ACDC is expected to take approximately the same time as the forward pass, it takes noticeably longer. To compute the parameter gradients one needs the input into the D operation and the gradient of the output from the A operation. As the aim of the layer is to reduce memory footprint it was decided instead to recompute these during the backward pass, increasing runtime while saving memory."}, {"heading": "6 EXPERIMENTS", "text": ""}, {"heading": "6.1 LINEAR LAYERS", "text": "In this section we show that we are able to approximate linear operators using ACDC as predicted by the theory of Section 3. These experiments serve two purposes\n1. They show that recovery of a dense linear operator by SGD is feasible in practice. The theory of Section 3 guarantees only that it is possible to approximate any operator, but does not provide guidance on how to find this approximation. Additionally, Huhtanen & Permki (2015) suggest that this is a difficult problem.\n2. They validate empirically that our decision to focus on ACDC over the complex AFDF does not introduce obvious difficulties into the approximation. The theory provides guarantees only for the complex case, and the experiments in this section suggest that restricting ourselves to real matrices is not a problem.\nWe investigate using ACDC on a synthetic linear regression problem\nY = XWtrue + \u01eb, (14)\nwhere X of size 10, 000 \u00d7 32 and Wtrue of size 32 \u00d7 32 are both constructed by sampling their entries uniformly at random in the unit interval. Gaussian noise \u01eb \u223c N (0, 10\u22124) is added to the generated targets.\nThe results of approximating the operatorWtrue using ACDCK for different values of K are shown in Figure 2. The theory of Section 3 predicts that, in the complex case, for a 32\u00d732 matrix it should be sufficient to have 32 layers of ACDC to express an arbitrary Wtrue.\nWe found that initialization of the matrices A and D to identity I, with Gaussian noise N (0, 10\u22122) added the diagonals in order to break symmetries, is essential for models having many ACDC layers. (We found the initialization to be robust to the specification of the noise added to the diagonals.)\nThe need for thoughtful initialization is very clear in Figure 2. With the right initialization (leftmost plot), the approximation results of Section 3 are confirmed, with improved accuracy as we increase the number of ACDC layers. However, if we use standard strategies for initializing linear layers (rightmost plot), we observe very poor optimization results as the number of ACDC layers increases.\nThis experiment confirmed that fewer layers suffice to arrive at a reasonable approximation of the original Wtrue than what the theory guarantees. With neural networks in mind this is a very relevant observation. It is well known that the linear layers of neural networks are compressible, indicating that we do not need to express an arbitrary linear operator in order to achieve good performance. Instead, we need only express a sufficiently interesting subset of matrices, and the result with 16 ACDC layers points to this being the case.\n3 http://docs.nvidia.com/cuda/cufft/#accuracy-and-performance\nIn Section 6.2 we show that by interspersing nonlinearities between ACDC layers in a convolutional network it is possible to use dramatically fewer ACDC layers than the theory suggests are needed while still achieving good performance."}, {"heading": "6.2 CONVOLUTIONAL NETWORKS", "text": "In this section we investigate replacing the fully connected layers of a deep convolutional network with a cascade of ACDC layers. In particular we use the CaffeNet architecture4 for ImageNet (Deng et al., 2009). We target the two fully connected layers located between features extracted from the last convolutional layer and the final logistic regression layer, which we replace with 12 stacked ACDC transforms interleaved with ReLU non-linearities and permutations. The permutations assure that adjacent SELLs are incoherent.\nThe model was trained using the SGD algorithm with learning rate 0.1 multiplied by 0.1 every 100,000 iterations, momentum 0.65 and weight decay 0.0005. The output from the last convolutional layer was scaled by 0.1, and the learning rates for each matrix A and D were multiplied by 24 and 12. All diagonal matrices were initialized from N (1, 0.061) distribution. No weight decay was applied to A or D. Additive biases were added to the matrices D, but not to A, as this sufficed to provide the ACDC layer with a bias terms just before the ReLU non-linearities. Biases were initialized to 0. To prevent the model from overfitting dropout regularization was placed before each of the last 5 SELL layers with dropout probability equal to 0.1.\nThe resulting model arrives at 43.26% error which is only 0.67% worse when compared to the reference model, so SELL confidently stays within 1% of the performance of the original network. We report this result, as well as a comparison to several other works in Table 1.\n4 https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_caffenet\nThe two fully connected layers of CaffeNet, consisting of more than 41 million parameters, are replaced with SELL modules which contain a combined 165, 888 parameters. These results agree with the hypothesis that neural networks are over-parameterized formulated by Denil et al. (2013) and supported by Yang et al. (2015). At the same time such a tremendous reduction without significant loss of accuracy suggests that SELL is a powerful concept and a way to use parameters efficiently.\nThis approach is an improvement over Deep Fried Convnets (Yang et al., 2015) and other FastFood (Le et al., 2013) based transforms in the sense that the layers remain narrow and become deep (potentially interleaved with non-linearites) as opposed to wide and shallow, while maintaining comparable or better performance. The result of narrower layers is that the final softmax classification layer requires substantially fewer parameters, meaning that the resulting compression ratio is higher.\nOur experiment shows that ACDC transforms are an attractive building block for feedforward convolutional architectures, that can be used as a structured alternative to fully connected layers, while fitting very well into the deep learning philosophy of introducing transformations executed in steps as the signal is propagated down the network rather than projecting to higher-dimensional spaces.\nIt should be noted that the method of pruning proposed in (Han et al., 2015b) and the follow-up method of pruning, quantizing and Huffman coding proposed in (Han et al., 2015a) achieve compression rates between x9 and x35 on AlexNet by applying a pipeline of reducing operations on a trained models. Usually it is necessary to perform at least a few iterations of such reductions to arrive at the stated compression rates. For the AlexNet model one such iteration takes 173 hours according to (Han et al., 2015b). On top of that as this method requires training the original full model the time cost of that operation should be taken into consideration as well.\nCompressing pipelines target models that are ready for deployment and function in the environment where amount of time spent on training is absolutely dominated by the time spent evaluating predictions. In contrast, SELL methods are appropriate for incorporation into the design of a model."}, {"heading": "7 CONCLUSION", "text": "We introduced a new Structured Efficient Linear Layer, which adds to a growing literature on using memory efficient structured transforms as efficient replacements for the dense matrices in the fully connected layers of neural networks. The structure of our SELL is motivated by matrix approximation results from Fourier optics, but has been specialized for efficient implementation on NVIDIA GPUs.\nWe have shown that proper initialization of our SELL allows us to build very deep cascades of SELLs that can be optimized using SGD. Proper initialization is simple, but is essential for training cascades of SELLs with more than a few layers. Working with deep and narrow cascades of SELLs makes our networks more parameter efficient than previous works using shallow and wide cascades because the cost of layers interfacing between the SELL and the rest of the network is reduced (e.g. the size of the input to the dense logistic regression layer of the network is much smaller).\nIn future work we plan to investigate replacing the diagonal layers of ACDC with other efficient structured matrices such as band or block diagonals. These alternatives introduce additional parameters in each layer, but may give us the opportunity to explore the continuum between depth and expressive power per layer more precisely."}], "references": [{"title": "The Fast Johnson Lindenstrauss Transform and approximate nearest neighbors", "author": ["Ailon", "Nir", "Chazelle", "Bernard"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Ailon et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2009}, {"title": "Speeding up neural networks for large scale classification using WTA", "author": ["Bakhtiary", "Amir H", "Lapedriza", "\u00c0gata", "Masip", "David"], "venue": "hashing. ArXiv,", "citeRegEx": "Bakhtiary et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bakhtiary et al\\.", "year": 2015}, {"title": "Weight uncertainty in neural networks", "author": ["Blundell", "Charles", "Cornebise", "Julien", "Kavukcuoglu", "Koray", "Wierstra", "Daan"], "venue": "In ICML,", "citeRegEx": "Blundell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Blundell et al\\.", "year": 2015}, {"title": "Compressing neural networks with the hashing trick", "author": ["Chen", "Wenlin", "Wilson", "James", "Tyree", "Stephen", "Weinberger", "Kilian Q", "Yixin"], "venue": "In ICML,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "An exploration of parameter redundancy in deep networks with circulant projections", "author": ["Cheng", "Yu", "Felix X", "R Feris", "Kumar", "Sanjiv", "Choudhary", "Alok", "Chang", "Shih-Fu"], "venue": "In ICCV,", "citeRegEx": "Cheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Memory bounded deep convolutional networks", "author": ["Collins", "Maxwell D", "Kohli", "Pushmeet"], "venue": "Technical report, University of Wisconsin-Madison,", "citeRegEx": "Collins et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2014}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. Fei-Fei"], "venue": "In CVPR09,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Predicting parameters in deep learning", "author": ["Denil", "Misha", "Shakibi", "Babak", "Dinh", "Laurent", "Ranzato", "Marc\u2019Aurelio", "de Freitas", "Nando"], "venue": "In NIPS,", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Matrix Computations", "author": ["Golub", "Gene H", "Van Loan", "Charles F"], "venue": null, "citeRegEx": "Golub et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Golub et al\\.", "year": 1996}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Gong", "Yunchao", "Liu", "Yang", "Ming", "Bourdev", "Lubomir"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "Technical report, Google DeepMind,", "citeRegEx": "Graves et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2015}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": "CoRR, abs/1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural networks", "author": ["Han", "Song", "Pool", "Jeff", "Tran", "John", "Dally", "William J"], "venue": "In NIPS,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Towards trainable media: Using waves for neural networkstyle training", "author": ["Hermans", "Michiel", "Vaerenbergh", "Thomas Van"], "venue": "arXiv preprint arXiv:1510.03776,", "citeRegEx": "Hermans et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermans et al\\.", "year": 2015}, {"title": "Distilling the knowledge in a neural network. ArXiv", "author": ["Hinton", "Geoffrey E", "Vinyals", "Oriol", "Dean", "Jeffrey"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Approximating ideal diffractive optical systems", "author": ["Huhtanen", "Marko"], "venue": "Journal of Mathematical Analysis and Applications,", "citeRegEx": "Huhtanen and Marko.,? \\Q2008\\E", "shortCiteRegEx": "Huhtanen and Marko.", "year": 2008}, {"title": "Factoring matrices into the product of circulant and diagonal matrices", "author": ["Huhtanen", "Marko", "Permki", "Allan"], "venue": "Journal of Fourier Analysis and Applications,", "citeRegEx": "Huhtanen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huhtanen et al\\.", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "Annual Conference on Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Fastfood \u2013 approximating kernel expansions in loglinear time", "author": ["Le", "Quoc", "Sarl\u00f3s", "Tam\u00e1s", "Smola", "Alex"], "venue": "In ICML,", "citeRegEx": "Le et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "Sparse convolutional neural networks", "author": ["Liu", "Baoyuan", "Wang", "Min", "Foroosh", "Hassan", "Tappen", "Marshall", "Pensky", "Marianna"], "venue": "In CVPR,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Algorithmic design of diffractive optical systems for information processing", "author": ["M\u00fcller-Quade", "J\u00f6rn", "Aagedal", "Harald", "Beth", "Th", "Schmid", "Michael"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "M\u00fcller.Quade et al\\.,? \\Q1998\\E", "shortCiteRegEx": "M\u00fcller.Quade et al\\.", "year": 1998}, {"title": "Tensorizing neural networks", "author": ["Novikov", "Alexander", "Podoprikhin", "Dmitry", "Osokin", "Anton", "Vetrov"], "venue": "In NIPS,", "citeRegEx": "Novikov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Novikov et al\\.", "year": 2015}, {"title": "Efficient parallel algorithms for optical computing with the dft primitive", "author": ["Reif", "John", "Tyagi", "Akhilesh"], "venue": "Applied Optics,", "citeRegEx": "Reif et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Reif et al\\.", "year": 1997}, {"title": "FitNets: Hints for thin deep nets", "author": ["Romero", "Adriana", "Ballas", "Nicolas", "Kahou", "Samira Ebrahimi", "Chassang", "Antoine", "Gatta", "Carlo", "Bengio", "Yoshua"], "venue": "In ICLR,", "citeRegEx": "Romero et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2015}, {"title": "Random projections through multiple optical scattering: Approximating kernels at the speed of light", "author": ["Saade", "Alaa", "Caltagirone", "Francesco", "Carron", "Igor", "Daudet", "Laurent", "Dremeau", "Angelique", "Gigan", "Sylvain", "Krzakala", "Florent"], "venue": "arXiv preprint arXiv:1510.06664,", "citeRegEx": "Saade et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Saade et al\\.", "year": 2015}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["Sainath", "Tara N", "Kingsbury", "Brian", "Sindhwani", "Vikas", "Arisoy", "Ebru", "Ramabhadran", "Bhuvana"], "venue": "In ICASSP,", "citeRegEx": "Sainath et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2013}, {"title": "Decomposing a matrix into circulant and diagonal factors", "author": ["Schmid", "Michael", "Steinwandt", "Rainer", "Mller-Quade", "Jrn", "Rtteler", "Martin", "Beth", "Thomas"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Schmid et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Schmid et al\\.", "year": 2000}, {"title": "Structured transforms for small-footprint deep learning", "author": ["Sindhwani", "Vikas", "Sainath", "Tara N", "Kumar", "Sanjiv"], "venue": "In NIPS,", "citeRegEx": "Sindhwani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sindhwani et al\\.", "year": 2015}, {"title": "End-to-end memory networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": "In NIPS,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Cho", "Kyunghyun", "Courville", "Aaron", "Salakhutdinov", "Ruslan", "Zemel", "Richard", "Bengio", "Yoshua"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition", "author": ["Xue", "Jian", "Li", "Jinyu", "Gong", "Yifan"], "venue": "In INTERSPEECH,", "citeRegEx": "Xue et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2013}, {"title": "Deep fried convnets", "author": ["Yang", "Zichao", "Moczulski", "Marcin", "Denil", "Misha", "de Freitas", "Nando", "Smola", "Alex", "Song", "Le", "Wang", "Ziyu"], "venue": "In ICCV,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 32, "context": "Other specialized network modules including LSTMs; Gated Recurrent Units; the attentional mechanisms used for image captioning (Xu et al., 2015), and also for reading and writing in the Neural Turing Machine (Graves et al.", "startOffset": 127, "endOffset": 144}, {"referenceID": 10, "context": ", 2015), and also for reading and writing in the Neural Turing Machine (Graves et al., 2015) and Memory Networks (Sukhbaatar et al.", "startOffset": 71, "endOffset": 92}, {"referenceID": 30, "context": ", 2015) and Memory Networks (Sukhbaatar et al., 2015) are all built from compositions of linear layers and nonlinear modules, such as sigmoid, softmax and ReLU layers.", "startOffset": 28, "endOffset": 53}, {"referenceID": 7, "context": "Indeed, several studies focusing on feedforward perceptrons and convolutional networks have shown that the parametrisation of linear layers is extremely wasteful, with up to 95% of the parameters being redundant (Denil et al., 2013; Gong et al., 2014; Sainath et al., 2013).", "startOffset": 212, "endOffset": 273}, {"referenceID": 9, "context": "Indeed, several studies focusing on feedforward perceptrons and convolutional networks have shown that the parametrisation of linear layers is extremely wasteful, with up to 95% of the parameters being redundant (Denil et al., 2013; Gong et al., 2014; Sainath et al., 2013).", "startOffset": 212, "endOffset": 273}, {"referenceID": 27, "context": "Indeed, several studies focusing on feedforward perceptrons and convolutional networks have shown that the parametrisation of linear layers is extremely wasteful, with up to 95% of the parameters being redundant (Denil et al., 2013; Gong et al., 2014; Sainath et al., 2013).", "startOffset": 212, "endOffset": 273}, {"referenceID": 20, "context": "Fastfood (Le et al., 2013), our second SELL example, extends fast random projections as follows: \u03a6 = D\u03031HPD\u03032HD\u03033.", "startOffset": 9, "endOffset": 26}, {"referenceID": 34, "context": "(4) In (Yang et al., 2015), the authors introduce an adaptive variant of Fastfood, with the random diagonal matrices replaced by diagonal matrices of parameters, and show that it outperforms the random counterpart when applied to the problem of replacing one of the fully connected layers of a convolutional neural network for ImageNet (Jia et al.", "startOffset": 7, "endOffset": 26}, {"referenceID": 17, "context": ", 2015), the authors introduce an adaptive variant of Fastfood, with the random diagonal matrices replaced by diagonal matrices of parameters, and show that it outperforms the random counterpart when applied to the problem of replacing one of the fully connected layers of a convolutional neural network for ImageNet (Jia et al., 2014).", "startOffset": 317, "endOffset": 335}, {"referenceID": 34, "context": "In addition, instead of using a (single) shallow SELL as in previous works (Yang et al., 2015; Cheng et al., 2015; Sindhwani et al., 2015), we consider deep SELLs:", "startOffset": 75, "endOffset": 138}, {"referenceID": 4, "context": "In addition, instead of using a (single) shallow SELL as in previous works (Yang et al., 2015; Cheng et al., 2015; Sindhwani et al., 2015), we consider deep SELLs:", "startOffset": 75, "endOffset": 138}, {"referenceID": 29, "context": "In addition, instead of using a (single) shallow SELL as in previous works (Yang et al., 2015; Cheng et al., 2015; Sindhwani et al., 2015), we consider deep SELLs:", "startOffset": 75, "endOffset": 138}, {"referenceID": 4, "context": "Cheng et al. (2015) introduced a SELL consisting of the product of a circulant matrix (R) and a random diagonal matrix (D\u03031).", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "Cheng et al. (2015) introduced a SELL consisting of the product of a circulant matrix (R) and a random diagonal matrix (D\u03031). Since circulant matrices can be diagonalized with the discrete Fourier transform (Golub & Van Loan, 1996), this SELL falls within our general notation: \u03a6 = D\u03031R = D\u03031FD2F . (5) Sindhwani et al. (2015) introduced a Toeplitz-like structured transform, within the framework of displacement operators.", "startOffset": 0, "endOffset": 327}, {"referenceID": 4, "context": "Cheng et al. (2015) introduced a SELL consisting of the product of a circulant matrix (R) and a random diagonal matrix (D\u03031). Since circulant matrices can be diagonalized with the discrete Fourier transform (Golub & Van Loan, 1996), this SELL falls within our general notation: \u03a6 = D\u03031R = D\u03031FD2F . (5) Sindhwani et al. (2015) introduced a Toeplitz-like structured transform, within the framework of displacement operators. Since Toeplitz matrices can be \u201cembedded\u201d in circulant matrices, they can also be diagonalized with the discrete Fourier transform (Golub & Van Loan, 1996). In this work, we introduce a SELL that could be thought of as an adaptive variant of the method of Cheng et al. (2015). In addition, instead of using a (single) shallow SELL as in previous works (Yang et al.", "startOffset": 0, "endOffset": 700}, {"referenceID": 22, "context": "The two-dimensional Fourier transform can be carried out with one lens in free space, while the operation of a diffractive element can be described by a diagonal matrix (Reif & Tyagi, 1997; M\u00fcller-Quade et al., 1998; Huhtanen, 2008; Schmid et al., 2000).", "startOffset": 169, "endOffset": 253}, {"referenceID": 28, "context": "The two-dimensional Fourier transform can be carried out with one lens in free space, while the operation of a diffractive element can be described by a diagonal matrix (Reif & Tyagi, 1997; M\u00fcller-Quade et al., 1998; Huhtanen, 2008; Schmid et al., 2000).", "startOffset": 169, "endOffset": 253}, {"referenceID": 22, "context": "The two-dimensional Fourier transform can be carried out with one lens in free space, while the operation of a diffractive element can be described by a diagonal matrix (Reif & Tyagi, 1997; M\u00fcller-Quade et al., 1998; Huhtanen, 2008; Schmid et al., 2000). So, in principle, it is possible to envision an optical implementation of our deep SELL. Very recently, Hermans & Vaerenbergh (2015) considered using waves for neural network learning.", "startOffset": 190, "endOffset": 388}, {"referenceID": 22, "context": "The two-dimensional Fourier transform can be carried out with one lens in free space, while the operation of a diffractive element can be described by a diagonal matrix (Reif & Tyagi, 1997; M\u00fcller-Quade et al., 1998; Huhtanen, 2008; Schmid et al., 2000). So, in principle, it is possible to envision an optical implementation of our deep SELL. Very recently, Hermans & Vaerenbergh (2015) considered using waves for neural network learning. Their cascaded system consisting of products of diagonal matrices of phase shifts and unitary matrices can be thought of as another futuristic avenue for implementing deep SELL. Even more recently, Saade et al. (2015) disclosed an invention that peforms optical analog random projections.", "startOffset": 190, "endOffset": 658}, {"referenceID": 7, "context": "As mentioned earlier, many studies have shown that the parametrisation of linear layers is extremely wasteful (Denil et al., 2013; Gong et al., 2014; Sainath et al., 2013).", "startOffset": 110, "endOffset": 171}, {"referenceID": 9, "context": "As mentioned earlier, many studies have shown that the parametrisation of linear layers is extremely wasteful (Denil et al., 2013; Gong et al., 2014; Sainath et al., 2013).", "startOffset": 110, "endOffset": 171}, {"referenceID": 27, "context": "As mentioned earlier, many studies have shown that the parametrisation of linear layers is extremely wasteful (Denil et al., 2013; Gong et al., 2014; Sainath et al., 2013).", "startOffset": 110, "endOffset": 171}, {"referenceID": 33, "context": "Several methods based on low-rank decomposition and sparseness have been proposed to eliminate parameter redundancy at test time, but they provide only a partial solution as the full network must be instantiated during training (Collins & Kohli, 2014; Xue et al., 2013; Blundell et al., 2015; Liu et al., 2015; Han et al., 2015b).", "startOffset": 228, "endOffset": 329}, {"referenceID": 2, "context": "Several methods based on low-rank decomposition and sparseness have been proposed to eliminate parameter redundancy at test time, but they provide only a partial solution as the full network must be instantiated during training (Collins & Kohli, 2014; Xue et al., 2013; Blundell et al., 2015; Liu et al., 2015; Han et al., 2015b).", "startOffset": 228, "endOffset": 329}, {"referenceID": 21, "context": "Several methods based on low-rank decomposition and sparseness have been proposed to eliminate parameter redundancy at test time, but they provide only a partial solution as the full network must be instantiated during training (Collins & Kohli, 2014; Xue et al., 2013; Blundell et al., 2015; Liu et al., 2015; Han et al., 2015b).", "startOffset": 228, "endOffset": 329}, {"referenceID": 3, "context": "Hashing techniques have been proposed to reduce the number of parameters (Chen et al., 2015; Bakhtiary et al., 2015).", "startOffset": 73, "endOffset": 116}, {"referenceID": 1, "context": "Hashing techniques have been proposed to reduce the number of parameters (Chen et al., 2015; Bakhtiary et al., 2015).", "startOffset": 73, "endOffset": 116}, {"referenceID": 14, "context": "Distillation (Hinton et al., 2015; Romero et al., 2015) also offers a way of compressing neural networks, as a post-processing step.", "startOffset": 13, "endOffset": 55}, {"referenceID": 25, "context": "Distillation (Hinton et al., 2015; Romero et al., 2015) also offers a way of compressing neural networks, as a post-processing step.", "startOffset": 13, "endOffset": 55}, {"referenceID": 4, "context": "As mentioned earlier, many studies have shown that the parametrisation of linear layers is extremely wasteful (Denil et al., 2013; Gong et al., 2014; Sainath et al., 2013). In spite of this redundancy, there has been little success in improving the linear layer, since natural extensions, such as low rank factorizations, lead to poor performance when trained end to end. For instance, Sainath et al. (2013) demonstrate significant improvements in reducing the number of parameters of the output softmax layers, but only modest improvements for the hidden linear layers.", "startOffset": 111, "endOffset": 408}, {"referenceID": 1, "context": ", 2015; Bakhtiary et al., 2015). Hashes have irregular memory access patterns and, consequently, good performance on large GPU-based platforms is an open problem. Distillation (Hinton et al., 2015; Romero et al., 2015) also offers a way of compressing neural networks, as a post-processing step. Novikov et al. (2015) use a multi-linear transform (Tensor-Train decomposition) to attain significant reductions in the number of parameters in some of the linear layers of convolutional networks.", "startOffset": 8, "endOffset": 318}, {"referenceID": 4, "context": "0 Cheng et al. (2015) (Circulant CNN 2) 0.", "startOffset": 2, "endOffset": 22}, {"referenceID": 4, "context": "0 Cheng et al. (2015) (Circulant CNN 2) 0.40% > 16.3M < x3.8 Novikov et al. (2015) (TT4 FC FC) 0.", "startOffset": 2, "endOffset": 83}, {"referenceID": 4, "context": "0 Cheng et al. (2015) (Circulant CNN 2) 0.40% > 16.3M < x3.8 Novikov et al. (2015) (TT4 FC FC) 0.30% - x3.9 Novikov et al. (2015) (TT4 TT4 FC) 1.", "startOffset": 2, "endOffset": 130}, {"referenceID": 4, "context": "0 Cheng et al. (2015) (Circulant CNN 2) 0.40% > 16.3M < x3.8 Novikov et al. (2015) (TT4 FC FC) 0.30% - x3.9 Novikov et al. (2015) (TT4 TT4 FC) 1.30% - x7.4 Yang et al. (2015) (Finetuned SVD 1) 0.", "startOffset": 2, "endOffset": 175}, {"referenceID": 4, "context": "0 Cheng et al. (2015) (Circulant CNN 2) 0.40% > 16.3M < x3.8 Novikov et al. (2015) (TT4 FC FC) 0.30% - x3.9 Novikov et al. (2015) (TT4 TT4 FC) 1.30% - x7.4 Yang et al. (2015) (Finetuned SVD 1) 0.14% 46.6M x1.3 Yang et al. (2015) (Finetuned SVD 2) 1.", "startOffset": 2, "endOffset": 229}, {"referenceID": 4, "context": "0 Cheng et al. (2015) (Circulant CNN 2) 0.40% > 16.3M < x3.8 Novikov et al. (2015) (TT4 FC FC) 0.30% - x3.9 Novikov et al. (2015) (TT4 TT4 FC) 1.30% - x7.4 Yang et al. (2015) (Finetuned SVD 1) 0.14% 46.6M x1.3 Yang et al. (2015) (Finetuned SVD 2) 1.22% 23.4M x2.0 Yang et al. (2015) (Adaptive Fastfood 16) 0.", "startOffset": 2, "endOffset": 283}, {"referenceID": 6, "context": "In particular we use the CaffeNet architecture4 for ImageNet (Deng et al., 2009).", "startOffset": 61, "endOffset": 80}, {"referenceID": 34, "context": "This approach is an improvement over Deep Fried Convnets (Yang et al., 2015) and other FastFood (Le et al.", "startOffset": 57, "endOffset": 76}, {"referenceID": 20, "context": ", 2015) and other FastFood (Le et al., 2013) based transforms in the sense that the layers remain narrow and become deep (potentially interleaved with non-linearites) as opposed to wide and shallow, while maintaining comparable or better performance.", "startOffset": 27, "endOffset": 44}, {"referenceID": 7, "context": "These results agree with the hypothesis that neural networks are over-parameterized formulated by Denil et al. (2013) and supported by Yang et al.", "startOffset": 98, "endOffset": 118}, {"referenceID": 7, "context": "These results agree with the hypothesis that neural networks are over-parameterized formulated by Denil et al. (2013) and supported by Yang et al. (2015). At the same time such a tremendous reduction without significant loss of accuracy suggests that SELL is a powerful concept and a way to use parameters efficiently.", "startOffset": 98, "endOffset": 154}], "year": 2017, "abstractText": "The linear layer is one of the most pervasive modules in deep learning representations. However, it requiresO(N) parameters and O(N) operations. These costs can be prohibitive in mobile applications or prevent scaling in many domains. Here, we introduce a deep, differentiable, fully-connected neural network module composed of diagonal matrices of parameters, A and D, and the discrete cosine transform C. The core module, structured as ACDC, has O(N) parameters and incurs O(N logN) operations. We present theoretical results showing how deep cascades of ACDC layers approximate linear layers. ACDC is, however, a stand-alone module and can be used in combination with any other types of module. In our experiments, we show that it can indeed be successfully interleaved with ReLU modules in convolutional neural networks for image recognition. Our experiments also study critical factors in the training of these structured modules, including initialization and depth. Finally, this paper also provides a connection between structured linear transforms used in deep learning and the field of Fourier optics, illustrating how ACDC could in principle be implemented with lenses and diffractive elements.", "creator": "LaTeX with hyperref package"}}}