{"id": "1511.06147", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Coreset-Based Adaptive Tracking", "abstract": "We propose a method for learning from streaming visual data using a compact, constant size representation of all the data that was seen until a given moment. Specifically, we construct a 'coreset' representation of streaming data using a parallelized algorithm, which is an approximation of a set with relation to the squared distances between this set and all other points in its ambient space. We learn an adaptive object appearance model from the coreset tree in constant time and logarithmic space and use it for object tracking by detection. Our method obtains excellent results for object tracking on three standard datasets over more than 100 videos. The ability to summarize data efficiently makes our method ideally suited for tracking in long videos in presence of space and time constraints. We demonstrate this ability by outperforming a variety of algorithms on the TLD dataset with 2685 frames on average. This coreset based learning approach can be applied for both real-time learning of small, varied data and fast learning of big data. As an example, we use a single method to visualize the full size and complexity of a TLD dataset with 30,000 streams of data. The result is, we have a very real-time approach for large video releases, including large, medium-scale streaming. These approaches provide high efficiency for streamers to accurately understand the size and complexity of the stream.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 19 Nov 2015 12:59:20 GMT  (5112kb,D)", "http://arxiv.org/abs/1511.06147v1", "8 pages, 5 figures, In submission to IEEE TPAMI (Transactions on Pattern Analysis and Machine Intelligence)"]], "COMMENTS": "8 pages, 5 figures, In submission to IEEE TPAMI (Transactions on Pattern Analysis and Machine Intelligence)", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["abhimanyu dubey", "nikhil naik", "dan raviv", "rahul sukthankar", "ramesh raskar"], "accepted": false, "id": "1511.06147"}, "pdf": {"name": "1511.06147.pdf", "metadata": {"source": "CRF", "title": "Coreset-Based Adaptive Tracking", "authors": ["Abhimanyu Dubey", "Nikhil Naik", "Dan Raviv", "Rahul Sukthankar", "Ramesh Raskar"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014Object tracking, learning from video, object recognition, feature measurements\nF"}, {"heading": "1 INTRODUCTION", "text": "Real-world computer vision systems are increasingly required to provide robust real-time performance under memory and bandwidth constraints for tasks such as tracking, on-the-fly object detection, and visual search. Robots with visual sensors may be required to perform all of these tasks while interacting with their environment. On another extreme, IP cameras, life logging devices, and other visual sensors may want to hold a summary of the data stream seen over a long time. Hence, new algorithms that enable real-world computer vision applications under a tight space-time budget are a necessity. In this paper, we propose new algorithms with low space-time complexity for learning from streaming visual data, inspired by research in computational geometry.\nIn recent years, computational geometry researchers have proposed techniques to obtain small representative sets of points from larger sets with constant or logarithmic space-time complexity [6], [14], [16]. We propose to adapt these techniques to retain a summary of visual data and train object appearance models in constant time and space from this summary. Specifically, we perform summarization of a video stream based on \u201ccoresets\u201d, which are defined as a small set of points that approximately maintain the properties of the original set with respect to the distance between the set and other structures in its ambient space with theoretical guarantees [18]. We demonstrate that the coreset formulation is useful for learning object appearance models for detection and tracking. Coresets have been primarily used as a technique to convert \u201cbig data\u201d into manageable sizes [27]. In contrast, we employ coresets for reducing small data to \u201ctiny data\u201d, and perform real-time classifier training.\nWe use the object appearance model learned from the coreset representation, for visual tracking by detection.\n\u2022 Abhimanyu Dubey is an undergraduate student at the Indian Institute of Technology, New Delhi, India. \u2022 Rahul Sukthankar is a research scientist at Google. \u2022 Nikhil Naik, Dan Raviv, and Ramesh Raskar are with the Media Labora-\ntory at Massachusetts Institute of Technology.\nTracking systems typically retain just an appearance model for the object [20], [35] and optionally, data from some prior frames [15], [31]. In contrast, we compute a summary of all the previous frames in constant time (on average) and update our appearance model continuously with this summary. The summarization process improves the appearance model with time. As a result, our method is beneficial for tracking objects in very long video sequences that need to deal with variation in lighting, background, and object appearance, along with the object leaving and re-entering the video.\nWe demonstrate the potential of this technique with a simple tracker that combines detections from a linear SVM trained from the coreset with a Kalman filter. We call this no-frills tracker the \u201cCoreset Adaptive Tracker (CAT).\u201d CAT obtains competitive performance on three standard datasets, the CVPR2013 Tracking Benchmark [33], the Princeton Tracking Benchmark [30], and the TLD Dataset [21]. Our method is found to be superior to popular methods for tracking longer video sequences from the TLD dataset.\nWe must emphasize that the primary goal of this paper is not to develop the best algorithm for tracking\u2014which is evident from our simple design. Rather, our goal is to demonstrate the potential of coresets as a data reduction technique for learning from visual data. We summarize our contributions below."}, {"heading": "1.1 Contributions", "text": "\u2022 We propose an algorithm for learning and detection of\nobjects from streaming data by training a classifier in constant time using coresets. We construct coresets using a parallel algorithm and prove that this algorithm requires constant time (on average) and logarithmic space. \u2022 We introduce the concept of hierarchical sampling from coresets, which improves learning performance significantly when compared to the original formulation [14]. \u2022 We obtain competitive results on standard benchmarks [21], [30], [33]. Our method is especially suited for longer videos (with several hundred frames), since it summarizes data from all the frames in the video so far.\nar X\niv :1\n51 1.\n06 14\n7v 1\n[ cs\n.C V\n] 1\n9 N\nov 2\n01 5\n2"}, {"heading": "2 RELATED WORK", "text": "First, we introduce the literature on coresets, followed by a summary of the prior art on online learning for tracking by detection.\nCoresets and Data Reduction: The term coreset was first introduced by Agarwal et al. [2] in their seminal work on k-median and k-means problems. Badoiu et al. [6] propose a coreset for clustering using a subset of input points to generate the solution, while Har-Peled and Mazumdar [18] provide a different solution for coreset construction which includes points that may not be a subset of the original set. Feldman et al. [13] demonstrate that a weak coreset can be generated independent of number of points in the data or their feature dimensions. We follow the formulation of Feldman et al. [14], which provides an eminently parallelizable streaming algorithm for coreset construction with a theoretical error bound. This formulation has been recently used for applications in robotics [12] and for summarization of time series data [27].\nOnline Learning for Detection and Tracking: The problem of object tracking by detection has been well studied in computer vision [29]. Early important examples of learning based methods include [8], [11], [20]. Ross et al. [28] learn a low dimensional subspace representation of object appearance using incremental algorithms for PCA. Li et al. [24] propose a compressive sensing tracking method using efficient l1 norm minimization. Discriminative algorithms solve the tracking problem using a binary classification model for object appearance. Avidan [3] combines optical flow with SVMs and performs tracking by minimizing the SVM classification score, and extends this method with an \u201censemble tracker\u201d using Adaboost [4]. Grabner et al. [17] develop a semi-supervised boosting framework to tackle the problem of drift due to incorrect detections. Kwon et al. [22] represent the object using multiple appearance models, obtained from sparse PCA, which are integrated using an MCMC framework. Babenko et al. [5] introduce the semi-supervised multiple instance learning (MIL) algorithm. Hare et al. [19] learn an online structured output SVM classifier for adaptive tracking. Zhang et al. [34] develop an object appearance model based on efficient feature extraction in the compressed domain and use the features discriminatively to separate the object from its background. Kalal et al. [21] introduce the TLD algorithm, where a pair of \u201cexpert\u201d learners select positive and negative examples in a semi-supervised manner for real-time tracking. Gao et al. [15] combine results of a tracker learnt using prior knowledge from auxiliary samples and another tracker learnt with samples from recent frames. Our work contributes to this literature with a novel coreset representation for model-based tracking."}, {"heading": "3 CONSTRUCTING CORESETS", "text": "In this section, we introduce the key theoretical ideas behind coresets and low-rank approximation and provide new results for the time and space complexity of a parallel algorithm for coreset construction."}, {"heading": "3.1 Coresets", "text": "We denote time series data of d dimensions as rows in a matrix A, and search for a subset or a subspace of A such that the sum of squared distances from the rest of the domain to this summarized structure is an (1 + )- approximation of that of A, where is a small constant (0 \u2264 \u2264 1). Specifically, we call A\u0303 a \u2018coreset\u2019 of A if\n(1\u2212 )dist2(A,S) \u2264 dist2(A\u0303, S) + c \u2264 (1 + )dist2(A,S), (1)\nwhere dist2(A,S) denotes the sum of squared distances from each row onA to its closest point in S and c is constant. Eq. (1) can be rewritten in a matrix form using the Frobenius norm [14] as\n(1\u2212 )\u2016AY \u20162F \u2264 \u2016A\u0303Y \u20162F + c \u2264 (1 + )\u2016AY \u20162F , (2)\nwhere Y is a d\u00d7 (d\u2212 k) orthogonal matrix. Going forward we denote A\u0303 \u2248 A if A\u0303 is a (1 + )-approximation of A. The best low dimensional approximation can be obtained by calculating the k dimensional eigen-structure, e.g., using Singular Value Decomposition (SVD). Several results have been recently proposed for obtaining a low rank approximation of the data for summarization more efficiently than SVD. However, very few of these algorithms are designed for a streaming setting [10], [14], [16], [26].\nMotivated by [14], we adopt a tree formulation to approximate the low rank subspace which best represents the data. Unlike other methods, the tree formulation has sufficiently low time complexity for a real-time learner, considering the constants. Using this formulation, the time complexity for the summarization and learning steps is constant (on average) in between observations. In the next section we formalize and prove time and space guarantees for this approach."}, {"heading": "3.2 Coreset Tree", "text": "The key idea here is that a low rank approximation can be \u201capproximated\u201d using a merge-and-reduce technique. The origin of this approach goes back to Bentley and Saxe [7], and was further adopted for coresets in [2]. In this work, following Feldman et al. [14], we generate a tree structure, which holds a summary of the streaming data.\nWe denote Ci \u2208 Rn\u00d7d as a leaf in the tree constructed from n rows of d dimensional features. The total number of data points captured until time t is marked by p(t). For ease of calculations, we define 2q(t) = p(t)n . Thus, bq(t)c is the depth of the tree in time t. We construct a binary tree from the streaming data, and compress the data by merging the leaves, whenever two adjacent leaves get formed at the same level. If p(t) = n2q0 , only the root node remains in memory after repeated merging of leaves. For the next n2q0 streamed points, the right side of the binary tree keeps growing and merging, until it is reduced to a single node again, when n2q0+1 data points arrive (Figure 1).\nIf Ci and Cj are two (1 + ) coresets then their concatenation [ Ci Cj ] is also an (1 + ) coreset. Since\u2225\u2225\u2225\u2225[CiCj ] Y \u2225\u2225\u2225\u22252 F = \u2016CiY \u20162F + \u2016CjY \u20162F , (3)\n3 C1 1 C1 C2 2 C1 C2 C123 C1 C2 C12 C3 4\nC1 C2\nC34\nC3 C4\nC12\n6\nC1 C2\nC34\nC3 C4\nC12\nC1234 75\nC1 C2 C3 C4\nC12\nlevel 0 level 0 level 1 level 1\nlevel 1 level 1 level 2\nFig. 1: Tree construction: Coresets are constructed using a merge-and-reduce approach. Whenever two coreset nodes are present at the same level, they are merged to construct a higher level coreset (shown in green). The nodes that have been merged are deleted from the memory (shown in red). We prove in Section 3 that the space requirement for tree construction is O(log p(t)) in the worst case, where p(t) is the number of data points at time t, and on average the time complexity is constant per time step.\nthe proof follows trivially from the definition of a coreset. This identity implies that two nodes which are (1 + ) approximation can be concatenated and then re-compressed back to size n. This is a key feature of the tree formulation, which repeatedly performs compression from 2n\u00d7d to n\u00d7d. Each computation can be considered constant in time and space. However, this formulation also reduces the quality of the approximation, since the merged coreset is an (1 + )2 approximation of the two nodes. For every level of the tree, a multiplication error of (1 + ) is introduced from the leaf node to the root."}, {"heading": "3.2.1 Data Approximation in the Tree", "text": "We denote the entire set up to time t by \u222atCi, where Ci are the leaves. We denote the coreset tree by 2\u222atCi . It follows that\nTheorem 1. If \u2200i C\u0303i is a (1 + /q(t)) approximation of Ci then the collapse of the tree 2\u222atCi (i.e. the root) is a (1 + /3) coreset of \u222atCi Proof. We concatenate and compress sets of two leaves at every level of the tree. Compressing a 2n \u00d7 d points into n\u00d7d points introduces an additional multiplicative approximation of (1+ /q(t)). Since the maximum depth of the tree at time t is q(t), and(\n1 +\nq(t)\n)q(t) \u2264 exp( /6) < (1 + /3), (4)\nfor < 0.1, we conclude that if we compress 2n\u00d7d points to n\u00d7d with an (1+ /q(t)) approximation, the root of the tree will be a (1+ /3) approximation of the entire data \u222atCi."}, {"heading": "3.2.2 Space and Time Complexity", "text": "We now provide new proofs for the time and space complexity of coreset tree generation per time step and for the entire set.\nTheorem 2. The space complexity to hold a 2\u222atCi tree in memory is O(ndq(t)), where n is the size of the coreset of points with dimension d.\nProof. In a coreset tree, two children of the same parent node cannot be in the memory stack simultaneously, which\nimplies that, between two full collapses of the tree, the memory stack is increased at most by O(nd), which is the size of one leaf. Hence, the space complexity S(t) can be derived as follows \u2013\nS(n2q(t)) = 1\nS(p(t)) \u2264 nd+max p\u0302(t) S(p\u0302(t)) (5) n2q(t)\u22121 < p\u0302(t) < n2q(t) & n2q(t) < p(t) < n2q(t)+1\nThis recursive formulation results in a space requirement of S(p) \u2264 O(nd log(p(t)/n)) = O(ndq(t)). Assuming n and d are constants chosen a priori, the complexity of the memory is logarithmic in the number of points seen so far.\nTheorem 3. The time complexity of coreset tree construction between time step t and t+1 is O(log(p(t)/n)nd2) in the worst case, which is logarithmic in p, the total number of points.\nProof. In the worst case, we perform log(p(t)/n) compressions from the lower level all the way to the root. The compressions are performed using SVD. Since the time complexity of each SVD algorithm is O(nd2), assuming n > d,\nT (t+ 1)\u2212 T (t) \u2264 O(log(p(t)/n)nd2) = O(q(t)nd2) = O(q(t)), (6)\nwhere n and d are considered constants.\nThis scenario occurs for the last data point of a perfect subtree (p(t) = n2q(t)).\nFinally we wish to explore the total and average time complexity to summarize p points. On average, the time complexity is constant and does not depend on p.\nTheorem 4. The time complexity for a complete collapse of a tree to a single coreset of size n \u00d7 d is O(pd2), where d the dimension of each point and p is the number of points. The average time complexity per point is constant.\nProof. p points are split into p/n leaves with n\u00d7d dimension each. The time complexity of SVD is O(nd2), assuming n > d. A total of p(t)/n SVDs will be computed for compressing a perfect tree. Hence the total time complexity T (t) for all cycles until time t becomes\nT (t) = O ( p(t)\nn nd2\n) = O(p(t)d2) = O(p(t)) (7)\nassuming d is constant. We further infer that the average time complexity T\u0302 (t) is O(1) as\nT\u0302 (t) = T (t)\np(t) = O\n( p(t)d2\np(t)\n) = O(d2) = O(1). (8)"}, {"heading": "4 CAT: CORESET ADAPTIVE TRACKER", "text": "In the previous section, we describe an efficient method to obtain a summarization of all the data till a point in a streaming setting, with strong theoretical guarantees. In this section, we use the coreset tree for tracking by detection using a coreset appearance model for the object.\n4 Ci Ci+1 feature vector Ci Ci+1 Ci level = 0 level = 0\n(a) Compression\nHOG Feature\nExtraction\nlevel = 1\nt = 0\nt = n-2 t = n-1\nt = n\nt = 1\nVideo Frames\nCoreset Tree Construction\n(c) Detection\nt = n+1 t = n+2\nt = n+i\nCoreset Node\nCi+1\nlevel = 0\nDetection and Feature Extraction\n(b) Training\nn\nd\nCi level = 1\nFig. 2: Tracking by detection using coresets: Our system for learning from streaming data contains three main components, which run in parallel. (a) Compression using coresets: For each frame, we extract HoG features from the object. After n frames are processed, a new leaf is inserted into the coreset tree. The tree is compressed recursively by merging nodes at the same level using SVD. (b) Training: A linear SVM for object appearance is trained using data points from the coreset tree after every n frames are processed. The maximum size of the training set is 2n \u00d7 d. (c) Detection: The trained model is used for sliding window detection on every new frame. Features extracted from the detected area are inserted into the coreset tree."}, {"heading": "4.1 Coreset Tree Implementation", "text": "The coreset tree is stored as a stack of coreset nodes SC . Each coreset node SC [i] has two attributes level and features. level denotes the height of the coreset node in the tree, and features are the image features extracted from a frame. In a streaming setting, once n frames enter the frame stack, we create a coreset node with level = 0. We merge two coreset nodes when they have the same level."}, {"heading": "4.2 Coreset Appearance Model", "text": "In the coreset tree, any node at level q with size n represents n2q frames, since it is formed after repeatedly merging 2q leaf nodes. Nodes with different levels belong to different temporal instances in streaming data. Nodes higher up in the tree have older training samples, whereas a node with level 0 contains the last n frames. This is not ideal for training a tracking system, since more recent frames should have a representation in the learning process. To account for recent data, we propose an hierarchical sampling of the entire tree, constructing an (at most) 2n\u00d7d-size summarization of the data seen until a given time. For each SC [i] we calculate wi = 2(level(SC [i])\u2212level(top(SC)), and select the first nwi elements from SC [i]. This summarized data constitutes our \u201ccoreset appearance model\u201d, which is used to train an object detector."}, {"heading": "4.2.1 Advantages of Hierarchical Sampling", "text": "While the coreset tree formulation has been inspired by Feldman et al. [14], our idea of hierarchical sampling of the entire tree allows to account for recent data, which is crucial for many learning problems which need to avoid a temporal bias in the training set, and especially for tracking by detection. We demonstrate in Table 3 that hierarchical sampling improves the learning performance significantly as compared to just using the data from the root of the tree."}, {"heading": "4.3 System Implementation", "text": "In the first frame , user input is used to select a region of interest (ROI) for the object to be tracked. We resize the ROI to a 64 \u00d7 64 square from which we extract 2 \u00d7 2 HOG features from the RGB channels. We also extract HoG features from small affine transformations of the input frame and add them to our frame stack. The affine transformations include scaling, translation and rotation. The number of affine transformations are determined by the number of additional frames required to make the frame stack size reach a maximum of 25% of the total video length. For example, if the coreset size is 5% of the total length of the video, we add 4 transformations per frame. This process gives us the advantage of having more\u2014albeit synthetically generated\u2014samples to train the classifier, and makes the tracker robust to small object scale changes and translations.\nInitially, since no coreset appearance model has been trained, we need to use an alternate method for tracking (we use SCM [35] in our current implementation). Once first n frames have been tracked, the first leaf of the tree is formed. Data classification, compression, and training now run in parallel (Figure 2). These components are summarized below \u2013 \u2022 Classification: Detect the object in the stream from the\nlast trained classifier. \u2022 Compression: Update a coreset tree, by inserting new\nleaves and summarizing data whenever possible. \u2022 Training: Train an SVM classifier from the hierarchical\ncoreset appearance model.\nClassification If a classifier has been trained, we feed each sliding window to the classifier and obtain a confidence value. After hard thresholding and non-maximum suppression, we detect an ROI. To remove noise in the position of the ROI, we employ a Kalman filter. We use Expectation Maximization (EM) to obtain its parameters from the centers of the returned predictions of the previous n frames. The EM returns learnt parameters for prediction, which are used to estimate the latent position of the bounding box center. This latent position is used to update the Kalman filter parameters for the next iteration. Once the center of the bounding box is obtained, we calculate the true ROI. We extract features from the ROI and feed it to a feature stack SP . Until the first n frames arrive we obtain the ROI from SCM [35] since no classifier has been trained. This process continues till the last frame.\nCompression If |SP | > n, we pop the top n elements SP,n from the feature stack and push a coreset node containing elements SP,n into the coreset stack SC with level = 0. If level(SC [i]) = level(SC [i + 1]), they are merged by constructing a coreset from SC [i] and SC [i+ 1]. We store the coreset in SC [i], and delete SC [i+1]. The level for node SC [i] is increased by one. This process continues recursively.\nTraining If at least one leaf has been constructed and the size of the coreset stack changes, we train a one-class linear SVM using samples from SC . An (at most) 2n\u00d7d-size appearance\n5 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nOverlap Threshold\nS uc ce ss\nR at e\nSuccess Plots of OPE TGPR [0.567] CAT_25 [0.541] SCM [0.523] ASLA [0.500] Struck [0.496] CAT_10 [0.479] TLD [0.466] VTS [0.460] VTD [0.455] CSK [0.451] CXT [0.432] LSK [0.406] MTT [0.372]\n0 10 20 30 40 50 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nLocation Error Threshold\nS uc ce ss\nR at e\nPrecision Plots of OPE TGPR [0.801] CAT_25 [0.689] SCM [0.627] CAT_10 [0.621] Struck [0.619] VTS [0.594] VTD [0.589] CSK [0.586] ASLA [0.583] TLD [0.576] CXT [0.548] LSK [0.516] ORIA [0.468]\nFig. 3: Our methods (CAT 10 and CAT 25) obtains competitive performance on the CVPR2013 Tracking Benchmark on videos with 500 frames or longer. CAT 25 outperforms several top algorithms, such as Struck and SCM.\nAlgorithm Mean Target Type Target Size Movement Occlusion Motion TypeScore Human Animal Rigid Large Small Slow Fast Yes No Active Passive CAT 25 0.52 0.52 0.55 0.63 0.49 0.49 0.65 0.38 0.41 0.58 0.59 0.44 TGPR [15] 0.47 0.36 0.51 0.58 0.46 0.48 0.62 0.41 0.35 0.65 0.56 0.44 CAT 10 0.46 0.46 0.49 0.55 0.46 0.45 0.61 0.33 0.30 0.54 0.52 0.40 Struck [19] 0.44 0.35 0.47 0.53 0.45 0.44 0.58 0.39 0.30 0.64 0.54 0.41 VTD [22] 0.45 0.31 0.49 0.54 0.39 0.46 0.57 0.37 0.28 0.63 0.54 0.39 RGB [30] 0.42 0.27 0.41 0.55 0.32 0.46 0.51 0.36 0.35 0.47 0.56 0.34 TLD [21] 0.38 0.29 0.35 0.44 0.32 0.38 0.52 0.30 0.34 0.39 0.50 0.31 MIL [5] 0.37 0.32 0.37 0.38 0.37 0.35 0.46 0.31 0.26 0.49 0.40 0.34\nTABLE 1: We use the Princeton Tracking Benchmark [30] to compare the precision of CAT with other algorithms using results reported by [1], [25]. We obtain state of the art results in overall precision and lead the benchmark in 9 out of 11 categories.\nmodel is generated by hierarchical sampling of the tree as described in Section 4.2."}, {"heading": "5 EVALUATION", "text": "We implement the tracking system in C++ with Boost, Eigen, OpenCV, pThreads, and libFreenect libraries. We evaluate the performance of CAT using three publicly available datasets at two coreset sizes\u201410% and 25%. We denote the tracker with coreset size 10% as CAT 10, and denote the tracker with coreset size 25% as CAT 25."}, {"heading": "5.1 CVPR2013 Tracking Benchmark", "text": "We first evaluate our algorithm using the CVPR2013 Tracking Benchmark [33] which includes 50 annotated sequences. We use two evaluation metrics to compare the performance of tracking algorithms. The first metric is the precision plot, which shows the percentage of frames whose estimated location is within the given threshold distance of the ground truth. As the representative precision score for each tracker, we use the score for a threshold of 20 pixels. The second metric is the success plot. Given the tracked bounding box rt and the ground truth bounding box ra, the overlap score is defined as S = |rt\u2229ra||rt\u222ara| , where \u2229 and \u222a represent the intersection and union of two regions and | \u00b7 | denotes the number of pixels in the region. We report the success rate as the fraction of frames whose overlap S is larger than some threshold, with the threshold varying from 0 to 1. The area under curve (AUC) of the success plot is used to compare different algorithms.\nThe strength of our method lies in tracking objects from videos with several hundred frames or longer. So we compare a variety of trackers using videos that are 500 frames or longer (21 out of 50 videos) using the One-Pass Evaluation (OPE) method [33]. Figure 3 shows that we are placed second on this competitive dataset for longer videos in both metrics. We outperform important recent methods including SCM [35] and Struck [19]."}, {"heading": "5.2 Princeton Tracking Benchmark", "text": "The Princeton Tracking Benchmark [30] is a collection of 100 manually annotated RGBD tracking videos, with 214 frames on average. The videos contain significant variation in lighting, background, object size, orientation, pose, motion, and occlusion. The reported score is an average over all frames of precision, which is the ratio of overlap between the ground truth and detected bounding box around the object. We report the performance of all algorithms (including CAT) for RGB data using Princeton Tracking Benchmark evaluation server [1] and from the recent work by Liang et al [25] to ensure consistency in evaluation. CAT 25 obtains state of the art performance on this dataset, and CAT 10 ranks third (Table 1). Note that we obtain reasonable precision in presence of occlusions, even though we have no explicit way for occlusion handling. Sample detections are shown in Figure 5."}, {"heading": "5.3 TLD Dataset", "text": "The coreset formulation summarizes all the data it has seen so far. Therefore we expect the coreset appearance model\n6 Algorithm Success RateAverage Motocross Panda VW Car chase #Frames 2685 2665 3000 8576 9928 CAT 25 0.66 0.85 0.58 0.67 0.85 TLD [21] 0.57 0.86 0.25 0.67 0.81 CAT 10 0.56 0.77 0.49 0.62 0.79 Struck [19] 0.40 0.57 0.21 0.54 0.31 TGPR [15] 0.38 0.62 0.12 0.62 0.18 SCM [35] 0.36 0.50 0.13 0.47 0.14\nTABLE 2: CAT obtains state of the art performance on the TLD dataset [21]. We report the average success rate over all videos, as well as success rate for the four longest videos.\nTraining\nDetection\nDetection\nDetection\nFig. 4: Object instance detection: In a qualitative experiment, we learn the coreset appearance model by tracking an object with plain background in 76 frames of video (first row). This model can accurately detect and track the same object in different backgrounds (rows 2, 3, 4) and handle illumination changes (rows 3,4).\nto adapt to pose and lighting variations as it trains using additional data, and perform well on very long videos. To demonstrate its adaptive behavior, we use the TLD dataset [21], which contains ten videos with 2685 frames of labeled video on average. For all experiments with the TLD dataset (Tables 2, 4, 5, and 6) we report the success rate at a threshold of 0.5. Tables 2, 5, and 6 show that CAT 25 significantly outperforms the state of the art, including methods like TGPR [15], which perform better than CAT on the CVPR2013 Tracking Benchmark. CAT 10 is placed third, and it\u2019s success rate is similar to that of TLD. These results support our intuition that the ability of CAT to summarize all data provides a boost in performance over long videos."}, {"heading": "5.4 Object Instance Recognition", "text": "Since the coreset appearance model learns adaptively from the entire data and does not depend on recency as most tracking methods, it can be used for object instance recognition as well. In a qualitative experiment (Figure 4), we show that a pre-learnt model can be used to detect the same object under varying background, illumination, pose, and deformations."}, {"heading": "5.5 Empirical Evaluation of Time Complexity", "text": "The coreset formulation is used to train a learning algorithm in constant time and space (Section 3). We compare the time complexity of training with the coreset tree and training using all the data points (Figure 6-(left)). For a linear\nSVM implementation with linear time complexity [9], the learning time for training with all data points can become prohibitively large, in contrast to the constant, small time needed for learning from coresets. We pay for the reduced cost of training the learning algorithm by the additional time and space requirements of coreset tree formation. We prove in Section 3 that the time complexity of tree construction is constant on average. We validate this result empirically in Figure 6-(right). Other coreset formulations [16] may be used to reduce the space requirements to constant, at the cost of a higher value of the constant for time complexity."}, {"heading": "5.6 Comparison with Sampling Methods", "text": "In comparison to CAT, sampling can achieve reduction in space and time complexity without the additional overhead of coreset computation. We validate CAT versus random sampling and sub-sampling using the TLD dataset using multiple values of n, the coreset size. For random sampling (RS), we use n random frames from all the frames seen so far. For sub-sampling (SS), we use n uniformly spaced frames. CAT consistently outperform both RS and SS (Table 4). Thus, the summarization of data by coresets allows us to learn a more complex object appearance model as compared to sampling."}, {"heading": "5.7 Coreset Size Selection", "text": "The coreset size n determines the time and space complexity of CAT. In this paper, we evaluate the performance of CAT at coreset sizes which are 10% (CAT 10) or 25% (CAT 25) of the total video size. Using such large values for coreset size is not a strict requirement, but rather an artifact of the datasets used for evaluation. A majority of videos in the three standard datasets are short\u2014114 out of 155 videos have less than 400 frames. We model the object using dense HOG features, which leads to a very high dimensional representation. Hence, the SVM requires sufficient number of data points for good prediction on such short videos. We set n to 10% or 25% of the video size to obtain consistent performance across videos of various sizes.\nHowever, CAT maintains high detection accuracy with small coreset sizes as well. In Table 5, we evaluate CAT with various coreset sizes\u20141%, 2.5%, 5% and 10%\u2014on the TLD dataset. Increasing the coreset size results in an increase in success rate for tracking, since the model captures more information about object appearance. However, the tracking performance is still satisfactory for coreset size as small as 1% of the video size, which is equal to 27 frames on average for the TLD dataset.\nFurthermore, CAT performs well, and in fact improves with time, on sufficiently long videos with small coreset sizes that are independent of the number of frames in a video. In Table 6, we compare the performance of CAT with fixed, small coreset size with two top performing methods on extremely long videos. We generate the extremely long videos from the four largest TLD videos by appending clips that are 10% in length of the original video. The clips are randomly flipped, time-reversed, shifted in contrast and brightness, translated by small amounts, and corrupted with Gaussian noise. CAT shows an increase in accuracy with increasing number of frames. In contrast, the success rate\n7\nof TLD and TGPR reduces with an increase in number of frames. The maximum coreset size in the 100K experiment is 2.5% of the original video. Yet CAT maintains excellent tracking accuracy.\nIn summary, the impressive performance of CAT\u2014a simple tracker based on appearance models trained with small coreset sizes\u2014in these two experiments demonstrates the utility of the coreset formulation."}, {"heading": "6 DISCUSSION", "text": "Tracking in Long Videos: A majority of tracking algorithms are unable to directly make use of data from all the previous\nAlgorithm % Frames Used For Training25 37.5 50 62.5 75 Coreset Size =512\nCAT 0.408 0.449 0.483 0.534 0.582 SS 0.332 0.358 0.396 0.408 0.424 RS 0.218 0.221 0.251 0.279 0.301 Coreset Size = 768 CAT 0.502 0.559 0.604 0.637 0.668 SS 0.378 0.415 0.438 0.474 0.502 RS 0.257 0.299 0.326 0.368 0.385 Coreset Size = 1024 CAT 0.546 0.621 0.657 0.701 0.732 SS 0.418 0.492 0.54 0.587 0.633 RS 0.321 0.36 0.381 0.419 0.458\nTABLE 4: We show that CAT consistently obtains better success rate than random sampling (RS) and sub-sampling (SS) using the TLD dataset. CAT also shows a greater marginal increase in success rate with increase in n as compared to RS and SS, since it generates a better summarization of the data.\nCoreset Size Success RateMean Motocross VW Panda Carchase 1% 0.49 0.74 0.48 0.59 0.73 2.5% 0.51 0.76 0.49 0.59 0.74 5% 0.53 0.77 0.49 0.61 0.74 10% (CAT 10) 0.56 0.77 0.49 0.62 0.79 25% (CAT 25) 0.66 0.85 0.58 0.67 0.85\nTABLE 5: We evaluate CAT at several coreset sizes as the percent of video size using the TLD dataset. CAT performs satisfactorily at coreset size as small as 1% of the video size.\nframes. At best, they retain a few \u2018good\u2019 frames to learn [31] or maintain an auxiliary set of earlier frames [15]. Our method can retain a summary of all frames seen so far using logarithmic space with an efficient algorithm. This is an important advantage for practical systems with space and time constraints.\nSimplicity: The utility of the coreset formulation is evident from the fact that we obtain state of the art experimental results with a simple tracker that consists of a linear classifier and a Kalman filter. The primary goal of this work is to explore the benefit of coresets for learning from streaming visual data, rather than to compete on tracking benchmarks. Additional improvement could be obtained via choosing the optimal coreset size or exploring various hierarchical sampling methods. Performance may also improve by incorporating traditional methods for handling occlusion and drift.\nReal-time Performance: Our current implementation, running on commodity hardware (Intel i7 Processor, 8 GB RAM), performs coreset construction and SVM training in parallel at \u223c20 FPS for small coreset sizes (n \u2264 64). Due to the high computational complexity of sliding window detection, the total speed is \u223c3 FPS. Faster detection methods (e.g., efficient subwindow search [23] and parallelization [32]) can be easily incorporated into our system to achieve real-time performance."}, {"heading": "7 CONCLUDING REMARKS", "text": "In this paper, we proposed an efficient method for training object appearance models using streaming data. The key idea is to use coresets to compress object features through time to a constant size and perform learning in constant time using hierarchical sampling of coresets. Our method\n8 Algorithm Number of Frames10K 20K 50K 75K 100K CAT (n=500) 0.522 0.559 0.578 0.586 0.594 CAT (n=1000) 0.606 0.634 0.658 0.669 0.673 CAT (n=1500) 0.627 0.674 0.697 0.701 0.702 CAT (n=2000) 0.685 0.702 0.718 0.725 0.726 CAT (n=2500) 0.713 0.724 0.736 0.743 0.744\nTLD [21] 0.579 0.573 0.569 0.567 0.567 TGPR [15] 0.382 0.368 0.365 0.364 0.362\nTABLE 6: Success Rate Comparison: CAT performs well, and in fact improves with time, on very long videos with small, fixed coreset size which is independent of the total number of frames.\nis ideal for efficient tracking in long videos in real world scenarios. Since we maintain a compact summary of all previous data, our learning model improves with time. We provide experimental validation for this insight with strong performance on the TLD dataset which contains videos with 2685 frames on average. The ideas described in this paper complement the current expertise in the computer vision community on developing trackers and detectors. Combining our framework with existing methods should lead to fruitful online approaches, particularly when \u2018old\u2019 data should be taken into account. While we explore the use of coresets for object tracking in this paper, we hope that our technique can be adapted for a variety of applications, such as image collection summarization, video summarization and more."}], "references": [{"title": "Approximating extent measures of points", "author": ["P.K. Agarwal", "S. Har-Peled", "K.R. Varadarajan"], "venue": "Journal of the ACM, 51(4):606\u2013635", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Support vector tracking", "author": ["S. Avidan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(8):1064\u20131072", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Ensemble tracking", "author": ["S. Avidan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(2):261\u2013271", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Robust object tracking with online multiple instance learning", "author": ["B. Babenko", "M.-H. Yang", "S. Belongie"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(8):1619\u20131632", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Approximate clustering via core-sets", "author": ["M. B\u0101doiu", "S. Har-Peled", "P. Indyk"], "venue": "ACM Symposium on Theory of Computing, pages 250\u2013257", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Decomposable searching problems i: Staticto-dynamic transformation", "author": ["J. Bentley", "J. Saxe"], "venue": "Journal of Algorithms, 1(4):301\u2013358", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1980}, {"title": "Eigentracking: Robust matching and tracking of articulated objects using a view-based representation", "author": ["M.J. Black", "A.D. Jepson"], "venue": "European Conference on Computer Vision, pages 329\u2013342", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1996}, {"title": "LIBSVM: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology, 2(3):27", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Numerical linear algebra in the streaming model", "author": ["K.L. Clarkson", "D.P. Woodruff"], "venue": "ACM Symposium on Theory of Computing", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Kernel-based object tracking", "author": ["D. Comaniciu", "V. Ramesh", "P. Meer"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(5):564\u2013577", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "K-robots clustering of moving sensors using coresets", "author": ["D. Feldman", "S. Gil", "R. Knepper", "B. Julian", "D. Rus"], "venue": "IEEE International Conference on Robotics and Automation, pages 881\u2013888", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "A PTAS for kmeans clustering based on weak coresets", "author": ["D. Feldman", "M. Monemizadeh", "C. Sohler"], "venue": "ACM Symposium on Computational Geometry, pages 11\u201318", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Turning big data into tiny data: Constant-size coresets for k-means", "author": ["D. Feldman", "M. Schmidt", "C. Sohler"], "venue": "PCA and projective clustering. ACM-SIAM Symposium on Discrete Algorithms, pages 1434\u20131453", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Transfer learning based visual tracking with gaussian processes regression", "author": ["J. Gao", "H. Ling", "W. Hu", "J. Xing"], "venue": "European Conference on Computer Vision, pages 188\u2013203", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Relative errors for deterministic low-rank matrix approximations", "author": ["M. Ghashami", "J.M. Phillips"], "venue": "ACM-SIAM Symposium on Discrete Algorithms", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised online boosting for robust tracking", "author": ["H. Grabner", "C. Leistner", "H. Bischof"], "venue": "European Conference on Computer Vision, pages 234\u2013247", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "On coresets for k-means and kmedian clustering", "author": ["S. Har-Peled", "S. Mazumdar"], "venue": "ACM Symposium on Theory of Computing", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Struck: Structured output tracking with kernels", "author": ["S. Hare", "A. Saffari", "P.H. Torr"], "venue": "IEEE Internation Conference on Computer Vision, pages 263\u2013270", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust online appearance models for visual tracking", "author": ["A.D. Jepson", "D.J. Fleet", "T.F. El-Maraghi"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(10):1296\u20131311", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Tracking-learningdetection", "author": ["Z. Kalal", "K. Mikolajczyk", "J. Matas"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(7):1409\u20131422", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Visual tracking decomposition", "author": ["J. Kwon", "K.M. Lee"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pages 1269\u2013 1276", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Beyond sliding windows: Object localization by efficient subwindow search", "author": ["C.H. Lampert", "M.B. Blaschko", "T. Hofmann"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20138", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Real-time visual tracking using compressive sensing", "author": ["H. Li", "C. Shen", "Q. Shi"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pages 1305\u20131312", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Adaptive objectness for object tracking", "author": ["P. Liang", "C. Liao", "X. Mei", "H. Ling"], "venue": "arXiv preprint arXiv:1501.00909", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Simple and deterministic matrix sketching", "author": ["E. Liberty"], "venue": "ACM Conference on Knowledge Discovery and Data Mining", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Coresets for k-segmentation of streaming data", "author": ["G. Rosmanm", "M. Volkov", "D. Feldman", "W. Fisher", "D. Rus"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Incremental learning for robust visual tracking", "author": ["D.A. Ross", "J. Lim", "R.-S. Lin", "M.-H. Yang"], "venue": "International Journal of Computer Vision, 77(1-3):125\u2013141", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Visual tracking: An experimental survey", "author": ["A.W.M. Smeulders", "D.M. Chu", "R. Cucchiara", "S. Calderara", "A. Dehghan", "M. Shah"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Tracking revisited using RGBD camera: Unified benchmark and baselines", "author": ["S. Song", "J. Xiao"], "venue": "IEEE Internation Conference on Computer Vision, pages 233\u2013240", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Self-paced learning for long-term tracking", "author": ["J. Supancic", "D. Ramanan"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pages 2379\u20132386", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Sliding-windows for rapid object class localization: A parallel technique", "author": ["C. Wojek", "G. Dork\u00f3", "A. Schulz", "B. Schiele"], "venue": "Pattern Recognition, pages 71\u201381", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Online object tracking: A benchmark", "author": ["Y. Wu", "J. Lim", "M.-H. Yang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pages 2411\u20132418", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "Real-time compressive tracking", "author": ["K. Zhang", "L. Zhang", "M.-H. Yang"], "venue": "European Conference on Computer Vision, pages 864\u2013877", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust object tracking via sparsity-based collaborative model", "author": ["W. Zhong", "H. Lu", "M.-H. Yang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pages 1838\u20131845", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "In recent years, computational geometry researchers have proposed techniques to obtain small representative sets of points from larger sets with constant or logarithmic space-time complexity [6], [14], [16].", "startOffset": 191, "endOffset": 194}, {"referenceID": 12, "context": "In recent years, computational geometry researchers have proposed techniques to obtain small representative sets of points from larger sets with constant or logarithmic space-time complexity [6], [14], [16].", "startOffset": 196, "endOffset": 200}, {"referenceID": 14, "context": "In recent years, computational geometry researchers have proposed techniques to obtain small representative sets of points from larger sets with constant or logarithmic space-time complexity [6], [14], [16].", "startOffset": 202, "endOffset": 206}, {"referenceID": 16, "context": "Specifically, we perform summarization of a video stream based on \u201ccoresets\u201d, which are defined as a small set of points that approximately maintain the properties of the original set with respect to the distance between the set and other structures in its ambient space with theoretical guarantees [18].", "startOffset": 299, "endOffset": 303}, {"referenceID": 25, "context": "Coresets have been primarily used as a technique to convert \u201cbig data\u201d into manageable sizes [27].", "startOffset": 93, "endOffset": 97}, {"referenceID": 18, "context": "Tracking systems typically retain just an appearance model for the object [20], [35] and optionally, data from some prior frames [15], [31].", "startOffset": 74, "endOffset": 78}, {"referenceID": 33, "context": "Tracking systems typically retain just an appearance model for the object [20], [35] and optionally, data from some prior frames [15], [31].", "startOffset": 80, "endOffset": 84}, {"referenceID": 13, "context": "Tracking systems typically retain just an appearance model for the object [20], [35] and optionally, data from some prior frames [15], [31].", "startOffset": 129, "endOffset": 133}, {"referenceID": 29, "context": "Tracking systems typically retain just an appearance model for the object [20], [35] and optionally, data from some prior frames [15], [31].", "startOffset": 135, "endOffset": 139}, {"referenceID": 31, "context": "\u201d CAT obtains competitive performance on three standard datasets, the CVPR2013 Tracking Benchmark [33], the Princeton Tracking Benchmark [30], and the TLD Dataset [21].", "startOffset": 98, "endOffset": 102}, {"referenceID": 28, "context": "\u201d CAT obtains competitive performance on three standard datasets, the CVPR2013 Tracking Benchmark [33], the Princeton Tracking Benchmark [30], and the TLD Dataset [21].", "startOffset": 137, "endOffset": 141}, {"referenceID": 19, "context": "\u201d CAT obtains competitive performance on three standard datasets, the CVPR2013 Tracking Benchmark [33], the Princeton Tracking Benchmark [30], and the TLD Dataset [21].", "startOffset": 163, "endOffset": 167}, {"referenceID": 12, "context": "\u2022 We introduce the concept of hierarchical sampling from coresets, which improves learning performance significantly when compared to the original formulation [14].", "startOffset": 159, "endOffset": 163}, {"referenceID": 19, "context": "\u2022 We obtain competitive results on standard benchmarks [21], [30], [33].", "startOffset": 55, "endOffset": 59}, {"referenceID": 28, "context": "\u2022 We obtain competitive results on standard benchmarks [21], [30], [33].", "startOffset": 61, "endOffset": 65}, {"referenceID": 31, "context": "\u2022 We obtain competitive results on standard benchmarks [21], [30], [33].", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "[2] in their seminal work on k-median and k-means problems.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] propose a coreset for clustering using a subset of input points to generate the solution, while Har-Peled and Mazumdar [18] provide a different solution for coreset construction which includes points that may not be a subset of the original set.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[6] propose a coreset for clustering using a subset of input points to generate the solution, while Har-Peled and Mazumdar [18] provide a different solution for coreset construction which includes points that may not be a subset of the original set.", "startOffset": 123, "endOffset": 127}, {"referenceID": 11, "context": "[13] demonstrate that a weak coreset can be generated independent of number of points in the data or their feature dimensions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14], which provides an eminently parallelizable streaming algorithm for coreset construction with a theoretical error bound.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "This formulation has been recently used for applications in robotics [12] and for summarization of time series data [27].", "startOffset": 69, "endOffset": 73}, {"referenceID": 25, "context": "This formulation has been recently used for applications in robotics [12] and for summarization of time series data [27].", "startOffset": 116, "endOffset": 120}, {"referenceID": 27, "context": "Online Learning for Detection and Tracking: The problem of object tracking by detection has been well studied in computer vision [29].", "startOffset": 129, "endOffset": 133}, {"referenceID": 6, "context": "Early important examples of learning based methods include [8], [11], [20].", "startOffset": 59, "endOffset": 62}, {"referenceID": 9, "context": "Early important examples of learning based methods include [8], [11], [20].", "startOffset": 64, "endOffset": 68}, {"referenceID": 18, "context": "Early important examples of learning based methods include [8], [11], [20].", "startOffset": 70, "endOffset": 74}, {"referenceID": 26, "context": "[28] learn a low dimensional subspace representation of object appearance using incremental algorithms for PCA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] propose a compressive sensing tracking method using efficient l1 norm minimization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Avidan [3] combines optical flow with SVMs and performs tracking by minimizing the SVM classification score, and extends this method with an \u201censemble tracker\u201d using Adaboost [4].", "startOffset": 7, "endOffset": 10}, {"referenceID": 2, "context": "Avidan [3] combines optical flow with SVMs and performs tracking by minimizing the SVM classification score, and extends this method with an \u201censemble tracker\u201d using Adaboost [4].", "startOffset": 175, "endOffset": 178}, {"referenceID": 15, "context": "[17] develop a semi-supervised boosting framework to tackle the problem of drift due to incorrect detections.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] represent the object using multiple appearance models, obtained from sparse PCA, which are integrated using an MCMC framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[5] introduce the semi-supervised multiple instance learning (MIL) algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[19] learn an online structured output SVM classifier for adaptive tracking.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34] develop an object appearance model based on efficient feature extraction in the compressed domain and use the features discriminatively to separate the object from its background.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] introduce the TLD algorithm, where a pair of \u201cexpert\u201d learners select positive and negative examples in a semi-supervised manner for real-time tracking.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] combine results of a tracker learnt using prior knowledge from auxiliary samples and another tracker learnt with samples from recent frames.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "(1) can be rewritten in a matrix form using the Frobenius norm [14] as", "startOffset": 63, "endOffset": 67}, {"referenceID": 8, "context": "However, very few of these algorithms are designed for a streaming setting [10], [14], [16], [26].", "startOffset": 75, "endOffset": 79}, {"referenceID": 12, "context": "However, very few of these algorithms are designed for a streaming setting [10], [14], [16], [26].", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "However, very few of these algorithms are designed for a streaming setting [10], [14], [16], [26].", "startOffset": 87, "endOffset": 91}, {"referenceID": 24, "context": "However, very few of these algorithms are designed for a streaming setting [10], [14], [16], [26].", "startOffset": 93, "endOffset": 97}, {"referenceID": 12, "context": "Motivated by [14], we adopt a tree formulation to approximate the low rank subspace which best represents the data.", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "The origin of this approach goes back to Bentley and Saxe [7], and was further adopted for coresets in [2].", "startOffset": 58, "endOffset": 61}, {"referenceID": 0, "context": "The origin of this approach goes back to Bentley and Saxe [7], and was further adopted for coresets in [2].", "startOffset": 103, "endOffset": 106}, {"referenceID": 12, "context": "[14], we generate a tree structure, which holds a summary of the streaming data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14], our idea of hierarchical sampling of the entire tree allows to account for recent data, which is crucial for many learning problems which need to avoid a temporal bias in the training set, and especially for tracking by detection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "Initially, since no coreset appearance model has been trained, we need to use an alternate method for tracking (we use SCM [35] in our current implementation).", "startOffset": 123, "endOffset": 127}, {"referenceID": 33, "context": "Until the first n frames arrive we obtain the ROI from SCM [35] since no classifier has been trained.", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "44 TGPR [15] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "40 Struck [19] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "41 VTD [22] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 28, "context": "39 RGB [30] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 19, "context": "34 TLD [21] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 3, "context": "31 MIL [5] 0.", "startOffset": 7, "endOffset": 10}, {"referenceID": 28, "context": "TABLE 1: We use the Princeton Tracking Benchmark [30] to compare the precision of CAT with other algorithms using results reported by [1], [25].", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "TABLE 1: We use the Princeton Tracking Benchmark [30] to compare the precision of CAT with other algorithms using results reported by [1], [25].", "startOffset": 139, "endOffset": 143}, {"referenceID": 31, "context": "We first evaluate our algorithm using the CVPR2013 Tracking Benchmark [33] which includes 50 annotated sequences.", "startOffset": 70, "endOffset": 74}, {"referenceID": 31, "context": "So we compare a variety of trackers using videos that are 500 frames or longer (21 out of 50 videos) using the One-Pass Evaluation (OPE) method [33].", "startOffset": 144, "endOffset": 148}, {"referenceID": 33, "context": "We outperform important recent methods including SCM [35] and Struck [19].", "startOffset": 53, "endOffset": 57}, {"referenceID": 17, "context": "We outperform important recent methods including SCM [35] and Struck [19].", "startOffset": 69, "endOffset": 73}, {"referenceID": 28, "context": "The Princeton Tracking Benchmark [30] is a collection of 100 manually annotated RGBD tracking videos, with 214 frames on average.", "startOffset": 33, "endOffset": 37}, {"referenceID": 23, "context": "We report the performance of all algorithms (including CAT) for RGB data using Princeton Tracking Benchmark evaluation server [1] and from the recent work by Liang et al [25] to ensure consistency in evaluation.", "startOffset": 170, "endOffset": 174}, {"referenceID": 19, "context": "85 TLD [21] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 17, "context": "79 Struck [19] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "31 TGPR [15] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 33, "context": "18 SCM [35] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 19, "context": "TABLE 2: CAT obtains state of the art performance on the TLD dataset [21].", "startOffset": 69, "endOffset": 73}, {"referenceID": 19, "context": "To demonstrate its adaptive behavior, we use the TLD dataset [21], which contains ten videos with 2685 frames of labeled video on average.", "startOffset": 61, "endOffset": 65}, {"referenceID": 13, "context": "Tables 2, 5, and 6 show that CAT 25 significantly outperforms the state of the art, including methods like TGPR [15], which perform better than CAT on the CVPR2013 Tracking Benchmark.", "startOffset": 112, "endOffset": 116}, {"referenceID": 7, "context": "For a linear SVM implementation with linear time complexity [9], the learning time for training with all data points can become prohibitively large, in contrast to the constant, small time needed for learning from coresets.", "startOffset": 60, "endOffset": 63}, {"referenceID": 14, "context": "Other coreset formulations [16] may be used to reduce the space requirements to constant, at the cost of a higher value of the constant for time complexity.", "startOffset": 27, "endOffset": 31}, {"referenceID": 12, "context": "Algorithm Success Rate Precision Learning from root of the tree [14] 0.", "startOffset": 64, "endOffset": 68}, {"referenceID": 12, "context": "In the original formulation [14], the classifier would be trained from the root of the coreset tree alone, ignoring more recent data.", "startOffset": 28, "endOffset": 32}, {"referenceID": 29, "context": "At best, they retain a few \u2018good\u2019 frames to learn [31] or maintain an auxiliary set of earlier frames [15].", "startOffset": 50, "endOffset": 54}, {"referenceID": 13, "context": "At best, they retain a few \u2018good\u2019 frames to learn [31] or maintain an auxiliary set of earlier frames [15].", "startOffset": 102, "endOffset": 106}, {"referenceID": 21, "context": ", efficient subwindow search [23] and parallelization [32]) can be easily incorporated into our system to achieve real-time performance.", "startOffset": 29, "endOffset": 33}, {"referenceID": 30, "context": ", efficient subwindow search [23] and parallelization [32]) can be easily incorporated into our system to achieve real-time performance.", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "744 TLD [21] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "567 TGPR [15] 0.", "startOffset": 9, "endOffset": 13}], "year": 2015, "abstractText": "We propose a method for learning from streaming visual data using a compact, constant size representation of all the data that was seen until a given moment. Specifically, we construct a \u201ccoreset\u201d representation of streaming data using a parallelized algorithm, which is an approximation of a set with relation to the squared distances between this set and all other points in its ambient space. We learn an adaptive object appearance model from the coreset tree in constant time and logarithmic space and use it for object tracking by detection. Our method obtains excellent results for object tracking on three standard datasets over more than 100 videos. The ability to summarize data efficiently makes our method ideally suited for tracking in long videos in presence of space and time constraints. We demonstrate this ability by outperforming a variety of algorithms on the TLD dataset with 2685 frames on average. This coreset based learning approach can be applied for both real-time learning of small, varied data and fast learning of big data.", "creator": "LaTeX with hyperref package"}}}