{"id": "1511.06251", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Stochastic Modified Equations and Adaptive Stochastic Gradient Algorithms", "abstract": "Stochastic gradient algorithms (SGA) are increasingly popular in machine learning applications and have become \"the algorithm\" for extremely large scale problems. Although there are some convergence results, little is known about their dynamics. In this paper, We propose the method of stochastic modified equations (SME) to analyze the dynamics of the SGA. Using this technique, we can give precise characterizations for both the initial convergence speed and the eventual oscillations, at least in some special cases. Furthermore, the SME formalism allows us to characterize various speed-up techniques, such as introducing momentum, adjusting the learning rate and the mini-batch sizes. Previously, these techniques relied mostly on heuristics. Besides introducing simple examples to illustrate the SME formalism, we also apply the framework to improve the relaxed randomized Kaczmarz method for solving linear equations. The SME framework is a precise and unifying approach to understanding and improving the SGA, and has the potential to be applied to many more stochastic algorithms. As with all other SGA techniques, there are some exceptions that we can think of. For example, the stochastic model that has been discussed in the previous section uses the same mechanism for the SME formalism, but has a different approach, using different methods to represent all the different linear equations.\n\n\nThe SME formalism\nThe SME formalism is an open-source specification of SGE parameters and methods. It contains a number of principles for generating a linear equation using parameters from the SGE parameter library and using parameters from the standard RAN. In this section, we will show the principle of using the SGE parameter library, and the SME formalism.\nSGE parameters\nThis is the first approach to understand the standard RAN. For example, to generate a linear equation, we first need to generate a random variable in the sGE parameter library. In addition, to generate a random variable in the sGE parameter library, we first need to generate a normal variable in the sGE parameter library. The SGE parameter library, according to this principle, is a constant that is not constant. To create a random variable in the SGE parameter library, we first need to generate a random variable in the sGE parameter library.\nThe SGE parameter library, according to this principle, is a constant that is not constant. To create a random variable in the sGE parameter library, we first need to generate a random variable in the sGE", "histories": [["v1", "Thu, 19 Nov 2015 16:49:33 GMT  (3851kb)", "http://arxiv.org/abs/1511.06251v1", null], ["v2", "Fri, 20 Nov 2015 19:58:15 GMT  (4801kb)", "http://arxiv.org/abs/1511.06251v2", "Changes: 1. Fixed a sign mistake in eq. (74). 2. Factor of d in eq. (98), and thus figure 10's estimate of k^*. 3. Fixed some typos and figure scales"], ["v3", "Tue, 20 Jun 2017 13:56:33 GMT  (1326kb,D)", "http://arxiv.org/abs/1511.06251v3", "Major changes including a proof of the weak approximation, asymptotic expansions and application-oriented adaptive algorithms"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["qianxiao li", "cheng tai", "weinan e"], "accepted": true, "id": "1511.06251"}, "pdf": {"name": "1511.06251.pdf", "metadata": {"source": "CRF", "title": "Dynamics of Stochastic Gradient Algorithms", "authors": ["Qianxiao Li"], "emails": ["qianxiao@math.princeton.edu", "chengt@math.princeton.edu", "weinan@math.princeton.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n06 25\n1v 1\nKeywords: stochastic gradient algorithms, stochastic modified equations, dynamics, optimal control"}, {"heading": "1. Introduction", "text": "Stochastic Gradient algorithms (SGA) are often used to solve optimization problems of the form\nmin x\u2208Rd\nf(x) := 1\nn\nn \u2211\ni=1\nfi(x), (1)\nwhere n is typically large. The basic form of SGA iteration is the following:\nxk+1 = xk \u2212 \u03b7\u2207f\u03b3k(xk), (2)\nwhere {\u03b3k} are i.i.d random variables uniformly distributed over {1, 2, \u00b7 \u00b7 \u00b7 , n}. Compared with the deterministic gradient descent algorithms (GD), the SGA requires only one evaluation of the gradients at each time step. This makes it appealing when gradient evaluations are expensive. SGA algorithms have been increasingly popular in machine learning applications and have become \u201cthe algorithm\u201d for extremely large scale problems. Due to the stochastic nature of the algorithm, its behavior is different from deterministic algorithms in several aspects. Figure 1 shows a typical application of GD and SGA. We see that the noisy dynamics of the SGA is significantly different from that of the GD algorithm. Note that the computational cost for the GD is n times that of the SGA. Hence, to compare the two, we hereafter define an iteration of the SGA to be n steps of (2).\nAlthough there have been some convergence results of the SGA algorithms and their variants (Shamir and Zhang, 2012; Moulines and Bach, 2011; Needell et al., 2014; Xiao and Zhang, 2014; Shalev-Shwartz and Zhang, 2014; Bach and Moulines, 2013), less is known about their dynamics. For example, the convergence rate gives an error bound for the asymptotic behavior of the algorithms, but for practitioners, the initial convergence speed is equally important, if not more. We need more precise characterizations of the dynamics of the algorithms other than asymptotic error bounds. In this paper, we make an attempt in this direction. In particular, we are interested in the following questions:\n1. How to characterize the dynamics of SGA algorithms?\n2. It is often observed that SGA is much faster than GD at the beginning, but starts oscillating at some point and eventually GD has higher precision. How to quantify the initial acceleration? When does the acceleration break down? How to characterize the oscillation?\n3. Momentum is often used in conjunction with the SGA algorithm, how does this affect the dynamics?\n4. In practice, choosing the learning rate schedule has been a delicate issue. How does learning rate affect the solution? And how do we choose the optimal learning rate schedule?\n5. Often, mini-batch is also used. How does mini-batch sizes affect the solution? And if we are allowed to change batch-size during the iteration, what is the optimal batch-size schedule?\nThese questions are difficult in the general case, but they are of both theoretical and practical importance. In the following, we develop a tool that can be used to provide answers to the above questions, at least in some simple cases. The key idea is to approximate the dynamics of SGA iterations by a suitable modified stochastic differential equation. This is inspired by the classical method of modified equations.\nThe method of modified equations is a means of analyzing finite difference schemes. Its origin may be attributed to Hirt (1968); Noh and Protter (1960); Daly (1963); Warming and Hyett (1974). The early applications was the analysis of the consistency and stability of numerical methods for partial differential equations, see Hirt (1968); Warming and Hyett (1974). The method of modified equations consists of creating a differential equation which approximates the given numerical scheme more accurately than the original equation. So the numerical scheme can be analyzed by studying the modified equations.\nWhen using the methodology of modified equations to study SGA, we are faced with an important problem: unlike classical numerical schemes for solving differential equations, the SGA involve random sampling and classical modified equations cannot deal with such stochasticity. To address this issue, we introduce the method stochastic modified equations. Instead of associating a differential equation with the numerical scheme, we associate with it a stochastic differential equation. More concretely, the SGA iteration is modeled as a numerical scheme for solving an SDE of the diffusion type. The SDE is then used to analyze the properties of the the numerical scheme, in a similar spirit as the method of modified equations. We call this technique the method of stochastic modified equations (SME).\nThe flavor of results given by the SME analysis is very different from those given by convergence analysis. Instead of giving error bounds, the SME gives the precise dynamics. For simple examples, the initial convergence rate can be computed, thus we know not only that the rate is linear but also exactly how many times faster the SGA is than GD. It also gives the transition point when this acceleration ceases to persist and the distribution of the eventual oscillation. Even though the SME is an approximation method, the results are often surprisingly accurate.\nMoreover, with the SME, we can go further to study various speed-up techniques for the SGA. For example, the problem of selecting the optimal learning rate schedule can be readily formulated as a control problem. The associated Hamilton-Jacobi-Bellman equation\ncan be solved exactly in some special interesting cases. Such exact calculations can provide important and new insights to designing optimized speed-up procedures in practice.\nAs an application of the SME, we consider solving linear equations using stochastic gradient algorithms. Using SGA to solve linear equations can be seen as a relaxation of the well known randomized Kaczmarz algorithm. For some recent reference, see e.g. Strohmer and Vershynin (2009); Needell et al. (2014). Introducing the relaxation, the Kaczmarz iteration rule is\nxk+1 = xk + \u03b1k b\u03b3k \u2212 \u3008a\u03b3 k , xk\u3009\n\u2016a\u03b3k\u20162 a \u03b3k . (3)\nWe study this problem from the SME perspective. Both the initial exponential convergence rate and the eventual oscillation can be computed. We compute the optimal learning rate schedule and the optimal batch-size schedule given a fixed computational budget. Numerical results are given to demonstrate the improvements.\nWe stress here that the goal of this paper is to introduce the SME framework to analyze the dynamics of the SGA. Where possible, we also perform exact calculations so as to elucidate various features of the SGA and provide guiding principles to optimizing the SGA in general. A rigorous treatment of some results, such as the sense of which the SME approximates the SGA, is not within the scope of the current work. However, we also emphasize that such statements can be made rigorous.\nThe rest of the paper is organized as follows. In section 2, we introduce the SME technique, followed by two concrete examples. In section 3, we use the SME approach to characterize various speed-up techniques for the SGA. An application of the framework to improving the relaxed randomized Karczmarz method is discussed in section 4. Lastly, a discussion and conclusion is given in section 5."}, {"heading": "2. Stochastic modified equations", "text": "In this section we introduce the concept of the SME, followed by two concrete examples to illustrate the methodology. As discussed before, the main purpose of this paper is to introduce this technique and how we can use it to gain some understandings of the stochastic gradient methods. Therefore, we sacrificed some mathematical rigor in the derivation. The exact statement such as in what sense the SME approximates the original SGA will not be dealt here. Instead, we use some numerical examples to justify the SME approximation.\nThe motivation behind the SME is the following. Recall that at each step of the SGA iterations, a function f\u03b3k is picked randomly and uniformly from the n functions. As Ef\u03b3k = f , f\u03b3k is an unbiased estimator of f . Hence\nExk+1 = xk \u2212 \u03b7\u2207f(xk). (4)\nThis can be seen as the forward Euler scheme for solving the differential equation:\nx\u2032(t) = \u2212\u2207f(x) (5)\nwith step size \u2206t = \u03b7. Under mild conditions on f , the scheme converges for any time interval [0, T ] as \u03b7 \u2192 0. Hence the behavior of the expectation of SGA iterations is well characterized by (5).\nBut this approximation does not capture the variance of the iterates, hence can only seen as a first order approximation. If the variance does not converge, the actual implementations would still fail even if the expectation converges. Therefore, it is important to approximate the variance as well.\nRewrite the SGA iteration:\nxk+1 \u2212 xk = \u2212\u03b7\u2207f(xk) + \u03b7(\u2207f(xk)\u2212\u2207f\u03b3k(xk)). (6)\n\u2207f(xk) is the deterministic drift term, and \u2207f(xk)\u2212\u2207f\u03b3k(xk) is the noise term with mean 0. Intuitively, in the small \u03b7 limit, the accumulation of the noise term over several iterations over a fixed small time interval will converge in distribution to a random variable with mean 0 and finite covariance. As a second order approximation, we further approximate this noise term by a normal variate whose covariance matches the empirical covariance, which is:\n\u03a3(xk) := cov(\u2207f\u03b3k(xk))\n= E\n[\n1\nn\n\u2211\ni\n(\u2207fi(xk)\u2212\u2207f(xk))(\u2207fi(xk)\u2212\u2207f(xk))T ] . (7)\nThe expectation is taken over the distribution of {\u03b3k}. We could continue to approximate the higher order terms, but as it turns out that a second order approximation is sufficient to characterize the dynamics of the SGA iterations. For small \u03b7, the SGA iterations behave like a diffusion process under a potential f . It is naturally linked to the following N -dimensional SDE:\ndX(t) = \u2212\u2207f(X(t))dt+ \u03c3(X(t))dB(t). (8)\nThe usual Euler-Maruyama scheme for solving this SDE is\nxk+1 = xk \u2212\u2206t\u2207f(xk) + \u221a \u2206t\u03c3(xk)zk, (9)\nwhere the zks are independent d-dimensional N (0, I) random variables and \u03c3(x)\u03c3T (x) is the covariance matrix. The SGA iteration with learning rate \u03b7 can be thought of as the Eular-Maruyama scheme for solving (8). Comparing the drift and volatility terms in (6) and (9), we see that\n{\n\u2206t = \u03b7 \u03c3(x)\u03c3(x)T = \u03b7\u03a3(x). (10)\nTherefore, for small \u03b7, the SGA iterations are approximated by the following SDE:\ndX(t) = \u2212\u2207f(X(t))dt+\u221a\u03b7\u03c3(X(t))dB(t), \u03c3(x)\u03c3(x)T = \u03a3(x). (11)\nWe call this equation the stochastic modified equation (SME).\nNow we study two examples to concretely illustrate the technique. Simple as they are, they can provide us with many insights, which helps us understand more complex problems."}, {"heading": "2.1 One dimensional example", "text": "Let f1(x) = (x\u2212 a)2 and f2(x) = (x\u2212 b)2 where x, a, b \u2208 R. We want to minimize\nf(x) = 1\n2 f1(x) +\n1 2 f2(x). (12)\nLet \u03b1 = 12(a+ b) be the point where minimum is obtained. The GD step for solving this problem is\nxk+1 = xk \u2212 \u03b7\u2207f(xk), (13)\nand we have xk+1 \u2212 \u03b1 = (1\u2212 2\u03b7)(xk \u2212 \u03b1) = (1\u2212 2\u03b7)k(x0 \u2212 \u03b1), (14)\ni.e., the convergence is linear. On the other hand, the SGA step is\nxk+1 = xk \u2212 \u03b7\u2207f\u03b3k(xk), (15)\nwhere {\u03b3k} are i.i.d random variable that takes value 1 and 2 with probability 1/2. Figure 2(a) shows the GD and SGA iterations for a = \u22121, b = 1, \u03b7 = 0.005. There are some\ninteresting features in this figure. First, SGA path approaches to 0 twice as fast as GD. When it is close to 0, it begins to oscillate. The amplitude of oscillation is related to \u03b7. Let N be the total number of SGA iterations. We now make these observations precise. The SME for this problem is:\ndX = \u22122(X \u2212 \u03b1)dt+ 2\u221a\u03b7dB, X(0) = x0 (16)\nfrom [0, T ] where T = \u03b7N is the final time. Figure 2(b) shows the SME approximation for one sample path. Both the initial convergence and eventual oscillation are well captured.\nThe SDE is the mean-reverting Ornstein-Uhlenbeck process and the exact solution is given by\nX(t) = \u03b1+ (x0 \u2212 \u03b1)e\u22122t + 2 \u221a \u03b7 \u222b t\n0 e2(s\u2212t)dB(s). (17)\nA lot of information about the dynamics about the SGA iterations can be read out from this solution.\nAcceleration. Using Ito\u0300\u2019s isometry, we have\n{\nE[X(t)|X(0) = x0] = \u03b1+ (x0 \u2212 \u03b1)e\u22122t Var[X(t)|X(0) = x0] = \u03b7(1 \u2212 e\u22124t).\n(18)\nWe see that initially, the variance is small, thus the behavior of SGA resembles GD and has the same convergence rate as GD. However, as the SGA requires only one evaluation of the gradient at each iteration, it is exactly twice as fast as GD. The evolution of the distribution of X(t) is characterized by the Fokker-Planck equation:\n{\n\u2202p(x,t) \u2202t = \u2202(2(x\u2212\u03b1)p(x,t)) \u2202x + 2\u03b7 \u2202p(x,t) \u2202x2 p(x, 0) = \u03b4(x\u2212 x0). (19)\nThe solution is\np(x, t|x0, 0) = \u221a\n1 2\u03c0\u03b7(1 \u2212 e\u22124t) exp ( \u2212(x\u2212 \u03b1\u2212 (x\u2212 \u03b1)e \u22122tx0) 2 2\u03b7(1 \u2212 e\u22124t) ) . (20)\nThus, X(t) \u223c N (\u03b1 + (x0 \u2212 \u03b1)e\u22122t, \u03b7(1 \u2212 e\u22124t)). Let t \u2192 \u221e, the distribution converges to N (\u03b1, \u03b7).\nBreak down. The acceleration breaks down when\nx0e \u22122t \u2248 \u221a\u03b7. (21)\nThis happens around\nk \u2248 1 4\u03b7 log\n(\nx20 \u03b7\n)\n(22)\nSGA iterations.\nLearning rate. We see that learning rate \u03b7 enters in two ways. The initial convergence speed, and the stationary distribution. Doubling the learning rate implies twice speedup in the beginning (if converges) and twice the variance of stationary distribution.\nTo summarize what we learned from this example: the SGA iteration converges linearly and is initially twice as fast as GD, after k \u2248 14\u03b7 log(x20/\u03b7) steps, the acceleration breaks down. Eventually, it converges to N (\u03b1, \u03b7) in distribution. This example conveys the basic idea of using SME to analyze SGA dynamics. And as we will see, the technique can be useful in more complicated problems as well."}, {"heading": "2.2 Multi-dimensional example", "text": "Our second example is a multi-dimensional problem. Consider minimizing\nf(x) = 1\n2n\nn \u2211\ni=1\n(\u03bbi(xi + 1) 2 + \u03bbi(xi \u2212 1)2), x \u2208 Rd. (23)\nMinimizing this function is equivalent to solving a particular inconsistent linear system Ax = b where \u03bbi represent the eigenvalues of A\nTA. For this objective function, we have {\n(\u2207f(x))i = 2\u03bbixin , (\u03c3\u03c3T )ij = 4\u03bb2i (x 2 i+1) n \u03b4ij \u2212 4\u03bbi\u03bbjxixjn2 .\n(24)\nUsing the technique introduced before, we get the following SME:\ndXi(t) = \u2212 2\u03bbi\nn Xi(t)dt+\n\u221a \u03b7\u03c3(X(t))dBi(t), i = 1, \u00b7 \u00b7 \u00b7 , n. (25)\nObtaining the exact solution to (25) is complicated. But note that due to the linearity of the drift term, for small \u03b7 the SME is close to a Gaussian process. Hence, we can obtain a lot of information about the dynamics from studying the moment equations. Taking expectations on the equations, we have\ndEXi(t) = \u2212 2\u03bbi\nn Xi(t)dt, i = 1, \u00b7 \u00b7 \u00b7 , n. (26)\nThe solution is\nEXi(t) = xi exp(\u2212 2\u03bbit\nn ). (27)\nThis is the same as the deterministic case with gradient descent. We see the expectation converges linearly. Yet, more important is the second moment because the variance could still explode even if the expectation converges. Taking derivative of X2(t) and taking expectation, we have\ndEX2i (t) = (\u2212 4\u03bbi n + 4\u03b7\u03bb2i n \u2212 4\u03b7\u03bb 2 i n2 )EXi(t) 2dt+ 4\u03b7\u03bb2i n dt (28)\nThe solution is\nEX2i (t) = \u03bbi\u03b7\n1\u2212 n\u22121 n \u03bbi\u03b7 + (x2i \u2212\n\u03bbi\u03b7\n1\u2212 n\u22121 n \u03bbi\u03b7 )e\u2212 4t\u03bbi n e\n4t\u03b7\u03bb2i (n\u22121)\nn2 . (29)\nWe immediately see that for small \u03b7, the convergence rate is linear in the beginning, but is slower than the deterministic case with GD. The variance of the gradients introduces a positive exponent that slows down the convergence. To ensure overall convergence, we should have \u03b7 < n(n\u22121)\u03bbi for i = 1, \u00b7 \u00b7 \u00b7 , n.\nThe variance of the eventual oscillation is \u03bbi\u03b7 1\u2212n\u22121\nn \u03bbi\u03b7\n. If \u03b7 is small, then\nE(X2i ) \u2217 \u2248 \u03bbi\u03b7. (30)\nThe overall second moment is E\u2016X(t)\u201622 = \u2211 i EXi(t) 2. The transition from exponential convergence to oscillation happens when\n\u2211\ni\n\u03bbi\u03b7\n1\u2212 n\u22121 n \u03bbi\u03b7 \u2248\n\u2211\ni\n(x2i \u2212 \u03bbi\u03b7\n1\u2212 n\u22121 n \u03bbi\u03b7 )e\u2212 4t\u03bbi n e\n4t\u03b7\u03bb2i (n\u22121)\nn2 . (31)\nAs a sanity check, we run the SGA algorithm for sufficient time to demonstrate the above calculations. We take \u03bb to be evenly distributed from 0 to 100 in log scale. Figure 3 shows one sample path. The eventual variance is computed for some different learning rates, as shown in Table 1. The transition point is marked as red dot in Figure 3(a).\nWe see that the initial convergence rate and the eventual variance are both characterized very well with the SME approximation, and gets better for smaller learning rates.\nFrom the expression of the moments, we see that the overall convergence speed is controlled by \u03bbmin, and the learning rate is limited by the largest \u03bbmax. Let s =\n\u03bbmin \u03bbmax\nbe the reciprocal of the condition number, then this corresponds to a convergence rate of O (\nexp(\u22124sk n ) ) in the discrete case."}, {"heading": "3. Acceleration techniques.", "text": "We saw in the previous section that the SME provides a precise characterization of the dynamics of the SGA when the learning rate is sufficiently small. In this section, we show how the SME can also be useful for analyzing and improving speed-up techniques of the classical SGA. We will discuss three of these in detail, namely adding momentum, adjusting the learning rates and implementing mini-batch."}, {"heading": "3.1 SGA with momentum", "text": "The classical momentum method is a technique for accelerating gradient descent. For quadratic functions, it can be shown that the momentum reduces the number of iterations from O(\u03ba) to O(\u221a\u03ba) where \u03ba is the condition number of the problem. More recently, the momentum has been used in conjunction with SGA in large scale machine learning problems such as training deep neural networks. It has been empirically observed that SGA with momentum accelerates convergence, see for example, Sutskever et al. (2013). We study the effects of momentum using the SME approach.\nThe usual SGA with momentum proceeds as follows. Set x0 = x0, v 0 = v0, then iterate\nthe following two steps:\n{\nvk+1 = \u00b5vk \u2212 \u03b7\u2207f\u03b3k(xk) xk+1 = xk + vk+1,\n(32)\nfor k = 0, 1, 2, \u00b7 \u00b7 \u00b7 , where {\u03b3k} are i.i.d random variables uniform over {1, \u00b7 \u00b7 \u00b7 , n}. Using the SME technique as introduced in the previous section, we arrive at the following SDE:\n{\ndX = 1 \u03b7 V dt dV = (\u00b5\u22121 \u03b7 V \u2212\u2207f(X)dt+\u221a\u03b7\u03c3(X,V )dB (33)\nUnlike the previous case, the SDE above involves two variables and can be recast as a second order differential equation for X.\nFigure 4(a) shows a sample path of the SGA with momentum and the SDE approximation for solving an inconsistent linear equation. We see a good agreement in the initial descent as well as the size of eventual fluctuations.\nFor general f , we may not be able to get a closed form solution. But we can always analyze the moment equations, as long as they are closed, to get useful information. Taking expectations on the equations we see that the first order moments corresponds to the deterministic gradient descent algorithm with momentum. The second moment equations are as follows:\n\n \n \ndEXiXj = ( 1 \u03b7 EXiVj + 1 \u03b7 EXjVi)dt dEXiVj = ( \u00b5\u22121 \u03b7\nEXiVj \u2212 EXi\u2207f(X)j + 1\u03b7EViVj)dt dEViVj = ( 2(\u00b5\u22121) \u03b7 EViVj \u2212 EVi\u2207f(X)j \u2212 EVj\u2207f(Xt)i + \u03b7(\u03c3\u03c3T )ij)dt (34)\nfor i, j \u2208 {1, \u00b7 \u00b7 \u00b7 , d}. To give a concrete example, let f be the one defined in (23). Among the moment equations, it is EX2i that we care most about as it is directly related to the convergence. The relevant equations are :\n\n  \n  \ndEX2i dt = 2 \u03b7 EXiVi dEXiVi dt = \u22122\u03bbi n EX2i + \u00b5\u22121 \u03b7 EXiVi + 1 \u03b7 EV 2i dEV 2i dt = 4\u03b7\u03bb2i (n\u22121) n2 EX2i \u2212 4\u03bbin EXiVi + 2(\u00b5\u22121) \u03b7 EV 2i dt,\n(35)\nfor i = 1, 2, \u00b7 \u00b7 \u00b7 , d. First, we analyze the asymptotic behavior of the solution. Setting the right hand side of (35) to zero, we get the stationary solution: \n  \n  \nE(X2i ) \u2217 = n\u03b7\u03bbi\nn\u2212(n\u22121)\u03b7\u03bbi\u2212n\u00b5\nE(XiVi) \u2217 = 0 E(V 2i ) \u2217 = 2\u03b72\u03bb2i n\u2212(n\u22121)\u03b7\u03bbi\u2212n\u00b5\n(36)\nfor i = 1, \u00b7 \u00b7 \u00b7 , d. This solution tells us that the eventual oscillation of X is amplified compared with the plain SGA. In particular, if \u03b7 is small,\nE(X2i ) \u2217 \u2248 \u03b7\u03bbi\n1\u2212 \u00b5. (37)\nCompared with the plain SGA in (30), we see that the eventual variance is amplified by a factor of 1/(1 \u2212 \u00b5).\nNext, we analyze the convergence speed. To do this, we need to compute the eigenvalues of the coefficient matrix of (35), which is\nCi =\n\n \n0 2 \u03b7\n0 \u22122\u03bbi n \u00b5\u22121 \u03b7 1 \u03b7\n4\u03b7\u03bb2i (n\u22121) n2 \u22124\u03bbi n 2(\u00b5\u22121) \u03b7\n\n  , i = 1, \u00b7 \u00b7 \u00b7 , d. (38)\nThe characteristic polynomial for Ci is cubic in \u03be, its roots are complicated. Note that 4\u03b7\u03bb2i (n\u22121)\nn2 is small compared with the other entries when \u03b7 is small, hence we perform a\nperturbation analysis by considering the following matrix\nCi(\u03b4) =\n\n \n0 2 \u03b7\n0 \u22122\u03bbi n \u00b5\u22121 \u03b7 1 \u03b7\n\u03b4 \u22124\u03bbi n 2(\u00b5\u22121) \u03b7\n\n  . (39)\nThe eigenvalues of Ci(\u03b4) can be write as a series of powers of \u03b4:\n\u03be(\u03b4) = c0 + c1\u03b4 + c2\u03b4 2 + \u00b7 \u00b7 \u00b7 . (40)\nFor small \u03b7 , Ci(0) is already a good approximation. The characteristic polynomial of Ci(0) is\np(\u03be, \u03bbi) = \u2212 1\nn\u03b72 (1\u2212 \u00b5+ \u03b7\u03be)(8\u03bbi + 2n\u03be \u2212 2n\u00b5\u03be + n\u03b7\u03be2). (41)\nLet \u03bei,0(\u03bbi), \u03bei,1(\u03bbi), \u03bei,2(\u03bbi) be the roots of p(\u03be, \u03bbi). For distinct roots, the solution of (35) has the form\nci,0e t\u03bei,0 + ci,1e t\u03bei,1 + ci,2e t\u03bei,2 . (42)\nThe convergence is controlled by the real part of \u03be. For given \u03b7, we can choose \u00b5 to ensure fast overall convergence. The optimal \u00b5\u2217 is given by\nmin \u03b8,\u00b5 \u03b8\nsubject to \u03b8 \u2265 |e\u03bei,0(\u03bbi)|, |e\u03bei,1(\u03bbi)|, |e\u03bei,2(\u03bbi)| \u03bei,0(\u03bbi), \u03bei,1(\u03bbi) and \u03bei,2(\u03bbi) are roots of p(\u03be, \u03bbi).\n(43)\nLet \u03b4 be the smallest among all \u03bbi, then it can be verified that\n\u00b5\u2217 = \u221a n\u2212 2\u221a2\u03b7\u03b4\u221a\nn and \u03b8\u2217 = exp(\u22122 \u221a 2\u03b4\u221a n\u03b7 ), (44)\nwhere \u03b4 = \u03bbmin. One can proceed to compute the next order approximation, and doing so introduces an o(\u03b7) modification to (44). Hence the above \u00b5\u2217 is sufficient for analyzing Ci.\nTherefore, for small \u03b7, the above overall convergence rate translates into\nO ( (1 + k\u03b7 + k2\u03b72) exp(\u22122 \u221a 2\u03b4\u03b7\u221a n k) )\n(45)\nfor the discrete SGA iterations with momentum. We see that for fixed learning rate \u03b7, whereas the convergence rate of the SGA depends linearly on \u03bbmin, adding a momentum reduces the dependence to the square root of \u03bbmin. Figure 4(b) illustrates the usage of momentum in example with \u03b7 = 1e \u2212 5, \u00b5 = 0.9. The predicted variance is 0.047 and the actual variance is 0.047. Some other configurations are listed in Table 2.\nWe see an acceleration of momentum from Figure 4(b). On the other hand, if we use SGA with a learning rate \u03b71\u2212\u00b5\u2217 , the convergence rate is also of order O(exp(\u2212 \u221a 2\u03b4\u03b7\u221a n k)) provided it converges. And the eventual variances are also the same. Hence using momentum with parameter \u00b5\u2217 is effectively equivalent to using the plain SGA with learning rate multiplied by 11\u2212\u00b5\u2217 ."}, {"heading": "3.2 Optimal learning rate", "text": "Recall that when the SGA is applied to minimize a generic function, one often observes good performance at early times. However, after a number of iterations the objective function begins to fluctuate and fails to decrease further. We called this point the transition point of the SGA. In practice, one common way to avoid hitting this point is to gradually decrease the learning rate. The optimal learning rate problem can be phrased as follows: given a fixed running time, how does one choose a learning rate schedule so as to minimize the error at the end of the run?"}, {"heading": "3.2.1 Optimal control setup", "text": "We now apply the SME formalism to cast the above into an optimal control problem. The iteration step for the SGA with adjusted learning rates can be written as\nxk+1 = xk \u2212 \u03b7uk\u2207fk ( xk ) , (46)\nwhere uk \u2208 [0, 1] represents the factor by which the learning rate at the current step is reduced. The corresponding SME is\n{ dX (t) = \u2212u (t)\u2207f (X (t)) dt+\u221a\u03b7u (t)\u03c3 (X (t)) dB (t) , X (0) = x0\n(47)\nfor 0 \u2264 t \u2264 T = \u03b7N . As before, we have the restriction u (t) \u2208 [0, 1] for all t \u2208 [0, T ]. At the end of the run, the squared error is \u2016X (T )\u2212 x\u2217\u20162, which we would like to minimize. Hence, the optimal learning rate problem can be cast as a stochastic control problem:\nmin u\u2208U\nE \u2016X (T )\u2212 x\u2217\u20162 subject to (47), (48)\nwhere U contains processes u which are adapted to X and satisfies u (t) \u2208 [0, 1] for t \u2208 [0, T ]. In general, solving the above stochastic control problem requires the solution of the associated Hamilton-Jacobi-Bellman (HJB) equation, which is a second order partial differential equation. Even for simple examples in one dimension, this can be rather involved.\nWe shall adopt an alternative route that is analytically tractable and still reveals key features of the optimal learning rate problem. Let us denote\nm (t) := E \u2016X (t)\u2212 x\u2217\u20162 , (49)\nand assume that we can derive from (47) a closed moment equation1\n{\nm\u0307 (t) = F (u (t) ,m (t)) , m (0) = x20 (50)\nNow, instead of (48), we can consider the optimal control problem\nmin u\u2208U m (T ) subject to (50), (51)\nwhere U contains processes u which satisfy u (t) \u2208 [0, 1] but are no longer dependent on individual sample paths X. When \u03b7 is small, however, the fluctuations around expected values are small and hence we expect the problem (51) to closely approximate the problem (48). Now, solving (51) for simple examples is tractable as they involve only first order PDEs.\nFor concreteness, let us again consider the example d = 1, n = 2 and\nf (x) = 1\n2 (f1 (x) + f2 (x)) =\n1\n2\n( (x\u2212 1)2 + (x+ 1)2 ) . (52)\nThe modified equation with controlled learning rate is\ndX (t) = \u22122u (t)X (t) dt+ 2\u221a\u03b7u (t) dB (t) , (53)\nand the moment equation is\nm\u0307 (t) = F (u (t) ,m (t)) := 4 ( \u03b7u (t) 2 \u2212 u (t)m (t) ) . (54)\nRecall that m (t) = EX2, since x\u2217 = 0. We now solve the optimal control problem (51) exactly. For 0 \u2264 t \u2264 T and m \u2265 0, Define the value function\nV (m, t) := min u\u2208U\n{m (T ) |m\u0307 (t) = F (u (t) ,m (t)) ,m (t) = m} . (55)\n1. This can always be done if f is quadratic, e.g. f (x) = \u2016Ax\u2212 b\u20162.\nThen, the solution of (51) is simply V ( x20, 0 )\n. By the dynamic programming principle, V satisfies the following HJB equation\n\n \n \nVt + min u\u2208[0,1] {F (u,m)Vm} = 0, V (m,T ) = m, V (0, t) = 0.\n(56)\nThe optimal control is given by\nu\u2217 = argmin u {F (u,m)Vm} (57)"}, {"heading": "3.2.2 Exact Solution of the HJB", "text": "First, we perform the minimization over u:\nmin u\u2208[0,1]\n{F (u,m)Vm} =\n\n \n \n\u2212m2 \u03b7 Vm m \u2264 2\u03b7, Vm \u2265 0, 4(\u03b7 \u2212m)Vm m > 2\u03b7, Vm \u2265 0 or m \u2264 \u03b7, Vm < 0, 0 m > \u03b7, Vm < 0.\n(58)\nThe minimizing control is\nu\u2217 =\n\n \n  m 2\u03b7 m \u2264 2\u03b7, Vm \u2265 0, 1 m > 2\u03b7, Vm \u2265 0 or m \u2264 \u03b7, Vm < 0, 0 m > \u03b7, Vm < 0.\n(59)\nSubstituting (58) back into (56) yields a Hamilton-Jacobi equation that can be easily solved by method of characteristics. The solution is\nV (m, t) =\n\n  \n  \nm\u03b7 \u03b7+m(T\u2212t) m \u2264 2\u03b7, 2\u03b7 1+2(T\u2212t\u2217(m)\u2212t) m > 2\u03b7, 0 \u2264 t < T \u2212 t\u2217 (m) , \u03b7 + (m\u2212 \u03b7) e\u22124(T\u2212t) m > 2\u03b7, T \u2212 t\u2217 (m) \u2264 t \u2264 T,\n(60)\nwhere\nt\u2217 (m) = 1\n4 log\n(\nm \u03b7 \u2212 1\n)\n. (61)\nObserve that Vm \u2265 0 for all m \u2265 0 and 0 \u2264 t \u2264 T , thus the corresponding optimal control can be found by first solving for the optimally controlled process m\u2217 in (54) with u = u\u2217 given by (59), and then substituting the result back into (59). We have\nm\u2217 (t) =\n\n  \n  \n\u03b7x20 \u03b7+x20t\nx0 \u2264 \u221a 2\u03b7,\n\u03b7 + ( x20 \u2212 \u03b7 ) e\u22124t x0 > \u221a 2\u03b7, 0 \u2264 t < t\u2217 ( x20 )\n, 2\u03b7\n1+2(t\u2212t\u2217(x20)) x0 >\n\u221a 2\u03b7, t\u2217 (\nx20 )\n\u2264 t \u2264 T, (62)\nNotice that by identifying t with k\u03b7, the optimal control derived above can be directly translated into a learning rate schedule in the SGA iteration. Let x0 > \u221a 2\u03b7, which represents the case where the initial guess is far from the minimum. The optimal control tells us that in order to minimize the error at the end of the run, we use the original learning rate until time k\u2217 = t\u2217/\u03b7, after which we decrease the learning rate by a factor of uk = 1/ (1 + 2\u03b7 (k \u2212 k\u2217)) (see Figure 5, where we plot the optimally controlled error process m\u2217 and the optimal control u\u2217 for \u03b7 = 0.05, x0 = 1, N = 50). Notice that k\n\u2217 is consistent with the transition time found in (22). The fact that we do not decrease the learning rate before the transition time is precisely because for k \u2264 k\u2217, the drift of the SDE dominates the volatility and the classical SGA is very efficient. It is only after k\u2217 when the error starts to fluctuate that we have to decrease the learning rate. The optimal control then shows that the best way to do so is to decrease it like 1/ (1 + 2\u03b7 (k \u2212 k\u2217)).\nWe see that for this example, the SME approach allows us to derive a precise optimal strategy for decreasing the learning rate. Despite its simplicity, the calculation reveals some key insights. In the literature, a learning rate schedule of O (1/k) is usually proposed to ensure convergence (Shamir and Zhang, 2012). Here, we showed that this scaling is asymptotically correct, but for optimal performance, we should only apply it after the transition time. Before then, we should use a constant learning rate. These insights can be used to optimize more general minimization problems such as the case of f = \u2016Ax\u2212 b\u20162, which we will discuss in Section 4."}, {"heading": "3.3 Optimal mini-batch size", "text": "Another approach to optimize the SGA is by applying mini-batch, where at each iteration we use more than one sample of the gradient \u2207fk. This is essentially a variance reduction technique, where a larger batch size results in a lower variance. However, increasing the batch size also incurs computational overheads. We will employ the stochastic modified equation to find a balance between the two."}, {"heading": "3.3.1 Optimal control setup", "text": "The SGA iteration with mini-batching can be written as\nxk+1 = xk \u2212 \u03b7 1 1 + uk\n1+uk \u2211\nj=1\n\u2207f\u03b3j ( xk ) , (64)\nwhere uk is a non-negative integer and hence 1+uk is the batch size at the current iteration. The corresponding stochastic modified equation is\ndX (t) = \u2212\u2207f (X (t)) dt+ \u221a\n\u03b7\n1 + u (t) \u03c3 (X (t)) dB (t) , (65)\nwhere u (t) \u2265 0 for all t \u2208 [0, T ]. To perform exact calculations, we again consider the one dimensional example (52), for which the modified equation is\ndX (t) = \u22122X (t) dt+ 2 \u221a\n\u03b7\n1 + u (t) dB (t) , (66)\nand the moment equation is\n{ m\u0307 = F (u (t) ,m (t)) := 4 (\n\u03b7 1+u \u2212m\n)\n,\nm (0) = x20. (67)\nConsider the following optimal control problem\nmin u\u2208U\n{\nm (T ) + \u03b3\n\u03b7\n\u222b T\n0 u (s) ds\n}\nsubject to (67), (68)\nwhere U contains processes u such that u (t) \u2265 0. The constant \u03b3 measures the unit cost of introducing an extra gradient sample. Define the value function\nV (m, t) := min u\u2208U\n{\nm (T ) + \u03b3\n\u03b7\n\u222b T\nt\nu (s) ds \u2223 \u2223 \u2223 m\u0307 (t) = F (u (t) ,m (t)) ,m (t) = m\n}\n. (69)\nThe corresponding HJB equation is \n  \n  \nVt +min u\u22650\n{\nF (u,m)Vm + \u03b3\n\u03b7 u\n}\n= 0,\nV (m,T ) = m, V (0, t) = 0.\n(70)\nThe optimal control is given by\nu\u2217 = argmin u\n{\nF (u,m)Vm + \u03b3\n\u03b7 u\n}\n. (71)"}, {"heading": "3.3.2 Exact Solution of the HJB", "text": "Performing the minimization over u, we have\nmin u\u22650\n{\nF (u,m)Vm + \u03b3\n\u03b7 u\n}\n=\n{\n4 (\u03b7 \u2212m)Vm Vm \u2264 \u03b34\u03b72 , 4 (\u221a \u03b3Vm \u2212mVm ) \u2212 \u03b3 \u03b7 Vm > \u03b3 4\u03b72 , (72)\nwith\nargmin u\n{\nF (u,m)Vm + \u03b3\n\u03b7 u\n}\n=\n{\n0 Vm \u2264 \u03b34\u03b72 , 2\u03b7 \u221a\nVm/\u03b3 \u2212 1 Vm > \u03b34\u03b72 . (73)\nThe resulting Hamilton-Jacobi equation is solved by the method of characteristics. We have\nV (m, t) =\n\n \n \n\u03b7 + (m\u2212 \u03b7) e\u22124(T\u2212t) \u03b3 > 4\u03b72, me\u22124(T\u2212t) + (T \u2212 t) \u03b3\n\u03b7 + 2\n\u221a \u03b3 ( 1\u2212 e\u22122(T\u2212t) )\n\u03b3 \u2264 4\u03b72, T \u2212 t\u0303 < t \u2264 T e\u22124(T\u2212t) (m\u2212 \u03b7) + 2\u221a\u03b3 + \u03b3\n\u03b7\n( t\u0303\u2212 34 )\n\u03b3 \u2264 4\u03b72, 0 \u2264 t \u2264 T \u2212 t\u0303, , (74)\nwhere\nt\u0303 = 1\n4 log\n(\n4\u03b72\n\u03b3\n)\n. (75)\nWith the expression for V , we can obtain the optimal control and optimally controlled process by solving (73) and (67). We obtain\nu\u2217 (t) =\n\n \n \n0 \u03b3 > 4\u03b72, 0 \u03b3 \u2264 4\u03b72, 0 \u2264 t < T \u2212 t\u0303, 2\u03b7e2(t\u2212T )/ \u221a \u03b3 \u2212 1 \u03b3 \u2264 4\u03b72, T \u2212 t\u0303 \u2264 t \u2264 T,\n(76)\nand\nm\u2217 (t) =\n\n \n \n\u03b7 + ( x20 \u2212 \u03b7 ) e\u22124t \u03b3 > 4\u03b72, \u03b7 + (\nx20 \u2212 \u03b7 ) e\u22124t \u03b3 \u2264 4\u03b72, 0 \u2264 t \u2264 T \u2212 t\u0303, \u221a \u03b3e2(T\u2212t) \u2212 \u03b34\u03b7e4(T\u2212t) + ( x20 \u2212 \u03b7 ) e\u22124t \u03b3 \u2264 4\u03b72, T \u2212 t\u0303 < t \u2264 T. (77)\nAs before, by setting t = k\u03b7 we can obtain a precise batch-size strategy for the SGA. Consider the case \u03b3 \u2264 4\u03b72, corresponding to the situation where the computational cost for adding gradient samples is not too large and mini-batching can be applied. Expression (76) says that we should not apply mini-batch for early times (k \u2264 N \u2212 k\u0303 = ( T \u2212 t\u0303 ) /\u03b7). After N \u2212 k\u0303 steps, we apply mini-batch with a batch size that is exponentially increasing in k. See figure 6 for an illustration for \u03b7 = 0.05, x0 = 1, N = 50. The final batch size is\n1 + u\u2217 (\u03b7N) = 2\u03b7\u221a \u03b3 \u2212 1. (78)\nIn particular, the optimal control result states that instead of using a batch size that is constant in time (as is often applied in practice, see for example, Krizhevsky et al. (2012); LeCun et al. (1989); Bengio (2012)), the better strategy is to perform an aggressive minibatching at the end. On a qualitative level, the latter makes sense because increasing batch size decreases the variance. However, the variance of the uncontrolled error process is O (\u03b7) at large times, thus with mini-batching only a finite amount of steps is required to decrease this variance to a desired magnitude. Any mini-batching before this is wasted computation. The optimal control solution makes these statements precise: N \u2212 k\u0303 is the exact time that we should start mini-batching and (76) gives the optimal batch size schedule. Applying the control in practice requires the value of the constant \u03b3, which is not available in practice. What is available, however, is the amount of additional computation that we are willing to introduce, which in turn allows us to determine the value of \u03b3 by integrating (76) in time. See Section 4.4."}, {"heading": "4. Application to General Linear Equation", "text": "In this section, we translate the insights obtained thus far to practical procedures, and apply them to solving general linear equations, one of the most basic and important testbeds for SGA algorithms."}, {"heading": "4.1 Dynamics", "text": "The essential dynamical feature are the same as the example in Section 2.2. For general A,\n1\nn\n\u2211\ni\n\u2207fi(x)\u2207fi(x)T = 2\nn\n\u2211\ni\nai(a T i x\u2212 bi)2aTi\n\u2207f(x)\u2207f(x)T = 4 n2\nAT (Ax\u2212 b)(Ax\u2212 b)TA (79)\nThe SME is then: {\ndX = \u2212 2 m AT (AX \u2212 b)dt+\u221a\u03b7\u03c3(X)dB, \u03c3(x)\u03c3(x)T = 1 n \u2211 i\u2207fi(x)\u2207fi(x)T \u2212\u2207f(x)\u2207f(x)T . (80)\nThe drift matrix and the volatility matrix do not commute in general, hence we cannot expect to diagonalize them simultaneously. To carry on the analysis, we need to make some approximations. We replace the volatility term by a constant matrix \u03c3(x\u2217). This makes sense because eventually, Xt will be oscillating around x\n\u2217, \u03c3(x\u2217) is therefore a first order approximation. In the beginning of the iterations, the drift term dominates, the error caused by approximating the volatility term is negligible.\nLet ATA = U\u039bUT be the eigenvalue decomposition, let X = UY + x\u2217 with ATAx\u2217 = AT b. We have\ndY = \u2212 2 n \u039bY dt+ \u221a \u03b7UT\u03c3(x\u2217)dB. (81)\nLet \u03b2 = diag(UT\u03c3(x\u2217)\u03c3(x\u2217)TU) and \u03bb = diag(\u039b). The second moment equations are\ndEY 2i = \u2212 4\nn \u03bbiEY\n2 i dt+ \u03b7\u03b2idt (82)\nfor i = 1, \u00b7 \u00b7 \u00b7 , d. We have\nEY 2i (t) = \u03b7n\u03b2i 4\u03bbi + (y2i \u2212 \u03b7n\u03b2i 4\u03bbi )e\u2212 4\u03bbit n . (83)\nSubstitute Y = UT (X \u2212 x\u2217), we have\nE\u2016X \u2212 x\u2217\u20162 = \u2211\ni\nEY 2i . (84)\nThis expression gives both the initial exponential convergence rate and the eventual oscillation. In our numerical simulations, we found the above first order approximation is very accurate. Figure 7(a) shows an example where A \u2208 R200\u00d7100, b \u2208 R20, both with i.i.d N (0, 1) entries, \u03b7 = 1e\u22125. The matrices are generated using MATLAB r2015b, with random seed 721. In this example, the predicted eventual variance is 0.000488; the actual variance is 0.000495. This demonstrates the validity of the SME approximation.\nNote that if the system Ax = b is consistent, first order approximation of \u03c3(Xt) at x \u2217 gives zero matrix, second order approximation gives quadratic terms, corresponding to the geometric Brownian motion type behavior. Therefore, as long as it converges, it converges linearly all the way to optimum."}, {"heading": "4.2 Momentum", "text": "Now, let us study the SGA with momentum applied to solving the inconsistent equation Ax = b. In particular, we demonstrate the choice of optimum \u00b5. Using the same same notation, we write X = UY + x\u2217, V = UZ. Then the second moment equations become\n\n \n \ndY 2i = 2 \u03b7 YiZidt dYiZi = (\u2212 2n\u03bbiY 2i + \u00b5\u22121 \u03b7 YiZi + 1 \u03b7 Z2i )dt dZ2i = (\u2212 4n\u03bbiYiZi + 2(\u00b5\u22121) \u03b7 Z2i + \u03b7\u03b2i)dt\n(85)\nfor i = 1, \u00b7 \u00b7 \u00b7 , d. Setting the right hand size to zero, we see that EY 2i = \u03b2i\u03b7n4\u03bbi(\u00b5\u22121) . Hence\nE\u2016Xt \u2212 x\u2217\u20162 = \u2211\ni\n\u03b2i\u03b7n\n4\u03bbi(\u00b5\u2212 1) . (86)\nThis characterizes the variance. The optimal \u00b5\u2217 is given by\n\u00b5\u2217 = \u221a n\u2212 2\u221a2\u03b4\u03b7\u221a\nn , \u03b4 = \u03bbmin. (87)\nFigure 8(a) shows a comparison of performance with optimal \u00b5 and other choices. Choosing a smaller \u00b5 gives slower initial convergence, but the final oscillation is also small. Choosing the optimal \u00b5 gives best initial convergence speed, but the eventual oscillation is also larger. Choosing larger \u00b5 is the worst, it gives slower initial convergence and larger eventual oscillation. Figure 8(b) shows the performance of different choices of \u00b5 around \u00b5\u2217. Each dot in the figure represents the average of the mean square error at the end of 25 iterations, which is about the transition time for the \u00b5\u2217. The average is taken over 200 trials. As can be seen, the performance is very sensitive to the choices of \u00b5. Choosing smaller or larger values than \u00b5\u2217 both lead to inferior performance. From a practical point of view, when the optimal \u00b5 cannot be reliably estimated, it is safer to use smaller momentum.\nOur previous analysis shows that for the optimal \u00b5\u2217, if we run SGA without momentum but with a learning rate which is the base learning rate multiplied by 1/(1 \u2212 \u00b5\u2217), both the initial convergence rate and the eventual oscillation will be similar. Figure 9 shows this phenomenon."}, {"heading": "4.3 Learning rate schedule", "text": "Here, we apply the insights gained from Section 3.2 to select the optimal learning rate. Let us define the total error\nm (t) := E \u2016X (t)\u2212 x\u2217\u20162 .\nSumming expression (82) over i, we have\ndm = \u2212 4 n\nd \u2211\ni=1\n\u03bbiEY 2 i dt+ \u03b7\u03b2dt, (88)\nwhere \u03b2 = Tr ( \u03c3 (x\u2217)\u03c3T (x\u2217) )\n. Let \u03b4 denote the smallest singular value of A, then we have the bound\nm\u0307 (t) \u2264 \u22124\u03b4 n m (t) + \u03b7\u03b2. (89)\nFor large times, the error is dominated by the eigenmode associated with \u03b4. Hence the above bound is in fact a good approximation. Thus, we consider the following moment equation\n{\nm\u0307 (t) = \u22124\u03b4 n m (t) + \u03b7\u03b2, m (0) = m0, (90)\nwhere m0 := \u2016x0 \u2212 x\u2217\u20162. The relevant moment equation for a time varying learning rate is (c.f. (54))\n{\nm\u0307 (t) = \u22124\u03b4 n m (t)u (t) + \u03b7\u03b2u (t)2 , m (0) = m0 (91)\nThus, an optimal learning rate schedule can be found by considering the optimal control problem\nmin u\u2208U m (T ) subject to (91). (92)\nFollowing the procedure outlined in Section 3.2, we can solve the associated HJB equation to obtain the optimal control\nu\u2217 (t) =\n{\n1 0 \u2264 t < t\u2217 (m0) , n\nn+2\u03b4(t\u2212t\u2217(m0)) t \u2217 (m0) \u2264 t \u2264 T,\n(93)\nwhere\nt\u2217 = n\n4\u03b4 log\n(\n4\u03b4m0 n\u03b2\u03b7 \u2212 1 ) . (94)\nFor simplicity, we have only considered the case where m0 is large (m0 > \u03b2\u03b7n 2\u03b4 ). This is usually the relevant case for relatively well conditioned A and generic initial guesses x0. To apply the control (93), we have to first estimate2 m0 and \u03b2 since they depend on the unknown quantity x\u2217. Observe that f is strongly convex with parameter \u03b4. Hence we have\nm0 = \u2016x0 \u2212 x\u2217\u20162\n\u2264 1 \u03b4 (\u2207f (x0)\u2212\u2207f (x\u2217))T (x0 \u2212 x\u2217) \u2264 1 2 \u2016x0 \u2212 x\u2217\u20162 + 1 2\u03b42 \u2016\u2207f (x0)\u20162 . (95)\n2. We stress here that we are only looking for a rough estimate, as we have checked that the performance is not very sensitive to these estimates.\nHence, m0 \u2264 1\u03b42 \u2016\u2207f (x0)\u2016 2 and we use the latter as an estimate of the former. Note that a slight over-estimate of t\u2217 is not detrimental to performance. To estimate \u03b2, recall that\n\u03b2 = Tr\n\n\n2\nn\nn \u2211\nj=1\naja T j\n( aTj x \u2217 \u2212 bj\n)2\n\n . (96)\nFor generic A and x\u2217, we assume that each aTj x \u2217 \u2212 bj is of order 1, and hence, we have\n\u03b2 \u2248 2 n TrATA. (97)\nUsing these estimates, we can apply the optimal control to the minimization problem and the result is shown in Figure 10, where we fixed d = 100, n = 200, \u03b7 = 0.001. We see that the performance of the controlled process is superior to both the uncontrolled process and the badly controlled process, where we immediately apply the 1/k schedule on the learning rate at the beginning of the SGA."}, {"heading": "4.4 Mini-batch schedule", "text": "The moment equation for mini-batch control is\nm\u0307 (t) = \u22124\u03b4 n m (t) +\n\u03b7\u03b2\n1 + u (t) , (98)\nand the corresponding optimal control problem is {\nminu\u2208U\n{\nm (T ) + \u03b3 \u03b7 \u222b T 0 u (s) ds }\nsubject to (98) and m (0) = m0. (99)\nWe shall only consider the case where \u03b3 is sufficiently small (\u03b3 \u2264 \u03b2\u03b72) so that mini-batching is desirable. Solving the HJB, we obtain the optimal control\nu\u2217 (t) =\n{\n0 0 \u2264 t < T \u2212 t\u0303, \u03b7 \u221a \u03b2/\u03b3e 2\u03b4 n (t\u2212T ) \u2212 1 T \u2212 t\u0303 \u2264 t \u2264 T,\n(100)\nwhere\nt\u0303 = n\n4\u03b4 log\n(\n\u03b2\u03b72\n\u03b3\n)\n(101)\nWe can estimate \u03b2 as before, but we also have to determine \u03b3. To do this, we first fix M to be the number of gradient samples we are going to use in addition to the usual SGA. Thus M measures the computational overhead. Now, it is easy to see that\nM = 1\n\u03b7\n\u222b T\n0 u\u2217 (s) ds =\nn\n4\u03b4\u03b72\n(\nlog\n(\n\u03b3\n\u03b2\u03b72\n)\n+ 2\u03b7\n\u221a\n\u03b2 \u03b3 \u2212 2\n)\n. (102)\nPut \u03b3 = \u01eb\u03b72 with \u01eb \u226a 1. Then,\nM = n\n2\u03b4\u03b72\n( \u221a\n\u03b2 \u01eb \u2212 1\n)\n+ n\n4\u03b4\u03b72 log\n(\n\u01eb\n\u03b2\n)\n. (103)\nFor small \u01eb, the first term dominates. Hence, we can express \u03b3 (via \u01eb) as a function of M . We obtain\n\u03b3 = \u03b2\u03b72n2\n(2\u03b4\u03b7M + n)2 . (104)\nHence,\nu\u2217 (t) =\n{\n0 0 \u2264 t < T \u2212 t\u0303, (1 + 2\u03b4\u03b7M\nn )e\n2\u03b4(t\u2212T ) n \u2212 1 T \u2212 t\u0303 \u2264 t \u2264 T,\n(105)\nand\nt\u0303 = n\n2\u03b4 log\n(\n1 + 2\u03b4\u03b7M\nn\n)\n. (106)\nApplying (105) to the problem of minimizing f = \u2016Ax\u2212 b\u20162, we see that this mini-batching schedule out-performs the schedule where batch size is constant in time, while having the same overhead M . See Figure 11, where d = 100, n = 200, \u03b7 = 0.001,M = 106. Note that due to the setup of the control problem, we have fixed the total running time N , as well as the total overhead M . Thus, we see in Figure 11 that the optimally controlled error process has a flat region approximately between iterations 100 to 370. This is of course not optimal from the practical point of view, since the computations in this region are wasted. In practice, we should instead apply mini-batch right after k\u2217, the transition point of the SGA. In this case, it is approximately at iteration 100. To obtain this type of solutions, we have to consider a optimal control problem with variable end times, which is analytically more involved. However, we see that by combining the insights from section 4.3 it is not hard to devise such controls on a heuristic level."}, {"heading": "5. Discussion", "text": "In this paper, we have introduced the stochastic modified equations approach to analyzing stochastic gradient algorithms. For simple prototypical examples, this allows us to perform exact calculations that quantify the precise stochastic dynamics of the SGA. The salient features can be summarized as follows:\n\u2022 At small times, the SGA iteration is very efficient since in the corresponding SME, the drift dominates the volatility. In this regime, for generic problems one expects the SGA to be as fast as GD, but the latter involves n times the computational cost (due to gradient evaluations), where n is the number of functions that make up the objective function f .\n\u2022 When the error is O(\u03b7), we enter a regime where the SGA fails to further decrease the objective function due to variance. The SME reflects this as the volatility dominating the drift.\n\u2022 We call the division between the two regimes the transition time of the SGA. Past this time, various variance reduction techniques should be applied.\nWith regards to the last point, we saw that the SME is also a useful framework to study various speed-up techniques. Previously, the analysis of these techniques is mostly restricted\nto heuristic arguments. In Sections 3 and 4, we studied three such modifications to the plain SGA, namely adding momentum, adjusting the learning rate and adjusting the mini-batch size. The key findings are:\n\u2022 Momentum. Adding momentum improves the initial convergence of the SGA, but introduces a higher asymptotic variance. The parameter \u00b5 must be optimally chosen, and the performance of the momentum SGA depends sensitively on the relationship between \u00b5 and the learning rate \u03b7. SGA with momentum \u00b5\u2217 has the same order of convergence rate as the plan SGA with learning rate \u03b7/(1\u2212 \u00b5\u2217).\n\u2022 Learning rate. Using optimal control, we derived exactly the optimal strategy for setting the learning rate in simple examples: we wait until the transition time of the SGA, after which we decrease the learning rate with a schedule that is asymptotically O (1/k). This is shown to be superior to following a 1/k schedule from the beginning, as proposed in some literature.\n\u2022 Mini-batch size. Again, using optimal control we showed that the best mini-batch strategy is to only perform an aggressive batch-size increment towards the end of the SGA run. The batch-size schedule and the duration of mini-batching can be determined from \u03b7 and the prescribed allowed overhead M . We showed that the above strategy is superior to the usual approach where a constant mini-batch size is applied throughout.\nThe phenomenon described above is expected to be generic for problems beyond the scope of linear equations, and our analysis provides useful guidelines for designing optimal speed-up strategies.\nWe stress here that the application of the SME methodology is not limited to the scope presented in this paper. It can be used to study and improve other modifications to the plain stochastic gradient algorithm as well. For example, the SVRG algorithm (Johnson and Zhang, 2013) can be formulated as a time-delayed SME. On a broader perspective, we expect the SME approach to be useful in analyzing other stochastic algorithms, much like how modified equations are ubiquitous in traditional numerical analysis.\nOn the theoretical side, it is also interesting to see whether the SME approximation provides an alternative approach to proving rigorous results for the SGA. For instance, the asymptotic convergence properties of the SGA can be translated to large time properties of the SME. The latter is well-studied in the stochastic differential equations literature, and this approach may yield stronger theorems compared to the current collection of rigorous results for the SGA. For example, little is known about the properties of the SGA when applied to non-convex objective functions, as the tools used to establish rigorously results in the literature mostly relied on convexity. However, the SME approach may surmount this difficulty, since the invariant properties of SDE with drift derived from a non-convex potential can be readily posed as escape problems (Freidlin et al., 2012). These issues are worthy of exploration in future works."}], "references": [{"title": "Non-strongly-convex smooth stochastic approximation with convergence rate o (1/n)", "author": ["Francis Bach", "Eric Moulines"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bach and Moulines.,? \\Q2013\\E", "shortCiteRegEx": "Bach and Moulines.", "year": 2013}, {"title": "Practical recommendations for gradient-based training of deep architectures", "author": ["Yoshua Bengio"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Bengio.,? \\Q2012\\E", "shortCiteRegEx": "Bengio.", "year": 2012}, {"title": "The stability properties of a coupled pair of non-linear partial difference equations", "author": ["Bart J Daly"], "venue": "Mathematics of Computation,", "citeRegEx": "Daly.,? \\Q1963\\E", "shortCiteRegEx": "Daly.", "year": 1963}, {"title": "Random perturbations of dynamical systems, volume 260", "author": ["Mark I Freidlin", "Joseph Sz\u00fccs", "Alexander D Wentzell"], "venue": "Springer Science & Business Media,", "citeRegEx": "Freidlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Freidlin et al\\.", "year": 2012}, {"title": "Heuristic stability theory for finite-difference equations", "author": ["CW Hirt"], "venue": "Journal of Computational Physics,", "citeRegEx": "Hirt.,? \\Q1968\\E", "shortCiteRegEx": "Hirt.", "year": 1968}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Johnson and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Johnson and Zhang.", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks. Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Yann LeCun", "Bernhard Boser", "John S Denker", "Donnie Henderson", "Richard E Howard", "Wayne Hubbard", "Lawrence D Jackel"], "venue": "Neural computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Non-asymptotic analysis of stochastic approximation algorithms for machine learning", "author": ["Eric Moulines", "Francis R Bach"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Moulines and Bach.,? \\Q2011\\E", "shortCiteRegEx": "Moulines and Bach.", "year": 2011}, {"title": "Stochastic gradient descent, weighted sampling, and the randomized kaczmarz algorithm", "author": ["Deanna Needell", "Rachel Ward", "Nati Srebro"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Needell et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Needell et al\\.", "year": 2014}, {"title": "Difference methods and the equations of hydrodynamics", "author": ["WF Noh", "MH Protter"], "venue": "Technical report,", "citeRegEx": "Noh and Protter.,? \\Q1960\\E", "shortCiteRegEx": "Noh and Protter.", "year": 1960}, {"title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "Mathematical Programming,", "citeRegEx": "Shalev.Shwartz and Zhang.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz and Zhang.", "year": 2014}, {"title": "Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes", "author": ["Ohad Shamir", "Tong Zhang"], "venue": "arXiv preprint arXiv:1212.1824,", "citeRegEx": "Shamir and Zhang.,? \\Q2012\\E", "shortCiteRegEx": "Shamir and Zhang.", "year": 2012}, {"title": "A randomized kaczmarz algorithm with exponential convergence", "author": ["Thomas Strohmer", "Roman Vershynin"], "venue": "Journal of Fourier Analysis and Applications,", "citeRegEx": "Strohmer and Vershynin.,? \\Q2009\\E", "shortCiteRegEx": "Strohmer and Vershynin.", "year": 2009}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "venue": "In Proceedings of the 30th international conference on machine learning", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "The modified equation approach to the stability and accuracy analysis of finite-difference methods", "author": ["RF Warming", "BJ Hyett"], "venue": "Journal of computational physics,", "citeRegEx": "Warming and Hyett.,? \\Q1974\\E", "shortCiteRegEx": "Warming and Hyett.", "year": 1974}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["Lin Xiao", "Tong Zhang"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Xiao and Zhang.,? \\Q2014\\E", "shortCiteRegEx": "Xiao and Zhang.", "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": "Although there have been some convergence results of the SGA algorithms and their variants (Shamir and Zhang, 2012; Moulines and Bach, 2011; Needell et al., 2014; Xiao and Zhang, 2014; Shalev-Shwartz and Zhang, 2014; Bach and Moulines, 2013), less is known about their dynamics.", "startOffset": 91, "endOffset": 241}, {"referenceID": 8, "context": "Although there have been some convergence results of the SGA algorithms and their variants (Shamir and Zhang, 2012; Moulines and Bach, 2011; Needell et al., 2014; Xiao and Zhang, 2014; Shalev-Shwartz and Zhang, 2014; Bach and Moulines, 2013), less is known about their dynamics.", "startOffset": 91, "endOffset": 241}, {"referenceID": 9, "context": "Although there have been some convergence results of the SGA algorithms and their variants (Shamir and Zhang, 2012; Moulines and Bach, 2011; Needell et al., 2014; Xiao and Zhang, 2014; Shalev-Shwartz and Zhang, 2014; Bach and Moulines, 2013), less is known about their dynamics.", "startOffset": 91, "endOffset": 241}, {"referenceID": 16, "context": "Although there have been some convergence results of the SGA algorithms and their variants (Shamir and Zhang, 2012; Moulines and Bach, 2011; Needell et al., 2014; Xiao and Zhang, 2014; Shalev-Shwartz and Zhang, 2014; Bach and Moulines, 2013), less is known about their dynamics.", "startOffset": 91, "endOffset": 241}, {"referenceID": 11, "context": "Although there have been some convergence results of the SGA algorithms and their variants (Shamir and Zhang, 2012; Moulines and Bach, 2011; Needell et al., 2014; Xiao and Zhang, 2014; Shalev-Shwartz and Zhang, 2014; Bach and Moulines, 2013), less is known about their dynamics.", "startOffset": 91, "endOffset": 241}, {"referenceID": 0, "context": "Although there have been some convergence results of the SGA algorithms and their variants (Shamir and Zhang, 2012; Moulines and Bach, 2011; Needell et al., 2014; Xiao and Zhang, 2014; Shalev-Shwartz and Zhang, 2014; Bach and Moulines, 2013), less is known about their dynamics.", "startOffset": 91, "endOffset": 241}, {"referenceID": 3, "context": "Its origin may be attributed to Hirt (1968); Noh and Protter (1960); Daly (1963); Warming and Hyett (1974).", "startOffset": 32, "endOffset": 44}, {"referenceID": 3, "context": "Its origin may be attributed to Hirt (1968); Noh and Protter (1960); Daly (1963); Warming and Hyett (1974).", "startOffset": 32, "endOffset": 68}, {"referenceID": 2, "context": "Its origin may be attributed to Hirt (1968); Noh and Protter (1960); Daly (1963); Warming and Hyett (1974).", "startOffset": 69, "endOffset": 81}, {"referenceID": 2, "context": "Its origin may be attributed to Hirt (1968); Noh and Protter (1960); Daly (1963); Warming and Hyett (1974). The early applications was the analysis of the consistency and stability of numerical methods for partial differential equations, see Hirt (1968); Warming and Hyett (1974).", "startOffset": 69, "endOffset": 107}, {"referenceID": 2, "context": "Its origin may be attributed to Hirt (1968); Noh and Protter (1960); Daly (1963); Warming and Hyett (1974). The early applications was the analysis of the consistency and stability of numerical methods for partial differential equations, see Hirt (1968); Warming and Hyett (1974).", "startOffset": 69, "endOffset": 254}, {"referenceID": 2, "context": "Its origin may be attributed to Hirt (1968); Noh and Protter (1960); Daly (1963); Warming and Hyett (1974). The early applications was the analysis of the consistency and stability of numerical methods for partial differential equations, see Hirt (1968); Warming and Hyett (1974). The method of modified equations consists of creating a differential equation which approximates the given numerical scheme more accurately than the original equation.", "startOffset": 69, "endOffset": 280}, {"referenceID": 12, "context": "Strohmer and Vershynin (2009); Needell et al.", "startOffset": 0, "endOffset": 30}, {"referenceID": 9, "context": "Strohmer and Vershynin (2009); Needell et al. (2014). Introducing the relaxation, the Kaczmarz iteration rule is x = x + \u03b1 b\u03b3k \u2212 \u3008a\u03b3 k , xk\u3009 \u2016a\u03b3\u20162 a \u03b3k .", "startOffset": 31, "endOffset": 53}, {"referenceID": 14, "context": "It has been empirically observed that SGA with momentum accelerates convergence, see for example, Sutskever et al. (2013). We study the effects of momentum using the SME approach.", "startOffset": 98, "endOffset": 122}, {"referenceID": 12, "context": "In the literature, a learning rate schedule of O (1/k) is usually proposed to ensure convergence (Shamir and Zhang, 2012).", "startOffset": 97, "endOffset": 121}, {"referenceID": 5, "context": "In particular, the optimal control result states that instead of using a batch size that is constant in time (as is often applied in practice, see for example, Krizhevsky et al. (2012); LeCun et al.", "startOffset": 160, "endOffset": 185}, {"referenceID": 5, "context": "In particular, the optimal control result states that instead of using a batch size that is constant in time (as is often applied in practice, see for example, Krizhevsky et al. (2012); LeCun et al. (1989); Bengio (2012)), the better strategy is to perform an aggressive minibatching at the end.", "startOffset": 160, "endOffset": 206}, {"referenceID": 1, "context": "(1989); Bengio (2012)), the better strategy is to perform an aggressive minibatching at the end.", "startOffset": 8, "endOffset": 22}], "year": 2017, "abstractText": "Stochastic gradient algorithms (SGA) are increasingly popular in machine learning applications and have become \u201cthe algorithm\u201d for extremely large scale problems. Although there are some convergence results, little is known about their dynamics. In this paper, We propose the method of stochastic modified equations (SME) to analyze the dynamics of the SGA. Using this technique, we can give precise characterizations for both the initial convergence speed and the eventual oscillations, at least in some special cases. Furthermore, the SME formalism allows us to characterize various speed-up techniques, such as introducing momentum, adjusting the learning rate and the mini-batch sizes. Previously, these techniques relied mostly on heuristics. Besides introducing simple examples to illustrate the SME formalism, we also apply the framework to improve the relaxed randomized Kaczmarz method for solving linear equations. The SME framework is a precise and unifying approach to understanding and improving the SGA, and has the potential to be applied to many more stochastic algorithms.", "creator": "LaTeX with hyperref package"}}}