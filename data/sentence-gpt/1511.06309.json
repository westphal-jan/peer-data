{"id": "1511.06309", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Spatio-temporal video autoencoder with differentiable memory", "abstract": "We describe a new spatio-temporal video autoencoder, based on a classic spatial image autoencoder and a novel nested temporal autoencoder. The temporal encoder is represented by a differentiable visual memory composed of convolutional long short-term memory (LSTM) cells that integrate changes over time. Here we target motion changes and use as temporal decoder a robust optical flow prediction module together with an image sampler serving as built-in feedback loop. The architecture is end-to-end differentiable. At each time step, the system receives as input a video frame, predicts the optical flow based on the current observation and the LSTM memory state as a dense transformation map, and applies it to the current frame to generate the next frame. By minimising the reconstruction error between the predicted next frame and the corresponding ground truth next frame, we train the whole system to extract features useful for motion estimation without any supervision effort. We believe these features can in turn facilitate learning high-level tasks such as path planning, semantic segmentation, or action recognition, reducing the overall supervision effort. In the current experiment, we are able to capture a spatial temporal time difference in each dimension and predict the same motion pattern for each channel. We propose that a spatial temporal decoder has a small delay period between each channel and the previous one that is the first time the system has received a temporal and spatial temporal difference in its present state. In the next experiment, we use the motion prediction module to perform the spatial temporal decoder-based learning task in the next step with a spatial temporal representation of the motion pattern and a spatial representation of the motion pattern in a real time window, as in other scenarios, such as image image stabilization, the image quality of a movie or audio sound. Our experiments demonstrate that an ensemble of three parallel, high-performance high-performance LSTM cells (1, 2, and 3) demonstrate a rapid time delay in the temporal decoder and an advantage over the local LSTM system. We also demonstrate that the LSTM memory in the LSTM cell is also efficiently maintained in the LSTM cell. This means that when the LSTM cell is in a virtual state (i.e., in a virtual state) the LSTM cell is in a virtual state at an amplitude that is similar to the LSTM cell's LSTM memory. Thus, the LSTM cell's LSTM memory is only capable of detecting motion variations", "histories": [["v1", "Thu, 19 Nov 2015 19:06:28 GMT  (4655kb,D)", "http://arxiv.org/abs/1511.06309v1", null], ["v2", "Mon, 30 Nov 2015 21:07:11 GMT  (5783kb,D)", "http://arxiv.org/abs/1511.06309v2", null], ["v3", "Sat, 6 Aug 2016 16:24:58 GMT  (3811kb,D)", "http://arxiv.org/abs/1511.06309v3", null], ["v4", "Wed, 10 Aug 2016 14:46:49 GMT  (3811kb,D)", "http://arxiv.org/abs/1511.06309v4", "The experiments section has been extended and a direct application to weakly-supervised video segmentation through label propagation has been included"], ["v5", "Thu, 1 Sep 2016 11:36:40 GMT  (3811kb,D)", "http://arxiv.org/abs/1511.06309v5", "The experiments section has been extended and a direct application to weakly-supervised video segmentation through label propagation has been included"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["viorica patraucean", "ankur handa", "roberto cipolla"], "accepted": false, "id": "1511.06309"}, "pdf": {"name": "1511.06309.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["DIFFERENTIABLE MEMORY", "Viorica P\u0103tr\u0103ucean", "Ankur Handa", "Roberto Cipolla"], "emails": ["vp344@cam.ac.uk", "ah781@cam.ac.uk", "rc10001@cam.ac.uk"], "sections": [{"heading": "1 INTRODUCTION", "text": "High-level understanding of video sequences is crucial for any autonomous intelligent agent, e.g. semantic segmentation is indispensable for navigation, or object detection skills condition any form of interaction with objects in a scene. The recent success of convolutional neural networks in tackling these high-level tasks for static images opens up the path for numerous applications. However, transferring the capabilities of these systems to tasks involving video sequences is not trivial, on the one hand due to the lack of video labelled data, and on the other hand due to convnets\u2019 inability of exploiting temporal redundancy present in videos. Motivated by these two shortcomings, we focus on reducing the supervision effort required to train recurrent neural networks, which are known for their ability to handle sequential input data (Williams & Zipser, 1995; Hochreiter et al., 2000).\nIn particular, we describe a spatio-temporal video autoencoder integrating a differentiable short-term memory module whose (unsupervised) training is geared towards motion estimation and prediction (Horn & Schunck, 1994). This choice has biological inspiration. The human brain has a complex system of visual memory modules, including visual short-term memory (VSTM), iconic memory, and long-term memory (Hollingworth, 2004). Among them, VSTM is responsible mainly for understanding visual changes (movement, light changes) in dynamic environments, by integrating visual stimuli over periods of time (Phillips, 1974; Magnussen, 2000). Moreover, the fact that infants are able to handle occlusion, containment, and covering events by the age of 2.5 months (Baillargeon, 2004) could suggest that the primary skills acquired by VSTM are related to extracting features useful for motion understanding. These features in turn could generate objectness awareness based on the simple principle that points moving together belong to the same object. Understanding objectness is crucial for high-level tasks such as semantic segmentation or action recognition (Alexe et al., 2012). In this spirit, our approach is similar to the recent work of Agrawal et al. (2015), who show that the features learnt by exploiting (freely-available) ego-motion information as supervision data are as good as features extracted with human-labelled supervision data. We believe that our work is complementary to their approach and integrating them could lead to an artificial agent with enhanced vision capabilities.\nar X\niv :1\n51 1.\n06 30\n9v 1\n[ cs\n.L G\n] 1\n9 N\nov 2\n01 5\nOur implementation draws inspiration from standard video encoders and compression schemes, and suggests that deep video autoencoders should differ conceptually from spatial image autoencoders. A video autoencoder need not reproduce by heart an entire video sequence. Instead, it should be able to encode the significant differences that would allow it to reconstruct a frame given a previous frame. To this end, we use a classic convolutional image encoder \u2013 decoder with a nested memory module composed of convolutional LSTM cells, acting as temporal encoder. Since we focus on learning features for motion prediction, we use as temporal decoder a robust optical flow prediction module together with an image sampler, which provides immediate feedback on the predicted flow map. At each time step, the system receives as input a video frame, predicts the optical flow based on the current frame and the LSTM content as a dense transformation map, and applies it to the current frame to predict the next frame. By minimising the reconstruction error between the predicted next frame and the ground truth next frame, we are able to train the whole system for motion prediction with no supervision effort. Other modules handling other types of variations like light changes could be added in parallel, inspired by neuroscience findings which suggest that VSTM is composed of a series of different modules specialised in handling the low-level processes triggered by visual changes, all of them connected to a shared memory module (Magnussen, 2000). Note that at the hardware level, this variations-centred reasoning is similar to event-based cameras (Boahen, 2005), which have started to make an impact in robotic applications (Kim et al., 2014).\nOur contribution resides at the experimental level. We propose a spatio-temporal version of LSTM cells to serve as a basic form of visual short-term memory (temporal encoder), and make available an end-to-end differentiable architecture with built-in feedback loop, that allows effortless training and experimentation with the goal of understanding the role of such an artificial visual short-term memory in low-level visual processes and its implications in high-level visual tasks. All our code (Torch implementation) is available online."}, {"heading": "2 RELATED WORK", "text": "Architectures based on LSTM cells (Hochreiter & Schmidhuber, 1997) have been very successful in various tasks involving one-dimensional temporal sequences: speech recognition (Sak et al., 2014), machine translation (Sutskever et al., 2014), music composition (Eck & Schmidhuber), due to their ability to preserve information over long periods of time. Multi-dimensional LSTM networks have been proposed to deal with (2D) images (Graves et al., 2007) or (3D) volumetric data (Stollenga et al., 2015), treating the data as spatial sequences. Since in our work we aim at building a visual short-term memory, customised LSTM cells that deal with temporal sequences of spatial data represent a natural choice.\nRecently, Srivastava et al. (2015) proposed an LSTM-based video autoencoder, which aims at generating past and future frames in a sequence, in an unsupervised manner. However, their LSTM implementation treats the input as linear sequences by linearising the frames or by using 1D frame representations produced after the last fully-connected layer of a convolutional neural network. This results in a large number of parameters, since the activations of the LSTM cells use fully-connected multiplicative operations, which are not necessarily useful for 2D images, since natural image statistics indicate only local correlations. Another difference is in the architecture setup. We are not interested in training a black-box to produce past and future frames in the sequence. Instead we aim at a transparent setup, and train a more generic memory module together with specialised modules able to decode the memory content and on which appropriate constraints (sparsity, smoothness) can be imposed.\nOur approach is partially related to optical flow estimation works like DeepFlow (Weinzaepfel et al., 2013) and FlowNet (Fischer et al., 2015). However, these methods use purely supervised training to establish matches between consecutive pairs of frames in a sequence, and then apply a variational smoothing. Our architecture is end-to-end differentiable and integrates a smoothness penalty (Huber penalty) to ensure that nearby pixels follow a locally smooth motion, and requires no labelled training data.\nAs mentioned in the previous section, our work is similar in spirit to (Agrawal et al., 2015), by establishing a direct link between vision and motion, in an attempt to reduce supervision effort for high-level scene understanding tasks. However, instead of relying on IMU input from which we can only estimate a global transformation representing the ego-motion, we predict a dense flow map by\nintegrating visual stimuli over time using a memory module, and then we use a built-in feedback loop to assess the prediction. The dense flow map is useful to explain dynamic environments, where different objects can follow different motion models, hence a global transformation is not sufficient (Menze & Geiger, 2015)."}, {"heading": "3 ARCHITECTURE", "text": "Our architecture consists of a temporal autoencoder nested into a spatial autoencoder (see Figure 1). At each time step, the network takes as input a video frame Yt of size H \u00d7W , and generates an output of the same size, representing the predicted next frame, Y\u0303t+1. In the following, we describe each of the modules in detail."}, {"heading": "3.1 SPATIAL AUTOENCODER E AND D", "text": "The spatial autoencoder is a classic convolutional encoder \u2013 decoder architecture. The encoder E contains a convolutional layer, followed by tanh non-linearity and a spatial max-pooling with subsampling layer. The decoder D mirrors the encoder, except for the non-linearity layer, and uses nearest-neighbour spatial upsampling to bring the output back to the size of the original input. After the forward pass through the spatial encoder Yt\nE\u2212\u2192 xt, the size of the feature maps xt is d\u00d7 h\u00d7w, d being the number of features, and h and w the height and width after downsampling, respectively."}, {"heading": "3.2 TEMPORAL AUTOENCODER", "text": "The goal of the temporal autoencoder is to capture significant changes due to motion (ego-motion or movement of the objects in the scene), that would allow it to predict the visual future, knowing\nthe past and the present. In a classic spatial autoencoder (Masci et al., 2011), the encoder and decoder learn proprietary feature spaces that allow an optimal decomposition of the input using some form of regularisation to prevent learning a trivial mapping. The encoder decides freely upon a decomposition based on its current feature space, and the decoder constrains the learning of its own feature space to satisfy this decomposition and to reconstruct the input, using usually operations very similar to the encoder, and having the same number of degrees of freedom. Differently from this, the proposed temporal autoencoder has a decoder with a small number of trainable parameters, whose role is mainly to provide immediate feedback to the encoder, but without the capacity of amending encoder\u2019s mistakes like in the spatial case. In optimisation terms, the error during learning is attributed mainly to the encoder, which is now more constrained to produce sensible feature maps.\n3.2.1 MEMORY MODULE LSTM\nThe core of the proposed architecture is the memory module playing the role of a temporal encoder. We aim at building a basic visual short-term memory, which preserves locality and layout ensuring a fast access and bypassing more complicated addressing mechanisms like those used by neural Turing machines (Graves et al., 2014). To this end, we use customised spatio-temporal LSTM cells with the same layout as the input. At each time step t, the LSTM module receives as input a new video frame after projection in the spatial feature space. This is used together with the memory content and output of the previous step t\u2212 1 to compute the new memory activations. This operation can be easily understood by unfolding the recurrent module over a number of time steps (see Figure 2).\nClassic LSTM cells operate over sequences of linear inputs and perform biased linear (fullyconnected) multiplications, followed by non-linearities to compute gate and cell activations. In our case, to deal with the spatial and local nature of the video frames, we replace the linear multiplication operation with spatial local convolutions.\nAll-in-all, the activations of a spatio-temporal convolutional LSTM cell at time t are given by:\n it = \u03c3(xt \u2217 wxi + ht\u22121 \u2217 whi + wibias) ft = \u03c3(xt \u2217 wxf + ht\u22121 \u2217 whf + wfbias) c\u0303t = tanh(xt \u2217 wxc\u0303 + ht\u22121 \u2217 whc\u0303 + wc\u0303bias) ct = c\u0303t it + ct\u22121 ft ot = \u03c3(xt \u2217 wxo + ht\u22121 \u2217 who + wobias) ht = ot tanh(ct)\n(1)\nwhere xt represents the input at time t, i.e. the feature maps of the frame t; it, ft, c\u0303t, and ot represent the input, forget, cell, and output gates, respectively; ct, ct\u22121, ht, and ht\u22121 are the memory and output activations at time t and t\u22121, respectively; \u03c3 and tanh are the sigmoid and hyperbolic tangent non-linearities; \u2217 represents the convolution operation, and the Hadamard product. For input feature maps of size d\u00d7h\u00d7w, the LSTM module outputs a memory map of size dm\u00d7h\u00d7w, where dm is the number of temporal features learnt by the memory. Note that the recurrent connections operate only over the temporal dimension, and use local convolutions to capture spatial context. We implemented and experimented with spatial recurrent 2D LSTM cells as well, to gain more flexibility in handling spatial context. However, since the recurrent connections require sequential processing, the execution time becomes prohibitive especially during training; therefore we decided to sacrifice the spatial recurrent connections. Note that a similar convolutional LSTM implementation was recently used by Shi et al. (2015) for precipitation nowcasting."}, {"heading": "3.2.2 OPTICAL FLOW PREDICTION \u0398 WITH HUBER PENALTY H", "text": "The optical flow prediction module generates a dense transformation map T , having the same height and width as the memory output, with one 2D flow vector per pixel, representing the displacement in x and y directions due to motion between consecutive frames. T allows predicting the next frame by warping the current frame. We use two convolutional layers with relatively large kernels (15 \u00d7 15) to regress from the memory feature space to the space of flow vectors. Large kernels are needed since the magnitude of the predicted optical flow is limited by the size of the filters. To ensure local smoothness, we need to penalise the local gradient of the flow map OT . We add a penalty module H whose role is to forward its input unchanged during the forward pass, and to inject non-smoothness error gradient during the backward pass, towards the modules that precede\nit in the architecture and that might have contributed to the error. We use Huber loss as penalty (2) with its corresponding derivative (3), due to its edge-preserving capability (Werlberger et al., 2009). The gradient of the flow map is obtained by convolving the map with a fixed (non-trainable) 2\u00d73\u00d73 filter, corresponding to a 5-point stencil, and 0 bias.\nH\u03b4(aij) = { 1 2a 2 ij , for |aij | \u2264 \u03b4,\n\u03b4(|aij | \u2212 12\u03b4), otherwise, (2)\n\u2207H\u03b4(aij) = { aij , for|aij | \u2264 \u03b4, \u03b4sign(aij), otherwise.\n(3)\nHere, aij represent the elements of OT . In our experiments, we used \u03b4 = 10\u22123."}, {"heading": "3.2.3 GRID GENERATOR GG AND IMAGE SAMPLER S", "text": "The grid generator GG and the image sampler S output the predicted feature maps x\u0303t+1 of the next frame after warping the current feature maps xt with the flow map produced by the \u0398 module. We use similar differentiable grid generator and image sampler as STN (Jaderberg et al., 2015). The output of S, of size d\u00d7 h\u00d7w, is considered as a fixed h\u00d7w grid, holding at each (xo, yo) position a feature map entry of size 1\u00d7 1\u00d7 d. We modified the grid generator to accept one transformation per pixel, instead of a single transformation for the entire image as in STN. Given the flow map T , GG computes for each element in the grid the source position (xs, ys) in the input feature map from where S needs to sample to fill in the position (xo, yo):(\nxs ys\n) = T (xo, yo) ( xo yo 1 ) , T (\u00b7, \u00b7) = ( 1 0 tx 0 1 ty ) . (4)\nThe \u0398 module outputs two parameters (tx, ty) for each pixel. The forward pass of GG is given by equation (4). In the backward pass, GG simply backpropagates the derivative of equation (4) w.r.t. the input parameters (tx, ty)."}, {"heading": "3.2.4 LOSS FUNCTION", "text": "Training the network comes down to minimising the reconstruction error between the predicted next frame and the ground truth next frame, under specific constraints. We use mean squared error to quantify the reconstruction error. Additionally, we inject penalty gradients during backpropagation at different stages of the architecture. As mentioned before, we use Huber smoothness penalty on the optical flow map. The same constraint is applied on the spatial feature maps xt, together with an L1 sparsity penalty on the memory feature maps ht. From our experiments, it appears important to not use sparsity constraints on the spatial feature maps as classic image autoencoders do. Since imposing sparsity on the spatial feature maps would require also the preprocessing of the images (subtracting mean, or local contrast normalisation) to discard low frequencies and keep only the contours, we argue that this is not optimal for motion estimation, where low frequencies are important as well. An edge-preserving smoothness constraint, which corresponds to a robust sparsity constraint on the first order derivatives, is more appropriate for the spatial feature maps. Hence, the overall objective to minimise is given by:\nLt = \u2016Y\u0303t+1 \u2212 Yt+1\u20162 + w1L1(ht) + w2H(OT ) + w3H(Oxt), (5)\nwhere w1, w2, and w3 are hard-coded parameters used to weight the sparsity and smoothness constraints, respectively. In our experiments, w1 = w2 = w3 = 10\u22121."}, {"heading": "3.3 NETWORK PARAMETERS", "text": "The whole network has 703,651 trainable parameters distributed as described in Table 1. The spatial encoder and decoder have 32 filters each, size 7 \u00d7 7. The memory module (LSTM ) has 45 filters, size 7 \u00d7 7, and the optical flow regressor \u0398 has 2 convolutional layers, each with 2 filters of size 15 \u00d7 15, and a 1 \u00d7 1 convolutional layer. The other modules: GG , H , and S have no trainable parameters."}, {"heading": "4 TRAINING", "text": "To train the proposed architecture, we follow a setup similar to (Stollenga et al., 2015); we use rmsprop, with parameters = 10\u22125, \u03c1 = 0.9, and a decaying learning rate following the rule\n\u03b7 = 10\u22124 ( 100 \u221a 1 2 )epoch . The biases of the forget gates are initialised to 1; for the other gates we set the biases to 0. The training data is represented by multiple gray-scale video sequences. At each time step, we sample a subsequence of \u03c4 consecutive frames, and the goal is to reconstruct the next frame, (\u03c4 + 1)th, in the sequence. Therefore, the network is unfolded over \u03c4 steps (as illustrated in Figure 2). After processing the subsequence, we update the parameters and we reset the memory. The initialisation of all the convolutional layers except those involved in the memory module was done using xavier method (Glorot & Bengio, 2010). We hard-code the convolutional optical flow regression module to start from 0 optical flow, i.e. no movement, by adding a 1 \u00d7 1 convolutional layer at the end of the regression module, whose weights and bias are initialised to 0. During training, before each parameter update, we clip the gradients to lie in a predefined range, to avoid numerical issues (Graves, 2013).\nOur implementation is based on Torch library (Collobert et al.) and extends the rnn package. All our code is available online (Con). The training was done on an 880M Nvidia GPU (8G of GPU memory)."}, {"heading": "5 EXPERIMENTS", "text": "We ran experiments on synthetic and real datasets, to debug the individual modules composing the architecture and to evaluate the overall performance.\nTo confirm the ability of the grid generator GG and image sampler S to generate the next frame given the current frame and the correct optical flow map, we performed sanity checks on synthetic video sequences for which the ground truth optical flow is known. We used Sintel dataset (Butler et al., 2012), which contains 23 synthetic video sequences, with 50 frames each, and ground truth optical flow maps. Figure 3 shows an example of warping. Note that since the flow displacement is significant in this dataset, the sampling result can contain artifacts, especially near boundaries. The average error induced by the sampler on this dataset is of 0.004. However, this test was done by sampling directly from the input frames, no spatial encoder \u2013 decoder was used. In our architecture, these artifacts will be washed out to a certain extent by the convolutional decoder.\nThe training and validation of the overall architecture was done using a set of video sequences extracted from PROST dataset (Santner et al., 2010) and ViSOR dataset (Vezzani & Cucchiara, 2010). Namely, we selected 11 video sequences for training, with 11027 frames in total, taken by non-moving cameras, which contain moving objects in scenes with relatively cluttered background. We selected two extra sequences (2000 frames) for validation. As a sanity check, we also ran experiments on moving MNIST dataset (Srivastava et al., 2015), which consists of 10k sequences of 20 frames each, obtained by moving (translating) MNIST digit images inside a square of size 64\u00d764, using uniform random sampling to obtain direction and velocity; the sequences can contain several overlapping digits in one frame. We used 8k sequences for training and 2k for validation.\nAs a pre-calibration step, we started the training in both cases with purely static sequences, i.e. feeding in the same frame at all \u03c4 time steps, so that the system reaches a meaningful state before feeding in sequences containing movement.\nIt is worth mentioning that we tried to run experiments on KITTI odometry dataset (Geiger et al., 2012), which contains video sequences captured by a driving car, and Sintel dataset mentioned above, which are classic datasets used in the optical flow community. However, the training failed to converge on these datasets. KITTI exhibits mostly self-motion, where the entire scene moves in terms of optical flow, making the prediction very hard and, in the same time, not useful for the purposes of our work: if all the points in the scene move, it is difficult to delineate objects. The same stands for Sintel dataset, which contains a complex combination of camera self-motion and moving objects, with a relatively low frame rate, leading to very large (non-realistic) displacements between consecutive frames.\nFigure 5 shows qualitative results obtained on moving MNIST dataset."}, {"heading": "6 CONCLUSION AND FUTURE WORK", "text": "We proposed an original spatio-termporal video autoencoder based on an end-to-end differentiable architecture that allows unsupervised training for motion prediction. The core of the architecture is a module implementing a convolutional version of long short-term memory (LSTM) cells to be used as a form of artificial visual short-term memory. The immediate next step is to incorporate the pre-trained spatial and memory modules within a supervised setup targeting a high-level task, e.g. semantic segmentation. The proposed architecture is a proof-of-concept, and possibly its performance would improve by making the whole architecture deeper.\nOn a more general note, we believe that our work can open up the path to a number of exciting directions. Due to the built-in feedback loop, various experiments can be carried out effortlessly, to develop further the basic memory module that we proposed. In particular, investigating on the size of the memory and its resolution/downsampling could shed some light into the causes of some geometric optical illusions; e.g. when storing in memory an overly-downsampled version of the visual input, this could affect the capacity of correctly perceiving the visual stimuli, in accordance\nwith Shannon-Nyquist sampling theorem (Shannon, 1949). Equally, we hypothesise that our convolutional version of LSTM cells can lead to ambiguous memory activations, i.e. the same activation would be produced when presenting a temporally moving boundary or a spatially repeated boundary. This could imitate the illusory motion experienced by biological VSTMs, i.e. static repeated patterns that produce a false perception of movement (Conway et al., 2005).\nIn the current implementation the cells\u2019 layout follows the input\u2019s layout, and all the weights are shared, meaning that the entire content is represented at the same resolution. However, neuroscience studies suggest that this is not the case for biological memories, hence experiments with non-shared weights to allow different resolutions depending on the spatial distribution could be carried out. Last but not least, the proposed architecture composed of memory module and built-in feedback loop could be applied for static images as a compression mechanism, in an attempt to train an iconic memory, similar to the inspirational work on jigsaw video compression (Kannan et al., 2007). The memory addressing in that case could use a content-based mechanism similar to NTM (Graves et al., 2014)."}, {"heading": "ACKNOWLEDGMENTS", "text": "We are greatly indebted to the Torch community for their efforts to maintain this great library, and we are grateful to CSIC Cambridge for funding this work."}], "references": [{"title": "Learning to see by moving", "author": ["Agrawal", "Pulkit", "Carreira", "Jo\u00e3o", "Malik", "Jitendra"], "venue": "CoRR, abs/1505.01596,", "citeRegEx": "Agrawal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2015}, {"title": "Measuring the objectness of image windows", "author": ["B. Alexe", "T. Deselaers", "V. Ferrari"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Alexe et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Alexe et al\\.", "year": 2012}, {"title": "Infants\u2019 physical world", "author": ["Baillargeon", "Rene\u00e9"], "venue": "American Psychological Society,", "citeRegEx": "Baillargeon and Rene\u00e9.,? \\Q2004\\E", "shortCiteRegEx": "Baillargeon and Rene\u00e9.", "year": 2004}, {"title": "Neuromorphic microchips", "author": ["K. Boahen"], "venue": "Scientific American,", "citeRegEx": "Boahen,? \\Q2005\\E", "shortCiteRegEx": "Boahen", "year": 2005}, {"title": "A naturalistic open source movie for optical flow evaluation", "author": ["D.J. Butler", "J. Wulff", "G.B. Stanley", "M.J. Black"], "venue": "In European Conf. on Computer Vision (ECCV),", "citeRegEx": "Butler et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Butler et al\\.", "year": 2012}, {"title": "Neural basis for a powerful static motion illusion", "author": ["Conway", "Bevil R", "Kitaoka", "Akiyoshi", "Yazdanbakhsh", "Arash", "Pack", "Christopher C", "Livingstone", "Margaret S"], "venue": "The Journal of Neuroscience,", "citeRegEx": "Conway et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Conway et al\\.", "year": 2005}, {"title": "Flownet: Learning optical flow with convolutional networks", "author": ["Fischer", "Philipp", "Dosovitskiy", "Alexey", "Ilg", "Eddy", "H\u00e4usser", "Philip", "Hazirbas", "Caner", "Golkov", "Vladimir", "van der Smagt", "Patrick", "Cremers", "Daniel", "Brox", "Thomas"], "venue": "CoRR, abs/1504.06852,", "citeRegEx": "Fischer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fischer et al\\.", "year": 2015}, {"title": "Are we ready for autonomous driving? the kitti vision benchmark suite", "author": ["Geiger", "Andreas", "Lenz", "Philip", "Urtasun", "Raquel"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Geiger et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Geiger et al\\.", "year": 2012}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In AISTATS,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "CoRR, abs/1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Multi-dimensional recurrent neural networks", "author": ["Graves", "Alex", "Fern\u00e1ndez", "Santiago", "Schmidhuber", "J\u00fcrgen"], "venue": "In Artificial Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2007}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies", "author": ["Hochreiter", "Sepp", "Informatik", "Fakultat F", "Bengio", "Yoshua", "Frasconi", "Paolo", "Schmidhuber", "Jurgen"], "venue": "In Field Guide to Dynamical Recurrent Networks", "citeRegEx": "Hochreiter et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2000}, {"title": "Constructing visual representations of natural scenes: the roles of short- and long-term visual memory", "author": ["A. Hollingworth"], "venue": "J Exp Psychol Hum Percept Perform.,", "citeRegEx": "Hollingworth,? \\Q2004\\E", "shortCiteRegEx": "Hollingworth", "year": 2004}, {"title": "Artificial intelligence in perspective. chapter Determining Optical Flow: A Retrospective", "author": ["Horn", "Berthold K. P", "B.G. Schunck"], "venue": null, "citeRegEx": "Horn et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Horn et al\\.", "year": 1994}, {"title": "Spatial transformer networks", "author": ["Jaderberg", "Max", "Simonyan", "Karen", "Zisserman", "Andrew", "Kavukcuoglu", "Koray"], "venue": "CoRR, abs/1506.02025,", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "Clustering appearance and shape by learning jigsaws", "author": ["Kannan", "Anitha", "Winn", "John", "Rother", "Carsten"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kannan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2007}, {"title": "Simultaneous mosaicing and tracking with an event camera", "author": ["Kim", "Hanme", "Handa", "Ankur", "Benosman", "Ryad", "Ieng", "Sio-Ho\u0131", "Davison", "Andrew J"], "venue": "In British Machine Vision Conference (BMVC)", "citeRegEx": "Kim et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "Low-level memory processes in vision", "author": ["Magnussen", "Svein"], "venue": "Trends in Neurosciences,", "citeRegEx": "Magnussen and Svein.,? \\Q2000\\E", "shortCiteRegEx": "Magnussen and Svein.", "year": 2000}, {"title": "Stacked convolutional autoencoders for hierarchical feature extraction", "author": ["Masci", "Jonathan", "Meier", "Ueli", "Cirean", "Dan", "Schmidhuber", "Jrgen"], "venue": "In Artificial Neural Networks and Machine Learning,", "citeRegEx": "Masci et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Masci et al\\.", "year": 2011}, {"title": "Object scene flow for autonomous vehicles", "author": ["Menze", "Moritz", "Geiger", "Andreas"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Menze et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Menze et al\\.", "year": 2015}, {"title": "On the distinction between sensory storage and short-term visual memory", "author": ["W.A. Phillips"], "venue": "Perception & Psychophysics,", "citeRegEx": "Phillips,? \\Q1974\\E", "shortCiteRegEx": "Phillips", "year": 1974}, {"title": "Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition", "author": ["Sak", "Hasim", "Senior", "Andrew W", "Beaufays", "Fran\u00e7oise"], "venue": "CoRR, abs/1402.1128,", "citeRegEx": "Sak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "PROST Parallel Robust Online Simple Tracking", "author": ["Santner", "Jakob", "Leistner", "Christian", "Saffari", "Amir", "Pock", "Thomas", "Bischof", "Horst"], "venue": "In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Santner et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Santner et al\\.", "year": 2010}, {"title": "Communication in the presence of noise", "author": ["C.E. Shannon"], "venue": "Proceedings of the IRE,", "citeRegEx": "Shannon,? \\Q1949\\E", "shortCiteRegEx": "Shannon", "year": 1949}, {"title": "Convolutional LSTM network: A machine learning approach for precipitation nowcasting", "author": ["Shi", "Xingjian", "Chen", "Zhourong", "Wang", "Hao", "Yeung", "Dit-Yan", "Wong", "Wai-Kin", "Woo", "Wangchun"], "venue": "CoRR, abs/1506.04214,", "citeRegEx": "Shi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2015}, {"title": "Unsupervised learning of video representations using lstms", "author": ["Srivastava", "Nitish", "Mansimov", "Elman", "Salakhutdinov", "Ruslan"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Parallel multidimensional lstm, with application to fast biomedical volumetric image segmentation", "author": ["Stollenga", "Marijn", "Byeon", "Wonmin", "Liwicki", "Marcus", "Schmidhuber", "J\u00fcrgen"], "venue": "CoRR, abs/1506.07452,", "citeRegEx": "Stollenga et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stollenga et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Video surveillance online repository (visor): an integrated framework", "author": ["Vezzani", "Roberto", "Cucchiara", "Rita"], "venue": "Multimedia Tools and Applications,", "citeRegEx": "Vezzani et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vezzani et al\\.", "year": 2010}, {"title": "Deepflow: Large displacement optical flow with deep matching", "author": ["P. Weinzaepfel", "J. Revaud", "Z. Harchaoui", "C. Schmid"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "Weinzaepfel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Weinzaepfel et al\\.", "year": 2013}, {"title": "Anisotropic Huber-L1 optical flow", "author": ["Werlberger", "Manuel", "Trobin", "Werner", "Pock", "Thomas", "Wedel", "Andreas", "Cremers", "Daniel", "Bischof", "Horst"], "venue": "In Proceedings of the British Machine Vision Conference (BMVC),", "citeRegEx": "Werlberger et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Werlberger et al\\.", "year": 2009}, {"title": "Backpropagation. chapter Gradient-based Learning Algorithms for Recurrent Networks and Their Computational Complexity", "author": ["Williams", "Ronald J", "Zipser", "David"], "venue": null, "citeRegEx": "Williams et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Williams et al\\.", "year": 1995}], "referenceMentions": [{"referenceID": 12, "context": "Motivated by these two shortcomings, we focus on reducing the supervision effort required to train recurrent neural networks, which are known for their ability to handle sequential input data (Williams & Zipser, 1995; Hochreiter et al., 2000).", "startOffset": 192, "endOffset": 242}, {"referenceID": 13, "context": "The human brain has a complex system of visual memory modules, including visual short-term memory (VSTM), iconic memory, and long-term memory (Hollingworth, 2004).", "startOffset": 142, "endOffset": 162}, {"referenceID": 21, "context": "Among them, VSTM is responsible mainly for understanding visual changes (movement, light changes) in dynamic environments, by integrating visual stimuli over periods of time (Phillips, 1974; Magnussen, 2000).", "startOffset": 174, "endOffset": 207}, {"referenceID": 1, "context": "Understanding objectness is crucial for high-level tasks such as semantic segmentation or action recognition (Alexe et al., 2012).", "startOffset": 109, "endOffset": 129}, {"referenceID": 0, "context": "In this spirit, our approach is similar to the recent work of Agrawal et al. (2015), who show that the features learnt by exploiting (freely-available) ego-motion information as supervision data are as good as features extracted with human-labelled supervision data.", "startOffset": 62, "endOffset": 84}, {"referenceID": 3, "context": "Note that at the hardware level, this variations-centred reasoning is similar to event-based cameras (Boahen, 2005), which have started to make an impact in robotic applications (Kim et al.", "startOffset": 101, "endOffset": 115}, {"referenceID": 17, "context": "Note that at the hardware level, this variations-centred reasoning is similar to event-based cameras (Boahen, 2005), which have started to make an impact in robotic applications (Kim et al., 2014).", "startOffset": 178, "endOffset": 196}, {"referenceID": 22, "context": "Architectures based on LSTM cells (Hochreiter & Schmidhuber, 1997) have been very successful in various tasks involving one-dimensional temporal sequences: speech recognition (Sak et al., 2014), machine translation (Sutskever et al.", "startOffset": 175, "endOffset": 193}, {"referenceID": 28, "context": ", 2014), machine translation (Sutskever et al., 2014), music composition (Eck & Schmidhuber), due to their ability to preserve information over long periods of time.", "startOffset": 29, "endOffset": 53}, {"referenceID": 10, "context": "Multi-dimensional LSTM networks have been proposed to deal with (2D) images (Graves et al., 2007) or (3D) volumetric data (Stollenga et al.", "startOffset": 76, "endOffset": 97}, {"referenceID": 27, "context": ", 2007) or (3D) volumetric data (Stollenga et al., 2015), treating the data as spatial sequences.", "startOffset": 32, "endOffset": 56}, {"referenceID": 30, "context": "Our approach is partially related to optical flow estimation works like DeepFlow (Weinzaepfel et al., 2013) and FlowNet (Fischer et al.", "startOffset": 81, "endOffset": 107}, {"referenceID": 6, "context": ", 2013) and FlowNet (Fischer et al., 2015).", "startOffset": 20, "endOffset": 42}, {"referenceID": 0, "context": "As mentioned in the previous section, our work is similar in spirit to (Agrawal et al., 2015), by establishing a direct link between vision and motion, in an attempt to reduce supervision effort for high-level scene understanding tasks.", "startOffset": 71, "endOffset": 93}, {"referenceID": 8, "context": "Multi-dimensional LSTM networks have been proposed to deal with (2D) images (Graves et al., 2007) or (3D) volumetric data (Stollenga et al., 2015), treating the data as spatial sequences. Since in our work we aim at building a visual short-term memory, customised LSTM cells that deal with temporal sequences of spatial data represent a natural choice. Recently, Srivastava et al. (2015) proposed an LSTM-based video autoencoder, which aims at generating past and future frames in a sequence, in an unsupervised manner.", "startOffset": 77, "endOffset": 388}, {"referenceID": 19, "context": "In a classic spatial autoencoder (Masci et al., 2011), the encoder and decoder learn proprietary feature spaces that allow an optimal decomposition of the input using some form of regularisation to prevent learning a trivial mapping.", "startOffset": 33, "endOffset": 53}, {"referenceID": 25, "context": "Note that a similar convolutional LSTM implementation was recently used by Shi et al. (2015) for precipitation nowcasting.", "startOffset": 75, "endOffset": 93}, {"referenceID": 31, "context": "We use Huber loss as penalty (2) with its corresponding derivative (3), due to its edge-preserving capability (Werlberger et al., 2009).", "startOffset": 110, "endOffset": 135}, {"referenceID": 15, "context": "We use similar differentiable grid generator and image sampler as STN (Jaderberg et al., 2015).", "startOffset": 70, "endOffset": 94}, {"referenceID": 27, "context": "To train the proposed architecture, we follow a setup similar to (Stollenga et al., 2015); we use rmsprop, with parameters = 10\u22125, \u03c1 = 0.", "startOffset": 65, "endOffset": 89}, {"referenceID": 4, "context": "We used Sintel dataset (Butler et al., 2012), which contains 23 synthetic video sequences, with 50 frames each, and ground truth optical flow maps.", "startOffset": 23, "endOffset": 44}, {"referenceID": 23, "context": "The training and validation of the overall architecture was done using a set of video sequences extracted from PROST dataset (Santner et al., 2010) and ViSOR dataset (Vezzani & Cucchiara, 2010).", "startOffset": 125, "endOffset": 147}, {"referenceID": 26, "context": "As a sanity check, we also ran experiments on moving MNIST dataset (Srivastava et al., 2015), which consists of 10k sequences of 20 frames each, obtained by moving (translating) MNIST digit images inside a square of size 64\u00d764, using uniform random sampling to obtain direction and velocity; the sequences can contain several overlapping digits in one frame.", "startOffset": 67, "endOffset": 92}, {"referenceID": 7, "context": "It is worth mentioning that we tried to run experiments on KITTI odometry dataset (Geiger et al., 2012), which contains video sequences captured by a driving car, and Sintel dataset mentioned above, which are classic datasets used in the optical flow community.", "startOffset": 82, "endOffset": 103}, {"referenceID": 24, "context": "with Shannon-Nyquist sampling theorem (Shannon, 1949).", "startOffset": 38, "endOffset": 53}, {"referenceID": 5, "context": "static repeated patterns that produce a false perception of movement (Conway et al., 2005).", "startOffset": 69, "endOffset": 90}, {"referenceID": 16, "context": "Last but not least, the proposed architecture composed of memory module and built-in feedback loop could be applied for static images as a compression mechanism, in an attempt to train an iconic memory, similar to the inspirational work on jigsaw video compression (Kannan et al., 2007).", "startOffset": 265, "endOffset": 286}], "year": 2017, "abstractText": "We describe a new spatio-temporal video autoencoder, based on a classic spatial image autoencoder and a novel nested temporal autoencoder. The temporal encoder is represented by a differentiable visual memory composed of convolutional long short-term memory (LSTM) cells that integrate changes over time. Here we target motion changes and use as temporal decoder a robust optical flow prediction module together with an image sampler serving as built-in feedback loop. The architecture is end-to-end differentiable. At each time step, the system receives as input a video frame, predicts the optical flow based on the current observation and the LSTM memory state as a dense transformation map, and applies it to the current frame to generate the next frame. By minimising the reconstruction error between the predicted next frame and the corresponding ground truth next frame, we train the whole system to extract features useful for motion estimation without any supervision effort. We believe these features can in turn facilitate learning high-level tasks such as path planning, semantic segmentation, or action recognition, reducing the overall supervision effort.", "creator": "LaTeX with hyperref package"}}}