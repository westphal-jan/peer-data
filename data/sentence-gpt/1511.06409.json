{"id": "1511.06409", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Learning to Generate Images with Perceptual Similarity Metrics", "abstract": "Deep networks are increasingly being applied to problems involving image synthesis, e.g., generating images from textual descriptions, or generating reconstructions of an input image in an autoencoder architecture. In this paper, we present a simplified approach to understand how image synthesis can be done with these methods: we demonstrate that the computational power of image synthesis and processing are not mutually exclusive.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 19 Nov 2015 21:57:46 GMT  (405kb,D)", "http://arxiv.org/abs/1511.06409v1", "Submitted to ICLR 2016"], ["v2", "Thu, 17 Mar 2016 17:21:56 GMT  (652kb,D)", "http://arxiv.org/abs/1511.06409v2", null], ["v3", "Tue, 24 Jan 2017 02:03:41 GMT  (4999kb,D)", "http://arxiv.org/abs/1511.06409v3", null]], "COMMENTS": "Submitted to ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["jake snell", "karl ridgeway", "renjie liao", "brett d roads", "michael c mozer", "richard s zemel"], "accepted": false, "id": "1511.06409"}, "pdf": {"name": "1511.06409.pdf", "metadata": {"source": "CRF", "title": "LEARNING TO GENERATE IMAGES WITH PERCEPTUAL SIMILARITY METRICS", "authors": ["Karl Ridgeway", "Jake Snell", "Brett D. Roads", "Richard S. Zemel", "Michael C. Mozer"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "Recently, interest in developing methods for training neural networks to synthesize images has exploded. The reason for this surge is threefold. First, the problem of image generation spans a wide range of difficulty, from synthetic images to handwritten digits to naturally cluttered and highdimensional scenes, the latter of which provides a fertile development and testing ground for generative models. Second, learning good generative models of images involves learning new representations. Such representations are believed to be useful for a variety of machine learning tasks, such as classification or clustering, and can also transfer between tasks. Third, image generation is fun and captures popular imagination, as efforts such as Google\u2019s Inceptionism machine demonstrate.\nOne of the primary methods for learning generative models of images is the autoencoder architecture. Autoencoders are made up of two functions, an encoder and a decoder. The encoder compresses an image into a feature vector, typically of low dimension, and the decoder takes that vector as input and reconstructs the original image as output. The autoencoder is trained to reproduce an image that is similar to the input, where similarity is typically measured in terms of the Euclidean distance between the image and its reconstruction. In a probabilistic autoencoder, where the output is viewed as a distribution over images, the model is trained to maximize the log-likelihood of the original image under this distribution.\nAutoencoders use a full-reference metric to compare an original image and its reconstruction. Such a metric is based on the complete pixel-based representation of the image. The simplest full-reference metric is mean square error (MSE), which is computed by averaging the square of the pixel intensity differences for every pixel in an image. However, MSE is known to be a poor representation of human judgments of quality. For example, a distorted image created by decreasing the contrast can yield the same MSE as one created by increasing the contrast, but the two distortions can yield quite different human judgments of visual quality; and distorting an image with salt-and-pepper impulse noise obtains a small MSE but is judged as having low visual quality relative to the original image.\nar X\niv :1\n51 1.\n06 40\n9v 1\n[ cs\n.L G\n] 1\n9 N\nov 2\n01 5\nIn this paper, we explore the effects of incorporating a loss function that, unlike MSE, is grounded in human perceptual judgements. We show that this perceptually-optimized loss leads to generated images that are judged to be of higher quality. We also show that representations learned via this perceptually-optimized loss are better suited for image classification."}, {"heading": "2 RELATED WORK", "text": ""}, {"heading": "2.1 MODELS FOR GENERATING IMAGES", "text": "The primary class of image-generating neural networks are autoencoders. There are two main types of autoencoders. The first set are deterministic, which directly map the input through hidden layers and output a reconstruction of the original image. Typically, MSE is used to evaluate the reconstruction. The second type are probabilistic models. With these models the key issue concerns the intractability of inference in the latent variables, e.g., Helmholtz Machines (Dayan et al., 1995) and variational autoencoders (VAE) (Kingma & Welling, 2013). The encoder is used to approximate a posterior distribution and the decoder is used to stochastically reconstruct the data from latent variables. Gregor et al. (2015) further introduced the Deep Recurrent Attention Writer (DRAW), extending the VAE approach by incorporating a novel differentiable attention mechanism. In all of these methods, the model output is treated as a distribution, and the evaluation of this output is the log-likelihood of the original image.\nA second class of generative models are variants of Boltzmann Machines (Smolensky, 1986; Hinton & Sejnowski, 1986) and Deep Belief Networks (Hinton et al., 2006) While these models are very powerful, each iteration of training requires a computationally costly step of MCMC to approximate derivatives of an intractable partition function (normalization constant), making it difficult to scale them to large datasets.\nA more recent approach to learning generative image models involves directly training a generator, which maps samples drawn from a uniform distribution through a deep neural network that outputs images, and trains to make the set of images generated by the model indistinguishable from real images. Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) is a paradigm that involves training a discriminator that attempts to distinguish real from generated images, along with a generator that attempts to trick the discriminator. Recently, Denton et al. (2015) have scaled this approach by training conditional GANs at each level of a Laplacian pyramid of images. An alternative approach, moment-matching networks (Li et al., 2015), directly trains the generator to make the statistics of these two distributions match. Neither of these approaches, moment-matching or adversarial, directly train the network to reconstruct each training image, so they do not utilize any error measure on an image and its reconstruction.\nAutoencoders and deep belief nets have one advantage over models that directly generate images, such as GANs: they interpret images in addition to generating images. For example, Krizhevsky & Hinton (2011) used deep autoencoders to discover compact codes that were better for classifying images than using the raw image data. In this paper, we use a similar autoencoder/image-classification paradigm as a tool to compare and evaluate models."}, {"heading": "2.2 PERCEPTION-BASED ERROR METRICS", "text": "As digitization of photos and videos became commonplace in the 1990s, the need for digital compression became apparent. Lossy compression schemes distorted image data, and it was important to quantify the drop in quality resulting from compression in order to optimize the compression scheme. Because compressed digital artifacts are eventually used by humans, researchers attempted to develop full-reference image quality metrics that take into account features to which the human visual system is sensitive and that ignore features to which it is insensitive. Some of these metrics are built on complex models of the human visual system, such as the Sarnoff JND model (Lubin, 1998), the visual differences predictor (Daly, 1992), the moving picture quality metric (Van den Branden Lambrecht & Verscheure, 1996), and the perceptual distortion metric (Winkler, 1998).\nOther metrics take more of an engineering approach, and are based on the extraction and analysis of specific features of an image to which human perception is sensitive. The most popular of these metrics is the structural similarity metric (SSIM) (Wang et al., 2004), which aims to match\nthe luminance, contrast, and structure information in an image. Other such metrics are the visual information fidelity metric (Sheikh & Bovik, 2006), which is an information theory-based measure, and the visual signal-to-noise ratio (Chandler & Hemami, 2007).\nFinally, there are transform-based methods, which compare the images after some transformation has been applied. Some of these methods include DCT/wavelets, discrete orthonormal transforms, and singular value decomposition."}, {"heading": "2.2.1 STRUCTURAL SIMILARITY", "text": "In this paper, we train autoencoders with the structural-similarity (SSIM) metric and compare to autoencoders trained with MSE. We chose the SSIM metric for our initial investigation because it is well accepted and frequently utilized in the literature. Further, its pixelwise gradient has a simple analytical form and is inexpensive to compute. In this work, we focus on the original grayscale SSIM, although there are interesting variations and improvements including color SSIM (Hassan & Bhagvati, 2012), and multiscale SSIM (Wang et al., 2003).\nThe original SSIM metric, as described in Wang et al. (2004), is a pixelwise measure that compares corresponding pixels in two images, denoted x and y, with three comparison functions\u2014luminance (I), contrast (C), and structure (S)\u2014defined in Equation 1:\nI(x, y) = 2\u00b5x\u00b5y + C1 \u00b52x + \u00b5 2 y + C1\nC(x, y) = 2\u03c3x\u03c3y + C2 \u03c32x + \u03c3 2 y + C2\nS(x, y) = \u03c3xy + C3 \u03c3x\u03c3y + C3\n(1)\nThe variables \u00b5x, \u00b5y , \u03c3x, and \u03c3y denote mean pixel intensity and the standard deviations of pixel intensity in a local image patch centered at either x or y. Following Wang et al. (2004), we chose a square neighborhood of 5 pixels on either side of x or y, resulting in 11\u00d7 11 patches. The variable \u03c3xy denotes the sample correlation coefficient between corresponding pixels in the patches centered at x and y. The constants C1, C2, and C3 are small values added for numerical stability. The three comparison functions are combined to form the SSIM score:\nSSIM(x, y) = I(x, y)\u03b1 \u00b7 C(x, y)\u03b2 \u00b7 S(x, y)\u03b3 (2)\nIn our work, we weight each function equally (\u03b1 = \u03b2 = \u03b3 = 1) and set C1 = C2 to end up with the formula for SSIM:\nSSIM(x, y) = (2\u00b5x\u00b5y + C1)(2\u03c3xy + C2)\n(\u00b52x + \u00b5 2 y + C1)(\u03c3 2 x + \u03c3 2 y + C2)\n(3)\nOur objective is to minimize the loss related to the sum of structural-similarity scores across all image pixels,\nL(X,Y ) = \u2212 \u2211 i SSIM(Xi, Yi),\nwhereX and Y are the original and reconstructed images, and i is an index over image pixels. Equation 2.2.1 has a simple analytical derivative, as found in Wang & Simoncelli (2008), and therefore it is trivial to perform gradient descent in the SSIM-related loss."}, {"heading": "3 METHODOLOGY", "text": ""}, {"heading": "3.1 NETWORK ARCHITECTURES", "text": "Our primary experiments are based on two autoencoder architectures: a fully-connected network and a convolutional network. Each architecture was constructed with a bottleneck layer\u2014the middle layer of the deep autoencoder; we used 256 nodes for the fully-connected net and either 256 or 512 nodes for the convolutional net. Each architecture was trained either with MSE or SSIM-related loss. We refer to a specific model by its architecture, the size of the bottleneck layer, and the loss function it was trained with. The fully-connected models are referred to as FC-256-{SSIM,MSE}, and the convolutional models are referred to as Conv-{256,512}-{SSIM,MSE}. Through the use of multiple model variants, we aim to demonstrate that our results are robust to architectural details."}, {"heading": "3.1.1 FULLY CONNECTED ARCHITECTURE", "text": "We adopted the fully-connected autoencoder architecture of Krizhevsky & Hinton (2011), detailed in the left panel of Table 1. Every layer has rectified linear activation functions, except the bottleneck and output layers, which have tanh activations. Krizhevsky & Hinton (2011) trained their network by stacking restricted Boltzmann machines and then fine tuning with back propagation. Instead, we train our network from scratch using back propagation and stochastic gradient descent."}, {"heading": "3.1.2 CONVOLUTIONAL ARCHITECTURE", "text": "Because of the popularity and power of convolutional nets for image processing, we included a convolutional autoencoder in our experiments. The right panel of Table 1 shows the structure of our model which uses convolutional layers to encode the input and then deconvolutional layers to decode the feature representation in the bottleneck layer. The deconvolutional layers are implemented as convolutional layers that are preceded by an upsampling step that creates a layer with 2 times the dimensions of the input layer by repeating the values of the input. To explore the role of the capacity of the convolutional layer, we built models with both 256 and 512 node bottlenecks. The specifics of convolutional autoencoders are described in Masci et al. (2011)."}, {"heading": "3.1.3 ACTIVATION QUANTIZATION", "text": "In order to enforce a strong compression of the signal in our autoencoders, we force the activations of nodes in the bottleneck layer to be binary (\u22121 or +1). Following Krizhevsky & Hinton (2011), we threshold the activations in the forward pass and use the original continuous value for the purpose of gradient calculation during back propagation. We perform this quanitization both during training and testing and all results reported are based on the quantized bottleneck-layer representations. Krizhevsky & Hinton (2011) quantized in order to obtain binary codes for hash table indexing. Our interest is to enforce a more categorical representation. We also experimented with models in which the bottleneck layer was not quantized, and we found that the quantized representations were better at predicting image classification (e.g., dog versus cat versus airplane)."}, {"heading": "3.2 DATA SETS AND TRAINING METHODOLOGY", "text": "We train autoencoders using a subset of approximately two million images of the 80 million tinyimages data set (Torralba et al., 2008), consisting of the first 30 images for every English proper noun. The images in this dataset have dimensions 32\u00d732 pixels and consist of three RGB color channels. We mapped the three color channels to a single grayscale channel using the python pillow library\u2019s convert function. The input pixels are rescaled to the range [\u22121, 1], to match the tanh activation function on all of our output layers.\nAll testing and evaluation of our models used the CIFAR-10 data set, which consists of 60,000 color images, each drawn from one of ten categories. We chose a diverse data set for training in order ensure that the autoencoders were learning general statistical characteristics of images, and not peculiarities of the CIFAR-10 data set. The CIFAR-10 color images were converted to a single grayscale channel, as was done for the training data set. We divided the CIFAR-10 images into a search database (48,000 images) and a query list (12,000 images). The purpose of these two subsets will be explained in our results section. One use of search database was as a validation set to determine when to stop training: training terminated when the reconstruction error\u2014as measured by the appropriate training metric, either MSE or SSIM\u2014stopped improving following one complete pass through the training set.\nWe train using mini-batches of size 64. The SSIM and MSE metrics are scaled differently, so we performed empirical explorations to set the learning rate appropriately for each. For MSE, we use a learning rate of 5\u00d7 10\u22125, and for SSIM 5\u00d7 10\u22122. All architectures were trained with a momentum of 0.9 and with weight decay of 5\u00d7 10\u22125."}, {"heading": "4 RESULTS", "text": "As expected, the MSE-optimized nets tend to achieve better MSE reconstruction scores, and the SSIM-optimized nets tend to achieve better SSIM reconstruction scores. Figure 1 shows the relative performance of each network on the metric it is supposed to optimize. Each bar indicates the proportion of images for which an architecture trained to optimize performance metric X obtains better performance than an architecture trained to optimize the other performance metric, when evaluated on metric X . The fact that all bars are above 50% indicates that training on one metric or the other has a significant influence on the resulting models.\nTo further compare models trained with the SSIM and MSE metric, we utilize both subjective and objective characteristics of the model output. Subjective characteristics are determined by asking humans to judge image reconstruction quality. Objective characteristics are determined by examining categorical clustering of the bottleneck-layer representations."}, {"heading": "4.1 JUDGMENTS OF IMAGE RECONSTRUCTION QUALITY", "text": "Do human observers prefer reconstructions produced by the SSIM-optimized networks or by the MSE-optimized networks? We collected judgments of perceptual quality on Amazon Mechanical Turk. Participants were presented with a sequence of image triplets with the original (reference) image in the center and the SSIM- and MSE-optimized reconstructions on either side. Participants were instructed to select which of the two reconstructions they preferred. Half the time the SSIM-optimized reconstruction appeared on the left and half the time it appeared on the right. All reconstructions came from the FC-256-{SSIM,MSE} networks.\nIn a first study, twenty participants provided preference judgments on the same set of 100 randomly selected images from the CIFAR-10 data set. For each image triple, we recorded the proportion of participants who choose the SSIM reconstruction of the image over the MSE reconstruction. Figure 2 shows the distribution of inter-participant preference for SSIM reconstructions across all 100 images. If participants were choosing randomly, we would expect to see roughly 50% preference for most images. However, a plurality of images have over 90% inter-participant agreement on SSIM, and almost no images have MSE reconstructions that are preferred over SSIM reconstructions by a majority of participants.\nFigure 3a shows the sixteen image triplets for which the largest proportion of participants preferred the SSIM reconstruction. The original image is shown in the center of the triplet and the MSE- and SSIM-optimized reconstructions appear on the left and right, respectively. (In the actual experiment, the two reconstructions were flipped on half of the trials.) In this Figure, the SSIM reconstructions all show important object details that are lost in the MSE reconstructions.\nFigure 3b shows the sixteen image triples for which the smallest proportion of participants preferred the SSIM reconstruction. In the first 15 of these images, still a majority (55-80%) of participants preferred the SSIM reconstruction to the MSE reconstruction; only in the image in the lower right corner did a majority prefer the MSE reconstruction (60%). In this Figure, the SSIM-optimized reconstructions still seem to show as much detail as the MSE-optimized reconstructions, and the inconsistency in the ratings may indicate that the two reconstructions are of about equal quality.\nIn a second study on Mechanical Turk, twenty new participants each provided preference judgments on a randomly drawn set of 100 images and their reconstructions. The images were different for each participant; consequently, a total of 2000 images were judged. Participants preferred the SSIM- over MSE-optimized reconstructions by nearly a 7:1 ratio: the SSIM reconstruction was chosen for 86.25% of the images. Examining individual participants, The participant choosing SSIM reconstructions the least still preferred them 63% of the time, and the participant choosing SSIM reconstructions the most preferred them 99% of the time."}, {"heading": "4.2 EVALUATION OF LEARNED REPRESENTATIONS", "text": "In the previous section, we showed that using a perceptually-aligned training objective improves the quality of image synthesis, as judged by human observers. In this section, we go further and claim that the SSIM objective leads to the discovery of internal representations in the neural net that are more closely tied to the category associated with an image.\nWe examine the compressed representations of the image in the bottleneck layer, which we\u2019ll refer to as the image code. As explained in the Methodology section above, these codes are binary vectors of either 256 or 512 elements. If the SSIM objective biases learning toward the discovery of codes that convey good information about the object present in an image, then we should see categorical clustering of codes. That is, the code associated with the image of one dog should be more similar to codes for images of other dogs than perhaps the code for an image of a visually similar cat. Using the method of Krizhevsky & Hinton (2011), we probe the network with a set of query images and we use the corresponding code to index into a search database\u2014a set of 48,000 images whose codes and category labels have been stored. Using the search database to identify the k nearest neighbors\nin Hamming distance, we can compute the proportion of the nearest k that have the same category label as the query image. Codes that embody category information will yield a higher score.\nFigure 4 shows the k-NN classification results for each of the six models on the CIFAR-10 test set. The abscissa specifies k \u2208 {1, ..., 10} and the ordinate indicates the mean proportion of the k nearest neighbors that are of the same class as the query. For each of the three architectures, the SSIMoptimized model obtains better classification performance than the MSE-optimized model. The difference is larger for the two convolutional architectures than for the fully connected architecture. We speculate that convolutional nets might benefit more because the SSIM measure itself is also based on a convolution operator, and convolutional networks are able to more efficiently represent the type of information that SSIM tries to preserve.\nTo more directly link SSIM with codes that embody object-category information, Figure 5 shows mean MSE and SSIM reconstruction scores for the six architectures (top and middle rows), along with the proportion correct classification for k = 10 (bottom row). Note that the classification performance is correlated with the SSIM score but not the MSE score."}, {"heading": "4.3 QUALITATIVE EXPERIMENTS ON RECURRENT IMAGE GENERATION", "text": "In order to further explore the role of perceptual losses in learning models for image generation, we adapt the DRAW model of Gregor et al. (2015) to be trained with an arbitrary differentiable image similarity metric. The DRAW model generates an image by sampling a latent zt \u223c P (Zt) for each of a fixed number of timesteps. Each zt is passed as input to a decoder RNN. The output of the decoder RNN is used to update ct, the model\u2019s accumulated representation of the output image (also known as the canvas). Due to the intractable posterior over zt given an image x, the decoder RNN is simultaneously trained with an encoder RNN that produces a variational approximation Q(Zt|x) to\n1 k -\nM S\nE\n400\n600\nS S\nIM\n0.7\n0.8\n0.9\nT o p -1\n0 C\no rr\ne c t\n27\n28\n29\n30\nFC-256-SSIM FC-256-MSE Conv-256-SSIM Conv-256-MSE Conv-512-SSIM Conv-512-MSE\nFigure 5: Comparison of the networks on MSE and SSIM metrics and top-10 classification performance. The MSE scores are reversed so that they have the same polarity as the SSIM and classification scores.\nthe true posterior. During training, the expected sum of the KL-divergence of P (Zt) from Q(Zt|x) and the negative log probability of x under the model is minimized:\nLDRAW = Ez\u223cQ [ T\u2211 t=1 KL(Q(Zt|x)||P (Zt))\u2212 logD(x|cT ) ] (4)\nwhere D(X|cT ) is the model of the input data given the canvas at the final timestep. We modify this learning objective by replacing \u2212 logD(x|cT ) with an arbitrary loss between images \u2206(x, y) and weighting the resulting sum. We call this modification Expected-Loss DRAW (EL-DRAW). Its objective takes the following form:\nLEL\u2212DRAW = Ez\u223cQ [ T\u2211 t=1 KL(Q(Zt|x)||P (Zt)) + C \u00b7\u2206(x, x\u0302) ] (5)\nwhere C is a constant governing the trade-off between the latent loss and image-specific loss, and x\u0302 = f(cT |z) is the model\u2019s deterministic prediction of the image given the final state of the canvas. We trained EL-DRAW on grayscale 32 x 32 images of dogs from the CIFAR-10 dataset. We experimented with both MSE and negative SSIM as our image-specific loss \u2206 and trained models with a range of C for each. Models trained with different loss functions will in general require separately chosen settings of C due to differences in scaling. We chose a setting for each loss that represented a comparable tradeoff between reconstruction error and KL-divergence from the prior. Details regarding the dataset and experimental hyperparameters are provided in Section 6.1 of the supplementary material, and our method for selecting C values that makes the comparison fair is described in Section 6.2.\nTest reconstructions produced by both the MSE-optimized and SSIM-optimized EL-DRAW models are shown in Figure 6. Samples from both models are visualized in Figure 7. The reconstructions and samples from the SSIM-optimized EL-DRAW model are noticeably sharper than those of the MSE-optimized model. The superiority of the former samples is due to the use of a perceptuallygrounded loss, which is better suited to capturing\u2014and generating\u2014salient details in images."}, {"heading": "5 DISCUSSION AND FUTURE WORK", "text": "We have investigated the consequences of replacing the standard MSE loss function with a perceptually-grounded loss function, SSIM, in neural networks that generate images. Human ob-\nservers judge SSIM-optimized images to be of higher quality than MSE-optimized images. Beyond this subjective measure, we also showed that the compressed representations of SSIM-optimized autoencoders preserve more information about object categories as compared to those of MSEoptimized autoencoders. These key results hold for both fully-connected and convolutional architectures and for various bottleneck sizes. In addition, we have shown that recurrent neural network architectures also benefit from training with SSIM in that they produce qualitatively better reconstructions and generated samples compared to those obtained from training with MSE.\nWith respect to the experiments on recurrent image generation, we plan to go beyond qualitative assessment of the EL-DRAW model, both by collecting human judgments of reconstruction quality, and by evaluating held-out data likelihood. The latter could be accomplished for example by Parzen window estimation, or by Hybrid Monte Carlo (HMC) methods as done by Kingma & Welling (2013). We are also investigating how probabilistic generative models can be formulated by taking into account perceptual loss.\nGiven our encouraging results, it seems appropriate to investigate other perceptually-grounded loss functions. SSIM is the low-hanging fruit because it is differentiable. Nonetheless, even blackbox loss functions can be cached into a forward model neural net (Jordan & Rumelhart, 1992) that maps image pairs into a quality measure. We can then back propagate through the forward model to transform a loss derivative expressed in perceptual quality into a loss derivative expressed in terms of individual output node activities. This flexible framework will allow us to combine multiple perceptually-grounded loss functions. Further, we can refine any perceptually-grounded loss functions with additional data obtained from human preference judgments, such as those we collected in the present set of experiments."}, {"heading": "6 SUPPLEMENTARY MATERIAL", "text": ""}, {"heading": "6.1 EXPERIMENTAL DETAILS OF EL-DRAW TRAINING", "text": "For the experiments with the EL-DRAW model in section 4.3, we used the predefined test splits of CIFAR-10 to form a test set of 1,000 images with class dog, and randomly selected 4,500 of the training images of class dog as our training set. We used the remaining 500 images of class dog for validation.\nAs in the original DRAW model, we took P (Z) to be a standard Gaussian with zero mean and unit variance for each latent dimension. We chose the logistic sigmoid function to be the activation applied to the final canvas, i.e. x\u0302 = \u03c3(cT |z) = 11+exp(\u2212cT ) .\nWe used similar hyperparameters to those of the CIFAR DRAW model trained by Gregor et al. (2015): 400 hidden units for the encoder and decoder LSTM, 200 dimensions for each latent zt, and 5x5 size for read and write operations. Our deviation in architecture was to use 32 timesteps rather than 64 in order to mitigate difficulties training the model due to exploding gradients. We clipped gradients during training by independently scaling the gradient for each weight matrix and bias vector such that the norm of each gradient was at most 10. We used the Adam method of Kingma & Ba (2014) to optimize the network."}, {"heading": "6.2 CHOICE OF C IN EL-DRAW OBJECTIVE", "text": "The value of C in the EL-DRAW objective (Equation 5) governs the trade-off between the KL loss and reconstruction error. As C increases, the model will put greater emphasis on reconstructions. At the same time, the KL-divergence of the prior from the approximate posterior will increase, leading to poorer samples. Selecting a value of C is further complicated due to the different scaling depending on the choice of the image-specific loss \u2206.\nWe trained MSE-optimized and SSIM-optimized EL-DRAW models with a range of values of C from 1 to 1000. We then evaluated the KL component of LEL\u2212DRAW on the validation set and attempted to select a setting of C for each loss that yielded comparable KL divergences. We chose C = 10 for MSE, yielding a validation KL loss of 52.8535 and C = 500 for SSIM, which yielded\nFigure 8: Test reconstructions of EL-DRAW with C = 1 and \u2206 as binary cross-entropy.\nFigure 9: Samples from EL-DRAW with C = 1 and \u2206 as binary cross-entropy.\na validation KL loss of 57.4709. Thus we would expect the SSIM-optimized model to have slightly better reconstructions at the expense of slightly worse samples. However, despite this, we observe in Figure 7 that samples from the SSIM-optimized network are noticeably better."}, {"heading": "6.3 DRAW AS A SPECIAL CASE OF EL-DRAW", "text": "As mentioned by Gregor et al. (2015), a natural choice of D for the DRAW model in the case of binary data is the Bernoulli distribution. We note that this setting of DRAW can be viewed as a special case of EL-DRAW, in which C is set to be 1 and \u2206 is taken to be binary cross-entropy. For the sake of completeness, we provide reconstructions and samples of EL-DRAW trained with this setting of C and \u2206 in Figures 8 and 9, respectively."}], "references": [{"title": "Vsnr: A wavelet-based visual signal-to-noise ratio for natural images", "author": ["D.M. Chandler", "S.S. Hemami"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Chandler and Hemami,? \\Q2007\\E", "shortCiteRegEx": "Chandler and Hemami", "year": 2007}, {"title": "Visible differences predictor: an algorithm for the assessment of image fidelity", "author": ["S.J. Daly"], "venue": "SPIE/IS&T", "citeRegEx": "Daly,? \\Q1992\\E", "shortCiteRegEx": "Daly", "year": 1992}, {"title": "Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks", "author": ["E. Denton", "S. Chintala", "A. Szlam", "R. Fergus"], "venue": null, "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Generative adversarial networks. arXiv 1406.266v1 [stat.ML", "author": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Structural Similarity Measure for Color Images", "author": ["Hassan", "Mohammed", "Bhagvati", "Chakravarthy"], "venue": "International Journal of Computer Applications", "citeRegEx": "Hassan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hassan et al\\.", "year": 2012}, {"title": "Learning and relearning in boltzmann machines", "author": ["G.E. Hinton", "T.J. Sejnowski"], "venue": "Volume 1: Foundations,", "citeRegEx": "Hinton and Sejnowski,? \\Q1986\\E", "shortCiteRegEx": "Hinton and Sejnowski", "year": 1986}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Forward models: Supervised learning with a distal teacher", "author": ["M.I. Jordan", "D.E. Rumelhart"], "venue": "Cognitive Science,", "citeRegEx": "Jordan and Rumelhart,? \\Q1992\\E", "shortCiteRegEx": "Jordan and Rumelhart", "year": 1992}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma and Welling,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2013}, {"title": "Using very deep autoencoders for content-based image retrieval", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey E"], "venue": "In ESANN. Citeseer,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2011}, {"title": "Generative Moment Matching Networks", "author": ["Li", "Yujia", "Swersky", "Kevin", "Zemel", "Rich"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A human vision system model for objective image fidelity and target detectability measurements", "author": ["Lubin", "Jeffrey"], "venue": "In Proc. EUSIPCO,", "citeRegEx": "Lubin and Jeffrey.,? \\Q1998\\E", "shortCiteRegEx": "Lubin and Jeffrey.", "year": 1998}, {"title": "Stacked convolutional autoencoders for hierarchical feature extraction", "author": ["Masci", "Jonathan", "Meier", "Ueli", "Cire\u015fan", "Dan", "Schmidhuber", "J\u00fcrgen"], "venue": "In Artificial Neural Networks and Machine Learning\u2013", "citeRegEx": "Masci et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Masci et al\\.", "year": 2011}, {"title": "Image information and visual quality", "author": ["Sheikh", "Hamid Rahim", "Bovik", "Alan C"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Sheikh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sheikh et al\\.", "year": 2006}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["P. Smolensky"], "venue": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition,", "citeRegEx": "Smolensky,? \\Q1986\\E", "shortCiteRegEx": "Smolensky", "year": 1986}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["Torralba", "Antonio", "Fergus", "Rob", "Freeman", "William T"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Torralba et al\\.,? \\Q1958\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 1958}, {"title": "Perceptual quality measure using a spatiotemporal model of the human visual system", "author": ["C.J. Van den Branden Lambrecht", "O. Verscheure"], "venue": "In Electronic Imaging: Science & Technology,", "citeRegEx": "Lambrecht and Verscheure,? \\Q1996\\E", "shortCiteRegEx": "Lambrecht and Verscheure", "year": 1996}, {"title": "Maximum differentiation (mad) competition: A methodology for comparing computational models of perceptual quantities", "author": ["Wang", "Zhou", "Simoncelli", "Eero P"], "venue": "Journal of Vision,", "citeRegEx": "Wang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Multi-scale structural similarity for image quality assessment", "author": ["Wang", "Zhou", "Simoncelli", "Eero P", "Bovik", "Alan C"], "venue": "IEEE Asilomar Conference on Signals, Systems and Computers,", "citeRegEx": "Wang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2003}, {"title": "Image quality assessment: from error visibility to structural similarity", "author": ["Wang", "Zhou", "Bovik", "Alan Conrad", "Sheikh", "Hamid Rahim", "Simoncelli", "Eero P"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Wang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2004}, {"title": "2015), a natural choice of D for the DRAW model in the case", "author": ["Gregor"], "venue": null, "citeRegEx": "Gregor,? \\Q2015\\E", "shortCiteRegEx": "Gregor", "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": "A second class of generative models are variants of Boltzmann Machines (Smolensky, 1986; Hinton & Sejnowski, 1986) and Deep Belief Networks (Hinton et al.", "startOffset": 71, "endOffset": 114}, {"referenceID": 7, "context": "A second class of generative models are variants of Boltzmann Machines (Smolensky, 1986; Hinton & Sejnowski, 1986) and Deep Belief Networks (Hinton et al., 2006) While these models are very powerful, each iteration of training requires a computationally costly step of MCMC to approximate derivatives of an intractable partition function (normalization constant), making it difficult to scale them to large datasets.", "startOffset": 140, "endOffset": 161}, {"referenceID": 3, "context": "Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) is a paradigm that involves training a discriminator that attempts to distinguish real from generated images, along with a generator that attempts to trick the discriminator.", "startOffset": 39, "endOffset": 64}, {"referenceID": 12, "context": "An alternative approach, moment-matching networks (Li et al., 2015), directly trains the generator to make the statistics of these two distributions match.", "startOffset": 50, "endOffset": 67}, {"referenceID": 2, "context": "Gregor et al. (2015) further introduced the Deep Recurrent Attention Writer (DRAW), extending the VAE approach by incorporating a novel differentiable attention mechanism.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Recently, Denton et al. (2015) have scaled this approach by training conditional GANs at each level of a Laplacian pyramid of images.", "startOffset": 10, "endOffset": 31}, {"referenceID": 2, "context": "Recently, Denton et al. (2015) have scaled this approach by training conditional GANs at each level of a Laplacian pyramid of images. An alternative approach, moment-matching networks (Li et al., 2015), directly trains the generator to make the statistics of these two distributions match. Neither of these approaches, moment-matching or adversarial, directly train the network to reconstruct each training image, so they do not utilize any error measure on an image and its reconstruction. Autoencoders and deep belief nets have one advantage over models that directly generate images, such as GANs: they interpret images in addition to generating images. For example, Krizhevsky & Hinton (2011) used deep autoencoders to discover compact codes that were better for classifying images than using the raw image data.", "startOffset": 10, "endOffset": 697}, {"referenceID": 1, "context": "Some of these metrics are built on complex models of the human visual system, such as the Sarnoff JND model (Lubin, 1998), the visual differences predictor (Daly, 1992), the moving picture quality metric (Van den Branden Lambrecht & Verscheure, 1996), and the perceptual distortion metric (Winkler, 1998).", "startOffset": 156, "endOffset": 168}, {"referenceID": 21, "context": "The most popular of these metrics is the structural similarity metric (SSIM) (Wang et al., 2004), which aims to match", "startOffset": 77, "endOffset": 96}, {"referenceID": 20, "context": "In this work, we focus on the original grayscale SSIM, although there are interesting variations and improvements including color SSIM (Hassan & Bhagvati, 2012), and multiscale SSIM (Wang et al., 2003).", "startOffset": 182, "endOffset": 201}, {"referenceID": 19, "context": "In this work, we focus on the original grayscale SSIM, although there are interesting variations and improvements including color SSIM (Hassan & Bhagvati, 2012), and multiscale SSIM (Wang et al., 2003). The original SSIM metric, as described in Wang et al. (2004), is a pixelwise measure that compares corresponding pixels in two images, denoted x and y, with three comparison functions\u2014luminance (I), contrast (C), and structure (S)\u2014defined in Equation 1:", "startOffset": 183, "endOffset": 264}, {"referenceID": 19, "context": "Following Wang et al. (2004), we chose a square neighborhood of 5 pixels on either side of x or y, resulting in 11\u00d7 11 patches.", "startOffset": 10, "endOffset": 29}, {"referenceID": 14, "context": "The specifics of convolutional autoencoders are described in Masci et al. (2011).", "startOffset": 61, "endOffset": 81}, {"referenceID": 4, "context": "In order to further explore the role of perceptual losses in learning models for image generation, we adapt the DRAW model of Gregor et al. (2015) to be trained with an arbitrary differentiable image similarity metric.", "startOffset": 126, "endOffset": 147}], "year": 2017, "abstractText": "Deep networks are increasingly being applied to problems involving image synthesis, e.g., generating images from textual descriptions, or generating reconstructions of an input image in an autoencoder architecture. Supervised training of image-synthesis networks typically uses a pixel-wise squared error (SE) loss to indicate the mismatch between a generated image and its corresponding target image. We propose to instead use a loss function that is better calibrated to human perceptual judgments of image quality: the structural-similarity (SSIM) score of Wang, Bovik, Sheikh, and Simoncelli (2004). Because the SSIM score is differentiable, it is easily incorporated into gradient-descent learning. We compare the consequences of using SSIM versus SE loss on representations formed in deep autoencoder and recurrent neural network architectures. SSIM-optimized representations yield a superior basis for image classification compared to SE-optimized representations. Further, human observers prefer images generated by the SSIMoptimized networks by nearly a 7:1 ratio. Just as computer vision has advanced through the use of convolutional architectures that mimic the structure of the mammalian visual system, we argue that significant additional advances can be made in modeling images through the use of training objectives that are well aligned to characteristics of human perception.", "creator": "LaTeX with hyperref package"}}}