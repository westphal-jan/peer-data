{"id": "1511.06732", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Sequence Level Training with Recurrent Neural Networks", "abstract": "Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the BLEU score: a popular metric to compare a sequence to a reference. On three different tasks, our approach outperforms several strong baselines for greedy generation, and it matches their performance with beam search, while being several times faster. For instance, a parallel approach to searching for the following words is also possible, while an approach to searching for the following words is also possible.\n\n\n\n\n\nIn each case, our model is evaluated to generate the entire sequence, or multiple times. The task to compute the BLEU score would then proceed as follows:\n\nThe BLEU score would be calculated on a three-dimensional graph of the top ten sentences, for the BLEU score.\n\nNote that this analysis will be limited to the following words:\nThe BLEU score would be calculated on a 3-dimensional graph of the top ten sentences, for the BLEU score.\nThe BLEU score would be calculated on a four-dimensional graph of the top ten sentences, for the BLEU score.\nFor each word, the BLEU score would be calculated on the following words, as they have been computed in a parallel context.\nOur solution involves the following:\nThe BLEU score would be calculated on a 3-dimensional graph of the top ten sentences, for the BLEU score.\nThe BLEU score would be calculated on a 4-dimensional graph of the top ten sentences, for the BLEU score.\nThe BLEU score would be calculated on a three-dimensional graph of the top ten sentences, for the BLEU score.\nThe BLEU score would be calculated on a four-dimensional graph of the top ten sentences, for the BLEU score.\nFor each word, the BLEU score would be calculated on a 3-dimensional graph of the top ten sentences, for the BLEU score.\nWhen we generate the BLEU score, we have to generate the full sequence from scratch.\nAs described above, we can build a new generation of", "histories": [["v1", "Fri, 20 Nov 2015 19:25:54 GMT  (1935kb,D)", "http://arxiv.org/abs/1511.06732v1", null], ["v2", "Mon, 14 Dec 2015 16:11:27 GMT  (1960kb,D)", "http://arxiv.org/abs/1511.06732v2", null], ["v3", "Tue, 15 Dec 2015 16:51:31 GMT  (1960kb,D)", "http://arxiv.org/abs/1511.06732v3", null], ["v4", "Wed, 6 Jan 2016 06:24:58 GMT  (1963kb,D)", "http://arxiv.org/abs/1511.06732v4", null], ["v5", "Fri, 12 Feb 2016 16:05:32 GMT  (1963kb,D)", "http://arxiv.org/abs/1511.06732v5", null], ["v6", "Wed, 4 May 2016 13:43:39 GMT  (1963kb,D)", "http://arxiv.org/abs/1511.06732v6", null], ["v7", "Fri, 6 May 2016 21:18:46 GMT  (1995kb,D)", "http://arxiv.org/abs/1511.06732v7", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["marc'aurelio ranzato", "sumit chopra", "michael auli", "wojciech zaremba"], "accepted": true, "id": "1511.06732"}, "pdf": {"name": "1511.06732.pdf", "metadata": {"source": "CRF", "title": "RECURRENT NEURAL NETWORKS", "authors": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "emails": ["wojciech}@fb.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Natural language is the most natural form of communication for humans. It is therefore essential that interactive AI systems are capable of generating text. A wide variety of applications rely on text generation, including machine translation, video/text summarization, question answering, among others. From a machine learning perspective, text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context. For instance, given an image the model may be expected to generate an appropriate caption for it, or, given a sentence in English language the model may be expected to translate it into French.\nPopular choices for text generation models are language models based on n-grams (Kneser & Ney, 1995), feed-forward neural networks (Morin & Bengio, 2005), and recurrent neural networks (RNNs; Mikolov et al., 2010). These models when used as is to generate text suffer from two major drawbacks. First, they are trained to predict the next word given the previous ground truth words as input. However, at test time, the resulting models are used to generate an entire sequence by predicting one word at a time, and by feeding the generated word back as input at the next time step. This process is very brittle because the model was trained on a different distribution of inputs, namely, words drawn from the data distribution, as opposed to words drawn from the model distribution. As a result the errors made along the way will quickly accumulate. We refer to this discrepancy as exposure bias which occurs when a model is only exposed to the training data distribution, instead of its own predictions. Second, the loss function used to train these models is at the word level. A popular choice is the cross-entropy loss used to maximize the probability of the next correct word. However, the performance of these models is typically evaluated using discrete metrics. One such metric is called BLEU, which measures the n-gram overlap between the model generation and the reference text. Training these models to directly optimize for BLEU is hard because a) it is not differentiable (Rosti et al., 2011), and b) combinatorial optimization is required to determine which sub-string maximizes BLEU given some context. Prior attempts (McAllester et al., 2010) at optimizing BLEU have been restricted to linear models.\nThis paper proposes a novel training algorithm which results in improved text generation compared to standard models. The algorithm tries to address the two issues discussed above. First, while training the generative model we avoid the exposure bias by using model predictions at training time. Second, we directly optimize for our final evaluation metric, namely BLEU. Our proposed methodology borrows ideas from the reinforcement learning literature (Sutton & Barto, 1988). In particular, we build on the REINFORCE algorithm proposed by Williams (1992), to achieve the\nar X\niv :1\n51 1.\n06 73\n2v 1\n[ cs\n.L G\n] 2\n0 N\nov 2\n01 5\nabove two objectives. While sampling from the model during training is quite a natural step for the REINFORCE algorithm, optimizing directly for BLEU can also be achieved by it. REINFORCE side steps the issues associated with BLEU optimization by not requiring rewards (or losses) to be differentiable.\nWhile REINFORCE appears to be well suited to tackle the text generation problem, it suffers from a significant issue. The problem setting of text generation has a very large action space which makes it extremely difficult to learn with an initial random policy. Specifically, the search space for text generation is of size O(WT ), whereW is the number of words in the vocabulary (typically of the order of 104 or more) and T is the length of the sentence (typically around 10 to 30).\nTowards that end, we introduce Mixed Incremental Cross-Entropy Reinforce (MIXER), which is our first major contribution of this work. MIXER is an easy-to-implement recipe to make REINFORCE work well for text generation applications. It is based on two key ideas: incremental learning and the use of a hybrid loss function which combines both REINFORCE and cross-entropy (see Sec. 3.2.2 for details). Both ingredients are essential to training with large action spaces. In MIXER, the model starts from the optimal policy given by cross-entropy training (as opposed to a random one), from which it then slowly deviates, in order to make use of its own predictions, as done at test time.\nOur second contribution of this work is the thorough empirical evaluation on three different tasks, namely, Text Summarization, Machine Translation and Image Captioning. We compare against several strong baselines, including, RNNs trained with cross-entropy and Data as Demonstrator (DAD) (Bengio et al., 2015; Venkatraman et al., 2015). We also compare MIXER with another simple yet novel model that we propose in this paper. We call it the End-to-End BackProp model (see Sec. 3.1.3 for details). Our results show that MIXER with a simple greedy search achieves much better accuracy compared to the baselines on all the three tasks. In addition we show that MIXER with greedy search is as accurate as the cross entropy model augmented with beam search at inference time as a post-processing step. This is particularly remarkable because MIXER with greedy search is at least 3 times faster than the cross entropy model with a beam of size 3. Lastly, we note that MIXER and beam search are complementary to each other and our experiments show that MIXER performs competitively to other models when beam search is used."}, {"heading": "2 RELATED WORK", "text": "Sequence models are typically trained to predict the next word using the cross-entropy loss (a.k.a. negative log-likelihood loss). At test time, it is common to use beam search to explore multiple alternative paths (Sutskever et al., 2014; Bahdanau et al., 2015; Rush et al., 2015). While this improves generation by typically one or two BLEU points (Papineni et al., 2002), it also comes at a cost: it makes generation at least k times slower, where k is the number of active paths in the beam (see Sec. 3.1.1 for more details).\nThe key idea of this work is to improve generation by letting the model use its own predictions at training time, as first advocated by Daume III et al. (2009). In their seminal work, the authors first noticed that structured prediction problems can be cast as a particular instance of reinforcement learning, and they then proposed SEARN, an algorithm to learn such structured prediction tasks. The basic idea is to let the model use its own predictions at training time to produce a sequence of actions (e.g., the choice of the next word). Then, a search algorithm is run to determine the optimal action at each time step, and a classifier (a.k.a. policy) is trained to predict that action. A similar idea was later proposed by Ross et al. (2011) in an imitation learning framework. Unfortunately, for text generation it is generally intractable to compute an oracle of the optimal target word given the words predicted so far.\nThe oracle issue was later addressed by an algorithm called Data As Demonstrator (DAD) (Venkatraman et al., 2015), whereby the target action at step k is the k-th action taken by the optimal policy (ground truth sequence) regardless of which input is fed to the system, whether it is ground truth, or the model\u2019s prediction. This idea was also recently tested for text generation applications by Bengio et al. (2015), who had the same motivation as our work (see Sec. 3.1.2 for more details). While DAD usually improves generation, it seems unsatisfactory to force the model to predict a certain word regardless of the preceding words."}, {"heading": "3 MODELS", "text": "The learning algorithms we describe in the following sections are agnostic to the choice of the underlying model, as long as it is parametric. In this work, we focus on Recurrent Neural Networks (RNNs) as they are a popular choice for text generation (Mikolov et al., 2010). RNN is a parametric model that takes as input a word wt \u2208 W at each time step t \u2208 [1, T ], together with an internal representation ht. This internal representation is a real-valued vector which encodes the history of words it has seen so far. Optionally, the RNN can also take as input an additional context vector ct. It learns a recursive function to compute ht and also it outputs the distribution over the next word:\nht+1 = \u03c6\u03b8(wt,ht, ct) (1) wt+1 \u223c p\u03b8(w|wt,ht+1) = p\u03b8(w|wt, \u03c6\u03b8(wt,ht, ct)) (2)\nThe parametric expression for p\u03b8 and \u03c6\u03b8 depends on the type of RNN. In this paper, we use standard RNNs (Elman, 1990; Mikolov et al., 2010) unless otherwise specified. In this case, we have (ignoring biases):\nht+1 = \u03c3(Mi1(wt) +Mhht +Mcct) (3) ot+1 = Moht+1 (4) wt+1 \u223c softmax(ot+1) (5)\nwhere the parameters of the model \u03b8 are the set of matrices {Mo,Mi,Mh,Mc}1, softmax(x) is a vector whose components are exj/ \u2211 k e\nxk , and 1(i) is an indicator vector with only the i-th component set to 1 and the rest to 0. We assume the first word of the sequence is a special token indicating the beginning of a sequence, denoted by w1 = \u2205. All entries of the first hidden state h1 are set to a constant value.\nNext, we are going to introduce both baselines and the model we propose. As we describe these models, it is useful to keep in mind the key characteristics of a text generation system, as outlined in Table 1. There are three dimensions which are important when training a model for text generation: exposure bias which can adversely affect generation at test time, ability to fully back-propagate gradients (including with respect to the chosen inputs at each time step), and loss operating at the sequence level. We will start our journey discussing models that do not possess any of these desirable features, and then move towards models that better satisfy our requirements. The last model we propose, dubbed MIXER, has all the desiderata."}, {"heading": "3.1 WORD-LEVEL TRAINING", "text": "In this section we review a collection of methodologies used for training text generation models at the word level, that is, optimizing the prediction of only one word ahead of time. We start with the simplest and the most popular method which optimizes the cross-entropy loss at every time step. We then discuss a recently proposed modification to the standard cross-entropy training which explicitly uses the model predictions during training. We finish by proposing a simple yet novel baseline which uses its model prediction during training and also has the ability to back propagate the gradients through the entire sequence. While these extensions tend to make generation more robust, they still lack explicit supervision at the sequence level."}, {"heading": "3.1.1 CROSS ENTROPY TRAINING (XENT)", "text": "Cross-entropy loss (XENT) maximizes the probability of the observed sequence according to the model. In particular, if the target sequence is [w1, w2, . . . , wT ], then XENT training involves minimizing the following loss function:\nL = \u2212 log p(w1, . . . , wT ) = \u2212 log T\u220f t=1 p(wt|w1, . . . , wt\u22121) = T\u2211 t=1 log p(wt|w1, . . . , wt\u22121). (6)\nWhen using an RNN, each term p(wt|w1, . . . , wt\u22121) is modeled as a parametric function as given in Eq. 5. The above loss function trains the model to be good at greedily predicting the next word at each time step without considering the whole sequence. Training proceeds by truncated backpropagation through time (Rumelhart et al., 1986; Mikolov et al., 2010).\nOnce trained, one can use the model to generate an entire sequence as follows. Let wgt denote the word generated by the model at the t-th time step. Then the next word is generated by:\nwgt+1 = argmaxw p\u03b8(w|wgt ,ht+1). (7)\nNotice that, the model is trained to maximize p\u03b8(w|wt,ht+1), where wt is the word in the ground truth sequence. However, during generation the model is used as p\u03b8(w|wgt ,ht+1). In other words, during training the model is only exposed to the ground truth words. However, at test time the model has only access to its own predictions, which may not be correct. As a result, during generation the model can potentially deviate quite far from the actual sequence to be generated. Figure 1 illustrates this discrepancy.\nThe generation described by Eq. 7 is a greedy left-to-right process which does not necessarily produce the most likely sequence according to the model, because:\nT\u220f t=1 max wt+1 p\u03b8(wt+1|wgt ,ht+1) \u2264 max w1,...,wT T\u220f t=1 p\u03b8(wt+1|wgt ,ht+1)\n1\u03b8 may include also the additional parameters used to compute c.\nThe most likely sequence [w1, w2, . . . , wT ] might contain a word wt which is sub-optimal at an intermediate time-step t. This phenomena is commonly known as a search error.\nBeam Search Equation 7 always chooses the highest scoring next word candidate at each time step. At test time we can reduce the effect of search error by pursuing not only one but k next word candidates at each point which is commonly known as beam search. While still approximate, this strategy can recover higher scoring sequences that are often also better in terms of our final evaluation metric. The algorithm maintains the k highest scoring partial sequences, where k is a hyper-parameter. Setting k = 1 reduces the algorithm to a greedy left-to-right search (Eq. 7). The downside of such an exploration of multiple paths is that it significantly slows down the generation process. The time complexity grows linearly in k because we need to perform k forward passes for our network which is the most time intensive operation. As a result, beam search generation is k times slower than greedy search (Eq. 7). Pseudo-code of beam search is shown in Algorithm 2 of Supplementary Material."}, {"heading": "3.1.2 DATA AS DEMONSTRATOR (DAD)", "text": "Conventional training with XENT suffers from exposure bias since training uses ground truth words as opposed to model predictions. DAD, proposed in (Venkatraman et al., 2015) and also used in (Bengio et al., 2015) for sequence generation, addresses this issue by mixing the ground truth training data with model predictions.\nAt each time step and with a certain probability, DAD takes as input either the prediction from the model at the previous time step or the ground truth data. Bengio et al. (2015) proposed different annealing schedules for the probability of choosing the ground truth word. The annealing schedules are such that at the beginning, the algorithm always chooses the ground truth words. However, as the training progresses the model predictions are selected more often. This has the effect of making the model somewhat more aware of how it will be used at test time. Figure 2 illustrates the algorithm.\nA major limitation of this approach is that at every time step the target labels are always selected from the ground truth data, regardless of how the input word is chosen. As a result, the targets may not be aligned with the generated sequence. This in turn will forcefully train the model to predict a potentially incorrect sequence. For instance, if the ground truth sequence is \u201cI took a long walk\u201d and the model has so far predicted \u201cI took a walk\u201d, DAD will force the model to predict the word \u201cwalk\u201d a second time, since it is the next word in the ground truth sequence. Finally, gradients are not back-propagated through the samples drawn by the model and the XENT loss is still at the word level. It is not well understood how these problems affect generation."}, {"heading": "3.1.3 END-TO-END BACKPROP (E2E)", "text": "In our quest to bridge the gap between the way the text generation models are trained and the way they are used, we also experimented with a novel modification to the standard training of RNNs. This is perhaps the most natural and naive approach approximating sequence level training, which can also be interpreted as computationally efficient approximation to beam search. The key idea is that at time step t + 1 we propagate as input the top k words predicted at the previous time step instead of the ground truth word. Specifically, we take the output distribution over words from the\nprevious time step t, and pass it through a k-max layer. This layer zeros all but the the k largest values and re-normalizes them to sum to one. The re-normalized distribution is used as input at the current time step: {it+1,j , vt+1,j}j=1,...,k = k-max p\u03b8(wt+1|wt, ht), (8) where it+1,j are indexes of the words with k highest probabilities and vt+1,j are their corresponding scores. At the next time step, instead of taking the ground truth word as input, we take the k highest scoring previous words as input whose contributions is weighted by their scores. Smoothing the input in this way makes the whole process differentiable and trainable using standard backpropagation of the error using the cross-entropy loss of Equation 6. Compared to beam search, this can be interpreted as fusing the k possible next hypothesis together into a single path, as illustrated in Figure 3. In practice, we also employ a schedule, whereby we let the model use its own top-k predictions more and more as training proceeds. At the beginning it uses only ground truth words. After a few epochs, we use top-k predictions for the last \u2206 steps of the sequence. Afterwards, the RNN uses its own predictions for the last 2\u2206 steps, on so on so forth.\nWhile this algorithm is a simple way to expose the model to its own predictions, the loss function optimized is still XENT at each time step, and therefore, it operates at the word level. There is no explicit supervision at the sequence level while training the model."}, {"heading": "3.2 SEQUENCE LEVEL TRAINING", "text": "We now introduce a novel algorithm for sequence level training, which we call Mixed Incremental Cross-Entropy Reinforce (MIXER). The proposed method not only avoids the exposure bias problem, but it also directly optimizes for the final evaluation metric, namely, BLEU. Since MIXER can be viewed as an extension of the REINFORCE algorithm, we first describe the REINFORCE algorithm from the perspective of sequence generation."}, {"heading": "3.2.1 REINFORCE", "text": "In order to apply the REINFORCE algorithm (Williams, 1992; Zaremba & Sutskever, 2015) to the problem of sequence generation we cast our problem in the reinforcement learning (RL) framework (Sutton & Barto, 1988). Our generative model (the RNN) can be viewed as an agent, which interacts with the external environment (the words and the context vector it sees as input at every time step). The parameters of this agent defines a policy, whose execution results in the agent picking an action. In the sequence generation setting, an action would refer to predicting the next word in the sequence at each time step. After taking an action the agent updates its internal state (the state of the hidden units of RNN). Once the agent has reached the end of a sequence, it observes a reward. We can choose any reward function. Here, we use BLEU (Papineni et al., 2002) since this is the metric we use at test time. BLEU is essentially a geometric mean over n-gram precision scores as well as a brevity penalty. In order to avoid our reward function to evaluate to zero for sequences where there are no higher order n-gram matches, we resort to smoothing the BLEU score, similarly to Liang et al. (2006).2 Like in imitation learning, we have a training set of optimal sequences of\n2The smoothing constant is set to 0.1.\nactions. During training we choose actions according to the current policy and only observe a reward at the end of the sequence (or after maximum sequence length), by comparing the sequence of actions from the current policy against the optimal action sequence. The goal of training is to find the parameters of the agent that maximize the expected reward.\nWe define our loss as the negative expected reward: L\u03b8 = \u2212 \u2211\nwg1 ,...,w g T\np\u03b8(w g 1 , . . . , w g T )r(w g 1 , . . . , w g T ) = \u2212E[wg1 ,...wgT ]\u223cp\u03b8r(w g 1 , . . . , w g T ), (9)\nwhere wgn is the word chosen by our model at the n-th time step, and r is the reward associated with the generated sequence. In practice, we approximate this expectation with a single sample from the distribution of actions implemented by the RNN (right hand side of the equation above). We refer the reader to prior work (Zaremba & Sutskever, 2015; Williams, 1992) for the full derivation of the gradients. Here, we directly report the partial derivatives and their interpretation. The derivatives w.r.t. parameters are:\n\u2202L\u03b8 \u2202\u03b8 = \u2211 t \u2202L\u03b8 \u2202ot \u2202ot \u2202\u03b8\n(10)\nwhere ot is the input to the softmax as in Equation 5. The gradient of the loss L\u03b8 with respect to ot is given by:\n\u2202L\u03b8 \u2202ot = (r(wg1 , . . . , w g T )\u2212 r\u0304t+1)(p\u03b8(wt+1|wgt ,ht+1, ct)\u2212 1(wgt+1)), (11)\nwhere r\u0304t+1 is the average reward at time t+ 1.\nThe interpretation of this weight update rule is straightforward. While Equation 10 is standard backpropagation (a.k.a. chain rule), Equation 11 is almost exactly the same as the gradient of a multiclass logistic regression classifier. In logistic regression, the gradient is the difference between the prediction and the actual 1-of-N representation of the target word:\n\u2202LXENT\u03b8 \u2202ot = p\u03b8(wt+1|wt,ht+1, ct)\u2212 1(wt+1)\nTherefore, Equation 11 says that the chosen word wgt+1 acts like a surrogate target for our output distribution, p\u03b8(wt+1|wgt ,ht+1, ct) at time t. REINFORCE first establishes a baseline r\u0304t+1, and then either encourages a word choice wgt+1 if r > r\u0304t+1, or discourages it if r < r\u0304t+1. The actual derivation suggests that the choice of this average reward r\u0304t is useful to decrease the variance of the gradient estimator since in Equation. 9 we use a single sample from the distribution of actions.\nIn our implementation, the baseline r\u0304t is estimated by a linear regressor which takes as input the hidden states ht of the RNN. The parameters of the regressor are trained by minimizing the mean squared loss: ||r\u0304t \u2212 r||2. In order to prevent feedback loops, we do not backpropagate this error through the recurrent network (Zaremba & Sutskever, 2015).\nREINFORCE is an elegant algorithm to train at the sequence level using any user-defined reward. In the present exposition we use the BLEU metric as the reward, however one could just as easily use any other metric, such as, ROUGE or METEOR. When presented as is, one major drawback associated with the algorithm is that it assumes a random policy to start with. This assumption can make the learning for large action spaces very challenging. Unfortunately, text generation is such a setting where the cardinality of the action set is in the order of 104 (the number of words in the vocabulary). This leads to a very high branching factor where it is extremely hard for a random policy to improve in any reasonable amount of time. In the next section we describe the MIXER algorithm which addresses these issues, better targeting text generation applications."}, {"heading": "3.2.2 MIXED INCREMENTAL CROSS-ENTROPY REINFORCE (MIXER)", "text": "The MIXER algorithm borrows ideas both from DAGGER (Ross et al., 2011) and DAD (Venkatraman et al., 2015; Bengio et al., 2015) and modifies the REINFORCE appropriately. The first key idea is to change the initial policy of REINFORCE to make sure the model can effectively deal with the large action space of text generation. Instead of starting from a poor random policy and training the model to converge towards the optimal policy, we do the exact opposite. We start from\nData: a set of sequences w1, . . . , wT , with their corresponding context c. Result: RNN optimized for generation initialize RNN at random and set NXENT, NXE+R and \u2206; for s = T , 1, \u2212\u2206 do\nif s == T then train RNN for NXENT epochs using XENT only; else train RNN for NXE+R epochs. Use XENT loss in the first s steps, and REINFORCE (sampling from the model) in the remaining T \u2212 s steps;\nend end\nAlgorithm 1: MIXER pseudo-code.\nthe optimal policy and then slowly deviate from it to let the model explore and make use of its own predictions. We first train the RNN with the usual cross-entropy loss for NXENT epochs using the ground truth sequences. This ensures that we start off with a much better policy than random because now the model can focus on a good part of the search space. This can be better understood by comparing the perplexity of a language model that is randomly initialized versus one that is trained. Perplexity is a measure of uncertainty of the prediction and, roughly speaking, it corresponds to the average number of words the model is \u2018hesitating\u2019 about when making a prediction. A good language model trained on one of our datasets has perplexity of 50, whereas a random model is likely to have perplexity close to the size of the vocabulary, which is about 104.\nThe second idea is to introduce model predictions during training with an annealing schedule in order to gradually teach the model to produce stable sequences. Let T be the length of the sequence. After the initial NXENT epochs, we continue training the model for NXE+R epochs, such that, for every sequence we use the XENT loss for the first (T \u2212\u2206) steps, and the REINFORCE algorithm for the remaining \u2206 steps. In our experiments \u2206 is typically set to two or three. Next we anneal the number of steps for which we use the XENT loss for every sequence to (T \u2212 2\u2206) and repeat the training for another NXE+R epochs. We repeat this process until only REINFORCE is used to train the whole sequence. See Algorithm 1 for exact details.\nWe call this algorithm Mixed Incremental Cross-Entropy Reinforce (MIXER) because we combine both XENT and REINFORCE, and we use incremental learning (a.k.a. curriculum learning). The overall algorithm is illustrated in Figure 4. By the end of training, the model can make effective use of its own predictions, consistently to its use at test time."}, {"heading": "4 EXPERIMENTS", "text": "In all our experiments, we train conditional RNNs by unfolding them up to a certain maximum length. We chose this length to cover about 95% of the target sentences in the datasets we consider. The remaining sentences are cropped to the chosen maximum length. For training, we use stochastic\ngradient descent with mini-batches of size 32 and we reset the hidden states at the beginning of each sequence. Before updating the parameters we re-scale the gradients if their norm is above 10 (Mikolov et al., 2010).\nFor all the tasks we use the same architecture, namely, a standard RNN with 128 hidden units. We search over the values of other hyper-parameter, such as the initial learning rate, the various scheduling parameters, number of epochs, etc., using a separate validation set. We then take the model that performed best on the validation set and compute the BLEU score (computing precision up to at most tri-grams) on the test set. In the following sections we report results on the test set only. Greedy generation is performed by taking the most likely word at each time step."}, {"heading": "4.1 TEXT SUMMARIZATION", "text": "For the summarization task, we only consider the problem of abstractive summarization, where, given a piece of \u201csource\u201d text, we aim at generating its summary (the \u201ctarget\u201d text). The dataset we use to train and evaluate our models consists of a subset of the Gigaword corpus (Graff et al., 2003) as described in Rush et al. (2015). This is a collection of news articles taken from different sources over the past two decades. Our version is organized as a set of example pairs, where each pair is composed of the first sentence of a news article (the source sentence) and the headline of the corresponding news article (the target sentence). The summarization task then reduces to generating the target sentence given the source sentence. We apply the same pre-processing described in (Rush et al., 2015), which consists of lower-casing and removal of very infrequent words. Infrequent words are mapped to a special token denoted by \u201c<unk>\u201d. There are 12321 unique words in the source dictionary and 6828 unique words in the target dictionary. The number of sample pairs in the training set is 179414, 22568 in the validation set, and 22259 in the test set. The average sequence length of the target headlines is about 10. We considered sequences up to 15 words to comply with our initial constraint of covering about 95% of the data.\nOur generative model is a conditional RNN, where the conditioning is provided by a convolutional attention module similar to the one described in (Rush et al., 2015). The words in the source context are embedded and averaged over windows of size 5, yielding vectors st. Then, the actual context vector ct is computed as a weighted sum of these st, where the weights are computed via a softmax on the dot products between the current hidden state ht and the vectors st themselves, a mechanism known as attention (Bahdanau et al., 2015). Note that in the final results we used a standard RNN architecture to generate the target headlines. We also tried LSTMs as our generative model, however they did not improve performance. We conjecture that this might be due to the fact that the target sentences are rather short for this task."}, {"heading": "4.2 MACHINE TRANSLATION", "text": "For the translation task, we chose the same model architecture as for the summarization task. We use data from the German-English machine translation track of the IWSLT 2014 evaluation campaign (Cettolo et al., 2014). The corpus consists of sentence-aligned subtitles of TED and TEDx talks. We pre-process the training data using the tokenizer of the Moses toolkit (Koehn et al., 2007) and remove any casing. The training data comprises about 160000 sentences where the average English sentence is 17.5 words long and the average German sentence is 18.5 words long. In order to retain at least 95% of this data, we unrolled our RNN for 25 steps. We concatenated dev2010 and dev2012 to form a validation set of 2052 sentences. The test set is a combination of tst2010, tst2011 and tst2012 and it contains 4698 sentences. The English dictionary has 23328 words while the German dictionary has 32964 words."}, {"heading": "4.3 IMAGE CAPTIONING", "text": "For the image captioning task, we use the MSCOCO dataset (Lin et al., 2014). We use the entire training set provided by the authors, which consists of around 80k images. We then took the original validation set (consisting of around 40k images) and randomly sampled (without replacement) 5000 images for validation and another 5000 for test. There are 5 different captions for each image. At training time we sample one of these captions, while at test time we report the maximum BLEU score across the five captions.\nFor this task, the context is represented by 1024 features extracted by a Convolutional Neural Network (CNN) trained on the Imagenet dataset (Deng et al., 2009); we do not back-propagate through these features. We use a standard RNN as our generative model. Similar to (Bengio et al., 2015), the image features are provided to the generative model as the first word in the sequence. We perform minimal pre-processing of the text data. This includes lower-casing all the words and removing all the words which appear less than 3 times in the training corpus. As a result the total number of unique words in our dataset is 10012. Keeping in mind the 95% rule, we unroll the RNN for 15 steps."}, {"heading": "4.4 RESULTS", "text": "In order to validate MIXER, we compute BLEU score on the test set. For instance, for every document in the test set of the summarization task, we predict the headline and then compute the BLEU score with respect to the ground truth title. The input provided to the system is only the context and the beginning of sentence token. We apply the same protocol to the baseline methods as well. BLEU scores on the test set are reported in Figure 5. We observe that MIXER produces the best generations. MIXER improves generation over XENT by up to 2 BLEU points across all the three different tasks we considered.\nUnfortunately the E2E approach did not prove to be very effective instead. Training at the sequence level and directly optimizing for BLEU yields better generations than turning a sequence of discrete decisions into a differentiable process amenable to standard back-propagation of the error. Finally, DAD is usually better than the XENT, but not as good as MIXER.\nNext, we experimented with beam search. The results in Figure 6 suggest that all methods, including MIXER, improve the quality of their generation by using beam search. However, the extent of the\nimprovement is very much task dependent. We observe that the greedy performance of MIXER (i.e., without beam search) is usually matched by the baseline methods when they use beam search with k equal to 3. However, in this setting, MIXER is 3 times faster since it relies only on greedy search. DAD seems to benefit the most from beam search, but for large value of k all methods do fairly well.\nOn the image captioning task, LSTMs do perform better, yielding a gain of 1 BLEU point over standard RNNs. The gain offered by LSTM and MIXER are complementary to each other though. An LSTMs trained with MIXER yields an additional BLEU score point compared to a standard RNN trained with MIXER, which is still two BLEU points better than an LSTM trained by XENT.\nIt is worth mentioning that the REINFORCE baseline did not work for these applications. Exploration from a random policy has too little chance of success. We do not report it since we were never able to make it converge. Using a the hybrid XENT-REINFORCE loss without incremental learning is also insufficient to make training take off from random chance (within a reasonable waiting time of several tens of epochs). In order to gain some insight on what kind of schedule works, we report in Table 2 of Supplementary Material the best values we found after grid search over the hyper-parameters of MIXER. Finally, we report some examples of generations in Figure 8 of Supplementary Material, showing that also qualitatively MIXER generally produces better generations."}, {"heading": "5 CONCLUSIONS", "text": "Our work is motivated by two major deficiencies in training the current generative models for text generation: exposure bias and a loss which does not operate at the sequence level. Reinforcement learning is a framework that can address these issues. First, at training time the model is used to generate an entire sequence of actions. Second, the reward does not need to factor over individual words nor does it need to be differentiable. Therefore, we can easily and directly operate at the sequence level, generate at training time and optimize our model towards BLEU, our test time evaluation metric. One challenge with reinforcement learning is that it struggles with very large action spaces such as for text generation.\nThe algorithm we propose, MIXER, deals with this issue and enables successful training of reinforcement learning models for text generation. We achieve this by replacing the initial random policy with the optimal policy of a cross-entropy trained model and by gradually exposing the model more and more to its own predictions in an incremental learning framework.\nOur results show that MIXER outperforms three strong baselines for greedy generation and it is very competitive with beam search. The approach we propose is agnostic to the underlying model or the form of the reward function.\nIn future work we would like to design better estimation techniques for the average reward r\u0304t, because poor estimates can lead to slow convergence of both REINFORCE and MIXER. Finally, our training algorithm relies on a single sample while it would be interesting to investigate the effect of more comprehensive search methods at training time."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank David Grangier, Tomas Mikolov, Leon Bottou, Ronan Collobert and Laurens van der Maaten for their insightful comments. We also would like to thank Alexander M. Rush for his help in preparing the data set for the summarization task."}, {"heading": "6 SUPPLEMENTARY MATERIAL", "text": ""}, {"heading": "6.1 MODELS", "text": "Input: model p\u03b8, beam size k Result: sequence of words [wg1 , w g 2 , . . . , w g n] empty heaps {Ht}t=1,...T ; an empty hidden state vector h1; H1.push(1, [[\u2205],h1]); for t\u2190 1 to T \u2212 1 do\nfor i\u2190 1 to min(k,#Ht) do (p, [[w1, w2, . . . , wt],h])\u2190 Ht.pop(); h\u2032 = \u03c6\u03b8(w,h) ; for w\u2032 \u2190 k-most likely words w\u2032 from p\u03b8(w\u2032|wt,h) do\np\u2032 = p \u2217 p\u03b8(w\u2032|w,h); Ht+1.push(p\u2032, [[w1, w2, . . . , wt, w\u2032],h\u2032]);\nend end\nend (p, [[w1, w2, . . . , wT ],h])\u2190 HT .pop(); Output: [w1, w2, . . . , wT ]\nAlgorithm 2: Pseudo-code of beam search with beam size k."}, {"heading": "6.2 EXPERIMENTS", "text": "CONTEXT: masked gunmen opened fire on a palestinian minister and a top economic official in a jenin restaurant wednesday , the latest in a series attacks on palestinian officials in the increasingly lawless west bank GROUND TRUTH: palestinian cabinet minister survives restaurant shooting XENT: gunmen kill palestinian minister DAD: gunmen kill palestinian , top official in west bank E2E: gunmen kill palestinian , palestinian minister MIXER: gunmen kill palestinian minister in jenin\nCONTEXT: a chinese government official on sunday dismissed reports that the government was delaying the issuing of third generation -lrb- #g -rrb- mobile phone licenses in order to give a developing <unk> system an advantage GROUND TRUTH: foreign phone operators to get equal access to china \u2019s #g market XENT: china dismisses report of #g mobile phone phone DAD: china denies <unk> <unk> mobile phone licenses E2E: china \u2019s mobile phone licenses delayed MIXER: china official dismisses reports of #g mobile licenses\nCONTEXT: greece risks bankruptcy if it does not take radical extra measures to fix its finances , prime minister george papandreou warned on tuesday , saying the country was in a \u2018\u2018 wartime situation GROUND TRUTH: greece risks bankruptcy without radical action XENT: greece warns <unk> measures to <unk> finances DAD: greece says no measures to <unk> <unk> E2E: greece threatens to <unk> measures to <unk> finances MIXER: greece does not take radical measures to <unk> deficit\nCONTEXT: the indonesian police were close to identify the body parts resulted from the deadly explosion in front of the australian embassy by the dna test , police chief general <unk> <unk> said on wednesday GROUND TRUTH: indonesian police close to <unk> australian embassy bomber XENT: indonesian police close to <unk> DAD: indonesian police close to <unk> E2E: indonesian police close to monitor deadly australia MIXER: indonesian police close to <unk> parts of australian embassy\nCONTEXT: hundreds of catholic and protestant youths attacked security forces with <unk> bombs in a flashpoint area of north belfast late thursday as violence erupted for the second night in a row , police said GROUND TRUTH: second night of violence erupts in north belfast XENT: urgent hundreds of catholic and <unk> <unk> in <unk> DAD: hundreds of belfast <unk> <unk> in n. belfast E2E: hundreds of catholic protestant , <unk> clash with <unk> MIXER: hundreds of catholic <unk> attacked in north belfast\nCONTEXT: uganda \u2019s lord \u2019s resistance army -lrb- lra -rrb- rebel leader joseph <unk> is planning to join his commanders in the ceasefire area ahead of talks with the government , ugandan army has said GROUND TRUTH: rebel leader to move to ceasefire area XENT: uganda \u2019s <unk> rebel leader to join ceasefire DAD: ugandan rebel leader to join ceasefire talks E2E: ugandan rebels <unk> rebel leader MIXER: ugandan rebels to join ceasefire in <unk>\nCONTEXT: a russian veterinary official reported a fourth outbreak of dead domestic poultry in a suburban moscow district sunday as experts tightened <unk> following confirmation of the presence of the deadly h#n# bird flu strain GROUND TRUTH: tests confirm h#n# bird flu strain in # <unk> moscow <unk> XENT: russian official reports fourth flu in <unk> DAD: bird flu outbreak in central china E2E: russian official official says outbreak outbreak outbreak in <unk> MIXER: russian official reports fourth bird flu\nCONTEXT: a jewish human rights group announced monday that it will offer <unk> a dlrs ##,### reward for information that helps them track down those suspected of participating in nazi atrocities during world war ii GROUND TRUTH: jewish human rights group offers reward for information on nazi suspects in lithuania XENT: jewish rights group announces <unk> to reward for war during world war DAD: rights group announces <unk> dlrs dlrs dlrs reward E2E: jewish rights group offers reward for <unk> MIXER: jewish human rights group to offer reward for <unk>\nCONTEXT: a senior u.s. envoy reassured australia \u2019s opposition labor party on saturday that no decision had been made to take military action against iraq and so no military assistance had been sought from australia GROUND TRUTH: u.s. envoy meets opposition labor party to discuss iraq XENT: australian opposition party makes progress on military action against iraq DAD: australian opposition party says no military action against iraq E2E: us envoy says no decision to take australia \u2019s labor MIXER: u.s. envoy says no decision to military action against iraq"}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["S. Bengio", "O. Vinyals", "N. Jaitly", "N. Shazeer"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Report on the 11th iwslt evaluation campaign", "author": ["M. Cettolo", "J. Niehues", "S. St\u00fcker", "L. Bentivogli", "M. Federico"], "venue": "In Proc. of IWSLT,", "citeRegEx": "Cettolo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cettolo et al\\.", "year": 2014}, {"title": "Search-based structured prediction as classification", "author": ["H. Daume III", "J. Langford", "D. Marcu"], "venue": "Machine Learning Journal,", "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Imagenet: a large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.J. Li", "K. Li", "L. Fei-Fei"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Finding structure in time", "author": ["Elman", "Jeffrey L"], "venue": "Cognitive Science,", "citeRegEx": "Elman and L.,? \\Q1990\\E", "shortCiteRegEx": "Elman and L.", "year": 1990}, {"title": "Improved backing-off for M-gram language modeling", "author": ["Kneser", "Reinhard", "Ney", "Hermann"], "venue": "In Proc. of the International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Kneser et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kneser et al\\.", "year": 1995}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Koehn", "Philipp", "Hoang", "Hieu", "Birch", "Alexandra", "Callison-Burch", "Chris", "Federico", "Marcello", "Bertoldi", "Nicola", "Cowan", "Brooke", "Shen", "Wade", "Moran", "Christine", "Zens", "Richard", "Dyer", "Bojar", "Ondrej", "Constantin", "Herbst", "Evan"], "venue": "In Proc. of ACL Demo and Poster Sessions,", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "An end-to-end discriminative approach to machine translation", "author": ["Liang", "Percy", "Bouchard-C\u00f4t\u00e9", "Alexandre", "Taskar", "Ben", "Klein", "Dan"], "venue": null, "citeRegEx": "Liang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Microsoft coco: Common objects in context", "author": ["T.Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Dollar", "C.L. Zitnick"], "venue": "Technical report,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Direct loss minimization for structured prediction", "author": ["D. McAllester", "T. Hazan", "J. Keshet"], "venue": "In NIPS,", "citeRegEx": "McAllester et al\\.,? \\Q2010\\E", "shortCiteRegEx": "McAllester et al\\.", "year": 2010}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafit", "L. Burget", "J. Cernock", "S. Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "In AISTATS,", "citeRegEx": "Morin and Bengio,? \\Q2005\\E", "shortCiteRegEx": "Morin and Bengio", "year": 2005}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.J. Zhu"], "venue": "In ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["S. Ross", "G.J. Gordon", "J.A. Bagnell"], "venue": "In AISTATS,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Expected bleu training for graphs: Bbn system description for wmt11 system combination task", "author": ["Rosti", "Antti-Veikko I", "Zhang", "Bing", "Matsoukas", "Spyros", "Schwartz", "Richard"], "venue": "In Proc. of WMT,", "citeRegEx": "Rosti et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rosti et al\\.", "year": 2011}, {"title": "Learning internal representations by backpropagating", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "errors. Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["A.M. Rush", "S. Chopra", "J. Weston"], "venue": "In EMNLP,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc"], "venue": "In Proc. of NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1988\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1988}, {"title": "Improving multi-step prediction of learned time series models", "author": ["A. Venkatraman", "M. Hebert", "J.A. Bagnell"], "venue": "In AAAI,", "citeRegEx": "Venkatraman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venkatraman et al\\.", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams,? \\Q1992\\E", "shortCiteRegEx": "Williams", "year": 1992}, {"title": "Reinforcement learning neural turing machines", "author": ["W. Zaremba", "I. Sutskever"], "venue": "Technical report,", "citeRegEx": "Zaremba and Sutskever,? \\Q2015\\E", "shortCiteRegEx": "Zaremba and Sutskever", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "Popular choices for text generation models are language models based on n-grams (Kneser & Ney, 1995), feed-forward neural networks (Morin & Bengio, 2005), and recurrent neural networks (RNNs; Mikolov et al., 2010).", "startOffset": 185, "endOffset": 213}, {"referenceID": 15, "context": "Training these models to directly optimize for BLEU is hard because a) it is not differentiable (Rosti et al., 2011), and b) combinatorial optimization is required to determine which sub-string maximizes BLEU given some context.", "startOffset": 96, "endOffset": 116}, {"referenceID": 10, "context": "Prior attempts (McAllester et al., 2010) at optimizing BLEU have been restricted to linear models.", "startOffset": 15, "endOffset": 40}, {"referenceID": 10, "context": "Prior attempts (McAllester et al., 2010) at optimizing BLEU have been restricted to linear models. This paper proposes a novel training algorithm which results in improved text generation compared to standard models. The algorithm tries to address the two issues discussed above. First, while training the generative model we avoid the exposure bias by using model predictions at training time. Second, we directly optimize for our final evaluation metric, namely BLEU. Our proposed methodology borrows ideas from the reinforcement learning literature (Sutton & Barto, 1988). In particular, we build on the REINFORCE algorithm proposed by Williams (1992), to achieve the", "startOffset": 16, "endOffset": 655}, {"referenceID": 1, "context": "We compare against several strong baselines, including, RNNs trained with cross-entropy and Data as Demonstrator (DAD) (Bengio et al., 2015; Venkatraman et al., 2015).", "startOffset": 119, "endOffset": 166}, {"referenceID": 20, "context": "We compare against several strong baselines, including, RNNs trained with cross-entropy and Data as Demonstrator (DAD) (Bengio et al., 2015; Venkatraman et al., 2015).", "startOffset": 119, "endOffset": 166}, {"referenceID": 18, "context": "At test time, it is common to use beam search to explore multiple alternative paths (Sutskever et al., 2014; Bahdanau et al., 2015; Rush et al., 2015).", "startOffset": 84, "endOffset": 150}, {"referenceID": 0, "context": "At test time, it is common to use beam search to explore multiple alternative paths (Sutskever et al., 2014; Bahdanau et al., 2015; Rush et al., 2015).", "startOffset": 84, "endOffset": 150}, {"referenceID": 17, "context": "At test time, it is common to use beam search to explore multiple alternative paths (Sutskever et al., 2014; Bahdanau et al., 2015; Rush et al., 2015).", "startOffset": 84, "endOffset": 150}, {"referenceID": 13, "context": "While this improves generation by typically one or two BLEU points (Papineni et al., 2002), it also comes at a cost: it makes generation at least k times slower, where k is the number of active paths in the beam (see Sec.", "startOffset": 67, "endOffset": 90}, {"referenceID": 20, "context": "The oracle issue was later addressed by an algorithm called Data As Demonstrator (DAD) (Venkatraman et al., 2015), whereby the target action at step k is the k-th action taken by the optimal policy (ground truth sequence) regardless of which input is fed to the system, whether it is ground truth, or the model\u2019s prediction.", "startOffset": 87, "endOffset": 113}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Rush et al., 2015). While this improves generation by typically one or two BLEU points (Papineni et al., 2002), it also comes at a cost: it makes generation at least k times slower, where k is the number of active paths in the beam (see Sec. 3.1.1 for more details). The key idea of this work is to improve generation by letting the model use its own predictions at training time, as first advocated by Daume III et al. (2009). In their seminal work, the authors first noticed that structured prediction problems can be cast as a particular instance of reinforcement learning, and they then proposed SEARN, an algorithm to learn such structured prediction tasks.", "startOffset": 8, "endOffset": 458}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Rush et al., 2015). While this improves generation by typically one or two BLEU points (Papineni et al., 2002), it also comes at a cost: it makes generation at least k times slower, where k is the number of active paths in the beam (see Sec. 3.1.1 for more details). The key idea of this work is to improve generation by letting the model use its own predictions at training time, as first advocated by Daume III et al. (2009). In their seminal work, the authors first noticed that structured prediction problems can be cast as a particular instance of reinforcement learning, and they then proposed SEARN, an algorithm to learn such structured prediction tasks. The basic idea is to let the model use its own predictions at training time to produce a sequence of actions (e.g., the choice of the next word). Then, a search algorithm is run to determine the optimal action at each time step, and a classifier (a.k.a. policy) is trained to predict that action. A similar idea was later proposed by Ross et al. (2011) in an imitation learning framework.", "startOffset": 8, "endOffset": 1047}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Rush et al., 2015). While this improves generation by typically one or two BLEU points (Papineni et al., 2002), it also comes at a cost: it makes generation at least k times slower, where k is the number of active paths in the beam (see Sec. 3.1.1 for more details). The key idea of this work is to improve generation by letting the model use its own predictions at training time, as first advocated by Daume III et al. (2009). In their seminal work, the authors first noticed that structured prediction problems can be cast as a particular instance of reinforcement learning, and they then proposed SEARN, an algorithm to learn such structured prediction tasks. The basic idea is to let the model use its own predictions at training time to produce a sequence of actions (e.g., the choice of the next word). Then, a search algorithm is run to determine the optimal action at each time step, and a classifier (a.k.a. policy) is trained to predict that action. A similar idea was later proposed by Ross et al. (2011) in an imitation learning framework. Unfortunately, for text generation it is generally intractable to compute an oracle of the optimal target word given the words predicted so far. The oracle issue was later addressed by an algorithm called Data As Demonstrator (DAD) (Venkatraman et al., 2015), whereby the target action at step k is the k-th action taken by the optimal policy (ground truth sequence) regardless of which input is fed to the system, whether it is ground truth, or the model\u2019s prediction. This idea was also recently tested for text generation applications by Bengio et al. (2015), who had the same motivation as our work (see Sec.", "startOffset": 8, "endOffset": 1645}, {"referenceID": 11, "context": "In this work, we focus on Recurrent Neural Networks (RNNs) as they are a popular choice for text generation (Mikolov et al., 2010).", "startOffset": 108, "endOffset": 130}, {"referenceID": 11, "context": "In this paper, we use standard RNNs (Elman, 1990; Mikolov et al., 2010) unless otherwise specified.", "startOffset": 36, "endOffset": 71}, {"referenceID": 16, "context": "Training proceeds by truncated backpropagation through time (Rumelhart et al., 1986; Mikolov et al., 2010).", "startOffset": 60, "endOffset": 106}, {"referenceID": 11, "context": "Training proceeds by truncated backpropagation through time (Rumelhart et al., 1986; Mikolov et al., 2010).", "startOffset": 60, "endOffset": 106}, {"referenceID": 1, "context": "Figure 2: Illustration of DAD (Bengio et al., 2015; Venkatraman et al., 2015).", "startOffset": 30, "endOffset": 77}, {"referenceID": 20, "context": "Figure 2: Illustration of DAD (Bengio et al., 2015; Venkatraman et al., 2015).", "startOffset": 30, "endOffset": 77}, {"referenceID": 20, "context": "DAD, proposed in (Venkatraman et al., 2015) and also used in (Bengio et al.", "startOffset": 17, "endOffset": 43}, {"referenceID": 1, "context": ", 2015) and also used in (Bengio et al., 2015) for sequence generation, addresses this issue by mixing the ground truth training data with model predictions.", "startOffset": 25, "endOffset": 46}, {"referenceID": 1, "context": ", 2015) and also used in (Bengio et al., 2015) for sequence generation, addresses this issue by mixing the ground truth training data with model predictions. At each time step and with a certain probability, DAD takes as input either the prediction from the model at the previous time step or the ground truth data. Bengio et al. (2015) proposed different annealing schedules for the probability of choosing the ground truth word.", "startOffset": 26, "endOffset": 337}, {"referenceID": 21, "context": "In order to apply the REINFORCE algorithm (Williams, 1992; Zaremba & Sutskever, 2015) to the problem of sequence generation we cast our problem in the reinforcement learning (RL) framework (Sutton & Barto, 1988).", "startOffset": 42, "endOffset": 85}, {"referenceID": 13, "context": "Here, we use BLEU (Papineni et al., 2002) since this is the metric we use at test time.", "startOffset": 18, "endOffset": 41}, {"referenceID": 8, "context": "In order to avoid our reward function to evaluate to zero for sequences where there are no higher order n-gram matches, we resort to smoothing the BLEU score, similarly to Liang et al. (2006).2 Like in imitation learning, we have a training set of optimal sequences of The smoothing constant is set to 0.", "startOffset": 172, "endOffset": 192}, {"referenceID": 21, "context": "We refer the reader to prior work (Zaremba & Sutskever, 2015; Williams, 1992) for the full derivation of the gradients.", "startOffset": 34, "endOffset": 77}, {"referenceID": 14, "context": "The MIXER algorithm borrows ideas both from DAGGER (Ross et al., 2011) and DAD (Venkatraman et al.", "startOffset": 51, "endOffset": 70}, {"referenceID": 20, "context": ", 2011) and DAD (Venkatraman et al., 2015; Bengio et al., 2015) and modifies the REINFORCE appropriately.", "startOffset": 16, "endOffset": 63}, {"referenceID": 1, "context": ", 2011) and DAD (Venkatraman et al., 2015; Bengio et al., 2015) and modifies the REINFORCE appropriately.", "startOffset": 16, "endOffset": 63}, {"referenceID": 11, "context": "Before updating the parameters we re-scale the gradients if their norm is above 10 (Mikolov et al., 2010).", "startOffset": 83, "endOffset": 105}, {"referenceID": 17, "context": "We apply the same pre-processing described in (Rush et al., 2015), which consists of lower-casing and removal of very infrequent words.", "startOffset": 46, "endOffset": 65}, {"referenceID": 17, "context": "Our generative model is a conditional RNN, where the conditioning is provided by a convolutional attention module similar to the one described in (Rush et al., 2015).", "startOffset": 146, "endOffset": 165}, {"referenceID": 0, "context": "Then, the actual context vector ct is computed as a weighted sum of these st, where the weights are computed via a softmax on the dot products between the current hidden state ht and the vectors st themselves, a mechanism known as attention (Bahdanau et al., 2015).", "startOffset": 241, "endOffset": 264}, {"referenceID": 16, "context": ", 2003) as described in Rush et al. (2015). This is a collection of news articles taken from different sources over the past two decades.", "startOffset": 24, "endOffset": 43}, {"referenceID": 2, "context": "We use data from the German-English machine translation track of the IWSLT 2014 evaluation campaign (Cettolo et al., 2014).", "startOffset": 100, "endOffset": 122}, {"referenceID": 7, "context": "We pre-process the training data using the tokenizer of the Moses toolkit (Koehn et al., 2007) and remove any casing.", "startOffset": 74, "endOffset": 94}, {"referenceID": 9, "context": "For the image captioning task, we use the MSCOCO dataset (Lin et al., 2014).", "startOffset": 57, "endOffset": 75}, {"referenceID": 4, "context": "For this task, the context is represented by 1024 features extracted by a Convolutional Neural Network (CNN) trained on the Imagenet dataset (Deng et al., 2009); we do not back-propagate through these features.", "startOffset": 141, "endOffset": 160}, {"referenceID": 1, "context": "Similar to (Bengio et al., 2015), the image features are provided to the generative model as the first word in the sequence.", "startOffset": 11, "endOffset": 32}], "year": 2017, "abstractText": "Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the BLEU score: a popular metric to compare a sequence to a reference. On three different tasks, our approach outperforms several strong baselines for greedy generation, and it matches their performance with beam search, while being several times faster.", "creator": "LaTeX with hyperref package"}}}