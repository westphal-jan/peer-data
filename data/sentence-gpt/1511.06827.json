{"id": "1511.06827", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2015", "title": "GradNets: Dynamic Interpolation Between Neural Architectures", "abstract": "In machine learning, there is a fundamental trade-off between ease of optimization and expressive power. Neural Networks, in particular, have enormous expressive power and yet are notoriously challenging to train. The nature of that optimization challenge changes over the course of learning, especially in the near future. To test this in machines learning, we used the new Artificial Intelligence Engine (AI Engine), a software-based system that makes machine learning the most difficult to train.\n\n\n\n\nMachine learning takes us through an entire series of phases \u2014 learning a series of phases before we begin the training and training. At each stage, we learn to train for training and then train for a final set of training phases that are similar to training in real life. This can be as complex as a train. To learn a training phase, we need to train for two specific tasks \u2014 each of which can be trained in real time for at least three seconds.\n\nFor each of these three phases, the machine learns two basic types of rules for training. First, each training phase represents a learning phase. The second phase represents an important learning phase, meaning that each training phase represents a learning phase. The first training phase represents the learning phase, which requires learning during training time. For the second training phase, the machine learns the learning phase, and then learns the training phase by itself. Then, when the machine learns the training phase, it learns the training phase by itself.\nThe first training phase is an important learning phase. As with the other training phase, the machine learns the learning phase by itself. In this case, the machine learns the training phase by itself. As we learn the training phase, the machine learns the training phase by itself. This way, the machine learns the training phase by itself, because it has learned the training phase by itself.\nThe machine learns the training phase by itself. This way, the machine learns the training phase by itself. This way, the machine learns the training phase by itself. This way, the machine learns the training phase by itself. This way, the machine learns the training phase by itself. This way, the machine learns the training phase by itself. This way, the machine learns the training phase by itself. This way, the machine learns the training phase by itself. This way, the machine learns the training phase by itself. This way, the machine learns the training phase by itself. This way, the machine learns the training phase by itself.\nMachine learning is also the best training process in all of the", "histories": [["v1", "Sat, 21 Nov 2015 03:50:49 GMT  (106kb,D)", "http://arxiv.org/abs/1511.06827v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["diogo almeida", "nate sauder"], "accepted": false, "id": "1511.06827"}, "pdf": {"name": "1511.06827.pdf", "metadata": {"source": "CRF", "title": "GRADNETS: DYNAMIC INTERPOLATION BETWEEN NEURAL ARCHITECTURES", "authors": ["Diogo Almeida", "Nate Sauder"], "emails": ["diogo@enlitic.com", "nate@enlitic.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Deep learning has become the state of the art system for many machine learning challenges (Krizhevsky et al. (2012), Girshick et al. (2014)). Many of deep learning's benefits come from its ability to automatically learn abstract representations from raw input features, its scalability to large datasets, and its exponential growth in expressive power with depth. As a consequence of this great expressive power, deep learning can suffer from slow convergence, over-fitting, poor optima, difficulty of optimization, and acute sensitivity to initialization and hyper-parameters. Deep learning practitioners often optimize for the best possible generalization.\nDropout and rectified linear units (ReLUs) are two of the more important algorithmic breakthroughs of the last 5 years. Indeed, both were essential to the breakthrough results of Krizhevsky et al. (2012) that reduced the error rate on the ImageNet Challenge of 2012 by almost 40%. Both of these techniques sacrifice ease of optimization early in training in exchange for expressive power. Dropout's addition of stochastic noise improves performance by enabling larger networks and preventing coadaptation of weights albeit at a significant cost of convergence speed (Hinton et al., 2012). Despite being easier to optimize than sigmoids or hyperbolic tangents, ReLUs (Nair & Hinton, 2010) increase the expressive power of networks while being more difficult to initialize and optimize than linear models (Saxe et al., 2013) or leaky ReLUs (Maas et al., 2013).\nTraditionally, architectures are chosen before training and remain static throughout training. Machine learning practitioners often choose expressive power and generalizability over ease of training in these static architectures.\nGradNets, dynamic interpolation between architecture decisions during training, allows for architectures that maintain the desired expressive power and generalizability but also benefit from easier optimization.\nIn particular, applying the GradNets framework to both dropout and ReLUs have led to improved accuracy on CIFAR-10, convergence with levels of dropout that static networks diverge on, and successful training of multi-layer perceptrons of up to 200 layers on MNIST through smoother information flow.\n\u2217Equal contribution. Author ordering determined by coin flip.\nar X\niv :1\n51 1.\n06 82\n7v 1\n[ cs\n.L G\n] 2\n1 N\nov 2\n01 5"}, {"heading": "2 METHOD DESCRIPTION", "text": "A GradNet architectural component is exceptionally simple: a weighted mean of two architectural components where the weight g is annealed from 0 to 1 over training. For example, a gradual ReLU anneals from an identity layer to a ReLU layer (see Figure 1).\nIn this paper, we use a linear schedule with a single hyperparameter \u03c4 as the epoch when the annealing is complete:\ng = min(t/\u03c4, 1)\nwhere t is the epoch number.\nThe GradNet framework is exceptionally flexible as it can be applied to any pair of network components that result in a tensor of the same shape. Our examples can be found in Table 1."}, {"heading": "3 EXPERIMENTS", "text": ""}, {"heading": "3.1 BASELINE ARCHITECTURE", "text": ""}, {"heading": "3.1.1 CIFAR-10", "text": "Experiments in sections 3.2 through 3.7 were performed on CIFAR-10. Data augmentation was applied with a 50% probability to horizontally flip the input image. We employ the network topology of Scalable Bayesian Optimization Using Deep Neural Networks (Snoek et al., 2015). When not using gradual dropout, activations after each convolutional layer were dropped out with probability 0.3. When Batch Normalization was used, the mean and variance at test time were attained by taking an exponential moving average of train time means and variances. The network was optimized using Adam with a batch size of 256 and early stopping and initialized using orthogonal initialization. \u03c4 for GradNets was set to be 100 epochs."}, {"heading": "3.1.2 MNIST", "text": "Experiments in section 3.8 were performed on MNIST. A multilayer perceptron was used with 512 units per hidden layer, which was optimized with Adam with a batch size of 500 and early stopping and initialized using orthogonal initialization. \u03c4 for GradNets was set to be 5 epochs."}, {"heading": "3.1.3 CLUTTERED MNIST", "text": "Experiments in section 3.9 were performed on Cluttered MNIST(Mnih et al., 2014). An architecture with 2 convolutional layers, each 3x3 with 32 filters followed by a max pooling layer, and 2 fully connected layers, with 256 and 10 units respectively, was used to classify 20x20 pixel images. The input 60x60 pixel images were downsampled either with 3x3 mean pooling or an affine Spatial Transformer Network (Jaderberg et al., 2015), whose localization network had an architecture of 2 convolutional layers, each 5x5 with 20 filters followed by a max pooling layer, and 2 fully connected layers, with 50 and 10 units respectively with 512 units per hidden layer, Models were optimized with Adam with a batch size of 500 and early stopping and initialized using orthogonal initialization. \u03c4 for GradNets was set to be 5 epochs."}, {"heading": "3.2 GRADUAL RELUS (GRELUS)", "text": "ReLUs are the dominant form of nonlinearity in modern deep neural networks, however they do not allow gradients to flow well leading to several proposed solutions: leaky and very leaky ReLUs or ReLUs with parameterized slope for the negative part (He et al., 2015). On the other hand, linear networks are easier to optimize due to their convexity, but don't allow for the learning of abstract hierarchical feature detectors. By beginning with fully linear networks and interpolating towards fully ReLU networks, we gain the ease of optimization of linear networks and also the expressive power of ReLU networks. We call them Gradual ReLUs.\nInstead of interpolating between two separate outputs, it would be equivalent to interpolate the slope of the negative part of leaky ReLUs resulting in a technique with no overhead over traditional ReLU networks.\nTable 2 shows that applying GReLUs to a large CNN architecture shows that they outperform not only ReLUs, but also their leaky, very leaky, and parameteric variants.\nIn addition to testing the interpolation between a network with no nonlinearity and a ReLU, we also interpolated between an absolute value and ReLU (see Inverse GReLU in Table 2). This is equivalent to annealing from a ReLU leak of -1 instead of 1. The standard GReLU outperforms this architecture, showing that the initial linearity is important, but this architecture also outperforms standard ReLUs. One possible reason for this is that having a network be dynamic changes the trajectory of optimization for the better."}, {"heading": "3.3 GRADUAL DROPOUT", "text": "Stochastic dropout is an excellent regularizer but negatively affects convergence speed so much so that some state of the art architectures even remove it (Ioffe & Szegedy, 2015). Furthermore, with sufficiently high levels of dropout, networks don't train at all. Thus, there is a balance between regularizing networks that are easily overfit and the ability for a network to be optimized. In gradual dropout, beginning training with zero dropout and annealing up to higher levels (even as high as\n0.9) means that early training from randomly initialized weights does not suffer from excessive noise while the final networks can be sufficiently large yet regularized to generalize well to other domains.\nTable 3 shows that, similarly to with GReLUs, applying gradual dropout to the same CNN architecture shows that it not only outperforms using a static amount of dropout, but allows using much larger amounts of dropout to gain better performance. This is despite \u03c4 appearing to be too low - the results for p = 0.9 show that the GradNet interpolates too aggressively.\nA future extension of this technique could be used to find the optimal amount of dropout for a network in a single training run by seeing at which point the network's performance starts to drop, instead of relying on an expensive hyperparameter search."}, {"heading": "3.4 GRADUAL POOLING", "text": "Max pooling is the most commonly used subsampling approach for state of the art networks. However, low level features are not informative early in training and thus max pooling obstructs gradient flow to most of these early units. To address this problem, we use the same effect of annealing from linear to non-linear networks. In particular, we capture this intuition by annealing from mean pooling units to max pooling units. As seen in Table 4, this combined approach does better than either mean or max pooling as well as taking an average, learning a weighted average, or learning an attention mechanism (Lee et al., 2015)."}, {"heading": "3.5 GRADUAL BATCH NORMALIZATION", "text": "Batch Normalization (BN) is a state of the art architectural component that greatly aids convergence and accuracy. However, it cannot easily be executed at test time, especially when evaluating on single data points. There are several clever ways of computing means and variances to work around this fundamental problem however, ideally, we would like the convergence and accuracy benefits\nof BN while avoiding issues at validation time. Using the GradNet framework to anneal from BN layers to identity layers, we generate a network that is trivial to evaluate at validation time while exploiting all the benefits of batch normalization."}, {"heading": "3.6 GRADUAL CONVOLUTIONS", "text": "Shallower networks are easier to train early but less powerful than deep networks. Accordingly, it is intuitively appealing to leverage the easier convergence of shallower networks while maintaining the power of very deep networks. Two proposed solutions, Highway networks (Srivastava et al., 2015) and Network in Network (NiN) (Lin et al., 2013) are emblematic of this problem: NiN is very deep but challenging to train while Highway networks require much larger depths than normal to match baseline performance resulting in greatly increased computational cost. For the depth used, highway networks perform significantly worse than the baseline network while interpolating from identity layers into convolutional layers (thereby progressively adding depth) results in almost no loss of performance. On the other hand, interpolating from convolutional layers into NiN layers results in significant improvements in performance."}, {"heading": "3.7 COMBINING GRADNETS", "text": "In Table 7, we demonstrate the composability of GradNets."}, {"heading": "3.8 GRELU DEPTH EXPERIMENTS", "text": "The trend in neural networks has been towards ever deeper architectures, which is aligned with the theoretical results indicating that they have exponentially greater expressibility. However, deeper networks exacerbate the challenges we have highlighted. For example, poor information flow in ReLU networks. As shown on Figure 2, using the GradNet framework not only improves the performance for slightly deeper networks but also allows for the successful training of significantly deeper networks which standard networks do not successfully train."}, {"heading": "3.9 GRADUAL SPATIAL TRANSFORMER NETWORKS", "text": "Spatial transformer networks (Jaderberg et al., 2015) are a powerful new type of attentional model that makes performing an affine glimpse differentiable. This makes it significantly easier to train than reinforcement learning methods however it still struggles with convergence. In particular, when randomly initialized, the localization network can transform the original image off the screen or zoom in excessively.\nSince the localization network depends on the signal from the classification network, it makes sense to train the classification network first before tuning the localization network. The flexibility of the\nGradNet framework allows for a computational implementation of this intuition. In particular, we gradually anneal from a simple downsample to an affine spatial transformer network.\nAs shown in Table 8, the regular spatial transformer often diverges. However the GradNet implementation does not diverge and yet accomplishes the same top level results."}, {"heading": "4 RELATED WORK", "text": ""}, {"heading": "4.1 PRECONDITIONING", "text": "Hinton & Salakhutdinov (2006) achieved breakthrough results by training deeper multi-layer networks through preconditioning early layers by greedy training each layer in succession. More recently, Yosinski et al. (2014) initialized from known successful weights thereby dropping the network immediately into a lower point of the loss surface and avoiding getting stuck in the many local optima that arise with deeper and larger networks. Distillation (Hinton et al., 2015) is another technique that uses existing cumbersome models to extract structure from the data before transferring it to a less cumbersome model.\nGradNets embody this philosophy by uniting stages of training that are easy to optimize and powerful with the additional benefit that training can be done jointly in a single training phase."}, {"heading": "4.2 RECTIFIED LINEAR UNITS (RELU) ALTERNATIVES", "text": "Plenty of ReLU alternatives have been proposed and indeed some have been integral to winning competitions. For the most part, these experiments have shown that networks are robust to the slope\nfor the negative part. Whereas ReLUs saturate all gradients on the negative side to zero; leaky ReLUs allow that information to flow. Accordingly, very leaky ReLUs have been extremely successful at deep learning competitions. PReLUs (He et al., 2015) instead allow for training the slope on the negative edge of the ReLU thus allowing the network to train how linear its nonlinearities should be. In randomized ReLUs (Xu et al., 2015), slopes are randomly chosen within a specific range before being frozen during testing. The ensuing stochastic regularization proved successful on the Kaggle data science bowl competition.\nRather than making a static trade-off between ease of optimization and expressive power, GradNets allow us to dynamically change this trade-off during training."}, {"heading": "4.3 VERY DEEP NETWORKS", "text": "Highway networks (Srivastava et al., 2015) are compositions of layers where a gating tensor controls the interpolation between an identity layer and a regular fully connected layer. The smooth varying enables learning much deeper networks however the computation of the gating factor at each layer requires twice the computation compared to standard networks.\nSaxe et al. (2013) showed the benefits of using deep linear networks as a medium for studying the learning dynamics of deep neural networks. In particular, they found that with precise initializations they could achieve depth independent learning times for deep linear networks.\nGradNets leverage insights from both these results: borrowing the theoretically justified initializations for linear networks while gaining the optimization benefits of the interpolation operation in highway networks without incurring their additional computational cost. These results speak to GradNet's ability to exploit the theoretical insights of simpler architectures while training very complex architectures."}, {"heading": "4.4 ATTENTIONAL MODELS", "text": "Interpolation between architectures is very similar to the soft attention used to interpolate between differentiable data structures (Joulin & Mikolov (2015), Grefenstette et al. (2015)). These use backpropagation to learn the weights for interpolation.\nHowever, soft attention incurs the computational cost of computing all possible outputs instead of controlling interpolation to converge to a single output."}, {"heading": "5 CONCLUSION", "text": "A difficult choice in neural network design is navigating the fundamental trade-off between ease of optimization and expressiveness. GradNets are a framework to help this choice by exploiting the ease of optimization early in training and the expressiveness late in training. Our experiments show that not only does this allow greater amounts of stochastic regularization as well as successful training of extremely deep networks but it also consistently improves performance at almost no computational cost. Finally, the results on spatial transformer networks speak to the ability of GradNets to dramatically ease training of complex architectural components and to enable new classes of components to be used in neural networks.\nFuture work includes interpolating between any simpler, easier to train architectural components and a more complex expressive component that generalizes better. Due to the results on deeper networks, GradNets may provide enormous benefits to recurrent architectures. Furthermore, the\nframework itself can be extended by adapting the interpolation policy using smarter techniques, such as reinforcement learning."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank NVIDIA for their generosity in providing access to part of their cluster in support of Enlitic's mission and our research."}], "references": [{"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jagannath"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Learning to transduce with unbounded memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "arXiv preprint arXiv:1506.02516,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1502.01852,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["Hinton", "Geoffrey", "Vinyals", "Oriol", "Dean", "Jeff"], "venue": "arXiv preprint arXiv:1503.02531,", "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Spatial transformer networks", "author": ["Jaderberg", "Max", "Simonyan", "Karen", "Zisserman", "Andrew", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1506.02025,", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1503.01007,", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Generalizing pooling functions in convolutional neural networks: Mixed", "author": ["Lee", "Chen-Yu", "Gallagher", "Patrick W", "Tu", "Zhuowen"], "venue": "gated, and tree. arXiv preprint arXiv:1509.08985,", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Maas", "Andrew L", "Hannun", "Awni Y", "Ng", "Andrew Y"], "venue": "In Proc. ICML,", "citeRegEx": "Maas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "arXiv preprint arXiv:1312.6120,", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Scalable bayesian optimization using deep neural networks", "author": ["Snoek", "Jasper", "Rippel", "Oren", "Swersky", "Kevin", "Kiros", "Ryan", "Satish", "Nadathur", "Sundaram", "Narayanan", "Patwary", "Md", "Ali", "Mostofa", "Adams", "Ryan P"], "venue": "arXiv preprint arXiv:1502.05700,", "citeRegEx": "Snoek et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2015}, {"title": "Empirical evaluation of rectified activations in convolutional network", "author": ["Xu", "Bing", "Wang", "Naiyan", "Chen", "Tianqi", "Li", "Mu"], "venue": "arXiv preprint arXiv:1505.00853,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "How transferable are features in deep neural networks", "author": ["Yosinski", "Jason", "Clune", "Jeff", "Bengio", "Yoshua", "Lipson", "Hod"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "Dropout's addition of stochastic noise improves performance by enabling larger networks and preventing coadaptation of weights albeit at a significant cost of convergence speed (Hinton et al., 2012).", "startOffset": 177, "endOffset": 198}, {"referenceID": 14, "context": "Despite being easier to optimize than sigmoids or hyperbolic tangents, ReLUs (Nair & Hinton, 2010) increase the expressive power of networks while being more difficult to initialize and optimize than linear models (Saxe et al., 2013) or leaky ReLUs (Maas et al.", "startOffset": 214, "endOffset": 233}, {"referenceID": 11, "context": ", 2013) or leaky ReLUs (Maas et al., 2013).", "startOffset": 23, "endOffset": 42}, {"referenceID": 5, "context": "Deep learning has become the state of the art system for many machine learning challenges (Krizhevsky et al. (2012), Girshick et al.", "startOffset": 91, "endOffset": 116}, {"referenceID": 0, "context": "(2012), Girshick et al. (2014)).", "startOffset": 8, "endOffset": 31}, {"referenceID": 0, "context": "(2012), Girshick et al. (2014)). Many of deep learning's benefits come from its ability to automatically learn abstract representations from raw input features, its scalability to large datasets, and its exponential growth in expressive power with depth. As a consequence of this great expressive power, deep learning can suffer from slow convergence, over-fitting, poor optima, difficulty of optimization, and acute sensitivity to initialization and hyper-parameters. Deep learning practitioners often optimize for the best possible generalization. Dropout and rectified linear units (ReLUs) are two of the more important algorithmic breakthroughs of the last 5 years. Indeed, both were essential to the breakthrough results of Krizhevsky et al. (2012) that reduced the error rate on the ImageNet Challenge of 2012 by almost 40%.", "startOffset": 8, "endOffset": 754}, {"referenceID": 15, "context": "We employ the network topology of Scalable Bayesian Optimization Using Deep Neural Networks (Snoek et al., 2015).", "startOffset": 92, "endOffset": 112}, {"referenceID": 12, "context": "9 were performed on Cluttered MNIST(Mnih et al., 2014).", "startOffset": 35, "endOffset": 54}, {"referenceID": 7, "context": "The input 60x60 pixel images were downsampled either with 3x3 mean pooling or an affine Spatial Transformer Network (Jaderberg et al., 2015), whose localization network had an architecture of 2 convolutional layers, each 5x5 with 20 filters followed by a max pooling layer, and 2 fully connected layers, with 50 and 10 units respectively with 512 units per hidden layer, Models were optimized with Adam with a batch size of 500 and early stopping and initialized using orthogonal initialization.", "startOffset": 116, "endOffset": 140}, {"referenceID": 2, "context": "ReLUs are the dominant form of nonlinearity in modern deep neural networks, however they do not allow gradients to flow well leading to several proposed solutions: leaky and very leaky ReLUs or ReLUs with parameterized slope for the negative part (He et al., 2015).", "startOffset": 247, "endOffset": 264}, {"referenceID": 10, "context": "As seen in Table 4, this combined approach does better than either mean or max pooling as well as taking an average, learning a weighted average, or learning an attention mechanism (Lee et al., 2015).", "startOffset": 181, "endOffset": 199}, {"referenceID": 7, "context": "Spatial transformer networks (Jaderberg et al., 2015) are a powerful new type of attentional model that makes performing an affine glimpse differentiable.", "startOffset": 29, "endOffset": 53}, {"referenceID": 3, "context": "Distillation (Hinton et al., 2015) is another technique that uses existing cumbersome models to extract structure from the data before transferring it to a less cumbersome model.", "startOffset": 13, "endOffset": 34}, {"referenceID": 14, "context": "More recently, Yosinski et al. (2014) initialized from known successful weights thereby dropping the network immediately into a lower point of the loss surface and avoiding getting stuck in the many local optima that arise with deeper and larger networks.", "startOffset": 15, "endOffset": 38}, {"referenceID": 2, "context": "PReLUs (He et al., 2015) instead allow for training the slope on the negative edge of the ReLU thus allowing the network to train how linear its nonlinearities should be.", "startOffset": 7, "endOffset": 24}, {"referenceID": 16, "context": "In randomized ReLUs (Xu et al., 2015), slopes are randomly chosen within a specific range before being frozen during testing.", "startOffset": 20, "endOffset": 37}, {"referenceID": 14, "context": "Saxe et al. (2013) showed the benefits of using deep linear networks as a medium for studying the learning dynamics of deep neural networks.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "4 ATTENTIONAL MODELS Interpolation between architectures is very similar to the soft attention used to interpolate between differentiable data structures (Joulin & Mikolov (2015), Grefenstette et al. (2015)).", "startOffset": 180, "endOffset": 207}], "year": 2015, "abstractText": "In machine learning, there is a fundamental trade-off between ease of optimization and expressive power. Neural Networks, in particular, have enormous expressive power and yet are notoriously challenging to train. The nature of that optimization challenge changes over the course of learning. Traditionally in deep learning, one makes a static trade-off between the needs of early and late optimization. In this paper, we investigate a novel framework, GradNets, for dynamically adapting architectures during training to get the benefits of both. For example, we can gradually transition from linear to non-linear networks, deterministic to stochastic computation, shallow to deep architectures, or even simple downsampling to fully differentiable attention mechanisms. Benefits include increased accuracy, easier convergence with more complex architectures, solutions to test-time execution of batch normalization, and the ability to train networks of up to 200 layers.", "creator": "LaTeX with hyperref package"}}}