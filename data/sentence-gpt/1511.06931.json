{"id": "1511.06931", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2015", "title": "Evaluating Prerequisite Qualities for Learning End-to-End Dialog Systems", "abstract": "A long-term goal of machine learning is to build intelligent conversational agents. One recent popular approach is to train end-to-end models on a large amount of real dialog transcripts between humans (Sordoni et al., 2015; Vinyals &amp; Le, 2015; Shang et al., 2015). However, this approach leaves many questions unanswered as an understanding of the precise successes and shortcomings of each model is hard to assess. A contrasting recent proposal are the bAbI tasks (Weston et al., 2015b) which are synthetic data that measure the ability of learning machines at various reasoning tasks over toy language. Unfortunately, those tests are very small and hence may encourage methods that do not scale. In this work, we propose a suite of new tasks of a much larger scale that attempt to bridge the gap between the two regimes. Choosing the domain of movies, we provide tasks that test the ability of models to answer factual questions (utilizing OMDB), provide personalization (utilizing MovieLens), carry short conversations about the two, and finally to perform on natural dialogs from Reddit. We provide a dataset covering 75k movie entities and with 3.5M training examples. We present results of various models on these tasks, and evaluate their performance.\n\n\n\nThese task proposals were recently reviewed at a seminar at the University of Utah, and are currently available to the public in the US. We hope that our work will lead to a better understanding of this problem and what is needed to improve the performance of the AI-based AI-based systems.\nThe goal of this paper was to investigate a proposed computational optimization model for the ability of training artificial neural networks in humans to generate highly relevant results that are not necessarily comparable to those obtained from the current model. We sought to test the possibility of a better optimization model for all of the existing models and proposed a system for such an optimization model. We obtained an estimated learning rate of 0.75 m2 for each of these tasks using a combination of the two approaches. We did not attempt to replicate the model, which we consider to be the best (at least in part) to do so.\nThe training tasks performed on a total of 382 participants in a simulated-learning system were implemented, with a total of 373 participants in a simulated-learning system. Each task in each task was measured over 2 weeks, with the mean (\u223c20) (P<0.05) (Table 2).\nResults of these simulations are available in", "histories": [["v1", "Sat, 21 Nov 2015 22:26:49 GMT  (50kb)", "https://arxiv.org/abs/1511.06931v1", null], ["v2", "Tue, 15 Dec 2015 09:31:59 GMT  (50kb)", "http://arxiv.org/abs/1511.06931v2", null], ["v3", "Wed, 6 Jan 2016 04:51:54 GMT  (51kb)", "http://arxiv.org/abs/1511.06931v3", null], ["v4", "Fri, 1 Apr 2016 06:22:44 GMT  (52kb)", "http://arxiv.org/abs/1511.06931v4", null], ["v5", "Fri, 15 Apr 2016 20:22:13 GMT  (52kb)", "http://arxiv.org/abs/1511.06931v5", null], ["v6", "Tue, 19 Apr 2016 15:30:29 GMT  (52kb)", "http://arxiv.org/abs/1511.06931v6", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["jesse dodge", "reea gane", "xiang zhang", "antoine bordes", "sumit chopra", "alexander miller", "arthur szlam", "jason weston"], "accepted": true, "id": "1511.06931"}, "pdf": {"name": "1511.06931.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["jessedodge@fb.com", "agane@fb.com", "xiangz@fb.com", "abordes@fb.com", "spchopra@fb.com", "ahm@fb.com", "aszlam@fb.com", "jase@fb.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n06 93\n1v 6\n[ cs\n.C L\n] 1\n9 A\npr 2\nA long-term goal of machine learning is to build intelligent conversational agents. One recent popular approach is to train end-to-end models on a large amount of real dialog transcripts between humans (Sordoni et al., 2015; Vinyals & Le, 2015; Shang et al., 2015). However, this approach leaves many questions unanswered as an understanding of the precise successes and shortcomings of each model is hard to assess. A contrasting recent proposal are the bAbI tasks (Weston et al., 2015b) which are synthetic data that measure the ability of learning machines at various reasoning tasks over toy language. Unfortunately, those tests are very small and hence may encourage methods that do not scale. In this work, we propose a suite of new tasks of a much larger scale that attempt to bridge the gap between the two regimes. Choosing the domain of movies, we provide tasks that test the ability of models to answer factual questions (utilizing OMDB), provide personalization (utilizing MovieLens), carry short conversations about the two, and finally to perform on natural dialogs from Reddit. We provide a dataset covering \u223c75k movie entities and with \u223c3.5M training examples. We present results of various models on these tasks, and evaluate their performance."}, {"heading": "1 INTRODUCTION", "text": "With the recent employment of Recurrent Neural Networks (RNNs) and the large quantities of conversational data available on websites like Twitter or Reddit, a new type of dialog system is emerging. Such end-to-end dialog systems (Ritter et al., 2011; Shang et al., 2015; Vinyals & Le, 2015; Sordoni et al., 2015) directly generate a response given the last user utterance and (potentially) the context from previous dialog turns without relying on the intermediate use of a dialog state tracking component like in traditional dialog systems (e.g. in Henderson (2015)). These methods are trained to imitate user-user conversations and do not need any hand-coding of attributes and labels for dialog states and goals like state tracking methods do. Being trained on large corpora, they are robust to many language variations and seem to mimic human conversations to some extent.\nIn spite of their flexibility and representational power, these neural network based methods lack pertinent goal-oriented frameworks to validate their performance. Indeed, traditional systems have a wide range of well defined evaluation paradigms and benchmarks that measure their ability to track user states and/or to reach user-defined goals (Walker et al., 1997; Paek, 2001; Griol et al., 2008; Williams et al., 2013). Recent end-to-end models, on the other hand, rely either on very few human scores (Vinyals & Le, 2015), crowdsourcing (Ritter et al., 2011; Shang et al., 2015) or machine translation metrics like BLEU (Sordoni et al., 2015) to judge the quality of the generated language only. This is problematic because these evaluations do not assess if end-to-end systems can conduct dialog to achieve pre-defined objectives, but simply whether they can generate correct language that could fit in the context of the dialog; in other words, they quantify their chit-chatting abilities.\n\u2217The first three authors contributed equally.\nTo fill in this gap, this paper proposes a collection of four tasks designed to evaluate different prerequisite qualities of end-to-end dialog systems. Focusing on the movie domain, we propose to test if systems are able to jointly perform: (1) question-answering (QA), (2) recommendation, (3) a mix of recommendation and QA and (4) general dialog about the topic, which we call chit-chat. All four tasks have been chosen because they test basic capabilities we expect a dialog system performing insightful movie recommendation should have while evaluation on each of them can be well defined without the need of human-in-the-loop (e.g. via Wizard-of-Oz strategies (Whittaker et al., 2002)). Our ultimate goal is to validate if a single model can solve the four tasks at once, which we assert is a pre-requisite for an end-to-end dialog system supposed to act as a movie recommendation assistant, and by extension a general dialog agent as well. At the same time we advocate developing methods that make no special engineering for this domain, and hence should generalize to learning on tasks and data from other domains easily.\nIn contrast to the bAbI tasks which test basic capabilities of story understanding systems (Weston et al., 2015b), the tasks have been created using large-scale real-world sources (OMDb1, MovieLens2 and Reddit3). Overall, the dataset covers \u223c75k movie entities (movie, actor, director, genre, etc.) with \u223c3.5M training examples: even if the dataset is restricted to a single domain, it is large and allows a great variety of discussions, language and user goals. We evaluate on these tasks the performance of various neural network models that can potentially create end-to-end dialogs, ranging from simple supervised embedding models (Bai et al., 2009), RNNs with Long Short-Term Memory (LSTMs) (Hochreiter & Schmidhuber, 1997), and attention-based models, in particular Memory Networks (Sukhbaatar et al., 2015). To validate the quality of our results, we also apply our best performing model, Memory Networks, in other conditions by comparing it on the Ubuntu Dialog Corpus (Lowe et al., 2015) against baselines trained by the authors of the corpus. We show that they outperform all baselines by a wide margin."}, {"heading": "2 THE MOVIE DIALOG DATASET", "text": "We introduce a set of four tasks to test the ability of end-to-end dialog systems, focusing on the domain of movies and movie related entities. They aim to test five abilities which we postulate as being key towards a fully functional general dialog system (i.e., not specific to movies per se):\n\u2022 QA Dataset: Tests the ability to answer factoid questions that can be answered without relation to previous dialog. The context consists of the question only.\n\u2022 Recommendation Dataset: Tests the ability to provide personalized responses to the user via recommendations (in this case, of movies) rather than universal facts as above.\n\u2022 QA+Recommendation Dataset: Tests the ability of maintaining short dialogs involving both factoid and personalized content where conversational state has to be maintained.\n\u2022 Reddit Dataset: Tests the ability to identify most likely replies in discussions on Reddit.\n\u2022 Joint Dataset: All our tasks are dialogs. They can be combined into a single dataset, testing the ability of an end-to-end model to perform well at all skills at once.\nSample input contexts and target replies from the tasks are given in Tables 1-4. The datasets are available at: http://fb.ai/babi."}, {"heading": "2.1 QUESTION ANSWERING (QA)", "text": "The first task we build is to test whether a dialog agent is capable of answering simple factual questions. The dataset was built from the Open Movie Database (OMDb)4 which contains metadata about movies. The subset we consider contains \u223c15k movies, \u223c10k actors and \u223c6k directors. We also matched these movies to the MovieLens dataset5 to attribute tags to each movie. We build a knowledge base (KB) directly from the combined data, stored as triples such as (THE DARK HORSE,\n1http://en.omdb.org 2http://movielens.org 3http://reddit.com/r/movie 4Downloaded from http://beforethecode.com/projects/omdb/download.aspx. 5http://grouplens.org/datasets/movielens/\nSTARRED ACTOR, BETTE DAVIS) and (MOONRAKER, HAS TAG, JAMES BOND), with 8 different relation types involving director, writer, actor, release date, genre, tags, rating and imdb votes.\nWe distinguish 11 classes of question, corresponding to different kinds of edges in our KB: actor to movie (\u201cWhat movies did Michael J Fox star in?\u201d), movie to actors (\u201cWho starred in Back to The Future?\u201d), movie to director, director to movie, movie to writer, writer to movie, movie to tags, tag to movie, movie to year, movie to genre and movie to language. For each question type there is a set of possible answers. Using SimpleQuestions, an existing open-domain question answering dataset based on Freebase (Bordes et al., 2015) we identified the subset of questions posed by those human annotators that covered our question types. We expanded this set to cover all of our KB by substituting the actual entities in those questions to also apply them to other questions, e.g. if the original question written by an annotator was \u201cWhat movies did Michael J Fox star in?\u201d, we created a pattern \u201cWhat movies did [@actor] star in?\u201d which we substitute for any other actors in our set, and repeat this for all annotations. We split the questions into training, development and test sets with \u223c96k, 10k and 10k examples, respectively.\nTo simplify evaluation rather than requiring the generation of sentences containing the answers, we simply ask a model to output a list, which is ranked as the possible set of answers. We then use standard ranking metrics to evaluate the list, making the results easy to interpret. Our main results report the hits@1 metric (i.e. is the top answer correct); other metrics are given in the appendix."}, {"heading": "2.2 RECOMMENDATION DATASET", "text": "Not all questions about movies in dialogs have an objective answer, independent of the person asking; indeed much of human dialog is based on opinons and personalized responses. One of the simplest dialogs of this type to evaluate is that of recommendation, where we can utilize existing data resources. We again employ the MovieLens dataset which features a user \u00d7 item matrix of movie ratings, rated from 1 to 5. We filtered the set of movies to be the same set as in the QA task and additionally only kept movies that had at least 2 ratings, giving around \u223c 11k movies.\nTo use this data for evaluating dialog, we then use it to generate dialog exchanges. We first select a user at random; this will be the user who is participating in the dialog, and then sample 1-8 movies that the user has rated 5. We then form a statement intended to express the user\u2019s feelings about these movies, according to a fixed set of natural language templates, one of which is selected randomly. See Table 2 for some examples. From the remaining set of movies the same user gave a rating of 5, we select one to be the answer.\nThere are \u223c110k users in the training, \u223c1k users in the development set and \u223c1k for test. We follow the procedure above sampling users with replacement and generate 1M training examples and 10k development and test set examples, respectively. To evaluate the performance of a model, just as in the first task, we evaluate a ranked list of answers. In our main results we measure hits@100, i.e. 1\nif the provided answer is in the top 100, and 0 otherwise, rather than hits@1 as this task is harder than the last.\nNote that we expect absolute hits@k numbers to be lower for this task than for QA due to incomplete labeling (\u201cmissing ratings\u201d): in recommendation there is no exact right answer, and it is not surprising the actual single true label is not always at the top position, i.e. the top predictions of the model may be good as well, but we do not have their labels. One can thus view the ranking metric as a kind of lower bound on performance of actually labeling all the predictions using human annotations, which would be time consuming and no longer automatic, and hence undesirable for algorithm development. This is standard in recommendation, see e.g. Cremonesi et al. (2010)."}, {"heading": "2.3 QA+RECOMMENDATION DIALOG", "text": "The tasks presented so far only involve questions followed by responses, with no context from previous dialog. This task aims at evaluating responses in the context of multiple previous exchanges, while remaining straightforward enough that evaluation and analysis are still tractable. We hence combine the question answering and recommendation tasks from before in a multi-response dialog, where dialogs consist of 3 exchanges (3 turns from each participant).\nThe first exchange requires a recommendation similar to Task 1 except that they also specify what genre or topic they are interested in, e.g. \u201cI\u2019m looking for a Music movie\u201d, where the answer might be \u201cSchool of Rock\u201d, as in the example of Table 3.\nIn the second exchange, given the model\u2019s response (movie suggestion), the user asks a factoid question about that suggestion, e.g. \u201cWhat else is that about?\u201d, \u201cWho stars in that?\u201d and so on. This question refer back to the previous dialog, making context important.\nIn the third exchange, the user asks for a alternative recommendation, and provides extra information about their tastes, e.g. \u201cI like Tim Burton movies more\u201d. Again, context of the last two exchanges should help for best performance.\nWe thus generate 1M examples of such 6 line dialogs (3 turns from each participant) for training, and \u223c10k for development and testing respectively. We can evaluate the performance of models across all the lines of dialog (e.g., all \u223c30k responses from the test set), but also only on the 1st (Recommendation), 2nd (QA) or 3rd exchange (Similarity) for a more fine-grained analysis. We again use a ranking metric (here, hits@10), just as in our previous tasks."}, {"heading": "2.4 REDDIT DISCUSSION", "text": "Our fourth task is to predict responses in movie discussions using real conversation data taken directly from Reddit, a website where registered community members can submit content in various areas of interest, called \u201csubreddits\u201d. We selected the movie subreddit6 to match our other tasks.\nThe original discussion data is potentially between multiple participants. To simplify the setup, we flatten this to appear as two participants (parent and comment), just as in our other tasks. In this way we collected \u223c1M dialogs, of which 10k are reserved for a development set, and another 10k for the\n6https://www.reddit.com/r/movies, selecting from the dataset available at https://www.reddit.com/r/datasets/comments/3bxlg7.\ntest set. Of the dialogs, \u223c76% involve a single exchange, \u223c17% have at least two exchanges, and 7% have at least three exchanges (the longest exchange is length 50).\nTo evaluate the performance of models, we again separate the problem of evaluating the quality of a response from that of language generation by considering a ranking setup, in line with other recent works (Sordoni et al., 2015). We proceed as follows: we select a further 10k comments for the development set and another 10k for the test set which have not appeared elsewhere in our dataset, and use these as potential candidates for ranking during evaluation. For each exchange, given the input context, we rank 10001 possible candidates: the true response given in the dataset, plus the 10k \u201cnegative\u201d candidates just described. The model has to rank the true response as high as possible. Similar to recommendation as described before we do not expect absolute hits@k performance to be as high as for QA due to incomplete labeling. As with Task 3, we can evaluate on all the data, or only on the 1st, 2nd or 3rd exchange, and so on. We also identified the subset of the test set where there is an entity match with at least two entities from Tasks 1-3, where one of the entities appears in the input, and the other in the response: this subset serves to evaluate the impact of using a knowledge base for conducting such a dialog."}, {"heading": "2.5 JOINT TASK", "text": "Finally, we consider a task made of the combination of all four of the previous ones. At both training and test time examples consist of exchanges from any of the datasets, sampled at random, whereby the conversation is \u2018reset\u2019 at each sample, so that the context history only ever includes exchanges from the current conversation.\nWe consider this to be the most important task, as it tests whether a model can not only produce chit-chat (Task 4) but also can provide meaningful answers during dialog (Tasks 1-3). On the other hand, the point of delineating the separate tasks is to evaluate exactly which types of dialog a model is succeeding at or not. That all the datasets are in the same domain is crucial to testing the ability of models at performing well on all tasks jointly. If the domains were different, then the vocabularies would be trivially non-overlapping, allowing to learn effectively separate models inside a single one."}, {"heading": "2.6 RELATION TO EXISTING EVALUATION FRAMEWORKS", "text": "Traditional dialog systems consist of two main modules: (1) a dialog state tracking component that tracks what has happened in a dialog, incorporating into a pre-defined explicit state structure system outputs, user utterances, context from previous turns, and other external information, and (2) a response generator. Evaluation of the dialog state tracking stage is well defined since the PARADISE framework (Walker et al., 1997) and subsequent initiatives (Paek, 2001; Griol et al., 2008), including recent competitons (Williams et al., 2013; Henderson et al., 2014) as well as situated variants (Rojas-Barahona et al., 2012). However, they require fine grained data annotations in terms of labeling internal dialog state and precisely defined user intent (goals). As a result, they do not really scale to large domains and dialogs with high variability in terms of language. Because of language ambiguity and variation, evaluation of the response generation step is complicated and usually relies on human judgement (Walker et al., 2003).\nEnd-to-end dialog systems do not rely on explicit internal state and hence do not have state tracking modules, they directly generate responses given user utterances and dialog context and hence can not be evaluated using state tracking test-beds. Unfortunately, as for response generator modules, their evaluation is ill-defined as it is difficult to objectively rate at scale the fit of returned responses. Most existing work (Ritter et al., 2011; Shang et al., 2015; Vinyals & Le, 2015; Sordoni et al., 2015) chose to use human ratings, which does not easily scale. Sordoni et al. (2015) also use the BLEU score to compare to actual user utterances but this is not a completely satisfying measure of success,\nespecially when used in a chit-chat setting where there are no clear goals and hence measures of success. Lowe et al. (2015) use a similar ranking evaluation to ours, but only in a chit-chat setting.\nOur approach of providing a collection of tasks to be jointly solved is related to the evaluation framework of the bAbI tasks (Weston et al., 2015a) and of the collection of sequence prediction tasks of Joulin & Mikolov (2015). However, unlike them, our Tasks 1-3 are much closer to real dialog, being built from human-written text, and with Task 4 actually involving real dialog from Reddit. The design of our tasks is such that all test one or more key characteristics a dialog system should have but also that an unambiguous answer is expected after each dialog act. In that sense, it follows the the notion of dialog evaluation by a reference answer introduced in (Hirschman et al., 1990). The application of movie recommender systems is connected to that of TV program suggestion proposed by Ramachandran et al. (2014), except that we frame it so that we can generate systematic evaluation from it, where they only rely on human judgement at small scale."}, {"heading": "3 MODELS", "text": ""}, {"heading": "3.1 MEMORY NETWORKS", "text": "Memory Networks (Weston et al., 2015c; Sukhbaatar et al., 2015) are a recent class of models that perform language understanding by incorporaring a memory component that potentially includes both long-term memory (e.g., to remember facts about the world) and short-term context (e.g., the last few turns of dialog). They have only been evaluated in a few setups: question answering (Bordes et al., 2015), language modeling (Sukhbaatar et al., 2015; Hill et al., 2015), and language understanding on the bAbI tasks (Weston et al., 2015a), but not so far on dialog tasks such as ours.\nWe employ the MemN2N architecture of Sukhbaatar et al. (2015) in our experiments, with some additional modifications to construct both long-term and short-term context memories. At any given time step we are given as input the history of the current conversation: messages from the user cui at time step i and the corresponding responses cri from the model itself at the corresponding time steps, i = 1, . . . , t\u2212 1. At the current time t we are only given the input cut and the model has to respond.\nRetrieving long-term memories For each word in the last N messages we perform a hash lookup to return all long-term memories (sentences) from a database that also contain that word. Words above a certain frequency cutoff can be ignored to avoid sentences that only share syntax or unimportant words. We employ the movie knowledge base of Sec. 2.1 for our long-term memories, but potentially any text dataset could be used. See Figure 5 for an example of this process.\nAttention over memories The sentences hj , j = 1, . . . , H returned from the hashing step plus the messages from the current conversation form the memory of the Memory Network7:\nx = (cu 1 , . . . , cut\u22121, c r 1 , . . . , crt\u22121, h1, . . . , hH).\nThe last user input cut is embedded using a matrix A of size d \u00d7 V where d is the embedding dimension and V is the size of the vocabulary, giving u = Acut . Each memory xi is embedded using the same matrix, givingmi = Axi. The match between the input and the memories is then computed by taking the inner product followed by a softmax: pi = Softmax(u\u22a4mi) giving a probability vector over the memories. The output memory representation is then constructed with o = R \u2211 i pimi whereR is a d\u00d7d rotation matrix8. The memory output is then added to the original input q = o+cut . This procedure can then be stacked in what is called multiple \u201chops\u201d of attention over the memory.\nGenerating the final prediction The final prediction is then defined as: a\u0302 = Softmax(q\u22a4Wy1, . . . , q\u22a4WyC) where there are C candidate responses in y, and W is of dimension V \u00d7 d. For Tasks 1-3 the candidates are the set of words in the vocabulary, which are ranked for final evaluation, whereas for Task 4 the candidates are target respones (sentences).\nThe whole model is trained using stochastic gradient descent by minimizing a standard cross-entropy loss between a\u0302 and the true label a.\n7We also add time features to each memory to denote their position following (Sukhbaatar et al., 2015). 8Optionally, different dictionaries can be used for inputs, memories and outputs instead of being shared."}, {"heading": "3.2 SUPERVISED EMBEDDING MODELS", "text": "While one of the major uses of word embedding models is to learn unsupervised embeddings over large unlabeled datasets such as in Word2Vec (Mikolov et al., 2013) there are also very effective word embedding models for training supervised models when labeled data is available. The simplest approach which works suprisingly well is to sum the word embeddings of the input and the target independently and then compare them with a similarity metric such as inner product or cosine similarity. A ranking loss is used to ensure the correct targets are ranked higher than any other targets. Several variants of this approach exist. For matching two documents supervised semantic indexing (SSI) was shown to be superior to unsupervised latent semantic indexing (LSI) (Bai et al., 2009). Similar methods were shown to outperform SVD for recommendation (Weston et al., 2013). However, we do not expect this method to work as well on question answering tasks, as all the memorization must occur in the individual word embeddings, which was shown to perform poorly in (Bordes et al., 2014). For example, consider asking the question \u201cwho was born in Paris?\u201d and requiring the word embedding for Paris to effectively contain all the pertinent information. However, for rarer items requiring less storage, performance may not be as degraded. In general we believe this is a surprisingly strong baseline that is often neglected in evaluations. Our implementation corresponds to a Memory Network with no attention over memory."}, {"heading": "3.3 RECURRENT LANGUAGE MODELS", "text": "Recurrent Neural Networks (RNNs) have proven successful at several tasks involving natural language, language modeling (Mikolov et al., 2011), and have been applied recently to dialog (Sordoni et al., 2015; Vinyals & Le, 2015; Shang et al., 2015). LSTMs are not known however for tasks such as QA or item recommendation, and so we expect them to find our datasets challenging.\nThere are a large number of variants of RNNs, including Long-Short Term Memory activation units (LSTMs) (Hochreiter & Schmidhuber, 1997), bidirectional LSTMs (Graves et al., 2012), seq2seq models (Sutskever et al., 2014), RNNs that take into account the document context (Mikolov & Zweig, 2012) and RNNs that perform attention over their input in various different ways (Bahdanau et al., 2015; Hermann et al., 2015; Rush et al., 2015). Evaluating all these variants\nis beyond the scope of this work and we instead use standard LSTMs as our baseline method9. However, we note that LSTMs with attention have many properties in common with Memory Networks if the attention is applied over the same memory setup."}, {"heading": "3.4 QUESTION ANSWERING SYSTEMS", "text": "For the particular case of Task 1 we can apply existing question answering systems. There has been a recent surge in interest in such systems that try to answer a question posed in natural language by converting it into a database search over a knowledge base (Berant & Liang, 2014; Kwiatkowski et al., 2013; Fader et al., 2014), which is a setup natural for our QA task also. However, such systems cannot easily solve any of our other tasks, for example our recommendation Task 2 does not involve looking up a factoid answer in a database. Nevertheless, this allows us to compare the performance of end-to-end systems performant on all our tasks to a standard QA benchmark. We chose the method of Bordes et al. (2014)10 as our baseline. This system learns embeddings that match questions to database entries, and then ranks the set of entries, and has been shown to achieve good performance on the WEBQUESTIONS benchmark (Berant et al., 2013)."}, {"heading": "3.5 SINGULAR VALUE DECOMPOSITION", "text": "Singular Value Decomposition (SVD) is a standard benchmark for recommendation, being at the core of the best ensemble results in the Netflix challenge, see Koren & Bell (2011) for a review. However, it has been shown to be outperformed by other flavors of matrix factorization, in particular by using a ranking loss rather than squared loss (Weston et al., 2013) which we will compare to (cf. sec 3.2), as well as improvements like SVD++ (Koren, 2008). Collaborative filtering methods are applicable to Task 2, but cannot easily be used for any of the other tasks. Even for Task 2, while our dialog models use textual input, as shown in Table 2, SVD requires a user \u00d7 item matrix, so for this baseline we preprocessed the text to assign each entity an ID, and throw away all other text. In contrast, the end-to-end dialog models have to learn to process the text as part of the task."}, {"heading": "3.6 INFORMATION RETRIEVAL MODELS", "text": "To select candidate responses a standard baseline is nearest neighbour information retrieval (IR) (Isbell et al., 2000; Jafarpour et al., 2010; Ritter et al., 2011; Sordoni et al., 2015). Two simple variants are often tried: given an input message, either (i) find the most similar message in the (training) dataset and output the response from that exchange; or (ii) find the most similar response to the input directly. In both cases the standard measure of similarity is tf-idf weighted cosine similarity between the bags of words. Note that that the Supervised Embedding Models of Sec. 3.2 effectively implement the same kind of model (ii) but with a learnt similarity measure. It has been shown previously that method (ii) performs better (Ritter et al., 2011), and our initial IR experiments showed the same result. Note that while (non-learning) IR systems can also be applied to other tasks such as QA (Kolomiyets & Moens, 2011) they require significant tuning to do so. Here we stick to a vanilla vector space model and hence only apply an IR baseline to Task 4."}, {"heading": "4 RESULTS", "text": "Our main results across all the models and tasks are given in Table 4. Supervised Embeddings and Memory Networks are tested in two settings: trained and tested on all tasks separately, or jointly on the combined Task 5. Other methods are only evaluated on independent tasks. In all cases, parameter search was performed on the development sets; parameter choices are provided in the appendix.\nAnswering Factual Questions Memory Networks and the baseline QA system are the two methods that have an explicit long-term memory via access to the knowledge base (KB). On the task of answering factual questions where the answers are contained in the KB, they outperform the other methods convincingly, with LSTMS being particularly poor. The latter is not unexpected as that method is good at language modeling, not question answering, see e.g. Weston et al. (2015b). The\n9We used the code available at: https://github.com/facebook/SCRNNs 10We used the \u2018Path Representation\u2019 for the knowledge base, as described in Sec. 3.1 of Bordes et al. (2014).\nbaseline QA system, which is designed for this task, is superior to Memory Networks, indicating there is still room for improvement in that model. On the other hand, the latter\u2019s much more general design allows it to perform well on our other dialog tasks, whereas the former is task specific.\nMaking Recommendations In this task a long-term memory does not bring any improvement, with LSTMs, Supervised Embeddings and Memory Networks all performing similarly, and all outperforming the SVD baseline. Here, we conjecture LSTMs can perform well because it looks much more like a language modeling task, i.e. the input is a sequence of similar recommendations.\nUsing Dialog History In both QA+Recommendations (Task 3) and Reddit (Task 4) Memory Networks outperform Supervised Embeddings due to their better use of context. This can be seen by breaking down the results by length of context: in the first response they perform similarly, but Memory Networks show a relative improvement on the second and third responses, see Tables 9 and 10 in the appendix. Note that these improvements come from the short term memory (dialog history), not from the use of the KB, as we show Memory Networks results without access to the KB and they perform similarly. We believe the QA performance in these cases is not hindered by the lack of a KB because we ask questions based on fewer relations than in Task 1 and it is easier to store the knowledge directly in the word embeddings. The baseline IR model in Task 4 benefits from context too, it is compared with and without in Table 10. LSTMs perform poorly: the posts in Reddit are quite long and the memory of the LSTM is relatively short, as pointed out by Sordoni et al. (2015). In that work they employed a linear reranker that used LSTM prediction as features to better effect. Testing more powerful recurrent networks such as LSTMs with attention on these benchmarks remains as future work (although the latter is related to Memory Networks, which we do report).\nJoint Learning A truly end-to-end dialog system has to be good at all the skills in Tasks 1-4 (and more besides, i.e. this is necessary, but not sufficient). We thus report results on our Combined Task for Supervised Embeddings and Memory Networks. Supervised Embeddings still have the same failings as before on Tasks 1 and 3, but now seem to perform even more poorly due to the difficulty of encoding all the necessary skills in the word embeddings, so e.g., they now do significantly worse on Task 4. This is despite us trying word embeddings of up to 2000 dimensions. Memory Networks fare better, having only a slight loss in performance on Tasks 2-4 and a slight gain in Task 1. In their case, the modeling power is not only in the word embeddings, but also in the attention over the long-term and short-term memory, so it does not need as much capacity in the word embeddings. However, the best achievable models would presumably have some improvement from training across all the tasks, not a loss, and would perform at least as well as all the individual task baselines (i.e. in this case, perform better at Task 1)."}, {"heading": "5 UBUNTU DIALOGUE CORPUS RESULTS", "text": "As no other authors have yet published results on our new benchmark, to validate the quality of our results we also apply our best performing model in other conditions by comparing it on the Ubuntu Dialog Corpus (Lowe et al., 2015). In particular, this also allows us to compare to more sophisticated\nLSTMs models that are trained discriminatively using metric learning, as well as additional baseline methods all trained by the authors. The Ubuntu Dialog Corpus contains almost 1M dialogs of more than 7 turns on average (900k dialogs for training, 20k for validation and 20k for testing), and 100M million words. The corpus was scraped from the Ubuntu IRC channel logs where users ask questions about issues they are having with Ubuntu and get answers by other users. Most chats can involve more than two users but a series of heuristics to disentangle them into dyadic dialogs was used.\nThe evaluation is similar to that of Reddit (Task 4): each correct answer has to be retrieved among a set of 10, mixed with 9 randomly chosen candidate utterances. We report the Hits@1 in Table 7.11 We used the same MemN2N architecture as before. all models were selected using validation accuracy. On this dataset, which has longer dialogs than those from the Movie Dialog Corpus, we can see that running more hops on the memory with the MemN2N improves performance: the 1-hop model performs similarly to the LSTM but with 2-hops and more we can gain more than a +8% increase over the previous best reported model. Using even more hops still improves over 1-hop but not much over 2-hops."}, {"heading": "6 CONCLUSION", "text": "We have presented a new set of benchmark tasks designed to evaluate end-to-end dialog systems. The movie dialog dataset measures how well such models can perform at both goal driven dialog, of both objective and subjective goals thanks to evaluation metrics on question answering and recommendation tasks, and at less goal driven chit-chat. A true end-to-end model should perform well at all these tasks, being a necessary but not sufficient condition for a fully functional dialog agent.\nWe showed that some end-to-end neural networks models can perform reasonably across all tasks compared to standard per-task baselines. Specifically, Memory Networks that incorporate short and long term memory can utilize local context and knowledge bases of facts to boost performance. We believe this is promising because we showed these same architectures also perform well on a separate dialog task, the Ubuntu Dialog Corpus, and have been shown previously to work well on the synthetic but challenging bAbI tasks of Weston et al. (2015a), and have no special engineering for the tasks or domain. However, some limitations remain, in particular they do not perform as well as stand-alone QA systems for QA, and performance is also degraded rather than improved when training on all four tasks at once. Future work should try to overcome these problems.\nWhile our dataset focused on movies, there is nothing specific to the task design which could not be transferred immediately to other domains, for example sports, music, restaurants, and so on. Future work should create new tasks in this and other domains to ensure that models are firstly not overtuned for these goals, and secondly to test further skills \u2013 and to motivate the development of algorithms to be skillful at them."}, {"heading": "A FURTHER EXPERIMENTAL DETAILS", "text": "Dictionary For all models we built a dictionary using all the known entities in the KB (e.g. \u201cBruce Willis\u201d and \u201cDie Hard\u201d are single dictionary elements). This allows us to output a single symbol for QA and Recommendation in order to predict an entity, rather than having to construct the answer out of words, making training and evaluation of the task simpler. The rest of the dictionary is built of unigrams that are not covered by our entity dictionary, where we removed other words (but not entities) with frequency less than 5. Overall this gives a dictionary of size 189472, which includes 75542 entities. All entries and texts were lower-cased. Our text parser to convert to the dictionary representation is then very simple: it goes left to right, consuming the largest n-gram at each step.\nMemory Networks For most of the tasks the optimal number of hops was 1, except for Task 3 where 2 or 3 hops outperform 1. See Table 9 and the parameter choices in Sec. B. For the joint task (Task 5), to achieve best performance we increased the capacity compared to the individual task models by using different dictionaries for the input, memory and output layers, see Sec. B. Additionally, we pre-trained the weights by training without the long-term memory for speed.\nSupervised Embedding Models We tried two flavors of supervised embedding model: (i) a model f(x, y) = x\u22a4U\u22a4Uy (\u201csingle dictionary model\u201d); and (ii) a model f(x, y) = x\u22a4U\u22a4V y (\u201ctwo dictionary model\u201d). That is, the latter has two sets of word embeddings depending on whether the word is in the input+context, or the label. The input and context are concatenated together to form a bag of words in either case. It turns out method (i) works better on Tasks 1 and 4, and method (ii) works better on Tasks 2 & 3. Some of the reasons why that is so are easy to understand: on Tasks 2 and 3 (recommendations) a single dictionary model favors predicting the same movies that are already in the input context, which are never correct. However, it appears that on Tasks 1 and 4 the two dictionary model appears to overfit to some degree. This partially explains why the model overall is worse on the joint dataset (Task 5). See Sec. B for more details.\nLSTMs LSTMs performed poorly on Task 4 and we spent some time trying to improve these results. Despite the perplexity looking reasonable (\u223c96 on the training set, and \u223c105 on the validation set) after training for \u223c6 days, we still obtain poor results distinguishing between candidates. We also tried Seq2Seq models (without attention or metric learning) and did not obtain improvements. Part of the problem is that posts in Reddit vary from very short (a few words) to very long (several paragraphs) and one natural procedure to try \u2013 computing the probability of those sequences seeded by the input \u2013 gives very unbalanced results, and tends to select the shorter ones, ending up with worse than random performance. Further, computationally the whole procedure is then very slow compared to all other methods tested. Memory Networks and supervised embeddings need to compute the inner product between embedded inputs and outputs, and hence the the candidates can be embedded once and cached for the whole test set. This trick is not applicable to the method described above rendering it much slower. To deal with the speed issue one can use our supervised embedding model as a first step, and then only reranking the top 100 results with the LSTM to make it tractable, however performance is still poor as mentioned. We obtained improved results by instead adopting the approach of Narasimhan et al. (2015): we take the representation for a dialog message as the average embedding over the hidden states as the symbols are consumed (at each step of the recurrence). We also note that Lowe et al. (2015) report good results (on a different dataset, the Ubuntu Corpus) by training an additional metric learner on top of an LSTM representation, which we have not tried. However, we do compare that approach to Memory Networks on that corpus in Section 5.\nInformation Retrieval Aside from the models described in the main paper, we also experimented with a hybrid relevance feedback approach: find the most similar message in the history, add the response to the query (with a certain weight) and then score candidate responses with the combined input. However, the relevance feedback model did not help: as we increase the feedback parameter (how much to use the retrieved response) the model only degrades, see Table 10 for the performance adding with a weight of 0.5."}, {"heading": "B OPTIMAL HYPER-PARAMETER VALUES", "text": "Hyperparameters of all learning models have been set using grid search on the validation set. The main hyperparameters are embedding dimension d, learning rate \u03bb, number of dictionaries w, num-\nber of hops K for MemNNs and unfolding depth blen for LSTMs. All models are implemented in the Torch library (see torch.ch).\nTask 1 (QA)\n\u2022 QA System of Bordes et al. (2014): \u03bb = 0.001, d = 50.\n\u2022 Supervised Embedding Model: \u03bb = 0.05, d = 50, w = 1.\n\u2022 MemN2N: \u03bb = 0.005, d = 50, w = 1, K = 1.\n\u2022 LSTM: \u03bb = 0.001, d = 100, blen = 10.\nTask 2 (Recomendation)\n\u2022 SVD: d = 50.\n\u2022 Supervised Embedding Model: \u03bb = 0.005, d = 200, w = 2.\n\u2022 MemN2N: \u03bb = 0.01, d = 1000, w = 1, K = 1.\n\u2022 LSTM: \u03bb = 0.01, d = 100, blen = 10.\nTask 3 (QA+Recommendation)\n\u2022 Supervised Embedding Model: \u03bb = 0.005, d = 1000, w = 2.\n\u2022 MemN2N: \u03bb = 0.001, d = 50, w = 1, K = 3.\n\u2022 LSTM: \u03bb = 0.001, d = 100, blen = 10.\nTask 4 (Reddit)\n\u2022 Supervised Embedding Model: \u03bb = 0.1, d = 1000, w = 1.\n\u2022 MemN2N: \u03bb = 0.01, d = 1000, w = 1, K = 1.\n\u2022 LSTM: \u03bb = 0.01, d = 512, blen = 15.\nJoint Task We chose hyperparameters by taking the mean performance over the four tasks, after scaling each task by the best performing model on that task on the development set in order to normalize the metrics.\n\u2022 Supervised Embedding Model: \u03bb = 0.01, d = 1000, w = 2.\n\u2022 MemN2N: \u03bb = 0.005, d = 1000, w = 3.\nUbuntu Dialog Corpus Hyperparameters of the MemN2N have been set using grid search on the validation set. We report the best models with K = 1, 2, 3, 4 in the paper; other hyperparameters were \u03bb = 0.001, d = 256."}, {"heading": "C FURTHER DETAILED RESULTS", "text": "C.1 BREAKDOWN OF TASK 1 (QA) RESULTS BY QUESTION TYPE\nC.2 BREAKDOWN OF TASK 3 (QA+RECOMMENDATION) RESULTS BY RESPONSE TYPE\nC.3 BREAKDOWN OF TASK 4 (REDDIT) RESULTS BY RESPONSE TYPE"}], "references": [{"title": "Supervised semantic indexing", "author": ["Bai", "Bing", "Weston", "Jason", "Grangier", "David", "Collobert", "Ronan", "Sadamasa", "Kunihiko", "Qi", "Yanjun", "Chapelle", "Olivier", "Weinberger", "Kilian"], "venue": "In Proceedings of the 18th ACM conference on Information and knowledge management,", "citeRegEx": "Bai et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bai et al\\.", "year": 2009}, {"title": "Semantic parsing via paraphrasing", "author": ["Berant", "Jonathan", "Liang", "Percy"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL\u201914),", "citeRegEx": "Berant et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Berant", "Jonathan", "Chou", "Andrew", "Frostig", "Roy", "Liang", "Percy"], "venue": "In EMNLP, pp", "citeRegEx": "Berant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Question answering with subgraph embeddings", "author": ["Bordes", "Antoine", "Chopra", "Sumit", "Weston", "Jason"], "venue": "In Proc. EMNLP,", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Large-scale simple question answering with memory networks", "author": ["Bordes", "Antoine", "Usunier", "Nicolas", "Chopra", "Sumit", "Weston", "Jason"], "venue": "arXiv preprint arXiv:1506.02075,", "citeRegEx": "Bordes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Performance of recommender algorithms on top-n recommendation tasks", "author": ["Cremonesi", "Paolo", "Koren", "Yehuda", "Turrin", "Roberto"], "venue": "In Proceedings of the fourth ACM conference on Recommender systems,", "citeRegEx": "Cremonesi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cremonesi et al\\.", "year": 2010}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["Fader", "Anthony", "Zettlemoyer", "Luke", "Etzioni", "Oren"], "venue": "In Proceedings of 20th SIGKDD Conference on Knowledge Discovery and Data Mining (KDD\u201914),", "citeRegEx": "Fader et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "Supervised sequence labelling with recurrent neural networks, volume 385", "author": ["Graves", "Alex"], "venue": null, "citeRegEx": "Graves and Alex,? \\Q2012\\E", "shortCiteRegEx": "Graves and Alex", "year": 2012}, {"title": "A statistical approach to spoken dialog systems design and evaluation", "author": ["Griol", "David", "Hurtado", "Llu\u0131\u0301s F", "Segarra", "Encarna", "Sanchis", "Emilio"], "venue": "Speech Communication,", "citeRegEx": "Griol et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Griol et al\\.", "year": 2008}, {"title": "Machine learning for dialog state tracking: A review", "author": ["Henderson", "Matthew"], "venue": "In Proceedings of The First International Workshop on Machine Learning in Spoken Language Processing,", "citeRegEx": "Henderson and Matthew.,? \\Q2015\\E", "shortCiteRegEx": "Henderson and Matthew.", "year": 2015}, {"title": "The second dialog state tracking challenge", "author": ["Henderson", "Matthew", "Thomson", "Blaise", "Williams", "Jason"], "venue": "In 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann", "Karl Moritz", "Ko\u010disk\u00fd", "Tom\u00e1\u0161", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Hill", "Felix", "Bordes", "Antoine", "Chopra", "Sumit", "Weston", "Jason"], "venue": "arXiv preprint arXiv:1511.02301,", "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Beyond class a: A proposal for automatic evaluation of discourse", "author": ["Hirschman", "Lynette", "Dahl", "Deborah A", "McKay", "Donald P", "Norton", "Lewis M", "Linebarger", "Marcia C"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Hirschman et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Hirschman et al\\.", "year": 1990}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Cobot in lambdamoo: A social statistics agent", "author": ["Isbell", "Charles Lee", "Kearns", "Michael", "Kormann", "Dave", "Singh", "Satinder", "Stone", "Peter"], "venue": "In AAAI/IAAI,", "citeRegEx": "Isbell et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Isbell et al\\.", "year": 2000}, {"title": "Filter, rank, and transfer the knowledge: Learning to chat", "author": ["Jafarpour", "Sina", "Burges", "Christopher JC", "Ritter", "Alan"], "venue": "Advances in Ranking,", "citeRegEx": "Jafarpour et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jafarpour et al\\.", "year": 2010}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "arXiv preprint:", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "A survey on question answering technology from an information retrieval perspective", "author": ["Kolomiyets", "Oleksandr", "Moens", "Marie-Francine"], "venue": "Information Sciences,", "citeRegEx": "Kolomiyets et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kolomiyets et al\\.", "year": 2011}, {"title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model", "author": ["Koren", "Yehuda"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Koren and Yehuda.,? \\Q2008\\E", "shortCiteRegEx": "Koren and Yehuda.", "year": 2008}, {"title": "Advances in collaborative filtering", "author": ["Koren", "Yehuda", "Bell", "Robert"], "venue": "In Recommender systems handbook,", "citeRegEx": "Koren et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Koren et al\\.", "year": 2011}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["Kwiatkowski", "Tom", "Choi", "Eunsol", "Artzi", "Yoav", "Zettlemoyer", "Luke"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP\u201913),", "citeRegEx": "Kwiatkowski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2013}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["Lowe", "Ryan", "Pow", "Nissan", "Serban", "Iulian", "Pineau", "Joelle"], "venue": "arXiv preprint arXiv:1506.08909,", "citeRegEx": "Lowe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "Context dependent recurrent neural network language model", "author": ["Mikolov", "Tomas", "Zweig", "Geoffrey"], "venue": "In SLT,", "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Extensions of recurrent neural network language model", "author": ["Mikolov", "Tom\u00e1\u0161", "Kombrink", "Stefan", "Burget", "Luk\u00e1\u0161", "\u010cernock\u1ef3", "Jan Honza", "Khudanpur", "Sanjeev"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["Narasimhan", "Karthik", "Kulkarni", "Tejas", "Barzilay", "Regina"], "venue": "arXiv preprint arXiv:1506.08941,", "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "An end-to-end dialog system for tv program discovery", "author": ["Ramachandran", "Deepak", "Yeh", "Peter Z", "Jarrold", "William", "Douglas", "Benjamin", "Ratnaparkhi", "Adwait", "Provine", "Ronald", "Mendel", "Jeremy", "Emfield", "Adam"], "venue": "In Spoken Language Technology Workshop (SLT), 2014 IEEE,", "citeRegEx": "Ramachandran et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ramachandran et al\\.", "year": 2014}, {"title": "Data-driven response generation in social media", "author": ["Ritter", "Alan", "Cherry", "Colin", "Dolan", "William B"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "An end-to-end evaluation of two situated dialog systems", "author": ["Rojas-Barahona", "Lina M", "Lorenzo", "Alejandra", "Gardent", "Claire"], "venue": "In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Rojas.Barahona et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rojas.Barahona et al\\.", "year": 2012}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Rush", "Alexander M", "Chopra", "Sumit", "Weston", "Jason"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Shang", "Lifeng", "Lu", "Zhengdong", "Li", "Hang"], "venue": "arXiv preprint arXiv:1503.02364,", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to contextsensitive generation of conversational responses", "author": ["Sordoni", "Alessandro", "Galley", "Michel", "Auli", "Michael", "Brockett", "Chris", "Ji", "Yangfeng", "Mitchell", "Margaret", "Nie", "Jian-Yun", "Gao", "Jianfeng", "Dolan", "Bill"], "venue": "Proceedings of NAACL,", "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "End-to-end memory networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": "Proceedings of NIPS,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A neural conversational model", "author": ["Vinyals", "Oriol", "Le", "Quoc"], "venue": "arXiv preprint arXiv:1506.05869,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Paradise: A framework for evaluating spoken dialogue agents", "author": ["Walker", "Marilyn A", "Litman", "Diane J", "Kamm", "Candace A", "Abella", "Alicia"], "venue": "In Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics,", "citeRegEx": "Walker et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Walker et al\\.", "year": 1997}, {"title": "A trainable generator for recommendations in multimodal dialog", "author": ["Walker", "Marilyn A", "Prasad", "Rashmi", "Stent", "Amanda"], "venue": "In INTERSPEECH,", "citeRegEx": "Walker et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2003}, {"title": "Towards AI-complete question answering: A set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "arXiv preprint:", "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Learning to rank recommendations with the k-order statistic loss", "author": ["Weston", "Jason", "Yee", "Hector", "Weiss", "Ron J"], "venue": "In Proceedings of the 7th ACM conference on Recommender systems,", "citeRegEx": "Weston et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2013}, {"title": "Towards ai-complete question answering: a set of prerequisite toy tasks", "author": ["Weston", "Jason", "Bordes", "Antoine", "Chopra", "Sumit", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1502.05698,", "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Fish or fowl: A wizard of oz evaluation of dialogue strategies in the restaurant domain", "author": ["Whittaker", "Steve", "Walker", "Marilyn A", "Moore", "Johanna D"], "venue": "In LREC,", "citeRegEx": "Whittaker et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Whittaker et al\\.", "year": 2002}, {"title": "The dialog state tracking challenge", "author": ["Williams", "Jason", "Raux", "Antoine", "Ramachandran", "Deepak", "Black", "Alan"], "venue": "In Proceedings of the SIGDIAL 2013 Conference,", "citeRegEx": "Williams et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2013}, {"title": "embedded once and cached for the whole test set. This trick is not applicable to the method described above rendering it much slower. To deal with the speed issue one can use our supervised embedding model as a first step, and then only reranking the top 100 results with the LSTM to make it tractable, however performance is still poor as mentioned. We obtained improved results by instead adopting the approach", "author": ["Narasimhan"], "venue": null, "citeRegEx": "Narasimhan,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan", "year": 2015}], "referenceMentions": [{"referenceID": 32, "context": "One recent popular approach is to train end-to-end models on a large amount of real dialog transcripts between humans (Sordoni et al., 2015; Vinyals & Le, 2015; Shang et al., 2015).", "startOffset": 118, "endOffset": 180}, {"referenceID": 31, "context": "One recent popular approach is to train end-to-end models on a large amount of real dialog transcripts between humans (Sordoni et al., 2015; Vinyals & Le, 2015; Shang et al., 2015).", "startOffset": 118, "endOffset": 180}, {"referenceID": 28, "context": "Such end-to-end dialog systems (Ritter et al., 2011; Shang et al., 2015; Vinyals & Le, 2015; Sordoni et al., 2015) directly generate a response given the last user utterance and (potentially) the context from previous dialog turns without relying on the intermediate use of a dialog state tracking component like in traditional dialog systems (e.", "startOffset": 31, "endOffset": 114}, {"referenceID": 31, "context": "Such end-to-end dialog systems (Ritter et al., 2011; Shang et al., 2015; Vinyals & Le, 2015; Sordoni et al., 2015) directly generate a response given the last user utterance and (potentially) the context from previous dialog turns without relying on the intermediate use of a dialog state tracking component like in traditional dialog systems (e.", "startOffset": 31, "endOffset": 114}, {"referenceID": 32, "context": "Such end-to-end dialog systems (Ritter et al., 2011; Shang et al., 2015; Vinyals & Le, 2015; Sordoni et al., 2015) directly generate a response given the last user utterance and (potentially) the context from previous dialog turns without relying on the intermediate use of a dialog state tracking component like in traditional dialog systems (e.", "startOffset": 31, "endOffset": 114}, {"referenceID": 36, "context": "Indeed, traditional systems have a wide range of well defined evaluation paradigms and benchmarks that measure their ability to track user states and/or to reach user-defined goals (Walker et al., 1997; Paek, 2001; Griol et al., 2008; Williams et al., 2013).", "startOffset": 181, "endOffset": 257}, {"referenceID": 8, "context": "Indeed, traditional systems have a wide range of well defined evaluation paradigms and benchmarks that measure their ability to track user states and/or to reach user-defined goals (Walker et al., 1997; Paek, 2001; Griol et al., 2008; Williams et al., 2013).", "startOffset": 181, "endOffset": 257}, {"referenceID": 42, "context": "Indeed, traditional systems have a wide range of well defined evaluation paradigms and benchmarks that measure their ability to track user states and/or to reach user-defined goals (Walker et al., 1997; Paek, 2001; Griol et al., 2008; Williams et al., 2013).", "startOffset": 181, "endOffset": 257}, {"referenceID": 28, "context": "Recent end-to-end models, on the other hand, rely either on very few human scores (Vinyals & Le, 2015), crowdsourcing (Ritter et al., 2011; Shang et al., 2015) or machine translation metrics like BLEU (Sordoni et al.", "startOffset": 118, "endOffset": 159}, {"referenceID": 31, "context": "Recent end-to-end models, on the other hand, rely either on very few human scores (Vinyals & Le, 2015), crowdsourcing (Ritter et al., 2011; Shang et al., 2015) or machine translation metrics like BLEU (Sordoni et al.", "startOffset": 118, "endOffset": 159}, {"referenceID": 32, "context": ", 2015) or machine translation metrics like BLEU (Sordoni et al., 2015) to judge the quality of the generated language only.", "startOffset": 49, "endOffset": 71}, {"referenceID": 27, "context": "Such end-to-end dialog systems (Ritter et al., 2011; Shang et al., 2015; Vinyals & Le, 2015; Sordoni et al., 2015) directly generate a response given the last user utterance and (potentially) the context from previous dialog turns without relying on the intermediate use of a dialog state tracking component like in traditional dialog systems (e.g. in Henderson (2015)).", "startOffset": 32, "endOffset": 369}, {"referenceID": 41, "context": "via Wizard-of-Oz strategies (Whittaker et al., 2002)).", "startOffset": 28, "endOffset": 52}, {"referenceID": 0, "context": "We evaluate on these tasks the performance of various neural network models that can potentially create end-to-end dialogs, ranging from simple supervised embedding models (Bai et al., 2009), RNNs with Long Short-Term Memory (LSTMs) (Hochreiter & Schmidhuber, 1997), and attention-based models, in particular Memory Networks (Sukhbaatar et al.", "startOffset": 172, "endOffset": 190}, {"referenceID": 33, "context": ", 2009), RNNs with Long Short-Term Memory (LSTMs) (Hochreiter & Schmidhuber, 1997), and attention-based models, in particular Memory Networks (Sukhbaatar et al., 2015).", "startOffset": 142, "endOffset": 167}, {"referenceID": 22, "context": "To validate the quality of our results, we also apply our best performing model, Memory Networks, in other conditions by comparing it on the Ubuntu Dialog Corpus (Lowe et al., 2015) against baselines trained by the authors of the corpus.", "startOffset": 162, "endOffset": 181}, {"referenceID": 4, "context": "Using SimpleQuestions, an existing open-domain question answering dataset based on Freebase (Bordes et al., 2015) we identified the subset of questions posed by those human annotators that covered our question types.", "startOffset": 92, "endOffset": 113}, {"referenceID": 5, "context": "Cremonesi et al. (2010).", "startOffset": 0, "endOffset": 24}, {"referenceID": 32, "context": "To evaluate the performance of models, we again separate the problem of evaluating the quality of a response from that of language generation by considering a ranking setup, in line with other recent works (Sordoni et al., 2015).", "startOffset": 206, "endOffset": 228}, {"referenceID": 36, "context": "Evaluation of the dialog state tracking stage is well defined since the PARADISE framework (Walker et al., 1997) and subsequent initiatives (Paek, 2001; Griol et al.", "startOffset": 91, "endOffset": 112}, {"referenceID": 8, "context": ", 1997) and subsequent initiatives (Paek, 2001; Griol et al., 2008), including recent competitons (Williams et al.", "startOffset": 35, "endOffset": 67}, {"referenceID": 42, "context": ", 2008), including recent competitons (Williams et al., 2013; Henderson et al., 2014) as well as situated variants (Rojas-Barahona et al.", "startOffset": 38, "endOffset": 85}, {"referenceID": 10, "context": ", 2008), including recent competitons (Williams et al., 2013; Henderson et al., 2014) as well as situated variants (Rojas-Barahona et al.", "startOffset": 38, "endOffset": 85}, {"referenceID": 29, "context": ", 2014) as well as situated variants (Rojas-Barahona et al., 2012).", "startOffset": 37, "endOffset": 66}, {"referenceID": 37, "context": "Because of language ambiguity and variation, evaluation of the response generation step is complicated and usually relies on human judgement (Walker et al., 2003).", "startOffset": 141, "endOffset": 162}, {"referenceID": 28, "context": "Most existing work (Ritter et al., 2011; Shang et al., 2015; Vinyals & Le, 2015; Sordoni et al., 2015) chose to use human ratings, which does not easily scale.", "startOffset": 19, "endOffset": 102}, {"referenceID": 31, "context": "Most existing work (Ritter et al., 2011; Shang et al., 2015; Vinyals & Le, 2015; Sordoni et al., 2015) chose to use human ratings, which does not easily scale.", "startOffset": 19, "endOffset": 102}, {"referenceID": 32, "context": "Most existing work (Ritter et al., 2011; Shang et al., 2015; Vinyals & Le, 2015; Sordoni et al., 2015) chose to use human ratings, which does not easily scale.", "startOffset": 19, "endOffset": 102}, {"referenceID": 8, "context": ", 1997) and subsequent initiatives (Paek, 2001; Griol et al., 2008), including recent competitons (Williams et al., 2013; Henderson et al., 2014) as well as situated variants (Rojas-Barahona et al., 2012). However, they require fine grained data annotations in terms of labeling internal dialog state and precisely defined user intent (goals). As a result, they do not really scale to large domains and dialogs with high variability in terms of language. Because of language ambiguity and variation, evaluation of the response generation step is complicated and usually relies on human judgement (Walker et al., 2003). End-to-end dialog systems do not rely on explicit internal state and hence do not have state tracking modules, they directly generate responses given user utterances and dialog context and hence can not be evaluated using state tracking test-beds. Unfortunately, as for response generator modules, their evaluation is ill-defined as it is difficult to objectively rate at scale the fit of returned responses. Most existing work (Ritter et al., 2011; Shang et al., 2015; Vinyals & Le, 2015; Sordoni et al., 2015) chose to use human ratings, which does not easily scale. Sordoni et al. (2015) also use the BLEU score to compare to actual user utterances but this is not a completely satisfying measure of success,", "startOffset": 48, "endOffset": 1210}, {"referenceID": 13, "context": "In that sense, it follows the the notion of dialog evaluation by a reference answer introduced in (Hirschman et al., 1990).", "startOffset": 98, "endOffset": 122}, {"referenceID": 21, "context": "Lowe et al. (2015) use a similar ranking evaluation to ours, but only in a chit-chat setting.", "startOffset": 0, "endOffset": 19}, {"referenceID": 21, "context": "Lowe et al. (2015) use a similar ranking evaluation to ours, but only in a chit-chat setting. Our approach of providing a collection of tasks to be jointly solved is related to the evaluation framework of the bAbI tasks (Weston et al., 2015a) and of the collection of sequence prediction tasks of Joulin & Mikolov (2015). However, unlike them, our Tasks 1-3 are much closer to real dialog, being built from human-written text, and with Task 4 actually involving real dialog from Reddit.", "startOffset": 0, "endOffset": 321}, {"referenceID": 13, "context": "In that sense, it follows the the notion of dialog evaluation by a reference answer introduced in (Hirschman et al., 1990). The application of movie recommender systems is connected to that of TV program suggestion proposed by Ramachandran et al. (2014), except that we frame it so that we can generate systematic evaluation from it, where they only rely on human judgement at small scale.", "startOffset": 99, "endOffset": 254}, {"referenceID": 33, "context": "Memory Networks (Weston et al., 2015c; Sukhbaatar et al., 2015) are a recent class of models that perform language understanding by incorporaring a memory component that potentially includes both long-term memory (e.", "startOffset": 16, "endOffset": 63}, {"referenceID": 4, "context": "They have only been evaluated in a few setups: question answering (Bordes et al., 2015), language modeling (Sukhbaatar et al.", "startOffset": 66, "endOffset": 87}, {"referenceID": 33, "context": ", 2015), language modeling (Sukhbaatar et al., 2015; Hill et al., 2015), and language understanding on the bAbI tasks (Weston et al.", "startOffset": 27, "endOffset": 71}, {"referenceID": 12, "context": ", 2015), language modeling (Sukhbaatar et al., 2015; Hill et al., 2015), and language understanding on the bAbI tasks (Weston et al.", "startOffset": 27, "endOffset": 71}, {"referenceID": 3, "context": "They have only been evaluated in a few setups: question answering (Bordes et al., 2015), language modeling (Sukhbaatar et al., 2015; Hill et al., 2015), and language understanding on the bAbI tasks (Weston et al., 2015a), but not so far on dialog tasks such as ours. We employ the MemN2N architecture of Sukhbaatar et al. (2015) in our experiments, with some additional modifications to construct both long-term and short-term context memories.", "startOffset": 67, "endOffset": 329}, {"referenceID": 33, "context": "We also add time features to each memory to denote their position following (Sukhbaatar et al., 2015).", "startOffset": 76, "endOffset": 101}, {"referenceID": 25, "context": "While one of the major uses of word embedding models is to learn unsupervised embeddings over large unlabeled datasets such as in Word2Vec (Mikolov et al., 2013) there are also very effective word embedding models for training supervised models when labeled data is available.", "startOffset": 139, "endOffset": 161}, {"referenceID": 0, "context": "For matching two documents supervised semantic indexing (SSI) was shown to be superior to unsupervised latent semantic indexing (LSI) (Bai et al., 2009).", "startOffset": 134, "endOffset": 152}, {"referenceID": 39, "context": "Similar methods were shown to outperform SVD for recommendation (Weston et al., 2013).", "startOffset": 64, "endOffset": 85}, {"referenceID": 3, "context": "However, we do not expect this method to work as well on question answering tasks, as all the memorization must occur in the individual word embeddings, which was shown to perform poorly in (Bordes et al., 2014).", "startOffset": 190, "endOffset": 211}, {"referenceID": 24, "context": "Recurrent Neural Networks (RNNs) have proven successful at several tasks involving natural language, language modeling (Mikolov et al., 2011), and have been applied recently to dialog (Sordoni et al.", "startOffset": 119, "endOffset": 141}, {"referenceID": 32, "context": ", 2011), and have been applied recently to dialog (Sordoni et al., 2015; Vinyals & Le, 2015; Shang et al., 2015).", "startOffset": 50, "endOffset": 112}, {"referenceID": 31, "context": ", 2011), and have been applied recently to dialog (Sordoni et al., 2015; Vinyals & Le, 2015; Shang et al., 2015).", "startOffset": 50, "endOffset": 112}, {"referenceID": 34, "context": ", 2012), seq2seq models (Sutskever et al., 2014), RNNs that take into account the document context (Mikolov & Zweig, 2012) and RNNs that perform attention over their input in various different ways (Bahdanau et al.", "startOffset": 24, "endOffset": 48}, {"referenceID": 11, "context": ", 2014), RNNs that take into account the document context (Mikolov & Zweig, 2012) and RNNs that perform attention over their input in various different ways (Bahdanau et al., 2015; Hermann et al., 2015; Rush et al., 2015).", "startOffset": 157, "endOffset": 221}, {"referenceID": 30, "context": ", 2014), RNNs that take into account the document context (Mikolov & Zweig, 2012) and RNNs that perform attention over their input in various different ways (Bahdanau et al., 2015; Hermann et al., 2015; Rush et al., 2015).", "startOffset": 157, "endOffset": 221}, {"referenceID": 21, "context": "There has been a recent surge in interest in such systems that try to answer a question posed in natural language by converting it into a database search over a knowledge base (Berant & Liang, 2014; Kwiatkowski et al., 2013; Fader et al., 2014), which is a setup natural for our QA task also.", "startOffset": 176, "endOffset": 244}, {"referenceID": 6, "context": "There has been a recent surge in interest in such systems that try to answer a question posed in natural language by converting it into a database search over a knowledge base (Berant & Liang, 2014; Kwiatkowski et al., 2013; Fader et al., 2014), which is a setup natural for our QA task also.", "startOffset": 176, "endOffset": 244}, {"referenceID": 2, "context": "This system learns embeddings that match questions to database entries, and then ranks the set of entries, and has been shown to achieve good performance on the WEBQUESTIONS benchmark (Berant et al., 2013).", "startOffset": 184, "endOffset": 205}, {"referenceID": 1, "context": "We chose the method of Bordes et al. (2014)10 as our baseline.", "startOffset": 23, "endOffset": 44}, {"referenceID": 39, "context": "However, it has been shown to be outperformed by other flavors of matrix factorization, in particular by using a ranking loss rather than squared loss (Weston et al., 2013) which we will compare to (cf.", "startOffset": 151, "endOffset": 172}, {"referenceID": 15, "context": "To select candidate responses a standard baseline is nearest neighbour information retrieval (IR) (Isbell et al., 2000; Jafarpour et al., 2010; Ritter et al., 2011; Sordoni et al., 2015).", "startOffset": 98, "endOffset": 186}, {"referenceID": 16, "context": "To select candidate responses a standard baseline is nearest neighbour information retrieval (IR) (Isbell et al., 2000; Jafarpour et al., 2010; Ritter et al., 2011; Sordoni et al., 2015).", "startOffset": 98, "endOffset": 186}, {"referenceID": 28, "context": "To select candidate responses a standard baseline is nearest neighbour information retrieval (IR) (Isbell et al., 2000; Jafarpour et al., 2010; Ritter et al., 2011; Sordoni et al., 2015).", "startOffset": 98, "endOffset": 186}, {"referenceID": 32, "context": "To select candidate responses a standard baseline is nearest neighbour information retrieval (IR) (Isbell et al., 2000; Jafarpour et al., 2010; Ritter et al., 2011; Sordoni et al., 2015).", "startOffset": 98, "endOffset": 186}, {"referenceID": 28, "context": "It has been shown previously that method (ii) performs better (Ritter et al., 2011), and our initial IR experiments showed the same result.", "startOffset": 62, "endOffset": 83}, {"referenceID": 36, "context": "Weston et al. (2015b). The We used the code available at: https://github.", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "1 of Bordes et al. (2014).", "startOffset": 5, "endOffset": 26}, {"referenceID": 32, "context": "LSTMs perform poorly: the posts in Reddit are quite long and the memory of the LSTM is relatively short, as pointed out by Sordoni et al. (2015). In that work they employed a linear reranker that used LSTM prediction as features to better effect.", "startOffset": 123, "endOffset": 145}, {"referenceID": 22, "context": "As no other authors have yet published results on our new benchmark, to validate the quality of our results we also apply our best performing model in other conditions by comparing it on the Ubuntu Dialog Corpus (Lowe et al., 2015).", "startOffset": 212, "endOffset": 231}, {"referenceID": 22, "context": "Methods with \u2020 have been ran by Lowe et al. (2015).", "startOffset": 32, "endOffset": 51}], "year": 2016, "abstractText": "A long-term goal of machine learning is to build intelligent conversational agents. One recent popular approach is to train end-to-end models on a large amount of real dialog transcripts between humans (Sordoni et al., 2015; Vinyals & Le, 2015; Shang et al., 2015). However, this approach leaves many questions unanswered as an understanding of the precise successes and shortcomings of each model is hard to assess. A contrasting recent proposal are the bAbI tasks (Weston et al., 2015b) which are synthetic data that measure the ability of learning machines at various reasoning tasks over toy language. Unfortunately, those tests are very small and hence may encourage methods that do not scale. In this work, we propose a suite of new tasks of a much larger scale that attempt to bridge the gap between the two regimes. Choosing the domain of movies, we provide tasks that test the ability of models to answer factual questions (utilizing OMDB), provide personalization (utilizing MovieLens), carry short conversations about the two, and finally to perform on natural dialogs from Reddit. We provide a dataset covering \u223c75k movie entities and with \u223c3.5M training examples. We present results of various models on these tasks, and evaluate their performance.", "creator": "LaTeX with hyperref package"}}}