{"id": "1511.07972", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2015", "title": "Learning with Memory Embeddings", "abstract": "Embedding learning, a.k.a. Cabbage.\n\n\n\nSo it\u2019s no wonder that these are some of the hottest, fastest, most popular computer systems in the world, especially those that have been working since the 1970s. We have a great list of the most popular computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just one of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers in the world, with just two of them, and the best computers", "histories": [["v1", "Wed, 25 Nov 2015 07:06:09 GMT  (19kb)", "https://arxiv.org/abs/1511.07972v1", "7 pages"], ["v2", "Tue, 1 Dec 2015 05:53:38 GMT  (22kb)", "http://arxiv.org/abs/1511.07972v2", "7 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"], ["v3", "Wed, 16 Dec 2015 23:38:03 GMT  (376kb,D)", "http://arxiv.org/abs/1511.07972v3", "14 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"], ["v4", "Mon, 21 Dec 2015 22:35:39 GMT  (218kb,D)", "http://arxiv.org/abs/1511.07972v4", "14 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"], ["v5", "Mon, 25 Jan 2016 20:02:39 GMT  (285kb,D)", "http://arxiv.org/abs/1511.07972v5", "14 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"], ["v6", "Wed, 13 Apr 2016 17:23:42 GMT  (439kb,D)", "http://arxiv.org/abs/1511.07972v6", "29 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"], ["v7", "Thu, 21 Apr 2016 04:40:58 GMT  (293kb,D)", "http://arxiv.org/abs/1511.07972v7", "29 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"], ["v8", "Thu, 5 May 2016 14:57:41 GMT  (297kb,D)", "http://arxiv.org/abs/1511.07972v8", "29 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"], ["v9", "Sat, 7 May 2016 09:06:15 GMT  (291kb,D)", "http://arxiv.org/abs/1511.07972v9", "29 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale Representation Learning"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG", "authors": ["volker tresp", "crist\\'obal esteban", "yinchong yang", "stephan baier", "denis krompa{\\ss}"], "accepted": false, "id": "1511.07972"}, "pdf": {"name": "1511.07972.pdf", "metadata": {"source": "CRF", "title": "Learning with Memory Embeddings", "authors": ["Volker Tresp", "Crist\u00f3bal Esteban", "Yinchong Yang", "Stephan Baier"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Embedding learning, a.k.a. representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].1 A key concept is a mapping of the knowledge graph to a tensor representation whose entries are predicted by models using latent representations of generalized entities. Latent variable models are well suited to deal with the high dimensionality and sparsity of typical knowledge graphs. In recent publications the embedding models were extended to also consider temporal evolutions, time patterns and subsymbolic representations [48, 49]. These extended models were used successfully to predict clinical\n1Some authors make a distinction between latent representations, which are application specific, and embeddings, which are identical across applications and might represent universal properties of entities [120, 124].\nar X\niv :1\n51 1.\n07 97\n2v 9\n[ cs\n.A I]\n7 M\nay 2\n01 6\nHuman Memory\nevents like procedures, lab measurements, and diagnoses. In this paper, we attempt to map these embedding models, which were developed purely as solutions to technical problems, to various cognitive memory functions. Our approach follows the tradition of latent semantic analysis (LSA), which is a classical representation learning approach that on the one hand has found a number of technical applications and on the other hand could be related to cognitive semantic memories [88, 87, 38].\nCognitive memory functions are typically classified as long-term, short-term, and sensory memory, where long-term memory has the subcategories declarative memory and non-declarative memory [42, 6, 131, 14, 35, 54, 57]. Figure 1 shows these main categories and finer subcategories and shows the role of working memory [9]. There is evidence that these main cognitive categories are partially dissociated from one another in the brain, as expressed in their differential sensitivity to brain damage [54]. However, there is also evidence indicating that the different memory functions are not mutually independent and support each other [76, 61].\nThe paper is organized as follows. In the next section, we introduce the unique-representation hypothesis as the basis for exchanging information between different memory functions. We present the different tensor representations of the main memory functions and discuss offline learning of the models. In Section 3 we introduce different representations for the indicator mapping function used in the memory models and in Section 4 we show how likely triples can be generated from the model using a simulated-annealing based sampling perspective. In Section 5 we discuss the path from sensory input to a semantic representation of scene information and to long-term semantic and episodic memory. In Section 6 we explain how the different memory representations form the basis of a prediction system and relate this to working memory. Section 7 represents the main results of this paper in form of a discussion of a number of postulated hypotheses for human memory. Section 8 contains our conclusions."}, {"heading": "2 Memories and Their Tensor Embeddings", "text": ""}, {"heading": "2.1 Unique-Representation Hypothesis", "text": "In this section we discuss how the different memory functions can be coded as tensors and how inference and generalization can be achieved by coupled tensor decompositions.\nWe begin by considering declarative memories. The prime example of a declarative memory is the semantic memory which stores general world knowledge about entities. Second, there is concept memory which stores information about the concepts in the world and their hierarchical organization. In contrast to the general setting in machine learning, in this paper entities are the prime focus and concepts are of secondary interest. Finally, episodic memory stores information of general and personal events [139, 140, 141, 54]. Whereas semantic memory concerns information we \u201cknow\u201d, episodic memory concerns information we \u201cremember\u201d [57]. The portion of episodic memory that concerns an individual\u2019s life involving personal experiences is called autobiographic memory.\nSemantic memories and episodic memories are long-term memories. In contrast, we also consider sensory memory, which is the shortest-time element of memory. It is the ability to retain impressions of sensory information after the original stimuli have ended [54].\nFinally, working memory is the topic of Section 6. Working memory uses the other memories for tasks like prediction, decision support and other high-level functions.\nThe unique-representation hypothesis assumed in this paper is that each entity or concept ei, each predicate ep and each time step et has a unique latent representation \u2014ai, ap, respectively, at\u2014 in form of a vector of real numbers. The assumption is that the representations are shared between all memory functions, and this permits information exchange and inference between the different memories. For simplicity we assume that the dimensionalities of these latent representations are all identical r\u0303 such that ai P Rr\u0303, ap P Rr\u0303, and at P Rr\u0303. Figure 3 shows a simple network realization."}, {"heading": "2.2 A Semantic Knowledge Graph Model", "text": "A technical realization of a semantic memory is a knowledge graph (KG) which is a triple-oriented knowledge representation. Popular large-scale KGs are DBpedia [7], YAGO [134], Freebase [21], NELL [27], and the Google Knowledge Graph [126].\nHere we consider a slight extension to the subject-predicate-object triple form by adding the value in the form (es, ep, eo; Value) where Value is a function of s, p, o and, e.g., can be a Boolean variable (True or 1, False or 0) or a real number. Thus (Jack, likes, Mary; True) states that Jack (the subject or head entity) likes Mary (the object or tail entity). Note that es and eo represent the entities for subject index s and object index o. To simplify notation we also consider ep to be a generalized entity associated with predicate type with index p. We encode attributes also as triples, mostly to simplify the discussion.\nWe now consider an efficient representation of a KG. With this representation, it is also possible to generalize from known facts to new facts (inductive inference). First, we introduce the threeway semantic adjacency tensor X where the tensor element xs,p,o is the associated Value of the triple (es, ep, eo). Here s \u201c 1, . . . , S, p \u201c 1, . . . , P , and o \u201c 1, . . . , O. One can also define a companion tensor \u0398 with with the same dimensions as X and with entries \u03b8s,p,o. It contains the natural parameters of the model and the connection to X for Boolean variables is\nP pxs,p,o|\u03b8s,p,oq \u201c sigp\u03b8s,p,oq (1)\nwhere sigpargq \u201c 1{p1 ` expp\u00b4argqq is the logistic function (Bernoulli likelihood) . If xs,p,o is a real number then we can use a Gaussian distribution with P pxs,p,o|\u03b8s,p,oq \u201e N p\u03b8s,p,o, \u03c32q. Unless specified otherwise, we will assume a Bernoulli distribution for the rest of the paper.\nAs mentioned, the key concept in embedding learning is that each entity e has an r\u0303-dimensional latent vector representation a P Rr\u0303. In particular, the embedding approaches used for modeling KGs assume that \u03b8semantics,p,o \u201c f semanticpaes ,aep ,aeoq. (2) Here, the function f semanticp\u00a8q predicts the value of the natural parameter. In the case of a KG with a Bernoulli likelihood, sigp\u03b8semantics,p,o q represents the confidence that the Value of the triple (es, ep, eo) is true and we call the function an indicator mapping function and we discuss examples in the next section.\nLatent representation approaches have been used very successfully to model large KGs, such as the YAGO KG, the DBpedia KG and parts of the Google KG. It has been shown experimentally that models using latent factors perform well in these high-dimensional and highly sparse domains. Since an entity has a unique representation, independent of its role as a subject or an object, the model permits the propagation of information across the KG. For example if a writer was born in Munich, the model can infer that the writer is also born in Germany and probably writes in the German language [104, 105]. Stochastic gradient descent (SGD) is typically being used as an iterative approach for finding both optimal latent representations and optimal parameters in f semanticp\u00a8q [106, 85]. For a recent review, please consult [106].\nDue to the approximation, sigp\u03b8semanticJack,marriedTo,eq might be smaller than one for the true spouse. The approximation also permits inductive inference: We might get a large sigp\u03b8semanticJack,marriedTo,eq also for persons e that are likely to be married to Jack and sigp\u03b8semantics,p,o q can, in general, be interpreted as a confidence value for the triple pes, ep, eoq. More complex queries on semantic models involving existential quantifier are discussed in [84].\nA concept memory would technically correspond to classes with a hierarchical subclass structure. In [103, 102] such a structure was learned from the latent representations by hierarchical clustering. In KGs, a hierarchical structure is described by type and subclass relations.\nLatent representations for modeling semantic memory functions have a long history in cognitive modeling, e.g., in latent semantic analysis [87] which is restricted to attribute-based representations. Generalizations towards probabilistic models are probabilistic latent semantic indexing [72] and latent Dirichlet allocation [20]. Latent clustering and topic models [78, 146, 2] are extensions toward multi-relational domains and use discrete latent representations. See also [93, 62, 63]. Spreading activation is the basis of the teachable language comprehender (TLC), which is a network model\nof semantic memory [30]. Associate models are the symbolic ACT-R [4, 5] and SAM [114]. [107] explores holographic embeddings with representation learning to model associative memories. An attractive feature here is that the compositional representation has the same dimensionality as the representation of its constituents. Connectionists memory models are described in [73, 96, 28, 82, 67, 68]."}, {"heading": "2.3 An Event Model for Episodic Memory", "text": "Whereas a semantic KG model reflects the state of the world, e.g, of a clinic and its patients, observations and actions describe factual knowledge about discrete events, which, in our approach, are represented by an episodic event tensor. In a clinical setting, events might be a prescription of a medication to lower the cholesterol level, the decision to measure the cholesterol level and the measurement result of the cholesterol level; thus events can be, e.g., actions, decisions and measurements.\nThe episodic event tensor is a four-way tensor Z where the tensor element zs,p,o,t is the associated Value of the quadruple (es, ep, eo, et). The indicator mapping function then is\n\u03b8episodics,p,o,t \u201c f episodicpaes ,aep ,aeo ,aetq\nwhere we have added a representation for the time of an event by introducing the generalized entity et with latent representation aet . This latent representation compresses all events that happen at time t.\nAs examples, the individual can recall \u201cWho did I meet last week?\u201d by eo \u201c arg maxe \u03b8 episodic Myself,meet,e,LastWeek and \u201cWhen did I meet Jack?\u201d by et \u201c arg maxe \u03b8 episodic Myself,meet,Jack,e.\nExamples from our clinical setting would be: (Jack, orderBloodTest, Cholesterol, Week34; True) for the fact that a cholesterol blood test was ordered in week 34 and (Jack, hasBloodTest, Cholesterol, Week34; 160) for the result of the blood test. Note that we consider an episodic event memory over different subjects, predicates and objects; thus episodic event memory can represent an extensive event context!\nAn event model can be related to the cognitive concept of an episodic memory (Figure 1). Episodic memory represents our memory of experiences and specific events in time in a serial form (a \u201cmental time travel\u201d), from which we can reconstruct the actual events that took place at any given point in our lives [127]2. In contrast to semantic memory, it requires recollection of a prior experience [140].\nFor a particular instance in time t, the \u201cslice\u201d of the event tensor Zt describes events as a, typically very sparse, triple graph. Some of the elements of this triple graph will affect changes in the KG [48, 49] (see also the discussion in Section 7). For example the event model might record a diagnosis which then becomes a fact in the KG. Also the common representations for subject, predicate, and object lead to a transfer from the event model to the semantic KG model (see also the discussion in Section 7)."}, {"heading": "2.4 Autobiographical Event Tensor", "text": "In some applications we want to consider the episodic information specific to an individual. For example, in a patient model, one is interested in what happened to the individual at time t and not what happened to all patients at time t. The autobiographical event tensor is simply the sub-tensor Zs concerning the events of the individual only. We then obtain a personal time es\u201ci,t with latent representation aes\u201ci,t . Whereas aet is a latent representation for all events for all patients at time t, aes\u201ci,t is a latent representation for all events for patients i at time t [48, 49].\nThe autobiographical event tensor would correspond to the autobiographical memory, which stores autobiographical events of an individual on a semantic abstraction level [33, 54]. The autobiographical event tensor can be related to Baddeley\u2019s episodic buffer and, in contrast to Tulving\u2019s concept of episodic memory, is a temporary store and is considered to be a part of working memory [10, 76, 11].\n2http://www.human-memory.net/types episodic.html"}, {"heading": "2.5 A Sensory Buffer", "text": "We assume that the sensor input consists of Q-channels and that at each time step t a buffer is constructed of N samples of the Q channels. \u03b3 \u201c 0, . . . , N specifies the time location within the buffer (see also Figure 2). In contrast to the event buffer, the sensory buffer operates at a subsymbolic level. Technically it might represent measurements like temperature and pressure, and in a cognitive model, it might represent input channels from the senses. The sensory buffer might be related to the mini-batches in Spark Streaming where data is captured in buffers that hold seconds to minutes of the input streams [149].\nThe sensory buffer is described by a three-way tensor U where the tensor element uq,\u03b3,t is the associated Value of the triple (eq, e\u03b3 , et). eq is a generalized entity for the q-th sensory channel, e\u03b3 specifies the time location in the buffer and et is a generalized entity representing the complete buffer at time t.\nWe model \u03b8sensoryq,\u03b3,t \u201c f sensorypaeq ,ae\u03b3 ,aetq\nwhere aeq is the latent representations for the sensor channel eq and ae\u03b3 is the latent representations for e\u03b3 . Latent components corresponds to complex time patterns (chunks) whose amplitudes are determined by the components of aet ; thus complex sensory events and sensory patterns can be modelled.\nIn a technical application [49], the sensors measure, e.g., wind speed, temperature, and humidity at the location of wind turbines and the sensory memory retains the measurements from t\u00b4 1 to t. In human cognition, sensory memory (milliseconds to a second) represents the ability to retain impressions of sensory information after the original stimuli have ended [139, 31, 54]. The transfer of sensory memory to short-term memory (e.g., the autobiographical episodic buffer) is the first step in some memory models, in particular in the modal theory of Atkinson and Shiffrin [6]. New evidence suggests that short-term memory is not the sole gateway to long-term memory [54]. Sensory memory is thought to be located in the brain regions responsible for the corresponding sensory processing. Sensory memory can be the basis for sequence learning and the detection of complex time patterns."}, {"heading": "2.6 Comment", "text": "The different memories and their tensor representations and models are summarized in Figure 2. Under the unique-representation hypothesis assumed in this paper, the latent representations of generalized entities are central for retrieval and prediction: the memory does not need to store all the facts and relationships about an entity. Also, there is no need to explicitly store the semantic graph explicitly. At any time, an approximation to the graph can be reconstructed from the latent representations. See also the discussion in Section 7."}, {"heading": "2.7 Cost Functions", "text": "Each memory function generates a term in the cost function (see Appendix) and all terms can be considered in training to adapt all latent representations and all parameters in the various functional mappings. Note that this is a global optimization step involving all available data.3 In general, we assumed a unique-representation for an entity, for example we assume that aes is the same in the prediction model and in the semantic model. Sometimes it makes sense to relax that assumption and only assume some form of a coupling. Technically there are a number of possibilities: For example, the prediction model might be trained on its own cost function, using the latent representations from the knowledge graph as an initialization; alternatively, one can use different weights for the different cost function terms. Some investigators propose that only some dimensions of the latent representations should be shared [3, 1]. 4 [89, 19, 17] contain extensive discussions on the transfer of latent representations. It is important to note that by considering only conditional probability\n3In human memory, one might speculate that this might be a step performed during sleep. 4In the technical solutions [48, 49], we got best results by focussing on the cost function that corresponded to the problem to solve. For example in prediction tasks we optimized the latent representations and the parameters using the prediction cost function.\nmodels (e.g., Value, conditioned on subject, predicate and object), no global normalization needs to be considered in training."}, {"heading": "3 Modelling the Indicator Mapping Function", "text": ""}, {"heading": "3.1 Using General Function Approximators", "text": "Consider the semantic KG. Here, the indicator mapping function f semanticp\u00a8q can be modelled as a general function approximator, such as a feedforward \u201cmultiway\u201d neural network (NN), where the index neurons representing es, ep, and eo are activated at the input and the response is generated at the output, as shown in the top of Figure 4. With this model it would be easy to query for the plausibility of a triple pes, ep, eo; Valueq, but other queries would be more difficult to handle. An alternative model is shown at the bottom of Figure 4 with inputs es and ep and where a function approximator predicts a latent representation vector hobject with components\nhobjectr \u201c f semantic, objectr paes ,aepq r \u201c 1, . . . , r\u0303.\nThe function f semanticp\u00a8q is now calculated as an inner product between the predicted latent representation and the latent representation of the objects as\nf semanticpaes ,aep ,aeoq \u201c aJeof semantic, objectpaes ,aepq \u201c aJeoh object. (3)\nHere, f semantic, object \u201c pf semantic, object1 , . . . , f semantic, object r\u0303 qJ.\nThus the response to the query pJack, likes, ?q can be obtained by activating the index neurons for Jack and likes at the input and by considering index neurons at the outputs with large values. Note that with f \u201c f semantic, objectp\u00a8q, a function approximator produces a latent representation vector h and the activation of the output index neurons corresponds to the likelihood that eo is the right answer. We call this modelling approach indicator mapping by representation prediction."}, {"heading": "3.2 Tensor Decompositions", "text": "Tensor decompositions have also shown excellent performance in modelling KGs [106]. In tensor decompositions, the indicator mapping function f semanticp\u00a8q is implemented as a multilinear model. Of particular interest are the PARAFAC model (canonical decomposition) with\nf semanticpaes ,aep ,aeoq \u201c r\u0303 \u00ff\nr\u201c1 aes,r aep,r aeo,r\nand the Tucker model with\nf semanticpaes ,aep ,aeoq \u201c r\u0303 \u00ff\nr1\u201c1\nr\u0303 \u00ff\nr2\u201c1\nr\u0303 \u00ff\nr3\u201c1 aes,r1 aep,r2 aeo,r3 gpr1, r2, r3q.\nHere, gpr1, r2, r3q P R are elements of the core tensor G P Rr\u0303\u02c6r\u0303\u02c6r\u0303. Finally, the RESCAL model [104] is a Tucker2 model with\nf semanticpaes ,aep ,aeoq \u201c r\u0303 \u00ff\nr1\u201c1\nr\u0303 \u00ff\nr2\u201c1 aes,r1 aeo,r2 gpr1, r2, epq\nwith core tensor G P Rr\u0303\u02c6r\u0303\u02c6P . In all these models, we use the constraint that a generalized entity has a unique latent representation.\nAn attractive feature of tensor decompositions is that, due to their multilinearity, representation prediction models can easily be constructed: For the PARAFAC model, hobjectr \u201c aes,r aep,r, for Tucker, hobjectr \u201c\n\u0159r\u0303 r1\u201c1 \u0159r\u0303 r2\u201c1 aes,r1aep,r2 gpr1, r2, rq and for RESCAL h object r \u201c\n\u0159r\u0303 r1\u201c1 aes,r1 gpr1, r, epq. The architectures for the Tucker model are drawn in Figure 5.\n),,(,, ops eee semanticsemantic ops f aaa=q"}, {"heading": "4 Querying Memories", "text": ""}, {"heading": "4.1 Function Approximator Models", "text": "In many application one is interested in retrieving triples with a high likelihood, conditioned on some information, thus we are essentially faced with an optimization problem. To answer a query of the form pJack, likes, ?q we need to solve\narg max aeo\nf semanticpJack, likes,aeoq.\nOf course one is often interested in a set of likely answers.\nWe suggest to address querying via a simulated annealing approach. We define an energy function Eps, p, oq \u201c \u00b4f semanticpaes ,aep ,aeoq and define a Boltzmann distribution as\nP ps, p, oq \u201c 1 Zp\u03b2q exp\u03b2f semanticpaes ,aep ,aeoq.\nHere Zp\u03b2q is the partition function that normalizes the distribution and \u03b2 \u011b 0 is an inverse temperature. Note that we now have generated a probability distribution where subject, predicate, and object are the random variables!5\nNow to answer the query, pJack, likes, ?q, we sample from\nP po|s, pq \u201c 1 Zps, p, \u03b2q exp\u03b2f semanticpaes ,aep ,aeoq\nwith s \u201c Jack and p \u201c likes. The artificial inverse temperature \u03b2 \u011b 0 can determine if we are interested in just sampling the most likely response (large \u03b2) or are also interested in responses with a smaller probability (small \u03b2). Similarly, we can derive models for P ps|p, oq and P pp|s, oq.6"}, {"heading": "4.2 Tensor Models", "text": "By enforcing nonnegativity of the factors and the core tensor entries, we can define a probabilistic model for a Tucker model with Eps, p, oq \u201c \u00b4 log f semanticpaes ,aep ,aeoq as\nP ps, p, oq9 \u02dc r\u0303 \u00ff\nr1\u201c1\nr\u0303 \u00ff\nr2\u201c1\nr\u0303 \u00ff\nr3\u201c1 aes,r1 aep,r2 aeo,r3 gpr1, r2, r3q\n\u00b8\u03b2\n. (4)\nAn attractive feature of tensor models is that marginals and conditionals can easily be obtained. Here, we look at the Tucker model. For P po|s, pq we we can use the Equation 4 with appropriate normalization. For P pp|sqwe use the same equation where we replace aeo with a\u0304object \u201c \u0159\no aeo . For P psq we use the same equation again where we replace in addition aep with a\u0304predicate \u201c \u0159\np aep . As shown in the architecture in Figure 6, these operations can easily be implemented. Marginalization means that the index neurons are all active, indicated by the vector of ones in the figure.7\nWe can use these models to generate samples from the distribution by first generating a sample for s from P psq, then a sample from p from P pp|sq, and finally a sample from o using P po|s, pq. By repeating this process we can obtain independent samples from P ps, p, oq! Note that there is a certain equivalence between tensor models and sum-product networks, where similar operations for marginals and conditionals can be defined [111].\nWe can generalize the approach to all memory functions by defining suitable energy functions. We want to emphasize that we use the probability distributions only for query-answering and not for learning!"}, {"heading": "5 From Sensory Memory to Semantic Decoding", "text": "We now consider the situation that a new sensor input becomes available for time t. With all other latent representations and functional mappings fixed, the challenge is to calculate a new latent representation htimet . Since for a new sensory input at time t, the only available information is the sensory buffer u:,:,t there is a clear information propagation from sensory input to the episodic memory. We assume a nonlinear map of the form\nhtimet \u201c fM pvecpu:,:,tqq (5)\nwhere fM p\u00a8q is a function to be learned [147] (see Figure 8) and where vecpu:,:,tq are vectorized representations from the portion of the sensory tensor associated with the individual at time t. Depending on the application, fM p\u00a8q can be a simple linear map, or it can be a second to last layer in a deep neural network as in the face recognition application DeepFace [136, 101]. In general, we assume that fM p\u00a8q is realized by a set of functions, where each function focusses on different aspects of the sensory inputs (Figure 8). For example, if the sensory input is an image, one function might analyse color, another ones shape and a third one texture.\nOne can think of htimet as the latent representation of a query; the decoding in the semantic decoder then corresponds to the answer to the query.\n5Previously, only the Value conditioned on subject, predicate, and object was random. 6In the Appendix in Subsection 9.2 (Figure 11) we describe how samples from P psq, P pp|sq, and P po|s, pq can be obtained. 7Note that to derive the equations for marginalization and conditioning we work with \u03b2 \u201c 1; \u03b2 \u2030 1 is relevant during sampling.\nAssuming that a Tucker model is used for decoding, the conditional probability becomes89\nP ps, p, o|vecpu:,:,tqq9 \u02dc r\u0303 \u00ff\nr1\u201c1\nr\u0303 \u00ff\nr2\u201c1\nr\u0303 \u00ff\nr3\u201c1\nr\u0303 \u00ff\nr4\u201c1 aes,r1 aep,r2 aeo,r3 ht,r4 gpr1, r2, r3, r4q\n\u00b8\u03b2\n. (6)\nA sampling approach for decoding with a Tucker model is shown in Figure 7.\nFor general function approximators one needs to train separate models for the different conditional and marginal probabilities, as discussed in Subsection 9.2 (Figure 12).\nNote that in the decoding step we transfer information from a subsymbolic sensory representation to a symbolic semantic representation.\nAlso note that, in pure perception, no learning of any kind needs to be involved. Only when the sensory input is significant, e.g. novel, unexpected or attached with emotions, then the time index neuron et is generated which stores htimet as its latent representation aet . By this operation an episode or event is generated. The time index neuron and its latent representation are eventually transferred to long-term episodic memory (Figure 8)."}, {"heading": "6 Predictions with Memory Embeddings and Working Memory", "text": "In this section we focus on working memory, which orchestrates the different memory functions, e.g. for prediction and decision making. In a way working memory represents the intelligence on top of the memory functions and links to complex decision making and consciousness have been made. Here we will focus on the restricted but important task of prediction. For example, in a clinical\n8To ensure nonnegativity one might want to model htimet \u201c exp fM pvecpu:,:,tqq. 9This equation describes a special form of a conditional random field. With proper local normalization it\nbecomes a conditional multinomial probabilistic mixture model.\nsetting, it is important to know what should be done next (e.g., prediction of a medical procedure) or what event will happen next (e.g., prediction of a medical diagnosis).\nWe propose that prediction should be happen at the level of the latent representation for time, i.e., h\u0302, which is the output of the sensory map, and we consider two cases."}, {"heading": "6.1 ARX Model for Predicting Latent Representations of Time", "text": "Here we assume that h\u0302 is a deterministic function of the sensory input via Equation 5 but not of past time latent representations. There might be time dependencies in the sensory input; due to high dimensionality of the input, it is easier to model the dependencies between the latent representations instead, as h\u0302timet \u201c f predictpaet\u00b41 ,aet\u00b42 , . . . ,aet\u00b4W ,aeindiviualq. But note that this model is only used for prediction htimet and as soon as the sensory input is available, it overrides the prediction with Equation 5! The model is also suitable for novelty detection: if h\u0302 is different from htimet , then the sensory scene might be novel.\nNote that we also include the latent representation of the individual aeindiviual which can be interpreted as a representation of the state of the individual.\nThe model can be interpreted as an autoregressive model on the latent representations with external inputs, ARX (Figure 9, top). The parameter W\u0303 is the size of the time window and might be related to the capacity of short-term memory, i.e., the number of items the working memory can consider in decision making."}, {"heading": "6.2 Recurrent Model", "text": "Here we extend the model is Equation 7 to include past information of the latent representation as\nhtimet \u201c fRNN pvecpu:,:,tq,aet\u00b41 ,aeindiviualq. (7) Note that this is the structure of a recurrent neural network and the assumption is that the latent state depends on both sensory input and the previous latent state. The architecture is shown in Figure 9, bottom.\nBoth models are reasonable for different purposes and make different assumptions. In fact, both models might play a role in human cognition.\nAlternatively one might use networks with additional memory buffers and attention mechanisms [71, 143, 60, 86, 58]."}, {"heading": "7 Hypotheses on Human Memory", "text": "This section speculates about the relevance of the presented models to human memory functions. In particular we present several concrete hypotheses. Figure 10 shows the overall model and explains the flow of sensory input to long-term memory and semantic decoding."}, {"heading": "7.1 Triple Hypothesis", "text": "A main assumption of course is that semantic memory is described by triples, and that episodic memory is described by triples in time, i.e., quadruples. In a way this is the perspective from which this paper has been written. Arguments for this representation are that higher-order relations can always be reduced to triples and that triple representations have large practical significance and have been used in large-scale KGs."}, {"heading": "7.2 Unique-representation Hypothesis for Entities and Predicates", "text": "The unique-representation hypothesis states that each generalized entity e is represented by an index neuron and a unique (rather high-dimensional) latent representation ae that is stored as weight patterns connecting the index neurons with neurons in the representation layer (see Section 2 and\nshown in Figure 3). Note that the weight vectors might be very sparse and in some models nonnegative. They are the basis for episodic memory and semantic memory. The latent representations integrate all that is known about a generalized entity and can be instrumented for prediction and decision support in working memory. Among other advantages, a common representation would explain why background information about an entity is seemingly effortlessly integrated into sensor scene understanding and decision support by humans, at least for entities familiar to the individual.\nResearchers have reported on a remarkable subset of medial temporal lobe (MTL) neurons that are selectively activated by strikingly different pictures of given individuals, landmarks or objects and in some cases even by letter strings with their names [113, 112]. For example, neurons have been shown to selectively respond to famous actors like \u201cHalle Berry\u201d. Thus a local encoding of index neurons seems biologically plausible.\nAs stated before, we do not insist that index neurons representing single entities exist as such in the brain, rather that there is a level of abstraction, which is equivalent to an index neuron, e.g., an ensemble of neurons.\nOur hypothesis supports both locality and globality of encoding [96, 43], since index neurons are local representations of generalized entities, whereas the representation layers would be highdimensional and non-local.\nFigure 10 shows index layers and representation layers for entities and relation types on the left. Note, that in the figure we draw different index neurons for entities in their roles as subject and object. In a way this is an artefact of the visualization of the sampling process. We maintain the hypothesis that an entity has a unique index neuron and a unique latent representation.\nAn interesting question is if the latent dimensions have a sensible and maybe useful interpretation, which the brain might exploit!\nOften neurons with similar receptive fields are clustered together in sensory cortices and form a topographic map [57]. Topological maps might also be the organizational form of neurons representing entities. Thus, entities with similar latent representations might be topographically close. A detailed atlas of semantic categories has been established in extensive fMRI studies showing the involvement of the lateral temporal cortex (LTC), the ventral temporal cortex (VTC), the lateral parietal cortex (LPC), the medial parietal cortex (MPC), the medial prefrontal cortex, the superior prefrontal cortex (SPFC) and the inferior prefrontal cortex (IPFC) [74].\nAlthough the established assumption is that no new neurons are generated in the adult cortex, topographic maps might change, e.g., due to injury, and exhibit considerably plasticity. Consequently, one might speculate that index neurons for novel entities not yet represented in the cortex need to be integrated in the existing topographic organization. This would not be a contradiction to our model, since, although we require some representation for index neurons, it is irrelevant which individual neurons represent which entities. Index and representation neurons for new entities might be allocated in the hippocampus, although, and their function later be transferred to the cortex."}, {"heading": "7.3 Representation of Concepts", "text": "So far our discussion focussed on generalized entities and their latent representations and similarity between entities was expressed by the similarity in their latent representations. In contrast, machine learning is typically concerned with the assignments of entities to concepts. Concepts bring a certain order: for example one can imply certain properties by knowing that Cloe is a cat. Concept learning is not the main focus of this paper and we only want to describe one simple realization. Consider that we treat a concept simply as another entity with its own latent representation, as, e.g., in [105]. We can introduce the relation type type, which links entities with their concepts. The inductive inference during model learning can then materialize that Cloe is also a mammal and a living being and that, by default, it has typical cat-attributes."}, {"heading": "7.4 Spatial Representations", "text": "In our proposed model, we can treat locations just as any other entity. An example would be pMary, observedIn,TownHall,LastFridayq. To model that the individual her- or himself was at the Townhall last Friday, a triple would be sufficient such as pmeLocation,TownHall,LastFridayq and\nan individual\u2019s spatial decoding might be done by a dedicated circuitry separate from semantic decoding."}, {"heading": "7.5 Sensory Input is Transformed into a Latent Representation for Time", "text": "In our model we assume that each sensory impression is decoded into a time latent representation htimet \u201c aet byM -map fM p\u00a8q, which actually might be implemented as a set of modules, responsible for different aspects of the sensory input.\nThus, htimet is a representation shared between the sensory buffer and the episodic memory and might play a role in the phonological loop and the visuospatial sketchpad. fM p\u00a8q is the most challenging component in the system.10 The training of fM p\u00a8q to refine its operation would correspond to perceptual learning in cognition. In the brain, fM p\u00a8q would likely be implemented by the different sensor pathways, e.g., the visual pathway and the auditory pathway and could contain internal feedback loops. Note that we would assume that the connection between the sensory representation and the time-representation is to some degree bi-directional, thus the time representation also feeds back to sensory impressions."}, {"heading": "7.6 New Representations are formed in the Hippocampus and are then Transferred to Long-Term Episodic and Semantic Memories", "text": "If sensory impressions are significant, a time index neuron et is formed and sensory information is quickly implemented as a weight pattern aet \u201c htimet , as shown in Figures 8 and 10. The time index neurons might be ordered sequentially, so the brain maintains a notion of temporal closeness and temporal order. Index neurons for time, i.e., et, might be formed in the hippocampal region of the brain. Evidence for time cells have recently been found [46, 44, 80, 79]. It has been observed that the hippocampus becomes activated when the temporal order of events is being processed [91, 119, 118]. Our model is in accordance with the concept that perceived sensations are decoded in the various sensory areas of the cortex, and then combined in the brains hippocampus into one single experience.\nAccording to our proposed model, the hippocampus would need to assign new time neurons during lifetime. In fact, it has been observed that the adult macaque monkey forms a few thousand new neurons daily [57, 59], possibly to encode new information [16]. Neurogenesis has been established in the dentate gyrus (part of the hippocampal formation) which is thought to contribute to the formation of new episodic memories.\nThe hippocampus might be the place where new index neurons and representations are generated in general, i.e., also for new places and entities. Certainly, the hippocampus is involved in forming new spatial representations. There are multiple, functionally specialized, cell types of the hippocampalentorhinal circuit, such as place, grid, and border cells [99]. Place cells fire selectively at one or few locations in the environment. Place, grid and border cells likely to interact with each other to yield a global representation of the individuals changing position. Once encoded, the memories must be consolidated. Spatial memories, as other memories, are thought to be slowly induced in the neocortex by a gradual recruitment of neocortical memory circuits in long-term storage of hippocampal memories [97, 132, 52, 99].\nThe fast implementation of weight patterns in the hippocampal area is discussed under the term synaptic consolidation and occurs within minutes to hours, and as such is considered the \u201cfast\u201d type of consolidation.\nAccording to our theory, the hippcampus would need to be well connected to the association areas of the cortex. Indeed, the hippocampus receives inputs from the unimodal and polymodal association areas of the cortex (visual, auditory, somatosensory) by a pathway involving the perirhinal and parahippocampal cortices which project to the entorhinal cortex which then projects to the hippocampus. All these structures are part of the MTL. The perirhinal and parahippocampal cortices also project back to the association areas of the cortex [54].\n10A simple special case is when u:,:,t already is on a semantic level. This is the case in the medical application described in [49, 48] where u:,:,t describes procedures and diagnosis and one can think of fM p\u00a8q as being an encoder system and f episodicp\u00a8q as being a decoder and the complex as being an autoencoder [24, 70].\nFigure 10 (bottom right) also indicates a slow transfer to long-term episodic memory. The hypothesis is that the index neurons and their latent representation form the basis for episodic memory! Biologically, this is referred to as system consolidation, where hippocampus-dependent memories become independent of the hippocampus over a period of weeks to years. According to the standard model of memory consolidation [132, 51] memory is retained in the hippocampus for up to one week after initial learning, representing the hippocampus-dependent stage. Later the hippocampus representations of this information become active in explicit (conscious) recall or implicit (unconscious) recall like in sleep. During this stage the hippocampus is \u201cteaching\u201d the cortex more and more about the information and when the information is recalled it strengthens the cortico-cortical connection thus making the memory hippocampus-independent. Therefore from one week and beyond the initial training experience, the memory is slowly transferred to the neo-cortex where it becomes permanently stored. In this sense the MTL would act as a relay station for the various perceptual input that make up a memory and stores it as a whole event. After this has occurred the MTL directs information towards the neocortex to provide a permanent representation of the memory.\nIn our technical model we consider two mechanisms for the transfer: Index neurons generated in the hippocampus and their representation pattern might become part of the episodic memory, or neurons in the episodic memory are trained by replay: this teaching process would be performed by the activation of the time index neurons, which then activate the \u201csketchpad\u201d aet which then trains the weight patterns of time index neurons in long-term episodic memory.\nAs events are transferred from the hippocampus to episodic memory, index neurons for places and entities and their latent representations would be consolidated in semantic long-term memory.\nThe frontal cortex, associated with higher functionalities, plays a role in which new information gets encoded as episodic and semantic memory and what gets forgotten [57].\nThe consolidation of memory might be guided by novelty, attention, and emotional significance. There is growing evidence that the amygdala is instrumental for storing emotionally significant memories. The amygdala belongs to the MTL and consists of several nuclei but is not considered to be a part of memory itself [26]. The amygdala and the orbitofrontal cortex might also provide reward-related information to the hippocampus [118].\nIt has been shown in many studies that a loss of function of the hippocampus/MTL brain region leads to a loss of the consolidation of memory to episodic long-term memory, but that this loss does not affect semantic memory. Our model supports this hypothesis, since semantic memory only relies on the latent representation of subject, predicate, and object, whereas episodic memory also relies on a latent representation of time, i.e., aet ."}, {"heading": "7.7 Tensor Memory Hypothesis", "text": "The hypothesis states that semantic memory and episodic memory are implemented as functions applied to the latent representations involved in the generalized entities which include entities, predicates, and time. Thus neither the knowledge graph nor the tensors ever needs to be stored explicitly! Due to the similarity to tensor decomposition, we call this the tensor memory hypothesis."}, {"heading": "7.8 The Semantic Decoding Hypothesis and Association", "text": "htimet is generated from sensory input and is the basis for episodic memory. For a semantic interpretation of sensory input and for a recall of episodic memory, htimet can be rapidly decoded by the semantic decoder shown in the center of Figure 10. As discussed in Sections 5 and 4, our model suggests that decoding happens by the generation of ps, p, oq-triples by a stochastic sampling procedure. Since a sensory input, in general, is described by several triples, this generation process is repeated several times, generating a number of ps, p, oq-triples. By sequential sampling, only one triples is active at a time and the ensemble of triples represents the query answer. Sequential sampling might also be influenced by attention mechanisms, e.g., in the decoding of complex scenes [145, 142, 77].\nThe proposed model can be related to encoder-decoder networks [135] which produce text sequences, whereas we produce a set of likely triples. fM p\u00a8q would be the encoder, potentially with internal feedback loops, htimet would be the representation shared between encoder and decoder, and the semantic decoder in our proposed model would correspond to the decoder.\nA clear indication that a semantic decoding is happening quickly is that an individual can describe a scene verbally immediately after it has happened.11\nIn the past, a number of neural winner-takes-all networks have been proposed where the neuron with the largest activation wins over all other neurons, which are driven to inactivity [94, 69]. Due to the inherent noise in real spiking neurons, it is likely that winner-takes-all networks select one of the neurons with large activities, not necessarily the one with the largest activity. Thus winner-takesall sampling might be close to the sampling process specified in the theoretical model. One might speculate that a winner-tales-all operation is performed in the complex formed by the dentate gyrus and the region III of hippocampus proper (CA3). It is known that CA3 contains many feedback connections, essential for winner-takes-all computations [95, 56, 118]. CA3 is sometimes modelled as a continuous attractor neural network (CANN) with excitatory recurrent colateral connections and global inhibition [118].\nThe sampling denoises the scene interpretation. Each ps, p, oq-sample represents a sharp hypothesis; an advantage of the sampling approach is that no complex feedback mechanisms are required for the generation of attractors, as in other approaches.\nThe proposed sampling procedure is a step-wise procedure which generates independent samples. An alternative might be a Gibbs sampler which could be implemented as easily. The advantage of a Gibbs sampler is that it does not require marginalization; a disadvantage is that the generated samples are not independent. On the other hand, correlated samples might be the basis for free recall, associative thinking and chaining.\nFor association we can fix an entity s, generate its latent representation aes and then sample a new entity s1 based on this latent representation, thus, we can explore entities that are very similar to the original entity. Thus Barack Obama might produce Michelle Obama. During sampling the roles of subjects might be interchanged. Thus the triple (Obama, presidentof, USA) might produce samples describing properties and relationships of the USA.\nThe restricted Boltzmann machine (RBM) might be an interesting option for supporting the decoding process [128, 66].\nAs discussed in the caption of Figure 10 it is even possible to operate the model in reverse: If we consider a person s to be the input, marginalize out p and o and consider htimet as the output, then we can recall when we met the person by exciting the time index neuron, and we can even recall her appearance by operating fM p\u00a8q in reverse. According to our model the recall of episodic memory would be driven by an activation of the time latent representation aet , which is then semantically decoded and elucidates sensory impressions. This fits the subjective feeling of a reconstruction of past memory.\nM -mapping, prediction, and semantic decoding are fast operations possibly involving many parts of the cortex.12\nThe semantic coding and decoding in our proposed model might biologically be located in the MTL. There is growing evidence that the hippocampus plays an important role not just in encoding but also in decoding of memory and is involved in the retrieval of information from long-term memory [54]. The binding of items and cortex (BIC) theory states that the perirhinal cortex (anterior part of the parahippocampal region) connects to the \u201cwho\u201d and what\u201d pathways of unimodal sensory brain regions. In our model this information is decoded into ps, p, oq-triples. In contrast the \u201cwhen\u201d and \u201cwhere\u201d parts pass through the posterior part of the parahippocampal region. Both types of information then pass through the entorhinal cortex but only converge within the hippocampus where it enables a full recognition of an episodic event [45, 39, 115, 54]. The \u201cwhat\u201d pathway is involved in the anterior temporal (AT) system also involving parts of the temporal lobe (ventral temporopolar cortex) and is associated with semantic memory. The \u201cwhere\u201d pathway is part of the posterior\n11The language considered here is very simple and consists of triple statements. 12The physicist Eugene Wigner has speculated on the \u201cThe Unreasonable Effectiveness of Mathematics in the Natural Sciences\u201d [144]; in other words mathematics is the right code for the natural sciences. Similarly, semantics might be considered the language for the world, in as far as humans are involved and one might speculate about its unreasonable effectiveness as well.\nmedial (PM) system also involving parts of the parietal cortex (retrospinal cortex) and is associated with semantic memory."}, {"heading": "7.9 Semantic Memory and Episodic Memory", "text": "As discussed, episodic memory is implemented in form of time index neurons and their latent representations aet , and is decoded using the latent representations for subjects, predicates and objects. But what about semantic memory? In Section 3 (Figures 4 and Figures 6) we describe a semantic memory which is implemented as a separate indicator mapping function that is also based on the latent representations of subject, predicate and object.\nBiologically it might be quite challenging to transfer episodic memory into semantic memory. An alternative, with a number of interesting consequences, is that the semantic memory is generated from episodic memory by marginalizing time, as shown in the bottom of Figure 7. In this interpretation, semantic memory is a long-term storage for episodic memory. Thus to answer the query \u201cwhat events happened at time t\u201d, the system needs to retrieve aet and perform a semantic decoding into ps, p, oq-triples. In contrast, to decode a triple from semantic memory, aet is replaced with a\u0304 \u201c \u0159\nt aet , which can either be calculated by inputting a vectors of ones or by learning a long-term average (Figure 12(D)).13\nThis form of a semantic memory is very attractive since it requires no additional modelling effort and can use the same structures that are needed for episodic memory! It has been argued that semantic memory is information we have encountered repeatedly, so often that the actual learning episodes are blurred [32, 57]. A gradual transition from episodic to semantic memory can take place, in which episodic memory reduces its sensitivity and association to particular events, so that the information can be generalized as semantic memory. Without doubt, semantic and episodic memories support one another [61]. Thus some theories speculate that episodic memory may be the \u201cgateway\u201d to semantic memory [12, 131, 8, 133, 129, 97, 148, 86]. [98] is a recent overview on the topic. Our model would also support the alternative view of Tulving that episodic memory depends on the semantic memory, i.e., the representations of entities and predicates [141, 57]. But note that studies have also found an independent formation of semantic memories, in case that the episodic memory is dysfunctional, as in certain amnesic patients: Amnesic patients might learn new facts without remembering the episodes during which they have learned the information [54]. This phenomenon is supported by our proposed model since there is a direct path from sensory input to the representations of subject, predicate and object.\nOur model supports inductive inference in form of a probabilistic materialization. Certainly humans are capable of some form of logical inference, but this might be a faculty of working memory. The approximations that are performed in the tensor models, respectively in the the multiway neural networks, lead to a form of a probabilistic materialization, or unconscious inference: As an example, consider that we know that Max lives in Munich. The probabilistic materialization that happens in the factorization should already predict that Max also lives in Bavaria and in Germany. Thus both facts and inductively inferred facts about an entity are represented in its local environment. There is a certain danger in probabilistic materialization, since it might lead to overgeneralizations, reaching from national prejudice to false memories. In fact in many studies it has been shown that individuals produce false memories but are personally absolutely convinced of their truthfulness [117, 92].\nOur model assumes symmetrical connections between index neurons and representation neurons. The biological plausibility of symmetric weights has been discussed intensely in computational neuroscience and many biologically oriented models have that property [73, 69]. Reciprocal connectivity is abundant in the brain, but perfect symmetry is typically not observed."}, {"heading": "7.10 Online Learning and the Semantic-Attractor Learning Hypothesis", "text": "An interesting feature of the proposed model is that no learning or adaptation is necessary in operation, as long as sensory information can be described by the entities and predicates already known. The only structural adaptation that happens online is the forming of the index neuron et and its representation pattern aet .\n13One can also easily be only considering semantic memory of a certain time span by just inputting ones for the time index neurons of interest.\nIf decoding is not successful, e.g., if the decoded triples have low likelihood, one might consider a mechanism for introducing new index neurons with new latent representations for entities and predicates not yet stored in memory. Thus, only when the available resources (entities and predicates) are insufficient for explaining the sensory data, new index neurons for entities and predicates are introduced.\nAt a slower time scale it might be necessary to fine-tune all parameters in the system, possibly also the latent representations for entities and predicates. One might look at the model in Figure 10 as a complex neural network with inputs u:,:,t and targets ps, p, oq, possibly with some recurrence via the prediction module. Powerful learning algorithms are available to train such a system in a supervised way, and this might be the solution in a technical application. Of course for a biological system, the target information is unavailable.\nSo how can such a complex system be trained without clear target information? The future prediction model can be trained to lead to high quality predictions of future sensory inputs [70, 36, 116, 81, 83, 137, 64, 55, 53]. For the remaining parameters we suggest a form of bootstrap learning: the model parameters should be adapted such that they lead to stable semantic interpretation of sensory input. We call this the semantic-attractor learning hypothesis: In a sense the semantic descriptions form attractors for decoded sensory data and, conversely, the attractors are adapted based on sensory data. This can be related to the phenomenon of \u201cemergence\u201d which is a process whereby larger patterns and regularities arise through interactions among smaller or simpler entities that themselves do not exhibit such properties. Thus the emerging semantics hypothesis is that the semantic description is an emergent property of the sensory inputs!"}, {"heading": "7.11 Working Memory Exploits the Memory Representations for Tasks like Prediction and Decision Making", "text": "On the top right of Figure 10 we see a future-prediction model which estimates the next htimet \u201c aet based on its past values and based on the latent representation for the individual aes . Note that aes is not considered constant; for example, an individual might be diagnosed with a disease, which would be reflected in a change in aes . Large differences between predicted and sensory-decoded latent representations aet represent novelty and might be a component of an attention mechanism. As discussed before, novelty might be an important factor that determines which sensory information is stored in episodic memory, as speculated by other models and supported by cognitive studies [37, 75, 123, 53, 15].\nAn interesting aspect is that the predicted htimet can be semantically decoded for a cognitive analysis of predicted events (see Figure 10) and can lead to mental imagery, a sensory representation of predicted events. Mental imagery can be viewed as the conscious and explicit manipulation of simulations in working memory to predict future events [13]. The link between episodic memory and mental imagery has been studied in [122] and [65].\nIn Section 6 we discussed a predictive ARX model and an RNN model. In human cognition, both might be significant: The RNN would be part of the model dynamics, whereas the ARX model would purely serve as a predictive component.\nPrediction of events and actions on a semantic level is sometimes considered to be one of the important functions of a cognitive working memory [108]. Working memory is the limited-capacity store for retaining information over the short term and for performing mental operations on the contents of this store. As in our prediction model, the contents of working memory could either originate from sensory input, the episodic buffer, or from semantic memory [54]. Cognitive models of working memory are described in [12, 9, 11, 34, 47] and computational models are described in [100, 41, 50, 25, 76, 108].\nThe terms \u201cpredictive brain\u201d and \u201canticipating brain\u201d emphasize the importance of \u201clooking into the future\u201d, namely prediction, preparation, anticipation, prospection or expectations in various cognitive domains [29]. Prediction has been a central concept in recent trends in computational neuroscience, in particular in recent Bayesian approaches to brain modelling [70, 36, 116, 81, 83, 137, 64, 55, 53]. In some of these approaches, probabilistic generative models generate hypothesis about observations (top-down) assuming hidden causes, which are then aligned with actual observations (bottom-up).\nWorking memory is not the only brain structure involved in prediction. Predictive control is crucial for fast and ballistic movements where the cerebellum plays a crucial role in implicit tasks. The cerebellum is involved in trial-and-error learning based on predictive error signals [54]. Reward prediction is a task of the basal ganglia where dopamine neurons encode both present rewards and future rewards, as a basis for reinforcement learning [54, 57].\nWorking memory, assumed to be located in the frontal cortex, can use the representations in Figure 10 in many ways, not just for prediction. In general, working memory is closely tied to complex problem solving, planning, organizing, and decision support, and might assume an important role in consciousness. There is evidence that a strong working memory is associated with general intelligence [57].\nOne influential cognitive model of working memory is Baddeleys multicomponent model [12]. Cognitive control is executed by a central executive system. It is supported by two subsystems responsible for maintenance and rehearsal: the phonological loop, which maintains verbal information and the visuospatial sketchpad, which maintains visual and spatial information. More recently the episodic buffer has been added to the model. The episodic buffer integrates short-term and longterm memory, holding and manipulating a limited amount of information from multiple domains in time and spatially sequenced episodes (Figure 1). There is an emerging consensus that functions of working memory are located in the prefrontal cortex and that a number of other brain areas are recruited [109, 54]. More precisely, the central executive is attributed to the dorsolateral prefrontal cortex, the phonological loop with the left ventrolateral prefrontal cortex (the semantic information is anterior to the phonological information) and the visuospatial sketchpad in the right ventrolateral prefrontal cortex [57]. The function of the frontal lobe, in particular of the orbitofrontal cortex, includes the ability to project future consequences (predictions) resulting from current actions [57]."}, {"heading": "8 Conclusions and Discussion", "text": "We have discussed how a number of technical memory functions can be realized by representation learning and we have made the connection to human memory. A key assumption is that a knowledge graph does not need to be stored explicitly, but only latent representations of generalized entities need to be stored from which the knowledge graph can be reconstructed and inductive inference can be performed (tensor memory hypothesis). Thus, in contrast to the knowledge graph, where an entity is represented by a single node in a graph and its links, in embedding learning, an entity has a distributed representation in form of a latent vector, i.e., in form of multiple latent components. Unique representations lead to a global propagation of information across all memory functions during learning [104].\nWe proposed that the latent representation for a time t, which summarizes all sensory information present at time t, is the basis for episodic memory and that semantic memory depends on the latent representations of subject, predicate, and object. One theory we support is that semantic memory is a long-term aggregation of episodic memory. The full episodic experience depends on both semantic (\u201cwho\u201d and \u201cwhat\u201d) and context representations (\u201cwhere\u201d and \u201cwhen\u201d). On the other hand there is also a certain independence: the pure storage of episodic memory does not depend on semantic memory and semantic memory can be acquired even without a functioning episodic memory. The same relationships between semantic and episodic memories can be found in the human brain.\nThe latent representations of the semantic memory, episodic memory, and sensory memory can support working memory functions like prediction and decision support. In addition to the latent representations, the models contain parameters (e.g., neural network weights) in mapping functions, memory models and prediction models. One can make a link between those parameters and implicit skill memory [121]. Refining the mapping from sensory input to its latent representation corresponds to perceptual learning in cognition.\nWe showed how both a recall of previous memories and the mental imagery of future events and sensory impressions can be supported by the presented model.\nMore details on concrete technical solutions can be found in [48, 49] where we also present successful applications to clinical decision modeling, sensor network modeling and recommendation engines."}, {"heading": "9 Appendix", "text": ""}, {"heading": "9.1 Cost Functions", "text": "The cost function is the sum of several terms. The tilde notation X\u0303 indicates subsets which correspond to the facts known in training. If only positive facts with Value \u201c True are known, negative facts can be generated using, e.g., local closed world assumptions [106]. We use negative log-likelihood cost terms. For a Bernoulli likelihood, \u00b4 logP px|\u03b8q \u201c logr1 ` exptp1 \u00b4 2xq\u03b8us (cross-entropy) and for a Gaussian likelihood \u00b4 logP px|\u03b8q \u201c const` 12\u03c32 px\u00b4 \u03b8q 2."}, {"heading": "9.1.1 Semantic KG Model", "text": "The cost term for the semantic KG model is\ncostsemantic \u201c \u00b4 \u00ff\nxs,p,oPX\u0303\nlogP pxs,p,o|\u03b8semantics,p,o pA,W qq\nwhere A stands for the latent representations and W stands for the parameters in the functional mapping.\n9.1.2 Episodic Event Model\ncostepisodic \u201c \u00b4 \u00ff\nzs,p,o,tPZ\u0303\nlogP pzs,p,o,t|\u03b8episodics,p,o,t pA,W qq\n9.1.3 Sensory Buffer\ncostsensory \u201c \u00b4 \u00ff\nuq,\u03b3,tPU\u0303\nlogP puq,\u03b3,t|\u03b8sensoryq,\u03b3,t pA,W qq"}, {"heading": "9.1.4 Future-Prediction Model", "text": "The cost function for the ARX prediction model is\ncostpredict \u201c \u00b4 \u00ff\nt\nlogP paet |f predictpaet\u00b41 ,aet\u00b42 , . . . ,aet\u00b4W ,aeindiviual , A,W q"}, {"heading": "9.1.5 Regularizer", "text": "To regularize the solution we add \u03bbA}A}2F ` \u03bbW }W }2F\nwhere } \u00a8 }F is the Frobenious norm and where \u03bbA \u011b 0 and \u03bbW \u011b 0 are regularization parameters. If we use M -mappings, we regularize M instead of A and we include \u03bbM }M}2F ."}, {"heading": "9.2 Sampling using Function Approximators", "text": "Figure 11 shows how samples using function approximators (e.g., a NN) can be generated for the semantic KG and Figure 12 shows the semantic decoding.\nC: D:B:"}], "references": [{"title": "Data fusion in metabolomics using coupled matrix and tensor factorizations", "author": ["Evrim Acar", "Rasmus Bro", "Age K Smilde"], "venue": "Proceedings of the IEEE,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Mixed Membership Stochastic Blockmodels", "author": ["Edoardo M. Airoldi", "David M. Blei", "Stephen E. Fienberg", "Eric P. Xing"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Generalized singular value decomposition for comparative analysis of genome-scale expression data sets of two different organisms", "author": ["Orly Alter", "Patrick O Brown", "David Botstein"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "The architecture of cognition", "author": ["John R Anderson"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1983}, {"title": "ACT-R: A theory of higher level cognition and its relation to visual attention", "author": ["John R Anderson", "Michael Matessa", "Christian Lebiere"], "venue": "Human-Computer Interaction,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Human memory: A proposed system and its control processes", "author": ["Richard C Atkinson", "Richard M Shiffrin"], "venue": "The psychology of learning and motivation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1968}, {"title": "DBpedia: A Nucleus for a Web of Open Data. In The Semantic Web, Lecture Notes in Computer Science", "author": ["S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Cognitive psychology and human memory", "author": ["Alan Baddeley"], "venue": "Trends in neurosciences,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1988}, {"title": "The episodic buffer: a new component of working memory", "author": ["Alan Baddeley"], "venue": "Trends in cognitive sciences,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Working memory: theories, models, and controversies", "author": ["Alan Baddeley"], "venue": "Annual review of psychology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Simulation, situated conceptualization, and prediction", "author": ["Lawrence W Barsalou"], "venue": "Philosophical Transactions of the Royal Society of London B: Biological Sciences,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Remembering: A study in experimental and social psychology, volume 14", "author": ["Frederic C Bartlett"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1995}, {"title": "Novelty or surprise", "author": ["Andrew Barto", "Marco Mirolli", "Gianluca Baldassarre"], "venue": "Frontiers in psychology,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "A computational principle for hippocampal learning and neurogenesis. Hippocampus", "author": ["Suzanna Becker"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Deep learning of representations for unsupervised and transfer learning", "author": ["Yoshua Bengio"], "venue": "Unsupervised and Transfer Learning Challenges in Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pierre Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Antoine Bordes", "Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": "In AAAI\u201911,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Translating Embeddings for Modeling Multi-relational Data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Auto-association by multilayer perceptrons and singular value decomposition", "author": ["Herv\u00e9 Bourlard", "Yves Kamp"], "venue": "Biological cybernetics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1988}, {"title": "Computational models of working memory: putting longterm memory into context", "author": ["Neil Burgess", "Graham Hitch"], "venue": "Trends in cognitive sciences,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "The amygdala and emotional memory", "author": ["Larry Cahill", "Ralf Babinsky", "Hans J Markowitsch", "James L McGaugh"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1995}, {"title": "Toward an architecture for never-ending language", "author": ["Andrew Carlson", "Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R Hruschka Jr.", "Tom M Mitchell"], "venue": "learning. AAAI,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Neural network models for pattern recognition and associative memory", "author": ["Gail A Carpenter"], "venue": "Neural networks,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1989}, {"title": "Whatever next? predictive brains, situated agents, and the future of cognitive science", "author": ["Andy Clark"], "venue": "Behavioral and Brain Sciences,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "A spreading-activation theory of semantic processing", "author": ["Allan M Collins", "Elizabeth F Loftus"], "venue": "Psychological review,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1975}, {"title": "Iconic memory and visible persistence", "author": ["Max Coltheart"], "venue": "Perception & psychophysics,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1980}, {"title": "The construction of autobiographical memories in the self-memory system", "author": ["Martin A Conway", "Christopher W Pleydell-Pearce"], "venue": "Psychological review,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2000}, {"title": "Attention and memory", "author": ["Nelson Cowan"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1997}, {"title": "What are the differences between long-term, short-term, and working memory", "author": ["Nelson Cowan"], "venue": "Progress in brain research,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "The helmholtz machine", "author": ["Peter Dayan", "Geoffrey E Hinton", "Radford M Neal", "Richard S Zemel"], "venue": "Neural computation,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1995}, {"title": "Learning and selective attention", "author": ["Peter Dayan", "Sham Kakade", "P Read Montague"], "venue": "nature neuroscience,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2000}, {"title": "Indexing by latent semantic analysis", "author": ["Scott C. Deerwester", "Susan T. Dumais", "Thomas K. Landauer", "George W. Furnas", "Richard A. Harshman"], "venue": "JASIS, 41(6):391\u2013407,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1990}, {"title": "Imaging recollection and familiarity in the medial temporal lobe: a three-component model", "author": ["Rachel A Diana", "Andrew P Yonelinas", "Charan Ranganath"], "venue": "Trends in cognitive sciences,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2007}, {"title": "Knowledge Vault: A Web-scale Approach to Probabilistic Knowledge Fusion", "author": ["Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "Thomas Strohmann", "Shaohua Sun", "Wei Zhang"], "venue": "In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Neurocomputational models of working memory", "author": ["Daniel Durstewitz", "Jeremy K Seamans", "Terrence J Sejnowski"], "venue": "Nature neuroscience,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2000}, {"title": "Bringing the grandmother back into the picture: A memory-based view of object recognition", "author": ["Shimon Edelman", "Tomaso Poggio"], "venue": "International journal of pattern recognition and artificial intelligence,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1992}, {"title": "Time cells in the hippocampus: a new dimension for mapping memories", "author": ["Howard Eichenbaum"], "venue": "Nature Reviews Neuroscience,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "The medial temporal lobe and recognition memory", "author": ["Howard Eichenbaum", "AR Yonelinas", "Charan Ranganath"], "venue": "Annual review of neuroscience,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2007}, {"title": "Towards a functional organization of episodic memory in the medial temporal lobe", "author": ["Howard Eichenbaum", "Magdalena Sauvage", "Norbert Fortin", "Robert Komorowski", "Paul Lipton"], "venue": "Neuroscience & Biobehavioral Reviews,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2012}, {"title": "Long-term working memory", "author": ["K Anders Ericsson", "Walter Kintsch"], "venue": "Psychological review,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1995}, {"title": "Predicting sequences of clinical events by using a personalized temporal latent embedding model", "author": ["Crist\u00f3bal Esteban", "Danilo Schmidt", "Denis Krompa\u00df", "Volker Tresp"], "venue": "In Proceedings of the IEEE International Conference on Healthcare Informatics,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}, {"title": "Predicting the co-evolution of event and knowledge graphs", "author": ["Crist\u00f3bal Esteban", "Volker Tresp", "Yinchong Yang", "Stephan Baier", "Denis Krompa\u00df"], "venue": "arXiv preprint,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2015}, {"title": "Interactions between frontal cortex and basal ganglia in working memory: a computational model. Cognitive, Affective", "author": ["Michael J Frank", "Bryan Loughry", "Randall C OReilly"], "venue": "Behavioral Neuroscience,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2001}, {"title": "The organization of recent and remote memories", "author": ["Paul W Frankland", "Bruno Bontempi"], "venue": "Nature Reviews Neuroscience,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2005}, {"title": "\u03b1camkii-dependent plasticity in the cortex is required for permanent", "author": ["Paul W Frankland", "Cara O\u2019Brien", "Masuo Ohno", "Alfredo Kirkwood", "Alcino J Silva"], "venue": "memory. Nature,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2001}, {"title": "The free-energy principle: a unified brain theory", "author": ["Karl Friston"], "venue": "Nature Reviews Neuroscience,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2010}, {"title": "Cognitive Neuroscience: The biology of the mind", "author": ["Michael S Gazzaniga", "Richard B Ivry", "George Ronald Mangun"], "venue": "New York: WW Norton, fourth edition edition,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2013}, {"title": "Towards a mathematical theory of cortical micro-circuits", "author": ["Dileep George", "Jeff Hawkins"], "venue": "PLoS Comput Biol,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2009}, {"title": "Computational models of the hippocampal region: linking incremental learning and episodic memory", "author": ["Mark A Gluck", "Martijn Meeter", "Catherine E Myers"], "venue": "Trends in cognitive sciences,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2003}, {"title": "Learning and memory: From brain to behavior", "author": ["Mark A Gluck", "Eduardo Mercado", "Catherine E Myers"], "venue": "Palgrave Macmillan,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2013}, {"title": "Deep learning. Book in preparation for", "author": ["Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "venue": null, "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2015}, {"title": "Neurogenesis in the neocortex of adult", "author": ["Elizabeth Gould", "Alison J Reeves", "Michael SA Graziano", "Charles G Gross"], "venue": "primates. Science,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 1999}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2014}, {"title": "Interdependence of episodic and semantic memory: evidence from neuropsychology", "author": ["Daniel L Greenberg", "Mieke Verfaellie"], "venue": "Journal of the International Neuropsychological society,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2010}, {"title": "Google and the mind predicting fluency with pagerank", "author": ["Thomas L Griffiths", "Mark Steyvers", "Alana Firl"], "venue": "Psychological Science,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2007}, {"title": "Topics in semantic representation", "author": ["Thomas L Griffiths", "Mark Steyvers", "Joshua B Tenenbaum"], "venue": "Psychological review,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2007}, {"title": "Bayesian models of cognition. In The Cambridge Handbook of Computational Psychology", "author": ["Thomas L Griffiths", "Charles Kemp", "Joshua B Tenenbaum"], "venue": null, "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2008}, {"title": "Deconstructing episodic memory with construction", "author": ["Demis Hassabis", "Eleanor A Maguire"], "venue": "Trends in cognitive sciences,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2007}, {"title": "A practical guide to training restricted boltzmann machines. Momentum", "author": ["Geoffrey Hinton"], "venue": null, "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2010}, {"title": "Implementing semantic networks in parallel hardware. In Parallel models of associative memory, pages 161\u2013187", "author": ["Geoffrey E Hinton"], "venue": null, "citeRegEx": "67", "shortCiteRegEx": "67", "year": 1981}, {"title": "Parallel Models of Associative Memory: Updated Edition", "author": ["Geoffrey E Hinton", "James A Anderson"], "venue": null, "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2014}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2006}, {"title": "Autoencoders, minimum description length, and helmholtz free energy", "author": ["Geoffrey E Hinton", "Richard S Zemel"], "venue": "Advances in neural information processing systems,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 1994}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 1997}, {"title": "Probabilistic latent semantic indexing", "author": ["Thomas Hofmann"], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 1999}, {"title": "Neural networks and physical systems with emergent collective computational abilities", "author": ["John J Hopfield"], "venue": "Proceedings of the national academy of sciences,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 1982}, {"title": "Natural speech reveals the semantic maps that tile human cerebral cortex", "author": ["Alexander G. Huth", "Wendy A. de Heer", "Thomas L. Griffiths", "Fr\u00e9d\u00e9ric E. Theunissen", "Jack L. Gallant"], "venue": null, "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2016}, {"title": "Bayesian surprise attracts human attention", "author": ["Laurent Itti", "Pierre F Baldi"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2005}, {"title": "The mind and brain of short-term memory", "author": ["John Jonides", "Richard L Lewis", "Derek Evan Nee", "Cindy A Lustig", "Marc G Berman", "Katherine Sledge Moore"], "venue": "Annual review of psychology,", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2008}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2015}, {"title": "Learning systems of concepts with an infinite relational model", "author": ["Charles Kemp", "Joshua B. Tenenbaum", "Thomas L. Griffiths", "Takeshi Yamada", "Naonori Ueda"], "venue": "In Proceedings of the Twenty-First National Conference on Artificial Intelligence,", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 2006}, {"title": "Entorhinal\u2013 hippocampal neuronal circuits bridge temporally discontiguous events", "author": ["Takashi Kitamura", "Christopher J Macdonald", "Susumu Tonegawa"], "venue": "Learning & memory (Cold Spring Harbor, NY),", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2015}, {"title": "Entorhinal cortical ocean cells encode specific contexts and drive context-specific fear", "author": ["Takashi Kitamura", "Chen Sun", "Jared Martin", "Lacey J Kitch", "Mark J Schnitzer", "Susumu Tonegawa"], "venue": "memory. Neuron,", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2015}, {"title": "The bayesian brain: the role of uncertainty in neural coding and computation", "author": ["David C Knill", "Alexandre Pouget"], "venue": "Trends in Neurosciences,", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2004}, {"title": "Self-organization and associative memory, volume", "author": ["Teuvo Kohonen"], "venue": null, "citeRegEx": "82", "shortCiteRegEx": "82", "year": 2012}, {"title": "Bayesian integration in force estimation", "author": ["Konrad P K\u00f6rding", "Shih-pi Ku", "Daniel M Wolpert"], "venue": "Journal of Neurophysiology,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2004}, {"title": "Probabilistic Latent- Factor Database Models", "author": ["Denis Krompa\u00df", "Xueyan Jiang", "Maximilian Nickel", "Volker Tresp"], "venue": "In Proceedings of the 1st Workshop on Linked Data for Knowledge Discovery (ECML PKDD),", "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2014}, {"title": "Type-constrained representation learning in knowledge graphs. In The Semantic Web\u2013ISWC 2015, pages 640\u2013655", "author": ["Denis Krompa\u00df", "Stephan Baier", "Volker Tresp"], "venue": null, "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2015}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": "arXiv preprint arXiv:1506.07285,", "citeRegEx": "86", "shortCiteRegEx": "86", "year": 2015}, {"title": "A solution to plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Thomas K Landauer", "Susan T Dumais"], "venue": "Psychological review,", "citeRegEx": "87", "shortCiteRegEx": "87", "year": 1997}, {"title": "An introduction to latent semantic analysis", "author": ["Thomas K Landauer", "Peter W Foltz", "Darrell Laham"], "venue": "Discourse processes,", "citeRegEx": "88", "shortCiteRegEx": "88", "year": 1998}, {"title": "A specific role of the human hippocampus in recall of temporal sequences", "author": ["Hanne Lehn", "Hill-Aina Steffenach", "Niels M van Strien", "Dick J Veltman", "Menno P Witter", "Asta K H\u00e5berg"], "venue": "The Journal of Neuroscience,", "citeRegEx": "91", "shortCiteRegEx": "91", "year": 2009}, {"title": "The myth of repressed memory: False memories and allegations of sexual abuse", "author": ["Elizabeth Loftus", "Katherine Ketcham"], "venue": null, "citeRegEx": "92", "shortCiteRegEx": "92", "year": 1996}, {"title": "Semantic and associative priming in highdimensional semantic space", "author": ["Kevin Lund", "Curt Burgess", "Ruth Ann Atchley"], "venue": "Proceedings of the 17th annual conference of the Cognitive Science Society,", "citeRegEx": "93", "shortCiteRegEx": "93", "year": 1995}, {"title": "On the computational power of winner-take-all", "author": ["Wolfgang Maass"], "venue": "Neural computation,", "citeRegEx": "94", "shortCiteRegEx": "94", "year": 2000}, {"title": "Simple memory: A theory for archicortex", "author": ["D Marr"], "venue": "Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences,", "citeRegEx": "95", "shortCiteRegEx": "95", "year": 1971}, {"title": "Distributed memory and the representation of general and specific information", "author": ["James L McClelland", "David E Rumelhart"], "venue": "Journal of Experimental Psychology: General,", "citeRegEx": "96", "shortCiteRegEx": "96", "year": 1985}, {"title": "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory", "author": ["James L McClelland", "Bruce L McNaughton", "Randall C O\u2019Reilly"], "venue": "Psychological review,", "citeRegEx": "97", "shortCiteRegEx": "97", "year": 1995}, {"title": "Interactions between episodic and semantic memory", "author": ["Neal W Morton"], "venue": "Technical report, Vanderbilt Computational Memory Lab,", "citeRegEx": "98", "shortCiteRegEx": "98", "year": 2013}, {"title": "Place cells, grid cells, and memory", "author": ["May-Britt Moser", "David C Rowland", "Edvard I Moser"], "venue": "Cold Spring Harbor perspectives in biology,", "citeRegEx": "99", "shortCiteRegEx": "99", "year": 2015}, {"title": "Neural net architectures for temporal sequence processing", "author": ["Michael C Mozer"], "venue": "Santa Fe Institute Studies in the Sciences of Complexity,", "citeRegEx": "100", "shortCiteRegEx": "100", "year": 1993}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "101", "shortCiteRegEx": "101", "year": 2011}, {"title": "Tensor factorization for relational learning", "author": ["Maximilian Nickel"], "venue": "PhD thesis, Ludwig Maximilian University of Munich,", "citeRegEx": "102", "shortCiteRegEx": "102", "year": 2013}, {"title": "Learning Taxonomies from Multi-Relational Data via Hierarchical Link-Based Clustering", "author": ["Maximilian Nickel", "Volker Tresp"], "venue": "In Learning Semantics. Workshop at NIPS\u201911,", "citeRegEx": "103", "shortCiteRegEx": "103", "year": 2011}, {"title": "A Three-Way Model for Collective Learning on Multi-Relational Data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "104", "shortCiteRegEx": "104", "year": 2011}, {"title": "Factorizing YAGO: scalable machine learning for linked data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 21st International Conference on World Wide Web, WWW", "citeRegEx": "105", "shortCiteRegEx": "105", "year": 2012}, {"title": "A review of relational machine learning for knowledge graphs: From multi-relational link prediction to automated knowledge graph construction", "author": ["Maximilian Nickel", "Kevin Murphy", "Volker Tresp", "Evgeniy Gabrilovich"], "venue": "Proceedings of the IEEE,", "citeRegEx": "106", "shortCiteRegEx": "106", "year": 2015}, {"title": "Holographic embeddings of knowledge graphs", "author": ["Maximilian Nickel", "Lorenzo Rosasco", "Tomaso Poggio"], "venue": "arXiv preprint arXiv:1510.04935,", "citeRegEx": "107", "shortCiteRegEx": "107", "year": 2015}, {"title": "Making working memory work: a computational model of learning in the prefrontal cortex and basal ganglia", "author": ["Randall C O\u2019Reilly", "Michael J Frank"], "venue": "Neural computation,", "citeRegEx": "108", "shortCiteRegEx": "108", "year": 2006}, {"title": "11 a biologically based computational model of working memory. Models of working memory: Mechanisms of active maintenance and executive control, page", "author": ["Randall C O\u2019Reilly", "Todd S Braver", "Jonathan D Cohen"], "venue": null, "citeRegEx": "109", "shortCiteRegEx": "109", "year": 1999}, {"title": "Learning distributed representations of concepts using linear relational embedding", "author": ["Alberto Paccanaro", "Geoffrey E Hinton"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "110", "shortCiteRegEx": "110", "year": 2001}, {"title": "Sum-product networks: A new deep architecture", "author": ["Hoifung Poon", "Pedro Domingos"], "venue": "In Computer Vision Workshops (ICCV Workshops),", "citeRegEx": "111", "shortCiteRegEx": "111", "year": 2011}, {"title": "Invariant visual representation by single neurons in the human brain", "author": ["R Quian Quiroga", "Leila Reddy", "Gabriel Kreiman", "Christof Koch", "Itzhak Fried"], "venue": null, "citeRegEx": "112", "shortCiteRegEx": "112", "year": 2005}, {"title": "Concept cells: the building blocks of declarative memory functions", "author": ["Rodrigo Quian Quiroga"], "venue": "Nature Reviews Neuroscience,", "citeRegEx": "113", "shortCiteRegEx": "113", "year": 2012}, {"title": "SAM: A theory of probabilistic search of associative memory. The psychology of learning and motivation", "author": ["Jeroen GW Raaijmakers", "Richard M Shiffrin"], "venue": "Advances in research and theory,", "citeRegEx": "114", "shortCiteRegEx": "114", "year": 1981}, {"title": "Binding items and contexts the cognitive neuroscience of episodic memory", "author": ["Charan Ranganath"], "venue": "Current Directions in Psychological Science,", "citeRegEx": "115", "shortCiteRegEx": "115", "year": 2010}, {"title": "Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects", "author": ["Rajesh PN Rao", "Dana H Ballard"], "venue": "Nature neuroscience,", "citeRegEx": "116", "shortCiteRegEx": "116", "year": 1999}, {"title": "Creating false memories: Remembering words not presented in lists", "author": ["Henry L Roediger", "Kathleen B McDermott"], "venue": "Journal of experimental psychology: Learning, Memory, and Cognition,", "citeRegEx": "117", "shortCiteRegEx": "117", "year": 1995}, {"title": "A computational theory of episodic memory formation in the hippocampus", "author": ["Edmund T Rolls"], "venue": "Behavioural brain research,", "citeRegEx": "118", "shortCiteRegEx": "118", "year": 2010}, {"title": "The noisy brain. Stochastic dynamics as a principle of brain function.(Oxford", "author": ["ET Rolls", "G Deco"], "venue": null, "citeRegEx": "119", "shortCiteRegEx": "119", "year": 2010}, {"title": "Autoextend: Extending word embeddings to embeddings for synsets and lexemes", "author": ["Sascha Rothe", "Hinrich Sch\u00fctze"], "venue": "arXiv preprint arXiv:1507.01127,", "citeRegEx": "120", "shortCiteRegEx": "120", "year": 2015}, {"title": "Implicit memory: History and current status", "author": ["Daniel L Schacter"], "venue": "Journal of experimental psychology: learning, memory, and cognition,", "citeRegEx": "121", "shortCiteRegEx": "121", "year": 1987}, {"title": "The future of memory: remembering", "author": ["Daniel L Schacter", "Donna Rose Addis", "Demis Hassabis", "Victoria C Martin", "R Nathan Spreng", "Karl K Szpunar"], "venue": "imagining, and the brain. Neuron,", "citeRegEx": "122", "shortCiteRegEx": "122", "year": 2012}, {"title": "Driven by compression progress: A simple principle explains essential aspects of subjective beauty, novelty, surprise, interestingness, attention, curiosity, creativity, art, science, music, jokes", "author": ["J\u00fcrgen Schmidhuber"], "venue": "In Anticipatory Behavior in Adaptive Learning Systems,", "citeRegEx": "123", "shortCiteRegEx": "123", "year": 2009}, {"title": "Introducing the Knowledge Graph: things, not strings, May 2012.  URL http://googleblog.blogspot.com/2012/05/ introducing-knowledge-graph-things-not.html", "author": ["Amit Singhal"], "venue": null, "citeRegEx": "126", "shortCiteRegEx": "126", "year": 2012}, {"title": "Cognitive Psychology: Pearson New International Edition: Mind and Brain", "author": ["Edward E Smith", "Stephen M Kosslyn"], "venue": "Pearson Higher Ed,", "citeRegEx": "127", "shortCiteRegEx": "127", "year": 2013}, {"title": "Harmony theory: Problem solving, parallel cognitive models, and thermal physics", "author": ["Paul Smolensky", "Mary S Riley"], "venue": "Technical report, DTIC Document,", "citeRegEx": "128", "shortCiteRegEx": "128", "year": 1984}, {"title": "A bayesian analysis of dynamics in free recall", "author": ["Richard Socher", "Samuel Gershman", "Per Sederberg", "Kenneth Norman", "Adler J Perotte", "David M Blei"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "129", "shortCiteRegEx": "129", "year": 2009}, {"title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "130", "shortCiteRegEx": "130", "year": 2013}, {"title": "Memory and brain", "author": ["Larry R Squire"], "venue": null, "citeRegEx": "131", "shortCiteRegEx": "131", "year": 1987}, {"title": "Retrograde amnesia and memory consolidation: a neurobiological perspective", "author": ["Larry R Squire", "Pablo Alvarez"], "venue": "Current opinion in neurobiology,", "citeRegEx": "132", "shortCiteRegEx": "132", "year": 1995}, {"title": "Word association spaces for predicting semantic similarity effects in episodic memory. Experimental cognitive psychology and its applications: Festschrift in honor of Lyle", "author": ["Mark Steyvers", "Richard M Shiffrin", "Douglas L Nelson"], "venue": null, "citeRegEx": "133", "shortCiteRegEx": "133", "year": 2004}, {"title": "Yago: A Core of Semantic Knowledge", "author": ["Fabian M. Suchanek", "Gjergji Kasneci", "Gerhard Weikum"], "venue": "In Proceedings of the 16th International Conference on World Wide Web,", "citeRegEx": "134", "shortCiteRegEx": "134", "year": 2007}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "135", "shortCiteRegEx": "135", "year": 2014}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Yaniv Taigman", "Ming Yang", "Marc\u2019Aurelio Ranzato", "Lars Wolf"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "136", "shortCiteRegEx": "136", "year": 2014}, {"title": "Theory-based bayesian models of inductive learning and reasoning", "author": ["Joshua B Tenenbaum", "Thomas L Griffiths", "Charles Kemp"], "venue": "Trends in cognitive sciences,", "citeRegEx": "137", "shortCiteRegEx": "137", "year": 2006}, {"title": "Materializing and querying learned knowledge", "author": ["Volker Tresp", "Yi Huang", "Markus Bundschus", "Achim Rettinger"], "venue": "Proc. of IRMLeS,", "citeRegEx": "138", "shortCiteRegEx": "138", "year": 2009}, {"title": "Episodic and semantic memory 1", "author": ["Endel Tulving"], "venue": "Organization of Memory. London: Academic,", "citeRegEx": "139", "shortCiteRegEx": "139", "year": 1972}, {"title": "Elements of episodic memory", "author": ["Endel Tulving"], "venue": null, "citeRegEx": "140", "shortCiteRegEx": "140", "year": 1985}, {"title": "Episodic memory: from mind to brain", "author": ["Endel Tulving"], "venue": "Annual review of psychology,", "citeRegEx": "141", "shortCiteRegEx": "141", "year": 2002}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "142", "shortCiteRegEx": "142", "year": 2015}, {"title": "The unreasonable effectiveness of mathematics in the natural sciences. richard courant lecture in mathematical sciences delivered at new york university", "author": ["Eugene P Wigner"], "venue": "Communications on pure and applied mathematics,", "citeRegEx": "144", "shortCiteRegEx": "144", "year": 1959}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.03044,", "citeRegEx": "145", "shortCiteRegEx": "145", "year": 2015}, {"title": "Infinite Hidden Relational Models", "author": ["Zhao Xu", "Volker Tresp", "Kai Yu", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 22nd International Conference on Uncertainity in Artificial Intelligence,", "citeRegEx": "146", "shortCiteRegEx": "146", "year": 2006}, {"title": "Embedding mapping approaches for tensor factorization and knowledge graph modelling", "author": ["Yinchong Yang", "Crist\u00f3bal", "Volker Tresp"], "venue": "In ESWC,", "citeRegEx": "147", "shortCiteRegEx": "147", "year": 2016}, {"title": "The Cognitive Neuroscience of Semantic Memory", "author": ["Eiling Yee", "Evangelia G Chrysikou", "Sharon L Thompson-Schill"], "venue": "Oxford Handbook of Cognitive Neuroscience,", "citeRegEx": "148", "shortCiteRegEx": "148", "year": 2014}, {"title": "Discretized streams: an efficient and fault-tolerant model for stream processing on large clusters", "author": ["Matei Zaharia", "Tathagata Das", "Haoyuan Li", "Scott Shenker", "Ion Stoica"], "venue": "In Presented as part of the,", "citeRegEx": "149", "shortCiteRegEx": "149", "year": 2012}], "referenceMentions": [{"referenceID": 15, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 113, "endOffset": 138}, {"referenceID": 14, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 113, "endOffset": 138}, {"referenceID": 16, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 113, "endOffset": 138}, {"referenceID": 53, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 113, "endOffset": 138}, {"referenceID": 103, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 214, "endOffset": 256}, {"referenceID": 129, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 214, "endOffset": 256}, {"referenceID": 97, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 214, "endOffset": 256}, {"referenceID": 19, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 214, "endOffset": 256}, {"referenceID": 20, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 214, "endOffset": 256}, {"referenceID": 121, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 214, "endOffset": 256}, {"referenceID": 36, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 214, "endOffset": 256}, {"referenceID": 99, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 214, "endOffset": 256}, {"referenceID": 100, "context": "representation learning, is an essential ingredient of successful natural language models and deep architectures [125, 18, 17, 19, 90, 58] and has been the basis for modelling large-scale semantic knowledge graphs [110, 138, 104, 22, 23, 130, 40, 106, 107].", "startOffset": 214, "endOffset": 256}, {"referenceID": 43, "context": "In recent publications the embedding models were extended to also consider temporal evolutions, time patterns and subsymbolic representations [48, 49].", "startOffset": 142, "endOffset": 150}, {"referenceID": 44, "context": "In recent publications the embedding models were extended to also consider temporal evolutions, time patterns and subsymbolic representations [48, 49].", "startOffset": 142, "endOffset": 150}, {"referenceID": 113, "context": "These extended models were used successfully to predict clinical Some authors make a distinction between latent representations, which are application specific, and embeddings, which are identical across applications and might represent universal properties of entities [120, 124].", "startOffset": 270, "endOffset": 280}, {"referenceID": 49, "context": "Figure 1: Organization of human memory [54, 57].", "startOffset": 39, "endOffset": 47}, {"referenceID": 52, "context": "Figure 1: Organization of human memory [54, 57].", "startOffset": 39, "endOffset": 47}, {"referenceID": 83, "context": "Our approach follows the tradition of latent semantic analysis (LSA), which is a classical representation learning approach that on the one hand has found a number of technical applications and on the other hand could be related to cognitive semantic memories [88, 87, 38].", "startOffset": 260, "endOffset": 272}, {"referenceID": 82, "context": "Our approach follows the tradition of latent semantic analysis (LSA), which is a classical representation learning approach that on the one hand has found a number of technical applications and on the other hand could be related to cognitive semantic memories [88, 87, 38].", "startOffset": 260, "endOffset": 272}, {"referenceID": 34, "context": "Our approach follows the tradition of latent semantic analysis (LSA), which is a classical representation learning approach that on the one hand has found a number of technical applications and on the other hand could be related to cognitive semantic memories [88, 87, 38].", "startOffset": 260, "endOffset": 272}, {"referenceID": 5, "context": "Cognitive memory functions are typically classified as long-term, short-term, and sensory memory, where long-term memory has the subcategories declarative memory and non-declarative memory [42, 6, 131, 14, 35, 54, 57].", "startOffset": 189, "endOffset": 217}, {"referenceID": 122, "context": "Cognitive memory functions are typically classified as long-term, short-term, and sensory memory, where long-term memory has the subcategories declarative memory and non-declarative memory [42, 6, 131, 14, 35, 54, 57].", "startOffset": 189, "endOffset": 217}, {"referenceID": 11, "context": "Cognitive memory functions are typically classified as long-term, short-term, and sensory memory, where long-term memory has the subcategories declarative memory and non-declarative memory [42, 6, 131, 14, 35, 54, 57].", "startOffset": 189, "endOffset": 217}, {"referenceID": 31, "context": "Cognitive memory functions are typically classified as long-term, short-term, and sensory memory, where long-term memory has the subcategories declarative memory and non-declarative memory [42, 6, 131, 14, 35, 54, 57].", "startOffset": 189, "endOffset": 217}, {"referenceID": 49, "context": "Cognitive memory functions are typically classified as long-term, short-term, and sensory memory, where long-term memory has the subcategories declarative memory and non-declarative memory [42, 6, 131, 14, 35, 54, 57].", "startOffset": 189, "endOffset": 217}, {"referenceID": 52, "context": "Cognitive memory functions are typically classified as long-term, short-term, and sensory memory, where long-term memory has the subcategories declarative memory and non-declarative memory [42, 6, 131, 14, 35, 54, 57].", "startOffset": 189, "endOffset": 217}, {"referenceID": 49, "context": "There is evidence that these main cognitive categories are partially dissociated from one another in the brain, as expressed in their differential sensitivity to brain damage [54].", "startOffset": 175, "endOffset": 179}, {"referenceID": 71, "context": "However, there is also evidence indicating that the different memory functions are not mutually independent and support each other [76, 61].", "startOffset": 131, "endOffset": 139}, {"referenceID": 56, "context": "However, there is also evidence indicating that the different memory functions are not mutually independent and support each other [76, 61].", "startOffset": 131, "endOffset": 139}, {"referenceID": 130, "context": "Finally, episodic memory stores information of general and personal events [139, 140, 141, 54].", "startOffset": 75, "endOffset": 94}, {"referenceID": 131, "context": "Finally, episodic memory stores information of general and personal events [139, 140, 141, 54].", "startOffset": 75, "endOffset": 94}, {"referenceID": 132, "context": "Finally, episodic memory stores information of general and personal events [139, 140, 141, 54].", "startOffset": 75, "endOffset": 94}, {"referenceID": 49, "context": "Finally, episodic memory stores information of general and personal events [139, 140, 141, 54].", "startOffset": 75, "endOffset": 94}, {"referenceID": 52, "context": "Whereas semantic memory concerns information we \u201cknow\u201d, episodic memory concerns information we \u201cremember\u201d [57].", "startOffset": 107, "endOffset": 111}, {"referenceID": 49, "context": "It is the ability to retain impressions of sensory information after the original stimuli have ended [54].", "startOffset": 101, "endOffset": 105}, {"referenceID": 6, "context": "Popular large-scale KGs are DBpedia [7], YAGO [134], Freebase [21], NELL [27], and the Google Knowledge Graph [126].", "startOffset": 36, "endOffset": 39}, {"referenceID": 125, "context": "Popular large-scale KGs are DBpedia [7], YAGO [134], Freebase [21], NELL [27], and the Google Knowledge Graph [126].", "startOffset": 46, "endOffset": 51}, {"referenceID": 18, "context": "Popular large-scale KGs are DBpedia [7], YAGO [134], Freebase [21], NELL [27], and the Google Knowledge Graph [126].", "startOffset": 62, "endOffset": 66}, {"referenceID": 24, "context": "Popular large-scale KGs are DBpedia [7], YAGO [134], Freebase [21], NELL [27], and the Google Knowledge Graph [126].", "startOffset": 73, "endOffset": 77}, {"referenceID": 117, "context": "Popular large-scale KGs are DBpedia [7], YAGO [134], Freebase [21], NELL [27], and the Google Knowledge Graph [126].", "startOffset": 110, "endOffset": 115}, {"referenceID": 97, "context": "For example if a writer was born in Munich, the model can infer that the writer is also born in Germany and probably writes in the German language [104, 105].", "startOffset": 147, "endOffset": 157}, {"referenceID": 98, "context": "For example if a writer was born in Munich, the model can infer that the writer is also born in Germany and probably writes in the German language [104, 105].", "startOffset": 147, "endOffset": 157}, {"referenceID": 99, "context": "Stochastic gradient descent (SGD) is typically being used as an iterative approach for finding both optimal latent representations and optimal parameters in f semanticp \u0308q [106, 85].", "startOffset": 172, "endOffset": 181}, {"referenceID": 80, "context": "Stochastic gradient descent (SGD) is typically being used as an iterative approach for finding both optimal latent representations and optimal parameters in f semanticp \u0308q [106, 85].", "startOffset": 172, "endOffset": 181}, {"referenceID": 99, "context": "For a recent review, please consult [106].", "startOffset": 36, "endOffset": 41}, {"referenceID": 79, "context": "More complex queries on semantic models involving existential quantifier are discussed in [84].", "startOffset": 90, "endOffset": 94}, {"referenceID": 96, "context": "In [103, 102] such a structure was learned from the latent representations by hierarchical clustering.", "startOffset": 3, "endOffset": 13}, {"referenceID": 95, "context": "In [103, 102] such a structure was learned from the latent representations by hierarchical clustering.", "startOffset": 3, "endOffset": 13}, {"referenceID": 82, "context": ", in latent semantic analysis [87] which is restricted to attribute-based representations.", "startOffset": 30, "endOffset": 34}, {"referenceID": 67, "context": "Generalizations towards probabilistic models are probabilistic latent semantic indexing [72] and latent Dirichlet allocation [20].", "startOffset": 88, "endOffset": 92}, {"referenceID": 17, "context": "Generalizations towards probabilistic models are probabilistic latent semantic indexing [72] and latent Dirichlet allocation [20].", "startOffset": 125, "endOffset": 129}, {"referenceID": 73, "context": "Latent clustering and topic models [78, 146, 2] are extensions toward multi-relational domains and use discrete latent representations.", "startOffset": 35, "endOffset": 47}, {"referenceID": 136, "context": "Latent clustering and topic models [78, 146, 2] are extensions toward multi-relational domains and use discrete latent representations.", "startOffset": 35, "endOffset": 47}, {"referenceID": 1, "context": "Latent clustering and topic models [78, 146, 2] are extensions toward multi-relational domains and use discrete latent representations.", "startOffset": 35, "endOffset": 47}, {"referenceID": 86, "context": "See also [93, 62, 63].", "startOffset": 9, "endOffset": 21}, {"referenceID": 57, "context": "See also [93, 62, 63].", "startOffset": 9, "endOffset": 21}, {"referenceID": 58, "context": "See also [93, 62, 63].", "startOffset": 9, "endOffset": 21}, {"referenceID": 27, "context": "of semantic memory [30].", "startOffset": 19, "endOffset": 23}, {"referenceID": 3, "context": "Associate models are the symbolic ACT-R [4, 5] and SAM [114].", "startOffset": 40, "endOffset": 46}, {"referenceID": 4, "context": "Associate models are the symbolic ACT-R [4, 5] and SAM [114].", "startOffset": 40, "endOffset": 46}, {"referenceID": 107, "context": "Associate models are the symbolic ACT-R [4, 5] and SAM [114].", "startOffset": 55, "endOffset": 60}, {"referenceID": 100, "context": "[107] explores holographic embeddings with representation learning to model associative memories.", "startOffset": 0, "endOffset": 5}, {"referenceID": 68, "context": "Connectionists memory models are described in [73, 96, 28, 82, 67, 68].", "startOffset": 46, "endOffset": 70}, {"referenceID": 89, "context": "Connectionists memory models are described in [73, 96, 28, 82, 67, 68].", "startOffset": 46, "endOffset": 70}, {"referenceID": 25, "context": "Connectionists memory models are described in [73, 96, 28, 82, 67, 68].", "startOffset": 46, "endOffset": 70}, {"referenceID": 77, "context": "Connectionists memory models are described in [73, 96, 28, 82, 67, 68].", "startOffset": 46, "endOffset": 70}, {"referenceID": 62, "context": "Connectionists memory models are described in [73, 96, 28, 82, 67, 68].", "startOffset": 46, "endOffset": 70}, {"referenceID": 63, "context": "Connectionists memory models are described in [73, 96, 28, 82, 67, 68].", "startOffset": 46, "endOffset": 70}, {"referenceID": 118, "context": "Episodic memory represents our memory of experiences and specific events in time in a serial form (a \u201cmental time travel\u201d), from which we can reconstruct the actual events that took place at any given point in our lives [127]2.", "startOffset": 220, "endOffset": 225}, {"referenceID": 131, "context": "In contrast to semantic memory, it requires recollection of a prior experience [140].", "startOffset": 79, "endOffset": 84}, {"referenceID": 43, "context": "Some of the elements of this triple graph will affect changes in the KG [48, 49] (see also the discussion in Section 7).", "startOffset": 72, "endOffset": 80}, {"referenceID": 44, "context": "Some of the elements of this triple graph will affect changes in the KG [48, 49] (see also the discussion in Section 7).", "startOffset": 72, "endOffset": 80}, {"referenceID": 43, "context": "Whereas aet is a latent representation for all events for all patients at time t, aes\u201ci,t is a latent representation for all events for patients i at time t [48, 49].", "startOffset": 157, "endOffset": 165}, {"referenceID": 44, "context": "Whereas aet is a latent representation for all events for all patients at time t, aes\u201ci,t is a latent representation for all events for patients i at time t [48, 49].", "startOffset": 157, "endOffset": 165}, {"referenceID": 29, "context": "The autobiographical event tensor would correspond to the autobiographical memory, which stores autobiographical events of an individual on a semantic abstraction level [33, 54].", "startOffset": 169, "endOffset": 177}, {"referenceID": 49, "context": "The autobiographical event tensor would correspond to the autobiographical memory, which stores autobiographical events of an individual on a semantic abstraction level [33, 54].", "startOffset": 169, "endOffset": 177}, {"referenceID": 8, "context": "The autobiographical event tensor can be related to Baddeley\u2019s episodic buffer and, in contrast to Tulving\u2019s concept of episodic memory, is a temporary store and is considered to be a part of working memory [10, 76, 11].", "startOffset": 207, "endOffset": 219}, {"referenceID": 71, "context": "The autobiographical event tensor can be related to Baddeley\u2019s episodic buffer and, in contrast to Tulving\u2019s concept of episodic memory, is a temporary store and is considered to be a part of working memory [10, 76, 11].", "startOffset": 207, "endOffset": 219}, {"referenceID": 9, "context": "The autobiographical event tensor can be related to Baddeley\u2019s episodic buffer and, in contrast to Tulving\u2019s concept of episodic memory, is a temporary store and is considered to be a part of working memory [10, 76, 11].", "startOffset": 207, "endOffset": 219}, {"referenceID": 139, "context": "The sensory buffer might be related to the mini-batches in Spark Streaming where data is captured in buffers that hold seconds to minutes of the input streams [149].", "startOffset": 159, "endOffset": 164}, {"referenceID": 44, "context": "In a technical application [49], the sensors measure, e.", "startOffset": 27, "endOffset": 31}, {"referenceID": 130, "context": "In human cognition, sensory memory (milliseconds to a second) represents the ability to retain impressions of sensory information after the original stimuli have ended [139, 31, 54].", "startOffset": 168, "endOffset": 181}, {"referenceID": 28, "context": "In human cognition, sensory memory (milliseconds to a second) represents the ability to retain impressions of sensory information after the original stimuli have ended [139, 31, 54].", "startOffset": 168, "endOffset": 181}, {"referenceID": 49, "context": "In human cognition, sensory memory (milliseconds to a second) represents the ability to retain impressions of sensory information after the original stimuli have ended [139, 31, 54].", "startOffset": 168, "endOffset": 181}, {"referenceID": 5, "context": ", the autobiographical episodic buffer) is the first step in some memory models, in particular in the modal theory of Atkinson and Shiffrin [6].", "startOffset": 140, "endOffset": 143}, {"referenceID": 49, "context": "New evidence suggests that short-term memory is not the sole gateway to long-term memory [54].", "startOffset": 89, "endOffset": 93}, {"referenceID": 2, "context": "Some investigators propose that only some dimensions of the latent representations should be shared [3, 1].", "startOffset": 100, "endOffset": 106}, {"referenceID": 0, "context": "Some investigators propose that only some dimensions of the latent representations should be shared [3, 1].", "startOffset": 100, "endOffset": 106}, {"referenceID": 16, "context": "4 [89, 19, 17] contain extensive discussions on the transfer of latent representations.", "startOffset": 2, "endOffset": 14}, {"referenceID": 14, "context": "4 [89, 19, 17] contain extensive discussions on the transfer of latent representations.", "startOffset": 2, "endOffset": 14}, {"referenceID": 43, "context": "In the technical solutions [48, 49], we got best results by focussing on the cost function that corresponded to the problem to solve.", "startOffset": 27, "endOffset": 35}, {"referenceID": 44, "context": "In the technical solutions [48, 49], we got best results by focussing on the cost function that corresponded to the problem to solve.", "startOffset": 27, "endOffset": 35}, {"referenceID": 99, "context": "Tensor decompositions have also shown excellent performance in modelling KGs [106].", "startOffset": 77, "endOffset": 82}, {"referenceID": 97, "context": "Finally, the RESCAL model [104] is a Tucker2 model with", "startOffset": 26, "endOffset": 31}, {"referenceID": 104, "context": "By repeating this process we can obtain independent samples from P ps, p, oq! Note that there is a certain equivalence between tensor models and sum-product networks, where similar operations for marginals and conditionals can be defined [111].", "startOffset": 238, "endOffset": 243}, {"referenceID": 137, "context": "where f p \u0308q is a function to be learned [147] (see Figure 8) and where vecpu:,:,tq are vectorized representations from the portion of the sensory tensor associated with the individual at time t.", "startOffset": 41, "endOffset": 46}, {"referenceID": 127, "context": "Depending on the application, f p \u0308q can be a simple linear map, or it can be a second to last layer in a deep neural network as in the face recognition application DeepFace [136, 101].", "startOffset": 174, "endOffset": 184}, {"referenceID": 94, "context": "Depending on the application, f p \u0308q can be a simple linear map, or it can be a second to last layer in a deep neural network as in the face recognition application DeepFace [136, 101].", "startOffset": 174, "endOffset": 184}, {"referenceID": 66, "context": "Alternatively one might use networks with additional memory buffers and attention mechanisms [71, 143, 60, 86, 58].", "startOffset": 93, "endOffset": 114}, {"referenceID": 55, "context": "Alternatively one might use networks with additional memory buffers and attention mechanisms [71, 143, 60, 86, 58].", "startOffset": 93, "endOffset": 114}, {"referenceID": 81, "context": "Alternatively one might use networks with additional memory buffers and attention mechanisms [71, 143, 60, 86, 58].", "startOffset": 93, "endOffset": 114}, {"referenceID": 53, "context": "Alternatively one might use networks with additional memory buffers and attention mechanisms [71, 143, 60, 86, 58].", "startOffset": 93, "endOffset": 114}, {"referenceID": 106, "context": "Researchers have reported on a remarkable subset of medial temporal lobe (MTL) neurons that are selectively activated by strikingly different pictures of given individuals, landmarks or objects and in some cases even by letter strings with their names [113, 112].", "startOffset": 252, "endOffset": 262}, {"referenceID": 105, "context": "Researchers have reported on a remarkable subset of medial temporal lobe (MTL) neurons that are selectively activated by strikingly different pictures of given individuals, landmarks or objects and in some cases even by letter strings with their names [113, 112].", "startOffset": 252, "endOffset": 262}, {"referenceID": 89, "context": "Our hypothesis supports both locality and globality of encoding [96, 43], since index neurons are local representations of generalized entities, whereas the representation layers would be highdimensional and non-local.", "startOffset": 64, "endOffset": 72}, {"referenceID": 38, "context": "Our hypothesis supports both locality and globality of encoding [96, 43], since index neurons are local representations of generalized entities, whereas the representation layers would be highdimensional and non-local.", "startOffset": 64, "endOffset": 72}, {"referenceID": 52, "context": "Often neurons with similar receptive fields are clustered together in sensory cortices and form a topographic map [57].", "startOffset": 114, "endOffset": 118}, {"referenceID": 69, "context": "A detailed atlas of semantic categories has been established in extensive fMRI studies showing the involvement of the lateral temporal cortex (LTC), the ventral temporal cortex (VTC), the lateral parietal cortex (LPC), the medial parietal cortex (MPC), the medial prefrontal cortex, the superior prefrontal cortex (SPFC) and the inferior prefrontal cortex (IPFC) [74].", "startOffset": 363, "endOffset": 367}, {"referenceID": 98, "context": ", in [105].", "startOffset": 5, "endOffset": 10}, {"referenceID": 41, "context": "Evidence for time cells have recently been found [46, 44, 80, 79].", "startOffset": 49, "endOffset": 65}, {"referenceID": 39, "context": "Evidence for time cells have recently been found [46, 44, 80, 79].", "startOffset": 49, "endOffset": 65}, {"referenceID": 75, "context": "Evidence for time cells have recently been found [46, 44, 80, 79].", "startOffset": 49, "endOffset": 65}, {"referenceID": 74, "context": "Evidence for time cells have recently been found [46, 44, 80, 79].", "startOffset": 49, "endOffset": 65}, {"referenceID": 84, "context": "It has been observed that the hippocampus becomes activated when the temporal order of events is being processed [91, 119, 118].", "startOffset": 113, "endOffset": 127}, {"referenceID": 112, "context": "It has been observed that the hippocampus becomes activated when the temporal order of events is being processed [91, 119, 118].", "startOffset": 113, "endOffset": 127}, {"referenceID": 111, "context": "It has been observed that the hippocampus becomes activated when the temporal order of events is being processed [91, 119, 118].", "startOffset": 113, "endOffset": 127}, {"referenceID": 52, "context": "In fact, it has been observed that the adult macaque monkey forms a few thousand new neurons daily [57, 59], possibly to encode new information [16].", "startOffset": 99, "endOffset": 107}, {"referenceID": 54, "context": "In fact, it has been observed that the adult macaque monkey forms a few thousand new neurons daily [57, 59], possibly to encode new information [16].", "startOffset": 99, "endOffset": 107}, {"referenceID": 13, "context": "In fact, it has been observed that the adult macaque monkey forms a few thousand new neurons daily [57, 59], possibly to encode new information [16].", "startOffset": 144, "endOffset": 148}, {"referenceID": 92, "context": "There are multiple, functionally specialized, cell types of the hippocampalentorhinal circuit, such as place, grid, and border cells [99].", "startOffset": 133, "endOffset": 137}, {"referenceID": 90, "context": "Spatial memories, as other memories, are thought to be slowly induced in the neocortex by a gradual recruitment of neocortical memory circuits in long-term storage of hippocampal memories [97, 132, 52, 99].", "startOffset": 188, "endOffset": 205}, {"referenceID": 123, "context": "Spatial memories, as other memories, are thought to be slowly induced in the neocortex by a gradual recruitment of neocortical memory circuits in long-term storage of hippocampal memories [97, 132, 52, 99].", "startOffset": 188, "endOffset": 205}, {"referenceID": 47, "context": "Spatial memories, as other memories, are thought to be slowly induced in the neocortex by a gradual recruitment of neocortical memory circuits in long-term storage of hippocampal memories [97, 132, 52, 99].", "startOffset": 188, "endOffset": 205}, {"referenceID": 92, "context": "Spatial memories, as other memories, are thought to be slowly induced in the neocortex by a gradual recruitment of neocortical memory circuits in long-term storage of hippocampal memories [97, 132, 52, 99].", "startOffset": 188, "endOffset": 205}, {"referenceID": 49, "context": "The perirhinal and parahippocampal cortices also project back to the association areas of the cortex [54].", "startOffset": 101, "endOffset": 105}, {"referenceID": 44, "context": "This is the case in the medical application described in [49, 48] where u:,:,t describes procedures and diagnosis and one can think of f p \u0308q as being an encoder system and f p \u0308q as being a decoder and the complex as being an autoencoder [24, 70].", "startOffset": 57, "endOffset": 65}, {"referenceID": 43, "context": "This is the case in the medical application described in [49, 48] where u:,:,t describes procedures and diagnosis and one can think of f p \u0308q as being an encoder system and f p \u0308q as being a decoder and the complex as being an autoencoder [24, 70].", "startOffset": 57, "endOffset": 65}, {"referenceID": 21, "context": "This is the case in the medical application described in [49, 48] where u:,:,t describes procedures and diagnosis and one can think of f p \u0308q as being an encoder system and f p \u0308q as being a decoder and the complex as being an autoencoder [24, 70].", "startOffset": 239, "endOffset": 247}, {"referenceID": 65, "context": "This is the case in the medical application described in [49, 48] where u:,:,t describes procedures and diagnosis and one can think of f p \u0308q as being an encoder system and f p \u0308q as being a decoder and the complex as being an autoencoder [24, 70].", "startOffset": 239, "endOffset": 247}, {"referenceID": 123, "context": "According to the standard model of memory consolidation [132, 51] memory is retained in the hippocampus for up to one week after initial learning, representing the hippocampus-dependent stage.", "startOffset": 56, "endOffset": 65}, {"referenceID": 46, "context": "According to the standard model of memory consolidation [132, 51] memory is retained in the hippocampus for up to one week after initial learning, representing the hippocampus-dependent stage.", "startOffset": 56, "endOffset": 65}, {"referenceID": 52, "context": "The frontal cortex, associated with higher functionalities, plays a role in which new information gets encoded as episodic and semantic memory and what gets forgotten [57].", "startOffset": 167, "endOffset": 171}, {"referenceID": 23, "context": "The amygdala belongs to the MTL and consists of several nuclei but is not considered to be a part of memory itself [26].", "startOffset": 115, "endOffset": 119}, {"referenceID": 111, "context": "The amygdala and the orbitofrontal cortex might also provide reward-related information to the hippocampus [118].", "startOffset": 107, "endOffset": 112}, {"referenceID": 135, "context": ", in the decoding of complex scenes [145, 142, 77].", "startOffset": 36, "endOffset": 50}, {"referenceID": 133, "context": ", in the decoding of complex scenes [145, 142, 77].", "startOffset": 36, "endOffset": 50}, {"referenceID": 72, "context": ", in the decoding of complex scenes [145, 142, 77].", "startOffset": 36, "endOffset": 50}, {"referenceID": 126, "context": "The proposed model can be related to encoder-decoder networks [135] which produce text sequences, whereas we produce a set of likely triples.", "startOffset": 62, "endOffset": 67}, {"referenceID": 87, "context": "In the past, a number of neural winner-takes-all networks have been proposed where the neuron with the largest activation wins over all other neurons, which are driven to inactivity [94, 69].", "startOffset": 182, "endOffset": 190}, {"referenceID": 64, "context": "In the past, a number of neural winner-takes-all networks have been proposed where the neuron with the largest activation wins over all other neurons, which are driven to inactivity [94, 69].", "startOffset": 182, "endOffset": 190}, {"referenceID": 88, "context": "It is known that CA3 contains many feedback connections, essential for winner-takes-all computations [95, 56, 118].", "startOffset": 101, "endOffset": 114}, {"referenceID": 51, "context": "It is known that CA3 contains many feedback connections, essential for winner-takes-all computations [95, 56, 118].", "startOffset": 101, "endOffset": 114}, {"referenceID": 111, "context": "It is known that CA3 contains many feedback connections, essential for winner-takes-all computations [95, 56, 118].", "startOffset": 101, "endOffset": 114}, {"referenceID": 111, "context": "CA3 is sometimes modelled as a continuous attractor neural network (CANN) with excitatory recurrent colateral connections and global inhibition [118].", "startOffset": 144, "endOffset": 149}, {"referenceID": 119, "context": "The restricted Boltzmann machine (RBM) might be an interesting option for supporting the decoding process [128, 66].", "startOffset": 106, "endOffset": 115}, {"referenceID": 61, "context": "The restricted Boltzmann machine (RBM) might be an interesting option for supporting the decoding process [128, 66].", "startOffset": 106, "endOffset": 115}, {"referenceID": 49, "context": "There is growing evidence that the hippocampus plays an important role not just in encoding but also in decoding of memory and is involved in the retrieval of information from long-term memory [54].", "startOffset": 193, "endOffset": 197}, {"referenceID": 40, "context": "Both types of information then pass through the entorhinal cortex but only converge within the hippocampus where it enables a full recognition of an episodic event [45, 39, 115, 54].", "startOffset": 164, "endOffset": 181}, {"referenceID": 35, "context": "Both types of information then pass through the entorhinal cortex but only converge within the hippocampus where it enables a full recognition of an episodic event [45, 39, 115, 54].", "startOffset": 164, "endOffset": 181}, {"referenceID": 108, "context": "Both types of information then pass through the entorhinal cortex but only converge within the hippocampus where it enables a full recognition of an episodic event [45, 39, 115, 54].", "startOffset": 164, "endOffset": 181}, {"referenceID": 49, "context": "Both types of information then pass through the entorhinal cortex but only converge within the hippocampus where it enables a full recognition of an episodic event [45, 39, 115, 54].", "startOffset": 164, "endOffset": 181}, {"referenceID": 134, "context": "The physicist Eugene Wigner has speculated on the \u201cThe Unreasonable Effectiveness of Mathematics in the Natural Sciences\u201d [144]; in other words mathematics is the right code for the natural sciences.", "startOffset": 122, "endOffset": 127}, {"referenceID": 52, "context": "13 This form of a semantic memory is very attractive since it requires no additional modelling effort and can use the same structures that are needed for episodic memory! It has been argued that semantic memory is information we have encountered repeatedly, so often that the actual learning episodes are blurred [32, 57].", "startOffset": 313, "endOffset": 321}, {"referenceID": 56, "context": "Without doubt, semantic and episodic memories support one another [61].", "startOffset": 66, "endOffset": 70}, {"referenceID": 122, "context": "Thus some theories speculate that episodic memory may be the \u201cgateway\u201d to semantic memory [12, 131, 8, 133, 129, 97, 148, 86].", "startOffset": 90, "endOffset": 125}, {"referenceID": 7, "context": "Thus some theories speculate that episodic memory may be the \u201cgateway\u201d to semantic memory [12, 131, 8, 133, 129, 97, 148, 86].", "startOffset": 90, "endOffset": 125}, {"referenceID": 124, "context": "Thus some theories speculate that episodic memory may be the \u201cgateway\u201d to semantic memory [12, 131, 8, 133, 129, 97, 148, 86].", "startOffset": 90, "endOffset": 125}, {"referenceID": 120, "context": "Thus some theories speculate that episodic memory may be the \u201cgateway\u201d to semantic memory [12, 131, 8, 133, 129, 97, 148, 86].", "startOffset": 90, "endOffset": 125}, {"referenceID": 90, "context": "Thus some theories speculate that episodic memory may be the \u201cgateway\u201d to semantic memory [12, 131, 8, 133, 129, 97, 148, 86].", "startOffset": 90, "endOffset": 125}, {"referenceID": 138, "context": "Thus some theories speculate that episodic memory may be the \u201cgateway\u201d to semantic memory [12, 131, 8, 133, 129, 97, 148, 86].", "startOffset": 90, "endOffset": 125}, {"referenceID": 81, "context": "Thus some theories speculate that episodic memory may be the \u201cgateway\u201d to semantic memory [12, 131, 8, 133, 129, 97, 148, 86].", "startOffset": 90, "endOffset": 125}, {"referenceID": 91, "context": "[98] is a recent overview on the topic.", "startOffset": 0, "endOffset": 4}, {"referenceID": 132, "context": ", the representations of entities and predicates [141, 57].", "startOffset": 49, "endOffset": 58}, {"referenceID": 52, "context": ", the representations of entities and predicates [141, 57].", "startOffset": 49, "endOffset": 58}, {"referenceID": 49, "context": "But note that studies have also found an independent formation of semantic memories, in case that the episodic memory is dysfunctional, as in certain amnesic patients: Amnesic patients might learn new facts without remembering the episodes during which they have learned the information [54].", "startOffset": 287, "endOffset": 291}, {"referenceID": 110, "context": "In fact in many studies it has been shown that individuals produce false memories but are personally absolutely convinced of their truthfulness [117, 92].", "startOffset": 144, "endOffset": 153}, {"referenceID": 85, "context": "In fact in many studies it has been shown that individuals produce false memories but are personally absolutely convinced of their truthfulness [117, 92].", "startOffset": 144, "endOffset": 153}, {"referenceID": 68, "context": "The biological plausibility of symmetric weights has been discussed intensely in computational neuroscience and many biologically oriented models have that property [73, 69].", "startOffset": 165, "endOffset": 173}, {"referenceID": 64, "context": "The biological plausibility of symmetric weights has been discussed intensely in computational neuroscience and many biologically oriented models have that property [73, 69].", "startOffset": 165, "endOffset": 173}, {"referenceID": 65, "context": "So how can such a complex system be trained without clear target information? The future prediction model can be trained to lead to high quality predictions of future sensory inputs [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 182, "endOffset": 220}, {"referenceID": 32, "context": "So how can such a complex system be trained without clear target information? The future prediction model can be trained to lead to high quality predictions of future sensory inputs [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 182, "endOffset": 220}, {"referenceID": 109, "context": "So how can such a complex system be trained without clear target information? The future prediction model can be trained to lead to high quality predictions of future sensory inputs [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 182, "endOffset": 220}, {"referenceID": 76, "context": "So how can such a complex system be trained without clear target information? The future prediction model can be trained to lead to high quality predictions of future sensory inputs [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 182, "endOffset": 220}, {"referenceID": 78, "context": "So how can such a complex system be trained without clear target information? The future prediction model can be trained to lead to high quality predictions of future sensory inputs [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 182, "endOffset": 220}, {"referenceID": 128, "context": "So how can such a complex system be trained without clear target information? The future prediction model can be trained to lead to high quality predictions of future sensory inputs [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 182, "endOffset": 220}, {"referenceID": 59, "context": "So how can such a complex system be trained without clear target information? The future prediction model can be trained to lead to high quality predictions of future sensory inputs [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 182, "endOffset": 220}, {"referenceID": 50, "context": "So how can such a complex system be trained without clear target information? The future prediction model can be trained to lead to high quality predictions of future sensory inputs [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 182, "endOffset": 220}, {"referenceID": 48, "context": "So how can such a complex system be trained without clear target information? The future prediction model can be trained to lead to high quality predictions of future sensory inputs [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 182, "endOffset": 220}, {"referenceID": 33, "context": "As discussed before, novelty might be an important factor that determines which sensory information is stored in episodic memory, as speculated by other models and supported by cognitive studies [37, 75, 123, 53, 15].", "startOffset": 195, "endOffset": 216}, {"referenceID": 70, "context": "As discussed before, novelty might be an important factor that determines which sensory information is stored in episodic memory, as speculated by other models and supported by cognitive studies [37, 75, 123, 53, 15].", "startOffset": 195, "endOffset": 216}, {"referenceID": 116, "context": "As discussed before, novelty might be an important factor that determines which sensory information is stored in episodic memory, as speculated by other models and supported by cognitive studies [37, 75, 123, 53, 15].", "startOffset": 195, "endOffset": 216}, {"referenceID": 48, "context": "As discussed before, novelty might be an important factor that determines which sensory information is stored in episodic memory, as speculated by other models and supported by cognitive studies [37, 75, 123, 53, 15].", "startOffset": 195, "endOffset": 216}, {"referenceID": 12, "context": "As discussed before, novelty might be an important factor that determines which sensory information is stored in episodic memory, as speculated by other models and supported by cognitive studies [37, 75, 123, 53, 15].", "startOffset": 195, "endOffset": 216}, {"referenceID": 10, "context": "Mental imagery can be viewed as the conscious and explicit manipulation of simulations in working memory to predict future events [13].", "startOffset": 130, "endOffset": 134}, {"referenceID": 115, "context": "The link between episodic memory and mental imagery has been studied in [122] and [65].", "startOffset": 72, "endOffset": 77}, {"referenceID": 60, "context": "The link between episodic memory and mental imagery has been studied in [122] and [65].", "startOffset": 82, "endOffset": 86}, {"referenceID": 101, "context": "Prediction of events and actions on a semantic level is sometimes considered to be one of the important functions of a cognitive working memory [108].", "startOffset": 144, "endOffset": 149}, {"referenceID": 49, "context": "As in our prediction model, the contents of working memory could either originate from sensory input, the episodic buffer, or from semantic memory [54].", "startOffset": 147, "endOffset": 151}, {"referenceID": 9, "context": "Cognitive models of working memory are described in [12, 9, 11, 34, 47] and computational models are described in [100, 41, 50, 25, 76, 108].", "startOffset": 52, "endOffset": 71}, {"referenceID": 30, "context": "Cognitive models of working memory are described in [12, 9, 11, 34, 47] and computational models are described in [100, 41, 50, 25, 76, 108].", "startOffset": 52, "endOffset": 71}, {"referenceID": 42, "context": "Cognitive models of working memory are described in [12, 9, 11, 34, 47] and computational models are described in [100, 41, 50, 25, 76, 108].", "startOffset": 52, "endOffset": 71}, {"referenceID": 93, "context": "Cognitive models of working memory are described in [12, 9, 11, 34, 47] and computational models are described in [100, 41, 50, 25, 76, 108].", "startOffset": 114, "endOffset": 140}, {"referenceID": 37, "context": "Cognitive models of working memory are described in [12, 9, 11, 34, 47] and computational models are described in [100, 41, 50, 25, 76, 108].", "startOffset": 114, "endOffset": 140}, {"referenceID": 45, "context": "Cognitive models of working memory are described in [12, 9, 11, 34, 47] and computational models are described in [100, 41, 50, 25, 76, 108].", "startOffset": 114, "endOffset": 140}, {"referenceID": 22, "context": "Cognitive models of working memory are described in [12, 9, 11, 34, 47] and computational models are described in [100, 41, 50, 25, 76, 108].", "startOffset": 114, "endOffset": 140}, {"referenceID": 71, "context": "Cognitive models of working memory are described in [12, 9, 11, 34, 47] and computational models are described in [100, 41, 50, 25, 76, 108].", "startOffset": 114, "endOffset": 140}, {"referenceID": 101, "context": "Cognitive models of working memory are described in [12, 9, 11, 34, 47] and computational models are described in [100, 41, 50, 25, 76, 108].", "startOffset": 114, "endOffset": 140}, {"referenceID": 26, "context": "The terms \u201cpredictive brain\u201d and \u201canticipating brain\u201d emphasize the importance of \u201clooking into the future\u201d, namely prediction, preparation, anticipation, prospection or expectations in various cognitive domains [29].", "startOffset": 212, "endOffset": 216}, {"referenceID": 65, "context": "Prediction has been a central concept in recent trends in computational neuroscience, in particular in recent Bayesian approaches to brain modelling [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 149, "endOffset": 187}, {"referenceID": 32, "context": "Prediction has been a central concept in recent trends in computational neuroscience, in particular in recent Bayesian approaches to brain modelling [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 149, "endOffset": 187}, {"referenceID": 109, "context": "Prediction has been a central concept in recent trends in computational neuroscience, in particular in recent Bayesian approaches to brain modelling [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 149, "endOffset": 187}, {"referenceID": 76, "context": "Prediction has been a central concept in recent trends in computational neuroscience, in particular in recent Bayesian approaches to brain modelling [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 149, "endOffset": 187}, {"referenceID": 78, "context": "Prediction has been a central concept in recent trends in computational neuroscience, in particular in recent Bayesian approaches to brain modelling [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 149, "endOffset": 187}, {"referenceID": 128, "context": "Prediction has been a central concept in recent trends in computational neuroscience, in particular in recent Bayesian approaches to brain modelling [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 149, "endOffset": 187}, {"referenceID": 59, "context": "Prediction has been a central concept in recent trends in computational neuroscience, in particular in recent Bayesian approaches to brain modelling [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 149, "endOffset": 187}, {"referenceID": 50, "context": "Prediction has been a central concept in recent trends in computational neuroscience, in particular in recent Bayesian approaches to brain modelling [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 149, "endOffset": 187}, {"referenceID": 48, "context": "Prediction has been a central concept in recent trends in computational neuroscience, in particular in recent Bayesian approaches to brain modelling [70, 36, 116, 81, 83, 137, 64, 55, 53].", "startOffset": 149, "endOffset": 187}, {"referenceID": 49, "context": "The cerebellum is involved in trial-and-error learning based on predictive error signals [54].", "startOffset": 89, "endOffset": 93}, {"referenceID": 49, "context": "Reward prediction is a task of the basal ganglia where dopamine neurons encode both present rewards and future rewards, as a basis for reinforcement learning [54, 57].", "startOffset": 158, "endOffset": 166}, {"referenceID": 52, "context": "Reward prediction is a task of the basal ganglia where dopamine neurons encode both present rewards and future rewards, as a basis for reinforcement learning [54, 57].", "startOffset": 158, "endOffset": 166}, {"referenceID": 52, "context": "There is evidence that a strong working memory is associated with general intelligence [57].", "startOffset": 87, "endOffset": 91}, {"referenceID": 102, "context": "There is an emerging consensus that functions of working memory are located in the prefrontal cortex and that a number of other brain areas are recruited [109, 54].", "startOffset": 154, "endOffset": 163}, {"referenceID": 49, "context": "There is an emerging consensus that functions of working memory are located in the prefrontal cortex and that a number of other brain areas are recruited [109, 54].", "startOffset": 154, "endOffset": 163}, {"referenceID": 52, "context": "More precisely, the central executive is attributed to the dorsolateral prefrontal cortex, the phonological loop with the left ventrolateral prefrontal cortex (the semantic information is anterior to the phonological information) and the visuospatial sketchpad in the right ventrolateral prefrontal cortex [57].", "startOffset": 306, "endOffset": 310}, {"referenceID": 52, "context": "The function of the frontal lobe, in particular of the orbitofrontal cortex, includes the ability to project future consequences (predictions) resulting from current actions [57].", "startOffset": 174, "endOffset": 178}, {"referenceID": 97, "context": "Unique representations lead to a global propagation of information across all memory functions during learning [104].", "startOffset": 111, "endOffset": 116}, {"referenceID": 114, "context": "One can make a link between those parameters and implicit skill memory [121].", "startOffset": 71, "endOffset": 76}, {"referenceID": 43, "context": "More details on concrete technical solutions can be found in [48, 49] where we also present successful applications to clinical decision modeling, sensor network modeling and recommendation engines.", "startOffset": 61, "endOffset": 69}, {"referenceID": 44, "context": "More details on concrete technical solutions can be found in [48, 49] where we also present successful applications to clinical decision modeling, sensor network modeling and recommendation engines.", "startOffset": 61, "endOffset": 69}], "year": 2016, "abstractText": "Embedding learning, a.k.a. representation learning, has been shown to be able to model large-scale semantic knowledge graphs. A key concept is a mapping of the knowledge graph to a tensor representation whose entries are predicted by models using latent representations of generalized entities. Latent variable models are well suited to deal with the high dimensionality and sparsity of typical knowledge graphs. In recent publications the embedding models were extended to also consider time evolutions, time patterns and subsymbolic representations. In this paper we map embedding models, which were developed purely as solutions to technical problems for modelling temporal knowledge graphs, to various cognitive memory functions, in particular to semantic and concept memory, episodic memory, sensory memory, short-term memory, and working memory. We discuss learning, query answering, the path from sensory input to semantic decoding, and the relationship between episodic memory and semantic memory. We introduce a number of hypotheses on human memory that can be derived from the developed mathematical models. There are four main hypotheses. The first one is that semantic memory is described as triples and that episodic memory is described as triples in time. A second main hypothesis is that generalized entities have unique latent representations which are shared across memory functions and that are the basis for prediction, decision support and other functionalities executed by working memory (tensor memory hypothesis). A third main hypothesis is that the latent representation for a time t, which summarizes all sensory information available at time t, is the basis for episodic memory. Finally, our proposed model suggests that semantic memory and episodic memory depend on each other: Episodic decoding depends on semantic memory and semantic memory is developed as a long term store of episodic memory. On the other hand there is also a certain independence: the pure storage of episodic memory does not depend on semantic memory and semantic memory can be acquired even without a functioning episodic memory. The same relationships between semantic and episodic memories can be found in the human brain.", "creator": "LaTeX with hyperref package"}}}