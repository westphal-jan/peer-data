{"id": "1511.08228", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2015", "title": "Neural GPUs Learn Algorithms", "abstract": "Learning an algorithm from examples is a fundamental problem that has been widely studied. Recently it has been addressed using neural networks, in particular by Neural Turing Machines (NTMs). These are fully differentiable computers that use backpropagation to learn their own programming concepts, using learning learning techniques that are similar to those of a neural network. Furthermore, when training neural network models, learning methods are learned from memory, learning methods are learned from memory. Hence the problem is that neural networks can learn from memory more rapidly. In this context, a learning model is needed to train neural network models. To train neural network models, learning methods need to be used to train neural network models.\n\n\n\nFor each neuron, learning can be trained via neural networks. Neural networks have many features which may be needed for learning. For example, they provide a way to train neural network models. Neural networks can be trained from memory. Neural networks can have many features which may be needed for learning. For example, they provide a way to train neural network models. Neural networks can have many features which may be needed for learning. For example, they provide a way to train neural network models. Neural networks can have many features which may be needed for learning. For example, they provide a way to train neural network models. Neural networks can have many features which may be needed for learning. Neural networks can have many features which may be needed for learning. For example, they provide a way to train neural network models. Neural networks can have many features which may be needed for learning. Neural networks can have many features which may be needed for learning. Neural networks can have many features which may be needed for learning. Neural networks can have many features which may be needed for learning.", "histories": [["v1", "Wed, 25 Nov 2015 21:17:43 GMT  (18kb)", "http://arxiv.org/abs/1511.08228v1", null], ["v2", "Wed, 6 Jan 2016 14:44:27 GMT  (24kb)", "http://arxiv.org/abs/1511.08228v2", null], ["v3", "Tue, 15 Mar 2016 00:20:54 GMT  (19kb)", "http://arxiv.org/abs/1511.08228v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["{\\l}ukasz kaiser", "ilya sutskever"], "accepted": true, "id": "1511.08228"}, "pdf": {"name": "1511.08228.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["lukaszkaiser@google.com", "ilyasu@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n08 22\n8v 1\n[ cs\n.L G\n] 2\n5 N\nov 2\n01 5"}, {"heading": "1 INTRODUCTION", "text": "Deep neural networks have recently proven successful at various tasks, such as computer vision (Krizhevsky et al., 2012), speech recognition (Dahl et al., 2012), and in other domains. Recurrent neural networks based on long short-term memory (LSTM) cells (Hochreiter & Schmidhuber, 1997) have been successfully applied to a number of natural language processing tasks. Sequence-tosequence recurrent neural networks with such cells can learn very complex tasks in an end-to-end manner, such as translation (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014), parsing (Vinyals & Kaiser et al., 2015), speech recognition (Graves & Jaitly, 2014) or image caption generation (Vinyals et al., 2014). Since so many tasks can be solved with essentially one model, a natural question arises: is this model the best we can hope for in supervised learning?\nIt turns out that despite its recent success the sequence-to-sequence model has limitations. In its basic architecture, the entire input is encoded into a single fixed-size vector, so the model cannot generalize to inputs much longer than this fixed capacity. One way to resolve this problem is by using an attention mechanism (Bahdanau et al., 2014). This allows the network to inspect arbitrary parts of the input in every decoding step, so the basic limitation is removed. But other problems remain, and Joulin & Mikolov (2015) show a number of basic algorithmic tasks on which sequenceto-sequence LSTM networks fail to generalize. They propose a stack-augmented recurrent network, and it works on some problems, but is limited in other ways.\nIn the best case, one would desire a neural network model able to learn arbitrarily complex algorithms given enough resources. Neural Turing Machines (Graves et al., 2014) have this theoretical property. However, they are not computationally efficient because they use soft attention and because they tend to be of considerable depth. Their depth makes the training objective difficult to optimize and impossible to parallelize. Their use of soft attention requires accessing the entire memory in order to simulate 1 step of computation, which introduces substantial overhead. These\ntwo factors make learning complex algorithms using Neural Turing Machines difficult. These issues are not limited to Neural Turing Machines, they apply to other architectures too, such as stack-RNNs (Joulin & Mikolov, 2015) or (De)Queue-RNNs (Graves et al., 2014). One can try to partially alleivate these problems using hard attention and reinforcement learning, but such non-differentiable models do not learn well at present (Zaremba & Sutskever, 2015b).\nIn this work we present a neural network model, the Neural GPU, that addresses the above issues. It is a Turing-complete model capable of learning arbitrary algorithms in principle, similarly to a Neural Turing Machine. But, in contrast to Neural Turing Machines, it is designed to be as parallel and as shallow as possible. It is closer to a GPU than to a Turing machine since it uses a smaller number of parallel computational steps. We show that the Neural GPU works in multiple experiments:\n\u2022 A neural GPU can learn long binary multiplication from examples. It is the first neural network able to learn an algorithm whose runitme is superlinear in the size of its input. Trained on upto 20-bit numbers, we see no single error on any inputs we tested, and we tested on numbers upto 2000 bits long.\n\u2022 The same architecture can also learn long binary addition and a number of other algorithmic tasks, such as counting, copying sequences, reversing them, or duplicating them."}, {"heading": "1.1 RELATED WORK", "text": "The learning of algorithms with neural networks has seen a lot of interest after the success of sequence-to-sequence neural networks on language processing tasks (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014). An attempt was even made to learn to evaluate simple python programs with a pure sequence-to-sequence model (Zaremba & Sutskever, 2015a), but more success was seen with more complex models. Neural Turing Machines (Graves et al., 2014) were shown to learn a number of basic sequence transformations and memory access patters, and their reinforcement learning variant (Zaremba & Sutskever, 2015b) has reasonable perfomance on a number of tasks as well. Stack, Queue and DeQueue networks (Grefenstette et al., 2015) were also shown to learn basic sequence transformations such as bigram flipping or sequence reversal.\nThe Grid LSTM (Kalchbrenner et al., 2015) is another powerful architecture that can learn to multiply 15-digit decimal numbers. As we will see in the next section, the Grid-LSTM is quite similar to the Neural GPU \u2013 the main difference is that the Neural GPU is less recurrent and is explicitly constructed from the highly parallel convolution operator.\nMost comparable to this work are the prior experiments with the stack-augmented RNNs (Joulin & Mikolov, 2015). These networks manage to learn and generalize to unseen lengths on a number of algorithmic tasks. But, as we show in Section 3.1, stack-augmented RNNs trained to add numbers upto 20-bit long generalize only to \u223c100-bit numbers, never to 200-bit ones, and never without error. Still, their generalization is the best we were able to obtain without using the Neural GPU and far surpasses a baseline LSTM sequence-to-sequence model with attention.\nThe quest for learning algorithms has been pursued much more widely with tools other than neural networks. It is known under names such as program synthesis, program induction, automatic programming, or inductive synthesis, and has a long history with many works that we do not cover here; see, e.g., Gulwani (2010) and Kitzelmann (2010) for a more general perspective.\nSince one of our results is the synthesis of an algorithm for long binary addition, let us recall how this problem has been addressed without neural networks. Importantly, there are two cases of this problem with different complexity. The easier case is when the two numbers that are to be added are aligned at input, i.e., if the first (lower-endian) bit of the first number is presented at the same time as the first bit of the second number, then come the second bits, and so on, as depicted below for x = 9 = 8 + 1 and y = 5 = 4 + 1 written in binary with least-significant bit left.\nInput 1 0 0 1 (x and y aligned) 1 0 1 0 Desired Ouput (x+ y) 0 1 1 1\nIn this representation the triples of bits from (x, y, x + y), e.g., (1, 1, 0) (0, 0, 1) (0, 1, 1) (1, 0, 1) as in the figure above, form a regular language. To learn binary addition in this representation it\ntherefore suffices to find a regular expression or an automaton that accepts this language, which can be done with a variant of Anguin\u2019s algorithm (Angluin, 1987). But only few interesting functions have regular representations, as for example long multiplication does not (Blumensath & Gra\u0308del, 2000). It is therefore desireable to learn long binary addition without alignment, for example when x and y are provided one after another. This is the representation we use in the present paper.\nInput (x, y) 1 0 0 1 + 1 0 1 0 Desired Ouput (x+ y) 0 1 1 1"}, {"heading": "2 THE NEURAL GPU", "text": "Before we introduce the Neural GPU, let us recall the architecture of a Gated Recurrent Unit (GRU) (Cho et al., 2014). A GRU is similar to an LSTM, but its input and state are the same size, which makes it easier for us to generalize it later; a highway network could have also been used (Srivastava et al., 2015), but it lacks the reset gate. GRUs have shown performance similar to LSTMs on a number of tasks (Chung et al., 2014; Greff et al., 2015). A GRU takes an input vector x and a current state vector s, and outputs:\nGRU(x, s) = u\u2299 s+ (1\u2212 u)\u2299 tanh(Wx+ U(r \u2299 s) +B), where\nu = \u03c3(W \u2032x+ U \u2032s+B\u2032) and r = \u03c3(W \u2032\u2032x+ U \u2032\u2032s+B\u2032\u2032).\nIn the equations above, W,W \u2032,W \u2032\u2032, U, U \u2032, U \u2032\u2032 are matrices and B,B\u2032, B\u2032\u2032 are bias vectors; these are the parameters that will be learned. We write Wx for a matrix-vector multiplication and r \u2299 s for elementwise vector multiplication. The vectors u and r are called gates since their elements are in [0, 1] \u2014 u is the update gate and r is the reset gate.\nIn recurrent neural networks a unit like GRU is applied at every step and the result is both passed as new state and used to compute the output. In a Neural GPU we do not process a new input in every step. Instead, all inputs are written into the starting state s0. This state has 2-dimensional strucutre: it consists of w \u00d7 h vectors of m numbers, i.e., it is a 3-dimensional tensor of shape [w, h,m]. This mental image evolves in time in a way defined by a convolutional gated recurrent unit:\nCGRU(s) = u\u2299 s+ (1\u2212 u)\u2299 tanh(U \u2217 (r \u2299 s) +B), where\nu = \u03c3(U \u2032 \u2217 s+B\u2032) and r = \u03c3(U \u2032\u2032 \u2217 s+B\u2032\u2032).\nU \u2217 s above denotes the convolution of a kernel bank U with the mental image s. A kernel bank is a 4-dimensional tensor of shape [kw, kh,m,m], i.e., it contains kw \u00b7kh \u00b7m2 parameters, where kw and kh are kernel width and height. It is applied to a mental image s of shape [w, h,m] which results in another mental image U \u2217 s of the same shape defined by:\nU \u2217 s[x, y, i] =\nkw/2\u2211\nu=\u2212kw/2\nkh/2\u2211\nv=\u2212kh/2\nm\u2211\nc=1\ns[x+ u, y + v, c] \u00b7 U [u, v, c, i].\nIn the equation above the index x+ u might sometimes be negative or larger than the size of s, and in such cases we assume the value is 0. This corresponds to the standard convolution operator used in convolutional neural networks with zero padding on both sides and stride 1. Using the standard operator has the advantage that it is heavily optimized (see Section 4 for Neural GPU performance). New work on faster convolutions, e.g., Lavin & Gray (2015), can be directly used in a Neural GPU.\nKnowing how a CGRU gate works, the definition of a l-layer Neural GPU is simple, as depicted in Figure 1. The given sequence i = (i1, . . . , in) of n discrete symbols from {0, . . . , I} is first embedded into the mental image s0 by concatenating the vectors obtained from an embedding lookup of the input symbols into its first column. More precisely, we create the starting mental image s0 of shape [w, n,m] by using an embedding matrix E of shape [I,m] and setting s0[0, k, :] = E[ik] for all k = 1 . . . n (here i1, . . . , in is the input). All other elements of s0 are set to 0. Then, we apply l different CGRU gates in turn for n steps to produce the final mental image sfin:\nst+1 = CGRUl(CGRUl\u22121 . . .CGRU1(st) . . .) and sfin = sn.\nThe result of a Neural GPU is produced by multiplying each item in the first column of sfin by an output matrix O to obtain the logits lk = Osfin[0, k, :] and then selecting the maximal one: ok = argmax(lk). During training we use the standard loss function, i.e., we compute a softmax over the logits lk and use the negative log probability of the target as the loss.\nSince all components of a Neural GPU are clearly differentiable, we can train using any stochastic gradient descent optimizer. For the results presented in this paper we used the Adam optimizer (Kingma & Ba, 2014) with \u03b5 = 10\u22125. The number of layers was set to l = 2, the width of mental images was constant at w = 4, the number of maps in each mental image point was set to m = 24, and the convolution kernels width and height was always kw = kh = 3.\nComputational power of Neural GPUs. While the above definition is simple, it might not be immediately obvious what kind of functions a Neural GPU can compute. Why can we expect it to be able to perform long multiplication? To answer such questions it is useful to draw an analogy between a Neural GPU and a discrete 2-dimensional cellular automaton. Except for being discrete and the lack of a gating mechanism, such automata are quite similar to Neural GPUs. Of course, these are large exceptions. Dense representations have often more capacity than purely discrete states and the gating mechanism is crucial to avoid vanishing gradients during training. But the computational power of cellular automata is much better understood. In particular, it is well known that a cellular automaton can exploit its parallelism to multiply two n-bit numbers in O(n) steps using Atrubin\u2019s algorithm. We recommend the online book (Vivien) to get an understanding of this algorithm and the computational power of cellular automata."}, {"heading": "3 EXPERIMENTS", "text": "In this section, we present experiments showing that a Neural GPU can successfully learn a number of algorithmic tasks and generalize well beyond the lengths that it was trained on. This section is organized as follows. We start with the two tasks we focused on, long binary addition and long binary multiplication. We present results for those tasks and compare with other models. Then, to demonstrate the generality of the model, we show that Neural GPUs perform well on a few other tasks as well. Finally we discuss the training procedure and highlight important techniques."}, {"heading": "3.1 ADDITION AND MULTIPLICATION", "text": "The two core tasks on which we study the performance of Neural GPUs are long binary addition and long binary multiplication. We chose them because they are fundamental tasks and because there is no known linear-time algorithm for long multiplication. As described in Section 2, we input a sequence of discrete symbols into the network and we read out a sequence of symbols again. For binary addition, we use a set of 4 symbols: {0, 1,+,PAD} and for multiplication we use {0, 1, \u00b7,PAD}. The PAD symbol is only used for padding so we depict it as empty space below.\nLong binary addition (badd) is the task of adding two numbers represented lower-endian in binary notation. We always add numbers of the same length, but we allow them to have 0s at start, so numbers of differing lengths can be padded to equal size. Given two d-bit numbers the full\nsequence length is n = 2d+ 1, as seen in the example below, representing (1 + 4) + (2 + 4+ 8) = 5 + 14 = 19 = (16 + 2 + 1).\nInput 1 0 1 0 + 0 1 1 1 Ouput 1 1 0 0 1\nLong binary multiplication (bmul) is the task of multiplying two binary numbers, represented lower-endian. Again, we always multiply numbers of the same length, but we allow them to have 0s at start, so numbers of differing lengths can be padded to equal size. Given two d-bit numbers, the full sequence length is again n = 2d+1, as seen in the example below, representing (2+4)\u00b7(2+8) = 6 \u00b7 10 = 60 = 32 + 16 + 8 + 4.\nInput 0 1 1 0 \u00b7 0 1 0 1 Ouput 0 0 1 1 1\nModels. We compare three different models on the above tasks. In addition to the Neural GPU described above, we include a baseline LSTM recurrent neural network with an attention mechanism. We call this model LSTM+A as it is exactly the same as described in (Vinyals & Kaiser et al., 2015). It is a 3-layer model with 64 units in each LSTM cell in each layer, which results in about 200K parameters (the Neural GPU uses m = 24 and has about 30K paramters). Both the Neural GPU and the LSTM+A baseline were trained using all the techniques described below, including curriculum training and gradient noise. Finally, on binary addition, we also include the stack-RNN model from (Joulin & Mikolov, 2015). This model was not trained using our training regime, but in exactly the way as provided in its source code, only with nmax = 41. To match our training procedure, we ran it 729 times with different random seeds and we report the best obtained result.\nResults. We measure also the ratio of fully correct output sequences and report the results in Table 1. For both tasks, we show first the error at the maximum length seen during training, i.e., for 20-bit numbers. Note that LSTM+A is not able to learn long binary multiplication at this length, it does not even fit the training data. Then we report numbers for sizes not seen during training.\nAs you can see, a Neural GPU can learn a multiplication algorithm that generalizes perfectly, at least as far as we were able to test (technical limits of our implementation prevented us from testing much above 2000 bits). Even for the simpler task of binary addition, stack-RNNs work only upto length 100. This is still much better than the LSTM+A baseline which only generalizes to length 25."}, {"heading": "3.2 OTHER ALGORITHMIC TASKS", "text": "In addition to the two main tasks above, we tested Neural GPUs on the following simpler algorithmic tasks. The same architecture as used above was able to solve all of the tasks described below, i.e., after being trained on sequences of length upto 41 we were not able to find any error on sequences on any length we tested (upto 4001).\nCopying sequences is the simple task of producing on output the same sequence as on input. It is very easy for a Neural GPU, in fact all models converge quickly and generalize perfectly.\nReversing sequences is the task of reversing a sequence of bits. The length n here is exactly the length of the sequence, as in the example below.\nInput 0 0 1 0 1 1 1 0 1 Ouput 1 0 1 1 1 0 1 0 0\nDuplicating sequences is the task of duplicating the input bit sequence on output twice, as in the example below. We use the padding symbol on input to make it match the output length. We trained on sequences of inputs upto 20 bits, so outputs were upto 40-bits long, and tested on inputs upto 2000 bits long.\nInput 0 0 1 1 Ouput 0 0 1 1 0 0 1 1\nCounting by sorting bits is the task of sorting the input bit sequence on output. Since there are only 2 symbols to sort, this is a counting tasks \u2013 the network must count how many 0s are in the input and produce the output accordingly, as in the example below.\nInput 1 0 1 1 0 0 1 0 Ouput 0 0 0 0 1 1 1 1"}, {"heading": "3.3 TRAINING TECHNIQUES", "text": "Here we describe the training methods that we used to improve our results. Note that we applied these methods to the LSTM+A baseline as well, to keep the above comparison fair. We focus on the most important elements of our training regime, all less relevant details can be found in the code which will be released as open-source.1\nGrid search. Each result we report is obtained by running a grid search over 36 = 729 instances. We consider 3 settings of the learning rate, initial parameters scale, and 4 other hyperparameters discussed below: the relaxation pull factor, curriculum progress threshold, gradient noise scale, and dropout. An important effect of running this grid search is also that we train 729 models with different random seeds every time. Usually only a few of these models generalize to 2000-bit numbers, but a significant fraction works well on 200-bit numbers, as discussed below.\nCurriculum learning. We use a curriculum learning approach inspired by Zaremba & Sutskever (2015a). This means that we train on 7-digit numbers only after crossing a curriculum progress threshold (e.g., over 90% fully correct outputs) on 6-digit numbers. Except that with 20% probability we pick a minibatch of d-digit numbers with d chosen uniformly at random between 1 and 20.\nGradients noise. To improve training speed and stability we add noise to gradients in each training step. Inspired by the schedule from Welling & Teh (2011), we add to gradients a noise drawn from the normal distribution with mean 0 and variance inversely proportional to the square root of stepnumber (i.e., with standard deviation proportional to the 4-th root of step-number). We mutiply this noise by the gradient noise scale and, to avoid noise in converged models, we also multiply it by the fraction of non-fully-correct outputs (which is 0 for a perfect model).\nGate cutoff. In Section 2 we defined the gates in a CGRU using the sigmoid function, e.g., we wrote u = \u03c3(U \u2032 \u2217 s + B\u2032). Usually the standard sigmoid function is used, \u03c3(x) = 1\n1+e\u2212x . We found that adding a hard threshold on the top and bottom helps slightly in our setting, so we use 1.1\u03c3(x)\u2212 0.1 cut to the interval [0, 1], i.e., \u03c3\u2032(x) = max(0,min(1, 1.1\u03c3(x)\u2212 0.1))."}, {"heading": "3.3.1 DROPOUT ON RECURRENT CONNECTIONS", "text": "Dropout is a widely applied technique for regularizing neural networks. But when applying it to recurrent networks, it has been counter-productive to apply it on recurrent connections \u2013 it only worked when applied to the non-recurrent ones, as reported by Pham et al. (2014).\n1The code will be available at https://github.com/lukaszkaiser/NeuralGPU/.\nSince a Neural GPU does not have non-recurrent connections it might seem that dropout will not be useful for this architecture. Surprisingly, we found the contrary \u2013 it is useful and improves generalization. The key to using dropout effectively in this setting is to set a small dropout rate.\nWhen we run a grid search for dropout rates we vary them between 6%, 9%, and 13.5%, meaning that over 85% of the values are always preserved. It turns out that even this small dropout has large effect since we apply it to the whole mental image si in each step i. Presumably the network now learns to include some redundancy in its internal representation and generalization benefits from it.\nWithout dropout we usually see only a few models from a 729 grid search generalize reasonably, while with dropout it is a much larger fraction and they generalize to higher lengths. In particular, dropout was necessary to train models for multiplication that generalize to 2000 bits."}, {"heading": "3.3.2 PARAMETER SHARING RELAXATION.", "text": "To improve optimization of our deep network we use a relaxation technique for shared parameters which works as follows. Instead of training with parameters shared across time-steps we use r identical sets of non-shared parameters (we often use r = 6, larger numbers work better but use more memory). At time-step t of the Neural GPU we use the i-th set if t mod r = i.\nThe procedure described above relaxes the network, as it can now perform different operations in different time-steps. Training becomes easier, but we now have r parameters instead of the single shared set we want. To unify them we add a term to the cost function representing the distance of each parameter from the average of this parameter in all the r sets. This term in the final cost function is multiplied by a scalar which we call the relaxation pull. If the relaxation pull is 0, the network behaves as if the r parameter sets were separate, but when it is large, the cost forces the network to unify the parameters across different set.\nDuring training, we gradually increase the relaxation pull. We start with a small value and every time the curriculum makes progress, e.g., when the model learns to perform well on 6-digit numbers, we multiply the relaxation pull by a relaxation pull factor (e.g., by 1.2). When the curriculum reaches the maximal length we average the parameters from all sets and continue to train with a single shared parameter set.\nThis method is crucial for learning multiplication. Without it, a Neural GPU with m = 24 has trouble to even fit the training set, and the few models that manage to do it do not generalize well. With relaxation almost all models in our 729 runs manage to fit the training data."}, {"heading": "4 DISCUSSION", "text": "What does not work? Although the results presented above are encouraging, it is natural to ask: what are the cases when Neural GPU training does not work well?\nFor one, using decimal inputs degrades performance. All tasks above can easily be formulated with decimal inputs instead of binary ones. One could hope that a Neural GPU will work well in this case too, maybe with a larger m. We experimented with this formulation and our results were far worse than when the representation was binary: decimal addition did not generalize to 2000 digits (it did work for 200), and decimal multiplication had problems even fitting the training data, despite parameter sharing relaxation. The fact that it is much better for a Neural GPU to use binary inputs stands in contrast to standard RNNs which perform well with shorter inputs and more symbols.\nAnother problem is that often only a few models in a 729 grid search generalize to very long unseen instances. Among those 729models, there usually are many models that generalize to 40 or even 200 bits, but only a few working without error for 2000-bit numbers. Using dropout and gradient noise improves the reliability of training and generalization, but maybe another technique could help even more. How could we make more models achieve good generalization? One idea that looks natural is to try to reduce the number of parameters by decreasing m. Surprisingly, this does not seem to have as much influence. In addition to the m = 24 results presented above we ran experiments with m = 32 and m = 64 and the results were similar.\nWhy use width? The Neural GPU is defined using two-dimensional convolutions and in our experiments one of the dimensions is always set to 4. Doing so is not necessary since a onedimensional Neural GPU that uses four times larger m can represent every function representable by the original one. In fact we trained a model for long binary multiplication that generalized to 2000-bit numbers using a Neural GPU with width 1 and m = 64. However, the width of the Neural GPU increases the amount of information carried in its hidden state without increasing the number of its parameters. Thus it can be thought of as a factorization and might be useful for other tasks.\nSpeed and data efficiency. Neural GPUs use the standard, heavily optimized convolution operation and are fast. We experimented with a 2-layer Neural GPU for n = 64 and m = 24. After unfolding in time it has 128 layers of CGRUs, each operating on a 4\u00d7 64\u00d7 24 mental image. The joint forward-backward step time for this network was about 0.6s on an NVIDIA Tesla K40 GPU.\nWe were also surprised by how data-efficient a Neural GPU can be. The experiments presented above were all performed using 10K random training data examples for each training length. Since we train on upto 20-bit numbers this adds to about 200K training examples. We tried to train using only 100 examples per length, so about 2000 total trainig instances. We were surprised to see that it actually worked well for binary addition: there were models that generalized well to 200-bit numbers and to all lengths below despite such small training set. But we never managed to train a good model for binary multiplication with that little training data."}, {"heading": "5 CONCLUSIONS AND FUTURE WORK", "text": "The results presented in Table 1 show clearly that there is a qualitative difference between what can be achieved with a Neural GPU and what was possible with previous archietctures. In particular, for the first time, we show a neural network that learns a non-trivial superlinear-time algorithm in a way that generalized to much higher lengths without errors.\nThis opens the way to use neural networks in domains that were previously only addressed by discrete methods, such as program synthesis. With the surprising data efficiency of Neural GPUs it could even be possible to replicate previous program synthesis results, e.g., Kaiser (2012), but in a more scalable way. It is also interesting that a Neural GPU can learn symbolic algorithms without using any discrete state at all, and adding dropout and noise only improves its performance.\nAnother promising future work is to apply Neural GPUs to language processing tasks. Good results have already been obtained on translation with a convolutional architecture over words (Kalchbrenner & Blunsom, 2013) and adding gating and recursion, like in a Neural GPU, should allow to train much deeper models without overfitting. Finally, the parameter sharing relaxation technique we introduced can be applied to any deep recurrent network and has the potential to improve RNN training in general."}], "references": [{"title": "Learning regaular sets from queries and counterexamples", "author": ["Angluin", "Dana"], "venue": "Information and Computation,", "citeRegEx": "Angluin and Dana.,? \\Q1987\\E", "shortCiteRegEx": "Angluin and Dana.", "year": 1987}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Automatic Structures", "author": ["Blumensath", "Achim", "Gr\u00e4del", "Erich"], "venue": "In Proceedings of LICS", "citeRegEx": "Blumensath et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Blumensath et al\\.", "year": 2000}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "van Merrienboer", "Bart", "Gulcehre", "Caglar", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "CoRR, abs/1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung", "Junyoung", "G\u00fcl\u00e7ehre", "\u00c7aglar", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "CoRR, abs/1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["Dahl", "George E", "Yu", "Dong", "Deng", "Li", "Acero", "Alex"], "venue": "IEEE Transactions on Audio, Speech & Language Processing,", "citeRegEx": "Dahl et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Graves", "Alex", "Jaitly", "Navdeep"], "venue": "In Proceedings ICML", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Learning to transduce with unbounded memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "CoRR, abs/1506.02516,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "LSTM: A search space odyssey", "author": ["Greff", "Klaus", "Srivastava", "Rupesh Kumar", "Koutn\u0131\u0301k", "Jan", "Steunebrink", "Bas R", "Schmidhuber", "J\u00fcrgen"], "venue": "CoRR, abs/1503.04069,", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Dimensions in program synthesis", "author": ["Gulwani", "Sumit"], "venue": "In Proceedings of PPDP 2010,", "citeRegEx": "Gulwani and Sumit.,? \\Q2010\\E", "shortCiteRegEx": "Gulwani and Sumit.", "year": 2010}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "CoRR, abs/1503.01007,", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "Learning games from videos guided by descriptive complexity", "author": ["Kaiser", "\u0141ukasz"], "venue": "In Proceedings of the AAAI-12,", "citeRegEx": "Kaiser and \u0141ukasz.,? \\Q2012\\E", "shortCiteRegEx": "Kaiser and \u0141ukasz.", "year": 2012}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Nal", "Blunsom", "Phil"], "venue": "In Proceedings EMNLP", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Grid long short-term memory", "author": ["Kalchbrenner", "Nal", "Danihelka", "Ivo", "Graves", "Alex"], "venue": "CoRR, abs/1507.01526,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Inductive programming: A survey of program synthesis techniques", "author": ["Kitzelmann", "Emanuel"], "venue": "In Approaches and Applications of Inductive Programming, AAIP 2009,", "citeRegEx": "Kitzelmann and Emanuel.,? \\Q2010\\E", "shortCiteRegEx": "Kitzelmann and Emanuel.", "year": 2010}, {"title": "Imagenet classification with deep convolutional neural network", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Fast algorithms for convolutional neural networks", "author": ["Lavin", "Andrew", "Gray", "Scott"], "venue": "CoRR, abs/1509.09308,", "citeRegEx": "Lavin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lavin et al\\.", "year": 2015}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Pham", "Vu", "Bluche", "Th\u00e9odore", "Kermorvant", "Christopher", "Louradour", "J\u00e9r\u00f4me"], "venue": "In Frontiers in Handwriting Recognition (ICFHR),", "citeRegEx": "Pham et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": "In Advances in Neural Information Processing Systems, pp. 3104\u20133112,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Vinyals", "Kaiser", "Koo", "Petrov", "Sutskever", "Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "CoRR, abs/1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "author": ["Max", "Teh", "Yee Whye"], "venue": "In Proceedings of ICML", "citeRegEx": "Welling et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2011}, {"title": "Learning to execute", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "CoRR, abs/1410.4615,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}, {"title": "Reinforcement learning neural turing machines", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "CoRR, abs/1505.00521,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 17, "context": "Deep neural networks have recently proven successful at various tasks, such as computer vision (Krizhevsky et al., 2012), speech recognition (Dahl et al.", "startOffset": 95, "endOffset": 120}, {"referenceID": 5, "context": ", 2012), speech recognition (Dahl et al., 2012), and in other domains.", "startOffset": 28, "endOffset": 47}, {"referenceID": 20, "context": "Sequence-tosequence recurrent neural networks with such cells can learn very complex tasks in an end-to-end manner, such as translation (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014), parsing (Vinyals & Kaiser et al.", "startOffset": 136, "endOffset": 201}, {"referenceID": 1, "context": "Sequence-tosequence recurrent neural networks with such cells can learn very complex tasks in an end-to-end manner, such as translation (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014), parsing (Vinyals & Kaiser et al.", "startOffset": 136, "endOffset": 201}, {"referenceID": 3, "context": "Sequence-tosequence recurrent neural networks with such cells can learn very complex tasks in an end-to-end manner, such as translation (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014), parsing (Vinyals & Kaiser et al.", "startOffset": 136, "endOffset": 201}, {"referenceID": 22, "context": ", 2015), speech recognition (Graves & Jaitly, 2014) or image caption generation (Vinyals et al., 2014).", "startOffset": 80, "endOffset": 102}, {"referenceID": 1, "context": "One way to resolve this problem is by using an attention mechanism (Bahdanau et al., 2014).", "startOffset": 67, "endOffset": 90}, {"referenceID": 6, "context": "Neural Turing Machines (Graves et al., 2014) have this theoretical property.", "startOffset": 23, "endOffset": 44}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2014; Cho et al., 2014), parsing (Vinyals & Kaiser et al., 2015), speech recognition (Graves & Jaitly, 2014) or image caption generation (Vinyals et al., 2014). Since so many tasks can be solved with essentially one model, a natural question arises: is this model the best we can hope for in supervised learning? It turns out that despite its recent success the sequence-to-sequence model has limitations. In its basic architecture, the entire input is encoded into a single fixed-size vector, so the model cannot generalize to inputs much longer than this fixed capacity. One way to resolve this problem is by using an attention mechanism (Bahdanau et al., 2014). This allows the network to inspect arbitrary parts of the input in every decoding step, so the basic limitation is removed. But other problems remain, and Joulin & Mikolov (2015) show a number of basic algorithmic tasks on which sequenceto-sequence LSTM networks fail to generalize.", "startOffset": 8, "endOffset": 869}, {"referenceID": 6, "context": "These issues are not limited to Neural Turing Machines, they apply to other architectures too, such as stack-RNNs (Joulin & Mikolov, 2015) or (De)Queue-RNNs (Graves et al., 2014).", "startOffset": 157, "endOffset": 178}, {"referenceID": 20, "context": "The learning of algorithms with neural networks has seen a lot of interest after the success of sequence-to-sequence neural networks on language processing tasks (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014).", "startOffset": 162, "endOffset": 227}, {"referenceID": 1, "context": "The learning of algorithms with neural networks has seen a lot of interest after the success of sequence-to-sequence neural networks on language processing tasks (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014).", "startOffset": 162, "endOffset": 227}, {"referenceID": 3, "context": "The learning of algorithms with neural networks has seen a lot of interest after the success of sequence-to-sequence neural networks on language processing tasks (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014).", "startOffset": 162, "endOffset": 227}, {"referenceID": 6, "context": "Neural Turing Machines (Graves et al., 2014) were shown to learn a number of basic sequence transformations and memory access patters, and their reinforcement learning variant (Zaremba & Sutskever, 2015b) has reasonable perfomance on a number of tasks as well.", "startOffset": 23, "endOffset": 44}, {"referenceID": 7, "context": "Stack, Queue and DeQueue networks (Grefenstette et al., 2015) were also shown to learn basic sequence transformations such as bigram flipping or sequence reversal.", "startOffset": 34, "endOffset": 61}, {"referenceID": 14, "context": "The Grid LSTM (Kalchbrenner et al., 2015) is another powerful architecture that can learn to multiply 15-digit decimal numbers.", "startOffset": 14, "endOffset": 41}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2014; Cho et al., 2014). An attempt was even made to learn to evaluate simple python programs with a pure sequence-to-sequence model (Zaremba & Sutskever, 2015a), but more success was seen with more complex models. Neural Turing Machines (Graves et al., 2014) were shown to learn a number of basic sequence transformations and memory access patters, and their reinforcement learning variant (Zaremba & Sutskever, 2015b) has reasonable perfomance on a number of tasks as well. Stack, Queue and DeQueue networks (Grefenstette et al., 2015) were also shown to learn basic sequence transformations such as bigram flipping or sequence reversal. The Grid LSTM (Kalchbrenner et al., 2015) is another powerful architecture that can learn to multiply 15-digit decimal numbers. As we will see in the next section, the Grid-LSTM is quite similar to the Neural GPU \u2013 the main difference is that the Neural GPU is less recurrent and is explicitly constructed from the highly parallel convolution operator. Most comparable to this work are the prior experiments with the stack-augmented RNNs (Joulin & Mikolov, 2015). These networks manage to learn and generalize to unseen lengths on a number of algorithmic tasks. But, as we show in Section 3.1, stack-augmented RNNs trained to add numbers upto 20-bit long generalize only to \u223c100-bit numbers, never to 200-bit ones, and never without error. Still, their generalization is the best we were able to obtain without using the Neural GPU and far surpasses a baseline LSTM sequence-to-sequence model with attention. The quest for learning algorithms has been pursued much more widely with tools other than neural networks. It is known under names such as program synthesis, program induction, automatic programming, or inductive synthesis, and has a long history with many works that we do not cover here; see, e.g., Gulwani (2010) and Kitzelmann (2010) for a more general perspective.", "startOffset": 8, "endOffset": 1890}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2014; Cho et al., 2014). An attempt was even made to learn to evaluate simple python programs with a pure sequence-to-sequence model (Zaremba & Sutskever, 2015a), but more success was seen with more complex models. Neural Turing Machines (Graves et al., 2014) were shown to learn a number of basic sequence transformations and memory access patters, and their reinforcement learning variant (Zaremba & Sutskever, 2015b) has reasonable perfomance on a number of tasks as well. Stack, Queue and DeQueue networks (Grefenstette et al., 2015) were also shown to learn basic sequence transformations such as bigram flipping or sequence reversal. The Grid LSTM (Kalchbrenner et al., 2015) is another powerful architecture that can learn to multiply 15-digit decimal numbers. As we will see in the next section, the Grid-LSTM is quite similar to the Neural GPU \u2013 the main difference is that the Neural GPU is less recurrent and is explicitly constructed from the highly parallel convolution operator. Most comparable to this work are the prior experiments with the stack-augmented RNNs (Joulin & Mikolov, 2015). These networks manage to learn and generalize to unseen lengths on a number of algorithmic tasks. But, as we show in Section 3.1, stack-augmented RNNs trained to add numbers upto 20-bit long generalize only to \u223c100-bit numbers, never to 200-bit ones, and never without error. Still, their generalization is the best we were able to obtain without using the Neural GPU and far surpasses a baseline LSTM sequence-to-sequence model with attention. The quest for learning algorithms has been pursued much more widely with tools other than neural networks. It is known under names such as program synthesis, program induction, automatic programming, or inductive synthesis, and has a long history with many works that we do not cover here; see, e.g., Gulwani (2010) and Kitzelmann (2010) for a more general perspective.", "startOffset": 8, "endOffset": 1912}, {"referenceID": 3, "context": "Before we introduce the Neural GPU, let us recall the architecture of a Gated Recurrent Unit (GRU) (Cho et al., 2014).", "startOffset": 99, "endOffset": 117}, {"referenceID": 4, "context": "GRUs have shown performance similar to LSTMs on a number of tasks (Chung et al., 2014; Greff et al., 2015).", "startOffset": 66, "endOffset": 106}, {"referenceID": 8, "context": "GRUs have shown performance similar to LSTMs on a number of tasks (Chung et al., 2014; Greff et al., 2015).", "startOffset": 66, "endOffset": 106}, {"referenceID": 19, "context": "But when applying it to recurrent networks, it has been counter-productive to apply it on recurrent connections \u2013 it only worked when applied to the non-recurrent ones, as reported by Pham et al. (2014). The code will be available at https://github.", "startOffset": 184, "endOffset": 203}], "year": 2015, "abstractText": "Learning an algorithm from examples is a fundamental problem that has been widely studied. Recently it has been addressed using neural networks, in particular by Neural Turing Machines (NTMs). These are fully differentiable computers that use backpropagation to learn their own programming. Despite their appeal NTMs have a weakness that is caused by their sequential nature: they cannot be parallelized and are are hard to train due to their large depth when unfolded. We present a neural network architecture to address this problem: the Neural GPU. It is based on a type of convolutional gated recurrent unit and, like the NTM, is computationally universal. Unlike the NTM, the Neural GPU is highly parallel which makes it easier to train and efficient to run. An essential property of algorithms is their ability to handle inputs of arbitrary size. We show that the Neural GPU can be trained on short instances of an algorithmic task and successfully generalize to long instances. We verified it on a number of tasks including long addition and long multiplication of numbers represented in binary. We train the Neural GPU on numbers with upto 20 bits and observe no errors whatsoever while testing it, even on much longer numbers. To achieve these results we introduce a technique for training deep recurrent networks: parameter sharing relaxation. We also found a small amount of dropout and gradient noise to have a large positive effect on learning and generalization.", "creator": "LaTeX with hyperref package"}}}