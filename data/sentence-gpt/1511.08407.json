{"id": "1511.08407", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2015", "title": "The Mechanism of Additive Composition", "abstract": "We prove an upper bound for the bias of additive composition (Foltz et al., 1998; Landauer and Dutnais, 1997; Mitchell and Lapata, 2010), a widely used method for computing meanings of phrases by averaging the vector representations of their constituent words. The result endorses additive composition as a reasonable operation for calculating meanings of phrases, which is the first theoretical analysis on compositional frameworks from a machine learning point of view (Tjernholm and Dutnais, 2008; Orenner, 2010).\n\n\n\nIn the first study of additive composition, we investigated the composition of words with the ability to categorize the constituent words by categories. First, we tested the use of a simple, intuitive approach to the composition of phrases. This method would also examine the potential contribution of the words to the analysis of the word categories, especially those using multiple sentences. In the first study, we investigated whether words used in more complex, highly complex words can be used to quantify the composition of phrases that may be associated with other meanings. The second study examined the relative impact of a simple sentence on the composition of terms. Finally, we considered the effect of simple sentences on the composition of terms, and discussed the effect of an initial sentence on the composition of words in complex terms.\n\nThe first study examined the effects of complex words in the composition of terms in the structure of sentences. These studies suggested that words that have been associated with other meaning and meaning in complex language are more powerful, more effective, and more concise than words that are unrelated to each other (e.g., Gertrude and Deutnais, 2007). We identified the effects of complex words in the structure of words in a simple sentence, and concluded that the effect of simple sentences on the composition of sentences could be evaluated for the same purpose in a similar way (Sergens et al., 2001; Dutnais, 2009). In this study, we investigated the effect of complex words on the composition of terms in words and to identify the relationship between complex words and the function of terms in these words, using a large, standardized framework. In a third study, we evaluated the impact of simple sentences on the composition of terms in the structure of words by grouping all words of the word categories by using a different formula. These two studies found that a small effect on the composition of terms in terms of terms of terms of terms of words could be evaluated for the same purpose in terms of terms of terms of words of", "histories": [["v1", "Thu, 26 Nov 2015 14:58:17 GMT  (1936kb,D)", "http://arxiv.org/abs/1511.08407v1", "Submitted to Journal of Machine Learning Research; Under Review"], ["v2", "Fri, 18 Dec 2015 23:34:40 GMT  (1942kb,D)", "http://arxiv.org/abs/1511.08407v2", "Submitted to Journal of Machine Learning Research; Under Review"], ["v3", "Mon, 6 Jun 2016 04:28:21 GMT  (1939kb,D)", "http://arxiv.org/abs/1511.08407v3", "In this version we have revised the mathematical proof. Now the theory is built on more fundamental assumptions -- an assumption we call it \"Generalized Zipf's Law\" and linked to a Hierarchical Pitman-Yor Process"], ["v4", "Tue, 7 Mar 2017 02:39:58 GMT  (2002kb,D)", "http://arxiv.org/abs/1511.08407v4", "More explanations on theory and additional experiments added. Accepted by Machine Learning Journal"]], "COMMENTS": "Submitted to Journal of Machine Learning Research; Under Review", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["ran tian", "naoaki okazaki", "kentaro inui"], "accepted": false, "id": "1511.08407"}, "pdf": {"name": "1511.08407.pdf", "metadata": {"source": "CRF", "title": "The Mechanism of Additive Composition", "authors": ["Ran Tian", "Naoaki Okazaki", "Kentaro Inui"], "emails": ["tianran@ecei.tohoku.ac.jp", "okazaki@ecei.tohoku.ac.jp", "inui@ecei.tohoku.ac.jp"], "sections": [{"heading": null, "text": "Keywords: compositional distributional semantics, bias and variance, approximation error bounds, natural language data"}, {"heading": "1. Introduction", "text": "The decomposition of generalization errors into bias and variance (Geman et al., 1992) is one of the most profound insights of learning theory. Bias is caused by low capacity of models when the training samples are assumed to be infinite, whereas variance is caused by overfitting to finite samples. In this paper, we apply the analysis to a new set of problems in compositional distributional semantics, the research in calculations of meanings of natural language phrases by vector representations of their constituent words. We prove an upper bound for the bias of a widely used compositional framework, the additive composition (Foltz et al., 1998; Landauer and Dutnais, 1997; Mitchell and Lapata, 2010).\nCalculations of meanings are fundamental problems in Natural Language Processing (NLP). In recent years, vector representations have seen great success at conveying meanings of individual words. The vectors are constructed from statistics of surrounding contexts of the words, based on the distributional hypothesis that words occurring in similar contexts tend to have similar meanings (Harris, 1954). For example, given a target word t, a common approach is to use the probability P (c|t) of t co-occurring with another word c in its surrounding context, and use the i-th word ci \u2208 C in a context lexicon C to define the i-th entry of the vector of t as a function of P (ci|t). As a result, all words are represented by vectors in a |C|-dimensional space such that, the vectors of semantically similar words\nc\u00a92000 Ran Tian, Naoaki Okazaki and Kentaro Inui.\nar X\niv :1\n51 1.\n08 40\n7v 1\n[ cs\n.C L\n] 2\n6 N\nare close to each other. It has been shown that the cosine similarities of such word vectors can capture similarities between words considerably well (Levy et al., 2015), thus the next natural question is to extend the vector representations to phrases and even sentences.\nBased on the success of distributional hypothesis, it is expected that at least for short phrases, the meanings can still be represented by vectors constructed from surrounding contexts. However, a main obstacle here is that phrases are far more sparse than individual words. For example, in the British National Corpus (BNC) (The BNC Consortium, 2007), which contains about 100 million word tokens, there are about 16000 lemmatized words which occur more than 200 times, but only about 46000 such bigrams, far less than the 160002 possible two-word combinations. In other words, there are too few training samples for even two-word phrases. Therefore, a direct estimation of the surrounding contexts of a phrase can have large sampling error. Alternatively, Mitchell and Lapata (2010) propose to construct vector representations of phrases by combining word vectors, based on the linguistic intuition that meanings of phrases are \u201ccomposed\u201d from the meanings of their constituent words. From a machine learning point of view, word vectors have smaller sampling error, or lower variance, because words are more abundant than phrases. Therefore, a compositional framework that calculates meanings of phrases by combining word vectors is more favorable, if the bias of this composition operation is also small.\nA number of compositional frameworks have been proposed in the literature. Some are complicated methods that rely on the underlying linguistic structures of phrases (Baroni and Zamparelli, 2010; Socher et al., 2012; Paperno et al., 2014), which are rationalized by linguistic intuitions (Coecke et al., 2010). Others have sought supports from cognitive science by comparing with human judgments (Mitchell and Lapata, 2010). However, for none of them the bias has ever been analyzed previously. The most widely used framework is the additive composition (Foltz et al., 1998; Landauer and Dutnais, 1997), in which the meanings of phrases are calculated by averaging the word vectors. Yet, it was unknown if this average is by any means related to statistics of contexts surrounding the phrases.\nIn this paper, we show that the bias of additive composition of two-word phrases is bounded from above (Section 2). This result provides a learning-theoretical support for additive composition, and we expect it to inspire more theoretical analysis on other composition operations. Such theories would be important for the pursuit of better compositional frameworks, given that the empirical evaluation of compositional frameworks is already considerably complicated, due to different choices of the word vectors, the composition operations, and the methods for estimating parameters (Dinu et al., 2013). What is worse, newly proposed word vectors may not necessarily conform to previously established composition operations. Therefore, theories would be useful for guiding the empirical research.\nIn particular, our theory of additive composition leads to the following predictions.\n1. The upper bound of the bias depends on specific conditions on the function applied to P (ci|t) as the i-th entry of the vector representation, which suggests that the conditions on the function are crucial for additive compositionality (Section 2.1). This constraint provides a rigorous and unified explanation for the additive compositionality of several recently proposed word vectors, including the Skip-Gram with Negative Sampling (SGNS) (Mikolov et al., 2013b), the GloVe model (Pennington et al., 2014), the Hellinger PCA (Lebret and Collobert, 2014) and a CCA model (Stratos et al.,\n2015). Despite many speculations on these empirically established models, our explanation has a sound argument and experimental verifications.\n2. At least for two-word phrases, a novel type of vector representations can solve a long-standing issue of additive composition, namely the unawareness of word order (Section 2.2). Unlike previous alternative approaches, the vectors are still composed by taking average, retaining the merits of being parameter-free and having a bias bound.\n3. Regarding dimension reduction methods for training word vectors, the loss function of Singular Value Decomposition (SVD) is probably more suitable for maintaining additive compositionality, compared to other popular methods such as SGNS and GloVe (Section 2.3).\nAssumptions and implications of our theory are experimentally verified (Section 5), showing their correctness. Furthermore, extrinsic evaluations confirm that the above predictions correlate with human judgments (Section 6), demonstrating the usefulness of our theory in explaining additive compositionality.\nUnlike a typical empirical research in NLP, we develop new mathematics in this work to capture the nature of compositional distributional semantics. Also different from a general machine learning theory, we make essential use of properties specific to natural language data. As a theoretical research, we believe this work brings new topics to machine learning and will find further applications in NLP."}, {"heading": "2. Theoretical Results", "text": "A vector representation in the distributional paradigm is constructed from context features, such that each dimension is a function of co-occurrence with a context word. More precisely, we consider that each target is associated with its context, which is usually defined as a window of context words surrounding the target. For example, in Table 1, we show the contexts of the target word tokens \u201ctax\u201d and \u201crate\u201d as words occurring within a window of size 5 to each side of the targets. When a target word t is given, we obtain the set of tokens of the target word in a corpus, and then we obtain the distribution of context words cooccurring with the target word tokens. This distribution determines the probability P (c|t) of a word c co-occurring with t, which is used for constructing the vector representation. In this paper, we use the notation t to denote interchangeably the target word, the obtained\nset of tokens of the target word, or the distribution of its context words. Later, we will consider other sets of tokens or the distributions of words in other contexts. Those sets of tokens or distributions are not obtained from a word as described here, but we still regard them as targets and denote such general targets as \u03c4 , as long as the co-occurring probability P (c|\u03c4) can be defined. For constructing the vector representation, we fix a lexicon C of context words, and we define the i-th entry of the vector representation as a function of P (ci|\u03c4) where ci \u2208 C is the i-th word in the lexicon. Formally,\nw\u03c4 := ( s(ci, \u03c4) ) i\nwhere s(ci, \u03c4) is a function of the form\ns(ci, \u03c4) := r \u00b7 ( F (P (ci|\u03c4))\u2212 a(\u03c4)\u2212 b(ci) ) . (1)\nHere, r is a scalar and F (\u00b7), a(\u00b7) and b(\u00b7) are real-valued functions. Though there are other methods for constructing word vectors, such as neural networks (Collobert et al., 2011), we consider this formalization as general enough to cover a wide range of distributional word vectors in the literature, including some popular models such as SGNS and GloVe. Indeed, if we set F (p) := ln p and b(ci) := lnP (ci), then s(ci, \u03c4) becomes the Point-wise Mutual Information (PMI) function which has long been widely adopted in NLP (Church and Hanks, 1990; Dagan et al., 1994; Turney, 2001; Turney and Pantel, 2010). The SGNS model is a matrix factorization of PMI (Levy and Goldberg, 2014b), and the more general form of shift terms a(t) and b(ci) are explicitly introduced by GloVe (Pennington et al., 2014). As for other forms of F , it has been reported that one can achieve better empirical results by setting F (p) := \u221a p instead of F (p) := p (Lebret and Collobert, 2014; Stratos et al., 2015) (see Section 2.1 for further discussion). Our formalization provides a unified point of view for all these works. The parameter r is introduced here to normalize the scales of vectors.\nIn a similar manner, we can specify the context of a bigram (e.g. the context of tax rate in Table 1), and construct vector representations for two-word phrases such as \u201ct1 t2\u201d. We use t1t2 to denote the unordered bigram (i.e. either \u201ct1 t2\u201d or \u201ct2 t1\u201d), and use t1 t2 to denote the ordered one. The probabilities of these targets co-occurring with a context word c are denoted by P (c|t1t2) and P (c|t1 t2), respectively. Then, the functions s(ci, t1t2) and s(ci, t1 t2), and the vectors wt1t2 and wt1 t2 , are defined the same as above. A compositional framework can thus be regarded as a method of combining vectors wt1 and wt2 to approximate the vector wt1t2 (or wt1 t2 , if one cares about the word order, which we will discuss in Section 2.2). We use the Euclidean distance\nEt1t2 := \u2016wt1t2 \u2212 COMP(wt1 ,wt2)\u2016\nto measure the error in this approximation. Here, COMP(\u00b7, \u00b7) is the composition operation defined in the framework.\nIf COMP has parameters, it is a widely adopted approach to learn the parameters by minimizing this error Et1t2 for bigrams observed in a corpus (Dinu et al., 2013; Baroni and Zamparelli, 2010; Guevara, 2010). However, bigrams are considerably sparse in real corpora, so there is the possibility that such parameter optimizations can be overfitting.\nOn the other hand, the following additive composition is also widely used:\nCOMP(wt1 ,wt2) := 1\n2 (wt1 + wt2).\nSince this operation does not have parameters, it does not overfit. In other words, additive composition has lower variance compared to other composition operations that have parameters. However, the bias of additive composition should also be small for it to be reasonable. More precisely, bias is defined as the error Et1t2 of approximation under the condition that the corpus is sufficiently large for any plausible phrase to occur sufficiently many times. When training samples are sufficient, the error is caused by the low capacity of additive composition to fit the precise phrase vector. In this paper, we prove an upper bound for this approximation error, which endorses additive composition as a reasonable approximation method. Previously, the analysis of bias and variance has never been applied to compositional frameworks; we expect our work to inspire more followups on this topic.\nThe key to our bias bound is the observation that, every word occurring in the context of tktl also occurs in the contexts of tk and tl. As we see from Table 1, if a token of tk (e.g. \u201ctax\u201d) comes from a bigram of tktl (e.g. \u201ctax rate\u201d) and if the context windows are not too small, the context of the token of tk is almost the same as the context of the bigram of tktl. Therefore, we can decompose the co-occurrence probability P (c|tk) (resp. P (c|tl)) into two components, either coming from the bigram tktl or not:\nP (c|tk) = \u03c0k\\lP (c|tk \\ tl) + (1\u2212 \u03c0k\\l)P (c|tktl), (2) P (c|tl) = \u03c0l\\kP (c|tl \\ tk) + (1\u2212 \u03c0l\\k)P (c|tktl). (3)\nHere, tk \\ tl (resp. tl \\ tk) denotes the set of tokens of the target word tk (resp. tl) which do not come from tktl; and the probability of a given tk (resp. tl) token not coming from tktl is denoted by \u03c0k\\l (resp. \u03c0l\\k). In practice, \u03c0k\\l and \u03c0l\\k can be estimated as\n\u03c0k\\l = 1\u2212 Freq(tktl)/Freq(tk) and \u03c0l\\k = 1\u2212 Freq(tktl)/Freq(tl),\nwhere Freq(\u00b7) denotes the frequency count in a corpus. We can view \u03c0k\\l and \u03c0l\\k as indicating how weak the \u201ccollocation\u201d between tk and tl is. When \u03c0k\\l and \u03c0l\\k are small, tk tends to occur with tl together, so both wtk and wtl will be highly correlated with wtktl , then the approximation error Etktl will be small. This intuition is formalized as 1 below.\nClaim 1 Let s(ci, \u03c4) be defined as in (1). We normalize a(\u03c4), b(ci) and r such that, the entries of each w\u03c4 sum to 0, the centroid of the collection {wtktl} of phrase vectors is shifted to 0, and their norms {\u2016wtktl\u2016} average to 1. Assume that\nlim p\u21920\nF \u2032(p) p\u03b1\u22121 = Constant 6= 0 for some \u03b1 \u2264 0.5. (4)\nThen, the following upper bound Etktl \u2264 \u221a 1\n2 (\u03c02k\\l + \u03c0 2 l\\k + \u03c0k\\l\u03c0l\\k) (5)\nholds approximately in a sufficiently large corpus.\nAs we expected, for more \u201ccollocational\u201d phrases, since \u03c0k\\l and \u03c0l\\k are smaller, the upper bound (5) becomes stronger. Note that the normalization of a(\u03c4), b(ci) and r in 1 is necessary, because without normalization, one can arbitrarily change Etktl by varying a(\u03c4), b(ci) and r. In addition, this normalization constraints the relation between the Euclidean distance Etktl and the cosine similarity cos(wtktl ,wtk + wtl), which is the most widely used similarity measure in practice. When the norm \u2016wtktl\u2016 is approximately fixed to 1, a smaller Euclidean distance mostly indicates a higher cosine similarity and vice versa.\nThe proof of 1 (Theorem 5) is given in Section 4, based on several assumptions namely Zipf\u2019s law, the Assumption I. and the Assumption II. described in Section 4. These assumptions and the upper bound are experimentally verified in Section 5. The result is an upper bound for the bias (i.e. an upper bound for Etktl under the condition that the corpus is infinitely large), which endorses the additive composition 12(wtk + wtl) as a reasonable approximation for the meaning of tktl because, when tktl is rare or unseen in a real corpus, the vector representation wtktl cannot be reliably estimated from the corpus; yet, the distributional hypothesis asserts that wtktl can represent the meaning of tktl once it can be estimated from a sufficiently large ideal corpus. The bias bound proves that wtktl can be reasonably approximated by 12(wtk + wtl) in the ideal corpus, in addition wtk and wtl can be reliably estimated from the real corpus since individual words are abundant. Therefore, additive composition is suitable for representing meanings of phrases rare or unseen in a real corpus. Furthermore, it is suitable for frequently occurring phrases as well, because such phrases are usually strongly collocational and the upper bound (5) implies that the bias will be small in this case.\nMoreover, several empirical implications can be drawn from our bias bound, as described in the subsections below."}, {"heading": "2.1 The Choice of Function F", "text": "The condition (4) in 1 specifies a nontrivial constraint on the function F . Roughly speaking, it requires that F (p) has large slope as p tends to 0, for the bias bound to hold. This condition suggests that the asymptotic behavior of F (p) at the limit p \u2192 0 may be a crucial factor in additive compositionality. In particular, since F (p) := ln p satisfies (4) with \u03b1 = 0, and F (p) := \u221a p with \u03b1 = 0.5, we expect the bias in additive composition of vector representations using theses settings of F to be bounded as predicted; however, a bias bound is not guaranteed for F (p) := p or F (p) := p ln p, as these settings do not satisfy (4). Therefore, vector representations using F (p) := p or F (p) := p ln p are probably less compatible to additive composition than F (p) := ln p and F (p) := \u221a p.\nDifferent settings of function F have been considered in previous research, and speculations have been made about the reason of semantic additivity of some of the vector representations. In Pennington et al. (2014), the authors noted that logarithm is a homomorphism from multiplication to addition, and used this property to justify F (p) := ln p for training semantically additive word vectors, based but on the unverified hypothesis that multiplications of co-occurrence probabilities have specialties in semantics. On the other hand, Lebret and Collobert (2014) proposed to use F (p) := \u221a p, which is motivated by the Hellinger distance between two probability distributions, and reported its being better than F (p) := p. Stratos et al. (2015) proposed a similar but more general and better-motivated\nmodel, which attributed F (p) := \u221a p to an optimal choice that stabilizes the variances of Poisson random variables. Based on the assumption that co-occurrence counts are generated by a Poisson process, the authors pointed out that F (p) := \u221a p may have the effect of stabilizing the variance in estimating the word vectors. In contrast, our theory shows clearly that F affects the bias of additive composition, besides the variance. All in all, none of the previous research can explain why F (p) := ln p and F (p) := \u221a p are both good choices but F (p) := p is not. To give a flavor of the effects that the function F can cause on the vector representations, here we show an outstanding example that is formalized as Lemma 2 in Section 4.4 and used in proof of our bias bound.\nClaim 2 Assume that a(\u03c4), b(ci) and r are normalized as in 1. If F satisfies (4) and the corpus is sufficiently large, then the norms {\u2016wtktl\u2016} of all phrase vectors are approximately equal to 1.\n2 is closely related to the properties of natural language data, namely Zipf\u2019s law and the Assumption I. in Section 4.3. Roughly speaking, Assumption I. suggests that all P (c|\u03c4) tend to behave the same as P (c), independent of \u03c4 , as Freq(c) tends to 0. When \u03b1 is small enough, F (p) has large slope as p tends to 0, so F (P (c|\u03c4)) will \u201cemphasize\u201d those P (c|\u03c4) that are small, which tend to behave the same as P (c). Therefore, the norms {\u2016wtktl\u2016} all become approximately the same as F emphasizes similar behaviors. The threshold of \u03b1 being small enough, namely \u03b1 \u2264 0.5, is determined by Zipf\u2019s law.\nBoth 2 and the effect of F on the bias bound (5) are experimentally verified (Section 5). Furthermore, extrinsic evaluations show that F indeed drastically affects additive compositionality as judged by humans (Section 6); while F (p) := ln p and F (p) := \u221a p perform similarly well, F (p) := p and F (p) := p ln p are much worse."}, {"heading": "2.2 Handling Word Order in Additive Composition", "text": "By considering the vector representation wt1t2 we have ignored the word order and conflated the phrases \u201ct1 t2\u201d and \u201ct2 t1\u201d. Though meanings of these two phrases are usually related somehow, to treat a compositional framework as approximating wt1t2 instead of wt1 t2 would certainly be troublesome, especially when one tries to extend our theory to longer phrases or even sentences. As the following famous example (Landauer et al., 1997) shows, meanings of sentences can differ greatly as the word order changes.\na. It was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem.\nb. That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious.\nTherefore, we need to handle the changes of meanings brought by different word ordering. Traditionally, additive composition is thought as unsuitable for this purpose because we always have wt1 + wt2 = wt2 + wt1 . However, the commutativity can be broken by defining different contexts for \u201cleft-hand-side\u201d words and \u201cright-hand-side\u201d words, denoted by tL and tR respectively, so the co-occurring probabilities P (c|tL) and P (c|tR) will be different, therefore 12(wtL1 + wtR2 ) and 12(wtR1 + wtL2 ) will be different vectors. In this section, we\npropose the Near-far Context that defines different contexts for tL and tR such that, the phrase vector wt1 t2 for the ordered bigram t1 t2 can be approximated by\n1 2(wtL1 + wtR2 ).\nIn Near-far Context, context words are assigned labels, either N or F ; we redefine the lexicon C as a lexicon of N -F labeled context words, and regard words with different labels as different entries in the lexicon. As we have discussed previously, the key fact about additive composition is that, every word occurring in the context of t1t2 (i.e., both \u201ct1 t2\u201d and \u201ct2 t1\u201d) also occurs in the contexts of t1 and t2. Thus, our idea of handling word order in Near-far Context is that, to make every labeled word occurring in the context of t1 t2 contribute to the contexts of both tL1 and t R 2 , but none of the labeled words occurring in the context of t2 t1 do. This is achieved by assigning N -F labels to words surrounding t L or tR in different ways. For any target word or bigram, we label the nearer two words to each side by N , and the farther two words to each side by F ; except that, for tL we assign labels in the \u201cleft-hand-side way\u201d by skipping one word adjacent to the right, and for tR we assign labels in the \u201cright-hand-side way\u201d by skipping one word adjacent to the left. Therefore, surrounding the ordered bigram t1 t2, the context words for t1 t2, t L 1 and t R 2 are all labeled the same (Figure 1); on the other hand, surrounding the reverse-ordered bigram t2 t1, the context words for t L 1 and t R 2 are labeled differently. As a result, no labeled word occurring in the context of t2 t1 contributes to both contexts of t L 1 and t R 2 , so we predict that the error \u2016wt2 t1 \u2212 12(wtL1 + wtR2 )\u2016 is likely to be larger than \u2016wt1 t2 \u2212 1 2(wtL1 + wtR2 )\u2016, which means that wtL1 + wtR2 approximates the meaning of t1 t2 rather than t2 t1.\nFormally, we have the following equations similar to (2) and (3):\nP (c|tLk ) = \u03c0(k\\l)LP (c|(tk \\ tl)L) + (1\u2212 \u03c0(k\\l)L)P (c|tk tl), (6) P (c|tRl ) = \u03c0(l\\k)RP (c|(tl \\ tk)R) + (1\u2212 \u03c0(l\\k)R)P (c|tk tl). (7)\nHere, (tk\\tl)L (resp. (tl\\tk)R) denotes the set of tokens of the target word tk (resp. tl) which do not come from tk tl, and of which the context words are labeled in the \u201cleft-hand-side way\u201d (resp. \u201cright-hand-side way\u201d); the probability of a given token of tk (resp. tl) being such is denoted by \u03c0(k\\l)L (resp. \u03c0(l\\k)R). In practice, \u03c0(k\\l)L and\u03c0(l\\k)R can be estimated as\n\u03c0(k\\l)L = 1\u2212 Freq(tk tl)/Freq(tk) and \u03c0(l\\k)R = 1\u2212 Freq(tk tl)/Freq(tl).\nThen, we have a claim similar to 1.\nClaim 3 Under the same conditions as in 1, the following upper bound\n\u2016wtk tl \u2212 1\n2 (wtLk + wtRl )\u2016 \u2264\n\u221a 1\n2 (\u03c02 (k\\l)L + \u03c0 2 (l\\k)R + \u03c0(k\\l)L\u03c0(l\\k)R)\nholds approximately in a sufficiently large corpus.\nThis is a bias bound for the approximation of meaning of the ordered bigram t1 t2 by the additive composition 12(wtL1 + wtR2 ). This bias bound, and the prediction that the error \u2016wt2 t1 \u2212 12(wtL1 + wtR2 )\u2016 for approximating the reverse-ordered t2 t1 can exceed the bound are experimentally verified, and we show that by using the additive composition of Near-far Context vectors, we can assess meaning similarities between ordered bigrams (Section 5.2)."}, {"heading": "2.3 Dimension Reduction", "text": "By far we have only discussed vector representations that have a high dimension equal to the lexicon size |C|. In practice, people mainly use low-dimensional vectors or \u201cword embeddings\u201d to represent meanings of words. Many of these embeddings, including SGNS and GloVe, can be formalized as factorizing the matrix ( s(ci, tj) ) i,j , which is equivalent to the finding of a d-dimensional vector vt (where d |C|) for each target word t, and a (|C|\u00d7d)-matrix A such that \u2211 j L(Avtj ,wtj ) is minimized for some loss function L(\u00b7, \u00b7). In other words, Avt is trained as a good approximation for wt.\nNaturally, we expect the loss function L to account for a major factor of word embeddings that could affect performance. Although there are empirical investigations on other detailed designs of some word embeddings (e.g. how to count co-occurrences with contexts, see Levy et al. 2015), their loss functions have not been explicitly discussed previously. Can the loss functions affect the additive compositionality of trained word vectors? In this section, we discuss this topic from a viewpoint of bounding the error \u2016vt1t2 \u2212 12(vt1 + vt2)\u2016.\nSVD When L is the L2-loss, the minimization problem has a closed-form solution given by the Singular Value Decomposition (SVD). More precisely, one uses SVD to factorize the matrix ( s(ci, tj) ) i,j into the form U\u03a3V >, where U , V are orthonormal and \u03a3 is diagonal.\nThen, \u03a3 is truncated to the top d singular values, denoted by \u03a3d. One solves A as U \u221a\n\u03a3d and vtj as the j-th column vector of \u221a \u03a3dV\n>. Such word vectors have been used in Lebret and Collobert (2014), Stratos et al. (2015) and Levy et al. (2015).\nFor L2-loss, we can assume that\n\u2016Avt1 \u2212wt1\u2016 \u2264 \u03b51, \u2016Avt2 \u2212wt2\u2016 \u2264 \u03b52 and \u2016Avt1t2 \u2212wt1t2\u2016 \u2264 \u03b53,\nwhere \u03b51, \u03b52 and \u03b53 are minimized. Therefore, by triangular inequality and 1,\n\u2016A \u00b7 ( vt1t2 \u2212 1\n2 (vt1 + vt2)\n) \u2016 \u2264 \u2016wt1t2 \u2212 1\n2 (wt1 + wt2)\u2016+\n1 2 (\u03b51 + \u03b52) + \u03b53\n\u2264 \u221a 1\n2 (\u03c021\\2 + \u03c0 2 2\\1 + \u03c01\\2\u03c02\\1) +\n1 2 (\u03b51 + \u03b52) + \u03b53\nwhich suggests that \u2016vt1t2 \u2212 12(vt1 + vt2)\u2016 is bounded from above. However, the same argument cannot be directly applied to other loss functions, because a general loss may not satisfy a triangular inequality, and a bound for Euclidean distance may not always imply a bound for the loss function or vice versa. Especially, we consider the loss functions of some popular embedding methods here.\nGloVe The GloVe model (Pennington et al., 2014) is a dimension reduction of vector representations in which F (p) := ln p. For Avt = (vi)i and wt = (wi)i, the loss function is given by\nLt(vi, wi) = f ( #(ci, t) ) (vi \u2212 wi)2.\nIn words, GloVe uses a weighted L2-loss, where the weight f ( #(ci, t) ) is a function of the co-occurrence count #(ci, t). The function f is set to constant when #(ci, t) is larger than a given threshold, and it decreases to 0 when #(ci, t)\u2192 0. To minimize this loss function, GloVe uses stochastic gradient descent methods such as AdaGrad (Duchi et al., 2011).\nSGNS Originally inspired by neural networks (Mikolov et al., 2013b), SGNS is also a dimension reduction of vector representations in which F (p) := ln p. The training of SGNS is based on the Noise Contrastive Estimation (NCE) (Gutmann and Hyva\u0308rinen, 2012), therefore two inherited parameters can affect the loss function, namely the number k of noise samples per data point, and the distribution Pnoise(\u00b7) of noise. The loss function of SGNS is given in the following claim.\nClaim 4 For Avt = (vi)i and wt = (wi)i, the loss function of SGNS is given by Lt(vi, wi) = P (t)D\u03c6i ( vi + ln(kPnoise(ci)), wi + ln(kPnoise(ci)) ) ,\nwhere D\u03c6i(\u00b7, \u00b7) is the Bregman divergence associated to the convex function\n\u03c6i(x) = ( P (ci|t) + kPnoise(ci) ) ln ( exp(x) + kPnoise(ci) ) .\nWhen k \u2192 +\u221e, D\u03c6i converges to another Bregman divergence D\u03d5 which is associated to \u03d5(x) = exp(x).\nThe proof of 4 is found in Appendix B. We draw a graph of the SGNS loss function in Figure 3, where vi \u2212 wi is plotted as x and D\u03c6i ( vi + ln(kPnoise(ci)), wi + ln(kPnoise(ci)) ) is plotted as y. The graph grows faster at vi \u2212 wi \u2192 +\u221e than at vi \u2212 wi \u2192 \u2212\u221e, suggesting that an overestimation of wi by vi will be punished more than an underestimation. In addition, the loss function weighs more on frequent context words, as indicated by the P (ci|t) coefficient in the equation of the limit curve y = P (ci|t)(exp(x)\u2212 x\u2212 1). Therefore,\nSGNS tends to enforce underestimations of wi for frequent context words (as overestimation will be costly), and to compensate wi for rare ones (i.e., since overestimations on rare context words are affordable, they will be done if necessary). This could be a special property of SGNS, different from a (weighted) L2-loss.\nOne property that is common in GloVe and SGNS, but not in SVD is that, the loss functions of GloVe and SGNS both weigh less on rare context words. As a result, the trained Avt may fail to approximate the low co-occurrence part of wt, of which the behavior is emphasized by the choice of function F , as we have discussed in Section 2.1. Since this emphasized behavior forms a crucial part in derivation of our bias bound, we expect word vectors trained by GloVe or SGNS to be less respectful to the bound. Therefore, we conjecture that the loss functions of these models can hurt additive compositionality.\nThe above discussion is only exploratory and cannot fully comply with practice because, after vt are trained by dimension reduction, people usually re-scale all the norms of vt to 1, then they use these normalized word vectors for additive composition. It is not clear why this normalization step can usually result in better performance.\nNevertheless, in our experiments, we find that word vectors trained by SVD can preserve the upper bound (5) well, even after the normalization step is conducted. On the other hand, word vectors trained by GloVe or SGNS are less respectful to the bound as predicted (Section 5.3). Furthermore, extrinsic evaluations also show that word vectors trained by SVD might be more compatible to additive composition, as judged by humans (Section 6)."}, {"heading": "3. Related Work", "text": "Additive composition is the classical approach to approximating the semantics of phrases and/or sentences (Foltz et al., 1998; Landauer and Dutnais, 1997). Compared to other composition operations, vector addition/average has either served as a strong baseline (Mitchell and Lapata, 2008), or remained one of the most competitive methods until recently (Banea et al., 2014). Additive composition has been successfully integrated into several NLP systems as well. For example, Tian et al. (2014) use vector additions for assessing meaning similarities between paraphrase candidates in a logic-based textual entailment recognition system (e.g. the similarity between \u201cblamed for death\u201d and \u201ccause loss of life\u201d is calculated by the cosine similarity between sums of word vectors vblame+vdeath and vcause+vloss+vlife); in Iyyer et al. (2015), the average of word vectors in a whole sentence/document is fed into a deep neural network for sentiment analysis and question answering, which achieves near state-of-the-art accuracies with minimum training time. Other semantic relations have been represented by vector additions as well, such as analogy (e.g. the vector vking\u2212vman+vwoman turns out to be close to vqueen, illustrating the relation \u201cman is to king as woman is to queen\u201d, see Mikolov et al. 2013a), and synonymy (i.e. the sum of word vectors in a set of synonyms is used to represent that \u201csynset\u201d, see Rothe and Schu\u0308tze 2015). We believe all these utilities can be related to our theory for additive composition somehow, for example a hypothesis that relates word analogy with additive composition of phrases is described in Section 6.2. Thus, we expect our theory to provide new insights into these previous works, for example, about the word vectors being used.\nWord-order dependent syntactic effects on meaning have been considered as the most important lack in additive composition (Landauer, 2002). Driven by this point of view, a\nnumber of advanced compositional frameworks have been proposed to cope with word order and/or syntactic information (Mitchell and Lapata, 2008; Zanzotto et al., 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014). The ordinary approach is by introducing new parameters to represent differences in word positions or syntactic roles. For example, in a two-word phrase, one can first transform the two word vectors by different matrices before adding them, in which the two matrices are parameters (Mitchell and Lapata, 2008); or, regarding different syntactic roles, one can assign matrices to adjectives for modifying vectors of nouns (Baroni and Zamparelli, 2010); further, one can insert neural network layers between parents and their children in a syntactic tree (Socher et al., 2012). An empirical comparison of a broad range of compositional models, with an accessible introduction to the literature can be found in Blacoe and Lapata (2012). One theoretical issue of these methods, however, is that they lack learning guarantee. In contrast, our proposal of the Near-far Context demonstrates that, at least for two-word phrases, word order can still be handled by additive composition, in a parameter-free way with a proven bias bound.\nError bounds in approximation schemes have been extensively studied in statistical learning theory (Vapnik, 1995; Gnecco and Sanguineti, 2008), and especially for neural networks (Niyogi and Girosi, 1999; Burger and Neubauer, 2001). Since we have formalized compositional frameworks as approximation schemes, there is a good chance to apply the theories of approximation error bounds to this problem, especially for advanced compositional frameworks that have many parameters. Though the theories are usually established on general settings, we see a great potential in using properties that are specific to natural language data, as we demonstrate in this work.\nThere have been consistent efforts toward understanding the stochastic behavior of natural languages. Zipf\u2019s law (Zipf, 1935) is a classical observation and still finds new applications in recent results (Kobayashi, 2014). Advanced Bayesian language models such as the hierarchical Pitman-Yor process (Teh, 2006) and the topic model (Blei, 2012) have been proposed, which we expect could further refine our theory, for example, by considering the additive compositionality of topics."}, {"heading": "4. Proof of the Bias Bound", "text": "In order to derive the bias bound, we should study statistics of the vector representation w\u03c4 as a whole, rather than delve into some specific co-occurrence probabilities. Our idea is to view ci \u2208 C (1 \u2264 i \u2264 |C|) as a random sample drawn from a probability space \u2126|C|, and view P (\u00b7), P (\u00b7|\u03c4) and s(\u00b7, \u03c4) as random variables (i.e., functions from \u2126|C| to R). For brevity, we denote P (\u00b7|\u03c4) by P\u03c4 and s(\u00b7, \u03c4) by S\u03c4 . Then, by the law of large numbers,\n1\n|C| E2t1t2 : E|C|[(St1t2 \u2212\n1 2 (St1 + St2)) 2], (8)\nwhere E|C|[\u00b7] denotes the expectation of a random variable on \u2126|C|, and \u201c:\u201d denotes equality at the limit of an infinitely large corpus, i.e. |C| \u2192 \u221e. Thus, we have converted our goal of bounding Et1t2 into an estimation of the second moment of a random variable, namely X := St1t2 \u2212 12(St1 + St2).\nIn order to discuss the limit |C| \u2192 \u221e, we formally define \u2126|C| as follows. Firstly, we assume a measure space \u2126 which contains all (infinitely many) possible words, and each\nword is measured uniformly. Assume a measurable function Freq : \u2126\u2192 R representing the word frequency. Then, letting fn be the frequency of the n-th frequent word, we define\n\u2126n := {c \u2208 \u2126 | Freq(c) \u2265 fn},\nso \u2126|C| contains the (finitely many) words of which the frequencies are greater than f|C|. We define a probability on \u2126|C| by normalizing the measure of \u2126 such that the entire subspace \u2126|C| \u2282 \u2126 is measured as 1.\n4.1 Normalization of a(\u03c4), b(ci), and r\nRecall that E|C|[X 2] = V|C|[X] +E|C|[X] 2, where V|C|[\u00b7] denotes the variance. By adjusting a(\u03c4), we can always set E|C|[S\u03c4 ] = 0, so that E|C|[X] = 0 and V|C|[X] is unaffected, which results in a smaller error E|C|[X\n2] = V|C|[X]. Therefore, we focus on this case and assume that a(\u03c4) is adjusted such that E|C|[S\u03c4 ] = 0 (i.e. the entries of w\u03c4 sum to 0).\nIn order to fairly compare Et1t2 for different F , the scale r should also be normalized. Since both \u2016wt1t2\u2016 and Et1t2 scale linearly with r, we first use b(ci) to shift w\u03c4 such that the collection {wtktl} of phrase vectors has a centroid 0, then we set r such that the norms {\u2016wtktl\u2016} average to 1. The shift step prevents {wtktl} from clustering around one point, and it is sufficient to focus on one setting of b(ci), because b(ci) does not affect the error (i.e., it is canceled out in the expression of Et1t2). In Section 4.4, we will approximately calculate these terms for a specific family of function F ."}, {"heading": "4.2 Zipf\u2019s Law", "text": "An assumption essential to our calculation is Zipf\u2019s law (Zipf, 1935), which states that the frequency of any word is inversely proportional to its rank in the frequency table. In other words, we have f1/fn = n. Now, consider the probability Pr(Freq \u2265 fn). By definition, there are a total of n words of which the frequencies are greater than fn. Meanwhile, there are a total of |C| words in \u2126|C| and each occurs uniformly, so we have Pr(Freq \u2265 fn) = n/|C|. Therefore,\nPr(Freq \u2265 fn) = n\n|C| = f1 |C| \u00b7 fn = f|C| fn . (9)\nReplacing fn in (9) by x, we obtain the cumulative distribution function of Freq:\nPr(Freq \u2265 x) = f|C|\nx (f|C| \u2264 x). (10)\nThen, by differentiating (10), we obtain the probabilistic density function of Freq:\nPr(x \u2264 Freq < x+ dx) = f|C|\nx2 dx (f|C| \u2264 x). (11)\nWe view the random variable P as a constant multiple of Freq, namely\nP = \u03c9 \u00b7 Freq, (12)\nwhere \u03c9 is a constant. It is known that Zipf\u2019s law holds well on many natural language corpora. We refer to Montemurro (2001), Ha et al. (2002), and Clauset et al. (2009) for detailed analyses and empirical tests."}, {"heading": "4.3 An Assumption on P\u03c4", "text": "In effect, Zipf\u2019s law specifies the distribution of the random variable Freq in (11), which in turn determines the distribution of P via Equation (12). We need assumptions that specify the distributions of random variables P\u03c4 as well, where \u03c4 can be a bigram t1t2 or a restricted target word t1 \\ t2. We do not expect a random P (c|\u03c4) for c \u223c \u2126|C| to be too different from P (c), especially when Freq(c) is small because, by Zipf\u2019s law there are a lot of context words whose frequencies are small, but intuitively only a small number of c \u2208 \u2126|C| are semantically related to the target \u03c4 and will have a P (c|\u03c4) drastically different from P (c). Therefore, we make the following Assumption I.:\nI. There exist \u03ba > 0 and \u03b2 6= 0 such that,\nlim x\u21920 E[ ( ln P\u03c4 P )2 | Freq = x]/x\u03ba = \u03b2 (13) and lim\nx\u21920 E[ln P\u03c4 P | Freq = x]2/x\u03ba = 0. (14)\nHere, E[\u00b7 | Freq = x] denotes the conditional expectation, which is calculated by taking the expectation on c \u2208 \u2126 such that Freq(c) = x. Equation (13) means that P (c|\u03c4) is likely to be closer to P (c) as Freq(c) becomes smaller, so the conditional second moment of ln(P\u03c4/P ) decreases to 0 at a rate of x\u03ba. Moreover, Equation (14) assumes that the squared conditional first moment of ln(P\u03c4/P ) decreases even faster.\nAssumption I. is experimentally verified in Section 5. It is used in calculations of the first and second moments of F (P\u03c4 ) for a wide range of function F . The approach, called the \u201cdelta method\u201d (Oehlert, 1992), is as follows. For brevity, we use Yx := ln(P\u03c4/P ) \u2223\u2223 Freq=x to denote the random variable ln(P\u03c4/P ) restricted to the condition \u201cFreq = x\u201d. When x is small, Equation (13) suggests that E[Y 2x ] is small, so Yx is likely to be near to 0, hence for any smooth function \u03d5 we can use \u03d5\u2032(0)Yx + \u03d5(0) to approximate \u03d5(Yx). Therefore, we can derive that E[\u03d5(Yx)] \u2248 \u03d5\u2032(0)E[Yx] + \u03d5(0), etc. Formally, this approximation can be justified under the limit x\u2192 0, as the following lemma suggests.\nLemma 1 Let \u03d5x(y) be a family of smooth functions parameterized by x. If \u03d5x satisfies\n(i) \u03d5x(0) = 0 and limx\u21920 \u03d5 \u2032 x(0) = \u03b8\n(ii) {\u03d5x(y)/y} is equicontinuous at y = 0\n(iii) {(\u03d5x(Yx)\u2212 \u03d5\u2032x(0)Yx)2/x\u03ba} is uniformly integrable\nthen we have\nlim x\u21920\nE[\u03d5x(Yx) 2]/x\u03ba = \u03b82\u03b2 and lim\nx\u21920 E[\u03d5x(Yx)]\n2/x\u03ba = 0.\nThe condition (ii) in Lemma 1 is usually easy to confirm. Condition (iii) is technical but rather mild, and we believe the distribution of Yx is tame enough for this condition to be satisfied by most \u03d5x that we consider. The proof of Lemma 1 is found in Appendix A.\nAs an example, if F (p) := p\u03b1, we set \u03d5x(y) := exp(\u03b1y)\u2212 1 and Lemma 1 implies that\nlim x\u21920 E[ ( ( P\u03c4 P )\u03b1 \u2212 1 )2 | Freq = x]/x\u03ba = \u03b12\u03b2. (15)\nThen, the second moment E|C|[F (P\u03c4 ) 2] can be calculated by integrating E[\u03d5x(Yx) 2] with respect to x, using the probability density of Freq = x given in (11). This calculation is fully exploited in Section 4.4 where we derive a condition for good choices of function F ."}, {"heading": "4.4 Effects of the Function F", "text": "We characterize the function F (p) by its asymptotic behavior at p \u2192 0, which is natural because in constructing vector representations, p is substituted by small probability values. Formally, we define an index \u03b1 associated to F to be such that\nlim p\u21920\nF \u2032(p) p\u03b1\u22121 = \u03b8 where \u03b8 is a nonzero real number,\nindicating the \u201csteepness\u201d of F at 0. For example, when F is a power function F (p) := p\u03b1 (\u03b1 6= 0), its index coincides with the power \u03b1. When F (p) := ln p, we have \u03b1 = 0. The index \u03b1 turns out to be a crucial factor in determining the properties of vector representations, as we explain below.\nWe consider the difference between F (P\u03c4 ) and F (P ). The intuition is that, when \u03b1 is smaller, F has larger slope near 0, so the differences between small P\u03c4 and P get more amplified in the expression F (P\u03c4 )\u2212 F (P ). This amplified part will dominate the behavior of F (P\u03c4 )\u2212 F (P ) if \u03b1 is small enough; and its behavior can be calculated as follows.\nWe set\n\u03d5x(y) := F (\u03c9x exp(y))\u2212 F (\u03c9x)\n(\u03c9x)\u03b1 ,\nthen, by integrating conditional expectations with respect to the probability density of Freq = x given in (11) we have\nE|C|[(F (P\u03c4 )\u2212 F (P ))2]\n= \u222b f1 f|C| E[(F (P\u03c4 )\u2212 F (P ))2 | Freq = x] Pr(x \u2264 Freq < x+ dx)\n= f|C| \u222b f1 f|C| E[(F (P\u03c4 )\u2212 F (P ))2 | Freq = x] x2 dx\n= f|C| \u222b f1 f|C| E[ (F (P\u03c4 )\u2212F (P ) Freq\u03b1 )2 | Freq = x] x2\u22122\u03b1 dx\n= \u03c92\u03b1f|C| \u222b f1 f|C| E[ (F (P\u03c4 )\u2212F (P ) P\u03b1 )2 | Freq = x] x2\u22122\u03b1 dx\n= \u03c92\u03b1f|C| \u222b f1 f|C| E[\u03d5x(Yx) 2] x2\u22122\u03b1 dx.\n(16)\nNow, the limit of \u03d5\u2032x(0) is calculated as\nlim x\u21920 \u03d5\u2032x(0) = lim x\u21920\nF \u2032(\u03c9x)\n(\u03c9x)\u03b1\u22121 = \u03b8,\nhence by Lemma 1, E[\u03d5x(Yx) 2] is asymptotically equivalent to \u03b82\u03b2x\u03ba at x \u2192 0. Thus, the integrand in (16) is asymptotically equivalent to \u03b82\u03b2x\u22122+2\u03b1+\u03ba. Therefore, if \u03b1 is small enough such that \u22122 + 2\u03b1 + \u03ba < 0, we have limx\u21920 \u03b82\u03b2x\u22122+2\u03b1+\u03ba = \u221e so this asymptotic behavior will dominate the integral (16). In other words, we expect the following\nN|C| := \u03b8 2\u03b2\u03c92\u03b1f|C| \u222b f1 f|C|\ndx\nx2\u22122\u03b1\u2212\u03ba\nto be a good approximation for (16) if \u22122 + 2\u03b1+ \u03ba < 0. Formally, we have the following lemma.\nLemma 2 Assume that \u22122 + 2\u03b1+ \u03ba \u2264 \u22121 and{( F (P\u03c4 )\u2212 F (P )\u2212 F \u2032(P )P ln(P\u03c4/P ) )2 /P 2\u03b1+\u03ba \u2223\u2223 Freq=x } is uniformly integrable. Then\nE|C|[(F (P\u03c4 )\u2212 F (P ))2] N|C| : 1 and E|C|[F (P\u03c4 )\u2212 F (P )]2 N|C| : 0,\nand the equality approximately holds for \u22122 + 2\u03b1+ \u03ba < 0.\nThe second moment E|C|[(F (P\u03c4 )\u2212F (P ))2] can be viewed as a measure of the difference between F (P\u03c4 ) and F (P ). This difference is \u201csmall\u201d compared to the second moment E|C|[F (P ) 2], as the following lemma suggests.\nLemma 3 Assume that \u03b1 \u2264 0.5 and {( F (P\u03c4 )/F (P )\u2212 1 )2\u2223\u2223 Freq=x } is uniformly integrable. Then E|C|[(F (P\u03c4 )\u2212 F (P ))2]\nE|C|[F (P )2] : 0,\nand the equality approximately holds for \u03b1 < 1.\nIn other words, if we define the vector representations as w\u03c4 := (F (P (ci|\u03c4)))i and assume that \u03b1 < 1, then in a large corpus, every w\u03c4 will be close to the vector w := (F (P (ci)))i, when one judges from the scale of \u2016w\u2016. This observation suggests that we use w as an approximation of the centroid of {wtktl} which, as we have discussed in Section 4.1, needs to be shifted to 0 by normalizing b(ci). Therefore, we set b(ci) := F (P (ci)). We find this approximation admissible in experiments (Section 5).\nWhen b(ci) = F (P (ci)), we set a(\u03c4) := E|C|[F (P\u03c4 )\u2212 F (P )] to make E|C|[S\u03c4 ] = 0, so S\u03c4 = r \u00b7 ( F (P\u03c4 )\u2212 F (P )\u2212 E|C|[F (P\u03c4 )\u2212 F (P )] ) .\nThen, if \u22122 + 2\u03b1+ \u03ba < 0, Lemma 2 implies that\nE|C|[S 2 \u03c4 ] \u2248 r2N|C|.\nSince N|C| is independent of the target \u03c4 , this equation suggests that, when vector representations are constructed as in Section 4.1 and if \u22122 + 2\u03b1 + \u03ba < 0, the norms of these vectors are almost the same. This prediction is experimentally verified in Section 5.\nTherefore, in the above case we can set\nr := 1\u221a\n|C| \u00b7N|C|\nto make the averaged norm approximately equal to 1. The following lemma is a crucial piece in proof of our bias bound and also dependent on the choice of function F . Proofs of all lemmas are found in Appendix A.\nLemma 4 Assume that \u22122 + 2\u03b1+ \u03ba \u2264 \u22121 and{( F (Ptk)\u2212 \u03c0k\\lF (Ptk\\tl)\u2212 (1\u2212 \u03c0k\\l)F (Ptktl) )2 /P 2\u03b1+\u03ba \u2223\u2223 Freq=x } is uniformly integrable. Then\nE|C|[ ( F (Ptk)\u2212 \u03c0k\\lF (Ptk\\tl)\u2212 (1\u2212 \u03c0k\\l)F (Ptktl) )2 ]\nN|C| : 0,\nand the equality approximately holds for \u22122 + 2\u03b1+ \u03ba < 0."}, {"heading": "4.5 An Upper Bound for the Bias of Additive Composition", "text": "The proof of our bias bound needs a second assumption, which is based on the intuition that, contexts of the restricted target words tk \\ tl and tl \\ tk are \u201cirrelevant\u201d, because the targets are different words (i.e. tk and tl respectively) that are not occurring together. Therefore, as random variables, we expect F (Ptk\\tl)\u2212 F (P ) and F (Ptl\\tk)\u2212 F (P ) to be independent. On the other hand, contexts of the target bigram tktl could be somehow related to that of tk \\ tl (resp. tl \\ tk), because they both co-occur with the word tk (resp. tl). Therefore, we expect F (Ptktl)\u2212 F (P ) and F (Ptk\\tl)\u2212 F (P ) (resp. F (Ptl\\tk)\u2212 F (P )) to have positive correlation. Formally, we make the following Assumption II.:\nII. F (Ptk\\tl) \u2212 F (P ) and F (Ptl\\tk) \u2212 F (P ) are independent random variables, whereas F (Ptktl)\u2212F (P ) and F (Ptk\\tl)\u2212F (P ) (resp. F (Ptl\\tk)\u2212F (P )) have positive correlation.\nThen, we can state our bias bound as Theorem 5 below.\nTheorem 5 Assume Zipf \u2019s law, Assumption I. for \u03c4 = tktl, tk \\ tl, and Assumption II. Let F be a smooth function that satisfies the following conditions:\n(i) lim p\u21920\nF \u2032(p) p\u03b1\u22121 = \u03b8 6= 0,\n(ii) {( F (P\u03c4 )\u2212 F (P )\u2212 F \u2032(P )P ln(P\u03c4/P ) )2 /P 2\u03b1+\u03ba \u2223\u2223 Freq=x } is uniformly integrable,\n(iii) {( F (Ptk)\u2212\u03c0k\\lF (Ptk\\tl)\u2212(1\u2212\u03c0k\\l)F (Ptktl) )2 /P 2\u03b1+\u03ba \u2223\u2223 Freq=x } is uniformly integrable.\nLet\nS\u03c4 = 1\u221a |C| \u00b7N|C| \u00b7 ( F (P\u03c4 )\u2212 F (P )\u2212 E|C|[F (P\u03c4 )\u2212 F (P )] ) .\nThen, if |C| \u2192 \u221e and \u22122 + 2\u03b1+ \u03ba \u2264 \u22121 we have\n|C| \u00b7 E|C|[ ( Stktl \u2212 1\n2 (Stk + Stl)\n)2 ] \u2264 1\n2 (\u03c02k\\l + \u03c0 2 l\\k + \u03c0k\\l\u03c0l\\k),\nand the inequality approximately holds for \u22122 + 2\u03b1+ \u03ba < 0.\nProof We set\nX := Stktl \u2212 12(Stk + Stl)\nX\u0303 := (\u03c0k\\l + \u03c0l\\k)Stktl \u2212 \u03c0k\\lStk\\tl \u2212 \u03c0l\\kStl\\tk \u22061 := Stk \u2212 \u03c0k\\lStk\\tl \u2212 (1\u2212 \u03c0k\\l)Stktl \u22062 := Stl \u2212 \u03c0l\\kStl\\tk \u2212 (1\u2212 \u03c0l\\k)Stktl .\nThen\nX = 1 2 X\u0303 \u2212 1 2 \u22061 \u2212 1 2 \u22062,\nhence by triangular inequality,\u221a E|C|[X2] \u2264 1\n2\n\u221a E|C|[X\u03032] + 1\n2\n\u221a E|C|[\u2206 2 1] + 1\n2\n\u221a E|C|[\u2206 2 2]. (17)\nNow, let\n\u2206\u03031 := 1\u221a |C| \u00b7N|C| \u00b7 ( F (Ptk)\u2212 \u03c0k\\lF (Ptk\\tl)\u2212 (1\u2212 \u03c0k\\l)F (Ptktl) ) \u2206\u03032 :=\n1\u221a |C| \u00b7N|C|\n\u00b7 ( F (Ptl)\u2212 \u03c0l\\kF (Ptl\\tk)\u2212 (1\u2212 \u03c0l\\k)F (Ptktl) ) .\nThen, since \u2206\u03031 \u2212\u22061 (resp. \u2206\u03032 \u2212\u22062) is a constant and E|C|[\u22061] = 0 (resp. E|C|[\u22062] = 0), we have\nE|C|[\u2206 2 1] = V|C|[\u2206\u03031] \u2264 E|C|[\u2206\u030321] E|C|[\u2206 2 2] = V|C|[\u2206\u03032] \u2264 E|C|[\u2206\u030321] . (18)\nBy Lemma 4, we have\n|C| \u00b7 E|C|[\u2206\u030321] : 0 and |C| \u00b7 E|C|[\u2206\u030322] : 0. (19)\nWe calculate E|C|[X\u0303 2] as follows.\nE|C|[X\u0303 2] = (\u03c0k\\l + \u03c0l\\k) 2E|C|[S 2 tktl ] + \u03c02k\\lE|C|[S 2 tk\\tl ] + \u03c0 2 l\\kE|C|[S 2 tl\\tk ]\n\u2212 2(\u03c0k\\l + \u03c0l\\k)\u03c0k\\lE|C|[StktlStk\\tl ] \u2212 2(\u03c0k\\l + \u03c0l\\k)\u03c0l\\kE|C|[StktlStl\\tk ] + 2\u03c0k\\l\u03c0l\\kE|C|[Stl\\tkStk\\tl ]\n.\nBy Lemma 2, we have\n|C| \u00b7 E|C|[S2tktl ] : 1, |C| \u00b7 E|C|[S 2 tk\\tl ] : 1, and |C| \u00b7 E|C|[S 2 tl\\tk ] : 1.\nBy Assumption II., we have\nE|C|[StktlStk\\tl ] \u2265 0, E|C|[StktlStl\\tk ] \u2265 0, and E|C|[Stl\\tkStk\\tl ] = 0.\nTherefore,\n|C| \u00b7 E|C|[X\u03032] \u2264 (\u03c0k\\l + \u03c0l\\k)2 + \u03c02k\\l + \u03c0 2 l\\k. (20)\nCombining (17), (18), (19) and (20), we get\n|C| \u00b7 E|C|[X2] \u2264 1\n2 (\u03c02k\\l + \u03c0 2 l\\k + \u03c0k\\l\u03c0l\\k),\nwhich is the bias bound.\nRoughly speaking, in the above proof, we have used \u03c0k\\lStk\\tl+(1\u2212\u03c0k\\l)Stktl and \u03c0l\\kStl\\tk+ (1\u2212 \u03c0l\\k)Stktl to approximate Stk and Stl , respectively. Thus, when Stk and Stl are added up, the Stktl components merge, and the independent components Stk\\tl and Stl\\tk tend to cancel out each other, making the result close to Stktl .\nExperimentally, we verify the upper bound on a real corpus, and confirm that the condition on \u03b1 (i.e. the asymptotic behavior of F (p) at p\u2192 0) is crucial for this bound to hold (Section 5.1)."}, {"heading": "5. Experimental Verification", "text": "We use the British National Corpus (BNC) (The BNC Consortium, 2007) for verifying the assumptions and implications of our theory. The corpus contains about 100 million word tokens, including written texts and utterances in British English. For constructing vector representations, we use lemmatized words annotated in the corpus, and count the co-occurrences within context windows not crossing sentence boundaries. The sizes of the context windows are 5 to each side for a target word, and 4 for a target bigram. We extract all unigrams, ordered bigrams and unordered bigrams which occur more than 200 times. This results in 16,210 unigrams, 45,793 ordered bigrams and 45,398 unordered bigrams as targets. For the lexicon C of context words, we use the same set of unigrams.\nThe co-occurrence probability P (ci|\u03c4) is estimated by\nP (ci|\u03c4) = #(ci, \u03c4)\u2211 c\u2208C #(c, \u03c4) + 1 |C| ,\nwhere #(c, \u03c4) is the event count of a target \u03c4 co-occurring with a context word c. We add an extra small number 1/|C| to the co-occurrence probability to avoid a 0 value, because P (ci|\u03c4) is fed to the function F for constructing the vector representation, where F could diverge at 0 (e.g. F (p) := ln p). With the extra term 1/|C|, the sum \u2211 ci\u2208C P (ci|\u03c4) results in 2 instead of 1, but our theory is irrelevant to this property. Such P (ci|\u03c4) is used in all experiments presented in this paper, but we have also confirmed that when F is well\ndefined at 0 (e.g. F (p) := \u221a p), calculating P (ci|\u03c4) without 1/|C| will only slightly affect the performance of vector representations.\nNow, we verify Assumption I. in Section 4.3. In order to estimate the conditional expectation E[\u00b7 | Freq = x], we use E[\u00b7 |x \u2264 Freq < x+ 500] which is calculated as expectations conditioned on such c \u2208 \u2126|C| that x \u2264 Freq(c) < x + 500. In Figure 4, we plot x ranging from 500 to 20000, and calculate the average of the following collections as y, with their standard deviations as error bars. We find empirically \u03ba \u2248 0.9 and use this setting here.\n(a) { E[ ( ln(Ptk/P ) )2 |x \u2264 Freq < x+ 500]/x0.9} for all tk (b) { E[ ( ln(Ptktl/P )\n)2 |x \u2264 Freq < x+ 500]/x0.9} for all tktl (c) { E[ (\u221a Ptk/P \u2212 1 )2 |x \u2264 Freq < x+ 500]/x0.9} for all tk\n(d) { E[ln(Ptktl/P ) |x \u2264 Freq < x+ 500]2/x0.9 } for all tktl\nThe graph (a) in Figure 4 shows that y tends to a nonzero constant \u03b2 \u2248 0.0002 at x \u2192 0, verifying (13) for target words tk. Similarly, y in graph (b) approaches almost the same value as in (a), which verifies (13) for target bigrams tktl, and justifies the use of a constant \u03b2 in (13) across all types of targets. Furthermore, we find that y approximately tends to \u03b2/4 in graph (c), which agrees with the implication (15). In graph (d), we confirm (14).\nNext, we verify the predictions in Section 4.4 that when we set w\u03c4 := (F (P (ci|\u03c4)))i and w := (F (P (ci)))i, the centroid of {wtktl} will be close to w; and if vector representations are constructed as in Section 4.1, the norms of these vectors will be almost the same at \u03b1 \u2264 0.5. Hence, in Figure 5, we plot \u03b1 = 0, . . . , 1 as x, using F (p) := ln p (\u03b1 = 0) and F (p) := x\u03b1 (\u03b1 = 0.1, . . . , 1) as delegates for the different \u03b1. Then, the blue curve shows the distance between w and the centroid of {wtktl} as y, of which the scale is normalized such that \u2016w\u2016 = 1. It verifies that w is indeed close to the centroid of {wtktl}. And the red curve shows the standard deviation of the norms {\u2016wtktl\u2016} as y, of which the scale is normalized as in Section 4.1, namely the average of {\u2016wtktl\u2016} is set to 1 after the centroid of {wtktl} is shifted to 0. This curve confirms that, as long as \u03b1 \u2264 0.5, the norms have little deviation; however, once \u03b1 gets larger, the deviation begins to explode. This is not astonishing because, as we have argued in the discussion before Lemma 2, when \u03b1 is large, high frequency context words will largely contribute to the norms of wtktl , and since the behaviors of these high frequency parts vary a lot for different tktl as demonstrated by the long error bars in Figure 4(d), the norms deviate."}, {"heading": "5.1 The Choice of Function F", "text": "In this section, we verify the bias bound (5) for bigrams observed in the BNC, and confirm the effects of the function F . We normalize a(\u03c4), b(ci), and r as in Section 4.1, and for each bigram tktl that occurs more than 200 times in the BNC, we plot\u221a\n1 2 (\u03c02k\\l + \u03c0 2 l\\k + \u03c0k\\l\u03c0l\\k) as x and \u2016wtktl \u2212 1 2 (wtk + wtl)\u2016 as y.\nThe graphs are shown in Figure 6. We demonstrate different choices of F as shown above each graph. It confirms that, if F is chosen such that \u03b1 \u2264 0.5 (i.e. F (p) := ln p in (a) and F (p) := \u221a p in (b)), then the theoretical error bound y \u2264 x (red solid lines) fits sharply to the graph. When \u03b1 gets larger, if seems that the errors are no longer bounded from above. In Section 6, we extrinsically evaluate the additive compositionality of vector representations, and find that F is also a crucial factor there; while F (p) := ln p and F (p) := \u221a p evaluate similarly well, F (p) := p and F (p) := p ln p do not. This suggests that our bias bound actually has the power of predicting additive compositionality, demonstrating the usefulness of our theory. In contrast, it seems that the averaged level of approximation errors of observed bigrams (as shown as green dashed lines in Figure 6) is less predictive, for the poor performing settings F (p) := p and F (p) := p ln p have even lower levels. This emphasizes a particular caveat that, choosing composition operations by minimizing the averaged error level, which is a widely adopted approach in practice (Dinu et al., 2013;\nBaroni and Zamparelli, 2010; Guevara, 2010), may not always be justifiable. Here if we consider the function F as a parameter in additive composition, choosing it by the average of errors on observed bigrams will only result in a worse setting. Therefore, we see how important a learning theory for the research of composition is."}, {"heading": "5.2 Handling Word Order in Additive Composition", "text": "For vector representations constructed from the Near-far Contexts (Section 2.2), we have a similar bias bound as given in 3, for approximating ordered bigrams. The formalization and proof are done simply by redefining C as the lexicon of N -F labeled context words, assuming Zipf\u2019s law for this lexicon, and in Assumption I., II. and Theorem 5 replacing tktl, tk \\ tl and tl \\ tk by tk tl, (tk \\ tl)L and (tl \\ tk)R, respectively.\nIn this section, we experimentally verify the bias bound for additive composition of Nearfar Context vectors, and qualitatively show that the composition can be used for assessing similarities between ordered bigrams. In Figure 7 and Figure 8, we plot\u221a\n1\n2\n( \u03c02 (k\\l)L + \u03c0 2 (l\\k)R + \u03c0(k\\l)L\u03c0(l\\k)R ) as x,\n\u2016wtk tl \u2212 1\n2 (wtLk + wtRl )\u2016 as y in (a), and \u2016wtk tl \u2212\n1 2 (wtRk + wtLl )\u2016 as y in (b)\nfor every ordered bigram tk tl that occurs more than 200 times in the BNC. We confirm that, the additive composition 12(wtLk + wtRl ) approximates wtk tl with errors bounded by y \u2264 x (red solid lines). On the other hand, the reverse-ordered composition 12(wtRk + wtLl\n) does not conform to this error bound, suggesting that the composition recognizes word order. In Table 2, we show the nearest 8 word pairs for each of 8 ordered bigrams, measured by cosine similarity, using the additive composition 12(vtL1 + vtR2 ) to represent the meaning of any word pair \u201ct1 t2\u201d, where vtL1 and vtR2\nare 200-dimensional SVD reductions of wtL1 and wtR2 respectively, with F (p) := \u221a p. The table shows that additive composition of Near-far Context vectors can indeed distinguish word order, for example, \u201cpose problem\u201d is near to \u201carise dilemma\u201d but not to \u201cdilemma arise\u201d, and \u201cproblem pose\u201d is near to \u201cdifficulty cause\u201d but not to \u201ccause difficulty\u201d. It is also noteworthy that \u201cnot enough\u201d is similar to \u201calways want\u201d, showing some degree of semantic similarity that is beyond the word level. We believe this ability of additive composition by Near-far Context vectors to\ncompute meanings of ordered word pairs is already highly useful, because there are only a few bigrams of which the meanings can be directly learned from the corpus."}, {"heading": "5.3 Dimension Reduction", "text": "In this section, we verify the prediction in Section 2.3 that word vectors trained by SVD can preserve the bias bound (5) in additive composition better than GloVe and SGNS. In Figure 9, we use normalized word vectors that are constructed from ordinary contexts in the BNC and reduced to 200 dimensions by different reduction methods. We use F (p) := ln p and F (p) := \u221a p in (a) and (b) respectively, both trained by SVD. The GloVe model and SGNS use F (p) := ln p, and are shown in (c) and (d) respectively. We plot\n\u221a 1\n2 (\u03c02k\\l + \u03c0 2 l\\k + \u03c0k\\l\u03c0l\\k) as x and \u2016vtktl \u2212\n1 2 (vtk + vtl)\u2016 as y.\nThe graphs show that vectors trained by SVD still largely conform to the bias bound y \u2264 x (red solid lines), but vectors trained by GloVe or SGNS no longer do. Our extrinsic evaluations in Section 6 also show that SVD might perform better than GloVe and SGNS."}, {"heading": "6. Extrinsic Evaluations of Additive Compositionality", "text": "In this section, we test additive composition on human annotated data sets to see if our theoretical predictions correlate with human judgments. We conduct a phrase similarity task and a word analogy task."}, {"heading": "6.1 Phrase Similarity", "text": "In a data set1 created by Mitchell and Lapata (2010), phrase pairs are annotated with similarity scores. Each instance in the data is a (phrase1, phrase2, similarity) triplet, and each phrase consists of two words. The similarity score is annotated by humans, ranging from 1 to 7, indicating how similar the meanings of the two phrases are. For example, one annotator assessed the similarity between \u201cvast amount\u201d and \u201clarge quantity\u201d as 7 (the highest), and the similarity between \u201chear word\u201d and \u201cremember name\u201d as 1 (the lowest). Phrases are divided into three categories: Verb-Object, Compound Noun, and Adjective-Noun. Each category has 108 phrase pairs, and they are annotated by 18 human participants (i.e., 1,944 instances in each category). Using this data set, we can compare the human ranking of phrase similarity with the one calculated from cosine similarities between vector-based compositions. We use Spearman\u2019s \u03c1 to measure how correlated the two rankings are.\nVector representations are extracted from the BNC, in the same settings as in Section 5. We show in Figure 10 the distributions of how many times the phrases in the data occur as bigrams in the corpus. The figure indicates that, a large portion of the phrases are rare or unseen as bigrams, so their meanings cannot be directly extracted as bigram vectors from the corpus. Therefore, the data is suitable for testing compositions of word vectors.\nAfter the vector representations are extracted, they are reduced to 200-dimensional and normalized. The dimension is selected by observing the top 800 singular values of the vector representations. As illustrated in Figure 11, the decreases of the singular values flatten to a constant rate at a rank of about 200. This indicates that the most characteristic features in the vector representations can be projected into 200 dimensions. In our preliminary experiments, we have confirmed that a dimension of 200 performs better than 100, 500 or no dimension reduction.\n1. http://homepages.inf.ed.ac.uk/s0453356/\nFor training word vectors, we use the random projection algorithm (Halko et al., 2011) for SVD, and Stochastic Gradient Descent (SGD) (Bottou, 2012) for SGNS and GloVe. Since these are randomized algorithms, we run each test 20 times and report the mean performance with standard deviation. We tune SGD learning rates by checking convergence of the objectives, and get slightly better results than the default training parameters set in the software of SGNS2 and GloVe3.\nAs pointed out by Levy et al. (2015), there are other detailed settings that can vary in SGNS and GloVe. We make these settings close enough to be comparable but emphasize the differences of loss functions. More precisely, we use no subsampling and set the number of negative samples to 2 in SGNS, and use the default loss function in GloVe with cutoff threshold set to 10. In addition, the implementation of SGNS and GloVe both weigh context words by a function of their distances to the targets, which we disable (i.e. equal weights are used for all context words) so as to make them compatible with the contexts that we consider in this paper.\nThe test results are shown in Table 3. We compare different settings of function F , Ordinary and Near-far Contexts, and different dimension reductions. When using ordinary contexts and SVD reduction, we find that the functions ln (F (p) := ln p) and sqrt (F (p) := \u221a p) perform similarly well, whereas id (F (p) := p) and xlnx (F (p) := p ln p) are much worse, confirming our predictions in Section 2.1. As for Near-far Context vectors (Section 2.2), we find that the Nearfar-sqrt-SVD setting has a high performance, demonstrating that Near-far Context can improve additive composition. On the other hand, Nearfar-ln-SVD is worse, for which one reason could be that, the function ln emphasizes lower frequency context words, so it combined with Near-far labels could make context words more sparse; or relevantly, some important syntactic markers (e.g. \u201cthe\u201d, \u201ca\u201d) might get obscured because they have high frequency. Finally, we find that SVD is consistently good and usually better than GloVe and SGNS, which supports our arguments in Section 2.3.\nWe report some additional test results as references. In Table 3, the \u201cTensor Product\u201d row shows the results using compositions of the Ordinary-ln-SVD word vectors by tensor products instead of averages, which means that the similarity between two phrases \u201ca1 b1\u201d and \u201ca2 b2\u201d is calculated as the product of the cosine similarities cos(a1, a2) \u00b7 cos(b1, b2).\n2. https://code.google.com/p/word2vec/ 3. http://nlp.stanford.edu/projects/glove/\nThe numbers are worse than additive composition, which suggests that a similar phrase may be more than a sequence of separately similar words. In the \u201cUpper Bound\u201d row, we show the best possible Spearman\u2019s \u03c1 for this task, which are less than 1 because there are disagreements between human annotators. Compared to these numbers, we find that the performance of additive composition on compound nouns is remarkably high. Furthermore, in \u201cMuraoka et al.\u201d, we cite the best results reported by Muraoka et al. (2014), which has experimented on many different composition methods. And in \u201cDeep Neural\u201d, we test additive compositions of word vectors trained by deep neural networks (normalized 200-dimensional vectors trained by Turian et al. 2010, using the model of Collobert et al. 2011). These results cannot be directly compared because the word vectors are learned from different corpora, but we can fairly say that additive composition is a powerful method for assessing phrase similarity, and linear dimension reduction might be more suitable for training additive compositional word vectors than deep neural networks. Therefore, our theory on additive composition is about the state-of-the-art."}, {"heading": "6.2 Word Analogy", "text": "The word analogy task is to solve questions of the form \u201ca is to b as c is to ?\u201d, and one elegant approach is to find the word vector that is most similar to vb \u2212 va + vc (Mikolov et al., 2013a). For example, in order to answer the question \u201cman is to king as woman is to ?\u201d, one needs to calculate vking \u2212 vman + vwoman and find out its most similar word vector, which may probably turns out to be vqueen, indicating the correct answer queen.\nAs pointed out by Levy and Goldberg (2014a), the key to solving analogy questions is the ability to \u201cadd\u201d (resp. \u201csubtract\u201d) some aspects to (resp. from) a concept. For example, king is a concept of human that has the aspects of being royal and male. If we can \u201csubtract\u201d the aspect male from king and \u201cadd\u201d the aspect female to it, then we could\nprobably get the concept queen. Thus, the vector-based solution proposed by Mikolov et al. (2013a) as above is essentially assuming that \u201cadding\u201d and \u201csubtracting\u201d aspects can be realized by adding and subtracting word vectors. Why is this assumption admissible?\nWe believe this assumption is closely related to additive compositionality. Because, if an aspect is represented by an adjective (e.g. male) and a concept is represented by a noun (e.g. human), we can usually \u201cadd\u201d the aspect to the concept by simply arranging the adjective and the noun to form a phrase (e.g. male human). Therefore, as the meaning of the phrase can be calculated by additive composition (e.g. vmale + vhuman), we have indeed realized the \u201caddition\u201d of aspects by addition of word vectors. In short, since semantically man \u2248 male human, king \u2248 royal male human, woman \u2248 female human and queen \u2248 royal female human, we expect the following by additive composition for phrases.\nvman \u2248 vmale + vhuman vking \u2248 vroyal + vmale + vhuman\nvwoman \u2248 vfemale + vhuman vqueen \u2248 vroyal + vfemale + vhuman\nHere, \u201c\u2248\u201d denotes proximity between vectors in the sense of cosine similarity. From these approximate equalities, we can imply that vking\u2212vman+vwoman \u2248 vroyal+vfemale+vhuman \u2248 vqueen, which solves the word analogy task.\nTherefore, we expect the word analogy task to serve as an extrinsic evaluation of additive compositionality. For this purpose, we conduct the task on the standard Msr4 (Mikolov et al., 2013a) and Google5 (Mikolov et al., 2013b) data sets. Each instance in the data is a 4-tuple of words that are subject to \u201ca is to b as c is to d\u201d, and the task is to find out d from a, b and c. Word vectors are trained from surface forms in the BNC, with other settings the same as previous. Tuples containing out-of-vocabulary words are removed from the data, which results in 4382 tuples in Msr and 8906 in Google6.\nThe test results are shown in Table 4. Again, we find that ln and sqrt perform similarly well but id and xlnx are worse, confirming that the choice of function F can drastically affect the performance on word analogy as well, which is considered related to additive compositionality. Also, we confirm that SVD can be better than SGNS and GloVe, which gives more support to the conjecture that word vectors trained by SVD may be more compatible to additive composition.\n4. http://research.microsoft.com/en-us/projects/rnn/ 5. https://code.google.com/p/word2vec/ 6. These are about half the size of the original data sets."}, {"heading": "7. Conclusion", "text": "In this paper, we have developed a theory of additive composition regarding its bias. The theory has explained why and how additive composition works, making useful suggestions about how to improve the additive compositionality, which include the choice of a transformation function, the awareness of word order, and the dimension reduction methods. Predictions made by the theory are verified empirically, and have shown positive correlations with human judgments. In short, we have revealed the mechanism of additive composition.\nHowever, we note that our theory is not \u201cproof\u201d of additive composition being a \u201cgood\u201d compositional framework. As a generalization error bound usually is in machine learning theory, our bound for the bias does not show if additive composition is \u201cgood\u201d; rather, it specifies some factors that can affect the errors. If we have generalization error bounds for other composition operations, a comparison between such bounds can bring useful insights into the choices of compositional frameworks in specific cases. We expect our bias bound to inspire more results in the research of semantic composition.\nMoreover, we believe this line of theoretical research can be pursued further. In computational linguistics, the idea of treating semantics and semantic relations by algebraic operations on distributional context vectors is relatively new (Clarke, 2012). Therefore, the relation between linguistic theories and our approximation theory of semantic composition is left largely unexplored. For example, the intuitive distinction between compositional (e.g. high price) and non-compositional (e.g. white lie) phrases is currently ignored in our theory. Our bias bound treats both cases by a single collocation measure. Can one improve the bound by taking account of this distinction, and/or other kinds of linguistic knowledge? This is an intriguing question for future work."}, {"heading": "Appendix A. Proofs of Lemmas", "text": "In this section, we prove all the lemmas in Section 4. The proofs depend on some preliminary propositions.\nProposition 6 (Chebyshev\u2019s Inequality) Let Yi (i = 1, . . . , k) be nonzero random variables, and assume that the second moments E[Y 2i ] exist. Then, for any \u03c1 > 0, we have\nPr( k\u22c3 i=1 { Y 2i \u2265 \u03c1 \u00b7 E[Y 2i ] } ) \u2264 k \u03c1 .\nProof Let 1Y 2i \u2265\u03c1\u00b7E[Y 2i ] be the random variable which equals 1 if Y 2i \u2265 \u03c1 \u00b7 E[Y 2i ] and 0 otherwise. Then, note that\n1Y 2i \u2265\u03c1\u00b7E[Y 2i ] \u2264 Y\n2 i\n\u03c1 \u00b7 E[Y 2i ] ,\nhence we have\nPr( k\u22c3 i=1 { Y 2i \u2265 \u03c1 \u00b7 E[Y 2i ] } ) \u2264 k\u2211 i=1 Pr(Y 2i \u2265 \u03c1 \u00b7 E[Y 2i ])\n= k\u2211 i=1 E[1Y 2i \u2265\u03c1\u00b7E[Y 2i ] ]\n\u2264 k\u2211 i=1 E[ Y 2i \u03c1 \u00b7 E[Y 2i ] ] = k\n\u03c1 .\nThe inequality is proven.\nWhen all E[Y 2i ] are small, Proposition 6 gives a quantitative estimation of how likely every Yi will be small. It is used in proof of the following proposition.\nProposition 7 Let Yi,x (i = 1, . . . , k) be random variables parameterized by x. Assume that, for some \u03ba > 0 given, limx\u21920E[Y 2 i,x]/x\n\u03ba exists for each i. Let \u03c8x(y) be a series of smooth functions of k variables, parameterized by x. Assume that both \u03c8x and d\u03c8x vanish at 0. Also, assume that {\u03c8x(y)2/\u2016y\u20162} is equicontinuous at 0, and {\u03c8x(Y1,x, . . . , Yk,x)2/x\u03ba} is uniformly integrable. Then, we have limx\u21920E[\u03c8x(Y1,x, . . . , Yk,x) 2]/x\u03ba = 0.\nProof For any > 0, we show that E[\u03c8x(Y1,x, . . . , Yk,x) 2]/x\u03ba < if x is sufficiently small. First, since {\u03c8x(Y1,x, . . . , Yk,x)2/x\u03ba} is uniformly integrable, there exists \u03b41 > 0 such that E[\u03c8x(Y1,x, . . . , Yk,x)\n2]/x\u03ba |H] < /2 for any condition H which satisfies Pr(H) < \u03b41. Let \u03c1 be sufficiently large such that k/\u03c1 < \u03b41. Then, let Hx be the condition that\nHx := k\u22c3 i=1 { Y 2i,x \u2265 \u03c1 \u00b7 E[Y 2i,x] } ,\nso we can imply that Pr(Hx) \u2264 k/\u03c1 < \u03b41, by Proposition 6. Hence, we have\nE[\u03c8x(Y1,x, . . . , Yk,x) 2]/x\u03ba |Hx] <\n2 . (21)\nOn the other hand, we consider the condition \u00acHx, which is Y 2i,x < \u03c1 \u00b7 E[Y 2i,x] for each i. Since E[Y 2i,x] \u2192 0, we can set \u03c1 \u00b7 E[Y 2i,x] arbitrarily small, so Y 2i,x can be arbitrarily small under this condition. We take\n(i) E[Y 2i,x] < M \u00b7 x\u03ba, where M is some constant; hence Y 2i,x < \u03c1Mx\u03ba.\nSince both \u03c8x and d\u03c8x vanish at 0, we have \u03c8x(y) 2/\u2016y\u20162 \u2192 0 at \u2016y\u2016 \u2192 0. Since {\u03c8x(y)2/\u2016y\u20162} is equicontinuous at 0, there exists a uniform \u03b42 > 0 such that,\n(ii) \u03c8x(y) 2 < 2k\u03c1M \u00b7 \u2016y\u20162 for any y which satisfies \u2016y\u20162 < \u03b42.\nBy taking x sufficiently small, we can make k\u03c1Mx\u03ba < \u03b42. Then, by (i), we have\nY 21,x + . . .+ Y 2 k,x < k\u03c1Mx \u03ba < \u03b42,\nthus, by (ii), we have\n\u03c8x(Y1,x, . . . , Yk,x) 2 < 2k\u03c1M \u00b7 (Y 21,x + . . .+ Y 2k,x) < 2 x\u03ba.\nThen, we have\nE[\u03c8x(Y1,x, . . . , Yk,x) 2]/x\u03ba | \u00acHx] <\n2 . (22)\nCombining (21) and (22), we have E[\u03c8x(Y1,x, . . . , Yk,x) 2]/x\u03ba < if x is sufficiently small.\nProposition 7 is the core part of this section. Roughly speaking, it means that if all E[Y 2i ] are small, and if \u03c8 and d\u03c8 both vanish at 0, then E[\u03c8(Y1, . . . , Yk) 2] \u2248 0.\nA.1 Proof of Lemma 1 Recall that Yx := ln(P\u03c4/P ) \u2223\u2223 Freq=x , and by Assumption I. we have\nlim x\u21920\nE[Y 2x ]/x \u03ba = \u03b2 and lim\nx\u21920 E[Yx]\n2/x\u03ba = 0.\nLet \u03c8x(y) := \u03d5x(y)\u2212 \u03d5\u2032x(0)y. Then, applying Proposition 7 to \u03c8x(Yx) we get\nlim x\u21920\nE[\u03c8x(Yx) 2]/x\u03ba = 0.\nBy definition of \u03c8x we have\nE[\u03d5x(Yx) 2] = \u03d5\u2032x(0) 2E[Y 2x ] + 2\u03d5 \u2032 x(0)E[Yx\u03c8x(Yx)] + E[\u03c8x(Yx) 2],\nand by Cauchy-Schwarz Inequality,\nlim x\u21920 \u2223\u2223E[Yx\u03c8x(Yx)]\u2223\u2223/x\u03ba \u2264 lim x\u21920 \u221a E[Y 2x ] \u00b7 E[\u03c8x(Yx)2]/x\u03ba = 0.\nTherefore,\nlim x\u21920\nE[\u03d5x(Yx) 2]/x\u03ba = lim x\u21920 \u03d5\u2032x(0) 2E[Y 2x ]/x \u03ba = \u03b82\u03b2.\nSimilarly, since\nE[\u03d5x(Yx)] = \u03d5 \u2032 x(0)E[Yx] + E[\u03c8x(Yx)]\nand\nlim x\u21920\nE[\u03c8x(Yx)] 2/x\u03ba \u2264 lim\nx\u21920 E[\u03c8x(Yx)\n2]/x\u03ba = 0,\nwe have\nlim x\u21920\nE[\u03d5x(Yx)]/x \u03ba 2 = lim x\u21920 \u03d5\u2032x(0)E[Yx]/x \u03ba 2 = 0.\nThe formulas are proven.\nA.2 Proof of Lemma 2\nWe set\n\u03d5x(y) := F (\u03c9x exp(y))\u2212 F (\u03c9x)\n(\u03c9x)\u03b1 ,\nthen\nlim x\u21920 \u03d5\u2032x(0) = lim x\u21920\nF \u2032(\u03c9x)\n(\u03c9x)\u03b1\u22121 = \u03b8,\nhence by Lemma 1 we have\nlim x\u21920\nE[\u03d5x(Yx) 2]/x\u03ba = \u03b82\u03b2 and lim\nx\u21920 E[\u03d5x(Yx)]\n2/x\u03ba = 0.\nRecall the calculation of (16), so we have\nE|C|[(F (P\u03c4 )\u2212 F (P ))2] = \u03c92\u03b1f|C| \u222b f1 f|C| E[\u03d5x(Yx) 2] x2\u22122\u03b1 dx.\nRecall that\nN|C| := \u03b8 2\u03b2\u03c92\u03b1f|C| \u222b f1 f|C|\ndx\nx2\u22122\u03b1\u2212\u03ba .\nWhen \u22122 + 2\u03b1+ \u03ba \u2264 \u22121, we have\nlim \u03b4\u21920 \u222b f1 \u03b4\ndx\nx2\u22122\u03b1\u2212\u03ba =\u221e,\nso we can apply L\u2019Ho\u0302pital\u2019s rule in the following:\nlim |C|\u2192\u221e E|C|[(F (P\u03c4 )\u2212 F (P ))2] N|C| = lim \u03b4\u21920\n\u222b f1 \u03b4 E[\u03d5x(Yx) 2] x2\u22122\u03b1 dx\n\u03b82\u03b2 \u222b f1 \u03b4\ndx\nx2\u22122\u03b1\u2212\u03ba\n= lim \u03b4\u21920\nd\nd\u03b4 \u222b f1 \u03b4 E[\u03d5x(Yx) 2] x2\u22122\u03b1 dx\n\u03b82\u03b2 d\nd\u03b4 \u222b f1 \u03b4\ndx\nx2\u22122\u03b1\u2212\u03ba\n= lim \u03b4\u21920\nE[\u03d5\u03b4(Y\u03b4) 2]\n\u03b82\u03b2\u03b4\u03ba = 1.\nA similar calculation gives E|C|[(F (P\u03c4 )\u2212 F (P ))]2 \u2264 \u03c92\u03b1f|C| \u222b f1 f|C| E[\u03d5x(Yx)] 2 x2\u22122\u03b1 dx,\nand hence E|C|[F (P\u03c4 )\u2212 F (P )]2/N|C| : 0.\nA.3 Proof of Lemma 3\nBy calculations similar to (16), we have E|C|[(F (P\u03c4 )\u2212 F (P ))2] = \u03c9f|C| \u222b f1 f|C| E[(F (\u03c9x exp(Yx))\u2212 F (\u03c9x))2] \u03c9x \u00b7 1 x dx\nand\nE|C|[F (P ) 2] = \u03c9f|C| \u222b f1 f|C| F (\u03c9x)2 \u03c9x \u00b7 1 x dx.\nAssume lim x\u21920 F (x) 6= 0, then from lim x\u21920 x0.5 = 0 we can imply\n\u2223\u2223 lim x\u21920 F (x) x0.5 \u2223\u2223 > 0.\nOtherwise if lim x\u21920\nF (x) = 0, by L\u2019Ho\u0302pital\u2019s rule and the condition \u03b1 \u2264 0.5 we have\n\u2223\u2223 lim x\u21920 F (x) x0.5 \u2223\u2223 = \u2223\u2223 lim x\u21920 F \u2032(x) 0.5x0.5\u22121 \u2223\u2223 > 0.\nTherefore, in any case\nlim x\u21920\nF (x)2\nx > 0.\nHence\nlim \u03b4\u21920 \u222b f1 \u03b4 F (\u03c9x)2 \u03c9x \u00b7 1 x dx =\u221e.\nSo we can apply L\u2019Ho\u0302pital\u2019s rule in the following:\nlim |C|\u2192\u221e E|C|[(F (P\u03c4 )\u2212 F (P ))2] E|C|[F (P )2] = lim \u03b4\u21920\nd\nd\u03b4 \u222b f1 \u03b4 E[(F (\u03c9x exp(Yx))\u2212 F (\u03c9x))2] \u03c9x \u00b7 1 x dx\nd\nd\u03b4 \u222b f1 \u03b4 F (\u03c9x)2 \u03c9x \u00b7 1 x dx\n= lim \u03b4\u21920 E[(F (\u03c9\u03b4 exp(Y\u03b4))\u2212 F (\u03c9\u03b4))2] F (\u03c9\u03b4)2\n= lim \u03b4\u21920\nE [(F (\u03c9\u03b4 exp(Y\u03b4)) F (\u03c9\u03b4) \u2212 1 )2]\n(23)\nFor any fixed y, if lim x\u21920 F (x) <\u221e and lim x\u21920 F (x) 6= 0, we have\nlim \u03b4\u21920\nF (\u03c9\u03b4 exp(y)) F (\u03c9\u03b4) \u2212 1 = F (0) F (0) \u2212 1 = 0.\nOtherwise, if lim x\u21920 F (x) =\u221e or lim x\u21920 F (x) = 0 we can apply L\u2019Ho\u0302pital\u2019s rule, so\nlim \u03b4\u21920\nF (\u03c9\u03b4 exp(y))\nF (\u03c9\u03b4) \u2212 1 = lim \u03b4\u21920\nexp(y)F \u2032(\u03c9\u03b4 exp(y))\nF \u2032(\u03c9\u03b4) \u2212 1 = exp(y)\u03b1 \u2212 1.\nIn any case, by Assumption I. we have Y\u03b4 converges to 0 in probability at \u03b4 \u2192 0, so\nF (\u03c9\u03b4 exp(Y\u03b4))\nF (\u03c9\u03b4) \u2212 1 converges to 0 in probability.\nThis convergence in probability combined with our assumption that{(F (\u03c9\u03b4 exp(Y\u03b4)) F (\u03c9\u03b4) \u2212 1 )2} is uniformly integrable\nimplies that ( F (\u03c9\u03b4 exp(Y\u03b4))/F (\u03c9\u03b4)\u2212 1 )2 converges to 0 in the L1-norm. In other words,\nlim \u03b4\u21920\nE [(F (\u03c9\u03b4 exp(Y\u03b4)) F (\u03c9\u03b4) \u2212 1 )2] = 0.\nThis combined with (23) proves the lemma.\nA.4 Proof of Lemma 4 We set Y1,x := ln(Ptk\\tl/P ) \u2223\u2223 Freq=x and Y2,x := ln(Ptktl/P ) \u2223\u2223 Freq=x . Define\n\u03c8x(y1, y2) := 1\n(\u03c9x)\u03b1\n( F ( \u03c0k\\l\u03c9x exp(y1) + (1\u2212 \u03c0k\\l)\u03c9x exp(y2) ) \u2212 \u03c0k\\lF ( \u03c9x exp(y1) ) \u2212 (1\u2212 \u03c0k\\l)F ( \u03c9x exp(y2) )) .\nThen, since\n\u03c8x(0, 0) = 0, \u2202\n\u2202y1 \u03c8x(0, 0) = 0 and\n\u2202\n\u2202y2 \u03c8x(0, 0) = 0,\nwe can apply Proposition 7 to \u03c8x(Y1,x, Y2,x). Therefore\nlim x\u21920\nE[\u03c8x(Y1,x, Y2,x) 2]\nx\u03ba = 0.\nBy a calculation similar to (16), we have\nE|C|[(F (Ptk)\u2212 \u03c0k\\lF (Ptk\\tl)\u2212 (1\u2212 \u03c0k\\l)F (Ptktl)) 2] = \u03c92\u03b1f|C| \u222b f1 f|C| E[\u03c8x(Y1,x, Y2,x) 2] x2\u22122\u03b1 dx.\nRecall that\nN|C| := \u03b8 2\u03b2\u03c92\u03b1f|C| \u222b f1 f|C|\ndx\nx2\u22122\u03b1\u2212\u03ba .\nWhen \u22122 + 2\u03b1+ \u03ba \u2264 \u22121, we have\nlim \u03b4\u21920 \u222b f1 \u03b4\ndx\nx2\u22122\u03b1\u2212\u03ba =\u221e,\nso we can apply L\u2019Ho\u0302pital\u2019s rule in the following:\nlim |C|\u2192\u221e\nE|C|[(F (Ptk)\u2212 \u03c0k\\lF (Ptk\\tl)\u2212 (1\u2212 \u03c0k\\l)F (Ptktl)) 2]\nN|C|\n= lim \u03b4\u21920\nd\nd\u03b4 \u222b f1 \u03b4 E[\u03c8x(Y1,x, Y2,x) 2] x2\u22122\u03b1 dx\n\u03b82\u03b2 d\nd\u03b4 \u222b f1 \u03b4\ndx\nx2\u22122\u03b1\u2212\u03ba\n= lim \u03b4\u21920\nE[\u03c8\u03b4(Y1,\u03b4, Y2,\u03b4) 2]\n\u03b82\u03b2\u03b4\u03ba = 0.\nThe formula is proven."}, {"heading": "Appendix B. The Loss Function of SGNS", "text": "In this section, we discuss the loss function of SGNS. The model is originally proposed as as an ad hoc objective function using the negative sampling technique (Mikolov et al., 2013b), without any explicit explanation on what is optimized and what is the loss. It is later shown that SGNS is a factorization of the shifted-PMI matrix (Levy and Goldberg, 2014b), but the loss function for this factorization remains unspecified. Here, we give a re-explanation of the SGNS model, with the loss function explicitly stated.\nB.1 Noise Contrastive Estimation\nThe original objective function of SGNS is proposed as an adaptation of the Noise Contrastive Estimation (NCE) method, but in fact SGNS is using NCE without any adaptation. NCE (Gutmann and Hyva\u0308rinen, 2012) is a method for solving the classical problem that, given a sample (xi) N i=1 (wherein xi \u2208 X ) drawn from an unknown probability distribution Pdata, and a function family f(\u00b7; \u03b8) : X \u2192 R\u22650 parameterized by \u03b8, to find the optimal \u03b8\u2217 such that f(x; \u03b8\u2217) approximates the distribution Pdata(x) best. An alternative to NCE is the Maximum Likelihood Estimation (MLE), in which \u03b8\u2217 is chosen as to maximize the log-likelihood of the sample (xi) N i=1, with respect to the constraint that f(\u00b7; \u03b8\u2217) should be a probability:\n\u03b8\u2217MLE = arg max \u03b8 N\u2211 i=1 ln f(xi; \u03b8), s.t. \u2211 x\u2208X f(x; \u03b8) = 1.\nFor MLE, the constraint \u2211\nx\u2208X f(x; \u03b8) = 1 is important, because f(x; \u03b8) can tend to arbitrarily large if we maximize the log-likelihood without the constraint. NCE finds \u03b8\u2217 in a different way. It firstly mixes (xi) with a noise sample drawn from a known distribution Pnoise, each data point xi mixed with k noise points yi,1, . . . , yi,k \u223c Pnoise. Hence\nPr(x is data | x) = Pdata(x) Pdata(x) + kPnoise(x) , (24)\nwhich calculates the probability of a given point x \u2208 X being a data point. Pdata is unknown in (24), so we approximate Pr(x is data | x) by g(x; \u03b8) as follows:\ng(x; \u03b8) := f(x; \u03b8)\nf(x; \u03b8) + kPnoise(x) . (25)\nThen, NCE maximizes the log-likelihood of \u201cxi being data and yi,1, . . . , yi,k being noise\u201d:\n\u03b8\u2217NCE = arg max \u03b8 N\u2211 i=1 ( ln g(xi; \u03b8) + k\u2211 j=1 ln(1\u2212 g(yi,j ; \u03b8)) ) . (26)\nThe most important point of NCE is that, f(x; \u03b8) will not tend to infinity even we maximize (26) without the constraint \u2211 x\u2208X f(x; \u03b8) = 1. This is because making f(x; \u03b8) large will accordingly make 1\u2212 g(yi,j ; \u03b8) small, which will decrease the likelihood of \u201cyi,1, . . . , yi,k being noise\u201d. No longer necessary to repeatedly calculate \u2211 x\u2208X f(x; \u03b8) during parameter update, NCE usually results in efficient training algorithms.\nB.2 The Skip-Gram model with Negative Sampling\nLet P (c|t) be the co-occurrence probability of a context word c and a target word t. SGNS approximates P (c|t) by the function family f(c, t; u,v) := exp(uc \u00b7vt+ln(kPnoise(c))), using NCE to optimize parameters. The training data is a collection C of co-occurring (t, c) pairs. The parameters are matrices u and v, the column vectors of which correspond to c and t, respectively. Pnoise is the known noise distribution, used here as a function of c.\nSubstituting this f(c, t; u,v) into (25), we get\ng(c, t; u,v) = exp(uc \u00b7 vt + ln(kPnoise(c)))\nexp(uc \u00b7 vt + ln(kPnoise(c))) + kPnoise(c) = \u03c3(uc \u00b7 vt),\nwhere \u03c3(x) = 1/{1 + exp(\u2212x)} is the sigmoid function. Substituting the obtained g(c, t; u,v) into (26), we get\narg max u,v \u2211 (t,c)\u2208C ( ln\u03c3(uc \u00b7 vt) + k\u2211 j=1\nnj\u223cPnoise\nln(1\u2212 \u03c3(unj \u00b7 vt)) )\n(27)\nwhich is exactly the objective function of SGNS proposed in Mikolov et al. (2013b).\nB.3 Proof of 4\nSince SGNS is using f(c, t; u,v) := exp(uc \u00b7 vt + ln(kPnoise(c))) to approximate P (c|t), it is using uc \u00b7 vt to approximate s(c, t) := lnP (c|t) \u2212 ln(kPnoise(c)). This form of s(c, t) is compatible to our general formalization of vector representations, as defined in Equation (1), Section 2. More precisely, if we set the function F (p) := ln p, the shift terms a(t) := 0 and b(c) := ln(kPnoise(c)), then the vector vt trained by SGNS is a dimension reduction of the vector representation wt that we considered in this paper.\nIn order to prove 4, we let N be the number of (t, c) pairs that are observed in the training data C. Then, we consider the 1/N multiple of the objective (27):\nO(u,v) := 1\nN \u2211 (t\u2032,c\u2032)\u2208C ( ln\u03c3(uc\u2032 \u00b7 vt\u2032) + k\u2211 j=1\nnj\u223cPnoise\nln(1\u2212 \u03c3(unj \u00b7 vt\u2032)) ) .\nThe above sum is taken across the training data C, in which the term ln\u03c3(uc \u00b7 vt) appears P (c, t) times (i.e. we have a probability P (c, t) for the pair (t\u2032, c\u2032) to be equal to (t, c)), and the term ln(1\u2212 \u03c3(uc \u00b7 vt)) appears kPnoise(c)P (t) times (i.e. we have a probability Pnoise(c) for nj = c, and a probability P (t) for t \u2032 = t). Hence,\nO(u,v) = \u2211 c,t P (t) ( P (c|t) ln\u03c3(uc \u00b7 vt) + kPnoise(c) ln(1\u2212 \u03c3(uc \u00b7 vt)) ) ,\nwhere the sum is taken across every unique combination of c and t. We know that the optimal of O(u,v) is taken at uc \u00b7 vt = s(c, t), so we set\nM := \u2211 c,t P (t) ( P (c|t) ln\u03c3(s(c, t)) + kPnoise(c) ln(1\u2212 \u03c3(s(c, t))) ) .\nThen, maximizing O(u,v) is equivalent to minimizing M\u2212O(u,v), and by some calculation, we find that M \u2212O(u,v) = \u2211 c,t P (t)D\u03c6(uc \u00b7 vt + b(c), s(c, t) + b(c)),\nwhere D\u03c6(p, q) := \u03c6(p)\u2212 \u03c6(q)\u2212 \u03c6\u2032(q)(p\u2212 q) is the Bregman divergence associated with the convex function\n\u03c6(x) = (P (c|t) + kPnoise(c)) ln(exp(x) + kPnoise(c)).\nThe limit of D\u03c6 at k \u2192 +\u221e can be easily calculated."}], "references": [{"title": "Simcompass: Using deep learning word embeddings to assess cross-level similarity", "author": ["Carmen Banea", "Di Chen", "Rada Mihalcea", "Claire Cardie", "Janyce Wiebe"], "venue": "In Proceedings of SemEval,", "citeRegEx": "Banea et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Banea et al\\.", "year": 2014}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Marco Baroni", "Roberto Zamparelli"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Baroni and Zamparelli.,? \\Q2010\\E", "shortCiteRegEx": "Baroni and Zamparelli.", "year": 2010}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["William Blacoe", "Mirella Lapata"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Blacoe and Lapata.,? \\Q2012\\E", "shortCiteRegEx": "Blacoe and Lapata.", "year": 2012}, {"title": "Probabilistic topic models", "author": ["David M. Blei"], "venue": "Commun. ACM,", "citeRegEx": "Blei.,? \\Q2012\\E", "shortCiteRegEx": "Blei.", "year": 2012}, {"title": "Stochastic gradient descent tricks", "author": ["L\u00e9on Bottou"], "venue": null, "citeRegEx": "Bottou.,? \\Q2012\\E", "shortCiteRegEx": "Bottou.", "year": 2012}, {"title": "Error bounds for approximation with neural networks", "author": ["Martin Burger", "Andreas Neubauer"], "venue": "Journal of Approximation Theory,", "citeRegEx": "Burger and Neubauer.,? \\Q2001\\E", "shortCiteRegEx": "Burger and Neubauer.", "year": 2001}, {"title": "Word association norms, mutual information, and lexicography", "author": ["Kenneth Ward Church", "Patrick Hanks"], "venue": "Comput. Linguist.,", "citeRegEx": "Church and Hanks.,? \\Q1990\\E", "shortCiteRegEx": "Church and Hanks.", "year": 1990}, {"title": "A context-theoretic framework for compositionality in distributional semantics", "author": ["Daoud Clarke"], "venue": "Comput. Linguist.,", "citeRegEx": "Clarke.,? \\Q2012\\E", "shortCiteRegEx": "Clarke.", "year": 2012}, {"title": "Power-law distributions in empirical data", "author": ["Aaron Clauset", "Cosma Rohilla Shalizi", "M.E.J. Newman"], "venue": "SIAM Rev.,", "citeRegEx": "Clauset et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Clauset et al\\.", "year": 2009}, {"title": "Mathematical foundations for a compositional distributional model of meaning", "author": ["Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark"], "venue": "Linguistic Analysis,", "citeRegEx": "Coecke et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Similarity-based estimation of word cooccurrence probabilities", "author": ["Ido Dagan", "Fernando Pereira", "Lillian Lee"], "venue": "In Proceedings of ACL,", "citeRegEx": "Dagan et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 1994}, {"title": "General estimation and evaluation of compositional distributional semantic models", "author": ["Georgiana Dinu", "Nghia The Pham", "Marco Baroni"], "venue": "In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality,", "citeRegEx": "Dinu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dinu et al\\.", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "The measurement of textual coherence with latent semantic analysis", "author": ["Peter W. Foltz", "Walter Kintsch", "Thomas K. Landauer"], "venue": "Discourse Process,", "citeRegEx": "Foltz et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Foltz et al\\.", "year": 1998}, {"title": "Neural networks and the bias/variance dilemma", "author": ["Stuart Geman", "Elie Bienenstock", "Ren\u00e9 Doursat"], "venue": "Neural Comput.,", "citeRegEx": "Geman et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Geman et al\\.", "year": 1992}, {"title": "Approximation error bounds via rademachers complexity", "author": ["Giorgio Gnecco", "Marcello Sanguineti"], "venue": "Applied Mathematical Sciences,", "citeRegEx": "Gnecco and Sanguineti.,? \\Q2008\\E", "shortCiteRegEx": "Gnecco and Sanguineti.", "year": 2008}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Grefenstette and Sadrzadeh.,? \\Q2011\\E", "shortCiteRegEx": "Grefenstette and Sadrzadeh.", "year": 2011}, {"title": "A regression model of adjective-noun compositionality in distributional semantics", "author": ["Emiliano Guevara"], "venue": "In Proceedings of the Workshop on GEometrical Models of Natural Language Semantics,", "citeRegEx": "Guevara.,? \\Q2010\\E", "shortCiteRegEx": "Guevara.", "year": 2010}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["Michael U. Gutmann", "Aapo Hyv\u00e4rinen"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Gutmann and Hyv\u00e4rinen.,? \\Q2012\\E", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen.", "year": 2012}, {"title": "Extension of zipf\u2019s law to words and phrases", "author": ["Le Quan Ha", "E.I. Sicilia-Garcia", "Ji Ming", "F.J. Smith"], "venue": "In Proceedings of Coling,", "citeRegEx": "Ha et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ha et al\\.", "year": 2002}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P.G. Martinsson", "J.A. Tropp"], "venue": "SIAM Rev.,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III"], "venue": "In Proceedings of ACL,", "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Perplexity on reduced corpora", "author": ["Hayato Kobayashi"], "venue": "In Proceedings of ACL,", "citeRegEx": "Kobayashi.,? \\Q2014\\E", "shortCiteRegEx": "Kobayashi.", "year": 2014}, {"title": "On the computational basis of learning and cognition: Arguments from LSA", "author": ["Thomas K. Landauer"], "venue": "The psychology of learning and motivation,", "citeRegEx": "Landauer.,? \\Q2002\\E", "shortCiteRegEx": "Landauer.", "year": 2002}, {"title": "A solution to platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Thomas K Landauer", "Susan T. Dutnais"], "venue": "Psychological review,", "citeRegEx": "Landauer and Dutnais.,? \\Q1997\\E", "shortCiteRegEx": "Landauer and Dutnais.", "year": 1997}, {"title": "How well can passage meaning be derived without using word order? a comparison of latent semantic analysis and humans", "author": ["Thomas K. Landauer", "Darrell Laham", "Bob Rehder", "M.E. Schreiner"], "venue": "In Proceedings of Annual Conference of the Cognitive Science Society,", "citeRegEx": "Landauer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1997}, {"title": "Word embeddings through Hellinger PCA", "author": ["R\u00e9mi Lebret", "Ronan Collobert"], "venue": "In Proceedings of EACL,", "citeRegEx": "Lebret and Collobert.,? \\Q2014\\E", "shortCiteRegEx": "Lebret and Collobert.", "year": 2014}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of CoNLL,", "citeRegEx": "Levy and Goldberg.,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "In Advances in NIPS,", "citeRegEx": "Levy and Goldberg.,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": "Trans. ACL,", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Advances in NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Vector-based models of semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata"], "venue": "In Proceedings of ACL-HLT,", "citeRegEx": "Mitchell and Lapata.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata"], "venue": "Cognitive Science,", "citeRegEx": "Mitchell and Lapata.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Beyond the zipf-mandelbrot law in quantitative linguistics", "author": ["M.A. Montemurro"], "venue": "Physica A,", "citeRegEx": "Montemurro.,? \\Q2001\\E", "shortCiteRegEx": "Montemurro.", "year": 2001}, {"title": "Finding the best model among representative compositional models", "author": ["Masayasu Muraoka", "Sonse Shimaoka", "Kazeto Yamamoto", "Yotaro Watanabe", "Naoaki Okazaki", "Kentaro Inui"], "venue": "In Proceedings of PACLIC,", "citeRegEx": "Muraoka et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Muraoka et al\\.", "year": 2014}, {"title": "Generalization bounds for function approximation from scattered noisy data", "author": ["Partha Niyogi", "Federico Girosi"], "venue": "Advances in Computational Mathematics,", "citeRegEx": "Niyogi and Girosi.,? \\Q1999\\E", "shortCiteRegEx": "Niyogi and Girosi.", "year": 1999}, {"title": "A note on the delta method", "author": ["Gary W. Oehlert"], "venue": "The American Statistician,", "citeRegEx": "Oehlert.,? \\Q1992\\E", "shortCiteRegEx": "Oehlert.", "year": 1992}, {"title": "A practical and linguisticallymotivated approach to compositional distributional semantics", "author": ["Denis Paperno", "Nghia The Pham", "Marco Baroni"], "venue": "In Proceedings of ACL,", "citeRegEx": "Paperno et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Paperno et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Autoextend: Extending word embeddings to embeddings for synsets and lexemes", "author": ["Sascha Rothe", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of ACL-IJCNLP,", "citeRegEx": "Rothe and Sch\u00fctze.,? \\Q2015\\E", "shortCiteRegEx": "Rothe and Sch\u00fctze.", "year": 2015}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Model-based word embeddings from decompositions of count matrices", "author": ["Karl Stratos", "Michael Collins", "Daniel Hsu"], "venue": "In Proceedings of ACL-IJCNLP,", "citeRegEx": "Stratos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stratos et al\\.", "year": 2015}, {"title": "A hierarchical bayesian language model based on pitman-yor processes", "author": ["Yee Whye Teh"], "venue": "In Proceedings of ACL,", "citeRegEx": "Teh.,? \\Q2006\\E", "shortCiteRegEx": "Teh.", "year": 2006}, {"title": "Logical inference on dependency-based compositional semantics", "author": ["Ran Tian", "Yusuke Miyao", "Takuya Matsuzaki"], "venue": "In Proceedings of ACL,", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of ACL,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Mining the web for synonyms: PMI-IR versus LSA on TOEFL", "author": ["Peter D. Turney"], "venue": "In Proceedings of EMCL,", "citeRegEx": "Turney.,? \\Q2001\\E", "shortCiteRegEx": "Turney.", "year": 2001}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D. Turney", "Patrick Pantel"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "Turney and Pantel.,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Estimating linear models for compositional distributional semantics", "author": ["Fabio Massimo Zanzotto", "Ioannis Korkontzelos", "Francesca Fallucchi", "Suresh Manandhar"], "venue": "In Proceedings of Coling,", "citeRegEx": "Zanzotto et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zanzotto et al\\.", "year": 2010}, {"title": "The Psychobiology of Language: An Introduction to Dynamic Philology", "author": ["George K. Zipf"], "venue": null, "citeRegEx": "Zipf.,? \\Q1935\\E", "shortCiteRegEx": "Zipf.", "year": 1935}], "referenceMentions": [{"referenceID": 14, "context": "Abstract We prove an upper bound for the bias of additive composition (Foltz et al., 1998; Landauer and Dutnais, 1997; Mitchell and Lapata, 2010), a widely used method for computing meanings of phrases by averaging the vector representations of their constituent words.", "startOffset": 70, "endOffset": 145}, {"referenceID": 25, "context": "Abstract We prove an upper bound for the bias of additive composition (Foltz et al., 1998; Landauer and Dutnais, 1997; Mitchell and Lapata, 2010), a widely used method for computing meanings of phrases by averaging the vector representations of their constituent words.", "startOffset": 70, "endOffset": 145}, {"referenceID": 34, "context": "Abstract We prove an upper bound for the bias of additive composition (Foltz et al., 1998; Landauer and Dutnais, 1997; Mitchell and Lapata, 2010), a widely used method for computing meanings of phrases by averaging the vector representations of their constituent words.", "startOffset": 70, "endOffset": 145}, {"referenceID": 15, "context": "Introduction The decomposition of generalization errors into bias and variance (Geman et al., 1992) is one of the most profound insights of learning theory.", "startOffset": 79, "endOffset": 99}, {"referenceID": 14, "context": "We prove an upper bound for the bias of a widely used compositional framework, the additive composition (Foltz et al., 1998; Landauer and Dutnais, 1997; Mitchell and Lapata, 2010).", "startOffset": 104, "endOffset": 179}, {"referenceID": 25, "context": "We prove an upper bound for the bias of a widely used compositional framework, the additive composition (Foltz et al., 1998; Landauer and Dutnais, 1997; Mitchell and Lapata, 2010).", "startOffset": 104, "endOffset": 179}, {"referenceID": 34, "context": "We prove an upper bound for the bias of a widely used compositional framework, the additive composition (Foltz et al., 1998; Landauer and Dutnais, 1997; Mitchell and Lapata, 2010).", "startOffset": 104, "endOffset": 179}, {"referenceID": 30, "context": "It has been shown that the cosine similarities of such word vectors can capture similarities between words considerably well (Levy et al., 2015), thus the next natural question is to extend the vector representations to phrases and even sentences.", "startOffset": 125, "endOffset": 144}, {"referenceID": 1, "context": "Some are complicated methods that rely on the underlying linguistic structures of phrases (Baroni and Zamparelli, 2010; Socher et al., 2012; Paperno et al., 2014), which are rationalized by linguistic intuitions (Coecke et al.", "startOffset": 90, "endOffset": 162}, {"referenceID": 42, "context": "Some are complicated methods that rely on the underlying linguistic structures of phrases (Baroni and Zamparelli, 2010; Socher et al., 2012; Paperno et al., 2014), which are rationalized by linguistic intuitions (Coecke et al.", "startOffset": 90, "endOffset": 162}, {"referenceID": 39, "context": "Some are complicated methods that rely on the underlying linguistic structures of phrases (Baroni and Zamparelli, 2010; Socher et al., 2012; Paperno et al., 2014), which are rationalized by linguistic intuitions (Coecke et al.", "startOffset": 90, "endOffset": 162}, {"referenceID": 9, "context": ", 2014), which are rationalized by linguistic intuitions (Coecke et al., 2010).", "startOffset": 57, "endOffset": 78}, {"referenceID": 34, "context": "Others have sought supports from cognitive science by comparing with human judgments (Mitchell and Lapata, 2010).", "startOffset": 85, "endOffset": 112}, {"referenceID": 14, "context": "The most widely used framework is the additive composition (Foltz et al., 1998; Landauer and Dutnais, 1997), in which the meanings of phrases are calculated by averaging the word vectors.", "startOffset": 59, "endOffset": 107}, {"referenceID": 25, "context": "The most widely used framework is the additive composition (Foltz et al., 1998; Landauer and Dutnais, 1997), in which the meanings of phrases are calculated by averaging the word vectors.", "startOffset": 59, "endOffset": 107}, {"referenceID": 12, "context": "Such theories would be important for the pursuit of better compositional frameworks, given that the empirical evaluation of compositional frameworks is already considerably complicated, due to different choices of the word vectors, the composition operations, and the methods for estimating parameters (Dinu et al., 2013).", "startOffset": 302, "endOffset": 321}, {"referenceID": 24, "context": "It has been shown that the cosine similarities of such word vectors can capture similarities between words considerably well (Levy et al., 2015), thus the next natural question is to extend the vector representations to phrases and even sentences. Based on the success of distributional hypothesis, it is expected that at least for short phrases, the meanings can still be represented by vectors constructed from surrounding contexts. However, a main obstacle here is that phrases are far more sparse than individual words. For example, in the British National Corpus (BNC) (The BNC Consortium, 2007), which contains about 100 million word tokens, there are about 16000 lemmatized words which occur more than 200 times, but only about 46000 such bigrams, far less than the 160002 possible two-word combinations. In other words, there are too few training samples for even two-word phrases. Therefore, a direct estimation of the surrounding contexts of a phrase can have large sampling error. Alternatively, Mitchell and Lapata (2010) propose to construct vector representations of phrases by combining word vectors, based on the linguistic intuition that meanings of phrases are \u201ccomposed\u201d from the meanings of their constituent words.", "startOffset": 126, "endOffset": 1034}, {"referenceID": 40, "context": ", 2013b), the GloVe model (Pennington et al., 2014), the Hellinger PCA (Lebret and Collobert, 2014) and a CCA model (Stratos et al.", "startOffset": 26, "endOffset": 51}, {"referenceID": 27, "context": ", 2014), the Hellinger PCA (Lebret and Collobert, 2014) and a CCA model (Stratos et al.", "startOffset": 27, "endOffset": 55}, {"referenceID": 10, "context": "Though there are other methods for constructing word vectors, such as neural networks (Collobert et al., 2011), we consider this formalization as general enough to cover a wide range of distributional word vectors in the literature, including some popular models such as SGNS and GloVe.", "startOffset": 86, "endOffset": 110}, {"referenceID": 6, "context": "Indeed, if we set F (p) := ln p and b(ci) := lnP (ci), then s(ci, \u03c4) becomes the Point-wise Mutual Information (PMI) function which has long been widely adopted in NLP (Church and Hanks, 1990; Dagan et al., 1994; Turney, 2001; Turney and Pantel, 2010).", "startOffset": 168, "endOffset": 251}, {"referenceID": 11, "context": "Indeed, if we set F (p) := ln p and b(ci) := lnP (ci), then s(ci, \u03c4) becomes the Point-wise Mutual Information (PMI) function which has long been widely adopted in NLP (Church and Hanks, 1990; Dagan et al., 1994; Turney, 2001; Turney and Pantel, 2010).", "startOffset": 168, "endOffset": 251}, {"referenceID": 47, "context": "Indeed, if we set F (p) := ln p and b(ci) := lnP (ci), then s(ci, \u03c4) becomes the Point-wise Mutual Information (PMI) function which has long been widely adopted in NLP (Church and Hanks, 1990; Dagan et al., 1994; Turney, 2001; Turney and Pantel, 2010).", "startOffset": 168, "endOffset": 251}, {"referenceID": 48, "context": "Indeed, if we set F (p) := ln p and b(ci) := lnP (ci), then s(ci, \u03c4) becomes the Point-wise Mutual Information (PMI) function which has long been widely adopted in NLP (Church and Hanks, 1990; Dagan et al., 1994; Turney, 2001; Turney and Pantel, 2010).", "startOffset": 168, "endOffset": 251}, {"referenceID": 40, "context": "The SGNS model is a matrix factorization of PMI (Levy and Goldberg, 2014b), and the more general form of shift terms a(t) and b(ci) are explicitly introduced by GloVe (Pennington et al., 2014).", "startOffset": 167, "endOffset": 192}, {"referenceID": 27, "context": "As for other forms of F , it has been reported that one can achieve better empirical results by setting F (p) := \u221a p instead of F (p) := p (Lebret and Collobert, 2014; Stratos et al., 2015) (see Section 2.", "startOffset": 139, "endOffset": 189}, {"referenceID": 43, "context": "As for other forms of F , it has been reported that one can achieve better empirical results by setting F (p) := \u221a p instead of F (p) := p (Lebret and Collobert, 2014; Stratos et al., 2015) (see Section 2.", "startOffset": 139, "endOffset": 189}, {"referenceID": 12, "context": "If COMP has parameters, it is a widely adopted approach to learn the parameters by minimizing this error Et1t2 for bigrams observed in a corpus (Dinu et al., 2013; Baroni and Zamparelli, 2010; Guevara, 2010).", "startOffset": 144, "endOffset": 207}, {"referenceID": 1, "context": "If COMP has parameters, it is a widely adopted approach to learn the parameters by minimizing this error Et1t2 for bigrams observed in a corpus (Dinu et al., 2013; Baroni and Zamparelli, 2010; Guevara, 2010).", "startOffset": 144, "endOffset": 207}, {"referenceID": 18, "context": "If COMP has parameters, it is a widely adopted approach to learn the parameters by minimizing this error Et1t2 for bigrams observed in a corpus (Dinu et al., 2013; Baroni and Zamparelli, 2010; Guevara, 2010).", "startOffset": 144, "endOffset": 207}, {"referenceID": 39, "context": "In Pennington et al. (2014), the authors noted that logarithm is a homomorphism from multiplication to addition, and used this property to justify F (p) := ln p for training semantically additive word vectors, based but on the unverified hypothesis that multiplications of co-occurrence probabilities have specialties in semantics.", "startOffset": 3, "endOffset": 28}, {"referenceID": 27, "context": "On the other hand, Lebret and Collobert (2014) proposed to use F (p) := \u221a p, which is motivated by the Hellinger distance between two probability distributions, and reported its being better than F (p) := p.", "startOffset": 19, "endOffset": 47}, {"referenceID": 27, "context": "On the other hand, Lebret and Collobert (2014) proposed to use F (p) := \u221a p, which is motivated by the Hellinger distance between two probability distributions, and reported its being better than F (p) := p. Stratos et al. (2015) proposed a similar but more general and better-motivated", "startOffset": 19, "endOffset": 230}, {"referenceID": 26, "context": "As the following famous example (Landauer et al., 1997) shows, meanings of sentences can differ greatly as the word order changes.", "startOffset": 32, "endOffset": 55}, {"referenceID": 27, "context": "Such word vectors have been used in Lebret and Collobert (2014), Stratos et al.", "startOffset": 36, "endOffset": 64}, {"referenceID": 27, "context": "Such word vectors have been used in Lebret and Collobert (2014), Stratos et al. (2015) and Levy et al.", "startOffset": 36, "endOffset": 87}, {"referenceID": 27, "context": "Such word vectors have been used in Lebret and Collobert (2014), Stratos et al. (2015) and Levy et al. (2015). For L2-loss, we can assume that \u2016Avt1 \u2212wt1\u2016 \u2264 \u03b51, \u2016Avt2 \u2212wt2\u2016 \u2264 \u03b52 and \u2016Avt1t2 \u2212wt1t2\u2016 \u2264 \u03b53, where \u03b51, \u03b52 and \u03b53 are minimized.", "startOffset": 36, "endOffset": 110}, {"referenceID": 40, "context": "GloVe The GloVe model (Pennington et al., 2014) is a dimension reduction of vector representations in which F (p) := ln p.", "startOffset": 22, "endOffset": 47}, {"referenceID": 13, "context": "To minimize this loss function, GloVe uses stochastic gradient descent methods such as AdaGrad (Duchi et al., 2011).", "startOffset": 95, "endOffset": 115}, {"referenceID": 19, "context": "The training of SGNS is based on the Noise Contrastive Estimation (NCE) (Gutmann and Hyv\u00e4rinen, 2012), therefore two inherited parameters can affect the loss function, namely the number k of noise samples per data point, and the distribution Pnoise(\u00b7) of noise.", "startOffset": 72, "endOffset": 101}, {"referenceID": 14, "context": "Additive composition is the classical approach to approximating the semantics of phrases and/or sentences (Foltz et al., 1998; Landauer and Dutnais, 1997).", "startOffset": 106, "endOffset": 154}, {"referenceID": 25, "context": "Additive composition is the classical approach to approximating the semantics of phrases and/or sentences (Foltz et al., 1998; Landauer and Dutnais, 1997).", "startOffset": 106, "endOffset": 154}, {"referenceID": 33, "context": "Compared to other composition operations, vector addition/average has either served as a strong baseline (Mitchell and Lapata, 2008), or remained one of the most competitive methods until recently (Banea et al.", "startOffset": 105, "endOffset": 132}, {"referenceID": 0, "context": "Compared to other composition operations, vector addition/average has either served as a strong baseline (Mitchell and Lapata, 2008), or remained one of the most competitive methods until recently (Banea et al., 2014).", "startOffset": 197, "endOffset": 217}, {"referenceID": 24, "context": "Word-order dependent syntactic effects on meaning have been considered as the most important lack in additive composition (Landauer, 2002).", "startOffset": 122, "endOffset": 138}, {"referenceID": 0, "context": "Compared to other composition operations, vector addition/average has either served as a strong baseline (Mitchell and Lapata, 2008), or remained one of the most competitive methods until recently (Banea et al., 2014). Additive composition has been successfully integrated into several NLP systems as well. For example, Tian et al. (2014) use vector additions for assessing meaning similarities between paraphrase candidates in a logic-based textual entailment recognition system (e.", "startOffset": 198, "endOffset": 339}, {"referenceID": 0, "context": "Compared to other composition operations, vector addition/average has either served as a strong baseline (Mitchell and Lapata, 2008), or remained one of the most competitive methods until recently (Banea et al., 2014). Additive composition has been successfully integrated into several NLP systems as well. For example, Tian et al. (2014) use vector additions for assessing meaning similarities between paraphrase candidates in a logic-based textual entailment recognition system (e.g. the similarity between \u201cblamed for death\u201d and \u201ccause loss of life\u201d is calculated by the cosine similarity between sums of word vectors vblame+vdeath and vcause+vloss+vlife); in Iyyer et al. (2015), the average of word vectors in a whole sentence/document is fed into a deep neural network for sentiment analysis and question answering, which achieves near state-of-the-art accuracies with minimum training time.", "startOffset": 198, "endOffset": 683}, {"referenceID": 33, "context": "number of advanced compositional frameworks have been proposed to cope with word order and/or syntactic information (Mitchell and Lapata, 2008; Zanzotto et al., 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014).", "startOffset": 116, "endOffset": 293}, {"referenceID": 49, "context": "number of advanced compositional frameworks have been proposed to cope with word order and/or syntactic information (Mitchell and Lapata, 2008; Zanzotto et al., 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014).", "startOffset": 116, "endOffset": 293}, {"referenceID": 1, "context": "number of advanced compositional frameworks have been proposed to cope with word order and/or syntactic information (Mitchell and Lapata, 2008; Zanzotto et al., 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014).", "startOffset": 116, "endOffset": 293}, {"referenceID": 9, "context": "number of advanced compositional frameworks have been proposed to cope with word order and/or syntactic information (Mitchell and Lapata, 2008; Zanzotto et al., 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014).", "startOffset": 116, "endOffset": 293}, {"referenceID": 17, "context": "number of advanced compositional frameworks have been proposed to cope with word order and/or syntactic information (Mitchell and Lapata, 2008; Zanzotto et al., 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014).", "startOffset": 116, "endOffset": 293}, {"referenceID": 42, "context": "number of advanced compositional frameworks have been proposed to cope with word order and/or syntactic information (Mitchell and Lapata, 2008; Zanzotto et al., 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014).", "startOffset": 116, "endOffset": 293}, {"referenceID": 39, "context": "number of advanced compositional frameworks have been proposed to cope with word order and/or syntactic information (Mitchell and Lapata, 2008; Zanzotto et al., 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014).", "startOffset": 116, "endOffset": 293}, {"referenceID": 33, "context": "For example, in a two-word phrase, one can first transform the two word vectors by different matrices before adding them, in which the two matrices are parameters (Mitchell and Lapata, 2008); or, regarding different syntactic roles, one can assign matrices to adjectives for modifying vectors of nouns (Baroni and Zamparelli, 2010); further, one can insert neural network layers between parents and their children in a syntactic tree (Socher et al.", "startOffset": 163, "endOffset": 190}, {"referenceID": 1, "context": "For example, in a two-word phrase, one can first transform the two word vectors by different matrices before adding them, in which the two matrices are parameters (Mitchell and Lapata, 2008); or, regarding different syntactic roles, one can assign matrices to adjectives for modifying vectors of nouns (Baroni and Zamparelli, 2010); further, one can insert neural network layers between parents and their children in a syntactic tree (Socher et al.", "startOffset": 302, "endOffset": 331}, {"referenceID": 42, "context": "For example, in a two-word phrase, one can first transform the two word vectors by different matrices before adding them, in which the two matrices are parameters (Mitchell and Lapata, 2008); or, regarding different syntactic roles, one can assign matrices to adjectives for modifying vectors of nouns (Baroni and Zamparelli, 2010); further, one can insert neural network layers between parents and their children in a syntactic tree (Socher et al., 2012).", "startOffset": 434, "endOffset": 455}, {"referenceID": 16, "context": "Error bounds in approximation schemes have been extensively studied in statistical learning theory (Vapnik, 1995; Gnecco and Sanguineti, 2008), and especially for neural networks (Niyogi and Girosi, 1999; Burger and Neubauer, 2001).", "startOffset": 99, "endOffset": 142}, {"referenceID": 37, "context": "Error bounds in approximation schemes have been extensively studied in statistical learning theory (Vapnik, 1995; Gnecco and Sanguineti, 2008), and especially for neural networks (Niyogi and Girosi, 1999; Burger and Neubauer, 2001).", "startOffset": 179, "endOffset": 231}, {"referenceID": 5, "context": "Error bounds in approximation schemes have been extensively studied in statistical learning theory (Vapnik, 1995; Gnecco and Sanguineti, 2008), and especially for neural networks (Niyogi and Girosi, 1999; Burger and Neubauer, 2001).", "startOffset": 179, "endOffset": 231}, {"referenceID": 50, "context": "Zipf\u2019s law (Zipf, 1935) is a classical observation and still finds new applications in recent results (Kobayashi, 2014).", "startOffset": 11, "endOffset": 23}, {"referenceID": 23, "context": "Zipf\u2019s law (Zipf, 1935) is a classical observation and still finds new applications in recent results (Kobayashi, 2014).", "startOffset": 102, "endOffset": 119}, {"referenceID": 44, "context": "Advanced Bayesian language models such as the hierarchical Pitman-Yor process (Teh, 2006) and the topic model (Blei, 2012) have been proposed, which we expect could further refine our theory, for example, by considering the additive compositionality of topics.", "startOffset": 78, "endOffset": 89}, {"referenceID": 3, "context": "Advanced Bayesian language models such as the hierarchical Pitman-Yor process (Teh, 2006) and the topic model (Blei, 2012) have been proposed, which we expect could further refine our theory, for example, by considering the additive compositionality of topics.", "startOffset": 110, "endOffset": 122}, {"referenceID": 1, "context": ", 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014). The ordinary approach is by introducing new parameters to represent differences in word positions or syntactic roles. For example, in a two-word phrase, one can first transform the two word vectors by different matrices before adding them, in which the two matrices are parameters (Mitchell and Lapata, 2008); or, regarding different syntactic roles, one can assign matrices to adjectives for modifying vectors of nouns (Baroni and Zamparelli, 2010); further, one can insert neural network layers between parents and their children in a syntactic tree (Socher et al., 2012). An empirical comparison of a broad range of compositional models, with an accessible introduction to the literature can be found in Blacoe and Lapata (2012). One theoretical issue of these methods, however, is that they lack learning guarantee.", "startOffset": 8, "endOffset": 868}, {"referenceID": 50, "context": "2 Zipf\u2019s Law An assumption essential to our calculation is Zipf\u2019s law (Zipf, 1935), which states that the frequency of any word is inversely proportional to its rank in the frequency table.", "startOffset": 70, "endOffset": 82}, {"referenceID": 33, "context": "We refer to Montemurro (2001), Ha et al.", "startOffset": 12, "endOffset": 30}, {"referenceID": 19, "context": "We refer to Montemurro (2001), Ha et al. (2002), and Clauset et al.", "startOffset": 31, "endOffset": 48}, {"referenceID": 8, "context": "(2002), and Clauset et al. (2009) for detailed analyses and empirical tests.", "startOffset": 12, "endOffset": 34}, {"referenceID": 38, "context": "The approach, called the \u201cdelta method\u201d (Oehlert, 1992), is as follows.", "startOffset": 40, "endOffset": 55}, {"referenceID": 33, "context": "1 Phrase Similarity In a data set1 created by Mitchell and Lapata (2010), phrase pairs are annotated with similarity scores.", "startOffset": 46, "endOffset": 73}, {"referenceID": 21, "context": "For training word vectors, we use the random projection algorithm (Halko et al., 2011) for SVD, and Stochastic Gradient Descent (SGD) (Bottou, 2012) for SGNS and GloVe.", "startOffset": 66, "endOffset": 86}, {"referenceID": 4, "context": ", 2011) for SVD, and Stochastic Gradient Descent (SGD) (Bottou, 2012) for SGNS and GloVe.", "startOffset": 55, "endOffset": 69}, {"referenceID": 4, "context": ", 2011) for SVD, and Stochastic Gradient Descent (SGD) (Bottou, 2012) for SGNS and GloVe. Since these are randomized algorithms, we run each test 20 times and report the mean performance with standard deviation. We tune SGD learning rates by checking convergence of the objectives, and get slightly better results than the default training parameters set in the software of SGNS2 and GloVe3. As pointed out by Levy et al. (2015), there are other detailed settings that can vary in SGNS and GloVe.", "startOffset": 56, "endOffset": 429}, {"referenceID": 35, "context": "Furthermore, in \u201cMuraoka et al.\u201d, we cite the best results reported by Muraoka et al. (2014), which has experimented on many different composition methods.", "startOffset": 17, "endOffset": 93}, {"referenceID": 28, "context": "As pointed out by Levy and Goldberg (2014a), the key to solving analogy questions is the ability to \u201cadd\u201d (resp.", "startOffset": 18, "endOffset": 44}, {"referenceID": 31, "context": "Thus, the vector-based solution proposed by Mikolov et al. (2013a) as above is essentially assuming that \u201cadding\u201d and \u201csubtracting\u201d aspects can be realized by adding and subtracting word vectors.", "startOffset": 44, "endOffset": 67}, {"referenceID": 7, "context": "In computational linguistics, the idea of treating semantics and semantic relations by algebraic operations on distributional context vectors is relatively new (Clarke, 2012).", "startOffset": 160, "endOffset": 174}, {"referenceID": 19, "context": "NCE (Gutmann and Hyv\u00e4rinen, 2012) is a method for solving the classical problem that, given a sample (xi) N i=1 (wherein xi \u2208 X ) drawn from an unknown probability distribution Pdata, and a function family f(\u00b7; \u03b8) : X \u2192 R\u22650 parameterized by \u03b8, to find the optimal \u03b8\u2217 such that f(x; \u03b8\u2217) approximates the distribution Pdata(x) best.", "startOffset": 4, "endOffset": 33}, {"referenceID": 31, "context": "which is exactly the objective function of SGNS proposed in Mikolov et al. (2013b).", "startOffset": 60, "endOffset": 83}], "year": 2017, "abstractText": "We prove an upper bound for the bias of additive composition (Foltz et al., 1998; Landauer and Dutnais, 1997; Mitchell and Lapata, 2010), a widely used method for computing meanings of phrases by averaging the vector representations of their constituent words. The result endorses additive composition as a reasonable operation for calculating meanings of phrases, which is the first theoretical analysis on compositional frameworks from a machine learning point of view. The theory also suggests ways to improve additive compositionality, including: transforming entries of distributional word vectors by a function that meets a specific condition, constructing a novel type of vector representations to make additive composition sensitive to word order, and utilizing singular value decomposition to train word vectors.", "creator": "TeX"}}}