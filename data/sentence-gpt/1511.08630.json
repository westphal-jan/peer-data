{"id": "1511.08630", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Nov-2015", "title": "A C-LSTM Neural Network for Text Classification", "abstract": "Neural network models have been demon- strated to be capable of achieving remarkable performance in sentence and document mod- eling. Convolutional neural network (CNN) and recurrent neural network (RNN) are two mainstream architectures for such modeling tasks, which adopt totally different ways of understanding natural languages. In this work, we combine the strengths of both architectures and propose a novel and unified model called C-LSTM for sentence representation and text classification.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Fri, 27 Nov 2015 11:44:17 GMT  (35kb)", "http://arxiv.org/abs/1511.08630v1", null], ["v2", "Mon, 30 Nov 2015 07:20:46 GMT  (35kb)", "http://arxiv.org/abs/1511.08630v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chunting zhou", "chonglin sun", "zhiyuan liu", "francis c m lau"], "accepted": false, "id": "1511.08630"}, "pdf": {"name": "1511.08630.pdf", "metadata": {"source": "CRF", "title": "A C-LSTM Neural Network for Text Classification", "authors": ["Chunting Zhou", "Chonglin Sun", "Zhiyuan Liu", "Francis C.M. Lau"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n08 63\n0v 1\n[ cs\n.C L\n] 2\n7 N"}, {"heading": "1 Introduction", "text": "As one of the core steps in NLP, sentence modeling aims at representing sentences as meaningful features for tasks such as sentiment classification. Traditional sentence modeling uses the bag-ofwords model which often suffers from the curse of dimensionality; others use composition based methods instead, e.g., an algebraic operation over semantic word vectors to produce the semantic sentence vector. However, such methods may not\nperform well due to the loss of word order information. More recent models for distributed sentence representation fall into two categories according to the form of input sentence: sequence-based models and tree-structured models. Sequence-based models construct sentence representations from word sequences by taking in account the relationship between successive words (Johnson and Zhang, 2015). Tree-structured models treat each word token as a node in a syntactic parse tree and learn sentence representations from leaves to the root in a recursive manner (Socher et al., 2013b).\nConvolutional neural networks (CNNs) and recurrent neural networks (RNNs) have emerged as two widely used architectures and are often combined with sequence-based or tree-structured models (Tai et al., 2015; Lei et al., 2015; Tang et al., 2015; Kim, 2014; Kalchbrenner et al., 2014; Mou et al., 2015).\nOwing to the capability of capturing local correlations of spatial or temporal structures, CNNs have achieved top performance in computer vision, speech recognition and NLP. For sentence modeling, CNNs perform excellently in extracting n-gram features at different positions of a sentence through convolutional filters, and can learn short and long-range relations through pooling operations. CNNs have been successfully combined with both sequence-based model (Denil et al., 2014; Kalchbrenner et al., 2014) and tree-structured model (Mou et al., 2015) in sentence modeling.\nThe other popular neural network architecture \u2013 RNN \u2013 is able to handle sequences of any length and capture long-term dependencies. To avoid the\nproblem of gradient exploding or vanishing in the standard RNN, Long Short-term Memory RNN (LSTM) (Hochreiter and Schmidhuber, 1997) and other variants (Cho et al., 2014) were designed for better remembering and memory accesses. Along with the sequence-based (Tang et al., 2015) or the tree-structured (Tai et al., 2015) models, RNNs have achieved remarkable results in sentence or document modeling.\nTo conclude, CNN is able to learn local response from temporal or spatial data but lacks the ability of learning sequential correlations; on the other hand, RNN is specilized for sequential modelling but unable to extract features in a parallel way. It has been shown that higher-level modeling of xt can help to disentangle underlying factors of variation within the input, which should then make it easier to learn temporal structure between successive time steps (Pascanu et al., 2014). For example, Sainath et al. (Sainath et al., 2015) have obtained respectable improvements in WER by learning a deep LSTM from multi-scale inputs. We explore training the LSTM model directly from sequences of higherlevel representaions while preserving the sequence order of these representaions. In this paper, we introduce a new architecture short for C-LSTM by combining CNN and LSTM to model sentences. To benefit from the advantages of both CNN and RNN, we design a simple end-to-end, unified architecture by feeding the output of a one-layer CNN into LSTM. The CNN is constructed on top of the pre-trained word vectors from massive unlabeled text data to learn higher-level representions of n-grams. Then to learn sequential correlations from higher-level suqence representations, the feature maps of CNN are organized as sequential window features to serve as the input of LSTM. In this way, instead of constructing LSTM directly from the input sentence, we first transform each sentence into successive window (n-gram) features to help disentangle factors of variations within sentences. We choose sequence-based input other than relying on the syntactic parse trees before feeding in the neural network, thus our model doesn\u2019t rely on any external language knowledge and complicated pre-processing.\nIn our experiments, we evaluate the semantic sentence representations learned from C-LSTM\nwith two tasks: sentiment classification and 6-way question classification. Our evaluations show that the C-LSTM model can achieve excellent results with several benchmarks as compared with a wide range of baseline models. We also show that the combination of CNN and LSTM outperforms individual multi-layer CNN models and RNN models, which indicates that LSTM can learn longterm dependencies from sequences of higher-level representations better than the other models."}, {"heading": "2 Related Work", "text": "Deep learning based neural network models have achieved great success in many NLP tasks, including learning distributed word, sentence and document representation (Mikolov et al., 2013b; Le and Mikolov, 2014), parsing (Socher et al., 2013a), statistical machine translation (Devlin et al., 2014), sentiment classification (Kim, 2014), etc. Learning distributed sentence representation through neural network models requires little external domain knowledge and can reach satisfactory results in related tasks like sentiment classification, text categorization.\nIn many recent sentence representation learning works, neural network models are constructed upon either the input word sequences or the transformed syntactic parse tree. Among them, convolutional neural network (CNN) and recurrent neural network (RNN) are two popular ones.\nThe capability of capturing local correlations along with extracting higher-level correlations through pooling empowers CNN to model sentences naturally from consecutive context windows. In (Collobert et al., 2011), Collobert et al. applied convolutional filters to successive windows for a given sequence to extract global features by max-pooling. As a slight variant, Kim et al. (2014) proposed a CNN architecture with multiple filters (with a varying window size) and two \u2018channels\u2019 of word vectors. To capture word relations of varying sizes, Kalchbrenner et al. (2014) proposed a dynamic k-max pooling mechanism. In a more recent work (Lei et al., 2015), Tao et al. apply tensor-based operations between words to replace linear operations on concatenated word vectors in the standard convolutional layer and explore\nthe non-linear interactions between nonconsective n-grams. Mou et al. (2015) also explores convolutional models on tree-structured sentences.\nAs a sequence model, RNN is able to deal with variable-length input sequences and discover long-term dependencies. Various variants of RNN have been proposed to better store and access memories (Hochreiter and Schmidhuber, 1997; Cho et al., 2014). With the ability of explicitly modeling time-series data, RNNs are being increasingly applied to sentence modeling. For example, Tai et al. (2015) adjusted the standard LSTM to tree-structured topologies and obtained superior results over a sequential LSTM on related tasks.\nIn this paper, we stack CNN and LSTM in a unified architecture for semantic sentence modeling. The combination of CNN and LSTM can be seen in some computer vision tasks like image caption (Xu et al., 2015) and speech recognition (Sainath et al., 2015). Most of these models use multi-layer CNNs and train CNNs and RNNs separately or throw the output of a fully connected layer of CNN into RNN as inputs. Our approach is different: we apply CNN to text data and feed consecutive window features directly to LSTM, and so our architecture enables LSTM to learn long-range dependencies from higher-order sequential features. In (Li et al., 2015), the authors suggest that sequence-based models are sufficient to capture the compositional semantics for many NLP tasks, thus in this work the CNN is directly built upon word sequences other than the syntactic parse tree. Our experiments on sentiment classification and 6-way question classification tasks clearly demonstrate the superiority of our model over single CNN or LSTM model and other related sequence-based models."}, {"heading": "3 C-LSTM Model", "text": "The architecture of the C-LSTM model is shown in Figure 1, which consists of two main components: convolutional neural network (CNN) and long shortterm memory network (LSTM). The following two subsections describe how we apply CNN to extract higher-level sequences of word features and LSTM to capture long-term dependencies over window feature sequences respectively."}, {"heading": "3.1 N-gram Feature Extraction through Convolution", "text": "The one-dimensional convolution involves a filter vector sliding over a sequence and detecting features at different positions. Let xi \u2208 Rd be the d-dimensional word vectors for the i-th word in a sentence. Let x \u2208 RL\u00d7d denote the input sentence where L is the length of the sentence. Let k be the length of the filter, and the vector m \u2208 Rk\u00d7d is a filter for the convolution operation. For each position j in the sentence, we have a window vector wj with k consecutive word vectors, denoted as:\nwj = [xj ,xj+1, \u00b7 \u00b7 \u00b7 ,xj+k\u22121] (1)\nHere, the commas represent row vector concatenation. A filter m convolves with the window vectors (k-grams) at each position in a valid way to generate a feature map c \u2208 RL\u2212k+1; each element cj of the feature map for window vector wj is produced as follows:\ncj = f(wj \u25e6m+ b), (2)\nwhere \u25e6 is element-wise multiplication, b \u2208 R is a bias term and f is a nonlinear transformation function that can be sigmoid, hyperbolic tangent, etc. In our case, we choose ReLU (Nair and Hinton, 2010) as the nonlinear function. The C-LSTM model uses multiple filters to generate multiple feature maps. For n filters with the same length, the generated n\nfeature maps can be rearranged as feature representations for each window wj ,\nW = [c1; c2; \u00b7 \u00b7 \u00b7 ; cn] (3)\nHere, semicolons represent column vector concatenation and ci is the feature map generated with the i-th filter. Each row Wj of W \u2208 R(L\u2212k+1)\u00d7n is the new feature representation generated from n filters for the window vector at position j. The new successive higher-order window representations then are fed into LSTM which is described below.\nA max-over-pooling or dynamic k-max pooling is often applied to feature maps after the convolution to select the most or the k-most important features. However, LSTM is specified for sequence input, and pooling will break such sequence organization due to the discontinuous selected features. Since we stack an LSTM neural neural network on top of the CNN, we will not apply pooling after the convolution operation."}, {"heading": "3.2 Long Short-Term Memory Networks", "text": "Recurrent neural networks (RNNs) are able to propagate historical information via a chain-like neural network architecture. While processing sequential data, it looks at the current input xt as well as the previous output of hidden state ht\u22121 at each time step. However, standard RNNs becomes unable to learn long-term dependencies as the gap between two time steps becomes large. To address this issue, LSTM was first introduced in (Hochreiter and Schmidhuber, 1997) and reemerged as a successful architecture since Ilya et al. (2014) obtained remarkable performance in statistical machine translation. Although many variants of LSTM were proposed, we adopt the standard architecture (Hochreiter and Schmidhuber, 1997) in this work.\nThe LSTM architecture has a range of repeated modules for each time step as in a standard RNN. At each time step, the output of the module is controlled by a set of gates in Rd as a function of the old hidden state ht\u22121 and the input at the current time step xt: the forget gate ft, the input gate it, and the output gate ot. These gates collectively decide how to update the current memory cell ct and the current hidden state ht. We use d to denote the memory dimension in the LSTM and all vectors in this\narchitecture share the same dimension. The LSTM transition functions are defined as follows:\nit = \u03c3(Wi \u00b7 [ht\u22121, xt] + bi) (4)\nft = \u03c3(Wf \u00b7 [ht\u22121, xt] + bf )\nqt = tanh(Wq \u00b7 [ht\u22121, xt] + bq)\not = \u03c3(Wo \u00b7 [ht\u22121, xt] + bo)\nct = ft \u2299 ct\u22121 + it \u2299 qt\nht = ot \u2299 tanh(ct)\nHere, \u03c3 is the logistic sigmoid function that has an output in [0, 1], tanh denotes the hyperbolic tangent function that has an output in [\u22121, 1], and \u2299 denotes the elementwise multiplication. To understand the mechanism behind the architecture, we can view ft as the function to control to what extent the information from the old memory cell is going to be thrown away, it to control how much new information is going to be stored in the current memory cell, and ot to control what to output based on the memory cell ct. LSTM is explicitly designed for time-series data for learning long-term dependencies, and therefore we choose LSTM upon the convolution layer to learn such dependencies in the sequence of higher-level features."}, {"heading": "4 Learning C-LSTM for Text Classification", "text": "For text classification, we regard the output of the hidden state at the last time step of LSTM as the document representation and we add a softmax layer on top. We train the entire model by minimizing the cross-entropy error. Given a training sample x(i) and its true label y(i) \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , k} where k is the number of possible labels and the estimated probabilities y\u0303(i)j \u2208 [0, 1] for each label j \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , k}, the error is defined as:\nL(x(i), y(i)) = k\u2211\nj=1\n1{y(i) = j} log(y\u0303 (i) j ), (5)\nwhere 1{condition} is an indicator such that 1{condition is true} = 1 otherwise 1{condition is false} = 0. We employ stochastic gradient descent (SGD) to learn the model parameters and adopt the optimizer RMSprop (Tieleman and Hinton, 2012)."}, {"heading": "4.1 Padding and Word Vector Initialization", "text": "First, we use maxlen to denote the maximum length of the sentence in the training set. As the convolution layer in our model requires fixed-length input, we pad each sentence that has a length less than maxlen with special symbols at the end that indicate the unknown words. For a sentence in the test dataset, we pad sentences that are shorter than maxlen in the same way, but for sentences that have a length longer than maxlen, we simply cut extra words at the end of these sentences to reach maxlen.\nWe initialize word vectors with the publicly available word2vec vectors1 that are pre-trained using about 100B words from the Google News Dataset. The dimensionality of the word vectors is 300. We also initialize the word vector for the unknown words from the uniform distribution [-0.25, 0.25]. We then fine-tune the word vectors along with other model parameters during training."}, {"heading": "4.2 Regularization", "text": "For regularization, we employ two commonly used techniques: dropout (Hinton et al., 2012) and L2 weight regularization. We apply dropout to prevent co-adaptation. In our model, we either apply dropout to word vectors before feeding the sequence of words into the convolutional layer or to the output of LSTM before the softmax layer. The L2 regularization is applied to the weight of the softmax layer."}, {"heading": "5 Experiments", "text": "We evaluate the C-LSTM model on two tasks: (1) sentiment classification, and (2) question type classification. In this section, we introduce the datasets and the experimental settings."}, {"heading": "5.1 Datasets", "text": "Sentiment Classification: Our task in this regard is to predict the sentiment polarity of movie reviews. We use the Stanford Sentiment Treebank (SST) benchmark (Socher et al., 2013b). This dataset consists of 11855 movie reviews and are split into train (8544), dev (1101), and test (2210). Sentences in this corpus are parsed and all phrases along with the sentences are fully annotated with\n1http://code.google.com/p/word2vec/\n5 labels: very positive, positive, neural, negative, very negative. We consider two classification tasks on this dataset: fine-grained classification with 5 labels and binary classification by removing neural labels. For the binary classification, the dataset has a split of train (6920) / dev (872) / test (1821). Since the data is provided in the format of sub-sentences, we train the model on both phrases and sentences but only test on the sentences as in several previous works (Socher et al., 2013b; Kalchbrenner et al., 2014). Question type classification: Question classification is an important step in a question answering system that classifies a question into a specific type, e.g. \u201cwhat is the highest waterfall in the United States?\u201d is a question that belongs to \u201clocation\u201d. For this task, we use the benchmark TREC (Li and Roth, 2002). TREC divides all questions into 6 categories, including location, human, entity, abbreviation, description and numeric. The training dataset contains 5452 labelled questions while the testing dataset contains 500 questions."}, {"heading": "5.2 Experimental Settings", "text": "We implement our model based on Theano (Bastien et al., 2012) \u2013 a python library, which supports efficient symbolic differentiation and transparent use of a GPU. To benefit from the efficiency of parallel computation of the tensors, we train the model on a GPU. For text preprocessing, we only convert all characters in the dataset to lower case.\nFor SST, we conduct hyperparameter (number of filters, filter length in CNN; memory dimension in LSTM; dropout rate and which layer to apply, etc.) tuning on the validation data in the standard split. For TREC, we hold out 1000 samples from the training dataset for hyperparameter search and train the model using the remaining data.\nIn our final settings, we only use one convolutional layer and one LSTM layer for both tasks. For the filter size, we investigated filter lengths of 2, 3 and 4 in two cases: a) single convolutional layer with the same filter length, and b) multiple convolutional layers with different lengths of filters in parallel. Here we denote the number of filters of length i by ni for ease of clarification. For the first case, each n-gram window is transformed into ni convoluted\nfeatures after convolution and the sequence of window representations is fed into LSTM. For the latter case, since the number of windows generated from each convolution layer varies when the filter length varies (see L\u2212k+1 below equation (3)), we cut the window sequence at the end based on the maximum filter length that gives the shortest number of windows. Each window is represented as the concatenation of outputs from different convolutional layers. We also exploit different combinations of different filter lengths. We further present experimental analysis of the exploration on filter size later. According to the experiments, we choose a single convolutional layer with filter length 3.\nFor SST, the number of filters of length 3 is set to be 150 and the memory dimension of LSTM is set to be 150, too. The word vector layer and the LSTM layer are dropped out with a probability of 0.5. For TREC, the number of filters is set to be 300 and the memory dimension is set to be 300. The word vector layer and the LSTM layer are dropped out with a probability of 0.5. We also add L2 regularization with a factor of 0.001 to the weights in the softmax layer for both tasks."}, {"heading": "6 Results and Model Analysis", "text": "In this section, we show our evaluation results on sentiment classification and question type classification tasks. Moreover, we give some model analysis on the filter size configuration."}, {"heading": "6.1 Sentiment Classification", "text": "The results are shown in Table 1. We compare our model with a large set of well-performed models on the Stanford Sentiment Treebank.\nGenerally, the baseline models consist of recursive models, convolutional neural network models, LSTM related models and others. The recursive models employ a syntactic parse tree as the sentence structure and the sentence representation is computed recursively in a bottom-up manner along the parse tree. Under this category, we choose recursive autoencoder (RAE), matrix-vector (MV-RNN), tensor based composition (RNTN) and multi-layer stacked (DRNN) recursive neural network as baselines. Among CNNs, we compare with Kim\u2019s (2014) CNN model with fine-tuned word vectors (CNN-non-static) and multi-channels (CNNmultichannel), DCNN with dynamic k-max pool-\ning, Tao\u2019s CNN (Molding-CNN) with low-rank tensor based non-linear and non-consecutive convolutions. Among LSTM related models, we first compare with two tree-structured LSTM models (Dependence Tree-LSTM and Constituency TreeLSTM) that adjust LSTM to tree-structured network topologies. Then we implement one-layer LSTM and Bi-LSTM by ourselves. Since we could not tune the result of Bi-LSTM to be as good as what has been reported in (Tai et al., 2015) even if following their untied weight configuration, we report our own results. For other baseline methods, we compare against SVM with unigram and bigram features, NBoW with average word vector features and paragraph vector that infers the new paragraph vector for unseen documents.\nTo the best of our knowledge, we achieve the fourth best published result for the 5-class classification task on this dataset. For the binary classification task, we achieve comparable results with respect to the state-of-the-art ones. From Table 1, we have the following observations: (1) Although we did not beat the state-of-the-art ones, as an endto-end model, the result is still promising and comparable with thoes models that heavily rely on linguistic annotations and knowledge, especially syntactic parse trees. This indicates C-LSTM will be more feasible for various scenarios. (2) Comparing our results against single CNN and LSTM models shows that LSTM does learn long-term dependencies across sequences of higher-level representations better. We could explore in the future how to learn more compact higher-level representations by replacing standard convolution with other non-\nlinear feature mapping functions or appealing to tree-structured topologies before the convolutional layer."}, {"heading": "6.2 Question Type Classification", "text": "The prediction accuracy on TREC question classification is reported in Table 2. We compare our model with a variety of models. The SVM classifier uses unigrams, bigrams, wh-word, head word, POS tags, parser, hypernyms, WordNet synsets as engineered features and 60 hand-coded rules. Ada-CNN is a self-adaptiive hierarchical sentence model with gating networks. Other baseline models have been introduced in the last task. From Table 2, we have the following observations: (1) Our result consistently outperforms all published neural baseline models, which means that C-LSTM captures intentions of TREC questions well. (2) Our result is close to that of the state-of-the-art SVM that depends on highly engineered features. Such engineered features not only demands human laboring but also leads to the error propagation in the existing NLP tools, thus couldn\u2019t generalize well in other datasets and tasks. With the ability of automatically learning semantic sentence representations, C-LSTM doesn\u2019t require any human-designed features and has a better scalibility."}, {"heading": "6.3 Model Analysis", "text": "Here we investigate the impact of different filter configurations in the convolutional layer on the model performance.\nIn the convolutional layer of our model, filters are used to capture local n-gram features. Intuitively, multiple convolutional layers in parallel with differ-\nent filter sizes should perform better than single convolutional layers with the same length filters in that different filter sizes could exploit features of different n-grams. However, we found in our experiments that single convolutional layer with filter length 3 always outperforms the other cases.\nWe show in Figure 2 the prediction accuracies on the 6-way question classification task using different filter configurations. Note that we also observe the similar phenomenon in the sentiment classification task. For each filter configuration, we report in Figure 2 the best result under extensive grid-search on hyperparameters. It it shown that single convolutional layer with filter length 3 performs best among all filter configurations. For the case of multiple convolutional layers in parallel, it is shown that filter configurations with filter length 3 performs better that those without tri-gram filters, which further confirms that tri-gram features do play a significant role in capturing local features in our tasks. We conjecture that LSTM could learn better semantic sentence representations from sequences of tri-gram features."}, {"heading": "7 Conclusion and Future Work", "text": "We have described a novel, unified model called CLSTM that combines convolutional neural network with long short-term memory network (LSTM). CLSTM is able to learn phrase-level features through\na convolutional layer; sequences of such higherlevel representations are then fed into the LSTM to learn long-term dependencies. We evaluated the learned semantic sentence representations on sentiment classification and question type classification tasks with very satisfactory results.\nWe could explore in the future ways to replace the standard convolution with tensor-based operations or tree-structured convolutions. We believe LSTM will benefit from more structured higher-level representations."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Modelling, visualising and summarising documents with a single convolutional neural network. arXiv preprint arXiv:1406.3830", "author": ["Denil et al.2014] Misha Denil", "Alban Demiraj", "Nal Kalchbrenner", "Phil Blunsom", "Nando de Freitas"], "venue": null, "citeRegEx": "Denil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2014}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin et al.2014] Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul"], "venue": null, "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. The Computing Research Repository (CoRR)", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Irsoy", "Cardie2014] Ozan Irsoy", "Claire Cardie"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Irsoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Irsoy et al\\.", "year": 2014}, {"title": "Effective use of word order for text categorization with convolutional neural networks. Human Language Technologies: The 2015", "author": ["Johnson", "Zhang2015] Rie Johnson", "Tong Zhang"], "venue": "Annual Conference of the North American Chapter of the ACL,", "citeRegEx": "Johnson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences. Association for Computational Linguistics (ACL)", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of Empirical Methods on Natural Language Processing", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc Le", "Tomas Mikolov"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Molding cnns for text: non-linear, non-consecutive convolutions", "author": ["Lei et al.2015] Tao Lei", "Regina Barzilay", "Tommi Jaakkola"], "venue": "In Proceedings of Empirical Methods on Natural Language Processing", "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "Learning question classifiers", "author": ["Li", "Roth2002] Xin Li", "Dan Roth"], "venue": "In Proceedings of the 19th international conference on Computational linguisticsVolume", "citeRegEx": "Li et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Li et al\\.", "year": 2002}, {"title": "When are tree structures necessary for deep learning of representations", "author": ["Li et al.2015] Jiwei Li", "Dan Jurafsky", "Eudard Hovy"], "venue": "In Proceedings of Empirical Methods on Natural Language Processing", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Discriminative neural sentence modeling by tree-based convolution", "author": ["Mou et al.2015] Lili Mou", "Hao Peng", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin"], "venue": "Unpublished manuscript: http://arxiv. org/abs/1504. 01106v5. Version,", "citeRegEx": "Mou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Hinton2010] Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["Oriol Vinyals", "Andrew Senior", "Hasim Sak"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing", "citeRegEx": "Sainath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2015}, {"title": "From symbolic to sub-symbolic information in question classification", "author": ["Silva et al.2011] Joao Silva", "Lu\u0131\u0301sa Coheur", "Ana Cristina Mendes", "Andreas Wichert"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Silva et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrixvector spaces", "author": ["Brody Huval", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of Empirical Methods on Natural Language Processing,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Parsing with compositional vector grammars", "author": ["John Bauer", "Christopher D Manning", "Andrew Y Ng"], "venue": "Proceedings of the ACL conference. Citeseer", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks. Association for Computational Linguistics (ACL)", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Tang et al.2015] Duyu Tang", "Bing Qin", "Ting Liu"], "venue": "In Proceedings of Empirical Methods on Natural Language Processing", "citeRegEx": "Tang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Lecture 6.5 - rmsprop, coursera: Neural networks for machine learning", "author": ["Tieleman", "Hinton2012] T. Tieleman", "G Hinton"], "venue": null, "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "In Proceedings of 2015th International Conference on Ma-", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Self-adaptive hierarchical sentence", "author": ["Zhao et al.2015] Han Zhao", "Zhengdong Lu", "Pascal Poupart"], "venue": null, "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 24, "context": "Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have emerged as two widely used architectures and are often combined with sequence-based or tree-structured models (Tai et al., 2015; Lei et al., 2015; Tang et al., 2015; Kim, 2014; Kalchbrenner et al., 2014; Mou et al., 2015).", "startOffset": 189, "endOffset": 300}, {"referenceID": 12, "context": "Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have emerged as two widely used architectures and are often combined with sequence-based or tree-structured models (Tai et al., 2015; Lei et al., 2015; Tang et al., 2015; Kim, 2014; Kalchbrenner et al., 2014; Mou et al., 2015).", "startOffset": 189, "endOffset": 300}, {"referenceID": 25, "context": "Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have emerged as two widely used architectures and are often combined with sequence-based or tree-structured models (Tai et al., 2015; Lei et al., 2015; Tang et al., 2015; Kim, 2014; Kalchbrenner et al., 2014; Mou et al., 2015).", "startOffset": 189, "endOffset": 300}, {"referenceID": 10, "context": "Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have emerged as two widely used architectures and are often combined with sequence-based or tree-structured models (Tai et al., 2015; Lei et al., 2015; Tang et al., 2015; Kim, 2014; Kalchbrenner et al., 2014; Mou et al., 2015).", "startOffset": 189, "endOffset": 300}, {"referenceID": 9, "context": "Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have emerged as two widely used architectures and are often combined with sequence-based or tree-structured models (Tai et al., 2015; Lei et al., 2015; Tang et al., 2015; Kim, 2014; Kalchbrenner et al., 2014; Mou et al., 2015).", "startOffset": 189, "endOffset": 300}, {"referenceID": 16, "context": "Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have emerged as two widely used architectures and are often combined with sequence-based or tree-structured models (Tai et al., 2015; Lei et al., 2015; Tang et al., 2015; Kim, 2014; Kalchbrenner et al., 2014; Mou et al., 2015).", "startOffset": 189, "endOffset": 300}, {"referenceID": 3, "context": "CNNs have been successfully combined with both sequence-based model (Denil et al., 2014; Kalchbrenner et al., 2014) and tree-structured", "startOffset": 68, "endOffset": 115}, {"referenceID": 9, "context": "CNNs have been successfully combined with both sequence-based model (Denil et al., 2014; Kalchbrenner et al., 2014) and tree-structured", "startOffset": 68, "endOffset": 115}, {"referenceID": 16, "context": "model (Mou et al., 2015) in sentence modeling.", "startOffset": 6, "endOffset": 24}, {"referenceID": 1, "context": "problem of gradient exploding or vanishing in the standard RNN, Long Short-term Memory RNN (LSTM) (Hochreiter and Schmidhuber, 1997) and other variants (Cho et al., 2014) were designed for better remembering and memory accesses.", "startOffset": 152, "endOffset": 170}, {"referenceID": 25, "context": "Along with the sequence-based (Tang et al., 2015) or the tree-structured (Tai et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 24, "context": ", 2015) or the tree-structured (Tai et al., 2015) models, RNNs have achieved remarkable results in sentence or document modeling.", "startOffset": 31, "endOffset": 49}, {"referenceID": 18, "context": "(Sainath et al., 2015) have obtained respectable improvements in WER by learning a deep LSTM from multi-scale inputs.", "startOffset": 0, "endOffset": 22}, {"referenceID": 4, "context": ", 2013a), statistical machine translation (Devlin et al., 2014), sentiment classification (Kim, 2014), etc.", "startOffset": 42, "endOffset": 63}, {"referenceID": 10, "context": ", 2014), sentiment classification (Kim, 2014), etc.", "startOffset": 34, "endOffset": 45}, {"referenceID": 2, "context": "In (Collobert et al., 2011), Collobert et al.", "startOffset": 3, "endOffset": 27}, {"referenceID": 10, "context": "As a slight variant, Kim et al. (2014)", "startOffset": 21, "endOffset": 39}, {"referenceID": 12, "context": "In a more recent work (Lei et al., 2015), Tao et al.", "startOffset": 22, "endOffset": 40}, {"referenceID": 9, "context": "To capture word relations of varying sizes, Kalchbrenner et al. (2014) proposed a dynamic k-max pooling mechanism.", "startOffset": 44, "endOffset": 71}, {"referenceID": 1, "context": "Various variants of RNN have been proposed to better store and access memories (Hochreiter and Schmidhuber, 1997; Cho et al., 2014).", "startOffset": 79, "endOffset": 131}, {"referenceID": 27, "context": "The combination of CNN and LSTM can be seen in some computer vision tasks like image caption (Xu et al., 2015) and speech recognition (Sainath et al.", "startOffset": 93, "endOffset": 110}, {"referenceID": 18, "context": ", 2015) and speech recognition (Sainath et al., 2015).", "startOffset": 31, "endOffset": 53}, {"referenceID": 14, "context": "In (Li et al., 2015), the authors suggest that sequence-based models are sufficient to capture the compositional semantics for many NLP tasks, thus in this work the CNN is directly built upon word sequences other than the syntactic parse tree.", "startOffset": 3, "endOffset": 20}, {"referenceID": 13, "context": "Mou et al. (2015) also explores convolutional models on tree-structured sentences.", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": "Various variants of RNN have been proposed to better store and access memories (Hochreiter and Schmidhuber, 1997; Cho et al., 2014). With the ability of explicitly modeling time-series data, RNNs are being increasingly applied to sentence modeling. For example, Tai et al. (2015) adjusted the standard LSTM to tree-structured topologies and obtained superior results over a sequential LSTM on related tasks.", "startOffset": 114, "endOffset": 280}, {"referenceID": 5, "context": "For regularization, we employ two commonly used techniques: dropout (Hinton et al., 2012) and L2 weight regularization.", "startOffset": 68, "endOffset": 89}, {"referenceID": 20, "context": "We use the Stanford Sentiment Treebank (SST) benchmark (Socher et al., 2013b). This dataset consists of 11855 movie reviews and are split into train (8544), dev (1101), and test (2210).", "startOffset": 56, "endOffset": 168}, {"referenceID": 20, "context": "We use the Stanford Sentiment Treebank (SST) benchmark (Socher et al., 2013b). This dataset consists of 11855 movie reviews and are split into train (8544), dev (1101), and test (2210). Sentences in this corpus are parsed and all phrases along with the sentences are fully annotated with", "startOffset": 56, "endOffset": 185}, {"referenceID": 9, "context": "Since the data is provided in the format of sub-sentences, we train the model on both phrases and sentences but only test on the sentences as in several previous works (Socher et al., 2013b; Kalchbrenner et al., 2014).", "startOffset": 168, "endOffset": 217}, {"referenceID": 0, "context": "We implement our model based on Theano (Bastien et al., 2012) \u2013 a python library, which supports efficient symbolic differentiation and transparent use of a GPU.", "startOffset": 39, "endOffset": 61}, {"referenceID": 9, "context": "5 (Kalchbrenner et al., 2014) Paragraph Vector 48.", "startOffset": 2, "endOffset": 29}, {"referenceID": 20, "context": "9 (Socher et al., 2012) RNTN 45.", "startOffset": 2, "endOffset": 23}, {"referenceID": 10, "context": "2 (Kim, 2014) CNN-multichannel 47.", "startOffset": 2, "endOffset": 13}, {"referenceID": 10, "context": "1 (Kim, 2014) DCNN 48.", "startOffset": 2, "endOffset": 13}, {"referenceID": 9, "context": "8 (Kalchbrenner et al., 2014) Molding-CNN 51.", "startOffset": 2, "endOffset": 29}, {"referenceID": 12, "context": "6 (Lei et al., 2015)", "startOffset": 2, "endOffset": 20}, {"referenceID": 24, "context": "7 (Tai et al., 2015) Constituency Tree-LSTM 51.", "startOffset": 2, "endOffset": 20}, {"referenceID": 24, "context": "0 (Tai et al., 2015) LSTM 46.", "startOffset": 2, "endOffset": 20}, {"referenceID": 10, "context": "Among CNNs, we compare with Kim\u2019s (2014) CNN model with fine-tuned word vectors (CNN-non-static) and multi-channels (CNNmultichannel), DCNN with dynamic k-max pool-", "startOffset": 28, "endOffset": 41}, {"referenceID": 10, "context": "6 Kim (2014)", "startOffset": 2, "endOffset": 13}, {"referenceID": 10, "context": "2 Kim (2014)", "startOffset": 2, "endOffset": 13}, {"referenceID": 9, "context": "0 Kalchbrenner et al. (2014)", "startOffset": 2, "endOffset": 29}, {"referenceID": 24, "context": "Since we could not tune the result of Bi-LSTM to be as good as what has been reported in (Tai et al., 2015) even if following their untied weight configuration, we report our own results.", "startOffset": 89, "endOffset": 107}], "year": 2015, "abstractText": "Neural network models have been demonstrated to be capable of achieving remarkable performance in sentence and document modeling. Convolutional neural network (CNN) and recurrent neural network (RNN) are two mainstream architectures for such modeling tasks, which adopt totally different ways of understanding natural languages. In this work, we combine the strengths of both architectures and propose a novel and unified model called C-LSTM for sentence representation and text classification. C-LSTM utilizes CNN to extract a sequence of higher-level phrase representations, and are fed into a long short-term memory recurrent neural network (LSTM) to obtain the sentence representation. C-LSTM is able to capture both local features of phrases as well as global and temporal sentence semantics. We evaluate the proposed architecture on sentiment classification and question classification tasks. The experimental results show that the C-LSTM outperforms both CNN and LSTM and can achieve excellent performance on these tasks.", "creator": "LaTeX with hyperref package"}}}