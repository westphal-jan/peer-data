{"id": "1511.08967", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2015", "title": "Robotic Search & Rescue via Online Multi-task Reinforcement Learning", "abstract": "Reinforcement learning (RL) is a general and well-known method that a robot can use to learn an optimal control policy to solve a particular task. We would like to build a versatile robot that can learn multiple tasks, but using RL for each of them would be prohibitively expensive in terms of both time and wear-and-tear on the robot. To remedy this problem, we use the Policy Gradient Efficient Lifelong Learning Algorithm (PG-ELLA), an online multi-task RL algorithm that enables the robot to efficiently learn multiple consecutive tasks by sharing knowledge between these tasks to accelerate learning and improve performance.\n\n\n\n\n\nThe PG-ELLA has been used to provide both a high-speed learning experience for the robot, as well as a training model that uses the PEMI-VOC and a model to evaluate the reliability and effectiveness of the particular robot. We will be working with a range of key suppliers for both the PG-ELLA and the PEMI-VOC for further review.\n\nWe will be focusing on the PEMI-VOC, based on the PEMI-VOC, as a low-cost, easy-to-learn, efficient, non-trivial learning algorithm, as well as evaluating the safety of both models.\nWe will use the PEMI-VOC as a general model for the PEMI-VOC to evaluate the reliability and effectiveness of the particular robot.\n\nIn this blog post, I will demonstrate the concept of RL learning using the PEMI-VOC, as shown below:\nThe PEMI-VOC is a combination of a PEMI-VOC-Sensitive Randomized Randomization Model and a PEMI-VOC-Sensitive Randomization Model.\nWe can learn more about the PEMI-VOC for more information about the PEMI-VOC, and use it for further information about the PEMI-VOC and PEMI-VOC for more information on the PEMI-VOC.\nThe PEMI-VOC is a simple (optional) algorithm for learning a specific task by using the PEMI-VOC.\nTo learn more about the PEMI-VOC for more information about the PEMI-VOC, we will refer to the PEMI-VOC as a general model for the PEMI-VOC, as", "histories": [["v1", "Sun, 29 Nov 2015 04:33:51 GMT  (1976kb,D)", "http://arxiv.org/abs/1511.08967v1", "7 pages"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.RO", "authors": ["lisa lee"], "accepted": false, "id": "1511.08967"}, "pdf": {"name": "1511.08967.pdf", "metadata": {"source": "META", "title": "Robotic Search & Rescue via Online Multi-task Reinforcement Learning", "authors": ["Lisa Lee", "Eric Eaton", "Haitham Bou Ammar", "Christopher Clingerman"], "emails": ["LILEE@PRINCETON.EDU", "EEATON@SEAS.UPENN.EDU"], "sections": [{"heading": "1. Introduction", "text": "In the robotic search & rescue problem (S&R), we have a robot whose task is to search and rescue hidden target objects in the environment. Reinforcement learning (RL) is one method the robot can utilize to learn an optimal action selection rule, referred to as an optimal policy. Unfortunately, due to large state spaces that are common in robotics, the learning time and memory requirements can quickly become prohibitively expensive as the robot\u2019s task becomes more complex. The robot may run out of time or experience mechanical failure before learning is complete, thereby making RL often impractical. Moreover, when the agent is given a new task in a different environment, it must\nre-learn a new policy from scratch as it loses the knowledge it gained from previous experiences.\nMulti-task learning (MTL) helps to ameliorate this problem by enabling the agent to use its experience from previously learned tasks to improve learning performance in a related but different task (Caruana, 1997). In batch MTL, multiple tasks are learned simultaneously, while in online MTL, the tasks are learned consecutively. The latter method is of more interest for S&R applications, as it allows the agent to continually build on its knowledge over a sequence of tasks, thereby lessening the learning cost for every new task. Using MTL, we can train the robot in simulation ahead of time over a series of tasks, then transfer the learned knowledge to the real robot. The end result is an intelligent, flexible machine which can quickly adapt to new scenarios using learned knowledge, and which can learn new tasks more efficiently (Thrun, 1996).\nA robot capable of learning new tasks quickly by building upon previously learned knowledge would be vastly useful in applications such as S&R, where time constraints are an issue and where the robot is often presented with similar tasks in various scenarios with different environment dynamics.\nIn this project, we framed the problem as a reinforcement learning (RL) problem, where the agent\u2019s task is to find the target object in an environment. To allow variations in the tasks, we changed the coefficient \u00b5t of ground friction for each task t. This is a simplified version of the S&R problem, focusing on having the robot learn across different scenarios (in this case, ground friction). This setup is applicable in situations where the robot has to drive over various types of grounds, such as rough carpet or sand.\nUsing ROS and MATLAB, we began by implementing two single-task RL methods on a Turtlebot 2: Q-learning and policy gradient RL (PG). Then we implemented the Policy Gradient Efficient Lifelong Learning Algorithm (PGELLA), an online MTL algorithm using PG that allows ef-\nar X\niv :1\n51 1.\n08 96\n7v 1\n[ cs\n.A I]\n2 9\nN ov\n2 01\nficient sharing of knowledge between consecutive tasks to accelerate learning (Bou Ammar et al., 2014).\nResults and analysis of these implementations can be found in Section 5. In particular, we discuss experimental results of using a user policy to improve the learning performance of Q-learning, and demonstrate how increasing the size of the state space can exacerbate the learning cost for Q-learning. We also compare the learning speed of PG and Q-learning, and show that the robot learns noticeably faster using PG.\nThis project marks the first time that the Efficient Lifelong Learning Algorithm (ELLA) (Ruvolo & Eaton, 2013) was implemented on a real robot. PG-ELLA has been proven to be effective for simple dynamical systems (Bou Ammar et al., 2014), and we created several demonstration videos showing its effectiveness. However due to time constraints, we have not done complete empirical analysis of our PGELLA implementation, instead leaving this to future work."}, {"heading": "2. Single-Task Reinforcement Learning", "text": "The reinforcement learning problem can be formulated as a Markov decision process (MDP), where the problem model \u3008S,A, P,R, , \u03b3\u3009 consists of a state space S, a set of actions A, a state transition probability function P : S \u00d7 A \u00d7 S 7\u2192 [0, 1] describing the environment\u2019s dynamics, a reward function r : S \u00d7 A \u00d7 S 7\u2192 R, and a discount factor \u03b3 \u2208 [0, 1] which trades off the importance of sooner versus later rewards. We have an agent and an environment that interact continually, the agent selecting actions and the environment responding to those actions and presenting new situations to the agent. At each time step t \u2208 Z+, the agent receives the environment\u2019s current state st \u2208 S, and on that basis selects an action at \u2208 A(st), where A(st) is the set of legal actions in state st. One time step later, as a consequence of the agent\u2019s action and the current state, the environment sends a numerical reward r(st,at, st+1) \u2208 R and a new state st+1 to the agent. The agent\u2019s goal is to maximize the total expected discounted return, defined as\nRt = \u221e\u2211 k=0 \u03b3krt+k+1 ,\nwhere rt := r(st,at, st+1).\nAt each time step, the agent updates its policy \u03c0t : S \u00d7 A 7\u2192 [0, 1], where for each (s, a) \u2208 S \u00d7 A, \u03c0t(s, a) is the probability that the agent selects at = a if st = s. Reinforcement learning techniques specify how the agent implements its policy as a result of its experience. Two RL techniques that we implemented are Q-learning and policy gradient RL (PG), which we briefly desribe below."}, {"heading": "2.1. Q-learning", "text": "In Q-learning, we have a finite discrete state space S and a finite set of discrete actionsA. The algorithm stores a function Q : S\u00d7A 7\u2192 R which estimates the quality of a stateaction pair. Before learning starts, Q is initialized to return an arbitrary fixed value. Then in every time step t \u2208 Z+, the algorithm chooses an action at = argmaxaQt(st, a) with the highestQ-value, then updates its estimate ofQ via a simple value iteration update,\nQt+1(st, at) = (1\u2212 \u03b1)Qt(st, at)\ufe38 \ufe37\ufe37 \ufe38 old value\n+ \u03b1 rt+1\ufe38\ufe37\ufe37\ufe38 reward + \u03b3\ufe38\ufe37\ufe37\ufe38 discount\nfactor\nmax a Q(st+1, a)\ufe38 \ufe37\ufe37 \ufe38 estimate of optimal\nfuture value\n ,\nwhere \u03b1 \u2208 (0, 1] is the learning rate and \u03b3 \u2208 [0, 1] is the discount factor.\nWhile Q-learning works with discrete states and actions, in robotics it is preferable to work with continuous states and actions, as a robot\u2019s action parameters and sensor readings often take on continuous values. Therefore, a more useful RL technique for our purposes is the policy gradient RL (PG), which allows learning with continuous states and actions."}, {"heading": "2.2. Policy Gradient Reinforcement Learning", "text": "In policy gradient RL (PG), we have a (possibly infinite) set of states S \u2286 Rd and a set of actionsA \u2286 Rm. The general goal is to optimize the expected return of the policy \u03c0 with\nparameters \u03b8 \u2208 Rd, defined by\nJ (\u03b8) = \u222b T p\u03b8(\u03c4 )R(\u03c4 )d\u03c4\nwhere T is the set of all possible trajectories. A trajectory \u03c4 = [s1:T+1,a1:T ] denotes a series of states s1:H+1 = [s1, s2, . . . , sH+1] and actions a1:H = [a1,a2, . . . ,aH ]. The probability of a trajectory \u03c4 given policy \u03b8 is denoted by p\u03b8(\u03c4 ), and its average per time step return R(\u03c4) is\nR(\u03c4 ) = 1\nH H\u2211 t=1 r(st,at, st+1).\nUsing the Markov assumption, we can compute the probability of a trajectory as a product of probabilities of states and actions:\np\u03b8(\u03c4 ) = p(s1) H\u220f t=1 p(st+1 | st,at)\u03c0(at | st, t) ,\nwhere p(s1) denotes the initial state distribution, and p(st+1 | st,at) denotes the next state distribution conditioned on the last state and action.\nThe next action is sampled from a normal distribution\nat+1 \u223c N (\u03b8Tt \u03c6(st), \u03c3t) ,\nwhere \u03c6 : S 7\u2192 Rd is a feature vector. The value iteration update is given by\n\u03b8t+1 = \u03b8t + \u03b1\u2207 \u03c3t+1 = \u03c3t + \u03b1\u03c3t\u2207 ,\nwhere \u03b1 \u2208 (0, 1] is the learning rate and \u2207 := \u2207\u03b8J (\u03b8). For a derivation of the value iteration update, we refer the reader to Kober & Peters (2008)."}, {"heading": "3. Multi-Task Reinforcement Learning", "text": "Often in robotics, single-task RL methods suffer from slow convergence speed due to the huge size of the state space. Online MTL provides a solution to this problem by allowing knowledge to be shared between multiple tasks. In the following sections we present the Policy Gradient Efficient Lifelong Learning Algorithm (PG-ELLA), an online MTL algorithm that extends PG to enable a computationally efficient sharing of knowledge between consecutive tasks (Bou Ammar et al., 2014). PG-ELLA is based on the Efficient Lifelong Learning Algorithm (ELLA), an online MTL algorithm that has proven to be effective in the supervised MTL scenario (Ruvolo & Eaton, 2013)."}, {"heading": "3.1. The Online MTL Problem", "text": "In the online MTL setting, the agent is presented with a series of tasks Z(1), . . . ,Z(Tmax), where each task t is an MDP \u2329 S(t), A(t), P (t), R(t), \u03b3(t) \u232a with initial state distribution P (t)0 . The agent learns the tasks consecutively, training on multiple trajectories for each task before moving on to the next. We assume that the agent does not know the task order, their distribution, or the total number of tasks.\nThe agent\u2019s goal is to find a set of optimal policies \u03a0\u2217 = {\u03c0\u2217\n\u03b8(1) , . . . , \u03c0\u2217 \u03b8(Tmax) } with corresponding parameters\n\u0398\u2217 = {\u03b8(1)\u2217, . . . ,\u03b8(Tmax)\u2217}. The agent aims to optimize its learned policies for all tasks Z(1), . . . ,Z(T ), where T is the number of tasks seen so far."}, {"heading": "3.2. PG-ELLA", "text": "As in Section 2.2, we let d be the dimension of the state space, i.e., S(t) \u2286 Rd for each task t. PG-ELLA extends the PG algorithm to use a shared latent basis L \u2208 Rd\u00d7k (where k is the number of latent components) that enables transfer of knowledge between multiple tasks. We assume that the control parameters \u03b8(t) (t \u2208 {1, . . . , Tmax}) can be modeled as a linear combination of the columns of L, thereby allowing us to write \u03b8(t) = Ls(t), where s(t) \u2208 Rk is a vector of coefficients.\nThe aim of the algorithm can be represented by the following objective function,\nmin L\n1\nT T\u2211 t=1 min s(t) \u2212J (\u03b8(t))\ufe38 \ufe37\ufe37 \ufe38 model fit to\nindividual task\n+\u00b5 \u2225\u2225\u2225s(t)\u2225\u2225\u2225\n1\ufe38 \ufe37\ufe37 \ufe38 sparsity +\u03bb \u2016L\u20162F\ufe38 \ufe37\ufe37 \ufe38 maximize\nshared knowledge\n, (3.1)\nwhere \u03b8(t) = Ls(t), \u2016\u00b7\u20161 is the L1 norm, and \u2016\u00b7\u2016F is the Frobenius norm. Just as in PG, we maximize the expected return J ( \u03b8(t) ) (or equivalently, minimize \u2212J ( \u03b8(t) ) ) of the policy \u03c0\u03b8(t) , for each task t. We also minimize the sparsity \u2225\u2225s(t)\u2225\u2225 1\nof the coefficient vectors s(t) for each task t, to ensure that each latent basis component encodes a maximal reusable piece of knowledge. Lastly, we minimize the complexity \u2016L\u20162F of the shared latent basis L, in order to maximize the shared knowledge among tasks.\nThere are essentially two main steps to the algorithm, for every task t: first, we fit the model to the individual task by minizing the term \u2212J ( \u03b8(t) ) + \u00b5 \u2225\u2225s(t)\u2225\u2225 1\nwith respect to s(t); then we maximize the shared knowledge across all tasks by minimizing \u03bb \u2016L\u20162F. For the exact PG-ELLA algorithm and other details, please see Bou Ammar et al. (2014)."}, {"heading": "4. Methods", "text": "We implemented Q-Learning, PG, and PG-ELLA each as a separate ROS package, and trained the robot in simulation using Gazebo.1. For PG and PG-ELLA, we also used the Policy Gradient Toolbox,2 a MATLAB toolbox for policy gradient methods; and the MATLAB ROS I/O Package,3 which allows MATLAB to exchange messages with other nodes on the ROS network.\nFor each of the three RL packages, we created four nodes\u2014Agent, Environment, World Control, and Teleop\u2014that communicate with each other using ROS messages. For PG and PG-ELLA, we also have a fifth note\u2014MATLAB\u2014that receives trajectories from Agent and compute the robot\u2019s policy using the episodic REINFORCE policy gradient algorithm (Williams, 1992). A diagram is shown in Fig 2."}, {"heading": "4.1. Representation", "text": ""}, {"heading": "4.1.1. STATE SPACE", "text": "Let (xA, yA)\u1d40 and (xG, yG)\u1d40 be the locations of the agent and of the target object, respectively, on a 2D map of the\n1gazebosim.org 2ias.tu-darmstadt.de/Research /PolicyGradientToolbox 3mathworks.com/ros\nenvironment. Let v \u2208 R2 be the vector whose initial and terminal points are (xA, yA)\u1d40 and (xG, yG)\u1d40, respectively. Let u \u2208 R2 be a vector whose initial point is (xA, yA)\u1d40 and is parallel to the agent\u2019s orientation. Then for PG and PG-ELLA, we used a state space given by {(d, \u03c9)\u1d40}, where d = \u2016v\u2016 is the distance (in meters) from the agent to the target object, and \u03c9 is the counterclockwise angle (in degrees) from u to v. (See Fig. 3.)\nFor Q-learning, we discretized the state space to be {([d], [\u03c9]/12)\u1d40}, where [x] rounds x to the nearest integer, for any x \u2208 R; and a/b \u2208 Z+ is the quotient of a divided by b, for any a, b \u2208 Z. We took the quotient [\u03c9]/12 (which takes on values in {1, . . . , 30}) rather than [\u03c9], in order to reduce the size of the state space.\nIt is reasonable to define our state space in this way, as there are S&R scenarios where the robot has already located the target object, and knows the GPS coordinates of itself and of the target object."}, {"heading": "4.1.2. ACTION SPACE", "text": "For PG and PG-ELLA, the action space is {(v`, v\u03c9)\u1d40 : v`, v\u03c9 \u2208 R}, where v` is the robot\u2019s linear velocity (in meters/second) and v\u03c9 is the robot\u2019s angular velocity (in radians/second). In order to prevent abnormal motor behavior, in cases where the robot learns a bizarre policy, we restrict the range of v` and v\u03c9 to [\u22121.5, 1.5]. In other words, if the robot\u2019s policy returns a value for v` (or v\u03c9) that exceeds or is below the fixed range, then the robot simply uses the maximum (1.5) or minimum (-1.5) value, respectively.\nFor Q-learning, which requires a discrete action space, we chose the actions to be simply {Forward, Left, Right}."}, {"heading": "4.2. Reward function", "text": "We defined the reward function r : S \u00d7 A \u00d7 S 7\u2192 R in a way such that the agent is penalized for taking too many\nsteps to reach the goal, and is rewarded for finding the target object. More specifically, the reward function we chose for Q-learning is\nr(st, at, st+1) = { 100 if [d] < 1 and [\u03c9]/12 < 2 \u22121 otherwise ,\nwhere ([d], [\u03c9]/12)\u1d40 = st+1. Similarly, the reward function we chose for PG and PG-ELLA is\nr(st,at, st+1) = { 100 if d < 0.55 and \u03c9 < 0.2 \u22120.5|v`| otherwise ,\nwhere (d, \u03c9)\u1d40 = st+1 and v` is the linear velocity of at."}, {"heading": "4.3. Tasks", "text": "To allow variations in the tasks, we changed the ground friction coefficient\u00b5t for each task t. Ideally we would like to draw the friction coefficients from a normal distribution for each task, but due to time constraints, we instead used fixed friction coefficients for five different tasks (see Table 1)."}, {"heading": "4.4. Parameter optimization", "text": "For all three RL implementations, we used a discount factor of \u03b3 = 0.9."}, {"heading": "4.4.1. LEARNING RATES", "text": "Determining optimal learning rates for these RL techniques is very important, as they can either make or break the algorithm\u2019s performance.\nAfter testing Q-learning with different learning rates \u03b1, and measuring their learning performance, we determined that learning rates between 0.01 and 0.2 work well for our purposes. In our analysis of Q-learning\u2019s performance in Section 5.1, we use a learning rate of \u03b1 = 0.1.\nSince the action space {(v`, v\u03c9)\u1d40} for PG is 2-dimensional, the episodic REINFORCE algorithm inputs two learning rates \u03b1` and \u03b1\u03c9 , one for each of v` and v\u03c9 respectively. Letting U = {10\u2212j : j \u2208 {3, 4, . . . , 8}}, we chose learning rates for each task using the following method: we tested every pair (\u03b1`, \u03b1\u03c9) \u2208 U \u00d7 U and chose the pair that yielded the policy with the highest average cumulative reward per episode. Table 1 lists the pair of learning rates chosen for each task using this method."}, {"heading": "5. Testing and Analysis", "text": ""}, {"heading": "5.1. Q-learning", "text": ""}, {"heading": "5.1.1. USER POLICY", "text": "One drawback of the Q-learning algorithm is that the learning cost becomes prohibitively expensive as the state space grows large. To try to speed up the learning, we implemented a user policy \u03c0u : S \u00d7 A 7\u2192 [0, 1] that allows the robot to learn by demonstration. The idea is to use the user\u2019s trajectories to bootstrap the Q-table for learning. The user\u2019s trajectories are sub-optimal, so RL will refine them to create \u03c0.\nWe first manually drive the robot around using keyboard teleop, creating an arbitrary number of user trajectories Tu that eventually lead the robot to the target object. Then these user trajectories are stored and used to compute the user policy \u03c0u, where for every state-action pair (s, a), \u03c0u(s, a) is the probability that the user chooses action a in state s. When the robot follows the user policy \u03c0u in a state s, it computes the nearest state s\u2032 \u2208 Tu to s, then chooses the most probable action argmaxa \u03c0u(s \u2032, a).\nThe robot initially starts out with numbers p, q \u2208 [0, 1], p+q \u2264 1, such that for every time step t, the robot chooses a random action with probability p, follows the user policy with probability q, and chooses the best action based on its current policy \u03c0t otherwise. Over time, as the robot\u2019s policy \u03c0t becomes closer to optimal, p and q are slowly decreased until the robot can autonomously complete its task without the user\u2019s help. In our experiments, we decreased p and q by 1% every 1,000 episodes.\nIn Fig. 4, we compare the results from two experiments I and II, where in I we initially set p = .2 and q = .65 (the robot having a relatively high probability of following \u03c0u), and in II we initially set p = .2 and q = 0 (no user policy). We ran both experiments for 4,000 episodes each, recording the cumulative reward and the total number of steps taken to reach the target in every episode. In both graphs the plots seem to converge after about 1,700 episodes, although the plots from experiment II have greater variance. This suggests that the learning rates for Q-learning with and without user policy are similar, although having user\npolicy may more quickly yield a more stable policy."}, {"heading": "5.1.2. SIZE OF THE STATE SPACE", "text": "The learning cost for Q-learning grows increasingly expensive as the size of the state space grows. To illustrate this effect, we also experimented Q-learning with a larger 3- dimensional state representation {([d], [\u03b2]/12, [\u03b6]/12)\u1d40}, where \u03b2 (resp. \u03b6) is the counterclockwise angle in degrees from the x-axis of the 2D map of the environment to v (resp. u). (See Fig. 3 for an illustration.) We note that \u03c9 = \u03b2 \u2212 \u03b6.\nIn Fig. 5, we compare Q-learning\u2019s performance with the 2-dimensional state representation {([d], [\u03c9]/12)\u1d40} and with the 3-dimensional state representation {([d], [\u03b2]/12, [\u03b6]/12)\u1d40}. In both plots, the agent\u2019s performance improves at a faster rate and achieves significantly better results with the smaller state space. This illustrates the problem with Q-learning in a realistic environment, and shows where MTL could be especially beneficial."}, {"heading": "5.2. Policy Gradient Reinforcement Learning", "text": "For PG, we also used a \u201cuser policy\u201d in order to quicken learning. More specifically, we experimentally determined a user policy \u03c0u with parameters \u03b8u that (at least qualitatively) performs only slightly worse than optimal. To analyze the performance of PG, we initialized the parameters \u03b8 of the robot\u2019s policy \u03c0 to zero and then trained the robot for 400 episodes, using \u03c0u to select its actions. Then for another 400 episodes, we used \u03c0 to select the robot\u2019s actions, recording the cumulative reward and the number of steps to reach the target for every episode. Using these criteria, we\nfound that \u03c0 performs at least as well, if not better, than the user policy \u03c0u (see Table 2).\nWe note that the robot learned a policy almost as good as optimal in a shorter period of time than it did using Qlearning. PG has several advantages over Q-learning for this given task, as function approximation allows PG to handle continuous actions and states and even imperfect state information."}, {"heading": "5.3. PG-ELLA", "text": "Due to the lack of time and resources, we have not done a complete empirical analysis of our PG-ELLA implementation. For instance, the ROS I/O Package could not be installed on the GRASP Lab workstation computers because the MATLAB software installed on these computers were outdated, and thus all of the experiments for both PG and PG-ELLA had to be run on a personal laptop. The limited experiments with PG-ELLA thus far were not as successful, primarily because many of PG-ELLA\u2019s assumptions\nwere violated. For instance, we did not have enough tasks nor enough trajectories. We leave a more comprehensive empirical analysis of PG-ELLA to future work."}, {"heading": "6. Conclusion & Future Works", "text": "Because reinforcement learning can easily become prohibitively expensive, we worked with a simplified domain in the limited time we had for this project. But now the next step is to carry PG-ELLA into a more complex domain and do empirical analysis on its performance. We can add less informative features to the state vector, such as the robot\u2019s position (xA, yA) on the map, or the robot\u2019s perceptionbased data (e.g., is the target visible?). We can also add obstacles to the environment, essentially turning the map into a maze. Additionally, we can have variable environment dynamics: for example, instead of having a uniform ground friction coefficient, we can place patches of rough carpet or sand on the ground. Lastly, we can move PGELLA onto a robot in the real world, where imperfect and noisy sensor data are common.\nOf course, learning will quickly become more expensive, as we have demonstrated with the larger state space for Qlearning in Section 5.1.2. But we hope that through the sharing of knowledge between multiple tasks, the robot will be able to overcome the prohibitive cost of reinforcement learning."}, {"heading": "7. Acknowledgements", "text": "This work was supported by the National Science Foundation REU Site at the GRASP Laboratory in the University of Pennsylvania. I would like to express my very great appreciation to Professor Eric Eaton, my research advisor, for his patient guidance and valuable suggestions during the planning and development of this research work. I would also like to express my sincere gratitude to my mentors Christopher Clingerman and Dr. Haitham Bou Ammar for their invaluable help and support during this research project."}], "references": [{"title": "Online multi-task learning for policy gradient methods", "author": ["H. Bou Ammar", "E. Eaton", "P. Ruvolo", "M.E. Taylor"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Ammar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2014}, {"title": "Multitask Learning", "author": ["R. Caruana"], "venue": "PhD thesis,", "citeRegEx": "Caruana,? \\Q1997\\E", "shortCiteRegEx": "Caruana", "year": 1997}, {"title": "Policy search for motor primitives in robotics", "author": ["J. Kober", "J. Peters"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Kober and Peters,? \\Q2008\\E", "shortCiteRegEx": "Kober and Peters", "year": 2008}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Explanation-Based Neural Network Learning: A Lifelong Learning Approach", "author": ["S. Thrun"], "venue": null, "citeRegEx": "Thrun,? \\Q1996\\E", "shortCiteRegEx": "Thrun", "year": 1996}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams,? \\Q1992\\E", "shortCiteRegEx": "Williams", "year": 1992}], "referenceMentions": [{"referenceID": 1, "context": "Multi-task learning (MTL) helps to ameliorate this problem by enabling the agent to use its experience from previously learned tasks to improve learning performance in a related but different task (Caruana, 1997).", "startOffset": 197, "endOffset": 212}, {"referenceID": 4, "context": "The end result is an intelligent, flexible machine which can quickly adapt to new scenarios using learned knowledge, and which can learn new tasks more efficiently (Thrun, 1996).", "startOffset": 164, "endOffset": 177}, {"referenceID": 0, "context": "For the exact PG-ELLA algorithm and other details, please see Bou Ammar et al. (2014).", "startOffset": 66, "endOffset": 86}, {"referenceID": 5, "context": "For PG and PG-ELLA, we also have a fifth note\u2014MATLAB\u2014that receives trajectories from Agent and compute the robot\u2019s policy using the episodic REINFORCE policy gradient algorithm (Williams, 1992).", "startOffset": 177, "endOffset": 193}], "year": 2015, "abstractText": "Reinforcement learning (RL) is a general and well-known method that a robot can use to learn an optimal control policy to solve a particular task. We would like to build a versatile robot that can learn multiple tasks, but using RL for each of them would be prohibitively expensive in terms of both time and wear-and-tear on the robot. To remedy this problem, we use the Policy Gradient Efficient Lifelong Learning Algorithm (PG-ELLA), an online multi-task RL algorithm that enables the robot to efficiently learn multiple consecutive tasks by sharing knowledge between these tasks to accelerate learning and improve performance. We implemented and evaluated three RL methods\u2014Q-learning, policy gradient RL, and PG-ELLA\u2014on a ground robot whose task is to find a target object in an environment under different surface conditions. In this paper, we discuss our implementations as well as present an empirical analysis of their learning performance.", "creator": "LaTeX with hyperref package"}}}