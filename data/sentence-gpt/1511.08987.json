{"id": "1511.08987", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2015", "title": "How do the naive Bayes classifier and the Support Vector Machine compare in their ability to forecast the Stock Exchange of Thailand?", "abstract": "This essay investigates the question of how the naive Bayes classifier and the support vector machine compare in their ability to forecast the Stock Exchange of Thailand. The theory behind the SVM and the naive Bayes classifier is explored. The algorithms are trained using data from the month of January 2010, extracted from the MarketWatch.com website. Input features are selected based on previous studies of the SET100 Index. The Weka 3 software is used to create models from the labeled training data. Mean squared error and proportion of correctly classified instances, and a number of other error measurements are the used to compare the two algorithms. This essay shows that these two algorithms are currently not advanced enough to accurately model the stock exchange. Nevertheless, the naive Bayes is better than the support vector machine at predicting the Stock Exchange of Thailand.\n\n\n\n\nThe SVM, the naive Bayes, is not based on anything at all. It is based on the two Bayesian Bayesian models (see below). The first is Bayesian and Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then Bayesian, and then", "histories": [["v1", "Sun, 29 Nov 2015 09:57:42 GMT  (699kb)", "http://arxiv.org/abs/1511.08987v1", "16 pages"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["napas udomsak"], "accepted": false, "id": "1511.08987"}, "pdf": {"name": "1511.08987.pdf", "metadata": {"source": "CRF", "title": "How do the naive Bayes classifier and the Support Vector Machine compare in their ability to forecast the Stock Exchange of Thailand?", "authors": ["Napas Udomsak"], "emails": [], "sections": [{"heading": null, "text": "the naive Bayes classifier and the support vector machine compare in their ability to forecast the Stock Exchange of Thailand. The theory behind the SVM and the naive Bayes classifier is explored. The algorithms are trained using data from the month of January 2010, extracted from the MarketWatch.com website. Input features are selected based on previous studies of the SET100 Index. The Weka 3 software is used to create models from the labeled training data. Mean squared error and proportion of correctly classified instances, and a number of other error measurements are the used to compare the two algorithms. This essay shows that these two algorithms are currently not advanced enough to accurately model the stock exchange. Nevertheless, the naive Bayes is better than the support vector machine at predicting the Stock Exchange of Thailand.\nI. INTRODUCTION\nachine learning is a branch of artificial intelligence that is concerned with the\nconstruction of models from data (KOVAHI, Ron and Provost, Foster, 1998). In supervised machine learning, a subfield of machine learning, computers derive models from labeled training data. Recently, the field of machine learning has seen a rise in the popularity of probabilistic and statistical models. Notably, the Naive Bayes, Artificial Neural Networks (ANN), and Support Vector Machines (SVM).\nThe aim of supervised machine learning is that given a set of \ud835\udc41 training examples, {(\ud835\udc651, \ud835\udc661), \u2026 (\ud835\udc65\ud835\udc5b , \ud835\udc66\ud835\udc5b)} where \ud835\udc65\ud835\udc56 is the feature vector \ud835\udc56 \ud835\udc61\u210e example, and \ud835\udc66\ud835\udc56 is its label, or class, to derive a function \ud835\udc54: \ud835\udc4b \u2192 \ud835\udc4c mapping input feature \ud835\udc65 to label \ud835\udc66 such that \ud835\udc4b is the input space {\ud835\udc651, \u2026 , \ud835\udc65\ud835\udc5b} and \ud835\udc4c is the output space of possible classes {\ud835\udc661, \u2026 , \ud835\udc66\ud835\udc5b} (MOHRI, Mehryar et al., 2012).\nThe prediction and forecasting of financial markets has been of interest to artificial intelligence researchers since the dawn of learning algorithms (OU, Phichhang and Wang, Hengshan, 2009). A stock market index is a statistical composite of the movement of the overall market. This index will reflect the performance of all companies in the stock market over a period of time.\nAs an economist and a computer scientist, investigating the stock market using computer algorithms is of great interest to me. Furthermore, if this experiment produces an accurate model a trading\nstrategy could be created that would allow for profits to be derived from the buying and selling of stocks.\nIn this essay, I will be looking at the Stock Exchange of Thailand, specifically the SET100 index. Data from the month of January 2010 will be extracted and used to train a naive Bayes classifier and a support vector machine. I will then use a number of performance indicators to compare the algorithms. This will allow me to answer the question: How do the naive Bayes classifier and the support vector machine (SVM) compare in their ability to forecast the Stock Exchange of Thailand?"}, {"heading": "II. ANALYSIS", "text": ""}, {"heading": "A. The Stock Exchange", "text": "The stock exchange is a place where buyers and sellers come together to trade shares. A stock market index measures the value of a section of the stock market. The SET100 is the primary index of the Stock Exchange of Thailand (SET). It tracks the prices of the top 100 companies on the SET ranked by market capitalization and liquidity. It can be calculated using the following formula (PHAISARN, Sutheebanjard and Wichian, Premchaiswadi, 2010):\nSET 100 = Current market value \u00d7 100\nBase market value"}, {"heading": "B. The Efficient Market Hypothesis (EMH)", "text": "The EMH was developed by Fama in 1970 (FAMA, Eugene F, 1970). It states that the price of a security reflects the complete market information. Should there be a change in financial outlook, the market is perfectly efficient, and will therefore instantly adjust the security price to reflect this new information. The EMH is controversial and often disputed. Its supporters claim that attempts at predicting stock price through technical or fundamental analysis is pointless as the market will immediately reflect any new information discovered. Hence, an abnormal profit cannot be obtained. Nevertheless, there are three different forms of the EMH. The weak EMH states that only historical information is embedded in the stock price. The semi-strong form claims that the price represents all historical information and all available public information, while the strong EMH suggests that the current stock price represents all available\nM\nhistorical, public, and private insider information. Fama himself has considered the strong EMH to be invalid."}, {"heading": "C. Random walk theory", "text": "A random walk is one in which the next step, or steps, cannot be predicted based on the information about past steps. In the context of a stock market, this theory proposes that the short-run direction of stock price cannot be predicted. Therefore investment services, earning predictions, and complicated chart patterns are all but a hoax. The stock\u2019s future prices take a completely random unpredictable path. In this theory technical and fundamental analysis are considered feeble attempts at trying to beat the market. Kendall and Hill first proposed this theory in 1953 and shocked many economists (KENDALL, Maurice George and Hill, Bradford A., 1953). However, after further debate and research, the formal random walk theory market was devised.\nThe random walk theory and the EMH are compatible. The market can move in unpredictable directions while being efficient. Both theories suggest that an abnormal profit cannot be achieved from stock price prediction"}, {"heading": "D. Fundamental Analysis", "text": "Fundamental analysis looks at the numerical indicators that describe the company underlying the stock e.g. P/E ratio (MCCLURE, Ben, 2014). An analysts looks at the fundamentals of a company in order to determine whether the stock is under, or overvalued and then buying, or selling the stock, respectively."}, {"heading": "E. Technical Analysis", "text": "Technical analysts or \u2018chartists\u2019 are not concerned about the company\u2019s fundamentals. Technical analysis aims to derive patterns and trends from past price to predict future price (a form of time series analysis) (INTERACTIVE DATA CORP, 2014). Historical data are tested using specific rules for buying and selling in order to evaluate whether or not a profit can be made by following the same strategy in the future. Technical analysis is based on the assumption that the pattern derived from historical data will apply to future data."}, {"heading": "F. The naive Bayes classifier", "text": "Bayesian classifiers are statistical classifiers that can predict the probability of a class membership, i.e. the probability that a given sample belongs to a specific class. Bayesian classifiers apply Bayes theorem in\norder to produce probabilities of class membership. The naive Bayes classifier, a special type of Bayesian classifier, utilizes a naive assumption of conditional independence between attributes. This assumption exists merely to simplify computation and is therefore \u201cnaive\u201d (STUART, Russell J. and Norvig, Peter, 2009).\nSuppose X is a single sample represented by a ndimensional vector {x1, x2, \u2026 ,xn} that contains the n attributes of x. In Bayesian statistics, X is called the \u201cevidence\u201d. Now, suppose H is a hypothesis that X belongs to some class C. In classification, we want to find the probability of P(H|X) \u2013 the probability that X belongs to class C given the evidence {x1, x2, \u2026 ,xn}. P(H|X) is known as the posteriori probability of H given X.\nThe naive Bayes classifier works as follows: Given a set of labelled training samples, T. Each item in T contains a label, C1 , C2, \u2026, Ck for k possible classes. Each item in T also contains an ndimensional vector{x1, x2, \u2026, xn} that represent the n attributes {A1, A2, \u2026, An} which describe that item. Given a single sample X, the classifier will find the class Ci that maximizes P(Ci|X). In other words, X belongs to Ci if and only if P(Ci|X) > P(Cj|X) for 1 \u2264 j \u2264 k and j \u2260 i; Choosing a class based on this rule is also known as maximum-likelihood estimation (WAZIRI, Victor Onomza, 2013).\nBy Bayes theorem (INTERNATIONAL BACCALAUREATE, 2012), P(Ci|X) can also be calculated using the following formula:\nP(\ud835\udc36\ud835\udc56|\ud835\udc4b) = P(\ud835\udc4b|\ud835\udc36\ud835\udc56) P(\ud835\udc36\ud835\udc56)\nP(\ud835\udc4b)\nSince P(X) is equal for all classes, P(Ci|X) is maximized when P(X|Ci)P(Ci) is maximized. If P(Ci) is not known, it is common to assume a uniform distribution for P(Ci) such that P(C1) = P(C2) = \u2026 = P(Ck). Hence, P(Ci|X) \u221d P(X| Ci) and our goal becomes to maximize P(X|Ci). Priori probability, P(Ci), can also be estimated by the following approximation:\nP(\ud835\udc36\ud835\udc56) = freq(\ud835\udc36\ud835\udc56 in \ud835\udc47)\nsize(\ud835\udc47)\nwhere T is the training set used to create the classifier. Because it is computationally expensive to calculate P(X|Ci), the na\u00efve assumption of conditional independence is utilized to allow us to make the following approximation:\nP(\ud835\udc4b|\ud835\udc36\ud835\udc56) = P(\ud835\udc651, \ud835\udc652, \u2026 , \ud835\udc65\ud835\udc5b|\ud835\udc36\ud835\udc56) \u2245 \u220f P(\ud835\udc65\ud835\udc58|\ud835\udc36\ud835\udc56\nn\nk=1\n)\nP(x1|Ci), P(x2|Ci), \u2026, P(xn|Ci) can now be easily\ncalculated from the labelled training set.\nRecall that xk represents the measured value of an attribute Ak. If Ak is categorical, then the P(xk|Ci) of can be calculated by counting the occurrence of xk in samples labelled Ci divided by the number of Ci examples in T, i.e.:\nP(\ud835\udc65\ud835\udc56| \ud835\udc36\ud835\udc56) = freq(\ud835\udc36\ud835\udc56 containing \ud835\udc65\ud835\udc56 in T)\nfreq(\ud835\udc36\ud835\udc56 in \ud835\udc47)\nHowever, if Ak is continuous then we can assume that it is approximated by a Gaussian distribution (INTERNATIONAL BACCALAUREATE, 2012) with a mean, \u03bc, and a standard deviation, \u03c3, defined by:\n\ud835\udc54(\ud835\udc65, \ud835\udf07, \ud835\udf0e) = 1\n\u221a2\ud835\udf0b\ud835\udf0e \ud835\udc52\n\u2212 (\ud835\udc65\u2212\ud835\udf07)2\n2\ud835\udf0e2\nwhere \u03bc and \u03c3 can be estimated by:\n\ud835\udf07 = \u2211 \ud835\udc65\ud835\udc58 in \ud835\udc36\ud835\udc56\n\ud835\udc5b \ud835\udc58=1\n\ud835\udc5b\n\ud835\udf0e = \u2211 \ud835\udc65\ud835\udc58\n2 in \ud835\udc36\ud835\udc56 \ud835\udc5b \ud835\udc58=1\n\ud835\udc5b \u2212 (\n\u2211 \ud835\udc65\ud835\udc58 in \ud835\udc36\ud835\udc56 \ud835\udc5b \ud835\udc58=1\n\ud835\udc5b )\n2\nwith this we can calculate now calculate P(xk|Ci):\n\ud835\udc43(\ud835\udc65\ud835\udc58|\ud835\udc36\ud835\udc56) = \ud835\udc54(\ud835\udc65\ud835\udc58 , \ud835\udf07\ud835\udc36\ud835\udc56 , \ud835\udf0e\ud835\udc36\ud835\udc56)"}, {"heading": "G. Laplace Smoothing", "text": "In the event that Ak is categorical and xk does not fit within the bounds of any category of Ak such that P(xk|Ci) = 0 then P(X|Ci) = 0 because recall that P(X|Ci) is calculated by P(\ud835\udc4b|\ud835\udc36\ud835\udc56) \u2245 \u220f P(\ud835\udc65\ud835\udc58|\ud835\udc36\ud835\udc58 n k=1 ). Hence, to prevent this error, the Laplacian correction can be utilized to deal with zero probability values (PRABHAKAR, Raghavan et al., 2008).\nWith the Laplacian estimator, suppose that there is a sample X that contains feature xk that describes the attribute Ak of X. If xk does not fit within the bounds of any category then instead of assigning the probability P(xk|Ci) = 0, we use the below formula to estimate the probability of P(xk|Ci).\nP(\ud835\udc65\ud835\udc58|\ud835\udc36\ud835\udc56) = 1\nfreq(\ud835\udc65\ud835\udc58in \ud835\udc47)"}, {"heading": "H. Support Vector Machines", "text": "Support vector machines (SVM) are another tool used in supervised machine learning to classify a sample X to a class labels C. SVMs work by deriving a linear decision boundary that can represent a nonlinear class boundary through a non-linear mapping of input vectors xk into a higher-dimensional feature space. The linear model is constructed in the higherdimensional feature space to represent a non-linear decision boundary (WESTON, Jason, 2004).\nLeft: Non-linear decision boundary in 2 dimensions\nRight: Linear decision boundary in 3 dimensions\nIn the new space a maximum margin hyper-plane is derived from training data. This maximum margin hyper-plane provides maximum separation between two classes. This hyper-plane is derived from the examples closest to it, all other examples are considered irrelevant in defining the decision boundary.\nFor the linearly separable case where there two classes and the data is represented by three attributes x1, x2, x3, there is no need to map to a higherdimensional space and thus the maximum margin hyper-plane will have an equation of the following form:\n\ud835\udc66 = \ud835\udc640 + \ud835\udc641\ud835\udc651 + \ud835\udc642\ud835\udc652 + \ud835\udc643\ud835\udc653 where y is the outcome, xi, are the attributes. The\nfour weights, wi, are learned from the training data.\nThe maximum margin hyper-plane can also be\nrepresented in terms of the support vectors.\n\ud835\udc66 = \ud835\udc4f + \u2211 \ud835\udefc\ud835\udc56\ud835\udc66\ud835\udc56\ud835\udc65(\ud835\udc56) \u22c5 \ud835\udc65\nwhere yi is the class outcome of the specific training example x(i) and the \u2219 is dot product. The vector x represents a test example and x(i) are the support vectors that are used to determine the decision boundary. In this equation, \u03b1i and b are parameters that are optimized in the process of finding the maximum margin hyper plane. At implementation this turns in to a linearly constrained quadratic programming problem whereby the support vectors x(i) are found, and parameters, b and \u03b1i , are determined (KIM, Kyoungjae, 2003).\nFor the nonlinearly separable case where the input must be mapped to a higher-dimensional feature space, the decision boundary can be represented as follows:\n\ud835\udc66 = \ud835\udc4f + \u2211 \ud835\udefc\ud835\udc56\ud835\udc66\ud835\udc56\ud835\udc3e(\ud835\udc65(\ud835\udc56), \ud835\udc65)\nwhere K(x(i),x) is defined as the kernel function.\nThere are a number kernel functions that we can choose from. These functions define how the SVM performs the mapping of input features to a higher\ndimensional space.\nCommon kernel functions include the polynomial kernel K(x,y) = (xy+1)d where d is the degree of the polynomial and the Gaussian radial basis function K(x,y) = exp(\u22121/\u03b42(x-y)2) where \u03b42 is the bandwidth of the radial basis function. \u03b42 is usually selected via a grid search (HUANG, Wei et al., 2005)."}, {"heading": "I. Advantages and Disadvantages of the naive Bayes classifier and SVM", "text": "The naive Bayes classifier is easy to implement because we can easily produce probability estimates using the formulas detailed in the section 2.4. Studies have shown that the naive Bayes can produce good parameter estimates with small data sets (UDDIN, Ashraf et al., 2012).\nOn the other hand, the main disadvantage of this classifier is its naive assumption of conditional independence. In practice, dependencies exist between variables, especially in a complicated system like the stock market. These dependencies are ignored by the naive Bayes classifier which would cause it to produce less accurate predictions (ESMAEL, Bilal, 2013).\nThe maximum margin decision boundary utilized by the SVM is built upon a unique principle of structural risk minimization whereby the decision boundary is derived through minimizing the upper bound of the generalization error, allowing SVMs to be very resilient against the over-fitting problem. Because the optimization of parameters in an SVM can be achieved through solving a linearly constrained quadratic programming problem, the solution will always be a unique global optimum (HUANG, Wei et al., 2005).\nSVMs require relatively complicated highdimensional models that take a longer time to train when compared to the naive Bayes. Their method of mapping input features into higher-dimensional spaces is controlled by a kernel function has to be manually selected. The selection of an inappropriate kernel function can heavily detriment the performance of a SVM (CHRISTOPHER, Burges JC., 1998). Parameter tuning of the SVM must also be performed manually using a grid search. Once again, in appropriate selection of the sigma parameter of the radial basis kernel can lead to issues like over fitting. Research has shown that when dealing with extremely large data sets SVMs result in high algorithmic complexity and extensive memory requirements (HORVATH, Gabor et al., 2003).\nOne algorithm is not categorically better than the other. It is difficult to predict the real-world performance of learning algorithms based on their theoretical advantages and disadvantages, as each data\nset has its own unique patterns."}, {"heading": "III. EXPERIMENT", "text": ""}, {"heading": "A. Hypothesis", "text": "Based on the theory discussed above, I believe that a support vector machine will produce better predictions of the stock market direction than a naive Bayes classifier. The SVMs ability to model non-linear decision boundaries using a linear model will be advantageous as the stock market is most definitely a complicated nonlinear process. The naive assumption of conditional independence, that is central to the naive Bayes classifier, will be penalizing to its performance. Because of the inter-connected nature of the stock market, it is unlikely that a change in one factor will be completely independent to a change in another factor in real life."}, {"heading": "B. Hardware and software specifications", "text": "CPU: Intel Core i5-3337U 1.80Ghz RAM: 4.00 GB OS: Windows 8 x64 Required software: Java SE Runtime Environment 7 and Weka 3"}, {"heading": "C. Method", "text": "Input features:\n1. Nikkei 255 Index (NK) 2. Hang Seng Index (HS) 3. SET 100 Index (SET) 4. USDTHB Exchange Rate (USDTHB) 5. S&P 500 Index (SP) 6. COMEX Gold Futures (GOLD)\nThese variables have been chosen from previous studies on factors that affect stock market direction (SUTHEEBANJARD, Phaisarn and Premchaiswadi, Wichian, 2009). External factors such as the index have been chosen to represent the market sentiment, whereas internal factors like the SET index itself should tell the classifier about the internal situation of the market.\nFirst, the data of the input variables will be extracted for the period 1st January 2010 \u2013 1st February 2010 from MarketWatch.com (see Appendix A on pg.18 for program code used to extract data).\nSecondly, the data will be processed and the\npercentage change between each day will be found.\nThirdly, the stock market direction will be classified either UP or DOWN (this becomes the class label). Hence, a single sample, X, in the training set T will look as follows (see Appendix B on pg.22 for full training set):\nFourthly, create a naive Bayes Classifier and a SVM from the data using the WEKA software (see Appendix C pg.23 for step-by-step guide)\nFinally, calculate indicators of classifier\nperformance over 10-fold cross validation"}, {"heading": "IV. RESULTS", "text": "SVM Naive Bayes\nCorrectly\nclassified instances\n56 % 66 %\nMean absolute\nerror\n0.43 0.38\nRoot mean\nsquared error\n0.65 0.54\nRelative\nabsolute error\n86 % 77 %\nRoot relative\nsquared error\n130 % 108 %\n*data is based on naive Bayes trained on continuous values (see Appendix D for parameters of Gaussian fitting) and SVM parameter \u03b42 = 0\nConfusion matrix for SVM:\n( \ud835\udc4e \ud835\udc4f \u2190 classified as\n11 5 a = up 8 6 b = down ) Confusion matrix for naive Bayes:\n( \ud835\udc4e \ud835\udc4f \u2190 classified as\n13 3 a = up 7 7 b = down )"}, {"heading": "A. Evaluation", "text": "During the experiment a limited amount of data was chosen to train these models, a larger training data set will produce different models that might be better. The method of testing chosen was 10-fold cross validation, while this helps analyse the extent of over-fitting, the model is still ultimately still being tested on its own training data. A completely independent data set could have been used to produce better indicators of performance. The data is assumed to be continuous in the training of models, some studies have shown that discretization could be beneficial to the performance of these models, especially the naive Bayes classifier. In future experiments, I could discretize the data before using them to create models."}, {"heading": "B. Conclusion", "text": "The results of 10-fold stratified cross validation show that the naive Bayes displays better performance in terms of the proportion of correctly classified instances, and lower error on every measurement of\nerror. This does not support my hypothesis. I believe this could be due to the SVM\u2019s over complexity and the Gaussian smoothing that the naive Bayes classifier was able to benefit from.\nUltimately, both models were able to predict the stock market to some degree. While, these results cannot be used to support or dispute the EMH as the data is all in the past, it does suggest that perhaps the stock market does not follow a random walk. It is probable that behind all the noise and chaos of market there is a complex non-linear process, however today\u2019s algorithms are clearly not yet advanced enough to accurately model such a process.\nFuture research should consider resolving the conditional independence assumption by modelling the relationships between variables. I believe research of more advanced kernel functions will be the key to improving the performance of the support vector machine.\nIn conclusion, the naive Bayes was able to forecast the stock market better than the SVM based on data from the month of January 2010."}], "references": [{"title": "A tutorial on support vector machines for pattern recognition", "author": ["CHRISTOPHER", "Burges JC."], "venue": "Data mining and knowledge discovery. II(2), pp.121-167. ESMAEL, Bilal. 2013. www.researchgate.net.", "citeRegEx": "CHRISTOPHER and JC.,? 1998", "shortCiteRegEx": "CHRISTOPHER and JC.", "year": 1998}, {"title": "Advances in Learning Theory: Methods, Model and Applications", "author": ["BASU"], "venue": "IOS Press. HUANG, Wei, Yoshiteru NAKAMORI, and ShouYang WANG. 2005. Forecasting stock market movement direction with support vector machine.", "citeRegEx": "BASU,? 2003", "shortCiteRegEx": "BASU", "year": 2003}, {"title": "The analysis of economic time-series-part i: Prices", "author": ["Organization. KENDALL", "Maurice George", "Bradford A. HILL."], "venue": "Journal of the Royal Statistical Society. Series A (General). I(116), pp.11-34.", "citeRegEx": "KENDALL et al\\.,? 1953", "shortCiteRegEx": "KENDALL et al\\.", "year": 1953}, {"title": "Financial time series forecasting using support vector machines", "author": ["KIM", "Kyoung-jae."], "venue": "Neurocomputing. I(55), pp.307-319. KOEHLER, Anne B. and Rob J. HYNDMAN. 2006. Another look at measures of forecast accuracy.", "citeRegEx": "KIM and Kyoung.jae.,? 2003", "shortCiteRegEx": "KIM and Kyoung.jae.", "year": 2003}, {"title": "Foundations of Machine Learning", "author": ["Ameet TALWALKAR."], "venue": "The MIT Press. OU, Phichhang and Hengshan WANG. 2009. Prediction of stock market index movement by ten data mining techniques. Modern Applied Science.", "citeRegEx": "TALWALKAR.,? 2012", "shortCiteRegEx": "TALWALKAR.", "year": 2012}, {"title": "Introduction to information retrieval", "author": ["PRABHAKAR", "Raghavan", "Christopher D. MANNING", "Hinrich SCH\u00dcTZE"], "venue": null, "citeRegEx": "PRABHAKAR et al\\.,? \\Q2008\\E", "shortCiteRegEx": "PRABHAKAR et al\\.", "year": 2008}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["STUART", "Russell J.", "Peter NORVIG."], "venue": "Prentice Hall. SUTHEEBANJARD, Phaisarn and Wichian PREMCHAISWADI. 2009. Factors analysis on stock", "citeRegEx": "STUART et al\\.,? 2009", "shortCiteRegEx": "STUART et al\\.", "year": 2009}], "referenceMentions": [], "year": 2014, "abstractText": "This essay investigates the question of how the naive Bayes classifier and the support vector machine compare in their ability to forecast the Stock Exchange of Thailand. The theory behind the SVM and the naive Bayes classifier is explored. The algorithms are trained using data from the month of January 2010, extracted from the MarketWatch.com website. Input features are selected based on previous studies of the SET100 Index. The Weka 3 software is used to create models from the labeled training data. Mean squared error and proportion of correctly classified instances, and a number of other error measurements are the used to compare the two algorithms. This essay shows that these two algorithms are currently not advanced enough to accurately model the stock exchange. Nevertheless, the naive Bayes is better than the support vector machine at predicting the Stock Exchange of Thailand.", "creator": "Microsoft\u00ae Word 2013"}}}