{"id": "1511.09263", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2015", "title": "Scalable and Accurate Online Feature Selection for Big Data", "abstract": "Feature selection is important in many big data applications. There are at least two critical challenges. Firstly, in many applications, the dimensionality is extremely high, in millions, and keeps growing fast and rapidly. Secondly, if you are going to see a change in your data structure, do not be surprised to learn that you will no longer be able to use this feature to build real applications.\n\n\n\n\nAs a result, you need to be able to use this feature to build real applications. In many small applications, you should avoid this if you have a lot of data. The important thing is that the developer can keep a good clean, consistent and complete account, and that no longer needs to rely on other data sources. So, you will not be able to keep the data of any of the applications with it.\nThe key to maintaining your data is not to change any of the data sources to make them better, but to provide a better way to handle them.\nThere are other uses of the feature in many big data applications. In many big data applications, you should not be able to use this feature to build real applications. In many small applications, you should avoid this if you have a lot of data. The important thing is that the developer can keep a good clean, consistent and complete account, and that no longer needs to rely on other data sources. So, you will not be able to keep the data of any of the applications with it.\nSo, you will not be able to keep the data of any of the applications with it.\nIf you have to do things in a consistent manner, you will not be able to use the feature to build real applications. In many small applications, you should avoid this if you have a lot of data. The important thing is that the developer can keep a good clean, consistent and complete account, and that no longer needs to rely on other data sources. So, you will not be able to keep the data of any of the applications with it. In many small applications, you should not be able to use this feature to build real applications. In many small applications, you should avoid this if you have a lot of data. The important thing is that the developer can keep a good clean, consistent and complete account, and that no longer needs to rely on other data sources.\nIf you have to do things in a consistent manner, you will not be able to use the feature to build real applications. In many small applications, you should avoid this if", "histories": [["v1", "Mon, 30 Nov 2015 12:11:43 GMT  (401kb)", "https://arxiv.org/abs/1511.09263v1", null], ["v2", "Thu, 7 Jan 2016 01:04:16 GMT  (451kb)", "http://arxiv.org/abs/1511.09263v2", null], ["v3", "Mon, 25 Jul 2016 03:11:09 GMT  (451kb)", "http://arxiv.org/abs/1511.09263v3", null], ["v4", "Thu, 28 Jul 2016 01:49:01 GMT  (451kb)", "http://arxiv.org/abs/1511.09263v4", "This paper has been accepted by the journal of ACM Transactions on Knowledge Discovery from Data (TKDD) and will be available soon"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kui yu", "xindong wu", "wei ding", "jian pei"], "accepted": false, "id": "1511.09263"}, "pdf": {"name": "1511.09263.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["KUI YU", "XINDONG WU", "WEI DING", "JIAN PEI"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n09 26\n3v 4\n[ cs\n.L G\n] 2\n8 Ju\nl 2 01\n6\nA\nScalable and Accurate Online Feature Selection for Big Data\nKUI YU, Simon Fraser University XINDONG WU, Hefei University of Technology and University of Vermont WEI DING, University of Massachusetts Boston JIAN PEI, Simon Fraser University\nFeature selection is important in many big data applications. Two critical challenges closely associate with big data. Firstly, in many big data applications, the dimensionality is extremely high, in millions, and keeps growing. Secondly, big data applications call for highly scalable feature selection algorithms in an online manner such that each feature can be processed in a sequential scan. We present SAOLA, a Scalable and Accurate OnLine Approach for feature selection in this paper. With a theoretical analysis on bounds of the pairwise correlations between features, SAOLA employs novel pairwise comparison techniques and maintain a parsimonious model over time in an onlinemanner. Furthermore, to deal with upcoming features that arrive by groups, we extend the SAOLA algorithm, and then propose a new group-SAOLA algorithm for online group feature selection. The group-SAOLA algorithm can online maintain a set of feature groups that is sparse at the levels of both groups and individual features simultaneously. An empirical study using a series of benchmark real data sets shows that our two algorithms, SAOLA and group-SAOLA, are scalable on data sets of extremely high dimensionality, and have superior performance over the state-of-the-art feature selection methods.\nAdditional Key Words and Phrases: Online feature selection, Extremely high dimensionality, Group features, Big data"}, {"heading": "1. INTRODUCTION", "text": "In data mining and machine learning, the task of feature selection is to choose a subset of relevant features and remove irrelevant and redundant features from high-dimensional data towards maintaining a parsimonious model [Guyon and Elisseeff 2003; Liu and Yu 2005; Xiao et al. 2015; Zhang et al. 2015]. In the era of big data today, many emerging applications, such as social media services, high resolution images, genomic data analysis, and document data analysis, consume data of extremely high dimensionality, in the order of millions or more [Wu et al. 2014; Zhai et al. 2014; Chen et al. 2014; Yu et al. 2015a; Yu et al. 2015b]. For example, the Web Spam Corpus 2011 [Wang et al. 2012] collected approximately 16 million features (attributes) for web spam page detection, and the data set from KDD CUP 2010 about using educational data mining to accurately predict student performance includes more than 29 million features. The scalability of feature selection methods becomes critical to tackle millions of features [Zhai et al. 2014]. Moreover, in many applications, feature selection has to be conducted in an online manner. For example, in SINA Weibo, hot topics in behavior in Weibo keep changing daily. When a novel hot topic appears, it may come with a set of new keywords (a.k.a. a set of features). And then some of the new keywords may serve as key features to identify new hot topics. Another example is feature selection in bioinformatics, where acquiring the full set of features for every training instance is expensive because of the high cost in conducting wet lab experiments [Wang et al. 2013]. When it is impossible to wait for a complete set of features, it is practical to conduct feature selection from\nAuthor\u2019s addresses: K. Yu, School of Computing Science, Simon Fraser University, Burnaby, BC, Canada, kuiy@sfu.ca; X. Wu, Department of Computer Science, Hefei University of Technology, Hefei, 230009, China and Department of Computer Science, University of Vermont, Burlington, 05405, USA, email: xwu@cs.uvm.edu; W. Ding, Department of Computer Science, University of Massachusetts Boston, Boston, MA, USA, ding@cs.umb.edu; J. Pei, School of Computing Science,Simon Fraser University, Burnaby, BC, Canada, jpei@cs.sfu.ca.\nthe features available so far, and consume new features in an online manner as they become available. To search for a minimal subset of features that leads to the most accurate prediction model, two types of feature selection approaches were proposed in the literature, namely, batch methods [Brown et al. 2012; Woznica et al. 2012; Javed et al. 2014] and online methods [Wu et al. 2013; Wang et al. 2013]. A batch method requires loading the entire training data set into memory. This is obviously not scalable when handling large-scale data sets that exceed memory capability. Moreover, a batch method has to access the full feature set prior to the learning task [Wu et al. 2013; Wang et al. 2013]. Online feature selection has two major approaches. One assumes that the number of features on training data is fixed while the number of data points changes over time, such as the OFS algorithm [Hoi et al. 2012; Wang et al. 2013] that performs feature selection upon each data instance. Different from OFS, the other online method assumes that the number of data instances is fixed while the number of features changes over time, such as the Fast-OSFS [Wu et al. 2013] and alphainvesting algorithms [Zhou et al. 2006]. This approach maintains a best feature subset from the features seen so far by processing each feature upon its arrival. Wang et al. [Wang et al. 2013] further proposed the OGFS (Online Group Feature Selection) algorithm by assuming that feature groups are processed in a sequential scan. It is still an open research problem to efficiently reduce computational cost when the dimensionality is in the scale of millions or more [Wu et al. 2013; Zhai et al. 2014]. In this paper, we propose online feature selection to tackle extremely highdimensional data for big data analytics, our contributions are as follows.\n\u2014We conduct a theoretical analysis to derive a low bound on pairwise correlations between features to effectively and efficiently filter out redundant features. \u2014With this theoretical analysis, we develop SAOLA, a Scalable and Accurate OnLine Approach for feature selection. The SAOLA algorithm employs novel online pairwise comparisons to maintain a parsimonious model over time. We analyze the upper bound of the gap of information gain between selected features and optimal features. \u2014 To deal with new features that arrive by groups, we extend the SAOLA algorithm, namely, the group-SAOLA algorithm. The group-SAOLA algorithm can online yield a set of feature groups that is sparse between groups as well as within each group for maximizing its predictive performance for classification. \u2014An extensive empirical study using a series of benchmark data sets illustrates that our two methods, both SAOLA and group-SAOLA, are scalable on data sets of extremely high dimensionality, and have superior performance over the state-of-the-art online feature selection methods.\nThe rest of the paper is organized as follows. Section 2 reviews related work. Section 3 proposes our SAOLA algorithm, and Section 4 presents the group-SAOLA algorithm. Section 5 reports our experimental results. Finally, Section 6 concludes the paper and our future work."}, {"heading": "2. RELATED WORK", "text": "Given a set of input features on a training data set, the problem of feature selection is to select a subset of relevant features from input features without performance degradation of prediction models. There are two types of feature selection approaches proposed in the literature, namely, the batch methods and the online methods. Standard batch methods can be broadly classified into three categories: filter, wrapper and embedded methods. A wrapper method performs a forward or backward strategy in the space of all possible feature subsets, using a classifier of choice to evaluate each subset. Although this method has high accuracy, the exponential\nnumber of possible subsets makes the method computationally expensive in general [Kohavi and John 1997]. The embedded methods attempt to simultaneously maximize classification performance and minimize the number of features used based on a classification or regression model with specific penalties on coefficients of features [Tibshirani 1996; Weston et al. 2000; Zhou et al. 2011]. A filter method is independent of any classifiers, and applies evaluation measures such as distance, information, dependency, or consistency to select features [Dash and Liu 2003; Forman 2003; Peng et al. 2005; Song et al. 2012; Liu et al. 2014]. Then the filter methods build a classifier using selected features. Due to their simplicity and low computational cost, many filter methods have been proposed to solve the feature selection problem, such as the well-established mRMR (minimal-Redundancy-Maximal-Relevance) algorithm [Peng et al. 2005] and the FCBF (Fast Correlation-Based Filter) algorithm [Yu and Liu 2004]. Recently, Zhao et al. [Zhao et al. 2013] proposed a novel framework to consolidate different criteria to handle feature redundancies. To bring almost two decades of research on heuristic filter criteria under a single theoretical interpretation, Brown et al. [Brown et al. 2012] presented a unifying framework for information theoretic feature selection using an optimized loss function of the conditional likelihood of the training labels. To deal with high dimensionality, Tan et al. [Tan et al. 2010; Tan et al. 2014] proposed the efficient embedded algorithm, the FGM (Feature Generating Machine) algorithm, and Zhai et al. [Zhai et al. 2012] further presented the GDM (Group Discovery Machine) algorithm that outperforms the FGM algorithm. Group feature selection is also an interesting topic in batch methods, and it selects predictive feature groups rather than individual features. For instance, in image processing, each image can be represented by multiple kinds of groups, such as SIFT for shape information and Color Moment for color information. The lasso method (Least Absolute Shrinkage and Selection Operator) was proposed by Tibshirani [Tibshirani 1996] for shrinkage and variable selection, which minimizes the sum of squared errors with the L1 penalty on the sum of the absolute values of the coefficients of features. Based on the lasso method, Yuan and Lin [Yuan and Lin 2006] proposed a group lasso method to select grouped variables for accurate prediction in regression. Later, the sparse group lasso criterion as an extension of the group lasso method, was proposed by Friedman et al. [Friedman et al. 2010], which enables to encourage sparsity at the levels of both features and groups simultaneously. It is difficult for the standard batch method to operate on high dimensional data analytics that calls for dynamic feature selection, because the method has to access all features before feature selection can start. Online feature selection has two research lines. One assumes that the number of features on training data is fixed while the number of data points changes over time [Hoi et al. 2012]. Recently, Wang et al. [Wang et al. 2013] proposed an online feature selection method, OFS, which assumes data instances are sequentially added. Different from OFS, the other online approach assumes that the number of data instances is fixed while the number of features changes over time. Perkins and Theiler [Perkins and Theiler 2003] firstly proposed the Grafting algorithm based on a stagewise gradient descent approach. Grafting treats the selection of suitable features as an integral part of learning a predictor in a regularized learning framework, and operates in an incremental iterative fashion, gradually building up a feature set while training a predictor model using gradient descent. Zhou et al. [Zhou et al. 2006] presented Alpha-investing which sequentially considers new features as additions to a predictive model by modeling the candidate feature set as a dynamically generated stream. However, Alpha-investing requires the prior information of the original feature set and never evaluates the redundancy among the selected features.\nTo tackle the drawbacks, Wu et al. [Wu et al. 2010; Wu et al. 2013] presented the OSFS (Online Streaming Feature Selection) algorithm and its faster version, the FastOSFS algorithm. To handle online feature selection with grouped features, Wang et al. [Wang et al. 2013] proposed the OGFS (Online Group Feature Selection) algorithm. However, the computational cost inherent in those three algorithms may still be very expensive or prohibitive when the dimensionality is extremely high in the scale of millions or more. The big data challenges on efficient online processing and scalability motivate us to develop a scalable and online processing method to deal with data with extremely high dimensionality."}, {"heading": "3. THE SAOLA ALGORITHM FOR ONLINE FEATURE SELECTION", "text": ""}, {"heading": "3.1. Problem Definition", "text": "In general, a training data set D is defined by D = {(di, ci), 1 \u2264 i \u2264 N}, where N is the number of data instances, di is a multidimensional vector that contains numP features, and ci is a class label within the vector of the class attribute C. The feature set F on D is defined by F = {F1, F2, \u00b7 \u00b7 \u00b7 , FnumP }. The problem of feature selection on D is to select a subset of relevant features from F to maximize the performance of prediction models. The features in F are categorized into four disjoint groups, namely, strongly relevant, redundant, non-redundant, and irrelevant features [Kohavi and John 1997; Yu and Liu 2004], and the goal of feature selection is to remove redundant or irrelevant features from F while keeping strongly relevant or non-redundant features. The mathematical notations used in this paper are summarized in Table I.\nDefinition 3.1 (Irrelevant Feature). [Kohavi and John 1997] Fi is an irrelevant feature to C, if and only if \u2200S \u2286 F \\ {Fi} and \u2200fi, \u2200ci, \u2200s for which P (S = s, Fi = fi) > 0 and P (C = ci|S = s, Fi = fi) = P (C = ci|S = s).\nDefinition 3.2 (Markov Blanket [Koller and Sahami 1995]). A Markov blanket of feature Fi, denoted as M \u2286 F \\ {Fi} makes all other features independent of Fi given M , that is, \u2200Y \u2208 F \\ (M \u222a {Fi}) s.t. P (Fi|M,Y ) = P (Fi|M).\nBy Definition 3.2, a redundant feature is defined by [Yu and Liu 2004] as follows.\nDefinition 3.3 (Redundant Feature). A feature Fi \u2208 F is a redundant feature and hence should be removed from F , if it has a Markov blanket within F .\nWe also denoteD by D = {Fi, C}, 1 \u2264 i \u2264 numP , which is a sequence of features that is presented in a sequential order, where Fi = {f1, f2, ..., fN}\nT denotes the ith feature containing N data instances, and C denotes the vector of the class attribute. IfD is processed in a sequential scan, that is, one dimension at a time, we can process high-dimensional data not only with limited memory, but also without requiring its complete set of features available. The challenge is that, as we process one dimension at a time, at any time ti, how to online maintain a minimum size of feature subset S\u22c6ti of maximizing its predictive performance for classification. Assuming S \u2286 F is the feature set containing all features available till time ti\u22121, S \u22c6 ti\u22121 represents the selected feature set at ti\u22121, and Fi is a new coming feature at time ti, our problem can be formulated as follows:\nS\u22c6ti = argminS\u2032 {|S\u2032| : S\u2032 = argmax\n\u03b6\u2286{S\u22c6 ti\u22121 \u222aFi}\nP (C|\u03b6)}. (1)\nWe can further decompose it into the following key steps:\n\u2014Determine the relevance of Fi to C. Firstly, we determine whether Eq.(2) holds or not.\nP (C|Fi) = P (C). (2)\nIf so, Fi is cannot add any additional discriminative power with respect to C, thus Fi should be discarded. Hence, Eq.(2) helps us identify a new feature that either is completely useless by itself with respect to C, or needs to be combined with other features. If not, we further evaluate whether Fi carries additional predictive information to C given S\u22c6ti\u22121 , that is, whether Eq.(3) holds. If Eq.(3) holds, Fi has a Markov blanket in S\u22c6ti\u22121 , and thus Fi should be discarded.\nP (C|S\u22c6ti\u22121 , Fi) = P (C|S \u22c6 ti\u22121 ). (3)\n\u2014Calculate S\u22c6ti with the inclusion of Fi. Once Fi is added to S \u22c6 ti\u22121 , at time ti, Sti={S \u22c6 ti\u22121 ,\nFi}, we then solve Eq.(4) to prune Sti to satisfy Eq.(1).\nS\u22c6ti = argmax \u03b6\u2286Sti P (C|\u03b6). (4)\nAccordingly, solving Eq.(1) is decomposed to how to sequentially solve Eq.(2) to Eq.(4) at each time point. Essentially, Eq.(3) and Eq.(4) deal with the problem of feature redundancy."}, {"heading": "3.2. Using Mutual Information to Solve Eq.(1)", "text": "To solve Eq.(1), we will employ mutual information to calculate correlations between features. Given two features Y and Z, the mutual information between Y and Z is defined as follows.\nI(Y ;Z) = H(Y )\u2212H(Y |Z). (5)\nThe entropy of feature Y is defined as\nH(Y ) = \u2212\u03a3yi\u2208Y P (yi) log2 P (yi). (6)\nAnd the entropy of Y after observing values of another feature Z is defined as\nH(Y |Z) = \u2212\u03a3zj\u2208ZP (zj)\u03a3yi\u2208Y P (yi|zi) log2 P (yi|zi), (7)\nwhere P (yi) is the prior probability of value yi of feature Y , and P (yi|zi) is the posterior probability of yi given the value zi of feature Z. According to Eq.(6) and Eq.(7), the joint entropy H(X,Y ) between features X and Y is defined as follows.\nH(X,Y ) = \u2212\u03a3xi\u2208X\u03a3yi\u2208Y P (xi, yi) log2 P (xi, yi) = \u2212\u03a3xi\u2208X\u03a3yi\u2208Y P (xi, yi) log2 P (xi)P (yi|xi) = \u2212\u03a3xi\u2208XP (xi) log2 P (xi)\u2212 (\u2212\u03a3xi\u2208X\u03a3yi\u2208Y P (xi, yi) log2 P (yi|xi)) = H(X) +H(Y |X).\n(8)\nFrom Equations (5) to (8), the conditional mutual information is computed by\nI(X ;Y |Z) = H(X |Z)\u2212H(X |Y Z) = H(X,Z) +H(Y, Z)\u2212H(X,Y, Z)\u2212H(Z).\n(9)\nWhy can we use mutual information to solve Eq.(1)? Based on the work of [Brown et al. 2012], Eq.(1) is to identify a minimal subset of features to maximize the conditional likelihood of the class attribute C. Let S = {S\u03b8 \u222a S\u03b8\u0304} represent the feature set containing all features available at time ti where S\u03b8 indicates the set of selected features and S\u03b8\u0304 denotes the unselected features. Assuming p(C|S\u03b8) denotes the true class distribution while q(C|S\u03b8) represents the predicted class distribution given S\u03b8, then Eq.(1) can be reformulated as L(C|S\u03b8, D) = \u220fN k=1 q(c k|Sk\u03b8 ), where L(C|S\u03b8, D) denotes the conditional likelihood of the class attribute C given S\u03b8 and D. The conditional log-likelihood of L(C|S\u03b8, D) is calculated as follows.\n\u2113(C|S\u03b8, D) = 1\nN\nN \u2211\nk=1\nlog q(ck|Sk\u03b8 ) (10)\nBy the work of [Brown et al. 2012], Eq.(10) can be re-written as follows.\n\u2113(C|S\u03b8, D) = 1\nN\nN \u2211\nk=1\nlog q(ck|Sk\u03b8 )\np(ck|Sk\u03b8 ) +\n1\nN\nN \u2211\nk=1\nlog p(ck|Sk\u03b8 )\np(ci|Sk) +\n1\nN\nN \u2211\nk=1\nlog p(ck|Sk) (11)\nTo negate Eq.(11) and use Exy to represent statistical expectation, the following\nequation holds1.\n\u2212 \u2113(C|S\u03b8, D) = Exy\n{\nlog p(ck|Sk\u03b8 )\nq(ck|Sk\u03b8 )\n}\n+ Exy\n{\nlog p(ck|Sk)\np(ck|Sk\u03b8 )\n}\n\u2212 Exy\n{\nlog p(ck|Sk)\n}\n(12)\nIn Eq.(12), the first term is a likelihood ratio between the true and predicted class distributions given S\u03b8, averaged over the input data space. The second term equals to I(C;S\u03b8\u0304|S\u03b8), that is, the conditional mutual information between C and S\u03b8\u0304, given S\u03b8 [Brown et al. 2012]. The final term is H(C|S), the conditional entropy of C given all features, and is an irreducible constant.\nDefinition 3.4 (Kullback Leibler distance [Kullback and Leibler 1951]). The Kullback Leibler distance between two probability distributions P (X) and Q(X) is defined as KL(P (X)||Q(X)) = \u03a3xi\u2208XP (xi) log P (xi) Q(xi) = Ex log { P (X) Q(X)}.\n1Please refer to Section 3.1 of [Brown et al. 2012] for the details on how to get Eq.(11) and Eq.(12).\nThen Eq.(12) can be re-written as follows.\nlim N\u2192\u221e \u2212\u2113(C|S\u03b8, D) = KL(p(C|S\u03b8)||q(C|S\u03b8)) + I(C;S\u03b8\u0304|S\u03b8) +H(C|S) (13)\nWe estimate the distribution q(ck|Sk\u03b8 ) using discrete data. The probability of a value ck of X, p(C = xk) is estimated by maximum likelihood, the frequency of occurrences of {C = ck} divided by the total number of data instances N . Since the Strong Law of Large Numbers assures that the sample estimate using q converges almost surely to the expected value (the true distribution p), in Eq.(13), KL(p(C|S\u03b8)||q(C|S\u03b8)) will approach zero with a large N [Shlens 2014]. Since I(C;S) = I(C;S\u03b8) + I(C;S\u03b8\u0304|S\u03b8) holds, minimizing I(C;S\u03b8\u0304|S\u03b8) is equivalent to maximizing I(C;S\u03b8). Accordingly, by Eq.(13), the relationship between the optima of the conditional likelihood and that of the conditional mutual information is achieved as follows.\nargmax S\u03b8 L(C|S\u03b8, D) = argmin S\u03b8 I(C;S\u03b8\u0304|S\u03b8) (14)\nEq.(14) concludes that if I(C;S\u03b8\u0304|S\u03b8) is minimal, then L(C|S\u03b8, D) is maximal. Therefore, using mutual information as a correlation measure between features, we propose a series of heuristic solutions to Eq.(2), Eq.(3), and Eq.(4) in the next section."}, {"heading": "3.3. The Solutions to Equations (2) to (4)", "text": "We can apply Definitions 3.2 and 3.3 to solve Eq.(3) and Eq.(4). However, it is computationally expensive to use Definitions 3.2 and 3.3 when the number of features within S\u22c6ti\u22121 is large. Due to evaluating whether Fi is redundant with respect to S \u22c6 ti\u22121 using the standard Markov blanket filtering criterion (Definitions 3.2 and 3.3), it is necessary to check all the subsets of S\u22c6ti\u22121 (the total number of subsets is 2 |S\u22c6ti\u22121 |) to determine which subset subsumes the predictive information that Fi has about C, i.e., the Markov blanket of Fi. If such a subset is found, Fi becomes redundant and is removed. When handling a larger number of features, it is computationally prohibitive to check all the subsets of S\u22c6ti\u22121 .\nAccordingly, methods such as greedy search are a natural to address this problem. In the work of [Wu et al. 2013], a k-greedy search strategy is adopted to evaluate redundant features. It checks all subsets of size less than or equal to \u03b9 (1 \u2264 \u03b9 \u2264 |S\u22c6ti\u22121 |), where \u03b9 is a user-defined parameter. However, when the size of S\u22c6ti\u22121 is large, it is still computationally prohibitive to evaluate the subsets of size up to \u03b9. Moreover, selecting a proper value of \u03b9 is difficult. Therefore, those challenges motivate us to develop a scalable and online processing method to solve Eq.(3) and Eq.(4) for big data analytics. In this section, to cope with computational complexity, we propose a series of heuristic solutions for Equations (2) to (4) using pairwise comparisons to calculate online the correlations between features, instead of computing the correlations between features conditioned on all feature subsets.\n3.3.1. Solving Eq.(2). Assuming S\u22c6ti\u22121 is the selected feature subset at time ti\u22121, and at time ti, a new feature Fi comes, to solve Eq.(2), given a relevance threshold \u03b4 (we will provide the detailed discussion of the parameter \u03b4 in Section 3.4.2), if I(Fi;C) > \u03b4 (0 \u2264 \u03b4 < 1), Fi is said to be a relevant feature to C; otherwise, Fi is discarded as an irrelevant feature and will never be considered again.\n3.3.2. Solving Eq.(3). If Fi is a relevant feature, at time ti, how can we determine whether Fi should be kept given S \u22c6 ti\u22121 , that is, whether I(C;Fi|S \u22c6 ti\u22121\n) = 0? If \u2203Y \u2208 S\u22c6ti\u22121 such that I(Fi;C|Y ) = 0, it testifies that adding Fi alone to S \u22c6 ti\u22121 does not increase the\npredictive capability of S\u22c6ti\u22121 . With this observation, we solve Eq.(3) with the following lemma. Lemma 1 I(X ;Y |Z) \u2265 0. Lemma 2 With the current feature subset S\u22c6ti\u22121 at time ti\u22121 and a new feature Fi at time ti, if \u2203Y \u2208 S \u22c6 ti\u22121 such that I(Fi;C|Y ) = 0, then I(Fi;Y ) \u2265 I(Fi;C).\nProof. Considering Eq.(5) and Eq.(9), the following holds.\nI(Fi;C) + I(Fi;Y |C) = H(Fi)\u2212H(Fi|C) +H(Fi|C)\u2212H(Fi|Y C) = H(Fi)\u2212H(Fi|Y C).\n(15)\nI(Fi;Y ) + I(Fi;C|Y ) = H(Fi)\u2212H(Fi|Y ) +H(Fi|Y )\u2212H(Fi|Y C) = H(Fi)\u2212H(Fi|Y C).\n(16)\nBy Equations (15) and (16), the following holds.\nI(Fi;C|Y ) = I(Fi;C) + I(Fi;Y |C)\u2212 I(Fi;Y ). (17)\nWith Eq.(17), if I(Fi;C|Y ) = 0 holds, we get the following,\nI(Fi;Y ) = I(Fi;C) + I(Fi;Y |C). (18)\nUsing Eq.(18) and Lemma 1, the bound of I(Fi;Y ) is achieved.\nI(Fi;Y ) \u2265 I(Fi;C). (19)\nLemma 2 proposes a pairwise correlation bound between features to testify whether a new feature can increase the predictive capability of the current feature subset. Meanwhile, if I(Fi;C|Y ) = 0 holds, Lemma 3 answers what the relationship between I(Y ;C) and I(Fi;C) is.\nLemma 3 With the current feature subset S\u22c6ti\u22121 at time ti\u22121 and a new feature Fi at\ntime ti, \u2203Y \u2208 S \u22c6 ti\u22121 , if I(Fi;C|Y ) = 0 holds, then I(Y ;C) \u2265 I(Fi;C).\nProof. With Eq.(9), we get I(Y ;Fi|C) = I(Fi;Y |C). With Eq.(18) and the following equation,\nI(Y ;C|Fi)\u2212 I(Y ;C) = I(Y ;Fi|C) \u2212 I(Fi;Y ). (20)\nwe get the following,\nI(Y ;C|Fi) = I(Y ;C)\u2212 I(Fi;C).\nCase 1: if I(Y ;C|Fi) = 0, then the following equation holds.\nI(Y ;C) = I(Fi;C). (21)\nCase 2: if I(Y ;C|Fi) > 0, we get the following.\nI(Y ;C) > I(Fi;C). (22)\nBy Eq.(21) and Eq.(22), Lemma 3 is proven. According to Lemma 3, we can see that if I(Y ;C|Fi) = 0 and I(Fi;C|Y ) = 0, then I(Y ;C) exactly equals to I(Fi;C). Fi and Y can replace each other. In Lemma 3, if we only consider Case 2, by Lemma 2, with the current feature subset S\u22c6ti\u22121 at time ti\u22121 and a new feature Fi at time ti, \u2203Y \u2208 S \u22c6 ti\u22121 , if I(Fi;C|Y ) = 0 holds, then the following is achieved.\nI(Y ;C) > I(Fi;C) and I(Fi;Y ) \u2265 I(Fi;C). (23)\nWith Eq.(23), we deal with Eq.(3) as follows. With a new feature Fi at time ti, \u2203Y \u2208 S\u22c6ti\u22121 , if Eq.(23) holds, then Fi is discarded; otherwise, Fi is added to S \u22c6 ti\u22121 .\n3.3.3. Solving Eq.(4). Once Fi is added to S\u2217ti\u22121 at time ti, we will check which features within S\u2217ti\u22121 can be removed due to the new inclusion of Fi. If \u2203Y \u2208 S \u22c6 ti\u22121 such that I(C;Y |Fi) = 0, then Y can be removed from S \u22c6 ti\u22121 .\nSimilar to Eq.(17) and Eq.(18), if I(C;Y |Fi) = 0, we have I(Y ;Fi) \u2265 I(Y ;C). At the same time, if I(C;Y |Fi) = 0, similar to Eq.(22), we can get,\nI(Fi;C) > I(Y ;C). (24)\nWith the above analysis, we get the following,\nI(Fi;C) > I(Y ;C) and I(Y ;Fi) \u2265 I(Y ;C). (25)\nAccordingly, the solution to Eq.(4) is as follows. With the feature subset S\u2217ti at time ti and Fi \u2208 S \u2217 ti , if \u2203Y \u2208 S\u2217ti such that Eq.(25) holds, then Y can be removed from S \u2217 ti ."}, {"heading": "3.4. The SAOLA Algorithm and An Analysis", "text": "Using Eq.(23) and Eq.(25), we propose the SAOLA algorithm in detail, as shown in Algorithm 1. The SAOLA algorithm is implemented as follows. At time ti, as a new feature Fi arrives, if I(Fi, C) \u2264 \u03b4 holds at Step 5, then Fi is discarded as an irrelevant feature and SAOLA waits for a next coming feature; if not, at Step 11, SAOLA evaluates whether Fi should be kept given the current feature set S \u2217 ti\u22121 . If \u2203Y \u2208 S\u22c6ti\u22121 such that Eq.(18) holds, we discard Fi and never consider it again. Once Fi is added to S \u2217 ti\u22121 at time ti, S \u2217 ti\u22121 will be checked whether some features within S\u2217ti\u22121 can be removed due to the new inclusion of Fi. At Step 16, if \u2203Y \u2208 S \u22c6 ti\u22121 such that Eq.(20) holds, Y is removed.\nALGORITHM 1: The SAOLA Algorithm.\n1: Input: Fi: predictive features, C: the class attribute; \u03b4: a relevance threshold (0 \u2264 \u03b4 < 1), S\u22c6ti\u22121 : the selected feature set at time ti\u22121;\nOutput: S\u2217ti : the selected feature set at time ti; 2: repeat 3: get a new feature Fi at time ti; 4: /*Solve Eq.(2)*/ 5: if I(Fi;C) \u2264 \u03b4 then 6: Discard Fi; 7: Go to Step 21; 8: end if 9: for each feature Y \u2208 S\u2217ti\u22121 do 10: /*Solve Eq.(3)*/ 11: if I(Y ;C) > I(Fi;C) & I(Fi;Y ) \u2265 I(Fi;C) then 12: Discard Fi; 13: Go to Step 21; 14: end if 15: /*Solve Eq.(4)*/ 16: if I(Fi;C) > I(Y ;C) & I(Fi;Y ) \u2265 I(Y ;C) then 17: St\u2217\ni\u22121 = St\u2217 i\u22121 \u2212 Y ;\n18: end if 19: end for 20: S\u2217ti = St\u2217i\u22121 \u222a Fi; 21: until no features are available 22: Output S\u2217ti ;\n3.4.1. The Approximation of SAOLA. To reduce computational cost, the SAOLA algorithm conducts a set of pairwise comparisons between individual features instead of conditioning on a set of features, as the selection criterion for choosing features. This is essentially the idea behind the well-established batch feature selection algorithms, such as mRMR [Peng et al. 2005] and FCBF [Yu and Liu 2004]. Due to pairwise comparisons, our algorithm focuses on finding an approximate Markov blanket (the parents and children of the class attribute in a Bayesian network [Aliferis et al. 2010]) and does not attempt to discover positive interactions between features (there exists a positive interaction between Fi and Fj with respect to C even though Fi is completely useless by itself with respect to C, but Fi can provide significantly discriminative power jointly with Fj [Jakulin and Bratko 2003; Zhao and Liu 2007]). In the following, we will discuss the upper bound of the gap of information gain between an approximate Markov blanket and an optimal feature set for feature selection. Given a data set D, by Definition 3.2 in Section 3.1, if we have the optimal feature subset M \u2208 S, that is the Markov blanket of C at time ti, and S\u03b8 \u2208 S is the feature set selected by SAOLA, and S\u03b8\u0304 represents {S \\ S\u03b8}, then according to the chain rule of mutual information, we get I((S\u03b8, S\u03b8\u0304);C) = I(S\u03b8;C)+ I(C;S\u03b8\u0304|S\u03b8). Thus, when S\u03b8 takes the values of the optimal feature subset M , which perfectly captures the underlying distribution p(C|M), then I(C;S\u03b8\u0304|S\u03b8) would be zero. By Eq.(13), we get the following.\n\u2212\u2113(C|S\u03b8, D) = KL(p(C|S\u03b8)||q(C|S\u03b8)) + I(C;S\u03b8\u0304|S\u03b8) +H(C|S) \u2264 KL(p(C|S\u03b8)||q(C|S\u03b8)) + I(C;M) +H(C|S)\n(26)\nMeanwhile, in Eq.(26), the value of KL(p(C|S\u03b8)||q(C|S\u03b8)) depends on how well q can approximate p, given the selected feature set S\u03b8. By Eq.(13) in Section 3.2, KL(p(C|S\u03b8)||q(C|S\u03b8)) will approach zero as N \u2192 \u221e. Therefore, the upper bound of the gap of information gain between the selected features and optimal features can be re-written as Eq.(27). Closer S\u03b8 is to M , smaller the gap in Eq.(27) is.\nlim N\u2192\u221e \u2212\u2113(C|S\u03b8, D) \u2264 I(C;M) +H(C|S) (27)\nOur empirical results in Section 5.4 have validated that the gap between an optimal algorithm (exact Markov blanket discovery algorithms) and SAOLA, is small using small sample-to-ratio data sets in real-world. Furthermore, SAOLA (pairwise comparisons) is much more scalable than exact Markov blanket discovery algorithms (conditioning on all possible feature subsets) when the number of data instances or dimensionality is large.\n3.4.2. Handling Data with Continuous Values. Finally, for data with discrete values, we use the measure of mutual information, while for data with continuous values, we adopt the best known measure of Fisher\u2019s Z-test [Pen\u0303a 2008] to calculate correlations between features. In a Gaussian distribution,Normal(\u00b5,\u03a3), the population partial correlation p(FiY |S) between feature Fi and feature Y given a feature subset S is calculated as follows.\np(FiY |S) = \u2212((\n\u2211\nFiY S )\u22121)FiY\n(( \u2211 FiY S )\u22121)FiFi(( \u2211 FiY S )\u22121)Y Y\n(28)\nIn Fisher\u2019s Z-test, under the null hypothesis of conditional independence between Fi and Y given S, p(FiY |S) = 0. Assuming \u03b1 is a given significance level and \u03c1 is the p-value returned by Fisher\u2019s Z-test, under the null hypothesis of the conditional independence between Fi and Y , Fi and Y are uncorrelated to each other, if \u03c1 > \u03b1; otherwise, Fi and Y are correlated to each other, if \u03c1 \u2264 \u03b1. Accordingly, at time t, a new feature Fi correlated to C is discarded given S\u2217ti\u22121 , if \u2203Y \u2208 S \u2217 ti\u22121 s.t. pY,C > pFi,C and pY,Fi > pFi,C .\n3.4.3. The Parameters of SAOLA. In Algorithm 1, we discuss the parameters used by the SAOLA algorithm in detail as follows.\n\u2014Relevance threshold \u03b4. It is a user-defined parameter to determine relevance thresholds between features and the class attribute. We calculate symmetrical uncertainty [Press et al. 1996] instead of I(X,Y ), which is defined by\nSU(X,Y ) = 2I(X,Y )\nH(X) +H(Y ) .\nThe advantage of SU(X,Y ) over I(X,Y ) is that SU(X,Y ) normalizes the value of I(X,Y ) between 0 and 1 to compensate for the bias of I(X,Y ) toward features with more values. In general, we set 0 \u2264 \u03b4 < 1. \u2014Correlation bounds of I(Fi;Y ). According to Eq.(23) and Eq.(25), at Steps 11 and 16 of the SAOLA algorithm, I(Fi;C) and I(Y ;C) (min(I(Fi;C), I(Y ;C))) are the correlation bounds of I(Fi;Y ), respectively. To further validate the correlation bounds, at Steps 11 and 16, by setting I(Y ;C) and I(Fi;C) tomax(I(Fi;C), I(Y ;C)) respectively, we can derive a variant of the SAOLA algorithm, called SAOLA-max (the SAOLAmax algorithm uses the same parameters as the SAOLA algorithm, except for the correlation bounds of I(Fi;Y ) in Steps 11 and 16). We will conduct an empirical study on the SAOLA and SAOLA-max algorithms in Section 5.4.1. \u2014Selecting a fixed number of features. For different data sets, using the parameters \u03b1 or \u03b4, SAOLA returns a different number of selected features. Assuming the number of selected features is fixed to k, to modify our SAOLA to select k features, a simple way is to keep the top k features in the current selected feature set S\u2217ti with the highest correlations with the class attribute while dropping the other features from S\u2217ti after Step 20 in Algorithm 1.\n3.4.4. The Time Complexity of SAOLA. The major computation in SAOLA is the computation of the correlations between features (Steps 5 and 11 in Algorithm 1). At time ti, assuming the total number of features is up to P and |S\u2217ti | is the number of the currently selected feature set, the time complexity of the algorithm is O(P |S\u2217ti |). Accordingly, the time complexity of SAOLA is determined by the number of features within |S\u2217ti |. But the strategy of online pairwise comparisons guarantees the scalability of SAOLA, even when the size of |S\u2217ti | is large. Comparing to SAOLA, Fast-OSFS employs a k-greedy search strategy to filter out redundant features by checking feature subsets for each feature in S\u2217ti . At time ti, the best time complexity of Fast-OSFS is O(|S\u2217ti |\u03b9 |S\u2217ti |), where \u03b9|S \u2217 ti | denotes all subsets of size less than or equal to \u03b9 (1 \u2264 \u03b9 \u2264 |S\u22c6ti\u22121 |) for checking. With respect to Alphainvesting, at time ti, the time complexity of Alpha-investing is O(P |S \u2217 ti |2). Since Alphainvesting only considers adding new features but never evaluates the redundancy of selected features, the feature set S\u2217ti always has a large size. Thus, when the size of candidate features is extremely high and the size of |S\u2217ti | becomes large, Alpha-investing and Fast-OSFS both become computationally intensive or even prohibitive. Moreover, how to select a suitable value of \u03b9 for Fast-OSFS in advance is a hard problem, since different data sets may require different \u03b9 to search for a best feature subset."}, {"heading": "4. A GROUP-SAOLA ALGORITHM FOR ONLINE GROUP FEATURE SELECTION", "text": "The SAOLA algorithm selects features only at the individual feature level. When the data possesses certain group structure, the SAOLA algorithm cannot directly deal with features with group structures. In this section, we extend our SAOLA algorithm, and propose a novel group-SAOLA algorithm to select feature groups which are sparse at the levels of both features and groups simultaneously in an online manner."}, {"heading": "4.1. Problem Definition", "text": "Suppose G = {G1, G2, \u00b7 \u00b7 \u00b7 , Gi, \u00b7 \u00b7 \u00b7 , GnumG} represents numG feature groups without overlapping, and Gi \u2282 F denotes the i\nth feature group. We denote D by D = {Gi, C}, 1 \u2264 i \u2264 numG}, which is a sequence of feature groups that is presented in a sequential order. If we process those numG groups in a sequential scan, at any time ti, the challenge is how to simultaneously optimize selections within each group as well as between those groups to achieve a set of groups, \u03a8ti , containing a set of selected groups that maximizes its predictive performance for classification. Assuming Gi\u22121 \u2282 G is the set of all feature groups available till time ti\u22121 and Gi is a new coming group at time ti, our problem can be formulated as follows:\n\u03a8ti = argmaxG\u03b6\u2286{Gi\u22121\u222aGi} P (C|G\u03b6) s.t. (a)\u2200Fi \u2208 Gj , Gj \u2282 \u03a8ti , P (C|Gj \\ {Fi}, Fi) 6= P (C|Gj \\ {Fi}) (b)\u2200Gj \u2282 \u03a8ti , P (C|\u03a8ti \\Gj , Gj) 6= P (C|\u03a8ti \\Gj).\n(29)\nEq. (29) attempts to yield a solution at time ti that is sparse at the levels of both intra-groups (constraint (a)) and inter-groups (constraint (b)) simultaneously for maximizing its predictive performance for classification.\nDefinition 4.1 (Irrelevant groups). If \u2203Gi \u2282 G s.t. I(C;Gi) = 0, thenGi is considered as an irrelevant feature group.\nDefinition 4.2 (Group redundancy in inter-groups). If \u2203Gi \u2282 G s.t. I(C;Gi|G \\Gi) = 0, then Gi is a redundant group.\nDefinition 4.3 (Feature redundancy in intra-groups). \u2200Fi \u2208 Gi, if \u2203S \u2282 Gi \\ {Fi} s.t. I(C;Fi|S) = 0, then Fi can be removed from Gi.\nWith the above definitions, our design to solve Eq. (29) consists of three key steps: at time ti, firstly, if Gi is an irrelevant group, then we discard it; if not, secondly, we evaluate feature redundancy in Gi to make it as parsimonious as possible at the intragroup level; thirdly, we remove redundant groups from the currently selected groups. The solutions to those three steps are as follows.\n\u2014The solution to remove irrelevant groups. At time ti, if group Gi \u2282 G comes, if \u2200Fi \u2208 Gi s.t. I(C;Fi) = 0 (or given a relevance threshold \u03b4, I(C;Fi) \u2264 \u03b4 (0 \u2264 \u03b4 < 1), then Gi is regarded as an irrelevant feature group, and thus it can be discarded. \u2014The solution to determine group redundancy in inter-groups. Assume \u03a8ti is the set of groups selected at time ti\u22121 and Gi is a coming group at time ti. If \u2200Fi \u2208 Gi, \u2203Fj \u2208 Gj , and Gj \u2282 \u03a8ti s.t. I(Fj ;C) > I(Fi;C) and I(Fj ;Fi) \u2265 I(Fi;C), then Gi is a redundant group, and then can be removed. \u2014The solution to feature redundancy in intra-groups. If Gi is not a redundant group, we further prune Gi to make it as parsimonious as possible using Theorem 1 in Section 3.2.2. If \u2203Fj \u2208 Gi s.t. \u2203Y \u2208 Gi \\ {Fj}, I(Y ;C) > I(Fj ;C) and I(Fj ;Y ) \u2265 I(Fj ;C) holds, then Fj can be removed from Gi."}, {"heading": "4.2. The Group-SAOLA Algorithm", "text": "With the above analysis, we propose the Group-SAOLA algorithm in Algorithm 2. In Algorithm 2, from Steps 5 to 8, if Gi is an irrelevant group, it will be discarded. If not, Step 11 and Step 16 prune Gi by removing redundant features from Gi. At Step 22 and Step 26, the group-SAOLA removes both redundant groups in {\u03a8ti\u22121 \u222aGi} and redundant features in each group currently selected. Our Group-SAOLA algorithm can\nALGORITHM 2: The group-SAOLA Algorithm.\n1: Input: Gi: feature group; C: the class attribute; \u03b4: a relevance threshold (0 \u2264 \u03b4 < 1); \u03a8ti\u22121 : the set of selected groups at time ti\u22121; Output: \u03a8ti : the set of selected groups at time ti; 2: repeat 3: A new group Gi comes at time ti; 4: /*Evaluate irrelevant groups*/ 5: if \u2200Fi \u2208 Gi, I(Fi;C) \u2264 \u03b4 then 6: Discard Gi; 7: Go to Step 39; 8: end if 9: /*Evaluate feature redundancy in Gi*/ 10: for j=1 to |Gi| do 11: if \u2203Y \u2208 {Gi \u2212 {Fj}}, I(Y ;C) > I(Fj ;C) & I(Y ;Fj) \u2265 I(Fj;C) then 12: Remove Fj from Gi; 13: Continue; 14: end if 15: /*Otherwise*/ 16: if I(Fj ;C) > I(Y ;C) & I(Fj ;Y ) \u2265 I(Y ;C) then 17: Remove Y from Gi; 18: end if 19: end for 20: /*Evaluate group redundancy in {\u03a8ti\u22121 \u222aGi}*/ 21: for j=1 to |\u03a8ti\u22121 | do 22: if \u2203Fk \u2208 Gj , \u2203Fi \u2208 Gi, I(Fi;C) > I(Fk;C) & I(Fi;Fk) \u2265 I(Fk;C) then 23: Remove Fk from Gj \u2282 \u03a8ti\u22121 ; 24: end if 25: /*Otherwise*/ 26: if I(Fk;C) > I(Fi;C) & I(Fk;Fi) \u2265 I(Fi;C) then 27: Remove Fi from Gi; 28: end if 29: if Gj is empty then 30: \u03a8ti\u22121 = \u03a8ti\u22121 \u2212Gj ; 31: end if 32: if Gi is empty then 33: Break; 34: end if 35: end for 36: if Gi is not empty then 37: \u03a8ti = \u03a8ti\u22121 \u222aGi; 38: end if 39: until no groups are available 40: Output \u03a8ti ;\nonline yield a set of groups that is sparse between groups as well as within each group simultaneously for maximizing its classification performance at any time ti."}, {"heading": "5. EXPERIMENT RESULTS", "text": ""}, {"heading": "5.1. Experiment Setup", "text": "We use fourteen benchmark data sets as our test beds, including ten high-dimensional data sets [Aliferis et al. 2010; Yu et al. 2008] and four extremely high-dimensional data sets, as shown in Table II. The first ten high-dimensional data sets include two biomedical data sets (hiva and breast-cancer), three NIPS 2003 feature selection chal-\nlenge data sets (dexter, madelon, and dorothea), and two public microarray data sets (lung-cancer and leukemia), two massive high-dimensional text categorization data sets (ohsumed and apcj-etiology), and the thrombin data set that is chosen from KDD Cup 2001. The last four data sets with extremely high dimensionality are available at the Libsvm data set website2. In the first ten high-dimensional data sets, we use the originally provided training and validation sets for the three NIPS 2003 challenge data sets and the hiva data set, and for the remaining six data sets, we randomly select instances for training and testing (see Table II for the number instances for training and testing.). In the news20 data set, we use the first 9996 data instances for training and the rest for testing while in the url1 data set, we use the first day data set (url1) for training and the second day data set (url2) for testing. In the kdd2010 and webspam data sets, we randomly select 20,000 data instances for training, and 100,000 and 78,000 data instances for testing, respectively. Thus, as for the lung-cancer, breast-cancer, leukemia, ohsumed, apcjetiology, thrombin, kdd10, and webspam data sets, we run each data set 10 times, and report the highest prediction accuracy and the corresponding running time and number of selected features. Our comparative study compares the SAOLA algorithm with the following algorithms:\n\u2014Three state-of-the-art online feature selection methods: Fast-OSFS [Wu et al. 2013], Alpha-investing [Zhou et al. 2006], and OFS [Wang et al. 2013]. Fast-OSFS and Alpha-investing assume features on training data arrive one by one at a time while OFS assumes data examples come one by one; \u2014Three batch methods: one well-established algorithm of FCBF [Yu and Liu 2004], and two state-of-the-art algorithms, SPSF-LAR [Zhao et al. 2013] and GDM [Zhai et al. 2012].\nThe algorithms above are all implemented in MATLAB except for the GDM algorithm that is implemented in C language. We use three classifiers, KNN and J48 provided in the Spider Toolbox3, and SVM4 to evaluate a selected feature subset in the experiments. The value of k for the KNN classifier is set to 1 and both SVM and KNN use the linear kernel. All experiments were conducted on a computer with Intel(R) i7-2600, 3.4GHz CPU, and 24GB memory. In the remaining sections, the parameter\n2http://www.csie.ntu.edu.tw/\u223ccjlin/libsvmtools/datasets/ 3http://people.kyb.tuebingen.mpg.de/spider/ 4http://www.csie.ntu.edu.tw/ cjlin/libsvm/\n\u03b4 for SAOLA is set to 0 for discrete data while the significance level \u03b1 for SAOLA is set to 0.01 for Fisher\u2019s Z-test for continuous data (the effect of \u03b4 and \u03b1 on SAOLA was given in Section 5.4.3.). The significance level is set to 0.01 for Fast-OSFS, and for Alpha-investing, the parameters are set to the values used in [Zhou et al. 2006]. We evaluate SAOLA and its rivals based on prediction accuracy, error bar, AUC, sizes of selected feature subsets, and running time. In the remaining sections, to further analyze the prediction accuracies and AUC of SAOLA against its rivals, we conduct the following statistical comparisons.\n\u2014Paired t-tests are conducted at a 95% significance level and the win/tie/lose (w/t/l for short) counts are summarized. \u2014To validate whether SAOLA and its rivals have no significant difference in prediction accuracy or AUC, we conduct the Friedman test at a 95% significance level [Dems\u030car 2006], under the null-hypothesis, which states that the performance of SAOLA and that of its rivals have no significant difference, and calculate the average ranks using the Friedman test (how to calculate the average ranks, please see [Dems\u030car 2006].). \u2014When the null-hypothesis at the Friedman test is rejected, we proceed with the Nemenyi test [Dems\u030car 2006] as a post-hoc test. With the Nemenyi test, the performance of two methods is significantly different if the corresponding average ranks differ by at least the critical difference (how to calculate the critical difference, please see [Dems\u030car 2006].).\nWe organize the remaining parts as follows. Section 5.2 compares SAOLA with online feature selection algorithms. Section 5.3 gives a comparison of SAOLA with batch feature selection methods, and Section 5.4 conducts an analysis of the effect of parameters on SAOLA. Section 5.5 compares Group-SAOLA with its rivals."}, {"heading": "5.2. Comparison of SAOLA with Three Online Algorithms", "text": "5.2.1. Prediction Accuracy, Running Time and the Number of Selected Features of SAOLA. Since Fast-OSFS and Alpha-investing can only deal with the first ten high-dimensional data sets in Table II due to high computational cost, we compare them with SAOLA only on the first ten high-dimensional data sets. Accordingly, in the following tables, the notation \u201c-\u201d denotes an algorithm fails to a data set because of excessive running time. The OFS algorithm is a recently proposed online feature selection method. Since OFS uses a user-defined parameter k to control the size of the final selected feature subset, we set k, i.e., the number of selected features to the top 5, 10, 15 ,..., 100 features, then selecting the feature set with the highest prediction accuracy as the reporting result. Tables III, IV, and V summarize the prediction accuracies of SAOLA against FastOSFS, Alpha-investing, and OFS using the KNN, J48 and SVM classifiers. The win/tie/loss (w/t/l for short) counts of SAOLA against Fast-OSFS, Alpha-investing, and OFS are summarized in Tables III, IV, and V. The highest prediction accuracy is highlighted in bold face. Tables VI and VII give the number of selected features and running time of SAOLA, Fast-OSFS, Alpha-investing, and OFS. We have the following observations. (1) SAOLA vs. Fast-OSFS. With the counts of w/t/l in Tables III and V, we observe that SAOLA is very competitive with Fast-OSFS. In Table IV, we can see that SAOLA is superior to Fast-OSFS. Fast-OSFS selects fewer features than SAOLA on all data sets as shown in Table VI. The explanation is that Fast-OSFS employs a k-greedy search strategy to filter out redundant features by checking the feature subsets in the current feature set for each feature while SAOLA only uses pairwise comparisons. But as shown in Table VII, this strategy makes Fast-OSFS very expensive in computa-\ntion and even prohibitive on some data sets, such as apcj-etiology and the last four extremely high-dimensional data sets of Table II, as the size of the current feature set is large at each time point. (2) SAOLA vs. Alpha-investing. From Tables III to V, we can see that SAOLA outperforms Alpha-investing onmost data sets using the three classifiers. Alpha-investing selects manymore features than SAOLA on ohsumed, apcj-etiology,dorothea, and throm-\nbin since Alpha-investing only considers to add new features but never evaluates the redundancy of selected features. An exception is that Alpha-investing only selects one feature on the dexter data set. A possible explanation is that the dexter data set is a very sparse real-valued data set. Furthermore, Alpha-investing is less efficient than SAOLA as shown in Table VII.\nTo validate whether SAOLA, Fast-OSFS, and Alpha-investing have no significant difference in prediction accuracy, with the Friedman test at 95% significance level, under the null-hypothesis, which states that the performance of SAOLA and that of Fast-OSFS and Alpha-investing have no significant difference, with respect to J48 in Table III, the average ranks for SAOLA, Fast-OSFS, and Alpha-investing are 2.35, 2.40, and 1.25 (the higher the average rank, the better the performance), respectively. The null-hypothesis is rejected. Then we proceed with the Nemenyi test as a post-hoc test. With the Nemenyi test, the performance of two methods is significantly different if the corresponding average ranks differ by at least the critical difference. With the Nemenyi test, the critical difference is up to 1.047. Thus, with the critical difference and the average ranks calculated above, the prediction accuracy of SAOLA and that of Fast-OSFS have no significant difference, but SAOLA is significantly better than Alpha-investing. As for the KNN classifier, the average ranks for SAOLA, Fast-OSFS, and Alphainvesting are 2.45, 1.85, and 1.705 in Table IV, respectively. Meanwhile, as for SVM in Table V, the average ranks for SAOLA, Fast-OSFS, and Alpha-investing are 2.30, 2.25, and 1.45, respectively. Using KNN and SVM, the null-hypothesis cannot be rejected, and thus, the prediction accuracy of SAOLA and that of Fast-OSFS and Alphainvesting have no significant difference. In summary, in prediction accuracy, SAOLA is very competitive with Fast-OSFS, and is superior to Alpha-investing. Furthermore, Fast-OSFS and Alpha-investing cannot deal with extremely high-dimensional data sets due to computational cost while SAOLA is accurate and scalable. (3) SAOLA vs. OFS. With Tables III to V, we evaluate whether the performance of SAOLA and that of OFS have no significant difference in prediction accuracy using the Friedman test at 95% significance level. For the J48 and SVM classifiers, we observe the same average ranks for SAOLA and OFS, 1.64 and 1.36, respectively. Regarding SVM, the average ranks for SAOLA and OFS are 1.61 and 1.39, respectively. Accordingly, although SAOLA is better than OFS on prediction accuracy using the w/t/l counts and the average ranks, SAOLA and OFS have no significant difference in prediction accuracy.\nHowever, from Table VI, we can see that SAOLA selects fewer features than OSF on all data sets except for hiva, breast-cancer, ohsumed, apcj-etiology, news20, and kdd10. Moreover, Table VII gives the running time of SAOLA and OFS. As for OFS, we record the running time of the feature subset with the highest accuracy as its running time. SAOLA is faster than OFS, except for the dorothea and thrombin data sets. The dorothea and thrombin data sets only include 800 samples and 2000 samples, respectively. When the number of data samples becomes large and the number of features of training data is increased to millions, OFS become very costly, and SAOLA is still scalable and efficient. The explanation is that the time complexity of SAOLA is determined by the number of features within the currently selected feature set, and the strategy of online pairwise comparisons makes SAOLA very scalable, even when the size of the current feature set is large. Moreover, setting a desirable size of a feature set selected by OFS in advance is a non-trivial task.\n5.2.2. AUC and Error Bar of SAOLA. In this section, we further evaluate SAOLA and the three online algorithms using the error bar and AUC metrics.\nTable VIII reports the average AUC of J48, KNN and SVM for each algorithm. Using the w/t/l counts, we can see that our SAOLA is better than Fast-OSFS and Alphainvesting. To further validate whether SAOLA, Fast-OSFS, and Alpha-investing have no significant difference in AUC, with the Friedman test, the null-hypothesis is rejected, and the average ranks calculated for SAOLA, Fast-OSFS, and Alpha-investing are 2.6, 2.1, and 1.3, respectively. Then we proceed with the Nemenyi test as a post-hoc test. With the Nemenyi test, the performance of two methods is significantly different if the corresponding average\nranks differ by at least the critical difference. With the Nemenyi test, the critical difference is up to 1.047. Accordingly, the AUC of SAOLA and that of Fast-OSFS have no significant difference, but SAOLA is significantly better than Alpha-investing. Figure 1 shows the average AUC of J48, KNN and SVM and its standard deviation for SAOLA and OFS. We can see that SAOLA outperforms to OFS on all 14 data sets. From Table VIII and Figure 1, we can conclude that none of SAOLA, Fast-OSFS, Alpha-investing, and OFS can effectively deal with highly class-imbalanced data sets, such as hiva, apcj-etiology, and ohsumed. Finally, we give the error bars of SAOLA, Fast-OSFS, Alpha-investing, and OFS. Since we randomly select instances from the lung-cancer, breastcancer, leukemia, ohsumed, apcj-etiology, thrombin, kdd10, and webspam data sets for training and testing, we only report the error bars of the four online algorithms on those eight data sets using the SVM classifier. For each data set, we randomly run each algorithm 10 times, and then compute the average prediction accuracy and the corresponding standard deviation. Figure 2 plots the average prediction accuracy and an error bar that denotes a distance of the standard deviation above and below this average prediction accuracy on each data set. Figure 2 shows that SAOLA achieves a higher average prediction accuracy and a lower standard deviation than Alphainvesting and OFS while being highly comparable with Fast-OSFS. This further confirms that SAOLA is very competitive with Fast-OSFS and superior to Alpha-investing and OFS.\n5.2.3. Stability of SAOLA. The stability of feature selection is one of the criteria to measure the performance of a feature selection algorithm by quantifying the \u2018similarity\u2019 between two selected feature sets, and was first discussed by Kalousis et al. [Kalousis et al. 2007]. In this section, we employ the measure proposed by Yu et al. [Yu et al. 2008] to evaluate the stabilities of SAOLA, Fast-OSFS, Alpha-investing, and OFS. This measure constructs a weighted complete bipartite graph, where the two node sets correspond to two different feature sets, and weights assigned to the arcs are the normalized mutual information between the features at the nodes, also sometimes referred to as the symmetrical uncertainty. The Hungarian algorithm is then applied to identify the maximum weighted matching between the two node sets, and the overall similarity between two sets is the final matching cost. To evaluate the stabilities, each data set was randomly partitioned into five folds, each fold containing 1/5 of all the samples. SAOLA and its rivals under comparison\nwere repeatedly applied to four out of the five folds. This process was repeated 30 times to generate different subsamples for each data set. Then the average stabilities over 30 subsamples on each data set are as the results of SAOLA, Fast-OSFS, Alphainvesting, and OFS. Figure 3 shows the stabilities of SAOLA, Fast-OSFS, and Alpha-investing (we do not plot the stability of Alpha-investing on the dexter data set, since Alpha-investing only selects one feature on the dexter data set.). We can see that the Alpha-investing algorithm is the most stable feature selection algorithm among the three online methods. The explanation is that the Alpha-investing algorithm only considers adding features while never removes redundant features. SAOLA and Fast-OSFS aim to select a minimum subset of features necessary for constructing a classifier of best predictive accuracy and discard features which are relevant to the class attribute but highly correlated to the selected ones. Among a set of highly correlated features, different ones may be selected under different settings of SAOLA and Fast-OSFS. Therefore, from Figure 3, we can see that SAOLA is very competitive with Fast-OSFS. Meanwhile, from Figure 4, we can observe that SAOLA is more stable than OFS. Such observation illustrates that even if OFS can select large subsets of features, it is still less stable than SAOLA. The possible explanation is that OFS assumes that data examples come one by one while SAOLA assume that features on training data arrive one by one at a time.\n5.2.4. The Effect of Large Data Sets on SAOLA. To evaluate the effect of large data sets on SAOLA, we use the data sets, connect-4 (60,000/7,557 objects and 126 attributes) (note: 60,000/7,557 objects denote 60,000 data instances for training while 7,557 ones for testing), ijcnn1 (190,000/1,681 objects and 22 attributes), covtype (571,012/10,000 objects and 54 attributes), poker (1,020,000/5,010 objects and 11 attributes), and realsim (70,000/2,309 objects and 20,958 attributes) from the machine learning data set repository 5.\n5http://mldata.org/repository/data/\nTable IX gives the running time of SAOLA and its rivals. In Table IX, \u201c-\u201d denotes that an algorithm fails to the data set due to the high computational cost (exceeding three days). We can see that SAOLA is more scalable to deal with data with large numbers of data instances than Fast-OSFS, Alpha-investing, and OFS. Furthermore, Figure 5 gives the prediction accuracy of the four algorithms using the SVM classifier. Since Fast-OSFS fails on the connect-4 and real-sim data sets while Alpha-investing cannot run on the real-sim data set, Figure 5 does not plot the prediction accuracies of Fast-OSFS and Alpha-investing on those data sets. SAOLA is better than Fast-OSFS and OFS and competitive with Alpha-investing on prediction accuracy."}, {"heading": "5.3. Comparison with the Three Batch Methods", "text": "5.3.1. Running Time, the Number of Selected Features, and Prediction Accuracy of SAOLA. Since FCBF and SPSF-LAR can only deal with the first ten high-dimensional data sets in Table II, in the following tables and figures, we compare FCBF and SPSF-LAR with our proposed algorithm only on those ten high-dimensional data sets in terms of size of selected feature subsets, running time, and prediction accuracy. The information threshold for FCBF is set to 0. We set the user-defined parameter k, i.e., the number of selected features to the top 5, 10, 15 ,..., 65 features for the SPSF-LAR algorithm, choose the feature subsets of the highest prediction accuracy, and record the running time and the size of this feature set as the running time and the number of selected features of SPSF-LAR, respectively. We also select the GDM algorithm [Zhai et al. 2012] which is one of the most recent batch feature selection methods in dealing with very large dimensionality. The GDM algorithm is an efficient embedded feature selection method using cutting plane strategy and correlation measures as constraints to minimize the correlation among the selected features. GDM uses a user-defined parameter to control the size of the final selected feature subset. We set the selected feature subset sizes to the top 10, 20, 30, ..., 260 features for the GDM algorithm, report the running time of the feature subset with the highest accuracy as the running time of GDM, and choose the highest prediction accuracies achieved among those selected feature subsets. From Figure 6, we can conclude that FCBF selects the most features among SAOLA, FCBF and SPSF-LAR while SAOLA and SPSF-LAR are similar to each other. As shown in Figure 7, we can observe that SAOLA is the fastest algorithm among SAOLA, FCBF and SPSF-LAR while SPSF-LAR is the slowest. The explanation is that the time complexity of the algorithm is O(numP |S\u2217ti |) where numP is the number of features\nand |S\u2217ti | is the number of selected features at time ti, while the time complexity of SPFS-LAR is O(numPNk + Nk3) where N is the number of data samples and k the number of selected features. The computational costs of SAOLA and FCBF are very competitive since both of them employ pairwise comparisons to calculate the correlations between features. But when the number of data instances or the number of features is large, SAOLA is faster than FCBF, such as on ohsumed, apcj-etiology, dorothea, and thrombin. A possible reason is that SAOLA online evaluates both the new coming features (Step 11) and the current feature set (Step 16) at each time point to make the selected feature set as parsimonious as possible, since the size of the selected feature set at each time point is the key to determine the running time of SAOLA. With the same information threshold \u03b4, FCBF prefers to selecting more features than SAOLA. Figure 8 shows the running time of SAOLA against GDM. Since GDM is implemented in C++, we developed a C++ version of SAOLA for the comparison with GDM, in addition to its Matlab version. In Figure 8, we only give the last four data sets with extremely high dimensionality in Table II, since on the the first ten data sets, the running time of both SAOLA and GDM is no more than ten seconds. We can see that although GDM is a wrapper-like feature selection method, both GDM and SAOLA are very efficient to handle extremely high-dimensional data sets. Except for the news20 data set, SAOLA is a little faster than GDM. On the sparse data sets, SAOLA is faster than GDM, while on the dense data sets, such as the news20 data set, GDM is faster than SAOLA. Finally, Figure 9 reports the number of selected features of SAOLA comparing to GDM. Except for the breast-cancer data set, SAOLA selects fewer features than GDM to achieve the very competitive prediction accuracy with GDM. Finally, Tables X to XII report the prediction accuracies of SAOLA against FCBF, SPSF-LAR, and GDM. With the counts of w/t/l in the last rows of Tables X to XII, we can see that even without requiring the entire feature set on a training data set in ad-\nvance, SAOLA is still very competitive with FCBF, SPSF-LAR and GDM in prediction accuracy. Using the Friedman test at 95% significance level, for J48, the average ranks for SAOLA, FCBF and SPSF-LAR are 2.05, 1.90, and 2.05, respectively. For KNN, the average ranks for SAOLA, Fast-OSFS, and SPSF-LAR are 1.90, 2.25 and 1.85, respectively, while regarding SVM, the average ranks are 2.15, 1.65, and 2.20. Thus, with the Friedman test at 95% significance level, using KNN, J48 and SVM, the null-hypothesis cannot be rejected, and thus SAOLA, FCBF and SPSF-LAR have no significant difference in prediction accuracy. Accordingly, we conclude that the performance of SAOLA is highly comparable to that of FCBF and SPSF-LAR. With regard to the prediction accuracies of SAOLA and GDM, we can see that our algorithm is very competitive with GDM on J48, KNN, and SVM. With the Friedman test at 95% significance level, for J48, the average ranks for SAOLA and GDM are 1.32 and 1.68, respectively. As for KNN, the average ranks for SAOLA and GDM are 1.39 and 1.61, respectively. Using KNN and J48, the null-hypothesis cannot be rejected, accordingly, the SAOLA and GDM do not have significant difference in prediction accuracy. As for SVM, the null-hypothesis is rejected, and the average ranks for SAOLA and GDM are 1.25 and 1.75, respectively. Then we proceed with the Nemenyi test as a posthoc test. With the Nemenyi test, the critical difference is up to 0.5238. Thus, with the critical difference and the average ranks calculated above, GDM is significantly better than SAOLA. From the results above, we can see that the GDM algorithm is inferior\nto SAOLA on some data sets, such as thrombin and news20, since those data sets are very sparse. However, Fisher\u2019s Z-test and information gain employed by SAOLA can deal with those spare data sets well. In summary, our SAOLA algorithm is a scalable and accurate online approach.Without requiring a complete set of features on a training data set before feature selection starts, SAOLA is very competitive with the well-established and state-of-the-art FCBF, SPSF-LAR, and GDM methods.\n5.3.2. AUC and Error Bar of SAOLA. In this section, we compare SAOLA with the three batch algorithms using the error bar and AUC metrics. Table XIII reports the average AUC of J48, KNN and SVM for each algorithm. Using the w/t/l counts, we can see that our SAOLA is very competitive with FCBF and SPSFLAR. To further validate with the Friedman test and the null-hypothesis, the average ranks calculated for SAOLA, FCBF and SPSF-LAR are 1.8, 1.95, and 2.15, respectively. Accordingly, the AUC of SAOLA and that of FCBF and SPSF-LAR have no significant difference. Figure 10 shows the average AUC of J48, KNN and SVM and its standard deviation for SAOLA and GDM. From Figure 10, GDM outperforms SAOLA on most data sets, but the AUC of SAOLA is highly comparable to that of GDM. Moreover, we can see that none of these algorithms, SAOLA, FCBF, SPSF-LAR, and GDM, can effectively deal with highly class-imbalanced data sets.\nWe give the error bars of SAOLA, FCBF, SPSF-LAR, and GDMusing SVM classifiers (on the lung-cancer, breast-cancer, leukemia, ohsumed, apcj-etiology, thrombin, kdd10, and webspam data sets) as shown in Figure 11. From Figure 11, SAOLA is very competitive with FCBF. Although GDM and SPSF-LAR achieve a higher prediction accuracy and a lower standard deviation than SAOLA, our SAOLA is highly comparable with those two batch methods."}, {"heading": "5.4. Comparison with Markov Blanket Discovery Algorithms", "text": "In this section, we compare SAOLA and FCBF (discovery of approximateMarkov blankets) with two state-of-the-art exact Markov blanket discovery algorithms, IAMB (Incremental Association Markov Blanket) [Tsamardinos and Aliferis 2003] and MMMB (Max-Min Markov Blanket) [Tsamardinos et al. 2006]. The IAMB algorithm finds Markov blankets conditioned on the selected feature set currently, while the MMMB algorithm discovers Markov blankets conditioned on all possible feature subsets of the selected feature set currently6. Under certain assumptions (sufficient number of data instances and reliably statistical tests), IAMB and MMMB can return the Markov blanket of a given target feature [Pen\u0303a et al. 2007; Aliferis et al. 2010]. Using the four NIPS2003 feature selection challenge data sets, arcene (100 data instances, 10,000 features), dexter (300 data instances, 20,000 features), dorothea (800 data instances,\n6We implement IAMB and MMMB using the package of Causal Explorer at http://www.dsl-lab.org/causal explorer/.\n100,000 features), and gisette (6,000 data instance, 5,000 features), we empirically study SAOLA, FCBF, IAMB, and MMMB using SVM, KNN, and J48. From Table XIV to Table XVI, we can see that using small sample-to-feature ratio data sets, such as arcene, dexter, dorothea, SAOLA and FCBF are competitive with IAMB and MMMB, and even better than IAMB and MMMB sometimes. The explanation is that the number of data instances required by IAMB to identify a Markov blanket is at least exponential in the size of the Markov blanket since IAMB considers a conditional independence test to be reliable when the number of data instances in D is at least five times the number of degrees of freedom in the test. MMMB mitigates this problem to some extent. But the performance of IAMB andMMMBmay be inferior to SAOLA and FCBF as the sample-to-feature ratio becomes very small. Furthermore, as the size of a Markov blanket becomes large, MMMB is very slow due to expensive computation costs, such as on the dorothea data set (the running time exceeds three days). But on the gisette data set, due to the large number of data instances, IAMB and MMMB are significantly better than SAOLA and FCBF, but MMMB and IAMB take much more running time, especially MMMB. Those empirical results conclude that the\nperformance of SAOLA is very close to that of IAMB and MMMB on small sample-tofeature ratio data sets, but SAOLA is much more scalable than IAMB and MMMB on data sets with both many data instances and extremely high dimensionality."}, {"heading": "5.5. Analysis of the Effect of Parameters on SAOLA", "text": "5.5.1. Analysis of Correlation Bounds. In Section 3.3, we set the derived the correlation bound of I(Fi;Y ) to min(I(Fi;C), I(Y ;C)). In this section, we investigate SAOLA using the opposite bound, i.e., max(I(Fi;C), I(Y ;C)), which we term the SAOLA-max algorithm. In the experiments, SAOLA-max uses the same parameters as SAOLA. Table XVIII shows the prediction accuracies of SAOLA and SAOLA-max. With the summary of the w/t/l counts in Table XVIII, we can see that SAOLA is very competitive with SAOLA-max in prediction accuracy. With the Friedman test at 95% significance level, As for SVM, SAOLA gets the higher average rank than SAOLA-max. For KNN, the null-hypothesis cannot be rejected. The average ranks calculated from the Friedman test for SAOLA and SAOLA-max are 1.46 and 1.54, respectively. With respect to J48, the average ranks for SAOLA and SAOLA-max are 1.43 and 1.57, respectively. The Friedman test testifies that SAOLA and SAOLA-max have no significant difference in prediction accuracy, although SAOLA-max gets the higher average ranks using the J48 and KNN classifiers.\nMoreover, Figure 12 shows the average AUC of J48, KNN and SVM and its standard deviation for SAOLA and SAOLA-max. We can see that SAOLA and SAOLA-max are very competitive with each other on all 14 data sets. However, on the running time, Table XIX shows that SAOLA is much more efficient than SAOLA-max on all data sets, especially on those of extremely high dimensionality. In Table XIX, we can also see that SAOLA selects fewer features than SAOLA-max. The explanation is that SAOLA-max uses a bigger relevance threshold (\u03b42 = max(I(X ;C), I(Y ;C)) for removing redundant features than SAOLA (\u03b42 = min(I(X ;C), I(Y ;C)). Clearly, the larger the relevance threshold \u03b42, more features are added to the current feature set (see Steps 11 and 16 of Algorithm 1). Compared to SAOLA-max, we can conclude that it is accurate and scalable to use the correlation bound, \u03b42 = min(I(X ;C), I(Y ;C) in the SAOLA algorithm, for pairwise comparisons to filter out redundant features.\nFinally, we evaluate the stabilities of SAOLA and SAOLA-max using the stability measure proposed by Yu et al. [Yu et al. 2008]. Each data set was randomly partitioned into five folds, each fold containing 1/5 of all the samples. SAOLA and SAOLA-max were repeatedly applied to four out of the five folds. This process was repeated 30 times to generate different subsamples for each data set. Then the average stabilities over 30 subsamples on each data set are as the results of SAOLA and SAOLA-max. Figure 13 shows the stabilities of SAOLA and SAOLA-max. We can conclude that SAOLA is very\ncompetitive with SAOLA-max on the measure of stability, although SAOLA-max can select large subsets of features.\n5.5.2. The Effect of Input Order of Features. Since the dimensions are presented in a sequential scan, does the input order of the features have an impact on the quality of the selected feature set? To evaluate the effect on the SAOLA algorithm, we generate 30 trials in which each trial represents a random ordering of the features in the input feature set. We apply the SAOLA algorithm to each randomized trial and report the average prediction accuracies and standard deviations (accuracy\u00b1deviation) in Table XX (in the following sections, we only give the prediction accuracy of SAOLA using KNN and J48, since the prediction accuracy of SAOLA using SVM is very similar to that of SAOLA using KNN.).\nOn the last eight very high-dimensional data sets, the results in Table XX confirm that varying the order of the incoming features does not affect much the final outcomes. Our explanation is that with various feature orders, Steps 11 and 16 of Algorithm 1 can select the feature with the highest correlation with the class attribute among a set of correlated features and remove the corresponding correlated features of this feature. The only difference is that in some feature orders, the final feature subset may include some weakly relevant features. For example, assuming at time t, Fi arrives and has only one feature Y that satisfies Eq.(18) in the input features, and Y arrived before Fi and has stayed in the currently selected feature set S \u2217 ti\u22121 . Then Fi can be removed at time t given Y . But if Fi arrives before Y , and Y is removed before Fi\u2019s arrival, Fi cannot be removed later and may be kept in the final feature set. This also explains why there is a little fluctuation of standard deviations in Table XX.\n5.5.3. The Effect of \u03b4 and \u03b1. The SAOLA algorithm has two versions: SAOLA with information gain for discrete data and SAOLA with Fisher\u2019s Z-test for continuous data. For both versions, SAOLA needs to set a relevance threshold (\u03b4 in Algorithm 1) in advance to determine whether two features are relevant. For discrete data, we set 11 different relevance thresholds for SAOLA and tuned \u03b4 using cross validation on the dorothea and thrombin data sets. From Figure 14, we can see that in the term of prediction accuracy, the relevance thresholds do not have a significant impact on the SAOLA algorithm. For Fisher\u2019s Z-test, the relevance threshold is the significance level, \u03b1, and is always set to 0.01 or 0.05. Table XXI shows the results of SAOLA under the different significance levels. It is clear that a significant level does not impose a significant impact on the SAOLA algorithm either."}, {"heading": "5.6. Comparison of Group-SAOLA with OGFS and Sparse Group Lasso", "text": "In this section, we compare our group-SAOLA algorithm with the state-of-the-art online group feature selection methods, OGFS [Wang et al. 2013] and a well-established batch group feature selection method, Sparse Group Lasso [Friedman et al. 2010]. As for the first ten high-dimensional data sets in Table II, from dexter to thrombin, we randomly separated each data set into 100 feature groups without overlapping, while for\nthe last four data sets with extremely high dimensionality, each data set was randomly separated into 10,000 feature groups without overlapping. For parameter settings of the Sparse Group Lasso algorithm, \u03bb1 \u2208 [0.01, 0.1] and \u03bb2 \u2208 [0.01, 0.1]. For the groupSAOLA algorithm, the parameters, \u03b4 for discrete data and \u03b1 for continuous data, are the same as the SAOLA algorithm. In the following comparisons, since the prediction accuracy of SAOLA using SVM has no significant difference than that of SAOLA using KNN and J48, we only report the prediction accuracy of SAOLA using KNN and J48. In this section we compare group-SAOLA with OGFS and Sparse Group Lasso in terms of prediction accuracy, sizes of selected feature subsets, number of selected groups, and running time on the 14 high-dimensional data sets in Table II. We repeated this process 10 times to generate different sets of feature groups of each data set. The results as shown in Table XXII, Table XXIII, Figures 15 to 18 are the average ones on those 10 sets of feature groups. Table XXII summarizes the prediction accuracies of Group-SAOLA against OGFS and Sparse Group Lasso using the KNN and J48 classifiers. The highest prediction accuracy is highlighted in bold face. Table XXIII illustrates the running time, sizes of selected feature subsets, and numbers of selected groups of Group-SAOLA against OGFS and Sparse Group Lasso. In Tables XXII and XXIII, SGLasso is the abbreviation for Sparse Group Lasso.\n5.6.1. Comparison of Group-SAOLA with OGFS. In this section we compare OGFS with group-SAOLA on the 14 high-dimensional data sets in Table II, and the results are as shown in Tables XXII to XXIII, Figures 15 to 18. In Tables XXII, from the the win/tie/lose counts in the last rows of the table, we observe that Group-SAOLA never loses against OGFS on all of the 14 high-dimensional data sets. To evaluate whether the prediction accuracy of group-SAOLA and that of OGFS have no significant difference, using the Friedman test, for the J48 classifier, the nullhypothesis is rejected, and the average ranks for group-SAOLA and OGFS are 1.8929 and 1.1071, respectively. Then we proceed with the Nemenyi test as a post-hoc test, and the critical difference is up to 0.6885. The difference between 1.8929 and 1.1071 is bigger than this critical difference, then group-SAOLA is significantly better than OGFS in prediction accuracy. For the KNN classifier, the null-hypothesis cannot be rejected. The average ranks for group-SAOLA and OGFS are 1.75 and 1.25, respectively. Accordingly, for KNN, group-SAOLA and OGFS have no significant difference in prediction accuracy. Figure 15 gives the error bars (the left figure) and AUC (the right figure) of groupSAOLA and OGFS using the KNN classifier. We can see that group-SAOLA clearly outperforms OGFS using the AUC and error bar metrics. Furthermore, Table XXIII illustrates that group-SAOLA is faster than OGFS on most data sets. When the number of features increases to millions and the number of feature groups becomes large, OGFS becomes very costly, but our group-SAOLA is still scalable and efficient. The explanation is that the time complexity of group-SAOLA is determined by the number of features within the currently selected feature groups, and the strategy of online redundancy detection within the currently selected feature groups makes group-SAOLA very scalable. Meanwhile, from Table XXIII, we observe that group-SAOLA selects fewer features than OGFS. From the selected numbers of groups and selected numbers of features, we can see that group-SAOLA not only selects the smaller number of feature groups, but also achieves more sparsity of within groups than OGFS. Figure 16 gives the results of the numbers of selected groups and the corresponding prediction accuracy for group-SAOLA, and OGFS using the KNN classifier on the 14 data sets in Table II. The best possible mark for each graph is at the upper left corner, which selects the fewest groups with the highest prediction accuracy. We can see that group-SAOLA selects fewer groups while gets higher prediction accuracies than OGFS on all the 14 data sets in Table II, except for the ohsumed data set. However, on the ohsumed data set, on the prediction accuracy, group-SAOLA is very competitive with OGFS.\nFigure 17 gives the results of the numbers of selected groups and the corresponding prediction accuracy for Group-SAOLA and OGFS using the J48 classifier. We can see that Group-SAOLA selects fewer groups while gets higher prediction accuracies than OGFS on all the fourteen data sets.\n5.6.2. Comparison of Group-SAOLA with Sparse Group Lasso. Since Sparse Group Lasso7 can only deal with the first ten high-dimensional data sets in Table II due to high computational costs, in this section we compare it with group-SAOLA on those first ten high-dimensional data sets. With Table XXII, using the Friedman test, for the J48 classifier, the null-hypothesis cannot be rejected, and the average ranks for group-SAOLA and Sparse Group Lasso are 1.5 and 1.5, respectively. For the KNN classifier, the null-hypothesis is accepted,\n7The codes are available at http://yelab.net/software/SLEP/\nand the average ranks for group-SAOLA and Sparse Group Lasso are 1.6 and 1.4, respectively. Accordingly, for the J48 and KNN classifiers, group-SAOLA and Sparse Group Lasso have no significant difference in prediction accuracy. Furthermore, Table XXIII shows that group-SAOLA is much faster than Sparse Group Lasso, and group-SAOLA selects fewer features than Sparse Group Lasso. Figure 18 gives the AUC (the left figure) and error bars (the right figure) of groupSAOLA and Sparse Group Lasso using the KNN classifier. Figure 18 illustrates that group-SAOLA is very competitive with Sparse Group Lasso using the AUC and error bar metrics. Figures 16 and 17 give the results of the numbers of selected groups and the corresponding prediction accuracy for group-SAOLA and Sparse Group Lasso using the KNN and J48 classifiers, respectively. The best possible mark for each graph is at the upper left corner. We can see that group-SAOLA prefers to select few groups, in comparison to Sparse Group Lasso. Meanwhile, group-SAOLA is very competitive with Sparse Group Lasso in terms of prediction accuracy. In summary, the group-SAOLA algorithm is a scalable and accurate online group feature selection approach. This validates that without requiring a complete set of feature groups on a training data set before feature selection starts, group-SAOLA is very competitive comparing to the well-established the Sparse Group Lasso algorithm."}, {"heading": "6. CONCLUSIONS AND FUTURE WORK", "text": "In this paper, we presented the SAOLA algorithm, a scalable and accurate online approach to tackle feature selection with extremely high dimensionality. We conducted a theoretical analysis and derived a lower bound of correlations between features for pairwise comparisons, and then proposed a set of online pairwise comparisons to maintain a parsimonious model over time. To deal with the group structure information in features, we extended the SAOLA algorithm, and then proposed a novel group-SAOLA algorithm to deal with features that arrive by groups. The group-SAOLA algorithm can online maintain a set of feature groups that is sparse between groups and within each group simultaneously. Using a series of benchmark data sets, we compared the SAOLA and group-SAOLA algorithms with state-of-the-art online feature selection methods and well-established batch feature selection algorithms. The empirical study demonstrated that the SAOLA and group-SAOLA algorithms are both scalable on data sets of extremely high dimensionality, have superior performance over state-of-the-art online feature selection methods, and are very competitive with state-of-the-art batch feature selection methods in prediction accuracy, while much faster in running time.\nIn this work, we have used online pairwise comparisons to calculate the correlations between features without further exploring positive feature interactions between features. Moreover, from the AUC results reported in the work, we can see that SAOLA and its rivals, including the three online algorithms and three batch methods, cannot effectively deal with class-imbalanced data. Thus, we will further explore the following directions in online feature selection: efficient and effective methods to discover positive feature interactions between features, and accurate and scalable online algorithms to handle class-imbalanced data. Meanwhile, our empirical studies have validated that SAOLA (using pairwise comparisons) is competitive with IAMB and MMMB (using multiple comparisons). To conduct thoroughly theoretical analysis and empirical studies on why pairwise feature correlations (instead of conditioning on all possible feature subsets) may be sufficient in practice deserve further exploration in our future work."}, {"heading": "Acknowledgments", "text": "The preliminary version of this manuscript with the title \u201cTowards Scalable and Accurate Online Feature Selection for Big Data\u201d was published in the proceedings of 14th IEEE International Conference on Data Mining (ICDM2014), 660-669. This work is partly supported by a PIMS Post-Doctoral Fellowship Award of the Pacific Institute for the Mathematical Sciences, Canada."}], "references": [{"title": "Local causal and markov blanket induction for causal discovery and feature selection for classification part I: Algorithms and empirical evaluation", "author": ["Constantin F Aliferis", "Alexander Statnikov", "Ioannis Tsamardinos", "Subramani Mani", "Xenofon D Koutsoukos."], "venue": "Journal of Machine Learning Research 11 (2010), 171\u2013234.", "citeRegEx": "Aliferis et al\\.,? 2010", "shortCiteRegEx": "Aliferis et al\\.", "year": 2010}, {"title": "Conditional likelihood maximisation: A unifying framework for information theoretic feature selection", "author": ["Gavin Brown", "Adam Pocock", "Ming-Jie Zhao", "Mikel Luj\u00e1n."], "venue": "Journal of Machine Learning Research 13 (2012), 27\u201366.", "citeRegEx": "Brown et al\\.,? 2012", "shortCiteRegEx": "Brown et al\\.", "year": 2012}, {"title": "A hybrid memory built by SSD and DRAM to support in-memory Big Data analytics", "author": ["Zhiguang Chen", "Yutong Lu", "Nong Xiao", "Fang Liu."], "venue": "Knowledge and Information Systems 41, 2 (2014), 335\u2013354.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Consistency-based search in feature selection", "author": ["Manoranjan Dash", "Huan Liu."], "venue": "Artificial intelligence 151, 1 (2003), 155\u2013176.", "citeRegEx": "Dash and Liu.,? 2003", "shortCiteRegEx": "Dash and Liu.", "year": 2003}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["Janez Dem\u0161ar."], "venue": "The Journal of Machine Learning Research 7 (2006), 1\u201330.", "citeRegEx": "Dem\u0161ar.,? 2006", "shortCiteRegEx": "Dem\u0161ar.", "year": 2006}, {"title": "An extensive empirical study of feature selection metrics for text classification", "author": ["George Forman."], "venue": "The Journal of machine learning research 3 (2003), 1289\u20131305.", "citeRegEx": "Forman.,? 2003", "shortCiteRegEx": "Forman.", "year": 2003}, {"title": "A note on the group lasso and a sparse group lasso", "author": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani."], "venue": "arXiv preprint arXiv:1001.0736 (2010).", "citeRegEx": "Friedman et al\\.,? 2010", "shortCiteRegEx": "Friedman et al\\.", "year": 2010}, {"title": "An introduction to variable and feature selection", "author": ["Isabelle Guyon", "Andr\u00e9 Elisseeff."], "venue": "Journal of Machine Learning Research 3 (2003), 1157\u20131182.", "citeRegEx": "Guyon and Elisseeff.,? 2003", "shortCiteRegEx": "Guyon and Elisseeff.", "year": 2003}, {"title": "Online feature selection for mining big data", "author": ["Steven CHHoi", "Jialei Wang", "Peilin Zhao", "Rong Jin."], "venue": "Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications. ACM, 93\u2013100.", "citeRegEx": "CHHoi et al\\.,? 2012", "shortCiteRegEx": "CHHoi et al\\.", "year": 2012}, {"title": "Analyzing attribute dependencies", "author": ["Aleks Jakulin", "Ivan Bratko."], "venue": "PKDD 2003. Springer-Verlag, 229\u2013240.", "citeRegEx": "Jakulin and Bratko.,? 2003", "shortCiteRegEx": "Jakulin and Bratko.", "year": 2003}, {"title": "The correctness problem: evaluating the ordering of binary features in rankings", "author": ["Kashif Javed", "Mehreen Saeed", "Haroon A Babri."], "venue": "Knowledge and information systems 39, 3 (2014), 543\u2013563.", "citeRegEx": "Javed et al\\.,? 2014", "shortCiteRegEx": "Javed et al\\.", "year": 2014}, {"title": "Stability of feature selection algorithms: a study on high-dimensional spaces", "author": ["Alexandros Kalousis", "Julien Prados", "Melanie Hilario."], "venue": "Knowledge and information systems 12, 1 (2007), 95\u2013116.", "citeRegEx": "Kalousis et al\\.,? 2007", "shortCiteRegEx": "Kalousis et al\\.", "year": 2007}, {"title": "Wrappers for feature subset selection", "author": ["Ron Kohavi", "George H John."], "venue": "Artificial intelligence 97, 1 (1997), 273\u2013324.", "citeRegEx": "Kohavi and John.,? 1997", "shortCiteRegEx": "Kohavi and John.", "year": 1997}, {"title": "Toward optimal feature selection", "author": ["Daphne Koller", "Mehran Sahami."], "venue": "ICML-1995. 284\u2013292.", "citeRegEx": "Koller and Sahami.,? 1995", "shortCiteRegEx": "Koller and Sahami.", "year": 1995}, {"title": "On information and sufficiency", "author": ["Solomon Kullback", "Richard A Leibler."], "venue": "The Annals of Mathematical Statistics (1951), 79\u201386.", "citeRegEx": "Kullback and Leibler.,? 1951", "shortCiteRegEx": "Kullback and Leibler.", "year": 1951}, {"title": "An efficient orientation distance\u2013 based discriminative feature extraction method for multi-classification.Knowledge and information systems", "author": ["Bo Liu", "Yanshan Xiao", "S Yu Philip", "Zhifeng Hao", "Longbing Cao"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Toward integrating feature selection algorithms for classification and clustering", "author": ["Huan Liu", "Lei Yu."], "venue": "IEEE Transactions on Knowledge and Data Engineering 17, 4 (2005), 491\u2013502.", "citeRegEx": "Liu and Yu.,? 2005", "shortCiteRegEx": "Liu and Yu.", "year": 2005}, {"title": "Learning gaussian graphical models of gene networks with false discovery rate control", "author": ["Jose M Pe\u00f1a."], "venue": "EvoBIO-2008. 165\u2013176.", "citeRegEx": "Pe\u00f1a.,? 2008", "shortCiteRegEx": "Pe\u00f1a.", "year": 2008}, {"title": "Towards scalable and data efficient learning of Markov boundaries", "author": ["Jose M Pe\u00f1a", "Roland Nilsson", "Johan Bj\u00f6rkegren", "Jesper Tegn\u00e9r."], "venue": "International Journal of Approximate Reasoning 45, 2 (2007), 211\u2013232.", "citeRegEx": "Pe\u00f1a et al\\.,? 2007", "shortCiteRegEx": "Pe\u00f1a et al\\.", "year": 2007}, {"title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy", "author": ["Hanchuan Peng", "Fuhui Long", "Chris Ding."], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 27, 8 (2005), 1226\u20131238.", "citeRegEx": "Peng et al\\.,? 2005", "shortCiteRegEx": "Peng et al\\.", "year": 2005}, {"title": "Online feature selection using grafting", "author": ["Simon Perkins", "James Theiler."], "venue": "ICML. 592\u2013599.", "citeRegEx": "Perkins and Theiler.,? 2003", "shortCiteRegEx": "Perkins and Theiler.", "year": 2003}, {"title": "Numerical recipes in C", "author": ["William H Press", "Saul A Teukolsky", "William T Vetterling", "Brian P Flannery."], "venue": "Vol. 2. Citeseer.", "citeRegEx": "Press et al\\.,? 1996", "shortCiteRegEx": "Press et al\\.", "year": 1996}, {"title": "Notes on Kullback-Leibler Divergence and Likelihood", "author": ["Jonathon Shlens."], "venue": "arXiv preprint arXiv:1404.2000 (2014).", "citeRegEx": "Shlens.,? 2014", "shortCiteRegEx": "Shlens.", "year": 2014}, {"title": "Feature selection via dependence maximization", "author": ["Le Song", "Alex Smola", "Arthur Gretton", "Justin Bedo", "Karsten Borgwardt."], "venue": "Journal of Machine Learning Research 13 (2012), 1393\u20131434.", "citeRegEx": "Song et al\\.,? 2012", "shortCiteRegEx": "Song et al\\.", "year": 2012}, {"title": "Towards Ultrahigh Dimensional Feature Selection for Big Data", "author": ["Mingkui Tan", "Ivor W Tsang", "Li Wang."], "venue": "Journal of Machine Learning Research 15 (2014), 1371\u20131429.", "citeRegEx": "Tan et al\\.,? 2014", "shortCiteRegEx": "Tan et al\\.", "year": 2014}, {"title": "Learning sparse svm for feature selection on very high dimensional datasets", "author": ["Mingkui Tan", "Li Wang", "Ivor W Tsang."], "venue": "ICML-2010. 1047\u20131054.", "citeRegEx": "Tan et al\\.,? 2010", "shortCiteRegEx": "Tan et al\\.", "year": 2010}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani."], "venue": "Journal of the Royal Statistical Society. Series B (Methodological) (1996), 267\u2013288.", "citeRegEx": "Tibshirani.,? 1996", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "Towards principled feature selection: Relevancy, filters and wrappers", "author": ["Ioannis Tsamardinos", "Constantin F Aliferis."], "venue": "Proceedings of the ninth international workshop on Artificial Intelligence and Statistics. Morgan Kaufmann Publishers: Key West, FL, USA.", "citeRegEx": "Tsamardinos and Aliferis.,? 2003", "shortCiteRegEx": "Tsamardinos and Aliferis.", "year": 2003}, {"title": "The max-min hill-climbing Bayesian network structure learning algorithm", "author": ["Ioannis Tsamardinos", "Laura E Brown", "Constantin F Aliferis."], "venue": "Machine learning 65, 1 (2006), 31\u201378.", "citeRegEx": "Tsamardinos et al\\.,? 2006", "shortCiteRegEx": "Tsamardinos et al\\.", "year": 2006}, {"title": "Evolutionary Study of Web Spam: Webb Spam Corpus 2011 versus Webb Spam Corpus 2006", "author": ["De Wang", "Danesh Irani", "Calton Pu."], "venue": "CollaborateCom-2012. 40\u201349.", "citeRegEx": "Wang et al\\.,? 2012", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Online Feature Selection and Its Applications", "author": ["Jialei Wang", "Peilin Zhao", "Steven CH Hoi", "Rong Jin."], "venue": "IEEE Transactions on Knowledge and Data Engineering (2013), 1\u201314.", "citeRegEx": "Wang et al\\.,? 2013", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Feature selection for SVMs", "author": ["Jason Weston", "Sayan Mukherjee", "Olivier Chapelle", "Massimiliano Pontil", "Tomaso Poggio", "Vladimir Vapnik."], "venue": "NIPS, Vol. 12. 668\u2013674.", "citeRegEx": "Weston et al\\.,? 2000", "shortCiteRegEx": "Weston et al\\.", "year": 2000}, {"title": "Model mining for robust feature selection", "author": ["Adam Woznica", "Phong Nguyen", "Alexandros Kalousis."], "venue": "ACM SIGKDD-2012. ACM, 913\u2013921.", "citeRegEx": "Woznica et al\\.,? 2012", "shortCiteRegEx": "Woznica et al\\.", "year": 2012}, {"title": "Online feature selection with streaming features", "author": ["XindongWu", "Kui Yu", "Wei Ding", "HaoWang", "Xingquan Zhu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "XindongWu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "XindongWu et al\\.", "year": 2013}, {"title": "Online streaming feature selection", "author": ["Xindong Wu", "Kui Yu", "Hao Wang", "Wei Ding."], "venue": "Proceedings of the 27th international conference on machine learning (ICML-10). 1159\u20131166.", "citeRegEx": "Wu et al\\.,? 2010", "shortCiteRegEx": "Wu et al\\.", "year": 2010}, {"title": "Data mining with big data", "author": ["Xindong Wu", "Xingquan Zhu", "Gong-Qing Wu", "Wei Ding."], "venue": "Knowledge and Data Engineering, IEEE Transactions on 26, 1 (2014), 97\u2013107.", "citeRegEx": "Wu et al\\.,? 2014", "shortCiteRegEx": "Wu et al\\.", "year": 2014}, {"title": "Feature-selection-based Dynamic Transfer Ensemble Model for Customer Churn Prediction", "author": ["Jin Xiao", "Yi Xiao", "Annqiang Huang", "Dunhu Liu", "Shouyang Wang."], "venue": "Knowledge and Information Systems 43, 1 (2015), 29\u201351.", "citeRegEx": "Xiao et al\\.,? 2015", "shortCiteRegEx": "Xiao et al\\.", "year": 2015}, {"title": "Classification with Streaming Features: An Emerging-Pattern Mining Approach", "author": ["Kui Yu", "Wei Ding", "Dan A Simovici", "Hao Wang", "Jian Pei", "Xindong Wu."], "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD) 9, 4 (2015), 30.", "citeRegEx": "Yu et al\\.,? 2015a", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Tornado Forecasting with Multiple Markov Boundaries", "author": ["Kui Yu", "Dawei Wang", "Wei Ding", "Jian Pei", "David L Small", "Shafiqul Islam", "Xindong Wu."], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2237\u20132246.", "citeRegEx": "Yu et al\\.,? 2015b", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Stable feature selection via dense feature groups", "author": ["Lei Yu", "Chris Ding", "Steven Loscalzo."], "venue": "ACM SIGKDD-2008. ACM, 803\u2013811.", "citeRegEx": "Yu et al\\.,? 2008", "shortCiteRegEx": "Yu et al\\.", "year": 2008}, {"title": "Efficient feature selection via analysis of relevance and redundancy", "author": ["Lei Yu", "Huan Liu."], "venue": "Journal of Machine Learning Research 5 (2004), 1205\u20131224.", "citeRegEx": "Yu and Liu.,? 2004", "shortCiteRegEx": "Yu and Liu.", "year": 2004}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["Ming Yuan", "Yi Lin."], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 68, 1 (2006), 49\u201367.", "citeRegEx": "Yuan and Lin.,? 2006", "shortCiteRegEx": "Yuan and Lin.", "year": 2006}, {"title": "The Emerging \u201cBig Dimensionality", "author": ["Yiteng Zhai", "Y Ong", "I Tsang."], "venue": "Computational Intelligence Magazine, IEEE 9, 3 (2014), 14\u201326.", "citeRegEx": "Zhai et al\\.,? 2014", "shortCiteRegEx": "Zhai et al\\.", "year": 2014}, {"title": "Discovering Support and Affiliated Features from Very High Dimensions", "author": ["Yiteng Zhai", "Mingkui Tan", "Ivor Tsang", "Yew Soon Ong."], "venue": "ICML-2012. 1455\u20131462.", "citeRegEx": "Zhai et al\\.,? 2012", "shortCiteRegEx": "Zhai et al\\.", "year": 2012}, {"title": "Scaling Cut Criterionbased Discriminant Analysis for Supervised Dimension Reduction", "author": ["Xiangrong Zhang", "Yudi He", "Licheng Jiao", "Ruochen Liu", "Ji Feng", "Sisi Zhou."], "venue": "Knowledge and information systems 43, 3 (2015), 633\u2013655.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Searching for Interacting Features", "author": ["Zheng Zhao", "Huan Liu"], "venue": "In IJCAI,", "citeRegEx": "Zhao and Liu.,? \\Q2007\\E", "shortCiteRegEx": "Zhao and Liu.", "year": 2007}, {"title": "On similarity preserving feature selection", "author": ["Zheng Zhao", "Lei Wang", "Huan Liu", "Jieping Ye."], "venue": "IEEE Transactions on Knowledge and Data Engineering 25 (2013), 619\u2013632.", "citeRegEx": "Zhao et al\\.,? 2013", "shortCiteRegEx": "Zhao et al\\.", "year": 2013}, {"title": "Streamwise feature selection", "author": ["Jing Zhou", "Dean P Foster", "Robert A Stine", "Lyle H Ungar."], "venue": "Journal of Machine Learning Research 7 (2006), 1861\u20131885.", "citeRegEx": "Zhou et al\\.,? 2006", "shortCiteRegEx": "Zhou et al\\.", "year": 2006}, {"title": "Manifold elastic net: a unified framework for sparse dimension reduction", "author": ["Tianyi Zhou", "Dacheng Tao", "Xindong Wu."], "venue": "Data Mining and Knowledge Discovery 22, 3 (2011), 340\u2013371.", "citeRegEx": "Zhou et al\\.,? 2011", "shortCiteRegEx": "Zhou et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 36, "context": "In data mining and machine learning, the task of feature selection is to choose a subset of relevant features and remove irrelevant and redundant features from high-dimensional data towards maintaining a parsimonious model [Guyon and Elisseeff 2003; Liu and Yu 2005; Xiao et al. 2015; Zhang et al. 2015].", "startOffset": 223, "endOffset": 303}, {"referenceID": 44, "context": "In data mining and machine learning, the task of feature selection is to choose a subset of relevant features and remove irrelevant and redundant features from high-dimensional data towards maintaining a parsimonious model [Guyon and Elisseeff 2003; Liu and Yu 2005; Xiao et al. 2015; Zhang et al. 2015].", "startOffset": 223, "endOffset": 303}, {"referenceID": 35, "context": "In the era of big data today, many emerging applications, such as social media services, high resolution images, genomic data analysis, and document data analysis, consume data of extremely high dimensionality, in the order of millions or more [Wu et al. 2014; Zhai et al. 2014; Chen et al. 2014; Yu et al. 2015a; Yu et al. 2015b].", "startOffset": 244, "endOffset": 330}, {"referenceID": 42, "context": "In the era of big data today, many emerging applications, such as social media services, high resolution images, genomic data analysis, and document data analysis, consume data of extremely high dimensionality, in the order of millions or more [Wu et al. 2014; Zhai et al. 2014; Chen et al. 2014; Yu et al. 2015a; Yu et al. 2015b].", "startOffset": 244, "endOffset": 330}, {"referenceID": 2, "context": "In the era of big data today, many emerging applications, such as social media services, high resolution images, genomic data analysis, and document data analysis, consume data of extremely high dimensionality, in the order of millions or more [Wu et al. 2014; Zhai et al. 2014; Chen et al. 2014; Yu et al. 2015a; Yu et al. 2015b].", "startOffset": 244, "endOffset": 330}, {"referenceID": 37, "context": "In the era of big data today, many emerging applications, such as social media services, high resolution images, genomic data analysis, and document data analysis, consume data of extremely high dimensionality, in the order of millions or more [Wu et al. 2014; Zhai et al. 2014; Chen et al. 2014; Yu et al. 2015a; Yu et al. 2015b].", "startOffset": 244, "endOffset": 330}, {"referenceID": 38, "context": "In the era of big data today, many emerging applications, such as social media services, high resolution images, genomic data analysis, and document data analysis, consume data of extremely high dimensionality, in the order of millions or more [Wu et al. 2014; Zhai et al. 2014; Chen et al. 2014; Yu et al. 2015a; Yu et al. 2015b].", "startOffset": 244, "endOffset": 330}, {"referenceID": 29, "context": "For example, the Web Spam Corpus 2011 [Wang et al. 2012] collected approximately 16 million features (attributes) for web spam page detection, and the data set from KDD CUP 2010 about using educational data mining to accurately predict student performance includes more than 29 million features.", "startOffset": 38, "endOffset": 56}, {"referenceID": 42, "context": "The scalability of feature selection methods becomes critical to tackle millions of features [Zhai et al. 2014].", "startOffset": 93, "endOffset": 111}, {"referenceID": 30, "context": "Another example is feature selection in bioinformatics, where acquiring the full set of features for every training instance is expensive because of the high cost in conducting wet lab experiments [Wang et al. 2013].", "startOffset": 197, "endOffset": 215}, {"referenceID": 1, "context": "To search for a minimal subset of features that leads to the most accurate prediction model, two types of feature selection approaches were proposed in the literature, namely, batch methods [Brown et al. 2012; Woznica et al. 2012; Javed et al. 2014] and online methods [Wu et al.", "startOffset": 190, "endOffset": 249}, {"referenceID": 32, "context": "To search for a minimal subset of features that leads to the most accurate prediction model, two types of feature selection approaches were proposed in the literature, namely, batch methods [Brown et al. 2012; Woznica et al. 2012; Javed et al. 2014] and online methods [Wu et al.", "startOffset": 190, "endOffset": 249}, {"referenceID": 10, "context": "To search for a minimal subset of features that leads to the most accurate prediction model, two types of feature selection approaches were proposed in the literature, namely, batch methods [Brown et al. 2012; Woznica et al. 2012; Javed et al. 2014] and online methods [Wu et al.", "startOffset": 190, "endOffset": 249}, {"referenceID": 30, "context": "2014] and online methods [Wu et al. 2013; Wang et al. 2013].", "startOffset": 25, "endOffset": 59}, {"referenceID": 30, "context": "Moreover, a batch method has to access the full feature set prior to the learning task [Wu et al. 2013; Wang et al. 2013].", "startOffset": 87, "endOffset": 121}, {"referenceID": 30, "context": "One assumes that the number of features on training data is fixed while the number of data points changes over time, such as the OFS algorithm [Hoi et al. 2012; Wang et al. 2013] that performs feature selection upon each data instance.", "startOffset": 143, "endOffset": 178}, {"referenceID": 47, "context": "2013] and alphainvesting algorithms [Zhou et al. 2006].", "startOffset": 36, "endOffset": 54}, {"referenceID": 30, "context": "[Wang et al. 2013] further proposed the OGFS (Online Group Feature Selection) algorithm by assuming that feature groups are processed in a sequential scan.", "startOffset": 0, "endOffset": 18}, {"referenceID": 42, "context": "It is still an open research problem to efficiently reduce computational cost when the dimensionality is in the scale of millions or more [Wu et al. 2013; Zhai et al. 2014].", "startOffset": 138, "endOffset": 172}, {"referenceID": 31, "context": "The embedded methods attempt to simultaneously maximize classification performance and minimize the number of features used based on a classification or regression model with specific penalties on coefficients of features [Tibshirani 1996; Weston et al. 2000; Zhou et al. 2011].", "startOffset": 222, "endOffset": 277}, {"referenceID": 48, "context": "The embedded methods attempt to simultaneously maximize classification performance and minimize the number of features used based on a classification or regression model with specific penalties on coefficients of features [Tibshirani 1996; Weston et al. 2000; Zhou et al. 2011].", "startOffset": 222, "endOffset": 277}, {"referenceID": 19, "context": "A filter method is independent of any classifiers, and applies evaluation measures such as distance, information, dependency, or consistency to select features [Dash and Liu 2003; Forman 2003; Peng et al. 2005; Song et al. 2012; Liu et al. 2014].", "startOffset": 160, "endOffset": 245}, {"referenceID": 23, "context": "A filter method is independent of any classifiers, and applies evaluation measures such as distance, information, dependency, or consistency to select features [Dash and Liu 2003; Forman 2003; Peng et al. 2005; Song et al. 2012; Liu et al. 2014].", "startOffset": 160, "endOffset": 245}, {"referenceID": 15, "context": "A filter method is independent of any classifiers, and applies evaluation measures such as distance, information, dependency, or consistency to select features [Dash and Liu 2003; Forman 2003; Peng et al. 2005; Song et al. 2012; Liu et al. 2014].", "startOffset": 160, "endOffset": 245}, {"referenceID": 19, "context": "Due to their simplicity and low computational cost, many filter methods have been proposed to solve the feature selection problem, such as the well-established mRMR (minimal-Redundancy-Maximal-Relevance) algorithm [Peng et al. 2005] and the FCBF (Fast Correlation-Based Filter) algorithm [Yu and Liu 2004].", "startOffset": 214, "endOffset": 232}, {"referenceID": 46, "context": "[Zhao et al. 2013] proposed a novel framework to consolidate different criteria to handle feature redundancies.", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": "[Brown et al. 2012] presented a unifying framework for information theoretic feature selection using an optimized loss function of the conditional likelihood of the training labels.", "startOffset": 0, "endOffset": 19}, {"referenceID": 25, "context": "[Tan et al. 2010; Tan et al. 2014] proposed the efficient embedded algorithm, the FGM (Feature Generating Machine) algorithm, and Zhai et al.", "startOffset": 0, "endOffset": 34}, {"referenceID": 24, "context": "[Tan et al. 2010; Tan et al. 2014] proposed the efficient embedded algorithm, the FGM (Feature Generating Machine) algorithm, and Zhai et al.", "startOffset": 0, "endOffset": 34}, {"referenceID": 43, "context": "[Zhai et al. 2012] further presented the GDM (Group Discovery Machine) algorithm that outperforms the FGM algorithm.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "[Friedman et al. 2010], which enables to encourage sparsity at the levels of both features and groups simultaneously.", "startOffset": 0, "endOffset": 22}, {"referenceID": 30, "context": "[Wang et al. 2013] proposed an online feature selection method, OFS, which assumes data instances are sequentially added.", "startOffset": 0, "endOffset": 18}, {"referenceID": 47, "context": "[Zhou et al. 2006] presented Alpha-investing which sequentially considers new features as additions to a predictive model by modeling the candidate feature set as a dynamically generated stream.", "startOffset": 0, "endOffset": 18}, {"referenceID": 34, "context": "[Wu et al. 2010; Wu et al. 2013] presented the OSFS (Online Streaming Feature Selection) algorithm and its faster version, the FastOSFS algorithm.", "startOffset": 0, "endOffset": 32}, {"referenceID": 30, "context": "[Wang et al. 2013] proposed the OGFS (Online Group Feature Selection) algorithm.", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": "(1)? Based on the work of [Brown et al. 2012], Eq.", "startOffset": 26, "endOffset": 45}, {"referenceID": 1, "context": "By the work of [Brown et al. 2012], Eq.", "startOffset": 15, "endOffset": 34}, {"referenceID": 1, "context": "The second term equals to I(C;S\u03b8\u0304|S\u03b8), that is, the conditional mutual information between C and S\u03b8\u0304, given S\u03b8 [Brown et al. 2012].", "startOffset": 111, "endOffset": 130}, {"referenceID": 1, "context": "1 of [Brown et al. 2012] for the details on how to get Eq.", "startOffset": 5, "endOffset": 24}, {"referenceID": 19, "context": "This is essentially the idea behind the well-established batch feature selection algorithms, such as mRMR [Peng et al. 2005] and FCBF [Yu and Liu 2004].", "startOffset": 106, "endOffset": 124}, {"referenceID": 0, "context": "Due to pairwise comparisons, our algorithm focuses on finding an approximate Markov blanket (the parents and children of the class attribute in a Bayesian network [Aliferis et al. 2010]) and does not attempt to discover positive interactions between features (there exists a positive interaction between Fi and Fj with respect to C even though Fi is completely useless by itself with respect to C, but Fi can provide significantly discriminative power jointly with Fj [Jakulin and Bratko 2003; Zhao and Liu 2007]).", "startOffset": 163, "endOffset": 185}, {"referenceID": 21, "context": "We calculate symmetrical uncertainty [Press et al. 1996] instead of I(X,Y ), which is defined by", "startOffset": 37, "endOffset": 56}, {"referenceID": 0, "context": "We use fourteen benchmark data sets as our test beds, including ten high-dimensional data sets [Aliferis et al. 2010; Yu et al. 2008] and four extremely high-dimensional data sets, as shown in Table II.", "startOffset": 95, "endOffset": 133}, {"referenceID": 39, "context": "We use fourteen benchmark data sets as our test beds, including ten high-dimensional data sets [Aliferis et al. 2010; Yu et al. 2008] and four extremely high-dimensional data sets, as shown in Table II.", "startOffset": 95, "endOffset": 133}, {"referenceID": 47, "context": "2013], Alpha-investing [Zhou et al. 2006], and OFS [Wang et al.", "startOffset": 23, "endOffset": 41}, {"referenceID": 30, "context": "2006], and OFS [Wang et al. 2013].", "startOffset": 15, "endOffset": 33}, {"referenceID": 46, "context": "Fast-OSFS and Alpha-investing assume features on training data arrive one by one at a time while OFS assumes data examples come one by one; \u2014Three batch methods: one well-established algorithm of FCBF [Yu and Liu 2004], and two state-of-the-art algorithms, SPSF-LAR [Zhao et al. 2013] and GDM [Zhai et al.", "startOffset": 266, "endOffset": 284}, {"referenceID": 43, "context": "2013] and GDM [Zhai et al. 2012].", "startOffset": 14, "endOffset": 32}, {"referenceID": 47, "context": "01 for Fast-OSFS, and for Alpha-investing, the parameters are set to the values used in [Zhou et al. 2006].", "startOffset": 88, "endOffset": 106}, {"referenceID": 11, "context": "[Kalousis et al. 2007].", "startOffset": 0, "endOffset": 22}, {"referenceID": 39, "context": "[Yu et al. 2008] to evaluate the stabilities of SAOLA, Fast-OSFS, Alpha-investing, and OFS.", "startOffset": 0, "endOffset": 16}, {"referenceID": 43, "context": "We also select the GDM algorithm [Zhai et al. 2012] which is one of the most recent batch feature selection methods in dealing with very large dimensionality.", "startOffset": 33, "endOffset": 51}, {"referenceID": 28, "context": "In this section, we compare SAOLA and FCBF (discovery of approximateMarkov blankets) with two state-of-the-art exact Markov blanket discovery algorithms, IAMB (Incremental Association Markov Blanket) [Tsamardinos and Aliferis 2003] and MMMB (Max-Min Markov Blanket) [Tsamardinos et al. 2006].", "startOffset": 266, "endOffset": 291}, {"referenceID": 18, "context": "Under certain assumptions (sufficient number of data instances and reliably statistical tests), IAMB and MMMB can return the Markov blanket of a given target feature [Pe\u00f1a et al. 2007; Aliferis et al. 2010].", "startOffset": 166, "endOffset": 206}, {"referenceID": 0, "context": "Under certain assumptions (sufficient number of data instances and reliably statistical tests), IAMB and MMMB can return the Markov blanket of a given target feature [Pe\u00f1a et al. 2007; Aliferis et al. 2010].", "startOffset": 166, "endOffset": 206}, {"referenceID": 39, "context": "[Yu et al. 2008].", "startOffset": 0, "endOffset": 16}, {"referenceID": 30, "context": "In this section, we compare our group-SAOLA algorithm with the state-of-the-art online group feature selection methods, OGFS [Wang et al. 2013] and a well-established batch group feature selection method, Sparse Group Lasso [Friedman et al.", "startOffset": 125, "endOffset": 143}, {"referenceID": 6, "context": "2013] and a well-established batch group feature selection method, Sparse Group Lasso [Friedman et al. 2010].", "startOffset": 86, "endOffset": 108}], "year": 2016, "abstractText": "Feature selection is important in many big data applications. Two critical challenges closely associate with big data. Firstly, in many big data applications, the dimensionality is extremely high, in millions, and keeps growing. Secondly, big data applications call for highly scalable feature selection algorithms in an online manner such that each feature can be processed in a sequential scan. We present SAOLA, a Scalable and Accurate OnLine Approach for feature selection in this paper. With a theoretical analysis on bounds of the pairwise correlations between features, SAOLA employs novel pairwise comparison techniques and maintain a parsimonious model over time in an onlinemanner. Furthermore, to deal with upcoming features that arrive by groups, we extend the SAOLA algorithm, and then propose a new group-SAOLA algorithm for online group feature selection. The group-SAOLA algorithm can online maintain a set of feature groups that is sparse at the levels of both groups and individual features simultaneously. An empirical study using a series of benchmark real data sets shows that our two algorithms, SAOLA and group-SAOLA, are scalable on data sets of extremely high dimensionality, and have superior performance over the state-of-the-art feature selection methods.", "creator": "LaTeX with hyperref package"}}}