{"id": "1512.00077", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2015", "title": "Decoding Hidden Markov Models Faster Than Viterbi Via Online Matrix-Vector (max, +)-Multiplication", "abstract": "In this paper, we present a novel algorithm for the maximum a posteriori decoding (MAPD) of time-homogeneous Hidden Markov Models (HMM), improving the worst-case running time of the classical Viterbi algorithm by a logarithmic factor. In our approach, we interpret the Viterbi algorithm as a repeated computation of matrix-vector $(\\max, +)$-multiplications. On time-homogeneous HMMs, this computation is online: a matrix, known in advance, has to be multiplied with several vectors revealed one at a time. Our main contribution is an algorithm solving this version of matrix-vector $(\\max,+)$-multiplication in subquadratic time, by performing a polynomial preprocessing of the matrix. Employing this fast multiplication algorithm, we solve the MAPD problem in $O(mn^2/ \\log n)$ time for any time-homogeneous HMM of size $n$ and observation sequence of length $m$, with an extra polynomial preprocessing cost negligible for $m &gt; n$. To the best of our knowledge, this is the first algorithm for the MAPD problem requiring subquadratic time per observation, under the assumption -- usually verified in practice -- that the transition probability matrix does not change with time. In the present work, we will be using a matrix of finite number of times to compute the shortest time of the entire vector. Our main contribution is an algorithm solving the MAPD problem in $O(mn^2/ \\log n)$ time for any time-homogeneous HMM of size $n$ and observation sequence of length $m$, with an extra polynomial preprocessing cost negligible for $m &gt; n$. To the best of our knowledge, this is the first algorithm for the MAPD problem requiring subquadratic time per observation, under the assumption -- usually verified in practice -- that the transition probability matrix does not change with time. In the present work, we will be using a matrix of finite number of times to compute the shortest time of the entire vector. Our main contribution is an algorithm solving the MAPD problem in $O(mn^2/ \\log n)$ time for any time-homogeneous HMM of size $n$ and observation sequence of length $m$ and observation sequence of length $m$ and observation sequence of length $m$ and observation sequence of length $m$ and observation sequence of length $m$ and", "histories": [["v1", "Mon, 30 Nov 2015 22:38:07 GMT  (1618kb,D)", "https://arxiv.org/abs/1512.00077v1", "AAAI 2016, to appear"], ["v2", "Fri, 11 Dec 2015 10:40:51 GMT  (1618kb,D)", "http://arxiv.org/abs/1512.00077v2", "AAAI 2016, to appear"]], "COMMENTS": "AAAI 2016, to appear", "reviews": [], "SUBJECTS": "cs.LG cs.DS cs.IT math.IT", "authors": ["massimo cairo", "gabriele farina", "romeo rizzi"], "accepted": true, "id": "1512.00077"}, "pdf": {"name": "1512.00077.pdf", "metadata": {"source": "CRF", "title": "Decoding Hidden Markov Models Faster Than Viterbi Via Online Matrix-Vector (max,+)-Multiplication", "authors": ["Massimo Cairo", "Gabriele Farina", "Romeo Rizzi"], "emails": ["massimo.cairo@unitn.it", "gabriele2.farina@mail.polimi.it", "romeo.rizzi@univr.it"], "sections": [{"heading": "Introduction", "text": "Hidden Markov Models (HMMs) are simple probabilistic models originally introduced (Viterbi 1967) to decode convolutional codes. Due to their universal and fundamental nature, these models have successfully been applied in several fields, with many important applications, such as gene prediction (Haussler and Eeckman 1996), speech, gesture and optical character recognition (Gales 1998; Huang, Ariki, and Jack 1990; Starner, Weaver, and Pentland 1998; Agazzi and Kuo 1993), and part-of-speech tagging (Kupiec 1992). Their applications to bioinformatics began in the early 1990 and soon exploded to the point that cur-\nCopyright c\u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nrently they hold a recognized place in that field (Yoon 2009; M\u00e4kinen et al. 2015).\nA HMM describes a stochastic process generating a sequence of observations y1, y2, . . . , yn. Internally, a sequence of hidden states x1, x2, . . . , xn is generated according to a Markov chain. At each time instant t = 1, 2, . . . , n, a symbol yt is observed according to a probability distribution depending on xt. We consider only time-homogeneous HMMs, i.e. models whose parameters do not depend on the time t. While this assumption covers the majority of applications, some notable exceptions involving timeinhomogeneous models are known (Lafferty, McCallum, and Pereira 2001). Maximum a posteriori decoding (MAPD). Since the states of the model are hidden, i.e. only the generated symbols can be observed, a natural problem associated with HMMs is the MAPD problem: given a HMMM and an observed sequence of symbols Y of length m, find any state path X through M maximizing the joint probability of X and Y . We call any suchX a most probable state path explaining the observation Y . Traditionally, the MAPD problem is solved by the Viterbi algorithm (Viterbi 1967), inO(mn2) time and O(mn) memory for any model of size n and observation sequence of length m.\nOver the years, much effort has been put into lowering the cost of the Viterbi algorithm, both in terms of memory and of running time. (Grice, Hughey, and Speck 1997) showed that a checkpointing technique can be employed to reduce the memory complexity to O(\u221am \u00b7 n); refinements of this idea (embedded checkpointing) deliver a family of time-memory tradeoffs, culminating into an O(n logm) memory solution with a slightly increased running time O(mn2 logm).\nAt the same time, several works reducing the time complexity of the algorithm in the average-case were developed (\u0160r\u00e1mek 2007; Churbanov and Winters-Hilt 2008; Felzenszwalb, Huttenlocher, and Kleinberg 2004; Esposito and Radicioni 2009; Kaji et al. 2010). Many of these works make assumptions on the structure of T , and may lose the optimality or degenerate to the worst case \u0398(mn2) opera-\nar X\niv :1\n51 2.\n00 07\n7v 2\n[ cs\n.L G\n] 1\n1 D\ntions when these assumptions are not fulfilled. In (Lifshits et al. 2009), the authors show a method to speed up the decoding of HMMs by aO(logm) factor, by precomputing all possible observation sequences of length logm, in a fashion similar to the Four Russians method. This requires the number of such sequences to be \u201csmall\u201d. In the same work, the authors also show that it is possible to compress the observation sequence to achieve speedups proportional to the compression ratios. However, this latter method seems to require the observation sequence to be available in advance.\nTo the best of our knowledge no algorithm achieving a worst-case running time better thanO(mn2) is known under the only assumption of time-homogeneousness. Approach. We give an algorithm solving the MAPD problem for time-homogeneous HMMs with time complexity asymptotically lower than O(mn2), in the worst case. We regard the MAPD problem as an iterated computation of a matrix-vector multiplication. For time-homogeneous models, the matrix is known in advance and does not change with time. However, the sequence of vectors to be multiplied cannot be foreseen, as each vector depends on the result of the previous computation; this rules out the possibility to batch the vectors into a matrix and defer the computation. We call this version of the problem, in which a fixed matrix has to be multiplied with several vectors revealed one at a time, \u201cthe online matrix-vector multiplication (OMV MUL) problem\u201d.\nConsider the problem of multiplying a n\u00d7 n matrix with a column vector of size n. Without further assumptions, the trivial O(n2) time algorithm is optimal, since all the n2 elements of the matrix have to be read at least once. However, under the assumption that the matrix is known in advance and can be preprocessed, this trivial lower bound ceases to hold. Algorithms faster than the trivial quadratic one are known for the OMV MUL problem over finite semirings (Williams 2007), as well as over real numbers with standard (+, \u00b7)-multiplication, if the matrix has only a constant number of distinct values (Liberty and Zucker 2009).\nHowever, none of the above algorithm can be applied to time-homogeneous HMMs, as their decoding relies on online real matrix-vector (max,+)-multiplication (ORMV (max,+)-MUL). In the specific case of real (max,+)-multiplication, subcubic algorithms have been known for years (Dobosiewicz 1990; Chan 2008; 2015) for the matrix-matrix multiplication problem, with important applications to graph theory and boolean matrix multiplication, among others. However, we are not aware of any algorithm solving the ORMV (max,+)-MUL problem in subquadratic time. Note that the ORMV (max,+)-MUL can be used to compute the OMV MUL over the Boolean semiring: for this problem, it has been conjectured (Henzinger et al. 2015) that no \u201ctruly polynomially subquadratic\u201d algorithm1 exists for the ORMV (max,+)-MUL problem.\nWe reduce the ORMV (max,+)-MUL problem to a multi-dimensional geometric dominance problem, following an approach similar to that of (Bremner et al. 2006; Chan 2008). Then, the geometric problem is solved by a\n1That is, running in timeO(n2\u2212\u03b5) for some \u03b5 > 0 after a polynomial preprocessing or the matrix.\ndivide-and-conquer algorithm, which can be regarded as a transposition of the algorithm of (Chan 2008) to the online setting. Our technique yields a worst-case O(mn2/ log n) algorithm, called GDFV, solving the MAPD problem after a polynomial preprocessing of the model. Contributions. Our key contributions are as follows: (i) we extend the geometric dominance reporting problem introduced in (Chan 2008) to the online setting; (ii) we solve the ORMV (max,+)-MUL problem in O(n2/ log n) time after a polynomial preprocessing of the n \u00d7 n matrix; (iii) we show an algorithm solving the MAPD problem on timehomogeneous HMMs in O(mn2/ log n) time in the worstcase, after a polynomial preprocessing of the model.\nFinally, we experimentally evaluate the performance of our algorithms, with encouraging results. Currently the problem sets in which we outperform Viterbi are limited, but we hope that the approach we propose will open the way to further improvements on this problem in future works."}, {"heading": "Preliminaries", "text": ""}, {"heading": "Notation", "text": "The i-th component of a vector v is denoted by v[i]; similarly, M[i, j] denotes the entry of row i and column j, in matrix M. Indices will always be considered as starting from 1. Given two vectors a and b of dimension n, such that a[i] \u2264 b[i] for every coordinate index i = 1, . . . , n, we write a b and say that b dominates a, or, equivalently, that (a,b) is a dominating pair.\nGiven a matrix or vector M with non-negative entries, we write logM to mean the matrix or vector that is obtained from M by applying the logarithm on every component. We will almost always work with the extended set R\u2217 = R \u222a {\u2212\u221e}, so that we can write log 0 = \u2212\u221e. We assume that \u2212\u221e+ x = x+ (\u2212\u221e) = \u2212\u221e and x \u2265 \u2212\u221e for all x \u2208 R\u2217."}, {"heading": "Hidden Markov Models (HMMs)", "text": "We formally introduce the concept of time-homogeneous Hidden Markov Models.\nDefinition 1. A time-homogeneous HMM is a tuple M = (S,A,\u03a0, T , E), composed of: \u2022 a set S = {s1, . . . , sn} of n hidden states; n is called the\nsize of the model, \u2022 an output alphabet A = {a1, . . . , a|A|}, \u2022 a probability distribution vector \u03a0 = {\u03c01, . . . , \u03c0n} over\nthe initial states, \u2022 a matrix T = {ts(s\u2032)}s,s\u2032 \u2208S of transition probabilities\nbetween states, \u2022 a matrix E = {es(a)}a\u2208As\u2208S of emission probabilities. Matrices T and E are stochastic, i.e., the entries of every row sum up to 1.\nFor notational convenience, we relabel the states of a HMM with natural numbers, i.e. we let S = {1, . . . , n}.\nAs stated in the introduction, HMMs define generative processes over the alphabet A. The initial state x0 \u2208 S is chosen according to the distribution \u03a0; then, at each step, a symbol a is generated according to the probability distribution ex(a), where x is the current state; a new state x\u2032 is\nchosen according to the probability distribution induced by tx(x\n\u2032), and the process repeats. The probability of a state path X = (x1, . . . , xm) joint to an observation sequence Y = (y1, . . . , ym) is computed as:\nPr(X,Y ) = \u03c0x1 ( m\u22121\u220f i=1 txi(xi+1) )( m\u220f i=1 exi(yi) ) .\nThe Viterbi algorithm The Viterbi algorithm consists of two phases: in the first phase, a simple dynamic programming approach is used to determine the probability of the most probable state path ending in each possible state. In the second phase, the data stored in the dynamic programming table is used to reconstruct a most probable state path. Definition 2. Assume given a HMMM = (S,A,\u03a0, T , E) and an observed sequence A = (a1, . . . , am). For every s \u2208 S and i = 1, . . . ,m, denote by qi(s) the probability of any most probable path ending in state s explaining the observation Ai\u22121 = (a1, . . . , ai\u22121).\nBy definition of qi(s), any most probable path explaining A has probability maxs\u2208S {es,am \u00b7 qm(s)}. The qi(s) values can be computed inductively. Indeed, q1(s) = \u03c0s for all s \u2208 S, while for every i > 1 and s \u2208 S it holds:\nqi(s) = max s\u2032 \u2208S {qi\u22121(s\u2032) \u00b7 ts\u2032(s) \u00b7 es\u2032(ai\u22121)} . (1) In order to compute all the n values qi(s), for any fixed i > 1, \u0398(n2) comparisons have to be performed. This phase is in fact the bottleneck of the algorithm.\nThe second phase of the algorithm uses the qi(s) values to reconstruct an optimal path in O(mn) time. We will not deal with this second and faster part, and only mention that most of the previously developed solutions for it, including the memory saving ones (\u0160r\u00e1mek 2007; Churbanov and Winters-Hilt 2008), are still applicable once the first part has been carried out based on our approach.\nOnline matrix-vector multiplication problem We formally introduce the online matrix-vector (max,+)multiplication problem briefly discussed in the introduction. Definition 3 (Matrix-vector (max,+)-multiplication). Given am\u00d7nmatrix A and a n-dimensional column vector b over R\u2217, their (max,+)-multiplication A \u2217b is a column vector of dimension m whose i-th component is:\n(A \u2217 b)[i] = nmax j=1 {A[i, j] + b[j]} .\nProblem 1 (ORMV (max,+)-MUL problem). Given a m \u00d7 n matrix A over R\u2217, perform a polynomial-time preprocess of it, so that the product A \u2217 b can be computed for any input vector b of size n.\nLemma 1 is a simple result that bridges the gap between the MAPD problem and the ORMV (max,+)-MUL problem, showing that any fast solution to Problem 1 can be turned into a fast solution for the MAPD problem. Lemma 1. Any algorithm for Problem 1 computing A \u2217 b in T (m,n) time after U(m,n) preprocessing time can be used to solve the MAPD problem for any time-homogeneous\nHMM of size n and observation sequence of length m in O(m \u00b7 T (n, n)) time after a U(n, n) time preprocessing. Proof. Denote by tT the transpose of the state transition matrix T ofM. Furthermore, for every i = 1, . . . ,m, introduce the pair of vectors\nqi = (qi(1), . . . , qi(n)), ei = (e1(ai), . . . , en(ai)). The value qi(s) corresponding to instant i > 0 and state s \u2208 S can be computed as follows, in logaritmic scale:\nlog qi(s) = log max s\u2032 \u2208S\n{qi\u22121(s\u2032) \u00b7 ts\u2032(s) \u00b7 es\u2032(ai\u22121)}\n= max s\u2032 \u2208S\n{log(ts\u2032(s)) + log(qi\u22121(s\u2032) es\u2032(ai\u22121))}\n= ((log tT ) \u2217 (logqi\u22121 + log ei\u22121))[s]. Notice that the n \u00d7 n matrix log tT depends only on the model and is time-invariant; therefore, we can compute qi(s) for all s = 1, . . . , n in batch with an instance of ORMV (max,+)-MUL:\nlogqi = (log tT ) \u2217 (logqi\u22121 + log ei\u22121).\nOnce the qi(s) values have been computed, we can use the second part of the Viterbi algorithm out of the box. The time required for the multiplication \u2013 the bottleneck of the algorithm \u2013 is O(T (n, n)) by hypothesis, hence the final algorithm has time complexityO(m\u00b7T (n, n)). The time complexity of the preprocessing is U(n, n).\nFrom multiplication to geometric dominance Consider the following geometric problem, which apparently has no relation with the (max,+)-multiplication, nor with the Viterbi algorithm.\nProblem 2 (Online geometric dominance reporting). Let B be a set of d-dimensional vectors2. Given a vector p \u2208 B, we define its domination set as \u03b4B(p) = {b \u2208 B : b p}. Preprocess B so that at a later time the set \u03b4B(p) can be computed for any input vector p.\nLemma 2. Any algorithm solving Problem 2 in O(T (d, |B|) + |\u03b4B(p)|) time after a preprocessing time O(U(d, |B|)) can be turned into an algorithm solving the ORMV (max,+)-MUL problem for any m \u00d7 t matrix in O(m + t \u00b7 T (t,m)) time, with preprocessing time O(mt2 + t \u00b7 U(t,m)). Proof. This constructive proof comes in different stages. First, the result is obtained under the simplifying assumptions that (i) neither A nor b have any \u2212\u221e entry, and (ii) the maximum sum on any row, A[\u00b7, j] + b[j], is achieved by exactly one value of the column index j. Later, these conditions will be dropped.\nObserve that (A \u2217 b)[i] = A[i, j\u2217] + b[j\u2217] iff A[i, j\u2217] + b[j\u2217] \u2265 A[i, j] + b[j] (2)\nfor every column index j. Under assumption (i), this inequality can be rewritten as\nA[i, j]\u2212A[i, j\u2217] \u2264 b[j\u2217]\u2212 b[j]. 2The coordinates of the vectors can range over any chosen totally-ordered set, as long as any two coordinate values can be compared in constant time.\nDefining the values ai,j\u2217(j) = A[i, j]\u2212A[i, j\u2217], bj\u2217(j) = b[j\u2217]\u2212 b[j] for all the feasible values of i, j, j\u2217, we obtain (A \u2217b)[i] = A[i, j\u2217] +b[j\u2217] \u21d0\u21d2 ai,j\u2217(j) \u2264 bj\u2217(j) \u2200j. (3) Notice that the last expression is actually a statement of geometric dominance, between the two t-dimensional vectors A\u0303i,j\u2217 = (ai,j\u2217(1), . . . , ai,j\u2217(t)) and b\u0303j\u2217 = (bj\u2217(1), . . . , bj\u2217(t)). This immediately leads to the following algorithm.\nAlgorithm 1 m\u00d7 t matrix-vector (max,+)-multiplication 1: procedure PREPROCESS(A) 2: for j\u2217 = 1, . . . , t do 3: for i = 1, . . . ,m do 4: A\u0303i,j\u2217 \u2190 (ai,j\u2217(1), . . . , ai,j\u2217(t)) 5: Bj\u2217 \u2190 {A\u03031,j\u2217 , . . . , A\u0303m,j\u2217} 6: Preprocess Bj\u2217\n1: procedure MULTIPLY(b) . Returns A \u2217 b 2: for j\u2217 = 1, . . . , t do 3: b\u0303j \u2190 (bj\u2217(1), . . . , bj\u2217(t)) 4: \u03b4 \u2190 \u03b4Bj\u2217 (b\u0303j\u2217), as defined in Problem 2 5: for all A\u0303i,j\u2217 \u2208 \u03b4 do 6: m[i]\u2190 A[i, j\u2217] + b[j\u2217] 7: return m\nLine 6 in procedure PREPROCESS requires U(t,m) time, making the total preprocessing cost of MULTIPLY O(mt2 + t \u00b7 U(t,m)). As for the running time, Line 4 of MULTIPLY takes O(T (t,m) + |\u03b4|) time, and is executed t times. Under assumption (ii), there is only one column j\u2217 that satisfies Equation 3 for each row i, hence the total number of elements appearing in \u03b4 is exactly m. As a consequence, Line 6 of MULTIPLY is executed m times and the total time complexity of MULTIPLY is O(m+ t \u00b7 T (m, t)).\nWe now relax assumptions (i) and (ii) by applying a transformation of the input. Instead of working on R\u2217, we work on triples over N \u00d7 R \u00d7 N. The matrix A is transformed as follows: each element x > \u2212\u221e is replaced by \u30080, x, 0\u3009, while each occurence of \u2212\u221e is replaced by \u3008\u22121, 0, 0\u3009. The input vector b is transformed similary, but the third coordinate is used to hold the index of the replaced element. Namely, element b[j] = x is replaced by \u30080, x, j\u3009 if x > \u2212\u221e and by \u3008\u22121, 0, j\u3009 otherwise. One can informally regard the first two entries of each triple \u3008a, b, \u00b7\u3009 as a shorthand for the value a \u00b7\u221e+ b. Any two triples are compared according to lexicographical order, while addition and subtraction are performed element-wise.\nThe crucial observation is that, if A[i, j\u2217] + b[j\u2217] > A[i, j]+b[j] before the transformation, then the same holds also after the transformation. Hence, we can solve the transformed problem to obtain the solution of the original problem. Our algorithm can be applied as it is to the transformed problem: indeed, the inequality in Equation 2 can be rearranged without any further assumption; moreover, there can be no two distinct columns j and j\u2032 achieving the maximum, as the two triples A[i, j] + b[j] and A[i, j\u2032] + b[j\u2032] differ at\nleast on the third element. Once the output vector is obtained, replace each triple \u3008k, x, j\u3009 with x if k = 0 and with \u2212\u221e otherwise. Conveniently, the third element j holds the index of the column achieving the maximum.\nLemma 3 shows that every fast algorithm for the OMV (max,+)-MUL of narrow rectangular matrices can be turned into a OMV (max,+)-MUL algorithm for square matrices.\nLemma 3. Any algorithm computing the OMV (max,+)MUL of a m \u00d7 t matrix in T (m, t) time and U(m, t) preprocessing time, can be used to multiply any m\u00d7 n matrix, n \u2265 t, inO(n/t \u00b7 (T (m, t)+m)) time andO(n/t \u00b7U(m, t)) preprocessing time. Proof. Assume without loss of generality that n is an integer multiple of t (otherwise, add columns to A and elements to b with value \u2212\u221e until the condition is met). The idea is to split A and b into n/t blocks, each of size m\u00d7 t and t\u00d7 1 respectively: A = (A1| \u00b7 \u00b7 \u00b7 |An/t), b = (b1| \u00b7 \u00b7 \u00b7 |bn/t). Observe that (A\u2217b)[i] = maxn/t`=1 {(A` \u2217 b`)[i]} . This immediately leads to the following algorithm. First, we preprocess each block A` in U(m, t) time with the given algorithm, so that the product A` \u2217 b` can be later computed in T (m, t) time. As soon as the vector b is received, compute m` = A` \u2217 b` for all ` = 1, . . . , n/t, and finally the output vector by (A \u2217 b)[i] = maxn/t`=1 {m`[i]}.\nThe time analysis is straightforward. The computation of each m` takes T (m, t) time. There are n/t such computations and merging the results takesO(m\u00b7n/t) time, yielding a total time ofO(n/t \u00b7(T (m, t)+m)). The total preprocessing time is O(n/t \u00b7 U(m, t)).\nBefore continuing with the next section, where the main result of this paper will be discussed, we state the following theorem, whose proof is available in the Appendix of this paper.\nTheorem 1. Problem 2 can be solved in O(d log |B| + |\u03b4B(p)|) time per query, after a O(|B|d+1) time and space preprocessing.\nAn O(n2/ log n) algorithm (GDFV) Lemma 4. There exists an algorithm solving Problem 2 in O(d cd\u03b5 |B|\u03b5 + |\u03b4B(p)|) time for every \u03b5 \u2208 (0, 1], where c\u03b5 := 1/(2\n\u03b5\u22121). The preprocessing requiresO(c\u2032\u03b5d |B|1+\u03b5) time and memory for every \u03b5 \u2208 (0, log2 3/2], where c\u2032\u03b5 := 1/(21+\u03b5 \u2212 2). Proof. We develop a simple divide-and-conquer algorithm; we assume without loss of generality that |B| is a power of two. Overview. If d = 0, return \u03b4B(p) = B. If B contains only one vector b, check if b p and return either {b} or the empty set accordingly. In all the other cases, split B into two sets B\u2212 and B+ of size |B|/2, according to the median d-th coordinate \u03b3 of the vectors in B, so that b\u2212[d] \u2264 \u03b3 \u2264 b+[d] for all b\u2212 \u2208 B\u2212 and b+ \u2208 B+. Now consider the d-th coordinate of the query vector p: if it is strictly less than \u03b3, then\nall the vectors in B+ do not occur in the solution. Hence, solve the problem recursively on B\u2212. Otherwise, both the sets B+ and B\u2212 need to be considered; however, the d-th coordinate for the vectors in B\u2212 is known to be \u2264 p[d] and can be dropped. Hence, solve the problem recursively on B+ and p and on (B\u2212)\u2032 and p\u2032, where the apostrophe denotes the discard of the last coordinate, and merge the solutions. The recursive step is summarized by the following recurrence:\n\u03b4B(p) = { \u03b4B\u2212(p) if p[d] < \u03b3, \u03b4B+(p) \u222a \u03b4(B\u2212)\u2032(p\u2032) otherwise.\nIn order to make the algorithm faster, we exploit the fact that B is known in advance. At preprocessing time, we build a tree that guides the execution of the algorithm, where each node u corresponds to a subproblem Bu over a dudimensional space. The root corresponds to the original set B. If |Bu| \u2265 2 and du \u2265 1, then the node u stores the median value \u03b3 and has three children, corresponding to the subproblems B+u , B\u2212u and (B\u2212u )\u2032. Otherwise, u is a leaf storing the content of Bu. We analyze the cost of building the tree later on. For now, notice that the size of the tree is at most polynomial in |B|: the height is at most log |B|, as the value |Bu| halves at each level, so the nodes are at most O(3log2 |B|) = O(|B|log2 3) = O(|B|1.59). Time analysis. Our algorithm starts from the root node, and visits recursively the nodes in the tree that are needed to solve the problem. When we reach a leaf u with du = 0, we output Bu (which is not empty) in O(|Bu|) time. If instead du > 0 and |Bu| = 1, we pay O(d) time to check if b p, and O(1) to output b if needed. On internal nodes, we only pay constant extra time as the median coordinate \u03b3 is known from the tree. The cost of producing the output is O(|\u03b4B(p)|), and is measured separately. Hence, the running time isO(Td(|B|)+|\u03b4B(p)|) where Td(n) satisfies the linear recurrence relation\nTd(n) = 1 + max\n{ Td(n/2)\nTd\u22121(n/2) + Td(n/2)\n= Td\u22121(n/2) + Td(n/2) + 1,\n(4)\nwith base cases Td(1) = d and T0(n) = 0. (The time required to handle this last case is included in O(|\u03b4B(p)|)). We show by induction that Td(n) \u2264 T d(n) := d cd\u03b5 n\u03b5, for any chosen \u03b5 \u2208 (0, 1], where c\u03b5 := 1/(2\u03b5 \u2212 1). Notice that c\u03b5 \u2265 1 for \u03b5 \u2208 (0, 1], thus the statement is true for the base cases as T d(n) \u2265 d. Assuming the inductive hypothesis, we obtain for n \u2265 2 and d \u2265 1: Td(n) \u2264 T d(n/2) + T d\u22121(n/2) + 1\n= d cd\u03b5 (n/2) \u03b5 + (d\u2212 1) cd\u22121\u03b5 (n/2)\u03b5 + 1 \u2264 d cd\u03b5 (n/2)\u03b5 + d cd\u22121\u03b5 (n/2)\u03b5\n= d cd\u03b5 n \u03b5 1 + c\n\u22121 \u03b5\n2\u03b5 = T d(n) 1 + 2\u03b5 \u2212 1 2\u03b5 = T d(n),\ncompleting the induction. Thus, the time complexity of the algorithm is O(d cd\u03b5 |B|\u03b5 + |\u03b4B(p)|). Preprocessing. The tree is built starting from the root. Finding the median d-th coordinate \u03b3, computing B+u , B\u2212u\nand (B\u2212u )\u2032, and storing the data in the node u, all require O(|Bu|) time and memory. Hence, the time and memory cost to build the tree is O(Ud(|B|)), where Ud(n) satisfies the recurrence\nUd(n) = 2Ud(n/2) + Ud\u22121(n/2) + n with base cases U0(n) = n and Ud(1) = 1. We show by induction that\nUd(n) \u2264 Ud(n) := 3c\u2032\u03b5 d n1+\u03b5 \u2212 2n\nfor any chosen \u03b5 \u2208 (0, log2 3/2], where c\u2032\u03b5 := 1/(21+\u03b5 \u2212 2). Notice that c\u2032\u03b5 \u2265 1 for \u03b5 \u2208 (0, log2 3/2]. Hence, the statement is true for the base cases, as Ud(n) \u2265 3c\u2032\u03b5d n1+\u03b5\u2212 2n \u2265 3n\u2212 2n \u2265 n. Assuming the inductive hypothesis, we obtain for n \u2265 2 and d \u2265 1: Ud(n) \u2264 2Ud(n/2) + Ud\u22121(n/2) + n\n= 2 \u00b7 (3 c\u2032\u03b5 d (n/2)1+\u03b5 \u2212 n) + (3 c\u2032\u03b5 d\u22121 (n/2)1+\u03b5 \u2212 n) + n\n= 2 \u00b7 3 c\u2032\u03b5 d (n/2)1+\u03b5 + 3 c\u2032\u03b5 d\u22121 (n/2)1+\u03b5 \u2212 2n\n= 3 c\u2032\u03b5 d n1+\u03b5 \u00b7 2 + c\n\u2032 \u03b5 \u22121\n21+\u03b5 \u2212 2n\n= 3 c\u2032\u03b5 d n1+\u03b5 \u2212 2n = Ud(n).\ncompleting the induction. Hence, the time and memory cost of the preprocessing phase isO(Ud(|B|)) = O(c\u2032\u03b5d n\u03b5).\nWe remark that the time complexity and the recurrence of the simple algorithm given in the above proof differ from those of Chan by necessity, as the online setting requires each vector to be treated separately. On the other hand, his result follows directly from ours: Theorem 2 ((Chan 2008), Lemma 2.1). Given n red/blue points in Rd\u2217 we can report all K dominating pairs in O(kd\u03b5 n1+\u03b5 + K) time for any \u03b5 \u2208 (0, 1), where k\u03b5 := 2\u03b5/(2\u03b5 \u2212 1). Proof. After reading the set B of blue points, preprocess them as described in the proof of Lemma 4. Then, for each red point p, perform a query to find all the dominators of p, i.e. \u03b4B(p); simply flush out the union of all the dominating pairs obtained. By Lemma 4, the cost of the preprocessing is O((21+\u03b5 \u2212 2)\u2212d n1+\u03b5) = O(kd\u03b5 n1+\u03b5). On the other hand, each of the n queries takes time O(d(2\u03b5 \u2212 1)\u2212d n\u03b5), that is3 O(kd\u03b5 n\u03b5), excluding the output; the overhead due to the actual output of the pairs is O(K). The final cost of the algorithm is therefore O(cd\u03b5 n1+\u03b5 +K) as desired.\nFinally, Lemmas 1, 4, and 3 combine into the following. Theorem 3. There exists an algorithm solving the MAPD problem in O(mn2/ log n) time after a polynomial preprocessing of the model, for any HMM of size n and observation sequence of length m. Proof. It is enough to show how to solve the ORMV (max,+)-MUL problem for any n \u00d7 n matrix in O(n2/ log n) time. To this end, apply Lemma 3 with m = n and t = \u03b1 log2 n, where \u03b1 \u2208 (0, 1/2) \u2286 R. This\n3Indeed, using the binomial expansion formula we have:\nkd\u03b5 = ( 1 + 1\n2\u03b5 \u2212 1\n)d \u2265 d ( 1\n2\u03b5 \u2212 1\n)d\u22121 = \u2126(d cd\u03b5).\ngives a running time of O(n \u00b7 T (\u03b1 log2 n, n) + n2/ log n), where T (d, n) is the cost of computing the domination set of a d-dimensional vector over a fixed set of n points (see Problem 2). Substituting the bounds of Lemma 4 into the time complexity, yields a polynomial preprocessing cost, and the following time bound for each multiplication:\nO((2\u03b5 \u2212 1)\u2212\u03b1 log2 n\u03b1n1+\u03b5 log2 n+ n2/ log n) = = O(n1+\u03b5\u2212\u03b1 log2(2\u03b5\u22121) log2 n+ n2/ log n)\nfor all \u03b5 \u2208 (0, 1). Setting \u03b5 = 2\u03b1, the exponent of the first term becomes 1 + \u03b1(2 \u2212 log2(4\u03b1 \u2212 1)) < 2 for all 0 < \u03b1 < 1/2. Therefore, the time complexity of the algorithm is O(n2/ log n).\nWe call the resulting algorithm geometric dominance faster Viterbi (GDFV)."}, {"heading": "Experimental evaluation", "text": "Methodology. All the algorithms are implemented in the C++11 language, compiled using the clang compiler and run on the OSX 10.10.3 operating system. The main memory is a 8GB 1600MHz DDR3 RAM, and the processor is an Intel Core i7-4850HQ CPU, with 6MB shared L3 cache. All the matrices and vectors used for the experiments have entries sampled from a uniform distribution over (0, 1] \u2286 R. Results. How does our proposed ORMV (max,+)-MUL algorithm for narrow matrices compare to the trivial one? We analyze the throughput of Algorithm 1, based on the geometric subroutines exposed in the proof of Lemma 4, comparing it with the trivial multiplication approach. For every chosen pair (n, t), we run 25 tests, each of which consists of an online multiplication of a n \u00d7 t matrix with 10 000 vectors. The results of our tests are summarized in Figure 1, where we see that our algorithm can be up to 4 times faster than the trivial one. This is mainly due to the fact that the number of accesses to the tree is much less than n \u00b7 t, and to the lower number of comparisons needed to find the answer. See the Appendix for a more thorough analysis of the average number of accesses to the decision trees.\nHow does the complete GDFV algorithm compare with the Viterbi algorithm? We experimentally evaluate the first phase of the GDFV algorithm, i.e. the computation of the qi(s) values defined in Equation 1. This is the most expensive task in the decoding of HMMs. We implement the algorithm as described in the proof of Theorem 3, using \u03b1 = 0.25, that is splitting the n \u00d7 n transition probability matrix T of the model in approximately n/2 blocks when n \u2264 4000. We summarize the results in Figure 2, where we see that our algorithm is roughly twice as fast as the Viterbi algorithm, in line with expectations. However, we note that the amount of memory required by our algorithm makes it impractical for larger values of \u03b1. Indeed, we have verified that when the memory pressure becomes high other factors slow down the implemented algorithm, such as cache and page misses, or, for bigger allocations, the hard drive latency."}, {"heading": "Conclusion and future works", "text": "In this paper, we give the first algorithm for the maximum a posteriori decoding (MAPD) of time-homogeneous Hidden Markov Models requiring asymptotically less than O(mn2) operations in the worst case. To this end, we first introduce an online geometric dominance reporting problem, and propose a simple divide-and-conquer solution, generalizing the classical result by (Chan 2008). At an intermediate step, we also give the first algorithm solving the online matrix-vector (max,+)-multiplication problem over R\u2217 in subquadratic time after a polynomial preprocessing of the matrix. Finally, we apply the faster multiplication to the MAPD problem.\nFurthermore, we think that our proposal paves the way to several unexplored questions which we intend to explore in future works: \u2022 cut larger polylogarithmic factors, by splitting cases in\nEquation 4 in a different manner, as in (Chan 2015); \u2022 study and implement a more succinct version of the deci-\nsion tree, in order to mitigate the memory footprint; \u2022 analyze the relationship of our work with other existing\nheuristics, such as CarpeDiem (Esposito and Radicioni 2009); \u2022 combine our speed-up to the one delivered by the approach in (Lifshits et al. 2009). Notice that this would require further assumptions on the observation sequence; \u2022 we note that the decision trees built by our algorithm could be implemented at a hardware level, resulting in specialized chips performing asymptotically less that O(n2) operations per observed symbol, in the worst case. \u2022 investigate \u201ctruly polynomially subquadratic\u201d solutions for the MAPD problem, at the expense of an exponential preprocessing of the model. As a concrete example, we present the following theorem, which is a corollary of Theorem 1 (a formal proof can be found in the Appendix): Theorem 4. The MAPD problem on time\u2013homogeneous HMMs can be solved in O(mn3/2\u221alog n) time with an O ( n1+ \u221a n/ logn ) time and space preprocessing."}, {"heading": "Acknowledgements", "text": "Massimo Cairo was supported by the Department of Computer Science, University of Verona under PhD grant \u201cComputational Mathematics and Biology\u201d.\nWe would like to thank Marco Elver, Nicola Gatti, Zu Kim, and Luigi Laura for their valuable suggestions."}, {"heading": "Appendix", "text": ""}, {"heading": "Missing proofs", "text": "We prove Theorem 1 and Theorem 4. Theorem 1. Problem 2 can be solved in O(d log |B| + |\u03b4B(p)|) time per query, after a O(|B|d+1) time and space preprocessing. Proof. For every coordinate k = 1, . . . , d, define the sequence bk1 , . . . ,b k |B| containing all the vectors in B ordered by their k\u2013th coordinate. Namely: bk1 [k] \u2264 bk2 [k] \u2264 \u00b7 \u00b7 \u00b7 \u2264 bk|B|[k]. Given an input vector p, let rk \u2208 {0, . . . , |B|} be the last position in which p can be inserted in the sequence bk1 , . . . ,b k |B|, while maintaining its ordering according to the k\u2013th coordinate. That is, rk is the only index that satisfies: bk1 [k], . . . ,b k rk\n[k] \u2264 p[k] < bkrk+1[k], . . . ,bk|B|[k]. Now, observe that the solution to a query can be obtained as:\n\u03b4B(p) = {b \u2208 B : b[k] \u2264 p[k] \u2200k} = \u22c2 k {bk1 , \u00b7 \u00b7 \u00b7 ,bkrk}.\nThe algorithm works as follows. At preprocessing time, we order the vectors according to each coordinate k = 1, . . . , d. Then, we precompute and store in a lookup table the solution \u03b4B(p) = \u22c2 k{bk1 , \u00b7 \u00b7 \u00b7 ,bkrk} for each of the |B + 1|d possible choices of the values r1, . . . , rd. This takes O(|B|d+1) time and space, and dominates the cost of the preprocessing phase.\nOnce the input vector p is received, rk is computed by binary search for each coordinate k = 1, . . . , d. Then, the solution for the values r1, . . . , rd just obtained is looked up in the table. The time for lookup is dominated by the O(d log |B|) time of the binary searches, yielding a total running time of O(d log |B| + |\u03b4B(p)|) to compute and return the output.\nTheorem 4. The MAPD problem on time\u2013homogeneous HMMs can be solved in O(mn3/2\u221alog n) time with an O ( n1+ \u221a n/ logn ) time and space preprocessing.\nProof. We apply consecutively Theorem 1, Lemma 2, Lemma 3 and Lemma 1. First, by Theorem 1, we solve the online dominance reporting problem in O(d log |B|) time. Second, we apply Lemma 2 to this algorithm, and obtain a solution for the online matrix\u2013vector (max,+)multiplication for a rectangular m \u00d7 t matrix running in O(m + t2 log |m|) time. Third, by Lemma 3, this algorithm can be used to solve the multiplication for n \u00d7 n matrices in O(n/t \u00b7 (n + t2 log n)) time for any choice of t. Choosing t = \u221a n/ log n, the running time is O(n\u221an log n) = O(n3/2 log1/2 n) with a preprocessing cost of O(n \u221a n/ logn+1) time and space. Finally, by Lemma 1, we employ this algorithm to solve the MAPD problem in O(mn3/2 log1/2 n) time, with O(n1+ \u221a n/ logn) time and space required for the preprocessing."}, {"heading": "Experimental evaluation", "text": ""}], "references": [{"title": "Hidden markov model based optical character recognition in the presence of deterministic transformations", "author": ["O.E. Agazzi", "Kuo", "S.-s."], "venue": "Pattern recognition 26(12):1813\u2013 1826.", "citeRegEx": "Agazzi et al\\.,? 1993", "shortCiteRegEx": "Agazzi et al\\.", "year": 1993}, {"title": "Necklaces, convolutions, and x+ y", "author": ["D. Bremner", "T.M. Chan", "E.D. Demaine", "J. Erickson", "F. Hurtado", "J. Iacono", "S. Langerman", "P. Taslakian"], "venue": "Algorithms\u2013 ESA 2006. Springer. 160\u2013171.", "citeRegEx": "Bremner et al\\.,? 2006", "shortCiteRegEx": "Bremner et al\\.", "year": 2006}, {"title": "All-pairs shortest paths with real weights in o (n 3/log n) time", "author": ["T.M. Chan"], "venue": "Algorithmica 50(2):236\u2013243.", "citeRegEx": "Chan,? 2008", "shortCiteRegEx": "Chan", "year": 2008}, {"title": "Speeding up the four russians algorithm by about one more logarithmic factor", "author": ["T.M. Chan"], "venue": "Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, 212\u2013217. SIAM.", "citeRegEx": "Chan,? 2015", "shortCiteRegEx": "Chan", "year": 2015}, {"title": "Implementing em and viterbi algorithms for hidden markov model in linear memory", "author": ["A. Churbanov", "S. Winters-Hilt"], "venue": "BMC bioinformatics 9(1):224.", "citeRegEx": "Churbanov and Winters.Hilt,? 2008", "shortCiteRegEx": "Churbanov and Winters.Hilt", "year": 2008}, {"title": "A more efficient algorithm for the min-plus multiplication", "author": ["W. Dobosiewicz"], "venue": "International journal of computer mathematics 32(1-2):49\u201360.", "citeRegEx": "Dobosiewicz,? 1990", "shortCiteRegEx": "Dobosiewicz", "year": 1990}, {"title": "Carpediem: Optimizing the viterbi algorithm and applications to supervised sequential learning", "author": ["R. Esposito", "D.P. Radicioni"], "venue": "The Journal of Machine Learning Research 10:1851\u20131880.", "citeRegEx": "Esposito and Radicioni,? 2009", "shortCiteRegEx": "Esposito and Radicioni", "year": 2009}, {"title": "Fast algorithms for large-state-space hmms with applications to web usage analysis", "author": ["P.F. Felzenszwalb", "D.P. Huttenlocher", "J.M. Kleinberg"], "venue": "Advances in NIPS 16:409\u2013416.", "citeRegEx": "Felzenszwalb et al\\.,? 2004", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2004}, {"title": "Maximum likelihood linear transformations for hmm-based speech recognition", "author": ["M.J. Gales"], "venue": "Computer speech & language 12(2):75\u201398.", "citeRegEx": "Gales,? 1998", "shortCiteRegEx": "Gales", "year": 1998}, {"title": "Reduced space sequence alignment", "author": ["J. Grice", "R. Hughey", "D. Speck"], "venue": "Computer applications in the biosciences : CABIOS 13(1):45\u201353.", "citeRegEx": "Grice et al\\.,? 1997", "shortCiteRegEx": "Grice et al\\.", "year": 1997}, {"title": "A generalized hidden markov model for the recognition of human genes in dna", "author": ["D.K.D. Haussler", "Eeckman", "M.G.R.F.H."], "venue": "Proc. Int. Conf. on Intelligent Systems for Molecular Biology, St. Louis, 134\u2013142.", "citeRegEx": "Haussler et al\\.,? 1996", "shortCiteRegEx": "Haussler et al\\.", "year": 1996}, {"title": "Unifying and strengthening hardness for dynamic problems via the online matrix-vector multiplication conjecture", "author": ["M. Henzinger", "S. Krinninger", "D. Nanongkai", "T. Saranurak"], "venue": "Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC \u201915,", "citeRegEx": "Henzinger et al\\.,? 2015", "shortCiteRegEx": "Henzinger et al\\.", "year": 2015}, {"title": "Hidden Markov models for speech recognition, volume 2004", "author": ["X.D. Huang", "Y. Ariki", "M.A. Jack"], "venue": "Edinburgh university press Edinburgh.", "citeRegEx": "Huang et al\\.,? 1990", "shortCiteRegEx": "Huang et al\\.", "year": 1990}, {"title": "Efficient staggered decoding for sequence labeling", "author": ["N. Kaji", "Y. Fujiwara", "N. Yoshinaga", "M. Kitsuregawa"], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, 485\u2013494. Association for Computational Linguistics.", "citeRegEx": "Kaji et al\\.,? 2010", "shortCiteRegEx": "Kaji et al\\.", "year": 2010}, {"title": "Robust part-of-speech tagging using a hidden markov model", "author": ["J. Kupiec"], "venue": "Computer Speech & Language 6(3):225\u2013242.", "citeRegEx": "Kupiec,? 1992", "shortCiteRegEx": "Kupiec", "year": 1992}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J.D. Lafferty", "A. McCallum", "F.C.N. Pereira"], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning, ICML \u201901, 282\u2013289. San Francisco, CA, USA: Morgan", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "The mailman algorithm: A note on matrix\u2013vector multiplication", "author": ["E. Liberty", "S.W. Zucker"], "venue": "Information Processing Letters 109(3):179\u2013182.", "citeRegEx": "Liberty and Zucker,? 2009", "shortCiteRegEx": "Liberty and Zucker", "year": 2009}, {"title": "Speeding up hmm decoding and training by exploiting sequence repetitions", "author": ["Y. Lifshits", "S. Mozes", "O. Weimann", "M. Ziv-Ukelson"], "venue": "Algorithmica 54(3):379\u2013399.", "citeRegEx": "Lifshits et al\\.,? 2009", "shortCiteRegEx": "Lifshits et al\\.", "year": 2009}, {"title": "Genome-Scale Algorithm Design", "author": ["V. M\u00e4kinen", "D. Belazzougui", "F. Cunial", "A.I. Tomescu"], "venue": "Cambridge University Press.", "citeRegEx": "M\u00e4kinen et al\\.,? 2015", "shortCiteRegEx": "M\u00e4kinen et al\\.", "year": 2015}, {"title": "The on-line viterbi algorithm", "author": ["R. \u0160r\u00e1mek"], "venue": "KAI FMFI UK, Bratislava, m\u00e1j.", "citeRegEx": "\u0160r\u00e1mek,? 2007", "shortCiteRegEx": "\u0160r\u00e1mek", "year": 2007}, {"title": "Real-time american sign language recognition using desk and wearable computer based video", "author": ["T. Starner", "J. Weaver", "A. Pentland"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 20(12):1371\u20131375.", "citeRegEx": "Starner et al\\.,? 1998", "shortCiteRegEx": "Starner et al\\.", "year": 1998}, {"title": "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm", "author": ["A.J. Viterbi"], "venue": "Information Theory, IEEE Transactions on 13(2):260\u2013269.", "citeRegEx": "Viterbi,? 1967", "shortCiteRegEx": "Viterbi", "year": 1967}, {"title": "Matrix-vector multiplication in subquadratic time:(some preprocessing required)", "author": ["R. Williams"], "venue": "SODA, volume 7, 995\u20131001.", "citeRegEx": "Williams,? 2007", "shortCiteRegEx": "Williams", "year": 2007}, {"title": "Hidden markov models and their applications in biological sequence analysis", "author": ["Yoon", "B.-J."], "venue": "Current genomics 10(6):402.", "citeRegEx": "Yoon and B..J.,? 2009", "shortCiteRegEx": "Yoon and B..J.", "year": 2009}], "referenceMentions": [{"referenceID": 21, "context": "Hidden Markov Models (HMMs) are simple probabilistic models originally introduced (Viterbi 1967) to decode convolutional codes.", "startOffset": 82, "endOffset": 96}, {"referenceID": 8, "context": "Due to their universal and fundamental nature, these models have successfully been applied in several fields, with many important applications, such as gene prediction (Haussler and Eeckman 1996), speech, gesture and optical character recognition (Gales 1998; Huang, Ariki, and Jack 1990; Starner, Weaver, and Pentland 1998; Agazzi and Kuo 1993), and part-of-speech tagging (Kupiec 1992).", "startOffset": 247, "endOffset": 345}, {"referenceID": 14, "context": "Due to their universal and fundamental nature, these models have successfully been applied in several fields, with many important applications, such as gene prediction (Haussler and Eeckman 1996), speech, gesture and optical character recognition (Gales 1998; Huang, Ariki, and Jack 1990; Starner, Weaver, and Pentland 1998; Agazzi and Kuo 1993), and part-of-speech tagging (Kupiec 1992).", "startOffset": 374, "endOffset": 387}, {"referenceID": 18, "context": "rently they hold a recognized place in that field (Yoon 2009; M\u00e4kinen et al. 2015).", "startOffset": 50, "endOffset": 82}, {"referenceID": 21, "context": "Traditionally, the MAPD problem is solved by the Viterbi algorithm (Viterbi 1967), inO(mn2) time and O(mn) memory for any model of size n and observation sequence of length m.", "startOffset": 67, "endOffset": 81}, {"referenceID": 19, "context": "oped (\u0160r\u00e1mek 2007; Churbanov and Winters-Hilt 2008; Felzenszwalb, Huttenlocher, and Kleinberg 2004; Esposito and Radicioni 2009; Kaji et al. 2010).", "startOffset": 5, "endOffset": 146}, {"referenceID": 4, "context": "oped (\u0160r\u00e1mek 2007; Churbanov and Winters-Hilt 2008; Felzenszwalb, Huttenlocher, and Kleinberg 2004; Esposito and Radicioni 2009; Kaji et al. 2010).", "startOffset": 5, "endOffset": 146}, {"referenceID": 6, "context": "oped (\u0160r\u00e1mek 2007; Churbanov and Winters-Hilt 2008; Felzenszwalb, Huttenlocher, and Kleinberg 2004; Esposito and Radicioni 2009; Kaji et al. 2010).", "startOffset": 5, "endOffset": 146}, {"referenceID": 13, "context": "oped (\u0160r\u00e1mek 2007; Churbanov and Winters-Hilt 2008; Felzenszwalb, Huttenlocher, and Kleinberg 2004; Esposito and Radicioni 2009; Kaji et al. 2010).", "startOffset": 5, "endOffset": 146}, {"referenceID": 17, "context": "In (Lifshits et al. 2009), the authors show a method to speed up the decoding of HMMs by aO(logm) factor, by precomputing all possible observation sequences of length logm, in a fashion similar to the Four Russians method.", "startOffset": 3, "endOffset": 25}, {"referenceID": 22, "context": "Algorithms faster than the trivial quadratic one are known for the OMV MUL problem over finite semirings (Williams 2007), as well as over real numbers with standard (+, \u00b7)-multiplication, if the matrix has only a constant number of distinct values (Liberty and Zucker 2009).", "startOffset": 105, "endOffset": 120}, {"referenceID": 16, "context": "Algorithms faster than the trivial quadratic one are known for the OMV MUL problem over finite semirings (Williams 2007), as well as over real numbers with standard (+, \u00b7)-multiplication, if the matrix has only a constant number of distinct values (Liberty and Zucker 2009).", "startOffset": 248, "endOffset": 273}, {"referenceID": 5, "context": "In the specific case of real (max,+)-multiplication, subcubic algorithms have been known for years (Dobosiewicz 1990; Chan 2008; 2015) for the matrix-matrix multiplication problem, with important applications to graph theory and boolean matrix multiplication, among others.", "startOffset": 99, "endOffset": 134}, {"referenceID": 2, "context": "In the specific case of real (max,+)-multiplication, subcubic algorithms have been known for years (Dobosiewicz 1990; Chan 2008; 2015) for the matrix-matrix multiplication problem, with important applications to graph theory and boolean matrix multiplication, among others.", "startOffset": 99, "endOffset": 134}, {"referenceID": 11, "context": "Note that the ORMV (max,+)-MUL can be used to compute the OMV MUL over the Boolean semiring: for this problem, it has been conjectured (Henzinger et al. 2015) that no \u201ctruly polynomially subquadratic\u201d algorithm1 exists for the ORMV (max,+)-MUL problem.", "startOffset": 135, "endOffset": 158}, {"referenceID": 1, "context": "We reduce the ORMV (max,+)-MUL problem to a multi-dimensional geometric dominance problem, following an approach similar to that of (Bremner et al. 2006; Chan 2008).", "startOffset": 132, "endOffset": 164}, {"referenceID": 2, "context": "We reduce the ORMV (max,+)-MUL problem to a multi-dimensional geometric dominance problem, following an approach similar to that of (Bremner et al. 2006; Chan 2008).", "startOffset": 132, "endOffset": 164}, {"referenceID": 2, "context": "divide-and-conquer algorithm, which can be regarded as a transposition of the algorithm of (Chan 2008) to the online setting.", "startOffset": 91, "endOffset": 102}, {"referenceID": 2, "context": "Our key contributions are as follows: (i) we extend the geometric dominance reporting problem introduced in (Chan 2008) to the online setting; (ii) we solve the ORMV (max,+)-MUL problem in O(n2/ log n) time after a polynomial preprocessing of the n \u00d7 n matrix; (iii) we show an algorithm solving the MAPD problem on timehomogeneous HMMs in O(mn2/ log n) time in the worstcase, after a polynomial preprocessing of the model.", "startOffset": 108, "endOffset": 119}, {"referenceID": 19, "context": "for it, including the memory saving ones (\u0160r\u00e1mek 2007; Churbanov and Winters-Hilt 2008), are still applicable once the first part has been carried out based on our approach.", "startOffset": 41, "endOffset": 87}, {"referenceID": 4, "context": "for it, including the memory saving ones (\u0160r\u00e1mek 2007; Churbanov and Winters-Hilt 2008), are still applicable once the first part has been carried out based on our approach.", "startOffset": 41, "endOffset": 87}, {"referenceID": 2, "context": "Theorem 2 ((Chan 2008), Lemma 2.", "startOffset": 11, "endOffset": 22}, {"referenceID": 2, "context": "To this end, we first introduce an online geometric dominance reporting problem, and propose a simple divide-and-conquer solution, generalizing the classical result by (Chan 2008).", "startOffset": 168, "endOffset": 179}, {"referenceID": 3, "context": "Furthermore, we think that our proposal paves the way to several unexplored questions which we intend to explore in future works: \u2022 cut larger polylogarithmic factors, by splitting cases in Equation 4 in a different manner, as in (Chan 2015); \u2022 study and implement a more succinct version of the deci-", "startOffset": 230, "endOffset": 241}, {"referenceID": 6, "context": "sion tree, in order to mitigate the memory footprint; \u2022 analyze the relationship of our work with other existing heuristics, such as CarpeDiem (Esposito and Radicioni 2009); \u2022 combine our speed-up to the one delivered by the approach in (Lifshits et al.", "startOffset": 143, "endOffset": 172}, {"referenceID": 17, "context": "sion tree, in order to mitigate the memory footprint; \u2022 analyze the relationship of our work with other existing heuristics, such as CarpeDiem (Esposito and Radicioni 2009); \u2022 combine our speed-up to the one delivered by the approach in (Lifshits et al. 2009).", "startOffset": 237, "endOffset": 259}], "year": 2015, "abstractText": "In this paper, we present a novel algorithm for the maximum a posteriori decoding (MAPD) of timehomogeneous Hidden Markov Models (HMM), improving the worst-case running time of the classical Viterbi algorithm by a logarithmic factor. In our approach, we interpret the Viterbi algorithm as a repeated computation of matrix-vector (max,+)multiplications. On time-homogeneous HMMs, this computation is online: a matrix, known in advance, has to be multiplied with several vectors revealed one at a time. Our main contribution is an algorithm solving this version of matrix-vector (max,+)-multiplication in subquadratic time, by performing a polynomial preprocessing of the matrix. Employing this fast multiplication algorithm, we solve the MAPD problem in O(mn/ logn) time for any time-homogeneous HMM of size n and observation sequence of length m, with an extra polynomial preprocessing cost negligible for m > n. To the best of our knowledge, this is the first algorithm for the MAPD problem requiring subquadratic time per observation, under the only assumption \u2013 usually verified in practice \u2013 that the transition probability matrix does not change with time.", "creator": "TeX"}}}