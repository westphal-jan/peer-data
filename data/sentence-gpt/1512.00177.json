{"id": "1512.00177", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2015", "title": "LSTM Neural Reordering Feature for Statistical Machine Translation", "abstract": "Artificial neural networks are powerful models, which have been widely applied into many aspects of machine translation, such as language modeling and translation modeling. Though notable improvements have been made in these areas, the reordering problem still remains a challenge in statistical machine translations. In this paper, we present a novel neural reordering model that directly models word pairs and alignment and also attempts to quantify a number of problems associated with the reordering problem.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 1 Dec 2015 08:43:19 GMT  (322kb)", "http://arxiv.org/abs/1512.00177v1", "6 pages"], ["v2", "Tue, 8 Mar 2016 01:17:22 GMT  (0kb,I)", "http://arxiv.org/abs/1512.00177v2", "6 pages, withdrawn by the author due to a error in formula"], ["v3", "Thu, 16 Jun 2016 10:01:49 GMT  (240kb,D)", "http://arxiv.org/abs/1512.00177v3", "6 pages, accepted by NAACL2016 short paper"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["yiming cui", "shijin wang", "jianfeng li"], "accepted": true, "id": "1512.00177"}, "pdf": {"name": "1512.00177.pdf", "metadata": {"source": "CRF", "title": "LSTM Neural Reordering Feature for Statistical Machine Translation", "authors": ["Yiming Cui", "Shijin Wang", "Jianfeng Li", "Yuguang Wang"], "emails": ["ymcui@iflytek.com", "sjwang3@iflytek.com", "jfli3@iflytek.com", "ygwang2@iflytek.com"], "sections": [{"heading": "1 Introduction", "text": "In statistical machine translation, the language model, translation model, and reordering model are the three most important components. Among these models, the reordering model plays an important role in phrase-based machine translation (Koehn et al., 2003), and it still remains a major challenge in current study.\nIn recent years, various phrase reordering methods have been proposed for phrase-based SMT systems, which can be classified into two broad categories:\n(1) Distance-based RM: Penalize phrase displacements with respect to the degree of non-monotonicity (Koehn et al., 2003). (2) Lexicalized RM: Conditions reordering probabilities on current phrase pairs. According to the orientation determinants, lexicalized reordering model can further be classified into word-based RM (Tillman, 2004), phrase-based RM (Koehn et al., 2007), and hierarchical phrase-based RM (Galley and Manning, 2008).\nFurthermore, some researchers proposed a reordering model that conditions both current and previous phrase pairs by utilizing recursive auto-encoders (Li et al., 2014), which is applied into decoding step.\nIn this paper, we propose a novel neural reordering feature by including longer context for predicting orientations. We utilize a long short-term memory recurrent neural network (LSTM-RNN) (Hochreiter and Schmidhuber, 1997), and directly models word pairs to predict its most probable orientation. Experimental results on NIST OpenMT12 Arabic-English and Chinese-English translation show that our neural reordering model achieves significant improvements over various baselines via 1000-best rescoring."}, {"heading": "2 Related Work", "text": "Recently, various neural network models have been applied into machine translation. Feed-forward neural language model was first proposed by Bengio et al. (2003), which was a breakthrough in language modeling. Mikolov et al. (2011) proposed to use recurrent neural network in language modeling, which can include much longer context history for predicting next word. Experimental results show that RNNbased language model significantly outperforms standard feed-forward language model.\nDevlin et al. (2014) proposed a neural network joint model (NNJM) by conditioning both source and target language context for target word predicting. Though the network architecture is a simple feedforward neural network, the results have shown significant improvements over state-of-the-art baselines.\nSundermeyer et al. (2014) also put forward a neural translation model, by utilizing LSTM-based RNN and Bidirectional RNN. In bidirectional RNNs, the target word is conditioned on not only the history but also future source context, which forms a full source sentence for predicting target words.\nLi et al. (2013) proposed to use a recursive auto-encoder (RAE) to map each phrase pairs into continuous vectors, and handle reordering problems with a classifier. Also, they suggested that by both including current and previous phrase pairs to determine the phrase orientations could achieve further improvements in accuracy (Li et al., 2014).\nBy far, we have noticed that this is the first time to use LSTM-RNN in reordering model. We could include much longer context information to determine phrase orientations using RNN architecture. Furthermore, by utilizing the LSTM layer, the network is able to capture much longer range dependencies than standard RNNs.\nBecause we need to record fixed history information in decoding step, we only utilize LSTM-RNN reordering model as a feature in 1000-best rescoring step. As word alignments are known after generating n-best list, it is possible to use LSTM-RNN reordering model to score each hypothesis."}, {"heading": "3 Lexicalized Reordering Model", "text": "In traditional statistical machine translations, lexicalized reordering models (Koehn et al., 2007) have been widely used. It considers alignments of current and previous phrase pairs to determine the orientation.\nFormally, when given source language sentence \ud835\udc53 = \ud835\udc53#, \u2026 , \ud835\udc53& , target language sentence \ud835\udc52 = \ud835\udc52#, \u2026 , \ud835\udc52& , and phrase alignment\ud835\udc4e = \ud835\udc4e#, \u2026 , \ud835\udc4e& , the lexicalized reordering model can be illustrated in\nEquation 1, which only conditions on \ud835\udc4e)*# and \ud835\udc4e), i.e. previous and current alignment.\n\ud835\udc5d \ud835\udc90 \ud835\udc86, \ud835\udc87 = \ud835\udc43 \ud835\udc5c) \ud835\udc52), \ud835\udc5312, \ud835\udc4e)*#, \ud835\udc4e)\n&\n)3#\n(1)\nIn Equation 1, the \ud835\udc5c) represents the set of phrase orientations. For example, in the most commonly used MSD-based orientation type, \ud835\udc5c) takes three values: M stands for monotone, S for swap, and D for discontinuous. The definition of MSD-based orientation is shown in Equation 2.\n\ud835\udc5c) = \ud835\udc40, \ud835\udc56\ud835\udc53 \ud835\udc4e) \u2212 \ud835\udc4e)*# = 1 \ud835\udc46, \ud835\udc56\ud835\udc53 \ud835\udc4e) \u2212 \ud835\udc4e)*# = \u22121 \ud835\udc37, \ud835\udc56\ud835\udc53 |\ud835\udc4e) \u2212 \ud835\udc4e)*#| \u2260 1\n(2)\nFor other orientation types, such as LR and MSLR are also widely used, whose definition can be found on Moses official website1.\nRecent studies on reordering model suggest that by also conditioning previous phrase pairs can improve context sensitivity and reduce reordering ambiguity."}, {"heading": "4 LSTM Neural Reordering Model", "text": "In order to include more context information for determining reordering, we propose to use a recurrent neural network, which has been shown to perform considerably better than standard feed-forward architectures in sequence prediction (Mikolov et al., 2011). However, RNN with conventional back-propagation training suffers from gradient vanishing issues (Bengio et al., 1994).\nLater, the long short-term memory was proposed for solving gradient vanishing problem, and it could catch longer context than standard RNNs with sigmoid activation functions. In this paper, we adopt LSTM architecture for training neural reordering model.\n1 http://www.statmt.org/moses/?n=FactoredTraining.BuildReorderingModel"}, {"heading": "4.1 Training Data Processing", "text": "For reducing model complexity and easy implementation, our neural reordering model is purely lexicalized and trained on word-level. We will take LR orientation for explanations, while other orientation types (MSD, MSLR) can be induced similarly. Given a sentence pair and its alignment information, we can induce the word-based reordering information by following steps. Note that, we always evaluate the model in the order of target sentence. Figure 1 shows an example of data processing.\n(1) If current target word is one-to-one alignment, then we can directly induce its orientations (left or right). (2) If current source/target word is one-to-many alignment, then we judge its orientation by considering its first aligned target/source word, and the other aligned target/source words are annotated as \u201c<follow>\u201d reordering type, which means these word pairs inherent the orientation of previous word pair. (3) If current source/target word is not aligned to any target/source words, we introduce a \u201c<null>\u201d token in its opposite side, and annotate this word pair as \u201c<follow>\u201d reordering type."}, {"heading": "4.2 LSTM Network Architecture", "text": "After processing the training data, we can directly utilize the word pairs and its orientation to train a neural reordering model. Given a word pair and its orientation, a neural reordering model can be illustrated by Equation 3.\n\ud835\udc5d \ud835\udc90 \ud835\udc86, \ud835\udc87 = \ud835\udc43 \ud835\udc5c) \ud835\udc52#) , \ud835\udc53# 12, \ud835\udc4e)*#, \ud835\udc4e)\n&\n)3#\n(3)\nWhere \ud835\udc52#) = {\ud835\udc52#, \u2026 , \ud835\udc52)}, \ud835\udc53# 12 = {\ud835\udc53#, \u2026 , \ud835\udc5312}. Inclusion of history word pairs is done with recurrent neural network, which is known for its capability of learning history information. The LSTM RNN architecture for reordering model is depicted in Figure 2, and corresponding equations are shown in Equation 4 to 6. \ud835\udc66) = \ud835\udc4a# \u2217 \ud835\udc5312 + \ud835\udc4aC \u2217 \ud835\udc52) (4)\n\ud835\udc67) = \ud835\udc3f\ud835\udc46\ud835\udc47\ud835\udc40(\ud835\udc66),\ud835\udc4aH, \ud835\udc66#)*#) (5)\n\ud835\udc43 \ud835\udc5c) \ud835\udc52#) , \ud835\udc53# 12, \ud835\udc4e)*#, \ud835\udc4e) = \ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a(\ud835\udc4aM \u2217 \ud835\udc67)) (6)\nThe input layer consists both source and target language word, which is in one-hot representation. Then we perform a linear transformation of input layer to a projection layer. We adopt extended-LSTM as our hidden layer implementation, which consists of three gating units, i.e. input, forget and output gates. We omit rather extensive LSTM equations here, which can be found in (Graves and Schmidhuber, 2005). The output layer is composed by orientation types. For e.g., in LR condition, the output layer\ncontains two units: \u201cleft\u201d and \u201cright\u201d orientation. Finally, we apply softmax function to obtain normalized probabilities of each orientation."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Setups", "text": "We tested our approach on Arabic-English and Chinese-English translation. The training corpus selected 7M words of high quality parallel training for Arabic and 4M words for Chinese, which is selected from NIST OpenMT12. We used the SAMA tokenizer2 for Arabic word tokenization, and in-house segmenter for Chinese words. The statistics of development and test sets are illustrated in Table 1. All development and test sets have 4 references for each segment.\nThe baseline systems are built with the open-source phrase-based SMT toolkit Moses (Koehn et al., 2007). Word alignment and phrase extraction are done by GIZA++ (Och and Ney, 2003) with L0-normalization (Vaswani et al., 2012), and grow-diag-final refinement rule (Koehn et al., 2003). Monolingual part of training data is used to train a 5-gram language model using SRILM (Stolcke, 2002). Parameter tuning is done by K-best MIRA (Cherry and Foster, 2012). For guarantee of result stability, we tune every system 5 times independently, and take the average BLEU score (Clark et al., 2011). The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). The statistical significance test is also carried out with paired bootstrap resampling method (Koehn, 2004) with \ud835\udc5d < 0.001 intervals. Our models are evaluated in a 1000-best rescoring step, and all features in n-best list as well as LSTM-RNN reordering feature are retuned via K-best MIRA algorithm.\nFor neural network training, we use all parallel text in the baseline training. As a trade-off between computational cost and performance, the projection layer and hidden layer are set to 100, which is enough for our task. We use an initial learning rate of 0.01, and trained model for a total of 10 epochs. Input and output vocabulary are set to 100K and 50K respectively, and other words are mapped to a \u201c<UNK>\u201d token."}, {"heading": "5.2 Results on different orientation types", "text": "Firstly, we test our model on the baseline that contains word-based reordering model with LR orientation. The results are shown in Table 2. As we can see that, among various orientation types (LR, MSD, MSLR), our model could give consistent improvements over baseline system. The overall BLEU improvements range from 0.42 to 0.79 for Arabic-English, and 0.31 to 0.72 for Chinese-English. All results are significantly better than baselines (\ud835\udc5d < 0.001 level). 2 https://catalog.ldc.upenn.edu/LDC2010L01\nIn the meantime, we also find that \u201cLeft-Right\u201d based orientation methods, such as LR and MSLR, consistently outperform MSD-based orientations. The may caused by non-separability problem, which means that MSD-based methods are vulnerable to the change of context, and weak in resolving reordering ambiguities. Similar conclusion can be found in Li et al. (2014)."}, {"heading": "5.3 Results on different reordering baselines", "text": "We also test our approach on various baselines, which either contains word-based, phrase-based, or hierarchical phrase-based reordering model. We only show the results of MSLR orientation, which is relatively superior than others according to the results in Section 5.2.\nIn Table 3, we can see that though we add a strong hierarchical phrase based reordering model in the baseline, our model can still bring a maximum gain of 0.59 BLEU score, which suggest that our model is applicable and robust in various circumstances. However, we have noticed that the gains in Ar-En system is relatively greater than that in Zh-En system. This is probably because hierarchical reordering features tend to work better for Chinese words, and thus our model will bring little remedy to its baseline."}, {"heading": "6 Conclusion", "text": "We present a novel work that build a reordering model using LSTM-RNN, which is much sensitive to the change of context and introduce rich context information for reordering prediction. Furthermore, the proposed model is purely lexicalized and easy to realize. Experimental results on 1000-best rescoring show that our neural reordering feature is robust and could give consistent improvements over various baseline systems. In future, we are planning to extend our word-based LSTM reordering model to phrase-based one, in order to dissolve much more ambiguities and improve reordering accuracy.\nReference Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient descent\nis difficult. In IEEE Transactions on Neural Networks, 5(2): 157-166.\nYoshua Bengio, R\u00e9jean Ducharme, PascalVincent, and Christian Jauvin. 2003. A neural probabilistic language model. In Journal of Machine Learning Research, 3:1137-1155.\nColin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Proceedings of 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human language technologies, pages 427-436.\nJonathon H. Clark, Chris Dyer, Alon Lavie, and Noah A.Smith. 2011. Better hypothesis testing for statistical machine translation: controlling for optimizer instability. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers, pages 176-181.\nJacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast\nand Robust Neural Network Joint Models for Statistical Machine Translation. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics, pages 1370-1380.\nMichel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 848-856.\nAlex Graves and J\u00fcrgen Schmidhuber. 2005. Framewise phoneme classification with bidirectional LSTM networks. In Proceedings of ICJNN, pages 2047-2052.\nSepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. In Neural computation, 9(8): 1735- 1780.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris dyer, Ond\u0159ej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL) on Interactive Poster and Demonstration Sessions, pages 177-180.\nPhilipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388-395.\nPhilipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational linguistics on Human Language Technology, vol. 1, pages 48-54.\nPeng Li, Yang Liu, and Maosong Sun. 2013. Recursive autoencoders for ITG-based translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567-577.\nPeng Li, Yang Liu, Maosong Sun, Tatsuya Izuha, and Dakun Zhang. 2014. A Neural Reordering Model for Phrasebased Translation. In Proceedings of the 25th International Conference on Computational Linguistics: Technical Papers, pages 1897-1907, Dublin, Ireland. Tomas Mikolov, Stefan Kombrink, Lukas Burget, JH Cernocky, and Sanjeev Khudanpur. 2011. Extensions of recurrent neural network language model. In 2011 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 5528-5531.\nFranz Josef Och and Hermann Ney. 2000. A comparison of alignment models for statistical machine translation. In Proceedings of the 18th International Conference on Computational Linguistics, pages 1086-1090.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA.\nMartin Sundermeyer, Tamer Alkhouli, Joern Wuebker, and Hermann Ney. 2014. Translation Modeling with Bidirectional Recurrent Neural Networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 14-25.\nAndreas Stolcke. 2002. SRILM \u2013 An Extensible Language Modeling Toolkit. In Proceedings of the International Conference on Speech and Language Processing (ICSLP), vol. 2, pages 901-904, Denver, CO.\nChristoph Tillman. 2004. A Unigram Orientation Model for Statistical Machine Translation. In Proceedings of 2004 Conference of the North American Chapter of the Association for Computational Linguistics: Human language technologies, pages 101-104.\nAshish Vaswani, Liang Huang, and David Chiang. 2012. Smaller Alignment Models for Better Translations: Unsupervised word Alignment with the l0-norm. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 311-319."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi."], "venue": "IEEE Transactions on Neural Networks, 5(2): 157-166.", "citeRegEx": "Bengio et al\\.,? 1994", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "PascalVincent", "Christian Jauvin."], "venue": "Journal of Machine Learning Research, 3:1137-1155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Batch tuning strategies for statistical machine translation", "author": ["Colin Cherry", "George Foster."], "venue": "Proceedings of 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human language technologies, pages 427-436.", "citeRegEx": "Cherry and Foster.,? 2012", "shortCiteRegEx": "Cherry and Foster.", "year": 2012}, {"title": "Better hypothesis testing for statistical machine translation: controlling for optimizer instability", "author": ["Jonathon H. Clark", "Chris Dyer", "Alon Lavie", "Noah A.Smith."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers, pages 176-181.", "citeRegEx": "Clark et al\\.,? 2011", "shortCiteRegEx": "Clark et al\\.", "year": 2011}, {"title": "Fast and Robust Neural Network Joint Models for Statistical Machine Translation", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul."], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics, pages 1370-1380.", "citeRegEx": "Devlin et al\\.,? 2014", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "A simple and effective hierarchical phrase reordering model", "author": ["Michel Galley", "Christopher D. Manning."], "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 848-856.", "citeRegEx": "Galley and Manning.,? 2008", "shortCiteRegEx": "Galley and Manning.", "year": 2008}, {"title": "Framewise phoneme classification with bidirectional LSTM networks", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "Proceedings of ICJNN, pages 2047-2052.", "citeRegEx": "Graves and Schmidhuber.,? 2005", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8): 17351780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Moses: open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris dyer", "Ond\u0159ej Bojar", "Alexandra Constantin", "Evan Herbst."], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL) on Interactive Poster and Demonstration Sessions, pages 177-180.", "citeRegEx": "Koehn et al\\.,? 2007", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn."], "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388-395.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational linguistics on Human Language Technology, vol. 1, pages 48-54.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Recursive autoencoders for ITG-based translation", "author": ["Peng Li", "Yang Liu", "Maosong Sun."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567-577.", "citeRegEx": "Li et al\\.,? 2013", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "A Neural Reordering Model for Phrasebased Translation", "author": ["Peng Li", "Yang Liu", "Maosong Sun", "Tatsuya Izuha", "Dakun Zhang."], "venue": "Proceedings of the 25th International Conference on Computational Linguistics: Technical Papers, pages 1897-1907, Dublin, Ireland. Tomas Mikolov, Stefan Kombrink, Lukas Burget, JH Cernocky, and Sanjeev Khudanpur. 2011. Extensions of recurrent neural network language model. In 2011 IEEE International Conference on Acoustics, Speech and", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "A comparison of alignment models for statistical machine translation", "author": ["Franz Josef Och", "Hermann Ney."], "venue": "Proceedings of the 18th International Conference on Computational Linguistics, pages 1086-1090.", "citeRegEx": "Och and Ney.,? 2000", "shortCiteRegEx": "Och and Ney.", "year": 2000}, {"title": "Bleu: a Method for Automatic Evaluation of Machine Translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Translation Modeling with Bidirectional Recurrent Neural Networks", "author": ["Martin Sundermeyer", "Tamer Alkhouli", "Joern Wuebker", "Hermann Ney."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 14-25.", "citeRegEx": "Sundermeyer et al\\.,? 2014", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2014}, {"title": "SRILM \u2013 An Extensible Language Modeling Toolkit", "author": ["Andreas Stolcke."], "venue": "Proceedings of the International Conference on Speech and Language Processing (ICSLP), vol. 2, pages 901-904, Denver, CO.", "citeRegEx": "Stolcke.,? 2002", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "A Unigram Orientation Model for Statistical Machine Translation", "author": ["Christoph Tillman."], "venue": "Proceedings of 2004 Conference of the North American Chapter of the Association for Computational Linguistics: Human language technologies, pages 101-104.", "citeRegEx": "Tillman.,? 2004", "shortCiteRegEx": "Tillman.", "year": 2004}, {"title": "Smaller Alignment Models for Better Translations: Unsupervised word Alignment with the l0-norm", "author": ["Ashish Vaswani", "Liang Huang", "David Chiang."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 311-319.", "citeRegEx": "Vaswani et al\\.,? 2012", "shortCiteRegEx": "Vaswani et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "Among these models, the reordering model plays an important role in phrase-based machine translation (Koehn et al., 2003), and it still remains a major challenge in current study.", "startOffset": 101, "endOffset": 121}, {"referenceID": 10, "context": "In recent years, various phrase reordering methods have been proposed for phrase-based SMT systems, which can be classified into two broad categories: (1) Distance-based RM: Penalize phrase displacements with respect to the degree of non-monotonicity (Koehn et al., 2003).", "startOffset": 251, "endOffset": 271}, {"referenceID": 17, "context": "According to the orientation determinants, lexicalized reordering model can further be classified into word-based RM (Tillman, 2004), phrase-based RM (Koehn et al.", "startOffset": 117, "endOffset": 132}, {"referenceID": 8, "context": "According to the orientation determinants, lexicalized reordering model can further be classified into word-based RM (Tillman, 2004), phrase-based RM (Koehn et al., 2007), and hierarchical phrase-based RM (Galley and Manning, 2008).", "startOffset": 150, "endOffset": 170}, {"referenceID": 5, "context": ", 2007), and hierarchical phrase-based RM (Galley and Manning, 2008).", "startOffset": 42, "endOffset": 68}, {"referenceID": 12, "context": "Furthermore, some researchers proposed a reordering model that conditions both current and previous phrase pairs by utilizing recursive auto-encoders (Li et al., 2014), which is applied into decoding step.", "startOffset": 150, "endOffset": 167}, {"referenceID": 7, "context": "We utilize a long short-term memory recurrent neural network (LSTM-RNN) (Hochreiter and Schmidhuber, 1997), and directly models word pairs to predict its most probable orientation.", "startOffset": 72, "endOffset": 106}, {"referenceID": 0, "context": "Feed-forward neural language model was first proposed by Bengio et al. (2003), which was a breakthrough in language modeling.", "startOffset": 57, "endOffset": 78}, {"referenceID": 0, "context": "Feed-forward neural language model was first proposed by Bengio et al. (2003), which was a breakthrough in language modeling. Mikolov et al. (2011) proposed to use recurrent neural network in language modeling, which can include much longer context history for predicting next word.", "startOffset": 57, "endOffset": 148}, {"referenceID": 12, "context": "Also, they suggested that by both including current and previous phrase pairs to determine the phrase orientations could achieve further improvements in accuracy (Li et al., 2014).", "startOffset": 162, "endOffset": 179}, {"referenceID": 8, "context": "In traditional statistical machine translations, lexicalized reordering models (Koehn et al., 2007) have been widely used.", "startOffset": 79, "endOffset": 99}, {"referenceID": 0, "context": "However, RNN with conventional back-propagation training suffers from gradient vanishing issues (Bengio et al., 1994).", "startOffset": 96, "endOffset": 117}, {"referenceID": 6, "context": "We omit rather extensive LSTM equations here, which can be found in (Graves and Schmidhuber, 2005).", "startOffset": 68, "endOffset": 98}, {"referenceID": 8, "context": "The baseline systems are built with the open-source phrase-based SMT toolkit Moses (Koehn et al., 2007).", "startOffset": 83, "endOffset": 103}, {"referenceID": 18, "context": "Word alignment and phrase extraction are done by GIZA++ (Och and Ney, 2003) with L0-normalization (Vaswani et al., 2012), and grow-diag-final refinement rule (Koehn et al.", "startOffset": 98, "endOffset": 120}, {"referenceID": 10, "context": ", 2012), and grow-diag-final refinement rule (Koehn et al., 2003).", "startOffset": 45, "endOffset": 65}, {"referenceID": 16, "context": "Monolingual part of training data is used to train a 5-gram language model using SRILM (Stolcke, 2002).", "startOffset": 87, "endOffset": 102}, {"referenceID": 2, "context": "Parameter tuning is done by K-best MIRA (Cherry and Foster, 2012).", "startOffset": 40, "endOffset": 65}, {"referenceID": 3, "context": "For guarantee of result stability, we tune every system 5 times independently, and take the average BLEU score (Clark et al., 2011).", "startOffset": 111, "endOffset": 131}, {"referenceID": 14, "context": "The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002).", "startOffset": 71, "endOffset": 94}, {"referenceID": 9, "context": "The statistical significance test is also carried out with paired bootstrap resampling method (Koehn, 2004) with p < 0.", "startOffset": 94, "endOffset": 107}, {"referenceID": 11, "context": "Similar conclusion can be found in Li et al. (2014).", "startOffset": 35, "endOffset": 52}], "year": 2015, "abstractText": "Artificial neural networks are powerful models, which have been widely applied into many aspects of machine translation, such as language modeling and translation modeling. Though notable improvements have been made in these areas, the reordering problem still remains a challenge in statistical machine translations. In this paper, we present a novel neural reordering model that directly models word pairs and alignment. By utilizing LSTM recurrent neural networks, much longer context could be learned for reordering prediction. Experimental results on NIST OpenMT12 Arabic-English and Chinese-English 1000-best rescoring task show that our LSTM neural reordering feature is robust and achieves significant improvements over various baseline systems.", "creator": "Word"}}}