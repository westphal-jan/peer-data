{"id": "1512.02181", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2015", "title": "The Teaching Dimension of Linear Learners", "abstract": "Teaching dimension is a learning theoretic quantity that specifies the minimum training set size to teach a target model to a learner. Previous studies on teaching dimension focused on version-space learners which maintain all hypotheses consistent with the training data, and cannot be applied to modern machine learners which select a specific hypothesis via optimization. This paper presents the first known teaching dimension for ridge regression, support vector machines, and logistic regression. These concepts have been extensively used in modeling neural networks and models of network networks. This work has also been used to model neural networks of networks.\n\n\n\n\nThe first of our paper covers the basic concept of training dimension, which is the most commonly defined problem in modern training. It contains the following definitions:\nFor training dimension, a model is defined using an optimization in a given set. The model's goal is to train the models to see if they are correct. The model's goal is to learn the minimum training set size, which may be useful in learning the maximum training set size.\nFor training dimension, a model is defined using an optimization in a given set. The model's goal is to learn the minimum training set size, which may be useful in learning the maximum training set size. For training dimension, a model is defined using an optimization in a given set. The model's goal is to train the models to see if they are correct. The model's goal is to learn the minimum training set size, which may be useful in learning the maximum training set size. For training dimension, a model is defined using an optimization in a given set. The model's goal is to learn the minimum training set size, which may be useful in learning the maximum training set size. For training dimension, a model is defined using an optimization in a given set. The model's goal is to learn the minimum training set size, which may be useful in learning the maximum training set size. For training dimension, a model is defined using an optimization in a given set. The model's goal is to learn the minimum training set size, which may be useful in learning the minimum training set size. For training dimension, a model is defined using an optimization in a given set. The model's goal is to learn the minimum training set size, which may be useful in learning the maximum training set size. For training dimension, a model is defined using an optimization in a given set. The model's goal is to learn the minimum training set size, which may be useful in learning the maximum training set size. For training", "histories": [["v1", "Mon, 7 Dec 2015 19:24:55 GMT  (21kb)", "http://arxiv.org/abs/1512.02181v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ji liu", "xiaojin zhu 0001", "hrag ohannessian"], "accepted": true, "id": "1512.02181"}, "pdf": {"name": "1512.02181.pdf", "metadata": {"source": "CRF", "title": "The Teaching Dimension of Linear Learners", "authors": ["Ji Liu"], "emails": ["jliu@cs.rochester.edu,", "jerryzhu@cs.wisc.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 2.\n02 18\n1v 1\n[ cs\n.L G\n] 7\nD ec"}, {"heading": "1 Introduction", "text": "Consider a teacher who knows both a target model and the learning algorithm used by a machine learner. The teacher wants to teach the target model to the learner by constructing a training set. The training set does not need to contain independent and identically distributed items drawn from some distribution. Furthermore, the teacher can construct any item in the input space. How many training items are needed? This is the question addressed by the teaching dimension [13, 23]. We give the precise definition in section 2, but first illustrate the intuition with an example.\nConsider integers x \u2208 {1 . . . 10} and threshold classifiers h\u03b8 on them, so that h\u03b8(x) returns -1 if x < \u03b8 and 1 if x \u2265 \u03b8. Now let the hypothesis space H consist of eleven classifiers H = {h\u03b8 | \u03b8 \u2208 {1 . . . 11}}. Let the learner be a version-space learner, namely it maintains a version space {h\u03b8 \u2208 H | h\u03b8 consistent with the training set}. If we want to teach a target model (in this paper we use hypothesis and model exchangeably), say h9, to such a learner, we can construct a training set that results in a singleton version space {h9}. It is easy to see that the training set D = {(x1 = 8, y1 = \u22121), (x2 = 9, y2 = 1)} is the smallest set for this purpose. We say that the teaching dimension of h9 with respect to H is TD(h9) = |D| = 2. Similarly, TD(h11) = 1 because D = {(x1 = 10, y1 = \u22121)} suffices. In fact, TD(h\u2217\u03b8) = 1 for target model \u03b8\u2217 = 1 or 11, and 2 for \u03b8\u2217 = 2, 3, . . . , 10.\nThe astute reader may notice that this example does not apply to continuous spaces. To see this, let us extend x \u2208 R and H = {h\u03b8 | \u03b8 \u2208 R}. The learner\u2019s version space under any linearly separable training set would now be represented by the interval between the two closest oppositely labeled items. It is impossible for the version-space learner to pick out a unique target model h\u03b8\u2217 with a finite training set. In other words, TD(h\u03b8\u2217) = \u221e for all target models \u03b8\u2217. This is counterintuitive\nbecause ostensibly we can teach any one of the \u201cmodern\u201d machine learning algorithms such as a support vector machine (SVM) with only two training items: D = {(x1 = \u03b8\u2217 \u2212 \u01eb, y1 = \u22121), (x2 = \u03b8\u2217 + \u01eb, y2 = 1)} with any \u01eb > 0.\nThe issue here is that a version-space learner is not equipped with the ability to pick the maxmargin (or any other specific) hypothesis from the version space. In contrast, an SVM is not a version-space learner in our terminology; we have stronger knowledge from optimization on how it picks a specific hypothesis from the hypothesis space. This paper will utilize such knowledge to derive teaching dimensions that are distinct from classic teaching dimension analysis (e.g. [12]). Specifically, we extend teaching dimension to linear learners that learn by regularized empirical risk minimization:\nAopt(D) := Argmin\u03b8\u2208Rd n\u2211\ni=1\n\u2113(x\u22a4i \u03b8, yi) + \u03bb\n2 \u2016\u03b8\u20162A\n\ufe38 \ufe37\ufe37 \ufe38\n=:f(\u03b8)\n. (1)\nHere, we identify H with Rd, h with \u03b8, the loss function \u2113 is either smooth or convex in the first argument, \u03bb > 0 is the regularization coefficient, and A is a positive semidefinite matrix. \u2016 \u00b7 \u2016A is the Mahalanobis norm: \u2016\u03b8\u2016A := \u221a \u03b8\u22a4A\u03b8. We follow the convention in optimization when we use the capitalized Argmin to emphasize that it returns a set that achieves the minimum. The teacher can construct a training set with any items in Rd. The alternative pool-based teaching setting, where the teacher is given a finite pool of candidate training items and must select items from that pool, is not studied in this paper. By linear learners we mean the input x and the parameter \u03b8 interact only via their inner product x\u22a4\u03b8. Linear learners include SVMs, logistic regression, and linear regression. Our analysis technique involves a novel application of the Karush-Kuhn-Tucker (KKT) conditions.\nTo our knowledge, this paper gives the first known values of teaching dimension for ridge regression, SVM, and logistic regression. We summarize our main results in Table 1. The table separately lists homogeneous (without a bias term) and inhomogeneous (with a bias term) versions of the linear learners. The teaching goal refers to the intention of the teacher: is teaching considered successful only when the learner learns the exact target parameter, or when the learner learns the correct decision boundary (which can be achieved by any positive scaling of the target parameter). See section 3 for definition of the target parameters \u03b8\u2217,w\u2217 and the constant \u03c4max. The target parameters are assumed to be nonzero. We will also present the corresponding minimum teaching set construction in section 3."}, {"heading": "2 Classic Teaching Dimension and its Limitations", "text": "Let X denote the input space and Y \u2286 R the output space. A hypothesis is a function h : X \u2192 Y. In this paper we identify a hypothesis h\u03b8 with its model parameter \u03b8. The hypothesis space H is a set of hypotheses. By training item we mean a pair (x, y) \u2208 X \u00d7 Y. A training set is a multiset D = {(x1, y1) . . . (xn, yn)} where repeated items are allowed. Importantly, for the purpose of teaching we do not assume that D be drawn i.i.d. from a distribution. Let D denote the set of all training sets of all sizes. A learning algorithm A : D \u2192 2H takes in a training set D \u2208 D and outputs a subset of the hypothesis space H. That is, A does not necessarily return a unique hypothesis.\nClassic teaching dimension analysis is restricted to the version-space learner Avs:\nAvs(D) = {h \u2208 H | h is consistent with D }. (2)\nThat is, the learner Avs keeps track of the version space. Let the target model be h\u03b8\u2217 \u2208 H. Teaching is successful if the teacher identifies a training set D \u2208 D such that Avs(D) = {h\u03b8\u2217} the singleton set. Such a D is called a teaching set of h\u03b8\u2217 with respect to H. The teaching dimension of the hypothesis h\u03b8\u2217 is the minimum size of the teaching set:\nTD(h\u03b8\u2217) =\n{ minD\u2208D |D|, for D a teaching set of h\u03b8\u2217\n\u221e, if no teaching set exists (3)\nFurthermore, the teaching dimension of the whole hypothesis space H is defined by the hardest hypothesis: TD(H) = maxh\u2208H TD(h). In this paper we will focus on the fine-grained teaching dimension of individual hypothesis TD(h).\nClassic teaching dimension analysis has several limitations: the learner is assumed to be a version-space learner Avs, and the hypothesis space is typically finite or countably infinite. As the example in section 1 showed, these fail to capture the teaching dimension of \u201cmodern\u201d machine learners which has Rd as input space and picks a unique hypothesis via regularized empirical risk minimization (1). Furthermore, the target model can be ambiguous when the learner is a classifier: should the learner learn the exact target parameter \u03b8\u2217, or the target decision boundary? In linear models any scaled parameter c\u03b8\u2217 with c > 0 produces the same target decision boundary. These limitations motivate us to generalize the teaching dimension in the next section."}, {"heading": "3 Main Results", "text": "To make teaching dimension\u2019s dependency on the learning algorithm explicit, henceforth we write teaching dimension with two arguments as\nTD(h\u2217,A) (4)\nwhere h\u2217 \u2208 H is the target model, and A : D \u2192 2H is the learning algorithm which given a training set D \u2208 D returns a set of hypotheses A(D). We define teaching dimension to be the size of the smallest training set D such that A(D) = {h\u2217}, the singleton set containing the target model. With this notation, the classic teaching dimension is TD(h\u2217,Avs) where Avs is the version space learning algorithm (2). In this paper we focus on Aopt in (1) instead, namely linear learners in Rd. Linear learners include many popular members such as both homogeneous (without a\nbias term) and inhomogeneous (with a bias term) versions of linear regression, SVM, and logistic regression. In addition, the linear interaction between x and \u03b8 makes the loss function subgradient easy to compute, though in principle our analysis technique is applicable to other optimizationbased learners, too. In this section our goal is to teach the exact parameter \u03b8\u2217, consequently our teaching dimension of interest is TD(\u03b8\u2217,Aopt). (5) Later in section 4 for classification we will teach the decision boundary instead.\nHow to reason about our teaching dimension TD(\u03b8\u2217,Aopt)? It is the size of the smallest training set D with which (1) has a unique solution \u03b8\u2217. Our strategy is to first establish a number of lower bounds LB \u2264 TD(\u03b8\u2217,Aopt) by showing that any training set with which (1) has a unique solution \u03b8\u2217 must have at least LB items. Section 3.1 is devoted to such lower bounds. The actual teaching dimension is learner dependent. In sections 3.2 and 3.3 we construct specific teaching sets for three popular learners: ridge regression, SVM, and logistic regression. These teaching sets uniquely returns \u03b8\u2217 via (1). By definition, the size of these teaching sets is an upper bound on TD(\u03b8\u2217,Aopt), respectively. If the lower and upper bounds match, we would have identified the teaching dimension TD(\u03b8\u2217,Aopt)."}, {"heading": "3.1 Lower Bounds on Teaching Dimension TD(\u03b8\u2217,Aopt)", "text": "In this section we provide three general lower bounds on the teaching dimension. These lower bounds capture different aspects of a teaching set, and should be used in conjunction (i.e. taking the maximum) when applicable. We will instantiate these lower bounds for specific learners in section 3.2. In the following let X and Y be the feasible region of all xi\u2019s and yi\u2019s respectively. We will use the notation \u22021\u2113(\u00b7, \u00b7) in the following way: if \u2113(\u00b7, \u00b7) is smooth, then it denotes a singleton set only containing the gradient w.r.t. the first argument; if \u2113(\u00b7, \u00b7) is convex, then it denotes the subdifferential w.r.t the first argument.\nLB1 comes from a degree-of-freedom perspective. It is necessary to have this amount of training items for a unique solution to exist in (1).\nTheorem 1. Given any target model \u03b8\u2217, there is a degree-of-freedom lower bound on the number of training items to obtain a unique solution \u03b8\u2217 from solving (1):\nLB1 =\n{\nd\u2212 Rank(A) + 1, if A\u03b8\u2217 6= 0 d\u2212 Rank(A), otherwise.\n(6)\nProof. Let n\u2217 be the minimal number of training items to ensure a unique solution \u03b8\u2217. First consider the case n\u2217 = 0. It happens if and only if \u03b8\u2217 = 0 and Rank(A) = d, which is a special case of A\u03b8\u2217 = 0. Clearly, this case is consistent with LB1. Next consider the case n\u2217 \u2265 1. Since \u03b8\u2217 solves (1), the KKT condition holds:\n\u2212\u03bbA\u03b8\u2217 \u2208 n\u2217\u2211\ni=1\n\u22021\u2113(x \u22a4 i \u03b8 \u2217, yi)xi. (7)\nWe seek all \u03b4 such that \u03b8\u2217 + \u03b4 satisfies\nA(\u03b8\u2217 + \u03b4) = A\u03b8\u2217 and x\u22a4i (\u03b8 \u2217 + \u03b4) = x\u22a4i \u03b8 \u2217 \u2200i = 1, \u00b7 \u00b7 \u00b7 , n\u2217, (8)\nFor any such \u03b4 , simple algebra verifies that \u03b8\u2217+ t\u03b4 satisfies the KKT condition (7) for any t \u2208 [0, 1]. Consequently, \u03b8\u2217 + \u03b4 also solves the problem in (1). To see this, we consider two situations:\n\u2022 If the loss function \u2113(\u00b7, \u00b7) is convex in the first argument, the KKT condition is a sufficient optimality condition, which means that \u03b8\u2217 + \u03b4 solves (1).\n\u2022 If the loss function \u2113(\u00b7, \u00b7) is smooth (not necessary convex) in the first argument, we have f(\u03b8\u2217) = f(\u03b8\u2217 + \u03b4) by using the Taylor expansion (recall f is defined in equation 1):\nf(\u03b8\u2217 + \u03b4) =f(\u03b8\u2217) + \u3008\u2207f(\u03b8\u2217 + t\u03b4), \u03b4\u3009 (for some t \u2208 [0, 1])\n=f(\u03b8\u2217) +\n\u2329 n\u2217\u2211\ni=1\n\u22071\u2113(x\u22a4i (\u03b8\u2217 + t\u03b4), yi)xi + \u03bbA(\u03b8\u2217 + t\u03b4)), \u03b4 \u232a\n=f(\u03b8\u2217) +\n\u2329 n\u2217\u2211\ni=1\n\u22071\u2113(x\u22a4i \u03b8\u2217, yi)xi + \u03bbA\u03b8\u2217\n\ufe38 \ufe37\ufe37 \ufe38\n=0 due to the KKT condition (7)\n, \u03b4\n\u232a\n=f(\u03b8\u2217).\nTherefore, \u03b8\u2217 + \u03b4 also solves (1). However, the uniqueness of \u03b8\u2217 requires \u03b4 = 0 to be the only value satisfying (8). This is equivalent to say\nNull(A) \u2229Null(Span{x1, \u00b7 \u00b7 \u00b7 ,xn\u2217}) = {0}. (9) It indicates that Rank(A) + Dim(Span{x1, \u00b7 \u00b7 \u00b7 ,xn\u2217}) \u2265 d. From n\u2217 \u2265 Dim(span{x1, \u00b7 \u00b7 \u00b7 ,xn\u2217}), we have n\u2217 \u2265 d \u2212 Rank(A). We proved the general case for LB1.\nIf we have A\u03b8\u2217 6= 0, we can further improve LB1. Let g\u2217 = (g\u22171 , . . . , g\u2217n\u2217)\u22a4 be the vector satisfying\n\u2212\u03bbA\u03b8\u2217 = n\u2217\u2211\ni=1\ng\u2217i xi and g \u2217 i \u2208 \u22021\u2113(x\u22a4i \u03b8\u2217, yi) \u2200i = 1, 2, \u00b7 \u00b7 \u00b7 , n\u2217. (10)\nSince \u03b8\u2217 satisfies the KKT condition, such vector g\u2217 must exist. Applying A\u03b8\u2217 6= 0 to (10), we have g\u2217 6= 0 and Dim (Span{A.1, A.2, \u00b7 \u00b7 \u00b7 , A.d} \u2229 Span{x1, \u00b7 \u00b7 \u00b7 ,xn\u2217}) \u2265 1. (11) To satisfy (9), we must have\nd = Dim (Span{A.1, A.2, \u00b7 \u00b7 \u00b7 , A.d,x1, \u00b7 \u00b7 \u00b7 ,xn\u2217}) . Using the fact in linear algebra\nDim (Span{A.1, A.2, \u00b7 \u00b7 \u00b7 , A.d,x1, \u00b7 \u00b7 \u00b7 ,xn\u2217}) =Dim (Span{A.1, A.2, \u00b7 \u00b7 \u00b7 , A.d}) \ufe38 \ufe37\ufe37 \ufe38\n=Rank(A)\n+\nDim (Span{x1, \u00b7 \u00b7 \u00b7 ,xn\u2217}) \ufe38 \ufe37\ufe37 \ufe38 \u2264n\u2217 \u2212 Dim (Span{A.1, A.2, \u00b7 \u00b7 \u00b7 , A.d} \u2229 Span{x1, \u00b7 \u00b7 \u00b7 ,xn\u2217}) \ufe38 \ufe37\ufe37 \ufe38\n\u22651 (from (11))\nWe conclude that n\u2217 \u2265 d\u2212 Rank(A) + 1. We completed the proof for LB1.\nLB2 observes that the regularizer acts as a prior. If \u03bb is large, more items are needed to sway the prior toward the target \u03b8\u2217.\nTheorem 2. Given any target model \u03b8\u2217, there is a strength-of-regularization lower bound on the required number of training items to obtain a unique solution \u03b8\u2217 from solving (1):\nLB2 =\n \n\n\u2308\n\u03bb ( sup\u03b1\u2208R,y\u2208Y ,g\u2208\u2212\u22021\u2113(\u03b1\u2016\u03b8\u2217\u20162A,y) \u03b1g )\u22121\n\u2309\n, if A has full rank and \u03b8\u2217 6= 0\n0, otherwise.\n(12)\nProof. When A has full rank we have an equivalent expression for the KKT condition (7):\n\u2212\u03bbA 12\u03b8\u2217 \u2208 n\u2217\u2211\ni=1\nA\u2212 1 2xi\u22021\u2113(x \u22a4 i \u03b8 \u2217, yi) \u2200i = 1, \u00b7 \u00b7 \u00b7 , n\u2217. (13)\nLet us decompose A\u2212 1 2xi for all i = 1, \u00b7 \u00b7 \u00b7 , n\u2217 into A\u2212 1 2xi = \u03b1iA 1 2\u03b8\u2217 + ui, where ui is orthogonal to A 1 2\u03b8\u2217: u\u22a4i A 1 2\u03b8\u2217 = 0. Equivalently xi = \u03b1iA\u03b8\u2217 +A 1 2ui. Applying this decomposition, we have\nx\u22a4i \u03b8 \u2217 = \u03b1i\u2016\u03b8\u2217\u20162A + u\u22a4i A 1 2\u03b8\u2217 = \u03b1i\u2016\u03b8\u2217\u20162A.\nPutting it back in (13) we obtain\n\u2212\u03bbA 12\u03b8\u2217 \u2208 n\u2217\u2211\ni=1\n(\n\u03b1iA 1 2\u03b8\u2217 + ui\n)\n\u22021\u2113(\u03b1i\u2016\u03b8\u2217\u20162A, yi) \u2200i = 1, \u00b7 \u00b7 \u00b7 , n\u2217. (14)\nSince ui is orthogonal to A 1 2\u03b8\u2217, (14) can be rewritten as\n\u2203\u03b1i \u2208 R, \u2203yi \u2208 Y, \u2203gi \u2208 \u22021\u2113(\u03b1i\u2016\u03b8\u2217\u20162A, yi) \u2200i = 1, \u00b7 \u00b7 \u00b7 , n\u2217 (15)\nsatisfying\nn\u2217\u2211\ni=1\ngiui = 0\n\u2212 \u03bbA 12\u03b8\u2217 = A 12\u03b8\u2217 n\u2217\u2211\ni=1\n\u03b1igi (16)\nSince A\u03b8\u2217 6= 0, we have A 12\u03b8\u2217 6= 0 and (16) is equivalent to \u2212\u03bb = \u2211n\u2217i=1 \u03b1igi. It follows that\n\u03bb = \u2212 n\u2217\u2211\ni=1 \u03b1igi \u2264 n\u2217 sup \u03b1\u2208R,y\u2208Y ,g\u2208\u22021\u2113(\u03b1\u2016\u03b8\u2217\u20162A,y) \u2212\u03b1g = n\u2217 sup \u03b1\u2208R,y\u2208Y ,g\u2208\u2212\u22021\u2113(\u03b1\u2016\u03b8\u2217\u20162A,y) \u03b1g\nIt indicates the lower bound for n\u2217\nn\u2217 \u2265 \u2308\n\u03bb\nsup\u03b1\u2208R,y\u2208Y ,g\u2208\u2212\u22021\u2113(\u03b1\u2016\u03b8\u2217\u20162A,y) \u03b1g\n\u2309\n.\nLB1 and LB2 apply to all generalized linear learners. Due to the popularity of inhomogeneous margin-based linear learners (which include the standard form of SVM and logistic regression), we provide a tighter lower bound LB3 for such learners in Theorem 3. For inhomogeneous margin-based linear learners the learning algorithm Aopt solves a special form of (1):\nAopt(D) = Argminw,b n\u2211\ni=1\n\u2113(yi(x \u22a4 i w + b)) +\n\u03bb 2 \u2016w\u20162A. (17)\nLB3 will prove to be instrumental in computing the teaching dimension for those learners. Following standard notation, we define \u03b8 = [w; b] wherew \u2208 Rd is the weight vector and b \u2208 R the bias (offset) term. Note \u03b8 \u2208 Rd+1 now. The d \u00d7 d regularization matrix A applies only to w while b is not regularized. Furthermore, margin-based linear learners have loss functions defined on the margin y(x\u22a4w + b). This loss function structure will play a key role in obtaining LB3.\nTheorem 3. Assume matrix A in (17) has full rank and w\u2217 6= 0. Given any target model [w\u2217; b\u2217], there is an inhomogeneous-margin lower bound on the required number of training items to obtain a unique solution [w\u2217; b\u2217] from solving (17):\nLB3 =\n\n   \u03bb\n(\nsup \u03b1\u2208R,g\u2208\u2212\u2202\u2113(\u03b1\u2016w\u2217\u20162\nA )\n\u03b1g\n)\u22121\n   . (18)\nProof. Let D = {xi, yi}i=1,\u00b7\u00b7\u00b7 ,n be a teaching set for [w\u2217; b\u2217]. The following KKT condition needs to be satisfied:\n0 \u2208 n\u2211\ni=1\n\u2202\u2113(yi(x \u22a4 i w \u2217 + b\u2217))yi [ xi 1 ] + [ \u03bbAw\u2217 0 ] . (19)\nIf we construct a new training set\nD\u0302 =\n{\nx\u0302i = xi + b\u2217\n\u2016w\u2217\u20162A Aw\u2217, y\u0302i = yi\n}\ni=1,\u00b7\u00b7\u00b7 ,n\nthen [w\u2217; 0] satisfies the KKT condition defined on D\u0302. This can be verified as follows:\nn\u2211\ni=1\n\u2202\u2113(y\u0302i(x\u0302 \u22a4 i w \u2217))y\u0302i [ x\u0302i 1 ] + [ \u03bbAw\u2217 0 ]\n=\nn\u2211\ni=1\n\u2202\u2113(yi(x \u22a4 i w \u2217 + b\u2217))yi\n[\nxi + b\u2217\n\u2016w\u2217\u20162 A\nAw\u2217\n1\n]\n+\n[ \u03bbAw\u2217\n0\n]\n=\nn\u2211\ni=1\n\u2202\u2113(yi(x \u22a4 i w \u2217 + b\u2217))yi [ xi 1 ] + [ \u03bbAw\u2217 0 ] \ufe38 \ufe37\ufe37 \ufe38\n\u220b0 from (19)\n+\n[ b\u2217\n\u2016w\u2217\u20162 A\nAw\u2217\n0\n] n\u2211\ni=1\n\u2202\u2113(yi(x \u22a4 i w \u2217 + b\u2217))yi\n\ufe38 \ufe37\ufe37 \ufe38\n\u220b0 from (19)\n\u220b0 (20)\nwhere 0 \u2208 \u2211ni=1 \u2202\u2113(yi(x\u22a4i w\u2217 + b\u2217))yi is from the bias dimension in (19). It follows that\n0 \u2208 n\u2211\ni=1\n\u2202\u2113(y\u0302ix\u0302 \u22a4 i w \u2217)y\u0302ix\u0302i + \u03bbAw \u2217\nwhich is equivalent to\n0 \u2208 n\u2211\ni=1\n\u2202\u2113(y\u0302ix\u0302 \u22a4 i w \u2217)A\u2212 1 2 y\u0302ix\u0302i \ufe38\ufe37\ufe37\ufe38\n=:zi\n+\u03bbA 1 2w\u2217\n=\nn\u2211\ni=1\n\u2202\u2113(z\u22a4i w \u2217)A\u2212 1 2zi + \u03bbA 1 2w\u2217. (21)\nWe decompose A\u2212 1 2zi = \u03b1iA 1 2w\u2217+ui where ui satisfies u\u22a4i A 1 2w\u2217 = 0. Applying this decomposition to (21), we have\n\u03bbA 1 2w\u2217 \u2208\nn\u2211\ni=1\n\u2212\u2202\u2113(\u03b1i\u2016w\u2217\u20162A)(\u03b1iA 1 2w\u2217 + ui). (22)\nSince ui is orthogonal to A 1 2w\u2217, (22) implies that\n\u03bbA 1 2w\u2217 \u2208\nn\u2211\ni=1\n\u2212\u2202\u2113(\u03b1i\u2016w\u2217\u20162A)\u03b1iA 1 2w\u2217\nSince w\u2217 6= 0 we have\n\u03bb \u2208 n\u2211\ni=1\n\u2212\u2202\u2113(\u03b1i\u2016w\u2217\u20162A)\u03b1i\nTogether with n\u2211\ni=1\n\u2212\u2202\u2113(\u03b1i\u2016w\u2217\u20162A)\u03b1i \u2264 n sup \u03b1\u2208R,g\u2208\u2212\u2202\u2113(\u03b1\u2016w\u2217\u20162\nA )\n\u03b1g,\nwe obtain LB3."}, {"heading": "3.2 The Teaching Dimension TD(\u03b8\u2217,Aopt) of Three Homogeneous Learners", "text": "We now turn to upper bounding teaching dimension by constructing teaching sets. To prove that we indeed have a teaching set for a target \u03b8\u2217, we need to show that \u03b8\u2217 is a solution of (1), and the solution is unique. The size of any such teaching set is an upper bound on the teaching dimension. The teaching dimension itself is determined if such an upper bound matches the corresponding lower bound. We show that this is indeed the case for our constructed teaching sets. For the sake of reference we preview in Table 2 the instantiated lower bounds that we will use in this section; their derivation will be shown below.\nTeaching dimension is learner-dependent. We choose three learners to study their teaching dimension due to these learners\u2019 popularity in machine learning: ridge regression, SVM, and logistic regression. It turns out that homogeneous and inhomogeneous versions of these learners require different analysis. We devote this section to the homogeneous version where the regularizer matrix\nA = I the identity matrix, and the next section to the inhomogeneous version. It is possible to extend our analysis to other linear learners of the form (1).\nIt is easy to see that if the target model \u03b8\u2217 = 0, we do not need any training data to uniquely obtain the target model from (1). In the following, we only consider the nontrivial case \u03b8\u2217 6= 0.\nHomogeneous ridge regression solves the following optimization problem:\nmin \u03b8\u2208Rd\nn\u2211\ni=1\n1 2 (x\u22a4i \u03b8 \u2212 yi)2 + \u03bb 2 \u2016\u03b8\u20162 (23)\nWe only need one training item to uniquely obtain any nonzero target model \u03b8\u2217, as the following construction shows.\nProposition 1. Given any target model \u03b8\u2217 6= 0, the following is a teaching set for homogeneous ridge regression (23):\nx1 = a\u03b8 \u2217, y1 = \u03bb+ \u2016x1\u20162 a\n(24)\nwhere a can be any nonzero real number.\nProof. We simply verify the KKT condition to see that \u03b8\u2217 is a solution to (23) by applying the construction in (24). The uniqueness of \u03b8\u2217 is guaranteed by the strong convexity of (23).\nWe encourage the reader to distinguish two senses of uniqueness. The teaching set itself is not necessarily unique. In the construction (24), any a 6= 0 leads to a valid teaching set. Nonetheless, any one of the teaching sets will lead to the unique solution \u03b8\u2217 in (23).\nCorollary 1. The teaching dimension TD(\u03b8\u2217,Ahomridge) = 1 for homogeneous ridge regression and target \u03b8\u2217 6= 0.\nProof. Substituting A by I in LB1 (6), we obtain the lower bound d \u2212 Rank(I) + 1 = 1 which matches the teaching set size in (24).\nHomogeneous SVM solves the problem:\nmin \u03b8\u2208Rd\nn\u2211\ni=1\nmax(1\u2212 yix\u22a4i \u03b8, 0) + \u03bb\n2 \u2016\u03b8\u20162. (25)\nTo teach this learner one training item is in general not enough: we will show that we need \u2308 \u03bb\u2016\u03b8\u2217\u20162 \u2309 training items. In fact, we will construct such a teaching set consisting of identical training items. It is well-known in the teaching literature that a teaching set does not need to consist of i.i.d. samples from a distribution, and can look unusual. It is possible to incorporate additional constraints into a teaching problem if one wants the training items to be diverse, but we do not consider that in the present paper.\nProposition 2. Given any target model \u03b8\u2217 6= 0, the following is a teaching set for homogeneous SVM (25). There are n = \u2308 \u03bb\u2016\u03b8\u2217\u20162 \u2309 identical training items, each taking the form\nxi = \u03bb\u03b8\u2217\n\u2308\u03bb\u2016\u03b8\u2217\u20162\u2309 , yi = 1. (26)\nProof. We only need to verify that the KKT condition holds for \u03b8\u2217. Due to the strong convexity of (25) uniqueness is guaranteed automatically. We denote the subgradient \u2202amax(1 \u2212 a, 0) = \u2212\u22021 max(1\u2212 a, 0) = \u2212I(a), where\nI(a) =\n \n\n1, if a < 1\n[0, 1], if a = 1\n0, otherwise\n. (27)\nThe KKT condition is\nn\u2211\ni=1\n\u2212yixi\u22021 max(1\u2212 yix\u22a4i \u03b8\u2217, 0) + \u03bb\u03b8\u2217\n=\nn\u2211\ni=1\n\u2212yixiI(yix\u22a4i \u03b8\u2217) + \u03bb\u03b8\u2217\n=\u2212 n \u03bb\u03b8 \u2217 \u2308\u03bb\u2016\u03b8\u2217\u20162\u2309I ( \u03bb\u2016\u03b8\u2217\u20162 \u2308\u03bb\u2016\u03b8\u2217\u20162\u2309 ) + \u03bb\u03b8\u2217 =\u2212 \u03bb\u03b8\u2217I (\n\u03bb\u2016\u03b8\u2217\u20162 \u2308\u03bb\u2016\u03b8\u2217\u20162\u2309\n)\n+ \u03bb\u03b8\u2217\n\u220b0\nwhere the last line is due to I (\n\u03bb\u2016\u03b8\u2217\u20162 \u2308\u03bb\u2016\u03b8\u2217\u20162\u2309\n)\ngiving either the set [0, 1] or the value 1.\nCorollary 2. The teaching dimension TD(\u03b8\u2217,Ahomsvm) = \u2308 \u03bb\u2016\u03b8\u2217\u20162 \u2309 for homogeneous SVM and target \u03b8\u2217 6= 0. Proof. We show this number matches LB2. Let A = I, \u2113(a, b) = max(1 \u2212 ab, 0), and consider the denominator of (12):\nsup \u03b1\u2208R,y\u2208Y ,g\u2208\u2212\u22021\u2113(\u03b1\u2016\u03b8\u2217\u20162,y) \u03b1g = sup \u03b1,y\u2208{\u22121,1},g\u2208yI(y\u03b1\u2016\u03b8\u2217\u20162) \u03b1g\n= sup \u03b1,g\u2208I(\u03b1\u2016\u03b8\u2217\u20162) \u03b1g = 1\n\u2016\u03b8\u2217\u20162\nwhere the first equality is due to \u22021\u2113(a, b) = \u2212bI(ab). Therefore, LB2 = \u2308 \u03bb\u2016\u03b8\u2217\u20162 \u2309 which matches the construction in (26).\nHomogeneous logistic regression solves the problem:\nmin \u03b8\u2208Rd\nn\u2211\ni=1\nlog(1 + exp{\u2212yix\u22a4i \u03b8}) + \u03bb\n2 \u2016\u03b8\u20162 (28)\nThe situation is similar to homogeneous SVM. However, due to the negative log likelihood term we have a coefficient defined by the Lambert W function [11], which we denote by Wlam. Recall the defining equation for Lambert W function is Wlam(x)e Wlam(x) = x. We further define\n\u03c4max := max t\nt\n1 + et = Wlam(1/e) \u2248 0.2785. (29)\nFor any value a \u2264 \u03c4max, we define \u03c4\u22121(a) as the solution to a = t1+et . By using the Lambert W function \u03c4\u22121(a) can be expressed as \u03c4\u22121(a) \u2261 a\u2212Wlam(\u2212aea).\nProposition 3. Given any target model \u03b8\u2217 6= 0, the following is a teaching set for homogeneous logistic regression (28). There are n =\n\u2308 \u03bb\u2016\u03b8\u2217\u20162 \u03c4max \u2309 identical training items, each takes the form\nxi = \u03c4 \u22121\n(\n\u03bb\u2016\u03b8\u2217\u20162 \u2308 \u03bb\u2016\u03b8\u2217\u20162 \u03c4max\n\u2309\u22121) \u03b8\u2217\n\u2016\u03b8\u2217\u20162 , yi = 1. (30)\nProof. We first verify that \u03b8\u2217 is a solution to (28) based on the teaching set construction in (30). We only need to verify the gradient of (28) is zero. Computing the gradient of (28), we have\nn\u2211\ni=1\n\u2212yixi 1 + exp{yix\u22a4i \u03b8\u2217} + \u03bb\u03b8\u2217\n=\u2212 n xi 1 + exp { \u03c4\u22121 (\n\u03bb\u2016\u03b8\u2217\u20162 \u2308 \u03bb\u2016\u03b8\u2217\u20162 \u03c4max\n\u2309\u22121 )} + \u03bb\u03b8\u2217\n=\u2212 n \u03c4\u22121\n(\n\u03bb\u2016\u03b8\u2217\u20162 \u2308 \u03bb\u2016\u03b8\u2217\u20162 \u03c4max\n\u2309\u22121 )\n1 + exp\n{ \u03c4\u22121 ( \u03bb\u2016\u03b8\u2217\u20162 \u2308 \u03bb\u2016\u03b8\u2217\u20162 \u03c4max\n\u2309\u22121)} \u03b8\u2217 \u2016\u03b8\u2217\u20162 + \u03bb\u03b8 \u2217\n=\u2212 n\u03bb\u2016\u03b8\u2217\u20162 \u2308 \u03bb\u2016\u03b8\u2217\u20162 \u03c4max \u2309\u22121 \u03b8\u2217 \u2016\u03b8\u2217\u20162 + \u03bb\u03b8 \u2217\n=0,\nwhere the third equality uses the fact \u03bb\u2016\u03b8\u2217\u20162 \u2308 \u03bb\u2016\u03b8\u2217\u20162 \u03c4max \u2309\u22121 \u2264 \u03c4max and the property a = \u03c4 \u22121(a) 1+e\u03c4\u22121(a) . The strong convexity of (28) automatically implies uniqueness.\nCorollary 3. The teaching dimension TD(\u03b8\u2217,Ahomlog ) = \u2308 \u03bb\u2016\u03b8\u2217\u20162 \u03c4max \u2309 for homogeneous logistic regression and target \u03b8\u2217 6= 0.\nProof. We show that the number matches LB2. In (12) let A = I and \u2113(a, b) = log(1+ exp{\u2212ab}). The denominator of LB2 is:\nsup \u03b1\u2208R,y\u2208Y ,g\u2208\u2212\u22021\u2113(\u03b1\u2016\u03b8\u2217\u20162,y) \u03b1g = sup \u03b1,y\u2208{\u22121,1},g=y(1+exp{y\u03b1\u2016\u03b8\u2217\u20162})\u22121 \u03b1g\n= sup \u03b1,g=(1+exp{\u03b1\u2016\u03b8\u2217\u20162})\u22121 \u03b1g\n=sup \u03b1\n\u03b1\n1 + exp{\u03b1\u2016\u03b8\u2217\u20162} =\u2016\u03b8\u2217\u2016\u22122 sup\nt\nt\n1 + exp{t} =\n\u03c4max \u2016\u03b8\u2217\u20162 ,\nwhich implies LB2 = \u2308 \u03bb\u2016\u03b8\u2217\u20162 \u03c4max \u2309 ."}, {"heading": "3.3 The Teaching Dimension TD(\u03b8\u2217,Aopt) of Three Inhomogeneous Learners", "text": "Inhomogeneous learners are defined by \u03b8 = [w; b] where the weight vector w \u2208 Rd and the scalar offset b \u2208 R. The offset b is not regularized. Similar to the previous section, we need to instantiate the teaching dimension lower bounds and design the teaching sets. We show that the size of our teaching set exactly matches the lower bound for inhomogeneous ridge regression, and differs from the lower bound of inhomogeneous SVM and logistic regression by at most one due to rounding. Therefore, up to rounding we also establish the teaching dimension for these inhomogeneous learners.\nInhomogeneous ridge regression solves the problem:\nmin w\u2208Rd,b\u2208R\nn\u2211\ni=1\n1 2 (x\u22a4i w + b\u2212 yi)2 + \u03bb 2 \u2016w\u20162 (31)\nProposition 4. Given any target model [w\u2217; b\u2217], if w\u2217 = 0 (b\u2217 can be an arbitrary value), the following is a teaching set for inhomogeneous ridge regression (31) with n = 1:\nx1 = 0, y1 = b \u2217. (32)"}, {"heading": "If w\u2217 6= 0, any n = 2 items satisfying the following are a teaching set for a 6= 0:", "text": "x1 \u2212 x2 = aw\u2217, y1 = x\u22a41 w\u2217 + b\u2217 + \u03bb a , y2 = y1 \u2212 a\u2016w\u2217\u20162 \u2212 2 \u03bb a . (33)\nProof. We first prove the case for w\u2217 = 0. We can verify that the KKT condition is satisfied by designing x1 and y1 as in (32):\n(x\u22a41 w \u2217 + b\u2217 \u2212 y1)x1 + \u03bbw\u2217 =0\nx\u22a41 w \u2217 + b\u2217 \u2212 y1 =0.\nThe uniqueness of [w\u2217; b\u2217] is indicated by the strong convexity of (31) when n = 1.\nWe then prove the case for w\u2217 6= 0. With simple algebra, we can verify the KKT condition holds via the construction in (33):\n(x\u22a41 w \u2217 + b\u2217 \u2212 y1)x1 + (x\u22a42 w\u2217 + b\u2217 \u2212 y2)x2 + \u03bbw\u2217 =0\n(x\u22a41 w \u2217 + b\u2217 \u2212 y1) + (x\u22a42 w\u2217 + b\u2217 \u2212 y2) =0.\nSimilarly, the uniqueness is implied by the strong convexity of (31) when n = 2.\nCorollary 4. The teaching dimension for inhomogeneous ridge regression with target \u03b8\u2217 = [w\u2217; b\u2217] is TD(\u03b8\u2217,Ainhridge) = 1 if target w\u2217 = 0, or TD(\u03b8\u2217,Ainhridge) = 2 if w\u2217 6= 0, regardless of the target offset b\u2217.\nProof. We match the lower bound LB1 in (6). Note \u03b8\u2217 = [w\u2217; b\u2217] \u2208 Rd+1, and A in this case is a (d + 1) \u00d7 (d + 1) matrix with the d \u00d7 d identity matrix Id padded with one additional row and column of zeros for the offset. Therefore Rank(A) = Rank(Id) = d. When w\n\u2217 = 0, A\u03b8\u2217 = 0 and LB1 = (d + 1) \u2212 Rank(A) = 1. When w\u2217 6= 0, A\u03b8\u2217 6= 0 and LB1 = (d + 1) \u2212 Rank(A) + 1 = 2. These lower bounds match the teaching set sizes in (32) and (33), respectively.\nInhomogeneous SVM solves the problem:\nmin w\u2208Rd,b\u2208R\nn\u2211\ni=1\nmax(1\u2212 yi(x\u22a4i w + b), 0) + \u03bb\n2 \u2016w\u20162 (34)\nProposition 5. Given any target model [w\u2217; b\u2217] with w\u2217 6= 0, the following is a teaching set for inhomogeneous SVM (34). We need n = 2 \u2308 \u03bb\u2016w\u2217\u20162\n2\n\u2309\ntraining items, half of which are identical pos-\nitive items xi = x+, yi = 1, \u2200i \u2208 { 1, \u00b7 \u00b7 \u00b7 , n2 } and half identical negative items xi = x\u2212, yi = \u22121, \u2200i \u2208 { n 2 + 1, \u00b7 \u00b7 \u00b7 , n } . x+ and x\u2212 can be designed as any vectors satisfying\nx\u22a4+w \u2217 = 1\u2212 b\u2217, x\u2212 = x+ \u2212\n2w\u2217\n\u2016w\u2217\u20162 . (35)\nProof. Unlike in previous learners (including homogeneous SVM), we no longer have strong convexity w.r.t. b. In order to prove that (35) is a teaching set, we need to verify the KKT condition and verify solution uniqueness.\nWe first verify the KKT condition to show that the solution under (35) includes the target model [w\u2217; b\u2217]. From (35), we have\nx\u22a4+w \u2217 + b\u2217 = 1, x\u22a4\u2212w \u2217 + b\u2217 = \u22121. (36)\nApplying them to the KKT condition and using the notation in (27) we obtain\n\u2212 n 2 I(x\u22a4+w \u2217 + b\u2217) [ x+ 1 ] + n 2 I(\u2212x\u22a4\u2212w\u2217 \u2212 b\u2217) [ x\u2212 1 ] + [ \u03bbw\u2217 0 ]\n=\u2212 n 2 I(1) [ x+ 1 ] + n 2 I(1) [ x\u2212 1 ] + [ \u03bbw\u2217 0 ]\n\u2283n 2 I(1)\n[ x\u2212 \u2212 x+\n0\n]\n+\n[ \u03bbw\u2217\n0\n]\nsetting the last dimension to 0\n=I(1)\n[\n\u2212 n\u2016w\u2217\u20162w\u2217 0\n]\n+\n[ \u03bbw\u2217\n0\n]\napplying (35)\n\u2287I(1) [ \u2212\u03bbw\u2217\n0\n]\n+\n[ \u03bbw\u2217\n0\n]\nobserving n \u2265 \u03bb\u2016w\u2217\u20162\n\u220b0.\nIt proves that [w\u2217; b\u2217] solves (34) by our teaching set construction. Next we prove uniqueness by contradiction. We use f(w, b) to denote the objective function in (34) under the teaching set. It is easy to verify that f(w\u2217, b\u2217) = \u03bb2\u2016w\u2217\u20162. Assume that there exists another solution [w\u0304; b\u0304] different from [w\u2217; b\u2217]. We can obtain \u2016w\u0304\u20162 \u2264 \u2016w\u2217\u20162 due to\n\u03bb 2 \u2016w\u2217\u20162 = f(w\u2217, b\u2217) = f(w\u0304, b\u0304) \u2265 \u03bb 2 \u2016w\u0304\u20162.\nThe second equality is due to [w\u0304; b\u0304] being a solution; the inequality is due to whole-part relationship. Therefore, there are only two possibilities for the norm of w\u0304: \u2016w\u0304\u2016 = \u2016w\u2217\u2016 or \u2016w\u0304\u2016 = t\u2016w\u2217\u2016 for some 0 \u2264 t < 1. Next we will show that both cases are impossible.\n(Case 1) For the case \u2016w\u0304\u2016 = \u2016w\u2217\u2016, we have\nf(w\u0304, b\u0304) = n\n2 max\n( 1\u2212 (x\u22a4+w\u0304 + b\u0304), 0 ) + n\n2 max\n( 1 + (x\u22a4\u2212w\u0304 + b\u0304), 0 ) + \u03bb\n2 \u2016w\u0304\u20162\n= n\n2 max\n\n x \u22a4 +(w \u2217 \u2212 w\u0304) + (b\u2217 \u2212 b\u0304) \ufe38 \ufe37\ufe37 \ufe38\n=:\u2206+\n, 0\n\n  +\nn 2 max\n\n \u2212x\u22a4\u2212(w\u2217 \u2212 w\u0304)\u2212 (b\u2217 \u2212 b\u0304) \ufe38 \ufe37\ufe37 \ufe38\n=:\u2206\u2212\n, 0\n\n \n+ \u03bb 2 \u2016w\u2217\u20162\n= n\n2 max (\u2206+, 0) +\nn 2 max (\u2206\u2212, 0) + f(w \u2217, b\u2217).\nFrom f(w\u0304, b\u0304) = f(w\u2217, b\u2217), it follows \u2206+ \u2264 0 and \u2206\u2212 \u2264 0. Since\n0 \u2265 \u2206+ +\u2206\u2212 = (x+ \u2212 x\u2212)\u22a4(w\u2217 \u2212 w\u0304) = 2(w\u2217)\u22a4(w\u2217 \u2212 w\u0304) \u2016w\u2217\u20162 = 2\u2212 2 w\u0304\u22a4w\u2217 \u2016w\u2217\u20162 ,\nwe have w\u0304\u22a4w\u2217 \u2265 \u2016w\u2217\u20162. But because \u2016w\u0304\u2016 = \u2016w\u2217\u2016, we must have w\u0304 = w\u2217. Applying this new observation to \u2206+ \u2264 0 and \u2206\u2212 \u2264 0, we obtain b\u2217 = b\u0304. It means that [w\u2217; b\u2217] = [w\u0304; b\u0304], contradicting our assumption [w\u2217; b\u2217] 6= [w\u0304; b\u0304].\n(Case 2) Next we turn to the case \u2016w\u0304\u2016 = t\u2016w\u2217\u2016 for some t \u2208 [0, 1). Recall our assumption that [w\u0304; b\u0304] solves (34). Then it follows that the following specific construction [w\u0302, b\u0302] solves (34) as well:\nw\u0302 = tw\u2217, b\u0302 = tb\u2217. (37)\nTo see this, we consider the following optimization problem:\nmin w,b\nL(w, b) := n 2 max(1\u2212 (x\u22a4+w + b), 0) + n 2 max(1 + (x\u22a4\u2212w + b), 0)\ns.t. \u2016w\u2016 \u2264 t\u2016w\u2217\u2016. (38)\nSince [w\u0304; b\u0304] solves (34), it is easy to see that [w\u0304; b\u0304] solves (38) too, otherwise there exists a solution for (38) which gives a lower function value on (34). Then we can verify that [w\u0302; b\u0302] solves (38) as well by showing the following optimality condition holds:\n\u2212 [ \u2202L(w,b)\n\u2202w \u2202L(w,b)\n\u2202b ]\u2223 \u2223 \u2223 \u2223 \u2223 [w\u0302;b\u0302] \u2229 N\u2016w\u2016\u2264t\u2016w\u2217\u2016(w\u0302, b\u0302) \ufe38 \ufe37\ufe37 \ufe38\nNormal cone to the set {[w; b] : \u2016w\u2016 \u2264 t\u2016w\u2217\u2016} at [w\u0302; b\u0302]\n6= \u2205 (39)\nBecause of (36) and (37), we have x\u22a4+w\u0302 + b\u0302 = t < 1. Thus at [w\u0302; b\u0302] the subgradient is\n\u2212 [ \u2202L(w,b)\n\u2202w \u2202L(w,b)\n\u2202b ]\u2223 \u2223 \u2223 \u2223 \u2223 [w\u0302;b\u0302] = n 2 [ x+ \u2212 x\u2212 0 ] = [ nw\u2217 \u2016w\u2217\u20162 0 ]\n(40)\nAnd the normal cone is\nN\u2016w\u2016\u2264t\u2016w\u2217\u2016(w\u0302, b\u0302) = { s [ w\u2217\n0 ] \u2223 \u2223 \u2223 \u2223 \u2223 s \u2265 0 } . (41)\nThe intersection is non-empty by choosing s = n\u2016w\u2217\u20162 . Since both [w\u0302; b\u0302] and [w\u0304; b\u0304] solve (38), we have L(w\u0302, b\u0302) = L(w\u0304, b\u0304). Together with \u2016w\u0302\u2016 = \u2016w\u0304\u2016, we have\nf(w\u0302, b\u0302) = L(w\u0302, b\u0302) + \u03bb 2 \u2016w\u0302\u20162 = f(w\u0304, b\u0304) = f(w\u2217, b\u2217).\nTherefore, we proved that [w\u0302; b\u0302] solves (34) as well. To see the contradiction, let us check the function value of f(w\u0302, b\u0302) via a different route:\nf(w\u0302, b\u0302) =f(tw\u2217, tb\u2217)\n=\nn\n2\u2211\ni=1\nmax ( 1\u2212 t(x\u22a4+w\u2217 + b\u2217), 0 ) +\nn\n2\u2211\ni=1\nmax (\n1 + t(x\u22a4\u2212w \u2217 + b\u2217), 0\n)\n+ \u03bb 2 \u2016w\u2217\u20162t2\n=\nn\n2\u2211\ni=1\nmax (1\u2212 t, 0) + n 2\u2211\ni=1\nmax (1\u2212 t, 0) + \u03bb 2 \u2016w\u2217\u20162t2\n=n(1\u2212 t)\u2212 \u03bb 2 \u2016w\u2217\u20162(1\u2212 t2) + \u03bb 2 \u2016w\u2217\u20162 \u2265n(1\u2212 t)\u2212 n 2 (1\u2212 t2) + \u03bb 2 \u2016w\u2217\u20162 = n\n2 (1\u2212 t)2 + f(w\u2217, b\u2217)\n>f(w\u2217, b\u2217),\nwhere the first inequality uses the fact that n \u2265 \u03bb\u2016w\u2217\u20162. It contradicts our early assertion f(w\u0302, b\u0302) = f(w\u2217, b\u2217). Putting cases 1 and 2 together we prove uniqueness.\nOur construction of the teaching set in (35) requires n = 2 \u2308 \u03bb\u2016w\u2217\u20162\n2\n\u2309\ntraining items. This is an\nupper bound on the teaching dimension. Meanwhile, we show below that the inhomogeneous SVM lower bound is LB3 = \u2308 \u03bb\u2016w\u2217\u20162 \u2309 . There can be a difference of at most one between the lower and upper bounds, which we call the \u201crounding effect.\u201d We suspect that this small gap is a technicality and not intrinsic. However, at present we do not have a teaching set construction that bridges this gap. Therefore, we state the teaching dimension as an interval in the following corollary and leave the precise value as an open question for future research.\nCorollary 5. The teaching dimension for inhomogeneous SVM and target \u03b8\u2217 = [w\u2217; b\u2217] where w\u2217 6= 0 is in the interval \u2308 \u03bb\u2016w\u2217\u20162 \u2309 \u2264 TD(\u03b8\u2217,Ainhsvm) \u2264 2 \u2308 \u03bb\u2016w\u2217\u20162\n2\n\u2309\n.\nProof. The upper bound directly follows Proposition 5. We only need to show the lower bound LB3 = \u2308 \u03bb\u2016w\u2217\u20162 \u2309 in Theorem 3. Let A = I, \u2113(a) = max(1 \u2212 a, 0), and consider the denominator of (18):\nsup \u03b1\u2208R,g\u2208\u2212\u2202\u2113(\u03b1\u2016w\u2217\u20162) \u03b1g = sup \u03b1,g\u2208I(\u03b1\u2016w\u2217\u20162)\n\u03b1g = 1\n\u2016w\u2217\u20162\nwhere the first equality is due to \u2202\u2113(a) = \u2212I(a). Therefore, LB3 = \u2308 \u03bb\u2016w\u2217\u20162 \u2309 which proves the lower bound.\nInhomogeneous logistic regression solves the problem\nmin w\u2208Rd,b\u2208R\nn\u2211\ni=1\nlog(1 + exp{\u2212yi(x\u22a4i w + b)}) + \u03bb\n2 \u2016w\u20162 (42)\nProposition 6. To create a teaching set for target model [w\u2217; b\u2217] with nonzero w\u2217 for inhomogeneous logistic regression (42), we can use n = 2 \u2308 \u03bb\u2016w\u2217\u20162 2\u03c4max \u2309 training items where xi = x+, yi = 1, \u2200i \u2208 { 1, \u00b7 \u00b7 \u00b7 , n2 } and xi = x\u2212, yi = \u22121, \u2200i \u2208 { n 2 + 1, \u00b7 \u00b7 \u00b7 , n } . x+ and x\u2212 can be designed as any vectors satisfying\nx\u22a4+w \u2217 = t\u2212 b\u2217, x\u2212 = x+ \u2212\n2t\n\u2016w\u2217\u20162w \u2217, (43)\nwhere the constant t is defined by t := \u03c4\u22121 ( \u03bb\u2016w\u2217\u20162\nn\n)\n.\nProof. We first point out that for t to be well-defined the argument to \u03c4\u22121() has to be bounded \u03bb\u2016w\u2217\u20162\nn \u2264 \u03c4max. This implies n \u2265 \u03bb\u2016w \u2217\u20162 \u03c4max\n. The size of our proposed teaching set is the smallest among all such symmetric construction that satisfy this constraint.\nWe verify that the KKT condition to show the construction in (43) includes the solution [w\u2217; b\u2217]. From (43), we have\nx\u22a4+w \u2217 + b\u2217 = t x\u22a4\u2212w \u2217 + b\u2217 = \u2212t.\nWe apply them and the teaching set construction to compute the gradient of (42):\n\u2212 n 2\n1\n1 + exp{x\u22a4+w\u2217 + b\u2217}\n[ x+ 1 ] + n 2\n1\n1 + exp{\u2212x\u22a4\u2212w\u2217 \u2212 b\u2217}\n[ x\u2212 1 ] + [ \u03bbw\u2217 0 ]\n=\u2212 n 2\n1\n1 + exp{t} [ x+ 1 ] + n 2\n1\n1 + exp{t} [ x\u2212 1 ] + [ \u03bbw\u2217 0 ]\n=\u2212 n\u2016w\u2217\u20162 t 1 + exp{t}\n[ w\u2217\n0\n]\n+\n[ \u03bbw\u2217\n0\n]\n=\u2212 n\u2016w\u2217\u20162 \u03bb\u2016w\u2217\u20162 n [ w\u2217 0 ] + [ \u03bbw\u2217 0 ]\n=0.\nThis verifies the KKT condition. Finally we show uniqueness. The Hessian matrix of the objective function (42) under our training set (43) is:\nn\n2 exp{t} (1 + exp{t})2 \ufe38 \ufe37\ufe37 \ufe38\n:=a\n[ x+x \u22a4 + + x\u2212x \u22a4 \u2212 x+ + x\u2212\nx\u22a4+ + x \u22a4 \u2212 2\n]\n\ufe38 \ufe37\ufe37 \ufe38\n:=A\n+\u03bb [ I 0 0\u22a4 0 ]\n\ufe38 \ufe37\ufe37 \ufe38\n:=B\n.\nNote a > 0 and A = [ x+ 1 ] [ x+ 1 ] + [ x\u2212 1 ] [ x\u2212 1 ] is positive semi-definite. We show that aA+\u03bbB is positive definite. Suppose not. Then there exists [u; v] 6= 0 such that [u; v]\u22a4(aA+\u03bbB)[u; v] = 0. This implies [u; v]\u22a4(aA)[u; v] + \u03bbu\u22a4u = 0. Since the first term is non-negative due to A being positive semi-definite, u = 0. But then we have 2av2 = 0 which implies [u; v] = 0, a contradiction. Therefore uniqueness is guaranteed.\nCorollary 6. The teaching dimension for inhomogeneous logistic regression and target \u03b8\u2217 = [w\u2217; b\u2217] where w\u2217 6= 0 is in the interval \u2308 \u03bb\u2016w\u2217\u20162 \u03c4max \u2309 \u2264 TD(\u03b8\u2217,Ainhlog ) \u2264 2 \u2308 \u03bb\u2016w\u2217\u20162 2\u03c4max \u2309 .\nProof. The upper bound directly follows Proposition 6. We only need to show the lower bound\u2308 \u03bb\u2016w\u2217\u20162 \u03c4max \u2309\nby applying LB3 in Theorem 3. Let A = I and \u2113(a) = log(1 + exp{\u2212a}) and consider the denominator of (18):\nsup \u03b1\u2208R,g\u2208\u2202\u2113(\u2212\u03b1\u2016w\u2217\u20162) \u03b1g = sup \u03b1,g=(1+exp{\u03b1\u2016w\u2217\u20162})\u22121 \u03b1g\n=sup \u03b1\n\u03b1\n1 + exp{\u03b1\u2016w\u2217\u20162} =\u2016w\u2217\u2016\u22122 sup\nt\nt\n1 + exp{t} =\n\u03c4max \u2016w\u2217\u20162 ,\nwhich implies LB3 = \u2308 \u03bb\u2016w\u2217\u20162 \u03c4max \u2309 ."}, {"heading": "4 Teaching a Decision Boundary Instead of a Parameter", "text": "In section 3 we considered the teaching goal where the learner is required to learn the exact target parameter \u03b8\u2217. But when the learner is a classifier often a weaker teaching goal is sufficient, namely teaching the learner a target decision boundary. In this section we consider this teaching goal. Equivalently, such a goal is defined by the set of parameters that produce the target decision boundary. Teaching is successful if the learner arrives at any one parameter within that set.\nIn the case of inhomogeneous linear learners, the linear decision boundary {x | x\u22a4w\u2217 + b\u2217 = 0} is identified with the parameter set {t[w\u2217; b\u2217] : t > 0}. Here we assume w\u2217 is nonzero. The parameter \u03b8\u2217 = [w\u2217; b\u2217] is just a representative member of the set. Homogeneous linear learners are similar without b\u2217. We denote the corresponding \u201cdecision boundary\u201d teaching dimension by TD({t\u03b8\u2217},Aopt). This notation extends our earlier definition of TD by allowing the first argument to be a set, with the understanding that the teaching goal is for the learned model to be an element in the set. It immediately follows that\nTD({t\u03b8\u2217},Aopt) = min t>0 TD(t\u03b8\u2217,Aopt). (44)\nSince it is sufficient to teach the parameter t\u03b8\u2217 for some t > 0 in order to teach the decision boundary, we can choose the best t that minimizes TD(t\u03b8\u2217,Aopt). For SVM and logistic regression \u2013 either homogeneous or inhomogeneous \u2013 the teaching dimension TD(t\u03b8\u2217,Aopt) depends on \u2016t\u03b8\u2217\u2016 (see Table 1). We can choose t sufficiently small to drive down the teaching set size toward its minimum (which is nonzero because of the ceiling function). Specifically, for any fixed parameter \u03b8\u2217 representing the target decision boundary:\n\u2022 (homogeneous SVM): we can choose t \u2264 1\u221a \u03bb\u2016\u03b8\u2217\u2016 so that TD({t\u03b8 \u2217},Ahomsvm) = 1; \u2022 (homogeneous logistic regression): we can choose t \u2264 \u221a \u03c4max\u221a \u03bb\u2016\u03b8\u2217\u2016 so that TD({t\u03b8 \u2217},Ahomlog ) = 1; \u2022 (inhomogeneous SVM): we can choose t \u2264 \u221a 2\u221a\n\u03bb\u2016w\u2217\u2016 so that TD({t\u03b8 \u2217},Ainhsvm) = 2;\n\u2022 (inhomogeneous logistic regression): we can choose t \u2264 \u221a 2\u03c4max\u221a \u03bb\u2016w\u2217\u2016 so that TD({t\u03b8 \u2217},Ainhlog ) = 2.\nThe resulting teaching dimension TD({t\u03b8\u2217},Aopt) is listed in Table 1 on the row marked by \u201cdecision boundary.\u201d The teaching set construction is the same as in sections 3.2 and 3.3, respectively, but with t\u03b8\u2217."}, {"heading": "5 Related Work", "text": "Teaching dimension as a learning-theoretic quantity has attracted a long history of research. It was proposed independently in [13, 23]. Subsequent theoretical developments can be found in e.g. [26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12]. Most of them assume little extra knowledge on the learner other than that it is consistent with the training data. While such version-space learners are elegant object of theoretical study, they diverge from the practice of modern machine learning. Our present paper is among the first to extend teaching dimension to optimization-based machine learners.\nTeaching dimension is distinct from VC dimension. For a finite hypothesis space H, Goldman and Kearns [13] proved the relation\nV C(H)/ log(|H|) \u2264 TD(H) \u2264 V C(H) + |H| \u2212 2V C(H). (45)\nThese inequalities are somewhat weak, as Goldman and Kearns had shown both cases where one quantity is much larger than the other. The distinction between TD and VC dimension is also present in our setting. For example, by inspecting the inhomogeneous SVM column in Table 1 we note that TD does not depend on the dimensionality d of the feature space Rd. To see why this makes intuitive sense, note two d-dimensional points are sufficient to specify any bisecting hyperplane in Rd. On the other hand, recall that the VC dimension for inhomogeneous hyperplanes in Rd is d + 1. Further quantification of the relation between TD and VC (and other capacity measures) remains an open research question.\nThe teaching setting we considered is also distinct from active learning. In teaching the teacher knows the target model a priori and her goal is to encode the target model as a training set, knowing that the decoder is special (namely a specific machine learning algorithm). This communication perspective highlights the difference to active learning, which must explore the hypothesis space to find the target model. Consequently, the teaching dimension can be dramatically smaller than the active learning query complexity for the same learner and hypothesis space. For example, Zhu [24] demonstrated that to learn a 1D threshold classifier within \u01eb error, the teaching dimension is a constant TD=2 regardless of \u01eb, while active learning would require O(log 1\n\u01eb ) queries which can be\narbitrarily larger than TD. While the present paper focused on the theory of optimal teaching, there are practical applications, too. One such application is computer-aided personalized education. The human student is modeled by a computational cognitive model, or equivalently the learning algorithm. The educational goal is encoded in the target model. The optimal teaching set is then well-defined, and represents the best personalized lesson for the student [25, 24, 16]. Patil et al. showed that human students learn statistically significantly better under such optimal teaching set compared to an i.i.d. training set [21]. Because contemporary cognitive models often employ optimization-based machine learners, our teaching dimension study helps to characterize these optimal lessons.\nAnother application of optimal teaching is in computer security. In particular, optimal teaching is the mathematical formalism to study the so-called data poisoning attacks [8, 20, 19, 1]. Here the \u201cteacher\u201d is an attacker who has a nefarious target model in mind. The \u201cstudent\u201d is a learning agent (such as a spam filter) which accepts data and adapts itself. The attacker wants to minimally manipulate the input data in order to manipulate the learning agent toward the attacker\u2019s target model. Teaching dimension quantifies the difficulty of data-poisoning attacks, and enables research on defenses.\nTeaching dimension also has applications in interactive machine learning to quantify the minimum human interaction necessary [10], and in formal synthesis to generate computer programs satisfying a specification [15]."}, {"heading": "6 Conclusion", "text": "We have presented a generalization on teaching dimension to optimization-based learners. To the best of our knowledge, our teaching dimension for ridge regression, SVM, and logistic regression is new; so are the lower bounds and our analysis technique in general.\nThere are many possible extensions to the present work. For example, one may extend our analysis to nonlinear learners. This can potentially be achieved by using the kernel trick on the linear learners. As another example, one may allow \u201capproximate teaching\u201d by relaxing the teaching goal, such that teaching is considered successful if the learner arrives at a model close enough to the target model. Taken together, the present paper and its extensions are expected to enrich our understanding of optimal teaching and enable novel applications."}], "references": [{"title": "Data poisoning attacks against autoregressive models", "author": ["S. Alfeld", "X. Zhu", "P. Barford"], "venue": "AAAI,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Queries revisited", "author": ["D. Angluin"], "venue": "Theoretical Computer Science, 313(2):175\u2013194,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Teachers, learners and black boxes", "author": ["D. Angluin", "M. Krikis"], "venue": "COLT,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning from different teachers", "author": ["D. Angluin", "M. Krikis"], "venue": "Machine Learning, 51(2):137\u2013163,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Measuring teachability using variants of the teaching dimension", "author": ["F.J. Balbach"], "venue": "Theor. Comput. Sci., 397(1-3):94\u2013113, May", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Teaching randomized learners", "author": ["F.J. Balbach", "T. Zeugmann"], "venue": "COLT, pages 229\u2013243,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Recent developments in algorithmic teaching", "author": ["F.J. Balbach", "T. Zeugmann"], "venue": "Proceedings of the 3rd International Conference on Language and Automata Theory and Applications, pages 1\u201318,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "The security of machine learning", "author": ["M. Barreno", "B. Nelson", "A.D. Joseph", "J.D. Tygar"], "venue": "Machine Learning Journal, 81(2):121\u2013148,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Self-directed learning and its relation to the VC-dimension and to teacher-directed learning", "author": ["S. Ben-David", "N. Eiron"], "venue": "Machine Learning, 33(1):87\u2013104,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "Mixed-initiative active learning", "author": ["M. Cakmak", "A. Thomaz"], "venue": "ICML Workshop on Combining Learning Strategies to Reduce Label Cost,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "On the LambertW function", "author": ["R. Corless", "G. Gonnet", "D. Hare", "D. Jeffrey", "D. Knuth"], "venue": "Advances in Computational Mathematics, 5(1):329\u2013359,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "Recursive teaching dimension, VC-dimension and sample compression", "author": ["T. Doliwa", "G. Fan", "H.U. Simon", "S. Zilles"], "venue": "Journal of Machine Learning Research, 15:3107\u20133131,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "On the complexity of teaching", "author": ["S. Goldman", "M. Kearns"], "venue": "Journal of Computer and Systems Sciences, 50(1):20\u201331,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1995}, {"title": "Teaching a smarter learner", "author": ["S. Goldman", "H. Mathias"], "venue": "Journal of Computer and Systems Sciences, 52(2):255267,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1996}, {"title": "A theory of formal synthesis via inductive learning", "author": ["S. Jha", "S.A. Seshia"], "venue": "CoRR, abs/1505.03953,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "How do humans teach: On curriculum learning and teaching dimension", "author": ["F. Khan", "X. Zhu", "B. Mutlu"], "venue": "NIPS,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Complexity of teaching by a restricted number of examples", "author": ["H. Kobayashi", "A. Shinohara"], "venue": "COLT, pages 293\u2013302,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "A model of interactive teaching", "author": ["H.D. Mathias"], "venue": "J. Comput. Syst. Sci., 54(3):487\u2013501,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "The security of latent Dirichlet allocation", "author": ["S. Mei", "X. Zhu"], "venue": "AISTATS,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Using machine teaching to identify optimal training-set attacks on machine learners", "author": ["S. Mei", "X. Zhu"], "venue": "AAAI,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimal teaching for limited-capacity human learners", "author": ["K. Patil", "X. Zhu", "L. Kopec", "B. Love"], "venue": "NIPS,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Being taught can be faster than asking questions", "author": ["R.L. Rivest", "Y.L. Yin"], "venue": "COLT,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1995}, {"title": "Teachability in computational learning", "author": ["A. Shinohara", "S. Miyano"], "venue": "New Generation Computing, 8(4):337\u2013348,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1991}, {"title": "Machine teaching for Bayesian learners in the exponential family", "author": ["X. Zhu"], "venue": "NIPS,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Machine teaching: an inverse problem to machine learning and an approach toward optimal education", "author": ["X. Zhu"], "venue": "AAAI,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Models of cooperative teaching and learning", "author": ["S. Zilles", "S. Lange", "R. Holte", "M. Zinkevich"], "venue": "Journal of Machine Learning Research, 12:349\u2013384,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 12, "context": "How many training items are needed? This is the question addressed by the teaching dimension [13, 23].", "startOffset": 93, "endOffset": 101}, {"referenceID": 22, "context": "How many training items are needed? This is the question addressed by the teaching dimension [13, 23].", "startOffset": 93, "endOffset": 101}, {"referenceID": 11, "context": "[12]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "For any such \u03b4 , simple algebra verifies that \u03b8\u2217+ t\u03b4 satisfies the KKT condition (7) for any t \u2208 [0, 1].", "startOffset": 97, "endOffset": 103}, {"referenceID": 0, "context": "\u2022 If the loss function l(\u00b7, \u00b7) is smooth (not necessary convex) in the first argument, we have f(\u03b8\u2217) = f(\u03b8\u2217 + \u03b4) by using the Taylor expansion (recall f is defined in equation 1): f(\u03b8\u2217 + \u03b4) =f(\u03b8\u2217) + \u3008\u2207f(\u03b8\u2217 + t\u03b4), \u03b4\u3009 (for some t \u2208 [0, 1]) =f(\u03b8\u2217) + \u3008 n \u2211", "startOffset": 230, "endOffset": 236}, {"referenceID": 0, "context": "I(a) = \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 1, if a < 1 [0, 1], if a = 1 0, otherwise .", "startOffset": 27, "endOffset": 33}, {"referenceID": 0, "context": "where the last line is due to I ( \u03bb\u2016\u03b8\u2217\u20162 \u2308\u03bb\u2016\u03b8\u2217\u20162\u2309 ) giving either the set [0, 1] or the value 1.", "startOffset": 74, "endOffset": 80}, {"referenceID": 10, "context": "However, due to the negative log likelihood term we have a coefficient defined by the Lambert W function [11], which we denote by Wlam.", "startOffset": 105, "endOffset": 109}, {"referenceID": 12, "context": "It was proposed independently in [13, 23].", "startOffset": 33, "endOffset": 41}, {"referenceID": 22, "context": "It was proposed independently in [13, 23].", "startOffset": 33, "endOffset": 41}, {"referenceID": 25, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 6, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 1, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 2, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 13, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 17, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 5, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 4, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 16, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 3, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 21, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 8, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 11, "context": "[26, 7, 2, 3, 14, 18, 6, 5, 17, 4, 22, 9, 12].", "startOffset": 0, "endOffset": 45}, {"referenceID": 12, "context": "For a finite hypothesis space H, Goldman and Kearns [13] proved the relation V C(H)/ log(|H|) \u2264 TD(H) \u2264 V C(H) + |H| \u2212 2 C(H).", "startOffset": 52, "endOffset": 56}, {"referenceID": 23, "context": "For example, Zhu [24] demonstrated that to learn a 1D threshold classifier within \u01eb error, the teaching dimension is a constant TD=2 regardless of \u01eb, while active learning would require O(log 1 \u01eb ) queries which can be arbitrarily larger than TD.", "startOffset": 17, "endOffset": 21}, {"referenceID": 24, "context": "The optimal teaching set is then well-defined, and represents the best personalized lesson for the student [25, 24, 16].", "startOffset": 107, "endOffset": 119}, {"referenceID": 23, "context": "The optimal teaching set is then well-defined, and represents the best personalized lesson for the student [25, 24, 16].", "startOffset": 107, "endOffset": 119}, {"referenceID": 15, "context": "The optimal teaching set is then well-defined, and represents the best personalized lesson for the student [25, 24, 16].", "startOffset": 107, "endOffset": 119}, {"referenceID": 20, "context": "training set [21].", "startOffset": 13, "endOffset": 17}, {"referenceID": 7, "context": "In particular, optimal teaching is the mathematical formalism to study the so-called data poisoning attacks [8, 20, 19, 1].", "startOffset": 108, "endOffset": 122}, {"referenceID": 19, "context": "In particular, optimal teaching is the mathematical formalism to study the so-called data poisoning attacks [8, 20, 19, 1].", "startOffset": 108, "endOffset": 122}, {"referenceID": 18, "context": "In particular, optimal teaching is the mathematical formalism to study the so-called data poisoning attacks [8, 20, 19, 1].", "startOffset": 108, "endOffset": 122}, {"referenceID": 0, "context": "In particular, optimal teaching is the mathematical formalism to study the so-called data poisoning attacks [8, 20, 19, 1].", "startOffset": 108, "endOffset": 122}, {"referenceID": 9, "context": "Teaching dimension also has applications in interactive machine learning to quantify the minimum human interaction necessary [10], and in formal synthesis to generate computer programs satisfying a specification [15].", "startOffset": 125, "endOffset": 129}, {"referenceID": 14, "context": "Teaching dimension also has applications in interactive machine learning to quantify the minimum human interaction necessary [10], and in formal synthesis to generate computer programs satisfying a specification [15].", "startOffset": 212, "endOffset": 216}], "year": 2015, "abstractText": "Teaching dimension is a learning theoretic quantity that specifies the minimum training set size to teach a target model to a learner. Previous studies on teaching dimension focused on version-space learners which maintain all hypotheses consistent with the training data, and cannot be applied to modern machine learners which select a specific hypothesis via optimization. This paper presents the first known teaching dimension for ridge regression, support vector machines, and logistic regression. We also exhibit optimal training sets that match these teaching dimensions. Our approach generalizes to other linear learners.", "creator": "LaTeX with hyperref package"}}}