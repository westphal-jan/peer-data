{"id": "1512.02433", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Dec-2015", "title": "Minimum Risk Training for Neural Machine Translation", "abstract": "We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to evaluation metrics. Experiments on Chinese-English and English-French translation show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system, where a large portion of the data can be read for a wide range of tasks. Our approach also uses a more natural and targeted approach to learning and memory encoding. Further, our approach also shows that our approach achieves a more appropriate response for specific problems.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 8 Dec 2015 12:42:00 GMT  (8kb,D)", "http://arxiv.org/abs/1512.02433v1", "9 pages"], ["v2", "Wed, 9 Dec 2015 14:20:36 GMT  (8kb,D)", "http://arxiv.org/abs/1512.02433v2", "Some typos corrected; the case-sensitiveness of BLEU scores are given"], ["v3", "Wed, 15 Jun 2016 00:07:05 GMT  (103kb,D)", "http://arxiv.org/abs/1512.02433v3", "Accepted for publication in Proceedings of ACL 2016"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shiqi shen", "yong cheng", "zhongjun he", "wei he", "hua wu", "maosong sun", "yang liu 0005"], "accepted": true, "id": "1512.02433"}, "pdf": {"name": "1512.02433.pdf", "metadata": {"source": "CRF", "title": "Minimum Risk Training for Neural Machine Translation", "authors": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "emails": ["liuyang2011@tsinghua.edu.cn."], "sections": [{"heading": "1 Introduction", "text": "Recently, end-to-end neural machine translation (NMT) [Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015] has attracted increasing attention from the community. Providing a new paradigm for machine translation, NMT aims at training a single, large neural network that directly transforms a source-language sentence to a target-language sentence without explicitly modeling latent structures (e.g., word alignment, phrase segmentation, phrase reordering, and SCFG derivation) present in conventional statistical machine translation (SMT) [Brown et al., 1993; Koehn et al., 2003; Chiang, 2005].\nCurrent NMT models build on the encoder-decoder framework [Cho et al., 2014; Sutskever et al., 2014], with an encoder to read and encode a source language sentence into a vector and a decoder to generate a target-language sentence from the vector. While early efforts encode the input into a fixedlength vector, Bahdanau et al. [2015] introduce the mechanism of attention to dynamically generate a context vector for a target word being generated.\nAlthough NMT models have achieved results on par with or better than conventional SMT, they still suffer a major drawback: the models are optimized\n\u2217Yang Liu is the corresponding author: liuyang2011@tsinghua.edu.cn.\nar X\niv :1\n51 2.\n02 43\n3v 1\n[ cs\n.C L\nto maximize the likelihood of training data instead of evaluation metrics that actually quantify translation quality. Ranzato et al. [2015] indicate two drawbacks of maximum likelihood estimation (MLE) for neural machine translation: (1) the models are only exposed to the training data distribution instead of model predictions and (2) the loss function is defined at the word level instead of sentence level.\nIn this work, we introduce minimum risk training (MRT) for neural machine translation. The new training objective is to minimize the expected loss on the training data. One advantage of MRT is that it allows for arbitrary loss functions, which are not necessarily differentiable. In addition, our approach does not assume the specific architectures of NMT and can be applied to any end-to-end NMT systems. While MRT has been widely used in conventional SMT [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012] and deep learning based MT [Gao et al., 2014], to the best of our knowledge, this work is the first effort to introduce MRT into end-to-end neural machine translation. Experiments on Chinese-English and English-French show that MRT leads to significant improvements over MLE on a state-of-the-art NMT system [Bahdanau et al., 2015]."}, {"heading": "2 Background", "text": "Given a source-language sentence x = x1, . . . .xm, . . .xM and a target-language sentence y = y1, . . . ,yn, . . . ,yN , end-to-end neural MT directly models the translation probability:\nP (y|x;\u03b8) = N\u220f n=1 P (yn|x,y<n;\u03b8) (1)\nwhere \u03b8 is a set of model parameters and y<n = y1, . . . ,yn\u22121. Predicting the n-th target word can be done by using a recurrent neural network:\nP (yn|x,y<n;\u03b8) \u221d exp { q(yn\u22121, zn, cn,\u03b8) } (2)\nwhere zn is the n-th hidden state on the target side, cn is the context for generating the n-th target word, and q is a non-linear function. Current NMT approaches differ in calculating zn and cn and defining q. Please refer to [Sutskever et al., 2014; Bahdanau et al., 2015] for more details.\nGiven a set of training examples D = {\u3008x(s),y(s)\u3009}Ss=1, the standard training objective is to maximize the log likelihood of the training data:\n\u03b8\u0302MLE = argmax \u03b8\n{ L(\u03b8) } (3)\n= argmax \u03b8\n{ log\nS\u220f s=1 P (y(s)|x(s);\u03b8)\n} (4)\n= argmax \u03b8 { S\u2211 s=1 N(s)\u2211 n=1 logP (y(s)n |x(s),y (s) <n;\u03b8) } (5)\nThe partial derivative with respect to a model parameter \u03b8i is calculated as\n\u2202L(\u03b8) \u2202\u03b8i = S\u2211 s=1 N(s)\u2211 n=1 \u2202P (yn|x(s),y<n;\u03b8)/\u2202\u03b8i P (yn|x(s),y<n;\u03b8)\n(6)\nRanzato et al. [2015] argue that MLE for neural machine translation suffers from two drawbacks. First, while the models are trained only on the training data distribution, they are used to generate target words based on previous model predictions at test time. This is referred to as exposure bias [Ranzato et al., 2015]. Second, MLE usually uses the cross-entropy loss to maximize the probability of the next correct word, which might hardly correlate well with evaluation metrics such as BLEU [Papineni et al., 2002].\nAs a result, it is important to introduce new training algorithms for end-toend neural machine translation that optimize model parameters directly with respect to evaluation metrics."}, {"heading": "3 Minimum Risk Training for Neural Machine", "text": "Translation\nMinimum risk training (MRT), which aims to minimize the expected loss on the training data, has been widely used in conventional statistical machine translation [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012] and deep learning based MT [Gao et al., 2014]. In this work, we introduce MRT into end-to-end neural machine translation. The training objective is given by\n\u03b8\u0302MRT = argmin \u03b8\n{ J (\u03b8) } (7)\n= argmin \u03b8 { S\u2211 s=1 \u2211 y\u2208Y(x(s)) P (y|x(s);\u03b8)\u2206(y,y(s)) } (8)\n= argmin \u03b8 { S\u2211 s=1 Ey|x(s);\u03b8 [ \u2206(y,y(s)) ]} (9)\nwhere \u2206(y,y(s)) is a loss function that measures the discrepancy between a predicted translation y and the training example y(s) and the expected loss Ey|x(s);\u03b8 [ \u2206(y,y(s)) ] is referred to as risk.\nThe partial derivative with respect to a model parameter \u03b8i is calculated as\n\u2202J (\u03b8) \u2202\u03b8i = S\u2211 s=1 Ey|x(s);\u03b8\n[ \u2206(y,y(s))\nN(s)\u2211 n=1 \u2202P (yn|x(s),y<n;\u03b8)/\u2202\u03b8i P (yn|x(s),y<n;\u03b8)\n] (10)\nUnfortunately, these expectations in Eq.(10) are usually intractable to calculate due to the exponential search space as well as the non-decomposability of the loss function.\nTo alleviate this problem, we propose to only use a subset of the search space and introduce a new training objective:\nJ\u0303 (\u03b8) = S\u2211 s=1 \u2211 y\u2208S(x(s)) P (y|x(s);\u03b8)\u03b1\u2211 y\u2032\u2208S(x(s)) P (y \u2032|x(s);\u03b8)\u03b1 \u2206(y,y(s)) (11)\n= S\u2211 s=1 \u2211 y\u2208S(x(s)) Q(y|x(s);\u03b8, \u03b1)\u2206(y,y(s)) (12)\n= S\u2211 s=1 Ey|x(s);\u03b8,\u03b1 [ \u2206(y,y(s)) ] (13)\nwhere S(x(s)) \u2282 Y(x(s)) is a subset of the full search space, Q(y|x(s);\u03b8, \u03b1) is a distribution defined on the subspace S(x(s)), and \u03b1 is a hyper-parameter that controls the sharpness of the Q distribution [Och, 2003].\nIntuitively, while MLE aims to maximize the likelihood of training data, our training objective is to discriminate between candidates in the sampled subspace S(x(s)). In practice, we find that this approximation works well (see Section 4).\nTherefore, the partial derivative with respect to a model parameter \u03b8i of J\u0303(\u03b8) is given by\n\u2202J\u0303(\u03b8)\n\u2202\u03b8i\n= \u03b1 S\u2211 s=1 Ey|x(s);\u03b8,\u03b1\n[ \u2206(y,y(s))\u00d7\n( N\u2211 n=1 \u2202P (yn|x(s),y<n;\u03b8)/\u2202\u03b8i P (yn|x(s),y<n;\u03b8) \u2212\nEy\u2032|x(s);\u03b8,\u03b1 [ N\u2211 n=1 \u2202P (y\u2032n|x(s),y\u2032<n;\u03b8)/\u2202\u03b8i P (y\u2032n|x(s),y\u2032<n;\u03b8) ])] (14)"}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Setup", "text": "We evaluated our approach on two translation tasks: Chinese-English and English-French. The evaluation metric is case-insensitive BLEU [Papineni et al., 2002] as calculated by the multi-bleu.perl script.\nFor Chinese-English, the training data consists of 2.56M pairs of sentences with 67.5M Chinese words and 74.8M English words, respectively. We used the\nNIST 2006 dataset as the validation set for optimizing hyper-parameters and model selection and the NIST 2002, 2003, 2004, 2005, and 2008 datasets as test sets. For English-French, to compare with the results reported by previous work on neural machine translation [Sutskever et al., 2014; Bahdanau et al., 2015], we used the same subset of the WMT 2014 training corpus that contains 12M sentence pairs with 348M French words and 304M English words. The concatenation of news-test 2012 and news-test 2013 serves as the validation set and news-test 2014 as the test set.\nWe compare our approach with two state-of-the-art SMT and NMT systems:\n1. Moses [Koehn and Hoang, 2007]: a phrase-based SMT system;\n2. GroundHog [Bahdanau et al., 2015]: an attention-based NMT system with maximum likelihood estimation.\nMoses uses the parallel corpus to train the phrase-based translation model and the target part to train a 4-gram language model using the SRILM toolkit [Stolcke, 2002]. The log-linear model is trained using the minimum error rate training (MERT) algorithm [Och, 2003].\nGroundHog uses the parallel corpus to train the neural translation model on a cluster with 20 Telsa K40 GPUs. It takes the cluster about one hour to train 20,000 minibatchs, each of which contains 80 sentences.\nOur system extends GroundHog by replacing maximum likelihood estimation with minimum risk training. We initialize our model with the RNNsearch50 model [Bahdanau et al., 2015]. We sample 100 candidate translations from the search space to build the Q distribution. As the samples might be identical, we remove duplicate candidates in the subspace. The gold-standard translation in the training data is included in the sampled space. We set the vocabulary size to 30K for each language. The training time for MRT is longer than MLE: 13,000 minibatchs can be processed in one hour on the same cluster. The beam size for decoding is 10. The hyper-parameter is set to 0.005. The loss function is negative smoothed sentence-level BLEU."}, {"heading": "4.2 Results on Chinese-English Translation", "text": "Table 1 shows BLEU scores on Chinese-English datasets. We find that introducing minimum risk training into neural MT leads to surprisingly substantial improvements over Moses and GroundHog with MLE as the training criterion (up to +7.74 and +6.86 BLEU points, respectively) across all test sets. All the improvements are statistically significant.\nTable 2 shows some example translations. We find that Moses translates a Chinese string \u201cyi wei fuze yu pingrang dangju da jiaodao de qian guowuyuan guanyuan\u201d that requires long-distance reordering in a wrong way, which is a notorious challenge for statistical machine translation. In contrast, GroundHogMLE seems to overcome this problem in this example thanks to the capability of gated RNNs to capture long-distance dependency. However, as MLE uses\na loss function defined only at the word level, its translation lacks sentencelevel consistency: \u201cchinese\u201d occurs twice while \u201ctwo senate\u201d is missing. By optimizing the model parameters directly with respect to sentence-level BLEU, GroundHog-MRT seems to be able to generate translations more consistently at the sentence level."}, {"heading": "4.3 Results on English-French Translation", "text": "Table 3 shows the results on English-French translation. We list existing endto-end NMT systems that are comparable to our system. All these systems use the same subset of the WMT 2014 training corpus and adopt MLE as the training criterion. They differ in network architectures and vocabulary sizes. Our GroundHog-MLE system achieves a BLEU score on par with those of Bahdanau et al. [2015] and Jean et al. [2015]. GroundHog-MRT achieves the highest BLEU score in this setting even with a smaller vocabulary size than Luong et al. [2015] and Sutskever et al. [2014]. 1 Note that our approach does not assume specific architectures and can in principle be applied to any NMT systems."}, {"heading": "5 Related Work", "text": "Our work originates from the minimum risk training algorithms in conventional statistical machine translation [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012]. Och [2003] describes a smoothed error count to allow for calculating gradients, which directly inspires us to use a parameter \u03b1 to adjust the smoothness of the objective function. Smith and Eisner [2006] introduce minimum risk annealing for training log-linear models that is capable of gradually annealing to focus on the 1-best hypothesis. He and Deng [2012] apply minimum risk training to learning phrase translation probabilities.\n1We have not adopted the recently introduced techniques such as ensemble of multiple model predictions [Sutskever et al., 2014], increasing vocabulary by importance sampling [Jean et al., 2015], and addressing the rare word problem [Luong et al., 2015] to further improve translation performance. We leave this for future work.\nMore recently, Gao et al. [2014] use MRT for learning continuous phrase representations for statistical machine translation. The difference is that they use MRT to optimize a sub-model of statistical machine translation while we are interested in directly optimizing end-to-end neural translation models.\nThe Mixed Incremental Cross-Entropy Reinforce (MIXER) algorithm [Ranzato et al., 2015] is in spirit most close to our work. Building on the REINFORCE algorithm proposed by Willams [1992], MIXER allows for incremental learning and the use of hybrid loss function that combines both REINFORCE and cross-entropy. The major difference is that Ranzato et al. [2015] leverage reinforcement learning while our work resorts to minimum risk training. Our experiments confirm their finding that taking evaluation metric into account when optimizing model parameters does help to improve sequence-level text generation."}, {"heading": "6 Conclusion", "text": "In this paper, we have presented a framework for minimum risk training in end-to-end neural machine translation. Experiments show that minimum risk training leads to significant improvements over maximum likelihood estimation for neural machine translation, especially for distantly-related languages such as Chinese and English.\nIn the future, we plan to test our approach on more language pairs and more end-to-end neural MT systems. We believe that minimum risk training will also benefit end-to-end neural architectures for other NLP tasks."}, {"heading": "Acknowledgements", "text": "This work was done when Shiqi Shen and Yong Cheng visited Baidu as interns. Maosong Sun and Hua Wu are supported by the 973 Program (2014CB340501 & 2014CB34505). Yang Liu is supported by the National Natural Science Foundation of China (No. 61522204 and No. 61432013). This research is also supported by the Singapore National Research Foundation under its International Research Centre@Singapore Funding Initiative and administered by the IDM Programme."}], "references": [{"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["David Chiang"], "venue": "In Proceedings of ACL,", "citeRegEx": "Chiang.,? \\Q2005\\E", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning continuous phrase representations for translation modeling", "author": ["Jianfeng Gao", "Xiaodong He", "Wen tao Yih", "Li Deng"], "venue": "In Proceedings of ACL,", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Maximum expected bleu training of phrase and lexicon translation models", "author": ["Xiaodong He", "Li Deng"], "venue": "In Proceedings of ACL,", "citeRegEx": "He and Deng.,? \\Q2012\\E", "shortCiteRegEx": "He and Deng.", "year": 2012}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Sebastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "In Proceedings of ACL,", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Kalchbrenner and Blunsom.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Factored translation models", "author": ["Philipp Koehn", "Hieu Hoang"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Koehn and Hoang.,? \\Q2007\\E", "shortCiteRegEx": "Koehn and Hoang.", "year": 2007}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz J. Och", "Daniel Marcu"], "venue": "In Proceedings of HLT-NAACL,", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "In Proceedings of ACL,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz J. Och"], "venue": "In Proceedings of ACL,", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Sequence level training with recurrent neural networks", "author": ["MarcAurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "Minimum risk annealing for training loglinear models", "author": ["David A. Smith", "Jason Eisner"], "venue": "In Proceedings of ACL,", "citeRegEx": "Smith and Eisner.,? \\Q2006\\E", "shortCiteRegEx": "Smith and Eisner.", "year": 2006}, {"title": "Srilm - am extensible language modeling toolkit", "author": ["Andreas Stolcke"], "venue": "In Proceedings of ICSLP,", "citeRegEx": "Stolcke.,? \\Q2002\\E", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Willams"], "venue": "Machine Learning,", "citeRegEx": "Willams.,? \\Q1992\\E", "shortCiteRegEx": "Willams.", "year": 1992}], "referenceMentions": [{"referenceID": 5, "context": "Recently, end-to-end neural machine translation (NMT) [Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015] has attracted increasing attention from the community.", "startOffset": 54, "endOffset": 133}, {"referenceID": 14, "context": "Recently, end-to-end neural machine translation (NMT) [Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015] has attracted increasing attention from the community.", "startOffset": 54, "endOffset": 133}, {"referenceID": 7, "context": ", word alignment, phrase segmentation, phrase reordering, and SCFG derivation) present in conventional statistical machine translation (SMT) [Brown et al., 1993; Koehn et al., 2003; Chiang, 2005].", "startOffset": 141, "endOffset": 195}, {"referenceID": 0, "context": ", word alignment, phrase segmentation, phrase reordering, and SCFG derivation) present in conventional statistical machine translation (SMT) [Brown et al., 1993; Koehn et al., 2003; Chiang, 2005].", "startOffset": 141, "endOffset": 195}, {"referenceID": 1, "context": "Current NMT models build on the encoder-decoder framework [Cho et al., 2014; Sutskever et al., 2014], with an encoder to read and encode a source language sentence into a vector and a decoder to generate a target-language sentence from the vector.", "startOffset": 58, "endOffset": 100}, {"referenceID": 14, "context": "Current NMT models build on the encoder-decoder framework [Cho et al., 2014; Sutskever et al., 2014], with an encoder to read and encode a source language sentence into a vector and a decoder to generate a target-language sentence from the vector.", "startOffset": 58, "endOffset": 100}, {"referenceID": 0, "context": ", 2003; Chiang, 2005]. Current NMT models build on the encoder-decoder framework [Cho et al., 2014; Sutskever et al., 2014], with an encoder to read and encode a source language sentence into a vector and a decoder to generate a target-language sentence from the vector. While early efforts encode the input into a fixedlength vector, Bahdanau et al. [2015] introduce the mechanism of attention to dynamically generate a context vector for a target word being generated.", "startOffset": 8, "endOffset": 358}, {"referenceID": 9, "context": "While MRT has been widely used in conventional SMT [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012] and deep learning based MT [Gao et al.", "startOffset": 51, "endOffset": 105}, {"referenceID": 12, "context": "While MRT has been widely used in conventional SMT [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012] and deep learning based MT [Gao et al.", "startOffset": 51, "endOffset": 105}, {"referenceID": 3, "context": "While MRT has been widely used in conventional SMT [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012] and deep learning based MT [Gao et al.", "startOffset": 51, "endOffset": 105}, {"referenceID": 2, "context": "While MRT has been widely used in conventional SMT [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012] and deep learning based MT [Gao et al., 2014], to the best of our knowledge, this work is the first effort to introduce MRT into end-to-end neural machine translation.", "startOffset": 133, "endOffset": 151}, {"referenceID": 8, "context": "Ranzato et al. [2015] indicate two drawbacks of maximum likelihood estimation (MLE) for neural machine translation: (1) the models are only exposed to the training data distribution instead of model predictions and (2) the loss function is defined at the word level instead of sentence level.", "startOffset": 0, "endOffset": 22}, {"referenceID": 14, "context": "Please refer to [Sutskever et al., 2014; Bahdanau et al., 2015] for more details.", "startOffset": 16, "endOffset": 63}, {"referenceID": 11, "context": "This is referred to as exposure bias [Ranzato et al., 2015].", "startOffset": 37, "endOffset": 59}, {"referenceID": 10, "context": "Second, MLE usually uses the cross-entropy loss to maximize the probability of the next correct word, which might hardly correlate well with evaluation metrics such as BLEU [Papineni et al., 2002].", "startOffset": 173, "endOffset": 196}, {"referenceID": 9, "context": "Minimum risk training (MRT), which aims to minimize the expected loss on the training data, has been widely used in conventional statistical machine translation [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012] and deep learning based MT [Gao et al.", "startOffset": 161, "endOffset": 215}, {"referenceID": 12, "context": "Minimum risk training (MRT), which aims to minimize the expected loss on the training data, has been widely used in conventional statistical machine translation [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012] and deep learning based MT [Gao et al.", "startOffset": 161, "endOffset": 215}, {"referenceID": 3, "context": "Minimum risk training (MRT), which aims to minimize the expected loss on the training data, has been widely used in conventional statistical machine translation [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012] and deep learning based MT [Gao et al.", "startOffset": 161, "endOffset": 215}, {"referenceID": 2, "context": "Minimum risk training (MRT), which aims to minimize the expected loss on the training data, has been widely used in conventional statistical machine translation [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012] and deep learning based MT [Gao et al., 2014].", "startOffset": 243, "endOffset": 261}, {"referenceID": 9, "context": "where S(x) \u2282 Y(x) is a subset of the full search space, Q(y|x;\u03b8, \u03b1) is a distribution defined on the subspace S(x), and \u03b1 is a hyper-parameter that controls the sharpness of the Q distribution [Och, 2003].", "startOffset": 193, "endOffset": 204}, {"referenceID": 10, "context": "The evaluation metric is case-insensitive BLEU [Papineni et al., 2002] as calculated by the multi-bleu.", "startOffset": 47, "endOffset": 70}, {"referenceID": 14, "context": "For English-French, to compare with the results reported by previous work on neural machine translation [Sutskever et al., 2014; Bahdanau et al., 2015], we used the same subset of the WMT 2014 training corpus that contains 12M sentence pairs with 348M French words and 304M English words.", "startOffset": 104, "endOffset": 151}, {"referenceID": 6, "context": "Moses [Koehn and Hoang, 2007]: a phrase-based SMT system;", "startOffset": 6, "endOffset": 29}, {"referenceID": 13, "context": "Moses uses the parallel corpus to train the phrase-based translation model and the target part to train a 4-gram language model using the SRILM toolkit [Stolcke, 2002].", "startOffset": 152, "endOffset": 167}, {"referenceID": 9, "context": "The log-linear model is trained using the minimum error rate training (MERT) algorithm [Och, 2003].", "startOffset": 87, "endOffset": 98}, {"referenceID": 6, "context": "Moses is a state-ofthe-art phrase-based statistical machine translation system [Koehn and Hoang, 2007] that uses the minimum error rate training (MERT) algorithm for training the log-linear model.", "startOffset": 79, "endOffset": 102}, {"referenceID": 4, "context": "45 Jean et al. [2015] single gated RNN with search 30K 29.", "startOffset": 3, "endOffset": 22}, {"referenceID": 4, "context": "45 Jean et al. [2015] single gated RNN with search 30K 29.97 Luong et al. [2015] single LSTM with 4 layers 40K 29.", "startOffset": 3, "endOffset": 81}, {"referenceID": 4, "context": "45 Jean et al. [2015] single gated RNN with search 30K 29.97 Luong et al. [2015] single LSTM with 4 layers 40K 29.50 Luong et al. [2015] single LSTM with 6 layers 40K 30.", "startOffset": 3, "endOffset": 137}, {"referenceID": 4, "context": "45 Jean et al. [2015] single gated RNN with search 30K 29.97 Luong et al. [2015] single LSTM with 4 layers 40K 29.50 Luong et al. [2015] single LSTM with 6 layers 40K 30.40 Sutskever et al. [2014] single LSTM with 4 layers 80K 30.", "startOffset": 3, "endOffset": 197}, {"referenceID": 4, "context": "[2015] and Jean et al. [2015]. GroundHog-MRT achieves the highest BLEU score in this setting even with a smaller vocabulary size than Luong et al.", "startOffset": 11, "endOffset": 30}, {"referenceID": 4, "context": "[2015] and Jean et al. [2015]. GroundHog-MRT achieves the highest BLEU score in this setting even with a smaller vocabulary size than Luong et al. [2015] and Sutskever et al.", "startOffset": 11, "endOffset": 154}, {"referenceID": 4, "context": "[2015] and Jean et al. [2015]. GroundHog-MRT achieves the highest BLEU score in this setting even with a smaller vocabulary size than Luong et al. [2015] and Sutskever et al. [2014]. 1 Note that our approach does not assume specific architectures and can in principle be applied to any NMT systems.", "startOffset": 11, "endOffset": 182}, {"referenceID": 9, "context": "Our work originates from the minimum risk training algorithms in conventional statistical machine translation [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012].", "startOffset": 110, "endOffset": 164}, {"referenceID": 12, "context": "Our work originates from the minimum risk training algorithms in conventional statistical machine translation [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012].", "startOffset": 110, "endOffset": 164}, {"referenceID": 3, "context": "Our work originates from the minimum risk training algorithms in conventional statistical machine translation [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012].", "startOffset": 110, "endOffset": 164}, {"referenceID": 14, "context": "1We have not adopted the recently introduced techniques such as ensemble of multiple model predictions [Sutskever et al., 2014], increasing vocabulary by importance sampling [Jean et al.", "startOffset": 103, "endOffset": 127}, {"referenceID": 4, "context": ", 2014], increasing vocabulary by importance sampling [Jean et al., 2015], and addressing the rare word problem [Luong et al.", "startOffset": 54, "endOffset": 73}, {"referenceID": 8, "context": ", 2015], and addressing the rare word problem [Luong et al., 2015] to further improve translation performance.", "startOffset": 46, "endOffset": 66}, {"referenceID": 3, "context": "Our work originates from the minimum risk training algorithms in conventional statistical machine translation [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012]. Och [2003] describes a smoothed error count to allow for calculating gradients, which directly inspires us to use a parameter \u03b1 to adjust the smoothness of the objective function.", "startOffset": 146, "endOffset": 177}, {"referenceID": 3, "context": "Our work originates from the minimum risk training algorithms in conventional statistical machine translation [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012]. Och [2003] describes a smoothed error count to allow for calculating gradients, which directly inspires us to use a parameter \u03b1 to adjust the smoothness of the objective function. Smith and Eisner [2006] introduce minimum risk annealing for training log-linear models that is capable of gradually annealing to focus on the 1-best hypothesis.", "startOffset": 146, "endOffset": 370}, {"referenceID": 3, "context": "Our work originates from the minimum risk training algorithms in conventional statistical machine translation [Och, 2003; Smith and Eisner, 2006; He and Deng, 2012]. Och [2003] describes a smoothed error count to allow for calculating gradients, which directly inspires us to use a parameter \u03b1 to adjust the smoothness of the objective function. Smith and Eisner [2006] introduce minimum risk annealing for training log-linear models that is capable of gradually annealing to focus on the 1-best hypothesis. He and Deng [2012] apply minimum risk training to learning phrase translation probabilities.", "startOffset": 146, "endOffset": 527}, {"referenceID": 11, "context": "The Mixed Incremental Cross-Entropy Reinforce (MIXER) algorithm [Ranzato et al., 2015] is in spirit most close to our work.", "startOffset": 64, "endOffset": 86}, {"referenceID": 2, "context": "More recently, Gao et al. [2014] use MRT for learning continuous phrase representations for statistical machine translation.", "startOffset": 15, "endOffset": 33}, {"referenceID": 2, "context": "More recently, Gao et al. [2014] use MRT for learning continuous phrase representations for statistical machine translation. The difference is that they use MRT to optimize a sub-model of statistical machine translation while we are interested in directly optimizing end-to-end neural translation models. The Mixed Incremental Cross-Entropy Reinforce (MIXER) algorithm [Ranzato et al., 2015] is in spirit most close to our work. Building on the REINFORCE algorithm proposed by Willams [1992], MIXER allows for incremental learning and the use of hybrid loss function that combines both REINFORCE and cross-entropy.", "startOffset": 15, "endOffset": 492}, {"referenceID": 2, "context": "More recently, Gao et al. [2014] use MRT for learning continuous phrase representations for statistical machine translation. The difference is that they use MRT to optimize a sub-model of statistical machine translation while we are interested in directly optimizing end-to-end neural translation models. The Mixed Incremental Cross-Entropy Reinforce (MIXER) algorithm [Ranzato et al., 2015] is in spirit most close to our work. Building on the REINFORCE algorithm proposed by Willams [1992], MIXER allows for incremental learning and the use of hybrid loss function that combines both REINFORCE and cross-entropy. The major difference is that Ranzato et al. [2015] leverage reinforcement learning while our work resorts to minimum risk training.", "startOffset": 15, "endOffset": 666}], "year": 2017, "abstractText": "We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to evaluation metrics. Experiments on Chinese-English and EnglishFrench translation show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system.", "creator": "LaTeX with hyperref package"}}}