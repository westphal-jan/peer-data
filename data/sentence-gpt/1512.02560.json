{"id": "1512.02560", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Dec-2015", "title": "Deep Learning for Single and Multi-Session i-Vector Speaker Recognition", "abstract": "The promising performance of Deep Learning (DL) in speech recognition has motivated the use of DL in other speech technology applications such as speaker recognition. Given i-vectors as inputs, the authors proposed an impostor selection algorithm and a universal model adaptation process in a hybrid system based on Deep Belief Networks (DBN) and Deep Neural Networks (DNN) to discriminatively model each target speaker. In order to have more insight into the behavior of DL techniques in both single and multi-session speaker enrollment tasks, some experiments have been carried out in this paper in both scenarios. Additionally, the parameters of the global model, referred to as universal DBN (UDBN), are normalized before adaptation. UDBN normalization facilitates training DNNs specifically with more than one hidden layer. Experiments are performed on the NIST SRE 2006 corpus. It is shown that the proposed impostor selection algorithm and UDBN adaptation process enhance the performance of conventional DNNs 8-20 % and 16-20 % in terms of EER for the single and multi-session tasks, respectively. In both scenarios, the proposed architectures outperform the baseline systems obtaining up to 17 % reduction in EER. While we did not have to take into consideration the underlying dynamics of DL techniques and the design of the models, we did have to consider the possible drawbacks of using DL to help integrate the dynamics of DNNs into speech training tasks and other training tasks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 8 Dec 2015 17:34:49 GMT  (1125kb,D)", "http://arxiv.org/abs/1512.02560v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["omid ghahabi", "javier hernando"], "accepted": false, "id": "1512.02560"}, "pdf": {"name": "1512.02560.pdf", "metadata": {"source": "CRF", "title": "Deep Learning for Single and Multi-Session i-Vector Speaker Recognition", "authors": ["Omid Ghahabi", "Javier Hernando"], "emails": ["omid.ghahabi@upc.edu;", "javier.hernando@upc.edu)."], "sections": [{"heading": null, "text": "Index Terms\u2014Deep Neural Network, Deep Belief Network, Restricted Boltzmann Machine, i-Vector, Speaker Recognition.\nI. INTRODUCTION\nTHE recent compact representation of speech utterancesknown as i-vector [1] has become the state-of-the-art in the text-independent speaker recognition. Given speaker labels for the background data, there are also some post-processing techniques such as Probabilistic Linear Discriminant Analysis (PLDA) [2], [3] to compensate speaker and session variabilities and, therefore, to increase the overall performance of the system.\nOn the other hand, the success of deep learning techniques in speech processing, specifically in speech recognition (e.g., [4]\u2013[8]), has inspired the community to make use of those techniques in speaker recognition as well. Three main commonly used techniques are Restricted Boltzmann Machines (RBM), Deep Belief Networks (DBN), and Deep Neural Networks (DNN). Different combinations of RBMs have been used in [9], [10] to classify i-vectors and in [11] to learn speaker and channel factor subspaces in a PLDA simulation. RBMs and DBNs have been used to extract a compact representation of speech signals from acoustic features [12] and i-vectors [13]. RBMs have also been employed\nO. Ghahabi and J. Hernando are with TALP Research Center, Department of Signal Theory and Communications, Universitat Politecnica de Catalunya - BarcelonaTech, Spain (e-mail: omid.ghahabi@upc.edu; javier.hernando@upc.edu).\nThis work has been funded by the Spanish project SpeechTech4All (TEC2012-38939-C03-02) and the European project CAMOMILE (PCIN2013-067).\nin [14] as a non-linear transformation and dimension reduction stage for GMM supervectors. DBNs have been used in [15] as unsupervised feature extractors and in [16] as speaker feature classifiers. Furthermore, in [17]\u2013[19] they have been integrated in an adaptation process to provide a better initialization for DNNs. DNNs have been utilized to extract Baum-Welch statistics for supervector and i-vector extraction [20]\u2013[23]. DNN bottleneck features are recently employed in the i-vector framework [24], [25]. Additionally, different types of i-vectors represented by DNN architectures are proposed in [26], [27].\nThe main attention of the National Institute of Standard and Technology (NIST) over the last two years to combine i-vectors with new machine learning techniques [28], [29] encouraged the authors to extend the prior works developed in [17], [18]. The authors took advantage of unsupervised learning of DBNs to train a global model referred to as Universal DBN (UDBN) and DNN supervised learning to model each target speaker discriminatively. To provide a balanced training, an impostor selection algorithm and to cope with few training data a UDBN-adaptation process was proposed.\nIn this work, deep architectures with different number of layers are explored for both single and multi-session speaker enrollment tasks. The parameters of the global model are normalized before adaptation. Normalization helps to facilitate training networks specifically where more than one hidden layer is used. The top layer pre-training proposed in [17] is not used in this work. The reason is that it emphasizes on the top layer connection weights and avoids the lower hidden layers to learn enough from the input data. This fact is of more importance where higher number of hidden layers are used. It is supposed, in this work, that there is no labeled background data. Moreover, no unsupervised labeling technique (e.g., [13], [30]) is employed. This work shows how DNN architectures can be more efficient in this particular task. Experimental results performed on the NIST SRE 2006 corpus [31] show that the proposed architectures outperform the baseline systems in both single and multi-session speaker enrollment tasks."}, {"heading": "II. DEEP LEARNING", "text": "Deep Learning (DL) referes to a branch of machine learning techniques which attempts to learn high level features from data. Since 2006 [32], [33], DL has become a new area of research in many applications of machine learning and signal processing. Various deep learning architectures have been used in speech processing (e.g., [7], [8], [34]\u2013[36]). Deep Neural Networks (DNN), Deep Belief networks (DBN), and Restricted Boltzmann Machines (RBM) are three main\nar X\niv :1\n51 2.\n02 56\n0v 1\n[ cs\n.S D\n] 8\nD ec\n2 01\n5\n2 techniques we have used in this work to discriminatively model each target speaker given input i-vectors.\nDNNs are feed-forward neural networks with multiple hidden layers (Fig. 1a). They are trained using discriminative back-propagation algorithms given class labels of input vectors. The training algorithm tries to minimize a loss function between the class labels and the outputs. For classification tasks, cross entropy is often used as the loss function and the softmax is commonly used as the activation function at the output layer [37]. Typically, the parameters of DNNs are initialized with small random numbers. Recently, it has been shown that there are more efficient techniques for parameter initialization [38]\u2013[40]. One of those techniques consists in initializing DNN with DBN parameters, which it is often referred to as unsupervised pre-training or just hybrid DBNDNN [5], [41]. It has empirically been shown that this pretraining stage can set the weights of the network closer to an optimum solution than random initialization [38]\u2013[40].\nDBNs are generative models with multiple hidden layers of stochastic units above a visible layer which represents a data vector (Fig. 1b). The top two layers are undirected and the other layers have top-down directed connections to generate the data. There is an efficient greedy layer wised algorithm to train DBN parameters [33]. In this case, DBN is divided in two-layer sub-networks and each one is treated as an RBM (Fig. 1c). When the first RBM corresponding to the visible units is trained, its parameters are frozen and the outputs are given to the RBM above as input vectors. This process is repeated until the top two layers are reached.\nRBMs are generative models constructed from two undirected layers of stochastic hidden and visible units (Fig. 2a). RBM training is based on maximum likelihood criterion using the stochastic gradient descent algorithm [5], [33]. The gradient is estimated by an approximated version of the Contrastive Divergence (CD) algorithm which is called CD-1 [32], [33]. As it is shown in Fig. 2b, CD-1 consists of three steps. At first, hidden states (h) are computed given visible unit values (v). Secondly, given h, v is reconstructed. Thirdly, hidden unit values are computed given the reconstructed v. Finally, the change of connection weights is given as follows,\n\u2206wij \u2248 \u2212\u03b7 (\u3008vihj\u3009data \u2212 \u3008vihj\u3009recon) (1)\nwhere \u03b7 is the learning rate, wij represents the weight between the visible unit i and the hidden unit j, and \u3008.\u3009data and \u3008.\u3009recon denote the expectations when the hidden state values are driven from the input visible data and the reconstructed data, respectively. The biases are updated in a similar way. More theoretical and practical details can be found in [32], [33], [42]. The whole training algorithm is given in [14].\nIn all of these networks, it is possible to update the parameters after processing each training example, but it is often more efficient to divide the whole input data (batch) into smaller size batches (minibatch) and to update the parameters by averaging the gradients over each minibatch. The parameter updating procedure is repeated when the whole available input data are processed. Each iteration is called an epoch.\n(a) (b) (c)\nFig. 1: (a) DNN ,(b) DBN , and (c) DBN training/DNN pre-training.\n(a) (b)\nFig. 2: (a) RBM and (b) RBM training."}, {"heading": "III. DEEP LEARNING FOR I-VECTORS", "text": "The success of the i-vector approach in speaker recognition and DL techniques in speech processing applications has encouraged the research community to combine those techniques for speaker recognition [17]\u2013[19], [21], [22], [24], [27], [43]. Two kinds of combination can be considered. DL techniques can be used in the i-vector extraction process [21], [22], [24], [27], [43], or applied after i-vector computation [9]\u2013[11], [17]\u2013 [19]. In order to have more insight into the behavior of DL techniques on i-vectors, in this work the authors extend the preliminary study developed in [17], [18].\nAn i-vector [1] is a low rank vector, typically between 400 and 600, representing a speech utterance. Feature vectors of a speech signal can be modeled by a set of Gaussian Mixtures (GMM) adapted from a Universal Background Model (UBM). The mean vectors of the adapted GMM are stacked to build the supervector m. The supervector can be further modeled as follows,\nm = mu +T\u03c9 (2)\nwhere mu is the speaker- and session-independent mean supervector typically from UBM, T is the total variability matrix, and \u03c9 is a hidden variable. The mean of the posterior distribution of \u03c9 is referred to as i-vector. This posterior distribution is conditioned on the Baum-Welch statistics of the given speech utterance. The T matrix is trained using the Expectation-Maximization (EM) algorithm given the centralized Baum-Welch statistics from background speech utterances. More details can be found in [1].\nIn the state-of-the-art speaker recognition systems, all the speech utterances, including background, train and test, are converted to i-vectors. Background and train i-vectors are typically called impostor and target i-vectors, respectively. The main objective in this work is to train a two-class DNN for each target speaker given target and impostor i-vectors. In the single-session target speaker enrollment task, only one i-vector is available, meanwhile in the multi-session one, several ivectors are available per each target speaker. DNNs and in\n3\ngeneral neural networks usually need a large number of input samples to be trained, and deeper networks require more input data. The lack of enough target samples for training each DNN yields two main problems. Firstly, the number of target and impostor samples will be highly unbalanced, one or some few target samples against thousands of impostor samples. Learning from such unbalanced data will result in biased DNNs towards the majority class. In other words, DNNs will have a much higher prediction accuracy over the majority class. Secondly, as we need to decrease the number of impostor samples to solve the first problem, the total number of samples for training the network will be very few. A network trained with such few data is highly probable to be overfitted.\nFig. 3 shows the block diagram of the proposed approach to discriminatively model target speakers. In this block diagram, we propose two main contributions to tackle the above problems. The balanced training block attempts to decrease the number of impostor samples and, in the contrary, to increase the number of target ones in a reasonable and effective way. The most informative impostor samples for target speakers are first selected by the proposed impostor selection algorithm. After that, the selected impostors are clustered and the cluster centroids are considered as final impostor samples for each target speaker model. Impostor centroids and target samples are then divided equally into minibatches to provide balanced impostor and target data in each minibatch.\nOn the other hand, the DBN adaptation block is proposed to compensate the lack of enough input data. As DBN training does not need any labeled data, the whole background ivectors are used to build a global model, which is referred to as Universal DBN (UDBN). The parameters of UDBN are then adapted to the balanced data obtained for each target speaker. It is worth noting that as the minimum divergence training algorithm [44] is used in the i-vector extraction process, i-vectors will have a standard normal distribution N (0, 1). Therefore, they will be compatible with Gaussian RBMs (GRBM) in DBN architectures, which assume a zeromean unit-variance normal distribution for inputs. At the end, given target/impostor labels, adapted DBN, and balanced data, a DNN is discriminatively trained for each target speaker. In the two following sections, we will describe these two main\ncontributions in more details."}, {"heading": "IV. BALANCED TRAINING", "text": "As speaker models in the proposed method will be finally discriminative, they need both positive and negative data as inputs. However, the problem is that the amount of positive and negative data are highly unbalanced in this case which yields biasing towards the majority class. Some of the most straightforward ways to deal with unbalanced data problem are explored in [45]\u2013[47] [48], [49]. One of the simplest commonly used method is data sampling. The data of the majority class is undersampled and, in the contrary, the data of the minority class is oversampled. The effectiveness of these techniques is highly dependent on the data structure.\nIn the proposed approach in Fig. 3, the amount of impostors is decreased in two steps, namely selection and clustering. On the other hand, the amount of target samples is increased by either replication or combination. After that, balanced target and impostor samples are distributed equally among minibatches.\nA. Impostor Selection and Clustering\nThe objective is to decrease the large number of negative samples in a reasonable way. Our proposal has two main steps. First, only those impostor i-vectors which are more informative for the training dataset are selected. Informative impostor means, in this case, the impostor which is not only representative to a given target but also is close statistically to other targets in the dataset. For a real application, it makes sense to select those impostors who are globally close to all enrolled speakers. When the target speakers are changed, the selected impostors can be reselected according to the new target dataset. Second, as the number of selected impostor samples is still high in comparison to the number of target ones, they are clustered by the k-means algorithm using the cosine distance criterion. The centroids of the clusters are then used as the final negative samples.\nThe selection method is inspired from a data-driven background data selection technique proposed in [50]. In that technique, given all available impostors a Support Vector Machine\n(SVM) classifier is trained for each target speaker. The number of times each impostor is selected as a support vector, in all training SVM models, is called impostor support vector frequency [50]. Impostor examples with higher frequencies are then selected as the refined impostor dataset. However, SVM training for each target speaker would be computationally costly. Moreover, as our final discriminative models will be DNNs, it would not be worth to employ this technique as such. Instead, we have proposed to use cosine distance as an efficient and a fast criterion for comparing i-vectors. We compare each target i-vector with all impostor ones in the background dataset. Those N impostors which are close to each target i-vector are treated like support vectors in [50]. Then the \u03ba impostors with higher frequencies are selected as the most informative ones. The parameters N and \u03ba are determined experimentally. The whole algorithm is shown in Fig. 4 and can be summarized as follows,\n1) Set impostor frequencies fm = 0, 1 \u2264 m \u2264M 2) For each target i-vector \u03c9t, 1 \u2264 t \u2264 T\na) Compute cosine (\u03c9t,\u03bdm), 1 \u2264 m \u2264M b) Select the N impostors with the highest scores c) For the selected impostors fm \u2190 fm + 1\n3) Sort impostors descendingly based on their fm 4) Select the first \u03ba impostors as the final ones.\nwhere cosine (\u03c9t,\u03bdm) is the cosine score between target ivector \u03c9t and the impostor i-vector \u03bdm in the background dataset, M and T are the number of impostor and target speakers, respectively. It is worth noting that in the case of multi-session target enrollment, the average of the i-vectors available per each target speaker will be considered in the above selection algorithm."}, {"heading": "B. Target Replication or Combination", "text": "In order to balance positive and negative samples, the number of target samples is increased as many as the number of impostor cluster centroids obtained in section IV-A. In the single session enrollment task, the i-vector of each target speaker is simply replicated as many as the number of cluster centroids. Replicated target i-vectors will not act exactly the same as each other in the pre-training process of DNNs due to the sampling noise created in RBM training [42]. Moreover, in\nboth adaptation and supervised learning stages the replicated versions make the target and impostor classes having the same weights when the network parameters are being updated. In multi-session task, the available i-vectors of each target speaker can be combined, i.e., the average of every n i-vectors is considered as a new target i-vector.\nOnce the number of positive and negative samples are balanced, they are distributed equally among minibatches. In other words, each minibatch contains the same number of impostors and targets. If target samples in the multi-session task are not combined, the same target samples but different impostor ones are shown to the network in each minibatch (Fig. 5). The optimum numbers of impostor clusters and minibatches will be determined experimentally in section. VI."}, {"heading": "V. UNIVERSAL DBN AND ADAPTATION", "text": "Unlike DNNs, which need labeled data for training, DBNs do not necessarily need such labeled data as inputs. Hence, they can be used for unsupervised training of a global model referred to as Universal DBN (UDBN) [17]. UDBN is trained by feeding background i-vectors from different speakers. The training procedure is carried out layer by layer using RBMs as described in Sec. II.\nIt is shown that pre-training techniques can initialize DNNs better than simply random numbers [38]\u2013[40]. However, when a few numbers of input samples are available, just pre-training may not be enough to achieve a good model. In this case, we have proposed in [17] to adapt UDBN parameters to the balanced data obtained for each target speaker. Adaptation is carried out by pre-training each network initialized by UDBN parameters. In order to avoid overfitting, only a few iterations will be used for adaptation. It is supposed that UDBN can learn both speaker and channel variabilities from the background data. Therefore, UDBN will provide a more meaningful initial point for each target model than a simple random initialization. The study in [39] has shown that pre-training is robust with respect to the random initialization seed. The use of UDBN parameters makes target models almost independent from the random seeds.\nIn contrast to [17], [18], in this work we normalize the UDBN parameters before adaptation. Normalization is carried out by simply scaling down the maximum absolute value of connection weights to 0.01. In this way, connection weights will have a dynamic range similar to that typically used\n5 Speaker 1\nSpeaker 2\nUDBN\nHidden Units\nV is ib le U ni ts\n100 200 300 400 500\n50\n100 150 200 250 300 350 400\n\u22120.01\n\u22120.005\n0\n0.005\n0.01\nHidden Units\nV is ib le U ni ts\n100 200 300 400 500\n50\n100 150 200 250 300 350 400 \u22120.01 \u22120.005 0 0.005\n0.01\nHidden Units\nV is ib le U ni ts\n100 200 300 400 500\n50\n100 150 200 250 300 350 400\n\u22125\n0\n5\n10 x 10\u22123\nAdapt\nFig. 6: Comparison of the adapted connection weights between the visible and the first hidden units for two different speakers.\nfor random initialization. Correspondingly, bias terms are multiplied by 0.01 to be closer to zero. This is because the bias terms are usually set to zero when the connection weights are randomly initialized. UDBN parameter normalization facilitates training the networks specifically where more than one hidden layer is used. In this way, the same learning rates and the number of epochs tuned for random initialized DNNs can also be used for adapted DNNs in the supervised learning stage.\nFig. 6 compares the adapted UDBN connection weights between the input layer and the first hidden one for two different speakers. As it can be seen in this figure, adaptation sets speaker-specific initial points for each model.\nOnce the adaptation process is completed, a DNN is initialized with the adapted DBN parameters for each target speaker. Then the minibatch stochastic gradient descent backpropagation is carried out for fine-tuning. The softmax and the logistic sigmoid will be the activation functions of the top label layer and the other hidden layers, respectively.\nIf the input labels in the training phase are chosen as (1, 0) and (0, 1) for target and impostor i-vectors, respectively, the final output score in the testing phase will be computed in a Log Likelihood Ratio (LLR) form as follows,\nLLR = log(o1)\u2212 log(o2) (3)\nwhere o1 and o2 represent, respectively, the output of the first and the second units of the top layer. LLR computation helps to gaussianize the true and false score distributions which can be useful for score fusion. In addition, to make the fine-tuning process more efficient a momentum factor is used to smooth out the updates, and the weight decay regularization is used to penalize large weights. The top layer pre-training proposed in [17] is not used in this work. The reason is that it gives the emphasis on the top layer connection weights and avoids the lower layers, closer to the input, to learn enough from the input data. This fact will be more important when higher number of hidden layers are used."}, {"heading": "VI. EXPERIMENTAL RESULTS", "text": "The block-diagram of Fig. 3 has been implemented for both single and multi-session speaker verification tasks. The effectiveness of two main contributions proposed in the figure will be shown in this section."}, {"heading": "A. Baseline and Dataset", "text": "All the experiments in this work are carried out on the NIST SRE 2006 evaluation [31]. In both training and testing phases signals have approximately two-minute total speech duration. The whole core condition has been used for the single session task, in which there are 816 target models and 51,068 trials. On the other hand, 8 conversation sides are available per each target speaker in the multi-session task and the protocol contains 699 target models and 31,080 trials. NIST SRE 2004 and 2005 are used as the background data. It is worth noting that in the case of NIST 2005 only the speech signals of those speakers who do not appear in NIST SRE 2006 are used.\nFrequency Filtering (FF) features [51] are used in the experiments. FFs, like MFCCs, are decorrelated version of log Filter Bank Energies (FBE) [51]. It has been shown that FF features achieve a performance equal to or better than MFCCs [51]. Features are extracted every 10 ms using a 30 ms Hamming window. The number of static FF features is 16 and together with delta FF and delta log energy, 33-dimensional feature vectors are produced. Before feature extraction, speech signals are subjected to an energy-based silence removal process.\nThe gender-independent UBM is represented as a diagonal covariance, 512-component GMM. All the i-vectors are 400- dimensional. The i-vector extraction process is carried out using ALIZE open source software [52]. UBM and T matrix are trained using more than 6,000 speech signals collected from NIST SRE 2004 and 2005. Performance is evaluated using the Detection Error Tradeoff (DET) curves, the Equal Error Rate (EER), and the minimum of the Decision Cost Function (minDCF) calculated using CM = 10, CFA = 1, PT = 0.01.\nIt is supposed in all experiments that there is no labeled background data and, therefore, no channel compensation technique is used. The aim of the work is to show how DNN architectures can be more efficient in this particular task. In the baseline systems, whitened and length normalized i-vectors are classified using cosine distance. In the multi-session task, the average of the available i-vectors per each target speaker is first length normalized and then compared with the test i-vector using cosine distance. In DNN experiments, raw ivectors without whitening and length normalization are used."}, {"heading": "B. Single-Session Experiments", "text": "At first, we need to choose the size of DNNs in terms of the hidden layer size and the number of layers. The number of hidden units in each layer is taken as a power of 2 greater than the input layer size. Since the input layer size is 400, the hidden layer size is chosen as 512. We explore DNNs with up to three hidden layers in all experiments. We do not go further than three layers because the computational complexity is increased considerably and also no significant gain is observed.\n6 500 1000 2000 3000 4000 5000 8 8.5 9 9.5\nNumber of Selected Impostors (\u03ba)\nE E\nR (\n% )\nDNN\u22121L\nN = 10\nN = 50\nN = 100\n(a)\n200 500 1000 1500 2000 2500 3000\n7.1\n7.2\n7.3\n7.4\n7.5\n7.6\nNumber of Selected Impostors (\u03ba)\nE E\nR (\n% )\nDNN\u22122L\nN = 10\nN = 50\nN = 100\n(b)\nFig. 7: Determination of the parameters of the proposed impostor selection algorithm for (a) 1 hidden layer and (b) 2 hidden layer DNNs. N and \u03ba are, respectively, the number of local and global nearest impostor i-vectors to the target i-vectors.\nAs it is shown in Fig. 3, the first step to train DNNs is to balance the number of target and impostor input ivectors in each minibatch. The number of minibatches and the number of impostor centroids are set experimentally to 3 and 12, respectively. Each minibatch will include four impostor centroids and four replicated target samples.\nAfter that, we train a DNN for each target speaker using the whole impostor background data and random initialization. In this case, the whole background i-vectors are clustered using the k-means algorithm and the centroids are considered as impostor samples. In this work, we use the uniform distribution U (0, 0.01) for random initialization as the experimental results showed that it achieves slightly better performance than the normal distributionN (0, 0.01) used in the prior work [17]. We tune the parameters of the networks and keep them fixed in all other experiments. One, two, and three hidden layer DNNs are trained with the learning rates of 0.001, 0.005, and 0.08 and with the number of epochs of 30, 100, and 500, respectively. Momentum and weight decay are set, respectively, to 0.9 and 0.0012 for all DNNs. In the following, we show the effect of each contribution proposed in Sec. III.\nBackground i-vectors are extracted from the same speech signals used for training UBM and T matrix. The two parameters N and \u03ba, the number of local and global selected impostors in the proposed impostor selection method, need to\nbe determined experimentally. Figures. 7a and 7b illustrate the variability of EER in terms of these two parameters for one and two hidden layer DNNs, respectively. The same behavior can be observed for minDCF curves. DNN with three hidden layers act similar to two-layer ones. DNN examples shown in these two figures are initialized randomly. As it can be seen, DNNs with more than one hidden layer tend to achieve better results with fewer number of global selected impostors in comparison to networks with only one hidden layer. In all cases, the best performance is obtained by selecting fewer local impostors. Hence, for all DNNs N is set to 10 and \u03ba is set to 2,000, 300, and 500 for one, two, and three layer DNNs, respectively.\nUDBN is trained with the same background i-vectors used for impostor selection. As the input i-vectors are real-valued, a Gaussian-Bernoulli RBM (GRBM) [5], [42] is used to train the connection weights between the visible and the first hidden layer units. The rest of the connection weights are trained with Bernoulli-Bernoulli RBMs. The learning rate and the number of epochs are set to 0.014 and 200 for GRBM, and to 0.06 and 120 for the rest of RBMs in UDBN, respectively. Momentum and weight decay are set, respectively, to 0.9 and 0.0002 for all RBMs. Unlike in [17] and [18] where the authors used the UDBN parameters as such, in this work we normalize the connection weights so that the maximum absolute value is 0.01. This is the maximum value of the random numbers we used to initialize DNNs. Additionally, we scale down the bias terms by 0.01. Normalization helps to facilitate training DNNs with higher number of layers. Moreover, we can use the same learning rates we tuned for random initialized DNNs.\nExperimental results showed that the most part of the improvement due to the adaptation process comes from the adaptation of the connection weights between the input layer and the first hidden layer for all DNNs. The adaptation of the other layers has no positive effect or it improves the performance slightly. We adapted the networks up to two layers. The learning rate of adaptation is set to 0.001 and 0.0001 for the first and the second layers, respectively. The number of epochs for the first layer is set to 10, 20, and 15 for DNN-1L, -2L, and -3L, respectively. DNN-1L stands for a one hidden layer DNN. The number of epochs for the second layer is set, respectively, to 15 and 20 for DNN-2L and DNN3L.\nTable I summarizes the effect of each proposed contribution. In the first row of the table, DNNs are initialized randomly and the impostor cluster centroids are obtained on the whole background data. As it can be seen in this row, adding more hidden layers to the network improves the performance. However, they still work worse than the baseline system in which ivectors are classified using cosine distance. EER and minDCF for the baseline system are 7.18% and 0.0324, respectively. Impostor selection improves the performance to a great extent for all the networks. However, the biggest improvement due to the adaptation process is observed in DNNs with one hidden layer. The best results are obtained using both impostor selection and adaptation techniques which show an 8-20% and 10-17% relative improvements in terms of EER and minDCF, respectively, in comparison to the conventional DNNs. The\n7\nlast row of the table shows the fusion of DNN systems with the baseline in the score level. Scores of each system are first mean and variance normalized and then summed. Although DNNs with one hidden layer yield slightly better results, DNNs with more layers provide complementary information to the baseline system. This confirms the theoretical hypothesis which states that more hidden layers more abstractions from the input data. The fusion of baseline and DNNs with three hidden layers achieves the best results corresponding to an 8% relative improvement for both EER and minDCF in comparison to the baseline system. We have also combined the scores of DNNs with different number of hidden layers, but no gain is observed.\nThe DET curve in Fig. 8 compares the best systems in all operating points. As it is shown in this figure, DNNs with one hidden layer achieve better results than the baseline and the combination of 3-layer DNNs with the baseline works the best in all operating points."}, {"heading": "C. Multi-Session Experiments", "text": "The same configuration used for the single session task is also applied for the multi-session one. The number of minibatches is set to 3. In each minibatch, all 8 target i-vectors accompanying with 8 impostor cluster centroids are shown to the network. Therefore, the size of each minibatch and the total number of impostor clusters will be 16 and 24, respectively. As the combination of the i-vectors of each target speaker did not help the training of the networks, we replicated the target i-vectors in every minibatch as it was shown in Fig. 5.\nWe start training the networks with the same parameters tuned for the single session experiments. However, as the target i-vector samples per each training speaker are different from each other in the multi-session task, the number of epochs and the learning rates need to be slightly re-tuned. We have set the learning rates to 0.001, 0.01, and 0.08 and the number of epochs to 30, 100, and 500 for one layer to three layer DNNs, respectively.\nResults are summarized in Table II. Around 12% relative improvements are achieved in all DNNs employing impostor selection technique proposed in this work. With the same parameters obtained for the single session task, we re-selected the impostors for the new multi-session data set. The adaptation process improves the performance up to 8%. As in the single session task, adaptation is more effective in the one hidden layer DNNs. For all the networks, only the parameters of the first hidden layer are adapted because no more improvement was observed adapting the other layers. Adaptation is carried out by the learning rate of 0.001 for all DNNs and the number of epochs of 10, 10, and 25 for DNNs with one to three layers, respectively. The best results are obtained with three layer DNNs when the two proposed techniques are used together. It shows more than 20% improvement of EER and minDCF in comparison to the conventional three layer DNNs. Compared to the baseline system in which EER and minDCF are obtained 4.2% and 0.0191, respectively, the proposed three hidden layer DNNs achieve more than 17% and 10% improvements in terms of EER and minDCF, respectively. Fusion with the baseline system at the score level improves the results in all cases. Fusion is effective mostly on the minDCF which increase the improvement from 10% to 15%.\nFig. 9 compares the DET curves of the best results obtained in table II. As it can be seen in this figure, DNN-3L outperforms clearly the baseline and the DNN-1L in all operating points. However, fusion with the baseline system improves the performance only for the operating points with higher false alarm probabilities."}, {"heading": "VII. CONCLUSION", "text": "A hybrid system based on Deep Belief Networks (DBN) and Deep Neural Networks (DNN) has been proposed in this work for speaker recognition to discriminatively model target speakers with available i-vectors. In order to have more insight into the behavior of these techniques in both single and multi-session speaker enrollment tasks, the experiments are carried out in both scenarios. Two main contributions have been proposed to make DNNs more efficient in this\n8\nparticular task. Firstly, the most informative impostors have been selected and clustered to provide a balanced training. Secondly, each DNN has been initialized with the speaker specific parameters adapted from a global model, which has been referred to as Universal DBN (UDBN). The parameters of UDBN are normalized before adaptation, which facilitates the training of DNNs specifically with more than one hidden layer. Experiments are performed on the NIST SRE 2006 corpus. It was shown that in both scenarios the proposed architectures outperform the baseline systems with up to 17% and 10% in terms of EER and minDCF, respectively."}], "references": [{"title": "Front-end factor analysis for speaker verification", "author": ["N. Dehak", "P. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788\u2013798, May 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Probabilistic linear discriminant analysis for inferences about identity", "author": ["S.J.D. Prince", "J.H. Elder"], "venue": "IEEE 11th International Conference on Computer Vision, 2007. ICCV 2007, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Bayesian speaker verification with heavy tailed priors", "author": ["P. Kenny"], "venue": "Speaker and Language Recognition Workshop (IEEE Odyssey), 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Investigation of full-sequence training of deep belief networks for speech recognition", "author": ["A. Mohamed", "D. Yu", "L. Deng"], "venue": "Proc. Interspeech, 2010, p. 28462849.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Context-dependent pretrained deep neural networks for large-vocabulary speech recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 30\u201342, Jan. 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Acoustic modeling using deep belief networks", "author": ["A. Mohamed", "G.E. Dahl", "G. Hinton"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 14\u201322, Jan. 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath"], "venue": "IEEE Signal Processing Magazine, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Context Dependent Phone Models For LSTM RNN Acoustic Modelling", "author": ["A. Senior", "H. Sak", "I. Shafran"], "venue": "Proc. ICASSP, 2015, pp. 4585\u20134589.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Preliminary investigation of boltzmann machine classifiers for speaker recognition", "author": ["T. Stafylakis", "P. Kenny", "M. Senoussaoui", "P. Dumouchel"], "venue": "Proc. Odyssey, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "First attempt of boltzmann machines for speaker verification", "author": ["M. Senoussaoui", "N. Dehak", "P. Kenny", "R. Dehak", "P. Dumouchel"], "venue": "Proc. Odyssey, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "PLDA using gaussian restricted boltzmann machines with application to speaker verification", "author": ["T. Stafylakis", "P. Kenny", "M. Senoussaoui", "P. Dumouchel"], "venue": "Proc. Interspeech, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Speaker recognition by means of deep belief networks", "author": ["V. Vasilakakis", "S. Cumani", "P. Laface"], "venue": "Biometric Technologies in Forensic Science, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "STC speaker recognition system for the NIST i-vector challenge", "author": ["S. Novoselov", "T. Pekhovsky", "K. Simonchik"], "venue": "Odyssey: The Speaker and Language Recognition Workshop, 2014, pp. 231\u2013240.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Restricted boltzmann machine supervectors for speaker recognition", "author": ["O. Ghahabi", "J. Hernando"], "venue": "Proc. ICASSP, 2015, pp. 4804\u20134808.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["H. Lee", "Y. Largman", "P. Pham", "A. Ng"], "venue": "Advances in neural information processing systems, vol. 22, pp. 10961104, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Feature classification by means of deep belief networks for speaker recognition", "author": ["P. Safari", "O. Ghahabi", "J. Hernando"], "venue": "Proc. EUSIPCO, 2015, pp. 2162\u20132166.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep belief networks for i-vector based speaker recognition", "author": ["O. Ghahabi", "J. Hernando"], "venue": "Proc. ICASSP, May 2014, pp. 1700\u20131704.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "i-vector modeling with deep belief networks for multi-session speaker recognition", "author": ["O. Ghahabi", "J. Hernando"], "venue": "Proc. Odyssey, 2014, pp. 305\u2013310.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Global impostor selection for DBNs in multi-session i-vector speaker recognition", "author": ["O. Ghahabi", "J. Hernando"], "venue": "Advances in Speech and Language Technologies for Iberian Languages, Lecture Notes in Artificial Intelligence. Springer International Publishing, Nov. 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Using deep belief networks for vector-based speaker recognition", "author": ["W.M. Campbell"], "venue": "Proc. Interspeech, 2014, pp. 676\u2013680.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "A novel scheme for speaker recognition using a phonetically-aware deep neural network", "author": ["Y. Lei", "N. Scheffer", "L. Ferre", "M. Mclaren"], "venue": "Proc. ICASSP, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks for extracting baum-welch statistics for speaker recognition", "author": ["P. Kenny", "V. Gupta", "T. Stafylakis", "P. Ouellet", "J. Alam"], "venue": "Proc. Odyssey, 2014, pp. 293\u2013298.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving speaker recognition performance in the domain adaptation challenge using deep neural networks", "author": ["D. Garcia-Romero", "Xiaohui Zhang", "A. McCree", "D. Povey"], "venue": "2014 IEEE Spoken Language Technology Workshop (SLT), Dec. 2014, pp. 378\u2013383.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Advances In Deep Neural Network Approaches To Speaker Recognition", "author": ["M. Mclaren", "Y. Lei", "L. Ferre"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Neural Network Approaches to Speaker and Language Recognition", "author": ["F. Richardson", "D. Reynolds", "N. Dehak"], "venue": "IEEE Signal Processing Letters, vol. 22, no. 10, pp. 1671\u20131675, Oct. 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks for small footprint text-dependent speaker verification", "author": ["E. Variani", "Xin Lei", "E. McDermott", "I. Lopez Moreno", "J. Gonzalez- Dominguez"], "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 2014, pp. 4052\u20134056.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep feature for text-dependent speaker verification", "author": ["Yuan Liu", "Yanmin Qian", "Nanxin Chen", "Tianfan Fu", "Ya Zhang", "Kai Yu"], "venue": "Speech Communication, vol. 73, pp. 1\u201313, Oct. 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical speaker clustering methods for the nist i-vector challenge", "author": ["E. Khoury", "L. El Shafey", "M. Ferras", "S. Marcel"], "venue": "Odyssey: The Speaker and Language Recognition Workshop, 2014, pp. 254\u2013259.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, July 2006.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y-W. Teh"], "venue": "Neural Computation, vol. 18, no. 7, pp. 1527\u20131554, May 2006.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "Modeling spectral envelopes using restricted boltzmann machines and deep belief networks for statistical parametric speech synthesis", "author": ["Z-H. Ling", "L. Deng", "D. Yu"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 21, no. 10, pp. 2129\u20132139, 2013.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep belief networks based voice activity detection", "author": ["X-L. Zhang", "J. Wu"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 21, no. 4, pp. 697\u2013710, 2013.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep Convolutional Neural Networks for Large-scale Speech Tasks", "author": ["Tara N. Sainath", "Brian Kingsbury", "George Saon", "Hagen Soltau", "Abdelrahman Mohamed", "George Dahl", "Bhuvana Ramabhadran"], "venue": "Neural Networks, vol. 64, pp. 39\u201348, Apr. 2015.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Learning for Acoustic Modeling in Parametric Speech Generation: A systematic review of existing techniques and future trends", "author": ["Zhen-Hua Ling", "Shi-Yin Kang", "Heiga Zen", "A. Senior", "M. Schuster", "Xiao- Jun Qian", "H.M. Meng", "Li Deng"], "venue": "IEEE Signal Processing Magazine, vol. 32, no. 3, pp. 35\u201352, May 2015.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Exploring Strategies for Training Deep Neural Networks", "author": ["H. Larochelle", "Y. Bengio", "J. Louradour", "P. Lamblin"], "venue": "Journal of Machine Learning Research, vol. 10, pp. 1\u201340, June 2009.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training", "author": ["E. Dumitru", "P. Manzagol", "Y. Bengio", "S. Bengio", "P. Vincent"], "venue": "The Twelfth International Conference on Artificial Intelligence and Statistics (AIST ATS09), pp. 153\u2013160.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 0}, {"title": "Why Does Unsupervised Pre-training Help Deep Learning", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P. Manzagol", "P. Vincent", "S. Bengio"], "venue": "J. Mach. Learn. Res., vol. 11, pp. 625\u2013660, Mar. 2010.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "A practical guide to training restricted boltzmann machines", "author": ["G.E. Hinton"], "venue": "Neural Networks: Tricks of the Trade, number 7700 in Lecture Notes in Computer Science, pp. 599\u2013619. Springer Berlin Heidelberg, Jan. 2012.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "A Unified Deep Neural Network for Speaker and Language Recognition", "author": ["R. Fred", "R. Douglas", "N. Dehak"], "venue": "arXiv:1504.00923 [cs, stat], 2015, arXiv: 1504.00923.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "A study of interspeaker variability in speaker verification", "author": ["P. Kenny", "P. Ouellet", "N. Dehak", "V. Gupta", "P. Dumouchel"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 16, no. 5, pp. 980\u2013988, July 2008.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning from imbalanced data", "author": ["H. He", "E.A. Garcia"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 21, no. 9, pp. 1263\u20131284, Sept. 2009.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2009}, {"title": "Cost-sensitive learning methods for imbalanced data", "author": ["N. Thai-Nghe", "Z. Gantner", "L. Schmidt-Thieme"], "venue": "The 2010 International Joint Conference on Neural Networks (IJCNN), July 2010, pp. 1\u20138.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Supervised neural network modeling: An empirical investigation into learning from imbalanced data with labeling errors", "author": ["T.M. Khoshgoftaar", "J. Van Hulse", "A. Napolitano"], "venue": "IEEE Transactions on Neural Networks, vol. 21, no. 5, pp. 813\u2013830, May 2010.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}, {"title": "An insight into classification with imbalanced data: Empirical results and current trends on using data intrinsic characteristics", "author": ["V. Lpez", "A. Fernndez", "S. Garca", "V. Palade", "F. Herrera"], "venue": "Information Sciences, vol. 250, pp. 113\u2013141, Nov. 2013.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "MWMOTE-majority weighted minority oversampling technique for imbalanced data set learning", "author": ["S. Barua", "M.M. Islam", "Xin Yao", "K. Murase"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 26, no. 2, pp. 405\u2013425, Feb. 2014.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Data-driven background dataset selection for SVM-based speaker verification", "author": ["M. McLaren", "R. Vogt", "B. Baker", "S. Sridharan"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 18, no. 6, pp. 1496\u20131506, Aug. 2010.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2010}, {"title": "Time and frequency filtering of filter-bank energies for robust HMM speech recognition", "author": ["C. Nadeu", "D. Macho", "J. Hernando"], "venue": "Speech Communication, vol. 34, no. 12, pp. 93\u2013114, Apr. 2001.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "THE recent compact representation of speech utterances known as i-vector [1] has become the state-of-the-art in the text-independent speaker recognition.", "startOffset": 73, "endOffset": 76}, {"referenceID": 1, "context": "Given speaker labels for the background data, there are also some post-processing techniques such as Probabilistic Linear Discriminant Analysis (PLDA) [2], [3] to compensate speaker and session variabilities and, therefore, to increase the overall performance of the system.", "startOffset": 151, "endOffset": 154}, {"referenceID": 2, "context": "Given speaker labels for the background data, there are also some post-processing techniques such as Probabilistic Linear Discriminant Analysis (PLDA) [2], [3] to compensate speaker and session variabilities and, therefore, to increase the overall performance of the system.", "startOffset": 156, "endOffset": 159}, {"referenceID": 3, "context": ", [4]\u2013[8]), has inspired the community to make use of those techniques in speaker recognition as well.", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": ", [4]\u2013[8]), has inspired the community to make use of those techniques in speaker recognition as well.", "startOffset": 6, "endOffset": 9}, {"referenceID": 8, "context": "Different combinations of RBMs have been used in [9], [10] to classify i-vectors and in [11] to learn speaker and channel factor subspaces in a PLDA simulation.", "startOffset": 49, "endOffset": 52}, {"referenceID": 9, "context": "Different combinations of RBMs have been used in [9], [10] to classify i-vectors and in [11] to learn speaker and channel factor subspaces in a PLDA simulation.", "startOffset": 54, "endOffset": 58}, {"referenceID": 10, "context": "Different combinations of RBMs have been used in [9], [10] to classify i-vectors and in [11] to learn speaker and channel factor subspaces in a PLDA simulation.", "startOffset": 88, "endOffset": 92}, {"referenceID": 11, "context": "RBMs and DBNs have been used to extract a compact representation of speech signals from acoustic features [12] and i-vectors [13].", "startOffset": 106, "endOffset": 110}, {"referenceID": 12, "context": "RBMs and DBNs have been used to extract a compact representation of speech signals from acoustic features [12] and i-vectors [13].", "startOffset": 125, "endOffset": 129}, {"referenceID": 13, "context": "in [14] as a non-linear transformation and dimension reduction stage for GMM supervectors.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "DBNs have been used in [15] as unsupervised feature extractors and in [16] as speaker feature classifiers.", "startOffset": 23, "endOffset": 27}, {"referenceID": 15, "context": "DBNs have been used in [15] as unsupervised feature extractors and in [16] as speaker feature classifiers.", "startOffset": 70, "endOffset": 74}, {"referenceID": 16, "context": "Furthermore, in [17]\u2013[19] they have been integrated in an adaptation process to provide a better initialization for DNNs.", "startOffset": 16, "endOffset": 20}, {"referenceID": 18, "context": "Furthermore, in [17]\u2013[19] they have been integrated in an adaptation process to provide a better initialization for DNNs.", "startOffset": 21, "endOffset": 25}, {"referenceID": 19, "context": "DNNs have been utilized to extract Baum-Welch statistics for supervector and i-vector extraction [20]\u2013[23].", "startOffset": 97, "endOffset": 101}, {"referenceID": 22, "context": "DNNs have been utilized to extract Baum-Welch statistics for supervector and i-vector extraction [20]\u2013[23].", "startOffset": 102, "endOffset": 106}, {"referenceID": 23, "context": "DNN bottleneck features are recently employed in the i-vector framework [24], [25].", "startOffset": 72, "endOffset": 76}, {"referenceID": 24, "context": "DNN bottleneck features are recently employed in the i-vector framework [24], [25].", "startOffset": 78, "endOffset": 82}, {"referenceID": 25, "context": "Additionally, different types of i-vectors represented by DNN architectures are proposed in [26], [27].", "startOffset": 92, "endOffset": 96}, {"referenceID": 26, "context": "Additionally, different types of i-vectors represented by DNN architectures are proposed in [26], [27].", "startOffset": 98, "endOffset": 102}, {"referenceID": 16, "context": "The main attention of the National Institute of Standard and Technology (NIST) over the last two years to combine i-vectors with new machine learning techniques [28], [29] encouraged the authors to extend the prior works developed in [17], [18].", "startOffset": 234, "endOffset": 238}, {"referenceID": 17, "context": "The main attention of the National Institute of Standard and Technology (NIST) over the last two years to combine i-vectors with new machine learning techniques [28], [29] encouraged the authors to extend the prior works developed in [17], [18].", "startOffset": 240, "endOffset": 244}, {"referenceID": 16, "context": "The top layer pre-training proposed in [17] is not used in this work.", "startOffset": 39, "endOffset": 43}, {"referenceID": 12, "context": ", [13], [30]) is employed.", "startOffset": 2, "endOffset": 6}, {"referenceID": 27, "context": ", [13], [30]) is employed.", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": "Since 2006 [32], [33], DL has become a new area of research in many applications of machine learning and signal processing.", "startOffset": 11, "endOffset": 15}, {"referenceID": 29, "context": "Since 2006 [32], [33], DL has become a new area of research in many applications of machine learning and signal processing.", "startOffset": 17, "endOffset": 21}, {"referenceID": 6, "context": ", [7], [8], [34]\u2013[36]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": ", [7], [8], [34]\u2013[36]).", "startOffset": 7, "endOffset": 10}, {"referenceID": 30, "context": ", [7], [8], [34]\u2013[36]).", "startOffset": 12, "endOffset": 16}, {"referenceID": 32, "context": ", [7], [8], [34]\u2013[36]).", "startOffset": 17, "endOffset": 21}, {"referenceID": 33, "context": "For classification tasks, cross entropy is often used as the loss function and the softmax is commonly used as the activation function at the output layer [37].", "startOffset": 155, "endOffset": 159}, {"referenceID": 34, "context": "Recently, it has been shown that there are more efficient techniques for parameter initialization [38]\u2013[40].", "startOffset": 98, "endOffset": 102}, {"referenceID": 36, "context": "Recently, it has been shown that there are more efficient techniques for parameter initialization [38]\u2013[40].", "startOffset": 103, "endOffset": 107}, {"referenceID": 4, "context": "One of those techniques consists in initializing DNN with DBN parameters, which it is often referred to as unsupervised pre-training or just hybrid DBNDNN [5], [41].", "startOffset": 155, "endOffset": 158}, {"referenceID": 34, "context": "It has empirically been shown that this pretraining stage can set the weights of the network closer to an optimum solution than random initialization [38]\u2013[40].", "startOffset": 150, "endOffset": 154}, {"referenceID": 36, "context": "It has empirically been shown that this pretraining stage can set the weights of the network closer to an optimum solution than random initialization [38]\u2013[40].", "startOffset": 155, "endOffset": 159}, {"referenceID": 29, "context": "There is an efficient greedy layer wised algorithm to train DBN parameters [33].", "startOffset": 75, "endOffset": 79}, {"referenceID": 4, "context": "RBM training is based on maximum likelihood criterion using the stochastic gradient descent algorithm [5], [33].", "startOffset": 102, "endOffset": 105}, {"referenceID": 29, "context": "RBM training is based on maximum likelihood criterion using the stochastic gradient descent algorithm [5], [33].", "startOffset": 107, "endOffset": 111}, {"referenceID": 28, "context": "The gradient is estimated by an approximated version of the Contrastive Divergence (CD) algorithm which is called CD-1 [32], [33].", "startOffset": 119, "endOffset": 123}, {"referenceID": 29, "context": "The gradient is estimated by an approximated version of the Contrastive Divergence (CD) algorithm which is called CD-1 [32], [33].", "startOffset": 125, "endOffset": 129}, {"referenceID": 28, "context": "More theoretical and practical details can be found in [32], [33], [42].", "startOffset": 55, "endOffset": 59}, {"referenceID": 29, "context": "More theoretical and practical details can be found in [32], [33], [42].", "startOffset": 61, "endOffset": 65}, {"referenceID": 37, "context": "More theoretical and practical details can be found in [32], [33], [42].", "startOffset": 67, "endOffset": 71}, {"referenceID": 13, "context": "The whole training algorithm is given in [14].", "startOffset": 41, "endOffset": 45}, {"referenceID": 16, "context": "The success of the i-vector approach in speaker recognition and DL techniques in speech processing applications has encouraged the research community to combine those techniques for speaker recognition [17]\u2013[19], [21], [22], [24], [27], [43].", "startOffset": 202, "endOffset": 206}, {"referenceID": 18, "context": "The success of the i-vector approach in speaker recognition and DL techniques in speech processing applications has encouraged the research community to combine those techniques for speaker recognition [17]\u2013[19], [21], [22], [24], [27], [43].", "startOffset": 207, "endOffset": 211}, {"referenceID": 20, "context": "The success of the i-vector approach in speaker recognition and DL techniques in speech processing applications has encouraged the research community to combine those techniques for speaker recognition [17]\u2013[19], [21], [22], [24], [27], [43].", "startOffset": 213, "endOffset": 217}, {"referenceID": 21, "context": "The success of the i-vector approach in speaker recognition and DL techniques in speech processing applications has encouraged the research community to combine those techniques for speaker recognition [17]\u2013[19], [21], [22], [24], [27], [43].", "startOffset": 219, "endOffset": 223}, {"referenceID": 23, "context": "The success of the i-vector approach in speaker recognition and DL techniques in speech processing applications has encouraged the research community to combine those techniques for speaker recognition [17]\u2013[19], [21], [22], [24], [27], [43].", "startOffset": 225, "endOffset": 229}, {"referenceID": 26, "context": "The success of the i-vector approach in speaker recognition and DL techniques in speech processing applications has encouraged the research community to combine those techniques for speaker recognition [17]\u2013[19], [21], [22], [24], [27], [43].", "startOffset": 231, "endOffset": 235}, {"referenceID": 38, "context": "The success of the i-vector approach in speaker recognition and DL techniques in speech processing applications has encouraged the research community to combine those techniques for speaker recognition [17]\u2013[19], [21], [22], [24], [27], [43].", "startOffset": 237, "endOffset": 241}, {"referenceID": 20, "context": "DL techniques can be used in the i-vector extraction process [21], [22], [24], [27], [43], or applied after i-vector computation [9]\u2013[11], [17]\u2013 [19].", "startOffset": 61, "endOffset": 65}, {"referenceID": 21, "context": "DL techniques can be used in the i-vector extraction process [21], [22], [24], [27], [43], or applied after i-vector computation [9]\u2013[11], [17]\u2013 [19].", "startOffset": 67, "endOffset": 71}, {"referenceID": 23, "context": "DL techniques can be used in the i-vector extraction process [21], [22], [24], [27], [43], or applied after i-vector computation [9]\u2013[11], [17]\u2013 [19].", "startOffset": 73, "endOffset": 77}, {"referenceID": 26, "context": "DL techniques can be used in the i-vector extraction process [21], [22], [24], [27], [43], or applied after i-vector computation [9]\u2013[11], [17]\u2013 [19].", "startOffset": 79, "endOffset": 83}, {"referenceID": 38, "context": "DL techniques can be used in the i-vector extraction process [21], [22], [24], [27], [43], or applied after i-vector computation [9]\u2013[11], [17]\u2013 [19].", "startOffset": 85, "endOffset": 89}, {"referenceID": 8, "context": "DL techniques can be used in the i-vector extraction process [21], [22], [24], [27], [43], or applied after i-vector computation [9]\u2013[11], [17]\u2013 [19].", "startOffset": 129, "endOffset": 132}, {"referenceID": 10, "context": "DL techniques can be used in the i-vector extraction process [21], [22], [24], [27], [43], or applied after i-vector computation [9]\u2013[11], [17]\u2013 [19].", "startOffset": 133, "endOffset": 137}, {"referenceID": 16, "context": "DL techniques can be used in the i-vector extraction process [21], [22], [24], [27], [43], or applied after i-vector computation [9]\u2013[11], [17]\u2013 [19].", "startOffset": 139, "endOffset": 143}, {"referenceID": 18, "context": "DL techniques can be used in the i-vector extraction process [21], [22], [24], [27], [43], or applied after i-vector computation [9]\u2013[11], [17]\u2013 [19].", "startOffset": 145, "endOffset": 149}, {"referenceID": 16, "context": "In order to have more insight into the behavior of DL techniques on i-vectors, in this work the authors extend the preliminary study developed in [17], [18].", "startOffset": 146, "endOffset": 150}, {"referenceID": 17, "context": "In order to have more insight into the behavior of DL techniques on i-vectors, in this work the authors extend the preliminary study developed in [17], [18].", "startOffset": 152, "endOffset": 156}, {"referenceID": 0, "context": "An i-vector [1] is a low rank vector, typically between 400 and 600, representing a speech utterance.", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "More details can be found in [1].", "startOffset": 29, "endOffset": 32}, {"referenceID": 39, "context": "It is worth noting that as the minimum divergence training algorithm [44] is used in the i-vector extraction process, i-vectors will have a standard normal distribution N (0, 1).", "startOffset": 69, "endOffset": 73}, {"referenceID": 40, "context": "Some of the most straightforward ways to deal with unbalanced data problem are explored in [45]\u2013[47] [48], [49].", "startOffset": 91, "endOffset": 95}, {"referenceID": 42, "context": "Some of the most straightforward ways to deal with unbalanced data problem are explored in [45]\u2013[47] [48], [49].", "startOffset": 96, "endOffset": 100}, {"referenceID": 43, "context": "Some of the most straightforward ways to deal with unbalanced data problem are explored in [45]\u2013[47] [48], [49].", "startOffset": 101, "endOffset": 105}, {"referenceID": 44, "context": "Some of the most straightforward ways to deal with unbalanced data problem are explored in [45]\u2013[47] [48], [49].", "startOffset": 107, "endOffset": 111}, {"referenceID": 45, "context": "The selection method is inspired from a data-driven background data selection technique proposed in [50].", "startOffset": 100, "endOffset": 104}, {"referenceID": 45, "context": "The number of times each impostor is selected as a support vector, in all training SVM models, is called impostor support vector frequency [50].", "startOffset": 139, "endOffset": 143}, {"referenceID": 45, "context": "Those N impostors which are close to each target i-vector are treated like support vectors in [50].", "startOffset": 94, "endOffset": 98}, {"referenceID": 37, "context": "Replicated target i-vectors will not act exactly the same as each other in the pre-training process of DNNs due to the sampling noise created in RBM training [42].", "startOffset": 158, "endOffset": 162}, {"referenceID": 16, "context": "Hence, they can be used for unsupervised training of a global model referred to as Universal DBN (UDBN) [17].", "startOffset": 104, "endOffset": 108}, {"referenceID": 34, "context": "It is shown that pre-training techniques can initialize DNNs better than simply random numbers [38]\u2013[40].", "startOffset": 95, "endOffset": 99}, {"referenceID": 36, "context": "It is shown that pre-training techniques can initialize DNNs better than simply random numbers [38]\u2013[40].", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "In this case, we have proposed in [17] to adapt UDBN parameters to the balanced data obtained for each target speaker.", "startOffset": 34, "endOffset": 38}, {"referenceID": 35, "context": "The study in [39] has shown that pre-training is robust with respect to the random initialization seed.", "startOffset": 13, "endOffset": 17}, {"referenceID": 16, "context": "In contrast to [17], [18], in this work we normalize the UDBN parameters before adaptation.", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "In contrast to [17], [18], in this work we normalize the UDBN parameters before adaptation.", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "The top layer pre-training proposed in [17] is not used in this work.", "startOffset": 39, "endOffset": 43}, {"referenceID": 46, "context": "Frequency Filtering (FF) features [51] are used in the experiments.", "startOffset": 34, "endOffset": 38}, {"referenceID": 46, "context": "FFs, like MFCCs, are decorrelated version of log Filter Bank Energies (FBE) [51].", "startOffset": 76, "endOffset": 80}, {"referenceID": 46, "context": "It has been shown that FF features achieve a performance equal to or better than MFCCs [51].", "startOffset": 87, "endOffset": 91}, {"referenceID": 16, "context": "01) used in the prior work [17].", "startOffset": 27, "endOffset": 31}, {"referenceID": 4, "context": "As the input i-vectors are real-valued, a Gaussian-Bernoulli RBM (GRBM) [5], [42] is used to train the connection weights between the visible and the first hidden layer units.", "startOffset": 72, "endOffset": 75}, {"referenceID": 37, "context": "As the input i-vectors are real-valued, a Gaussian-Bernoulli RBM (GRBM) [5], [42] is used to train the connection weights between the visible and the first hidden layer units.", "startOffset": 77, "endOffset": 81}, {"referenceID": 16, "context": "Unlike in [17] and [18] where the authors used the UDBN parameters as such, in this work we normalize the connection weights so that the maximum absolute value is 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "Unlike in [17] and [18] where the authors used the UDBN parameters as such, in this work we normalize the connection weights so that the maximum absolute value is 0.", "startOffset": 19, "endOffset": 23}], "year": 2015, "abstractText": "The promising performance of Deep Learning (DL) in speech recognition has motivated the use of DL in other speech technology applications such as speaker recognition. Given ivectors as inputs, the authors proposed an impostor selection algorithm and a universal model adaptation process in a hybrid system based on Deep Belief Networks (DBN) and Deep Neural Networks (DNN) to discriminatively model each target speaker. In order to have more insight into the behavior of DL techniques in both single and multi-session speaker enrollment tasks, some experiments have been carried out in this paper in both scenarios. Additionally, the parameters of the global model, referred to as universal DBN (UDBN), are normalized before adaptation. UDBN normalization facilitates training DNNs specifically with more than one hidden layer. Experiments are performed on the NIST SRE 2006 corpus. It is shown that the proposed impostor selection algorithm and UDBN adaptation process enhance the performance of conventional DNNs 8-20% and 16-20% in terms of EER for the single and multi-session tasks, respectively. In both scenarios, the proposed architectures outperform the baseline systems obtaining up to 17% reduction in EER.", "creator": "LaTeX with hyperref package"}}}