{"id": "1512.02736", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Dec-2015", "title": "Window-Object Relationship Guided Representation Learning for Generic Object Detections", "abstract": "In existing works that learn representation for object detection, the relationship between a candidate window and the ground truth bounding box of an object is simplified by thresholding their overlap. This paper shows information loss in this simplification and picks up the relative location/size information discarded by thresholding. We propose a representation learning pipeline to use the relationship as supervision for improving the learned representation in object detection. To provide more information on how to improve the learning pipeline, we can apply this new knowledge to the class-method representation.\n\n\n\nThis paper is based on data from the class-method representation. A set of object oriented concepts, which are defined by a simple model in a model, can be used to evaluate the class-method representation. Data are stored in the class-method representation. The class-method representation defines an object object. It can only be trained by one observer.\nData are collected from a class-method representation. Each observer can store a model class that contains its model class. The class-method representation includes a representation of a class representing the class-method representation. Data are kept in a class-method representation. The object-method representation is then stored in a class-method representation. Data are used to perform a function that calculates the probability of a function from a given instance. The value is an empty number from the class-method representation.\nData are stored in a class-method representation. This is done by using the following functions:\nfrom class-method representation. A function with a field of the current value, but a field of the current value, but a field of the current value, but a field of the current value, but a field of the current value, but a field of the current value, and a field of the current value, and a field of the current value, and a field of the current value, and a field of the current value, and a field of the current value, and a field of the current value, and a field of the current value, and a field of the current value, and a field of the current value, and a field of the current value, and a field of the current value, and a field of the current value, and a field of the current value, and a field of the current value, and a field of the current value, and a field of the current value, and a field of the current value, and a field of the current value, and a field of the current value, and a field of the current", "histories": [["v1", "Wed, 9 Dec 2015 03:32:21 GMT  (402kb,D)", "http://arxiv.org/abs/1512.02736v1", "9 pages, including 1 reference page, 6 figures"]], "COMMENTS": "9 pages, including 1 reference page, 6 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.MM", "authors": ["xingyu zeng", "wanli ouyang", "xiaogang wang"], "accepted": false, "id": "1512.02736"}, "pdf": {"name": "1512.02736.pdf", "metadata": {"source": "CRF", "title": "Window-Object Relationship Guided Representation Learning for Generic Object Detections", "authors": ["Xingyu ZENG", "Wanli OUYANG", "Xiaogang WANG"], "emails": ["xyzeng@ee.cuhk.edu.hk", "wlouyang@ee.cuhk.edu.hk", "xgwang@ee.cuhk.edu.hk"], "sections": [{"heading": "1. Introduction", "text": "Object detection is the task of finding the bounding boxes of objects from images. It is challenging due to variations in illumination, texture, color, size, aspect ratio, deformation, background clutter, and occlusion. In order to handle these variations, good features for robustly representing the discriminative information of objects are critical. Initially, researchers employed manually designed features [12, 2, 13]. Recent works [9, 10, 17, 11, 1] have demonstrated the power of learning features with deep neural networks from large-scale data. It advances the state-of-the-art of object detection substantially [7, 16, 24, 8, 20, 14].\nRepresentation learning for object detection was consid-\nered as a multi-class problem [7, 3], in which a candidate window is classified as containing an object of category c or background, decided by thresholding the overlap between the candidate window and the ground truth bounding box.\nIn this paper, we show that representation learning for\nar X\niv :1\n51 2.\n02 73\n6v 1\n[ cs\n.C V\n] 9\nD ec\n2 01\nobject detection is beyond a multi-class problem. The relationship between the candidate window and the ground truth bounding box of the object, which is called the window-object relationship in this paper, provides rich information to guide representation learning for object detection. However, such information is lost in existing representation learning frameworks which largely simplify the window-object relationship by threhsolding the overlap. Some examples of person detection are shown in Figure 1. The candidate windows in Figure 1(b)-(g) may contain the upper body (a) or the legs (e) of a person, the left (c) or right (f) body of the person, and may have a smaller (c) or larger (d) size than the ground truth. They are all labeled as the same class \u201cperson\u201d in existing representation learning frameworks for object detection, because their overlaps with the ground truth bounding box are all above 0.5. However their visual content and semantic meanings have significant difference. If the deep neural network is required to classify all these candidate windows into the same class, it is easy for the model to get confused and it becomes difficult to learn representation capturing semantically meaningful visual patterns, since the supervision is weak. Such ambiguity can be can resolved by using the window-object relationship as supervision during training, which well reflect all types of variations mentioned above. Being aware of these variations in supervision, it is easier for the model to disentangle these variation factors in the learned representations.\nThe contributions of this work are summarized below. First, we propose a representation learning pipeline by using the window-object relationship as supervision so that the learned features are more sensitive to locations and sizes of objects. By distinguishing and predicting window-object relationship, the learned representation captures more semantically meaningful visual patterns of candidate windows on objects. Experimental results show that the representation learned by our approach can improve mAP of object detection by 6.4% on ILSVRC2014.\nSecond, two objective functions are designed to encode the window-object relationship. Since the window-object relationship is complex, our experiments show that direction prediction on the relative translation and scale variation in a similar way as bounding box regression does not improve representation learning. Instead, under each object category, we cluster candidate windows into subclasses according to the window-object relationship. Both visual cues and window-object relationship of candidate windows in the same subclass have less variations. Given the cropped image region of a candidate window as input, the deep neural network predicts the subclass as well as the relative translation and scale variation under the subclass during representation learning. Different subclasses employ different regressors to estimate the relative translation and scale variation.\nThird, the idea is also extended to model the relationship between a candidate window and objects of other classes in its neighborhood, given the cropped image region of the candidate window. An illustration is shown in Figure 2. The learned feature representation can make such prediction because it captures the pose information indicating existence of neighbor objects from the cropped image region (e.g. the pose of the person in Figure 2 indicates that he rides on a motorbike) and the image region may include parts of neighbor objects. All these disturbing factors explained in Figure 1 and 2 are nonlinearly coupled in the image region and deteriorate the detection accuracy. With windowobject relationship as supervision, they are disentangled in the learned feature representation and can be better removed in the later fine-turning stage or by a SVM classifier.\nFourth, we show that the window-object relationship can be better modeled by taking image regions with multiple contexts and multiple rotations as input, which includes multiple types of contextual information. This is different from commonly used multi-scale deep models, which take the same image region of different resolutions as input. Compared with the baseline, the multi-context and multirotation input improves the mAP by 2.2% on ILSVRC2014. By adding the supervision of window-object relationship on multi-context and multi-rotation, the mAP was further improved by 4.2% on ILSVRC2014."}, {"heading": "2. Relative Work", "text": "RCNN [7] is a widely used object detection pipeline based on CNN [17, 20, 14]. It first pre-trains the representation by classifying 1.2 million images from ImageNet into 1, 000 categories and then fine-tunes it by classifying object detection bounding boxes on the target detection dataset. People improved RCNN by proposing better structures of CNN [20, 17]. Ouyang et al. [14] improved pre-training by classifying the bounding boxes of the images from ImageNet instead of the whole images. All these works posed representation learning as a multi-class problem without effort on exploring window-object relationship.\nA group of works tried to solve detection with regression\n[21, 19, 23]. Given the whole image as input, Szegedy et al. [21] used DNN to regress the binary masks of an object bounding box and its subboxes. Szegedy et al. [19] used CNN to directly predict the coordinates of object bounding boxes. AttentionNet [23] initially treated the whole image as a bounding box, and iteratively refined it. They quantified the way of adjusting the bounding box into several directions, and made decision at each step. Since the locations and sizes of objects in images have large variations, direct prediction is challenging. Although some promising results were obtained on PASCAL VOC, these works have not reported state-of-the-art result on ImageNet yet, which includes a much larger number of object categories and test images. AttentionNet required training separate networks for different categories and is not scalable. It only reported the result of one category (i.e. \u201chuman\u201d) on PASCAL VOC and its average precision is lower than ours by 2%, while our learned representation is shared by a large number of categories. Different from these approaches, we explore window-object relationship to improve representation learning, while our test pipeline is similar as RCNN. Moreover, we observe that directly predicting the locations and sizes of candidate windows does not improve representation learning, since the window-object relationship is complex. Supervision needs to be carefully designed.\nIn RCNN, bounding box regression was used as the last step to refine the locations of candidate windows. However, it was not used to learn feature representation. The recently proposed Fast RCNN [6] jointly predicted object categories and locations of candidate windows as multi-task learning. 0.8% meanAP improvement is observed on PASCAL 07 dataset. However, this multi-task learning only improves meanAP by 0.2% point in the ILSVRC2014.\nIn this paper, multi-context and multi-rotation input is used. The related work [5] cropped multiple subregions as the input of CNN. Besides enriching the representation, our motivation of employing multi-context and multi-rotation input is to make CNN less confused about the relationship between candidate windows and objects. Details will be given in Section 3.3.2."}, {"heading": "3. Method", "text": "In order to provide readers with a clear picture of the whole framework, we first explain the object detection pipeline at the test stage. The major contributions comes from representation learning, whose details are provided in Section 3.2 - Section 3.5."}, {"heading": "3.1. Object detection at the testing stage", "text": "As in Fig. 3, the object detection pipeline is as follows: 1) Selective search in [18] is adopted to obtain candidate\nwindows. 2) A candidate window is used to extract features as below:\n2.1) For a candidate window bs = (x, y,W,H) with size (W,H) and center (x, y), crop images I(\u03bb,bs) with sizes (\u03bbW, \u03bbH), \u03bb \u2208 \u039b and center (x, y). The cropped images and the candidate window have the same center location (x, y). \u03bb is the scale of a contextual region. The choice of the scale set \u039b is detailed in Section 3.3.2. 2.2) Rotate the cropped image by degrees r \u2208 R and pad it with surrounding context to obtain I(r, \u03bb,bs), R = {0\u25e6, 45\u25e6, 90\u25e6}. 2.3) The cropped images I(r, \u03bb,bs) with different sizes and rotations are warped into the same size and treated as the input of CNN for extracting their features, i.e. fr,\u03bb = f(r, \u03bb, I(r, \u03bb,bs)) where f(r, \u03bb, \u2217) denotes the CNN for extracting features from I(r, \u03bb,bs), fr,\u03bb denotes the vector of features extracted for rotation r and scale \u03bb. For the candidate window bs, there are six cropped images I(r, \u03bb,bs) with (r, \u03bb) being (0\u25e6, 0.8), (0\u25e6, 1.2), (45\u25e6, 1.2), (90\u25e6, 1.2), (0\u25e6, 1.8), and (0\u25e6, 2.7) in our experiment. In the experiments, the structure of CNN is chosen as GoogleNet [20] for different settings of (r, \u03bb). And there are six branches of GoogleNets for the six settings of (r, \u03bb). The learned parameters for the six branches of GoogleNets are different. 2.4) The extracted features are then concatenated into F = concat(r,\u03bb){fr,\u03bb}, where concat is the operation for concatenating features into a vector.\n3) Extracted features are used by C binary-class SVM to classify each candidate window. The score of each SVM measures the confidence on the candidate window containing a specific object class.\nThe steps are similar to RCNN [7] except for multi-context and multi-rotation input. Our major novelties come from how to train the feature extractor f in Step 2.3 of Fig. 3."}, {"heading": "3.2. Representation learning pipeline", "text": "Our proposed pipeline is as follows and shown in Fig. 5. a) Pretrain CNN using the ImageNet 1000-class classifica-\ntion and localization data. b) Use the CNN trained in the previous step for initializa-\ntion. Train the CNN by estimating the window-object relationship. Details are given in Section 3.3. c) Use the CNN trained in the previous step for initialization. Train the CNN by estimating the window-multiobjects relationship. Details are given in Section 3.4. d) Use the CNN trained in the previous step for initialization. Train the CNN for C+1-classification problem. C is the number of object classes, plus 1 for background. C = 20 for PASCAL VOC and C = 200 for ILSVRC2014. Details are given in Section 3.5.\nSince the pipeline above is used for learning feature representation, except for the difference in the output layer, the\nnetwork structures are the same for all the training steps above. And the responses of the last CNN layer before the output layer are treated as feature representation."}, {"heading": "3.2.1 Window-object relationship label preparation", "text": ""}, {"heading": "3.3. Learning the window-object relationship", "text": "The idea is to have CNN distinguish candidate windows containing different parts of the same object or having different sizes. For example, a candidate window containing the upper body of a person and another one containing the legs were classified as the same category (i.e. \u201cperson\u201d) in\nexisting works, but are considered as different configurations of window-object relationship in our approach.\nTo distinguish candidate windows of the same object class, we cluster training samples in each class into subsets with similar relative locations. Denote by bi,s = (xi,s, yi,s,Wi,s, Hi,s) the i-th candidate window at the training stage with center (xi,s, yi,s) and size (Wi,s, Hi,s). Its ground-truth bounding box is denoted by bi,g = (xi,g, yi,g,Wi,g, Hi,g). Candidate windows at the training stage are from selective search [18] and ground truth bounding boxes. The relative location and size between the candidate window and the ground-truth bounding box (normalized by the size of candidate window) are:\nli,loc = [ (xi,s \u2212 xi,g)/Wi,s, (yi,s \u2212 yi,g)/Hi,s, (1) log(Wi,s/Wi,g), log(Hi,s/Hi,g) ] . (2)\nThe relative location and size above are used for describing the window-object relationship. With features li,loc, affinity propagation(AP) [4] is used to group candidate windows with similar window-object relationship into N clusters. Denote the cluster label for the ith candidate window by ni \u2208 {1, . . . , N}. Fig. 4 shows some clustering results where each cluster corresponds to a specific visual pattern and relative location setting. In window-object relationship prediction, the labels for a given candidate window are the relative location and size li,loc, and the cluster label li,cls."}, {"heading": "3.3.1 Loss function of window-object relationship", "text": "With the CNN parameters obtained from Step a) in Section 3.3 as initialization, we continue to train the CNN by\npredicting window-object relationship. The CNN\u2019s 1000- way classification layer in Step a) is replaced by two fully connected (fc) layers. The layer that predicts the location and size for cluster n, denoted by {\u0303li,loc,n|n = 1, . . . , N}, is called the location prediction layer. The other layer that predicts the cluster label n\u0303i for the ith candidate window is called the cluster prediction layer. Both layers use the last feature extraction layer of the CNN as input. The output dimension of the location prediction layer is 4N and the output dimension of the layer that outputs n\u0303i is N . Softmax is used for the cluster prediction layer. The following loss on window-object relationship is used:\nL = Lcls + Lloc, (3) Lcls = \u2212 \u2211 i ni log(n\u0303i),\nLloc = \u2211 i \u2016\u0303li,loc,ni \u2212 li,loc\u201622.\nLcls is the loss on predicting the window-object relationship cluster, Lloc is the loss on predicting the relative location and size. With the loss in (3), CNN is required to distinguish different window-object relationship clusters and to recognize where the actual relative location is. For the ith candidate window, the CNN outputs N location prediction vectors, i.e. l\u0303i,loc,n for n = 1, . . . , N so that the CNN learn the location prediction for different clusters separately. For the ith candidate window, only the cluster ni is used for supervising the location prediction. Therefore, different window-object relationship clusters have their own parameters learned separately by CNN to prediction location. For example, the location bias for the cluster with windowabove-object relationship can be different from the bias for the cluster with window-below-object relationship. Since\nthe relative locations in the same cluster have less variation, this divide-and-conquer strategy makes prediction easier. With the loss function defined, CNN parameters are learned by BP and stochastic gradient descent."}, {"heading": "3.3.2 Multi-context and multi-rotation", "text": "When the location and size of a candidate window is different from that of the ground truth bounding box, the candidate window only have partial visual content of the object. The limited view results in difficulty for CNN to figure out the visual difference between object classes. For example, it is hard to tell whether it is an ipod or a monitor if one can only see the screen, but it becomes much easier if the whole object and its contextual region is provided, as shown in Fig. 6 (top row). When occlusion happens, the ground truth bounding boxes may contain different amount of object parts and thus have different sizes. Without a region larger than the ground truth as input, it is confusing for CNN to decide the bounding box size. In Fig. 6 (bottom row), the ground truth box for a standing unoccluded person should cover more parts of human body than the one with legs occluded. When the image region cropped from a candidate window only covers the upper body of this person, it is difficult to predict whether the person\u2019s legs are occluded or not. When predicting the relative location between the candidate window and the ground truth, CNN should output a smaller box if occluded, but a larger box otherwise. CNN can handle this difficulty when the input contains a larger region than the ground truth. On the other hand, if the region is much larger than the object, the resolution of the object may not be high enough after normalizing the cropped image region to a standard size as the input of CNN.\nTo handle the problems above, we use multiple scales of\ncontextual regions as the input for CNN. The feature learning procedure still focuses on predicting the window-object relationship. We use 4 scales for cropping images, 0.8, 1.2, 1.8, 2.7, which are linear in log scale. 1.2 is the only scale chosen in [7] and is set as default value in many existing works. In the supplementary material, we prove that the cropped image with scale 2.7 is sufficient to cover most of the ground-truth region when the overlap between the window and the object is greater than 0.5. Even if the overlap between the candidate window and ground truth bounding box is 0.37, the cropped image with scale 2.7 can cover more than 50% of the ground truth region. 1.8 is obtained by linear interpolation between 1.2 and 2.7 in log scale. 0.8 is chosen because some candidate windows can be larger than the ground truth bounding box, as shown by the first image in Fig. 4. A cropped image with a smaller scale can help these windows to fit the actual scale of the object.\nObject rotation results in drastic appearance variation. Rotation is adopted together with multiple scales for the cropped images to make the network more robust to appearance variations.\nRegarding the relative locations labels are used in this training step, we choose to not merge training samples with different scales and rotations together and adopt multiple networks with each network is trained with one kind of training samples. All those networks share the same network structure but different parameters."}, {"heading": "3.4. Window-multi-object relationship prediction", "text": "The previous training step does not consider the coexistence of multiple object instances in the same image, which happens frequently and forms layout configurations. For the example in Fig. 5, the person have a helmet on his head and a rugby ball in his arms. To further enrich the feature representation, we extend the training procedure by predicting\nthe window-multi-objects relationship. The window-multi-objects relationship can be formulated in answering three basic questions, whether other instances exist in neighborhood, where they are and what they are. We start with the relative location and size li,loc defined in (2) to describe the pairwise relationship between a candidate window and multiple objects. The li,loc for all ground truth bounding boxes are used as features to obtain K clusters, which are used for describing the windowmulti-objects layout. Given a candidate window, its surrounding ground truth objects are assigned to their closest clusters and K labels are obtained with each label standing for what kind of object in the corresponding cluster. CNN has K classification layers and each layer is a multi-class classifier for its corresponding configuration of windowobject relationship. Therefore, CNN is required to predict the probability of what object exists in each location cluster.\nWe keep the loss function discussed in 3.3.1 and add K cross entropy loss terms. The weights of all losses are set to be 1. In this step, three kinds of labels are applied to each training sample, 1) the window-object cluster label, 2) the relative location between the window and the object, 3) and K labels to represent window-multi-object relationship.\n3.5. Finetuning for C+1-classification\nSince the ultimate goal is to detect C classes of objects, we use CNN obtained in the previous step as initialization and continue to train it for the C+ 1 classification problem. Cross entropy loss function is used. As discussed in section 3.3.2, several scales and rotation degrees are adopted, the networks for different rotations or scales are jointly learned. The features fr,\u03bb extracted from CNN for all scales \u03bb and rotations r are concatenated into a vector of features for the C + 1-class classification problem. Once features are learned, we fix the CNN parameters and learn 200 classspecific linear SVMs for object detection as in [7]. Be reminded that although multiple concepts, such as windowobject relationship, clusters of locations and sizes, other object instances and window-multi-object-relationship, are proposed in the training stage, their goal is to improve the learning of feature representation f in Fig. 3 and none of them appears in test."}, {"heading": "4. Experimental results", "text": ""}, {"heading": "4.1. Experimental setting", "text": "The implementation of our framework adopts GoogleNet [20] as CNN structure. 1000-class pretraining is based on the ILSVRC2014 classification and localization dataset. The learned representation is evaluated on the two datasets below. Most evaluation on component analysis of our training pipeline is conducted on ILSVRC2014 since it is much larger in scale and contains more object categories. The overall results and comparison with the state-of-the-art are\nfinally reported on both datasets. The ILSVRC2014 object detection dataset contains 200 object categories and is split into three subsets, i.e. train, validation and test data. The validation subset is split into val1 and val2 in [7]. We follow the same setting. In the training step d), we use both train and val1 subsets, but in the training step b) and c), we use only val1 subset. Because many positive samples are not labeled in the train subset and it may bring in label noise for window-object relationship and window-multi-object relationship.\nThe PASCAL VOC2007 dataset contains 20 object categories. Following the most commonly used approach in [7], we finetune the network with the trainval set and evaluate the performance on the test set."}, {"heading": "4.2. Component analysis on the training pipeline", "text": ""}, {"heading": "4.2.1 Comparison with baselines", "text": "In order to evaluate the effectiveness of our major innovations, several baselines are compared on ILSVRC2014 and the results are summarized in Table 1. (1) RCNN choosing GoogLeNet as the CNN structure. It is equivalent to removing step b) and c) in our training pipeline and only taking the single context of scale 1.2 without rotation as input in test. (2) Since our method concatenates features from seven GoogLeNets, one may question the improvement comes from model averaging. This baseline randomly initializes seven GoogLeNets, concatenate their features to train SVM and follow the RCNN pipeline. (3) Take multi-context and multi-rotation input (i.e. using the test pipeline in Fig. 3) without supervision of window-object relationship (i.e. removing step b) and c) from our training pipeline). (4) Our pipeline with single context of scale 1.2 and without rotation. (5) Our pipeline excluding window-multi-objects relationship (i.e. step c)) in training. (6) Our complete pipeline.\nThe result shows that increasing model complexity by model averaging bring marginal improvement. Multicontext and multi-rotation input improves the RCNN baseline by 2.2% and adding supervision of window-object relationship to it further obtains the gain of 4.2% in mAP, which is significant. Window-multi-objects relationship contributes 0.5% gain in mAP. The gain of adding supervision of window-object relationship to multi-context and multi-rotation input is larger than that added to a singlecontext input. It indicates that multi-context and multirotation input helps CNN better predict window-object relationship."}, {"heading": "4.2.2 Clustering window-object relationship", "text": "Window-object relationship is clustered in our approach as introduced in Section 3.3. Its effectiveness is evaluated in this section. Steps a), b), and d) are used. The cropped image has only one setting of rotation and scale, i.e. (r, \u03bb) = (0\u25e6, 1.2), which is the standard setting used in [7, 6]. If only steps a) and d) are used, this corresponds to the RCNN baseline. If window-object relationship clustering is not used, the relative location and object class labels are used for learning features, which is the scheme in Fast RCNN [6], the mAP improvement is 0.2%. With clustering, the mAP improvement is 1.2%. Step b) is less effective without clustering and only brings 0.2% improvement alone. Without clustering, a single regressor is learned for each class, relative locations and sizes cannot be accurately predicted and the learned features are less effective. Under each cluster, the configurations of locations and sizes are much simplified."}, {"heading": "4.2.3 Investigation on using multiple scales", "text": "Based on the training pipeline using steps a)+b)+d) with window-object relationship clustering, Table 3 shows the influence of using multiple scales. The network with four scales has mAP 45.5%, obtaining 4.2% improvement compared with single scale. Based on the scale 1.2, the mAP improvements brought by an extra scale in descending order are 2.7, 1.8, and 0.8. This fits commonsense: a larger contextual region is more helpful in eliminating visual similarity between candidate boxes of different categories. More scales provide better performance, which shows that feature representations learned with different scales are complementary to each other.\nTo figure out the effectiveness of employing multiple contextual scales for feature learning, we also run the configuration in which network parameters for all the four scales are shared and fixed to be that trained in scale 1.2. When using one shared network learned from scale 1.2, the employment of multiple contextual scales simply adds more visual cues, while training different networks for different scales, multiple contextual scales help feature learning through predicting window-object relationship which is our motivation. Compared with the network with shared\nnetwork parameters, the networks with distinct parameters for different scales obtain 3.4% mAP improvement. This shows that the use of multiple contextual scales is helpful to learn better features."}, {"heading": "4.2.4 Investigation on rotation", "text": "Table 4 shows the experimental results on using multiple rotation degrees and scales. Table 4 demonstrates that the performance improves mAP by 2.0% for single scale and 0.3% for multiple scales with the help of rotation."}, {"heading": "4.3. Overall results", "text": "Ouyang et al. [14] showed that pre-training CNN with bounding boxes of objects instead of whole images in step a) could improve the detection accuracy significantly. It is also well known that using the bounding box regression [7] to refine the locations of candidate windows in the last step of the detection pipeline is effective. In order to compete with the state-of-the-art, we incorporate the two existing technologies into our framework to boost the performance in the final evaluation.\nTable 5 summarizes the top ranked results on val2 and test datasets from ILSVRC2014 object challenge and\ndemonstrates the effectiveness of our training pipeline. Flair [22] was the winner of ILSCRC2013. GoogleNet, DeepID-Net, DeepInsight, UvA-Euvision and Berkeley Vision were the top-ranked participants of ILSVRC2014 and GoogleNet was the winner.\nTable 6 reports the results on PASCAL VOC. Since the state-of-art approach Fast RCNN (FRCN) [6] reported their performance of models trained on both VOC07 trainval and VOC12 trainval, we also evaluate our approach with the same training strategy. It has significant improvement on sate-of-the-art. It also outperforms the approaches of directly predicting bounding box locations from images."}, {"heading": "5. Conclusion", "text": "This paper proposes a training pipeline that uses the window-object relationship for improving the representation learning. In order to help the CNN to estimate these relationships, multiple scales of contextual informations and rotations are utilized. Extensive componentwise experimental evaluation on ILSVRC14 object detection dataset validate the improvement from the proposed training pipeline. Our approach outperforms the sate-ofthe-art on both ILSVRC14 and PASCAL VOC07 datasets."}], "references": [{"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "BMVC,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "CVPR,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "ICML, pages 647\u2013655,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Clustering by passing messages between data points", "author": ["B.J. Frey", "D. Dueck"], "venue": "science, 315(5814):972\u2013976,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Object detection via a multiregion & semantic segmentation-aware cnn model", "author": ["S. Gidaris", "N. Komodakis"], "venue": "arXiv preprint arXiv:1505.01749,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast r-cnn", "author": ["R. Girshick"], "venue": "arXiv preprint arXiv:1504.08083,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ECCV.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "NIPS,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q.V. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G.S. Corrado", "J. Dean", "A.Y. Ng"], "venue": "ICML,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "ICLR,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Distinctive image features from scale-invarian keypoints", "author": ["D. Lowe"], "venue": "IJCV, 60(2):91\u2013110,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Multiresolution gray-scale and rotation invariant texture classification with local binary patterns", "author": ["T. Ojala", "M. Pietikainen", "T. Maenpaa"], "venue": "IEEE Trans. PAMI, 24(7):971\u2013987,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Deepid-net: multi-stage and deformable deep convolutional neural networks for object detection", "author": ["W. Ouyang", "P. Luo", "X. Zeng", "S. Qiu", "Y. Tian", "H. Li", "S. Yang", "Z. Wang", "Y. Xiong", "C. Qian"], "venue": "arXiv preprint arXiv:1409.3505,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "IJCV,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Segmentation as selective search for object recognition", "author": ["A. Smeulders", "T. Gevers", "N. Sebe", "C. Snoek"], "venue": "ICCV,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Object detection using deep neural networks, 2015. US Patent 20,150,170,002", "author": ["C. Szegedy", "D. Erhan", "A.T. Toshev"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks for object detection", "author": ["C. Szegedy", "A. Toshev", "D. Erhan"], "venue": "NIPS,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Fisher and vlad with flair", "author": ["K.E.A. van de Sande", "C.G.M. Snoek", "A.W.M. Smeulders"], "venue": "In CVPR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Attentionnet: Aggregating weak directions for accurate object detection", "author": ["D. Yoo", "S. Park", "J.-Y. Lee", "A. Paek", "I.S. Kweon"], "venue": "arXiv preprint arXiv:1506.07704,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Generic object detection with dense neural patterns and regionlets", "author": ["W.Y. Zou", "X. Wang", "M. Sun", "Y. Lin"], "venue": "BMVC,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "4% in mean average precision (mAP) on ILSVRC2014 [15].", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "On the challenging ILSVRC2014 test dataset [15], 48.", "startOffset": 43, "endOffset": 47}, {"referenceID": 5, "context": "On PASCAL VOC, it outperforms the state-of-the-art result of Fast RCNN [6] by 3.", "startOffset": 71, "endOffset": 74}, {"referenceID": 11, "context": "Initially, researchers employed manually designed features [12, 2, 13].", "startOffset": 59, "endOffset": 70}, {"referenceID": 1, "context": "Initially, researchers employed manually designed features [12, 2, 13].", "startOffset": 59, "endOffset": 70}, {"referenceID": 12, "context": "Initially, researchers employed manually designed features [12, 2, 13].", "startOffset": 59, "endOffset": 70}, {"referenceID": 8, "context": "Recent works [9, 10, 17, 11, 1] have demonstrated the power of learning features with deep neural networks from large-scale data.", "startOffset": 13, "endOffset": 31}, {"referenceID": 9, "context": "Recent works [9, 10, 17, 11, 1] have demonstrated the power of learning features with deep neural networks from large-scale data.", "startOffset": 13, "endOffset": 31}, {"referenceID": 16, "context": "Recent works [9, 10, 17, 11, 1] have demonstrated the power of learning features with deep neural networks from large-scale data.", "startOffset": 13, "endOffset": 31}, {"referenceID": 10, "context": "Recent works [9, 10, 17, 11, 1] have demonstrated the power of learning features with deep neural networks from large-scale data.", "startOffset": 13, "endOffset": 31}, {"referenceID": 0, "context": "Recent works [9, 10, 17, 11, 1] have demonstrated the power of learning features with deep neural networks from large-scale data.", "startOffset": 13, "endOffset": 31}, {"referenceID": 6, "context": "It advances the state-of-the-art of object detection substantially [7, 16, 24, 8, 20, 14].", "startOffset": 67, "endOffset": 89}, {"referenceID": 15, "context": "It advances the state-of-the-art of object detection substantially [7, 16, 24, 8, 20, 14].", "startOffset": 67, "endOffset": 89}, {"referenceID": 23, "context": "It advances the state-of-the-art of object detection substantially [7, 16, 24, 8, 20, 14].", "startOffset": 67, "endOffset": 89}, {"referenceID": 7, "context": "It advances the state-of-the-art of object detection substantially [7, 16, 24, 8, 20, 14].", "startOffset": 67, "endOffset": 89}, {"referenceID": 19, "context": "It advances the state-of-the-art of object detection substantially [7, 16, 24, 8, 20, 14].", "startOffset": 67, "endOffset": 89}, {"referenceID": 13, "context": "It advances the state-of-the-art of object detection substantially [7, 16, 24, 8, 20, 14].", "startOffset": 67, "endOffset": 89}, {"referenceID": 6, "context": "ered as a multi-class problem [7, 3], in which a candidate window is classified as containing an object of category c or background, decided by thresholding the overlap between the candidate window and the ground truth bounding box.", "startOffset": 30, "endOffset": 36}, {"referenceID": 2, "context": "ered as a multi-class problem [7, 3], in which a candidate window is classified as containing an object of category c or background, decided by thresholding the overlap between the candidate window and the ground truth bounding box.", "startOffset": 30, "endOffset": 36}, {"referenceID": 6, "context": "RCNN [7] is a widely used object detection pipeline based on CNN [17, 20, 14].", "startOffset": 5, "endOffset": 8}, {"referenceID": 16, "context": "RCNN [7] is a widely used object detection pipeline based on CNN [17, 20, 14].", "startOffset": 65, "endOffset": 77}, {"referenceID": 19, "context": "RCNN [7] is a widely used object detection pipeline based on CNN [17, 20, 14].", "startOffset": 65, "endOffset": 77}, {"referenceID": 13, "context": "RCNN [7] is a widely used object detection pipeline based on CNN [17, 20, 14].", "startOffset": 65, "endOffset": 77}, {"referenceID": 19, "context": "People improved RCNN by proposing better structures of CNN [20, 17].", "startOffset": 59, "endOffset": 67}, {"referenceID": 16, "context": "People improved RCNN by proposing better structures of CNN [20, 17].", "startOffset": 59, "endOffset": 67}, {"referenceID": 13, "context": "[14] improved pre-training by classifying the bounding boxes of the images from ImageNet instead of the whole images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21, 19, 23].", "startOffset": 0, "endOffset": 12}, {"referenceID": 18, "context": "[21, 19, 23].", "startOffset": 0, "endOffset": 12}, {"referenceID": 22, "context": "[21, 19, 23].", "startOffset": 0, "endOffset": 12}, {"referenceID": 20, "context": "[21] used DNN to regress the binary masks of an object bounding box and its subboxes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] used CNN to directly predict the coordinates of object bounding boxes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "AttentionNet [23] initially treated the whole image as a bounding box, and iteratively refined it.", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "The recently proposed Fast RCNN [6] jointly predicted object categories and locations of candidate windows as multi-task learning.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "The related work [5] cropped multiple subregions as the input of CNN.", "startOffset": 17, "endOffset": 20}, {"referenceID": 17, "context": "3, the object detection pipeline is as follows: 1) Selective search in [18] is adopted to obtain candidate windows.", "startOffset": 71, "endOffset": 75}, {"referenceID": 19, "context": "In the experiments, the structure of CNN is chosen as GoogleNet [20] for different settings of (r, \u03bb).", "startOffset": 64, "endOffset": 68}, {"referenceID": 6, "context": "The steps are similar to RCNN [7] except for multi-context and multi-rotation input.", "startOffset": 30, "endOffset": 33}, {"referenceID": 17, "context": "Candidate windows at the training stage are from selective search [18] and ground truth bounding boxes.", "startOffset": 66, "endOffset": 70}, {"referenceID": 3, "context": "With features li,loc, affinity propagation(AP) [4] is used to group candidate windows with similar window-object relationship into N clusters.", "startOffset": 47, "endOffset": 50}, {"referenceID": 6, "context": "2 is the only scale chosen in [7] and is set as default value in many existing works.", "startOffset": 30, "endOffset": 33}, {"referenceID": 6, "context": "Once features are learned, we fix the CNN parameters and learn 200 classspecific linear SVMs for object detection as in [7].", "startOffset": 120, "endOffset": 123}, {"referenceID": 19, "context": "The implementation of our framework adopts GoogleNet [20] as CNN structure.", "startOffset": 53, "endOffset": 57}, {"referenceID": 6, "context": "The validation subset is split into val1 and val2 in [7].", "startOffset": 53, "endOffset": 56}, {"referenceID": 6, "context": "Following the most commonly used approach in [7], we finetune the network with the trainval set and evaluate the performance on the test set.", "startOffset": 45, "endOffset": 48}, {"referenceID": 6, "context": "2), which is the standard setting used in [7, 6].", "startOffset": 42, "endOffset": 48}, {"referenceID": 5, "context": "2), which is the standard setting used in [7, 6].", "startOffset": 42, "endOffset": 48}, {"referenceID": 5, "context": "If window-object relationship clustering is not used, the relative location and object class labels are used for learning features, which is the scheme in Fast RCNN [6], the mAP improvement is 0.", "startOffset": 165, "endOffset": 168}, {"referenceID": 21, "context": "approach Flair [22] RCNN[7] Berkeley Vision UvA-Euvision DeepInsight DeepID-Net GoogleNet ours val2(sgl) n/a 31.", "startOffset": 15, "endOffset": 19}, {"referenceID": 6, "context": "approach Flair [22] RCNN[7] Berkeley Vision UvA-Euvision DeepInsight DeepID-Net GoogleNet ours val2(sgl) n/a 31.", "startOffset": 24, "endOffset": 27}, {"referenceID": 5, "context": "RCNN and FRCN results come from [6].", "startOffset": 32, "endOffset": 35}, {"referenceID": 13, "context": "[14] showed that pre-training CNN with bounding boxes of objects instead of whole images in step a) could improve the detection accuracy significantly.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "It is also well known that using the bounding box regression [7] to refine the locations of candidate windows in the last step of the detection pipeline is effective.", "startOffset": 61, "endOffset": 64}, {"referenceID": 21, "context": "Flair [22] was the winner of ILSCRC2013.", "startOffset": 6, "endOffset": 10}, {"referenceID": 5, "context": "Since the state-of-art approach Fast RCNN (FRCN) [6] reported their performance of models trained on both VOC07 trainval and VOC12 trainval, we also evaluate our approach with the same training strategy.", "startOffset": 49, "endOffset": 52}], "year": 2015, "abstractText": "In existing works that learn representation for object detection, the relationship between a candidate window and the ground truth bounding box of an object is simplified by thresholding their overlap. This paper shows information loss in this simplification and picks up the relative location/size information discarded by thresholding. We propose a representation learning pipeline to use the relationship as supervision for improving the learned representation in object detection. Such relationship is not limited to object of the target category, but also includes surrounding objects of other categories. We show that image regions with multiple contexts and multiple rotations are effective in capturing such relationship during the representation learning process and in handling the semantic and visual variation caused by different window-object configurations. Experimental results show that the representation learned by our approach can improve the object detection accuracy by 6.4% in mean average precision (mAP) on ILSVRC2014 [15]. On the challenging ILSVRC2014 test dataset [15], 48.6% mAP is achieved by our single model and it is the best among published results. On PASCAL VOC, it outperforms the state-of-the-art result of Fast RCNN [6] by 3.3% in absolute mAP.", "creator": "LaTeX with hyperref package"}}}