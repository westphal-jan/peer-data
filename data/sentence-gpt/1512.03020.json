{"id": "1512.03020", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Dec-2015", "title": "Learning measures of semi-additive behaviour", "abstract": "In business analytics, measure values, such as sales numbers or volumes of cargo transported, are often summed along values of one or more corresponding categories, such as time or shipping container. However, not every measure should be added by default (e.g., one might more typically want a mean over the heights of a set of people); similarly, some measures should only be summed within certain constraints (e.g., population measures need not be summed over years). In systems such as Watson Analytics, the exact additive behaviour of a measure is often determined by a human expert. In this work, we propose a small set of features for this issue. We use these features in a case-based reasoning approach, where the system suggests an aggregation behaviour, with 86% accuracy in our collected dataset. The analysis of our data using an integrated, multi-point graph allows for an estimate of what the average cost of goods is on the average, relative to the average cost of goods. For this task, we employ the combination of the above features in an integrated analysis of the data. A separate approach is implemented for individual individuals, such as consumers in particular who have no idea what their household means, and a similar approach, such as using the combined data from a company's data set, to estimate what their total purchasing power is. In particular, our data is in a single-point graph where users are grouped by their home values and purchasing price. For example, users are divided by their family's household size and their consumption of goods for each one, which results in a large margin of error that results in a high level of error, including only a small margin of error. In our system, each individual individual user is divided into 2 distinct groups, where users are grouped by their house size, buying price, and purchasing price. If each individual user is divided by their household size, then the two groups are divided into the one-year range. In other words, individual users are divided into two distinct groups, with each of them purchasing price and purchasing price each one-year range. In this way, the combined data shows that individuals have spent almost a quarter of their time and cost money on goods, and that this can be used to account for a large margin of error that is not a problem for large households. In a separate approach, we employ the combination of the above features in an integrated analysis of the data. A separate approach is implemented for individual individuals, such as consumers in particular who have no idea what their household means, and a similar", "histories": [["v1", "Wed, 9 Dec 2015 19:52:55 GMT  (710kb,D)", "http://arxiv.org/abs/1512.03020v1", "7 pages, 11 figures, 5 tables"]], "COMMENTS": "7 pages, 11 figures, 5 tables", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["hamidreza chinaei", "mohsen rais-ghasem", "frank rudzicz"], "accepted": false, "id": "1512.03020"}, "pdf": {"name": "1512.03020.pdf", "metadata": {"source": "CRF", "title": "Learning measures of semi-additive behaviour", "authors": ["Hamidreza Chinaei", "Mohsen Rais-Ghasem", "Frank Rudzicz"], "emails": ["chinaei@cs.toronto.edu", "ghasem@ca.ibm.com", "frank@cs.toronto.edu"], "sections": [{"heading": "1. INTRODUCTION", "text": "In business analytics, measure values are often summed, but often other aggregation measures, such as means or variances, is more appropriate. For example, the dataset in Figure 1 contains employment statistics for Australian states for 2007 and 2008; clearly, while summing over partitions by year may be appropriate, but summing over the entire column is not. The result of a default aggregation (sum) over these data is shown in Figure 2 given the user question: \u201dWhat are the values of Total Fully Employed by State?\u201d. Figure 3 shows another example dataset that tracks information about loan requests received by branches of a bank. The number of clients is incorrectly added in a branch by branch analysis (see Figure 4). Here, adding the loan amounts for each branch makes sense, but adding the number of clients does not. This problem is usually avoided\n\u2217Work done while at IBM Canada.\nby defining the correct default aggregation behaviour ahead of time, through data modeling techniques, which often requires hand-tuning each variable in a dataset.\nIn this work, we propose an alternative approach. We introduce a small set of features and develop them in a CBR system to suggest better default aggregations and hence\nar X\niv :1\n51 2.\n03 02\n0v 1\n[ cs\n.A I]\n9 D\nec 2\n01 5\neliminate or at least accelerate prior, data-specific modeling. This is done by extracting potentially useful features from the data and by applying case-based reasoning (CBR) [Riesbeck and Schank, 1989; Kolodner, 1993].\nOur CBR system is fed a few use cases representing an aggregation context (so called known cases), consisting of (i) a measure item whose default aggregation is being learned or queried (e.g. \u201cTotal Fully Employed\u201d in Figure 1); (ii) one or more category items for which the given measure has repeated values that need to be aggregated (e.g. \u201cState\u201d and \u201cYear\u201d in Figure 1); (iii) the expected default aggregate which is one of sum, average, or last-period.\nIn this work, we propose a small set of features that, when used in our CBR system, improve the learning of appropriate aggregate actions up to 86% accuracy. In the rest of this paper, we first briefly explain CBR and our CBR system architecture in Section 2. We describe our extracted features for our CBR package in Section 3. The similarity measure for each feature is described in Section 4. We then go through our empirical settings in Section 5. Finally, we\nconclude and address future directions in Section 6."}, {"heading": "2. CBR ARCHITECTURE", "text": "CBR has been applied successfully in many practical domains [Lamontagne and Plaza, 2014; Jalali and Leake, 2014; Chen et al., 2014; Dong et al., 2014; Freyne and Smyth, 2010; Jurisica, 1997]. The principal idea in CBR is to draw parallels between a new case and those that have been already solved.\nFigure 5 shows an example case represented by feature values that need to be extracted. The action for each known case is defined and learned. The best action for the current case is selected using the most similar cases to the current one. That is, the current case is compared to the known cases using feature similarity measures and the case features, then the action for the most similar case, among all known cases, is selected for and applied to the current case.\nFigure 6 demonstrates the architecture of our developed aggregate learner. The user\u2019s spreadsheet is submitted to the system followed by the user\u2019s question(s). The system extracts the features for the spreadsheet particularly for category and measure columns. These features are described in Section 3. The action learner component receives the features and finds the aggregate action for each measure column based on CBR."}, {"heading": "3. EXTRACTED FEATURES", "text": "At the heart of our system, and any CBR system, lies a carefully selected feature set that is used to measure the similarity of the current case to known cases, as depicted in Figure 5. The role that each feature plays in the overall similarity measurement (i.e., the feature weights) can be assigned manually or automatically using a number of optimization techniques similar to those of Lamontagne and Guyard [2014].\nTable 1 demonstrates the representation of a case using the extracted features for the bank loan example in Figure 3. Our feature set consists of factors that we found most use-\nful in establishing the default aggregation behaviour, most notably the following features:\n1. Semantic annotations of measure columns and category columns, similar to column concepts as defined in Rais-Ghasem et al. [2013];\n2. Association type between category columns and measure columns;\n3. Coefficient of variation (CoV) as an indication of trends of measure values in the context of given categories.\nSemantic annotations For the semantic annotations of measures and categories (feature 1), consider the measure column \u201cLoan Amount (x1000)\u201d, and the category column \u201cBranch\u201d. The semantic annotations of the measure column are captured as metric and monetary, and the semantics annotations of the category column is captured as attribute [Rais-Ghasem et al., 2013]. In particular, we used a designation shared by many metrics in our known cases in our CBR system, and specifically avoided depending on explicit semantic knowledge about items.\nAssociation types A simple analysis of data values in Figure 3 reveals that values of column \u201cA\u201d (\u201cBranch\u201d) and \u201cB\u201d (\u201cCustomer ID\u201d) have a one-to-one association type, whereas the values of column \u201cA\u201d and \u201cD\u201d (Loan Amount (x1000)) have a one-tomany association type. That is, for each \u201cBranch\u201d number there is only one value for \u201cnumber of customer\u201d, thus a one-to-one association type. Note that the association type feature between a category column and measure column can be assigned four different values: one-to-one, one-to-many, many-to-one, and many-to-many.\nAs useful as this might be (i.e., it may suggest a non-additive behaviour for B in the context of A), it is not sufficient to identify the correct behaviour for the employment use case in Figure 1. That is, for each category column \u201cYear\u201d (or \u201cState\u201d) there is many \u201cTotal Fully Employed\u201d values, thus many-to-many association type. However, addition is not the correct aggregate behaviour for \u201c\u2019Total Fully Employed\u2019 over \u201cYear\u201d. We thus introduce the third feature that can potentially solve this limitation.\nValue trends We observe that trends of value dynamics are often useful, especially in combination with the aforementioned association type and semantic annotation of category (e.g., \u201cYear\u201d is a temporal category, whereas \u201cBranch\u201d is not).\nTo quantify data trends, we use the coefficient of variation (CoV):\nCoV = \u03c3/\u00b5\nwhere \u03c3 is the standard deviation and \u00b5 is the mean. Here, CoV acts as normalized standard variation. The lower the CoV measure for a column of data, the greater the trend tended to be in early empirical tests. This can be seen in Figure 7 to Figure 10.\nFigure 7 shows the CoV for the employment status. The figure shows that the CoV for\u201cTotally Fully Employed\u201dover \u201cYear\u201d is close to 0. Note that the average of CoV over different states is 0.01.\nWe have done a similar analysis for data in Watson Analytics that contains temperatures for Canadian cities. Figure 8 shows a snapshot of this spreadsheet. Figure 9 shows the CoV for the temperature, averaged over the category column (cities). This figure shows that the CoV for \u201cMean Temperature\u201d over \u201cDay of Year\u201d is between 0 and 1. That is, the average CoV over different cities is 0.48. In this analysis, to get sensible CoV values we had to transform the measure values to positive (a negative measure value occurs in the column). To do so, we add the absolute value of the minimum value of the measure column to all values in that measure column.\nFinally, we follow a similar analysis over bank loan data, whose CoV is shown in Figure 10. Here, CoV for \u201cLoan Amount\u201d over \u201cCustomer ID\u201d is close to 1 and the average CoV over different branches is 1.14.\nThus an interesting pattern emerges, as summarized in Table 2. CoV for measure values repeated for a category (e.g., various employment figures for different years) had strong associations with expected aggregation functions. Using CoV in combination with other features to propose default aggregation automatically, in general, within IBM Wat-\nson Analytics."}, {"heading": "4. FEATURE SIMILARITY FUNCTION", "text": "The similarity function for each feature is defined as follows.\nFeature 1 (column concepts):\nSim(f1(c1), f1(c2)) = card( f1(c1) \u2229 f1(c2) ) \u2217 2\ncard( f1(c1) ) + card( f1(c2) )\nwhere c1 and c2 are case 1 and case 2, respectively; and f1(ci) returns the value of feature one for case i. Note that feature 1 is the list of column concepts. In the above formula, card() returns the cardinality of each set. Effectively, the similarity is the number of common concepts (feature 1 values) for case 1 and case 2 divided by the total number of column concepts. Clearly, if the two cases share no concept, the similarity is 0, and if the two cases are identical, the similarity is 1.\nFeature 2 (association type between columns): This similarity measure is defined by our domain experts as follows: If the association types are the same for the two cases then similarity is 1. The similarity between many-to-many and many-to-one association type is 0.5. The similarity between one-to-one and anything else (one-to-many or manyto-many) is 0.\nFeature 3 (value trend): Here, the sigmoid function defines similarity between 0 and 1.\nSim(f3(c1), f3(c2)) = 1\n1 + e\u2212x\nwhere f3(ci) is the CoV values (feature 3) for case ci; x computes how close CoV values for case 1 and case 2 are, defined as : 1/(|f3(c1) \u2212 f3(c2)|) . Recall that CoV is the coefficient of variation as defined in Section 3.\nThe similarity between two cases is calculated based on the similarity of their feature values (the three introduced feature-based similarities above). Each feature-based similarity produces values between 0 and 1, and the total similarity function is the weighted sum of the three feature-based similarity (in which the weights are set to 1 in this work). Thus the total similarity is a value between 0 and 1."}, {"heading": "5. EXPERIMENTS", "text": "We collected about 100 such use cases from IBM Watson Analytics and used 65% of them to be used as the known cases in our CBR system and the remaining 35% to evaluate our approach. Each case includes a question posed on a 2-dimensional spreadsheet of data in which we explicitly denote the category and measure columns, done manually by our domain experts. We then extracted the three features described in Section 3 for all the known cases in our CBR system. We then tested our approach using the remaining evaluation data.\nOur evaluations use a \u2018majority voting\u2019 scheme in which the top k most similar cases to the test case are selected and the aggregate action for the test case is selected based on the most frequent aggregate action of these k most similar cases. In the extreme, if k = 1, the action of the most similar \u2018known\u2019 case is picked as the aggregate action for the test case. For the majority vote, we empirically set k = 3, given that we find no significant difference in sweeping k from 3 to 6. Further increases of k > 6 results in in lower accuracy.\nTable 3 shows that the measure column\u2019s CoV leads to the highest accuracy when each feature is used in isolation. Furthermore, when all the three features are used together, the approach achieves 86% accuracy using majority vote.\nFurthermore, we calculate the accuracy trend over known cases. Figure 11 shows by increasing the known cases, while the testing data remains the same, our approach achieves better accuracy. Specifically, the accuracy increases from 70% to 90% by increasing the number of known cases from 10 to 55.\nNotice that, in this work, we use the same feature weights for all the features (all feature weight are 1). In the future, we are going to learn feature weights using optimization techniques similar to those used by Lamontagne and Guyard [2014].\nFinally, we calculate the system-level elapsed time in milliseconds for different tasks, shown in Table 4. The feature selection task (for all known cases) takes less than two minutes by a ThinkPad (T410s) with i5-2.4GHz and 8GB of RAM, and using Win 7 Professional 64bit.\nThe average time for learning aggregate actions of each test\ncase is about 8 milliseconds. The learning time varies for each test case (between 1 and 28 milliseconds) since the feature extraction for each test case is done at run time and, in particular, calculating the CoV highly depends on the number of columns in the data."}, {"heading": "5.1 Examples of experimental results", "text": "Here we isolate three test cases and their most similar cases as identified by our approach, shown in Table 5. In the first two cases, our approach has been able to find the correct aggregate action, however our approach fails in the third case to learn the correct aggregate action.\nOur approach does not learn the correct default aggregate action for test case 3 (code coverage of each component). The suggested action by our approach is sum instead of lastperiod. The code coverage spreadsheet keeps the percentage of code that has been tested over time for each component of a software. Thus, the correct aggregate action is taking last-value. We believe that this error is caused small number of rows in the code coverage spreadsheet. In particular, the calculated CoV for data in the code coverage dataset does not represent the trend of coverage values well enough."}, {"heading": "6. CONCLUSION", "text": "In this work, we proposed a method for learning semi-additive behaviour in data analytics tools, chiefly IBM Watson Analytics in which these experiments were performed. Specifically, we introduced three features that are used in casebased reasoning for learning aggregate actions in business analytics. In particular, the data trend of a variable is an important factor that can be used for learning the default aggregate of that variable. We calculated the data trend using coefficient of variation (CoV), whose use can empirically increase the accuracy up to 30%. Overall, our approach is able to learn the default aggregate of our testing data with up to 90% accuracy. In this work, we used the same feature weights for all the features (all feature weights are 1.0). In the future, we intend to learn feature weights using optimization techniques such as linear programming, and we are collecting more test cases to experiment our approach on larger data sets."}, {"heading": "Acknowledgment", "text": "This research is supported in part by a contribution from IBM Canada."}], "references": [{"title": "Sentiment and preference guided social recommendation", "author": ["Y.Y. Chen", "X. Ferrer", "N. Wiratunga", "E. Plaza"], "venue": "Proceedings of 22nd International Conference on Case-Based Reasoning Research and Development, (ICCBR\u201914), Cork, Ireland.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Further experiments in opinionated product recommendation", "author": ["R. Dong", "M.P. O\u2019Mahony", "B. Smyth"], "venue": "In Proceedings of 22nd International Conference on Case-Based Reasoning Research and Development,", "citeRegEx": "Dong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "Visualization for the masses: Learning from the experts", "author": ["J. Freyne", "B. Smyth"], "venue": "Case-Based Reasoning. Research and Development, pages 111\u2013125. Springer.", "citeRegEx": "Freyne and Smyth,? 2010", "shortCiteRegEx": "Freyne and Smyth", "year": 2010}, {"title": "On retention of adaptation rules", "author": ["V. Jalali", "D. Leake"], "venue": "Proceedings of 22nd International Conference on Case-Based Reasoning Research and Development, (ICCBR\u201914), Cork, Ireland.", "citeRegEx": "Jalali and Leake,? 2014", "shortCiteRegEx": "Jalali and Leake", "year": 2014}, {"title": "Similarity-based retrieval for diverse bookshelf software repository users", "author": ["I. Jurisica"], "venue": "Proceedings of the 1997 conference of the Centre for Advanced Studies on Collaborative research (CASCON\u201997), Toronto, ON, Canada.", "citeRegEx": "Jurisica,? 1997", "shortCiteRegEx": "Jurisica", "year": 1997}, {"title": "Case-based learning, volume 10", "author": ["J.L. Kolodner"], "venue": "Springer.", "citeRegEx": "Kolodner,? 1993", "shortCiteRegEx": "Kolodner", "year": 1993}, {"title": "Learning case feature weights from relevance and ranking feedback", "author": ["L. Lamontagne", "A.B. Guyard"], "venue": "Proceedings of the 27th International Florida Artificial Intelligence Research Society Conference (FLAIRS\u201914), Pensacola Beach, Florida, USA.", "citeRegEx": "Lamontagne and Guyard,? 2014", "shortCiteRegEx": "Lamontagne and Guyard", "year": 2014}, {"title": "Towards semantic data analysis", "author": ["M. Rais-Ghasem", "R. Grosset", "M. Petitclerc", "Q. Wei"], "venue": "Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research (CASCON\u201913), Toronto, Ontario, Canada.", "citeRegEx": "Rais.Ghasem et al\\.,? 2013", "shortCiteRegEx": "Rais.Ghasem et al\\.", "year": 2013}, {"title": "Inside CaseBased Reasoning", "author": ["C.K. Riesbeck", "R.C. Schank"], "venue": "L. Erlbaum Associates Incorporation, Hillsdale, NJ, USA.", "citeRegEx": "Riesbeck and Schank,? 1989", "shortCiteRegEx": "Riesbeck and Schank", "year": 1989}], "referenceMentions": [{"referenceID": 8, "context": "This is done by extracting potentially useful features from the data and by applying case-based reasoning (CBR) [Riesbeck and Schank, 1989; Kolodner, 1993].", "startOffset": 112, "endOffset": 155}, {"referenceID": 5, "context": "This is done by extracting potentially useful features from the data and by applying case-based reasoning (CBR) [Riesbeck and Schank, 1989; Kolodner, 1993].", "startOffset": 112, "endOffset": 155}, {"referenceID": 3, "context": "CBR has been applied successfully in many practical domains [Lamontagne and Plaza, 2014; Jalali and Leake, 2014; Chen et al., 2014; Dong et al., 2014; Freyne and Smyth, 2010; Jurisica, 1997].", "startOffset": 60, "endOffset": 190}, {"referenceID": 0, "context": "CBR has been applied successfully in many practical domains [Lamontagne and Plaza, 2014; Jalali and Leake, 2014; Chen et al., 2014; Dong et al., 2014; Freyne and Smyth, 2010; Jurisica, 1997].", "startOffset": 60, "endOffset": 190}, {"referenceID": 1, "context": "CBR has been applied successfully in many practical domains [Lamontagne and Plaza, 2014; Jalali and Leake, 2014; Chen et al., 2014; Dong et al., 2014; Freyne and Smyth, 2010; Jurisica, 1997].", "startOffset": 60, "endOffset": 190}, {"referenceID": 2, "context": "CBR has been applied successfully in many practical domains [Lamontagne and Plaza, 2014; Jalali and Leake, 2014; Chen et al., 2014; Dong et al., 2014; Freyne and Smyth, 2010; Jurisica, 1997].", "startOffset": 60, "endOffset": 190}, {"referenceID": 4, "context": "CBR has been applied successfully in many practical domains [Lamontagne and Plaza, 2014; Jalali and Leake, 2014; Chen et al., 2014; Dong et al., 2014; Freyne and Smyth, 2010; Jurisica, 1997].", "startOffset": 60, "endOffset": 190}, {"referenceID": 6, "context": ", the feature weights) can be assigned manually or automatically using a number of optimization techniques similar to those of Lamontagne and Guyard [2014].", "startOffset": 127, "endOffset": 156}, {"referenceID": 7, "context": "Semantic annotations of measure columns and category columns, similar to column concepts as defined in Rais-Ghasem et al. [2013];", "startOffset": 103, "endOffset": 129}, {"referenceID": 7, "context": "The semantic annotations of the measure column are captured as metric and monetary, and the semantics annotations of the category column is captured as attribute [Rais-Ghasem et al., 2013].", "startOffset": 162, "endOffset": 188}, {"referenceID": 6, "context": "In the future, we are going to learn feature weights using optimization techniques similar to those used by Lamontagne and Guyard [2014].", "startOffset": 108, "endOffset": 137}], "year": 2015, "abstractText": "In business analytics, measure values, such as sales numbers or volumes of cargo transported, are often summed along values of one or more corresponding categories, such as time or shipping container. However, not every measure should be added by default (e.g., one might more typically want a mean over the heights of a set of people); similarly, some measures should only be summed within certain constraints (e.g., population measures need not be summed over years). In systems such as Watson Analytics, the exact additive behaviour of a measure is often determined by a human expert. In this work, we propose a small set of features for this issue. We use these features in a case-based reasoning approach, where the system suggests an aggregation behaviour, with 86% accuracy in our collected dataset.", "creator": "LaTeX with hyperref package"}}}