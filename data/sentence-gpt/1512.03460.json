{"id": "1512.03460", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Dec-2015", "title": "Neural Self Talk: Image Understanding via Continuous Questioning and Answering", "abstract": "In this paper we consider the problem of continuously discovering image contents by actively asking image based questions and subsequently answering the questions being asked. The key components include a Visual Question Generation (VQG) module and a Visual Question Answering module, in which Recurrent Neural Networks (RNN) and Convolutional Neural Network (CNN) are used. Given a dataset that contains images, questions and their answers, both modules are trained at the same time, with the difference being VQG uses the images as input and the corresponding questions as output, while VQA uses images and questions as input and the corresponding answers as output. We evaluate the self talk process subjectively using Amazon Mechanical Turk, which show effectiveness of the proposed method.\n\n\n\n\n\n\nThe main challenge we faced was the difficulty of implementing simple semantic processing using VQG. The current goal of the MLEM class is to perform a \"search\" of image data for images that contain any values with a given semantic level (i.e., a visual representation of all images). The MLEM class is a flexible approach, that can be modified, or even tweaked, with various other modifications. The MLEM is simple enough that it can be implemented in combination with existing MLEM-based algorithms.\nThis paper shows the current MLEM approach to VQG using Python. It is similar to MLEM-based algorithms, which implement only the subset of image types to which the class exists. The most interesting part of this paper is the approach to generating the \"Search\" data in the VQG class using Python. The main goal of the MLEM class is to find a semantic representation of all images in a visual representation of images that contain any values with a given semantic level. The MLEM class is a flexible approach, that can be modified, or even tweaked, with various other modifications. The MLEM class is a flexible approach, that can be modified, or even modified, with various other modifications. The most interesting part of this paper is the approach to generating the \"Search\" data in the VQG class using Python. The main goal of the MLEM class is to find a semantic representation of all images in a visual representation of images that contain any values with a given semantic level. The most interesting part of this paper is the approach to generating the \"Search\" data in the VQG class using Python. The main goal of the MLEM class is to find a", "histories": [["v1", "Thu, 10 Dec 2015 21:58:46 GMT  (7020kb,D)", "http://arxiv.org/abs/1512.03460v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.RO", "authors": ["yezhou yang", "yi li", "cornelia fermuller", "yiannis aloimonos"], "accepted": false, "id": "1512.03460"}, "pdf": {"name": "1512.03460.pdf", "metadata": {"source": "CRF", "title": "Neural Self Talk: Image Understanding via Continuous Questioning and Answering", "authors": ["Yezhou Yang", "Yi Li", "Cornelia Fermuller", "Yiannis Aloimonos"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Acclaimed as \u201cone of the last cognitive tasks to be performed well by computers\u201d [Stork, 1998], exploring and analyzing novel visual scenes is a journey of continuous discovery, which requires not just passively detecting objects and segmenting the images, but arguably more importantly, actively asking the right questions and subsequently closing the semantic loop by answering the questions being asked.\nThis paper proposes a framework that can continuously discover novel questions on an image, and then provide legitimate answers. This \u201cself talk\u201d approach for image understanding goes beyond visual classification by introducing a theoretically infinite interaction between a natural language question generation module and a visual question answering module. Under this architecture, the \u201cthought process\u201d for image understanding can be revealed by a sequence of consecutive question and answer pairs (Fig. 1).\nOur \u201cself talk\u201d framework has two \u201cexecutives\u201d that takes their roles iteratively: 1) question generation, which is responsible for asking the right questions, and 2) question answering, which accepts the questions and generate potential answers. With the rapid development in computer vision and machine learning [Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2014; Karpathy and Li, 2014; Vinyals et al., 2014;\nChen and Zitnick, 2014] there are a few tools developed for this seemingly intuitive philosophy in Artificial Intelligence, but self-talk is certainly beyond the aggregation of tools, because it is fundamentally a challenging chicken egg problem.\n1) Questions from a single image can be as diversified as possible. Researchers have attempted a few approaches that mostly centered on asking limited questions such as \u201cwhat\u201d (e.g., object and action recognition) and \u201cwhere\u201d (e.g., place recognition). Unfortunately, questions can be anything related or unrelated to the given picture. This puzzling issue of unconstrained questions can be traced back to the original Turing test1, and the solution is still elusive.\nLuckily, researchers have advanced the viewpoint that if we are able to develop a semantic understanding of a visual scene, we should be able to produce natural language descriptions of such semantics. This \u201cimage captioning\u201d perspective are indeed exciting achievements, but it is only limited to generate descriptive captions, thus we propose to consider the question \u201cCan we generate questions, based on images?\u201d.\n2) Evaluating the correctness of automatic questions answering is in the realm of Turing test. The \u201cVisual\n1 \u201cWould the questions have to be sums, or could I ask it what it had had for breakfast?\u201d Turing \u201cOh yes, anything.\u201d [Rapaport, 2005]\nar X\niv :1\n51 2.\n03 46\n0v 1\n[ cs\n.C V\n] 1\n0 D\nec 2\n01 5\nQuestion Answering\u201d [Antol et al., 2015] problem recently becomes an important area in computer vision and machine learning, and sometimes it is referred as Visual Turing challenge [Malinowski and Fritz, 2014]. A few approaches [Malinowski et al., 2015; Ren et al., 2015] have shown that deep neural nets again can be trained to answer a related question for an arbitrary scene with promising success.\n3) The semantic loop between the above two \u201cexecutives\u201d must be closed. While the above two \u201cexecutives\u201d are very interesting entities, they cannot achieve the \u201cself talking\u201d by their own. On one hand, the image captioning task neglects the importance of the thought process behind the appearance. Also, the amount of information covered by a finite language description is limited. These limitations have been pointed out by several recent works [Johnson et al., 2015; Schuster et al., 2015; Aditya et al., 2015] and have been addressed partially by introducing middle layer knowledge representations. On the other hand, the setting of the visual question answering task requires as input a related question given by human beings. These questions themselves inevitably contain information about the image, which are recognized by human beings and only available through human intervention. Several recent results on the image VQA benchmarks indicate that language only information seem to contribute to the most of the good performance and how important the role of the visual recognition is still unclear.\nIn our formalism, the input of the final deep trained system is solely an image. Both questions and answers are generated from the trained models. Also, we want to argue that the capability of raising relevant and reasonable questions actively is the key to intelligent machinery. Thus, the main contributions of this paper are twofold: 1) we propose to automatically generate \u201cself talk\u201d for arbitrary image understanding, a conceptually intuitive yet AI-challenging task; 2) we propose an image question generation module based on deep learning method.\nFigure. 2 illustrates the flow chart of our approach (Sec.3). In Sec.4, we report experiments on two publicly available datasets (DAQUAR for indoor domain [Malinowski and Fritz, 2014] and COCO for arbitrary domain [Antol et al., 2015]). Specificaly, we 1) evaluate the quality of the generated questions using standard language based metrics similar to image captioning and 2) use Amazon Mechanical Turk (AMT)-based evaluations of the generated question-answer pairs. We further discuss the insights from experimental results and challenges beyond them."}, {"heading": "2 Related Work", "text": "Our work is related mainly to three lines of research of natural image understanding: 1) question generation, 2) image captioning and 3) visual question answering.\nQuestion Generation is one of the key challenges in natural languages. Previous approaches of question generation from natural language sentences are mainly through template matching in a conservative manner [Brown et al., 2005; Heilman and Smith, 2010; Ali et al., 2010]. [Ren et al., 2015] proposed to use parsing based approach to synthetically create question and answer pairs from image annota-\ntions. In this paper, we propose a visual question generation module through a technique directly adapted from image captioning system [Karpathy and Li, 2014], which is data driven and the potential output questions space is significantly larger than previous parsing or template based approaches, and the trained module only takes in image as input.\nIn Image Captioning, in addition to the deep neural nets based approaches mentioned in Sec. 1 we also share our roots with the works of generating textual descriptions. This includes the works that retrieves and ranks sentences from training sets given an image such as [Hodosh et al., 2013], [Farhadi et al., 2010], [Ordonez et al., 2011], [Socher et al., 2014]. [Elliott and Keller, 2013], [Kulkarni et al., 2011], [Kuznetsova et al., 2012], [Yang et al., 2011], [Yao et al., 2010] are some of the works that have generated descriptions by stitching together annotations or applying templates on detected image content.\nIn the filed of Visual Question Answering, very recently researchers spent a significant amount of efforts on both creating datasets and proposing new models [Antol et al., 2015; Malinowski et al., 2015; Gao et al., 2015; Ma et al., 2015]. Interestingly both [Antol et al., 2015] and [Gao et al., 2015] adapted MS-COCO [Lin et al., 2014] images and created an open domain dataset with human generated questions and answers. The creation of these visual question answering testbed costs more than 20 people year of effort using Amazon Turk platform, and some questions are very challenging which actually require logical reasoning in order to answer correctly. Both [Malinowski et al., 2015] and [Gao et al., 2015] use recurrent networks [] to encode the sentence and output the answer. Specifically, [Malinowski et al., 2015] applies a single network to handle both encoding and decoding, while [Gao et al., 2015] divides the task into an encoder network and a decoder one. More recently, the work from [Ren et al., 2015] reported state-of-the-art VQA performance using multiple benchmarks. The progress is mainly due to formulating the task as a classification problem and focusing on the domain of questions that can be answered with one word. The visual question answering module adopts this approach."}, {"heading": "3 Self talk: Theory and Practice", "text": ""}, {"heading": "3.1 Theory and Motivation", "text": "The phenomenon of \u201cself talk\u201d has been studied in the field of psychology for hundreds of years. The term is defined\nas a special form of intrapersonal communication: a communicator\u2019s internal use of language or thought. Using the terms of computer science and engineering, it could be useful to envision intrapersonal communication occurring in the mind of the individual in a model which contains a sender, receiver, and a potential feedback loop. This process happens consciously or sub-consciously in our mind. The capability of self-raising questions and answer them is also crucial for learning. Question raising and answering facilitate the learning process. For example, in the field of education, reciprocal questioning has been studied as a strategy, where students take on the role of the teacher by formulating their own list of questions about a reading material. In this paper, we regard this as another challenge for computers, and we believe that one key to intelligence is raising the right questions.\nThe benefits of modeling scene understanding task as a revealing of the \u201cself talk\u201d of the intelligent agents are mainly twofold: 1) the understanding of the scene can be revealed step by step and the failure cases could be tracked to specific question answer pairs. In other words, the process is more transparent; 2) theoretically the number of questions could be infinite and the question and answer loop could be never ending. This is especially crucial for active agent, such as movable robots, while their view of the scene keeps changing by moving around space, and the \u201cself talk\u201d in this scenario is never-ending. For a specific task, such as scene category recognition, this formulation has been proven to be efficient [Yu et al., 2011].\nFrom a practical point of view, the revealing of \u201cself talk\u201d makes computers more human like, and the presented system has application potential in creating robotic companions [Yu et al., 2011]. Note that as human being, we make mistakes, and some of them are \u201ccute\u201d mistakes. In Sec. 4, we show that our system makes many \u201ccute\u201d mistakes too, which actually makes it more human-like."}, {"heading": "3.2 Our Approach", "text": "We have two hypotheses to validate in this work: 1) with the current progress in image captioning, a system can be trained to generate reasonable and relevant questions, and 2) by incorporating it with a visual question answering system, a system could be trained to generate human like \u201cself talk\u201d with promising success.\nIn this section, we introduce a frustratingly straightforward policy to generate a sequence of questions for the purpose of \u201cself talk\u201d. We repeat this sampling process q = QuestionSampling(I) N times (five times typically in our experiments). For each question qi generated and the accompanied original image I , we pass it through the VQA module a = V isualAnswer(q, I) to achieve an answer ai. In such a manner we achieve the \u201cself talk\u201d question and answers pairs {(q1, a1), ..., (qN , aN )}. The \u201cself talk\u201d is further evaluated by Amazon Mechanical Turk based human evaluation.\nQuestion Generation In this section, we assume an input set of images and their questions raised by human annotators. In our scenario, these are full images and their questions set. We adopted the method from [Karpathy and Li, 2014], where a simple but\nAlgorithm 1 A Primitive \u201cSelf Talk\u201d Generation Algorithm 1: procedure SELFTALKGENERATION((I)) 2: i\u2190 1 3: while i \u2264 N do 4: qi = QuestionSampling(I) 5: ai = V isualAnswer(qi, I) 6: i = i+ 1\nreturn {(q1, a1), ..., (qN , aN )}\neffective extension is introduced from previously developed Recurrent Neural Networks (RNNs) [] based language models to train image captioning model effectively. For the purpose of a self-contained work, we briefly go over the method here.\nSpecifically, during the training of our image question generation module, the multimodal RNN takes the image pixels I and a sequence of input vectors (x1, ..., xT ). It then computes a sequence of hidden states (h1, ..., hT ) and a sequence of outputs (y1, ..., yT ) by iterating the following recurrence relation from t = 1 to t = T .\nbv = Whi[CNN\u03b8c(I)] (1) ht = f(Whxxt +Whhht\u22121 + bh + 1(t = 1) bv) (2) yt = softmax(Whoht + bo), (3)\nIn the equations above, Whi, Whx, Whh, Woh, xi and bh,bo are learnable parameters, and CNN\u03b8c(I) is the last layer of a pre-trained Convolutional Neural Network (CNN) []. The output vector yt holds the (unnormalized) log probabilities of words in the dictionary and one additional dimension for a special END token. In the approach, the image context vector bv to the RNN is only given at the first iteration. A typical size of the hidden layer of the RNN is 512 neurons.\nThe RNN is trained to combine a word (xt), the previous context (ht1) to predict the next word (yt) in the generated question. The RNNs predictions on the image information bv via bias interactions on the first step. The training proceeds as follows (refer to Figure.3a)): First set h0 = 0, x1 to a special START vector, and the desired label y1 as the first word in the training question. Then set x2 to the word vector of the first word and expect the network to predict the second word, etc. Finally, on the last step when xT represents the last word, the target label is set to a special END token. The cost function is to maximize the log probability assigned to the target labels (here, a Softmax classifier).\nDuring testing time, to generate one question, we first compute the image representation bv , and then set h0 = 0, x1 to the START vector and compute the distribution over the first word y1 . We sample each word in the question from the distribution, set its embedding vector as x2 , and repeat this process until the END token is generated. In the rest of the paper, we denote this question generation process as q = QuestionSampling(I).\nQuestion Answering In this section, we assume an input set of images and their annotated question answer pairs from human labelers. We adopted the approach from [Ren et al., 2015], which introduced a model builds directly on top of the long short-term memory (LSTM) [Hochreiter and Schmidhuber, 1997] sen-\ntence model and is called the VIS+LSTM model. It treats the image as one word of the question as shown in Figure.3b).\nThe model uses the last hidden layer of the 19-layer Oxford VGG Conv Net [Simonyan and Zisserman, 2014] trained on ImageNet 2014 Challenge as the visual embeddings. The CNN part of the model is kept frozen during training. The model also uses word embedding model from general purpose skip-gram embedding [Pennington et al., 2014]. In our experiments, the word embedding is kept dynamic (trained with the rest of the model). Please refer to [Ren et al., 2015] for the details. In the rest of the paper, we denote this trained VQA module as a = V isualAnswer(q, I).\nAmazon Mechanical Turk based Evaluation For the generated question answer (\u201cself talk\u201d) pairs, since there are no groundtruth annotations that could be used for automatic evaluation, we designed a Amazon Mechanical Turk (AMT) based human evaluation metric to report.\nWe ask the Turkers to imagine they have a companion robot whose name is \u201cself talker\u201d. Once they bring the robot to a place shown in the image give, the robot started to generate questions and then self-answer the questions as if he is talking to himself. Specifically we ask the Turkers to evaluate three metrics: 1) Readability: how readable the \u201cself talker\u201d \u2019s \u201cself talk\u201d. Scores range from 1: not readable at all, to 5: no grammatical errors. Grammatically sound \u201cself-talk\u201d have better readability ratings; 2) Correctness: how correct the \u201cself talk\u201d is. \u201cself-talk\u201d content that correctly describes the image content with higher precision have better correctness ratings (range from 1 to 5); 3) Human likeness: how human-like does the robot perform (range from 1 to 5)."}, {"heading": "4 Experiments", "text": "We test the presented approach on two visual question answering (VQA) datasets, namely, DARQUAR [Malinowski and Fritz, 2014] and MSCOCO-VQA [Antol et al., 2015].\nIn the experiments on these two datasets, we first report the question generation performance using standard image captioning language based evaluation metric. Then, in order to evaluate the performance of the \u201cself talk\u201d we report the AMT results and provide further discussion."}, {"heading": "4.1 Datasets", "text": "We first briefly describe the two testing-beds we are using for the experiments.\nDAQUAR: Indoor Scenes: DAQUAR [Malinowski and Fritz, 2014] vqa dataset contains 12,468 human question answer pairs on 1,449 images of indoor scene. The training set contains 795 images and 6,793 question answer pairs, and the testing set contains 654 images and 5,675 question answer pairs. We run experiments for the full dataset with all classes, instead of their reduced set where the output space is restricted to only 37 object categories and 25 test images in total. This is because the full dataset is much more challenging and the results are more meaningful in statistics.\nCOCO: General Domain: MSCOCO-VQA [Antol et al., 2015] is the latest VQA dataset that contains openended questions about arbitrary images collect from the Internet. This dataset contains 369,861 questions and 3,698,610 ground truth answers based on 123,287 MSCOCO images. These questions and answers are sentence-based and openended. The training and testing split follows MSCOCO-VQA official split. Specifically, we use 82,783 images for training and 40,504 validation images for testing. The variation of the images in this dataset is large and till now it is considered as the largest general domian VQA dataset. The effort of collecting this dataset cost over 20 people year working time using Amazon Mechanical Turk interface."}, {"heading": "4.2 Question Generation Evaluation", "text": "We now evaluate the ability of our RNN model to raise questions about a given image. We first trained our Multimodal RNN to generate questions on full images with the goal of verifying that the model is rich enough to support the mapping from image data to sequences of words. We report the BLEU [Papineni et al., 2002], METEOR [Lavie, 2014], ROUGE [Lin, 2004] and CIDEr [Vedantam et al., 2014] scores computed with the coco-caption code [Chen et al., 2015]. Each method evaluates a candidate generated question by measuring how well it matches a set of several reference questions (averagely eight questions for DAQUAR dataset, and three questions for MSCOCO-VQA) written by humans.\nTo further validate the performance of question generation, we further list the performance metrics reported in the stateof-the-art image captioning work [Karpathy and Li, 2014]. From Table. 1, except CIDEr score, the question generation performance is comparable with the state-of-the-art image captioning performance. Note that for CIDEr score is a consensus based metric. The facts that, 1) coco-VQA has three reference ground-truth questions while coco-Caption has five and 2) human annotated questions by its nature varies more than captions, makes it hard to achieve high CIDEr score for question generation task."}, {"heading": "4.3 \u201cSelf talk\u201d Evaluation", "text": "In Table. 2 we report the average score as well as its standard deviation for each metric. We randomly drawn 100 and 1000 testing samples from DAQUAR and MSCOCO-VQA testing sets for the human evaluation reported here. From the human evaluation, we can see that the questions generated have achieved close to human readability. The correctness of the generated \u201cself talk\u201d averagely has some relevance to the image and according to Turkers, the imagined companion robot acts averagely beyond \u201ca bit like human being\u201d but below the \u201chalf human, half machine\u201d category.\nWe also asked the Turkers to choose from five immediate feelings after their companion robot\u2019s performance. Fig. 7 and Fig. 8 depicts the feedback we got from the users. Given the fact that the performance of the \u201cself talker\u201d robot is still far from human performance, most of the Turkers thought they like such a robot or feel its amusing. And only very few of the users felt scared, which indicates that our image understanding performance is far from being trapped into the so-called \u201cuncanny valley\u201d [Mori et al., 2012] of machine intelligence. At the end of our evaluation, we also asked Turkers to comment about what the robot\u2019s performance. Some\nexample comments could be found in Fig. 6."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we consider the image understanding problem as a self-questioning and answer process and we present a primitive \u201cself talk\u201d generation method based on two deep neural network modules. From the experimental evaluation on both the performance of question generation and final \u201cself talk\u201d pairs, we show that the presented method achieved a decent amount of success. There are still several potential pathways to improve the performance of intelligent \u201cself talk\u201d.\nThe role of common-sense knowledge. Common-sense knowledge has a crucial role in question raising and answering process for human beings [Aditya et al., 2015]. The experimental result shows that our system by learning the model from large annotated question answer pairs, it implicitly encodes a certain level of common-sense. The real challenge is to deal with situations that the visual input conflicts with the common-sense learned from context data. In our experiment, it seems that the model is biased towards to trust his common sense more than the visual input. How to incorporate either logical or numerical forms of common-sense into end-to-end based image understanding system is still an open problem.\nCreating a story-line. When human beings perform intrapersonal communication, we tend to follow a logic flow or so-called story-line. This requires a question generation modules that takes in consideration the answers from previous questions for consideration. This indicates a more sophisticated dialogue generation process (such as a cognitive dialogue [Aloimonos and Fermu\u0308ller, 2015]), and it can also potentially prevent self-contradictions happened in this paper\u2019s generated results (see last comment in Fig. 6).\nAs indicated from AMT feedback, human users felt it is cute and fondness to have a robot companion that moves around and talkative. Another open avenue is to integrate the current trained model onto a robot platform and through interaction with users to continuously refine its trained model."}], "references": [{"title": "From images to sentences through scene description graphs using commonsense reasoning and knowledge", "author": ["Somak Aditya", "Yezhou Yang", "Chitta Baral", "Cornelia Fermuller", "Yiannis Aloimonos"], "venue": "arXiv preprint arXiv:1511.03292,", "citeRegEx": "Aditya et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of QG2010: The Third Workshop on Question Generation", "author": ["Husam Ali", "Yllias Chali", "Sadid A Hasan. Automation of question generation from sentences"], "venue": "pages 58\u201367,", "citeRegEx": "Ali et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "The cognitive dialogue: A new model for vision implementing common sense reasoning", "author": ["Yiannis Aloimonos", "Cornelia Ferm\u00fcller"], "venue": "Image and Vision Computing, 34:42\u201344,", "citeRegEx": "Aloimonos and Ferm\u00fcller. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Vqa: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh"], "venue": "International Conference on Computer Vision (ICCV),", "citeRegEx": "Antol et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic question generation for vocabulary assessment", "author": ["Brown et al", "2005] Jonathan C Brown", "Gwen A Frishkoff", "Maxine Eskenazi"], "venue": "In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,", "citeRegEx": "al. et al\\.,? \\Q2005\\E", "shortCiteRegEx": "al. et al\\.", "year": 2005}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["Xinlei Chen", "C Lawrence Zitnick"], "venue": "arXiv preprint arXiv:1411.5654,", "citeRegEx": "Chen and Zitnick. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Microsoft coco captions: Data collection and evaluation server", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Dollar", "C Lawrence Zitnick"], "venue": "arXiv preprint arXiv:1504.00325,", "citeRegEx": "Chen et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1411.4389,", "citeRegEx": "Donahue et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Image description using visual dependency representations", "author": ["Elliott", "Keller", "2013] Desmond Elliott", "Frank Keller"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Elliott et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2013}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["Farhadi et al", "2010] Ali Farhadi", "Mohsen Hejrati", "Mohammad Amin Sadeghi", "Peter Young", "Cyrus Rashtchian", "Julia Hockenmaier", "David Forsyth"], "venue": "In Proceedings of the 11th European Conference on Computer Vi-", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Are you talking", "author": ["Gao et al", "2015] Haoyuan Gao", "Junhua Mao", "Jie Zhou", "Zhiheng Huang", "Lei Wang", "Wei Xu"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Good question! statistical ranking for question generation", "author": ["Heilman", "Smith", "2010] Michael Heilman", "Noah A Smith"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Heilman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Heilman et al\\.", "year": 2010}, {"title": "Neural computation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber. Long short-term memory"], "venue": "9(8):1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Framing image description as a ranking task: Data", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier"], "venue": "models and evaluation metrics. Journal of Artificial Intelligence Research, pages 853\u2013899,", "citeRegEx": "Hodosh et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "author": ["Justin Johnson", "Ranjay Krishna", "Michael Stark", "Jia Li", "Michael Bernstein", "Li Fei-Fei. Image retrieval using scene graphs"], "venue": "June", "citeRegEx": "Johnson et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Fei-Fei Li"], "venue": "arXiv preprint arXiv:1412.2306,", "citeRegEx": "Karpathy and Li. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S Zemel"], "venue": "arXiv preprint arXiv:1411.2539,", "citeRegEx": "Kiros et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Baby talk: Understanding and generating image descriptions", "author": ["Girish Kulkarni", "Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi", "Alexander C Berg", "Tamara L Berg"], "venue": "Proceedings of the 24th CVPR,", "citeRegEx": "Kulkarni et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Collective generation of natural image descriptions. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume", "author": ["Kuznetsova et al", "2012] Polina Kuznetsova", "Vicente Ordonez", "Alexander C. Berg", "Tamara L. Berg", "Yejin Choi"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Michael Denkowski Alon Lavie"], "venue": "ACL 2014, page 376,", "citeRegEx": "Lavie. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "Computer Vision\u2013ECCV 2014, pages 740\u2013755. Springer,", "citeRegEx": "Lin et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin"], "venue": "Text summarization branches out: Proceedings of the ACL-04 workshop, volume 8,", "citeRegEx": "Lin. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning to answer questions from image using convolu", "author": ["Ma et al", "2015] Lin Ma", "Zhengdong Lu", "Hang Li"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Towards a visual turing challenge", "author": ["Mateusz Malinowski", "Mario Fritz"], "venue": "arXiv preprint arXiv:1410.8027,", "citeRegEx": "Malinowski and Fritz. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask your neurons: A neuralbased approach to answering questions about images", "author": ["Mateusz Malinowski", "Marcus Rohrbach", "Mario Fritz"], "venue": "arXiv preprint arXiv:1505.01121,", "citeRegEx": "Malinowski et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan L Yuille"], "venue": "arXiv preprint arXiv:1410.1090,", "citeRegEx": "Mao et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "The uncanny valley [from the field", "author": ["Masahiro Mori", "Karl F MacDorman", "Norri Kageki"], "venue": "Robotics & Automation Magazine, IEEE, 19(2):98\u2013100,", "citeRegEx": "Mori et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["Ordonez et al", "2011] Vicente Ordonez", "Girish Kulkarni", "Tamara L. Berg"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni et al", "2002] Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "al. et al\\.,? \\Q2002\\E", "shortCiteRegEx": "al. et al\\.", "year": 2002}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), pages 1532\u20131543,", "citeRegEx": "Pennington et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "The turing test: Verbal behavior as the hallmark of intelligence edited by stuart shieber", "author": ["William J. Rapaport"], "venue": "Computational Linguistics, 31(3):407\u2013412, September", "citeRegEx": "Rapaport. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Exploring models and data for image question answering", "author": ["Mengye Ren", "Ryan Kiros", "Richard Zemel"], "venue": "arXiv preprint arXiv:1505.02074,", "citeRegEx": "Ren et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Generating semantically precise scene graphs from textual descriptions for improved image retrieval", "author": ["Schuster et al", "2015] Sebastian Schuster", "Ranjay Krishna", "Angel Chang", "Li Fei-Fei", "Christopher D. Manning"], "venue": "In Proceedings of the Fourth Workshop on Vision and Language,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "HAL\u2019s Legacy: 2001\u2019s Computer as Dream and Reality", "author": ["David G Stork"], "venue": "MIT Press,", "citeRegEx": "Stork. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Cider: Consensusbased image description evaluation", "author": ["Vedantam", "C Lawrence Zitnick", "Devi Parikh"], "venue": "arXiv preprint arXiv:1411.5726,", "citeRegEx": "Vedantam et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vedantam et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "Vinyals et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Corpus-guided sentence generation of natural images", "author": ["Yang et al", "2011] Yezhou Yang", "Ching Lik Teo", "III Hal Daum\u00e9", "Yiannis Aloimonos"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "I2t: Image parsing to text description", "author": ["Benjamin Z. Yao", "Xiong Yang", "Liang Lin", "Mun Wai Lee", "Song Chun Zhu"], "venue": "Proceedings of the IEEE, 98(8):1485\u2013 1508,", "citeRegEx": "Yao et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In Computer Vision (ICCV)", "author": ["Xiaodong Yu", "Cornelia Fermuller", "Ching Lik Teo", "Yezhou Yang", "Yiannis Aloimonos. Active scene recognition with vision", "language"], "venue": "2011 IEEE International Conference on, pages 810\u2013817. IEEE,", "citeRegEx": "Yu et al.. 2011", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 34, "context": "Acclaimed as \u201cone of the last cognitive tasks to be performed well by computers\u201d [Stork, 1998], exploring and analyzing novel visual scenes is a journey of continuous discovery, which requires not just passively detecting objects and segmenting the images, but arguably more importantly, actively asking the right questions and subsequently closing the semantic loop by answering the questions being asked.", "startOffset": 81, "endOffset": 94}, {"referenceID": 30, "context": "\u201d [Rapaport, 2005] ar X iv :1 51 2.", "startOffset": 2, "endOffset": 18}, {"referenceID": 3, "context": "Question Answering\u201d [Antol et al., 2015] problem recently becomes an important area in computer vision and machine learning, and sometimes it is referred as Visual Turing challenge [Malinowski and Fritz, 2014].", "startOffset": 20, "endOffset": 40}, {"referenceID": 23, "context": ", 2015] problem recently becomes an important area in computer vision and machine learning, and sometimes it is referred as Visual Turing challenge [Malinowski and Fritz, 2014].", "startOffset": 148, "endOffset": 176}, {"referenceID": 24, "context": "A few approaches [Malinowski et al., 2015; Ren et al., 2015] have shown that deep neural nets again can be trained to answer a related question for an arbitrary scene with promising success.", "startOffset": 17, "endOffset": 60}, {"referenceID": 31, "context": "A few approaches [Malinowski et al., 2015; Ren et al., 2015] have shown that deep neural nets again can be trained to answer a related question for an arbitrary scene with promising success.", "startOffset": 17, "endOffset": 60}, {"referenceID": 14, "context": "These limitations have been pointed out by several recent works [Johnson et al., 2015; Schuster et al., 2015; Aditya et al., 2015] and have been addressed partially by introducing middle layer knowledge representations.", "startOffset": 64, "endOffset": 130}, {"referenceID": 0, "context": "These limitations have been pointed out by several recent works [Johnson et al., 2015; Schuster et al., 2015; Aditya et al., 2015] and have been addressed partially by introducing middle layer knowledge representations.", "startOffset": 64, "endOffset": 130}, {"referenceID": 23, "context": "4, we report experiments on two publicly available datasets (DAQUAR for indoor domain [Malinowski and Fritz, 2014] and COCO for arbitrary domain [Antol et al.", "startOffset": 86, "endOffset": 114}, {"referenceID": 3, "context": "4, we report experiments on two publicly available datasets (DAQUAR for indoor domain [Malinowski and Fritz, 2014] and COCO for arbitrary domain [Antol et al., 2015]).", "startOffset": 145, "endOffset": 165}, {"referenceID": 1, "context": "Previous approaches of question generation from natural language sentences are mainly through template matching in a conservative manner [Brown et al., 2005; Heilman and Smith, 2010; Ali et al., 2010].", "startOffset": 137, "endOffset": 200}, {"referenceID": 31, "context": "[Ren et al., 2015] proposed to use parsing based approach to synthetically create question and answer pairs from image annotaFigure 2: The flow chart or our approach.", "startOffset": 0, "endOffset": 18}, {"referenceID": 15, "context": "In this paper, we propose a visual question generation module through a technique directly adapted from image captioning system [Karpathy and Li, 2014], which is data driven and the potential output questions space is significantly larger than previous parsing or template based approaches, and the trained module only takes in image as input.", "startOffset": 128, "endOffset": 151}, {"referenceID": 13, "context": "This includes the works that retrieves and ranks sentences from training sets given an image such as [Hodosh et al., 2013], [Farhadi et al.", "startOffset": 101, "endOffset": 122}, {"referenceID": 17, "context": "[Elliott and Keller, 2013], [Kulkarni et al., 2011], [Kuznetsova et al.", "startOffset": 28, "endOffset": 51}, {"referenceID": 38, "context": ", 2011], [Yao et al., 2010] are some of the works that have generated descriptions by stitching together annotations or applying templates on detected image content.", "startOffset": 9, "endOffset": 27}, {"referenceID": 3, "context": "In the filed of Visual Question Answering, very recently researchers spent a significant amount of efforts on both creating datasets and proposing new models [Antol et al., 2015; Malinowski et al., 2015; Gao et al., 2015; Ma et al., 2015].", "startOffset": 158, "endOffset": 238}, {"referenceID": 24, "context": "In the filed of Visual Question Answering, very recently researchers spent a significant amount of efforts on both creating datasets and proposing new models [Antol et al., 2015; Malinowski et al., 2015; Gao et al., 2015; Ma et al., 2015].", "startOffset": 158, "endOffset": 238}, {"referenceID": 3, "context": "Interestingly both [Antol et al., 2015] and [Gao et al.", "startOffset": 19, "endOffset": 39}, {"referenceID": 20, "context": ", 2015] adapted MS-COCO [Lin et al., 2014] images and created an open domain dataset with human generated questions and answers.", "startOffset": 24, "endOffset": 42}, {"referenceID": 24, "context": "Both [Malinowski et al., 2015] and [Gao et al.", "startOffset": 5, "endOffset": 30}, {"referenceID": 24, "context": "Specifically, [Malinowski et al., 2015] applies a single network to handle both encoding and decoding, while [Gao et al.", "startOffset": 14, "endOffset": 39}, {"referenceID": 31, "context": "More recently, the work from [Ren et al., 2015] reported state-of-the-art VQA performance using multiple benchmarks.", "startOffset": 29, "endOffset": 47}, {"referenceID": 39, "context": "For a specific task, such as scene category recognition, this formulation has been proven to be efficient [Yu et al., 2011].", "startOffset": 106, "endOffset": 123}, {"referenceID": 39, "context": "From a practical point of view, the revealing of \u201cself talk\u201d makes computers more human like, and the presented system has application potential in creating robotic companions [Yu et al., 2011].", "startOffset": 176, "endOffset": 193}, {"referenceID": 15, "context": "We adopted the method from [Karpathy and Li, 2014], where a simple but Algorithm 1 A Primitive \u201cSelf Talk\u201d Generation Algorithm", "startOffset": 27, "endOffset": 50}, {"referenceID": 31, "context": "We adopted the approach from [Ren et al., 2015], which introduced a model builds directly on top of the long short-term memory (LSTM) [Hochreiter and Schmidhuber, 1997] sen-", "startOffset": 29, "endOffset": 47}, {"referenceID": 12, "context": ", 2015], which introduced a model builds directly on top of the long short-term memory (LSTM) [Hochreiter and Schmidhuber, 1997] sen-", "startOffset": 94, "endOffset": 128}, {"referenceID": 33, "context": "The model uses the last hidden layer of the 19-layer Oxford VGG Conv Net [Simonyan and Zisserman, 2014] trained on ImageNet 2014 Challenge as the visual embeddings.", "startOffset": 73, "endOffset": 103}, {"referenceID": 29, "context": "The model also uses word embedding model from general purpose skip-gram embedding [Pennington et al., 2014].", "startOffset": 82, "endOffset": 107}, {"referenceID": 31, "context": "Please refer to [Ren et al., 2015] for the details.", "startOffset": 16, "endOffset": 34}, {"referenceID": 23, "context": "We test the presented approach on two visual question answering (VQA) datasets, namely, DARQUAR [Malinowski and Fritz, 2014] and MSCOCO-VQA [Antol et al.", "startOffset": 96, "endOffset": 124}, {"referenceID": 3, "context": "We test the presented approach on two visual question answering (VQA) datasets, namely, DARQUAR [Malinowski and Fritz, 2014] and MSCOCO-VQA [Antol et al., 2015].", "startOffset": 140, "endOffset": 160}, {"referenceID": 23, "context": "DAQUAR: Indoor Scenes: DAQUAR [Malinowski and Fritz, 2014] vqa dataset contains 12,468 human question answer pairs on 1,449 images of indoor scene.", "startOffset": 30, "endOffset": 58}, {"referenceID": 3, "context": "COCO: General Domain: MSCOCO-VQA [Antol et al., 2015] is the latest VQA dataset that contains openended questions about arbitrary images collect from the Internet.", "startOffset": 33, "endOffset": 53}, {"referenceID": 19, "context": ", 2002], METEOR [Lavie, 2014], ROUGE [Lin, 2004] and CIDEr [Vedantam et al.", "startOffset": 16, "endOffset": 29}, {"referenceID": 21, "context": ", 2002], METEOR [Lavie, 2014], ROUGE [Lin, 2004] and CIDEr [Vedantam et al.", "startOffset": 37, "endOffset": 48}, {"referenceID": 35, "context": ", 2002], METEOR [Lavie, 2014], ROUGE [Lin, 2004] and CIDEr [Vedantam et al., 2014] scores computed with the coco-caption code [Chen et al.", "startOffset": 59, "endOffset": 82}, {"referenceID": 6, "context": ", 2014] scores computed with the coco-caption code [Chen et al., 2015].", "startOffset": 51, "endOffset": 70}, {"referenceID": 15, "context": "To further validate the performance of question generation, we further list the performance metrics reported in the stateof-the-art image captioning work [Karpathy and Li, 2014].", "startOffset": 154, "endOffset": 177}, {"referenceID": 15, "context": "coco-Caption [Karpathy and Li, 2014] .", "startOffset": 13, "endOffset": 36}, {"referenceID": 26, "context": "And only very few of the users felt scared, which indicates that our image understanding performance is far from being trapped into the so-called \u201cuncanny valley\u201d [Mori et al., 2012] of machine intelligence.", "startOffset": 163, "endOffset": 182}, {"referenceID": 0, "context": "Common-sense knowledge has a crucial role in question raising and answering process for human beings [Aditya et al., 2015].", "startOffset": 101, "endOffset": 122}, {"referenceID": 2, "context": "This indicates a more sophisticated dialogue generation process (such as a cognitive dialogue [Aloimonos and Ferm\u00fcller, 2015]), and it can also potentially prevent self-contradictions happened in this paper\u2019s generated results (see last comment in Fig.", "startOffset": 94, "endOffset": 125}], "year": 2015, "abstractText": "In this paper we consider the problem of continuously discovering image contents by actively asking image based questions and subsequently answering the questions being asked. The key components include a Visual Question Generation (VQG) module and a Visual Question Answering module, in which Recurrent Neural Networks (RNN) and Convolutional Neural Network (CNN) are used. Given a dataset that contains images, questions and their answers, both modules are trained at the same time, with the difference being VQG uses the images as input and the corresponding questions as output, while VQA uses images and questions as input and the corresponding answers as output. We evaluate the self talk process subjectively using Amazon Mechanical Turk, which show effectiveness of the proposed method.", "creator": "LaTeX with hyperref package"}}}