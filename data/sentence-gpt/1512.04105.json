{"id": "1512.04105", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Dec-2015", "title": "Policy Gradient Methods for Off-policy Control", "abstract": "Off-policy learning refers to the problem of learning the value function of a way of behaving, or policy, while following a different policy. Gradient-based off-policy learning algorithms, such as GTD and TDC/GQ, converge even when using function approximation and incremental updates. However, they have been developed for the case of a fixed behavior policy.\n\n\n\n\n\nThe concept of optimization\nOne of the simplest and most effective strategies to achieve a fixed behavior policy is to adopt a strategy for reducing errors when they are being made by a model, or an algorithm that does not have such values. A model is a learning strategy. The following techniques apply to the approach that involves the use of algorithms or an algorithm that does not have such values.\nThe following types of optimization models are defined as\n\nThe most common, non-standard optimization model is:\nIn general, these are known as the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method. It can be learned in one step by one.\nIn general, these are known as the optimization method, or the optimization method, or the optimization method. It can be learned in one step by one. In general, these are known as the optimization method, or the optimization method, or the optimization method, or the optimization method. It can be learned in one step by one.\nFor example, a model can be a learning algorithm, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or the optimization method, or", "histories": [["v1", "Sun, 13 Dec 2015 19:20:14 GMT  (39kb,D)", "http://arxiv.org/abs/1512.04105v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["lucas lehnert", "doina precup"], "accepted": false, "id": "1512.04105"}, "pdf": {"name": "1512.04105.pdf", "metadata": {"source": "META", "title": "Policy Gradient Methods for Off-policy Control", "authors": ["Lucas Lehnert", "Doina Precup"], "emails": ["lucas.lehnert@mail.mcgill.ca", "dprecup@cs.mcgill.ca"], "sections": [{"heading": "1 Introduction", "text": "One fundamental concept in Reinforcement Learning (RL) is Temporal Difference (TD) learning introduced by Sutton [9]. In TD-learning, methods such as TD(0) are used for policy evaluation where one tries to learn the value of a given state under a fixed policy. The extension to the control case is called Q-learning where the value function is defined on state-action pairs. The control policy is then computed from these action values. One of the first Q-learning algorithms was proposed by Watkins and Dayan [14] which simultaneously searches and evaluates a policy by varying its action value estimates. Watkins and Dayan\u2019s Q-learning algorithm is an off-policy algorithm as the policy that is searched and evaluated is strictly greedy with respect to the current action values, but for control the agent uses a \u03b5-greedy policy. This facilitates exploration as the agent is allowed to make a random move with \u03b5 probability to obtain representative samples and facilitate the search for a policy that generates high rewards.\nRecently, gradient-based off-policy learning algorithms were introduced such as GTD [11] and TDC [13] which are also proven to be convergent under off-policy learning with linear value function approximation. The extension to Q-learning, GQ(\u03bb) [5], is also convergent under off-policy learning but only if the control policy is fixed. For the control case this is not sufficient as the agent has to explore its environment to be able to search and find a good policy. The reason why convergence cannot be guaranteed is that a non-stationary policy causes drift in the distribution from which transition samples are generated. While this drift is necessary for the agent to find a good policy, it can also cause oscillations in the value function estimates and the algorithm to not converge. SARSA also suffers from this problem and is only guaranteed to converge to a sub-space of policies [4, 3]. Within this sub-space the value function estimates may oscillate indefinitely.\nIn this paper we present a new gradient-based TD-learning algorithm that is similar to GQ but also incorporates policy gradients to correct for the drift in the distribution from which transitions are sampled. Similar to the policy gradient framework [12] we directly analyze the interaction between the policy gradient and the distribution from which transitions are sampled. As a result, our algorithm iterates over the sequence Markov Chains induced by the variation in the value function estimates and therefore policies. This makes our algorithm similar to policy iteration\nar X\niv :1\n51 2.\n04 10\n5v 1\n[ cs\n.A I]\n1 3\nD ec\n2 01\nmethods such as [8]. However, rather than evaluating and then improving the policy in consecutive steps, our method simultaneously improves and evaluates the current policy."}, {"heading": "2 Q-learning with Policy Gradients", "text": "We consider an MDP M = \u3008S,A, t, r, \u03b3\u3009 where S is a finite state space and A is a finite action space. The transition function t : S \u00d7 A \u00d7 S \u2192 (0, 1) is stochastic, the reward function is defined as r : S \u00d7 A \u2192 R, and the discount factor \u03b3 \u2208 (0, 1). As in [13, 5] we consider the linear function approximation case with a basis function \u03c6 : S \u00d7A \u2192 Rk and define the state-action value function as\nQ\u03b8(s, a) = \u03b8 >\u03c6(s, a) \u2248 Q(s, a) = E [ \u221e\u2211 t=0 \u03b3trt+1 \u2223\u2223\u2223\u2223\u2223s0 = s, a0 = a ] . (1)\nLet Q\u03b8 = \u03a6\u03b8 \u2208 R|S\u00d7A| be the vector of all state-action values and similarly R \u2208 R|S\u00d7A| be the vector of all rewards. We are assuming that the MDP is ergodic and that a limit distribution ds,a = limt\u2192\u221e P{st = s, at = a} exists. Letting D be a diagonal matrix with the limit distribution on its diagonal we define the norm ||v||2D = v>Dv. The Mean Squared Projected Bellman Error introduced by [13] is\nMSPBE(\u03b8) = ||Q\u03b8 \u2212\u03a0T\u03b8Q\u03b8||2D, (2)\nwhere \u03a0 = \u03a6(\u03a6D\u03a6)\u22121\u03a6>D is the projection matrix and the Bellman operator applied to the action value function is defined as\nT\u03b8Q\u03b8 def = R+ \u03b3P\u03b8Q\u03b8. (3)\nOur approach differs to TDC and GQ in that we view the Bellman operator and the stationary distribution over state-action pairs as parametric in the value function parameter \u03b8. For the stationary distribution we assume that\nds,a = ds\u03c0\u03b8(a|s) = [\nlim t\u2192\u221e\nP{st = s} ] \u03c0\u03b8(a|s). (4)\nThis changes the way derive the gradient of the MSPBE as we assume additional dependencies on the parameter vector \u03b8 through the action selection probabilities \u03c0\u03b8(a|s)."}, {"heading": "2.1 Gradient Derivation", "text": "To obtain the gradient of the MSPBE objective, [13] have shown\nMSPBE(\u03b8) = ( \u03a6>D(T\u03b8Q\u03b8 \u2212Q\u03b8) )> (\u03a6>D\u03a6)\u22121 ( \u03a6>D(T\u03b8Q\u03b8 \u2212Q\u03b8) ) .\nTo simplify the gradient calculation we assume \u03b8 = [\u03b81, ..., \u03b8n] > and compute the partial dervative with respect to \u03b8i, which we denote with \u2202i:\n\u2202iMSPBE(\u03b8) =\n= \u2202i [( \u03a6>D(T\u03c0\u03b8 Q\u03b8 \u2212Q\u03b8) )> (\u03a6>D\u03a6)\u22121 ( \u03a6>D(T\u03c0\u03b8 Q\u03b8 \u2212Q\u03b8) )] = 2\u2202i [( \u03a6>D(T\u03c0\u03b8 Q\u03b8 \u2212Q\u03b8) )>] (\u03a6>D\u03a6)\u22121 ( \u03a6>D(T\u03c0\u03b8 Q\u03b8 \u2212Q\u03b8)\n) + ( \u03a6>D(T\u03c0\u03b8 Q\u03b8 \u2212Q\u03b8) )> \u2202i [ (\u03a6>D\u03a6)\u22121 ] ( \u03a6>D(T\u03c0\u03b8 Q\u03b8 \u2212Q\u03b8) ) .\nFor the derivative of the inverse feature covariance we have \u2202i [ (\u03a6>D\u03a6)\u22121 ] = \u2212(\u03a6>D\u03a6)\u22121\u2202i(\u03a6>D\u03a6)(\u03a6>D\u03a6)\u22121\n= \u2212(\u03a6>D\u03a6)\u22121(\u03a6>\u2202iD\u03a6)(\u03a6>D\u03a6)\u22121.\nPlugging this back into the gradient above we obtain\n\u2202iMSPBE(\u03b8) = = 2 ( \u03a6>\u2202iD(T \u03c0 \u03b8 Q\u03b8 \u2212Q\u03b8) + \u03a6>D\u2202i(T\u03c0\u03b8 Q\u03b8 \u2212Q\u03b8) )> (\u03a6>D\u03a6)\u22121 ( \u03a6>D(T\u03c0\u03b8 Q\u03b8 \u2212Q\u03b8) ) \u2212 ( \u03a6>D(T\u03c0\u03b8 Q\u03b8 \u2212Q\u03b8) )> (\u03a6>D\u03a6)\u22121(\u03a6>\u2202iD\u03a6)(\u03a6 >D\u03a6)\u22121 ( \u03a6>D(T\u03c0\u03b8 Q\u03b8 \u2212Q\u03b8) ) .\nFor the partial derivative on the Bellman error we have\n\u2202i[T\u03b8Q\u03b8 \u2212Q\u03b8] = \u2202i[R+ \u03b3P\u03b8\u03a6\u03b8 \u2212 \u03a6\u03b8] = \u03b3\u2202iP\u03b8\u03a6\u03b8 + \u03b3P\u03b8\u2202i[\u03a6\u03b8]\u2212 \u2202i[\u03a6\u03b8] = \u03b3\u2202iP\u03b8\u03a6\u03b8 + \u03b3P\u03b8\u03a6:,i \u2212 \u03a6:,i,\nwhere \u03a6:,i is the ith column of \u03a6. Plugging this back into the MSPBE gradient we have\n\u2202iMSPBE(\u03b8) = 2 ( \u03a6>\u2202iD(T\u03b8Q\u03b8 \u2212Q\u03b8) + \u03a6>D(\u03b3\u2202iP\u03b8\u03a6\u03b8 + \u03b3P\u03b8\u03a6:,i \u2212 \u03a6:,i) )> (\u03a6>D\u03a6)\u22121 ( \u03a6>D(T\u03b8Q\u03b8 \u2212Q\u03b8) ) \u2212 ( \u03a6>D(T\u03b8Q\u03b8 \u2212Q\u03b8) )> (\u03a6>D\u03a6)\u22121(\u03a6>\u2202iD\u03a6)(\u03a6 >D\u03a6)\u22121 ( \u03a6>D(T\u03b8Q\u03b8 \u2212Q\u03b8)\n) = 2 ( \u03a6>\u2202iD(T\u03b8Q\u03b8 \u2212Q\u03b8) )> (\u03a6>D\u03a6)\u22121 ( \u03a6>D(T\u03b8Q\u03b8 \u2212Q\u03b8)\n) + 2 ( \u03a6>D(\u03b3\u2202iP\u03b8\u03a6\u03b8 + \u03b3P\u03b8\u03a6:,i \u2212 \u03a6:,i) )> (\u03a6>D\u03a6)\u22121 ( \u03a6>D(T\u03b8Q\u03b8 \u2212Q\u03b8)\n) \u2212 ( \u03a6>D(T\u03b8Q\u03b8 \u2212Q\u03b8) )> (\u03a6>D\u03a6)\u22121(\u03a6>\u2202iD\u03a6)(\u03a6 >D\u03a6)\u22121 ( \u03a6>D(T\u03b8Q\u03b8 \u2212Q\u03b8) ) . (5)"}, {"heading": "2.2 Sampling the Gradient", "text": "To derive a stochastic gradient descend algorithm we rewrite (5) as expectations. Let\nw = (\u03a6>D\u03a6)\u22121 ( \u03a6>D(T\u03b8Q\u03b8 \u2212Q\u03b8) ) = E [ \u03c6\u03c6> ]\u22121 E [\u03b4\u03c6] , where \u03c6 = \u03c6(s, a) with s, a \u223c ds,a and the TD-error being\n\u03b4 = r(s, a) + \u03b3 \u2211 a\u2032 \u03b8>\u03c6\u2032 \u2212 \u03b8>\u03c6\nwith \u03c6\u2032 = \u03c6s\u2032,a\u2032 and P{s\u2032|s, a} = t(s, a, s\u2032). This simplifies the partial derivative to\n\u2202iMSPBE(\u03b8) = 2 [( \u03a6>\u2202iD(T\u03b8Q\u03b8 \u2212Q\u03b8) )> + ( \u03a6>D(\u03b3\u2202iP\u03b8\u03a6\u03b8 + \u03b3P\u03b8\u03a6:,i \u2212 \u03a6:,i) )>] w\u2212w>(\u03a6>\u2202iD\u03a6)w.\n(6) For the first matrix term we have\n\u03a6>\u2202iD(T\u03b8Q\u03b8 \u2212Q\u03b8) = \u03a6>\u2202iD(R+ \u03b3P\u03b8Q\u03b8 \u2212Q\u03b8) = \u03a6>diag {\u2202ids,a}s,a (R+ \u03b3P\u03b8Q\u03b8 \u2212Q\u03b8)\n= \u03a6>diag {ds\u2202i\u03c0\u03b8(a|s)}s,a (R+ \u03b3P\u03b8Q\u03b8 \u2212Q\u03b8) = [ ds1\u2202i\u03c0\u03b8(a1|s1)\u03c6s1,a1 \u00b7 \u00b7 \u00b7 dsn\u2202i\u03c0\u03b8(am|sn)\u03c6sn,am ]  \u03b4s1,a1... \u03b4sn,am  = \u2211 s,a ds\u2202i\u03c0\u03b8(a|s)\u03c6>\u03b4\n= E [ \u2202i\u03c0\u03b8 \u03c0\u03b8 \u03b4\u03c6> ] ,\nwhere the expectation is over s, a \u223c ds,a, \u2202i\u03c0\u03b8/\u03c0\u03b8 = \u2202i\u03c0\u03b8(a|s)/\u03c0\u03b8(a|s), and P{s\u2032|s, a} = t(s, a, s\u2032) for the TD-error. For the second matrix term we denote the ith component of \u03c6 as \u03c6i. Expanding this term we have\n\u03a6>D(\u03b3\u2202iP\u03b8\u03a6\u03b8 + \u03b3P\u03b8\u03a6:,i \u2212 \u03a6:,i)\n= \u03a6>D \u03b3  \u2211 s\u2032,a\u2032 t(s1, a1, s \u2032)\u2202i\u03c0\u03b8(a \u2032|s\u2032)\u03b8>\u03c6\u2032\n...\u2211 s\u2032,a\u2032 t(sn, am, s \u2032)\u2202i\u03c0\u03b8(a \u2032|s\u2032)\u03b8>\u03c6\u2032\n+ \u03b3  \u2211 s\u2032,a\u2032 t(s1, a1, s \u2032)\u03c0\u03b8(a \u2032|s\u2032)\u03c6\u2032i\n...\u2211 s\u2032,a\u2032 t(sn, am, s \u2032)\u03c0\u03b8(a \u2032|s\u2032)\u03c6\u2032i\n\u2212  \u03c6 i s1,a1 ...\n\u03c6isn,am\n \n= \u03a6>D   \u03b3 \u2211 s\u2032,a\u2032 t(s1, a1, s \u2032)\u2202i\u03c0\u03b8(a \u2032|s\u2032)\u03b8>\u03c6\u2032 + \u03b3 \u2211 s\u2032,a\u2032 t(s1, a1, s \u2032)\u03c0\u03b8(a \u2032|s\u2032)\u03c6\u2032i \u2212 \u03c6is1,a1\n... \u03b3 \u2211 s\u2032,a\u2032 t(sn, am, s \u2032)\u2202i\u03c0\u03b8(a \u2032|s\u2032)\u03b8>\u03c6\u2032 + \u03b3 \u2211 s\u2032,a\u2032 t(sn, am, s \u2032)\u03c0\u03b8(a \u2032|s\u2032)\u03c6\u2032i \u2212 \u03c6isn,am\n \n= \u2211 s,a ds,a\u03c6 > \u03b3\u2211 s\u2032,a\u2032 t(s, a, s\u2032)\u2202i\u03c0\u03b8(a \u2032|s\u2032)\u03b8>\u03c6\u2032 + \u03b3 \u2211 s\u2032,a\u2032 t(s, a, s\u2032)\u03c0\u03b8(a \u2032|s\u2032)\u03c6\u2032i \u2212 \u03c6i  = E [( \u03b3E [ \u2202i\u03c0 \u2032 \u03b8\n\u03c0\u2032\u03b8 \u03b8>\u03c6\u2032\n] + \u03b3E [ \u03b8>\u03c6\u2032i ] \u2212 \u03c6i ) \u03c6> ] ,\nwhere \u2202i\u03c0 \u2032 \u03b8/\u03c0 \u2032 \u03b8 = \u2202i\u03c0\u03b8(a \u2032|s\u2032)/\u03c0\u03b8(a\u2032|s\u2032). For the third term we obtain\nw>(\u03a6>\u2202iD\u03a6)w = w > [\u03c6s1,a1 \u00b7 \u00b7 \u00b7 \u03c6sn,am] diag {ds\u2202i\u03c0\u03b8(a|s)}s,a  \u03c6s1,a1... \u03c6sn,am  w\n= w> (\u2211 s,a ds\u2202i\u03c0\u03b8(a|s)\u03c6\u03c6> ) w\n= w>E [ \u2202i\u03c0\u03b8 \u03c0\u03b8 \u03c6\u03c6> ] w.\nAssembling the MSPBE gradient then gives\n\u2212 1 2 \u2207\u03b8MSPBE(\u03b8)\n= \u2212 { E [ \u2207\u03b8\u03c0\u03b8 \u03c0\u03b8 \u03b4\u03c6\u2032> ] + E [( \u03b3E [ \u2207\u03b8\u03c0\u2032\u03b8 \u03c0\u2032\u03b8 \u03b8>\u03c6\u2032 ] + \u03b3E [\u03c6\u2032]\u2212 \u03c6 ) \u03c6> ]} w + 1 2 w>E [ \u2207\u03b8\u03c0\u03b8 \u03c0\u03b8 \u03c6\u03c6> ] w\n= \u2212E [ \u2207\u03b8\u03c0\u03b8 \u03c0\u03b8 \u03b4\u03c6> ] w \u2212 E [( \u03b3E [ \u2207\u03b8\u03c0\u2032\u03b8 \u03c0\u2032\u03b8 \u03b8>\u03c6\u2032 ] + \u03b3E [\u03c6\u2032]\u2212 \u03c6\u2032 ) \u03c6> ] w + 1 2 w>E [ \u2207\u03b8\u03c0\u03b8 \u03c0\u03b8 \u03c6\u03c6> ] w\n= E [ \u03c6\u03c6> ] E [ \u03c6\u03c6> ]\u22121 E [\u03b4\u03c6]\ufe38 \ufe37\ufe37 \ufe38 =w \u2212\u03b3E [ \u03c6\u2032\u03c6> ] w \u2212 E [ \u2207\u03b8\u03c0\u03b8 \u03c0\u03b8 \u03b4\u03c6> ] w \u2212 \u03b3E [ \u2207\u03b8\u03c0\u2032\u03b8 \u03c0\u2032\u03b8 \u03b8>\u03c6\u2032\u03c6> ] w + 1 2 w>E [ \u2207\u03b8\u03c0\u03b8 \u03c0\u03b8 \u03c6\u03c6> ] w\n= E [\u03b4\u03c6]\u2212 \u03b3E [ \u03c6\u2032\u03c6> ] w \u2212 E [ \u2207\u03b8\u03c0\u03b8 \u03c0\u03b8 \u03b4\u03c6> ] w \u2212 \u03b3E [ \u2207\u03b8\u03c0\u2032\u03b8 \u03c0\u2032\u03b8 \u03b8>\u03c6\u2032\u03c6> ] w + 1 2 w>E [ \u2207\u03b8\u03c0\u03b8 \u03c0\u03b8 \u03c6\u03c6> ] w (7)\nTo derive an iterative algorithm we follow [11] and derive two timescale update rules to learn the parameter vector \u03b8 and approximate the auxiliary weight vector w with\nw \u2190 w + \u03b2(\u03b4 \u2212 \u03c6>w)\u03c6. (8)\nSampling the gradient above then gives the update rule \u03b8 \u2190 \u03b8 + \u03b1 [ \u03b4\u03c6\u2212 \u03b3\u03c6\u2032\u03c6>w \u2212 \u2207\u03b8\u03c0\u03b8\n\u03c0\u03b8 \u03b4\u03c6>w \u2212 \u03b3\u2207\u03b8\u03c0 \u2032 \u03b8 \u03c0\u2032\u03b8 (\u03b8>\u03c6\u2032)(\u03c6>w) + 1 2 \u2207\u03b8\u03c0\u03b8 \u03c0\u03b8\n(w>\u03c6)2 ] . (9)\nNote that this update rule contains the standard TDC/GQ term plus correction terms that are in the direction of the policy gradient.\nAlgorithm 1 shows the resulting algorithm, which we call PGQ for Policy-Gradient Q-learning. This algorithm uses linear function approximation and updates are done in O(k), where k is the number of basis functions used. After making a transition, we do not want to sample the next action using the old parameter estimate and rather use the updated \u03b8 estimate. To do this we have to the calculate expected values \u03c6 and \u03c6 \u2207 analytically over the next possible actions.\nAlgorithm 1 Policy Gradient Q-learning Input: A transition sample (s, a, r, s\u2032).\n\u03c1\u2190 \u03c0\u03b8(a|s)b(a|s) \u03c1\u2207 \u2190 \u2207\u03b8\u03c0\u03b8(a|s)b(a|s) \u03c6\u2190 \u2211 a\u2032 \u03c0\u03b8(a\n\u2032|s\u2032)\u03c6(s\u2032, a\u2032) \u03c6 \u2207 \u2190 \u2211 a\u2032 \u2207\u03b8\u03c0\u03b8(a\u2032|s\u2032)\u03b8>\u03c6(s\u2032, a\u2032) \u03b4 \u2190 r + \u03b3\u03b8>\u03c6\u2212 \u03b8>\u03c6 \u03b8 \u2190 \u03b8 + \u03b1 ( \u03b4\u03c6\u2212 \u03b3\u03c6\u2032(\u03c6>w)\u2212 \u03c1\u2207\u03b4(\u03c6>w)\u2212 \u03b3\u03c6\u2207(\u03c6>w) + 12 (w >\u03c6)2 ) w \u2190 w + \u03b2(\u03b4 \u2212 \u03c6>w)\u03c6"}, {"heading": "3 Baird Counter Example", "text": "We have tested our method on the \u201dstar\u201d Baird counter example [1] and compared it with Q-learning and GQ [5]. For this 7 state version divergence of Q-learning is monotonic and GQ is known to converge [7]. We initialize the parameter vector \u03b8 corresponding to the action that transitions to the 7th centre state with (1, 1, 1, 1, 1, 1, 1, 10) and the remaining parameter entries with 1. The discount factor is set to \u03b3 = 0.99. In our experiments we do not assume a hard-coded policy that ensures uniform exploration over state-action pairs but look at the control case where actions are selected using a Boltzmann policy where the probability of selecting a specific action is\n\u03c0\u03b8(a|s) = exp(\u03b8>\u03c6(s, a)/\u03c4)\u2211 b exp(\u03b8 >\u03c6(s, b)/\u03c4) . (10)\nUpdating was done either through sampling transitions (s, a, s\u2032, r) according to hard coded distributions or either through simulating trajectories through the MDP. For the sampled version we have sampled the state s according to a uniform distribution over all 7 states, the action a was sampled with probability \u03c0\u03b8(a|s) and the next state s\u2032 was sampled according to the transition model. Figure 1 shows the MSPBE error for the sampled update experiment. Q-learning diverges monotonically and both GQ and PGQ converge to a zero MSPBE.\nFor the trajectory based experiments we have sampled one of the seven start states uniformly and then executed transitions through the MDP. While transitioning or updating the parameter vector we have measured the MSPBE using a uniform stationary distribution over states. Figure 2 shows the MSPBE and the Mean Squared TD-error (MSTDE) defined in [2] of the parameter vector \u03b8 at each step of the simulation."}, {"heading": "4 Conclusion", "text": "We have presented a new gradient based TD-learning algorithm that incorporates policy gradients. The resulting algorithm is similar to GQ/TDC but also has a correction term in the direction of the gradient of the target policy. Our analysis assumes a dependency of the Markov chain on the\nparameter vector \u03b8 through the target policy. This allows our algorithm to correctly step over a sequence of different Markov chains and account for the drift in the distribution from which transition data is sampled due to changes in the parameter vector.\nOne next research direction is to extend this method to the non-linear function approximation case. Maei [6] present the first gradient based TD algorithm that converges in this case. One may able to draw on their results for our work. For the derivation of our algorithm we only assumed the Bellman operator to be parametric in the parameter estimate, which lead to the additional policy gradient terms. No further assumptions were made on the Bellman operator and the value function terms in the MSPBE objective, so in the non-linear function approximation case one would obtain gradients of the value function here. However, one would have to analyze the projection operator in the MSPBE objective differently."}], "references": [{"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["Leemon Baird"], "venue": "In Proceedings of the Twelfth International Conference on Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "Policy evaluation with temporal differences: A survey and comparison", "author": ["Christoph Dann", "Gerhard Neumann", "Jan Peters"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Chattering in sarsa(lambda) - a cmu learning lab internal report", "author": ["Geoffrey J. Gordon"], "venue": "Technical report,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Reinforcement learning with function approximation converges to a region", "author": ["Geoffrey J. Gordon"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Gq(lambda): A general gradient algorithm for temporal-difference prediction learning with eligibility traces", "author": ["H.R. Sutton R.S. Maei"], "venue": "Proceedings of the Third Conference on Artificial General Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Convergent temporaldifference learning with arbitrary smooth function approximation", "author": [], "venue": "In Advances in Neural Information Processing Systems 22,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Toward off-policy learning control with function approximation", "author": [], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "A convergent form of approximate policy iteration", "author": ["Theodore J. Perkins", "Doina Precup"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Learning to predict by the methods of temporal differences", "author": ["Richard S. Sutton"], "venue": "Mach. Learn.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1988}, {"title": "Reinforcement Learning: An Introduction", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S. Sutton", "David McAllester", "Satinder Singh", "Yishay Mansour"], "venue": "In IN ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["Richard S. Sutton", "Hamid Reza Maei", "Doina Precup", "Shalabh Bhatnagar", "David Silver", "Csaba Szepesv\u00e1ri", "Eric Wiewiora"], "venue": "Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}], "referenceMentions": [{"referenceID": 11, "context": "Gradient-based off-policy learning algorithms, such as GTD and TDC/GQ [13], converge even when using function approximation and incremental updates.", "startOffset": 70, "endOffset": 74}, {"referenceID": 8, "context": "One fundamental concept in Reinforcement Learning (RL) is Temporal Difference (TD) learning introduced by Sutton [9].", "startOffset": 113, "endOffset": 116}, {"referenceID": 11, "context": "Recently, gradient-based off-policy learning algorithms were introduced such as GTD [11] and TDC [13] which are also proven to be convergent under off-policy learning with linear value function approximation.", "startOffset": 97, "endOffset": 101}, {"referenceID": 4, "context": "The extension to Q-learning, GQ(\u03bb) [5], is also convergent under off-policy learning but only if the control policy is fixed.", "startOffset": 35, "endOffset": 38}, {"referenceID": 3, "context": "SARSA also suffers from this problem and is only guaranteed to converge to a sub-space of policies [4, 3].", "startOffset": 99, "endOffset": 105}, {"referenceID": 2, "context": "SARSA also suffers from this problem and is only guaranteed to converge to a sub-space of policies [4, 3].", "startOffset": 99, "endOffset": 105}, {"referenceID": 10, "context": "Similar to the policy gradient framework [12] we directly analyze the interaction between the policy gradient and the distribution from which transitions are sampled.", "startOffset": 41, "endOffset": 45}, {"referenceID": 7, "context": "methods such as [8].", "startOffset": 16, "endOffset": 19}, {"referenceID": 11, "context": "As in [13, 5] we consider the linear function approximation case with a basis function \u03c6 : S \u00d7A \u2192 R and define the state-action value function as Q\u03b8(s, a) = \u03b8 >\u03c6(s, a) \u2248 Q(s, a) = E [ \u221e \u2211", "startOffset": 6, "endOffset": 13}, {"referenceID": 4, "context": "As in [13, 5] we consider the linear function approximation case with a basis function \u03c6 : S \u00d7A \u2192 R and define the state-action value function as Q\u03b8(s, a) = \u03b8 >\u03c6(s, a) \u2248 Q(s, a) = E [ \u221e \u2211", "startOffset": 6, "endOffset": 13}, {"referenceID": 11, "context": "The Mean Squared Projected Bellman Error introduced by [13] is MSPBE(\u03b8) = ||Q\u03b8 \u2212\u03a0T\u03b8Q\u03b8||D, (2) where \u03a0 = \u03a6(\u03a6D\u03a6)\u22121\u03a6>D is the projection matrix and the Bellman operator applied to the action value function is defined as T\u03b8Q\u03b8 def = R+ \u03b3P\u03b8Q\u03b8.", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "1 Gradient Derivation To obtain the gradient of the MSPBE objective, [13] have shown", "startOffset": 69, "endOffset": 73}, {"referenceID": 0, "context": "We have tested our method on the \u201dstar\u201d Baird counter example [1] and compared it with Q-learning and GQ [5].", "startOffset": 62, "endOffset": 65}, {"referenceID": 4, "context": "We have tested our method on the \u201dstar\u201d Baird counter example [1] and compared it with Q-learning and GQ [5].", "startOffset": 105, "endOffset": 108}, {"referenceID": 6, "context": "For this 7 state version divergence of Q-learning is monotonic and GQ is known to converge [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "Figure 2 shows the MSPBE and the Mean Squared TD-error (MSTDE) defined in [2] of the parameter vector \u03b8 at each step of the simulation.", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "Maei [6] present the first gradient based TD algorithm that converges in this case.", "startOffset": 5, "endOffset": 8}], "year": 2015, "abstractText": "Off-policy learning refers to the problem of learning the value function of a way of behaving, or policy, while following a different policy. Gradient-based off-policy learning algorithms, such as GTD and TDC/GQ [13], converge even when using function approximation and incremental updates. However, they have been developed for the case of a fixed behavior policy. In control problems, one would like to adapt the behavior policy over time to become more greedy with respect to the existing value function. In this paper, we present the first gradient-based learning algorithms for this problem, which rely on the framework of policy gradient in order to modify the behavior policy. We present derivations of the algorithms, a convergence theorem, and empirical evidence showing that they compare favorably to existing approaches.", "creator": "LaTeX with hyperref package"}}}