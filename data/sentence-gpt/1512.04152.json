{"id": "1512.04152", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Dec-2015", "title": "Fighting Bandits with a New Kind of Smoothness", "abstract": "We define a novel family of algorithms for the adversarial multi-armed bandit problem, and provide a simple analysis technique based on convex smoothing. We prove two main results. First, we show that regularization via the \\emph{Tsallis entropy}, which includes EXP3 as a special case, achieves the $\\Theta(\\sqrt{TN})$ minimax regret. Second, we show that a wide class of perturbation methods achieve a near-optimal regret as low as $O(\\sqrt{TN \\log N})$ if the perturbation distribution has a bounded hazard rate. For example, the Gumbel, Weibull, Frechet, Pareto, and Gamma distributions all satisfy this key property. We also show that the \\(\\theta(\\frac{t}(N)}$ and \\theta(\\frac{t}(N)}$ have a large class of perturbations, but not the best-fitting function of each. Finally, the groupings are shown in the Fig. 3a.\n\n\n\nTo achieve our work, we generate a small but powerful network of distributed adversarial quantum networks (with the support of two large groups of perturbations) to generate a series of randomly generated random images. We show that there are at least 3,000 network-level networks that can be used for the problem. By analyzing the network-level features, we show that the ensemble and network-level network-level network-level networks are relatively common. The network-level network-level network-level network-level network-level networks are relatively easy to solve because it can be used to solve problem-specific, distributed adversarial quantum networks.\n\n\nIn addition to the robust network-level network-level networks, the ensemble and network-level networks are a complex, distributed and multi-dimensional, distributed and heterogeneous. The clustering and homogeneous networks is a major reason for the growth of distributed adversarial quantum networks. In the late 20th century, in a number of ways, such as network-level networks, many large-scale and multi-dimensional network-level networks were able to be generalized to be distributed in any sense. A network-level network-level network-level network-level network-level network-level network-level network-level-level-network-level-network-level-network-level-network-level-network-level-network-level-network-level-network", "histories": [["v1", "Mon, 14 Dec 2015 01:57:02 GMT  (28kb,D)", "http://arxiv.org/abs/1512.04152v1", "In Proceedings of NIPS, 2015"]], "COMMENTS": "In Proceedings of NIPS, 2015", "reviews": [], "SUBJECTS": "cs.LG cs.GT stat.ML", "authors": ["jacob d abernethy", "chansoo lee", "ambuj tewari"], "accepted": true, "id": "1512.04152"}, "pdf": {"name": "1512.04152.pdf", "metadata": {"source": "CRF", "title": "Fighting Bandits with a New Kind of Smoothness", "authors": ["Jacob Abernethy", "Chansoo Lee", "Ambuj Tewari"], "emails": ["jabernet@umich.edu", "chansool@umich.edu", "tewaria@umich.edu"], "sections": [{"heading": null, "text": "\u221a TN) minimax\nregret. Second, we show that a wide class of perturbation methods achieve a near-optimal regret as low as O( \u221a TN logN) if the perturbation distribution has a bounded hazard rate. For example, the Gumbel, Weibull, Frechet, Pareto, and Gamma distributions all satisfy this key property."}, {"heading": "1 Introduction", "text": "The classic multi-armed bandit (MAB) problem, generally attributed to the early work of Robbins (1952), poses a generic online decision scenario in which an agent must make a sequence of choices from a fixed set of options. After each decision is made, the agent receives some feedback in the form of a loss (or gain) associated with her choice, but no information is provided on the outcomes of alternative options. The agent\u2019s goal is to minimize the total loss over time, and the agent is thus faced with the balancing act of both experimenting with the menu of choices while also utilizing the data gathered in the process to improve her decisions. The MAB framework is not only mathematically elegant, but useful for a wide range of applications including medical experiments design (Gittins, 1996), automated poker playing strategies (Van den Broeck et al., 2009), and hyperparameter tuning (Pacula et al., 2012).\nEarly MAB results relied on stochastic assumptions (e.g., IID) on the loss sequence (Gittins et al., 2011; Lai and Robbins, 1985; Auer et al., 2002). As researchers began to establish non-stochastic, worst-case guarantees for sequential decision problems such as prediction with expert advice (Littlestone and Warmuth, 1994), a natural question arose as to whether similar guarantees were possible for the bandit setting. The pioneering work of Auer, Cesa-Bianchi, Freund, and Schapire (2003) answered this in the affirmative by showing that their algorithm EXP3 possesses nearly-optimal regret bounds with matching lower bounds. Attention later turned to the bandit version of online linear optimization, and several associated guarantees were published the following decade (McMahan and Blum, 2004; Flaxman et al., 2005; Dani and Hayes, 2006; Dani et al., 2008; Abernethy et al., 2012).\nNearly all proposed methods have relied on a particular algorithmic blueprint; they reduce the bandit problem to the full-information setting, while using randomization to make decisions and to estimate the losses. A well-studied family of algorithms for the full-information setting is Follow the Regularized Leader (FTRL), which optimizes the objective function of the following form:\narg min x\u2208K\n\u3008L, x\u3009+ \u03bbR(x) (1)\nwhere K is the decision set, L is (an estimate of) the cumulative loss vector, and R is a regularizer, a convex function with suitable curvature to stabilize the objective. The choice of regularizer R is critical to the algorithm\u2019s performance. For example, the EXP3 algorithm (Auer, 2003) regularizes with the entropy function and achieves a nearly optimal regret bound when K is the probability sim-\nar X\niv :1\n51 2.\n04 15\n2v 1\n[ cs\n.L G\n] 1\n4 D\nec 2\n01 5\nplex. For a general convex set, however, other regularizers such as self-concordant barrier functions (Abernethy et al., 2012) have tighter regret bounds.\nAnother class of algorithms for the full information setting is Follow the Perturbed Leader (FTPL) (Kalai and Vempala, 2005) whose foundations date back to the earliest work in adversarial online learning (Hannan, 1957). Here we choose a distribution D on RN , sample a random vector Z \u223c D, and solve the following linear optimization problem\narg min x\u2208K\n\u3008L+ Z, x\u3009 (2)\nFTPL is computationally simpler than FTRL due to the linearity of the objective, but it is analytically much more complex due to the randomness. For every different choice of D, an entirely new set of techniques had to be developed (Devroye et al., 2013; Van Erven et al., 2014). Rakhlin et al. (2012) and Abernethy et al. (2014) made some progress towards unifying the analysis framework. Their techniques, however, are limited to the full-information setting.\nIn this paper, we propose a new analysis framework for the multi-armed bandit problem that unifies the regularization and perturbation algorithms. The key element is a new kind of smoothness property, which we call differential consistency. It allows us to generate a wide class of both optimal and near-optimal algorithms for the adversarial multi-armed bandit problem. We summarize our main results:\n1. We show that regularization via the Tsallis entropy leads to the state-of-the-art adversarial MAB algorithm, matching the minimax regret rate of Audibert and Bubeck (2009) with a tighter constant. Interestingly, our algorithm fully generalizes EXP3.\n2. We show that a wide array of well-studied noise distributions lead to near-optimal regret bounds (matching those of EXP3). Furthermore, our analysis reveals a strikingly simple and appealing sufficient condition for achieving O( \u221a T ) regret: the hazard rate of the\nnoise distribution must be bounded by a constant in the tail region. We conjecture that this requirement is in fact both necessary and sufficient."}, {"heading": "2 Gradient-Based Prediction Algorithms for the Multi-Armed Bandit", "text": "Let us now introduce the adversarial multi-armed bandit problem. On each round t = 1, . . . , T , a learner must choose a distribution pt \u2208 \u2206N over the set of N available actions. The adversary (Nature) chooses a loss vector gt \u2208 [\u22121, 0]N . The learner plays action it sampled according to pt and suffers the loss gt,it . The learner observes only a single coordinate gt,it and receives no information as to the values gt,j for j 6= it. This limited information feedback is what makes the bandit problem much more challenging than the full-information setting in which the entire gt is observed.\nThe learner\u2019s goal is to minimize the regret. Regret is defined to be the difference in the realized loss and the loss of the best fixed action in hindsight:\nRegretT := max i\u2208[N ] T\u2211 t=1 (gt,i \u2212 gt,it). (3)\nTo be precise, we consider the expected regret, where the expectation is taken with respect to the learner\u2019s randomization.\nLoss vs. Gain Note: The maximization in (3) would imply that g is strictly speaking a negative gain. Nevertheless, we use the term loss, as we impose the assumption that gt \u2208 [\u22121, 0]N throughout the paper."}, {"heading": "2.1 The Gradient-Based Algorithmic Template", "text": "We study a particular algorithmic template described in Framework 1, which is a slight variation of the Gradient Based Prediction Algorithm (GBPA) of Abernethy et al. (2014). Note that the algorithm (i) maintains an unbiased estimate of the cumulative losses G\u0302t, (ii) updates G\u0302t by adding a singleround estimate g\u0302t, and (iii) uses the gradient of a convex function \u03a6\u0303 as sampling distribution pt. The choice of \u03a6\u0303 is flexible, it must be a differentiable convex function such that its gradient is always a probability distribution.\nFramework 1 may appear restrictive but it has served as the basis for much of the published work on adversarial MAB algorithms (Auer et al., 2003; Kujala and Elomaa, 2005; Neu and Barto\u0301k, 2013) mainly for two reasons. First, the GBPA framework encompasses all FTRL and FTPL algorithms, which are the core techniques for sequential prediction algorithms (Abernethy et al., 2014). Second, although there is some flexibility, any unbiased estimation scheme would require some kind of inverse-probability scaling. Information theory tells us that the unbiased estimates of a quantity that is observed with only probabilty p must necessarily involve fluctuations that scale as O(1/p).\nFramework 1: Gradient-Based Prediction Alg. (GBPA) Template for Multi-Armed Bandits. GBPA(\u03a6\u0303): \u03a6\u0303 is a differentiable convex function such that\u2207\u03a6\u0303 \u2208 \u2206N and \u2207i\u03a6\u0303 > 0 for all i. Initialize G\u03020 = 0 for t = 1 to T do\nNature: A loss vector gt \u2208 [\u22121, 0]N is chosen by the Adversary Sampling: Learner chooses it according to the distribution p(G\u0302t\u22121) = \u2207\u03a6t(G\u0302t\u22121) Cost: Learner \u201cgains\u201d loss gt,it Estimation: Learner \u201cguesses\u201d g\u0302t :=\ngt,it pit (G\u0302t\u22121) eit\nUpdate: G\u0302t = G\u0302t\u22121 + g\u0302t\nLemma 2.1. Define \u03a6(G) \u2261 maxiGi so that we can write the expected regret of GBPA(\u03a6\u0303) as ERegretT = \u03a6(GT )\u2212 \u2211T t=1\u3008\u2207\u03a6\u0303(G\u0302t\u22121), gt\u3009.\nThen, the expected regret of GBPA(\u03a6\u0303) can be written as:\nERegretT \u2264 \u03a6\u0303(0)\u2212 \u03a6(0)\ufe38 \ufe37\ufe37 \ufe38 overestimation penalty\n+Ei1,...,it\u22121 [\n\u03a6(G\u0302T )\u2212 \u03a6\u0303(G\u0302T )\ufe38 \ufe37\ufe37 \ufe38 underestimation penalty + T\u2211 t=1 Eit [D\u03a6\u0303(G\u0302t, G\u0302t\u22121)|G\u0302t\u22121]\ufe38 \ufe37\ufe37 \ufe38 divergence penalty ] ,\n(4) where the expectations are over the sampling of it.\nProof. Let \u03a6\u0303 be a valid convex function for GBPA. Consider GBPA(\u03a6\u0303) run on the loss sequence g1, . . . , gT . The algorithm produces a sequence of estimated losses g\u03021, . . . , g\u0302T . Now consider GBPA-FI(\u03a6\u0303), which is GBPA(\u03a6\u0303) run with the full information on the deterministic loss sequence g\u03021, . . . , g\u0302T (there is no estimation step, and the learner updates G\u0302t directly). The regret of this run can be written as\n\u03a6(G\u0302T )\u2212 \u2211T t=1\u3008\u2207\u03a6\u0303(G\u0302t\u22121), g\u0302t\u3009 (5)\nand \u03a6(GT ) \u2264 \u03a6(G\u0302T ) by the convexity of \u03a6. Hence, Equation 5 is an upper bound the regret. The rest of the proof is a fairly well-known result in online learning literature; see, for example, (CesaBianchi and Lugosi, 2006, Theorem 11.6) or (Abernethy et al., 2014, Section 2). For completeness, we included the full proof in Appendix A."}, {"heading": "2.2 A New Kind of Smoothness", "text": "What has emerged as a guiding principle throughout machine learning is that the stability of an algorithm leads to performance guarantees\u2014that is, small modifications of the input data should not\ndramatically alter the output. In the context of GBPA, algorithm\u2019s output (prediction in each time step) is by definition the dervative \u2207\u03a6\u0303, and its stability corresponds to the Lipschitz-continuity of the gradient. Abernethy et al. (2014) proved that a uniform bound the norm of\u22072\u03a6\u0303 directly gives a regret guarantee for the full-information setting.\nIn the bandit setting, however, a uniform bound on \u22072\u03a6\u0303 is insufficient; the regret (Lemma 2.1) involves terms of the form D\u03a6\u0303(G\u0302t\u22121 + g\u0302t, G\u0302t\u22121), where the incremental quantity g\u0302t can scale as large as the inverse of the smallest probability of p(G\u0302t\u22121). What is needed is a stronger notion of the smoothness that bounds\u22072\u03a6\u0303 in correspondence with\u2207\u03a6\u0303, and we propose the following definition: Definition 2.2 (Differential Consistency). For constants \u03b3,C > 0, we say that a convex function \u03a6\u0303(\u00b7) is (\u03b3,C)-differentially-consistent if for all G \u2208 (\u2212\u221e, 0]N ,\n\u22072ii\u03a6\u0303(G) \u2264 C(\u2207i\u03a6\u0303(G))\u03b3 .\nIn other words, the rate in which we decrease pi should approach 0 as pi approaches 0. This gurantees that the algorithm continues to explore. We now prove a generic bound that we will use in the following two sections to derive regret guarantees.\nTheorem 2.3. Suppose \u03a6\u0303 is (\u03b3,C)-differentially-consistent for constants C, \u03b3 > 0. Then divergence penalty at time t in Lemma 2.1 can be upper bounded as:\nEit [D\u03a6\u0303(G\u0302t, G\u0302t\u22121)|G\u0302t\u22121] \u2264 C\n2 N\u2211 i=1 ( \u2207i\u03a6\u0303(G\u0302t\u22121) )\u03b3\u22121 .\nProof. For the sake of clarity, we drop the subscripts; we use G\u0302 to denote the cumulative estimate G\u0302t\u22121, g\u0302 to denote the marginal estimate g\u0302t = G\u0302t \u2212 G\u0302t\u22121, and g to denote the true loss gt. Note that by definition of Algorithm 1, g\u0302 is a sparse vector with one non-zero and non-positive coordinate g\u0302it = gt,i/\u2207i\u03a6\u0303(G\u0302). Plus, it is conditionally independent given G\u0302. For a fixed it, Let\nh(r) := D\u03a6\u0303(G\u0302+ rg\u0302/\u2016g\u0302\u2016, G\u0302) = D\u03a6\u0303(G\u0302+ reit , G\u0302), so that h\u2032\u2032(r) = (g\u0302/\u2016g\u0302\u2016)>\u22072\u03a6\u0303 ( G\u0302+ tg\u0302/\u2016g\u0302\u2016 ) (g\u0302/\u2016g\u0302\u2016) = e>it\u2207 2\u03a6\u0303 ( G\u0302\u2212 teit ) eit . Now we write:\nEit [D\u03a6\u0303(G\u0302+ g\u0302, G\u0302)|G\u0302] = \u2211N i=1 P[it = i] \u222b \u2016g\u0302\u2016 0 \u222b s 0 h\u2032\u2032(r) dr ds\n= \u2211N i=1\u2207i\u03a6\u0303(G\u0302) \u222b \u2016g\u0302\u2016 0 \u222b s 0 e>i \u22072\u03a6\u0303 ( G\u0302\u2212 rei ) ei dr ds\n\u2264 \u2211N i=1\u2207i\u03a6\u0303(G\u0302) \u222b \u2016g\u0302\u2016 0 \u222b s 0 C ( \u2207i\u03a6\u0303(G\u0302\u2212 rei) )\u03b3 dr ds\n\u2264 \u2211N i=1\u2207i\u03a6\u0303(G\u0302) \u222b \u2016g\u0302\u2016 0 \u222b s 0 C ( \u2207i\u03a6\u0303(G\u0302) )\u03b3 dr ds\n= C \u2211N i=1 ( \u2207i\u03a6\u0303(G\u0302) )1+\u03b3 \u222b \u2016g\u0302\u2016 0 \u222b s 0 dr ds\n= C2 \u2211N i=1 ( \u2207i\u03a6\u0303(G\u0302) )\u03b3\u22121 g2i .\nThe first inequality is by the supposition. The second inequality is due to the convexity of \u03a6\u0303 which guarantees that\u2207i is an increasing function in the i-th coordinate; this step critically depends on the loss-only assumption that g is always non-positive."}, {"heading": "3 A Minimax Bandit Algorithm via Tsallis Smoothing", "text": "Auer et al. (2003) proved that their EXP3 algorithm achieves O( \u221a TN logN) regret and that any multi-armed bandit algorithm suffers \u2126( \u221a TN) regret. A few years later, Audibert and Bubeck (2009) resolved this gap with Implicitly Normalized Forecaster (INF), which later was shown to be equivalent to Mirror Descent (Audibert et al., 2011) on the probability simplex. EXP3 corresponds to INF with potential function \u03c8(x) = exp(\u03b7x), while using \u03c8(x) = (\u2212\u03b7x)\u2212q with q > 1 gives an optimal algorithm that has regret at most 2 \u221a 2TN (Bubeck and Cesa-Bianchi, 2012, Theorem 5.7).\nWhat we present in this section is essentially a reformulation of a particular subfamily of INF, which includes INF with the above two potential functions. Our reformulation leads to a very simple and intuitive analysis based on differential consistency, and a natural interpolation between the two seemingly unrelated potential functions.\nLet us first note that EXP3 is an instance of GBPA where the potential function \u03a6\u0303(\u00b7) is the Fenchel conjugate of the Shannon entropy. For any p \u2208 \u2206N , the (negative) Shannon entropy is defined as H(p) := \u2211 i pi log pi, and its Fenchel conjugate is H\n?(G) = supp\u2208\u2206N {\u3008p,G\u3009 \u2212 \u03b7H(p)}. In fact, we have a closed-form expression for the supremum: H?(G) = 1\u03b7 log ( \u2211 i exp(\u03b7Gi)) . By inspecting the gradient of the above expression, it is easy to see that EXP3 chooses the distribution pt = \u2207H?(G) every round. Now we will replace the Shannon entropy with the Tsallis entropy1 (Tsallis, 1988), defined as:\nS\u03b1(p) = 1\n1\u2212 \u03b1\n( 1\u2212\nN\u2211 i=1 p\u03b1i\n) for 0 < \u03b1 < 1.\nInterestingly, the Shannon entropy is an asymptotic special case of the Tsallis entropy, i.e.,\nS\u03b1(\u00b7)\u2192 H(\u00b7) as \u03b1\u2192 1.\nTheorem 3.1. Let \u03a6\u0303(G) = maxp\u2208\u2206N {\u3008p,G\u3009 \u2212 \u03b7S\u03b1(p)}. Then the GBPA(\u03a6\u0303) has regret at most\nERegret \u2264 \u03b7N 1\u2212\u03b1 \u2212 1 1\u2212 \u03b1\ufe38 \ufe37\ufe37 \ufe38\noverestimation penalty\n+ N\u03b1T\n2\u03b7\u03b1\ufe38 \ufe37\ufe37 \ufe38 divergence penalty . (6)\nBefore proving the theorem, we note that it immediately recovers the EXP3 upper bound as a special case \u03b1 \u2192 1. An easy application of L\u2019Ho\u0302pital\u2019s rule shows that as \u03b1 \u2192 1, N\n1\u2212\u03b1\u22121 1\u2212\u03b1 \u2192 logN and N\u03b1/\u03b1 \u2192 N . Choosing \u03b7 = \u221a\n(N logN)/T , we see that the right-hand side of (6) tends to 2 \u221a TN logN . However the choice \u03b1 \u2192 1 is clearly not the optimal choice, as we show in the following statement, which directly follows from the theorem once we see that N1\u2212\u03b1\u2212 1 < N1\u2212\u03b1.\nCorollary 3.2. For any \u03b1 \u2208 (0, 1), if we choose \u03b7 = \u221a\nT (1\u2212\u03b1) 2\u03b1 N \u03b1\u2212 12 then we have\nERegret \u2264 \u221a\n2TN \u03b1(1\u2212\u03b1) .\nIn particular, the choice of \u03b1 = 12 gives a regret of no more than 2 \u221a\n2TN , recovering (Bubeck and Cesa-Bianchi, 2012, Theorem 5.7).\nProof of Theorem 3.1. We will bound each penalty term in Lemma 2.1. Since S\u03b1 is non-positive, the underestimation penalty is upper bounded by 0 and the overestimation penalty is at most (\u2212minS\u03b1). The minimum of S\u03b1 occurs at (1/N, . . . , 1/N). Hence,\n(overestimation penalty) \u2264 \u2212 \u03b7 1\u2212 \u03b1\n( 1\u2212\nN\u2211 i=1 1 N\u03b1\n) \u2264 \u03b7N\n1\u2212\u03b1 \u2212 1 1\u2212 \u03b1 . (7)\nNow it remains to upper bound the divergence penalty. Straightforward calculus gives\n\u22072S\u03b1(p) = \u03b7\u03b1diag(p\u03b1\u221221 , . . . , p \u03b1\u22122 N ).\nLet I\u2206N (\u00b7) be the function where I\u2206N (x) = 0 for x \u2208 \u2206N and I\u2206N (x) =\u221e for x /\u2208 \u2206N . Define a function S\u0302\u03b1(\u00b7) := S\u03b1(\u00b7) + I\u2206N (\u00b7), which is the convex conjugate of \u03a6\u0303. Following the setup of Penot (1994), \u22072S\u03b1(p) is a sub-hessian of S\u0302\u03b1(p). We now apply Proposition 3.2 of the same reference. Let (pG, G) be a pair such that \u2207\u03a6\u0303(G) = pG. Since \u22072S\u03b1(p) is invertible, it follows that (\u22072S\u03b1(pG))\u22121 is a super-hessian of \u03a6\u0303 at G. Hence, for any G,\n\u22072\u03a6\u0303(G) (\u03b7\u03b1)\u22121diag ( (pG) 2\u2212\u03b1 1 , . . . , (pG) 2\u2212\u03b1 N (G) ) .\n1More precisely, the function we give here is the negative Tsallis entropy according to its original definition.\nThat is, \u03a6\u0303 is (2\u2212 \u03b1, (\u03b7\u03b1)\u22121)-differentially-consistent, and thus applying Theorem 2.3 gives\nD\u03a6\u0303(G\u0302t, G\u0302t\u22121) \u2264 (2\u03b7\u03b1) \u22121 N\u2211 i=1 ( pi(G\u0302t\u22121) )1\u2212\u03b1 .\nSince the 1\u03b1 -norm and the 1 1\u2212\u03b1 -norm are dual to each other, we can apply Ho\u0308lder\u2019s inequality to any probability distribution p1, . . . , pN and obtain\nN\u2211 i=1 p1\u2212\u03b1i = N\u2211 i=1 p1\u2212\u03b1i \u00b7 1 \u2264 ( N\u2211 i=1 p 1\u2212\u03b1 1\u2212\u03b1 i )1\u2212\u03b1( N\u2211 i=1 1 1 \u03b1 )\u03b1 = (1)1\u2212\u03b1N\u03b1 = N\u03b1.\nSo, the divergence penalty is at most (2\u03b7\u03b1)\u22121N\u03b1, which completes the proof."}, {"heading": "4 Near-Optimal Bandit Algorithms via Stochastic Smoothing", "text": "Let D be a continuous distribution over an unbounded support with probability density function f and cumulative density function F . Consider the GBPA with potential function of the form:\n\u03a6\u0303(G;D) = E Z1,...,ZN iid\u223cD max i {Gi + Zi} (8)\nwhich is a stochastic smoothing of (maxiGi) function. Since the max function is convex, \u03a6\u0303 is also convex. By Bertsekas (1973), we can swap the order of differentiation and expectation:\n\u2207\u03a6\u0303(G;D) = E Z1,...,ZN iid\u223cD ei\u2217 , where i\u2217 = arg max i=1,...,N {Gi + Zi}. (9)\nEven if the function is not differentiable everywhere, the swapping is still possible with any subgradient under some mild conditions. Hence, the ties between coordinates (which happen with probability zero anyways) can be resolved in an arbitrary manner. It is clear that \u2207\u03a6\u0303 is in the probability simplex, and note that\n\u2202\u03a6\u0303 \u2202Gi = EZ1,...,ZN1{Gi + Zi > Gj + Zj ,\u2200j 6= i}\n= EG\u0303j\u2217 [PZi [Zi > G\u0303j\u2217 \u2212Gi]] = EG\u0303j\u2217 [1\u2212 F (G\u0303j\u2217 \u2212Gi)] (10)\nwhere G\u0303j\u2217 = maxj 6=iGj + Zj . The unbounded support condition guarantees that this partial derivative is non-zero for all i given any G. So, \u03a6\u0303(G;D) satisfies the requirements of Algorithm 1. Despite the fact that perturbation-based algorithms provide a natural randomized decision strategy, they have seen little applications mostly because they are hard to analyze. But one should expect general results to be within reach: the EXP3 algorithm, for example, can be viewed through the lens of perturbations, where the noise is distributed according to the Gumbel distribution. Indeed, an early result of Kujala and Elomaa (2005) showed that a near-optimal MAB strategy comes about through the use of exponentially-distributed noise, and the same perturbation strategy has more recently been utilized in the work of Neu and Barto\u0301k (2013) and Koca\u0301k et al. (2014). However, a more general understanding of perturbation methods has remained elusive. For example, would Gaussian noise be sufficient for a guarantee? What about, say, the Weibull distribution?"}, {"heading": "4.1 Connection to Follow the Perturbed Leader", "text": "The sampling step of the bandit GBPA (Framework 1) with a stochastically smoothed function (Equation 8) can be done efficiently. Instead of evaluating the expectation (Equation 9), we simply take a random sample. In fact, this is equivalent to Follow the Perturbed Leader Algorithm (FTPL) (Kalai and Vempala, 2005) applied to the bandit setting. On the other hand, the estimation step is hard because generally there is no closed-form expression for\u2207\u03a6\u0303. To address this issue, Neu and Barto\u0301k (2013) proposed Geometric Resampling (GR). GR uses an iterative resampling process to estimate \u2207\u03a6\u0303. They showed that if we stop after M iterations, the extra regret due to the estimation bias is at most NTeM (additive term). That is, all our GBPA regret bounds in this section hold for the corresponding FTPL algorithm with an extra additive NTeM term.. This term, however, does not affect the asymptotic regret rate as long as M = NT , because the lower bound for any algorithm is of the order \u221a NT ."}, {"heading": "4.2 Hazard Rate analysis", "text": "In this section, we show that the performance of the GBPA(\u03a6\u0303(G;D)) can be characterized by the hazard function of the smoothing distribution D. The hazard rate is a standard tool in survival analysis to describe failures due to aging; for example, an increasing hazard rate models units that deteriorate with age while a decreasing hazard rate models units that improve with age (a counter intuitive but not illogical possibility). To the best of our knowledge, the connection between hazard rates and design of adversarial bandit algorithms has not been made before. Definition 4.1 (Hazard rate function). Hazard rate function of a distribution D is\nhD(x) := f(x)\n1\u2212 F (x) For the rest of the section, we assume thatD is unbounded in the direction of +\u221e, so that the hazard function is well-defined everywhere. This assumption is for the clarity of presentation and can be easily removed (Appendix B).\nTheorem 4.2. The regret of the GBPA with \u03a6\u0303(G) = EZ1,...,Zn\u223cD maxi{Gi + \u03b7Zi} is at most:\n\u03b7EZ1,...,Zn\u223cD [ max i Zi ] \ufe38 \ufe37\ufe37 \ufe38\noverestimation penalty\n+ N(suphD)\n\u03b7 T\ufe38 \ufe37\ufe37 \ufe38 divergence penalty\nProof. We analyze each penalty term in Lemma 2.1. Due to the convexity of \u03a6, the underestimation penalty is non-positive. The overestimation penalty is clearly at most EZ1,...,Zn\u223cD[maxi Zi], and Lemma 4.3 proves the N(suphD) upper bound on the divergence penalty.\nIt remains to prove the tuning parameter \u03b7. Suppose we scale the perturbation Z by \u03b7 > 0, i.e., we add \u03b7Zi to each coordinate. It is easy to see that E[maxi=1,...,n \u03b7Xi] = \u03b7E[maxi=1,...,nXi]. For the divergence penalty, let F\u03b7 be the CDF of the scaled random variable. Observe that F\u03b7(t) = F (t/\u03b7) and thus f\u03b7(t) = 1\u03b7f(t/\u03b7). Hence, the hazard rate scales by 1/\u03b7, which completes the proof.\nLemma 4.3. The divergence penalty of the GBPA with \u03a6\u0303(G) = EZ1,...,Zn\u223cD maxi{Gi + \u03b7Zi} is at most N(suphD) each round.\nProof. Recall the gradient expression in Equation 10. We upper bound the i-th diagonal entry of the Hessian, as follows:\n\u22072ii\u03a6\u0303(G) = \u2202\n\u2202Gi EG\u0303j\u2217 [1\u2212 F (G\u0303j\u2217 \u2212Gi)] = EG\u0303j\u2217\n[ \u2202\n\u2202Gi (1\u2212 F (G\u0303j\u2217 \u2212Gi))\n] = EG\u0303j\u2217 f(G\u0303j\u2217 \u2212Gi)\n= EG\u0303j\u2217 [h(G\u0303j\u2217 \u2212Gi)(1\u2212 F (G\u0303j\u2217 \u2212Gi))] (11)\n\u2264 (suph)EG\u0303j\u2217 [1\u2212 F (G\u0303j\u2217 \u2212Gi)]\n= (suph)\u2207i(G)\nwhere G\u0303j\u2217 = maxj 6=i{Gj + Zj} which is a random variable independent of Zi. We now apply Theorem 2.3 with \u03b3 = 1 and C = (suph) to complete the proof.\nCorollary 4.4. Follow the Perturbed Leader Algorithm with distributions in Table 1 (restricted to a certain range of parameters), combined with Geometric Resampling (Section 4.1) withM = \u221a NT , has an expected regret of order O( \u221a TN logN).\nTable 1 provides the two terms we need to bound. We derive the third column of the table in Appendix C using Extreme Value Theory (Embrechts et al., 1997). Note that our analysis in the proof of Lemma 4.3 is quite tight; the only place we have an inequality is where we upper bound the hazard rate. It is thus reasonable to pose the following conjecture: Conjecture 4.5. If a distribution D has a monotonically increasing hazard rate hD(x) that does not converge as x\u2192 +\u221e (e.g., Gaussian), then there is a sequence of losses that will incur at least a linear regret.\nThe intuition is that if adversary keeps incurring a high loss for the i-th arm, then with high probability G\u0303j\u2217 \u2212 Gi will be large. So, the expectation in Equation 11 will be dominated by the hazard function evaluated at large values of G\u0303j\u2217 \u2212Gi.\nAcknowledgments. J. Abernethy acknowledges the support of NSF under CAREER grant IIS1453304. A. Tewari acknowledges the support of NSF under CAREER grant IIS-1452099."}, {"heading": "A Proof of the GBPA Regret Bound (Lemma 2.1)", "text": "Lemma A.1. The expected regret of Algorithm 2 can be written as:\nERegret = \u03a6\u0303(0)\u2212 \u03a6(0)\ufe38 \ufe37\ufe37 \ufe38 overestimation penalty + \u03a6(GT )\u2212 \u03a6\u0303(GT )\ufe38 \ufe37\ufe37 \ufe38 underestimation penalty + T\u2211 t=1 D\u03a6\u0303(Gt, Gt\u22121)\ufe38 \ufe37\ufe37 \ufe38 divergence penalty\nProof. Note that since \u03a60(0) = 0,\n\u03a6\u0303(GT ) = ( \u03a6\u0303(0)\u2212 \u03a60(0) )\ufe38 \ufe37\ufe37 \ufe38 overestimation penalty + T\u2211 t=1 \u03a6\u0303(Gt)\u2212 \u03a6\u0303(Gt\u22121)\n= ( \u03a6\u0303(0)\u2212 \u03a60(0) )\ufe38 \ufe37\ufe37 \ufe38 overestimation penalty + T\u2211 t=1 \u3008\u2207\u03a6\u0303(Gt\u22121), `t)\u3009+D\u03a6\u0303(Gt, Gt\u22121)\nTherefore,\nERegret def= E [ \u03a6(GT )\u2212\nT\u2211 t=1 \u3008\u03a6\u0303(Gt\u22121), gt\u3009\n]\n= E (\u03a6(GT )\u2212 \u03a6\u0303(GT ))\ufe38 \ufe37\ufe37 \ufe38 underestimation penalty +\u03a6\u0303(GT )\u2212 T\u2211 t=1 \u3008\u03a6\u0303(Gt\u22121), gt\u3009  = E\n(\u03a6(GT )\u2212 \u03a6\u0303(GT ))\ufe38 \ufe37\ufe37 \ufe38 underestimation penalty + ( \u03a6\u0303(0)\u2212 \u03a60(0) )\ufe38 \ufe37\ufe37 \ufe38 overestimation penalty +D\u03a6\u0303(Gt, Gt\u22121) "}, {"heading": "B Relaxing Assumptions on the Distribution", "text": "B.1 Mirroring trick for extending the support\nLet X have support on x > 0 with density f and CDF F . Let us define Y by mirroring the density of X around zero, i.e., Y has density g(y) = 12f(|y|) and CDF G(y) = 1 2 (1 + sign(y)F (|y|)). Note that |Y | is distributed as X and hence, E[max\ni Yi] \u2264 E[max i |Yi|] = E[max i Xi].\nThe hazard hY (y) for y \u2265 0 is f(y)/(1\u2212F (y)) and for y < 0 is f(\u2212y)/(1+F (\u2212y)) \u2264 F (\u2212y)/(1\u2212 F (\u2212y)). Therefore,\nsup y hY (y) = sup x>0 hX(x).\nThis proves the following lemma.\nLemma B.1. If a random variable X has support on the non-negative reals with density f(x) and we define Y as the mirrored version with density g(y) = 12f(|y|). Then, we have\nE[max i Yi] \u2264 E[max i Xi],\nsup y hY (y) = sup x>0 hX(x)\nwhere hX , hY are hazard rates of X,Y respectively.\nB.2 Conditioning trick for unbounded hazard rate near zero\nSuppose F (x) is the CDF of a random variableX whose hazard rate is bounded for x \u2265 1 but blows up near zero. Then define Y as X conditioned on X \u2265 1. That is, Y has CDF, for y > 0:\nG(y) = P (X \u2265 1 + y|X > 1) = F (1 + y)\u2212 F (1) 1\u2212 F (1)\nand density g(y) = f(1 + y)/(1 \u2212 F (1)), y > 0. So the hazard rate hY (y) is g(y)/(1 \u2212 G(y)) = f(1 + y)/(1\u2212 F (1 + y)) = hX(1 + y). Therefore,\nsup y>0 hY (y) = sup x>1 hX(x)\nwhich makes the hazard rate of Y now bounded. This we have proved the lemma below.\nLemma B.2. If a hazard function of X is bounded for x > 1 and blows up only for small values of x then we can condition on X > 1 to define a new random variable whose hazard rate is now bounded.\nThe same technique can be applied for any arbitrary constant other than 1, but for the family of random variables we considered, it suffices to condition on X \u2265 1."}, {"heading": "C Detailed derivation of extreme value behavior", "text": ""}, {"heading": "C.1 Maximum of iid Gumbel", "text": "The CDF of the Gumbel distribution is exp(\u2212 exp(\u2212x)) and the expected value is \u03b30, the Euler (Euler-Mascheroni) constant. Thus, the CDF of the maximum of n iid Gumbel random variables is (exp(\u2212 exp(\u2212x)))N = exp(\u2212 exp(\u2212(x \u2212 logN))) which is also Gumbel but with the mean increased by logN ."}, {"heading": "C.2 Maximum of iid Frechet", "text": "The CDF of Frechet is exp(\u2212x\u2212\u03b1) and it has mean \u0393(1 \u2212 1\u03b1 ) as long as \u03b1 > 1 (otherwise it is infinite). Hence, the CDF of the maximum of N iid Frechet random variables is\n(exp(\u2212x\u2212\u03b1))N = exp(\u2212Nx\u2212\u03b1) = exp ( \u2212 ( x\nN 1 \u03b1\n)\u2212\u03b1)\nwhich is also Frechet but with mean scaled by N1/\u03b1."}, {"heading": "C.3 Maximum of iid Weibull", "text": "Let Xi have modified Weibull distribution with CDF 1\u2212 exp(\u2212(x+ 1)k + 1). Thus, P (maxiXi > t) \u2264 NP (X1 > t) = N exp(\u2212(t+ 1)k + 1). For non-negative random variable X and any u > 0, we have,\nE[X] = \u222b \u221e\n0 P (X > x)dx \u2264 u+ \u222b \u221e u P (X > x)dx.\nAssume k = 1/m where m \u2265 1 is a positive integer. Therefore,\nE[max i Xi] \u2264 u+ \u222b \u221e u N exp(\u2212(x+ 1)k + 1)dx\n\u2264 u+ 3N \u222b \u221e u exp(\u2212(x+ 1)k)dx\n= u+ 3N \u222b \u221e u+1 exp(\u2212x1/m)dx\n= u+ 3Nm\u0393(m, (1 + u)1/m)dx\nwhere \u0393(m,x) is the incomplete Gamma function that for a positive integer m and x > 1 simplifies to\n\u0393(m,x) = (m\u2212 1)!e\u2212x m\u22121\u2211 k=0 xk k! \u2264 (m\u2212 1)!e\u2212x m\u22121\u2211 k=0 xm k!\n= (m\u2212 1)!e\u2212xxm m\u22121\u2211 k=0 1 k! \u2264 (m\u2212 1)!e\u2212xxm \u221e\u2211 k=0 1 k! \u2264 3(m\u2212 1)!e\u2212xxm.\nPlugging this back above, we get, for any u > 0,\nE[max i Xi] \u2264 u+ 9Nm!e\u2212(1+u)\n1/m\n(1 + u).\nNow choose u = logmN + 1 to get\nE[max i Xi] \u2264 logmN + 9Nm!\nlogmN\nN \u2264 10m! logmN."}, {"heading": "C.4 Maximum of iid Gamma", "text": "Let Y be the maximum of N iid Gamma(\u03b1, \u03b2) ramdom variables. Then, Y\u2212dNcN follows Gumbel distribution, where cN = \u03b2\u22121 and dN = \u03b2\u22121(logN + (\u03b1 \u2212 1) log logN \u2212 log \u0393(\u03b1)). In the language of extreme value theory, Gamma distribution belongs to the maximum domain of attraction of Gumbel distribution with parameters (Embrechts et al., 1997). As mentioned in Section C.1, Gumbel distribution has mean \u03b30."}, {"heading": "C.5 Maximum of iid Pareto", "text": "Let Xi have modified Pareto distribution with CDF 1 \u2212 1/(1 + x)\u03b1. Thus, P (maxiXi > t) \u2264 NP (X1 > t) = N/(1 + x) \u03b1. For non-negative random variable X and any u > 0, we have,\nE[X] = \u222b \u221e\n0 P (X > x)dx \u2264 u+ \u222b \u221e u P (X > x)dx.\nTherefore, for \u03b1 > 1,\nE[max i Xi] \u2264 u+ \u222b \u221e u\nN\n(1 + x)\u03b1 dx\n= u+ N\n(\u03b1\u2212 1)(1 + u)\u03b1\u22121 .\nSetting u = N1/\u03b1 \u2212 1 gives the bound\nE[max i Xi] \u2264\n\u03b1\n\u03b1\u2212 1 N1/\u03b1."}, {"heading": "D Hazard Functions of Modified Distributions and the Frechet Case", "text": ""}, {"heading": "D.1 Pareto distribution", "text": "Using the conditioning trick, we consider, for \u03b1 > 1 (otherwise mean is infinite), the modified Pareto distribution with pdf f(x) = \u03b1(x+1)\u03b1+1 supported on (0,\u221e). Its CDF is 1 \u2212 1/(x + 1)\n\u03b1. Its hazard function is h(x) = \u03b1x+1 which decreases in x and is bounded by \u03b1. Expected maximum of N iid Pareto random variables is bounded by \u03b1N1/\u03b1/(\u03b1 \u2212 1) (see Appendix C.5). This gives a regret bound of \u221a NT \u221a \u03b12N1/\u03b1/(\u03b1\u2212 1)."}, {"heading": "D.2 Frechet distribution", "text": "The CDF of Frechet is exp(\u2212x\u2212\u03b1), x > 0 where \u03b1 > 0 is a shape parameter. The hazard function of Frechet distribution is h(x) = \u03b1x\u2212\u03b1\u22121 exp(\u2212x\n\u2212\u03b1) 1\u2212exp(\u2212x\u2212\u03b1) which is hard to optimize analytically but\ncan be upper bounded, for \u03b1 > 1, via elementary calculations given below, by 2\u03b1. The CDF of the maximum ofN iid Frechet random variables is exp(\u2212(x/N1/\u03b1)\u2212\u03b1) which is also Frechet (but with mean scaled by N1/\u03b1) with expected value N1/\u03b1\u0393(1\u2212 1\u03b1 ) (as long as \u03b1 > 1, otherwise expectation is infinite). Thus, the regret bound we get is O (\u221a NT \u221a \u03b1N1/\u03b1\u0393(1\u2212 1\u03b1 ) ) . Setting \u03b1 = logN makes the regret bound O( \u221a TN logN). Our choice of \u03b1 is larger than 1 as soon as N > 2.\nD.2.1 Elementary calculations for bounding Frechet distribution\u2019s hazard rate\nFor \u03b1 > 1, we want to show that supx>0 h(x) \u2264 2\u03b1 where\nh(x) = \u03b1x\u2212\u03b1\u22121 exp(\u2212x\u2212\u03b1)\n1\u2212 exp(\u2212x\u2212\u03b1) .\nFirst, consider the case x \u2265 1. In this case, define y = x\u03b1 and note that y \u2265 1. Then, we have\nh(x) = \u03b1\nxy exp(\u22121/y) 1\u2212 exp(\u22121/y) \u2264 \u03b1 y exp(\u22121/y) 1\u2212 exp(\u22121/y)\n\u2264 \u03b1 y\n1\n1\u2212 (1\u2212 1/(2y)) = 2\u03b1.\nThe first inequality holds because x \u2265 1. The second holds because exp(\u22121/y) < 1 and exp(\u22121/y) \u2264 1\u2212 1/(2y) for y \u2265 1. Next, consider the case x < 1. Define y = 1/x and note that y > 1. Then, we have\nh(x) = \u03b1 x\u03b1+1 exp(\u2212x\u2212\u03b1) 1\u2212 exp(\u2212x\u2212\u03b1) \u2264 \u03b1 x\u03b1+1 exp(\u2212x\u2212\u03b1) 1\u2212 exp(\u22121)\n= \u03b1\n1\u2212 e\u22121 y\u03b1+1 exp(\u2212y\u03b1) \u2264 2\u03b1y\u03b1+1 exp(\u2212y\u03b1).\nTo show an upper bound of 2\u03b1, it therefore suffices to show that supy>1 g(y) \u2264 1 where g(y) = y\u03b1+1 exp(\u2212y\u03b1). We will show this now. Note that\ng\u2032(y) = (\u03b1+ 1)y\u03b1 exp(\u2212y\u03b1)\u2212 y\u03b1+1\u03b1y\u03b1\u22121 exp(\u2212y\u03b1) = y\u03b1 exp(\u2212y\u03b1) ((\u03b1+ 1)\u2212 \u03b1y\u03b1) ,\nwhich means that g(y) is monotonically increasing on the interval (1, y0) and monotonically decreasing on the interval (y0,+\u221e) where y0 = ( \u03b1+1 \u03b1 )1/\u03b1 . We therefore have,\nsup y>1 g(y) = g(y0) =\n( 1 + 1\n\u03b1\n)(1+1/\u03b1) exp (\u2212(1 + 1/\u03b1)) \u2264 22 exp(\u22122) = 4/e2 \u2264 1,\nwhere the first inequality above holds because \u03b1 > 1. Note that, for \u03b1 > 1, the function \u03b1 7\u2192( 1 + 1\u03b1 )(1+1/\u03b1) exp (\u2212(1 + 1/\u03b1)) decreases monotonically."}, {"heading": "D.3 Weibull distribution", "text": "The CDF of Weibull is 1\u2212exp(\u2212xk) for x > 0 (and 0 otherwise) where k > 0 is a shape parameter. The density is kxk\u22121 exp(\u2212xk) and hazard rate is kxk\u22121. For k > 1, hazard rate monotonically increases and is therefore unbounded for large x. When k < 1, the hazard rate is unbounded for small values of x. Note that Weibull includes exponential as a special case when k = 1.\nLet k = 1/m for some positive integer m \u2265 1 and using the conditioning trick, consider a modified Weibull with CDF 1\u2212 exp(\u2212(x+ 1)k + 1). Density is k(x+ 1)k\u22121 exp(\u2212(x+ 1)k + 1) and hazard is k(x+1)k\u22121 which is bounded by k. When k < 1 we get tails heavier than the exponential but not as heavy as a Pareto or a Frechet. The expected value of the maximum of N iid (modified) Weibull random variables with parameter k = 1/m scales as O(m!(logN)m) (see Appendix C.3). Thus, we get the regret bound O( \u221a NT \u221a m!(log n)m). Thus, the entire modified Weibull family yields\nO( \u221a Npolylog(N) \u221a T ) regret bounds. The best bound is obtained when m = 1, i.e. when the Weibull becomes an exponential."}], "references": [{"title": "Interior-point methods for full-information and bandit online learning", "author": ["Jacob Abernethy", "Elad Hazan", "Alexander Rakhlin"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Abernethy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2012}, {"title": "Online linear optimization via smoothing", "author": ["Jacob Abernethy", "Chansoo Lee", "Abhinav Sinha", "Ambuj Tewari"], "venue": "In COLT,", "citeRegEx": "Abernethy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2014}, {"title": "Minimax policies for adversarial and stochastic bandits", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck"], "venue": "In COLT,", "citeRegEx": "Audibert and Bubeck.,? \\Q2009\\E", "shortCiteRegEx": "Audibert and Bubeck.", "year": 2009}, {"title": "Minimax policies for combinatorial prediction games", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck", "G\u00e1bor Lugosi"], "venue": "In COLT,", "citeRegEx": "Audibert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2011}, {"title": "Using confidence bounds for exploitation-exploration trade-offs", "author": ["Peter Auer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Auer.,? \\Q2003\\E", "shortCiteRegEx": "Auer.", "year": 2003}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer"], "venue": "Machine learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM Journal of Computuataion,", "citeRegEx": "Auer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2003}, {"title": "Stochastic optimization problems with nondifferentiable cost functionals", "author": ["Dimitri P. Bertsekas"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Bertsekas.,? \\Q1973\\E", "shortCiteRegEx": "Bertsekas.", "year": 1973}, {"title": "Regret analysis of stochastic and nonstochastic multiarmed bandit problems", "author": ["S\u00e9bastien Bubeck", "Nicolo Cesa-Bianchi"], "venue": "arXiv preprint arXiv:1204.5721,", "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "Prediction, Learning, and Games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Robbing the bandit: less regret in online geometric optimization against an adaptive adversary", "author": ["V. Dani", "T.P. Hayes"], "venue": "In SODA,", "citeRegEx": "Dani and Hayes.,? \\Q2006\\E", "shortCiteRegEx": "Dani and Hayes.", "year": 2006}, {"title": "The price of bandit information for online optimization", "author": ["Varsha Dani", "Thomas Hayes", "Sham Kakade"], "venue": "In NIPS,", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Prediction by random-walk perturbation", "author": ["Luc Devroye", "G\u00e1bor Lugosi", "Gergely Neu"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Devroye et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Devroye et al\\.", "year": 2013}, {"title": "Reliability Engineering. Wiley Series in Systems", "author": ["E.A. Elsayed"], "venue": "Engineering and Management. Wiley,", "citeRegEx": "Elsayed.,? \\Q2012\\E", "shortCiteRegEx": "Elsayed.", "year": 2012}, {"title": "Modelling Extremal Events", "author": ["P. Embrechts", "C. Kl\u00fcppelberg", "T. Mikosch"], "venue": "For Insurance and Finance. Applications of mathematics. Springer,", "citeRegEx": "Embrechts et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Embrechts et al\\.", "year": 1997}, {"title": "Online convex optimization in the bandit setting: gradient descent without a gradient", "author": ["Abraham D. Flaxman", "Adam Tauman Kalai", "H. Brendan McMahan"], "venue": "In SODA,", "citeRegEx": "Flaxman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "Quantitative methods in the planning of pharmaceutical research", "author": ["John Gittins"], "venue": "Drug Information Journal,", "citeRegEx": "Gittins.,? \\Q1996\\E", "shortCiteRegEx": "Gittins.", "year": 1996}, {"title": "Multi-armed bandit allocation indices", "author": ["John Gittins", "Kevin Glazebrook", "Richard Weber"], "venue": null, "citeRegEx": "Gittins et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gittins et al\\.", "year": 2011}, {"title": "Approximation to bayes risk in repeated play", "author": ["J. Hannan"], "venue": "Contributions to the Theory of Games,", "citeRegEx": "Hannan.,? \\Q1957\\E", "shortCiteRegEx": "Hannan.", "year": 1957}, {"title": "Efficient algorithms for online decision problems", "author": ["Adam Kalai", "Santosh Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai and Vempala.,? \\Q2005\\E", "shortCiteRegEx": "Kalai and Vempala.", "year": 2005}, {"title": "Efficient learning by implicit exploration in bandit problems with side observations", "author": ["Tom\u00e1\u0161 Koc\u00e1k", "Gergely Neu", "Michal Valko", "Remi Munos"], "venue": "In NIPS,", "citeRegEx": "Koc\u00e1k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koc\u00e1k et al\\.", "year": 2014}, {"title": "On following the perturbed leader in the bandit setting", "author": ["Jussi Kujala", "Tapio Elomaa"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Kujala and Elomaa.,? \\Q2005\\E", "shortCiteRegEx": "Kujala and Elomaa.", "year": 2005}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "Herbert Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "The weighted majority algorithm", "author": ["Nick Littlestone", "Manfred K. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "Online geometric optimization in the bandit setting against an adaptive adversary", "author": ["H. Brendan McMahan", "Avrim Blum"], "venue": "In COLT,", "citeRegEx": "McMahan and Blum.,? \\Q2004\\E", "shortCiteRegEx": "McMahan and Blum.", "year": 2004}, {"title": "An efficient algorithm for learning with semi-bandit feedback", "author": ["Gergely Neu", "G\u00e1bor Bart\u00f3k"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Neu and Bart\u00f3k.,? \\Q2013\\E", "shortCiteRegEx": "Neu and Bart\u00f3k.", "year": 2013}, {"title": "Hyperparameter tuning in bandit-based adaptive operator selection", "author": ["Maciej Pacula", "Jason Ansel", "Saman Amarasinghe", "Una-May OReilly"], "venue": "In Applications of Evolutionary Computation,", "citeRegEx": "Pacula et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pacula et al\\.", "year": 2012}, {"title": "Sub-hessians, super-hessians and conjugation", "author": ["Jean-Paul Penot"], "venue": "Nonlinear Analysis: Theory, Methods & Applications,", "citeRegEx": "Penot.,? \\Q1994\\E", "shortCiteRegEx": "Penot.", "year": 1994}, {"title": "Relax and randomize: From value to algorithms", "author": ["Sasha Rakhlin", "Ohad Shamir", "Karthik Sridharan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rakhlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2012}, {"title": "Some aspects of the sequential design of experiments", "author": ["Herbert Robbins"], "venue": "Bull. Amer. Math. Soc.,", "citeRegEx": "Robbins.,? \\Q1952\\E", "shortCiteRegEx": "Robbins.", "year": 1952}, {"title": "Possible generalization of boltzmann-gibbs statistics", "author": ["Constantino Tsallis"], "venue": "Journal of Statistical Physics,", "citeRegEx": "Tsallis.,? \\Q1988\\E", "shortCiteRegEx": "Tsallis.", "year": 1988}, {"title": "Monte-carlo tree search in poker using expected reward distributions", "author": ["Guy Van den Broeck", "Kurt Driessens", "Jan Ramon"], "venue": "In Advances in Machine Learning,", "citeRegEx": "Broeck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Broeck et al\\.", "year": 2009}, {"title": "Follow the leader with dropout perturbations", "author": ["Tim Van Erven", "Wojciech Kotlowski", "Manfred K Warmuth"], "venue": "In COLT,", "citeRegEx": "Erven et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Erven et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "The MAB framework is not only mathematically elegant, but useful for a wide range of applications including medical experiments design (Gittins, 1996), automated poker playing strategies (Van den Broeck et al.", "startOffset": 135, "endOffset": 150}, {"referenceID": 26, "context": ", 2009), and hyperparameter tuning (Pacula et al., 2012).", "startOffset": 35, "endOffset": 56}, {"referenceID": 17, "context": ", IID) on the loss sequence (Gittins et al., 2011; Lai and Robbins, 1985; Auer et al., 2002).", "startOffset": 28, "endOffset": 92}, {"referenceID": 22, "context": ", IID) on the loss sequence (Gittins et al., 2011; Lai and Robbins, 1985; Auer et al., 2002).", "startOffset": 28, "endOffset": 92}, {"referenceID": 5, "context": ", IID) on the loss sequence (Gittins et al., 2011; Lai and Robbins, 1985; Auer et al., 2002).", "startOffset": 28, "endOffset": 92}, {"referenceID": 23, "context": "As researchers began to establish non-stochastic, worst-case guarantees for sequential decision problems such as prediction with expert advice (Littlestone and Warmuth, 1994), a natural question arose as to whether similar guarantees were possible for the bandit setting.", "startOffset": 143, "endOffset": 174}, {"referenceID": 24, "context": "Attention later turned to the bandit version of online linear optimization, and several associated guarantees were published the following decade (McMahan and Blum, 2004; Flaxman et al., 2005; Dani and Hayes, 2006; Dani et al., 2008; Abernethy et al., 2012).", "startOffset": 146, "endOffset": 257}, {"referenceID": 15, "context": "Attention later turned to the bandit version of online linear optimization, and several associated guarantees were published the following decade (McMahan and Blum, 2004; Flaxman et al., 2005; Dani and Hayes, 2006; Dani et al., 2008; Abernethy et al., 2012).", "startOffset": 146, "endOffset": 257}, {"referenceID": 10, "context": "Attention later turned to the bandit version of online linear optimization, and several associated guarantees were published the following decade (McMahan and Blum, 2004; Flaxman et al., 2005; Dani and Hayes, 2006; Dani et al., 2008; Abernethy et al., 2012).", "startOffset": 146, "endOffset": 257}, {"referenceID": 11, "context": "Attention later turned to the bandit version of online linear optimization, and several associated guarantees were published the following decade (McMahan and Blum, 2004; Flaxman et al., 2005; Dani and Hayes, 2006; Dani et al., 2008; Abernethy et al., 2012).", "startOffset": 146, "endOffset": 257}, {"referenceID": 0, "context": "Attention later turned to the bandit version of online linear optimization, and several associated guarantees were published the following decade (McMahan and Blum, 2004; Flaxman et al., 2005; Dani and Hayes, 2006; Dani et al., 2008; Abernethy et al., 2012).", "startOffset": 146, "endOffset": 257}, {"referenceID": 15, "context": "The classic multi-armed bandit (MAB) problem, generally attributed to the early work of Robbins (1952), poses a generic online decision scenario in which an agent must make a sequence of choices from a fixed set of options.", "startOffset": 88, "endOffset": 103}, {"referenceID": 2, "context": ", 2011; Lai and Robbins, 1985; Auer et al., 2002). As researchers began to establish non-stochastic, worst-case guarantees for sequential decision problems such as prediction with expert advice (Littlestone and Warmuth, 1994), a natural question arose as to whether similar guarantees were possible for the bandit setting. The pioneering work of Auer, Cesa-Bianchi, Freund, and Schapire (2003) answered this in the affirmative by showing that their algorithm EXP3 possesses nearly-optimal regret bounds with matching lower bounds.", "startOffset": 31, "endOffset": 394}, {"referenceID": 4, "context": "For example, the EXP3 algorithm (Auer, 2003) regularizes with the entropy function and achieves a nearly optimal regret bound when K is the probability sim-", "startOffset": 32, "endOffset": 44}, {"referenceID": 0, "context": "For a general convex set, however, other regularizers such as self-concordant barrier functions (Abernethy et al., 2012) have tighter regret bounds.", "startOffset": 96, "endOffset": 120}, {"referenceID": 19, "context": "Another class of algorithms for the full information setting is Follow the Perturbed Leader (FTPL) (Kalai and Vempala, 2005) whose foundations date back to the earliest work in adversarial online learning (Hannan, 1957).", "startOffset": 99, "endOffset": 124}, {"referenceID": 18, "context": "Another class of algorithms for the full information setting is Follow the Perturbed Leader (FTPL) (Kalai and Vempala, 2005) whose foundations date back to the earliest work in adversarial online learning (Hannan, 1957).", "startOffset": 205, "endOffset": 219}, {"referenceID": 12, "context": "For every different choice of D, an entirely new set of techniques had to be developed (Devroye et al., 2013; Van Erven et al., 2014).", "startOffset": 87, "endOffset": 133}, {"referenceID": 10, "context": "For every different choice of D, an entirely new set of techniques had to be developed (Devroye et al., 2013; Van Erven et al., 2014). Rakhlin et al. (2012) and Abernethy et al.", "startOffset": 88, "endOffset": 157}, {"referenceID": 0, "context": "(2012) and Abernethy et al. (2014) made some progress towards unifying the analysis framework.", "startOffset": 11, "endOffset": 35}, {"referenceID": 2, "context": "We show that regularization via the Tsallis entropy leads to the state-of-the-art adversarial MAB algorithm, matching the minimax regret rate of Audibert and Bubeck (2009) with a tighter constant.", "startOffset": 145, "endOffset": 172}, {"referenceID": 6, "context": "Framework 1 may appear restrictive but it has served as the basis for much of the published work on adversarial MAB algorithms (Auer et al., 2003; Kujala and Elomaa, 2005; Neu and Bart\u00f3k, 2013) mainly for two reasons.", "startOffset": 127, "endOffset": 193}, {"referenceID": 21, "context": "Framework 1 may appear restrictive but it has served as the basis for much of the published work on adversarial MAB algorithms (Auer et al., 2003; Kujala and Elomaa, 2005; Neu and Bart\u00f3k, 2013) mainly for two reasons.", "startOffset": 127, "endOffset": 193}, {"referenceID": 25, "context": "Framework 1 may appear restrictive but it has served as the basis for much of the published work on adversarial MAB algorithms (Auer et al., 2003; Kujala and Elomaa, 2005; Neu and Bart\u00f3k, 2013) mainly for two reasons.", "startOffset": 127, "endOffset": 193}, {"referenceID": 1, "context": "First, the GBPA framework encompasses all FTRL and FTPL algorithms, which are the core techniques for sequential prediction algorithms (Abernethy et al., 2014).", "startOffset": 135, "endOffset": 159}, {"referenceID": 0, "context": "We study a particular algorithmic template described in Framework 1, which is a slight variation of the Gradient Based Prediction Algorithm (GBPA) of Abernethy et al. (2014). Note that the algorithm (i) maintains an unbiased estimate of the cumulative losses \u011ct, (ii) updates \u011ct by adding a singleround estimate \u011dt, and (iii) uses the gradient of a convex function \u03a6\u0303 as sampling distribution pt.", "startOffset": 150, "endOffset": 174}, {"referenceID": 0, "context": "Abernethy et al. (2014) proved that a uniform bound the norm of\u2207\u03a6\u0303 directly gives a regret guarantee for the full-information setting.", "startOffset": 0, "endOffset": 24}, {"referenceID": 3, "context": "A few years later, Audibert and Bubeck (2009) resolved this gap with Implicitly Normalized Forecaster (INF), which later was shown to be equivalent to Mirror Descent (Audibert et al., 2011) on the probability simplex.", "startOffset": 166, "endOffset": 189}, {"referenceID": 2, "context": "A few years later, Audibert and Bubeck (2009) resolved this gap with Implicitly Normalized Forecaster (INF), which later was shown to be equivalent to Mirror Descent (Audibert et al.", "startOffset": 19, "endOffset": 46}, {"referenceID": 30, "context": "Now we will replace the Shannon entropy with the Tsallis entropy1 (Tsallis, 1988), defined as:", "startOffset": 66, "endOffset": 81}, {"referenceID": 27, "context": "Following the setup of Penot (1994), \u2207S\u03b1(p) is a sub-hessian of \u015c\u03b1(p).", "startOffset": 23, "endOffset": 36}, {"referenceID": 7, "context": "By Bertsekas (1973), we can swap the order of differentiation and expectation: \u2207\u03a6\u0303(G;D) = E Z1,.", "startOffset": 3, "endOffset": 20}, {"referenceID": 20, "context": "Indeed, an early result of Kujala and Elomaa (2005) showed that a near-optimal MAB strategy comes about through the use of exponentially-distributed noise, and the same perturbation strategy has more recently been utilized in the work of Neu and Bart\u00f3k (2013) and Koc\u00e1k et al.", "startOffset": 27, "endOffset": 52}, {"referenceID": 20, "context": "Indeed, an early result of Kujala and Elomaa (2005) showed that a near-optimal MAB strategy comes about through the use of exponentially-distributed noise, and the same perturbation strategy has more recently been utilized in the work of Neu and Bart\u00f3k (2013) and Koc\u00e1k et al.", "startOffset": 27, "endOffset": 260}, {"referenceID": 20, "context": "Indeed, an early result of Kujala and Elomaa (2005) showed that a near-optimal MAB strategy comes about through the use of exponentially-distributed noise, and the same perturbation strategy has more recently been utilized in the work of Neu and Bart\u00f3k (2013) and Koc\u00e1k et al. (2014). However, a more general understanding of perturbation methods has remained elusive.", "startOffset": 264, "endOffset": 284}, {"referenceID": 19, "context": "In fact, this is equivalent to Follow the Perturbed Leader Algorithm (FTPL) (Kalai and Vempala, 2005) applied to the bandit setting.", "startOffset": 76, "endOffset": 101}, {"referenceID": 19, "context": "In fact, this is equivalent to Follow the Perturbed Leader Algorithm (FTPL) (Kalai and Vempala, 2005) applied to the bandit setting. On the other hand, the estimation step is hard because generally there is no closed-form expression for\u2207\u03a6\u0303. To address this issue, Neu and Bart\u00f3k (2013) proposed Geometric Resampling (GR).", "startOffset": 77, "endOffset": 286}, {"referenceID": 14, "context": "We derive the third column of the table in Appendix C using Extreme Value Theory (Embrechts et al., 1997).", "startOffset": 81, "endOffset": 105}], "year": 2015, "abstractText": "We define a novel family of algorithms for the adversarial multi-armed bandit problem, and provide a simple analysis technique based on convex smoothing. We prove two main results. First, we show that regularization via the Tsallis entropy, which includes EXP3 as a special case, achieves the \u0398( \u221a TN) minimax regret. Second, we show that a wide class of perturbation methods achieve a near-optimal regret as low as O( \u221a TN logN) if the perturbation distribution has a bounded hazard rate. For example, the Gumbel, Weibull, Frechet, Pareto, and Gamma distributions all satisfy this key property.", "creator": "LaTeX with hyperref package"}}}