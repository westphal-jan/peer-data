{"id": "1512.04960", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Dec-2015", "title": "A Light Touch for Heavily Constrained SGD", "abstract": "Projected stochastic gradient descent (SGD) is often the default choice for large-scale optimization in machine learning, but requires a projection after each update. For heavily-constrained objectives, we propose an efficient extension of SGD that stays close to the feasible region while only applying constraints probabilistically at each iteration. Theoretical analysis shows a good trade-off between per-iteration work and the number of iterations needed, indicating compelling advantages on problems with a large number of constraints onto which projecting is expensive and inefficient. For example, SGD may be used to approximate a maximum possible distance in a given direction using Gaussian methods that require a constant constant, while using a constant (typically 2) Gaussian method. This might be the case in situations where an agent must have been able to predict the expected distance from the end of the transformation as a parameter, which would allow for better choice for larger models with large-scale optimization.", "histories": [["v1", "Tue, 15 Dec 2015 21:07:02 GMT  (55kb)", "https://arxiv.org/abs/1512.04960v1", null], ["v2", "Mon, 24 Oct 2016 20:30:25 GMT  (74kb)", "http://arxiv.org/abs/1512.04960v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrew cotter", "maya gupta", "jan pfeifer"], "accepted": false, "id": "1512.04960"}, "pdf": {"name": "1512.04960.pdf", "metadata": {"source": "CRF", "title": "A Light Touch for Heavily Constrained SGD", "authors": ["Andrew Cotter", "Maya Gupta", "Jan Pfeifer"], "emails": ["acotter@google.com", "mayagupta@google.com", "janpf@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 2.\n04 96\n0v 2\n[ cs\n.L G\n] 2\n4 O\nct 2"}, {"heading": "1 Introduction", "text": "Many machine learning problems can benefit from the addition of constraints. For example, one can learn monotonic functions by adding appropriate constraints to ensure or encourage positive derivatives everywhere [e.g. Archer and Wang, 1993, Sill, 1998, Spouge et al., 2003, Daniels and Velikova, 2010, Gupta et al., 2016]. Submodular functions can often be learned from noisy examples by imposing constraints to ensure submodularity holds. Another example occurs when one wishes to guarantee that a classifier will correctly label certain \u201ccanonical\u201d examples, which can be enforced by constraining the function values on those examples. See Qu and Hu [2011] for some other examples of constraints useful in machine learning.\nHowever, these practical uses of constraints in machine learning are impractical in that the number of constraints may be very large, and scale poorly with the number of features d or number of training samples n. In this paper we propose a new strategy for tackling such heavily-constrained problems, with guarantees and compelling convergence rates for large-scale convex problems.\nA standard approach for large-scale empirical risk minimization is projected stochastic gradient descent [e.g. Zinkevich, 2003, Nemirovski et al., 2009]. Each SGD iteration is computationally cheap, and the algorithm converges quickly to a solution good enough for machine learning needs. However, this algorithm requires a projection onto the feasible region after each stochastic gradient step, which can be prohibitively slow if there are many non-trivial constraints, and is not easy to parallelize. Recently, Frank-Wolfe-style algorithms [e.g. Hazan and Kale, 2012, Jaggi, 2013] have been proposed that remove the projection, but require a constrained linear optimization at each iteration.\nWe propose a new strategy for large-scale constrained optimization that, like Mahdavi et al. [2012], moves the constraints into the objective and finds an approximate solution of the resulting unconstrained problem, projecting the (potentially-infeasible) result onto the constraints only once, at the end. Their work focused on handling only one constraint, but as they noted, multiple constraints g1(x) \u2264 0, g2(x) \u2264 0, . . . , gm(x) \u2264 0 can be reduced to one constraint by replacing the m constraints with their maximum: maxi gi(x) \u2264 0. However, this still requires that all m\nThis version was also presented at the 29th Conference on Learning Theory (COLT 2016).\nconstraints be checked at every iteration. In this paper, we focus on the computational complexity as a function of the number of constraints m, and show that it is possible to achieve good convergence rates without checking constraints so often.\nThe key challenge to handling a large number of constraints is determining which constraints are active at the optimum of the constrained problem, which is likely to be only a small fraction of the total constraint set. For example, for linear inequality constraints on a d-dimensional problem, no more than d of the constraints will be active at the optimum, and furthermore, once the active constraints are known, the problem reduces to solving the unconstrained problem that results from projecting onto them, which is typically vastly easier.\nTo identify and focus on the important constraints, we propose learning a probability distribution over them constraints that concentrates on the most-violated, and sampling constraints from this evolving distribution at each iteration. We call this approach LightTouch because at each iteration only a few constraints are checked, and the solution is only nudged toward the feasible set. LightTouch is suitable for convex problems, but we also propose a variant, MidTouch, that enjoys a superior convergence rate on strongly convex problems. These two algorithms are introduced and analyzed in Section 3.\nOur proposed strategy removes the per-iteration m-dependence on the number of constraint evaluations. LightTouch and MidTouch do need more iterations to converge, but each iteration is faster, resulting in a net performance improvement. To be precise, we show that the total number of constraint checks required to achieve \u01eb-suboptimality when optimizing a non-strongly convex objective decreases from O(m/\u01eb2) to O\u0303((lnm)/\u01eb2+m(lnm)3/2/\u01eb3/2)\u2014notice that the m-dependence of the dominant (in \u01eb) term has decreased from m to lnm. For a \u03bb-strongly convex objective, the dominant (again in \u01eb) term in our bound on the number of constraint checks decreases from O(m/\u03bb2\u01eb) to O\u0303((lnm)/\u03bb2\u01eb), but like the non-strongly convex result this bound contains lower-order terms with worse m-dependencies. A more careful comparison of the performance of our algorithms can be found in Section 4.\nWhile they check fewer than m constraints per iteration, these algorithms do need to pay a O(m) per-iteration arithmetic cost. When each constraint is expensive to check, this cost can be neglected. However, when the constraints are simple to check (e.g. box constraints, or the lattice monotonicity constraints considered in our experiments), it can be partially addressed by transforming the problem into an equivalent one with fewer more costly constraints. This, as well as other practical considerations, are discussed in Section 5.\nExperiments on a large-scale real-world heavily-constrained ranking problem show that our proposed approach works well in practice. This problem was too large for a projected SGD implementation using an off-the-shelf quadratic programming solver to perform projections, but was amenable to an approach based on a fast approximate projection routine tailored to this particular constraint set. Measured in terms of runtime, however, LightTouch was still significantly faster. Each constraint in this problem is trivial, requiring only a single comparison operation to check, so the aforementioned O(m) arithmetic cost of LightTouch is a significant issue. Despite this, LightTouch was roughly as fast as the Mahdavi et al. [2012]-like algorithm FullTouch. In light of other experiments showing that LightTouch checks dramatically fewer constraints in total than FullTouch, we believe that LightTouch is well-suited to machine learning problems with many nontrivial constraints."}, {"heading": "2 Heavily Constrained SGD", "text": "Consider the constrained optimization problem:\nmin w\u2208W f (w) (1)\ns.t. gi (w) \u2264 0 \u2200i \u2208 {1, . . . ,m} ,\nwhere W \u2286 Rd is bounded, closed and convex, and f : W \u2192 R and all gi : W \u2192 R are convex (our notation is summarized in Table 1). We assume that W is a simple object, e.g. an \u21132 ball, onto which it is inexpensive to project, and that the \u201ctrickier\u201d aspects of the domain are specified via the constraints gi(w) \u2264 0. Notice that we\nconsider constraints written in terms of arbitrary convex functions, and are not restricted to e.g. only linear or quadratic constraints."}, {"heading": "2.1 FullTouch: A Relaxation with a Feasible Minimizer", "text": "We build on the approach of Mahdavi et al. [2012] to relax Equation 1. Defining g(w) = maxi gi(w) and introducing a Lagrange multiplier \u03b1 yields the equivalent optimization problem:\nmax \u03b1\u22650 min w\u2208W f (w) + \u03b1g (w) . (2)\nDirectly optimizing over w and \u03b1 is problematic because the optimal value for \u03b1 is infinite for any w that violates a constraint. Instead, we follow Mahdavi et al. [2012, Section 4.2] in relaxing the problem by adding an upper bound of \u03b3 on \u03b1, and using the fact that max0\u2264\u03b1\u2264\u03b3 \u03b1g(w) = \u03b3max(0, g(w)).\nIn the following lemma, we show that, with the proper choice of \u03b3, any minimizer of this relaxed objective is a feasible solution of Equation 1, indicating that using stochastic gradient descent (SGD) to minimize the relaxation (h(w) in the lemma below) will be effective.\nLemma 1. Suppose that f is Lf -Lipschitz, i.e. |f(w)\u2212 f(w\u2032)| \u2264 Lf \u2016w \u2212 w\u2032\u20162 for all w,w\u2032 \u2208 W , and that there is a constant \u03c1 > 0 such that if g(w) = 0 then \u2225 \u2225\u2207\u030c \u2225 \u2225\n2 \u2265 \u03c1 for all \u2207\u030c \u2208 \u2202g(w), where \u2202g(w) is the subdifferential of g(w).\nFor a parameter \u03b3 > 0, define: h (w) = f (w) + \u03b3max {0, g (w)} .\nAlgorithm 1 (FullTouch) Minimizes f on W subject to the single constraint g(w) \u2264 0. For problems with m constraints gi(w) \u2264 0, let g(w) = maxi gi(w), in which case differentiatingmax{0, g(w)} (line 4) requires evaluating all m constraints. This algorithm\u2014our starting point\u2014is similar to those proposed by Mahdavi et al. [2012], and like their algorithms only contains a single projection, at the end, projecting the potentially-infeasible result vector w\u0304.\nHyperparameters: T , \u03b7 1 Initialize w(1) \u2208 W arbitrarily 2 For t = 1 to T : 3 Sample \u2206\u030c(t) // stochastic subgradient of f(w(t)) 4 Let \u2206\u030c(t)w = \u2206\u030c(t) + \u03b3\u2207\u030cmax{0, g(w(t))} 5 Update w(t+1) = \u03a0w(w(t) \u2212 \u03b7\u2206\u030c(t)w ) // \u03a0w projects its argument onto W w.r.t. \u2016\u00b7\u20162 6 Average w\u0304 = (\n\u2211T t=1 w (t))/T 7 Return \u03a0g(w\u0304) // optional if small constraint violations are acceptable\nIf \u03b3 > Lf/\u03c1, then for any infeasible w (i.e. for which g(w) > 0):\nh (w) > h (\u03a0g (w)) = f (\u03a0g (w)) and \u2016w \u2212\u03a0g (w)\u20162 \u2264 h (w)\u2212 h (\u03a0g (w))\n\u03b3\u03c1\u2212 Lf ,\nwhere \u03a0g (w) is the projection of w onto the set {w \u2208 W : g(w) \u2264 0} w.r.t. the Euclidean norm.\nProof. In Appendix C.\nThe strategy of applying SGD to h(w), detailed in Algorithm 1, which we call FullTouch, has the same \u201cflavor\u201d as the algorithms proposed by Mahdavi et al. [2012], and we use it as a baseline comparison point for our other algorithms.\nApplication of a standard SGD bound to FullTouch shows that it converges at a rate with no explicit dependence on the number of constraints m, measured in terms of the number of iterations required to achieve some desired suboptimality (see Appendix C.1), although the \u03b3 parameter can introduce an implicit d or m-dependence, depending on the constraints (discussed in Section 2.2). The main drawback of FullTouch is that each iteration is expensive, requiring the evaluation of all m constraints, since differentiation of g requires first identifying the most-violated. This is the key issue we tackle with the LightTouch algorithm proposed in Section 3."}, {"heading": "2.2 Constraint-Dependence of \u03b3", "text": "The conditions on Lemma 1 were stated in terms of g, instead of the individual gis, because it is difficult to provide suitable conditions on the \u201ccomponent\u201d constraints without accounting for their interactions.\nFor a point w where two or more constraints intersect, the subdifferential of g(w) consists of all convex combinations of subgradients of the intersecting constraints, with the consequence that even if each of the subgradients of the gi(w)s has norm at least \u03c1\u2032, subgradients of g(w) will generally have norms smaller than \u03c1\u2032. Exactly how much smaller depends on the particular constraints under consideration. We illustrate this phenomenon with the following examples, but note that, in practice, \u03b3 should be chosen experimentally for any particular problem, so the question of the d and m-dependence of \u03b3 is mostly of theoretical interest.\nBox Constraints Consider the m = 2d box constraints gi(w) = \u2212wi \u2212 1 and gi+d(w) = wi \u2212 1, all of which have gradients of norm 1. At most d constraints can intersect (at a corner of the [\u22121, 1]d box), all of which are mutually orthogonal, so the norm of any convex combination of their gradients is lower bounded by that of their average, \u03c1 = 1/ \u221a d. Hence, one should choose \u03b3 > \u221a dLf .\nAs in the above example, \u03b3 \u221d \u221a\nmin(m, d) will suffice when the subgradients of intersecting constraints are at least orthogonal, and \u03b3 can be smaller if they always have positive inner products. However, if subgradients of intersecting constraints tend to point in opposing directions, then \u03b3 may need to be much larger, as in our next example:\nOrdering Constraints Suppose the m = d \u2212 1 constraints order the components of w as w1 \u2264 w2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 wd, for which gi(w) = (wi \u2212 wi+1)/ \u221a 2, gradients of which again have norm 1. All of these constraints may be active\nsimultaneously, in which case there is widespread cancellation in the average gradient (e1 \u2212 ed)/(m \u221a 2), where ei is the ith standard unit basis vector. The norm of this average gradient is \u03c1 = 1/m, so we should choose \u03b3 > (d\u2212 1)Lf . In light of this example, one begins to wonder if a suitable \u03b3 will necessarily exist\u2014fortunately, the convexity of g enables us to prove a trivial bound as long as g(v) is strictly negative for some v \u2208 W : Lemma 2. Suppose that there exists a v \u2208 W for which g(v) < 0, and let Dw \u2265 supw,w\u2032\u2208W \u2016w \u2212 w\u2032\u20162 bound the diameter of W . Then \u03c1 = \u2212g(v)/Dw satisfies the conditions of Lemma 1.\nProof. Let w \u2208 W be a point for which g(w) = 0, and \u2207\u030c \u2208 \u2202g(w) an arbitrary subgradient. By convexity, g(v) \u2265 g(w) + \u2329 v \u2212 w, \u2207\u030c \u232a . The Cauchy-Schwarz inequality then gives that:\ng(v) \u2265 \u2212\u2016v \u2212 w\u20162 \u2225 \u2225\u2207\u030c \u2225 \u2225 2 ,\nfrom which the claim follows immediately.\nLinear Constraints Consider the constraints Aw b, with each row of A having unit norm, bmin = mini bi > 0, and W being the \u21132 ball of radius r. It follows from Lemma 2 that \u03b3 > (2r/bmin)Lf suffices. Notice that the earlier box constraint example satisfies these assumptions (with bmin = 1 and r = \u221a d).\nAs the above examples illustrate, subgradients of g will be large at the boundary if subgradients of the gis are large, and the constraints intersect at sufficiently shallow angles that, representing boundary subgradients of g as convex combinations of subgradients of the gis, the components reinforce each other, or at least do not cancel too much. This requirement is related to the linear regularity assumption introduced by Bauschke [1996], and considered recently by Wang et al. [2015]."}, {"heading": "3 A Light Touch", "text": "This section presents the main contribution of this paper: an algorithm that stochastically samples a small subset of the m constraints at each SGD iteration, updates the parameters based on the subgradients of the sampled constraints, and carefully learns the distribution over the constraints to produce a net performance gain.\nWe first motivate the approach by considering an oracle, then explain the algorithm and present convergence results for the convex (Section 3.2) and strongly convex (Section 3.3) cases."}, {"heading": "3.1 Wanted: An Oracle For the Most Violated Constraint", "text": "Because FullTouch only needs to differentiate the most violated constraint at each iteration, it follows that if one had access to an oracle that identified the most-violated constraint, then the overall convergence rate (including the cost of each iteration) could only depend on m through \u03b3. This motivates us to learn to predict the most-violated constraint, ideally at a significantly better than linear-in-m rate.\nTo this end, we further relax the problem of minimizing h(w) (defined in Lemma 1) by replacing \u03b3max(0, g(w)) with maximization over a probability distribution (as in Clarkson et al. [2010]), yielding the equivalent convex-linear\nAlgorithm 2 (LightTouch) Minimizes f on W subject to the constraints gi(w) \u2264 0 for i \u2208 {1, . . . ,m}. The algorithm learns an auxiliary probability distribution p (lines 9\u201313) estimating how likely it is that each constraint is the mostviolated. We assume that k \u2264 m: if k > m, then the user is willing to check m constraints per iteration anyway, so FullTouch is the better choice. Like FullTouch, this algorithm finds a potentially-infeasible solution w\u0304 which is only projected onto the feasible region at the end. Notice that while the p-update checks only k constraints, it does require O(m) arithmetic operations. This issue is discussed further in Section 5.1.\nHyperparameters: T , \u03b7, k 1 Initialize w(1) \u2208 W arbitrarily 2 Initialize p(1) \u2208 \u2206m to the uniform distribution 3 Initialize \u00b5(1)j = max{0, gj(w(1))} // 0 if w(1) is feasible 4 For t = 1 to T : 5 Sample \u2206\u030c(t) // stochastic subgradient of f(w(t)) 6 Sample i(t) \u223c p(t) 7 Let \u2206\u030c(t)w = \u2206\u030c(t) + \u03b3\u2207\u030cmax{0, gi(t)(w(t))} 8 Update w(t+1) = \u03a0w(w(t) \u2212 \u03b7\u2206\u030c(t)w ) // \u03a0w projects its argument onto W w.r.t. \u2016\u00b7\u20162 9 Sample S(t) \u2286 {1, . . . ,m} with \u2223 \u2223S(t) \u2223 \u2223 = k uniformly without replacement\n10 Let \u2206\u0302(t)p = \u03b3\u00b5(t) + (\u03b3m/k) \u2211 j\u2208S(t) ej(max{0, gj(w(t))} \u2212 \u00b5 (t) j ) 11 Let \u00b5(t+1)j = max{0, gj(w(t))} if j \u2208 S(t), otherwise \u00b5 (t+1) j = \u00b5 (t) j 12 Update p\u0303(t+1) = exp(ln p(t) + \u03b7\u2206\u0302(t)p ) // element-wise exp and ln 13 Project p(t+1) = p\u0303(t+1)/ \u2225 \u2225p\u0303(t+1) \u2225 \u2225\n1\n14 Average w\u0304 = ( \u2211T t=1 w (t))/T 15 Return \u03a0g(w\u0304) // optional if small constraint violations are acceptable\noptimization problem:\nmax p\u2208\u2206m min w\u2208W h\u0303 (w, p) (3)\nwhere h\u0303 (w, p) = f (w) + \u03b3\nm \u2211\ni=1\npimax {0, gi (w)} .\nHere, \u2206m is the m-dimensional simplex. We propose optimizing over w and p jointly, thereby learning the mostviolated constraint, represented by the multinoulli distribution p over constraint indices, at the same time as we optimize over w."}, {"heading": "3.2 LightTouch: Stochastic Constraint Handling", "text": "To optimize Equation 3, our proposed algorithm (Algorithm 2, LightTouch) iteratively samples stochastic gradients \u2206\u030c (t) w w.r.t. w and \u2206\u0302 (t) p w.r.t. p of h\u0303(w, p), and then takes an SGD step on w and a multiplicative step on p:\nw(t+1) = \u03a0w\n( w(t) \u2212 \u03b7\u2206\u030c(t)w )\nand p(t+1) = \u03a0p ( exp ( ln p(t) + \u03b7\u2206\u0302(t)p )) ,\nwhere the exp and ln of the p-update are performed element-wise, \u03a0w projects onto W w.r.t. the Euclidean norm, and \u03a0p onto \u2206m via normalization (i.e. dividing its parameter by its sum).\nThe key to getting a good convergence rate for this algorithm is to choose \u2206\u030cw and \u2206\u0302p such that they are both inexpensive to compute, and tend to have small norms. For \u2206\u030cw, this can be accomplished straightforwardly, by sampling a constraint index i according to p, and taking:\n\u2206\u030cw = \u2206\u030c + \u03b3\u2207\u030cmax {0, gi (w)} ,\nwhere \u2206\u030c is a stochastic subgradient of f and \u2207\u030cmax(0, gi(w)) is a subgradient of max(0, gi(w)). Calculating each such \u2206\u030cw requires differentiating only one constraint, and it is easy to verify that \u2206\u030cw is a subgradient of h\u0303 w.r.t. w in expectation over \u2206\u030c and i. Taking Gf to be a bound on the norm of \u2206\u030c and Gg on the norms of subgradients of the gis shows that \u2206\u030cw\u2019s norm is bounded by Gf + \u03b3Gg.\nFor \u2206\u0302p, some care must be taken. Simply sampling a constraint index j uniformly and defining:\n\u2206\u0302p = \u03b3mej max {0, gj (w)} ,\nwhere ej is the jth m-dimensional standard unit basis vector, does produce a \u2206\u0302p that in expectation is the gradient of h\u0303 w.r.t. p, but it has a norm bound proportional to m. Such potentially large stochastic gradients would result in the number of iterations required to achieve some target suboptimality being proportional to m2 in our final bound.\nA typical approach to reducing the variance (and hence the expected magnitude) of \u2206\u0302p is minibatching: instead of sampling a single constraint index j at every iteration, we could instead sample a subset S of size |S| = k without replacement, and use:\n\u2206\u0302p = \u03b3m\nk\n\u2211 j\u2208S ej max {0, gj (w)} .\nThis is effective, but not enough, because reducing the variance by a factor of k via minibatching requires that we check k times more constraints. For this reason, in addition to minibatching, we center the stochastic gradients, as is done by the well-known SVRG algorithm [Johnson and Zhang, 2013], by storing a gradient estimate \u03b3\u00b5 with \u00b5 \u2208 Rm, at each iteration sampling a set S of size |S| = k uniformly without replacement, and computing:\n\u2206\u0302p = \u03b3\u00b5+ \u03b3m\nk\n\u2211 j\u2208S ej (max {0, gj(w)} \u2212 \u00b5j) . (4)\nWe then update the jth coordinate of \u00b5 to be \u00b5j = max {0, gj(w)} for every j \u2208 S. The norms of the resulting stochastic gradients will be small if \u03b3\u00b5 is a good estimate of the gradient, i.e. \u00b5j \u2248 max(0, gj(w)). The difference between \u00b5j and max(0, gj(w)) can be bounded in terms of how many consecutive iterations may have elapsed since \u00b5j was last updated. It turns out (see Lemma 4 in Appendix C.2) that this quantity can be bounded uniformly by O((m/k) ln(mT )) with high probability, which implies that if the gis are Lg-Lipschitz, then |gj(w)\u2212 \u00b5j | \u2264 Lg\u03b7(Gf + \u03b3Gg)O((m/k) ln(mT )), since at most O((m/k) ln(mT )) updates of magnitude \u03b7(Gf + \u03b3Gg) may have occurred since \u00b5j was last updated. Choosing \u03b7 \u221d 1/ \u221a T , as is standard, moves this portion\n(the \u201cvariance portion\u201d) of the \u2206\u0302p-dependence out of the dominant O(1/ \u221a T ) term and into a subordinate term in our final bound.\nThe remainder of the \u2206\u0302p-dependence (the \u201cmean portion\u201d) depends on the norm of E[\u2206\u0302p] = \u03b3 \u2211\nj ej max(0, gj(w)). It is here that our use of multiplicative p-updates becomes significant, because with such updates the relevant norm is the \u2113\u221e norm, instead of e.g. the \u21132 norm (as would be the case if we updated p using SGD), thus we can bound \u2225 \u2225\n\u2225E[\u2206\u0302p] \u2225 \u2225 \u2225\n\u221e with no explicit m-dependence.\nThe following theorem on the convergence rate of LightTouch is proved by applying a mirror descent bound for saddle point problems while bounding the stochastic gradient norms as described above.\nTheorem 1. Suppose that the conditions of Lemma 1 apply, with g(w) = maxi(gi(w)). Define Dw \u2265 max{1, \u2016w \u2212 w\u2032\u20162} as a bound on the diameter of W (notice that we also choose Dw to be at least 1), Gf \u2265 \u2225 \u2225\u2206\u030c(t) \u2225 \u2225\n2 and Gg \u2265\n\u2225 \u2225\u2207\u030cmax(0, gi(w)) \u2225 \u2225\n2 as uniform upper bounds on the (stochastic) gradient magnitudes\nof f and the gis, respectively, for all i \u2208 {1, . . . ,m} and w,w\u2032 \u2208 W . We also assume that all gis are Lg-Lipschitz w.r.t. \u2016\u00b7\u20162, i.e. |gi(w)\u2212 gi(w\u2032)| \u2264 Lg \u2016w \u2212 w\u2032\u20162. Our result will be expressed in terms of a total iteration count T\u01eb satisfying:\nT\u01eb = O\n(\n(lnm)D2w (Gf + \u03b3Gg + \u03b3LgDw) 2 ln 1\u03b4\n\u01eb2\n)\n.\nDefine:\nk =\n\n  \nm (1 + lnm) 3/4\n\u221a 1 + ln 1\u03b4 \u221a 1 + lnT\u01eb\nT 1/4 \u01eb\n\n  \n.\nIf k \u2264 m, then we optimize Equation 1 using T\u01eb iterations of Algorithm 2 (LightTouch), basing the stochastic gradients w.r.t. p on k constraints at each iteration, and using the step size:\n\u03b7 =\n\u221a 1 + lnmDw\n(Gf + \u03b3Gg + \u03b3LgDw) \u221a T\u01eb .\nIf k > m, then LightTouch would check more than m constraints per iteration anyway, so we instead use T\u01eb iterations of Algorithm 1 (FullTouch) with the step size:\n\u03b7 = Dw\n(Gf + \u03b3Gg) \u221a T\u01eb .\nIn either case, we perform T\u01eb iterations, requiring a total of C\u01eb \u201cconstraint checks\u201d (evaluations or differentiations of a single gi):\nC\u01eb =O\u0303\n(\n(lnm)D2w (Gf + \u03b3Gg + \u03b3LgDw) 2 ln 1\u03b4\n\u01eb2\n+ m (lnm)\n3/2 D 3/2 w (Gf + \u03b3Gg + \u03b3LgDw) 3/2 ( ln 1\u03b4\n)5/4\n\u01eb3/2\n)\n.\nand with probability 1\u2212 \u03b4:\nf (\u03a0g (w\u0304))\u2212 f (w\u2217) \u2264 h (w\u0304)\u2212 h (w\u2217) \u2264 \u01eb and \u2016w\u0304 \u2212\u03a0g (w\u0304)\u20162 \u2264 \u01eb\n\u03b3\u03c1\u2212 Lf ,\nwhere w\u2217 \u2208 {w \u2208 W : \u2200i.gi(w) \u2264 0} is an arbitrary constraint-satisfying reference vector.\nProof. In Appendix C.2.\nThe most important thing to notice about this theorem is that the dominant terms in the bounds on the number of iterations and number of constraint checks are roughly \u03b32 lnm times the usual 1/\u01eb2 convergence rate for SGD on a non-strongly convex objective. The lower-order terms have a worse m-dependence, however, with the result that, as the desired suboptimality \u01eb shrinks, the algorithm performs fewer constraint checks per iteration until ultimately (once \u01eb is on the order of 1/m2) only a constant number are checked during each iteration."}, {"heading": "3.3 MidTouch: Strong Convexity", "text": "To this point, we have only required that the objective function f be convex. However, roughly the same approach also works when f is taken to be \u03bb-strongly convex, although we have only succeeded in proving an in-expectation result, and the algorithm, Algorithm 3 (MidTouch), differs from LightTouch not only in that the w updates use a 1/\u03bbt step size, but also in being a two-phase algorithm, the first of which, like FullTouch, checks every constraint at each iteration, and the second of which, like LightTouch with k = 1, checks only two. The following theorem bounds the convergence rate if we perform T1 \u2248 m\u03c42 iterations in the first phase and T2 \u2248 \u03c43 in the second, where the parameter \u03c4 determines the total number of iterations performed:\nAlgorithm 3 (MidTouch) Minimizes a \u03bb-strongly convex f on W subject to the constraints gi(w) \u2264 0 for i \u2208 {1, . . . ,m}. The algorithm consists of two phases: the first T1 iterations proceed like FullTouch, with every constraint being checked; the final T2 iterations proceed like LightTouch, with only a constant number of constraints being checked during each iteration, and an auxiliary probability distribution p being learned along the way. Notice that while second-phase p-update checks only one constraint, it, like LightTouch, requires O(m) arithmetic operations. This issue is discussed further in Section 5.1.\nHyperparameters: T1, T2, \u03b7 1 // First phase 2 Initialize w(1) \u2208 W arbitrarily 3 For t = 1 to T1: 4 Sample \u2206\u030c(t) // stochastic subgradient of f(w(t)) 5 Let \u2206\u030c(t)w = \u2206\u030c(t) + \u03b3\u2207\u030cmax{0, g(w(t))} 6 Update w(t+1) = \u03a0w(w(t) \u2212 (1/\u03bbt)\u2206\u030c(t)w ) // \u03a0w projects its argument onto W w.r.t. \u2016\u00b7\u20162 7 // Second phase 8 Average w(T1+1) = (\n\u2211T1 t=1 w (t))/T1 // initialize second phase to result of first 9 Initialize p(T1+1) \u2208 \u2206m to the uniform distribution\n10 Initialize \u00b5(T1+1)j = max{0, gj(w(T1+1))} 11 For t = T1 + 1 to T1 + T2: 12 Sample \u2206\u030c(t) 13 Sample i(t) \u223c p(t) 14 Let \u2206\u030c(t)w = \u2206\u030c(t) + \u03b3\u2207\u030cmax{0, gi(t)(w(t))} 15 Update w(t+1) = \u03a0w(w(t) \u2212 (1/\u03bbt)\u2206\u030c(t)w ) 16 Sample j(t) \u223c Unif{1, . . . ,m} 17 Let \u2206\u0302(t)p = \u03b3\u00b5(t) + \u03b3mej(t)(max{0, gj(t)(w(t))} \u2212 \u00b5(t)j(t) ) 18 Let \u00b5(t+1)k = \u00b5 (t) k if k 6= j(t), otherwise \u00b5 (t+1) j(t) = max{0, gj(t)(w(t))} 19 Update p\u0303(t+1) = exp(ln p(t) + \u03b7\u2206\u0302(t)p ) // element-wise exp and ln 20 Project p(t+1) = p\u0303(t+1)/ \u2225 \u2225p\u0303(t+1) \u2225 \u2225\n1\n21 Average w\u0304 = ( \u2211T1+T2 t=T1+1 w(t))/T2 22 Return \u03a0g(w\u0304) // optional if small constraint violations are acceptable\nTheorem 2. Suppose that the conditions of Lemma 1 apply, with g(w) = maxi(gi(w)). Define Gf \u2265 \u2225 \u2225\u2206\u030c(t) \u2225 \u2225\n2\nand Gg \u2265 \u2225 \u2225\u2207\u030cmax(0, gi(w)) \u2225 \u2225\n2 as uniform upper bounds on the (stochastic) gradient magnitudes of f and the gis,\nrespectively, for all i \u2208 {1, . . . ,m}. We also assume that f is \u03bb-strongly convex, and that all gis are Lg-Lipschitz w.r.t. \u2016\u00b7\u20162, i.e. |gi(w)\u2212 gi(w\u2032)| \u2264 Lg \u2016w \u2212 w\u2032\u20162 for all w,w\u2032 \u2208 W . If we run Algorithm 3 (MidTouch) with the p-update step size \u03b7 = \u03bb/2\u03b32L2g for T\u01eb1 iterations in the first phase and T\u01eb2 in the second:\nT\u01eb1 =O\u0303\n(\nm (lnm) 2/3 (Gf + \u03b3Gg + \u03b3Lg) 4/3\n\u03bb4/3\u01eb2/3 +\nm2 (lnm) (Gf + \u03b3Gg)\n\u03bb \u221a \u01eb\n)\n,\nT\u01eb2 =O\u0303\n(\n(lnm) (Gf + \u03b3Gg + \u03b3Lg) 2\n\u03bb2\u01eb +\nm3/2 (lnm) 3/2 (Gf + \u03b3Gg) 3/2\n\u03bb3/2\u01eb3/4\n)\n,\nrequiring a total of C\u01eb \u201cconstraint checks\u201d (evaluations or differentiations of a single gi):\nC\u01eb =O\u0303\n(\n(lnm) (Gf + \u03b3Gg + \u03b3Lg) 2\n\u03bb2\u01eb +\nm 3/2 (lnm) 3/2 (Gf + \u03b3Gg) 3/2\n\u03bb3/2\u01eb3/4\n+ m2 (lnm) 2/3 (Gf + \u03b3Gg + \u03b3Lg) 4/3\n\u03bb4/3\u01eb2/3 +\nm3 (lnm) (Gf + \u03b3Gg)\n\u03bb \u221a \u01eb\n)\n,\nthen: E [\n\u2016\u03a0g(w\u0304)\u2212 w\u2217\u201622 ] \u2264 E [ \u2016w\u0304 \u2212 w\u2217\u201622 ] \u2264 \u01eb,\nwhere w\u2217 = argmin{w\u2208W:\u2200i.gi(w)\u22640} f(w) is the optimal constraint-satisfying reference vector.\nProof. In Appendix D.\nNotice that the above theorem bounds not the suboptimality of \u03a0g(w\u0304), but rather its squared Euclidean distance from w\u2217, for which reason the denominator of the highest order term depends on \u03bb2 rather than \u03bb. Like Theorem 1 in the non-strongly convex case, the dominant terms above, both in terms of the total number of iterations and number of constraint checks, match the usual 1/\u01eb convergence rate for unconstrained strongly-convex SGD with an additional \u03b32 lnm factor, while the lower-order terms have a worse m-dependence. As before, fewer constraint checks will be performed per iteration as \u01eb shrinks, reaching a constant number (on average) once \u01eb is on the order of 1/m6."}, {"heading": "4 Theoretical Comparison", "text": "Table 2 compares upper bounds on the convergence rates and per-iteration costs when applied to a convex (but not necessarily strongly convex) problem for LightTouch, FullTouch, projected SGD, the online Frank-Wolfe algorithm of Hazan and Kale [2012], and a Frank-Wolfe-like online algorithm for optimization over a polytope [Garber and Hazan, 2013]. The latter algorithm, which we refer to as LLO-FW, achieves convergence rates comparable to projected SGD, but uses a local linear oracle instead of a projection or full linear optimization. To simplify the presentation, the dependencies on Lg, Gf and Gg have been dropped\u2014please refer to Theorems 1 and 2 and the cited references for the complete statements. Table 3 contains the same comparison (without online Frank-Wolfe) for \u03bb-strongly convex problems.\nAt each iteration, all of these algorithms must find a stochastic subgradient of f . In addition, each iteration of LightTouch and MidTouch must perform O(m) arithmetic operations (for the m-dimensional vector operations used when updating p)\u2014this issue will be discussed further in Section 5.1. However, projected SGD must project its iterate onto the constraints w.r.t. the Euclidean norm, online Frank-Wolfe must perform a linear optimization subject to the constraints, and LLO-FW must evaluate a local linear oracle, which amounts to essentially local linear optimization.\nLightTouch, MidTouch and FullTouch share the same \u03b3-dependence, but the m-dependence of the convergence rate of LightTouch and MidTouch is logarithmically worse. The number of constraint evaluations, however, is better: in the non-strongly convex case, ignoring all but the m and \u01eb dependencies, FullTouch will checkO(m/\u01eb2) constraints, while LightTouch will check only O\u0303((lnm)/\u01eb2+m/\u01eb3/2), a significant improvement when \u01eb is small. Hence, particularly for problems with many expensive-to-evaluate constraints, one would expect LightTouch to converge much more rapidly. Likewise, for \u03bb-strongly convex optimization, the dominant (in \u01eb) terms in the bounds on the number of constraint evaluations go as m/\u01eb for FullTouch, and as (lnm)/\u01eb for MidTouch, although the lower-order terms in the MidTouch bound are significantly more complex than in the non-strongly convex case (see Table 3 for full details).\nComparing with projected SGD, online Frank-Wolfe and LLO-FW is less straightforward, not only because we\u2019re comparing upper bounds to upper bounds (with all of the uncertainty that this entails), but also because we must relate the value of \u03b3 to the cost of performing the required projection, constrained linear optimization or local linear oracle evaluation. We note, however, that for non-strongly convex optimization, the \u01eb-dependence of the convergence rate bound is worse for online Frank-Wolfe (1/\u01eb3) than for the other algorithms (1/\u01eb2), and that unless the constraints have some special structure, performing a projection can be a very expensive operation.\nFor example, with general linear inequality constraints, each constraint check performed by LightTouch, MidTouch or FullTouch requires O(d) time, whereas each linear program optimized by online Frank-Wolfe could be solved in O(d2m) time [Nemirovski, 2004, Chapter 10.1], and each projection performed by SGD in O((dm)3/2) time [Goldfarb and Liu, 1991]. When the constraints are taken to be arbitrary convex functions, instead of linear functions, projections may be even more difficult.\nWe believe that in many cases \u03b32 will be roughly on the order of the dimension d, or number of constraints m, whichever is smaller, although it can be worse for difficult constraint sets (see Section 2.2). In practice, we have found that a surprisingly small \u03b3\u2014we use \u03b3 = 1 in our experiments (Section 6)\u2014often suffices to result in convergence to a feasible solution. With this in mind, and in light of the fact that a fast projection, linear optimization, or local linear oracle evaluation may only be possible for particular constraint sets, we believe that our algorithms compare favorably with the alternatives.\nAlgorithm 4 (Practical LightTouch) Our proposed \u201cpractical\u201d algorithm combining LightTouch and MidTouch, along with the changes discussed in Section 5.\nHyperparameters: T , \u03b7w, \u03b7p 1 Initialize w(1) \u2208 W arbitrarily 2 Initialize p(1) \u2208 \u2206m to the uniform distribution 3 Initialize \u00b5(1)j = max{0, gj(w(1))} // 0 if w(1) is feasible 4 For t = 1 to T : 5 Let \u03b7(t)w = \u03b7w/t if f is strongly convex, \u03b7w/ \u221a t otherwise 6 Set k(t)f , k (t) g and k (t) p as described in Section 5.2 7 Sample \u2206\u030c(t)1 , . . . , \u2206\u030c (t)\nk (t) f\ni.i.d. // stochastic subgradients of f(w(t))\n8 Sample i(t)1 , . . . , i (t)\nk (t) g\n\u223c p(t) i.i.d.\n9 Let \u2206\u030c(t)w = (1/k (t) f )\n\u2211k (t) f\nj=1 \u2206\u030c (t) j + (\u03b3/k (t) g ) \u2211k(t)g j=1 \u2207\u030cmax{0, gi(t)j (w\n(t))} 10 Update w(t+1) = \u03a0w(w(t) \u2212 \u03b7(t)w \u2206\u030c(t)w ) // \u03a0w projects its argument onto W w.r.t. \u2016\u00b7\u20162 11 Sample S(t) \u2286 {1, . . . ,m} with \u2223 \u2223S(t) \u2223 \u2223 = k (t) p uniformly without replacement 12 Let \u2206\u0302(t)p = \u03b3\u00b5(t) + (\u03b3m/k (t) p ) \u2211 j\u2208S(t) ej(max{0, gj(w(t))} \u2212 \u00b5 (t) j ) 13 Let \u00b5(t+1)j = max{0, gj(w(t))} if j \u2208 S(t), otherwise \u00b5 (t+1) j = \u00b5 (t) j 14 Update p\u0303(t+1) = exp(ln p(t) + \u03b7p\u2206\u0302 (t) p ) // element-wise exp and ln 15 Project p(t+1) = p\u0303(t+1)/ \u2225 \u2225p\u0303(t+1) \u2225 \u2225\n1\n16 Average w\u0304 = ( \u2211T t=1 w (t))/T 17 Return \u03a0g(w\u0304) // optional if small constraint violations are acceptable"}, {"heading": "5 Practical Considerations", "text": "Algorithms 2 and 3 were designed primarily to be easy to analyze, but in real world-applications we recommend making a few tweaks to improve performance. The first of these is trivial: using a decreasing w-update step size \u03b7 (t) w = \u03b7w/ \u221a t when optimizing a non-strongly convex objective, and \u03b7(t)w = \u03b7w/t for a strongly-convex objective. In both cases we continue to use a constant p-update step size \u03b7p. This change, as well as that described in Section 5.2, is included in Algorithm 4."}, {"heading": "5.1 Constraint Aggregation", "text": "A natural concern about Algorithms 2 and 3 is that O(m) arithmetic operations are performed per iteration, even when only a few constraints are checked. When each constraint is expensive, this is a minor issue, since this cost will be \u201cdrowned out\u201d by that of checking the constraints. However, when the constraints are very cheap, and the O(m) arithmetic cost compares disfavorably with the cost of checking a handful of constraints, it can become a bottleneck.\nOur solution to this issue is simple: transform a problem with a large number of cheap constraints into one with a smaller number of more expensive constraints. To this end, we partition the constraint indices 1, . . . ,m into m\u0303 sets {Mi} of size at most \u2308m/m\u0303\u2309, defining g\u0303i(w) = maxj\u2208Mi gj(w), and then apply LightTouch or MidTouch on the m\u0303 aggregated constraints g\u0303i(w) \u2264 0. This makes each constraint check \u2308m/m\u0303\u2309 times more expensive, but reduces the dimension of p from m to m\u0303, shrinking the per-iteration arithmetic cost to O(m\u0303)."}, {"heading": "5.2 Automatic Minibatching", "text": "Because LightTouch takes a minibatch size k as a parameter, and the constants from which we derive the recommended choice of k (Theorem 1) are often unknown, a user is in the uncomfortable position of having to perform a parameter search not only over the step sizes \u03b7w and \u03b7p, but also the minibatch size. Furthermore, the fact that the theoreticallyrecommended k is a decreasing function of T indicates that it might be better to check more constraints in early iterations, and fewer in later ones. Likewise, MidTouch is structured as a two-phase algorithm, in which every iteration checks every constraint in the first phase, and only a constant number in the second, but it seems more sensible for the number of constraint checks to decrease gradually over time.\nIn addition, for both algorithms, it would be desirable to support separate minibatching of the loss and constraint stochastic subgradients (w.r.t. w), in which case there would be three minibatching parameters to determine: kf , kg and kp. This makes things even harder for the user, since now there are three additional parameters that must be specified.\nTo remove the need to specify any minibatch-size hyperparameters, and to enable the minibatch sizes to change from iteration-to-iteration, we propose a heuristic that will automatically determine the minibatch sizes k(t)f , k (t) g and k (t) p for each of the stochastic gradient components at each iteration. Intuitively, we want to choose minibatch sizes in such a way that the stochastic gradients are both cheap to compute and have low variance. Our proposed heuristic does this by trading-off the computational cost and \u201cbound impact\u201d of the overall stochastic gradient, where the \u201cbound impact\u201d is a variance-like quantity that approximates the impact that taking a step with particular minibatch sizes has on the relevant convergence rate bound.\nSuppose that we\u2019re about to perform the tth iteration, and know that a single stochastic subgradient \u2206\u030c of f(w) (corresponding to the loss portion of \u2206\u030cw) has variance (more properly, covariance matrix trace) v\u0304 (t) f and requires a computational investment of c\u0304(t)f units. Similarly, if we define \u2206\u030cg by sampling i \u223c p and taking \u2206\u030cg = \u03b3\u2207\u030cmax{0, gi(w)} (corresponding to the constraint portion of \u2206\u030cw), then we can define variance and cost estimates of \u2206\u030cg to be v\u0304 (t) g and c\u0304 (t) g , respectively. Likewise, we take v\u0304 (t) p and c\u0304 (t) p to be estimates of the variance and cost of a (non-minibatched version of) \u2206\u0302p.\nIn all three cases, the variance and cost estimates are those of a single sample, implying that a stochastic subgradient of f(w) averaged over a minibatch of size k(t)f will have variance v\u0304 (t) f /k (t) f and require a computational investment of c\u0304 (t) f k (t) f , and likewise for the constraints and distribution. In the context of Algorithm 4, with minibatch sizes of k (t) f , k (t) g and k (t) p , we define the overall bound impact b and computational cost c of a single update as:\nb = \u03b7 (t) w v\u0304 (t) f\nk (t) f\n+ \u03b7 (t) w v\u0304 (t) g\nk (t) g\n+ \u03b7pv\u0304\n(t) p\nk (t) p\nand c = c\u0304(t)f k (t) f + c\u0304 (t) g k (t) g + c\u0304 (t) p k (t) p .\nWe should emphasize that the above definition of b is merely a useful approximation of how these quantities truly affect our bounds.\nGiven the three variance and three cost estimates, we choose minibatch sizes in such a way as to minimize both the computational cost and bound impact of an update. Imagine that we are given a fixed computational budget c. Then our goal will be to choose the minibatch sizes in such a way that b is minimized for this budget, a problem that is easily solved in closed form:\n[\nk (t) f , k (t) g , k (t) p\n]\n\u221d\n\n\n\u221a \u221a \u221a \u221a \u03b7 (t) w v\u0304 (t) f\nc\u0304 (t) f\n,\n\u221a \u221a \u221a \u221a \u03b7 (t) w v\u0304 (t) g\nc\u0304 (t) g\n,\n\u221a \u221a \u221a \u221a \u03b7pv\u0304 (t) p\nc\u0304 (t) p\n\n .\nWe propose choosing the proportionality constant (and thereby the cost budget c) in such a way that k(t)f = 2 (enabling us to calculate sample variances, as explained below), and round the two other sizes to the nearest integers, lowerbounding each so that k(t)g \u2265 2 and k(t)p \u2265 1.\nWhile the variances and costs are not truly known during optimization, they are easy to estimate from known quantities. For the costs c\u0304(t)f , c\u0304 (t) g and c\u0304 (t) p , we simply time how long each past stochastic gradient calculation has taken, and then average them to estimate the future costs. For the variances v\u0304(t)f and v\u0304 (t) g , we restrict ourselves to minibatch sizes k (t) f , k (t) g \u2265 2, calculate the sample variances v(t)f and v (t) g of the stochastic gradients at each iteration, and then average over all past iterations (either uniformly, or a weighted average placing more weight on recent iterations).\nFor v\u0304(t)p , the situation is a bit more complicated, since the p-updates are multiplicative (so we should use an \u2113\u221e variance) and centered as in Equation 4. Upper-bounding the \u2113\u221e norm with the \u21132 norm and using the fact that the minibatch S(t) is independently sampled yields the following crude estimate:\nv(t)p = \u03b3 2m2\n\n\n1\nk (t) p\n\u2211\ni\u2208S(t)\n( \u00b5i \u2212max { 0, gi ( w(t) )})2\n\n ,\nWe again average v(t)p across past iterations to estimate v\u0304 (t) p ."}, {"heading": "6 Experiments", "text": "We validated the performance of our practical variant of LightTouch (Algorithm 4) on a YouTube ranking problem in the style of Joachims [2002], in which the task is to predict what a user will watch next, given that they have just viewed a certain video. In this setting, a user has just viewed video a, was presented with a list of candidate videos to watch next, and clicked on b+, with b\u2212 being the video immediately preceding b+ in the list (if b+ was the first list element, then the example is thrown out).\nWe used an anonymized proprietary dataset consisting of n = 612 587 training pairs of feature vectors (x+, x\u2212), where x+ is a vector of 12 features summarizing the similarity between a and b+, and x\u2212 between a and b\u2212.\nWe treat this as a standard pairwise ranking problem, for which the goal is to estimate a function f(\u03a6(x)) = \u3008w,\u03a6(x)\u3009 such that f(\u03a6(x+)) > f(\u03a6(x\u2212)) for as many examples as possible, subject to the appropriate regularization (or, in this case, constraints). Specifically, the (unconstrained) learning task is to minimize the average empirical hinge loss:\nmin w\u2208W\n1\nn\nn \u2211\ni=1\n( max { 0, 1\u2212 \u2329 w,\u03a6 ( x+i ) \u2212 \u03a6 ( x\u2212i )\u232a}) .\nAll twelve of the features were designed to provide positive evidence\u2014in other words, if any one increases (holding the others fixed), then we expect f(\u03a6(x)) to increase. We have found that using constraints to enforce this monotonicity property results in a better model in practice.\nWe define \u03a6(\u00b7) as in lattice regression using simplex interpolation [Garcia et al., 2012, Gupta et al., 2016], an approach which works well at combining a small number of informative features, and more importantly (for our purposes) enables one to force the learned function to be monotonic via linear inequality constraints on the parameters. For the resulting problem, the feature vectors have dimension d = 212 = 4096, we chose W to be defined by the box constraints \u221210 \u2264 wi \u2264 10 in each of the 4096 dimensions, and the total number of monotonicity-enforcing linear inequality constraints is m = 24 576.\nEvery \u03a6(x) contains only d + 1 = 13 nonzeros and can be computed in O(d ln d) time. Hence, stochastic gradients of f are inexpensive to compute. Likewise, checking a monotonicity constraint only requires a single comparison between two parameter values, so although there are a large number of them, each constraint is very inexpensive to check."}, {"heading": "6.1 Implementations", "text": "We implemented all algorithms in C++. Before running our main experiments, we performed crude parameter searches on a power-of-four grid (i.e. . . . , 1/16, 1/4, 1, 4, 16, . . .). For each candidate value we performed roughly 10 000 iterations, and chose the parameter that appeared to result in the fastest convergence in terms of the objective function.\nLightTouch Our implementation of LightTouch includes all of the suggested changes of Section 5, including the constraint aggregation approach of Section 5.1, although we used no aggregation until our timing comparison (Section 6.3). For automatic minibatching, we took weighted averages of the variance estimates as v\u0304(t+1) \u221d v(t) + \u03bdv\u0304(t). We found that up-weighting recent estimates (taking \u03bd < 1) resulted in a noticeable improvement, but that the precise value of \u03bd mattered little (we used \u03bd = 0.999). Based on the grid search described above, we chose \u03b3 = 1, \u03b7w = 16 and \u03b7p = 1/16.\nFullTouch Our FullTouch implementation differs from that in Algorithm 1 only in that we used a decreasing step size \u03b7 (t) w = \u03b7w/ \u221a t. As with LightTouch, we chose \u03b3 = 1 and \u03b7w = 16 based on a grid search.\nProjectedSGD We implemented Euclidean projections onto lattice monotonicity constraints using IPOPT [Wa\u0308chter and Biegler, 2006] to optimize the resulting sparse 4096-dimensional quadratic program. However, the use of a QP solver for projected SGD\u2014a very heavyweight solution\u2014resulted in an implementation that was too slow to experiment with, requiring nearly four minutes per projection (observe that our experiments each ran for millions of iterations).\nApproxSGD This is an approximate projected SGD implementation using the fast approximate update procedure described in Gupta et al. [2016], which is an active set method that, starting from the current iterate, moves along the boundary of the feasible region, adding constraints to the active set as they are encountered, until the desired step is exhausted (this is reminiscent of the local linear oracles considered by Garber and Hazan [2013]). This approach is particularly well-suited to this particular constraint set because (1) when checking constraints for possible inclusion in the active set, it exploits the sparsity of the stochastic gradients to only consider monotonicity constraints which could possibly be violated, and (2) projecting onto an intersection of active monotonicity constraints reduces to uniformly averaging every set of parameters that are \u201clinked together\u201d by active constraints. Like the other algorithms, we used step sizes of \u03b7(t)w = \u03b7w/ \u221a t and chose \u03b7w = 64 based on the grid search (recall that \u03b7w = 16 was better for the other two algorithms).\nIn every experiment we repeatedly looped over a random permutation of the training set, and generated plots by averaging over 5 such runs (with the same 5 random permutations) for each algorithm."}, {"heading": "6.2 Constraint-check Comparison", "text": "In our first set of experiments, we compared the performance of LightTouch, FullTouch and ApproxSGD in terms of the number of stochastic subgradients of f drawn, and the number of constraints checked. Because LightTouch\u2019s automatic minibatching fixes k(t)f = 2 (with the other two minibatch sizes being automatically determined), in these experiments we used minibatch sizes of 2 for FullTouch and ApproxSGD, guaranteeing that all three algorithms observe the same number of stochastic subgradients of f at each iteration.\nThe left-hand plot of Figure 1 shows that all three algorithms converge at roughly comparable per-iteration rates, with ApproxSGD having a slight advantage over FullTouch, which itself converges a bit more rapidly than LightTouch. The right-hand plot shows a striking difference, however\u2014LightTouch reaches a near-optimal solution having checked more than 10\u00d7 fewer constraints than FullTouch. Notice that we plot the suboptimalities of the projected iterates \u03a0w(w\n(t)) rather than of the w(t)s themselves, in order to emulate the final projection (line 7 of Algorithm 1 and 17 of Algorithm 4), and guarantee that we only compare the average losses of feasible intermediate solutions.\nIn Figure 2, we explore how well our algorithms enforce feasibility, and how effective automatic minibatching is at choosing minibatch sizes. The left-hand plot shows that both FullTouch has converged to a nearly-feasible solution\nafter roughly 10 000 iterations, and LightTouch (unsurprisingly) takes more, perhaps 100 000 or so. In the right-hand plot, we see that, in line with our expectations (see Section 5.2), LightTouch\u2019s automatic minibatching results in very few constraints being checked in late iterations.\n6.3 Timing Comparison\nOur final experiment compared the wall-clock runtimes of our implementations. Note that, because each monotonicity constraint can be checked with only a single comparison (compare with e.g. O(d) arithmetic operations for a dense linear inequality constraint), the O(m) arithmetic cost of maintaining and updating the probability distribution p over the constraints is significant. Hence, in terms of the constraint costs, this is nearly a worse-case problem for LightTouch. We experimented with power-of-4 constraint aggregate sizes (Section 5.1), and found that using m\u0303 = 96 aggregated constraints, each of size 256, worked best.\nFullTouch, without minibatching, draws a single stochastic subgradient of f and checks every constraint at each iteration. However, it would seem to be more efficient to use minibatching to look at more stochastic subgradients at each iteration, and therefore fewer constraints per stochastic subgradient of f . Hence, for FullTouch, we again searched over power-of-4 minibatch sizes, and found that 16 worked best.\nFor ApproxSGD, the situation is less clear-cut. On the one hand, increasing the minibatch size results in fewer approximate projections being performed per stochastic subgradient of f . On the other, averaging more stochastic\nsubgradients results in less sparsity, slowing down the approximate projection. We found that the latter consideration wins out\u2014after searching again over power-of-4 minibatch sizes, we found that a minibatch size of 1 (i.e. no minibatching) worked best.\nFigure 3 contains the results of these experiments, showing that both FullTouch and LightTouch converge significantly faster than ApproxSGD. Interestingly, ApproxSGD is rather slow in early iterations (clipped off in plot), but accelerates in later iterations. We speculate that the reason for this behavior is that, close to convergence, the steps taken at each iteration are smaller, and therefore the active sets constructed during the approximate projection routine do not grow as large. FullTouch enjoys a small advantage over LightTouch until both algorithms are very close to convergence, but based on the results of Section 6.2, we believe that this advantage would reverse if there were more constraints, or if the constraints were more expensive to check."}, {"heading": "7 Conclusions", "text": "We have proposed an efficient strategy for large-scale heavily constrained optimization, building on the work of Mahdavi et al. [2012], and analyze its performance, demonstrating that, asymptotically, our approach requires many fewer constraint checks in order to converge.\nWe build on these theoretical results to propose a practical variant. The most significant of these improvements is based on the observation that our algorithm takes steps based on three separate stochastic gradients, and that trading off the variances of computational costs of these three components is beneficial. To this end, we propose a heuristic for dynamically choosing minibatch sizes in such a way as to encourage faster convergence at a lower computational cost.\nExperiments on a real-world 4096-dimensional machine learning problem with 24 576 constraints and 612 587 training examples\u2014too large for a QP-based implementation of projected SGD\u2014showed that our proposed method is effective. In particular, we find that, in practice, our technique checks fewer constraints per iteration than competing algorithms, and, as expected, checks ever fewer as optimization progresses."}, {"heading": "Acknowledgments", "text": "We thank Kevin Canini, Mahdi Milani Fard, Andrew Frigyik, Michael Friedlander and Seungil You for helpful discussions, and proofreading earlier drafts."}, {"heading": "A Mirror Descent", "text": "Mirror descent [Nemirovski and Yudin, 1983, Beck and Teboulle, 2003] is a meta-algorithm for stochastic optimization (more generally, online regret minimization) which performs gradient updates with respect to a meta-parameter, the distance generating function (d.g.f.). The two most widely-used d.g.f.s are the squared Euclidean norm and negative Shannon entropy, for which the resulting MD instantiations are stochastic gradient descent (SGD) and a multiplicative updating algorithm, respectively. These are precisely the two d.g.f.s which our constrained algorithm will use for the updates of w and p. We\u2019ll here give a number of results which differ only slightly from \u201cstandard\u201d ones, beginning with a statement of an online MD bound adapted from Srebro et al. [2011]:\nTheorem 3. Let \u2016\u00b7\u2016 and \u2016\u00b7\u2016\u2217 be a norm and its dual. Suppose that the distance generating function (d.g.f.) \u03a8 is 1-strongly convex w.r.t. \u2016\u00b7\u2016. Let \u03a8\u2217 be the convex conjugate of \u03a8, and take B\u03a8(w|w\u2032) = \u03a8(w) \u2212 \u03a8(w\u2032) \u2212 \u3008\u2207\u03a8(w\u2032), w \u2212 w\u2032\u3009 to be the associated Bregman divergence. Take ft : W \u2192 R to be a sequence of convex functions on which we perform T iterations of mirror descent starting from w(1) \u2208 W:\nw\u0303(t+1) = \u2207\u03a8\u2217 ( \u2207\u03a8 ( w(t) ) \u2212 \u03b7\u2207\u030cft ( w(t) )) ,\nw(t+1) = argmin w\u2208W B\u03a8\n( w \u2223 \u2223 \u2223 w\u0303(t+1) ) ,\nwhere \u2207\u030cft(w(t)) \u2208 \u2202ft(w(t)) is a subgradient of ft at w(t). Then:\n1\nT\nT \u2211\nt=1\n(\nft\n( w(t) ) \u2212 ft (w\u2217) ) \u2264 B\u03a8 ( w\u2217 \u2223 \u2223 w(1) )\n\u03b7T +\n\u03b7\n2T\nT \u2211\nt=1\n\u2225 \u2225 \u2225\u2207\u030cft ( w(t) )\u2225 \u2225 \u2225 2\n\u2217 ,\nwhere w\u2217 \u2208 W is an arbitrary reference vector.\nProof. This proof is essentially the same as that of Srebro et al. [2011, Lemma 2]. By convexity:\n\u03b7 (\nft\n( w(t) ) \u2212 ft (w\u2217) ) \u2264 \u2329 \u03b7\u2207\u030cft ( w(t) ) , w(t) \u2212 w\u2217 \u232a\n\u2264 \u2329 \u03b7\u2207\u030cft ( w(t) ) , w(t) \u2212 w\u0303(t+1) \u232a + \u2329 \u03b7\u2207\u030cft ( w(t) ) , w\u0303(t+1) \u2212 w\u2217 \u232a ."}, {"heading": "By Ho\u0308lder\u2019s inequality, \u3008w\u2032, w\u3009 \u2264 \u2016w\u2032\u2016 \u2016w\u2016\u2217. Also, \u03a8(w) = supv(\u3008v, w\u3009\u2212\u03a8\u2217(v)) is maximized when\u2207\u03a8\u2217(v) = w,", "text": "so \u2207\u03a8(\u2207\u03a8\u2217(v)) = v. These results combined with the definition of w\u0303(t+1) give:\n\u03b7 (\nft\n( w(t) ) \u2212 ft (w\u2217) ) \u2264 \u2225 \u2225 \u2225\u03b7\u2207\u030cft ( w(t) )\u2225 \u2225 \u2225\n\u2217\n\u2225 \u2225 \u2225w(t) \u2212 w\u0303(t+1) \u2225 \u2225 \u2225\n+ \u2329 \u2207\u03a8 ( w(t) ) \u2212\u2207\u03a8 ( w\u0303(t+1) ) , w\u0303(t+1) \u2212 w\u2217 \u232a .\nUsing Young\u2019s inequality and the definition of the Bregman divergence:\n\u03b7 (\nft\n( w(t) ) \u2212 ft (w\u2217) ) \u2264 1 2\n\u2225 \u2225 \u2225\u03b7\u2207\u030cft ( w(t) )\u2225 \u2225 \u2225 2\n\u2217 +\n1\n2\n\u2225 \u2225 \u2225w(t) \u2212 w\u0303(t+1) \u2225 \u2225 \u2225 2\n+B\u03a8\n( w\u2217 \u2223 \u2223 \u2223 w(t) ) \u2212B\u03a8 ( w\u2217 \u2223 \u2223 \u2223 w\u0303(t+1) ) \u2212B\u03a8 ( w\u0303(t+1) \u2223 \u2223 \u2223 w(t) ) .\nApplying the 1-strong convexity of \u03a8 to cancel the \u2225 \u2225w(t) \u2212 w\u0303(t+1) \u2225 \u2225 2 /2 and B\u03a8(w\u0303(t+1) | w(t)) terms:\n\u03b7 (\nft\n( w(t) ) \u2212 ft (w\u2217) ) \u2264 \u03b7 2\n2\n\u2225 \u2225 \u2225\u2207\u030cft ( w(t) )\u2225 \u2225 \u2225 2\n\u2217 +B\u03a8\n( w\u2217 \u2223 \u2223 \u2223 w(t) ) \u2212B\u03a8 ( w\u2217 \u2223 \u2223 \u2223 w\u0303(t+1) ) .\nSumming over t, using the nonnegativity of B\u03a8, and dividing through by \u03b7T gives the claimed result.\nIt is straightforward to transform Theorem 3 into an in-expectation result for stochastic subgradients:\nCorollary 1. Take ft : W \u2192 R to be a sequence of convex functions, and F a filtration. Suppose that we perform T iterations of stochastic mirror descent starting from w(1) \u2208 W , using the definitions of Theorem 3:\nw\u0303(t+1) = \u2207\u03a8\u2217 ( \u2207\u03a8 ( w(t) ) \u2212 \u03b7\u2206\u030c(t) ) ,\nw(t+1) = argmin w\u2208W B\u03a8\n( w \u2223 \u2223 \u2223 w\u0303(t+1) ) ,\nwhere \u2206\u030c(t) is a stochastic subgradient of ft, i.e. E[\u2206\u030c(t) | Ft\u22121] \u2208 \u2202ft(w(t)), and \u2206\u030c(t) is Ft-measurable. Then:\n1\nT\nT \u2211\nt=1\nE\n[\nft\n( w(t) ) \u2212 ft (w\u2217) ] \u2264 B\u03a8 ( w\u2217 \u2223 \u2223 w(1) )\n\u03b7T +\n\u03b7\n2T\nT \u2211\nt=1\nE\n[\n\u2225 \u2225 \u2225\u2206\u030c(t) \u2225 \u2225 \u2225 2\n\u2217\n]\n,\nwhere w\u2217 \u2208 W is an arbitrary reference vector.\nProof. Define f\u0303t (w) = \u2329 \u2206\u030c(t), w \u232a\n, and observe that applying the non-stochastic MD algorithm of Theorem 3 to the sequence of functions f\u0303t results in the same sequence of iterates w(t) as does applying the above stochastic MD update to the sequence of functions ft. Hence:\n1\nT\nT \u2211\nt=1\n(\nf\u0303t\n( w(t) ) \u2212 f\u0303t (w\u2217) ) \u2264 B\u03a8 ( w\u2217 \u2223 \u2223 w(1) )\n\u03b7T +\n\u03b7\n2T\nT \u2211\nt=1\n\u2225 \u2225 \u2225\u2206\u030c(t) \u2225 \u2225 \u2225 2\n\u2217 . (5)\nBy convexity, ft(w(t)) \u2212 ft(w\u2217) \u2264 \u2329 E[\u2206\u030c(t) | Ft\u22121], w(t) \u2212 w\u2217 \u232a , while f\u0303t(w(t)) \u2212 f\u0303t(w\u2217) = \u2329 \u2206\u030c(t), w(t) \u2212 w\u2217 \u232a\nby definition. Taking expectations of both sides of Equation 5 and plugging in these inequalities yields the claimed result.\nWe next prove a high-probability analogue of the Corollary 1, based on a martingale bound of Dzhaparidze and van Zanten [2001]:\nCorollary 2. In addition to the assumptions of Corollary 1, suppose that, with probability 1 \u2212 \u03b4\u03c3 , \u03c3 satisfies the following uniformly for all t \u2208 {1, . . . , T }:\n\u2225 \u2225 \u2225 E [ \u2206\u030c(t) \u2223 \u2223 \u2223 Ft\u22121 ] \u2212 \u2206\u030c(t) \u2225 \u2225 \u2225\n\u2217 \u2264 \u03c3.\nThen, with probability 1\u2212 \u03b4\u03c3 \u2212 \u03b4, the above \u03c3 bound will hold, and:\n1\nT\nT \u2211\nt=1\n(\nft\n( w(t) ) \u2212 ft (w\u2217) ) \u2264 B\u03a8 ( w\u2217 \u2223 \u2223 w(1) )\n\u03b7T +\n\u03b7\n2T\nT \u2211\nt=1\n\u2225 \u2225 \u2225\u2206\u030c(t) \u2225 \u2225 \u2225 2\n\u2217 +\n\u221a 2R\u2217\u03c3 \u221a\nln 1\u03b4\u221a T + 2R\u2217\u03c3 ln 1 \u03b4 3T ,\nwhere w\u2217 \u2208 W is an arbitrary reference vector and R\u2217 \u2265 supw\u2208W \u2016w \u2212 w\u2217\u2016 bounds the radius of W centered on w\u2217.\nProof. Define f\u0303t (w) = \u2329 \u2206\u030c(t), w \u232a\nas in the proof of Corollary 1, and observe that Equation 5 continues to apply. Define a sequence of random variables M0 = 0, Mt = Mt\u22121 + \u2329 E[\u2206\u030c(t) | Ft\u22121]\u2212 \u2206\u030c(t), w(t) \u2212 w\u2217 \u232a\n, and notice that M forms a martingale w.r.t. the filtration F . From this definition, Ho\u0308lder\u2019s inequality gives that:\n|Mt \u2212Mt\u22121| \u2264 \u2225 \u2225 \u2225E [ \u2206\u030c(t) \u2223 \u2223 \u2223 Ft\u22121 ] \u2212 \u2206\u030c(t) \u2225 \u2225 \u2225\n\u2217\n\u2225 \u2225 \u2225w(t) \u2212 w\u2217 \u2225 \u2225 \u2225 \u2264 R\u2217\u03c3.\nthe above holding with probability 1 \u2212 \u03b4\u03c3 . Plugging a = R\u2217\u03c3 and L = TR2\u2217\u03c32 into the Bernstein-type martingale inequality of Dzhaparidze and van Zanten [2001, Theorem 3.3] gives:\nPr\n{\n1 T MT \u2265 \u01eb\n} \u2264 \u03b4\u03c3 + exp ( \u2212 3T \u01eb 2\n6R2\u2217\u03c3 2 + 2R\u2217\u03c3\u01eb\n)\n.\nSolving for \u01eb using the quadratic formula and upper-bounding gives that, with probability 1\u2212 \u03b4\u03c3 \u2212 \u03b4:\n1\nT\nT \u2211\nt=1\n\u2329\nE\n[ \u2206\u030c(t) \u2223 \u2223 \u2223 Ft\u22121 ] \u2212 \u2206\u030c(t), w(t) \u2212 w\u2217 \u232a \u2264 \u221a 2R\u2217\u03c3 \u221a\nln 1\u03b4\u221a T + 2R\u2217\u03c3 ln 1 \u03b4 3T .\nAs in the proof of Corollary 1, ft(w(t)) \u2212 ft(w\u2217) \u2264 \u2329 E[\u2206\u030c(t) | Ft\u22121], w(t) \u2212 w\u2217 \u232a , while f\u0303t(w(t)) \u2212 f\u0303t(w\u2217) = \u2329\n\u2206\u030c(t), w(t) \u2212 w\u2217 \u232a by definition, which combined with Equation 5 yields the claimed result.\nAlgorithm 2 (LightTouch) jointly optimizes over two sets of parameters, for which the objective is convex in the first and linear (hence concave) in the second. The convergence rate will be determined from a saddle-point bound, which we derive from Corollary 2 by following Nemirovski et al. [2009], Rakhlin and Sridharan [2013], and simply applying it twice:\nCorollary 3. Let \u2016\u00b7\u2016w and \u2016\u00b7\u2016\u03b1 be norms with duals \u2016\u00b7\u2016w\u2217 and \u2016\u00b7\u2016\u03b1\u2217. Suppose that \u03a8w and \u03a8\u03b1 are 1-strongly convex w.r.t. \u2016\u00b7\u2016w and \u2016\u00b7\u2016\u03b1, have convex conjugates \u03a8\u2217w and \u03a8\u2217\u03b1, and associated Bregman divergences B\u03a8w and B\u03a8\u03b1 , respectively.\nTake f : W \u00d7A \u2192 R to be convex in its first parameter and concave in its second, let F be a filtration, and suppose that we perform T iterations of MD:\nw\u0303(t+1) = \u2207\u03a8\u2217w ( \u2207\u03a8w ( w(t) ) \u2212 \u03b7\u2206\u030c(t)w ) ,\nw(t+1) = argmin w\u2208W B\u03a8w\n( w \u2223 \u2223 \u2223 w\u0303(t+1) ) ,\n\u03b1\u0303(t+1) = \u2207\u03a8\u2217\u03b1 ( \u2207\u03a8\u03b1 ( \u03b1(t) ) + \u03b7\u2206\u0302(t)\u03b1 ) ,\n\u03b1(t+1) = argmin \u03b1\u2208A B\u03a8\u03b1\n( \u03b1 \u2223 \u2223 \u2223 \u03b1\u0303(t+1) ) ,\nwhere \u2206\u030c(t)w is a stochastic subgradient of f(w(t), \u03b1(t)) w.r.t. its first parameter, and \u2206\u0302 (t) \u03b1 a stochastic supergradient w.r.t. its second, with both \u2206\u030c(t)w and \u2206\u0302 (t) \u03b1 being Ft-measurable. We assume that, with probabilities 1\u2212\u03b4\u03c3w and 1\u2212\u03b4\u03c3\u03b1 (respectively), \u03c32w and \u03c3 2 \u03b1 satisfy the following uniformly for all t \u2208 {1, . . . , T }:\n\u2225 \u2225 \u2225 E [ \u2206\u030c(t)w \u2223 \u2223 \u2223 Ft\u22121 ] \u2212 \u2206\u030c(t)w \u2225 \u2225 \u2225\nw\u2217 \u2264 \u03c3w and\n\u2225 \u2225 \u2225 E [ \u2206\u0302(t)\u03b1 \u2223 \u2223 \u2223 Ft\u22121 ] \u2212 \u2206\u030c(t)w \u2225 \u2225 \u2225\n\u03b1\u2217 \u2264 \u03c3\u03b1.\nUnder these conditions, with probability 1\u2212 \u03b4\u03c3w \u2212 \u03b4\u03c3\u03b1 \u2212 2\u03b4, the above \u03c3w and \u03c3\u03b1 bounds will hold, and:\n1\nT\nT \u2211\nt=1\n( f ( w(t), \u03b1\u2217 ) \u2212 f ( w\u2217, \u03b1(t) ))\n\u2264B\u03a8w ( w\u2217 \u2223 \u2223 w(1) ) +B\u03a8\u03b1 ( \u03b1\u2217 \u2223 \u2223 \u03b1(1) )\n\u03b7T +\n\u03b7\n2T\nT \u2211\nt=1\n(\n\u2225 \u2225 \u2225\u2206\u030c(t)w \u2225 \u2225 \u2225 2 w\u2217 + \u2225 \u2225 \u2225\u2206\u0302(t)\u03b1 \u2225 \u2225 \u2225 2 \u03b1\u2217\n)\n+\n\u221a 2 (Rw\u2217\u03c3w +R\u03b1\u2217\u03c3\u03b1) \u221a\nln 1\u03b4\u221a T + 2 (Rw\u2217\u03c3w +R\u03b1\u2217\u03c3\u03b1) ln 1 \u03b4 3T ,\nwhere w\u2217 \u2208 W and \u03b1\u2217 \u2208 A are arbitrary reference vectors, and Rw\u2217 \u2265 \u2016w \u2212 w\u2217\u2016w and R\u03b1\u2217 \u2265 \u2016\u03b1\u2212 \u03b1\u2217\u2016\u03b1 bound the radii of W and A centered on w\u2217 and \u03b1\u2217, respectively.\nProof. This is a convex-concave saddle-point problem, which we will optimize by playing two convex optimization algorithms against each other, as in Nemirovski et al. [2009], Rakhlin and Sridharan [2013]. By Corollary 2, with probability 1\u2212 \u03b4\u03c3w \u2212 \u03b4 and 1\u2212 \u03b4\u03c3\u03b1 \u2212 \u03b4, respectively:\n1\nT\nT \u2211\nt=1\n( f ( w(t), \u03b1(t) ) \u2212 f ( w\u2217, \u03b1(t) ))\n\u2264B\u03a8w ( w\u2217 \u2223 \u2223 w(1) )\n\u03b7T +\n\u03b7\n2T\nT \u2211\nt=1\n\u2225 \u2225 \u2225\u2206\u030c(t)w \u2225 \u2225 \u2225 2\nw\u2217 +\n\u221a 2Rw\u2217\u03c3w \u221a\nln 1\u03b4\u221a T + 2Rw\u2217\u03c3w ln 1 \u03b4 3T ,\n1\nT\nT \u2211\nt=1\n( f ( w(t), \u03b1\u2217 ) \u2212 f ( w(t), \u03b1(t) ))\n\u2264B\u03a8\u03b1 ( \u03b1\u2217 \u2223 \u2223 \u03b1(1) )\n\u03b7T +\n\u03b7\n2T\nT \u2211\nt=1\n\u2225 \u2225 \u2225\u2206\u0302(t)\u03b1 \u2225 \u2225 \u2225 2\n\u03b1\u2217 +\n\u221a 2R\u03b1\u2217\u03c3\u03b1 \u221a\nln 1\u03b4\u221a T + 2R\u03b1\u2217\u03c3\u03b1 ln 1 \u03b4 3T .\nAdding these two inequalities gives the claimed result."}, {"heading": "B SGD for Strongly-Convex Functions", "text": "For \u03bb-strongly convex objective functions, we can achieve a faster convergence rate for SGD by using the step sizes \u03b7t = 1/\u03bbt. Our eventual algorithm (Algorithm 3) for strongly-convex heavily-constrained optimization will proceed in two phases, with the second phase \u201cpicking up\u201d where the first phase \u201cleft off\u201d, for which reason we present a convergence rate, based on Shalev-Shwartz et al. [2011, Lemma 2], that effectively starts at iteration T0 by using the step sizes \u03b7t = 1/\u03bb(T0 + t):\nTheorem 4. Take ft : W \u2192 R to be a sequence of \u03bb-strongly convex functions on which we perform T iterations of stochastic gradient descent starting from w(1) \u2208 W:\nw(t+1) = \u03a0w\n( w(t) \u2212 \u03b7t\u2207\u030cft ( w(t) )) ,\nwhere \u2207\u030cft ( w(t) ) \u2208 \u2202ft ( w(t) ) is a subgradient of ft at w(t), and \u2225 \u2225\u2207\u030cft ( w(t) )\u2225 \u2225 2 \u2264 G for all t. If we choose \u03b7t = 1\n\u03bb(T0+t) for some T0 \u2208 N, then:\n1\nT\nT \u2211\nt=1\n(\nft\n( w(t) ) \u2212 ft (w\u2217) ) \u2264 G 2 (1 + lnT )\n2\u03bbT + \u03bbT0 2T\n\u2225 \u2225 \u2225w(1) \u2212 w\u2217 \u2225 \u2225 \u2225 2\n2 ,\nwhere w\u2217 \u2208 W is an arbitrary reference vector and G \u2265 \u2225 \u2225\u2207\u030cft ( w(t) )\u2225 \u2225\n2 bounds the subgradient norms for all t.\nProof. This is nothing but a small tweak to Shalev-Shwartz et al. [2011, Lemma 2]. Starting from Equations 10 and 11 of that proof:\nT \u2211\nt=1\n(\nft\n( w(t) ) \u2212 ft (w\u2217) )\n\u2264G 2\n2\nT \u2211\nt=1\n\u03b7t + T \u2211\nt=1\n(\n1 2\u03b7t \u2212 \u03bb 2\n)\n\u2225 \u2225 \u2225 w(t) \u2212 w\u2217 \u2225 \u2225 \u2225 2\n2 \u2212\nT \u2211\nt=1\n1\n2\u03b7t\n\u2225 \u2225 \u2225 w(t+1) \u2212 w\u2217 \u2225 \u2225 \u2225 2\n2 .\nTaking \u03b7t = 1\u03bb(T0+t) :\nT \u2211\nt=1\n(\nft\n( w(t) ) \u2212 ft (w\u2217) )\n\u2264G 2\n2\u03bb\n(\n1\nT0 + 1 +\n\u222b T0+T\nt=T0+1\ndt\nt\n)\n+ \u03bbT0 2\n\u2225 \u2225 \u2225w(1) \u2212 w\u2217 \u2225 \u2225 \u2225 2 2 \u2212 \u03bb (T0 + T ) 2 \u2225 \u2225 \u2225w(T+1) \u2212 w\u2217 \u2225 \u2225 \u2225 2 2 .\nDividing through by T , simplifying and bounding yields the claimed result.\nAs we did Appendix A, we convert this into a result for stochastic subgradients:\nCorollary 4. Take ft : W \u2192 R to be a sequence of \u03bb-strongly convex functions, and F a filtration. Suppose that we perform T iterations of stochastic gradient descent starting from w(1) \u2208 W:\nw(t+1) = \u03a0w\n( w(t) \u2212 \u03b7t\u2206\u030c(t) ) ,\nwhere \u2206\u030c(t) is a stochastic subgradient of ft, i.e. E[\u2206\u030c(t) | Ft\u22121] \u2208 \u2202ft(w(t)), and \u2206\u030c(t) is Ft-measurable. If we choose \u03b7t =\n1 \u03bb(T0+t) for some T0 \u2208 N, then:\n1\nT\nT \u2211\nt=1\nE\n[\nft\n( w(t) ) \u2212 ft (w\u2217) ] \u2264 G 2 (1 + lnT )\n2\u03bbT + \u03bbT0 2T\n\u2225 \u2225 \u2225w(1) \u2212 w\u2217 \u2225 \u2225 \u2225 2\n2 ,\nwhere w\u2217 \u2208 W is an arbitrary reference vector and G \u2265 \u2225 \u2225\u2206\u030c(t) \u2225 \u2225\n2 bounds the stochastic subgradient norms for all t.\nProof. Same proof technique as Corollary 1, but based on Theorem 4 rather than Theorem 3.\nWe now use this result to prove an in-expectation saddle point bound:\nCorollary 5. Let \u2016\u00b7\u2016\u03b1 and \u2016\u00b7\u2016\u03b1\u2217 be a norm and its dual. Suppose that \u03a8\u03b1 is 1-strongly convex w.r.t. \u2016\u00b7\u2016\u03b1, and has convex conjugate \u03a8\u2217\u03b1 and associated Bregman divergence B\u03a8\u03b1 .\nTake f : W \u00d7A \u2192 R to be \u03bb-strongly convex in its first parameter and concave in its second, let F be a filtration, and suppose that we perform T iterations of SGD on w and MD on \u03b1:\nw(t+1) = \u03a0w\n(\nw(t) \u2212 1 \u03bb (T0 + t) \u2206\u030c(t)w\n)\n,\n\u03b1\u0303(t+1) = \u2207\u03a8\u2217\u03b1 ( \u2207\u03a8\u03b1 ( \u03b1(t) ) + \u03b7\u2206\u0302(t)\u03b1 ) ,\n\u03b1(t+1) = argmin \u03b1\u2208A B\u03a8\u03b1\n( \u03b1 \u2223 \u2223 \u2223 \u03b1\u0303(t+1) ) ,\nwhere \u2206\u030c(t)w is a stochastic subgradient of f(w(t), \u03b1(t)) w.r.t. its first parameter, and \u2206\u0302 (t) \u03b1 a stochastic supergradient w.r.t. its second, with both \u2206\u030c(t)w and \u2206\u0302 (t) \u03b1 being Ft-measurable. Then:\n1\nT\nT \u2211\nt=1\nE\n[ f ( w(t), \u03b1\u2217 ) \u2212 f ( w\u2217, \u03b1(t) )]\n\u2264G 2 w (1 + lnT )\n2\u03bbT + \u03bbT0 2T\n\u2225 \u2225 \u2225w(1) \u2212 w\u2217 \u2225 \u2225 \u2225 2\n2 +\nB\u03a8\u03b1 ( \u03b1\u2217 \u2223 \u2223 \u03b1(1) )\n\u03b7T +\n\u03b7\n2T\nT \u2211\nt=1\nE\n[\n\u2225 \u2225 \u2225\u2206\u0302(t)\u03b1 \u2225 \u2225 \u2225 2\n\u03b1\u2217\n]\n,\nwhere w\u2217 \u2208 W and \u03b1\u2217 \u2208 A are arbitrary reference vectors, and Gw \u2265 \u2225 \u2225 \u2225 \u2206\u030c (t) w \u2225 \u2225 \u2225\n2 bounds the stochastic subgradient\nnorms w.r.t. w for all t.\nProof. As we did in the proof of Corollary 3, we will play two convex optimization algorithms against each other. By Corollaries 4 and 1:\n1\nT\nT \u2211\nt=1\nE\n[ f ( w(t), \u03b1(t) ) \u2212 f ( w\u2217, \u03b1(t) )] \u2264G 2 w (1 + lnT )\n2\u03bbT + \u03bbT0 2T\n\u2225 \u2225 \u2225 w(1) \u2212 w\u2217 \u2225 \u2225 \u2225 2\n2 ,\n1\nT\nT \u2211\nt=1\nE\n[ f ( w(t), \u03b1\u2217 ) \u2212 f ( w(t), \u03b1(t) )] \u2264B\u03a8\u03b1 ( \u03b1\u2217 \u2223 \u2223 \u03b1(1) )\n\u03b7T +\n\u03b7\n2T\nT \u2211\nt=1\nE\n[\n\u2225 \u2225 \u2225\u2206\u0302(t)\u03b1 \u2225 \u2225 \u2225 2\n\u03b1\u2217\n]\n,\nAdding these two inequalities gives the claimed result."}, {"heading": "C Analyses of FullTouch and LightTouch", "text": "We begin by proving that, if \u03b3 is sufficiently large, then optimizing the relaxed objective, and projecting the resulting solution, will bring us close to the optimum of the constrained objective.\nLemma 1. In the setting of Section 2, suppose that f is Lf -Lipschitz, i.e. |f(w)\u2212 f(w\u2032)| \u2264 Lf \u2016w \u2212 w\u2032\u20162 for all w,w\u2032 \u2208 W , and that there is a constant \u03c1 > 0 such that if g(w) = 0 then \u2225 \u2225\u2207\u030c \u2225 \u2225\n2 \u2265 \u03c1 for all \u2207\u030c \u2208 \u2202g(w), where \u2202g(w)\nis the subdifferential of g(w).\nFor a parameter \u03b3 > 0, define: h (w) = f (w) + \u03b3max {0, g (w)} .\nIf \u03b3 > Lf/\u03c1, then for any infeasible w (i.e. for which g(w) > 0):\nh (w) > h (\u03a0g (w)) = f (\u03a0g (w)) and \u2016w \u2212\u03a0g (w)\u20162 \u2264 h (w)\u2212 h (\u03a0g (w))\n\u03b3\u03c1\u2212 Lf ,\nwhere \u03a0g (w) is the projection of w onto the set {w \u2208 W : g(w) \u2264 0} w.r.t. the Euclidean norm.\nProof. Let w \u2208 W be an arbitrary infeasible point. Because f is Lf -Lipschitz:\nf (w) \u2265 f (\u03a0g (w))\u2212 Lf \u2016w \u2212\u03a0g (w)\u20162 . (6)\nSince \u03a0g(w) is the projection of w onto the constraints w.r.t. the Euclidean norm, we must have by the first order optimality conditions that there exists a \u03bd \u2265 0 such that:\n0 \u2208 \u2202 \u2016w \u2212\u03a0g (w)\u201622 + \u03bd\u2202g (\u03a0g (w)) .\nThis implies that w \u2212 \u03a0g(w) is a scalar multiple of some \u2207\u030c \u2208 \u2202g(\u03a0g(w)). Because g is convex and \u03a0g (w) is on the boundary, g(w) \u2265 g(\u03a0g(w)) + \u2329 \u2207\u030c, w \u2212\u03a0g(w) \u232a = \u2329 \u2207\u030c, w \u2212\u03a0g(w) \u232a , so:\ng(w) \u2265 \u03c1 \u2016w \u2212\u03a0g(w)\u20162 . (7)\nCombining the definition of h with Equations 6 and 7 yields:\nh (w) \u2265 f (\u03a0g (w)) + (\u03b3\u03c1\u2212 Lf ) \u2016w \u2212\u03a0g(w)\u20162 .\nBoth claims follow immediately if \u03b3\u03c1 > Lf .\nC.1 Analysis of FullTouch\nWe\u2019ll now use Lemma 1 and Corollary 2 to bound the convergence rate of SGD on the function h of Lemma 1 (this is FullTouch). Like the algorithm itself, the convergence rate is little different from that found by Mahdavi et al. [2012] (aside from the bound on \u2016w\u0304 \u2212\u03a0g(w\u0304)\u20162), and is included here only for completeness. Lemma 3. Suppose that the conditions of Lemma 1 apply, with g(w) = maxi(gi(w)). Define Dw \u2265 supw,w\u2032\u2208W \u2016w \u2212 w\u2032\u20162 as the diameter of W , Gf \u2265 \u2225 \u2225\u2206\u030c(t) \u2225 \u2225 2 and Gg \u2265 \u2225 \u2225\u2207\u030cmax(0, gi(w)) \u2225 \u2225 2 as uniform upper bounds on the (stochastic) gradient magnitudes of f and the gis, respectively.\nIf we optimize Equation 1 using Algorithm 1 (FullTouch) with the step size:\n\u03b7 = Dw\n(Gf + \u03b3Gg) \u221a T ,\nthen with probability 1\u2212 \u03b4:\nf (\u03a0g (w\u0304))\u2212 f (w\u2217) \u2264 h (w\u0304)\u2212 h (w\u2217) \u2264 UF , and \u2016w\u0304 \u2212\u03a0g (w\u0304)\u20162 \u2264 UF\n\u03b3\u03c1\u2212 Lf ,\nwhere w\u2217 \u2208 {w \u2208 W : \u2200i.gi(w) \u2264 0} is an arbitrary constraint-satisfying reference vector, and:\nUF \u2264 ( 1 + 2 \u221a 2 ) Dw (Gf + \u03b3Gg) \u221a 1 + ln 1\n\u03b4\n\u221a\n1 T +\n8DwGf ln 1 \u03b4\n3T .\nProof. We choose \u03a8(w) = \u2016w\u201622 /2, for which the mirror descent update rule is precisely SGD. Because \u03a8w is (half of) the squared Euclidean norm, it is trivially 1-strongly convex w.r.t. the Euclidean norm, so \u2016\u00b7\u2016 = \u2016\u00b7\u2016\u2217 = \u2016\u00b7\u20162. Furthermore, B\u03a8(w\u2217 | w(1)) \u2264 D2w/2 and R\u2217 \u2264 Dw. We may upper bound the 2-norm of our stochastic gradients as \u2225 \u2225 \u2225\u2206\u030c (t) w \u2225 \u2225 \u2225\n2 \u2264 Gf + \u03b3Gg . Only the f -portion of the\nobjective is stochastic, so the error of the \u2206\u030c(t)w s can be trivially upper bounded, with probability 1, with \u03c3 = 2Gf .\nHence, by Corollary 2 (takingFt to be e.g. the smallest \u03c3-algebra making \u2206\u030c(t), . . . , \u2206\u030c(t) measurable), with probability 1\u2212 \u03b4:\n1\nT\nT \u2211\nt=1\n( h ( w(t) ) \u2212 h (w\u2217) ) \u2264 D 2 w\n2\u03b7T +\n\u03b7 (Gf + \u03b3Gg) 2\n2 +\n2 \u221a 2DwGf \u221a\nln 1\u03b4\u221a T + 8DwGf ln 1 \u03b4 3T .\nPlugging in the definition of \u03b7, moving the average defining w\u0304 inside h by Jensen\u2019s inequality, substituting f(w\u2217) = h(w\u2217) because w\u2217 satisfies the constraints, applying Lemma 1 and simplifying yields the claimed result.\nIn terms of the number of iterations required to achieve some desired level of suboptimality, this bound on UF may be expressed as:\nTheorem 5. Suppose that the conditions of Lemmas 1 and 3 apply, and that \u03b7 is as defined in Lemma 3.\nIf we optimize Equation 1 using T\u01eb iterations of Algorithm 1 (FullTouch):\nT\u01eb = O\n(\nD2w (Gf + \u03b3Gg) 2 ln 1\u03b4\n\u01eb2\n)\n,\nthen UF \u2264 \u01eb with probability 1 \u2212 \u03b4. where w\u2217 \u2208 {w \u2208 W : \u2200i.gi(w) \u2264 0} is an arbitrary constraint-satisfying reference vector.\nProof. Based on the bound of Lemma 3, define:\nx = \u221a T ,\nc = 8\n3 DwGf ln\n1 \u03b4 ,\nb = ( 1 + 2 \u221a 2 )\nDw (Gf + \u03b3Gg)\n\u221a\n1 + ln 1\n\u03b4 ,\na =\u2212 \u01eb,\nand consider the polynomial 0 = ax2 + bx+ c. Roots of this polynomial are xs for which UF = \u01eb, while for xs larger than any root we\u2019ll have that UF \u2264 \u01eb. Hence, we can bound the T required to ensure \u01eb-suboptimality by bounding the roots of this polynomial. By the Fujiwara bound [Wikipedia, 2015]:\nT\u01eb \u2264 max { 4 (\n9 + 4 \u221a 2 )\nD2w (Gf + \u03b3Gg) 2 ( 1 + ln 1\u03b4 )\n\u01eb2 , 16DwGf ln\n1 \u03b4\n3\u01eb\n}\n, (8)\ngiving the claimed result.\nC.2 Analysis of LightTouch\nBecause we use the reduced-variance algorithm of Johnson and Zhang [2013], and therefore update the remembered gradient \u00b5 one random coordinate at a time, we must first bound the maximum number of iterations over which a coordinate can go un-updated:\nLemma 4. Consider a process which maintains a sequence of vectors s(t) \u2208 Nm for t \u2208 {1, . . . , T }, where s(1) is initialized to zero and s(t+1) is derived from s(t) by independently sampling k = |St| \u2264 m random indices St \u2286 {1, . . . ,m} uniformly without replacement, and then setting s(t+1)j = t for j \u2208 St and s (t+1) j = s (t) j for j /\u2208 St. Then, with probability 1\u2212 \u03b4: max t,j ( t\u2212 s(t)j ) \u2264 1 + 2m k ln ( 2mT \u03b4 ) .\nProof. This is closely related to the \u201ccoupon collector\u2019s problem\u201d [Wikipedia, 2014]. We will begin by partitioning time into contiguous size-n chunks, with 1, . . . , n forming the first chunk, n+ 1, . . . , 2n the second, and so on.\nWithin each chunk the probability that any particular index was never sampled is ((m \u2212 k)/m)n, so by the union bound the probability that any one of the m indices was never sampled is bounded by m((m\u2212 k)/m)n:\nm\n(\nm\u2212 k m\n)n\n\u2264 exp ( lnm+ n ln ( m\u2212 k m )) \u2264 exp ( lnm\u2212 nk m ) .\nDefine n = \u2308(m/k) ln(2mT/\u03b4)\u2309, so:\nm\n(\nm\u2212 k m\n)n\n\u2264 exp ( lnm\u2212 ln ( 2mT\n\u03b4\n))\n\u2264 \u03b4 2T .\nThis shows that for this choice of n, the probability of there existing an index which is never sampled in some particular batch is bounded by \u03b4/2T . By the union bound, the probability of any of \u2308T/n\u2309 batches containing an index which is never sampled is bounded by (\u03b4/2T )\u2308T/n\u2309 \u2264 (\u03b4/2n) + (\u03b4/2T ) \u2264 \u03b4. If every index is sampled within every batch, then over the first n\u2308T/n\u2309 \u2265 T steps, the most steps which could elapse over which a particular index is not sampled is 2n\u2212 2 (if the index is sampled on the first step of one chunk, and the last step of the next chunk), which implies the claimed result.\nWe now combine this bound with Corollary 2 and make appropriate choices of the two d.g.f.s to yield a bound on the LightTouch convergence rate:\nLemma 5. Suppose that the conditions of Lemma 1 apply, with g(w) = maxi(gi(w)). Define Dw \u2265 supw,w\u2032\u2208W max{1, \u2016w \u2212 w\u2032\u20162} as a bound on the diameter of W (notice that we also choose Dw to be at least 1), Gf \u2265 \u2225 \u2225\u2206\u030c(t) \u2225 \u2225\n2 and Gg \u2265\n\u2225 \u2225\u2207\u030cmax(0, gi(w)) \u2225 \u2225\n2 as uniform upper bounds on the (stochastic) gradient\nmagnitudes of f and the gis, respectively, for all i \u2208 {1, . . . ,m}. We also assume that all gis are Lg-Lipschitz w.r.t. \u2016\u00b7\u20162, i.e. |gi(w) \u2212 gi(w\u2032)| \u2264 Lg \u2016w \u2212 w\u2032\u20162 for all w,w\u2032 \u2208 W . Define:\nk =\n\n  \nm (1 + lnm) 3/4\n\u221a 1 + ln 1\u03b4 \u221a 1 + lnT T 1/4   \n\n.\nIf k \u2264 m and we optimize Equation 1 using Algorithm 2 (LightTouch), basing the stochastic gradients w.r.t. p on k constraints at each iteration, and using the step size:\n\u03b7 =\n\u221a 1 + lnmDw\n(Gf + \u03b3Gg + \u03b3LgDw) \u221a T ,\nthen it holds with probability 1\u2212 \u03b4 that:\nf (\u03a0g (w\u0304))\u2212 f (w\u2217) \u2264 h (w\u0304)\u2212 h (w\u2217) \u2264 UL, and \u2016w\u0304 \u2212\u03a0g (w\u0304)\u20162 \u2264 UL\n\u03b3\u03c1\u2212 Lf ,\nwhere w\u2217 \u2208 {w \u2208 W : \u2200i.gi(w) \u2264 0} is an arbitrary constraint-satisfying reference vector, and:\nUL \u2264 67 \u221a 1 + lnmDw (Gf + \u03b3Gg + \u03b3LgDw)\n\u221a\n1 + ln 1\n\u03b4\n\u221a\n1 T .\nIf k > m, then we should fall-back to using FullTouch, in which case the result of Lemma 3 will apply.\nProof. We choose \u03a8w(w) = \u2016w\u201622 /2 and \u03a8p(p) = \u2211m\ni=1 pi ln pi to be the squared Euclidean norm divided by 2 and the negative Shannon entropy, respectively, which yields the updates of Algorithm 2. We assume that the \u2206\u030c(t)s are random variables on some probability space (depending on the source of the stochastic gradients of f ), and likewise the its and jts on another, so Ft may be taken to be the product of the smallest \u03c3-algebras which make \u2206\u030c(1), . . . , \u2206\u030c(t) and i1, j1, . . . , it, jt measurable, respectively, with conditional expectations being taken w.r.t. the product measure. Under the definitions of Corollary 3 (taking \u03b1 = p), with probability 1\u2212 \u03b4\u03c3w \u2212 \u03b4\u03c3p \u2212 2\u03b4\u2032:\n1\nT\nT \u2211\nt=1\nh\u0303 ( w(t), p\u2217 ) \u2212 1 T\nT \u2211\nt=1\nh\u0303 ( w\u2217, p(t) )\n\u2264B\u03a8w ( w\u2217 \u2223 \u2223 w(1) ) +B\u03a8p ( p\u2217 \u2223 \u2223 p(1) )\n\u03b7T +\n\u03b7\n2T\nT \u2211\nt=1\n(\n\u2225 \u2225 \u2225\u2206\u030c(t)w \u2225 \u2225 \u2225 2 w\u2217 + \u2225 \u2225 \u2225\u2206\u0302(t)p \u2225 \u2225 \u2225 2 p\u2217\n)\n+\n\u221a 2 (Rw\u2217\u03c3w +Rp\u2217\u03c3p) \u221a\nln 1\u03b4\u2032\u221a T + 4 (Rw\u2217\u03c3w +Rp\u2217\u03c3p) ln 1 \u03b4\u2032 3T .\nAs in the proof of Lemma 3, \u03a8w is 1-strongly convex w.r.t. the Euclidean norm, so \u2016\u00b7\u2016w = \u2016\u00b7\u2016w\u2217 = \u2016\u00b7\u20162, B\u03a8w(w\u2217 | w(1)) \u2264 D2w/2 and Rw\u2217 \u2264 Dw. Because \u03a8p is the negative entropy, which is 1-strongly convex w.r.t. the 1-norm (this is Pinsker\u2019s inequality), \u2016\u00b7\u2016p = \u2016\u00b7\u20161 and \u2016\u00b7\u2016p\u2217 = \u2016\u00b7\u2016\u221e, implying that Rp\u2217 = 1. Since p(1) is initialized to the uniform distribution, B\u03a8p(p \u2217 | p(1)) = DKL(p\u2217 | p(1)) \u2264 lnm.\nThe stochastic gradient definitions of Algorithm 2 give that \u2225 \u2225 \u2225\u2206\u030c (t) w \u2225 \u2225 \u2225\nw\u2217 \u2264 Gf + \u03b3Gg and \u03c3w \u2264 2(Gf + \u03b3Gg) with\nprobability 1 = 1 \u2212 \u03b4\u03c3w by the triangle inequality, and h\u0303(w\u2217, p(t)) = f(w\u2217) because w\u2217 satisfies the constraints. All of these facts together give that, with probability 1\u2212 \u03b4\u03c3p \u2212 \u03b4\u2032:\n1\nT\nT \u2211\nt=1\nh\u0303 ( w(t), p\u2217 ) \u2212 f (w\u2217)\n\u2264D 2 w + 2 lnm\n2\u03b7T +\n\u03b7\n2T\nT \u2211\nt=1\n(\n(Gf + \u03b3Gg) 2 + \u2225 \u2225 \u2225\u2206\u0302(t)p \u2225 \u2225 \u2225 2\n\u221e\n)\n+\n\u221a 2 (2Dw(Gf + \u03b3Gg) + \u03c3p) \u221a\nln 1\u03b4\u2032\u221a T + 4 (2Dw (Gf + \u03b3Gg) + \u03c3p) ln 1 \u03b4\u2032 3T .\nWe now move the average defining w\u0304 inside h\u0303 (which is convex in its first parameter) by Jensen\u2019s inequality, and use the fact that there exists a p\u2217 such that h\u0303(w, p\u2217) = h(w) to apply Lemma 1:\nUL \u2264 D2w + 2 lnm\n2\u03b7T +\n\u03b7\n2T\nT \u2211\nt=1\n(\n(Gf + \u03b3Gg) 2 + \u2225 \u2225 \u2225\u2206\u0302(t)p \u2225 \u2225 \u2225 2\n\u221e\n)\n(9)\n+\n\u221a 2 (2Dw(Gf + \u03b3Gg) + \u03c3p) \u221a\nln 1\u03b4\u2032\u221a T + 4 (2Dw (Gf + \u03b3Gg) + \u03c3p) ln 1 \u03b4\u2032 3T .\nBy the triangle inequality and the fact that (a+ b)2 \u2264 2a2 + 2b2: \u2225\n\u2225 \u2225\u2206\u0302(t)p\n\u2225 \u2225 \u2225 2\n\u221e \u2264 2\n\u2225 \u2225 \u2225E [ \u2206\u0302(t)p \u2223 \u2223 \u2223 Ft\u22121 ]\u2225 \u2225 \u2225 2\n\u221e + 2\n\u2225 \u2225 \u2225E [ \u2206\u0302(t)p \u2223 \u2223 \u2223 Ft\u22121 ] \u2212 \u2206\u0302(t)p \u2225 \u2225 \u2225 2\n\u221e\n\u2264 2\u03b32L2gD2w + 2 \u2225 \u2225 \u2225E [ \u2206\u0302(t)p \u2223 \u2223 \u2223 Ft\u22121 ] \u2212 \u2206\u0302(t)p \u2225 \u2225 \u2225 2 \u221e \u2264 2\u03b32L2gD2w + 2\u03c32p.\nSubstituting into Equation 9 and using the fact that a+ b \u2264 (\u221aa+ \u221a b)2:\nUL \u2264 D2w + 2 lnm\n2\u03b7T +\n\u03b7\n2\n( Gf + \u03b3Gg + \u221a 2\u03b3LgDw )2 + \u03b7\u03c32p (10)\n+\n\u221a 2 (2Dw(Gf + \u03b3Gg) + \u03c3p) \u221a\nln 1\u03b4\u2032\u221a T + 4 (2Dw (Gf + \u03b3Gg) + \u03c3p) ln 1 \u03b4\u2032 3T .\nWe will now turn our attention to the problem of bounding \u03c3p. Notice that because we sample i.i.d. jts uniformly at every iteration, they form an instance of the process of Lemma 4 with \u00b5(t)j = max(0, gj(w (s (t) j ))), showing that with probability 1\u2212 \u03b4\u03c3p: max t,j ( t\u2212 s(t)j ) \u2264 1 + 2m k ln ( 2mT\n\u03b4\u03c3p\n)\n. (11)\nBy the definition of \u2206\u0302(t)p (Algorithm 2):\n\u2225 \u2225 \u2225E [ \u2206\u0302(t)p \u2223 \u2223 \u2223 Ft\u22121 ] \u2212 \u2206\u0302(t)p \u2225 \u2225 \u2225 2\n\u221e\n=\u03b32\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225   m \u2211\nj=1\nej max { 0, gj ( w(t) )} \u2212 \u00b5(t) \n\u2212 m k \u2211\nj\u2208St\n( ej max { 0, gj ( w(t) )} \u2212 ej\u00b5(t)j )\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\n\u221e\n\u2264\u03b32 ( m\u2212 k k\n)2\nmax j\n( max {\n0, gj\n( w(t) )} \u2212 \u00b5(t)j )2\n\u2264\u03b32 ( m\u2212 k k\n)2\nL2g\n\u2225 \u2225\n\u2225w(t) \u2212 w(s (t) j )\n\u2225 \u2225 \u2225\n2\n2\n\u2264\u03b32 ( m\u2212 k k\n)2\nL2g\u03b7 2 (Gf + \u03b3Gg)\n2 ( t\u2212 s(t)j )2\n\u2264\u03b32 ( m\u2212 k k\n)2\nL2g\u03b7 2 (Gf + \u03b3Gg)\n2\n(\n1 + 2m\nk ln\n(\n2mT\n\u03b4\u03c3p\n))2\n\u22646\u03b32 (m\nk\n)4\nL2g\u03b7 2 (Gf + \u03b3Gg)\n2\n(\n1 + ln\n(\nmT\n\u03b4\u03c3p\n))2\nwhere in the second step we used the definition of the \u221e-norm, in the third we used the Lipschitz continuity of the gis (and hence of their positive parts), in the fourth we bounded the distance between two iterates with the number of iterations times a bound on the total step size, and in the fifth we used Equation 11. This shows that we may define:\n\u03c3p = \u221a 6\u03b3 (m\nk\n)2\nLg\u03b7 (Gf + \u03b3Gg)\n(\n1 + ln\n(\nmT\n\u03b4\u03c3p\n))\n,\nand it will satisfy the conditions of Corollary 3. Notice that, due to the \u03b7 factor, \u03c3p will be decreasing in T . Substituting the definitions of \u03b7 and \u03c3p into Equation 10, choosing \u03b4\u03c3p = \u03b4\u2032 = \u03b4/3 and using the assumption that Dw \u2265 1 gives\nthat with probability 1\u2212 \u03b4:\nUL \u2264 2 ( 1 + \u221a 2 )\u221a 1 + ln 3 \u221a 1 + lnmDw (Gf + \u03b3Gg + \u03b3LgDw) \u221a 1 + ln 1\n\u03b4\n(\n1\u221a T\n)\n+\n( 2 \u221a 3 + 8\n3\n)\n(1 + ln 3) 3/2\n(m\nk\n)2\n(1 + lnm) 3/2\nDw (Gf + \u03b3Gg)\n(\n1 + ln 1\n\u03b4\n)3/2 ( 1 + lnT\nT\n)\n+ 2\n(\n3 + 2\n\u221a\n2\n3\n)\n(1 + ln 3) 2 (m\nk\n)4\n(1 + lnm) 7/2\nDw (Gf + \u03b3Gg)\n(\n1 + ln 1\n\u03b4\n)2 (\n(1 + lnT ) 2\nT 3/2\n)\n.\nRounding up the constant terms:\nUL \u2264 7 \u221a 1 + lnmDw (Gf + \u03b3Gg + \u03b3LgDw)\n\u221a\n1 + ln 1\n\u03b4\n(\n1\u221a T\n)\n+ 19 (m\nk\n)2\n(1 + lnm) 3/2 Dw (Gf + \u03b3Gg)\n(\n1 + ln 1\n\u03b4\n)3/2 ( 1 + ln T\nT\n)\n+ 41 (m\nk\n)4\n(1 + lnm) 7/2\nDw (Gf + \u03b3Gg)\n(\n1 + ln 1\n\u03b4\n)2 (\n(1 + lnT ) 2\nT 3/2\n)\n.\nSubstituting the definition of k, simplifying and bounding yields the claimed result.\nIn terms of the number of iterations required to achieve some desired level of suboptimality, this bound on UL and the bound of Lemma 3 on UF may be combined to yield the following:\nTheorem 1. Suppose that the conditions of Lemmas 1 and 5 apply. Our result will be expressed in terms of a total iteration count T\u01eb satisfying:\nT\u01eb = O\n(\n(lnm)D2w (Gf + \u03b3Gg + \u03b3LgDw) 2 ln 1\u03b4\n\u01eb2\n)\n.\nDefine k in terms of T\u01eb as in Lemma 5. If k \u2264 m, then we optimize Equation 1 using T\u01eb iterations of Algorithm 2 (LightTouch) with \u03b7 as in Lemma 5. If k > m, then we use T\u01eb iterations of Algorithm 1 (FullTouch) with \u03b7 as in Lemma 3. In either case, we perform T\u01eb iterations, requiring a total of C\u01eb \u201cconstraint checks\u201d (evaluations or differentiations of a single gi):\nC\u01eb =O\u0303\n(\n(lnm)D2w (Gf + \u03b3Gg + \u03b3LgDw) 2 ln 1\u03b4\n\u01eb2\n+ m (lnm)\n3/2 D 3/2 w (Gf + \u03b3Gg + \u03b3LgDw) 3/2 ( ln 1\u03b4\n)5/4\n\u01eb3/2\n)\n.\nand with probability 1\u2212 \u03b4:\nf (\u03a0g (w\u0304))\u2212 f (w\u2217) \u2264 h (w\u0304)\u2212 h (w\u2217) \u2264 \u01eb and \u2016w\u0304 \u2212\u03a0g (w\u0304)\u20162 \u2264 \u01eb\n\u03b3\u03c1\u2212 Lf ,\nwhere w\u2217 \u2208 {w \u2208 W : \u2200i.gi(w) \u2264 0} is an arbitrary constraint-satisfying reference vector.\nProof. Regardless of the value of k, it follows from Lemmas 5 and 3 that:\nUL, UF \u2264 67 \u221a 1 + lnmDw (Gf + \u03b3Gg + \u03b3LgDw)\n\u221a\n1 + ln 1\n\u03b4\n\u221a\n1 T +\n8DwGf ln 1 \u03b4\n3T .\nAs in the proof of Theorem 5, we define:\nx = \u221a T ,\nc = 8\n3 DwGf ln\n1 \u03b4 ,\nb =67 \u221a 1 + lnmDw (Gf + \u03b3Gg + \u03b3LgDw)\n\u221a\n1 + ln 1\n\u03b4\n\u221a\n1 T ,\na =\u2212 \u01eb,\nand consider the polynomial 0 = ax2 + bx+ c. Any upper bound on all roots x = \u221a T of this polynomial will result in a lower-bound the values of T for which UL, UF \u2264 \u01eb with probability 1 \u2212 \u03b4. By the Fujiwara bound [Wikipedia, 2015]:\nT\u01eb = max\n{\n(134) 2 (1 + lnm)D2w (Gf + \u03b3Gg + \u03b3LgDw) 2 ( 1 + ln 1\u03b4 )\n\u01eb2 , 16DwGf ln\n1 \u03b4\n3\u01eb\n}\n,\ngiving the claimed bound on T\u01eb. For C\u01eb, we observe that we will perform no more than k + 1 constraint checks at each iteration (k + 1 by LightTouch if k \u2264 m, and m+ 1 by FullTouch if k > m), and substitute the above bound on T\u01eb into the definition of k, yielding:\n(k + 1)T\u01eb \u22642T\u01eb +m (1 + lnm) 3/4\n\u221a\n1 + ln 1\n\u03b4 T\n3/4 \u01eb\n\u221a\n1 + lnT\u01eb\n\u2264max { 2 (134) 2 (1 + lnm)D2w (Gf + \u03b3Gg + \u03b3LgDw) 2 ( 1 + ln 1\u03b4 )\n\u01eb2 , 32DwGf ln\n1 \u03b4\n3\u01eb\n}\n+max\n{\n(134) 3/2 m (1 + lnm) 3/2 D 3/2 w (Gf + \u03b3Gg + \u03b3LgDw) 3/2 ( 1 + ln 1\u03b4\n)5/4\n\u01eb3/2 ,\n(\n16\n3\n)3/4 m (1 + lnm) 3/4 D 3/4 w G 3/4 f ( 1 + ln 1\u03b4 )5/4\n\u01eb3/4\n\n\n\n\u221a\n1 + lnT\u01eb.\ngiving the claimed result (notice the \u221a 1 + lnT\u01eb factor on the RHS, for which reason we have a O\u0303 bound on C\u01eb, instead of O)."}, {"heading": "D Analysis of MidTouch", "text": "We now move on to the analysis of our LightTouch variant for \u03bb-strongly convex objectives, Algorithm 3 (MidTouch). While we were able to prove a high-probability bound for LightTouch, we were unable to do so for MidTouch, because the extra terms resulting from the use of a Bernstein-type martingale inequality were too large (since the other terms shrank as a result of the strong convexity assumption). Instead, we give an in-expectation result, and leave the proof of a corresponding high-probability bound to future work.\nOur first result is an analogue of Lemmas 3 and 5, and bounds the suboptimality achieved by MidTouch as a function of the iteration counts T1 and T2 of the two phases:\nLemma 6. Suppose that the conditions of Lemma 1 apply, with g(w) = maxi(gi(w)). Define Gf \u2265 \u2225 \u2225\u2206\u030c(t) \u2225 \u2225\n2\nand Gg \u2265 \u2225 \u2225\u2207\u030cmax(0, gi(w)) \u2225 \u2225\n2 as uniform upper bounds on the (stochastic) gradient magnitudes of f and the gis,\nrespectively, for all i \u2208 {1, . . . ,m}. We also assume that f is \u03bb-strongly convex, and that all gis are Lg-Lipschitz w.r.t. \u2016\u00b7\u20162, i.e. |gi(w) \u2212 gi(w\u2032)| \u2264 Lg \u2016w \u2212 w\u2032\u20162 for all w,w\u2032 \u2208 W .\nIf we optimize Equation 1 using Algorithm 3 (MidTouch) with the p-update step size \u03b7 = \u03bb/2\u03b32L2g, then:\nE\n[ \u2016\u03a0g(w\u0304)\u2212 w\u2217\u201622 ] \u2264 E [ \u2016w\u0304 \u2212 w\u2217\u201622 ]\n\u2264 2 (Gf + \u03b3Gg)\n2 (2 + lnT1 + lnT2) + 8\u03b3 2L2g lnm\n\u03bb2T2 +\n3m4 (1 + lnm) 2 (Gf + \u03b3Gg) 2\n\u03bb2T 21 ,\nwhere w\u2217 = argmin{w\u2208W:\u2200i.gi(w)\u22640} f(w) is the optimal constraint-satisfying reference vector.\nProof. As in the proof of Lemma 3, the first phase of Algorithm 3 is nothing but (strongly convex) SGD on the overall objective function h, so by Corollary 4:\n1\nT1\nT1 \u2211\nt=1\nE\n[ h ( w(t) ) \u2212 h (w\u2217) ] \u2264 G 2 w (1 + lnT1)\n2\u03bbT1 ,\nso by Jensen\u2019s inequality:\nE\n[ h ( w(T1+1) ) \u2212 h (w\u2217) ] \u2264 G 2 w (1 + lnT1)\n2\u03bbT1 . (12)\nFor the second phase, as in the proof of Lemma 5, we choose \u03a8p(p) = \u2211m\ni=1 pi ln pi to be negative Shannon entropy, which yields the second-phase updates of Algorithm 3. By Corollary 5:\n1\nT2\nT2 \u2211\nt=T1+1\nE\n[ h\u0303 ( w(t), p\u2217 ) \u2212 h\u0303 ( w\u2217, p(t) )]\n\u2264G 2 w (1 + lnT )\n2\u03bbT2 + \u03bbT1 2T2\n\u2225 \u2225 \u2225w(T1+1) \u2212 w\u2217 \u2225 \u2225 \u2225 2\n2 +\nB\u03a8p ( p\u2217 \u2223 \u2223 p(T1+1) )\n\u03b7T2 +\n\u03b7\n2T2\nT2 \u2211\nt=T1+1\nE\n[\n\u2225 \u2225 \u2225\u2206\u0302(t)p \u2225 \u2225 \u2225 2\np\u2217\n]\n.\nAs before, \u2016\u00b7\u2016p = \u2016\u00b7\u20161, \u2016\u00b7\u2016p\u2217 = \u2016\u00b7\u2016\u221e, and B\u03a8p(p\u2217 | p(T1+1)) = DKL(p\u2217 | p(T1+1)) \u2264 lnm. Hence:\n1\nT2\nT2 \u2211\nt=T1+1\nE\n[ h\u0303 ( w(t), p\u2217 ) \u2212 h\u0303 ( w\u2217, p(t) )]\n\u2264G 2 w (1 + lnT2)\n2\u03bbT2 + \u03bbT1 2T2\n\u2225 \u2225 \u2225w(T1+1) \u2212 w\u2217 \u2225 \u2225 \u2225 2\n2 +\nlnm \u03b7T2 + \u03b7 2T2\nT2 \u2211\nt=T1+1\nE\n[\n\u2225 \u2225 \u2225\u2206\u0302(t)p \u2225 \u2225 \u2225 2\n\u221e\n]\n.\nSince h is \u03bb-strongly convex and w\u2217 is optimal, \u2225 \u2225w(T1+1) \u2212 w\u2217 \u2225 \u2225\n2 2 \u2264 2\u03bb(h(w(T1+1))\u2212 h(w\u2217)). By Equation 12:\n1\nT2\nT2 \u2211\nt=T1+1\nE\n[ h\u0303 ( w(t), p\u2217 ) \u2212 h\u0303 ( w\u2217, p(t) )]\n\u2264G 2 w (2 + lnT1 + lnT2)\n2\u03bbT2 +\nlnm \u03b7T2 + \u03b7 2T2\nT2 \u2211\nt=T1+1\nE\n[\n\u2225 \u2225 \u2225\u2206\u0302(t)p \u2225 \u2225 \u2225 2\n\u221e\n]\n.\nSince the (uncentered) second moment is equal to the mean plus the variance, and using the fact that h\u0303(w\u2217, p(t)) = f(w\u2217) since all constraints are satisfied at w\u2217:\n1\nT2\nT2 \u2211\nt=T1+1\nE\n[ h\u0303 ( w(t), p\u2217 )] \u2212 f (w\u2217) (13)\n\u2264G 2 w (2 + lnT1 + lnT2)\n2\u03bbT2 +\nlnm \u03b7T2 + \u03b7 2T2\nT2 \u2211\nt=T1+1\n(\nE\n[\u2225\n\u2225 \u2225\u2206\u0302(t)p\n\u2225 \u2225 \u2225\n\u221e\n])2 + \u03b7\u03c32p 2 ,\nwhere \u03c32p is the variance of \u2225 \u2225 \u2225 \u2206\u0302 (t) p \u2225 \u2225 \u2225\n\u221e . Next observe that:\n(\nE\n[\u2225\n\u2225 \u2225\u2206\u0302(t)p\n\u2225 \u2225 \u2225\n\u221e\n])2\n=\n(\nE\n[\nmax j\u2208{1,...,m}\n\u03b3max {\n0, gj\n( w(t) )}\n])2\n\u2264\u03b32L2gE [ \u2225 \u2225 \u2225w(t) \u2212 w\u2217 \u2225 \u2225 \u2225 2\n2\n]\n\u2264 2\u03b32L2g\n\u03bb E\n[ h\u0303 ( w(t), p\u2217 ) \u2212 h\u0303 (w\u2217, p\u2217) ] ,\nthe first step using the fact that the gjs are Lg-Lipschitz and Jensen\u2019s inequality. For the second step, we choose p\u2217 such that w\u2217, p\u2217 is a minimax optimal pair (recall that w\u2217 is optimal by assumption), and use the \u03bb-strong convexity of h\u0303. Substituting into Equation 13 and using the fact that h\u0303(w\u2217, p\u2217) = f(w\u2217):\n(\n1\u2212 \u03b7\u03b32L2g\n\u03bb\n)(\n1\nT2\nT2 \u2211\nt=T1+1\nE\n[ h\u0303 ( w(t), p\u2217 )] \u2212 f (w\u2217) ) \u2264 G 2 w (2 + lnT1 + lnT2)\n2\u03bbT2 +\nlnm \u03b7T2 + \u03b7\u03c32p 2 .\nSubstituting \u03b7 = \u03bb/2\u03b32L2g and using Jensen\u2019s inequality:\nE\n[ h\u0303 (w\u0304, p\u2217) ] \u2212 f (w\u2217) \u2264 G 2 w (2 + lnT1 + lnT2)\n\u03bbT2 +\n4\u03b32L2g lnm\n\u03bbT2 + \u03bb\u03c32p 2\u03b32L2g . (14)\nWe now follow the proof of Lemma 5 and bound \u03c32p. By the definition of \u2206\u0302 (t) p (Algorithm 3):\n\u03c32p =E\n[\n\u2225 \u2225 \u2225 E [ \u2206\u0302(t)p \u2223 \u2223 \u2223 Ft\u22121 ] \u2212 \u2206\u0302(t)p \u2225 \u2225 \u2225 2\n\u221e\n]\n=\u03b32E\n\n \n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225   m \u2211\nj=1\nej max { 0, gj ( w(t) )} \u2212 \u00b5(t)  \u2212m ( ejt max { 0, gjt ( w(t) )} \u2212 ejt\u00b5 (t) jt )\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\n\u221e\n\n \n\u2264\u03b32 (m\u2212 1)2 E [\nmax j\n( max {\n0, gj\n( w(t) )} \u2212 \u00b5(t)j )2\n]\n.\nThe indices j are sampled uniformly, so the maximum time maxj(t\u2212 s(t)j ) since we last sampled the same index is an instance of the coupon collector\u2019s problem Wikipedia [2014]. Because the gjs are Lg-Lipschitz:\n\u03c32p \u2264\u03b32 (m\u2212 1)2 L2gE [\nmax j\n\u2225 \u2225\n\u2225w(t) \u2212 w(s (t) j )\n\u2225 \u2225 \u2225\n2\n2\n]\n\u2264 \u03b32 (m\u2212 1)2 L2gG2w\n\u03bb2T 21 E\n[\nmax j\n( t\u2212 s(t)j )2\n]\n\u2264 \u03b32m4\n(\n1 + (lnm) 2 + \u03c0 2 /6 )\nL2gG 2 w\n\u03bb2T 21\n\u2264 3\u03b32m4 (1 + lnm)2 L2gG 2 w\n\u03bb2T 21 ,\nthe second step because, between iteration s(t)j and iteration t we will perform t \u2212 s (t) j updates of magnitude at most Gw/\u03bbT1, and the third step because, as an instance of the coupon collector\u2019s problem, maxj(t\u2212 s(t)j ) has expectation mHm \u2264 m+m lnm (Hm is the mth harmonic number) and variance m2\u03c02/6. Substituting into Equation 14:\nE\n[ h\u0303 (w\u0304, p\u2217) ] \u2212 f (w\u2217) \u2264 G 2 w (2 + lnT1 + lnT2)\n\u03bbT2 +\n4\u03b32L2g lnm\n\u03bbT2 +\n3m4 (1 + lnm) 2 G2w\n2\u03bbT 21 .\nBy the \u03bb-strong convexity of h\u0303:\nE\n[ \u2016w\u0304 \u2212 w\u2217\u201622 ] \u2264 2G 2 w (2 + lnT1 + lnT2)\n\u03bb2T2 +\n8\u03b32L2g lnm\n\u03bb2T2 +\n3m4 (1 + lnm) 2 G2w\n\u03bb2T 21 .\nUsing the facts that \u2016\u03a0g(w\u0304)\u2212 w\u2217\u2016 \u2264 \u2016w\u0304 \u2212 w\u2217\u2016 because w\u2217 is feasible, and that Gw = Gf + \u03b3Gg , completes the proof.\nWe now move on to the main result: a bound on the number of iterations (equivalently, the number of stochastic loss gradients) and constraint checks required to achieve \u01eb-suboptimality:\nTheorem 2. Suppose that the conditions of Lemmas 1 and 6 apply, with the p-update step size \u03b7 as defined in Lemma 6. If we run Algorithm 3 (MidTouch) for T\u01eb1 iterations in the first phase and T\u01eb2 in the second:\nT\u01eb1 =O\u0303\n(\nm (lnm) 2/3 (Gf + \u03b3Gg + \u03b3Lg) 4/3\n\u03bb4/3\u01eb2/3 +\nm2 (lnm) (Gf + \u03b3Gg)\n\u03bb \u221a \u01eb\n)\n,\nT\u01eb2 =O\u0303\n(\n(lnm) (Gf + \u03b3Gg + \u03b3Lg) 2\n\u03bb2\u01eb +\nm3/2 (lnm) 3/2 (Gf + \u03b3Gg) 3/2\n\u03bb3/2\u01eb3/4\n)\n,\nrequiring a total of C\u01eb \u201cconstraint checks\u201d (evaluations or differentiations of a single gi):\nC\u01eb =O\u0303\n(\n(lnm) (Gf + \u03b3Gg + \u03b3Lg) 2\n\u03bb2\u01eb +\nm3/2 (lnm) 3/2 (Gf + \u03b3Gg) 3/2\n\u03bb3/2\u01eb3/4\n+ m2 (lnm) 2/3 (Gf + \u03b3Gg + \u03b3Lg) 4/3\n\u03bb4/3\u01eb2/3 +\nm3 (lnm) (Gf + \u03b3Gg)\n\u03bb \u221a \u01eb\n)\n,\nthen: E [\n\u2016\u03a0g(w\u0304)\u2212 w\u2217\u201622 ] \u2264 E [ \u2016w\u0304 \u2212 w\u2217\u201622 ] \u2264 \u01eb,\nwhere w\u2217 = argmin{w\u2208W:\u2200i.gi(w)\u22640} f(w) is the optimal constraint-satisfying reference vector.\nProof. We begin by introducing a number \u03c4 \u2208 R with \u03c4 \u2265 1 that will be used to define the iteration counts T1 and T2 as:\nT1 = \u2308 m\u03c42 \u2309\nand T2 = \u2308 \u03c43 \u2309 .\nBy Lemma 6, the above definitions imply that:\nE\n[ \u2016\u03a0g(w\u0304)\u2212 w\u2217\u201622 ]\n\u2264 2 (Gf + \u03b3Gg)\n2 (4 + lnm+ 5 ln \u03c4) + 8\u03b32L2g lnm\n\u03bb2\u03c43 +\n3m4 (1 + lnm) 2 (Gf + \u03b3Gg) 2\n\u03bb2m2\u03c44\n\u2264 10 (1 + lnm) (Gf + \u03b3Gg + \u03b3Lg) 2 (1 + ln \u03c4)\n\u03bb2\u03c43 +\n3m2 (1 + lnm) 2 (Gf + \u03b3Gg) 2\n\u03bb2\u03c44 .\nDefining \u01eb = E [ \u2016\u03a0g(w\u0304)\u2212 w\u2217\u201622 ] and rearranging:\n\u03bb2\u01eb\n(\n\u03c4\n(1 + ln \u03c4) 1/3\n)4\n\u2264 10 (1 + lnm) (Gf + \u03b3Gg + \u03b3Lg)2 (\n\u03c4\n(1 + ln \u03c4) 1/3\n)\n+ 3m2 (1 + lnm) 2 (Gf + \u03b3Gg) 2 .\nWe will now upper-bound all roots of the above equation with a quantity \u03c4\u01eb, for which all \u03c4 \u2265 \u03c4\u01eb will result in \u01eb-suboptimality. By the Fujiwara bound [Wikipedia, 2015], and including the constraint that \u03c4 \u2265 1:\n\u03c4\u01eb\n(1 + ln \u03c4\u01eb) 1/3\n\u2264max\n\n\n\n1, 2\n(\n10 (1 + lnm) (Gf + \u03b3Gg + \u03b3Lg) 2\n\u03bb2\u01eb\n)1/3\n,\n2\n(\n3m2 (1 + lnm) 2 (Gf + \u03b3Gg) 2\n2\u03bb2\u01eb\n)1/4 \n\n\n.\nSubstituting the above bound on \u03c4\u01eb into the definitions of T1 and T2 gives the claimed magnitudes of these T\u01eb1 and T\u01eb2, and using the fact that the C\u01eb = O(mT\u01eb1 + T\u01eb2) gives the claimed bound on C\u01eb."}], "references": [{"title": "Application of the back propagation neural network algorithm with monotonicity constraints for two-group classification problems", "author": ["N.P. Archer", "S. Wang"], "venue": "Decision Sciences,", "citeRegEx": "Archer and Wang.,? \\Q1993\\E", "shortCiteRegEx": "Archer and Wang.", "year": 1993}, {"title": "Projection Algorithms and Monotone Operators", "author": ["H.H. Bauschke"], "venue": "Ph.D. Thesis, Simon Fraser University,", "citeRegEx": "Bauschke.,? \\Q1996\\E", "shortCiteRegEx": "Bauschke.", "year": 1996}, {"title": "Mirror descent and nonlinear projected subgradient methods for convex optimization", "author": ["A. Beck", "M. Teboulle"], "venue": "Oper. Res. Lett.,", "citeRegEx": "Beck and Teboulle.,? \\Q2003\\E", "shortCiteRegEx": "Beck and Teboulle.", "year": 2003}, {"title": "Sublinear optimization for machine learning", "author": ["K.L. Clarkson", "E. Hazan", "D.P. Woodruff"], "venue": "In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Clarkson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Clarkson et al\\.", "year": 2010}, {"title": "Monotone and partially monotone neural networks", "author": ["H. Daniels", "M. Velikova"], "venue": "IEEE Trans. Neural Networks,", "citeRegEx": "Daniels and Velikova.,? \\Q2010\\E", "shortCiteRegEx": "Daniels and Velikova.", "year": 2010}, {"title": "On Bernstein-type inequalities for martingales", "author": ["K. Dzhaparidze", "J.H. van Zanten"], "venue": "Stochastic Processes and their Applications,", "citeRegEx": "Dzhaparidze and Zanten.,? \\Q2001\\E", "shortCiteRegEx": "Dzhaparidze and Zanten.", "year": 2001}, {"title": "Playing non-linear games with linear oracles. In FOCS, pages 420\u2013428", "author": ["D. Garber", "E. Hazan"], "venue": "IEEE Computer Society,", "citeRegEx": "Garber and Hazan.,? \\Q2013\\E", "shortCiteRegEx": "Garber and Hazan.", "year": 2013}, {"title": "Optimized regression for efficient function evaluation", "author": ["E.K. Garcia", "R. Arora", "M. Gupta"], "venue": "IEEE Trans. Image Processing,", "citeRegEx": "Garcia et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Garcia et al\\.", "year": 2012}, {"title": "Monotonic calibrated interpolated look-up", "author": ["M.R. Gupta", "A. Cotter", "J. Pfeifer", "K. Voevodski", "K. Canini", "A. Mangylov", "W. Moczydlowski", "A. van Esbroeck"], "venue": "tables. JMLR,", "citeRegEx": "Gupta et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2016}, {"title": "Projection-free online learning", "author": ["E. Hazan", "S. Kale"], "venue": "In ICML\u201912,", "citeRegEx": "Hazan and Kale.,? \\Q2012\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2012}, {"title": "Revisiting Frank-Wolfe: Projection-free sparse convex optimization", "author": ["M. Jaggi"], "venue": "In ICML\u201913,", "citeRegEx": "Jaggi.,? \\Q2013\\E", "shortCiteRegEx": "Jaggi.", "year": 2013}, {"title": "Optimizing search engines using clickthrough data", "author": ["T. Joachims"], "venue": "In KDD\u201902,", "citeRegEx": "Joachims.,? \\Q2002\\E", "shortCiteRegEx": "Joachims.", "year": 2002}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "In NIPS\u201913,", "citeRegEx": "Johnson and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Johnson and Zhang.", "year": 2013}, {"title": "Stochastic gradient descent with only one projection", "author": ["M. Mahdavi", "T. Yang", "R. Jin", "S. Zhu", "J. Yi"], "venue": "In NIPS\u201912,", "citeRegEx": "Mahdavi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mahdavi et al\\.", "year": 2012}, {"title": "Lecture notes: Interior point polynomial time methods in convex programming", "author": ["A. Nemirovski"], "venue": null, "citeRegEx": "Nemirovski.,? \\Q2004\\E", "shortCiteRegEx": "Nemirovski.", "year": 2004}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nemirovski et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nemirovski et al\\.", "year": 2009}, {"title": "Generalized constraint neural network regression model subject to linear priors", "author": ["Y. Qu", "B. Hu"], "venue": "IEEE Trans. on Neural Networks,", "citeRegEx": "Qu and Hu.,? \\Q2011\\E", "shortCiteRegEx": "Qu and Hu.", "year": 2011}, {"title": "Optimization, learning, and games with predictable sequences", "author": ["A. Rakhlin", "K. Sridharan"], "venue": "In NIPS\u201913,", "citeRegEx": "Rakhlin and Sridharan.,? \\Q2013\\E", "shortCiteRegEx": "Rakhlin and Sridharan.", "year": 2013}, {"title": "Pegasos: Primal Estimated sub-GrAdient SOlver for SVM", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical Programming,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Least squares isotonic regression in two dimensions", "author": ["J. Spouge", "H. Wan", "W.J. Wilbur"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Spouge et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Spouge et al\\.", "year": 2003}, {"title": "On the universality of online mirror descent", "author": ["N. Srebro", "K. Sridharan", "A. Tewari"], "venue": "In NIPS\u201911,", "citeRegEx": "Srebro et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2011}, {"title": "On the implementation of a primal-dual interior point filter line search algorithm for large-scale nonlinear programming", "author": ["A. W\u00e4chter", "L.T. Biegler"], "venue": "Mathematical Programming,", "citeRegEx": "W\u00e4chter and Biegler.,? \\Q2006\\E", "shortCiteRegEx": "W\u00e4chter and Biegler.", "year": 2006}, {"title": "Random Multi-Constraint Projection: Stochastic Gradient Methods for Convex Optimization with Many Constraints", "author": ["M. Wang", "Y. Chen", "J. Liu", "Y. Gu"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In ICML\u201903,", "citeRegEx": "Zinkevich.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich.", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Archer and Wang, 1993, Sill, 1998, Spouge et al., 2003, Daniels and Velikova, 2010, Gupta et al., 2016]. Submodular functions can often be learned from noisy examples by imposing constraints to ensure submodularity holds. Another example occurs when one wishes to guarantee that a classifier will correctly label certain \u201ccanonical\u201d examples, which can be enforced by constraining the function values on those examples. See Qu and Hu [2011] for some other examples of constraints useful in machine learning.", "startOffset": 0, "endOffset": 441}, {"referenceID": 0, "context": "Archer and Wang, 1993, Sill, 1998, Spouge et al., 2003, Daniels and Velikova, 2010, Gupta et al., 2016]. Submodular functions can often be learned from noisy examples by imposing constraints to ensure submodularity holds. Another example occurs when one wishes to guarantee that a classifier will correctly label certain \u201ccanonical\u201d examples, which can be enforced by constraining the function values on those examples. See Qu and Hu [2011] for some other examples of constraints useful in machine learning. However, these practical uses of constraints in machine learning are impractical in that the number of constraints may be very large, and scale poorly with the number of features d or number of training samples n. In this paper we propose a new strategy for tackling such heavily-constrained problems, with guarantees and compelling convergence rates for large-scale convex problems. A standard approach for large-scale empirical risk minimization is projected stochastic gradient descent [e.g. Zinkevich, 2003, Nemirovski et al., 2009]. Each SGD iteration is computationally cheap, and the algorithm converges quickly to a solution good enough for machine learning needs. However, this algorithm requires a projection onto the feasible region after each stochastic gradient step, which can be prohibitively slow if there are many non-trivial constraints, and is not easy to parallelize. Recently, Frank-Wolfe-style algorithms [e.g. Hazan and Kale, 2012, Jaggi, 2013] have been proposed that remove the projection, but require a constrained linear optimization at each iteration. We propose a new strategy for large-scale constrained optimization that, like Mahdavi et al. [2012], moves the constraints into the objective and finds an approximate solution of the resulting unconstrained problem, projecting the (potentially-infeasible) result onto the constraints only once, at the end.", "startOffset": 0, "endOffset": 1688}, {"referenceID": 13, "context": "Despite this, LightTouch was roughly as fast as the Mahdavi et al. [2012]-like algorithm FullTouch.", "startOffset": 52, "endOffset": 74}, {"referenceID": 12, "context": "p \u03bc Remembered gradient coordinates [Johnson and Zhang, 2013] k Minibatch size in LightTouch\u2019s p-update w\u0304 Average iterate w\u0304 = ( \u2211T t=1 w )/T consider constraints written in terms of arbitrary convex functions, and are not restricted to e.", "startOffset": 36, "endOffset": 61}, {"referenceID": 12, "context": "p \u03bc Remembered gradient coordinates [Johnson and Zhang, 2013] k Minibatch size in LightTouch\u2019s p-update w\u0304 Average iterate w\u0304 = ( \u2211T t=1 w )/T consider constraints written in terms of arbitrary convex functions, and are not restricted to e.g. only linear or quadratic constraints. 2.1 FullTouch: A Relaxation with a Feasible Minimizer We build on the approach of Mahdavi et al. [2012] to relax Equation 1.", "startOffset": 37, "endOffset": 385}, {"referenceID": 13, "context": "This algorithm\u2014our starting point\u2014is similar to those proposed by Mahdavi et al. [2012], and like their algorithms only contains a single projection, at the end, projecting the potentially-infeasible result vector w\u0304.", "startOffset": 66, "endOffset": 88}, {"referenceID": 13, "context": "This algorithm\u2014our starting point\u2014is similar to those proposed by Mahdavi et al. [2012], and like their algorithms only contains a single projection, at the end, projecting the potentially-infeasible result vector w\u0304. Hyperparameters: T , \u03b7 1 Initialize w \u2208 W arbitrarily 2 For t = 1 to T : 3 Sample \u2206\u030c // stochastic subgradient of f(w) 4 Let \u2206\u030c w = \u2206\u030c + \u03b3\u2207\u030cmax{0, g(w(t))} 5 Update w = \u03a0w(w \u2212 \u03b7\u2206\u030c w ) // \u03a0w projects its argument onto W w.r.t. \u2016\u00b7\u20162 6 Average w\u0304 = ( \u2211T t=1 w )/T 7 Return \u03a0g(w\u0304) // optional if small constraint violations are acceptable If \u03b3 > Lf/\u03c1, then for any infeasible w (i.e. for which g(w) > 0): h (w) > h (\u03a0g (w)) = f (\u03a0g (w)) and \u2016w \u2212\u03a0g (w)\u20162 \u2264 h (w)\u2212 h (\u03a0g (w)) \u03b3\u03c1\u2212 Lf , where \u03a0g (w) is the projection of w onto the set {w \u2208 W : g(w) \u2264 0} w.r.t. the Euclidean norm. Proof. In Appendix C. The strategy of applying SGD to h(w), detailed in Algorithm 1, which we call FullTouch, has the same \u201cflavor\u201d as the algorithms proposed by Mahdavi et al. [2012], and we use it as a baseline comparison point for our other algorithms.", "startOffset": 66, "endOffset": 976}, {"referenceID": 1, "context": "This requirement is related to the linear regularity assumption introduced by Bauschke [1996], and considered recently by Wang et al.", "startOffset": 78, "endOffset": 94}, {"referenceID": 1, "context": "This requirement is related to the linear regularity assumption introduced by Bauschke [1996], and considered recently by Wang et al. [2015]. 3 A Light Touch This section presents the main contribution of this paper: an algorithm that stochastically samples a small subset of the m constraints at each SGD iteration, updates the parameters based on the subgradients of the sampled constraints, and carefully learns the distribution over the constraints to produce a net performance gain.", "startOffset": 78, "endOffset": 141}, {"referenceID": 1, "context": "This requirement is related to the linear regularity assumption introduced by Bauschke [1996], and considered recently by Wang et al. [2015]. 3 A Light Touch This section presents the main contribution of this paper: an algorithm that stochastically samples a small subset of the m constraints at each SGD iteration, updates the parameters based on the subgradients of the sampled constraints, and carefully learns the distribution over the constraints to produce a net performance gain. We first motivate the approach by considering an oracle, then explain the algorithm and present convergence results for the convex (Section 3.2) and strongly convex (Section 3.3) cases. 3.1 Wanted: An Oracle For the Most Violated Constraint Because FullTouch only needs to differentiate the most violated constraint at each iteration, it follows that if one had access to an oracle that identified the most-violated constraint, then the overall convergence rate (including the cost of each iteration) could only depend on m through \u03b3. This motivates us to learn to predict the most-violated constraint, ideally at a significantly better than linear-in-m rate. To this end, we further relax the problem of minimizing h(w) (defined in Lemma 1) by replacing \u03b3max(0, g(w)) with maximization over a probability distribution (as in Clarkson et al. [2010]), yielding the equivalent convex-linear 5", "startOffset": 78, "endOffset": 1337}, {"referenceID": 12, "context": "For this reason, in addition to minibatching, we center the stochastic gradients, as is done by the well-known SVRG algorithm [Johnson and Zhang, 2013], by storing a gradient estimate \u03b3\u03bc with \u03bc \u2208 R, at each iteration sampling a set S of size |S| = k uniformly without replacement, and computing: \u2206\u0302p = \u03b3\u03bc+ \u03b3m k \u2211", "startOffset": 126, "endOffset": 151}, {"referenceID": 6, "context": "4 Theoretical Comparison Table 2 compares upper bounds on the convergence rates and per-iteration costs when applied to a convex (but not necessarily strongly convex) problem for LightTouch, FullTouch, projected SGD, the online Frank-Wolfe algorithm of Hazan and Kale [2012], and a Frank-Wolfe-like online algorithm for optimization over a polytope [Garber and Hazan, 2013].", "startOffset": 349, "endOffset": 373}, {"referenceID": 6, "context": "3], and Garber and Hazan [2013, Theorem 2]. Notice that because this table compares upper bounds to upper bounds, subsequent work may improve these bounds further. #Iterations to achieve #Constraint checks to achieve \u01eb-suboptimality \u01eb-suboptimality FullTouch \u03b3 D w \u01eb2 m\u03b3D w \u01eb2 LightTouch (lnm)\u03b3 D w \u01eb2 (lnm)\u03b3D w \u01eb2 + m(lnm) /2\u03b3 /2D3 w \u01eb/2 Projected SGD D 2 w \u01eb2 N/A (projection) Online Frank-Wolfe D 3 w \u01eb3 N/A (linear optimization) LLO-FW d\u03bd D w \u01eb2 N/A (local linear oracle) then: E [ \u2016\u03a0g(w\u0304)\u2212 w\u20162 ] \u2264 E [ \u2016w\u0304 \u2212 w\u20162 ] \u2264 \u01eb, where w\u2217 = argmin{w\u2208W:\u2200i.gi(w)\u22640} f(w) is the optimal constraint-satisfying reference vector. Proof. In Appendix D. Notice that the above theorem bounds not the suboptimality of \u03a0g(w\u0304), but rather its squared Euclidean distance from w\u2217, for which reason the denominator of the highest order term depends on \u03bb rather than \u03bb. Like Theorem 1 in the non-strongly convex case, the dominant terms above, both in terms of the total number of iterations and number of constraint checks, match the usual 1/\u01eb convergence rate for unconstrained strongly-convex SGD with an additional \u03b3 lnm factor, while the lower-order terms have a worse m-dependence. As before, fewer constraint checks will be performed per iteration as \u01eb shrinks, reaching a constant number (on average) once \u01eb is on the order of 1/m. 4 Theoretical Comparison Table 2 compares upper bounds on the convergence rates and per-iteration costs when applied to a convex (but not necessarily strongly convex) problem for LightTouch, FullTouch, projected SGD, the online Frank-Wolfe algorithm of Hazan and Kale [2012], and a Frank-Wolfe-like online algorithm for optimization over a polytope [Garber and Hazan, 2013].", "startOffset": 8, "endOffset": 1593}, {"referenceID": 11, "context": "6 Experiments We validated the performance of our practical variant of LightTouch (Algorithm 4) on a YouTube ranking problem in the style of Joachims [2002], in which the task is to predict what a user will watch next, given that they have just viewed a certain video.", "startOffset": 141, "endOffset": 157}, {"referenceID": 21, "context": "ProjectedSGD We implemented Euclidean projections onto lattice monotonicity constraints using IPOPT [W\u00e4chter and Biegler, 2006] to optimize the resulting sparse 4096-dimensional quadratic program.", "startOffset": 100, "endOffset": 127}, {"referenceID": 7, "context": "ApproxSGD This is an approximate projected SGD implementation using the fast approximate update procedure described in Gupta et al. [2016], which is an active set method that, starting from the current iterate, moves along the boundary of the feasible region, adding constraints to the active set as they are encountered, until the desired step is exhausted (this is reminiscent of the local linear oracles considered by Garber and Hazan [2013]).", "startOffset": 119, "endOffset": 139}, {"referenceID": 6, "context": "[2016], which is an active set method that, starting from the current iterate, moves along the boundary of the feasible region, adding constraints to the active set as they are encountered, until the desired step is exhausted (this is reminiscent of the local linear oracles considered by Garber and Hazan [2013]).", "startOffset": 289, "endOffset": 313}, {"referenceID": 13, "context": "7 Conclusions We have proposed an efficient strategy for large-scale heavily constrained optimization, building on the work of Mahdavi et al. [2012], and analyze its performance, demonstrating that, asymptotically, our approach requires many fewer constraint checks in order to converge.", "startOffset": 127, "endOffset": 149}, {"referenceID": 2, "context": "\u2225 \u03b1\u2217 1\u2212 \u03b4\u03c3 Probability that \u03c3 bound holds 1\u2212 \u03b4\u03c3w Probability that \u03c3w bound holds 1\u2212 \u03b4\u03c3\u03b1 Probability that \u03c3\u03b1 bound holds A Mirror Descent Mirror descent [Nemirovski and Yudin, 1983, Beck and Teboulle, 2003] is a meta-algorithm for stochastic optimization (more generally, online regret minimization) which performs gradient updates with respect to a meta-parameter, the distance generating function (d.g.f.). The two most widely-used d.g.f.s are the squared Euclidean norm and negative Shannon entropy, for which the resulting MD instantiations are stochastic gradient descent (SGD) and a multiplicative updating algorithm, respectively. These are precisely the two d.g.f.s which our constrained algorithm will use for the updates of w and p. We\u2019ll here give a number of results which differ only slightly from \u201cstandard\u201d ones, beginning with a statement of an online MD bound adapted from Srebro et al. [2011]: Theorem 3.", "startOffset": 181, "endOffset": 910}, {"referenceID": 14, "context": "The convergence rate will be determined from a saddle-point bound, which we derive from Corollary 2 by following Nemirovski et al. [2009], Rakhlin and Sridharan [2013], and simply applying it twice: Corollary 3.", "startOffset": 113, "endOffset": 138}, {"referenceID": 14, "context": "The convergence rate will be determined from a saddle-point bound, which we derive from Corollary 2 by following Nemirovski et al. [2009], Rakhlin and Sridharan [2013], and simply applying it twice: Corollary 3.", "startOffset": 113, "endOffset": 168}, {"referenceID": 14, "context": "This is a convex-concave saddle-point problem, which we will optimize by playing two convex optimization algorithms against each other, as in Nemirovski et al. [2009], Rakhlin and Sridharan [2013].", "startOffset": 142, "endOffset": 167}, {"referenceID": 14, "context": "This is a convex-concave saddle-point problem, which we will optimize by playing two convex optimization algorithms against each other, as in Nemirovski et al. [2009], Rakhlin and Sridharan [2013]. By Corollary 2, with probability 1\u2212 \u03b4\u03c3w \u2212 \u03b4 and 1\u2212 \u03b4\u03c3\u03b1 \u2212 \u03b4, respectively: 1 T T \u2211", "startOffset": 142, "endOffset": 197}, {"referenceID": 13, "context": "Like the algorithm itself, the convergence rate is little different from that found by Mahdavi et al. [2012] (aside from the bound on \u2016w\u0304 \u2212\u03a0g(w\u0304)\u20162), and is included here only for completeness.", "startOffset": 87, "endOffset": 109}, {"referenceID": 12, "context": "2 Analysis of LightTouch Because we use the reduced-variance algorithm of Johnson and Zhang [2013], and therefore update the remembered gradient \u03bc one random coordinate at a time, we must first bound the maximum number of iterations over which a coordinate can go un-updated: Lemma 4.", "startOffset": 74, "endOffset": 99}], "year": 2016, "abstractText": "Minimizing empirical risk subject to a set of constraints can be a useful strategy for learning restricted classes of functions, such as monotonic functions, submodular functions, classifiers that guarantee a certain class label for some subset of examples, etc. However, these restrictions may result in a very large number of constraints. Projected stochastic gradient descent (SGD) is often the default choice for large-scale optimization in machine learning, but requires a projection after each update. For heavily-constrained objectives, we propose an efficient extension of SGD that stays close to the feasible region while only applying constraints probabilistically at each iteration. Theoretical analysis shows a compelling trade-off between per-iteration work and the number of iterations needed on problems with a large number of constraints.", "creator": "LaTeX with hyperref package"}}}