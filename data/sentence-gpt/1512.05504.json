{"id": "1512.05504", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2015", "title": "Blind, Greedy, and Random: Ordinal Approximation Algorithms for Matching and Clustering", "abstract": "We study Matching, Clustering, and related problems in a partial information setting, where the agents' true utilities are hidden, and the algorithm only has access to ordinal preference information. Our model is motivated by the fact that in many settings, agents cannot express the numerical values of their utility for different outcomes, but are still able to rank the outcomes in their order of preference. Specifically, we study problems where the ground truth exists in the form of a weighted graph of agent utilities, but the algorithm receives as input only a preference ordering for each agent induced by the underlying weights. For each agent in a match, there is a choice between the top and bottom of the tree. We also report that in the case of the two top and bottom branches, the optimal candidate is a selected picker whose choice is the top picker. This results in a system that is not very efficient in generating a distribution which can produce more efficient outcomes.\n\n\n\n\nIn the case of real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real-world real", "histories": [["v1", "Thu, 17 Dec 2015 09:29:48 GMT  (36kb)", "https://arxiv.org/abs/1512.05504v1", "This paper contains the results that appeared in a AAAI'16 paper along with several new results for other problems"], ["v2", "Mon, 1 Aug 2016 21:29:51 GMT  (25kb)", "http://arxiv.org/abs/1512.05504v2", "This paper contains the results that appeared in a AAAI'16 paper along with some new results for other problems"]], "COMMENTS": "This paper contains the results that appeared in a AAAI'16 paper along with several new results for other problems", "reviews": [], "SUBJECTS": "cs.GT cs.AI", "authors": ["elliot anshelevich", "shreyas sekar"], "accepted": false, "id": "1512.05504"}, "pdf": {"name": "1512.05504.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n51 2.\n05 50\n4v 2\n[ cs\n.G T\n] 1\nA ug"}, {"heading": "1 Introduction", "text": "Consider the Maximum Weighted Matching (MWM) problem, where the input is an undirected complete graph G = (N , E) and the weight of an edge w(i, j) represents the utility of matching agent i with agent j. The objective is to form a matching (collection of disjoint edges) that maximizes the total utility of the agents. The problem of matching agents and/or items is at the heart of a variety of diverse applications and it is no surprise that this problem and its variants have received extensive consideration in the algorithmic literature [26]. Perhaps, more importantly, maximum weighted matching is one of the few non-trivial combinatorial optimization problems that can be solved optimally in poly-time [14]. In comparison, we study the MWM problem in a partial information setting where the lack of precise knowledge regarding agents\u2019 utilities acts as a barrier against computing optimal matchings, efficiently or otherwise.\nMore generally, in this work, we also look at other graph optimization problems such as clustering in a similar partial information setting, where optimal computation is preemptively stymied by the NP-Hardness of the problem (even in the full information case). This includes the problem of clustering agents to maximize the total weight of edges inside each cluster (Max k-sum), Densest\n\u2217A subset of the results in this paper appeared in the Proceedings of AAAI\u201916\nk-subgraph, and the max traveling salesman problem. Furthermore, for the majority of this work, we assume that the edge weights obey the triangle inequality, since in many important applications it is natural to expect that the weights have some geometric structure. Such structure occurs, for instance, when the agents are points in a metric space and the weight of an edge is the distance between the two endpoints.\nPartial Information - Ordinal Preferences\nA crucial question in algorithm and mechanism design is: \u201cHow much information about the agent utilities does the algorithm designer possess?\u201d. The starting point for the rest of our paper is the observation that in many natural settings, it is unreasonable to expect the mechanism to know the exact weights of the edges in G [7, 30]. For example, when pairing up students for a class project, it may be difficult to precisely quantify the synergy level for every pair of students; ordinal questions such as \u2018who is better suited to partner with student x: y or z?\u2019 may be easier to answer. Such a situation would also arise when the graph represents a social network of agents, as the agents themselves may not be able to express \u2018exactly how much each friendship is worth\u2019, but would likely be able to form an ordering of their friends from best to worst. This phenomenon has also been observed in social choice settings, in which it is much easier to obtain ordinal preferences instead of true agent utilities [2, 30].\nMotivated by this, we consider a model where for every agent i \u2208 N , we only have access to a preference ordering among the agents in N \u2212 {i} so that if w(i, j) > w(i, k), then i prefers j to k. The common approach in Learning Theory while dealing with such ordinal settings is to estimate the \u2018true ground state\u2019 based on some probabilistic assumptions on the underlying utilities [29, 31]. In this paper we take a different approach, and instead focus on the more demanding objective of designing robust algorithms, i.e., algorithms that provide good performance guarantees no matter what the underlying weights are.\nDespite the large body of literature on computing matchings in settings with preference orderings, there has been much less work on quantifying the quality of these matchings. As is common in much of social choice theory, most papers (implicitly) assume that the underlying utilities cannot be measured or do not even exist, and hence there is no clear way to define the quality of a matching [1, 4, 19]. In such papers, the focus therefore is on computing matchings that satisfy normative properties such as stability or optimize a measure of efficiency that depends only on the preference orders, e.g., average rank. On the other hand, the literature on approximation algorithms usually follows the utilitarian approach [20] of assigning a numerical quality to every solution; the presence of input weights is taken for granted. Our work combines the best of both worlds: we do not assume the availability of numerical information (only its latent existence), and yet our approximation algorithms must compete with algorithms that know the true input weights."}, {"heading": "Model and Problem Statements", "text": "For all of the problems studied in this paper, we are given as input a set N of points or agents with |N | = N , and for each i \u2208 N , a strict preference ordering Pi over the agents in N \u2212 {i} . We assume that the input preference orderings are derived from a set of underlying hidden edge weights (w(x, y) for edge x, y \u2208 N ), which satisfy the triangle inequality, i.e., for x, y, z \u2208 N , w(x, y) \u2264 w(x, z) + w(y, z). These weights are considered to represent the ground truth, which is not known to the algorithm. We say that the preferences P are induced by weights w if \u2200x, y, z \u2208 N , if x prefers y to z, then w(x, y) \u2265 w(x, z). Our framework captures a number of well-motivated settings (for matching and clustering problems); we highlight two of them below.\n1. Forming Diverse Teams Our setting and objectives align with the research on diversity maximization algorithms, a topic that has gained significant traction, particularly with respect to forming diverse teams that capture distinct perspectives [22, 27]. In these problems,\neach agent corresponds to a point in a metric space: this point represents the agents\u2019s beliefs, skills, or opinions. Given this background, our matching problem essentially reduces to selecting diverse teams (of size two) based on different diversity goals, since points that are far apart (w(x, y) is large) contribute more to the objective. For instance, one can imagine a teacher pairing up her students who possess differing skill sets or opinions for a class project, which is captured by the maximum weighted matching problem. In section 4, we tackle the problem of forming diverse teams of arbitrary sizes by extending our model to encompass clustering, and team formation.\n2. Friendship Networks In structural balance theory [10], the statement that a friend of a friend is my friend is folklore; this phenomenon is also exhibited by many real-life social networks [18]. More generally, we can say that a graph with continuous weights has this property if w(x, y) \u2265 \u03b1[w(x, z) + w(y, z)] \u2200x, y, z, for some suitably large \u03b1 \u2264 12 . Friendship networks bear a close relationship to our model; in particular every graph that satisfies the friendship property for \u03b1 \u2265 13 must have metric weights, and thus falls within our framework.\nIn this paper our main goal is to form ordinal approximation algorithms for weighted matching problems, which we later extend towards other problems. An algorithm A is said to be ordinal if it only takes preference orderings (P )i\u2208N as input (and not the hidden numerical weights w). It is an \u03b1-approximation algorithm if for all possible weights w, and the corresponding induced preferences P , we have that OPT (w) A(P ) \u2264 \u03b1. Here OPT (w) is the total value of the maximum weight solution with respect to w, and A(P ) is the value of the solution returned by the algorithm for preferences (P )i\u2208N . In other words, such algorithms produce solutions which are always a factor \u03b1 away from optimum, without actually knowing what the weights w are.\nIn the rest of the paper, we focus primarily on the Maximum Weighted Matching(MWM) problem where the goal is to compute a matching to maximize the total (unknown) weight of the edges inside. A close variant of the MWM problem that we will also study is the Max k-Matching (Mk-M) problem where the objective is to select a maximum weight matching consisting of at most k edges. In addition, we also provide ordinal approximations for the following problems:\nMax k-Sum Given an integer k, partition the nodes into k disjoint sets (S1, . . . , Sk) of equal size\nin order to maximize \u2211k\ni=1 \u2211 x,y\u2208Si\nw(x, y). (It is assumed that N is divisible by k). When k = N/2, Max k-sum reduces to the Maximum Weighted Matching problem.\nDensest k-subgraph Given an integer k, compute a set S \u2286 N of size k to maximize the weight of the edges inside S.\nMax TSP In the maximum traveling salesman problem, the objective is to compute a tour T (cycle that visits each node in N exactly once) to maximize \u2211(x,y)\u2208T w(x, y)."}, {"heading": "Challenges and Techniques", "text": "We describe the challenges involved in designing ordinal algorithms through the lens of the Maximum Weighted Matching problem. First, different sets of edge weights may give rise to the same preference ordering and moreover, for each of these weights, the optimum matching can be different. Therefore, unlike for the full information setting, no algorithm (deterministic or randomized) can compute the optimum matching using only ordinal information. More generally, the restriction that only ordinal information is available precludes almost all of the well-known algorithms for computing a matching. So, what kind of algorithms use only preference orderings? One algorithm which can still be implemented is a version of the extremely popular greedy matching algorithm, in which we successively select pairs of agents who choose each other as their top choice. Another trivial algorithm is to choose a matching at random: this certainly does not require any numerical\ninformation! It is not difficult to show that both these algorithms actually provide an ordinal 2-approximation for the maximum weight matching. The main result of this paper, however, is that by interleaving these basic greedy and random techniques in non-trivial ways, it is actually possible to do much better, and obtain a 1.6-approximation algorithm. Moreover, these techniques can further be extended and tailored to give ordinal approximation algorithms for much more general problems."}, {"heading": "Our Contributions", "text": "Our main results are summarized in Table 1. All of the problems that we study have a rich history of algorithms for the full information setting. As seen in the table, our ordinal algorithms provide approximation factors that are close to the best known for the full information versions. In other words, we show that it is possible to find good solutions to such problems even without knowing any of the true weights, using only ordinal preference information instead.\nOur central result in this paper is an ordinal 1.6-approximation algorithm for max-weight matching; this is obtained by a careful interleaving of greedy and random matchings. We also present a deterministic 2-approximation algorithm for Max k-Matching. Note that Max k-Matching for k = N2 is the same as the MWM problem.\nWe also provide a general way to use matching algorithms as a black-box to form ordinal approximation algorithms for other problems: given an ordinal \u03b1-approximation for max-weight matching, we show how to obtain a 2\u03b1, 2\u03b1, and 43\u03b1 approximation for Max k-Sum, Densest kSubgraph, and Max-TSP respectively. Plugging in the appropriate values of \u03b1 for deterministic and randomized algorithms yields the results in Table 1.\nIn total, our results indicate that for matching and clustering problems with metric preferences, ordinal algorithms perform almost as well as algorithms which know the underlying metric weights. Techniques: More generally, one of our main contributions is a framework that allows the design of algorithms for problems where the (metric) weights are hidden. Our framework builds on two simple techniques, greedy and random, and establishes an interesting connection between graph density, matchings, and greedy edges. We believe that this framework may be useful for designing ordinal approximation algorithms in the future."}, {"heading": "Related Work", "text": "Broadly speaking, the cornucopia of algorithms proposed in the matching literature belong to one of two classes: (i) Ordinal algorithms that ignore agent utilities, and focus on (unquantifiable) axiomatic properties such as stability, and (ii) Optimization algorithms where the numerical utilities are fully specified. From our perspective, algorithms belonging to the former class, with the\nexception of Greedy, do not result in good approximations for the hidden optimum, whereas the techniques used in the latter (e.g., [11, 12]) depend heavily on improving cycles and thus, are unsuitable for ordinal settings. A notable exception to the above dichotomy is the class of optimization problems studying ordinal measures of efficiency [1, 9], for example, the average rank of an agent\u2019s partner in the matching. Such settings often involve the definition of \u2018new utility functions\u2019 based on given preferences, and thus are fundamentally different from our model where preexisting cardinal utilities give rise to ordinal preferences.\nThe idea of preference orders induced by metric weights (or a more general utility space) was first considered in the work of Irving et al. [23]. Subsequent work has focused mostly on analyzing the greedy algorithm or on settings where the agent utilities are explicitly known [3, 15]. Most similar to our work is the recent paper by Filos-Ratsikas et al. [17], who prove that for one-sided matchings, no ordinal algorithm can provide an approximation factor better than \u0398( \u221a N). In contrast, for two-sided matchings, there is a simple (greedy) 2-approximation algorithm even when the hidden weights do not obey the metric inequality.\nAs with Matching, all of the problems studied in this paper have received considerable attention in the literature for the full information case with metric weights. In particular, metric Densest Subgraph (also known as Maximum Dispersion or Remote Clique) is quite popular owing to its innumerable applications [6, 5]. The close ties between the optimum solutions for Matching and Max k-sum, and Densest k-subgraph was first explored by Feo and Khellaf [16], and later by Hassin et al. [21]; our black-box mechanism to transform arbitrary matchings into solutions for other problems can be viewed as a generalization of their results. In addition, we also provide improved algorithms for these problems (see Table 1) that do not depend on matchings; for Max k-sum, the bound that we obtain for the ordinal setting is as good as that of the best-known algorithm for the full information setting. Distortion in Social Choice Our work is similar in motivation to the growing body of research studying settings where the voter preferences are induced by a set of hidden utilities [2, 7, 8, 28, 30]. The voting protocols in these papers are essentially ordinal approximation algorithms, albeit for a very specific problem of selecting the utility-maximizing candidate from a set of alternatives.\nFinally, other models of incomplete information have been considered in the Matching literature, most notably Online Algorithms [24] and truthful algorithms (for strategic agents) [13]. Given the strong motivations for preference rankings in settings with agents, it would be interesting to see whether algorithms developed for other partial information models can be extended to our setting."}, {"heading": "2 Framework for Ordinal Matching Algorithms", "text": "In this section, we present our framework for developing ordinal approximation algorithms and establish tight upper and lower bounds on the performance of algorithms that select matching edges either greedily or uniformly at random. As a simple consequence of this framework, we show that the algorithms that sequentially pick all of the edges greedily or uniformly at random both provide 2-approximations to the maximum weight matching. In the following section, we show how to improve this performance by picking some edges greedily, and some randomly. Finally, we remark that for the sake of convenience and brevity, we will often assume that N is even, and sometimes that it is also divisible by 3. As we discuss in the Appendix, our results still hold if this is not the case, with only minor modifications."}, {"heading": "Fundamental Subroutine: Greedy", "text": "We begin with Algorithm 1 that describes a simple greedy procedure for outputting a matching: at each stage, the algorithm picks one edge (x, y) such that the both x and y prefer this edge to all of the other available edges. We now develop some notation required to analyze this procedure.\nDefinition (Undominated Edges) Given a set E of edges, (x, y) \u2208 E is said to be an undominated edge if for all (x, a) and (y, b) in E, w(x, y) \u2265 w(x, a) and w(x, y) \u2265 w(y, b).\ninput : Edge set E, preferences P (N ), k output: Matching MG with k edges while E is not empty (AND) |MG| < k do\npick an undominated edge e = (x, y) from E and add it to MG; remove all edges containing x or y from E;\nend\nAlgorithm 1: Greedy k-Matching Algorithm\nGiven a set E, let us use the notation E\u22a4 to denote the set of undominated edges in E. Finally, we say that an edge set E is complete if \u2203 some S \u2286 N such that E is the complete graph on the nodes in S (minus the self-loops). We make the following two observations regarding undominated edges\n1. Every edge set E has at least one undominated edge. In particular, any maximum weight edge in E is obviously an undominated edge.\n2. Given an edge set E, one can efficiently find at least one edge in E\u22a4 using only the ordinal preference information. A naive algorithm for this is as follows. Consider starting with an arbitrary node x. Let (x, y) be its first choice out of all the edges in E (i.e., y is x\u2019s first choice of all the nodes it has an edge to in E). Now consider y\u2019s first choice. If it is x, then the edge (x, y) must be undominated, as desired. If instead it is some z 6= x, then continue this process with z. Eventually this process must cycle, giving us a cycle of nodes x0, x1, . . . , xl\u22121 such that xi is the top preference of xi\u22121, taken with respect to mod l. This means that all edges in this cycle have equal weight, even though we do not know what this weight is, since xi preferring xi+1 over xi\u22121 means that w(xi, xi+1) \u2265 w(xi, xi\u22121). Moreover, the edge weights of all edges in this cycle must be the highest ones incident on the nodes in this cycle, since they are all top preferences of the nodes. Therefore, all edges in this cycle are undominated, as desired.\nIn general, an edge set E may have multiple undominated edges that are not part of a cycle. Our first lemma shows that these different edges are comparable in weight.\nLemma 2.1. Given a complete edge set E, the weight of any undominated edge is at least half as much as the weight of any other edge in E, i.e., if e = (x, y) \u2208 E\u22a4, then for any (a, b) \u2208 E, we have w(x, y) \u2265 12w(a, b). This is true even if (a, b) is another undominated edge.\nProof. Since (x, y) is an undominated edge, and since E is a complete edge set this means that w(x, y) \u2265 w(x, a), and w(x, y) \u2265 w(x, b). Now, from the triangle inequality, we get w(a, b) \u2264 w(a, x) + w(b, x) \u2264 2w(x, y).\nIt is not difficult to see that when k = N2 , the output of Algorithm 1 coincides with that of the extremely popular greedy algorithm that picks the maximum weight edge at each iteration, and therefore, our algorithm yields an ordinal 2-approximation for the MWM problem. Our next result shows that the approximation factor holds even for Max k-Matching, for any k: this is not a trivial result because at any given stage there may be multiple undominated edges and therefore for k < N2 , the output of Algorithm 1 no longer coincides with that the well known greedy algorithm. In fact, we show the following much stronger lemma,\nLemma 2.2. Given k = \u03b1N2 , and k \u2217 = \u03b1\u2217N2 , the performance of the greedy k-matching with respect to the optimal k\u2217-matching (i.e., OPT (k \u2217)\nGreedy(k)) is given by,\n1. max (2, 2 \u03b1\u2217\n\u03b1 ) if \u03b1\u2217 + \u03b1 < 1\n2. max (2, \u03b1\u2217 + 1\n\u03b1 \u2212 1) if \u03b1\u2217 + \u03b1 \u2265 1\nThus, for example, when \u03b1\u2217 = 1, and \u03b1 = 23 , we get the factor of 0.5, i.e., in order to obtain a half-approximation to the optimum perfect matching, it suffices to greedily choose two-thirds as many edges as in the perfect matching.\nProof. We show the claim via a charging argument where every edge in the optimum matching M\u2217 is charged to one or more edges in the greedy matching M . Specifically, we can imagine that each edge e \u2208 M contains a certain (not necessarily integral) number of slots se, initialized to zero, that measure the number of edges in M\u2217 charged to e. Our proof will proceed in the form of an algorithm: initially U = M\u2217 denotes the set of uncharged edges. In each iteration, we remove some edge from U , charge its weight to some edges in M and increase the value of se for the corresponding edges so that the following invariant always holds: \u2211 e\u2208M sewe \u2265 \u2211 e\u2217\u2208M\u2217\\U we\u2217 . Finally, we can bound the performance ratio using the quantity maxe\u2208M se. We describe our charging algorithm in three phases. Before we describe the first phase, consider any edge e\u2217 = (a, b) in M\u2217. The edge must belong to one of the following two types.\n1. (Type I) Some edge(s) consisting of a or b (both a and b) are present in M .\n2. (Type II) No edge in M has a or b as an endpoint.\nSuppose thatM\u2217 containsm1 Type I edges, andm2 Type II edges. We know thatm1+m2 = k \u2217. Also, let T \u2286 M denote the top m12 edges in M , i.e., the m12 edges with the highest weight. In the first charging phase, we cover all the Type I edges using only the edges in T , and so that no more than two slots of each edge are required.\nClaim 2.3. (First Phase) There exists a mechanism by which we can charge all Type I edges in M\u2217 to the edges in T so that \u2211 e\u2208T sewe \u2265 \u2211 e\u2208TypeI(M\u2217) we and for all e \u2208 T , se \u2264 2.\nProof. We begin by charging the Type I edges to arbitrary edges in M , and then transfer the slots that are outside T to edges in T . Consider any Type I edge e\u2217 = (a, b): without loss of generality, suppose that e = (a, c) is the first edge containing either a or b that was added to M by the greedy algorithm. Since the greedy algorithm only adds undominated edges, we can infer that we \u2265 we\u2217 (or else e would be dominated by e\u2217). Using this idea, we we charge the Type I edges as follows\n(Algorithm: Phase I (Charging)) Repeat until U contains no Type I edge: pick a type I edge e\u2217 = (a, b) from U . Suppose that e = (a, c) is the first edge containing either a or b that was added to M . Since we \u2265 we\u2217 , charge e\u2217 to e, i.e., increase se by one and remove e\u2217 from U .\nAt the end of the above algorithm, U contains no type I edge. Moreover, \u2211\ne\u2208M se = m1 since every Type I edge requires only one slot. Finally, for every e = (x, y) \u2208 M , se \u2264 2. This is because any edge charged to (x, y) must contain at least one of x or y. Now, without altering the set of uncharged edges U , we provide a mechanism to transfer the slots to edges in T . The following procedure is based on the observation that for every e, e\u2032 \u2208 M such that e\u2032 \u2208 T and e /\u2208 T , we\u2032 \u2265 we.\n(Algorithm: Phase I (Slot Transfer)) Repeat until se = 0 for every edge outside T : pick e /\u2208 T such that se > 0. Pick any edge e\u2032 \u2208 T such that se < 2. Transfer the edge originally charged to e to e\u2032, i.e., decrease se by one and increase se\u2032 by one.\nNotice that at the end of the above mechanism, \u2211\ne\u2208T se = m1, se \u2264 2 for all e \u2208 T , and se = 0 for all e \u2208 M \\ T .\nNow, consider any type II edge e\u2217. We make a strong claim: for every e \u2208 M , we \u2265 12we\u2217 . This follows from Lemma 2.1 since at the instant when e was added to M , e was an undominated edge in the edge set E and e\u2217 was also present in the edge set. Therefore, each type II edge can be charged using two (unit) slots from any of the edges in M (or any combination of them). We now describe the second phase of our charging algorithm that charges nodes only to edges in M \\ T , recall that there k \u2212 m12 such edges.\n(Second Phase) Repeat until se = 2 for all e \u2208 M \\ T (or) until U is empty: pick any arbitrary edge e\u2217 from U and e \u2208 M \\ T such that se = 0. Since we\u2217 \u2264 2we, charge e\u2217 using two slots of e, i.e., increase se by two and remove e \u2217 from U .\nDuring the second phase, every edge in M\u2217 is charged to exactly (two slots of) one edge in M \\ T . Therefore, the number of edges removed from U during this phase is min (m2, k \u2212 m12 ). Since the number of uncharged edges at the beginning of Phase I was exactly m2, we conclude that the number of uncharged edges at the end of the second phase, i.e., |U | is min (0,m2 \u2212 k + m12 ). If |U | = 0, we are done, otherwise we can charge the remaining edges in U uniformly to all the edges in M using a fractional number of slots, i.e.,\n(Third Phase) Repeat until U = \u2205: pick any arbitrary edge e\u2217 from U . Since we\u2217 \u2264 2we for all e \u2208 M , charge e\u2217 uniformly to all edges in M , i.e., increase se by 2k for every e \u2208 M and remove e\u2217 from M\u2217.\nNow, in order to complete our analysis, we need to obtain an upper bound for se over all edges in e. Recall that at the end of phase II, se \u2264 2 for all e \u2208 M . In the third phase, se increased by 2 k for every edge in U , and the number of edges in U is min (0,m2 \u2212 k + m12 ). Therefore, at the end of the third phase, we have that for every e \u2208 M ,\nse \u2264 2 + 2\nk [min (0,m2 \u2212 k + m1 2 ).]\nSince m1 +m2 = k \u2217, we can simplify the second term above and get\nse \u2264 2 + 2\nk [min (0, m2 2 + k\u2217 2 \u2212 k)] (1)\n= 2 +min(0, m2 + k\n\u2217\nk \u2212 2)] = min(2, m2 + k\n\u2217\nk ). (2)\nHow large can m2 be? Clearly, m2 \u2264 k\u2217. But a more careful bound can be obtained using the fact that the m2 Type II edges have no node in common with any of the k edges inM . But the total number of nodes is N , therefore, 2m2+2k \u2264 N or m2 \u2264 N2 \u2212k. This gives us m2 \u2264 min(k\u2217, N2 \u2212k). Depending on what the minimum is, we get two cases:\n1. Case I: k\u2217 \u2264 N2 \u2212 k or equivalently, k\u2217 + k \u2264 N2 . Substituting m2 \u2264 k\u2217 in Equation 1, we get that for all e, se \u2264 min(2, 2k \u2217 k ). Replacing k by \u03b1N2 and k \u2217 by \u03b1\u2217 N2 , we get that when\n\u03b1+ \u03b1\u2217 \u2264 1, se \u2264 min(2, 2\u03b1 \u2217\n\u03b1 ).\n2. Case II: k\u2217 \u2265 N2 \u2212 k or equivalently \u03b1\u2217 +\u03b1 \u2265 1. Substituting m2 \u2264 N2 \u2212 k in Equation 1, we\nget that se \u2264 min(2, N 2 + k \u2217 \u2212 k k ) or equivalently se \u2264 min(2, \u03b1 \u2217+1 \u03b1 \u2212 1).\nPlugging in k = k\u2217 in the above lemma immediately gives us the following corollary.\nCorollary 2.4. Algorithm 1 is a deterministic, ordinal 2-approximation algorithm for the Max kMatching problem for all k, and therefore a 2-approximation algorithm for the Maximum Weighted Matching problem."}, {"heading": "Fundamental Subroutine: Random", "text": "An even simpler matching algorithm is simply to form a matching completely at random; this does not even depend on the input preferences. This is formally described in Algorithm 2. In what follows, we show upper and lower bounds on the performance of Algorithm 1 for different edges sets.\ninput : Edge set E, k output: Matching MR with k edges while E is not empty (AND) |MR| < k do\npick an edge from E uniformly at random. Add this edge e = (x, y) to MR; remove all edges containing x or y from E;\nend\nAlgorithm 2: Random k-Matching Algorithm\nLemma 2.5. (Lower Bound)\n1. Suppose G = (T,E) is a complete graph on the set of nodes T \u2286 N with |T | = n. Then, the expected weight of the random (perfect) matching returned by Algorithm 2 for the input E is E[w(MR)] \u2265 1n \u2211 (x,y)\u2208E w(x, y).\n2. Suppose G = (T1, T2, E) is a complete bipartite graph on the set of nodes T1, T2 \u2286 N with |T1| = |T2| = n. Then, the weight of the random (perfect) matching returned by Algorithm 2 for the input E is E[w(MR)] = 1 n \u2211 (x,y)\u2208E w(x, y).\nProof. We show both parts of the theorem using simple symmetry arguments. For the complete (non-bipartite) graph, let M be the set of all perfect matchings in E. Then, we argue that every matching M in M is equally likely to occur. Therefore, the expected weight of MR is\nE[w(MR)] = 1 |M| \u2211\nM\u2208M\nw(M) = \u2211\ne=(x,y)\u2208E\npew(x, y), (3)\nwhere pe is the probability of edge e occurring in the matching. Since the edges are chosen uniformly at random, the probability that a given edge is present in MR is the same for all edges in E. So \u2200e, we have the following bound of pe, which we can substitute in Equation 3 to get the first result.\npe = |MR| |E| =\nn/2 n(n\u2212 1)/2 = 1 n\u2212 1 \u2265 1 n ,\nFor the second case, where E is the set of edges in a complete bipartite graph, it is not hard to see that once again every edge e is present in the final matching with equal probability. Therefore, pe = |MR| |E| = n n2 = 1 n .\nLemma 2.6. (Upper Bound) Let G = (T,E) be a complete subgraph on the set of nodes T \u2286 S with |T | = n, and let M be any perfect matching on the larger set S. Then, the following is an upper bound on the weight of M ,\nw(M) \u2264 2 n\n\u2211\nx\u2208T y\u2208T\nw(x, y) + 1\nn\n\u2211\nx\u2208T y\u2208S\\T\nw(x, y)\nProof. Fix an edge e = (x, y) \u2208 M . Then, by the triangle inequality, the following must hold for every node z \u2208 T : w(x, z) + w(y, z) \u2265 w(x, y). Summing this up over all z \u2208 T , we get\n\u2211\nz\u2208T\nw(x, z) + w(y, z) \u2265 nw(x, y) = n(we).\nOnce again, repeating the above process over all e \u2208 M , and then all z \u2208 T we have\nnw(M) \u2264 2 \u2211\nx\u2208T y\u2208T\nw(x, y) + \u2211\nx\u2208T y\u2208S\\T\nw(x, y)\nEach (x, y) \u2208 E appears twice in the RHS: once when we consider the edge in M containing x, and once when we consider the edge with y.\nWe conclude by proving that picking edges uniformly at random yields a 2-approximation for the MWM problem.\nClaim 2.7. Algorithm 2 is an ordinal 2-approximation algorithm for the Maximum Weighted"}, {"heading": "Matching problem.", "text": "Proof. From Lemma 2.5, we know that in expectation, the matching output by the algorithm when the input is N has a weight of at least 1\nN \u2211 x\u2208N ,y\u2208N w(x, y). Substituing T = S = N in\nLemma 2.6 and M = OPT (max-weight matching) gives us the following upper bound on the weight of OPT , w(OPT ) \u2264 2\nN \u2211 x\u2208N ,y\u2208N w(x, y) \u2264 2E[w(MR)]."}, {"heading": "Lower Bound Example for Ordinal Matchings", "text": "Before presenting our algorithms, it is important to understand the limitations of settings with ordinal information. As mentioned in the Introduction, different sets of weights can give rise to the same preference ordering, and therefore, we cannot suitably approximate the optimum solution for every possible weight. We now show that even for very simple instances, there can be no deterministic 1.5-approximation algorithm, and no randomized 1.25-approximation algorithm.\nClaim 2.8. No deterministic ordinal approximation algorithm can provide an approximation factor better than 1.5, and no randomized ordinal approximation algorithm can provide an approximation factor better than 1.25 for Maximum Weighted Matching. No ordinal algorithm, deterministic or randomized can provide an approximation factor better than 2 for the Max k-Matching problem.\nProof. Consider an instance with 4 nodes having the following preferences: (i) a : b > c > d, (ii) b : a > d > c, (iii) c : a > b > d, (iv) d : b > a > c. Since the matching {(a, d), (b, c)} is weakly dominated, it suffices to consider algorithms that randomize between M1 = {(a, b), (c, d)}, and M2 = {(a, c), (b, d)}, or deterministically chooses one of them.\nNow, consider the following two sets of weights, both of which induce the above preferences but whose optima are M2 and M1 respectively: W1 := w(a, b) = w(a, c) = w(b, d) = w(a, d) = w(b, c) = 1, w(c, d) = \u01eb, and W2 := w(a, b) = 2, w(a, c) = w(b, d) = w(c, d) = w(a, d) = w(b, c) = 1.\nThe best deterministic algorithm always chooses the matching M2, but for the weights W2, this is only a 32 -approximation to OPT.\nConsider any randomized algorithm that chooses M1 with probability x, and M2 with probability (1\u2212x). With a little algebra, we can verify that just for W1, and W2, the optimum randomized algorithm has x = 25 , yielding an approximation factor of 1.25.\nFor the Max k-Matching problem, our results are tight. For small values of k, it is impossible for any ordinal algorithm to provide a better than 2-approximation factor. To see why, consider an instance with 2N nodes {a1, b1, a2, b2, . . . , aN , bN}. Every ai\u2019s first choice is bi and vice-versa, the other preferences can be arbitrary. Pick some i uniformly at random and set w(ai, bi) = 2, and all the other weights are equal to 1. For k = 1, it is easy to see that no randomized algorithm can always pick the max-weight edge and therefore, as N \u2192 \u221e, we get a lower bound of 2.\nSince Max k-sum is a strict generalization of Maximum Weighted Matching, the same lower bounds for Maximum Matching hold for Max k-sum as well."}, {"heading": "3 Ordinal Matching Algorithms", "text": "Here we present a better ordinal approximation than simply taking the random or greedy matching. The algorithm first performs the greedy subroutine until it matches 23 of the agents. Then it either creates a random matching on the unmatched agents, or it creates a random matching between the unmatched agents and a subset of agents which are already matched. We show that one of these matchings is guaranteed to be close to optimum in weight. Unfortunately since we have no access to the weights themselves, we cannot simply choose the best of these two matchings, and thus are forced to randomly select one, giving us good performance in expectation. More formally, the algorithm is:\ninput : N , P (N ) output: Perfect Matching M Initialize E to the be complete graph on N , and M1 = M2 = \u2205; Let M0 be the output returned by Algorithm 1 for E,k = 2 3 N 2 ; Let B be the set of nodes in N not matched in M0, and EB is the complete graph on B.; First Algorithm; M1 = M0 \u222a (The perfect matching output by Algorithm 2 on EB); Second Algorithm ; Choose half the edges from M0 uniformly at random and add them to M2; Let A be the set of nodes in M0 \\M2; Let Eab be the edges of the complete bipartite graph (A,B); Run Algorithm 2 on the set of edges in Eab to obtain a perfect bipartite matching and add the edges returned by the algorithm to M2; Final Output Return M1 with probability 0.5 and M2 with probability 0.5.\nAlgorithm 3: 1.6-Approximation Algorithm for Maximum Weight Matching\nTheorem 3.1. For every input ranking, Algorithm 3 returns a 85 = 1.6-approximation to the maximum-weight matching.\nProof. First, we provide some high-level intuition on why this algorithm results in a significant improvement over the standard half-optimal greedy and randomized approaches. Observe that in order to obtain a half-approximation to OPT , it is sufficient to greedily select 23 (N/2) edges (substitute \u03b1 = 23 , \u03b1 \u2217 = 1 in Lemma 2.2). Choosing all N2 edges greedily would be overkill, and so\nwe choose the remaining edges randomly in the First Algorithm of Alg 3. Now, let us denote by Top, the set of 23N nodes that are matched greedily. The main idea behind the second Algorithm is that if the first one performs poorly (not that much better than half), then, all the \u2018good edges\u2019 must be going across the cut from Top to Bottom (B). In other words, \u2211 (x,y)\u2208Top\u00d7B w(x, y) must be large, and therefore, the randomized algorithm for bipartite graphs should perform well. In summary, since we randomized between the first and second algorithms, we are guaranteed that at least one of them should have a good performance for any given instance.\nWe now prove the theorem formally. By linearity of expectation, E[w(M)] = 0.5(E[w(M1)] + E[w(M2)]). Now, look at the first algorithm, sinceM0 has two-thirds as many edges as the optimum matching, we get from Lemma 2.2 that w(M0) \u2265 12w(OPT ). As mentioned in the algorithm, B is the set of nodes that are not present in M0; since we randomly match the nodes in B to other nodes in B, the expected weight of the random algorithm (from Lemma 2.5 with n = N3 ) is 3 N \u2211 (x,y)\u2208B w(x, y). Therefore, we get the following lower bound on the weight of M1,\nE[w(M1)] \u2265 OPT\n2 +\n3\nN\n\u2211\n(x,y)\u2208B\nw(x, y).\nNext, look at the second algorithm: half the edges from M0 are added to M2. A constitutes the set of N3 nodes from M0 that are not present in M2, these nodes are randomly matched to those in B. Let MAB denote the matching going \u2018across the cut\u2019 from A to B. Since the set A is chosen randomly from the nodes in Top, the expected weight of the matching from A to B is given by,\nE[w(MAB)] = \u2211\nS\u2282Top |S|=N\n3\nE [w(MAB) | (A = S)]Pr(A = S)\n= \u2211\nS\u2282Top |S|=N\n3\nPr(A = S) \u2211\n(x,y)\u2208S\u00d7B\n3\nN w(x, y)\n= \u2211\n(x,y)\u2208Top\u00d7B\n3\nN w(x, y)Pr(x \u2208 A)\n= \u2211\n(x,y)\u2208Top\u00d7B\n3 N w(x, y)\u00d7 1 2 .\nThe second equation above comes from Lemma 2.5 Part 2 for n = N3 , and the last step follows from the observation that Pr(x \u2208 A) is exactly equal to the probability that the edge containing x in M0 is not added to M2, which is one half (since the edge is chosen with probability 0.5). We can now bound the performance of M2 as follows,\nE[w(M2)] = 1\n2 E[w(M0)] + E[w(MAB)]\n= 1\n2 w(M0) +\n3\n2N\n\u2211\n(x,y)\u2208Top\u00d7B\nw(x, y).\nNow, let us apply Lemma 2.6 to the set T = B (n = N3 ), with OPT being the matching: we get that w(OPT ) \u2264 6\nN \u2211 x\u2208B,y\u2208B w(x, y)+ 3 N \u2211 x\u2208Top,y\u2208B w(x, y) or equivalently 3 2N \u2211 (x,y)\u2208Top\u00d7B w(x, y) \u2265\nw(OPT ) 2 \u2212 3N \u2211 x\u2208B,y\u2208B w(x, y). Substituting this in the above equation for M2 along with the fact that w(M0) \u2265 w(OPT )2 , we get the following lower bound for the performance of M2 in terms of OPT,\nE[w(M2)] \u2265 w(OPT )\n4 +\nw(OPT ) 2 \u2212 3 N\n\u2211\nx\u2208B,y\u2208B\nw(x, y).\nRecall that\nE[w(M1)] \u2265 w(OPT )\n2 +\n3\nN\n\u2211\nx\u2208B,y\u2208B\nw(x, y).\nThe final bound comes from adding the two quantities above and multiplying by half."}, {"heading": "Matching without the Metric Assumption", "text": "We now discuss the general case where the hidden weights do not obey the triangle inequality. From our discussion in Section 2, we infer that Algorithm 1 still yields a 2-approximation to the MWM problem as its output coincides with that of the classic greedy algorithm. No deterministic algorithm can provide a better approximation; consider the same preference orderings as Claim 2.8 and the following two sets of consistent weights: (i) w(c, d) = \u01eb, other weights are 1, and (ii) w(a, b) = 1, other weights are \u01eb. The only good choice for case (ii) is the matching M1, which yields a 2-approximation for case (i).\nWe now go one step further and show that even if we are allowed to utilize randomized mechanisms, we still can\u2019t do that much better than an ordinal 2-approximation factor.\nClaim 3.2. When the hidden weights do not obey the triangle inequality, there exist a set of preferences such that no randomized algorithm can provide an ordinal approximation factor better than 53 for every set of weights consistent with these preferences.\nProof. The instance consists of 8 nodes (ai, bi) 4 i=1, and we describe the preferences in a slightly unconventional but more intuitive manner. Every node p \u2208 N is assigned a rank r(p) and two or more nodes may have the same rank. Now, the preference ordering for a node q is simply a list of the nodes in N \u2212{q} sorted in the ascending order of their rank and ties in the rank can be broken arbitrarily. For this instance, we set rank(ai) = rank(bi) = i for i = 1 to 4. So for instance, a1\u2019s preference ordering can be: b1 > a2 > b2 > a3 > b3 > a4 > b4.\nFor all of the weights that we consider, w(a1, b1) = 1, w(p, q) = 0 if p = {a3, a4}, q = {b3, b4}. Moreover, for a given i, the weight of all edges going out of ai and bi are the same. By symmetry, there exists an optimal randomized mechanism for this instance that randomizes among the following six matchings, choosing matching Mi with probability xi.\n1. M1 = {(a1, b1), (a2, b2), (a3, b3), (a4, b4)}\n2. M2 = {(a1, b1), (a2, a3), (b2, b3), (a4, b4)}\n3. M3 = {(a1, a2), (b1, b2), (a3, b3), (a4, b4)}\n4. M4 = {(a1, a2), (b1, a3), (b2, b3), (a4, b4)}\n5. M5 = {(a1, a3), (b1, b3), (a2, a4), (b2, b4)}\n6. M6 = {(a1, a3), (b1, b3), (a2, b2), (a4, b4)}.\nWe now construct sets of weights consistent with the preferences. Suppose that the optimal randomized strategy A provides an ordinal approximation factor of 1\nc for some c \u2208 (0, 1]. Then for\nevery set of weights W , it is true that\nOPT (W ) E[A(w)] \u2264 1 c (4)\nWe now explicitly construct some weights and derive an upper bound of 35 on c, which implies the no randomized strategy can have an approximation factor better than 53 for this instance.\n1. All weights are zero except w(a1, b1) = 1. For this instance OPT = 1, and only M1,M2 give non-zero utility, so A(W ) = x1 + x2. Applying Equation 4, we get x1 + x2 \u2265 c.\n2. w(a1, a2) = w(a1, b2) = w(b1, a2) = w(b1, b2) = 1, rest are zero. OPT = 2, A(w) = x1 + x2 + 2x3 + x4. The corrresponding inequality is x1 + x2 + 2x3 + x4 \u2265 2c.\n3. All weights going out of a1, b1 are one. Among the remaining weights, only w(a2, b2) = 1. OPT = 3. A(W ) = 2x1 + x2 + 2x3 + 2x4 + 2x5 + 3x6, giving us the inequality, 2x1 + x2 + 2x3 + 2x4 + 2x5 + 3x6 \u2265 3c.\n4. The final instance has all weights coming out of a1, a2, b1, b2 to be one. OPT = 4. The final inequality is A(W ) = 2x1 + 3x2 + 2x3 + 3x4 + 4x5 + 3x6 \u2265 4c.\nAdding all of the 4 inequalities above gives us \u2211 xi \u2265 106 c. Since \u2211\nxi = 1, we get that c \u2264 35 , which completes the proof.\nWe hypothesize that as we extend the instance in the above claim, as N \u2192 \u221e, we should obtain a lower bound of 2, which meets our upper bound. For Max k-matching, the situation is much more bleak; no algorithm, deterministic or randomized can provide a reasonable approximation factor if k is small. As we did before, consider an instance with 2N nodes {a1, b1, a2, b2, . . . , aN , bN}. Every ai\u2019s first choice is bi and vice-versa, the other preferences can be arbitrary. Pick some i uniformly at random and set w(ai, bi) = 1, and all the other weights are equal to \u01eb. For k = 1, it is easy to see that every randomized algorithm obtains non-zero utility only with probability 1 N , whereas OPT = 1. Therefore, the ordinal approximation factor for any random algorithm is N and as N \u2192 \u221e, the factor becomes unbounded. Moreover, for other values of k, the ordinal approximation factor for the same instance is N\nk ."}, {"heading": "4 Matching as a Black-Box for other Problems", "text": "In this section, we highlight the versatility of matchings by showing how matching algorithms can be used as a black-box to obtain good (ordinal) approximation algorithms for other problems. These reductions serve as stand-alone results, as the algorithms for matching are easy to implement as well as extremely common in settings with preference lists. Moreover, future improvements on the ordinal approximation factor for matchings can be directly plugged in to obtain better bounds for these problems."}, {"heading": "Informal Statement of Results", "text": "1. Max k-Sum: Given an \u03b1-approximate perfect matching M , we can obtain a nice clustering as follows: \u201csimply divide M into k equal sized sets (with N2k edges in each) and form clusters using the nodes in each of the equal-sized sets.\u201d It turns out that this simple mechanism provides a 2\u03b1-approximation to the optimum clustering. Plugging in our 1.6-approximation algorithm, we immediately get a 3.2-approximation algorithm for Max k-sum.\n2. Densest k-Subgraph: Suppose we are provided an \u03b1-approximate matching M of size k2 , how good is the set S containing the k nodes in M? Using Lemma 2.6, we can establish that\nS is at least as dense as w(M)2 , and the density of the optimum solution is at most \u03b1w(M). Therefore, S is a 2\u03b1-approximation to the optimum set of size k. This easy-to-implement mechanism directly yields a 4-approximation algorithm for Densest k-subgraph.\n3. Max TSP: Given an \u03b1-approximate perfect matching M , any tour T containing M is a 2\u03b1-approximation since the weight of the optimum tour cannot be more than twice that of\nthe optimum matching. However, if we carefully form T using only undominated edges, we can show that the resulting solution is a 43 -approximation to the optimum tour. Plugging in \u03b1 = 1.6, we get an ordinal 2.14-approximation algorithm for Max TSP."}, {"heading": "Formal Results", "text": "Theorem 4.1. 1. Any \u03b1-approximation algorithm for the Maximum Weight Perfect Matching problem can be used to obtain a 2\u03b1-approximation for the Max k-Sum problem.\n2. Any \u03b1-approximation algorithm for the Maximum Weight k2 -Matching problem can be used to obtain a 2\u03b1-approximation for the Densest k-Subgraph problem.\n3. Any \u03b1-approximation algorithm for the Maximum Weight Perfect Matching problem can be used to obtain a 4\n3\u2212 4 N\n\u03b1-approximation for the Max TSP problem.\nProof. (Part 1) Suppose that we are provided a perfect matching M that is a \u03b1-approximation to the optimum matching. We use the following simple procedure to cluster the nodes into k-clusters:\n1. Initialize k empty clusters S1, S2, . . . , Sk, and M \u2032 = M .\n2. While \u2203 some Si such that |Si| 6= Nk 3. Pick some edge e \u2208 M \u2032, add both the end-points of e to Si, remove e from M \u2032.\nThe only important property we require is that for every edge e \u2208 M , both its end points belong to the same cluster. We now prove that this a 2\u03b1-approximation to the optimum max k-sum solution (OPT = (O1, . . . , Ok)). Let M \u2217 be the optimum perfect matching, and let c = N k . Finally, since the end points of every edge in M belong to the same cluster, without l.o.g, let Mi denote the edges of M that are present in the cluster Si. First, we establish a lower bound on the quality of our solution S,\nw(S) =\nk\u2211\ni=1\n\u2211\n(x,y)\u2208Si\nw(x, y)\n\u2265 k\u2211\ni=1\nc 2 w(Mk) (Lemma 2.6)\n= 1\n2 cw(M).\nNow, we establish an upper bound for OPT in terms of w(M). Suppose that M\u2217(Oi) is the maximum weight perfect matching on the set of nodes Oi. Then,\nw(OPT ) = k\u2211\ni=1\n\u2211\n(x,y)\u2208Oi\nw(x, y)\n\u2264 k\u2211\ni=1\ncw(M\u2217(Oi)) (Corollary ??)\n\u2264 cw( k\u22c3\ni=1\nM\u2217(Oi)) \u2264 cw(M\u2217)\n\u2264 c\u03b1w(M).\nReconciling the two bounds gives us the desired factor of 2\u03b1.\n(Part 2) Once again, we use M\u2217 to denote the optimum k2 -matching and M to denote the \u03b1-approximation. Let O be the optimum solution to the Densest k-subgraph problem for the given value of k. Then our algorithm simply returns the solution S compromising of the endpoints of all the edges in M . The proof is quite similar to the proof for Part 1.\nw(O) = \u2211\n(x,y)\u2208O\nw(x, y) \u2264 kw(M\u2217(O)) \u2264 kw(M\u2217)\n\u2264 \u03b1kw(M) \u2264 \u03b1(2 \u2211\n(x,y)\u2208S\nw(x, y)).\n(Part 3) Let OPT denote the optimum solution to the Max TSP problem, and M\u2217, the maximum weight perfect matching. Then, it is not hard to see that w(M\u2217) is at least 12w(OPT ) since the sets consisting of only the odd or even edges from OPT are perfect matchings. Now, as a first step towards showing our black-box result, we provide a very general procedure in the following lemma that takes as input any arbitrary matching M , and outputs a Hamiltonian path (tour minus one edge) Q whose weight is at least 32w(M) minus a small factor that vanishes as N increases.\nLemma 4.2. Given any matching M with k edges, there exists an efficient ordinal algorithm that computes a Hamiltonian path Q containing M such that the weight of the Hamiltonian path in expectation is at least\n[ 3 2 \u2212 1 k ]w(M).\nProof. We first provide the algorithm, followed by its analysis. Suppose that K is the set of nodes contained in M .\n1. Select a node i \u2208 K uniformly at random. Suppose that e(i) is the edge in M containing i.\n2. Initialize Q = M .\n3. Order the edges in M arbitrarily into (e1, e2, . . . , ek) with the constraint that e1 = e(i).\n4. For j = 2 to k,\n5. Let x be a node in ej\u22121 having degree one in Q (if j = 2 choose x 6= i) and ej = (y, z).\n6. If y >x z, add (x, y) to Q, else add (x, z) to Q.\nSuppose that Q(j) consists of the set of nodes in Q for a given value of j (at the end of that iteration of the algorithm). We claim that w(Q) \u2265 32w(M) \u2212 w(e1), which we prove using the following inductive hypothesis,\nw(Q(j)) \u2265 w(M) + j\u2211\nr=2\n1 2 w(er)\nConsider the base case when j = 2. Suppose that e1 = (i, a), and e2 = (x, y). Without loss of generality, suppose that a prefers x to y, then in that iteration, we add (a, x) to Q. By the triangle inequality, we also know that w(x, y) \u2264 w(x, a)+w(y, a) \u2264 2w(x, a). Therefore, at the end of that iteration, we have\nw(Q) = w(M) + w(x, a) \u2265 w(M) + 1 2 w(e2). (5)\nThe inductive step follows similarly. For some value of j, let x be the degree one node in ej\u22121, and ej = (y, z). Suppose that x prefers y to z, then using the same argument as above, we know that (x, y) is added to our desired set and that w(x, y) \u2265 12w(y, z). The claim follows in an almost similar fashion to Equation 5 and the inductive hypothesis.\nIn conclusion, the total weight of the path is 32w(M) \u2212 w(e1). Since the first node i is chosen uniformly at random, every edge in M has an equal probability (p = 1\nk ) of being e1. So, in\nexpectation, the weight of the tour is 32w(M) \u2212 1kw(M), which completes the lemma.\nThe rest of the proof for the black-box mechanism follows almost directly from the lemma. Suppose that M is an \u03b1-approximation to the optimum matching M\u2217. Then, applying the lemma we get a path Q whose weight is at least [ 32 \u2212 2N ]w(M). We complete this path to form a tour T getting,\nw(T ) \u2265 [ 3 2 \u2212 2 N ]w(M) \u2265 [ 3 2 \u2212 2 N ] w(M\u2217) \u03b1\n\u2265 [ 3 4 \u2212 1 N ] w(OPT ) \u03b1 .\nSome basic algebra yields the desired bound.\nUsing the above black-box theorem and the results of the previous sections, we immediately obtain the following: a 3.2-approximation algorithms for Max k-Sum, a 4-approximation algorithm for Densest k-Subgraph, and a 2.14-approximation algorithm for Max TSP."}, {"heading": "5 Conclusion", "text": "In this paper we study ordinal algorithms, i.e., algorithms which are aware only of preference orderings instead of the hidden weights or utilities which generate such orderings. Perhaps surprisingly, our results indicate that for many problems including Matching, Densest Subgraph, and Traveling Salesman, ordinal algorithms perform almost as well as the best algorithms which know the underlying metric weights. This indicates that for settings involving agents where it is expensive, or impossible, to obtain the true numerical weights or utilities, one can use ordinal mechanisms without much loss in welfare.\nWhile many of our algorithms are randomized, and the quality guarantees are \u201cin expectation\u201d, similar techniques can be used to obtain weaker bounds for deterministic algorithms (bounds of 2 for Matching, and of 4 for the other problems considered). It may also be possible to improve the deterministic approximation factor for matching to be better than 2: although this seems to be a difficult problem which would require novel techniques, such an algorithm would immediately provide new deterministic algorithms for the other problems using our black-box reductions. Finally, it would be very interesting to see how well ordinal algorithms perform if the weights obeyed some structure other than the metric inequality."}, {"heading": "A Friendship Networks", "text": "The classic theory of Structural Balance [10] argues that agents embedded in a social network must exhibit the property that \u2018a friend of a friend is a friend\u2019. This phenomenon has also been observed in many real-life networks (see for example [18]). Mathematically, if we have an unweighted social network G = (V,E) of (say) friendships, it is easy to check if this property holds. If for any (i, j) \u2208 E, (j, k) is also an edge, then (i, k) must also belong to E. For this reason, this property has also been referred to as transitive or triadic closure.\nWhat about weighted networks that capture the \u2018intensity of friendships\u2019? There is no obvious way as to how this property can be extended to weighted graphs without placing heavy constraints on the weights. For example, if we impose that for every edge (i, j), and every node k, w(i, k) \u2265 min(w(i, j), w(j, k)), we immediately condemn all triangles to be isosceles (w.r.t the weights). Instead, we argue that a more reasonable mathematical property that extends triadic closure to Weighted graphs is the following\nDefinition (\u03b1-Weighted Friendship Property) Given a social network G = (V,E,W ), and a fixed parameter \u03b1 \u2208 [0, 12 ], for every (i, j, k): w(i, k) \u2265 \u03b1[w(i, j) + w(j, k)].\nIn a nutshell, this property captures the idea that if (i, j) is a \u2018good edge\u2019, and (j, k) is a good edge, then so is (i, k). The parameter \u03b1 gives us some flexibility on how stringently we can impose the property. Notice that in a sense, this property appears to be the opposite of the metric inequality, here we require that w(i, k) is not too small compared to w(i, j) + w(j, k). However, we show that this is not the case; in fact for every \u03b1 \u2265 13 , any weighted graph that satisfies the \u03b1-Weighted Friendship Property must also satisfy the metric inequality.\nClaim A.1. Suppose that G = (V,E,W ) is a weighted complete graph that satisfies the \u03b1-Weighted Friendship Property for some \u03b1 \u2208 [ 13 , 12 ]. Then, for every (i, j, k), we have w(i, j) \u2264 w(i, k)+w(j, k).\nProof. Without loss of generality, it suffices to show the proof for the case where w(i, j) is the heaviest edge in the triangle (i, j, k). Now consider w(i, k),w(j, k) and without loss of generality, suppose that w(i, k) \u2265 w(j, k). Then, since the \u03b1-friendship property is obeyed, we have\nw(j, k) \u2265 \u03b1[w(i, j) + w(i, k)] \u2265 \u03b1w(i, j) + \u03b1w(j, k).\nThis gives us that w(i, j) \u2264 1\u2212\u03b1 \u03b1 w(j, k). It is easy to verify that for \u03b1 \u2208 [ 13 , 12 ], the quantity 1\u2212\u03b1 \u03b1 \u2264 2. Therefore, we get\nw(i, j) \u2264 2w(j, k) \u2264 w(i, k) + w(j, k)."}, {"heading": "B Odd Number of Agents: Extensions", "text": "In many of our algorithms, we assumed that N (the number of agents) is even for the sake of convenience and in order to capture our main ideas concisely without worrying about the boundary cases. Here, we show that all of our algorithms can be extended to the case where N is odd or not divisible by 3 with only minor modifications to the proofs and bounds obtained."}, {"heading": "Matching", "text": "We begin by arguing that of all our algorithms and proofs for matching hold even when N is odd. First of all, it is not hard to see that our framework does not really depend on the parity of N\nand the lemmas on the greedy and random techniques carry over to the case when N is not even. In particular, note that in Lemma 2.5, we had that E[w(MR)] \u2265 |MR||E| \u2211 x,y\u2208N w(x, y), where E is the total number of edges in the complete graph. When N is odd, |MR| = N\u221212 , and |E| is still N(N\u22121)\n2 . Therefore, we still get that E(w(MR)] \u2265 1N \u2211\nx,y\u2208N w(x, y). Next, we argue that our main 1.6-algorithm still holds for arbitrary N (not divisible by two and/or three) with an \u01eb multiplicative error term that vanishes as N \u2192 \u221e. In Algorithm 3, when N is not divisible by three, we may need to choose a matching with \u2308N3 \u2309 edges for M0. Let B be the largest matching outside of M0, then |B| = \u230aN6 \u230b. Then, the second sub-routine in Algorithm 3 proceeds by selecting |B| edges from M0, and matching those nodes arbitrarily to the nodes in B. Ideally, we would like \u2308N3 \u2309 = |M0| = 2|B| = 2\u230aN6 \u230b. The worst case multiplicative error happens when \u2308N3 \u2309 is much larger than 2\u230aN6 \u230b; this happens when N is odd, and has the form 3p + 2 for some positive integer p. With some basic algebra, we can show that the multiplicative error is at most 78(2N\u22123) , which approaches zero as N increases.\nMax k-Sum\nIn the case of the black-box theorem, the 2\u03b1 reduction still holds if we modify the algorithm as follows when N\nk is odd. Instead of selecting the optimum perfect matching, we need to select\na matching of size 12 ( N k \u2212 1) \u2217 k = 12 (N \u2212 k), assign every pair of matched nodes to the same cluster, and the unmatched nodes arbitrarily. The rest of the proof is the same. Now when we apply this black-box result, we can no longer invoke the 1.6-approximation algorithm for perfect matchings and only use the 2-approximation greedy algorithm for a matching that selects N\u2212k2 edges, and therefore, the black-box result only yields a 4-approximation when N\nk is odd, but our\nmain algorithm gives a 2-approximation algorithm irrespective of its parity.\nDensest k-Subgraph\nAll of the proofs for the Densest k-subgraph hold. When k is odd, we simply select a matching with \u230ak2 \u230b edges instead of k2 . The rest of the proof is exactly the same."}, {"heading": "Max TSP", "text": "During the proofs for Max TSP, we extensively make use of the fact that w(M\u2217) \u2265 w(T \u2217)\n2 , where M\u2217 is the optimum matching, and T \u2217 is the optimal tour. This may no longer be true when N is odd. However, suppose that M\u2217f is the optimum fractional matching, it is not hard to verify that w(M\u2217f ) \u2265 w(T \u2217) 2 ; this follows from taking T \u2217 and choosing each edge with probability 12 . Moreover, observe that all of our proofs in this paper for the greedy algorithm (namely Lemma 2.2) are true when we compare the solution to the optimum fractional matching of a given size. Therefore, the black-box result itself carries over."}], "references": [{"title": "Approximating optimal social choice under metric preferences", "author": ["Elliot Anshelevich", "Onkar Bhardwaj", "John Postl"], "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Geometric stable roommates", "author": ["Esther M. Arkin", "Sang Won Bae", "Alon Efrat", "Kazuya Okamoto", "Joseph S.B. Mitchell", "Valentin Polishchuk"], "venue": "Inf. Process. Lett.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Social welfare in one-sided matching markets without money", "author": ["Anand Bhalgat", "Deeparnab Chakrabarty", "Sanjeev Khanna"], "venue": "In Proceedings of the Workshop on Approximation Algorithms for Combinatorial Optimization Problems (APPROX", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Consideration set generation in commerce search", "author": ["Sayan Bhattacharya", "Sreenivas Gollapudi", "Kamesh Munagala"], "venue": "In Proceedings of the 20th International Conference on World Wide Web,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "An improved analysis for a greedy remoteclique algorithm using factor-revealing", "author": ["Benjamin Birnbaum", "Kenneth J Goldman"], "venue": "lps. Algorithmica,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Optimal social choice functions: A utilitarian view", "author": ["Craig Boutilier", "Ioannis Caragiannis", "Simi Haber", "Tyler Lu", "Ariel D. Procaccia", "Or Sheffet"], "venue": "Artif. Intell.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Voting almost maximizes social welfare despite limited communication", "author": ["Ioannis Caragiannis", "Ariel D. Procaccia"], "venue": "Artif. Intell.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Welfare maximization and truthfulness in mechanism design with ordinal preferences", "author": ["Deeparnab Chakrabarty", "Chaitanya Swamy"], "venue": "In Proceedings of the 5th Innovations in Theoretical Computer Science conference (ITCS", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Clustering and structural balance in graphs", "author": ["James A Davis"], "venue": "Social networks. A developing paradigm,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1977}, {"title": "Improved linear time approximation algorithms for weighted matchings", "author": ["Doratha E. Drake", "Stefan Hougardy"], "venue": "In Proceedings of APPROX", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Approximating maximum weight matching in near-linear time", "author": ["Ran Duan", "Seth Pettie"], "venue": "In Proceedings of the 51st Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Truthful assignment without money", "author": ["Shaddin Dughmi", "Arpita Ghosh"], "venue": "In Proceedings of the 11th ACM Conference on Electronic Commerce (EC 2010),,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Paths, trees, and flowers", "author": ["Jack Edmonds"], "venue": "Canadian Journal of mathematics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1965}, {"title": "The price of matching with metric preferences", "author": ["Yuval Emek", "Tobias Langner", "Roger Wattenhofer"], "venue": "In Proceedings of the 23rd Annual European Symposium of Algorithms (ESA", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "A class of bounded approximation algorithms for graph partitioning", "author": ["Thomas A Feo", "Mallek Khellaf"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1990}, {"title": "Social welfare in onesided matchings: Random priority and beyond", "author": ["Aris Filos-Ratsikas", "Jie Zhang"], "venue": "In Proceedings of the 7th International Symposium on Algorithmic Game Theory (SAGT", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Birds of a feather, or friend of a friend? using exponential random graph models to investigate adolescent social networks. Demography", "author": ["Steven M Goodreau", "James A Kitts", "Martina Morris"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "The stable marriage problem: structure and algorithms", "author": ["Dan Gusfield", "Robert W Irving"], "venue": "MIT press,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1989}, {"title": "Cardinal welfare, individualistic ethics, and interpersonal comparisons of utility", "author": ["John C Harsanyi"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1976}, {"title": "Approximation algorithms for maximum dispersion", "author": ["Refael Hassin", "Shlomi Rubinstein", "Arie Tamir"], "venue": "Operations research letters,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1997}, {"title": "Composable core-sets for diversity and coverage maximization", "author": ["Piotr Indyk", "Sepideh Mahabadi", "Mohammad Mahdian", "Vahab S. Mirrokni"], "venue": "In Proceedings of the 33rd ACM Symposium on Principles of Database Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "An efficient algorithm for the optimal stable marriage", "author": ["Robert W Irving", "Paul Leather", "Dan Gusfield"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1987}, {"title": "Online weighted matching", "author": ["Bala Kalyanasundaram", "Kirk Pruhs"], "venue": "J. Algorithms,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1993}, {"title": "Deterministic 7/8-approximation for the metric maximum tsp", "author": ["Lukasz Kowalik", "Marcin Mucha"], "venue": "Theoretical Computer Science,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Matching theory, volume 367", "author": ["L\u00e1szl\u00f3 Lov\u00e1sz", "Michael D Plummer"], "venue": "American Mathematical Soc.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Multi-agent team formation: Diversity beats strength", "author": ["Leandro Soriano Marcolino", "Albert Xin Jiang", "Milind Tambe"], "venue": "IJCAI", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Give a hard problem to a diverse team: Exploring large action spaces", "author": ["Leandro Soriano Marcolino", "Haifeng Xu", "Albert Xin Jiang", "Milind Tambe", "Emma Bowring"], "venue": "Proceedings of the Twenty-Eigth AAAI Conference on Artificial Intelligence", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Learning mixed multinomial logit model from ordinal data", "author": ["Sewoong Oh", "Devavrat Shah"], "venue": "In Proceedings of the 28th Annual Conference on Neural Information Processing Systems (NIPS", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "The distortion of cardinal preferences in voting", "author": ["Ariel D. Procaccia", "Jeffrey S. Rosenschein"], "venue": "In Proceedings of 10th Intl. Workshop on Cooperative Information Agents CIA", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2006}], "referenceMentions": [{"referenceID": 24, "context": "The problem of matching agents and/or items is at the heart of a variety of diverse applications and it is no surprise that this problem and its variants have received extensive consideration in the algorithmic literature [26].", "startOffset": 222, "endOffset": 226}, {"referenceID": 12, "context": "Perhaps, more importantly, maximum weighted matching is one of the few non-trivial combinatorial optimization problems that can be solved optimally in poly-time [14].", "startOffset": 161, "endOffset": 165}, {"referenceID": 5, "context": "The starting point for the rest of our paper is the observation that in many natural settings, it is unreasonable to expect the mechanism to know the exact weights of the edges in G [7, 30].", "startOffset": 182, "endOffset": 189}, {"referenceID": 28, "context": "The starting point for the rest of our paper is the observation that in many natural settings, it is unreasonable to expect the mechanism to know the exact weights of the edges in G [7, 30].", "startOffset": 182, "endOffset": 189}, {"referenceID": 0, "context": "This phenomenon has also been observed in social choice settings, in which it is much easier to obtain ordinal preferences instead of true agent utilities [2, 30].", "startOffset": 155, "endOffset": 162}, {"referenceID": 28, "context": "This phenomenon has also been observed in social choice settings, in which it is much easier to obtain ordinal preferences instead of true agent utilities [2, 30].", "startOffset": 155, "endOffset": 162}, {"referenceID": 27, "context": "The common approach in Learning Theory while dealing with such ordinal settings is to estimate the \u2018true ground state\u2019 based on some probabilistic assumptions on the underlying utilities [29, 31].", "startOffset": 187, "endOffset": 195}, {"referenceID": 2, "context": "As is common in much of social choice theory, most papers (implicitly) assume that the underlying utilities cannot be measured or do not even exist, and hence there is no clear way to define the quality of a matching [1, 4, 19].", "startOffset": 217, "endOffset": 227}, {"referenceID": 17, "context": "As is common in much of social choice theory, most papers (implicitly) assume that the underlying utilities cannot be measured or do not even exist, and hence there is no clear way to define the quality of a matching [1, 4, 19].", "startOffset": 217, "endOffset": 227}, {"referenceID": 18, "context": "On the other hand, the literature on approximation algorithms usually follows the utilitarian approach [20] of assigning a numerical quality to every solution; the presence of input weights is taken for granted.", "startOffset": 103, "endOffset": 107}, {"referenceID": 20, "context": "Forming Diverse Teams Our setting and objectives align with the research on diversity maximization algorithms, a topic that has gained significant traction, particularly with respect to forming diverse teams that capture distinct perspectives [22, 27].", "startOffset": 243, "endOffset": 251}, {"referenceID": 25, "context": "Forming Diverse Teams Our setting and objectives align with the research on diversity maximization algorithms, a topic that has gained significant traction, particularly with respect to forming diverse teams that capture distinct perspectives [22, 27].", "startOffset": 243, "endOffset": 251}, {"referenceID": 8, "context": "Friendship Networks In structural balance theory [10], the statement that a friend of a friend is my friend is folklore; this phenomenon is also exhibited by many real-life social networks [18].", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "Friendship Networks In structural balance theory [10], the statement that a friend of a friend is my friend is folklore; this phenomenon is also exhibited by many real-life social networks [18].", "startOffset": 189, "endOffset": 193}, {"referenceID": 12, "context": "Problem Full Info Our Results (Ordinal Bounds) Deterministic Randomized Max Weighted Matching 1 [14] 2 1.", "startOffset": 96, "endOffset": 100}, {"referenceID": 24, "context": "6 Max k-Matching 1 [26] 2 2 Max k-Sum 2 [16, 21] 4 3.", "startOffset": 19, "endOffset": 23}, {"referenceID": 14, "context": "6 Max k-Matching 1 [26] 2 2 Max k-Sum 2 [16, 21] 4 3.", "startOffset": 40, "endOffset": 48}, {"referenceID": 19, "context": "6 Max k-Matching 1 [26] 2 2 Max k-Sum 2 [16, 21] 4 3.", "startOffset": 40, "endOffset": 48}, {"referenceID": 4, "context": "2 Densest k-Subgraph 2 [6, 21] 4 4 Max TSP 87 [25] 3.", "startOffset": 23, "endOffset": 30}, {"referenceID": 19, "context": "2 Densest k-Subgraph 2 [6, 21] 4 4 Max TSP 87 [25] 3.", "startOffset": 23, "endOffset": 30}, {"referenceID": 23, "context": "2 Densest k-Subgraph 2 [6, 21] 4 4 Max TSP 87 [25] 3.", "startOffset": 46, "endOffset": 50}, {"referenceID": 9, "context": ", [11, 12]) depend heavily on improving cycles and thus, are unsuitable for ordinal settings.", "startOffset": 2, "endOffset": 10}, {"referenceID": 10, "context": ", [11, 12]) depend heavily on improving cycles and thus, are unsuitable for ordinal settings.", "startOffset": 2, "endOffset": 10}, {"referenceID": 7, "context": "A notable exception to the above dichotomy is the class of optimization problems studying ordinal measures of efficiency [1, 9], for example, the average rank of an agent\u2019s partner in the matching.", "startOffset": 121, "endOffset": 127}, {"referenceID": 21, "context": "[23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Subsequent work has focused mostly on analyzing the greedy algorithm or on settings where the agent utilities are explicitly known [3, 15].", "startOffset": 131, "endOffset": 138}, {"referenceID": 13, "context": "Subsequent work has focused mostly on analyzing the greedy algorithm or on settings where the agent utilities are explicitly known [3, 15].", "startOffset": 131, "endOffset": 138}, {"referenceID": 15, "context": "[17], who prove that for one-sided matchings, no ordinal algorithm can provide an approximation factor better than \u0398( \u221a N).", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "In particular, metric Densest Subgraph (also known as Maximum Dispersion or Remote Clique) is quite popular owing to its innumerable applications [6, 5].", "startOffset": 146, "endOffset": 152}, {"referenceID": 3, "context": "In particular, metric Densest Subgraph (also known as Maximum Dispersion or Remote Clique) is quite popular owing to its innumerable applications [6, 5].", "startOffset": 146, "endOffset": 152}, {"referenceID": 14, "context": "The close ties between the optimum solutions for Matching and Max k-sum, and Densest k-subgraph was first explored by Feo and Khellaf [16], and later by Hassin et al.", "startOffset": 134, "endOffset": 138}, {"referenceID": 19, "context": "[21]; our black-box mechanism to transform arbitrary matchings into solutions for other problems can be viewed as a generalization of their results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Distortion in Social Choice Our work is similar in motivation to the growing body of research studying settings where the voter preferences are induced by a set of hidden utilities [2, 7, 8, 28, 30].", "startOffset": 181, "endOffset": 198}, {"referenceID": 5, "context": "Distortion in Social Choice Our work is similar in motivation to the growing body of research studying settings where the voter preferences are induced by a set of hidden utilities [2, 7, 8, 28, 30].", "startOffset": 181, "endOffset": 198}, {"referenceID": 6, "context": "Distortion in Social Choice Our work is similar in motivation to the growing body of research studying settings where the voter preferences are induced by a set of hidden utilities [2, 7, 8, 28, 30].", "startOffset": 181, "endOffset": 198}, {"referenceID": 26, "context": "Distortion in Social Choice Our work is similar in motivation to the growing body of research studying settings where the voter preferences are induced by a set of hidden utilities [2, 7, 8, 28, 30].", "startOffset": 181, "endOffset": 198}, {"referenceID": 28, "context": "Distortion in Social Choice Our work is similar in motivation to the growing body of research studying settings where the voter preferences are induced by a set of hidden utilities [2, 7, 8, 28, 30].", "startOffset": 181, "endOffset": 198}, {"referenceID": 22, "context": "Finally, other models of incomplete information have been considered in the Matching literature, most notably Online Algorithms [24] and truthful algorithms (for strategic agents) [13].", "startOffset": 128, "endOffset": 132}, {"referenceID": 11, "context": "Finally, other models of incomplete information have been considered in the Matching literature, most notably Online Algorithms [24] and truthful algorithms (for strategic agents) [13].", "startOffset": 180, "endOffset": 184}], "year": 2016, "abstractText": "We study Matching and other related problems in a partial information setting where the agents\u2019 utilities for being matched to other agents are hidden and the mechanism only has access to ordinal preference information. Our model is motivated by the fact that in many settings, agents cannot express the numerical values of their utility for different outcomes, but are still able to rank the outcomes in their order of preference. Specifically, we study problems where the ground truth exists in the form of a weighted graph, and look to design algorithms that approximate the true optimum matching using only the preference orderings for each agent (induced by the hidden weights) as input. If no restrictions are placed on the weights, then one cannot hope to do better than the simple greedy algorithm, which yields a half optimal matching. Perhaps surprisingly, we show that by imposing a little structure on the weights, we can improve upon the trivial algorithm significantly: we design a 1.6-approximation algorithm for instances where the hidden weights obey the metric inequality. Using our algorithms for matching as a black-box, we also design new approximation algorithms for other closely related problems: these include a a 3.2-approximation for the problem of clustering agents into equal sized partitions, a 4-approximation algorithm for Densest k-subgraph, and a 2.14-approximation algorithm for Max TSP. These results are the first non-trivial ordinal approximation algorithms for such problems, and indicate that we can design robust algorithms even when we are agnostic to the precise agent utilities.", "creator": "LaTeX with hyperref package"}}}