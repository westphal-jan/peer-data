{"id": "1512.06757", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2015", "title": "GraphConnect: A Regularization Framework for Neural Networks", "abstract": "Deep neural networks have proved very successful in domains where large training sets are available, but when the number of training samples is small, their performance suffers from overfitting. Prior methods of reducing overfitting such as weight decay, Dropout and DropConnect are data-independent. This paper proposes a new method, GraphConnect, that is data-dependent, and is motivated by the observation that data of interest lie close to a manifold of potential problems for neural networks. The goal of the graphConnect method is to measure the neural networks on a set of questions. Data are not only correlated to one particular set of questions, they can also be modeled on other datasets. To understand the neural networks that fit into one dataset, we have created a network of two large data sets with identical weights, all representing 1\u2013100 times as big. For each dataset, one data set contains one training set of 20,000 training sets. The output of each training set includes the training set of 40,000 training sets. In this dataset, each training set includes the training set of 10,000 training sets. The training set of 10,000 training sets includes the training set of 10,000 training sets.\n\n\n\n\n\nTo see the results from the graphConnect method in action, you need to enter a string in the top left corner of the graph, then enter a value in the left corner of the graph, and the key to enter a value in the right corner of the graph. If you want to add more data, simply enter this string into the graph. You need to enter a value in the bottom right corner of the graph and enter a value in the right corner of the graph, then enter a value in the right corner of the graph and enter a value in the right corner of the graph.\n\n\nA key to the graphConnect method is to use the following three different ways:\n\nFirst, to see which data you should use:\nThe first one is a data-independent search for \u201c_1,\u201c_2,\u201c_3. For example, you can either return a dataset with a different value or an algorithm with a different value or a different value. The other approach is to choose a single data-independent search for a data-independent search for \u201c_1,\u201c_2,\u201c_3. In this way, you can search for a dataset with different values and an algorithm with a different value or a different value. The other approach is to choose a", "histories": [["v1", "Mon, 21 Dec 2015 18:42:45 GMT  (979kb,D)", "http://arxiv.org/abs/1512.06757v1", null], ["v2", "Wed, 27 Jan 2016 03:21:15 GMT  (0kb,I)", "http://arxiv.org/abs/1512.06757v2", "Theorems need more validation"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["jiaji huang", "qiang qiu", "robert calderbank", "guillermo sapiro"], "accepted": false, "id": "1512.06757"}, "pdf": {"name": "1512.06757.pdf", "metadata": {"source": "CRF", "title": "GraphConnect: A Regularization Framework for Neural Networks", "authors": ["Jiaji Huang", "Qiang Qiu", "Robert Calderbank", "Guillermo Sapiro"], "emails": ["guillermo.sapiro}@duke.edu"], "sections": [{"heading": "1. Introduction", "text": "Neural networks have proved very successful in domains where large training sets are available, since their capacity can be increased by adding layers or by increasing the number of units in a layer. When the number of training samples is small their performance suffers from overfitting. The degree of overfitting is measured by the generalization error, which is the difference between the loss on the training set and the loss on the test set. The best known bounds on the generalization error arise from the VapnikChervonenkis (VC) dimension [17], which captures the inherent complexity of a family of classifiers. However it is\ndistribution agnostic, leading to a loose and pessimistic upper bound [4].\nIn the last decade, distribution dependent measures of complexity have been developed, such as the Rademacher complexity [2], which can lead to tighter bounds on the generalization error. Rademacher complexity has been used to show that the generalization error of a neural network depends more on the size of the weights than on the size of the network [1]. This theoretical result supports a form of regularization called weight decay that simply encourages the `-2 norm of all parameters to be small. A unified theoretical treatment of norm-based control of deep neural networks is given in [12], and this theory is used to provide insight into max-out networks in [6].\nDropout is a new form of regularization, where for each training example, forward propagation involves randomly deleting half the activations in each layer [7]. Dropconnect is a recent generalization of Dropout, where a randomly selected subset of weights within the network is set to zero [18]. Both methods introduce randomness into training so that the learned network is in some sense a statistical average of an ensemble of realizations. Adding a Dropconnect layer to a neural network can reduce the Rademacher complexity by a factor of the dropconnect rate [18].\nIn this paper we propose a fundamentally different approach to control the complexity of a neural network, thereby preventing overfitting. Our approach differs from the aforementioned methods in that it is data dependent. It is motivated by the empirical observation that data of interest typically lie close to a manifold, an assumption that has previously assisted machine learning tasks such as nonlinear embedding [10], semi-supervised labeling [11], and multitask classification [5]. The underlying idea in these works is to encourage the relationships between the learned decisions to resemble a graph representing the manifold structure.\nIn this work we propose to use graph structures, derived from training data, to regularize deep neural networks. We present theoretical and experimental results that\n1\nar X\niv :1\n51 2.\n06 75\n7v 1\n[ cs\n.C V\n] 2\n1 D\nec 2\ndemonstrate the importance of this new data dependent approach to prevention of overfitting. Previous experimental studies [19], applying graph regularization to the semisupervised embedding problem, did not explain how the approach might control capacity. In particular, it is not clear from [19] whether graph-based regularization has better generalization ability than standard approaches such as weight decay. In contrast we demonstrate, both experimentally and theoretically that graph regularization outperforms weight decay, and that when the number of training samples is extremely limited, it is able to significantly improve performance of a deep neural network. An illustrative example is given in Fig. 1, where we transform 1,000 test samples (Fig. 1a) via learned networks regularized by weight decay (Fig. 1b) and our proposed GraphConnect (Fig. 1c, 1d). GraphConnect significantly improves the discriminability than does weight decay."}, {"heading": "2. GraphConnect", "text": "A note on the notation: Upper and lower case bold letters denote matrices and vectors respectively. Plain letters denote scalars. We consider L-way classification using a deep neural network. Given a datum x, the network first learns a multidimensional feature g(x), and then applies a softmax classifier to generate a probability distribution over the L classes. We use cross entropy loss to measure the effectiveness of the learning machine, and view the deep network as a function from a datum x to the corresponding loss `(x).\nThe average loss achieved on the training set {x1, . . . ,xN} is the empirical loss `emp, given by\n`emp = 1\nN N\u2211 i=1 `i.\nThe expected loss ` is estimated from a large test set and is given by\n` = Ex[`(x)].\nThe difference ` \u2212 `emp between the expected loss on the test data and the empirical loss on the training data is the generalization error. When training samples are scarce, statistical learning theory predicts overfitting to the training data [17]. The larger the generalization error, the more severe is the problem of overfitting. We recall from [8] that the generalization error is (almost surely) bounded by the empirical Rademacher complexity [2], of the loss function. Hence we can reduce the degree of overfitting by controlling the empirical Rademacher complexity.\nDefinition 1 (Empirical Rademacher Complexity). Let D be a probability distribution on a set X and assume that x1, . . . ,xN are independent samples from D. Let U be a class of functions mapping from X to R. The empirical Rademacher complexity of U is\nR\u0302N (U) = E\u03c3i\n[ 2\nN sup u\u2208U \u2223\u2223\u2223\u2223\u2223 N\u2211 i=1 \u03c3iu(xi) \u2223\u2223\u2223\u2223\u2223 : x1, . . . ,xN ] ,\nwhere the \u03c3i\u2019s are independent uniform {\u00b11}-valued random variables.\nRemark 1. Over-fitting occurs when a statistical model describes random error or noise instead of the underlying signal. We seek to minimize the empirical Rademacher complexity because it measures correlation with random errors. However we need to keep the objective of classification in mind, since we can reduce the empirical Rademacher complexity to zero by simply mapping every datum x to 0. Therefore minimization of Rademacher complexity needs to be performed over a class of functions U that is able to discriminate between classes.\nDropconnect, [18], is a recent method for regularizing large fully connected layers within neural networks. A layer consists of a linear feature extraction, followed by activation, followed by a softmax classifier. The Rademacher complexity of the composite layer differs from\nthe Rademacher complexity of the linear feature extraction component by a multiplicative factor that is determined by the classifier [18]. Hence we follow [18] and focus on the Rademacher complexity of the linear feature extraction."}, {"heading": "2.1. Analysis: Regularizing a Linear Layer", "text": "The functions U appearing in Definition 1 map multidimensional inputs to scalars, therefore in order to make use of Rademacher complexity we need to work with individual coordinate entries of multidimensional features. Given an input z to a linear layer, we consider the linear map f(z) = v>z, where the linear weights v are from a set V specified by graph-based regularization. More formally we consider the function family,\nF = { f(z)|f(z) = v>z,v \u2208 V } . (1)\nSuppose now that we have learned a graph where symmetric edge weights W encode the relationships between N input samples Z def= [z1, . . . , zN ]. The set V that defines the function family F is given by{\nv : 1N\n( (1\u2212 \u03b7) \u2211N i,j=1Wi,j [f(zi)\u2212 f(zj)]2 + \u03b7\u2016v\u20162 ) \u2264 B 2\n4\n} ,\n(2) for some positive constantB and for some \u03b7 \u2208 (0, 1]. When \u03b7 = 1, this condition enforces conventional weight decay, and as \u03b7 approaches 0, it enforces graph regularization.\nLet 1 be the all-one vector, let D be the diagonal matrix with entries W1, and let L = D \u2212W be the graph Laplacian. The Laplacian is symmetric, hence\nN\u2211 i,j=1 Wi,j [f(zi)\u2212 f(zj)]2 = v>(ZLZ>)v, (3)\nand since the Laplacian is also diagonally dominant, it is positive semidefinite. Therefore by introducing identity matrix I, we can describe the set V very simply as\nV = { v \u2223\u2223\u2223\u2223v> ((1\u2212 \u03b7)ZLZ> + \u03b7I)v \u2264 NB24 } . (4)\nNote that since \u03b7 > 0, the matrix (1 \u2212 \u03b7)ZLZ> + \u03b7I is positive definite. We now bound R\u0302N (F). Theorem 1 (Empirical Rademacher Complexity of Graph Regularization). Let F be the class of linear functions defined in Eq. (1), where V is the set defined in Eq. (4), and let Z def = [z1, . . . , zN ] \u2208 Rn\u00d7N be the sample set on which the Rademacher complexity R\u0302N (F) is evaluated. Then\nR\u0302N (F) \u2264 B\n\u221a\u221a\u221a\u221a\u221a tr [ Z> ( (1\u2212 \u03b7)ZLZ> + \u03b7I )\u22121 Z ] N\nProof. See the supplementary material.\nGraph regularization includes weight decay regularization as the special case \u03b7 = 1. In the definition of V and Theorem 1, we have excluded the value \u03b7 = 0 so that the inverse [(1\u2212 \u03b7)ZLZ> + \u03b7I]\u22121 exists. However, in our experiments with only a graph regularizer (\u03b7 = 0), we have also observed strong generalization performance.\nReducing the Rademacher complexity of a single linear layer is an important step in reducing the Rademacher complexity of a multi-layer network. The bound in Theorem 1 depends on the eigenvalues of the Laplacian L, which in turn depend on the edge weights W of the graph. Whatever graph we choose, the Rademacher complexity of graph regularization will be at least as good as that of weight decay. We now provide an example to show that by choosing an appropriate graph we can achieve significant improvements.\nExample. Consider the classical MNIST data. There are 10 classes in the dataset, and samples are 784-dimensional (28x28 images). We randomly select N samples (N/10 samples per class), remove the sample mean, and form the 784 \u00d7 N matrix Z. The edge weights W are given by Wi,j = exp ( \u2212\u2016zi\u2212zj\u2016 2\n\u03b32\n) , where \u03b3 is the average\nof all pairwise distances. We form the Laplacian L and evaluate the bound given in Theorem 1 for several values of N . We observe in Fig. 2 that the upper bound decreases significantly as \u03b7 moves away from 1 (weight decay), showing the added value of graph regularization. As the sample size N increases, the bound decreases steadily, consistent with our intuition about the generalization error."}, {"heading": "2.2. Analysis: Regularizing Multiple Layers", "text": "We now extend our analysis to include the effects in intermediate layers of pooling and of activation functions such as rectifiers. We consider aK-layer network that maps\nan input x onto a multidimensional feature g(x), then restrict to a single dimension to obtain a scalar g(x) given by\ng(x) = v>KsK\u22121(\u00b7 \u00b7 \u00b7 s2(V2s1(V1x))), where vK ,Vi \u2208 V , (5) where the nonlinear mapping sk(\u00b7) represents activation and pooling. V1, . . . ,VK\u22121 are matrices representing linear layers, and vk is a vector that maps input to a single coordinate in the final output feature. The V1, . . . ,VK\u22121 and vK are taken from a set V that is defined by the property\n1\nN N\u2211 i,j=1 Wi,j [g(xi)\u2212 g(xj)]2 \u2264 B2 4 . (6)\nThe symmetric edge weights W encode the relationships between the N input samples samples [x1, . . . ,xN ].\nSet g(X) = [g(x1), . . . , g(xN )]> and recall that\nN\u2211 i,j=1 Wi,j [g(xi)\u2212 g(xj)]2 = g(X)Lg(X)>,\nwhere L is as before the Lapacian of W. As above, we want to work with a positive definite matrix so we add a small multiple of the identity matrix In. We now derive an upper bound on the empirical Rademacher complexity for the function class G defined by\nG = { g(x)| g(x) = v>Ks(\u00b7 \u00b7 \u00b7 s(V2s(V1x)))},where\ng(X)(L+ I)g(X)> \u2264 NB 2\n4\n} .\n(7) Theorem 2. R\u0302N (G) \u2264 B \u221a tr[(L+ In)\u22121] N\nProof. See supplementary material.\nRemark 2. Theorem 2 provides an upper bound on complexity that is not very sensitive to the number of layers in the network, in contrast to weight decay where complexity is exponential in the number of layers [20].\n2.3. The GraphConnect Algorithm\nThe theory presented in Section 2.1 and 2.2 applies to scalar valued functions of vector inputs, but the generalization to vector valued functions is straightforward. Eq. (2) becomes\n1\nN N\u2211 i,j=1 Wi,j\u2016f(zi)\u2212 f(zj)\u20162 \u2264 B2 4 , (8)\nwhere f(z) is the multidimensional output of a linear layer. Similarly, when we consider a K-layer network, Eq. (6) becomes\n1\nN N\u2211 i,j=1 Wi,j\u2016g(xi)\u2212 g(xj)\u20162 \u2264 B2 4 , (9)\nwhere g(x) is the multidimensional feature output at layer K.\nFig. 3 describes two flavors of GraphConnect, that are two different ways of using a graph to regularize a neural network. The first GraphConnect-One uses the constraint given by Eq. (8) to regularize individual layers, and this method can be applied to all layers or to some layers but not others. The second GraphConnect-All uses the constraint given by Eq. (9) to regularize the final learned features. Implementation requires multiplying the GraphConnect regularizer by some \u03bb > 0 then adding this quantity to the original objective function used to train the neural network. We conclude this section by showing that both GraphConnect regularization schemes require only minor changes to the standard back-propagation algorithm.\nGradient Descent Solver for GraphConnect-One We seek to minimize\n`emp + J,\nwhere J is the GraphConnect regularization term given by\nJ = \u03bb N\u2211 i,j=1 Wi,j\u2016f(zi)\u2212 f(zj)\u20162 = \u03bbtr[f(Z)Lf(Z)>].\nThe gradient of J w.r.t. f(Z) is\n\u2202J\n\u2202f(Z) = 2\u03bbf(Z)L.\nSo we just need to add an extra term 2\u03bbf(Z)L to the original gradient with respect to f(Z).\nGradient Descent Solver for GraphConnect-All The\nanalysis is very similar and we just need to add an extra term 2\u03bbg(X)L to the original gradient with respect to g(X). GraphConnect regularization requires only minor modifications to standard back propagation algorithms and is very efficient in practice. In the next section we demonstrate that when the training set is small, it can lead to significant improvements in classification performance."}, {"heading": "2.4. Discussion", "text": "Generalization error differs from empirical Rademacher complexity by a multiplicative factor that is determined by the overall softmax classifier [18]. The method of graph regularization can be applied to a single layer or to multiple layers. It is designed to learn attributes that are present in data samples, in contrast to weight decay, Dropout [7], and Dropconnect [18] which are designed to prevent learning non-attributes (overfitting to random error or noise). The approach taken in both Dropout and Dropconnect is to introduce randomness into training so that learned network is in some sense a statistical average of an ensemble of realizations. They complement our approach of regularizing output using a graph and we plan to explore a combination of these approaches in future work."}, {"heading": "3. Experiments", "text": "We are particularly interested in how the graph regularization methods in Section 2.3 compare with the conventional weight decay. The experiments section are organized as follows. In section 3.1, we use the MNIST dataset to show that the generalization error of GraphConnectOne and GraphConnect-All are both signifiantly smaller than weight decay, and they achieve superior classification accuracy especially when the training set is small. Section 3.2 presents extensive comparisons between the proposed GraphConnect and weight decay on CIFAR-10 and SVHN. Section 3.3 demonstrates the improved performance of GraphConnect on the face verification task."}, {"heading": "3.1. MNIST Proof of Concept", "text": "The MNIST dataset contains approximately 60,000 training images (28 \u00d7 28) and 10,000 test images. While state-of-the-art methods often use the entire training set, we are interested in quantifying what is possible with much smaller training sets. Table 1 describes the network architecture that we use to compare graph regularization with the standard weight decay.\nWe begin by using 500 training samples (50 per class) to train two neural networks. Image mean is estimated from the training set and subtracted as a preprocessing step. The first experiment uses GraphConnect-One to regularize the outputs of layers 3 and 5. The second uses GraphConnectAll to regularize the output of layer 6. The graph edge\nweights Wi,j are given by\nWi,j =\n{ exp ( \u2016xi\u2212xj\u20162\n\u03c32c\n) if xi,xj \u2208 class c\n0 if xi,xj \u2208 different classes ,\nwhere the diameter \u03c3c is an estimate of the average distance between pairs of samples in class c. We tune the regularizer for each network, and select the value that maximizes classification accuracy. We then calculate the empirical loss `emp on the training set, the loss ` on the test data, and the generalization error `emp\u2212`. The results presented in Fig. 4 show that the two variants of GraphConnect have approximately the same generalization error and that both are superior to weight decay, in particular with small training sets. Fig. 1 illustrates why GraphConnect outperforms weight decay. We take 1000 test samples, transform them using the learned networks, then embed the features into a two dimensional coordinate via PCA and represent each class by a different color. All learned features (Fig. 1b to 1d) are more discriminative than the initial data (Fig. 1a). Moreover, it is evident that both GraphConnect-One and GraphConnect-All better distinguishes the different classes than does weight decay.\nNext we vary the size of the training set from 500 to 6,000, and repeat the above experiment. We tune the regularizer for each method and select the value to maximize classification accuracy. When the number of training samples is small, Fig. 4b shows GraphConnect yields a generalization error that is significantly smaller than that yielded by weight decay. Performance becomes broadly similar as the size of the training set increases. The same trend is evident in Fig. 4c which compares classification accuracy of the three methods.\nSince performance of the two variants of GraphConnect is broadly similar, and since GraphConnect-One involves tuning multiple regularizers, we focus on GraphConnectAll in the sequel."}, {"heading": "3.2. Comparison on CIFAR-10 and SVHN", "text": "CIFAR-10 and SVHN are benchmark RGB image datasets, each containing 10 classes, that are more challenging than the MNIST benchmark because of more significant intra-class variation (see Fig. 5). We compare regularization using GraphConnect-All with regularization using weight decay on these two datasets. Table 2 specifies the network architecture (similar to [7]), all images are mean-subtracted in a preprocessing step, and the graph weights W used in GraphConnect-All are computed in the same fashion as for the MNIST experiment. The network transforms sample images into 2048-dimensional features that are input to a softmax classifier, and the cross entropy loss is evaluated on the classifier output.\nWe first train the network on a very small training set (subset of the whole training ensemble), and evaluate the\n1Arts by courtesy of https://www.kaggle.com/c/cifar-10 and http://ufldl.stanford.edu/housenumbers/\nempirical loss `emp on the training set and the expected loss ` on test set. The regularizer for each method is chosen such that the best classification accuracy is achieved. Fig 6a and 7a show how `emp and ` vary throughout iterations on these two datasets. Weight-decay overfits on the training set and its test loss increases after some iterations. In contrast, GraphConnect has a smaller gap between ` and `emp, indicating smaller generalization error.\nNext we vary the size of the training set and evaluate the generalization error (Figs. 6b and 7b) and classification accuracy (Fig. 6c and 7c). GraphConnect exhibits smaller generalization error than weight decay. The classification error is also smaller but less significant than the generalization error since cross entropy loss is a nonlinear func-\ntion w.r.t. the probability produced by softmax classifier. Compared with the MNIST example (Section3.1), the improvement in classification accuracy is modest because in contrast to the MNIST benchmark, the intra-class variation here is substantial. More sophisticated preprocessing methods such as contrast normalization [13] and ZCA whitening [9] will reduce intra-class variation, and we would expect them to improve performance further. We leave this direction for future research."}, {"heading": "3.3. Face Verification on LFW", "text": "We now evaluate GraphConnect on face verification, using the Labeled Faces in the Wild (LFW) benchmark dataset. The face verification task is to decide, when presented with a pair of facial images, whether the two images represent the same subject. Impressive verification accuracies are possible when deep neural networks are able to train on extremely large labeled training sets [14, 16]. The training sets are often proprietary, making it difficult to reproduce these successes, but that is not our aim in this work. Given the same network architecture, we seek to compare the performance of GraphConnect with that of weight de-\ncay. We adopt the experimental framework used in [3], and train a deep network on the WDRef dataset, where each face is described using a high dimensional LBP feature (available at 2) that is reduced to a 5,000-dimensional feature using PCA. The WDRef dataset is significantly smaller than the proprietary datasets in [14, 15, 16]. For example, [16] uses 4.4 million labeled faces from 4,030 individuals. [14] and [15] use 202,599 labeled faces from 10,177 individuals, while WDRef contains 2,995 subjects with only about 30 samples per subject, clearly a much more challenging task.\nWe consider the two-layer fully connected network described in Tab. 3, where the activation function is a rectifier. The network transforms a 5,000-dimensional input vector to a 2,000-dimensional feature vector, which is then input to a softmax classifier. The network parameters are learned using WDRef and the testing is carried out on the LFW dataset. Our focus is the expressiveness of the learned feature, so we do not employ advanced verification methods such as those used in [3] (those will make the study of\n2http://home.ustc.edu.cn/chendong/\nthe network itself very obscure). Instead, we simply compute the Euclidean distance between a pair of (learned) face features and compare it with a threshold to make a decision.\nWe vary the number of training samples per class and evaluate verification performance. We report results for the value of the regularization parameter that optimizes verification accuracy. Fig. 8a compares verification accuracies for GraphConnect and weight decay as a function of the size of the training set. GraphConnect consistently outperforms weight decay.\nFig. 8b compares the ROCs curves when a training set of size 64K is used. Corresponding Area Under Curves (AUCs) are reported in Tab. 4. As a baseline, we also evaluate the verification performance on the initial LBP features (without any learning). We observe from Fig. 8b and Tab. 4\nthat the learned features significantly outperform the initial LBP features, while GraphConnect further improves upon weight decay, validating the effectiveness of GraphConnect regularization when training set is small."}, {"heading": "4. Conlusion", "text": "We have proposed GraphConnect, a data-dependent framework for regularizing deep neural networks, and we have compared performance against data-independent methods of regularization that are in widespread use. We proved that the empirical Rademacher complexity of GraphConnect is smaller than that of weight decay, justifying our claim that it is better at preventing overfitting. We presented experimental results that validate our theoretical claims, showing that when the training set is small the improvements in generalization error are significant. Our proposed framework is complementary to data-independent approaches that prevent overfitting, such as Dropout and DropConnect, and future work will explore the value of combining these methods."}], "references": [{"title": "The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network", "author": ["P. Bartlett"], "venue": "IEEE Transactions on Information Theory, 44(2):525\u2013536,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research, 3:463\u2013482,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Bayesian face revisited: A joint formulation", "author": ["D. Chen", "X. Cao", "L. Wang", "F. Wen", "J. Sun"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "How tight are the vapnikchervonenkis bounds", "author": ["D. Cohn", "G. Tesauro"], "venue": "Neural Computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1992}, {"title": "Learning multiple tasks with kernel methods", "author": ["T. Evgeniou", "C. Micchelli", "M. Pontil"], "venue": "Journal of Machine Learning Research, 6:615\u2013637,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Max-out networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A.C. Courville", "Y. Bengio"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Empirical margin distributions and bounding the generalization error of combined classifiers", "author": ["V. Koltchinskii", "D. Panchenko"], "venue": "Annals of Statistics, 30(1):1\u201350,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["B. Mikhail", "P. Niyogi"], "venue": "Neural computation, 15(6):1373\u20131396,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["B. Mikhail", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of Machine Learning Research, 7:2399\u20132434,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Normbased capacity control in neural networks arxiv preprint arxiv:1503.00036 (2015)", "author": ["B. Neyshabur", "R. Tomioka", "N. Srebro"], "venue": "In The 28th Conference on Learning Theory (COLT),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Convolutional neural networks applied to house numbers digit classification", "author": ["P. Sermanet", "S. Chintala", "Y. LeCun"], "venue": "International Conference on Pattern Recognition (ICPR),", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning face representation by joint identification-verification", "author": ["Y. Sun", "Y. Chen", "X. Wang", "X. Tang"], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 1988\u20131996,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning face representation from predicting 10,000 classes", "author": ["Y. Sun", "X. Wang", "X. Tang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1891\u20131898,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. Taigman", "M. Yang", "M.A. Ranzato", "L. Wolf"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1701\u20131708,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "An overview of statistical learning theory", "author": ["V. Vapnik"], "venue": "IEEE Transactions on Neural Networks, 10(5):988999,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning via semi-supervised embedding", "author": ["J. Weston", "F. Ratle", "H. Mobahi", "R. Collobert"], "venue": "Neural Networks: Tricks of the Trade. Springer Berlin Heidelberg, pages 639\u2013 655,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Robustness and generalization", "author": ["H. Xu", "S. Mannor"], "venue": "Machine learning, 86(3):391\u2013423,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 16, "context": "The best known bounds on the generalization error arise from the VapnikChervonenkis (VC) dimension [17], which captures the inherent complexity of a family of classifiers.", "startOffset": 99, "endOffset": 103}, {"referenceID": 3, "context": "However it is distribution agnostic, leading to a loose and pessimistic upper bound [4].", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "In the last decade, distribution dependent measures of complexity have been developed, such as the Rademacher complexity [2], which can lead to tighter bounds on the generalization error.", "startOffset": 121, "endOffset": 124}, {"referenceID": 0, "context": "Rademacher complexity has been used to show that the generalization error of a neural network depends more on the size of the weights than on the size of the network [1].", "startOffset": 166, "endOffset": 169}, {"referenceID": 11, "context": "A unified theoretical treatment of norm-based control of deep neural networks is given in [12], and this theory is used to provide insight into max-out networks in [6].", "startOffset": 90, "endOffset": 94}, {"referenceID": 5, "context": "A unified theoretical treatment of norm-based control of deep neural networks is given in [12], and this theory is used to provide insight into max-out networks in [6].", "startOffset": 164, "endOffset": 167}, {"referenceID": 6, "context": "Dropout is a new form of regularization, where for each training example, forward propagation involves randomly deleting half the activations in each layer [7].", "startOffset": 156, "endOffset": 159}, {"referenceID": 17, "context": "Dropconnect is a recent generalization of Dropout, where a randomly selected subset of weights within the network is set to zero [18].", "startOffset": 129, "endOffset": 133}, {"referenceID": 17, "context": "Adding a Dropconnect layer to a neural network can reduce the Rademacher complexity by a factor of the dropconnect rate [18].", "startOffset": 120, "endOffset": 124}, {"referenceID": 9, "context": "It is motivated by the empirical observation that data of interest typically lie close to a manifold, an assumption that has previously assisted machine learning tasks such as nonlinear embedding [10], semi-supervised labeling [11], and multitask classification [5].", "startOffset": 196, "endOffset": 200}, {"referenceID": 10, "context": "It is motivated by the empirical observation that data of interest typically lie close to a manifold, an assumption that has previously assisted machine learning tasks such as nonlinear embedding [10], semi-supervised labeling [11], and multitask classification [5].", "startOffset": 227, "endOffset": 231}, {"referenceID": 4, "context": "It is motivated by the empirical observation that data of interest typically lie close to a manifold, an assumption that has previously assisted machine learning tasks such as nonlinear embedding [10], semi-supervised labeling [11], and multitask classification [5].", "startOffset": 262, "endOffset": 265}, {"referenceID": 18, "context": "Previous experimental studies [19], applying graph regularization to the semisupervised embedding problem, did not explain how the approach might control capacity.", "startOffset": 30, "endOffset": 34}, {"referenceID": 18, "context": "In particular, it is not clear from [19] whether graph-based regularization has better generalization ability than standard approaches such as weight decay.", "startOffset": 36, "endOffset": 40}, {"referenceID": 16, "context": "When training samples are scarce, statistical learning theory predicts overfitting to the training data [17].", "startOffset": 104, "endOffset": 108}, {"referenceID": 7, "context": "We recall from [8] that the generalization error is (almost surely) bounded by the empirical Rademacher complexity [2], of the loss function.", "startOffset": 15, "endOffset": 18}, {"referenceID": 1, "context": "We recall from [8] that the generalization error is (almost surely) bounded by the empirical Rademacher complexity [2], of the loss function.", "startOffset": 115, "endOffset": 118}, {"referenceID": 17, "context": "Dropconnect, [18], is a recent method for regularizing large fully connected layers within neural networks.", "startOffset": 13, "endOffset": 17}, {"referenceID": 17, "context": "the Rademacher complexity of the linear feature extraction component by a multiplicative factor that is determined by the classifier [18].", "startOffset": 133, "endOffset": 137}, {"referenceID": 17, "context": "Hence we follow [18] and focus on the Rademacher complexity of the linear feature extraction.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "Theorem 2 provides an upper bound on complexity that is not very sensitive to the number of layers in the network, in contrast to weight decay where complexity is exponential in the number of layers [20].", "startOffset": 199, "endOffset": 203}, {"referenceID": 17, "context": "Generalization error differs from empirical Rademacher complexity by a multiplicative factor that is determined by the overall softmax classifier [18].", "startOffset": 146, "endOffset": 150}, {"referenceID": 6, "context": "It is designed to learn attributes that are present in data samples, in contrast to weight decay, Dropout [7], and Dropconnect [18] which are designed to prevent learning non-attributes (overfitting to random error or noise).", "startOffset": 106, "endOffset": 109}, {"referenceID": 17, "context": "It is designed to learn attributes that are present in data samples, in contrast to weight decay, Dropout [7], and Dropconnect [18] which are designed to prevent learning non-attributes (overfitting to random error or noise).", "startOffset": 127, "endOffset": 131}, {"referenceID": 6, "context": "Table 2 specifies the network architecture (similar to [7]), all images are mean-subtracted in a preprocessing step, and the graph weights W used in GraphConnect-All are computed in the same fashion as for the MNIST experiment.", "startOffset": 55, "endOffset": 58}, {"referenceID": 12, "context": "More sophisticated preprocessing methods such as contrast normalization [13] and ZCA whitening [9] will reduce intra-class variation, and we would expect them to improve performance further.", "startOffset": 72, "endOffset": 76}, {"referenceID": 8, "context": "More sophisticated preprocessing methods such as contrast normalization [13] and ZCA whitening [9] will reduce intra-class variation, and we would expect them to improve performance further.", "startOffset": 95, "endOffset": 98}, {"referenceID": 13, "context": "Impressive verification accuracies are possible when deep neural networks are able to train on extremely large labeled training sets [14, 16].", "startOffset": 133, "endOffset": 141}, {"referenceID": 15, "context": "Impressive verification accuracies are possible when deep neural networks are able to train on extremely large labeled training sets [14, 16].", "startOffset": 133, "endOffset": 141}, {"referenceID": 2, "context": "We adopt the experimental framework used in [3], and train a deep network on the WDRef dataset, where each face is described using a high dimensional LBP feature (available at 2) that is reduced to a 5,000-dimensional feature using PCA.", "startOffset": 44, "endOffset": 47}, {"referenceID": 13, "context": "The WDRef dataset is significantly smaller than the proprietary datasets in [14, 15, 16].", "startOffset": 76, "endOffset": 88}, {"referenceID": 14, "context": "The WDRef dataset is significantly smaller than the proprietary datasets in [14, 15, 16].", "startOffset": 76, "endOffset": 88}, {"referenceID": 15, "context": "The WDRef dataset is significantly smaller than the proprietary datasets in [14, 15, 16].", "startOffset": 76, "endOffset": 88}, {"referenceID": 15, "context": "For example, [16] uses 4.", "startOffset": 13, "endOffset": 17}, {"referenceID": 13, "context": "[14] and [15] use 202,599 labeled faces from 10,177 individuals, while WDRef contains 2,995 subjects with only about 30 samples per subject, clearly a much more challenging task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[14] and [15] use 202,599 labeled faces from 10,177 individuals, while WDRef contains 2,995 subjects with only about 30 samples per subject, clearly a much more challenging task.", "startOffset": 9, "endOffset": 13}, {"referenceID": 2, "context": "Our focus is the expressiveness of the learned feature, so we do not employ advanced verification methods such as those used in [3] (those will make the study of", "startOffset": 128, "endOffset": 131}], "year": 2015, "abstractText": "Deep neural networks have proved very successful in domains where large training sets are available, but when the number of training samples is small, their performance suffers from overfitting. Prior methods of reducing overfitting such as weight decay, Dropout and DropConnect are data-independent. This paper proposes a new method, GraphConnect, that is data-dependent, and is motivated by the observation that data of interest lie close to a manifold. The new method encourages the relationships between the learned decisions to resemble a graph representing the manifold structure. Essentially GraphConnect is designed to learn attributes that are present in data samples in contrast to weight decay, Dropout and DropConnect which are simply designed to make it more difficult to fit to random error or noise. Empirical Rademacher complexity is used to connect the generalization error of the neural network to spectral properties of the graph learned from the input data. This framework is used to show that GraphConnect is superior to weight decay. Experimental results on several benchmark datasets validate the theoretical analysis, and show that when the number of training samples is small, GraphConnect is able to significantly improve performance over weight decay.", "creator": "LaTeX with hyperref package"}}}