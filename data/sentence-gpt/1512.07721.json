{"id": "1512.07721", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Dec-2015", "title": "Measuring pattern retention in anonymized data -- where one measure is not enough", "abstract": "In this paper, we explore how modifying data to preserve privacy affects the quality of the patterns discoverable in the data. For any analysis of modified data to be worth doing, the data must be as close to the original as possible. Therein lies a problem -- how does one make sure that modified data still contains the information it had before modification? This question is not the same as asking if an accurate classifier can be built from the modified data.\n\n\n\nIf we consider the following data to be the right fit, then how do we make sure that the original data is correct?\nWe can test the following:\n(1) The original data is not the same as the original data. The original data may not be the same as the original data. The original data may not be the same as the original data. (2) The original data may not be the same as the original data. (3) A full-scale validation procedure is required to validate the original data for the first time. (4) A fully-scale validation procedure is required to validate the original data for the first time. (5) An analysis of the original data will be required to evaluate the original data for the first time. (6) A full-scale validation procedure is required to validate the original data for the first time. (7) The modified data must be a full-scale validation procedure. (8)\nFor the first time, each modified data must be the same as the original data. The original data must be the same as the original data.\nThe original data must be the same as the original data. (9) The original data may not be the same as the original data. (10) An accurate test of a valid validation procedure will be required to evaluate the original data for the first time.\nFinally, all the changes in the original data must be the same as the original data. (11) All the changes in the original data must be the same as the original data. (12) This problem is not the same as asking whether modified data is correct when changes are made in the original data. (13) The original data must be the same as the original data.\nTo make the test more detailed, we will do a test with a single example which uses the standard version of the original data. For example, we would use a method from the standard version of the original data to determine the correct version of the original data.\nA complete test of a valid validation procedure will", "histories": [["v1", "Thu, 24 Dec 2015 05:36:02 GMT  (418kb)", "http://arxiv.org/abs/1512.07721v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["sam fletcher", "md zahidul islam"], "accepted": false, "id": "1512.07721"}, "pdf": {"name": "1512.07721.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Sam Fletcher", "Md Zahidul Islam"], "emails": ["safletcher@csu.edu.au", "zislam@csu.edu.au"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 2.\n07 72\n1v 1\n[ cs\n.A I]\n2 4\nD ec\nIn this paper, we explore how modifying data to preserve privacy affects the quality of the patterns discoverable in the data. For any analysis of modified data to be worth doing, the data must be as close to the original as possible. Therein lies a problem \u2013 how does one make sure that modified data still contains the information it had before modification? This question is not the same as asking if an accurate classifier can be built from the modified data. Often in the literature, the prediction accuracy of a classifier made from modified (anonymized) data is used as evidence that the data is similar to the original. We demonstrate that this is not the case, and we propose a new methodology for measuring the retention of the patterns that existed in the original data. We then use our methodology to design three measures that can be easily implemented, each measuring aspects of the data that no pre-existing techniques can measure. These measures do not negate the usefulness of prediction accuracy or other measures \u2013 they are complementary to them, and support our argument that one measure is almost never enough.\nKeywords: Machine Learning, Data Mining, Patterns, Rules, Utility Measures, Privacy 2015 MSC: 00-01, 99-00\n\u2217Corresponding author Email addresses: safletcher@csu.edu.au (Sam Fletcher), zislam@csu.edu.au (Md\nZahidul Islam)\nPreprint submitted to Knowledge-Based Systems December 25, 2015"}, {"heading": "1. Introduction", "text": "When data contains information about people, preserving the privacy of those people is often an important concern. For example, government legislation might require a minimum level of anonymization (i.e. de-identification) of any data accessible to parties who were not given explicit permission by the individuals the data describes. Alternatively, individuals may refuse to provide their data if strong privacy guarantees are not made. Of course, the point of collecting the data in the first place is usually to discover interesting and useful patterns, and so the preservation of privacy needs to be done in a way that also preserves the utility of the data. While discovering patterns can be done manually using expert domain knowledge, the size and complexity of modern datasets has led to increasing reliance on data mining techniques. Modern datasets that contain information about people include medical data, financial data, social data, and law-enforcement data, among others. They are also often very large, sometimes containing data about hundreds of thousands of individuals. Data mining techniques such as decision forests [1, 2], association rule mining [3] and frequent pattern mining [4] are applied to these datasets in order to extract patterns, where the patterns are usually in the form X \u2192 y. X is a collection of antecedents (i.e. conditions or requirements) that when true for a record r (i.e. instance or tuple), predict that a consequent (i.e. label or class value) y is also true for that record. Usually, each record is a collection of attributes A (i.e. features) describing an individual person, and the antecedents in X will contain conditions for specific values v \u2208 a of attributes a \u2208 A that some records will meet and others will not.\nIn order to preserve privacy, the data can be modified in a variety of ways depending on the how the data is accessible to parties beyond the data owners (henceforth referred to as \u201cthe public\u201d). If the data owners are fully releasing the data to the public after some modification, noise might be added to each individual\u2019s values in a way that maintains the overall distribution of values while hiding the original values of any single individual. Common techniques\ninclude additive noise [5, 6, 7] and multiplicative noise [8]. Alternatively, groups of values could be \u201cgeneralized\u201d to a single value, making values that were once different indistinguishable from each other. This is the approach k-anonymity [9] and its sibling techniques (e.g. l-diversity [10]) use. If the data is remaining under the control of the data owner and the public is merely allowed to query the data, output perturbation can be used to modify the results of the individual queries. Differential privacy [11, 12, 13] is the most well-known technique to use this approach. Differential Privacy can also be used to generate a synthetic dataset, where new records are created using information from the original dataset. Henceforth in the paper we will be referring to \u201cmodified\u201d versions of the original data, but synthetic data is an equally valid application of our proposal. Regardless of the method used to modify the data, the aim is the same: to protect the privacy of each individual in the data, without destroying the utility of the data.\nSome degradation of dataset D\u2019s utility is unavoidable though, since the data is no longer the same after anonymization has occurred \u2013 the data is less truthful by definition. This is known as the privacy-utility trade-off, and optimizing this trade-off is key to a successful privacy-preservation technique. In order to assess the utility of a dataset modified to preserve privacy, it is currently common practice [14, 15, 7] to use a variety of data mining techniques (such as decision forests) to discover patterns in the modified dataset M , and then see if those patterns can correctly predict the labels of future records. \u201cFuture records\u201d are simulated by excluding some (unmodified) records from the data mining process. Since the labels of these excluded records are already known, the user can tell if the patterns discovered in M predicted the labels correctly. Records used in this way are often known as the \u201ctest dataset\u201d, T [16]. The Prediction Accuracy of the patterns discovered in M can then be compared to the Prediction Accuracy of the patterns discovered in the original dataset D. Other measures such as F-measure [17] and AUC [18] can be similarly used.\nHowever this approach of measuring utility has a shortcoming: it cannot tell the user if the patterns discovered in M are the same patterns discovered in D.\nThere is no way of knowing if the modifications made to D caused the original patterns to change (or disappear), or if weaker patterns became strong enough to become more prominent. We propose a solution to this shortcoming of the traditional approach: a methodology for measuring the retention of the original patterns in M . We implement our proposed methodology with three straightforward measures of pattern retention, but they are by no means exhaustive. What makes a pattern useful to a user can vary wildly depending on their needs, and it would be misguided to blindly apply a \u201cone size fits all\u201d measure to all privacy preservation scenarios, devoid of context."}, {"heading": "1.1. Problem Statement", "text": "In this paper, we frame the problem from the perspective of the data owner, where the data owner is wishing to release their data to the public in a way that protects the privacy of each individual in the data. They do not wish to secure and maintain a server that outputs perturbed results to queries asked by the public, nor do they want to define groupings of values for each of the attributes present in the data. In this paper we will focus on the scenario in which noise is added to the values of the individuals. We use both continuous\n(i.e. numerical) and discrete (i.e. categorical) data. Note that the specific method of anonymization is not the focus, rather the focus is on how to measure the utility of the data once the anonymization method has been used.\nThe problem the data owner is facing is how to know how much degradation is occurring in when privacy-preservation techniques are applied to their data. They can use data mining techniques to find patterns in the modified data M and see if those patterns are accurate, but there is no way to tell if the patterns are the same as the patterns that could be found in the original data D.\nWhich patterns the data owner deems important enough to monitor is outside the scope of this paper. What makes a pattern valuable can vary depending on the needs of the user, and measures have been developed to assess different aspects of patterns, such as a pattern\u2019s support or coverage [20, 21], confidence [22], conciseness [23], peculiarity [24], or many other aspects depending on the user\u2019s needs [25, 26, 27, 28]. These measures are often collectively referred to as interestingness measures. How the patterns are discovered is also outside of the scope of this paper \u2013 any patterns in the form X \u2192 y are applicable to the solution proposed in this paper. In our experiments, we arbitrarily use the CART decision tree algorithm [29] to generate a collection of patterns. The number of patterns that are monitored can be as high as the data owners likes.\nWe define a dataset D as a two-dimensional table made up of independent rows, each defined by the values it possesses in each column. Each row represents a record r \u2208 D, and each column represents an attribute a \u2208 A. Each a is made up of its own set of values, with each r possessing one value av per a, written as ra = av; \u2200a \u2208 A. When D is modified to preserve privacy, we denote the modified dataset as M . A dataset made up of records excluded from D for testing purposes is symbolized as T .\nA \u201cpattern\u201d can be defined as a rule X \u2192 y \u2208 Y , where X is an antecedent, representing a set of conditions in the form a = av that when met, predicts that a consequent Y will equal y [16]. Y is an attribute that has been chosen as the consequent (i.e. class attribute or label) of the pattern. If a record r meets\nevery condition in X (i.e. r \u2208 \u03c3X(D)) 1, it is predicted to have a label Y = y either with certainty or with some probability (i.e. 0 \u2264 P (rY = y) \u2264 1). Both a and Y can be either continuous or discrete; if continuous, other operators can be used such as a > av and Y < y. Note that we abuse notation and simplify X \u2192 y to just X when it is clear from context that we mean the whole pattern.\nWe refer to the set of all patterns {Xi; \u2200i} discovered in a dataset D as ZD. If ZD is used to predict Y for all records in T , we write the average accuracy of these predictions (that is, the \u201cPrediction Accuracy\u201d) as \u03b1(ZD|T ). In words, this can be read as \u201cThe accuracy of ZD at correctly predicting class values of records in T \u201d. Some examples of patterns can be seen in Table 1, including the probability of r having a label y. Our proposed methodology and its implementations are independent of methods for discovering patterns \u2013 any patterns in the form X \u2192 y are applicable, regardless of whether they were manually found, found with a decision tree or via association rule mining or frequent pattern mining, or any other method."}, {"heading": "1.2. Our Contribution", "text": "Our contribution can be summarized as follows:\n\u2022 We propose a novel methodology for measuring the pattern retention of a\ndataset after it has been modified (or had synthetic data generated from it) with a privacy-preservation technique.\n\u2022 We implement and test three measures that use our methodology and\ndemonstrate their sensitivity to changes in pattern retention.\n\u2022 Using a thought experiment, we demonstrate that other pre-existing mea-\nsures are not sensitive to changes in pattern retention, while our measures are.\n1Read \u03c3 as the mathematical symbol for selection. For example, \u03c3p(q) is the subset of elements in q for which p is true. p can either be a statement such as Y = y, or a set of statements such as X, in which case all statements in set p must be true for an element in q in order for that element to be in the set \u03c3p(q).\n\u2022 We also provide a correlation matrix of our three measures and three pre-\nexisting measures, showing that there is almost no correlation between the performance of a classifier built from the modified data, and the retention of the original patterns. This demonstrates that no single measure can be expected to inform the user (e.g. the data owner) about every change in the data after anonymization, and that our methodology captures information that no pre-existing measure does.\nWe also make the code for our three implementations of our proposed method-\nology available online.2\nIn Sect. 2 we propose a generalized methodology for measuring the retention of patterns in modified data. In Sect. 3 we present three implementations of the proposed methodology. In Sect. 4 we discuss three pre-existing utility measures. In Sect. 5 we use a thought experiment to explore our measures alongside preexisting measures. In Sect. 6 we detail our experiments, and in Sect. 7 we present our empirical findings. We summarize our thoughts and conclude the paper in Sect. 8."}, {"heading": "2. A Methodology for Measuring Pattern Retention", "text": "While Prediction Accuracy is an excellent measure when evaluating the utility of a classifier or model [30, 31], care needs to be taken when extending its use to the privacy-preservation domain. It has been common in the past for privacy-preservation techniques to have their effect on the quality (utility) of the data measured with prediction accuracy [14, 32, 15, 7]. This necessitates applying a data mining technique to the anonymized data M to build a classifier, or discovering a collection of patterns with another technique, and then testing the ability for that collection of patterns3 ZM to accurately predict the\n2The code can be found at http://csusap.csu.edu.au/\u223czislam/ or you can email us. 3Note that a classifier is semantically the same thing as a collection of patterns if it can\nbe broken down into antecedents and consequents.\nclass value of records in a testing dataset T . The accuracy of ZM can then be compared to the accuracy of ZD (i.e. \u03b1(ZD|T )\u2212 \u03b1(ZM |T )), and the difference is considered to be how much the privacy-preservation technique has affected the data. See Fig. 1 for a graphical representation of the data and classifiers used in this discussion.\nWe see two problems with this current methodology:\n(1) It only tells the user if the particular technique used to find the patterns\nin M produced a good classifier / model / list of patterns. Perhaps some\namount of implicit assumptions can be made about the ability of other techniques to perform similarly well (\u201ctechnique f did well, so techniques g and h probably produce similar results\u201d), but there is by no means anything explicitly said by one ZM about the universal quality of all data mining techniques applied to M .\n(2) The user cannot tell if the patterns in ZM are the same patterns that can be\ndiscovered in D (such as the patterns that would be discovered if the same data mining technique was applied to D, producing ZD). Some patterns might still be there, while others might not, and the patterns that are still there might have changed in any number of ways (such as changes in the support or confidence of the pattern, or the values av used in the conditions in X).\nA solution to (1) is for the data owner to build a ZM and ZD with every possible data mining technique they think is worth checking [14]. This solution is extremely computationally expensive, and does not address (2). If this solution is not used, then a user must either trust the implicit assumption that other data mining techniques will perform similarly, or release the collection of patterns ZM that they did test, and not release M to the public at all [14]. To the best of our knowledge, aside from our preliminary investigation [32], no solution to (2) currently exists in the literature.4\nWe therefore propose a methodology that addresses both problems. Rather than discovering a collection of patterns in M (i.e. ZM ) that may or may not have any relation to the patterns in D (i.e. ZD), we propose that the data owner defines a collection of patterns found in D and evaluates if the data in M still follows those patterns. No ZM needs to be computed in order to measure the pattern retention of M . Nor is T required, unless it is desired for other, unrelated testing.\nThis methodology removes the problem described in (1), since there is no\n4This paper is an extension of that investigation, not previously published in a journal.\nlonger any data mining technique being applied to M . It also solves (2) since the same patterns that were found in D are being used to evaluate M .\nThe next natural question is: how exactly do we evaluate M with ZD? There are many potential implementations of our proposed methodology, but we provide three examples below in Sect. 3. The first measure (in Sect. 3.1) evaluates how much of ZD as a whole has been retained in M . The next two measures (in Sect. 3.2 and Sect. 3.3) evaluate the presence of each pattern in ZD (i.e. Xi; \u2200i) separately, offering the data owner the ability to check for changes in individual patterns, as well as seeing the average change. Other implementations can easily be designed to meet the needs of the data owner. Every dataset has its own nuances, and it is usually advantageous to take those nuances into account when measuring the effect of privacy-preservation techniques, rather than trying to use a \u201ccatch-all\u201d approach. The release of data to the public will be a onetime event (once it\u2019s out there, there\u2019s no taking it back!), and so spending additional resources to properly evaluate D and M is likely worth it."}, {"heading": "3. Implementations of the Methodology", "text": ""}, {"heading": "3.1. Pattern Accuracy", "text": "Introduced by us in a 2014 conference [32] and not published in a journal until now, Pattern Accuracy is a simple measure that compares D and M . Like its name might suggest, it is very similar to Prediction Accuracy in that it measures the average accuracy of a collection of patterns at predicting the class value Y of some data. However instead of predicting the class value of some testing data T , it predicts the class values of the modified data M . If the Prediction Accuracy of M is written as \u03b1(ZM |T ), then the Pattern Accuracy of M is written as \u03b1(ZD|M). In a privacy-preservation scenario the point is to compare M \u2019s performance to D, so Prediction Accuracy becomes \u03b1(ZD|T )\u2212 \u03b1(ZM |T ), and the Pattern Accuracy equivalent is therefore \u03b1(ZD|D) \u2212 \u03b1(ZD|M). 5 Note\n5Remember that M is a modified version of D, with each record in M corresponding to an\nunaltered version in D\nthat while \u03b1(ZD|D) should not be used to assess the quality of a classifier due to the risk of of overfitting, in this paper we are not concerned with the generality of the patterns. How ZD is created or defined is outside the scope of this paper. Instead, the difference between \u03b1(ZD|D) and \u03b1(ZD|M) tells us the difference in the number of records that are contributing to the prediction made by each pattern (where the prediction is the majority class label). Pattern Accuracy is a way of measuring the presence of D\u2019s patterns in M ; if \u03b1(ZD|D)\u2212 \u03b1(ZD|M) is close to zero, then the user knows that a similar number of records in D and M are contributing to the correct predictions made by the classifier built from D. Since the records in M are just modified versions of the records in D, this is a valuable thing to know! If \u03b1(ZD|D) \u2212 \u03b1(ZD|M) is closer to one, the user knows that the records were modified in a way that reduced the presence of the patterns found in ZD. If \u03b1(ZD|D) \u2212 \u03b1(ZD|M) is negative, this is actually just as bad as positive result of similar magnitude, because D is trusted data \u2013 any random modifications made to D is further from the trusted data by definition, even if some quality metrics increase. Ideally we want every pattern in ZD to be just as prevalent in M as it is in D; no more, no less. Thus we define Pattern Accuracy as:\nPattern Accuracy = |\u03b1(ZD|D)\u2212 \u03b1(ZD|M)| . (1)\nPattern Accuracy evaluates whether ZD, as a whole, can correctly predict Y\nfor records in M . Since it uses an identical process to Prediction Accuracy (with the user simply having to redirect the measure to check M rather than T ), it gains all of the benefits of Prediction Accuracy such as low computation time and conceptual simplicity. What it does not do, however, is evaluate the presence of each pattern individually (i.e. Xi; \u2200i). There are almost always multiple patterns that predict the same y \u2208 Y , so it is possible that some patterns no longer have records in M that follow them (and instead those records follow different patterns) without the Pattern Accuracy result changing. As long as the record\u2019s new pattern still correctly predicts y, the Pattern Accuracy measure\nis insensitive to this change. The following two measures avoid this insensitivity by evaluating the pattern retention in M on a per-pattern basis, rather than evaluating the entire pattern list as a whole."}, {"heading": "3.2. Pattern Support Distance (PSD)", "text": "The \u201csupport\u201d of a pattern is the number of records in a dataset that a pattern covers [21, 22], and can be represented as |\u03c3X(D)| when describing the support of patternX in datasetD. Whether Y is predicted correctly is irrelevant when measuring support. To measure the support for X in D compared to M , we can calculate |\u03c3X(D)| and |\u03c3X(M)|. By comparing these results, a user knows how much the presence of an individual pattern X has changed due to the modifications made to D (resulting in M). This level of granularity allows the user to use their domain knowledge to make specific assessments of the status of each X . This can naturally be repeated for all X \u2208 ZD. To summarize the overall support retention of ZD for a dataset M , the mean difference can be calculated:\nPSD = 1\n|ZD| \u00d7 |D|\n\u2211\nX\u2208ZD\n| |\u03c3X(D)| \u2212 |\u03c3X(M)| | . (2)\nNote that each pattern contributes equally to the mean difference. Patterns with higher support are not assumed to be more important, since each X in ZD should have already been assessed by the user as being important enough to worry about preserving in M , and we are now only interested in if the original support has changed. It should be noted though that patterns in ZD with very low support cannot reduce in size by as much as patterns with high support \u2013 support cannot go below 0 \u2013 so the presence of many patterns with low support risks \u201cdiluting\u201d PSD.6 However it is normal for such small patterns to be considered as idiosyncrasies ofD, and not generalizing to future records (such\n6This effect is caused whenever many small differences are averaged alongside several large differences. The presence of near-zero numbers effectively reduces the average, diluting the larger differences. There is nothing inherently wrong with this, but it is usually undesirable.\nas T ), and so most data mining algorithms automatically remove them from ZD [16]. This is sometimes referred to as the \u201cminimum support threshold\u201d.\nPSD (Pattern Support Distance) has a defined lower and upper limit of 0 \u2264 PSD \u2264 1, allowing for an intuitive interpretation of the result, such as: \u201cThe average percentage change in the prevalence of a pattern when modifying D into M\u201d.\nThe aim of privacy preservation is to (1) make any individual record difficult to identify, while (2) leaving the patterns as unaffected as possible [5, 33, 7]. The pattern support difference (PSD) of M accurately measures the second half of this aim, allowing researchers to make more informed assessments of the overall success of a privacy preservation technique. The specific records that matched each X in D is irrelevant \u2013 X will still be just as prevalent in M as it was in D if other records take the place of the records that no longer follow X . In order for a record to change which pattern it matches, its values must have changed during the modification process enough for it to legitimately meet the conditions of a different pattern.\nWhile Pattern Accuracy somewhat measures the presence (support) of Xi \u2208 ZD; \u2200i in M , PSD does so with precision, removing any uncertainty about the presence of each pattern in M ."}, {"heading": "3.3. Pattern Label Distance (PLD)", "text": "Say a record r \u2208 D meets the conditions of a certain pattern Xi \u2208 ZD (i.e. r \u2208 \u03c3Xi(D)). When D is modified to M , it is possible that r will be changed in a way that causes it to meet the conditions of a different pattern Xj \u2208 ZD (i.e. r \u2208 \u03c3Xj (M)). If this occurs, the distribution of labels (i.e. Y ) will change for both Xi and Xj , since rY has been removed from Xi\u2019s distribution of class labels and added to Xj\u2019s. The purpose of a pattern is often to predict Y , and so it is important to know how much that prediction might have changed in M . Pattern Accuracy measures this to an extent, as Xi and Xj might predict different class labels and a maximum of one of those predictions can be correct for a record r. But it is also possible that the two patterns will predict the\nsame class label, leading to no change in the Pattern Accuracy of M compared to D (at least as far as r is concerned). The consequent of any pattern X is usually the most common class label to occur out of all the records in \u03c3X(D), with any other class labels being ignored [16]. This has the effect of making X \u2019s prediction of Y = y appear identically confident7 regardless of how high or low the frequency of y is in \u03c3X(M) compared to \u03c3X(D), as long as it remains the most frequent class label.\nTo avoid these problems, we use the Chi-squared histogram distance [34] to\nmeasure differences in the distribution of Y between \u03c3X(D) and \u03c3X(M):\n\u03c72(\u03c3X(D), \u03c3X(M)) = 1\n2\n\u2211\ny\u2208Y\n(fDy \u2212 f M y ) 2\nfDy + f M y\n, (3)\nwhere fDy is the relative frequency of the class label y in \u03c3X(D) (in other words, the fraction of records in \u03c3X(D) that have rY = y), and similarly for f M y in respect to \u03c3X(M).\nJust like with Chi-squared hypothesis testing, Chi-squared histogram distance becomes unstable if there are less than five samples. This limitation is automatically handled if a minimum support threshold for each pattern X was applied when making ZD; otherwise we recommend discounting any patterns that have less than five class labels (i.e. ignore patterns X \u2208 ZD where |\u03c3X(D)| < 5).\nEven if the majority y value in \u03c3X(D) occurs even more frequently in \u03c3X(M) (and thus has increased confidence), this should not be considered as an improvement unless the modification that created M was aiming to improve pattern utility. In scenarios such as privacy preservation, the distribution of Y for \u03c3X(D) is considered to be the ground truth. PLD (Pattern Label Distance) successfully captures this scenario, where any distance away from D is a reduc-\n7\u201cConfidence\u201d refers to the certainty or reliability of a pattern \u2013 that is, how frequent the most frequent label is [20]. If 100% of the records in a pattern have the same class label, then that pattern can be considered highly reliable.\ntion in utility by definition. The mean Chi-squared histogram distance of all patterns in ZD can then be easily calculated:\nPLD = 1\n|ZD|\n\u2211\nX\u2208ZD\n\u03c72(\u03c3X(D), \u03c3X(M)) . (4)\nIt should be noted that Chi-squared histogram distance is invariant to the number of records [34], and so the support of a pattern (both in D and M) does not affect the result. If the support of each pattern is deemed relevant by the user, |\u03c3X(D)| can easily be taken into account as well. We do not recommend combining a pattern\u2019s support difference and label distribution distance into a single result, as the results are likely to be far more informative when separate. This is true for both single patterns and the mean results (PSD and PLD). Chi-squared histogram distance is also invariant to the number of labels, so it is not restricted to datasets or patterns with a particularly sized Y . This is often a concern with popular measures such as AUC [18] and F-measure [17], where non-binary class attributes need to be treated with care [35]."}, {"heading": "4. Related Utility Measures", "text": "As mentioned in Sect. 1, it is common in the literature for a privacypreserving technique\u2019s impact on data utility to be quantified using Prediction Accuracy: that is, by comparing \u03b1(ZM |T ) to \u03b1(ZD|T ) [16]. Other common measures are F-measure [17] and AUC [18], where again ZM is compared to ZD using T (refer to Fig. 1 for a graphical representation of these data and classifiers). We use Prediction Accuracy, F-measure and AUC in our experiments.\nFormally, Prediction Accuracy can be written as:\n\u03b1(ZD|T ) = 1\n|T |\n\u2211\nr\u2208T\n1(rY = ypredicted) , (5)\nwhere ypredicted is the class value y \u2208 Y that ZD predicts r to have, and rY is r\u2019s actual class value. The indicator function, 1(x), returns 1 if x is true; otherwise 0. Thus, Prediction Accuracy is the fraction of records that have their class label correctly predicted.\nAUC and F-measure are most appropriate when Y is binary (i.e. |Y | = 2), and where y1 \u2208 Y is the \u201cimportant\u201d class label, or the class label that the user is trying to correctly predict, and y2 \u2208 Y is unimportant. These labels are often referred to as the \u201cpositive\u201d and \u201cnegative\u201d labels, respectively. A \u201cTrue Positive\u201d (TP) label is therefore a label that was correctly predicted to be positive, a \u201cFalse Positive\u201d (FP) is a label that was predicted to be positive but was not, and similarly for \u201cTrue Negative\u201d (TN) and \u201cFalse Negative\u201d (FN).8\nF-measure [17] can be formally written in terms of precision ( TP TP+FP ) and\nrecall ( TP TP+FN ) as:\nF\u03b2 = (1 + \u03b2 2)\u00d7\nprecision \u00d7 recall\n\u03b22 \u00d7 precision+ recall , (6)\nwhere \u03b2 is very often equal to 1, so that recall and precision have equal weighting. We use \u03b2 = 1 in our experiments.\nMeanwhile, AUC [18] is shorthand for \u201cArea under the ROC curve\u201d, with \u201cROC\u201d in turn being short for \u201cReceiver Operating Characteristic\u201d. The ROC curve describes the trade-off between TP (benefits) and FP (costs). Often it is plotted on axes with the TP Rate ( TP TP+FN ) as the y axis and the FP Rate ( FP FP+TN ) as the x axis. Thus, AUC is the area under this curve. It represents the probability that ZD is more likely to predict a positive label as positive than to predict a negative label as positive. It has become popular in the machine learning community as of late, despite some problems it has when comparing different classifiers [36, 37]."}, {"heading": "5. A Thought Experiment", "text": "We use a thought experiment to demonstrate the sensitivity of our measures to changes in the data that are not detected by pre-existing measures. We will use the toy data and classifiers seen in Fig. 1. The patterns in ZD have\n8Using this notation, we can actually re-write Prediction Accuracy as TP+TN TP+TN+FP+FN .\nbeen written out in Table 2, along with their support and confidence. After modifying D with a privacy-preservation technique, the result is M as seen in Fig. 1. The classifier ZM was then made from that modified data; we present the patterns in Table 3. We then assess the quality of M using six measures: our three implementations of our proposed methodology, as well as Prediction Accuracy, AUC and F-measure. The results are tabulated in Table 4.\nSeveral things have happened here. Firstly, Prediction Accuracy was completely incapable of detecting any changes in M compared to D. It is possible that the user does not actually care that M is different, and is only interested in being able to make good predictions on future data (and that is fine). However, if the user makes any assumption about the similarity between M and D with"}, {"heading": "D 0.00 0.00 0.00 0.67 0.67 0.80", "text": ""}, {"heading": "M 0.50 0.08 0.34 0.67 0.50 0.67", "text": "Prediction Accuracy, they have made a very dangerous mistake. As we can see visually in Fig. 1, ZM is radically different from ZD. Due to the changes present in M , the patterns discovered in M are very different from the patterns discovered in D. Our proposed methodology solves the issue of quantifying the visual intuition one has about the differences between ZM and ZD. Pattern Accuracy, PSD and PLD were all able to accurately identify the differences between D and M that they are designed to identify: the overall retention of ZD\u2019s patterns, the changes in the patterns\u2019 support and the changes in the patterns\u2019 class label distribution, respectively.\nAUC and F-measure were able to detect some changes, but it is important to recognize that these changes do not represent any connection between ZD and ZM . Both measures started by calculating the true and false predictions of the positive and negative labels of ZD using T , and then they made similar calculations of ZM using T . At no point was ZM actually compared to ZD, except indirectly, in much the same way that Prediction Accuracy indirectly compares them. As demonstrated by Prediction Accuracy\u2019s results in Table 4 though, there is no guarantee that any of these indirect comparisons will detect any differences at all. Even if they do, how does the user use that information, except to prompt them to look at the patterns discovered with their human intuition? Pattern Accuracy, PSD and PLD offer concrete results about M \u2019s retention of D\u2019s patterns."}, {"heading": "6. Experiment Methodology", "text": "To empirically evaluate our three measures, we carry out the below experiments and present the results in Sect. 7. For our experiments, the patterns are generated using decision trees. Note that the patterns could just have easily been manually created, generated from a different classifier, filtered using any number of interestingness measures, hand-picked from a list of generated patterns, or by any other means that outputs patterns in the form X \u2192 y.\nWe use 17 datasets publicly available in the UCI Machine Learning Repository [19]. To generate a collection of patterns for each dataset (i.e. ZD), we run the CART algorithm [29], with a minimum leaf size (i.e. minimum support threshold) of |D| \u00d7 0.02 and a maximum tree depth of 12. By generating patterns in this way, we produce a set of realistic patterns for each dataset, with patterns also varying in length (i.e. the number of conditions in X). Another advantage of generating our patterns in this way is that the deterministic nature of CART allows for others to replicate our ZD\u2019s exactly.\nOur datasets range in size from 653 to 58000 records, 6 to 62 attributes, and 2 to 18 class labels, and include both numerical and categorical attributes. The number of patterns (i.e. |ZD|) ranges from 11 to 37. The details of the datasets are summarized in Table 5.\nFor experiments involving AUC and F-measure (e.g. Table 6, discussed later) we limit our experiments to datasets with binary labels, where these measures are known to work best."}, {"heading": "6.1. Privacy-Preservation Techniques", "text": "To simulate various modifications to a dataset, we add noise to the data in two simple ways. Each type of noise represents a different scenario respectively: where attribute and multi-attribute (i.e. multivariate) distributions are flattened (i.e. made more uniform); and where attribute distributions and most multi-attribute distributions are preserved. We simulate these scenarios using additive noise. Using these two scenarios, we explore what a user can learn from\nour three implementations of our proposed methodology, and how they compare to Prediction Accuracy, AUC and F-measure.\nThe two types noise addition we use are listed below. Note that these are simple toy noise addition techniques, and are not part of this paper\u2019s contribution.\nUniform Noise (UN). A user-defined percentage of values in the dataset D are changed, with the result being M . If a value ra is changed and a is a continuous attribute, the new value is selected from a uniform distribution between the minimum and maximum values of a. If a is a discrete (i.e. categorical) attribute,\nra is changed to any unique value in the set a, with each value having an equal probability of being selected. Values are randomly selected, with the original value having no effect on the new value. This has the effect of flattening the distribution of values for all attributes, as well as flattening all multivariate distributions.\nGaussian Noise (GN). A user-defined percentage of values in the dataset D are changed, with the result being M . If a value ra is changed and a is a continuous attribute, a random number is selected from a Gaussian distribution with a mean of zero and a variance equal to a\u2019s variance, and added to ra. If a is a discrete (i.e. categorical) attribute, ra is changed to a value that is randomly selected from a\u2019s original set of values (including repeated values). GN therefore maintains the distribution of values for both numerical and categorical attributes. Additionally, continuous values are changed in a way that takes into account the original value. This means that each record\u2019s continuous values are likely to remain close to their original values, and thus the multivariate distribution of the dataset is likely to be preserved.\nNeither of these noise types add noise to the label Y . For each type of noise, we increase the percentage chance of changing a value in 2% increments, from 0% to 30%. For each increment, for each type of noise, for each dataset, we use 10-fold cross-validation iteratively 10 times (for a total of 100 tests), creating a new set of patterns ZD each time. For each test, we collect the result of six measures: Pattern Accuracy, PSD, PLD, Prediction Accuracy and AUC and F-measure."}, {"heading": "6.2. Pearson\u2019s Correlation Coefficient", "text": "By comparing the results of each measure as noise increases, for each dataset, we calculate the measures\u2019 correlation to each other using Pearson\u2019s correlation coefficient (i.e. Pearson\u2019s r value) [38]. We calculate their correlation for each noise type separately. The coefficient has a range of \u22121 \u2264 r \u2264 1, where a result close to 1 indicates a high positive correlation (as one measure increases, so does\nthe other measure), a result close to -1 indicates a high negative correlation (as one measure increases, the other decreases), and a result close to 0 indicates low correlation (the result of one measure has little bearing on the result of the other). To standardize the results of different datasets, we look at the difference between each measure\u2019s result on M compared to D. In other words, for each level of noise, we subtract the result that the measure achieved when there was zero noise. This has no effect on our proposed measures (which always equal 0 when there is no noise), and simply causes Prediction Accuracy, AUC and F-measure to be reported as the difference between the \u201ctrue\u201d result and the \u201cnoisy\u201d result."}, {"heading": "7. Results", "text": "To demonstrate the information a user can learn about individual pattern retention, Fig. 2 presents the support and Chi-squared histogram distance of the example patterns shown in Table 1, as UN increases. In this example, we can see that the four patterns are affected quite differently by the noise addition. Some of the observations a data scientist could make about these four patterns are:\n\u2022 X3 has gone from representing over 8000 of the of 30162 records in Adult\nto representing only 3000 records in the modified version of Adult by the time UN has reached 30%.\n\u2022 Despite this massive change in support, the distribution of the class labels\nin X3 is actually almost exactly the same at all noise levels.\n\u2022 The same cannot be said for X2, where a massive change in support (from\nless than 1000 to roughly 4000) has been accompanied by a massive change in the distribution of class labels as well.\n\u2013 If this observation caused the user to investigate further, they would\nfind that X2\u2019s change in label distribution was enough to completely\nflip the prediction the pattern is making! At 30% noise, the reported probability of a record having each label is Pr(Income \u2264 $50, 000) = 0.68, Pr(Income > $50, 000) = 0.32, compared to the probabilities shown in Table 1: Pr(Income \u2264 $50, 000) = 0.05, Pr(Income > $50, 000) = 0.95. It would be incredibly damaging to any analysis performed with M if the data analyst trusted this pattern.\n\u2022 X0 and X1 represent a much smaller proportion of the Adult dataset,\nand have undergone moderate changes in support. X0 has grown larger, while X1 has become smaller, but neither saw enough change in label distribution to cause concern.\n\u2022 These patterns in Adult were discovered with a decision tree, along with\n31 other patterns that underwent a variety of changes in support and label distribution similar to the changes shown in Fig. 2.\nAfter averaging the support distance and Chi-squared histogram distance of all patterns and thus calculating PSD and PLD, we can compare their assessments of M \u2019s pattern retention for each dataset. We also compare PSD and PLD to the assessment made by Pattern Accuracy. For each dataset, we present the results of PSD, PLD and Pattern Accuracy as UN increases in Fig. 3. Note that for Pattern Accuracy, we present the percentage of cases where ZD incorrectly predicts the label of records in M so that lower values signify better pattern retention for all three measures.9 As more noise is added, we observe that all three measures trend up as expected, but upon closer inspection we can see that they do not do so at identical rates."}, {"heading": "7.1. Correlations between utility measures", "text": "The differences in trends seen in Fig. 3 are quantified by the correlations between the measures, seen in Table 6. The correlations are calculated using Pearson\u2019s correlation coefficient [38] as described in Sect. 6.2, and Prediction\n9That is, pattern accuracy error = 1\u2212 pattern accuracy.\nAccuracy, AUC and F-measure are included as well. Unlike Fig. 3, the correlations are calculated using the nine datasets with binary labels for the benefit of AUC and F-measure.10\nOne observation we can make about Table 6 is that despite all the measures using the same data, they do not always agree with each other. Just because Prediction Accuracy decreases does not mean that F-measure also decreases, for example. Another observation is that Prediction Accuracy, F-measure and AUC have very weak correlations with any of our implementations of our proposed\n10The correlations among PLD, PSD, Pattern Accuracy and Prediction Accuracy when\nusing the datasets shown in Fig. 3 are similar to those shown in Table 6.\nmethodology. This is interesting, and confirms our suspicions that just because a good classifier (that is, a classifier that achieves good results) can be made from noisy data, does not mean that the patterns in the noisy data have the same properties as the original patterns, or even that the original patterns are in the noisy data at all. For example if a user observed a particular amount of Prediction Accuracy loss after modifying D to M , there is no way to tell how much the support of the patterns in D might have changed."}, {"heading": "8. Discussion", "text": "None of our proposed measures can tell a user if a good classifier can be made from M . They are not trying to! If a user wishes to learn that, they can use machine learning algorithms on M and see if the resulting classifier has good performance, using measures such as Prediction Accuracy. Doing so, however, will not tell them if those machine learning algorithms found the same patterns that existed in D. That is where our proposed methodology \u2013 and our implementations of that methodology \u2013 come in.\nPattern Accuracy, PSD and PLD should not be interpreted as exhaustively measuring all aspects of pattern retention. Rather, they are examples of quantifying specific effects a privacy-preservation technique can have on data. It is the responsibility of the data scientist performing the anonymization of D to assess what properties of a dataset are relevant or important, and then to measure how those properties might have changed after data modification or synthesization. Pattern Accuracy measures the overall retention of the original patterns; PSD measures changes in pattern support, per pattern; PLD measures changes in label distribution per pattern; other measures might focus on quantifying changes in pattern conciseness or peculiarity or any number of other properties that might make patterns interesting to a user.\nPrediction Accuracy is currently heavily relied upon in privacy-preservation research. While the measure itself is very useful, it should not be viewed as an all-encompassing measure of the quality of modified or synthetic data, but\nrather as another example of quantifying a specific property \u2013 the ability for accurate classifiers to be built using a variety of machine-learning algorithms.\nMeasuring properties of ZD in M is straightforward, both conceptually and computationally, and can be easily used in conjunction with Prediction Accuracy and other measures. It enables the user to quantify aspects ofM that previously could only be assessed with experience or intuition."}], "references": [{"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine learning ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Knowledge discovery through SysFor: a systematically developed forest of multiple decision trees", "author": ["M.Z. Islam", "H. Giggins"], "venue": "in: Ninth Australasian Data Mining Conference-Volume 121, Australian Computer Society, Inc., Ballarat, Australia", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Privacy preserving mining of association rules", "author": ["A. Evfimievski", "R. Srikant", "R. Agrawal", "J. Gehrke"], "venue": "Information Systems 29 (4) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Frequent pattern mining: current status and future directions", "author": ["J. Han", "H. Cheng", "D. Xin", "X. Yan"], "venue": "Data Mining and Knowledge Discovery 15 (1) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Privacy-preserving Data Mining", "author": ["R. Agrawal", "R. Srikant"], "venue": "in: Proceedings of the 2000 ACM SIGMOD Conference on Management of Data, Dallas, Texas", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "On the design and quantification of privacy preserving data mining algorithms", "author": ["D. Agrawal", "C. Aggarwal"], "venue": "in: Proceedings of the twentieth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems., ACM", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Privacy preserving data mining: A noise addition framework using a novel clustering technique", "author": ["M.Z. Islam", "L. Brankovic"], "venue": "Knowledge-Based Systems 24 (8) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Random projection-based multiplicative data perturbation for privacy preserving distributed data mining", "author": ["K. Liu", "H. Kargupta", "J. Ryan"], "venue": "Knowledge and Data Engineering, IEEE Transactions on 18 (1) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "k-anonymity: A model for protecting privacy", "author": ["L. Sweeney"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems 10 (05) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "ldiversity: Privacy beyond k-anonymity", "author": ["A. Machanavajjhala", "D. Kifer", "J. Gehrke", "M. Venkitasubramaniam"], "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD) 1 (1) ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Differential Privacy", "author": ["C. Dwork"], "venue": "in: Automata, languages and programming, Vol. 4052, Springer Berlin Heidelberg, Venice, Italy", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["C. Dwork", "F. McSherry", "K. Nissim", "A. Smith"], "venue": "Theory of Cryptography ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Mechanism Design via Differential Privacy", "author": ["F. McSherry", "K. Talwar"], "venue": "48th Annual IEEE Symposium on Foundations of Computer Science (FOCS\u201907) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "The cost of privacy: destruction of data-mining utility in anonymized data publishing", "author": ["J. Brickell", "V. Shmatikov"], "venue": "in: Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, ACM", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Top-down specialization for information and privacy preservation", "author": ["B. Fung", "K. Wang", "P. Yu"], "venue": "in: Proceedings of the 21st International Conference on Data Engineering, IEEE", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Data mining: concepts and techniques", "author": ["J. Han", "M. Kamber", "J. Pei"], "venue": "Morgan Kaufmann Publishers", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "The meaning and use of the area under a receiver operating characteristic (ROC) curve", "author": ["J. Hanley", "B. McNeil"], "venue": "Radiology 143 (1) ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1982}, {"title": "M", "author": ["K. Bache"], "venue": "Lichman, UCI Machine Learning Repository ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Selecting the right interestingness measure for association patterns", "author": ["P.-N. Tan", "V. Kumar", "J. Srivastava"], "venue": "in: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, Vol. 2, ACM Press, New York, USA", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Generality is predictive of prediction accuracy", "author": ["G. Webb", "D. Brain"], "venue": "in: Proceedings of the 2002 Pacific Rim Knowledge Acquisition Workshop ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Hiding Association Rules by Using Confidence and Support", "author": ["E. Dasseni", "V. Verykios", "A.K. Elmagarmid", "E. Bertino"], "venue": "in: Information Hiding, Purdue University, Springer Berlin Heidelberg", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2001}, {"title": "Small is beautiful: discovering the minimal set of unexpected patterns", "author": ["B. Padmanabhan", "A. Tuzhilin"], "venue": "in: Proceedings of the 6th ACM SIGKDD international conference on Knowledge discovery and data mining, ACM", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "Peculiarity oriented multidatabase mining", "author": ["N. Zhong", "Y. Yao", "M. Ohshima"], "venue": "IEEE Transactions on Knowledge and Data Engineering 15 (4) ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "Interestingness measures for data mining: a survey", "author": ["L. Geng", "H.J. Hamilton"], "venue": "ACM Computing Surveys 38 (3) ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Selecting the right objective measure for association analysis", "author": ["P.-N. Tan", "V. Kumar", "J. Srivastava"], "venue": "Information Systems 29 (4) ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "A clustering of interestingness measures", "author": ["B. Vaillant", "P. Lenca", "S. Lallich"], "venue": "in: Discovery Science, Springer Berlin Heidelberg", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "Measuring Information Quality for Privacy Preserving Data Mining", "author": ["S. Fletcher", "M.Z. Islam"], "venue": "International Journal of Computer Theory and Engineering 7 (1) ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Classification and regression trees", "author": ["L. Breiman", "J. Friedman", "C. Stone", "R. Olshen"], "venue": "Chapman & Hall/CRC", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1984}, {"title": "An experimental comparison of performance measures for classification", "author": ["C. Ferri", "J. Hern\u00e1ndez-Orallo", "R. Modroiu"], "venue": "Pattern Recognition Letters 30 (1) ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "A systematic analysis of performance measures for classification tasks", "author": ["M. Sokolova", "G. Lapalme"], "venue": "Information Processing &Management 45 (4) ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Quality evaluation of an anonymized dataset", "author": ["S. Fletcher", "M.Z. Islam"], "venue": "in: 22nd International Conference on Pattern Recognition, IEEE, Stockholm, Sweden", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Data Swapping: Balancing Privacy Against Precision in Mining for Logic Rules", "author": ["V. Estivill-Castro", "L. Brankovic"], "venue": "Data Warehousing and Knowledge Discovery (DaWaK \u201999) ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1999}, {"title": "The quadratic-chi histogram distance family", "author": ["O. Pele", "M. Werman"], "venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 6312 LNCS (PART 2) ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Comparing classification results between n-ary and binary problems", "author": ["M. Felkin"], "venue": "in: Quality Measures in Data Mining, Springer Berlin Heidelberg", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "Measuring classifier performance: acoherent alternative to the area under the ROC curve", "author": ["D.J. Hand"], "venue": "Machine Learning 77 (1) ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "AUC: A misleading measure of the performance of predictive distribution models", "author": ["J.M. Lobo", "A. Jim\u00e9nez-valverde", "R. Real"], "venue": "Global Ecology and Biogeography 17 (2) ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2008}, {"title": "On lines and planes of closest fit to systems of points in space", "author": ["K. Pearson"], "venue": "The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 2 (11) ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1901}], "referenceMentions": [{"referenceID": 0, "context": "Data mining techniques such as decision forests [1, 2], association rule mining [3] and frequent pattern mining [4] are applied to these datasets in order to extract patterns, where the patterns are usually in the form X \u2192 y.", "startOffset": 48, "endOffset": 54}, {"referenceID": 1, "context": "Data mining techniques such as decision forests [1, 2], association rule mining [3] and frequent pattern mining [4] are applied to these datasets in order to extract patterns, where the patterns are usually in the form X \u2192 y.", "startOffset": 48, "endOffset": 54}, {"referenceID": 2, "context": "Data mining techniques such as decision forests [1, 2], association rule mining [3] and frequent pattern mining [4] are applied to these datasets in order to extract patterns, where the patterns are usually in the form X \u2192 y.", "startOffset": 80, "endOffset": 83}, {"referenceID": 3, "context": "Data mining techniques such as decision forests [1, 2], association rule mining [3] and frequent pattern mining [4] are applied to these datasets in order to extract patterns, where the patterns are usually in the form X \u2192 y.", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "include additive noise [5, 6, 7] and multiplicative noise [8].", "startOffset": 23, "endOffset": 32}, {"referenceID": 5, "context": "include additive noise [5, 6, 7] and multiplicative noise [8].", "startOffset": 23, "endOffset": 32}, {"referenceID": 6, "context": "include additive noise [5, 6, 7] and multiplicative noise [8].", "startOffset": 23, "endOffset": 32}, {"referenceID": 7, "context": "include additive noise [5, 6, 7] and multiplicative noise [8].", "startOffset": 58, "endOffset": 61}, {"referenceID": 8, "context": "This is the approach k-anonymity [9] and its sibling techniques (e.", "startOffset": 33, "endOffset": 36}, {"referenceID": 9, "context": "l-diversity [10]) use.", "startOffset": 12, "endOffset": 16}, {"referenceID": 10, "context": "Differential privacy [11, 12, 13] is the most well-known technique to use this approach.", "startOffset": 21, "endOffset": 33}, {"referenceID": 11, "context": "Differential privacy [11, 12, 13] is the most well-known technique to use this approach.", "startOffset": 21, "endOffset": 33}, {"referenceID": 12, "context": "Differential privacy [11, 12, 13] is the most well-known technique to use this approach.", "startOffset": 21, "endOffset": 33}, {"referenceID": 13, "context": "In order to assess the utility of a dataset modified to preserve privacy, it is currently common practice [14, 15, 7] to use a variety of data mining techniques (such as decision forests) to discover patterns in the modified dataset M , and then see if those patterns can correctly predict the labels of future records.", "startOffset": 106, "endOffset": 117}, {"referenceID": 14, "context": "In order to assess the utility of a dataset modified to preserve privacy, it is currently common practice [14, 15, 7] to use a variety of data mining techniques (such as decision forests) to discover patterns in the modified dataset M , and then see if those patterns can correctly predict the labels of future records.", "startOffset": 106, "endOffset": 117}, {"referenceID": 6, "context": "In order to assess the utility of a dataset modified to preserve privacy, it is currently common practice [14, 15, 7] to use a variety of data mining techniques (such as decision forests) to discover patterns in the modified dataset M , and then see if those patterns can correctly predict the labels of future records.", "startOffset": 106, "endOffset": 117}, {"referenceID": 15, "context": "Records used in this way are often known as the \u201ctest dataset\u201d, T [16].", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "Other measures such as F-measure [17] and AUC [18] can be similarly used.", "startOffset": 46, "endOffset": 50}, {"referenceID": 17, "context": "Table 1: A selection of patterns discovered in the \u201cAdult\u201d dataset [19].", "startOffset": 67, "endOffset": 71}, {"referenceID": 18, "context": "What makes a pattern valuable can vary depending on the needs of the user, and measures have been developed to assess different aspects of patterns, such as a pattern\u2019s support or coverage [20, 21], confidence [22], conciseness [23], peculiarity [24], or many other aspects depending on the user\u2019s needs [25, 26, 27, 28].", "startOffset": 189, "endOffset": 197}, {"referenceID": 19, "context": "What makes a pattern valuable can vary depending on the needs of the user, and measures have been developed to assess different aspects of patterns, such as a pattern\u2019s support or coverage [20, 21], confidence [22], conciseness [23], peculiarity [24], or many other aspects depending on the user\u2019s needs [25, 26, 27, 28].", "startOffset": 189, "endOffset": 197}, {"referenceID": 20, "context": "What makes a pattern valuable can vary depending on the needs of the user, and measures have been developed to assess different aspects of patterns, such as a pattern\u2019s support or coverage [20, 21], confidence [22], conciseness [23], peculiarity [24], or many other aspects depending on the user\u2019s needs [25, 26, 27, 28].", "startOffset": 210, "endOffset": 214}, {"referenceID": 21, "context": "What makes a pattern valuable can vary depending on the needs of the user, and measures have been developed to assess different aspects of patterns, such as a pattern\u2019s support or coverage [20, 21], confidence [22], conciseness [23], peculiarity [24], or many other aspects depending on the user\u2019s needs [25, 26, 27, 28].", "startOffset": 228, "endOffset": 232}, {"referenceID": 22, "context": "What makes a pattern valuable can vary depending on the needs of the user, and measures have been developed to assess different aspects of patterns, such as a pattern\u2019s support or coverage [20, 21], confidence [22], conciseness [23], peculiarity [24], or many other aspects depending on the user\u2019s needs [25, 26, 27, 28].", "startOffset": 246, "endOffset": 250}, {"referenceID": 23, "context": "What makes a pattern valuable can vary depending on the needs of the user, and measures have been developed to assess different aspects of patterns, such as a pattern\u2019s support or coverage [20, 21], confidence [22], conciseness [23], peculiarity [24], or many other aspects depending on the user\u2019s needs [25, 26, 27, 28].", "startOffset": 304, "endOffset": 320}, {"referenceID": 24, "context": "What makes a pattern valuable can vary depending on the needs of the user, and measures have been developed to assess different aspects of patterns, such as a pattern\u2019s support or coverage [20, 21], confidence [22], conciseness [23], peculiarity [24], or many other aspects depending on the user\u2019s needs [25, 26, 27, 28].", "startOffset": 304, "endOffset": 320}, {"referenceID": 25, "context": "What makes a pattern valuable can vary depending on the needs of the user, and measures have been developed to assess different aspects of patterns, such as a pattern\u2019s support or coverage [20, 21], confidence [22], conciseness [23], peculiarity [24], or many other aspects depending on the user\u2019s needs [25, 26, 27, 28].", "startOffset": 304, "endOffset": 320}, {"referenceID": 26, "context": "What makes a pattern valuable can vary depending on the needs of the user, and measures have been developed to assess different aspects of patterns, such as a pattern\u2019s support or coverage [20, 21], confidence [22], conciseness [23], peculiarity [24], or many other aspects depending on the user\u2019s needs [25, 26, 27, 28].", "startOffset": 304, "endOffset": 320}, {"referenceID": 27, "context": "In our experiments, we arbitrarily use the CART decision tree algorithm [29] to generate a collection of patterns.", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "A \u201cpattern\u201d can be defined as a rule X \u2192 y \u2208 Y , where X is an antecedent, representing a set of conditions in the form a = av that when met, predicts that a consequent Y will equal y [16].", "startOffset": 184, "endOffset": 188}, {"referenceID": 28, "context": "While Prediction Accuracy is an excellent measure when evaluating the utility of a classifier or model [30, 31], care needs to be taken when extending its use to the privacy-preservation domain.", "startOffset": 103, "endOffset": 111}, {"referenceID": 29, "context": "While Prediction Accuracy is an excellent measure when evaluating the utility of a classifier or model [30, 31], care needs to be taken when extending its use to the privacy-preservation domain.", "startOffset": 103, "endOffset": 111}, {"referenceID": 13, "context": "It has been common in the past for privacy-preservation techniques to have their effect on the quality (utility) of the data measured with prediction accuracy [14, 32, 15, 7].", "startOffset": 159, "endOffset": 174}, {"referenceID": 30, "context": "It has been common in the past for privacy-preservation techniques to have their effect on the quality (utility) of the data measured with prediction accuracy [14, 32, 15, 7].", "startOffset": 159, "endOffset": 174}, {"referenceID": 14, "context": "It has been common in the past for privacy-preservation techniques to have their effect on the quality (utility) of the data measured with prediction accuracy [14, 32, 15, 7].", "startOffset": 159, "endOffset": 174}, {"referenceID": 6, "context": "It has been common in the past for privacy-preservation techniques to have their effect on the quality (utility) of the data measured with prediction accuracy [14, 32, 15, 7].", "startOffset": 159, "endOffset": 174}, {"referenceID": 13, "context": "A solution to (1) is for the data owner to build a ZM and ZD with every possible data mining technique they think is worth checking [14].", "startOffset": 132, "endOffset": 136}, {"referenceID": 13, "context": "If this solution is not used, then a user must either trust the implicit assumption that other data mining techniques will perform similarly, or release the collection of patterns ZM that they did test, and not release M to the public at all [14].", "startOffset": 242, "endOffset": 246}, {"referenceID": 30, "context": "To the best of our knowledge, aside from our preliminary investigation [32], no solution to (2) currently exists in the literature.", "startOffset": 71, "endOffset": 75}, {"referenceID": 30, "context": "Pattern Accuracy Introduced by us in a 2014 conference [32] and not published in a journal until now, Pattern Accuracy is a simple measure that compares D and M .", "startOffset": 55, "endOffset": 59}, {"referenceID": 19, "context": "Pattern Support Distance (PSD) The \u201csupport\u201d of a pattern is the number of records in a dataset that a pattern covers [21, 22], and can be represented as |\u03c3X(D)| when describing the support of patternX in datasetD.", "startOffset": 118, "endOffset": 126}, {"referenceID": 20, "context": "Pattern Support Distance (PSD) The \u201csupport\u201d of a pattern is the number of records in a dataset that a pattern covers [21, 22], and can be represented as |\u03c3X(D)| when describing the support of patternX in datasetD.", "startOffset": 118, "endOffset": 126}, {"referenceID": 15, "context": "as T ), and so most data mining algorithms automatically remove them from ZD [16].", "startOffset": 77, "endOffset": 81}, {"referenceID": 4, "context": "The aim of privacy preservation is to (1) make any individual record difficult to identify, while (2) leaving the patterns as unaffected as possible [5, 33, 7].", "startOffset": 149, "endOffset": 159}, {"referenceID": 31, "context": "The aim of privacy preservation is to (1) make any individual record difficult to identify, while (2) leaving the patterns as unaffected as possible [5, 33, 7].", "startOffset": 149, "endOffset": 159}, {"referenceID": 6, "context": "The aim of privacy preservation is to (1) make any individual record difficult to identify, while (2) leaving the patterns as unaffected as possible [5, 33, 7].", "startOffset": 149, "endOffset": 159}, {"referenceID": 15, "context": "The consequent of any pattern X is usually the most common class label to occur out of all the records in \u03c3X(D), with any other class labels being ignored [16].", "startOffset": 155, "endOffset": 159}, {"referenceID": 32, "context": "To avoid these problems, we use the Chi-squared histogram distance [34] to measure differences in the distribution of Y between \u03c3X(D) and \u03c3X(M):", "startOffset": 67, "endOffset": 71}, {"referenceID": 18, "context": "\u201cConfidence\u201d refers to the certainty or reliability of a pattern \u2013 that is, how frequent the most frequent label is [20].", "startOffset": 116, "endOffset": 120}, {"referenceID": 32, "context": "It should be noted that Chi-squared histogram distance is invariant to the number of records [34], and so the support of a pattern (both in D and M) does not affect the result.", "startOffset": 93, "endOffset": 97}, {"referenceID": 16, "context": "This is often a concern with popular measures such as AUC [18] and F-measure [17], where non-binary class attributes need to be treated with care [35].", "startOffset": 58, "endOffset": 62}, {"referenceID": 33, "context": "This is often a concern with popular measures such as AUC [18] and F-measure [17], where non-binary class attributes need to be treated with care [35].", "startOffset": 146, "endOffset": 150}, {"referenceID": 15, "context": "1, it is common in the literature for a privacypreserving technique\u2019s impact on data utility to be quantified using Prediction Accuracy: that is, by comparing \u03b1(ZM |T ) to \u03b1(ZD|T ) [16].", "startOffset": 181, "endOffset": 185}, {"referenceID": 16, "context": "Other common measures are F-measure [17] and AUC [18], where again ZM is compared to ZD using T (refer to Fig.", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "Meanwhile, AUC [18] is shorthand for \u201cArea under the ROC curve\u201d, with \u201cROC\u201d in turn being short for \u201cReceiver Operating Characteristic\u201d.", "startOffset": 15, "endOffset": 19}, {"referenceID": 34, "context": "It has become popular in the machine learning community as of late, despite some problems it has when comparing different classifiers [36, 37].", "startOffset": 134, "endOffset": 142}, {"referenceID": 35, "context": "It has become popular in the machine learning community as of late, despite some problems it has when comparing different classifiers [36, 37].", "startOffset": 134, "endOffset": 142}, {"referenceID": 17, "context": "We use 17 datasets publicly available in the UCI Machine Learning Repository [19].", "startOffset": 77, "endOffset": 81}, {"referenceID": 27, "context": "ZD), we run the CART algorithm [29], with a minimum leaf size (i.", "startOffset": 31, "endOffset": 35}, {"referenceID": 36, "context": "Pearson\u2019s r value) [38].", "startOffset": 19, "endOffset": 23}, {"referenceID": 36, "context": "The correlations are calculated using Pearson\u2019s correlation coefficient [38] as described in Sect.", "startOffset": 72, "endOffset": 76}], "year": 2015, "abstractText": "In this paper, we explore how modifying data to preserve privacy affects the quality of the patterns discoverable in the data. For any analysis of modified data to be worth doing, the data must be as close to the original as possible. Therein lies a problem \u2013 how does one make sure that modified data still contains the information it had before modification? This question is not the same as asking if an accurate classifier can be built from the modified data. Often in the literature, the prediction accuracy of a classifier made from modified (anonymized) data is used as evidence that the data is similar to the original. We demonstrate that this is not the case, and we propose a new methodology for measuring the retention of the patterns that existed in the original data. We then use our methodology to design three measures that can be easily implemented, each measuring aspects of the data that no pre-existing techniques can measure. These measures do not negate the usefulness of prediction accuracy or other measures \u2013 they are complementary to them, and support our argument that one measure is almost never enough.", "creator": "LaTeX with hyperref package"}}}