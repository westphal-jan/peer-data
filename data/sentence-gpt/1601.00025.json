{"id": "1601.00025", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Dec-2015", "title": "Write a Classifier: Predicting Visual Classifiers from Unstructured Text", "abstract": "People typically learn through exposure to visual concepts associated with linguistic descriptions. For instance, teaching visual object categories to children is often accompanied by descriptions in text or speech. In a machine learning context, these observations motivates us to ask whether this learning process could be computationally modeled to learn visual classifiers. More specifically, the main question of this work is how to utilize purely textual description of visual classes with no training images, to learn explicit visual classifiers for them. We propose and investigate two baseline formulations, based on regression and domain transfer that predict a linear classifier. Then, we propose a new constrained optimization formulation that combines a regression function and a knowledge transfer function with additional constraints to predict the linear classifier parameters for new classes. We also propose a generic kernelized models where a kernel classifier, in the form defined by the representer theorem, is predicted. The kernelized models allow defining any two RKHS kernel functions in the visual space and text space, respectively, and could be useful for other applications. We finally propose a kernel function between unstructured text descriptions that builds on distributional semantics, which shows an advantage in our setting and could be useful for other applications. We applied all the studied models to predict visual classifiers for two fine-grained categorization datasets, and the results indicate successful predictions of our final model against several baselines that we designed.\n\n\n\nThe main data source for this paper is provided in the table below. The data in the table below represents the overall prediction models, but they are not meant to represent the general prediction models. The mean mean predictions are used to compare the model models to other models, but not to represent the general prediction models.\nThe mean predictions are used to compare the model models to other models, but not to represent the general prediction models. The mean predictions are used to compare the model models to other models, but not to represent the general prediction models. The mean predictions are used to compare the model models to other models, but not to represent the general prediction models.\nIn summary, the prediction model is a simple model based on standard inference. We provide a simple model that is more flexible in the context of a deep-learning approach.\nFirst, we can choose whether to use a learning technique to analyze a subset of visual representations. The choice of what to use in this approach depends on the type of training image or images the image is available from. For instance, we are able to create a neural network, which in turn produces a network of", "histories": [["v1", "Thu, 31 Dec 2015 22:23:34 GMT  (1811kb,D)", "https://arxiv.org/abs/1601.00025v1", "Journal submission. arXiv admin note: text overlap witharXiv:1506.08529"], ["v2", "Wed, 28 Dec 2016 02:13:59 GMT  (2280kb,D)", "http://arxiv.org/abs/1601.00025v2", "(TPAMI) Transactions on Pattern Analysis and Machine Intelligence 2017"]], "COMMENTS": "Journal submission. arXiv admin note: text overlap witharXiv:1506.08529", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["mohamed elhoseiny", "ahmed elgammal", "babak saleh"], "accepted": false, "id": "1601.00025"}, "pdf": {"name": "1601.00025.pdf", "metadata": {"source": "CRF", "title": "Write a Classifier: Predicting Visual Classifiers from Unstructured Text", "authors": ["Mohamed Elhoseiny", "Babak Saleh"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014Language and Vision, Zero Shot Learning, Unstructured Text, Noisy Text.\nI. INTRODUCTION\nOne of the main challenges for scaling up object recognition systems is the lack of annotated images for real-world categories. Typically there are few images available for training classifiers for most of these categories. This is reflected in the number of images per category available for training in most object categorization datasets, which, as pointed out in [1], shows a Zipf distribution. The problem of lack of training images becomes even more severe when we target recognition problems within a general category, i.e., fine-grained categorization, for example building classifiers for different bird species or flower types (there are estimated over 10000 living bird species, similar for flowers). The largest bird image datasets contain only few hundred categories (e.g., CUBirds 200 dataset [2]). However, descriptions about all the living birds are available in textual form (e.g., [3], [4]). Researchers try to exploit shared knowledge between categories to target such scalability issue. This motivated many researchers who looked into approaches that learn visual classifiers from few examples, e.g. [5], [6], [7]. This even motivated more recent works on zero-shot learning of visual categories, where there are no training images available for test categories (unseen\n1Reproducing Kernel Hilbert Space\nclasses), e.g. [8]. Such approaches exploit the similarity (visual or semantic) between seen classes and unseen ones, or describe unseen classes in terms of a learned vocabulary of semantic visual attributes.\nIn contrast to the lack of reasonably sized training sets for a large number of real world categories and subordinate categories, there are abundant of textual descriptions of these categories. This comes in the form of dictionary entries, encyclopedia articles, and various online resources. For example, it is possible to find several good descriptions of a \u201cbobolink\u201d in encyclopedias of birds, while there are only a few images available for that bird online.\nThe main question we address in this paper is how to use purely textual description of categories with no training images to learn visual classifiers for these categories. In other words, we aim at zero-shot learning of object categories where the description of unseen categories comes in the form of typical text such as an encyclopedia entry; see Fig. 1. We explicitly address the question of how to automatically decide which information to transfer between classes without the need of human intervention. In contrast to most related work, we go beyond the simple use of tags and image captions, and apply standard Natural Language Processing techniques to typical text to learn visual classifiers.\nFine-grained categorization refers to classification of highly similar objects. This similarity can be due to natural intrinsic characteristics of subordinates of one category of objects (e.g. different breeds of dogs) or artificial subcategories of an object class (different types of airplanes). Diverse applications of fine-grained categories range from classification of natural species [2], [9], [10], [11] to retrieval of different types of commercial products [12]. In this problem, when we learn from an expert about different species of birds, the teacher will not just give you sample images of each species and their class labels; the teacher will tell you about discriminative visual or non-visual features for each species, similarities and\nar X\niv :1\n60 1.\n00 02\n5v 2\n[ cs\n.C V\n] 2\n8 D\nec 2\n01 6\n2 The Bobolink (Dolichonyx oryzivorus) is a small New World blackbird and the only member of genus Dolichonyx. Description: Adults are 16-18 cm (6-8 in) long with short finch-like bills. They weigh about 1 oz. Adult males are mostly black, although they do display creamy napes, and white scapulars, lower backs and rumps. Adult females are mostly light brown, although their coloring includes black streaks on the back and flanks, and dark stripes on the head; their wings and tails are darker. The collective name for a group of bobolinks is a chain. Distribution and movement: These birds migrate to Argentina, Bolivia and Paraguay. One bird was tracked flying 12,000 mi over the course of the year, and up to 1,100 mi in one day. They often migrate in flocks, feeding on cultivated grains and rice, which leads to them being considered a pest by farmers in some areas. Although Bobolinks migrate long distances, they have rarely been sighted in Europe-like many vagrants from the Americas, the overwhelming majority of records are from the British Isles. Each fall, Bobolinks gather in large numbers in South American rice fields, where they are inclined to eat grain. This has earned them the name \u201dricebird\u201d in these parts. However, they are called something entirely different in Jamaica (Butterbirds) where they are collected as food, being that they are very fat as they pass through on migration.\nFig. 2: Top: Example Wikipedia article about the Painted Bunting, with an example image. Bottom: The proposed learning setting. For each category we are give one (or more) textual description (only a synopsis of a larger text is shown), and a set of training images. Our goal is to be able to predict a classifier for a category based only on the narrative (zero-shot learning).\ndifferences between species, hierarchical relations between species, and many other aspects. The same learning experience takes place when you read a book or a web page to learn about different species of birds; For example, Fig. 2 shows an example narrative about the Bobolink. Typically, the narrative tells you about the bird\u2019s taxonomy, highlights discriminative features about that bird and discusses similarities and differences between species, as well as within-species variations (male vs. female). The narrative might eventually show very few example images, which are often selected wisely to illustrate certain visual aspects that might be hard to explain in the narrative. This learning strategy using textual narrative and images makes the learning effective without the huge number of images that a typical visual learning algorithm would need to learn the class boundaries.\nHowever, a narrative about a specific species does not contain only \u201cvisually\u201d relevant information, but also gives abundant information about the species\u2019s habitat, diet, mating habits, etc., which are not relevant for visual identification. In a sense, this information might be textual clutter for that task. The same problem takes place in images. While one image can be very effective in highlighting an important feature for learning, many images might have a lot of visual clutter that makes their uses in learning not effective. Thus, a picture can be worth a thousand words, but not always, and an abundant number of pictures might not be the most effective way for learning. Similarly, one text paragraph can be worth a thousand pictures for learning a concept, but not always, and large amounts of text might not necessarily be effective.\nThe contribution of the paper is on exploring this new problem, which to the best of our knowledge, is firstly explored\nin the computer vision community in an earlier version of this work [13]. We learn from an image corpus and a textual corpus, however not in the form of image-caption pairs, instead the only alignment between the corpora is at the level of the category. In particular, we address the problem of formulating a visual classifier prediction function \u03a6(\u00b7), which predicts a classifier of unseen visual class given its text description; see figure 2. While a part of this work was published in [13], we extend the work here to study more formulations to solve the problem in Sec. V (B,E). In addition, we propose a kernel method to explicitly predict a kernel classifier in the form defined in the representer theorem [14]. The kernelized prediction has an advantage that it opens the door for using any kind of side information about classes, as long as kernels can be used on the side information representation. The side information can be in the form of textual, parse trees, grammar, visual representations, concepts in the ontologies (adopted in NLP domain), or any form. We focus here on unstructured text descriptions. The image features also do not need to be in a vectorized format. The kernelized classifiers also facilitate combining different types of features through a multikernel learning (MKL) paradigm, where the fusion of different features can be effectively achieved.\nBeyond the introduction and the related work sections, the paper is structured as follows: Section III and IV details the problem definition and relation to regression and knowledge transfer models. Section V shows different formulations of \u03a6(\u00b7) that we studied to predict a linear visual classifier; see figure 2. Section VI presents a kernelized version of our approach where \u03a6(\u00b7) predicts a kernel classifier in the form defined by the representer theorem [14]. Section VII\n3 presents our proposed distributional semantic kernel between unstructured text description, which is applicable to our kernel formulation and can be useful for other applications as well. Section VIII presents our experiments on Flower Dataset [9] and Caltech-UCSD dataset [15] for both the linear and the kernel classifier predictions."}, {"heading": "II. RELATED WORK", "text": "We focus our related work discussion on three related lines of research: \u201czero/few-shot learning\u201d, \u201cvisual knowledge transfer\u201d, and \u201cLanguage and Vision\u201d.\nZero/Few-Shot Learning: Motivated by the practical need to learn visual classifiers of rare categories, researchers have explored approaches for learning from a single image (oneshot learning [16], [6], [17], [7]) or even from no images (zero-shot learning). One way of recognizing object instances from previously unseen test categories (the zero-shot learning problem) is by leveraging knowledge about common attributes and shared parts. Typically an intermediate semantic layer is introduced to enable sharing knowledge between classes and facilitate describing knowledge about novel unseen classes, e.g. [18]. For instance, given adequately labeled training data, one can learn classifiers for the attributes occurring in the training object categories. These classifiers can then be used to recognize the same attributes in object instances from the novel test categories. Recognition can then proceed on the basis of these learned attributes [8], [19]. Such attribute-based \u201cknowledge transfer\u201d approaches use an intermediate visual attribute representation to enable describing unseen object categories.\nTypically attributes [8], [19] are manually defined by humans to describe shape, color, surface material, e.g., furry, striped, etc. Therefore, an unseen category has to be specified in terms of the used vocabulary of attributes. Rohrbach et al. [20] investigated extracting useful attributes from large text corpora. In [21], an approach was introduced for interactively defining a vocabulary of attributes that are both human understandable and visually discriminative. Huang et al. [22] relaxed the attribute independence assumption by modeling correlation between attributes to achieve better zero shot performance, as opposed to prior models.\nSimilar to the setting of zero-shot learning, we use classes with training data (seen classes) to predict classifiers for classes with no training data (unseen classes). In contrast to attributes based method (e.g., [8], [19]), in our work we do not use any explicit attributes. The description of a new category is purely textual and the process is completely automatic without human annotation beyond the class labels.\nVisual Knowledge Transfer: Our work can be seen in the context of knowledge sharing and inductive transfer. In general, knowledge transfer aims at enhancing recognition by exploiting shared knowledge between classes. Most existing research focused on knowledge sharing within the visual domain only, e.g. [23]; or exporting semantic knowledge at the level of category similarities and hierarchies, e.g. [24], [1]. We go beyond the state-of-the-art to explore cross-domain knowledge sharing and transfer. We explore how knowledge from the visual and textual domains can be used to learn across-domain correlation, which facilitates prediction of visual classifiers from textual description.\nLanguage and Vision: The relation between linguistic semantic representations and visual recognition has been explored. For example in [5], it was shown that there is a strong correlation between semantic similarity between classes, based on WordNet, and confusion between classes. Linguistic semantics in terms of nouns from WordNet [25] have been used in collecting large-scale image datasets such as ImageNet[26] and Tiny Images [27]. It was also shown that hierarchies based on WordNet are useful in learning visual classifiers, e.g. [1].\nOne of the earliest work on learning from images and text corpora is the work of Barnard et al. [28], which showed that learning a joint distribution of words and visual elements facilitates clustering the images in a semantic way, generating illustrative images from a caption, and generating annotations for novel images. There has been an increasing recent interest in the intersection between computer vision and natural language processing with researches that focus on generating textual description of images and videos, e.g. [29], [30], [31], [32]. This includes generating sentences about objects, actions, attributes, spatial relation between objects, contextual information in the images, scene information, etc. Based on the success of sequence to sequence training of neural nets in machine translation (e.g., [33]), impressive works has been recently proposed for image captioning (e.g., [34], [35], [36], [37]). In contrast, our work is different in two fundamental ways. In terms of the goal, we do not target generating textual description from images, instead we target predicting classifiers from text, in a zero-shot setting. In terms of the learning setting, the textual descriptions that we use is at the level of the category and do not come in the form of imagecaption pairs, as in typical datasets used for text generation from images, e.g. [38].\nThere are several recent works that studies unannotated text with images. In [39], [40], word embedding language models (e.g. [41]) were adopted to represent class names as vectors, which require training using a big text-corpus. Their goal is to embed images into the language space then perform classification. In [42], a similar yet multimodal approach was adopted for Multimedia Event Detection in videos instead of object classification. There are several differences between these works and our method. First, one limitation of the adopted language model is that it produces only one vector per word, which causes problems when a word has multiple meanings. Second, these methods assumes that each class is represented by one or few-words and hence can not represent a class text description that typically contains multiple paragraphs in our setting. Third, our goal is different which is to map the text description to an explicit classifier in the visual domain, i.e.the opposite direction of their goal. Fourth, these models do not support non-linear classification, supported by the kernelized version proposed in this work. Finally, we focus on fine-grained recognition, which is a very challenging task."}, {"heading": "III. PROBLEM DEFINITION", "text": "Fig 2 illustrates the learning setting. The information in our problem comes from two different domains: the visual domain and the textual domain, denoted by V and T , respectively. Similar to traditional visual learning problems, we are given training data in the form V = {(xi, li)}N , where xi is an\n4\nimage and li \u2208 {1 \u00b7 \u00b7 \u00b7Nsc} is its class label. We denote the number of classes available at training as Nsc, where sc indicates \u201cseen classes\u201d. As typically done in visual classification setting, we can learn Nsc binary one-vs-all classifiers, one for each of these classes.\nOur goal is to be able to predict a classifier for a new category based only on the learned classes and a textual description(s) of that category. In order to achieve that, the learning process has to also include textual description of the seen classes (as shown in Fig 2 ). Depending on the domain we might find a few, a couple, or as little as one textual description to each class. We denote the textual training data for class j by {ti \u2208 T }j . In this paper we assume we are dealing with the extreme case of having only one textual description available per class, which makes the problem even more challenging. For simplicity, the text description of class j is denoted by tj . However, the formulation we propose in this paper directly applies to the case of multiple textual descriptions per class.\nIn this paper, we discuss the task of predicting visual classifier \u03a6(t\u2217) from an unseen text description t\u2217 in linear form or RKHS kernalized form, defined as follows"}, {"heading": "A. Linear Classifier", "text": "Let us consider a typical linear classifier in the feature space in the form\nfj(x) = c T j \u00b7 x\nwhere x (bold) is the visual feature vector of an image x (not bold) amended with 1 and cj \u2208 Rdv is the linear classifier parameters for class j. Given a test image, its class is determined by\nl\u2217 = arg max j fj(x) (1)\nSimilar to the visual domain, the raw textual descriptions have to go through a feature extraction process. Let us denote the linear extracted textual feature by T = {tj \u2208 Rdt}j=1\u00b7\u00b7\u00b7Nsc , where tj is the features of text description tj (not bold). Given a textual description t\u2217 of a new unseen category U with linear feature vector representation t\u2217, the problem can now be defined as predicting a one-vs-all linear classifier parameters \u03a6(t\u2217) = c(t\u2217) \u2208 Rdv , such that it can be directly used to classify any test image x as (also see Table I)\nc(t\u2217) T \u00b7 x > 0 if x belongs to U c(t\u2217) T \u00b7 x < 0 otherwise (2)"}, {"heading": "B. Kernel Classifier", "text": "For kernel classifiers, we assume that each of the domains is equipped with a kernel function corresponding to a reproducing kernel Hilbert space (RKHS). Let us denote the kernel for V by k(\u00b7, \u00b7), and the kernel for T by g(\u00b7, \u00b7).\nAccording to the generalized representer theorem [14], a minimizer of a regularized empirical risk function over an RKHS could be represented as a linear combination of kernels, evaluated on the training set. Adopting the representer theorem on classification risk function, we define a kernel-classifier of a visual class j as follows\nwhere x \u2208 V is the test image, xi is the ith image in the training data V , k(x) = [k(x, x1), \u00b7 \u00b7 \u00b7 , k(x, xN ), 1]T, \u03b2j = [\u03b21j \u00b7 \u00b7 \u00b7\u03b2Nj , b]T. Having learned fj(x\u2217) for each class j (for example using SVM classifier), the class label of the test image x can be predicted by Eq. 1, similar to the linear case. Eq. 3 also shows how \u03b2j is related to cj in the linear classifier, where k(x, x\u2032) = \u03d5(x)T \u00b7\u03d5(x\u2032) and \u03d5(\u00b7) is a feature map that does not have to be explicitly defined given the definition of k(\u00b7, \u00b7) on V . Hence, our goal in the kernel classifier prediction is to predict \u03b2(t\u2217) instead of c(t\u2217) since it is sufficient to define ft\u2217(x) for a text description t\u2217 of an unseen class given k(x)\nIt is clear that fj(x) could be learned for all classes with training data j \u2208 1 \u00b7 \u00b7 \u00b7Nsc, since there are examples for the seen classes; we denote the kernel-classifier parameters of the seen classes as Bsc = {\u03b2j}Nsc ,\u2200j. However, it is not obvious how to predict ft\u2217(x) for an unseen class given its text description t\u2217. Similar to the linear classifier prediction, our main notion is to use the text description t\u2217, associated with unseen class, and the training data to directly predict the unseen kernel-classifier parameters. In other words, the kernel classifier parameters of the unseen class is a function of its text description t\u2217 , the image training data V and the text training data {tj}, j \u2208 1 \u00b7 \u00b7 \u00b7Nsc; i.e.\nft\u2217(x) = \u03b2(t\u2217) T \u00b7 k(x),\nft\u2217(x) could be used to classify new points that belong to an unseen class as follows: 1) one-vs-all setting ft\u2217(x) \u2277 0 ; or 2) in a Multi-class prediction as in Eq 1. In this case, \u03a6(t\u2217) = \u03b2(t\u2217); see Table I. In contrast to the linear classifier prediction, there is no need to explicitly represent an image x or a text description t by features, which are denoted by the bold symbols in the previous section. Rather, only k(\u00b7, \u00b7) and g(\u00b7, \u00b7) must be defined which leads to more general classifiers."}, {"heading": "IV. RELATION TO REGRESSION AND KNOWLEDGE TRANSFER MODELS", "text": "We introduce two possible frameworks for this problem and discuss potential limitations for them. In this background section, we focus on predicting linear classifiers for simplicity, which motivates the evaluated linear classifier formulations that follow in Sec V."}, {"heading": "A. Regression Models", "text": "A straightforward way to solve this problem is to pose it as a regression problem where the goal is to use the textual data and the learned classifiers, {(tj , cj)}j=1\u00b7\u00b7\u00b7Nsc to learn a regression function from the textual feature domain to the visual classifier domain, i.e., a function c(\u00b7) : Rdt \u2192 Rdv . The question is which regression model would be suitable for this\n5 problem? and would posing the problem in this way lead to reasonable results?\nA typical regression model, such as ridge regression [43] or Gaussian Process (GP) Regression [44], learns the regressor to each dimension of the output domain (the parameters of a linear classifier) separately, i.e., a set of functions ci(\u00b7) : Rdt \u2192 R. Clearly this will not capture the correlation between the visual classifier dimensions. Instead, a structured prediction regressor would be more suitable since it would learn the correlation between the input and output domain. However, even a structured prediction model will only learn the correlation between the textual and visual domain through the information available in the input-output pairs (tj , cj). Here the visual domain information is encapsulated in the prelearned classifiers and prediction does not have access to the original data in the visual domain. Instead, we need to directly learn the correlation between the visual and textual domain and use that for prediction.\nAnother fundamental problem that a regressor would face, is the sparsity of the data; the data points are the textual description-classifier pairs, and typically the number of classes can be very small compared to the dimension of the classifier space (i.e.Nsc dv). In a setting like that, any regression model is bound to suffer from an under fitting problem. This can be best explained in terms of GP regression, where the predictive variance increases in the regions of the input space where there are no data points. This will result in poor prediction of classifiers at such regions."}, {"heading": "B. Knowledge Transfer Models", "text": "An alternative formulation is to pose the problem as domain adaptation from the textual to the visual domain. In the computer vision context, domain adaptation work has focused on transferring categories learned from a source domain, with a given distribution of images, to a target domain with a different distribution, e.g., images or videos from different sources [45], [46], [47], [48]. What we need is an approach that learns the correlation between the textual domain features and the visual domain features, and uses that correlation to predict new visual classifier given textual features.\nIn particular, in [47] an approach for learning cross domain transformation was introduced. In that work a regularized asymmetric transformation between points in two domains were learned. The approach was applied to transfer learned categories between different data distributions, both in the visual domain. A particular attractive characteristic of [47], over other domain adaptation models, is that the source and target domains do not have to share the same feature spaces or the same dimensionality.\nWhile a totally different setting is studied in [47], it inspired us to formulate the zero-shot learning problem as a domain transfer problem. This can be achieved by learning a linear transfer function W between T and V . The transformation matrix W can be learned by optimizing, with a suitable regularizer, over constraints of the form tTWx \u2265 l if t \u2208 T and x \u2208 V belong to the same class, and tTWx \u2264 u otherwise. Here l and u are model parameters. This transfer function acts as a compatibility function between the textual features and visual features, which gives high values if they are\nfrom the same class and a low value if they are from different classes.\nIt is not hard to see that this transfer function can act as a classifier. Given a textual feature t\u2217 and a test image, represented by x, a classification decision can be obtained by tT\u2217Wx \u2277 b where b is a decision boundary which can be set to (l+ u)/2. Hence, our desired predicted classifier in Eq 2 can be obtained as c(t\u2217) = tT\u2217W (note that the features vectors are amended with ones). However, since learning W was done over seen classes only, it is not clear how the predicted classifier c(t\u2217) will behave for unseen classes. There is no guarantee that such a classifier will put all the seen data on one side and the new unseen class on the other side of that hyperplane."}, {"heading": "V. FORMULATIONS FOR PREDICTING A LINEAR", "text": "CLASSIFIER FORM ( \u03a6(t\u2217) = c(t\u2217))\nThe proposed formulations in this section aims at predicting a linear hyperplane parameter c of a one-vs-all classifier for a new unseen class given a textual description, encoded as a feature vector t\u2217 and the knowledge learned at the training phase from seen classes2. We start by defining the learning components that are used by the formulations described in this section:\nClassifiers: a set of linear one-vs-all classifiers {cj} are learned, one for each seen class. Probabilistic Regressor: Given {(tj , cj)} a regressor is learned that can be used to give a prior estimate for preg(c|t) (Details in Sec V-A). Domain Transfer: Given T and V , a domain transfer function encoded in the matrix W, is learned which captures the correlation between the textual and visual domains (Details in Sec V-C).\nEach of the following subsections show a different approach to predict a linear classifier from t\u2217 as \u03a6(t\u2217) = c(t\u2217); see Sec III-A. The final approach (E), which achieves the best performance, combines regression, domain transfer, and additional constraints. We compare between these alternative formulations (A to E) in our experiments. Hyper-parameter selection is detailed in the supplementary materials for all the approaches."}, {"heading": "A. Probabilistic Regressor", "text": "There are different regressors that can be used, however we need a regressor that provide a probabilistic estimate preg(c|(t)). For the reasons explained in Sec III, we also need a structure prediction approach that is able to predict all the dimensions of the classifiers together. For these reasons, we use the Twin Gaussian Process (TGP) [49]. TGP encodes the relations between both the inputs and structured outputs using Gaussian Process priors. This is achieved by minimizing the Kullback-Leibler divergence between the marginal GP of the outputs (i.e. classifiers in our case) and observations (i.e. textual features). The estimated regressor output (c\u0303(t\u2217)) in\n2The notations follow from Subsection III-A\n6 Pure \u00a0textual \u00a0descrip/on \u00a0of \u00a0an \u00a0 unknown \u00a0object \u00a0class: \u00a0 \u00a0\nCorrela/on \u00a0between \u00a0Text \u00a0descrip/on \u00a0 \u00a0 and \u00a0Images: \u00a0 Learned \u00a0on \u00a0Known \u00a0object \u00a0classes \u00a0during \u00a0 Training: \u00a0\nCorr(T, \u00a0X) \u00a0\nLearned\t\r \u00a0Visual\t\r \u00a0classifiers\t\r \u00a0as\t\r \u00a0Probability:\t\r \u00a0\nP(c|t) \u00a0 Likelihood\t\r \u00a0of\t\r \u00a0Visual\t\r \u00a0classifier\t\r \u00a0\u201cC\u201d\t\r \u00a0for\t\r \u00a0 an \u00a0object \u00a0described \u00a0by \u00a0text \u00a0\u201cT\u201d. \u00a0 Learned \u00a0on \u00a0Known \u00a0object \u00a0classes \u00a0 during \u00a0Training: \u00a0  \u00a0\nWhite \u00a0Arum \u00a0Lily: \u00a0 It \u00a0is \u00a0a \u00a0rhizomatous \u00a0herbaceous \u00a0 perennial \u00a0plant, \u00a0evergreen \u00a0 where \u00a0rainfall \u00a0andtemperatures \u00a0 are \u00a0adequate, \u00a0deciduous \u00a0where \u00a0 there \u00a0is \u00a0a \u00a0dry \u00a0season. \u00a0\nGarden \u00a0Phlox: \u00a0 Flowers \u00a0may \u00a0be \u00a0pale \u00a0blue, \u00a0 violet, \u00a0pink, \u00a0bright \u00a0red, \u00a0or \u00a0 white. \u00a0Many \u00a0are \u00a0fragrant. \u00a0 FerHlized\t\r \u00a0flowers\t\r \u00a0typically\t\r \u00a0 produce \u00a0one \u00a0relaHvely \u00a0large \u00a0 seed. \u00a0\nSweet \u00a0Pea: \u00a0 It \u00a0is \u00a0an \u00a0annual \u00a0climbing \u00a0plant, \u00a0 growing \u00a0to \u00a0a \u00a0height \u00a0of \u00a01\u20132 \u00a0 meters \u00a0(nearly \u00a0six \u00a0feet \u00a0and \u00a0six \u00a0 inches), \u00a0where \u00a0suitable \u00a0 support \u00a0is \u00a0available. \u00a0\nFire \u00a0Lily: \u00a0 These \u00a0plants \u00a0grow \u00a0in \u00a0 mountain \u00a0meadows \u00a0and \u00a0 rocks. \u00a0They \u00a0prefer \u00a0calcareous \u00a0 soils \u00a0in \u00a0warm, \u00a0sunny \u00a0places, \u00a0 but \u00a0also \u00a0grows \u00a0on \u00a0slightly \u00a0acid \u00a0 soils. \u00a0 v \u00a0 v \u00a0\nv \u00a0\nv \u00a0\nv \u00a0 v \u00a0v \u00a0\nv \u00a0 v \u00a0\nv \u00a0 v \u00a0\nPredic/ng\t\r \u00a0the\t\r \u00a0Visual\t\r \u00a0Classifier:\t\r \u00a0 In\t\r \u00a0this\t\r \u00a0step\t\r \u00a0we\t\r \u00a0generate\t\r \u00a0a\t\r \u00a0visual\t\r \u00a0classifier\t\r \u00a0for\t\r \u00a0\u201cFire\t\r \u00a0 Lily\u201d \u00a0\u2013as \u00a0unknown \u00a0class\u2014 \u00a0 By \u00a0opHmizing \u00a0a \u00a0cost \u00a0funcHon \u00a0based \u00a0on \u00a0\u201cCorr(T,X)\u201d \u00a0 and \u00a0\u201cP(c|t)\u201d, \u00a0which \u00a0we \u00a0have \u00a0learned \u00a0on \u00a0\u201cKnown \u00a0 Classes\u201d. \u00a0\nFig. 3: Illustration of the Proposed Linear Prediction Framework (Constrained Regression and Domain Transfer) for the task Zero-shot learning from textual description (Linear Formulation (E))\nTGP is given by the solution of the following non-linear optimization problem [49] 3.\n\u03a6(t\u2217) = c\u0303(t\u2217) =argmin c\n[KC(c, c)\u2212 2kc(c)Tu\u2212 \u03b7 log(\nKC(c, c\u2212 kc(c)T(KC + \u03bbcI)\u22121kc(c))] (4)\nwhere u = (KT + \u03bbtI)\u22121kt(t\u2217), \u03b7 = KT (t\u2217, t\u2217) \u2212 k(t\u2217)\nTu, and KT (tl, tm) and KC(cl, cm) are Gaussian kernel for input feature t and output vector c, respectively. kc(c) = [KC(c, c1), \u00b7 \u00b7 \u00b7 ,KC( c, cNsc)]T. kt(t\u2217) = [KT (t\u2217, t1), \u00b7 \u00b7 \u00b7 ,KT (t\u2217, tNsc)]T. \u03bbt and \u03bbc are regularization parameters to avoid overfitting. This optimization problem can be solved using a second order, BFGS quasi-Newton optimizer with cubic polynomial line search for optimal step size selection [49]. In this case, the classifier dimensions are predicted jointly. Hence, preg(c|t\u2217) is defined as a normal distribution.\npreg(c|t\u2217) = N (\u00b5c = c\u0303(t\u2217),\u03a3c = I) (5) The reason that \u03a3c = I is that TGP does not provide predictive variance, unlike Gaussian Process Regression. However, it has the advantage of handling the dependency between the dimensions of the classifiers c given the textual features t."}, {"heading": "B. Constrained Probabilistic Regressor", "text": "We also investigated formulations that use regression to predict an initial hyperplane c\u0303(t\u2217) as described in section V-A, which is then optimized to put all seen data in one side, i.e.\n\u03a6(t\u2217) = c\u0302(t\u2217) = argmin c,\u03b6i\n[cTc + \u03b1\u03c8(c, c\u0303(t\u2217)) + C N\u2211 i=1 \u03b6i]\ns.t. : \u2212cTxi \u2265 \u03b6i, \u03b6i \u2265 0, i = 1, \u00b7 \u00b7 \u00b7 , N where \u03c8(\u00b7, \u00b7) is a similarity function between hyperplanes, e.g., a dot product used in this work, \u03b1 is its constant weight, and C is the weight to the soft constraints of existing images as negative examples (inspired by linear SVM formulation). We call this class of methods constrained GPR/TGP, since c\u0303(t\u2217) is initially predicted through GPR or TGP.\n3notice we are using c\u0303 to denote the output of the regressor, while using c\u0302 to denote the output of the final optimization problem in Eq 8"}, {"heading": "C. Domain Transfer (DT)", "text": "To learn the domain transfer function W we adapted the approach in [47] as follows. Let T be the textual feature data matrix and X be the visual feature data matrix where each feature vector is amended with a 1. Notice that amending the feature vectors with a 1 is essential in our formulation since we need tTW to act as a classifier. We need to solve the following optimization problem\nmin W r(W) + \u03bb \u2211 i ci(TWX T) (6)\nwhere ci\u2019s are loss functions over the constraints and r(\u00b7) is a matrix regularizer. It was shown in [47], under condition on the regularizer, that the optimal W is in the form of W\u2217 = TK\n\u2212 12 T L \u2217K \u2212 12 X X T, where KT = TTT, KX = XXT. L\u2217 is computed by minimizing the following minimization problem\nmin L [r(L) + \u03bb \u2211 p cp(K 1 2 TLK 1 2 X)], (7)\nwhere cp(K 1 2 TLK 1 2 X) = (max(0, (l \u2212 eiK 1 2 TLK 1 2 Xej))) 2 for same class pairs of index i,j, or = (max(0, (eiK 1 2 TLK 1 2\nXej \u2212 u)))2 otherwise, where ek is a one-hot vector of zeros except a one at the kth element, and u > l. In our work, we used l = 2, u = \u22122 (note any appropriate l and u can work). We used a Frobenius norm regularizer. This energy is minimized using a second order BFGS quasi-Newton optimizer. Once L is computed W\u2217 is computed using the transformation above. Finally \u03a6(t\u2217) = c(t\u2217) = tT\u2217W, simplifying W \u2217 as W."}, {"heading": "D. Constrained-DT", "text": "We also investigated constrained-DT formulations that learns a transfer matrix W and enforce tTjW to be close to the classifiers learned on seen data, {cj} ,i.e.\nmin W r(W) + \u03bb1 \u2211 i ci(TWX T) + \u03bb2 \u2211 j \u2016cj \u2212 tTjW\u20162\nA classifier can be then obtained by \u03a6(t\u2217) = c(t\u2217) = tT\u2217W.\n7"}, {"heading": "E. Constrained Regression and Domain Transfer for classifier prediction", "text": "Fig 3 illustrates our final framework which combines regression (formulation A (using TGP)) and domain transfer (formulation C) with additional constraints. This formulation combines the three learning components described in the beginning of this section. Each of these components contains partial knowledge about the problem. The question is how to combine such knowledge to predict a new classifier given a textual description. The new classifier has to be consistent with the seen classes. The new classifier has to put all the seen instances at one side of the hyperplane, and has to be consistent with the learned domain transfer function. This leads to the following constrained optimization problem\n\u03a6(t\u2217) = c\u0302(t\u2217) =argmin c,\u03b6i\n[ cTc\u2212 \u03b1t\u2217TWc\u2212 \u03b3 ln(preg(c|t\u2217))\n+ C \u2211 \u03b6i ]\ns.t. : \u2212(cTxi) \u2265 \u03b6i, \u03b6i \u2265 0, i = 1 \u00b7 \u00b7 \u00b7N t\u2217\nTWc \u2265 l \u03b1, \u03b3, C, l : hyperparameters\n(8)\nThe first term is a regularizer over the classifier c. The second term enforces that the predicted classifier has high correlation with tT\u2217W; W is learnt by Eq 10. The third term favors a classifier that has high probability given the prediction of the regressor. The constraints \u2212cTxi \u2265 \u03b6i enforce all the seen data instances to be at the negative side of the predicted classifier hyperplane with some missclassification allowed through the slack variables \u03b6i. The constraint t\u2217TWc \u2265 l enforces that the correlation between the predicted classifier and t\u2217TW is no less than l, this is to enforce a minimum correlation between the text and visual features.\nSolving for c\u0302 as a quadratic program: According to the definition of preg(c|t\u2217) for TGP, ln p(c|t\u2217) is a quadratic term in c in the form\n\u2212 ln p(c|t\u2217) \u221d (c\u2212 c\u0303(t\u2217))T(c\u2212 c\u0303(t\u2217)) = cTc\u2212 2cTc\u0303(t\u2217) + c\u0303(t\u2217)Tc\u0303(t\u2217)\n(9)\nWe reduce \u2212 ln p(c|t\u2217) to \u22122cTc\u0303(t\u2217)), since 1) c\u0303(t\u2217)Tc\u0303(t\u2217) is a constant (i.e.does not affect the optimization), 2) cTc is already included as regularizer in equation 8. In our setting, the dot product is a better similarity measure between two hyperplanes. Hence, \u22122cTc\u0303(t\u2217) is minimized. Given \u2212 ln p(c|t\u2217) from the TGP and W, Eq 8 reduces to a quadratic program on c with linear constraints. We tried different quadratic solvers, however the IBM CPLEX solver 4 gives the best performance in speed and optimization for our problem."}, {"heading": "VI. FORMULATIONS FOR PREDICTING A KERNEL", "text": "CLASSIFIER FORM ( \u03a6(t\u2217) = \u03b2(t\u2217) )\nPrediction of \u03a6(t\u2217) = \u03b2(t\u2217) (Sec. III-B), is decomposed into training (domain transfer) and prediction phases, detailed as follows\n4http://www-01.ibm.com/software/integration/optimization/cplex-optimizer"}, {"heading": "A. Kernelized Domain Transfer", "text": "During training, we firstly learn Bsc = {\u03b2j}, j = 1 \u2192 Nsc as SVM-kernel classifiers based on the training data and defined by k(\u00b7, \u00b7) visual kernel. Then, we learn a kernel domain transfer function to transfer the text description information t\u2217 \u2208 T to kernel-classifier parameters \u03b2 \u2208 RN+1 in V domain. We call this domain transfer function \u03b2DA(t\u2217), which has the form of \u03a8Tg(t\u2217), where g(t\u2217) = [g(t\u2217, t1) \u00b7 \u00b7 \u00b7 g(t\u2217, tNsc)]T; \u03a8 is an Nsc \u00d7 N + 1 matrix, which transforms t to kernel classifier parameters for the class that t\u2217 represents.\nWe aim to learn \u03a8 from V and {tj}, j = 1 \u00b7 \u00b7 \u00b7Nsc, such that g(t)T\u03a8k(x) > l if t and x correspond to the same class, g(t)T\u03a8k(x) < u otherwise. Here l controls similarity lowerbound if t and x correspond to same class, and u controls similarity upper-bound if t and x belong to different classes. In our setting, the term \u03a8Tg(tj) should act as a classifier parameter for class j in the training data. Therefore, we introduce penalization constraints to our minimization function if \u03a8T g(tj) is distant from \u03b2j \u2208 Bsc, where ti corresponds to the class that \u03b2i classifies. we model the kernel domain transfer function as follows\n\u03a8\u2217 = arg min \u03a8 L(\u03a8) = [ 1 2 r(\u03a8) + \u03bb1 \u2211 k ck(G \u03a8 K)+\n\u03bb2 Nsc\u2211 i=1 \u2016\u03b2i \u2212\u03a8 T g(ti)\u20162\n(10)\nwhere, G is an Nsc \u00d7 Nsc symmetric matrix, such that both the ith row and the ith column are equal to g(ti), i = 1 : Nsc; K is an N + 1\u00d7N matrix, such that the ith column is equal to k(xi), xi, i = 1 : N . ck\u2019s are loss functions over the constraints defined as ck(G \u03a8 K)) = (max(0, (l\u2212 1TiG \u03a8 K1j)))2 for same class pairs of index i and j, or = r \u00b7 (max(0, (1TiG \u03a8 K1j \u2212 u)))2 otherwise, where 1i is an Nsc \u00d7 1 vector with all zeros except at index i, 1j is an N \u00d7 1 vector with all zeros except at index j. This leads to that ck(G \u03a8 K)) = (max(0, (l \u2212 g(ti)T \u03a8 k(xj)))2 for same class pairs of index i and j, or = r \u00b7 (max(0, (g(ti)T \u03a8 k(xj) \u2212 u)))2otherwise, where u > l, r = nd\nns such that nd and ns are the number of pairs (i, j) of\ndifferent classes and similar pairs respectively. Finally, we used a Frobenius norm regularizer for r(\u03a8).\nThe objective function in Eq 10 controls the involvement of the constraints ck by the term multiplied by \u03bb1, which controls its importance; we call it Cl,u(\u03a8). While, the trained classifiers penalty is captured by the term multiplied by \u03bb2; we call it C\u03b2(\u03a8). One important observation on C\u03b2(\u03a8), is that it reaches zero when \u03a8 = G\u22121BT, where B = [\u03b21 \u00b7 \u00b7 \u00b7\u03b2Nsc ], since it could be rewritten as C\u03b2(\u03a8) = \u2016BT \u2212G \u03a8\u20162F .\nWe minimize L(\u03a8) by gradient-based optimization using a quasi-Newton optimizer. Our gradient derivation of L(\u03a8) leads to the following form\n\u2202L(\u03a8)\n\u2202\u03a8 = \u03a8+\u03bb1 \u00b7 \u2211 i,j g(ti)k(xj) Tvij+r \u00b7\u03bb2 \u00b7(G2 \u03a8\u2212GB) (11)\nwhere vij = \u22122\u00b7max(0, (l\u2212g(ti)T \u03a8 k(xj)) if i and j correspond to the same class, 2 \u00b7 max(0, (g(ti)T \u03a8 k(xj) \u2212 u) otherwise. Another approach that can be used to minimize L(\u03a8) is through alternating projection using Bregman algorithm [50], where \u03a8 is updated by a single constraint every iteration.\n8"}, {"heading": "B. Kernel Classifier Prediction", "text": "We study two ways to infer the final kernel-classifier prediction. (1) Direct Kernel Domain Transfer Prediction, denoted by \u201cDT-kernel\u201d, (2) One-class SVM adjusted DT Prediction, denoted by \u201cSVM-DT kernel\u201d. Hyper-parameter selection is attached in the supplementary materials. The source code is available here https://sites.google.com/site/mhelhoseiny/ computer-vision-projects/write kernel classifier.\nDirect Domain Transfer (DT) Prediction: By construction a classifier of an unseen class can be directly computed from our trained domain transfer model as follows\n\u03a6(t\u2217) = \u03b2\u0303DT (t\u2217) = \u03a8 \u2217T g(t\u2217) (12)\nOne-class-SVM adjusted DT (SVM-DT) Prediction: In order to increase separability against seen classes, we adopted the inverse of the idea of the one class kernel-svm, whose main idea is to build a confidence function that takes only positive examples of the class. Our setting is the opposite scenario; seen examples are negative examples of the unseen class. In order to introduce our proposed adjustment method, we start by presenting the one-class SVM objective function. The Lagrangian dual of the one-class SVM [51] can be written as\n\u03b2\u2217+ =argmin \u03b2\n[ \u03b2TK \u2032 \u03b2 \u2212 \u03b2Ta ] s.t. : \u03b2T1 = 1, 0 \u2264 \u03b2i \u2264 C; i = 1 \u00b7 \u00b7 \u00b7N\n(13)\nwhere K \u2032 is an N \u00d7 N matrix, K \u2032 (i, j) = k(xi, xj), \u2200xi, xj \u2208 Sx(i.e.in the training data), a is an N \u00d7 1 vector, ai = k(xi, xi), C is a hyper-parameter . It is straightforward to see that, if \u03b2 is aimed to be a negative decision function instead, the objective function would become in the following form\n\u03b2\u2217\u2212 =argmin \u03b2\n[ \u03b2TK \u2032 \u03b2 + \u03b2Ta ] s.t. : \u03b2T1 = \u22121,\u2212C \u2264 \u03b2i \u2264 0; i = 1 \u00b7 \u00b7 \u00b7N\n(14)\nWhile \u03b2\u2217\u2212 = \u2212\u03b2\u2217+, the objective function in Eq 14 of the one-negative class SVM inspires us with the idea to adjust the kernel-classifier parameters to increase separability of the unseen kernel-classifier against the points of the seen classes, which leads to the following objective function\n\u03a6(t\u2217) = \u03b2\u0302(t\u2217) =argmin \u03b2\n[ \u03b2TK \u2032 \u03b2 \u2212 \u03b6\u03b2\u0302DT (t\u2217) TK \u2032 \u03b2 + \u03b2Ta ] s.t. : \u03b2T1 = \u22121, \u03b2\u0302 T DTK \u2032 \u03b2 > l,\u2212C \u2264 \u03b2i \u2264 0; \u2200i C, \u03b6, l : hyper-parameters, (15)\nwhere \u03b2\u0302DT is the first N elements in \u03b2\u0303DT (t \u2217) \u2208 RN+1, 1 is an N \u00d7 1 vector of ones. The objective function, in Eq 8, pushes the classifier of the unseen class to be highly correlated with the domain transfer prediction of the kernel classifier, while putting the points of the seen classes as negative examples. It is not hard to see that Eq 15 is a quadratic program in \u03b2, which could be solved using any quadratic solver. It is worth to mention that linear classifier prediction in Eq 8 (best Linear formulation in our results) predicts classifiers by solving an optimization problem of size N+dv+1 variables, dv+1 linearclassifier parameters and N slack variables. In contrast, the kernelized objective function (Eq 15) solves a quadratic program of only N variables, and predicts a kernel-classifier instead with fewer parameters. Using very high-dimensional features will not affect the optimization complexity."}, {"heading": "VII. DISTRIBUTIONAL SEMANTIC (DS) KERNEL FOR TEXT DESCRIPTIONS", "text": "We propose a distributional semantic kernel g(\u00b7, \u00b7) = gDS(\u00b7, \u00b7) to define the similarity between two text descriptions in T domain. While this kernel is applicable to kernel classifier predictors presented in Sec VI, it could be used for other applications. We start by distributional semantic models in [41], [52] to represent the semantic manifold Ms, and a function vec(\u00b7) that maps a word to a K\u00d71 vector inMs. The main assumption behind this class of distributional semantic model is that similar words share similar context. Mathematically speaking, these models learn a vector for each word wn, such that p(wn|(wn\u2212L, wn\u2212L+1, \u00b7 \u00b7 \u00b7 , wn+L\u22121, wn+L) is maximized over the training corpus, where 2\u00d7L is the context window size. Hence similarity between vec(wi) and vec(wj) is high if they co-occurred a lot in context of size 2 \u00d7 L in the training text-corpus. We normalize all the word vectors to length 1 under L2 norm, i.e., \u2016vec(\u00b7)\u20162 = 1.\nLet us assume a text description D that we represent by a set of triplets D = {(wl, fl, vec(wl)), l = 1 \u00b7 \u00b7 \u00b7M}, where wl is a word that occurs in D with frequency fl and its corresponding word vector is vec(wl) in Ms. We drop the stop words from D. We define F = [f1, \u00b7 \u00b7 \u00b7 , fM ]T and P = [vec(w1), \u00b7 \u00b7 \u00b7 , vec(wM )]T, where F is an M \u00d7 1 vector of term frequencies and P is an M \u00d7K matrix of the corresponding term vectors.\nGiven two text descriptions Di and Dj which contain Mi and Mj terms respectively. We compute Pi (Mi \u00d7 1) and Vi (Mi\u00d7K) for Di and Pj (Mj \u00d7 1) and Vj (Mj \u00d7K) for Dj . Finally gDS(Di, Dj) is defined as\ngDS(Di, Dj) = FTi PiP T jFj (16)\nOne advantage of this similarity measure is that it captures semantically related terms. It is not hard to see that the standard Term Frequency (TF) similarity could be thought of as a special case of this kernel where vec(wl)Tvec(wm) = 1 if wl = wm, 0 otherwise, i.e., different terms are orthogonal. However, in our case the word vectors are learnt through a distributional semantic model which makes semantically related terms have higher dot product (vec(wl)Tvec(wm))."}, {"heading": "VIII. EXPERIMENTS", "text": ""}, {"heading": "A. Datasets and Features", "text": "Datasets: We evaluated our methods using two large datasets, widely used for fine-grained categorization: CU200 Birds [15] dataset (200 classes - 6033 images) and the Oxford Flower102 [9] dataset (102 classes - 8189 images). We augmented these datasets with a textual description of each category. The CUB200 Birds image dataset was created based on birds that have a corresponding Wikipedia article, so we have developed a tool to automatically extract Wikipedia articles given the class name. The tool succeeded to automatically generate 178 articles, and the remaining 22 articles was extracted manually from Wikipedia. These mismatches happen when article title is a different synonym of the same bird class. On the other hand, for Flower dataset, the tool managed to generate only 16 classes from Wikipedia out of 102 since the Flower classes do not necessarily have corresponding Wikipedia articles. The\n9 remaining 86 articles were generated manually for each class from Wikipedia, Plant Database 5, Plant Encyclopedia 6, and BBC articles 7. The collected textual descriptions for Flowers and Birds datasets are available here https://sites.google.com/ site/mhelhoseiny/1202-Elhoseiny-sup.zip . Textual Feature Extraction: The textual features were extracted in two phases. The first phase is an indexing phase that generates textual features with tf-idf (Term FrequencyInverse Document Frequency) configuration (Term frequency as local weighting while inverse document frequency as a global weighting). The tf-idf is a measure of how important a word is to a text corpus. The tf-idf value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to control for the fact that some words are generally more common than others. We used the normalized frequency of a term in the given textual description [53]. The inverse document frequency is a measure of whether the term is common; in this work we used the standard logarithmic idf [53]. The second phase is a dimensionality reduction step, in which Clustered Latent Semantic Indexing (CLSI) algorithm [54] is used. CLSI is a low-rank approximation approach for dimensionality reduction, used for document retrieval. In the Flower Dataset, tf-idf features \u2208 R8875 and after CLSI the final textual features \u2208 R102. In the Birds Dataset, tf-idf features is in R7086 and after CLSI the final textual features is in R200. Visual features Extraction: We used the Classemes features [55] as the visual feature for our experiments, where they provide an intermediate semantic representation of the input image. Classemes features are output of a set of classifiers corresponding to a set of C category labels, which are drawn from an appropriate term list defined in [55], and not related to our textual features. For each category c \u2208 {1 \u00b7 \u00b7 \u00b7C}, a set of training images is gathered by issuing a query on the category label to an image search engine. After a set of coarse feature descriptors (Pyramid HOG, GIST, etc.) is extracted, a subset of feature dimensions was selected [55], and a one-versus-all classifier \u03d5c is trained for each category. The classifier output is real-valued, and is such that \u03d5c(x) > \u03d5c(y) implies that x is more similar to class c than y is. Given an image x, the feature vector (descriptor) used to represent it is the Classemes vector [\u03d51(x), \u00b7 \u00b7 \u00b7 , \u03d5dv (x)], dv = 2569.\nFor Kernel classifier prediction, we evaluated these features and also additional representations for text descriptions and images. For text, we performed experiments with the proposed distributional semantic kernel and using Recurrent Nets. For images, we evaluated (a) CNN features and (b), combined kernel over different features learnt by MKL (multiple kernel learning)). Details are discussed later in Subsection VIII-C."}, {"heading": "B. Experimental Results for Linear Classifier Prediction", "text": "Evaluation Methodology: Following zero-shot learning literature, we evaluated the performance of an unseen classifier in a one-vs-all setting where the test images of unseen classes are considered to be the positives and the test images from the\n5http://plants.usda.gov/java/ 6http://www.theplantencyclopedia.org/wiki/Main Page 7http://www.bbc.co.uk/science/0/\nseen classes are considered to be the negatives. We computed the ROC curve and report the area under that curve (AUC) as a comparative measure of different approaches. In zeroshot learning setting the test data from the seen classes are typically very large compared to those from unseen classes. This makes other measures, such as accuracy, useless since high accuracy can be obtained even if all the unseen class test data are classified incorrectly; hence we used ROC curves, which are independent of this problem. Training/Testing ZSL Splits Super Category Unseen (SC-Unseen) Split). This is Zeroshot setting split for both CUB and Flower Datasets (first defined in our work [13]). Five-fold cross validation over the classes were performed, where in each fold 4/5 of the classes are considered as \u201cseen classes\u201d and are used for training and 1/5th of the classes were considered as \u201cunseen classes\u201d where their classifiers are predicted and tested. Within each of these class-folds, the data of the seen classes are further split into training and test sets. The hyperparameters for the approach were selected through another five-fold cross validation within the class-folds (i.e. the 80% training classes are further split into 5 folds to select the hyper-parameters). We made the seen-unseen folds used in our experiments available here https://sites.google.com/site/ mhelhoseiny/computer-vision-projects/Write a Classifier. In contrast to the SC-seen split, discussed next, this split was designed such that bird subspecies that belong to the same super-category should either belong to either the training or the test split. Super Category Seen (SC-Seen) Split a (150-50) Split on CUB 2011 dataset [56]: We also evaluate our work on another zero-shot learning split for CUB 2011 dataset, which is used in some recent works (e.g, [56], [57]). We investigated the difference between this training/testing split and found that most of the unseen/test classes in split defined in [56] are actually seen in some-perspective. In particular, we found a common feature in this split is that for each group of related subordinate categories, the majority of the group subspecies is used during training and one of them is left as unseen. For instance, all subspecies of Albatrosses are included among the training classes except one for testing (i.e., training on Laysan_Albatross and Sooty_Albatross, and testing on Black_footed_Albatross). At test time, a zero-shot learning model is asked to discriminate between Black_footed_Albatross and other classes that are not related to Albatross which is relatively easier given that the model has seen already Albatrosses during training. Hence, we name this split Super Category Seen(SC-Seen) Split. Instead in our Super Category Unseen (SC-Unseen) Split, the whole set of albatrosses and other unseen subordinate categories are completely unseen and at test time the model is asked to discriminate between different types of Albatrosses from just their text. This make the SC-Unseen split much more difficult than SC-Seen split. All of our CUB dataset was based on 2010 version (with 6033 images) and on the SC-Unseen split and Wikipedia Articles from 2012. In order to show our results in comparison with some recently published work, we applied our methods on the SC-Seen Split discuss our findings in Sec VIII-F).\n10\nBaselines: Since our work is the first to predict classifiers based on pure textual description, there are no other reported results to compare against. However, for further comparisons we designed three state-of-the-art baselines to compare against, which are designed to be inline with our argument in Sec III. Namely we used: 1) A Gaussian Process Regressor (GPR) [44], 2) Twin Gaussian Process (TGP) [49] as a structured regression method, 3) Domain Transfer (DT) [47]. The TGP and DT baselines are of particular importance since they are incorporated in our formulation. It has to be noted that we also evaluate TGP and DT as alternative formulations that we are proposing for the problem, none of them was used in the same context before.\nResults: Table II shows the average AUCs for the final linear approach in comparison to the three baselines on both datasets. GPR performed poorly in all classes in both data sets, which was expected since it is not a structure prediction approach. The DT formulation outperformed TGP in the flower dataset but slightly underperformed on the Bird dataset. The proposed approach outperformed all the baselines on both datasets, with significant improvement on the flower dataset. It is also clear that the TGP performance was improved on the Bird dataset since it has more classes (more points are used for prediction). Fig 4 shows the ROC curves for our approach on best predicted unseen classes from the Birds dataset on the Left and Flower dataset on the middle. Fig 5 shows the AUC for all the classes on Flower dataset.\nFig 4, on the right, shows the improvement over (A) GPR, A(TGP), and (C) DT for each class, where the improvement is calculated as (our AUC- baseline AUC)/ baseline AUC %. Table III shows the percentage of the classes which our approach makes a prediction improvement for each of the three baselines. Table IV shows the five classes in Flower dataset\nObject Class Index\nwhere our approach made the best average improvement. This table shows that in these cases both TGP and DT performed poorly while our formulation that is based on both of them has a significant improvement. This shows that our formulation does not simply combine the best of the two approaches but can significantly improve the prediction performance.\nTo evaluate the effect of the constraints in the objective function in Eq 8, we removed the constraints \u2212(cTxi) \u2265 \u03b6i which enforces all the seen examples to be on the negative side of the predicted classifier hyperplane and evaluated the approach. The result on the flower dataset (using one fold) was reduced to average AUC=0.59 compared to AUC=0.65 with the constraints. Similarly, we evaluated the effect of the constraint tT\u2217Wc \u2265 l. The result was reduced to average AUC=0.58 compared to AUC=0.65 with the constraint. This illustrates the importance of this constraint in the formulation.\n11\nConstrained Baselines: Table II also shows the average AUCs for the constrained baseline formulations, namely (B) Constrained GPR Regression, (B) Constrained TGP Regression and (D) Constrained DT; see section V. As previously discussed, GPR performed poorly, while, as expected, TGP performed better. Adding constraints to GPR/TGP improved their performance. Combining regression and DT gave significantly better results for classes where both approaches individually perform poorly, as can be seen in Table II. We performed an additional experiment, where W is computed using Constrained Domain Transfer (CDT). Then, the unseen classifier is predicted using equation 8 with \u03b3 = 0, which performs worse. This indicates that adding constraints to align to seen classifiers hurts the learnt domain transfer function on unseen classes. In conclusion, the final formulation (Eq 8) that combines TGP and DT with additional constraints performs the best in both Birds and Flower datasets. The effect of TGP is very limited since it was trained on sparse points which is reflected in the setting of \u03b1 (weight for DT) and \u03b3 (weight for TGP) to 100 and 1 respectively after hyper parameter tuning on a validation set."}, {"heading": "C. Experimental Results for Kernel Classifier Prediction", "text": "1) Additional Evaluation Metrics: In addition to the AUC, discussed in the previous section, we report two additional metrics while evaluating and comparing the kernel classifier prediction to the linear classifier prediction, detailed as follows |Nsc| to |Nsc + 1|Recall: this metric check how the learned classifiers of the seen classes confuse the predicted classifiers, when they are involved in a multi-class classification problem of Nsc + 1 classes. We use Eq 1 to predict label l\u2217 with the maximum confidence of an image x\u2217, such that l\u2217 \u2208 Lsc \u222a lus, lus is the label of the ground truth unseen class, and Lsc is the set of seen class labels. We compute the recall under this setting. This metric is computed for each predicted unseen classifier and the average is reported.\nMulticlass Accuracy of Unseen classes (MAU): Under this setting, we aim to evaluate the performance of the unseen classifiers against each others. Firstly, the classifiers of all unseen categories are predicted. Then, we use Eq 1 to predict the label with the maximum confidence of a test image x, such that its label l\u2217us \u2208 Lus, where Lus is the set of all unseen class labels that only have text descriptions.\n2) Comparisons to Linear Classifier Prediction: We compare the kernel methods to the linear prediction discussed earlier, which predicts a linear classifier from textual descriptions ( T space in our framework). The goal is to check whether the predicted kernelized classifier outperforms the predicted linear classifier. We used the same features on the visual domain and the textual domains detailed in subsection VIII-A.\nWe denote our kernel Domain Transfer prediction and one class SVM adjusted DT prediction presented in Section VI-B\nby \u201cDT-kernel\u201d and \u201cSVM-DT-kernel\u201d respectively. We compared against linear classifier prediction (Linear Formulation (E) approach, denoted by just Linear Classifier). We also compared against the linear direct domain transfer (Linear Formulation (C), denoted by DT-linear). In our kernel approaches, we used Gaussian rbf-kernel as a similarity measure in T and V spaces (i.e.k(d, d\u2032) = exp(\u2212\u03bb||d\u2212 d\u2032||)).\nRecall metric : The recall of the SVM-DT kernel approach is 44.05% for Birds dataset and 40.34% for Flower dataset, while it is 36.56% for Birds and 31.33% for Flower by best Linear Classifier prediction (E). This indicates that the predicted classifier is less confused by the classifiers of the seen categories compared with Linear Classifier prediction; see table V (top part)\nMAU metric: It is worth to mention that the multiclass accuracy for the trained seen classifiers is 51.3% and 15.4%, using the classeme features, on Flower dataset and Birds dataset8, respectively. Table V (middle part) shows the average MAU metric over three seen/unseen splits for Flower dataset and one split on Birds dataset, respectively. Furthermore, the relative improvements of our SVM-DT-kernel approach is reported against the baselines. On Flower dataset, it is interesting to see that our approach achieved 9.1% MAU, showing 182% improvement over random guess, by predicting the unseen classifiers using just textual features as privileged information (i.e. T domain). It is important to mention that we achieved also 13.4%( 268% improvement over random guess), in one of the splits (the 9.1% is the average over 3 seen/unseen splits). Similarly on Birds dataset, we achieved 3.4% MAU from text features, 132% the random guess performance (further improved up to 224% in next experiments). In addition to the unseen class performance, we report the performance on seen classes as an upper bound of zero-shot learning for both Flower (50.7%) and Birds datasets (16%).\nAUC metric: Table V (bottom part) shows the average AUC on the two datasets, compared to the baselines. More results and figures for our kernel approach are attached in the supplementary materials.\nLooking at table V, notice that the proposed approach performs marginally similar to some baselines from AUC perspective. However, there is a clear improvement in MAU and Recall metrics. These results show the advantage of predicting classifiers in kernel space. Furthermore, the table shows that our SVM-DT-kernel approach outperforms our DT-kernel model. This indicates the advantage of the class separation, which is adjusted by the SVM-DT-kernel model.\n3) Multiple Kernel Learning (MKL) Experiment: This experiment shows the added value of proposing a kernelized zero-shot learning approach. We conducted an experiment where the final kernel on the visual domain is produced by Multiple Kernel Learning [60]. For the images, we extracted kernel descriptors for Birds dataset. Kernel descriptors provide a principled way to turn any pixel attribute to patch-level features, and are able to generate rich features from various recognition cues. We specifically used four types of kernels introduced by [61] as follows: Gradient Match Kernels that captures image variation based on predefined kernels on image\n8Birds dataset is known to be a challenging dataset for fine-grained with engineered-features\n12\ngradients. Color Match Kernel that describes patch appearance using two kernels on top of RGB and normalized RGB for regular images and intensity for grey images. These kernels capture image variation and visual apperances. For modeling the local shape, Local Binary Pattern kernels have been applied. We computed these kernel descriptors on local image patches with fixed size 16 x 16 sampled densely over a grid with step size 8 in a spatial pyramid setting with four layers. The dense features are vectorized using codebooks of size 1000. This process ended up with a 120,000 dimensional feature for each image (30,000 for each type). Having extracted the four types of descriptors, we compute an rbf kernel matrix for each type separately. We learn the bandwidth parameters for each rbf kernel by cross validation on the seen classes. Then, we generate a new kernel kmkl(d, d\u2032) = \u22114 i=1 wiki(d, d\n\u2032), such that wi is a weight assigned to each kernel. We learn these weights by applying Bucak\u2019s Multiple Kernel Learning algorithm [62]. Then, we applied our approach where the MKL-kernel is used in the visual domain and rbf kernel in the text domain similar to the previous experiments.\nTo compare the kernel prediction approach to the linear prediction approach (formulation (E)) under this setting, we concatenated all kernel descriptors to form a 120,000 dimensional feature vector in the visual domain. As highlighted in the kernel approach section, the linear prediction approach solves a quadratic program of N + dv + 1 variables for each unseen class. Due to the large dimensionality of data (dv = 120, 000), this is not tractable. To make this setting applicable, we reduced the dimensionality of the feature vector into 4000 using PCA. This highlights the benefit of the kernelized approach since the quadratic program in Eq 15 does not depend on the dimensionality of the feature space. Table VI shows MAU for the kernel prediction approaches under this setting against linear prediction. The results show the benefit of zero-shot kernel prediction where an arbitrary kernel could be used to improve the performance."}, {"heading": "D. Multiple Representation Experiment and Distributional Semantic(DS) Kernel", "text": "This experiment elaborates the performance of kernel approach on different representations of text T and visual domains V . In this experiment, we extracted Convolutional Neureal Network(CNN) image features for the Visual domain. We used caffe [63] implementation of [64]. Then, we extracted\nthe sixth activation feature of the CNN (FC6) since we found it works the best on the standard classification setting. We found this consistent with the results of [65] over different CNN layers. While using TFIDF feature of text description and CNN features for images, we achieved 2.65% for the linear version and 4.2% for the rbf kernel on both text and images. We further improved the performance to 5.35% by using our proposed Distributional Semantic (DS) kernel in the text domain and rbf kernel for images. In this DS experiment, we used the distributional semantic model by [41] trained on GoogleNews corpus (100 billion words) resulting in a vocabulary of size 3 million words, and word vectors of K = 300 dimensions. This experiment shows the value of both the kernelized approach and the proposed kernel in Sec VII. We also applied the zero shot learning approach in [59] which has the lowest performance in our settings; see Table VII.\nAttributes Experiment: Our goal is not attribute prediction. However, it was interesting to see the behavior of our method where T space is defined from attributes instead of text. In contrast to attribute-based models, which fully utilize attribute information to build attribute classifiers, we do not learn attribute classifiers. In this experiment, our method uses only the first moment of information of the attributes (i.e. the average attribute vector). We decided to compare to an attribute-based approach from this perspective. In particular, we applied the DAP attribute-based model [66], [8] to the Birds dataset, which is widely adopted in many applications (e.g., [67], [68]). For visual domain, we used classeme features in this experiment (presented in table V experiments.\nAn interesting result is that our approach achieved 5.6% MAU (224% the random guess performance); see Table VIII. In contrast, we get 4.8% multiclass accuracy using DAP approach [66]. In this setting, we also measured the Nsc to Nsc + 1 average recall. We found the recall measure is 76.7% for our SVM-DT-kernel, while it is 68.1% on the DAP approach, which reflects better true positive rate (positive class is the unseen one). Most importantly, we achieved these results without learning any attribute classifiers, as in [66]. When comparing the results of our approach using attributes (Table VIII) vs. textual description (Table V)9 as the T space used for prediction, it is clear that the attribute features give better predictions. This support our hypothesis that the\n9We are refering to the experiment that uses classeme as visual features to have a consistent comparison to here\nTABLE VI: Kernel: MAU on a seenunseen split-Birds Dataset (MKL)\nMAU SVM-DT kernel-rbf (text) 4.10 %\nLinear Classifier 2.74\nTABLE VII: Kernel: MAU on a seenunseen split-Birds Dataset (CNN image features, text description)\nMAU SVM-DT kernel (V-rbf, T -DS kernel) 5.35 % SVM-DT kernel (V-rbf, T -rbf on TFIDF) 4.20 % Order Embedding [58] 3.3 %\nLinear Classifier (TFIDF text) 2.65 % [59] 2.3%\nAcc (all classes seen) 45.6%\nTABLE VIII: Kernel: Recall and MAU on a seen-unseen split-Birds Dataset (Attributes)\nRecall SVM-DT kernel-rbf 76.7 % Lampert DAP [8] 68.1 %\nMAU SVM-DT kernel-rbf 5.6 %\nDT kernel-rbf 4.03 % Lampert DAP [8] 4.8 %\n13\nmore meaningful the T domain, the better the performance on V domain. This indicates that if a better textual representation is used, a better performance can be achieved. Attributes are good semantic representations of classes yet it is difficult to define attributes for an arbitrary class and further measure the confidence of each one. In contrast, it is much easier to find an unstructured text description for visual classes."}, {"heading": "E. Experiments using deep image-sentence similarity and more recent Zero-shot learning methods", "text": "We used a state of the art model [58] for image-sentence similarity by breaking down each text document into sentences and considering it as a positive sentence for all images in the corresponding class. Then we measure the similarities between an image to class by averaging its similarity to all sentences in that class. Images were encoded using VGGNet [69] and sentences were encoded by an RNN with GRU activations [33]. The MAU on Birds dataset for this experiments resulted in 3.3% MAU which is better that the Linear Classifier in Table VII. However, our kernel method (Eq 15) over deep features is still performing 2.03% better (i.e. 5.35% MAU)."}, {"heading": "F. SC-Seen Split on CUB 2011 [70]", "text": "We report the zero-shot performance on the SC-Seen (Super Category Seen) split, detailed in subsection A. We applied both our linear and kernel method and compare against recently published results in our setting [57], [58], [71], [56]. We performed all the experiments in the previous sections (best zero-shot performance CUB dataset on SC-Unseen split is 5.35% on SC-Unseen split designed in [13]. It is not hard to see that the performance of our methods (both linear and kernel) on SC-Seen split is significantly better than SC-Unseen split designed in [13]. Our kernel approach results on SC-Seen split is 33.5% which is the best performing methods as shown in table IX. When we used a binary version of Term Frequency (each word has 1 if exist, 0 otherwise), our performance is 26.5%. This big performance gap shows how challenging is SC-Unseen (Super Category Unseen) split compared to the SC-Seen split. It is important to mention that the assumption of using existing images as negative examples is not valid on this split. Hence, we did not enforce this constraint on SCSeen Split (constraints in Eq. 8 for linear and in Eq. 15 for the kernel version). Hence, the prediction is dominated by our Domain Transfer function. When we added these constraints our performance goes down from 33.5% to 8% which is expected due to the incorrectness of the assumption on this split. Based on this result, we encourage future researchers on this problem to report the performance on both SC-Unseen Split and SC-Seen Split, where we showed that SC-Unseen Split is more challenging."}, {"heading": "IX. CONCLUSION", "text": "We explored the problem of predicting visual classifiers from textual description of classes with no training images. We investigated and experimented with different formulations for the problem within the fine-grained categorization context. We first proposed a novel formulation that captures information between the visual and textual domains by involving knowledge transfer from textual features to visual features, which indirectly leads to predicting a linear visual classifier described\nby the text. We also proposed a new zero-shot learning technique to predict kernel-classifiers of unseen categories using information from a privilege space. We formulated the problem as domain transfer function from text description to the visual classification space, while supporting kernels in both domains. We proposed a one-class SVM adjustment to our domain transfer function in order to improve the prediction. We validated the performance of our model by several experiments. We illustrated the value of proposing a kernelized version by applying kernels generated by Multiple Kernel Learning (MKL) and achieved better results. In the future, we aim to improve this model by learning the unseen classes jointly and on a larger scale. Acknowledgment. This research was funded by NSF award IIS-1409683 and IIS-1218872."}], "references": [{"title": "Learning to share visual appearance for multiclass object detection", "author": ["R. Salakhutdinov", "A. Torralba", "J.B. Tenenbaum"], "venue": "CVPR, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "The caltech-ucsd birds-200-2011 dataset", "author": ["C. Wah", "S. Branson", "P. Welinder", "P. Perona", "S. Belongie"], "venue": "2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Avibase", "author": ["D. Lepage"], "venue": "http://avibase.bsc-eoc.org/, 2016, [Online; accessed 19-July-2016].", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "All About Birds16", "author": ["S. Belongie", "P. Perona", "G. Van Horn", "S.B."], "venue": "info.allaboutbirds.org/nabirds, 2016, [Online; accessed 31-July-2016].", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "What does classifying more than 10,000 image categories tell us?", "author": ["J. Deng", "A.C. Berg", "K. Li", "L. Fei-Fei"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "A bayesian approach to unsupervised one-shot learning of object categories", "author": ["L. Fe-Fei", "R. Fergus", "P. Perona"], "venue": "CVPR. IEEE, 2003, pp. 1134\u20131141.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Cross-generalization: Learning novel classes from a single example by feature replacement", "author": ["E. Bart", "S. Ullman"], "venue": "CVPR, 2005.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning to detect unseen object classes by betweenclass attribute transfer", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "In CVPR, 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Automated flower classification over large number of classes", "author": ["M.-E. Nilsback", "A. Zisserman"], "venue": "ICVGIP, 2008.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning models for object recognition from natural language descriptions.", "author": ["J. Wang", "K. Markert", "M. Everingham"], "venue": "in BMVC,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Dog breed classification using part localization", "author": ["J. Liu", "A. Kanazawa", "D. Jacobs", "P. Belhumeur"], "venue": "ECCV, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Finegrained visual classification of aircraft", "author": ["S. Maji", "E. Rahtu", "J. Kannala", "M. Blaschko", "A. Vedaldi"], "venue": "arXiv preprint arXiv:1306.5151, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Write a classifier: Zero shot learning using purely text descriptions", "author": ["M. Elhoseiny", "B. Saleh", "A. Elgammal"], "venue": "ICCV, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "A generalized representer theorem", "author": ["B. Sch\u00f6lkopf", "R. Herbrich", "A.J. Smola"], "venue": "COLT, 2001.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Caltech-UCSD Birds 200", "author": ["P. Welinder", "S. Branson", "T. Mita", "C. Wah", "F. Schroff", "S. Belongie", "P. Perona"], "venue": "California Institute of Technology, Tech. Rep., 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning from one example through shared densities on transforms", "author": ["E.G. Miller", "N.E. Matsakis", "P.A. Viola"], "venue": "CVPR, 2000.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Object classification from a single example utilizing class relevance metrics", "author": ["M. Fink"], "venue": "NIPS, 2004.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Zeroshot learning with semantic output codes", "author": ["M. Palatucci", "D. Pomerleau", "G.E. Hinton", "T.M. Mitchell"], "venue": "NIPS, 2009.  14", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I. Endres", "D. Hoiem", "D.A. Forsyth"], "venue": "CVPR, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Combining language sources and robust semantic relatedness for attribute-based knowledge transfer", "author": ["M. Rohrbach", "M. Stark", "G. Szarvas", "B. Schiele"], "venue": "Parts and Attributes Workshop at ECCV, 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Interactively building a discriminative vocabulary of nameable attributes", "author": ["D. Parikh", "K. Grauman"], "venue": "CVPR, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning hypergraph-regularized attribute predictors", "author": ["S. Huang", "M. Elhoseiny", "A. Elgammal", "D. Yang"], "venue": "CVPR, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning and using taxonomies for fast visual categorization", "author": ["G. Griffin", "P. Perona"], "venue": "CVPR. IEEE, 2008, pp. 1\u20138.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Semantic label sharing for learning with many categories", "author": ["R. Fergus", "H. Bernal", "Y. Weiss", "A. Torralba"], "venue": "ECCV, 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Wordnet: A lexical database for english", "author": ["G.A. Miller"], "venue": "COMMUNI- CATIONS OF THE ACM, 1995.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1995}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "CVPR, 2009.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["A. Torralba", "R. Fergus", "W.T. Freeman"], "venue": "PAMI, 2008.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Clustering art", "author": ["K. Barnard", "P. Duygulu", "D. Forsyth"], "venue": "CVPR, 2001.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2001}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "ECCV, 2010.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "CVPR, 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Corpus-guided sentence generation of natural images", "author": ["Y. Yang", "C.L. Teo", "H. Daum\u00e9 III", "Y. Aloimonos"], "venue": "EMNLP, 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Generating natural-language video descriptions using text-mined knowledge", "author": ["N. Krishnamoorthy", "G. Malkarnenkar", "R. Mooney", "K. Saenko", "U. Lowell", "S. Guadarrama"], "venue": "NAACL HLT 2013, p. 10, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "EMNLP, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR, 2015.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "2015.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "ICML, 2015.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A. Yuille"], "venue": "ICLR, 2015.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": "NIPS, 2011.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Devise: A deep visual-semantic embedding model.", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "M. Ranzato", "T. Mikolov"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "Zero shot learning through cross-modal transfer", "author": ["R. Socher", "M. Ganjoo", "H. Sridhar", "O. Bastani", "C.D. Manning", "A.Y. Ng"], "venue": "NIPS, 2013.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS, 2013.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Zeroshot event detection by multimodal distributional semantic embedding of videos", "author": ["M. Elhoseiny", "J. Liu", "H. Cheng", "H. Sawhney", "A. Elgammal"], "venue": "AAAI, 2016.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Ridge Regression: Biased Estimation for Nonorthogonal Problems", "author": ["A.E. Hoerl", "R.W. Kennard"], "venue": "Technometrics, 1970.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1970}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2005}, {"title": "Cross-domain video concept detection using adaptive svms", "author": ["J. Yang", "R. Yan", "A.G. Hauptmann"], "venue": "MULTIMEDIA, 2007.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2007}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "ECCV, 2010.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "What you saw is not what you get: Domain adaptation using asymmetric kernel transforms", "author": ["B. Kulis", "K. Saenko", "T. Darrell"], "venue": "CVPR, 2011.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}, {"title": "Visual event recognition in videos by learning from web data", "author": ["L. Duan", "D. Xu", "I.W.-H. Tsang", "J. Luo"], "venue": "TPAMI, 2012.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Twin gaussian processes for structured prediction", "author": ["L. Bo", "C. Sminchisescu"], "venue": "IJCV, 2010.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2010}, {"title": "Parallel Optimization: Theory, Algorithms, and Applications", "author": ["Y. Censor", "S. Zenios"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1997}, {"title": "Some properties of the gaussian kernel for one class learning", "author": ["P.F. Evangelista", "M.J. Embrechts", "B.K. Szymanski"], "venue": "ICANN, 2007.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2007}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR, 2013.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["G. Salton", "C. Buckley"], "venue": "IPM, 1988.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1988}, {"title": "Clsi: A flexible approximation scheme from clustered term-document matrices", "author": ["D. Zeimpekis", "E. Gallopoulos"], "venue": "SDM, 2005.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2005}, {"title": "Efficient object category recognition using classemes", "author": ["L. Torresani", "M. Szummer", "A. Fitzgibbon"], "venue": "ECCV, 2010.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2010}, {"title": "Evaluation of output embeddings for fine-grained image classification", "author": ["Z. Akata", "S. Reed", "D. Walter", "H. Lee", "B. Schiele"], "venue": "CVPR, 2015.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2015}, {"title": "Less is more: zeroshot learning from online textual documents with noise suppression", "author": ["R. Qiao", "L. Liu", "C. Shen", "A. v. d. Hengel"], "venue": "CVPR, 2016.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2016}, {"title": "Order-embeddings of images and language", "author": ["I. Vendrov", "R. Kiros", "S. Fidler", "R. Urtasun"], "venue": "ICLR, 2016.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2016}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["M. Norouzi", "T. Mikolov", "S. Bengio", "Y. Singer", "J. Shlens", "A. Frome", "G.S. Corrado", "J. Dean"], "venue": "ICLR, 2014.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiple kernel learning algorithms", "author": ["M. Gonen", "E. Alpaydin"], "venue": "JMLR, 2011.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2011}, {"title": "Kernel descriptors for visual recognition", "author": ["L. Bo", "X. Ren", "D. Fox"], "venue": "NIPS, 2010.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-label multiple kernel learning by stochastic approximation: Application to visual object recognition.", "author": ["S.S. Bucak", "R. Jin", "A.K. Jain"], "venue": null, "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2010}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "ACM Multimedia, 2014.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems (NIPS), 2012.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2012}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "ICML, 2014.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2014}, {"title": "Attribute-based classification for zero-shot visual object categorization", "author": ["C. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "TPAMI, vol. 36, no. 3, pp. 453\u2013465, March 2014.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2014}, {"title": "Video event recognition using concept attributes", "author": ["J. Liu", "Q. Yu", "O. Javed", "S. Ali", "A. Tamrakar", "A. Divakaran", "H. Cheng", "H. Sawhney"], "venue": "WACV, 2013.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2013}, {"title": "Evaluating knowledge transfer and zero-shot learning in a large-scale setting", "author": ["M. Rohrbach", "M. Stark", "B. Schiele"], "venue": "CVPR, 2011.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2011}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR, 2015.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2015}, {"title": "Label-embedding for attribute-based classification", "author": ["Z. Akata", "F. Perronnin", "Z. Harchaoui", "C. Schmid"], "venue": "CVPR, 2013.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "This is reflected in the number of images per category available for training in most object categorization datasets, which, as pointed out in [1], shows a Zipf distribution.", "startOffset": 143, "endOffset": 146}, {"referenceID": 1, "context": ", CUBirds 200 dataset [2]).", "startOffset": 22, "endOffset": 25}, {"referenceID": 2, "context": ", [3], [4]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": ", [3], [4]).", "startOffset": 7, "endOffset": 10}, {"referenceID": 4, "context": "[5], [6], [7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[5], [6], [7].", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "[5], [6], [7].", "startOffset": 10, "endOffset": 13}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Diverse applications of fine-grained categories range from classification of natural species [2], [9], [10], [11] to retrieval of different types of commercial products [12].", "startOffset": 93, "endOffset": 96}, {"referenceID": 8, "context": "Diverse applications of fine-grained categories range from classification of natural species [2], [9], [10], [11] to retrieval of different types of commercial products [12].", "startOffset": 98, "endOffset": 101}, {"referenceID": 9, "context": "Diverse applications of fine-grained categories range from classification of natural species [2], [9], [10], [11] to retrieval of different types of commercial products [12].", "startOffset": 103, "endOffset": 107}, {"referenceID": 10, "context": "Diverse applications of fine-grained categories range from classification of natural species [2], [9], [10], [11] to retrieval of different types of commercial products [12].", "startOffset": 109, "endOffset": 113}, {"referenceID": 11, "context": "Diverse applications of fine-grained categories range from classification of natural species [2], [9], [10], [11] to retrieval of different types of commercial products [12].", "startOffset": 169, "endOffset": 173}, {"referenceID": 12, "context": "The contribution of the paper is on exploring this new problem, which to the best of our knowledge, is firstly explored in the computer vision community in an earlier version of this work [13].", "startOffset": 188, "endOffset": 192}, {"referenceID": 12, "context": "While a part of this work was published in [13], we extend the work here to study more formulations to solve the problem in Sec.", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "In addition, we propose a kernel method to explicitly predict a kernel classifier in the form defined in the representer theorem [14].", "startOffset": 129, "endOffset": 133}, {"referenceID": 13, "context": "Section VI presents a kernelized version of our approach where \u03a6(\u00b7) predicts a kernel classifier in the form defined by the representer theorem [14].", "startOffset": 144, "endOffset": 148}, {"referenceID": 8, "context": "Section VIII presents our experiments on Flower Dataset [9] and Caltech-UCSD dataset [15] for both the linear and the kernel classifier predictions.", "startOffset": 56, "endOffset": 59}, {"referenceID": 14, "context": "Section VIII presents our experiments on Flower Dataset [9] and Caltech-UCSD dataset [15] for both the linear and the kernel classifier predictions.", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": "Zero/Few-Shot Learning: Motivated by the practical need to learn visual classifiers of rare categories, researchers have explored approaches for learning from a single image (oneshot learning [16], [6], [17], [7]) or even from no images (zero-shot learning).", "startOffset": 192, "endOffset": 196}, {"referenceID": 5, "context": "Zero/Few-Shot Learning: Motivated by the practical need to learn visual classifiers of rare categories, researchers have explored approaches for learning from a single image (oneshot learning [16], [6], [17], [7]) or even from no images (zero-shot learning).", "startOffset": 198, "endOffset": 201}, {"referenceID": 16, "context": "Zero/Few-Shot Learning: Motivated by the practical need to learn visual classifiers of rare categories, researchers have explored approaches for learning from a single image (oneshot learning [16], [6], [17], [7]) or even from no images (zero-shot learning).", "startOffset": 203, "endOffset": 207}, {"referenceID": 6, "context": "Zero/Few-Shot Learning: Motivated by the practical need to learn visual classifiers of rare categories, researchers have explored approaches for learning from a single image (oneshot learning [16], [6], [17], [7]) or even from no images (zero-shot learning).", "startOffset": 209, "endOffset": 212}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Recognition can then proceed on the basis of these learned attributes [8], [19].", "startOffset": 70, "endOffset": 73}, {"referenceID": 18, "context": "Recognition can then proceed on the basis of these learned attributes [8], [19].", "startOffset": 75, "endOffset": 79}, {"referenceID": 7, "context": "Typically attributes [8], [19] are manually defined by humans to describe shape, color, surface material, e.", "startOffset": 21, "endOffset": 24}, {"referenceID": 18, "context": "Typically attributes [8], [19] are manually defined by humans to describe shape, color, surface material, e.", "startOffset": 26, "endOffset": 30}, {"referenceID": 19, "context": "[20] investigated extracting useful attributes from large text corpora.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "In [21], an approach was introduced for interactively defining a vocabulary of attributes that are both human understandable and visually discriminative.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "[22] relaxed the attribute independence assumption by modeling correlation between attributes to achieve better zero shot performance, as opposed to prior models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": ", [8], [19]), in our work we do not use any explicit attributes.", "startOffset": 2, "endOffset": 5}, {"referenceID": 18, "context": ", [8], [19]), in our work we do not use any explicit attributes.", "startOffset": 7, "endOffset": 11}, {"referenceID": 22, "context": "[23]; or exporting semantic knowledge at the level of category similarities and hierarchies, e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24], [1].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[24], [1].", "startOffset": 6, "endOffset": 9}, {"referenceID": 4, "context": "For example in [5], it was shown that there is a strong correlation between semantic similarity between classes, based on WordNet, and confusion between classes.", "startOffset": 15, "endOffset": 18}, {"referenceID": 24, "context": "Linguistic semantics in terms of nouns from WordNet [25] have been used in collecting large-scale image datasets such as ImageNet[26] and Tiny Images [27].", "startOffset": 52, "endOffset": 56}, {"referenceID": 25, "context": "Linguistic semantics in terms of nouns from WordNet [25] have been used in collecting large-scale image datasets such as ImageNet[26] and Tiny Images [27].", "startOffset": 129, "endOffset": 133}, {"referenceID": 26, "context": "Linguistic semantics in terms of nouns from WordNet [25] have been used in collecting large-scale image datasets such as ImageNet[26] and Tiny Images [27].", "startOffset": 150, "endOffset": 154}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "[28], which showed that learning a joint distribution of words and visual elements facilitates clustering the images in a semantic way, generating illustrative images from a caption, and generating annotations for novel images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29], [30], [31], [32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[29], [30], [31], [32].", "startOffset": 6, "endOffset": 10}, {"referenceID": 30, "context": "[29], [30], [31], [32].", "startOffset": 12, "endOffset": 16}, {"referenceID": 31, "context": "[29], [30], [31], [32].", "startOffset": 18, "endOffset": 22}, {"referenceID": 32, "context": ", [33]), impressive works has been recently proposed for image captioning (e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 33, "context": ", [34], [35], [36], [37]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 34, "context": ", [34], [35], [36], [37]).", "startOffset": 8, "endOffset": 12}, {"referenceID": 35, "context": ", [34], [35], [36], [37]).", "startOffset": 14, "endOffset": 18}, {"referenceID": 36, "context": ", [34], [35], [36], [37]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 37, "context": "[38].", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "In [39], [40], word embedding language models (e.", "startOffset": 3, "endOffset": 7}, {"referenceID": 39, "context": "In [39], [40], word embedding language models (e.", "startOffset": 9, "endOffset": 13}, {"referenceID": 40, "context": "[41]) were adopted to represent class names as vectors, which require training using a big text-corpus.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "In [42], a similar yet multimodal approach was adopted for Multimedia Event Detection in videos instead of object classification.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "According to the generalized representer theorem [14], a minimizer of a regularized empirical risk function over an RKHS could be represented as a linear combination of kernels,", "startOffset": 49, "endOffset": 53}, {"referenceID": 42, "context": "A typical regression model, such as ridge regression [43] or Gaussian Process (GP) Regression [44], learns the regressor to each dimension of the output domain (the parameters of a linear classifier) separately, i.", "startOffset": 53, "endOffset": 57}, {"referenceID": 43, "context": "A typical regression model, such as ridge regression [43] or Gaussian Process (GP) Regression [44], learns the regressor to each dimension of the output domain (the parameters of a linear classifier) separately, i.", "startOffset": 94, "endOffset": 98}, {"referenceID": 44, "context": ", images or videos from different sources [45], [46], [47], [48].", "startOffset": 42, "endOffset": 46}, {"referenceID": 45, "context": ", images or videos from different sources [45], [46], [47], [48].", "startOffset": 48, "endOffset": 52}, {"referenceID": 46, "context": ", images or videos from different sources [45], [46], [47], [48].", "startOffset": 54, "endOffset": 58}, {"referenceID": 47, "context": ", images or videos from different sources [45], [46], [47], [48].", "startOffset": 60, "endOffset": 64}, {"referenceID": 46, "context": "In particular, in [47] an approach for learning cross domain transformation was introduced.", "startOffset": 18, "endOffset": 22}, {"referenceID": 46, "context": "A particular attractive characteristic of [47], over other domain adaptation models, is that the source and target domains do not have to share the same feature spaces or the same dimensionality.", "startOffset": 42, "endOffset": 46}, {"referenceID": 46, "context": "While a totally different setting is studied in [47], it inspired us to formulate the zero-shot learning problem as a domain transfer problem.", "startOffset": 48, "endOffset": 52}, {"referenceID": 48, "context": "For these reasons, we use the Twin Gaussian Process (TGP) [49].", "startOffset": 58, "endOffset": 62}, {"referenceID": 48, "context": "TGP is given by the solution of the following non-linear optimization problem [49] 3.", "startOffset": 78, "endOffset": 82}, {"referenceID": 48, "context": "This optimization problem can be solved using a second order, BFGS quasi-Newton optimizer with cubic polynomial line search for optimal step size selection [49].", "startOffset": 156, "endOffset": 160}, {"referenceID": 46, "context": "To learn the domain transfer function W we adapted the approach in [47] as follows.", "startOffset": 67, "endOffset": 71}, {"referenceID": 46, "context": "It was shown in [47], under condition on the regularizer, that the optimal W is in the form of W\u2217 =", "startOffset": 16, "endOffset": 20}, {"referenceID": 49, "context": "Another approach that can be used to minimize L(\u03a8) is through alternating projection using Bregman algorithm [50], where \u03a8 is updated by a single constraint every iteration.", "startOffset": 109, "endOffset": 113}, {"referenceID": 50, "context": "The Lagrangian dual of the one-class SVM [51] can be written as", "startOffset": 41, "endOffset": 45}, {"referenceID": 40, "context": "We start by distributional semantic models in [41], [52] to represent the semantic manifold Ms, and a function vec(\u00b7) that maps a word to a K\u00d71 vector inMs.", "startOffset": 46, "endOffset": 50}, {"referenceID": 51, "context": "We start by distributional semantic models in [41], [52] to represent the semantic manifold Ms, and a function vec(\u00b7) that maps a word to a K\u00d71 vector inMs.", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "Datasets: We evaluated our methods using two large datasets, widely used for fine-grained categorization: CU200 Birds [15] dataset (200 classes - 6033 images) and the Oxford Flower102 [9] dataset (102 classes - 8189 images).", "startOffset": 118, "endOffset": 122}, {"referenceID": 8, "context": "Datasets: We evaluated our methods using two large datasets, widely used for fine-grained categorization: CU200 Birds [15] dataset (200 classes - 6033 images) and the Oxford Flower102 [9] dataset (102 classes - 8189 images).", "startOffset": 184, "endOffset": 187}, {"referenceID": 52, "context": "We used the normalized frequency of a term in the given textual description [53].", "startOffset": 76, "endOffset": 80}, {"referenceID": 52, "context": "The inverse document frequency is a measure of whether the term is common; in this work we used the standard logarithmic idf [53].", "startOffset": 125, "endOffset": 129}, {"referenceID": 53, "context": "The second phase is a dimensionality reduction step, in which Clustered Latent Semantic Indexing (CLSI) algorithm [54] is used.", "startOffset": 114, "endOffset": 118}, {"referenceID": 54, "context": "Visual features Extraction: We used the Classemes features [55] as the visual feature for our experiments, where they provide an intermediate semantic representation of the input image.", "startOffset": 59, "endOffset": 63}, {"referenceID": 54, "context": "Classemes features are output of a set of classifiers corresponding to a set of C category labels, which are drawn from an appropriate term list defined in [55], and not related to our textual features.", "startOffset": 156, "endOffset": 160}, {"referenceID": 54, "context": ") is extracted, a subset of feature dimensions was selected [55], and a one-versus-all classifier \u03c6c is trained for each category.", "startOffset": 60, "endOffset": 64}, {"referenceID": 12, "context": "This is Zeroshot setting split for both CUB and Flower Datasets (first defined in our work [13]).", "startOffset": 91, "endOffset": 95}, {"referenceID": 55, "context": "Super Category Seen (SC-Seen) Split a (150-50) Split on CUB 2011 dataset [56]: We also evaluate our work on another zero-shot learning split for CUB 2011 dataset, which is used in some recent works (e.", "startOffset": 73, "endOffset": 77}, {"referenceID": 55, "context": "g, [56], [57]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 56, "context": "g, [56], [57]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 55, "context": "We investigated the difference between this training/testing split and found that most of the unseen/test classes in split defined in [56] are actually seen in some-perspective.", "startOffset": 134, "endOffset": 138}, {"referenceID": 43, "context": "Namely we used: 1) A Gaussian Process Regressor (GPR) [44], 2) Twin Gaussian Process (TGP) [49] as a structured regression method, 3) Domain Transfer (DT) [47].", "startOffset": 54, "endOffset": 58}, {"referenceID": 48, "context": "Namely we used: 1) A Gaussian Process Regressor (GPR) [44], 2) Twin Gaussian Process (TGP) [49] as a structured regression method, 3) Domain Transfer (DT) [47].", "startOffset": 91, "endOffset": 95}, {"referenceID": 46, "context": "Namely we used: 1) A Gaussian Process Regressor (GPR) [44], 2) Twin Gaussian Process (TGP) [49] as a structured regression method, 3) Domain Transfer (DT) [47].", "startOffset": 155, "endOffset": 159}, {"referenceID": 59, "context": "We conducted an experiment where the final kernel on the visual domain is produced by Multiple Kernel Learning [60].", "startOffset": 111, "endOffset": 115}, {"referenceID": 60, "context": "We specifically used four types of kernels introduced by [61] as follows: Gradient Match Kernels that captures image variation based on predefined kernels on image", "startOffset": 57, "endOffset": 61}, {"referenceID": 61, "context": "We learn these weights by applying Bucak\u2019s Multiple Kernel Learning algorithm [62].", "startOffset": 78, "endOffset": 82}, {"referenceID": 62, "context": "We used caffe [63] implementation of [64].", "startOffset": 14, "endOffset": 18}, {"referenceID": 63, "context": "We used caffe [63] implementation of [64].", "startOffset": 37, "endOffset": 41}, {"referenceID": 64, "context": "We found this consistent with the results of [65] over different CNN layers.", "startOffset": 45, "endOffset": 49}, {"referenceID": 40, "context": "In this DS experiment, we used the distributional semantic model by [41] trained on GoogleNews corpus (100 billion words) resulting in a vocabulary of size 3 million words, and word vectors of K = 300 dimensions.", "startOffset": 68, "endOffset": 72}, {"referenceID": 58, "context": "We also applied the zero shot learning approach in [59] which has the lowest performance in our settings; see Table VII.", "startOffset": 51, "endOffset": 55}, {"referenceID": 65, "context": "In particular, we applied the DAP attribute-based model [66], [8] to the Birds dataset, which is widely adopted in many applications (e.", "startOffset": 56, "endOffset": 60}, {"referenceID": 7, "context": "In particular, we applied the DAP attribute-based model [66], [8] to the Birds dataset, which is widely adopted in many applications (e.", "startOffset": 62, "endOffset": 65}, {"referenceID": 66, "context": ", [67], [68]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 67, "context": ", [67], [68]).", "startOffset": 8, "endOffset": 12}, {"referenceID": 65, "context": "8% multiclass accuracy using DAP approach [66].", "startOffset": 42, "endOffset": 46}, {"referenceID": 65, "context": "Most importantly, we achieved these results without learning any attribute classifiers, as in [66].", "startOffset": 94, "endOffset": 98}, {"referenceID": 57, "context": "20 % Order Embedding [58] 3.", "startOffset": 21, "endOffset": 25}, {"referenceID": 58, "context": "65 % [59] 2.", "startOffset": 5, "endOffset": 9}, {"referenceID": 7, "context": "7 % Lampert DAP [8] 68.", "startOffset": 16, "endOffset": 19}, {"referenceID": 7, "context": "03 % Lampert DAP [8] 4.", "startOffset": 17, "endOffset": 20}, {"referenceID": 57, "context": "We used a state of the art model [58] for image-sentence similarity by breaking down each text document into sentences and considering it as a positive sentence for all images in the corresponding class.", "startOffset": 33, "endOffset": 37}, {"referenceID": 68, "context": "Images were encoded using VGGNet [69] and sentences were encoded by an RNN with GRU activations [33].", "startOffset": 33, "endOffset": 37}, {"referenceID": 32, "context": "Images were encoded using VGGNet [69] and sentences were encoded by an RNN with GRU activations [33].", "startOffset": 96, "endOffset": 100}, {"referenceID": 69, "context": "SC-Seen Split on CUB 2011 [70]", "startOffset": 26, "endOffset": 30}, {"referenceID": 56, "context": "We applied both our linear and kernel method and compare against recently published results in our setting [57], [58], [71], [56].", "startOffset": 107, "endOffset": 111}, {"referenceID": 57, "context": "We applied both our linear and kernel method and compare against recently published results in our setting [57], [58], [71], [56].", "startOffset": 113, "endOffset": 117}, {"referenceID": 55, "context": "We applied both our linear and kernel method and compare against recently published results in our setting [57], [58], [71], [56].", "startOffset": 125, "endOffset": 129}, {"referenceID": 12, "context": "35% on SC-Unseen split designed in [13].", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "It is not hard to see that the performance of our methods (both linear and kernel) on SC-Seen split is significantly better than SC-Unseen split designed in [13].", "startOffset": 157, "endOffset": 161}, {"referenceID": 56, "context": "02 % Less is more [57] CVPR 2016 29.", "startOffset": 18, "endOffset": 22}, {"referenceID": 57, "context": "0 % Order Embedding [58] ICLR 2016 17.", "startOffset": 20, "endOffset": 24}, {"referenceID": 55, "context": "[56] CVPR 2015 with Word2vec 23.", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "[56] CVPR 2015 with GloVE 28.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "People typically learn through exposure to visual concepts associated with linguistic descriptions. For instance, teaching visual object categories to children is often accompanied by descriptions in text or speech. In a machine learning context, these observations motivates us to ask whether this learning process could be computationally modeled to learn visual classifiers. More specifically, the main question of this work is how to utilize purely textual description of visual classes with no training images, to learn explicit visual classifiers for them. We propose and investigate two baseline formulations, based on regression and domain transfer, that predict a linear classifier. Then, we propose a new constrained optimization formulation that combines a regression function and a knowledge transfer function with additional constraints to predict the parameters of a linear classifier. We also propose a generic kernelized models where a kernel classifier is predicted in the form defined by the representer theorem. The kernelized models allow defining and utilizing any two RKHS kernel functions in the visual space and text space, respectively. We finally propose a kernel function between unstructured text descriptions that builds on distributional semantics, which shows an advantage in our setting and could be useful for other applications. We applied all the studied models to predict visual classifiers on two fine-grained and challenging categorization datasets (CU Birds and Flower Datasets), and the results indicate successful predictions of our final model over several baselines that we designed.", "creator": "LaTeX with hyperref package"}}}