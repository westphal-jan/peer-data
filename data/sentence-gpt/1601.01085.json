{"id": "1601.01085", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2016", "title": "Incorporating Structural Alignment Biases into an Attentional Neural Translation Model", "abstract": "Neural encoder-decoder models of machine translation have achieved impressive results, rivalling traditional translation models. However their modelling formulation is overly simplistic, and omits several key inductive biases built into traditional models. In this paper we extend the attentional neural translation model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions. Finally, we extend the neural translation model to include non-conformity-based optimization (NAF) models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 6 Jan 2016 06:03:17 GMT  (1408kb,D)", "http://arxiv.org/abs/1601.01085v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["trevor cohn", "cong duy vu hoang", "ekaterina vymolova", "kaisheng yao", "chris dyer", "gholamreza haffari"], "accepted": true, "id": "1601.01085"}, "pdf": {"name": "1601.01085.pdf", "metadata": {"source": "CRF", "title": "Incorporating Structural Alignment Biases into an Attentional Neural Translation Model", "authors": ["Trevor Cohn", "Kaisheng Yao", "Gholamreza Haffari"], "emails": ["tcohn@unimelb.edu.au", "vhoang2@student.unimelb.edu.au", "evylomova@student.unimelb.edu.au", "kaisheng.YAO@microsoft.com", "cdyer@cmu.edu", "gholamreza.haffari@monash.edu"], "sections": [{"heading": "1 Introduction", "text": "Recently, models of end-to-end machine translation based on neural network classification have been shown to produce excellent translations, rivalling or in some cases surpassing traditional statistical machine translation systems (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). This is despite the neural approaches using an overall simpler model, with fewer assumptions about the learning and prediction problem.\nBroadly, neural approaches are based around the notion of an encoder-decoder (Sutskever et al., 2014), in which the source language is encoded into a distributed representation, followed by a decoding step which generates the target translation. We focus\non the attentional model of translation (Bahdanau et al., 2015) which uses a dynamic representation of the source sentence while allowing the decoder to attend to different parts of the source as it generates the target sentence. The attentional model raises intriguing opportunities, given the correspondence between the notions of attention and alignment in traditional word-based machine translation models (Brown et al., 1993).\nIn this paper we map modelling biases from word based translation models into the attentional model, such that known linguistic elements of translation can be better captured. We incorporate absolute positional bias whereby word order tends to be similar between the source sentence and its translation (e.g., IBM Model 2 and (Dyer et al., 2013)), fertility whereby each instance of a source word type tends to be translated into a consistent number of target tokens (e.g., IBM Models 3, 4, 5), relative position bias whereby prior preferences for monotonic alignments/attention can be encouraged (e.g., IBM Model 4, 5 and HMM-based Alignment (Vogel et al., 1996)), and alignment consistency whereby the attention in both translation directions are encourged to agree (e.g. symmetrization heuristics (Och and Ney, 2003) or joint modelling (Liang et al., 2006; Ganchev et al., 2008)).\nWe provide an empirical analysis of incorporating the above structural biases into the attentional model, considering low resource translation scenario over four language-pairs. Our results demonstrate consistent improvements over vanila encoder-\nar X\niv :1\n60 1.\n01 08\n5v 1\n[ cs\n.C L\n] 6\nJ an\ndecoder and attentional model in terms of the perplexity and BLEU score, e.g. up to 3.5 BLEU points when re-ranking the candiate translations generated by a state-of-the-art phrase based model."}, {"heading": "2 The attentional model of translation", "text": "We start by reviewing the attentional model of translation (Bahdanau et al., 2015), as illustrated in Fig. 1, before presenting our extensions in \u00a73.\nEncoder The encoding of the source sentence is formulated using a pair of RNNs (denoted bi-RNN) one operating left-to-right over the input sequence and another operating right-to-left,\nh\u2192i = RNN(h \u2192 i\u22121, r (s) si ) h\u2190i = RNN(h \u2192 i+1, r (s) si )\nwhere h\u2192i and h \u2190 i are the RNN hidden states. The left-to-right RNN function is defined as\nh\u2192i = tanh ( W\u2192si r (s) si +W \u2192 shh \u2192 i\u22121 + b \u2192 s ) (1)\nwhere h\u21920 \u2208 RH is a learned parameter vector, as are R(s) \u2208 RVS\u00d7E , W\u2192si \u2208 RH\u00d7E , W\u2192sh \u2208 RH\u00d7H and b\u2192s \u2208 RH , with H the number of hidden units, VS the size of the source vocabulary and E the word embedding dimensionality.1 Each source word is\n1Similarly, h\u21900 \u2208 RH ,W\u2190si \u2208 RH\u00d7E ,W\u2190sh \u2208 RH\u00d7H , b\u2190s \u2208 RH are the parameters of the right-to-left RNN.\nthen represented as a pair of hidden states, one from each RNN, ei = [ h\u2192i h\u2190i ] . This encodes not only the word but also its left and right context, which can provide important evidence for its translation.\nA crucial question is how this dynamic sized matrix E = [e1, e2, . . . , eI ] \u2208 RI\u00d7H can be used in the decoder to generate the target sentence. As with Sutskever\u2019s encoder-decoder, the target sentence is created left-to-right using a RNN, while the encoded source is used to bias the process as an auxiliary input. The mechanism for this bias is by attentional vectors, i.e. vectors of scores over each source sentence location, which are used to aggregate the dynamic source encoding into a fixed length vector.\nDecoder The decoder operates as a standard RNN over the translation t, formulated as follows\ngj = tanh ( W(th)gj\u22121 +W (ti)r (t) tj\u22121 +W (ta)cj ) (2)\nuj = tanh ( gj +W (uc)cj +W (ui)r (t) tj\u22121 ) (3)\ntj \u223c softmax ( W(ou)uj + b (to) )\n(4)\nwhere the decoder RNN is defined analogously to Eq 1 but with an additional input, the source attention component cj \u2208 R2H and weighting matrix W(ta) \u2208 RH\u00d72H . The hidden state of the recurrence is then passed through a single hidden layer2 (Eq 3) in combination with the source attention and target word using weighting matrices W(uc) \u2208 RH\u00d72H and W(ui) \u2208 RH\u00d7E . In Eq 4 this vector is transformed to be target vocabulary sized, using weight matrix W(ou) \u2208 RVT\u00d7H and bias b(to) \u2208 RVT , after which a softmax is taken, and the resulting normalised vector used as the parameters of a Categorical distribution in generating the next target word.\nThe presentation above assumes a simple RNN is used to define the recurrence over hidden states, however we can easily use alternative formulations of recurrent networks including multiplelayer RNNs, gated recurrent units (GRU; Cho et al. (2014)), or long short-term memory (LSTM; Hochreiter and Schmidhuber (1997)) units. These\nNote that we use a long short term memory unit (Hochreiter and Schmidhuber, 1997) in place of the RNN, shown here for simplicity of exposition.\n2In Bahdanau et al. (2015) they use a max-out layer for this final step, however we found this to be a needless complication, and instead use a standard hidden layer with tanh activation.\nmore advanced methods allow for more efficient learning of more complex concepts, particularly long distance effects. Empirically we found LSTMs to be the best performing, and therefore use these units herein.\nThe last key detail is the attentional component cj in Eqs 2 and 3, which is defined as follows\nfji = v > tanh ( W(ae)ei +W (ah)gj\u22121 ) (5)\n\u03b1j = softmax (fj) cj = \u2211 i \u03b1jiei\nwith the scalars fji denoting the compatibility between the target hidden state gj\u22121 and the source encoding ei. This is defined as a neural network with one hidden layer of size A and a single output, parameterised by W(ae) \u2208 RA\u00d72H , W(ah) \u2208 RA\u00d7H and v \u2208 RA. The softmax then normalises the scalar compatibility values such that for a given target word j, the values of \u03b1j can be interpreted as alignment probabilities to each source location. Finally, these alignments are used to to reweight the source components E to produce a fixed length context representation.\nTraining of this model is done by minimising the cross-entropy of the target sentence, measured word-by-word as for a language model. We use standard stochastic gradient optimisation using the back-propagation technique for computation of partial derivatives according to the chain rule."}, {"heading": "3 Incorporating Structural Biases", "text": "The attentional model, as described above, provides a powerful and elegant model of translation in which alignments between source and target words are learned through the implicit conditioning context afforded by the attention mechanism. Despite its elegance, the attentional model omits several key components of a traditional alignment models such as the IBM models (Brown et al., 1993) and Vogel\u2019s hidden Markov Model (Vogel et al., 1996) as implemented in the GIZA++ toolkit (Och and Ney, 2003). Combining the strengths of this highly successful body of research into a neural model of machine translation holds potential to further improve modelling accuracy of neural techniques. Below we out-\nline methods for incorporating these factors as structural biases into the attentional model."}, {"heading": "3.1 Position bias", "text": "First we consider position bias, based on the observation that a word at a given relative position in the source tends to align to a word at a similiar relative position in the target, iI \u2248 j J (IBM 2). Related, alignments tend to occur near the diagonal (Dyer et al., 2013), when considering the alignments as a binary I \u00d7 J matrix (illustrated in Figure 1), where the cell at (i, j) denotes whether an alignment exists between source word i and target word j.\nWe include a position bias through redefining the pre-normalised attention scalars fji in Eq 5 as:\nfji = v > tanh ( W(ae)ei +W (ah)gj\u22121+\nW(ap)\u03c8(j, i, I) )\n(6)\nwhere the new component in the input is a simple feature function of the positions in the source and target sentences and the source length,\n\u03c8(j, i, I) = [ log(1 + j), log(1 + i), log(1 + I) ]> and W(ap) \u2208 RA\u00d73. We exclude the target length J as this is unknown during decoding, as a partial translation can have several (infinite) different lengths. The use of the log(1+\u00b7) function is to avoid numerical instabilities from widely varying sentence lengths. The non-linearity in Eq 6 allows for complex functions of these inputs to be learned, such as relative positions and approximate distance from the diagonal, as well as their interactions with the other inputs (e.g., to learn that some words are exceptional cases where a diagonal bias should not apply)."}, {"heading": "3.2 Markov condition", "text": "The HMM model of translation (Vogel et al., 1996) is based on a Markov condition over alignment random variables, to allow the model to learn local effects such as when i \u2190 j is aligned then it is likely that i + 1 \u2190 j + 1 or i \u2190 j + 1. These correspond to local diagonal alignments or one-to-many alignments, respectively. In general, there are many correlations between the alignments of a word and the word immediately to its left.\nMarkov conditioning can also be incorporated in a similar manner to positional bias, by augmenting the attentional input from Eqs 5 and 6 to include:\nfji = v > tanh ( . . .+W(am)\u03be1(\u03b1j\u22121; i) ) (7)\nwhere . . . abbreviates the ei, gj\u22121 and \u03c8 components from Eq 6, and \u03be1(\u03b1j\u22121) provides a fixed dimensional representation of the attention state for the preceding word. It is not immediately obvious how to incorporate the previous attention vector as \u03b1 is dynamically sized to match the source sentence length, thus using it directly would not generalise over sentences of different lengths. For this reason, we make a simplification by just considering local moves offset by \u00b1k positions, that is,\n\u03be1(\u03b1j\u22121; i) = [ \u03b1j\u22121,i\u2212k, .., \u03b1j\u22121,i, .., \u03b1j\u22121,i+k ]> with W(am) \u2208 RA\u00d7(2k+1). Our approach is likely to capture the most important alignments patterns forming the backbone of the alignment HMM, namely monotone, 1-to-many, and local inversions."}, {"heading": "3.3 Fertility", "text": "Fertility is the propensity for a word to be translated as a consistent number of words in the other language, e.g., Iseseisvusdeklaratsioon translates as (the) Declaration of Independence. Fertility is a central component in the IBM models 3\u20135 (Brown et al., 1993). Incorporating fertility into the attentional model is a little more involved, and we present two techniques for doing so.\nLocal fertility First we consider a feature-based technique, which includes the following features\n\u03be2(\u03b1<j ; i) = \u2211 j\u2032<j \u03b1j\u2032,i\u2212k, .., \u2211 j\u2032<j \u03b1j\u2032,i, .., \u2211 j\u2032<j \u03b1j\u2032,i+1 >\nand the corresponding feature weights, i.e., W(af) \u2208 RA\u00d7(2k+1). These sums represent the total alignment score for the surrounding source words, similar to fertility in a traditional latent variable model, which is the sum over binary alignment random variables. A word which already has several alignments can be excluded from participating in more alignments, thus combating the garbage collection problem. Conversely words that tend to need high fertility can be learned through the interactions between\nthese features and the word and context embeddings in Eq 7.\nGlobal fertility A second, more explicit, technique for incorporating fertility is to include this as a modelling constraint. Initially we considered a soft constraint based on the approach in (Xu et al., 2015), where an image captioning model was biased to attend to every pixel in the image exactly once. In our setting, the same idea can be applied through adding a regularisation term to the\ntraining objective of the form \u2211\ni\n( 1\u2212 \u2211 j \u03b1j,i )2 .\nHowever this method is overly restrictive: enforcing that every word is used exactly once is not appropriate in translation where some words are likely to be dropped (e.g., determiners and other function words), while others might need to be translated several times to produce a phrase in the target language.3 For this reason we develop an alternative method, based around a contextual fertility model, p(fi|s, i) = N ( \u00b5(ei), \u03c3 2(ei) )\nwhich scores the fertility of source word i, defined as fi = \u2211 j \u03b1j,i, using a normal distribution4 parameterised by \u00b5 and \u03c32, both positive scalar valued non-linear functions of the source word encoding ei. This is incorporated into the training objective as an additional additive term, \u2211 i log p(fi|s, i), for each training sentence.\nThis formulation allows for greater consistency in translation, through e.g., learning which words tend to be omitted from translation, or translate as several words. Compared to the fertility model in IBM 3\u20135 (Brown et al., 1993), ours uses many fewer parameters through working over vector embeddings, and moreover, the BiRNN encoding of the source means that we learn context-dependent fertilities, which can be useful for dealing with fixed syntactic patterns or multi-word expressions.\n3 Modern decoders (Koehn et al., 2003) often impose the restriction of each word being translated exactly once, however this is tempered by their use of phrases as translation units rather than words, which allow for higher fertility in contiguous translation chunks.\n4The normal distribution is deficient, as it has support for all scalar values, despite fi being bounded above and below (0 \u2264 fi \u2264 J). This could be corrected by using a truncated normal, or various other choices of distribution."}, {"heading": "3.4 Bilingual Symmetry", "text": "So far we have considered a conditional model of the target given the source, modelling p(t|s). However it is well established for latent variable translation models that the alignments improve if p(s|t) is also modelled and the inferences of both directional models are combined \u2013 evidenced by the symmetrisation heuristics used in most decoders (Koehn et al., 2005), and also by explicit joint agreement training objectives (Liang et al., 2006; Ganchev et al., 2008). The rationale is that both models make somewhat independent errors, so an ensemble stands to gain from variance reduction.\nWe propose a method for joint training of two directional models as pictured in Figure 2. Training twinned models involves optimising L = \u2212 log p(t|s)\u2212 log p(s|t) + \u03b3B where, as before, we consider only a single sentence pair, for simplicity of notation. This corresponds to a pseudo-likelihood objective, with the B linking the two models.5 The B component considers the alignment (attention) matrices, \u03b1s\u2192t \u2208 RJ\u00d7I and \u03b1t\u2190s \u2208 RI\u00d7J , and attempts to make these close to one another for both translation directions (see Fig. 2). To achieve this, we use a \u2018trace bonus\u2019, inspired by (Levinboim et al., 2015), formulated as\nB = \u2212 tr(\u03b1s\u2190t >\u03b1s\u2192t) = \u2211 j \u2211 i \u03b1s\u2190ti,j \u03b1 s\u2192t j,i .\nAs the alignment cells are normalised using the softmax and thus take values in [0,1], the trace term is bounded above by min(I, J) which occurs when the two alignment matrices are transposes of each\n5We could share some parameters, e.g., the word embedding matrices, however we found this didn\u2019t make much difference versus using disjoint parameter sets. We set \u03b3 = 1 herein.\nother, representing perfect one-to-one alignments in both directions"}, {"heading": "4 Experiments", "text": "Datasets. We conducted our experiments with four language pairs, translating between English\u2194 Romanian, Estonian, Russian and Chinese. These languages were chosen to represent a range of translation difficulties, including languages with significant morphological complexity (Estonian, Russian). We focus on a (simulated) low resource setting, where only a limited amount of training data is available. This serves to demonstrate the robustness and generalisation of our model on sparse data \u2013 something that has not yet been established for neural models with millions of parameters with vast potential for over-fitting.\nTable 1 shows the statistics of the training sets.6 For Chinese-English, the data comes from the BTEC corpus, where the number of training sentence pairs is 44,016. We used \u2018devset1 2\u2019 and \u2018devset 3\u2019 as the development and test sets, respectively, and in both cases used only the first reference for evaluation. For other language pairs, the data come from the Europarl corpus (Koehn, 2005), where we used 100K sentence pairs for training, and 3K for development and 2K for testing.7\nModels and Baselines. We have implemented our neural translation model with linguistic features in C++ using the CNN library.8 We compared\n6For all datasets words were thresholded for training frequency\u2265 5, with uncommon training and unseen testing words replaced by an \u2329unk\u232a symbol.\n7The first 100K sentence pairs were used for training, while the development and test were drawn from the last 100K sentence pairs, taking the first 2K for testing and the last 3K for development.\n8https://github.com/clab/cnn/\nour proposed model against our implementations of the attentional model (Bahdanau et al., 2015) and encoder-decoder architecture (Sutskever et al., 2014). As the baseline, we used a state-of-the-art phrase-based statistical machine translation model built using Moses (Koehn et al., 2007) with the standard features: relative-frequency and lexical translation model probabilities in both directions; distortion model; language model and word count. We used KenLM (Heafield, 2011) to create 3-gram language models with Kneser-Ney smoothing on the target side of the bilingual training corpora.\nEvaluation Measures. Following previous work (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Neubig et al., 2015), we evaluated all neural models using test set perplexities and in a re-ranking setting, using BLEU (Papineni et al., 2002) measure. For re-ranking, we generated 100-best translations using the baseline phrase-based model, to which we added log probability features from our neural models alongside the features of the underlying phrase-based model."}, {"heading": "4.1 Analysis of Alignment Biases", "text": "We start by investigating the effect of various linguistic constraints, described in Section 3, on the attentional model. Table 2 presents the perplexity of trained models for Chinese\u2192English translation. For comparison, we report the results of an encoderdecoder-based neural translation model (Sutskever et al., 2014) as the baseline. All other results are for the attentional model with a single-layer LSTM as encoder and two-layer LSTM as decoder, using 512 embedding, 512 hidden, and 256 alignment dimen-\nsions. For each model, we also report the number of its parameters. Models are trained using stochastic gradient, allowing up to 20 epochs. For each model the best perplexity on the held-out development set is reported, which was achieved in 5-10 epochs for most cases.\nAs expected, the vanilla attentional model greatly improves over encoder-decoder (perplexity of 4.77 vs. 5.35), clearly making good use of the additional context. Adding the combined positional bias, local fertility, and Markov structure (denoted by +align) further decreases the perplexity to 4.56. Adding the global fertility (+glofer) is detrimental, however, increases perplexity to 5.20. Interestingly, global fertility does helps to reduce the perplexity (to 4.31) when using with pre-training setting (+align+gloferpre). In this case, it is refining an already excellent model from which reliable global fertility estimates can be obtained. This finding is consistent with the other languages, see Figure 3 which shows typical learning curves of different variants of the attentional model. Note that when global fertility is added to the vanilla attentional model with alignment features, it significantly slows down training as it limits exploration in early training iterations, however it does bring a sizeable win when used to fine-tune a pre-trained model. Finally, the bilingual symmetry also helps to reduce the perplexity scores when used with the alignment features, however, does not combine well with global fertility\n(+align+sym+glofer-pre). This is perhaps an unsurprising result as both methods impose a often-times similar regularising effect over the attention matrix.\nFigure 4 illustrates the different attention matrices inferred by the various model variants. Note the difference between the base attentional model and its variant with alignment features (\u2018+align\u2019), where more weight is assigned to diagonal and 1-to-many alignments. Global fertility pushes more attention to the sentinel symbols \u2329s\u232a and \u2329/s\u232a. Determiners and prepositions in English show much lower fertility than nouns, while Estonian nouns have even higher fertility. This accords with Estonian morphology, wherein nouns are inflected with rich case marking, e.g., no\u0303ukoguga has the cogitative -ga suffix, meaning \u2018with\u2019, and thus translates as several English words (with the council). The right-most col-\numn corresponds to joint symmetric training, with many more confident attention values especially for consistent 1-to-many alignments (difficult in English and raskeid in Estonian, an adjective in partitive case meaning some difficult)."}, {"heading": "4.2 Full Results", "text": "The perplexity results of the neural models for the two translation directions across the four language pairs are presented in Table 3.a and 3.b. In all cases, our work achieves lower perplexities compared to the vanilla attentional model and the encoder-decoder architecture, owing to the linguistic constraints.\nTable 4 presents the BLEU scores for the reranking setting for the translating into English from our four languages. We compare re-ranking set-\ntings using the log probabilities produced by our model as additional features vs. using log probabilities from the vanilla attentional model and the encoder-decoder. The re-rankers based on our model are significantly better than the rest for Chinese and Estonian, and on par with the other for Russian and Romanian\u2192English. In all cases our model has performance at least 1 BLEU point better than the baseline phrase-based system. It is worth noting that for Chinese-English, our re-ranker leads to an increase of almost 3 points in the BLEU score using an ensemble of neural models with different configurations.9"}, {"heading": "5 Related Work", "text": "Kalchbrenner and Blunsom (2013) were the first to propose a full neural model of translation, using a convolutional network as the source encoder, fol-\n9We use the outputs of 6\u201312 models trained in both directions, using different alignment and fertility options, and using a smaller dimensionality than earlier (100 embedding, 100 hidden and 50 attention dimensions).\nlowed by an RNN decoder to generate the target translation. This was extended in Sutskever et al. (2014), who replaced the source encoder with an RNN using a Long Short-Term Memory (LSTM), and Bahdanau et al. (2015) who introduced the notion of \u201cattention\u201d to the model, whereby the source context can dynamically change during the decoding process to attend to the most relevant parts of the source sentence Luong et al. (2015) refined the attention mechanism to be more local, by constraining attention to a text span, whose words\u2019 representations are averaged. To leverage the attention history, (Luong et al., 2015) made use of the attention vector of the previous position when generating the attention vector for the next position, similar in spirit to our method for incorporating alignment structural biases. Concurrent with our work, Cheng et al. (2015) proposed a similar agreementbased joint training for bidirectional attention-based neural machine translation, and showed significant improvement in the BLEU score for the large data French\u2194English translation."}, {"heading": "6 Conclusion", "text": "We have shown that the attentional model of translation does not capture many well known properties of traditional word-based translation models, and proposed several ways of imposing these as structural biases on the model. We show improvements across several challenging language pairs in a low-resource setting, both in perplexity and re-ranking evaluations. In future work we intend to investigate the model performance on larger datasets, and incorporate further linguistic information such as morphological representations."}, {"heading": "Acknowledgements", "text": "The work reported here was started at JSALT 2015 in UW, Seattle, and was supported by JHU via grants from NSF (IIS), DARPA (LORELEI), Google, Microsoft, Amazon, Mitsubishi Electric, and MERL. Dr Cohn was supported by the ARC (Future Fellowship)."}], "references": [{"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Brown et al.1993] Peter E. Brown", "Stephen A. Della Pietra", "Vincent J. Della Pietra", "Robert L. Mercer"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Agreement-based joint training for bidirectional attention-based neural machine translation", "author": ["Cheng et al.2015] Yong Cheng", "Shiqi Shen", "Zhongjun Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": null, "citeRegEx": "Cheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation", "author": ["K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["Dyer et al.2013] Chris Dyer", "Victor Chahuneau", "Noah A. Smith"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics:", "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Better alignments = better translations", "author": ["Jo\u00e3o V. Gra\u00e7a", "Ben Taskar"], "venue": "In Proceedings of ACL-08: HLT,", "citeRegEx": "Ganchev et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ganchev et al\\.", "year": 2008}, {"title": "KenLM: faster and smaller language model queries", "author": ["Kenneth Heafield"], "venue": "In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation,", "citeRegEx": "Heafield.,? \\Q2011\\E", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Statistical phrase-based translation", "author": ["Koehn et al.2003] Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "In Proceedings of North American Chapter of the Association for Computational Linguistics on Human Language Technology,", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Edinburgh system description for the 2005 IWSLT speech translation evaluation", "author": ["Koehn et al.2005] Philipp Koehn", "Amittai Axelrod", "Alexandra Birch", "Chris Callison-Burch", "Miles Osborne", "David Talbot", "Michael White"], "venue": "In IWSLT,", "citeRegEx": "Koehn et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2005}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": "In Proc. ACL Interactive Poster and Demonstration Sessions,", "citeRegEx": "Bertoldi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bertoldi et al\\.", "year": 2007}, {"title": "Europarl: A Parallel Corpus for Statistical Machine Translation", "author": ["Philipp Koehn"], "venue": "In Conference Proceedings: the tenth Machine Translation Summit,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Model invertibility regularization: Sequence alignment with or without parallel data", "author": ["Ashish Vaswani", "David Chiang"], "venue": "In Proceedings of the North American Chapter of the Association", "citeRegEx": "Levinboim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levinboim et al\\.", "year": 2015}, {"title": "Alignment by agreement", "author": ["Liang et al.2006] Percy Liang", "Ben Taskar", "Dan Klein"], "venue": "In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL),", "citeRegEx": "Liang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Hieu Pham", "Christopher D. Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Neural reranking improves subjective quality of machine translation: NAIST at WAT2015", "author": ["Neubig et al.2015] Graham Neubig", "Makoto Morishita", "Satoshi Nakamura"], "venue": "In Proceedings of the 2nd Workshop on Asian Translation", "citeRegEx": "Neubig et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neubig et al\\.", "year": 2015}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Och", "Ney2003] Franz Josef Och", "Hermann Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Och et al\\.", "year": 2003}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th Annual Meeting on Association", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "HMM-based word alignment in statistical translation", "author": ["Vogel et al.1996] Stephan Vogel", "Hermann Ney", "Christoph Tillmann"], "venue": "In Proceedings of the International Conference on Computational Linguistics (COLING),", "citeRegEx": "Vogel et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Vogel et al\\.", "year": 1996}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "Recently, models of end-to-end machine translation based on neural network classification have been shown to produce excellent translations, rivalling or in some cases surpassing traditional statistical machine translation systems (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 231, "endOffset": 310}, {"referenceID": 18, "context": "Broadly, neural approaches are based around the notion of an encoder-decoder (Sutskever et al., 2014), in which the source language is encoded into a distributed representation, followed by a decoding step which generates the target translation.", "startOffset": 77, "endOffset": 101}, {"referenceID": 0, "context": "The attentional model raises intriguing opportunities, given the correspondence between the notions of attention and alignment in traditional word-based machine translation models (Brown et al., 1993).", "startOffset": 180, "endOffset": 200}, {"referenceID": 3, "context": ", IBM Model 2 and (Dyer et al., 2013)), fertility whereby each instance of a source word type tends to be translated into a consistent number of target tokens (e.", "startOffset": 18, "endOffset": 37}, {"referenceID": 19, "context": ", IBM Model 4, 5 and HMM-based Alignment (Vogel et al., 1996)), and alignment consistency whereby the attention in both translation directions are encourged to agree (e.", "startOffset": 41, "endOffset": 61}, {"referenceID": 13, "context": "symmetrization heuristics (Och and Ney, 2003) or joint modelling (Liang et al., 2006; Ganchev et al., 2008)).", "startOffset": 65, "endOffset": 107}, {"referenceID": 4, "context": "symmetrization heuristics (Och and Ney, 2003) or joint modelling (Liang et al., 2006; Ganchev et al., 2008)).", "startOffset": 65, "endOffset": 107}, {"referenceID": 2, "context": "The presentation above assumes a simple RNN is used to define the recurrence over hidden states, however we can easily use alternative formulations of recurrent networks including multiplelayer RNNs, gated recurrent units (GRU; Cho et al. (2014)), or long short-term memory (LSTM; Hochreiter and Schmidhuber (1997)) units.", "startOffset": 228, "endOffset": 246}, {"referenceID": 2, "context": "The presentation above assumes a simple RNN is used to define the recurrence over hidden states, however we can easily use alternative formulations of recurrent networks including multiplelayer RNNs, gated recurrent units (GRU; Cho et al. (2014)), or long short-term memory (LSTM; Hochreiter and Schmidhuber (1997)) units.", "startOffset": 228, "endOffset": 315}, {"referenceID": 0, "context": "Despite its elegance, the attentional model omits several key components of a traditional alignment models such as the IBM models (Brown et al., 1993) and Vogel\u2019s hidden Markov Model (Vogel et al.", "startOffset": 130, "endOffset": 150}, {"referenceID": 19, "context": ", 1993) and Vogel\u2019s hidden Markov Model (Vogel et al., 1996) as implemented in the GIZA++ toolkit (Och and Ney, 2003).", "startOffset": 40, "endOffset": 60}, {"referenceID": 3, "context": "Related, alignments tend to occur near the diagonal (Dyer et al., 2013), when considering the alignments as a binary I \u00d7 J matrix (illustrated in Figure 1), where the cell at (i, j) denotes whether an alignment exists between source word i and target word j.", "startOffset": 52, "endOffset": 71}, {"referenceID": 19, "context": "The HMM model of translation (Vogel et al., 1996) is based on a Markov condition over alignment random variables, to allow the model to learn local effects such as when i \u2190 j is aligned then it is likely that i + 1 \u2190 j + 1 or i \u2190 j + 1.", "startOffset": 29, "endOffset": 49}, {"referenceID": 0, "context": "Fertility is a central component in the IBM models 3\u20135 (Brown et al., 1993).", "startOffset": 55, "endOffset": 75}, {"referenceID": 20, "context": "Initially we considered a soft constraint based on the approach in (Xu et al., 2015), where an image captioning model was biased to attend to every pixel in the image exactly once.", "startOffset": 67, "endOffset": 84}, {"referenceID": 0, "context": "Compared to the fertility model in IBM 3\u20135 (Brown et al., 1993), ours uses many fewer parameters through working over vector embeddings, and moreover, the BiRNN encoding of the source means that we learn context-dependent fertilities, which can be useful for dealing with fixed syntactic patterns or multi-word expressions.", "startOffset": 43, "endOffset": 63}, {"referenceID": 8, "context": "3 Modern decoders (Koehn et al., 2003) often impose the restriction of each word being translated exactly once, however this is tempered by their use of phrases as translation units rather than words, which allow for higher fertility in contiguous translation chunks.", "startOffset": 18, "endOffset": 38}, {"referenceID": 9, "context": "However it is well established for latent variable translation models that the alignments improve if p(s|t) is also modelled and the inferences of both directional models are combined \u2013 evidenced by the symmetrisation heuristics used in most decoders (Koehn et al., 2005), and also by explicit joint agreement training objectives (Liang et al.", "startOffset": 251, "endOffset": 271}, {"referenceID": 13, "context": ", 2005), and also by explicit joint agreement training objectives (Liang et al., 2006; Ganchev et al., 2008).", "startOffset": 66, "endOffset": 108}, {"referenceID": 4, "context": ", 2005), and also by explicit joint agreement training objectives (Liang et al., 2006; Ganchev et al., 2008).", "startOffset": 66, "endOffset": 108}, {"referenceID": 12, "context": "To achieve this, we use a \u2018trace bonus\u2019, inspired by (Levinboim et al., 2015), formulated as", "startOffset": 53, "endOffset": 77}, {"referenceID": 11, "context": "For other language pairs, the data come from the Europarl corpus (Koehn, 2005), where we used 100K sentence pairs for training, and 3K for development and 2K for testing.", "startOffset": 65, "endOffset": 78}, {"referenceID": 18, "context": ", 2015) and encoder-decoder architecture (Sutskever et al., 2014).", "startOffset": 41, "endOffset": 65}, {"referenceID": 5, "context": "We used KenLM (Heafield, 2011) to create 3-gram language models with Kneser-Ney smoothing on the target side of the bilingual training corpora.", "startOffset": 14, "endOffset": 30}, {"referenceID": 18, "context": "Following previous work (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Neubig et al., 2015), we evaluated all neural models using test set perplexities and in a re-ranking setting, using BLEU (Papineni et al.", "startOffset": 24, "endOffset": 124}, {"referenceID": 15, "context": "Following previous work (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Neubig et al., 2015), we evaluated all neural models using test set perplexities and in a re-ranking setting, using BLEU (Papineni et al.", "startOffset": 24, "endOffset": 124}, {"referenceID": 17, "context": ", 2015), we evaluated all neural models using test set perplexities and in a re-ranking setting, using BLEU (Papineni et al., 2002) measure.", "startOffset": 108, "endOffset": 131}, {"referenceID": 18, "context": "For comparison, we report the results of an encoderdecoder-based neural translation model (Sutskever et al., 2014) as the baseline.", "startOffset": 90, "endOffset": 114}, {"referenceID": 14, "context": "To leverage the attention history, (Luong et al., 2015) made use of the attention vector of the previous position when generating the attention vector for the next position, similar in spirit to our method for incorporating alignment structural biases.", "startOffset": 35, "endOffset": 55}, {"referenceID": 16, "context": "This was extended in Sutskever et al. (2014), who replaced the source encoder with an RNN using a Long Short-Term Memory (LSTM), and Bahdanau et al.", "startOffset": 21, "endOffset": 45}, {"referenceID": 16, "context": "This was extended in Sutskever et al. (2014), who replaced the source encoder with an RNN using a Long Short-Term Memory (LSTM), and Bahdanau et al. (2015) who introduced the notion of \u201cattention\u201d to the model, whereby the source context can dynamically change during the decoding process to attend to the most relevant parts of the source sentence Luong et al.", "startOffset": 21, "endOffset": 156}, {"referenceID": 13, "context": "(2015) who introduced the notion of \u201cattention\u201d to the model, whereby the source context can dynamically change during the decoding process to attend to the most relevant parts of the source sentence Luong et al. (2015) refined the attention mechanism to be more local, by constraining attention to a text span, whose words\u2019 representations are averaged.", "startOffset": 200, "endOffset": 220}, {"referenceID": 1, "context": "Concurrent with our work, Cheng et al. (2015) proposed a similar agreementbased joint training for bidirectional attention-based neural machine translation, and showed significant improvement in the BLEU score for the large data French\u2194English translation.", "startOffset": 26, "endOffset": 46}], "year": 2016, "abstractText": "Neural encoder-decoder models of machine translation have achieved impressive results, rivalling traditional translation models. However their modelling formulation is overly simplistic, and omits several key inductive biases built into traditional models. In this paper we extend the attentional neural translation model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions. We show improvements over a baseline attentional model and standard phrase-based model over several language pairs, evaluating on difficult languages in a low resource setting.", "creator": "LaTeX with hyperref package"}}}