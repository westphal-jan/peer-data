{"id": "1601.01343", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2016", "title": "Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation", "abstract": "Named Entity Disambiguation (NED) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base (KB) (e.g., Wikipedia). In this paper, we propose a novel embedding method specifically designed for NED. The proposed method jointly maps words and entities into the same continuous vector space. We extend the skip-gram model by using two models. The KB graph model learns the relatedness of entities using the link structure of the KB, whereas the anchor context model aims to align vectors such that similar words and entities occur close to one another in the vector space by leveraging KB anchors and their context words. By combining contexts based on the proposed embedding with standard NED features, we achieved state-of-the-art accuracy of 93.1% on the standard CoNLL dataset and 85.2% on the TAC 2010 dataset. Our code and pre-trained vectors will be made available online.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 6 Jan 2016 22:19:20 GMT  (29kb)", "http://arxiv.org/abs/1601.01343v1", null], ["v2", "Sat, 19 Mar 2016 07:31:47 GMT  (30kb)", "http://arxiv.org/abs/1601.01343v2", null], ["v3", "Sun, 1 May 2016 06:39:19 GMT  (30kb)", "http://arxiv.org/abs/1601.01343v3", null], ["v4", "Fri, 10 Jun 2016 01:51:26 GMT  (30kb)", "http://arxiv.org/abs/1601.01343v4", "Accepted at CoNLL 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ikuya yamada", "hiroyuki shindo", "hideaki takeda", "yoshiyasu takefuji"], "accepted": false, "id": "1601.01343"}, "pdf": {"name": "1601.01343.pdf", "metadata": {"source": "CRF", "title": "Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation", "authors": ["Ikuya Yamada", "Hiroyuki Shindo", "Hideaki Takeda", "Yoshiyasu Takefuji"], "emails": ["ikuya@ousia.jp", "shindo@is.naist.jp", "takeda@nii.ac.jp", "takefuji@sfc.keio.ac.jp"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 1.\n01 34\n3v 1\n[ cs\n.C L\n] 6\nJ an\n2 01"}, {"heading": "1 Introduction", "text": "Named Entity Disambiguation (NED) is the task of resolving ambiguous mentions of entities to their referent entities in a knowledge base (KB) (e.g., Wikipedia). NED has lately been extensively studied (Cucerzan, 2007; Mihalcea and Csomai, 2007; Milne and Witten, 2008b; Ratinov et al., 2011) and used as a fundamental component in numerous tasks, such as information extraction, knowledge base population (McNamee and Dang, 2009; Ji et al., 2010), and semantic search (Blanco et al., 2015). We use Wikipedia as our KB in this paper.\nThe main difficulty in NED is ambiguity in the meaning of entity mentions. For example, the mention \u201cWashington\u201d in a document can refer to various entities, such as the state, or the capital in the US, the actor Denzel Washington, the first US president George Washington, and so on. In order to resolve these ambiguous mentions into references to the correct entities, early approaches focused on modeling textual context, such as the similarity between contextual words and encyclopedic descriptions of a candidate entity (Bunescu and Pasca, 2006; Mihalcea and Csomai, 2007). Most state-of-the-art methods use more sophisticated global approaches, wherein all mentions in a document are simultaneously disambiguated based on global coherence among disambiguation decisions.\nWord embedding methods are also becoming increasingly popular (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014). These involve learning continuous vector representations of words from large, unstructured text corpora. The vectors are designed to capture the semantic similarity of words when similar words are placed near one another in a relatively low-dimensional vector space.\nIn this paper, we propose a method to construct a novel embedding that jointly maps words and entities into the same continuous vector space. In this model, similar words and entities are placed close to one another in a vector space. Hence, we can measure the similarity between any pairs of items (i.e., words, entities, and a word and an entity) by simply computing their cosine similarity. This enables us to easily measure the contextual information for NED, such as the similarity between a context word\nand a candidate entity, and the relatedness of entities required to model coherence.\nOur model is based on the skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b), a recently proposed embedding model that learns to predict each context word given the target word. Our model consists of the following three models based on the skip-gram model: 1) the conventional skipgram model that learns to predict the neighboring words given the target word in the text corpora, 2) the KB graph model that learns to estimate the neighboring entities given the target entity in the link graph of the KB, and 3) the anchor context model that learns to predict the neighboring words given the target entity using the anchors and their context words in the KB. By jointly optimizing these models, our method simultaneously learns the embedding of words and entities.\nBased on our proposed embedding, we also develop a straightforward NED method that computes two contexts using the proposed embedding: textual context similarity, and coherence. Textual context similarity is measured based on vector similarity between an entity and words in the document. Coherence is measured based on the relatedness between the target entity and other entities in the document. Our NED method combines these contexts with several standard features (e.g., prior probability) using supervised machine learning.\nWe tested the proposed method using two standard NED datasets: the CoNLL dataset and the TAC 2010 dataset. Experimental results revealed that our method outperformed state-of-the-art methods on both datasets by significant margins. Moreover, we conducted experiments to separately assess the quality of the vector representations of words and entities using several standard word similarity datasets and an entity relatedness dataset, and discovered that our method successfully learns the quality representations of words and entities."}, {"heading": "2 Joint Embedding of Words and Entities", "text": "In this section, we first describe the conventional skip-gram model for learning word embedding. We then explain our method to construct an embedding that jointly maps words and entities into the same continuous d-dimensional vector space. We extend\nthe skip-gram model by adding the KB graph model and the anchor context model."}, {"heading": "2.1 Skip-gram Model for Word Similarity", "text": "The training objective of the skip-gram model is to find word representations that are useful to predict context words given the target word. Formally, given a sequence of T words w1, w2, ..., wT , the model aims to maximize the following objective function:\nLw =\nT \u2211\nt=1\n\u2211\n\u2212c\u2264j\u2264c,j 6=0\nlog P (wt+j |wt) (1)\nwhere c is the size of the context window, wt denotes the target word, and wt+j is its context word. The conditional probability P (wt+j |wt) is computed using the following softmax function:\nP (wt+j |wt) = exp(Vwt\n\u22a4 Uwt+j)\n\u2211\nw\u2208W exp(Vwt \u22a4Uw)\n(2)\nwhere W is a set containing all words in the vocabulary, and Vw \u2208 Rd and Uw \u2208 Rd denote the vectors of word w in matrices V and U, respectively.\nThe skip-gram model is trained to optimize the above function Lw, and V are used as the resulting vector representations of words."}, {"heading": "2.2 Extending the Skip-gram Model", "text": "We extend the skip-gram model to learn the vector representations of entities. We expand matrices V and U to include the vectors of entities Ve \u2208 Rd and Ue \u2208 Rd in addition to the vectors for words."}, {"heading": "2.2.1 KB Graph Model", "text": "We use an internal link structure in KB to enable the model to learn the relatedness between pairs of entities. Wikipedia Link-based Measure (WLM) (Milne and Witten, 2008a) is a method to measure entity relatedness based on its link structure. It has been used as a standard method to compute the relatedness of entities for modeling coherence in past NED studies. The relatedness between two entities is computed using the following function:\nWLM(e1, e2) = 1\u2212 log max(|Ce1 |,|Ce2 |)\u2212log |Ce1\u2229Ce2 |\nlog |E|\u2212log min(|Ce1 |,|Ce2 |) (3)\nwhere E is the set of all entities in KB and Ce is the set of entities with a link to an entity e. Intuitively,\nWLM assumes that entities with similar incoming links are related. Despite its simplicity, WLM yields state-of-the-art performance (Hoffart et al., 2012).\nInspired by WLM, the KB graph model simply learns to place entities with similar incoming links near one another in the vector space. We formalize this as the following objective function:\nLe = \u2211\nei\u2208E\n\u2211\neo\u2208Cei ,ei 6=eo\nlogP (eo|ei) (4)\nWe compute the conditional probability P (eo|ei) using the following softmax function:\nP (eo|ei) = exp(Vei\n\u22a4 Ueo)\n\u2211\ne\u2208E exp(Vei \u22a4Ue)\n(5)\nWe train the model to predict the incoming links Ce given an entity e. Therefore, Ce plays a similar role to context words in the skip-gram model."}, {"heading": "2.2.2 Anchor Context Model", "text": "If we add only the KB graph model to the skipgram model, the vectors of words and entities do not interact, and can be placed in different subspaces of the vector space. To address this issue, we introduce the anchor context model to place similar words and entities near one another in the vector space.\nThe idea underlying this model is to leverage KB anchors and their context words to train the model. As mentioned in Section 1, we use Wikipedia as a KB. It contains many internal anchors that can be safely treated as unambiguous occurrences of referent KB entities. By using these anchors, we can easily obtain many occurrences of entities and their corresponding context words directly from the KB.\nAs in the skip-gram model, we simply train the model to predict the context words of an entity pointed to by the target anchor. The objective function is as follows:\nLa = \u2211\n(ei,Q)\u2208A\n\u2211\nwo\u2208Q\nlogP (wo|ei) (6)\nwhere A denotes a set of anchors in the KB, each of which contains a pair of a referent entity ei and a set of its context words Q. Here, Q contains the previous c words and the next c words. Note that |A| equals the number of internal anchors in the KB. As\nin past models, the conditional probability P (wo|ei) is computed using the softmax function:\nP (wo|ei) = exp(Vei\n\u22a4 Uwo)\n\u2211\nw\u2208W exp(Vei \u22a4Uw)\n(7)\nUsing the proposed model, we align the vector representations of words and entities by placing words and entities with similar context words close to one another in the vector space."}, {"heading": "2.3 Training", "text": "Considering the three model components mentioned above, we propose the following objective function by linearly combining the above objective functions:\nL = Lw + Le + La (8)\nThe training of the model is intended to maximize the above function, and the resulting matrix V is used to embed words and entities.\nOne of the problems in training our model is that the normalizers contained in the softmax functions P (wt+j |wt), P (eo|ei), and P (wo|ei) are computationally very expensive because they involve summation over all words W or entities E. To address this problem, we use negative sampling (NEG) (Mikolov et al., 2013b) to convert original objective functions into computationally feasible ones. NEG is defined by the following objective function:\nlog \u03c3(Vwt \u22a4 Uwt+j) + \u2211g i=1 Ewi\u223cPneg(w)\n[\nlog \u03c3(\u2212Vwt \u22a4 Uwi)\n]\n(9)\nwhere \u03c3(x) = 1/(1+exp(\u2212x)) and g is the number of negative samples. We replace the logP (wt+j |wt) term in Eq. (1) with the above objective function. Consequently, the objective function is transformed from that in Eq. (1) to a simple objective function of the binary classification to distinguish the observed word wt from words drawn from noise distribution Pneg(w). We also replace log P (eo|ei) in Eq. (4) and log P (wo|ei) in Eq. (6) in the same manner.\nNote that NEG takes a negative distribution Pneg(w) as a free parameter. Following (Mikolov et al., 2013b), we use the unigram distribution of words (U(w)) raised to the 3/4th power (i.e., U(w)3/4/Z , where Z is a normalization constant) in the skip-gram model and the anchor context model. In the KB graph model, we use a uniform distribution over KB entities E as the negative distribution.\nWe use Wikipedia to train all the above models. Optimization is carried out simultaneously to maximize the transformed objective function by iterating over Wikipedia pages several times. We use stochastic gradient descent (SGD) for the optimization. The optimization is performed using a multiprocess-based implementation of our model using Python, Cython, and NumPy configured with OpenBLAS with storing matrices V and U in the shared memory. To improve speed, we decide not to introduce locks to the shared matrices."}, {"heading": "3 Named Entity Disambiguation Using Embedding", "text": "In this section, we explain our NED method using our proposed embedding. Let us formally define the task. Given a set of entity mentions M = {m1,m2, ...,mN} in a document d with an entity set E = {e1, e2, ..., eK} in the KB, the task is defined as resolving mentions (e.g., \u201cWashington\u201d) into their referent entities (e.g., Washington D.C.).\nWe introduce two measures that have been frequently observed in past NED studies: entity prior P (e) and prior probability P (e|m). We define entity prior P (e) = |Ae,\u2217|/|A\u2217,\u2217| where A\u2217,\u2217 denotes all anchors in the KB and Ae,\u2217 is the set of anchors that point to entity e. Prior probability is defined as P (e|m) = |Ae,m|/|A\u2217,m| where A\u2217,m represents all anchors with the same surface as mention m in KB and Ae,m is a subset of A\u2217,m that points to entity e.\nWe separate the NED task into two sub-tasks: candidate generation and mention disambiguation. In candidate generation, candidates of referent entities are generated for each mention. Details of candidate generation are provided in Section 4.4.1."}, {"heading": "3.1 Mention Disambiguation", "text": "Given a document d and mention m with its candidate referent entities {e1, e2, ..., ek} generated in the candidate generation step, the task is to disambiguate mention m by selecting the most relevant entity from the candidate entities.\nThe key to improving the performance of this task is to effectively model the context. We propose two novel methods to model the context using the proposed embedding. Further, we combine these two models with several standard NED features using su-\npervised machine learning."}, {"heading": "3.1.1 Modeling Textual Context", "text": "Textual context is designed based on the assumption that an entity is more likely to appear if the context of a given mention is similar to that of the entity.\nWe propose a method to measure the similarity between textual context and entity using the proposed embedding by first deriving the vector representation of the context and then computing the similarity between the context and the entity using cosine similarity. To derive the vector of context, we average the vectors of context words:\n~vcw = 1\n|Wcm |\n\u2211\nw\u2208Wcm\n~vw (10)\nwhere Wcm is a set of the context words of mention m and ~vw \u2208 V denotes the vector representation of word w. We use all noun words in document d as context words.1 Moreover, we ignore a context word if the surface of mention m contains it.\nWe then measure the similarity between candidate entity and the derived textual context by using cosine similarity between ~vcw and the vector of entity ~ve."}, {"heading": "3.1.2 Modeling Coherence", "text": "It has been revealed that effectively modeling coherence in the assignment of entities to mentions is important for NED. However, this is a chicken-andegg problem because the assignment of entities to mentions, which is required to measure coherence, is not possible prior to performing NED.\nTo address this problem, we introduce a simple two-step approach: we first train the machine learning model using the coherence score among unambiguous mentions2, in addition to other features, and then retrain the model using the coherence score among the predicted entity assignments instead.\nTo estimate coherence, we first calculate the vector representation of the context entities and measure the similarity between the vector of the context entities and that of the target entity e. Note that context entities are unambiguous entities in the first step, and predicted entities are used instead in the second step.\n1We used Apache OpenNLP tagger to detect nouns. https://opennlp.apache.org/\n2We consider that mention m unambiguously refers to entity e if its prior probability P (e|m) is greater than 0.95.\nTo derive the vector representation of context entities, we average their vector representations:\n~vce = 1\n|Ecm |\n\u2211\ne\u2217\u2208Ecm\n~ve\u2217 (11)\nwhere Ecm denotes the set of context entities described above.\nTo estimate the coherence score, we again use cosine similarity between the vector of entity ~ve and that of context entities ~vce ."}, {"heading": "3.1.3 Learning to Rank", "text": "To combine the proposed contextual information described above with standard NED features, we employ a method of supervised machine learning to rank the candidate entities given mention m and document d.\nIn particular, we use Gradient Boosted Regression Trees (GBRT) (Friedman, 2001), a state-of-the-art point-wise learning-to-rank algorithm widely used for various tasks, which has been recently adopted for the sort of tasks for which we employ it here (Meij et al., 2012). GBRT consists of an ensemble of regression trees, and predicts a relevance score given an instance. We use the GBRT implementation in scikit-learn3 and the logistic loss is used as the loss function. The main parameters of GBRT are the number of iterations \u03b7, the learning rate \u03b2, and the maximum depth of the decision trees \u03be.\nWith regard to the features of machine learning, we first use prior probability (P (e|m)) and entity prior (P (e)). Further, we include a feature representing the maximum prior probability of the candidate entity e of all mentions in the document. We also add the number of entity candidates for mention m as a feature. The above set of four features is called base features in the rest of the paper.\nWe also use several string similarity features used in past work on NED (Meij et al., 2012). These features aim to capture the similarity between the title of entity e and the surface of mention m, and consist of the edit distance, whether the title of entity e exactly equals or contains the surface of mention m, and whether the title of entity e starts or ends with the surface of mention m.\n3 http://scikit-learn.org/\nFinally, we include contextual features measured using the proposed embedding. We use cosine similarity between the candidate entity and the textual context (see Section 3.1.1), and similarity between an entity and contextual entities (see Section 3.1.2). Furthermore, we include the rank of entity e among candidate entities of mention m, sorted according to these two similarity scores in descending order."}, {"heading": "4 Experiments", "text": "In this section, we describe the setup and results of our experiments. In addition to experiments on the NED task, we conducted two experiments\u2014one involving a word similarity and another involving an entity relatedness\u2014in order to test the effectiveness of our method in capturing pairwise similarity between pairs of words as well as pairs of entities. We first describe the details of the training of the embedding and then present the experimental results."}, {"heading": "4.1 Training for the Proposed Embedding", "text": "To train the proposed embedding, we used the December 2014 version of the Wikipedia dump4. We first removed the pages for navigation, maintenance, and discussion, and used the remaining 4.9 million pages. We parsed the Wikipedia pages and extracted text and anchors from each page. We further tokenized the text using the Apache OpenNLP tokenizer. We also filtered out rare words that appeared fewer than five times in the corpus. We thus obtained approximately 2 billion tokens and 73 million anchors. The total number of words and entities in the embedding were approximately 2.1 million and 5 million, respectively. Consequently, the number of rows of matrices V and U were 7.1 million.\nThe number of dimensions d of the embedding was set to 500. Following (Mikolov et al., 2013b), we also used learning rate \u03b1 = 0.025 which linearly decreased with the iterations of the Wikipedia dump. Regarding the other parameters, we set the\n4The dump was retrieved from Wikimedia Downloads. http://dumps.wikimedia.org/\nsize of the context window c = 10 and the negative samples g = 30. The model was trained online by iterating over pages in the Wikipedia dump 10 times. The training lasted approximately five days using a server with a 40-core CPU on Amazon EC2."}, {"heading": "4.2 Word Similarity", "text": "In order to test the quality of vector representations of words, we used three standard word similarity datasets: the WordSim-353 dataset (Finkelstein et al., 2002), the MC dataset (Miller and Charles, 1991), and the RG dataset (Rubenstein and Goodenough, 1965) that contain 353, 65, and 30 word pairs, respectively. Each word pair has a gold-standard similarity score assigned by human judges.\nWe used cosine similarity to calculate similarity score between any pair of words. Following past work, we computed the correlation between similarity scores through human judgments on a set of word pairs using Spearman\u2019s rank correlation coefficient. Here, we adopted the skip-gram model as baseline.\nWe used our implementation to train the skipgram model. Furthermore, the following parameters were used to train the model: d = 500, c = 10, g = 30, and \u03b1 = 0.025. We trained the model by iterating over the Wikipedia dump 10 times.\nTable 1 shows the results. Compared to the skipgram model, our method performed comparably on the WordSim-353 dataset, and slightly better on other datasets, thus showing that the proposed extension to the skip-gram model can be also beneficial for improving word representations."}, {"heading": "4.3 Entity Relatedness", "text": "To test the quality of the vector representation of entities, we conducted an experiment using a dataset for entity relatedness created by Ceccarelli et al. (Ceccarelli et al., 2013). The dataset consists of training, test, and validation sets, and we only use the test set. The test set contains 3,314 entities, where each entity has 91 candi-\ndate entities with gold-standard labels indicating whether the two entities are related. Following (Huang et al., 2015), we obtained the ranked order of the candidate entities using cosine similarity between the target entity and each of the candidate entities, and computed the two standard measures: normalized discounted cumulative gain (NDCG) (Ja\u0308rvelin and Keka\u0308la\u0308inen, 2002) and mean average precision (MAP) (Manning et al., 2008). We adopted WLM as baseline.\nTable 2 shows the results. The score for WLM was obtained from Huang et al. (Huang et al., 2015). Our method clearly outperformed WLM. The results show that our method accurately captures pairwise entity relatedness."}, {"heading": "4.4 Named Entity Disambiguation", "text": ""}, {"heading": "4.4.1 Setup", "text": "We now explain our experimental setup for the NED task. We tested the performance of our proposed method on two standard NED datasets: the CoNLL dataset and the TAC 2010 dataset. The details of these datasets are provided below. Moreover, as with the corpus used in the embedding, we used the December 2014 version of the Wikipedia dump as the referent KB, and to derive the prior probability as well as the entity prior.\nTo find the best parameters for our machine learning model, we ran a parameter search on the CoNLL development set. We used \u03b7 = 10, 000 trees, and tested all combinations of the learning rate \u03b2 = {0.01, 0.02, 0.03, 0.05} and the maximum depth of the decision trees \u03be = {3, 4, 5}. We computed their accuracy on the dataset, and found that the parameters did not significantly affect performance (1.0% at most). We used \u03b2 = 0.02 and \u03be = 4 which yielded the best performance.\nCoNLL The CoNLL dataset is a popular NED dataset constructed by Hoffart et al. (Hoffart et al., 2011). The dataset is based on NER data from the CoNLL 2003 shared task, and consists of training, development, and test sets, containing 946, 216, and 231 documents, respectively. We trained our machine learning model using the training set and reported its performance using the test set. We also used the development set for the parameter tuning described\nabove. Following (Hoffart et al., 2011), we only used 27,816 mentions with valid entries in the KB and reported the standard micro- (aggregates over all mentions) and macro- (aggregates over all documents) accuracies of the top-ranked candidate entities to assess disambiguation performance. For candidate generation, we used a public dataset5 built by Pershina et al. (Pershina et al., 2015).\nTAC 2010 The TAC 2010 dataset is another popular NED dataset constructed for the Text Analysis Conference (TAC)6 (Ji et al., 2010). The dataset is based on news articles from various agencies and Web log data, and consists of a training and a test set containing 1,043 and 1,013 documents, respectively. Following past work (He et al., 2013; Chisholm and Hachey, 2015), we used mentions only with a valid entry in the KB, and reported the micro-accuracy score of the top-ranked candidate entities. We trained our model using the training set and assessed its performance using the test set. We trained our model using the training set and assessed its performance using the test set. Consequently, we evaluated our model on 1,020 mentions contained in the test set. For candidate generation, we used a dictionary that was directly built from the Wikipedia dump mentioned previously. Similar to past work, we retrieved possible mention surfaces of an entity from (1) the title of the entity, (2) the title of another entity redirecting to the entity, and (3) the names of anchors that point to the entity. Further, we retained the top 50 candidates through their entity priors for computational efficiency."}, {"heading": "4.4.2 Comparison with State-of-the-art Methods", "text": "We compared our method with the following recently proposed state-of-the-art methods:\n\u2022 Hoffart et al. (Hoffart et al., 2011) is a graphbased approach that finds a dense subgraph of entities in a document to address NED.\n\u2022 He et al. (He et al., 2013) uses deep neural networks to derive the representations of entities and mention contexts and applies them to NED.\n\u2022 Chisholm and Hachey\n5 https://github.com/masha-p/PPRforNED 6 http://www.nist.gov/tac/\n(Chisholm and Hachey, 2015) uses a Wikilinks dataset (Singh et al., 2012) to improve the performance of NED."}, {"heading": "4.4.3 Results", "text": "Table 3 shows the experimental results of our proposed method as well as those of state-of-the-art methods. Our proposed method achieved a 93.1% micro-accuracy and 92.6% macro-accuracy on the CoNLL dataset, and 85.2% micro-accuracy on the TAC 2010 dataset. Our method significantly outperformed all the other state-of-the-art methods on both datasets by significant margins."}, {"heading": "4.4.4 Feature Study", "text": "We conducted a feature study on our method. We began with base features, added various features to our system incrementally, and reported their impact on performance. We then introduced our two-step approach to achieve the final results.\nTable 4 shows the results. Surprisingly, we attained results comparable with those of most stateof-the-art methods on the both datasets by only us-\ning base features. Adding string similarity features slightly further improved performance.\nWe observed significant improvement when adding textual context features based on our proposed embedding. Our method outperformed other state-of-the-art methods without using coherence.\nFurther, coherence based on unambiguous entity mentions and our two-step approach significantly improved performance on the CoNLL dataset. However, it did not contribute to performance on the TAC 2010 dataset. This was because of the significant difference in the density of entity mentions between the datasets. The CoNLL dataset contains approximately 20 entity mentions per document, but the TAC 2010 only contains approximately one mention per document which is unarguably insufficient to model coherence."}, {"heading": "4.4.5 Error Analysis", "text": "We also conducted an error analysis on the CoNLL test set. We observed that approximately 48.6% errors were caused by metonymy mentions (Ling et al., 2015) (i.e., mentions with more than one plausible annotation). In particular, our NED method often erred when an incorrect entity was highly popular and exactly matched the mention surface (e.g., \u201cSouth Africa\u201d referring to the entity South Africa national rugby union team rather than the entity South Africa). This makes sense because our machine learning model uses the popularity statistics of the KB (i.e., prior probability and entity prior), and the string similarity between the title of the entity and the mention surface. This problem is discussed further in (Ling et al., 2015)."}, {"heading": "5 Related Work", "text": "Early NED methods addressed the problem as a well-studied word sense disambiguation problem (Mihalcea and Csomai, 2007). These methods primarily focused on modeling the similarity of textual (local) context. Most recent stateof-the-art methods focus on modeling coherence among disambiguated entities in the same document (Cucerzan, 2007; Milne and Witten, 2008b; Hoffart et al., 2011; Ratinov et al., 2011). These approaches have also been called collective or global approaches in the literature.\nLearning the representations of entities for NED has been addressed in past literature. Guo and Barbosa (Guo and Barbosa, 2014) used random walks on KB graphs to construct vector representations of entities and documents to address NED. Blanco et al. (Blanco et al., 2015) proposed a method to map entities into the word embedding (i.e., Word2vec (Mikolov et al., 2013b)) space using entity descriptions in the KB and applied it for NED. He et al. (He et al., 2013) used deep neural networks to compute representations of entities and contexts of mentions directly from the KB. Similarly, Sun et al. (Sun et al., 2015) proposed a method based on deep neural networks to model representations of mentions, contexts of mentions, and entities. Huang et al. (Huang et al., 2015) also leveraged deep neural networks to learn entity representations such that the consequent pairwise entity relatedness was more suitable than of a standard method (i.e., WLM) for NED. Further, Hu et al. (Hu et al., 2015) used hierarchical information in the KB to build entity embedding and applied it to model coherence. Unlike these methods, our proposed approach involves jointly learning vector representations of entities as well as words, hence enabling the accurate computation of the semantic similarity among its items to model both the textual context and coherence.\nFurthermore, in the context of knowledge graph embedding, another tenor of recent works has been published (Bordes et al., 2011; Socher et al., 2013; Lin et al., 2015). These methods focus on learning vector representations of entities to primarily address the link prediction task that aims to predict a new fact based on existing facts in KB. Particularly, Wang et al. (Wang et al., 2014) have recently revealed that the joint modeling of the embedding of words and entities can improve performance in several tasks including the link prediction task, which is somewhat analogous to our experimental results."}, {"heading": "6 Conclusions", "text": "In this paper, we proposed an embedding method to jointly map words and entities into the same continuous vector space. Our method enables us to effectively model both textual and global contexts. Further, armed with these context models, our NED method significantly outperforms state-of-the-\nart NED methods. In future work, we intend to improve our model by leveraging relevant knowledge, such as relations in a knowledge graph (e.g., Freebase). We would also like to seek applications of our proposed embedding other than NED.\nThe code of the proposed embedding method and the pre-trained vectors used in our experiments will be made publicly available before the conference."}], "references": [{"title": "Fast and Space-Efficient Entity Linking for Queries", "author": ["Blanco et al.2015] Roi Blanco", "Giuseppe Ottaviano", "Edgar Meij"], "venue": "In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining (WSDM),", "citeRegEx": "Blanco et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Blanco et al\\.", "year": 2015}, {"title": "Learning Structured Embeddings of Knowledge Bases", "author": ["Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": "In Proceedings of the 25th Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "Using Encyclopedic Knowledge for Named Entity Disambiguation", "author": ["Bunescu", "Pasca2006] Razvan Bunescu", "Marius Pasca"], "venue": "In Proceedings of the 11th Conference of the European Chapter of the Association", "citeRegEx": "Bunescu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2006}, {"title": "Learning Relatedness Measures for Entity Linking", "author": ["Claudio Lucchese", "Salvatore Orlando", "Raffaele Perego", "Salvatore Trani"], "venue": "In Proceedings of the 22nd ACM International Conference on Information and Knowledge", "citeRegEx": "Ceccarelli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ceccarelli et al\\.", "year": 2013}, {"title": "Entity Disambiguation with Web Links. Transactions of the Association for Computational Linguistics, 3:145\u2013156", "author": ["Chisholm", "Hachey2015] Andrew Chisholm", "Ben Hachey"], "venue": null, "citeRegEx": "Chisholm et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chisholm et al\\.", "year": 2015}, {"title": "Large-Scale Named Entity Disambiguation Based on Wikipedia Data", "author": ["Silviu Cucerzan"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning", "citeRegEx": "Cucerzan.,? \\Q2007\\E", "shortCiteRegEx": "Cucerzan.", "year": 2007}, {"title": "Placing Search in Context: The Concept Revisited", "author": ["Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "Finkelstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2002}, {"title": "Greedy Function Approximation: A Gradient Boosting Machine", "author": ["Jerome H. Friedman"], "venue": "The Annals of Statistics,", "citeRegEx": "Friedman.,? \\Q2001\\E", "shortCiteRegEx": "Friedman.", "year": 2001}, {"title": "Entity Linking with a Unified Semantic Representation", "author": ["Guo", "Barbosa2014] Zhaochen Guo", "Denilson Barbosa"], "venue": "In Proceedings of the 23rd International Conference on World Wide Web (WWW),", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Learning Entity Representation for Entity Disambiguation", "author": ["He et al.2013] Zhengyan He", "Shujie Liu", "Mu Li", "Ming Zhou", "Longkai Zhang", "Houfeng Wang"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "He et al\\.,? \\Q2013\\E", "shortCiteRegEx": "He et al\\.", "year": 2013}, {"title": "Robust Disambiguation of Named Entities in Text", "author": ["Mohamed Amir Yosef", "Ilaria Bordino", "Hagen F\u00fcrstenau", "Manfred Pinkal", "Marc Spaniol", "Bilyana Taneva", "Stefan Thater", "Gerhard Weikum"], "venue": null, "citeRegEx": "Hoffart et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffart et al\\.", "year": 2011}, {"title": "KORE: Keyphrase Overlap Relatedness for Entity Disambiguation", "author": ["Stephan Seufert", "Dat Ba Nguyen", "Martin Theobald", "Gerhard Weikum"], "venue": "In Proceedings of the 21st ACM International Conference on Informa-", "citeRegEx": "Hoffart et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hoffart et al\\.", "year": 2012}, {"title": "Entity Hierarchy Embedding", "author": ["Hu et al.2015] Zhiting Hu", "Poyao Huang", "Yuntian Deng", "Yingkai Gao", "Eric Xing"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural", "citeRegEx": "Hu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2015}, {"title": "Leveraging Deep Neural Networks and Knowledge Graphs for Entity Disambiguation", "author": ["Huang et al.2015] Hongzhao Huang", "Larry Heck", "Heng Ji"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Cumulated gain-based evaluation of IR techniques", "author": ["J\u00e4rvelin", "Kek\u00e4l\u00e4inen2002] Kalervo J\u00e4rvelin", "Jaana Kek\u00e4l\u00e4inen"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "J\u00e4rvelin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "J\u00e4rvelin et al\\.", "year": 2002}, {"title": "Overview of the TAC 2010 Knowledge Base Population Track", "author": ["Ji et al.2010] Heng Ji", "Ralph Grishman", "Hoa Trang Dang", "Kira Griffitt", "Joe Ellis"], "venue": "In Proceeding of Text Analytics Conference (TAC)", "citeRegEx": "Ji et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2010}, {"title": "Learning Entity and Relation Embeddings for Knowledge Graph Completion", "author": ["Lin et al.2015] Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"], "venue": "In Proceedings of the 29th AAAI Conference on Artificial Intelligence (AAAI)", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Design Challenges for Entity Linking. Transactions of the Association for Computational Linguistics, 3:315\u2013328", "author": ["Ling et al.2015] Xiao Ling", "Sameer Singh", "Daniel S. Weld"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Introduction to Information Retrieval", "author": ["Prabhakar Raghavan", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Overview of the TAC 2009 Knowledge Base Population Track", "author": ["McNamee", "Dang2009] P McNamee", "HT Dang"], "venue": "In Proceeding of Text Analysis Conference (TAC)", "citeRegEx": "McNamee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "McNamee et al\\.", "year": 2009}, {"title": "Adding Semantics to Microblog Posts", "author": ["Meij et al.2012] Edgar Meij", "Wouter Weerkamp", "Maarten de Rijke"], "venue": "In Proceedings of the Fifth ACM International Conference on Web Search and Data Mining (WSDM),", "citeRegEx": "Meij et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Meij et al\\.", "year": 2012}, {"title": "Wikify!: Linking Documents to Encyclopedic Knowledge", "author": ["Mihalcea", "Csomai2007] Rada Mihalcea", "Andras Csomai"], "venue": "In Proceedings of the Sixteenth ACM Conference on Information and Knowledge Management (CIKM),", "citeRegEx": "Mihalcea et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2007}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Greg Corrado", "Kai Chen", "Jeffrey Dean"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Contextual Correlates of Semantic Similarity", "author": ["Miller", "Charles1991] George A. Miller", "Walter G. Charles"], "venue": "Language and Cognitive Processes,", "citeRegEx": "Miller et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1991}, {"title": "An Effective, Low-Cost Measure of Semantic Relatedness Obtained from Wikipedia Links", "author": ["Milne", "Witten2008a] David Milne", "Ian H. Witten"], "venue": "In Proceedings of the First AAAI Workshop on Wikipedia and Artificial Intelligence (WIKIAI)", "citeRegEx": "Milne et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Milne et al\\.", "year": 2008}, {"title": "Learning to Link with Wikipedia", "author": ["Milne", "Witten2008b] David Milne", "Ian H. Witten"], "venue": "In Proceeding of the 17th ACM Conference on Information and Knowledge Management (CIKM),", "citeRegEx": "Milne et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Milne et al\\.", "year": 2008}, {"title": "GloVe: Global Vectors for Word Representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Personalized Page Rank for Named Entity Disambiguation", "author": ["Yifan He", "Ralph Grishman"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics:", "citeRegEx": "Pershina et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pershina et al\\.", "year": 2015}, {"title": "Local and Global Algorithms for Disambiguation to Wikipedia", "author": ["Ratinov et al.2011] Lev Ratinov", "Dan Roth", "Doug Downey", "Mike Anderson"], "venue": "In Proceedings of the 49th Annual Meeting of the Associa-", "citeRegEx": "Ratinov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2011}, {"title": "Contextual Correlates of Synonymy", "author": ["Rubenstein", "John B. Goodenough"], "venue": "Communications of the ACM,", "citeRegEx": "Rubenstein et al\\.,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein et al\\.", "year": 1965}, {"title": "Wikilinks: A Large-scale Cross-Document Coreference Corpus Labeled via Links to Wikipedia", "author": ["Singh et al.2012] Sameer Singh", "Amarnag Subramanya", "Fernando Pereira", "Andrew McCallum"], "venue": "Technical Report UM-CS-2012-015", "citeRegEx": "Singh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2012}, {"title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion", "author": ["Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Modeling Mention, Context and Entity with Neural Networks for Entity Disambiguation", "author": ["Sun et al.2015] Yaming Sun", "Lei Lin", "Duyu Tang", "Nan Yang", "Zhenzhou Ji", "Xiaolong Wang"], "venue": "In Proceedings of the Twenty-Fourth International Joint Conference", "citeRegEx": "Sun et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2015}, {"title": "Knowledge Graph and Text Jointly Embedding", "author": ["Wang et al.2014] Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "NED has lately been extensively studied (Cucerzan, 2007; Mihalcea and Csomai, 2007; Milne and Witten, 2008b; Ratinov et al., 2011) and used as a fundamental component in numerous tasks, such as information extraction, knowledge base population (McNamee and Dang, 2009; Ji et al.", "startOffset": 40, "endOffset": 130}, {"referenceID": 29, "context": "NED has lately been extensively studied (Cucerzan, 2007; Mihalcea and Csomai, 2007; Milne and Witten, 2008b; Ratinov et al., 2011) and used as a fundamental component in numerous tasks, such as information extraction, knowledge base population (McNamee and Dang, 2009; Ji et al.", "startOffset": 40, "endOffset": 130}, {"referenceID": 15, "context": ", 2011) and used as a fundamental component in numerous tasks, such as information extraction, knowledge base population (McNamee and Dang, 2009; Ji et al., 2010), and semantic search (Blanco et al.", "startOffset": 121, "endOffset": 162}, {"referenceID": 0, "context": ", 2010), and semantic search (Blanco et al., 2015).", "startOffset": 29, "endOffset": 50}, {"referenceID": 27, "context": "Word embedding methods are also becoming increasingly popular (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014).", "startOffset": 62, "endOffset": 133}, {"referenceID": 11, "context": "Despite its simplicity, WLM yields state-of-the-art performance (Hoffart et al., 2012).", "startOffset": 64, "endOffset": 86}, {"referenceID": 7, "context": "In particular, we use Gradient Boosted Regression Trees (GBRT) (Friedman, 2001), a state-of-the-art point-wise learning-to-rank algorithm widely used for various tasks, which has been recently adopted for the sort of tasks for which we employ it here (Meij et al.", "startOffset": 63, "endOffset": 79}, {"referenceID": 20, "context": "In particular, we use Gradient Boosted Regression Trees (GBRT) (Friedman, 2001), a state-of-the-art point-wise learning-to-rank algorithm widely used for various tasks, which has been recently adopted for the sort of tasks for which we employ it here (Meij et al., 2012).", "startOffset": 251, "endOffset": 270}, {"referenceID": 20, "context": "We also use several string similarity features used in past work on NED (Meij et al., 2012).", "startOffset": 72, "endOffset": 91}, {"referenceID": 6, "context": "In order to test the quality of vector representations of words, we used three standard word similarity datasets: the WordSim-353 dataset (Finkelstein et al., 2002), the MC dataset (Miller and Charles, 1991), and the RG dataset (Rubenstein and Goodenough, 1965) that contain 353, 65, and 30 word pairs, respectively.", "startOffset": 138, "endOffset": 164}, {"referenceID": 3, "context": "(Ceccarelli et al., 2013).", "startOffset": 0, "endOffset": 25}, {"referenceID": 13, "context": "Following (Huang et al., 2015), we obtained the ranked order of the candidate entities using cosine similarity between the target entity and each of the candidate entities, and computed the two standard measures: normalized discounted cumulative gain (NDCG) (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002) and mean average precision (MAP) (Manning et al.", "startOffset": 10, "endOffset": 30}, {"referenceID": 18, "context": ", 2015), we obtained the ranked order of the candidate entities using cosine similarity between the target entity and each of the candidate entities, and computed the two standard measures: normalized discounted cumulative gain (NDCG) (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002) and mean average precision (MAP) (Manning et al., 2008).", "startOffset": 300, "endOffset": 322}, {"referenceID": 13, "context": "(Huang et al., 2015).", "startOffset": 0, "endOffset": 20}, {"referenceID": 10, "context": "(Hoffart et al., 2011).", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "Following (Hoffart et al., 2011), we only used 27,816 mentions with valid entries in the KB and reported the standard micro- (aggregates over all mentions) and macro- (aggregates over all documents) accuracies of the top-ranked candidate entities to assess disambiguation performance.", "startOffset": 10, "endOffset": 32}, {"referenceID": 28, "context": "(Pershina et al., 2015).", "startOffset": 0, "endOffset": 23}, {"referenceID": 15, "context": "TAC 2010 The TAC 2010 dataset is another popular NED dataset constructed for the Text Analysis Conference (TAC)6 (Ji et al., 2010).", "startOffset": 113, "endOffset": 130}, {"referenceID": 9, "context": "Following past work (He et al., 2013; Chisholm and Hachey, 2015), we used mentions only with a valid entry in the KB, and reported the micro-accuracy score of the top-ranked candidate entities.", "startOffset": 20, "endOffset": 64}, {"referenceID": 10, "context": "(Hoffart et al., 2011) is a graphbased approach that finds a dense subgraph of entities in a document to address NED.", "startOffset": 0, "endOffset": 22}, {"referenceID": 9, "context": "(He et al., 2013) uses deep neural networks to derive the representations of entities and mention contexts and applies them to NED.", "startOffset": 0, "endOffset": 17}, {"referenceID": 31, "context": "(Chisholm and Hachey, 2015) uses a Wikilinks dataset (Singh et al., 2012) to improve the performance of NED.", "startOffset": 53, "endOffset": 73}, {"referenceID": 17, "context": "6% errors were caused by metonymy mentions (Ling et al., 2015) (i.", "startOffset": 43, "endOffset": 62}, {"referenceID": 17, "context": "This problem is discussed further in (Ling et al., 2015).", "startOffset": 37, "endOffset": 56}, {"referenceID": 5, "context": "Most recent stateof-the-art methods focus on modeling coherence among disambiguated entities in the same document (Cucerzan, 2007; Milne and Witten, 2008b; Hoffart et al., 2011; Ratinov et al., 2011).", "startOffset": 114, "endOffset": 199}, {"referenceID": 10, "context": "Most recent stateof-the-art methods focus on modeling coherence among disambiguated entities in the same document (Cucerzan, 2007; Milne and Witten, 2008b; Hoffart et al., 2011; Ratinov et al., 2011).", "startOffset": 114, "endOffset": 199}, {"referenceID": 29, "context": "Most recent stateof-the-art methods focus on modeling coherence among disambiguated entities in the same document (Cucerzan, 2007; Milne and Witten, 2008b; Hoffart et al., 2011; Ratinov et al., 2011).", "startOffset": 114, "endOffset": 199}, {"referenceID": 0, "context": "(Blanco et al., 2015) proposed a method to map entities into the word embedding (i.", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "(He et al., 2013) used deep neural networks to compute representations of entities and contexts of mentions directly from the KB.", "startOffset": 0, "endOffset": 17}, {"referenceID": 33, "context": "(Sun et al., 2015) proposed a method based on deep neural networks to model representations of mentions, contexts of mentions, and entities.", "startOffset": 0, "endOffset": 18}, {"referenceID": 13, "context": "(Huang et al., 2015) also leveraged deep neural networks to learn entity representations such that the consequent pairwise entity relatedness was more suitable than of a standard method (i.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "(Hu et al., 2015) used hierarchical information in the KB to build entity embedding and applied it to model coherence.", "startOffset": 0, "endOffset": 17}, {"referenceID": 1, "context": "Furthermore, in the context of knowledge graph embedding, another tenor of recent works has been published (Bordes et al., 2011; Socher et al., 2013; Lin et al., 2015).", "startOffset": 107, "endOffset": 167}, {"referenceID": 32, "context": "Furthermore, in the context of knowledge graph embedding, another tenor of recent works has been published (Bordes et al., 2011; Socher et al., 2013; Lin et al., 2015).", "startOffset": 107, "endOffset": 167}, {"referenceID": 16, "context": "Furthermore, in the context of knowledge graph embedding, another tenor of recent works has been published (Bordes et al., 2011; Socher et al., 2013; Lin et al., 2015).", "startOffset": 107, "endOffset": 167}, {"referenceID": 34, "context": "(Wang et al., 2014) have recently revealed that the joint modeling of the embedding of words and entities can improve performance in several tasks including the link prediction task, which is somewhat analogous to our experimental results.", "startOffset": 0, "endOffset": 19}], "year": 2017, "abstractText": "Named Entity Disambiguation (NED) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base (KB) (e.g., Wikipedia). In this paper, we propose a novel embedding method specifically designed for NED. The proposed method jointly maps words and entities into the same continuous vector space. We extend the skip-gram model by using two models. The KB graph model learns the relatedness of entities using the link structure of the KB, whereas the anchor context model aims to align vectors such that similar words and entities occur close to one another in the vector space by leveraging KB anchors and their context words. By combining contexts based on the proposed embedding with standard NED features, we achieved state-of-the-art accuracy of 93.1% on the standard CoNLL dataset and 85.2% on the TAC 2010 dataset. Our code and pre-trained vectors will be made available online.", "creator": "LaTeX with hyperref package"}}}