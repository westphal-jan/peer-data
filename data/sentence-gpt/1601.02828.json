{"id": "1601.02828", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jan-2016", "title": "Learning Hidden Unit Contributions for Unsupervised Acoustic Model Adaptation", "abstract": "This work presents a broad study on the adaptation of neural network acoustic models by means of learning hidden unit contributions (LHUC) -- a method that linearly re-combines hidden units in a speaker- or environment-dependent manner using small amounts of unsupervised adaptation data. We also extend LHUC to a speaker adaptive training (SAT) framework that leads to more adaptable DNN acoustic model, which can work in both a speaker-dependent and a speaker-independent manner, without the requirement to maintain auxiliary speaker-dependent feature extractors or to introduce significant speaker-dependent changes to the DNN structure. Through a series of experiments on four different speech recognition benchmarks (TED talks, Switchboard, AMI meetings and Aurora4) and over 270 test speakers we show that LHUC in both its test-only and SAT variants results in consistent word error rate reductions ranging from 5% to 23% relative depending on the task and the degree of mismatch between training and test data. We find that the more experienced speaker-dependent model, the better the performance of the tests. These findings demonstrate that when performing SAT training on a speaker-dependent SAT variant (TUS) the more accurate predictions are expressed.", "histories": [["v1", "Tue, 12 Jan 2016 12:33:56 GMT  (474kb,D)", "http://arxiv.org/abs/1601.02828v1", "13 pages, Submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing"], ["v2", "Wed, 13 Jul 2016 17:47:07 GMT  (2236kb,D)", "http://arxiv.org/abs/1601.02828v2", "14 pages, 9 Tables, 11 Figues in IEEE/ACM Transactions on Audio, Speech and Language Processing, Vol. 24, Num. 8, 2016"]], "COMMENTS": "13 pages, Submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.SD", "authors": ["pawel swietojanski", "jinyu li", "steve renals"], "accepted": false, "id": "1601.02828"}, "pdf": {"name": "1601.02828.pdf", "metadata": {"source": "CRF", "title": "Learning Hidden Unit Contributions for Unsupervised Acoustic Model Adaptation", "authors": ["Pawel Swietojanski"], "emails": ["p.swietojanski@ed.ac.uk", "s.renals@ed.ac.uk", "jinyu.li@microsoft.com"], "sections": [{"heading": null, "text": "I. INTRODUCTION AND SUMMARY\nSPEECH recognition accuracies have improved substan-tially over the past several years through the use of (deep) neural network (DNN) acoustic models. Hinton et al [1] report word error rate (WER) reductions between 10\u201332% across a wide variety of tasks, compared with discriminatively trained Gaussian mixture model (GMM) based systems. These results use neural networks as part of both hybrid DNN/HMM (hidden Markov model) systems [1]\u2013[5] in which the neural network provides a scaled likelihood estimate to replace the GMM, and as tandem or bottleneck feature systems [6], [7] in which the neural network is used as a discriminative feature extractor for a GMM-based system. For many tasks it has been observed that GMM-based systems (with tandem or bottleneck features) that have been adapted to the talker are more accurate than unadapted hybrid DNN/HMM systems [8]\u2013[10], indicating that the adaptation of DNN acoustic models is an important topic that merits investigation.\nAcoustic model adaptation [11] aims to normalise the mismatch between training and runtime data distributions that\nP Swietojanski and S Renals are with the Centre for Speech Technology Research, University of Edinburgh, UK, email: {p.swietojanski,s.renals}@ed.ac.uk\nJ Li is with Microsoft Corporation. One Microsoft Way, WA, USA, email: jinyu.li@microsoft.com\nPS and SR were supported by EPSRC Programme Grant grant EP/I031022/1 (Natural Speech Technology). One of the K40 GPGPU boards used in this research was donated by NVIDA Corporation.\narises owing to the acoustic variability across speakers, as well as other distortions introduced by the channel or acoustic environment. In this paper we investigate unsupervised modelbased adaptation of DNN acoustic models to speakers and to acoustic environments, using a recently introduced method called Learning Hidden Unit Contributions (LHUC) [12]\u2013 [14]. We present the LHUC approach both in the context of test-only adaptation, and an extension to speaker-adaptive training (SAT), referred to as SAT-LHUC [14]. We present an extensive experimental analysis using four standard corpora: TED talks [15], AMI [16], Switchboard [17] and Aurora4 [18]. These experiments include: adaptation of both cross-entropy and sequence trained DNN acoustic models (Sec. VI-A\u2013 VI-C); an analysis in terms of the quality of adaptation targets, quality of adaptation data and the amount of adaptation data (Sec. VI-D); complementarity with feature-space adaptation techniques based on maximum likelihood linear regression [19] (Sec. VI-E); and application to combined speaker and environment adaptation (Sec. VII)."}, {"heading": "II. REVIEW OF NEURAL NETWORK ACOUSTIC ADAPTATION", "text": "Approaches to the adaptation of neural network acoustic models can be considered as operating either in the feature space, or in the model space, or as a hybrid approach in which speaker-, utterance-, or environment-dependent auxiliary features are appended to the standard acoustic features.\nThe dominant technique for estimating feature space transforms is constrained (feature-space) MLLR, referred to as CMLLR or fMLLR [19]. fMLLR is an adaptation method developed for GMM-based acoustic models, in which an affine transform of the input acoustic features is estimated by maximising the log-likelihood that the model generates the adaptation data based on first pass alignments. To use fMLLR with a DNN-based system, it is first necessary to train a complete GMM-based system, which is then used to estimate a single input transform per speaker. The transformed feature vectors are then used to train a DNN in a speaker adaptive manner and another set of transforms is estimated (using the GMM) during evaluation for unseen speakers. This technique has been shown to be effective in reducing WER across several different data sets, in both hybrid and tandem approaches [1], [4], [8], [9], [20]\u2013[23]. Similar techniques have also been developed to operate directly on neural networks. The linear input network (LIN) [24], [25] defines an additional speakerdependent layer between the input features and the first hidden layer, and thus has a similar effect to fMLLR. This technique ar X\niv :1\n60 1.\n02 82\n8v 1\n[ cs\n.C L\n] 1\n2 Ja\nn 20\n16\nhas been further developed to include the use of a tied variant of LIN in which each of the input frames is constrained to have the same linear transform \u2013 feature-space discriminative linear regression (fDLR) [4], [26]. LIN or fDLR have been mostly used in test-only adaptation schemes; to make use of fMLLR transforms one needs to perform SAT training, which can usually better compensate against variability in acoustic space.\nAn alternative speaker-adaptive training approach \u2013 auxiliary features \u2013 augments the acoustic feature vectors with additional speaker-specific features computed for each speaker at both training and test stages. There has been considerable recent work exploring the use of i-vectors [27] for this purpose. I-vectors, which can be regarded as basis vectors which span a subspace of speaker variability, were first used for adaptation in a GMM framework by Karafiat et al [28]. Saon et al [29] used i-vectors to augment the input features of DNN-based acoustic models, and showed that augmenting the input features with 100-dimensional i-vectors for each speaker resulted in a 10% relative reduction in WER on Switchboard (and a 6% reduction when the input features had been transformed using fMLLR). Gupta et al [30] obtained similar results, and Karanasou et al [31] presented an approach in which the ivectors were factorised into speaker and environment parts. Miao et al [32] proposed to transform i-vectors using an auxiliary DNN which produced speaker-specific transforms of the original feature vectors, similar to fMLLR. Other examples of auxiliary features include the use of speaker-specific bottleneck features obtained from a speaker separation DNN used in a distant speech recognition task [33], the use of outof-domain tandem features [23], and speaker codes [34]\u2013[36] in which a specific set of units for each speaker is optimised. Speaker codes require speaker adaptive (re-)training, owing to the additional connection weights between codes and hidden units.\nModel-based adaptation relies on a direct update of DNN parameters. Liao [37] investigated supervised and unsupervised adaptation of different weight subsets using a few minutes of adaptation data. On a large net (60M weights), up to 5% relative improvement was observed for unsupervised adaptation when all weights were adapted. Yu et al [38] have explored the use of regularisation for adapting the weights of a DNN, using the Kullback-Liebler (KL) divergence between the speaker-independent output distribution and the speakeradapted output distributions, resulting in a 3% relative improvement on Switchboard. This approach was also recently used to adapt all parameters of sequence-trained models [39]. A variant of this approach reduces the number of speakerspecific parameters through a factorisation based on singular value decomposition [40]. Ochiai et al [41] have also explored regularised speaker adaptive training with a speaker-dependent layer.\nDirectly adapting the weights of a large DNN results in extremely large speaker-dependent parameter sets, and a computationally intensive adaptation process. Smaller subsets of the DNN weights may be modified, including output layer biases [26], the bias and slope of hidden units [42] or training the models with differentiable pooling operators [43], which\nare then adapted in SD fashion. Siniscalchi et al [44] also investigated the use of Hermite polynomial activation functions, whose parameters are estimated in a speaker adaptive fashion. One can also adapt the top layer in a Bayesian fashion resulting in a maximum a posteriori (MAP) approach [45], or address the sparsity of context-dependent tied-states when few adaptation data-points are available by using multi-task adaptation, using monophones to adapt the context-dependent output layer [46], [47]. A similar approach, but using a hierarchical output layer (tied-states followed by monophones) rather than multi-task adaptation, has also been proposed [48]."}, {"heading": "III. LEARNING HIDDEN UNIT CONTRIBUTIONS (LHUC)", "text": "A DNN may be viewed as a set of adaptive basis functions, \u03c8(\u00b7). Under certain assumptions on the family of target functions f\u2217 (as well as on the model structure itself) the neural network can act as an universal approximator [49]\u2013[51]. That is, given some vector of input random variables x \u2208 Rd then there exists a neural network fn(x) : Rd \u2192 R of the form\nfn(x) = n\u2211 k=1 rk\u03c8(w > k x+ bk) (1)\nwhich can approximate f\u2217 with an arbitrarily small error with respect to a distance measure such as mean square error (provided n is sufficiently large):\n||f\u2217(x)\u2212 fn(x)||2 \u2264 . (2) In (1) \u03c8 : R \u2192 R is an element-wise non-linear operation applied after an affine transformation which forms an adaptive basis function parametrised by a set of biases bk \u2208 R and a weight vector wk \u2208 Rdx . The target approximation may then be constructed as a linear combination of the basis functions, each weighted by r \u2208 R. The formulation can be extended to m-dimensional mappings fn(x) : Rd \u2192 Rm simply by splicing the models in (1) m times. The properties also hold true when considering deeper (nested) models [49] (Corollaries 2.6 and 2.7).\nDNN training results in the hidden units learning a joint representation of the target function and becoming specialised and complementary to each other. Generalisation corresponds to the learned combination of basis functions continuing to approximate the target function when applied to unseen test data. This interpretation motivates the idea of using LHUC\u2013 Learning Hidden Unit Contributions \u2013 for test-set adaptation. In LHUC the network\u2019s basis functions, previously estimated using a large amount of training data, are kept fixed. Adaptation involves modifying the combination of hidden units in order to minimise the adaptation loss based on the adaptation data. Fig. 1 illustrates this approach for a regression problem, where the adaptation is performed by linear re-combination of basis functions changing only the r parameters from eq. (1).\nThe key idea of LHUC is to explicitly parametrise the amplitudes of each hidden unit, using a speaker-dependent amplitude function. Let hl,sj denote the j-th hidden unit activation (basis) in layer l, and let rl,sj \u2208 R denote the sth speaker-dependent amplitude function:\nhl,sj = \u03be(r l,s j ) \u25e6 \u03c8j ( wl>j x+ b l j ) . (3)\nThe amplitude is modelled using a function \u03be : R \u2192 R+ \u2013 typically a sigmoid with range (0, 2) [13], but an identity function could be used [52]. wlj is the jth column of the corresponding weight matrix Wl \u2208 Rdx\u00d7dh , blj denotes the bias, \u03c8 is the hidden unit activation function (unless stated otherwise, this is assumed to be sigmoid), and \u25e6 denotes a Hadamard product. \u03be constrains the range of the hidden unit amplitude scaling (compare with Fig. 1) hence directly affecting the adaptation transform capacity \u2013 this may be desirable when adapting with potentially noisy unsupervised targets (see Sec. VI-A). LHUC adaptation progresses setting the speaker-specific amplitude parameters rl,sj using gradient descent with targets provided by the adaptation data.\nThe idea of directly learning hidden unit amplitudes was proposed in the context of an adaptive learning rate schedule by Trentin [53], and was later applied to supervised speaker adaptation by Abdel-Hamid and Jiang [12]. The approach was extended to unsupervised adaptation, non-sigmoid nonlinearities, and large vocabulary speech recognition by Swietojanski and Renals [13]. Other adaptive transfer function approaches for speaker adaptation have also been proposed [42], [44], as have \u201cbasis\u201d approaches [54]\u2013[56]. However, the basis in the latter works involved re-tuning parallel models on some pre-defined clusters (gender, speaker, environment) in a supervised manner; the adaptation then relied on learning linear combination coefficients for those sub-models on adaptation data."}, {"heading": "IV. SPEAKER ADAPTIVE TRAINING LHUC (SAT-LHUC)", "text": "When LHUC is applied as a test-only adaptation it assumes that the set of speaker-independent basis functions estimated on the training data provides a good starting point for further tuning to the underlying data distribution of the adaptation data (Fig. 1). However, one can derive a counter-example where this assumption fails: the top plot of Fig. 2 shows example training data uniformly drawn from two competing distributions f1(a) and f1(b) where the linear recombination of the resulting basis in the average model (Fig 2 bottom), provides a poor approximation of adaptation data.\nThis motivates combining LHUC with speaker adaptive training (SAT) [57] in which the hidden units are trained to capture both good average representations and speakerspecific representations, by estimating speaker-specific hidden unit amplitudes for each training speaker. This is visualised in Fig. 3 where, given the prior knowledge of which datapoint comes from which distribution, we estimate a set of parallel LHUC transforms (one per distribution) as well as one extra transform which is responsible for modelling average properties. The top of Fig. 3 shows the same experiment as in Fig 2 but with three LHUC transforms \u2013 one can see that the 4-hidden-unit MLP in this scenario was able to capture each of the underlying distributions as well as the average aspect well, given the LHUC transform. At the same time, the resulting basis functions (Fig 3, bottom) are a better starting point for the adaptation (Fig. 3, middle).\nThe examples presented in Figs. 2 and 3 could be solved by breaking the symmetry through rebalancing the number of training data-points for each function, resulting in less trivial and hence more adaptable basis functions in the average\nmodel. However, as we will show experimentally later, similar effects are also present in high-dimensional speech data, and SAT-LHUC training allows more tunable canonical acoustic models to be built, that can be better tailored to particular speakers through adaptation.\nFor SAT-LHUC, test-only adaptation remains the same as for LHUC, that is, the set of speaker-dependent LHUC parameters \u03b8sLHUC = {{rl,sj } dlh j=1}Ll=1 is inserted for each of test speakers and their values optimised from unsupervised adaptation data. We also use a set of LHUC transforms \u03b8sLHUC , where s = 1 . . . S, for the training speakers which are jointly optimised with the speaker-independent parameters \u03b8SI = {Wl,bl}Ll=1. There is an additional speakerindependent LHUC transform, denoted by \u03b80LHUC , which allows the model to be used in speaker-independent fashion, for example, to produce first pass adaptation targets.\nTo perform SAT training with LHUC, we use the negative log likelihood and maximise the posterior probability of obtaining the correct context-dependent tied-state ct given observation vector xt at time t:\nLSAT (\u03b8SI , \u03b8SD) = \u2212 \u2211 t\u2208D logP (ct|xst ; \u03b8SI ; \u03b8mtLHUC) (4)\nwhere s denotes the sth speaker, mt \u2208 {0, s} selects the SI or SD LHUC transforms from \u03b8SD \u2208 {\u03b80LHUC , . . . , \u03b8SLHUC}\nbased on a Bernoulli distribution:\nkt \u223c Bernoulli(\u03b3) (5)\nmt = { s if kt = 0 0 if kt = 1\n(6)\nwhere \u03b3 is a hyper-parameter specifying the probability the given example is treated as SI. The SI/SD split (determined by equations (5) and (6)) can be performed at speaker, utterance or frame level. We further investigate this aspect in section VI-B. The SAT-LHUC model structure is depicted in Fig 4; notice the alternative routes of forward and backward passes for different speakers.\nDenote by \u2202LSAT /\u2202hl,sj the error back-propagated to the jth unit at the lth layer (eq. (3)). To back propagate through the transform one needs to element-wise multiply it by the transform itself, as follows:\n\u2202LSAT \u2202\u03c8lj = \u2202LSAT \u2202hl,sj \u25e6 \u03be(rl,sj ) . (7)\nTo obtain the gradient with respect to rl,sj :\n\u2202LSAT \u2202rl,sj = \u2202LSAT \u2202hl,sj \u25e6 \u2202\u03be(rl,sj ) \u2202rl,sj \u25e6 \u03c8lj . (8)\nWhen performing mini-batch SAT training one needs to explicitly take account of the fact that different data-points may flow through different transforms: hence the resulting gradient for rl,sj for the sth speaker is the sum of the partial gradients belonging to speaker s:\n\u2202LSAT \u2202rl,sj\n= \u2211\nt,mt=s\n\u2202LSAT \u2202hl,sj \u25e6 \u2202\u03be(rl,sj ) \u2202rl,sj \u25e6 \u03c8lj , (9)\nor 0 in case no data-points for sth speaker in the given minibatch were selected. All adaptation methods studied in this paper require first-pass decoding to obtain adaptation targets to either estimate fMLLR transforms for unseen speakers or to perform DNN speaker-dependent parameter update."}, {"heading": "V. EXPERIMENTAL SETUPS", "text": "We experimentally investigated LHUC and SAT-LHUC using four different corpora: the TED talks corpus [15] following the IWSLT evaluation protocol (www.iwslt.org); the Switchboard corpus of conversational telephone speech [17] (ldc.upenn.edu); the AMI meetings corpus [16], [58] (corpus. amiproject.org); and the Aurora4 corpus of read speech with artificially corrupted acoustic environments [18] (catalog.elra. info). Unless explicitly stated otherwise, the models share similar structure across the tasks \u2013 DNNs with 6 hidden layers (2,048 units in each) using a sigmoid non-linearity. The output logistic regression layer models the distribution of context-dependent clustered tied states [5]. The features are presented in 11 (\u00b15) frame long context windows. All the adaptation experiments, unless explicitly stated otherwise, were performed unsupervised.\nBelow, we briefly describe each of the above corpora and its specific experimental configurations. The collective summary of adaptation-related statistics for each corpora are given in Table I. Note that we adapt to the headset or the side of a conversation, rather than the actual speaker (unless stated otherwise). As a result, the actual number of clusters (or estimated transforms) during training may differ from the number of physical speakers in the data.\nTED: We carried out experiments using a corpus of publicity available TED talks (www.ted.com) following the IWSLT ASR evaluation protocol [59] (iwslt.org). The training data consisted of 143 hours of speech (813 talks) and the systems follow our previously described recipe [9]. In this work however, compared to our previous works [9], [13], [43], our systems employ more accurate language models developed for our IWSLT\u20132014 systems [60]: in particular, the final reported results use a 4-gram language model estimated from 751 million words. The baseline TED acoustic models are trained on unadapted PLP features with first and second order time derivatives. We present results on four predefined IWSLT test sets: dev2010, tst2010, tst2011 and tst2013 containing 8, 11, 8 and 28 ten-minute talks respectively. We use tst2010 and/or tst2013 to perform more detailed analyses. A collective summary of results on all TED testsets is reported in Sec. VI-F.\nAMI: We follow the Kaldi GMM recipe described in [61] and use acoustics from either Individual Headset Microphone (IHM) or Single Distant Microphone (SDM). On this corpus, in addition to cepstral features, we also train a separate set of models using 40 mel-filter-bank (FBANK) features for which fMLLR transforms cannot be easily obtained, so\nLHUC offers an interesting adaptation alternative. For AMI, we also evaluate the effectiveness of LHUC and SAT-LHUC on AMs that utilise convolutional layers [8], [62], [63], where the CNN networks were trained as described in [64] but with 300 convolutional filters. We decode with a pruned 3- gram language model estimated from 800k words of AMI training transcripts interpolated with an LM trained on Fisher conversational telephone speech transcripts (1M words).\nSwitchboard: We use the Kaldi GMM recipe [65], [66], using Switchboard\u20131 Release 2 (LDC97S62). Our baseline unadapted acoustic models were trained on LDA/MLLT features. The results are reported on the full Hub5 00 set (LDC2002S09) to which we will refer as eval2000. The eval2000 contains two types of data, Switchboard (SWBD) \u2013 which is better matched to the training data \u2013 and CallHome English (CHE). Our reported results use 3-gram LMs estimated from Switchboard and Fisher data.\nAurora4: The Aurora 4 task is a small scale, medium vocabulary noise and channel ASR robustness task based on the Wall Street Journal corpus [18]. We train our ASR models using the multi-condition training set. One half of the training utterances were recorded using a primary Sennheiser microphone, and the other half was collected using one of 18 other secondary microphones. The multi-condition set contains noisy utterances corrupted with one of six different noise types (airport, babble, car, restaurant, street traffic and train station) at 10-20 dB SNR. The standard Aurora 4 test set (eval92) consists of 330 utterances, which are used in 14 test conditions (4620 utterances in total). The same six noise types used during training are used to create noisy test utterances with SNRs ranging from 5-15dB SNR, resulting in a total of 14 test sets. These test sets are commonly grouped into 4 subsets \u2013 clean (group A, 1 test case), noisy (group B, 6 test cases), clean with channel distortion (group C, 1 test case) and noisy with channel distortion (group D, 6 test cases). We decode with the standard task\u2019s bigram LM."}, {"heading": "VI. RESULTS", "text": ""}, {"heading": "A. LHUC hyperparameters", "text": "Our initial study concerned the hyper-parameters used with LHUC adaptation. First, we used the TED talks to investigate how the word error rate (WER) is affected by adapting different layers in the model using LHUC transforms. The results, graphed in Fig. 5 (a), indicated that adapting only the bottom layer brings the largest drop in WER; however, adapting more layers further improves the accuracy for both LHUC and SAT-LHUC approaches (adapting the other way round \u2013 starting from the top layer \u2013 is much less effective [13]). Since obtaining the gradients for the r parameters at each layer is inexpensive compared to the overall backpropagation, and we want to adapt at least the bottom layer, we apply LHUC to each layer for the rest of this work.\nFig. 5 (b) shows WERs for the number of adaptation iterations. The results indicate that one sweep over the adaptation data (in this case tst2010) is sufficient and, more importantly, the model does not overfit when adapting with more iterations (despite the adaptation objective consistently\nimproving \u2013 Fig. 5 (c)). This suggests that it is not necessary to carefully regularise the model \u2013 for example, by KullbackLeibler divergence training [38] which is usually required when adapting the weights of one or more layers in a network.\nFinally, we explored how the form of the LHUC reparametrisation function \u03be affects the WER and frame error rate (FER) (Fig. 5 (c) and Table II). For test-only adaptation only a small WER difference (0.1% absolute) is observed, regardless of the large difference in frame accuracies. This supports our previous observation that LHUC is robust against over-fitting. For SAT-LHUC training, a less constrained parametrisation was found to give better WERs for the SI model. Based on our control experiments, during SAT-LHUC training, setting \u03be to be the identity function (linear r) gave similar results to \u03be(r) = max(0, r) and \u03be(r) = exp(r) and all were better than re-parametrising with \u03be(r) = 2/(1 + exp(\u2212r)). This is expected as for full training the last approach constrains the range of back-propagated gradients. From now on, if not stated otherwise, we will use \u03be(r) = exp(r) in the remainder of this paper.\nWe adapt our all models with the learning rate set to 0.8 (regardless of \u03be(\u00b7)) and the basic training of both the SI and the SAT-LHUC models was performed with the initial learning rate set to 0.08 and was later adjusted according to the newbob learning scheme [67]."}, {"heading": "B. SAT-LHUC", "text": "As described in section IV, SAT-LHUC training aims to regularise the hidden unit feature receptors so that they capture not just the average characteristics of training data, but also specific features of the different distributions the data was drawn from (for example, different training speakers). As a result, the model can be better tailored to unseen speakers by putting more importance to those units that were useful for training speakers with similar characteristics.\nPrior to SAT-LHUC training we need to decide on how and which data should be used to estimate speaker-dependent and speaker-independent transforms. In this work we train SAT-LHUC models with frame-level [14], segment-level and speaker-level clusters. For speaker- and segment-level transforms we decide which speakers or segments are going to be treated as SI or SD prior to training. For the frame-level SAT-LHUC approach, the SI/SD decisions are made separately for each data-point during training. In either scenario we ensure that the overall SD/SI ratio determined by \u03b3 parameter is satisfied. The WER results for each of these three approaches (\u03b3 = 0.5) are reported in Table III. Speaker-level SAT-LHUC training provides the highest WERs for both SI and SD decodes. Segment-level and frame-level SAT-LHUC training result in similar WERs for SI decodes, with a small advantage (0.1% abs.) for the frame-level approach after adaptation.\nFig. 6 gives more insight on how the ratio of SI and SD data (determined by \u03b3) affects the WER of the first-pass and adapted systems on TED tst20131. The SI/SD split mainly affects the first pass accuracies with a substantial increase in SI WER when less than 30% of the data is used to estimate the SI LHUC transforms. However, once adapted, all variants obtained lower WERs compared to the baseline SI and LHUC adapted model. For instance, when \u03b3 = 0.5 the SAT-LHUC systems operating in SI mode obtained similar accuracies to the baseline SI model (22%WER); however, the adapted SAT-LHUC model gave around 1% absolute (6% relative) decrease in WER compared with the SI baseline test-only adapted LHUC model. The adaptation results for speaker-level SAT-LHUC training were worse by around 0.4% absolute compared to segment- or frame-level SAT-LHUC training. However, the difference, as shown experimentally in [14], is mostly due to poorer quality adaptation targets resulting from the corresponding first pass SAT-LHUC systems rather than the differences in learned representations. Managing a good trade-off between SI and SD ratios for SAT-LHUC is nevertheless an important aspect to take into account, and in our experience using around 50\u201360% of data for the SI transform is a good task-independent setting. If different models for SI and SD decodes are acceptable, then further small gains in accuracy are observed [14].\n1These results are compatible with further SAT-LHUC results using this tst2013 set in [14]\nWe report the baseline LHUC and SAT-LHUC comparisons on TED and AMI data in Tables IV and V, respectively (further results, including a comparison to fMLLR transforms and on Switchboard data are in the next sections). On TED (Table IV), SAT-LHUC models operating in SI mode (\u03b3 = 0.6) have comparable WERs to SI models; however, adaptation resulted in a WER reduction of 0.3\u20131.1% absolute (2\u20136% relative) compared to test-only adaptation of the SI models. Similar results were observed on the AMI data (Table V) where for both DNN and CNN models trained on FBANK features LHUC adaptation decreased the WER by 2% absolute (7% relative) and SAT-LHUC training improved this result by 4% relative for DNN models. As expected, the SAT-LHUC gain for CNNs was smaller when compared to DNN models, since the CNN layer can learn different patterns for different speakers which may be selected through the max-pooling operator at run-time."}, {"heading": "C. Sequence model adaptation", "text": "Model-based adaptation of sequence-trained DNNs (SEDNN) is more challenging compared to adapting networks\ntrained using cross-entropy: a mismatched adaptation objective (here cross-entropy) can easily erase sequence information from the weight matrices due to the well-known effect of catastrophic forgetting [68] in neural networks. Indeed Huang and Gong [39] report no gain from adapting SE-DNN models with a KL divergence regularised [38] cross-entropy adaptation objective and supervised adaptation targets. In those experiments, all weights in the model were updated and one needs to perform KL divergence regularised sequence level adaptation to further improve on top of SE-DNN. It remains to be answered if one can get similar improvements using SE-DNN adaptation and first-pass transcripts.\nIn this work we adapt state-level minimum Bayes risk (sMBR) [69], [70] sequence-trained models with LHUC approach and report results on TED tst2011 and tst2013 in Table VI. We kept all the LHUC adaptation hyper-parameters the same as for CE models and obtained around 2% absolute (10.9% relative) WER reductions on tst2013 for both SI and fMLLR SAT adapted SE-DNN systems. Interestingly, the obtained adaptation gain was similar to the cross-entropy models and LHUC adaptation did not seem to disrupt the learned model\u2019s sequence representation.\nWe compared also our adaptation results to the most accurate system of the IWSLT\u20132013 TED transcription evaluation, which performed both feature- and model-space speaker adaptation [71]. For model-space adaptation that system used a method which adapts DNNs with a speaker-dependent layer [41]. The results are reported in Table VII where in the first block one can see a standard sequence-trained featurespace adapted system build from TED and 150 hours of outof-domain data scoring 15.7% WER, similar to the WER of\nour TED system (15.4%), which also for IWSLT utilised 100 hours of out-of-domain AMI data. The 0.3% difference could be explained by characteristics of the out-of-domain data used (tst2013 is characterised by a large proportion of nonnative speakers which is also typical for AMI data, hence benefits more our baseline systems). When comparing both adaptation approaches operating in an unsupervised manner one can see that LHUC gives much bigger improvements in WER compared to speaker-dependent layer, 2.1% vs. 0.6% absolute (13.6% vs. 4.3% relative) on tst2013. This allows our single-model system to match a considerably more sophisticated post-processing pipeline [71], as outlined in Table VII. For less mismatched data (tst2011) adaptation is less important and our system has a WER 0.8% absolute higher compared with the more sophisticated system.\nFrom these experiments we conclude that LHUC is an effective way to adapt sequence models in unsupervised manner using a cross-entropy objective function, without risk of removing learned sequence information."}, {"heading": "D. Other aspects of adaptation", "text": "Amount of adaptation data: Fig 7 shows the effect of the amount of adaptation data on WER for LHUC and SAT-LHUC adapted models. As little as 10s of unsupervised adaptation data is already able to substantially decrease WERs (by 0.5\u2013 0.8% absolute). The improvement for SAT-LHUC adaptation compared with LHUC is considerably larger \u2013 roughly by a factor of two up to 30s adaptation data. As the duration of adaptation data increases the difference gets smaller; however SAT-LHUC results in consistently lower WERs than LHUC in all cases (including full two pass adaptation).\nWe also investigated supervised (oracle) adaptation by aligning the acoustics with the reference transcriptions (dashed lines). Given supervised adaptation targets, LHUC and SAT-LHUC further substantially decrease WERs, with SAT-LHUC giving a consistent advantage over LHUC.\nQuality of adaptation targets: Since our approach relies on a first-pass decoding, we investigated the extent to which LHUC is sensitive to the quality of the adaptation targets. In this experiment we explored the differences resulting from different language models, and assumed that the first pass\nadaptation data was generated by either an SI or a SAT-LHUC model operating in SI mode. The main results are shown in Fig 8 where the solid lines show WERs obtained with a pruned 3-gram LM and different types of adaptation targets resulting from re-scoring the adaptation data with stronger LMs. One can see there is not much difference unless the adaptation data was re-scored with the largest 4-gram LM. This improvement diminishes in the final adapted system after re-scoring. This suggests that the technique is not very sensitive to the quality of adaptation targets. This trend holds regardless of the amount of data used for adaptation (ranging from 10s to several minutes per speaker). In related work [32] LHUC was employed using alignments obtained from an SIGMM system with a 8.1% absolute higher WER than the corresponding SI DNN, and substantial gains were obtained over the unadapted SI DNN baseline \u2013 although the WER reduction was considerably smaller (1% absolute) compared to adaptation with alignments obtained with the corresponding SI DNN.\nQuality of data: We also investigated how the quality of the acoustic data itself affects the adaptation accuracies, keeping the other ASR components fixed. We performed an experiment on the AMI corpus using speech captured by individual headset microphones (IHM) and a single distant tabletop microphone (SDM). In case of IHM we adapt to the headset; in this experiment we assume we have speaker labels for the SDM data2. The results are reported in Table VIII: LHUC adaptation improves the accuracy in both experiments, although the gain for the SDM condition is smaller; however, the SDM system is characterised by twice as large WERs.\nOne-shot adaptation: By one-shot adaptation we mean the scenario in which LHUC transforms were estimated once for a held-out speaker and then used many times in a single pass system for this speaker. We performed those experiments on AMI IHM data, and report results on dev and eval which\n2In a real scenario for SDM data one would have to perform speaker diarisation in order to obtain speaker labels.\ncontain 21 and 16 unique speakers taking part in 18 and 16 different meetings, respectively. Each speaker participates in multiple meetings: to some degree, adapting to a speaker in one meeting, then applying the adaptation transform to the same speaker in the other meetings simulates a real-life condition where it is possible to assume the speaker identity without necessity of performing speaker diarisation (e.g. personal devices). The results of this experiment (Table IX) indicate that LHUC retains the accuracies of two-pass systems by providing almost identical results when comparing LHUC estimated in a full two-pass system and when the transforms are re-used in the LHUC.one-shot experiment."}, {"heading": "E. Complementarity to feature normalisation", "text": "Feature-space adaptation using fMLLR is probably the most reliable current form of speaker adaptation, so it is of great interest to explore how complementary the proposed approaches are to SAT training with fMLLR transforms.3\nWe compared LHUC and SAT-LHUC to SAT-fMLLR training using TED tst2010 (Fig 9, red curves). We also compared both techniques, including a comparison in terms of the amount of data used to estimate each type of transform. fMLLR transforms estimated on 10s of unsupervised data result in an increase in WER compared with the SI-trained\n3Due to space constraints we do not make an explicit comparisons to other techniques such as auxiliary i-vector features or speaker-codes; however, the literature suggest that the use of i-vectors give similar [29] results when compared to fMLLR trained models. Related recent studies also show LHUC is at least as good as the standard use of i-vector features [32], [72].\nbaseline (16.1% vs. 15.0%). When combined with LHUC or SAT-LHUC some of this deterioration was recovered (similar results using LHUC alone were reported in Fig 7). For more adaptation data (30s or more) fMLLR improved the accuracies by around 1\u20132% absolute and combination with LHUC (or SAT-LHUC) resulted in an additional 1% reduction in WER (see also Table X in the next section for further results).\nWe also investigated (in a rather unrealistic experiment) how much mismatch in feature space one can normalise in model space with LHUC. To do so, we used a SAT-fMLLR trained model with unadapted PLP features which gave a large increase in WER (26% vs 15%). Then, using unsupervised adaptation targets obtained from the feature-mismatched decoding both LHUC and SAT-LHUC were applied. The results (also presented in Fig. 9) indicate that a very large portion of the WER increase can be effectively compensated in model space \u2013 more than 8% absolute. As found before, test-only reparametrisation functions (exp(r) vs. 2/(1 + exp(\u2212r))) have negligible impact on the adaptation results, and SAT-LHUC again provides better results."}, {"heading": "F. Adaptation Summary", "text": "In this section we summarise our results, applying LHUC and SAT-LHUC to TED, AMI, and Switchboard. Table X contains results for four IWSLT test sets (dev2010, tst2010, tst2011, and tst2013): in most scenarios SAT-LHUC results in a lower WER than LHUC and both techniques are complementary with SAT-fMLLR training.\nSimilar conclusions can be drawn from experiments on AMI (Table XI) where LHUC and SAT-LHUC were found to effectively adapt DNN and CNN models trained on FBANK features. SAT-LHUC trained DNN models gave the same final results as the more complicated SAT-fMLLR+LHUC system.\nOn Switchboard, in contrast to other corpora, we observed that test-only LHUC does not match the WERs obtained from SAT-fMLLR models (Table XII). The SI system has a WER o 21.7% compared with 20.7% for the test-only LHUC and 20.2% for the SAT-fMLLR system. The improvement obtained using test-only LHUC is comparable to that obtained with other test-only adaptation techniques, e.g. feature-space discriminative linear regression (fDLR) [4], but neither of these matches SAT trained feature transform models. This could be due to the fact Switchboard data is narrow-band and as such contains less information for discrimination between speakers [73], especially when estimating relevant statistics from small amounts of unsupervised adaptation data. Another potential reason could be related to the fact that the Switchboard part of eval2000 is characterised by a large overlap between training and test speakers \u2013 36 out of 40 test speakers are observed in training [74], which limits the need for adaptation, but also enables models to learn much more accurate speaker-characteristics during supervised speaker adaptive training.\nAdaptation using SAT-LHUC (20.3% WER) almost matches SAT-fMLLR (20.2%). We also observe that LHUC performs relatively better under more mismatched conditions (the Callhome (CHE) subset of eval2000), similar to what we observed on TED.\nFinally, in Fig 10 we show the WERs obtained for 200 speakers across the TED, AMI, and SWBD test sets. We observe that for 89% of speakers LHUC and SAT-LHUC adaptation reduced the WER, and that SAT-LHUC gives a consistent reduction over LHUC."}, {"heading": "VII. LHUC FOR FACTORISATION", "text": "We have applied LHUC to adapt to both the speaker and the acoustic environment. If multi-condition data is available for a speaker, then it is possible to define a set of joint speakerenvironment LHUC transforms. Alternatively, we can estimate two set of transforms \u2013 for speaker rS and for environment rE \u2013 and then linearly interpolate them to derive a combined transform r\u0302SE as follows:\n\u03be ( r\u0302lSE ) = \u03b1\u03be ( rlS ) + (1\u2212 \u03b1)\u03be ( rlE )\n(10)\nThe idea is similar to LHUC applied to channel normalisation between distant and close talking scenarios [75], except we use two independently estimated transforms.\nWe adapted baseline multi-condition trained DNN models [76] to the speaker (rS) and the environment (rE). The rS transforms were estimated only on clean speech; similarly the environment transforms were estimated for each scenario (one set of rE per scenario) using multiple speakers (hence, we have 7 different environmental transforms). To avoid learning joint speaker-environment transforms the target speaker\u2019s data was removed from environment adaptation material (e.g. when estimating transforms for the restaurant environment, we use all restaurant data except the one for the target speaker).\nThe results (Table XIII) show that both standalone speaker or environment adaptation LHUC adaptation improve over an unadapted system (13.1%(S) and 13.3%(E) vs. 13.9%) but, as expected, a single transform estimated jointly on the target speaker and environment has a lower WER (12.4%). However, when interpolated with \u03b1 = 0.7 the result of the factorised model improves to 12.7% WER, although still higher that joint estimation. However, adaptation data for joint speakerenvironment adaptation is not available in many scenarios, and the factorised adaptation based on interpolation is more flexible.\nWe also train more competitive models following Rennie et al [77]: Maxout [78] CNN models were trained using annealed dropout. However, in contrast to [77], in this work we are\nuse alignments obtained by aligning a corresponding multicondition model as ground-truth labels, rather than replicating clean alignments to multi-condition data: this is likely to explain differences in the reported baselines (10.9% compared with 10.5% in [77]). The results for the joint optimisation are reported in Table XIV where one can notice large improvements with unsupervised LHUC adaptation.\nFinally, we visualise the top hidden layer activations of the annealed dropout Maxout CNN using stochastic neighbourhood embedding (tSNE) [79] for one utterance recorded under clean and noisy (restaurant) conditions (Fig. 11)."}, {"heading": "VIII. CONCLUSIONS", "text": "We have presented the LHUC approach to unsupervised adaptation of neural network acoustic models in both testonly (LHUC) and SAT (SAT-LHUC) frameworks, evaluating them using four standard speech recognition corpora: TED talks as used in the IWSLT evaluations, AMI, Switchboard, and Aurora4. Our experimental results indicate that both LHUC and SAT-LHUC can provide significant improvements in WER (5\u201323% relative depending on test set and task). LHUC adaptation works well unsupervised and with small amounts of data (as little as 10s), is complementary to feature space normalisation transforms such as SAT-fMLLR, and can be used for unsupervised adaptation of sequence-trained DNN acoustic models using a cross-entropy adaptation objective\nfunction. Furthermore we have demonstrated that it can be applied in a factorised way, estimating and interpolating separate transforms for adaptation to the acoustic environment and speaker."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G Hinton", "L Deng", "D Yu", "GE Dahl", "A Mohamed", "N Jaitly", "A Senior", "V Vanhoucke", "P Nguyen", "TN Sainath", "B Kingsbury"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, Nov 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Connectionist Speech Recognition: A Hybrid Approach", "author": ["H Bourlard", "N Morgan"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Connectionist probability estimators in HMM speech recognition", "author": ["S Renals", "N Morgan", "H Bourlard", "M Cohen", "H Franco"], "venue": "IEEE Trans Speech and Audio Processing, vol. 2, pp. 161\u2013174, 1994.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1994}, {"title": "Feature engineering in context-dependent deep neural networks for conversational speech transcription", "author": ["F Seide", "X Chen", "D Yu"], "venue": "Proc. IEEE ASRU, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["GE Dahl", "D Yu", "L Deng", "A Acero"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 1, pp. 30\u201342, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Tandem connectionist feature extraction for conventional HMM systems", "author": ["H Hermansky", "DPW Ellis", "S Sharma"], "venue": "Proc. IEEE ICASSP, 2000, pp. 1635\u20131638.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Probabilistic and bottleneck features for LVCSR of meetings", "author": ["F Grezl", "M Karafiat", "S Kontar", "J Cernocky"], "venue": "Proc. IEEE ICASSP, 2007, pp. IV\u2013757\u2013IV\u2013760.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Deep convolutional neural networks for LVCSR", "author": ["TN Sainath", "A Mohamed", "B Kingsbury", "B Ramabhadran"], "venue": "Proc. IEEE ICASSP, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Revisiting hybrid and GMM- HMM system combination techniques", "author": ["P Swietojanski", "A Ghoshal", "S Renals"], "venue": "Proc. IEEE ICASSP, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Cambridge University transcription systems for the Multi-Genre Broadcast Challenge", "author": ["PC Woodland", "X Liu", "Y Qian", "C Zhang", "MJF Gales", "P Karanasou", "P Lanchantin", "L Wang"], "venue": "Proc. IEEE ASRU, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Speaker adaptation for continuous density HMMs: A review", "author": ["PC Woodland"], "venue": "Proceedings of the ISCA workshop on adaptation methods for speech recognition, 2001, pp. 11\u201319.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Rapid and effective speaker adaptation of convolutional neural network based models for speech recognition", "author": ["O Abdel-Hamid", "H Jiang"], "venue": "Proc. Interspeech, pp. 1248\u20131252.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1252}, {"title": "Learning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models", "author": ["P Swietojanski", "S Renals"], "venue": "Proc. IEEE SLT, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "SAT-LHUC: Speaker adaptive training for learning hidden unit contributions", "author": ["P Swietojanski", "S Renals"], "venue": "Proc. IEEE ICASSP, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Wit3: Web inventory of transcribed and translated talks", "author": ["M Cettolo", "C Girardi", "M Federico"], "venue": "Proc. EAMT, 2012, pp. 261\u2013268.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Unleashing the killer corpus: Experiences in creating the multi-everything AMI meeting corpus", "author": ["J Carletta"], "venue": "Language Resources and Evaluation, vol. 41, no. 2, pp. 181\u2013190, 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "SWITCHBOARD: Telephone speech corpus for research and development", "author": ["JJ Godfrey", "EC Holliman", "J McDaniel"], "venue": "Proc. IEEE ICASSP. IEEE, 1992, pp. 517\u2013520.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1992}, {"title": "Performance analysis of the Aurora large vocabulary baseline system", "author": ["N Parihar", "J Picone", "D Pearce", "HG Hirsch"], "venue": "Proc. EUSIPCO, 2004.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Maximum likelihood linear transformations for HMMbased speech recognition", "author": ["MJF Gales"], "venue": "Computer Speech and Language, vol. 12, pp. 75\u201398, April 1998.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Deep belief networks using discriminative features for phone recognition", "author": ["A Mohamed", "TN Sainath", "G Dahl", "B Ramabhadran", "GE Hinton", "MA Picheny"], "venue": "Proc. IEEE ICASSP, 2011, pp. 5060\u20135063.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Transcribing meetings with the AMIDA systems", "author": ["T Hain", "L Burget", "J Dines", "PN Garner", "F Gr\u00e9zl", "A El Hannani", "M Karaf\u0131\u0301at", "M Lincoln", "V Wan"], "venue": "IEEE Trans Audio, Speech and Language Processing, vol. 20, pp. 486\u2013498, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Auto-encoder bottleneck features using deep belief networks", "author": ["TN Sainath", "B Kingsbury", "B Ramabhadran"], "venue": "Proc. IEEE ICASSP, 2012, pp. 4153\u20134156.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-level adaptive networks in tandem and hybrid ASR systems", "author": ["P Bell", "P Swietojanski", "S Renals"], "venue": "Proc. IEEE ICASSP, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Speaker adaptation for hybrid HMM\u2013ANN continuous speech recognition system", "author": ["J Neto", "L Almeida", "M Hochberg", "C Martins", "L Nunes", "S Renals", "T Robinson"], "venue": "Proc. Eurospeech, 1995, pp. 2171\u20132174.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1995}, {"title": "Connectionist speaker normalization and adaptation", "author": ["V Abrash", "H Franco", "A Sankar", "M Cohen"], "venue": "Proc. Eurospeech, 1995, pp. 2183\u2013 2186.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1995}, {"title": "Adaptation of context-dependent deep neural networks for automatic speech recognition", "author": ["K Yao", "D Yu", "F Seide", "H Su", "L Deng", "Y Gong"], "venue": "Proc. IEEE SLT, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Front end factor analysis for speaker verification", "author": ["N Dehak", "PJ Kenny", "R Dehak", "P Dumouchel", "P Ouellet"], "venue": "IEEE Trans Audio, Speech and Language Processing, vol. 19, pp. 788\u2013798, 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "iVectorbased discriminative adaptation for automatic speech recognition", "author": ["M Karafiat", "L Burget", "P Matejka", "O Glembek", "J Cernozky"], "venue": "Proc. IEEE ASRU, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Speaker adaptation of neural network acoustic models using i-vectors", "author": ["G Saon", "H Soltau", "D Nahamoo", "M Picheny"], "venue": "Proc. IEEE ASRU, 2013, pp. 55\u201359.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "I-vector based speaker adaptation of deep neural networks for french broadcast audio transcription", "author": ["V Gupta", "P Kenny", "P Ouellet", "T Stafylakis"], "venue": "Proc. IEEE ICASSP, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptation of deep neural network acoustic models using factorised i-vectors", "author": ["P Karanasou", "Y Wang", "MJF Gales", "PC Woodland"], "venue": "Proc. Interspeech, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker adaptive training of deep neural network acoustic models using i-vectors", "author": ["Y Miao", "H Zhang", "F Metze"], "venue": "Audio, Speech, and Language Processing, IEEE/ACM Transactions on, vol. 23, no. 11, pp. 1938\u20131949, Nov 2015.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1938}, {"title": "Using neural network front-ends on far field multiple microphones based speech recognition", "author": ["Y Liu", "P Zhang", "T Hain"], "venue": "Proc. IEEE ICASSP, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Recnorm: Simultaneous normalisation and classification applied to speech recognition", "author": ["JS Bridle", "S Cox"], "venue": "Advances in Neural Information Processing Systems 3, 1990, pp. 234\u2013240.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1990}, {"title": "Fast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code", "author": ["O Abdel-Hamid", "H Jiang"], "venue": "Proc. IEEE ICASSP, 2013, pp. 4277\u20134280.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast adaptation of deep neural network based on discriminant codes for speech recognition", "author": ["S Xue", "O Abdel-Hamid", "J Hui", "L Dai", "Q Liu"], "venue": "Audio, Speech, and Language Processing, IEEE/ACM Transactions on, vol. 22, no. 12, pp. 1713\u20131725, Dec 2014.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker adaptation of context dependent deep neural networks", "author": ["H Liao"], "venue": "In Proc. ICASSP. 2013, pp. 7947\u20137951, IEEE.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "KL-divergence regularized deep neural network adaptation for improved large vocabulary speech recognition", "author": ["D Yu", "K Yao", "H Su", "G Li", "F Seide"], "venue": "Proc. IEEE ICASSP, 2013, pp. 7893\u20137897.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Regularized sequence-level deep neural network model adaptation", "author": ["Y Huang", "Y Gong"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Singular value decomposition based low-footprint speaker adaptation and personalization for deep neural network", "author": ["J Xue", "J Li", "D Yu", "M Seltzer", "Y Gong"], "venue": "Proc. IEEE ICASSP, 2014.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker adaptive training using deep neural networks", "author": ["T Ochiai", "S Matsuda", "X Lu", "C Hori", "S Katagiri"], "venue": "Proc. IEEE ICASSP, 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Investigating online low-footprint speaker adaptation using generalized linear regression and click-through data", "author": ["Y Zhao", "J Li", "J Xue", "Y Gong"], "venue": "Proc. IEEE ICASSP, 2015.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Differentiable pooling for unsupervised speaker adaptation", "author": ["P Swietojanski", "S Renals"], "venue": "Proc. IEEE ICASSP, 2015.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "Hermitian polynomial for speaker adaptation of connectionist speech recognition systems", "author": ["SM Siniscalchi", "J Li", "CH Lee"], "venue": "IEEE Trans Audio, Speech, and Language Processing, vol. 21, pp. 2152\u20132161, 2013.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "Maximum a posteriori adaptation of network parameters in deep models", "author": ["Z Huang", "S M Siniscalchi", "I-F Chen", "J Wu", "C-H Lee"], "venue": "arXiv preprint arXiv:1503.02108, 2015.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "Rapid adaptation for deep neural networks through multi-task learning", "author": ["Z Huang", "J Li", "S M Siniscalchi", "I-F Chen", "J Wu", "C-H Lee"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Structured output layer with auxiliary targets for context-dependent acoustic modelling", "author": ["P Swietojanski", "P Bell", "S Renals"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Speaker adaptation of deep neural networks using a hierarchy of output layers", "author": ["R Price", "K Iso", "K Shinoda"], "venue": "Proc. IEEE SLT, 2014.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K Hornik", "M Stinchcombe", "H White"], "venue": "Neural networks, vol. 2, no. 5, pp. 359\u2013366, 1989.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1989}, {"title": "Approximation capabilities of multilayer feedforward networks", "author": ["K Hornik"], "venue": "Neural Networks, vol. 4, no. 2, pp. 251 \u2013 257, 1991.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1991}, {"title": "Universal approximation bounds for superpositions of a sigmoidal function", "author": ["AR Barron"], "venue": "IEEE Transactions on Information Theory, vol. 39, no. 3, pp. 930\u2013945, 1993.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1993}, {"title": "Parameterised Sigmoid and ReLU Hidden Activation Functions for DNN Acoustic Modelling", "author": ["C Zhang", "PC Woodland"], "venue": "Proc. Interspeech, 2015.  SUBMITTED TO IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING  13", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}, {"title": "Networks with trainable amplitude of activation functions", "author": ["E Trentin"], "venue": "Neural Networks, vol. 14, pp. 471\u2013493, 2001.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2001}, {"title": "Multi-basis adaptive neural network for rapid adaptation in speech recognition", "author": ["C Wu", "M Gales"], "venue": "Proc. IEEE ICASSP, 2015.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2015}, {"title": "Cluster adaptive training for deep neural network", "author": ["T Tan", "Y Qian", "M Yin", "Y Zhuang", "K Yu"], "venue": "Proc. IEEE ICASSP, 2015.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2015}, {"title": "Context adaptive deep neural networks for fast acoustic model adaptation", "author": ["M Delcroix", "K Kinoshita", "T Hori", "T Nakatani"], "venue": "Proc. IEEE ICASSP, 2015.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2015}, {"title": "A compact model for speaker-adaptive training", "author": ["T Anastasakos", "J McDonough", "R Schwartz", "J Makhoul"], "venue": "Proc. ICSLP, 1996, pp. 1137\u2013 1140.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1996}, {"title": "Recognition and understanding of meetings: The AMI and AMIDA projects", "author": ["S Renals", "T Hain", "H Bourlard"], "venue": "Proc. of the IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU\u201907, Kyoto, 12 2007, IDIAP-RR 07-46.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2007}, {"title": "Overview of the IWSLT 2012 evaluation campaign", "author": ["M Federico", "M Cettolo", "L Bentivogli", "M Paul", "S St\u00fcker"], "venue": "Proc. IWSLT, 2012.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2012}, {"title": "The UEDIN system for the IWSLT 2014 evaluation", "author": ["P Bell", "P Swietojanski", "J Driesen", "M Sinclair", "F McInnes", "S Renals"], "venue": "Proc. IWSLT, 2014.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2014}, {"title": "Hybrid acoustic models for distant and multichannel large vocabulary speech recognition", "author": ["P Swietojanski", "A Ghoshal", "S Renals"], "venue": "Proc. IEEE ASRU, 2013.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "K. M\u00fcller"], "venue": "Neural Networks: Tricks of the Trade, chapter 2. Springer, 1998.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 1998}, {"title": "Applying convolutional neural networks concepts to hybrid NN\u2013HMM model for speech recognition", "author": ["O Abdel-Hamid", "A-R Mohamed", "J Hui", "G Penn"], "venue": "Proc. IEEE ICASSP, 2012, pp. 4277\u20134280.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2012}, {"title": "Convolutional neural networks for distant speech recognition", "author": ["P Swietojanski", "A Ghoshal", "S Renals"], "venue": "Signal Processing Letters, IEEE, vol. 21, no. 9, pp. 1120\u20131124, Sept. 2014.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence-discriminative training of deep neural networks", "author": ["K Vesely", "A Ghoshal", "L Burget", "D Povey"], "venue": "Proc. Interspeech, Lyon, France, August 2013.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2013}, {"title": "The Kaldi speech recognition toolkit", "author": ["D Povey", "A Ghoshal", "G Boulianne", "L Burget", "O Glembek", "N Goel", "M Hannemann", "P Motl\u0131\u0301\u010dek", "Y Qian", "P Schwarz", "J Silovsk\u00fd", "G Stemmer", "K Vesel\u00fd"], "venue": "Proc. IEEE ASRU, December 2011.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2011}, {"title": "Connectionist probability estimation in the DECIPHER speech recognition system", "author": ["S Renals", "N Morgan", "M Cohen", "H Franco"], "venue": "Proc. IEEE ICASSP, 1992.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 1992}, {"title": "Catastrophic forgetting in connectionist networks: Causes, consequences and solutions", "author": ["RM French"], "venue": "Trends in Cognitive Sciences, vol. 3, pp. 128\u2013135, 1999.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 1999}, {"title": "A novel loss function for the overall risk criterion based discriminative training of HMM models", "author": ["J Kaiser", "B Horvat", "Z Kacic"], "venue": "Proc. ICSLP, 2000, pp. 887\u2013890.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2000}, {"title": "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling", "author": ["B Kingsbury"], "venue": "Proc. IEEE ICASSP, 2009, pp. 3761\u20133764.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2009}, {"title": "The NICT ASR system for IWSLT 2013", "author": ["C-L Huang", "PR Dixon", "S Matsuda", "Y Wu", "X Lu", "M Saiko", "C Hori"], "venue": "Proc. IWSLT, 2013.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2013}, {"title": "On combining i-vectors and discriminative adaptation methods for unsupervised speaker normalization in dnn acoustic models", "author": ["L Samarakoon", "K C Sim"], "venue": "Proc. IEEE ICASSP, 2016.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2016}, {"title": "Human vs machine spoofing detection on wideband and narrowband data", "author": ["M Wester", "Z Wu", "J Yamagishi"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2015}, {"title": "2000 NIST evaluation of conversational speech recognition over the telephone: English and Mandarin performance results", "author": ["J Fiscus", "WM Fisher", "AF Martin", "MA Przybocki", "DS Pallett"], "venue": "Proc. Speech Transcription Workshop. Citeseer, 2000.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2000}, {"title": "Towards utterancebased neural network adaptation in acoustic modeling", "author": ["I Himawan", "P Motlicek", "M Ferras", "S Madikeri"], "venue": "Proc. IEEE ASRU, 2015.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2015}, {"title": "An investigation of deep neural networks for noise robust speech recognition", "author": ["M Seltzer", "D Yu", "Y Wang"], "venue": "Proc. IEEE ICASSP, 2013.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2013}, {"title": "Annealed dropout training of deep networks", "author": ["SJ Rennie", "V Goel", "S Thomas"], "venue": "Proc. IEEE SLT, 2014.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2014}, {"title": "Maxout networks", "author": ["IJ Goodfellow", "D Warde-Farley", "M Mirza", "A Courville", "Y Bengio"], "venue": "arXiv:1302.4389, 2013.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2013}, {"title": "Visualizing high-dimensional data using t-sne", "author": ["LJP van der Maaten", "GE Hinton"], "venue": "Journal of Machine Learning Research, vol. 9: 25792605, Nov 2008.  PLACE PHOTO HERE Pawel Swietojanski Biography text here. PLACE PHOTO HERE Jinyu Li Biography text here. PLACE PHOTO HERE  Steve Renals Biography text here.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Hinton et al [1] report word error rate (WER) reductions between 10\u201332% across a wide variety of tasks, compared with discriminatively trained Gaussian mixture model (GMM) based systems.", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "These results use neural networks as part of both hybrid DNN/HMM (hidden Markov model) systems [1]\u2013[5] in which the neural network provides a scaled likelihood estimate to replace the GMM, and as tandem or bottleneck feature systems [6], [7] in which the neural network is used as a discriminative feature extractor for a GMM-based system.", "startOffset": 95, "endOffset": 98}, {"referenceID": 4, "context": "These results use neural networks as part of both hybrid DNN/HMM (hidden Markov model) systems [1]\u2013[5] in which the neural network provides a scaled likelihood estimate to replace the GMM, and as tandem or bottleneck feature systems [6], [7] in which the neural network is used as a discriminative feature extractor for a GMM-based system.", "startOffset": 99, "endOffset": 102}, {"referenceID": 5, "context": "These results use neural networks as part of both hybrid DNN/HMM (hidden Markov model) systems [1]\u2013[5] in which the neural network provides a scaled likelihood estimate to replace the GMM, and as tandem or bottleneck feature systems [6], [7] in which the neural network is used as a discriminative feature extractor for a GMM-based system.", "startOffset": 233, "endOffset": 236}, {"referenceID": 6, "context": "These results use neural networks as part of both hybrid DNN/HMM (hidden Markov model) systems [1]\u2013[5] in which the neural network provides a scaled likelihood estimate to replace the GMM, and as tandem or bottleneck feature systems [6], [7] in which the neural network is used as a discriminative feature extractor for a GMM-based system.", "startOffset": 238, "endOffset": 241}, {"referenceID": 7, "context": "For many tasks it has been observed that GMM-based systems (with tandem or bottleneck features) that have been adapted to the talker are more accurate than unadapted hybrid DNN/HMM systems [8]\u2013[10], indicating that the adaptation of DNN acoustic models is an important topic that merits investigation.", "startOffset": 189, "endOffset": 192}, {"referenceID": 9, "context": "For many tasks it has been observed that GMM-based systems (with tandem or bottleneck features) that have been adapted to the talker are more accurate than unadapted hybrid DNN/HMM systems [8]\u2013[10], indicating that the adaptation of DNN acoustic models is an important topic that merits investigation.", "startOffset": 193, "endOffset": 197}, {"referenceID": 10, "context": "Acoustic model adaptation [11] aims to normalise the mismatch between training and runtime data distributions that", "startOffset": 26, "endOffset": 30}, {"referenceID": 11, "context": "In this paper we investigate unsupervised modelbased adaptation of DNN acoustic models to speakers and to acoustic environments, using a recently introduced method called Learning Hidden Unit Contributions (LHUC) [12]\u2013 [14].", "startOffset": 213, "endOffset": 217}, {"referenceID": 13, "context": "In this paper we investigate unsupervised modelbased adaptation of DNN acoustic models to speakers and to acoustic environments, using a recently introduced method called Learning Hidden Unit Contributions (LHUC) [12]\u2013 [14].", "startOffset": 219, "endOffset": 223}, {"referenceID": 13, "context": "We present the LHUC approach both in the context of test-only adaptation, and an extension to speaker-adaptive training (SAT), referred to as SAT-LHUC [14].", "startOffset": 151, "endOffset": 155}, {"referenceID": 14, "context": "We present an extensive experimental analysis using four standard corpora: TED talks [15], AMI [16], Switchboard [17] and Aurora4 [18].", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": "We present an extensive experimental analysis using four standard corpora: TED talks [15], AMI [16], Switchboard [17] and Aurora4 [18].", "startOffset": 95, "endOffset": 99}, {"referenceID": 16, "context": "We present an extensive experimental analysis using four standard corpora: TED talks [15], AMI [16], Switchboard [17] and Aurora4 [18].", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "We present an extensive experimental analysis using four standard corpora: TED talks [15], AMI [16], Switchboard [17] and Aurora4 [18].", "startOffset": 130, "endOffset": 134}, {"referenceID": 18, "context": "VI-D); complementarity with feature-space adaptation techniques based on maximum likelihood linear regression [19] (Sec.", "startOffset": 110, "endOffset": 114}, {"referenceID": 18, "context": "The dominant technique for estimating feature space transforms is constrained (feature-space) MLLR, referred to as CMLLR or fMLLR [19].", "startOffset": 130, "endOffset": 134}, {"referenceID": 0, "context": "This technique has been shown to be effective in reducing WER across several different data sets, in both hybrid and tandem approaches [1], [4], [8], [9], [20]\u2013[23].", "startOffset": 135, "endOffset": 138}, {"referenceID": 3, "context": "This technique has been shown to be effective in reducing WER across several different data sets, in both hybrid and tandem approaches [1], [4], [8], [9], [20]\u2013[23].", "startOffset": 140, "endOffset": 143}, {"referenceID": 7, "context": "This technique has been shown to be effective in reducing WER across several different data sets, in both hybrid and tandem approaches [1], [4], [8], [9], [20]\u2013[23].", "startOffset": 145, "endOffset": 148}, {"referenceID": 8, "context": "This technique has been shown to be effective in reducing WER across several different data sets, in both hybrid and tandem approaches [1], [4], [8], [9], [20]\u2013[23].", "startOffset": 150, "endOffset": 153}, {"referenceID": 19, "context": "This technique has been shown to be effective in reducing WER across several different data sets, in both hybrid and tandem approaches [1], [4], [8], [9], [20]\u2013[23].", "startOffset": 155, "endOffset": 159}, {"referenceID": 22, "context": "This technique has been shown to be effective in reducing WER across several different data sets, in both hybrid and tandem approaches [1], [4], [8], [9], [20]\u2013[23].", "startOffset": 160, "endOffset": 164}, {"referenceID": 23, "context": "The linear input network (LIN) [24], [25] defines an additional speakerdependent layer between the input features and the first hidden layer, and thus has a similar effect to fMLLR.", "startOffset": 31, "endOffset": 35}, {"referenceID": 24, "context": "The linear input network (LIN) [24], [25] defines an additional speakerdependent layer between the input features and the first hidden layer, and thus has a similar effect to fMLLR.", "startOffset": 37, "endOffset": 41}, {"referenceID": 3, "context": "has been further developed to include the use of a tied variant of LIN in which each of the input frames is constrained to have the same linear transform \u2013 feature-space discriminative linear regression (fDLR) [4], [26].", "startOffset": 210, "endOffset": 213}, {"referenceID": 25, "context": "has been further developed to include the use of a tied variant of LIN in which each of the input frames is constrained to have the same linear transform \u2013 feature-space discriminative linear regression (fDLR) [4], [26].", "startOffset": 215, "endOffset": 219}, {"referenceID": 26, "context": "There has been considerable recent work exploring the use of i-vectors [27] for this purpose.", "startOffset": 71, "endOffset": 75}, {"referenceID": 27, "context": "I-vectors, which can be regarded as basis vectors which span a subspace of speaker variability, were first used for adaptation in a GMM framework by Karafiat et al [28].", "startOffset": 164, "endOffset": 168}, {"referenceID": 28, "context": "Saon et al [29] used i-vectors to augment the input features of DNN-based acoustic models, and showed that augmenting the input features with 100-dimensional i-vectors for each speaker resulted in a 10% relative reduction in WER on Switchboard (and a 6% reduction when the input features had been transformed using fMLLR).", "startOffset": 11, "endOffset": 15}, {"referenceID": 29, "context": "Gupta et al [30] obtained similar results, and Karanasou et al [31] presented an approach in which the ivectors were factorised into speaker and environment parts.", "startOffset": 12, "endOffset": 16}, {"referenceID": 30, "context": "Gupta et al [30] obtained similar results, and Karanasou et al [31] presented an approach in which the ivectors were factorised into speaker and environment parts.", "startOffset": 63, "endOffset": 67}, {"referenceID": 31, "context": "Miao et al [32] proposed to transform i-vectors using an auxiliary DNN which produced speaker-specific transforms of the original feature vectors, similar to fMLLR.", "startOffset": 11, "endOffset": 15}, {"referenceID": 32, "context": "Other examples of auxiliary features include the use of speaker-specific bottleneck features obtained from a speaker separation DNN used in a distant speech recognition task [33], the use of outof-domain tandem features [23], and speaker codes [34]\u2013[36] in which a specific set of units for each speaker is optimised.", "startOffset": 174, "endOffset": 178}, {"referenceID": 22, "context": "Other examples of auxiliary features include the use of speaker-specific bottleneck features obtained from a speaker separation DNN used in a distant speech recognition task [33], the use of outof-domain tandem features [23], and speaker codes [34]\u2013[36] in which a specific set of units for each speaker is optimised.", "startOffset": 220, "endOffset": 224}, {"referenceID": 33, "context": "Other examples of auxiliary features include the use of speaker-specific bottleneck features obtained from a speaker separation DNN used in a distant speech recognition task [33], the use of outof-domain tandem features [23], and speaker codes [34]\u2013[36] in which a specific set of units for each speaker is optimised.", "startOffset": 244, "endOffset": 248}, {"referenceID": 35, "context": "Other examples of auxiliary features include the use of speaker-specific bottleneck features obtained from a speaker separation DNN used in a distant speech recognition task [33], the use of outof-domain tandem features [23], and speaker codes [34]\u2013[36] in which a specific set of units for each speaker is optimised.", "startOffset": 249, "endOffset": 253}, {"referenceID": 36, "context": "Liao [37] investigated supervised and unsupervised adaptation of different weight subsets using a few minutes of adaptation data.", "startOffset": 5, "endOffset": 9}, {"referenceID": 37, "context": "Yu et al [38] have explored the use of regularisation for adapting the weights of a DNN, using the Kullback-Liebler (KL) divergence between the speaker-independent output distribution and the speakeradapted output distributions, resulting in a 3% relative improvement on Switchboard.", "startOffset": 9, "endOffset": 13}, {"referenceID": 38, "context": "This approach was also recently used to adapt all parameters of sequence-trained models [39].", "startOffset": 88, "endOffset": 92}, {"referenceID": 39, "context": "A variant of this approach reduces the number of speakerspecific parameters through a factorisation based on singular value decomposition [40].", "startOffset": 138, "endOffset": 142}, {"referenceID": 40, "context": "Ochiai et al [41] have also explored regularised speaker adaptive training with a speaker-dependent layer.", "startOffset": 13, "endOffset": 17}, {"referenceID": 25, "context": "Smaller subsets of the DNN weights may be modified, including output layer biases [26], the bias and slope of hidden units [42] or training the models with differentiable pooling operators [43], which are then adapted in SD fashion.", "startOffset": 82, "endOffset": 86}, {"referenceID": 41, "context": "Smaller subsets of the DNN weights may be modified, including output layer biases [26], the bias and slope of hidden units [42] or training the models with differentiable pooling operators [43], which are then adapted in SD fashion.", "startOffset": 123, "endOffset": 127}, {"referenceID": 42, "context": "Smaller subsets of the DNN weights may be modified, including output layer biases [26], the bias and slope of hidden units [42] or training the models with differentiable pooling operators [43], which are then adapted in SD fashion.", "startOffset": 189, "endOffset": 193}, {"referenceID": 43, "context": "Siniscalchi et al [44] also investigated the use of Hermite polynomial activation functions, whose parameters are estimated in a speaker adaptive fashion.", "startOffset": 18, "endOffset": 22}, {"referenceID": 44, "context": "One can also adapt the top layer in a Bayesian fashion resulting in a maximum a posteriori (MAP) approach [45], or address the sparsity of context-dependent tied-states when few adaptation data-points are available by using multi-task adaptation, using monophones to adapt the context-dependent output layer [46], [47].", "startOffset": 106, "endOffset": 110}, {"referenceID": 45, "context": "One can also adapt the top layer in a Bayesian fashion resulting in a maximum a posteriori (MAP) approach [45], or address the sparsity of context-dependent tied-states when few adaptation data-points are available by using multi-task adaptation, using monophones to adapt the context-dependent output layer [46], [47].", "startOffset": 308, "endOffset": 312}, {"referenceID": 46, "context": "One can also adapt the top layer in a Bayesian fashion resulting in a maximum a posteriori (MAP) approach [45], or address the sparsity of context-dependent tied-states when few adaptation data-points are available by using multi-task adaptation, using monophones to adapt the context-dependent output layer [46], [47].", "startOffset": 314, "endOffset": 318}, {"referenceID": 47, "context": "A similar approach, but using a hierarchical output layer (tied-states followed by monophones) rather than multi-task adaptation, has also been proposed [48].", "startOffset": 153, "endOffset": 157}, {"referenceID": 48, "context": "Under certain assumptions on the family of target functions f\u2217 (as well as on the model structure itself) the neural network can act as an universal approximator [49]\u2013[51].", "startOffset": 162, "endOffset": 166}, {"referenceID": 50, "context": "Under certain assumptions on the family of target functions f\u2217 (as well as on the model structure itself) the neural network can act as an universal approximator [49]\u2013[51].", "startOffset": 167, "endOffset": 171}, {"referenceID": 48, "context": "The properties also hold true when considering deeper (nested) models [49] (Corollaries 2.", "startOffset": 70, "endOffset": 74}, {"referenceID": 13, "context": "Image reproduced from [14].", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "The amplitude is modelled using a function \u03be : R \u2192 R \u2013 typically a sigmoid with range (0, 2) [13], but an identity function could be used [52].", "startOffset": 93, "endOffset": 97}, {"referenceID": 51, "context": "The amplitude is modelled using a function \u03be : R \u2192 R \u2013 typically a sigmoid with range (0, 2) [13], but an identity function could be used [52].", "startOffset": 138, "endOffset": 142}, {"referenceID": 52, "context": "The idea of directly learning hidden unit amplitudes was proposed in the context of an adaptive learning rate schedule by Trentin [53], and was later applied to supervised speaker adaptation by Abdel-Hamid and Jiang [12].", "startOffset": 130, "endOffset": 134}, {"referenceID": 11, "context": "The idea of directly learning hidden unit amplitudes was proposed in the context of an adaptive learning rate schedule by Trentin [53], and was later applied to supervised speaker adaptation by Abdel-Hamid and Jiang [12].", "startOffset": 216, "endOffset": 220}, {"referenceID": 12, "context": "The approach was extended to unsupervised adaptation, non-sigmoid nonlinearities, and large vocabulary speech recognition by Swietojanski and Renals [13].", "startOffset": 149, "endOffset": 153}, {"referenceID": 41, "context": "Other adaptive transfer function approaches for speaker adaptation have also been proposed [42], [44], as have \u201cbasis\u201d approaches [54]\u2013[56].", "startOffset": 91, "endOffset": 95}, {"referenceID": 43, "context": "Other adaptive transfer function approaches for speaker adaptation have also been proposed [42], [44], as have \u201cbasis\u201d approaches [54]\u2013[56].", "startOffset": 97, "endOffset": 101}, {"referenceID": 53, "context": "Other adaptive transfer function approaches for speaker adaptation have also been proposed [42], [44], as have \u201cbasis\u201d approaches [54]\u2013[56].", "startOffset": 130, "endOffset": 134}, {"referenceID": 55, "context": "Other adaptive transfer function approaches for speaker adaptation have also been proposed [42], [44], as have \u201cbasis\u201d approaches [54]\u2013[56].", "startOffset": 135, "endOffset": 139}, {"referenceID": 56, "context": "This motivates combining LHUC with speaker adaptive training (SAT) [57] in which the hidden units are trained to capture both good average representations and speakerspecific representations, by estimating speaker-specific hidden unit amplitudes for each training speaker.", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "We experimentally investigated LHUC and SAT-LHUC using four different corpora: the TED talks corpus [15] following the IWSLT evaluation protocol (www.", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "org); the Switchboard corpus of conversational telephone speech [17] (ldc.", "startOffset": 64, "endOffset": 68}, {"referenceID": 15, "context": "edu); the AMI meetings corpus [16], [58] (corpus.", "startOffset": 30, "endOffset": 34}, {"referenceID": 57, "context": "edu); the AMI meetings corpus [16], [58] (corpus.", "startOffset": 36, "endOffset": 40}, {"referenceID": 17, "context": "org); and the Aurora4 corpus of read speech with artificially corrupted acoustic environments [18] (catalog.", "startOffset": 94, "endOffset": 98}, {"referenceID": 4, "context": "The output logistic regression layer models the distribution of context-dependent clustered tied states [5].", "startOffset": 104, "endOffset": 107}, {"referenceID": 58, "context": "com) following the IWSLT ASR evaluation protocol [59] (iwslt.", "startOffset": 49, "endOffset": 53}, {"referenceID": 8, "context": "The training data consisted of 143 hours of speech (813 talks) and the systems follow our previously described recipe [9].", "startOffset": 118, "endOffset": 121}, {"referenceID": 8, "context": "In this work however, compared to our previous works [9], [13], [43], our systems employ more accurate language models developed for our IWSLT\u20132014 systems [60]: in particular, the final reported results use a 4-gram language model estimated from 751 million words.", "startOffset": 53, "endOffset": 56}, {"referenceID": 12, "context": "In this work however, compared to our previous works [9], [13], [43], our systems employ more accurate language models developed for our IWSLT\u20132014 systems [60]: in particular, the final reported results use a 4-gram language model estimated from 751 million words.", "startOffset": 58, "endOffset": 62}, {"referenceID": 42, "context": "In this work however, compared to our previous works [9], [13], [43], our systems employ more accurate language models developed for our IWSLT\u20132014 systems [60]: in particular, the final reported results use a 4-gram language model estimated from 751 million words.", "startOffset": 64, "endOffset": 68}, {"referenceID": 59, "context": "In this work however, compared to our previous works [9], [13], [43], our systems employ more accurate language models developed for our IWSLT\u20132014 systems [60]: in particular, the final reported results use a 4-gram language model estimated from 751 million words.", "startOffset": 156, "endOffset": 160}, {"referenceID": 60, "context": "AMI: We follow the Kaldi GMM recipe described in [61]", "startOffset": 49, "endOffset": 53}, {"referenceID": 7, "context": "For AMI, we also evaluate the effectiveness of LHUC and SAT-LHUC on AMs that utilise convolutional layers [8], [62], [63], where the CNN networks were trained as described in [64] but with 300 convolutional filters.", "startOffset": 106, "endOffset": 109}, {"referenceID": 61, "context": "For AMI, we also evaluate the effectiveness of LHUC and SAT-LHUC on AMs that utilise convolutional layers [8], [62], [63], where the CNN networks were trained as described in [64] but with 300 convolutional filters.", "startOffset": 111, "endOffset": 115}, {"referenceID": 62, "context": "For AMI, we also evaluate the effectiveness of LHUC and SAT-LHUC on AMs that utilise convolutional layers [8], [62], [63], where the CNN networks were trained as described in [64] but with 300 convolutional filters.", "startOffset": 117, "endOffset": 121}, {"referenceID": 63, "context": "For AMI, we also evaluate the effectiveness of LHUC and SAT-LHUC on AMs that utilise convolutional layers [8], [62], [63], where the CNN networks were trained as described in [64] but with 300 convolutional filters.", "startOffset": 175, "endOffset": 179}, {"referenceID": 64, "context": "Switchboard: We use the Kaldi GMM recipe [65], [66], using Switchboard\u20131 Release 2 (LDC97S62).", "startOffset": 41, "endOffset": 45}, {"referenceID": 65, "context": "Switchboard: We use the Kaldi GMM recipe [65], [66], using Switchboard\u20131 Release 2 (LDC97S62).", "startOffset": 47, "endOffset": 51}, {"referenceID": 17, "context": "Aurora4: The Aurora 4 task is a small scale, medium vocabulary noise and channel ASR robustness task based on the Wall Street Journal corpus [18].", "startOffset": 141, "endOffset": 145}, {"referenceID": 12, "context": "LHUC and SAT-LHUC approaches (adapting the other way round \u2013 starting from the top layer \u2013 is much less effective [13]).", "startOffset": 114, "endOffset": 118}, {"referenceID": 37, "context": "This suggests that it is not necessary to carefully regularise the model \u2013 for example, by KullbackLeibler divergence training [38] which is usually required when adapting the weights of one or more layers in a network.", "startOffset": 127, "endOffset": 131}, {"referenceID": 66, "context": "08 and was later adjusted according to the newbob learning scheme [67].", "startOffset": 66, "endOffset": 70}, {"referenceID": 13, "context": "In this work we train SAT-LHUC models with frame-level [14], segment-level and speaker-level clusters.", "startOffset": 55, "endOffset": 59}, {"referenceID": 13, "context": "However, the difference, as shown experimentally in [14], is mostly due to poorer quality adaptation targets resulting from the corresponding first pass SAT-LHUC systems rather than the differences in learned representations.", "startOffset": 52, "endOffset": 56}, {"referenceID": 13, "context": "If different models for SI and SD decodes are acceptable, then further small gains in accuracy are observed [14].", "startOffset": 108, "endOffset": 112}, {"referenceID": 13, "context": "1These results are compatible with further SAT-LHUC results using this tst2013 set in [14]", "startOffset": 86, "endOffset": 90}, {"referenceID": 67, "context": "trained using cross-entropy: a mismatched adaptation objective (here cross-entropy) can easily erase sequence information from the weight matrices due to the well-known effect of catastrophic forgetting [68] in neural networks.", "startOffset": 203, "endOffset": 207}, {"referenceID": 38, "context": "Indeed Huang and Gong [39] report no gain from adapting SE-DNN models with a KL divergence regularised [38] cross-entropy adaptation objective and supervised adaptation targets.", "startOffset": 22, "endOffset": 26}, {"referenceID": 37, "context": "Indeed Huang and Gong [39] report no gain from adapting SE-DNN models with a KL divergence regularised [38] cross-entropy adaptation objective and supervised adaptation targets.", "startOffset": 103, "endOffset": 107}, {"referenceID": 68, "context": "In this work we adapt state-level minimum Bayes risk (sMBR) [69], [70] sequence-trained models with LHUC approach and report results on TED tst2011 and tst2013 in Table VI.", "startOffset": 60, "endOffset": 64}, {"referenceID": 69, "context": "In this work we adapt state-level minimum Bayes risk (sMBR) [69], [70] sequence-trained models with LHUC approach and report results on TED tst2011 and tst2013 in Table VI.", "startOffset": 66, "endOffset": 70}, {"referenceID": 70, "context": "We compared also our adaptation results to the most accurate system of the IWSLT\u20132013 TED transcription evaluation, which performed both feature- and model-space speaker adaptation [71].", "startOffset": 181, "endOffset": 185}, {"referenceID": 40, "context": "For model-space adaptation that system used a method which adapts DNNs with a speaker-dependent layer [41].", "startOffset": 102, "endOffset": 106}, {"referenceID": 70, "context": "IWSLT2013 winner system (numbers taken from [71]) DNN (sMBR) + HUB4 + WSJ 15.", "startOffset": 44, "endOffset": 48}, {"referenceID": 40, "context": "1 +++++ SAT on DNN [41] 7.", "startOffset": 19, "endOffset": 23}, {"referenceID": 59, "context": "5 Our system [60] DNN (sMBR) + AMI data 9.", "startOffset": 13, "endOffset": 17}, {"referenceID": 70, "context": "This allows our single-model system to match a considerably more sophisticated post-processing pipeline [71], as outlined in Table VII.", "startOffset": 104, "endOffset": 108}, {"referenceID": 31, "context": "In related work [32] LHUC was employed using alignments obtained from an SIGMM system with a 8.", "startOffset": 16, "endOffset": 20}, {"referenceID": 28, "context": "3Due to space constraints we do not make an explicit comparisons to other techniques such as auxiliary i-vector features or speaker-codes; however, the literature suggest that the use of i-vectors give similar [29] results when compared to fMLLR trained models.", "startOffset": 210, "endOffset": 214}, {"referenceID": 31, "context": "Related recent studies also show LHUC is at least as good as the standard use of i-vector features [32], [72].", "startOffset": 99, "endOffset": 103}, {"referenceID": 71, "context": "Related recent studies also show LHUC is at least as good as the standard use of i-vector features [32], [72].", "startOffset": 105, "endOffset": 109}, {"referenceID": 3, "context": "feature-space discriminative linear regression (fDLR) [4], but neither of these matches SAT trained feature transform models.", "startOffset": 54, "endOffset": 57}, {"referenceID": 72, "context": "This could be due to the fact Switchboard data is narrow-band and as such contains less information for discrimination between speakers [73], especially when estimating relevant statistics from small amounts of unsupervised adaptation data.", "startOffset": 136, "endOffset": 140}, {"referenceID": 73, "context": "Another potential reason could be related to the fact that the Switchboard part of eval2000 is characterised by a large overlap between training and test speakers \u2013 36 out of 40 test speakers are observed in training [74], which limits the need for adaptation, but also enables models to learn much more accurate speaker-characteristics during supervised speaker adaptive training.", "startOffset": 217, "endOffset": 221}, {"referenceID": 74, "context": "The idea is similar to LHUC applied to channel normalisation between distant and close talking scenarios [75], except we use two independently estimated transforms.", "startOffset": 105, "endOffset": 109}, {"referenceID": 75, "context": "We adapted baseline multi-condition trained DNN models [76] to the speaker (rS) and the environment (rE).", "startOffset": 55, "endOffset": 59}, {"referenceID": 76, "context": "We also train more competitive models following Rennie et al [77]: Maxout [78] CNN models were trained using annealed dropout.", "startOffset": 61, "endOffset": 65}, {"referenceID": 77, "context": "We also train more competitive models following Rennie et al [77]: Maxout [78] CNN models were trained using annealed dropout.", "startOffset": 74, "endOffset": 78}, {"referenceID": 76, "context": "However, in contrast to [77], in this work we are", "startOffset": 24, "endOffset": 28}, {"referenceID": 76, "context": "5% in [77]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 78, "context": "Finally, we visualise the top hidden layer activations of the annealed dropout Maxout CNN using stochastic neighbourhood embedding (tSNE) [79] for one utterance recorded under clean and noisy (restaurant) conditions (Fig.", "startOffset": 138, "endOffset": 142}], "year": 2017, "abstractText": "This work presents a broad study on the adaptation of neural network acoustic models by means of learning hidden unit contributions (LHUC) \u2013 a method that linearly re-combines hidden units in a speakeror environment-dependent manner using small amounts of unsupervised adaptation data. We also extend LHUC to a speaker adaptive training (SAT) framework that leads to more adaptable DNN acoustic model, which can work in both a speaker-dependent and a speaker-independent manner, without the requirement to maintain auxiliary speakerdependent feature extractors or to introduce significant speakerdependent changes to the DNN structure. Through a series of experiments on four different speech recognition benchmarks (TED talks, Switchboard, AMI meetings and Aurora4) and over 270 test speakers we show that LHUC in both its test-only and SAT variants results in consistent word error rate reductions ranging from 5% to 23% relative depending on the task and the degree of mismatch between training and test data. In addition we have investigated the effect of the amount of adaptation data per speaker, the quality of adaptation targets when estimating transforms in an unsupervised manner, the complementarity to other adaptation techniques, one-shot adaptation, and an extension to adapting DNNs trained in a sequence discriminative manner.", "creator": "LaTeX with hyperref package"}}}