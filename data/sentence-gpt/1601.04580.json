{"id": "1601.04580", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2016", "title": "Nonparametric Bayesian Storyline Detection from Microtexts", "abstract": "News events and social media are composed of evolving storylines, which capture public attention for a limited period of time. Identifying these storylines would enable many high-impact applications, such as tracking public interest and opinion in ongoing crisis events. However, this requires integrating temporal and linguistic information, and prior work takes a largely heuristic approach: a single person will have to search for evidence for a specific event and the individual will have to re-identify the events at a time when they occur.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 18 Jan 2016 15:46:00 GMT  (23kb)", "https://arxiv.org/abs/1601.04580v1", "This report is based on a rejected submission from the 2015 Conference on Empirical Methods on Natural Language Processing, incorporating some of the reviewers' suggestions for improvement"], ["v2", "Sat, 24 Sep 2016 20:27:47 GMT  (24kb)", "http://arxiv.org/abs/1601.04580v2", "Appeared at the Workshop on Computing News Storylines at the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP 2016)"]], "COMMENTS": "This report is based on a rejected submission from the 2015 Conference on Empirical Methods on Natural Language Processing, incorporating some of the reviewers' suggestions for improvement", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["vinodh krishnan", "jacob eisenstein"], "accepted": false, "id": "1601.04580"}, "pdf": {"name": "1601.04580.pdf", "metadata": {"source": "CRF", "title": "Nonparametric Bayesian Storyline Detection from Microtexts", "authors": ["Vinodh Krishnan"], "emails": ["krishnan.vinodh@gmail.com", "jacobe@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 1.\n04 58\n0v 2\n[ cs\n.C L\n] 2\n4 Se"}, {"heading": "1 Introduction", "text": "A long-standing goal for information retrieval and extraction is to identify and group textual references to ongoing events in the world (Allan, 2002). Success on this task would have applications in personalized news portals (Gabrilovich et al., 2004), intelligence analysis, disaster relief (Vieweg et al., 2010), and in understanding the properties of the news cycle (Leskovec et al., 2009). This task attains a new importance in the era of social media, where citizen journalists can document events as they unfold (Lotan et al., 2011), but where repetition and untrustworthy information can make the reader\u2019s task especially challenging (Becker et al., 2011; Marcus et al., 2011; Petrovic\u0301 et al., 2010).\nA major technical challenge is in fusing information from two heterogeneous data sources: textual content and time. Two different documents about a single event might use very different vocabulary, particularly in sparse social media data such as microblogs; conversely, two different sporting events might be described in nearly identical language, with differences only in the numerical outcome. Temporal information is therefore critical: in the first case, to find the commonalities across disparate writing styles, and in the second case, to identify the differences. A further challenge is that unlike in standard document clustering tasks, the number of events in a data stream is typically unknown in advance. Finally, there is a high premium on scalability, since online text is produced at a high rate.\nDue to these challenges, existing approaches for combining these modalities have been somewhat heuristic, relying on tunable parameters to control the tradeoff between textual and temporal similarity. In contrast, the Bayesian setting provides elegant formalisms for reasoning about latent structures (e.g., events) and their stochastically-generated realizations across text and time. In this paper, we describe one such model, based on the distancedependent Chinese Restaurant Process (dd-CRP; Blei and Frazier, 2011). This model is distinguished by the neat separation that it draws between textual content, which is treated as a stochastic emission from an unknown Multinomial distribution, and time, which is modeled as a prior on graphs over documents, through an arbitrary distance function. However, straightforward implementations of the dd-CRP are insufficiently scalable, and so the model\nhas been relatively underutilized in the NLP literature (Titov and Klementiev, 2011; Kim and Oh, 2011; Sirts et al., 2014). We describe improvements to Bayesian inference that make the application of this model feasible, and present encouraging empirical results on the Tweet Timeline Generation task from TREC 2014 (Lin et al., 2014)."}, {"heading": "2 Model", "text": "The basic task that we address is to group short text documents into an unknown number of storylines, based on their textual content and their temporal signature. The textual content may be extremely sparse \u2014 the typical Tweet is on the order of ten words long \u2014 so leveraging temporal information is crucial. Moreover, the temporal signal is multiscale: in the 24-hour news cycle, some storylines last for less than an hour, while others, like the disappearance of the Malaysian Airlines 370 plane in 2014, continue for weeks or months. In some cases, the temporal distribution of references to a storyline will be unimodal and well-described by a parametric model (Marcus et al., 2011); in other cases, it may be irregular, with bursts of activity followed by periods of silence (He et al., 2007). Finally, it will be crucial to produce an implementation that scales to large corpora.\nThe distance-dependent Chinese Restaurant Process (dd-CRP) meets many of these criteria (Blei and Frazier, 2011). In this model, the key idea is that each instance (document) i \u201cfollows\u201d another instance ci (where it is possible that ci = i), inducing a graph. We can compute a partitioning over instances by considering the connected components in the undirected version of the follower graph; these partitions correspond to \u201ctables\u201d in the conventional \u201cChinese Restaurant\u201d analogy (Aldous, 1985), or to clusters. The advantage of this approach is that it is fundamentally non-parametric, and it introduces a clean separation between the textual data and the covariates: the text is generated by a distribution associated with the partition, while the covariates are associated with the following links, which are conditioned on a distance function.\nThe distribution over follower links for document\ni has the following form,\nPr(ci = j) \u221d\n{\nf(di,j), i 6= j \u03b1, i = j, (1)\nwhere di,j is the distance between units i and j, and \u03b1 > 0 is a parameter of the model. Large values of \u03b1 induce more self-links and therefore more finegrained partitionings. Since we are concerned with temporal covariates, we define the distance function as follows:\nf(di,j) = e \u2212|ti\u2212tj | a . (2)\nThus, the likelihood of document i following document j decreases exponentially as the time gap |ti \u2212 tj | increases.\nThe text of each document i is represented by a vector of word counts wi. The likelihood distribution is multinomial, conditioned on a parameter \u03b8 associated with the partition to which document i belongs. By placing a Dirichlet prior on \u03b8, we can analytically integrate it out. Writing z(c) for the cluster membership induced by the follower graph c, we have:\nP (w | c; \u03b7) = \u220f\nk\nP ({wi : z (c) i = k}; \u03b7) (3)\n= \u220f\nk\n\u222b\n\u03b8\nP ({wi : z (c) i = k} | \u03b8)P (\u03b8; \u03b7)d\u03b8\n(4)\nGiven a multinomial likelihood P (w | \u03b8) and a (symmetric) Dirichlet prior P (\u03b8 | \u03b7), this integral has a closed-form solution as the DirichletMultinomial distribution (also known as the multivariate Polya distribution). The joint probability is therefore equal to the product of Equation 1 and Equation 4,\nP (w, c) = \u220f\ni\nP (ci;\u03b1, a) \u220f\nk\nP ({wi : z (c) i = k}; \u03b7).\n(5)\nThe model has three hyperparameters: \u03b1, which controls the likelihood of self-linking, and therefore affects the number of clusters; a, which controls the time scale of the distance function, and therefore affects the importance of the temporal dimension to the resulting clusters; and \u03b7, which controls the precision of the Dirichlet prior, and therefore the importance of rare words in the textual likelihood function.\nEstimation of these hyperparameters is described in \u00a7 3.2."}, {"heading": "3 Inference", "text": "The key sampling equation for the dd-CRP is the posterior likelihood,\nPr(ci = j | c\u2212i,w) \u221dPr(ci = j)P (w | c).\nThe prior is defined in Equation 1. Let \u2113 represent the likelihood under the partitioning induced when the link ci is cut. Now, the likelihood term has two cases: in the first case, j is already in the same connected component as i (even after cutting the link ci), so no components are merged by setting ci = j. In this case, the likelihood P (w | ci = j) is exactly equal to \u2113. In the second case, setting ci = j causes two clusters to be merged. This gives the likelihood,\nP (w | ci = j, c\u2212i)\n\u221d P ({wk : z\n(c) k = z (c) j \u2228 z (c) k = z (c) i })\nP ({wk : z (c) k = z (c) i })P ({wk : z (c) k = z (c) j })\n,\nwhere the constant of proportionality is exactly equal to \u2113. Each of the terms in the likelihood ratio is a Dirichlet Compound Multinomial likelihood. This likelihood function is itself a ratio of gamma functions; by eliminating constant terms and exploiting the identity \u0393(x + 1) = x\u0393(x), we can reduce the number of Gamma function evaluations required to compute this ratio to the number of words which appear in both clusters z(c)i and z (c) j . Words that occur in neither cluster can safely be ignored, and the gamma functions for words which occur in exactly one of the two clusters cancel in the numerator and denominator of the ratio. Note also that we only need compute the likelihood for ci with respect to each cluster, not for every possible follower link."}, {"heading": "3.1 Online inference", "text": "While we make every effort to accelerate the computation of individual Gibbs samples, the complexity of the basic algorithm is superlinear in the number of instances. This is due to the fact that each sample requires computing the probability of instance i joining every possible cluster, while the number of clusters itself grows with the number of instances\n(this growth is logarithmic in the Chinese Restaurant Process). Scalability to the streaming setting therefore requires more aggressive optimizations.\nTo get back to linear time complexity, we employ a fixed-lag sampling procedure (Doucet et al., 2000). After receiving instance i, we perform Gibbs sampling only within the fixed window [ti \u2212 \u03c4, ti], leaving cj fixed if tj < ti \u2212 \u03c4 . This approximate sampling procedure implicitly changes the underlying model, because there is no possibility of linking i to a later message j if the time gap tj \u2212 ti > \u03c4 .\nSince we are only interested in obtaining a single storyline clustering \u2014 rather than a full Bayesian distribution over clusterings \u2014 we perform annealing for samples towards the end of the sampling window. Specifically, we set the temperature to \u03b3 = 2.0 and exponentiate the sampling likelihood by the inverse temperature (Geman and Geman, 1984). This has the effect of interpolating between probabilistically-correct Gibbs sampling and a hard coordinate-ascent procedure."}, {"heading": "3.2 Hyperparameter estimation", "text": "The model has three parameters to estimate:\n\u2022 \u03b1, the concentration parameter of the dd-CRP \u2022 a, the offset of the distance function \u2022 \u03b7, the scale of the symmetric Dirichlet prior.\nWe interleave maximization-based updates to these parameters with sampling, in a procedure inspired by Monte Carlo Expectation Maximization (Wei and Tanner, 1990). Specifically, we compute gradients on the likelihood P (c) with respect to \u03b1 and a, and take gradient steps after every fixed number of samples. For the symmetric Dirichlet parameter \u03b7, we employ the heuristic from Minka (2012) by setting the parameter to \u03b7 = (K\u22121)/2\u2211\nk log pk , where K is the num-\nber of words that appear exactly once, and pk is the probability of choosing the kth word from the vocabulary under the unigram distribution for the entire corpus."}, {"heading": "4 TREC Evaluation", "text": "To test the efficacy of this approach, we evaluate on the Twitter Timeline Generation (TTG) task in the Microblog track of TREC 2014. It involves taking tweets based on a query Q at time T and returning\na summary that captures relevant information. We perform the task on 55 queries with different timestamps and compare our results with 13 groups that submitted 50 runs for this task in 2014.\nWe consider the following systems:\nBaseline We replace the distance-dependent prior with a standard Dirichlet prior. The number of clusters is heuristically set to 20. Annealed Gibbs sampling is employed for inference.\nOffline inference The dd-CRP model with offline inference procedure (described in \u00a7 3).\nOnline inference The dd-CRP model with online inference procedure (described in \u00a7 3.1).\nFor the online inference implementation, we set the size of window and number of iterations to five days and 500 respectively. For the baseline, the parameter of the Dirichlet prior was set to a vector of 0.5 for each cluster. These values were chosen through 10-fold cross validation.\nTo measure the quality of the clusterings obtained by these models, we compare the average weighted and unweighted F-measures for 55 TREC topics, using the evaluation scripts from the TREC TTG task. Overall results are shown in Table 1. The ONLINE MODEL has the best weighted F1 score, outperforming the offline version of the same model, even though its inference procedure is an approximation to the OFFLINE MODEL. It may be that its approximate inference procedure discourages longrange linkages, thus placing a greater emphasis on the temporal dimension. Both models were trained over 500 iterations, and the ONLINE MODEL was 30% faster to train than the offline model.\nCompared to the other 2014 TREC TTG systems, our dd-CRP models are competitive. Both models outperform all but one of the fourteen submissions on the unweighted F1 metric, and would have placed fourth on the weighted Fw1 metric. Note that the TREC evaluation scores both clustering quality and retrieval. We use only the baseline retrieval model, which achieved a mean average precision of 0.31. The competing systems shown in Table 1 all use retrieval models that are far superior: the retrieval model for top-ranked PKUICST team (line 4) achieved a mean average precision (MAP) of 0.59 (Lv et al., 2014), and the QCRI (Magdy et al.,\n2014) and and hltcoe (Xu et al., 2014) teams (lines 5 and 6) used retrieval models with MAP scores of at least 0.5. Bayesian dd-CRP storyline clustering was competitive with these timeline generation systems despite employing a far worse retrieval model, so improving the retrieval model to achieve parity with these alternative systems seems the most straightforward path towards better overall performance."}, {"heading": "5 Related work", "text": "Topic tracking and first-story detection are very well-studied tasks; space does not permit a complete analysis of the related work, but see (Allan, 2002) for a summary of \u201cfirst generation\u201d research. More recent non-Bayesian approaches have focused on string overlap (Suen et al., 2013), submodular optimization (Shahaf et al., 2012), and locality-sensitive hashing (Petrovic\u0301 et al., 2010). In Bayesian storyline analysis, the seminal models are Topics-OverTime (Wang and McCallum, 2006), which associates a parametric distribution over time with each topic (Ihler et al., 2006), and the Dynamic Topic Model (Blei and Lafferty, 2006), which models topic evolution as a linear dynamical system (Nallapati et al., 2007). Later work by Diao et al. (2012) offers a model for identifying \u201cbursty\u201d topics, with inference requiring dynamic programming. All these approaches require the number of topics to be identified in advance. Kim and Oh (2011) apply a distance-dependent Chinese Restaurant Franchise for temporal topic modeling; they evaluate using predictive likelihood rather than comparing against ground truth, and do not consider online inference.\nThe Infinite Topic-Cluster model (Ahmed et al., 2011a) is non-parametric over the number of storylines, through the use of the recurrent Chinese Restaurant Process (rCRP). The model is substantially more complex than our approach. Unlike the dd-CRP, the rCRP is Markovian in nature, so that the topic distribution at each point in time is conditioned on the previous epoch (or, at best, the previous K epochs, with complexity of inference increasing with K). This Markovian assumption creates probabilistic dependencies between the topic assignment for a given document and the documents that follow in subsequent epochs, necessitating an inference procedure that combines sequential\nMonte Carlo and Metropolis Hastings, and a custom data structure; this inference procedure was complex enough to warrant a companion paper (Ahmed et al., 2011b). The rCRP is also employed by Diao and Jiang (2013, 2014). In contrast, the dd-CRP makes no Markovian assumptions, and efficient inference is possible through relatively straightforward Gibbs sampling in a fixed window."}, {"heading": "6 Conclusion", "text": "We present a simple non-parametric model for clustering short documents (such as tweets) into storylines, which are conceptually coherent and temporally focused. Future work may consider learning more flexible temporal distance functions, which could potentially represent temporal periodicity or parametric models of content popularity.\nAcknowledgments We thank the reviewers for their helpful feedback. This research was supported by an award from the National Institutes for Health (R01GM112697-01), and by Google, through a Focused Research Award for Computational Journalism."}], "references": [{"title": "Unified analysis of streaming news", "author": ["Ahmed et al", "2011a Amr Ahmed", "Qirong Ho", "Jacob Eisenstein", "Eric Xing", "Alexander J. Smola", "Choon H. Teo"], "venue": "In WWW,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Online inference for the infinite topic-cluster model: Storylines from streaming text", "author": ["Ahmed et al", "2011b Amr Ahmed", "Qirong Ho", "Choon H Teo", "Jacob Eisenstein", "Eric P Xing", "Alex J Smola"], "venue": "In AISTATS,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Topic detection and tracking: event-based information organization, volume 12", "author": ["Allan", "2002 James Allan"], "venue": null, "citeRegEx": "Allan and Allan.,? \\Q2002\\E", "shortCiteRegEx": "Allan and Allan.", "year": 2002}, {"title": "Beyond trending topics: Real-world event identification on twitter", "author": ["Becker et al", "2011 Hila Becker", "Mor Naaman", "Luis Gravano"], "venue": "In Proceedings of the International Conference on Web and Social Media (ICWSM),", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Distance dependent chinese restaurant processes", "author": ["Blei", "Frazier", "2011 David M Blei", "Peter I Frazier"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2011}, {"title": "Dynamic topic models", "author": ["Blei", "Lafferty", "2006 David M Blei", "John D Lafferty"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Blei et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2006}, {"title": "A unified model for topics, events and users on twitter", "author": ["Diao", "Jiang", "2013 Qiming Diao", "Jing Jiang"], "venue": "In Proceedings of Empirical Methods for Natural Language Processing (EMNLP)", "citeRegEx": "Diao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Diao et al\\.", "year": 2013}, {"title": "Recurrent chinese restaurant process with a durationbased discount for event identification from twitter. In The 18th Pacific-Asia Conference on Knowledge Discovery and Data Mining (SDM\u201914)", "author": ["Diao", "Jiang", "2014 Qiming Diao", "Jing Jiang"], "venue": null, "citeRegEx": "Diao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Diao et al\\.", "year": 2014}, {"title": "Finding bursty topics from microblogs", "author": ["Diao et al", "2012 Qiming Diao", "Jing Jiang", "Feida Zhu", "Ee-Peng Lim"], "venue": "In Proceedings of the Association for Computational Linguistics (ACL),", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "On sequential monte carlo sampling methods for bayesian filtering", "author": ["Doucet et al", "2000 Arnaud Doucet", "Simon Godsill", "Christophe Andrieu"], "venue": "Statistics and computing,", "citeRegEx": "al. et al\\.,? \\Q2000\\E", "shortCiteRegEx": "al. et al\\.", "year": 2000}, {"title": "Newsjunkie: providing personalized newsfeeds via analysis of information", "author": ["Gabrilovich et al", "2004 Evgeniy Gabrilovich", "Susan Dumais", "Eric Horvitz"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2004\\E", "shortCiteRegEx": "al. et al\\.", "year": 2004}, {"title": "Stochastic relaxation, gibbs distributions, and the bayesian restoration of images", "author": ["Geman", "1984 Stuart Geman", "Donald Geman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Geman et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Geman et al\\.", "year": 1984}, {"title": "Analyzing feature trajectories for event detection", "author": ["He et al", "2007 Qi He", "Kuiyu Chang", "Ee-Peng Lim"], "venue": "In SIGIR,", "citeRegEx": "al. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al. et al\\.", "year": 2007}, {"title": "Adaptive event detection with time-varying poisson processes", "author": ["Ihler et al", "2006 Alexander Ihler", "Jon Hutchins", "Padhraic Smyth"], "venue": "In KDD,", "citeRegEx": "al. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al. et al\\.", "year": 2006}, {"title": "Accounting for data dependencies within a hierarchical dirichlet process mixture model", "author": ["Kim", "Oh", "2011 Dongwoo Kim", "Alice Oh"], "venue": "In cikm,", "citeRegEx": "Kim et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2011}, {"title": "Meme-tracking and the dynamics of the news cycle", "author": ["Leskovec et al", "2009 Jure Leskovec", "Lars Backstrom", "Jon Kleinberg"], "venue": "In Proceedings of Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Overview of the trec-2014 microblog track", "author": ["Lin et al", "2014 Jimmy Lin", "Miles Efron", "Yulu Wang", "Garrick Sherman"], "venue": "In Proceedings of the Twenty-Third Text REtrieval Conference", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "The arab spring\u2014 the revolutions were tweeted: Information flows during the 2011 tunisian and egyptian revolutions", "author": ["Lotan et al", "2011 Gilad Lotan", "Erhardt Graeff", "Mike Ananny", "Devin Gaffney", "Ian Pearce", "danah boyd"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "PKUICST at TREC 2014 Microblog Track: feature extraction for effective microblog search and adaptive clustering algorithms for TTG", "author": ["Lv et al", "2014 Chao Lv", "Feifan Fan", "Runwei Qiang", "Yue Fei", "Jianwu Yang"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Qcri at trec 2014: applying the kiss principle for the ttg task in the microblog", "author": ["Magdy et al", "2014 Walid Magdy", "Wei Gao", "Tarek Elganainy", "Zhongyu Wei"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Twitinfo: aggregating and visualizing microblogs for event exploration", "author": ["Marcus et al", "2011 Adam Marcus", "Michael S Bernstein", "Osama Badar", "David R Karger", "Samuel Madden", "Robert C Miller"], "venue": "In chi,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Estimating a dirichlet distribution. http://research. microsoft.com/en-us/um/people/minka/ papers/dirichlet/minka-dirichlet", "author": ["Minka", "2012 Thomas Minka"], "venue": null, "citeRegEx": "Minka and Minka.,? \\Q2012\\E", "shortCiteRegEx": "Minka and Minka.", "year": 2012}, {"title": "Multiscale topic tomography", "author": ["Nallapati et al", "2007 Ramesh M Nallapati", "Susan Ditmore", "John D Lafferty", "Kin Ung"], "venue": "In KDD,", "citeRegEx": "al. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al. et al\\.", "year": 2007}, {"title": "Streaming first story detection with application to twitter", "author": ["Petrovi\u0107 et al", "2010 Sa\u0161a Petrovi\u0107", "Miles Osborne", "Victor Lavrenko"], "venue": "In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL),", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Trains of thought: Generating information maps", "author": ["Shahaf et al", "2012 Dafna Shahaf", "Carlos Guestrin", "Eric Horvitz"], "venue": "In Proceedings of the Conference on World-Wide Web (WWW),", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Pos induction with distributional and morphological information using a distance-dependent chinese restaurant process", "author": ["Sirts et al", "2014 Kairit Sirts", "Jacob Eisenstein", "Micha Elsner", "Sharon Goldwater"], "venue": "In Proceedings of the Association", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Nifty: a system for large scale information flow tracking and clustering", "author": ["Suen et al", "2013 Caroline Suen", "Sandy Huang", "Chantat Eksombatchai", "Rok Sosic", "Jure Leskovec"], "venue": "In Proceedings of the Conference on World-Wide Web (WWW),", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "A bayesian model for unsupervised semantic parsing", "author": ["Titov", "Klementiev", "2011 Ivan Titov", "Alexandre Klementiev"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "Titov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Titov et al\\.", "year": 2011}, {"title": "Microblogging during two natural hazards events: What twitter may contribute to situational awareness", "author": ["Vieweg et al", "2010 Sarah Vieweg", "Amanda L. Hughes", "Kate Starbird", "Leysia Palen"], "venue": "In Proceedings of the SIGCHI Conference on Human Factors", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Topics over time: a non-markov continuous-time model of topical trends", "author": ["Wang", "McCallum", "2006 Xuerui Wang", "Andrew McCallum"], "venue": "In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Wang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2006}, {"title": "A monte carlo implementation of the em algorithm and the poor man\u2019s data augmentation algorithms", "author": ["Wei", "Tanner", "1990 Greg CG Wei", "Martin A Tanner"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Wei et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Wei et al\\.", "year": 1990}, {"title": "Hltcoe at trec 2014: Microblog and clinical decision support", "author": ["Xu et al", "2014 Tan Xu", "Paul McNamee", "Douglas W Oard"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "Later work by Diao et al. (2012) offers a model for identifying \u201cbursty\u201d topics, with inference requiring dynamic programming.", "startOffset": 14, "endOffset": 33}, {"referenceID": 6, "context": "Later work by Diao et al. (2012) offers a model for identifying \u201cbursty\u201d topics, with inference requiring dynamic programming. All these approaches require the number of topics to be identified in advance. Kim and Oh (2011) apply a distance-dependent Chinese Restaurant Franchise for temporal topic modeling; they evaluate using predictive likelihood rather than comparing against ground truth, and do not consider online inference.", "startOffset": 14, "endOffset": 224}], "year": 2016, "abstractText": "News events and social media are composed of evolving storylines, which capture public attention for a limited period of time. Identifying storylines requires integrating temporal and linguistic information, and prior work takes a largely heuristic approach. We present a novel online non-parametric Bayesian framework for storyline detection, using the distance-dependent Chinese Restaurant Process (dd-CRP). To ensure efficient linear-time inference, we employ a fixed-lag Gibbs sampling procedure, which is novel for the dd-CRP. We evaluate on the TREC Twitter Timeline Generation (TTG), obtaining encouraging results: despite using a weak baseline retrieval model, the dd-CRP story clustering method is competitive with the best entries in the 2014 TTG task.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}