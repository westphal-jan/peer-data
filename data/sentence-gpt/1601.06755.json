{"id": "1601.06755", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2016", "title": "The Utility of Hedged Assertions in the Emergence of Shared Categorical Labels", "abstract": "We investigate the emergence of shared concepts in a community of language users using a multi-agent simulation. We extend results showing that negated assertions are of use in developing shared categories, to include assertions modified by linguistic hedges. Results show that using hedged assertions positively affects the emergence of shared categories in two distinct ways. Firstly, using contraction hedges like `very' gives better convergence over time. Secondly, using expansion hedges such as `quite' reduces concept overlap. However, both these improvements come at a cost of slower speed of development. Finally, the results demonstrate that the evolution of concept convergence and usage of different types of concepts may influence the evolution of concepts across a range of contexts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 25 Jan 2016 20:24:50 GMT  (144kb,D)", "http://arxiv.org/abs/1601.06755v1", "AISB 2013, updated to include cross-reference to previous work"]], "COMMENTS": "AISB 2013, updated to include cross-reference to previous work", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.MA", "authors": ["martha lewis", "jonathan lawry"], "accepted": false, "id": "1601.06755"}, "pdf": {"name": "1601.06755.pdf", "metadata": {"source": "CRF", "title": "The Utility of Hedged Assertions in the Emergence of Shared Categorical Labels", "authors": ["Martha Lewis", "Jonathan Lawry"], "emails": ["martha.lewis@bristol.ac.uk,", "j.lawry@bristol.ac.uk"], "sections": [{"heading": "1 INTRODUCTION", "text": "An evolutionary approach to semantics enables the development in robots and autonomous agents of flexible, mutable concepts that could be learnt through interaction and can change over time [13]. This approach is investigated by Eyre and Lawry in [2], in which they develop a model of language evolution based in the label semantics framework. They show that using a mixture of positive and negated assertions enables the development of languages that are both shared, and discriminate effectively between elements within the environment. We extend this work to include assertions modified by the words \u2018very\u2019 and \u2018quite\u2019, and show that doing so improves performance in two ways. Use of the hedge \u2018very\u2019 improves levels of convergence attained. Using the hedge \u2018quite\u2019 reduces the amount of overlap within an agent\u2019s label set. We describe in detail the theoretical approach to concepts taken and linguistic hedges in the remainder of this section. Section 2 gives details of the mathematical and computational model used in the simulations. Section 3 gives results of the simulations which are discussed in section 4. Lastly, section 5 gives conclusions and further avenues of research."}, {"heading": "1.1 A representation model for concepts", "text": "We model concepts within the label semantics framework [7, 8], combined with prototype theory [11] and the conceptual spaces model of concepts [3]. Prototype theory offers an alternative to the classical theory of concepts, basing categorization on proximity to a prototype. This approach is based on experimental results where human subjects were found to view membership in a concept as a matter of degree, with some objects having higher membership than others [11]. Fuzzy set theory [15], in which an object x has a graded membership \u00b5L(x) in a concept L, was proposed as a formalism for prototype theory. However, numerous objections to its suitability have been made [10, 12, 6, 5, 4].\n1 University of Bristol, England, email: martha.lewis@bristol.ac.uk, j.lawry@bristol.ac.uk\nConceptual spaces theory renders concepts as convex regions of a conceptual space - a geometrical structure with quality dimensions and a distance metric. Examples are: the RGB colour cube, pictured in figure 1; physical dimensions of height, breadth and depth; or the taste tetrahedron. Since concepts are convex regions of such spaces, the centroid of such a region can naturally be viewed as the prototype of the concept.\nLabel semantics [7] is a random set approach to concepts which quantifies an agent\u2019s uncertainty about the extent of application of a concept. We refer to this as subjective uncertainty [8] to emphasise that it concerns the definition of concepts and categories, in contrast to stochastic uncertainty which concerns the state of the world. Lawry and Tang [8] combine the label semantics approach with conceptual spaces and prototype theory, to give a formalisation of concepts as based on a prototype and a threshold, located in a conceptual space.\nWithin this framework, agents use sets of labels LA = {L1, L2, ..., Ln} to describe an underlying conceptual space \u2126 with distance metric d(x, y) between points. The conceptual space could be, as mentioned, the RGB colour space. Labels Li would then be concepts such as \u2018red\u2019, \u2018blue\u2019, \u2018purple\u2019, \u2018orange\u2019 and so on. These labels are viewed as regions of the conceptual space. So the concept \u2018blue\u2019 is represented by the blue region in the colour cube. Within label semantics, these regions are specified by prototypes Pi and thresholds \u03b5i. This is in contrast to Ga\u0308rdenfors\u2019 original approach which is to view the space as partitioned by a Voronoi tessellation. If this latter approach is taken, each individual point in the conceptual space is allocated to exactly one label. With a prototype-threshold approach, it is easy to accommodate the idea of an object being accurately described by more than one concept, or conversely, some points within the space not being assigned to any concept. This difar X iv :1\n60 1.\n06 75\n5v 1\n[ cs\n.A I]\n2 5\nJa n\n20 16\nference is illustrated in figures 2 and 3.\nIn this model, however, agents are uncertain as to exactly where the thresholds lie. To illustrate this, consider the concept \u2018tall\u2019. It is easy to point out a tall person, and to point out a person who is not tall, but it is difficult to specify the exact threshold between \u2018tall\u2019 and \u2018not tall\u2019. This uncertainty concerning where the threshold lies is represented in the label semantics framework by saying that a threshold \u03b5i is drawn from a probability distribution \u03b4i. Labels Li are associated with neighbourhoods N \u03b5iLi = {~x \u2208 \u2126 : d(~x, Pi) \u2264 \u03b5i}, i.e. the region within the threshold. These ideas are represented in figure 4.\nThe threshold \u03b5i is uncertain, however, so there is some probability that \u03b5i in figure 4 is actually wide enough to include the object b, i.e. that Li is appropriate to describe b. The appropriateness \u00b5Li(x) of a\nlabel Li to describe an element x is then given by the probability that x lies within the neighbourhood N \u03b5iLi , i.e. that the distance d(x, Pi) is less than \u03b5i. So:\n\u00b5Li(x) = P (d(x, Pi) \u2264 \u03b5i) = \u222b \u221e d(x,Pi) \u03b4i(\u03b5i)d\u03b5i\nFigure 5 shows how this appropriateness measure works in a setup similar to that in figure 4.\nThis appropriateness measure is similar to Zadeh\u2019s description of fuzzy membership in a concept [15]."}, {"heading": "1.2 Linguistic hedges", "text": "Hedges are words or phrases such as \u2018very\u2019, \u2018quite\u2019, \u2018strictly speaking\u2019 which modify the domain of application of a concept. In particular, \u2018very\u2019, and \u2018quite\u2019 respectively contract or expand the domain of application of a concept, so that, for example, \u2018very tall\u2019 applies to fewer people than does \u2018tall\u2019, whereas \u2018quite tall\u2019 applies to more. Within fuzzy set theory, we expect that \u00b5veryL(x) \u2264 \u00b5L(x) and \u00b5quiteL(x) \u2265 \u00b5L(x). Applying this to the concept \u2018tall\u2019, again, this means that membership in the concept \u2018very tall\u2019 should always be less than membership in \u2018tall\u2019. So anyone who can be described as \u2018very tall\u2019 can also be described as \u2018tall\u2019. Zadeh [16] uses operations of concentration and dilation to render these ideas. Concentration is described as CON(\u00b5Li(x)) = (\u00b5Li(x))\n2 and dilation is often rendered as DIL(\u00b5Li(x)) = (\u00b5Li(x))\n1/2. However, we argue, as do [1], that Zadeh\u2019s formulae are, to an extent, arbitrary, since the notion of taking a power of a membership value does not correspond to anything that language users might do. Rather, it simply has some of the right effects. As with [1], we take a semantic approach.\nIn [9], we propose that a concept \u2018very L\u2019 or \u2018quite L\u2019 be rendered by considering that the prototype of \u2018very/quite L\u2019 is equal to that of the base concept L, but that the threshold of the hedged concept \u2018very/quite L\u2019 is respectively smaller or larger than that of the base concept. This approach is grounded in the idea that \u2018very/quite L\u2019 should apply to respectively fewer or more objects than L. Narrowing or widening the threshold achieves this in a natural way. This is illustrated in figure 6.\nOur model of the hedges \u2018very\u2019 and \u2018quite\u2019 therefore requires simply that v\u03b5i \u2264 \u03b5i and that q\u03b5i \u2265 \u03b5i. We implement this model in a version of the multi-agent simulation created in [2] in order to investigate how the use of these hedges in a model of language helps\nthe emergence of shared categories across a community of language users."}, {"heading": "2 METHODS", "text": ""}, {"heading": "2.1 Overview", "text": "To investigate the utility of hedged assertions we implement a multiagent simulation of a version of the category game [14], following [2], in which shared categories develop over time as a result of the interactions of the category users. An overview of the game is as follows. Agents use labels to describe a conceptual space \u2126. At each timestep, agents are randomly paired into speakers and listeners, and each pair is shown a distinct element x \u2208 \u2126. The speaker makes an assertion \u03b8 about the element based on its label set. The listener then updates its own label set to be more similar to that of the speaker, based on this assertion and a parameter w which can be thought of as the age of the speaker. The update made by the speaker is a combination of shifting the prototype of the relevant label and changing the size of the threshold. The aim is that after a number of timesteps, label sets across the population have converged to a common set of shared categories."}, {"heading": "2.2 Conceptual models", "text": "Each agent is equipped with the same number n of labels Li, with point prototypes Pi \u2208 \u2126, where \u2126 = [0, 1]3. At the start of the simulations the Pi are uniformly distributed around the space. Thresholds \u03b5i are also randomly initiated, and considered to be some multiple of a base threshold \u03b5. Each threshold \u03b5i \u223c U(0, bi), where again, the bi can be considered to be a multiple of some common b, and the bi are taken from U [0.5, 2]. The distance metric is Euclidean.\nEach agent therefore has a label set LA = {L1, L2, ...Ln}. These labels can be hedged to form a set LA+ = LA\u222a{very Li, quite Li : i = 1, ..., n}. Hedged concepts have the same prototype Pi as basic labels, but a scaled threshold v\u03b5i or q\u03b5i where v < 1 and q > 1. Agents can assert positive or negated, hedged or basic labels, giving an assertion set AS = {kLi,\u00ackLi : i = 1, ..., n; k = very, quite, basic}, where k = basic means that the label is not hedged."}, {"heading": "2.3 Assertion model", "text": "At each timestep, half the agents are designated speaker agents and make assertions, determined by the assertion model used.The assertion model is based on the probability of making a particular assertion \u03b8, given that the object being described is x. Following methods in [2, 8], we calculate the posterior probability of each \u03b8 \u2208 AS, given an element x \u2208 \u2126. The assertion made by a speaker agent is the assertion with the highest probability. The posterior probability of each \u03b8, given x, is determined by the appropriateness of the assertion \u03b8 to describe x, i.e. \u00b5\u03b8(x), and the prior probability P (\u03b8) of asserting \u03b8.\nWe first consider which sets of labels that are appropriate to describe x \u2208 \u2126. The probability that any particular set of labels F \u2286 LA are appropriate to describe x is given by a probability mass function mx : 2LA \u2192 [0, 1]. One way of determining mx is via the consonant selection function introduced in [8]. This states:\nDefinition 1 (Consonant selection function) Given non-zero appropriateness measures on basic labels \u00b5Li(x) : i = 1, ..., n ordered such that \u00b5Li(x) \u2265 \u00b5Li+1 for i = 1, ..., n, the consonant selection function identifies the mass function\nmx({L1, ..., Ln}) = \u00b5Ln(x) mx({L1, ..., Li}) = \u00b5Li(x)\u2212 \u00b5Li+1(x) for i = 1, ..n\u2212 1 mx(\u2205) = 1\u2212 \u00b5L1(x) mx(F ) = 0 if F 6= {L1, L2, ..., Lk} for some k \u2264 n\nBecause we have ordered the labels by \u00b5Li(x) \u2265 \u00b5Li+1 , if the label Li is appropriate to describe x, all labels Lj : j < i must also be appropriate to describe x. The quantity \u00b5Li(x) \u2212 \u00b5Li+1(x) corresponds to the idea that x in some sense lies between the thresholds \u03b5i+1 and \u03b5i, so that Li is appropriate to describe x, but Li+1 is not. We extend this definition to the case of hedged labels simply by considering all hedged labels as basic labels, explained in the example below.\nExample 2 (Determining the mass function) Suppose we are determining the mass function for subsets F \u2286 {kL1, kL2 : k = very, quite, basic}, given the point a \u2208 x1 \u00d7 x2, as illustrated in figure 7.\nSuppose that \u00b5quiteL2(a) = 0.9, \u00b5L2(a) = 0.7, \u00b5quiteL1(a) = 0.3, \u00b5veryL2(a) = 0.1, \u00b5L1(a) = 0, \u00b5veryL1(a) = 0, giving us the order \u00b5quiteL2(a) \u2265 \u00b5L2(a) \u2265 \u00b5quiteL1(a) \u2265 \u00b5veryL2(a) \u2265 \u00b5L1(a) \u2265 \u00b5veryL1(a). We may then assign probabilities to subsets of labels according to the consonant selection function:\nmx(F6) = mx({quite L2, L2, quite L1, very L2, L1, very L1}) = \u00b5veryL1(a) = 0 mx(F5) = mx({quite L2, L2, quite L1, very L2, L1}) = \u00b5L1(a)\u2212 \u00b5veryL1(a) = 0 mx(F4) = mx({quite L2, L2, quite L1, very L2}) = \u00b5veryL2(a)\u2212 \u00b5L1(a) = 0.1 mx(F3) = mx({quite L2, L2, quite L1}) = \u00b5quiteL1(a)\u2212 \u00b5veryL2(a) = 0.2 mx(F2) = mx({quite L2, L2}) = \u00b5L2(a)\u2212 \u00b5quiteL1(a) = 0.4 mx(F1) = mx({quite L2}) = \u00b5quiteL2(a)\u2212 \u00b5L2(a) = 0.2 mx(\u2205) = 1\u2212 \u00b5quiteL2(a) = 0.1\nHaving determined the probability mass function on sets of labels, a mass assignment on sets of assertions is then defined.\nDefinition 3 (Mass assignment on assertions) max : 2AS \u2192 [0, 1] is defined such that:\nmax(G) = \u2211\nF\u2286LA+:C(F )=G\nmx(F )\nwhere C (F ) = {\u03b8 \u2208 AS : F \u2208 \u03bb(\u03b8)}, and \u03bb(\u03b8) is defined recursively by\n\u03bb(kLi) = {F \u2286 LA+ : kLi \u2208 F} \u03bb(\u00ac\u03b8) = (\u03bb(\u03b8))c\n\u03bb(\u03b8 \u2227 \u03c6) = \u03bb(\u03b8) \u2229 \u03bb(\u03c6) \u03bb(\u03b8 \u2228 \u03c6) = \u03bb(\u03b8) \u222a \u03bb(\u03c6)\nThis definition has the implication that for Gi = Fi \u222a {\u00ackLj : kLj \u2208 LA+\\Fi}, max(Gi) = mx(Fi).\nThen the probability of an assertion \u03b8 being made, given that an object x is being described, can be calculated by summing over G \u2286 AS that contain \u03b8.\nDefinition 4 Given a prior distribution on AS, a posterior distribution given an object x can be calculated by:\nP (A = \u03b8|x) = \u2211\nG\u2286AS:\u03b8\u2208G\nmax(G)P (A = \u03b8|A \u2208 G)\n= P (\u03b8) \u2211\nG\u2286AS:\u03b8\u2208G\nmax(G)\nP (G)\nHere, P (G) = \u2211 \u03d5\u2208G P (\u03d5)\nThe value of P (\u03b8) for one particular label L is a product of two elements: the prior probability pp of making a positive assertion (or 1 \u2212 pp for a negated assertion); and the prior probability of making a hedged assertion, given by pv for making an assertion hedged with the word \u2018very\u2019, pq for making an assertion hedged with \u2018quite\u2019 or 1\u2212 pv \u2212 pq for making a basic assertion, summarised in table 1.\nThe prior probability of asserting any particular label Li \u2208 LA is uniform acrossLA. Hence the value ofP (\u03b8) calculated above should be divided by n, giving, for example,\nP (\u00acvL2) = pn \u2217 pv n\nExample 5 (Determining the posterior probability of assertion) Suppose, for an easy example, we want to calculate the probability of asserting \u2018very L1\u2019, given object a, as in example 2. We need to calculate\nP (A = \u2018very L1\u2019|a) = P (\u03b8) \u2211\nG\u2286AS:\u2018veryL1\u2019\u2208G\nmax(G)\nP (G)\nwhere Gi = Fi \u222a {\u00ackLj : kLj \u2208 LA+\\Fi}. However, the only subset Gi 3 \u2018very L1\u2019 is G6, so\nP (A = \u2018very L1\u2019|x) = P (\u2018very L1\u2019) max(G6)\nP (G6)\n= 0\nSuppose, for a more involved example, the label set LA+ is as in example 2, with pp = 0.7, pv = 0.7, pq = 0.2, and we want to determine P (A = quite L1|a). The prior probability P (quite L1) = 0.7\u22170.2\n2 = 0.07. So we have:\nP (A = quite L1|a) = 0.07 \u2211\nGi:quiteL1\u2208Gi\nmax(Gi)\nP (Gi)\n= 0.07( max(G6) P (G6) + max(G5) P (G5) + max(G4) P (G4) + max(G3) P (G3) ) = 0.07(0 + 0 + 0.1\u2211\n\u03d5\u2208G4 P (\u03d5) + 0.2\u2211 \u03d5\u2208G3 P (\u03d5) )\n= 0.07( 0.1\n0.54 +\n0.2 0.4 )\n= 0.048\nHaving calculated the probability of each assertion, the speaker agent makes the most probable assertion \u03b8 \u2208 AS."}, {"heading": "2.4 Updating algorithms", "text": "Once the speaker agent has made assertion \u03b8, the listener agent computes \u00b5\u03b8(x) based on its current label set. If \u00b5\u03b8(x) < w, where w is a parameter that can be thought of as the age of the speaker agent, the listener agent updates its label set LA by moving the prototype and/or changing the threshold of the concept, until \u00b5\u03b8(x) = w. Formulae for these updates are again based on [2]. A label defined by Pi and \u03b5i is updated to P \u2032i = Pi \u2212 \u03bb(x\u2212 Pi) and \u03b5\u2032i = \u03b1\u03b5i. Values for \u03bb and \u03b1 are sought, such that \u00b5\u2032\u03b8(x) = w.\n2.4.1 Case 1: \u03b8 = kLi\nRecall that \u03b5i \u223c U(0, bi), so that for x \u2208 \u2126,\n\u00b5kLi(x) = 1\u2212 ||x\u2212 Pi|| kbi < w by assumption.\nThe label Li is updated to L\u2032i, where P \u2032 i = Pi \u2212 \u03bb(x \u2212 Pi) and \u03b5\u2032i = \u03b1\u03b5i, such that \u00b5kL\u2032i(x) \u2265 w, and minimising the distance between the interpretations as measured by the Haussdorff distance between the two neighbourhoods,\nH (NLi ,NL\u2032i) = ||Pi \u2212 P \u2032 i ||+ |\u03b5i \u2212 \u03b5\u2032i| (1)\n= |\u03bb|||x\u2212 Pi||+ \u03b5bi b |1\u2212 \u03b1| (*)\nTo minimise the update, we set \u00b5kL\u2032i(x) = w, so:\nw = \u00b5kL\u2032i(x) = 1\u2212 ||x\u2212 P \u2032i || kb\u2032i = 1\u2212 |1\u2212 \u03bb|||x\u2212 Pi|| \u03b1kbi\nwhich gives\n\u03b1 = |1\u2212 \u03bb|||x\u2212 Pi||\n(1\u2212 w)kbi\n= (1\u2212 \u03bb)||x\u2212 Pi||\n(1\u2212 w)kbi since \u03bb = 1\u2192 P \u2032i = x\nTo updateLi we will always want \u03bb \u2265 0, \u03b1 \u2265 1, as we are dealing with a positive label.\nSubstituting \u03b1 into equation (*), we obtain\nH (NLi ,N \u2032 Li) = |\u03bb|||x\u2212 Pi||+ \u03b5bi b ( (1\u2212 |\u03bb|)||x\u2212 Pi|| (1\u2212 w)kbi \u2212 1)\n= |\u03bb|||x\u2212 Pi||(1\u2212 \u03b5 b(1\u2212 w)k ) + \u03b5||x\u2212 Pi|| b(1\u2212 w)k \u2212 \u03b5bi b\n(2)\nThen if 1 \u2212 \u03b5 b(1\u2212w)k > 0, i.e. \u03b5 < b(1 \u2212 w)k, the quantity (2)\ncan be minimised by setting \u03bb = 0 so \u03b1 = ||x\u2212Pi|| (1\u2212w)kbi . Otherwise, we have \u03b1 = 1, \u03bb = 1\u2212 (1\u2212w)kbi||x\u2212Pi|| . Since \u03b5 is a random variable, so is the choice between \u03bb and \u03b1. We therefore need a concrete updating rule. We update Pi and bi with the expected values of \u03bb and \u03b1 respectively. \u03b5 \u223c Uniform[0, b], so\nP (\u03b5 < b(1\u2212 w)k) = { (1\u2212 w)k if (1\u2212 w)k < 1 1 otherwise\nWe can therefore calculate\nE(\u03b1) =\n{ ||x\u2212Pi||\nbi + 1\u2212 (1\u2212 w)k if (1\u2212 w)k < 1\n||x\u2212Pi|| (1\u2212w)kbi\notherwise\nand\nE(\u03bb) = { (1\u2212 (1\u2212 w)k)(1\u2212 (1\u2212w)kbi||x\u2212Pi|| ) if (1\u2212 w)k < 1 0 otherwise"}, {"heading": "2.4.2 Case 2: \u03b8 = \u00ackLi", "text": "By an entirely similar argument, we obtain\nE(\u03b1) =\n{ ||x\u2212Pi||\nbi + 1\u2212 wk if wk < 1\n||x\u2212Pi|| wkbi\notherwise\nand\nE(\u03bb) = { (1\u2212 wk)(1\u2212 wkbi||x\u2212Pi|| ) if wq < 1 0 otherwise\nSo at each timestep, each listener agent, for whom \u00b5\u03b8(x) < w, updates the relevant label using the the quantities E(\u03b1), E(\u03bb)."}, {"heading": "2.5 Performance metrics", "text": "Performance metrics from [2] are used, measuring the Average Pairwise Distance between label sets (APD) and the Average Label Overlap (ALO). APD measures the difference in label sets in the community, and ALO indicates the extent to which an agent\u2019s concepts overlap. We seek low values for each metric.\nAPD is calculated using the Haussdorff distance between two neighbourhoods as given in equation 1. The difference between the label sets of any one pair of agents is given by\nIPD = n\u2211 i=1 H (N jLi ,N k Li)\nwhere n is the number of labels each agent has and j and k refer to distinct agents.\nThis is averaged over pairs of agents. There are N agents, therefore ( N 2 ) pairs, giving:\nAPD = 2\nN(N \u2212 1) N\u2211 k=j+1 N\u2211 j=1 IPDjk\nALO is the extent to which labels overlap. To calculate this, we take the maximum value of the intersection of a pair of labels, as measured by a min rule. We average this value over pairs of labels. The overlap within an individual\u2019s label set is therefore\nILO = 2\nn(n\u2212 1) n\u2211 j=i+1 n\u2211 i=1 max{min{\u00b5Li(x), \u00b5Lj (x) : x \u2208 \u2126}}\nAveraged across the population this is:\nALO = 1\nN N\u2211 k=1 ILOk\nwhere ILOk siginifies agent k\u2019s label overlap."}, {"heading": "2.6 Simulation process", "text": "Simulations with n = 100 agents were run for T = 104 timesteps. Agent weights w \u2208 [0.2, 0.8] were updated at each timestep in increments of 1/T . Whenw \u2265 0.8, agents are reborn with randomised labels and w = 0.2. 20 simulations are run for each reported combination of parameters.\n[2] show that if pp \u2208 [0.5, 0.6] then performance of the system changes from low ALO and high APD to vice versa at approximately pp = 0.56. We ran simulations in a slightly extended range for comparison, varying the prior probabilities pv, pb and pq of asserting the different hedges \u2018very\u2019, \u2018basic\u2019, and \u2018quite\u2019. We present results from three sets of parameters. As a baseline we run simulations with no hedges, i.e. pv = 0, pb = 1, pq = 0. To investigate the effects of using contraction hedges, we run simulations with parameters pv = 0.7, pb = 0.2, pq = 0.1. For expansion hedges, we use parameters pv = 0.1, pb = 0.2, pq = 0.7."}, {"heading": "3 RESULTS", "text": "The results presented show performance against the two metrics after 104 simulation timesteps. By this point, the population has generally reached a steady state in which performance does not greatly change.\nFigure 8 shows the steady state of APD achieved after 104 timesteps for a range of values pp \u2208 [0.4, 0.6]. Three sets of results are presented: results using unhedged assertions; results with a high prior probability of using contraction hedges; and results from simulations with a high prior probability of asserting expansion hedges, where these prior probabilities are as stated in 2.6.\nA high prior of asserting contracted labels reduces minimum APD achieved from 0.38 when pp = 0.56 or pp = 0.6 to 0.29 when pp = 0.57 (figure 8). Performing a paired t-test across the 20 simulations gives the mean difference between these values as 0.097. This difference is statistically significant with p < 0.001 and with 95% confidence interval [0.084, 0.110]. The median and range of results are given in figure 9. At pp = 0.57, ALO decreases, from 0.92 to 0.89 (figure 11). The mean value of this difference across the 20 simulations is 0.032. Again, this is statistically significant with p < 0.001 and 95% confidence interval of [0.029, 0.035], further illustrated in figure 10. These results imply that a high prior probability of asserting contraction hedges enables us to improve convergence between agents\u2019 label sets as well as reducing overlap within label sets slightly.\nWith a high prior probability of asserting expanded labels, lower values of ALO can be achieved when the probability of asserting positive labels is 0.45 , decreasing to 0.02 compared to 0.1, figure 11. The mean difference between these values across the 20 simulations is 0.083, which is statistically significant with p < 0.001 and a 95% confidence interval of [0.078, 0.089]. The data is represented in figure 13. At this value of pp, APD achieved is 0.73 compared to 0.85 for unhedged assertions, figure 8. The mean value of this difference across the 20 simulations is 0.12. This figure is statistically significant with p < 0.001 and 95% confidence interval [0.116, 0.126]. The data is again represented in figure 12. A high prior probability of asserting expansion hedges therefore enables minimal overlap to be maintained at low pp whilst improving convergence.\nWe can also examine how fast the community of agents arrives at a\nsteady state. Figure 14 shows that at short timescales (t < 2000), better convergence may be achieved allowing only unhedged assertions. In a more extreme case, figure 15 shows that for pp = 0.5, better performance on the ALO metric is only achieved after 7500 timesteps. Although this improvement takes a longer time to achieve, it goes together with improved performance on APD which is achieved in a similar timescale to the unhedged model 16."}, {"heading": "4 DISCUSSION", "text": "These results show that, in a model of language development across a population, hedged assertions can improve both the level of convergence to shared language as measured by average pairwise difference between label sets (APD) and, to an extent, the discriminatory power of individuals\u2019 label sets, as measured by average label\noverlap (ALO). The two different types of hedges improve performance in distinct ways. If overall convergence is important, a high prior probability of asserting contraction hedges should be used to improve performance on the APD metric. Conversely, if the ability of the agents to discriminate precisely between objects in the environment is more important, then expansion hedges, together with lower probabilities of asserting positive labels, should be used to maintain low levels of ALO whilst still improving performance on APD.\nThe improved performance against the two metrics is tempered by the fact that the speed at which the steady state is achieved is somewhat slower than when using simply unhedged assertions. However, the improvement in APD is seen relatively quickly at pp = 0.56, soon after the unhedged model has reached its steady state. The improvement in ALO when using expansion hedges, for pp = 0.5, does not occur until after 7, 500 timesteps, well after the unhedged model has reached its steady state. However, the improvement in perfor-\nmance on ALO goes together with improved performance on APD which is attained at the same speed as the in the unhedged model.\nIf the speed of development of shared categories is not important, the two types of hedges would be useful in different types of situation, depending whether convergence or discriminatory power is more important. This might be dependent on, for example, the structure of the underlying environment. In the current simulation, objects are presented uniformly across the space. If objects were distributed non-uniformly, perhaps clumping in various regions of the space, then perhaps the ability to discriminate precisely between different labels would be less important, since the environment provides that distinction naturally. Convergence to shared labels would then be more important.\nIf speed is important, using contraction hedges can still improve levels of convergence in a relatively short timeframe.\nThere are many parameters in the simulation that bear further investigation. The distribution of objects in the environment, as mentioned above, is likely to have an effect on performance again the two metrics. In the current simulations, hedge values of v = 0.5 and h = 2 are used. Increasing and decreasing these values could have an impact on performance, as would, perhaps, allowing agents to have difference values of v and h. The range of w allowed also affects performance. When w = [0.01, 0.99], agents no longer achieve high levels of convergence at pp > 0.55 (results not shown). Other weight ranges may positively affect performance, however."}, {"heading": "5 CONCLUSIONS", "text": "We have investigated the utility of hedged assertions in the development of a shared language, and shown that allowing agents to make hedged assertions improves the ability to develop common categories\nin two distinct ways. Firstly, using contraction hedges, i.e. words like \u2018very\u2019, allows improved levels of convergence to shared categories, whilst slightly improving the extent to which labels overlap. Secondly, using expansion hedges, or words like \u2018quite\u2019, enables the development of label sets that are more discriminatory of the environment and also have better levels of convergence. However, both these improvements come with a slower speed of development of shared labels. It may be possible to improve these speeds by tuning other parameters such as the age range of agents or the values of hedges used."}, {"heading": "ACKNOWLEDGEMENTS", "text": "Martha Lewis gratefully acknowledges support from EPSRC Grant No. EP/E501214/1"}], "references": [{"title": "Adjusting the core and/or the support of a fuzzy set-a new approach to fuzzy modifiers", "author": ["P. Bosc", "D. Dubois", "A. HadjAli", "O. Pivert", "H. Prade"], "venue": "Fuzzy Systems Conference, 2007. FUZZ-IEEE 2007. IEEE International, pp. 1\u20136. IEEE, ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Language games with vague categories and negations", "author": ["Henrietta Eyre", "Jonathan Lawry"], "venue": "Adaptive Behavior,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Conceptual spaces: The geometry of thought", "author": ["P. G\u00e4rdenfors"], "venue": "The MIT Press", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Inheritance of attributes in natural concept conjunctions", "author": ["J. Hampton"], "venue": "Memory & Cognition, 15(1), 55\u201371, ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1987}, {"title": "Conceptual combinations and fuzzy logic", "author": ["J. Hampton"], "venue": "Concepts and Fuzzy Logic, eds., R. Belohlavek and G. J. Klir, The MIT Press, ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Prototype theory and compositionality", "author": ["H. Kamp", "B. Partee"], "venue": "Cognition, 57(2), 129\u2013191, ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "A framework for linguistic modelling", "author": ["J. Lawry"], "venue": "Artificial Intelligence, 155(1-2), 1\u201339, ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Uncertainty modelling for vague concepts: A prototype theory approach", "author": ["J. Lawry", "Y. Tang"], "venue": "Artificial Intelligence, 173(18), 1539\u2013 1558, ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "A label semantics approach to linguistic hedges", "author": ["Martha Lewis", "Jonathan Lawry"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "On the adequacy of prototype theory as a theory of concepts", "author": ["D.N. Osherson", "E.E. Smith"], "venue": "Cognition, 9(1), 35\u201358, ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1981}, {"title": "Cognitive representations of semantic categories.", "author": ["E. Rosch"], "venue": "Journal of Experimental Psychology: General,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1975}, {"title": "Conceptual combination with prototype concepts", "author": ["E.E. Smith", "D.N. Osherson"], "venue": "Cognitive Science, 8(4), 337\u2013361, ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1984}, {"title": "Why we need evolutionary semantics", "author": ["Luc Steels"], "venue": "KI 2011: Advances in Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Coordinating perceptually grounded categories through language: A case study for colour", "author": ["Luc Steels", "Tony Belpaeme"], "venue": "Behavioral and brain sciences,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Fuzzy sets", "author": ["L.A. Zadeh"], "venue": "Information and Control, 8(3), 338\u2013353, ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1965}, {"title": "A fuzzy-set-theoretic interpretation of linguistic hedges", "author": ["L.A. Zadeh"], "venue": "Journal of Cybernetics, ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1972}], "referenceMentions": [{"referenceID": 12, "context": "1 INTRODUCTION An evolutionary approach to semantics enables the development in robots and autonomous agents of flexible, mutable concepts that could be learnt through interaction and can change over time [13].", "startOffset": 205, "endOffset": 209}, {"referenceID": 1, "context": "This approach is investigated by Eyre and Lawry in [2], in which they develop a model of language evolution based in the label semantics framework.", "startOffset": 51, "endOffset": 54}, {"referenceID": 6, "context": "1 A representation model for concepts We model concepts within the label semantics framework [7, 8], combined with prototype theory [11] and the conceptual spaces model of concepts [3].", "startOffset": 93, "endOffset": 99}, {"referenceID": 7, "context": "1 A representation model for concepts We model concepts within the label semantics framework [7, 8], combined with prototype theory [11] and the conceptual spaces model of concepts [3].", "startOffset": 93, "endOffset": 99}, {"referenceID": 10, "context": "1 A representation model for concepts We model concepts within the label semantics framework [7, 8], combined with prototype theory [11] and the conceptual spaces model of concepts [3].", "startOffset": 132, "endOffset": 136}, {"referenceID": 2, "context": "1 A representation model for concepts We model concepts within the label semantics framework [7, 8], combined with prototype theory [11] and the conceptual spaces model of concepts [3].", "startOffset": 181, "endOffset": 184}, {"referenceID": 10, "context": "This approach is based on experimental results where human subjects were found to view membership in a concept as a matter of degree, with some objects having higher membership than others [11].", "startOffset": 189, "endOffset": 193}, {"referenceID": 14, "context": "Fuzzy set theory [15], in which an object x has a graded membership \u03bcL(x) in a concept L, was proposed as a formalism for prototype theory.", "startOffset": 17, "endOffset": 21}, {"referenceID": 9, "context": "However, numerous objections to its suitability have been made [10, 12, 6, 5, 4].", "startOffset": 63, "endOffset": 80}, {"referenceID": 11, "context": "However, numerous objections to its suitability have been made [10, 12, 6, 5, 4].", "startOffset": 63, "endOffset": 80}, {"referenceID": 5, "context": "However, numerous objections to its suitability have been made [10, 12, 6, 5, 4].", "startOffset": 63, "endOffset": 80}, {"referenceID": 4, "context": "However, numerous objections to its suitability have been made [10, 12, 6, 5, 4].", "startOffset": 63, "endOffset": 80}, {"referenceID": 3, "context": "However, numerous objections to its suitability have been made [10, 12, 6, 5, 4].", "startOffset": 63, "endOffset": 80}, {"referenceID": 6, "context": "Label semantics [7] is a random set approach to concepts which quantifies an agent\u2019s uncertainty about the extent of application of a concept.", "startOffset": 16, "endOffset": 19}, {"referenceID": 7, "context": "We refer to this as subjective uncertainty [8] to emphasise that it concerns the definition of concepts and categories, in contrast to stochastic uncertainty which concerns the state of the world.", "startOffset": 43, "endOffset": 46}, {"referenceID": 7, "context": "Lawry and Tang [8] combine the label semantics approach with conceptual spaces and prototype theory, to give a formalisation of concepts as based on a prototype and a threshold, located in a conceptual space.", "startOffset": 15, "endOffset": 18}, {"referenceID": 14, "context": "This appropriateness measure is similar to Zadeh\u2019s description of fuzzy membership in a concept [15].", "startOffset": 96, "endOffset": 100}, {"referenceID": 15, "context": "Zadeh [16] uses operations of concentration and dilation to render these ideas.", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "However, we argue, as do [1], that Zadeh\u2019s formulae are, to an extent, arbitrary, since the notion of taking a power of a membership value does not correspond to anything that language users might do.", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "As with [1], we take a semantic approach.", "startOffset": 8, "endOffset": 11}, {"referenceID": 8, "context": "In [9], we propose that a concept \u2018very L\u2019 or \u2018quite L\u2019 be rendered by considering that the prototype of \u2018very/quite L\u2019 is equal to that of the base concept L, but that the threshold of the hedged concept \u2018very/quite L\u2019 is respectively smaller or larger than that of the base concept.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "We implement this model in a version of the multi-agent simulation created in [2] in order to investigate how the use of these hedges in a model of language helps", "startOffset": 78, "endOffset": 81}, {"referenceID": 13, "context": "To investigate the utility of hedged assertions we implement a multiagent simulation of a version of the category game [14], following [2], in which shared categories develop over time as a result of the interactions of the category users.", "startOffset": 119, "endOffset": 123}, {"referenceID": 1, "context": "To investigate the utility of hedged assertions we implement a multiagent simulation of a version of the category game [14], following [2], in which shared categories develop over time as a result of the interactions of the category users.", "startOffset": 135, "endOffset": 138}, {"referenceID": 0, "context": "Each agent is equipped with the same number n of labels Li, with point prototypes Pi \u2208 \u03a9, where \u03a9 = [0, 1].", "startOffset": 100, "endOffset": 106}, {"referenceID": 1, "context": "Following methods in [2, 8], we calculate the posterior probability of each \u03b8 \u2208 AS, given an element x \u2208 \u03a9.", "startOffset": 21, "endOffset": 27}, {"referenceID": 7, "context": "Following methods in [2, 8], we calculate the posterior probability of each \u03b8 \u2208 AS, given an element x \u2208 \u03a9.", "startOffset": 21, "endOffset": 27}, {"referenceID": 0, "context": "The probability that any particular set of labels F \u2286 LA are appropriate to describe x is given by a probability mass function mx : 2 \u2192 [0, 1].", "startOffset": 136, "endOffset": 142}, {"referenceID": 7, "context": "One way of determining mx is via the consonant selection function introduced in [8].", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "Definition 3 (Mass assignment on assertions) max : 2 \u2192 [0, 1] is defined such that:", "startOffset": 55, "endOffset": 61}, {"referenceID": 1, "context": "Formulae for these updates are again based on [2].", "startOffset": 46, "endOffset": 49}, {"referenceID": 1, "context": "5 Performance metrics Performance metrics from [2] are used, measuring the Average Pairwise Distance between label sets (APD) and the Average Label Overlap (ALO).", "startOffset": 47, "endOffset": 50}, {"referenceID": 1, "context": "[2] show that if pp \u2208 [0.", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "We investigate the emergence of shared concepts in a community of language users using a multi-agent simulation. We extend results showing that negated assertions are of use in developing shared categories, to include assertions modified by linguistic hedges. Results show that using hedged assertions positively affects the emergence of shared categories in two distinct ways. Firstly, using contraction hedges like \u2018very\u2019 gives better convergence over time. Secondly, using expansion hedges such as \u2018quite\u2019 reduces concept overlap. However, both these improvements come at a cost of slower speed of development.", "creator": "LaTeX with hyperref package"}}}