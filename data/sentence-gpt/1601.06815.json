{"id": "1601.06815", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2016", "title": "Very Efficient Training of Convolutional Neural Networks using Fast Fourier Transform and Overlap-and-Add", "abstract": "Convolutional neural networks (CNNs) are currently state-of-the-art for various classification tasks, but are computationally expensive. Propagating through the convolutional layers is very slow, as each kernel in each layer must sequentially calculate many dot products for a single forward and backward propagation which equates to $\\mathcal{O}(N^{2}n^{2})$ per kernel per layer where the inputs are $N \\times N$ arrays and the kernels are $n \\times n$ arrays. Convolution can be efficiently performed as a Hadamard product in the frequency domain. The bottleneck is the transformation which has a cost of $\\mathcal{O}(N^{2}\\log_2 N)$ using the fast Fourier transform (FFT). However, the increase in efficiency is less significant when $N\\gg n$ as is the case in CNNs. We mitigate this by using the \"overlap-and-add\" technique reducing the computational complexity to $\\mathcal{O}(N^2\\log_2 n)$ per kernel. This method increases the algorithm's efficiency in both the forward and backward propagation, reducing the training and testing time for CNNs. Our empirical results show our method reduces computational time by a factor of up to 16.3 times the traditional convolution implementation for a 8 $\\times$ 8 kernel and a 224 $\\times$ 224 image.\n\n\nFigure 2\nFigure 3\nIn this example, we run a task where the input is $N \\times N$ arrays, which are uniformly distributed from each image to one image. We find an additional advantage with our approach in minimizing the computational complexity to $\\mathcal{O}(N^{2}\\log_2 N)$ and using the slow Fourier transform (FFT). In this case, we run a task where the input is $N \\times N$ arrays, which are uniformly distributed from each image to one image. We find an additional advantage with our approach in minimizing the computational complexity to $\\mathcal{O}(N^{2}\\log_2 N)$ and using the slow Fourier transform (FFT). In this case, we run a task where the input is $N \\times N$ arrays, which are uniformly distributed from each image to one image. We find an additional advantage with our approach in minimizing the computational complexity to $mul-the-solve-out-of-art\nFigure 4\n", "histories": [["v1", "Mon, 25 Jan 2016 21:29:11 GMT  (1114kb,D)", "http://arxiv.org/abs/1601.06815v1", "British Machine Vision Conference 2015"]], "COMMENTS": "British Machine Vision Conference 2015", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["tyler highlander", "andres rodriguez"], "accepted": false, "id": "1601.06815"}, "pdf": {"name": "1601.06815.pdf", "metadata": {"source": "CRF", "title": "HIGHLANDER, RODRIGUEZ: EFFICIENT TRAINING OF CNNS USING FFT AND OAA 1 Very Efficient Training of Convolutional Neural Networks using Fast Fourier Transform and Overlap-and-Add", "authors": ["Tyler Highlander", "Andres Rodriguez"], "emails": ["highlander.2@wright.edu", "andres.rodriguez.8@us.af.mil"], "sections": [{"heading": "1 Introduction", "text": "Convolutional neural networks (CNNs) achieved state-of-the-art classification rates on various datasets [1, 4, 7], but require significant computational resources. For example, AlexNet [5] has over 60 million free parameters trained with stochastic gradient descent requiring thousands of forward and backward propagations through a network with 5 convolutional layers. A more recent CNN, GoogLeNet [12], has various layers within layers amounting to 59 convolutional layers. Propagating through these convolutional layers is the computational bottleneck of training and testing CNNs. Standard convolutional layers are slow, as each kernel must calculate many dot products for a single forward and backward propagation which equates to O(N2n2) per kernel, where the inputs are N\u00d7N arrays and the kernels are n\u00d7 n arrays. To converge to a local minimum, CNNs usually require hundreds of epochs. An epoch consists of propagating all the training samples in the dataset through the network once. In addition, it is common to train multiple CNNs for one task and compute an average\nc\u00a9 2015. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.\nar X\niv :1\n60 1.\n06 81\n5v 1\nof the multiple outputs in testing. With over one million training images in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) [1] and hundreds of epochs needed for training each CNN, reducing the complexity of the convolution operation reduces training time.\nConvolution can be efficiently computed in the frequency domain as a Hadamard product. The computational bottleneck is the Fourier transform between the space and the frequency domain. For an input of size N2, this can be efficiently computed using fast Fourier transforms (FFTs) with complexity O(N2 log2 N). Mathieu et al. [8] demonstrated that this reduces the training and testing time of CNNs. In their work they efficiently calculated FFTs on a GPU and used these transforms to perform convolutions via a Hadamard product in the frequency domain.\nIn this paper, we propose to use the overlap-and-add (OaA) technique [10] to further reduce the training and testing complexity to O(N2 log2 n) per kernel. Note that the overlapand-save [10] is a similar technique that may be marginally faster but has the same complexity. In a CNN convolutional layer, the input array and the set of K kernel arrays have a depth of size C, e.g., C = 3 for an RBG input image. The total number of convolutions in a CNN convolutional layer is KC; each channel of each kernel is convolved with the respective channel in the input array. For each of these convolutions, OaA should be used to improve efficiency with no cost in performance. Section 2 explains the OaA technique and our convolution implementation. In Section 3 we demonstrate that our method computationally outperforms (by a factor of up to 16.3) traditional implementations. We offer concluding remarks in Section 4."}, {"heading": "2 Overlap-and-Add", "text": "In OaA, the input is broken into N2/n2 (rounded up) blocks equal to the kernel size n\u00d7n. A convolution between each block and the kernel is computed and the results are overlapped and added. Figure 1 illustrates a simple 1-D overlap-and-add method for spacial convolution (that can easily generalize to 2-D). The input array is first split into smaller blocks that are the size of the kernel. Smaller convolutions are computed between the kernel and the block inputs. The resulting convolutions are overlapped by n\u2212 1, where n is the length of the kernel, and added together to create the same results as a traditional spacial convolution.\nFigure 1: 1-D Overlap-and-Add convolutions\nEach convolution in OaA can be efficiently computed in the frequency domain, where the bottleneck is the complexity of each 2-D fast Fourier transform O(n2 log2 n). The total complexity for the entire input and kernel is the number of blocks times the complexity of each block convolution, i.e.,O(N2 log2 n). Table 1 compares the complexity of each method, where spaceConv refers to the traditional convolution in the space domain, FFTconv refers to convolution via a Hadamard product in the frequency domain without overlap-and-add, and OaAconv refers to convolution using overlap-and-save where each smaller convolution is efficiently computed in the frequency domain.\nWhen N n as is the common case in CNN architectures, OaAconv reduces the computational complexity by a factor of n 2\nlog2 n over spaceConv and by a factor of logNlogn over FFTconv.\nFor example, for a 256\u00d7 256 input array with a 5\u00d7 5 kernel (typical values in a CNN architecture), spaceConv has a complexity of O(2562\u00d7 25), FFTconv has a complexity of O(2562\u00d78), and OaAconv has a complexity of O(2562\u00d72.3).\nThe overall time complexity of OaAconv can be further reduced by noting that all the block convolutions can be computed in parallel. If N2/n2 threads are available (a fair assumption for modern GPUs), the complexity on each thread is n2 log2 n, and the overall time complexity is O(max(N2,n2 log2 n)) which is usually O(N2). We can get additional speed up by taking advantage of the NVIDIA CUDA Fast Fourier Transform library (cuFFT) that computes each individual FFTs up to 10 times faster [9] (see also [13]). However, in order to have a fair comparison, our experiments in this paper are run on single threads. As part of this work, we created a Caffe [3] fork1 that uses a multi-thread GPU implementation of OaA for efficient convolutions.\nA possible area of concern for OaA is the additional cost of breaking the input into blocks and the overlapping and adding cost after each convolution. Our experiments show that the performance increase of the OaA technique outweigh these overhead costs.\nIt is worth noting that although OaA always reduces the computational complexity in testing (and training), it is particularly beneficial in implementations such as Sermanet et al. [11] when the test image is much larger than the training images, and uses a sliding window approach across a pyramid of scales for simultaneous detection (localization) and classification."}, {"heading": "3 Experiments and Results", "text": ""}, {"heading": "3.1 Training consistency", "text": "In this experiment we use each method of convolutions to train the CNN LeNet-5 [6] architecture using the MNIST [7] dataset. The goal of this experiment is to show empirically that the methods are equivalent. Each network is trained for only 100 epochs as all we need to\n1github.com/THighlander\nshow is consistency. Each CNN network is trained five times with each type of convolution technique, and their classification rate averages are shown in Table 2.\nAs expected, all three methods averaged within 0.07% of each other. The reason there is a non-zero difference is the random parameter initialization in training each CNN."}, {"heading": "3.2 Time vs. number of kernels", "text": "In this experiment we compare the required total propagation time through one convolutional layer as the number of kernels in the layer increases. To compare computational time, note that additional channels in the CNN convolutional layer input array can be treated as a multiplicative factor in the number of kernels, i.e., the computations required to convolve a C-channels input array with a set of K-channels kernels is equivalent to convolving a 1- channel input array with a set of 1-channel KC kernels. In our experiments, the input array is of size 32 \u00d7 32 and each kernel is of size 5 \u00d7 5, both with 1-channel. Figure 2 shows the speed-up factor of FFTconv and OaAconv compared to spaceConv in the forward propagation of the convolutional layer as the number of kernels varies. The number of kernels is varied from 25 to 750 with a discrete step of 25. Each \u201cnumber of kernels\u201d experiment is repeated 10 times and the results are averaged.\nFigure 2: Speed-up over spaceConv vs. number of kernels in forward propagation\nFFTconv and OaAconv outperform spaceConv, and OaAconv outperforms FFTconv at every step. FFTconv and OaAconv have an additional initialization cost: in OaA the input array must be divided, and in both methods the input (or divided input) array and kernel must be zero-padded, so that each side is N+n\u22121 in FFTconv and 2n\u22121 in OaAconv, prior to computing the Fourier transforms. As the number of kernels increases, these additional initialization costs becomes less significant.\nTo quantify the initialization cost of FFTconv and OaAconv, we convolved an input array of size 224 \u00d7 224 with a kernel of size 8 \u00d7 8. This experiment is repeated 10 times and averaged. OaAconv spends 1.6% of its total calculations on handling the overhead, while FFTconv spends 8.2%. In our implementation spaceConv does not require any overhead. The large difference between the input size and kernel size causes FFTconv to have a larger overhead than OaAconv.\nFigure 3 shows the speed-up over spaceConv vs. number of kernels for the backward propagation. Our method outperforms FFTconv more than in the forward propagation. The reason for this is that the backward propagation contains two actual convolutions per kernel: one convolution to propagate the error through the layer and another to calculate the change in weight generated by this error.\nFigure 3: Speed-up over spaceConv vs. number of kernels in backward propagation\nThese experiments show that the additional initialization costs of using OaAconv and FFTconv are mitigated by the lower complexity of these methods. The more kernels used, the larger the performance increase of our method."}, {"heading": "3.3 Time vs. kernel size", "text": "In this experiment we vary the size of the kernel while keeping the input size constant. The size of the input array is 64 \u00d7 64. The number of kernels used is held constant at 100. Figures 4 and 5 show the speed-up over spaceConv vs. kernel size for the forward and backward propagation, respectively. The kernel sizes vary from 1 to 64 with a discrete step of 1. Each \u201ckernel size\u201d experiment is repeated 10 times and the results are averaged.\nFigure 4: Speed-up over spaceConv vs. kernel size in forward propagation\nFigure 5: Speed-up over spaceConv vs. kernel size in backward propagation\nIn the forward propagation in Figure 4, the performance of FFTconv and OaAconv converge at 64 as expected. An interesting aspect of this graph is the various performance peaks at different kernel sizes. This is because the FFT software [2] is optimal for Fourier transforms with power of 2 sides along each dimension. We can leverage this fact when designing CNN architectures to further reduce computational requirements, and/or we can zero-pad to the nearest power of 2 to complement the design of the FFT algorithm. Note that the backward propagation has peak performances at different kernel sizes due to different zero-padding sizes prior to the Fourier transform."}, {"heading": "3.4 Time vs. input size", "text": "In this experiment we test the performance of multiple input sizes while holding the kernel size constant at 5 \u00d7 5. The input sizes varied from 4\u00d7 4 to 256\u00d7 256 with a discrete step of 4\u00d7 4 for the forward propagation experiment and a discrete step of 8\u00d7 8 for the backward propagation experiment. Different discrete steps are used to highlight the FFT algorithm\u2019s best performing size transforms for each propagation. Each \u201cinput size\u201d experiment is repeated 10 times and the results are averaged. Figures 6 and 7 show the speed-up over spaceConv vs. input size for the forward and backward propagation, respectively.\nFigure 6: Speed-up over spaceConv vs. input size in forward propagation\nFor inputs larger than 8\u00d78 input array sizes (the typical scenario for CNN architectures), OaAconv always outperforms the other methods. The speed-up in the backwards propagation is more significant. The error propagation is enhanced with OaAconv as the kernels are small.\n."}, {"heading": "4 Conclusion", "text": "In this paper we demonstrated that OaAconv can improve the efficiency of CNNs over traditional convolution and convolution via a Hadamard product in the frequency domain. Even though OaAconv must calculate more transforms than FFTconv, the fact that the transforms are smaller outweighs the cost of having to calculate more. In order to have fair comparisons we conducted our experiments on single threads. In practice OaA should be implemented such that each block convolution is computed in parallel, and each FFT is implemented with a GPU FFT library, e.g., cuFFT [9] to achieve maximum performance.\nIn future work we plan to optimize the FFT implementations for small size transforms, and design a CNN entirely in the frequency domain eliminating the bottleneck of the transforms. By creating a CNN that is in the frequency domain, the convolutional layers would only be the Hadamard products. The current challenge to this is to efficiently map an approximation of the non-linear transforms of CNNs (e.g., rectified linear units, sigmoid function, and/or hyperbolic tangent) to the frequency domain."}, {"heading": "Acknowledgements", "text": "We would like to thank Prof. Mateen Rizki of Wright State University for the helpful discussions, and the Air Force Office of Scientific Research (AFOSR) for providing the main funding for this work through LRIR 14Y06COR. Approved for public release, case number: 88ABW-2015-3481"}], "references": [{"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Fftw: An adaptive software architecture for the FFT", "author": ["Matteo Frigo", "Steven G Johnson"], "venue": "In Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": "Computer Science Department,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Yann LeCun", "Yoshua Bengio"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Mnist handwritten digit database", "author": ["Yann LeCun", "Corinna Cortes"], "venue": "AT&T Labs [Online]. http://yann. lecun. com/exdb/mnist,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Fast training of convolutional networks through FFTs", "author": ["Michael Mathieu", "Mikael Henaff", "Yann LeCun"], "venue": "International Conference on Learning Representations,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Cufft library", "author": ["CUDA Nvidia"], "venue": "http://docs.nvidia.com/cuda/cufft", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Discrete-time signal processing, volume 2. Prentice-hall", "author": ["Alan V Oppenheim", "Ronald W Schafer", "John R Buck"], "venue": "Englewood Cliffs,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1989}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Pierre Sermanet", "David Eigen", "Xiang Zhang", "Michael Mathieu", "Rob Fergus", "Yann LeCun"], "venue": "In International Conference on Learning Representations. CBLS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Fast convolutional nets with fbfft: A gpu performance evaluation", "author": ["Nicolas Vasilache", "Jeff Johnson", "Michael Mathieu", "Soumith Chintala", "Serkan Piantino", "Yann LeCun"], "venue": "International Conference on Learning Representations,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Convolutional neural networks (CNNs) achieved state-of-the-art classification rates on various datasets [1, 4, 7], but require significant computational resources.", "startOffset": 104, "endOffset": 113}, {"referenceID": 3, "context": "Convolutional neural networks (CNNs) achieved state-of-the-art classification rates on various datasets [1, 4, 7], but require significant computational resources.", "startOffset": 104, "endOffset": 113}, {"referenceID": 6, "context": "Convolutional neural networks (CNNs) achieved state-of-the-art classification rates on various datasets [1, 4, 7], but require significant computational resources.", "startOffset": 104, "endOffset": 113}, {"referenceID": 4, "context": "For example, AlexNet [5] has over 60 million free parameters trained with stochastic gradient descent requiring thousands of forward and backward propagations through a network with 5 convolutional layers.", "startOffset": 21, "endOffset": 24}, {"referenceID": 11, "context": "A more recent CNN, GoogLeNet [12], has various layers within layers amounting to 59 convolutional layers.", "startOffset": 29, "endOffset": 33}, {"referenceID": 0, "context": "With over one million training images in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) [1] and hundreds of epochs needed for training each CNN, reducing the complexity of the convolution operation reduces training time.", "startOffset": 104, "endOffset": 107}, {"referenceID": 7, "context": "[8] demonstrated that this reduces the training and testing time of CNNs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "In this paper, we propose to use the overlap-and-add (OaA) technique [10] to further reduce the training and testing complexity to O(N2 log2 n) per kernel.", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": "Note that the overlapand-save [10] is a similar technique that may be marginally faster but has the same complexity.", "startOffset": 30, "endOffset": 34}, {"referenceID": 8, "context": "We can get additional speed up by taking advantage of the NVIDIA CUDA Fast Fourier Transform library (cuFFT) that computes each individual FFTs up to 10 times faster [9] (see also [13]).", "startOffset": 166, "endOffset": 169}, {"referenceID": 12, "context": "We can get additional speed up by taking advantage of the NVIDIA CUDA Fast Fourier Transform library (cuFFT) that computes each individual FFTs up to 10 times faster [9] (see also [13]).", "startOffset": 180, "endOffset": 184}, {"referenceID": 2, "context": "As part of this work, we created a Caffe [3] fork1 that uses a multi-thread GPU implementation of OaA for efficient convolutions.", "startOffset": 41, "endOffset": 44}, {"referenceID": 10, "context": "[11] when the test image is much larger than the training images, and uses a sliding window approach across a pyramid of scales for simultaneous detection (localization) and classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "In this experiment we use each method of convolutions to train the CNN LeNet-5 [6] architecture using the MNIST [7] dataset.", "startOffset": 79, "endOffset": 82}, {"referenceID": 6, "context": "In this experiment we use each method of convolutions to train the CNN LeNet-5 [6] architecture using the MNIST [7] dataset.", "startOffset": 112, "endOffset": 115}, {"referenceID": 1, "context": "This is because the FFT software [2] is optimal for Fourier transforms with power of 2 sides along each dimension.", "startOffset": 33, "endOffset": 36}, {"referenceID": 8, "context": ", cuFFT [9] to achieve maximum performance.", "startOffset": 8, "endOffset": 11}], "year": 2016, "abstractText": "Convolutional neural networks (CNNs) are currently state-of-the-art for various classification tasks, but are computationally expensive. Propagating through the convolutional layers is very slow, as each kernel in each layer must sequentially calculate many dot products for a single forward and backward propagation which equates to O(N2n2) per kernel per layer where the inputs are N\u00d7N arrays and the kernels are n\u00d7 n arrays. Convolution can be efficiently performed as a Hadamard product in the frequency domain. The bottleneck is the transformation which has a cost of O(N2 log2 N) using the fast Fourier transform (FFT). However, the increase in efficiency is less significant when N n as is the case in CNNs. We mitigate this by using the \u201coverlap-and-add\u201d technique reducing the computational complexity to O(N2 log2 n) per kernel. This method increases the algorithm\u2019s efficiency in both the forward and backward propagation, reducing the training and testing time for CNNs. Our empirical results show our method reduces computational time by a factor of up to 16.3 times the traditional convolution implementation for a 8 \u00d7 8 kernel and a 224 \u00d7 224 image.", "creator": "LaTeX with hyperref package"}}}