{"id": "1601.08188", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jan-2016", "title": "Lipreading with Long Short-Term Memory", "abstract": "Lipreading, i.e. speech recognition from visual-only recordings of a speaker's face, can be achieved with a processing pipeline based solely on neural networks, yielding significantly better accuracy than conventional methods of processing.\n\n\n\nIn the current study, researchers in the field of neurobiological information processing were recruited from two different types of researchers to analyze the neural network of speech recognition from a participant's face and face in two different groups: in vivo (M), in vivo (M), in vitro (F), and in vivo (N). The data were used to develop a model that could compare the accuracy and accuracy of speech recognition to the brain's internal voice recognition system and in vivo (H). The study was supported by the National Institute for Brain Science, the University of Georgia at Atlanta (US Department of Energy), the University of Minnesota, and the United States Agency for Research on Cancer and Prevention and the National Institutes of Health (NIH). The study also identified a significant, positive correlation between speech recognition and the reliability of auditory recognition.\nTo evaluate the accuracy of the neural network of speech recognition from a participant's face and a speech recognition system, the researchers recruited two new type of scientists and analyzed the neural network of speech recognition from a participant's face and a speech recognition system. The new method was developed by combining two approaches: single-layer analysis with three-dimensional model, and individual-layer analysis.\nTo evaluate the accuracy of the neural network of speech recognition from a participant's face and a speech recognition system, the researchers recruited two new type of scientists and analyzed the neural network of speech recognition from a participant's face and a speech recognition system. The new method was developed by combining two approaches: single-layer analysis with three-dimensional model, and individual-layer analysis.\nTo evaluate the accuracy of the neural network of speech recognition from a participant's face and a speech recognition system, the researchers recruited two new type of scientists and analyzed the neural network of speech recognition from a participant's face and a speech recognition system. The new method was developed by combining two approaches: single-layer analysis with three-dimensional model, and individual-layer analysis.\nTo determine the accuracy of the neural network of speech recognition from a participant's face and a speech recognition system, the researchers recruited two new type of scientists and analyzed the neural network of speech recognition from a participant's face and a speech recognition system. The new method was developed by combining two approaches: single-layer analysis with three-dimensional model, and", "histories": [["v1", "Fri, 29 Jan 2016 16:48:07 GMT  (122kb,D)", "http://arxiv.org/abs/1601.08188v1", "Accepted for publication at ICASSP 2016"]], "COMMENTS": "Accepted for publication at ICASSP 2016", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["michael wand", "jan koutn\\'ik", "j\\\"urgen schmidhuber"], "accepted": false, "id": "1601.08188"}, "pdf": {"name": "1601.08188.pdf", "metadata": {"source": "CRF", "title": "LIPREADING WITH LONG SHORT-TERM MEMORY", "authors": ["Michael Wand", "Jan Koutn\u0131\u0301k", "J\u00fcrgen Schmidhuber"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014 Lipreading, Long Short-Term Memory, Recurrent Neural Networks, Image Recognition\n1. INTRODUCTION\nIt is well-known that humans understand speech not only by listening, but also by taking visual cues into account [1]. Hearing-impaired persons are in fact able to comprehend human speech by purely visual lipreading, i.e. by processing visual information from a speaker\u2019s lips and face. Consequently, research on making lipreading available to electronic speech recognition and processing systems has been of interest for some decades, with pioneering work done by Petajan [2]. His PhD thesis proposed to use lipreading to augment conventional automatic speech recognition (ASR), yet later researchers started to perform purely visual speech recognition [3], which is also the goal of this study. Lipreading systems typically consist of (at least) feature extraction and classification. The feature extraction can become quite complex: Many recent lipreading systems, e.g. [4, 5], use a lip tracking system as a first stage, followed by versatile image features such as Active Appearance Models [6] or\nThis research was supported by the FP7 Marie Curie Initial Training Network PROTOTOUCH (grant #317100) and the Swiss National Science Foundation grant \u201cAdvanced Reinforcement Learning\u201d (grant #156682).\nLocal Binary Patterns [7]. Classification is frequently done with Support Vector Machines (SVMs), e.g. [7], or Hidden Markov Models (HMMs), e.g. [4, 5, 8, 9].\nOur aim is to replace the complete visual speech recognition pipeline with a compact neural network architecture. Neural networks (NNs) have become increasingly popular in conventional speech recognition, first as feature extractors in an HMM-based architecture [10\u201312], more recently replacing the entire processing chain [13]. For the latter, the Long Short Term Memory (LSTM; [14]) architecture is typically used. Consequently, our approach to the lipreading problem uses a NN that chains feed-forward layers and LSTM layers, described in detail in subsection 4.2. Manual feature extraction is no longer required. The NN inputs are now the raw mouth images, as is common in modern computer vision tasks, but stands in stark contrast e.g. to [5, 7].\n2. RELATED WORK\nLipreading has been used as a complementary modality for speech recognition from noisy audio data [2, 15], as well as for purely visual speech recognition [3, 16, 17]. The latter gives rise to a Silent Speech interface, which is defined as a system \u201cenabling speech communication to take place when an audible acoustic signal is unavailable\u201d [18]. Silent Speech technology has a large number of applications: It allows persons with certain speech impairments (e.g. laryngectomees, whose voice box (larynx) has been removed) to communicate, as well as enabling confidential and undisturbing communication in public places [18]. Further uses of lipreading have been proposed, e.g. automatic speech extraction from surveillance videos and its interpretation for forensic purposes [5]. Lipreading has been augmented with ultrasound images of the tongue and vocal tract [19\u201321]. Furthermore, there are Silent Speech interfaces based on very different principles, like speech recognition from electromyography [22\u201325] or (electro-)magnetic articulography [26].\nNNs have been used in speech recognition as feature extractors in HMM-based speech recognizers [10, 11]. Neural networks, LSTMs in particular, started to replace larger parts of the speech processing chain previously dominated by HMMs. An end-to-end neural network system [27,28] finally outperformed HMM-based systems and achieved the best performance (16% error) on the large Switchboard Hub5\u201900\nar X\niv :1\n60 1.\n08 18\n8v 1\n[ cs\n.C V\n] 2\n9 Ja\nn 20\n16\nspeech recognition benchmark [29]. Massively parallel graphics processing units (GPUs) became available in the last few years. Since then, Convolutional NNs (CNNs) trained by gradient descent [30] dominate, e.g. [31], the area of image recognition, as well as related tasks like object detection and segmentation. The first CNN application in lipreading [17] uses the CNN as a preprocessor for an HMM-based sequence classifier.\n3. THE GRID DATA CORPUS\nOur experiments were performed using the GRID audiovisual corpus [32]1, consisting of video and audio recordings of 34 speakers saying 1000 sentences each. The total length of the recordings is 28 hours; two example video frames are shown in Figure 1. Each of the sentences has a fixed structure: command(4) + color(4) + preposition(4) + letter(25) + digit(10) + adverb(4), for example \u201cPlace red at J 2, please\u201d, where the number of alternatives words is given in parentheses. A total of 51 different words are contained in the GRID corpus; the alternative words for each of the six sentence parts are distributed uniformly. The letter W is excluded because its pronunciation is vastly longer than for any other letter.\nSentences have a fixed length of 3 seconds at a frame rate of 25 frames per second. Each sentences thus spans across 75 frames. Video data bis available in \u201cnormal\u201d and \u201chigh\u201d quality; the normal quality video with 360\u00d7288 pixel resolution, converted to greyscale, was used. Unreadable videos, in particular for speaker number 8, were discarded. The framelevel alignments distributed with the corpus were used to obtain word level segmentations of the video, causing the training dataset to consist of 6 \u00b7 1000 = 6000 single words per speaker. The acoustic part of the GRID corpus was not used.\nA 40 \u00d7 40 pixel window containing the mouth area from each video frame was extracted using the following procedure. The face area localized using the Mathematica FindFaces[ ] function was converted into the LAB colorspace. The A component pixels were multiplied element-wise by a Gaussian matrix (with mean located at 30% of of the\n1Publicly available at http://spandh.dcs.shef.ac.uk/ gridcorpus\nimage height along the middle column) and \u03c3 = 500 pixels, and rescaled into [0, 1] interval. The center of mass of pixels that have value above 0.9 was considered as the mouth center. The face area size was inflated by factor of 1.5, scaled to 128 pixel width, and a window of 40 \u00d7 40 pixels with the mouth coordinates in the center was extracted. This patch was converted to greyscale, the contrast was maximized (i.e. all pixel values were remapped to [0, 1] interval), and all the values in the complete dataset were standardized.\nSpeakers 1\u201319 from the entire GRID were used: speakers 1-9 form the development set that was used to determine the parameter settings; speakers 10\u201319 form the evaluation set, held back until the final evaluation of the systems. All experiments are speaker-dependent, i.e. training and test data for the classifiers were always taken from the same speaker. The results reported in this paper are averaged over the speakers. The data for each speaker was randomly divided into training, validation, and test sets, where the latter two contain five sample videos of each word, i.e. a total of 51 \u00b7 5 = 255 samples each. The training data is however highly unbalanced: For example, each letter from \u201ca\u201d to \u201cz\u201d appears 30 times, whereas each color appears 240 times.\n4. METHODS"}, {"heading": "4.1. Baseline Feature Extraction and Classification", "text": "The NN-based lipreader was compared to a baseline SVM classifier using conventional features, namely Eigenlips [15], which were used as a baseline feature in [17], and Histograms of Oriented Gradients (HOG) [33] as a more complex feature which yielded good performance in preliminary experiments.\nEigenlip features are created from raw frames by computing the PCA decomposition on the training data and then transforming all the images by multiplication with the PCA matrix, retaining only a certain number of dimensions ordered by maximal variance. HOG is originally a feature extractor for object recognition [33]; it divides the image window into small spatial regions (cells) and accumulates a local 1-D histogram of gradient directions or edge orientations over the pixels in each cell. Histogram entries are normalized over larger spatial areas. The HOG features were obtained using the VLFeat library [34].\nSince SVMs are not sequence classifiers, a single feature vector has to be computed from the sequence of frames representing each word, called sequence feature vector. Sequences vary in length. For example, a typical letter \u201ca\u201d is pronounced in 3\u20134 frames, whereas a longer word like \u201cplease\u201d can occupy more than 10 frames. In the sequence feature vector, all frames are stacked while enforcing a specified vector length: frames are repeated if a sequence is shorter than this length, neighboring frames of longer sequences are averaged."}, {"heading": "4.2. Neural Network Lipreader", "text": "Neural networks (NNs) consist of processing units (neurons) connected by trainable weighted connections. The neurons are typically organized in layers, which can be broadly distinguished as follows based on their connectivity: (1) feedforward NNs pass the input signal to the output neurons without allowing cyclic computations; (2) recurrent NNs wire the connections in a cycle which forms a temporal memory. NNs are, in the supervised case, typically trained by gradient descent, which is realized by error back-propagation through the layers followed by adjustment of the weights. In the case of recurrent NNs the error propagates along the time axis as well, referred to as back-propagation through time, implemented by unfolding the recurrent connections into a feedforward structure as deep as the length of the sequence. Such deep networks cause the gradient to explode or vanish [35], which can be fixed by replacing a single recurrent NN unit by a LSTM cell that avoids the problem by linear recurrent connection of a cell inside the LSTM unit [14]. The information flow through the LSTM cell is regulated by input, output and forget [36] gates using multiplicative connections. See [37] for detailed LSTM description, analysis and setup.\n5. EXPERIMENTS"}, {"heading": "5.1. Experimental Setup", "text": "Two feature extractions (Eigenlips and HOG), both combined with the SVM classifier, were compared to the LSTM lipreader. All parameters were optimized on the development speakers 1\u20139. The error on all systems is reported on the speaker-dependent test set, no training error is reported.\nIn a series of experiments on the speakers 1\u20139, the optimal configuration of the feature extraction for the SVM experiments was determined, as well as the best neural network structure. The best PCA cutoff for the Eigenlip features was at 100 components, the best HOG cell size was 8. Best SVM recognition results were obtained with a linear kernel at a sequence feature vector length of 6 frames. Increasing the fea-\nture vector length did not improve the accuracy, almost certainly because among the short video sequences containing letters, where most errors occur (see subsection 5.2), 6 frames already cover the entire information available. Taking this observation into account, we hypothesize that a dedicated sequence classifier like an HMM would not substantially improve the classification accuracy on this corpus.\nThe input data for the SVM can become very highdimensional, which we assume to be the reason why higherdegree polynomial SVM kernels yielded lower accuracy than linear kernels. Zhao et al. [7] report that second-degree polynomial SVM kernels perform the best, however their SVM classifier treats the sequential information differently.\nThe best LSTM lipreader consists of one feed-forward layer followed by two recurrent LSTM layers, 128 units (neurons / LSTM cells) each, and a softmax layer with 51 units that perform the word classification. The learning rate was set to 0.02, momentum was not used, and early stopping with a delay of 10 epochs was used. Weights were initialized from uniform distribution over the range [\u22120.05, 0.05]."}, {"heading": "5.2. Results", "text": "The word level classification accuracies are summarized in Table 1. The LSTM lipreader yields statistically significant improvement (one-tailed t-test with p = 0.05) over the conventional features combined with the SVM classifier. The Eigenlip and HOG features perform similarly, both worse than the LSTM lipreader. On the Evaluation Speakers 10\u2013 19, the LSTM lipreader improves the accuracy by 11.6%, compared to the best conventional solution (HOG + SVM).\nFigure 2 shows a typical confusion matrix on speaker 7 data, where classification was performed with the LSTM lipreader. The rows show reference word labels, the columns show hypotheses. The confusion on letters is far higher (upper part of the matrix) than on longer words (lower part of the matrix). For this speaker and configuration the accuracy on the letters is 69.8% (at 4% chance level), the accuracy on the non-letter words is 93.4% (at 3.8% chance level). The total accuracy is 82.0%.\nThe discrepancy between letters and other words is caused by three major factors: First, the letters from \u2019a\u2019 to \u2019z\u2019 are highly confusing even under optimal circumstances. In particular, this applies to voiced and voiceless versions of the same letter, like \u2018p\u2019 and \u2018b\u2019. Consequently, visemes (visual units for recognition) frequently do not distinguish between such similarly-looking sounds at all (see e.g. [38] and the references therein). Second, single letters video sequences are often are very short, sometimes consisting of only 3-4 frames. This means that very little data is available and also that the letter pronunciation is highly influenced by adjacent sounds. This context does not help distinguishing different letters, since the parts of the GRID sentences are statistically independent from each other. We note that [17] report phone accuracy on a corpus which consists of whole words: here the context plays a great role in improving recognition.\nThe confusion matrices are qualitatively similar across all speakers and all experimental setups \u2013 the longer words are recognized with close to 100% accuracy, whereas confusion is highest on the letters.\n6. CONCLUSION\nThis study shows that the neural network based lipreading system applied to raw images of the mouth regions achieves significantly better word accuracy than a system based on a conventional processing pipeline utilizing feature extraction and classification. The LSTM lipreader with a single feed-forward network, which learns the features automatically together with training the LSTM sequence classifier, consistently achieved almost 80% word accuracy in speakerdependent lipreading.\nThe experiments, not described in this paper, also included the highly popular CNNs instead of the fully connected feed-forward layer, but the results did not improve.\nOne of the possible reasons is that the small, 40 \u00d7 40 pixel area already contains just enough information for the classification. Experiments with CNNs and large image sizes as well as evaluation of a speaker-independent LSTM lipreader are the subject of future experiments.\n7. REFERENCES\n[1] H. McGurk and J. MacDonald, \u201cHearing Lips and Seeing Voices,\u201d Nature, vol. 264, no. 5588, pp. 746 \u2013 748, 1976.\n[2] E. D. Petajan, \u201cAutomatic Lipreading to Enhance Speech Recognition (Speech Reading) ,\u201d Ph.D. dissertation, University of Illinois at Urbana-Champaign, 1984.\n[3] G. I. Chiou and J.-N. Hwang, \u201cLipreading from Color Video,\u201d IEEE Transactions on Image Processing, vol. 6, no. 8, pp. 1192 \u2013 1195, 1997.\n[4] Y. Lan, R. Harvey, B.-J. Theobald, E.-J. Ong, and R. Bowden, \u201cComparing Visual Features for Lipreading,\u201d in Proc. of the Int. Conference on Auditory-Visual Speech Processing, 2009, pp. 102 \u2013 106.\n[5] R. Bowden, S. Cox, R. Harvey, Y. Lan, E.-J. Ong, G. Owen, and B.-J. Theobald, \u201cRecent Developments in Automated Lip-reading,\u201d in Proc. SPIE, 2013.\n[6] T. F. Cootes, G. J. Edwards, and C. J. Taylor, \u201cActive Appearance Models,\u201d IEEE Trans. on Pattern Anal. and Machine Intel., vol. 23, no. 6, pp. 681 \u2013 685, 2001.\n[7] G. Zhao, M. Barnard, and M. Pietika\u0308inen, \u201cLipreading With Local Spatiotemporal Descriptors,\u201d IEEE Trans.s on Multimedia, vol. 11, no. 7, pp. 1254 \u2013 1265, 2009.\n[8] T. Hueber, G. Chollet, B. Denby, G. Dreyfus, and M. Stone, \u201cContinuous-Speech Phone Recognition from Ultrasound and Optical Images of the Tongue and Lips,\u201d in Proc. Interspeech, 2007, pp. 658\u2013661.\n[9] F. Tao and C. Busso, \u201cLipreading Approach for Isolated Digits Recognition Under Whisper and Neutral Speech,\u201d in Proc. Interspeech, 2014, pp. 1154 \u2013 1158.\n[10] H. Bourlard and N. Morgan, Connectionist Speech Recognition. A Hybrid Approach. Kluwer Academic Publishers, 1994.\n[11] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury, \u201cDeep Neural Networks for Acoustic Modeling in Speech Recognition,\u201d IEEE Signal Proc. Magazine, vol. 29, no. 6, pp. 82 \u2013 97, 2012.\n[12] A. Graves, N. Jaitly, and A. Mohamed, \u201cHybrid Speech Recognition with Deep Bidirectional LSTM,\u201d in Proc. ASRU, 2013, pp. 273 \u2013 278.\n[13] A. Graves, A. Mohamed, and G. Hinton, \u201cSpeech Recognition with Deep Recurrent Neural Networks,\u201d in Proc. ICASSP, 2013, pp. 6645 \u2013 6649.\n[14] S. Hochreiter and J. Schmidhuber, \u201cLong Short-Term Memory,\u201d Neural Comp., vol. 9, pp. 1735 \u2013 1780, 1997.\n[15] C. Bregler and Y. Konig, \u201c\u2018Eigenlips\u2019 for Robust Speech Recognition,\u201d in Proc. ICASSP, 1994, pp. 669 \u2013 672.\n[16] R. Bowden, S. Cox, R. Harvey, Y. Lan, E.-J. Ong, G. Owen, and B.-J. Theobald, \u201cIs Automated Conversion of Video to Text a Reality?\u201d in Proc. SPIE, 2012.\n[17] K. Noda, Y. Yamaguchi, K. Nakadai, H. G. Okuno, and T. Ogata, \u201cLipreading using Convolutional Neural Network,\u201d in Proc. Interspeech, 2014, pp. 1149 \u2013 1153.\n[18] B. Denby, T. Schultz, K. Honda, T. Hueber, and J. Gilbert, \u201cSilent Speech Interfaces,\u201d Speech Communication, vol. 52, no. 4, pp. 270 \u2013 287, 2010.\n[19] B. Denby and M. Stone, \u201cSpeech Synthesis from Real Time Ultrasound Images of the Tongue,\u201d in Proc. ICASSP, 2004, pp. I\u2013685 \u2013 I\u2013688.\n[20] T. Hueber, G. Aversano, G. Chollet, B. Denby, G. Dreyfus, Y. Oussar, P. Roussel, and M. Stone, \u201cEigentongue Feature Extraction for an Ultrasound-based Silent Speech Interface,\u201d in Proc. ICASSP, 2007, pp. I\u2013 1245 \u2013 I\u20131248.\n[21] T. Hueber, E.-L. Benaroya, G. Chollet, B. Denby, G. Dreyfus, and M. Stone, \u201cDevelopment of a Silent Speech Interface Driven by Ultrasound and Optical Images of the Tongue and Lips,\u201d Speech Communication, vol. 52, pp. 288 \u2013 300, 2010.\n[22] N. Sugie and K. Tsunoda, \u201cA Speech Prosthesis Employing a Speech Synthesizer \u2013 Vowel Discrimination from Perioral Muscle Activities and Vowel Production,\u201d IEEE Transactions on Biomedical Engineering, vol. 32, no. 7, pp. 485 \u2013 490, 1985.\n[23] M. S. Morse, S. H. Day, B. Trull, and H. Morse, \u201cUse of Myoelectric Signals to Recognize Speech,\u201d in Proc. 11th Annual Conference of the IEEE Engineering in Medicine and Biology Society, 1989, pp. 1793 \u2013 1794.\n[24] T. Schultz and M. Wand, \u201cModeling Coarticulation in Large Vocabulary EMG-based Speech Recognition,\u201d Speech Comm., vol. 52, no. 4, pp. 341 \u2013 353, 2010.\n[25] M. Wand, M. Janke, and T. Schultz, \u201cTackling Speaking Mode Varieties in EMG-based Speech Recognition,\u201d IEEE Transaction on Biomedical Engineering, vol. 61, no. 10, pp. 2515 \u2013 2526, 2014.\n[26] M. J. Fagan, S. R. Ell, J. M. Gilbert, E. Sarrazin, and P. M. Chapman, \u201cDevelopment of a (Silent) Speech Recognition System for Patients Following Laryngectomy,\u201d Medical Engineering and Physics, vol. 30, pp. 419 \u2013 425, 2008.\n[27] A. Graves, S. Ferna\u0301ndez, F. Gomez, and J. Schmidhuber, \u201cConnectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks,\u201d in Proc. ICML, 2006, pp. 369 \u2013 376.\n[28] A. Graves and N. Jaitly, \u201cTowards End-To-End Speech Recognition with Recurrent Neural Networks,\u201d in Proc. ICML, 2014, pp. 1764 \u2013 1772.\n[29] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates, and A. Y. Ng, \u201cDeepSpeech: Scaling up endto-end speech recognition,\u201d arXiv: 1412: 5567v1, 2014.\n[30] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackeland, \u201cBackpropagation Applied to Handwritten Zip Code Recognition,\u201d Neural Computation, vol. 1, pp. 541 \u2013 551, 1989.\n[31] D. Ciresan, U. Meier, J. Masci, and J. Schmidhuber, \u201cA Committee of Neural Networks for Traffic Sign Classification,\u201d in Proc. IJCNN, 2011, pp. 1918 \u2013 1921.\n[32] M. Cooke, J. Barker, S. Cunningham, and X. Shao, \u201cAn Audio-Visual Corpus for Speech Perception and Automatic Speech Recognition,\u201d Journal of the Acoustical Soc. of America, vol. 120, no. 5, pp. 2421 \u2013 2424, 2006.\n[33] N. Dalal and B. Triggs, \u201cHistograms of Oriented Gradients for Human Detection,\u201d in Proc. CVPR, vol. 1, 2005, pp. 886 \u2013 893.\n[34] A. Vedaldi and B. Fulkerson, \u201cVLFeat: An Open and Portable Library of Computer Vision Algorithms,\u201d http: //www.vlfeat.org/, 2008.\n[35] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber, A Field Guide to Dynamical Recurrent Neural Networks. IEEE Press, 2001, ch. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies.\n[36] F. A. Gers, J. Schmidhuber, and F. Cummins, \u201cLearning to forget: Continual prediction with LSTM,\u201d Neural Computation, vol. 12, no. 10, pp. 2451\u20132471, 2000.\n[37] K. Greff, R. K. Srivastava, J. Koutn\u0131\u0301k, B. R. Steunebrink, and J. Schmidhuber, \u201cLSTM: A search space odyssey,\u201d CoRR, vol. abs/1503.04069, 2015. [Online]. Available: http://arxiv.org/abs/1503.04069\n[38] L. Cappelletta and N. Harte, \u201cViseme Definitions Comparison for Visual-only Speech Recognition,\u201d in Proc. EUSIPCO, 2011, pp. 2109 \u2013 2113."}], "references": [{"title": "Hearing Lips and Seeing Voices", "author": ["H. McGurk", "J. MacDonald"], "venue": "Nature, vol. 264, no. 5588, pp. 746 \u2013 748, 1976.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1976}, {"title": "Automatic Lipreading to Enhance Speech Recognition (Speech Reading)", "author": ["E.D. Petajan"], "venue": "Ph.D. dissertation, University of Illinois at Urbana-Champaign, 1984.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1984}, {"title": "Lipreading from Color Video", "author": ["G.I. Chiou", "J.-N. Hwang"], "venue": "IEEE Transactions on Image Processing, vol. 6, no. 8, pp. 1192 \u2013 1195, 1997.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Comparing Visual Features for Lipreading", "author": ["Y. Lan", "R. Harvey", "B.-J. Theobald", "E.-J. Ong", "R. Bowden"], "venue": "Proc. of the Int. Conference on Auditory-Visual Speech Processing, 2009, pp. 102 \u2013 106.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Recent Developments in Automated Lip-reading", "author": ["R. Bowden", "S. Cox", "R. Harvey", "Y. Lan", "E.-J. Ong", "G. Owen", "B.-J. Theobald"], "venue": "Proc. SPIE, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Active Appearance Models", "author": ["T.F. Cootes", "G.J. Edwards", "C.J. Taylor"], "venue": "IEEE Trans. on Pattern Anal. and Machine Intel., vol. 23, no. 6, pp. 681 \u2013 685, 2001.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Lipreading With Local Spatiotemporal Descriptors", "author": ["G. Zhao", "M. Barnard", "M. Pietik\u00e4inen"], "venue": "IEEE Trans.s on Multimedia, vol. 11, no. 7, pp. 1254 \u2013 1265, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Continuous-Speech Phone Recognition from Ultrasound and Optical Images of the Tongue and Lips", "author": ["T. Hueber", "G. Chollet", "B. Denby", "G. Dreyfus", "M. Stone"], "venue": "Proc. Interspeech, 2007, pp. 658\u2013661.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Lipreading Approach for Isolated Digits Recognition Under Whisper and Neutral Speech", "author": ["F. Tao", "C. Busso"], "venue": "Proc. Interspeech, 2014, pp. 1154 \u2013 1158.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Connectionist Speech Recognition", "author": ["H. Bourlard", "N. Morgan"], "venue": "A Hybrid Approach. Kluwer Academic Publishers,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Proc. Magazine, vol. 29, no. 6, pp. 82 \u2013 97, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Hybrid Speech Recognition with Deep Bidirectional LSTM", "author": ["A. Graves", "N. Jaitly", "A. Mohamed"], "venue": "Proc. ASRU, 2013, pp. 273 \u2013 278.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Speech Recognition with Deep Recurrent Neural Networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "Proc. ICASSP, 2013, pp. 6645 \u2013 6649.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Long Short-Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comp., vol. 9, pp. 1735 \u2013 1780, 1997.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "Eigenlips\u2019 for Robust Speech Recognition", "author": ["C. Bregler", "Y. Konig"], "venue": "Proc. ICASSP, 1994, pp. 669 \u2013 672.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1994}, {"title": "Lipreading using Convolutional Neural Network", "author": ["K. Noda", "Y. Yamaguchi", "K. Nakadai", "H.G. Okuno", "T. Ogata"], "venue": "Proc. Interspeech, 2014, pp. 1149 \u2013 1153.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Silent Speech Interfaces", "author": ["B. Denby", "T. Schultz", "K. Honda", "T. Hueber", "J. Gilbert"], "venue": "Speech Communication, vol. 52, no. 4, pp. 270 \u2013 287, 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Speech Synthesis from Real Time Ultrasound Images of the Tongue", "author": ["B. Denby", "M. Stone"], "venue": "Proc. ICASSP, 2004, pp. I\u2013685 \u2013 I\u2013688.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Eigentongue Feature Extraction for an Ultrasound-based Silent Speech Interface", "author": ["T. Hueber", "G. Aversano", "G. Chollet", "B. Denby", "G. Dreyfus", "Y. Oussar", "P. Roussel", "M. Stone"], "venue": "Proc. ICASSP, 2007, pp. I\u2013 1245 \u2013 I\u20131248.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Development of a Silent Speech Interface Driven by Ultrasound and Optical Images of the Tongue and Lips", "author": ["T. Hueber", "E.-L. Benaroya", "G. Chollet", "B. Denby", "G. Dreyfus", "M. Stone"], "venue": "Speech Communication, vol. 52, pp. 288 \u2013 300, 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "A Speech Prosthesis Employing a Speech Synthesizer \u2013 Vowel Discrimination from Perioral Muscle Activities and Vowel Production", "author": ["N. Sugie", "K. Tsunoda"], "venue": "IEEE Transactions on Biomedical Engineering, vol. 32, no. 7, pp. 485 \u2013 490, 1985.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1985}, {"title": "Use of Myoelectric Signals to Recognize Speech", "author": ["M.S. Morse", "S.H. Day", "B. Trull", "H. Morse"], "venue": "Proc. 11th Annual Conference of the IEEE Engineering in Medicine and Biology Society, 1989, pp. 1793 \u2013 1794.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1989}, {"title": "Modeling Coarticulation in Large Vocabulary EMG-based Speech Recognition", "author": ["T. Schultz", "M. Wand"], "venue": "Speech Comm., vol. 52, no. 4, pp. 341 \u2013 353, 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Tackling Speaking Mode Varieties in EMG-based Speech Recognition", "author": ["M. Wand", "M. Janke", "T. Schultz"], "venue": "IEEE Transaction on Biomedical Engineering, vol. 61, no. 10, pp. 2515 \u2013 2526, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Development of a (Silent) Speech Recognition System for Patients Following Laryngectomy", "author": ["M.J. Fagan", "S.R. Ell", "J.M. Gilbert", "E. Sarrazin", "P.M. Chapman"], "venue": "Medical Engineering and Physics, vol. 30, pp. 419 \u2013 425, 2008.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "Proc. ICML, 2006, pp. 369 \u2013 376.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Towards End-To-End Speech Recognition with Recurrent Neural Networks", "author": ["A. Graves", "N. Jaitly"], "venue": "Proc. ICML, 2014, pp. 1764 \u2013 1772.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "DeepSpeech: Scaling up endto-end speech recognition", "author": ["A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates", "A.Y. Ng"], "venue": "arXiv: 1412: 5567v1, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Backpropagation Applied to Handwritten Zip Code Recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackeland"], "venue": "Neural Computation, vol. 1, pp. 541 \u2013 551, 1989.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1989}, {"title": "A Committee of Neural Networks for Traffic Sign Classification", "author": ["D. Ciresan", "U. Meier", "J. Masci", "J. Schmidhuber"], "venue": "Proc. IJCNN, 2011, pp. 1918 \u2013 1921.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "An Audio-Visual Corpus for Speech Perception and Automatic Speech Recognition", "author": ["M. Cooke", "J. Barker", "S. Cunningham", "X. Shao"], "venue": "Journal of the Acoustical Soc. of America, vol. 120, no. 5, pp. 2421 \u2013 2424, 2006.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Histograms of Oriented Gradients for Human Detection", "author": ["N. Dalal", "B. Triggs"], "venue": "Proc. CVPR, vol. 1, 2005, pp. 886 \u2013 893.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "VLFeat: An Open and Portable Library of Computer Vision Algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "http: //www.vlfeat.org/, 2008.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "A Field Guide to Dynamical Recurrent Neural Networks", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": "IEEE Press,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2001}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["F.A. Gers", "J. Schmidhuber", "F. Cummins"], "venue": "Neural Computation, vol. 12, no. 10, pp. 2451\u20132471, 2000.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2000}, {"title": "LSTM: A search space odyssey", "author": ["K. Greff", "R.K. Srivastava", "J. Koutn\u0131\u0301k", "B.R. Steunebrink", "J. Schmidhuber"], "venue": "CoRR, vol. abs/1503.04069, 2015. [Online]. Available: http://arxiv.org/abs/1503.04069", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Viseme Definitions Comparison for Visual-only Speech Recognition", "author": ["L. Cappelletta", "N. Harte"], "venue": "Proc. EUSIPCO, 2011, pp. 2109 \u2013 2113.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "It is well-known that humans understand speech not only by listening, but also by taking visual cues into account [1].", "startOffset": 114, "endOffset": 117}, {"referenceID": 1, "context": "Consequently, research on making lipreading available to electronic speech recognition and processing systems has been of interest for some decades, with pioneering work done by Petajan [2].", "startOffset": 186, "endOffset": 189}, {"referenceID": 2, "context": "His PhD thesis proposed to use lipreading to augment conventional automatic speech recognition (ASR), yet later researchers started to perform purely visual speech recognition [3], which is also the goal of this study.", "startOffset": 176, "endOffset": 179}, {"referenceID": 3, "context": "[4, 5], use a lip tracking system as a first stage, followed by versatile image features such as Active Appearance Models [6] or", "startOffset": 0, "endOffset": 6}, {"referenceID": 4, "context": "[4, 5], use a lip tracking system as a first stage, followed by versatile image features such as Active Appearance Models [6] or", "startOffset": 0, "endOffset": 6}, {"referenceID": 5, "context": "[4, 5], use a lip tracking system as a first stage, followed by versatile image features such as Active Appearance Models [6] or", "startOffset": 122, "endOffset": 125}, {"referenceID": 6, "context": "Local Binary Patterns [7].", "startOffset": 22, "endOffset": 25}, {"referenceID": 6, "context": "[7], or Hidden Markov Models (HMMs), e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4, 5, 8, 9].", "startOffset": 0, "endOffset": 12}, {"referenceID": 4, "context": "[4, 5, 8, 9].", "startOffset": 0, "endOffset": 12}, {"referenceID": 7, "context": "[4, 5, 8, 9].", "startOffset": 0, "endOffset": 12}, {"referenceID": 8, "context": "[4, 5, 8, 9].", "startOffset": 0, "endOffset": 12}, {"referenceID": 9, "context": "Neural networks (NNs) have become increasingly popular in conventional speech recognition, first as feature extractors in an HMM-based architecture [10\u201312], more recently replacing the entire processing chain [13].", "startOffset": 148, "endOffset": 155}, {"referenceID": 10, "context": "Neural networks (NNs) have become increasingly popular in conventional speech recognition, first as feature extractors in an HMM-based architecture [10\u201312], more recently replacing the entire processing chain [13].", "startOffset": 148, "endOffset": 155}, {"referenceID": 11, "context": "Neural networks (NNs) have become increasingly popular in conventional speech recognition, first as feature extractors in an HMM-based architecture [10\u201312], more recently replacing the entire processing chain [13].", "startOffset": 148, "endOffset": 155}, {"referenceID": 12, "context": "Neural networks (NNs) have become increasingly popular in conventional speech recognition, first as feature extractors in an HMM-based architecture [10\u201312], more recently replacing the entire processing chain [13].", "startOffset": 209, "endOffset": 213}, {"referenceID": 13, "context": "For the latter, the Long Short Term Memory (LSTM; [14]) architecture is typically used.", "startOffset": 50, "endOffset": 54}, {"referenceID": 4, "context": "to [5, 7].", "startOffset": 3, "endOffset": 9}, {"referenceID": 6, "context": "to [5, 7].", "startOffset": 3, "endOffset": 9}, {"referenceID": 1, "context": "Lipreading has been used as a complementary modality for speech recognition from noisy audio data [2, 15], as well as for purely visual speech recognition [3, 16, 17].", "startOffset": 98, "endOffset": 105}, {"referenceID": 14, "context": "Lipreading has been used as a complementary modality for speech recognition from noisy audio data [2, 15], as well as for purely visual speech recognition [3, 16, 17].", "startOffset": 98, "endOffset": 105}, {"referenceID": 2, "context": "Lipreading has been used as a complementary modality for speech recognition from noisy audio data [2, 15], as well as for purely visual speech recognition [3, 16, 17].", "startOffset": 155, "endOffset": 166}, {"referenceID": 15, "context": "Lipreading has been used as a complementary modality for speech recognition from noisy audio data [2, 15], as well as for purely visual speech recognition [3, 16, 17].", "startOffset": 155, "endOffset": 166}, {"referenceID": 16, "context": "The latter gives rise to a Silent Speech interface, which is defined as a system \u201cenabling speech communication to take place when an audible acoustic signal is unavailable\u201d [18].", "startOffset": 174, "endOffset": 178}, {"referenceID": 16, "context": "laryngectomees, whose voice box (larynx) has been removed) to communicate, as well as enabling confidential and undisturbing communication in public places [18].", "startOffset": 156, "endOffset": 160}, {"referenceID": 4, "context": "automatic speech extraction from surveillance videos and its interpretation for forensic purposes [5].", "startOffset": 98, "endOffset": 101}, {"referenceID": 17, "context": "Lipreading has been augmented with ultrasound images of the tongue and vocal tract [19\u201321].", "startOffset": 83, "endOffset": 90}, {"referenceID": 18, "context": "Lipreading has been augmented with ultrasound images of the tongue and vocal tract [19\u201321].", "startOffset": 83, "endOffset": 90}, {"referenceID": 19, "context": "Lipreading has been augmented with ultrasound images of the tongue and vocal tract [19\u201321].", "startOffset": 83, "endOffset": 90}, {"referenceID": 20, "context": "Furthermore, there are Silent Speech interfaces based on very different principles, like speech recognition from electromyography [22\u201325] or (electro-)magnetic articulography [26].", "startOffset": 130, "endOffset": 137}, {"referenceID": 21, "context": "Furthermore, there are Silent Speech interfaces based on very different principles, like speech recognition from electromyography [22\u201325] or (electro-)magnetic articulography [26].", "startOffset": 130, "endOffset": 137}, {"referenceID": 22, "context": "Furthermore, there are Silent Speech interfaces based on very different principles, like speech recognition from electromyography [22\u201325] or (electro-)magnetic articulography [26].", "startOffset": 130, "endOffset": 137}, {"referenceID": 23, "context": "Furthermore, there are Silent Speech interfaces based on very different principles, like speech recognition from electromyography [22\u201325] or (electro-)magnetic articulography [26].", "startOffset": 130, "endOffset": 137}, {"referenceID": 24, "context": "Furthermore, there are Silent Speech interfaces based on very different principles, like speech recognition from electromyography [22\u201325] or (electro-)magnetic articulography [26].", "startOffset": 175, "endOffset": 179}, {"referenceID": 9, "context": "NNs have been used in speech recognition as feature extractors in HMM-based speech recognizers [10, 11].", "startOffset": 95, "endOffset": 103}, {"referenceID": 10, "context": "NNs have been used in speech recognition as feature extractors in HMM-based speech recognizers [10, 11].", "startOffset": 95, "endOffset": 103}, {"referenceID": 25, "context": "An end-to-end neural network system [27,28] finally outperformed HMM-based systems and achieved the best performance (16% error) on the large Switchboard Hub5\u201900 ar X iv :1 60 1.", "startOffset": 36, "endOffset": 43}, {"referenceID": 26, "context": "An end-to-end neural network system [27,28] finally outperformed HMM-based systems and achieved the best performance (16% error) on the large Switchboard Hub5\u201900 ar X iv :1 60 1.", "startOffset": 36, "endOffset": 43}, {"referenceID": 27, "context": "speech recognition benchmark [29].", "startOffset": 29, "endOffset": 33}, {"referenceID": 28, "context": "Since then, Convolutional NNs (CNNs) trained by gradient descent [30] dominate, e.", "startOffset": 65, "endOffset": 69}, {"referenceID": 29, "context": "[31], the area of image recognition, as well as related tasks like object detection and segmentation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "The first CNN application in lipreading [17] uses the CNN as a preprocessor for an HMM-based sequence classifier.", "startOffset": 40, "endOffset": 44}, {"referenceID": 30, "context": "Our experiments were performed using the GRID audiovisual corpus [32]1, consisting of video and audio recordings of 34 speakers saying 1000 sentences each.", "startOffset": 65, "endOffset": 69}, {"referenceID": 0, "context": "gridcorpus image height along the middle column) and \u03c3 = 500 pixels, and rescaled into [0, 1] interval.", "startOffset": 87, "endOffset": 93}, {"referenceID": 0, "context": "all pixel values were remapped to [0, 1] interval), and all the values in the complete dataset were standardized.", "startOffset": 34, "endOffset": 40}, {"referenceID": 14, "context": "The NN-based lipreader was compared to a baseline SVM classifier using conventional features, namely Eigenlips [15], which were used as a baseline feature in [17], and Histograms of Oriented Gradients (HOG) [33] as a more complex feature which yielded good performance in preliminary experiments.", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "The NN-based lipreader was compared to a baseline SVM classifier using conventional features, namely Eigenlips [15], which were used as a baseline feature in [17], and Histograms of Oriented Gradients (HOG) [33] as a more complex feature which yielded good performance in preliminary experiments.", "startOffset": 158, "endOffset": 162}, {"referenceID": 31, "context": "The NN-based lipreader was compared to a baseline SVM classifier using conventional features, namely Eigenlips [15], which were used as a baseline feature in [17], and Histograms of Oriented Gradients (HOG) [33] as a more complex feature which yielded good performance in preliminary experiments.", "startOffset": 207, "endOffset": 211}, {"referenceID": 31, "context": "HOG is originally a feature extractor for object recognition [33]; it divides the image window into small spatial regions (cells) and accumulates a local 1-D histogram of gradient directions or edge orientations over the pixels in each cell.", "startOffset": 61, "endOffset": 65}, {"referenceID": 32, "context": "The HOG features were obtained using the VLFeat library [34].", "startOffset": 56, "endOffset": 60}, {"referenceID": 33, "context": "Such deep networks cause the gradient to explode or vanish [35], which can be fixed by replacing a single recurrent NN unit by a LSTM cell that avoids the problem by linear recurrent connection of a cell inside the LSTM unit [14].", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "Such deep networks cause the gradient to explode or vanish [35], which can be fixed by replacing a single recurrent NN unit by a LSTM cell that avoids the problem by linear recurrent connection of a cell inside the LSTM unit [14].", "startOffset": 225, "endOffset": 229}, {"referenceID": 34, "context": "The information flow through the LSTM cell is regulated by input, output and forget [36] gates using multiplicative connections.", "startOffset": 84, "endOffset": 88}, {"referenceID": 35, "context": "See [37] for detailed LSTM description, analysis and setup.", "startOffset": 4, "endOffset": 8}, {"referenceID": 6, "context": "[7] report that second-degree polynomial SVM kernels perform the best, however their SVM classifier treats the sequential information differently.", "startOffset": 0, "endOffset": 3}, {"referenceID": 36, "context": "[38] and the references therein).", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "We note that [17] report phone accuracy on a corpus which consists of whole words: here the context plays a great role in improving recognition.", "startOffset": 13, "endOffset": 17}], "year": 2016, "abstractText": "Lipreading, i.e. speech recognition from visual-only recordings of a speaker\u2019s face, can be achieved with a processing pipeline based solely on neural networks, yielding significantly better accuracy than conventional methods. Feedforward and recurrent neural network layers (namely Long Short-Term Memory; LSTM) are stacked to form a single structure which is trained by back-propagating error gradients through all the layers. The performance of such a stacked network was experimentally evaluated and compared to a standard Support Vector Machine classifier using conventional computer vision features (Eigenlips and Histograms of Oriented Gradients). The evaluation was performed on data from 19 speakers of the publicly available GRID corpus. With 51 different words to classify, we report a best word accuracy on held-out evaluation speakers of 79.6% using the end-toend neural network-based solution (11.6% improvement over the best feature-based solution evaluated).", "creator": "LaTeX with hyperref package"}}}