{"id": "1602.01198", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2016", "title": "k-variates++: more pluses in the k-means++", "abstract": "k-means++ seeding has become a de facto standard for hard clustering algorithms. In this paper, our first contribution is a two-way generalisation of this seeding, k-variates++, that includes the sampling of general densities rather than just a discrete set of Dirac densities anchored at the point locations, and a generalisation of the well known Arthur-Vassilvitskii (AV) approximation guarantee, in the form of a bias+variance approximation bound of the global optimum. This approximation exhibits a reduced dependency on the \"noise\" component with respect to the optimal potential --- actually approaching the statistical lower bound. We show that k-variates++ reduces to efficient (biased seeding) clustering algorithms tailored to specific frameworks; these include distributed, streaming and on-line clustering, with direct approximation results for these algorithms. Finally, we present a novel application of k-variates++ to differential privacy. For either the specific frameworks considered here, or for the differential privacy setting, there is little to no prior results on the direct application of k-means++ and its approximation bounds --- state of the art contenders appear to be significantly more complex and / or display less favorable (approximation) properties. We stress that our algorithms can still be run in cases where there is \\textit{no} closed form solution for the population minimizer. We demonstrate the applicability of our analysis via experimental evaluation on several domains and settings, displaying competitive performances vs state of the art.\n\n\n\nIn this paper, we will explore the feasibility of a technique for evaluating the optimization performance of differential privacy for discrete privacy in a broad range of applications. This paper uses the same techniques described above as the K-dauk-sampling method to quantify the performance of differential privacy in a high-performance real-time, high-performance real-time, high-performance real-time machine-readable database. We conclude that we have demonstrated that differential privacy is a good candidate for low-to-high-performance privacy. In this paper, we propose that we show that we can evaluate the optimization performance of differential privacy in a large-scale application, with a robust and efficient distribution of the total number of algorithms necessary. By this approach, we demonstrate that differential privacy is very attractive for low-to-high-performance privacy. We have shown that we can evaluate the optimization performance of differential privacy in a high-performance real-time, high-performance real-time", "histories": [["v1", "Wed, 3 Feb 2016 06:31:09 GMT  (3242kb,D)", "https://arxiv.org/abs/1602.01198v1", null], ["v2", "Sat, 13 Feb 2016 00:36:41 GMT  (3242kb,D)", "http://arxiv.org/abs/1602.01198v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["richard nock", "rapha\u00ebl canyasse", "roksana boreli", "frank nielsen"], "accepted": true, "id": "1602.01198"}, "pdf": {"name": "1602.01198.pdf", "metadata": {"source": "CRF", "title": "k-variates++: more pluses in the k-means++", "authors": ["Richard Nock", "Roksana Boreli"], "emails": ["richard.nock@nicta.com.au", "raphael.canyasse@polytechnique.edu", "roksana.boreli@nicta.com.au", "Frank.Nielsen@acm.org"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n01 19"}, {"heading": "1 Introduction", "text": "Arthur-Vassilvitskii\u2019s (AV) k-means++ algorithm has been extensively used to address the hard membership clustering problem, due to its simplicity, experimental performance and guaranteed approximation of the global optimum; the goal being the k-partitioning of a dataset so as to minimize the sum of within-cluster squared distances to the cluster center (Arthur & Vassilvitskii, 2007), i.e., a centroid or a population minimizer (Nock et al., 2016).\nThe k-means++ non-uniform seeding approach has also been utilized in more complex settings, including tensor clustering, distributed, data stream, on-line and parallel clustering, clustering with non-metric distortions and even clustering with distortions not allowing population minimizers in closed form (Ailon et al., 2009; Balcan et al., 2013; Jegelka et al., 2009; Liberty et al., 2014; Nock et al., 2008; Nielsen & Nock, 2015). However, apart from the non-uniform seeding, all these algorithms are distinct and (seemingly) do not share many common properties.\nFinally, the application of k-means++ in some scenarios is still an open research topic, due to the related constraints \u2013 e.g., there is limited prior work in a differentially private setting (Nissim et al., 2007; Wang et al., 2015). Our contribution \u2014 In a nutshell, we describe a generalisation of the k-means++ seeding process, k-variates++, which still delivers an efficient approximation of the global optimum, and can be used to obtain and analyze efficient algorithms for a wide range of settings, including: distributed, streamed, on-line clustering, (differentially) private clustering, etc. . We proceed in two steps.\nFirst, we describe k-variates++ and analyze its approximation properties. We leverage two major components of k-means++: (i) data-dependent probes (specialized to observed data in the k-means++) are used to compute the weights for selecting centers, and (ii) selection of centers is based on an arbitrary family of densities (specialized to Diracs in the k-means++). Informally, the approximation properties (when only (ii) is considered), can be shown as:\nexpected cost(k-variates++) \u2264 (2 + log k) \u00b7 \u03a6 , with\n\u03a6 . = 6 \u00b7 optimal noise-free cost + 2 \u00b7 noise (bias + variance), where \u201cnoise\u201d refers to the family of densities (note that constants are explicit in the bound). The dependence on these densities is arguably smaller than expectable (factor 2 for noise vs 6 for global optimum). There is also not much room for improvement: we show that the guarantee approaches the Fre\u0301chet-Crame\u0301r-RaoDarmois lowerbound.\nSecond, we use this general algorithm in two ways. We use it directly in a differential privacy setting, addressing a conjecture of (Nissim et al., 2007) with weaker assumptions. We also demonstrate the use of this algorithm for a reduction to other biased seeding algorithms for distributed, streamed or on-line clustering, and obtain the approximation bounds for these algorithms. This simple reduction technique allows us to analyze lightweight algorithms that compare favorably to the state of the art in the related domains (Ailon et al., 2009; Balcan et al., 2013; Liberty et al., 2014), from the approximation, assumptions and / or complexity aspects. Experiments against state of the art for the distributed and differentially private settings display that solid performance improvement can be obtained.\nThe rest of this paper is organised as follows: Section 2 presents k-variates++. Section 3 presents approximation properties for distributed, streamed and on-line clustering that use a reduction from k-variates++. Section 4 presents direct applications of k-variates++ to differential\nprivacy. Section 5 presents experimental results. Last Section discusses extensions (to more distortion measures) and conclude. In order not to laden the paper\u2019s body, an Appendix, starting page 18, provides all proofs, extensive experiments and additional remarks on the paper\u2019s content.\n2 k-variates++ We consider the hard clustering problem (Banerjee et al., 2005; Nock et al., 2016): given set A \u2282 Rd and integer k > 0, find centers C \u2282 Rd which minimizes the L22 potential to the centers (here, c(a) .= arg minc\u2208C \u2016a\u2212 c\u201622):\n\u03c6(A;C) . = \u2211 a\u2208A \u2016a\u2212 c(a)\u201622 , (2)\nAlgorithm 0 describes k-variates++. um denotes the uniform distribution over A (|A| = m). The parenthood with k-means++ seeding, which we name \u201ck-means++\u201d for short1 (Arthur & Vassilvitskii, 2007) can be best understood using Figure 1 (the red parts in Figure 1 are pinpointed in Algorithm 0). k-means++ is a random process that generates cluster centers from observed data A. It can be modelled using a two-stage generative process for a mixture of Dirac distributions: the first stage involves random variable Qt \u223c Mult(m,\u03c0t) whose parameters \u03c0t \u2208 4m (the m-dim probability simplex) are computed from the data and previous centers; sampling Qt chooses the Dirac distribution, which is then \u201csampled\u201d for one center (and the process iterates). All the crux of the technique is the design of \u03c0t, which, under no assumption of the data, yield in expectation a k-means potential for the centers chosen that is within 8(2 + log k) of the global optimum (Arthur & Vassilvitskii, 2007).\nk-variates++ generalize the process in two ways: first, the update of \u03c0t depends on data and previous probes, using a sequence of probe functions \u2118t : A \u2192 Rd (\u2118 = Id,\u2200t in k-means++). Second, Diracs are replaced by arbitrary but fixed local (sometimes also called noisy) distributions with parameters2 (\u00b5a,\u03b8a) that depend on A.\nLet Copt \u2282 Rd denote the set of k centers minimizing (2) on A. Let copt(a) .= arg minc\u2208Copt \u2016a\u2212 c\u201622 (a \u2208 A), and\n\u03c6opt . = \u2211 a\u2208A \u2016a\u2212 copt(a)\u201622 , (3)\n\u03c6bias . = \u2211 a\u2208A \u2016\u00b5a \u2212 copt(a)\u201622 , (4)\n\u03c6var . = \u2211 a\u2208A tr (\u03a3a) . (5)\n\u03c6opt is the optimal noise-free potential, \u03c6bias is the bias of the noise3, and \u03c6var its variance, with \u03a3a . = Ex\u223cpa [(x\u2212\u00b5a)(x\u2212\u00b5a)>] the covariance matrix of pa. Notice that when \u00b5a = a, \u03c6bias = \u03c6opt. Otherwise, it may hold that \u03c6bias < \u03c6opt, and even \u03c6bias = 0 if expectations coincide with Copt. Let Copt denote the partition of A according to the centers in Copt. We say that probe function \u2118t is \u03b7-stretching if, informally, replacing points by their probes does not distort significantly the observed potential of an optimal cluster, with respect to its actual optimal potential. The formal definition follows.\nDefinition 1 Probe functions \u2118t are said \u03b7-stretching on A, for some \u03b7 \u2265 0, iff the following holds: for any cluster A \u2208 Copt and any a0 \u2208 A such that \u03c6(\u2118t(A); {\u2118t(a0)}) 6= 0, for any set of at most k centers C \u2282 Rd,\n\u03c6(A;C) \u03c6(A; {a0}) \u2264 (1 + \u03b7) \u00b7 \u03c6(\u2118t(A);C) \u03c6(\u2118t(A); {\u2118t(a0)}) ,\u2200t . (6)\n1Both approaches can be completed with the same further local monotonous optimization steps like Lloyd or Hartigan iterations; furthermore, it is the biased seeding which holds the approximation properties of k-means++.\n2Because expectations are the major parameter for clustering, we split the parameters in the form of \u00b5a (expectation) and \u03b8a (other parameters, e.g. covariance matrix).\n3We term it bias by analogy with supervised classification, considering that the expectations of the densities could be used as models for the cluster centers (Kohavi & Wolpert, 1996).\nSince \u03c6(A;Copt) = \u2211 a0\u2208A \u03c6(A; {a0}) (Arthur & Vassilvitskii, 2007) (Lemma 3.2), Definition 1 roughly states that the potential of an optimal cluster with respect to a set of cluster centers, relatively to its potential with respect to the optimal set of centers, does not blow up through probe function \u2118t. The identity function is trivially 0-stretching, for any A. Many local transformations would be eligible for \u03b7-stretching probe functions with \u03b7 small, including local translations, mappings to core-sets (Har-Peled & Mazumdar, 2004), mappings to Voronoi diagram cell centers (Boissonnat et al., 2010), etc. Notice that ineq. (6) has to hold only for optimal clusters and not any clustering of A. Let E[\u03c6(A;C)] .= \u222b \u03c6(A|C)dp(C) denote the expected potential over the random sampling of C in k-variates++.\nTheorem 2 For any dataset A, any sequence of \u03b7-stretching probe functions \u2118t and any density {pa,a \u2208 A}, the expected potential of k-variates++ satisfies:\nE[\u03c6(A;C)] \u2264 (2 + log k) \u00b7 \u03a6 , (7) with \u03a6 .= (6 + 4\u03b7)\u03c6opt + 2\u03c6bias + 2\u03c6var.\n(Proof in page 19) Five remarks are in order. First, we retrieve the result of (Arthur & Vassilvitskii, 2007) in their setting (\u03b7 = \u03c6var = 0, \u03c6bias = \u03c6opt). Second, in the case where \u03c6bias < \u03c6opt, we may beat AV\u2019s bound. This is not due to an improvement of the algorithm, but to a finer analysis which shows that special settings may \u201cnaturally\u201d favor the improvement. We shall see one example in the distributed clustering case. Third, apart from being \u03b7-stretching, there is no constraint on the choice of probe functions \u2118t: it can be randomized, iteration dependent, etc. Fourth, the algorithm can easily be generalized to the case where points are weighted. Last, as we show in the following Lemma, the dependence in noise in ineq. (7) can hardly be improved in our framework.\nLemma 3 Suppose each point in A is replaced (i.i.d.) by a point sampled in pa with \u03a3a = \u03a3. Then any clustering algorithm suffers: E[\u03c6(A;C)] = \u2126(|A|tr (\u03a3)). (Proof in page 22) We make use of k-variates++ in two different ways. First, we show that it can be used to prove approximation properties for algorithms operating in different clustering settings: distributed clustering, streamed clustering and on-line clustering. The proof involves a reduction (see page 23) from k-variates++ to each of these algorithms. By reduction, we mean there exists distributions and probe functions (even non poly-time computable) for which k-variates++ yields the same result in expectation as the other algorithm, thus directly yielding an approximability ratio of the global optimum for this latter algorithm via Theorem 2. Second, we show how kvariates++ can directly be specialized to address settings for which no efficient application of k-means++ was known.\n3 Reductions from k-variates++ Despite tremendous advantages, k-means++ has a serious downside: it is difficult to parallelize, distribute or stream it under relevant communication, space, privacy and/or time resource constraints (Bahmani et al., 2012). Although extending k-means clustering to these settings has been a major research area in recent years, there has been no obvious solution to tailoring kmeans++ (Ackermann et al., 2010; Ailon et al., 2009; Bahmani et al., 2012; Balcan et al., 2013; Liberty et al., 2014; Shindler et al., 2011) (and others).\nAlgorithm 1 Dk-means++ (// PDk-means++) Input: Forgy nodes (Fi,Ai), i \u2208 [n], for t = 1, 2, ..., k Round 1 : N\u2217 picks i\u2217 \u223cqDt [n] and asks Fi\u2217 for a center; Round 2 : Fi\u2217 picks a \u223cui\u2217 Ai\u2217 and sends a to Fi,\u2200i;\n// PDk-means++: Fi\u2217 sends x \u223c p(\u00b5a,\u03b8a) to Fi,\u2200i; Round 3 : \u2200i,Fi updates Dt(Ai) and sends it to N\u2217;\nOutput: C = set of broadcasted as (or xs);\nDistributed clustering We consider horizontally partitioned data among peers, in line with (Bahmani et al., 2012), and a setting significantly more restrictive than theirs: each peer can only locally run the standard operations of Forgy initialisation (that is, uniform random seeding) on its own data, unlike for example the biased distributions of (Bahmani et al., 2012). This is consistent with the notion that data handling peers are not necessarily computationally intensive resources. Additionally, due to privacy constraints, we limit the data sharing between nodes. We denote the nodes handling the data Forgy nodes. We have n such nodes, (Fi,Ai), i \u2208 [n], where Ai is the dataset held by Fi. To enable more complex operations necessary to implement k-variates++, we introduce a special node, N\u2217, that has high computation power, but is not allowed to handle any data (points) from the Forgy nodes. We therefore split the location of the computational power from the location of the data. We also prevent the Forgy nodes from exchanging any data between themselves, with the sole exception of cluster centers. We note that none of the algorithms of (Ailon et al., 2009; Balcan et al., 2013; Bahmani et al., 2012) would be applicable to this setting without non-trivial modifications affecting their properties.\nAlgorithm 1 defines the mechanism that is consistent with our setting. It includes two variants: a protected version Dk-means++ where Forgy nodes directly share local centers and a private\nversion PDk-means++ where the nodes share noisy centers, such as to ensure a differentially private release of centers (with relevant noise calibration). Notations used in Algorithm 1 are as follows. Let Dt(Ai) . = \u2211 a\u2208Ai Dt(a) and q D ti . = Dt(Ai) \u00b7 ( \u2211 j Dt(Aj)) \u22121 if t > 1 and qDti .\n= 1/n otherwise. Also, ui is uniform distribution on [mi], with mi . = |Ai|.\nTheorem 4 Let \u03c6Fs .\n= \u2211 i\u2208[n] \u2211 a\u2208Ai \u2016c(Ai)\u2212a\u201622 be the total spread of the Forgy nodes (c(Ai) . =\n(1/mi) \u00b7 \u2211 Ai a). At iteration k, the expected potential on the total data A .= \u222aiAi satisfies ineq. (7) with\n\u03a6 . =\n{ 10\u03c6opt + 6\u03c6 F s (Dk-means++)\n10\u03c6opt + 4\u03c6 F s + 2\u03c6var (PDk-means++)\n. (8)\nHere, \u03c6opt is the optimal potential on total data A.\n(Proof in page 23) We note that the optimal potential is defined on the total data. The dependence on \u03c6Fs , which is just the peer-wise variance of data, is thus rather intuitive. A positive point is that \u03c6Fs is weighted by a factor smaller than the factor that weights the optimal potential. Another positive point is that this parameter can be computed from data, and among peers, without disclosing more data. Hence, it may be possible to estimate the loss against the centralized, k-means++ setting, taking as reference eq. (8). To gain insight in the leverage that Theorem 4 provides, Table 1 compares Dk-means++ to (Balcan et al., 2013)\u2019s (\u03b5 is the coreset approximation parameter), even though the latter approach would not be applicable to our restricted framework. To be fair, we assume that the algorithm used to cluster the coreset in (Balcan et al., 2013) is k-means++. We note that, considering the communication complexity and the number of data points shared, Algorithm 1 is a clear winner. In fact, Algorithm 1 can also win from the approximability standpoint. The dependence in \u03b5 prevents to fix it too small in (Balcan et al., 2013). Comparing the bounds in row (III) shows that if \u03b5 > 1/4, then we can also be better from the approximability standpoint if the spread satisfies \u03c6Fs = O(\u03c6opt). While this may not be feasible over arbitrary data, it becomes more realistic on several real-world scenarii, when Forgy nodes aggregate \u201clocal\u201d data with respect to features, e.g., state-wise insurance data, city-wise financial data, etc. When n increases, this also becomes more realistic.\nStreaming clustering We have access to a stream S, with an assumed finite size: S is a sequence of points a1,a2, ...,am. We authorise the computation / output of the clustering at the end of the stream, but the memory n allowed for all operations satisfies n < m, such as n = m\u03b1 with \u03b1 < 1 in (Ailon et al., 2009). We assume for simplicity that each point can be stored in one storage memory unit. Algorithm 2 (Sk-means++) presents our approach. It relies on the standard \u201ctrick\u201d of summarizing massive datasets via compact representations (synopses) before processing them (Indyk et al., 2014). The approximation properties of Sk-means++, proven using a reduction from k-variates++, hold regardless of the way synopses are built. They show that two key parameters may guide its choice: the spread of the synopses, analogous to the spread of Forgy nodes for distributed clustering, and the stretching properties of the synopses used as centers.\nTheorem 5 Let \u2118(a) .= arg mins\u2032\u2208S \u2016a \u2212 s\u2032\u201622,\u2200a \u2208 S. Let \u03c6\u2118s . = \u2211 a\u2208S \u2016\u2118(a) \u2212 a\u201622 be the spread of \u2118 on synopses set S. Let \u03b7 > 0 such that \u2118 is \u03b7-stretching on S. Then the expected\nAlgorithm 2 Sk-means++ Input: Stream S Step 1: S .= {(sj,mj), i \u2208 [n]} \u2190 SYNOPSIS(S, n); Step 2: for t = 1, 2, ..., k\n2.1: if t = 1 then let sj \u223cun S else sj \u223cqSt S s.t.\nqSt (sj) . = mjDt(sj) \u2211 j\u2032\u2208[n] mj\u2032Dt(sj\u2032) \u22121 ; (9) // Dt(sj) . = minc\u2208C \u2016sj \u2212 c\u201622;\n2.2: C\u2190 C \u222a {sj}; Output: Cluster centers C;\nAlgorithm 3 OLk-means++ Input: Minibatch Sj , current weighted centers C; Step 1: if j = 1 then let s \u223cu1 S1 else s \u223cqOj Sj s.t.\nqOj (s) . = Dt(s) \u2211 s\u2032\u2208Sj Dt(s \u2032) \u22121 ; (10) // Dt(s) . = minc\u2208C \u2016s\u2212 c\u201622;\nStep 2: C\u2190 C \u222a {s};\npotential of Sk-means++ on stream S satisfies ineq. (7) with\n\u03a6 . = (8 + 4\u03b7)\u03c6opt + 2\u03c6 \u2118 s ,\nHere, \u03c6opt is the optimal potential on stream S.\n(Proof in page 25) It is not surprising to see that Sk-means++ looks like a generalization of (Ailon et al., 2009) and almost matches it (up to the number of centers delivered) when k\u2032 k synopses are learned from k\u2032-means#. Yet, we rely on a different \u2014 and more general \u2014 analysis of its approximation properties. Table 1 compares properties of Sk-means++ to (Ailon et al., 2009) (\u03b7 relates to approximation of the k-means objective in inner loop).\nOn-line clustering This setting is probably the farthest from the original setting of the k-means++ algorithm. Here, points arrive in a sequence, finite, but of unknown size and too large to fit in memory (Liberty et al., 2014). We make no other assumptions \u2013 the sequence can be random, or chosen by an adversary. Therefore, the expected analysis we make is only with respect to the internal randomisation of the algorithm, i.e., for the fixed stream sequence as it is observed. We do not assume a feedback for learning (common for supervised learning); so, we do not assume that the algorithm has to predict a cluster for each point that arrives, yet it has to be easily modifiable to do so.\nOur approach is summarized in Algorithm 3 (OLk-means++), a variation of k-means++ which consists of splitting the stream S into minibatches Sj for j = 1, 2, ..., each of which is used to sample one center. u1 denotes the uniform distribution with support S1. Let R . = maxa,a\u2032\u2208S \u2016a \u2212 a\u2032\u20162( \u221e) be the diameter of S.\nTheorem 6 Let \u03c2 > 0 be the largest real such that the following conditions are met (for any A \u2208 Copt, j \u2265 1): for any set of at most k centers C, \u2211 a,a\u2032\u2208A \u2016a \u2212 a\u2032\u201622 \u2265 \u03c2 \u00b7 (|A| 2 ) R2 and\u2211\na\u2208A\u2229Sj \u2016a\u2212 c(a)\u201622 \u2265 \u03c2 \u00b7 \u2211 a\u2208A \u2016a\u2212 c(a)\u201622 (with c(a) defined in eq. (2)). Then the expected\npotential of OLk-means++on stream S satisfies ineq. (7) with\n\u03a6 . = ( 4 + 32\n\u03c22\n) \u00b7 \u03c6opt ,\nwhere \u03c6opt is the optimal potential on stream S.\n(Proof in page 26) Notice that loss function \u03c6(S,C) in eq. (2) implies the finiteness of S, and the existence of \u03c2 > 0; also, the second condition implies \u03c2 \u2264 1. In (Liberty et al., 2014), the clustering algorithm is required to have space and time at most polylog in the length of the stream. Hence, each minibatch can be reasonably large with respect to the stream \u2014 the larger they are, the larger \u03c2 . The knowledge of \u03c2 is not necessary to run OLk-means++; it is just a part of the approximation bound which quantifies the loss in approximation due to the fact that centers are computed from the partial knowledge of the stream. Table 1 compares properties of OLk-means++ to (Liberty et al., 2014) (we picked the fully on-line, non-heuristic algorithm). To compare the bounds, suppose that batches have the same size, b, so that log k = log(m/b). If batches are at least polylog size, up to what is hidden in the big-Oh notation, our approximation can be quite competitive when \u03c2 is large, e.g., if d is large and optimal clusters are not too small.\n4 Direct use of k-variates++ The most direct application domain of k-variates++ is differential privacy. Several algorithms have independently emphasised the idea that powerful mechanisms may be amended via a carefully designed noise mechanism to broaden their scope with new capabilities, without overly challenging their original properties. Examples abound (Hardt & Price, 2014; Kalai & Vempala, 2005; Chaudhuri et al., 2011; Chichignoud & Lousteau, 2014), etc. Few approaches are related to clustering, yet noise injected is big \u2014 the existence of a smaller, sufficient noise, was conjectured in (Nissim et al., 2007) \u2014 and approaches rely on a variety of assumptions or knowledge about the optimum (See Table 1) (Nissim et al., 2007; Wang et al., 2015). To apply k-variates++, we consider that \u2118t = Id,\u2200t, and assume 0 < R \u221e s.t. maxa,a\u2032\u2208A \u2016a \u2212 a\u2032\u20162 \u2264 R (a current assumption in the field (Dwork & Roth, 2014)).\nA general likelihood ratio bound for k-variates++ We show that the likelihood ratio of the same clustering for two \u201cclose\u201d instances is governed by two quantities that rely on the neighborhood function. Most importantly for differential privacy, when densities p(\u00b5a,\u03b8a) are carefully chosen, this ratio always \u2192 1 as a function of m, which is highly desirable for differential privacy. We let NNN(a) . = arg mina\u2032\u2208N \u2016a \u2212 a\u2032\u20162 denote the nearest neighbour of a in N, and let c(A) . = (1/|A|) \u00b7\u2211a\u2208A a.\nDefinition 7 We say that neighborhood in A is \u03b4w-spread for some \u03b4w > 0 iff for any N \u2286 A with |N| = k \u2212 1, and any B \u2286 A with |B| = |A| \u2212 1,\u2211\na\u2208B\n\u2016a\u2212 NNN(a)\u201622 \u2265 R2\n\u03b4w . (11)\nDefinition 8 We say that neighborhood in A is \u03b4s-monotonic for some \u03b4s > 0 iff the following holds. \u2200N \u2286 A with |N| \u2208 {1, 2, ..., k \u2212 1}, for any A \u2286 A\\N which is N-packed, we have:\u2211\na\u2208A\n\u2016a\u2212 NNN(a)\u201622\n\u2264 (1 + \u03b4s) \u00b7 \u2211 a\u2208A \u2016a\u2212 NNN\u222a{c(A)}(a)\u201622 . (12)\nSet A is said N-packed iff there exists x \u2208 Rd satisfying x = arg minc\u2208N\u222a{x} \u2016a\u2212 c\u201622, \u2200a \u2208 A. It is worthwhile remarking that as long as k < |A| \u221e, both 0 < \u03b4w \u221e and 0 < \u03b4s \u221e always exist. Informally, \u03b4w brings that the sum of squared distances to any subset of k\u2212 1 centers in A must not be negligible against the diameter R. \u03b4s yields a statement a bit more technical, but it roughly reduces to stating that adding one center to any set of at most k \u2212 1 points that are already close to each other should not decrease significantly the overall potential to the set of centers. Figure 2 provides a schematic view of the property, showing that the modifications of the potential can be very local, thus yielding small \u03b4s in ineq. (12). The following Theorem uses the definition of neighbouring samples: samples A and A\u2032 are neighbours, written A \u2248 A\u2032, iff they differ by one point. We also define P[C|A] to be the density of output C given input data A.\nTheorem 9 Fix \u2118t = Id (\u2200t) and densities p(\u00b5.,\u03b8.) having the same support \u2126 in k-variates++. Suppose there exists %(R) > 0 such that densities p(\u00b5.,\u03b8.) satisfy the following pointwise likelihood\nratio constraint:\np(\u00b5a\u2032 ,\u03b8a\u2032 )(x)\np(\u00b5a,\u03b8a)(x) \u2264 %(R) ,\u2200a,a\u2032 \u2208 A,\u2200x \u2208 \u2126 . (13)\nThen, there exists a function f(.) such that, for any \u03b4w, \u03b4s > 0 such that A is \u03b4w-spread and \u03b4s-monotonic, for any A\u2032 \u2248 A, for any k > 0 and any C \u2282 \u2126 of size k output by Algorithm k-variates++ on whichever of A or A\u2032, the likelihood ratio of C given A and A\u2032 is upperbounded as:\nP[C|A\u2032] P[C|A] \u2264 (1 + \u03b4w) k\u22121+f(k)\u00b7\u03b4w\u00b7(1 + \u03b4s)k\u22121\u00b7%(R) . (14)\n(Proof in page 28) Notice that Theorem 9 makes just one assumption (13) about the densities, so it can be applied in fairly general settings, such as for regular exponential families (Banerjee et al., 2005). These are a key choice because they extensively cover the domain of distortions for which the average is the population minimiser.\nAn (almost) distribution-free 1 + o(1) likelihood ratio We now show that if A is sampled i.i.d. from any distribution D which satisfies the mild assumption that it is locally bounded everywhere (or almost surely) in a ball, then with high probability the right-hand side of ineq. (14) is 1 + o(1) where the little-oh vanishes with m. The proof, of independent interest, involves an explicit bound on \u03b4w and \u03b4s.\nTheorem 10 Suppose A with |A| = m > 1 sampled i.i.d. from distribution D whose support contains a L2 ball B2(0, R) with density inside in between m > 0 and M \u2265 m. Let \u03c1D .= M/ m (\u2265 1). For any 0 < \u03b4 < 1/2, if (i) A \u2282 B(0, R) and (ii) the number of clusters k meets:\nk \u2264 \u03b4 2 4\u03c1D \u00b7 \u221am , (15)\nthen there is probability 1\u2212\u03b4 over the sampling of A that k-variates++, instantiated as in Theorem 9, satisfies P[C|A\u2032]/P[C|A] \u2264 1 + \u03c1k\nD \u00b7 g(m, k, d,R), \u2200A\u2032 \u2248 A, with\ng(m, k, d,R) . = 4\nm 1 4 + 1 d+1\n+\n( 64\nk 2 d\n)k \u00b7 %(2R)\nm . (16)\n(Proof in page 34) The key informal statement of Theorem 10 is that one may obtain with high probability some \u201cgood\u201d datasets A, i.e., for which \u03b4w, \u03b4s are small, under very weak assumptions about the domain at hand. The key point is that if one has access to the sampling, then one can resample datasets A until a good one comes.\nApplications to differential privacy Let M be any algorithm which takes as input A and k, and returns a set of k centers C. Let PM [C|A] denote the probability, over the internal randomisation of M , that M returns C given A and k (k, fixed, is omitted in notations). Following is the definition of differential privacy (Dwork et al., 2006), tailored for conciseness to our clustering problem.\nDefinition 11 M is -differentially private (DP) for k clusters iff for any neighbors A \u2248 A\u2032, set C of k centers,\nPM [C|A\u2032]/PM [C|A] \u2264 exp . (17)\nA relaxed version of -DP is ( , \u03b4)-DP, in which we require PM [C|A\u2032] \u2264 PM [C|A] \u00b7 exp + \u03b4; thus, -DP = ( , 0)-DP (Dwork & Roth, 2014). We show that low noise may be affordable to satisfy ineq. (17) using Laplace distribution, Lap(\u03c3/ \u221a 2). We refer to the Laplace mechanism as a popular mechanism which adds to the output of an algorithm a sufficiently large amount of Laplace noise to be -DP. We refer to (Dwork et al., 2006) for details, and assume from now on that data belong to a L1 ball B1(0, R).\nTheorem 12 Using notations and setting of Theorem 9, let\n\u0303 . = log ( exp( )\u2212 (1 + \u03b4w)k\u22121 f(k) \u00b7 \u03b4w \u00b7 (1 + \u03b4s)k\u22121 ) . (18)\nThen, k-variates++ with p(\u00b5.,\u03b8.) a product of Lap(\u03c31/ \u221a 2), for \u03c31 . = 2 \u221a\n2R/\u0303, both meets ineq. (17) and its expected potential satisfies ineq. (7) with\n\u03a6 = \u03a61 . = 8 \u00b7 ( \u03c6opt + mR2\n\u03032\n) . (19)\nOn the other hand, if we opt for \u03c32 . = 2 \u221a\n2kR/ , then k-variates++ is an instance of the Laplace mechanism and its expected potential satisfies ineq. (7) with\n\u03a6 = \u03a62 . = 8 \u00b7 ( \u03c6opt + mk2R2\n2\n) . (20)\n(Proof in page 41) A question is how do \u03c31 (resp. \u03a61) and \u03c32 (resp. \u03a62) compare with each other, and how do they compare to the state of the art (Nissim et al., 2007; Wang et al., 2015) (we only consider methods with provable approximation bounds of the global optimum). The key fact is that, if m is sufficiently large, then it happens that we can fix \u03b4w = O(1/m) and \u03b4s = O(1). The proof of Theorem 10 (page 34) and the experiments (page 44) display that such regimes are indeed observed. In this case, it is not hard to show that \u0303 = \u2126( + logm), granting \u03c31 = o(\u03c32) since\n\u03c31 = O\n( R\n+ log(m)\n) , (21)\ni.e. the noise guaranteeing ineq. (17) vanishes at 1/ log(m) rate. Consequently, in this regime, \u03a61 in eq. (19) becomes:\n\u03a61 = O\u0303 ( \u03c6opt +\nmR2\n( + logm)2\n) , (22)\nignoring all factors other than those noted. Thus, the noise dependence grows sublinearly in m. Since in this setting, unless all datapoints are the same, \u03b4w and \u03b4s for A and any possible neighbor\n4 5\n6 7\n8 9\n10 k 0\n10 20\n30 40\n50\np\n-4 -2 0 2 4 6 8\n\u03c1\u03c6\nA\u2032 are within 1 + o(1), it is also possible to overestimate \u03b4w and \u03b4s to still have \u03b4w = O(1/m) and \u03b4s = O(1) and grant -DP for k-variates++. Otherwise, the setting of Theorem 10 can be used to grant ( , \u03b4)-DP without any tweak. Table 1 compares k-variates++ to (Nissim et al., 2007; Wang et al., 2015) in this large sample regime, which is actually a prerequisite for (Nissim et al., 2007; Wang et al., 2015). NotationO\u2217 removes all dependencies in their model parameters (assumptions, model parameters, and \u03b4 for the ( , \u03b4)-DP in (Wang et al., 2015)), and \u03bb is the separability assumption parameter (Nissim et al., 2007)4. The approximation bounds in (Nissim et al., 2007) consider Wasserstein distance between (estimated / optimal) centers, and not the potential involving data points like us. To obtain bounds that can be compared, we have used the simple trick that the observed potential is, up to a constant, no more than the optimal potential plus a fonction of the distance between (estimated / optimal) centers. This somewhat degrades the bound, but not enough for the observed discrepancies with our bound to reverse or even vanish. It is clear from the bounds that the noise dependence is significantly in our favor, and our bound is also significantly better at least when k is not too large."}, {"heading": "5 Experiments", "text": "The experiments carried out are provided in extenso in the Appendix (from page 44). Dk-means++ vs k-means++ and k-means\u2016 (Bahmani et al., 2012) To address algorithms that can be reduced from k-variates++ (Section 3), we have tested Dk-means++ vs state of the art approach k-means\u2016; to be fair with Dk-means++, we use k-means++ seeding as the reclustering algorithm in k-means\u2016. Parameters are in line with (Bahmani et al., 2012). To control the spread of Forgy nodes \u03c6Fs (Theorem 4), each peer\u2019s initial data consists of points uniformly sampled in a random hyperrectangle in a space of d = 50 (expected number of peers points mi = 500, \u2200i). We sample\n4\u03bb is named \u03c6 in (Nissim et al., 2007). We use \u03bb to avoid confusion with clustering potentials.\npeers until a total of m \u2248 20000 point is sampled. Then, each point moves with p% chances to a uniformly sampled peer. We checked that \u03c6Fs blows up with p, i.e., >20 times for p = 50% with respect to p = 0. A remarkable phenomenon was the fact that, even when the number of peers n is quite large (dozens on average), Dk-means++ is able to beat both k-means++ and k-means\u2016, even for large values of p, as computed by ratio \u03c1\u03c6(H) . = 100 \u00b7 (\u03c6(Dk-means++) \u2212 \u03c6(H))/\u03c6(H) for H \u2208 {k-means++, k-means\u2016} (Figure 3). Another positive point is that the amount of data to compute a center for Dk-means++ is in average \u2248 n times smaller than k-means\u2016.\nThe fact that Dk-means++, which locally implements the biased seeding, may be able to beat k-means++, which globally implements this seeding technique, is not surprising, and in fact may come from the leverage brought by the compartmentalization of distributed data: as discussed in deeper details in page 47, this may even improve the approximability ratio of Dk-means++ so that it beats the AV bound. k-variates++ vs Forgy-DP and GUPT To address algorithms that can be obtained via a direct use of k-variates++ (Section 4), we have tested it in a differential privacy framework vs state of the art approach GUPT (Mohan et al., 2012). We let = 1 in our experiments. We also compare it to Forgy DP (F-DP), which is just Forgy initialisation in the Laplace mechanism, with noise rate (standard dev.) \u221d kR/ . In comparison, the noise rate for GUPT is \u221d kR/(` ) at the end of its aggregation process, where ` is the number of blocks. Table 2 gives results for the average (over the choices of k) parameters used, k, \u0303, and ratio \u03c1\u2032\u03c6 where \u03c1 \u2032 \u03c6(H) . = \u03c6(H)/\u03c6(k-variates++) \u2014 values above 1 indicate better results for k-variates++. We use \u0303 as the equivalent for k-variates++, i.e. the value that guarantees ineq. (17). From Theorem 12, when \u0303 > , this brings a smaller noise magnitude, desirable for clustering. The obtained results show that k-variates++ becomes more of a contender with increasing m, but its relative performance tends to decrease with increasing k. This is in accordance with the \u201cgood\u201d regime of Theorem 12. Results on synthetic domains display the same patterns, along with the fact that relative performances of k-variates++ improves with d, making it a relevant choice for \u201dbig\u201d domains.\nIn fact, extensive experiments on synthetic data (page 44) show that intuitions regarding the sublinear noise regime in eq. (22) are experimentally observed, and furthermore they may happen for quite small values of m."}, {"heading": "6 Discussion and Conclusion", "text": "We first show in this paper that the k-means++ analysis of Arthur and Vassilvitskii can be carried out on a significantly more general scale, aggregating various clustering frameworks of interest and for which no trivial adaptation of k-means++ was previously known. Our contributions stand at two levels: (i) we provide the \u201cmeta\u201d algorithm, k-variates++, and two key results, one on its\napproximation abilities of the global optimum, and one on the likelihood ratio of the centers it delivers. We do expect further applications of these results, in particular to address several other key clustering problems: stability, generalisation and smoothed analysis (Arthur et al., 2011; von Luxburg, 2010); (ii) we provide two examples of application. The first is a reduction technique from k-variates++, which shows a way to obtain straight approximabilty results for other clustering algorithms, some being efficient proxies for the generalisation of existing approaches (Ailon et al., 2009). The second is a direct application of k-variates++ to differential privacy, exhibiting a noise component significantly better than existing approaches (Nissim et al., 2007; Wang et al., 2015).\nWe have not discussed here the possibility to replace the L22 distortion which computes the potential by elements from large and interesting classes \u2014 clustering being a huge practical problem, it is indeed reasonable to tailor the distortion to the application at hand. One example are Bregman divergences, that fail simple metric transforms (Acharyya et al., 2013). Another example are total divergences, that fail the simple computation of the population minimizers (Nock et al., 2016; Liu et al., 2012). Some do not even admit population minimizers in closed form (Nielsen & Nock, 2015). It turns out that k-variates++, and its good approximation properties, can be extended to such cases (see page 42) for total Jensen divergence (Nielsen & Nock, 2015)."}, {"heading": "7 Acknowledgments", "text": "Thanks are due to Stephen Hardy, Guillaume Smith, Wilko Henecka and Max Ott for stimulating discussions and feedback on the subject. Nicta is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Center of Excellence Program."}, {"heading": "Appendix \u2014 Table of contents", "text": "Appendix on proofs Pg 19 Proof of Theorem 2 Pg 19 Proof of Lemma 3 Pg 22 Comments on Table 1 Pg 22 Proofs of Theorems 4, 5 and 6 Pg 23 \u21aa\u2192 Proof of Theorem 4 Pg 23 \u21aa\u2192 Proof of Theorem 5 Pg 25 \u21aa\u2192 Proof of Theorem 6 Pg 26 Proof of Theorem 9 Pg 28 Proof of Theorem 10 Pg 34 Proof of Theorem 12 Pg 41 Extension to non-metric spaces Pg 42"}, {"heading": "Appendix on experiments Pg 44", "text": "Experiments on Theorem 12 and the sublinear noise regime Pg 44 Experiments with Dk-means++, k-means++ and k-means\u2016 Pg 47 Experiments with k-variates++ and GUPT Pg 51"}, {"heading": "8 Appendix on Proofs", "text": "Several proofs rely on properties of the k-means++ algorithm that are not exploited in the proof of (Arthur & Vassilvitskii, 2007). We assume here the basic knowledge of the proof technique of (Arthur & Vassilvitskii, 2007)."}, {"heading": "Proof of Theorem 2", "text": "Let A denote a subset of A, and c(A) .= (1/|A|) \u00b7\u2211a\u2208A a the barycenter of A. It is well known that c(A) = arg mina\u2032\u2208Rd \u2211 a\u2208A \u2016a\u2212 a\u2032\u201622, so the potential of A,\n\u03c6(A) . = \u2211 a\u2208A \u2016a\u2212 c(A)\u201622 (23)\nis just the optimal potential of A if A defines a cluster in the optimal clustering. We also define the noisy potential of A as:\n\u03c6N(A) . = \u2211 a\u2208A \u222b \u2126a \u2016x\u2212 c(A)\u201622dpa(x) . (24)\nThe proof of Theorem 2 follows the same path as the proof of Theorem 3.1 in (Arthur & Vassilvitskii, 2007). Instead of reproducing the proof, we shall assume basic knowledge of the original proof and will just provide the side Lemmata that are sufficient for our more general result. The first Lemma is a generalization of Lemma 3.2 in (Arthur & Vassilvitskii, 2007).\nLemma 13 Let Copt denotes the optimal partition of A according to eq. (2). Let A be an arbitrary cluster in Copt. Let C be a single-cluster clustering whose center is chosen at random by one step of Algorithm k-variates++ (i.e. for t = 1). Then\nE[\u03c6(A)] = \u03c6opt(A) + \u03c6Nopt(A) . (25)\nProof The expected potential of cluster A is\nE[\u03c6(A;C = \u2205)]\n= 1 |A| \u00b7 \u2211 a0\u2208A \u222b \u2126a0 \u2211 a\u2208A \u2016a\u2212 x\u201622dpa0(x) = 1\n|A| \u00b7 \u2211 a0\u2208A \u222b \u2126a0 \u2211 a\u2208A \u2016a\u2212 c(A) + c(A)\u2212 x\u201622dpa0(x)\n= 1 |A| \u00b7 \u2211 a0\u2208A\n( \u2211 a\u2208A \u2016a\u2212 c(A)\u201622 + |A| \u00b7 \u222b \u2126a0 \u2016x\u2212 c(A)\u201622dpa0(x)\n+2 \u2211 a\u2208A\u3008a\u2212 c(A), c(A)\u2212 \u222b \u2126a0 xdpa0(x)\u3009\n)\n= 1 |A| \u00b7 \u2211 a0\u2208A\n \u2211 a\u2208A \u2016a\u2212 c(A)\u201622 + |A| \u00b7 \u222b \u2126a0 \u2016x\u2212 c(A)\u201622dpa0(x) +2\u3008 \u2211 a\u2208A\na\u2212 |A|c(A)\ufe38 \ufe37\ufe37 \ufe38 =0 , c(A)\u2212 a0\u3009  =\n\u2211 a\u2208A \u2016a\u2212 c(A)\u201622 + \u2211 a\u2208A \u222b \u2126a0 \u2016x\u2212 c(A)\u201622dpa(x)\n= \u03c6opt(A) + \u03c6 N opt(A) ,\nas claimed.\nWhen pa is a Dirac anchored at a, we recover Lemma 3.2 in (Arthur & Vassilvitskii, 2007). The following Lemma generalizes Lemma 3.3 in (Arthur & Vassilvitskii, 2007).\nLemma 14 Suppose that the optimal clustering Copt is \u03b7-probe approximable. Let A be an arbitrary cluster inCopt, and letC be an arbitrary clustering with centers C. Suppose that the reference point a chosen according to (1) in Step 2.1 is in A. Then the random point x picked in Step 2.2 brings an expected potential that satisfies\nE[\u03c6(A)] \u2264 (6 + 4\u03b7) \u00b7 \u03c6opt(A) + 2 \u00b7 \u03c6Nopt(A) . (26) Proof Let us denote c?(u) .= arg minx\u2208C \u2016u\u2212x\u201622 (since C 6= Copt in general, c?(u) 6= copt(u)), and D(a) .= \u2016a \u2212 c?(a)\u201622 the contribution of a \u2208 A to the k-means potential defined by C. We have, using Lemma 3.3 in (Arthur & Vassilvitskii, 2007) and Lemma 13,\nEx[\u03c6(A;C \u222a {x})] = \u2211 a0\u2208A Dt(a0)\u2211 a\u2208ADt(a) \u00b7 \u2211 a\u2208A \u222b \u2126a0 min{D(a), \u2016a\u2212 x\u201622}dpa0(x) . (27)\nThe triangle inequality gives, for any a \u2208 A,\u221a Dt(a0)\n. = \u2016\u2118t(a0)\u2212 c?(\u2118t(a0))\u20162 \u2264 \u2016\u2118t(a0)\u2212 c?(\u2118t(a))\u20162 \u2264 \u2016\u2118t(a0)\u2212 \u2118t(a)\u20162 + \u2016\u2118t(a)\u2212 c?(\u2118t(a))\u20162 ; (28)\nsince (a+ b)2 \u2264 2a2 + 2b2, then Dt(a0) \u2264 2\u2016\u2118t(a0)\u2212 \u2118t(a)\u201622 + 2Dt(a), and so, after averaging over A,\nDt(a0) \u2264 2 |A| \u2211 a\u2208A \u2016\u2118t(a0)\u2212 \u2118t(a)\u201622 + 2 |A| \u2211 a\u2208A Dt(a) , (29)\nand eq. (27) can be upperbounded as:\nEx[\u03c6(A;C \u222a {x})] \u2264 2 |A| \u2211 a0\u2208A \u2211 a\u2208A \u2016\u2118t(a0)\u2212 \u2118t(a)\u201622\u2211 a\u2208ADt(a) \u00b7 \u2211 a\u2208A \u222b \u2126a0 min{D(a), \u2016a\u2212 x\u201622}dpa0(x)\n+ 2 |A| \u2211 a0\u2208A \u2211 a\u2208ADt(a)\u2211 a\u2208ADt(a) \u00b7 \u2211 a\u2208A \u222b \u2126a0 min{D(a), \u2016a\u2212 x\u201622}dpa0(x)\n\u2264 2|A| \u2211 a0\u2208A \u2211 a\u2208AD(a)\u2211 a\u2208ADt(a) \u00b7 \u2211 a\u2208A\n\u2016\u2118t(a0)\u2212 \u2118t(a)\u201622\ufe38 \ufe37\ufe37 \ufe38 . =P1 + 2\n|A| \u2211 a0\u2208A \u2211 a\u2208A \u222b \u2126a0\n\u2016a\u2212 x\u201622dpa0(x)\ufe38 \ufe37\ufe37 \ufe38 . =P2 . (30)\nWe bound the two potentials P1 and P2 separately, starting with P1. Fix any a0 \u2208 A. If \u2211 a\u2208A \u2016\u2118t(a)\u2212 \u2118t(a0)\u201622 = 0, then trivially(\u2211 a\u2208A D(a) ) \u00b7 (\u2211 a\u2208A \u2016\u2118t(a0)\u2212 \u2118t(a)\u201622 ) \u2264 (1 + \u03b7) \u00b7 (\u2211 a\u2208A Dt(a) ) \u00b7 (\u2211 a\u2208A \u2016a0 \u2212 a\u201622 ) ,(31)\nsince the right-hand side cannot be negative. If \u2211 a\u2208A \u2016\u2118t(a) \u2212 \u2118t(a0)\u201622 6= 0, then since \u2118t is \u03b7-stretching, we have:\u2211 a\u2208A \u2016a\u2212 c?(a)\u201622\u2211 a\u2208A \u2016a\u2212 a0\u201622 \u2264 (1 + \u03b7) \u00b7 \u2211 a\u2208A \u2016\u2118t(a)\u2212 c?(\u2118t(a))\u201622\u2211 a\u2208A \u2016\u2118t(a)\u2212 \u2118t(a0)\u201622 , (32)\nwhich is exactly ineq. (31) after rearranging the terms. Ineq (31) implies\nP1 \u2264 2(1 + \u03b7) \u00b7 1 |A| \u2211 a0\u2208A \u2211 a\u2208A \u2016a0 \u2212 a\u201622\n= 4(1 + \u03b7) \u00b7 \u03c6opt(A) , (33)\nwhere the equality follows from (Arthur & Vassilvitskii, 2007), Lemma 3.2. Also, Lemma 13 brings\nP2 = 2 \u00b7 1 |A| \u2211 a0\u2208A \u222b \u2126a0 \u2211 a\u2208A \u2016a\u2212 x\u201622dpa0(x)\n= 2\u03c6opt(A) + 2\u03c6 N opt(A) . (34)\nWe therefore get\nEx[\u03c6(A;C \u222a {x})] \u2264 (6 + 4\u03b7) \u00b7 \u03c6opt(A) + 2 \u00b7 \u03c6Nopt(A) , (35)\nas claimed.\nAgain, we recover Lemma 3.3 in (Arthur & Vassilvitskii, 2007) when pa is a Dirac and the probe function \u2118 = Id. The rest of the proof of Theorem 2 consists of the same steps as Theorem 3.1 in (Arthur & Vassilvitskii, 2007), after having remarked that \u03c6Nopt(A) can be simplified:\n\u03c6Nopt(A) = \u2211 a\u2208A \u222b \u2126a0 \u2016x\u2212 c(A)\u201622dpa(x)\n= \u2211 a\u2208A \u222b \u2126a0 \u2016x\u201622dpa(x)\u2212 2\u3008c(A),\u00b5a\u3009+ \u2016c(A)\u201622\n= \u2211 a\u2208A \u222b \u2126a0 \u2016x\u2212 \u00b5a\u201622dpa(x) + \u2016\u00b5a\u201622 \u2212 2\u3008c(A),a\u3009+ \u2016c(A)\u201622\n= \u2211 a\u2208A { tr (\u03a3a) + \u2016\u00b5a \u2212 c(A)\u201622 } = \u03c6bias(A) + \u03c6var(A) . (36)"}, {"heading": "Proof of Lemma 3", "text": "The proof is a simple application of the Fre\u0301chet-Crame\u0301r-Rao-Darmois bound. Consider the simple case k = 1 and a spherical Gaussian noise for p with a single point in A. Renormalize both sides of (7) by m .= |A| so that (1/m)\u2211a\u2208A tr (\u03a3a) = tr (\u03a3). One sees that the left hand side of ineq. (7) is just an estimator of the variance of pa, which, by Fre\u0301chet-Darmois-Crame\u0301r-Rao bound, has to be at least the inverse of the Fisher information, that is in this case, the trace of the covariance matrix, i.e. tr (\u03a3)."}, {"heading": "Comments on Table 1", "text": "(Wang et al., 2015) are concerned with approximating subspace clustering, and so they are using a very different potential function, which is, between two subspaces S and S\u2032, d(S, S\u2032) = \u2016UU> \u2212 U\u2032U\u2032>\u2016F , where U (resp. U\u2032) is an orthonormal basis for S (resp. S\u2032). To obtain an idea of the approximation on the k-means clustering problem that their technique yields, we compute \u03c6 in the projected space, using the fact that, because of the triangle inequality and the fact that projections are linear and do not increase norms,\n\u2016projU(a)\u2212 projU\u2032(a\u2032)\u20162 = \u2016(projU(a)\u2212 projU(a\u2032)) + (projU(a\u2032)\u2212 projU\u2032(a\u2032))\u20162 (37) \u2264 \u2016projU(a)\u2212 projU(a\u2032)\u20162 + \u2016projU(a\u2032)\u2212 projU\u2032(a\u2032))\u20162(38) \u2264 \u2016projU(a)\u2212 projU(a\u2032)\u20162 + 2\u2016a\u2032\u20162 . (39)\nTo account for the approximation in the inequalities, we then discard the rightmost term, replacing therefore \u2016projU(a)\u2212 projU\u2032(a\u2032)\u20162 by \u2016projU(a)\u2212 projU(a\u2032)\u20162, which amounts, in the approximation bounds, to remove the dependence in the dimension. At this price, and using the trick to transfer the wasserstein distance between centers to L22 potential between points to cluster centers, we obtain the approximation bound in (\u03b2) of Table 1. While it has to be used with care, its main interest is in showing that the price to pay because of the noise component is in fact not decreasing in m."}, {"heading": "Proofs of Theorems 4, 5 and 6", "text": "The proof of these Theorems uses a reduction from k-variates++ to the corresponding algorithms, meaning that there exists particular probe functions and densities for which the set of centers delivered by k-variates++ is the same as the one delivered by the corresponding algorithms.\nDefinition 15 Let H (parameters omitted) be any hard membership k-clustering algorithm. We way that k-variates++ reduces to H iff there exists data, densities and probe functions depending on the instance of H such that, in expectation over the internal randomisation of H, the set of centers delivered by H are the same as the ones delivered by k-variates++. We note it\nk-variates++ H . (40)\nHence, whenever k-variates++ H, Theorem 2 immediately gives a guarantee for the approximation of the global optimum in expectation for H, but this requires the translation of the parameters involved in \u03a6 in ineq. (7) to involve only parameters from H. In all our examples, this translation poses no problem at all."}, {"heading": "Proof of Theorem 4", "text": "Figure 4 presents the architecture of message passing in the Dk-means++/PDk-means++ framework. We first focus on the protected scheme, Dk-means++. We reduce k-variates++ to Algorithm\n1 using identity probe functions: \u2118t = Id, \u2200t. The trick in reduction relies on the densities. We let p\u00b5a,\u03b8a be uniform over the subset Ai to which a belongs. Thus, the support of densities is discrete, and C is a subset of A; furthermore, the probability qt(a) that a \u2208 Ai is chosen at iteration t in k-variates++ actually simplifies to a convenient expression:\nqt(a) = q D ti \u00b7 ui , (41)\nwhere we recall that\nqDti . =\n{ Dt(Ai) \u00b7 ( \u2211 j Dt(Aj))\n\u22121 if t > 1 (1/n) otherwise . (42)\nHence, picking a can be equivalently done by first picking Ai using qDt , and then, given the i chosen, sampling uniformly at random a in Ai, which is what Forgy nodes do. We therefore get the equivalence between Algorithm 1 and k-variates++ as instantiated.\nLemma 16 With data, densities and probes defined as before, k-variates++ Dk-means++. To get the approximability ratio of Dk-means++, we translate the parameters of \u03a6 in ineq. (7). First, since (a+ b)2 \u2264 2a2 + 2b2,\n\u03c6bias . = \u2211 a\u2208A \u2016\u00b5a \u2212 copt(a)\u201622\n= \u2211 i\u2208[n] \u2211 a\u2208Ai \u2016c(Ai)\u2212 copt(a)\u201622 (43)\n= \u2211 i\u2208[n] \u2211 a\u2208Ai \u2016c(Ai)\u2212 a+ a\u2212 copt(a)\u201622\n\u2264 2 \u2211 i\u2208[n] \u2211 a\u2208Ai \u2016c(Ai)\u2212 a\u201622 + 2 \u2211 a\u2208A \u2016a\u2212 copt(a)\u201622\n= 2\u03c6Fs + 2\u03c6opt . (44)\nFurthermore,\n\u03c6var . = \u2211 a\u2208A tr (\u03a3a)\n= \u2211 a\u2208A \u222b \u2126a \u2016x\u2212 \u00b5a\u201622dpa(x)\n= \u2211 i\u2208[n] \u2211 a\u2208Ai \u2211 a\u2032\u2208Ai 1 mi \u00b7 \u2016a\u2032 \u2212 c(Ai)\u201622\n= \u2211 i\u2208[n] \u2211 a\u2208Ai \u2016a\u2212 c(Ai)\u201622 = \u03c6Fs . (45)\nThere remains to plug ineq. (44) and eq. (45) in Theorem 2, along with \u03b7 = 0 (since \u2118 = Id), to get E[\u03c6(A;C)] \u2264 (2 + log k) \u00b7 (10\u03c6opt + 6\u03c6s), as in Theorem 4.\nThe private version, PDk-means++, follows immediately by leaving \u03c6var in \u03a6 instead of carrying eq. (45). This ends the proof of Theorem 4.\n\u2118 ="}, {"heading": "Proof of Theorem 5", "text": "The proof proceeds in the same way as for Theorem 4. The probe function (the same for every iteration, \u2118t = \u2118,\u2200t) is already defined in the statement of Theorem 5, from the definition of synopses. The distributions p\u00b5a,\u03b8a are Diracs anchored at the probe (synopses) locations. The centers chosen in k-variates++ are thus synopses, and it is not hard to check that the probability to pick a synopsis sj at iteration t factors in the same way as in the definition of qSt in eq. (9). We therefore get the equivalence between Algorithm 2 and k-variates++ as instantiated.\nLemma 17 With data, densities and probes defined as before, k-variates++ Sk-means++. The proof of the approximation property of Sk-means++ then follows from the fact that \u03c6var = 0 (Diracs) and\n\u03c6bias . = \u2211 a\u2208A \u2016\u00b5a \u2212 copt(a)\u201622\n= \u2211 a\u2208A \u2016\u2118(a)\u2212 copt(a)\u201622\n= \u2211 a\u2208A \u2016\u2118(a)\u2212 a+ a\u2212 copt(a)\u201622\n\u2264 2 \u2211 a\u2208A \u2016\u2118(a)\u2212 a\u201622 + 2 \u2211 a\u2208A \u2016a\u2212 copt(a)\u201622\n= 2 \u2211 a\u2208S \u2016\u2118(a)\u2212 a\u201622 + 2 \u2211 a\u2208S \u2016a\u2212 copt(a)\u201622 = 2\u03c6\u2118s + 2\u03c6opt (46)\n(using again (a+ b)2 \u2264 2a2 + 2b2). Using Theorem 2, this brings the statement of the Theorem.\nFigure 5 shows that the \u201dquality\u201d of the probe function (spread \u03c6\u2118s , stretching factor \u03b7) stem from the quality of the Voronoi diagram induced by the synopses in S."}, {"heading": "Proof of Theorem 6", "text": "The proof proceeds in the same way as for Theorem 4. The the reduction from k-variates++ to OLk-means++ relies on two things: first, the uniform choice of the first center in k-means++ can be replaced by picking the center uniformly in any subset of the data: it does not change the expected approximation properties of the algorithm (this comes from Lemma 3.4 in (Arthur & Vassilvitskii, 2007)); therefore, the choice q1 . = um in k-variates++ can be replaced with q1 . = u1 (uniform with support A1). Second, a particular probe function needs to be devised, sketched in Figure 6. Basically, all probe functions of a minibatch are the same: each point in the minibatch is probed to itself, while points occurring outside the minibatch are probed to their closest center. The reduction proceeds in the following steps: we first let A be the complete set of points in the stream S. Then, we let Aj denote the set of points of minibatch Sj . Remark that minibatch Aj occurs in the stream before Aj\u2032 for j < j\u2032, and minibatches induce a partition of A. Let j(t) denote the batch related to iteration t in k-variates++. We define the following probe function \u2118t(a) in k-variates++, letting Aj the minibatch to which a belongs (we do not necessarily have j = j(t)):\n\u2022 if j = j(t), then \u2118t(a) .= a;\n\u2022 else \u2118t(a) .= arg minc\u2208C \u2016a\u2212 c\u201622 (remark that |C| \u2265 1 in this case). Finally, densities p(\u00b5.,\u03b8.) are Diracs anchored at selected points, like in k-means++. We get the equivalence between Algorithm 3 and k-variates++ as instantiated.\nLemma 18 With data, densities and probes defined as before, k-variates++ OLk-means++. The proof is immediate, since each minibatch is hit by a center exactly once in OLk-means++, and when one subset Aj is hit by a center, then the probe function makes that no other center can be sampled again from Aj (all contributions to the density qt are then zero in Aj). We now finish the proof of Theorem 6 by showing the same approximability ratio for k-variates++ as reduced. Because optimal clusters are \u03c2-wide with respect to stream S, we have\n1 |A| \u00b7 \u2211 a,a\u2032\u2208A \u2016a\u2212 a\u2032\u201622 \u2265 \u03c2 \u00b7R .\nRecall that c(A) .= (1/|A|) \u00b7\u2211a\u2208A a. For any a0 \u2208 A, it holds that: 1\n|A| \u2212 1 \u00b7 \u2211 a\u2208A \u2016a\u2212 a0\u201622 \u2265 1 |A| \u2212 1 \u00b7 \u2211 a\u2208A \u2016a\u2212 c(A)\u201622 (47)\n= 1 |A| \u2212 1 \u00b7 ( 1 2|A| \u00b7 \u2211 a,a\u2032\u2208A \u2016a\u2212 a\u2032\u201622 ) (48)\n= 1 4 \u00b7 2|A|(|A| \u2212 1) \u00b7 \u2211 a,a\u2032\u2208A \u2016a\u2212 a\u2032\u201622 \u2265 \u03c2 4 \u00b7R . (49)\nIneq. (47) holds because c(A) is the population minimizer for optimal cluster A (see e.g., (Arthur & Vassilvitskii, 2007), Lemma 2.1). Since probes are points of A,\n\u03c6(\u2118j(A); {\u2118j(a0)}) \u2264 |A| \u00b7R\n\u2264 4|A| \u03c2(|A| \u2212 1) \u00b7 \u2211 a\u2208A \u2016a\u2212 a0\u201622 . (50)\nOn the other hand, we have: \u03c6(\u2118t(A);C) = \u2211\na\u2208A\u2229Sj\n\u2016a\u2212 c(a)\u201622 , (51)\nbut since minibatches are \u03c2 accurate, \u2211 a\u2208A\u2229Sj \u2016a\u2212 c(a)\u201622 \u2265 \u03c2 \u00b7 \u2211 a\u2208A \u2016a\u2212 c(a)\u201622. Therefore, for any a0 \u2208 A, \u03c6(\u2118t(A);C)\n\u03c6(\u2118t(A); {\u2118t(a0)}) \u2265\n( \u03c22(|A| \u2212 1)\n4|A|\n) \u00b7 \u2211 a\u2208A \u2016a\u2212 c(a)\u201622\u2211 a\u2208A \u2016a\u2212 a0\u201622\n=\n( \u03c22(|A| \u2212 1)\n4|A|\n) \u00b7 \u03c6(A;C) \u03c6(A; {a0}) . (52)\nIn other words, probe functions are \u03b7-stretching, for any \u03b7 satisfying:\n\u03b7 \u2265 4|A| \u03c22(|A| \u2212 1) \u2212 1 , (53)\nand they are therefore \u03b7-stretching for \u03b7 = 8/\u03c22 \u2212 1. There remains to check that, because of the densities chosen,\n\u03c6bias = \u03c6opt , (54) \u03c6var = 0 . (55)\nThis ends the proof of Theorem 6."}, {"heading": "Proof of Theorem 9", "text": "To simplify notations in the proof, we let pa(x) denote the value of density p(\u00b5a,\u03b8a) on some x \u2208 \u2126. Let us denote Seq(n : k) the number of sequences of integers in set {1, 2, ..., n} having exactly k elements, whose cardinal is |Seq(n : k)| = n!/(n\u2212 k)!. For any sequence I \u2208 Seq(n : k), we let Ii denote its ith element. For any set C . = {c1, c2, ..., ck} returned by Algorithm k-variates++with input instance set A .= {a1,a2, ...,an} \u2282 \u2126, the density of C given A is:\nP[C|A] = \u2211 \u03c3\u2208Sk \u2211 I\u2208Seq(n:k) p(\u03c3, I,C|A) , (56)\nwhere Sk denotes the symmetric group on k elements, and the following shorthand is used:\np(\u03c3, I,C|A) .= k\u220f i=1 qi(aIi)paIi (c\u03c3(i)) , (57)\nwhere qi is computed using eq. (1) and taking into account the modification due to the choice of each Ij for j < i in the sequence I .\nIn the following, we let A and A\u2032 denote two sets of points that differ from one a (they have the same size), say an \u2208 A and a\u2032n \u2208 A\u2032, an 6= a\u2032n. We analyze:\nP[C|A\u2032] P[C|A] =\n\u2211 \u03c3\u2208Sk \u2211 I\u2208Seq(n:k) p(\u03c3, I,C|A\u2032)\u2211\n\u03c3\u2208Sk\n\u2211 I\u2208Seq(n:k) p(\u03c3, I,C|A) . (58)\nUsing the definition of q(.), we refine p(\u03c3, I,C|A) as\np(\u03c3, I,C|A) = N(I)\u220fk i=1 M(I i|A) \u00b7 k\u220f i=1 paIi (c\u03c3(i)) , (59)\nwhere\nN(I) . = j\u220f i=2 \u2016aIi \u2212 NNIi(aIi)\u201622 , (60)\nM(I i|A) .= {\nn if i = 1\u2211n j=1 \u2016aj \u2212 NNIi(aj)\u201622 otherwise , (61)\nand I i is the prefix sequence I1, I2, ..., Ii\u22121, and NNIi(a) . = arg minj\u2264i\u22121 \u2016a\u2212 aIj\u20162 is the nearest neighbor of a in the prefix sequence. Notice that there is a factor 1/m for q(.) at the first iteration that we omit in N(I) since it disappears in the ratio in eq. (58).\nWe analyze separately each element in (59), starting with N(I). We define the swapping operation s`(I) that returns the sequence in which aI` and aI`+1 are permuted, for 1 \u2264 ` \u2264 k \u2212 1. This incurs non-trivial modifications in N(s`(I)) compared to N(I), since the nearest neighbors of aI` and aI`+1 may change in the permutation:\nN(s`(I)) = `\u22121\u220f i=2 \u2016aIi \u2212 NNIi(aIi)\u201622\n\u00b7 \u2016aI`+1 \u2212 NNI`(aI`+1)\u201622 \u00b7 \u2016aI` \u2212 NNI`\u222a{I`+1}(aI`)\u201622\ufe38 \ufe37\ufe37 \ufe38 6=\u2016aI`\u2212NNI` (aI` )\u2016 2 2\u00b7\u2016aI`+1\u2212NNI`+1 (aI`+1 )\u2016 2 2\n\u00b7 k\u220f\ni=`+2\n\u2016aIi \u2212 NNIi(aIi)\u201622 (62)\n(I \u222a {j} indicates that element j is put at the end of the sequence). We want to quantify the maximal increase in N(s`(I)) compared to N(I). The following Lemma shows that the maximal increase ratio is actually a constant, and thus does not depend on the data.\nLemma 19 The following holds true:\nN(s1(I)) = N(I) , (63) N(s`(I)) \u2264 (1 + \u03b7)2N(I) ,\u22002 \u2264 ` \u2264 k \u2212 1 . (64)"}, {"heading": "Here, 0 \u2264 \u03b7 \u2264 3 is a constant.", "text": "The proof stems directly from the following Lemma.\nLemma 20 For any non-empty N \u2286 A and x \u2208 \u2126, let NNN(x) denote the nearest neighbor of x in N. There exists a constant 0 \u2264 \u03b7 \u2264 3 such that for any ai,aj \u2208 A and any nonempty subset N \u2286 A\\{ai,aj},\n\u2016ai \u2212 NNN(ai)\u20162 \u2016ai \u2212 NNN\u222a{aj}(ai)\u20162 \u2264 (1 + \u03b7) \u00b7 \u2016aj \u2212 NNN(aj)\u20162\u2016aj \u2212 NNN\u222a{ai}(aj)\u20162 . (65)\nProof Since \u2016aj \u2212 NNN\u222a{ai}(aj)\u20162 \u2264 \u2016aj \u2212 NNN(aj)\u20162, the proof is true for \u03b7 = 0 when NNN(ai) = NNN\u222a{aj}(ai). So suppose that NNN(ai) 6= NNN\u222a{aj}(ai), implying NNN\u222a{aj}(ai) = aj . We distinguish two cases. Case 1/2, if NNN\u222a{ai}(aj) = ai, then we are reduced to showing that \u2016ai \u2212 NNN(ai)\u20162 \u2264 (1 + \u03b7)\u2016aj\u2212NNN(aj)\u20162 under the conditions (C) that N\u2229B(ai, \u2016ai\u2212aj\u20162) = \u2205 and N\u2229B(aj, \u2016ai\u2212 aj\u20162) = \u2205. Here, B(a, r) denotes the open ball of center a and radius R. The triangle inequality and conditions (C) bring\n\u2016ai \u2212 NNN(ai)\u20162 \u2264 \u2016ai \u2212 aj\u20162 + \u2016aj \u2212 NNN(ai)\u20162 \u2264 \u2016aj \u2212 NNN(aj)\u20162 + \u2016aj \u2212 NNN(ai)\u20162 . (66)\nIf NNN(ai) = NNN(aj) then the inequality holds for \u03b7 = 1. Otherwise, suppose that \u2016aj \u2212 NNN(ai)\u20162 > 3\u2016aj \u2212 NNN(aj)\u20162. The triangle inequality yields again \u2016aj \u2212 NNN(ai)\u20162 \u2264 \u2016aj \u2212 ai\u20162 + \u2016ai \u2212 NNN(ai)\u20162, and so we have the inequality:\n3\u2016aj \u2212 NNN(aj)\u20162 < \u2016aj \u2212 ai\u20162 + \u2016ai \u2212 NNN(ai)\u20162 , (67)\nand since (C) holds, \u2016aj \u2212 NNN(aj)\u20162 \u2265 \u2016aj \u2212 ai\u20162 which implies\n\u2016aj \u2212 NNN(aj)\u20162 < 1\n2 \u00b7 \u2016ai \u2212 NNN(ai)\u20162 . (68)\nOn the other hand, the triangle inequality brings again\n\u2016ai \u2212 NNN(aj)\u20162 \u2264 \u2016ai \u2212 aj\u20162 + \u2016aj \u2212 NNN(aj)\u20162 \u2264 2 \u00b7 \u2016aj \u2212 NNN(aj)\u20162 (69) < 2 \u00b7 1\n2 \u00b7 \u2016ai \u2212 NNN(ai)\u20162 = \u2016ai \u2212 NNN(ai)\u20162 , (70)\na contradiction since \u2016ai \u2212 NNN(ai)\u20162 \u2264 \u2016ai \u2212 al\u20162, \u2200al \u2208 N by definition. Ineq. (69) uses (C) and ineq. (70) uses ineq. (68). Hence, if NNN(ai) 6= NNN(aj) then since \u2016aj \u2212 NNN(ai)\u20162 \u2264 3\u2016aj \u2212 NNN(aj)\u20162, ineq. (66) brings \u2016ai\u2212 NNN(ai)\u20162 \u2264 4 \u00b7 \u2016aj \u2212 NNN(aj)\u20162, and the inequality holds for \u03b7 = 3. Case 2/2, if NNN\u222a{ai}(aj) 6= ai, then it implies NNN\u222a{ai}(aj) = NNN(aj) and so\n\u2203a\u2217 \u2208 N : \u2016aj \u2212 a\u2217\u20162 \u2264 \u2016aj \u2212 ai\u20162 . (71)\nIneq. (65) reduces to proving\n\u2016ai \u2212 NNN(ai)\u20162 \u2264 (1 + \u03b7) \u00b7 \u2016ai \u2212 aj\u20162 , (72)\nbut \u2016ai\u2212a\u2217\u20162 \u2264 \u2016ai\u2212aj\u20162 +\u2016aj\u2212a\u2217\u20162 \u2264 2\u2016ai\u2212aj\u20162, and since a\u2217 \u2208 N, \u2016ai\u2212NNN(ai)\u20162 \u2264 \u2016ai\u2212a\u2217\u20162 \u2264 2\u2016ai\u2212aj\u20162, and (72) is proved for \u03b7 = 1. This achieves the proof of Lemma 20. Let I be any sequence not containing the index of a\u2032n, and let I(i) denote the sequence in which we replace aIi by the index of a \u2032 n. The sequence of swaps\nI(k) = (sk\u22121 \u25e6 ... \u25e6 si+1 \u25e6 si)(I(i)) (73)\nproduces a sequence I(k) in which all elements different from a\u2032n are in the same relative order as they are in I with respect to each other, and a\u2032n is pushed to the end of the sequence in k\nth rank. We also have\nN(I(i)) \u2264 (1 + \u03b7)2(k\u2212i)N(I(k)) . (74)\nAll the properties we need on N(.) are now established. We turn to the analysis of M(I i|A).\nLemma 21 For any \u03b4s > 0 such that A is \u03b4s-monotonic, the following holds. For any N \u2286 A with |N| \u2208 {1, 2, ..., k \u2212 1}, \u2200x,x\u2032 \u2208 \u2126, we have:\u2211\na\u2208A \u2016a\u2212 NNN\u222a{x}(a)\u201622 \u2264 (1 + \u03b4s) \u00b7 \u2211 a\u2208A \u2016a\u2212 NNN\u222a{x\u2032}(a)\u201622 . (75)\nProof Since adding a point to N cannot increase the potential \u2211 a\u2208A \u2016a\u2212NNN\u222a{x}(a)\u201622, it comes\u2211\na\u2208A \u2016a\u2212 NNN\u222a{x}(a)\u201622 \u2264 \u2211 a\u2208A \u2016a\u2212 NNN(a)\u201622 ,\u2200x \u2208 \u2126 . (76)\nConsider any x\u2032 \u2208 \u2126 such that\u2211a\u2208A \u2016a\u2212NNN\u222a{x\u2032}(a)\u201622 = \u2211a\u2208A \u2016a\u2212NNN(a)\u201622, i.e., all points of A are closer to a point in N than they are from x\u2032. In this case, we obtain from ineq. (76),\u2211\na\u2208A \u2016a\u2212 NNN\u222a{x}(a)\u201622 \u2264 \u2211 a\u2208A \u2016a\u2212 NNN\u222a{x\u2032}(a)\u201622 , (77)\nand since \u03b4s > 0, the statement of the Lemma holds. More interesting is the case wherex\u2032 \u2208 \u2126 is such that\u2211a\u2208A \u2016a\u2212NNN\u222a{x\u2032}(a)\u201622 <\u2211a\u2208A \u2016a\u2212 NNN(a)\u201622, implying x\u2032 6\u2208 N. In this case, let A .\n= {a \u2208 A : NNN\u222a{x\u2032}(a) = x\u2032}, which is then non-empty. Let us denote for short c(A) .= (1/|A|) \u00b7\u2211a\u2208A a. Since x\u2032 6\u2208 N, A\u2229N = \u2205, and since A is \u03b4s-monotonic, then it comes from ineq. (76)\u2211\na\u2208A \u2016a\u2212 NNN\u222a{x}(a)\u201622 \u2264 (1 + \u03b4s) \u00b7 \u2211 a\u2208A \u2016a\u2212 NNN\u222a{c(A)}(a)\u201622 . (78)\nWe have:\u2211 a\u2208A \u2016a\u2212 NNN\u222a{c(A)}(a)\u201622 = \u2211 a\u2208A\\A \u2016a\u2212 NNN\u222a{c(A)}(a)\u201622 + \u2211 a\u2208A \u2016a\u2212 NNN\u222a{c(A)}(a)\u201622\n\u2264 \u2211 a\u2208A\\A \u2016a\u2212 NNN\u222a{c(A)}(a)\u201622 + \u2211 a\u2208A \u2016a\u2212 c(A)\u201622\n\u2264 \u2211 a\u2208A\\A \u2016a\u2212 NNN\u222a{c(A)}(a)\u201622 + \u2211 a\u2208A \u2016a\u2212 x\u2032\u201622 . (79)\nEq. (79) holds because the arithmetic average is the population minimizer of L22. Because of the definition of A, \u2211\na\u2208A\\A \u2016a\u2212 NNN\u222a{c(A)}(a)\u201622 \u2264 \u2211 a\u2208A\\A \u2016a\u2212 NNN(a)\u201622\n= \u2211 a\u2208A\\A \u2016a\u2212 NNN\u222a{x\u2032}(a)\u201622 , (80)\nand, still because of the definition of A,\u2211 a\u2208A \u2016a\u2212 x\u2032\u201622 = \u2211 a\u2208A \u2016a\u2212 NNN\u222a{x\u2032}(a)\u201622 , (81)\nso we get from (80) and (81) \u2211 a\u2208A\\A \u2016a \u2212 NNN\u222a{c(A)}(a)\u201622 + \u2211 a\u2208A \u2016a \u2212 x\u2032\u201622 \u2264 \u2211 a\u2208A \u2016a \u2212 NNN\u222a{x\u2032}(a)\u201622, and finally from ineq. (79),\u2211 a\u2208A \u2016a\u2212 NNN\u222a{c(A)}(a)\u201622 \u2264 \u2211 a\u2208A \u2016a\u2212 NNN\u222a{x\u2032}(a)\u201622 , (82)\nwhich, using ineq. (78), completes the proof of Lemma 21.\nLemma 22 The following holds true, for any i > 1, any A\u2032 \u2248 A, any \u03b4w, \u03b4s > 0:\nA is \u03b4w-spread \u21d2 (n 6\u2208 I i \u21d2M(I i|A) \u2264 (1 + \u03b4w) \u00b7M(I i|A\u2032)) , (83) A is \u03b4s-monotonic \u21d2 (n \u2208 I i \u21d2M(I i|A) \u2264 (1 + \u03b4s) \u00b7M(I i|A\u2032)) . (84)\nProof Suppose first that n 6\u2208 I i. In this case, since A is \u03b4w-spread,\nM(I i|A) = n\u2211 j=1 \u2016aj \u2212 NNIi(aj)\u201622\n= n\u22121\u2211 j=1 \u2016aj \u2212 NNIi(aj)\u201622 + \u2016an \u2212 NNIi(aj)\u201622\n\u2264 n\u22121\u2211 j=1 \u2016aj \u2212 NNIi(aj)\u201622 +R2\n\u2264 (1 + \u03b4w) \u00b7 n\u22121\u2211 j=1 \u2016aj \u2212 NNIi(aj)\u201622 (85)\n\u2264 (1 + \u03b4w) \u00b7 ( n\u22121\u2211 j=1 \u2016aj \u2212 NNIi(aj)\u201622 + \u2016a\u2032n \u2212 NNIi(a\u2032n)\u201622 ) = (1 + \u03b4w) \u00b7M(I i|A\u2032) , (86)\nas indeed computing the nearest neighbors do not involve the nth element of the sets, i.e. an or a\u2032n. We have used in ineq. (85) the fact that A is \u03b4w-spread.\nWhen n \u2208 I i, eq. (84) is an immediate consequence of Lemma 21 in which the distinct elements of A and A\u2032 play the role of x and x\u2032.\nLemma 23 For any \u03b4w > 0, if A is \u03b4w-spread, then for any N \u2286 A with |N| = k \u2212 1, \u2200x \u2208 \u2126, it holds that \u2016x\u2212 NNN(x)\u201622 \u2264 \u03b4w \u2211 a\u2208A \u2016a\u2212 NNN(a)\u201622.\nProof Follows directly from the fact that \u2016x\u2212 NNN(x)\u201622 \u2264 R2 by assumption. Letting I(k) denote a sequence containing element n pushed to the end of the sequence, we get:\u2211\n\u03c3\u2208Sk \u2211 I\u2208Seq+(n:k) p(\u03c3, I,C|A\u2032)\n= \u2211 \u03c3\u2208Sk \u2211 I\u2208Seq+(n:k) N(I)\u220fk i=1M(I i|A\u2032) \u00b7 pa\u2032n(c\u03c3(i)) \u00b7 k\u220f i=1:Ii 6=n paIi (c\u03c3(i)) \u2264 (1 + \u03b7)2(k\u22122)\n\u00b7 \u2211 \u03c3\u2208Sk \u2211 I\u2208Seq+(n:k) N(I(k))\u220fk i=1M(I i|A\u2032) \u00b7 pa\u2032n(c\u03c3(i)) \u00b7 k\u220f i=1:Ii 6=n paIi (c\u03c3(i)) . (87)\nNow, take any element I \u2208 Seq+(n : k) with a\u2032n in position k, and change a\u2032n by some a \u2208 A. Any of these changes generates a different element I \u2032 \u2208 Seq\u2212(n : k), and so using Lemma 23 and the following two facts:\n\u2022 the fact that\npa\u2032n(c\u03c3(i)) \u2264 %(R) \u00b7 pa(c\u03c3(i)) , (88)\nfor any a \u2208 A,\n\u2022 the fact that, if A is \u03b4s-monotonic,\nM(I ia|A) \u2264 (1 + \u03b4s) \u00b7M(I i|A) , (89)\nfor any a \u2208 A not already in the sequence, where Ia denotes the sequence I in which a\u2032n has been replaced by a,\nwe get from ineq. (87),\u2211 \u03c3\u2208Sk \u2211 I\u2208Seq+(n:k) p(\u03c3, I,C|A\u2032)\n\u2264 (1 + \u03b7)2(k\u22122) \u00b7 (1 + \u03b4s)k\u22121 \u00b7 \u03b4w\n\u00b7%(R) \u00b7 \u2211 \u03c3\u2208Sk \u2211 I\u2208Seq\u2212(n:k) N(I)\u220fk i=1 M(I i|A) \u00b7 k\u220f i=1 paIi (c\u03c3(i)) . (90)\nLemma 24 For any \u03b4w, \u03b4s > 0 such that A is \u03b4w-spread and \u03b4s-monotonic, for any A\u2032 \u2248 A, we have:\nP[C|A\u2032] P[C|A] \u2264 (1 + \u03b4w)\nk\u22121 \u00b7 ( 1 + \u03b4w \u00b7 (\n1 + \u03b4s 1 + \u03b4w\n)k\u22121 \u00b7 (1 + \u03b7)2(k\u22122) \u00b7 %(R) ) . (91)\nProof We get from the fact that A is \u03b4w-spread,\u2211 \u03c3\u2208Sk \u2211 I\u2208Seq\u2212(n:k) p(\u03c3, I,C|A\u2032) \u2264 (1 + \u03b4w)k\u22121 \u00b7 \u2211 \u03c3\u2208Sk \u2211 I\u2208Seq\u2212(n:k) p(\u03c3, I,C|A) , (92)\nand furthermore ineq. (90) yields:\nP[C|A\u2032] P[C|A] =\n\u2211 \u03c3\u2208Sk \u2211 I\u2208Seq(n:k) p(\u03c3, I,C|A\u2032)\u2211\n\u03c3\u2208Sk\n\u2211 I\u2208Seq(n:k) p(\u03c3, I,C|A)\n\u2264\n (1 + \u03b4w)k\u22121 \u00b7\u2211\u03c3\u2208Sk\u2211I\u2208Seq\u2212(n:k) p(\u03c3, I,C|A)+\u2211 \u03c3\u2208Sk \u2211 I\u2208Seq+(n:k) p(\u03c3, I,C|A\u2032)  \u2211 \u03c3\u2208Sk \u2211 I\u2208Seq(n:k) p(\u03c3, I,C|A)\n\u2264 (1 + \u03b4w)k\u22121\n\u00b7\n \u2211 \u03c3\u2208Sk \u2211 I\u2208Seq\u2212(n:k) p(\u03c3, I,C|A) +\n\u03b4w \u00b7 (\n1+\u03b4s 1+\u03b4w )k\u22121 \u00b7 (1 + \u03b7)2(k\u22122) \u00b7 %(R) \u00b7\u2211\u03c3\u2208Sk\u2211I\u2208Seq\u2212(n:k) p(\u03c3, I,C|A\u2032)  \u2211 \u03c3\u2208Sk \u2211 I\u2208Seq(n:k) p(\u03c3, I,C|A)\n= (1 + \u03b4w) k\u22121 \u00b7 ( 1 + \u03b4w \u00b7 ( 1 + \u03b4s 1 + \u03b4w )k\u22121 \u00b7 (1 + \u03b7)2(k\u22122) \u00b7 %(R) )\n\u00b7 \u2211 \u03c3\u2208Sk \u2211 I\u2208Seq\u2212(n:k) p(\u03c3, I,C|A)\u2211\n\u03c3\u2208Sk \u2211 I\u2208Seq(n:k) p(\u03c3, I,C|A)\ufe38 \ufe37\ufe37 \ufe38\n\u22641\n.\nThis ends the proof of Lemma 24.\nSince\n(1 + \u03b4w) k\u22121 \u00b7 ( 1 + \u03b4w \u00b7 ( 1 + \u03b4s 1 + \u03b4w )k\u22121 \u00b7 (1 + \u03b7)2(k\u22122) \u00b7 %(R) ) = (1 + \u03b4w) k\u22121 + (1 + \u03b7)2(k\u22122) \u00b7 \u03b4w \u00b7 (1 + \u03b4s)k\u22121 \u00b7 %(R) ,\nand \u03b7 \u2264 3 from Lemma 19, we get Theorem 9 with\nf(k) . = 42k\u22124 . (93)"}, {"heading": "Proof of Theorem 10", "text": "Assume that density D contains a L2 ball B2(0, R) of radiusR, centered without loss of generality in 0. Fix 0 < \u03ba < m \u2212 1. For any \u03b1 \u2208 (0, 1) and N \u2286 A with |N| \u2208 {1, 2, ...,\u03ba} .= [\u03ba]\u2217, let N \u2295 \u03b1 .= \u222ax\u2208NB2(x,\u03b1 \u00b7 R) be the union of all small balls centered around each element of N, each of radius \u03b1 \u00b7R. An important quantity is\nq\u2217 .\n= min N\u2286A,|N|\u2208[\u03ba]\u2217 \u00b5(B2(0, R)\\N \u2295 \u03b1) \u00b5(B2(0, R))\n(94)\nthe minimal mass of B2(0, R)\\N \u2295 \u03b1 relatively to B2(0, R) as measured using D. As depicted in Figure 7, q\u2217 is a minimal value of the probability to escape the neighborhoods of N \u2295 \u03b1 when\nsampling points according to D in ball B2(0, R). If, for some \u03b1 that shall depend upon the dimension d and \u03ba, q\u2217 is large enough, then the spread of points drawn shall guarantee \u201dsmall\u201d values for \u03b4w and \u03b4s.\nThis is formalized in the following Theorem, which assumes m = M = 1, i.e. the ball has uniform density. Theorem 10 is a direct consequence of this Theorem.\nTheorem 25 Suppose A \u2282 B2(0, R). For any \u03b4 \u2208 (0, 1), if\nm \u2265 3 ( \u03ba\nq\u2217\u03b42\n)2 , (95)\nthen there is probability \u2265 1 \u2212 \u03b4 over its sampling that A is \u03b4w-spread and \u03b4s-monotonic for the following values of \u03b4w, \u03b4s:\n\u03b4w = 1\nq\u2217(1\u2212 \u03b4)(m\u2212 \u03ba\u2212 1)\u03b12 , (96)\n\u03b4s = m m\u2212 \u03ba \u00b7 (\n2 min {\n1 4 , q\u2217(1\u2212 \u03b4)\n} \u00b7 \u03b1\n)2 \u2212 1 . (97)\nProof We first prove the following Lemma.\nLemma 26 Suppose A \u2282 B2(0, R). Let q\u2217 be defined as in eq. (94). Then for any \u03b4 \u2208 (0, 1), if m meets ineq. (95), then there is probability \u2265 1\u2212 \u03b4 that\n|(B2(0, R)\\N \u2295 \u03b1) \u2229 (A\\N)| \u2265 q\u2217(1\u2212 \u03b4)(m\u2212 \u03ba) ,\u2200N \u2286 A, |N| \u2208 [\u03ba]\u2217 . (98)\nProof Since we assume A \u2282 B2(0, R), Chernoff bounds imply that for any fixed N \u2286 A with |N| \u2208 [\u03ba]\u2217,\nPD [ |(B2(0, R)\\N \u2295 \u03b1) \u2229 (A\\N)| |A\\N| \u2264 q\u2217(1\u2212 \u03b4) ] \u2264 exp ( \u2212\u03b42q\u2217 |A\\N| /2 ) . (99)\nNow, remark that \u03ba\u2211 j=1 ( m j ) \u2264 m\u03ba ,\u2200m,\u03ba \u2265 1 . (100)\nThis can be proven by induction, m being fixed: it trivially holds for \u03ba = 1 and \u03ba = 2, and furthermore\n\u03ba\u2211 j=1 ( m j ) = \u03ba\u22121\u2211 j=1 ( m j ) + ( m \u03ba ) \u2264 m\u03ba\u22121 + m!\n(m\u2212 \u03ba)!\u03ba! , (101)\nby induction at rank \u03ba\u2212 1. To prove that the right-hand side of (101) is no more than m\u03ba, we just have to remark that\nm! (m\u2212 \u03ba)!\u03ba!m\u03ba\u22121 < m \u03ba!\n\u2264 m\u2212 1 , (102)\nas long as \u03ba > 1 and m > 1. So, the property at rank \u03ba \u2212 1 for \u03ba > 1 implies property at rank \u03ba, which concludes the induction.\nSo, we have at most m\u03ba choices for N, so relaxing the choice of N, we get PD [ \u2203N \u2286 A, |N| = \u03ba : |(B2(0, R)\\N \u2295 \u03b1) \u2229AN||AN| \u2264 q\u2217(1\u2212 \u03b4) ]\n\u2264 m\u03ba exp ( \u2212\u03b4\n2q\u2217(m\u2212 \u03ba) 2\n) . (103)\nWe want to compute the minimal m such that the right-hand side is no more than \u03b4, this being equivalent to\n\u03b42q\u2217m \u2265 2 log ( m\u03ba\n\u03b4\n) + \u03ba\u03b42q\u2217 ,\nwhich, since \u03b4 \u2208 (0, 1), is ensured if \u03b42q\u2217m \u2265 2\u03ba log (m \u03b4 ) + \u03ba\u03b42q\u2217 . (104)\nSuppose\nm = 3\n( \u03ba\nq\u2217\u03b42\n)2 .\nSince we trivially have \u03ba2/(q\u2217\u03b42)2 \u2265 \u03ba\u03b42q\u2217 (\u03ba \u2265 1, q\u2217 \u2208 (0, 1), \u03b4 \u2208 (0, 1)), it is sufficient to prove:\n2\u03ba\nq\u2217\u03b42 \u2265 2 log 3 + 2 log\n( \u03ba2\nq2\u2217\u03b4 5\n) , (105)\nwhich, again observing that \u03b4 \u2208 (0, 1), holds if we can prove\n\u03ba q\u2217\u03b42 \u2265 log 2 + 3 2 \u00b7 log\n( \u03ba\nq\u2217\u03b42\n) , (106)\nwhich is equivalent to showing x \u2265 (3/2) log x + log 2 for x \u2265 1, which indeed holds (end of the proof of Lemma 26).\nThe consequence of Lemma 26 is the following: if A \u2282 B2(0, R) and m satisfies (95), then for any N \u2286 A with |N| = k \u2212 1, and any B \u2286 A with |B| = |A| \u2212 1,\u2211\na\u2208B\n\u2016a\u2212 NNN(a)\u201622 \u2265 q\u2217(1\u2212 \u03b4)(m\u2212 \u03ba\u2212 1)\u03b12 \u00b7R2 , (107)\nand so from Definition 7 A is \u03b4s-spread for:\n\u03b4w = 1\nq\u2217(1\u2212 \u03b4)(m\u2212 \u03ba\u2212 1)\u03b12 . (108)\nNow, suppose we add a single point x\u2217 in N. If, for some fixed \u03b1\u2217 \u2208 (0,\u03b1/2],\nx\u2217 6\u2208 a\u2295 \u03b1\u2217 ,\u2200a \u2208 A , (109)\nthen because of (107),\u2211 a\u2208A \u2016a\u2212 NNN\u222a{x\u2217}(a)\u201622 \u2265 (m\u2212 \u03ba) \u00b7min { \u03b12\u2217, q\u2217(1\u2212 \u03b4)\u03b12 } \u00b7R2 . (110)\nOtherwise, consider one a\u2217 for which x\u2217 \u2208 a\u2217 \u2295 \u03b1\u2217. If we replace a\u2217 by x\u2217 in all N in which a\u2217 belongs to in Lemma 26, then because x\u2217 \u2295 \u03b1\u2217 \u2282 a\u2217 \u2295 \u03b1, it comes from Lemma 26:\u2211\na\u2208A\n\u2016a\u2212 NNN\u222a{x\u2217}(a)\u201622 \u2265 1\n4 \u00b7 (m\u2212 \u03ba) \u00b7 q\u2217(1\u2212 \u03b4)\u03b12 \u00b7R2 . (111)\nWe thus get in all cases\u2211 a\u2208A \u2016a\u2212 NNN\u222a{c(A)}(a)\u201622 \u2265 min { \u03b12 4 ,\u03b12\u2217, q\u2217(1\u2212 \u03b4)\u03b12 } (m\u2212 \u03ba) \u00b7 q\u2217(1\u2212 \u03b4) \u00b7R2 ,(112)\nwhere c(A) is the arithmetic average computed according to the definition of \u03b4s-monotonicity, of any A \u2286 A\\N. Since N \u2286 A \u2282 B2(0, R), we have \u2211 a\u2208A \u2016a\u2212 NNN(a)\u201622 \u2264 4mR2, and so\u2211\na\u2208A\n\u2016a\u2212 NNN(a)\u201622 \u2264 4m min { \u03b12\n4 ,\u03b12\u2217, q\u2217(1\u2212 \u03b4)\u03b12 } (m\u2212 \u03ba) \u00b7 q\u2217(1\u2212 \u03b4) \u00b7 \u2211 a\u2208A \u2016a\u2212 NNN\u222a{c(A)}(a)\u201622 ,(113)\nimplying from Definition 8 that \u03b4s-monotonicity holds with:\n\u03b4s = m m\u2212 \u03ba \u00b7 4 min { \u03b12\n4 ,\u03b12\u2217, q\u2217(1\u2212 \u03b4)\u03b12 } \u00b7 q\u2217(1\u2212 \u03b4) \u2212 1 . (114)\nThe statement of the Theorem follows with \u03b1\u2217 = \u03b1/2 (end of the proof of Theorem 25).\nWe finish the proof of Theorem 10. We have\nq\u2217 \u2265 1\u2212 \u03ba\u03b1d , (115)\nwhere the lowerbound corresponds to the case where all neighborhoods in N \u2295 \u03b1 are distinct and included in B2(0, R). So we have, for any fixed choice of \u03b1 \u2208 (0, 1),\n\u03b4w \u2264 1\n\u03b12 \u00b7 (1\u2212 \u03ba\u03b1d)(1\u2212 \u03b4)(m\u2212 \u03ba\u2212 1) . (116)\nTo minimize this upperbound, we pick \u03b1 to maximize \u03b12 \u00b7 (1 \u2212 \u03ba\u03b1d) with \u03b1 \u2208 (0, 1), which is easily achieved picking\n\u03b1 =\n( 1\n\u03ba(d+ 1)\n) 1 d\n, (117)\nand yields\n\u03b4w \u2264 ( 1 + 1\nd\n) \u00b7 1\n(\u03ba(d+ 1)) 2 d (1\u2212 \u03b4)(m\u2212 \u03ba\u2212 1) \u2264 ( 1 + 1\nd ) \u00b7 1 \u03ba 2 d (1\u2212 \u03b4)(m\u2212 \u03ba\u2212 1) . (118)\nBut we have for this choice, 1\u2212 \u03ba\u03b1d = d/(d+ 1) \u2265 1/2, so as long as\n\u03b4 < 1/2 , (119)\nwe shall have q\u2217(1\u2212 \u03b4) > 1/4 and so we shall have\n\u03b4s + 1 = 64 \u00b7 m m\u2212 \u03ba \u00b7 1 \u03b12\n\u2264 64 \u00b7 m m\u2212 \u03ba \u00b7 1 \u03ba 2 d . (120)\nWe now go back to ineq. (14), which reads:\nP[C|A\u2032] P[C|A] \u2264 %1 + %2 , (121)\nwith\n%1 . = (1 + \u03b4w) k\u22121 , (122) %2 .\n= f(k) \u00b7 \u03b4w \u00b7 (1 + \u03b4s)k\u22121 \u00b7 %(R) . (123) We upperbound separately both terms.\nLemma 27 Suppose ineqs (119) and (15) are met. Then\n%1 \u2264 1 + 4\nm 1 4 + 1 d+1\n. (124)\nProof Since d \u2265 1 and \u03b4 < 1/2, we get from ineq. (118) (using \u03ba = k)\n(1 + \u03b4w) k\u22121 \u2264 ( 1 + ( 1 + 1\nd ) \u00b7 1 k 2 d (1\u2212 \u03b4)(m\u2212 k \u2212 1) )k\u22121\n\u2264 ( 1 + 2\nk 2 d (1\u2212 \u03b4)(m\u2212 k \u2212 1)\n)k\u22121\n\u2264 ( 1 + 4\nk 2 d (m\u2212 k \u2212 1)\n)k\u22121 . (125)\nLet h(k) be the right-hand side of ineq. (125). h(1) trivially meets ineq. (124). When k \u2265 2, h decreases until k = 2(m\u2212 1)/(d + 2) and then increases. We thus just need to check ineq. (124) for k = 2 and k = \u221a m from ineq. (15). We get h(2) = 1 + 4/(41/d(m \u2212 3)). For ineq. (124) to be satisfied, we need to have 41/d(m \u2212 3) \u2265 m 14 + 1d+1 , which holds if m \u2265 3 + m3/4 (d \u2265 1), that is, m \u2265 8. But since ineqs (119) and (15) are satisfied, we have m \u2265 16k2/\u03b42 \u2265 64k2 \u2265 64, and so h(2) satisfies ineq. (124).\nThere remains to check ineq. (124) for k = \u221a m. We have\nh( \u221a m) = ( 1 +\n4\nm 1 d (m\u2212\u221am\u2212 1)\n)\u221am\u22121\n\u2264 ( 1 + 4\nm 1 d (m\u2212\u221am)\n)\u221am\n\u2264 ( 1 + 2\n\u221a m \u00b7m 14 + 1d\n)\u221am , (126)\nsince any m \u2265 64, we have m\u2212\u221am \u2265 2m3/4. To conclude, ineq (126) yields\nh( \u221a m) \u2264 ( 1 +\n2 \u221a m \u00b7m 14 + 1d )\u221am \u2264 exp ( 2\nm 1 4 + 1 d ) \u2264 1 + 4\nm 1 4 + 1 d\n. (127)\nThe penultimate ineq. comes from 1 + x \u2264 expx, and the last one comes from the fact that exp(2x) \u2264 1 + 4x for x \u2264 1. Since m 14 + 1d \u2265 m 14 + 1d+1 , we obtain the statement of the Lemma for h( \u221a m). This concludes the proof of Lemma 27.\nLemma 28 Suppose ineqs (119) and (15) are met. Then\n%2 \u2264 ( 64\nk 2 d\n)k \u00b7 %(2R)\nm . (128)\nProof We fix \u03ba = k, use f(k) = 42k\u22124 (eq. 93), so we get\n%2 = 4 2k\u22122 \u00b7 ( 1 + 1\nd ) \u00b7 1 k 2 d (1\u2212 \u03b4)(m\u2212 k \u2212 1) \u00b7 ( 64 \u00b7 m m\u2212 k \u00b7 1 k 2 d )k\u22121 \u00b7 %(2R)\n\u2264 2 \u00b7 64k\u22121 \u00b7 ( 1 + 1\nd ) \u00b7 1 k 2k d (m\u2212 k \u2212 1) \u00b7 ( 1 + k m\u2212 k )k\u22121 \u00b7 %(2R) (129)\n\u2264 4 \u00b7 1 (m\u2212 k \u2212 1) \u00b7\n( 1 +\nk\nm\u2212 k )k\u22121 \ufe38 \ufe37\ufe37 \ufe38\n. =%3\n\u00b764k\u22121 \u00b7 1 k 2k d \u00b7 %(2R) , (130)\nusing the fact that \u03b4 < 1/2 and d \u2265 1. Now, we also have( 1 + k\nm\u2212 k\n)k\u22121 \u2264 exp ( k2\nm\u2212 k\n) (131)\n\u2264 e , (132)\nas long as k \u2264 (1/16) \u00b7 \u221am, and furthermore, since m \u2265 64 (see the proof of Lemma 27), we also have 1/(m\u2212 k \u2212 1) \u2264 5/m. We thus obtain\n%3 \u2264 20e\nm\n\u2264 64 m , (133)\nwhich yields\n%2 \u2264 ( 64\nk 2 d\n)k \u00b7 %(2R)\nm , (134)\nas claimed.\nPutting altogether Lemmata 27 and 28, we get:\nP[C|A\u2032] P[C|A] \u2264 1 +\n4\nm 1 4 + 1 d+1\n+\n( 64\nk 2 d\n)k \u00b7 %(2R)\nm , (135)\nas claimed. There remains to check that, with our choice of \u03b1, the constraint on m in (95) is satisfied if\nm \u2265 12k 2\n\u03b44 (136)\nsince q\u2217 \u2265 d/(d+ 1). We obtain the sufficient constraint on k:\nk \u2264 \u03b4 2 4 \u00b7 \u221am , (137)\nwhich proves Theorem 10 when m = M = 1.\nWhen the density do not satisfy m = M = 1 we just have to remark that the lowerbound on q\u2217 is now\nq\u2217 \u2264 \u03b5m \u03b5M \u00b7 (1\u2212 \u03ba\u03b1d) . (138)\nIneq. (118) becomes \u03b4w \u2264 \u03b5M \u03b5m \u00b7 ( 1 + 1 d ) \u00b7 1 \u03ba 2 d (1\u2212 \u03b4)(m\u2212 \u03ba\u2212 1) , (139)\nineq. (120) becomes\n\u03b4s + 1 \u2264 \u03b5M \u03b5m \u00b7 64 \u00b7 m m\u2212 \u03ba \u00b7 1 \u03ba 2 d . (140)\nSo, the only difference with the m = M = 1 is the ratio \u03b5M/\u03b5m (\u2265 1) which multiplies all quantities of interest, and yields, in lieu of ineq. (135),\nP[C|A\u2032] P[C|A] \u2264 1 + ( \u03b5M \u03b5m )k \u00b7 (\n4\nm 1 4 + 1 d+1\n+\n( 64\nk 2 d\n)k \u00b7 %(2R)\nm\n) , (141)\nwhich is the statement of Theorem 10."}, {"heading": "Proof of Theorem 12", "text": "When p(\u00b5a,\u03b8a) is a product of Laplace distributions Lap(b) (b being the scale parameter of the distribution (Dwork & Roth, 2014)), condition in ineq. (13) becomes:\np(\u00b5a\u2032 ,\u03b8a\u2032 )(x)\np(\u00b5a,\u03b8a)(x) \u2264 exp (\u2016a\u2212 a\u2032\u20161 b ) = exp (\u221a 2\u2016a\u2212 a\u2032\u20161\n\u03c31\n)\n\u2264 exp ( 2 \u221a 2R\n\u03c31\n) , \u2200a,a\u2032 \u2208 A, \u2200x \u2208 \u2126 , (142)\nassuming A \u2282 B1(0, R). Let us fix %(R) .= exp ( 2 \u221a 2R/\u03c31 ) . Since B1(0, R) \u2282 B2(0, R) (the L2 ball), we now want (1+\u03b4w)k\u22121 +f(k) \u00b7\u03b4w \u00b7 (1 + \u03b4s)k\u22121 \u00b7%(R) = exp( ). Solving for \u03c31 yields:\n\u03c31 = 2 \u221a 2R log (\nexp( )\u2212(1+\u03b4w)k\u22121 f(k)\u00b7\u03b4w\u00b7(1+\u03b4s)k\u22121 ) , (143) as claimed. The proof that k-variates++ meets ineq. (7) with\n\u03a6 = \u03a61 . = 8 \u00b7 ( \u03c6opt + mR2\n\u03032\n) (144)\ncomes from a direct application of Theorem 2, with\n\u03b7 = 0 ,\n\u03c6bias = \u03c6opt , \u03c6var = m \u00b7 ( 2 \u221a 2R\n\u0303\n)2 .\nThe statements for \u03c32 and \u03a62 are direct applications of the Laplace mechanism properties (Dwork & Roth, 2014; Dwork et al., 2006)."}, {"heading": "Extension to non-metric spaces", "text": "Since its inception, the k-means++ seeding technique has been successfully adapted to various distortion measures D(\u00b7\u2016\u00b7) to handle non-Euclidean features (Jegelka et al., 2009; Nock et al., 2008, 2016). Similarly, our extended seeding technique can be adapted to these scenarii: this boils down to putting the distortion as a free parameter of the algorithm, replacing Dt(a) (eq. (1)) by Dt(a) . = mina\u2032\u2208PD(a\u2016a\u2032). For example, by noticing that the squared Euclidean distance is merely an example of Bregman divergences (the well-known canonical divergences in information geometry of dually flat spaces), k-variates++ can be been extended to that family of dissimilarities (Nock et al., 2008). But more interesting examples now appear, that build on constraints that distortions have to satisfy for certain problems, like the invariance to rotations of the coordinate space. This is all the more challenging in practice for clustering since sometimes no-closed form solution are available for some of these divergences. Because it bypasses the construction of the population minimisers, k-variates++ offers an elegant solution to the problem. Such hard distortions include the skew Jeffreys \u03b1-centroids (Nock et al., 2016). This also include the recent class of total Bregman/Jensen divergences that are examples of conformal divergences (Nielsen & Nock, 2015; Nock et al., 2016). We give an example of the extension of k-variates++to the total Jensen divergence, to show that k-variates++ can approximate the optimal clustering even without closed form solutions for the population minimisers (Nielsen & Nock, 2015). For any convex function \u03d5 : Rd \u2192 R and \u03b1 \u2208 (0, 1), the skew Jensen divergence is\nJ\u03b1(a,a \u2032) . = \u03b1\u03d5(a) + (1\u2212 \u03b1)\u03d5(a\u2032)\u2212 \u03d5(\u03b1a+ (1\u2212 \u03b1)a\u2032) , (145)\nand the total Jensen divergence is\ntJ\u03b1(a,a \u2032) . = 1\u221a 1 + U2 \u00b7 J\u03b1(a,a\u2032) , (146)\nwhere U .= (\u03d5(a) \u2212 \u03d5(a\u2032))/\u2016a \u2212 a\u2032\u20162. There is no closed form solution for the population minimiser of tJ\u03b1, yet we can prove the following Theorem, which builds upon Theorem 3 in (Nielsen & Nock, 2015).\nTheorem 29 In k-variates++, replace Dt(a) (eq. (1)) by Dt(a) . = mina\u2032\u2208P tJ\u03b1(a,a \u2032) ans suppose for simplicity that probe functions are identity: \u2118t = Id,\u2200t. Denote \u03c6opt the optimal noisefree potential of the clustering problem using tJ\u03b1 as distortion measure. Then there exists a constant \u03c9 > 0 such that for any choice of densities p\u00b5.,\u03b8. , the expected tJ\u03b1-potential \u03c6 of kvariates++ satisfies:\nE[\u03c6(A;C)] \u2264 \u03c9 \u00b7 log k \u00b7 (6\u03c6opt + 2\u03c6bias + 2\u03c6var) , (147)\nwhere \u03c6var is defined in Theorem 2 and \u03c6bias is defined in eq. (4)."}, {"heading": "9 Appendix on Experiments", "text": "Experiments on Theorem 12 and the sublinear noise regime \u21aa\u2192 comments on \u0303 An important parameter of Theorem 12 is \u0303, which replaces in the computation of the noise standard deviation in \u03c31: the larger it is compared to , the less noise we can put while still ensuring P[C|A\u2032]/P[C|A] \u2264 exp in Definition 11. Recall its formula:\n\u0303 . = log ( exp( )\u2212 (1 + \u03b4w)k\u22121 f(k) \u00b7 \u03b4w \u00b7 (1 + \u03b4s)k\u22121 ) . (148)\nThe experimental setting is the following one: we repeatedly sample clusters that are uniform in a subset of the domain (with limited, random size), taken to be a d-dimensional hyperrectangle of randomly chosen edge lengths. Each cluster contains a randomly picked number of points between\n1 and 1000. After each cluster is picked, we updated an estimation of \u03b4w and \u03b4s:\n\u2022 we compute \u03b4w by randomly picking B and N for a total number of nest iterations, with nest = 5000;\n\u2022 we compute \u03b4s by randomly picking N for a total number of nest iterations. Instead of computing A then x, we opt for the fast proxy which consists in replacing c(A) by a random data point, thus without making the N-packed test. This should reasonably overestimate \u03b4s and thus slightly loosen our approximation bounds.\nFigure 8 shows the dataset obtained for d = 10 at the end of the process. Predictably, the distribution on the whole space looks like a highly non-uniform cover by locally uniform clusters. Tables 4, 5 and 6 display results obtained for three different values of and three different values for the couple (d, k). To test the large sample regime intuition and the fact that the the noise dependence grows sublinearly in m, we have regressed in each plot \u0303 as a function of m for\n\u0303(m) = a+ b logm . (149)\nThe plots obtained confirm a good approximation of this intuition, but they also display some more good news. The smaller , the larger can be \u0303 relatively to , by order of magnitudes if is small. Hence, despite the fact that we evetually overestimate \u03b4s, we still get large \u0303. Furthermore, if k is small, this \u201dlarge sample\u201d regime in which \u0303 > actually happens for quite small values of m.\nAlso, one may remark that the curves all look like an approximate translation of the same curve. This is not surprising, since we can reformulate\n\u0303 = + log ( 1\u2212 U ) + g(m) , (150)\nwhene U .= (1 + \u03b4w)k\u22121 and g do not depend on . It happens that \u03b4w quickly decreases to very small values (bringing also a separate empirical validation of its behavior as computed in ineq. (139) in the proof of Theorem 10). Hence, we rapidly get for small m some \u0303 that looks like\n\u0303 \u2248 + log ( 1\u2212 1 + o(1) ) + g(m)\n\u2248 h( ) + g(m) , (151)\nwhich may explain what is observed experimentally. We can sumarise the global picture for \u0303 vs by saying that it becomes more and more in favor of \u0303 as data size (d or m) increase, but become less in favor of \u0303 as the number of clusters k increases (predictably).\n\u21aa\u2192 comments on \u03b4w and \u03b4s Table 7 presents the estimated values of \u03b4w and \u03b4s for the settings of Tables 4, 5 and 6. We wanted to test the intuition as to whether, for m sufficiently large, it would hold that \u03b4w = O(1/m) while \u03b4s = O(1). The essential part is on \u03b4w, since such a behaviour would be sufficient for the sublinear growth of the noise dependence. One can check that such behaviours are indeed observed, and more: \u03b4w converges very rapidly to zero, at least for all settings in which\nwe have tested data generation. Another quite good news, is that \u03b4s seems indeed to be \u03b8(1), but for an actual value which is also not large, so the denominator of eq. (148) is actually driven by f(k), even when, as we already said, we may have a tendency to overestimate \u03b4s with our randomized procedure.\nExperiments with Dk-means++, k-means++ and k-means\u2016 ? Experiments on synthetic data We have generated a set of m \u224820 000 points using the same kind of clusters as in the experiments related to Theorem 12: we add \u201dtrue\u201d clusters until the total number of points exceeds 20 000. To simulate the spread of data among peers (Forgy nodes) and evaluate the influence of the spread of Forgy nodes (\u03c6Fs ) for Dk-means++, we have devised the following protocol: let us name \u201dtrue\u201d clusters the hyperrectangle clusters used to build the dataset. Each true cluster corresponds to the data held by a peer. Then, for some p \u2208 [0, 100] (%), each point in each true cluster moves into another cluster, with probability p. The choice of the target cluster is made uniformly at random. Thus, as p increases, we get a clustering problem in which the data held by peers is more and more spread, and for which the spread of Forgy nodes\n\u03c6Fs increases. Figure 9 presents a typical example of the spread for p = 50%. Notice that in this case many Forgy nodes have data spreading through a much larger domain than the initial, true clusters. Figure 10 displays that this happens indeed, as \u03c6Fs is multiplied by a factor exceeding 20 (compared to \u03c6Fs at p = 0) for the largest values of p.\nWe have compared Dk-means++ to k-means++ and k-means\u2016 (Bahmani et al., 2012). In the case of that latter algorithm, we follow the paper\u2019s statements and pick the number of outer iterations to be dlog \u03c61e, where \u03c61 is the potential for one Forgy-chosen center. We also pick ` = 2k, considering that it is a value which gives some of the best experimental results in (Bahmani et al., 2012). Finally, we recluster the points at the end of the algorithm using k-means++. For each algorithm H \u2208 {k-means++, k-means\u2016}, we run it on the complete dataset and its results are averaged over 10 runs. We run Dk-means++ for each p \u2208 {0%, 1%, ..., 50%}. More precisely, for each p, we average the results of Dk-means++ over 10 runs. We use as metric the relative increase in the potential of Dk-means++ compared to H:\n\u03c1\u03c6(H) . = \u03c6(Dk-means++)\u2212 \u03c6(H)\n\u03c6(H) \u00b7 100 . (152)\nthat we plot as a function of \u03c6Fs , or surface plot as a function of (k, p). The intuition for the former plot is that the larger \u03c6Fs , the larger should be this ratio, since the data held by peers spreads across the domain and each peer is constrained to pick its centers with uniform seeding.\n\u21aa\u2192 Dk-means++ vs k-means++ Figure 8 presents results for \u03c1\u03c6(k-means++) = f(\u03c6Fs ) obtained for various k. First, the intuition is indeed confirmed for k = 8, 9, 10, but an interesting phenomenon appears for k = 5: Dk-means++ almost consistently beats k-means++. The decrease in the average potential ranges up to 3%. Furthermore, this happens even for large values of \u03c6Fs . Finally, for all but one value of k, there exists spread values for which Dk-means++ beats k-means++.\nThe surface plot in Figure 3 displays that superior performances of Dk-means++ are probably not random. One possible explanation to this phenomenon relies on the expression of \u03c6bias given in the proof of Theorem 4 (eq. (43)), recalled here:\n\u03c6bias . = \u2211 a\u2208A \u2016\u00b5a \u2212 copt(a)\u201622\n= \u2211 i\u2208[n] \u2211 a\u2208Ai \u2016c(Ai)\u2212 copt(a)\u201622 .\n(153)\nRecall that \u03c6bias can be < \u03c6opt, and it can even be zero, in which case Theorem 2 says that the approximation bound may actually be better than that of k-means++ in (Arthur & Vassilvitskii, 2007) (furthermore, \u03b7 = 0 for Dk-means++). Hence, what happens is pobably that in several cases, there exists a union of peers data (the number of peers is larger than k) that gives a at least reasonably good approximation of the global optimum. In all our experiments indeed, we obtained a number of peers larger than 30.\n\u21aa\u2192 Dk-means++ vs k-means\u2016 Figure 3 appear to display performances for Dk-means++ that are even more in favor of Dk-means++, compared to k-variates++. Figure 9 presents results for \u03c1\u03c6(k-means\u2016) = f(\u03c6Fs ) obtained for various k. The fact that each of them is a vertical translation of a picture in Figure 8 comes from the fact that the results of k-means\u2016 and k-means++ do not depend on the spread of the neighbors \u03c6Fs .\n? Experiments on real world data We consider the EuropeDiff dataset5 (Dataset characteristics provided in Table 10). Figures 11 and 12 give the results for the equivalent settings of the experimental data. To simulate N peers with real data, reasonably spread geographically, we have\n5http://cs.joensuu.fi/sipu/datasets/\nsampled N points (\u201dpeer centers\u201d) with k-means++ seeding in data and then aggregated for each peer the subset of data in the corresponding Voronoi 1-NN cell. We then simulate the spread for parameter p as in the simulated data. Figures 11 and 12 globally display (and confirm) the same trends as for the simulated data. They, however, clearly emphasize this time that the spread of Forgy nodes \u03c6Fs is one key parameter that drives the performances of Dk-means++. Notice also that Dk-means++ remains on this dataset competitive up to p \u2265 30%, which means that it remains competitive when a significant proportion of peers\u2019 data is scattered without any constraint.\nTo further address the way the spread of Forgy nodes affects results, we have used another real world data with highly non-uniform distribution, Mopsi-Finland locations5 (m = 13467, d = 2). We have sampled peers using two different schemes for the peer centers: k-means++ and Forgy. In this latter initialisation, we just pick peer centers at random. In the former k-means++ initialisation, the initial peer centers are much more evenly geographically spread before we complete the peers data with the closest points. They remain more spread after the p% uniform displacement of data between peers, as shown on the top plots of Figure 13. What is interesting about this data is that it displays that if peers\u2019 data are indeed geographically located, then Dk-means++ is competitive up to quite reasonable values of p \u2264 20% (depending on k). That, is Dk-means++ works well\nwhen each peer aggregates 80 % data which is reasonably \u201dlocalized in the domain\u201d and 20 % data which can be located everywhere in the domain.\nExperiments with k-variates++ and GUPT Among the state-of-the-art approaches against which we could compare k-variates++, there are two major contenders, PINQ (McSherry, 2010) and GUPT (Mohan et al., 2012). Even when PINQ is a broad system, we switched our preferences to GUPT for the following reasons. The performance of k-means based on PINQ relies on two principal factors: the initialisation (like in the non differentially private version) and the number of iterations. To compete against heavily tuned specific applications, like k-variates++, this scheme requires substantial work for its optimisation. For example, if one allocates part of the privacy budget to release a differential private initialisation, the noise has to be proportional to the domain width, which would release poor centers. Also, generating points uniformly at random from the domain, to obtain data-independent initial centers, yields to a poor initialisation. Finally, the number of iterations has to be tuned very carefully: if too small, the algorithm keeps poor solutions; if too large, the number of iteration increase the added noise for privacy and harms PINQ\u2019s final accuracy. We thus chose GUPT. k-means implemented in the GUPT proceeds the following way: the dataset is cut in a certain number of blocks ` (following (Mohan et al., 2012), we fix ` = m0.4 in our experiments), the usual k-means algorithm is performed on each block. Before releasing the final centroids, results are aggregated and a noise is applied. Finally, we also compare against the vanilla approach of Forgy Initialisation using the Laplace mechanism. The noise rate (i.e., standard deviation) is then proportional to \u221d kR/ (we do not run k-means afterwards, hence the privacy budget remains \u201csmall\u201d). In comparison, GUPT adds noise \u221d kR/(` ) at the end of this aggregation process. Note that we disregard the fact that our data are multidimensional, which should require a finer-grained tuning of `, and choose to rely on the ` = m0.4 suggestion from (Mohan et al., 2012).\n\u21aa\u2192 Comparison on real world domains Our domains consist of 3 real-world datasets5. Lifesci contains the value of the top 10 principal components for a chemistry or biology experiment. Image\nis a 3D dataset with RGB vectors, and finally EuropeDiff is the differential coordinates of Europe map.\nTable 10 presents the extensinve results obtained, that are averaged in the paper\u2019s body. We have fixed = 1 in the differentially privacy parameters. The column \u0303 (eq. (18)) provides the differential privacy parameter which is equivalent from the protection standpoint, but exploits the computation of \u03b4w, \u03b4s (which we compute exactly, and not in a randomized way like in the experiments on Theorem 12 above) and ineq. (80). Therefore, each time \u0303 > (=1 in our applications), it means that our analysis brings a sizeable advantage over \u201craw protection\u201d by Laplace mechanism (in our application we chose for p\u00b5a,\u03b8a a Laplace distribution). R is computed from the data by an upperbound of the smallest enclosing ball radius. The results display several interesting patterns. First, the largest the domain, the better we compare with respect to the other algorithms. On EuropeDiff for example, we often have the ratio of the potentials \u03c6(GUPT)/\u03c6(k-variates++) of the order of dozens. Also, the performances of k-variates++ degrade if k increases, which is again consistent with the \u201cgood\u201d regime of Theorem 10.\n\u21aa\u2192 Comparison on synthetic domains The synthetic datasets contain points uniformly sampled on a unit d-ball, in low dimension d = 2 and higher dimension d = 15 , we generated datasets with size in {105, 106}.\nvs F, d\n= 2\nvs F, d\n= 15\nvs G\nU PT\n,d =\n2 vs\nG U\nPT ,d\n= 15\nFi gu\nre 14\n: k\n-v ar\nia te\ns+ +\nvs Fo\nrg y\nin iti\nal is\nat io\nn di\nff er\nen tia\nlly pr\niv at\ne an\nd G\nU PT\n.W e\nus e\nra tio \u03c1 \u2032 \u03c6\nbe tw\nee n\nth e\npo te\nnt ia\nlo f\nth e\nco nt\nen de r in (F -D P, G U PT ) ov er th e po te nt ia lo f k -v ar ia te s+ + (p ot en tia ls ar e av er ag ed 30 tim es ). T he m or e re d, th e be tte r is k -v ar ia te s+ + w ith re sp ec tt o th e co nt en de r. G re y va lu es in di ca te le ss po si tiv e ou tc om es fo rk -v ar ia te s+ +; w hi te va lu es in di ca te th at k -v ar ia te s+ + do es no t m an ag e to fin d an \u2032 la rg er th an , an d th us do es no tm an ag e to pu ts m al le rn oi se ra te th an in th e L ap la ce m ec ha ni sm ."}], "references": [{"title": "Bregman divergences and triangle inequality", "author": ["S. Acharyya", "A. Banerjee", "D. Boley"], "venue": "In Proc. of the 13 SIAM International Conference on Data Mining,", "citeRegEx": "Acharyya et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Acharyya et al\\.", "year": 2013}, {"title": "Streamkm++: A clustering algorithms for data streams", "author": ["Ackermann", "M.-R", "C. Lammersen", "M. M\u00e4rtens", "C. Raupach", "C. Sohler", "K. Swierkot"], "venue": "ALENEX,", "citeRegEx": "Ackermann et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ackermann et al\\.", "year": 2010}, {"title": "Streaming k-means approximation", "author": ["N. Ailon", "R. Jaiswal", "C. Monteleoni"], "venue": "In NIPS*22,", "citeRegEx": "Ailon et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2009}, {"title": "Scalable k-means++", "author": ["B. Bahmani", "B. Moseley", "A. Vattani", "R. Kumar", "S. Vassilvitskii"], "venue": "VLDB, pp", "citeRegEx": "Bahmani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bahmani et al\\.", "year": 2012}, {"title": "Distributed k-means and k-median clustering on general communication topologies", "author": ["Balcan", "M.-F", "S. Ehrlich", "Y. Liang"], "venue": "In NIPS*26,", "citeRegEx": "Balcan et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 1995}, {"title": "Differentially private empirical risk", "author": ["K. Chaudhuri", "C. Monteleoni", "Sarwate", "A.-D"], "venue": "minimization. JMLR,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2011}, {"title": "Adaptive noisy clustering", "author": ["M. Chichignoud", "S. Lousteau"], "venue": "IEEE Trans. IT,", "citeRegEx": "Chichignoud and Lousteau,? \\Q2014\\E", "shortCiteRegEx": "Chichignoud and Lousteau", "year": 2014}, {"title": "The algorithmic foudations of differential privacy", "author": ["C. Dwork", "A. Roth"], "venue": "Found. & Trends in TCS,", "citeRegEx": "Dwork and Roth,? \\Q2014\\E", "shortCiteRegEx": "Dwork and Roth", "year": 2014}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["C. Dwork", "F. McSherry", "K. Nissim", "A. Smith"], "venue": "TCC,", "citeRegEx": "Dwork et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2006}, {"title": "The noisy power method: a meta algorithm with applications", "author": ["M. Hardt", "E. Price"], "venue": "In NIPS*27,", "citeRegEx": "Hardt and Price,? \\Q2014\\E", "shortCiteRegEx": "Hardt and Price", "year": 2014}, {"title": "Composable core-sets for diversity and coverage maximization", "author": ["P. Indyk", "S. Mahabadi", "M. Mahdian", "Mirrokni", "V.-S"], "venue": "ACM PODS,", "citeRegEx": "Indyk et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Indyk et al\\.", "year": 2014}, {"title": "Efficient algorithms for online decision problems", "author": ["A. Kalai", "S. Vempala"], "venue": "J. Comp. Syst. Sc.,", "citeRegEx": "Kalai and Vempala,? \\Q2005\\E", "shortCiteRegEx": "Kalai and Vempala", "year": 2005}, {"title": "An algorithm for online k-means clustering", "author": ["E. Liberty", "R. Sriharsha", "M. Sviridenko"], "venue": "CoRR, abs/1412.5721,", "citeRegEx": "Liberty et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liberty et al\\.", "year": 2014}, {"title": "Shape retrieval using hierarchical total bregman soft clustering", "author": ["M. Liu", "Vemuri", "B.-C", "S. .i Amari", "F. Nielsen"], "venue": "IEEE Trans. PAMI,", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Privacy integrated queries: an extensible platform for privacy-preserving data analysis", "author": ["F. McSherry"], "venue": "Communications of the ACM,", "citeRegEx": "McSherry,? \\Q2010\\E", "shortCiteRegEx": "McSherry", "year": 2010}, {"title": "GUPT: privacy preserving data analysis made easy", "author": ["P. Mohan", "A. Thakurta", "E. Shi", "D. Song", "Culler", "D.-E"], "venue": "ACM SIGMOD,", "citeRegEx": "Mohan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohan et al\\.", "year": 2012}, {"title": "Smooth sensitivity and sampling in private data analysis", "author": ["K. Nissim", "S. Raskhodnikova", "A. Smith"], "venue": "ACM STOC, pp", "citeRegEx": "Nissim et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nissim et al\\.", "year": 2007}, {"title": "Mixed Bregman clustering with approximation guarantees", "author": ["R. Nock", "P. Luosto", "J. Kivinen"], "venue": "ECML, pp", "citeRegEx": "Nock et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nock et al\\.", "year": 2008}, {"title": "On conformal divergences and their population minimizers", "author": ["R. Nock", "F. Nielsen", "Amari", "S.-I"], "venue": "IEEE Trans. IT,", "citeRegEx": "Nock et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nock et al\\.", "year": 2016}, {"title": "Fast and accurate k-means for large datasets", "author": ["M. Shindler", "A. Wong", "A. Meyerson"], "venue": "In NIPS*24,", "citeRegEx": "Shindler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shindler et al\\.", "year": 2011}, {"title": "Clustering stability: an overview", "author": ["U. von Luxburg"], "venue": "Found. & Trends in ML,", "citeRegEx": "Luxburg,? \\Q2010\\E", "shortCiteRegEx": "Luxburg", "year": 2010}, {"title": "Differentially private subspace clustering", "author": ["Y. Wang", "Wang", "Y.-X", "A. Singh"], "venue": "In NIPS*28,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "2016). We give an example of the extension of k-variates++to the total", "author": ["Nock"], "venue": null, "citeRegEx": "2015 and Nock,? \\Q2016\\E", "shortCiteRegEx": "2015 and Nock", "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": ", a centroid or a population minimizer (Nock et al., 2016).", "startOffset": 39, "endOffset": 58}, {"referenceID": 2, "context": "The k-means++ non-uniform seeding approach has also been utilized in more complex settings, including tensor clustering, distributed, data stream, on-line and parallel clustering, clustering with non-metric distortions and even clustering with distortions not allowing population minimizers in closed form (Ailon et al., 2009; Balcan et al., 2013; Jegelka et al., 2009; Liberty et al., 2014; Nock et al., 2008; Nielsen & Nock, 2015).", "startOffset": 306, "endOffset": 432}, {"referenceID": 12, "context": "The k-means++ non-uniform seeding approach has also been utilized in more complex settings, including tensor clustering, distributed, data stream, on-line and parallel clustering, clustering with non-metric distortions and even clustering with distortions not allowing population minimizers in closed form (Ailon et al., 2009; Balcan et al., 2013; Jegelka et al., 2009; Liberty et al., 2014; Nock et al., 2008; Nielsen & Nock, 2015).", "startOffset": 306, "endOffset": 432}, {"referenceID": 17, "context": "The k-means++ non-uniform seeding approach has also been utilized in more complex settings, including tensor clustering, distributed, data stream, on-line and parallel clustering, clustering with non-metric distortions and even clustering with distortions not allowing population minimizers in closed form (Ailon et al., 2009; Balcan et al., 2013; Jegelka et al., 2009; Liberty et al., 2014; Nock et al., 2008; Nielsen & Nock, 2015).", "startOffset": 306, "endOffset": 432}, {"referenceID": 16, "context": ", there is limited prior work in a differentially private setting (Nissim et al., 2007; Wang et al., 2015).", "startOffset": 66, "endOffset": 106}, {"referenceID": 21, "context": ", there is limited prior work in a differentially private setting (Nissim et al., 2007; Wang et al., 2015).", "startOffset": 66, "endOffset": 106}, {"referenceID": 16, "context": "We use it directly in a differential privacy setting, addressing a conjecture of (Nissim et al., 2007) with weaker assumptions.", "startOffset": 81, "endOffset": 102}, {"referenceID": 2, "context": "This simple reduction technique allows us to analyze lightweight algorithms that compare favorably to the state of the art in the related domains (Ailon et al., 2009; Balcan et al., 2013; Liberty et al., 2014), from the approximation, assumptions and / or complexity aspects.", "startOffset": 146, "endOffset": 209}, {"referenceID": 12, "context": "This simple reduction technique allows us to analyze lightweight algorithms that compare favorably to the state of the art in the related domains (Ailon et al., 2009; Balcan et al., 2013; Liberty et al., 2014), from the approximation, assumptions and / or complexity aspects.", "startOffset": 146, "endOffset": 209}, {"referenceID": 18, "context": "2 k-variates++ We consider the hard clustering problem (Banerjee et al., 2005; Nock et al., 2016): given set A \u2282 R and integer k > 0, find centers C \u2282 R which minimizes the L2 potential to the centers (here, c(a) .", "startOffset": 55, "endOffset": 97}, {"referenceID": 3, "context": "3 Reductions from k-variates++ Despite tremendous advantages, k-means++ has a serious downside: it is difficult to parallelize, distribute or stream it under relevant communication, space, privacy and/or time resource constraints (Bahmani et al., 2012).", "startOffset": 230, "endOffset": 252}, {"referenceID": 1, "context": "Although extending k-means clustering to these settings has been a major research area in recent years, there has been no obvious solution to tailoring kmeans++ (Ackermann et al., 2010; Ailon et al., 2009; Bahmani et al., 2012; Balcan et al., 2013; Liberty et al., 2014; Shindler et al., 2011) (and others).", "startOffset": 161, "endOffset": 293}, {"referenceID": 2, "context": "Although extending k-means clustering to these settings has been a major research area in recent years, there has been no obvious solution to tailoring kmeans++ (Ackermann et al., 2010; Ailon et al., 2009; Bahmani et al., 2012; Balcan et al., 2013; Liberty et al., 2014; Shindler et al., 2011) (and others).", "startOffset": 161, "endOffset": 293}, {"referenceID": 3, "context": "Although extending k-means clustering to these settings has been a major research area in recent years, there has been no obvious solution to tailoring kmeans++ (Ackermann et al., 2010; Ailon et al., 2009; Bahmani et al., 2012; Balcan et al., 2013; Liberty et al., 2014; Shindler et al., 2011) (and others).", "startOffset": 161, "endOffset": 293}, {"referenceID": 12, "context": "Although extending k-means clustering to these settings has been a major research area in recent years, there has been no obvious solution to tailoring kmeans++ (Ackermann et al., 2010; Ailon et al., 2009; Bahmani et al., 2012; Balcan et al., 2013; Liberty et al., 2014; Shindler et al., 2011) (and others).", "startOffset": 161, "endOffset": 293}, {"referenceID": 19, "context": "Although extending k-means clustering to these settings has been a major research area in recent years, there has been no obvious solution to tailoring kmeans++ (Ackermann et al., 2010; Ailon et al., 2009; Bahmani et al., 2012; Balcan et al., 2013; Liberty et al., 2014; Shindler et al., 2011) (and others).", "startOffset": 161, "endOffset": 293}, {"referenceID": 3, "context": "Property Them Us (1) (Bahmani et al., 2012) Communication complexity O(n` \u00b7 log \u03c61) (expected) O(nk) (2) (Bahmani et al.", "startOffset": 21, "endOffset": 43}, {"referenceID": 3, "context": ", 2012) Communication complexity O(n` \u00b7 log \u03c61) (expected) O(nk) (2) (Bahmani et al., 2012) # data to compute one center m \u2264 maxi\u2208[n](m/mi) (3) (Bahmani et al.", "startOffset": 69, "endOffset": 91}, {"referenceID": 3, "context": ", 2012) # data to compute one center m \u2264 maxi\u2208[n](m/mi) (3) (Bahmani et al., 2012) Data points shared O(` \u00b7 log \u03c61) (expected) k (4) (Bahmani et al.", "startOffset": 60, "endOffset": 82}, {"referenceID": 3, "context": ", 2012) Data points shared O(` \u00b7 log \u03c61) (expected) k (4) (Bahmani et al., 2012) Approximation bound O((log k) \u00b7 \u03c6opt) (2 + log k) \u00b7 ( 10\u03c6opt + 6\u03c6 F s )", "startOffset": 58, "endOffset": 80}, {"referenceID": 2, "context": "(i) (Ailon et al., 2009) Time complexity (outer loop) \u2014 identical \u2014 (ii) (Ailon et al.", "startOffset": 4, "endOffset": 24}, {"referenceID": 2, "context": ", 2009) Time complexity (outer loop) \u2014 identical \u2014 (ii) (Ailon et al., 2009) Approximation bound (2 + log k)(1 + \u03b7) \u00b7 32\u03c6opt (2 + log k) \u00b7 ((8 + 4\u03b7)\u03c6opt + 2\u03c6s ) (a) (Liberty et al.", "startOffset": 56, "endOffset": 76}, {"referenceID": 12, "context": ", 2009) Approximation bound (2 + log k)(1 + \u03b7) \u00b7 32\u03c6opt (2 + log k) \u00b7 ((8 + 4\u03b7)\u03c6opt + 2\u03c6s ) (a) (Liberty et al., 2014) Knowledge required Lowerbound \u03c6\u2217 \u2264 \u03c6opt None (b) (Liberty et al.", "startOffset": 96, "endOffset": 118}, {"referenceID": 12, "context": ", 2014) Knowledge required Lowerbound \u03c6\u2217 \u2264 \u03c6opt None (b) (Liberty et al., 2014) Approximation bound O(logm \u00b7 \u03c6opt) (2 + log k) \u00b7 ( 4 + (32/\u03c2) ) \u03c6opt (A) (Nissim et al.", "startOffset": 57, "endOffset": 79}, {"referenceID": 16, "context": ", 2014) Approximation bound O(logm \u00b7 \u03c6opt) (2 + log k) \u00b7 ( 4 + (32/\u03c2) ) \u03c6opt (A) (Nissim et al., 2007) Knowledge required \u03bb(\u03c6opt) None (B) (Nissim et al.", "startOffset": 81, "endOffset": 102}, {"referenceID": 16, "context": ", 2007) Knowledge required \u03bb(\u03c6opt) None (B) (Nissim et al., 2007) Noise variance (\u03c3) O(\u03bbkR/ ) O(R/( + logm)) (C) (Nissim et al.", "startOffset": 44, "endOffset": 65}, {"referenceID": 16, "context": ", 2007) Noise variance (\u03c3) O(\u03bbkR/ ) O(R/( + logm)) (C) (Nissim et al., 2007) Approximation bound O(\u03c6opt +m\u03bbkR/ ) O(log k(\u03c6opt +mR/( + logm))) (\u03b1) (Wang et al.", "startOffset": 55, "endOffset": 76}, {"referenceID": 21, "context": ", 2007) Approximation bound O(\u03c6opt +m\u03bbkR/ ) O(log k(\u03c6opt +mR/( + logm))) (\u03b1) (Wang et al., 2015) Assumptions on \u03c6opt Several (separability, size of clusters, etc.", "startOffset": 77, "endOffset": 96}, {"referenceID": 21, "context": ") None (\u03b2) (Wang et al., 2015) Approximation bound O(\u03c6opt + km log(m)R/ ) O(log k(\u03c6opt +mR/( + logm)))", "startOffset": 11, "endOffset": 30}, {"referenceID": 3, "context": "\u03c61 is the expected potential of a clustering with a single cluster over the whole data and ` is in general \u03a9(k) (Bahmani et al., 2012).", "startOffset": 112, "endOffset": 134}, {"referenceID": 2, "context": "\u03b7 is the approximation factor of the optimum in (Ailon et al., 2009).", "startOffset": 48, "endOffset": 68}, {"referenceID": 16, "context": "1 in (Nissim et al., 2007).", "startOffset": 5, "endOffset": 26}, {"referenceID": 3, "context": "Distributed clustering We consider horizontally partitioned data among peers, in line with (Bahmani et al., 2012), and a setting significantly more restrictive than theirs: each peer can only locally run the standard operations of Forgy initialisation (that is, uniform random seeding) on its own data, unlike for example the biased distributions of (Bahmani et al.", "startOffset": 91, "endOffset": 113}, {"referenceID": 3, "context": ", 2012), and a setting significantly more restrictive than theirs: each peer can only locally run the standard operations of Forgy initialisation (that is, uniform random seeding) on its own data, unlike for example the biased distributions of (Bahmani et al., 2012).", "startOffset": 244, "endOffset": 266}, {"referenceID": 2, "context": "We note that none of the algorithms of (Ailon et al., 2009; Balcan et al., 2013; Bahmani et al., 2012) would be applicable to this setting without non-trivial modifications affecting their properties.", "startOffset": 39, "endOffset": 102}, {"referenceID": 3, "context": "We note that none of the algorithms of (Ailon et al., 2009; Balcan et al., 2013; Bahmani et al., 2012) would be applicable to this setting without non-trivial modifications affecting their properties.", "startOffset": 39, "endOffset": 102}, {"referenceID": 2, "context": "We authorise the computation / output of the clustering at the end of the stream, but the memory n allowed for all operations satisfies n < m, such as n = m with \u03b1 < 1 in (Ailon et al., 2009).", "startOffset": 171, "endOffset": 191}, {"referenceID": 10, "context": "It relies on the standard \u201ctrick\u201d of summarizing massive datasets via compact representations (synopses) before processing them (Indyk et al., 2014).", "startOffset": 128, "endOffset": 148}, {"referenceID": 2, "context": "(Proof in page 25) It is not surprising to see that Sk-means++ looks like a generalization of (Ailon et al., 2009) and almost matches it (up to the number of centers delivered) when k\u2032 k synopses are learned from k\u2032-means#.", "startOffset": 94, "endOffset": 114}, {"referenceID": 2, "context": "Table 1 compares properties of Sk-means++ to (Ailon et al., 2009) (\u03b7 relates to approximation of the k-means objective in inner loop).", "startOffset": 45, "endOffset": 65}, {"referenceID": 12, "context": "Here, points arrive in a sequence, finite, but of unknown size and too large to fit in memory (Liberty et al., 2014).", "startOffset": 94, "endOffset": 116}, {"referenceID": 12, "context": "In (Liberty et al., 2014), the clustering algorithm is required to have space and time at most polylog in the length of the stream.", "startOffset": 3, "endOffset": 25}, {"referenceID": 12, "context": "Table 1 compares properties of OLk-means++ to (Liberty et al., 2014) (we picked the fully on-line, non-heuristic algorithm).", "startOffset": 46, "endOffset": 68}, {"referenceID": 5, "context": "Examples abound (Hardt & Price, 2014; Kalai & Vempala, 2005; Chaudhuri et al., 2011; Chichignoud & Lousteau, 2014), etc.", "startOffset": 16, "endOffset": 114}, {"referenceID": 16, "context": "Few approaches are related to clustering, yet noise injected is big \u2014 the existence of a smaller, sufficient noise, was conjectured in (Nissim et al., 2007) \u2014 and approaches rely on a variety of assumptions or knowledge about the optimum (See Table 1) (Nissim et al.", "startOffset": 135, "endOffset": 156}, {"referenceID": 16, "context": ", 2007) \u2014 and approaches rely on a variety of assumptions or knowledge about the optimum (See Table 1) (Nissim et al., 2007; Wang et al., 2015).", "startOffset": 103, "endOffset": 143}, {"referenceID": 21, "context": ", 2007) \u2014 and approaches rely on a variety of assumptions or knowledge about the optimum (See Table 1) (Nissim et al., 2007; Wang et al., 2015).", "startOffset": 103, "endOffset": 143}, {"referenceID": 8, "context": "Following is the definition of differential privacy (Dwork et al., 2006), tailored for conciseness to our clustering problem.", "startOffset": 52, "endOffset": 72}, {"referenceID": 8, "context": "We refer to (Dwork et al., 2006) for details, and assume from now on that data belong to a L1 ball B1(0, R).", "startOffset": 12, "endOffset": 32}, {"referenceID": 16, "context": "\u03a62) compare with each other, and how do they compare to the state of the art (Nissim et al., 2007; Wang et al., 2015) (we only consider methods with provable approximation bounds of the global optimum).", "startOffset": 77, "endOffset": 117}, {"referenceID": 21, "context": "\u03a62) compare with each other, and how do they compare to the state of the art (Nissim et al., 2007; Wang et al., 2015) (we only consider methods with provable approximation bounds of the global optimum).", "startOffset": 77, "endOffset": 117}, {"referenceID": 16, "context": "Table 1 compares k-variates++ to (Nissim et al., 2007; Wang et al., 2015) in this large sample regime, which is actually a prerequisite for (Nissim et al.", "startOffset": 33, "endOffset": 73}, {"referenceID": 21, "context": "Table 1 compares k-variates++ to (Nissim et al., 2007; Wang et al., 2015) in this large sample regime, which is actually a prerequisite for (Nissim et al.", "startOffset": 33, "endOffset": 73}, {"referenceID": 16, "context": ", 2015) in this large sample regime, which is actually a prerequisite for (Nissim et al., 2007; Wang et al., 2015).", "startOffset": 74, "endOffset": 114}, {"referenceID": 21, "context": ", 2015) in this large sample regime, which is actually a prerequisite for (Nissim et al., 2007; Wang et al., 2015).", "startOffset": 74, "endOffset": 114}, {"referenceID": 21, "context": "NotationO\u2217 removes all dependencies in their model parameters (assumptions, model parameters, and \u03b4 for the ( , \u03b4)-DP in (Wang et al., 2015)), and \u03bb is the separability assumption parameter (Nissim et al.", "startOffset": 121, "endOffset": 140}, {"referenceID": 16, "context": ", 2015)), and \u03bb is the separability assumption parameter (Nissim et al., 2007)4.", "startOffset": 57, "endOffset": 78}, {"referenceID": 16, "context": "The approximation bounds in (Nissim et al., 2007) consider Wasserstein distance between (estimated / optimal) centers, and not the potential involving data points like us.", "startOffset": 28, "endOffset": 49}, {"referenceID": 3, "context": "Dk-means++ vs k-means++ and k-means\u2016 (Bahmani et al., 2012) To address algorithms that can be reduced from k-variates++ (Section 3), we have tested Dk-means++ vs state of the art approach k-means\u2016; to be fair with Dk-means++, we use k-means++ seeding as the reclustering algorithm in k-means\u2016.", "startOffset": 37, "endOffset": 59}, {"referenceID": 3, "context": "Parameters are in line with (Bahmani et al., 2012).", "startOffset": 28, "endOffset": 50}, {"referenceID": 16, "context": "We sample 4\u03bb is named \u03c6 in (Nissim et al., 2007).", "startOffset": 27, "endOffset": 48}, {"referenceID": 15, "context": "k-variates++ vs Forgy-DP and GUPT To address algorithms that can be obtained via a direct use of k-variates++ (Section 4), we have tested it in a differential privacy framework vs state of the art approach GUPT (Mohan et al., 2012).", "startOffset": 211, "endOffset": 231}, {"referenceID": 2, "context": "The first is a reduction technique from k-variates++, which shows a way to obtain straight approximabilty results for other clustering algorithms, some being efficient proxies for the generalisation of existing approaches (Ailon et al., 2009).", "startOffset": 222, "endOffset": 242}, {"referenceID": 16, "context": "The second is a direct application of k-variates++ to differential privacy, exhibiting a noise component significantly better than existing approaches (Nissim et al., 2007; Wang et al., 2015).", "startOffset": 151, "endOffset": 191}, {"referenceID": 21, "context": "The second is a direct application of k-variates++ to differential privacy, exhibiting a noise component significantly better than existing approaches (Nissim et al., 2007; Wang et al., 2015).", "startOffset": 151, "endOffset": 191}, {"referenceID": 0, "context": "One example are Bregman divergences, that fail simple metric transforms (Acharyya et al., 2013).", "startOffset": 72, "endOffset": 95}, {"referenceID": 18, "context": "Another example are total divergences, that fail the simple computation of the population minimizers (Nock et al., 2016; Liu et al., 2012).", "startOffset": 101, "endOffset": 138}, {"referenceID": 13, "context": "Another example are total divergences, that fail the simple computation of the population minimizers (Nock et al., 2016; Liu et al., 2012).", "startOffset": 101, "endOffset": 138}, {"referenceID": 21, "context": "Comments on Table 1 (Wang et al., 2015) are concerned with approximating subspace clustering, and so they are using a very different potential function, which is, between two subspaces S and S\u2032, d(S, S\u2032) = \u2016UU> \u2212 UU\u2016F , where U (resp.", "startOffset": 20, "endOffset": 39}, {"referenceID": 8, "context": "The statements for \u03c32 and \u03a62 are direct applications of the Laplace mechanism properties (Dwork & Roth, 2014; Dwork et al., 2006).", "startOffset": 89, "endOffset": 129}, {"referenceID": 17, "context": "For example, by noticing that the squared Euclidean distance is merely an example of Bregman divergences (the well-known canonical divergences in information geometry of dually flat spaces), k-variates++ can be been extended to that family of dissimilarities (Nock et al., 2008).", "startOffset": 259, "endOffset": 278}, {"referenceID": 18, "context": "Such hard distortions include the skew Jeffreys \u03b1-centroids (Nock et al., 2016).", "startOffset": 60, "endOffset": 79}, {"referenceID": 18, "context": "This also include the recent class of total Bregman/Jensen divergences that are examples of conformal divergences (Nielsen & Nock, 2015; Nock et al., 2016).", "startOffset": 114, "endOffset": 155}, {"referenceID": 3, "context": "We have compared Dk-means++ to k-means++ and k-means\u2016 (Bahmani et al., 2012).", "startOffset": 54, "endOffset": 76}, {"referenceID": 3, "context": "We also pick ` = 2k, considering that it is a value which gives some of the best experimental results in (Bahmani et al., 2012).", "startOffset": 105, "endOffset": 127}, {"referenceID": 14, "context": "Experiments with k-variates++ and GUPT Among the state-of-the-art approaches against which we could compare k-variates++, there are two major contenders, PINQ (McSherry, 2010) and GUPT (Mohan et al.", "startOffset": 159, "endOffset": 175}, {"referenceID": 15, "context": "Experiments with k-variates++ and GUPT Among the state-of-the-art approaches against which we could compare k-variates++, there are two major contenders, PINQ (McSherry, 2010) and GUPT (Mohan et al., 2012).", "startOffset": 185, "endOffset": 205}, {"referenceID": 15, "context": "k-means implemented in the GUPT proceeds the following way: the dataset is cut in a certain number of blocks ` (following (Mohan et al., 2012), we fix ` = m in our experiments), the usual k-means algorithm is performed on each block.", "startOffset": 122, "endOffset": 142}, {"referenceID": 15, "context": "Note that we disregard the fact that our data are multidimensional, which should require a finer-grained tuning of `, and choose to rely on the ` = m suggestion from (Mohan et al., 2012).", "startOffset": 166, "endOffset": 186}], "year": 2016, "abstractText": "k-means++ seeding has become a de facto standard for hard clustering algorithms. In this paper, our first contribution is a two-way generalisation of this seeding, k-variates++, that includes the sampling of general densities rather than just a discrete set of Dirac densities anchored at the point locations, and a generalisation of the well known Arthur-Vassilvitskii (AV) approximation guarantee, in the form of a bias+variance approximation bound of the global optimum. This approximation exhibits a reduced dependency on the \u201dnoise\u201d component with respect to the optimal potential \u2014 actually approaching the statistical lower bound. We show that k-variates++ reduces to efficient (biased seeding) clustering algorithms tailored to specific frameworks; these include distributed, streaming and on-line clustering, with direct approximation results for these algorithms. Finally, we present a novel application of k-variates++ to differential privacy. For either the specific frameworks considered here, or for the differential privacy setting, there is little to no prior results on the direct application of k-means++ and its approximation bounds \u2014 state of the art contenders appear to be significantly more complex and / or display less favorable (approximation) properties. We stress that our algorithms can still be run in cases where there is no closed form solution for the population minimizer. We demonstrate the applicability of our analysis via experimental evaluation on several domains and settings, displaying competitive performances vs state of the art. 1 ar X iv :1 60 2. 01 19 8v 2 [ cs .L G ] 1 3 Fe b 20 16", "creator": "LaTeX with hyperref package"}}}