{"id": "1602.01595", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2016", "title": "Many Languages, One Parser", "abstract": "We train a language-universal dependency parser on a multilingual collection of treebanks. The parsing model uses multilingual word embeddings alongside learned and specified typological information, enabling generalization based on linguistic universals and based on typological similarities. We evaluate our parser's performance on languages in the training set as well as on the unsupervised scenario where the target language has no trees in the training data, and find that multilingual training outperforms standard supervised training on a single language, and that generalization to unseen languages is competitive with existing model-transfer approaches. The performance is high as a result of multilingual training, and that is particularly high because the model-transfer approach has little to do with memory-wide integration of language into language. We identify two ways to achieve this by combining an unsupervised approach that focuses on the language and system, using unsupervised training as a means to test for specific languages, and using a more practical approach, using multilingual training in the training set as a model-transfer approach that works across language networks (WMS). We also introduce a simple alternative, called multilingual model-transformation. The approach involves removing the memory-wide interlink-to-language dependency of the unsupervised task in the training set, and integrating the unsupervised version of the unsupervised task in the training set as a model-transfer approach that focuses on the language and system. We conclude that the optimization of an unsupervised training model, and the potential for improvement in performance on these networks, is key in understanding how to leverage multilingual learning in training set, and is possible to build a system-transfer model that is in tune with existing model-transfer approaches.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 4 Feb 2016 08:51:18 GMT  (21kb)", "http://arxiv.org/abs/1602.01595v1", null], ["v2", "Wed, 2 Mar 2016 07:14:47 GMT  (27kb)", "http://arxiv.org/abs/1602.01595v2", null], ["v3", "Sat, 14 May 2016 03:07:40 GMT  (27kb)", "http://arxiv.org/abs/1602.01595v3", null], ["v4", "Tue, 26 Jul 2016 06:30:50 GMT  (27kb,D)", "http://arxiv.org/abs/1602.01595v4", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["waleed ammar", "george mulcaire", "miguel ballesteros", "chris dyer", "noah a smith"], "accepted": true, "id": "1602.01595"}, "pdf": {"name": "1602.01595.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Waleed Ammar", "George Mulcaire", "Miguel Ballesteros", "Chris Dyer", "Noah A. Smith"], "emails": ["wammar@cs.cmu.edu,", "gmulc@uw.edu,", "miguel.ballesteros@upf.edu", "cdyer@cs.cmu.edu,", "nasmith@cs.washington.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n01 59\n5v 1\n[ cs\n.C L\n] 4\nF eb"}, {"heading": "1 Introduction", "text": "Developing tools for processing many languages has long been an important goal in NLP (R\u00f6sner, 1988; Heid and Raab, 1989),1 but it was not until many researchers adopted statistical methods that it became economical to train multilingual NLP models. Although most NLP models still focus on English, a growing number of researchers develop and evaluate their models on more languages. The mainstream approach for multilingual NLP is to design language-specific models. For each language of interest, the resources necessary for training the model are created, and separate parameters are fit for each language separately. This approach is simple, effective and grants the flexibility\n1As of 2007, the total number of native speakers of the hundred most popular languages only accounts for 85% of the world\u2019s population (Wikipedia, 2016).\nof customizing the model or features to the needs of each language independently, but it is suboptimal for theoretical as well as practical reasons. Theoretically, the study of linguistic typology tells us that many languages share morphological, phonological, and syntactic phenomena (Bender, 2011). Practically, it is painful to deploy multilingual NLP tools because, for each language of interest, we need to configure, train, tune, monitor and occasionally update the model. Code-switching or codemixing (mixing more than one language in the same discourse) presents a challenge for monolinguallytrained NLP models, exacerbated on social media outlets (Barman et al., 2014).\nIn parsing, the availability of homogeneous syntactic dependency annotations in many languages (McDonald et al., 2013; Nivre et al., 2015b; Agic\u0301 et al., 2015; Nivre et al., 2015a) presents the opportunity to develop a language-universal dependency parser: one that is capable of parsing sentences in multiple languages of interest. Such language-universal parser can potentially replace an array of language-specific monolingually-trained parsers (for languages with a treebank). The same parser may also be used to parse languages for which no treebank is available, as explored by Cohen et al. (2011), McDonald et al. (2011), Zhang and Barzilay (2015), and Guo et al. (2016). Although multilingual dependency treebanks have been available for a decade via the 2006 and 2007 CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), the treebank of each language was annotated independently and used inconsistent annotation conventions.\nWe propose a parsing architecture that takes as input a sentence in any language that meets our resource requirements.2 The parser is trained on the union of available universal dependency treebanks in different languages. Our approach integrates and critically relies on several recent developments in the field including universal POS tags (Petrov et al., 2012), multilingual word clusters (T\u00e4ckstr\u00f6m et al., 2012), universal dependency treebanks (McDonald et al., 2013), neural network architectures for transitionbased parsing (Chen and Manning, 2014; Dyer et al., 2015), character-based embeddings (Ballesteros et al., 2015), multilingual word embeddings and language embeddings. We show that our language-universal parser outperforms (1) individual monolingually-trained parsers, and (2) previous multi-source model transfer methods used to parse low-resource languages. Our language-universal parser is publicly available at http://bit.ly/1SJGUKi."}, {"heading": "2 Overview", "text": "Our goal is to train a dependency parser for a set of target languages Lt, given treebanks in a set of source languages Ls. For all languages in L = Lt \u222a Ls, we require:\n\u2022 a trained part-of-speech tagger with the coarse tagset used in the universal dependency treebanks;\n\u2022 a bilingual dictionary or a parallel corpus with another language in L, such that all languages are connected); and\n\u2022 a monolingual corpus.\nWe optionally use typological properties which are readily available for thousands of languages, although the best performing parser does not make use of language typology. We also optionally use language-specific fine-grained POS tags when available.\nIn the fully supervised scenario (i.e., Lt \u2286 Ls), the mainstream approach has been to train one monolingual parser per target language and route\n2The minimum requirements are: (i) a coarse part-of-speech tagger, (ii) a bilingual dictionary or parallel corpus that \u201cconnects\u201d it (perhaps transitively) to a language with a treebank in the training dataset.\nsentences of a given language to the corresponding parser at test time. In contrast, our approach is to train one parsing model with the union of treebanks in all source languages, then use this single trained model to parse text in any of the target languages. This approach is expected to perform well if the model uses a similar representation for words and other syntactic elements across languages, so that data from each language informs parsing in all the others."}, {"heading": "3 Parsing Model", "text": "Recent advances in machine learning suggest that recurrent neural networks, especially long shortterm memory (LSTM) architectures, are capable of learning useful representations for modeling problems of sequential nature (Graves et al., 2013; Sutskever et al., 2014). In this section, we describe our language-universal parser, which extends the stack LSTM (S-LSTM) parser of Dyer et al. (2015)."}, {"heading": "3.1 Transition-based Parsing with S-LSTMs", "text": "Dyer et al.\u2019s S-LSTM parser, like most transitionbased parsers, can be understood as the sequential manipulation of three data structures: a buffer B (from which we read the token sequence), a stack S (which contains partially-built parses), and a list A of actions previously taken by the parser. In particular, we use the arc-standard parsing algorithm (Nivre, 2004).3\nAt each timestep t, a transition action is applied that alters these data structures according to Table 1, which is reproduced from Dyer et al. (2015).\nAlong with the discrete transitions above, the parser calculates a vector representation of the states of B, S, and A; at time t these are denoted by bt, st, and at, respectively. The parser state at time t is given by:\npt = max {0,W[st;bt;at] + d} (1)\nwhere the matrix W and the vector d are learned parameters. This continuous-state representation pt is\n3Since the arc-standard transition system assumes projectivity, we use pseudo-projective transformations (Nivre and Nilsson, 2005) of non-projective trees in the training treebanks.\nused to decide which action z to apply next, updating B, S, and A accordingly. The probability of an action z is defined to be:\np(z | pt) = exp\n( g\u22a4z pt + qz )\n\u2211 z\u2032\u2208A(S,B) exp ( g\u22a4 z\u2032 pt + qz\u2032 )\nwhere A(S,B) is the set of actions which meet the preconditions in Fig. 1, and gz and qz are parameters associated with each action type z.\nWe train the model by maximizing the loglikelihood of correct actions. At test time, the parser greedily chooses the most probable action in every time step until a complete parse tree is produced. Dyer et al. (2015) provide more details on the core parser.\nOptimization. Instead of updating the parameters with the gradient of each sentence, we use a mini-batch which includes one sentence sampled uniformly from each language. This prevents one source language with plenty of training data (e.g., German) from dominating source languages with small training data (e.g., Swedish). Therefore, each epoch updates the model a number of times equal to the size of the smallest training treebank used (Swedish). We use the stochastic optimization algorithm \u201cadam\u201d (Kingma and Ba, 2014) to fit model parameters, and use early stopping based on the UAS score on a development set.\nToken representation. For monolingual parsing, we represent each token by concatenating four vectors: a learned word embedding, a fixed pretrained word embedding, a coarse POS embedding and, when available, a fine-grained POS embedding. For language-universal parsing, we start with a delexicalized model where the token representation only consists of the coarse POS embedding. Most of the modifications we discuss in subsequent subsections\nprovide the parser with multilingually-defined information at the token level to help make better predictions."}, {"heading": "3.2 Lexical Embeddings", "text": "Previous work has shown that sacrificing lexical features in dependency parsing amounts to a substantial decrease in the performance of a dependency parser (Cohen et al., 2011; T\u00e4ckstr\u00f6m et al., 2012; Tiedemann, 2015; Guo et al., 2015). We use two kinds of multilingually-defined lexical features to improve our language-universal parser.\nBefore training the parser, we estimate Brown clusters of English words and project them via word alignments to words in other languages. This is similar to the projected clusters of T\u00e4ckstr\u00f6m et al. (2012). To go from Brown clusters to embeddings, we ignore the hierarchy within Brown clusters and assign a unique parameter vector to each cluster.\nWe also use Guo et al.\u2019s (2016) multilingual robust projection method to pretrain embeddings of words in all source and target languages. In this method, we start by learning embeddings for English words using the skip-gram model (Mikolov et al., 2013). The next step computes an embedding of each word in the other languages as the weighted average of English word embeddings, using word alignment probabilities as weights. The last step computes an embedding of words for which we did not already compute an embedding by averaging the embeddings of all words within an edit distance of 1 in the same language."}, {"heading": "3.3 Language Embeddings", "text": "While many languages, especially ones that belong to the same family, exhibit some similar syntactic phenomena (e.g., all languages have subjects, verbs,\nand objects), different languages often exhibit substantial syntactic differences. Some of these differences are easy to characterize (e.g., subject-verbobject vs. verb-subject-object, prepositions vs. postpositions, adjective-noun vs. noun-adjective), while others are subtle (e.g., number and positions of negation morphemes).\nConsequently, training a language-universal parser on treebanks in multiple source languages requires caution. While exposing the parser to a diverse set of syntactic patterns across many languages has the potential to improve its performance in each, dependency annotations in one language will, in some ways, contradict those in typologically different languages.\nFor instance, consider a context where the next word on the buffer is a noun, and the top word on the stack is an adjective, followed by a noun. Treebanks of languages where postpositive adjectives are typical (e.g., French) will often teach the parser to predict REDUCE-LEFT, while those of languages where prepositive adjectives are typical (e.g., English) will teach the parser to predict SHIFT.\nInspired by Naseem et al. (2012), we address this problem by informing the parser about the input language it is currently parsing. Given a vector embedding of the language of the input, l, we apply a squashed (tanh) affine transformation and feedin to the token and action representations, as well as directly to the parser state:\npt = max {0,W[st;bt;at; l] + d}\nWe used three language descriptors in l: a one-hot representation of the input language, one-hot representations of word-order properties of the input language,4 and smoothed values of all WALS typological features of the input language.5\n4The World Atlas of Language Structures (WALS; Dryer and Haspelmath, 2013) is an online portal documenting typological properties of 2,679 languages (as of July 2015). We use the same set of WALS features used by Zhang and Barzilay (2015), namely 82A (order of subject and verb), 83A (order of object and verb), 85A (order of adposition and noun phrase), 86A (order of genitive and noun), and 87A (order of adjective and noun).\n5Since WALS features are not annotated for all languages, we use the average value of all languages in the same genus."}, {"heading": "3.4 Fine-grained POS Tags", "text": "Tiedemann (2015) shows that omitting fine-grained POS tags significantly hurts the performance of a dependency parser. However, those fine-grained POS tagsets are defined monolingually and are only available for a subset of the languages with universal dependency treebanks.\nWe extend the token representation to include a fine-grained POS embedding (in addition to the coarse POS embedding). We stochastically dropout the fine-grained POS embedding with 50% probability (Srivastava et al., 2014) so that the parser can make use of fine-grained POS tags when available but stays reliable when the fine-grained POS tags are missing."}, {"heading": "4 Experiments", "text": "When parsing target languages with treebanks (i.e., Lt \u2286 Ls), we use the standard data splits for German (de), English (en), Spanish (es), French (fr), Italian (it), Portuguese (pt) and Swedish (sv) in the latest release (version 1.2) of the universal dependencies.6 Of those, three are Germanic languages (de, en, sv) and four are Romance languages (es, fr, it, pt).\nWe also compare to previous work on multisource model transfer (i.e., Lt \u2229 Ls = \u2205) which used different (and older) treebanks, namely version 2.0 of the universal dependency treebanks.7 Table 2 gives the number of sentences and words annotated for each language.\nFollowing Zhang and Barzilay (2015) and Guo et al. (2016), we use the gold-standard POS tags provided in all splits of the treebanks we use. We use the same multilingual Brown clusters and multilingual embeddings used by Guo et al. (2016), kindly provided by the authors.\n4.1 Target Languages with a Treebank (Lt = Ls)\nWe compare several variants of the languageuniversal parser with the monolingual baselines using unlabeled attachment scores (UAS) and labeled attachment scores (LAS). We use seven IndoEuropean languages as both the source and target\n6http://hdl.handle.net/11234/1-1548 7https://github.com/ryanmcd/uni-dep-tb\nlanguages, i.e., Lt = Ls.\nBaseline models. For each language, the strong baseline we use is a monolingually-trained S-LSTM parser with a token representation which concatenates: pretrained word embeddings (50 dimensions),8 learned word embeddings (50 dimensions), coarse POS tag embeddings (12 dimensions), fine POS tag embeddings (12 dimensions), and Brown cluster embeddings (12 dimensions). All embeddings are learned for each language independently, except the pretrained word embeddings. The parser uses a two-layer S-LSTM for each of S, B, and A. This parser is denoted monolingual.\nLanguage universal models. The training set concantenates the training sections of all languages, while the development set concatenates the first 300 sentences of the dev section of each language. We do not tune the hyperparameters of the S-LSTM parser, and use the same values used for training the monolingual baselines.\nThe first language-universal parser only uses coarse POS embeddings as the token representation. As shown in Table 3, this parser consistently performs much worse (12.5 LAS points on average) than the monolingual baselines.\nAdding lexical embeddings to the token representation as described in \u00a73.2, +lexical substantially improves the performance of language-universal models, recovering 83% of the LAS gap between monolingual and language-universal on average. +language ID further improves the performance using language embeddings as described in\n8These embeddings are treated as fixed inputs to the parser, and are not optimized towards the parsing objective. We use the same embeddings used in Guo et al. (2016).\n\u00a73.3, recovering another 12% of the original gap. This model uses the language ID descriptor. The other two language descriptors we considered, full typology and word order, also outperform the previous model (+lexical) and give an average LAS of 82.5 and 83.2, respectively.\nFinally, our best model is +fine-grained POS, which uses fine-grained POS embeddings when available (for English, Italian, Portuguese and Swedish). Surprisingly, adding fine-grained POS embeddings improve the performance even for languages where fine-grained POS tags are not available (i.e., German, Spanish and French). This model outperforms monolingual in five (out of seven) target languages, and wins on average by 0.3 LAS points. We emphasize that this model is only trained once on all languages, and the same model is used to parse the test set of each language, which simplifies the deployment of multilingual parsers in production systems."}, {"heading": "4.2 Target Languages without a Treebank", "text": "(Lt \u2229 Ls = \u2205)\nMcDonald et al. (2011) established that, when no treebank annotations are available in the target language, training on multiple source languages outperforms training on one (i.e., multi-source model transfer outperforms single-source model transfer). In this section, we evaluate the performance of our S-LSTM language-universal parser in this setup. We use two strong baseline multi-source model transfer parsers with no supervision in the target language:\n\u2022 Zhang and Barzilay (2015) is a graph-based arcfactored parsing model with a tensor-based scoring function. It takes typological properties of a language as input. We compare to the best\nreported configuration (i.e., the column titled \u201cOURS\u201d in Table 5 of Zhang and Barzilay, 2015).\n\u2022 Guo et al. (2016) is a transition-based neural-network parsing model based on Chen and Manning (2014). It uses a multilingual embeddings and Brown clusters as lexical features. We compare to the best reported configuration (i.e., the column titled \u201cMULTI-PROJ\u201d in Table 1 of Guo et al., 2016).\nFollowing Guo et al. (2016), for each target language, we train the parser on six other languages of the Indo-European family in the Google universal dependency treebanks version 2.09 (de, en, es, fr, it, pt, sv, excluding whichever is the target language). Our language-universal +lexical parser uses the same word embeddings and word clusters used in Guo et al. (2016), and does not use any typology information.10\nThe results in Table 4 show that, on average, our parser outperforms both baselines by more than 1\n9https://github.com/ryanmcd/uni-dep-tb/ 10In preliminary experiments, we found language embeddings to hurt the performance of language-universal models for target languages without a treebank.\npoint in both UAS and LAS, and gives the best LAS results in four (out of six) languages."}, {"heading": "5 Related Work", "text": "Our work builds on many past ideas about parsing low-resource languages with no or few tree annotations. Zeman and Resnik (2008) pioneered the model transfer approach by training a parser on a source language treebank then applying it to parse sentences in a target language. Cohen et al. (2011) and McDonald et al. (2011) used treebanks in multiple source languages for training unlexicalized parsers applied to target languages. To add lexical information, T\u00e4ckstr\u00f6m et al. (2012) used multilingual word clusters, while Xiao and Guo (2014), Guo et al. (2015) and Guo et al. (2016) used multilingual word embeddings. Naseem et al. (2012), T\u00e4ckstr\u00f6m et al. (2013), and Zhang and Barzilay (2015) used language typology to improve model transfer.\nAnother popular approach for cross-lingual parsing is to project tree annotations from the source language to the target language via a parallel corpus (Yarowsky et al., 2001; Hwa et al., 2005)\nor via automatically-translated sentences (Tiedemann et al., 2014). Ma and Xia (2014) used entropy regularization to learn from both parallel data (with projected annotations) and unlabeled data in the target language. Rasooli and Collins (2015) trained an array of target-language parsers on fully annotated trees, by iteratively decoding sentences in the target language with incomplete annotations."}, {"heading": "6 Conclusion", "text": "We propose a language-universal parser trained on multiple treebanks. The language universal approach simplifies the process of deploying parsers for multiple languages. We show that the languageuniversal parser, equipped with language embeddings and fine-grained POS embeddings, on average outperforms monolingually-trained parsers for target languages with a treebank. For languages with no treebank, we show that our parser outperforms previous work on cross-lingual multi-source model transfer."}, {"heading": "Acknowledgments", "text": "Waleed Ammar is supported by the Google fellowship in natural language processing. Miguel Ballesteros is supported by the European Commission under the contract numbers FP7-ICT-610411 (project MULTISENSOR) and H2020-RIA-645012 (project KRISTINA). Part of this material is based upon work supported by a subcontract with Raytheon BBN Technologies Corp. under DARPA Prime Contract No. HR0011-15-C-0013. We thank Jiang Guo\nfor sharing the multilingual word embeddings and multilingual word clusters. We thank Lori Levin, Ryan McDonald, J\u00f6rg Tiedemann, Yulia Tsvetkov and Yuan Zhang for helpful discussions."}], "references": [{"title": "Improved transition-based parsing by modeling characters instead of words with LSTMs", "author": ["Chris Dyer", "Noah A. Smith"], "venue": "In Proc. of EMNLP", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Code mixing: A challenge for language identification in the language of social media", "author": ["Barman et al.2014] Utsab Barman", "Amitava Das", "Joachim Wagner", "Jennifer Foster"], "venue": "EMNLP", "citeRegEx": "Barman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Barman et al\\.", "year": 2014}, {"title": "On achieving and evaluating language-independence in NLP", "author": ["Emily Bender"], "venue": null, "citeRegEx": "Bender.,? \\Q2011\\E", "shortCiteRegEx": "Bender.", "year": 2011}, {"title": "CoNLL-X shared task on multilingual dependency parsing", "author": ["Buchholz", "Marsi2006] Sabine Buchholz", "Erwin Marsi"], "venue": "In CoNLL-X", "citeRegEx": "Buchholz et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Buchholz et al\\.", "year": 2006}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher Manning"], "venue": "In Proc. of EMNLP", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Unsupervised structure prediction with non-parallel multilingual guidance", "author": ["Cohen et al.2011] Shay B. Cohen", "Dipanjan Das", "Noah A. Smith"], "venue": "In Proc. of EMNLP", "citeRegEx": "Cohen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2011}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proc. of ACL", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves et al.2013] Alan Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "In Proc. of ICASSP", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Crosslingual dependency parsing based on distributed representations", "author": ["Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu"], "venue": "In Proc. of ACL", "citeRegEx": "Guo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "A representation learning framework for multi-source transfer parsing", "author": ["Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu"], "venue": "In Proc. of AAAI", "citeRegEx": "Guo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2016}, {"title": "Collocations in multilingual generation", "author": ["Heid", "Raab1989] Ulrich Heid", "Sybille Raab"], "venue": "In Proc. of EACL", "citeRegEx": "Heid et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Heid et al\\.", "year": 1989}, {"title": "Bootstrapping parsers via syntactic projection across parallel texts", "author": ["Hwa et al.2005] Rebecca Hwa", "Philip Resnik", "Amy Weinberg", "Clara Cabezas", "Okan Kolak"], "venue": "Natural language engineering,", "citeRegEx": "Hwa et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hwa et al\\.", "year": 2005}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Unsupervised dependency parsing with transferring distribution via parallel guidance and entropy regularization", "author": ["Ma", "Xia2014] Xuezhe Ma", "Fei Xia"], "venue": "In Proc. of ACL", "citeRegEx": "Ma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2014}, {"title": "Multi-source transfer of delexicalized dependency parsers", "author": ["Slav Petrov", "Keith Hall"], "venue": "In Proc. of EMNLP", "citeRegEx": "McDonald et al\\.,? \\Q2011\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2011}, {"title": "Selective sharing for multilin", "author": ["Amir Globerson"], "venue": null, "citeRegEx": "Globerson.,? \\Q2012\\E", "shortCiteRegEx": "Globerson.", "year": 2012}, {"title": "Universal dependencies 1.0. LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics, Charles University in Prague", "author": ["Veronika Vincze", "Daniel Zeman"], "venue": null, "citeRegEx": "Vincze and Zeman.,? \\Q2015\\E", "shortCiteRegEx": "Vincze and Zeman.", "year": 2015}, {"title": "Incrementality in deterministic dependency parsing", "author": ["Joakim Nivre"], "venue": "In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together", "citeRegEx": "Nivre.,? \\Q2004\\E", "shortCiteRegEx": "Nivre.", "year": 2004}, {"title": "A universal part-of-speech tagset", "author": ["Petrov et al.2012] Slav Petrov", "Dipanjan Das", "Ryan McDonald"], "venue": "In Proc. of LREC", "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Density-driven crosslingual transfer of dependency parsers", "author": ["Rasooli", "Michael Collins"], "venue": "In Proc. of EMNLP", "citeRegEx": "Rasooli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasooli et al\\.", "year": 2015}, {"title": "The generation system of the semsyn project: Towards a taskindependent generator for german", "author": ["Deitmar R\u00f6sner"], "venue": "Advances in Natural Language Generation,", "citeRegEx": "R\u00f6sner.,? \\Q1988\\E", "shortCiteRegEx": "R\u00f6sner.", "year": 1988}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "In Proc. of JMLR", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Proc. of NIPS", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["Ryan McDonald", "Jakob Uszkoreit"], "venue": "In Proc. of NAACL-HLT", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2012\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "Token and type constraints for cross-lingual part-of-speech tagging", "author": ["Dipanjan Das", "Slav Petrov", "Ryan McDonald", "Joakim Nivre"], "venue": "In Proc. of TACL", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2013\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2013}, {"title": "Treebank translation for crosslingual parser induction", "author": ["Zeljko Agic", "Joakim Nivre"], "venue": "In Proc. of CoNLL", "citeRegEx": "Tiedemann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tiedemann et al\\.", "year": 2014}, {"title": "Cross-lingual dependency parsing with universal dependencies and predicted pos labels", "author": ["Jorg Tiedemann"], "venue": "In Proc. of Depling", "citeRegEx": "Tiedemann.,? \\Q2015\\E", "shortCiteRegEx": "Tiedemann.", "year": 2015}, {"title": "Distributed word representation learning for crosslingual dependency parsing", "author": ["Xiao", "Guo2014] Min Xiao", "Yuhong Guo"], "venue": "In Proc. of CoNLL", "citeRegEx": "Xiao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2014}, {"title": "Inducing multilingual text analysis tools via robust projection across aligned corpora", "author": ["Grace Ngai", "Richard Wicentowski"], "venue": "In Proc. of HLT", "citeRegEx": "Yarowsky et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Yarowsky et al\\.", "year": 2001}, {"title": "Cross-language parser adaptation between related languages", "author": ["Zeman", "Resnik2008] Daniel Zeman", "Philip Resnik"], "venue": "In Proc. of IJCNLP", "citeRegEx": "Zeman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zeman et al\\.", "year": 2008}, {"title": "Hierarchical low-rank tensors for multilingual transfer parsing", "author": ["Zhang", "Barzilay2015] Yuan Zhang", "Regina Barzilay"], "venue": "In Proc. of EMNLP", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "Developing tools for processing many languages has long been an important goal in NLP (R\u00f6sner, 1988; Heid and Raab, 1989),1 but it was not until many researchers adopted statistical methods that it became economical to train multilingual NLP models.", "startOffset": 86, "endOffset": 121}, {"referenceID": 2, "context": "Theoretically, the study of linguistic typology tells us that many languages share morphological, phonological, and syntactic phenomena (Bender, 2011).", "startOffset": 136, "endOffset": 150}, {"referenceID": 1, "context": "Code-switching or codemixing (mixing more than one language in the same discourse) presents a challenge for monolinguallytrained NLP models, exacerbated on social media outlets (Barman et al., 2014).", "startOffset": 177, "endOffset": 198}, {"referenceID": 5, "context": "for which no treebank is available, as explored by Cohen et al. (2011), McDonald et al.", "startOffset": 51, "endOffset": 71}, {"referenceID": 5, "context": "for which no treebank is available, as explored by Cohen et al. (2011), McDonald et al. (2011), Zhang and Barzilay (2015), and Guo et al.", "startOffset": 51, "endOffset": 95}, {"referenceID": 5, "context": "for which no treebank is available, as explored by Cohen et al. (2011), McDonald et al. (2011), Zhang and Barzilay (2015), and Guo et al.", "startOffset": 51, "endOffset": 122}, {"referenceID": 5, "context": "for which no treebank is available, as explored by Cohen et al. (2011), McDonald et al. (2011), Zhang and Barzilay (2015), and Guo et al. (2016). Although multilingual dependency treebanks have been available for a decade via the 2006 and 2007 CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al.", "startOffset": 51, "endOffset": 145}, {"referenceID": 18, "context": "Our approach integrates and critically relies on several recent developments in the field including universal POS tags (Petrov et al., 2012), multilingual word clusters (T\u00e4ckstr\u00f6m et al.", "startOffset": 119, "endOffset": 140}, {"referenceID": 23, "context": ", 2012), multilingual word clusters (T\u00e4ckstr\u00f6m et al., 2012), universal dependency treebanks (McDonald et al.", "startOffset": 36, "endOffset": 60}, {"referenceID": 6, "context": ", 2013), neural network architectures for transitionbased parsing (Chen and Manning, 2014; Dyer et al., 2015), character-based embeddings (Ballesteros et al.", "startOffset": 66, "endOffset": 109}, {"referenceID": 0, "context": ", 2015), character-based embeddings (Ballesteros et al., 2015), multilingual word embeddings and language embeddings.", "startOffset": 36, "endOffset": 62}, {"referenceID": 7, "context": "Recent advances in machine learning suggest that recurrent neural networks, especially long shortterm memory (LSTM) architectures, are capable of learning useful representations for modeling problems of sequential nature (Graves et al., 2013; Sutskever et al., 2014).", "startOffset": 221, "endOffset": 266}, {"referenceID": 22, "context": "Recent advances in machine learning suggest that recurrent neural networks, especially long shortterm memory (LSTM) architectures, are capable of learning useful representations for modeling problems of sequential nature (Graves et al., 2013; Sutskever et al., 2014).", "startOffset": 221, "endOffset": 266}, {"referenceID": 6, "context": "In this section, we describe our language-universal parser, which extends the stack LSTM (S-LSTM) parser of Dyer et al. (2015).", "startOffset": 108, "endOffset": 127}, {"referenceID": 17, "context": "In particular, we use the arc-standard parsing algorithm (Nivre, 2004).", "startOffset": 57, "endOffset": 70}, {"referenceID": 6, "context": "At each timestep t, a transition action is applied that alters these data structures according to Table 1, which is reproduced from Dyer et al. (2015).", "startOffset": 132, "endOffset": 151}, {"referenceID": 6, "context": "Dyer et al. (2015) provide more details on the core parser.", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "Previous work has shown that sacrificing lexical features in dependency parsing amounts to a substantial decrease in the performance of a dependency parser (Cohen et al., 2011; T\u00e4ckstr\u00f6m et al., 2012; Tiedemann, 2015; Guo et al., 2015).", "startOffset": 156, "endOffset": 235}, {"referenceID": 23, "context": "Previous work has shown that sacrificing lexical features in dependency parsing amounts to a substantial decrease in the performance of a dependency parser (Cohen et al., 2011; T\u00e4ckstr\u00f6m et al., 2012; Tiedemann, 2015; Guo et al., 2015).", "startOffset": 156, "endOffset": 235}, {"referenceID": 26, "context": "Previous work has shown that sacrificing lexical features in dependency parsing amounts to a substantial decrease in the performance of a dependency parser (Cohen et al., 2011; T\u00e4ckstr\u00f6m et al., 2012; Tiedemann, 2015; Guo et al., 2015).", "startOffset": 156, "endOffset": 235}, {"referenceID": 8, "context": "Previous work has shown that sacrificing lexical features in dependency parsing amounts to a substantial decrease in the performance of a dependency parser (Cohen et al., 2011; T\u00e4ckstr\u00f6m et al., 2012; Tiedemann, 2015; Guo et al., 2015).", "startOffset": 156, "endOffset": 235}, {"referenceID": 23, "context": "This is similar to the projected clusters of T\u00e4ckstr\u00f6m et al. (2012). To go from Brown clusters to embeddings, we ignore the hierarchy within Brown clusters and assign a unique parameter vector to each cluster.", "startOffset": 45, "endOffset": 69}, {"referenceID": 8, "context": "We also use Guo et al.\u2019s (2016) multilingual robust projection method to pretrain embeddings of words in all source and target languages.", "startOffset": 12, "endOffset": 32}, {"referenceID": 21, "context": "We stochastically dropout the fine-grained POS embedding with 50% probability (Srivastava et al., 2014) so that the parser can make use of fine-grained POS tags when available but stays reliable when the fine-grained POS tags are missing.", "startOffset": 78, "endOffset": 103}, {"referenceID": 8, "context": "Following Zhang and Barzilay (2015) and Guo et al. (2016), we use the gold-standard POS tags provided in all splits of the treebanks we use.", "startOffset": 40, "endOffset": 58}, {"referenceID": 8, "context": "Following Zhang and Barzilay (2015) and Guo et al. (2016), we use the gold-standard POS tags provided in all splits of the treebanks we use. We use the same multilingual Brown clusters and multilingual embeddings used by Guo et al. (2016), kindly provided by the authors.", "startOffset": 40, "endOffset": 239}, {"referenceID": 8, "context": "We use the same embeddings used in Guo et al. (2016). \u00a73.", "startOffset": 35, "endOffset": 53}, {"referenceID": 8, "context": "\u2022 Guo et al. (2016) is a transition-based neural-network parsing model based on Chen and Manning (2014).", "startOffset": 2, "endOffset": 20}, {"referenceID": 8, "context": "\u2022 Guo et al. (2016) is a transition-based neural-network parsing model based on Chen and Manning (2014). It uses a multilingual embeddings and Brown clusters as lexical features.", "startOffset": 2, "endOffset": 104}, {"referenceID": 8, "context": "Following Guo et al. (2016), for each target lan-", "startOffset": 10, "endOffset": 28}, {"referenceID": 8, "context": "Our language-universal +lexical parser uses the same word embeddings and word clusters used in Guo et al. (2016), and does not use any typology information.", "startOffset": 95, "endOffset": 113}, {"referenceID": 5, "context": "Cohen et al. (2011) and McDonald et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "Cohen et al. (2011) and McDonald et al. (2011) used treebanks in multiple source languages for training unlexicalized parsers applied to target languages.", "startOffset": 0, "endOffset": 47}, {"referenceID": 5, "context": "Cohen et al. (2011) and McDonald et al. (2011) used treebanks in multiple source languages for training unlexicalized parsers applied to target languages. To add lexical information, T\u00e4ckstr\u00f6m et al. (2012) used multi-", "startOffset": 0, "endOffset": 207}, {"referenceID": 8, "context": "lingual word clusters, while Xiao and Guo (2014), Guo et al. (2015) and Guo et al.", "startOffset": 50, "endOffset": 68}, {"referenceID": 8, "context": "lingual word clusters, while Xiao and Guo (2014), Guo et al. (2015) and Guo et al. (2016) used multilingual word embeddings.", "startOffset": 50, "endOffset": 90}, {"referenceID": 8, "context": "lingual word clusters, while Xiao and Guo (2014), Guo et al. (2015) and Guo et al. (2016) used multilingual word embeddings. Naseem et al. (2012), T\u00e4ckstr\u00f6m et al.", "startOffset": 50, "endOffset": 146}, {"referenceID": 8, "context": "lingual word clusters, while Xiao and Guo (2014), Guo et al. (2015) and Guo et al. (2016) used multilingual word embeddings. Naseem et al. (2012), T\u00e4ckstr\u00f6m et al. (2013), and Zhang and Barzilay (2015) used language", "startOffset": 50, "endOffset": 171}, {"referenceID": 8, "context": "lingual word clusters, while Xiao and Guo (2014), Guo et al. (2015) and Guo et al. (2016) used multilingual word embeddings. Naseem et al. (2012), T\u00e4ckstr\u00f6m et al. (2013), and Zhang and Barzilay (2015) used language", "startOffset": 50, "endOffset": 202}, {"referenceID": 28, "context": "pus (Yarowsky et al., 2001; Hwa et al., 2005)", "startOffset": 4, "endOffset": 45}, {"referenceID": 11, "context": "pus (Yarowsky et al., 2001; Hwa et al., 2005)", "startOffset": 4, "endOffset": 45}, {"referenceID": 8, "context": "4 Guo et al. (2016) 65.", "startOffset": 2, "endOffset": 20}, {"referenceID": 8, "context": "9 Guo et al. (2016) 55.", "startOffset": 2, "endOffset": 20}, {"referenceID": 25, "context": "or via automatically-translated sentences (Tiedemann et al., 2014).", "startOffset": 42, "endOffset": 66}, {"referenceID": 25, "context": "or via automatically-translated sentences (Tiedemann et al., 2014). Ma and Xia (2014) used entropy regularization to learn from both parallel data (with projected annotations) and unlabeled data in the target language.", "startOffset": 43, "endOffset": 86}, {"referenceID": 25, "context": "or via automatically-translated sentences (Tiedemann et al., 2014). Ma and Xia (2014) used entropy regularization to learn from both parallel data (with projected annotations) and unlabeled data in the target language. Rasooli and Collins (2015) trained an array of target-language parsers on fully annotated trees, by iteratively decoding sentences in the target language with incomplete annotations.", "startOffset": 43, "endOffset": 246}], "year": 2016, "abstractText": "We train a language-universal dependency parser on a multilingual collection of treebanks. The parsing model uses multilingual word embeddings alongside learned and specified typological information, enabling generalization based on linguistic universals and based on typological similarities. We evaluate our parser\u2019s performance on languages in the training set as well as on the unsupervised scenario where the target language has no trees in the training data, and find that multilingual training outperforms standard supervised training on a single language, and that generalization to unseen languages is competitive with existing model-transfer approaches.", "creator": "LaTeX with hyperref package"}}}