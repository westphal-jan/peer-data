{"id": "1602.02018", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Feb-2016", "title": "Compressive Spectral Clustering", "abstract": "Spectral clustering has become a popular technique due to its high performance in many contexts. It comprises three main steps: create a similarity graph between N objects to cluster, compute the first k eigenvectors of its Laplacian matrix to define a feature vector for each object, and run k-means on these features to separate objects into k classes. Each of these three steps becomes computationally intensive for large N and/or k-means.\n\n\n\nThe first step is to create a similarity graph with n n f (n n i) and a N f (n n i) to combine the first k eigenvectors and k eigenvectors.\nThis algorithm takes a K eigenvectors and a f (n i) together to find n f (n i) in n i. The first k eigenvectors have to be combined for k eigenvectors, and they must be combined for k eigenvectors.\nIn practice, such k eigenvectors can be used as well as to be more easily and quickly computed.\nThis is used in conjunction with the k eigenvectors, to generate a similarity graph with n eigenvectors, and to generate a similarity graph with n f (n i) and a k eigenvectors.\nThis simple algorithm takes a k eigenvectors and a k eigenvectors and a k eigenvectors. The second step is to combine the two k eigenvectors and a k eigenvectors.\nThis approach is used for the Likertas, also known as the Likertas, which can be used to create a similarity graph with n (n i) and a k eigenvectors.\nThe Likertas are often used as a model for hierarchical clustering in clusters. In this case, the Likertas are not all the same, and most are related to a subset of the k eigenvectors. However, some of the Likertas may have more distinct k eigenvectors in their respective clusters, which could be better served in some situations. The Likertas are usually found with clusters where only a small subset is involved in a cluster, which means that they can be used to generate a similarity graph. This is not necessary for clusters where a small subset is involved in a cluster, which means that they can be used to generate a similarity graph. In the case of a", "histories": [["v1", "Fri, 5 Feb 2016 13:42:27 GMT  (113kb,D)", "http://arxiv.org/abs/1602.02018v1", "15 pages, 2 figures"], ["v2", "Mon, 23 May 2016 13:21:56 GMT  (133kb,D)", "http://arxiv.org/abs/1602.02018v2", "12 pages, 2 figures"]], "COMMENTS": "15 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.DS cs.LG stat.ML", "authors": ["nicolas tremblay", "gilles puy", "r\u00e9mi gribonval", "pierre vandergheynst"], "accepted": true, "id": "1602.02018"}, "pdf": {"name": "1602.02018.pdf", "metadata": {"source": "CRF", "title": "Compressive spectral clustering", "authors": ["Nicolas Tremblay", "Gilles Puy", "R\u00e9mi Gribonval", "Pierre Vandergheynst"], "emails": [], "sections": [{"heading": null, "text": "its Laplacian matrix to define a feature vector for each object, and run k-means on these features to separate objects into k classes. Each of these three steps becomes computationally intensive for large N and/or k. We propose to speed up the last two steps based on recent results in the emerging field of graph signal processing: graph filtering of random signals, and random sampling of bandlimited graph signals. We prove that our method, with a gain in computation time that can reach several orders of magnitude, is in fact an approximation of spectral clustering, for which we are able to control the error. We test the performance of our method on artificial and real-world network data.\n1. Introduction\nSpectral clustering (SC) is a fundamental tool in data mining [1]. Given a set of N data points {x1, . . . ,xN}, the goal is to partition this set into k weakly inter-connected clusters. Several spectral clustering algorithms exist, e.g., [2\u20135], but all follow the same scheme. First, compute weights Wij > 0 that model the similarity between pairs of data points (xi,xj). This gives rise to a graph G with N nodes and adjacency matrix W = (Wij)16i,j6N \u2208 RN\u00d7N . Second, compute the first k eigenvectors Uk := (u1, . . . ,uk) \u2208 RN\u00d7k of the Laplacian matrix L \u2208 RN\u00d7N associated to G (see Sec. 2 for L\u2019s definition). And finally, run k-means using the rows of Uk as feature vectors to partition the N data points into k clusters. This k-way scheme is a generalisation of Fiedler\u2019s pioneering work [6].\nSC is mainly used in two contexts: 1) if the N data points show particular structures (e.g., concentric circles) for which naive k-means clustering fails; 2) if the input data is directly a graph G modeling a network [7], such as social, neuronal, or transportation networks. SC suffers nevertheless from three main computational bottlenecks for large N and/or k: the creation of the similarity matrix W; the partial eigendecomposition of the graph Laplacian matrix L; and k-means."}, {"heading": "1.1. Related work", "text": "Circumventing these bottlenecks has raised a significant interest in the past decade. Several authors have proposed ideas to tackle the eigendecomposition bottleneck, e.g., via the power method [8, 9], via a careful optimisation of diagonalisation algorithms in the context of SC [10], or via matrix column-subsampling such as in the Nystro\u0308m method [11], the nSPEC and cSPEC methods of [12], or in [13, 14]. All these methods aim to quickly compute feature vectors, but k-means is still applied on N feature vectors. Other authors have proposed to circumvent k-means in high dimension by subsampling a few data points out of the N available ones, applying SC on its reduced similarity graph, and interpolating the results back on the complete dataset. One can find such methods in [15] and [12]\u2019s eSPEC proposition, where two different interpolation methods are used. Both methods are heuristic: there is no proof that these methods approach the results of SC. Also, let us mention [16] that circumvents both the eigendecomposition and the k-means bottlenecks: the authors reduce the graph\u2019s size by successive aggregation of nodes, apply SC on this small graph, and propagate the results on the complete graph using kernel k-means to control interpolation errors. The kernel is computed so that kernel k-means and SC share the same objective function [17]. Finally, we mention\nThis work was partly funded by the European Research Council, PLEASE project (ERC-StG-2011-277906), and by the Swiss National Science Foundation, grant 200021-154350/1 - Towards Signal Processing on Graphs.\nN. Tremblay, G. Puy, R. Gribonval and P. Vandergheynst are with INRIA Rennes - Bretagne Atlantique, Campus de Beaulieu, FR-35042 Rennes Cedex, France. N. Tremblay and P. Vandergheynst are also with the Institute of Electrical Engineering, Ecole Polytechnique Fe\u0301de\u0301rale de Lausanne (EPFL), CH-1015 Lausanne, Switzerland.\n1\nar X\niv :1\n60 2.\n02 01\n8v 1\n[ cs\n.D S]\n5 F\neb 2\n01 6\nworks [18, 19] that concentrate on reducing the feature vectors\u2019 dimension in the k-means problem, but do not sidestep the eigendecomposition nor the large N issues."}, {"heading": "1.2. Contribution: compressive clustering", "text": "In this work, inspired by recent advances in the emerging field of graph signal processing [20, 21], we circumvent SC\u2019s last two bottlenecks and detail a fast approximate spectral clustering method for large datasets, as well as the supporting theory. We suppose that the Laplacian matrix L \u2208 RN\u00d7N of G is given. Our method is made of two ingredients.\nThe first ingredient builds upon recent works [22,23] that avoid the costly computation of the eigenvectors of L by filtering O(log(k)) random signals on G that will then serve as feature vectors to perform clustering. We show in this paper how to incorporate the effects of non-ideal, but computationally efficient, graph filters on the quality of the feature vectors used for clustering.\nThe second ingredient uses a recent sampling theory of bandlimited graph-signals [24] to reduce the computational complexity of k-means. Using the fact that the indicator vectors of each cluster are approximately bandlimited on G, we prove that clustering a random subset of O(k log(k)) nodes of G using random features vectors of size O(log(k)) is sufficient to infer rapidly and accurately the cluster label of all N nodes of the graph. Note that the complexity of k-means is reduced to O(k2 log2(k)) instead of O(Nk2) for SC. One readily sees that this method scales easily to large datasets, as will be demonstrated on artifical and real-world datasets containing up to N = 106 nodes.\nThe proposed compressive spectral clustering method can be summarised as follows:\n\u2022 generate a feature vector for each node by filtering O(log(k)) random Gaussian signals on G; \u2022 sample O(k log(k)) nodes from the full set of nodes; \u2022 cluster the reduced set of nodes; \u2022 interpolate the cluster indicator vectors back to the complete graph.\n2. Background"}, {"heading": "2.1. Graph signal processing", "text": "Let G = (V, E ,W) be an undirected weighted graph with V the set of N nodes, E the set of edges, and W the weighted adjacency matrix such that Wij = Wji > 0 is the weight of the edge between nodes i and j.\nThe graph Fourier matrix. Consider the graph\u2019s normalized Laplacian matrix L = I\u2212D\u22121/2WD\u22121/2 where I is the identity matrix in dimension N , and D is diagonal with Dii = \u2211 j 6=i Wij the strength of node i. L is real symmetric and positive semi-definite, therefore diagonalizable in an orthogonal basis. Its spectrum is composed of its set of sorted eigenvalues 0 = \u03bb1 6 . . . 6 \u03bbN 6 2 [25], and of the orthonormal matrix U := (u1|u2| . . . |uN ) containing its eigenvectors. We denote by \u039b \u2208 RN\u00d7N the diagonal matrix containing the eigenvalues of L. By analogy to the continuous Laplacian operator whose eigenfunctions are the classical Fourier modes and eigenvalues their squared frequencies, the columns of U are considered as the graph\u2019s Fourier modes, and { \u221a \u03bbl}l as its set of associated \u201cfrequencies\u201d [20]. Other types of graph Fourier matrices have been proposed, e.g., [26], but in order to exhibit the link between graph signal processing and SC (that partially diagonalizes the Laplacian matrix), the Laplacian-based Fourier matrix appears more natural.\nGraph filtering. The graph Fourier transform x\u0302 of a signal x defined on the nodes of the graph (called a graph signal) reads: x\u0302 = U\u1d40x. Given a continuous filter function h defined on [0, 2], its associated graph filter operator H \u2208 RN\u00d7N is defined as H := h(L) = Uh(\u039b)U\u1d40, where h(\u039b) := diag(h(\u03bb1), h(\u03bb2), \u00b7 \u00b7 \u00b7 , h(\u03bbN )). The signal x filtered by h is Hx. In the following, we consider ideal low-pass filters, denoted by h\u03bbc , that satisfy, for all \u03bb \u2208 [0, 2],\nh\u03bbc(\u03bb) = 1, if \u03bb 6 \u03bbc, and h\u03bbc(\u03bb) = 0, if not.(1)\nDenote by H\u03bbc the graph filter operator associated to h\u03bbc .\nAlgorithm 1 Spectral Clustering [2]\nInput: The Laplacian matrix L, the number of clusters k 1\u00b7 Compute Uk \u2208 RN\u00d7k, L\u2019s first k eigenvectors: Uk = (u1|u2| \u00b7 \u00b7 \u00b7 |uk). 2\u00b7 Form the matrix Yk \u2208 RN\u00d7k from Uk by normalizing each of Uk\u2019s rows to unit length: (Yk)ij = (Uk)ij / \u221a\u2211k j=1 U 2 ij . 3\u00b7 Treat each node i as a point in Rk by defining its feature vector fi \u2208 Rk as the transposed i-th row of Yk:\nfi := Y \u1d40 k\u03b4i,\nwhere \u03b4i(j) = 1 if j = i and 0 otherwise. 4\u00b7 To obtain k clusters, run k-means with the Euclidean distance: (2) Dij := \u2016fi \u2212 fj\u2016\nFast graph filtering. In order to filter a signal by h without diagonalizing L, one may approximate h by a polynomial h\u0303 of order p satisfying\nh\u0303(\u03bb) := p\u2211 l=0 \u03b1l\u03bb l ' h(\u03bb)\nfor all \u03bb \u2208 [0, 2], where \u03b11, . . . , \u03b1p \u2208 R. In matrix form, we have\nH\u0303 := h\u0303(L) = p\u2211 l=0 \u03b1lL l ' H.\nLet us highlight that we never compute the potentially dense matrix H\u0303 in practice. Indeed, we will only be interested in the result of the filtering operation, i.e., H\u0303x = \u2211p l=0 \u03b1lL\nlx \u2248 Hx for x \u2208 RN , that can be obtained with only p successive matrix-vector multiplications with L. The computational complexity of filtering a signal is thus O(p#E), where #E is the number of edges of G."}, {"heading": "2.2. Spectral clustering", "text": "We choose here Ng et al.\u2019s method [2] based on the normalized Laplacian as our standard SC method. The input is the adjacency matrix W representing the pairwise similarity of all the N objects to cluster1. After computing its Laplacian L, follow Alg. 1 to find k classes.\n3. Principles of CSC\nCompressive spectral clustering (CSC) circumvents two of SC\u2019s bottlenecks, the partial diagonalisation of the Laplacian and the high-dimensional k-means, thanks to the following ideas.\n1) Perform a controlled estimation D\u0303ij of the spectral clustering distance Dij (see Eq (2)), without partially diagonalizing the Laplacian, by fast filtering a few random signals with the polynomial approximation h\u0303\u03bbk of the ideal low pass filter h\u03bbk (see Eq. (1)). A theorem recently published independently by two teams [22, 23] shows that this is possible when there is no normalisation step (step 2 in Alg. 1) and when the order p of the polynomial approximation tends to infinity, i.e., when\nh\u0303\u03bbk = h\u03bbk . In Sec. 3.1, we provide a first extension of this theorem that takes into account normalisation. A complete extension that also takes into account the errors due to a finite-order polynomial approximation will be presented in Sec. 4.2.\n2) Run k-means on n randomly selected feature vectors out of the N available ones - thus clustering the corresponding n nodes into k groups - and interpolate the result back on the full graph. To guarantee robust reconstruction, we take advantage of recent results of [24] on random sampling of\n1In network analysis, the raw data is directly W. In the case where one starts with a set of data points (x1, . . . ,xN ), the first step consists in deriving W from the pairwise similarities s(xi,xj). See [27] for several choices of similarity measure s and several ways to create W from the s(xi,xj).\nk-bandlimited graph signals. In Sec. 3.2, we explain why these results are applicable to clustering and show that it is sufficient to sample O(k log k) features only! Note that to cluster a dataset into k groups, one needs at least k samples. The above bound is thus optimal up to the extra log k factor."}, {"heading": "3.1. Ideal filtering of random signals", "text": "Definition 3.1 (Local cumulative coherence). Given a graph G, the local cumulative coherence of order k at node i is2 vk(i) := \u2016U\u1d40k\u03b4i\u2016 = \u221a\u2211k j=1 U 2 ij.\nLet us define the diagonal matrix: Vk(i, i) = 1/vk(i). Note that we assume that vk(i) > 0. Indeed, in the pathologic cases where vk(i) = 0 for some nodes i, Step 2 of the standard SC algorithm cannot be run either. Now, consider the matrix R = (r1|r2| \u00b7 \u00b7 \u00b7 |rd) \u2208 RN\u00d7d consisting of d random signals ri, whose components are independent Bernouilli, Gaussian, or sparse (as in Theorem 1.1 of [28]) random variables. To fix ideas in the following, we consider the components as independent random Gaussian variables of mean zero and variance 1/d. Consider the coherence-normalized filtered version of R, VkH\u03bbkR \u2208 RN\u00d7d, and define node i\u2019s new feature vector f\u0303i \u2208 Rd as the transposed i-th line of this filtered matrix:\nf\u0303i := (VkH\u03bbkR) \u1d40\u03b4i.\nThe following theorem shows that, for large enough d, D\u0303ij := \u2225\u2225\u2225f\u0303i \u2212 f\u0303j\u2225\u2225\u2225 = \u2016(VkH\u03bbkR)\u1d40(\u03b4i \u2212 \u03b4j)\u2016\nis a good estimation of Dij with high probability.\nTheorem 3.2. Let \u2208]0, 1] and \u03b2 > 0 be given. If d is larger than 4 + 2\u03b2\n2/2\u2212 3/3 logN,\nthen with probability at least 1\u2212N\u2212\u03b2, we have (1\u2212 )Dij 6 D\u0303ij 6 (1 + )Dij .\nfor all (i, j) \u2208 {1, . . . , N}2.\nProof. The proof is an instance of the Johnson-Lindenstrauss lemma. Note that H\u03bbk = UkU \u1d40 k, and that Yk = VkUk. We rewrite \u2225\u2225\u2225f\u0303i \u2212 f\u0303j\u2225\u2225\u2225 in a form that will let us apply the Johnson-Lindenstrauss\nlemma of norm conservation: (3) \u2225\u2225\u2225f\u0303i \u2212 f\u0303j\u2225\u2225\u2225 = \u2225\u2225R\u1d40H\u1d40\u03bbkV\u1d40k(\u03b4i \u2212 \u03b4j)\u2225\u2225 = \u2016R\u1d40UkU\u1d40kV\u1d40k(\u03b4i \u2212 \u03b4j)\u2016 = \u2016R\u1d40Uk(fi \u2212 fj)\u2016\nwhere the fi are the standard SC feature vectors. Applying Theorem 1.1 of [28] (an instance of the Johnson-Lindenstrauss lemma) to \u2016R\u1d40Uk(fi \u2212 fj)\u2016, the following holds. If d is larger than:\n(4) 4 + 2\u03b2\n2/2\u2212 3/3 logN,\nthen with probability at least 1\u2212N\u2212\u03b2 , we have, \u2200(i, j) \u2208 {1, . . . , N}2:\n(1\u2212 ) \u2016Uk(fi \u2212 fj)\u2016 6 D\u0303ij 6 (1 + ) \u2016Uk(fi \u2212 fj)\u2016 . As the columns of Uk are orthonormal, we end the proof:\n\u2200(i, j) \u2208 [1, N ]2 \u2016Uk(fi \u2212 fj)\u2016 = \u2016fi \u2212 fj\u2016 = Dij .\nIn Sec. 4.2, we generalize this result to the real-world case where the low-pass filter is approximated by a finite order polynomial; we also prove that, as announced in the introduction, one only needs d = O(log k) features when using the downsampling scheme that we now detail.\n2Throughout this paper, \u2016.\u2016 stands for the usual `2-norm."}, {"heading": "3.2. Downsampling and interpolation", "text": "For j = 1, . . . , k, let us denote by cj \u2208 RN the ground-truth indicator vector of cluster Cj , i.e.,\n(cj)i := { 1 if i \u2208 Cj , 0 otherwise,\n\u2200i \u2208 {1, . . . , N}.\nTo estimate cj , one could run k-means on the N feature vectors {f\u03031, . . . , f\u0303N} , as done in [22, 23]. Yet, this is still inefficient for large N . To reduce the computational cost further, we propose to run k-means on a small subset of n feature vectors only. The goal is then to infer the labels of all N nodes from the labels of the n sampled nodes. To this end, we need 1) a low-dimensional model that captures the regularity of the vectors cj , 2) to make sure that enough information is preserved after sampling to be able to recover the vectors cj , and 3) an algorithm that rapidly and accurately estimates the vectors cj by exploiting their regularity."}, {"heading": "3.2.1. The low-dimensional model", "text": "For a simple regular (with nodes of same degree) graph of k disconnected clusters, it is easy to check that {c1, . . . , ck} form a set of orthogonal eigenvectors of L with eigenvalue 0. All indicator vectors cj therefore live in span(Uk). For general graphs, we assume that the indicator vectors cj live close to span(Uk), i.e., the difference between any cj and its orthogonal projection onto span(Uk) is small. Experiments in Section 5 will confirm that it is a good enough model to recover the cluster indicator vectors.\nIn graph signal processing words, one can say that cj is approximately k-bandlimited, i.e., its k first graph Fourier coefficients bear most of its energy. There has been recently a surge of interest around adapting classical sampling theorems to such bandlimited graph signals [29\u201332]. We rely here on the random sampling strategy proposed in [24] to select a subset of n nodes."}, {"heading": "3.2.2. Sampling and interpolation", "text": "The subset of feature vectors is selected by drawing n indices \u2126 := {\u03c91, . . . , \u03c9n} uniformly at random from {1, . . . , N} without replacement. Running k-means on the subset of features {f\u0303\u03c91 , . . . , f\u0303\u03c9n} thus yields a clustering of the n sampled nodes into k clusters. We denote by crj \u2208 Rn the resulting low-dimensional indicator vectors. Our goal is now to recover cj from c r j .\nConsider that k-means is able to correctly identify c1, . . . , ck \u2208 RN using the original set of features {f1, . . . ,fN} with the SC algorithm (otherwise, CSC is doomed to fail from the start). Results in [22,23] show that k-means is also able to identify the clusters using the feature vectors {f\u03031, . . . , f\u0303N}. This is explained theoretically by the fact that the distance between all pairs of feature vectors is preserved (see Theorem 3.2). Then, as choosing a subset {f\u0303\u03c91 , . . . , f\u0303\u03c9n} of {f\u03031, . . . , f\u0303N} does not change the distance between the feature vectors, we can hope that k-means correctly clusters the n sampled nodes, provided that each cluster is sufficiently sampled. Experiments in Sec. 5 will confirm this intuition. In this ideal situation, we have\n(5) crj = M cj ,\nwhere M \u2208 Rn\u00d7N is the sampling matrix satisfying:\nMij := { 1 if j = \u03c9i, 0 otherwise. (6)\nTo recover cj from its n observations c r j , Puy et al. [24] show that the solution to the optimisation problem\nmin x\u2208RN \u2225\u2225Mx\u2212 crj\u2225\u222522 + \u03b3 x\u1d40g(L)x,(7) is a faithful 3 estimation of cj , provided that cj is close to span(Uk) and that M satisfies the restricted isometry property (discussed in the next subsection). In (7), \u03b3 > 0 is a regularisation parameter\n3precise error bounds are provided in [24].\nand g a positive non-decreasing polynomial function (see Section 2.1 for the definition of g(L)). This reconstruction scheme is proved to be robust to: 1) observation noise, i.e., to imperfect clustering of the n nodes in our context; 2) model errors, i.e., the indicator vectors do not need to be exactly in span(Uk) for the method to work. Also, the performance is shown to depend on the ratio g(\u03bbk)/g(\u03bbk+1). The smaller it is, the better the reconstruction. To decrease this ratio, we decide to approximate the ideal high-pass filter g\u03bbk(\u03bb) = 1\u2212h\u03bbk(\u03bb) for the reconstruction. Remark that this filter favors the recovery of signals living in span(Uk). The approximation g\u0303\u03bbk of g\u03bbk is obtained using a polynomial (as in Sec. 2.1), which permits us to find fast algorithms to solve (7)."}, {"heading": "3.2.3. How many features to sample?", "text": "We terminate this section by providing the theoretical number of features n one needs to sample in order to make sure that the indicator vectors can be faithfully recovered. This number is driven by the following quantity.\nDefinition 3.3 (Global cumulative coherence). The global cumulative coherence of order k of the graph G is \u03bdk := \u221a N \u00b7 max16i6N {vk(i)} .\nIt is shown in [24] that \u03bdk \u2208 [k1/2, N1/2].\nTheorem 3.4 ( [24]). Let M be a random sampling matrix constructed as in (6). For any \u03b4, \u2208 ]0, 1[,\n(1\u2212 \u03b4) \u2016x\u201622 6 N\nn \u2016Mx\u201622 6 (1 + \u03b4) \u2016x\u2016 2 2(8)\nfor all x \u2208 span(Uk) with probability at least 1\u2212 provided that\nn > 6\n\u03b42 \u03bd2k log\n( k ) .\nThe above theorem presents a sufficient condition on n ensuring that M satisfies the restricted isometry property (8). This condition is required to ensure that the solution of (7) is an accurate estimation of cj . The above theorem thus indicates that sampling O(\u03bd 2 k log k) features is sufficient to recover the cluster indicator vectors. For a simple regular graph G made of k disconnected clusters, we have seen that Uk = (c1, . . . , ck) up to a normalisation of the vectors. Therefore, \u03bdk = N 1/2/mini{N1/2i }, where Ni is the size of the ith cluster. If the clusters have the same size Ni = N/k then \u03bdk = k 1/2, the lower bound on \u03bdk. In this simple optimal scenario, sampling O(\u03bd2k log k) = O(k log k) features is thus sufficient to recover the cluster indicator vectors.\nThe attentive reader will have noticed that for graphs where \u03bd2k \u2248 N , no downsampling is possible. Yet, a simple solution exists in this situation: variable density sampling. Indeed, it is proved in [24] that, whatever the graph G, there always exists an optimal sampling distribution such that n = O(k log k) samples are sufficient to satisfy Eq. (8). This distribution depends on the profile of the local cumulative coherence and can be estimated rapidly (see [24] for more details). In this paper, we only consider uniform sampling to simplify the explanations, but keep in mind that in practice results will always be improved if one uses variable density sampling. Note also that one cannot expect to sample less than k nodes to find k clusters. Up to the extra log(k), our result is optimal.\n4. CSC in practice\nWe have detailed the two fundamental theoretical notions supporting our algorithm, presented in Alg. 2 and discussed in Sec. 4.1. However, some steps in Alg. 2 still need to be clarified. In particular, Sec. 4.2 provides an extension of Theorem 3.2 that takes into account the use of a non-ideal low-pass filter (to handle the practical case where the order of the polynomial approximation is finite). This theorem in fine explains and justifies Step 4 of Alg. 2. Then, in Sec. 4.3, we discuss important details of the method such as the estimation of \u03bbk (Step 1) and the choice of the polynomial approximation (Step 2). We finish this section with complexity considerations.\nAlgorithm 2 Compressive Spectral Clustering\nInput: The Laplacian matrix L, the number of clusters k; and parameters typically set to n = 2k log k, d = 4 log n, p = 50 and \u03b3 = 10\u22123. 1\u00b7 Estimate L\u2019s k-th eigenvalue \u03bbk as in Sec. 4.3.\n2\u00b7 Compute the polynomial approximation h\u0303\u03bbk of order p of the ideal low-pass filter h\u03bbk . 3\u00b7 Generate d random Gaussian signals of mean 0 and variance 1/d: R = (r1|r2| \u00b7 \u00b7 \u00b7 |rd) \u2208 RN\u00d7d. 4\u00b7 Filter R with H\u0303\u03bbk = h\u0303\u03bbk(L) as in Sec. 2.1 and define, for each node i, its feature vector f\u0303i \u2208 Rd:\nf\u0303i = [( H\u0303\u03bbkR )\u1d40 \u03b4i ]/\u2225\u2225\u2225(H\u0303\u03bbkR)\u1d40 \u03b4i\u2225\u2225\u2225. 5\u00b7 Generate a random sampling matrix M \u2208 Rn\u00d7N as in Eq. (6) and keep only n feature vectors: (f\u0303\u03c91 | . . . |f\u0303\u03c9n)\u1d40 = M(f\u03031| . . . |f\u0303N )\u1d40. 6\u00b7 Run k-means on the reduced dataset with the Euclidean distance: D\u0303rij =\n\u2225\u2225\u2225f\u0303\u03c9i \u2212 f\u0303\u03c9j\u2225\u2225\u2225 to obtain k reduced indicator vectors crj \u2208 Rn, one for each cluster. 7\u00b7 Interpolate each reduced indicator vector crj with the optimisation problem of Eq. (7), to obtain the k indicator vectors c\u0303j \u2217 \u2208 RN on the full set of nodes."}, {"heading": "4.1. Algorithm", "text": "As for SC (see Sec. 2.2), the algorithm starts with the adjacency matrix W of a graph G. After computing its Laplacian L, the CSC algorithm is summarized in Alg. 2. The output c\u0303\u2217j (i) is not binary and in fact quantifies how much node i belongs to cluster j, useful for fuzzy partitioning. To obtain an exact partition of the nodes, we normalize each indicator vector c\u0303\u2217j , and assign node i to the\ncluster j for which c\u0303\u2217j (i)/ \u2225\u2225c\u0303\u2217j\u2225\u2225 is maximal. This is the procedure that we follow in the experiments of Sec. 5."}, {"heading": "4.2. Non-ideal filtering of random signals", "text": "In this section, we improve Theorem 3.2 by studying how the error of the polynomial approximation h\u0303\u03bbk of h\u03bbk propagates to the spectral distance estimation, and by taking into account the fact that k-means is performed on the reduced set of features (f\u0303\u03c91 | . . . |f\u0303\u03c9n)\u1d40 = M(f\u03031| . . . |f\u0303N )\u1d40. We denote by MYk \u2208 Rn\u00d7k the ideal reduced feature matrix. We have (f\u03c91 | \u00b7 \u00b7 \u00b7 |f\u03c9n)\u1d40 = M(f1| \u00b7 \u00b7 \u00b7 |fN )\u1d40 = MYk. The actual distances we want to estimate using random signals are thus, for all (i, j) \u2208 {1, . . . , n}2\nDrij := \u2225\u2225f\u03c9i \u2212 f\u03c9j\u2225\u2225 = \u2225\u2225Y\u1d40kM\u1d40(\u03b4ri \u2212 \u03b4rj )\u2225\u2225 ,\nwhere the {\u03b4ri } are here Diracs in n dimensions. Consider the random matrix R = (r1|r2| \u00b7 \u00b7 \u00b7 |rd) \u2208 RN\u00d7d constructed as in Sec. 3.1. Its filtered, normalized and reduced version is MVkH\u0303\u03bbkR \u2208 Rn\u00d7d. The new feature vector f\u0303\u03c9i \u2208 Rd associated to node \u03c9i is thus\nf\u0303\u03c9i = (MVkH\u0303\u03bbkR) \u1d40\u03b4ri .\nThe normalisation of Step 4 of Alg. 2 approximates fastly the action of Vk in the above equation. More details and justifications are provided in the \u201cImportant remark\u201d at the end of this section. The Euclidean distance between any two features reads as\nD\u0303rij := \u2225\u2225\u2225f\u0303\u03c9i \u2212 f\u0303\u03c9j\u2225\u2225\u2225 = \u2225\u2225\u2225R\u1d40H\u0303\u1d40\u03bbkV\u1d40kM\u1d40(\u03b4ri \u2212 \u03b4rj )\u2225\u2225\u2225 .\nWe now study how well D\u0303rij estimates D r ij .\nApproximation error. Denote e(\u03bb) the approximation error of the ideal low-pass filter:\n\u2200\u03bb \u2208 [0, 2], e(\u03bb) := h\u0303\u03bbk(\u03bb)\u2212 h\u03bbk(\u03bb).\nIn the form of graph filter operators, one has\nh\u0303\u03bbk(L) = H\u0303\u03bbk = H\u03bbk + E = h\u03bbk(L) + e(L).\nWe model the error e using two parameters: e1 (resp. e2) the maximal error for \u03bb 6 \u03bbk (resp. \u03bb > \u03bbk). We have\ne1 := sup \u03bb\u2208{\u03bb1,...,\u03bbk} |e(\u03bb)|, e2 := sup \u03bb\u2208{\u03bbk+1,...,\u03bbN} |e(\u03bb)|.\nThe resolution parameter. In some cases, the ideal reduced spectral distance Drij may be null. In such cases, approximating Drij = 0 using a non-ideal filter is not possible. In fact, non-ideal filtering introduces an irreducible error on the estimation of the feature vectors that is not possible to compensate in general. We thus introduce a resolution parameter Drmin below which the distances Drij do not need to be approximated exactly, but should remain below D r min (up to a tolerated error). Theorem 4.1 (General norm conservation theorem). Let Drmin \u2208 ] 0, \u221a 2 ]\nbe a chosen resolution parameter. For any \u03b4 \u2208 ]0, 1], \u03b2 > 0, if d is larger than\n16(2 + \u03b2) \u03b42 \u2212 \u03b43/3 log n,\nthen, for all (i, j) \u2208 {1, . . . , n}2,\n(1\u2212 \u03b4)Drij 6 D\u0303rij 6 (1 + \u03b4)Drij , if Drij > Drmin, and\nD\u0303rij < (1 + \u03b4)D r min, if D r ij < D r min,\nwith probability at least 1\u2212 2n\u2212\u03b2 provided that\u221a |e21 \u2212 e22| + \u221a 2 e2\nDrmin mini{vk(i)} 6\n\u03b4\n2 + \u03b4 .(9)\nProof. The proof follows the general ideas of Theorem 3.2\u2019s proof, with a more careful use of the Johnson-Lindenstrauss lemma that takes into account error propagation. Recall that:\nD\u0303rij := \u2225\u2225\u2225f\u0303\u03c9i \u2212 f\u0303\u03c9j\u2225\u2225\u2225 = \u2225\u2225\u2225R\u1d40H\u0303\u1d40\u03bbkV\u1d40kM\u1d40\u03b4rij\u2225\u2225\u2225 ,\nwhere \u03b4rij = \u03b4 r i \u2212 \u03b4rj . Given that H\u0303\u03bbk = H\u03bbk + E and using the triangle inequality in the definition of D\u0303rij , we obtain\u2225\u2225R\u1d40H\u1d40\u03bbkV\u1d40kM\u1d40\u03b4rij\u2225\u2225 \u2212 \u2225\u2225R\u1d40E\u1d40V\u1d40kM\u1d40\u03b4rij\u2225\u2225 6 D\u0303rij 6 \u2225\u2225R\u1d40E\u1d40V\u1d40kM\u1d40\u03b4rij\u2225\u2225 + \u2225\u2225R\u1d40H\u1d40\u03bbkV\u1d40kM\u1d40\u03b4rij\u2225\u2225 , We continue the proof by bounding\n\u2225\u2225R\u1d40H\u1d40\u03bbkV\u1d40kM\u1d40 \u03b4rij\u2225\u2225 and \u2225\u2225R\u1d40E\u1d40V\u1d40kM\u1d40 \u03b4rij\u2225\u2225 separately. Let \u03b4 \u2208]0, 1]. To bound\n\u2225\u2225R\u1d40H\u1d40\u03bbkV\u1d40kM\u1d40\u03b4rij\u2225\u2225, we set = \u03b4/2 in Theorem 3.2. This proves that if d is larger than\nd0 = 16(2 + \u03b2)\n\u03b42 \u2212 \u03b43/3 log n,\nthen with probability at least 1\u2212 n\u2212\u03b2 ,( 1\u2212 \u03b4\n2\n) Drij 6 \u2225\u2225R\u1d40H\u1d40\u03bbkV\u1d40kM\u1d40\u03b4rij\u2225\u2225 6 (1 + \u03b42 ) Drij ,\nfor all (i, j) \u2208 {1, . . . , n}2. To bound \u2225\u2225R\u1d40E\u1d40V\u1d40kM\u1d40\u03b4rij\u2225\u2225, we use Theorem 1.1 in [28]. This theorem proves that if d > d0, then with probability at least 1\u2212 n\u2212\u03b2 ,\u2225\u2225R\u1d40E\u1d40V\u1d40kM\u1d40\u03b4rij\u2225\u2225 6 (1 + \u03b42 )\u2225\u2225E\u1d40V\u1d40kM\u1d40\u03b4rij\u2225\u2225 ,\nfor all (i, j) \u2208 {1, . . . , n}2. Using the union bound and (10), we deduce that, with probability at least 1\u2212 2n\u2212\u03b2 ,(\n1\u2212 \u03b4 2\n) Drij \u2212 ( 1 + \u03b4\n2 )\u2225\u2225E\u1d40V\u1d40kM\u1d40\u03b4rij\u2225\u2225 6 D\u0303rij 6 (1 + \u03b42 )\u2225\u2225E\u1d40V\u1d40kM\u1d40\u03b4rij\u2225\u2225 + (1 + \u03b42 ) Drij ,\nfor all (i, j) \u2208 {1, . . . , n}2 provided that d > d0.\nThen, as e is bounded by e1 on the first k eigenvalues of the spectrum and by e2 on the remaining ones, we have\u2225\u2225E\u1d40V\u1d40kM\u1d40 \u03b4rij\u2225\u22252 = \u2225\u2225Ue(\u039b)U\u1d40V\u1d40kM\u1d40 \u03b4rij\u2225\u22252 = \u2225\u2225e(\u039b)U\u1d40V\u1d40kM\u1d40 \u03b4rij\u2225\u22252\n= N\u2211 l=1 e(\u03bbl) 2 \u2223\u2223(MVkul)\u1d40\u03b4rij\u2223\u22232\n6 e21 k\u2211 l=1 \u2223\u2223(MVkul)\u1d40\u03b4rij\u2223\u22232 + e22 N\u2211 l=k+1 \u2223\u2223(MVkul)\u1d40\u03b4rij\u2223\u22232 = e21\n\u2225\u2225U\u1d40kV\u1d40kM\u1d40\u03b4rij\u2225\u22252 + e22 (\u2225\u2225U\u1d40V\u1d40kM\u1d40\u03b4rij\u2225\u22252 \u2212 \u2225\u2225U\u1d40kV\u1d40kM\u1d40\u03b4rij\u2225\u22252) = (e21 \u2212 e22)\n\u2225\u2225U\u1d40kV\u1d40kM\u1d40\u03b4rij\u2225\u22252 + e22 \u2225\u2225U\u1d40V\u1d40kM\u1d40\u03b4rij\u2225\u22252 = (e21 \u2212 e22) (Drij)2 + e22\n\u2225\u2225V\u1d40kM\u1d40\u03b4rij\u2225\u22252 6 (e21 \u2212 e22) (Drij)2 +\n2 e22 mini{vk(i)2} .\nThe last step follows from the fact that\u2225\u2225V\u1d40kM\u1d40 \u03b4rij\u2225\u22252 = N\u2211 l=1 1 vk(l)2 \u2223\u2223(M\u1d40\u03b4rij)(l)\u2223\u22232 = 1vk(\u03c9i)2 + 1vk(\u03c9j)2 6 2mini{vk(i)}2\nDefine, for all (i, j) \u2208 {1, . . . , n}2: eij := \u221a |e21 \u2212 e22|Drij +\n\u221a 2e2\nmini{vk(i)} .\nThus, the above inequality may be rewritten as:\u2225\u2225E\u1d40V\u1d40kM\u1d40 \u03b4rij\u2225\u2225 6 eij , for all (i, j) \u2208 {1, . . . , n}2, which combined with (10) yields(\n1\u2212 \u03b4 2\n) Drij \u2212 ( 1 + \u03b4\n2\n) eij 6 D\u0303 r ij 6 ( 1 + \u03b4\n2\n) eij + ( 1 + \u03b4\n2\n) Drij ,\nfor all (i, j) \u2208 {1, . . . , n}2, with probability at least 1\u2212 2n\u2212\u03b2 provided that d > d0. Let us now separate two cases. In the case where Drij > D r min > 0, we have\neij = eij Drij Drij =\n(\u221a |e21 \u2212 e22|+\n\u221a 2e2\nDrij mini{vk(i)}\n) Drij\n6 (\u221a |e21 \u2212 e22|+\n\u221a 2e2\nDrmin mini{vk(i)}\n) Drij\n6 \u03b4\n2 + \u03b4 Drij .\nprovided that Eq. (9) holds. Combining the last inequality with (10) proves the first part of the theorem.\nIn the case where Drij < D r min, we have eij < \u221a |e21 \u2212 e22|Drmin +\n\u221a 2 e2\nmini{vk(i)} 6\n\u03b4\n2 + \u03b4 Drmin.\nprovided that Eq. (9) holds. Combining the last inequality with (10) terminates the proof.\nConsequence of Theorem 4.1. All distances smaller (resp. larger) than the chosen resolution parameter Drmin are estimated smaller than (1 + \u03b4)D r min (resp. correctly estimated up to a relative error \u03b4). Moreover, for a fixed distance estimation error \u03b4, the lower we decide to fix Drmin, the lower\nshould also be the errors e1 and/or e2 to ensure that Eq. (9) still holds, which implies an increase of the order p of the polynomial approximation of the ideal filter h\u03bbk , and ultimately, that means a higher computation time for the filtering operation of the random signals.\nImportant remark. The feature matrix VkH\u0303\u03bbkR can be easily computed if one knows the cut-off value \u03bbk and the local cumulative coherences vk(i). Unfortunately, this is not the case in practice. We propose a solution to estimate \u03bbk in Sec. 4.3. To estimate vk(i), one can use the results in Sec. 4 of [24] showing that vk(i) = \u2016U\u1d40k\u03b4i\u2016 \u2248 \u2016(H\u03bbkR)\u1d40\u03b4i\u2016. Thus, a practical way to estimate VkH\u0303\u03bbkR is to first compute H\u0303\u03bbkR and then normalize its rows to unit length, as done in Step 4 of Alg. 2."}, {"heading": "4.3. Polynomial approximation and estimation of \u03bbk", "text": "The polynomial approximation. Theorem 4.1 uses a separate control on e(\u03bb) below \u03bbk (with e1) and above \u03bbk (with e2). To have such a control in practice, one would need to use rational filters (ratio of two polynomials) to approximate h\u03bbk . Such filters have been introduced in the graph context [33], but they involve another optimisation step that would burden our main message. We prefer to simplify our analysis by using polynomials for which only the maximal error can be controlled. We write\n(10) em := max(e1, e2) = sup \u03bb\u2208{\u03bb1,...,\u03bbN}\n|e(\u03bb)| .\nIn this easier case, one can show that Theorem 4.1 is still valid if Eq. (9) is replaced by\n(11)\n\u221a 2 em\nDrmin mini{vk(i)} 6\n\u03b4\n2 + \u03b4 .\nIn our experiments, we could follow [34] and use truncated Chebychev polynomials to approximate the ideal filter, as these polynomials are known to require a small degree to ensure a given tolerated maximal error em. We prefer to follow [35] who suggest to use Jackson-Chebychev polynomials: Chebychev polynomials to which are added damping multipliers to alleviate the unwanted Gibbs oscillations around the cut-off frequency \u03bbk.\nThe polynomial\u2019s order p. For a fixed \u03b4, Drmin, and mini{vk(i)}, one should use the JacksonChebychev polynomial of smallest order p\u2217 ensuring that em satisfies Eq. (11), in order to optimize the computation time while making sure that Theorem 4.1 applies. Studying p\u2217 theoretically without computing the Laplacian\u2019s complete spectrum (see Eq. (10)) is beyond the scope of this paper. Experimentally, p = 50 yields good results (see Fig. 1c).\nEstimation of \u03bbk. The fast filtering step is based on the polynomial approximation of h\u03bbk , which is itself parametrized by \u03bbk. Unless we compute the first k eigenvectors of L, thereby partly loosing our efficiency edge on other methods, we cannot know the value of \u03bbk with infinite precision. To estimate it efficiently, we use Alg. 1 in [24], where an estimation of \u03bbk is obtained as a by-product. This estimation uses the fact that Tr(R\u1d40H\u1d40\u03bbkH\u03bbkR) = Tr(R \u1d40UkU \u1d40 kR) \u2248 k with high probability. Lemma 5.3 of [36] shows that this eigencount estimator yields the exact result (i.e., k) with probability 1\u2212 \u03b4 provided that d > 24k log(2/\u03b4). In practice, experiments in [35] show that fewer random signals are sufficient in some cases. Inspired by these results and as in [24], we choose 2 logN random signals for this estimation task. Starting from \u03bb = 2 and using the above estimator, the algorithm proceeds by dichotomy on \u03bb until the interval [0, \u03bb] contains k eigenvalues. For computational efficiency, we perform this eigencount task with Jackson-Chebychev polynomial approximation of the ideal low-pass filters."}, {"heading": "4.4. Complexity considerations", "text": "The complexity of steps 2, 3 and 5 of Alg. 2 are not detailed as they are insignificant compared to the others. First, note that fast filtering a graph signal costs O(p#E).4 Therefore, Step 1 costs O(p#E logN) per iteration of the dichotomy, and Step 4 costs O(p#E log n) (as d = O(log n)). Step 7 requires to solve Eq. (7) with the polynomial approximation of g\u03bbk(\u03bb) = 1 \u2212 h\u03bbk(\u03bb). When solved, e.g., by conjugate gradient or gradient descent, this step costs a fast filtering operation per\n4Recall that p is the order of the polynomial filter.\niteration of the solver and for each of the k classes. Step 7 thus costs O(p#Ek). Also, the complexity of k-means to cluster Q feature vectors of dimension r into k classes is O(kQr) per iteration. Therefore, Step 6 with Q = n and r = d = O(log(n)) costs O(kn log n). CSC\u2019s complexity is thus O (kn log n+ p#E (logN + log n+ k)) . In practice, we are interested in sparse graphs: #E = O(N). Using the fact that n = O(k log k), CSC\u2019s complexity simplifies to\nO ( k2 log2 k + pN (logN + k) ) .\nSC\u2019s k-means step has a complexity of O(Nk2) per iteration. In many cases5 this sole task is more expensive than the CSC algorithm. On top of this, SC has the additional complexity of computing the first k eigenvectors of L, for which the cost of ARPACK - a popular eigenvalue solver - is at least O(k3) (see details in Sec. 3.2 of [37]). In general, the complexity is much larger with additional terms growing linearly with N .\nThis study suggests that CSC is faster than SC for large N and/or k. The above algorithms\u2019 number of iterations are not taken into account as they are difficult to predict theoretically. Yet, the following experiments confirm the superiority of CSC over SC in terms of computational time.\n5. Experiments\nWe first perform well-controlled experiments on the Stochastic Block Model (SBM), a model of random graphs with community structures, that was showed suitable as a benchmark for SC in [38]. We also show performance results on a large real-world network. Implementation was done in Matlab R2015a, using the built-in function kmeans with 20 replicates, and the function eigs for SC. Experiments were done on a laptop with a 2.60 GHz Intel i7 dual-core processor running OS Fedora release 22 with 16 GB of RAM. The fast filtering part of CSC uses the gsp cheby op function of the GSP toolbox [39]. Equation (7) is solved using Matlab\u2019s gmres function. All our results may be reproduced with the toolbox provided here [40]."}, {"heading": "5.1. The Stochastic Block Model", "text": "What distinguishes the SBM from Erdos-Renyi graphs is that the probability of connection between two nodes i and j is not uniform, but depends on the community label of i and j. More precisely, the probability of connection between nodes i and j equals q1 if they are in the same community, and q2 if not. In a first approach, we look at graphs with k communities, all of same size N/k. Furthermore, instead of considering the probabilities, one may fully characterize a SBM by providing their ratio = q2q1 , as well as the average degree s of the graph. The larger , the more difficult the community structure\u2019s detection. In fact, Decelle et al. [41] show that a critical value c exists above which community detection is impossible at the large N limit: c = (s\u2212 \u221a s)/(s+ \u221a s(k \u2212 1))."}, {"heading": "5.2. Performance results", "text": "In Figs. 1 a-d), we compare the recovery performance of CSC versus SC for different parameters. The performance is measured by the Adjusted Rand similarity index [42] between the SBM\u2019s ground truth and the obtained partitions. It varies between \u22121 and 1. The higher it is, the better is the reconstruction. These figures show that the performance of CSC saturates at the default values of n, d, p and \u03b3 (see top of Alg. 2).\nWe also perform experiments on a SBM with N = 103, k = 20, s = 16 and hetereogeneous community sizes. More specifically, the list of community sizes is chosen to be: 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 50, 55, 60, 65, 70, 75, 80, 85, 90 and 95 nodes. In this scenario, there is no theoretical value of over which it is proven that recovery is impossible in the large N limit. Instead, we vary between 0 and 0.2 and show the recovery performance results with respect to n, d, p and \u03b3 in Fig. 2. Results are similar to the homogeneous case presented in Figs. 1 a-d).\nWe show in Fig. 1 e) the estimation results of \u03bbk for different values of . We see that it is overestimated in the SBM context. As long as the estimated value stays under \u03bbk+1, this overestimation\n5Roughly, all cases for which k2 > p(logN + k).\ndoes not have a strong impact on the method. On the other hand, as becomes larger than \u223c 0.06, our estimation of \u03bbk is larger than \u03bbk+1, which means that our feature vectors start to integrate some unwanted information from eigenvectors outside of span(Uk). Even though the impact of this additional information is application-dependent and in some cases insignificant, further efforts to improve the estimation of \u03bbk would be beneficial to our proposition.\nIn Figs. 1 f-g) we fix to c/4, n, d, p and \u03b3 to the values given in Alg. 2, and vary N and k. We compare the recovery performance and the time of computation of CSC, SC and Boutsidis\u2019 power method [8]. The power method (PM), in a nutshell, 1) applies the Laplacian matrix to the power r to k random signals, 2) computes the left singular vectors of the N \u00d7 k obtained matrix, to extract feature vectors, 3) applies k-means in high-dimension (like SC) with these feature vectors. In our experiments, we use r = 10. The recovery performances are nearly identical in all situations, even though CSC is only a few percents under SC and PM (Fig. f is zoomed around the high values of the recovery score). For the time of computation, the experiments confirm that all three methods are roughly linear in N and polynomial in k (Fig. g is plotted in log-log), with a lower exponent for CSC than for SC and PM; such that SC and PM are faster for k = 20 but CSC becomes up to an order of magnitude faster as k increases to 200. Note that the SBM is favorable to SC as Matlab\u2019s function eigs converges very fast in this case, e.g., for N = 105, it finds the first k = 200 eigenvectors in less\nthan 2 minutes! PM sidesteps successfully the cost of eigs, but the cost of k-means in high-dimension is still a strong bottleneck.\nWe finally compare CSC and SC on a real-world dataset: the Amazon co-purchasing network [43]. It is an undirected connected graph comprising N = 334 863 nodes and #E = 925 872 edges. The results are presented in Fig.1 h) for three values of k. As there is no clear ground truth in this case, we use the modularity [44] to measure the algorithm\u2019s clustering performance, a well-known cost function that measures how well a given partition separates a network in different communities. Note that the 20 replicates of k-means would not converge for SC with the default maximum number of iterations set to 100. For a fair comparison with CSC, we used only 2 replicates with a maximum number of iterations set to 1000 for SC\u2019s k-means step. We see that for the same clustering performance, CSC is much faster than SC, especially as k increases. The PM algorithm on this dataset does not perform well: even though the features are estimated quickly, they apparently do not form clear classes such that its k-means step takes even longer than SC\u2019s. For the three values of k, we stopped the PM algorithm after a time of computation exceeding SC\u2019s.\n6. Conclusion\nBy graph filtering O(log k) random signals with polynomial filters approximating the low-pass ideal filter, we are able to construct feature vectors whose interdistances approach the standard SC feature distances. Then, building upon compressive sensing results, we show that one can sample O(k log k) nodes from the set of N nodes, cluster this reduced set of nodes and interpolate the result back to the whole graph. If the low-dimensional k-means result is correct, i.e., if Eq. (5) is verified, we guarantee that the interpolation is a good approximation of the SC result. To improve the clustering result of the reduced set of nodes, one could build upon the concept of community cores [45]. In fact, as the filtering and the low-dimensional clustering steps are fairly cheap to compute, one could repeat these steps for different random signals, keep the sets of nodes that are always classified together and use only these stable \u201ccores\u201d for interpolation. Also, note that the SC algorithm based on the combinatorial Laplacian L = D \u2212W can also be successfully approximated by CSC. Using this Laplacian, the normalisation step of SC\u2019s algorithm (Step 2 of Alg. 1) is no longer required [27]. To adapt CSC to this version of SC, one only needs to skip the normalisation step of CSC: simply change\nstep 4 of Alg.2 to f\u0303i = ( H\u0303\u03bbkR )\u1d40 \u03b4i. In fact, the Fourier matrix (the eigenvector matrix of L) is still\northogonal in this case and all proofs stay valid. On the other hand, for the version of SC based on the random walk Laplacian L = I\u2212 D\u22121W [3], the Fourier matrix is no longer orthogonal, its inverse is no longer equal to its transpose, and the sampling theorems of [24] need yet to be extended to this case. Nevertheless, even without such potential improvements, our experiments show that CSC proves efficient and accurate in synthetic and real-world datasets; and could be preferred to SC for large N and/or k.\n. References\n[1] M. Nascimento and A. de Carvalho, \u201cSpectral methods for graph clustering a survey,\u201d European Journal of Operational Research, vol. 211, no. 2, pp. 221 \u2013 231, 2011. [2] A. Ng, M. Jordan, and Y. Weiss, \u201cOn spectral clustering: Analysis and an algorithm,\u201d in Advances in Neural\nInformation Processing Systems 14, T. Dietterich, S. Becker, and Z. Ghahramani, Eds. MIT Press, 2002, pp. 849\u2013856. [3] J. Shi and J. Malik, \u201cNormalized cuts and image segmentation,\u201d Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 888\u2013905, 2000. [4] M. Belkin and P. Niyogi, \u201cLaplacian eigenmaps for dimensionality reduction and data representation,\u201d Neural\ncomputation, vol. 15, no. 6, pp. 1373\u20131396, 2003. [5] L. Zelnik-Manor and P. Perona, \u201cSelf-tuning spectral clustering,\u201d in Advances in neural information processing\nsystems, 2004, pp. 1601\u20131608.\n[6] M. Fiedler, \u201cAlgebraic connectivity of graphs,\u201d Czechoslovak mathematical journal, vol. 23, no. 2, pp. 298\u2013305, 1973. [7] S. White and P. Smyth, \u201cA spectral clustering approach to finding communities in graph.\u201d in SDM, vol. 5. SIAM,\n2005, pp. 76\u201384.\n[8] K. P. Boutsidis, C. and A. Gittens, \u201cSpectral clustering via the power method - provably.\u201d in Proceedings of the\n32nd International Conference on Machine Learning (ICML-15), Lille, France, 2015, pp. 40\u201348.\n[9] F. Lin and W. W. Cohen, \u201cPower iteration clustering.\u201d in Proceedings of the 27th International Conference on Machine Learning (ICML-10), Haifa, Israel, 2010, pp. 655\u2013662. [10] T.-Y. Liu, H.-Y. Yang, X. Zheng, T. Qin, and W.-Y. Ma, \u201cFast large-scale spectral clustering by sequential shrinkage optimization,\u201d in Advances in Information Retrieval, 2007, pp. 319\u2013330. [11] C. Fowlkes, S. Belongie, F. Chung, and J. Malik, \u201cSpectral grouping using the nystrom method,\u201d Pattern Analysis\nand Machine Intelligence, IEEE Transactions on, vol. 26, no. 2, pp. 214\u2013225, 2004. [12] L. Wang, C. Leckie, K. Ramamohanarao, and J. Bezdek, \u201cApproximate spectral clustering,\u201d in Advances in\nKnowledge Discovery and Data Mining, 2009, pp. 134\u2013146.\n[13] X. Chen and D. Cai, \u201cLarge scale spectral clustering with landmark-based representation.\u201d in Proceedings of the 25th AAAI Conference on Artificial Intelligence, 2011. [14] T. Sakai and A. Imiya, \u201cFast spectral clustering with random projection and sampling,\u201d in Machine Learning and\nData Mining in Pattern Recognition, 2009, pp. 372\u2013384. [15] D. Yan, L. Huang, and M. Jordan, \u201cFast approximate spectral clustering,\u201d in Proceedings of the 15th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining, ser. KDD \u201909, New York, NY, USA, 2009,\npp. 907\u2013916. [16] I. Dhillon, Y. Guan, and B. Kulis, \u201cWeighted graph cuts without eigenvectors a multilevel approach,\u201d Pattern\nAnalysis and Machine Intelligence, IEEE Transactions on, vol. 29, no. 11, pp. 1944\u20131957, 2007. [17] M. Filippone, F. Camastra, F. Masulli, and S. Rovetta, \u201cA survey of kernel and spectral methods for clustering,\u201d\nPattern Recognition, vol. 41, no. 1, pp. 176 \u2013 190, 2008.\n[18] C. Boutsidis, A. Zouzias, M. W. Mahoney, and P. Drineas, \u201cStochastic dimensionality reduction for k-means clustering,\u201d arXiv, abs/1110.2897, 2011. [19] M. B. Cohen, S. Elder, C. Musco, C. Musco, and M. Persu, \u201cDimensionality reduction for k-means clustering and\nlow rank approximation,\u201d in Proceedings of the 47th Annual ACM on Symposium on Theory of Computing. ACM, 2015, pp. 163\u2013172. [20] D. Shuman, S. Narang, P. Frossard, A. Ortega, and P. Vandergheynst, \u201cThe emerging field of signal processing\non graphs: Extending high-dimensional data analysis to networks and other irregular domains,\u201d Signal Processing Magazine, IEEE, vol. 30, no. 3, pp. 83\u201398, 2013. [21] A. Sandryhaila and J. Moura, \u201cBig data analysis with signal processing on graphs: Representation and processing\nof massive data sets with irregular structure,\u201d Signal Processing Magazine, IEEE, vol. 31, no. 5, pp. 80\u201390, 2014. [22] N. Tremblay, G. Puy, P. Borgnat, R. Gribonval, and P. Vandergheynst, \u201cAccelerated spectral clustering using graph\nfiltering of random signals,\u201d in Acoustics, Speech and Signal Processing (ICASSP), IEEE International Conference\non, 2016, accepted. [23] D. Ramasamy and U. Madhow, \u201cCompressive spectral embedding: sidestepping the SVD,\u201d in Advances in Neural\nInformation Processing Systems 28, 2015, pp. 550\u2013558. [24] G. Puy, N. Tremblay, R. Gribonval, and P. Vandergheynst, \u201cRandom sampling of bandlimited signals on graphs,\u201d\narXiv, vol. abs/1511.05118, 2015.\n[25] F. Chung, Spectral graph theory. Amer Mathematical Society, 1997, no. 92. [26] A. Sandryhaila and J. Moura, \u201cDiscrete signal processing on graphs,\u201d Signal Processing, IEEE Transactions on,\nvol. 61, no. 7, pp. 1644\u20131656, 2013.\n[27] U. von Luxburg, \u201cA tutorial on spectral clustering,\u201d Statistics and Computing, vol. 17, no. 4, pp. 395\u2013416, 2007. [28] D. Achlioptas, \u201cDatabase-friendly random projections: Johnson-lindenstrauss with binary coins,\u201d Journal of Com-\nputer and System Sciences, vol. 66, no. 4, pp. 671 \u2013 687, 2003.\n[29] S. Chen, R. Varma, A. Sandryhaila, and J. Kovacevic, \u201cDiscrete signal processing on graphs: Sampling theory,\u201d Signal Processing, IEEE Transactions on, vol. 63, no. 24, pp. 6510\u20136523, 2015. [30] A. Anis, A. Gadde, and A. Ortega, \u201cEfficient sampling set selection for bandlimited graph signals using graph\nspectral proxies,\u201d arXiv, vol. abs/1510.00297, 2015. [31] M. Tsitsvero, S. Barbarossa, and P. D. Lorenzo, \u201cSignals on graphs: Uncertainty principle and sampling,\u201d arXiv,\nvol. abs/1507.08822, 2015. [32] A. Marques, S. Segarra, G. Leus, and A. Ribeiro, \u201cSampling of graph signals with successive local aggregations,\u201d\nSignal Processing, IEEE Transactions on, vol. PP, no. 99, pp. 1\u20131, 2015. [33] X. Shi, H. Feng, M. Zhai, T. Yang, and B. Hu, \u201cInfinite impulse response graph filters in wireless sensor networks,\u201d\nSignal Processing Letters, IEEE, vol. 22, no. 8, pp. 1113\u20131117, 2015.\n[34] D. Shuman, P. Vandergheynst, and P. Frossard, \u201cChebyshev polynomial approximation for distributed signal\nprocessing,\u201d in Distributed Computing in Sensor Systems and Workshops (DCOSS), International Conference on, 2011, pp. 1\u20138. [35] E. D. Napoli, E. Polizzi, and Y. Saad, \u201cEfficient estimation of eigenvalue counts in an interval,\u201d arXiv, vol. abs/1308.4275, 2013. [36] H. Avron and S. Toledo, \u201cRandomized algorithms for estimating the trace of an implicit symmetric positive semi-\ndefinite matrix,\u201d J. ACM, vol. 58, no. 2, pp. 8:1\u20138:34, 2011. [37] W.-Y. Chen, Y. Song, H. Bai, L. C.-J, and E. Chang, \u201cParallel spectral clustering in distributed systems,\u201d Pattern\nAnalysis and Machine Intelligence, IEEE Transactions on, vol. 33, no. 3, pp. 568\u2013586, 2011.\n[38] J. Lei and A. Rinaldo, \u201cConsistency of spectral clustering in stochastic block models,\u201d Ann. Statist., vol. 43, no. 1,\npp. 215\u2013237, 2015.\n[39] N. Perraudin, J. Paratte, D. Shuman, V. Kalofolias, P. Vandergheynst, and D. Hammond, \u201cGspbox: A toolbox for signal processing on graphs,\u201d arXiv, vol. abs/1408.5781, 2014. [40] http://perso.ens-lyon.fr/nicolas.tremblay/index.php?page=downloads. [41] A. Decelle, F. Krzakala, C. Moore, and L. Zdeborova\u0301, \u201cAsymptotic analysis of the stochastic block model for\nmodular networks and its algorithmic applications,\u201d Phys. Rev. E, vol. 84, p. 066106, 2011.\n[42] L. Hubert and P. Arabie, \u201cComparing partitions,\u201d Journal of classification, vol. 2, no. 1, pp. 193\u2013218, 1985. [43] J. Yang and J. Leskovec, \u201cDefining and evaluating network communities based on ground-truth,\u201d Knowledge and\nInformation Systems, vol. 42, no. 1, pp. 181\u2013213, 2015.\n[44] M. E. J. Newman and M. Girvan, \u201cFinding and evaluating community structure in networks,\u201d Phys. Rev. E, vol. 69, p. 026113, 2004. [45] M. Seifi, I. Junier, J.-B. Rouquier, S. Iskrov, and J.-L. Guillaume, \u201cStable community cores in complex networks,\u201d\nin Complex Networks, 2013, pp. 87\u201398."}], "references": [{"title": "Spectral methods for graph clustering a survey", "author": ["M. Nascimento", "A. de Carvalho"], "venue": "European Journal of Operational Research, vol. 211, no. 2, pp. 221 \u2013 231, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["A. Ng", "M. Jordan", "Y. Weiss"], "venue": "Advances in Neural Information Processing Systems 14, T. Dietterich, S. Becker, and Z. Ghahramani, Eds. MIT Press, 2002, pp. 849\u2013856.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 888\u2013905, 2000.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural computation, vol. 15, no. 6, pp. 1373\u20131396, 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Self-tuning spectral clustering", "author": ["L. Zelnik-Manor", "P. Perona"], "venue": "Advances in neural information processing systems, 2004, pp. 1601\u20131608.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Algebraic connectivity of graphs", "author": ["M. Fiedler"], "venue": "Czechoslovak mathematical journal, vol. 23, no. 2, pp. 298\u2013305, 1973.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1973}, {"title": "A spectral clustering approach to finding communities in graph.", "author": ["S. White", "P. Smyth"], "venue": "in SDM,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Spectral clustering via the power method - provably.", "author": ["C.K.P. Boutsidis", "A. Gittens"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML-15),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Power iteration clustering.", "author": ["F. Lin", "W.W. Cohen"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), Haifa, Israel,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Fast large-scale spectral clustering by sequential shrinkage optimization", "author": ["T.-Y. Liu", "H.-Y. Yang", "X. Zheng", "T. Qin", "W.-Y. Ma"], "venue": "Advances in Information Retrieval, 2007, pp. 319\u2013330.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Spectral grouping using the nystrom method", "author": ["C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 26, no. 2, pp. 214\u2013225, 2004.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Approximate spectral clustering", "author": ["L. Wang", "C. Leckie", "K. Ramamohanarao", "J. Bezdek"], "venue": "Advances in Knowledge Discovery and Data Mining, 2009, pp. 134\u2013146.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Large scale spectral clustering with landmark-based representation.", "author": ["X. Chen", "D. Cai"], "venue": "Proceedings of the 25th AAAI Conference on Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Fast spectral clustering with random projection and sampling", "author": ["T. Sakai", "A. Imiya"], "venue": "Machine Learning and Data Mining in Pattern Recognition, 2009, pp. 372\u2013384.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast approximate spectral clustering", "author": ["D. Yan", "L. Huang", "M. Jordan"], "venue": "Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD \u201909, New York, NY, USA, 2009, pp. 907\u2013916.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Weighted graph cuts without eigenvectors a multilevel approach", "author": ["I. Dhillon", "Y. Guan", "B. Kulis"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 29, no. 11, pp. 1944\u20131957, 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1944}, {"title": "A survey of kernel and spectral methods for clustering", "author": ["M. Filippone", "F. Camastra", "F. Masulli", "S. Rovetta"], "venue": "Pattern Recognition, vol. 41, no. 1, pp. 176 \u2013 190, 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Stochastic dimensionality reduction for k-means clustering", "author": ["C. Boutsidis", "A. Zouzias", "M.W. Mahoney", "P. Drineas"], "venue": "arXiv, abs/1110.2897, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Dimensionality reduction for k-means clustering and low rank approximation", "author": ["M.B. Cohen", "S. Elder", "C. Musco", "C. Musco", "M. Persu"], "venue": "Proceedings of the 47th Annual ACM on Symposium on Theory of Computing. ACM, 2015, pp. 163\u2013172.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains", "author": ["D. Shuman", "S. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst"], "venue": "Signal Processing Magazine, IEEE, vol. 30, no. 3, pp. 83\u201398, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Big data analysis with signal processing on graphs: Representation and processing of massive data sets with irregular structure", "author": ["A. Sandryhaila", "J. Moura"], "venue": "Signal Processing Magazine, IEEE, vol. 31, no. 5, pp. 80\u201390, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Accelerated spectral clustering using graph filtering of random signals", "author": ["N. Tremblay", "G. Puy", "P. Borgnat", "R. Gribonval", "P. Vandergheynst"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), IEEE International Conference on, 2016, accepted.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Compressive spectral embedding: sidestepping the SVD", "author": ["D. Ramasamy", "U. Madhow"], "venue": "Advances in Neural Information Processing Systems 28, 2015, pp. 550\u2013558.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Random sampling of bandlimited signals on graphs", "author": ["G. Puy", "N. Tremblay", "R. Gribonval", "P. Vandergheynst"], "venue": "arXiv, vol. abs/1511.05118, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Spectral graph theory", "author": ["F. Chung"], "venue": "Amer Mathematical Society,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1997}, {"title": "Discrete signal processing on graphs", "author": ["A. Sandryhaila", "J. Moura"], "venue": "Signal Processing, IEEE Transactions on, vol. 61, no. 7, pp. 1644\u20131656, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "A tutorial on spectral clustering", "author": ["U. von Luxburg"], "venue": "Statistics and Computing, vol. 17, no. 4, pp. 395\u2013416, 2007.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Database-friendly random projections: Johnson-lindenstrauss with binary coins", "author": ["D. Achlioptas"], "venue": "Journal of Computer and System Sciences, vol. 66, no. 4, pp. 671 \u2013 687, 2003.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2003}, {"title": "Discrete signal processing on graphs: Sampling theory", "author": ["S. Chen", "R. Varma", "A. Sandryhaila", "J. Kovacevic"], "venue": "Signal Processing, IEEE Transactions on, vol. 63, no. 24, pp. 6510\u20136523, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient sampling set selection for bandlimited graph signals using graph spectral proxies", "author": ["A. Anis", "A. Gadde", "A. Ortega"], "venue": "arXiv, vol. abs/1510.00297, 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Signals on graphs: Uncertainty principle and sampling", "author": ["M. Tsitsvero", "S. Barbarossa", "P.D. Lorenzo"], "venue": "arXiv, vol. abs/1507.08822, 2015.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Sampling of graph signals with successive local aggregations", "author": ["A. Marques", "S. Segarra", "G. Leus", "A. Ribeiro"], "venue": "Signal Processing, IEEE Transactions on, vol. PP, no. 99, pp. 1\u20131, 2015.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Infinite impulse response graph filters in wireless sensor networks", "author": ["X. Shi", "H. Feng", "M. Zhai", "T. Yang", "B. Hu"], "venue": "Signal Processing Letters, IEEE, vol. 22, no. 8, pp. 1113\u20131117, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Chebyshev polynomial approximation for distributed signal processing", "author": ["D. Shuman", "P. Vandergheynst", "P. Frossard"], "venue": "Distributed Computing in Sensor Systems and Workshops (DCOSS), International Conference on, 2011, pp. 1\u20138.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient estimation of eigenvalue counts in an interval", "author": ["E.D. Napoli", "E. Polizzi", "Y. Saad"], "venue": "arXiv, vol. abs/1308.4275, 2013.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Randomized algorithms for estimating the trace of an implicit symmetric positive semidefinite matrix", "author": ["H. Avron", "S. Toledo"], "venue": "J. ACM, vol. 58, no. 2, pp. 8:1\u20138:34, 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Parallel spectral clustering in distributed systems", "author": ["W.-Y. Chen", "Y. Song", "H. Bai", "L.C.-J", "E. Chang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 33, no. 3, pp. 568\u2013586, 2011.  Compressive spectral clustering  15", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Consistency of spectral clustering in stochastic block models", "author": ["J. Lei", "A. Rinaldo"], "venue": "Ann. Statist., vol. 43, no. 1, pp. 215\u2013237, 2015.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Gspbox: A toolbox for signal processing on graphs", "author": ["N. Perraudin", "J. Paratte", "D. Shuman", "V. Kalofolias", "P. Vandergheynst", "D. Hammond"], "venue": "arXiv, vol. abs/1408.5781, 2014.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications", "author": ["A. Decelle", "F. Krzakala", "C. Moore", "L. Zdeborov\u00e1"], "venue": "Phys. Rev. E, vol. 84, p. 066106, 2011.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Comparing partitions", "author": ["L. Hubert", "P. Arabie"], "venue": "Journal of classification, vol. 2, no. 1, pp. 193\u2013218, 1985.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1985}, {"title": "Defining and evaluating network communities based on ground-truth", "author": ["J. Yang", "J. Leskovec"], "venue": "Knowledge and Information Systems, vol. 42, no. 1, pp. 181\u2013213, 2015.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "Finding and evaluating community structure in networks", "author": ["M.E.J. Newman", "M. Girvan"], "venue": "Phys. Rev. E, vol. 69, p. 026113, 2004.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2004}, {"title": "Stable community cores in complex networks", "author": ["M. Seifi", "I. Junier", "J.-B. Rouquier", "S. Iskrov", "J.-L. Guillaume"], "venue": "Complex Networks, 2013, pp. 87\u201398.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Spectral clustering (SC) is a fundamental tool in data mining [1].", "startOffset": 62, "endOffset": 65}, {"referenceID": 1, "context": ", [2\u20135], but all follow the same scheme.", "startOffset": 2, "endOffset": 7}, {"referenceID": 2, "context": ", [2\u20135], but all follow the same scheme.", "startOffset": 2, "endOffset": 7}, {"referenceID": 3, "context": ", [2\u20135], but all follow the same scheme.", "startOffset": 2, "endOffset": 7}, {"referenceID": 4, "context": ", [2\u20135], but all follow the same scheme.", "startOffset": 2, "endOffset": 7}, {"referenceID": 5, "context": "This k-way scheme is a generalisation of Fiedler\u2019s pioneering work [6].", "startOffset": 67, "endOffset": 70}, {"referenceID": 6, "context": ", concentric circles) for which naive k-means clustering fails; 2) if the input data is directly a graph G modeling a network [7], such as social, neuronal, or transportation networks.", "startOffset": 126, "endOffset": 129}, {"referenceID": 7, "context": ", via the power method [8, 9], via a careful optimisation of diagonalisation algorithms in the context of SC [10], or via matrix column-subsampling such as in the Nystr\u00f6m method [11], the nSPEC and cSPEC methods of [12], or in [13, 14].", "startOffset": 23, "endOffset": 29}, {"referenceID": 8, "context": ", via the power method [8, 9], via a careful optimisation of diagonalisation algorithms in the context of SC [10], or via matrix column-subsampling such as in the Nystr\u00f6m method [11], the nSPEC and cSPEC methods of [12], or in [13, 14].", "startOffset": 23, "endOffset": 29}, {"referenceID": 9, "context": ", via the power method [8, 9], via a careful optimisation of diagonalisation algorithms in the context of SC [10], or via matrix column-subsampling such as in the Nystr\u00f6m method [11], the nSPEC and cSPEC methods of [12], or in [13, 14].", "startOffset": 109, "endOffset": 113}, {"referenceID": 10, "context": ", via the power method [8, 9], via a careful optimisation of diagonalisation algorithms in the context of SC [10], or via matrix column-subsampling such as in the Nystr\u00f6m method [11], the nSPEC and cSPEC methods of [12], or in [13, 14].", "startOffset": 178, "endOffset": 182}, {"referenceID": 11, "context": ", via the power method [8, 9], via a careful optimisation of diagonalisation algorithms in the context of SC [10], or via matrix column-subsampling such as in the Nystr\u00f6m method [11], the nSPEC and cSPEC methods of [12], or in [13, 14].", "startOffset": 215, "endOffset": 219}, {"referenceID": 12, "context": ", via the power method [8, 9], via a careful optimisation of diagonalisation algorithms in the context of SC [10], or via matrix column-subsampling such as in the Nystr\u00f6m method [11], the nSPEC and cSPEC methods of [12], or in [13, 14].", "startOffset": 227, "endOffset": 235}, {"referenceID": 13, "context": ", via the power method [8, 9], via a careful optimisation of diagonalisation algorithms in the context of SC [10], or via matrix column-subsampling such as in the Nystr\u00f6m method [11], the nSPEC and cSPEC methods of [12], or in [13, 14].", "startOffset": 227, "endOffset": 235}, {"referenceID": 14, "context": "One can find such methods in [15] and [12]\u2019s eSPEC proposition, where two different interpolation methods are used.", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "One can find such methods in [15] and [12]\u2019s eSPEC proposition, where two different interpolation methods are used.", "startOffset": 38, "endOffset": 42}, {"referenceID": 15, "context": "Also, let us mention [16] that circumvents both the eigendecomposition and the k-means bottlenecks: the authors reduce the graph\u2019s size by successive aggregation of nodes, apply SC on this small graph, and propagate the results on the complete graph using kernel k-means to control interpolation errors.", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "The kernel is computed so that kernel k-means and SC share the same objective function [17].", "startOffset": 87, "endOffset": 91}, {"referenceID": 17, "context": "works [18, 19] that concentrate on reducing the feature vectors\u2019 dimension in the k-means problem, but do not sidestep the eigendecomposition nor the large N issues.", "startOffset": 6, "endOffset": 14}, {"referenceID": 18, "context": "works [18, 19] that concentrate on reducing the feature vectors\u2019 dimension in the k-means problem, but do not sidestep the eigendecomposition nor the large N issues.", "startOffset": 6, "endOffset": 14}, {"referenceID": 19, "context": "In this work, inspired by recent advances in the emerging field of graph signal processing [20, 21], we circumvent SC\u2019s last two bottlenecks and detail a fast approximate spectral clustering method for large datasets, as well as the supporting theory.", "startOffset": 91, "endOffset": 99}, {"referenceID": 20, "context": "In this work, inspired by recent advances in the emerging field of graph signal processing [20, 21], we circumvent SC\u2019s last two bottlenecks and detail a fast approximate spectral clustering method for large datasets, as well as the supporting theory.", "startOffset": 91, "endOffset": 99}, {"referenceID": 21, "context": "The first ingredient builds upon recent works [22,23] that avoid the costly computation of the eigenvectors of L by filtering O(log(k)) random signals on G that will then serve as feature vectors to perform clustering.", "startOffset": 46, "endOffset": 53}, {"referenceID": 22, "context": "The first ingredient builds upon recent works [22,23] that avoid the costly computation of the eigenvectors of L by filtering O(log(k)) random signals on G that will then serve as feature vectors to perform clustering.", "startOffset": 46, "endOffset": 53}, {"referenceID": 23, "context": "The second ingredient uses a recent sampling theory of bandlimited graph-signals [24] to reduce the computational complexity of k-means.", "startOffset": 81, "endOffset": 85}, {"referenceID": 24, "context": "6 \u03bbN 6 2 [25], and of the orthonormal matrix U := (u1|u2| .", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "By analogy to the continuous Laplacian operator whose eigenfunctions are the classical Fourier modes and eigenvalues their squared frequencies, the columns of U are considered as the graph\u2019s Fourier modes, and { \u221a \u03bbl}l as its set of associated \u201cfrequencies\u201d [20].", "startOffset": 258, "endOffset": 262}, {"referenceID": 25, "context": ", [26], but in order to exhibit the link between graph signal processing and SC (that partially diagonalizes the Laplacian matrix), the Laplacian-based Fourier matrix appears more natural.", "startOffset": 2, "endOffset": 6}, {"referenceID": 1, "context": "Given a continuous filter function h defined on [0, 2], its associated graph filter operator H \u2208 RN\u00d7N is defined as H := h(L) = Uh(\u039b)UT, where h(\u039b) := diag(h(\u03bb1), h(\u03bb2), \u00b7 \u00b7 \u00b7 , h(\u03bbN )).", "startOffset": 48, "endOffset": 54}, {"referenceID": 1, "context": "In the following, we consider ideal low-pass filters, denoted by h\u03bbc , that satisfy, for all \u03bb \u2208 [0, 2], h\u03bbc(\u03bb) = 1, if \u03bb 6 \u03bbc, and h\u03bbc(\u03bb) = 0, if not.", "startOffset": 97, "endOffset": 103}, {"referenceID": 1, "context": "Algorithm 1 Spectral Clustering [2]", "startOffset": 32, "endOffset": 35}, {"referenceID": 1, "context": "l=0 \u03b1l\u03bb l ' h(\u03bb) for all \u03bb \u2208 [0, 2], where \u03b11, .", "startOffset": 29, "endOffset": 35}, {"referenceID": 1, "context": "\u2019s method [2] based on the normalized Laplacian as our standard SC method.", "startOffset": 10, "endOffset": 13}, {"referenceID": 21, "context": "A theorem recently published independently by two teams [22, 23] shows that this is possible when there is no normalisation step (step 2 in Alg.", "startOffset": 56, "endOffset": 64}, {"referenceID": 22, "context": "A theorem recently published independently by two teams [22, 23] shows that this is possible when there is no normalisation step (step 2 in Alg.", "startOffset": 56, "endOffset": 64}, {"referenceID": 23, "context": "To guarantee robust reconstruction, we take advantage of recent results of [24] on random sampling of", "startOffset": 75, "endOffset": 79}, {"referenceID": 26, "context": "See [27] for several choices of similarity measure s and several ways to create W from the s(xi,xj).", "startOffset": 4, "endOffset": 8}, {"referenceID": 27, "context": "1 of [28]) random variables.", "startOffset": 5, "endOffset": 9}, {"referenceID": 27, "context": "1 of [28] (an instance of the Johnson-Lindenstrauss lemma) to \u2016RUk(fi \u2212 fj)\u2016, the following holds.", "startOffset": 5, "endOffset": 9}, {"referenceID": 21, "context": ", f\u0303N} , as done in [22, 23].", "startOffset": 20, "endOffset": 28}, {"referenceID": 22, "context": ", f\u0303N} , as done in [22, 23].", "startOffset": 20, "endOffset": 28}, {"referenceID": 28, "context": "There has been recently a surge of interest around adapting classical sampling theorems to such bandlimited graph signals [29\u201332].", "startOffset": 122, "endOffset": 129}, {"referenceID": 29, "context": "There has been recently a surge of interest around adapting classical sampling theorems to such bandlimited graph signals [29\u201332].", "startOffset": 122, "endOffset": 129}, {"referenceID": 30, "context": "There has been recently a surge of interest around adapting classical sampling theorems to such bandlimited graph signals [29\u201332].", "startOffset": 122, "endOffset": 129}, {"referenceID": 31, "context": "There has been recently a surge of interest around adapting classical sampling theorems to such bandlimited graph signals [29\u201332].", "startOffset": 122, "endOffset": 129}, {"referenceID": 23, "context": "We rely here on the random sampling strategy proposed in [24] to select a subset of n nodes.", "startOffset": 57, "endOffset": 61}, {"referenceID": 21, "context": "Results in [22,23] show that k-means is also able to identify the clusters using the feature vectors {f\u03031, .", "startOffset": 11, "endOffset": 18}, {"referenceID": 22, "context": "Results in [22,23] show that k-means is also able to identify the clusters using the feature vectors {f\u03031, .", "startOffset": 11, "endOffset": 18}, {"referenceID": 23, "context": "[24] show that the solution to the optimisation problem", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "3precise error bounds are provided in [24].", "startOffset": 38, "endOffset": 42}, {"referenceID": 23, "context": "It is shown in [24] that \u03bdk \u2208 [k, N].", "startOffset": 15, "endOffset": 19}, {"referenceID": 23, "context": "4 ( [24]).", "startOffset": 4, "endOffset": 8}, {"referenceID": 23, "context": "Indeed, it is proved in [24] that, whatever the graph G, there always exists an optimal sampling distribution such that n = O(k log k) samples are sufficient to satisfy Eq.", "startOffset": 24, "endOffset": 28}, {"referenceID": 23, "context": "This distribution depends on the profile of the local cumulative coherence and can be estimated rapidly (see [24] for more details).", "startOffset": 109, "endOffset": 113}, {"referenceID": 1, "context": "\u2200\u03bb \u2208 [0, 2], e(\u03bb) := h\u0303\u03bbk(\u03bb)\u2212 h\u03bbk(\u03bb).", "startOffset": 5, "endOffset": 11}, {"referenceID": 27, "context": "1 in [28].", "startOffset": 5, "endOffset": 9}, {"referenceID": 23, "context": "4 of [24] showing that vk(i) = \u2016Uk\u03b4i\u2016 \u2248 \u2016(H\u03bbkR)\u03b4i\u2016.", "startOffset": 5, "endOffset": 9}, {"referenceID": 32, "context": "Such filters have been introduced in the graph context [33], but they involve another optimisation step that would burden our main message.", "startOffset": 55, "endOffset": 59}, {"referenceID": 33, "context": "In our experiments, we could follow [34] and use truncated Chebychev polynomials to approximate the ideal filter, as these polynomials are known to require a small degree to ensure a given tolerated maximal error em.", "startOffset": 36, "endOffset": 40}, {"referenceID": 34, "context": "We prefer to follow [35] who suggest to use Jackson-Chebychev polynomials: Chebychev polynomials to which are added damping multipliers to alleviate the unwanted Gibbs oscillations around the cut-off frequency \u03bbk.", "startOffset": 20, "endOffset": 24}, {"referenceID": 23, "context": "1 in [24], where an estimation of \u03bbk is obtained as a by-product.", "startOffset": 5, "endOffset": 9}, {"referenceID": 35, "context": "3 of [36] shows that this eigencount estimator yields the exact result (i.", "startOffset": 5, "endOffset": 9}, {"referenceID": 34, "context": "In practice, experiments in [35] show that fewer random signals are sufficient in some cases.", "startOffset": 28, "endOffset": 32}, {"referenceID": 23, "context": "Inspired by these results and as in [24], we choose 2 logN random signals for this estimation task.", "startOffset": 36, "endOffset": 40}, {"referenceID": 36, "context": "2 of [37]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 37, "context": "We first perform well-controlled experiments on the Stochastic Block Model (SBM), a model of random graphs with community structures, that was showed suitable as a benchmark for SC in [38].", "startOffset": 184, "endOffset": 188}, {"referenceID": 38, "context": "The fast filtering part of CSC uses the gsp cheby op function of the GSP toolbox [39].", "startOffset": 81, "endOffset": 85}, {"referenceID": 39, "context": "[41] show that a critical value c exists above which community detection is impossible at the large N limit: c = (s\u2212 \u221a s)/(s+ \u221a s(k \u2212 1)).", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "The performance is measured by the Adjusted Rand similarity index [42] between the SBM\u2019s ground truth and the obtained partitions.", "startOffset": 66, "endOffset": 70}, {"referenceID": 7, "context": "We compare the recovery performance and the time of computation of CSC, SC and Boutsidis\u2019 power method [8].", "startOffset": 103, "endOffset": 106}, {"referenceID": 41, "context": "We finally compare CSC and SC on a real-world dataset: the Amazon co-purchasing network [43].", "startOffset": 88, "endOffset": 92}, {"referenceID": 42, "context": "As there is no clear ground truth in this case, we use the modularity [44] to measure the algorithm\u2019s clustering performance, a well-known cost function that measures how well a given partition separates a network in different communities.", "startOffset": 70, "endOffset": 74}, {"referenceID": 43, "context": "To improve the clustering result of the reduced set of nodes, one could build upon the concept of community cores [45].", "startOffset": 114, "endOffset": 118}, {"referenceID": 26, "context": "1) is no longer required [27].", "startOffset": 25, "endOffset": 29}, {"referenceID": 2, "context": "On the other hand, for the version of SC based on the random walk Laplacian L = I\u2212 D\u22121W [3], the Fourier matrix is no longer orthogonal, its inverse is no longer equal to its transpose, and the sampling theorems of [24] need yet to be extended to this case.", "startOffset": 88, "endOffset": 91}, {"referenceID": 23, "context": "On the other hand, for the version of SC based on the random walk Laplacian L = I\u2212 D\u22121W [3], the Fourier matrix is no longer orthogonal, its inverse is no longer equal to its transpose, and the sampling theorems of [24] need yet to be extended to this case.", "startOffset": 215, "endOffset": 219}], "year": 2017, "abstractText": "Spectral clustering has become a popular technique due to its high performance in many contexts. It comprises three main steps: create a similarity graph between N objects to cluster, compute the first k eigenvectors of its Laplacian matrix to define a feature vector for each object, and run k-means on these features to separate objects into k classes. Each of these three steps becomes computationally intensive for large N and/or k. We propose to speed up the last two steps based on recent results in the emerging field of graph signal processing: graph filtering of random signals, and random sampling of bandlimited graph signals. We prove that our method, with a gain in computation time that can reach several orders of magnitude, is in fact an approximation of spectral clustering, for which we are able to control the error. We test the performance of our method on artificial and real-world network data.", "creator": "LaTeX with hyperref package"}}}