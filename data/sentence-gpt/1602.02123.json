{"id": "1602.02123", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Feb-2016", "title": "Sequence Classification with Neural Conditional Random Fields", "abstract": "The proliferation of sensor devices monitoring human activity generates voluminous amount of temporal sequences needing to be interpreted and categorized. Moreover, complex behavior detection requires the personalization of multi-sensor fusion algorithms. Conditional random fields (CRFs) are commonly used in structured prediction tasks such as part-of-speech tagging in natural language processing. Conditional probabilities guide the choice of each tag/label in the sequence conflating the structured prediction task with the sequence classification task where different models provide different categorization of the same sequence. The claim of this paper is that CRF models also provide discriminative models to distinguish between types of sequence regardless of the accuracy of the labels obtained if we calibrate the class membership estimate of the sequence. We introduce and compare different neural network based linear-chain CRFs and we present experiments on two complex sequence classification and structured prediction tasks to support this claim. The first experiment (C2M) utilized a sequence classification task using a model based on the recurrent neural network (rNN) of MQD. These analyses revealed that a large proportion of sequences are in a hierarchical group with high number of classes, with high number of classes and relatively low number of classes. We analyzed an estimated number of categories of classification and categorized specific class classes using the same model and used the same model to estimate the classification-based classification task. These results are shown in Fig. 3. C2M. In this model, the classification group (C2M) contains only 10 categories in which the classification group is defined. For each category, the classification group consists of 12 different classes and consists of 10 classes (C2M) consisting of 18 classes (C2M) consisting of 19 classes (C2M) comprising 18 classes (C2M) consisting of 20 classes (C2M) consisting of 21 classes (C2M) comprising 21 classes (C2M) consisting of 22 classes (C2M) comprising 22 classes (C2M) consisting of 23 classes (C2M) consisting of 24 classes (C2M) consisting of 25 classes (C2M) consisting of 26 classes (C2M) consisting of 27 classes (C2M) consisting of 28 classes (C2M) consisting of 29 classes (C2M) consisting of 30 classes (C2M) consisting of 31 classes (C2M) consisting of 32 classes (C2M) consisting of 33 classes (C2M) consisting of 34 classes (C2M) consisting of", "histories": [["v1", "Fri, 5 Feb 2016 19:19:46 GMT  (564kb,D)", "http://arxiv.org/abs/1602.02123v1", "14th International Conference on Machine Learning and Applications (ICMLA) 2015"]], "COMMENTS": "14th International Conference on Machine Learning and Applications (ICMLA) 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["myriam abramson"], "accepted": false, "id": "1602.02123"}, "pdf": {"name": "1602.02123.pdf", "metadata": {"source": "CRF", "title": "Sequence Classification with Neural Conditional Random Fields", "authors": ["Myriam Abramson"], "emails": ["myriam.abramson@nrl.navy.mil"], "sections": [{"heading": null, "text": "Index Terms\u2014hybrid learning algorithms, neurocrfs, sequence classification\nI. INTRODUCTION\nThe proliferation of sensor devices monitoring human activity generates voluminous amount of temporal sequences needing to be interpreted and categorized. Moreover, complex behavior detection requires the personalization of multisensor fusion algorithms. For example, stress detection from physiological measurements can involve the fusion of several variables such as pupil dilation, heart rate, and skin temperature. In addition, it is the relative measurement to other states rather than their absolute value that is more indicative of stress requiring the monitoring of sequences of states and their transitions. Conditional random fields (CRFs) are commonly used in structured prediction tasks such as part-of-speech tagging in natural language processing. Conditional probabilities guide the choice of each tag/label in the sequence conflating the structured prediction task with the sequence classification task where different models could provide different evaluations of the same sequence. The claim of this paper is that CRF models also provide discriminative models to distinguish between types of sequence regardless of the accuracy of the labels obtained, provided that the class membership estimate of the sequence be calibrated. In other words, the score obtained by the CRF model representing a ranking of the sequence given the model has to correlate with the empirical class membership probability [1]. The intuition underlying this claim is that hard-to-detect complex\nobservation patterns might not provide an accurate labeling while providing enough discriminatory evidence against other types of sequence. CRFs are a very flexible way of modeling variable-length sequences that can leverage from state-of-theart discriminative learners. We present experiments in the hand-writing word recognition task and in the Web analytics authentication task.\nThis paper is organized as follows. Section II describes related work in sequence classification. Section III provides an overview of CRFs. Section IV presents our methodology combining neural networks with CRFs. Section V presents our modeling and empirical evaluation of the hand-writing word recognition task and of the Web analytics personalization task. Section VI concludes with discussion and future work in this area."}, {"heading": "II. RELATED WORK", "text": "The problem that sequence classification addresses is introduced in [2], namely sequence classification is defined as learning the function mapping a sequence s to a class label l from a set of labels L. A distinction is made between predicting sequence labels where the entire sequence is available and a sequence of labels such as found in streaming data and addressed by structured prediction methods. This latter problem is termed the strong classification task. We argue in this paper that a stronger classification task might be to identify a type of sequence from the classification of its temporal components. Sequence classification methods include feature vectors, distance-based methods and model-based methods like discriminative k-Markov models or hidden Markov models (HMMs) if the sequence labels are not observed at test time.\nNeural conditional random fields or neuroCRFs have been investigated in [3] using deep neural networks showing the influence of 2 layers vs. 1 layer of hidden nodes as well as the number of hidden nodes in reducing the error rate for structured prediction tasks. In this work, the outputs of the deep neural network compute the weights of all the factors and leave the probabilistic framework of CRF intact.\nSimilar to HMMs, Long Short Term Memory (LSTM) recurrent neural networks (RNNs) [4] learn a sequence of labels from unsegmented data such as that found in handwriting or speech recognition. This capability, called temporal classification, is distinguished from framewise classification\nar X\niv :1\n60 2.\n02 12\n3v 1\n[ cs\n.L G\n] 5\nF eb\n2 01\n6\n2 where the training data is a sequence of pairwise input and output labels, suitable for supervised learning, and where the length of the sequence is known. LSTM RNNs\u2019 architecture consists of a hidden layer recurrent neural network with generative capabilities and adapted for deep learning with skip connections between hidden nodes at different levels. Unlike HMMs, there are no direct connections between the output nodes of the neural network (i.e., the labels of the sequence), but there are indirect connections through a prediction network from an output node to the next input. Consequently, LSTM RNNs can do sequence labeling as well as sequence generation through their predictive capability.\nIn [5], perceptrons were integrated as discriminative learners in the probabilistic framework of CRFs in the context of partof-speech tagging. The Viterbi decoding algorithm finds the best tagged sequence under the current weight parameters of feature-tag pairs. As in the perceptron algorithm, weight updates (0/1 loss) are triggered only when discrepancies occur.\nA distinction is made between discriminative and generative k-Markov models in the sequence classification of simple symbolic sequences [6]. Generative models estimate the probability distribution of features given a class from the training data and base their classification decision on the joint probability of the features and the class while discriminative models directly estimate the conditional probability of a class given the features. Similar to CRFs, discriminative kMarkov models maximize, using gradient descent, two sets of parameters, namely, the probability of a symbol si given preceding symbols in a given model and the joint probability of a sequence of symbols (assuming independence) given a model. In addition, this work shows that a hybrid approach initializing parameters with generative models can speed up convergence."}, {"heading": "III. CONDITIONAL RANDOM FIELDS", "text": "CRFs are a supervised method for structured prediction similar to HMMs [7] while relaxing the independence assumption of the observations and the Markov assumption [8]. The labels of the sequence must be provided at training time. CRFs address the strong classification problem [2] of predicting a sequence of labels but also take advantage of information from the entire sequence to estimate the probability of the entire sequence and therefore can also address the sequence classification problem. We distinguish between weakly-supervised CRFs where the labels are learned through an auxiliary classifier and strongly-supervised CRFs where the labels are known without ambiguity [9]. We describe below the derivation of CRFs from basic probabilistic principles and restrict our discussion to linear-chain CRFs for the classification of sequences.\nThe factorization of Bayesian nets according to conditional independence enables the tractable computation of the joint probability of a collection of random variables P (y\u0304) according to the structure of a graphical model as follows.\nP (y\u0304) = \u220f i p(yi|ypi ) (1)\nwhere ypi are the parents of yi. However, it is sometimes more natural to model a problem according to spatial or temporal proximity of the nodes rather than their conditional independence. For example, in a lattice-like graphical structure, the Markov blanket of a node does not obey the spatial neighborhood properties of the graph as expected [10]. It is therefore more natural to model such graphs as undirected graph models where the independence of the nodes is determined only by the absence of a connecting edge. It is possible to convert a directed graph to an undirected graph by \u201cmoralizing\u201d it (i.e. adding edges between nodes to indicate implicit dependence). The edges of an undirected graph cannot be weighted by conditional probabilities anymore but can be evaluated according to the \u201caffinity\u201d of the nodes defined by a potential function or factor \u03d5(x, y). Those factors are parameterized by a weight \u03b8c that can be learned from data using various methods. According to the Hammersley-Clifford theorem, the joint probability of the graph can then be obtained as follows:\nP (y\u0304|\u03b8\u0304) = 1 Z(\u03b8) \u220f c \u03d5(yc|\u03b8c) (2)\nwhere Z(\u03b8) is the partition function normalizing the product of factors in order to obtain a probability distribution.\nCRFs leverage from the undirected graph modeling approach to model the conditional distribution P (y\u0304|x\u0304) of a set of target variables y\u0304 and a set of observed variables x\u0304 to represent structured data. A factor represents the probability of a target variable y as a linear function of the weight parameters \u03b8 and the observed input variables x\u0304 which are not necessarily independent. If the observed input variables are indicator functions, \u03c6(x, y), then P (y|x\u0304, \u03b8\u0304) is defined as follows:\nP (y|x\u0304, \u03b8\u0304) = exp \u2211 j \u03b8j\u03c6j(x\u0304, y)\u2211\ny\u2032\u2208Y Z(x\u0304, y \u2032)\n(3)\nwhere Z(x\u0304, y\u2032) = exp \u2211 j \u03b8j\u03c6j(x\u0304, y\n\u2032). The weight parameters \u03b8 of the factors are typically learned using discriminative learning methods for the target variable y as an alternative to the probabilistic likelihood estimation method for computational efficiency reasons. There are two types of feature functions in representing a sequence [11]: (1) edge functions between two labels and (2) observation functions relating x and y. Generally, fj(yt\u22121, yt, x\u0304, t) represents a feature function combining both edge and observation functions (Fig. 1). The following are examples of both types of feature functions in the handwriting recognition domain:\n\u03c6j(yi\u22121, yi) =\n{ 1 if yi = \u201di\u201d and\nyi\u22121 = \u201dn\u201d 0 otherwise\n\u03c6j(yi, xi) =\n{ 1 if pixel(xi) = 1 and\nyi = \u201di\u201d 0 otherwise\nThe number of possible feature functions can be large but can be practically restricted to those found in the training set.\n3\nGeneralizing to \u201cglobal\u201d factors over the entire sequence of observations where Fj(x\u0304, y\u0304) = \u2211n t fj(yt\u22121, yt, x\u0304, t) and where t is the position in the sequence of length n, the probability of the sequence y\u0304 is then:\nP (y\u0304|x\u0304, \u03b8\u0304) = 1 Z(x\u0304, \u03b8\u0304) exp( \u2211 j \u03b8jFj(x\u0304, y\u0304)) (4)\nwhere Z(x\u0304, \u03b8\u0304) = \u2211 y\u0304\u2208nPY exp( \u2211 j \u03b8jFj(x\u0304, y\u0304)). The normalization constant Z is computationally intractable [11] but does not need to be computed when predicting the labels in the sequence given the weights of the feature functions or when evaluating a sequence against one model."}, {"heading": "IV. METHODOLOGY", "text": "We address the problem of sequence learning with a potentially infinite set of labels by learning several models. As in [3], we use the energy output of the output nodes, E(x, y), before squashing by the softmax function, as the score of the factors in the CRF. We leverage from the neural network to discover non-linear feature functions in the case of the multi-layer perceptrons (MLPs) and we compare and contrast different neural network architectures. The Viterbi algorithm [7], [5] guides the step-by-step predictions to maximize the choice of each label y with respect to the entire sequence. P (y\u0304|x\u0304, \u03b8\u0304) is then defined as follows:\nP (y\u0304|x\u0304, \u03b8\u0304) = arg maxt1...tn\n\u220fn t=1 P (yt|yt\u22121, x\u0304t, \u03b8\u0304)\nZ(x\u0304, \u03b8\u0304) (5)\nAlgorithm 1 describes the Viterbi evaluation of a sequence of observations x\u0304 delimited by START and STOP tags combined with an approximate probabilistic evaluation of the entire sequence given the model. P (y\u0304|x\u0304, \u03b8\u0304) can be approximated using a partition function, Z(x\u0304, \u03b8\u0304), that includes the maximum scores from the preceding step at each time step rather than the sum of scores of all possible sequences.\nWe compare and contrast the architectures of different neuroCRFs using the same methodology:\n1) Combination of two multilayer perceptrons (CRF-MLP) trained with backpropagation where one MLP learns the weights of the transition factors between labels and another MLP learns the weights of the observation factors. 2) Recurrent neural network [12] (CRF-RNN) trained with backpropagation where the activations of the hidden units at the previous time step are added to the inputs at the next time step in the sequence.\nAlgorithm 1 Viterbi algorithm for CRFs where forward and backtrack are functions as in the Viterbi algorithm and where alphas contains the information of all the possible outputs yt at each step t. input: model, x\u0304 //neural net and observation sequence output: y\u0304, P (y\u0304|x\u0304, \u03b8\u0304) //label sequence and probability viterbi_crfs (model,x\u0304)=\nt \u2190 0 \u03b1[t]\u2190 initialize (y0, x0) while t < length (x\u0304)\nt \u2190t + 1 \u03b1[t] \u2190 forward (x\u0304t, \u03b1[t\u2212 1])\nend yt \u2190 arg maxy \u03b1[t] y\u0304 \u2190 backtrack (yt,\u03b1) score \u2190 max\u03b1[t] P (y\u0304|x\u0304, \u03b8\u0304) \u2248 exp(score)\u2211\ny exp(\u03b1y [t])\nreturn y\u0304, score, P (y\u0304|x\u0304, \u03b8\u0304)\n3) Structured perceptron [5] (CRF-PRCPT) as described above II.\nThe different architectures compared are illustrated in Figs. 2, 3, and 4. Algorithm 2 describes the forward function of the Viterbi algorithm to compute P (y\u0304|x\u0304, \u03b8\u0304) in log space for CRF-MLP.\n4\nAlgorithm 2 Forward function of Viterbi algorithm for CRFMLP input: model, x\u0304, \u03b1[t\u2212 1] //model, observations at time t, and \u03b1[] //model consists of one MLP for observations, MLPobs, //and one MLP for edges, MLPedges. //\u03b1[t\u2212 1] is a list of tuples {to, from, score}maximizing //the score for each to label at the previous step t\u2212 1 output: \u03b1[t] forward (model,x\u0304, \u03b1[t\u2212 1])= obspreds \u2190 predict (x\u0304, MLPobs) edgepreds \u2190\u00d8 \u03b1[t]\u2190 \u00d8 foreach label \u2208 Ymodel, the set of labels for the model edgepreds \u2190edgepreds \u222a {predict (label, MLPedges)}\nforeach label \u2208 Ymodel to\u2032, score\u2032 \u2190\nargmax\u03b1[t\u22121](score + edgepreds[to]label + obspredslabel) \u03b1[t]\u2190 \u03b1[t] \u222a {label, to\u2032, score\u2032} return \u03b1[t]\nUsing stochastic gradient descent (SGD), training occurred when the label, as optimized for the overall sequence by the Viterbi algorithm, was incorrect. In addition, the weight updates were modulated with weight elimination regularization [13] for the MLPs:\nwij = wij \u2212 \u03b7xi(\u03b4j + 2\u03bb wij\n(1 + w2ij) 2\n)\nwhere wij is the weight on the connection between nodei and nodej , i and j denoting different contiguous layers, \u03b7 is the learning rate, xi is the activation at nodei, \u03b4j is the gradient at nodej as calculated by the backpropagation algorithm, and \u03bb is the regularization parameter.\nOur sequence classification metric is based on authentication biometrics using the false rejection rate (FRR) or false negatives and the false acceptance rate (FAR) or false positives leading to the identification of self vs. non-self. Several models are trained, each representing one type of sequence. Therefore, the conditional probability of a sequence given a model is not a calibrated probability because the training data is not assumed to represent the true distribution of all possible sequences and we do not know about other models (but we have examples of non-self). However, we can calibrate the score obtained evaluating sequences of self and non-self against the model with a threshold to report the equal error rate (EER), where the FRR equals the FAR, as follows. We map the scores\nobtained against testing examples of self to the positive class and scores obtained against testing examples of non-self to the negative class to obtain a linear model minimizing the sum of square errors separating the classes. We take the coefficient of regression for our threshold from which to compute the FRR and the FAR. Figure 5 illustrates how the threshold is determined from the scores of examples of self and nonself. We report results from the evaluation of this threshold on the validation set itself for a performance approximation. We report the r-square from the linear model to show the calibration of scores. The accuracy result is computed from the FRR and the FAR while the token accuracy is computed as the frequency of correct labels in the sequences. The F-score combines sequence classification accuracy and token accuracy results."}, {"heading": "V. EMPIRICAL EVALUATION", "text": "The MLPs used a fixed learning rate set at 0.5 and a regularization parameter \u03bb set at 0.001. All neural nets used 1000 SGD examples or less if convergence to zero-error occurred during training. The weights for all neural nets were initialized to small values drawn from a normal distribution with zero mean and standard deviation 0.00015. The number of hidden nodes for the MLP-based architectures, CRF-MLP and CRF-RNN, was set to ni+no4 as in [14], where ni is the number of inputs and no is the number of outputs. No attempt has been made to optimize the hyper-parameters of the different learners. The sigmoid function was the activation function for the nodes in the hidden layer and the derivative of the square loss function propagated the error at the output nodes.\nThe propagated loss in the structured perceptron was the difference between the predicted outcome and the actual outcome for each feature function relating inputs to outputs (0/1 loss). In addition, a mini-batch approach was used where the prediction error was averaged over 5 examples."}, {"heading": "A. OCR Dataset", "text": "The OCR dataset [15] contains 52152 16x8 raster images of letters composing 55 distinct words of length ranging from 3 to 14. The number of examples per word varies from 71 to 151. Table I describes this dataset characteristics. A model is\n5 Word Length #words #examples\n3 9 1283 5 4 568 6 6 768 7 5 695 8 6 750 9 8 1047 10 5 584 11 3 304 12 2 298 13 3 313 14 3 266\nTable I OCR DATASET CHARACTERISTICS\nFigure 6. Comparative results from neural-network based CRFs on the OCR dataset. Each data point is the average results for one word over 5 iterations.\nbuilt for each word and tested against other words of the same length. At each iteration, the data for each word is randomly partitioned into 2/3 training and 1/3 testing to evaluate the FRR and compared against a random sample of ~100 exemplars of different letters of the same length to evaluate the FAR. Table II and Fig. 6 illustrates the results averaged over 5 iterations. While there is no significant statistical difference in the accuracy results between CRF-MLP and CRF-RNN, there is a significant statistical difference (two-sided p-value < 0.05) for the accuracy results between CRF-MLP and CRF-PRCPT and also between CRF-RNN and CRF-PRCPT. However, CRF-PRCPT has a greater token accuracy value at a significant statistical difference from both CRF-MLP and CRF-RNN. There is also a greater token accuracy value at a significant statistical different between CRF-MLP and CRFRNN."}, {"heading": "B. Web Analytics", "text": "Another type of complex sequence can be found in our online activities. We extend previous work done in the context of Web browsing [9] to profile authentication from social media activities of Reddit users. Reddit is a public forum where anybody can create subreddits on any topics. We retrieved posts and comments using the Python Reddit API Wrapper (PRAW) from users associated to a seed user through at least one comment and from this pool of users selected at random 50 active users. There is a hard limit from Reddit to retrieve only the last 1000 posts and comments. The session pause delimiter (set to 30 minutes in Web browsing) was extended\nFigure 7. Frequencies of the most common subreddit by order visited for 3 users\nsequence Subreddit #unique Length Entropy #sessions Subreddits\n1 9.54 18580 1307 2 8.62 5305 703 3 7.90 2146 431 4 7.37 1155 293 5 6.80 655 194 6 6.49 428 155\n>6 - 1231 372\nTable III REDDIT DATASET SEQUENCE LENGTH CHARACTERISTICS FOR 50 USERS.\nto one hour due to the sparsity of posts and/or comments. In contrast to Web browsing, the data of Reddit activities is sparse as evidenced by the large number of singleton sessions (Table III). Empirical analysis shows that the frequencies of the most common subreddits for a particular user follow the temporal order in which they were accessed (Fig. 7). As stated in [16], commonality is not a good discriminator. The entropy of the subreddits, as a measure of commonality, decreases according to their position in the session. To capture discriminating sequences, we modeled sessions of Reddit activities as ngram sequences of length 4 and ignored shorter sequences. All users have sequences of various length up to length 6. Reddit user activities are modeled with CRFs as a sequence of posts/comments where the observations are the time-ofday, day-of-week, and subject header n-grams. A maximum of 100 most common n-grams per subreddit were extracted from the subject headers for computational efficiency. The labels are the subreddits of the posts/comments for a strongly supervised evaluation similar to the OCR dataset. Future work will evaluate the impact of a weakly-supervised approach with a subreddit concept hierarchy. Unlike the OCR dataset, the ngram sequences are not i.i.d. Each user dataset was temporally partitioned into 90% training and 10% testing.\nTable IV and Fig. 8 illustrates the results. There is a significant statistical difference (two-sided p-value < 0.05) in accuracy between the MLP-based architectures, CRF-MLP and CRF-RNN, and CRF-PRCPT but no statistical difference in accuracy between CRF-MLP and CRF-RNN. In addition, there is no statistical difference in token accuracy between CRF-RNN and CRF-PRCPT. We note that the correlation between labels is not as stable as in the OCR dataset which explains why CRF-MLP with an edge prediction network has\nlower token accuracy in this dataset."}, {"heading": "VI. CONCLUSION", "text": "In summary, we have argued that the discriminative capabilities of CRFs in the structured prediction task do not necessarily carry over to the sequence classification task. Toward that end, we have compared and contrasted different architectures of neuroCRFs in two different tasks, the OCR hand-writing recognition task and the Web analytics task, with the same methodology. We have shown that CRFs, as a structured prediction approach, can also be applied to sequence classification by training several models for different types of sequence with potentially different labels, and calibrating the scores of each model (before softmax squashing) with a threshold determined by a linear model. While the structured perceptron performs better overall at the structured prediction task, the discriminative power of MLP based architectures, CRF-MLP and CRF-RNN, carries over to the sequence classification task albeit with some degradation in the structured prediction task. We note that CRF-MLP does better than CRFRNN in the structured prediction task of the OCR hand-writing recognition task where the label transitions are consistent and modeled with an edge prediction network. Future work will further bridge the gap between sequence labeling and sequence classification as well as evaluate the impact of a weaklysupervised approach for user authentication in Web analytics with neural CRFs."}], "references": [{"title": "Transforming classifier scores into accurate multiclass probability estimates", "author": ["B. Zadrozny", "C. Elkan"], "venue": "Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 694\u2013699, ACM, 2002.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "A brief survey on sequence classification", "author": ["Z. Xing", "J. Pei", "E. Keogh"], "venue": "ACM SIGKDD Explorations Newsletter, vol. 12, no. 1, pp. 40\u201348, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Neural conditional random fields", "author": ["T. Do", "T. Arti"], "venue": "International Conference on Artificial Intelligence and Statistics, pp. 177\u2013184, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "CoRR, vol. abs/1308.0850, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["M. Collins"], "venue": "Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pp. 1\u20138, Association for Computational Linguistics, 2002.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Discriminatively trained markov model for sequence classification", "author": ["O. Yakhnenko", "A. Silvescu", "V. Honavar"], "venue": "Data Mining, Fifth IEEE International Conference on, pp. 8\u2013pp, IEEE, 2005.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "Proceedings of the IEEE, pp. 257\u2013 286, 1989.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1989}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "International Conference on Machine Learning ICML, 2001.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning temporal user profiles of web browsing behavior", "author": ["M. Abramson"], "venue": "6th ASE International Conference on Social Computing SocialCom, (Stanford, CA), May 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Machine learning: a probabilistic perspective", "author": ["K.P. Murphy"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "An introduction to conditional random fields", "author": ["C. Sutton", "A. McCallum"], "venue": "arXiv preprint arXiv:1011.4088, 2010.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1990}, {"title": "Learning users\u2019 interests by unobtrusively observing their normal behavior", "author": ["J. Goecks", "J. Shavlik"], "venue": "Proceedings of the 5th international conference on Intelligent user interfaces, pp. 129\u2013132, ACM, 2000.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "A comparison of approaches to on-line handwritten character recognition", "author": ["R.H. Kassel"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1995}, {"title": "Implicit authentication through learning user behavior", "author": ["E. Shi", "Y. Niu", "M. Jakobsson", "R. Chow"], "venue": "Information Security, pp. 99\u2013113, Springer, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "words, the score obtained by the CRF model representing a ranking of the sequence given the model has to correlate with the empirical class membership probability [1].", "startOffset": 163, "endOffset": 166}, {"referenceID": 1, "context": "The problem that sequence classification addresses is introduced in [2], namely sequence classification is defined as learning the function mapping a sequence s to a class label l from a set of labels L.", "startOffset": 68, "endOffset": 71}, {"referenceID": 2, "context": "Neural conditional random fields or neuroCRFs have been investigated in [3] using deep neural networks showing the influence of 2 layers vs.", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "Similar to HMMs, Long Short Term Memory (LSTM) recurrent neural networks (RNNs) [4] learn a sequence of labels from unsegmented data such as that found in handwriting or speech recognition.", "startOffset": 80, "endOffset": 83}, {"referenceID": 4, "context": "In [5], perceptrons were integrated as discriminative learners", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "A distinction is made between discriminative and generative k-Markov models in the sequence classification of simple symbolic sequences [6].", "startOffset": 136, "endOffset": 139}, {"referenceID": 6, "context": "CRFs are a supervised method for structured prediction similar to HMMs [7] while relaxing the independence assumption of the observations and the Markov assumption [8].", "startOffset": 71, "endOffset": 74}, {"referenceID": 7, "context": "CRFs are a supervised method for structured prediction similar to HMMs [7] while relaxing the independence assumption of the observations and the Markov assumption [8].", "startOffset": 164, "endOffset": 167}, {"referenceID": 1, "context": "CRFs address the strong classification problem [2] of", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "an auxiliary classifier and strongly-supervised CRFs where the labels are known without ambiguity [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 9, "context": "spatial neighborhood properties of the graph as expected [10].", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": "There are two types of feature functions in representing a sequence [11]: (1) edge functions between two labels and (2) observation functions relating x and y.", "startOffset": 68, "endOffset": 72}, {"referenceID": 10, "context": "The normalization constant Z is computationally intractable [11] but does not need to be computed when predicting the labels in the", "startOffset": 60, "endOffset": 64}, {"referenceID": 2, "context": "As in [3], we use the energy output of the output nodes, E(x, y), before squashing by the softmax function, as the score of", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "The Viterbi algorithm [7], [5] guides the step-by-step predictions to maximize the choice of each label y with respect to the entire sequence.", "startOffset": 22, "endOffset": 25}, {"referenceID": 4, "context": "The Viterbi algorithm [7], [5] guides the step-by-step predictions to maximize the choice of each label y with respect to the entire sequence.", "startOffset": 27, "endOffset": 30}, {"referenceID": 11, "context": "2) Recurrent neural network [12] (CRF-RNN) trained with backpropagation where the activations of the hidden units at the previous time step are added to the inputs at the next time step in the sequence.", "startOffset": 28, "endOffset": 32}, {"referenceID": 4, "context": "3) Structured perceptron [5] (CRF-PRCPT) as described above II.", "startOffset": 25, "endOffset": 28}, {"referenceID": 12, "context": "The number of hidden nodes for the MLP-based architectures, CRF-MLP and CRF-RNN, was set to ni+no 4 as in [14], where ni is the number of inputs and no is the number of outputs.", "startOffset": 106, "endOffset": 110}, {"referenceID": 13, "context": "The OCR dataset [15] contains 52152 16x8 raster images of letters composing 55 distinct words of length ranging from 3 to 14.", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": "We extend previous work done in the context of Web browsing [9] to profile authentication from social media activities of Reddit users.", "startOffset": 60, "endOffset": 63}, {"referenceID": 14, "context": "As stated in [16], commonality is not a good discriminator.", "startOffset": 13, "endOffset": 17}], "year": 2016, "abstractText": "The proliferation of sensor devices monitoring human activity generates voluminous amount of temporal sequences needing to be interpreted and categorized. Moreover, complex behavior detection requires the personalization of multi-sensor fusion algorithms. Conditional random fields (CRFs) are commonly used in structured prediction tasks such as part-of-speech tagging in natural language processing. Conditional probabilities guide the choice of each tag/label in the sequence conflating the structured prediction task with the sequence classification task where different models provide different categorization of the same sequence. The claim of this paper is that CRF models also provide discriminative models to distinguish between types of sequence regardless of the accuracy of the labels obtained if we calibrate the class membership estimate of the sequence. We introduce and compare different neural network based linearchain CRFs and we present experiments on two complex sequence classification and structured prediction tasks to support this claim.", "creator": "LaTeX with hyperref package"}}}