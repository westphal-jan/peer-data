{"id": "1602.02350", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2016", "title": "Solving Ridge Regression using Sketched Preconditioned SVRG", "abstract": "We develop a novel preconditioning method for ridge regression, based on recent linear sketching methods. By equipping Stochastic Variance Reduced Gradient (SVRG) with this preconditioning process, we obtain a significant speed-up relative to fast stochastic methods such as SVRG, SDCA and SAG. This method provides a significant reduction in error and uncertainty in uncertainty on the validity of SVRG and SAG models. We expect to have SVRG support in both HMD and ECS. Furthermore, we expect to gain an accurate resolution of the observed slope in the posterior-layer ridge regression model by using the Gaussian function. This method reduces the error rate to the upper-layer, with a slight increase of only 0.1% in SVRG.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Sun, 7 Feb 2016 08:37:18 GMT  (63kb)", "https://arxiv.org/abs/1602.02350v1", null], ["v2", "Thu, 26 May 2016 07:42:58 GMT  (70kb)", "http://arxiv.org/abs/1602.02350v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alon gonen", "francesco orabona", "shai shalev-shwartz"], "accepted": true, "id": "1602.02350"}, "pdf": {"name": "1602.02350.pdf", "metadata": {"source": "CRF", "title": "Solving Ridge Regression using Sketched Preconditioned SVRG", "authors": ["Alon Gonen", "Francesco Orabona", "Shai Shalev-Shwartz"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n02 35\n0v 2\n[ cs\n.L G\n] 2"}, {"heading": "1 Introduction", "text": "Consider the ridge regression problem:\nmin w\u2208Rd\n{\nL(w) = 1\nn\nn\u2211\ni=1\n1 2 (w\u22a4xi \u2212 yi)2 + \u03bb 2 \u2016w\u20162\n}\n, (1)\nwhere \u03bb > 0 is a regularization parameter, xi \u2208 Rd and yi \u2208 R for i = 1, \u00b7 \u00b7 \u00b7 , n the training data. We focus on the large scale regime, where both n and d are large. In this setting, stochastic iterative methods such as SDCA [15], SVRG [5], and SAG [11] have become a standard choice for minimizing the objective L. Specifically, the overall complexity of a recent improved variant of SVRG due to [21] depends on the average condition number, which is defined as follows. Denote the empirical correlation matrix and its eigenvalue decomposition by\nC := 1\nn\nn\u2211\ni=1\nxix \u22a4 i =\nd\u2211\ni=1\n\u03bbiuiu \u22a4 i . (2)\nThe average condition number of C + \u03bbI is defined as the ratio between the trace of the Hessian of L and its minimal eigenvalue:\n\u03ba\u0302 := \u03ba\u0302(C + \u03bbI) = tr(C + \u03bbI)\n\u03bbd(C + \u03bbI) =\nd\u2211\ni=1\n\u03bbi + \u03bb \u03bbd + \u03bb . (3)\n\u2217School of Computer Science, The Hebrew University, Jerusalem, Israel \u2020Yahoo Labs, New York, NY, USA \u2021School of Computer Science, The Hebrew University, Jerusalem, Israel\nThe mentioned variant of SVRG finds an \u01eb-approximate minimizer of L in time O\u0303((\u03ba\u0302+ n)d log(1/\u01eb)). Namely, the output of the algorithm, denoted w\u0302, satisfies E[L(w\u0302)] \u2212 L(w\u22c6) \u2264 \u01eb, where the expectation is over the randomness of the algorithm. For an accelerated version of the algorithm, we can replace \u03ba\u0302 by \u221a n\u03ba\u0302 [16, 7].\nThe regularization parameter, \u03bb, increases the smallest eigenvalue of C + \u03bbI to be at least \u03bb, thus improves the condition number and makes the optimization problem easier. However, to control the under/over fitting tradeoff, \u03bb has to decrease as n increases [14]. Moreover, in many machine learning applications \u03bbd approaches zero and it is usually smaller than the value of \u03bb. Overall, this yields a large condition number in most of the interesting cases.\nA well-known approach for reducing the average condition number is preconditioning. Concretely, for a (symmetric) positive definite (pd) matrix P \u2208 Rd\u00d7d, we define the preconditioned optimization problem as\nmin w\u0303\u2208Rd\nL\u0303(w\u0303) := L(P\u22121/2w\u0303) . (4)\nNote that w\u0303 is an \u01eb-approximate minimizer of L\u0303 if and only if w = P\u22121/2w\u0303 forms an \u01eb-approximate minimizer of L. Hence, we can minimize Equation (4) rather than Equation (1). As we shall see, the structure of the objective allows us to apply the preconditioning directly to the data (as a preprocessing step) and consequently rewrite the preconditioned objective as a ridge regression problem with respect to the preconditioned data (see Section 5.1). For a suitable choice of a matrix P , the average condition number is significantly reduced. Precisely, as will be apparent from the analysis, the pd matrix that minimizes the average condition number is P = C + \u03bbI , and the corresponding average condition number is d. However, we note that such preconditioning process would require both the computation of P\u22121/2 and the computation of P\u22121/2xi for each i \u2208 [n]. By first order conditions, computing (C + \u03bbI)\u22121/2 is equivalent to solving the original problem in Equation (1), rendering this \u201coptimal\u201d preconditioner useless.\nYet, the optimal preconditioner might not needed in many cases. In fact, a common empirical observation (see Section 6) is that (high-dimensional) machine learning problems tend to have few dominant features, while the other coordinates are strongly correlated with the stronger features. As a result, the spectrum of the correlation matrix decays very fast. Hence, it is natural to expect to gain a lot from devising preconditioning methods that focus on the stronger directions of the data.\nOur contributions are as follows. We develop a relatively cheap preconditioning method that, coupled with SVRG, assures to speed-up the convergence in practical applications while having a computational cost comparable to SVRG alone. In order to approximately extract the stronger directions while incurring a low computational cost, we rely on a variant of the Block Lanczos method due to [8] in order to compute an approximated truncated SVD (Singular Value Decomposition) of the correlation matrix C. Finally, by equipping SVRG with this preconditioner, we obtain our main result."}, {"heading": "2 Main Result", "text": "Theorem 1. Let k \u2208 [d] be a given parameter and assume that the regularization parameter, \u03bb, is larger than \u03bbd. Our preconditioning process runs in time O(ndk log(n)). By equipping the SVRG of [21] with this preconditioner, we find an \u01eb-approximate minimizer for Equation (1) (with probability at least 9/10) in additional runtime of\nO((\u03ba\u0303 + n + d)d log(1/\u01eb)), where \u03ba\u0303 = k\u03bbk+\n\u2211 i>k\n\u03bbi \u03bb or \u03ba\u0303 =\n( n(k\u03bbk+ \u2211 i>k \u03bbi)\n\u03bb\n)1/2\nif\nwe use accelerated SVRG.\nWhen the runtimes of both the (accelerated) SVRG and our preconditioned (accelerated) SVRG are controlled by the average condition number (and both runtimes dominate ndk), then ignoring logarithmic dependencies, we obtain a speed-up of order\nratio =\n\u2211d i=1 \u03bbi\nk \u03bbk + \u2211 i>k \u03bbi =\n\u2211k i=1 \u03bbi + \u2211 i>k \u03bbi\nk \u03bbk + \u2211 i>k \u03bbi . (5)\n(or \u221a \u2211d\ni=1 \u03bbi/(\u03bbkk + \u2211 i>k \u03bbi) if acceleration is used) over SVRG. If the spectrum\ndecays fast then k \u03bbk \u226a \u2211k i=1 \u03bbi and \u2211\ni>k \u03bbi \u226a k \u03bbk. In this case, the ratio will be large. Indeed, as we show in the experimental section, this ratio is often huge for relatively small k."}, {"heading": "2.1 Main challenges and perspective", "text": "While the idea of developing a preconditioner that focuses on the stronger directions of the data matrix sounds plausible, there are several difficulties that have to be solved.\n\u2022 First, since a preconditioner must correspond to an invertible transformation, it is not clear how to form a preconditioner based on a low rank approximation and, in particular, how should we treat the non-leading components.\n\u2022 One of the main technical challenges in our work is to translate the approximation guarantees of the Lanczos method into a guarantee on the resulted average condition number. The standard measures of success for low-rank approximation are based on either Frobenius norm or spectral norm errors. As will be apparent from the analysis (see Section 5.4), such bounds do not suffice for our needs. Our analysis relies on stronger per vector error guarantees Equation (9) due to [8].\nIt should be emphasized that while we use a variant of SVRG due to [21], we could equally use a variant of SDCA [13] or develop such a variant for SAG or SAGA. Furthermore, while we focus on the quadratic case, we believe that our ideas can be lifted to more general setting. For example, when applied to self-concordant functions, each step of Newton\u2019s method requires the minimization of a quadratic objective. Therefore, it is natural to ask if we can benefit from applying our method for approximating the Newton step."}, {"heading": "2.2 Bias-complexity tradeoff", "text": "As we mentioned above, \u03bb controls a tradeoff between underfitting and overfitting. In this view, we can interpret our result as follows. Assuming for simplicity that n \u2265 d and ignoring logarithmic dependencies, we note that if\n\u03bb = k\u03bbk +\n\u2211\ni>k \u03bbi\nnk , (6)\nthen the runtime of our preconditioned SVRG is O\u0303(ndk). For comparison, the runtime of (unconditioned) SVRG is O\u0303(ndk) if\n\u03bb = \u2211d i=1 \u03bbi nk . (7)\nThe ratio between the RHS of Equation (7) and Equation (6) is the ratio given in Equation (5). Hence, for a given \u201cruntime budget\u201d of order O\u0303(ndk), we can set the regularization parameter of the preconditioned SVRG to be smaller by this ratio. Similar interpretation holds for the accelerated versions."}, {"heading": "3 Related Work", "text": "Existing algorithms and their complexities: Since minimizing Equation (1) is equivalent to solving the system (C+\u03bbI)w = 1n \u2211n i=1 yixi, standard numerical linear algebra solvers such as Gaussian elimination can be used to solve the problem in time O(nd2).\nIterative deterministic methods, such as Gradient Descent (GD), finds an \u01eb-approximate minimizer in time nd\u03ba log(1/\u01eb), where \u03ba = \u03bb1(C+\u03bbI)\u03bbd(C+\u03bbI) is the condition number of C + \u03bbI (see Theorem 2.1.15 in [10]). The Kaczmarz algorithm [6] has an identical complexity. Both the Conjugate Gradient (CG) method [4] and the Accelerated Gradient Descent (AGD) algorithm of [9] enjoy a better runtime of nd \u221a \u03ba log(1/\u01eb). In fact, CG has a more delicate analysis (see Corollary 16.7 in [18]): If all but c \u2208 [d] eigenvalues of C+\u03bbI are contained in a range [a, b], then the runtime of CG is at most nd(c+ \u221a\nb/a log(1/\u01eb)). In particular, CG\u2019s runtime is at most O(nd2). Furthermore, following the interpretation of our main result in Section 2.2, we note that for a \u201cruntime budget\u201d of O\u0303(ndk), we can set the regularization parameter of CG to be of order \u03bbk/k 2 (which is usually much greater than the RHS of Equation (6)).\nLinear Sketching: Several recently developed methods in numerical linear algebra are based on the so-called sketch-and-solve approach, which essentially suggests that given a matrixA, we first replace it with a smaller random matrix AS, and then perform the computation on AS [20, 2, 12]. For example, it is known that if the entries of S are i.i.d. standard normal variables and S has p = \u2126(k/\u01eb) columns, then with high probability, the column space of AS contains a (1 + \u01eb) rank-k approximation to A with respect to the Frobenius norm. This immediately yields a fast PCA algorithm (see Section 4.1 in [20]).\nWhile the above sketch-and-solve approach sounds promising for this purpose, our analysis reveals that controlling the Frobenius norm error does not suffice for our needs. We need spectral norm bounds, which are known to be more challenging [19]. Furthermore, as mentioned above, the success of our conditioning method heavily depends on the stronger per vector error guarantees Equation (9) obtained by [8] which are not obtained by simpler linear sketching methods.\nSketched preconditioning: Recently, subspace embedding methods were used to develop cheap preconditioners for linear regression with respect to the squared loss [20]. Precisely, [2] considered the case \u03bb = 0 (i.e, standard least-squares) and developed a preconditioning method that reduces the average condition number to a constant. Thereafter, they suggest applying a basic solver such as CG. The overall running time is dominated by the preconditioning process which runs in time O\u0303(d3 + nd). Hence, a significant improvement over standard solvers is obtained if n \u226b d.\nThe main shortcoming of this method is that it does not scale well to large dimensions. Indeed, when d is very large, the overhead resulted from the preconditioning process can not be afforded.\nEfficient preconditioning based on random sampling: While we focus on reducing the dependence on the dimensionality of the data, other work investigated the gain from using only a random subset of the data points to form the conditioner [22]. The theoretical gain of this approach has been established under coherence assumptions [22]."}, {"heading": "4 Preliminaries", "text": ""}, {"heading": "4.1 Additional notation and definitions", "text": "Any matrix B \u2208 Rd\u00d7n of rank r can be written in (thin) SVD form as B = U\u03a3V \u22a4 = \u2211r\ni=1 \u03c3i(B)uiv \u22a4 i . The singular values are ordered in descending order. The spectral norm of B is defined by \u2016B\u2016 = \u03c31(B). The spectral norm is submultiplicative, i.e., \u2016AB\u2016 \u2264 \u2016A\u2016\u2016B\u2016 for all A and B. Furthermore, the spectral norm is unitary invariant, i.e., for all A and U such that the columns of U are orthonormal, \u2016UA\u2016 = \u2016A\u2016. For any k \u2208 [r], it is well known that the truncated SVD of B, Bk := Uk\u03a3kVk = \u2211k\ni=1 \u03c3i(B)uiv \u22a4 i , is the best rank-k approximation of B w.r.t. the spectral norm [17]. A twice continuously differentiable function f : Rd \u2192 R is said to be \u03b2-smooth if \u2016\u22072f(w)\u2016 \u2264 \u03b2 for all w, where \u22072f(w) is the Hessian of f at w. f is said to be \u03b1-strongly convex if \u03bbd(\u22072f(w)) \u2265 \u03b1 for all w. If g is convex and f is \u03b1-strongly convex, then f + g is \u03b1-strongly convex."}, {"heading": "4.2 Stochastic Variance Reduced Gradient (SVRG)", "text": "We consider a variant of the Stochastic Variance Reduced Gradient (SVRG) algorithm of [5] due to [21]. The algorithm is an epoch-based iterative method for minimizing an average, F (w) = 1N \u2211N i=1 fi(w), of smooth functions. It is assumed that each fi :\nAlgorithm 1 SVRG citexiao2014proximal\n1: Input: Functions f1, . . . , fn, \u03b21, . . . , \u03b2n 2: Parameters: w\u03040 \u2208 Rd, m, \u03b7, S \u2208 N 3: for s = 1, 2, . . . , S do 4: w\u0304 = w\u0304s\u22121 5: v\u0304 = \u2207F (w\u0304) 6: w0 = w\u0304 7: for t = 1, . . . ,m do # New epoch 8: Pick it \u2208 [N ] with probability qit = \u03b2it/ \u2211 \u03b2j\n9: vt = (\u2207fit(wt\u22121)\u2212\u2207fit(w\u0304))/qit + v\u0304 10: wt = wt\u22121 \u2212 \u03b7vt 11: end for 12: w\u0304s = 1 m \u2211m t=1 wt 13: end for 14: Output: the vector w\u0304S\nR d \u2192 R is convex and \u03b2i-smooth. The entire function F is assumed to be \u03b1-strongly convex. The algorithm is detailed in Algorithm 1. Its convergence rate depends on the averaged smoothness of the individual functions and the average condition number of F , defined as\n\u03b2\u0302 = 1\nN\nN\u2211\ni=1\n\u03b2i ; \u03ba\u0302F = \u03b2\u0302\n\u03b1 . (8)\nTheorem 2. [21] Fix \u01eb > 0. Running SVRG (Algorithm 1) with anyw0, S \u2265 log((F (w0)\u2212 minw\u2208Rd F (w))/\u01eb), m = \u2308\u03ba\u0302F \u2309, and \u03b7 = 0.1/\u03b2\u0302 yields an \u01eb-approximate minimizer of F . Furthermore, assuming that each single gradient \u2207fi(w) can be computed in time O(d), the overall runtime is O((\u03ba\u0302F +N)d log(\u01eb0/\u01eb)).\nIn the original definition of SVRG [5], the indices it are chosen uniformly at random from [n], rather than proportional to \u03b2i. As a result, the convergence rate depends on the maximal smoothness, max{\u03b2i}, rather than the average, \u03b2\u0302. It will be apparent from our analysis (see Theorem 4) that in our case, max{\u03b2i} is proportional to the maximum norm of any preconditioned xi. Since we rely on the improved variant of [21], our bound depends on the average of the \u03b2i\u2019s, which scale with the average norm of the preconditioned xi\u2019s. To simplify the presentation, in the sequel we refer to Algorithm 1 as SVRG."}, {"heading": "4.3 Randomized Block Lanczos", "text": "A randomized variant of the Block Lanczos method due to [8] is detailed1 in Algorithm 2. Note that the matrix U\u0303k\u03a3\u0303kV\u0303 \u22a4k forms an SVD of the matrix A\u0303k := Q(Q \u22a4A)k = U\u0303kU\u0303 \u22a4 k A.\n1More precisely, Algorithm 2 in [8] returns the projection matrix U\u0303kU\u0303\u22a4k , while we also compute the\nSVD of U\u0303kU\u0303\u22a4k A. The additional runtime is negligible.\nAlgorithm 2 Block Lanczos method [8]\n1: Input: A \u2208 Rd\u00d7n, k \u2264 d, \u01eb\u2032 \u2208 (0, 1) 2: q = \u0398 ( log(n)\u221a\n\u01eb\n)\n, p = qk, \u03a0 \u223c N (0, 1)n\u00d7k\n3: Compute K = [A\u03a0, (AA\u22a4)A\u03a0, . . . , (AA\u22a4)q\u22121A\u03a0] 4: Orthonormalize K\u2019s columns to obtain Q \u2208 Rd\u00d7qk 5: Compute the truncated SVD (Q\u22a4A)k = W\u0303k\u03a3\u0303kV\u0303 \u22a4k 6: Compute U\u0303k = QW\u0303k 7: Output: the matrices U\u0303k, \u03a3\u0303k, V\u0303k\nTheorem 3. [8] Consider the run of Algorithm 2 and denote A\u0303k = U\u0303k\u03a3\u0303kV\u0303k =\u2211k i=1 \u03c3\u0303iu\u0303iv\u0303 \u22a4 i . Denote the SVD of A by A = \u2211d i=1 \u03c3iviu \u22a4 i . The following bounds hold with probability at least 9/10:\n\u2016A\u2212 A\u0303k\u2016 \u2264 (1 + \u01eb\u2032)\u2016A\u2212Ak\u2016 \u2264 (1 + \u01eb\u2032)\u03c3k\n\u2200i \u2208 [k], |z\u22a4i AA\u22a4zi \u2212 u\u22a4i AA\u22a4ui| = |\u03c3\u03032i \u2212 \u03c32i | \u2264 \u01eb\u2032\u03c32k+1 . (9)\nThe runtime of the algorithm is O (\nndk log(n)\u221a \u01eb\u2032 + k 2(n+d) \u01eb\u2032\n)\n."}, {"heading": "5 Sketched Conditioned SVRG", "text": "In this section we develop our sketched conditioning method. By analyzing the properties of this conditioner and combining it with SVRG, we will conclude Theorem 1.\nRecall that we aim at devising cheaper preconditioners that lead to a significant reduction of the condition number. Specifically, given a parameter k \u2208 [d], we will consider only preconditioners P\u22121/2 for which both the computation of P\u22121/2 itself and the computation of the set {P\u22121/2xi, . . . , P\u22121/2xn} can be carried out in time O\u0303(ndk). We will soon elaborate more on the considerations when choosing the preconditioner, but first we would like to address some important implementation issues."}, {"heading": "5.1 Preconditioned regularization", "text": "In order to implement the preconditioning scheme suggested above, we should be able to find a simple form for the function L\u0303. In particular, since we would like to use SVRG, we should write L\u0303 as an average of n components whose gradients can be easily computed. Denote by x\u0303i = P\u22121/2xi for all i \u2208 [n]. Since for every i \u2208 [n], ((P\u22121/2w)\u22a4xi \u2212 yi)2 = (w\u22a4x\u0303i \u2212 yi)2, it seems natural to write L\u0303(w) = L(P\u22121/2w) as follows:\nL\u0303(w) = 1\nn\nn\u2211\ni=1\n1 2 (w\u22a4x\u0303i \u2212 yi)2 \ufe38 \ufe37\ufe37 \ufe38\n=:\u2113\u0303i\n+ \u03bb\n2 \u2016P\u22121/2w\u20162 .\nAssume momentarily that \u03bb = 0. Note that the gradient of \u2113\u0303i at any point w is given by \u2207\u2113\u0303i(wt) = (w\u22a4x\u0303i \u2212 yi)x\u0303i. Hence, by computing all the x\u0303i\u2019s in advance, we are able to apply SVRG directly to the preconditioned function and computing the stochastic gradients in time O(d).\nWhen \u03bb > 0, the computation of the gradient at some point w involves the computation of P\u22121w. We would like to avoid this overhead. To this end, we decompose the regularization function as follows. Denote the standard basis of Rd by e1, . . . , ed. Note that the function L can be rewritten as follows:\nL(w) = 1\nn+ d\nn+d\u2211\ni=1\n\u2113i(w) ,\nwhere \u2113i(w) = n+dn 1 2 (w \u22a4xi\u2212yi)2 for i = 1, . . . , n and \u2113n+i(w) = \u03bb(n+d)12 (w\u22a4ei)2 for i = 1, . . . , d. Finally, denoting bi = P\u22121/2ei for all i, we can rewrite the preconditioned function L\u0303 as follows:\nL\u0303(w) = 1\nn+ d\nn+d\u2211\ni=1\n\u2113\u0303i(w) ,\nwhere \u2113\u0303i(w) = n+dn 1 2 (w \u22a4x\u0303i\u2212yi)2 for i = 1, . . . , n and \u2113\u0303n+i(w) = \u03bb(n+d)12 (w\u22a4bi)2 for i = 1, . . . , d. By computing the x\u0303i\u2019s and the bi\u2019s in advance, we are able to apply SVRG while computing stochastic gradients in time O(d)."}, {"heading": "5.2 The effect of conditioning", "text": "We are now in position to address the following fundamental question: How does the choice of the preconditioner, P\u22121/2, affects the resulted average condition number of the function L\u0303 (8)? The following lemma upper bounds \u03ba\u0302L\u0303 by the average condition number of the matrix P\u22121/2(C +\u03bbI)P\u22121/2, which we denote by \u03ba\u0303 (when the identity of the matrix P is understood).\nTheorem 4. Let P\u22121/2 be a preconditioner. Then, the average condition number of L\u0303 is upper bounded by\n\u03ba\u0302L\u0303 \u2264 \u03ba\u0303 = tr(P\u22121/2(C + \u03bbI)P\u22121/2)\n\u03bbd(P\u22121/2(C + \u03bbI)P\u22121/2) .\nThe proof is in the appendix. Note that an optimal bound of O(d) is attained by the whitening matrix P\u22121/2 = (C + \u03bbI)\u22121/2."}, {"heading": "5.3 Exact sketched conditioning", "text": "Our sketched preconditioner is based on a random approximation of the best rankk approximation of the data matrix. It will be instructive to consider first a preconditioner that is based on an exact rank-k approximation of the data matrix. Let X \u2208 Rd\u00d7n be the matrix whose i-th columns is xi and let X\u0304 = n\u22121/2X . Denote by\nX\u0304 = \u2211rank(X\u0304)\ni=1 \u03c3iuiv \u22a4 i = U\u03a3V \u22a4 the SVD of X\u0304 and recall that X\u0304k = \u2211k i=1 \u03c3iuiv \u22a4 i\nis the best k-rank approximation of X\u0304 . Note that X\u0304X\u0304\u22a4 = C and therefore \u03c32i = \u03bbi(C) = \u03bbi. Furthermore, the left singular vectors of X\u0304 , u1, . . . , uk, coincide with the k leading eigenvectors of the matrix C. Consider the preconditioner,\nP\u22121/2 =\nk\u2211\ni=1\nuiu \u22a4 i\u221a\n\u03bbi + \u03bb + I \u2212\u2211ki=1 uiu\u22a4i\u221a \u03bbk + \u03bb ,\nwhere uk+1, . . . , ud are obtained from a completion of u1, . . . , uk to an orthonormal basis.\nLemma 1. Let k \u2208 [d] be a parameter and assume that the regularization parameter, \u03bb, is larger than \u03bbd. Using the exact sketched preconditioner, we obtain\n\u03ba\u0302L\u0303 \u2264 k\u03bbk +\n\u2211\ni>k \u03bbi\n\u03bb + d . (10)\nProof. A simple calculation shows that for i = 1, . . . , k,\n\u03bbi(P \u22121/2(C + \u03bbI)P\u22121/2) =\n\u03bbi + \u03bb \u03bbi + \u03bb = 1 .\nSimilarly, for i = k + 1, . . . , d,\n\u03bbi(P \u22121/2(C + \u03bbI)P\u22121/2) =\n\u03bbi + \u03bb \u03bbk + \u03bb .\nFinally,\n\u03bbd(P \u22121/2(C + \u03bbI)P\u22121/2) \u2265 \u03bb\n\u03bbk + \u03bb .\nCombining the above with Theorem 4, we obtain that\n\u03ba\u0302L\u0303 \u2264 tr(P\u22121/2(C + \u03bbI)P\u22121/2)\n\u03bbd(P\u22121/2(C + \u03bbI)P\u22121/2)\n\u2264 k\u03bbk + \u03bb \u03bb +\nd\u2211\ni=k+1\n\u03bbi + \u03bb\n\u03bb\n= k\u03bbk +\n\u2211\ni>k \u03bbi\n\u03bb + d ."}, {"heading": "5.4 Sketched conditioning", "text": "An exact computation of the SVD of the matrix X\u0304 takes O(nd2). Instead, we will use the Block Lanczos method in order to approximate the truncated SVD of X\u0304 . Specifically, given a parameter k \u2208 [d], we invoke the Block Lanczos method with the parameters X\u0304, k and \u01eb\u2032 = 1/2. Recall that the output has the form X\u0303k = U\u0303k\u03a3\u0303kV\u0303 \u22a4k = \u2211k\ni=1 \u03c3\u0303iu\u0303iv\u0303 \u22a4 i . Analogously to the exact sketched preconditioner, we define our sketched\npreconditioner by\nP\u22121/2 =\nk\u2211\ni=1\nu\u0303iu\u0303 \u22a4 i\n\u221a \u03c3\u03032i + \u03bb +\nI \u2212\u2211ki=1 u\u0303iu\u0303\u22a4i \u221a\n\u03c3\u03032k + \u03bb . (11)\nTheorem 5. Let k \u2208 [d] be a parameter and assume that the regularization parameter, \u03bb, is larger that \u03bbd. Using the sketched preconditioner defined in Equation (11), up to a multiplicative constant, we obtain the bound Equation (10) on the average condition number with probability at least 9/10.\nThe rest of this section is devoted to the proof of Theorem 5. We follow along the lines of the proof of Lemma 1. Up to a multiplicative constant, we derive the same upper and lower bounds on the eigenvalues of P\u22121/2(C + \u03bbI)P\u22121/2.\nFrom now on, we assume that the bounds in Theorem 3 (where \u01eb\u2032 = 1/2) hold. This assumption will be valid with probability of at least 9/10. We next introduce some notation. We can rewrite P\u22121/2 = U\u0303(\u03a3\u03032 + \u03bbI)\u22121/2U\u0303\u22a4 where \u03a3\u0303 is a diagonal d \u00d7 d with \u03a3\u0303i,i = \u03c3\u0303i if i \u2264 k and \u03a3\u0303i = \u03c3\u0303k if i > k. and the columns of U\u0303 are a completion of u\u03031, . . . , u\u0303k to an orthonormal basis. Recall that the SVD of X\u0304 is denoted by X\u0304 =\n\u2211d i=1 \u03c3iuiv \u22a4 i = U\u03a3V \u22a4.\nLemma 2. (Upper bound on the leading eigenvalue) We have\n\u03bb1(P \u22121/2(C + \u03bbI)P\u22121/2) \u2264 17 .\nProof. Since \u03bb1(P\u22121/2(C+\u03bbI)P\u22121/2) = \u2016P\u22121/2(C+\u03bbI)P\u22121/2\u2016 = \u2016P\u22121/2CP\u22121/2+ \u03bbP\u22121\u2016, using the triangle inequality we have that\n\u03bb1(P \u22121/2(C + \u03bbI)P\u22121/2) \u2264 \u2016P\u22121/2CP\u22121/2\u2016+ \u03bb\u2016P\u22121\u2016 .\nBy the definition of P we have that \u2016P\u22121\u2016 = 1 \u03c3\u03032 k +\u03bb and therefore the second summand on the right hand side of the above is at most \u03bb \u03c3\u03032 k +\u03bb\n\u2264 1. As to the first summand, recall that C = X\u0304X\u0304\u22a4 and therefore \u2016P\u22121/2CP\u22121/2\u2016 = \u2016X\u0304\u22a4P\u22121/2\u20162. We will show that \u2016X\u0304\u22a4P\u22121/2\u2016 \u2264 4 which will imply that \u2016P\u22121/2CP\u22121/2\u2016 \u2264 16. To do so, we first apply the triangle inequality,\n\u2016X\u0304\u22a4P\u22121/2\u2016 = \u2016(X\u0303k + (X\u0304 \u2212 X\u0303k))\u22a4P\u22121/2\u2016 \u2264 \u2016X\u0303\u22a4k P\u22121/2\u2016+ \u2016(X\u0304 \u2212 X\u0303k)\u22a4P\u22121/2\u2016 .\nLet us consider one term at the time. Recall that X\u0303k = U\u0303k\u03a3\u0303kV\u0303 \u22a4k . Since U\u0303 \u22a4 k U\u0303 \u2208 Rk,d is a diagonal matrix with ones on the diagonal, and since the spectral norm is invariant to multiplication by unitary matrices, we obtain that\n\u2016X\u0303\u22a4k P\u22121/2\u2016 = \u2016V\u0303k\u03a3\u0303kU\u0303\u22a4k U\u0303(\u03a3\u03032 + \u03bbI)\u22121/2U\u0303\u22a4\u2016 = \u2016\u03a3\u0303kU\u0303\u22a4k U\u0303(\u03a3\u03032 + \u03bbI)\u22121/2\u2016\n= max i\u2208[k]\n\u03c3\u0303i \u221a\n\u03c3\u03032i + \u03bb \u2264 max i\u2208[k]\n\u03c3\u0303i\n\u03c3\u0303i + \u221a \u03bb \u2264 1 .\nNext, by the submutiplicativity of the spectral norm,\n\u2016(X\u0304 \u2212 X\u0303k)\u22a4P\u22121/2\u2016 \u2264 \u2016X\u0304 \u2212 X\u0303k\u2016 \u00b7 \u2016P\u22121/2\u2016 .\nTheorem 3 implies that \u2016X\u0304 \u2212 X\u0303k\u2016 \u2264 32\u03c3k and\n\u2016P\u22121/2\u2016 = 1\u221a \u03c3\u03032k + \u03bb \u2264 1\u221a \u03c3\u03032k \u2264 1\u221a \u03c32k \u2212 (1/2)\u03c32k+1\n\u2264 1 \u03c3k \u221a 1 2 =\n\u221a 2 \u03c3k < 2 \u03c3k .\nHence, \u2016X\u0304 \u2212 X\u0303k\u2016 \u00b7 \u2016P\u22121/2\u2016 \u2264 3. Combining all of the above bounds concludes our proof.\nLemma 3. (Refined upper bound on the last d \u2212 k eigenvalues) For any i \u2208 {k + 1, . . . , d},\n\u03bbi\n( P\u22121/2(C + \u03bbI)P\u22121/2 ) \u2264 2(\u03bbi + \u03bb) \u03bbk + \u03bb .\nProof. Using the Courant minimax principle [1], we obtain the following bound for all i \u2208 {k + 1, . . . , d}:\n\u03bbi\n( P\u22121/2(C + \u03bbI)P\u22121/2 )\n= max M\u2286Rd:\ndim(M)=i\nmin x\u2208M: x 6=0\nx\u22a4P\u22121/2(C + \u03bbI)P\u22121/2x\n\u2016x\u20162\n= max M\u2286Rd:\ndim(M)=i\nmin x\u2208M: x 6=0\nx\u22a4P\u22121/2(C + \u03bbI)P\u22121/2x \u2016P\u22121/2x\u20162 \u00b7 \u2016P\u22121/2x\u20162 \u2016x\u20162\n\u2264\n\n  max\nM\u2286Rd: dim(M)=i\nmin x\u2208M: x 6=0\nx\u22a4P\u22121/2(C + \u03bbI)P\u22121/2x\n\u2016P\u22121/2x\u20162\n\n \u00d7\n\nmax x\u2208Rd: x 6=0\nx\u22a4P\u22121x\n\u2016x\u20162\n\n\n= \u03bbi (C + \u03bbI) \u00b7 \u03bb1(P\u22121) = (\u03bbi + \u03bb) \u00b7 (\u03c3\u03032k + \u03bb)\u22121 .\nFinally, using Theorem 3 we have that \u03c3\u03032k \u2265 \u03c32k \u2212 12\u03c32k+1 \u2265 12\u03c32k = 12\u03bbk and therefore,\n(\u03c3\u03032k + \u03bb) \u22121 \u2264 (12\u03bbk + \u03bb)\u22121 \u2264 2 (\u03bbk + \u03bb)\u22121 .\nLemma 4. (Lower bound on the smallest eigenvalue)\n\u03bbd(P \u22121/2CP\u22121/2) \u2265 \u03bb\n19(\u03bbk + \u03bb) .\nProof. Note that\n\u03bbd(P \u22121/2(C + \u03bbI)P\u22121/2) =\n1\n\u2016P 1/2(C + \u03bbI)\u22121P 1/2\u2016 , (12)\nso we can derive an upper bound on \u2016P 1/2(C + \u03bbI)\u22121P 1/2\u2016. Consider an arbitrary completion of v\u03031, . . . , v\u0303k to an orthonormal set, v\u03031, . . . , v\u0303d \u2208 Rn. Let V\u0303 \u2208 Rn\u00d7d be the matrix whose i-th column is v\u0303i. Since the spectral norm is unitary invariant and both U\u0303 and V\u0303 have orthonormal columns,\n\u2016P 1/2(C + \u03bbI)\u22121P 1/2\u2016 = \u2016U\u0303(\u03a3\u03032 + \u03bbI)1/2U\u0303\u22a4(C + \u03bbI)\u22121U\u0303(\u03a3\u03032 + \u03bbI)1/2U\u0303\u22a4\u2016 = \u2016V\u0303 (\u03a3\u03032 + \u03bbI)1/2U\u0303\u22a4(C + \u03bbI)\u22121U\u0303(\u03a3\u03032 + \u03bbI)1/2V\u0303 \u22a4\u2016 .\nDenote by Z\u0303 = U\u0303(\u03a3\u03032 + \u03bbI)1/2V\u0303 \u22a4. By the triangle inequality and the submutiplicativity of the spectral norm,\n\u2016Z\u0303\u22a4(C + \u03bbI)\u22121Z\u0303\u2016 \u2264 \u2016X\u0304\u22a4(C + \u03bbI)\u22121X\u0304\u2016 + \u2016(Z\u0303 \u2212 X\u0304)\u22a4(C + \u03bbI)\u22121(Z\u0303 \u2212 X\u0304)\u2016\n\u2264 \u2016X\u0304\u22a4(C + \u03bbI)\u22121X\u0304\u2016+ \u2016Z\u0303 \u2212 X\u0304\u20162\u2016(C + \u03bbI)\u22121\u2016 . (13)\nTo bound the first summand of Equation (13), we use the unitary invariance to obtain\n\u2016X\u0304\u22a4(C + \u03bbI)\u22121X\u0304\u2016 = \u2016V \u03a3U\u22a4U(\u03a32 + \u03bbI)\u22121U\u22a4U\u03a3V \u22a4\u2016\n= \u2016\u03a3(\u03a32 + \u03bbI)\u22121\u03a3\u2016 = max i \u03bb2i \u03bb2i + \u03bb \u2264 1 .\nFor the second summand of Equation (13), note that \u2016(C + \u03bbI)\u22121\u2016 = 1\u03bbd+\u03bb and that, using the triangle inequality,\n\u2016Z\u0303 \u2212 X\u0304\u2016 = \u2016(U\u0303\u03a3\u0303V\u0303 \u22a4 \u2212 X\u0304) + (Z\u0303 \u2212 U\u0303 \u03a3\u0303V\u0303 \u22a4)\u2016 \u2264 \u2016U\u0303\u03a3\u0303V\u0303 \u22a4 \u2212 X\u0304\u2016+ \u2016U\u0303((\u03a3\u03032 + \u03bbI)1/2 \u2212 \u03a3\u0303)V\u0303 \u22a4\u2016 .\nBy using unitary invariance together with the inequality \u221a \u03c3\u03032i + \u03bb \u2212 \u03c3\u0303i \u2264 \u221a \u03bb (which holds for every i), we get\n\u2016U\u0303((\u03a3\u03032 + \u03bbI)1/2 \u2212 \u03a3\u0303)V\u0303 \u22a4\u2016 = \u2016(\u03a3\u03032 + \u03bbI)1/2 \u2212 \u03a3\u0303\u2016 \u2264 \u221a \u03bb .\nHence, using the inequality (x+ y)2 \u2264 2x2 + 2y2, we obtain\n\u2016Z\u0303 \u2212 X\u0304\u20162 \u2264 2\u2016U\u0303\u03a3\u0303V\u0303 \u22a4 \u2212 X\u0304\u20162 + 2\u03bb .\nWe next derive an upper bound on \u2016U\u0303\u03a3\u0303V\u0303 \u22a4\u2212X\u0304\u2016. Since U\u0303 \u03a3\u0303V\u0303 \u22a4 = X\u0303k+\u03c3\u0303k \u2211d i=k+1 u\u0303iv\u0303 \u22a4 i ,\n\u2016U\u0303\u03a3\u0303V\u0303 \u22a4 \u2212 X\u0304\u2016 \u2264 \u2016X\u0303k \u2212 X\u0304\u2016+ \u03c3\u0303k \u2225 \u2225 \u2225 \u2225 \u2225 d\u2211\ni=k+1\nu\u0303iv\u0303 \u22a4 i \u2225 \u2225 \u2225 \u2225 \u2225 .\nUsing Theorem 3 we know that \u2016X\u0303k\u2212X\u0304\u2016 \u2264 1.5 \u03c3k and that \u03c3\u0303k \u2264 \u221a \u03c32k + 0.5 \u03c3 2 k+1 \u2264 1.5 \u03c3k. Combining this with the fact that \u2016 \u2211d i=k+1 u\u0303iv\u0303 \u22a4 i \u2016 = 1, we obtain\n\u2016U\u0303\u03a3\u0303V\u0303 \u22a4 \u2212 X\u0304\u2016 \u2264 3 \u03c3k .\nCombining the above inequalities, we obtain\n\u2016P 1/2(C + \u03bbI)\u22121P 1/2\u2016 \u2264 1 + 2 \u00b7 (3\u03c3k) 2 + 2\u03bb\n\u03bbd + \u03bb\n\u2264 19(\u03bbk + \u03bb) \u03bb ,\nand using Equation (12) we conclude our proof.\nProof. (of Theorem 5) The three last lemmas imply that the inequalities derived during the proof of Lemma 1 remain intact up to a multiplicative constant. Therefore, the bound Equation (10) on the condition number also holds up to a multiplicative constant. This completes the proof."}, {"heading": "5.5 Sketched Preconditioned SVRG", "text": "By equipping SVRG with the sketched preconditioner Equation (11), we obtain the Sketched Preconditioned SVRG (see Algorithm 3).\nProof. (of Theorem 1) The theorem follows from Theorem 5 and Theorem 2.\nAlgorithm 3 Sketched Preconditioned SVRG\n1: Input: x1, . . . , xn \u2208 Rd, y1, . . . , yn \u2208 R, \u01eb > 0 2: Parameters: \u03bb > 0, k \u2208 [d] 3: Let X\u0304 \u2208 Rd,n be the matrix whose i\u2019th column is (1/n)xi 4: Run the Block Lanczos method (Algorithm 2) with the input X\u0304, k, \u01eb\u2032 = 1/2 to\nobtain X\u0303k = U\u0303k\u03a3\u0303kV\u0303k 5: Let u\u0303i be the columns of U\u0303k and \u03c3\u0303i be the diagonal elements of \u03a3\u0303k 6: Form the preconditioner P\u22121/2 according to Equation (11) 7: Compute x\u0303i = P\u22121/2xi, bi = P\u22121/2ei 8: Let \u2113i(w) = n+dn 1 2 (w\n\u22a4x\u0303i \u2212 yi)2 for i = 1, . . . , n and \u2113i(w) = \u03bb(n + d)(w\u22a4bi)2 for i = n+ 1, . . . , n+ d 9: Let \u03b2i = n+dn \u2016x\u0303i\u20162 for i = 1, . . . , n and \u03b2i = \u03bb(n+d)\u2016bi\u2016 for i = n+1, . . . , n+ d. Let \u03b2\u0302 = 1n \u2211n+d i=1 \u03b2i\n10: Run SVRG (Algorithm 1) 11: Return w\u0302 = P 1/2w\u0303"}, {"heading": "6 The Empirical Gain of Sketched Preconditioning", "text": "In this section we empirically demonstrate the gain of our method. We consider both regression problems and binary classifications tasks, where the square loss serves as a surrogate for the zero-one loss. We use the following datasets:\n0 100 200 300 400 500 600 700 100\n101\n102\n103\n104\n105\n106\n(a) MNIST dataset.\n0 200 400 600 800 1,000 1,200 1,400 1,600 1,800 2,000 100\n101\n102\n103\n104\n(b) CIFAR-10 dataset.\n0 20 40 60 80 100 120 140 160 180 200 220 240 260 280 300 1\n1.05\n1.1\n1.15\n1.2\n1.25\n(c) RCV1 dataset.\n0 20 40 60 80 100 120 140 160 180 200 220 240 260 280 300 1\n1.02\n1.04\n1.06\n1.08\n1.1\n1.12\n1.14\n(d) Real-sim dataset.\n\u2022 real-sim:5 Here, n = 72309, d = 20958, and we consider a standard binary document classification task."}, {"heading": "6.1 Inspecting our theoretical speed-up", "text": "Recall that the ratio Equation (5) quantifies our theoretical speedup. Hence, we first empirically inspect the prefixes of the corresponding quantities (as a function of k) for each of the datasets (see Figure 1). We can see that while in MNIST and CIFAR-10 the ratio is large for small values of k, in RCV1 and real-sim the ratio increases very slowly (note that for the former two datasets we use logarithmic scale)."}, {"heading": "6.2 Empirical advantage of Sketched Preconditioned SVRG", "text": "We now evaluate Algorithm 3 and compare it to the SVRG algorithm of [21]. To minimally affect the inherent condition number, we added only a slight amount of regularization, namely, \u03bb = 10\u22128. The loss used is the square loss. The step size, \u03b7, is optimally tuned for each method. Similarly to previous work on SVRG [21, 5], the size of each epoch, m, is proportional to the number of points, n. We minimally preprocessed the data by average normalization: each instance vector is divided by the average \u21132-norm of the instances. The number of epochs is up to 60. Note that in all cases we choose a small preconditioning parameter, namely k = 30, so that the preprocessing time of Algorithm 3 is negligible. There is a clear correspondence between the ratios depicted in Figure 1 and the actual speedup. In other words, the empirical results strongly affirm our theoretical results."}, {"heading": "Acknowledgments", "text": "We thank Edo Liberty for helpful discussions. The work is supported by ICRI-CI and by the European Research Council (TheoryDL project).\n5https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/"}, {"heading": "A Omitted Proofs", "text": "Proof. (of Theorem 4) We first show that the average smoothness of L\u0303 is bounded by\n1\nn+ d\nn+d\u2211\ni=1\n\u03b2\u0303i \u2264 tr ( P\u22121/2 (C + \u03bbI)P\u22121/2 ) . (14)\nNote that for any w,\n\u22072\u2113\u0303i(w) = { n+d n x\u0303ix\u0303 \u22a4 i 1 \u2264 i \u2264 n,\n\u03bb(n+ d)bi\u2212nb \u22a4 i\u2212n n < i \u2264 n+ d .\nTherefore, using the fact that the spectral norm of a rank-1 psd matrix is equal to its trace, we obtain\n1\nn+ d\nn\u2211\ni=1\n\u03b2\u0303i = 1\nn+ d\nn+ d\nn\nn\u2211\ni=1\n\u2016x\u0303ix\u0303\u22a4i \u2016+ 1\nn+ d \u03bb(n+ d)\nd\u2211\nj=1\n\u2016bjb\u22a4j \u2016\n= 1\nn\nn\u2211\ni=1\ntr(x\u0303ix\u0303 \u22a4 i ) + \u03bb\nd\u2211\nj=1\ntr(bib \u22a4 i )\n= 1\nn tr(\nn\u2211\ni=1\nP\u22121/2xix \u22a4 i P\n\u22121/2) + \u03bb tr d\u2211\nj=1\n(P\u22121/2eie \u22a4 i P \u22121/2)\n= tr(P\u22121/2(C + \u03bbI)P\u22121/2) .\nHence, we deduce (14). We will conclude the theorem by showing that L\u0303 is \u03bbd(P\u22121/2(C + \u03bbI)P\u22121/2)strongly convex. Indeed, a similar calculation shows that the Hessian of L at any point w is given by\n\u22072L\u0303(w) = P\u22121/2(C + \u03bbI)P\u22121/2 .\nHence, we conclude the claimed bound."}], "references": [{"title": "Matrix analysis, volume 169", "author": ["Rajendra Bhatia"], "venue": "Springer Science & Business Media,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["Kenneth L Clarkson", "David P Woodruff"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization", "author": ["Roy Frostig", "Rong Ge", "Sham M Kakade", "Aaron Sidford"], "venue": "arXiv preprint arXiv:1506.07512,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Methods of conjugate gradients for solving linear systems", "author": ["Magnus Rudolph Hestenes", "Eduard Stiefel"], "venue": "NBS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1952}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Angen\u00e4herte aufl\u00f6sung von systemen linearer gleichungen", "author": ["Stefan Kaczmarz"], "venue": "Bulletin International de l\u2019Acade\u0301mie Polonaise des Sciences et des Lettres,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1937}, {"title": "A universal catalyst for firstorder optimization", "author": ["Hongzhou Lin", "Julien Mairal", "Zaid Harchaoui"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Randomized block krylov methods for stronger and faster approximate singular value decomposition", "author": ["Cameron Musco", "Christopher Musco"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "A method of solving a convex programming problem with convergence rate O(1/k)", "author": ["Yurii Nesterov"], "venue": "In Soviet Mathematics Doklady,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1983}, {"title": "Introductory lectures on convex optimization, volume 87", "author": ["Yurii Nesterov"], "venue": "Springer Science & Business Media,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["Nicolas L Roux", "Mark Schmidt", "Francis R Bach"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Improved approximation algorithms for large matrices via random projections", "author": ["Tamas Sarlos"], "venue": "In Foundations of Computer Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Sdca without duality, regularization, and individual convexity", "author": ["Shai Shalev-Shwartz"], "venue": "arXiv preprint arXiv:1602.01582,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["Shai Shalev-Shwartz", "Shai Ben-David"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "Mathematical Programming,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Numerical linear algebra, volume 50", "author": ["Lloyd N Trefethen", "David Bau III"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1997}, {"title": "Laplacian solvers and their algorithmic applications", "author": ["Nisheeth K Vishnoi"], "venue": "Theoretical Computer Science,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Randomized algorithms for low-rank matrix factorizations: sharp performance", "author": ["Rafi Witten", "Emmanuel Cand\u00e8s"], "venue": "bounds. Algorithmica,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Sketching as a tool for numerical linear algebra", "author": ["David P Woodruff"], "venue": "arXiv preprint arXiv:1411.4357,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["Lin Xiao", "Tong Zhang"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "On data preconditioning for regularized loss minimization", "author": ["Tianbao Yang", "Rong Jin", "Shenghuo Zhu", "Qihang Lin"], "venue": "Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "In this setting, stochastic iterative methods such as SDCA [15], SVRG [5], and SAG [11] have become a standard choice for minimizing the objective L.", "startOffset": 59, "endOffset": 63}, {"referenceID": 4, "context": "In this setting, stochastic iterative methods such as SDCA [15], SVRG [5], and SAG [11] have become a standard choice for minimizing the objective L.", "startOffset": 70, "endOffset": 73}, {"referenceID": 10, "context": "In this setting, stochastic iterative methods such as SDCA [15], SVRG [5], and SAG [11] have become a standard choice for minimizing the objective L.", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "Specifically, the overall complexity of a recent improved variant of SVRG due to [21] depends on the average condition number, which is defined as follows.", "startOffset": 81, "endOffset": 85}, {"referenceID": 15, "context": "For an accelerated version of the algorithm, we can replace \u03ba\u0302 by \u221a n\u03ba\u0302 [16, 7].", "startOffset": 72, "endOffset": 79}, {"referenceID": 6, "context": "For an accelerated version of the algorithm, we can replace \u03ba\u0302 by \u221a n\u03ba\u0302 [16, 7].", "startOffset": 72, "endOffset": 79}, {"referenceID": 13, "context": "However, to control the under/over fitting tradeoff, \u03bb has to decrease as n increases [14].", "startOffset": 86, "endOffset": 90}, {"referenceID": 7, "context": "In order to approximately extract the stronger directions while incurring a low computational cost, we rely on a variant of the Block Lanczos method due to [8] in order to compute an approximated truncated SVD (Singular Value Decomposition) of the correlation matrix C.", "startOffset": 156, "endOffset": 159}, {"referenceID": 20, "context": "By equipping the SVRG of [21] with this preconditioner, we find an \u01eb-approximate minimizer for Equation (1) (with probability at least 9/10) in additional runtime of O((\u03ba\u0303 + n + d)d log(1/\u01eb)), where \u03ba\u0303 = k\u03bbk+ \u2211 i>k \u03bbi \u03bb or \u03ba\u0303 = ( n(k\u03bbk+ \u2211 i>k \u03bbi) \u03bb )1/2 if we use accelerated SVRG.", "startOffset": 25, "endOffset": 29}, {"referenceID": 7, "context": "Our analysis relies on stronger per vector error guarantees Equation (9) due to [8].", "startOffset": 80, "endOffset": 83}, {"referenceID": 20, "context": "It should be emphasized that while we use a variant of SVRG due to [21], we could equally use a variant of SDCA [13] or develop such a variant for SAG or SAGA.", "startOffset": 67, "endOffset": 71}, {"referenceID": 12, "context": "It should be emphasized that while we use a variant of SVRG due to [21], we could equally use a variant of SDCA [13] or develop such a variant for SAG or SAGA.", "startOffset": 112, "endOffset": 116}, {"referenceID": 9, "context": "15 in [10]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 5, "context": "The Kaczmarz algorithm [6] has an identical complexity.", "startOffset": 23, "endOffset": 26}, {"referenceID": 3, "context": "Both the Conjugate Gradient (CG) method [4] and the Accelerated Gradient Descent (AGD) algorithm of [9] enjoy a better runtime of nd \u221a \u03ba log(1/\u01eb).", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "Both the Conjugate Gradient (CG) method [4] and the Accelerated Gradient Descent (AGD) algorithm of [9] enjoy a better runtime of nd \u221a \u03ba log(1/\u01eb).", "startOffset": 100, "endOffset": 103}, {"referenceID": 17, "context": "7 in [18]): If all but c \u2208 [d] eigenvalues of C+\u03bbI are contained in a range [a, b], then the runtime of CG is at most nd(c+ \u221a b/a log(1/\u01eb)).", "startOffset": 5, "endOffset": 9}, {"referenceID": 19, "context": "Linear Sketching: Several recently developed methods in numerical linear algebra are based on the so-called sketch-and-solve approach, which essentially suggests that given a matrixA, we first replace it with a smaller random matrix AS, and then perform the computation on AS [20, 2, 12].", "startOffset": 276, "endOffset": 287}, {"referenceID": 1, "context": "Linear Sketching: Several recently developed methods in numerical linear algebra are based on the so-called sketch-and-solve approach, which essentially suggests that given a matrixA, we first replace it with a smaller random matrix AS, and then perform the computation on AS [20, 2, 12].", "startOffset": 276, "endOffset": 287}, {"referenceID": 11, "context": "Linear Sketching: Several recently developed methods in numerical linear algebra are based on the so-called sketch-and-solve approach, which essentially suggests that given a matrixA, we first replace it with a smaller random matrix AS, and then perform the computation on AS [20, 2, 12].", "startOffset": 276, "endOffset": 287}, {"referenceID": 19, "context": "1 in [20]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 18, "context": "We need spectral norm bounds, which are known to be more challenging [19].", "startOffset": 69, "endOffset": 73}, {"referenceID": 7, "context": "Furthermore, as mentioned above, the success of our conditioning method heavily depends on the stronger per vector error guarantees Equation (9) obtained by [8] which are not obtained by simpler linear sketching methods.", "startOffset": 157, "endOffset": 160}, {"referenceID": 19, "context": "Sketched preconditioning: Recently, subspace embedding methods were used to develop cheap preconditioners for linear regression with respect to the squared loss [20].", "startOffset": 161, "endOffset": 165}, {"referenceID": 1, "context": "Precisely, [2] considered the case \u03bb = 0 (i.", "startOffset": 11, "endOffset": 14}, {"referenceID": 21, "context": "Efficient preconditioning based on random sampling: While we focus on reducing the dependence on the dimensionality of the data, other work investigated the gain from using only a random subset of the data points to form the conditioner [22].", "startOffset": 237, "endOffset": 241}, {"referenceID": 21, "context": "The theoretical gain of this approach has been established under coherence assumptions [22].", "startOffset": 87, "endOffset": 91}, {"referenceID": 16, "context": "the spectral norm [17].", "startOffset": 18, "endOffset": 22}, {"referenceID": 4, "context": "2 Stochastic Variance Reduced Gradient (SVRG) We consider a variant of the Stochastic Variance Reduced Gradient (SVRG) algorithm of [5] due to [21].", "startOffset": 132, "endOffset": 135}, {"referenceID": 20, "context": "2 Stochastic Variance Reduced Gradient (SVRG) We consider a variant of the Stochastic Variance Reduced Gradient (SVRG) algorithm of [5] due to [21].", "startOffset": 143, "endOffset": 147}, {"referenceID": 20, "context": "[21] Fix \u01eb > 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "In the original definition of SVRG [5], the indices it are chosen uniformly at random from [n], rather than proportional to \u03b2i.", "startOffset": 35, "endOffset": 38}, {"referenceID": 20, "context": "Since we rely on the improved variant of [21], our bound depends on the average of the \u03b2i\u2019s, which scale with the average norm of the preconditioned xi\u2019s.", "startOffset": 41, "endOffset": 45}, {"referenceID": 7, "context": "3 Randomized Block Lanczos A randomized variant of the Block Lanczos method due to [8] is detailed1 in Algorithm 2.", "startOffset": 83, "endOffset": 86}, {"referenceID": 7, "context": "1More precisely, Algorithm 2 in [8] returns the projection matrix \u0168k\u0168 k , while we also compute the SVD of \u0168k\u0168 k A.", "startOffset": 32, "endOffset": 35}, {"referenceID": 7, "context": "Algorithm 2 Block Lanczos method [8] 1: Input: A \u2208 Rd\u00d7n, k \u2264 d, \u01eb\u2032 \u2208 (0, 1) 2: q = \u0398 ( log(n) \u221a \u01eb ) , p = qk, \u03a0 \u223c N (0, 1)n\u00d7k 3: Compute K = [A\u03a0, (AA\u22a4)A\u03a0, .", "startOffset": 33, "endOffset": 36}, {"referenceID": 7, "context": "[8] Consider the run of Algorithm 2 and denote \u00c3k = \u0168k\u03a3\u0303k\u1e7ck = \u2211k i=1 \u03c3\u0303i\u0169i\u1e7d \u22a4 i .", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Using the Courant minimax principle [1], we obtain the following bound for all i \u2208 {k + 1, .", "startOffset": 36, "endOffset": 39}, {"referenceID": 2, "context": "Following [3], the classification task is to distinguish between the animal categories to the automotive ones.", "startOffset": 10, "endOffset": 13}, {"referenceID": 20, "context": "2 Empirical advantage of Sketched Preconditioned SVRG We now evaluate Algorithm 3 and compare it to the SVRG algorithm of [21].", "startOffset": 122, "endOffset": 126}, {"referenceID": 20, "context": "Similarly to previous work on SVRG [21, 5], the size of each epoch, m, is proportional to the number of points, n.", "startOffset": 35, "endOffset": 42}, {"referenceID": 4, "context": "Similarly to previous work on SVRG [21, 5], the size of each epoch, m, is proportional to the number of points, n.", "startOffset": 35, "endOffset": 42}], "year": 2016, "abstractText": "We develop a novel preconditioning method for ridge regression, based on recent linear sketching methods. By equipping Stochastic Variance Reduced Gradient (SVRG) with this preconditioning process, we obtain a significant speed-up relative to fast stochastic methods such as SVRG, SDCA and SAG.", "creator": "LaTeX with hyperref package"}}}